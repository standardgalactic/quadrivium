free ebooks ==>   www.ebook777.com
DIGITAL SIGNAL 
PROCESSING USING 
MATLAB FOR 
STUDENTS AND 
RESEARCHERS
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
DIGITAL SIGNAL 
PROCESSING USING 
MATLAB FOR 
STUDENTS AND 
RESEARCHERS
JOHN W. LEIS
University of Southern Queensland
A JOHN WILEY & SONS, INC., PUBLICATION
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
Copyright © 2011 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form 
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee 
to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, 
fax (978) 646-8600, or on the web at www.copyright.com. Requests to the Publisher for permission 
should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, 
Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts 
in preparing this book, they make no representations or warranties with respect to the accuracy or 
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of 
merchantability or ﬁtness for a particular purpose. No warranty may be created ore extended by sales 
representatives or written sales materials. The advice and strategies contained herein may not be 
suitable for your situation. You should consult with a professional where appropriate. Neither the 
publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, including 
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services please contact our Customer Care 
Department with the U.S. at 877-762-2974, outside the U.S. at 317-572-3993 or fax 317-572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print, 
however, may not be available in electronic format.
Library of Congress Cataloging-in-Publication Data:
Leis, John W. (John William), 1966-
 Digital Signal Processsing Using MATLAB for Students and Researchers / John W. Leis.
   p. cm
 Includes bibliographical references and index.
 ISBN 978-0-470-88091-3
 1. Signal processing–Digital techniques. 2. Signal processing–Mathematics–Data 
processing. 3. MATLAB. I. Title.
 TK5102.9.L4525 2011
 621.382′2–dc22
 
2010048285
Printed in Singapore.
10 9 8 7 6 5 4 3 2 1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
To Debbie, Amy, and Kate
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
CONTENTS
PREFACE 
XI
CHAPTER 1  WHAT IS SIGNAL PROCESSING? 
1
1.1 
Chapter Objectives 1
1.2 
Introduction 1
1.3 
Book Objectives 2
1.4 
DSP and ITS Applications 3
1.5 
Application Case Studies Using DSP 4
1.6 
Overview of Learning Objectives 12
1.7 
Conventions Used in This Book 15
1.8 
Chapter Summary 16
CHAPTER 2  MATLAB FOR SIGNAL PROCESSING 
19
2.1 
Chapter Objectives 19
2.2 
Introduction 19
2.3 
What Is MATLAB? 19
2.4 
Getting Started 20
2.5 
Everything Is a Matrix 20
2.6 
Interactive Use 21
2.7 
Testing and Looping 23
2.8 
Functions and Variables 25
2.9 
Plotting and Graphing 30
2.10 Loading and Saving Data 31
2.11 Multidimensional Arrays 35
2.12 Bitwise Operators 37
2.13 Vectorizing Code 38
2.14 Using MATLAB for Processing Signals 40
2.15 Chapter Summary 43
CHAPTER 3  SAMPLED SIGNALS AND DIGITAL PROCESSING 
45
3.1 
Chapter Objectives 45
3.2 
Introduction 45
3.3 
Processing Signals Using Computer Algorithms 45
3.4 
Digital Representation of Numbers 47
3.5 
Sampling 61
3.6 
Quantization 64
3.7 
Image Display 74
3.8 
Aliasing 81
3.9 
Reconstruction 84
3.10 Block Diagrams and Difference Equations 88
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
viii 
CONTENTS
3.11 Linearity, Superposition, and Time Invariance 92
3.12 Practical Issues and Computational Efﬁ ciency 95
3.13 Chapter Summary 98
CHAPTER 4  RANDOM SIGNALS 
103
4.1 
Chapter Objectives 103
4.2 
Introduction 103
4.3 
Random and Deterministic Signals 103
4.4 
Random Number Generation 105
4.5 
Statistical Parameters 106
4.6 
Probability Functions 108
4.7 
Common Distributions 112
4.8 
Continuous and Discrete Variables 114
4.9 
Signal Characterization 116
4.10 Histogram Operators 117
4.11 Median Filters 122
4.12 Chapter Summary 125
CHAPTER 5  REPRESENTING SIGNALS AND SYSTEMS 
127
5.1 
Chapter Objectives 127
5.2 
Introduction 127
5.3 
Discrete-Time Waveform Generation 127
5.4 
The z Transform 137
5.5 
Polynomial Approach 144
5.6 
Poles, Zeros, and Stability 146
5.7 
Transfer Functions and Frequency Response 152
5.8 
Vector Interpretation of Frequency Response 153
5.9 
Convolution 156
5.10 Chapter Summary 160
CHAPTER 6  TEMPORAL AND SPATIAL SIGNAL PROCESSING 
165
6.1 
Chapter Objectives 165
6.2 
Introduction 165
6.3 
Correlation 165
6.4 
Linear Prediction 177
6.5 
Noise Estimation and Optimal Filtering 183
6.6 
Tomography 188
6.7 
Chapter Summary 201
CHAPTER 7  FREQUENCY ANALYSIS OF SIGNALS 
203
7.1 
Chapter Objectives 203
7.2 
Introduction 203
7.3 
Fourier Series 203
7.4 
How Do the Fourier Series Coefﬁ cient Equations Come About? 209
7.5 
Phase-Shifted Waveforms 211
7.6 
The Fourier Transform 212
7.7 
Aliasing in Discrete-Time Sampling 231
7.8 
The FFT as a Sample Interpolator 233
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
CONTENTS 
ix
7.9 
Sampling a Signal over a Finite Time Window 236
7.10 Time-Frequency Distributions 240
7.11 Buffering and Windowing 241
7.12 The FFT 243
7.13 The DCT 252
7.14 Chapter Summary 266
CHAPTER 8  DISCRETE-TIME FILTERS 
271
8.1 
Chapter Objectives 271
8.2 
Introduction 271
8.3 
What Do We Mean by “Filtering”? 272
8.4 
Filter Speciﬁ cation, Design, and Implementation 274
8.5 
Filter Responses 282
8.6 
Nonrecursive Filter Design 285
8.7 
Ideal Reconstruction Filter 293
8.8 
Filters with Linear Phase 294
8.9 
Fast Algorithms for Filtering, Convolution, and Correlation 298
8.10 Chapter Summary 311
CHAPTER 9  RECURSIVE FILTERS 
315
9.1 
Chapter Objectives 315
9.2 
Introduction 315
9.3 
Essential Analog System Theory 319
9.4 
Continuous-Time Recursive Filters 326
9.5 
Comparing Continuous-Time Filters 339
9.6 
Converting Continuous-Time Filters to Discrete Filters 340
9.7 
Scaling and Transformation of Continuous Filters 361
9.8 
Summary of Digital Filter Design via Analog Approximation 371
9.9 
Chapter Summary 372
BIBLIOGRAPHY 
375
INDEX 
379
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 PREFACE 
 I was once asked what signal processing is. The questioner thought it had something 
to do with trafﬁ c lights. It became clear to me at that moment that although the 
theory and practice of signal processing in an engineering context has made possible 
the massive advances of recent times in everything from consumer electronics to 
healthcare, the area is poorly understood by those not familiar with digital signal 
processing (DSP). Unfortunately, such lack of understanding sometimes extends to 
those embarking on higher education courses in engineering, computer science, and 
allied ﬁ elds, and I believe it is our responsibility not simply to try to cover every 
possible theoretical aspect, but to endeavor to open the student ’ s eyes to the possible 
applications of signal processing, particularly in a multidisciplinary context. 
 With that in mind, this book sets out to provide the necessary theoretical and 
practical underpinnings of signal processing, but in a way that can be readily under-
stood by the newcomer to the ﬁ eld. The assumed audience is the practicing engineer, 
the engineering undergraduate or graduate student, or the researcher in an allied ﬁ eld 
who can make use of signal processing in a research context. The examples given 
to introduce the topics have been chosen to clearly introduce the motivation behind 
the topic and where it might be applied. Necessarily, a great deal of detail has to be 
sacriﬁ ced in order to meet the expectations of the audience. This is not to say that 
the theory or implementation has been trivialized. Far from it; the treatment given 
extends from the theoretical underpinnings of key algorithms and techniques to 
computational and numerical aspects. 
 The text may be used in a one - term or longer course in signal processing, and 
the assumptions regarding background knowledge have been kept to a minimum. 
Shorter courses may not be able to cover all that is presented, and an instructor may 
have to sacriﬁ ce some breadth in order to ensure adequate depth of coverage of 
important topics. The sections on fast convolution and ﬁ ltering, and medical image 
processing, may be omitted in that case. Likewise, recursive ﬁ lter design via analog 
prototyping may be omitted or left to a second course if time does not permit 
coverage. 
 A basic understanding of algebra, polynomials, calculus, matrices, and vectors 
would provide a solid background to studying the material, and a ﬁ rst course 
in linear systems theory is an advantage but is not essential. In addition to the 
aforementioned mathematical background, a good understanding of computational 
principles and coding, and a working knowledge of a structured programming 
language is desirable, as is prior study of numerical mathematics. Above all, these 
xi
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
xii 
PREFACE
should not be considered as a list of essential prerequisites; the reader who is lacking 
in some of these areas should not be deterred. 
 It is hoped that the problems at the end of each chapter, in conjunction with 
the various case studies, will give rise to a sufﬁ ciently rich learning environment, 
and appropriately challenging term projects may be developed with those problems 
as starting points. 
 John W. Leis 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER  1 
WHAT IS SIGNAL 
PROCESSING?  
 1.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should 
 1.  be able to explain the broad concept of digital signal processing (DSP); 
 2.  know some of the key terms associated with DSP; and 
 3.  be familiar with the conventions used in the book, both mathematical and for 
code examples. 
 1.2  INTRODUCTION 
 Signals are time - varying quantities which carry information. They may be, for 
example, audio signals (speech, music), images or video signals, sonar signals or 
ultrasound, biological signals such as the electrical pulses from the heart, commu-
nications signals, or many other types. With the emergence of high - speed, low - cost 
computing hardware, we now have the opportunity to analyze and process signals 
via computer algorithms. 
 The basic idea is straightforward: Rather than design complex circuits to 
process signals, the signal is ﬁ rst converted into a sequence of numbers and pro-
cessed via software. By its very nature, software is more easily extensible and more 
versatile as compared with hard - wired circuits, which are difﬁ cult to change. 
Furthermore, using software, we can build in more  “ intelligence ” into the operation 
of our designs and thus develop more human - usable devices. 
 A vitally important concept to master at the outset is that of an  algorithm : the 
logical sequence of steps which must be followed in order to generate a useful result. 
Although this deﬁ nition is applicable to general - purpose information processing, the 
key difference is in the nature of the data which are processed. In signal processing, 
the data sequence represents information which is not inherently digital and is 
usually imprecise. 
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
 For example, the algorithm for calculating the account balance in a person ’ s 
bank account after a transaction deals directly with numbers; the algorithm for 
determining whether a sample ﬁ ngerprint matches that of a particular person must 
cope with the imperfect and partially speciﬁ ed nature of the input data. It follows 
that the designer of the processing algorithm must understand the nature of the 
underlying sampled data sequence or  signal . 
 Furthermore, many signal processing systems are what is termed  real time ; 
that is, the result of the processing must be available within certain time constraints 
for it to be of use. If the result is not available in time, it may be of no use. For 
example, in developing a system which records the heart signal and looks for abnor-
malities, we may have a time frame of the order of seconds in which to react to any 
change in the signal pattern and to sound an alert. 
 The order of steps in the algorithm, and any parameters applicable to each 
step, must be decided upon by the designer. This may be done via theoretical analy-
sis, experimentation using typical signal data or, more often, a combination of the 
two. Furthermore, the processing time of the algorithm must often be taken into 
account: A speech recognition system which requires several minutes (or even 
seconds) to convert a simple spoken text into words may not ﬁ nd much practical 
application (even though it may be useful for theoretical studies). 
 Signal processing technology relies on several ﬁ elds, but the key ones are 
 Analog electronics   to capture the real - world quantity and to preprocess it 
into a form suitable for further digital computer manipulation 
 Digital representations   of the real world, which requires discrete sampling 
since the values of the real - world signal are sampled at predeﬁ ned, discrete 
intervals, and furthermore can only take on predeﬁ ned, discrete values 
 Mathematical analysis    in order to ﬁ nd ways of analyzing and understanding 
complex and time - varying signals; the mathematics helps deﬁ ne the pro-
cessing algorithms required.  
 Software algorithms  in order to implement the equations described by the 
mathematics on a computer system 
 Some examples of real - world signal processing problems are presented later in this 
chapter. 
 1.3  BOOK OBJECTIVES 
 This book adopts a  “ hands - on ” approach, with the following key objectives:
 1.  to introduce the ﬁ eld of signal processing in a clear and lucid style that empha-
sizes principles and applications in order to foster an intuitive approach to 
learning and  
 2.  to present signal processing techniques in the context of  “ learning outcomes ” 
based on self - paced experimentation in order to foster a self - directed investi-
gative approach. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
1.4 DSP AND ITS APPLICATIONS 
3
 It is hoped that by adopting this  “ learn - by - doing ” approach, the newcomer to the 
ﬁ eld will ﬁ rst develop an intuitive understanding of the theory and concepts, from 
which the analytical details presented will ﬂ ow logically. The mathematical and 
algorithmic details are presented in conjunction with the development of each broad 
topic area, using real - world examples wherever possible. Obviously, in some cases, 
this requires a little simpliﬁ cation of all the details and subtleties of the problem 
at hand. 
 By the end of the book, the reader will 
 1.  have an appreciation of where the various algorithmic techniques or  “ building 
blocks ” may be used to address practical signal processing problems, 
 2.  have sufﬁ cient insight to be able to develop signal processing algorithms for 
speciﬁ c problems, and 
 3.  be well enough equipped to be able to appreciate new techniques as they are 
developed in the research literature. 
 To gain maximum beneﬁ t from the presentation, it is recommended that the exam-
ples using MATLAB  ®  be studied by the reader as they are presented. MATLAB is 
a registered trademark of The MathWorks, Inc. For MATLAB product information, 
please contact 
 The MathWorks, Inc. 
 3 Apple Hill Drive 
 Natick, MA, 01760 - 2098 
 Tel: 508 - 647 - 7000 
 Fax: 508 - 647 - 7101 
 E - mail:  info@mathworks.com 
 MATLAB is discussed further in Chapter  2 . The reader is encouraged to experiment 
by changing some of the parameters in the code given to see their effect. All 
examples in the book will run under the academic version of MATLAB, without 
additional  “ toolboxes. ” 
 1.4  DSP AND  ITS APPLICATIONS 
 Application areas of signal processing have grown dramatically in importance in 
recent times, in parallel with the growth of powerful and low - cost processing cir-
cuits, and the reduction in price of computer memory. This has led, in turn, to many 
new applications, including multimedia delivery and handheld communication 
devices, with the convergence of computer and telecommunications technologies. 
However, many important applications of DSP may not be as immediately obvious:
 Speech recognition    provides a more natural interface to computer systems 
and information retrieval systems (such as telephone voice - response 
systems).  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
 Image recognition  involves recognizing patterns in images, such as character 
recognition in scanned text or recognizing faces for security systems, and 
handwriting recognition. 
 Image enhancement   is the improvement of the quality of digital images, for 
example, when degraded by noise on a communications channel or after 
suffering degradation over time on older recording media. 
 Audio enhancement and noise reduction   is the improvement of audio quality, 
particularly in  “ acoustically difﬁ cult ” environments such as vehicles. In 
cars and planes, for example, this is a desirable objective in order to 
improve passenger comfort and to enhance safety. 
 Digital music    in the entertainment industry uses special effects and 
enhancements — for example, adding three - dimensional sound  “ presence ” 
and simulating reverberation from the surroundings.  
 Communications and data transmission   relies heavily on signal processing. 
Error control, synchronization of data, and maximization of the data 
throughput are prime examples.  
 Biomedical applications    such as patient monitoring are indispensable in 
modern medical practice. Medical image processing and storage continues 
to attract much research attention.  
 Radar, sonar, and military applications   involve detection of targets, location 
of objects, and calculation of trajectories. Civilian applications of the Global 
Positioning System (GPS) are an example of complex signal processing 
algorithms which have been optimized to operate on handheld devices. 
 Note that these are all the subject of ongoing research, and many unsolved problems 
remain. These are also very difﬁ cult problems — consider, for example, speech recog-
nition and the many subtle differences between speakers. Exact algorithmic compari-
son using computers relies on precise matching — never mind the differences between 
people: Our own speech patterns are not entirely repeatable from one day to the next! 
 1.5  APPLICATION CASE STUDIES USING  DSP 
 Some application examples of DSP techniques are given in this section. It is certainly 
not possible to cover all possible aspects of DSP in the short space available nor to 
cover them in depth. Rather, the aim is to obtain a deeper insight into some problems 
which can be solved using DSP. 
 The following sections give two brief case studies of one - dimensional signal 
processing applications, where we have one sampled parameter varying with time, 
followed by two studies into two - dimensional signal processing applications. 
 1.5.1  Extracting Biomedical Signals 
 A great many applications of DSP exist in the medical ﬁ eld. Various measurement 
modalities — ultrasound, image, X - ray, and many others — are able to yield important 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
1.5 APPLICATION CASE STUDIES USING DSP 
5
diagnostic information to the clinician. In essence, the role of signal processing is 
to enhance the available measurements so as yield insight into the underlying signal 
properties. In some cases, the measurements may be compared to databases of exist-
ing signals so as to aid diagnosis. 
 Figure  1.1 shows the measurements taken in order to provide an electrocar-
diogram (ECG) signal, which is derived from a human heartbeat. The earliest ECG 
experiments were performed around a century ago, but the widespread application 
and use of the ECG was limited by the very small signal levels encountered. In the 
case illustrated in Figure  1.1 , a so - called three - lead ECG uses external connections 
on the skin, to the wrists, and ankles. It may be somewhat surprising that such 
external connections can yield information about the internal body signals control-
ling the heart muscles, and indeed the signals measured are quite small (of the order 
of microvolts to millivolts). The large amount of ampliﬁ cation necessary means that 
noise measurement is also ampliﬁ ed. In addition, both the sampling leads and the 
body itself act as antennas, receiving primarily a small signal at the frequency of 
the main electricity supply. This, of course, is unwanted. 
 FIGURE 1.1   Three - lead electrocardiograph (ECG) signals. Note the signiﬁ cant amount 
of interference (noise) in the raw signals (top three traces).  
ECG electrodes (3) and filtered signal
0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Time (s)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
 The unprocessed lead traces of Figure  1.1 clearly show the result of such 
interfering signals. Figure  1.2 shows a representative waveform after processing. 
The signal is sampled at 600 samples per second, and a digital ﬁ lter has been applied 
to help reduce the unwanted interference components. This type of  ﬁ ltering operation 
is one of the fundamental DSP operations. How to parameterize the digital ﬁ lter so 
that it removes as much as possible the unwanted interfering signal(s) and retains 
as much of the desired signal without alteration is an important part of the DSP ﬁ lter 
design process. Algorithms for digital (or discrete - time, quantized - amplitude) ﬁ lters 
and the design approaches applicable in various circumstances are discussed in 
Chapters  8 and  9 . 
 A great deal of additional information may be gleaned from the ECG signal. 
The most obvious is the heart rate itself, and visual inspection of the ﬁ ltered signal 
may be all that is necessary. However, in some circumstances, automatic monitoring 
without human intervention is desirable. The problem is not necessarily straightfor-
ward since, as well as the aforementioned interference, we have to cope with inherent 
physiological variability. The class of  correlation algorithms is applicable in this 
situation; correlation is covered in Chapter  6 . 
 1.5.2  Audio and Acoustics 
 Audio processing in general was one of the ﬁ rst application areas of DSP — and 
continues to be — as new applications emerge. In this case study, some parameters 
relating to room acoustics are derived from signal measurements. The experimental 
setup comprises a speaker which can synthesize various sounds, together with a 
microphone placed at a variable distance from the speaker, all in a room with 
unknown acoustic properties. 
 Consider ﬁ rst the case of generating a pure tone or sinusoid. We need to gener-
ate the samples corresponding to the mathematical sine function and do so at the 
 FIGURE 1.2   A ﬁ ltered electrocardiograph (ECG) signal. Note that the horizontal axis is 
now shown as the sample number; hence, the sample period must be known in order to 
translate the signal into time. The sample period is the reciprocal of the sample frequency  f s 
(in this case, 600 samples per second).  
0
500
1,000
1,500
2,000
2,500
3,000
Sample number
Filtered ECG fs = 600 Hz, fourth-order low-pass filtered
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
1.5 APPLICATION CASE STUDIES USING DSP 
7
required sampling rate (in this example, 96,000 samples per second). Figure  1.3 
shows the resulting measurements, with the microphone placed at various distances 
from the speaker. What is immediately clear is that the amplitude of the received 
signal decreases with the distance, as would be expected. Furthermore, the relative 
delay or  phase of the tone changes according to the distance. This change could be 
used to estimate the distance of the microphone from the speaker. One complication 
is that the signal clearly contains some additional noise, which must be reduced in 
order to make a more accurate measurement. 
 The estimates derived from Figure  1.3 yield a ﬁ gure of 12 - cm movement for 
the case where the microphone was moved from 5 to 15  cm, and an estimate of 18  cm 
for the 5 - to 25 - cm microphone movement. What factors inﬂ uence the accuracy of 
the result? Clearly, we must make an assumption for the speed of sound, but in 
addition, we need to determine the relative phase of the sinusoids as accurately as 
possible. Correlation algorithms (mentioned earlier) are of some help here. It also 
helps to maximize the sampling rate since a higher rate of sampling means that the 
sound travels a shorter distance between samples. 
 Other information about the speaker – microphone – room coupling can be 
gleaned by using a different test signal, which is not difﬁ cult to do using DSP tech-
niques. Figure  1.4 shows the result of using random noise for the output signal (the 
random noise is also termed  “ white noise ” by analogy with white light, which com-
prises all wavelengths). This type of signal is a broad - spectrum one, with no one 
 FIGURE 1.3   The response of the speaker – microphone system to a pure tone, with the 
microphone placed at various distances away from the source. The relative delay may be 
used to estimate distance. Note that the signals, particularly those at larger distances, have 
a larger component of noise contamination. 
0
0.002 0.004 0.006 0.008
0.01 0.012 0.014 0.016 0.018
0.02
−2,000
−1,500
−1,000
−500
0
500
1,000
1,500
2,000
Time (ms)
Sample value
25 cm
15 cm
5 cm
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
pure tone dominating over any other. The ﬁ gure shows the energy content over the 
frequency range of the output signal and the estimated energy content over the same 
range of frequencies as received by the microphone. This is a derived result in that 
all that is available directly are the sample values at each sampling instant, and it is 
necessary to derive the corresponding frequency components indirectly. The algo-
rithms for performing this analysis are introduced in Chapter  7 . From Figure  1.4 it 
is evident that the speaker – microphone channel exhibits what is termed a  “ bandpass ” 
characteristic — lower frequencies are reduced in amplitude (attenuated), and higher 
frequencies are also reduced substantially. This information may be used to com-
pensate for the shortcomings of a particular set of conditions — for example, boosting 
frequencies which have been reduced so as to compensate for the acoustics and 
speaker setup. 
 Figure  1.5 shows a different set of results as calculated from the broad -
 spectrum experiment. Here, we perform a system identiﬁ cation algorithm using the 
correlation function (as discussed in Chapter  6 ) at each of the microphone displace-
ments. Each response has been normalized to a  ± 1 amplitude level for easier com-
parison. It is clear that the displacement of each waveform corresponds to the time 
delay, as discussed. However, the shape of the graphs is approximately the same. 
This characteristic response shape is termed the  impulse response . This is a key 
concept in signal processing — the impulse response is that response produced by a 
system (in this case, the electroacoustic system) as a result of a single pulse. In most 
 FIGURE 1.4   The estimated frequency response of the audio system. This particular 
system clearly shows a marked reduction in the transmission of higher - frequency audio 
components.  
0
6
12
18
24
−30
−20
−10
0
10
Frequency (kHz)
Power (dB)
Output power spectrum
0
6
12
18
24
−30
−20
−10
0
10
Frequency (kHz)
Power (dB)
Input power spectrum
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
1.5 APPLICATION CASE STUDIES USING DSP 
9
cases, a single pulse is not a feasible test signal, and thus the impulse response cannot 
be measured directly. Rather, we use methods such as that described using white 
noise, to estimate the impulse response of the system. Chapter  6 discusses correlation 
and system identiﬁ cation, and Chapter  7 further examines the concept of frequency 
response. 
 1.5.3  Image Processing 
 The processing of digital pictures, in particular, and digital images from sensor 
arrays, in general, is an important aspect of DSP. Because of the larger processing 
and memory requirements inherent in two - dimensional pictures, this area was not 
as highly developed initially. Today, however, two - and even three - dimensional 
signals are routinely processed; one may even consider some implementations to be 
four dimensional, with the fourth dimension being time  t , along with spatial dimen-
sions  x ,  y , and  z . 
 Figure  1.6 illustrates the problem of determining whether a particular image 
taken with a digital camera is in focus. A number of images are taken with the lens 
at various positions relative to the image sensor. From the ﬁ gure, it is clear which 
of the four images presented is closest to being in focus. So we ask whether it is 
 FIGURE 1.5   The computed impulse response of the audio system, with the microphone 
placed at varying displacements. As well as the delay due to acoustic propagation, it is 
possible to derive the estimated impulse response, which is characteristic of the system. 
0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
−1
0
1
Time ms
Amplitude
Impulse response (5 cm)
0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
−1
0
1
Time ms
Amplitude
Impulse response (15 cm)
0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
−1
0
1
Time (ms)
Amplitude
Impulse response (25 cm)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
10 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
possible to develop an algorithm for determining this automatically. Figure  1.7 
shows the results of testing across a set of 40 images. We need to derive one param-
eter which represents the concept of  “ focus ” as we would interpret it. In this case, 
the algorithms developed consist of ﬁ ltering, which is akin to determining the sharp-
ness of the edges present in the image, followed by a detection algorithm. Figure 
 1.7 shows the residual signal energy present in the ﬁ ltered signal for each image, 
with the camera ’ s autofocus image shown for comparison. Clearly, the peak of the 
calculated parameter corresponds to the relative degree of focus of the image. 
Chapter  3 introduces the digital processing of images; Chapters  8 and  9 consider 
digital ﬁ lters in more detail. 
 1.5.4  Biomedical Visualization 
 Finally, we consider another biomedical application in the visual domain, but one 
that is not  “ imaging ” in the conventional sense of taking a picture. In this case, we 
investigate the area of computerized tomography (CT), which provides clinicians 
 FIGURE 1.6   Images for the focusing experiment. The key question is to be able to 
determine which of the set is closest to being in focus. 
Image 4
Image 11
Image 16
Image 26
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
1.5 APPLICATION CASE STUDIES USING DSP 
11
 FIGURE 1.7   The relative value of the frequency - ﬁ ltered root mean square energy 
derived for each image. This is seen to correspond to the relative focus of each image. In 
this case, we need to apply several DSP algorithms to the image data to synthesize one 
parameter which is representative of the quantity we desire, the degree of focus. The 
asterisk (*) indicates the energy in the camera ’ s auto - focused image. 
0
5
10
15
20
25
30
35
40
0
1
2
3
4
5
6
RMS energy of high-pass filtered image
Image number
Autofocus
with an unprecedented view inside the human body from external noninvasive scan-
ning measurements. 
 The fundamental idea of tomography is to take many projections through an 
object and to construct a visualization of the internals of the object using post-
processing algorithms. Figure  1.8 shows a schematic of the setup of this arrange-
ment. The source plane consists of X - ray or other electromagnetic radiations 
appropriate to the situation. A series of line projections are taken through the object, 
and the measurements at the other side are collated as illustrated by the graph in the 
ﬁ gure . This gives only one plane through the object — a cross - sectional view only. 
What we desire is a two - dimensional view of the contents of the object. In a math-
ematical sense, this means a value of  f ( x ,  y ) at every ( x ,  y ) point in a plane. A single 
cross - sectional slice does not give us an internal view, only the cross - sectional 
projection. 
 The key to the visualization is to take multiple cross sections, as illustrated in 
Figure  1.9 . This shows two cross sections of the object, each at an angle  θ from 
the  x axis. The projection of a particular point, as illustrated in the ﬁ gure, yields the 
equivalent density at the point ( x ,  y ) inside the object, shown as the dot where the 
two ray traces intersect. The ﬁ gure shows only two projections; in reality, we need 
to take multiple projections around the object in order to resolve all ambiguities in 
the value of the intensity  f ( x ,  y ). 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
12 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
 If we take sufﬁ cient projections around the object, its internals can be visual-
ized. Figure  1.10 illustrates this process. On the left, we see the so - called Shepp –
 Logan phantom, which is an  “ artiﬁ cial ” human head containing ellipses representing 
tissue of various densities (Shepp and Logan 1974). The image on the right shows 
the reconstruction of the phantom head, using a back projection algorithm which 
accumulates all the projections  P  θ  ( s ) for many angles,  θ , so as to approximate the 
internal density at a point  f ( x ,  y ). Figure  1.10 shows a deliberately lower - resolution 
reconstruction with a limited number of angular measurements so as to illustrate the 
potential shortcomings of the process. Chapter  6 investigates the signal processing 
required in more detail. 
 1.6  OVERVIEW OF LEARNING OBJECTIVES 
 This text has been designed to follow a structured learning path, with examples using 
MATLAB being an integral part. The chapters are organized as follows:
 Chapter  2  covers the basics of the MATLAB programming language and 
environment. The use of MATLAB is central to the development of the 
learn - by - doing approach, and the treatment given in this chapter will serve 
as a grounding for subsequent chapters. 
 FIGURE 1.8   The projection of a source through an object, which can be interpreted as 
the line integral or the Radon transform from source to detector through the object. 
x
y
s
Pθ(s)
f (x, y)
Object
Source
θ
s
u
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 FIGURE 1.9   Interpretation of the back projection algorithm for reconstructing the 
internal density of an object, commonly known as a CT scan. By taking multiple external 
measurements, a cross section of the human body can be formed. From this cross section, 
it is possible to estimate the internal density at the point  I , where the lines intersect. To 
obtain sufﬁ cient clarity of representation, a large number of such points must be produced, 
and this can be very computationally intensive.  
x
y
s
Pθ 1 (s)
s
u
s
Pθ 2 (s)
I
θ
 FIGURE 1.10   An example of the back projection algorithm for tomography. The 
Shepp – Logan head phantom image is shown on the left, with a low - resolution 
reconstruction shown on the right. The lower resolution allows the scan lines to be seen, as 
illustrated in the previous ﬁ gures. 
True image matrix f(x, y)
Back projection image matrix b(x, y)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
14 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
 Chapter  3   looks at how signals in the real world are acquired for processing 
by computer. Computer arithmetic is introduced since it is fundamental to 
any signal processing algorithms where the processing is performed digi-
tally. How signals in the real world are acquired is discussed, and from all 
these aspects, we can gauge the processing accuracy, speed, and memory 
space required for any given application. 
 Chapter  4   looks at random signals. The  “ noise ” which affects signals is often 
random, and thus it is important to understand the basics of how random 
signals are characterized. The examples in the chapter look at noise in both 
audio signals and images. 
 Chapter  5   introduces the representation of known signals via mathematical 
equations. This is important, as many signals used in communications 
systems, for example, must be generated in this way. This is followed by 
a look at how systems alter signals as they pass through, which is the fun-
damental mode of operation of many signal processing systems. 
 Chapter  6   looks at how we can sample signals in the time domain and the 
spatial domain. One important principle in this regard is called  “ correla-
tion, ” which is essentially comparing two signals to determine their degree 
of similarity. In the real world, any two given signals won ’ t ever be identical 
in a sample - for - sample sense. Correlation allows us to tell if two signals 
are  “ somewhat similar. ” The basic concept of correlation is then extended 
into noise ﬁ ltering, where a signal is contaminated by noise which we 
can estimate. Finally, spatial signal processing is examined, with a particu-
lar emphasis on the tomography problem as described in the example 
above. 
 Chapter  7   looks at signals from another perspective: their frequency content. 
This is a very important concept and is fundamentally important to a great 
many signal processing techniques. Consider, for example, two keys on a 
piano: They sound different because of the particular mixture of frequency 
components present. The technique called Fourier analysis is introduced 
here, as it allows the determination of which speciﬁ c frequency components 
are present in a given signal. A related frequency approach, called the 
cosine transform, is then introduced. This ﬁ nds a great deal of application 
in digital media transmission (e.g., digital television, compressed music, 
and the JPEG digital image format). 
 Chapter  8  examines how we can ﬁ lter, or alter subject to speciﬁ cations, a 
given signal to suit some purpose. This might be, for example, removing 
some interfering noise or enhancing one or more of the frequencies present. 
These algorithms, although very useful in processing signals, often require 
a large number of computations and hence can be slow to run on affordable 
computer hardware. For this reason, fast processing algorithms have been 
developed to obtain the same result with fewer computations. These are 
also covered in this chapter. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
1.7 CONVENTIONS USED IN THIS BOOK 
15
 Chapter  9  introduces a different type of discrete - time ﬁ lter, the so - called 
recursive ﬁ lter, which is used for more efﬁ cient processing in some circum-
stances. It is useful to have a good theoretical understanding of the possible 
approaches to digital ﬁ ltering, and this chapter complements the previous 
chapter in this regard. 
 1.7  CONVENTIONS USED IN THIS BOOK 
 Because signal processing is based on algorithms, and algorithms are based on 
mathematics, a large proportion of time must be spent explaining algorithms and 
their associated mathematics. A basic grounding in linear algebra and calculus is 
necessary for some sections, as is an understanding of complex numbers. Because 
we often have to explain the use of blocks of samples and how they are stored and 
processed, the concepts of vectors and matrices are essential. To avoid confusion 
between scalar values and vectors, vectors and matrices, and constants and variables, 
the conventions used throughout the book are shown in Table  1.1 . 
 MATLAB code in a script ﬁ le is shown as follows. It may be typed into the 
MATLAB command window directly or entered into a ﬁ le. 
  
 TABLE 1.1  Mathematical Notation Conventions Used in the Book 
 x 
 Scalar variables (lowercase) 
 N 
 Integer constants (uppercase) 
 n 
 Integer variable over a range, for example,  
x n
n
N
( )
=
−
∑
0
1
 
 j 
 Unit complex number,  −=
1
1
2
ej π /  
 x 
 Column vector (bold font, lowercase) 
 x k 
 k th column vector (in a matrix) 
 A 
 Matrix (bold font, uppercase) 
 a ij 
 Element ( i ,  j ) of matrix  A 
 A − 1 
 Inverse of matrix  A 
 A + 
 Pseudoinverse of matrix  A 
 p * 
 Complex conjugate of  p 
 R xy 
 Correlation matrix of  x and  y 
 E { · } 
 Expectation operator, average of a sequence 
  N 
 N - dimensional parameter space 
 | x | 
 Absolute value (magnitude) of  x 
 || x || 
 Norm (length) of vector  x 
 ˆx 
 Estimate of  x 
 δ x 
 Change in  x 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
16 
CHAPTER 1 WHAT IS SIGNAL PROCESSING?
 Where MATLAB script is entered interactively and the result is shown, it is 
presented as follows: 
  
 m  =  rand (2, 2) 
 m  = 
            0.9501       0.6068 
            0.2311       0.4860 
 inv (m) 
 ans  = 
            1.5117       − 1.8876 
            − 0.7190      2.9555  
 Where a MATLAB variable or function is described in - text, it is shown as 
follows:  fft (). 
 Each chapter concludes with a set of problems. Some of these focus on a 
mathematical solution only; some require the further development of MATLAB code 
described in the chapter; and some require both mathematical analysis and algorithm 
development.  
 1.8  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  The role of DSP and some application areas 
 •  The learning objectives for subsequent chapters 
 •  The notational conventions used throughout the book 
 REVIEW QUESTIONS 
 1.1.  Two of the case studies cited in this chapter have been associated with Nobel Prizes 
in the past. Which two? How has DSP enabled the original discoveries to fulﬁ ll their 
promise? 
 1.2.  Describe any applications of DSP in the room in which you are sitting. 
 1.3.  From the ECG in Figure  1.2 , estimate the heart rate. Write down the steps you took to 
do this. What practical problems may arise in implementing your algorithm? For 
 d  =  zeros (2  * N  + 1, 1); 
 d(2  * N  + 1)  = 1; 
 d(1)  = (1/j)  ˆ (2  * N);  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
1.8 CHAPTER SUMMARY 
17
example, a step such as  “ ﬁ nd the largest peak ”  could be ambiguous if there are several 
peaks. 
 1.4.  Suppose the head cross - sectional slice as depicted in Figure  1.10 is required to have a 
resolution of 2,048 points horizontally and 1,024 points vertically. How many data 
points are there in total? If the cross section has to be produced within 30 seconds, 
roughly how long would be available for the computation of each point? 
 1.5.  Consider the sound delay measurement system described in this chapter. Suppose the 
samples are taken at a rate of 10,000 samples per second. Look up an estimate of the 
speed of sound. How far would a sound wave travel in the time taken between samples? 
How could we increase the accuracy of the distance measurements? What other vari-
able aspects should be considered?  
 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER 2 
MATLAB FOR SIGNAL 
PROCESSING  
 2.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to:
 1.  use the MATLAB interactive window command line, and enter MATLAB 
scripts. 
 2.  use MATLAB function ﬁ les. 
 3.  be able to display image data from a matrix. 
 4.  be able to play back audio data from a vector. 
 5.  understand the concepts of signal data ﬁ le storage and data formats. 
 2.2  INTRODUCTION 
 MATLAB  ®  is used extensively throughout this text to illustrate practical signal 
processing concepts. This tutorial chapter introduces some of the features which are 
useful in this regard. Note that the introductory nature of the tutorial means that it 
is by no means exhaustive in its examination of MATLAB ’ s capabilities. 
 2.3  WHAT IS MATLAB? 
 MATLAB is a commercial software product available from The Mathworks. 1 
It consists of a main  “ engine ” to perform computations, and several (optional) 
extended - function libraries (called  “ toolboxes ” ) for special - purpose applications. 
The main MATLAB engine has many robust mathematical functions built - in. It is 
an  interpreted language — meaning that each line of source is parsed (analyzed and 
converted) in text form, without being converted into a native machine code execut-
able. This means that it may be slower to execute the given code in many circum-
stances than equivalent code written for a speciﬁ c processor. 
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
19
  1  Website:  http://www.mathworks.com/. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
20 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 It is worth noting that DSP systems are often developed in MATLAB, and 
implemented on the target hardware using the C language. This is because MATLAB 
provides an easier environment in which to design and test algorithms, and the 
so - called  “ target ” devices (the end - product) often have limited memory, display, or 
other constraints. The C language (and by extension, C + + ) is able to produce assem-
bly language code for target processors, which is optimized for that particular 
device ’ s instruction set. The MATLAB syntax is, in many ways, somewhat similar 
to the C programming language. 
 MATLAB may be used interactively or in a  “ batch ” mode for long computa-
tional scripts. Simply typing  “ help ” at the command prompt provides assistance with 
various features. MATLAB has a rich set of constructs for plotting scientiﬁ c graphs 
from raw or computed data, suitable for inclusion in reports and other documents. 
One particular advantage of MATLAB is that it is available on several different 
computer operating system platforms. This text makes no assumption about the 
particular version which is available, and does not assume the presence of any 
toolboxes. 
 2.4  GETTING STARTED 
 When MATLAB is started, the command window will appear. MATLAB may be 
used in one of two ways: by entering interactive commands, or to process a set of 
functions in a script ﬁ le.  “ Interactive mode ” means typing commands at the prompt 
in the MATLAB window. Command script ﬁ les, or  “ m - ﬁ les, ” may be called from 
the command window prompt or from other m - ﬁ les. The convention used in this 
text is that commands that are entered into the MATLAB command window or 
placed in a script ﬁ le are shown in a frame. If entered in a script ﬁ le, the ﬁ le becomes 
the command or function name, and must have a  .m extension (ﬁ le name sufﬁ x). 
 Assistance for commands may be obtained by typing help on the command 
line, followed by the speciﬁ c keyword. For example, in using the  disp command 
to display text, typing  help disp  shows the syntax of the command, together 
with a brief explanation of its operation. Two useful commands are  clear all  
and  close all , which clear all current variables, and close all display windows, 
respectively. 
 2.5  EVERYTHING IS A MATRIX 
 MATLAB may be used from the interactive window, and does not require variables 
to be declared, as in many procedural languages. Simply typing the name of the 
variable prints its value. One very important distinction is that  all variables are 
matrices — a scalar (single value) is just the special case of a 1  ×  1 matrix. A vector 
is of course a matrix with one dimension equal to unity. By convention (as in math-
ematics), vectors are assumed to be  “ column vectors ” — that is, of dimension  N  ×  1 
( N rows in one column). To picture this, we might have:
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.6 INTERACTIVE USE 
21
 
9
3
5
4
8
3
1
2
3
3
2
3 1
⎛
⎝
⎜
⎜⎜
⎞
⎠
⎟
⎟⎟
⎛
⎝
⎜
⎜⎜
⎞
⎠
⎟
⎟⎟
×
×
A
Matrix
A
Column Vector
. 
 2.6  INTERACTIVE USE 
 Suppose we have a matrix  M deﬁ ned as
 M =
−
⎛
⎝⎜
⎞
⎠⎟
π
π
e
0
1
.  
 This matrix may be entered into MATLAB using  
M  = [  pi exp ( pi ); 0  − 1 ] 
 M  = 
 
 3.1416      23.1407 
 
 0            − 1.0000
 Note how the semicolon is used to indicate  “ the next row, ” and that MATLAB 
echoes the variable after input. A semicolon  at the end of the input line suppresses 
this echo. If we want the transpose of  M , simply type  M ’  . To transform a matrix to 
a vector  “ column - wise, ” we type  M(:) as shown below.  
M  = [  pi exp ( pi ); 0  − 1 ]; 
 M ’ 
 ans  = 
 
 3.1416 
         0 
 
 23.1407 
 − 1.0000 
 
 M(:) 
 ans  = 
 
 3.1416 
 
 0 
 
 23.1407 
 
 − 1.0000
 If we have a vector  v deﬁ ned as
 v = ⎛
⎝⎜
⎞
⎠⎟
3
4 ,  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
22 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 Now suppose we want the vector dot - product (also called the  “ inner product ” ),
 p
v v
=
⋅.  
 To evaluate this, ﬁ rst type  
v  = [3 ; 4] 
 v  = 
 
 3 
 
 4 
 M * v 
 ans  = 
 
 101.9875 
 
 − 4.0000
p  = v. * v 
 p  = 
 
 9 
 
 16
 Then entering  
 sum (p) 
 ans  = 
 
 25
 will compute the column sum of the vector. Now suppose we wish to ﬁ nd
 q
vv
=
T.  
 Note that  v is a column vector (dimension 2  ×  1), and thus  q will be a 2  ×  2 
matrix:  
 size (v) 
 ans  = 
 
 2      1 
 q  = v * v ’ 
 q  = 
 
 9     12 
 
 12     16
 we could ﬁ nd the matrix – vector product  Mv as follows:  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.7 TESTING AND LOOPING 
23
 In order to ﬁ nd
 r
v v
=
T ,  
 we would use  
 size (v) 
 ans  = 
 
 2      1 
 r  = v ’ * v 
 r  = 
 
 25
 This is of course the vector dot product, computed in a different way. 
 Matrix – vector dimensionality can be the source of much confusion when using 
MATLAB — care must be taken to ensure the mathematical correctness of what we 
are asking it to calculate:   
v 
 v  = 
 
 3 
 
 4 
 v * v 
 
 ??? Error using  = = >  * 
 
 Inner matrix dimensions must agree.
 The fact that this example produces an error may be checked as above, using 
 size (v) . Of course, a matrix may in fact not contain any elements. If this has to 
be checked, the operation  isempty (a) will determine if matrix  a is empty. 
MATLAB will print empty matrices as  [] . 
 2.7  TESTING AND LOOPING 
 The basic structure of if - then - else tests is explained below. The code may be entered 
into an m - ﬁ le by typing  edit at the command prompt, entering the text, and saving 
the ﬁ le. If the ﬁ le were saved as, say,  examp.m , then it would be invoked by typing 
 examp in the command window.  
a  = 4; 
 b  = 6; 
 if ( a  = = 4 ) 
 
 disp ( ‘ equality ’ ); 
 end 
 Note that testing for equality (the  if () test) requires the use of  two equals 
signs  “ = = . ” A single  “ = ” implies assignment, as shown for variables  a and  b . Adding 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
24 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
the following text to the above ﬁ le shows the use of two (or more) conditions. The 
ﬁ rst test is to determine whether  a is equal to 4,  and  b is  not equal to 4. The second 
test is true if  a is greater than 0,  or  b is less than or equal to 10.  
 if ( (a  = = 4)  & & (b  
 ∼  = 4) ) 
 
 disp ( ‘ AND – NOT condition ’ ); 
 end 
 
 if ( (a  >  0) || (b   < =  10) ) 
 
 disp ( ‘ OR condition ’ ); 
 end 
 
 if ( a   >  0 ) 
 
 disp ( ‘ case one ’ ); 
 else 
 
 disp ( ‘ case two ’ ); 
 end 
 In some cases, multiple conditions have to be tested. That is, a value could be 
one of several possible candidates, or it could be a none of those. We could use 
multiple if..else..if..else statements, but that becomes unwieldy for more than a few 
cases. It is better to use the  switch - case construct, which conveniently gives us 
an  “ otherwise ” clause as follows.  
a  = 2; 
 b  = 3; 
 switch (a * b) 
 
 case 4 
 
 disp ( ‘ answer is 4 ’ ); 
 
 case a * b 
 
 disp ([ num2str (a * b)  ‘ of course ’ ]); 
 
 otherwise 
 
 disp ( ‘ not found ’ ); 
 end 
 The main constructs for iteration (looping) in MATLAB are the  for and 
 while statements. Note the use of the step increment of 0.1 in the following. The 
variable  x takes on values 0, 0.1, 0.2,  .  .  . 10 on each iteration:  
 for x  = 0:0.1:10 
 
 fprintf (1,  ‘ x is %d\n ’ , x); 
 end 
 The  while loop form may be used as shown in the following example. It is 
typically used where the terminating condition for the iteration is calculated within 
the loop, rather than iterating over a ﬁ xed range of values as with the  for loop.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.8 FUNCTIONS AND VARIABLES 
25
 Care should be exercised when testing for  exact equality, and where very large 
or very small numbers might be encountered. MATLAB has the built - in variables 
 realmin and  realmax for the smallest and largest positive numbers, and  eps 
for ﬂ oating - point relative accuracy. While the largest number MATLAB can repre-
sent is quite large, and the smallest number is quite small, certain situations may 
cause problems in signal processing code, particularly where iterative calculations 
are involved. For example, the following loop  should never terminate, because it 
calculates the sequence 1, 0.5, 0.25,  .  .  . . In theory, this sequence never reaches zero. 
However, entering and running the loop below shows that the loop does indeed 
terminate eventually:  
x  = 1; 
 
 while ( x   <  10 ) 
 
 x  = x * 1.1; 
 end 
Listing 2.1  Unexpected Loop Termination
r  = 1; 
 while ( r   >  0 ) 
 
 r  = r/2; 
 
 fprintf (1,  ‘ r = %d\n ’ , r); 
 end 
 2.8  FUNCTIONS AND VARIABLES 
 As mentioned, MATLAB code is placed in  “ m - ﬁ les, ” which are just plain text ﬁ les 
with extension (sufﬁ x)  .m . It is possible to edit these ﬁ les using the MATLAB built -
 in editor (or in fact any plain - text editor). In order to be able to ﬁ nd the place (folder/
directory) where the m - ﬁ les have been placed, they must be in the current MATLAB 
directory or in the MATLAB search path. Type  pwd to see the current directory, and 
 cd to change to another directory — for example, typing  cd  c:\matlab\work will 
change the current working directory to disk drive  c: in directory  \matlab\work . 
Type  path to see the current search path. 
 The commands  clear all and  close all are often used when beginning 
a MATLAB session. They are used to clear all variables and close all ﬁ gure windows, 
respectively. 
 MATLAB will use a ﬁ le called  startup.m in the current directory when it 
starts up, to set some initial conditions. This ﬁ le is optional, but can be useful in 
larger projects. A simple  startup.m ﬁ le is shown below:  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
26 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 The comment character  % makes the remainder of the line a comment (which 
is ignored by MATLAB). The  addpath command adds a new directory path to the 
existing search path (where MATLAB searches for ﬁ les).  path may be used to 
show the current search path. 
 As with any programming script, MATLAB allows the decomposition of 
problems into smaller sub - problems via  “ functions. ” Functions normally exist in 
separate m - ﬁ les, and are given an identical ﬁ le name to the function name itself, but 
with  .m appended. Arguments  passed to the function are placed in braces after the 
function name. Arguments  returned from  the function are placed on the left - hand 
side in square brackets. 
 Suppose we wish to calculate the inﬁ nite series for  e x . This is deﬁ ned for a 
scalar value  x as
 
f x
x
x
x
x
n
n
n
N
( ) = +
+
+
+
=
=∑
1
2
3
2
3
0
!
!
!.

 
 This deﬁ nition may also be used for a matrix - valued  x . The declaration in the ﬁ le 
 eseries.m to compute this above would look like:  
 function [y, epsilon]  = eseries(x, N)
 The values  x and  N are passed to the m - ﬁ le function  eseries() . The returned 
values are  y and  epsilon . Usually, the returned values are in order of importance 
from left to right, because in some situations, some of the return arguments are not 
required. In the present example,  y is the series approximation (which depends on 
the number of iterations,  N ), and  epsilon ( ε ) is the series approximation error for 
 N terms, deﬁ ned as
 ε =
−
( )
e
f x
x
.  
 We may not always wish to use the value of  ε . The MATLAB code for the function 
is shown in Listing 2.2. Note that this function is intended for demonstration pur-
poses only: MATLAB has a built - in function  expm () for this computation. Our 
 eseries function is used in a MATLAB session as follows:  
 % add extra paths    −  user library ﬁ les 
 addpath ( ‘ c:\matlab\lib ’ ); 
 
 % start in this directory 
 cd c:\matlab\work
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.8 FUNCTIONS AND VARIABLES 
27
 Comments (starting with percent signs  % ) at the beginning of the ﬁ le up to the 
ﬁ rst blank line may be used to provide information to the user on the calculation 
performed by the function and the expected arguments. Typing  help eseries will 
present this information to the user. Thus, it should normally contain a brief descrip-
tion of the m - ﬁ le, its inputs and outputs, and any usage limitations. It ’ s usually a 
good idea to develop a template with sufﬁ cient information for future use, and to 
use it consistently. For example:    
 % eseries.m  −  Exponent of a scalar/matrix using N terms of a series 
 % function [y, epsilon]  =  eseries(x, N) 
 % 
 % Calculate the exponent of a scalar 
 % or a matrix using a series approximation 
 % e 
 ∧  x  =  1  + x  + (x 
 ∧  2)/2!  + (x 
 ∧  3)/3! 
 % 
 % Inputs: 
 %     x  =  Exponent is calculated on this number. 
 %          May be a matrix. 
 %     N  =  The number of terms to use 
 %          in the expansion. 
 % Outputs: 
 %     y  =  the resulting expansion. 
 %     epsilon  =  the error 
 %              =  true value  −  estimated value 
 % 
 % Author: 
 % Date: 
 % Revision: 
 
 function [y, epsilon]  = eseries(x, N) 
 
 % Remainder of function follows .   .   . 
x  = 2.4; 
 N  = 10; 
 [y, epsilon]  = eseries(x, N); 
 y 
 y  = 
 
 11.0210 
 epsilon 
 epsilon  = 
 
 0.0022
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
28 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 Listing 2.2  An Example MATLAB Function File. Note the  function keyword. The 
ﬁ le name must match the function name, with a  .m extension.
 % eseries.m  −  Exponent of a scalar/matrix using N terms of a series 
 % function [y, epsilon]  =  eseries(x, N) 
 % compute exponential of a matrix 
 % input: 
 %     x  =  matrix to ﬁ nd exponent of 
 %     N  =  number of terms in approximation 
 % output: 
 %     y         =  matrix result 
 %     epsilon  =  error compared to true value 
 
 function [y, epsilon]  = eseries(x, N) 
 
 % input argument checking  
 if (  nargin  
 ∼   = 2 ) 
 
 disp ( ‘ eseries: requires ’ ); 
 
 disp ( ‘ x  = value for exponentiation ’ ); 
 
 disp ( ‘ N  = number of terms to use. ’ ); 
 
 
 error ( ‘ Invalid number of input arguments. ’ ); 
 end 
 
 [m, n]  =  size (x); 
 
 % accumulates the result 
 res  =  eye (m, n); 
 
 % numerator in each term of the expansion 
 % eye function gives an identity matrix 
 num  =  eye (m, n); 
 
 % denominator in each term of the expansion 
 % this is a scalar quantity 
 den  = 1; 
 
 % ﬁ rst term is unity (or an identity matrix) 
 % and is already calculated on initialization 
 for term  = 2:N 
  
 
 % update numerator  & denominator 
 
 num  = num * x; 
 
 den  = den * (term   −  1); 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.8 FUNCTIONS AND VARIABLES 
29
 A related command is  lookfor , which is useful if you know some keywords 
but not the function name. This command looks for the speciﬁ ed keyword in the 
ﬁ rst comment line. So  lookfor exponent would return the comment line from 
the above (as well as other functions containing this keyword). 
 It ’ s also wise to perform some argument checking: the variable  nargin 
contains the number of input arguments to the function:  
 % input argument checking 
 if (  nargin  
 ∼  = 2 ) 
 
 disp ( ‘ eseries: requires:  ’ ); 
 
 disp ( ‘ x  = value for exponentiation ’ ); 
 
 disp ( ‘ N  = number of terms to use. ’ ); 
 
 error ( ‘ Invalid number of arguments. ’ ); 
 end 
 More sophisticated display of variables may be performed using the 
 fprintf() function. For example,  
 for k  = 1:20 
 
 fprintf (1,  ‘ k = %d, ans = %.2f \n ’ , k, k * pi ); 
 end 
 The built - in function  fprintf() , discussed in the next section, is used for 
writing to ﬁ les. Here, the variable 1 refers to the ﬁ le number (1 being the standard 
output, or the MATLAB command window). The format speciﬁ er  %d is used for inte-
gers, and  %.2f displays a ﬂ oating - point number with two digits after the decimal 
place. The current display line may be terminated and a new one started using the 
control character  \n within a format string. Of course, there are many other formatting 
speciﬁ cations, such as  %s for strings — type  help fprintf for more information. 
 MATLAB has a large number of built - in functions. For example, to sort an 
array of numbers, the  sort() function sorts the numeric values in ascending order, 
and optionally returns the index of the sorted values in the original array:   
x  = [4 3 8 2]; 
 [array index]  =  sort (x) 
 array  = 
 
 2  3  4  8 
 index  = 
 
 4  2  1  3
 
 % update the running sum 
 
 res  = res  + (num/den); 
 end 
 
 % returned values 
 y  = res; 
 epsilon  =  exp (x)   −  y;
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
30 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 Note that MATLAB array indexing starts at 1 and not 0. This can be a source 
of confusion, especially since many algorithms are developed using zero as a base. 
Indexes  x(1) through to  x(4) in the above are valid, whereas index  x(0) produces 
the error message  Index into matrix is negative or zero . 
 2.9  PLOTTING AND GRAPHING 
 In addition to computation, MATLAB includes extensive facilities for the display 
of data in the form of various graphical plots. Here ’ s a simple example:  
 FIGURE 2.1   MATLAB plotting example — initial version. 
0
50
100
150
200
250
−2
−1
0
1
2
x  = 0: pi /100:2 * pi ; 
 
 wave1  =  sin (x * 10); 
 wave2  =  sin (x * 10)  +  cos (x * 14); 
 
 plot (wave1); 
 hold on; 
 plot (wave2);
 close all 
 plot (x, wave1,  ‘ b − ’ , x, wave2,  ‘ b: ’ ); 
 legend ( ‘ wave 1 ’ ,  ‘ wave 2 ’ ); 
 title ( ‘ Some Example Data Waveforms ’ ); 
 xlabel ( ‘ Angle ’ ); 
 ylabel ( ‘ Amplitude ’ );
 This generates a vector of  x - axis data  x from 0 to 2 π in steps of  π /100. The 
two waveforms  wave1 and  wave2 are then plotted. The  hold on stops MATLAB 
from replacing the ﬁ rst plot with the second. At this point, our plot looks as shown 
in Figure  2.1 . 
 For two or more plots superimposed, we would want to have a legend to 
distinguish them, plus some sensible labels on the  x - and  y - axes, together with an 
overall title:  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.10 LOADING AND SAVING DATA 
31
 Note that the axis limits could also be set using  axis ([0 2 *  pi   − 3 3]); . 
The graph now looks as shown in Figure  2.2 . There are a number of other plotting 
functions that are useful, including  mesh() and  surf() for three - dimensional 
plots, and  contour() and  subplot() for multiple small plots in a group. 
 2.10  LOADING AND SAVING DATA 
 It is often necessary to load some data from a ﬁ le, to save calculated data, or just to 
save the current variables from an interactive session for later resumption. The 
simplest case is to type  save  myﬁ le , which will save the current variables to a ﬁ le 
 myﬁ le.mat . Typing  load  myﬁ le will retrieve them. However, what about data 
ﬁ les which may be supplied via some other means? 
 A distinction must be made between  binary and  text (or  “ ASCII ” ) formatted 
ﬁ les. Text ﬁ les contain plain, readable text that can be viewed with any text editor. 
A simple text ﬁ le, such as  mydata.dat containing plain text on each line, might be 
as follows:  
 FIGURE 2.2   MATLAB plotting example (continued). 
0
1.5708
3.1416
4.7124
6.2832
−3
−2
−1
0
1
2
3
Angle
Amplitude
Some example data waveforms
Wave 1
Wave 2
 set ( gca ,  ‘ xlim ’ , [0 2 * pi ]); 
 set ( gca ,  ‘ xtick ’ , [0 0.25 0.5 0.75 1] * 2 * pi ); 
 set ( gca ,  ‘ ylim ’ , [ − 3 3]);
 line one 
 line two
 This data ﬁ le may be read using  
 We would like the extent of the  x - axis to encompass the entire plot and not to 
have the legend box partially obscure the plot. The MATLAB function  gca (get 
current axes) is useful for that:   
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
32 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 Note how the value  − 1 is returned if we attempt to read past the end of the 
ﬁ le. To read a  formatted text data ﬁ le, we must open the ﬁ le, read the data, and close 
the ﬁ le. Suppose now we have a data ﬁ le  myﬁ le.dat containing numeric data:  
FileId  =  fopen ( ‘ mydata.dat ’ ,  ‘ r ’ ); 
 s  =  fgetl (FileId) 
 s  = 
 
 line one 
 
 s  =  fgetl (FileId) 
 s  = 
 
 line two 
 
 s  =  fgetl (FileId) 
 s  = 
 
 − 1 
 
 fclose (FileId);
23.4 
 45.6 
 76.8
 We can use the  fscanf () function, in conjunction with the formatting speci-
ﬁ er  %f , to interpret the text as a ﬂ oating - point number:  
FileId  =  fopen ( ‘ mydata.dat ’ ,  ‘ r ’ ); 
 
 DataPoints  =  fscanf (FileId,  ‘ %f ’ ); 
 fclose (FileId); 
 
 DataPoints 
 DataPoints  = 
 
 23.4000 
 
 45.6000 
 
 76.8000
 It ’ s always a good idea to check the value returned from  fopen () to ensure 
that the ﬁ le exists. In an m - ﬁ le, we could use:  
FileName  =  ‘ mydata.dat ’ ; 
 FileId  =  fopen (FileName,  ‘ r ’ ); 
 
 if ( FileId   <  0 ) 
 
 fprintf (1,  ‘ %s does not exist\n ’ , FileName); 
 
 error ( ‘ open ﬁ le ’ ); 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.10 LOADING AND SAVING DATA 
33
 To write (output) to text ﬁ les, we can use the  fprintf() function and open 
the ﬁ le in write ( w ) mode:  
M  =  rand (4, 2) 
 M  = 
 
 0.7012   0.0475 
 
 0.9103   0.7361 
 
 0.7622   0.3282 
 
 0.2625   0.6326 
 FileId  =  fopen ( ‘ mydata.dat ’ ,  ‘ w ’ ); 
 fprintf (FileId,  ‘ %.2f\n ’ , M); 
 fclose (FileId);
 This gives an output ﬁ le,  mydata.dat , containing only one column (one value 
per line) as follows:  
0.70 
 0.91 
 0.76 
 0.26 
 0.05 
 0.74 
 0.33 
 0.63
 It appears that MATLAB has not written the data in the  “ expected ” format. 
The reason is that MATLAB takes variables  column - wise to fulﬁ ll the output request. 
Thus the matrix M has been taken down the ﬁ rst column, then down the second. To 
get a data ﬁ le of the same  “ shape, ” it is necessary to add two format ﬁ elds, and 
transpose the matrix so that M  T is now a 2  ×  4 matrix. The construct:  
 fprintf (FileId,  ‘ %.2f %.2f\n ’ , M ’ );
 will give an output ﬁ le  mydata.dat :  
0.70  0.05 
 0.91  0.74 
 0.76  0.33 
 0.26  0.63
 In addition to text ﬁ les as discussed above, one may encounter  binary data 
ﬁ les. These are  not viewable using text editors — they consist of  “ raw ” 8 or 16 - bit 
quantities (usually), which are machine readable and not human readable. The exact 
representation depends on the central processing unit (CPU) being used, and there 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
34 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
are different conventions in common use depending on the manufacturer. Integer 
representations may be converted between formats, but the situation for ﬂ oating -
 point numbers is much more problematic. 
 For this reason, the Institution of Electrical and Electronic Engineers (IEEE) 
ﬂ oating - point format is commonly used (IEEE ﬂ oating - point numbers are discussed 
in detail in Section  3.4 ). If the need to read or write binary ﬁ les arises, the appropri-
ate source code methods will often be supplied with the data. If not, they must be 
written using a combination of  fread() and  fwrite() . 
 Consider the following, which opens a ﬁ le for reading in binary mode using 
the IEEE ﬂ oating - point format:  
FileId  =  fopen (FileName,  ‘ rb ’ ,  ‘ ieee − le ’ ); 
 if ( FileId   <  0 ) 
 
 fprintf (1,  ‘ Cannot access %s\n ’ , FileName); 
 
 error ( ‘ myfunction() ’ ); 
 end 
ID       =  fread (FileId, 2,  ‘ uchar ’ );
FileSize  =  fread (FileId, 1,  ‘ ulong ’ );
 The  ieee - le indicates the use of the IEEE ﬂ oating - point format, using  “ little -
 endian ” byte ordering. This means that the least - signiﬁ cant byte is read ﬁ rst. Note 
the format speciﬁ er   ‘ rb ’  (read - only, in binary mode) as opposed to the text ﬁ les 
discussed previously, which were opened using  ‘  r  ’  mode (text is the default). To 
read the ﬁ le, instead of the  fscanf () as before with appropriate formatting conver-
sion speciﬁ ers like  %s for string,  %d for numeric and so forth, we are restricted to 
intrinsic machine data types such as  ‘ uchar ’  for an unsigned character (8 bit), 
 ‘ ulong ’  for an unsigned long (32 bit) and so forth. To read two characters followed 
by one 32 - bit integer, we could use: 
a  = 321.456; 
 i8  = uint8 (a) 
 i8  =  
 
 255 
 i16  = uint16 (a) 
 i16  =  
 
321 
 If the signal data ﬁ le is stored in 8 - or 16 - bit binary format, then it may be 
necessary to use the above functions to read in the raw data. In that case, it is usually 
necessary to convert to ﬂ oating point format using the  double conversion function. 
In the following, note how the number is truncated when converted to  uint8 format 
(unsigned 8 - bit integer), and precision is lost when converted to  uint16 format 
(unsigned 16 - bit integer).  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.11 MULTIDIMENSIONAL ARRAYS 
35
 In a similar manner, the  fwrite ()  function allows binary output to ﬁ les, using 
the  ‘  wb  ’  format speciﬁ er for the  fopen () call. Note that there is considerable scope 
for generating code which is machine dependent — that is, code scripts which will 
work on one platform (such as Windows) and not on another (such as Unix), and 
vice - versa. This is of course undesirable, and should be avoided whenever possible. 
 Most binary ﬁ les will contain some information at the start of the ﬁ le pertain-
ing to the characteristics of the data, in addition to the data itself. This  header 
information is somewhat speciﬁ c to the type of data ﬁ le. For example, an image ﬁ le 
will require information about the width of the picture, the height of the picture, the 
number of colors present, and so forth. 
 A great many standards for data representation exist in practice. For example, 
a common format found on Windows is the  “ bitmap ” ﬁ le format with extension 
 .bmp , whereas sound (audio, or  “ wave ” ) ﬁ les often use extension  .wav . Unfortunately, 
however, there is a plethora of different ﬁ le formats, even for (ostensibly) the same 
type of data. Other ﬁ le formats in common use include JPEG for image data ( .jpg 
extension), and MP3 for audio ( .mp3 extension). 
 Examples of MATLAB built - in functions which read binary data include 
 imread() to read image data, and  wavread() to read audio (wave) ﬁ les. These 
will be discussed in Section  2.14 . 
 2.11  MULTIDIMENSIONAL ARRAYS 
 In addition to scalar quantities, vectors (one - dimensional arrays), and matrices (two -
 dimensional arrays), MATLAB allows for multidimensional arrays. The  cat 
command is used to concatenate matrices along a given dimension to form multidi-
mensional arrays. 
 Multidimensional arrays are useful in certain types of signal processing algo-
rithms. Consider a video or television picture: it is comprised of successive still 
images, with each individual image in effect comprised of a matrix of pixels. What 
is needed is a structure to hold a series of matrices. This would, in effect, require 
three - dimensional indexing. 
 The  cat() operator may be understood with reference to Figure  2.3 . The 
matrices may be joined together either by abutting them horizontally, vertically, or 
one behind the other. 
 d  =  double (i16) 
 d  =  
 
 321 
 Whos 
 Name     Size        Bytes      Class 
 A        1  ×  1      8          double array 
 D        1  ×  1      8          double array 
 i16        1  ×  1      2          uint16 array 
 i8       1  ×  1      1          uint8 array
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
36 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 Such a data structure may be useful in processing video sequences: imagine 
each frame of video as a still picture. Each frame may be stored in a matrix, with 
the sequence of matrices comprising the video sequence stored as shown in the 
 cat(3) diagram in Figure  2.3 . The video image is formed by replaying the still 
images at the appropriate rate. Some MATLAB numerical examples of manipulating 
multidimensional arrays are shown below.  
 FIGURE 2.3   The MATLAB  cat() function for multidimensional arrays. Each matrix is 
shown as a square, and the possible ways three matrices may be aligned and joined 
( “ concatenated ” ) is shown. This may be imagined as stacking the matrices vertically, 
horizontally, or depth - wise. 
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
cat(1)
cat(2)
cat(3)
a  = [0 1; 2 3]; 
 b  = [4 5 ; 6 7]; 
 c  = [8 9 ; 10 11]; 
 
 md1  =  cat (1, a, b, c) 
 md1  = 
 
 0    1 
 
 2    3 
 
 4    5 
 
 6    7 
 
 8    9 
 
 10   11 
 size (md1) 
 ans  = 
 
 6 2 
 ndims(md1) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.12 BITWISE OPERATORS 
37
 2.12  BITWISE OPERATORS 
 As well as operations on integers and ﬂ oating - point numbers, operations on one or 
more binary digits (bits) are sometimes necessary. These  “ bitwise ” operations 
involve selecting out certain bits contained within an integer, inverting certain bits 
(converting binary 1s to 0s and vice - versa), and moving the position of bits within 
an integer. Bitwise operators are used in signal processing for encoding binary 
information for communications and storage. After all, any storage or transmission 
is ultimately just a sequence of bits, grouped into bytes, characters, words, and so 
forth. MATLAB provides many functions which operate on individual bits within 
an integer, some of which are listed in Table  2.1 . Section  3.4 deals with binary 
numbers in much greater detail. 
 ans  = 
 
 2 
 
 md2  =  cat (2, a, b, c) 
 md2  = 
 
 0  1  4  5   8   9 
 
 2  3  6  7  10  11 
 size (md2) 
 ans  = 
 
 2  6 
 ndims(md2) 
 ans  = 
 
 2 
 
 md3  =  cat (3, a, b, c) 
 md3(:, :, 1)  = 
 
 0  1 
 
 2  3 
 md3(:, :, 2)  = 
 
 4  5 
 
 6  7 
 md3(:, :, 3)  = 
 
 8   9 
 
 10  11 
 size (md3) 
 ans  = 
 
 2  2  3 
 ndims(md3) 
 ans  = 
 
 3
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
38 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 The example below illustrates the use of the  bitand() function. The use of 
the other bitwise functions is similar. The format speciﬁ er  %x is used to specify 
hexadecimal (base - 16) format display.  
 TABLE 2.1  Some MATLAB Functions that Operate at the 
Bit Level 
 bitand 
 Logical AND operator 
 bitor 
 Logical OR operator 
 bitxor 
 Logical XOR operator 
 bitget 
 Return a bit ’ s value 
 bitset 
 Set a bit ’ s value 
 bitcmp 
 Complement bits 
 bitshift 
 Shift a word left or right 
a  = 83;    % 0101 0011  =  83H 
 mask  = 15;  % 0000 1111   =  0FH  
 r  =  bitand (a, mask); 
 fprintf (1,  ‘ AND(%02x, % 02x)  = %02x\n ’ , a, mask, r); 
 AND(53, 0 f)  = 03
 2.13  VECTORIZING CODE 
 Generally speaking, it is desirable to try to  vectorize MATLAB code for efﬁ ciency, 
rather than use for/end and while/end loop constructs. The term  “ vectorize ” comes 
from the implication that the operation is being performed on a sequence (or vector) 
of numbers all at once. For example, adding the constant 4 to a vector of 200 
numbers can be optimized by loading the constant 4 and the loop counter (which 
counts up to 200) into registers of the CPU. Fortunately MATLAB hides most of 
this complexity from the programmer, but nevertheless must be given some  “ hints ” 
to help it perform these types of optimizations. 
 The reason for the increased efﬁ ciency of code vectorization is due to several 
factors. Of course, a signiﬁ cant factor is the way MATLAB is internally optimized 
to deal with matrices. Other reasons include the interpretive nature of MATLAB, 
the way MATLAB allocates variables dynamically, and the presence of large amounts 
of high - speed cache memory on contemporary computer systems. Since variables 
are not explicitly declared, the interpreter does not know the memory size needed 
in advance. 
 The obvious way to encode the zeroing of a 1,000 - element array is:  
 for ArrayIndex  = 1:1000 
 
 SampleValues(ArrayIndex, 1)  = 0; 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.13 VECTORIZING CODE 
39
 It is possible to  “ vectorize ” the loop into a single operation:  
SampleValues  =  zeros (1000, 1);
 A range of values may be initialized as follows (here, it is integers 1, 2, .  .  . , 
999, 1,000):  
SampleIndex  = 1:1000;
 The related  linspace() function can be used to initialize an array of known 
size over a range of values from minimum to maximum. Even if it is not possible 
to do away with  for loops, pre - allocating memory space saves execution time. This 
is typically done using an allocation of a zero matrix like that above. A little thought 
in using the most efﬁ cient coding method can result in considerable saving in com-
putation time. Consider the following code:  
 % using loops 
 clear all 
 
 N  = 1e4; 
 tstart  =  cputime ; 
 for k  = 1:N 
 
 x(k)  = k; 
 
 y(k)  = N − k  + 1; 
 end 
 
 for k  = 1:N 
 
 z(k)  = x(k)  + y(k); 
 end 
 tstop  =  cputime ; 
 fprintf (1,  ‘ Time using loops  = %.3f\n ’ , tstop   −  tstart);
 Simply by pre - allocating the required space for vectors  x ,  y , and  z as follows, 
it is possible to speed up the execution by a great deal:  
 % preallocated using loops 
 clear all 
 N  = 1e6; 
 tstart  =  cputime ; 
 x  =  zeros (N, 1); 
 y  =  zeros (N, 1); 
 z  =  zeros (N, 1); 
 for k  = 1:N 
 
 x(k)  = k; 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
40 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 Finally, it is often possible to take advantage of the fact that MATLAB treats 
all variables as matrices. The following code performs the same computation as the 
two previous examples, although signiﬁ cantly faster:  
 
 y(k)  = N − k  + 1; 
 end 
 
 for k  = 1:N 
 
 z(k)  = x(k)  + y(k); 
 end 
 tstop  =  cputime ; 
 fprintf (1,  ‘ Using preallocation  = %.3f\n ’ , tstop   −  tstart);
 % using vectorized code 
 clear all 
 
 N  = 1e6; 
 tstart  =  cputime ; 
 k  = 1:N; 
 x  = k; 
 y  = N − k  + 1; 
 z  = x  + y; 
 tstop  =  cputime ; 
 fprintf (1,  ‘ Using vectorization  = %.3f\n ’ , tstop   −  tstart);
 2.14  USING MATLAB FOR PROCESSING SIGNALS 
 We are now in a position to use MATLAB to process some signals. Once a signal 
is sampled (in digital form), it can be manipulated using mathematical methods. To 
illustrate, suppose we generate a set of  N  random samples in MATLAB as follows:  
N  = 1,000; 
 x  =  rand (N, 1);
 Now we can plot the random vector  x and play it through the speakers at the 
sample rate of 8,000 samples per second:  
 plot (x); 
 sound (x, 8,000);
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.14 USING MATLAB FOR PROCESSING SIGNALS 
41
 Next, consider simple image manipulation in MATLAB. Create an  N  ×  M 
image of random pixels as follows. Note that an image is usually thought of as being 
 width  ×  height , whereas a matrix is speciﬁ ed as  rows  ×  columns .  
 FIGURE 2.4   A random grayscale image of size 128  ×  64 (that is, 64 rows and 128 
columns of data).  
M  = 64; 
 N  = 128; 
 
 x  =  rand (M, N); 
 x  =  ﬂ oor (x * 256); 
 
 image (x); 
 set ( gca ,  ‘ PlotBoxAspectRatio ’ , [N M 1]); 
 axis ( ‘ off ’ ); 
 box ( ‘ on ’ );
 The size of the image on screen corresponds to the size of the matrix, with 
the image pixels scaled to integer values in the range 0 to 255. The reason for this 
will be explained further in Section  3.7 . The command to set the aspect ratio ensures 
that the pixels are square, rather than distorted to satisfy the size constraints of the 
available display area. 
 To extend this example further, we will make the image into a grayscale 
one, where 0 represents black, and 255 represents white. For convenience, the 
axis markers are turned off using  axis( ‘ off ’ ) . The resulting image is shown in 
Figure  2.4 .  
 image (x); 
 colormap ( gray [256]); 
 axis ( ‘ off ’ ); 
 box ( ‘ on ’ );
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
42 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 Finally, it is time to look at some  “ real ” signals: ﬁ rst, an image data ﬁ le. In 
the following, the bitmap image ﬁ le  ImageFile.bmp is loaded using  imread() , 
and an inverted or negative image is displayed in a ﬁ gure window. The exact details 
of the various functions required will be deferred until Section  3.7 , when the nature 
of the image signal itself is discussed.  
[img cmap]  =  imread ( ‘ ImageFile.bmp ’ ); 
 img  =  double (img); 
 
 invimg  = 255   −  img; 
 image (invimg); 
 colormap (cmap); 
 
 axis ( ‘ off ’ ); 
 set ( gca ,  ‘ DataAspectRatio ’ , [1 1 1]);
 Note that the bitmap (bmp) format in the above example stores the color 
palette alongside the image. If we use the above code directly on a JPEG ﬁ le (.jpg 
extension rather than .bmp), two changes will occur. First, the image matrix returned 
will be a three - dimensional array, with the third dimension being 3. These are for 
the red, green, and blue color components. Second, even if the image is a grayscale 
one, the colormap will not usually be present. This is easily remedied if we assume 
that the image has 256 levels, and we create a synthetic colormap:  
[img cmap]  =  imread ( ‘ ImageFile.jpg ’ ); 
 img  =  double (img); 
 
 if (  isempty (cmap) ) 
 
 % for JPEG, make 256 level colormap 
 
 cmap  = [0:255; 0:255; 0:255] ’ /255; 
 end 
 For audio (sound) signals, the  wavread () function is able to load an audio 
ﬁ le — in this case, the ﬁ le  AudioFile.wav . In the following code, the sound is 
played through the speakers at the correct rate, and at a slower rate (thus altering 
the pitch). Again, the exact details of the various functions required will be deferred 
until subsequent chapters, when the nature of the audio signal itself is discussed.   
[snd fs]  =  wavread ( ‘ AudioFile.wav ’ ); 
 sound (snd, fs); 
 
 tplay  =  length (snd)/fs; 
 pause (tplay  + 1); 
 sound (snd, fs/2);
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
2.15 CHAPTER SUMMARY 
43
 2.15  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  a review of MATLAB constructs and usage. 
 •  the role of MATLAB in digital signal processing. 
 •  reading and writing data ﬁ les, audio, and image ﬁ les.  
 PROBLEMS 
 2.1.  Enter and test the example in Listing 2.1.
 (a)  Replace the loop condition with  while (r   >   eps ) . Is this a more appropri-
ate test for terminating the loop? Why? 
 (b)  What are the values for  realmin ,  realmax , and  eps in MATLAB? 
 2.2.  Generate and play back a sinusoidal signal using
 t  = 0: 1/22,050: 2; 
 y  =  sin (2 * pi * 400 * t); 
 sound (y, 22050);  
 (a)  What is the signiﬁ cance of 22,050? 
 (b)  How long will this sound be, and how many samples will there be? 
 (c)  Repeat the sound playback using a sample rate of 8,000  Hz (samples/second) 
and then at 44,100  Hz. What do you notice? Can this be explained? Repeat for 
44,100  Hz. 
 2.3.  Repeat the steps used to generate Figure  2.4 , substituting  x  =   ﬂ oor (x * 32) ; 
 (a)  What is the pixel range? 
 (b)  What is the grayscale color range? 
 (c)    Now invert the image before display, by inserting after the  rand () function 
the line  x  =  1  −  x ; Verify that that the pixels have been inverted in 
magnitude. 
 2.4.  Section  2.14 showed how to create a grayscale image, with one byte per pixel, using 
a colormap. To create a full color image, we need 3 bytes for red, green, and blue color 
components.
 (a)  Create a random array of size 20  ×  10 for the red component. Repeat for blue 
and green. 
 (b)  Combine these three separate arrays into one 20  ×  10  ×  3 array as shown in 
Section  2.11 . 
 (c)  Display this multidimensional array using the  image () function. 
 (d)  Set the pixel in the upper - left corner of the array to purely red (that is, R  =  1, G  =  0, 
B  =  0). Redisplay the image array and verify that the pixel is shown as red only. 
 (e)  Repeat for green only and red only. 
 (f)  Try differing color combinations. What do you need to create a yellow pixel? 
Black? White?  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
44 
CHAPTER 2 MATLAB FOR SIGNAL PROCESSING
 2.5.  The following outline shows how to use MATLAB to sample audio input.
 fs  = 8,000; 
 tmax  = 4; 
 nbits  = 16; 
 nchan  = 1; 
 Recorder  = audiorecorder(fs, nbits, nchan); 
 record(Recorder); 
 fprintf (1,  ‘ Recording   .  .  .  \n ’ ); 
 pause (tmax); 
 stop(Recorder); 
 % convert to ﬂ oating - point vector and play back 
 yi  = getaudiodata(Recorder,  ‘ int16 ’ ); 
 y  =  double (yi); 
 y  = y/ max ( abs [y]); 
 plot (y); 
 sound (y, fs);  
 (a)  Sample your own voice, using the MATLAB code above. 
 (b)  Determine the size of the sample vector. Does this correspond to the sample 
rate you selected when saving the ﬁ le? 
 (c)  Plot the entire waveform (as above), and portions of it using vector subscripting. 
Note how the 16 - bit audio samples are converted to ﬂ oating - point values for 
calculation.  
 2.6.  A certain DSP problem requires a buffer of 32,768 samples. Explain why the following 
initialization is wrong:  x   =   zeros (32768) ; 
 2.7.  What does the  any () command do? Which one of these loop constructs is probably 
what was intended? Why?
 while ( any [h   >  0]) 
 h  = h   −  2 * pi ; 
 end 
 while ( any (h)   >  0) 
 h  = h   −  2 * pi ; 
 end 
 2.8.  We will later use the  “ sinc ” function, deﬁ ned as sin ( π t / T )/( π t / T ), where  t is a vector 
of time instants and  T is a constant. Explain why the following is incorrect, and how 
it may be corrected. What is the purpose of the  eps value?
 T  = 0.1; 
 t  =  − 0.5: 0.001: 0.5; 
 x  =  pi * t/T; 
 y  =  sin (x  +  eps )/(x  +  eps ); 
 plot (t, y);  
 2.9.  Enter the example multidimensional arrays as shown in the code example in Section 
 2.11 . By writing out the matrices and their elements, verify that the  cat () command 
operates as you expect. 
 
 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER  3 
SAMPLED SIGNALS AND 
DIGITAL PROCESSING 
 3.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to
 1.  explain the concepts of  sampling and  reconstruction ; 
 2.  convert decimal numbers into  unsigned binary , 2 ’ s complement  signed or 
 ﬂ oating - point formats, and convert binary numbers back to decimal; 
 3.  explain  quantization and signal - to - noise ratio (SNR); 
 4.  be able to use  difference equations and manipulate them for a given task; 
 5.  be able to iterate difference equations and turn this into code for a digital signal 
processing ( DSP )  implementation ; and 
 6.  explain ideal  reconstruction and derive the polynomial  interpolation for a 
signal. 
 3.2  INTRODUCTION 
 Signals invariably come from the real world. To process signals via computer, it is 
necessary to have some systematic means to acquire the signal so as to capture all 
necessary information about the signal. This signal acquisition must take into account 
not only the physical characteristics of the signal but also the necessity of using a 
numerical representation of the signal at discrete instants in time. 
 3.3  PROCESSING SIGNALS USING 
COMPUTER ALGORITHMS 
 Most signals are analog in nature. Sound, for example, is a pressure variation over 
time. Images are composed of various light wavelengths at various intensities, over 
a two - dimensional plane. Many other signal types exist, such as radio frequency 
waves, bioelectric potentials, and so forth. The key element is to ﬁ rst convert the 
45
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
46 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
signal to an electrical voltage followed by discrete sampling to convert the signal 
into a sequence of numbers over time. Once the signal has been captured in a digital 
form, it opens the door to a wide variety of signal processing techniques. 
 As shown in Figure  3.1 , the conversion from an analog form into a digital 
form is accomplished using an analog - to - digital (A/D) converter (also called an 
ADC). This is followed by some form of algorithm (or several algorithms used in 
conjunction) to process the discrete sequence of numbers that represent the signal. 
At this point, we may be interested in some information about the signal — for 
example, if the signal were a heart rhythm, we may be interested in detecting the 
onset of an abnormal pattern and sounding an alarm. 
 In other situations, the output may be fed to the converse of the A/D converter, 
the digital - to - analog (D/A) converter (also called a DAC), in order to produce an 
output signal. The output may be, for example, music that has been ﬁ ltered to 
enhance certain frequencies or an image which has had noise removed. In commu-
nications systems, the output may be a sequence of binary digits representing the 
samples, termed a  “ bitstream. ” This is then suitable for transmission or storage — for 
example, encoded audio on a CD or DVD, or perhaps audio streamed or downloaded 
over the Internet. 
 In Figure  3.1 , the output is represented as dots at the discrete sampling 
instants. The A/D conversion and subsequent computer processing takes a certain 
ﬁ nite amount of time. In theory, the faster the processing, the better the represen-
tation of the signal. However, this has several implications, particularly in terms 
of the cost of equipment (faster equipment is invariably more expensive). In 
addition, the amount of information processed need only be enough to faithfully 
represent the signal for the user — this consideration is particularly important in 
digital communications and storage. Naturally, the more data that are required to 
represent a signal, the longer it takes to transmit and the less that can be stored on 
a given medium. 
 There are several implications of discrete - time sampling followed by digital 
processing. The questions which immediately arise are 
 1.  What is an appropriate sampling rate, and does it depend on the signal being 
sampled? 
 FIGURE 3.1   Simpliﬁ ed view of the process of discrete - time sampling. A/D represents 
the analog - to - digital conversion, and D/A is the digital - to - analog conversion. The purpose 
of sampling and analyzing the signal may be either to ﬁ nd some information about the 
signal or to output the signal in some altered or enhanced form. 
Input signal
Information
about signal
Processing
algorithm
A/D
D/A
Output signal
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.4 DIGITAL REPRESENTATION OF NUMBERS 
47
 2.  Since digital computers use the binary system for representing numbers, what 
is a suitable number of bits to use for each sample? And does that also depend 
on the signal being sampled?  
 Although the exact requirements will take some time to spell out more precisely, 
some  “ order - of - magnitude ” ﬁ gures may be useful at this point in time:
 Digital audio   uses sampling rates of the order of 40  kHz, at 16  bits per 
sample (and of course two channels, left and right, if stereo reproduction 
is required). For lower quality (such as telephone speech), an 8 - kHz sample 
rate is often employed.  
 Digital photographs and computer images   require the order of 8  bits per 
sample for each primary color (red, green, and blue). An image may be of 
the order of 2,000  ×  1,000 samples (width  ×  height). 
 3.4  DIGITAL REPRESENTATION OF NUMBERS 
 Since we are dealing with representing signals using numbers, it is a worthwhile aim 
to have a working knowledge of how numbers are represented in the binary number 
system. This helps us in two regards. First, when an analog signal is converted into 
a digital representation (or vice versa), we need to be conversant with the internal 
number format; second, when we implement equations to manipulate those sampled 
signals, we need to perform arithmetic operations. The latter can be problematic and 
may be the source of unexpected or erroneous results. We need to be conversant 
with issues such as rounding and truncation of numerical results, as well as with less 
obvious issues with precision. Finally, calculation time is often an important issue 
in real - time DSP, and some appreciation of the computational complexity of the 
various arithmetic operations helps improve our designs. In the following sections, 
the intention is not to give an exhaustive treatment since there is ample material in 
this area to ﬁ ll entire textbooks. Rather, the salient features are addressed as they 
apply to DSP, as a starting point for further investigation if warranted. 
 3.4.1  Representing Integers 
 The basic representational unit in the binary system is the  bit or the binary digit. 
This is a base 2 number, having values of 0 or 1 only. Combining several bits into 
higher groupings allows us to represent a range of numerical values. Typical group-
ings used are a nibble (4  bits), a byte (8  bits), and a word (16, 32, or 64  bits, depending 
on the context). The place value system follows the same rules as the decimal 
system, except that the base is 2 rather than 10. The following shows an example 
of an 8 - bit unsigned number: 
 2  n  
 2 7 
 2 6 
 2 5 
 2 4 
 2 3 
 2 2 
 2 1 
 2 0 
 
 Decimal 
 128 
 64 
 32 
 16 
  8 
 4 
 2 
 1 
 
 Example 
 0 
 0 
 1 
 0 
  1 
 0 
 0 
 1 
 (1  +  8  +  32  =  41) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
48 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 This is an 8 - bit quantity, so we have  N  =  8  bits. The 2 0 bit position is usually 
called bit 0 or the least signiﬁ cant bit (LSB), and the 2  N  – 1 bit position is called the 
most signiﬁ cant bit (MSB). Mathematically, the bits  a i sum together to form a posi-
tive integer,  A , according to
 
 A
a
A
a
i
i
i
N
i
=
≥
=
=
−
∑2
0
0
1
0
1
for
and
or .  
 (3.1) 
 An  N - bit number with all 1 ’ s is the equivalent of 2  N  −  1. Thus, an 8 - bit number 
would have  N  =  8, and the binary value 1111  1111 is 255 in decimal 
(1  +  2  +  · · ·  +  64  +  128  =  255). This is equivalent to 256  −  1  =  255 in this case, or 
2 N  −  1 in the general case for any  N . 
 3.4.2  Signed Integers 
 Signed integers are usually (though not always) represented by so - called 2 ’ s comple-
ment notation. In this approach, the MSB represents the sign of the number: 0 for 
positive, 1 for negative. For positive numbers, we take the same approach as with 
unsigned numbers to determine the value. For negative numbers (MSB  =  1), we 
must ﬁ rst invert all binary values (convert 1 to 0 and 0 to 1) and add one. As a simple 
example, consider the 8 - bit number
 11111110.  
 This is a negative number, so we must follow the 2 ’ s complement procedure. 
Inverting all bits, we obtain
 0000 0001. 
 Then, adding one, we have
 0000 0010. 
 This is  + 2 in decimal, so the original (negative) number was  − 2. 
 If we were to interpret 1111 1110 as an unsigned number, it would be 254. So 
the 2 ’ s complement notation is equivalent to 2  N  −  A , where  A is the unsigned repre-
sentation. In this case, 2 8  −  2  =  256  −  2  =  254. 
 Whether a number is signed or unsigned is a matter of interpretation in the 
context in which it is used. Table  3.1 shows the signed and unsigned equivalent 
values of a 4 - bit binary number. 
 The key advantage of 2 ’ s complement representation is that arithmetic opera-
tions perform as expected. For example,  − 2  +  5  =  3,  − 4  +  4  =  0, and so forth. Note 
that in Table  3.1 , the range of unsigned numbers is 0  →  (2 N  −  1) and that for signed 
numbers is  − 2 N  − 1  →  + (2  N  − 1  −  1). For positive and negative numbers, the 2 ’ s comple-
ment notation may be uniﬁ ed to become
 
 A
a
a
N
N
i
i
i
N
= −
+
−
−
=
−
∑
2
2
1
1
0
2
.  
 (3.2) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.4 DIGITAL REPRESENTATION OF NUMBERS 
49
 This is valid for both positive (sign bit  a N  − 1  =  0) and negative ( a N  − 1  =  1) numbers. 
 MATLAB has data types for commonly used standard integer representations. 
These are given mnemonics such as uint8 for unsigned 8 - bit integers, int8 for 
signed 8 - bit integers, and so forth. The following example shows how to determine 
the maximum and minimum ranges for some integer types — these should be studied 
in conjunction with Table  3.1 . Note also how the logarithm to base 2 is used to 
determine the number of bits employed. 
  
 TABLE 3.1   Signed and Unsigned Interpretation of a 4 - Bit Quantity 
 Binary 
 Unsigned 
 Signed 
 Binary 
 Unsigned 
 Signed 
 0000 
 0 
 0 
 1000 
 8 
 − 8 
 0001 
 1 
 1 
 1001 
 9 
 − 7 
 0010 
 2 
 2 
 1010 
 10 
 − 6 
 0011 
 3 
 3 
 1011 
 11 
 − 5 
 0100 
 4 
 4 
 1100 
 12 
 − 4 
 0101 
 5 
 5 
 1101 
 13 
 − 3 
 0110 
 6 
 6 
 1110 
 14 
 − 2 
 0111 
 7 
 7 
 1111 
 15 
 − 1 
 intmin( ′ uint8 ′ ) 
 ans  = 
      0 
 intmax( ′ uint8 ′ ) 
 ans  = 
      255 
 intmax( ′ int8 ′ ) 
 ans  = 
      127 
 intmin( ′ int8 ′ ) 
 ans  = 
      − 128 
 log2 ( double (intmax   +  1)) 
 ans  = 
      31 
 intmax( ′ int16 ′ ) 
 ans  = 
      32767 
 intmin( ′ int16 ′ ) 
 ans  = 
      − 32768 
 intmax( ′ int32 ′ ) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
50 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 3.4.3  Binary Arithmetic 
 We now turn to the basic arithmetic of binary numbers. The basic rules are identical 
to decimal arithmetic with regard to place value and carry from one place to the 
next. So, for adding pairs of bits, we have
 0
0
0
+
=
 
 0
1
1
+ =  
 1
0
1
+
=  
 1 1
10 0
1
2
+ =
(
,
).
with
carry decimal
 
 It will be helpful in the following examples to also note that
 1 1 1
11 1
1
3
+ + =
(
,
).
with carry
decimal
 
 Consider the binary addition of 8 - bit numbers 13  +  22, as follows. We add bitwise 
as shown next, noting the carry of 1 ’ s. The decimal values of each binary number 
are shown in brackets for checking:
 
0 0 0 0
1 1 0 1
1
4
8
13
0 0 0 1
0 11 0
2
4
16
22
0 0 1 0
0 01 1
1
2
32
35
1
1
1
+
+
=
(
)
+
+
+
=
(
)
+
+
=
(
).
 
 Next, consider multiplication of binary numbers. For the speciﬁ c case of multiplica-
tion by 2, a shift - left by one bit position is all that is required. For multiplication by 
4, a shift of two bit positions is required, and so forth. Similarly, for division by two, 
a shift - right by one bit position gives the correct result (assuming we ignore any 
overﬂ ow bits and do not require fractions). 
 Generally speaking, multiplication algorithms are implemented in DSP hard-
ware and are not usually required to be coded in software. However, understanding 
the underlying algorithms can yield signiﬁ cant insight, especially into performance -
 related issues. To understand the procedure, it helps to revisit how we approach 
multiplication in decimal arithmetic. 
 For example, if we had to calculate 234  ×  23, it would be equivalent to adding 
the value 234 to itself in a loop of 23 iterations or, equivalently, adding 23 to itself 
over 234 iterations. Such an approach would be quite slow to execute, depending 
as it does on the magnitude of the numbers, and so highly undesirable. But using 
the concept of place value, we can employ the elementary multiplication algorithm 
as follows:
 ans  = 
      2147483647 
 intmax( ′ int64 ′ ) 
 ans  = 
      9223372036854775807  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.4 DIGITAL REPRESENTATION OF NUMBERS 
51
 
1
1
1
2
3
4
2
3
7
0
2
4
6
8
5
3
8
2
×
+
⋅
.
 
 Here, we multiply 3 by 234 and add it to the shifted product of 2 and 234. The 
multiplications are by one digit, and the shifting is in effect multiplication by 10 
(the base). A little thought shows that the time taken to perform this operation is 
independent of the magnitude of the values. It is dependent on the number of ﬁ gures 
in each value, but this is essentially constant in any given problem, although we may 
prepend leading zeros (thus, for calculation to four digits, 234 becomes 0234, and 
23 becomes 0023). 
 In the binary system, we can follow a similar procedure. Consider the follow-
ing example of the binary multiplication of 13  ×  6:
 
1 1 0 1
1
4
8
13
0 1 1 0
2
4
6
0 0 0 0
1 1 0 1
1 1 0 1
0 0 0 0
1 0 0 1 11 0
2
×
+
+
=
+
=
⋅
⋅⋅
⋅⋅⋅
+
(
)
(
)
(
4
8
64
78
+
+
=
).
 
 In fact, it is somewhat simpler than decimal multiplication in that the multiplication 
by one digit at each step is actually multiplication by zero (with a zero result always) 
or one (which is the original number repeated). 
 3.4.4  Representing Real Numbers 
 Consider the following hypothetical example, in which we take the square root of 
a number  N times and then square the result  N times iteratively. 
  
 x  = 11; 
 N  = 50; 
 for k  = 1:N 
      x  =  sqrt (x); 
      fprintf (1,  ′ %d %.20 f\n ′ , k, x); 
 end 
 for k  = 1:N 
      x  = x ˆ 2; 
      fprintf (1,  ′ %d %.20 f\n ′ , k, x); 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
52 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 One would expect that the result ought to always be what we started with — in 
this case, 11. But running the above for a reasonable number of iterations (say, 50) 
yields the following: 
 1 
 3.31662479035539980000 
 2 
 1.82116028683787180000 
 3 
 1.34950371871954160000 
 . . . 
 48 
 1.00000000000000840000 
 49 
 1.00000000000000420000 
 50 
 1.00000000000000200000 
 
 1 
 1.00000000000000400000 
 2 
 1.00000000000000800000 
 3 
 1.00000000000001600000 
 . . . 
 48 
 1.75505464632777850000 
 49 
 3.08021681159672370000 
 50 
 9.48773560644308670000 
 So we do not in fact end up where we started. What we have ended up with is 
a result of numerical (im)precision. If we increased  N above, eventually we would 
have a result of 1 exactly after the ﬁ rst loop, and it would not matter how many itera-
tions the second loop performs; the end result would still be 1. This may seem like 
a contrived case, but it illustrates the fact that numerical precision cannot be taken 
for granted. DSP algorithms often run continually, essentially indeﬁ nitely, and any 
rounding errors may well accumulate for long running times and/or large data sets. 
 So that brings us to the question of how to represent fractions. One method 
using an implied or ﬁ xed binary point is shown next:
 
2
2
2
2
2
2
2
2
2
8
4
2
1
0
0
1
0
1
0
3
2
1
0
1
2
3
4
1
2
1
4
1
8
1
16
n
⋅
⋅
−
−
−
−
Decimal
Example:
0
1
2
2 5625
1
2
1
16
+
+
=
(
)
.
.
 
 This  ﬁ xed - point format satisfactorily represents a range of numbers but has some 
shortcomings. For one, the maximum value is limited by the number of bits to the 
left of the binary point. Similarly, the minimum fraction is limited by the number 
of bits allocated to the right (here, the minimum step size is  1
16 ). A little thought also 
reveals that not all numbers can be represented exactly, and we must be satisﬁ ed 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.4 DIGITAL REPRESENTATION OF NUMBERS 
53
with the nearest approximation. For example, 0.21 decimal could not be represented 
exactly. The closest we could come is 0000 · 0011 to give 0.1875 decimal. 
 What is needed is a more general system for representing very large and very 
small numbers as accurately as possible. Floating - point numbers give us this 
ﬂ exibility. 
 3.4.5  Floating - Point Numbers 
 The ﬁ nal extension in the representation of numbers is the  ﬂ oating - point format. In 
ﬁ xed - point binary, the binary point is ﬁ xed and cannot move. In ﬂ oating - point rep-
resentations, the binary point  can move, giving extra range and precision than if we 
used the same number of bits in a ﬁ xed - point representation. There have been many 
ﬂ oating - point schemes proposed and implemented over the years, and here we cover 
the basic concepts using the Institution of Electrical and Electronic Engineers (IEEE) 
754 standard for ﬂ oating - point numbers. 1 
 The fundamental change with ﬂ oating - point numbers is that the binary point 
can move, and this ﬁ ts naturally with a normalized representation. Normalized 
numbers are where we have one nonzero digit to the left of the decimal point. For 
example, the following are normalized decimal numbers:
 2 153
2 153 103
,
.
=
×
 
 0 0167
1 67 10 2
.
.
.
=
×
− 
 So in general, we have
 
 Number →±
×
±
sF
B E, 
 (3.3) 
where
 s    =  sign, 0 or 1 (representing  + or  − ); 
  B    =  base, implied 2 for binary (base 10 for decimal); 
  F    =  fractional part; and 
  E    =  exponent (power of the base). 
 The fractional part  F is also called the  “ signiﬁ cand ” or  “ mantissa. ” The exponent  E 
could be negative to represent small numbers. In binary, we have
 
 Number →±
×
±
1
2
.
.
ffff
E
…
 
 (3.4) 
 The  f  ’ s represent bits (0 or 1), and there is an implied 1 to the left of the signiﬁ cand. 
In normalizing decimal numbers, this position would be a zero. But because we are 
using the binary system, this position could only be a 1 or a 0, and so we may as 
well make it a 1 to gain an extra bit ’ s worth in our representation. The IEEE754 
standard speciﬁ es a total of 32  bits for a single - precision quantity, as well as 64 - bit 
extended precision and 80 - bit double precision (4, 8, and 10  bytes, respectively). In 
the C language, these are called ﬂ oat, double, and long double, which may give rise 
  1  http://grouper.ieee.org/groups/754/ . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
54 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
to confusion in the terminology. On general - purpose CPUs, ﬂ oating - point registers 
are usually 10  bytes, so there is no speed advantage in using lesser precision (and it 
may actually be slower due to the conversion required). This is summarized below 
for convenience. 
 C language data type 
 IEEE754 name 
 Size (bits) 
 Size (bytes) 
 Float 
 Single precision 
 32 
 4 
 Double 
 Double precision 
 64 
 8 
 Long double 
 Double extended 
 80 
 10 
 We now describe in more detail the 32 - bit single - precision format of IEEE754. 
The same principles apply to 64 - and 80 - bit ﬂ oating - point data types, but the 32 - bit 
format is convenient to deal with for the purposes of explanation. 
 The bit layout is shown in Figure  3.2 . In this layout, we have a single sign bit 
(1  =  + , 0  =  − ), an 8 - bit exponent, and a 23 - bit fraction. The exponent is stored as a 
biased number, and so it is effectively the actual exponent plus 127. Thus, a binary 
range of 0 – 255 is actually an exponent range of  − 127 to  + 128. The 23 - bit fraction 
part has an implied 1 before the binary point. Below are some examples to illustrate 
the storage format and the conversion process. 
 Binary to ﬂ oating point is accomplished as follows. The binary representation 
is known, and we wish to determine the corresponding ﬂ oating - point value. 
 Start with a given single - precision binary value:
 01000010011000101000000000000000.  
 Referring to Figure  3.2 , this is stored as
 
Sign
Exponent
Fraction
0
10000100
11000101000000000000000
132
1
+
.76953125
 
 This is derived as follows. The sign bit is 0; hence, the number is positive. The 
exponent is 10000100 binary or 132 decimal. So, the actual multiplier is 
2 132 – 127  =  2 5  =  32. 
 The fractional part is assembled by taking the bits of the fraction and multiply-
ing by the place value:
 FIGURE 3.2   Floating - point format for a single - precision 32 - bit format. The 64 - and 
80 - bit formats follow a similar pattern, with larger exponent and fraction ﬁ elds, to give 
both a larger range of values able to be represented, as well as higher precision. 
1
8
23
±
1
S
. f f f · · ·
Exponent
biased by 127
   Fraction
implied            
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.4 DIGITAL REPRESENTATION OF NUMBERS 
55
 
1
2
1
2
0
2
0
2
0
2
1
2
0
2
1
2
0
2
1
2
1
2
3
4
5
6
7
8
9
×
+ ×
+
×
+
×
+
×
+
×
+
×
+ ×
+
×
+
=
−
−
−
−
−
−
−
−
−

+
+
+
=
1
4
1
64
1
256
0 7695312500
.
.
 
 The number to the left of the decimal point is 1 (a 1  bit is implied), so that the 
number is
 1 7695312500 2
56 6250
132 127
.
.
.
×
=
−
(
)
 
 Floating point to binary is accomplished as follows. We are given a ﬂ oating - point 
value of 56.625 and wish to determine the corresponding binary value. The binary 
value is derived as follows. First, 2 5  =  32 is the highest 2  n integer below 56, so 
the exponent is 5. This is stored as 127  +  5  =  132. The fraction is then 
 56 625 2
1 76953125
5
.
.
.
=
 We remove the implied 1 at the start, so the value stored 
is 0.76953125. We now need to reduce this fraction to binary, using an iterative 
procedure:
 
0 76953125
2
15390625
1
0 5390625
2
10781250
1
0 0781250
2
.
.
.
.
.
×
=
→
×
=
→
×
=
→
×
=
→
×
=
→
0 156250
0
0 156250
2
0 312500
0
0 312500
2
0 6250
0
0 6250
.
.
.
.
.
.
×
=
→
×
=
→
×
=
→
×
=
→
×
=
→
2
1250
1
0 250
2
0 50
0
0 50
2
10
1
0 0
2
0 0
0
0 0
2
0 0
.
.
.
.
.
.
.
.
.
0






 
 At each stage, we multiply by 2 and take the value to the left of the decimal point. 
This can only be 1 or 0. We take the fractional remainder and repeat on the next 
line. Finally, the binary numbers are read down the right - hand column. The resulting 
32 - bit value is formed as follows:
 
Sign
Exponent
Fraction
0
10000100
11000101000000000000000
132
1
+
.76953125
 
 Finally, there are some special values deﬁ ned by the standard worth mentioning. 
Zero is stored as all zeros, except for the sign, which can be 1 or 0. Plus/minus 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
56 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
inﬁ nity (as might result from, say, 1/0) is stored as 1 ’ s in the exponent, with the 
signiﬁ cand all zeros. Non - a - number (NaN) is an invalid ﬂ oating - point result, which 
sometimes arises (e.g., 0/0). It is stored as all ones in the exponent with a signiﬁ cand 
that is not all zeros. 
 In the light of the above explanations, it is instructive to see how MATLAB 
reports the results of some ﬂ oating - point constants. In the following,  single is single -
 precision 32 - bit ﬂ oating point;  double is 64 - bit format, and  eps  is the smallest incre-
ment deﬁ ned for the given data type: 
  
 log2 ( realmax ( ′ single ′ )) 
 ans  = 
      128 
 log2 ( eps ( ′ single ′ )) 
 ans  = 
      − 23 
 log2 ( realmax ( ′ double ′ )) 
 ans  = 
      1024 
 3.4.6  The Coordinate Rotation for Digital 
Computers ( CORDIC ) Algorithm 
 We ﬁ nish this section with a discussion on the more advanced topic of how to cal-
culate trigonometric and other functions. This can be accomplished using a power 
series iteration (as will be seen in subsequent chapters); however, one particular 
algorithm of note has found widespread application. The CORDIC algorithm is an 
ingenious method of computing various arithmetic functions using only straightfor-
ward binary operations with some precomputed values. 
 The basis of the CORDIC algorithm is the anticlockwise rotation of a point, 
 P , through an angle,  θ , as shown in Figure  3.3 . Of course, a clockwise rotation is 
easily accomplished by rotating through  − θ . 
 A standard derivation yields the rotation matrix for computing the new point 
from the old:
 
 x
y
x
y
new
new
⎛
⎝⎜
⎞
⎠⎟=
−
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
cos
sin
sin
cos
.
θ
θ
θ
θ
 
 (3.5) 
 So in other words, a rotation of  θ yields
 x
x
y
new =
−
cos
sin
θ
θ  
 
 y
x
y
new =
+
sin
cos .
θ
θ  
 (3.6) 
 The fundamental insight of the CORDIC family of algorithms is that the tan ( · ) 
function can be expressed as a power 2  −  i , where  i is an integer. Furthermore, since 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.4 DIGITAL REPRESENTATION OF NUMBERS 
57
multiplication by a negative power of 2 (i.e., division) is a shift - right of the binary 
number, the operations can be accomplished using a relatively straightforward digital 
logic. 
 We ﬁ rst rearrange the matrix form into an iterative form, where smaller 
rotations of  θ i are used to successively rotate the initial vector to the ﬁ nal vector, 
through an angle,  θ . The iterations are denoted by  i , which corresponds to the 
 i th bit:
 
 x
y
x
y
i
i
i
i
i
i
i
i
+
+
⎛
⎝⎜
⎞
⎠⎟=
−
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
1
1
cos
sin
sin
cos
.
θ
θ
θ
θ
 
 (3.7) 
 We now factor out the cos ( · ) function from the matrix:
 
 x
y
x
y
i
i
i
i
i
i
i
+
+
⎛
⎝⎜
⎞
⎠⎟=
−
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
1
1
1
1
cos
tan
tan
.
θ
θ
θ
 
 (3.8) 
 To achieve the aim of a straightforward binary implementation, we let each tan  θ i 
value be deﬁ ned as a power of 2:
 
 tan
tan
.
θ
θ
i
i
i
i
=
∴
=
−
−
−
2
2
1
 
 (3.9) 
 This has the aforementioned effect of allowing the multiplication to be performed 
by a binary shift. We apply the rotation successively, starting at angle  θ and initial 
points ( x ,  y ). The successive rotations by angle  θ i mean that the multiplicative 
constant over  N bits becomes the product of all the cos  θ i terms:
 FIGURE 3.3   Rotation of a point,  P , through an angle,  θ , to point  P ′ . 
P
P′
x
y
θ
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
58 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 
 K
i
i
N
=
=
−
∏cosθ
0
1
 
 (3.10) 
 
 =
(
)
−
−
=
−
∏cos tan
.
1
0
1
2 i
i
N
 
 (3.11) 
 For example, letting  N  =  8  bits,  K  =  0.60725911229889. Since this constant is inde-
pendent of the rotations, it can be precomputed and stored. The iteration itself 
becomes a process of forcing  θ to zero and of selecting a direction of rotation at 
each iteration. We select the rotation according to whether  θ is positive or negative, 
as shown in Equation  3.12 :
 
 
θ
θ
δ
δ
δ
δ
δθ
θ
δθ
θ
<
>
=
= −
= −
=
=
= −
−
−
−
−
0
0
2
2
2
2
x
y
x
y
y
x
y
x
i
i
i
i
i
i.
 
 (3.12) 
 These come directly from the rotation matrix using tan ( · ) functions. Each step is 
now a simple addition (or subtraction), with the the 2  −  i x and 2  −  i y terms easily 
computed, since they are binary shifts. At each step, the  x ,  y , and  θ values are updated 
according to their respective increments:
 
x
x
x
y
y
y
+
→
+
→
+
→
δ
δ
θ
δθ
θ.
 
 At the end of the iterations, the multiplication by  K performs the necessary scaling. 
This constant can be precomputed to any desired precision and stored in memory. 
 Listing 3.1 shows a straightforward implementation of the above algorithm. 
Note that we have used ﬂ oating - point calculations throughout, but of course the 
multiplication by 2  −  i requires only a shift operation if we use binary registers.  
 Figure  3.4 illustrates the rotation for a certain starting point and angle. Figure 
 3.5 shows the value of  θ over each iteration as it is forced to zero. 
 Since the factor  K was taken out of the iteration, we must multiply by it back 
in at the end. For the case illustrated in Figure  3.4 with  N  =  8  bits, we initialize the 
iterations with  x  =  1,  y  =  0, and  θ  =  π /3. Note that  N  =  8 does not yield a very high 
level of accuracy but is useful for the purposes of example to show the errors incurred. 
 Equation  3.6 yields  cos π
3 and  sin π
3, and we have the following results: 
 Parameter 
 Value 
 N 
 8 
 K 
 0.60725911229889 
 cos π
3 
 0.50000000000000 
 K x 
 0.50399802184864 
 sin π
3 
 0.86602540378444 
 K y 
 0.86370480719553 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.4 DIGITAL REPRESENTATION OF NUMBERS 
59
 Listing 3.1  CORDIC Algorithm Demonstration 
 x  = 1; 
 y  = 0; 
 theta  =  pi /3; 
 nbits  = 8; 
 %  determine K  =  cos(atan()) constant 
 K  = 1; 
 for i  = 0:nbits − 1 
      thetai  =  atan ( 2 ˆ ( − i) ); 
      K  = K * cos (thetai); 
 end 
 xtarget  = (x * cos (theta)   −  y * sin (theta))/K; 
 ytarget  = (x * sin (theta)  + y * cos (theta))/K; 
  for i  = 0:nbits − 1 
      %  atan  (theta) ——  since it is constant for each step, 
      %  it would be precomputed in practice and stored in a 
      % table 
      p  = 2 ˆ ( − i); 
      thetai  =  atan (p); 
      if ( theta   <  0.0 ) 
          dx  = y * p; 
          dy  =  − x * p; 
          dtheta  = thetai; 
      else 
          dx  =  − y * p; 
          dy  = x * p; 
          dtheta  =  − thetai; 
      end 
      x  = x  + dx; 
      y  = y  + dy; 
      theta  = theta  + dtheta; 
 end 
 Lastly, note that CORDIC is not one algorithm but a family of algorithms for 
computing various trigonometrical and other functions. The above is only an intro-
duction to the topic in order to give sufﬁ cient basis for understanding the fundamen-
tal principles. 
 3.4.7  Other Issues 
 Complex numbers are often required in signal processing algorithm implementa-
tions. These may be stored as real numbers in rectangular form  x
y
+
(
)
j
 or polar 
form ( rejθ). When assessing computational complexity, it is important to remember 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
60 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
that multiplication of two complex numbers effectively requires several real - number 
multiplication and addition/subtraction operations as is easily seen by expanding the 
rectangular form:
 
 a
b
c
d
ac
bd
ad
bc
+
(
)
+
(
) =
−
(
) +
+
(
)
j
j
j
.  
 (3.13) 
 In a practical sense, it is also important to remember to initialize all variables at the 
start of any iterative algorithms. As will be seen, we normally assume zero initial 
 FIGURE 3.4   Convergence of the CORDIC algorithm over an 8 - bit range. The box (  ) 
shows the starting point (in this case,  x  =  1,  y  =  0). The asterisk ( * ) shows the target, which 
in this case is (( x cos  θ  −  y sin  θ )/ K , ( x sin  θ  +  y cos  θ )/ K ), with angle  θ  =  π /3. The circle 
and vector show the current point at each iteration. 
0
1
2
0
1
2
Bit 0
0
1
2
0
1
2
Bit 1
0
1
2
0
1
2
Bit 2
0
1
2
0
1
2
Bit 3
0
1
2
0
1
2
Bit 4
0
1
2
0
1
2
Bit 5
0
1
2
0
1
2
Bit 6
Convergence to final point in CORDIC
0
1
2
0
1
2
Bit 7
 FIGURE 3.5   The convergence of the CORDIC algorithm. At each step, the angle  θ is 
forced toward zero. Each angular update step corresponds to an angle of 2 −  i , where  i is the 
bit position starting at zero.  
0
1
2
3
4
5
6
7
–0.5
0
0.5
Bit number
Error θ
Error convergence in CORDIC
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.5 SAMPLING 
61
conditions, meaning we assume all past sample values are zero at the start of an 
iteration. Since memory may be initialized to random values, it follows that the 
integer or ﬂ oating - point values we see, if not initialized, may start at random or 
unpredictable values. This can lead to puzzling and sometimes catastrophic behavior, 
for example, if a value happened to be initialized to the ﬂ oating - point equivalent of 
inﬁ nity. 
 3.5  SAMPLING 
 Sampling takes place at discrete - time instants, and the signal amplitude is approxi-
mated by a binary level. Regular time sampling is examined in this section, followed 
by  “ amplitude quantization ” (or simply  “ quantization ” ) in subsequent sections. 
 Figure  3.6 shows a conceptual realization of sampling. Since the real analog 
signal might vary in the time it takes to sample it, the signal must be  “ frozen ” while 
sampling takes place. This is shown as a switch which closes every  T seconds, fol-
lowed by the  “ hold ” device.  T is termed the  sample period and is measured in 
seconds or, more commonly, in milliseconds or microseconds. The reciprocal,  f s , is 
the sample rate and is measured in samples per second or hertz.
 
Sample period
s
Sample rate
Hz
s
s
=
=
=
T
f
f
T
1 .
 
 The term  “ zero - order hold ” (ZOH) is used to describe the fact that the signal is held 
constant during the sampling. The reconstruction after this is the  “ stairstep ” pattern 
as shown in Figure  3.6 . In the absence of any further information about the behavior 
of the signal  between the sampling instants, this may appear to be the best we can 
do. But, as will be shown in subsequent sections, it can be improved upon. 
 The output of the A/D converter is then a sequence of binary numbers repre-
senting the amplitudes at the sampling instants. These are termed  x ( n ), where the 
subscript  n means  “ the sample at time instant  n . ” Thus,  x ( n  −  1) is the measured 
 FIGURE 3.6   Sampling as a zero - order hold followed by A/D conversion. The switch 
closes at intervals of  T in order to sample the instantaneous value of the input. 
T
A/D
Hold
Signal
Stairstep
Numbers
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
62 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
sample value one sample instant ago, and  x ( n  −   4) is the sample value 4 T 
seconds ago. 
 If the discrete number stream were to be reversed, that is, converted back into 
an analog form, we would require an arrangement like that shown in Figure  3.7 . The 
D/A converter converts the digital (binary) value into an analog voltage and holds 
it for  T seconds. Note that the ZOH operation is in fact implicit in the D/A operation 
(again, because of the absence of any meaningful information about the true signal 
value between the sampling instants). 
 Mathematically, the operation of sampling may be realized as a multiplication 
of the continuous signal  x ( t ) by a discrete sampling or  “ railing ” function  r ( t ), where
 
 r t
t
nT
( ) =
=
⎧⎨⎩
1 0
0
.
:
:
.
otherwise  
 (3.14) 
 These sampling impulses are simply pulses of unit amplitude at exactly the sampling 
time instants. The sampled function is then as illustrated in Figure  3.8 , and mathe-
matically is
 
 
x n
x t r t
x nT
( ) =
( ) ( )
=
(
).
 
 (3.15) 
 Note that some texts use the notation  x ( n ) or  x ( nT ) rather than  x ( n ) to indicate that 
the signal  x is sampled rather than the continuous time signal  x ( t ). 
 Figure  3.9 illustrates this quantization process operating on a representative 
signal. The  “ stem - and - circle ” plots are used to indicate a sample. Each circle repre-
sents the amplitude of the particular sample. This type of plot is easily generated in 
MATLAB using the  stem () command, as shown next for a 10 - point random signal: 
  
 FIGURE 3.7   Reconstruction performed as digital - to - analog conversion followed by a 
zero - order hold. 
A/D
Hold
 FIGURE 3.8   The process of sampling may be imagined as the continuous signal being 
multiplied by unit sample impulses, each spaced at the precise sampling interval. 
×
Signal
Railing
Samples
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.5 SAMPLING 
63
 FIGURE 3.9   Converting a continuous signal to the corresponding discrete - time 
representation. The smoothly varying, continuous analog signal is sampled at discrete 
instants. These are represented by the lines at each sample instant, with the sample value 
represented as a circle. 
Original signal
Amplitude
Sample number
⇒
Sampled signal
Sample number
 FIGURE 3.10   The process of signal reconstruction. Each discrete sample value is held 
until a new sample arrives, thus approximating the original continuous signal as a stairstep 
approximation. Further ﬁ ltering smooths out the edges of the step, yielding the 
reconstructed signal. 
Sampled signal
Amplitude
Sample number
⇒
Reconstructed signal
Sample number
 x  =  randn (10,1); 
 stairs (x); 
 grid on  
 x  =  randn (10,1); 
 stem (x); 
 grid on;  
 Figure  3.10 shows the process of reconstruction from the discrete samples 
through D/A conversion. This type of plot is easily generated in MATLAB using the 
 stairs () command, as shown in the following example for a 10 - point random 
signal: 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
64 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 3.6  QUANTIZATION 
 Amplitude quantization is the process of representing the real analog signal by some 
particular level in terms of a certain  N - bit binary representation. Obviously, this 
introduces some error into the system because only 2  N discrete levels may be rep-
resented rather than the theoretically inﬁ nite number of levels present in the analog 
signal. To illustrate this, consider Figure  3.11 , which shows a 3 - bit quantizer. The 
input or  “ decision ” levels are shown on the horizontal axis as  x ( t ) since the input 
signal is continuous. The output or  “ reconstruction ” levels are shown in the vertical 
axis as  x k for discrete integer  k . Any of the three input amplitudes illustrated for  x ( t ) 
will be encoded as the same binary value of 010 and will produce the same output 
level when reconstructed at the D/A converter. 
 We now further explore the question of the number of bits to use in quantizing 
a particular signal. We do this ﬁ rst from an intuitive point of view and subsequently 
derive a theoretical result regarding the effect of the number of bits on the error 
introduced when a signal is quantized. 
 There are two issues which naturally arise from the process of quantizing a 
real - valued signal into a discrete representation. First, and easily overlooked, is the 
issue of the maximum range of the signal. Since we are using a ﬁ nite number of 
representational levels, they must span a certain signal range. Any input signal 
outside that range will be truncated or  “ clipped. ” The second issue is that, necessar-
ily, a ﬁ nite number of levels reduce the representational accuracy. This is termed the 
 “ quantization noise. ” 
 Both of these factors — the maximum or dynamic range and the quantization 
accuracy — should be considered together. For a ﬁ xed number of bits to allocate, we 
 FIGURE 3.11   A scalar quantizer characteristic, showing the input level  x ( t ), the binary 
output for each step, and the corresponding discrete reconstruction level  x k . 
x(t)
x k
Input
Reconstruction
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.6 QUANTIZATION 
65
 FIGURE 3.12   Quantization of signed signal using 3  bits, when the signal is 100% of the 
dynamic range. Signed value sampling is often used for audio (sound) sampling. 
Quantization of signed signal using 3 bits, signal 100% dynamic range
Analog signal
Quantized signal
Quantization error
0
0
64
64
128
128
128
–64
–64
–128
–128
–128
can span a certain range of signal, but that then compromises the accuracy of rep-
resentation of each level. Alternatively, if we allocate the available bits in such a 
way that the step size of the quantizer is small, the dynamic range will be limited. 
 Figures  3.12 – 3.14 illustrate these issues. A sinusoidal signal is used as an 
input, which is plotted against a range of 256 levels. Figure  3.12 shows the case of 
a 3 - bit quantizer (eight levels), where the signal span equals the quantizer span. The 
signal has positive and negative ranges, hence an average of zero. The lower panel 
shows the quantization error, which is simply the difference between the input signal 
and the quantized signal. Figure  3.13 shows the same experiment but with an 
unsigned signal source. Clearly, the same situation is occurring, and the quantizer 
span should be adjusted to 0 – 255, rather than  ± 128, as in the case of the signed input 
signal. The main point is that the quantization error is effectively the same and has 
an average value centered around zero. 
 Figure  3.14 shows the same quantization scenario as Figure  3.13 but with the 
signal occupying only 40% of the dynamic range of the quantizer. Clearly in this 
case, the quantization becomes  “ coarser ” even though the number of levels remains 
the same. This is because the quantizer step size is ﬁ xed. The result is that the 
quantization noise becomes larger when measured as a proportion of the input signal 
amplitude. 
 It should be pointed out that a 3 - bit quantization characteristic is somewhat 
unrealistic in practice. Such a value serves to illustrate the point graphically, however. 
So now let us turn to a statistical view of matters. Figure  3.15 shows the distribution 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 FIGURE 3.14   Quantization of an unsigned signal using 3  bits, when the signal is 40% of 
the dynamic range. Note how much larger the quantization noise is as a proportion of the 
source signal amplitude compared to when the signal occupies the full dynamic range 
(Figure  3.13 ). 
Quantization of unsigned signal using 3 bits, signal 40% dynamic range
Analog signal
Quantized signal
Quantization error
0
0
64
64
128
128
128
192
192
256
256
–128
 FIGURE 3.13   Quantization of an unsigned signal using 3  bits, when the signal is 100% 
of the dynamic range. The quantization error is the same as in the signed case (Figure 
 3.12 ). Unsigned value sampling is often used for image color components. 
Quantization of unsigned signal using 3 bits, signal 100% dynamic range
Analog signal
Quantized signal
Quantization error
0
0
64
64
128
128
128
192
192
256
256
–128
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.6 QUANTIZATION 
67
of a signal with a range of  ± 4, its quantized version (with  N  =  4  bits, and hence 16 
levels), and the distribution of the quantization error. We can see two things: ﬁ rst, 
that the operation of quantization serves to  “ thin out ” the probability density, so that 
the quantized signal is representative of the original in a statistical sense; second, 
the quantization error distribution is seen to be plus and minus half a quantizer step 
value. 
 3.6.1  Experiments with Quantization 
 In order to base our understanding of the effect of quantization, we consider some 
experiments based on real signals. What happens when we remove the bits from a 
sampled signal starting at the LSB? Starting at the MSB? 
 Figure  3.16 shows an image with the original 8 - bit resolution, and the succes-
sive removal of bits from the least signiﬁ cant end of the samples to obtain 4 and 
then 2  bits per pixel (bpp) images. The perceptual quality of such images depends 
on the display media and the relative size of the image, but it is quite clear that the 
2 - bit image is unacceptable in terms of quality. We can repeat the same experiment, 
 FIGURE 3.15   Quantization of a signal with a range of  ± 4 to  N  =  4  bits (16 levels). From 
top to bottom, we have the source signal probability distribution, the distribution of the 
quantized signal, and the distribution of the error. 
−4
−3
−2
−1
0
1
2
3
4
Signal value
Signal histogram
−4
−3
−2
−1
0
1
2
3
4
Signal value
Quantized signal histogram
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Signal value
Quantization error histogram
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
68 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
this time removing bits from the most signiﬁ cant end of each sample, as shown in 
Figure  3.17 . Comparing these, we can see that the MSBs in each sample effectively 
set the range of the signal, and when removed, the dynamic range is reduced. This 
is clearly shown in the histograms to the right of each image. However, removing 
the LSBs effectively increases the step size without changing the span of the 
representation. 
 We repeat the same experiments — removing bits to (re) - quantize a signal —
 this time with an audio signal. Figure  3.18 shows that removing the LSBs reduces 
the resolution by increasing the step size of the quantizer. The underlying signal is 
still present and indeed visible even at 8 - bit resolution. Removing the MSBs, as in 
Figure  3.19 , shows that the dynamic range of the signal is compromised. When 
played back, the sound of the latter is very much reduced in amplitude, whereas in 
the former case (removing LSBs, thus giving a coarser step), the presence of quan-
tization noise becomes very much apparent. 
 What conclusion do we draw from these experiments? First, the number of 
bits required for a faithful representation is very much dependent on the type of 
 FIGURE 3.16   Image quantization: removal of the least signiﬁ cant bits (LSBs). The 
corresponding histogram for each image is shown on the right. The quantization effectively 
increases the quantizer step size and thus reduces the accuracy of representation of the 
signal. In the case of images, this manifests itself as a stepping or  “ contouring ” of the 
image and is especially noticeable in regions of smoothly varying intensity. 
LSB removal
8 bits
4 bits
2 bits
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.6 QUANTIZATION 
69
signal. Here, images were represented with less than 8  bits per sample with reason-
able accuracy. However, audio was severely compromised when reduced from 16 
to 8 and fewer bits. The second conclusion is that rather than simply removing bits 
from the most or least signiﬁ cant end of a sample, the act of quantization really 
ought to be considered as a mapping (or remapping) of signal amplitudes according 
to a characteristic which is appropriate to the interpretation of the signal itself. 
 Finally, we need some basis to quantify the  “ noise ” introduced by the act of 
quantization. This noise could be visual noise (as in the preceding pictures) or audio 
noise, but in general terms, it is simply an error which we need to minimize. The 
following section investigates the nature of the noise in a mathematical sense. 
 3.6.2  Quantization and  SNR 
 The preceding examples have shown that we need some basis on which to allocate 
bits to a sample, so as to minimize the error or noise signal. In effect, we wish to 
 FIGURE 3.17   Image quantization: removal of the most signiﬁ cant bits (MSBs). The 
corresponding histogram for each image is shown on the right. The quantization effectively 
reduces the dynamic range of the signal, moving it toward black. In this example, we have 
compensated for the movement of the average so that the average is still midway 
brightness. Observation of the images shows that the range of intensities represented for 
6 - bit resolution is unacceptable. 
MSB removal
8 bits
7 bits
6 bits
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
70 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
minimize the noise power. What performance metric can we devise for this, and how 
is such a metric related to the number of bits actually employed in quantization? 
 Let the signal be  x ( n ) and the quantized (or approximated) signal be denoted 
as  ˆx n
( ). The quantization error is
 
 e n
x n
x n
( ) =
( ) −( )
ˆ
.  
 (3.16) 
 It does not particularly matter whether we deﬁ ne this as  x n
x n
( ) −( )
(
)
ˆ
 or 
 ˆx n
x n
( ) −( )
(
) since invariably we are not interested in whether the error is positive 
or negative, but rather the magnitude of the error. Furthermore, we are interested in 
the average error over a large number of samples. Thus, the performance measure 
will be related to the mean square error (MSE),
 
 MSE =
( )
∑
1
2
N
e
n
N
,  
 (3.17) 
where the length of the block  N is large enough to provide a good estimate of the 
average, and the error is squared. Squaring the error effectively negates the need for 
an absolute value. As it turns out, a squared error performance metric is not only 
intuitive (proportional to the signal power) but is also often mathematically easier 
 FIGURE 3.18   Sound quantization: removal of the least signiﬁ cant bits (LSBs). The 
corresponding histogram for each sound block is shown on the right. Starting at 16  bits, as 
we reduce the number of bits allocated, the dynamic range remains the same, but the step 
size becomes coarser. This manifests itself as an audible noise in the reconstructed signal, 
which is especially noticeable in softer sections of the audio (middle panels). 
LSB removal
12 bits
8 bits
6 bits
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.6 QUANTIZATION 
71
to manipulate. In some problems, particularly optimal and adaptive ﬁ ltering, we need 
to differentiate the signal, and differentiation of a squared value is easily handled 
by the chain rule of calculus. 
 The MSE as deﬁ ned above is useful, but the signiﬁ cance of the error is clearly 
also dependent on the signal itself. In other words, we need to determine the energy 
in the error signal in relation to the signal power itself. If the error is large, and the 
signal is large, the effect is less signiﬁ cant than if the error is large and the signal is 
small. So a better metric of performance is the quantizing SNR, deﬁ ned as the ratio 
of the signal power to the noise power and expressed in decibels: 2 
 
 SNR
dB
=
( )
( )
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
∑
∑
10
10
2
2
log
.
x
n
e
n
n
n
 
 (3.18) 
 For an equal step - size  N - bit quantizer, the step size  Δ is the ratio of the peak ampli-
tude to the number of steps. Assuming a range of  ± x max ,
 FIGURE 3.19   Sound quantization: removal of most signiﬁ cant bits (MSBs). The 
corresponding histogram for each sound block is shown on the right. The quantization 
effectively reduces the dynamic range of the signal. Since sounds tend to have a very wide 
dynamic range from the quietest sound to the loudest, we have severely compromised the 
performance of the system in doing this. For relatively quiet sections of the audio (as 
shown in the middle panels), the accuracy is still maintained, however. 
MSB removal
16 bits
14 bits
12 bits
  2  A decibel is deﬁ ned in terms of logarithm of power: 10  log 10 (power). 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
72 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 
 Δ = 2
2
x
N
max .  
 (3.19) 
 As Figure  3.15 indicates, it is reasonable to assume that the quantizing noise has an 
average of zero, and any particular value is equally likely plus or minus half a 
quantization step above and below the mean. This means that we have a zero mean 
value and a uniform distribution. 
 For any data set { x }, probability theory tells us that, given the probability 
density function  f ( X ), the mean and variance may be calculated as
 
 μ =
(
)
−∞
+∞∫
X f X dX  
 (3.20) 
 
 σ
μ
2
2
=
−
(
)
(
)
−∞
+∞∫
X
f X dX.  
 (3.21) 
 The variance of the quantizing noise is calculated by substituting the appropriate 
values into the above equation for the variance. We assume that the probability 
density  f ( X ) may be described by a uniform density with zero mean, and this 
seems correct given our previous observations on the distribution of quantizing 
noise. The error signal must range over one step size  Δ , and so the appropriate 
limits for integration are  ±(
)
Δ 2 . Additionally, probability theory tells us that 
the area under a density curve must be unity. Since the range of  x is now known to 
be  −(
)
Δ 2  to  +(
)
Δ 2 , the span of  x is  Δ , and thus  Δ · f ( X )  =  1. It follows that 
 f X
( ) = 1 Δ, which is a constant. So the equation for the variance of the quantization 
error becomes
 
 
σe
X
X
X
dX
X
2
2
2
2
3
2
2
3
1
1 1
3
1
3
2
2
=
=
⋅
=
⎛
⎝⎜
⎞
⎠⎟−−
⎛
⎝⎜
⎞
⎠⎟
−
+
=−
=+
∫
Δ
Δ
Δ
Δ
Δ
Δ
Δ
Δ
Δ
3
2
2
12
⎛
⎝
⎜
⎞
⎠
⎟
=
σe
Δ .
 
 (3.22) 
 This gives us the quantization error variance (or equivalently, the error power) in 
terms of the quantizer step size. But we would like to relate this to the number of 
levels rather than the step size. Equation  3.19 gives the link we need since it relates 
the signal maximum dynamic range to the number of levels. So substituting Equation 
 3.19 into Equation  3.22 yields
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.6 QUANTIZATION 
73
 
 σe
2
2
12
= Δ  
 (3.23) 
 
 
=
⋅
=
⋅
4
12 2
3 2
2
2
2
2
x
x
N
N
max
max .
 
 (3.24) 
 Therefore, the SNR is
 
SNR =
=
⋅
⎛
⎝⎜
⎞
⎠⎟
=
⋅
σ
σ
σ
σ
x
e
x
N
x
N
x
x
2
2
2
2
2
2
2
2
3 2
3
2
max
max
.
 
 Such a signal - to - noise power is normally expressed in decibels, so we have
 
 
SNR =
+
+
⎛
⎝⎜
⎞
⎠⎟
≈
+
+
10
3 10
2
10
4 77
6 02
2
10
10
2
10
2
2
log
log
log
.
.
max
N
x
x
N
σ
0
10
log
.
max
σx
x
⎛
⎝⎜
⎞
⎠⎟dB
 
 (3.25) 
 This result depends on the ratio  x max / σ x , which is the ratio of the peak amplitude to 
the standard deviation. 
 But since  σx
2 is effectively the signal power, we can say that  σ x is really the 
root mean square (RMS) of the signal, deﬁ ned as
 
 RMS =
( )
∑
1
2
N
x
n .  
 (3.26) 
 So, the above result really depends on the ratio of the peak signal amplitude to the 
RMS power of the signal, and we need an estimation of this value. Clearly, it will 
depend on the type of signal, but a reasonable assumption for this  “ loading ” factor 
is  x max  =  4 σ x . In that case,
 
 SNR
dB
≈
−
6
7 3
N
.
. 
 (3.27) 
 Equation  3.27 informs us of the important fact that the SNR is proportional to the 
number of bits used:
 
 SNR ∝N.  
 (3.28) 
 Note that this important general result does  not state that the SNR is proportional 
to the number of levels, for in that case we would have 2  N in the above. Put another 
way, adding 1  bit to the quantizer resolution gains an additional 6  dB of signal - to -
 noise performance. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
74 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 Note that there are several assumptions inherent in the above theory. First, 
 σ x / x max is really the RMS to peak value (since  RMS =
σx
2  when the average is 
zero). For a sine wave, this is  1
2
/
. For other waveforms, it will be different. For 
example, a  “ noiselike ” waveform may only have occasional peaks, so on average, 
the SNR will differ. The result is also only valid for a large number of quantizer 
steps (since we assumed that the quantization noise distribution is approximately 
uniform), and that relatively few samples are clipped (outside the range  ± 4 σ x ). 
 Moreover, the ultimate deﬁ nition of noise is perceptual — so we really need a 
subjective judgment rather than an objective measure like SNR. Finally, other ele-
ments in the signal chain also have an effect — for example, the quality of the viewing 
device for images and the speaker and audio ampliﬁ cation for sound. 
 So how does this theoretical result compare in practice? Table  3.2 summarizes 
the theoretical result as compared to an experimental result using a random signal 
source. As can be seen, the ﬁ gures are in close agreement. The approximate increase 
in SNR per bit is 6  dB, as predicted. This can be seen in Figure  3.20 . This is valid 
only for a large number of levels; hence, for 2 - bit quantization, the result does not 
agree as well. A second assumption is that the range of the quantizer is 
 Maximum
RMS
=
= ±
x
x
max
.
σ2
4 
 3.7  IMAGE DISPLAY 
 Quantization and representation of images deserve special mention because of the 
many possible ways a color image can be stored. This section explains these con-
cepts using some examples to illustrate. 
 TABLE 3.2   Signal - to - Noise as a Result of Quantization 
 Bits  N 
 Levels 2  N 
 Measured SNR (dB) 
 Theoretical SNR (dB) 
 2 
 4 
 1.9 
 4.8 
 3 
 8 
 9.7 
 10.8 
 4 
 16 
 16.2 
 16.8 
 5 
 32 
 22.5 
 22.8 
 6 
 64 
 28.7 
 28.8 
 7 
 128 
 34.8 
 34.8 
 8 
 256 
 40.9 
 40.8 
 9 
 512 
 46.9 
 46.8 
 10 
 1,024 
 53.0 
 52.8 
 11 
 2,048 
 59.0 
 58.8 
 12 
 4,096 
 65.0 
 64.8 
 13 
 8,192 
 71.1 
 70.8 
 14 
 16,384 
 77.1 
 76.8 
 The experimental result is obtained via simulation as described in the text using Gaussian noise with  σ  =  1. The 
theoretical result is obtained using Equation  3.25 with  σ x  =   1,  x max  =  4. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.7 IMAGE DISPLAY 
75
 3.7.1  Image Storage 
 Images are sampled spatially, in a grid pattern. A matrix of samples is the ideal 
mathematical representation for these samples, termed  “ pixels ” or  “ pels ” (picture 
elements). Of course there is a temporal (time) relationship to the position of 
each pixel — images are normally scanned left to right, top to bottom, as depicted in 
Figure  3.21 . 
 Since memory space is linear, the pixels themselves that form the image matrix 
 X ( m ,  n ) are mapped in the scan order. This is depicted in Figure  3.22 . Video 
 FIGURE 3.20   Quantization and measured signal to noise for a Gaussian random signal. 
2
3
4
5
6
7
8
9
10
11
12
13
14
0
20
40
60
80
Bits per sample
SNR (dB)
Signal−to−quantizing noise
 FIGURE 3.21   Images are sampled from left to right, top to bottom. The horizontal lines 
represent each row of pixels; the dotted lines represent the return back to the start of each 
scan line but are not visible. Finally, the dotted line from lower right to upper left 
represents the return to the start of the image. This would occur when video is being 
displayed, since each frame of video is, in effect, a still - image snapshot. 
Video screen
Visible
Horizontal retrace (next line)
Vertical retrace (next frame)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
76 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
sequences are effectively a concatenation of successive still - image frames, replayed 
at the appropriate frame rate. For the  k th frame, the pixels may be denoted 
 X k ( m ,  n ). Note that this illustrates a row - wise ordering of the pixels. Alternatively, 
the pixels could be stored columnwise. This in fact is the representation returned if 
we used the  “ colon ” operator in MATLAB. For example, if we have a hypothetical 
set of pixels as shown below, the colon operator extracts the values into a linear 
vector: 
  
 FIGURE 3.22   Video memory mapping. A two - dimensional image is mapped onto a 
one - dimensional memory. 
x
Linear offset of this pixel
y
w
h
p = y · w + x
 Xt  = X ′ 
 Xt  = 
     1      4      7 
     2      5      8 
     3      6      9 
 Xt(:) ′ 
 ans  = 
      1      2       3      4       5       6      7      8      9  
 X  = [1 2 3 ; 4 5 6 ; 7 8 9] 
 X  = 
    1     2      3 
    4     5      6 
    7     8      9 
 X(:) ′ 
 ans  = 
      1      4       7      2       5      8       3      6      9 
 If we desired the pixels in row order, we simply transpose the matrix: 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.7 IMAGE DISPLAY 
77
 Note that Figure  3.22 indicates the memory offset as  p  =  wy  +  x , where  p is 
the linear offset;  w is the width of the image; and ( x ,  y ) are the pixel indexes (column, 
row). This assumes the ( x ,  y ) offsets are zero based. If using MATLAB indexes, it 
is necessary to subtract one from the  x and  y values before being used in this way. 
Finally, the conventional indexing of matrices is (row, column), whereas image 
coordinates are usually deﬁ ned as ( x ,  y ), which is the reverse order. 
 3.7.2  RGB Color Representation 
 Color representation is done using the three primary colors — red, green, and blue. 
Normally, 8  bits are reserved for each color separately. This is sometimes termed 
RGB or  full - color representation, and 2 24 colors are possible in theory. 
 To see how this works in practice, consider reading in a real image (in this 
case, a JPEG image ﬁ le). 
  
 NewImageMat  =  zeros (ImageHeight, ImageWidth, NumColorPlanes); 
 for y  = 1:ImageHeight 
      for x  = 1:ImageWidth 
           % get original pixel RGB value 
           redval  =  double (ImageMat(y, x, 1)); 
           greenval  =  double (ImageMat(y, x, 2)); 
           blueval  =  double (ImageMat(y, x, 3)); 
           % alter one or more color planes 
            %redval  = 0 ; 
           greenval  = 0; 
           %blueval  =  0 ; 
           % save pixel RGB value 
           NewImageMat(y, x, 1)  = redval; 
 FileName  =  ′ MyImage.Jpg ′ ; 
 [ImageMat cmap]  =  imread (FileName); 
 [ImageHeight ImageWidth NumColorPlanes]  =  size (ImageMat); 
 image (ImageMat) 
 set ( gca ,  ′ DataAspectRatio ′ , [1 1 1]); 
 axis ( ′ off ′ );  
 If this is an RGB image, the number of color planes stored in 
NumColorPlanes will equal 3. In the following, we loop over all image pixels 
horizontally and over all rows vertically. For each pixel value, we extract the red, 
green, and blue color values. So, the red value is stored ﬁ rst and extracted using 
redval  =  double (ImageMat(y, x, 1)). The next numerical value is the 
green value of that pixel and ﬁ nally the blue (indexes 2 and 3, respectively). 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
78 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 In this example, we have (arbitrarily) set the green component of each pixel to 
zero. Thus, the resulting image has no green components at all. Finally, note that the 
displayed image is converted to a uint8 value to yield 1  byte for each color plane. 
 To fully understand the effect of color mixing, consider a one - pixel image 
with RGB components such that red and green are present but with no blue 
component:
  
 img  =  zeros (1 ,1 ,3); 
 img(1 ,1 ,1)  = 1;  % red 
 img(1 ,1 ,2)  = 1;  % green 
 img(1 ,1 ,3)  = 0;  % blue 
 image (img); 
 axis ( ′ off ′ );  
           NewImageMat(y, x, 2)  = greenval; 
           NewImageMat(y, x, 3)  = blueval; 
           end 
 end 
 % convert to 8 - bit per RGB component 
 NewImageMatRGB8  = uint8(NewImageMat); 
 image (NewImageMatRGB8); 
 set ( gca ,  ′ DataAspectRatio ′ , [1 1 1 ]); 
 axis ( ′ off ′ );  
 This mixing of red and green only gives the color yellow. Note one other subtle 
but important issue in the above: We have used a double - precision image matrix. In 
that case, MATLAB expects the components to be in the range 0 – 1. However, if the 
RGB values were unsigned 8 - bit integers (the usual case), then they would be scaled 
to the range 0 – 255. 
 3.7.3  Palette Color Representation 
 To reduce storage requirements, some storage formats and display systems use a 
palette - based system, which uses a color lookup table (LUT) to indirectly produce 
the colors from the samples as shown in Figure  3.23 . Each pixel value is used to 
index a palette or table of colors in the video display hardware. Each entry contains 
one intensity level for each of the three primary colors. The palette must be chosen 
and optimized carefully for the given image, as the palette can represent only a ﬁ nite 
number of colors. For 8 - bit pixel values, this gives 2 8 possible colors as compared 
to 2 24 for full - color systems. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.7 IMAGE DISPLAY 
79
 Note that although using a palette - based system reduces the amount of memory 
required for the video display and reduces the transmission time on digital links by 
a factor of three, the palette itself is unique to the image being displayed. Therefore, 
it must be prepended to image ﬁ les when stored or transmitted separately. 
 The MATLAB command  colormap () is used to set the color palette (or 
color map) for a particular image ﬁ gure. This function expects an array of length 
equal to the number of quantization levels. Each entry contains three elements, 
ranging from 0.0 to 1.0, to represent the intensity of each of the three primary colors. 
 As a simple example of using a palette, the following shows how to generate 
a random grayscale image using a matrix. An 8 - bit (256 - level) display is used, 
although the pixels are quantized to one of only four possible levels: 
  
 FIGURE 3.23   Using the color palette as a lookup table to produce a pixel. 
Screen
Palette
Pixel
(index)
R
Mix
D/A
D/A
D/A
G
B
 M  = 64; 
 N  = 64; 
 x  =  rand (M, N); 
 m  =  ﬂ oor (255 * ( ﬂ oor (x  * 4)/3)); 
 image (m([1:4], [1:4])); 
 colormap ( gray (256)) 
 axis ( ′ off ′ );  
 The upper 4  ×  4 pixels from this image matrix are extracted using matrix 
subscripts as follows: 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
80 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 x  = 1:8:256; 
 m  = x(ones(20 ,1), :); 
 image (m); 
 z256  =  zeros (256,1); 
 cmap  = [ z256 z256 [1:256] ′ ]/256; 
 colormap (cmap); 
 axis ( ′ off ′ );  
 m([1:4], [1:4]) 
 ans  = 
      255       255        0          170 
      170       255        170        85 
      0         0           85         255 
       170       85         170        170  
 FIGURE 3.24   A magniﬁ ed view of a 4  ×  4 pixel image with four gray levels. Of course, 
in a realistic display scenario, each pixel would be quite small — effectively merging with 
other nearby pixels to form a continuous image if viewed at a normal distance. 
 These values are then displayed using the current color map with the  image 
() command and appear as shown in Figure  3.24 . Obviously, the pixel size in this 
image is exaggerated. Note that this is a grayscale image — color images are usually 
processed by decomposing the colors into the separate constituent primary colors 
(red, green, and blue) as described earlier. 
 The following example shows how to display an image using a blue - only scale 
by zeroing out the red and green components: 
  
 Monochrome or grayscale image representation is possible by setting each of 
the red, green, and blue components in the palette to equal values. For an 8 - bit image, 
this gives a linear scale of intensity, ranging linearly from black (level  =  0) to white 
(level  =  255). The following MATLAB code illustrates this: 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.8 ALIASING 
81
 FileName  =  ′ MyImage.bmp ′ ; 
 [ImageMat cmap]  =  imread (FileName); 
 if (  
 ˜  isempty (cmap) ) 
      disp ( ′ This image has a colormap ′ ); 
 end 
 [ImageHeight ImageWidth NumColorPlanes]  =  size (ImageMat); 
 image (ImageMat); 
 colormap (cmap); 
 set ( gca ,  ′ DataAspectRatio ′ , [1 1 1]); 
 axis ( ′ off ′ );  
 M  = 32; 
 N  = 128; 
 x  =  rand (M, N); 
 x  =  ﬂ oor (x * 256); 
 image (x); 
 colormap ( gray (256)); 
 axis ( ′ off ′ ); 
 set ( gca ,  ′ DataAspectRatio ′ , [1 1 1]);  
 We can alter the earlier RGB image processing code to read images which 
contain a color palette. The following code reads in an image and checks the returned 
value cmap, which is the matrix of color map values (notice that this was ignored 
in the earlier example). For an 8 - bit image, this matrix will be of the dimension 
256  ×  3 since there are 2 8 color index values, each with a red, green, and blue 
component. 
  
 Note that in this case, we have used a bitmap image (bmp ﬁ le extension). It 
should be made clear that the extension does not determine the presence of a 
palette — the extension determines how the data are internally stored (such as an 
uncompressed bitmap or a compressed JPEG). Both of these image types are able 
to encapsulate palette - based or RGB image data. A robust program will test for the 
presence of a color map and process the image accordingly. 
 3.8  ALIASING 
 Let us now again consider the problem of how fast to sample a given signal. If the 
sampling rate is not sufﬁ ciently fast, a problem called  aliasing occurs. Put simply, 
this makes the signal  “ appear ” to be slower than what it actually is. The sampling 
rate theorem states that we must sample at a rate which is greater than twice as fast 
as the highest frequency component present in order to be able to later reproduce 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
82 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
the original waveform faithfully. This minimum sampling rate is usually called the 
 Nyquist rate . 3 Put in another way, the sampling rate used dictates the highest fre-
quency that can be faithfully represented, and we have the following fundamental 
principle: 
 
 The  Nyquist frequency or  folding frequency is half the sampling rate and corresponds to 
the highest frequency which a sampled data system can reproduce without error. 
 The  Nyquist rate is twice the bandwidth of the signal, and we must sample above this 
rate in order to ensure that we can reproduce the signal without error. 
 The  minimum sampling frequency must be at least twice that of the highest frequency 
component present in the original signal. 
 Consider Figure  3.25 . In simple terms, we might ask what the input signal is 
doing  between samples. Intuitively, we might think that the signal must be smooth 
and  “ well behaved ” between samples. For this reason, a low - pass ﬁ lter is used before 
the sampling process as shown in Figure  3.26 . This is done in order to remove any 
frequencies higher than half the sampling frequency  fs 2
(
). This ﬁ lter is termed an 
 anti - aliasing ﬁ lter. If this ﬁ lter limits the highest frequency component in a given 
signal (called bandlimiting), then the Nyquist rate is applicable: 
 
 This may sound surprising and, in some ways, not really intuitive. If we think 
about this further, it might suggest that for a sinusoidal waveform, we only need two 
samples per complete cycle. But remember that it is a  lower bound —  in other words, 
we must sample  faster than this. 
 If we turn the argument around, then a given sampling rate will dictate the 
highest frequency which we can reproduce after sampling. This is called the Nyquist 
or folding frequency. 
 
 So, to summarize these important points: The fundamental theorem of sam-
pling states that we must sample at least twice as fast as the highest frequency 
component that we want to maintain after sampling. In practical terms, the sampling 
rate is chosen to be higher than this two - times rule above would suggest. For 
example, CD audio systems are designed to handle frequencies up to approximately 
20  kHz. The sample rate is 44.1  kHz — a little more than double the highest expected 
frequency. The precise reasons for this will be examined in a later chapter, after the 
theory of ﬁ ltering is developed. 
  3  Sometimes, the terms Nyquist or Shannon sampling theorem are used, after Harry Nyquist and Claude 
Shannon, who developed the key underpinnings of the sampling and digital transmission theory used 
today. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 FIGURE 3.26   The positioning of an anti - aliasing ﬁ lter on the input to a sampling 
system. It stops any signals above half the sampling rate from passing through — the signal 
is then said to be  “ band - limited. ” The resulting sample stream then contains enough 
information to  exactly reconstruct the signal as it was after the low - pass ﬁ lter. 
Low-pass
ZOH
A/D
 FIGURE 3.25   How aliasing arises. We have a 4 - Hz sine wave to be sampled, so the 
Nyquist rate is 8  Hz, and we should sample above that rate. In the upper waveform, we 
sample below the input frequency. Clearly, we cannot expect to capture the information 
about the waveform if we sample at a frequency lower than the waveform itself. The 
waveform appears to have a frequency of | f  −  f s |  =  |4  −  3|  =  1  Hz. Next, we sample a little 
above the waveform frequency. This is still inadequate, and frequency is effectively 
 “ folded ” down to | f  −  f s |  =  |4  −  5|  =  1  Hz. Finally, we sample at greater than twice the 
frequency (  f s  >  2 f or 10  >  2  ×  4  Hz). The sampled waveform does not produce a lower 
 “ folded - down ” frequency in this case. 
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Aliasing in sampling: f = 4 Hz, fs = 3 Hz
Time (second)
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Aliasing in sampling: f = 4 Hz, fs = 5 Hz
Time (second)
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
No aliasing in sampling: f = 4 Hz, fs = 10 Hz
Time (second)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
84 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 3.9  RECONSTRUCTION 
 For a different reason, a  reconstruction ﬁ lter is used after the output of the D/A 
system, as depicted in Figure  3.27 . This is also a low - pass ﬁ lter — one that removes 
the high - frequency components. The need for this ﬁ lter is a little more obvious: 
Consider the  “ stairstep ” reconstructions that have been shown so far. We need to 
remove the  “ jagged ” edges to reconstruct the original signal. This ﬁ lter also has a 
cutoff frequency of  fs 2. 
 So far, we have the signal of interest being ﬁ rst band - limited with the anti -
 aliasing input ﬁ lter, sampled at a sufﬁ cient rate, processed, output through a ZOH, 
and ﬁ nally reconstructed with a low - pass ﬁ lter. The obvious question is  “ how do we 
know this low - pass ﬁ ltering on the output is the ideal arrangement? ” In the process 
of answering this question, it becomes apparent that it is possible to calculate the 
values which the signal is  expected to take on between the known samples. 
This process is termed  “ interpolation. ” We will investigate reconstruction and inter-
polation now and return to the mathematical underpinnings of reconstruction in 
Section  8.7 . 
 3.9.1  Ideal Reconstruction 
 So far, we have used the ZOH as the output of the D/A system. The  ideal function 
for interpolating between samples is 4 
 FIGURE 3.27   A reconstruction ﬁ lter on the output of a sampled data system. Provided 
the signal was sampled at a rate greater than twice the highest frequency present in the 
original, the output of a reconstruction ﬁ lter will be identical to the original signal. Note 
that the zero - order hold (ZOH) is not present as a separate component as such; it simply 
represents how a practical D/A converter works in reality. 
Low-pass
ZOH
D/A
  4  This will be derived in Section  8.7 . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.9 RECONSTRUCTION 
85
 
 
h t
t
T
t
T
t
T
( )
sin
sin
.
=
=
π
π
π
 
 (3.29) 
 This  sin x
x
(
)
 or  “ sinc ” function is shown in Figure  3.28 , for the case of a sampling 
period  T  =  0.1. 
 Note that in the reconstruction function  h ( t ), the zero crossings are at  nT , where 
 n is an integer. This may easily be seen from the above equation for  h ( t  −  nT ) — the sine 
function is zero when  πt T is 0,  π , 2 π ,  …  , which is whenever  t  =  nT for any integer  n . 
What this means is that the reconstruction is done by a sample - weighted sinc function, 
where the sinc impulse from each sample does not interfere with the other sinc func-
tions from other samples at each sampling instant. This can be seen in Figure  3.28 , for 
the case where  T  =  0.1. In between the sampling instants, the sinc function provides 
the optimal interpolation. 
 To effect the ideal or perfect reconstruction, an inﬁ nite number of interpolation 
functions must be overlaid, one at each sample instant, and weighted by the sample 
value at that instant. The equation for each sinc function delayed to each sample 
instant  t  =  nT is
 
 h t
nT
t
nT
T
(
)
sin (
) .
−
=
−
c
π  
 (3.30) 
 To see this, consider Figure  3.29 , which illustrates the steps involved in taking the 
original continuous signal and sampling it at discrete intervals. In this case, the 
samples are at intervals of  T  =  0.1 second. Using the conventional ZOH, this would 
be reconstructed as shown. However, using the sinc function interpolation, a weighted 
sinc function centered at each sampling instant, we can theoretically obtain perfect 
reconstruction. Figure  3.29 shows one sinc function, and this single function is 
translated in time and scaled in amplitude as shown in Figure  3.30 . When all these 
 FIGURE 3.28   The  “ sinc ” function  sin π
π
t T
t T
(
) (
) for the ideal reconstruction of a 
sample at  t  =  0, with a sampling period of  T  =  0.1. 
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
−0.5
0
0.5
1
Amplitude
Time relative to sample instant at t = 0
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
86 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 FIGURE 3.30   The sinc functions scaled and centered on each sample (left). The sum of 
these sinc functions equals the original (right), if they are allowed to extend for inﬁ nite 
time in either direction. Here, the sinc functions are truncated to show the discrepancy 
between the sum and the original waveform.  
0
0.2
0.4
0.6
0.8
1.0
−4
−3
−2
−1
0
1
2
3
4
Overlaid reconstruction functions
0
0.2
0.4
0.6
0.8
1.0
−3
−2
−1
0
1
2
3
Original and reconstructed waveforms
 FIGURE 3.29  Illustrating the sampling of a signal. The original signal (upper left) is 
sampled at discrete - time instants. The reconstruction (lower left) can be done by holding 
each sample value until the next sample arrives. We can, however, reconstruct the waveform 
exactly if we use the sinc function. One sinc function centered on  t  =  0 is shown. 
0
0.5
1.0
−4
−2
0
2
4
Original continuous signal
0
0.5
1.0
−4
−2
0
2
4
Sampling points on continuous signal
0
0.5
1.0
−4
−2
0
2
4
Zero-order hold reconstruction
−2
−1
0
1
2
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1.0
Sinc reconstruction function
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.9 RECONSTRUCTION 
87
translated and scaled functions are added together, we can obtain an  exact recon-
struction. However, this is only valid for reconstruction functions extending to an 
inﬁ nite duration, in both positive and negative time (i.e., future and past). Obviously, 
this is not feasible in practice. Fortunately, we do not need to obtain perfect time 
reconstruction in order to capture sufﬁ cient information about the signal. 
 3.9.2  Polynomial Interpolation 
 Another interesting approach, rather than using a sinc function interpolation, is to 
use a polynomial to ﬁ t the wave at the sample points. The Lagrange interpolation 
function is ideal for this purpose. 5 
 The problem is to estimate all the points  x ( t ) over continuous time  t (or in 
practice, a large number of discrete points) given only samples  x ( t k ) at discrete points 
 t k . An  N th - order polynomial interpolation of  x ( t ) at known points  t k requires the use 
of  N  +  1 samples. To see why this is so, consider a linear interpolation of the form
 
 x t
at
b
( )
.
=
+
 
 (3.31) 
 This equation has two unknowns,  a and  b , and hence requires two data points for a 
unique solution. Thus, two data points are required for this ﬁ rst - order interpolator. 
Interpolation of order zero is effectively a ZOH, as has been used previously. A 
third - order interpolation (requiring four points) is shown in Figure  3.31 , with a ﬁ rst -
 order interpolation between each pair of points shown for comparison. Higher - order 
polynomial interpolations as  N  →  ∞ tend to a sinc function. 
 FIGURE 3.31   Lagrange interpolation for  N  =  3, with linear interpolation for comparison. 
0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
7
8
Time
Sample value
Lagrange order N = 3 polynomial interpolator
  5  Named after the French - Italian mathematician Joseph - Louis Lagrange. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
88 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 The Lagrange interpolation is given mathematically by
 
 x t
L t x t
k
k
k
N
( )
( ) ( ),
=
=∑
0
 
 (3.32) 
where  x ( t k ) is the value of the sample at instant  t k and  L k ( t ) is the interpolating poly-
nomial term multiplied by each sample value, and is given by
 
 L t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
k
k
k
N
k
k
k
k
( )
(
)
(
)(
)
(
)
(
)
(
)(
=
−
−
−
−
−
−
−
−
+
−
0
1
1
0
1
…
…
…
k
k
N
t
t
+
−
1)
(
).
…
 
 (3.33) 
 Note that the terms on the numerator are of the form ( t  −  t k ), and the terms on the 
denominator are of the form ( t k  −  t i ), with the single term ( t k  −  t k ) removed from both 
the numerator and denominator. At the sample time  t  =  t k (corresponding to the 
current  “ center ” of interpolation), the interpolating function  L k ( t k )  =  1 because 
the numerator will equal the denominator. At time instants corresponding to one of 
the other known samples, say,  t  =  t k  +  1, one of the numerator terms will be zero, 
and thus the interpolating function  L k ( t k  +  1)  =  0. Therefore, the interpolation is 
guaranteed to pass through the known samples with weighting of one:
 
 L t
n
k
n
k
k
n
( )
:
:
.
=
=
≠
⎧⎨⎩
1
0
 
 (3.34) 
 3.10  BLOCK DIAGRAMS AND 
DIFFERENCE EQUATIONS 
 Once the samples have been acquired and quantized, they can be processed by a 
signal processing algorithm. This is done using a mathematical  “ model ” for the 
processing. Such an input – output model is depicted in Figure  3.32 . The input at 
sample instant  n is denoted  x ( n ), and the output is  y ( n ). The transfer function from 
input to output characterizes the system: Given a known input, we can compute the 
corresponding output. This transfer function may be represented by its  impulse 
response —  that is, the response of the system when subjected to a single input of 
amplitude 1.0 at sample instant  n  =  0. 
 The output is represented as the set of amplitudes  y ( n ), as  n takes on the integer 
values 0, 1, 2, 3,  ...  . The impulse response is useful because the response of any 
linear system to  any given input may be thought of as being the summation of 
 FIGURE 3.32   System block diagram: input  x ( n ), output  y ( n ), and impulse response 
coefﬁ cients  h n . 
hn
x(nT )
x(n)
Samples
“System”
Time
Samples
Input
Output
Time
y(nT )
y(n)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.10 BLOCK DIAGRAMS AND DIFFERENCE EQUATIONS  
89
responses to as a series of sequential, scaled impulses. This is sometimes called the 
 “ principle of superposition. ” To see this, consider the simple  difference equation 
given by
 
 y n
x n
x n
y n
( )
.
( )
.
(
)
.
(
).
=
+
−
−
−
0 9
0 8
1
0 5
1  
 (3.35) 
 A realization of the impulse response of this equation in MATLAB is shown in the 
following. Note that MATLAB uses array indexes starting at 1, whereas the usual 
algorithmic description (as above) uses indexes starting at 0. Also, note that it is 
necessary to test for the proper indexing of the arrays. The resulting impulse response 
is shown in Figure  3.33 . 
  
 FIGURE 3.33   Impulse response samples  h n given the difference equation coefﬁ cients of 
Equation  3.35 . 
1
2
3
4
5
6
7
8
9
10
−0.2
0
0.2
0.4
0.6
0.8
1.0
Sample number
Output amplitude
Impulse response
 NT  = 10; 
 x  =  zeros (NT, 1); 
 y  =  zeros (NT, 1); 
 x(1)  = 1; 
 for n  = 1:NT 
      y(n)  = 0.9  * x(n); 
      if (n   >  1) 
          y(n)  = y(n)  + 0.8  * x(n    −  1); 
          y(n)  = y(n)    −  0.5  * y(n  −  1); 
       end 
 end 
 stem (y);  
 Now consider input sequence
 x n
( )
{ . ,
. , . }.
=
−
0 6
0 2 0 8
 
 As shown in Figure  3.34 , the response of the system (lower panel) is in fact the 
summation of the individual, scaled, and delayed impulse responses. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
90 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 Equation  3.35 is the difference equation for a speciﬁ c system. To expand the 
difference equation to cater for all possible systems we may encounter, we need a 
more general equation for the case where there may be more coefﬁ cients of either 
the input or output. The following general form will do this:
 
 y n
b x n
b x n
b x n
N
a y n
a y n
a y
N
M
( )
(
( )
(
)
(
))
(
(
)
(
)
=
+
−
+
+
−
−
−
+
−
+
+
0
1
1
2
1
1
2
…
…
(
)).
n
M
−
 
 (3.36) 
 Effectively, the coefﬁ cient of the left - hand side  y ( n ) is  a 0  =  1. 
 The implementation of this equation is shown in block diagram form in Figure 
 3.35 . Note how the output is fed back to the input via the  a k coefﬁ cients (which may 
or may not be present). At least one of the  b k terms must be present in order to couple 
the input  x ( n ) into the system. The choice of a negative sign for the  a coefﬁ cients 
is arbitrary (since the  a ’ s are constant coefﬁ cients). However, a negative sign is 
normally used to simplify later mathematical analysis.  
 In order to cater for difference equations of any arbitrary size, a more  “ general -
 purpose ” coding provides for easier modiﬁ cation — both of the number of coefﬁ -
cients and their values. The MATLAB code for one possible method of realizing 
this is shown below. Note that although it appears more complex, it is more general 
and thus more easily modiﬁ ed for use with other systems. 
 FIGURE 3.34   Visualizing the impulse response as the superposition of successive 
impulse responses.  
x(0) = 0.6 response
x(1) = −0.2 response
x(2) = 0.8 response
x = {0.6, −0.2, 0.8} response
0
0
0
0
1
1
1
1
–1
–1
–1
–1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.10 BLOCK DIAGRAMS AND DIFFERENCE EQUATIONS  
91
 FIGURE 3.35   Block diagram representation of a difference equation. Four feed - forward 
 b coefﬁ cients are shown, together with three feedback (a) coefﬁ cients. Of course, this could 
be increased to any arbitrary size.  
z −1
z −1
z −1
x(n)
x(n −1)
x(n −2)
x(n −3)
b0
b1
b2
b3
z −1
z −1
z −1
y(n −1)
y(n −2)
y(n −3)
−a1
−a2
−a3
y(n)
 NT  = 10; 
 b  = [0.9 0.8]; 
 N  =  length (b)   −  1; 
 a  = [1 0.5]; %  a(1)  =  1 always 
 M  =  length (a)   −  1; 
 x  =  zeros (NT, 1); 
 y  =  zeros (NT, 1); 
 x(1)  = 1; 
 for n  = 1:NT 
      y(n)  = 0; 
      for k  = 0:N 
           if ( (n    −  k)   >  0 ) 
               y(n)  = y(n)  + x(n   −  k)  * b(k  + 1); 
           end 
      end 
      for k  = 1:M 
           if ( (n    −  k)   >  0 ) 
              y(n)  = y(n)   −  y(n   −  k)  * a(k  + 1); 
            end 
      end 
 end 
 Setting x(1)  =  1 in the code provides an impulse input to the system (i.e., 
sample  x (0)  =  1). The variable NT is the total number of input – output points to 
iterate over. 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
92 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 Noting the symmetry in the basic difference equation, the loops for multiply-
ing the  b  and  a coefﬁ cients are seen to be virtually identical. Effectively,  a 0  =  1 
because  a 0 is implicitly the coefﬁ cient of  y ( n ), the output on the left - hand side of 
Equation  3.36 . Also, the question may be asked as to why the  a values are subtracted. 
The answer is simply that it is conventional, and since the  a ’ s are constant coefﬁ -
cients, it makes no difference whether they are deﬁ ned as positive or negative in the 
difference equation. 
 Since the iteration of difference equations is common in discrete - time systems, 
MATLAB includes a built - in function to perform the above difference equation 
iterations. This function is called  ﬁ lter (), which takes, as expected, the equation 
coefﬁ cients  a k ,  b k , and the input signal  x ( n ), and generates the output signal  y ( n ), as 
shown in the following example: 
  
 NT  = 10; 
 b  = [0.9 0.8]; 
 a  = [1 0.5]; 
 x  =  zeros (NT, 1); 
 x(1)  = 1; 
 y  =  ﬁ lter (b, a, x); 
 stem (y); 
 3.11  LINEARITY, SUPERPOSITION, 
AND TIME INVARIANCE 
 In iterating difference equations, it is normal to assume zero initial conditions; that 
is, samples prior to  t  =  0 or sample values of index  n  <  0 are assumed to be zero. 
This is easily done in practice by initializing all variables and arrays to zero at the 
start - up of the iterations. This brings us to several underlying principles which 
underpin the operation of discrete - time systems and models: linearity, superposition, 
and time invariance. 
 Consider  linearity ﬁ rst. Suppose we have a system deﬁ ned as
 
 y n
x n
x n
x n
x n
( )
.
( )
.
(
)
.
(
)
.
(
).
=
+
−
−
−
+
−
0 9
0 8
1
0 4
2
0 1
3  
 (3.37) 
 We can iterate this difference equation to determine the output in response to an 
impulse as a speciﬁ c case and extend that to any arbitrary input signal. Suppose we 
have an input sequence  x ( n ) as illustrated in Figure  3.36 , which happens to consist 
of ﬁ ve constant samples followed by zeros. The output  y ( n ) of the system deﬁ ned 
by this equation is shown. Now suppose we double the magnitude of the input. As 
shown, we expect this to translate to a linear multiplication of the output. This means 
the output is now 2 y ( n ). In general, for a linear system, if we have an input sequence 
 x ( n ) giving an output sequence  y ( n ), then  kx ( n )  →  ky ( n ). Furthermore, if we have 
two separate inputs, say,  x 1 ( n ) and  x 2 ( n ), and the output of a system in response to 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.11 LINEARITY, SUPERPOSITION, AND TIME INVARIANCE  
93
each applied separately is  y 1 ( n ) and  y 2 ( n ), respectively, then the output of the com-
bined inputs  x 1 ( n )  +  x 2 ( n ) will equal  y 1 ( n )  +  y 2 ( n ) if the system is  linear . 
 We now examine what happens when we apply a given input at a different 
time. This is illustrated in Figure  3.37 . We have an input  x ( n ) applied, which happens 
to be two positive samples in succession. The output  y ( n ) is shown. Now if we delay 
the  x ( n ) sequence and again apply to the linear difference equation, the output is 
clearly the same shape but delayed by a corresponding amount. In other words, 
if an input  x ( n ) gives an output  y ( n ), then delaying by  N samples, we have 
 x ( n  −  N )  →  y ( n  −  N ). Such a system is said to be  time invariant . 
 An essential consequence of linearity is that a linear system will also obey the 
 principle of superposition . This is illustrated in Figure  3.38 . We have an input  x ( n ), 
consisting of ﬁ ve nonzero samples, and the corresponding output  y ( n ). Now suppose 
we imagine the input  x ( n ) to be constructed from the sequences  x 1 ( n ) and  x 2 ( n ) as 
shown in the ﬁ gure — in other words,  x ( n )  =  x 1 ( n )  +  x 2 ( n ). Each of these, applied to 
the system independently, gives the outputs  y 1 ( n ) and  y 2 ( n ), respectively. So, if we 
use the combined signal  x 1 ( n )  +  x 2 ( n ) as input (which happens to equal  x ( n )), then 
the summation of the separate outputs is  y 1 ( n )  +  y 2 ( n ), which is identical to the 
original  y ( n ). Thus, the separate inputs are able to be applied separately, and the 
 FIGURE 3.36   Illustrating the  linearity of a system. Scaling the input scales the output 
accordingly, in a linear fashion. We can further extend this to multiple inputs to a system, 
which leads to the principle of superposition as illustrated in Figure  3.38 . 
Input x
Output y
Input x2 = 2x
Linearity
Output y2  (=2y)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
94 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
output will be identical to the sum of the separate outputs as if each sequence was 
applied independently. Mathematically, this may be summarized by saying that if 
 x 1 ( n )  →  y 1 ( n ) and  x 2 ( n )  −  y 2 ( n ), then  x 1 ( n )  +  x 2 ( n )  →  y 1 ( n )  +  y 2 ( n ). 
 So, let us look at a hypothetical example. Suppose we have a system whose 
characteristic is  y ( n )  =  x 2 ( n ). If we have an input to this system of  x ( n )  =  x 1 ( n )  +  x 2 ( n ), 
then the output would be ( x 1 ( n )  +  x 2 ( n )) 2 . But if we applied each input separately 
and added them together, it would be  x
n
x
n
1
2
2
2
( )
( )
+
, and this is clearly  not equal to 
( x 1 ( n )  +  x 2 ( n )) 2 . Hence, this is not a linear system. 
 To formalize this a little more, if the system performs an operation  f ( · ), then 
for constants  k 1 and  k 2 , a linear system obeys
 
 f k x n
k x n
k f x n
k f x n
{
( )
( )}
{ ( )
{
( )}.
1
1
2
2
1
1
2
2
+
=
+
 
 (3.38) 
 Finally, there is a very important class of algorithms called  adaptive algorithms , 
which do in fact have time - varying characteristics. Such algorithms are used to 
process speech, for example, which varies over time. If we examine a short enough 
time interval, the speech signal characteristics appear  “ approximately ” the same, 
though over a longer interval they change. Adaptive algorithms are also used in the 
 FIGURE 3.37   Illustrating the concept of  time invariance. If the input is time shifted, the 
output of the system is correspondingly shifted in time. 
Input x
Output y
Input x2 = delayed x
Time  invariance
Output y2  (=delayed y)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.12 PRACTICAL ISSUES AND COMPUTATIONAL EFFICIENCY  
95
form of adaptive ﬁ lters, which iteratively adjust themselves according to some steer-
ing function over time and attempt to converge on a ﬁ nal solution according to the 
signals presented to them. 
 Most of our work will be concerned with linear, time - invariant (LTI) systems. 
Real signals and systems often only approximate the above requirements, but if the 
approximation is close enough, the LTI theory can be successfully applied. 
 3.12  PRACTICAL ISSUES AND 
COMPUTATIONAL EFFICIENCY 
 The previous examples have implicitly assumed that we iterate the difference equa-
tion over a ﬁ xed maximum number of samples. Realistically, the system output may 
in fact go on indeﬁ nitely, thus apparently requiring an inﬁ nite amount of memory. 
Inspection of the difference (Equation  3.36 ) shows that it is only necessary to store 
the previous  N inputs, plus the present one, and  M previous outputs. 
 FIGURE 3.38   Illustrating the principle of  superposition. If the input is decomposed into 
separate inputs which are applied separately, the output is effectively superimposed as the 
sum of all inputs considered separately. A system must be linear in order for this to occur. 
Input x
Output y
Input x1
Output y1
Input x2
Output y2
Input x1+x2
Superposition
Output y1+y2
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
96 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 NT  = 10; 
 b  = [0.9 0.8]; 
 N  =  length (b)   −  1; 
 a  = [1 0.5]; 
 M  =  length (a)   −  1; 
 % input: impulse 
 x  =  zeros (N  + 1, 1); 
 x(1)  = 1; 
 FIGURE 3.39   Delay - line buffering using a  “ stack - based ” algorithm. The indexes are 
shown starting at zero, but it must be remembered that MATLAB indexes start at one. 
0
New sample
Memory index
Discard oldest sample
1
2
3
4
x(n)
x(n −1)
x(n −2)
x(n −3)
x(n −4)
 The obvious practical solution is to use a  “ stack ” architecture to store the 
previous inputs and outputs, as illustrated in Figure  3.39 . At the end of each iteration, 
the oldest sample is discarded. Sample  x ( n ) is stored in vector element x(1); sample 
 x ( n  −  1) is stored in vector element x(2), and so forth. 6 We can extend our MATLAB 
code for difference equations to implement this arrangement in a more general -
 purpose manner, as shown below. The variable yplot is only used for the purpose 
of saving the data over the iteration — in a practical system, it need not be saved. 
Note the segment labeled  “ age samples, ” which implements the stack in order to 
 “ push back ” samples on each iteration. 
 One problem with the stack - based storage of past samples is that it requires a 
considerable amount of copying for high - order systems. Apart from the calculation 
of the difference equation itself, for  N current and previous inputs and for  M current 
and previous outputs, a total of  N  +  M sample copy operations are required. One 
solution is to employ special - purpose digital signal processors which have dedicated 
copy - and - multiply instructions. 
  
  6  Remember that MATLAB starts array indexes at 1 and not 0. C starts indexes at 0. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.12 PRACTICAL ISSUES AND COMPUTATIONAL EFFICIENCY  
97
 Alternatively, the memory copying may be eliminated by using a circular 
buffer arrangement as shown in Figure  3.40 . Instead of physical copying, a modulo -
 N index is used to  “ dereference ” the actual samples required. A modulo variable is 
one which counts up to the modulo limit and then resets to zero. For example, count-
ing modulo - 5 gives the sequence 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,  ...  . 
 The indexing of the required samples is calculated with a modulus of the 
number of samples stored. Referring to Figure  3.40 , suppose there are ﬁ ve samples 
stored,  x ( n ) to  x ( n  −  4). These are assigned physical (memory) indexes 0 – 4, number-
ing from the left - hand side. At time  t k , suppose the current sample  x ( n ) has physical 
index 3 (starting from zero). Sample  x ( n  −  1) has index 3  −  1  =  2; sample  x ( n  −  2) 
is stored in location 3  −  2  =  1, and so forth. At time sample  t k  + 1 , sample  x ( n ) is stored 
in one location further on, in physical location 4. Sample  x ( n  −  1) is stored in loca-
tion 4  −  1  =  3. At the subsequent time instant, time  t k  + 2 in the diagram, the physical 
index is incremented to 5. However, since only samples up to  x ( n  −  4) are required 
in this application, we do not need to use another new memory location. 
 It is possible to reuse the oldest sample location, which was sample  x ( n  −  4) 
at time instant  t k  + 1 , but which would otherwise have become sample  x ( n  −  5) at time 
instant  t k  + 2 . When the index for the new sample  x ( n ) is incremented, we simply test 
if it is equal to the number of samples (in this case, 5). If it is, then the index is 
simply reset to 0. Calculating the true physical index of previous samples is then 
slightly more difﬁ cult. As before, sample  x ( n  −  1) is stored one location prior to the 
 % output 
 y  =  zeros (M  + 1, 1); 
 yplot  =  zeros (NT, 1);  % saves for plot 
 for n  = 1:NT 
      % difference equation 
      y(1)  = 0; 
      for k  = 0:N 
           y(1)  = y(1)  + x(k  + 1)  * b(k  + 1); 
      end 
      for k  = 1:M 
           y(1)  = y(1)   −  y(k  + 1)  * a(k  + 1); 
      end 
      yplot(n)  = y(1);  % save for plotting 
      % age samples 
      for k  = N  + 1: − 1:2 
           x(k)  = x(k   −  1); 
      end 
      for k  = M  + 1: − 1:2 
            y(k)  = y(k   −  1); 
      end 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
98 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
current sample. This now corresponds to 0  −  1  =  1. Sample  x ( n  −  2) is stored at 
index 0  −  2  =  − 2. Since memory location indices only start at 0, this represents a 
problem. The solution is simply to test whether the physical index thus calculated 
is negative, and if it is, simply add the number of samples to it. In this case, index 
 − 1 becomes  − 1  +  5  =  4. This tells us that sample  x ( n  −  1) is now stored at location 
4. As mentioned before, this type of  “ wrapping ” calculation is termed  “ modulo 
arithmetic. ” 
 3.13  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  The nature of  binary representations , including  integer , ﬁ xed, and  ﬂ oating -
 point numbers, and how these are applicable to signal - processing problems 
 •  Sampling and  quantization of analog signals and their reconstruction 
 •  A review of  sampling theory including time sampling and difference 
equations  
 •  Important concepts in linear systems, including  superposition ,  linearity , and 
 time invariance 
 •  The role of  transfer functions and  block diagram representations, including an 
understanding of how to  implement difference equations, and related efﬁ ciency 
issues 
 FIGURE 3.40   Circular buffering:  “ in - place ” buffering using modulo buffer pointer 
arithmetic. Sample  x ( n ) represents the current sample at each time. At time  t k  + 1 , we reuse 
the oldest sample space by  “ wrapping ” the current sample pointer around. 
x(n)
x(n −4)
x(n −3)
x(n −2)
x(n −1)
0
1
2
3
4
x(n −4)
x(n −3)
x(n −2)
x(n −1)
x(n)
0
1
2
3
4
x(n −3)
x(n −2)
x(n −1)
x(n)
0
1
2
3
4
tk
tk+1
tk+2
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.13 CHAPTER SUMMARY 
99
 PROBLEMS 
 3.1.  A CD can store approximately 650  MB of data and can store about 60 minutes of 
digital audio.
 (a)  Show that the effective data rate is approximately 1.4  Mbps. 
 (b)  Calculate the approximate sample rate, byte rate, and bit rate for a sample rate 
of 44.1   kHz (answers: 88,200 samples per second; 176,400 bytes per second; 
1,411,200  bps) 
 3.2.  Check the correctness of the negative values in Table  3.1 using Equation  3.2 . 
 3.3.  What is the trade - off between ﬁ xed - point and ﬂ oating - point number representations? 
How can it be that, using the same number of bits, a ﬂ oating - point number can rep-
resent much larger numbers as well as very small numbers?  
 3.4.  The code listing in Section  3.4.4 shows numbers with 20 digits of precision after the 
decimal point, but the last several digits appear to be runs of zeros. Explain why this 
is, in terms of the MATLAB value for  eps and the fact that the values are stored with 
precision  single . 
 3.5.  It was shown that MATLAB evaluates log2 (double(intmax  +  1)) to 31. Why not 32? 
What is the reason for  + 1? 
 3.6.  The multiplication of binary numbers in Section  3.4.3 showed how to multiply two 
 N - bit binary numbers. How many bits does the product require in order to store the 
largest possible unsigned result? 
 3.7.  The number 11.5 is stored as a 32 - bit ﬂ oating - point value in IEE754 format. Show 
that the biased exponent is 130 decimal (binary 10000010) and that the fractional 
part is 1.4375 decimal (binary 01110000000000000000000). Write the entire 32 - bit 
representation.  
 3.8.  Use the ﬂ oating - point CORDIC code given to compute  cos π 4
(
) and  sin π 4
(
). For 
8 - bit iterations, how close does the result come to the single - precision calculation? 
How close does it come using 16 - bit calculations? 
 3.9.  Reimplement the CORDIC code given to use only bitwise integer operations in 
MATLAB and verify its correct operation using the example given in the text. 
 3.10.  Work through the MATLAB image processing examples in Section  3.7 . Note the use 
of the various  set () commands and, in particular, how to set the aspect ratio. Using 
the  whos command, note the dimensions and data type of the image itself and the 
palette matrix where applicable.  
 3.11.  Given samples at times  t 0  =  1,  t 1  =  3 of value  x ( t 0 )  =  2 x ( t 1 )  =  5, determine the Lagrange 
interpolator. Plot this using a small step size for  t in order to verify that the line goes 
through the deﬁ ned points ( t 0 ,  x ( t 0 )) and ( t 1  x ( t 1 )). 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
100 
CHAPTER 3 SAMPLED SIGNALS AND DIGITAL PROCESSING
 3.12.  Given samples at times  t 0  =  1,  t 1  =  3,  t 2  =  4 of value  x ( t 0 )  =  2,  x ( t 1 )  =  5,  x ( t 2 )  =  4, 
determine the  Lagrange interpolator. Plot the known points above, together with the 
interpolated function. Use a small step size to plot the interpolated points and verify 
that the interpolated points go through the known data points. 
 3.13.  Check that the three difference equation methods given in Section  3.10 produce 
identical results.  
 3.14.  Find the difference equation and then the transfer function corresponding to the fol-
lowing implementation of an impulse response:
 b  = 1; 
 a  = [1   − 2 * cos ( pi /10) 1]; 
 x  =  zeros (100, 1); 
 x(1)  = 1; 
 y  =  ﬁ lter (b, a, x); 
 stem (y);  
 3.15.  Using the color mixing code in Section  3.7.2 , display a one - pixel image of the fol-
lowing colors:
 (a)  Red only 
 (b)  Green only 
 (c)  Blue only 
 (d)  Red  +  green 
 (e)  Red  +  blue 
 (f)  Green  +  blue 
 (g)  Red  +  blue 
 (h)  Red  +  green  +  blue 
 (i)  Red  +  0.5  ×  green 
 (j)  0.6  ×  red  +  0.5  ×  green  +  0.6  ×  blue 
 In each case, display the resulting image and verify the mixing of the colors. 
 3.16.  Repeat the previous question on color mixing, but use an unsigned 8 - bit image for 
the display. Does the rounding implied by the color weighting when scaling to integers 
make a perceptible difference? For example, 0.3  ×  255  =  76.5, and this would be an 
integer value of either 76 (truncated) or 77 (rounded). 
 3.17.  Using the color mixing code in Section  3.7.2 and the color mixing concepts as dis-
cussed, load an RGB image and convert it to a grayscale image. Take care to handle 
overﬂ ow issues correctly when adding the color components. 
 3.18.  Using the palette - based image display code in Section  3.7.3 , display the random image 
using the grayscale color map. Then, use another color map which is built into 
MATLAB, such as  hsv or  bone . What use could you think of for these  “ artiﬁ cial ” 
color maps? 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
3.13 CHAPTER SUMMARY 
101
 3.19.  The terms RMS, MSE, and SNR were used in Section  3.6.2 in connection with quan-
tization. These terms are important in many aspects of signal processing besides 
quantization.
 (a)  Explain in words the meaning of RMS, MSE, and SNR. What physical charac-
teristic do they represent? 
 (b)  MSE is deﬁ ned as 
 MSE =
−
∑
1
2
N
y n
x n
N ( ( )
( )) .
Explain each term of the equation. 
 (c)  RMS is deﬁ ned as 
 RMS =
∑
1
2
N
x
n
( )
Explain each term of the equation. 
 (d)  SNR in decibels is deﬁ ned as 
 SNR =
⎛
⎝
⎜⎜
⎞
⎠
⎟⎟
∑
∑
10
10
2
2
log
( )
( )
.
x
n
e n
n
n
Explain each term of the equation. 
 3.20.  The measurements 44, 46, 53, 34, and 22 are quantized to the nearest multiple of 4. 
Determine the quantized sequence, the MSE, and the SNR.  
 3.21.  Consider Figure  3.25 , which shows a 4 - Hz signal. What would happen if we sampled 
at 8 Hz? Why do we need to sample  above the Nyquist rate? 
 3.22.  The following code generates 2 seconds of 200 - Hz sine wave and plays it back. 
Change the frequency to 7,800  Hz and repeat the experiment. Can you explain the 
result? Why does the frequency appear to be the same? What other frequencies would 
give similar results and why? 
 fs  = 8,000; 
 T  = 1/fs; 
 tmax  = 2; 
 t  = 0:T:tmax;  % sample points 
 f  = 200;      % sound frequency 
 y  =  sin (2 * pi * f * t); 
 sound (y, fs);  
 
  
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER  4 
RANDOM SIGNALS 
 4.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to
 1.  differentiate between  random and  deterministic signals; 
 2.  explain the fundamental principles of  statistics , as they apply to signal 
processing; 
 3.  utilize  statistical distributions  in understanding and analyzing signal process-
ing problems; and 
 4.  explain and implement  histogram equalization  and  median ﬁ ltering . 
 4.2  INTRODUCTION 
 Random noise signals may be represented mathematically using statistical methods. 
It is important to understand the nature of  “ random ”  systems, and that  “ random ”  
does not necessarily mean  “ totally unpredictable. ” From a signal processing perspec-
tive, if we understand the nature of random ﬂ uctuations in a signal, we are in a better 
position to remove, or at least to minimize, their negative effects. To this end, some 
practical examples in image ﬁ ltering and image enhancement are included in this 
chapter to demonstrate some practical applications of theory. 
 4.3  RANDOM AND DETERMINISTIC SIGNALS 
 A broad but useful categorization of signal types is into two classes:
 1.  Random signals  are those which are not precisely predictable; that is, given 
the past history of a signal and the amplitude values it has taken on, it is not 
possible to precisely predict what particular value it will take on at certain 
instants in the future. The value of the signal may only be predicted subject 
to certain (hopefully known) probabilities. 
 2.  Deterministic signals are those which  are predictable. They may be described 
by equations, and from the recent past history of the signal, we can predict 
103
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
104 
CHAPTER 4 RANDOM SIGNALS
what amplitude values the signal will have at deﬁ ned time instants. Subject to 
unavoidable quantization errors, arithmetic precision errors, and rounding 
errors, the prediction is exact and has no error. 
 Random signals can still be modeled — they are described by a probabilistic model, 
allowing us to state the  likelihood of the signal taking on certain values in the future. 
We have encountered deterministic signals in the last chapter, where the difference 
equation was used to calculate new system output values. 
 Figure  4.1 illustrates the concepts of random and deterministic signals. It is 
immediately obvious that the signal in the upper panel is periodic (repeats itself over 
a certain interval or period,  τ ) and is therefore able to be predicted using a mathe-
matical model. This mathematical description of the signal and the resulting equa-
tions are termed the  signal model . Parameters may be determined for a particular 
model to describe a particular signal — in simple terms, consider a summation of 
sinusoidal waveforms:
 
 x t
A
t
A
t
( ) =
+
(
) +
+
(
)
1
1
1
2
2
2
sin
sin
.
Ω
Ω
ϕ
ϕ
 
 (4.1) 
 The parameters  A 1 and  A 2 represent amplitudes of each signal;  Ω 1 and  Ω 2 represent 
frequencies; and  φ 1 and  φ 2 are the phase shifts of each. This model would not 
adequately describe a signal which contains (say) three sinusoidal components, and 
 FIGURE 4.1   A comparison of random and deterministic signals. The question is: Are the 
dotted sections predictable, given only the previous observations (solid line)?  
0
5
10
15
20
25
−4
−2
0
2
4
Time
Amplitude
Deterministic data
0
5
10
15
20
25
−4
−2
0
2
4
Time
Amplitude
Random data
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.4 RANDOM NUMBER GENERATION 
105
the model would have to be extended to cater for that. The two - component model 
would be inaccurate. 
 For discrete systems, we can apply the general difference equation model:
 
 
y n
b x n
b x n
b x n
N
a y n
a y n
a y n
N
M
( ) =
( ) +
−
(
) +
+
−
(
)
(
)
−
−
(
) +
−
(
) +
+
0
1
1
2
1
1
2


−
(
)
(
)
M
.  
 (4.2) 
 The constants  N and  M must be known, and the  b and  a parameters must be found 
(or approximated), in order to specify the signal model. 
 The signal shown in the lower plot of Figure  4.1 appears somewhat random 
and hence cannot be predicted. However, passive examination can be somewhat 
misleading. We need mathematical tools and algorithms to determine whether signals 
are in fact truly random. Random signals may be described by the likelihood of 
having a particular value. Such a statistical model also requires several parameters 
to be determined. 
 In reality, real - world signals are often best described by a combination of a 
base deterministic component together with a random or noise component. The 
simplest such formulation is an additive noise model:
 
 x t
s t
v t
( ) = ( ) +
( )
α
,  
 (4.3) 
where  x ( t ) is the observed signal,  s ( t ) is the underlying periodic signal, and  v ( t ) 
is the additive random noise. The amount of additive noise is modeled by the 
parameter  α . 
 In a similar way, a deterministic system is one whose underlying impulse 
response may be characterized by a mathematical model. In digital signal processing, 
this is usually a linear time - invariant (LTI) difference equation, as explained in 
Section  3.11 . 
 Random signals will be considered in the following sections. In order to do 
this completely, it is necessary to ﬁ rst review some statistical theory, initially for 
one dimension but with an overview of multidimensional signals. Once the basic 
concepts have been developed, a practical application of the theory is then intro-
duced. This involves enhancing a  “ noise - corrupted ” image. This means that the 
signal data we have to process are no longer exactly the same as the original due 
to external factors beyond our control. These may be man - made or natural interfer-
ence in the case of a transmission system, or perhaps errors introduced in storing 
the signal. A useful approach is to imagine the unwanted noise being described 
(or modeled) as a random signal added to the desired image data signal, as per 
Equation  4.3 . 
 4.4  RANDOM NUMBER GENERATION 
 Random signal models are useful in real - world signal processing problems. Since 
we are considering digital signal processing, generating a random signal is equiva-
lent to ﬁ nding some method for generating random numbers. The ﬁ rst problem 
is the problem of randomness — what is truly random, after all? The operation of 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
106 
CHAPTER 4 RANDOM SIGNALS
computer code is perfectly repeatable: Given a sequence of steps (an algorithm) in 
code and an initial value for all data used, running the code a second time produces 
 exactly the same result as the previous time. Running it 100 times will produce 100 
identical results. So, using computers to generate random numbers presents a 
problem. To get a feel for this, consider generating a sequence of random samples 
using MATLAB ’ s  rand () function: 
  
 rand ( ’ seed ’ , 0) 
 rand 
 ans  = 
      0.2190 
 rand 
 ans  = 
      0.0470 
 rand 
 ans  = 
      0.6789  
 x  =  rand (1000, 1); 
 subplot (2, 1, 1);  stem (x(1:100)); 
 subplot (2, 1, 2);  stem (x(101:200));  
 Outwardly, the resulting samples will appear random — but are they? In fact, 
computer - based random number generators are termed  pseudorandom . The sequence 
appears random but is in fact repeatable. Therefore, it is not truly random. If the 
random number generator is restarted using an initial starting point or  “ seed, ” the 
sequence is identical: 
  
 The output on the ﬁ rst run is 0.2190, 0.0470, and 0.6789. On second and 
subsequent runs, the output is again 0.2190, 0.0470, and 0.6789. Thus, the samples 
are not, in the strictest sense, random. However, from one sample to the next, they 
are  “ random enough ” for most signal processing purposes. Furthermore, it clearly 
depends on the initial starting point or seed for the random number generator. It is 
not uncommon to sample a high - speed system clock for this purpose. 
 4.5  STATISTICAL PARAMETERS 
 The simplest statistic to compute is the  mean , which, mathematically, is in fact the 
 arithmetic mean . For  N signal samples, this is
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.5 STATISTICAL PARAMETERS 
107
 
 x
N
x n
n
N
=
( )
=
−
∑
1
0
1
.  
 (4.4) 
 This is what we commonly refer to as the  average . Strictly speaking, this quantity 
is the  sample mean because it is derived from a sample or subset of the entire popu-
lation (not to be confused with the fact that a signal itself is sampled, in the signal 
processing sense). The symbol  μ denotes the  population mean , which is the mean 
of values if we have the entire set of possible samples available. Usually, we 
work with a subset of observed (sampled) values; hence, the sample mean is 
what we are referring to. If we take different subsets of samples, the sample mean 
calculated from each set itself forms a distribution which is representative of the 
population mean. 
 For a continuous (nonsampled) signal, the mean over a time interval,  τ , is
 
 x
x t dt
=
( )
∫
1
0
τ
τ
.  
 (4.5) 
 Another parameter which will be shown in Section  4.11 to be very useful in 
designing image - restoring ﬁ lters is the  median . The median is the value for which 
half of the samples lie above it, and half lie below. For example, consider the 
sequence
 6
4
8
3
4.  
 The (sample) mean is easily calculated to be 5. The median is found by ﬁ rst ordering 
the samples in ascending order:
 3
4
4
6
8. 
 Crossing off 3 and 8, then 4 and 6, we are left with the median value of 4 
(actually the second 4). This is the sample with index  N −
(
)
=
−
(
)
=
1 2
5 1 2
2 in the 
ordered list. 
 Lastly, the  mode or  modal value is the most common value. It is found by 
counting the frequency of each sample. For example, the data used above have a 
frequency table which is easily derived: 
 Sample 
 0 
 1 
 2 
 3 
 4 
 5 
 6 
 7 
 8 
 Frequency 
 0 
 0 
 0 
 1 
 2 
 0 
 1 
 0 
 1 
 In this case, the mode is 4. Of course, the summation of the frequency counts 
must equal the total number of samples. 
 The  mean square is deﬁ ned as
 
 x
N
x
n
n
N
2
2
0
1
1
=
( )
=
−
∑
.  
 (4.6) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
108 
CHAPTER 4 RANDOM SIGNALS
 A similar quantity, the  variance , is deﬁ ned as
 
 σ2
2
0
1
1
=
( ) −
(
)
=
−
∑
N
x n
x
n
N
.  
 (4.7) 
 The variance,  σ 2 , and the standard deviation,  σ , are related to the spread of the data 
away from the mean. The greater the variance, the greater the average  “ spread ” of 
data points away from the mean. The limiting case — a variance of zero — implies 
that all the data points equal the mean.  
 4.6  PROBABILITY FUNCTIONS 
 Rather than one or two quantities (mean and variance) to characterize a signal, it is 
useful to have a more complete description in terms of the probability of observing 
the signal at or below a certain level. Many real - world signal processing applications 
(e.g., voice - based speaker identiﬁ cation) beneﬁ t from more advanced statistical 
characterizations. 
 A signal,  x ( t ), is shown in Figure  4.2 . Suppose a particular level,  X , is taken, 
where  X is a value the random variable  x can take on at some instant in time. Using 
the deﬁ nitions in the ﬁ gure, we could determine the proportion of time (in the 
whole observation record) that the signal spends under the nominated value  X . 
Mathematically, this is
 FIGURE 4.2   The cumulative probability of a signal is the time spent below a moving 
threshold,  X . The value of this function must rise from zero (when  X is most negative) to 
unity (when  X is most positive). The shape of the curve is dictated by the nature of the 
time signal. 
Calculation of cumulative probability function F(X)
Time
Amplitude
t1
t2
t3
t4
τ
X
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.6 PROBABILITY FUNCTIONS 
109
 
 F X
x t
X
( )
Pr{ ( )
}.
=
≤
 
 (4.8) 
 With reference to Figure  4.2 , for a particular value  X of signal  x ( t ) sampled over 
time interval  τ ,
 
 F X
t
t
t
t
(
) =
+
+
+
1
2
3
4
τ
.  
 (4.9) 
 This is termed the  cumulative probability . It is the probability of the signal being 
below a certain amplitude level. If we take equally likely random numbers in the 
interval [0, 1], what shape would the cumulative probability curve be? This can be 
demonstrated with the following MATLAB code: 
  
 N  = 100000; 
 x  =  rand (N, 1); 
 dX  = 0.01; 
 cumprob  = [ ]; 
 XX  = [ ]; 
 for X  =  − 4:dX:4; 
      n  =  length ( ﬁ nd (x   <  X)); 
      cumprob  = [cumprob n/N]; 
      XX  = [XX X]; 
 end 
 plot (XX, cumprob); 
 set ( gca ,  ’ ylim ’ , [0 1.1]);  
 As might be expected, the value of  F ( X ), or the variable cumprob in the 
code, is found to be linearly increasing from 0 to 1. The random number generator 
is  rand (), which generates  equally likely random numbers in the range 0 – 1. 
 If we use a  normal or  Gaussian distribution, the values around the mean are 
more likely. 1 This is easily demonstrated by changing  rand () above to  randn (). 
The result is an  “ S - shaped ” curve, gradually rising from 0 to become asymptotic to 
1. Gaussian noise is commonly used as a statistical model for  “ real - world ” noise 
sources. 
 A very much related concept is that of the  probability density function (PDF). 
As shown in Figure  4.3 , this is the likelihood that a signal at a given instant 
lies in a range from  X to  X  +  δ X . From Figure  4.3 , this is calculated over time 
interval  τ as
 
 f X
X
t
t
t
t
t
(
)
=
+
+
+
+
δ
δ
δ
δ
δ
δ
τ
1
2
3
4
5 .  
 (4.10) 
  1  Named after the German mathematician Johann Carl Friedrich Gauss. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
110 
CHAPTER 4 RANDOM SIGNALS
 Note that this is given the symbol  f ( X ) and that the deﬁ nition is a  density , which 
depends on the particular value of  δ X — hence,  δ X appears on the left - hand side of 
the deﬁ nition. 
 Using the deﬁ nition, it is relatively straightforward to write some MATLAB 
code to generate some random numbers in the interval [0, 1] and to determine their 
PDF, as follows: 
  
 N  = 100000; 
 x  =  rand (N, 1); 
 dX  = 0.01; 
 pdf  = [ ]; 
 XX  = [ ]; 
 for X  =  − 4:dX:4; 
      n  =  length ( ﬁ nd ((x   >  X)  & (x    <  (X  + dX)))); 
      prob  = n/N; 
      pdf  = [pdf prob/dX]; 
      XX  = [XX X]; 
 end 
 plot (XX, pdf);  
 FIGURE 4.3   The probability density of a signal: time spent in an interval  δ X wide, at 
offset  X . As the value of  X sweeps from  − ∞ to  + ∞ , the quantity of the signal present in the 
 δ X interval changes. Where the signal  “ spends ” most of its time, the PDF will be largest. 
Conversely, values of  X where the signal is not often spending time will result in a small 
PDF value. 
Calculation of probability density function f(X)
Time
Amplitude
δt1
δt
δX
2
δt3
δt4
δt5
τ
X
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.6 PROBABILITY FUNCTIONS 
111
 Intuitively, we would  expect the PDF of a set of uniformly distributed random 
numbers to be a straight line — this is, after all, why they are called  “ uniformly dis-
tributed. ” But the above example shows that the distribution is not precisely uniform. 
This is because we are making two assumptions about a random process: that we 
have an inﬁ nite number of samples and that the interval of measurement is inﬁ nitely 
small. First, in the code example given, the number of samples is  N , and ideally, we 
want the limit as  N  →  ∞ . Second, the density interval  δ X is chosen to be an  “ arbi-
trarily small ” number. Ideally, we want the limit as  δ X  →  0. 
 Changing the random number generator function  rand () to a Gaussian 
source using  randn () produces a  “ bell - shaped ” curve for the PDF. The cumulative 
probability and probability density for a Gaussian source are illustrated in Figure 
 4.4 . The  “ bump ” in the Gaussian PDF shows the range of values which are more 
likely. In most practical applications, it is this observation — that some values are 
more likely than others — that is crucial to understanding the nature of the signal. 
 The PDF allows us to determine the probability of the signal being between 
 X 1 and  X 2 — it is the  area under the  f ( X ) curve between these limits:
 
 Pr
.
X
x t
X
f X dX
X
X
1
2
1
2
≤
( ) ≤
{
} =
( )
∫
 
 (4.11) 
 Since the signal must exist between  ± ∞ with a probability of one, the total area must 
be equal to one:
 FIGURE 4.4   Relationship between cumulative probability and probability density, 
illustrated using a Gaussian distribution. 
−8
−6
−4
−2
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1.0
Sample value
Probability density
−8
−6
−4
−2
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1.0
Cumulative probability
Sample value
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
112 
CHAPTER 4 RANDOM SIGNALS
 
 
f X dX
(
)
=
−∞
+∞∫
1.  
 (4.12) 
 The relationship between  f ( X ) and  F ( X ) is also important. Since
 
 Pr
,
x t
X
F X
( ) ≤
{
} =
(
)  
 (4.13) 
 then it follows that
 
 Pr
.
x t
X
X
F X
X
( ) ≤
+
{
} =
+
(
)
δ
δ
 
 (4.14) 
 Combining these two, we can say that
 
 Pr
.
X
x t
X
X
F X
X
F X
≤
( ) ≤
+
{
} =
+
(
) −
(
)
δ
δ
 
 (4.15) 
 The left - hand side of this expression is  f ( X )  δ X , and so  f ( X ) can be calculated from
 
 f X
F X
X
F X
X
( )
.
=
+
(
) −
(
)
δ
δ
 
 (4.16) 
 This is the derivative of  F ( X  ), and hence the rate of change of the cumulative prob-
ability equals the probability density, or
 
 f X
dF X
dX
(
) =
(
);  
 (4.17) 
 that is, the slope of the  F ( X  ) curve (lower curve) in Figure  4.4 equals the  f ( X ) curve 
(upper curve). Note the points where the slope is a maximum and also where it is 
equal to zero. 
 In reverse, the area under the PDF curve from  − ∞ to some value  X equals the 
cumulative density, or
 
 F X
f x dx
X
(
) =
( )
−∞∫
;  
 (4.18) 
 that is, the area under the  f ( X ) curve (upper curve) in Figure  4.4 equals the  F ( X ) 
curve (lower curve). 
 The area starts off at zero (leftmost, or  X  =  − ∞ ) and becomes equal to one, as 
shown by the  F ( X ) curve becoming asymptotic to one. 
 4.7  COMMON DISTRIBUTIONS 
 The Gaussian distribution has been encountered several times. It has a particular 
equation which describes the PDF in terms of the mean and variance:
 
 f
x
x
x
Gaussian( ) =
−
−
(
)
⎧
⎨⎪
⎩⎪
⎫
⎬⎪
⎭⎪
1
2
2
2
2
σ
π
σ
exp
.  
 (4.19) 
 This equation can be used to determine the  mathematically expected or most likely 
value of a random source, which is known to be Gaussian, provided its mean and 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.7 COMMON DISTRIBUTIONS 
113
variance are known. Another common PDF signal model is the Laplacian 
distribution: 2 
 
 f
x
x
x
Laplacian ( ) =
−
−
⎧
⎨⎪
⎩⎪
⎫
⎬⎪
⎭⎪
1
2
2
σ
σ
exp
. 
 (4.20) 
 Of course, the uniform PDF is simply
 
 f
x
K
x
x
x
x
x
uniform
otherwise
( ) =
=
−
≤
≤
⎧
⎨⎪
⎩⎪
1
0
max
min
min
max
:
:
.  
 (4.21) 
 Figure  4.5 shows the effect of changing the mean of a Gaussian distribution. The 
overall shape is retained, but the center point is moved to coincide with the mean. 
 FIGURE 4.5   Probability density: effect of changing the distribution parameters. For a 
given distribution, we can change the mean and/or variance. Changing the mean moves the 
center peak, and changing the variance changes the spread of the distribution. The three 
types of distribution discussed in the text are also shown for comparison. 
−8
−4
0
4
8
0
0.1
0.2
0.3
0.4
x
f(x)
f(x)
f(x)
f(x)
Gaussian: μ = 0 and μ = 4
−8
−4
0
4
8
0
0.1
0.2
0.3
0.4
x
Gaussian: σ = 1 and σ = 2
−8
−4
0
4
8
0
0.1
0.2
0.3
0.4
x
Gaussian: μ = 0, σ = 1 and μ = 1, σ = 2
Changing distributions and distribution parameters
−8
−4
0
4
8
0
0.2
0.4
0.6
0.8
x
Gaussian, Laplacian, uniform
  2  Named after the French mathematician Pierre - Simon Laplace. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
114 
CHAPTER 4 RANDOM SIGNALS
 Figure  4.5 also shows the effect of changing the variance of a Gaussian dis-
tribution. The overall shape is retained but becomes somewhat more peaked as the 
variance tends to zero. A larger variance makes for a ﬂ atter curve — this is to be 
expected, as the area must equal one. The peak is still centered on the mean. 
 Note that the mean and variance of a signal may be calculated from the PDF 
alone. The mean of a signal,  x, is
 
 x
xf x dx
=
( )
∫
.  
 (4.22) 
 This is really the summation of the signal value, weighted by the PDF. As the signal 
value gets further away from the mean, it is weighted less by the PDF. Similarly, 
the mean square in terms of the PDF is
 
 x
x f x dx
2
2
=
( )
∫
,  
 (4.23) 
and the variance is
 
 σ2
2
=
−
(
)
( )
∫x
x
f x dx. 
 (4.24) 
 4.8  CONTINUOUS AND DISCRETE VARIABLES 
 The discrete - valued counterpart of the probability density is the  histogram . The 
histogram is simply the count of how many samples have each possible value in the 
range. Generation of the histogram may be visualized as in Figure  4.6 , where (for 
the sake of illustration) the histogram is shown on its side, with height proportional 
to the time the signal spends at a particular level. 
 Consider the image shown in Figure  4.7 . The range of pixel values is from 
0 to 255, but each pixel can only take on an integer value in this range. A pixel 
cannot have the value 67.2, for example. The histogram of this image is shown in 
Figure  4.8 . For each possible pixel value of 0, 1,  ...  , 255, a count of the number of 
 FIGURE 4.6   Illustrating histogram generation from raw samples. The signal (on the 
right) is analyzed and the fraction of time it spends at a certain level is plotted on the 
left - hand graph. This is then rotated on its side to become the histogram. The probability is 
this value scaled by the total number of values.  
Random signal
Probability
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.8 CONTINUOUS AND DISCRETE VARIABLES 
115
 FIGURE 4.7   A test image. This is a grayscale (luminance only) image, having 256 
levels. The size of the image is 1,024  ×  768 (width  ×  height). 
 FIGURE 4.8   The pixel histogram of the test image shown in Figure  4.7 . A grayscale bar 
is shown to indicate the relative brightness of each of the values. Thus, we see that there 
are few very dark pixels, a reasonable spread up to white, with a concentration in the 
middle gray region.  
0
32
64
96
128
160
192
224
255
0
1
2
3
4
5
6 x 104
Pixel value
Count
Image histogram
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
116 
CHAPTER 4 RANDOM SIGNALS
pixels having that value is found. The bars in the histogram represent the count 
for that pixel. Thus, the summation of all histogram bars must equal the total 
number of source samples — in this case, the number of pixels in the image, or 
1,024  ×  768  =  786,432. This is the same reason that the area under the continuous 
variable PDF is equal to unity. 
 The modal (most common) value is easily seen in the histogram. However, 
the mean cannot be seen directly. If the histogram were uniform, then the mean 
would be precisely in the middle, as would the median. In the histogram of Figure 
 4.8 , the presence of more, larger values will obviously  “ skew ” the mean away from 
the halfway point. 
 As a second example, Figure  4.9 shows a representative resting heart electro-
cardiogram (ECG) waveform. A heart monitor could be developed, which samples 
a patient ’ s heart and calculates the histogram of this parameter at certain intervals. 
An abnormal condition may be detected if the histogram deviates signiﬁ cantly from 
normal. In later chapters, it will be shown how the raw samples may be transformed 
into other parameters to give more ﬂ exibility in certain applications. 
 4.9  SIGNAL CHARACTERIZATION 
 Taking the histogram of the  difference between adjacent pixels of the test image 
yields the result shown in Figure  4.10 . It is tempting to use one of the models deﬁ ned 
earlier to approximate this PDF. The Gaussian distribution is often used to approxi-
mate noise in a signal, and the Laplacian distribution may be used as a model for 
certain image parameters (as will be seen in Section  7.13 ). Figure  4.11 shows this 
concept for values derived from sampled data. The following MATLAB code gener-
ates  N random numbers and plots their histogram: 
  
 N  = 1000; 
 x  =  randn (N, 1); 
 [n, XX]  =  hist (x, 40); 
 bar (XX, n,  ’ w ’ );  
 FIGURE 4.9   An electrocardiogram (heart waveform) sampled at 600  Hz. 
0
500
1,000
1,500
2,000
2,500
3,000
Sample number
Filtered ECG fs = 600 Hz, fourth-order low-pass filtered
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.10 HISTOGRAM OPERATORS 
117
 Some signals are in fact represented as a vector of samples: several scalar 
samples taken at the same instant or somehow related. This could represent some-
thing as simple as two adjacent pixels in an image or perhaps a signal sampled 
simultaneously with several sensors. Each sample is then a vector of values, giving 
rise to a  joint probability . This is illustrated in Figure  4.12 , where the measured 
signal density and a two - dimensional Gaussian ﬁ t are shown. The parameters of the 
Gaussian density (means and variances for each dimension) may be known in 
advance, and the goodness of ﬁ t of some measured data may be compared to the 
Gaussian model, thus determining the likelihood that the new samples come from 
the same source. 
 4.10  HISTOGRAM OPERATORS 
 We will now consider how the histogram is useful in solving two problems of 
practical interest:
 FIGURE 4.10   The image pixel difference histogram of the image shown in Figure  4.7 . 
−32
−16
−8
0
8
16
32
0
0.5
1.0
1.5
2.0 x 10
5
Pixel value
Count
Image histogram
 FIGURE 4.11   A PDF estimate — Gaussian and Laplacian distributions compared to the 
actual distribution of values  derived from the sampled data . 
−32
−24
−16
−8
0
8
16
24
32
0
0.02
0.04
0.06
0.08
0.1
Pixel difference value
Probability density
Models for image pixel difference probability density
Sampled data
Laplacian
Gaussian
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
118 
CHAPTER 4 RANDOM SIGNALS
 FIGURE 4.13   Illustrating histogram equalization, with the original image shown on the 
left. It is assumed that this image is not available. The image as presented is shown in the 
middle, and this is used as input to the histogram equalization algorithm. The output of 
the algorithm is shown on the right. 
Original as given
Whitened image
Equalized image
 FIGURE 4.12   PDF estimate in two dimensions. The measured distribution (upper) is 
modeled by a distribution equation (lower). 
X 1
X1
X 2
X2
f (X 1, X 2)
f (X 1, X 2)
 1.  Random sample generation in order to generate random numbers with a known 
distribution  
 2.  Sampled data modiﬁ cation  to compensate for errors or natural conditions when 
the signal was sampled    
 4.10.1  Histogram Equalization 
 Consider Figure  4.13 , which shows the original image, and the image which is avail-
able to use. The latter is clearly skewed toward the whiter end of the grayscale 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.10 HISTOGRAM OPERATORS 
119
spectrum; this could be caused by the sampling process to begin with, excessive 
illumination, overexposure of the image sensor, or other causes. So, if this is all we 
have to work with, can we recover the original image or at least an approximation 
to it? 
 This scenario may be understood in terms of the PDF. In Figure  4.14 , the PDF 
is shown skewed toward the black end of the luminance scale (toward zero).  
 The cumulative probability, shown below the PDF, may be used as a nonlinear 
mapping function: Pixels on the  x axis are mapped to values on the  y axis of the 
cumulative probability. Mathematically, this is a functional mapping of the form 
 y  =  f ( x ). 
 It is apparent that a small change in pixel value about the mean on the input 
will result in a large change in output luminance. The implementation of such an 
algorithm consists of two stages. The ﬁ rst pass through the data is needed to calculate 
the histogram itself. The second pass then uses the histogram as a lookup table, 
mapping each pixel of the input image in turn to a corresponding output pixel value, 
as determined by the histogram value at that pixel index. 
 The process as described ought to work for varying PDF mappings and non-
linearities. Figure  4.15 shows an image which has skews in both directions — toward 
black at one end and toward white at the other. The cumulative histogram of Figure 
 4.16 shows this. Only the middle image in Figure  4.15 is available to the algorithm, 
 FIGURE 4.14   Viewing histogram equalization as a mapping of pixels using the 
cumulative distribution. The sampled values in the image (which occur over the range 
indicated) are mapped into the much wider range of values as shown. Of course, the 
mapping may well be different for differing density functions. 
⎧
⎪⎪⎪⎪⎪⎪
⎨
⎪⎪⎪⎪⎪⎪⎩
X
X
Probability density f(X) 
Cumulative probability F(X)
Black
White
Output
Input
Black
White
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
120 
CHAPTER 4 RANDOM SIGNALS
and when the histogram equalization process is applied, the results are as shown in 
the right - hand image of the ﬁ gure. Clearly, it is able to substantially improve the 
apparent visual quality of the image. 
 4.10.2  Histogram Speciﬁ cation 
 The other problem, that of random sample generation, is illustrated in Figure  4.17 . 
Uniformly distributed random variables, which are comparatively easy to generate, 
are mapped from the  y axis of the given cumulative probability function to the  x 
 FIGURE 4.16   The distribution of pixels where lower - to mid - valued pixels are skewed 
toward black and higher - valued pixels skewed toward white. 
0
64
128
192
255
0
64
128
192
255
Output level
Input level
Gray level skewing
 FIGURE 4.15   Histogram equalization with skews in both directions. The image 
histogram is moved both toward the black end of the scale for darker pixels and toward the 
white end for lighter pixels, resulting in the middle image. Histogram equalization applied 
to this image results in the image shown on the far right. 
Original as given
Image with gray levels skewed
Equalized image
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.10 HISTOGRAM OPERATORS 
121
axis. Mathematically, this is an inverse function relationship of the form  x  =  f   − 1 ( y ). 
The result of this is a  “ concentrating ” of the output values around the area where 
the cumulative probability increases at the greatest rate. If the given mapping  F  − 1 ( X ) 
represents a cumulative Gaussian curve, the random sample output will approximate 
a Gaussian distribution. 
 To understand why this works as it does, consider that in both generation and 
equalization, we wish to convert random variables  u with a probability density  f u ( u ) 
to random variables  v with a probability density  f v ( v ). For the mapping
 
 u
v
→,  
 (4.25) 
 we require the cumulative probability of the input variable  u at some value  U to 
equal the cumulative probability of the mapped variable  v at some value  V . By 
deﬁ nition,
 
 F U
f
u du
u
u
U
(
) =
( )
∫0
 
 (4.26) 
and
 
 F V
f
v dv
v
v
V
( ) =
( )
∫0
.  
 (4.27) 
 If we want the cumulative histograms to be equal, then
 
 F U
F V
u
v
(
) =
( ).  
 (4.28) 
 Hence, for any variable mapping  u  →  v ,
 
 v
F F u
v
u
=
( )
−1
.  
 (4.29) 
 The generation of variables with a given distribution, say, Gaussian, from uniform 
variables is shown in Figure  4.18 . We assume that uniformly distributed variables 
may be generated.  u is distributed according to a uniform distribution; hence, the 
ﬁ rst stage of Equation  4.29 , or  F u ( u ), is a mapping  x  →  y . This is simply a constant 
scaling. The inverse function  Fv
−1(•) is then applied. This is a mapping from  y  →  x . 
 FIGURE 4.17   Histogram speciﬁ cation as a mathematical mapping. The input pixel 
values are mapped from that shown on the  y axis to the much smaller range on the  x axis. 
As with Figure  4.14 , the mapping may well be different for differing density functions. 
⎧
⎪⎪⎪⎪⎪
⎪
⎨
⎪⎪⎪⎪⎪⎪⎩
X
Cumulative probability F(X)
Output
Input
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
122 
CHAPTER 4 RANDOM SIGNALS
 Application of Equation  4.29 also allows changing the distribution of the data. 
The distribution may in fact be measured, and not a  “ standard ” distribution such a 
Gaussian. Figure  4.19 illustrates the process of changing from a Gaussian to a 
uniform distribution.  U is distributed according to a nonuniform distribution; hence, 
the ﬁ rst stage of Equation  4.29 , or  F u ( u ), is a mapping  x  →  y . The inverse function 
 Fv
−1(•) is then applied, which is a mapping  y  →  x . The difference between histogram 
equalization and the generation of samples from a known distribution is essentially 
just a reversal of the order of the projection functions. 
 4.11  MEDIAN FILTERS 
 As a simple example of statistical operations on samples, consider the problem 
of removing noise from an image. Figure  4.20 shows an image corrupted by so -
 called impulsive noise (also called  “ salt  ‘ n ’ pepper ” or  “ speckle ” noise). This 
might be the result of noise on a communications channel, for example, or perhaps 
the image might be one frame from a historic ﬁ lm, which has suffered from degra-
dation over time. Ideally, we would like to remove all noise and restore the 
original image. 
 A ﬁ rst thought might be to average each pixel out, with each pixel set to the 
average of its neighboring pixels. This causes the unwanted side effect of blurring 
the image, as illustrated in the ﬁ gure. Another way may be to set a threshold and to 
replace pixels exceeding this threshold with the average of the neighboring pixels. 
 FIGURE 4.18   Histogram generation: uniform to Gaussian random sample conversion. 
V
Fv(V )
V
f v(V )
Uniform to Gaussian: random sample generation
U
Fu(U)
U
f u(U)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.11 MEDIAN FILTERS 
123
 FIGURE 4.19   Histogram equalization: Gaussian (or other nonuniform) distribution to 
uniform sample conversion.  
U
Fu(U)
U
f u(U)
V
Fv(V )
V
f v(V )
Gaussian to uniform: histogram equalization
 FIGURE 4.20   Illustrating a 3  ×  3 image sub - block for the calculation of the median. 
Each square represents one pixel. The center pixel is the one currently being determined by 
the median ﬁ lter algorithm. 
pi−1,j −1
pi−1,j
pi−1,j +1
pi,j −1
pi,j
pi,j +1
pi+1 ,j −1
pi+1 ,j
pi+1 ,j +1
j
i
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
124 
CHAPTER 4 RANDOM SIGNALS
The problem is then how to set the threshold: If set too low, too many genuine image 
pixels will be altered, thus altering the image; if set too high, the noise will not be 
removed. 
 One approach is to use a type of nonlinear ﬁ lter: the  median ﬁ lter . In its sim-
plest form, this simply sets each pixel to the median of its neighboring pixels. No 
thresholding is involved. To understand this, imagine the pixels segmented into 
matrices of size 3  ×  3, as shown in Figure  4.21 . Each pixel in the middle is replaced 
by the median value of those in the deﬁ ned window. 
 As discussed previously, the median operator is simply the value of the middle 
pixel, when all pixels are ranked in order. For example, the values 7, 3, 9, 2, and 5 
ranked in order are 2, 3, 5, 7, and 9. The median is the value occurring in the middle 
of the ordered list, or 5. 
 The results of applying a 3  ×  3 median ﬁ lter are shown in Figures  4.21 and 
4.22 . Most, though not all, of the visible noise has been removed. Importantly, the 
image itself has been largely preserved, with no immediately obvious degradation 
in other areas (such as loss of detail and edges). The ﬁ gure also shows the result of 
a second pass over the image. Most of the residual impulses have been removed, 
with little degradation in the quality of the image itself. Median ﬁ lters belong to a 
broader class of nonlinear ﬁ lters termed  “ order statistic ” ﬁ lters. 
 FIGURE 4.22   The effect of multiple iterations on the median ﬁ lter. The noise - corrupted 
image is again on the far left, with the other images being the median ﬁ lter restored images 
after one and two iterations, respectively. 
Noisy image
Median iteration 1
Median iteration 2
 FIGURE 4.21   Restoration of an impulsive noise - corrupted image. The available image 
is corrupted by impulses (left). The median ﬁ lter produces the center image, which is 
markedly improved. The averaging ﬁ lter produces the right - hand image, which is 
noticeably smeared. 
Noisy image
Median filter
Averaging filter
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
4.12 CHAPTER SUMMARY 
125
 4.12  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  a review of  statistics as they apply to sampled data systems; 
 •  the concepts of statistical  parameters ,  probability distributions , and 
 histograms ; 
 •  the similarities and differences between  continuous and  discrete distributions; 
and 
 •  some applications of  median ﬁ lters  and  histogram equalization .  
 PROBLEMS 
 4.1.  For the probability density and cumulative probability example code in Section  4.6 , 
change the  rand () to  randn () and verify that the results are as expected. 
 4.2.  For the sequence of signal samples 34, 54, 56, 23, and 78, 
 (a)  calculate the mean, mean square, and variance; and 
 (b)  subtract the mean from each number, and with the new sequence, calculate the 
mean, mean square, and variance. Explain your results. 
 4.3.  For the sequence of samples { x }, write the equations deﬁ ning mean, mean square, 
and variance. Then, let a new sequence { y } be deﬁ ned as  y n
x n
x
( )
( )
=
−; that is, 
generate the mean - removed sequence.
 (a)  Derive the equations for the mean, mean square, and variance of the sequence 
{ y }. 
 (b)  What do you conclude? Does the removal of the mean from each sample in a 
sequence have the expected effect on the mean? On the variance? What about 
the mean square?  
 4.4.  For the sequence of samples { x }, write the equations deﬁ ning mean, mean square, 
and variance. Then, let a new sequence { y } be deﬁ ned as  y ( n )  =  α x ( n ), that is, each 
sample multiplied by a real constant  α (positive or negative).
 (a)  Derive the equations for the mean, mean square, and variance of the sequence 
{ y }. 
 (b)   What do you conclude? Are there any values of  α for which this conclusion 
would not hold? 
 4.5.  Repeat the above two problems using MATLAB, generating a sequence of random 
numbers for { x }. Try both  rand () and  randn () to generate the sequences. 
 4.6.  The mean, mean square, and variance are all interrelated.
 (a)  For a sampled sequence  x ( n ), prove that  σ2
2
2
=
−
x
x . 
 (b)  How could the above result be used to simplify the computation of the variance 
of a set of sampled data? 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
126 
CHAPTER 4 RANDOM SIGNALS
 4.7.  The probability density and cumulative density were discussed in Section  4.6 . The 
MATLAB code given in that section ﬁ nds the probability by taking a sample value 
 X and by determining the number of signal samples below  X or within the range 
 X  +  δ X .
 (a)  Explain why this is not a particularly efﬁ cient approach, in terms of the number 
of passes that would have to be made through the data. For  N input samples 
and  M points in the probability curve, roughly how many passes would be 
required to determine all  M output points? 
 (b)  If we produce a running count of the  M output data points, would we be able 
to use only one pass through the  N data points? 
 4.8.  Construct in MATLAB a histogram function myhist() for integer - valued data 
samples. Check that it works correctly by using randomly generated test data. Check 
your m - ﬁ le against the output of MATLAB ’ s  hist () function. 
 4.9.  Load an image in MATLAB (see Chapter 2 for details on how to do this). Convert 
the image to grayscale (luminance) values. Plot the histogram and cumulative histo-
gram. Check that you have the correct results by ensuring that the ﬁ nal value in the 
cumulative histogram equals the number of pixels in the image. 
 4.10.  Sketch the histogram and cumulative histogram as shown in Figure  4.14 for the cases 
were the image is too light, too dark, and ﬁ nally where the image has peaks at both 
the light and dark ends of the spectrum. Explain how histogram equalization works 
in each case, with reference to the cumulative histogram.  
 4.11.  The lower right - hand plot in Figure  4.5 includes the plot of a uniform distribution. 
For  x = 0,  σ  =  1, ﬁ nd the signal range and the constant scaling value in the PDF 
equation. 
 4.12.  Verify that the area under a Laplacian distribution is equal to one. 
 4.13.  The pixels in a 3  ×  3 image sub - block are as follows:
 3 
 4 
 2 
 9 
 ? 
 1 
 3 
 7 
 3 
 (a)  Estimate the unknown pixel in the center using both a  mean and a  median 
estimator. 
 (b)  Repeat if the pixel of value 1 is changed to 25. Are both the mean and median 
affected? Why or why not? 
 4.14.  A Rayleigh distribution has a cumulative distribution given by  F x
e x
( ) = −
−
1
2
2
2
/ σ  for 
 x  ≥  0.
 (a)  From  F ( x ), verify by differentiation that  f x
x e x
( ) =
−
σ
σ
2
2
2
2
/
 for  x  ≥  0. 
 (b)  Starting with  f ( x ), verify by integration that  F ( x ) is valid. 
 (c)  Sketch both  f ( x ) and  F ( x ). 
 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER  5 
REPRESENTING SIGNALS 
AND SYSTEMS 
 5.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to:
 1.  explain how sampled signals are  generated , and be able to generate  sampled 
waveforms using an equation. 
 2.  explain the relationship between continuous and discrete  frequency . 
 3.  derive the   z transform of a signal. 
 4.  convert from a  z transform to a  difference equation . 
 5.  determine the  stability of a given system. 
 6.  explain the relationship between a  pole – zero plot and a system ’ s  frequency 
response . 
 7.  explain what is meant by  convolution , and how to calculate a system ’ s response 
using convolution.     
 5.2  INTRODUCTION 
 We now turn our attention from analyzing a given sampled signal to synthesizing 
(generating) a sampled signal. Many applications depend on the ability to generate 
a signal: One important application area is to generate signals for communication 
systems. Once the theory for generating signals is introduced, it will be possible to 
analyze systems which alter a signal as it passes through a system. These two 
operations — analysis and synthesis — may be viewed as the inverse of each other, 
and much of the same theory applies equally well to either. 
 5.3  DISCRETE - TIME WAVEFORM GENERATION 
 Consider a sine wave as a point on a circle, sampled at discrete angular increments 
as shown in Figure  5.1 . The path traced out on the horizontal axis is sinusoidal. As 
127
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
128 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
will be seen in Chapter  7 , a fundamental principle of signal processing called 
 “ Fourier ’ s theorem ” states that periodic signals may be composed from a summation 
of sinusoids, of varying amplitude, phase and frequency. 
 Thus, the sine wave is of vital importance in signal processing. Firstly, we 
wish to generate a sine wave — or, more precisely, to generate the samples that con-
stitute a sampled sine wave. 
 5.3.1  Direct Calculation Approach 
 The obvious approach to calculating the sample points for a sine wave is simply 
direct calculation. A sinusoidal waveform is mathematically described as:
 
 x t
A
t
( )
sin(
),
=
+
Ω
ϕ  
 (5.1) 
where  A is the peak amplitude of the waveform (i.e., a peak - to - peak value of 2 A ), 
 Ω is the frequency in radians per second,  t is time, and  φ is a phase shift (which 
determines the starting point in the cycle). 
 If we sample at a time  t  =  nT , where  n is the sample number, we generate the 
sequence of samples  x ( n ) at time instants  nT :
 
 x n
A
n T
( )
sin(
).
=
+
Ω
ϕ  
 (5.2) 
 The direct calculation of this requires the evaluation of a trigonometric function, 
sin (). Deﬁ ning:
 
 ω = ΩT,  
 (5.3) 
 we have,
 
 x n
A
n
( )
sin(
).
=
+
ω
ϕ  
 (5.4) 
 From this, we can interpret  ω as the angle swept out over each sample. So we now 
have the terms 
 
 FIGURE 5.1   A sinusoidal waveform viewed as sampled points on a rotating circle. The 
amplitude corresponds to the radius of the circle, the phase corresponds to the starting 
point, the frequency is the rotational speed, and the sample rate of the waveform 
corresponds to the angle swept out on the circle between samples.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.3 DISCRETE-TIME WAVEFORM GENERATION 
129
 “ Real ” time  t is related to  “ sample time ” (or sample number)  n by: 
  
 Ω = 2π f  
 radians
second
radians
cycle
cycles
second
=
×
.  
 ω = ΩT  
 radians
sample
radians
second
seconds
sample
=
×
.  
 t
nT
=
 
 seconds
sample number
seconds
sample
=
×
.  
 T 
 sampling period 
 ω 
 “ digital ” frequency in radians per sample 
 Ω 
 “ real ” frequency in radians per second 
 “ Real ” frequency  Ω radians per second is related to  “ sample - scaled frequency ” 
 ω radians per sample by 
  
 As always, radian frequency and Hertz (Hz) frequency are related by 
  
 From the equation  x ( n )  =  A sin  n ω , we can see that the relative frequency  ω 
is the phase angle swept out per sample. An example serves to demonstrate a wave-
form generated in this manner, with playback as an audio signal. We generate two 
signals: The ﬁ rst using the  “ analog ” approach, where the waveform is  y a  =  sin  2 π ft , 
and time  t stepped in small increments. The argument  f is the frequency of the tone 
to generate (in this case, 400  Hz). Next, the  “ discrete ” approach uses  y d  =  sin  n ω , 
where  n is the sample instant (an integer quantity), and  ω is the  “ discrete ” frequency 
in radians per sample, as explained above. 1 
  
  1  Note carefully the difference between  Ω (capital Omega) and  ω (lower - case omega). 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
130 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 fs  = 8,000; 
 T  = 1/fs; 
 tmax  = 2; 
 t  = 0:T:tmax; 
 f  = 400; 
 Omega  = 2 * pi * f; 
 ya  =  sin (Omega * t); 
 sound (ya, fs); 
 pause (tmax); 
 n  = 0: round (fs * tmax); 
 omega  = Omega * T; 
 yd  =  sin (n * omega); 
 sound (yd, fs);  
  2  Strictly speaking, we  could generate nonperiodic signals in this way, but the signal would repeat itself 
at intervals corresponding to the table length. 
 Note well the fundamental difference here: The  “ analog ” approach uses small 
time increments  δ t , whereas the  “ discrete ” approach uses sample index  n . The cal-
culation of the trigonometric function (sine) is computationally  “ expensive ” — mean-
ing that the calculation may take a long time to perform, compared with the sample 
period. If the sample period  T is small enough, a processor may have difﬁ culty 
keeping up. So, it is worthwhile to take a closer look at how the samples may be 
generated in practice.  
 5.3.2  Lookup Table Approach 
 An obvious simpliﬁ cation is to precompute the sinusoidal samples and store them 
in a  lookup table (LUT). This works because the waveform is periodic, with only a 
ﬁ nite number of samples for each period of the waveform. If the waveform were 
not periodic, this approach would not be feasible. 2 The operation of the lookup table 
is shown diagrammatically in Figure  5.2 . 
 Once the values are precomputed and stored in the table, the index (or pointer) 
is simply stepped through the table. Each output sample sent to the D/A converter 
is the current entry in the table. When the pointer reaches the end of the table, it is 
simply reset to the start of the table again. 
 Amplitude scaling can be effected by a simple multiply operation on each 
sample as it is retrieved from the table. The frequency may be changed by stepping 
through not one, but several samples at a time. Finally, phase changes are effected 
by an abrupt change in the LUT index. To illustrate a phase change, suppose there 
were 256 samples in the table. The entire table represents 2 π radians, or 360 degrees. 
A phase advance of 90 o implies adding  256
90 360
64
×
(
) =
 samples to the current 
position. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.3 DISCRETE-TIME WAVEFORM GENERATION 
131
 With a lookup table, the resolution or granularity of samples (and phase 
changes) is governed by the number of samples stored in the table. For example, a 
phase change of  π 3 requires a noninteger increment of 42.67 samples in a table of 
256 samples. The effective number of samples may be increased by mathematical 
interpolation between known samples (as shown in Section  3.9.2 ), although this 
obviously adds to the complexity. 
 This design is useful in applications such as signal generation for communica-
tions systems. An extension of this concept, called direct digital synthesis (DDS), is 
often used in communication systems. The approach used in DDS extends the 
method described, but rather than having a single step increment for each sample, 
a phase register adds a ﬁ xed offset to the table address according to the desired 
frequency so as to step through the table at the required rate (and thus generate the 
desired frequency). An address accumulator has more bits than is required for the 
LUT index, and only the higher - order bits of this accumulator are used to address 
the LUT. This enables ﬁ ner control over the step size, and hence frequency 
generated. 
 5.3.3  Recurrence Relations 
 Another approach is to use the properties of trigonometric functions. The idea is to 
increment from the currently known position to the value of the sampled sinusoid 
at the next sample position. Applying the trigonometric expansions for sin ( x  +  y ) 
and cos ( x  +  y ), we have:
 
 cos(
)
cos
cos
sin
sin
n
n
n
ω
ω
ω
ω
ω
ω
+
=
−
 
 (5.5) 
 
 cos(
)
cos
cos
sin
sin .
n
n
n
ω
ω
ω
ω
ω
ω
+
=
−
 
 (5.6) 
 FIGURE 5.2   A lookup table for the generation of a sinusoidal waveform. The index 
pointer is incremented on every sample, to point to the next value in the table. After the 
pointer reaches the last sample in the table, it is reset back to the start of the table.  
Modulo-index pointer
Lookup table
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
132 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 We can set  ω to be a small step size, and let  n ω represent the current angle. Thus, 
the problem becomes one of progressing from the current sample value of sin   n ω to 
the next sample, sin  ( n ω  +  ω ). The current angle  n ω is varying, but the step size  ω 
is ﬁ xed. Therefore, the values of sin ω and cos ω may be precomputed, as they are 
likewise ﬁ xed. 
 Using both of the equations above, sin n ω and cos n ω are known, and the values 
of sin  ( n ω  +  ω ) and cos  ( n ω  +  ω ) are easily computed using only multiplication and 
addition operations — no  “ expensive ” (in computational terms) sine/cosine opera-
tions are required at each iteration. Note also that both sine and cosine waveforms 
(sometimes called  “ quadrature ” waveforms) are inherently provided by the algo-
rithm, as shown in Figure  5.3 . 
 The following MATLAB code illustrates the implementation of a waveform 
generator using these recurrence relations. Note that, as explained above, there are 
no time - consuming trigonometric calculations (sin  [] and cos  []) within the main 
sample - generating loop. 
  
  N  = 20; 
 w  = 2 * pi /N; 
 % running cos(w) and sin(w) values 
 cosw  =  cos (w); 
 sinw  =  sin (w); 
 % initial values 
 theta  = 0; 
 FIGURE 5.3   Quadrature sine and cosine waveforms, generated using recurrence 
equations. 
Sample number
Waveforms generated using recurrence relations
–1.0
–1.0
–0.5
–0.5
0.5
0.5
0
0
0
0
1.0
1.0
2
2
4
4
6
6
8
8
10
10
12
12
14
14
16
16
18
18
20
20
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.3 DISCRETE-TIME WAVEFORM GENERATION 
133
 5.3.4  Buffering, Timing, and Phase Errors 
 Once an appropriate method of generating the wave samples has been decided upon, 
it is necessary to think about how the samples are to be used in practice. If we are 
developing an audio or communications system, for example, the actual output of 
the sample values to the D/A hardware will likely be done by a separate digital signal 
processor chip. The reason for this is that the actual output of individual samples is 
usually done by interrupting the processor, which must then execute a routine to 
fetch the next sample from memory and send it to the hardware to generate the 
analog representation. If the sample rate is reasonably high, this constitutes a sig-
niﬁ cant burden on the host CPU. For this reason, the underlying DSP processor 
subsystem does not usually deal with individual samples, but rather blocks of 
samples. The size of the block is necessarily a trade - off between lower processor 
requirements (using a larger block) and the amount of delay which can be tolerated 
(requiring a smaller block). It is the responsibility of the operating system and 
application using the signal to arrange for a block to be generated. There are two 
practical problems which arise in this scenario: The phase of the generated wave-
form, and the timing of gaps in the blocks. We look at each in turn. 
 cosnw  = 1; 
 sinnw  = 0; 
 % arrays to store resulting sine/cosine values 
 %(not needed in practice, only to show the 
 % computed waveform values in this example) 
 sinvals  = []; 
 cosvals  = []; 
 % note that no trig functions are used in this 
 % loop — only multiply and add/subtract 
 for n  = 0:N − 1 
      % save values in array 
      sinvals  = [sinvals sinnw]; 
      cosvals  = [cosvals cosnw]; 
      % recurrence relations for new values of 
      % sin(nw  +  w) and cos(nw  +  w) 
      newcos  = cosnw * cosw  − sinnw * sinw; 
      newsin  = sinnw * cosw  + cosnw * sinw; 
      % new values become current ones 
      cosnw  = newcos; 
      sinnw  = newsin; 
 end 
 ﬁ gure (1); 
 stem (sinvals); 
 ﬁ gure (2); 
 stem (cosvals);  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
134 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 Suppose we have a block of output samples calculated and ready to transfer 
to the output device. We do this, and then start work on the next block. A problem 
arises if we start the next buffer using zero initial conditions. The blocks should 
seamlessly overlap, since the blocking is done only as a convenience to the hardware 
and software, and should not be visible in the output waveform. Figure  5.4 shows 
the situation which may arise. The middle of the upper plot is aligned with the block 
boundary. The waveform from block  m may terminate at any amplitude, according 
to the frequency of the wave and the number of samples. As illustrated, it appears 
that the waveform abruptly ends at a particular phase. Block ( m  +  1) then starts at 
zero phase. 
 What does this mean in practice? Consider an audio system which is required 
to generate a waveform. Some typical ﬁ gures will help ﬁ x the idea. Suppose the 
sampling rate is 8  kHz, and we wish to generate a 250  Hz sinewave. Suppose the 
block size is 400 samples. This equates to 20 blocks per second. If we do not take 
into account the block boundaries, the situation depicted in Figure  5.5 arises. The 
discontinuities at the end of each block manifest themselves as an additional com-
ponent at a rate equal to the block rate — in this case, 20  Hz. In audio systems, the 
noise thus introduced can be particularly objectionable at this frequency. 
 FIGURE 5.4   Illustrating the problem of matching the end of one block with the start of 
the next. Here, the block boundary occurs at index 100. In the upper plot, the subsequent 
block starts generating the sinusoid anew. The lower block starts the waveform using a 
phase which is adjusted so as to make the transition seamless. This phase is  φ  =  θ  +  ω , 
where  θ is the ending phase of the previous block, and  ω is the phase increment per 
sample. 
0
20
40
60
80
100
120
140
160
180
200
−2
−1
0
1
2
No phase correction
0
20
40
60
80
100
120
140
160
180
200
−2
−1
0
1
2
With phase correction
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.3 DISCRETE-TIME WAVEFORM GENERATION 
135
 The following shows how we initialize the system in order to generate such a 
waveform. The block buffer is Bbuf, and this is the buffer which is prepared by the 
main CPU and passed to the DSP subsystem. The buffer Sbuf contains all the sound 
samples. In practice, this would not be required, but since we are generating the 
entire waveform for subsequent playback, it is necessary for this demonstration. In 
this case, it stores 4 seconds of audio. 
  
 FIGURE 5.5   Showing multiple blocks and the discontinuity that can result at block 
boundaries. If this waveform were an audio waveform, the relative frequency of the blocks 
as compared with the waveform itself would mean that objectionable audible artifacts 
would be present.  
0
200
400
600
800
1,000
1,200
1,400
1,600
−2
−1
0
1
2
Output waveform with no phase correction
Sample index (block size = 400 samples)
 f  = 250;     % frequency in Hz 
 fs  = 8000;    % sampling frequency 
 B  = 20;      % blocks/sec 
 L  = 400;     % samples per block 
 D  = 4;     % duration in seconds 
 % block buffer and output buffer 
 Bbuf  =  zeros (L, 1); 
 Sbuf  =  zeros (fs * D, 1);  %  = B * L * D 
 Omega  = 2 * pi * f; 
 T  = 1/fs; 
 omega  = Omega * T; 
 NBlocks  = B * D;  
 The following shows how we generate the samples for each block of audio. 
Note that the phase increment per sample is  ω radians per sample as per earlier 
theory, and in the code is denoted as omega. At the end of each individual block 
calculation, we copy the data into the ﬁ nal output buffer. In a real - time system, this 
step would not be necessary, and would be replaced with a system call to transfer 
the data block of  L samples to the output DSP subsystem. 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
136 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 Now we address the problem of aligning the waveform, so that the transition 
between buffers is seamless. There are several ways to accomplish this, and the 
method below simply uses a phase start  φ (phi) in the per - sample calculation. The 
phase angle at the end of the block is then stored for the start of the next block. 
 Since this phase would continue to increment, at some point it might overﬂ ow. 
But because any phase over 2 π can be mapped back into the range 0 – 2 π , we subtract 
the overﬂ ow as shown in the loop. 
  
 pSbuf  = 1; 
 phi  = 0; 
 for   b  = 1:NBlocks 
       % generate one block worth of samples, correct phase
       % offset 
       for   n  = 1:L 
             theta  = (n  − 1) * omega  + phi; 
             x  =  sin (theta); 
             Bbufpc(n)  = x; 
       end 
       % phase for next block 
       phi  = theta  + omega;  % 1 * omega at next sample 
       % correct if over 2pi 
       while ( phi  > 2 * pi ) 
             phi  = phi  − 2 * pi ; 
       end 
       % save block in output buffer  
       Sbufpc(pSbuf:pSbuf  + L  − 1)  = Bbufpc; 
       % point to start of next output block 
       pSbuf  = pSbuf  + L; 
 end 
 pSbuf  = 1; 
 for   b  = 1:NBlocks 
       % generate one block worth of samples, phase
       % independent 
       for n  = 1:L 
            theta  = (n  − 1) * omega; 
            x  =  sin (theta); 
            Bbuf(n)  = x; 
       end 
        % save the block in the output buffer 
       Sbuf (pSbuf:pSbuf  + L  − 1)  = Bbuf; 
       % point to start of next output block 
       pSbuf  = pSbuf  + L; 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.4 THE Z TRANSFORM 
137
 The waveforms can be played back using  sound (Sbuf, fs) and  sound 
(Sbufpc, fs) for the phase - corrected approach. The resulting audio will exhibit 
an audible artifact at the block rate in the former case (in this case 20  Hz), which is 
not present in the latter case. 
 Another issue, which is caused by different factors but produces a similar type 
of waveform impairment, is the processing speed itself. In the preceding examples, 
we have calculated each buffer separately, and in practice would then pass the 
memory address of this buffer to the DSP subsystem. If we use the same block of 
memory, it would be necessary to wait until the output has been completed before 
preparing the new samples. This time would equate to  LT , or 50 milliseconds, using 
the current example. So not only would this delay be mandatory, there would be a 
delay while the main processor calculated the  L new samples, which may not be 
insigniﬁ cant. This would again result in audible artifacts, since there would be a 
delay while the output buffer is prepared. 
 The solution to this problem is to use a technique termed  double - buffering . In 
this approach, two buffers are used in an alternating fashion. While buffer  A is being 
prepared, the samples in buffer  B are output to the D/A converter. Once all samples 
in buffer  B have been used, buffer  A ought to be ready. All that is necessary is to 
pass the memory address of  A to the output system, and start preparing buffer  B 
(since it is no longer in use). Subsequently, the role of each buffer is again reversed: 
Buffer  B is output while  A is being updated. This ensures seamless transitions 
between buffers, and eliminates gaps between blocks.  
 5.4  THE  z TRANSFORM 
 Another approach to waveform generation is to use a difference equation (as intro-
duced in Section  3.10 ) to generate the output samples based on the present and past 
inputs, and possibly the past outputs. 
 This technique goes far beyond simple waveform generation; not only can we 
generate systems with an arbitrary response, but a similar approach may be used to 
analyze the behavior of systems in terms of time response and frequency response. 
 The principal tools required are the closely related  difference equation and the 
 z transform . Difference equations were encountered in Chapter  3 . To recap, a general 
difference equation may be represented as:
 
 
y n
b x n
b x n
b x n
N
a y n
a y n
a y
N
M
( )
( )
(
)
(
)
(
)
(
)
(
=
+
−
+
+
−
(
)
−
−
+
−
+
+
0
1
1
2
1
1
2


n
M
−
(
)) .  
 (5.7) 
 The coefﬁ cients  b k and  a k are constant coefﬁ cients, and determine the response 
of the system to an impulse or other input.  N and  M govern the number of past 
inputs and outputs respectively, which are used in the computation of the current 
output  y ( n ). 
 The closely related  z transform is a very important mathematical operator 
when dealing with discrete systems. We start with a time function  f ( t ), which is 
sampled at  t  =  nT . By deﬁ nition, the  z transform of  f ( t ) is denoted by  F ( z ), and 
calculated using the following formula:
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
138 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 
 
F z
f
z
f
z
f
z
f n z n
n
( )
( )
( )
( )
( )
.
=
+
+
+
=
−
−
−
=
∞
∑
0
1
2
0
1
2
0
…
 
 (5.8) 
 The time samples  f ( n ) are understood to mean  f ( nT ), with the sample period  T being 
implicit in the fact that it is a sampled - data system. It may seem strange to have an 
inﬁ nite series and negative powers of  z used. However, as will be shown in the fol-
lowing sections, the ideas embodied in this equation are fundamental to signal 
processing theory. 
 5.4.1  z  − 1 as a Delay Operator 
 Equation  5.8 has an interesting interpretation in the time domain. Using Figure  5.6 
as a guide, let  f ( n ) delayed by one sample be  f n
( ). Thus,  f n
f n
( )
(
)
=
−1 . 
 By deﬁ nition,
 
 F z
f
z
f
z
f
z
( )
( )
( )
( )
=
+
+
+
−
−
0
1
2
0
1
2
.  
 (5.9) 
 So,
 
 



…
F z
f
z
f
z
f
z
( )
( )
( )
( )
=
+
+
+
−
−
0
1
2
0
1
2
 
 (5.10) 
 
 
=
+
+
+
=
−
+
+
−
−
−
−



…
f
z
z
f
z
f
z
f
z
z
f
z
f
z
( )
( ( )
( )
)
(
)
( ( )
( )
0
1
2
1
0
1
0
1
0
1
0
1
0
11
0
1
1
+
=
−
+
−
…)
(
)
( ).
f
z
z F z
 
 (5.11) 
 FIGURE 5.6   Viewing multiplication by  z  − 1 as a delay of one sample. The upper diagram 
shows the original sequence  f ( n ); the lower shows  f ( n ) delayed by one sample. 
t
f(0)
f(1)
~
~
~
~
~
f(2)
f(3)
f(4)
t
f(−1)
Viewpoint
Viewpoint
f(0)
f(1)
f(2)
f(3)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.4 THE Z TRANSFORM 
139
 If we assume zero initial conditions,  f ( t )  =  0 for  t  <  0 and  f ( − 1)  =  0, and in that case, 
the above simpliﬁ es to:
 
 F z
z F z
( )
( ).
=
−1
 
 (5.12) 
 In words, this conclusion may be expressed as: 
 
 FIGURE 5.7   An input – output system model using  z transforms. If the sampled input  x ( n ) 
is transformed into  X ( z ), then the output  Y ( z )  =  X ( z )  H ( z ). From  Y ( z ), we can recover the 
samples  y ( n ). 
H(z)
x(nT )
X(z)
y(nT )
Y(z)
 The output of a linear, time - invarant system is equal to the input  X ( z )  multiplied by the 
transfer function  H ( z ). 
 Multiplying by  z  − 1 corresponds to a  delay of one sample. 
 In a similar way, it may be shown that multiplying by  z  −  k corresponds to a 
delay of  k samples. Note that the term  “ delay ” implies we are only referencing  past 
samples — multiplying by  z  +  k corresponds to  k samples in the  future , a situation which 
is not normally encountered. Also,  z 0 means a delay of 0 samples: Since  z 0  =  1, the 
notation is consistent, as multiplying by 1 is expected to leave things unchanged, 
since  F ( z ) · z 0  =  F ( z ) · 1  =  F ( z ). 
 5.4.2  z  Transforms and Transfer Functions 
 The  z transform is useful because it allows us to write a  transfer function for a given 
system. The transfer function is described in terms of the  z equations, and it is iter-
ated in a computer algorithm using the corresponding difference equation. The 
model may be depicted as in Figure  5.7 . The transfer function is multiplicative:
 
 Y z
X z H z
( )
( )
( ),
=
 
 (5.13) 
 or, in words:
 
 Or, equivalently,
 
 Linear time-invarant system
,
( )
( )
( ).
H z
Y z
X z
=
 
 (5.14) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
140 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 or, in words: 
  
 a  = 2; 
 N  = 20; 
 T  = 0.1; 
 %  “ continuous ” time 
 tc  = 0:0.01:N * T; 
 yc  =  exp ( − a * tc); 
 % samples at nT 
 n  = 0:N − 1; 
 y  =  exp ( − a * n * T); 
 plot (tc, yc,  ‘ − ’ , n * T, y,  ‘ o ’ ); 
 legend ( ‘ Continuous system ’ ,  ‘ Samples ’ ); 
 Transfer function = output
input .  
 Note that the time - domain functions are  not able to be expressed as a simple 
multiplication like the  z transfer functions:
 
 y n
h n x n
( )
( ) ( ).
≠
 
 (5.15) 
 The output of a linear system may also be computed using the technique of  convolu-
tion , which will be explained further in Section  5.9 . 
 5.4.3  Difference Equation and  z  Transform Examples 
 To introduce the transfer - function analysis by way of example, consider a decaying 
exponential function. The time function is  f ( t )  =  e  −  at , and therefore the sampled 
version is  f ( nT )  =  e  −  anT . The function and the samples, shown in Figure  5.8 , are 
generated using the following code: 
  
 FIGURE 5.8   An exponential decay curve and its sampled version. 
−0.5
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1.0
1.2
Time
Amplitude
Sampled exponential for t ≥ 0
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.4 THE Z TRANSFORM 
141
 Applying the deﬁ nition of a  z transform to this function we get:
 
F z
f
z
f
z
f
z
Z e
e z
e
z
e
z
at
aT
aT
( )
( )
( )
( )
{
}
=
+
+
+
∴
=
+
+
−
−
−
−
−
−
−
0
1
2
0
1
2
0
0
1
2

2
1
1
2
2
1
1
1
1
+
= +
+
+
+
= +
−
−
−
−
−
−
−
−


e
z
e
z
e
z
e
z F z
aT
aT
aT
aT
(
)
( )
 
 
F z
e
z
F z
e
z
aT
aT
( )(
)
( )
.
1
1
1
1
1
1
−
=
=
−
−
−
−
−
 
 Note that the  z transform is expressed as  negative powers of  z , because we can only 
access present or past samples. Multiplying by  z
z
, we can say that the  z transform 
of  e  −  at is:
 
 Z e
z
z
e
at
aT
{
}
.
−
−
=
−
 
 (5.16) 
 Now, we need to convert this  z transform to a difference equation. Using
 
 H z
Y z
X z
( )
( )
( ) ,
=
 
 (5.17) 
 we have:
 
 Y z
X z
e
z
aT
( )
( ) =
−
−
−
1
1
1  
 (5.18) 
 Cross - multiplying
 
 Y z
e
z
X z
aT
( )(
)
( ).
1
1
−
=
−
−
 
 (5.19) 
 We want the output  Y ( z ), so:
 
 Y z
X z
e
z Y z
aT
( )
( )
( ).
=
+
−
−1
 
 (5.20) 
 Converting to a difference equation is done by using the delay operator property 
of  z :
 
 y n
x n
e
y n
aT
( )
( )
(
).
=
+
−
−
1  
 (5.21) 
 Note how the delay - operator property of  z has been used to transform from a 
transfer - function representation in  z into a difference equation in terms of sample  n . 
A  z  − 1 in the  z equation becomes a one - sample delay in the time (difference) 
equation:
 
Y z
X z
e
z Y z
y n
x n
e
y n
aT
aT
( )
( )
( )
( )
( )
(
)
.
=
+
=
+
−
−
−
−
1
1



 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
142 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 The  impulse response of this system is obtained by setting  x ( n )  =  1 for  n  =  0, and 
 x ( n )  =  0 elsewhere. Table  5.1 shows the calculation steps involved. It is important 
to realize that the  y ( n ) outputs are in fact identical to samples of  e  −  at sampled at 
 t  =  nT . 
 This difference equation may be iterated to produce the sample values. This 
is coded in MATLAB as follows: 
  
 N  = 20; 
 T  = 0.1; 
 a  = 2; 
 x  =  zeros (N, 1); 
 y  =  zeros (N, 1); 
 x(1)  = 1; 
 for n  = 1:N 
      y (n)  = x (n); 
      if ( n  > 1 ) 
           y(n)  = y(n)  +  exp ( − a * T) * y(n  − 1); 
      end 
 end 
 stem (y); 
 TABLE 5.1   Calculating the Impulse Response for Sample Period  T  =  0.1 and Parameter  a  =  2 
 Sample number 
 Actual time 
 Impulse 
 y ( n )  =  x ( n )  +  e  −  aT y ( n  −  1) 
 n 
 t 
 x ( n ) 
 0 
 0 
 1 
 1.00 
 1 
 0.10 
 0 
 0.82 
 2 
 0.20 
 0 
 0.67 
 3 
 0.30 
 0 
 0.55 
 4 
 0.40 
 0 
 0.45 
 5 
 0.50 
 0 
 0.37 
 6 
 0.60 
 0 
 0.30 
 7 
 0.70 
 0 
 0.25 
 8 
 0.80 
 0 
 0.20 
 9 
 0.90 
 0 
 0.17 
 10 
 1.00 
 0 
 0.14 
  
  
  
  
 18 
 1.80 
 0 
 0.027 
 19 
 1.90 
 0 
 0.022 
 Note that  y ( n )  =  0 for  n  <  0, since we assume zero initial conditions. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.4 THE Z TRANSFORM 
143
 A useful waveform in practice is that of a sine wave; it is often used as a 
test signal to measure the response of systems. For continuous time  t , a sine wave 
is just:
 
 f t
A
t
( )
sin
.
=
Ω  
 (5.22) 
 The  z  transform of this will now be derived; the resulting difference equation will 
yield another method of generating sinusoidal waveforms. 
 The derivation from ﬁ rst principles can in fact be based on the previous result 
for  Z ( e  −  at ). Using the exponential of a complex number 3 :
 
e
e
j
j
j
j
θ
θ
θ
θ
θ
θ
=
+
∴
=
−
−
cos
sin
cos
sin .
 
 The sine (and cosine) functions may be expressed by subtracting or adding as 
required:
 e
e
e
e
j
j
j
j
j
θ
θ
θ
θ
θ
θ
−
=
+
=
−
−
2
2
sin
cos .
 
 Thus, sin  () may be expressed as:
 
 sin
(
).
θ
θ
θ
=
−
−
1
2j
j
j
e
e
 
 (5.23) 
 So,
 
 sin
(
).
Ω
Ω
Ω
t
e
e
t
t
=
−
−
1
2j
j
j
 
 (5.24) 
 Using the result of Equation  5.16 for the  z transform of  e  −  at , this becomes:
 
Z
t
z
z
e
z
z
e
z
z
e
z
e
T
T
T
T
{sin
}
(
)
(
)
Ω
Ω
Ω
Ω
Ω
=
−
−
−
⎛
⎝⎜
⎞
⎠⎟
=
−
−
−
+
−
−
+
1
2
2
j
j
j
j
j
j
(
)(
)
(
)
z
e
z
e
z
e
e
z
z e
e
T
T
T
T
T
T
−
−
⎛
⎝⎜
⎞
⎠⎟
=
−
−
+
+
+
−
+
−
+
−
j
j
j
j
j
j
j
Ω
Ω
Ω
Ω
Ω
Ω
2
2
1
2
2
2
1
2
⎛
⎝⎜
⎞
⎠⎟
=
−
+
⎛
⎝⎜
⎞
⎠⎟
z
T
z
z
T
j
jsin
cos
.
Ω
Ω
 
 Now  ω  =  Ω T as seen before, hence:
 
 Z
t
z
z
z
{sin
}
sin
cos
.
Ω
=
−
+
ω
ω
2
2
1
 
 (5.25) 
  3  Also known as Euler ’ s formula, after the mathematician Leonhard Euler. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
144 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 The difference equation follows from the input/output model:
 
 Y z
X z
z
z
z
( )
( )
sin
cos
.
=
−
+
ω
ω
2
2
1  
 (5.26) 
 Since we require negative powers of  z for causality, 4 we multiply the right - hand side 
by  z
z
−
−
2
2  to obtain:
 
 Y z
X z
z
z
z
( )
( )
sin
cos
.
=
−
+
−
−
−
1
1
2
1
2
ω
ω
 
 (5.27) 
 Cross - multiplying the terms on each side, we have:
 
 Y z
z
z
X z z
Y z
X z z
Y z
z
( )(
cos
)
( )
sin
( )
( )
sin
( )(
1
2
2
1
2
1
1
−
+
=
∴
=
+
−
−
−
−
−
ω
ω
ω
1
2
cos
).
ω−
−z
 
 (5.28) 
 Hence,
 
 y n
x n
y n
y n
( )
sin
(
)
cos
(
)
(
).
=
−
+
−
−
−
ω
ω
 
 
1
2
1
2  
 (5.29) 
 In terms of a difference equation, this is:
 
 y n
b x n
b x n
a y n
a y n
( )
(
( )
(
))
(
(
)
(
)),
=
+
−
−
−
+
−
0
1
1
2
1
1
2
 
 (5.30) 
with coefﬁ cients
 b 0   =  0 
 b 1   =  sin  ω 
 a 0   =  1 (as always) 
 a 1   =  − 2 cos  ω 
 a 2   =  1. 
 To produce a sine wave, we simply need to iterate the difference equation with a 
discrete impulse as input, as shown below. We can use the  ﬁ lter () function to do 
this, as introduced in Section  3.10 . 
 5.5  POLYNOMIAL APPROACH 
 The previous section showed how to iterate a difference equation in order to deter-
mine the output sequence. It is particularly important to understand the relationship 
between difference equations and their transforms. The  z transform of a linear system 
gives us the key to combining systems together to form more complex systems, since 
the  z transforms in combined blocks are able to be multiplied or added together as 
necessary. We now give another insight into this approach. 
 Suppose we have a difference equation:
 
 y n
x n
x n
x n
x n
( )
.
( )
.
(
)
.
(
)
.
(
).
=
+
−
−
−
+
−
0 9
0 8
1
0 4
2
0 1
3  
 (5.31) 
  4   That is, the input causes the output, and we have no references to future samples. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.5 POLYNOMIAL APPROACH 
145
 If we iterate this for a step input, we have: 
 n 
 x ( n ) 
 y ( n ) 
 Sample Instant 
 Input 
 Output 
 0 
 1 
 0.9 
 1 
 1 
 0.9  +  0.8  =  1.7 
 2 
 1 
 0.9  +  0.8  −  0.4  =  1.3 
 3 
 1 
 0.9  +  0.8  −  0.4  +  0.1  =  1.4 
 4 
 1 
 0.9  +  0.8  −  0.4  +  0.1  =  1.4 
 5 
 1 
 0.9  +  0.8  −  0.4  +  0.1  =  1.4 
 6 
 1 
 . . . 
 Now let us take a different approach. The corresponding  z transform is:
 
 H z
z
z
z
( )
.
.
.
.
.
=
+
−
+
−
−
−
0 9
0 8
0 4
0 1
1
2
3  
 (5.32) 
 If we subject this system  H ( z ) to an input  X ( z ), then the output  Y ( z ) must be the 
product  Y ( z )  =  H ( z )  X ( z ). Let the input be a step function  x ( n )  =  1. The  z transform 
of a step input 5 is:
 
 X z
z
( )
.
=
−
−
1
1
1  
 (5.33) 
 Hence, the output will be:
 
 Y z
H z X z
( )
( ) ( )
=
 
 (5.34) 
 
 =
−
+
−
+
−
−
−
−
1
1
0 9
0 8
0 4
0 1
1
1
2
3
z
z
z
z
( .
.
.
.
).  
 (5.35) 
 So now, if we invert this function  Y ( z ) into  y ( n ), we should have the same result as 
if we iterated the difference equation for  H ( z ) using the input  x ( n )  =  1. The above 
equation is effectively a polynomial division, so we can undertake a long division 
process to determine  Y ( z ). We ﬁ rst extend  H ( z ) with a (theoretically inﬁ nite) number 
of zeros:
 
 H z
z
z
z
z
z
( )
.
.
.
.
=
+
−
+
+
+
+
−
−
−
−
−
0 9
0 8
0 4
0 1
0
0
1
2
3
4
5
. 
 (5.36) 
 We perform the polynomial long division as shown below. 
 
1 
) -
-
-
-
-
0.9  +  1.7z–1  +  1.3z–2  
1.7z–1  –  0.4z–2  
1.3z–2  +  0.1z–3  
1.4z–3  +   0z–4  
1.4z–4  +   0z–5  
1.4z–4  –  1.4z–5  
.
...
1.4z–3  –  1.4z–4  
1.3z–2  –  1.3z–3  
1.7z–1  –  1.7z–2  
0.9  +  0.8z–1  –  0.4z–2 
0.9  –  0.9z–1
– z–1
+  1.4z–3   +  1.4z–4  +  1.4z–5  +  ...
 +  0.1z–3   +    0z–4   +   0z–5    +  ...
 
  5  See end of chapter problems. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
146 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 N  = 20; 
 b  = [1 0]; 
 a  = [1  − 0.9]; 
 x  =  zeros (N, 1); 
 x(1)  = 1; 
 y  =  ﬁ lter (b, a, x); 
 stem (y); 
 At each successive stage, we choose and multiply the top coefﬁ cient (starting with 
0.9) by the divisor (1  −  z  − 1 ) to give a product (in the ﬁ rst iteration, 0.9  −  0.9 z  − 1 ), 
which is then subtracted from the numerator sequence. Then the next term is brought 
down (downward arrows) and the process repeated. The output is read off the top 
as the sequence 0.9, 1.7, 1.3, 1.4, 1.4,  ...  . 
 Note that this is the  same result as we get in iterating the difference equation. 
The coefﬁ cients are identical, and the  z  −  k terms show the relative position  k . This 
corresponds to our earlier reasoning that  z  −  k is a delay of  k samples. 
 5.6  POLES, ZEROS, AND STABILITY 
 The  z domain equations provide a convenient way for converting between a sampled 
waveform and a difference equation. The transfer function in  z is also invaluable for 
the analysis of systems. The following shows the relationship between a transfer 
function and the time domain, and shows a way to graphically visualize a system ’ s 
expected response. 
 The  poles and  zeros of a system are quantities derived from its transfer func-
tion, and are fundamental to an understanding of how a system responds to a given 
input. They are deﬁ ned as follows:
 The Zeros of a system are those values of  z which make the  numerator B ( z ) 
equal to zero.  
 The Poles  of a system are those values of  z  which make the  denominator A ( z ) 
equal to zero.  
 Note that we are effectively solving for the roots of a polynomial, and hence the 
value (s) of  z may be complex numbers. As a simple example, consider a system 
with one real pole at  z  =  p . The transfer function is:
 
 H z
z
z
p
( ) =
−
.  
 (5.37) 
 The corresponding difference equation is:
 
 y n
x n
py n
( ) =
( ) +
−
(
)
1 .  
 (5.38) 
 The parameter  p controls how much of the previous output is fed back. This recursive 
term is crucial to the stability of the system. Suppose initially  p  =  0.9. We may 
implement this using the difference equation coding approach outlined earlier, or 
using the MATLAB  ﬁ lter () command: 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.6 POLES, ZEROS, AND STABILITY 
147
 Figure  5.9 shows the family of impulse responses with poles in a variety of 
locations on the real axis. From this, some observations may be made:
 1.  Starting from the origin, as the pole location gets closer to  ± 1, the response 
decays more slowly. 
 2.  Poles at positive values of  z show a smooth response, whereas poles at nega-
tive values of  z oscillate on alternating samples. 
 3.  Poles whose magnitude is less than 1 are stable, whereas poles whose magni-
tude is greater than one tend toward instability. 
 Now consider a system with complex poles. Because the coefﬁ cients of the time 
domain terms in the difference equation must be real, the coefﬁ cients of  z when 
written out as a polynomial must in turn be real. This in turn means that the roots 
of the numerator and denominator polynomials in  z (the zeros and poles respectively) 
must be complex conjugates. To give a simple second - order example, suppose a 
system has a transfer function:
 
 G z
z
z
p
z
p
( ) =
−
(
)
−
(
)
2
* ,  
 (5.39) 
where  * denotes complex conjugation. Let the pole  p be at  p
e
= 0 9
10
.
j π
. That is, a 
radius of 09 at an angle of  π 10. We may iterate the difference equation as shown 
 FIGURE 5.9   The impulse response of a system with poles on the real axis. Values of 
| p |  >  1 lead to a diverging response. Negative values of  p have an alternating  + / − response. 
These effects are cumulative (as in the lower - right ﬁ gure). That is, we have both alternation 
 and divergence. 
Response with poles on the real axis
p = +0.9
p = +0.6
p = +1.1
p = −0.8
p = −0.4
p = −1.2
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
148 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 roots (a) 
 ans  = 
      0.8560  + 0.2781 i 
      0.8560  − 0.2781 i 
 abs ( roots (a)) 
 ans  = 
      0.9000 
      0.9000 
 angle ( roots (a)) 
 ans  = 
      0.3142 
      − 0.3142  
previously using the coefﬁ cients derived from the polynomial expansion of the 
denominator of  G ( z ). Deﬁ ning p as the value of the pole location, the  poly () 
function may be used as shown below to expand the factored equation to obtain the 
denominator coefﬁ cients a. The numerator coefﬁ cients b are obtained directly, since 
the numerator is 1 z 2  + 0 z 1  + 0 z 0 : 
  
 p  = 0.9 * exp (j * pi /10) 
 p  = 
      0.8560  + 0.2781 i 
 a  =  poly ([p  conj (p)]) 
 a  =      
      1.0000 1.7119 0.8100 
 b  = [1 0 0];  
 Note how the function  conj () is used to ﬁ nd the complex conjugate. The 
pole locations may be checked using  roots (), which is the complement to 
 poly () 
  
 Note that MATLAB uses i to display the complex number  j =
−1, not j. 
By convention, the pole – zero plot of a system shows the poles labelled  ×  and the 
zeros labelled o. We can extend the MATLAB code to show the poles and zeros, 
together with the unit circle, as follows: 
  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.6 POLES, ZEROS, AND STABILITY 
149
 Note that we need to be a little careful with simple zeros at the origin, and in 
fact the above has  two zeros at  z  =  0, as shown in the context of Equation  5.39 . 
Having more poles than simple zeros will only affect the relative delay of the 
impulse response, not the response shape itself. 
 The  z plane pole/zero conﬁ guration resulting from this transfer function is 
shown in Figure  5.10 . Iterating over 40 samples, the impulse response of this system 
is shown on the right - hand side. Note that it oscillates and decays as time 
progresses. 
 Figure  5.11 shows various responses with complex poles. From this, some 
observations may again be made:
 1.  Starting from the origin, as the pole gets closer to a radius of one, the response 
decays more slowly. 
 2.  When the poles are  on the unit circle, the response oscillates indeﬁ nitely. 
 3.  When the poles are  outside the unit circle, the system becomes unstable. 
 FIGURE 5.10   The pole – zero plot for a system with poles at  z
e
=
±
0 9
10
.
/
jπ
 and two zeros 
at the origin is shown on the left. The unit circle (boundary of stability) is also shown. The 
corresponding impulse response is on the right. 
−1.5 −1 −0.5 0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1.0
1.5
22
z plane
Real
Imaginary
Unstable region
0
5
10
15
20
25
30
35
40
−2
−1
0
1
2
Impulse response
Sample number
Amplitude
 p  = 0.9 * exp (j * pi /10); 
 a  =  poly ([p  conj (p)]); 
 b  = (1 0 0); 
 theta  = 0: pi /100:2 * pi ; 
 c  = 1 * exp (j * theta); 
 plot ( real (c),  imag (c)); 
 set ( gca ,  ‘ PlotBoxAspectRatio ’ , [1 1 1]); 
 zvals  =  roots (b); 
 pvals  =  roots (a); 
 hold on 
 plot ( real (pvals),  imag (pvals),  ‘ X ’ ); 
 plot ( real (zvals),  imag (zvals),  ‘ O ’ ); 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
150 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 FIGURE 5.12   The  z plane and its interpretation in terms of stability (the unit circle) and 
frequency (the angle of the poles relative to  π ). 
ω = π
f = fs/2
 ω = 0
f = 0
ω = 2π
f = f s
 FIGURE 5.11   The impulse responses of a system with complex poles at various 
locations. 
Response with complex poles
p = 0.9e ±π/10
p = 1 e ±π/10
p = 1.2e ±π/10
p = 1 e ±π/4
p = 1 e ±π/2
p = 1 e ±π/20
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.6 POLES, ZEROS, AND STABILITY 
151
 4.  When the magnitude of the poles is greater than one, the response expands. 
 5.  When the magnitude of the poles is less than one, the response decays. 
 6.  The angle of the pole controls the relative frequency of oscillation. 
 The previous arguments reveal that the relative angle of the poles controls the fre-
quency of oscillation, and this may be visualized as shown in Figure  5.12 . The posi-
tive real axis is the  “ zero frequency ” axis, and the negative real axis is the  “ half 
sampling frequency ” or  fs 2 axis. This is the highest frequency a sampled data 
system can produce, because it is constrained by the sample rate. The location of 
the poles for an oscillator — which is termed a  marginally stable system — is shown 
in Figure  5.13 . The relative frequency of oscillation  ω is directly related to the angle 
of the poles, with an angle of  π radians corresponding to a frequency of oscillation 
of  fs 2. The true frequency of oscillation is found by:
 
 f
fs
osc = ω
ρ 2 .  
 (5.40) 
 This may be expressed in words as:
  
 FIGURE 5.13   The poles of a marginally stable system. It is termed  “ marginally stable ” 
because the poles are  on the unit circle. When subjected to an impulse input, the 
corresponding difference equation will produce a sinusoidally oscillating output at 
frequency  ω radians per sample. 
ω
 The true frequency of oscillation is equivalent to the angle of the upper - half  z - plane pole, 
scaled such that an angle of  π radians is equivalent to a real frequency of  fs 2 Hz. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
152 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 5.7  TRANSFER FUNCTIONS AND 
FREQUENCY RESPONSE 
 The  z domain notation introduced in the previous section may be extended further. 
Rather than simply generating waveforms with known frequency, amplitude and 
phase in response to an impulse input, we wish to have a system which can  “ shape ” 
the frequency content of any given signal. Ultimately, this will lead to digital ﬁ lters, 
which have numerous applications in audio, communications, noise removal, and 
elsewhere. Digital ﬁ lters are covered in Chapters  8 and  9 . 
 Suppose a system has a transfer function:
 
 H z
z
( ) =
−
0 5
4
.
.  
 (5.41) 
 The output is simply found by inference:
 
 y n
x n
( ) =
−
(
)
0 5
4
.
.  
 (5.42) 
 That is, a zero - frequency gain of 0.5 and a delay of four samples. Suppose the input 
is a sampled complex exponential of frequency  ω radians,
 
 x n
e n
( ) =
j ω.  
 (5.43) 
 Mathematically, the output is
 
 
y n
e
e
e
n
n
( ) =
=
⋅
−
(
)
−
0 5
0 5
4
4
.
.
.
j
j
j
ω
ω
ω
Input
Scaling
 
	

	  
 (5.44) 
 The multiplicative scaling consists of the magnitude term (0.5) and a phase delay 
of  − 4 ω radians. Remembering that the input frequency was  ω radians per sample, 
it is seen that the output frequency is the same as the input frequency. 
 In general, the output is equal to the input multiplied by a complex number 
representing the transfer function. Complex numbers have magnitude and phase — the 
magnitude is multiplied by the input amplitude, and the phase represents a time delay 
relative to the input frequency (recall that frequency is the rate of change of phase, 
 Ω = d
dt
ϕ
. The generalization of this result may be stated mathematically as:
 
 H e
H z
z e
j
j
ω
ω
(
) =
( )
=
.  
 (5.45) 
 or equivalently: 
 
 In order to obtain the  frequency response of a discrete (sampled - data) system, simply 
substitute
 z →ejω  
and evaluate for  ω  =  0  –  π . 
 This statement is quite fundamental to the understanding of the frequency 
response of digital systems, and we will now devote some time to the investigation 
of this result.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.8 VECTOR INTERPRETATION OF FREQUENCY RESPONSE  
153
 5.8  VECTOR INTERPRETATION OF 
FREQUENCY RESPONSE 
 If we factorize the transfer function numerator and denominator, it will have the 
general form:
 
 H z
K
z
z
z
z
( ) =
⋅
−
(
)
−
(
)(
)
−
(
)
−
(
)(
)
β
β
α
α
0
1
0
1

 . 
 (5.46) 
where  K is a gain term, the  β m are the zeros and the  α n are the poles. The magnitude 
response is found by substituting  z
e
=
jω and ﬁ nding the magnitude of the resulting 
complex number:
 
 H e
K
e
e
e
e
j
j
j
j
j
ω
ω
ω
ω
ω
β
β
α
α
(
) =
⋅
−
(
)
−
(
)(
)
−
(
)
−
(
)(
)
0
1
0
1

  
 (5.47) 
 
 =
⋅
−
−
−
−
K
e
e
e
e
j
j
j
j
ω
ω
ω
ω
β
β
α
α
0
1
0
1

 .
 
 (5.48) 
 To allow for a more intuitive geometric interpretation, consider each complex 
number as a vector in the  z plane. The terms  ejω
β
−
 and  ejω
α
−
 form a vector 
subtraction in the complex plane. Normal vector addition is carried out as shown in 
Figure  5.14 , adding the vectors  a and  b  “ head to tail ” to obtain the resultant  r :
 
 a
b
r
+
= .  
 (5.49) 
 In the case of sampled - date systems in terms of  z , the  “ vector ” quantities are complex 
numbers, written in the form:
 
 e
e
e
j
j
ω
ω
α
α
α
−
=
−
∠
1
3
.  
 (5.50) 
 FIGURE 5.14   Vector addition  a  +  b  =  r . The vector  b is added to the head (arrow) of 
vector  a , and the resultant vector  r is drawn from the start of  a to the end of  b . 
a
r
b
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
154 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 The ﬁ rst term on the right - hand side deﬁ nes a point at a radius of one, at an angle 
of  ω . This angle changes as the frequency of interest changes. The second term is 
the complex point representing a pole (or zero) of the system. Obviously, this is 
constant for the system under consideration. Because it is subtracted, the equivalent 
vector is reversed. This is shown in Figure  5.15 . 
 It may be seen that as the point on the unit circle approaches and then passes 
the angle of the upper pole, the resulting vector to that pole will pass through a 
minimum value. Thus, the response magnitude will be maximized, because (by 
deﬁ nition) the pole is in the denominator. Effectively, the angle of evaluation  ω is 
mapped into the horizontal axis of the frequency response curve (Figure  5.16 ), with 
the value of the response at that frequency shown as the vertical height of the curve. 
The resonant peak on the frequency response is seen to correspond to the pole angle 
in the pole – zero diagram on the complex plane. 
 This considers only one pole to be mapped into the frequency response. There 
are, of course, two poles to be considered, because as pointed out earlier, complex 
poles occur in conjugate pairs. However, since the conjugate pole is somewhat 
further away, its effect will be somewhat less. The net effect will be to draw the 
frequency down slightly, but for the purposes of approximation, it may be neglected 
in order to sketch an outline of the response. 
 As the angle  ω goes from 0 to  π , the top half of the unit circle in the  z plane 
is swept out. As  ω goes from  π to 2 π , the bottom half is covered. Remembering that 
 FIGURE 5.15   Frequency response as a vector evaluation. The speciﬁ c frequency  ω 
radians per sample deﬁ nes a point on the unit circle. The vector from the origin to the pole 
is subtracted (reversed), leaving the resultant as the vector from the pole to the point on the 
unit circle.  
×
−|α |e
α
1e  ω
r
ω
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.8 VECTOR INTERPRETATION OF FREQUENCY RESPONSE  
155
poles (and zeros) occur in complex - conjugate pairs, the magnitude response is seen 
to be symmetrical:  π to 2 π is a mirror image of that from 0 to  π . 
 The above gives a rough order - of - magnitude and shape estimate for the fre-
quency response. We now show how the frequency response can be calculated using 
MATLAB. Of course, it simply depends upon the theory that the frequency response 
is given by evaluating the function  H ( z ) using  z
e
=
jω. We will use the following 
example, which has two poles at radius  r at angle  θ ,
 
 H z
z
z
z
z
r
z
r z
( ) =
+
+
−(
)
+
1
0
0
2
2
1
0
2
1
2
0
cos
.
θ
 
 (5.51) 
 As shown in the MATLAB code below, the key point to take care with is the power 
of  z at each term in the numerator and denominator. 
  
 FIGURE 5.16   Frequency response of the system with poles at radius 0.9, angle  ± π /10. 
Frequency response
Frequency radians/sample
Amplitude
00
π/2
π
3π/2
2π
2
4
6
8
10
12
14
16
18
 r  = 0.9;                               %  pole radius 
 theta  =  pi /10;                        %  pole angle 
 omega  = 0: pi /100:2 * pi ;               % frequency response range 
 % coefﬁ cients of z 
 b  = [1 0 0];                           % b  = numerator 
 a  = [1  − 2 * r *  cos (theta) r * r];       % a  = denominator 
 %  poles  &  zeros 
 syszeros  =  roots (b); 
 syspoles  =  roots (a); 
 fprintf (1,  ‘ Pole angles:  ’ ); 
 fprintf (1,  ‘ % 2f  ’ ,  angle (syspoles)); 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
156 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 5.9  CONVOLUTION 
 Convolution is a method of computing the response of a system to a given input. 
Convolution is based on mathematical principles, but has a very intuitive interpreta-
tion. In essence, the output of any linear, time - invariant system may be imagined as 
the superposition of a series of impulses. The net output is the summation of all 
these outputs, suitably scaled according to the magnitude of the input and aligned 
according to the time of the input (i.e., when the system  “ sees ” each impulse). 
 Figure  5.17 shows the general input – output model of a system. The input to 
the system (the excitation waveform) generates an output. Obviously the output 
depends on the input and the nature of the system itself. This is where the transfer -
 function, and related difference - equation representations, come into play. Suppose 
we are given a difference equation:
 
 y n
x n
x n
x n
( ) =
( ) −
−
(
) +
−
(
)
0 8
0 4
1
0 2
2
.
.
.
.  
 (5.52) 
 Then the transfer function is:
 
 Y z
X z
x
z
( )
( )
=
−
+
−
−
0 8
0 4
0 2
1
2
.
.
.
.  
 (5.53) 
 The impulse response of this system is computed as we have seen previously: 
 n 
 x ( n ) 
 y ( n ) 
 
 0 
 1 
 0.8  ×  1 
 =  0.8 
 1 
 0 
 0.8  ×  0  −  0.4  ×  1 
 =  − 0.4 
 2 
 0 
 0.8  ×  0  −  0.4  ×  0  +  0.2  ×  1 
 =  0.2 
 3 
 0 
 0.8  ×  0  −  0.4  ×  0  +  0.2  ×  0 
 =  0.0 
 4 
 0 
 0.8  ×  0  −  0.4  ×  0  +  0.2  ×  0 
 =  0.0 
  
  
 etc. 
 
 FIGURE 5.17   The input - output model of a sampled - data system. 
H(z)
x(nT )
X(z)
y(nT )
Y(z)
 fprintf (1,  ‘ \n ’ ); 
 k  = 0; 
 for    omegak  = omega 
       k  = k  + 1; 
       expvec  =  exp (j  * [2: − 1:0]  * omegak); 
       H(k)  =  sum (b.  * expvec)./ sum (a.  * expvec); 
 end 
 plot (omega,  abs (H)); 
 set ( gca ,  ‘ xlim ’ , [0 2 * pi ]); 
 set ( gca ,  ‘ xtick ’ , [0:4] * pi /2); 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.9 CONVOLUTION 
157
 It is clear that the output dies away after the input is removed. Speciﬁ cally, if 
the system order is  P (the highest power of  z is  z  −  P ), then the last nonzero output 
term will be  P samples after the input is removed, regardless of whether the input 
is an impulse or not. More precisely, the impulse response lasts for  P  +  1 samples. 
 This particular system has only nonrecursive coefﬁ cients (coefﬁ cients of 
 x ( n ) ’ s). After three output terms, the output is always exactly zero. Hence, this is 
termed a  ﬁ nite impulse response or FIR system. 
 Next, suppose the same system:
 
 y n
x n
x n
x n
( ) =
( ) −
−
(
) +
−
(
)
0 8
0 4
1
0 2
2
.
.
.
.  
 (5.54) 
 is subjected to a ramp input of 0, 1, 2, 3, 4. The output at each iteration is determined 
as shown by the sequence of operations in Figure  5.18 . The output is seen to be 
calculated from the impulse response as follows:
 1.  Time - reverse the input sequence. 
 2.  Slide the  time - reversed input response coefﬁ cients past the impulse response. 
 3.  Integrate (add up) the sliding sequence, and we have the output, or system 
response, at each sampling instant (shown in bold). 
 This operation is  convolution — the input sequence is said to be  convolved with the 
impulse response of the system. Note that what we have really done is the polyno-
mial multiplication of input  X ( z ) and coefﬁ cients  H ( z ). The polynomial multiplica-
tion in the  z domain is the same as convolution in the time domain. We can easily 
perform this in MATLAB using the  conv () function: 
  
 FIGURE 5.18   Calculating the response of a system to a ramp input. The impulse 
response coefﬁ cients are  h n , the input is  x ( n ), producing an output  y ( n ). 
hk
0.8
–0.4
0.2
x(n)
4
3
2
1
0
⇒
y(n)
0
∑=
0
hk
0.8
–0.4
0.2
⇒
x(n)
4
3
2
1
0
⇒
y(n)
0.8
0
∑= 0.8
hk
0.8
–0.4
0.2
x(n)
4
3
2
1
0
⇒
y(n)
1.6
–0.4
0
∑= 1.2
hk
0.8
–0.4
0.2
x(n)
4
3
2
1
0
⇒
y(n)
2.4
–0.8
0.2
∑= 1.8
hk
0.8
–0.4
0.2
x(n)
4
3
2
1
0
⇒
y(n)
3.2
–1.2
0.4
∑= 2.4
hk
0.8
–0.4
0.2
x(n)
4
3
2
1
0
⇒
y(n)
–1.6
0.6
∑= −1.0
hk
0.8
–0.4
0.2
x(n)
4
3
2
1
0
y(n)
0.8
∑= 0.8
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
158 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 Note that this is actually polynomial multiplication. The convolution operation 
is in fact the same as multiplying two polynomials by shifting, multiplying, and 
aligning their coefﬁ cients. In this case, the polynomials are in fact the coefﬁ cients 
of the input  X ( z ) and the impulse response  H ( z ). 
 It also is a worthwhile exercise to use the above example to see what happens 
if we interchange  x ( n ) and  h k . That is, if we reverse the impulse response and hold 
the input sequence the same. The physical interpretation is of course that the input 
sequence is reversed, but mathematically, it does not matter whether the input or the 
impulse response is reversed. 
 So we now have a deﬁ nition of convolution in a signal - processing sense: 
 
 h  = [0.8  − 0.4 0.2]; 
 x  = [0 1 2 3 4]; 
 y  =  conv (h, x); 
 disp (y) 
 The output of a linear, time - invariant system is equal to the convolution of the input 
and the system ’ s impulse response. 
 Mathematically, convolution is deﬁ ned as:
 
 y n
h x n
k
k
k
P
( ) =
−
(
)
=
−
∑
0
1
.  
 (5.55) 
where
 h k   =  impulse response coefﬁ cients 
  P    =  length of the impulse response 
  k  
  =  index to sum over 
  y ( n )  =  output of the system 
 Inﬁ nite impulse response (IIR) systems, on the other hand, have recursive terms in 
the difference equation. The output depends not only on the input, but on  past outputs 
as well. A simple example is:
 
 y n
x n
y n
y n
( ) =
( ) −
−
(
) +
−
(
)
0 3
1
0 1
2
.
.
,  
 (5.56) 
 with transfer function
 
 Y z
X z
z
z
( )
( )
=
+
−
−
−
1
1
0 3
0 1
1
2
.
.
.  
 (5.57) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.9 CONVOLUTION 
159
 The  impulse response of this system is: 
 n 
 x ( n ) 
 y ( n ) 
 
 0 
 1 
 1.0 
 =  1.0 
 1 
 0 
 0  −  0.3  ×  1.0 
 =  − 0.3 
 2 
 0 
 0  −  0.3  ×  −  0.3  +  0.1  ×  1.0 
 =  0.19 
 3 
 0 
 0  −  0.3  ×  0.19  +  0.1  ×  − 0.3 
 =  − 0.087 
  
  
 etc. 
 
 This system has recursive coefﬁ cients (coefﬁ cients of  y [ n ] ’ s). The output theo-
retically  never reaches zero. Hence, it is termed an  inﬁ nite impulse response (IIR) 
system. 
 Calculating some more terms in the impulse response, we get:
 1
0 3
0 19
0 087
0 0451
0 02223
0 01117
0 00557
0 00279
0
−
+
−
+
−
+
−
+
+
.
.
.
.
.
.
.
.
.000697
… 
 We cannot calculate the output of the IIR system directly using convolution, as we 
did with the FIR system. However, noting that the output terms die away, an FIR 
 approximation could be written as:
 
 ˆ
.
.
.
.
y n
x n
x n
x n
x n
( ) =
( ) −
−
(
) +
−
(
) −
−
(
)
0 3
1
0 19
2
0 087
3
 
 (5.58) 
where the circumﬂ ex or caret ( “ hat ” ) above  y means  “ approximation to. ” The cor-
responding transfer function is
 
 
ˆ
.
.
.
.
Y z
X z
z
z
z
( )
( )
= −
+
−
−
−
−
1
0 3
0 19
0 087
1
2
3  
 (5.59) 
 The impulse response is then:
 1
0 3
0 19
0 087
0 0
0 0
0 0
0 0
0 0
−
+
−
.
.
.
.
.
.
.
.
…  
 Some conclusions on convolution, FIR, and IIR systems are worth summarizing:
 1.  Theoretically, the FIR (nonrecursive) system needs an inﬁ nite number of terms 
to equal the IIR (recursive) system. 
 2.  Depending on the accuracy required, it is possible to use a ﬁ xed - length FIR 
transfer function to approximate an IIR transfer function by truncating the 
impulse response. 
 3.  Systems can have poles only, zeros only, or both poles and zeros. If it has 
zeros only, there will be one or more poles at the origin in order to make the 
system causal (i.e., have only zero or negative powers of  z , and not depend 
on future samples). 
 These observations will be important later in the design and analysis of digital ﬁ lters. 
We will return to convolution in Chapter  8 , where it will be shown that the concept 
helps us understand the operation of digital ﬁ ltering. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
160 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
 5.10  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  the importance of and relationship between  difference equations ,  transforms , 
and the  impulse response . 
 •  the role of the  z  transform in signal processing. 
 •  the concept of  poles and zeros , and  system stability . 
 •  the concept of  convolution and how it is used to calculate a system ’ s output.  
 PROBLEMS 
 5.1.   Show that the output from the difference equation  y ( n )  =  0.9 x ( n )  +  0.8 x ( n  −  1)  −  0.4 x 
( n  −  2)  +  0.1 x ( n  −  3) when subjected to an input 1, 1, 1, 1, 1, 0, 0, 0,  . . .  is as follows:
 N 
 x ( n ) 
 y ( n ) 
 Sample Instant 
 Input 
 Output 
 0 
 1 
 0.9 
 1 
 1 
 0.9  +  0.8  =  1.7 
 2 
 1 
 0.9  +  0.8  −  0.4  =  1.3 
 3 
 1 
 0.9  +  0.8  −  0.4  +  0.1  =  1.4 
 4 
 1 
 0.9  +  0.8  −  0.4  +  0.1  =  1.4 
 5 
 0 
 0.9  ×  0  +  0.8  ×  1  −  0.4  ×  1  +  0.1  ×  1  =  0.5 
 6 
 0 
 − 0.4  +  0.1  =  − 0.3 
 8 
 0 
 0.1 
 8 
 0 
 0 
 9 
 0 
 0 
 10 
 0 
 . . . 
 5.2.  A system has poles at  r  =  0.9 and angle  ω
π
= 4 .
 (a)  Determine the difference equation. 
 (b)  Find the ﬁ rst eight terms resulting from an impulse input. 
 (c)  Repeat if the radius is 0.95, 1.1, 2, and 4. 
 5.3.  Convert the following difference equations into  z transforms:
 (a)  y ( n )  =  x ( n  −  2) 
 (b)  y ( n )  =  0.1 x ( n )  +  0.9 x ( n  −  1)  −  0.3 y ( n  −  1) 
 5.4.  Starting with the deﬁ nition of a  z transform, show that a unit step has a transform  z
z −1. 
 5.5.  Find the  z transform of  te  −  at , and the corresponding difference equation. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.10 CHAPTER SUMMARY 
161
 5.6.  Section  5.4.1 showed how  z  − 1 can be used to model a delay of one sample. What 
would  z  −  k mean? Prove that your inference is valid using a similar approach. 
 5.7.  Using the example impulse response of Equation  5.54 and ramp input sequence of 
Figure  5.18 (Section  5.9 ), show that reversing the impulse response with the input 
ﬁ xed produces the same result as reversing the input sequence and keeping the impulse 
coefﬁ cients ﬁ xed. Check using the  conv () function in MATLAB. 
 5.8.  Enter the MATLAB code in Section  5.3.1 for generating a pure - tone waveform.
 (a)  Vary the frequency  f , plot the waveform and listen to the result using the 
 sound () command. 
 (b)  Use the following approach to generate a frequency - modulated  “ chirp ” signal 
whose frequency increases over time, and again listen to the result. 
 Fs  =  8000; 
 dt   =  1/Fs;  
 N  =  8000;  
 t  =  0:dt:(N – 1) * dt;  
 f  =  400;  
 fi  =  [1: length (t)]/ length (t) * f; 
 y  =  sin (2 * pi * t. * fi);  
  plot (y(1:2000));  
 sound (y, Fs);  
 5.9.  Consider the phase compensation and audio buffering as described in Section  5.3.4 . 
Implement the code given, both with and without phase compensation. Are the phase 
artifacts audible if compensation is not employed? 
 5.10.  Again using the phase compensation and audio buffering as described in Section  5.3.4 . 
Generate a sinusoidal waveform, but with a gap of  D samples at the start of each 
buffer (i.e., introduce gaps in the continuous waveform). Start with  D  =  1, then prog-
ress to 10, 20, 100 samples. What length of gap is just noticeable?  
 5.11.  Derive the  z transforms and corresponding difference equations for the waveforms:
 •  x ( t )  =  cos  Ω t 
 •  x ( t )  =  sin ( Ω t  +  φ ) 
 •  x ( t )  =  cos ( Ω t  +  φ ) 
 •  Implement the difference equations using MATLAB and check that each generates 
the expected wave form in each case. 
 5.12.  Sketch the frequency response of a system with a pole at  z  =  1 and a pole at  z  =  − 1. 
Repeat for zeros at the same locations. 
 5.13.  The image data compression standard  JPEG2000 uses a technique called  sub - band 
coding to reduce the size of image data ﬁ les, and hence the space taken to store the 
image (or, equivalently, the time taken to send the image over a communications 
channel). This method uses a pair of ﬁ lters, called  quadrature mirror ﬁ lters (QMFs). 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
162 
CHAPTER 5 REPRESENTING SIGNALS AND SYSTEMS 
  6  The precise details are beyond the scope of this chapter. 
A very simple set of quadrature mirror ﬁ lters  h 0 ( n ) and  h 1 ( n ) may be deﬁ ned using 
the equations
 
F z
z
H
z
F z
H z
F
z
( ) =
+
(
)
( ) =
( )
( ) =
−
(
)
−
1
2
1
1
0
1
 
 (a)  What are the corresponding difference equations for ﬁ lters  H 0 ( z ) and  H 1 ( z ) that 
need to be implemented in this digital signal processing system? 
 (b)  Use the pole/zero diagram to sketch the frequency responses of each ﬁ lter, and 
hence explain how  H 0 ( z ) and  H 1 ( z ) form low - pass and high - pass ﬁ lters, 
respectively. 
 (c)  Show mathematically that the ﬁ lter pair is energy preserving — that is
 H
H
0
2
1
2
1
ω
ω
( ) +
( )
=  
 5.14.  Low - bitrate speech coders often employ a noise - shaping ﬁ lter, which involves chang-
ing the poles of a speech synthesis ﬁ lter. The goal is to  “ shape ”  the response such that 
more quantizing noise is concentrated in the spectrum where there is greater energy, 
thus making it less perceptually objectionable. 6 
 (a)  Suppose we have a ﬁ lter  G z
A z
( ) =
( )
1
 with poles at  z = (
) ± (
)
1 2
1 2
j
. 
Determine the transfer function, the angle of the poles, and the radius of the 
poles. 
 (b)  We employ a noise - shaping coefﬁ cient  γ , and a noise - shaping transfer function 
 G ( z / γ ). Determine this transfer function, pole angle, pole radius, and gain. 
 (c)  Show that the pole angle is unchanged in going from  G ( z ) to  G ( z / γ ). What 
happens to the pole radius? 
 (d)  Using the above result, sketch the two frequency responses. 
 (e)  Would this result hold in general for the same order of transfer function for 
other pole locations? Would it hold in general for higher - order transfer 
functions? 
 (f)  In practice, the above approach is used in a feedback conﬁ guration, with the 
output ﬁ ltered by  W z
A z
A z
( ) =
( )
(
)
/ γ . Sketch the frequency response of  W ( z ), 
and show that the gain is reduced in the vicinity of the pole frequency. 
 5.15.  Perform the convolution as tabulated in Figure  5.18 using multiplication of the cor-
responding  z transforms. Does the result equal that obtained by the iterative method 
as in the table?  
 5.16.  The output of a linear system is given by the product of the input and the transfer 
function, when all are expressed in the  z domain. That is, ( z )  =  X ( z )  H ( z ). Recall that 
the  z transform of  h n is deﬁ ned as  H ( z )  =  Σ n h n z  −  n , and that in the time domain, the 
output of a system is computed using the convolution as  y ( n )  =  Σ k h k x ( n  −  k ).
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
5.10 CHAPTER SUMMARY 
163
 (a)  Take the  z transform of both sides of the convolution equation. 
 (b)  By making the substitution  m  =  n  −  k and reversing the order of the summa-
tions, show that  Y ( z )  =  X ( z )  H ( z ). 
 5.17.  We are able to ﬁ nd the frequency response of a sampled - data system using the sub-
stitution  z
e
=
jω in the transfer function. Recall that the output of a discrete system is 
the convolution of the input and impulse response, given by  y ( n )  =  Σ k h k x ( n  −  k ). Let 
the input to a system be a sampled complex sinusoid  x n
e m
( ) =
j ω.
 (a)  Substitute this input into the convolution equation to derive an expression for 
 y ( n ). 
 (b)  Using the deﬁ nition of the  z transform  H ( z )  =  Σ n h n z  −  n , show that  y ( n ) may be 
written as the product of the sampled sinusoid and  H ( z ) when  z
e
=
jω. 
 
 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER 6 
TEMPORAL AND SPATIAL 
SIGNAL PROCESSING 
 6.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to:
 1.  explain and use  correlation as a signal processing tool. 
 2.  be able to  derive the  correlation equations for some mathematically deﬁ ned 
signals. 
 3.  derive the operating equations for  system identiﬁ cation in terms of 
correlation. 
 4.  derive the equations for  signal extraction when the signal is contaminated by 
noise, and explain the underlying assumptions behind the theory. 
 5.  derive the equations for  linear prediction and  optimal ﬁ ltering . 
 6.  write down the basic equations used in  tomography for reconstruction of an 
object ’ s interior from cross - sectional measurements. 
 6.2  INTRODUCTION 
 Signals, by deﬁ nition, are varying quantities. They may vary over time (temporal) 
or over an  x − y plane (spatial), over three dimensions, or perhaps over time and space 
(e.g., a video sequence). Understanding how signals change over time (or space) 
helps us in several key application areas. Examples include extracting a desired 
signal when it is mixed with noise, identifying (or at least, estimating) the coefﬁ -
cients of a system, and reconstructing a signal from indirect measurements 
(tomography).  
 6.3  CORRELATION 
 The term  “ correlation ” means, in general, the similarity between two sets of data. 
As we will see, in signal processing, it has a more precise meaning. Correlation in 
165
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
166 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
signal processing has a variety of applications, including removal of random noise 
from a periodic signal, pattern recognition, and system parameter identiﬁ cation. 
 To introduce the computation of correlation, consider ﬁ rst a simpler problem 
involving a set of measured data points ( x n ,  y n ). We wish to quantify the similarity 
between them — or more precisely, to determine if they are correlated (if one depends 
in some way on the other). Note that this does not necessarily mean a causal rela-
tionship (that one causes another). These data points may represent samples from 
the same signal at different times, or samples from two different but related signals. 
 We may picture the two sets of samples as shown on the axes of Figure  6.1 , 
on the assumption that there is a linear relationship between the two sets of measure-
ments. Because the measurements are not precise, some variation or  “ noise ” will 
cause the points to scatter from the ideal.  
 Mathematically, we wish to choose the value of  b describing the line  y  =  bx 
such that the error  ε between calculated (estimated) and measured points is mini-
mized over all data points. The error is the difference between the measured data 
point  y d and the straight line estimation using an equation for  y l . So how do we 
determine  b ? 
 We can employ the tools of calculus to help derive a solution. Since the error 
may be positive or negative, we take the square of the error:
 
 
ε
ε
=
−
∴
=
−
(
)
=
−
(
)
y
y
y
y
y
bx
d
l
d
l
d
2
2
2
 
 (6.1) 
 The average squared (mean square) error over  N points is:
 
 E
N
y
bx
d
N
ε2
2
1
(
) =
−
(
)
∑
.  
 (6.2) 
 FIGURE 6.1   Correlation interpreted as a line of best ﬁ t. The measured points are show 
as   , and the line shows an approximation to the points in the ( x ,  y ) plane. The error is 
shown between one particular point and the approximation; the goal is to adjust the slope 
of the line until the average error over all measurements is minimized ( min∑ε2). 
x
y
yd
yl
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.3 CORRELATION 
167
 For simplicity of notation, replace  y d with  y . This is a minimization problem, and 
hence the principles of function minimization are useful. The derivative with respect 
to the parameter  b is:
 
 ∂
∂
=
−
(
) −
(
)
∑
E
b
N
y
bx
x
N
1
2
.  
 (6.3) 
 Setting this gradient to zero,
 
 
1
2
0
1
1
2
2
2
N
xy
bx
N
xy
b N
x
b
xy
x
N
N
N
N
N
−
+
(
) =
∴
=
=
∑
∑
∑
∑
∑
.
 
 (6.4) 
 If the points  x n and  y n represent samples of signals  x ( n ) and  y ( n ), then  b is simply a 
scalar quantity describing the similarity of signal  x ( n ) with signal  y ( n ). If  b  =  0, there 
is no similarity; if  b  =  1, there is a strong correlation between one and the other. 
 6.3.1  Calculating Correlation 
 The  autocorrelation of a sampled signal is deﬁ ned as the product
 
 R
k
N
x n x n
k
xx
n
N
( ) =
( )
−
(
)
=
−
∑
1
0
1
,  
 (6.5) 
where  k is the  lag or delay. The correlation may be normalized by the mean square 
of the signal  R xx (0) giving:
 
 ρxx
xx
xx
k
R
k
R
( ) =
( )
( )
0 .  
 (6.6) 
 The subscript  xx denotes the fact that the signal is multiplied by a delayed version 
of itself. For a number of values of  k , the result is a set of correlation values — an 
 autocorrelation vector . If we limit ourselves to one - way relative delays in the lag 
parameter (as in most practical problems), we have a vector
 
 r =
( )
( )
−
(
)
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
R
R
R N
0
1
1

 
 (6.7) 
 Note that the term  “ correlation ” is used, and not  “ covariance. ” These terms are used 
differently in different ﬁ elds. In signal processing, correlation is normally used as 
we have deﬁ ned it here. However, it is often necessary to remove the mean of the 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
168 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
sample block before the correlation calculation. Sometimes, the correlation is nor-
malized by a constant factor, as will be described shortly. This is simply a constant 
scaling factor, and does not affect the shape of the resulting correlation plot. 
 To illustrate the calculation of autocorrelation, suppose we have an  N  =  4 
sample sequence. For  k  =  0, we compute the product of corresponding points 
 x (0) x (0)  +  x (1) x (1)  +  x (2) x (2)  +  x (3) x (3), as shown in the vectors below: 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 For  k  =  + 1, we compute the product  x (1) x (0)  +  x (2) x (1)  +  x (3) x (2), 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 · 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 For  k  =  − 1, we compute a similar product of aligned terms: 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 · 
 and so forth, for  k  =  ± 2,  ± 3,  ...  . Extrapolating the above diagrams, it may be seen 
that the lag may be positive or negative, and may in theory range over
 lag 
1,
2,
, 1, 0,1,
,
2,
1
k
N
N
N
N
→−
+
−
+
−
−
−


 
 The continuous - time equivalent of autocorrelation is the integral
 
 R
x t x t
dt
xx λ
τ
λ
τ
( ) =
( )
−
(
)
∫
1
,  
 (6.8) 
where  λ represents the signal lag. The integration is taken over some suitable inter-
val; in the case of periodic signals, it would be over one (or preferably, several) 
cycles of the waveform. 
 Autocorrelation is the measure of the similarity of a signal with a delayed 
version of the  same signal .  Cross - correlation is a measure of the similarity of 
a signal with a delayed version of  another signal . Discrete cross - correlation is 
deﬁ ned as
 
 R
k
N
x n y n
k
xy
n
N
( ) =
( )
−
(
)
=
−
∑
1
0
1
,  
 (6.9) 
where, again,  k is the positive or negative lag. The normalized correlation in this 
case is:
 
 ρxy
xy
xx
yy
k
R
k
R
R
( ) =
( )
( )
( )
0
0
.  
 (6.10) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.3 CORRELATION 
169
 The cross - correlation is computed in exactly the same way as autocorrelation, using 
a different second signal. For a simple  N  =  4 - point example, at a lag of  k  =  0, we 
have  x (0) y (0)  +  x (1) y (1)  +  · · · as shown: 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 · 
 y (0) 
 y (1) 
 y (2) 
 y (3) 
 · 
 At  k  =  ± 1, we have: 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 · 
 · 
 y (0) 
 y (1) 
 y (2) 
 y (3) 
 · 
 x (0) 
 x (1) 
 x (2) 
 x (3) 
 · 
 y (0) 
 y (1) 
 y (2) 
 y (3) 
 · 
 · 
 The process may be continued for  k  =  ± 2,  ± 3,  ...  . Multiplying out and com-
paring the terms above, it may be seen that autocorrelation is symmetrical, whereas 
in general cross - correlation is not. 
 The continuous - time equivalent of cross - correlation is the integral
 
 R
x t y t
dt
xy λ
τ
λ
τ
( ) =
( )
−
(
)
∫
1
.  
 (6.11) 
where  λ represents the signal lag. The integration is taken over some suitable inter-
val; in the case of periodic signals, over one cycle of the waveform (or, preferably, 
many times the period). 
 The cross - correlation (without normalization) between two equal - length 
vectors  x and  y may be computed as follows: 
  
 %  illustrating one - and two - sided correlation 
 %  set up data sequences 
 N  =  4; 
 x  =  [1:4] ’ ; 
 y  =  [6:9] ’ ; 
 %  two - sided correlation 
 ccr2  =   zeros (2  * N   −  1, 1); 
 for lag   =   − N  + 1 : N   −  1 
      cc  = 0; 
      for idx   =  1 : N 
           lagidx    =  idx   −  lag; 
           if ((lagidx   >=  1)  & & (lagidx   <=  N)) 
               cc   =  cc  + x (idx)  * y (lagidx); 
           end 
      end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
170 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 Note that we need nested loops: the outer loop to iterate over the range of correlation 
lags we require, and an inner loop to calculate the summation over all terms in each 
vector. Also, no attempt has been to optimize these loops: the loops are coded exactly 
as per the equations. It would be more efﬁ cient in practice to start the indexing of 
the inner  for loop at the very ﬁ rst nonzero product term, rather than utilize the  if  
test within the for loop as shown for the two - sided correlation. 
 The difference in computation time (and/or processor loading) may be negli-
gible for small examples such as this, but for very large signal vectors, such attention 
to efﬁ ciency pays off in terms of reduced execution time. 
 This slide - and - sum operation should remind us of convolution, which was 
introduced in Section  5.9 . The key difference is that in convolution, we reverse one 
of the sequences at the start. Since MATLAB has the  conv () function available, 
it must (implicitly) perform this reversal. Now, in computing the correlation, we do 
not require this reversal. Thus, if we reverse one of the sequences prior to convolu-
tion, we should negate the reversal within  conv (), and thus obtain our correlation. 
This is illustrated in the following, where we use the  “ ﬂ ip upside - down ” function 
 ﬂ ipud () to time - reverse the vector: 
  
      ccr2(lag  + N)   =  cc; 
 end 
 disp (ccr2) 
 %  one - sided correlation 
 ccr1  =  zeros (N, 1); 
 for lag  = 0 : N   −  1 
      cc  = 0; 
      for idx  = lag  + 1:N 
           lagidx  = idx   −  lag; 
           cc  = cc  + x(idx)  * y(lagidx); 
       end 
      ccr1(lag  + 1)  = cc; 
 end 
 disp (ccr1)  
 %  using convolution to calculate correlation 
 x  = [1:4] ’ ; 
 y  = [6:9] ’ ; 
 cc  =  conv (x,  ﬂ ipud (y)); 
 disp (cc) 
 Note that we need to take particular care to use  ﬂ ipud () or  ﬂ iplr () (ﬂ ip 
left - right) as appropriate here, depending upon whether the input vectors are column 
vectors or row vectors. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.3 CORRELATION 
171
 FIGURE 6.2  Visualizing correlation as applied to waveforms. The top waveform is ﬁ xed, 
the middle is the moving waveform, and the bottom panel shows the resulting correlation 
of the waveforms. In panel a, we see the initial situation, where there is no overlap. In 
panel b, as the second waveform is moved further on, the correlation is increasing. 
Input waveform
Lagged waveform (maximum negative lag)
Resulting correlation
Input waveform
Lagged waveform (negative lag)
Resulting correlation
(a) Time snapshot 1.
(b) Time snapshot 2.
 FIGURE 6.3   In panel a, the correlation has reached a peak. In panel b, the correlation is 
just starting to decline.  
Input waveform
Lagged waveform (zero lag)
Resulting correlation
Input waveform
Lagged waveform (positive lag)
Resulting correlation
(a) Time snapshot 3.
(b) Time snapshot 4.
 6.3.2  Extending Correlation to Signals 
 The calculation examples in the preceding section are for very short vectors. When 
considering signal processing applications, the vectors are generally quite long 
(perhaps thousands of samples), and so it helps to visualize matters not as discrete 
points, but as waveforms. Now that we have the calculation tools at our disposal, 
we can extend our understanding to longer vectors computed automatically. 
 Figures  6.2 through  6.4 show how the correlation product is computed for 
sinusoidal vectors as signals. One important practical aspect is the fact that the 
data window is ﬁ nite. This gives rise to a decreasing correlation function, as 
illustrated. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
172 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 6.3.3  Autocorrelation for Noise Removal 
 Suppose we have a sine wave, and wish to compute the autocorrelation. In MATLAB, 
this could be done using the code shown previously for correlation, but with a sine 
wave substituted for the input sample vector. 
  
 FIGURE 6.4   In panel a, the correlation is declining further. In panel b, the correlation is 
almost back to zero.  
Input waveform
Lagged waveform (positive lag)
Resulting correlation
Input waveform
Lagged waveform (maximum positive lag)
Resulting correlation
(a) Time snapshot 5.
(b) Time snapshot 6.
 t  = 0: pi /100:2  * 4  *  pi; 
 x  =  sin (t);  
 Note that the result has the form of a cosine waveform, tapered by a triangular 
envelope (as shown in Fig.  6.4 ). This is due to the fact that, in practice, we only 
have a ﬁ nite data record to work with. 
 Mathematically, we can examine this result as follows. Using the deﬁ nition of 
continuous autocorrelation
 
 R
x t x t
dt
xx λ
τ
λ
τ
( ) =
( )
−
(
)
∫
1
.  
 (6.12) 
 If the sine wave is
 
 x t
A
t
( ) =
+
(
)
sin
,
Ω
ϕ
 
 (6.13) 
 then,
 
 x t
A
t
−
(
) =
−
(
)+
(
)
λ
λ
ϕ
sin
.
Ω
 
 (6.14) 
 So,
 
 R
A
t
t
dt
xx λ
τ
ϕ
λ
ϕ
τ
( ) =
+
(
)
−
(
)+
(
)
∫
1
2
0
sin
sin
.
Ω
Ω
 
 (6.15) 
 Noting that  τ is one period of the waveform, and that  τ
π
= 2
Ω, this simpliﬁ es to:
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.3 CORRELATION 
173
 
 R
A
xx λ
λ
( ) =
2
2 cos
.
Ω
 
 (6.16) 
 This result can be generalized to other periodic signals. The autocorrelation will 
always retain the same period as the original waveform. What about additive random 
signals, which are often a good model for real - world signals? 
 If we have a Gaussian noise signal, its autocorrelation may be found in 
MATLAB using a random data source generated using x  =  randn (1,000, 1); 
The result, shown in Figure  6.5 , shows that there is only one strong component, at 
a lag of zero. The autocorrelation at lag zero is simply the mean - square of the signal, 
since we are multiplying each sample by itself. 
 However, at lags other than zero, we are multiplying two random values. On 
average, the summation of these (the autocorrelation at that point) will equal zero. 
Thus, the autocorrelation at lag zero of zero - mean white Gaussian noise is equal to 
the variance:
 
 R
k
k
Gaussian
elsewhere
( ) =
=
⎧⎨⎩
σ2
0
0
:
:
.  
 (6.17) 
 FIGURE 6.5   Autocorrelation of random Gaussian noise. This shows the autocorrelation 
function itself, which in theory is zero for all nonzero offsets. We also know that for 
additive signals, the correlations are effectively added together. 
0
100
200
300
400
500
600
700
800
900
1,000
−4
−2
0
2
4
Sample number
Amplitude
Gaussian noise signal
−1,000
−800
−600
−400
−200
0
200
400
600
800
1,000
−0.5
0
0.5
1
Correlation offset
Correlation
Normalized correlation
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
174 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 In practice, only an approximation to the ideal case exists. This is simply because 
we have a ﬁ nite — not inﬁ nite — number of samples to average over. 
 6.3.4  Correlation for Signal Extraction 
 We now have two useful pieces of information: ﬁ rst, that the autocorrelation of a 
periodic signal preserves the periodicity; second, that autocorrelation of a random 
signal is zero at all lags except for a lag of zero. 
 Suppose we have a zero - mean noise - corrupted signal  y ( n ), which is comprised 
of a (desired) periodic signal  x ( n ) with additive, uncorrelated noise  v ( n ). The observed 
signal may be written
 
 y n
x n
v n
( ) = ( )+ ( ).  
 (6.18) 
 Mathematically, the autocorrelation of the observed signal is
 
 
R
k
N
x n
v n
x n
k
v n
k
N
x n x n
k
x n v n
yy
N
( ) =
( )+ ( )
(
)
−
(
)+
−
(
)
(
)
=
( )
−
(
)+ ( )
−
∑
1
1
k
v n x n
k
v n v n
k
N
x n x n
k
N
x n v n
k
N
N
(
)
(
+ ( )
−
(
)+ ( )
−
(
))
=
( )
−
(
)+
( )
−
(
)
∑
∑
1
1
N
N
N
N
v n x n
k
N
v n v n
k
∑
∑
∑
+
( )
−
(
)+
( )
−
(
)
1
1
 
 (6.19) 
 
 
 
 (6.20) 
 Thus, we have an equation for the signal cross - correlation in terms of the various 
individual signal - noise cross - correlation terms. As we have shown, it is simply an 
additive sum. If the noise  v ( n ) is uncorrelated with itself then
 R
k
k
vv( ) =
≠
0
0
for all 
 
 If the noise is uncorrelated with the periodic signal, then for zero - mean signals the 
cross - correlation ought to equal zero, regardless of the lag chosen:
 R
k
k
R
k
k
xv
vx
( ) =
( ) =
0
0
for all 
for all  
 Hence, all terms but the ﬁ rst in Equation  6.20 cancel out, leaving only the autocor-
relation of the signal itself. 
 A sample input signal is shown in Figure  6.6 . Only the noisy waveform is 
assumed to be available, and so determining the period in the time domain is likely 
to be quite error - prone. Figure  6.7 shows the autocorrelation of the clean signal 
(upper panel) and the autocorrelation of the noisy signal (lower). In theory, the noise 
only affects the correlation at a lag of zero. Hence, the periodicity of the original 
noise - free signal may be determined with greater accuracy. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.3 CORRELATION 
175
 FIGURE 6.6   Time - domain waveforms: clean (upper) and noise - corrupted (lower). 
Ideally, we wish to determine the frequency of the clean signal from samples of the noisy 
signal only. The presence of noise makes this difﬁ cult to do. 
Noise-free waveform
Noisy waveform
0
0
0
0
10
10
20
20
30
30
40
40
30
30
60
60
70
70
80
80
90
90
100
100
1
1
–1
–1
2
2
–2
–2
 FIGURE 6.7   Correlation waveforms: clean (upper) and noise corrupted (lower). Apart from 
the addition at zero lag, the waveforms are nearly identical. The zero crossings of the correlation 
waveform are now well - deﬁ ned, making the frequency determination much more reliable. 
Noise-free waveform correlation
Noisy waveform correlation
Correlation lag k
0
0
0
0
20
20
40
40
60
60
80
80
100
100
–20
–20
–40
–40
–60
–60
–80
–80
–100
–100
50
50
–50
–50
 6.3.5  Correlation in System Identiﬁ cation 
 In many real - world applications, we wish to determine the transfer function of a 
system. Examples include telecommunications (where, e.g., the transfer function of 
a channel may be determined upon initialization of a system) and audio processing 
of room acoustics. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
176 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 Signal processing techniques may be applied to determine the impulse 
response of a system. This leads to a characterization of the frequency response, and 
frequency compensation may be performed in the form of equalizing or pulse -
 shaping ﬁ lters. 
 The impulse response of a system, and hence its FIR coefﬁ cients, may obvi-
ously be found by subjecting the system to an impulse input. However, in most 
practical applications, it is not feasible to subject a system to an impulse input. 
Hence, an alternative is required. 
 To begin, consider the output of a system subjected to an input  x ( n ) as:
 
 y n
b x n
b x n
b x n
( ) =
( )+
−
(
)+
−
(
)+
0
1
2
1
2
.  
 (6.21) 
 Now multiply each side by  x ( n ) to obtain
 y n x n
b x n x n
b x n
x n
b x n
x n
( ) ( ) =
( ) ( )+
−
(
) ( )+
−
(
) ( )+
0
1
2
1
2
.  
 Summing over a large number of samples  N , we have:
 
y n x n
b x n x n
b x n
x n
b x n
x n
b x n
N
N
( ) ( ) =
( ) ( )+
−
(
) ( )
(
+
−
(
) ( )+
−
(
∑
∑
0
1
2
3
1
2
3) ( )+
)
=
( ) ( )+
−
(
) ( )
+
−
(
) ( )+
−
∑
∑
∑
x n
b x n x n
b x n
x n
b x n
x n
b x n
N
N
N

0
1
2
3
1
2
3
1
2
0
1
2
3
(
) ( )+
=
( ) ( )+
−
(
) ( )
+
−
(
) ( )+
∑
∑
∑
∑
x n
b
x n x n
b
x n
x n
b
x n
x n
b
N
N
N
N

x n
x n
N
−
(
) ( )+
∑
3
.
 
 The summation terms may be recognized as scaled correlations. If  x ( n ) is Gaussian 
noise, then all terms on the right - hand side except  ∑
( ) ( )
N x n x n  will be zero, leaving
 
 
y n x n
b
x n x n
N
N
( ) ( ) =
( ) ( )
∑
∑
0
.  
 (6.22) 
 And so  b 0 is found as
 
 b
y n x n
x n x n
N
N
0 =
( ) ( )
( ) ( )
∑
∑
 
 (6.23) 
 Similarly, multiplying both sides by  x ( n  −  1) and summing over a large number of 
samples  N , the system coefﬁ cient  b 1 is found as
 
 b
y n x n
x n
x n
N
N
1
1
1
1
=
( )
−
(
)
−
(
)
−
(
)
∑
∑
 
 (6.24) 
 Continuing on, the terms  b 2 ,  b 3 ,  b 4 ,  ...  may be found in a similar manner. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.4 LINEAR PREDICTION 
177
 Thus, the system FIR model may be approximated subject to the assumptions 
detailed regarding the autocorrelation of the input and cross - correlation of the input 
and output. It is important to understand the assumptions made at the outset for 
developing the model, since these are also the model ’ s limitations. 
 6.4  LINEAR PREDICTION 
 We now turn to the problem of estimating the future values of a sequence. The solu-
tion to this problem requires the correlation values — not the single correlation values 
as discussed earlier, but rather a matrix of correlation values. This problem occurs 
often in signal processing, and may be found in signal coding and compression, 
ﬁ ltering, and other application areas. 
 To derive the problem and its general solution, suppose we have samples of 
a sequence  x ( n ), and want to predict the value at the next sample instant  ˆx n( ). The 
prediction error is:
 
 e n
x n
x n
( ) = ( )−( )
ˆ
.  
 (6.25) 
 One obvious way to form the estimate  ˆx n( ) is to base the estimate on past samples. 
If we estimate one sample ahead from the most recent sample, we could form the 
linear estimate  w 0 x ( n  −  1), where  w 0 is a constant to be determined. In this case, the 
estimate and error are:
 
 
ˆ
ˆ
.
x n
w x n
e n
x n
x n
x n
w x n
( ) =
−
(
)
( ) = ( )−( )
= ( )−
−
(
)
0
0
1
1
 
 (6.26) 
 The  w 0 may be thought of as a multiplying  “ weight, ” and for more than one  w , the 
prediction of the output is formed by the linear weighted sum of past samples. So 
we need to determine the value of the constant parameter  w 0 . The instantaneous 
squared error is
 
 e
n
x n
w x n
2
0
2
1
( ) =
( )−
−
(
)
(
) .  
 (6.27) 
 The error is squared because it could be positive or negative, and the squaring opera-
tion also allows us to form the derivative easily when ﬁ nding the optimal solution 
mathematically. This error value is only for one sample, but what we really need is 
the minimum error averaged over a block of  N samples. This is sometimes called 
the  “ cost function. ” So the average squared error is
 
 
e
N
e
n
N
x n
w x n
N
x
n
x n w x n
w
N
N
2
2
0
2
2
0
0
1
1
1
1
2
1
=
( )
=
( )−
−
(
)
(
)
=
( )−
( )
−
(
)+
∑
∑
2
2
1
x
n
N
−
(
)
(
)
∑
.
 
 (6.28) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
178 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 To minimize the cost function with respect to the parameter  w 0 , we take the 
derivative
 
 de
dw
N
x n x n
w x
n
N
2
0
0
2
1
0
2
1
2
1
=
−
( )
−
(
)+
−
(
)
(
)
∑
.  
 (6.29) 
 To ﬁ nd the minimum average error, we set the derivative equal to zero
 de
dw
2
0
0
= .  
 Expanding Equation  6.29 ,
 
 1
1
1
1
0
2
N
x n x n
w N
x
n
N
N
( )
−
(
) =
−
(
)
∑
∑
*
,  
 (6.30) 
 solving this for the optimal predictor coefﬁ cient  w0* yields:
 
 w
N
x n x n
N
x
n
N
N
0
2
1
1
1
1
*
.
=
( )
−
(
)
−
(
)
∑
∑
 
 (6.31) 
 We assume that the summation has to be taken over a sufﬁ ciently large number of 
samples to form the prediction with reasonable accuracy. We can use our previous 
deﬁ nition of correlation to simplify the above:
 
 R
N
x
n
N
0
1
1
2
( ) ≈
−
(
)
∑
 
 (6.32) 
 
 R
N
x n x n
N
1
1
1
( ) ≈
( )
−
(
)
∑
.  
 (6.33) 
 Hence, the optimal ﬁ rst - order predictor parameter  w0* is:
 
 w
R
R
0
1
0
*
.
=
( )
( )  
 (6.34) 
 For a better estimate, we could try to extend the prediction by using a second - order 
term. Following similar reasoning, for a second - order predictor, we have the estimate 
based on a linear combination of the last two samples
 
 
ˆ
ˆ
x n
w x n
w x n
e n
x n
x n
x n
w x n
w x n
( ) =
−
(
)+
−
(
)
∴( ) = ( )−( )
= ( )−
−
(
)+
0
1
0
1
1
2
1
−
(
)
(
)
∴
( ) =
( )−
−
(
)+
−
(
)
(
)
[
]
2
1
2
2
0
1
2
e
n
x n
w x n
w x n
.
 
 (6.35) 
 Again, over a sufﬁ ciently large number of samples, the average squared error is
 
 e
N
e
n
N
2
2
1
=
( )
∑
 
 (6.36) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.4 LINEAR PREDICTION 
179
 
 =
( )−
−
(
)+
−
(
)
(
)
(
)
∑
1
1
2
0
1
2
N
x n
w x n
w x n
N
.  
 (6.37) 
 Once again, to minimize the average error with respect to the predictor parameters 
 w 0 and  w 1 , we take derivatives. This time, we need partial derivatives with respect 
to each parameter in turn. We can employ the chain rule of calculus
 ∂
∂
= ∂
∂
∂
∂
⎛
⎝⎜
⎞
⎠⎟
e
w
e
u
u
w
to ﬁ nd the derivative
 
 ∂
∂
=
( )−
−
(
)+
−
(
)
(
)
[
] −
−
(
)
[
]
{
}
∑
e
w
N
x n
w x n
w x n
x n
N
2
0
0
1
1
2
1
2
1
. 
 (6.38) 
 Setting this derivative to zero  ∂
∂
=
(
)
e
w
2
0
0 , as we would with any minimization 
problem, it is possible to determine an expression for the optimal predictor  w0*:
 
 1
2
1
2
1
0
0
1
N
x n
w x n
w x n
x n
N
( )
[
{
−(
−
(
)+
−
(
))]× −
−
(
)
[
]} =
∑
*
*
.  
 (6.39) 
 Expanding the summation through individual terms gives
 
 
1
1
1
1
1
1
1
2
0
1
N
x n x n
w N
x n
x n
w N
x n
x n
N
N
N
( )
−
(
) =
−
(
)
−
(
)
+
−
(
)
−
(
)
∑
∑
∑
*
*
.
 
 (6.40) 
 Using autocorrelation functions as before,
 
 R
w R
w R
1
0
1
0
1
( ) =
( )+
( )
*
*
. 
 (6.41) 
 Similarly, by taking the partial derivative with respect to  w 1 yields
 
 R
w R
w R
2
1
0
0
1
( ) =
( )+
( )
*
*
.  
 (6.42) 
 Thus, we have two equations in two unknowns. In matrix form, these equations are:
 
 R
R
R
R
R
R
w
w
1
2
0
1
1
0
0
1
( )
( )
⎛
⎝⎜
⎞
⎠⎟=
( )
( )
( )
( )
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝
⎜⎜
⎞
⎠
⎟⎟
*
*
, 
 (6.43) 
 which is:
 
 r
Rw
=
*,  
 (6.44) 
where  r is a vector of correlation coefﬁ cients starting at  R (1),  R is a matrix of cor-
relation coefﬁ cients, and  w * the vector of optimal predictor coefﬁ cients, all with 
terms as deﬁ ned in Equation  6.43 . 
 Taking the  z transform of the predictor Equation  6.35 , we have:
 
 
E z
X z
X z
X z
w X z z
w X z z
X z
w z
w z
( ) =
( )−
( )
=
( )−
( )
+
( )
(
)
=
( )
−
+
−
−
−
ˆ
0
1
1
2
0
1
1
1
−
(
)
(
)
2
 
 (6.45) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
180 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 
 ∴
( )
( ) =
−
+
(
)
−
−
X z
E z
w z
w z
1
1
0
1
1
2 .  
 (6.46) 
 This gives us the required transfer function for the optimal predictor. To test this, 
we generate a data sequence of  N samples using a second - order system with complex 
poles, as shown in Listing  6.1 . This gives us a benchmark, since the optimal ﬁ lter 
ought to approach the coefﬁ cients of the system which generated the signal. 
 The poles are deﬁ ned and the coefﬁ cients calculated and stored in  a . From 
this, the system excitation (here a simple random sequence) produces the observed 
system output  x . Then we calculate the correlations and form the correlation matrix, 
and ﬁ nally solve for the coefﬁ cients. 
 Note how the output estimate is generated using the linear predictor coefﬁ -
cients. We use a ﬁ lter as deﬁ ned by Equation  6.46 , with coefﬁ cients determined 
using Equation  6.43 . 
 Listing 6.1   Testing the Linear Prediction Theory 
 N  = 1000; 
 r  = 0.9                    %  pole angle 
 omega  =  pi /10;            %  pole radius 
 p  = r  *  exp (j  * omega); 
 a  =  poly ([p  conj (p)]); 
 roots (a) 
 e  =  randn (N, 1);         %  system input 
 x  =  ﬁ lter (1, a, e);      %  response to input 
 %  calculate autocorrelations 
 R0  =  sum (x .  * x)/N; 
 R1  =  sum (x(1:N   −  1) .  * x(2:N))/N; 
 R2  =  sum (x(1:N   −  2) .  * x(3:N))/N; 
 %  autocorrelation matrix  &  vector 
 R  = [R0 R1; R1 R0]; 
 r  = [R1; R2]; 
 %  optimal predictor solution 
 w  =  inv (R)  * r; 
 %  optimal predictor parameters as a ﬁ lte r 
 ahat  = [1 ;  − w]; 
 %  predicted sample values 
 xhat  =  ﬁ lter (1, ahat, e); 
 a  = 
            1.0000         − 1.7119         0.8100 
 ahat ’  = 
            1.0000         − 1.7211         0.8287  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.4 LINEAR PREDICTION 
181
 6.4.1  Geometrical Interpretation 
 We now digress to examine a geometrical interpretation of the linear prediction 
problem. It gives the same result, but using an entirely different approach. The 
insight gained will be useful when studying optimal ﬁ ltering and other signal 
processing problems. 
 This method does not use a calculus - based minimization approach, but rather 
relies on the  orthogonality of vectors. Consider again a simple second - order predic-
tor given by
 
 ˆ
.
x n
w x n
w x n
( ) =
−
(
)+
−
(
)
0
1
1
2  
 (6.47) 
 If the prediction were perfect, we could write a matrix equation for each predicted 
sample as follows.
 
 
x
x
x N
x
x
x
x
x N
x N
2
3
1
1
0
2
1
2
3
( )
( )
−
(
)
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
=
( )
( )
( )
( )
−
(
)
−
(
)



⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
⎛
⎝⎜
⎞
⎠⎟
w
w
0
1
.  
 (6.48) 
 This may seem a little strange, but remember that the product of an  N  ×  2 matrix 
and a 2  ×   2 matrix is permissible (since the inner dimensions are both 2), and the 
result is the product of the outer dimensions, or  N  ×  2 (strictly, as we have deﬁ ned 
the terms here, ( N  −  2)  ×  2, because we assume the ﬁ rst two samples are not able 
to be predicted). 
 Here we have made the assumption that the ﬁ rst predictable sample is  x (2), 
since we need the two prior samples  x (1) and  x (0) to form a valid prediction. This 
may not always be the case; as we will see later, the output quantity (here the pre-
diction) may be formed differently, using successive overlapping blocks. The above 
equation may be written as a matrix - vector product
 
 y
Mw
=
,  
 (6.49) 
where the prediction  y is deﬁ ned to be a vector
 
 y =
( )
( )
−
(
)
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
x
x
x N
2
3
1

.  
 (6.50) 
 The matrix  M has columns formed by the input sequence, aligned to the predictor 
coefﬁ cients
 
 M =
( )
( )
( )
( )
−
(
)
−
(
)
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
x
x
x
x
x N
x N
1
0
2
1
2
3


.  
 (6.51) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
182 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 The predictor coefﬁ cients are in the form of a column vector
 
 w = ⎛
⎝⎜
⎞
⎠⎟
w
w
0
1
 
 (6.52) 
 Unless we are able to perfectly predict the next sample, there will be no values of 
 w 0 and  w 1 that satisfy equation  6.49 exactly. This is akin to the earlier example of 
the straight - line ﬁ t from measured or experimental data with error — unless the error 
is zero, the points will not lie precisely on the line of best ﬁ t. 
 Given this matrix interpretation, the quantity we seek to minimize is the dis-
tance between two vectors  y and  Mw . This is the Euclidean difference deﬁ ned as
 
 y
Mw
−
.  
 (6.53) 
 This is illustrated in Figure  6.8 . The vector  y lies above the plane, and the vector 
formed by the matrix - vector product  Mw lies on the plane. The quantity we need 
to minimize is really the distance || y  −  Mw ||, and this can only occur when it forms 
a vector perpendicular to the plane. So if we deﬁ ne the optimal solution as the one 
satisfying this criterion, the optimal value  w * is 
 
 w*
*
*
.
=
⎛
⎝
⎜⎜
⎞
⎠
⎟⎟
w
w
0
1
 
 (6.54) 
 Referring to Figure  6.8 ,  y is ﬁ xed in space, and as  w varies, the product  Mw ranges 
over the plane shown. It is said to form a subspace (the column space) of  M . 
 If  y  −  Mw is to have a minimum length, it must be perpendicular to the plane. 
Let  w * be such that  y  −  Mw * is perpendicular to the plane. Now, if two vectors are 
orthogonal, we know that their inner or vector dot product must be zero. So the inner 
(vector dot) product of  y  −  Mw with  Mw  must be zero. 
 FIGURE 6.8   Illustrating the orthogonality of vectors. The vector y is the desired output 
vector. We need to reach it as closely as possible using a vector Mw, where M is formed from 
the input samples (measured), and w is the vector of ﬁ lter weights (variable). The closest match 
is when the vector difference y  −  Mw is perpendicular to the plane deﬁ ned by Mw. In that case, 
the vector Mw * is the closest in a  “ least - squares ” sense, and w * is the optimal weight vector. 
Mw ∗
Mw
y
y − Mw
y − Mw ∗
O
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.5 NOISE ESTIMATION AND OPTIMAL FILTERING 
183
 That is
 
 Mw
y
Mw
(
)⋅
−
(
) =
*
0  
 (6.55) 
 
 ∴(
)
−
(
) =
Mw
y
Mw
T
*
.0  
 (6.56) 
 In the second line, we have used the fact that  a  ·  b  =  a T b . Since for any matrices  A 
and  B: 
 
 AB
B A
(
) =
T
T
T,  
 (6.57) 
where  T represents the transposition operator (write rows as columns, write columns 
as rows), the orthogonal solution reduces to:
 
 w M
y
Mw
T
T
−
(
) =
*
0  
 (6.58) 
 
 w
M y
M Mw
T
T
T
−
(
) =
*
.0  
 (6.59) 
 For this to hold, as long as  w T is nonzero, the term in brackets ( M T y  −  M T Mw * ) 
must be zero, and so:
 
 M Mw
M y
T
T
*
.
=
 
 (6.60) 
 Thus, the unique solution is:
 
 w
M M
M y
*
.
= (
)
−
T
T
1
 
 (6.61) 
 The quantity  M  +   =  ( M T M )  − 1 M T is also called the pseudoinverse. The following 
shows how to extend the previous linear - prediction example to implement this 
matrix - based solution. 
  
 y  = x(3:N) ; 
 M  = [x( 2:N   −  1)      x(1:N   −  2)] ; 
 w  =  inv (M ’   * M)  * M ’  * y 
 w  = 
   1.7222 
   − 0.8297  
 This is essentially a different route to the same end point. The numerical result 
as described will be slightly different to the earlier numerical result using derivatives, 
due to the way the starting points for the sequence have been deﬁ ned. 
 6.5  NOISE ESTIMATION AND OPTIMAL FILTERING 
 We now extend the above and apply the basic concepts to the problem of removing 
noise from a signal, where we have access to an estimate the noise. Such an approach 
is called a Wiener ﬁ lter. 1 Consider the problem where we want to minimize the noise 
in a signal, and we are able to sample the noise component separately. One received 
channel will comprise the signal plus noise, the other will consist predominantly of 
  1  Named after Norbert Wiener. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
184 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
noise only. Of course, the noise component is not precisely, sample - by - sample, 
identical to that received by the signal sensor. If it were, the problem would reduce 
to a simple sample - by - sample subtraction. 
 The situation is illustrated in Figure  6.9 . The noise source is correlated to some 
degree with the measured signal, and we represent this coupling via a linear transfer 
function. If we can estimate the linear transfer function, we can estimate the noise 
as received by the signal sensor, and thus subtract it out. 
 In the diagram, the output signal  ˆ( )
s n  at instant  n is
 
 ˆ( )
( )
( ),
s n
y n
n
T
=
−w x
 
 (6.62) 
where the weight vector  w is
 
 w =
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
−
w
w
wL
0
1
1

,  
 (6.63) 
and the sample vector  x n is deﬁ ned as the block of  L samples starting at instant  n 
 
 xn
x n
x n
x n
L
=
−
−
+
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
( )
(
)
(
)
.
1
1

 
 (6.64) 
 As before, we square the output to get an indicative error power
 
 ˆ ( )
( ( )
( ))
s
n
y n
n
T
2
2
=
−w x
 
 (6.65) 
 
 =
−
+
y
n
y n
n
n
T
T
2
2
2
( )
( )
( )
(
( )) .
w x
w x
 
 (6.66) 
 FIGURE 6.9  Wiener ﬁ ltering for noise cancellation. The ﬁ lter weights are updated for a 
block using both the noisy signal and the (approximately) noise - only signal. It is assumed 
that we do not have access to the clean signal, only the noise - contaminated signal. 
Furthermore, the noise signal is only an estimate, and we use its statistical properties to 
determine the optimal ﬁ lter. 
Σ
y(n) = s(n) + v(n)
ˆs(n) = s(n) + v(n) −ˆv(n)
x(n)
v(
Signal
Signal + Noise
Wiener
filter
Recovered signal
Noise
Noise
Coupling
n)
s(n)
+
−
ˆv(n)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.5 NOISE ESTIMATION AND OPTIMAL FILTERING 
185
 Now:
 
 
ˆ( )
( )
ˆ( )
( ( )
( ))
ˆ( )
( )
( ( )
ˆ( )).
s n
y n
v n
s n
v n
v n
s n
v n
v n
=
−
=
+
−
=
+
−
 
 (6.67) 
 Hence:
 
 ˆ ( )
( ( )
( )
ˆ( ))
( )
( )( ( )
ˆ( ))
( ( )
ˆ
s
n
s n
v n
v n
s n
s n v n
v n
v n
v
2
2
2
2
=
+
−
=
+
−
+
−( )) .
n
2  
 (6.68) 
 Taking the average or mathematical expectation:
 
 E s n
E s n
E s n v n
E s n v n
E v n
v n
{
( )}
{
( )}
2 { ( ) ( )}
2 { ( ) ( )}
{ ( )
( )
2
2
ˆ
ˆ
ˆ
=
+
−
+
−
) }
{
( )}
{( ( )
( )) }.
2
2
2
≈
+
−
E s n
E v n
v n
ˆ
  (6.69) 
 For perfect cancellation, the noise estimate  ˆ( )
v n  equals the noise  v ( n ), and the last 
term on the right is zero. Since  E { s 2 ( n )} is ﬁ xed, minimizing the difference between 
noise and noise estimate corresponds to minimizing the output power  E s n
{
( )}
2ˆ
. So 
we must minimize the output power, and to do this, we take Equation  6.66 , and take 
the expectation over the left - and right - hand sides to give:
 
 E s n
E y
n
E y n
n
E
n
T
T
{ ( )}
{
( )}
{ ( )
( )}
{(
( )) }
2
2
2
2
=
−
+
w x
w x
 
 (6.70) 
 
 =
−
+
σy
T
T
T
E y n
n
E
n
n
2
2 { ( )
( )}
{
( )
( ) }.
w x
w x
x
w
 
 (6.71) 
 We have a number of vector - vector and scalar - vector products here, 2 so for simplic-
ity, let us deﬁ ne the vector and matrix products:
 
 r = E y n x n
{ ( ) ( )}  
 (6.72) 
 
 R
x
x
= E
n
n
T
{ ( )
( )}.  
 (6.73) 
 So then the cost function  J that we need to minimize is:
 
 J
E s n
=
{
( )}
2ˆ
 
 (6.74) 
 
 =
−
+
σy
T
T
2
2r w
w Rw.  
 (6.75) 
 Using the identities
 
 ∂
∂(
) =
w w r
r
T
 
 (6.76) 
 
 ∂
∂
=
w w Rw
Rw
(
)
,
T
2
 
 (6.77) 
 the derivative becomes
 
 ∂
∂
= −
+
J
w
r
Rw
2
2
. 
 (6.78) 
 Setting this to zero to ﬁ nd the minimum - power solution:
 
 w
R r
=
−1 .  
 (6.79) 
  2  Recall that scalars are italic font, column vectors are lower case bold typeface, and matrices bold 
uppercase. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
186 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 Thus, we have a matrix solution to the estimation problem. The solution relies, again, 
on correlations formed as a vector and matrix. We can employ the  reshape () 
function in MATLAB to aid in forming the matrices, as shown in Listing  6.2 . In this 
example, we have a noise - free sinusoid which is  not visible, and a noise - corrupted 
sinusoid which  is visible. The noise is also measured separately, although of course 
it is only an estimate of the noise. Note how the  reshape () operator is used to 
form the sequence of  x ( n ) and  y ( n ) vectors, as matrices. 
 Listing 6.2   Testing the Wiener Filter Theory 
 N  = 10,000;                            %  number of samples in test 
 x  =  randn (1, N);                       %  noise in system 
 n  = 0:N  −  1; 
 yc  = 2.0  *  sin (2 * pi * n/(N   −  1) * 5);    %  clean signal (not visible) 
 %  noise coupling — unknown transfer function 
 b  = [1 0.8    − 0.4 0.1]; 
 v  =  ﬁ lter (b, 1, x); 
 y  = yc  + v; 
 L  = 4;   %  coefﬁ cient length 
 %  Wiener optimal solution 
 %  reshape signal vectors in order to calculate covariances 
 ym  =  reshape (y, L, N/L);     %  the output (y) vector 
 xm  =  reshape (x, L, N/L);     %  the input (x) vector 
 xv  = xm(1 ,:);                  %  y vector at intervals corresponding  
                                  % to the start of the xm ’ s above 
 R  = (xm  * xm ’ )/(N/L);         %  E(X X ˆ T) 
 r  = (ym  * xv ’ )/(N/L);         %  E(y X) 
 %  optimal weights 
 wopt  =  inv (R)  * r; 
 %  noise estimate 
 vest  =  ﬁ lter (wopt, 1, x); 
 %  error  = signal  — noise estimate 
 e  = y   −  vest; 
 wopt  = 
         1.0405 
         0.7406 
         − 0.3747 
         0.0843  
 The result is that wopt forms an estimate of b. This is shown for a block of 
noise - corrupted data in Figure  6.10 . The correlations are formed for the block of 
input data samples. 
 The process of ﬁ nding the optimal solution can be imagined in two dimensions 
as shown in Figure  6.11 as searching for the bottom of a bowl - shaped surface, where 
the two orthogonal axes form the weight vectors, and the cost function forms the 
third axis. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.5 NOISE ESTIMATION AND OPTIMAL FILTERING 
187
 FIGURE 6.10   Wiener ﬁ ltering example. The clean signal is what we desire, but it is not 
available to us. The noisy signal is available, and after the optimal ﬁ lter is applied, we have 
the signal estimate.  
Clean signal
Noisy signal
Signal estimate
 FIGURE 6.11   The performance surface in terms of the ﬁ lter coefﬁ cients. For two 
coefﬁ cients, we can imagine the error surface as a bowl - shape with the cost function as the 
vertical axis. The goal is to ﬁ nd the minimal cost, and hence the corresponding ﬁ lter 
weights. 
−2
−1
0
1
2
−2
−1
0
1
2
0
2
4
6
8
10
12
14
Weight w0
Performance surface for 2-tap filter (w0 = 0.8, w1 = −0.4)
Weight w1
Mean squared error
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
188 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 FIGURE 6.12   The tomography problem and one possible approach using the 
backprojection algorithm. The so - called head phantom (Shepp and Logan 1974) consists of 
ellipses of varying densities, intended to represent a cross - section of human tissue. This 
gives a basis on which to compare algorithms. The reconstructed interior projection is 
shown on the right. 
True image matrix f(x, y)
Backprojection image matrix b(x, y)
 6.6  TOMOGRAPHY 
 The temporal correlation of sampled data is clearly important in many signal pro-
cessing applications, such as ﬁ ltering and system identiﬁ cation as discussed in the 
previous sections. We now turn to the concept of spatial correlation, and in particular, 
we investigate the concept of tomography, which enables us to  “ see inside ” an object 
with only external measurements. This has enabled a great many medical and other 
applications in recent decades. We cover the basic principles, and introduce the class 
of algorithm for reconstruction called  backprojection . 
 Figure  6.12  gives an indicative picture of what we are trying to achieve. On 
the left we have a  “ test ” image, the so - called Shepp – Logan head phantom (Shepp 
and Logan 1974), which is intended to represent the relative densities of tissue in 
the human head. On the right is a reconstructed image, formed using only projections 
through the test image. The relative brightness of the various ellipses is intended to 
be proportional to the density encountered during passage through the object ’ s 
components.   
 In a real application, the various projections are taken by a measurement 
technique such as X - radiation of a cross - section. The projections are illustrated in 
Figure  6.13 . Clearly, many cross - sections are required to build up a reconstruction 
in two dimensions of the interior densities of the object. 
 The following sections ﬁ rst introduce the physical concepts, and then the 
signal processing approach to solving the problem. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.6 TOMOGRAPHY 
189
 6.6.1  Radon Transform 
 Each of the cross - sections taken through the object to be imaged may be thought of 
as a line integral, with the relative strength of the output signal proportional to the 
density, as illustrated in Figure  6.13 . Obviously this is a simpliﬁ cation of the under-
lying physics, but is sufﬁ cient for the purposes of developing the reconstruction 
algorithms. Further details on the physics of the measurement process may be found 
in many sources, such as Kak and Slaney (1988), Rosenfeld and Kak (1982) and 
Jain (1989). 
 The Radon transformation was developed some time ago, as a purely math-
ematical concept (see the translation in Radon 1986). The projection through an 
object is, in effect, an accumulation over each parallel ray passing through 
the object. 3 This projection results in a density cross - section, as shown. Mathema-
tically, the Radon transform provides a function  P  θ  ( s ), where  s is the perpendicular 
 FIGURE 6.13   Projection through an object composed of two ellipses. The upper plots 
are cross - sections viewed from above, showing all the projections in the direction of the 
arrows. The lower plots show the projected value, which is proportional to both the 
amount of time a ray spends in the particular ellipse, as well as the density of that 
ellipse. 
Cross−section
Cross−section
  3  There are other implementations such as fan - beam projections. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
190 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
displacement, and  θ is the angle of the source plane. The value of  P  θ  ( s ) for a particu-
lar projection is dependent on the density encountered by each ray, and is modeled 
with a point density  f ( x ,  y ) at point ( x ,  y ) within the object. The Radon transform is 
the line integral over densities  f , along the ray inclined at  θ and distance  s , as shown 
Figure  6.14 . 
 The Radon transform gives us an approach to model how  P  θ  ( s ) is formed from 
the internal densities  f ( x ,  y ). We can further detail the geometry of the situation as 
in Figure  6.14 , and write the equations describing the projection ray. In Figure  6.14 , 
the point  p represents a point traveling along the ray, and for a given projection, the 
value of angle  θ is a function of the external geometry. The ray itself is a known 
distance  s  from the origin; again, this is a function of the geometry of the projecting 
device. 
 Deﬁ ning  u as the distance in the  u  axis direction from  p to the intersection 
with the  s axis line in Figure  6.14 , we can drop perpendicular lines from  p ( x ,  y ), and 
use similar triangles for angle  θ to ﬁ nd that:
 
 x
s
u
=
−
cos
sin
θ
θ  
 (6.80) 
 
 y
s
u
=
+
sin
cos ,
θ
θ  
 (6.81) 
 or, in matrix form,
 
 x
y
s
u
⎛
⎝⎜
⎞
⎠⎟=
−
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
cos
sin
sin
cos
.
θ
θ
θ
θ
 
 (6.82) 
 FIGURE 6.14   Axes for the Radon transformation, and the ray path along which 
measurements are accumulated. Point  p moves along the ray as indicated, tracing through 
regions of various internal densities along the way. 
x
y
s
u
p(x, y)
r
s
ϕ
θ
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.6 TOMOGRAPHY 
191
 This is seen to be a rotation of the coordinate axes. We can invert this equation and 
solve for ( s ,  u )
 
 s
x
y
=
+
cos
sin
θ
θ  
 (6.83) 
 
 u
x
y
= −
+
sin
cos ,
θ
θ  
 (6.84) 
or
 
 s
u
x
y
⎛
⎝⎜
⎞
⎠⎟= −
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
cos
sin
sin
cos
.
θ
θ
θ
θ
 
 (6.85) 
 With this geometry 4 understood, and the projection measurements  P  θ  ( s ), we can 
write the Radon line integral or  “ ray - sum ” of  f ( x ,  y ) along  s at angle  θ as:
 
 P s
f s
u
s
u
du
s
θ
θ
θ
θ
θ
θ
π
( )
( cos
sin , sin
cos )
.
=
−
+
−∞<
< ∞
≤
≤
∫
0
  (6.86) 
 This describes the formation of the projection. To solve the problem, we need the 
reverse: we want the internal densities  f ( x ,  y ), which are computed indirectly from 
the projection measurements. 
 This gives us an insight into how the projections are generated in a mathemati-
cal sense. In order to determine the shape and density of internal objects that are 
hidden, we need to take additional projections. This is illustrated in Figures  6.15 and 
 6.16 , where the source plane has been rotated 90 ° . With multiple measurements, we 
ought to be able to generate an internal projection of the object, which corresponds 
mathematically to the values of  f ( x ,  y ). The situation is akin to looking at a building 
with the windows open, or trees in a park, and walking around the perimeter taking 
snapshot views at each angle, and attempting to determine a two - dimensional 
interior map from one - dimensional views. 
 6.6.2  Backprojection 
 Now that the generation of the projections is understood, we need to derive an 
algorithm for determining the internal values from these external projection mea-
surements. There are several possible approaches to this problem, including the 
solution of a set of linear equations, the approximate solution to a set of similarly 
formed equations, the use of the Fourier transform, and an accumulation algorithm 
called  backprojection . Although all but the ﬁ rst are workable in practice, the back-
projection algorithm is often employed and has the advantage of not requiring an 
iterative solution. Of course, it is not without shortcomings, too, as will be seen. 
 The development of the concepts for the backprojection approach are shown 
in Figure  6.17 . In this case, we have two measurement views, which in practice 
correspond to two rotational positions of the X - ray emission and measurement 
apparatus. The two views at differing angles  θ give us two perspectives at offset  s , 
  4  In passing, note that in the literature, different conventions exist; in particular, the rotated axes  s and  u 
are also termed  t and  s , respectively, and the projection  P  θ  ( s ) is also denoted  g ( s ,  θ ). In this text, we use 
axes  s and  u , and denote the projection as  P  θ  ( s ) so as to avoid possible confusion with  t being time. The 
use of the subscript also indicates that  θ is a discrete quantity. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 FIGURE 6.16   Projection through the same object, after the measurement apparatus has 
been rotated through 90 ° . A different projection is generated, and this gives us a second 
cross - section with which to build up a map of the object ’ s interior density. 
x
y
s
Pθ (s)
f (x, y)
θ
s
u
Source
Object
 FIGURE 6.15   The cross - sectional projection through an object. The source consists of 
parallel emitters, or one emitter traveling along the indicated path. The impediment along 
each ray from source to measurement forms the resultant density projection  P  θ  ( s ) for that 
particular angle  θ and parallel displacement from the origin  s . 
x
y
s
Pθ (s)
f (x, y)
Source
Object
θ
s
u
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.6 TOMOGRAPHY 
193
and projecting these rays back, we ﬁ nd their intersection point  I . Clearly, the inten-
sity  f ( x ,  y ) at point  I must have contributions from both measurements  P
s
θ1( ) and 
 P
s
θ2 ( ). To obtain a complete set of contributions, we need measurements over 180 ° 
in order to accumulate the value of density  f at point  I . 
 Mathematically, the backprojection may then be deﬁ ned as
 
 b x y
P x
y
d
( , )
( cos
sin )
.
=
+
∫
θ
π
θ
θ
θ
0
 
 (6.87) 
 This takes into account the value of each projection  P  θ  ( s ) at angle  θ and offset  s , 
and incorporates the axis rotations as developed earlier. The integration is performed 
over all angles  θ for a particular value of  s . One may imagine this as pivoting the 
projection apparatus around a semicircle, with each discrete angular measurement  k 
resulting in a projection  P
s
k
θ ( ) contributing to the estimate  ˆ( , )
f x y . 
 In practice, of course, we do not undertake a complete rotation for each point, 
but rather take a complete  “ slice ” of measurements to yield one set of projection 
values, and then rotate the projection system. Thus, all values of  P
s
k
θ ( ) are stored, 
the measurement apparatus rotated, and then another set of measurements taken. 
Finally, the backprojection algorithm is applied to reconstruct the interior map. 
 FIGURE 6.17   Interpretation of the backprojection algorithm for reconstructing the 
internal density of an object. Point  I is the intersection of the two projections illustrated, 
and its intensity is the sum of the two projection values.  
x
y
s
Pθ 1 (s)
θ
s
u
s
Pθ 2 (s)
I
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
194 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 Figure  6.18 shows the reconstructed image, at a lower resolution. This enables 
the projection lines to be seen in the reconstruction. Note that no additional alteration 
of the image was performed to see this — the lines are an artifact of the reconstruc-
tion. Figure  6.19 shows the backprojection reconstruction of a simple elliptical 
object with constant density. This shows one of the limitations of the approach. In 
effect, all projections are smeared across each ray through the object, and thus some 
points along a projection ray accumulate a value when they ought not to. These 
 FIGURE 6.18   An example of the backprojection algorithm for tomography. The 
Shepp – Logan head phantom image is shown on the left, with a low - resolution 
reconstruction shown on the right. The lower resolution allows the scan lines to be seen, as 
illustrated in the previous ﬁ gures. 
True image matrix f(x, y)
Backprojection image matrix b(x, y)
 FIGURE 6.19   A backprojection example, using an ellipsoidal - shaped object with 
constant density. The object is positioned at a known offset from the origin, and rotated by 
a known angle. 
True image matrix f(x, y)
Backprojection image matrix b(x, y)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.6 TOMOGRAPHY 
195
erroneous contributions decay with distance, and more projections and/or closer 
sampling of each projection does not eliminate the problem. Various solutions have 
been proposed by researchers, with the most widely used being a ﬁ ltering approach 
applied to the projections. This high - pass ﬁ ltering applied does, however, tend to 
emphasize any noise in the measurements, which is undesirable. 
 6.6.3  Backprojection Example 
 The backprojection algorithm as described can be tested using a mathematical model 
for an object. An ellipse is one such useful shape, and multiple ellipses may be 
combined as in the Shepp – Logan head phantom encountered earlier. 
 Recall that the formal deﬁ nition of an ellipse is that it is the locus of points 
equidistant from the foci. The equation of an ellipse is:
 
 x
A
y
B
2
2
2
2
1
+
= .  
 (6.88) 
 It is not difﬁ cult to verify that the  x intercept on the semi - major axis is  ± A , and that 
the  y intercept on the semi - minor axis is  ± B . 
 To be useful in tomography, we need to model the interior of the ellipse as 
well. This is most easily done by assuming a constant interior density  ρ , where the 
units of  ρ are determined by the physics of the object (but which are unimportant 
from the signal processing perspective). What we require is a set of equations to 
model the ellipse: speciﬁ cally, a projection density given the angle  θ and offset  s 
for an ellipse speciﬁ ed by parameters  A and  B . 
 This situation is depicted in Figure  6.20 . To begin with, we utilize an ellipse 
centered on the origin. We ﬁ rst derive the equations for the tangent at point ( x t ,  y t ), 
then for the ray at perpendicular offset  s as shown. The quantity  a ( θ ) is useful to 
simplify the notation.  
 FIGURE 6.20   Ellipse geometry for projection. 
A
B
x
y
(xt , yt )
xi
yi
θ
s
a(θ)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
196 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 After algebraic manipulation of the geometry as shown, we can determine the 
equation for the projection through the ellipse  P  θ  ( s ) at a given rotational angle  θ and 
offset  s for known ellipse geometry speciﬁ ed by  A and  B . The projection is
 
 P s
AB
a
a
s
θ
θ
θ
( )
( )
( )
,
=
−
2
2
2
2  
 (6.89) 
 
 where a
A
B
2
2
2
2
2
( )
cos
sin
.
θ
θ
θ
=
+
 
 (6.90) 
 So, for an assumed constant density  ρ , we can determine the projection quantity 
depending on whether the ray passes through the ellipse or not. If it passes through 
the ellipse, the projection is multiplied by the density  ρ . If not, the projection is zero, 
and so  P  θ  ( s ) becomes
 
 P s
AB
a
a
s
s
a
s
a
θ
ρ
θ
θ
θ
θ
( )
( )
( )
( )
( )
.
=
−
≤
>
⎧
⎨⎪
⎩⎪
2
0
2
2
2
for
for
 
 (6.91) 
 Next, we need to position the ellipse at any point in the measurement plane, and 
with any offset. We can effect this not be rederiving the above, but more easily by 
compensating for the rotation and translation separately. If we offset the ellipse 
center of (0, 0) by ( x o ,  y o ), as illustrated in Figure  6.21 , the distance to the center 
and angle to the center are simply 
 
 d
x
y
o
o
=
+
2
2  
 (6.92) 
 
 γ =
⎛
⎝⎜
⎞
⎠⎟
arctan y
x
o
o
.  
 (6.93) 
 FIGURE 6.21   Ellipse translated by ( x o ,  y o ). 
xo
yo
x
y
θ
γ
d cos (γ −θ)
s
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.6 TOMOGRAPHY 
197
 Next, consider a rotation by angle  α as shown in Figure  6.22 a. Clearly, the ray spends 
a longer time within the ellipse, and hence encounters proportionally more of the 
internal density  ρ . Considering the rotated axes ( x ′ ,  y ′ ), it is seen from the diagram 
that effectively, this case corresponds to the unrotated ellipse but with angle  θ  −  α . 
 Combining both translation and rotation, we have the situation depicted in 
Figure  6.22 b. From this diagram, the following substitutions can be derived. We 
simply have an altered effective angle  θ ′ according to the rotational component, 
and an effective displacement  s ′ that is reduced after translation by an amount 
 d cos( γ  −  θ ). So the substitutions we need are
 
 ′ →
−
θ
θ
α  
 (6.94) 
 
 ′ →
−
−
s
s
d cos(
),
γ
θ
 
 (6.95) 
 with  d and  γ as given in Equations  6.92 and  6.93 . The projection can then be imple-
mented using these equations as follows. We need the ellipse geometry (speciﬁ ed 
by  A and  B ), the ellipse translation (determined by [ x o ,  y o ]), the ellipse rotation  α , 
and the density within the ellipse  ρ . For a given projection angle  θ and offset  s , we 
may determine the projection  P  θ  ( s ) using the following function. 
  
 FIGURE 6.22   Ellipse rotation and translation. 
x
y
x′
y′
θ α
x
y
x o
yo
x ′
y ′
θ α
A
B
(a) Rotated by α.
(b) Both translated and rotated.
 function [P]  = ellipseproj(A, B, rho, theta, s, alpha, xo, yo) 
 gamma  =  atan2 (yo, xo); 
 d  =  sqrt ( xo  * xo  + yo  * yo); 
 thetanew  = theta   −  alpha; 
 snew  = s   −  d  *  cos ( gamma  −  theta); 
 %  use translated/rotated values 
 s  = snew; 
 theta  = thetanew; 
 %  ﬁ nd a ^ 2 (theta) 
 ct  =  cos (theta); 
 st  =  sin (theta); 
 a2  = A  * A  * ct  * ct  + B  * B  * st  * st; 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
198 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 This is the projection of one ray through the ellipse, with all possible para-
meters accounted for. The size of the ellipse  A ,  B , the ray angle  θ , the ellipse transla-
tion  x o ,  y o , ellipse angle  α , and ﬁ nally the ellipse density  ρ , must all be known. 
 We now need to take projections at discrete angles  θ k and determine the com-
posite result from all projections. Each projection forms a vector, and it is natural 
to store all projections in a matrix projmat with one projection set per row as 
follows: 
  
 function [projmat, svals, thetavals]  =  ...  
      ellipseprojmat(A, B, ntheta, ns, srange, rho, alpha, xo, yo) 
 thetamin  = 0; 
 thetamax  =  pi ; 
 %  each row is a projection at a certain angle 
 projmat  =  zeros (ntheta, ns); 
 smin  =  − srange; 
 smax  = srange; 
 dtheta  =  pi /(ntheta   −  1); 
 ds  = (smax   −  smin)/(ns   −  1); 
 svals  = smin:ds:smax; 
 thetavals  = thetamin:dtheta:thetamax; 
 pn  = 1; 
 for theta  = thetavals 
      %  calculate all points on the projection line 
       P  =  zeros (ns, 1); 
      ip  = 1; 
      for s  = svals 
           %  simple ellipse 
           [p]  = ellipseproj(A, B, rho, theta, s, alpha, xo, yo); 
           P(ip)  = p; 
           ip  = ip  + 1; 
      end 
       %  save projection as one row of matrix 
      projmat(pn, :)  = P ’ ; 
      pn  = pn  + 1; 
 end 
 atheta  =  sqrt (a2); 
 %  return value if outside ellipse 
 P  =  0; 
 if (  abs (s)   <=  atheta ) 
      %  inside ellipse 
      P  =  2  * rho  * A  * B / a2  *  sqrt (a2   −  s  * s); 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.6 TOMOGRAPHY 
199
 Finally, we are in a position to implement our backprojection algorithm. The 
backprojection operation itself was deﬁ ned in Equation  6.87 as
 
 b x y
P x
y
d
( , )
( cos
sin )
.
=
+
∫
θ
π
θ
θ
θ
0
 
 (6.96) 
 This example translates the width and height of the reconstructed image into normal-
ized coordinates in the range  ± 1. That is, real  x coordinates from  − 1 to  + 1 are 
translated to indexes 1 to  W . The translation equations are then
 x
x
W
i
=
−
−
⎛
⎝⎜
⎞
⎠⎟−
2
1
1
1,  
 for mapping image indexes to  ± 1, and
 x
x
W
i =
+
⎛
⎝⎜
⎞
⎠⎟
−
+
1
2
1
1
(
)
,  
 for mapping from real coordinate  x to the image index. For the  y coordinates, if 
we wish to use a range of  ± 1, we must incorporate the fact that the image matrix 
dimensions are 1 to H, but in fact from top (1) to bottom (H), whereas a more 
natural real - number co - ordinate system goes from  − 1 (bottom) to  + 1 (top). So  y  =  − 1 
corresponds to index H, and  y  =  + 1 corresponds to index 1. The index to real 
mapping is
 y
y
H
i
= −
−
−
⎛
⎝⎜
⎞
⎠⎟
1
2
1
1 ,  
 for index to real, and for real to index, it is
 y
y
H
i =
−
⎛
⎝⎜
⎞
⎠⎟
−
+
1
2
1
1
(
)
.  
 We incorporate all these considerations into the backprojection solution code as 
shown below. 
  
 function [b]  = bpsolve(W, H, projmat, svals, thetavals) 
 ntheta  =  length (thetavals); 
 ns  =  length (svals); 
 srange  = svals(ns); 
 b  =  zeros (H, W); 
 for iy  = 1:H 
      for ix  = 1:W 
           x  = 2  * (ix    −  1)/(W   −  1)  −  1; 
            y  = 1   −  2  * (iy   −  1)/(H    −  1); 
            %  projmat is the P values, each row is P(s) for a given theta 
           bsum  = 0; 
           for itheta  = 1:ntheta 
                theta  = thetavals(itheta); 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
200 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 We can now call the m - ﬁ les, thus developed as follows. Figure  6.19 shows the 
resulting reconstructed image using this code. 
  
 %  image parameters 
 W  = 400; 
 H  = 400; 
 %  object parameters 
 rho  = 200; 
 A  = 0.4; 
 B  = 0.2; 
 alpha  =  pi /10; 
 xo  = 0.3; 
 yo  = 0.5; 
 %  backprojection parameters 
 ntheta  = 100; 
 srange  = 2; 
 ns  = 100; 
 %  generate projections 
 [projmat, svals, thetavals]  =  ...  
    ellipseprojmat(A, B, ntheta, ns, srange, rho, alpha, xo, yo); 
 %  solve using backprojection 
 [b]  = bpsolve(W, H, projmat, svals, thetavals); 
 %  scale for image display 
 b  = b/ max ( max (b)); 
 b  = b  * 255; 
 bi  = uint8(b); 
                s  = x * cos (theta)  + y * sin (theta); 
                is  = (s  + srange)/(srange * 2) * (ns   −  1)  + 1; 
                is  =  round (is); 
                if (is    <  1) 
                     is  = 1; 
                end 
                if (is   >  ns) 
                     is  = ns; 
                 end 
                Ptheta  = projmat(itheta, is); 
                bsum  = bsum  + Ptheta; 
           end 
           b(iy, ix)  = bsum; 
      end 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
6.7 CHAPTER SUMMARY 
201
 ﬁ gure (1); 
 clf 
 image (bi); 
 colormap ( gray (256)); 
 box ( ’ on ’ ); 
 axis ( ’ off ’ );  
 6.7  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  a review of correlation theory, including auto - and cross - correlation. 
 •  the application of correlation in various signal processing problems and its 
limitations.  
 •  the key concepts of optimal ﬁ ltering, as applied to one - step prediction, as well 
as noise estimation and removal.  
 •  the key concepts of tomography, and the backprojection algorithm used to 
solve the inverse - Radon approximation.  
 PROBLEMS 
 6.1.  Using the step - by - step approach in Section  6.3.1 , determine whether autocorrelation 
is symmetrical about the origin. Repeat for cross - correlation. 
 6.2.  Verify that  conv () may be used to calculate correlation, as described in Section 
 6.3.1 . Does the order of vectors passed to the function matter? 
 6.3.  Derive an expression for the autocorrelation  R xx ( λ ) for a sinusoid  x ( t )  =  A sin  Ω t . 
 6.4.  A discrete signal sequence is denoted  x ( n ), with a corresponding offset version  x ( n  +  k ) 
at integer offset  k . Since  α 2  >  0 for real values of  α , it follows that ( x [ n ]  −  x [ n  +  k ]) 2  ≥  0.
 (a)  From this expression, take the summation over a block of length  N samples and 
write down the result by expanding the square and taking the summation 
through. 
 (b)  Assuming 
that 
the 
average 
signal 
power 
remains 
constant, 
 ∑
≈∑
−
−
x n x n
x n
k x n
k
( ) ( )
(
) (
). 
 Take the limit of the summation in (a) as  N  →  ∞ , and rewrite the summation terms 
as correlations (i.e.,  ∑
−
=
x n x n
k
R k
( ) (
)
( )). 
 (c)  From this, prove that the autocorrelation of a sequence at lag  k is always less 
than, or equal to, the autocorrelation at lag zero. 
 6.5.  Extend the second - order derivation in Equation  6.43 to a fourth - order predictor. 
How many correlation values need to be computed? What is the dimension of the 
matrix equations to be solved? 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
202 
CHAPTER 6 TEMPORAL AND SPATIAL SIGNAL PROCESSING
 6.6.  By writing each vector and matrix as its components, prove the results in Equation 
 6.76 and 6.77 used in the derivation of the Wiener ﬁ lter. 
 6.7.  The transformation in the example given for orthogonal decomposition uses a loop 
to transform each vector sample: 
 for k  = 1 : N 
 v  = X( :, k ); 
 vt  = T * v; 
 XT( :, k )  = vt; 
 end 
 What is an equivalent matrix - matrix multiplication that could effect this without a 
loop? 
 6.8.  Obtain an ECG waveform from the Signal Processing Information Base (SPIB) at 
 http://www.spib.rice.edu/spib.html . Estimate the heart rate using detection of zero -
 crossings (or crossing of any arbitrary threshold). Then perform autocorrelation on 
the waveform, and investigate whether the peak of the correlation corresponds to 
the heart period.  
 6.9.  Consider the backprojection algorithm for tomography. If 100 samples are taken at 
each slice, at increments of 2 degrees, over the range 0 – 180 degrees, estimate how 
much memory is required to store all the projections. If the projection slice is to be 
imaged using a 1,000  ×  1,000 image, what are the computations required for each 
pixel? 
 6.10.  Using the projection diagram of Figure  6.14 , prove Equations  6.80 and  6.81 . 
 6.11.  Implement the MATLAB code as developed in Section  6.6.3 . Instead of one simple 
ellipse, use three nonoverlapping ellipses. What happens if the ellipses overlap? 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
  
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER  7 
FREQUENCY ANALYSIS 
OF SIGNALS 
 7.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to:
 1.  deﬁ ne the  Fourier series and derive equations for a given time series. 
 2.  deﬁ ne the  Fourier transform and the relationship between its input and 
output. 
 3.  scale and  interpret a Fourier transform output. 
 4.  explain the use of frequency  window functions . 
 5.  explain the derivation of the  fast Fourier transform (FFT) , and its computa-
tional advantages. 
 6.  deﬁ ne the  discrete cosine transform (DCT) and explain its suitability for data 
compression applications.     
 7.2  INTRODUCTION 
 This chapter introduces techniques for determining the frequency content of signals. 
This is done primarily via the  Fourier transform , a fundamental tool in digital signal 
processing. We also introduce the related but distinct  DCT) , which ﬁ nds a great many 
applications in audio and image processing. 
 7.3  FOURIER SERIES 
 The  Fourier series 1 is an important technique for analyzing the frequency content 
of a signal. An understanding of the Fourier series is crucial to understanding a 
number of closely related  “ transform ” techniques. Used in reverse, it may also 
203
  1  Named after the French mathematician, Jean Baptiste Joseph Fourier. 
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
204 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
be used to synthesize arbitrary periodic waveforms — those that repeat themselves 
over time. 
 A  periodic waveform is one which repeats itself over time, as illustrated in 
Figure  7.1 . Many signals contain not one, but multiple periodicities. Figure  7.2 
shows some naturally occurring data: the monthly number of sunspots for the years 
1749 – 2010 (SIDC - team 1749 – 2010). An algorithm ought to be able to determine 
the underlying periodic frequency component(s) of any such signal.  
 The sunspot observation signal appears to have some periodicity, but determin-
ing the time range of this periodicity is difﬁ cult. Furthermore, there may in fact be 
more than one underlying periodic component. This is difﬁ cult, if not impossible, 
to spot by eye. 
 Mathematically, a waveform which repeats over some interval   τ may be 
described as  x ( t )  =  x ( t  +  τ ). The  period of the waveform is   τ . For a fundamental 
frequency of  Ω o radians/second or  f o Hz,
 FIGURE 7.1   A simple periodic waveform. In this simple case, it is easy to visually 
determine the time over which the waveform repeats itself. However, there may be multiple 
underlying periodic signals present. 
 FIGURE 7.2   Historical sunspot count data from SIDC - team (1749 – 2010). The signal 
appears to have some underlying periodicity, but it is difﬁ cult to be certain using direct 
observation of the waveform alone. 
1750
1800
1850
1900
1950
2000
0
50
100
150
200
250
300
Year
Sunspots
Historical sunspot data 1749–2010
http://sidc.oma.be/sunspot−data/
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.3 FOURIER SERIES 
205
 
 Ωo = 2π
τ
 
 (7.1) 
 
 τ = 1
fo
 
 (7.2) 
 Fourier ’ s theorem states that  any periodic function  x ( t ) may be decomposed into an 
inﬁ nite series of sine and cosine functions:
 
 
x t
a
a
t
a
t
a
t
b
t
b
t
b
o
o
o
o
o
( )
cos
cos
cos
sin
sin
=
+
+
+
+
+
+
0
1
2
3
1
2
2
3
2
Ω
Ω
Ω
Ω
Ω

3
0
0
0
1
3
sin
cos
sin
.
Ω
Ω
Ω
o
k
k
k
t
a
a
k
t
b
k
t
+
=
+
+
(
)
=
∞
∑
  
 (7.3) 
 The coefﬁ cients  a k and  b k are determined by solving the following integral equations, 
evaluated over one period of the input waveform.
 
 a
x t dt
0
0
1
= ∫
τ
τ
( )
 
 (7.4) 
 
 a
x t
k
t dt
k
o
= ∫
2
0
τ
τ
( )cos Ω
 
 (7.5) 
 
 b
x t
k
t dt
k
o
= ∫
2
0
τ
τ
( )sin
.
Ω
 
 (7.6) 
 The integration limits could equally be  −τ
2
 to  + τ
2
. This would still be over one period 
of the waveform, but with a different starting point. Using complex numbers, it is 
also possible to represent the Fourier series expansion more concisely as the series:
 
 x t
c e
k
k
t
k
k
o
( ) =
=−∞
=+∞
∑
j Ω  
 (7.7) 
 
 c
x t e
dt
k
k
t
o
=
−
∫
1
0
τ
τ
( )
j Ω
.  
 (7.8) 
 7.3.1  Fourier Series Example 
 To illustrate the application of the Fourier series, consider a square wave with period 
 τ  =  1 second and peak amplitude  ± 1 ( A  =  1) as shown in Figure  7.3 . The waveform 
is composed of a value of  + A for  t  =  0 to  t = τ 2, followed by a value of  − A for 
 t = τ 2 to  t  =  τ . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
206 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 The coefﬁ cient  a 0 is found from Equation  7.4 as:
 
 
a
x t dt
Adt
A dt
A t
A t
t
t
t
0
0
0
2
0
2
2
1
1
1
2
=
=
+
−
=
+ −
∫
∫
∫
=
=
=
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
τ
( )
(
)
=
−
⎛
⎝⎜
⎞
⎠⎟−
−
⎛
⎝⎜
⎞
⎠⎟
=
A
A
τ
τ
τ
τ
τ
2
0
2
0.
 
 (7.9) 
 The coefﬁ cients  a k are found from Equation  7.5 :
 
 
a
x t
k
t dt
A
k
t dt
A
k
t dt
k
o
o
o
=
=
+
−
∫
∫
∫
2
2
0
2
0
2
2
τ
τ
τ
τ
τ
τ
τ
( )cos
cos
(
)cos
Ω
Ω
Ω
=
−
=
=
=
=
=
2
1
2
1
2
1
2
0
2
2
A
k
k
t
A
k
k
t
A
k
k
o
o
t
t
o
o
t
t
τ
τ
τ
τ
π
τ
τ
τ
Ω
Ω
Ω
Ω
sin
sin
sin
2
2
0
2
1
2
2
2
2
π
τ
τ
τ
τ
π
π
τ τ
π
τ
τ
π
−
⎛
⎝⎜
⎞
⎠⎟
−
−
⎛
⎝⎜
⎞
⎠⎟
=
sin
sin
sin
sin
A
k
k
k
A
k
kπ
π
π
π
−
−
(
)
=
A
k
k
k
sin
sin
.
2
0
 
 (7.10) 
 FIGURE 7.3   Approximating a square waveform with a Fourier series. The Fourier series 
approximation to the true waveform is shown. The Fourier series has a limited number of 
components, hence is not a perfect approximation. 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−2
−1
0
1
2
Time
Amplitude
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.3 FOURIER SERIES 
207
 In a similar way, the coefﬁ cients  b k are found from Equation  7.6 :
 b
x t
k
t dt
k
o
= ∫
2
0
τ
τ
( )sin
,
Ω
 
and may be shown to equal
 
 b
A
k
k
k =
−
2
1
π
π
(
cos
).  
 (7.11) 
 When the integer  k is an odd number (1, 3, 5,  … ). the value cos  k π  =  − 1, and hence 
this reduces to:
 
 b
A
k
k
k =
=
4
1 3 5
π
, , ,
,
…  
 (7.12) 
 when  k is an even number (2, 4, 6,  … ), the value cos  k π  =  1, and hence the equation 
for  b k reduces to 0. The completed Fourier series representation obtained by substi-
tuting the speciﬁ c coefﬁ cients for this waveform (Equation  7.11 ) into the generic 
Fourier series Equation  7.3 , giving
 
 x t
A
t
t
k
k
( )
sin
sin
=
+
+
⎛
⎝
⎜
⎜⎜
⎞
⎠
⎟
⎟⎟
=
=
4
1
12
1
3
3 2
1
3
π
π
τ
π
τ
 



 



… .  
 (7.13) 
 The  components of the Fourier series in this case are shown in Figure  7.4 . These 
are the sine and cosine waveforms of each harmonic frequency, weighted by the 
coefﬁ cients as determined mathematically. Added together, these form the Fourier 
series approximation. The magnitudes of these components are identical to the 
coefﬁ cients of the sin( · ) and cos( · ) components derived mathematically. Figure  7.4 
shows that the cos( · ) components are all zero in this case. Note that this will not 
happen for all waveforms; it depends on the symmetry of the waveform, as will be 
demonstrated in the next section. 
 FIGURE 7.4   Approximating a square waveform with a Fourier series. The original 
waveform and its approximation was shown in Figure  7.3 . The component sine and cosine 
waves are shown at the left, and their respective magnitudes are shown on the right. 
0 0.2 0.4 0.6 0.8
1 1.2 1.4 1.6 1.8
2
−1.5
−1
−0.5
0
0.5
1
1.5
Time
Amplitude
Components
0
1
2
3
4
5
6
7
8
9
−2
−1
0
1
2
Coefficient number
a(k) (cosine) coefficients
0
1
2
3
4
5
6
7
8
9
−2
−1
0
1
2
Coefficient number
b(k) (sine) coefficients
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
208 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 7.3.2  Another Fourier Series Example 
 To investigate the general applicability of the technique, consider a second example 
waveform as shown in Figure  7.5 . This is a so - called  sawtooth waveform, in this 
case with a phase shift. To see that it has a phase shift, imagine extrapolating the 
waveform backwards for negative time. The extrapolated waveform would not be 
symmetrical about the  t  =  0 axis. Again, it is seen that the sine plus cosine approxi-
mation is valid, as the approximation in Figure  7.5 shows. The components which 
make up this particular waveform are shown in Figure  7.6 . Furthermore, as Figure 
 7.6 shows, we need both sine and cosine components in varying proportions to make 
up the approximation. 
 The code shown in Listing  7.1 implements the equations to determine the 
Fourier series coefﬁ cients, and plots the resulting Fourier approximation for 10 terms 
 FIGURE 7.5   Approximating a shifted sawtooth waveform with a Fourier series. The 
Fourier series approximation to the true waveform is shown. The Fourier series only has a 
limited number of components, hence is not a perfect approximation. The components are 
shown in Figure  7.6 . 
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
−2
−1
0
1
2
Time
Amplitude
 FIGURE 7.6   Approximating a shifted sawtooth waveform with a Fourier series. The 
component sine and cosine waves are shown at the left, and their respective magnitudes are 
shown on the right. 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Time
Amplitude
Components
0
1
2
3
4
5
6
7
8
9
−1
−0.5
0
0.5
1
Coefficient number
a(k) (cosine) coefficients
0
1
2
3
4
5
6
7
8
9
−1
−0.5
0
0.5
1
Coefficient number
b(k) (sine) coefficients
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.4 HOW DO THE FOURIER SERIES COEFFICIENT EQUATIONS COME ABOUT? 
209
 Listing 7.1   Generating a Fourier Series Approximation 
 dt  = 0.01; 
 T  = 1; 
 t  = [0:dt:T] ’ ; 
 omega0  = 2 * pi /T; 
 N  =  length (t); 
 N2  =  round (N/2); 
 x  = ones(N, 1); 
 x (N2  + 1:N)  =  − 1 * ones(N    −  N2, 1); 
 a(1)  = 1/T * ( sum (x) * dt); 
 xfs  = a(1) * ones( size (x)); 
 for k  = 1:10 
      ck  =  cos (k * omega0 * t);                    %  cosine component 
      a(k  + 1)  = 2/T * ( sum (x. * ck) * dt); 
      sk  =  sin (k * omega0 * t);                    %  sine component 
      b(k  + 1)  = 2/T * ( sum (x. * sk) * dt); 
      %  Fourier series approximation 
      xfs  = xfs + a(k + 1) * cos (k * omega0 * t) + b(k + 1) * sin (k * omega0 * t); 
      plot (t, x,  ’  − ’ , t, xfs,  ’ : ’ ); 
      legend ( ’ desired ’ ,  ’ approximated ’ ); 
      drawnow ;  pause (1); 
 end 
in the expansion. A square waveform is shown, but other types of waveform can be 
set up by changing the preamble where the sample values in the x vector are initial-
ized. The Fourier series calculation component need not be changed in order to 
experiment with different wave types. 
 7.4  HOW DO THE FOURIER SERIES COEFFICIENT 
EQUATIONS COME ABOUT? 
 We have so far applied the Fourier series equations to determine the coefﬁ cients for 
a given waveform. These equations were stated earlier without proof. It is instruc-
tive, however, to see how these equations are derived. 
 For convenience, we restate the basic Fourier waveform approximation, 
which is
 x t
a
a
k
t
b
k
t
k
k
k
( )
cos
sin
=
+
+
(
)
=
∞
∑
0
0
0
1
Ω
Ω
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
210 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 with coefﬁ cients  a k and  b k , for  k  =  1, 2,  ...  . To derive the coefﬁ cients, we take the 
integral of both sides over one period of the waveform.
 
 
x t dt
a dt
a
k
t dt
b
k
t dt
k
k
k
k
( )
cos
cos
.
0
0
0
0
0
1
0
0
1
τ
τ
τ
τ
∫
∫
∫
∑
∫
∑
=
=
+
=
∞
=
∞
Ω
Ω
 
 (7.14) 
 The sine and cosine integrals over one period are zero, hence:
 
 
x t dt
a
a
x t dt
( )
( )
.
0
0
0
0
0
0
1
τ
τ
τ
τ
∫
∫
=
+
+
∴
=
 
 (7.15) 
 This yields a method of computing  a 0 . Next, multiply both sides by cos  n Ω o t (where 
 n is an integer) and integrate from 0 to  τ ,
 
 
x t
n
t dt
a
n
t dt
a
k
t
n
t dt
o
o
k
o
o
k
( )cos
cos
cos
cos
Ω
Ω
Ω
Ω
0
0
0
0
1
τ
τ
τ
∫
∫
∫
∑
=
+
=
∞
+
=
+
+
∴
=
∫
∑
∫
=
∞
b
k
t
n
t dt
x t
n
t dt
a
a
k
o
o
k
o
n
n
sin
cos
( )cos
Ω
Ω
Ω
0
1
0
0
2
0
2
τ
τ
τ
τ
τ
x t
n
t dt
o
( )cos
.
Ω
0∫
 
 (7.16) 
 Note that several intermediate steps have been omitted for clarity — terms such as 
 ∫0
0
0
τ a
k
t
n
t dt
k cos
cos
Ω
Ω
 end up canceling out for  n  ≠  k . The algebra is straightfor-
ward, although a little tedious. 
 Similarly, by multiplying both sides by sin  n Ω o t and integrating from 0 to   τ , 
we obtain
 
 
x t
n
t dt
a
n
t dt
a
k
t
n
t dt
o
o
k
k
o
o
( )sin
sin
cos
sin
Ω
Ω
Ω
Ω
0
0
0
1
0
τ
τ
τ
∫
∫
∑
∫
=
+
=
∞
k
k
o
o
k
o
n
b
k
t
n
t dt
x t
n
t dt
b
=
∞
=
∞
∑
∫
∑
∫
+
=
+
+
1
0
1
0
0
0
2
sin
sin
( )sin
Ω
Ω
Ω
τ
τ
τ
∴
= ∫
b
x t
n
t dt
o
0
0
2
τ
τ
( )sin
.
Ω
 
 (7.17) 
 Replacing  n with  k in the above equations completes the derivation. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.5 PHASE-SHIFTED WAVEFORMS 
211
 7.5  PHASE - SHIFTED WAVEFORMS 
 The sine and cosine terms in the Fourier series may be combined into a single 
sinusoid with a phase shift using the relation:
 
 a
k
t
b
k
t
A
k
t
k
o
k
o
o
cos
sin
sin(
).
Ω
Ω
Ω
+
=
+ ϕ
 
 (7.18) 
 To prove this and ﬁ nd expressions for A and  φ , let
 a
b
A
cos
sin
sin(
).
θ
θ
θ
ϕ
+
=
+
 
 Expanding the right - hand side:
 
a
b
A
A
A
A
cos
sin
sin cos
cos sin
cos sin
sin cos .
θ
θ
θ
ϕ
θ
ϕ
ϕ
θ
ϕ
θ
+
=
+
=
+
 
 Equating the coefﬁ cients of cos  θ and sin  θ in turn,
 a
A
=
sinϕ  
 b
A
=
cos .
ϕ  
 Dividing, we get an expression for the phase shift:
 
a
b
A
A
a
b
=
=
∴
=
−
sin
cos
tan
tan
.
ϕ
ϕ
ϕ
ϕ
1
 
 The magnitude is found by squaring and adding terms,
 
a
b
A
A
A
A
A
a
b
2
2
2
2
2
2
2
2
2
2
2
2
+
=
+
=
+
(
)
=
∴
=
+
sin
cos
sin
cos
.
ϕ
ϕ
ϕ
ϕ
 
 To illustrate these equations involving phase shift, consider the following MATLAB 
code. The result, shown in Figure  7.7 , illustrates that a summation of sine and cosine 
can indeed be composed of a single phase - shifted sinusoid. 
 t  = 0:0.01:1; 
 a  = 3; 
 b  = 2; 
 omega  = (2 * pi ) * 4; 
 y1c  = a * cos (omega * t); 
 y1s  = b * sin (omega * t); 
 %  sine  +  cosine directly 
 y1  = a * cos (omega * t)  + b * sin (omega * t); 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
212 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 FIGURE 7.7   Converting a sine plus cosine waveform to a sine with phase shift. 
Direct—sum of sine and cosine
Indirect—phase–shifted sine
Time
Amplitude
Amplitude
0
0
0
0
0.1
0.1
0.2
0.2
0.3
0.3
0.4
0.4
0.5
0.5
0.6
0.6
0.7
0.7
0.8
0.8
0.9
0.9
1
1
2
2
4
4
–2
–2
–4
–4
 7.6  THE FOURIER TRANSFORM 
 The Fourier transform is closely related to the Fourier series described in the previ-
ous section. The fundamental difference is that the requirement to have a periodic 
signal is now relaxed. The Fourier transform is one of the most fundamental 
algorithms — if not  the fundamental algorithm — in digital signal processing. 
 7.6.1  Continuous Fourier Transform 
 The continuous - time Fourier transform allows us to convert a signal  x ( t ) in the time 
domain into its frequency domain counterpart  X ( Ω ), where  Ω is the true frequency 
in radians per second. Note that  both signals are continuous. The Fourier transform, 
or (more correctly) the continuous - time/continuous - frequency Fourier transform, is 
deﬁ ned as
 ﬁ gure (1); 
 plot (t, y1); 
 %  sine with phase shift 
 phi  =  atan2 (a, b); 
 A  =  sqrt (a * a  + b * b); 
 y2  = A * sin (omega * t  + phi); 
 ﬁ gure (2); 
 plot (t, y2);  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
213
 
 X
x t e
dt
t
( )
( )
.
Ω
Ω
=
−
−∞
∞∫
j
 
 (7.19) 
 The signal  x ( t ) is multiplied by a complex exponential  e
t
−jΩ . This is really just a 
concise way of representing sine and cosine, since Euler ’ s formula for the complex 
exponential gives us  e
t
t
t
−
=
−
j
j
Ω
Ω
Ω
cos
sin
. 
 The functions represented by the  e
t
−jΩ  term are sometimes called the  basis 
functions , as depicted in Figure  7.8 . In effect, for a given frequency  Ω , the signal 
 x ( t ) is multiplied by the basis functions at that frequency, with the result integrated 
( “ added up ” ) to yield an effective  “ weighting ” of that frequency component. This 
is repeated for all frequencies (all values of  Ω ) — or at least, all frequencies of inter-
est. The result is the  spectrum of the time signal. As will be seen later with some 
examples, the basis functions may be interpreted on the complex plane as shown in 
Figure  7.9 . 
 The inverse operation — to go from frequency  X ( Ω ) to time  x ( t ) — is:
 
 x t
X
e
d
t
( )
( )
.
=
−∞
∞∫
1
2π
Ω
Ω
Ω
j
 
 (7.20) 
 Note that the difference is in the exponent of the basis function, and the division by 
2 π . The latter is related to the use of radian frequency, rather than Hertz (cycles per 
second). 
 These two operations — Fourier transform and inverse Fourier transform — are 
completely reversible. That is, taking any  x ( t ), computing its Fourier transform, and 
then computing the inverse Fourier transform of the result, returns the original 
signal. 
 FIGURE 7.8   Sinusoidal basis functions for Fourier analysis: cosine and negative sine. 
cosine
–sine
0
0
0
0
π/2
π/2
π
π
3π/2
3π/2
2π
2π
0.5
0.5
1
1
–0.5
–0.5
–1
–1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
214 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 7.6.2  Discrete - Time Fourier Transform 
 The next stage is to consider a sampled signal. When a signal  x ( t ) is sampled at time 
instants  t  =  nT , resulting in the discrete sampled  x ( n ), what happens to the spectrum 
of the signal? The Fourier transform of a continuous signal is:
 X
x t e
dt
t
( )
( )
.
Ω
Ω
=
−
−∞
+∞∫
j
 
 The sampling impulses are impulse or  “ delta ” functions  δ ( t ) spaced at intervals 
of  T :
 r t
t
nT
n
( )
(
).
=
−
=−∞
+∞
∑δ
 
 The sampled function is the product of the signal at the sampling instants, and the 
sampling impulses:
 
 
x t
x t r t
x t
t
nT
s
n
( )
( ) ( )
( ) (
).
=
=
−
=−∞
+∞
∑
δ
 
 (7.21) 
 FIGURE 7.9   The sine/cosine basis functions may be viewed as a vector on the complex 
plane. 
−
Point on the
complex plane
sine (    )
cosine
 Fourier Transform: 
 Continuous time  →  Continuous frequency 
 To summarize the fundamental result from this section, we have: 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
215
 Since at the sample instants  t  =  nT , this becomes:
 x t
x nT
t
nT
s
n
( )
(
) (
).
=
−
=−∞
+∞
∑
δ
 
 The Fourier transform of the sampled signal is thus:
 
 
X
x t e
dt
x nT
t
nT e
dt
x n
s
t
t
n
( )
( )
(
) (
)
(
Ω
Ω
Ω
=
=
−
=
−
−∞
+∞
−
=−∞
+∞
−∞
+∞
∫
∑
∫
j
j
δ
T
t
nT e
dt
x nT e
t
n
n T
n
) (
)
(
)
.
δ
−
=
−
−∞
+∞
=−∞
+∞
−
=−∞
+∞
∫
∑
∑
j
j
Ω
Ω
 
 (7.22) 
 Note that the delta function has been used here, and the property that:
 
 
δ t
f t dt
f t
k
k
( ) ( )
=
( )
−∞
+∞∫
.  
 (7.23) 
 Since  t  =  nT (time) and  ω  =  Ω T (frequency), this yields the Fourier transform of a 
 sampled signal as
 
 X
x n e
n
n
( )
( )
.
ω
ω
=
−
=−∞
+∞
∑
j
 
 (7.24) 
 with
 
 t
n
T
seconds
samples
seconds
sample
=
×
 
 (7.25) 
and
 
 ω radians
sample
radians
second
seconds
sample
=
×
Ω
T
 
 (7.26) 
 The (continuous) Fourier transform of a sampled signal as derived above shows that 
since the complex exponential function is periodic, the spectrum will be periodic — it 
repeats cyclically, about intervals centered on the sample frequency. Furthermore, 
the frequency range  fs 2 to  f s is a mirror image of the range 0 to  fs 2. The discrete 
time Fourier transform (DTFT) may be visualized directly as the multiplication by 
basis function as illustrated in Figure  7.10 . The difference is that the basis functions 
of sine and cosine are multiplied by the sample values  x ( n ). The frequency  ω is, 
however, free to take on any value (it is  not sampled). 
 Since the frequency spectrum is still continuous, the inverse of the DTFT 
above is
 
 x n
X
e
d
n
( )
( )
=
−∫
1
2π
ω
ω
ω
π
π
j
 
 (7.27) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
216 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 7.6.3  The Discrete Fourier Transform (DFT) 
 The discrete - time/discrete - frequency Fourier transform — usually known as just the 
discrete Fourier transform or DFT — is the ﬁ nal stage in the conversion to an all -
 sampled system. As well as sampling the time waveform, only discrete frequency 
points are calculated. The DFT equation is:
 
 X k
x n e
n
n
N
k
( )
( )
,
=
−
=
−
∑
j ω
0
1
 
 (7.28) 
where
 
 ω
π
k
k
N
k
=
=
2
frequency of the 
sinusoid
th
.
 
 (7.29) 
 FIGURE 7.10   Basis functions for the DFT. 
1 cycle
1.5 cycles
2 cycles
10 cycles
0
π/2
π
3π/2
2π
0
π/2
π
3π/2
2π
0
π/2
π
3π/2
2π
0
π/2
π
3π/2
2π
 Discrete Time Fourier Transform: 
 Sampled time  →  Continuous frequency 
 To summarize the fundamental result from this section, we have 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
217
 The discrete basis functions are now at points on the complex plane, or  1⋅
−
e
n
k
j ω , as 
illustrated in Figure  7.11 . This gives rise to the result that the DFT is now cyclic, 
and in effect, only the frequencies up to  ω  =  π are of interest. A consequence of the 
frequency points going all the way around the unit circle is that when we take the 
DFT, the upper half of the resulting components are a mirror image of the lower 
half. This will be illustrated later using some examples.  
 Recall that the  z transform was deﬁ ned in Section  5.4 as the expansion:
 
 
F z
f
z
f
z
f
z
f n z n
n
( )
( )
.
=
( )
+
( )
+
( )
+
=
−
−
−
=
∞
∑
0
1
2
0
1
2
0

 
 (7.30) 
 Comparing this with the deﬁ nition of the DFT in Equation  7.28 , it may be seen that 
the DFT is really a special case of the  z transform with the substitution
 
 z
e
k
=
jω .  
 (7.31) 
 That is, the DFT is the  z transform evaluated at the discrete points around the unit 
circle deﬁ ned by the frequency  ω . Understanding this result is crucial to understand-
ing the applicability and use of Fourier transforms in sampled - data systems. 
 As in previous cases, there is an inverse operation to convert from the fre-
quency domain back to the time domain samples. In this case, it is the inverse DFT 
to convert frequency samples  X ( k ) back to time samples  x ( n ), and is given by:
 
 x n
N
X k e n
k
N
k
( )
( )
.
=
=
−
∑
1
0
1
j ω  
 (7.32) 
 FIGURE 7.11   DFT basis functions as points on the unit circle. This is described by a 
complex exponential  e
k
jω . 
ωk =
2πk
N
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
218 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 Note that the forward and inverse operations are quite similar: the difference is in 
the sign of the exponent and the division by  N . 
 To summarize the fundamental result from this section, we have: 
 
 Discrete Fourier Transform: 
 Sampled time  →  Sampled frequency 
 7.6.4  The Fourier Series and the DFT 
 Clearly, the Fourier series and Fourier transform are related, and we now wish to 
explore this idea. Recall that the Fourier series for a continuous function  x ( t ) was 
shown in Section  7.3 to be:
 
 
x t
a
a
t
a
t
a
t
b
t
b
t
b
o
o
o
o
o
( )
cos
cos
cos
sin
sin
=
+
+
+
+
+
+
0
1
2
3
1
2
2
3
2
Ω
Ω
Ω
Ω
Ω

3
0
0
0
1
3
sin
cos
sin
Ω
Ω
Ω
o
k
k
k
t
a
a
k
t
b
k
t
+
=
+
+
(
)
=
∞
∑
  
 (7.33) 
 with coefﬁ cients  a k and  b k :
 
 a
x t dt
0
0
1
= ∫
τ
τ
( )
 
 (7.34) 
 
a
x t
k
t dt
k
o
= ∫
2
0
τ
τ
( )cos Ω
 
 (7.35) 
 
b
x t
k
t dt
k
o
= ∫
2
0
τ
τ
( )sin Ω
. 
 (7.36) 
 As before, the integration limits could equally be  −τ 2 to  + τ 2. Also, the complex 
form is:
 
 x t
c e
k
k
t
k
k
o
( )
.
=
=−∞
=+∞
∑
j Ω
 
 (7.37) 
 with coefﬁ cients:
 
 c
x t e
dt
k
k
t
o
=
−
∫
1
0
τ
τ
( )
.
j Ω
 
 (7.38) 
 The discrete Fourier transform was shown to be
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
219
 
 X k
x n e
n
n
N
k
( )
( )
,
=
−
=
−
∑
j ω
0
1
 
 (7.39) 
 with  ω
π
k
k
N
= 2
, which is just the frequency of the  k th sinusoid. 
 Comparing the  c k terms in the Fourier series equation with the  X ( k ) terms in 
the DFT equation, it may be seen that they are essentially the same except for divi-
sion by the total time in the Fourier series. The DFT is really a discretized version 
of the Fourier series, with  N samples spaced  dt apart. In the limit as  T  →  0, the DFT 
becomes a continuous Fourier transform. 
 By expanding the sine - cosine Fourier series equations and the complex -
 number form, it may be observed that there is a correspondence between the 
terms:
 
a
c
a
e c
b
m c
k
k
k
k
0
0
2
2
=
= +
{ }
= −
{ }
R
I
.
 
 The division by  τ (total waveform period) in the Fourier series equations becomes 
division by  N in the Fourier transform equations. So, the Fourier transform outputs 
may be interpreted as scaled Fourier series coefﬁ cients:
 
a
N X
a
N
e X k
b
N
m X k
k
k
0
1
0
2
2
=
= +
{
}
= −
{
}
( )
( )
( ) .
R
I
 
 Another way to see the reason for the factor of two is to consider the  “ folding ” from 
0 to  N 2
1
− and  N 2 to  N  −  1. Half the  “ energy ” of the terms is in the lower half 
of the output, the other half is in the upper half of the output. An easy solution is to 
take the ﬁ rst half of the transform coefﬁ cients and to double them. The ﬁ rst coef-
ﬁ cient  X (0) is  not doubled, as its counterpart is the very ﬁ rst coefﬁ cient  after the 
output sequence, or the  N th coefﬁ cient. This will be illustrated shortly with some 
examples. 
 Note that it is important not to misunderstand the relationship between the 
Fourier series and the DFT. The Fourier series builds upon harmonics (integer mul-
tiples) of a fundamental frequency. The DFT dispenses with the integer harmonic 
constraint, and the limit of frequency resolution is dictated by the number of sample 
points. 
 7.6.5  Coding the DFT Equations 
 Some examples will now be given on the use and behavior of the DFT. First, we 
need a MATLAB implementation of the DFT equations: 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
220 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 For the sake of efﬁ ciency, the loops may also be coded in MATLAB using 
vectorized operations as follows (vectorizing code was covered in Chapter  2 — the 
numerical result does not change, but the calculation time is reduced). 
 
 %  MATLAB vectorized version 
 function [XX]  = dftv(x) 
 x  = x(:); 
 N  =  length (x); 
 XX  =  zeros (N, 1); 
 n  = [0:N  −  1] ’ ; 
 for k  = 0:N   −  1 
      wk  = 2 * pi * k/N; 
      XX(k  + 1)  =  sum (x. * exp ( − j * n * wk)); 
 end 
 7.6.6  DFT Implementation and Use: Examples in MATLAB 
 So, what about the relationship between the DFT component index and the actual 
frequency? The DFT spans the relative frequency range from 0 to 2 π radians 
per sample, which is equivalent to a real frequency range of 0  –  f s . There are  N  +  1 
components in this range, hence the spacing in frequency of each is  f
N
s
. The 
 “ zeroth ” component is the zero - frequency or  “ average ” value. The ﬁ rst component 
is one  N th of the way to  f s . This is reasonable, since a waveform of exactly one 
sinusoidal cycle over  N components has a frequency of one  N th of  f s . 
 The scaling in time and frequency of the DFT components will now be illus-
trated by way of some MATLAB examples. The following examples use the built - in 
MATLAB function  fft (), or fast Fourier transform. The details of this algorithm 
will be explained in Section  7.12 , but for now it is sufﬁ cient to know that it is identi-
cal to the DFT,  provided the number of input samples is a power of 2 . 
 %  function [ XX ]  =  dft ( x ) 
 % 
 %  compute the DFT directly 
 %  x  =  column vector of time samples (real) 
 %  XX  =  column vector of complex – valued 
 %      frequency – domain samples 
 function [XX]  = dft(x) 
 N  =  length (x); 
 XX  =  zeros (N, 1); 
 for k  = 0:N   −  1 
      wk  = 2 * pi * k/N; 
      for n  = 0:N   −   1 
            XX(k  + 1)  = XX(k  + 1)  + x(n  + 1) * exp ( − j * n * wk); 
      end 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
221
 To begin with, recall some basic sampling terminology and equations from 
Chapter  5 . A sinusoidal waveform was mathematically described as:
 x t
A
t
( )
sin
.
=
Ω  
 Sampled at  t  =  nT , this became the samples  x ( n ) at instant  n :
 
x n
A
nT
A
n T
( )
sin
sin
.
=
=
Ω
Ω
 
 Deﬁ ning  ω as
 ω = ΩT,  
 we have:
 
 x n
A
n
( )
sin
,
=
ω  
 (7.40) 
where  ω is measured in radians per sample. 
 For one complete cycle of the waveform in  N samples, the corresponding angle 
 ω swept out in one sample period will be just  ω
π
= 2
N. 
 7.6.6.1  DFT Example 1: Sine Waveform  To begin with a simple signal, we 
use MATLAB to generate one cycle of a sinusoidal waveform, and analyze it using 
the DFT. Since there is only one sinusoidal component, the resulting DFT array  X ( k ) 
should have only one component. Sixty - four samples are used for this example: 
 
 N  = 64; 
 n  = 0:N   −  1; 
 w  = 2 * pi /N; 
 x  =  sin (n * w); 
 stem (x); 
 XX  =  fft (x);  
 The zero - frequency or DC coefﬁ cient is zero as expected. The value is not 
precisely zero, because normal rounding errors apply: 
 
 XX(1)/N 
 ans  = 
        8.8524e   −  018  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
222 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 Note 2 that coefﬁ cient number 1 (at index 2) is  −1j. This may be interpreted in 
terms of the basis functions, since real numbers correspond to cosine components, 
and imaginary correspond to negative sine. So a positive sine coefﬁ cient equates to 
a negative imaginary value. Coefﬁ cients 64, 63, 62 are the complex conjugates of 
the samples at indexes 2, 3, and 4. 
 
 m  =  abs (XX); 
 p  =  angle (XX); 
 iz  =  ﬁ nd ( abs (m   <  1)); 
 p(iz)  =  zeros ( size (iz)); 
 subplot (2, 1, 1);  stem (m); 
 subplot (2, 1, 2);  stem (p);  
XX(N  −  2:N)/N * 2 
 ans  = 
        0.0  + 0.0i  0.0  + 0.0i  0.0  + 1.0i
 This is due to the fact that the frequency  ω k goes all the way around the unit 
circle, as shown previously. The range from 0 to  π corresponds to frequencies 0 to 
 fs 2 (half the sampling rate). The range from  π to 2 π is a  “ mirror image ” of the 
lower frequency components. Plotting the DFT magnitudes and phases is accom-
plished with: 
  
 Note that the phase angle of components with a small magnitude was set to 
zero for clarity in lines 3 and 4. 
 7.6.6.2  DFT Example 2: Waveform with a Constant Offset   Again, we 
generate a 64 - sample sine waveform, but this time with a zero - frequency or DC 
component. The code to test this is shown below, along with the results for the zero -
 frequency (DC) component, and the ﬁ rst frequency component. These have values 
of 5 and  −1j, as expected. 
 
  2  Remember that MATLAB uses i rather than j to denote the complex quantity  j =
−1. 
 XX(2:4)/N * 2 
 ans  = 
        0.0   −  1.0i  0.0   −  0.0i  0.0   −  0.0i  
 The scaled coefﬁ cients 1, 2, 3 are: 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
223
 Note that, as always, index 1 in the vector is the subscripted component  X (0). 
This result is as we would expect — we have a zero - frequency or DC component, 
together with a  −1j for the sinusoidal component. 
 7.6.6.3  DFT Example 3: Cosine Waveform   The previous example used a sine 
component. In this example, we generate a 64 - sample cosine waveform with a DC 
component: 
 
 N  = 64; 
 n  = 0:N   −  1; 
 w  = 2 * pi /N; 
 x  =  cos (n * w)  + 5; 
 stem (x); 
 XX  =  fft (x); 
 XX(1)/N 
 ans  = 
        5 
 XX(2:4)/N * 2 
 ans  = 
        1.0   −  0.0i  0  0.0  + 0.0i  
 N  = 64; 
 n  = 0:N   −  1; 
 w  = 2 * pi /N; 
 x  =  sin (n * w)  + 5; 
 stem (x); 
 XX  =  fft (x); 
 %  DC component 
 XX(1)/N 
 ans  = 
        5 
 %  ﬁ rst two frequency components 
 XX(2:4)/N * 2 
 ans  = 
        0.0   −  1.0i  0  0.0    −  0.0i 
 As with the earlier discussion on basis functions, the value of  1
0
+ j  indicates 
the presence of a cosine component. The magnitude of the value also tells us the 
magnitude of this component. 
 7.6.6.4  DFT Example 4: Waveform with Two Components   This example 
illustrates the process of resolving the underlying components of a waveform. We 
take a 256 - sample waveform with two additive components, and examine the DFT: 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
224 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 N  = 256; 
 x  =  zeros (N, 1); 
 x(1)  = 1; 
 for n  = 2:N 
      x(n)  =  − 1 * x(n   −  1); 
 end 
 f  =  fft (x); 
 stem ( abs (f)); 
 [v i]  =  max ( abs (f)) 
 v  = 
      256 
 i  = 
      129 
 N  = 256; 
 n  = 0:N  −  1; 
 w  = 2 * pi /N; 
 x  = 7 * cos (3 * n * w)  + 13 * sin (6 * n * w); 
 plot (x); 
 XX  =  fft (x); 
 XX(1)/N 
 ans  = 
        − 2.47e    −  016 
 XX(2:10)/N * 2 
 ans  = 
        Columns 1 through 4 
         0.0  + 0.0i  0.0  + 0.0i   7.0    −  0.0i  0.0  + 0.0i 
        Columns 5 through 8 
        0.0  + 0.0i  0.0  − 13.0i  0.0   −  0.0i  0.0   −  0.0i 
        Column 9 
        0.0   −  0.0i  
 Note that the DFT is able to resolve the frequency components present. The 
frequency resolution may be seen from the fact that there are  N points spanning up 
to  f s , and so the resolution of each component is  f
N
s
. 
 7.6.6.5  DFT Example 5: Limits in Frequency   To look more closely at the 
frequency scaling of the DFT, we generate an alternating sequence of samples having 
values  + 1,  − 1,  + 1,  − 1,  ...  . This is effectively a waveform at half the sampling 
frequency. 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
225
 In MATLAB, vector indices for a 256 - point vector are numbered from 1 to 
256, as illustrated in Figure  7.12 . Starting at 0, this corresponds to the range 0 – 255. 
Component number 129 contains the only frequency component. Now, the ﬁ rst half 
of the frequency components are numbered 1 – 128, so component 129 is the ﬁ rst 
component in the second half — and this corresponds to the input frequency, namely 
 f
fs
=
2. The frequency  f  =  f s is, in fact, the ﬁ rst component  after the last, or the 
257th component (not generated here). Thus the frequency interval is  f
N
s
. The 
ﬁ rst component, number 1, is actually the zero - frequency or  “ DC ” component. 
Hence, it is easier to imagine the indices from 0 to  N  −  1. Using the MATLAB 
indices of  k  =  1,  … ,  N , we have:
 The  “ true ” frequency of component 1 is  0
0
×
=
f
N
s
. 
 The true frequency of component 2 is  1× f
N
s . 
 The true frequency of component 3 is  2 × f
N
s . 
 The true frequency of component  k is  (
)
k
f
N
s
−
×
1
. 
 In the example, 
 The true frequency of component 257 (not generated) is  256
256
×
=
f
f
s
s. 
 The true frequency of component 129 is  128
256
2
×
=
f
f
s
s . 
 Figure  7.12 illustrates these concepts. 
 FIGURE 7.12    Illustrating DFT components for an  N  =  256 transform. MATLAB indices 
start at 1, whereas the DFT equations assume zero - based indexing. The set of upper half 
returned values is a mirror image of the lower half, with the  “ pivot point ” being the  fs 2
sample. 
1
2
0
1
Δ
Δ =
f s
N
0
128 129
Lower half
Upper half
127 128
f s
2
f s
256 257
255 256
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
226 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 7.6.6.6  DFT Example 6: Amplitude and Frequency   If we now alter both the 
amplitude and frequency of the input signal, the DFT result ought to reﬂ ect this. The 
waveform with ﬁ ve cycles of a simple sinusoidal waveform of amplitude 4 is cal-
culated as:
 
 
N
n
N
N
x n
n
=
=
…
−
=
=
64
0 1
1
2
4
5
, ,
,
( )
sin(
).
ω
π
ω
 
 (7.41) 
 The waveform and its DFT are shown in Figure  7.13 . Note in particular:
 1.  The index of the lower frequency peak and its corresponding frequency. 
 2.  The location of the higher frequency peak (the mirror image). 
 3.  The height of the frequency peaks. 
 7.6.6.7  DFT Example 7: Resolving Additive Waveforms   Extending the pre-
vious example, we now take the DFT of two additive sinusoidal waveforms, each 
with a different amplitude and frequency:
 FIGURE 7.13   DFT example: 5 cycles of amplitude 4 in the time and frequency domains. 
0
8
16
24
32
40
48
56
63
−10
−6
−2
2
6
10
Sampled waveform 4sin(5nω)
0
8
16
24
32
40
48
56
63
0
32
64
96
128
FFT of sampled waveform
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
227
 
 
N
n
N
N
x n
n
n
=
=
−
=
=
+
64
0 1
1
2
2
1
4
4
, ,
,
( )
sin(
)
sin(
).
…
ω
π
ω
ω
 
 (7.42) 
 The waveform and its DFT are shown in Figure  7.14 . Examination of the 
time domain waveform alone does not indicate the frequencies which are 
present. However, the Fourier transform clearly shows the frequencies and their 
amplitudes. Combining the sine/cosine components in this way eliminates the phase 
information, however. That is, it is not possible to determine from the ﬁ gures 
whether the underlying waveforms are sine or cosine. However, the complex 
components themselves yield this information, as we have seen in earlier 
examples. 
 7.6.6.8  DFT Example 8: Spectral Leakage   What happens when the period of 
the underlying waveform does not match that of the sampling window? That is, we 
do not have an integral number of cycles of the waveform to analyze. The waveform 
parameters are similar to those encountered previously:
 FIGURE 7.14   DFT example: two sinusoidal waveforms in the time and frequency 
domains. 
0
8
16
24
32
40
48
56
63
−10
−6
−2
2
6
10
Sampled waveform 2sin(1nω) + 4sin(4nω)
0
8
16
24
32
40
48
56
63
0
32
64
96
128
FFT of sampled waveform
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
228 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 
 
N
n
N
w
N
x n
n
=
=
…
−
=
( ) =
(
)
64
0 1
1
2
4
2 5
, ,
,
sin
.
.
π
ω
 
 (7.43) 
 The sampled waveform is shown in Figure  7.15 — note that there are  2 1
2 cycles in 
the analysis window, as expected. The product of some of the basis - vector wave-
forms and the input waveform will be nonzero, resulting in some nonzero sine and 
cosine components around the frequency of the input. 
 The DFT of this waveform is shown in Figure  7.15 . Note the  “ spillover ” in 
the frequency components, due to the fact that a noninteger number of cycles was 
taken for analysis. This is a fundamental problem in applying the DFT, and motivates 
the question: if we do not know in advance the expected frequencies contained in 
the waveform, how long do we need to sample for, in order for this problem not to 
occur? We have already determined that the resolution in the DFT is governed by 
the number of samples in the time domain, but now we have the further problem as 
illustrated in this example. To answer this question fully requires some further 
 FIGURE 7.15   DFT example: a nonintegral number of cycles of a sinusoidal waveform. 
In this case, the DFT does not resolve the waveform into a single component as might be 
expected.  
0
8
16
24
32
40
48
56
63
−10
−6
−2
2
6
10
Sampled waveform 4sin(2.5nω)
0
8
16
24
32
40
48
56
63
0
32
64
96
128
FFT of sampled waveform
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.6 THE FOURIER TRANSFORM 
229
analysis and consideration of the  “ sampling window ” in Section  7.9 . But clearly, we 
must be careful in interpreting the results produced by the DFT. 
 7.6.6.9  DFT Example 9: Additive Noise   Now consider the case of nonperi-
odic noise added to sinusoidal components. This type of scenario is typical of com-
munications and speech processing systems, where one or more periodic signals 
have noise superimposed.
 
 
N
n
N
w
N
x n
n
n
v n
=
=
−
=
( ) =
(
)+
(
)+
( )
64
0 1
1
2
2
1
4
4
, ,
,
sin
sin
.
…
π
ω
ω
α
 
 (7.44) 
 Here,  v ( n ) represents the additive noise signal, with the amount controlled by param-
eter   α . The waveform and its DFT are shown in Figure  7.16 . From these plots, it is 
evident that the periodic components still feature strongly in the frequency domain. 
The DFT step makes the underlying components clearer than in the time domain 
waveform. 
 FIGURE 7.16   DFT example: a complex sinusoidal waveform with additive noise in the 
time and frequency domains. 
0
8
16
24
32
40
48
56
63
−10
−6
−2
2
6
10
Sampled waveform 2sin(1nω) + 4sin(4nω) + α v(n)
0
8
16
24
32
40
48
56
63
0
32
64
96
128
FFT of sampled waveform
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
230 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 7.6.6.10  DFT Example 10: Noisy Waveform Restoration   This example con-
siders a waveform with additive noise. Thresholding is done in the frequency domain 
(note the MATLAB vectorized method of doing this). The real part of the inverse 
FFT,  ifft (), is taken, because although the imaginary part should be zero in the 
time domain, it may have a very small imaginary component due to rounding and 
the effects of ﬁ nite numerical precision. 
 The result, shown in Figure  7.17 , shows that the operation of taking the fre-
quency components, thresholding, and taking the inverse DFT is able to restore the 
signal. In practice, however, taking an arbitrary threshold may not be desirable, since 
the actual signal as presented may vary from the test signal shown. Thus, a more 
advanced approach may be warranted — for example, if more than one frame of data 
signal is available, we may be able to average over multiple frames of signal rather 
than base the results on one single frame.  
 
 FIGURE 7.17   DFT and noise removal. 
0
8
16
24
32
40
48
56
63
−10
−6
−2
2
6
10
Sampled waveform 2sin(1nω) + 4sin(4nω) + α v(n)
0
8
16
24
32
40
48
56
63
−10
−6
−2
2
6
10
IFFT of thresholded FFT
 N  = 64; 
 n  = 0:N  −  1; 
 w  = 2 * pi /N; 
 x  = 2 * sin (1 * n * w)  + 4 * sin (4 * n * w)  + 1 * randn (1,N); 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.7 ALIASING IN DISCRETE-TIME SAMPLING 
231
 7.7  ALIASING IN DISCRETE - TIME SAMPLING 
 The problem of choosing the sampling rate for a given signal was discussed in 
Section  3.8 . The Fourier transform now provides us with a more analytical way to 
look this problem, and the related problem of aliasing when we sample a signal. 
 In Section  3.5 , sampling was introduced as a multiplication of the continuous -
 time analog signal with the sampling pulses (the  “ railing ” function). The Fourier 
series expansion of the sampling impulses of frequency  Ω s can be shown to be:
 
 
r t
T
T
t
T
t
T
t
T
t
t
s
s
s
s
s
( ) =
+
+
+
+
=
+
+
1
2
2
2
2
3
2
1
2
2
cos
cos
cos
cos
cos
Ω
Ω
Ω
Ω
Ω

+
+
⎛
⎝⎜
⎞
⎠⎟
cos
.
3Ωst

 
 (7.45) 
 We know that sampling is equivalent to multiplying the signal  x ( t ) by the railing 
function  r ( t ) to obtain the sampled signal  x s ( t ). To simplify notation, we replace the 
cosine function with its complex exponential equivalent:
 
 
r t
T
t
t
t
T
t
s
s
s
s
( ) =
+
+
+
+
⎛
⎝⎜
⎞
⎠⎟
=
+
+
2
1
2
2
3
1 1
2
2
2
cos
cos
cos
cos
cos
Ω
Ω
Ω
Ω

Ω
Ω
Ω
Ω
Ω
Ω
s
s
t
t
t
t
t
t
T e
e
e
e
e
e
s
s
s
s
+
+
(
)
=
+
+
(
)+
+
(
)+
−
−
2
3
1
0
2
2
3
cos

j
j
j
j
j
j
j
j
Ω
Ω
Ω
s
s
s
t
t
k
t
k
e
T
e
+
(
)+
(
)
=
−
=−∞
+∞
∑
3
1

.
  (7.46) 
 The sampled signal is  x s ( t )  =  x ( t ) r ( t ), and we need to determine the effect of the 
sampling in the frequency domain. As before, the Fourier transform of the continu-
ous signal is:
 X
x t e
dt
t
Ω
Ω
( ) =
( )
−
−∞
+∞∫
j
.  
 Using these results, we can derive the Fourier transform of the sampled signal as 
follows.
 subplot (2,1,1); 
 plot (n,x); 
 f  =  fft (x); 
 i  =  ﬁ nd ( abs (f)    <  40); 
 f(i)  =  zeros ( size (i)); 
 xr  =  ifft (f); 
 subplot (2,1,2); 
 plot (n,xr);  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
232 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 
 
X
x t e
dt
x t r t e
dt
x t
T
e
s
s
t
t
k
ts
Ω
Ω
Ω
Ω
( ) =
( )
=
( ) ( )
=
( )
−
−∞
+∞
−
−∞
+∞
−∞
∫
∫
j
j
1
+∞
( )
−
−∞
+∞
−
−∞
+∞
∑
∫
∫
⎛
⎝⎜
⎞
⎠⎟
=
( )
r t
t
k
t
t
e
dt
T
x t e
e
dt
s





j
j
j
Ω
Ω
Ω
1
⎛
⎝
⎞
⎠
=
( )
⎛
⎝
⎞
⎠
=
( )
=−∞
+∞
−
−
(
)
−∞
+∞
=−∞
+∞
−
∑
∫
∑
k
k
t
k
T
x t e
dt
T
x t e
s
1
1
j
j
Ω
Ω
Ω
Ω
+
(
)
−∞
+∞
=
+∞
∫
∑⎛
⎝
⎞
⎠
k
t
k
s dt
0
.
 
 (7.47) 
 The integral (in brackets) is  X ( Ω  ±  k Ω s ), which is the spectrum  X ( Ω ) shifted by  ± k Ω s . 
So the spectrum of the sampled waveform is:
 
 X
T
X
k
s
s
k
Ω
Ω
Ω
( ) =
±
(
)
=
∞
∑
1
0
 
 (7.48) 
 The above equation demonstrates why aliasing occurs, and may be expressed in 
words as follows: 
 
 The spectrum of the signal  X ( Ω ) is  “ mirrored ” at
 ±
±
±
Ω
Ω
Ω
s
s
s
,
,
,
2
3
…  
 We can understand this further by returning to the ﬁ rst introduction of aliasing in 
Chapter  3 . Figure  7.18 shows the problem in the time domain, and now we are able 
to interpret this in terms of frequency. Referring to Figure  7.18 , we ﬁ rst sample 
below the input frequency. Clearly, we cannot expect to capture the information 
about the waveform if we sample at a frequency lower than the waveform itself, so 
this is just a hypothetical case. The waveform  appears to have a frequency of 
| f  −  f s |  =  |4  −  3|  =  1  Hz. Next, we sample at 5  Hz, which is a little above the wave-
form frequency. This is still inadequate, as shown. The frequency is effectively 
 “ folded ” down to | f  −  f s |  =  |4  −  5|  =  1  Hz. Finally, we sample at greater than twice 
the frequency ( f s  >  2 f , or 10  Hz  >  2  ×  4  Hz). The sampled waveform does not produce 
a lower  “ folded - down ” frequency. 
 Figure  7.19 illustrates this approach, from the frequency domain perspective. 
It shows that as the sampling frequency is reduced relative to the frequency spectrum 
of the signal being sampled, a point will be reached where the baseband spectrum 
and the  “ ghost ” spectrum overlap. Once this occurs, the signal cannot be inverted 
back into the time domain without aliasing occurring. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.8 THE FFT AS A SAMPLE INTERPOLATOR 
233
 FIGURE 7.18   How aliasing arises in the time domain. A 4 - Hz sinewave is sampled, so 
the Nyquist rate is 8  Hz, and we should sample above that rate. 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Aliasing in sampling: f = 4 Hz, fs = 3 Hz
Time (s)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Aliasing in sampling: f = 4 Hz, fs = 5 Hz
Time (s)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
No aliasing in sampling: f = 4 Hz, f s = 10 Hz
Time (s)
 7.8  THE FFT AS A SAMPLE INTERPOLATOR 
 As an interesting insight, consider the generation of missing samples in a sequence. 
This is related to the problem of interpolating the signal value between known 
samples, as introduced in Section  3.9.1 . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
234 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 To begin with, consider Figure  7.20 . This ﬁ gure shows zeros inserted into the 
sample sequence between every known sample, so as to generate a new sample 
sequence. This is about as simple an operation as we could imagine, and effectively 
doubles the sample rate (even though the new zero - valued samples obviously bear 
no relation to the true samples).  
 But what is the effect in the frequency domain? If we take the zero - inserted 
sequence and examine its FFT, we see, as in Figure  7.21 , that spurious components 
have been introduced. Considering the frequency - domain interpretation, we may say 
that these correspond to the high - frequency components introduced by the zero 
values at every second sample.  
 If we then remove these high - frequency components and take the inverse FFT, 
we see that the original signal has been  “ smoothed ” out, or interpolated, as shown 
in Figure  7.22 . The following MATLAB code shows how to use the  reshape () 
function to perform zero - sample interpolation: 
 
 FIGURE 7.19   Sampling rate and  “ folding frequency ” . The true spectrum is that shown 
over the range 0 to  fs 2. The secondary spectral images are artifacts of the sampling 
process. If the spectra overlap, aliasing occurs. 
Adequate sample rate compared with bandwidth
Marginal sample rate compared with bandwidth
Inadequate sample rate compared with bandwidth
0
0
+ f s/ 2
+ f s/ 2
−f s/ 2
−f s/ 2
+ f s
+ f s
−f s
−f s
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 FIGURE 7.20   Interpolation: the original signal and the signal with zeros inserted at 
every other sample.  
0
8
16
24
32
−2
−1
0
1
2
Original signal
0
8
16
24
32
40
48
56
64
−2
−1
0
1
2
Zero-inserted signal
 FIGURE 7.21   Interpolation: the DFT of the original signal and the DFT of the signal 
with zeros inserted at every other sample. 
0
8
16
24
32
0
8
16
24
32
FFT of original signal
0
8
16
24
32
40
48
56
64
0
8
16
24
32
FFT of zero-inserted signal
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
236 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 FIGURE 7.22   Interpolation after removing the high - frequency components in the 
zero - inserted sequence. 
0
8
16
24
32
−2
−1
0
1
2
Original signal
0
8
16
24
32
40
48
56
64
−2
−1
0
1
2
Interpolated signal
 7.9  SAMPLING A SIGNAL OVER 
A FINITE TIME WINDOW 
 Considering the preceding discussion, two problems are apparent with discrete - data 
spectrum estimation:
 1.  The sampling rate limits the maximum frequency we can observe, and may 
introduce aliasing if proper ﬁ ltering is not present at the input. 
 N  = 32; 
 n  = 0:N  −  1; 
 w  = 2 * pi /N; 
 x  =  sin (n * w); 
 f  =  fft (x); 
 xi  = [x;  zeros (1, N)]; 
 xi  =  reshape (xi, 2 * N, 1); 
 fi  =  fft (xi); 
 fi(32)  = 0; 
 fi(34)  = 0; 
 xr  = 2 * real ( ifft (fi));  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.9 SAMPLING A SIGNAL OVER A FINITE TIME WINDOW  
237
 2.  We can only sample data over a ﬁ nite time record. The continuous - time 
Fourier equations used inﬁ nite time limits, which is of course impractical. 
 Furthermore, what happens if the frequency content of the signal changes over the 
sampling time? We will examine this related issue in Section  7.10 . So the question 
we must examine theoretically is this: since we can only sample for a ﬁ nite amount 
of time, what is the effect on the spectrum? As we have seen, the Fourier Transform 
of a continuous signal is
 X
x t e
dt
t
Ω
Ω
( ) =
( )
−
−∞
+∞∫
j
.  
 If this is truncated to a ﬁ nite length (because we sample the waveform for a ﬁ nite 
time), say  ± τ , then the transform becomes
 
 X
x t e
dt
t
Ω
Ω
( ) =
( )
−
−
+∫
j
τ
τ
.  
 (7.49) 
 Suppose  x ( t ) is a sine wave of frequency  Ω o radians/second:
 
 x t
t
o
( ) = sin
.
Ω
 
 (7.50) 
 In exponential form,
 
 x t
e
e
o
o
t
t
( ) =
−
(
)
−
1
2j
j
j
Ω
Ω
.  
 (7.51) 
 The windowed spectrum becomes:
 
 X
e
e
e
dt
o
o
t
t
t
Ω
Ω
Ω
Ω
( ) =
−
(
)
−
−
−
+∫
1
2j
j
j
j
τ
τ
.  
 (7.52) 
 Performing the integration, the magnitude becomes a pair of sinc functions centered 
at  ± Ω o ,
 
 X
o
o
Ω
Ω
Ω
Ω
Ω
( ) =
−
(
)
+
+
(
)
τ
τ
τ
τ
sinc
sinc
,  
 (7.53) 
where the  “ sinc ” function is as previously deﬁ ned, of the form 3 
 
 sinc
.
θ
θ
θ
= sin
 
 (7.54) 
 Noting that the sinc function is a sin θ component multiplied by a 1/ θ component, 
we can deduce that the shape is sinusoidal in form, multiplied by an envelope that 
decays as 1/ θ . Here, the  θ is actually  Ω τ , with  Ω offset by the ﬁ xed frequency  Ω o . 
Thus, the resulting spectrum is a pair of sinc functions, centered at  ± Ω o . The sinc 
function   sinθ θ
(
)
 has zero crossings at  ± π ,  ± 2 π ,  … , and thus the zero crossings  Ω c 
of the sinc function are at:
 
 Ω
Ω
o
c
k
±
(
) = ±
τ
π  
 (7.55) 
 
 ∴
= ±
±
Ω
Ω
c
o
kπ
τ
 
 (7.56) 
  3  Some texts deﬁ ne  sinc t
t
t
= (
)
sinπ
π . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
238 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 That is, as the length of the window increases and  τ  →  ∞ , the zero crossings 
tend to be closer and closer to the  “ true ” frequency. In other words, the sinc 
function tends to an impulse at the frequency of the input sine wave. A 
 rectangular window over the time interval  − 2  ≤  t  ≤  2 is shown in Figure  7.23 . 
Note that the window is  “ implicit, ” in that we are taking only a ﬁ xed number of 
samples. 
 The type of spectral estimate resulting from a rectangular window is shown 
in the successive plots in Figures  7.24 and  7.25 . Each point in the lower panel is 
one point on the spectrum. This is derived by multiplying the input waveform by 
each of the sine and cosine basis functions, adding or integrating over the entire time 
period, and taking the magnitude of the resulting complex number. Figure  7.24 
shows the situation when the analysis frequency is relatively low, and Figure  7.25 
shows the situation when the analysis frequency is just higher than the input 
frequency. 
 This may also be understood by the fundamental theorem of convolution: 
multiplication in the time domain equals convolution in the frequency domain. Thus, 
multiplication of a signal which is inﬁ nite in extent, by a ﬁ nite rectangular window 
in the time domain, results in a spectrum which is the convolution of the true spec-
trum (an impulse in the case of a sine wave) with the frequency response of the 
rectangular window. The latter was shown to be a sinc function. Thus, the true fre-
quency estimates are  “ smeared ” by the sinc function. 
 FIGURE 7.23   Windowing: an inﬁ nite sine wave (upper) is multiplied by a rectangular 
window, to yield the windowed waveform (bottom). Note that if we do sample over a ﬁ nite 
time limit, we have effectively windowed the waveform. Ambiguity arises because our 
window (and hence the sampling) is not synchronized to the underlying signal. 
0
0
0
1
1
1
–1
–1
–1
0
0
0
1
1
1
2
2
2
3
3
3
4
4
4
–1
–1
–1
–2
–2
–2
–3
–3
–3
–4
–4
–4
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 FIGURE 7.24   The effect of a window on the spectrum. The upper panel shows the 
signal to be analyzed using the sine and cosine basis functions (middle). The resulting 
spectrum is built up in the lower plot; at this time instant, there is little correlation and so 
the amplitude is small.  
Original signal to analyze over finite time window
Time
Complex exponential = sine & cosine waves
Time
Resulting sum-of-product (spectrum)
Frequency
Current frequency point
 FIGURE 7.25   As the analysis frequency is increased, the signals align to a greater 
degree and so the spectral magnitude is larger. Clearly, the resultant spectrum is not a 
single value at the input frequency, as might be expected, but is spread over nearby 
frequencies. This gives rise to ambiguity in determining the true frequency content of the 
input signal. 
Original signal to analyze over finite time window
Time
Complex exponential = sine & cosine waves
Time
Resulting sum-of-product (spectrum)
Frequency
Current frequency point
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
240 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 FIGURE 7.26   The evolution of a  “ chirp ” signal, whose frequency increases over time. 
The darker sections correspond to greater energy in the signal at a particular time and 
frequency. On the left is a portion of the time domain waveform. On the right is the 
time - frequency plot. 
0
0.05
0.1
0.15
0.2
0.25
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Chirp waveform
Time (s)
Amplitude
Frequency Hz
Time (s)
Chirp time-frequency
0
0.2
0.4
0.6
0.8
1
1,000
900
800
700
600
500
400
300
200
100
 Fs  = 8000; 
 dt  = 1/Fs; 
 N  = 2 ˆ 13;      % ensure a power of 2 for FFT 
 t  = 0:dt:(N   −  1) * dt; 
 f  = 400; 
 fi  = (1: length (t))/ length (t) * f; 
 y  =  sin (2 * pi * t. * ﬁ ); 
 plot (y(1:2000)); 
 sound (y, Fs);  
 7.10  TIME - FREQUENCY DISTRIBUTIONS 
 In many applications, we are interested in not simply the spectral content of a signal, 
but  how that spectral content  evolves over time. Probably the best example is the 
speech signal — the frequency content of speech obviously changes over time. For a 
simpler real - world example, think of a vehicle with a siren moving toward an 
observer. Because of the Doppler effect, the frequency increases as the vehicle 
moves toward the observer, and decreases as it moves away. 
 An obvious way to approach this problem is to segment the samples into 
smaller frames or blocks. The Fourier analysis techniques can then be applied to 
each frame in turn. This is termed a  spectrogram . 
 The following generates a  “ chirp ” signal, which is one whose frequency 
increases over time: 
 
 Plotting the FFT magnitude shows only the average power in the signal over 
 all time. Using the spectrogram, we obtain a time - frequency plot as shown in Figure 
 7.26 . This shows higher energy with a darker color. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.11 BUFFERING AND WINDOWING 
241
 As an illustration of a more complicated signal, Figure  7.27 shows the spec-
trograms of a speech utterance, the sentence  “ today is Tuesday. ” To attain greater 
accuracy in the frequency domain, a longer sample record is required. However, this 
means less accuracy in the time domain, because the frequency content of the signal 
changes relatively quickly. 
 7.11  BUFFERING AND WINDOWING 
 The buffering required to implement a spectrogram is shown in Figure  7.28 . The 
samples are split into consecutive frames, and the DFT of each frame is taken. 
Ideally, a longer frame for the Fourier analysis gives better frequency resolution, 
since (as seen earlier) the resolution is  f
N
s
. There is, however, a trade - off required 
in determining the buffer length: a longer buffer gives better frequency resolution, 
but since there are fewer buffers, this gives poorer resolution in time. Conversely, 
smaller buffers give better time resolution, but poorer frequency resolution.  
 Because we are often trying to capture a  “ quasi - stationary ” signal such as the 
human voice, the direct buffering method can be improved upon in some situations. 
Figure  7.29 shows the use of overlapped frames, with a 25% overlap. Each new 
frame incorporates a percentage of samples from the previous frame. Typical overlap 
may be from 10 to 25% or more. 
 To smooth the overlap, a number of so - called  “ window functions ” have been 
developed. As described above, we have until now been using a rectangular window 
of the form:
 
 w
n
M
n =
≤
≤
⎧⎨⎩
1 0
0
0
.
:
:
.
otherwise  
 (7.57) 
 FIGURE 7.27   Spectrogram of voice waveform signal. The time domain plot (left) shows 
us the source waveform itself, but the time - frequency plot shows the frequency energy 
distribution over time. 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Speech waveform
Time (s)
Amplitude
Frequency Hz
Time (s)
Speech time-frequency
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
1,000
900
800
700
600
500
400
300
200
100
0
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
242 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 To improve upon this, we need to taper the edges of the blocks so that there is a 
smoother transition from one block to another. That is, the portions where the blocks 
overlap ought to have a contribution from the previous block, together with a con-
tribution from the next block. The conventional approach is to use a  “ window ” 
function to perform this tapering. A straightforward window function is a triangular -
 shaped window:
 
 w
n
M
n
M
n
M
M
n
M
n =
≤
≤
−
≤
≤
⎧
⎨
⎪
⎪
⎩
⎪
⎪
2
0
2
2
2
2
0
:
:
:
.
otherwise
 
 (7.58) 
 FIGURE 7.29   A signal record split into overlapping frames. 
Sample record
Frames
 FIGURE 7.28   A signal record split into nonoverlapping frames. 
Sample record
Frames
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.12 THE FFT 
243
 Two other types having a smoother shape are the Hanning window — for even - valued 
 M , deﬁ ned as
 
 w
n
M
n
M
n =
−
≤
≤
⎧
⎨⎪
⎩⎪
0 5
0 5
2
0
0
.
. cos
:
:
,
π
otherwise
 
 (7.59) 
and the Hamming window — for even - valued  M , deﬁ ned as:
 
 w
n
M
n
M
n =
−
≤
≤
⎧
⎨⎪
⎩⎪
0 54
0 46
2
0
0
.
.
cos
:
:
.
π
otherwise
 
 (7.60) 
 The Hamming window is often used, and it is easily implemented in MATLAB, as 
follows. 
 
 M  = 32; 
 n  = 0:M; 
 w  = 0.54   −  0.46 * cos (2 * pi * n/M); 
 stem (w);  
 A comparison of these window shapes is shown in Figure  7.30 . Different 
window shapes produce differing frequency component overﬂ ows (termed  “ side-
lobes ” ) in the frequency domain when applied to a rectangular window of sampled 
data. Windows will be encountered again in Chapter  8 , where the Hamming window 
is again applied, but in the context of designing a digital ﬁ lter, in order to adjust its 
frequency response. 
 7.12  THE FFT 
 The direct DFT calculation is computationally quite  “ expensive ” — meaning that the 
time taken to compute the result is signiﬁ cant when compared with the sample 
period. Not only is a faster method desirable, in real - time applications, it is essential. 
This section describes the so - called FFT, which substantially reduces the computa-
tion required to produce exactly the same result as the DFT. 
 The FFT is a key algorithm in many signal processing areas today. This is 
because its use extends far beyond simple frequency analysis — it may be used in a 
number of  “ fast ” algorithms for ﬁ ltering and other transformations. As such, a solid 
understanding of the FFT is well worth the intellectual effort. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
244 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 7.12.1  Manual Fourier Transform Calculations 
 To recap, the discrete - time Fourier transform (DFT) is deﬁ ned by:
 X k
x n e
n
n
N
k
( ) =
( )
−
=
−
∑
j ω
0
1
 
 ω
π
k
k
N
k
N
=
=
−
2
0 1
1
, ,
,
…
.  
 To grasp the basic idea of the number of arithmetic operations required, consider a 
four - point ( N  =  4) DFT calculation:
 
X k
n
n
n
n
X
x
e
x
e
x
e
( )
=
=
=
=
( ) = ( )
+ ( )
+ ( )
−
−
−
0
1
2
3
0
0
1
2
02
4 0
12
4 0
22
4
j
j
j
π
π
π0
32
4 0
02
4 1
12
4 1
22
4 1
3
1
0
1
2
+
( )
( ) = ( )
+
( )
+
( )
−
−
−
−
x
e
X
x
e
x
e
x
e
j
j
j
j
π
π
π
π
+
( )
( ) = ( )
+ ( )
+ ( )
+
−
−
−
−
x
e
X
x
e
x
e
x
e
3
2
0
1
2
32
4 1
02
4 2
12
4 2
22
4 2
j
j
j
j
π
π
π
π
x
e
X
x
e
x
e
x
e
x
3
3
0
1
2
32
4 2
02
4 3
12
4 3
22
4 3
( )
( ) = ( )
+ ( )
+ ( )
+
−
−
−
−
j
j
j
j
π
π
π
π
3
32
4 3
( )
−
e
j
π
.
 
 FIGURE 7.30   Window functions for smoothing frames. Top to bottom are Rectangular, 
Triangular, Hanning, and Hamming. The right - hand side shows the effect on a sine wave 
block. 
0
1
−1
1
0
1
−1
1
0
1
−1
1
0
1
Signal window functions
−1
1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.12 THE FFT 
245
 This is a small example, at about the limit for manual calculation. In practice, 
typically  N  =  256 to 1024 samples are required for reasonable resolution 
(depending on the application, and of course the sampling rate), and it is not 
unusual to require many more than this. From the above manual calculation, we can 
infer:
 1.  The number of complex additions  =  4  ×  3  ≈  N 2 . 
 2.  The number of complex multiplications  =  4  ×  4  =  N 2 . 
 Note that complex numbers are comprised of real and imaginary components, so 
there are in fact more operations — a complex number multiplication really requires 
four simple multiplications and two arithmetic additions:
 
a
b c
d
ac
ad
bc
bd
ac
bd
ad
bc
+
(
)
+
(
) =
+
+
−
=
−
(
)+
+
(
)
j
j
j
j
j
.  
 If  N  ≈  1000, and each multiply/add takes  T calc  =  1  μ s, the Fourier transform calcula-
tion would take of the order of  N 2 · T calc  =  1000 2  ×  10  − 6  =  1 second. 
 Referring to Figure  7.31 , we see that the exponents are cyclical, and this 
leads to the simpliﬁ cation: 
 p
p
⋅
→⎛
⎝⎜
⎞
⎠⎟(
)
2
4
2
4
4
π
π
mod
,  
where  N  =  4 and  “ mod ” is the modulo remainder after division:
 FIGURE 7.31   The DFT exponent as it moves around the unit circle. Clearly, the point at 
angle 2 nk π / N will be recalculated many times, depending on the values which  n and  k take 
on with respect to a given  N . 
0
4 × 2π
4
1 × 2π
4 , 5 × 2π
4 , · · ·
2 × 2π
4
3 × 2π
4
ω
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
246 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 
0
4
0
1
4
1
2
4
2
3
4
3
4
4
0
5
4
1
mod
mod
mod
mod
mod
mod
.
→
→
→
→
→
→
 
 Now, grouping the even - numbered and odd - numbered terms, we have:
 
X k( )
(
)
                                       
Events 0 2,








                                       
Odds 1 3,
(
)



X
x
e
x
e
x
e
x
0
0
2
1
3
02
4 0
22
4 0
12
4 0
( ) = ( )
+ ( )
+ ( )
+ ( )
−
−
−
j
j
j
π
π
π
e
X
x
e
x
e
x
e
x
e
−
−
−
−
( ) = ( )
+ ( )
+ ( )
+ ( )
j
j
j
j
32
4 0
02
4 1
22
4 1
12
4 1
1
0
2
1
3
π
π
π
π
−
−
−
−
−
( ) = ( )
+ ( )
+ ( )
+ ( )
j
j
j
j
32
4 1
02
4 2
22
4 2
12
4 2
2
0
2
1
3
π
π
π
π
X
x
e
x
e
x
e
x
e
j
j
j
j
j
32
4 2
02
4 3
22
4 3
12
4 3
3
0
2
1
3
π
π
π
π
X
x
e
x
e
x
e
x
e
( ) = ( )
+ ( )
+ ( )
+ ( )
−
−
−
−32
4 3
π
.  
 Note that there are common terms like ( x [0]  +  x [2]) in the  k  =  0 and  k  =  2 calcula-
tions. Now, a two - point ( N  =  2) DFT is:
 
k
X
x
e
n
x
e
n
=
( ) = ( )
=
+ ( )
=
−
−
0
0
0
0
1
1
02
2 0
12
2 0
:
j
j
π
π  
 k
X
x
e
x
e
=
( ) = ( )
+ ( )
−
−
1
1
0
1
02
2 1
12
2 1
:
.
j
j
π
π  
 Comparing with the four - point DFT, some terms are seen to be the same if we 
renumber the four - point algorithm terms  x (0) and  x (2) to become  x (0) and  x (1) for 
the two - point transform. This is shown in Figure  7.32 — the four - point DFT can be 
calculated by combining two two - point DFTs. This suggests a  “ divide - and - conquer ” 
approach, as will be outlined in the following section. 
 7.12.2  FFT Derivation via Decimation in Time 
 The goal of the FFT is to reduce the time required to compute the frequency com-
ponents of a signal. As the reduction in the amount of computation time is quite 
signiﬁ cant (several orders of magnitude), many  “ real - time ” applications in signal 
processing (such as speech analysis) have been developed. 4 
 If we write the discrete - time Fourier transform (DFT) as usual:
 X k
x n e
n
n
N
k
( ) =
( )
−
=
−
∑
j ω
0
1
,  
  4  The FFT decimation approach is generally attributed to J. W. Cooley and J. W. Tukey, although there 
are many variations. Surprisingly, although Cooley and Tukey ’ s work was published in 1965, it is thought 
that Gauss described the technique as long ago as 1805 (Heideman et al. 1984). 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.12 THE FFT 
247
 ω
π
k
k
N
k
N
=
=
−
2
0 1
1
, ,
,
,
…
 
and let  W
e
N
=
−j2π
, the DFT becomes
 
 X k
x n W nk
n
N
( ) =
( )
=
−
∑
0
1
.  
 (7.61) 
 We can split the  x ( n ) terms into odd - numbered ones ( n  =  1, 3, 5,  … ) and even -
 numbered ones ( n  =  0, 2, 4,  … ). If  N is an integer, then 2 N is an even integer, and 
2 N  +  1 is an odd integer. This results in:
 
X k
x
n W
x
n
n k
n
N
( ) =
(
)
+
+
(
)
(
)
=
−
∑
2
2
1
2
0
2 1
Even-numbered terms





W
x
n W
n
k
n
N
nk
n
N
2
1
0
2 1
2
0
2
+
(
)
=
−
=
∑
=
(
)
Odd-numbered terms





2 1
2
0
2 1
2
0
2 1
2
2
1
2
2
1
−
=
−
=
−
∑
∑
∑
+
+
(
)
=
(
)
+
+
(
)
x
n
W
W
x
n W
W
x
n
W
nk
k
n
N
nk
n
N
k
nk
n
N
nk
n
N
k
nk
n
N
x
n W
W
x
n
W
=
−
=
−
=
−
∑
∑
∑
=
(
)(
) +
+
(
)(
)
0
2 1
2
0
2 1
2
0
2 1
2
2
1
.
 
 FIGURE 7.32   A four - point FFT decomposition. This idea is used as the basis for the 
subsequent development of the FFT algorithm. 
x 0
x 1
x 2
x 3
X 0
X 1
X 2
X 3
x 0 + x 2
x 0 + x 2e−π
x 1 + x 3
x 1 + x 3e−π
Combine
Two-point
DFT
Two-point
DFT
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
248 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 Inspecting the above equation, it seems that we need a value for  W 2 , which can be 
found as follows:
 W
e
N
=
−j2π  
 
∴
= ⎛
⎝⎜
⎞
⎠⎟
=
−
−(
)
W
e
e
N
N
2
2
2
2
2
j
j
π
π
.
 
 The last term is just  W for  N 2 instead of  N . So we have:
 
X k
x
n W
nk
n
N
N
( ) =
(
)(
)
=
−
∑
2
2
0
2 1
2
DFT of even-indexed
sequence



	




+
+
(
)(
)
=
−
∑
W
x
n
W
k
nk
n
N
N
2
1
2
0
2 1
2
DFT of odd-indexed
sequence

	

 
 ∴
( ) =
( )+
( )
X k
E k
W F k
k
.  
 Now if  k
N
≥
2, then the equation for  X ( k ) can be simpliﬁ ed by replacing
 
 k
k
N
→
+
⎛
⎝⎜
⎞
⎠⎟
2 .  
 (7.62) 
 Then,
 
X k
N
x
n W
W
x
n
W
n k
N
n
N
k
N
+
⎛
⎝⎜
⎞
⎠⎟=
(
)
+
+
(
)
+
⎛
⎝⎜
⎞
⎠⎟
=
−
+
⎛
⎝⎜
⎞
⎠⎟
∑
2
2
2
1
2
2
0
2 1
2
2
2
0
2 1
2
0
2 1
2
2
2
2
1
n k
N
n
N
nk
nN
n
N
k
N
n
x
n W
W
W W
x
n
W
+
⎛
⎝⎜
⎞
⎠⎟
=
−
=
−
∑
∑
=
(
)
+
+
(
)
k
nN
n
N
W
.
=
−
∑
0
2 1
 
 As before,  W
e
N
=
−j2π
 and hence,
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.12 THE FFT 
249
 
W
e
e
n N
nN
N nN
n
=
=
=
−
−
j
j
2
2
1
π
π
for all ,
, 
and
 
W
e
e
N
N
N
N
2
2
2
1
=
=
= −
−
−
j
j
π
π
for all
.
 
 We previously deﬁ ned
 
 X k
E k
W F k
k
( ) =
( )+
( ).  
 (7.63) 
 Hence,
 
 X k
N
E k
W F k
k
+
⎛
⎝⎜
⎞
⎠⎟=
( )−
( )
2
.  
 (7.64) 
 Together, these show how the DFT of the original sequence can be calculated via 
DFTs of half - length sequences. To express this graphically, the  “ butterﬂ y ” diagram 
of Figure  7.33 is used. This indicates that for inputs  p and  q , the  q value is multiplied 
by a constant  α , and the result summed, to give outputs  p  ±  α q . A 4 - to - 8 DFT can 
then be shown as in Figure  7.34 . 
 The complete diagram for an eight - point FFT decomposition is shown in 
Figure  7.35 . The transform calculation begins with eight inputs, and computes four, 
two - point transforms. Then these are combined into two, four - point transforms, with 
the ﬁ nal stage resulting in the eight output points. Thus there are three stages for 
eight input points. In general, for  N input points the number of stages will be 
 
 S
N
= log
.
2
 
 (7.65) 
 This is where the reduction in complexity comes about: one large computation is 
reduced to several sequential, smaller computations. 
 FIGURE 7.33   The ﬂ ow diagram convention used in the DFT decomposition. The input 
is the pair of points on the left, and the two outputs are on the right. 
p + αq
p −αq
p
q
α
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
250 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 FIGURE 7.34   The calculation of an eight - point DFT from two, 4 - point DFTs. 
E (0)
E (1)
E (2)
E (3)
F (0)
F (1)
F (2)
F (3)
X (0)
X (1)
X (2)
X (3)
X (4)
X (5)
X (6)
X (7)
W 0
W 1
W 2
W 3
 FIGURE 7.35   An eight - point DFT decimation. Although it looks more complicated and 
employs multiple stages, each stage is recursively computed from the previous stage. Note 
also that the output is in the correct order (0, 1, 2,  ·  ·  · ) for the given input order. 
W 0
W 1
W 0
W 1
x(0)
x(4)
x(2)
x(6)
x(1)
x(5)
x(3)
x(7)
X (0)
X (1)
X (2)
X (3)
X (4)
X (5)
X (6)
X (7)
Stage 1
Stage 2
Stage 3
x(n)
X (k)
Four, two-point
transforms
Four, four-point
transforms
One, eight-point
transforms
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.12 THE FFT 
251
 To summarize, the steps for computing the DFT via decimation are:
 1.  Shufﬂ e the input order (discussed shortly). 
 2.  Compute  N 2, two - sample DFTs. 
 3.  Compute  N 4, four - sample DFTs. 
 4.  Continue until one,  N - sample DFT is computed. 
 Consider stepping through an  N  =  8 point DFT.
 X k
E k
W F k
k
( ) =
( )+
( )  
 X k
N
E k
W F k
k
+
⎛
⎝⎜
⎞
⎠⎟=
( )−
( )
2
 
 W
e
N
=
−j2π  
 Stage 1 : Four, two - point transforms:
 X
E
W F
0
0
0
0
( ) =
( )+
( ) 
 X
E
W F
1
0
0
0
( ) =
( )−
( )  
 Stage 2 : Two, four - point transforms:
 X
E
W F
0
0
0
0
( ) =
( )+
( ) 
 X
E
W F
1
1
1
1
( ) =
( )+
( )  
 X
E
W F
2
0
0
0
( ) =
( )−
( ) 
 X
E
W F
3
1
1
1
( ) =
( )−
( )  
 Stage 3 : One, eight - point transform:
 X
E
W F
0
0
0
0
( ) =
( )+
( ) 
 X
E
W F
1
1
1
1
( ) =
( )+
( )  
 X
E
W F
2
2
2
2
( ) =
( )+
( ) 
 X
E
W F
3
3
3
3
( ) =
( )+
( )  
 X
E
W F
4
0
0
0
( ) =
( )−
( ) 
 X
E
W F
5
1
1
1
( ) =
( )−
( )  
 X
E
W F
6
2
2
2
( ) =
( )−
( )  
 X
E
W F
7
3
3
3
( ) =
( )−
( )  
 There are log 2  N stages, with each stage having  N 2 complex multiply operations. 
The complexity is then of the order of   N
N
2
2
(
)
(
)
log
. This may be compared with 
a straightforward DFT complexity, which is of the order of  N 2 . 
 For example, if  N  =  1024 input samples, a DFT requires approximately 10 6 
operations, whereas an FFT requires approximately 500  ×  log 2 1024 or 5000 
operations, which is a 200 - fold reduction. To put this in perspective, if a complex 
arithmetic operation takes (for example) one microsecond, this is equivalent to 
reducing the total time taken to compute the DFT from from 1 second down to 5 
milliseconds, which is quite a substantial reduction. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
252 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 The only remaining complication is that of the  “ reordered ” input sequence. 
The original inputs were
 x
x
x
x
x
x
x
x
0
1
2
3
4
5
6
7
( )
( )
( )
( )
( )
( )
( )
( ) 
 Since each stage involves splitting the sequence into even - numbered and odd -
 numbered subsequences, it is really equivalent to examining the least - signiﬁ cant bit 
of the indexes (0  =  even, 1  =  odd). This is shown in Figure  7.36 . 
 Comparing the bit values of the indexes in the upper row and the lower row 
of the ﬁ gure, they are seen to be in reversed bit order:
 x
x
000
000
(
) →(
)  
 x
x
001
100
(
) →(
) 
 x
x
010
010
(
) →(
)  
 x
x
011
110
(
) →(
) 
 x
x
100
001
(
) →(
) 
 x
x
101
101
(
) →(
)  
 x
x
110
011
(
) →(
) 
 x
x
111
111
(
) →(
)  
 For obvious reasons, this is usually termed  “ bit - reversed ” addressing. As a conse-
quence of the successive partitioning of the input sequences by a factor of two, the 
number of input samples  must be a power of two — typically 256, 512, 1024, 2048, 
4096, or larger. 
 7.13  THE DCT 
 The discrete Fourier transform plays a very important — some would say essential —
 role in digital signal processing. A distinct but related transform, the DCT, has 
emerged as the method of choice for many digital compression systems. In particu-
 FIGURE 7.36   DFT decimation indices. At each stage, the lowest - order bit is used to 
partition the sequence into two. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.13 THE DCT 
253
lar, high deﬁ nition TV and JPEG still image compression are based on the DCT, as 
well as important audio coding standards such as MP3. This section introduces the 
concept of transform - domain coding with a focus on the DCT, and examines the 
concept of an optimal transform. 
 7.13.1  Transform Approach to Data Compression 
 In the preceding sections, we demonstrated that the Fourier transform provides a 
means to convert a time - sampled waveform into its constituent frequency compo-
nents. Importantly, the transform is invertible — that is, we can convert back from 
the frequency components in order to reconstruct a time domain waveform. In the 
ﬁ eld of data compression, the objective is to minimize the number of bits needed to 
encode a given source. The key link is that given humans ’ perception of audio and 
images, not all frequency components are necessarily important in encoding the 
source. In the Fourier domain, a simplistic view of this would be to discard the 
higher - frequency components, and encode only those in the lower bands, which are 
deemed necessary according to human perceptual criteria. So the system ﬂ ow is 
broadly as follows:
 
Data
Source
Forward
Transform
Quantize
Compressed Data
Inver
→
→


se
Transform
Recovered
Data
→
 
 The question then becomes how to determine the best type of transformation to be 
applied to the source data. Many researchers have investigated the use of the Fourier 
transform; however, the most widely used transform for image and picture coding/
compression is the DCT. In the case of image encoding, a greater degree of redun-
dancy may be removed by processing the image in blocks, rather that as a linear 
sequence. That is, we process the image in matrix blocks rather than vectors. For 
computational and practical reasons, a block size of 8  ×  8 is commonly chosen. 
 7.13.2  Deﬁ ning the Discrete Cosine Transform 
 As one might expect, the DCT is not unlike the DFT, in that it is based on trigono-
metric functions (Ahmed et al. 1974). However, where the DFT utilizes paired sine 
and cosine functions (condensed into one complex exponential), the DCT uses only 
cosine components. We will initially investigate the one - dimensional DCT in order 
to introduce the concepts. 
 Let the input vector  x deﬁ ne the input samples, taken as a block at some point 
in time. The output vector  y resulting from the transformation contains the transform 
coefﬁ cients. As will be seen, the process can be written as a matrix multiplication
 
 y
Ax
=
,  
 (7.66) 
where the transformation is represented as a transform matrix  A . The transformation 
matrix stores the coefﬁ cients, which are normally ﬁ xed. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
254 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 Given this terminology, the DCT in one dimension for a block of  N samples 
is deﬁ ned as
 
 y
x
k
N c
n
n
k
N
k
N
k
n
N
( ) =
( )
+
(
)
⎛
⎝⎜
⎞
⎠⎟
=
−
=
−
∑
2
2
1
2
0 1
1
0
1
cos
, ,
,
.
π

 
 (7.67) 
 The constant  c k is deﬁ ned as
 
 c
k
k
k =
=
≠
⎧
⎨⎪
⎩⎪
1
2
0
1
0
:
:
. 
 (7.68) 
 In order to understand the operation of the DCT, and in particular its application in 
data compression, consider a simple one - dimensional transform where the block size 
is two. The transformation matrix is
 
 A2 2
1
2
1
1
1
1
× =
−
⎛
⎝⎜
⎞
⎠⎟.  
 (7.69) 
 If we apply this transform to two adjacent data points, we see that the transformed 
output will be the sum and difference of the data points. In effect, we have a low -
 pass or average output as the ﬁ rst transform coefﬁ cient, and a difference or high - pass 
output as the second transform coefﬁ cient. The ﬁ rst coefﬁ cient, in effect, provides 
the broad approximation, and the second coefﬁ cient provides the detail. Furthermore, 
we can deﬁ ne the basis vectors as being formed by the rows of the transform matrix. 
 The inverse DCT (IDCT) is deﬁ ned as
 
 x
y
n
N
c
k
n
k
N
n
N
k
k
N
( ) =
( )
+
(
)
⎛
⎝⎜
⎞
⎠⎟
=
−
=
−
∑
2
2
1
2
0 1
1
0
1
cos
, ,
,
.
π

 
 (7.70) 
 The DCT is completely invertible. That is, performing the inverse DCT on the DCT 
of a vector gets back the original vector. This is equivalent to ﬁ nding the inverse of 
the transformation matrix  A . 
 As with the DFT, the concept of basis vectors helps our understanding of the 
DCT. Whereas the basis vectors for the DFT were sine and cosine waveforms, the 
basis vectors for the DCT are
 
 
bk
k
N c
n
k
N
n
N
k
N
=
+
(
)
⎛
⎝⎜
⎞
⎠⎟
=
−
=
−
2
2
1
2
0 1
1
0 1
1
cos
, ,
,
, ,
,
.
π


 
 (7.71) 
 The DCT is employed extensively in image encoding. For images, a more natural 
structure to employ is in two dimensions, since not only are images inherently two -
 dimensional, there is redundancy in both the vertical and horizontal directions. Thus, 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.13 THE DCT 
255
we need a two - dimensional DCT. This is an extension in both directions of the one -
 dimensional DCT. Formally, the two - dimensional DCT is deﬁ ned as
 
 Y
X
k l
N c c
m n
m
k
N
n
l
N
k
l
n
,
,
cos
cos
(
) =
(
)
+
(
)
⎛
⎝⎜
⎞
⎠⎟
+
(
)
⎛
⎝⎜
⎞
⎠⎟
2
2
1
2
2
1
2
π
π
=
−
=
−
∑
∑
=
−
0
1
0
1
0 1
1
N
m
N
k l
N
,
, ,
,
.

 
 (7.72) 
 Again, we have an inverse, which is deﬁ ned as
 
 X
Y
m n
N
c c
k l
m
k
N
n
l
N
k
l
l
,
,
cos
cos
(
) =
(
)
+
(
)
⎛
⎝⎜
⎞
⎠⎟
+
(
)
⎛
⎝⎜
⎞
⎠⎟
2
2
1
2
2
1
2
π
π
=
−
=
−
∑
∑
=
−
0
1
0
1
0 1
1
N
k
N
m n
N
,
, ,
,
.

 
 (7.73) 
 So let us look at the interpretation of the two - dimensional transform. Extracting the 
basis vectors, a pictorial representation is shown in Figure  7.37 . This gives us an 
intuitive insight into the DCT as a 2D data compressor: the transformed coefﬁ cients 
are, in effect, a linear combination of the basis vectors. As will be observed from 
the ﬁ gure, the basis vectors form various patterns according to the cosine functions 
in each direction. The lower - order coefﬁ cients are those in the upper left of the 
picture, and it may be observed that these map to lower frequency image character-
istics. Conversely, the lower - right side of the images shows the higher - frequency 
basis vectors (or, more correctly, basis matrices). 
 Since the reconstructed subblock is effectively a linear combination of the 
basis images, higher frequency components, which correspond to greater perceptual 
detail, may be omitted by simply not encoding the higher coefﬁ cients (those on the 
 FIGURE 7.37   The DCT basis images for an 8  ×  8 transform. Each block represents an 
8  ×  8 block of source pixels, and each block produces exactly one DCT coefﬁ cient. This 
coefﬁ cient represents the  “ weighting ” of the corresponding basis block. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
256 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
lower - right side of the output matrix). We will see how this is done, and how well 
it works in practice, in the next section.  
 7.13.3  Using the Discrete Cosine Transform 
 We now examine the operation of the DCT as a data compression transform. Figure 
 7.38 shows a source image to be encoded. The DCT is applied to each 8  ×  8 sub-
block in the image in turn, and the statistics of the coefﬁ cients are examined. The 
transform coefﬁ cients form an array of 8  ×  8 coefﬁ cient matrices, one per image 
subblock. 
 As discussed, the coefﬁ cients in the upper - left of the transformed matrix form 
the lower - level approximation to the subblock, and the lower - right coefﬁ cients add 
additional detail. Note that coefﬁ cient  Y (0, 0) is in effect the average (sometimes 
called  “ DC ” ) value of the source subblock, and this is easily seen by putting  k  =  l  =  0 
in the 2D DCT transform speciﬁ ed by Equation  7.72 . 
 The histograms of the ﬁ rst nine coefﬁ cients, corresponding to the 3  ×  3 coef-
ﬁ cients in the upper left of the transform, are shown in Figure  7.39 . Apart from the 
average coefﬁ cient, all others exhibit a highly peaked Laplacian - like distribution, 
and may be quantized accordingly. Furthermore, since the distribution of the coef-
ﬁ cients tends to zero, we reach an important conclusion: that the higher - frequency 
coefﬁ cients require fewer bits, and may in fact require no bits at all. This corresponds 
to setting that coefﬁ cient value to zero at the decompression stage. 
 So how well does this work in practice? The best way to gage is to examine 
some test cases; Figure  7.40 shows an image which has been compressed and 
decompressed using the DCT with the retention of only lower - order DCT transform 
coefﬁ cients. It is obvious that only a limited number of coefﬁ cients need to 
be retained for good quality image reconstruction. The compression attained is 
 FIGURE 7.38   A test image for DCT compression experiments. The image contains both 
contours of approximately the same luminance, and background areas of more detail. This 
allows us to see the effect of bitrate compression on the various aspects of a typical image. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.13 THE DCT 
257
substantial: for example, retaining only 4 coefﬁ cients out of 64 is a 16:1 compression 
ratio, equivalent to 0.5 bits per pixel. 
 Care should be taken in extrapolating this result: the quality of the recon-
structed image depends on many factors, including the image resolution to begin 
with, the reproduction media, and the image itself. In addition, we have made no 
effort to quantize the transformed coefﬁ cients. The methods discussed in Section  3.6 
may be applied, and in practice (in JPEG, for example), more complex bit - assignment 
codes are employed. However, from this simple example, the potential is clearly 
demonstrated. 
 7.13.4  Vectorspace Interpretation of 
Forward and Inverse Transforms 
 Let us now look at the transformation and inverse transformation operations them-
selves, and see how they can be written in terms of vector and matrix operations. 
The forward transform is a matrix - vector multiplication
 
 y
Ax
=
,  
 (7.74) 
 FIGURE 7.39   Histogram of DCT coefﬁ cients for a typical image. The upper - left 
histogram is the average value of the blocks. Each of the other histograms represents detail 
in each direction, at successively higher frequencies. The scale on the higher - frequency 
coefﬁ cients is identical, and it is seen that the distribution tends towards a value of zero for 
higher coefﬁ cients. 
Distribution of DCT coefficients
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
258 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 which takes an input block  x and yields another (transformed) vector  y . If we write 
the transform matrix as row vectors, then we can visualize this product as
 
 y
a
a
x
0
1
=
−
−
−
−
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟

|
|
.  
 (7.75) 
 In this case, we can see that the  basis vectors are the  rows of the matrix. Each output 
coefﬁ cient in  y is nothing more than a vector dot product,
 
 y0 =
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟⋅
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
|
|
|
|
.
a
x
0
 
 (7.76) 
 The next output term is  y 1  =  a 1 · x , and so forth. In effect, we are multiplying each 
basis vector by the input vector, to give each output coefﬁ cient in turn. 
 The inverse transform is a multiplication:
 
 x
By
=
.  
 (7.77) 
 FIGURE 7.40   DCT compression — upper left, original image, 8  ×  8 sub - blocks. Upper 
right, 1  ×  1 coefﬁ cient retained; Lower left, 2   ×  2 coefﬁ cients retained; Lower right, 3  ×  3 
coefﬁ cients retained. 
Original
1 × 1 Coefficient retained
2 × 2 Coefficients retained
3 × 3 Coefficients retained
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.13 THE DCT 
259
 Let us write the matrix as column vectors, and the vector y in terms of its 
components
 
 x
b
b
=
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
|
|
|
|
|
|
.
0
1
0
1


y
y
 
 (7.78) 
 We have the basis vectors as  columns . The output may then be written a little dif-
ferently, as
 
 x
b
b
0
1
=
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟+
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟+
y
y
0
1
|
|
|
|
.
  
 (7.79) 
 Thus, the output sum  x is a weighted sum of scalar - vector products. Each scalar 
weighting is the value taken from the input vector, which is actually a transformed 
component value. We weight the basis vector by this value, which represents its 
relative importance in determining the output. Importantly, the weights may be 
arranged in decreasing order, so as to successively approximate the output vector. 
This is where the application in transform data compression comes in: less important 
basis vectors are given less weight, if we have to economize on bandwidth.  
 7.13.5  Basis Vectors and Orthogonality 
 To further explore the transform approach, it is necessary to understand the decor-
relation properties of the transform. 
 For the purpose of example, let us return back to the simpler situation of a 
one - dimensional transform. The transformation is equivalent to multiplying the 
transform matrix by the data vector. The coefﬁ cients that comprise the matrix are 
what determines the transform (DCT, DFT, or another). Such a matrix - vector product 
can be visualized as a rotation of a point in space. Consider Figure  7.41 , which 
shows a point deﬁ ned by vector  x rotated to  y using a rotation matrix  A . 
 What is the matrix  A in this case? For the two - dimensional rotation, the result 
is well known: it is the rotation matrix. To derive this, let the starting point be the 
vector  x at angle  α :
 
 x = ⎛
⎝⎜
⎞
⎠⎟
x
x
0
1
 
 (7.80) 
 
 = ⎛
⎝⎜
⎞
⎠⎟
x
x
cos
sin
.
α
α
 
 (7.81) 
 Similarly, we can write for the rotated point  y 
 
 y = ⎛
⎝⎜
⎞
⎠⎟
y
y
0
1
 
 (7.82) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
260 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 
 =
+
(
)
+
(
)
⎛
⎝⎜
⎞
⎠⎟
x
x
cos
sin
.
α
θ
α
θ
 
 (7.83) 
 We can expand this using standard identities
 
 y
x
x
=
−
(
)
+
(
)
⎛
⎝⎜
⎞
⎠⎟
cos
cos
sin
sin
sin
cos
cos
sin
α
θ
α
θ
α
θ
α
θ
.  
 (7.84) 
 Using the components from Equation  7.81 , we have:
 
 y
y
x
x
x
x
0
1
0
1
0
1
⎛
⎝⎜
⎞
⎠⎟=
−
+
⎛
⎝⎜
⎞
⎠⎟
cos
sin
sin
cos
.
θ
θ
θ
θ
 
 (7.85) 
 This can now be written in matrix form as:
 
 y
y
x
x
0
1
0
1
⎛
⎝⎜
⎞
⎠⎟=
−
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
cos
sin
sin
cos
.
θ
θ
θ
θ
 
 (7.86) 
 So the matrix  A is in effect:
 
 A =
−
⎛
⎝⎜
⎞
⎠⎟
cos
sin
sin
cos
.
θ
θ
θ
θ
 
 (7.87) 
 The rows of this transformation matrix form the  basis vectors , which deﬁ ne the axes 
for the new vector  y . Let us denote the rows of  A by:
 b0 = −
⎛
⎝⎜
⎞
⎠⎟
cos
sin
θ
θ  
 b1 = ⎛
⎝⎜
⎞
⎠⎟
sin
cos
.
θ
θ
 
 FIGURE 7.41   Transformation as a rotation in two dimensions. Vector  x at angle  α is 
rotated to vector  y at angle ( α  +  θ ). 
x
y
α
θ
x 0
x 1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.13 THE DCT 
261
 These basis vectors are  orthogonal , which means that the inner or vector dot product 
is zero
 
 b
b
k
j
k
j
⋅
=
=
⎧⎨⎩
constant
otherwise
:
:
.
0
 
 (7.88) 
 Equivalently, since the basis vectors form the columns of the inverse transformation 
matrix  B , the product of the transpose and the matrix itself equals the identity matrix 
(possibly scaled by a constant  c ),
 
 B B
I
T
c
=
.  
 (7.89) 
 To place this on a practical footing, consider Figure  7.42 . Suppose we plot pairs of 
pixels in an image as a dot, with the horizontal and vertical ( x 0 and  x 1 ) axes being 
the value of the ﬁ rst and second pixel in any given pair. Because of the strong cor-
relation between pixels, we would expect the scatter plot to contain points mostly 
within the ellipse as shown. To encode a pair of pixels, we need the values along 
the  x 0 and  x 1 axes. However, if we use a set of rotated axes  b 0 and  b 1 as shown, we 
can encode the relative position of a point in a different way. The distance along the 
 b 0 axis might be relatively large, but the distance along the  b 1 axis would be com-
paratively smaller. Thus, the smaller average value requires less accuracy in encod-
ing its value. The encoded data is formed using the rotated axes, and at the decoder, 
it is a simple matter to revert to the original axes, since the axes are simply rotated 
through a certain angle. Note that the  b  axes are still perpendicular to each other, 
and their directions could be changed by 180 ° and still be perpendicular. 
 FIGURE 7.42   Hypothetical case of pairs of pixels in an image. If plotted against the  x 0 , 
 x 1 axes, we expect a concentration as shown by the ellipse. The points are better 
represented by the rotated axes  b , and will have a larger component on the  b 0 axis and 
much smaller component on the  b 1 axis. 
b0
b1
x0
x1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
262 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 Understanding the orthogonality of the basis vectors helps in understanding 
the operation of transformation in general, and in particular why an optimal trans-
formation exists. This is the subject of the next section. 
 7.13.6  Optimal Transform for Data Compression 
 In practice, the DCT works extremely well, and this is no doubt why it has found 
its way into many useful applications. A natural question then arises — what is the 
optimal transform? Investigating this question gives a great deal of insight into the 
transformation process. Unfortunately, the optimal transform in a mathematical 
sense is also of little practical use in real applications. 
 The interdependency of the samples may be examined by means of the correla-
tion matrix, which can be written as:
 
 R =
{
}
{
}
{
}
{
}
{
}
{
}
−
−
−
E x x
E x x
R x x
E x x
E x x
E x x
E x
D
D
D
0
0
0
1
0
1
1
0
1 1
1
1






1
0
1 1
1
1
x
E x
x
E x
x
D
D
D
{
}
{
}
{
}
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
−
−
−

.  
 (7.90) 
 If we take the samples as being of dimension  D (e.g.,  D  =  2 considers two pixels at 
a time), and create a concatenation of  N  such vectors, we form a matrix  X of dimen-
sion  D  ×  N . Then we can easily form the  D  ×  D correlation matrix using
 
 R
XX
= 1
N
T.  
 (7.91) 
 So what should the optimal transform do, and how could we derive such a transform? 
Recall that the  eigenvectors  e and  eigenvalues  λ of some matrix  M satisfy
 
 Me
e
= λ  
 (7.92) 
 
 M
I
−
=
λ
0. 
 (7.93) 
 An important property of the eigenvectors is that they are orthogonal,
 
 e
e
j
k
j
k
j
k
⋅
=
=
≠
⎧⎨⎩
1
0
:
:
.  
 (7.94) 
 Let the new data set { y } be formed from a linear transformation of the original data 
set { x }. If the transformation matrix (as yet unknown) is  A , then we transform each 
vector using
 
 y
Ax
=
 
 (7.95) 
 as before. The correlation matrix of  x may be formed using
 
 R
xx
x
T
E
=
{
}.  
 (7.96) 
 Similarly, the correlation matrix of  y is
 
 R
yy
y
T
E
=
{
}.  
 (7.97) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.13 THE DCT 
263
 So looking at the components of the correlation matrix and how it is formed in 
Equation  7.90 , we can say that we want the correlation matrix  R y to be a diagonal 
matrix. That is, the correlation of every component with every other component 
ought to be zero (or at least, as close to zero as possible). In this case, the (trans-
formed) data is said to be uncorrelated. Of course, the correlation of a component 
with itself cannot be zero, and these two facts together mean that the correlation 
matrix of the transformed data must form a diagonal matrix. So the correlation 
matrix of the new data is
 
 
R
yy
Ax Ax
Axx A
A
xx
A
AR A
y
T
T
T
T
T
T
x
T
E
E
E
E
=
{
}
=
(
)
{
}
=
{
}
=
{
}
=
.
 
 (7.98) 
 In deriving this, we have used the general identity ( AB ) T  =  B T A T , and also the fact 
that  A is a constant and is thus independent of the expectation operator. 
 The key to the transformation operation lies in the properties of the eigenval-
ues and eigenvectors of the autocorrelation matrix  R x . So applying Equation  7.92  to 
the autocorrelation matrix, and then premultiplying by the eigenvector,
 
 R e
e
x
k
k
k
= λ
 
 (7.99) 
 
 ∴
=
e R e
e
e
j
T
x
k
j
T
k
k
λ
 
 (7.100) 
 
 = λk
j
T
k
e e .  
 (7.101) 
 Since the eigenvectors are orthogonal, the right - hand side reduces to  λ k if  j  =  k , and 
zero otherwise. This is what we want: the correlation of the transformed data to be 
zero. If we compare Equation  7.98 with 7.101, we can see that the transform matrix 
 A would have each row being an eigenvector. So we have a way of deﬁ ning the 
optimal transformation matrix  A — it ought to have its rows equal to the eigenvectors 
of the correlation matrix.
 
 
|
|
|
|
.
y
x
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟=
−
−
−
−
−
−
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
−
e
e
eD
0
1
1

 
 (7.102) 
 This may be interpreted as a projection of the input vector onto each eigenvector in 
turn. So we now use the transformed data and calculate the correlation matrix as 
derived earlier in Equation  7.98 
 
 R
AR A
y
x
T
=
,  
 (7.103) 
 with  A composed of the eigenvectors along its rows, and  R x being a constant, the 
previous result yields
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
264 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 
 Ry
D
=
⎡
⎣
⎢
⎢
⎢
⎢
⎤
⎦
⎥
⎥
⎥
⎥
−
λ
λ
λ
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0

.  
 (7.104) 
 So we can see that the transformed data has an autocorrelation matrix which is 
diagonal, and thus the transformed data is uncorrelated. 
 Finally, if the eigenvalues are ordered in decreasing order, the transformed 
data may be  approximated using a weighted combination of the corresponding 
eigenvectors. The example below illustrates these calculations. We generate random 
data vectors  x , in which the ﬁ rst component is random with a Gaussian distribution, 
and the second component is correlated with the ﬁ rst. Thus the matrix  X is 2  ×  400. 
 
 N  = 400; 
 x1  =  randn (N, 1); 
 x2  =  randn (N, 1); 
 X  = [(x1) (0.8  * x1  + 0.4  * x2)] ’ ; %  each column is an observation vector  
 C  = (X * X ’ )/ length (X); 
 [V D]  =  eig (C); 
 % sort according to eigenvalue order 
 EigVals  =  diag (D); 
 [EigVals, EigIdx]  =  sort (EigVals); 
 EigVals  =  ﬂ ipud (EigVals); 
 EigIdx  =  ﬂ ipud (EigIdx); 
 % transform matrix 
 T  = V(:, EigIdx) ’  
 fprintf (1,  ’ Transform matrix:\n ’ ); 
 disp (T); 
 for k  = 1:N 
      v  = X(:, k); 
      vt  = T * v; 
      XT(:, k)  = vt; 
 end 
 We calculate the eigenvectors and eigenvalues of the correlation matrix using 
the  eig (); function. We then transform each observation vector in a loop, to yield 
the XT vector. 
 Figure  7.43 illustrates these concepts. The scatter plot shows the  x 1 ,  x 2 data, 
with the ellipse showing the concentration of the data. What we require is a set of 
two new axes as indicated. Using the untransformed data, the variance along both 
axes is large. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.13 THE DCT 
265
 FIGURE 7.43   Orthogonal axes for correlated data are formed by the eigenvectors of the 
correlation matrix. The rotated coordinate system thus has a larger variance on one axis and 
smaller variance on the other, corresponding to the major and minor axes of the ellipse in 
this two - dimensional example. 
−6
−4
−2
0
2
4
6
−6
−4
−2
0
2
4
6
x1
x2
Orthogonal axes
x1’
x2’
 However, using the rotated axis coordinate system, the variance is large along 
one axis but small along the other axis. This is the essence of the compression func-
tion: we can utilize a smaller number of high - variance components, and coarsely 
quantize or even not use the low - variance components. The variance corresponds to 
the major and minor axes of the ellipse, and the axes are determined by the eigen-
values of the correlation matrix. 
 The above implementation yields a correlation matrix of the raw data of
 1 0766
0 8308
0 8308
0 7956
.
.
.
.
⎛
⎝⎜
⎞
⎠⎟.  
 The eigenvector matrix is:
 
0 6455
0 7638
0 7638
0 6455
.
.
.
.
−
−
−
⎛
⎝⎜
⎞
⎠⎟.  
 The transform axes comprise each column vector of this matrix. The transform 
matrix is formed from the eigenvectors as rows:
 A = −
−
−
⎛
⎝⎜
⎞
⎠⎟
0 7638
0 6455
0 6455
0 7638
.
.
.
.
. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
266 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 The eigenvalues of the transformed data are:
 0 0935
1 7787
.
.
⎛
⎝⎜
⎞
⎠⎟,  
 which indicates a high correlation on one axis, and a much smaller correlation on 
the other. 
 The scalar dot product of the eigenvectors is zero, as we hoped for. After 
transformation, the correlation matrix of the transformed data is
 1 7787
0 0000
0 0000
0 0935
.
.
.
.
⎛
⎝⎜
⎞
⎠⎟.  
 This is a diagonal matrix, and shows that one component has signiﬁ cantly higher 
variance than the other. 
 So we have derived the optimal transformation to apply to given data in order 
to compact the input data representation into as few output values as possible. Since 
the transformation is invertible, we can reconstruct the original data. However, in 
practice, the eigenvectors are difﬁ cult to compute, and of course this computation 
depends on knowing the autocorrelation matrix, as shown above. The autocorrelation 
matrix itself is time - consuming to calculate, and requires a large sample of data. In 
the data compression/transmission problem, the decoder (receiver) also needs to 
know the transformation matrix to effect the reconstruction of the data. So even if 
we compute the optimal transform, it must be given somehow to the decoder, and 
this works against what we are trying to achieve (compressing the data into fewer 
bits). For these reasons, suboptimal transforms such as the DCT are normally used 
in practice, since they can approximate the optimal transform for typical sources. 
Although the compression is not optimum in a mathematical sense, it solves the 
problem of informing the decoder of the transform coefﬁ cients to use, since they 
are known in advance and determined by the DCT equations. 
 7.14  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  the  Fourier series and  Fourier transform . 
 •  the  interpretation of the Fourier transform. 
 •  the development of the  FFT algorithm. 
 •  the  DCT algorithm, its use in data compression, and relationship to the optimal 
decorrelating transform.         
 PROBLEMS 
 7.1.  Using the MATLAB code shown in Listing  7.1 , change the input signal to each of 
the following signals and examine the  a k and  b k coefﬁ cients.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.14 CHAPTER SUMMARY 
267
 (a)  A two - component sine wave:  x ( t )  =  sin 2 π t  +  sin(4  ×  2 π t ) 
 (b)  A sine plus cosine waveform:  x ( t )  =  cos 2 π t  +  2 sin 2 π t 
 Are they equal to the signal equation amplitudes/frequencies? 
 7.2.  It was stated in Section  7.4 that terms of the form  ∫0
0
τ a
k
t 
 n
t dt
k
o
cos
cos
Ω
Ω
 equate to 
zero for  n  ≠  k . Prove this statement mathematically by expanding the integration and 
noting that  τ
π
= 2
0
Ω . 
 7.3.  Use MATLAB to generate an input signal as described below, and use the Fourier 
series code as given in Listing  7.1 to determine the resulting Fourier series expansion. 
Examine the  a k and  b k coefﬁ cients, and calculate the corresponding  A and  φ . Does 
the Fourier series give the expected result in each case? 
 (a)  A phase - shifted sinusoidal wave (positive — shift to left) 
 x t
t
( ) =
+
⎛
⎝⎜
⎞
⎠⎟
sin 2
3
π
π  
 (b)  A phase - shifted sinusoidal wave (negative — shift to right) 
 x t
t
( ) =
−
⎛
⎝⎜
⎞
⎠⎟
sin 2
3
π
π  
 (c)  A square wave with a phase shift of  −π 2 radians. 
 (d)  A square wave with a phase shift of  +π 2 radians. 
 (e)  A random (nonperiodic) signal. 
 (f)  A triangular waveform. 
 (g)  A square wave with 10% duty cycle ( + for ﬁ rst 10%,  −  otherwise). 
 7.4.  Write a MATLAB function to perform the inverse DFT. Check that for any given time 
waveform, the DFT and IDFT are inverses of one another. 
 7.5.  Section  7.6.5 showed how to implement the DFT equation directly with two 
nested loops, and subsequently how to perform a conversion to a vectorized version 
with only one loop and one vector dot product within the loop. It is possible to 
code the DFT using only one matrix - vector multiplication, where the matrix is com-
posed of ﬁ xed coefﬁ cient values and the vector is the time - sampled input vector. Show 
how to implement the DFT in this way, and check your results using the DFT code 
given. 
 7.6.  Explain the following in terms of frequency:
     x  =  [1  − 1    1  − 1   1  − 1    1  − 1]; 
     XX   =  fft (x) 
            XX   =  0     0      0     0      8     0     0     0 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
268 
CHAPTER 7 FREQUENCY ANALYSIS OF SIGNALS
 7.7.  Explain the following:
         x   =  [3 5 7 9] 
         sum (x) 
             ans  =  24 
         fft (x) 
             ans  =  24.0      − 4.0   +  4.0i     − 4.0     − 4.0  − 4.0i   
 7.8.  An ECG is sampled at 500  Hz, and a data record of length 256 is used for an FFT 
analysis. What is the frequency resolution of the FFT components? If a resolution of 
1  Hz is required, what would the minimum data length be for analysis? 
 7.9.  We wish to investigate the following relationship:
 
x n
N
X k
n
N
k
N
( ) =
( )
=
−
=
−
∑
∑
2
0
1
2
0
1
1
.  
 This shows that the sum of the energy in a sampled signal (left - hand side) is propor-
tional to the sum of the energy in the frequency domain (Fourier transformed).
 (a)  Test this relationship using some data:
         N  =  32; 
         x  =  randn (N, 1); 
         sum ( abs (x). ˆ 2) 
         X  =  fft (x); 
         sum ( abs (X). ˆ 2)/N  
 Verify that this code implements the equations speciﬁ ed. Try for differing  N . 
 (b)  Write the energy summation term  ∑
( )
n x n
2  as  ∑
( )
( )
n x n x
n
*
, where the  * 
denotes the complex conjugate.  
 (c)  Substitute the inverse DFT for  x  *  ( n ). Reverse the order of the summation 
and simplify, to give the right - hand side  1
2
N
X k
k
(
)∑
( ) . This shows 
that the original relationship holds for all sets of samples  x ( n ). This 
relationship is a form of what is known as Parseval ’ s theorem. Energy is 
conserved in going from the time domain to the frequency domain, as we 
would expect.  
 7.10.  Determine the bits per pixel (bpp) rate for each of the DCT compressed images in 
Figure  7.40 . 
 7.11.  Given a transform matrix  A whose rows are orthogonal vectors, what is the 
product  AA T ? Given the vectorspace interpretation of transform and inverse trans-
form, is the relationship between forward transform matrix  A and its corresponding 
inverse  B ? 
 7.12.  Given a sequence in vector  x which has zero mean and unit variance, and correlation 
coefﬁ cient  ρ , 
 (a)  What is the correlation matrix  R x ? 
 (b)  Show that the eigenvalues are 1   ±  ρ . 
 (c)  Show that the eigenvectors are:
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
7.14 CHAPTER SUMMARY 
269
 c
c
1
1
1
1
⎛
⎝⎜
⎞
⎠⎟
−
⎛
⎝⎜
⎞
⎠⎟
and
.  
 (d)  What is the value of  c to make these vectors orthonormal (that is, both orthogo-
nal and having unit magnitude)?  
 (e)  What would be the optimal transform in that case? The inverse transform? 
 7.13.  Using the method explained in Section  7.13.2 , implement and test a DCT function in 
MATLAB as follows.
 (a)  Implement an  N  =  8 - point DCT to compute the DCT directly. 
 (b)  For an input of  x ( n )  =  1 for all  n , verify mathematically that the DCT output 
 y ( k ) should equal  N  for  k  =  0. 
 (c)  Verify that your implementation gives  y 0
8
( ) =
 for the case of  N  =  8 and 
 x ( n )  =  1. 
 7.14.  Fast algorithms have been put forward for computing the DCT. This question is based 
on using an  N - point FFT to compute an  N - point DCT (see Narashima and Peterson 
1978).
 (a)  If not done already, implement a DCT in MATLAB as in the previous 
question. 
 (b)  For an  N - point input sequence  x ( n ), create a new sequence  y ( n ), such that 
 y ( n )  =  x (2 n ) and  y ( N  −  1  −  n )  =  x (2 n  +  1) for  n  =  0,  ·  ·  ·  ,  N 2. Using the input 
sequence  x ( n )  =  [0, 1, 2, 3, 4, 5, 6, 7], verify by hand that  y ( n )  =  [0, 2, 4, 6, 7, 
5, 3, 1]. Note that the ﬁ rst half of  y ( n ) is made up of the even - indexed values 
from  x ( n ), and that the second half of  y ( n ) comprises the odd - indexed values 
from  x ( n ) in reversed order. 
 (c)  Implement the above reordering in MATLAB, and verify against the same test 
case. 
 (d)  Compute the transform values  F ( k ) using 
 F k
N c
e e
y n
k
k
N
( ) =
( )
(
)
{
}
−
(
)
2
2
R
j π
FFT
,  
where  ck = 1
2
 for  k  =  0, and  R e means take the real value (ignoring the 
imaginary component if present). Be sure to use the new sequence  y ( n ) in 
computing  F ( k ), and the original sequence  x ( n ) in computing the DCT itself. 
Verify against the output of the previous question that  F ( k ) produces the same 
DCT result for  x ( n ) for a random 8 - sample input case. 
 (e)  Considering that the above Fast DCT (FDCT) is based on an FFT and a 
complex - number multiplication for each term, what is the approximate speedup 
over the direct DCT calculation? 
 
 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER 8 
DISCRETE - TIME FILTERS 
 8.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to
 1.  explain the basic types of ﬁ lters and approaches to  ﬁ lter design ; 
 2.  design a  ﬁ nite impulse response (FIR) ﬁ lter for a given frequency 
speciﬁ cation; 
 3.  plot the  frequency response of a ﬁ lter; 
 4.  be conversant with ﬁ lter  windows and where they are used; and 
 5.  explain and implement  fast ﬁ ltering algorithms . 
 8.2  INTRODUCTION 
 This chapter introduces digital ﬁ lters, which are able to remove (ﬁ lter out) or 
enhance certain frequency components in a signal. Digital ﬁ lters are a key technol-
ogy in many audio processing applications and are fundamental to communications 
systems. The idea of a digital ﬁ lter is to either reduce or increase the strength of a 
signal but only over speciﬁ c frequency ranges. Digital ﬁ lters are used in various 
applications, a few of which are 
 1.  audio recording/playback/enhancement applications, 
 2.  noise removal, 
 3.  sub - band ﬁ ltering for audio and image storage and/or transmission, and 
 4.  narrow - band ﬁ ltering for selecting a communications channel. 
 To give a concrete example, consider Figure  8.1 , which shows a sample application —
 measuring the electrocardiogram (ECG) signals as derived from the beating of the 
human heart. Such a measurement has a great many clinical and diagnostic applica-
tions and is attractive because the signal can be measured at various points on a 
patient ’ s skin, thus monitoring the heart externally. The problem is that the evoked 
potentials on the skin are somewhat removed from the heart muscle itself, and hence 
the signals are tiny (of the order of microvolts to millivolts). Typically, an ampliﬁ ca-
tion of the order of 1,000 times is required to adequately quantize such signals. Such 
271
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
272 
CHAPTER 8 DISCRETE-TIME FILTERS
small signals with high ampliﬁ cation are very susceptible to noise from external 
sources — the body and connecting leads act as antennas which pick up electromag-
netic interference from power lines and other sources, and other muscle signals 
within the body are also present. Thus, a signal processing approach is needed to 
ﬁ lter the measured signal and leave only the desired portion. This has to be achieved 
without removing or altering the desired signal and without introducing any artifacts 
in the observed waveform.  
 8.3  WHAT DO WE MEAN BY  “ FILTERING ” ? 
 In order to understand ﬁ lters, the concepts of the  gain and  phase response of a system 
must be well understood. Figure  8.2 illustrates how we might have a sinusoidal 
waveform, which is the input to a ﬁ lter, and the corresponding output. Although 
most signals encountered in practice are more complex than a simple sinusoid, it 
may be recalled from the Fourier series in Chapter  7 that any periodic signal may 
 FIGURE 8.1   A ﬁ ltered electrocardiograph (ECG) signal. Note the signiﬁ cant amount of 
interference (noise) present in the raw electrode signals and the relatively  “ clean ” signal 
derived below. 
ECG electrodes (3) and filtered signal
0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Time (second)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.3 WHAT DO WE MEAN BY “FILTERING”? 
273
 FIGURE 8.2   The output of a ﬁ lter consists of one or more frequency components. The 
magnitude of each component will be altered according to the ﬁ lter ’ s transfer function, and 
the phase will show a change with respect to the input. Normally, we are interested in the 
steady - state response only — that is, the time after the transient response. 
Time
Amplitude
Time response of filter
Filtered
Input
Phase
Gain ratio
Transient response
be broken down into its component sine and cosine waves. Thus, we are quite 
entitled to consider only a simple sinusoidal waveform in our discussion of digital 
ﬁ lter responses, as complex waveforms are made up of many additive sinusoidal 
components. Put another way, the ﬁ lter acts on each component of the input wave-
form independently, with the ﬁ nal output being the summation of the effects on each 
individual component.  
 Returning to Figure  8.2 , we see that the output of our (as yet hypothetical) 
ﬁ lter is another sinusoid, with the following characteristics:
 1.  The frequency of the output is identical to that of the input. 
 2.  The amplitude may be greater or lesser. 
 3.  The time of occurrence of the output is changed with respect to the input. 
 The ratio of the output to input amplitudes is termed the gain of the ﬁ lter. The delay 
is expressed not in absolute terms but in relative terms of phase with respect to the 
input. The phase angle is a convenient way to represent this delay, as it inherently 
incorporates the frequency of the waveform (since phase is a relative quantity). 
 Our ﬁ lter may be subjected to not one, but numerous input frequencies. We 
require a concise method of showing the response of the ﬁ lter, which conveys the 
information we require. Such a graphical visualization of the gain and phase at each 
frequency is illustrated in Figure  8.3 . Usually, but not always, we are interested in 
the gain response. Note that the gain is often quoted in decibels, which is 20  log 10 | G |, 
where | G | is the output/input gain ratio. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
274 
CHAPTER 8 DISCRETE-TIME FILTERS
 The magnitude of a complex number may be found using the  abs () operator 
in MATLAB. The phase may be found using  angle (). Since the phase angle is 
cyclical over 2 π , the phase graphs often exhibit a  “ wrapping ” effect. For example, 
successive phase lags of  − 350 ° ,  − 355 ° , and  + 5 ° are better interpreted as  − 350 ° , 
 − 355 ° , and  − 365 ° . The MATLAB function  unwrap () is able to effect this for a 
given vector of complex numbers.  
 8.4  FILTER SPECIFICATION, DESIGN, 
AND IMPLEMENTATION 
 Designing and implementing a discrete - time ﬁ lter requires a three - step process:
 1.  Specify the frequency response, according to the problem at hand. This 
requires choosing an appropriate sample rate,  f s , and a response shape. 
 2.  Design the ﬁ lter itself by selecting an algorithm and by determining the 
coefﬁ cients. 
 3.  Implement the difference equation derived from the design step. 
 Generally speaking, the design process tends to be iterative since there is no ideal 
way of designing a ﬁ lter for all situations. Filtering is normally thought of as an 
operation in the frequency domain, although the operation of ﬁ ltering is imple-
mented as a difference equation in the time domain. Recall from Section  3.10 that 
the general form of a difference equation is
 FIGURE 8.3    The gain and phase response curves allow determination of the gain (upper) 
and phase (lower) of the ﬁ lter transfer function at any given frequency. The normalized 
frequency is shown ( ω radians per sample). This ﬁ lter is a ﬁ nite impulse response (FIR) 
ﬁ lter of order  N  =  201. The gain is shown as a ratio, and the phase is shown in radians. 
Bandpass filter gain and phase
ω
ω
0.5
1.0
0
0
0
0
–400
π/4
π/4
π/2
π/2
3π/4
3π/4
π
π
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.4 FILTER SPECIFICATION, DESIGN, AND IMPLEMENTATION  
275
 
 y n
b x n
b x n
b x n
N
a y n
a y n
a y
N
M
( )
(
( )
(
)
(
))
(
(
)
(
)
=
+
−
+
+
−
−
−
+
−
+
+
0
1
1
2
1
1
2


(
)).
n
M
−
 
 (8.1) 
 The operation of designing a ﬁ lter, once the speciﬁ cations for its response are known, 
essentially reduces to choosing the required transfer function order, and choosing 
the difference equation coefﬁ cients (all ( N  +  1),  b values, and  M ,  a values). As will 
be shown in the next section, this may be considered as  “ placing ” the poles and/or 
zeros of a system on the complex plane. 
 We may summarize the steps required to design and implement a discrete - time 
ﬁ lter as follows: 
 General Filter Design Steps 
 1.  Specify the response required and determine an appropriate sampling frequency,  f s . 
 2.  Determine the transfer function  G ( z ) from the speciﬁ cation. 
 3.  Let  Y z
X z
G z
( )
( ) =
( ). Solve for  Y ( z ). 
 4.  Convert  Y ( z ) into a difference equation, that is,  y ( n ) in terms of  x ( n ),  x ( n  −  1), 
 y ( n  −   1), and so on. 
 5.  Code a loop to sample the input  x ( n ), calculate  y ( n ), output  y ( n ), and buffer the 
samples. 
 This describes the process in general terms; for speciﬁ c ﬁ lter types described 
in the next sections, the process will vary a little in the details — primarily the cal-
culation of  G ( z ), which is the most technically difﬁ cult stage. Since we are examin-
ing digital or discrete - time ﬁ lters, the sampling rate needs to be chosen accordingly, 
so as to avoid aliasing. This was examined in detail in Section  3.8 . 
 In general, the determination of  G ( z ) is the most difﬁ cult step, and there are 
many ways to approach this. The following sections introduce some possible 
approaches to solving this problem. First, however, we need to more clearly specify 
what we mean by  specifying a ﬁ lter ’ s response. 
 8.4.1  Revisiting Poles and Zeros 
 A ﬁ lter is simply a transfer function, and we have examined these before. Recall 
that a transfer function is a ratio of a numerator polynomial in  z to a denominator 
polynomial in  z , and that we can factorize each into roots to give zeros and poles, 
respectively. So we can imagine the design process as one of specifying the poles 
and zeros in such a way as to give us the desired frequency response. 
 Of course, the implementation of the ﬁ lter will be done as a discrete - time dif-
ference equation, which is derived from the transfer function. So we are working in 
several domains:
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
276 
CHAPTER 8 DISCRETE-TIME FILTERS
 1.  the frequency domain (what we want), 
 2.  the time domain (how we implement what we want), and 
 3.  the transfer function (which describes the system in a mathematical sense). 
 The key to understanding the linkage between these abstract viewpoints is the poles 
and zeros themselves. Section  5.8 explained how the poles and zeros may be thought 
of as a set of vectors, with frequency response conceived graphically as a mapping 
from the given frequency to a point on the unit circle. Figure  8.4 illustrates this idea, 
for the case where we have two poles and two zeros. The poles and zeros describe 
the transfer function, with each factor of the numerator polynomial being of the 
form  “ ( z   –  zero). ” The denominator is formed in a similar fashion, as a product 
of  “ ( z  –  pole) ” terms. 
 The response of the system depends on the frequency of the signal which is 
applied to that system. This does not mean we are restricted to a single - frequency 
input — most real - world signals are composed of a multitude of input frequencies. 
But by the principles of superposition and linearity (refer to Section  3.11 ), we are 
able to consider each of these separately. 
 Figure  8.4 shows a snapshot of two frequencies as points on the unit circle. 
As introduced in Section  5.7 , the response is evaluated for a point,  1ejω, on the 
complex plane, where  ω is the frequency relative to the sampling frequency. Note 
that the radius of this point is one. Hence, as we vary the frequency from 0 (or  “ DC ” ) 
up to  ω  =  π (which corresponds to half the sample frequency), the point moves 
around the top half of the unit circle. The ﬁ gure shows the vectors to the poles, 
and the vectors to the zeros. The frequency response magnitude is then the 
product of the zero vector lengths divided by the product of the pole vector lengths. 
Clearly, when the frequency is such that the point on the unit circle is close to 
 FIGURE 8.4   Illustrating pole – zero vectors on the unit circle. The  “ real ” frequency in 
hertz relative to the sampling frequency is converted into a corresponding angle. We then 
draw vectors from the point  P
e
= 1 jω to the poles (shown as  × ) and zeros (shown as   ). As 
we change the input frequency, we change the angle  ω  =   P from the origin to point  P 
and redraw the vectors. 
−1.5 −1.0 −0.5
0
0.5
1.0
1.5
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
z Plane
Real
Imaginary
Unstable region
P
−1.5 −1.0
−0.5
0
0.5
1.0
1.5
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
z Plane
Real
Imaginary
Unstable region
P
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.4 FILTER SPECIFICATION, DESIGN, AND IMPLEMENTATION  
277
the pole, the magnitude will be large. Similarly, when the frequency is such that 
the point is close to a zero, the magnitude will be small. So, we can imagine that 
 designing the ﬁ lter comes down to a matter of specifying where the poles and/or 
zeros will be. The only real restriction is that, for stability, the poles must lie inside 
the unit circle. 
 8.4.2  Speciﬁ cation of a Filter ’ s Response 
 In this section, we look at how to  specify the frequency response of a ﬁ lter. If we 
have a broad idea of what frequency response we want to shape our signal into, we 
must ﬁ rst translate the general speciﬁ cation into something more precise. 
 The preceding section gave an intuitive insight into ﬁ lter design — we need to 
ﬁ nd the appropriate combination of poles and zeros to match the desired frequency 
response. Before we do that, however, we need to be clear about the frequency 
response and how we specify that more precisely. In pragmatic terms, ideal, perfectly 
sharp  “ brick wall ” ﬁ lters are not able to be physically realized. This can be under-
stood from the pole – zero vector concept — as the frequency point on the unit circle 
is approaching a pole (or zero), the vector lengths will  gradually change and will 
not have the abrupt change which we would like. 
 Furthermore, since the transfer function is speciﬁ ed in terms of numerator and 
denominator polynomials, we expect the frequency response (obtained by putting 
 z
e
=
jω) would be a  “ smooth ” function. We can see this if we start with a second -
 order polynomial (a quadratic equation), then extend to a third - order (cubic) poly-
nomial, and so forth. As we increase the order, we become increasingly able to 
approximate any function. However, polynomials are smooth functions, without any 
discontinuities. If we increase the order of the polynomial, we can get a  “ sharper ” 
change, but in theory it would require an  inﬁ nite order to get a step change. 
 To put this in perspective, consider Figure  8.5 , which shows a low - pass ﬁ lter. 
Such a ﬁ lter has a  passband so that lower frequencies are passed through, and fre-
quencies above the cutoff frequency are reduced in magnitude (attenuated). The ﬁ lter 
characteristic curve shows the frequency of the input waveform (horizontal) and the 
desired ﬁ lter gain (vertical). The hypothetical response plotted is a higher - order 
polynomial, which was ﬁ tted to try to match the ideal response having a passband 
from 0 to the cutoff frequency  f c . Not only do we see the ﬁ nite tapering of the 
response as it crosses the cutoff frequency  f c but we also have a  “ rippling ” response 
in the passband and also in the stopband. This, of course, is characteristic of a 
polynomial.   
 Figure  8.5 then helps us to make the link between what we want and what we 
can actually obtain in practice. The passband shows that the gain will vary by some 
amount plus or minus the desired gain. Similarly, the  stopband shows that the gain 
may vary from the ideal (often the ideal is zero; in the ﬁ gure, it is shown as 0.1 for 
the sake of example). Finally, the  transition band links the two and indicates how 
 “ sharp ” the ﬁ lter cutoff actually is. This is an amount on either side of the cutoff 
frequency  f c and often is the most critical parameter in designing a ﬁ lter. The pass-
band ripple is often a very important parameter, depending on the application. 
Finally, note in the ﬁ gure the fact that the  worst case is what we often have to design 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
278 
CHAPTER 8 DISCRETE-TIME FILTERS
toward — the highest ripple (or  “ lobe ” ) in the stopband is the closest to the passband 
and thus is most likely to adversely affect the performance of the system in an overall 
sense. 
 8.4.3  Filter Design as Pole – Zero Placement: A Notch Filter 
 We now put these concepts together with a concrete example. Suppose we wish to 
design a digital ﬁ lter to remove a certain frequency but pass all others. This often 
occurs when we want to remove power line  “ hum ” in audio systems, for example, 
or in processing the ECG signal described earlier. The 50 - or 60 - Hz interfering signal 
is obviously unwanted, and the signal of interest (audio or biomedical potential) is 
present in nearby frequency bands, and of a very small magnitude. 
 To design such a ﬁ lter, we need to have a sampling frequency in mind. This is 
chosen depending on the physical constraints of the system. As always, the Nyquist 
frequency of  fs 2 corresponds to  π in our pole – zero and frequency diagrams. Let 
the normalized frequency that we wish to remove be  ω n  rad/s (i.e., normalized to the 
sampling frequency of  ω s or 2 π radians per sample). 
 As seen previously, the gain is the product of the zero distances divided by 
the product of the pole distances, for any point on the unit circle. The angle of the 
point deﬁ nes the frequency. We place a zero at an angle  ω n on the unit circle, with 
 FIGURE 8.5   Filter speciﬁ cation for a low - pass ﬁ lter.  f c is the cutoff frequency. Because 
the ﬁ lter is not  “ ideal, ” we must specify the tolerance in the passband, in the stopband, and 
in the transition band.  
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Frequency
Gain
Specifying a filter response
Passband
Stopband
Transition band
f c
Specification band
Ideal response
Actual response
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.4 FILTER SPECIFICATION, DESIGN, AND IMPLEMENTATION  
279
the idea being to make the zero vector distance equal to zero at that frequency. We 
must also place a complex conjugate zero at angle  − ω n so that we have a realizable 
transfer function (i.e., no complex coefﬁ cients in the difference equation). 
 We then need two poles, so as to make the system causal; that is, we must 
remove the  z 2 terms, which will result from employing only two zeros. We put the 
poles at the origin so as not to affect the frequency response. The pole – zero plot is 
then as shown in Figure  8.6 , with the corresponding frequency response. The ﬁ lter 
does indeed have zero gain at the required notch (reject) frequency, but it is clear 
that the passband response (at frequencies away from the notch) has a nonuniform 
gain, well away from the required unity gain. Additionally, it is also easily seen that 
the gain on either side of the notch is not controlled very well (the notch is not very 
 “ sharp ” ). 
 To address these issues, we place poles just near the zeros. The rationale for 
this is that since the gain is the product of the zero distances divided by the product 
of the pole distances, at any point on the unit circle away from the zero – pole com-
bination, these distances will be approximately equal, and hence cancel. 
 This new pole – zero conﬁ guration is shown in Figure  8.7 a. Of course, we must 
take care to place the pole inside the unit circle so as to ensure stability. This is 
particularly important to keep in mind if sampling and coefﬁ cient quantization 
errors, or ﬁ nite - precision arithmetic errors, mean that the actual value in practice 
may drift outside the unit circle. The resulting frequency response is evidently much 
improved, as shown in Figure  8.7 b. The problem now is that the gain in the passband 
is not quite unity, as closer inspection of the ﬁ gure will show. We need to apply 
some theory to see why this is so, and if the situation can be improved. 
 The transfer function of the ﬁ lter so far is
 
 G z
z
e
z
e
z
r e
z
r e
n
n
n
n
p
p
( ) =
−
(
)
−
(
)
−
(
)
−
(
)
−
−
1
1
j
j
j
j
ω
ω
ω
ω
,  
 (8.2) 
 FIGURE 8.6   A notch ﬁ lter with zeros only (and simple poles at the origin). While there 
is a notch - like response around the desired frequency, the overall shape of the response is 
not what we would wish for. 
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
22
z Plane
Real
Imaginary
Unstable region
0
0.7854
1.5708
2.3562
3.1416
0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Gain (linear)
Frequency (radians per sample)
(a) Pole–zero diagram
(b) Frequency response
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
280 
CHAPTER 8 DISCRETE-TIME FILTERS
where the angle of the pole – zero pair is  ω n  and the radius of the pole is  r p . Expanding 
this and using  e
e
j
j
θ
θ
θ
+
=
−
2cos  gives
 
 
G z
z
ze
ze
z
zr e
zr e
r
z
z e
e
n
n
n
n
n
p
p
p
( ) =
−
−
+
−
−
+
=
−
+
−
+
−
+
2
2
2
2
1
j
j
j
j
j
ω
ω
ω
ω
ω
−
−
(
)+
−
+
(
)+
=
−
+
−
+
j
j
j
ω
ω
ω
ω
ω
n
n
n
z
zr
e
e
r
z
z
z
zr
r
p
p
n
p
n
1
2
1
2
2
2
2
2
cos
cos
p
2 .
 
 (8.3) 
 Now, inspection of the gain plot shows that we need a constant gain,  K , so as to 
adjust the gain of the ﬁ lter to one. We need this to occur for all points on the unit 
circle, except in the vicinity of the notch itself. Since all points on the unit circle are 
described by | z |  =  1 (a radius of one, at any angle), we need to multiply by the gain 
factor  K and evaluate
 
 KG z
z
( )
.
= =
1
1  
 (8.4) 
 So this becomes
 
 K
z
z
z
zr
r
n
p
n
p
2
2
2
2
1
2
1
−
+
−
+
=
cos
cos
.
ω
ω
 
 (8.5) 
 We can solve this for  K at a given value of pole radius  r p and angle (relative notch 
frequency)  ω n . Figure  8.8 shows the resulting response, which exhibits the desired 
characteristics. 
 Finally, the parameter  r p sets the pole radius. It can be used to adjust the 
 “ sharpness ” of the notch (in communications systems, this is also called the  Q 
factor). If we make  r p larger — that is, move the poles closer to the zeros — the type 
 FIGURE 8.7   A notch ﬁ lter with poles near the zeros. The response is closer to the 
desired characteristic; however, the gain is not exactly equal to one over regions away from 
the notch as we desire. 
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
z Plane
Real
Imaginary
Unstable region
0
0.7854
1.5708
2.3562
3.1416
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Gain (linear)
Frequency (radians per sample)
(a) Pole–zero diagram
(b) Frequency response
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.4 FILTER SPECIFICATION, DESIGN, AND IMPLEMENTATION  
281
of response we obtain is shown in Figure  8.8 b. Comparing the responses in Figure 
 8.8 a,b, it is clear that the width of the notch is reduced as  r p  →  1. 
 So let us look at what we have achieved. We have determined the polynomials 
for the numerator, denominator, and the multiplicative gain. The poles and zeros may 
be derived by factorizing the polynomials. The difference equation is found by letting 
the transfer function equal the output/input ratio, and from the difference equation, 
we can determine the code required to implement the digital ﬁ lter. We can be con-
ﬁ dent that the frequencies presented to the ﬁ lter (singly, or in combination) will be 
passed through or reduced in amplitude according to our frequency response plot. 
 For the speciﬁ c case of the notch ﬁ lter described in this section, the following 
steps were undertaken:  
 FIGURE 8.8   A notch ﬁ lter with poles and zeros and with the multiplicative gain 
adjusted. The adjustment of the gain is shown in (a), and we see that the gain is close to 
unity away from the vicinity of the notch. Further adjustment of the pole position is shown 
in (b) — this yields a sharper notch if desired.  
0
0.7854
1.5708
2.3562
3.1416
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Gain (linear)
Frequency (radians per sample)
0
0.7854
1.5708
2.3562
3.1416
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Gain (linear)
Frequency (radians per sample)
(a) Gain adjusted
(b) Poles moved closer to the zeros
 Notch Filter Design Steps 
 1.  From the signal bandwidth and notch frequency  f n , determine a suitable sampling 
frequency,  f s . 
 2.  Calculate the notch angle on the unit circle  ω  n . Remember that  fs 2 ↔π. 
 3.  Place the zeros on the unit circle and the poles at an angle,  ω  n , with radius less than one. 
 4.  Determine  G ( z ) from the above. 
 5.  Plot the frequency response using  z
e
=
jω for  ω  =  0  ...  π and iterate the above steps 
as necessary. 
 6.  Let  Y z
X z
G z
( )
( ) =
( ). Solve for  Y ( z ). 
 7.  Convert  Y ( z ) into a difference equation; ﬁ nd  y ( n ) in terms of  x ( n ),  x ( n  −  1),  y ( n  −  1), 
and so on. 
 8.  Code a loop to sample the input  x ( n ), calculate  y ( n ), output  y ( n ), and buffer the samples. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
282 
CHAPTER 8 DISCRETE-TIME FILTERS
 In general, the determination of  G ( z ) is the most difﬁ cult step, and there are 
many ways to approach this. The notch ﬁ lter serves as a stepping - stone to other 
design approaches considered in subsequent sections. First, though, we need to 
consider how to broadly categorize the various ﬁ lter response types. 
 8.5  FILTER RESPONSES 
 The  “ notch ” ﬁ lter is just of many ﬁ lter types we may need to use in practice. We 
now examine some other common ﬁ lter response types. 
 The most fundamental and commonly encountered type of ﬁ lter is the  low - pass 
ﬁ lter. An ideal low - pass ﬁ lter is depicted in Figure  8.9 . Frequencies up to a certain 
limiting frequency are passed through from input to output, with some gain factor; 
frequencies above this are eliminated altogether because the gain is zero. Most  “ real -
 world ” signals tend to be composed of predominantly lower - frequency components, 
and so low - pass ﬁ lters are commonly used to reduce the perception of noise, which 
often covers the entire bandwidth (or a large proportion of it). The converse, a  high -
 pass ﬁ lter, is shown in Figure  8.10 . Again, this is an idealized representation. Only 
frequencies above the cutoff are passed through. 
 In some applications, such as telecommunications, we wish to transmit several 
frequencies on the one channel. Each is borne by a carrier frequency, and the receiver 
must  “ pick out ” only one particular channel (frequency band) of interest. Obviously, 
this means that the frequency range of the ﬁ lter must be varied as different channels 
are selected — something which is quite easy to realize using digital ﬁ lters: It is 
simply a matter of changing the ﬁ lter coefﬁ cients. This operation, in its simplest 
form, is termed  bandpass ﬁ ltering, as depicted in Figure  8.11 . 
 The converse of the bandpass ﬁ lter is the bandstop ﬁ lter, as depicted in Figure 
 8.12 . This type of ﬁ lter ﬁ nds application in rejecting narrow - band noise (e.g., such 
as power supply interference). 
 FIGURE 8.9   A low - pass ﬁ lter with an idealized response. Any input component whose 
frequency is below the cutoff frequency is passed through with a gain of unity and hence 
appears at the output; any component whose frequency is above the cutoff is multiplied by 
a gain of zero and hence is removed entirely from the output. 
0
1
0
π/4
π/2
3π/4
π
Frequency (radians per sample)
Gain
Passed
Rejected
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.5 FILTER RESPONSES 
283
 FIGURE 8.10   A high - pass ﬁ lter with an idealized response. Frequency components 
above the cutoff are passed through with a gain of unity; frequency components below this 
are removed entirely. 
0
1
0
π/4
π/2
3π/4
π
Frequency (radians per sample)
Gain
Passed
Rejected
 FIGURE 8.11   A bandpass ﬁ lter with an idealized response. Only frequencies falling 
between the upper and lower frequency cutoff points are passed through from input to 
output; all other frequency components present in the input are eliminated from the output. 
0
1
0
π/4
π/2
3π/4
π
Frequency (radians per sample)
Gain
Passed
Rejected
Rejected
 FIGURE 8.12   A bandstop ﬁ lter with an idealized response. This is the complement of 
the bandpass ﬁ lter — it removes all frequency components falling within a certain stopband. 
0
1
0
π/4
π/2
3π/4
π
Frequency (radians per sample)
Gain
Passed
Passed
Rejected
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
284 
CHAPTER 8 DISCRETE-TIME FILTERS
 With analog electronics, bandpass ﬁ lters may be implemented using a cascade 
of a high - pass ﬁ lter with cutoff  Ω h and a low - pass ﬁ lter with cutoff  Ω l , such that 
 Ω l  >  Ω h . Likewise, a bandstop ﬁ lter may be implemented using the parallel addition 
of a low - pass ﬁ lter (cutoff  Ω l ) and a high - pass ﬁ lter (cutoff  Ω h ), such that  Ω l  <  Ω h . 
 If we desire anything more complicated, such as multiple passbands (as shown 
in Fig.  8.13 ), or passbands of varying width, then the design using analog signal 
processing becomes very complicated. Digital ﬁ lters, however, have no real increase 
in complexity when the speciﬁ cations are more complicated and are in fact able to 
realize virtually any ﬁ lter response characteristic that may be required. When we 
say that they have no increase in complexity, this is meant in terms of physical 
hardware. Of course, the software complexity (the difference equation implementa-
tion and the associated memory storage for samples and coefﬁ cients) may be greater. 
But this comes at essentially very little extra cost. 
 As might be expected, the idealized ﬁ lters outlined above are not realizable in 
practice. In effect, they would require a transfer function of inﬁ nite order. However, 
another advantage of digital ﬁ lters is that once the basic  “ A/D – processing – D/A ” 
system is available, increasing the order of the ﬁ lter (and hence making the charac-
teristic closer to the ideal) is simply a matter of increasing the number of coefﬁ cients. 
Although this may require a faster processor and/or more memory, such a cost is 
usually not excessive. 
 A great many ﬁ lter design algorithms are available to choose from. Often, 
these are in the form of computer - assisted packages. The initial decision steps are 
 1.  to choose whether the ﬁ lter has poles, zeros, or both; and 
 2.  to choose the order of the ﬁ lter. 
 The former may be viewed as a  “ pole – zero placement ” problem: We must place the 
poles and/or zeros at speciﬁ c locations in order to realize a certain frequency 
response. The relationship between poles, zeros, and the frequency response of a 
system was introduced in Section  5.8 . 
 FIGURE 8.13   A ﬁ lter with more than one passband. It is effectively a hybrid of the 
fundamental types encountered previously. Using digital signal processing, such a ﬁ lter is a 
little more complicated than a low - pass ﬁ lter, in either design or implementation. 
0
1
0
π/4
π/2
3π/4
π
Frequency (radians per sample)
Gain
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.6 NONRECURSIVE FILTER DESIGN 
285
 The question of whether the above steps lead to an acceptable performance of 
the ﬁ lter usually requires an iterative solution: a lower - order ﬁ lter may be tried, 
which may or may not satisfy the design requirements. Finally, the transfer function 
leads directly to the difference equation coefﬁ cients ( a k ’ s and  b k ’ s). 
 8.6  NONRECURSIVE FILTER DESIGN 
 We have now discussed the common ﬁ lter types and examined in detail the speciﬁ c 
design case of a notch ﬁ lter. In order to design for the more general responses shown, 
we need a more generally applicable technique. One such method is the FIR 
approach. FIR ﬁ lters may be contrasted with inﬁ nite impulse response (IIR) ﬁ lters, 
which are considered in the next chapter. 
 The difference equation describing an FIR ﬁ lter is
 
 y n
b x n
k
k
k
N
( ) =
−
(
)
=
−
∑
0
1
 
 (8.6) 
with a  z transform of
 
 H z
b z
k
k
k
N
( ) =
−
=
−
∑
0
1
.  
 (8.7) 
 This is shown in block diagram form in Figure  8.14 . We need a method to determine 
the  b k ﬁ lter coefﬁ cients. 
 8.6.1  Direct Method 
 We now introduce the direct method for FIR ﬁ lter design. It is based on the theory 
of the Fourier transform but requires some additional steps to make a useful ﬁ lter 
in practice. 
 FIGURE 8.14   Block diagram representation of an FIR ﬁ lter. The signal is delayed by 
each of the  z  − 1 blocks and is multiplied by a coefﬁ cient,  b k , at each stage. Finally, the result 
of the delayed weighted products is summed to form the output  y ( n ). 
z −1
z −1
z −1
x(n)
x(n −1)
x(n −2)
x(n −3)
y(n)
b0
b1
b2
b3
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
286 
CHAPTER 8 DISCRETE-TIME FILTERS
 The time domain impulse response of a ﬁ lter corresponding to a given (desired) 
frequency response may be calculated from the inverse Fourier transform of the 
desired frequency response:
 
 h
n
H
e
d
d
d
n
( ) =
( )
−∫
1
2π
ω
ω
ω
π
π
j
.  
 (8.8) 
 The samples  h d ( n ) from the above are time domain values, as indicated by the index 
 n . These are the time domain samples that would have the frequency response  H d ( ω ). 
How does that help us design a ﬁ lter? The conceptual leap is that we use these 
numbers as weighting coefﬁ cients in a difference equation to form the ﬁ lter itself. 
 To design a ﬁ lter using Equation  8.8 , we must obviously know  H d ( ω ). However, 
note that the integration limits are  ± π . So, the desired passband in the 0 to  π range 
must be mirrored into the negative frequency range of 0 to  − π . This is illustrated in 
Figure  8.15 for a low - pass ﬁ lter and in Figure  8.16 for a bandpass ﬁ lter. 
 FIGURE 8.15   Mirroring a ﬁ lter ’ s desired response for a low - pass ﬁ lter. The desired 
response from zero to the low - pass cutoff is mirrored in the negative frequency range. 
0
1
−π
−π/2
0
+π/2
+π
Frequency (radians per sample)
Gain
Mirror
Desired
 FIGURE 8.16   Mirroring a ﬁ lter ’ s desired response for a bandpass ﬁ lter. 
0
1
−π
−π/2
0
+π/2
+π
Frequency (radians per sample)
Gain
Mirror
Desired
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.6 NONRECURSIVE FILTER DESIGN 
287
 The reason for this may be understood by considering the desired time response 
coefﬁ cients  h d ( n ). These will be used as ﬁ lter coefﬁ cients and must be real numbers 
by deﬁ nition; that is, we need terms of the form  b k x ( n  −  k ), where  x ( n  −  k ) are the 
samples and  b k are the coefﬁ cients. Since for any  α and its complex conjugate  α * , 
the product ( z  −  α )( z  −  α * ) will have only real coefﬁ cients, so any complex number 
products in  G ( z ) must occur in pairs in order to cancel out and give real 
coefﬁ cients. 
 The impulse response  h d ( n ) thus computed will be inﬁ nite in extent. In a practi-
cal ﬁ lter, the order must be limited. This is obtained by truncating the impulse 
response. Assuming  N is odd, the restriction of the inﬁ nite - length impulse response 
to a ﬁ nite length may be accomplished by calculating  h d ( n ) over the range 
 −
−
≤
≤+
−
N
n
N
1
2
1
2
The MATLAB script shown next implements the above design equation. First, the 
desired frequency response  H d ( ω ) in the band 0 to  fs 2 (corresponding to 0 to  π 
radians) is mirrored over the range  − π  to 0. The integration is then performed as a 
simple rectangular numerical integration. 
 We then take the real part of the resulting  h d ( n ) values. This is because the 
results of the integration may have small imaginary values due to rounding. The 
computed impulse response  h d ( n ) exists over positive and negative time, which cannot 
be realized in a true ﬁ lter because it implies future sample references (or, equivalently, 
positive powers of  z in the transfer function). The solution to this problem is simply 
to delay the impulse response by half the length of the ﬁ lter. For odd  N , this is 
 N −
(
)1 2 samples. We then use these values as the coefﬁ cients of the FIR ﬁ lter. 
 dw  =  pi /400; 
 w  =  − pi :dw: pi ; 
 N  = 21; 
 % order; must be odd 
 H  =  zeros ( size (w)); 
 wLow  =  pi /4; 
 wHigh  =  pi /2; 
 % 0 to pi 
 i  =  ﬁ nd ((w   >=  wLow)  &  (w   <=  wHigh)); 
 H(i)  = ones( size (i)); 
 %  − pi to 0 is the mirror image 
 i  =  ﬁ nd ((w   >=  − wHigh)  & (w   <=   − wLow)); 
 H(i)  = ones( size (i)); 
 nlow  =  −  (N   −  1)/2; 
 nhigh  = (N   −  1)/2; 
 K  = 1/(2  *  pi ); 
 for n  = nlow : nhigh 
      h(n  −  nlow  + 1)  = K  *  sum (H.  *  exp (j  * w  * n)  * dw); 
 end 
 h  =  real (h);   % compensate for rounding errors 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
288 
CHAPTER 8 DISCRETE-TIME FILTERS
 Using the above truncation for  N  =    21 and cutoff range  ω
π
l =
4 to  ω
π
u =
2, 
the impulse response shown in Figure  8.17  results. Clearly, the impulse response is 
symmetrical. 
 The frequency response, obtained as in Section  5.7 by setting  z
e
=
jω, is shown 
in Figure  8.18 . The frequency response exhibits a relatively poor approximation to 
that desired. There are two underlying issues: ﬁ rst, the sharpness of the transition 
bands and the attenuation of the stopbands; and second, the oscillatory nature of the 
passband and stopband frequency response characteristics. We will deal with each 
of these issues in turn. 
 FIGURE 8.17     The impulse response of the initial FIR ﬁ lter design.  N  =  21,  ω
π
l =
4, 
 ω
π
u =
2.  
1
21
−0.4
−0.2
0
0.2
0.4
Impulse response
 FIGURE 8.18   The frequency response of the initial FIR ﬁ lter design, for  N  =  21, 
 ω
π
l =
4, and  ω
π
u =
2. Clearly, the response is adequate only as a ﬁ rst approximation to 
the desired passband and must be improved somewhat. 
Bandpass filter gain
ω
0.5
1
0
0
π/4
π/2
3π/4
π
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.6 NONRECURSIVE FILTER DESIGN 
289
 Recall the earlier discussion on ﬁ lter design as a pole – zero placement problem. 
Since this is an FIR ﬁ lter, it has only zeros (and simple poles due to the causality 
requirement). The locations of the zeros are shown in Figure  8.19 . Note that there 
are 20 zeros for a ﬁ lter with 21 coefﬁ cients (terms) in the ﬁ lter, and we need a further 
 z  − 20 term to make the ﬁ lter causal. 
 The problem with the response as calculated is that it is a poor approximation 
to the desired ideal response. One obvious step is to increase the order of the ﬁ lter. 
The second nonobvious step is to  “ taper ” the impulse response coefﬁ cients. Using 
the truncation method outlined above, we are effectively multiplying the ﬁ lter coef-
ﬁ cients by a rectangular window. Using a Hamming window of the form shown in 
Figure  8.20 , we multiply each impulse response coefﬁ cient by the corresponding 
window coefﬁ cient. The Hamming window equation is 1 
 
 ω
π
k
k
N
=
+
⎛
⎝⎜
⎞
⎠⎟
0 54
0 46
2
.
.
cos
. 
 (8.9) 
 This may be implemented in MATLAB using the following code to augment that 
shown previously: 
 FIGURE 8.19   The zeros of the transfer function for an FIR ﬁ lter with  N  =  21,  ω
π
l =
4, 
and  ω
π
u =
2. 
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
20
FIR filter zeros
Real
Imaginary
Unstable region
  1  If  n is taken as  −
−
(
)
N
1 2 to  +
−
(
)
N
1 2, the Hamming window is as above. If  n is 0 to  N  −  1, the 
Hamming equation uses  − 0.46. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
290 
CHAPTER 8 DISCRETE-TIME FILTERS
 FIGURE 8.21   Gain responses for the FIR direct method with differing orders.  N  =  31 
and  N  =  61,  ω
π
l =
4,  ω
π
u =
2. 
Bandpass order comparison
N = 31
N = 61
ω
0.5
1.0
0
0
π/4
π/2
3π/4
π
 FIGURE 8.20   A 21 - point Hamming window. The computed impulse response 
coefﬁ cients are multiplied by the corresponding point in the window to yield the ﬁ lter 
coefﬁ cients. This has the effect of smoothing the frequency response. 
1
21
0
0.2
0.4
0.6
0.8
1.0
Hamming window
Term in impulse response
Weighting
 k  = nlow : nhigh; 
 w  = 0.54  + 0.46  *  cos (2  *  pi  * k/N); 
 hw  = h.  * w;  
 Figures  8.21 and  8.22 show the effect of increasing the order and of window-
ing the coefﬁ cients, respectively. The increased order does indeed sharpen the 
response edges, at the expense of additional ringing in the passband and the stop-
band. If a window is used, it will tend to reduce this ringing; however, it also serves 
to dull the sharpness of the response edges. Therefore, the two techniques are nor-
mally used in tandem. One of the window types as discussed in Section  7.11 may 
be used. 
 One characteristic of FIR ﬁ lters is that they provide a nonﬂ at response in the 
passband and stopband, and the window clearly helps to ﬂ atten the response. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.6 NONRECURSIVE FILTER DESIGN 
291
However, the trade - off for obtaining a smoother passband response is that the transi-
tion bands between the stopband and the passband become wider. 
 Increasing the order certainly gives a sharper response; however, this comes 
at the expense of increased ﬁ lter order. Figure  8.23 shows the impulse response (and, 
with a window as desired, the ﬁ lter coefﬁ cients) for a ﬁ lter of order 201. Not only 
are greater memory and computational resources required but the delay will also 
increase linearly with the ﬁ lter order. It should be noted, however, that there are 
 FIGURE 8.22   Gain responses for the FIR direct method with and without a Hamming 
window.  N  =  201,  ω
π
l =
4,  ω
π
u =
2. 
Bandpass filter windowing
No window
Hamming
ω
0.5
1.0
0
0
π/4
π/2
3π/4
π
 FIGURE 8.23   The FIR impulse response for  N  =  201,  ω
π
l =
4, and  ω
π
u =
2. The 
delay inherent in such a long ﬁ lter is self - evident. The lower panel shows the center portion 
of the impulse response — clearly, the ﬁ lter is symmetric about the middle of the ﬁ lter. 
1
21
41
61
81
101
121
141
161
181
201
−0.4
−0.2
0
0.2
0.4
Full impulse response
Impulse response
−0.4
−0.2
0
0.2
0.4
Central terms
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
292 
CHAPTER 8 DISCRETE-TIME FILTERS
various  “ fast ” algorithms available to speed up the FIR ﬁ lter computation. These 
usually involve the fast Fourier transform (FFT) algorithm, and are very important 
in real - time implementations. These are explained in Section  8.9 . 
 To summarize, for the FIR design method discussed, the following steps were 
undertaken: 
 FIR Filter Design Steps 
 1.  From the characteristics of the signal to be ﬁ ltered, determine a sampling frequency, 
 f s . 
 2.  Determine the shape of ﬁ lter characteristic required (low - pass, high pass, or custom). 
 3.  Choose an initial ﬁ lter order,  N . 
 4.  Select a window function such as the Hamming window (Equation  8.9 ) and 
determine coefﬁ cients  w k . 
 5.  Sketch the ﬁ lter response shape  H d ( ω ) from 0 to  π and the mirrored response from 
 − π to 0. 
 6.  Apply the FIR design equation (Equation  8.8 ) to determine  h d ( n ). 
 7.  Delay the  h d ( n ) by half the ﬁ lter order for causality. 
 8.  Multiply the  w k by the delayed  h d ( n ) to obtain ﬁ lter coefﬁ cients  b k . 
 9.  Plot the frequency response using  z
e
=
jω for  ω  =  0  ...  π . 
 10.  Iterate the above steps, increasing  N as necessary. 
 11.  Let  Y z
X z
G z
( )
( ) =
( ). Solve for  Y ( z ). 
 12.  Convert  Y ( z ) into a difference equation; ﬁ nd  y ( n ) in terms of  x ( n ),  x ( n  −  1), 
 y ( n  −  1), and so on. 
 13.  Code a loop to sample the input  x ( n ), calculate  y ( n ), output  y ( n ), and buffer the 
samples. 
 8.6.2  Frequency Sampling Method 
 This method is quite similar in approach to the direct window method. Instead of 
the desired frequency response being completely speciﬁ ed, it is speciﬁ ed only as 
samples of the frequency response at regular intervals. Thus, the inverse discrete 
Fourier transform (DFT) yields the impulse response and, as before, these values 
are used as the coefﬁ cients of an FIR ﬁ lter. For  n
N
= −
−
(
)1 2 to  n
N
= +
−
(
)1 2 
and  N assumed odd, the design equation is
 
 h n
N
H k e
n k
N
k
N
N
( ) =
( )
=−
−
+
−
∑
1
2
1
2
1
2
j
π
.  
 (8.10) 
 Using this method, the desired frequency response is also sampled (the  H ( k )). The 
disadvantage is that we need to resample the frequency response, so care would have 
to be taken that no information is lost in choosing the number of samples to use. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.7 IDEAL RECONSTRUCTION FILTER 
293
 8.7  IDEAL RECONSTRUCTION FILTER 
 It was noted in Chapter  3 that an ideal reconstruction ﬁ lter — one which makes the 
reconstructed analog signal equal to the original — has an impulse response found 
by taking the inverse Fourier transform of the signal:
 
 x t
X
e
d
t
( ) =
( )
−∞
∞∫
1
2π
Ω
Ω
Ω
j
.  
 (8.11) 
 The scaling factor  1 2π comes about because of the use of radian frequency (remem-
ber that  Ω  =  2 π f and  f = 1 τ). Since we want a band - limited reconstruction without 
aliasing, this becomes the integration as shown in Figure  8.24 . 
 
 h t
X
e
d
s
t
s
s
( ) =
( )
−
+∫
1
2
2
Ω
Ω
Ω
Ω
Ω
Ω
j
.  
 (8.12) 
 This simpliﬁ es to
 
 
h t
t
T
t
T
t
T
( ) =
=
sin
sinc
π
π
π .
 
 (8.13) 
 Thus, the impulse response at each sample instant  t  =  nT must be
 
 h t
nT
t
nT
T
−
(
) =
−
(
)
sinc π
. 
 (8.14) 
 FIGURE 8.24   The frequency response of an ideal low - pass reconstruction ﬁ lter. The gain 
is unity over the range up to half the sampling frequency. For the purpose of the impulse 
response derivation, we must mirror the response in the negative frequency range. 
0
T
−Ωs/2
0
+Ω s/2
Frequency (radians per second)
Gain
Mirror
Desired
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
294 
CHAPTER 8 DISCRETE-TIME FILTERS
 This  “ sinc ” function for a sampling period of  T  =  0.1 is shown in Figure  8.25 . Note 
that in the reconstruction function  h ( t ), the zero crossings are at  nT . This may easily 
be seen from the above equation for  h ( t  −  nT ) — the sine function is zero when  πt T 
is 0,  π , 2 π ,  …  , which is whenever  t  =  nT for any integer  n . What this means is that 
the reconstruction is done by a sample - weighted sinc function where, at the sampling 
instants, the sinc impulses from the other samples do not interfere with the current 
sample. In between the sampling instants, the sinc function provides the optimal 
interpolation.    
 8.8  FILTERS WITH LINEAR PHASE 
 In general, a signal is composed of several frequency components, and a ﬁ lter has 
both  gain and  phase responses. Often, the gain response is what we are interested 
in, but sometimes the phase response is also crucial — either because we want a 
certain phase response for an application or because the incidental phase response 
of a ﬁ lter which has the desired magnitude response may produce undesired side 
effects. 
 The types of FIR ﬁ lters discussed have a linear phase — the phase angle 
increases linearly with frequency. IIR ﬁ lters do not in general have this characteris-
tic. This has signiﬁ cance in some applications such as ﬁ lters for digital communica-
tions, as demonstrated below. 
 Figures  8.26 and  8.27 show the gain and phase responses for IIR and FIR 
ﬁ lters, respectively. The design of IIR ﬁ lters will be considered in detail in Chapter 
 9 . For now, it is sufﬁ cient to understand that these are (arbitrarily) designed for a 
 FIGURE 8.25   The  “ sinc ” function sin  sin π
π
t T
t T
(
) (
)
(
) for the ideal reconstruction of 
a sample at  t  =  0, with a sampling period of  T  =  0.1. Note how the function is exactly zero 
at the sampling instants and that the magnitude of the peak is one. 
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
−0.5
0
0.5
1.0
Amplitude
Time relative to sample instant at t = 0
sin( πt
T )
( πt
T )
function with T  = 0.1
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 FIGURE 8.26   Gain and phase responses for an eighth - order recursive Chebyshev ﬁ lter 
with a bandpass response. The gain is shown on a linear scale. Below that is the phase as 
calculated and the  “ unwrapped ” phase. The design of recursive Chebyshev ﬁ lters will be 
discussed in Chapter  9 . 
0
1,000
2,000
3,000
4,000
5,000
6,000
7,000
8,000
9,000
10,000 11,000
1,000
2,000
3,000
4,000
5,000
6,000
7,000
8,000
9,000
10,000 11,000
1,000
2,000
3,000
4,000
5,000
6,000
7,000
8,000
9,000
10,000 11,000
0
0.2
0.4
0.6
0.8
1.0
Frequency (Hz)
Gain
0
−180
−90
0
90
180
Frequency (Hz)
Phase (degrees)
Chebyshev analog filter, eighth order, passband 6−8kHz
0
−2,000
−1,500
−1,000
−500
0
Frequency (Hz)
Phase (degrees)
 FIGURE 8.27   Gain and phase responses for a 128th order FIR nonrecursive ﬁ lter with a 
bandpass response. The gain is shown on a linear scale. Below that is the phase as 
calculated and the unwrapped phase.  
0
0.2
0.4
0.6
0.8
1.0
0
π
Gain
−180
−90
0
90
180
Frequency (radians per sample) (fs = 22 kHz)
Frequency (radians per sample) (fs = 22 kHz)
Frequency (radians per sample) (fs = 22 kHz)
Phase (degrees)
0
π
FIR digital filter, 128th order, fs = 22 kHz, passband 6−8 kHz
−10000
−5000
0
0
π
Phase (degrees)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
296 
CHAPTER 8 DISCRETE-TIME FILTERS
bandpass response for this example. The analog design clearly has a nonlinear phase, 
and this is translated into the discrete - time ﬁ lter through transformation. 
 Note that these ﬁ gures show the direct phase as calculated from the angle   H 
in each case. The phase will normally  “ wrap ” when crossing a 180 ° boundary. For 
example, a phase for successive frequencies of  − 178 ° ,  − 179 ° ,  − 180 ° , and  − 181 ° 
would actually be calculated as  − 178 ° ,  − 179 ° ,  − 180 ° , and  + 179 ° . The phase is a 
smooth function, and the discontinuity is an artifact of the fact that we are consider-
ing a circular function. The ﬁ gures show the unwrapped phase for easier comparison, 
but this is not normally done when showing ﬁ lter responses since the phase wrapping 
is understood. 
 The  phase delay of ﬁ lter is the amount of time delay of  each frequency com-
ponent. Filters with a nonlinear phase cause phase distortion because frequency 
components in the signal will each be delayed by an amount which is  not propor-
tional to frequency, thereby altering their time relationship. Mathematically, suppose 
a sinusoid of frequency  Ω  and phase  φ 1 goes into a system at time  t 1 . Sometime 
later, at time  t 2 , it emerges with a differing phase,  φ 2 . For simplicity, assume the 
input and output magnitudes are the same. Furthermore, the frequency is assumed 
to be unchanged as it passes through the linear system. So the input is described by 
sin( Ω t 1  +  φ 1 ) and the output is sin( Ω t 2  +  φ 2 ). For these to be equal,
 
 
Ω
Ω
Ω
ΩΔ
Δ
Δ
Δ
Ω
t
t
t
t
t
t
1
1
2
2
2
1
1
2
+
=
+
−
=
−
= −
= −
ϕ
ϕ
ϕ
ϕ
ϕ
ϕ
(
)
.
 
 (8.15) 
 What this indicates is that the relative time change in the waveform from input 
to output is proportional to the phase difference. Moreover, for the same relative 
time change, if the phase difference  Δ φ is (say) doubled, the frequency has to 
be doubled. The conclusion is that the phase must be proportional to the 
frequency. 
 To illustrate this, consider the Fourier series analysis in Figure  8.28 , where we 
examine the effect of phase more closely. If a constant phase is added to all the 
Fourier series components, the relative pulse shape is distorted. This is because the 
relative time relationship is changed, as argued above. However, if the phase is 
altered in such a way that it is directly proportional to the frequency of each com-
ponent, the pulse shape is preserved. It is important to note that in this example, the 
 magnitudes of each sinusoidal component are unchanged, whatever the phase. The 
phase simply determines the position in time relative to the frequency. The term 
 group delay is used to deﬁ ne the average time delay of the  composite signal at each 
frequency. 
 The important point to note is that IIR ﬁ lters invariably have a nonlinear phase 
response. In fact, analog ﬁ lters can only approximate a linear phase response. On 
the other hand, the FIR ﬁ lters discussed here always have a linear phase response, 
and thus composite waveforms do not suffer from this type of time domain 
distortion. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.8 FILTERS WITH LINEAR PHASE 
297
 FIGURE 8.28   The effect of phase on a pulse waveform. All three have the same Fourier 
series component magnitudes; hence, each magnitude frequency spectrum is identical. 
However, the phase relationship between components is altered — in sequence, they are 
correct phase, constant phase, and phase proportional to frequency. Clearly, the pulse shape 
is altered when the phase is  not proportional to frequency. 
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Time
Desired
Approximated
(a) The pulse waveform and its truncated Fourier series representation
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
−2
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Time
Desired
Approximated
(b) The truncated Fourier series representation with a constant phase added to each component
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
298 
CHAPTER 8 DISCRETE-TIME FILTERS
 8.9  FAST ALGORITHMS FOR FILTERING, 
CONVOLUTION, AND CORRELATION 
 So - called fast algorithms are common in signal processing, particularly when real -
 time constraints are present and/or very large data set computations are required. 
Simply throwing faster hardware at the problem is not always necessary (nor wise). 
Fast algorithms exist, which can signiﬁ cantly speed up the computations required 
for a ﬁ lter. The class of algorithms based on the FFT can be traced back over some 
time (see Stockham [1966] and Helms [1967]), but their relevance and importance 
is as great as ever. 
 The design of FIR ﬁ lters has been examined in previous sections, and despite 
all their advantages, a signiﬁ cant problem is that the ﬁ lter order tends to be fairly 
large to achieve reasonable performance (i.e., the difference equation requires a large 
number of terms). FIR ﬁ lter operation consists of multiplication and addition opera-
tions, one per each term calculated, and if there are a large number of terms, then 
the computational cost scales up accordingly. 
 In this section, we examine some ways in which we can speed up the process 
of FIR ﬁ ltering (and by extension, convolution, and correlation). This permits some 
ﬁ ltering operations to be run in real time when it would otherwise not be possible. 
Alternatively, it permits less expensive hardware to be utilized for the same task. 
Even non - real - time operations may beneﬁ t — the end - user performance becomes 
more acceptable, and large data set operations can be completed in seconds rather 
than in minutes (or minutes rather than hours). 
 In the process of developing the fast algorithms, it will become apparent that 
they are also useful for correlation operations. Recall that correlation (Section  6.3.1 ) 
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Time
Desired
Approximated
(c) The series with a phase proportional to frequency added to each component
FIGURE 8.28 (continued)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.9 FAST ALGORITHMS FOR FILTERING, CONVOLUTION, AND CORRELATION 
299
has a great many practical applications such as pattern recognition and system iden-
tiﬁ cation, so developing fast algorithms will beneﬁ t these too. 
 8.9.1  Convolution Revisited 
 Convolution was discussed in Section  5.9 , and Equation  8.16 shows the basic form 
of the convolution operation mathematically:
 
 y n
h x n
k
k
k
P
( )
.
=
−
(
)
=
−
∑
0
1
 
 (8.16) 
 So let us visualize this. Figure  8.29 shows the operation of convolution for short 
sequences denoted by vectors x and  h . The operation of Equation  8.16 may be 
 FIGURE 8.29   Illustrating the steps in involved in convolution. In this case, we have 
speciﬁ cally labeled it  “ linear ” convolution to distinguish from circular convolution (as 
explained in the text). One of the sequences — in this case, the vector  h — is time reversed 
and can be imagined as sliding past the ﬁ xed vector x. As the sequence is moved, the sum 
of the product of the overlapping terms is calculated. The total number of output terms is 
seen to be the sum of the lengths of each input sequence, minus one. 
Linear convolution
x(n)
=
N = 4 data points
x(0)
x(1)
x(2)
x(3)
hm
=
h0
h1
h2
Filter impulse response M = 3
Step 1
·
·
x(0)
x(1)
x(2)
x(3)
·
·
h2
h1
h0
·
·
·
·
·
∑0 = h0x(0)
Step 2
·
·
x(0)
x(1)
x(2)
x(3)
·
·
·
h2
h1
h0
·
·
·
·
∑1 = h0x(0) + h1x(0)
Step 3
·
·
x(0)
x(1)
x(2)
x(3)
·
·
·
·
h2
h1
h0
·
·
·
∑2 = h0x(2) + h1x(1) + h2x(0)
Step 4
·
·
x(0)
x(1)
x(2)
x(3)
·
·
·
·
·
h2
h1
h0
·
·
∑3 = h0x(3) + h1x(2) + h2x(1)
Step 5
·
·
x(0)
x(1)
x(2)
x(3)
·
·
·
·
·
·
h2
h1
h0
·
∑4 = h1x(3) + h2x(2)
Step 6
·
·
x(0)
x(1)
x(2)
x(3)
·
·
·
·
·
·
·
h2
h1
h0
∑5 = h2x(3)
Total outputs: 4 + 3 − 1 = 6(N + M − 1)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
300 
CHAPTER 8 DISCRETE-TIME FILTERS
visualized by time reversing the second sequence, sliding it all the way to the left, 
and then calculating the sum of product of terms which overlap. The sequence is 
then stepped one to the right, and the process is repeated. The time reversal may be 
understood from the equation. Since the output term is at index  n , the index into the 
input vector x is ( n  −  k ). Thus, as  k in incremented in evaluating the summation, the 
index into x goes further back in time  x ( n ),  x ( n  −  1),  x ( n  −  2),  ...  x ( n  −  P  +  1). For 
example, in Step 2 of Figure  8.29 , we multiply  h 0 and  h 1 by  x (1) and  x (0), respec-
tively, and in Step 3, we have [ x (2)  x (1)  x (0)] multiplied by [ h 0  h 1  h 2 ]. Imagining 
this as a time reversal helps in visualizing the process. 
 Figure  8.30 shows a concrete example of the process of convolution using 
simple numerical sequences. The result can be easily veriﬁ ed via manual calculation. 
Note that the inputs to the convolution process (the x and y vectors) may be inter-
changed without changing the end result. Finally, note that the number of output 
terms is the sum of the lengths of the input sequences, less one. This can be seen to 
be true since nonzero output values require an overlap of the input vectors of at least 
one (see Step 6 in Fig.  8.29 ). 
 8.9.2  FIR Filtering as Convolution 
 Now we turn to the ﬁ ltering operation since that is the primary motivation for our 
investigation. The basic operation of an FIR ﬁ lter is given in Equation  8.17 :
 
 
y n
b x n
b x n
b x n
N
b x n
k
N
k
k
N
( )
( )
(
)
(
)
(
).
=
+
−
+
+
−
=
−
=∑
0
1
0
1

 
 (8.17) 
 For each output point  y ( n ), we multiply the delayed inputs  x ( n  −  k ) by the weighting 
coefﬁ cients,  b k , as determined by the design procedure. Note that the highest order 
of the ﬁ lter delay is ( n  −  N ), and thus we may say that the ﬁ lter order is  N (i.e., a 
 z  −  N term will result in the transfer function). However, there are ( N  +  1) coefﬁ cients 
 FIGURE 8.30   Illustrating linear convolution using MATLAB. The  h sequence is time 
reversed and slides past the ﬁ xed sequence x, with a vector dot product calculated at each 
step. If we reverse the order of the inputs (i.e., hold the second sequence ﬁ xed and reverse/
slide the ﬁ rst), the same result ought to be produced. This is shown in the second 
convolution operation above — the outputs are clearly identical. 
x = [1 2 3
4 ] ;
h = [5 6
7 ] ;
Same result
conv (x , h)
ans =
5
16
34
52
45
28
conv (h , x)
ans =
5
16
34
52
45
28
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.9 FAST ALGORITHMS FOR FILTERING, CONVOLUTION, AND CORRELATION 
301
 FIGURE 8.31   Convolution is equivalent to nonrecursive FIR ﬁ ltering, as this example 
illustrates. Here we use the  conv () operator, which produces nearly identical results to 
the  ﬁ lter () operation. The initial outputs are the same, but the ﬁ ltering operation yields 
a shorter result — this is because the  “ tail ” where there is no full overlap for the last terms 
is not calculated. In effect, this is the overlap produced by convolution. The number of 
terms not calculated is the length of the second sequence minus one. 
x = [1 2 3
4 ] ;
h = [5 6
7 ] ;
Same result up to length of x
conv (x , h)
ans =
5
16
34
52
45
28
f i l t e r (h ,1 , x)
ans =
5
16
34
52
since the coefﬁ cients start at zero. This is a minor but important point to keep in 
mind for when we subsequently examine the convolution operator. 
 Now if we compare Equations  8.17 and  8.16 , we see that they are almost 
identical. We have an output  y ( n ) at time instant  n , which is formed as the weighted 
sum of current and past inputs as denoted by the  x ( n  −  k ) term. The weighting coef-
ﬁ cients in the ﬁ lter are denoted as  b k ’ s, and in the convolution as  h k ’ s. However, the 
same multiply – add operations and indexing order appears in both. Thus, we con-
clude that FIR ﬁ ltering is in fact identical to convolution of the data sequence  x with 
the impulse response  h , and furthermore that the coefﬁ cients actually form the 
impulse response itself. This is illustrated in Fig.  8.31 . 
 8.9.3  Relationship between Convolution and Correlation 
 Correlation was discussed in Chapter  6 . Recall that the operation of correlation may 
be described mathematically as
 
 R
k
N
x n y n
k
k
xy
n
N
( )
( ) (
)
:
,
=
−
=
−
∑
1
0
1
offset  
 (8.18) 
where, again,  k is the positive or negative offset (or lag). 
 Figure  8.32 illustrates these steps graphically, for short data sequences x and 
y. In the case of convolution, we imagined these as a data sequence and an impulse 
response, whereas now we consider them to be two data sequences (which may in 
fact be the same one, in the case of autocorrelation). Note that in the diagram, the 
second sequence is  not time reversed. This may be understood by reference to the 
correlation equation, and in particular the  y ( n  −  k ) term. The offset or lag  k is ﬁ xed 
for each output, and the summation is over index  n . Thus, compared to Equation 
 8.16 , we can see that the fundamental difference is that in the case of correlation, 
the second sequence is  not reversed, whereas with convolution, it  is reversed. Apart 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
302 
CHAPTER 8 DISCRETE-TIME FILTERS
from that, the multiply – add – shift steps are the same, as will be seen by comparing 
Figures  8.29 and  8.32 . 
 The similarities and differences between convolution and correlation are illus-
trated in Figure  8.33 . Correlation can be effected by ﬁ rst time reversing one of the 
sequences, followed by convolution (which itself reverses one of the sequences). 
Manual calculation of the results for Figure  8.33 will verify the correct results in 
each case (convolution and correlation). 
 8.9.4  Linear and Circular Convolution 
 Note that convolution produces a result whose length depends on the length of both 
the input vectors. Recall also that the DFT and its faster version, the FFT, produce 
a result which has the same length as the input block. In order to understand the fast 
convolution algorithms, it is necessary to investigate a variant of the convolution 
operation discussed so far. 
 FIGURE 8.32   The steps required to produce the correlation of sequences x and y. The 
second sequence slides past the ﬁ rst, with a vector dot product calculated at each step. In 
effect, this is the same as the convolution operation, except that the second sequence is  not 
reversed.  
Correlation
x(n)
=
N =4 x data points
x(0)
x(1)
x(2)
x(3)
y(n)
=
N =4 y data points
y(0)
y(1)
y(2)
y(3)
Step 1
·
·
·
x(0) x(1) x(2) x(3)
·
·
·
y(0) y(1) y(2) y(3)
·
·
·
·
·
·
Σ0 = x(0)y(3)
Step 2
·
·
·
x(0) x(1) x(2) x(3)
·
·
·
·
y(0) y(1) y(2) y(3)
·
·
·
·
·
Σ1 = x(0)y(2) + x(1)y(3)
Step 3
·
·
·
x(0) x(1) x(2) x(3)
·
·
·
·
·
y(0) y(1) y(2) y(3)
·
·
·
·
Σ2 = x(0)y(1) + x(1)y(2) + x(2)y(3)
Step 4
·
·
·
x(0) x(1) x(2) x(3)
·
·
·
·
·
·
y(0) y(1) y(2) y(3)
·
·
·
Σ3 = x(0)y(0) + x(1)y(1) + x(2)y(2) + x(3)y(3)
Step 5
·
·
·
x(0) x(1) x(2) x(3)
·
·
·
·
·
·
·
y(0) y(1) y(2) y(3)
·
·
Σ4 = x(1)y(0) + x(2)y(1) + x(3)y(2)
Step 6
·
·
·
x(0) x(1) x(2) x(3)
·
·
·
·
·
·
·
·
y(0) y(1) y(2) y(3)
·
Σ5 = x(2)y(0) + x(3)y(1)
Step 7
·
·
·
x(0) x(1) x(2) x(3)
·
·
·
·
·
·
·
·
·
y(0) y(1) y(2) y(3)
Σ6 = x(3)y(0)
Total outputs: 2 × 4 − 1 = 7 (2N − 1)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.9 FAST ALGORITHMS FOR FILTERING, CONVOLUTION, AND CORRELATION 
303
 The convolution equation and shorthand notation is
 
 y n
h x n
k
k
k
P
( )
(
)
=
−
=
−
∑
0
1
 
 (8.19) 
 
 y
h x
=
∗.  
 (8.20) 
 We have used the symbol  * to denote convolution. 2 The indexing in this equation 
traverses linearly from 0 to ( P  −  1). What if we kept the indexing to the length of 
the input blocks so as to produce an output block of the same length as the input 
block? Speciﬁ cally, what if we used modulo addressing and  “ wrapped ” the indexes 
as indicated in Figure  8.34 ? In this ﬁ gure, incrementing the address beyond the limit 
of the array wraps the address index back to zero. We will use the notation  x k
( )  to 
indicate that the array is circularly addressed by index  k . 
 FIGURE 8.33   The differences between convolution and correlation are illustrated in this 
numerical example. Convolution ﬂ ips one of the sequences, whereas correlation does not. 
In this example, we use the convolution operator to calculate correlation by ﬂ ipping the 
second sequence before it is passed to the calculation, since the convolution operator will 
reverse it back.  
x = [1 2 3
4 ] ;
h = [5 6
7 ] ;
Not the same
Convolution reverses h but correlation does not
conv (x , h)
ans =
5
16
34
52
45
28
conv (x ,
f l i p l r (h ))
ans =
7
20
38
56
39
20
 FIGURE 8.34   The operation of circular addressing for a buffer of  N samples is shown. 
When incrementing the index pointer, the index travels to the right until it reaches the end 
of the array, at which time it reverts back to zero. Similarly, decrementing the index pointer 
to zero and below wraps the pointer back to sample  N  −  1. This is termed modulo 
addressing, where the modulus is  N . 
Wrap
n < 0
x(0)
x(1)
x(2)
(· · ·)
x(N −3) x(N −2) x(N −1)
Wrap
≥
n
N
  2  Not to be confused with complex conjugation, where the  * is used as a superscript ( r * ), or the optimal 
value of a parameter  h , which is also denoted  h * . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
304 
CHAPTER 8 DISCRETE-TIME FILTERS
 What if we now employ circular addressing in calculating convolution? Our 
operating equation appears essentially the same, but we need to take care in the 
implementation to always use circular addressing:
 
 y n
h x n
k
k
k
P
( )
(
)
=
−
=
−
∑

0
1
 
 (8.21) 
 
 y n
h n
x n
( )
( )
( )
=
∗
 
 (8.22) 
 
 y n
h n
x n
( )
( )
( ).
=
⊛
 
 (8.23) 
 Unsurprisingly, this is called  circular convolution , and we will call the original 
convolution  linear convolution to distinguish it where necessary. The  * denotes 
regular or linear convolution; the  x n
( ) denotes circular addressing; and  « denotes 
circular convolution. 
 Figure  8.35 shows the calculation that occurs with circular convolution, in a 
similar way to how we illustrated linear convolution previously. The important point 
to note is that the number of output points is equal to the length of the longest input 
sequence (in this case, x). The indexing of the x samples is performed modulo the 
length of the array. So, when linear convolution would call for indexing of sample 
 x (4) (shown in Step 1 of the ﬁ gure), the index wraps back to 4  −  N  =  4  −  4  =  0. 
 Note, however, that the results of linear convolution and circular convolution 
are not identical. This may be seen by comparing Figure  8.29 and Figure  8.35 . Upon 
 FIGURE 8.35   Circular convolution when no zeros are padded except to the shorter 
sequence (if need be), so as to make the two sequences equal in length. 
Circular convolution—no padding except for length equalization
x(n)
=
N = 4 data points
x(0)
x(1)
x(2)
x(3)
hm
=
h0
h1
h2
Filter impulse response M = 3
Step 1
x(0)
x(1)
x(2)
x(3)
h0
·
h2
h1
Σ
Σ
Σ
Σ
0 = h0x(0) + h1x(3) + h2x(2)
Step 2
x(0)
x(1)
x(2)
x(3)
h1
h0
·
h2
1 = h0x(1) + h1x(0) + h2x(3)
Step 3
x(0)
x(1)
x(2)
x(3)
h2
h1
h0
·
2 = h0x(2) + h1x(1) + h2x(0)
Step 4
x(0)
x(1)
x(2)
x(3)
·
h2
h1
h0
3 = h0x(3) + h1x(2) + h2x(1)
Total outputs: 4 (N)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.9 FAST ALGORITHMS FOR FILTERING, CONVOLUTION, AND CORRELATION 
305
closer inspection, we see that some of the results at some of the steps are in fact 
the same — steps 3 and 4 produce the same result because the same  x and  h terms 
overlap. What is occurring is that the modulo addressing is wrapping the second 
sequence around (aliasing) so that incorrect output terms are calculated. How could 
we prevent this and still use circular convolution? The answer is to prevent the 
wrapping causing any  “ damage, ” and we can do this by the simple expedient of 
padding some zeros at the end of the array. Figure  8.36 shows the calculation steps 
for circular convolution, with one zero appended. There are still some terms calcu-
lated in the output which are not present in the linear convolution. Inspection shows 
that this occurs because the  h sequence overlaps more than one of the  x terms. So 
if we pad with two zeros, as shown in Figure  8.37 , the maximum overlap is only 
one (with the rest of the  h terms being multiplied by zero), and the correct linear 
convolution results.  
 The conclusion we reach is that we must pad the longer sequence with ( M  −  1) 
zeros, where  M is the length of the shorter sequence. It goes without saying that 
the shorter sequence is also padded with zeros, but this is to equalize its length 
with the (padded) longer sequence. These zeros are shown as dots ( · ) in the 
ﬁ gures. 
 FIGURE 8.36   Circular convolution showing padding with one zero. 
Circular convolution—pad with one zero
x(n)
=
N = 4 data points
x(0)
x(1)
x(2)
x(3)
hm
=
h0
h1
h2
Filter impulse response M = 3
Step 1
x(0)
x(1)
x(2)
x(3)
0
h0
·
·
h2
h1
0 = h0x(0)
Step 2
x(0)
x(1)
x(2)
x(3)
0
h1
h0
·
·
h2
1 = h0x(1) + h1x(0)
Step 3
x(0)
x(1)
x(2)
x(3)
0
h2
h1
h0
·
·
2 = h0x(2) + h1x(1) + h2x(0)
Step 4
x(0)
x(1)
x(2)
x(3)
0
·
h2
h1
h0
·
3 = h0x(3) + h1x(2) + h2x(1)
Step 5
x(0)
x(1)
x(2)
x(3)
0
·
·
h2
h1
h0
4 = h1x(3) + h2x(2)
Total outputs: 5 (N + 1)
Σ
Σ
Σ
Σ
Σ
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
306 
CHAPTER 8 DISCRETE-TIME FILTERS
 8.9.5  Using the  FFT to Perform Convolution 
 So now we have an understanding of linear convolution, circular convolution, and 
how the two are related. The ﬁ nal piece in the puzzle is to determine how this helps 
in developing a fast algorithm. 
 Figure  8.38 shows a numerical example of convolution and introduces the fast 
approach, which we will develop further. The ﬁ rst line shows the standard convolu-
tion. The second line calculates the output 
 
 y
x
h
=
( )⋅
IDFT DFT
DFT
(
( )),  
 (8.24) 
where DFT( ) calculates the DFT of the input vector, IDFT( ) calculates its inverse, 
and  · is the vector dot product. Before we look into why the DFT has been employed, 
let us examine the outputs as calculated in Figure  8.38 . With no zeros appended to 
 FIGURE 8.37   Circular convolution showing padding with two zeros. In this case, the 
number of zeros is one less than the length of the shorter sequence. This is the important 
case where the end of the  h sequence does not wrap around to the start of the x sequence 
in the last step(s). As a result, the circular convolution result is the same as the linear 
convolution.  
Circular convolution—pad with two zeros
x(n)
=
N = 4 data points
x(0)
x(1)
x(2)
x(3)
hm
=
h0
h1
h2
Filter impulse response M = 3
Step 1
x(0)
x(1)
x(2)
x(3)
0
0
h0
·
·
·
h2
h1
0 = h0x(0)
Step 2
x(0)
x(1)
x(2)
x(3)
0
0
h1
h0
·
·
·
h2
1 = h0x(1) + h1x(0)
Step 3
x(0)
x(1)
x(2)
x(3)
0
0
h2
h1
h0
·
·
·
2 = h0x(2) + h1x(1) + h2x(0)
Step 4
x(0)
x(1)
x(2)
x(3)
0
0
·
h2
h1
h0
·
·
3 = h0x(3) + h1x(2) + h2x(1)
Step 5
x(0)
x(1)
x(2)
x(3)
0
0
·
·
h2
h1
h0
·
4 = h1x(3) + h2x(2)
Step 6
x(0)
x(1)
x(2)
x(3)
0
0
·
·
·
h2
h1
h0
5 = h2x(2)
Total outputs: 6 (N + 2)
Σ
Σ
Σ
Σ
Σ
Σ
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.9 FAST ALGORITHMS FOR FILTERING, CONVOLUTION, AND CORRELATION 
307
x, the four output points calculated appear to have only the last two output values 
(34 and 52) in common with the  “ correct ” convolution. With a zero appended, three 
values correspond (34, 52, and 45). Finally, with two zeros appended, the full con-
volution is correctly calculated. Note that in the ﬁ gure, we have employed the 
 fft () library function in MATLAB (an FFT rather than a DFT), and this will form 
the basis of the fast algorithm. Furthermore, the  h sequence has zeros appended so 
that it has a length equal to the x vector (which itself may be augmented with 
zeros). This is so that the vector dot product can be calculated (recall that for  N input 
samples, the FFT calculates  N output values). 
 Let us look further into Figure  8.38 and the ﬁ rst FFT calculation. Although 
the last two values output (34 and 52) correspond to the true convolution, the ﬁ rst 
two (50 and 44) do not correspond. However, looking more carefully, it may be 
observed that 50  =  45  +  5 and 44  =  28  +    16. In other words, the last two correct 
convolution values are added to the ﬁ rst two to produce the (erroneous) convolution. 
In fact, this operation is not linear convolution but corresponds to circular convolu-
tion as introduced in the previous section. The same can be said of the calculation 
with one zero appended — the  “ incorrect ”  value of 33 is in fact 28  +  5, which again 
is circularly wrapped. 
 So, from this numerical example, it appears that Equation  8.24 is actually 
calculating the circular convolution. From what we have examined, circular convo-
lution can in fact be made into linear convolution by appending the appropriate 
number of zero values to the longer sequence — the appropriate number is in fact the 
length of the second sequence minus one, as shown previously. 
 FIGURE 8.38   A numerical example of fast convolution using the FFT. Note that we 
need to pad the second sequence with zeros so as to be equal in length to the ﬁ rst. To 
produce a  “ correct ” linear convolution, the ﬁ rst sequence must be padded with  P zeros, 
where  P is one less than the length of the second sequence. Failure to correctly pad with 
zeros produces erroneous results.  
x = [1 2 3
4 ] ;
h = [5 6
7 ] ;
linear convolution
conv (x , h)
5
16
34
52
45
28
no zeros appended
i f f t ( f f t ( [ x ] ) . * f f t ( [ h 0 ] ) )
50
44
34
52
one zero appended
i f f t ( f f t ( [ x
0 ] ) . * f f t ( [ h 0
0 ] ) )
33
16
34
52
45
two zeros appended
i f f t ( f f t ( [ x 0
0 ] ) . * f f t ( [ h 0 0
0 ] ) )
5
16
34
52
45
28
Identical
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
308 
CHAPTER 8 DISCRETE-TIME FILTERS
 So this now forms the basis of the fast convolution algorithm — we append 
zeros, calculate FFTs of both input sequences, multiply sample by sample, and 
calculate the inverse FFT. This may seem like a roundabout way to implement 
convolution (and hence FIR ﬁ ltering); however, let us consider the computational 
cost. For a direct convolution of block length  N , for each of the  N outputs, we need 
to calculate a multiply – add product. Thus, we may say that the complexity is pro-
portional to  N 2 . For the FFT approach, we need to perform three FFTs (actually, two 
FFTs and one inverse, but the complexity is identical). Recall that the number of 
operations for an FFT is proportional to   N
N
2
2
(
)log
. Thus, we would have three 
times this, plus the vector dot product. So, it is reasonable to say that the order - of -
 magnitude complexity is  N  log 2  N . 
 Thus, the fast approach, though more complicated to implement, will yield 
a substantial reduction in the number of computations, and hence execution time. 
Such an analysis depends on many factors, such as the processor instructions 
(e.g., the availability of a multiply – add instruction), how complex numbers are 
handled, the memory speed, whether the blocks can be held in faster cache memory, 
and so forth. 
 The actual improvement achieved depends on the system environment, but 
there is no doubt that even for relatively small blocks, the speedup is signiﬁ cant 
(and becomes even more pronounced for larger and larger blocks). 
 Now that the technique has been introduced, it is appropriate to place the 
theory on a ﬁ rmer footing. Recall that convolution is
 
 y n
h x n
m
m
m
M
( ) =
−
=
−
∑
(
)
0
1
 
 (8.25) 
and that the DFT is
` 
 X k
x n e
nk
N
n
N
( ) =
( )
−
=
−
∑
j2
0
1
π/ . 
 (8.26) 
 So, the DFT of  y ( n ) is
 
 Y k
y n e
nk
N
n
N
( ) =
( )
−
=
−
∑
j2
0
1
π/ .  
 (8.27) 
 Substituting the convolution equation for  y ( n ),
 
 
Y k
h x n
m
e
h
x n
m
m
m
M
nk
N
n
N
m
m
M
( ) =
−
⎛
⎝⎜
⎞
⎠⎟
=
−
(
)
=
−
−
=
−
=
−
∑
∑
∑
0
1
2
0
1
0
1
(
)
j
π/
e
nk
N
n
N
−
=
−
∑
j2
0
1
π/ .
 
 (8.28) 
 Multiplying the above by  e
e
mk
N
mk
N
−j
j
2
2
π
π
/
/ (which does not change the value, since we 
are effectively multiplying by one) and regrouping the summations, we have
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.9 FAST ALGORITHMS FOR FILTERING, CONVOLUTION, AND CORRELATION 
309
 
 
Y k
h
x n
m e
e
e
h
m
m
M
nk
N
n
N
mk
N
mk
N
m
m
( ) =
−
(
)
⋅
=
=
−
−
=
−
−
∑∑
0
1
2
0
1
2
2
j
j
j
π
π
π
/
/
/
=
−
−
−
−
=
−
∑∑
−
(
)
0
1
2
2
0
1
M
n m k
N
mk
N
n
N
x n
m e
e
j
j
(
) π
π
/
/
Shifted DFT







=
( )
=
−
−
∑h X k e
m
m
M
mk
N
0
1
2
j
π/ .
 
 (8.29) 
 For the  “ shifted DFT, ” it does not matter where we start the indexing if the samples 
in x are circularly addressed. The index ( n  −  m ) used to address samples in  x must 
correspond to the argument for the complex exponential — j(
)
n
m k
N
−
π/ . This just 
means that the indexing is delayed by  m samples. Since we are circularly addressing 
the array, the end summation is unchanged. As before, we denote circular addressing 
of any array  x ( n ) by  x n
( ). 
 The last line in the above derivation is just the product of the DFT of  h with 
the DFT of x using circular indexing, so we write it as
 
 
Y k
h X k e
H k
X k
m
m
M
mk
N
( ) =
( )
=
( )⋅
( )
=
−
−
∑
0
1
2


j
π/
.
 
 (8.30) 
 What this shows is that the DFT of the output can be calculated using the product 
of the DFTs of the two inputs. Furthermore, if we use circular addressing, the output 
is not the linear convolution of the two inputs but rather the circular convolution of 
the two inputs. Mathematically, we have calculated
 
 y n
h n
x n
( ) = ( )∗( )

 
 (8.31) 
 
 y n
h n
x n
( )
( )
( )
=
⊛
 
 (8.32) 
where  « means circular convolution. This is where Equation  8.23 comes from. 
Moreover, it justiﬁ es our use of the FFT in Equation  8.24 . 
 Using the FFT, which, as we have already seen, provides substantial compu-
tational savings, is the justiﬁ cation for using the FFT in the fast convolution 
algorithm. 
 8.9.6  Fast Convolution and Filtering 
for Continuous Sampling 
 One problem which may arise in many situations is the need to process samples in 
real time. In the above, the fast algorithm for convolution is based on block - oriented 
processing; that is, we must buffer a block of samples and then process them all at 
once. The ﬁ rst result is not available until the entire block has been processed. This 
does not occur if we use  “ direct ” implementation of the ﬁ ltering/convolution equa-
tion, since each output is calculated independently. But the direct calculation is 
slower to execute. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
310 
CHAPTER 8 DISCRETE-TIME FILTERS
 There are two common approaches to this problem. Both require buffering of 
a block, but we can choose the block size to be appropriate to the system design. 
Speciﬁ cally, the time delay will depend on the block size — the longer the block size, 
the longer the delay. A longer block size will yield faster average computation per 
sample but requires more memory and introduces a longer delay. What we do in 
practice is to employ an appropriate sized block for buffering. 
 Unfortunately, this has the potential to introduce discontinuities at the block 
boundary. Speciﬁ cally, if we refer back to Figure  8.31 , the last two samples in the 
calculated output ( M  −  1 in the general case) need to be factored into the ﬁ rst two 
outputs for the subsequent block. This is because the tail of the second vector will 
overlap both blocks at the boundary. 
 The two approaches to handle this problem are termed  “ overlap – add ” and 
 “ overlap – save. ” These are both based on the FFT fast convolution approach but 
handle the block discontinuity slightly differently. 
 Figure  8.39 shows the  overlap – add method. We have used an input size of 6, 
and because the second sequence is three samples long, we have an overlap of two 
samples. This means that the size for per - block processing, and hence the block 
length passed to the FFTs, is 6  +  2  =  8 (recall that the FFT requires the number of 
samples to be a power of two). Figure  8.39 shows the convolution of the concate-
 FIGURE 8.39   Convolution of sequential blocks using the overlap – add procedure. If the 
length of  h is  M , then ( M  −  1) zeros are appended, followed by circular convolution. The 
( M  −  1) terms at the end are saved and, when added to the ﬁ rst ( M  −  1) terms of the next 
block, produce the correct convolution for the continuous sequence. 
Fast convolution–overlap–add
x1
=
1
2
3
4
5
6
x2
=
7
8
9
10
11
12
x
=
x1 | x2
h
=
5
6
7
Convolution of both blocks
conv(x,h)
x
h
=
5
16
34
52
70
88
106
124 142 160 178 196 149
84
First block – [x1 | 0 0]
h
5
16
34
52
70
88
71
42
Second block – [x2 | 0 0]
h
35
82 142 160 178 196 149
84
Add
Result
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.10 CHAPTER SUMMARY 
311
nated blocks x 1 and x 2 . This is the benchmark for the correct result. However, we 
want to produce outputs in a shorter time frame — remember that in practice, the 
sequence may, in fact, be essentially inﬁ nite in length. So for the ﬁ rst block, we 
append two zeros and calculate the (fast) convolution. As shown earlier, the last two 
outputs [71 42] are the residual which must be added to the ﬁ rst two samples of the 
subsequent block, so we must save them.  
 In the next step, the second block x 2 again has two zeros appended, and with 
these six new samples, the circular convolution is performed. The ﬁ rst two samples 
are not used directly in the output but must be added to the last two values from the 
previous calculation — in this case, [35 82] is added to [71 42] to produce the two 
output samples, [106 124]. The following six output samples are taken from the last 
six values of the current convolution block (i.e., 142, 160,  …  , 84). The process as 
shown for the second block is then repeated — take new samples, append zeros, 
calculate block, add two samples from the beginning of this block to the end of the 
previous block. Note that although it seems the very ﬁ rst block is treated differently, 
in fact, it is identical if we imagine the overlap from the previous block to be 
[0 0] — that is, a zero vector upon initialization. 
 A second method for performing fast convolution and ﬁ ltering on continuous 
blocks is the  overlap – save  approach, as illustrated in Figure  8.40 . This method does 
not require the saving and adding of values in the region of overlap, as does the 
overlap – add method. It takes advantage of a problem noted earlier — the fact that the 
higher samples are wrapped around and  “ aliased ” into the lower samples. Referring 
to the ﬁ gure, we begin with two zeros prepended to the ﬁ rst six samples. A block 
length of 8 is then produced using the fast FFT - based convolution. The ﬁ rst two 
(aliased) samples are then discarded. For the second block, we prepend not two 
zeros, but the last two samples from the previous input block (in this case, the values 
[5 6]). Together with the six new samples, we have another calculation block of 8. 
The ﬁ rst two samples are again discarded. The process repeats as per the second 
block: Overlap the two samples from the previous input block, calculate, and discard 
the ﬁ rst two outputs. 
 For this reason, this approach has sometimes been referred to as  “ overlap –
 discard ” or  “ overlap – scrap, ” both of which arguably better describe the algorithm. 
To generalize the procedure, we must remember that the two overlapped and dis-
carded samples are in fact ( M  −  1) samples for an impulse response vector  h of 
length  M . 
 8.10  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  a review of  ﬁ lter speciﬁ cations in the frequency domain 
 •  the role of  FIR ﬁ lters 
 •  how ﬁ lters may be  implemented , including  fast algorithms 
 •  the link between fast ﬁ ltering algorithms and algorithms for  fast convolution 
and  fast correlation 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
312 
CHAPTER 8 DISCRETE-TIME FILTERS
 FIGURE 8.40   Convolution of sequential blocks using the overlap – save procedure. If the 
length of  h  is  M , then  M  −  1 zeros are prepended, followed by circular convolution. The 
 M  −  1 terms at the start are discarded. For subsequent blocks, the last  M  −  1 terms from 
the previous block are prepended, and the convolution/discard process is repeated. This 
produces the correct convolution for the continuous sequence.  
Fast convolution–overlap–save
x1
=
1
2
3
4
5
6
x2
=
7
8
9
10
11
12
x
=
x1 | x2
h
=
5
6
7
Convolution of both blocks
conv(x,h)
x
h
=
5
16
34
52
70
88
106 124
142 160 178 196 149
84
First block – [0 0 | x1]
h
71
42
5
16
34
52
70
88
Discard
Second block – [x1 (5:6) | x2]
h
174
144 106 124 142 160 178 196
Discard
 PROBLEMS 
 8.1.  By sketching the individual frequency responses, explain how bandstop and 
bandpass ﬁ lters may be realized using cascades of low - and high - pass ﬁ lters (refer to 
Section  8.5 ). 
 8.2.  A bandpass ﬁ lter is required to pass frequencies from 8 to 16  kHz using a sample rate 
of 96  kHz.
 (a)  For this sample rate, draw a frequency response plot showing how the required 
characteristic would map to normalized sampling frequencies in radians per 
sample. 
 (b)  Derive an expression for the ﬁ lter coefﬁ cients. 
 (c)  Explain how you would test the frequency response of the ﬁ lter using the 
coefﬁ cients derived. 
 (d)  Write a MATLAB script to evaluate the frequency response of the ﬁ lter. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
8.10 CHAPTER SUMMARY 
313
 (e)  What would you expect to happen as the ﬁ lter order is increased? If the coef-
ﬁ cients were windowed? 
 (f)  Under what circumstances would a sample rate of 48  kHz be acceptable? 
 8.3.  An ultrasonic signal of maximum bandwidth 40  kHz is to be processed for the purpose 
of measurement of distance. Would a sampling rate of 48  kHz be suitable? Would 
96  kHz be suitable? Explain your answer in each case. If an interfering signal of 
10  ±  1  kHz is known to interfere with the measurement sensor, determine the normal-
ized band edge frequencies for a bandstop ﬁ lter, in radians per sample. 
 8.4.  It was stated that for every complex - valued pole (or zero), we must also place a 
complex conjugate pole (or zero) on the  z plane so that we have a realizable transfer 
function (i.e., no complex coefﬁ cients in the difference equation).
 (a)  Create a transfer function with one pole at angle  ω p and radius  r  <  1. Derive 
the difference equation and show that it requires complex coefﬁ cients. 
 (b)  To the above, add a pole at the complex conjugate (i.e., angle  −  ω p at the same 
radius). Derive the difference equation and show that it requires only real 
coefﬁ cients. 
 8.5.  Design a notch ﬁ lter with two notches to cater for half - wave rectiﬁ ed sine wave 
interference from a power line. Plot the frequency response to verify that the response 
is correct.  
 8.6.  Design an FIR ﬁ lter which has two passbands: one from  π 10 to  2
10
π
 and another 
from  7
10
π
 to  8
10
π
. By plotting the frequency response magnitude, verify that the 
ﬁ lter meets the passband speciﬁ cations (choose the ﬁ lter order as you consider to be 
appropriate). Try different ﬁ lter orders and add a Hamming window to improve the 
response. 
 8.7.  A moving - average ﬁ lter simply computes the arithmetic average of the last  N samples.
 (a)  Write the difference equation for a three - term averaging ﬁ lter. 
 (b)  How many poles does the system have? How many zeros? 
 (c)  Plot the pole – zero diagram and from that, estimate the shape of the frequency 
response, from just above zero to the Nyquist frequency ( π radians per sample). 
What shape would you classify the frequency response as? Is that what you 
would expect from intuition? 
 8.8.  Repeat the above moving - average ﬁ lter exercise for a generic  N - term ﬁ lter and show 
that the transfer function is
 G z
N
z
z
N
( ) =
−
−
⎛
⎝⎜
⎞
⎠⎟
−
−
1
1
1
1
.  
 8.9.  A Hilbert transform ﬁ lter is a ﬁ lter that passes all frequencies within a speciﬁ ed band 
with a phase shift of  − 90 ° .
 (a)  If the passband is 0 to  π , verify that the desired frequency response is
 
 Hd ω
ω
π
π
ω
( ) = −
<
<
−
<
<
⎧⎨⎩
j
j
:
:
.
0
0  
 (8.33) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
314 
CHAPTER 8 DISCRETE-TIME FILTERS
 (b)  Show that the corresponding FIR ﬁ lter coefﬁ cients may be found from
 
 h
n
n
n
n
n =
−
(
)
≠
=
⎧
⎨⎪
⎩⎪
1 1
0
0
0
π
π
cos
:
:
. 
 (8.34) 
 Note: Treat  n  =  0 as a special case before performing the integration. 
 8.10.  As in the previous question, a Hilbert transform ﬁ lter is a ﬁ lter that passes all frequen-
cies within a speciﬁ ed band with a phase shift of  − 90 ° . Implement the numerical 
integration in order to calculate the FIR ﬁ lter coefﬁ cients for  N  =  201. Plot the gain 
and phase response. Compare the phase response to that of a bandpass ﬁ lter over the 
same range. Verify that the phase shift of  π 2 is present, as expected. 
 8.11.  A differentiator is a ﬁ lter whose output approximates the time derivative of the input 
signal. It has an ideal magnitude response that is proportional to frequency.
 (a)  If the passband is 0 to  π , verify that the desired frequency response is
 
 Hd ω
ω
π
ω
π
( ) =
−
<
<
j
:
.  
 (8.35) 
 (b)  Show that the corresponding FIR ﬁ lter coefﬁ cients may be found from
 
 h
n
n
n
n
n =
≠
=
⎧
⎨⎪
⎩⎪
cos
:
:
.
π
0
0
0
 
 (8.36) 
 Note: Treat  n  =  0 as a special case before performing the integration. 
 8.12.  As in the previous question, a differentiator is a ﬁ lter whose output approximates the 
time derivative of the input signal. Implement the numerical integration in order to 
calculate the FIR ﬁ lter coefﬁ cients for  N  =  201. Plot the gain and phase response. 
Verify that the gain response is as expected and explain the shape of the response. 
 8.13.  Verify the result shown in Figure  8.39 for two blocks, and continue for a third block. 
That is, the FFT - based overlap – add should yield the same result as the directly cal-
culated convolution.  
 8.14.  Verify the result shown in Figure  8.40 for two blocks and continue for a third block; 
that is, the FFT - based overlap – save should yield the same result as the directly cal-
culated convolution.  
 8.15.  Section  8.9.5 noted that the order of magnitude for computations required for direct 
convolution and ﬁ ltering was  N 2 , versus  N  log 2  N for the fast FFT - based methods. 
Consider  N over the range 4, 8, 16,  …  , 1,024 and compare these quantities. Where, 
approximately, is the point at which the fast approach becomes viable in terms of 
reduced complexity?  
  
 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 CHAPTER 9 
RECURSIVE FILTERS 
 9.1  CHAPTER OBJECTIVES 
 On completion of this chapter, the reader should be able to 
 1.  design a  Butterworth or  Chebyshev prototype  analog low - pass ﬁ lter and plot 
its response; 
 2.  scale an analog low - pass ﬁ lter to a given cutoff and transform it to a  high - pass 
or  bandpass characteristic ; 
 3.  take an analog ﬁ lter design and create a  digital ﬁ lter approximation ; and 
 4.  understand the possible  shortcomings of the conversion from an analog to 
digital ﬁ lter design and compensate for those where possible. 
 9.2  INTRODUCTION 
 This chapter covers recursive ﬁ lters, that is, ﬁ lters with  “ memory. ” This is another 
class of ﬁ lter; in some ways, recursive ﬁ lters are complementary to the nonrecursive 
ﬁ lters discussed previously, and they may be an appropriate alternative in many situ-
ations. Recursive ﬁ lters are generally more difﬁ cult to design but often result in 
lower - order ﬁ lters as compared to nonrecursive ﬁ lters. This means that they require 
less memory and fewer computational resources to implement, as well as having an 
output after a shorter time period. However, the trade - off is a more involved design 
process. This chapter considers the design of recursive ﬁ lters using the classical 
method of analog ﬁ lter prototyping. Although a brief introduction to continuous 
systems is given, some prior exposure to continuous systems theory is desirable. 
 9.2.1  Deﬁ ning a Recursive Filter 
 With the exception of the notch ﬁ lters introduced in Chapter  8 , all of the ﬁ lters 
discussed so far have a ﬁ nite - duration impulse response. This means that once the 
input is removed, the output will decay to zero. One might say that this would be 
true of any digital ﬁ lter, and in a practical sense, that is the case (assuming the 
transfer function is stable; sometimes, we want a  “ marginally stable ” system in order 
to build a digital oscillator). If we have pole at anywhere other than the origin, the 
output, theoretically, never reaches zero. 
315
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
316 
CHAPTER 9 RECURSIVE FILTERS
 For example, consider
 Y z
X z
z
z
( )
( ) =
−0 9. ,  
 which has a corresponding difference equation,
 y n
x n
y n
( )
( )
.
(
).
=
+
−
0 9
1  
 For any  x ( n ) applied then removed (i.e., subsequent  x ( n )  =  0), the output  y ( n ) will 
follow a decaying pattern and never reach zero (theoretically, at least). For example, 
if  x (0)  =  1 and subsequent  x ( n )  =  0, then  y ( n )  =  1, 0.9, 0.81,  . . .  . For this reason, this 
class of system is termed inﬁ nite impulse response (IIR). 
 The converse, where the impulse response does tend to a ﬁ nal settling value —
 in theory and in practice, has zeros and no poles (or possibly only simple poles at 
 z  =  0, which represent a time delay). For example,
 Y z
X z
z
z
( )
( ) =
+ 0 9.  
 has a corresponding difference equation,
 y n
x n
x n
( )
( )
.
(
).
=
+
−
0 9
1  
 Strictly speaking, this transfer function  does have a pole, but it is only a single pole 
at  z  =  0. If the input  x ( n ) has some (nonzero) value, and is then removed in subse-
quent samples, the output will equate to zero eventually. In the present case, if we 
have  x ( n )  =  1, 0, 0,  …  , then the corresponding output is 1, 0.9, 0, 0,  . . .  and zero 
ever after. This is a ﬁ nite impulse response (FIR) system. 
 FIR difference equations are nonrecursive, and hence there is no problem with 
instability. IIR ﬁ lters, on the other hand, have recursive coefﬁ cients — a type of 
 “ memory ” of past outputs — and, as a consequence, are shorter than FIR ﬁ lters for 
the same or similar frequency responses. The recursive coefﬁ cient can be seen in 
the preceding examples, where we have a term corresponding to previous outputs 
(  y ( n  −  1) in the above) in the IIR system but not in the FIR system. 
 In general, IIR ﬁ lters have the following characteristics:
 1.  They are usually based on discrete (digital) approximations of analog ﬁ lters. 
 2.  They have a nonlinear phase, a property which may or may not be important 
in any given application. 
 3.  A perfectly ﬂ at frequency response is possible, in either the passband or the 
stopband.  
 4.  IIR ﬁ lters are, in general, not easy to construct if an arbitrary frequency 
response is desired. 
 Because IIR ﬁ lters are based on continuous - time ﬁ lter designs, we will provide an 
overview of the necessary background on continuous - time systems in general. We 
do not attempt to give an exhaustive coverage, nor be mathematically rigorous, since 
such topics are covered in existing texts. Rather, we a give a concise summary of 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.2 INTRODUCTION 
317
the salient points of analog transfer functions before investigating the digital signal 
processing aspects, so as to serve as a reference point for the reader. 
 This is followed by a consideration of continuous - to - discrete conversion 
methods. There is no one  “ right ” method for converting continuous ( “ analog ” ) IIR 
ﬁ lters into discrete - time transfer functions, and we consider the two most 
widely used methods, which are known as the bilinear transformation (BLT) and the 
impulse invariant method (IIM). 
 Since the usual approach is to design a low - pass ﬁ lter and convert the design 
by changing the structure into a matching high - pass or bandpass response as required, 
subsequent sections will develop this concept and illustrate how MATLAB code 
may be developed to aid in both understanding the concepts involved and addressing 
the speciﬁ c design problem. 
 9.2.2  Recursive Discrete - Time Filter Structure 
 An IIR ﬁ lter is shown in block diagram form in Figure  9.2 . Because such ﬁ lters are 
based on analog designs, they must be converted into discrete form. This can be 
rather difﬁ cult, and in practice, only an approximation can be achieved. Realistically, 
any approximation will have shortcomings, and in order to produce good designs, 
it is important to understand what these shortcomings are. 
 Figure  9.1 shows the standard FIR ﬁ lter structure, which has already been 
studied in the previous chapter. The output  y ( n ) at sample  n is a weighted sum of 
the input  x ( n ) and past inputs [ x ( n  −  1),  x ( n  −  2),  … ]. The weighting coefﬁ cients 
[ b 0 ,  b 1 ,  … ] are ﬁ xed at the design stage, as is the number of coefﬁ cients (thereby 
 FIGURE 9.1   Block diagram representation of a ﬁ nite impulse response (FIR) ﬁ lter. The 
output is a linearly weighted summation of the input and delayed inputs. Four  “ taps ” 
and weights are shown, and of course this can be extended into any number of taps and 
weights. Depending on the application at hand, this structure may extend to hundreds 
of weights (or more). 
z −1
z −1
z −1
x(n)
x(n −1)
x(n −2)
x(n −3)
b0
b1
b2
b3
z −1
z −1
z −1
y(n −1)
y(n −2)
y(n −3)
−a1
−a2
−a3
y(n)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
318 
CHAPTER 9 RECURSIVE FILTERS
determining the order of the ﬁ lter). The ﬁ lter operation simply consists of calculating 
a new output,  y ( n ), at each sample instant  nT , with the the multiply – add operations 
required to compute the output deﬁ ned by the difference equation. The block diagram 
can be converted into an appropriate weighted - sum transversal structure (sometimes 
called  “ tapped delay line ” ) deﬁ ned by 
 
 y n
b x n
b x n
b x n
N
N
( ) =
( )+
−
(
)+
+
−
(
)
0
1
1

.  
 (9.1) 
 The IIR ﬁ lter may be considered as an extension of the FIR structure, wherein the 
output is fed back to the summation. This is shown in Figure  9.2 . The output  y ( n ) 
at time  nT depends not only on the present and past inputs but also on past outputs. 
The IIR difference equation in general is 
 
 
y n
b x n
b x x
b x n
N
a y n
a y n
a y n
N
M
( ) =
( )+
−
(
)+
+
−
(
)
(
)
−
−
(
)+
−
(
)+
+
0
1
1
2
1
1
2


−
(
)
(
)
M
.  
 (9.2) 
 It may be imagined that the IIR ﬁ lter has a memory of past outputs. This is why, in 
general, fewer coefﬁ cients are required for an IIR ﬁ lter to achieve a similar response 
to an FIR ﬁ lter. Moreover, the design process for IIR ﬁ lters is almost always more 
involved, as we will see shortly. 
 The usual approach for designing FIR ﬁ lters is a direct one: We can go from 
the desired frequency response to the impulse response, and then to the difference 
equation coefﬁ cients. However, because of the memory effect of IIR ﬁ lters, such a 
design approach is not possible for recursive ﬁ lters. This is because the ﬁ lter coef-
ﬁ cients themselves become the impulse response in the FIR case, but the same is 
not true for the IIR case. Instead, it is usual to start with an analog ﬁ lter equation 
(Laplace or  s domain) and to convert that to a discrete - time structure. The following 
sections deal with each of these issues in turn. First, however, we brieﬂ y review the 
Laplace transform in both the time domain and the frequency domain.  
 FIGURE 9.2   Block diagram representation of an inﬁ nite impulse response (IIR) ﬁ lter. 
This is an extension of the FIR ﬁ lter, in one important way: the feedback or recursion of 
the output. This constitutes a  “ memory ” effect and usually allows the ﬁ lter order to be 
lower for a similar frequency response speciﬁ cation. Note that this representation implies 
 a 0  =  1 in Equation  9.2 . This simply means that all other coefﬁ cients need to be normalized 
by this value if  a 0  ≠  1. 
z −1
z −1
z −1
x(n)
x(n −1)
x(n −2)
x(n −3)
y(n)
b0
b1
b2
b3
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.3 ESSENTIAL ANALOG SYSTEM THEORY 
319
 9.3  ESSENTIAL ANALOG SYSTEM THEORY 
 An analog or continuous - time system is one which has a smoothly varying response 
over any time range. The vast majority of the  “ real ” world is analog in nature. Almost 
any quantity which one cares to mention (sound, vibration, light, temperature) varies 
continuously over time, and we hope that our sampling of the quantity is sufﬁ ciently 
precise in time so as to be able to capture any event of importance. Discrete - time 
or digital systems, on the other hand, rely on snapshots of the analog world at the 
sampling instants. 
 It is not surprising, then, that there is a vast quantity of literature developed 
over the last hundred years or more analyzing continuous - time systems and, in 
particular, electrical systems. Electrical systems are represented as circuit elements 
(resistors, capacitors, inductors) with an observed quantity (current, voltage, charge) 
varying smoothly over time. The theory and mathematical tools for this are well 
developed, and some approaches to discrete - time systems rely on extensions or 
modiﬁ cations of the analog system theory. Perhaps the best - known mathematical 
tool for analyzing electrical networks is the Laplace transform. 
 9.3.1  The Laplace Transform 
 The solution of analog systems using differential equations to model their observed 
behavior — and hence to predict their future states — is the staple of much of the 
electrical and mechanical system theory. Differential equations are difﬁ cult to solve 
algebraically, and the Laplace transform has a rich history as a tool in aiding their 
solution. Mathematically, the Laplace transform is deﬁ ned as an operation on an 
observed system characteristic  f ( t ) using the notation  F s
f t
( ) =
( )
{
}
L
, where the 
transformation operation is deﬁ ned as
 
 F s
e
f t dt
st
( ) =
( )
−
∞∫0
.  
 (9.3) 
 Note that the standard notation is to use lowercase for the time function, and upper-
case for the transformed function. Thus, we have the Laplace transform pair 
 f ( t )  ↔  F ( s ) for a given  f ( t ). 
 A key result which can be derived from the deﬁ nition is the differentiation 
property. If we are prepared to assume that a system is initially at rest (i.e., zero 
initial conditions or  f (0)  =  0), then the operation of differentiation of  f ( t ) can be 
replaced with a multiplication of the corresponding Laplace transform by the 
operator  s . Mathematically,
 
 
f t
F s
df t
dt
sF s
( ) →
( )
( ) →
( )
 
 (9.4) 
 for zero initial conditions (  f ( t )  =  0 for  t  <  0). Thus, the operation of differentiation 
is replaced by a simple multiplication. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
320 
CHAPTER 9 RECURSIVE FILTERS
 Intuitively, the operation of integration is the  “ opposite ” of differentiation in 
calculus, and so it is satisfying to note that integration can be represented in a similar 
way by division by  s . Assuming zero initial conditions, we have
 
 
f t
F s
f t dt
F s
s
( ) →
( )
( )
→
( )
∫
.
 
 (9.5) 
 Of course, the above results have a sound mathematical basis, but we do not present 
the proofs here. 
 Another key result in linear systems and signal processing is the convolution 
property. Recall that the output of a linear system is the convolution integral per-
formed on (or, simply, the convolution of) the input signal and the system ’ s impulse 
response. This complex integral operation can be replaced by a simple multiplication 
of the respective Laplace transforms. If we deﬁ ne the input, output, and system 
impulse response as
 x t
X s
( ) →
( ) 
 y t
Y s
( ) →
( )  
 h t
H s
( ) →
( ),  
 then, for zero initial conditions, the system output may be calculated via convolution 
and Laplace transforms:
 
 y t
h
x t
d
t
( ) =
( )
−
(
)
∫
τ
τ
τ
0
 
 (9.6) 
 
 Y s
X s H s
( ) =
( )
( ).  
 (9.7) 
 Here we have used a common convention:  x ( t ) for the input to a system,  y ( t ) for the 
output, and  h ( t ) for the impulse response of a system. We can mathematically deter-
mine the Laplace transform of known signals (those signals which have an equation 
deﬁ ning their amplitude at a given time). Tables of standard transforms are widely 
available; a few common transforms are listed in Table  9.1 . 
 TABLE 9.1   Some Laplace Transforms 
 Description 
 Signal  f ( t ) 
 Transform  F ( s ) 
 Step at  t  =  0 
 u ( t ) 
 
1
s 
 Step at  t  =  τ 
 u ( t  −  τ ) 
 
e
s
s
−τ
 
 Decaying exponential for  t  ≥  0 
 e − at  
 
1
s
a
+
 
 Sine wave with frequency  Ω for  t  ≥  0 
 sin Ω t 
 
Ω
Ω
s2
2
+
 
 Cosine wave with frequency  Ω for  t  ≥  0 
 cos Ω t 
 
s
s2
2
+ Ω  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.3 ESSENTIAL ANALOG SYSTEM THEORY 
321
 In order to calculate a system ’ s output in response to an input, we need to 
know the impulse response of the system. Then, the Laplace multiplication operation 
can be applied to determine the output using  Y ( s )  =  X ( s ) H ( s ). Then, we need to map 
from  Y ( s ) back to  y ( t ) using algebraic expansion and the standard tables mentioned 
above. As an alternative to mathematically deriving transforms of systems and 
applying the convolution property, we can iteratively approximate the integrations 
implicit in the  1 s operation, as will be explained in the following section. 
 9.3.2  Time Response of Continuous Systems 
 The response of a continuous - time function consists of the values of  y ( t ) from  t  =  0 
until some speciﬁ ed time, given the input  x ( t ) and the coefﬁ cients of the transfer 
function. We now wish to develop a method for modeling the time response of an 
arbitrary system. Speciﬁ cally, we want to determine the response of a system to a 
given input, using only the impulse response. 
 If the system is linear and time invariant (i.e., the output can be calculated via 
superposition and the coefﬁ cients do not change over time), the impulse response 
completely characterizes the system. To develop this idea, suppose the transfer 
function is
 Y s
X s
s
s
s
( )
( ) =
+
+
+
2
1
2
4
2
.  
 Then, the coefﬁ cients  b  =  [2 1] and  a  =  [1 2 4] completely specify the system. We 
can rewrite this so as to have only negative powers of  s by multiplying the numerator 
and the denominator as follows:
 
Y s
X s
s
s
s
s
s
s
s
s
s
s
s
s
s
( )
( ) =
+
+
+
=
⋅
+
+
+
+
=
+
−
−
−
2
1
2
4
0
2
1
2
4
0
2
2
2
2
2
1
0
2
1
0
0
1 +
+
+
−
−
−
1
1
2
4
2
1
2
s
s
s
.
 
 This is easily rearranged to yield the output as a function of the input:
 
Y s
s
s
X s
s
s
s
Y s
X s
s
s
s
( )
+
+
(
) =
( )
+
+
(
)
( ) =
( )
+
+
−
−
−
−
−
−
1
2
4
0
2
1
0
2
1
1
2
0
1
2
0
1
22
1
2
2
4
(
)−
( )
+
(
)
−
−
Y s
s
s
.
 
 Since we only have negative powers of  s , we can apply the idea that division by the 
 s operator corresponds to integration. So, for any signal  x ( t ),
 X s
s X s
x t
( ) =
( ) →( )
0
 
 X s
s
s X s
x t dt
( ) =
( ) →
( )
−
∫
1
 
 X s
s
s X s
x t dt dt
( ) =
( ) →
( )
−
∫∫
2
2
.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
322 
CHAPTER 9 RECURSIVE FILTERS
 The integrals may be computed using a numerical approximation to the integral. The 
continuous integrations are converted into sums using
 
 
x t dt
x t
t
t
k
k
K
( )
≈
( )
=
=
−
∫
∑
0
0
1
τ
δ ,  
 (9.8) 
where the index  K is chosen to correspond to the limit  τ , and  x ( t k ) is the value of 
 x ( t ) for a speciﬁ c time  t  =  t k . The integration approach can be generalized as follows:
 
 
Y s
X s
b s
b s
b
s
a s
a s
a
s
b s
M
M
M
N
N
N
M
( )
( ) =
+
+
+
+
+
+
=
−
−
−
−
−
−
0
1
1
2
1
0
0
1
1
2
1
0
0


−
−
−
−
−
−
−
+
−
−
+
+
+
+
+
+
(
)
=
+
1
1
2
1
0
1
0
1
1
1
1
0
1
b s
b
s
s
a
a s
a
s
b s
b s
M
M
N
N
N
M
N
M


N
M
N
N
N
b
s
a
a s
a
s
−
−
−
+
−
−
−
+
+
+
+
+
+
1
1
1
0
1
1
1
1


.
 
 (9.9) 
 We need  y ( t ) on the left - hand side, and this is easily accomplished by dividing all 
 b m and  a n by  a 0 . So, without loss of generality, for  a 0  =  1, we have
  Y s
X s b s
b s
b
s
Y s
a s
a
s
M
N
M
N
M
N
N
N
( ) =
( )
+
+
+
(
)−
( )
+
+
−
−
−
−
−
+
−
−
−
0
1
1
1
1
1
1
1


+
(
)
1 . 
 (9.10) 
 We are then in a position to implement our solution. Using the coefﬁ cients 
deﬁ ned previously, we set up the simulation as follows. First, we need to set the 
maximum simulation time, a small time increment  δ t for the integrations, and 
the number of samples in the output. For an impulse at  t  =  0, we need the input 
initialized as  x
t
0
1
( ) =
δ . The highest power of  s is  N (assuming more poles than 
zeros), and thus we need  N integral terms xi and yi. 
 b  = [2 1]; 
 a  = [1 2 4]; 
 tmax  = 10; 
 dt  = tmax/1000; 
 L  =  round (tmax/dt); 
 x  =  zeros (L, 1); 
 x(1)  = 1/dt; 
 y  =  zeros (L, 1); 
 t  =  zeros (L, 1); 
 b  = b(:)/a(1); 
 a  = a(:)/a(1);
M  =  length (b); 
 N  =  length (a); 
 xi  =  zeros (N, 1); 
 yi  =  zeros (N, 1);  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.3 ESSENTIAL ANALOG SYSTEM THEORY 
323
 if (N  >  M) 
      % denominator order greater, pad with zeros on LHS to
      % make same length 
      bp  = [ zeros (N   −  M, 1); b]; 
 else 
      bp  = b; 
 end 
 ap  = [0; a(2:N)];    % cancel a0 multiplication by setting
                        % to zero 
 tcurr  = 0; 
 for n  = 1:L 
      t(n)  = tcurr; 
      ynew  =  sum (bp. * xi)    −  sum (ap. * yi); 
      y(n)  = ynew; 
      xi(1)  = x(n); 
      yi(1)  = y(n); 
      for i  = 2:N 
           xi(i)  = xi(i)  + xi(i   −  1) * dt; 
           yi(i)  = yi(i)  + yi(i   −  1) * dt; 
      end 
      tcurr  = tcurr  + dt; 
 end 
 In the above generalization, we have a term,  s M  −  N . This is implemented by 
shifting the  b coefﬁ cients to the right in an array — remember that the leftmost term 
is  s 0 , then the next is  s  − 1 , and so forth, each corresponding to a shift - right by one 
position in the array. 
 We are then in a position to implement the iteration of the differential equa-
tions using the following loop. The key steps are the weighting of each input and 
output term by their respective coefﬁ cients, which is accomplished using 
 sum (bp. * xi)  −  sum (ap. * yi), followed by the update of the integrals of  y ( t ) 
and  x ( t ) using a simple rectangular approximation to the area of the form 
xi(i)   +  xi(i   −  1) * dt. 
 We now have a general method of approximating a continuous - time system 
in order to obtain the impulse response. An example of this is shown in Figure  9.3 . 
This ﬁ gure also shows the locations of the poles of the transfer function. As with  z 
domain functions, the poles for an  s - domain function are deﬁ ned as the values of  s 
that make the denominator equal to zero. The next step is to develop a general 
method for obtaining the frequency response for a given continuous - time system. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
324 
CHAPTER 9 RECURSIVE FILTERS
 9.3.3  Frequency Response of Continuous Systems 
 To determine the frequency response of a continuous - time linear system, we need 
to substitute  s →jΩ. The reason for this may be seen by comparing the Laplace 
transform  F ( s ) with the Fourier transform  F jΩ
(
):
 
 F s
e
f t dt
st
( ) =
( )
−
∞∫0
 
 (9.11) 
 
 F
e
f t dt
t
j
j
Ω
Ω
(
) =
( )
−
∞∫0
.  
 (9.12) 
 When we come to implement the frequency response, we have each of the coefﬁ -
cients  b m and  a n multiplied by terms of the forms  s M  − 1 −  m and  s N  − 1 −  n . We do not need 
to convert to negative powers of  s as in the time response calculation since we do 
not need to keep track of what the signal values were at any given time. We simply 
need to perform the substitution of  jΩ for  s . In the numerator, this gives terms of 
the form  bm
M
m
jΩ
(
)
−−
1
; the denominator has terms of the form  an
N
n
jΩ
(
)
−−
1 . We 
initialize using 
 b  = b(:); 
 a  = a(:); 
 M  =  length (b); 
 N  =  length (a); 
 bpow  = [M   −  1: − 1:0]; 
 apow  = [N   −  1: − 1:0]; 
 bpow  = bpow(:); 
 apow  = apow(:); 
 L  =  round (MaxOmega/dOmega); 
 H  =  zeros (L, 1); 
 Omega  =  zeros (L, 1);  
 FIGURE 9.3   Impulse response of the system  
1
1
2
1
2s
s
+
+
 and the corresponding poles. 
0
2
4
6
8
10
12
14
16
18
20
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1.0
Impulse response
Time (second)
Output y(t)
−2.0 −1.5 −1.0 −0.5
0
0.5
1.0
1.5
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
stable
unstable
s Plane
Real
Imaginary
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.3 ESSENTIAL ANALOG SYSTEM THEORY 
325
 The loop to calculate a frequency response magnitude  H jΩ
(
)  and phase 
 ∠
(
)
H jΩ  at each frequency point  Ω is then as follows. This gives us the complex 
value  H jΩ
(
), but often we only want the magnitude  H jΩ
(
) . 
 % starting frequency 
 OmegaCurr  = 0.01; 
 for    n  = 1:L 
       Omega(n)  = OmegaCurr; 
       % j Omega terms 
       NumFreq  = ones(M, 1)  * j  * OmegaCurr; 
       DenFreq  = ones(N, 1)  * j  * OmegaCurr; 
       % coefﬁ cient x (j Omega) ˆ n terms 
       Num  = b. * (NumFreq. ˆ bpow); 
       Den  = a. * (DenFreq. ˆ apow); 
       Hcurr  =  sum (Num)/ sum (Den); 
       H(n)  = Hcurr; 
       OmegaCurr  = OmegaCurr  + dOmega; 
 end 
 FIGURE 9.4   Frequency response of the system  1
1
2
1
2
s
s
+
+
(
) and its poles. 
−40
−30
−20
−10
0
10
Gain (dB)
Frequency (rad/s)
Frequency response—magnitude
10
−2
10
−1
10
0
10
1
10
−2
10
−1
10
0
10
1
−200
−150
−100
−50
0
Phase (degrees)
Frequency (rad/s)
Frequency response—phase
−2.0 −1.5 −1.0 −0.5
0
0.5
1.0
1.5
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Stable
unstable
s Plane
Real
Imaginary
 So we now have a general method for determining the frequency response of 
a continuous - time system. An example frequency response is shown in Figure  9.4 , 
which again also shows the locations of the poles of the transfer function.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
326 
CHAPTER 9 RECURSIVE FILTERS
 Since we now know how to compute the frequency response from a general 
transfer function, we now return to the original problem: how to determine a general 
transfer function which gives us a desired frequency response. 
 9.4  CONTINUOUS - TIME RECURSIVE FILTERS 
 This section covers the  “ classical ” design of recursive ﬁ lters using two common 
approaches: the Butterworth and Chebyshev polynomials. It builds upon the founda-
tion of continuous - time systems as outlined in the previous section and develops 
algorithms and codes to design ﬁ lters using these approaches. The usual method 
employed is to design a low - pass ﬁ lter with a unity cutoff frequency and then to 
extend to a low - pass ﬁ lter of any desired frequency, or a different characteristic 
(high - pass, bandpass, or other). Methods for converting the analog ﬁ lter design into 
a discrete - time implementation follow in subsequent sections. 
 9.4.1  Butterworth Filter Structure 
 Butterworth ﬁ lters provide a maximally ﬂ at response in either the passband or stop-
band. They are described in general by the transfer function  G ( s ), deﬁ ned as
 
 G s
s
c
N
( )
=
+ ⎛
⎝⎜
⎞
⎠⎟
2
2
1
1
Ω
.  
 
(9.13)
 
 This is for a ﬁ lter with a cutoff of  Ω c radians per second. We will see later that it is 
usual practice to design for  Ω c  =  1 and then to simply replace  s with  s / Ω c to scale 
the ﬁ lter to any desired cutoff as required. Indeed, it may be that we do not need a 
low - pass ﬁ lter but another characteristic such as a high - pass or bandpass ﬁ lter. 
 Note that this equation describes the magnitude  squared , not the actual value 
 G ( s ) which we would like; that is,  G ( s ) is a complex number for any  s and, as such, has 
magnitude and phase. So, we need to undertake a little more derivation to ﬁ nd the ﬁ lter 
transfer function. Equation  9.13 deﬁ nes not  G ( s ), as we would like, but rather  G s( )
2. 
Now, for any complex number  a
re
x
y
=
=
+
j
j
θ
, the complex conjugate is 
 a
re
x
y
* =
=
−
−j
j
θ
. The product of a complex number and its conjugate is  a a
a
* =
2. 
Recognizing that the frequency response is found when we put  s →jΩ, what we 
need in order to solve the problem and to determine  G ( s ) is to ﬁ nd solutions for 
 G ( s ) G ( − s ), and to use only  “ half  ” of the solution. This is done by replacing all  s 2 terms 
with  s · ( − s )  =  − s 2 and then by selecting only half of the resulting factors (poles). 
 For a concrete example, consider the case of  N  =  1. The poles of  G s( )
2 are 
the solutions of 1  +  s 2 N  =  0, so with  s 2  →  − s 2 , we have
 
1
0
1
1
1
2
2 1
2
+ −(
)
=
∴
−(
) = −
=
= ±
s
s
s
s
N
.
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.4 CONTINUOUS-TIME RECURSIVE FILTERS 
327
 We choose the stable pole only ( s  =  − 1), which yields a transfer function of
 G s
s
( ) =
+
1
1.  
 Note that the setting of the cutoff frequency  Ω c to unity above does not compromise 
the solution: The cutoff frequency is simply a scaling factor which can be incorpo-
rated as desired (this will be examined in later sections). 
 Consider now a second - order example ( N  =  2) and assume  Ω c  =  1 again. The 
poles of  G s( )
2 are at
 
1
0
1
2
2
4
4
4
+ −(
) =
∴
= −
=
=
(
)
(
)
s
s
s
e
s
e
j
j
odd number
odd number
π
π
.
 
 In the above, we need to remember that  − 1 is  ejπ, and that an angle of  π could be 
3 π , 5 π ,  − 1 π , or in fact any odd multiple of  π . This is where the (odd number)  π term 
comes from — it is usually written as (2 k  +  1) π , where  k is an integer. 
 The easiest way to visualize this result is to plot the factors (poles of the 
transfer function), as illustrated in Figure  9.5 . Starting at a radius of one, we move 
anticlockwise at an angle of 1  ×  45 °  ejπ 4
(
), then 3  ×  45 ° , 5  ×  45 ° , and 7  ×  45 ° . 
 FIGURE 9.5   Roots of a Butterworth ﬁ lter. For the general solution, we start at  π 2 and 
rotate at an angle of  θ
π
p
k
N
=
+
(
)(
)
2
1
2
 to obtain each solution. The solution repeats after 
every 2 N roots found. 
θp
θp
Stable
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
328 
CHAPTER 9 RECURSIVE FILTERS
When we come to 9  ×  45 ° , we effectively end up back at 1  ×  45 ° . The stable poles 
(the two with a negative real part) are at 
 s
e
e
=
⎧
⎨⎪
⎩⎪
×
×
1
1
3 4
5 4
j
j
π
π .  
 So, the values of  s which satisfy the design equation are
 s = −
±
1
2
1
2
j
.  
 Taking product of the ( s  −  pole) terms, we have the transfer function
 G s
s
s
( ) =
+
+
1
2
1
2
.  
 In the above, the ( − s 2 ) 2 canceled the negative sign because  N was even. For a more 
general solution ( N odd or even), we again ﬁ nd poles of  G s( )
2 without making any 
assumptions about  N :
 
 
1
0
2
2
2
2
+ −(
)
=
−(
)
=
−
=
= −
(
)
(
)
s
s
e
s
e
s
e
N
N
N
j
j
j
odd number
odd number
π
π
odd number
odd number
odd number
(
)
(
)
(
)
=
=
π
π
π
π
N
N
N
s
e
s
e
e
j
j
j
j
2
2
2
s
e
e
k
k
N
=
+
(
)
j
j
π
π
2
2
1 2
: integer
 
 (9.14) 
 Using Figure  9.5 again, we start at an angle of  π 2 (90 ° ) and travel anticlockwise 
by an odd multiple of  π 2N. 
 We have examined the procedure for ﬁ rst - and second - order systems, and 
it becomes a little tedious for higher orders. More importantly, when we come 
to Chebyshev ﬁ lters in the next section, an analytical solution as found in 
Equation  9.14 is much harder to derive. What we would like is a general solution 
for any order so that we may specify a signal ﬁ lter of any order. To develop such 
an algorithmic solution, we again use a normalized cutoff,  Ω c  =  1, and write the 
expression for the poles as
 1
0
2
+ −(
)
=
s
N
 
 1
1
0
2
+ −
(
)
=
N
N
s
.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.4 CONTINUOUS-TIME RECURSIVE FILTERS 
329
 We can solve this for a given order  N by ﬁ rst realizing that the above is actually
 
 −
(
)
+
+
+
+
=
−
−
1
0
0
1
0
2
2
1
2
2
0
N
N
N
N
s
s
s
s

. 
 (9.15) 
 The position in the coefﬁ cient array is determined by the power of  s , which starts 
at 2 N . The value of this coefﬁ cient is ( − 1) N , so we set up the coefﬁ cients and ﬁ nd 
the roots of the polynomial as follows: 
 % remember to set the order N 
 d  =  zeros (2  * N  + 1, 1); 
 d(2  * N  + 1 )  = 1; 
 d(1)  = ( − 1) ˆ (N); 
 dr  =  roots (d); 
 i  =  ﬁ nd ( real (dr)   <  0); 
 drs  = dr(i); 
 p  =  poly (drs);  
 After setting up the coefﬁ cient array with the correct coefﬁ cients of  s (with 
powers from 2 N to 0), we ﬁ nd the roots of the Butterworth polynomial (denominator 
of | G | 2 ) using the  roots () command. Recall that we have the product  G ( s ) G ( − s ), 
of which we only effectively want half of the terms. Obviously, we want a stable 
system, so we need the terms which deﬁ ne poles in the negative half of the  s plane. 
That is easily done by selecting those whose real part is negative. With those stable 
coefﬁ cients, all that remains is to expand the roots into a polynomial using  poly 
(). If we calculate all coefﬁ cient values for the denominator in this manner for 
 N  =  1 – 10, we obtain 
 N  =   1:   1   1 
 N  =   2:   1   1.41    1 
 N  =   3:   1   2.00    2.00     1 
 N  =   4:   1   2.61    3.41     2.61     1 
 N  =   5:   1   3.24    5.24     5.24     3.24     1 
 N  =   6:    1   3.86    7.46     9.14     7.46      3.86     1 
 N  =   7:   1   4.49    10.10    14.59    14.59    10.10    4.49     1 
 N  =   8:   1   5.13    13.14    21.85    25.69    21.85    13.14    5.13    1 
 N  =    9:   1    5.76    16.58    31.16    41.99    41.99    31.16    16.58    5.76     1 
 N  = 10:   1   6.39    20.43    42.80    64.88    74.23    64.88    42.80    20.43    6.39    1 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
330 
CHAPTER 9 RECURSIVE FILTERS
 d  = [1.00    2.61    3.41    2.61    1.00] 
 roots (d) 
 ans  =  
      − 0.3818  + 0.9242i 
      − 0.3818   −  0.9242i 
      − 0.9232  + 0.3844i 
      − 0.9232   −  0.3844i  
 conv ([1 1],[1 1 1]) 
 ans  = 
       1 2 2 1  
 For example, for  N  =  2, the Butterworth polynomial is  s 2  +  1.41 s  +  1. 
Mathematically, the Butterworth polynomials in general are written as
 
 
B s
s
B s
s
s
B s
s
s
s
1
2
2
3
3
2
1
2
1
2
2
1
( )
=
+
( )
=
+
+
( )
=
+
+
+


.  
 (9.16) 
 The previous table of results can be checked using any standard table of Butterworth 
polynomials. Note, however, that such tables usually show the coefﬁ cients in factor-
ized form, so it may be necessary to use the  conv () function to expand the factors 
to get the polynomial coefﬁ cients themselves. For example, the tabulated polynomial 
for  N  =  3 is usually ( s  +  1)( s 2  +  s  +  1). To expand this, we take the polynomial coef-
ﬁ cients of each factor and apply polynomial multiplication: 
 This speciﬁ es the polynomial coefﬁ cients, which agrees with the case above 
for  N  =  3. The coefﬁ cients are read from left to right, so that the rightmost term 
is the coefﬁ cient of  s 0 , then  s 1 and so forth up to the highest order on the left ( s 3 in 
this case). 
 Now we have a set of stable polynomials in terms of  s . For example, if we 
take the polynomial coefﬁ cients for  N  =  4 and ﬁ nd the poles, we have 
 Clearly, these all have negative real parts, and as such would be stable in an 
analog ( s - domain) transfer function. Note also that when we have complex poles, 
they always occur in conjugate pairs. This process is illustrated in Figure  9.6 , which 
shows the location of the calculated poles in the  s plane. Note that the poles lie in 
a circle, as has been derived algebraically (refer back to Equation  9.14 , which gave 
the pole locations on the complex plane). Real poles may occur by themselves, but 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.4 CONTINUOUS-TIME RECURSIVE FILTERS 
331
 FIGURE 9.6   Poles of a low - pass Butterworth ﬁ lter, as the order is increased. Note that 
all the poles are shown here, and these are actually the poles of  B ( s )  B ( − s ). For a stable 
ﬁ lter, we select the poles of  B ( s ) to correspond to the left - half stable poles. 
−2.0 −1.5
−1.0 −0.5
0
0.5
1.0
1.5
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Stable
unstable
s Plane
Real
Imaginary
−2.0 −1.5
−1.0 −0.5
0
0.5
1.0
1.5
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Stable
unstable
s Plane
Real
Imaginary
complex poles always occur in conjugate pairs. Finally, the poles are symmetric 
about the imaginary axis. Thus, the process of deriving  G ( s ) corresponds to selecting 
those poles in the left half of the  s plane in the diagram. 
 Now that we have the ﬁ lter polynomials, we can investigate the frequency 
response using the techniques developed in Section  9.3.3 . As with all analog systems, 
this is found by substituting  jΩ for  s , where  Ω is the desired frequency in radians 
per second. To convert to  “ real ” frequency in cycles per second (Hz), recall 
that  Ω  =  2 π f , where  f is in hertz. Figure  9.7 shows the responses of Butterworth 
low - pass ﬁ lters using this approach as the order is increased. Note that the 
 − 3  dB point is also shown. This is the frequency where the gain is  1
2, since 
the gain in decibels at this point is  20
1
2
3
10
log (
) ≈−dB. 
 9.4.2  Chebyshev Filter Structure 
 Chebychev 1 ﬁ lters also utilize a polynomial for approximating the ﬁ lter transfer 
function. They provide a sharper roll - off than Butterworth ﬁ lters, but at the expense 
of a ripple in the passband. 2 Chebyshev ﬁ lters are described by the transfer function 
 G ( s ) deﬁ ned as
 
 G s
K
T
s
N
c
( )
=
+
⎛
⎝⎜
⎞
⎠⎟
2
2
2
1
ε
Ω
. 
 (9.17) 
  1   Chebyshev also made many other contributions to mathematics, including the Chebyshev or  L  ∞  distance. 
There are various other spellings arising from the translation, such as Chebycheff. 
  2  We consider here so - called Chebyshev Type I ﬁ lters. Type II ﬁ lters have ripples in the stopband. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
332 
CHAPTER 9 RECURSIVE FILTERS
 The function  T N ( · ) is the  N th order Chebyshev polynomial. Before describing the 
Chebyshev polynomial, we note that the poles of | G ( s )| 2 can be described in a similar 
fashion to those of the Butterworth structure. The poles of the function | G ( s )| 2 are 
the solutions of
 
 1
0
2
2
+
⎛
⎝⎜
⎞
⎠⎟=
ε T
s
N
c
Ω
. 
 (9.18) 
 The parameter  ε controls the shape of the passband — this will be explained shortly. 
Letting for now  ε  =  1 and once again setting the cutoff frequency  Ω c  =  1, we have 
poles at
 
 1
0
2
+
( ) =
T
s
N
.  
 (9.19) 
 It is reasonable to ask at this stage what happens when we have  Ω c not equal to 
one, and when  ε is not equal to one. These questions will be addressed in due 
course. For now, it is sufﬁ cient to know that  Ω c scales the frequency response (i.e., 
instead of a cutoff frequency of 1  rad/s, we have  Ω c rad/s), and second, that  ε has 
an effect on the ripple in the passband. These issues are best addressed after the 
Chebyshev polynomials are explained. Furthermore, the additional questions of 
 FIGURE 9.7   Butterworth continuous - time low - pass ﬁ lter responses. The ﬁ lter is 
normalized to a cutoff frequency of  Ω c  =  1. As we increase the order of the ﬁ lter, the cutoff 
grows sharper. A characteristic of Butterworth ﬁ lters is that the passband is ﬂ at (no ripples) 
over the passband of zero to  Ω c . 
0.1
1.0
10
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
N = 1
N = 2
N = 3
N = 4
N = 10
Gain (dB)
Frequency (rad/s)
Butterworth continuous-time filter responses
−3dB point
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.4 CONTINUOUS-TIME RECURSIVE FILTERS 
333
how to convert the ﬁ lter if we do not want a simple low - pass ﬁ lter, and how to 
convert from an analog ﬁ lter into a discrete - time digital ﬁ lter, will be expanded 
upon afterward. 
 So for now, we need to deﬁ ne the Chebyshev polynomial. There are many 
ways to approach this, and perhaps the simplest is via the recursive deﬁ nition
 
 
T
x
T x
x
T
x
xT
x
T
x
N
N
N
0
1
1
2
1
2
( ) =
( ) =
( ) =
( )−
( )
−
−
.
 
 (9.20) 
 The ﬁ rst two lines initialize the recursion, and the following line is applicable for 
all subsequent values of  N . We can thus apply this recursion to obtain the polynomi-
als as follows:
 
 
N
T
x
x
N
T x
x
x
x
x
x
N
T
x
x
x
=
→
( ) =
−
=
→
( ) =
−
(
)−
=
−
=
→
( ) =
2
2
1
3
2
2
1
4
3
4
2
4
2
2
3
2
3
4
3 −
(
)−
−
(
)
=
−
+
=
→
( ) =
−
+
3
2
1
8
8
1
5
16
20
5
2
4
2
5
5
3
x
x
x
x
N
T x
x
x
x.
 
 (9.21) 
 Figure  9.8 shows the shape of some low - order Chebyshev polynomials. There are a 
couple of points to note regarding the nature of the polynomials thus generated. First, 
the order of the polynomial equals  N . For example, for  N  =  4, we have a polynomial 
starting with  x 4 . For odd values of  N , the polynomial has no constant term, and for 
even values of  N , we have alternating  + 1 and  − 1 constant terms in each polynomial. 
These observations will be useful shortly, when we need to ﬁ nd the poles of the 
transfer function (and thus the shape of the ﬁ lter ’ s frequency response). 
 FIGURE 9.8   Some Chebyshev polynomials  T N ( x ) as deﬁ ned by the recursion in Equation 
 9.20 . Because of the use of normalized frequencies  s
c
Ω  in the ﬁ lter design equation, and 
the fact that  Ω  ≥  0, the portion of interest corresponds to the range  x  =  0 to 1 above. 
−1.0
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1.0
−1.0
−0.5
0
0.5
1.0
N = 2
N = 3
N = 4
N = 5
Chebyshev polynomials
x
TN(x)
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
334 
CHAPTER 9 RECURSIVE FILTERS
 The derivation of the polynomial coefﬁ cients is tedious, and it would be 
helpful to have an automated solution. We can do this by implementing the recursion 
deﬁ ned by Equation  9.20 . The multiplication 2 xT N  − 1 ( x ) at each stage is just a poly-
nomial multiplication, since we can write it as 2(1 x 1  +  0 x 0 ) T N  − 1 ( x ). Since polynomial 
multiplication can be implemented as the convolution operation, we can use the 
 conv () operator in MATLAB. If we have the recursion polynomial coefﬁ cients of 
the current stage in vector tc, then this multiplication for the subsequent stage 
can be calculated using 2 * conv ([1 0], tc), from which the  T N  − 2 ( x ) coefﬁ cients 
are subtracted (refer to Equation  9.20 ). 
 All the polynomial coefﬁ cients up to order  M may be calculated in a matrix 
of coefﬁ cients  T as shown next, with each row being a vector of coefﬁ cients. The 
recursion as deﬁ ned in Equation  9.20 then yields all polynomial coefﬁ cients. 
 M  = 10; 
 T  =  zeros (M  + 1, M  + 1); 
 T(1, M  + 1)  = 1; 
 T(2, M)  = 1; 
 for n  = 2:M 
      t  = 2 * conv ([1 0], T(n, :)); 
      t  = t(2:M  + 1  + 1); 
      T(n  + 1, :)  = t   −  T(n   −  1, :); 
 end 
 N  = 2 :    2     0  − 1 
 N  = 3 :    4     0  − 3        0 
 N  = 4 :    8     0  − 8        0 1 
 N  = 5 :    16     0  − 20      0 5       0 
 N  = 6 :    32     0  − 48      0 18      0  − 1 
 N  =  7 :    64     0  − 112    0 56      0  − 7     0 
 N  = 8 :    128 0  − 256     0 160     0  − 32     0 1 
 N  = 9 :    256 0  − 576     0 432     0  − 120    0 9   0 
 N  = 1 0: 512 0  − 1280    0 1120   0  − 400   0 50 0  − 1 
 The output of the above (edited for clarity) is the T matrix: 
 The ﬁ rst column is the value of  N , and the values in each row are the coefﬁ -
cients of  x  n in order of decreasing powers of  x . For example, the second - last line 
with the zero - valued coefﬁ cients removed is
 
 T
x
x
x
x
x
x
9
9
7
5
3
256
576
432
120
9
( ) =
−
+
−
+
.  
 (9.22) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.4 CONTINUOUS-TIME RECURSIVE FILTERS 
335
 We can select the coefﬁ cients for order  N from  T N using tc  =  T(N   +  1, 
M  +  1  −  N : M   +  1);. 
 Once we have the Chebyshev polynomials, we need incorporate them into a 
frequency domain speciﬁ cation. The discussion above created polynomials of the 
form  T N ( x ), but we actually need a ﬁ lter  G ( s ) deﬁ ned by
 
 G s
K
T
s
N
c
( )
=
+
⎛
⎝⎜
⎞
⎠⎟
2
2
2
1
ε
Ω
,  
 (9.23) 
where  K is a gain factor. So, the remaining problems are the expansion of 
 TN
2 ⋅( ), the square root operation, and the selection of the poles to use. These are all 
best considered together. The selection of the stable poles is not unlike the procedure 
used for Butterworth polynomials. To motivate the approach, consider the case of 
order  N  =  2 and  ε  =  1. The poles of the above equation will be deﬁ ned by the 
denominator of Equation  9.17 to become
 
 1
2
1
0
2
2
2
+
−
(
) =
ε
s
.  
 (9.24) 
 For the purposes of explanation, we will use  ε  =  1 for now, yielding
 
 4
4
2
0
4
2
s
s
−
+
= .  
 (9.25) 
 This gives us the denominator polynomial in terms of  s . Since we have the magnitude -
 squared function, we can use the same approach as with the Butterworth ﬁ lter case. 
We need to replace  s 2  →  ( s )( − s )  =  − s 2 . 
 The polynomial then becomes
 
 4
4
2
0
4
2
s
s
+
+
= .  
 (9.26) 
 So now we take the polynomial coefﬁ cients, ﬁ nd the stable roots, and expand to 
obtain 
 d2  = [4    0   4   0    2]; 
 d   =  roots (d2) 
 d   = 
      − 0.3218  + 0.7769i 
      − 0.3218    −  0.7769i 
      0.3218  + 0.7769i 
      0.3218   −  0.7769i 
 i  =  ﬁ nd ( real (d)   <  0); 
 as  =  poly (d (i)) 
 as  = 
     1.0000      0.6436      0.7071  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
336 
CHAPTER 9 RECURSIVE FILTERS
 Figure  9.9 shows the poles of Chebyshev ﬁ lters of various orders. Note 
that, like the Butterworth case, we are calculating the magnitude squared, and hence 
have twice as many poles plotted as we have in the transfer function. To deﬁ ne 
the transfer function, we select the stable poles (those in the left half of the  s plane) 
and expand out as a polynomial. Whereas the Butterworth poles formed a circle, 
the Chebyshev poles formed an ellipse with the major axis being the imaginary 
(vertical) axis. 
 To investigate the general solution, consider again the case of
 
T s
s
T
s
s
s
s
s
2
2
2
2
2
2
4
2
2
1
2
1 2
1
4
4
1
( ) =
−
∴
( ) =
−
(
)
−
(
)
=
−
+ .
 
 So, replacing  s 2  →  − s 2 ,
 
 4
4
2
0
2
2
1
s
s
s
s
n
n
( ) −
(
)
(
) −
( ) −
(
)
(
)+
=
=
=
 



 



.  
 (9.27) 
 Inspecting the above, it is evident that every  s 2 n  =  ( s 2 ) n term is replaced by  s 2 n ( − 1) n . 
If we have the Chebyshev coefﬁ cients calculated as explained earlier, then we can 
ﬁ nd the poles using the following: 
 FIGURE 9.9   Poles of a low - pass Chebyshev ﬁ lter, as the order is increased. Note that all 
the poles are shown here, and these are actually the poles of  G ( s ) G ( − s ). For a stable ﬁ lter, 
we select the poles of  G ( s ) to correspond to the left - half stable poles. 
−2.0 −1.5
−1.0 −0.5
0
0.5
1.0
1.5
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Stable
unstable
s Plane
Real
Imaginary
−2.0 −1.5
−1.0 −0.5
0
0.5
1.0
1.5
2.0
−2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
2.0
Stable
unstable
s Plane
Real
Imaginary
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.4 CONTINUOUS-TIME RECURSIVE FILTERS 
337
 The gain  K of the Chebyshev ﬁ lter is clearly dependent upon the roots of the 
denominator. Since the denominator will be factored into terms of the form ( s  −  p i ) 
for poles  p i , then the product when  s  =  0 (i.e., when  s = jΩ with  Ω  =  0) will be the 
product of all the  p i factors (note that if  p i is complex, then there will be a corre-
sponding term  (
*)
s
pi
−
 with complex conjugate pole, and the product will still be 
real). As will be seen when we plot the response curves, for an odd ﬁ lter order, the 
gain is unity at zero frequency; for an even order, the gain is  1
1
2
+ ε . Putting this 
together, the gain is
 
 c
p
N
p
N
i
i
=
−
+
⎧
⎨⎪
⎩⎪
∏
∏
:
:
.
odd
even
1
1
2ε
 
 (9.28) 
 So how does the ﬁ lter perform in practice — what is the actual frequency response 
like? Figure  9.10 shows the responses, again for low - pass ﬁ lters of various orders. 
Compared with to the Butterworth responses derived previously, the Chebyshev 
 tc  = tc(:)    %  polynomial coefﬁ cients for order N 
 %  square, multiply by epsilon squared, add one (will be
% 2N  + 1 coefﬁ cients) 
 d  =  conv (tc, tc); 
 d  = d * epsilon ˆ 2; 
 d(2 * N  + 1)  = d(2 * N  + 1)  + 1; 
 %  powers for s 
 spow  = [2 * N:  − 1:0] ’ ; 
 %  (s)( − s) terms 
 %  s ˆ 2  − >  − s.s  =  − s ˆ 2 
 %  s ˆ 4  − > ( − s.s)( − s.s)  = s ˆ 4 
 %  s ˆ 6  − > ( − s.s)( − s.s)( − s.s)  =  − s ˆ 6 
 %  spow/2 will always be an integer, since 
 %  we have T() ˆ 2 where T is the chebyshev poly 
 snew  = ( − 1) * ones(2 * N  + 1, 1); 
 svals  = snew. ˆ (spow/2); 
 %  ﬁ nally, the denominator coefﬁ cients 
 %  this is the entire polynomial for |G| ˆ 2, 
 %  so we take the stable (left   −  half s plane) roots 
 d2  = svals. * d; 
 %  ﬁ nd roots of chebyshev |G| ˆ 2, 
 %  then stable ones (real part negative) 
 d  =  roots (d2); 
 i  =  ﬁ nd ( real (d))   <  0); 
 %  expand to polynomial using only stable roots 
 as  =  poly (d(i));  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
338 
CHAPTER 9 RECURSIVE FILTERS
responses are sharper (as will be seen by comparing the responses for the same 
order). 
 The faster roll - off rate of the Chebyshev ﬁ lter comes at a price: the ripple in 
the passband. 3 This is an appropriate time to revisit the parameter  ε , which we previ-
ously had set to unity, since it controls the amount of ripple in the passband. This 
may be seen by looking at the Chebyshev ﬁ lter equation, where  ε is multiplied 
by the polynomial coefﬁ cients, which in effect means multiplication by the shape 
illustrated in Figure  9.8 . A larger  ε in effect means a larger contribution from the 
polynomial evaluation, and this may be seen by looking at the ﬁ lter responses in 
Figure  9.10 . 
 We can quantify the amount of ripple in the passband, and the gain magnitude 
at the cutoff frequency, as follows. First, note that in the Chebyshev polynomials, 
 T N (1)  =  1 for all  N . Thus, the magnitude at the cutoff where  s
c
=
=
jΩ
1 will be
 
 G
2
2
1
1
=
+ ε .  
 (9.29) 
 FIGURE 9.10   Chebyshev continuous - time low - pass ﬁ lter responses. Clearly, the 
higher - order ﬁ lter results in a sharper response, but the disadvantage is that more terms are 
required in the ﬁ lter polynomial, hence greater complexity in order to realize the ﬁ lter in 
practice.  
0.1
1.0
10.0
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
N = 1
N = 2
N = 3
N = 4
N = 10
Gain (dB)
Frequency (rad/s)
Chebyshev continuous-time filter responses
−3 dB point for ε =1 
  3  For Type II ﬁ lters, the ripple is in the stopband. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.5 COMPARING CONTINUOUS-TIME FILTERS 
339
 If we deﬁ ne the passband ripple to be  δ , then the gain at the cutoff is 1  −  δ . Thus, 
we must have
 
 
1
1
1
1
1
1
2
2
−
=
+
∴
= −
+
δ
ε
δ
ε
.
 
 (9.30) 
 Thus, for  ε  =  1, the gain at the cutoff is  1
2
0 707
≈.
, or  − 3  dB. For  ε  =  0.5, the gain 
is 1  −  δ  ≈  0.9 and the ripple is  δ  ≈  0.1. 
 9.5  COMPARING CONTINUOUS - TIME FILTERS 
 The preceding sections introduced some analog ﬁ lter types and their characteristics 
and their design. There are a great many analog ﬁ lter design approaches; we have 
only introduced the most widely used methods. It is, however, worth comparing 
what we have studied: How are the responses different and why would one choose 
one design over another? 
 It would be expected that a higher order for a ﬁ lter would result in a sharper 
transition, and this is shown for the Chebyshev ﬁ lter in Figure  9.11 . For Chebyshev 
ﬁ lters, a higher order has the side effect of increasing the number of ripples in the 
passband — in any given application, this may be tolerable, or it may be totally unac-
ceptable. If such ripples in the passband are undesirable, we may need to turn to a 
Butterworth design. In Chebyshev ﬁ lters, the passband ripple is controlled by the 
 FIGURE 9.11   Comparison of the effect of the ripple parameter  ε on the response of a 
Chebyshev ﬁ lter (a), and (b) the effect of the ﬁ lter order  N . For  ε  =  1, the gain varies 
between  1
2
0 707
≈.
 and unity, for a ripple of 3  dB. For  ε  =  0.5, the gain varies between 
approximately 0.9 and unity, for a ripple of around 1  dB. Note that the vertical gain scale is 
linear rather than logarithmic (dB), in order to highlight the shape of the response.  
0.01
0.1
1.0
10.0
0
0.2
0.4
0.6
0.8
1.0
Frequency (rad/s)
Gain (linear)
Chebyshev filter comparison, order N = 4
ε = 1
ε = 0.5
0.01
0.1
1.0
10.0
0
0.2
0.4
0.6
0.8
1.0
Frequency (rad/s)
Gain (linear)
Chebyshev filter comparison, ε = 0.5
N = 7
N = 10
(a) Fixed order N = 4, varying ε
(b) Fixed ε = 0.5, varying N
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
340 
CHAPTER 9 RECURSIVE FILTERS
parameter  ε . Figure  9.11 shows the effect of changing this parameter. The tradeoff 
is self - evident: A lower ripple may be obtained at the expense of a less sharp transi-
tion at the cutoff frequency. 
 Figure  9.12 compares the responses of Butterworth and Chebyshev design 
approaches. It is seen that for the same order, the Chebyshev has a sharper roll - off, 
which is usually a desirable characteristic. However, that comes at the expense of 
the passband ripple, which is inherent in the Chebyshev designs studied. 
 There are many other types of continuous - time ﬁ lters. The following sections 
describe methods for converting from a continuous ﬁ lter (function of  s ) into a dis-
crete ﬁ lter (function of  z ). These conversions are applicable to all types of  s - domain 
ﬁ lters. 
 9.6  CONVERTING CONTINUOUS - TIME 
FILTERS TO DISCRETE FILTERS 
 Once we have an analog ﬁ lter prototype, we can convert it into a digital (discrete) 
ﬁ lter and thus employ a difference equation to implement it. The following sections 
examine two commonly used approaches for the  s to  z conversion. 
 9.6.1  Impulse Invariant Method 
 The  impulse invariant method  is one of two commonly used approaches for convert-
ing a continuous - time function of  s into a discrete - time function of  z . The basic idea 
is, as the name suggests, to produce a discrete transfer function that has the same 
impulse response as the analog transfer function prototype. 
 FIGURE 9.12   Comparison of ﬁ lter designs, using the Butterworth and Chebyshev 
polynomials. Clearly the Chebyshev ﬁ lter has a sharper transition band, but has ripples in 
the passband. Note that the vertical gain scale is linear rather than logarithmic (dB), in 
order to highlight the shape of the response. 
0.01
0.1
1.0
10.0
0
0.2
0.4
0.6
0.8
1.0
Frequency (rad/s)
Gain (linear)
Butterworth−chebyshev filter comparison, order N = 10
Chebyshev
Butterworth
Chebyshev
Butterworth
0.01
0.1
1.0
10.0
0
0.2
0.4
0.6
0.8
1.0
Frequency (rad/s)
Gain (linear)
Butterworth−chebyshev filter comparison, order N = 4
(a) Order N =10 and ε = 0.25 for Chebyshev ﬁlter
(b) Order N = 4 and ε = 0.25 for Chebyshev ﬁlter
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
341
 The method works as follows. First, we decompose the continuous - time  s -
 domain function into ﬁ rst - order sections using partial fractions. For each, we then 
write the  z transform equivalent. It is necessary to scale by the sample period  T to 
keep the zero - frequency gain the same. Mathematically, this means that if we set 
the discrete impulse response  h d ( n ) equal to the scaled continuous response  h ( nT  ),
 h
n
T h nT
d ( ) =
(
)  
 then the sampled version  y d ( n ) will approximate the true system  y ( t ) at the sampling 
instants (when  t  =  nT ); that is,
 y
n
y nT
d ( ) = (
).  
 Each ﬁ rst - order section is converted from the discrete domain to the digital domain 
using the mapping
 
 1
s
p
T
z
z
e pT
+
→
−
⎛
⎝⎜
⎞
⎠⎟
−
;  
 (9.31) 
 that is, a general ﬁ rst - order section with a single pole at  s  =  − p is replaced with the 
corresponding discrete - time approximation. As well as the parameter  p , we need to 
specify the sample period  T (recall that this is the reciprocal of the sampling fre-
quency  f s ). In principle,  any transfer function can be decomposed into the sum of 
such ﬁ rst - order sections. The mapping is applied in turn to each term, when the  s -
 domain function is expressed as a sum of such ﬁ rst - order terms. Finally, the indi-
vidual ﬁ rst - order terms in  z can be algebraically recombined to form a single transfer 
function (numerator and denominator polynomials). To give an illustrative example, 
consider an integrator:
 
 H s
s
( ) = 1. 
 (9.32) 
 In this particular case,  p  =  0 in Equation  9.31 . The discrete version becomes
 
 H
z
T
z
z
d ( ) =
−
⎛
⎝⎜
⎞
⎠⎟
1 .  
 (9.33) 
 So, the difference equation is
 
 y n
x n T
y n
( ) =
( )
+
−
(
)1 .  
 (9.34) 
 This is a rectangular approximation to the integrator — in words, we could express 
an integrator as
 
y n
y n
x n T
( )
=
−
(
)
+
( )
=
+
1
New area
area so far
increment of area. 
 The frequency response is
 
 H
e
e
e
T
d
j
j
j
ω
ω
ω
(
) =
−
⎛
⎝⎜
⎞
⎠⎟
1
.  
 (9.35) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
342 
CHAPTER 9 RECURSIVE FILTERS
 We can generalize this approach to higher orders. Suppose we have a second - 
order system which is decomposed into a cascade of two ﬁ rst - order systems 
given by
 
 
H s
s
s
s
s
( ) =
+
(
)
+
(
)
=
+ + −
+
1
1
2
1
1
1
2 .
 
 (9.36) 
 The discrete approximation is
 
 H
z
z
z
e
z
z
e
T
d
T
T
( ) =
−
+
−
−
⎛
⎝⎜
⎞
⎠⎟
−
−2
.  
 (9.37) 
 The major problem with impulse invariant approximation is aliasing. Figure  9.13 a 
shows the effect of the mirror - image response centered on  f s . The response in the 
center region is the summation of the two (the lower  “ real ” response and the  “ folded ” 
version). It is clear that in order to minimize this problem, a high sample rate has 
to be chosen. 
 The speciﬁ c case shown in Figure  9.13 b is a comparison of ﬁ lters with 
the same analog transfer function with poles at  s = −
±
1
2
1
2, but differing 
sampling frequencies for the digital versions. For the case of a 2 - Hz sampling 
rate, the mirrored frequency response about  f s produces a tail which interferes 
with the response from 0 to  fs 2. Clearly, this introduces a certain amount of 
aliasing in the frequency response. The higher sampling rate (10   Hz) as shown in 
Figure  9.13 b moves the mirrored response further out, and thus the overlap is 
negligible. 
 9.6.2  Corrected Impulse Invariant Method 
 The usual method as described above does not take into account the discontinuity 
in the impulse response at  t  =  0. In 2000, it was independently shown (Jackson 2000; 
Mecklenbra ü ker 2000) that the correct form is slightly different to what is  “ tradition-
ally ” given. The ﬁ rst - order section correspondence is normally deﬁ ned as
 
 K
s
p
KT
z
z
e pT
+
→
−
⎛
⎝⎜
⎞
⎠⎟
−
.  
 (9.38) 
 However, the ﬁ rst - order sections ought to be converted using
 
 K
s
p
K T
z
e
z
e
pT
pT
+
→
+
−
⎛
⎝⎜
⎞
⎠⎟
−
−
2
.  
 (9.39) 
 What is the difference between these approaches? In practice, it is relatively small 
but worth examining. First, at DC (or zero frequency), we have  ω  =  0  rad per sample. 
As usual,  z
e
=
jω; hence,  z  =  1 at a frequency of zero. Further, we see many terms 
of the form  e  −  pT in both of the above expressions, where  p is the pole in the 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
343
 FIGURE 9.13   Aliasing in impulse invariant ﬁ lters. The diagram at the top shows that the 
response from zero to  fs 2 is mirrored at  f s going back to zero. The observed response is 
the sum of these and hence is not entirely correct. The worst - case error occurs in the 
middle, at  fs 2. One way to minimize this unwanted side effect is to increase the sampling 
frequency  f s so that the second  “ aliased ” response is shifted higher in frequency (further to 
the right).  
fs
fs/2
(a) Illustrating the problem of aliasing
0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
0
0.2
0.4
0.6
0.8
1.0
Sampled frequency response showing aliasing fs = 2 Hz
Frequency (Hz)
Gain
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1.0
Sampled frequency response showing aliasing fs = 10 Hz
Frequency (Hz)
Gain
(b) Aliasing as it occurs for the same discrete transfer function, as the sample rate is increased
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
344 
CHAPTER 9 RECURSIVE FILTERS
continuous - time transfer function  G ( s ). Now observe that (as deﬁ ned previously)  p 
will be a positive number (since, for stability, the  s poles must be negative). Using 
the expansion for
 e
x
x
x
x = +
+
+
+
1
2
3
2
3
!
!
,

we have
 
 e
pT
pT
pT
pT
−
= −
+ −
(
) +
≈−
1
2
1
2

,
 
 (9.40) 
where the last approximation is valid since  f s is generally large, and hence  T is small, 
compared to the pole  p . Thus, the original impulse invariant approximation (omitting 
the scaling constant  K for clarity) is
 
 
T
z
z
e
T
pT
T
pT
p
pT
−
⎛
⎝⎜
⎞
⎠⎟≈
−
−
(
)
=
=
−
1
1
1 ,
 
 (9.41) 
whereas the revised approximation is
 
 
T
z
e
z
e
T
pT
pT
T
pT
pT
p
T
pT
pT
2
2
1
1
1
1
2
2
1
+
−
⎛
⎝⎜
⎞
⎠⎟≈
+
−
(
)
(
)
−
−
(
))
=
−
(
)
=
−
−
−
2 .
 
 (9.42) 
 Thus, the difference is proportional to  T , and hence is generally small in practice. 
Furthermore, the difference only applies where the system response is discontinu-
ous at  t  =  0. Thus, the theoretical difference does  not apply to the second - order case 
shown in Figure  9.14 , which has a second - order prototype of  1
2
1
2s
s
+
+
(
). 
Figure  9.15 shows a case where the methods do in fact give differing results. 
 9.6.3  Automating the Impulse Invariant Method 
 In manipulating the algebra required for the impulse invariant transform, we ﬁ rst 
need to convert the given transfer function into a sum of fractions, for which we 
need partial fractions. As an example, suppose we have
 
 
1
1
2
s
s
+
(
)
+
(
).  
 (9.43) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
345
 FIGURE 9.14   A practical demonstration of the selection of sample frequency and its 
effect on aliasing in impulse invariant ﬁ lters. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
1.2
Frequency (Hz)
Gain
Impulse invariant transformation of second−order prototype
fs = 2 Hz
fs = 10 Hz
 FIGURE 9.15   Original and corrected impulse invariant ﬁ lter responses. The responses 
are shown in the sampled - frequency domain, with the lower panel showing a magniﬁ ed 
view over the lower frequency range. 
0
0.2
0.4
0.6
0.8
1.0
Gain
Standard and corrected impulse invariant method
0
π/2
π
Standard
Corrected
0
0.2
0.4
0.6
0.8
1.0
Frequency (radian per sample)
Gain
Standard and corrected impulse invariant method (detail)
0
π/8
Standard
Corrected
 Then the corresponding sum of fractions is
 
 
1
1
2
1
2
s
s
A
s
B
s
+
(
)
+
(
) =
+ +
+
.  
 (9.44) 
 We can expand the denominator as either a set of coefﬁ cients (using  conv ()) or 
roots (using  poly ()) to obtain the denominator polynomial. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
346 
CHAPTER 9 RECURSIVE FILTERS
 This is now in a form suitable for converting the polynomial ratio to partial 
fractions using the  residue () function, noting that the numerator is unity 
(bs  =  1). 
 [r p k]  =  residue (bs, as); 
 N  =  length (p);  
 az  = [1]; 
 for n  = 1:N 
      az  =  conv (az, [1    − exp (p(n) * T)]); 
 end 
 as  =  conv ([1 1], [1 2]) 
 as  = 
     1       3       2 
 as  =  poly ([ − 1  − 2]) 
 as  = 
     1       3       2  
 Calling  residue () gives us the poles and constant gain factor of each partial 
fraction term in p and r, respectively. In the general case, we wish to recombine all 
the terms into one transfer function, as a ratio of polynomials. To understand the 
approach, consider the following example. Suppose we have
 
 
1
1
2
3
1
2
3
s
s
s
A
s
B
s
C
s
+
(
)
+
(
)
+
(
) =
+ +
+
+
+
.  
 (9.45) 
 This can be solved using partial fractions to yield  A = 1
2
,  B  =  − 1, and  C = 1
2
. 
So, the recombination problem becomes
 
 A
s
B
s
C
s
A s
s
B s
s
C s
s
s
s
+ +
+
+
+
=
+
(
)
+
(
)+
+
(
)
+
(
)+
+
(
)
+
(
)
+
(
)
+
1
2
3
2
3
1
3
1
2
1
2
(
)
+
(
)
s
3
.   (9.46) 
 The denominator becomes the multiplication of each term, while the numerator 
becomes the multiplication of the numerator of each term in turn by the  other 
denominator terms. Noting that multiplication of the simple polynomial terms can 
be accomplished by the convolution operator  conv (), we can generalize the 
operation to the speciﬁ c case of the impulse invariant problem as deﬁ ned in 
Equation  9.31 . 
 The denominator of Equation  9.31 can be calculated as the expansion of all 
the individual terms:  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
347
 The numerator of Equation  9.31 may be calculated in a similar way. However, 
one must remember that all terms are multiplied out and added together,  except the 
one corresponding to the current factor, as shown in the nested loops below: 
 bz  =  zeros (N  + 1, 1);
 for n  = 1:N 
      t  = [r( n )    0]; 
      %  loop across other terms 
      for k  = 1:N 
           if (k  
 ˜   = n) 
               t  =  conv (t, [1   − exp (p(k) * T)]); 
           end 
      end 
      bz  = bz  + t(:); 
 end 
 bz  = bz(:) ’ ; 
 zgain  = T;  
 For the corrected impulse invariant method, the appropriate modiﬁ cations can 
be easily made to the above. Let us apply some test cases. The manual algebraic 
approach should of course agree with the MATLAB code as given above. For the 
ﬁ rst case, consider a very simple ﬁ rst - order ﬁ lter:
 
 G s
s
( ) =
+
1
1.  
 (9.47) 
 Using the standard method, this yields directly
 
 G z
Tz
z
e T
( ) =
−
−.  
 (9.48) 
 Since by inspection  p  =  1, we know from linear systems theory that the time constant 
is 1 second. Thus, for simplicity, a much faster sample rate, say,  T  =  0.1 for the 
purpose of the example, is chosen to yield
 
 G z
z
z
( ) =
−
0 1
0 9048
.
.
.  
 (9.49) 
 Using the corrected approach, the lookup of the ﬁ rst - order section becomes
 G z
T
z
e
z
e
T
T
( ) =
+
−
⎛
⎝⎜
⎞
⎠⎟
−
−
2
.  
 With  T  =   0.1, we have
 G z
z
z
( ) =
+
−
⎛
⎝⎜
⎞
⎠⎟
0 05
0 9048
0 9048
.
.
.
.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
348 
CHAPTER 9 RECURSIVE FILTERS
 As a second example, consider a cascade of two ﬁ rst - order sections given by the 
partial fraction expansion
 
 
1
1
2
1
1
1
2
s
s
s
s
+
(
)
+
(
) =
+
(
) +
−
+
(
).  
 (9.50) 
 Using the value of  p for each ﬁ rst - order section, the direct lookup gives
 G z
Tz e
e
z
z e
e
e
T
T
T
T
T
( ) =
−
(
)
−
+
(
)+
−
−
−
−
−
2
2
2
3 .  
With  T  =  0.1, we have
 G z
z
z
z
( ) =
−
+
0 1
0 0861
1 7236
0 7408
2
.
.
.
.
.  
 The corrected method yields the same result in this case. 
 9.6.4  Bilinear Transform Method 
 An alternative to the impulse invariant method (IIM) as discussed above is the 
bilinear transform (usually called BLT). This approach aims to obtain a direct 
discrete - time approximation using reasoning similar to that outlined earlier for the 
approximation of continuous systems. 
 Ultimately, what we want is some function of  z which we can use as a substi-
tute for  s in the continuous transforms. The resulting transfer function in  z will 
always be an approximation, but certain characteristics are desirable. 
 To motivate the development of such a substitution, consider a 
differentiator,
 G s
s
( ) = .  
 For an input  X ( s ) and an output  Y ( s ), this represents the differential equation
 y t
dx t
dt
( ) =
( ).  
 An approximation to this is
 y nT
x nT
x nT
T
T
(
) ≈
(
)−
−
(
);  
 so, the discrete - time approximation is
 Y z
z
T
X z
( ) =
−
( )
−
1
1
.  
 The transformation from continuous to discrete is given by
 s
z
T
→
−
−
1
1
.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
349
 Using this method of  “ backward differences, ” the imaginary (frequency) axis in the 
 s plane is  not mapped into the unit circle in the  z plane. So, although it may be a 
reasonable approximation in the time domain, the frequency mapping is not desir-
able. Furthermore, we would like to ensure that a stable system in  s will be a stable 
system in  z when converted. 
 Another mapping, which is almost as simple, does in fact map the imaginary 
(frequency) axis in the  s plane onto the unit circle in the  z plane. It is called the 
bilinear transform (BLT) and is deﬁ ned as the substitution
 
 s
T
z
z
→
−
+
⎛
⎝⎜
⎞
⎠⎟
2
1
1 .  
 (9.51) 
 Why does the mapping have this form? To answer that question, consider an 
integrator,
 G s
s
( ) = 1.  
 Using the BLT, this becomes, after some algebraic manipulation,
 
 
G z
G s
T
z
z
Y z
z Y z
s T
z
z
( ) =
( )
=
+
−
⎛
⎝⎜
⎞
⎠⎟
∴( )−
( ) =
=
−
+
⎛
⎝⎜
⎞
⎠⎟
−
−
−
2
1
1
1
1
1
2
1
1
T X z
z
y n
T x n
x n
y n
2
1
2
1
1
1
( )
+
(
)
∴( ) =
( )+
−
(
)
(
)+
−
(
)
−
.
 
 (9.52) 
 This is a trapezoidal approximation to the integrator — in words,
 
y n
y n
x n
x n
T
( )
=
−
(
)
+
( )+
−
(
)
⎛
⎝⎜
⎞
⎠⎟
=
+
1
1
2
New area
area so far
incremeent of area.
 
 The increment of area is the trapezoidal approximation — half sum of parallel sides 
times perpendicular distance. This is illustrated in Figure  9.16 . The integrator above 
results in a transfer function: 
 G z
T
z
z
( ) =
+
−
⎛
⎝⎜
⎞
⎠⎟
2
1
1 ;  
 that is, a pole at  z  =  1 and a zero at  z  =  − 1. The zero means that the response at 
 ω  =  π is exactly zero. 
 In effect, the range of continuous frequencies  Ω from 0 to  ∞ is mapped into 
discrete frequencies  ω from 0 to  π . 
 Furthermore, poles in the stable section of the  s plane (the left half of the 
plane) are mapped into the inside of the unit circle in the  z plane (which, as always, 
is the stable region). This is a considerable advantage because a stable continuous 
system will map to a stable discrete system. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
350 
CHAPTER 9 RECURSIVE FILTERS
 To demonstrate the use of the BLT, consider a ﬁ rst - order lag:
 G s
s
( ) =
+
1
1.  
 Using the BLT, this becomes
 G z
T
T
z
z
T
T
( ) =
+
+
+
−
+
⎛
⎝
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
2
1
2
2
.  
 The frequency mapping may be analyzed as follows. The  s domain has  s = jΩ, and 
the  z domain has  z
e
=
jω. Using the BLT
 
 s
T
z
z
→
−
+
⎛
⎝⎜
⎞
⎠⎟
2
1
1 ,  
 (9.53) 
 the system in question becomes
 j
j
j
Ω =
−
+
⎛
⎝⎜
⎞
⎠⎟
2
T
e
e
ω
ω
1
1 .  
 This may be simpliﬁ ed to yield
 
 Ω = 2
2
T tan
.
ω  
 (9.54) 
 This equation relates the discrete and analog frequencies. For a small  ω , this reduces 
to the familiar  ω  =  Ω T , as shown in Figure  9.17 ; that is, a linear mapping from 
continuous frequencies (radian per second) to discrete frequencies (radians per 
sample). 
 FIGURE 9.16   The bilinear transform of an integrator. In effect, the discrete operation 
implements the trapezoidal rule for numerical integration. 
x(n − 2)
x(n − 1)
x(n)
T
T
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
351
 The frequency mapping from  Ω to  ω is illustrated in Figure  9.18 , resulting in 
a  “ warping ” of the frequency axis. The upper plot in Figure  9.18 a shows the fre-
quency response in terms of  “ real ” frequency (radians per second), with the corre-
sponding lower plot showing the response in the digital domain (radians per sample). 
Clearly, this mapping results in a  “ squashing ” of the desired frequency response into 
a much smaller range. The mapping process is shown in Figure  9.18 b. For low 
frequencies (relative to the sample rate), the mapping from continuous to discrete is 
approximately linear, whereas as the analog frequency becomes higher, it is mapped 
into  π radians per sample in the discrete domain. 
 Another way to look at this is to consider a system described by a simple 
ﬁ rst - order lag,
 FIGURE 9.17   The function  y  =  tan  θ , showing linearity for  
. 
y = tan θ
Angle θ
Angle θ
tan θ
∞
↑
π
8
π
4
3π
8
π
2
0.25
0.5
0.5
0.75
1.0
1.0
1.5
00
00
2
4
6
8
10
12
 FIGURE 9.18   Illustrating the mapping from the continuous or analog domain to the 
discrete - time domain. 
ω
π
Discrete ω (radian per sample)
∞
Continuous Ω (rad/s)
0
1
2
3
4
5
6
0
2
4
6
8
10
Frequency Ω Hz
Frequency ω (radian per sample)
Mapping analog to digital frequency
π
(a) Frequency responses
(b) Frequency mapping
Ω
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
352 
CHAPTER 9 RECURSIVE FILTERS
 G s
s
a
( ) =
+
1
.  
 This system has gain 1/ a and cutoff frequency  Ω c  =  a  rad/s. Using the BLT, the 
transformed system becomes
 
 
G z
T
z
z
a
z
T z
a z
z
z T
a
( ) =
−
+
⎛
⎝⎜
⎞
⎠⎟+
=
+
(
)
−
(
)+
−
(
)
=
+
+
⎛
⎝⎜
⎞
⎠⎟+
1
2
1
1
1
2
1
1
1
2
a
T
z
a
T
z
a
T
a
T
−
⎛
⎝⎜
⎞
⎠⎟
=
+
+
⎛
⎝⎜
⎞
⎠⎟
+
−
+
⎛
⎝
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
2
1
2
2
2
.
 
 (9.55) 
 Now going back and putting  z
e
=
jω ,
 
 
G e
e
e
a
T
a
T
e
e
a
T
a
T
j
j
j
j
j
ω
ω
ω
ω
ω
(
) =
+
+
⎛
⎝⎜
⎞
⎠⎟
−
⎛
⎝⎜
⎞
⎠⎟
=
+
+
⎛
⎝⎜
⎞
⎠⎟
−
1
2
2
1
2
2
⎛
⎝⎜
⎞
⎠⎟
=
+
⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝⎜
⎞
⎠⎟
+
−
⎛
⎝⎜
⎞
⎠
+
−
+
e
e
e
e
a
T
e
a
T
j
j
j
j
j
ω
ω
ω
ω
ω
2
2
2
2
2
2
2
⎟
⎛
⎝⎜
⎞
⎠⎟
=
+
=
+
=
−
e
a
T
a
T
a
j
j
j
j
ω
ω
ω
ω
ω
ω
2
2
2
2
2
2 2
2
1
2
2
2
1
cos
cos
sin
sin
cos
+ j 2
2
T tan
.
ω
 
 (9.56) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
353
 So the mapping of the cutoff frequency  Ω c into the digital domain may be written 
as  Ωc
T
→(
)
(
)
2
2
tan ω
. The conclusion is that we need to compensate for the 
nonlinear mapping in the frequency domain. We illustrate how this is done by 
considering a second - order system,
 
 G s
s
s
( ) =
+
+
1
2
1
2
ζ
,  
 (9.57) 
where  ζ is a constant of the system (called the damping ratio). 
 As we have done previously, we design for a system with a normalized cutoff 
frequency of 1  rad/s and then scale the ﬁ lter to the required cutoff frequency. This 
is easily accomplished by replacing  s with  s
c
Ω
. In the present case, this gives
 G s
s
s
c
c
( ) =
⎛
⎝⎜
⎞
⎠⎟+
+
1
2
1
2
Ω
Ω
ζ
. 
 Transforming to a digital ﬁ lter using the bilinear transform, we have
 
G z
T
z
z
T
z
z
z
c
c
c
c
( ) =
⎛
⎝⎜
⎞
⎠⎟
−
+
⎛
⎝⎜
⎞
⎠⎟+
−
+ +
=
+
(
)
Ω
Ω
Ω
Ω
2
2
2
2
2
2
2
1
1
2
2
1
1
1
2
ζ
T
z
T z
z
z
c
c
⎛
⎝⎜
⎞
⎠⎟
−
(
) +
−
(
)
+
(
)+
+
(
)
2
2
2
2
1
2
2
1
1
1
ζΩ
Ω
.
 
 Letting  α = 2 T, we have
 
G z
z
z
z
z
z
z
z
z
c
c
c
c
( ) =
+
+
(
)
−
+
(
)+
−
(
)+
+
+
(
)
=
Ω
Ω
Ω
Ω
2
2
2
2
2
2
2
2
2
1
2
1
2
1
2
1
α
ζ
α
2
2
2
2
2
2
2
2
2
1
2
2
2
2
+
+
(
)
+
+
(
)+
−
+
(
)+
−
+
(
)
z
z
z
c
c
c
c
c
α
ζα
α
α
ζα
Ω
Ω
Ω
Ω
Ω
.
 
 Letting  γ
α
ζα
=
+
+
(
)
2
2
2
Ω
Ω
c
c , to simplify,
 
 G z
z
z
z
z
c
c
c
( ) =
+
+
+
−
(
) +
−
+
(
)
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
⎟
Ω
Ω
Ω
Ω
2
2
2
2
2
2
2
1
2
γ
α
γ
α
ζα
γ
2
c
2
.  
 (9.58) 
 This is the resulting discrete - time transfer function corresponding to the analog 
prototype. While not incorrect, it will suffer from distortion or warping, as was 
illustrated in Figure  9.18 . The warping distortion gets worse the closer the frequency 
is to the Nyquist frequency. What are we to do about this? One possibility is to 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
354 
CHAPTER 9 RECURSIVE FILTERS
simply employ a higher sampling rate so that the effect of the warping due to alias-
ing is diminished. But it would be good to try to compensate for the nonlinear 
mapping of the BLT so that at least we can be sure the analog frequency response 
matches the digital frequency response at some critical frequency of interest (usually 
the cutoff frequency). 
 How do we do this? It depends on two pieces of information. First, we can 
scale a ﬁ lter to any frequency  Ω c by dividing  s by  Ω c (this is covered in detail in 
Section  9.7 ). Second, we want the responses to match at this  “ critical ” frequency, 
and we know the mathematical mapping of this critical frequency when we go from 
analog to discrete — it is in fact  2
2
T
c
(
)
(
)
tan ω
. So, we can combine the scaling 
and compensation (called  “ prewarping ” ) using
 
 s
T
z
z
T
c
c
Ω =
−
+
⎛
⎝⎜
⎞
⎠⎟
2
1
1
2
2
tan
.
ω
 
 (9.59) 
 Clearly,  T 2 cancels. To simplify the algebra, let  β
ω
=
(
)
1
2
tan
/
c
. Then, our modi-
ﬁ ed transfer function becomes
 
 
G z
z
z
z
z
z
z
( ) =
−
+
⎛
⎝⎜
⎞
⎠⎟+
−
+
⎛
⎝⎜
⎞
⎠⎟+
=
+
(
)
−
(
) +
1
1
1
2
1
1
1
1
1
2
2
2
2
2
2
β
ζβ
β
ζβ z
z
z
z
z
z
z
z
z
z
−
(
)
+
(
)+
+
(
)
=
+
(
)
−
+
(
)+
−
(
)+
+
+
(
)
=
1
1
1
1
2
1
2
1
2
1
2
2
2
2
2
2
β
ζβ
2
2
2
2
2
2
2
2
1
2
1
2
2
2
1
1
2
1
2
+
+
+
+
(
)+
−
+
(
)+
−
+
(
)
=
+
+
(
)
+
+
z
z
z
z
z
β
ζβ
β
β
ζβ
β
ζβ
1
2 1
2
1
2
1
2
1
2
2
2
2
2
z
z
+
−
(
)
+
+
⎛
⎝⎜
⎞
⎠⎟+
−
+
+
+
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
β
β
ζβ
β
ζβ
β
ζβ
⎟
⎟
⎟
⎟
.
 
 (9.60) 
 Let  γ  =  ( β 2  +  2 ζ β  +  1),
 
 G z
z
z
z
z
( ) =
+
+
+
−
(
)
⎛
⎝⎜
⎞
⎠⎟+
−
+
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝
⎜
⎜
⎜
⎜
⎞
⎠
⎟
⎟
1
2
1
2 1
2
1
2
2
2
2
γ
β
γ
β
ζβ
γ
⎟
⎟
.  
 (9.61) 
 To illustrate the difference the prewarping may produce in practice, consider a ﬁ rst -
 order ﬁ lter with a required cutoff of 2.2  kHz. Figure  9.19 shows the prototype analog 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
355
 FIGURE 9.19   The bilinear transform (BLT) ﬁ lter for a normalized cutoff of 1  rad/s (a). 
This is also called the prototype ﬁ lter. The response scaled for the desired cutoff of 
2,200  Hz is shown in (b). 
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1.0
Unscaled frequency response
Frequency (rad/s)
Gain (linear)
0
2
4
6
8
10
12
14
16
18
20
0
0.2
0.4
0.6
0.8
1.0
Scaled frequency response fc = 2,200 Hz
Frequency (kHz)
Gain (linear)
(a) Normalized cutoff 1 rad/s
(b) Scaled to cutoff frequency 2.2 kHz
ﬁ lter, with a cutoff of 1  rad/s, which is then scaled for the required cutoff frequency 
(note that the gain at the cutoff is  1
2
/
). 
 Next, we must convert the design into a digital ﬁ lter and thus must choose the 
sample frequency. This is where the effect of the nonlinear mapping of the BLT 
becomes apparent. In order to choose the sampling rate, the cutoff of 2.2  kHz would 
correspond to a Nyquist rate of 2  ×  2.2  kHz, so we choose a rate above that. The 
next highest rate commonly available in practical systems is 8  kHz. So, with 
 f s  =  8,000  Hz, the Nyquist or folding frequency is  f s /2  =  4,000  Hz (corresponding to 
 π radians per sample). Thus, the cutoff when normalized to the sample rate becomes 
2,200/4,000    ×  π  =  0.55 π . 
 Figure  9.20 shows the calculated response (solid line) for the direct BLT. It is 
apparent that, as expected, the magnitude response is reduced because, as explained, 
the entire frequency range is  “ compressed, ” resulting in the analog gain curve being 
mapped into a ﬁ nite discrete frequency range. The inevitable consequence is that 
the gain is reduced at all points, and at the cutoff frequency in this example, it is 
reduced to approximately 0.5. 
 However, if we prewarp the critical frequency from the analog to digital 
domain and incorporate that into the BLT, the result is as shown by the dashed line 
in Figure  9.20 . Clearly, the cutoff frequency gain is correct, but this necessarily 
results in a slight modiﬁ cation of the overall frequency response. 
 Figure  9.20 also shows the result of using a higher sampling rate. We have 
chosen 20  kHz, and show both the prewarped and non - prewarped frequency response. 
The non - prewarped or direct BLT is very close to the desired gain at the critical 
cutoff frequency, and in this case, the difference between the two is insigniﬁ cant. 
However, this has come at the expense of more than doubling the sampling rate. 
 9.6.5  Automating the Bilinear Transform 
 The algebra required for the bilinear transform is relatively straightforward, though 
tedious. We simply need to substitute
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
356 
CHAPTER 9 RECURSIVE FILTERS
 s
T
z
z
→⎛
⎝⎜
⎞
⎠⎟
−
+
⎛
⎝⎜
⎞
⎠⎟
2
1
1  
 in the continuous - domain transfer function. For ﬁ rst - order systems, this is simple; 
for second - order systems, it becomes more involved. For higher - order systems, the 
chances of mistakes in the algebraic manipulation become much higher. Thus, it is 
desirable to develop an automated procedure to handle the bilinear transform for 
any arbitrary ﬁ lter transfer function  G ( s ). 
 We can do this by noting that any transfer function is the ratio of two polyno-
mials, with a constant gain. For example, the transfer function
 G s
s
s
( ) =
+
+
4
2
3
2
 
 could be decomposed into a numerator polynomial  G n ( s )  =  1 s 0 , a denominator 
polynomial  G d ( s )  =  1 s 2  +  2 s 1  +  3 s 0 and gain  K  =  4. In the general case, we could 
write
 
 G s
K
s
s
s
s
s
s
M
M
M
N
N
N
( ) =
+
+
+
+
+
+
−
−
β
β
β
α
α
α
0
1
1
0
0
1
1
0


.  
 (9.62) 
 The order of the numerator is  M and that of the denominator is  N . Both  G n ( s ) and 
 G d ( s ) could be factored into corresponding roots, which may be either real or 
complex. Thus, we have the product of roots
 
 G s
K
s
q
s
p
M
m
N
n
( ) =
−
(
)
−
(
)
Π
Π
,  
 (9.63) 
 FIGURE 9.20   Comparison of the bilinear transform, with and without prewarping (a). 
Note that for the prewarped version, the gain at the cutoff frequency equals the desired 
gain, whereas the non - prewarped version has a somewhat lower gain at the cutoff 
frequency. The bilinear transformation (BLT) (and prewarped BLT), with a higher sample 
rate (20  kHz vs. 8  kHz, ﬁ lter cutoff 2.2  kHz) is shown in (b). The higher sample rate clearly 
moves the response of the ﬁ lter without prewarping much closer to the desired response. 
0
0.2
0.4
0.6
0.8
1.0
0
π
Bilinear transform filter with fs = 8 kHz
Frequency (radian per sample)
Gain (linear)
Direct BLT
Prewarped
Note gain at cutoff
frequency 0.55π
0
0.2
0.4
0.6
0.8
1
0
π
Bilinear transform filter with fs = 20 kHz
Frequency (radian per sample)
Gain (linear)
Note gain at cutoff
frequency 0.22π
(a) Sample frequency 8 kHz
(b) Sample frequency 20 kHz
Direct BLT
Prewarped
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
357
where the  q m are the (possibly complex) zeros, and  p n are the (possibly complex) 
poles. 
 Consider ﬁ rst the case of the poles. We have terms such as  1 s
p
−
(
) with a 
pole at  s  =  p , which are replaced in the bilinear transform with
 
1
2
1
1
T
z
z
p
⎛
⎝⎜
⎞
⎠⎟
−
+
⎛
⎝⎜
⎞
⎠⎟−
.
We can rearrange this to get the pole/zero form as follows:
 
 
1
1
2
1
1
1
2
1
1
1
s
p
T
z
z
p
z
T
z
p z
z
−
=
⎛
⎝⎜
⎞
⎠⎟
−
+
⎛
⎝⎜
⎞
⎠⎟−
=
+
(
)
⎛
⎝⎜
⎞
⎠⎟
−
(
)−
+
(
)
=
+
(
)
−
⎛
⎝⎜
⎞
⎠⎟−
+
⎛
⎝⎜
⎞
⎠⎟
=
+
(
)
−
⎛
⎝⎜
⎞
⎠⎟
−
−
−
⎛
⎝
⎜
⎜
⎜
⎞
⎠
z T
p
p
T
z
T
p
z
T
p
T
p
2
2
1
2
2
2
⎟
⎟
⎟
=
+
(
)
−
⎛
⎝⎜
⎞
⎠⎟
+
+
−
⎛
⎝
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
z
T
p
z
p
T
p
T
1
2
2
2
.
 
 (9.64) 
 Thus, the resulting  z domain transfer function will have the following factors, and 
every pole in the  s domain will produce these three terms in the  z domain:
 
Numerator
Denominator
Gain
z
z
p
T
p
T
T
p
+
(
)
+
+
−
⎛
⎝
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
−
⎛
⎝⎜
⎞
1
2
2
1
2
⎠⎟
.
 
 Consider now the case of the zeros. We have terms such as ( s  −  q ) with a zero 
at  s  =  q , which are replaced in the bilinear transform with
 2
1
1
T
z
z
q
−
+ −
⎛
⎝⎜
⎞
⎠⎟.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
358 
CHAPTER 9 RECURSIVE FILTERS
We can rearrange this into the pole/zero form as follows:
 
 
s
q
T
z
z
q
T
z
q z
z
z T
−
=
⎛
⎝⎜
⎞
⎠⎟
−
+
⎛
⎝⎜
⎞
⎠⎟−
=
⎛
⎝⎜
⎞
⎠⎟
−
(
)−
+
(
)
+
(
)
=
−
2
1
1
1
2
1
1
1
2
q
q
T
z
T
q
z
q
T
q
T
z
⎛
⎝⎜
⎞
⎠⎟−
+
⎛
⎝⎜
⎞
⎠⎟
+
(
)
=
−
⎛
⎝⎜
⎞
⎠⎟
+
+
−
⎛
⎝
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
+
2
1
2
2
2
1
(
)
.
 
 (9.65) 
 Thus, the resulting  z domain transfer function will have the following factors:
 
Numerator
Denominator
Gain
z
q
T
q
T
z
T
q
+
+
−
⎛
⎝
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
+
(
)
−
⎛
⎝⎜
⎞
2
2
1
1
2
⎠⎟
.
 
 To convert the above to MATLAB code, we proceed as follows. First, we must ﬁ nd 
the zeros, which are the roots of the numerator coefﬁ cients in vector bs, the number 
of zeros, and the corresponding poles. To determine the poles and zeroes, we rec-
ognize as per the previous dealings with polynomials that in fact, polynomial mul-
tiplication is the same as convolution. Thus, we need to convolve the existing 
coefﬁ cients with the newly determined coefﬁ cients. 
 The multiplication by
 z
q
T
q
T
+
+
⎛
⎝⎜
⎞
⎠⎟
−
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
2
2
becomes a convolution with vector
 v =
+
⎛
⎝⎜
⎞
⎠⎟
−
⎛
⎝⎜
⎞
⎠⎟
⎡
⎣⎢
⎤
⎦⎥
1
2
2
q
T
q
T
T
.
These are then the coefﬁ cients of  z 1 and  z 0 , respectively. This could be implemented 
for all poles as follows: 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.6 CONVERTING CONTINUOUS-TIME FILTERS TO DISCRETE FILTERS  
359
 A similar construction is used for the zeros. Note the convolution with the 
vector  v T  =  (1 1), which arises from the term ( z  +  1). If we repeat this for the zeros, 
we end up with a problem — we have multiple poles and zeros at  z  =  − 1, that is, the 
factor ( z  +  1) on both the top and bottom lines. Mathematically, this is not incorrect; 
it is simply that our code as developed thus far cannot  “ cancel ” these terms as we 
would algebraically:
 
 
 
 (9.66) 
 The solution to this problem is to recognize that there will be exactly ( N  −  M ) factors 
of ( z  +  1) in the numerator, assuming  M  <  N (i.e., in the  s domain, more poles than 
zeros). Thus, for the zeros, we remove the convolution with  v T to leave 
 for m  = 1:M 
      q  = bsroots(m); 
      zgain  = zgain * (2/T   −  q); 
      gamm  =  − (q  + 2/T)/(2/T   −  q); 
      bz  =  conv (bz, [1 gamm]); 
 end 
 for n  = 1:N 
      p  = asroots(n); 
      zgain  = zgain/(2/T   −  p); 
      eta  = (p  + 2/T)/(p   −  2/T); 
      az  =  conv (az, [1 eta]); 
 end 
 bsroots  =  roots (bs); 
 M  =  length (bsroots); 
 asroots  =  roots (as); 
 N  = length (asroots); 
 zgain  = 1; 
 bz  = [1]; 
 az  = [1]; 
 for n  = 1:N 
      p  = asroots(n); 
      zgain  = zgain/(2/T   −  p); 
      eta  = (p  + 2/T)/(p   −  2/T); 
      bz  =  conv (bz, [1 1]); 
      az  =  conv (az, [1 eta]); 
 end 
 Similarly, for the poles, we could use 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
360 
CHAPTER 9 RECURSIVE FILTERS
 and ﬁ nally factor in ( z  +  1) N  −  M as follows 
 nz  = N   −  M; 
 for n  = 1:nz 
      bz  =  conv (bz, [1 1]); 
 end 
 We shall now develop algebraically some test cases for our bilinear transform 
method. The ﬁ rst is an obvious and simple ﬁ rst - order system described by
 G s
s
( ) =
+
1
1,  
 which becomes
 
G s
z
T
z
T
T
( ) =
+
(
)
+
⎛
⎝⎜
⎞
⎠⎟
+
−
+
⎡
⎣
⎢
⎢
⎢
⎤
⎦
⎥
⎥
⎥
⎛
⎝
⎜
⎜
⎜
⎞
⎠
⎟
⎟
⎟
1
2
1
1
2
1
2
.
 
 This may be tested against the algorithmic development as described. To extend 
the testing to a higher - order system, consider a simple second - order system 
described by
 G s
s
s
( ) =
+ +
1
1
2
,  
 which becomes
 G s
z
z
T
T
z
T
( ) =
+
+
(
)
⎛
⎝⎜
⎞
⎠⎟+ ⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟
+
−⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
2
2
2
2
2
1
2
2
1
2
2 2
⎞
⎠⎟
+ ⎛
⎝⎜
⎞
⎠⎟−⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟
z
T
T
2
2
1
2
.  
 Finally, we extend to a second - order system with a zero. Consider
 G s
s
s
s
( ) =
+
+ +
1
1
2
,  
 which, when transformed, becomes
 G s
T
z
z
T
T
T
z
( ) =
+
⎛
⎝⎜
⎞
⎠⎟
+
+
−
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟+ ⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟
2
1
2
1
2
2
2
1
2
2
2
2
2
2
2 2
2
2
1
+
−⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
+ ⎛
⎝⎜
⎞
⎠⎟−⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟
T
z
T
T
.  
 Each of these algebraic test cases can be used to check the numerical/algorithmic 
solution approach as described.  
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.7 SCALING AND TRANSFORMATION OF CONTINUOUS FILTERS  
361
 9.7  SCALING AND TRANSFORMATION 
OF CONTINUOUS FILTERS 
 A natural question arising at this point is how to design for any cutoff frequency 
and how to design different responses. The former question is relatively easy to 
answer; the latter is a little more difﬁ cult but follows along similar lines. In this 
section, we develop a method to cater for any arbitrary ﬁ lter cutoff frequency, fol-
lowed by an investigation of methods to convert the low - pass response  “ prototype ” 
ﬁ lter into high - pass, bandpass, or bandstop ﬁ lter response shapes. 
 Scaling a low - pass ﬁ lter to a cutoff frequency  Ω c is accomplished relatively 
easily, by the substitution of
 
 s
s
c
→Ω .  
 (9.67) 
 This seems intuitively reasonable since to determine the frequency response, we 
substitute  s = jΩ, and the resulting division by  Ω c in the scaling operation effec-
tively gives a ratio of  Ω Ωc. When  Ω  =  Ω c , the ratio is unity, and thus we effectively 
have a scaled or normalized response. 
 Along similar lines, transforming a low - pass ﬁ lter to a high - pass ﬁ lter requires 
the substitution
 
 s
s
→1.  
 (9.68) 
 Combining these two results, it is not difﬁ cult to see that converting a normalized 
low - pass ﬁ lter (i.e., one with cutoff 1  rad/s) into a high - pass ﬁ lter with cutoff fre-
quency  Ω c requires the substitution
 
 s
s
c
→Ω .  
 (9.69) 
 Converting a low - pass ﬁ lter to a bandpass ﬁ lter is somewhat less intuitive. We need 
to know the range of frequencies to pass, and these are speciﬁ ed by the passband 
frequency range  Ω l to  Ω u . To develop the method, we deﬁ ne the ﬁ lter passband (or 
bandwidth)  Ω b as the difference between upper and lower frequencies:
 
 Ω
Ω
Ω
b
u
l
=
−
.  
 (9.70) 
 We then need to deﬁ ne a center frequency  Ω o , which is calculated as the  geometric 
mean of the lower and upper cutoffs:
 
 Ω
Ω Ω
o
l
u
2 =
.  
 (9.71) 
 Finally, the substitution into the original low - pass ﬁ lter with normalized cutoff 
becomes
 
 s
s
s
b
o
→
+
1
2
2
Ω
Ω  
 (9.72) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
362 
CHAPTER 9 RECURSIVE FILTERS
 with  Ω b and  Ω o as above. Note that this increases the order of the ﬁ lter: For every 
 s , we have an  s 2 term after substitution. This aspect may be understood by consider-
ing that a bandpass ﬁ lter is effectively a high - pass ﬁ lter cascaded with a low - pass 
ﬁ lter. 
 To effect a bandstop response, the related transformation
 
 s
s
s
b
o
→
+
Ω
Ω
2
2  
 (9.73) 
 is applied. 
 The following sections elaborate on each of these methods in turn by giving 
an algebraic solution and then a corresponding algorithmic solution. 
 9.7.1  Scaling the Filter Response 
 To investigate the scaling of the normalized low - pass response into a low - pass 
response with an arbitrary cutoff, consider the transfer function
 G s
s
s
s
( ) =
+
+
+
6
3
2
4
2
.  
 The order of the numerator is  M  =  1 and that of the denominator is  N  =  2. Normally, 
this would be a ﬁ lter with a cutoff of 1  rad/s, for the Butterworth and Chebyshev 
designs presented earlier. We scale this low - pass ﬁ lter (keeping a low - pass ﬁ lter 
response) to a cutoff frequency,  Ω c , using
 s
s
c
→Ω  
 to obtain
 
G s
s
s
s
s
c
c
c
c
c
c
( ) =
⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟+
=
⋅
⎛
⎝⎜
⎞
⎠⎟
6
3
2
4
6
2
2
2
Ω
Ω
Ω
Ω
Ω
Ω
+
⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟+
=
+
+
+
3
2
4
6
3
2
4
2
2
2
2
s
s
s
s
s
c
c
c
c
c
c
Ω
Ω
Ω
Ω
Ω
Ω .
 
 We can generalize this as illustrated in Figure 9.21. Start with the low - pass function
 
 G s
b s
b s
b s
a s
a s
a s
M
M
M
N
N
N
( ) =
+
+
+
+
+
+
−
−
0
1
1
0
0
1
1
0


.  
 (9.74) 
 Then, we scale using  s
s
c
→
Ω  to give
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.7 SCALING AND TRANSFORMATION OF CONTINUOUS FILTERS  
363
 
 
G s
b
s
b
s
b
s
a
s
c
M
c
M
M
c
c
( ) =
⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
−
0
1
1
0
0
Ω
Ω
Ω
Ω

N
c
N
N
c
c
N
c
N
c
M
c
a
s
a
s
b
s
b
s
+
⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
=
⋅
⎛
⎝⎜
⎞
⎠⎟
+
−
1
1
0
0
1
Ω
Ω
Ω
Ω
Ω
Ω

⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝⎜
⎞
⎠⎟
+
+
−
−
M
M
c
c
N
c
N
N
b
s
a
s
a
s
a
s
1
0
0
1
1


Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
c
c
N
M
M
c
N
M
M
M
c
N
c
N
c
b
s
b
s
b
s
a
s
a
s
⎛
⎝⎜
⎞
⎠⎟
=
+
+
+
+
−
−
+
−
0
0
1
1
1
0
0
0
1
1

N
N
c
N
a
s
−+
+
1
0

Ω
.
 
 (9.75) 
 The highest power of variable  s in the numerator  M will remain unchanged. Similarly, 
the order of the denominator will remain  N . 
 To develop an algorithm for computing the coefﬁ cients in the polynomial ratio, 
we proceed as follows. We need to input the coefﬁ cients  b M and  a N , and the scaling 
factor  Ω c . 
 The orders of numerator  M and denominator  N are then initialized as follows. 
We also normalize by coefﬁ cient  a 0 : 
 % powers of s 
 M  =  length (bs)   −  1; 
 N  =  length (as)   −  1; 
 % coefﬁ cients in scaled transfer function 
 k  = as(1); 
 bss  = bs/k; 
 ass  = as/k;  
 p  = N  −  M; 
 for m  = 1:M  + 1 
      sc  = OmegaC ˆ p; 
      bss(m)  = bss(m) * sc; 
      p  = p  + 1; 
 end 
 If we let  p be the power of  Ω c , which starts at  N  −  M , we can calculate the 
numerator using the following loop. Remember that for an order  M , the number of 
coefﬁ cients will be  M  +  1. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
364 
CHAPTER 9 RECURSIVE FILTERS
 For the denominator, we can use a similar approach, noting that  p starts 
at zero: 
 p  = 0; 
 for n  = 1:N  + 1 
      sc  = OmegaC ˆ p; 
      ass(n)  = ass(n) * sc; 
      p  = p  + 1; 
 end 
 The polynomials bss and ass now deﬁ ne the scaled transfer function, with the 
scaling of the cutoff frequency deﬁ ned by  Ω c . The low - pass response at 1  rad/s is 
translated in frequency to  Ω c . 
 9.7.2  Converting Low - Pass to High - Pass 
 Suppose we have a transfer function,
 G s
s
s
s
( ) =
+
+
+
6
3
2
4
2
.  
 The order of the numerator is  M  =  1 and that of the denominator is  N  =  2. Normally, 
this would be a ﬁ lter with a cutoff of 1  rad/s, for the Butterworth and Chebyshev 
designs presented earlier (Fig.  9.21 ). 
 We can convert this low - pass ﬁ lter to a high - pass ﬁ lter with cutoff frequency 
 Ω c using the substitution deﬁ ned previously,
 s
s
c
→Ω ,  
 to obtain
 
G s
s
s
s
s
s
s
c
c
c
c
( ) =
⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟+
=
⋅
⎛
⎝⎜
⎞
⎠⎟+
6
3
2
4
6
3
2
2
2
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
c
c
c
c
c
c
c
s
s
s
s
s
s
s
⎛
⎝⎜
⎞
⎠⎟+
⎛
⎝⎜
⎞
⎠⎟+
=
+
+
+
2
1
1
2
0
2
0
1 1
0
2
2
4
6
3
1
2
4
.
 
 We can generalize this as illustrated in Figure 9.22. Start with the low - pass function
 
 G s
b s
b s
b s
a s
a s
a s
M
M
M
N
N
N
( ) =
+
+
+
+
−
−
0
1
1
0
0
1
1
0


.  
 (9.76) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.7 SCALING AND TRANSFORMATION OF CONTINUOUS FILTERS  
365
 FIGURE 9.21   Scaling a low - pass ﬁ lter for an arbitrary cutoff frequency. The upper plot 
shows the normalized response with a cutoff of 1  rad/s. The lower plot shows the response 
scaled using the method described in the text to a cutoff of 200  rad/s. The response  shape 
remains unchanged; only the cutoff frequency is altered. 
10−1
100
101
−80
−60
−40
−20
0
20
Gain (dB)
Frequency (rad/s)
Chebyshev filter, cutoff 1 rad/s
101
102
103
−80
−60
−40
−20
0
20
Gain (dB)
Frequency (rad/s)
Chebyshev filter, cutoff 200 rad/s
 Then, we scale using  s
s
c
→Ω  to give
 
 
G s
b
s
b
s
b
s
a
s
c
M
c
M
M
c
c
( ) =
⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
−
0
1
1
0
0
Ω
Ω
Ω
Ω

N
c
N
N
c
N
N
c
M
c
a
s
a
s
s
s
b
s
b
s
+
⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
=
⋅
⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝
−
1
1
0
0
1
Ω
Ω
Ω
Ω

⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝⎜
⎞
⎠⎟
+
+
−
−
M
M
c
c
N
c
N
N
c
b
s
a
s
a
s
a
s
1
0
0
1
1


Ω
Ω
Ω
Ω
⎛
⎝⎜
⎞
⎠⎟
=
+
+
+
+
−
−
−
+
−
0
0
1
1
1
0
0
0
1
1
b
s
b
s
b
s
a
s
a
s
c
M
N
M
c
M
N
M
M
c
N
c
N
c
N
Ω
Ω
Ω
Ω
Ω

1
0
+
+

a
s
N
c
N
Ω
.
 
 (9.77) 
 The highest power of variable  s in the numerator  M will remain unchanged. Similarly, 
the order of the denominator will remain  N . 
 To develop an algorithm for computing the coefﬁ cients in the polynomial ratio, 
we proceed as follows. We need to input the coefﬁ cients  b m and  a n , and the scaling 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
366 
CHAPTER 9 RECURSIVE FILTERS
factor  Ω c . The order of numerator and denominator are now both  N (since  N  >  M , 
we assume more poles than zeros in the low - pass prototype). 
 q  = N  −  M; 
 p  = M; 
 for m  = 1:M + 1 
      bshp(Ms + 1 − q)  = bs(m) * (OmegaC ˆ p); 
      q  = q  + 1; 
      p  = p   −  1; 
 end 
 q  = 0; 
 p  = N; 
 for n  = 1:N + 1 
      ashp(Ns + 1 − q)  = as(n) * (OmegaC ˆ p); 
      q  = q  + 1; 
      p  = p   −  1; 
 end 
 M  =  length (bs) − 1; 
 N  =  length (as) − 1; 
 Ms  = N; 
 Ns  = N; 
 bshp  =  zeros (Ms  + 1, 1); 
 ashp  =  zeros (Ns  + 1, 1);  
 It is important to remember that polynomial coefﬁ cients are stored in decreas-
ing powers from left to right. For example, if we have 6 s 2  +  5 s  +  4, we would store 
the coefﬁ cients in order of  s 2 s 1 s 0 as the array c  =  [6 5 4]. So, c(1)  =  6 is the coef-
ﬁ cient of  s 2 , and so forth. This appears when printed in the  “ natural ” order (i.e., 6, 
5, 4), but the indexing then becomes a little tricky, since in this example powers 2, 
1, 0 are at index 1, 2, 3 into array c. 
 If we let  q be the power of  s , which starts at  N  −  M , we can calculate the 
numerator using the following loop (remember that for an order  M , the number of 
coefﬁ cients will be  M  +  1). 
 For the denominator, we can use a similar approach, noting that  q starts at zero 
since we start from  s 0 at the leftmost term. 
 9.7.3  Converting Low - Pass to Band - Pass 
 In order to convert a low - pass ﬁ lter into a bandpass ﬁ lter with passband frequency 
 Ω l to  Ω u , we use the substitution deﬁ ned previously:
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.7 SCALING AND TRANSFORMATION OF CONTINUOUS FILTERS  
367
 
 s
s
s
o
b
→
+
2
2
Ω
Ω
,  
 (9.78) 
where the center frequency  Ω o is deﬁ ned by
 
 Ω
Ω Ω
o
l
u
2 =
,  
 (9.79) 
and the bandwidth  Ω b is deﬁ ned by
 
 Ω
Ω
Ω
b
u
l
=
−
.  
 (9.80) 
 To give an example, suppose we have a transfer function,
 G s
s
s
s
( ) =
+
+
+
6
3
2
4
2
.  
 The order of the numerator is  M  =  1 and that of the denominator is  N  =  2. Normally, 
this would be a ﬁ lter with a cutoff of 1  rad/s for the Butterworth and Chebyshev 
designs presented earlier (Fig.  9.22 ). 
 We convert this low - pass ﬁ lter to a bandpass ﬁ lter with center frequency  Ω o 
and bandwidth  Ω b using
 
 s
s
s
o
b
→
+
2
2
Ω
Ω
,  
 (9.81) 
where  Ω o and  Ω b are deﬁ ned as above. Note that now,  Ω o is not the  “ cutoff  ” fre-
quency but rather the geometric mean of the lower and upper cutoff frequencies. 
The present example then becomes
 
 
G s
s
s
s
s
s
s
c
b
c
b
c
b
( ) =
+
⎛
⎝⎜
⎞
⎠⎟+
+
⎛
⎝⎜
⎞
⎠⎟+
+
⎛
⎝⎜
⎞
⎠⎟+
6
3
2
4
2
2
2
2
2
2
2
Ω
Ω
Ω
Ω
Ω
Ω
= ⎛
⎝⎜
⎞
⎠⎟⋅
+
⎛
⎝⎜
⎞
⎠⎟+
+
⎛
⎝⎜
⎞
⎠⎟+
+
s
s
s
s
s
s
s
b
b
c
b
c
b
c
Ω
Ω
Ω
Ω
Ω
Ω
Ω
2
2
2
2
2
2
2
2
6
3
2
s
s
s
s
s
s
s
b
c
b
c
b
c
b
Ω
Ω
Ω
Ω
Ω
Ω
Ω
⎛
⎝⎜
⎞
⎠⎟+
=
+
(
) (
) +
+
(
) (
)
+
(
)
4
6
3
2
2 1
1
2
2
0
2
2
2
2 (
) +
+
(
) (
) +
+
(
) (
)
0
2
2 1
1
2
2
0
2
2
4
s
s
s
s
c
b
c
b
Ω
Ω
Ω
Ω
.
 
 (9.82) 
 We can generalize this approach as follows. Start with the low - pass function
 
 G s
b s
b s
b s
a s
a s
a s
M
M
M
N
N
N
( ) =
+
+
+
+
−
−
0
1
1
0
0
1
1
0


.  
 (9.83) 
 Then, scale using
 
 s
s
s
o
b
→
+
2
2
Ω
Ω
.  
 (9.84) 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
368 
CHAPTER 9 RECURSIVE FILTERS
 Then, we have
  
G s
b
s
s
b
s
s
b
s
s
c
b
M
c
b
M
M
c
b
( ) =
+
⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
+
+
+
−
0
2
2
1
2
2
1
2
2
Ω
Ω
Ω
Ω
Ω
Ω

⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝⎜
⎞
⎠⎟
+
+
+
−
0
0
2
2
1
2
2
1
2
2
a
s
s
a
s
s
a
s
c
b
N
c
b
N
N
c
Ω
Ω
Ω
Ω
Ω

s
s
s
b
s
s
b
s
s
b
b
b
N
c
b
M
c
b
Ω
Ω
Ω
Ω
Ω
Ω
Ω
⎛
⎝⎜
⎞
⎠⎟
= ⎛
⎝⎜
⎞
⎠⎟
⋅
+
⎛
⎝⎜
⎞
⎠⎟
+
+
⎛
⎝
0
0
2
2
1
2
2
⎜
⎞
⎠⎟
+
+
+
⎛
⎝⎜
⎞
⎠⎟
+
⎛
⎝⎜
⎞
⎠⎟
+
+
−
M
M
c
b
c
b
N
c
b
s
s
a
s
s
a
s
s
1
2
2
0
0
2
2
1
2
2

Ω
Ω
Ω
Ω
Ω
Ωb
N
N
c
b
c
M
b
N
M
a
s
s
b s
s
b s
⎛
⎝⎜
⎞
⎠⎟
+
+
+
⎛
⎝⎜
⎞
⎠⎟
=
+
(
) (
)
+
+
−
−
1
2
2
0
0
2
2
1
2

Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
Ω
c
M
b
N
M
M
c
b
N
c
N
b
s
b
s
s
a
s
s
a
2
1
1
2
2
0
0
2
2
0
(
)
(
)
+
+
+
(
) (
)
+
(
) (
) +
−
−
+

1
2
2
1
1
2
2
0
s
s
a
s
s
c
N
b
N
c
b
N
+
(
)
(
) +
+
+
(
) (
)
−
Ω
Ω
Ω
Ω

.
 
 
(9.85)
 
 FIGURE 9.22   Converting a low - pass ﬁ lter into a high - pass ﬁ lter. The upper plot shows 
the normalized low - pass response with a cutoff of 1  rad/s. The lower plot shows the 
response scaled and mapped into a high - pass response using the method described in the 
text to a cutoff of 50  rad/s. The response shape remains unchanged, though of course 
 “ ﬂ ipped ” with respect to frequency. 
10−1
100
101
−80
−60
−40
−20
0
20
Gain (dB)
Frequency (rad/s)
Chebyshev filter, cutoff 1 rad/s
101
102
103
−80
−60
−40
−20
0
20
Gain (dB)
Frequency (rad/s)
Chebyshev filter, high-pass, cutoff 50 rad/s
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.7 SCALING AND TRANSFORMATION OF CONTINUOUS FILTERS  
369
 M  =  length (bs) − 1; 
 N  =  length (as) − 1; 
 Ms  = M * 2  + (N − M); 
 Ns  = N * 2; 
 OmegaB  = OmegaU   −  OmegaL; 
 OmegaC  =  sqrt (OmegaL * OmegaU); 
 bsbp  =  zeros (Ms + 1, 1); 
 asbp  =  zeros (Ns + 1, 1);  
 p  = N;   % power of ( s ˆ 2 + OmegaC ˆ 2 ) 
 q  = 0;   % power of s OmegaB 
 for n  = 1:N + 1 
      % calc (s ˆ 2  + OmegaC ˆ 2) ˆ p ; 
      tv  = [1 0 OmegaC ˆ 2]; 
      rp  = [1] * as(n); 
      for k  = 1:p 
           rp  =  conv (rp, tv); 
      end 
      % calc(s Omega) ˆ q; 
      tv  = [OmegaB 0]; 
      rq  = [1]; 
      for k  = 1:q 
           rq  =  conv (rq, tv); 
      end 
 The order of the denominator will be 2 N , whereas the numerator will be 2 M  +  ( N  −  M ). 
To see this, note that we will have a ( s 2 )  N term in the denominator, and the numerator 
will have a ( s 2 ) M term multiplied by  s N  − M . Thus, adding the indices, we have the order 
2 M  +  ( N  −  M )  =  M  +  N . To develop an algorithm for computing the coefﬁ cients in 
the polynomial ratio, we proceed as follows. We need to input the coefﬁ cients  b m 
and  a n , and the bandwidth deﬁ ned by ( Ω u ,  Ω l ). According to the reasoning above, 
we have 
 The added complication we have now is that we have polynomials multiplied 
out. In the numerical example, we had terms such as  (
)
s
c
2
2
+ Ω
( s Ω b ). The MATLAB 
operator  conv () may be used to multiply out a polynomial. If we express the ﬁ rst 
factor as  1
0
2
1
2
0
s
s
s
c
+
+ Ω
 and the second as  Ω b s 1  +  0 s 0 , we would calculate the 
product as the convolution of the vectors  [
]
1 0
2
Ωc  and [ Ω b 0]. 
 Where we have a factor such as  s
c
N
2
2
+
(
)
Ω
, we perform the convolution  N 
times. So, we let  p be the power of  (
)
s
c
2
2
+ Ω
 and  q be the power of  s Ω b , and the 
denominator may then be calculated as follows. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
370 
CHAPTER 9 RECURSIVE FILTERS
 We have used tv as a temporary vector in the above. Note also the alignment 
of the resulting polynomials with the ﬁ nal output coefﬁ cient vector, performed by 
the start/end indexing ie : ie. For the numerator, we use a similar approach since 
we start from  s 0 at the leftmost term. 
 p  = M;          % power of ( s ˆ 2 + OmegaC ˆ 2) 
 q  = N   −  M;    % power of s OmegaB 
 for m  = 1:M + 1 
      % calculate (s ˆ 2  + OmegaC ˆ 2) ˆ p; 
      tv  = [1 0 OmegaC ˆ 2]; 
      rp  = [1] * bs(m); 
      for k  = 1:p 
           rp  =  conv (rp, tv); 
      end 
      % calculate the product (s Omega) ˆ q; 
      tv  = [OmegaB 0]; 
      rq  = [1]; 
      for k  = 1:q 
           rq  =  conv (rq, tv); 
      end 
      % (s ˆ 2 + OmegaC ˆ 2) ˆ p (s OmegaB) ˆ q 
      r  =  conv (rp, rq); 
      is  = Ms + 1   −   length (r) + 1; 
      ie  = Ms + 1; 
      bsbp(is:ie)  = bsbp(is:ie)  + r(:); 
      p  = p  −  1; 
      q  = q  + 1; 
 end 
      % ( s ˆ 2  + OmegaC ˆ 2) ˆ p (s OmegaB) ˆ q 
      r  =  conv (rp, rq); 
      is  = Ns + 1   −   length (r) + 1; 
      ie  = Ns + 1; 
      asbp(is:ie)  = asbp(is:ie)  + r(:); 
      p  = p  −  1; 
      q  = q  + 1; 
 end 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.8 SUMMARY OF DIGITAL FILTER DESIGN VIA ANALOG APPROXIMATION  
371
 FIGURE 9.23   Converting a low - pass ﬁ lter into a bandpass ﬁ lter. This type of ﬁ lter is 
effectively a high - pass and a low - pass ﬁ lter in cascade, with the high - pass cutoff being 
 lower than the low - pass cutoff. The example shows a cutoff range of 100 – 200  rad/s. It is 
clear that the low - pass response shape is mapped into the lower and upper components of 
the bandpass response. 
10−1
100
101
−80
−60
−40
−20
0
20
Gain (dB)
Frequency (rad/s)
Chebyshev filter, cutoff 1 rad/s
101
102
103
−80
−60
−40
−20
0
20
Gain (dB)
Frequency (rad/s)
Chebyshev filter, bandpass, 100–200 rad/s
 Figure  9.23 shows the analog low - pass prototype, and the result of the conver-
sion to a bandpass ﬁ lter. Once this ﬁ lter response is available, one of the methods 
discussed earlier (BLT or impulse invariant design) may be used to convert the ﬁ lter 
into a digital or discrete - time design. 
 9.8  SUMMARY OF DIGITAL FILTER DESIGN 
VIA ANALOG APPROXIMATION 
 There are many steps to be undertaken in designing a suitable ﬁ lter for a given 
application. This almost always requires a computer - based approach, combined with 
a sound understanding of the underlying theoretical and mathematical principles. 
For anything other than very low - order ﬁ lters, software is utilized to speed up the 
design process and to reduce the likelihood of errors. Usually, ﬁ lter design involves 
an iterative approach — a design is created, tested theoretically, and, if need be, 
further reﬁ ned. There are a great many approaches to design, and we have only 
attempted to summarize some of the common approaches here. The reader wishing 
to develop further specialist knowledge in this ﬁ eld is referred to the many technical 
books on the topic, as well as the academic literature in the bibliography. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
372 
CHAPTER 9 RECURSIVE FILTERS
 Recursive Filter Design Steps 
 1.  Specify the response shape and determine the  f s . 
 2.  Determine the appropriate ﬁ lter type (Butterworth, Chebyshev, other). 
 3.  Select the ﬁ lter order and other parameters as appropriate (e.g., allowable ripple). 
 4.  Design the low - pass prototype with normalized frequency response. 
 5.  Scale to the required frequency and convert the ﬁ lter type (if not low - pass). 
 6.  Convert to a discrete - time transfer function using either the BLT method or the impulse 
invariant method. 
 7.  Plot the sampled frequency response and iterate the above steps as necessary. 
 8.  Convert  Y ( z ) into a difference equation; ﬁ nd  y ( n ) in terms of  x ( n ),  x ( n  −  1),  y ( n  −  1), 
and so on. 
 9.  Code a loop to sample the input  x ( n ), calculate  y ( n ), output  y ( n ), and buffer the samples. 
 9.9  CHAPTER SUMMARY 
 The following are the key elements covered in this chapter:
 •  A review of  analog ﬁ lter theory including time and frequency response 
methods  
 •  The role of  IIR ﬁ lters 
 •  The theory behind two commonly employed types of analog ﬁ lter: the 
 Butterworth ﬁ lter and the  Chebyshev ﬁ lter. The former is maximally ﬂ at in the 
passband; the latter has a sharper transition but has a nonﬂ at passband response. 
 •  The transformation and scaling of ﬁ lters from the basic low - pass design to 
 high - pass and  bandpass responses 
 •  How to  transform analog ﬁ lters into discrete - time or digital ﬁ lters. Two 
approaches were considered: the  impulse invariant method , which matches the 
impulse response in the time domain, and the  bilinear transform , which is a 
direct substitution for the continuous - to - discrete approximation.     
 PROBLEMS 
 9.1.  Using the approach outlined in Section  9.3.2 , plot the impulse response of
 
1
1
2
1
2s
s
+
+
and compare to Figure  9.3 . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
9.9 CHAPTER SUMMARY 
373
 9.2.  Using the approach outlined in Section  9.3.3 , plot the frequency response of
 
1
1
2
1
2s
s
+
+
and compare to Figure  9.4 . Note that you will need to choose appropriate values for 
some of the parameters. Remember to plot decibels on the gain axis, and use a loga-
rithmic scale for the frequency axis using  set ( gca ,  ’ xscale ’ ,  ’ log ’ );. 
 9.3.  Consider the Butterworth ﬁ lter deﬁ ned in Section  9.4.1 .
 (a)  Show that as  N  →  ∞ , the ﬁ lter approaches the ideal or brick wall response. Do 
this by ﬁ nding the gain of the ﬁ lter in Equation  9.13 as  Ω  →  0 and as  Ω  →  ∞ , 
and then by determining the gain at the cutoff, where  Ω  =  Ω c . From these 
results, sketch the ﬁ lter gain characteristic. 
 (b)  Apply the normalized low - pass to high - pass conversion as explained in Section 
 9.7 . Show that the ﬁ lter approaches the ideal as  N  →  ∞ ; that is, ﬁ nd the limit 
as  Ω  →  0 and as  Ω  →  ∞ , then determine the gain at the cutoff frequency where 
 Ω  =  Ω c ; and ﬁ nally, sketch the gain characteristic. 
 (c)  Apply the normalized low - pass to bandpass transformation and ﬁ nd the gain for 
low frequencies, high frequencies, at the cutoff frequencies ( Ω  =  Ω l and  Ω  =  Ω u ), 
and within the passband at  Ω  =  Ω c . Sketch the ﬁ lter gain characteristic. 
 (d)  Repeat the previous question for the bandstop transformation. 
 9.4.  Chebyshev ﬁ lters (Section  9.4.2 ) are based on the theory and mathematics of the 
Chebyshev polynomial. The  N th order Chebyshev polynomial is denoted  T N ( x ).  T N ( x ) 
may be deﬁ ned in terms of cosine functions as  T N ( x )  =  cos ( N  cos  − 1  x ).
 (a)  Show that  T 0 ( x )  =  1 and  T 1 ( x )  =  x . 
 (b)  Show that the recursive deﬁ nition  T N ( x )  =  2 xT N  − 1 ( x )  −  T N  −  2 ( x ) for  N  >  0 holds. 
 (c)  Explain how the recursive deﬁ nition of  T N ( x ) applies to the graphs of the 
Chebyshev polynomials as shown in Figure  9.8 . 
 9.5.  Plot the continuous - time response for a fourth - order Butterworth low - pass ﬁ lter with 
cutoff frequency 25  kHz. Repeat this for a Chebyshev ﬁ lter. How do the responses 
compare?  
 9.6.  For the continuous - time system  G s
s
s
( ) =
+
+
(
)
1
2
1
2
,
 (a)  plot the frequency response over a reasonable range of frequencies and explain 
your choice of frequencies by referring to the transfer function;  
 (b)  scale to a frequency of 10  Hz and again plot the response; 
 (c)  use the direct bilinear transform method to determine the corresponding  z trans-
form; use a sample frequency of 100  Hz; and 
 (d)  use the bilinear transform method with prewarping, and determine the corre-
sponding  z transform; use the same sampling frequency and compare to the 
previous result and to the analog ﬁ lter response. 
 9.7.  Complete the ﬁ rst - order example using both the original and corrected impulse invari-
ant methods. Compare the resulting difference equation for each ﬁ lter. Using the pole –
 zero approach, sketch their respective frequency responses. 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
374 
CHAPTER 9 RECURSIVE FILTERS
 9.8.  Complete the second - order example using both the original and corrected impulse 
invariant methods.  
 9.9.  A discrete ﬁ lter with a low - pass response, cutoff 100  Hz, and a sample rate of 10  kHz 
is required.
 (a)  Design the continuous - time Butterworth ﬁ lter for these parameters using a 
normalized 1  rad per sample cutoff. Calculate the transfer function using algebra, 
and check your answer using the MATLAB code outlined in the text. 
 (b)  Scale the Butterworth ﬁ lter to the desired cutoff and plot the frequency response. 
 (c)  Convert the normalized Butterworth ﬁ lter to a discrete - time one using the bilin-
ear transform.  
 (d)  Plot the frequency response and compare to the design speciﬁ cation. 
 
 
 
 
 
 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
 BIBLIOGRAPHY 
 SELECTED WEB REFERENCES 
 Frigo ,  M. , and  S. G.  Johnson . n.d..  FFTW — fastest Fourier transform in the West .  http://www. 
fftw.org . 
 Goldberger ,  A. L. ,  L. A. N.  Amaral ,  L.  Glass ,  J. M.  Hausdorff ,  P. C.  Ivanov ,  R. G.  Mark ,  J. 
E.  Mietus ,  G. B.  Moody ,  C. - K.  Peng , and  H. E.  Stanley .  2000 .  PhysioBank, PhysioToolkit, 
and PhysioNet: components of a new research resource for complex physiologic signals . 
 Circulation  101 ( 23 ): e215 – e220 .  http://circ.ahajournals.org/cgi/content/full/101/23/e215 . 
 IEEE 754 Group .  2004 .  IEEE 754: standard for binary ﬂ oating - point arithmetic .  http://
grouper.ieee.org/groups/754 . 
 IEEE/NSF . n.d..  Signal processing information base .  http://spib.rice.edu/spib.html . 
 Kak ,  A. C. , and  M.  Slaney .  1988 .  Principles of Computerized Tomographic Imaging . 
 New York :  IEEE Press .  http://www.slaney.org/pct/index.html . 
 SIDC - team .  1749 – 2010 .  Monthly report on the international sunspot number .  http://www.
sidc.be/sunspot - data/ . 
 SELECTED ACADEMIC PAPERS 
 Ahmed ,  N. ,  T.  Natarajan , and  K. R.  Rao .  1974 .  Discrete cosine transform .  IEEE Transactions 
on Computers  C23 ( 1 ): 90 – 93 . 
 Black ,  H. S.  1934 .  Stabilized feed - back ampliﬁ ers .  American Institute of Electrical Engineers 
 53 : 114 – 120 . 
 Butterworth ,  S.  1930 .  On the theory of ﬁ lter ampliﬁ ers .  Experimental Wireless and the 
Wireless Engineer  7 : 536 – 541 . 
 Cooley ,  J. W. ,  P. A.  Lewis , and  P. D.  Welch .  1967a .  Application of the fast Fourier transform 
to computation of Fourier integrals, Fourier series, and convolution integrals .  IEEE 
Transactions on Audio and Electroacoustics  AU - 15 ( 2 ): 79 . 
 Cooley ,  J. W. ,  P. A.  Lewis , and  P. D.  Welch .  1967b .  Historical notes on the fast Fourier trans-
form .  IEEE Transactions on Audio and Electroacoustics  AU - 15 ( 2 ): 76 . 
 Fernandes ,  C. W. ,  M. D.  Bellar , and  M. M.  Werneck .  2010 .  Cross - correlation - based optical 
ﬂ owmeter .  IEEE Transactions on Instrumentation and Measurement  59 ( 4 ): 840 – 846 . 
 Frigo ,  M. , and  S. G.  Johnson .  1998 .  FFTW: an adaptive software architecture for the FFT . In 
 Proceedings of the International Conference on Acoustics, Speech, and Signal Processing . 
 1381 – 1384 . 
375
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
376 
BIBLIOGRAPHY
 Frigo ,  M. , and  S. G.  Johnson .  2005 .  The design and implementation of FFTW3 .  Proceedings 
of the IEEE  93 ( 2 ): 216 – 231 . 
 Goldberg ,  D.  1991 .  What every computer scientist should know about ﬂ oating - point arithme-
tic .  ACM Computing Surveys  23 ( 1 ). 
 Harris ,  F. J.  1978 .  On the use of Windows for harmonic analysis with the discrete Fourier 
transform .  Proceedings of the IEEE  66 ( 1 ): 51 – 83 . 
 Hayes ,  A. M. , and  G.  Musgrave .  1973 .  Correlator design for ﬂ ow measurement .  The Radio 
and Electronic Engineer  43 ( 6 ): 363 – 368 . 
 Heideman ,  M. T. ,  D. H.  Johnson , and  C. S.  Burrus .  1984 .  Gauss and the history of the fast 
Fourier transform .  IEEE ASSP Magazine  1 ( 4 ): 14 – 19 . 
 Helms ,  H. D.  1967 .  Fast Fourier transform method of computing difference equations and 
simulating ﬁ lters .  IEEE Transactions on Audio and Electroacoustics  AU - 15 ( 2 ): 85 . 
 Jackson ,  L. B.  2000 .  A correction to impulse invariance .  IEEE Signal Processing Letters 
 7 ( 10 ): 273 – 275 . 
 L ü ke ,  H. D.  1999 .  The origins of the sampling theorem .  IEEE Communications Magazine 
 37 ( 4 ): 106 – 108 . 
 McClellan ,  J. H. , and  T. W.  Parks .  1973 .  A uniﬁ ed approach to the design of optimum 
FIR linear - phase digital ﬁ lters .  IEEE Transactions on Circuit Theory  CT - 20 ( 6 ): 697 – 
701 . 
 Mecklenbra ü ker ,  W. F. G.  2000 .  Remarks on and correction to the impulse invariant method 
for the design of IIR digital ﬁ lters .  Signal Processing  80 : 1687 – 1690 . 
 Meijering ,  E.  2002 .  A chronology of interpolation: from ancient astronomy to modern signal 
and image processing .  Proceedings of the IEEE  90 ( 3 ): 319 – 342 . 
 Narashima ,  M. J. , and  A. M.  Peterson .  1978 .  On the computation of the discrete cosine trans-
form .  IEEE Signal Processing Magazine  COM - 26 ( 6 ): 934 – 936 . 
 Nyquist ,  H.  1928 .  Certain topics in telegraph transmission theory .  American Institute of 
Electrical Engineers  47 : 617 – 644 . Reprinted Proc IEE, 90(2), Feb 2002. 
 Nyquist ,  H.  2002 .  Certain topics in telegraph transmission theory .  Proceedings of the IEEE 
 90 ( 2 ): 280 – 305 . Reprint of classic paper. 
 Pitas ,  I. , and  A. N.  Venetsanopoulos .  1992 .  Order statistics in digital image processing . 
 Proceedings of the IEEE  80 : 1893 – 1921 . 
 Radon ,  J.  1986 .  On the determination of functions from their integral values along certain 
manifolds .  IEEE Transactions on Medical Imaging  MI - 5 ( 4 ): 170 – 176 . Translated from the 
German text of 1917 by P. C. Parks. 
 Shannon ,  C. E.  1948 .  A mathematical theory of communication .  Bell System Technical 
Journal  27 : 379 – 423 ,  623 – 656 .  http://cm.bell - labs.com/cm/ms/what/shannonday/paper.
html . 
 Shepp ,  L. A. , and  B. F.  Logan .  1974 .  Reconstructing interior head tissue from X - ray transmis-
sions .  IEEE Transactions on Nuclear Science  21 ( 1 ): 228 – 236 . 
 Stockham ,  T. G.  1966 .  High - speed convolution and correlation .  AFIPS Joint Computer 
Conferences  AU - 15 : 229 – 233 . 
 Stylianou ,  Y.  2000 .  A simple and fast way of generating a harmonic signal .  IEEE Signal 
Processing Letters  7 ( 5 ): 111 – 113 . 
 Tustin ,  A.  1947 .  A method of analysing the behaviour of linear systems in terms of time series . 
 Journal of the Institution of Electrical Engineers  94 : 130 – 142 . 
 Widrow ,  B.  2005 .  Thinking about thinking: the discovery of the LMS algorithm .  IEEE Signal 
Processing Magazine  22 ( 1 ): 100 – 106 . 
 Wilson ,  D. R. ,  D. R.  Corrall , and  R. F.  Mathias .  1973 .  The design and application of digital 
ﬁ lters .  IEEE Transactions on Industrial Electronics and Control Instrumentation 
 IECI - 20 ( 2 ): 68 – 74 . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
BIBLIOGRAPHY 
377
 SELECTED TEXTBOOKS 
 Acharya ,  T. , and  P. - S.  Tsai .  2005 .  JPEG2000 Standard for Image Compression .  Hoboken, NJ : 
 Wiley . 
 Elali ,  T. S. , ed.  2004 .  Discrete Systems and Digital Signal Processing with MATLAB .  Boca 
Raton, FL :  CRC . 
 Hamming ,  R. W.  1983 .  Digital Filters .  Englewood Cliffs, NJ :  Prentice - Hall . 
 Harris ,  R. W. , and  T. J.  Ledwidge .  1974 .  Introduction to Noise Analysis .  London :  Pion . 
 Haykin ,  S. n.d..  Adaptive Filter Theory .  Englewood Cliffs, NJ :  Prentice - Hall . 
 Ifeachor ,  E. C. , and  B. W.  Jervis .  1993 .  Digital Signal Processing: A Practical Approach . 
 Boston :  Addison - Wesley . 
 Ingle ,  V. K. , and  J. G.  Proakis .  2003 .  Digital Signal Processing Using MATLAB .  Stamford, 
CT :  Thomson Brooks Cole . 
 Jain ,  A. K.  1989 .  Fundamentals of Digital Image Processing .  Englewood Cliffs, NJ :  Prentice - 
Hall . 
 Jayant ,  N. S. , and  P.  Noll .  1984 .  Digital Coding of Waveforms .  Englewood Cliffs, NJ :  Prentice - 
Hall . 
 Kreyszig ,  E. n.d..  Advanced Engineering Mathematics .  Boston :  Addison - Wesley . 
    Kronenburger ,  J.  , and   J.   Sebeson  .  2008 .  Analog and Digital Signal Processing: an Integrated 
Computational Approach with MATLAB .  Clifton Park, NY :  Thomson Delmar Learning . 
 Kumar ,  B. P.  2005 .  Digital Signal Processing Laboratory .  Oxford :  Taylor and Francis . 
 Lathi ,  L. P.  1998 .  Signal Processing and Linear Systems .  Carmichael :  Berkeley Cambridge 
Press . 
 Leis ,  J. W.  2002 .  Digital Signal Processing: A MATLAB - Based Tutorial Approach .  Baldock, 
UK :  Research Studies Press . 
 Lockhart ,  G. B. , and  B. M. G.  Cheetham .  1989 .  Basic Digital Signal Processing .  London : 
 Butterworths . 
 Lyons ,  R. G.  1997 .  Understanding Digital Signal Processing .  Boston :  Addison Wesley . 
 Mitra ,  S. K.  2001 .  Digital Signal Processing: a Computer - Based Approach .  New York : 
 McGraw - Hill . 
 Oppenheim ,  A. V. , and  R. W.  Schafer .  1989 .  Discrete - Time Signal Processing .  Englewood 
Cliffs, NJ :  Prentice - Hall . 
 Orfanidis ,  S. J.  1996 .  Introduction to Signal Processing .  Englewood Cliffs, NJ :  Prentice - 
Hall . 
 Papoulis ,  A.  1980 .  Circuits and Systems: A Modern Approach .  New York :  Holt - Saunders . 
 Pennebaker ,  W. B. , and  J. L.  Mitchell .  1992 .  JPEG: Still Image Data Compression Standard . 
 New York :  Van Nostrand Reinhold . 
 Porat ,  B.  1997 .  A Course in Digital Signal Processing .  New York :  John Wiley & Sons . 
 Proakis ,  J. G. , and  D. G.  Manolakis . n.d..  Digital Signal Processing: Principles, Algorithms, 
and Applications .  Englewood Cliffs, NJ :  Prentice - Hall . 
 Rosenfeld ,  A. , and  A. C.  Kak .  1982 .  Digital Picture Processing .  Burlington, VT :  Academic 
Press/Elsevier . 
 Terrell ,  T. J. , and  L. - K.  Shark .  1996 .  Digital Signal Processing: A Student Guide .  New York : 
 Macmillan/McGraw - Hill . 
 Widrow ,  B. , and  S. D.  Stearns .  1995 .  Adaptive Signal Processing .  Englewood Cliffs, NJ : 
 Prentice - Hall . 
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
379
INDEX
Digital Signal Processing Using MATLAB for Students and Researchers, First Edition. John W. Leis.
© 2011 John Wiley & Sons, Inc. Published 2011 by John Wiley & Sons, Inc.
2’s complement, 48
A/D converter, 45
aliasing, 81–83, 231
analog ﬁ lter conversion, 340
anti-aliasing ﬁ lter, 82
arithmetic
ﬁ xed point, 52
ﬂ oating point, 53
integer, 50
audio quantization, 67
autocorrelation, 165
average, 106
backprojection algorithm, 191
bandpass ﬁ lter, 366
bilinear transform, 349
binary format, 34
binary multiplication, 51
binary representation, 47
bit allocation
audio & sound, 67
images, 67
SNR, 69
bitwise operators, 47
block diagram, 88
Butterworth ﬁ lters, 326–331
C programming language, 20
Chebyshev ﬁ lters, 331–339
color mixing, 77
color palette, 78–79
complex Fourier series, 205
continuous Fourier transform, 212
convolution, 156–159
deﬁ nition, 158
example, 157
impulse response, 156
overlap-add, 310–311
overlap-save, 311–312
convolution via FFT, 364–368
CORDIC, 56
correlation, 165
common signals, 171
noise removal, 204
cross-correlation, 168–169
cumulative probability, 108–109
D/A converter, 45
data ﬁ les
binary, 31–33
text, 34–35
DCT
deﬁ nition, 252
fast algorithm, 269
DCT. See discrete cosine transform
deterministic signals, 103
DFT. See discrete Fourier Transform
difference equation, 88
digital ﬁ lters, 271
Direct Digital Synthesis (DDS), 131
discrete cosine transform, 252
discrete Fourier transform (DFT), 
216–231
compared to Fourier series, 218
component scaling, 219
examples, 220–231
frequency scaling, 272
discrete-time ﬁ lters, 271
discrete-time waveforms, 127
distributions
Gaussian, 112
generating, 120
ECG
ﬁ ltering, 271
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
380 
INDEX
fast convolution, 299
fast correlation, 301
fast Fourier transform (FFT), 244–252
bit reversal, 252
complexity, 249
derivation, 246
interpolation, 233
FFT see fast Fourier Transform
ﬁ lter
bandpass, 283
bandstop, 283
highpass, 283
lowpass, 382
ﬁ lter design, 274
direct method, 285
frequency sampling method, 292
ﬁ lter scaling, 362
ﬁ ltering
overlap-add, 310–311
overlap-save, 311–312
ﬁ lters
lowpass to bandpass, 366
lowpass to highpass, 364
phase linearity, 294
scaling, 362
speciﬁ cation, 274
ﬁ nite impulse response (FIR), 157
FIR. See ﬁ nite impulse response
FIR ﬁ lters, 285
direct design method, 285
frequency sampling design method, 
292
FIR vs. IIR ﬁ lters, 316
ﬁ xed point arithmetic, 52
ﬂ oating point arithmetic, 53
ﬂ oating-point format, 53
Fourier series, 203–209
derivation, 209–210
phase shift, 211
Fourier series equations, 205
Fourier transform, 212
frequency response, 152
complex vector interpretation, 
153–155
MATLAB, 155–156
frequency scaling of ﬁ lters, 362
gain/phase response, 273
Gaussian distribution, 112
grayscale, 41–42
Hamming window, 289
heart waveform, 116
highpass ﬁ lter, 283
histogram, 114–115
histogram equalization, 118–120
histogram operators, 117
IIR. See inﬁ nite impulse response
IIR ﬁ lters, 316
image display, 74
image quantization, 67–69
image scanning, 75
impulse response, 88
impulse-invariant transform, 340–348
inﬁ nite impulse response (IIR), 158–159
integer arithmetic, 50
interpolation, 87–88, 233–236
joint probability, 117
JPEG images, reading, 42
Lagrange polynomial, 87–88
Laplacian distribution, 113
linear prediction, 177
linear time-invariant model, 92–95
linearity, 92
lookup table
waveform samples, 130–131
lookup table, palette, 78–79
MATLAB
arguments to functions, 26
audio reading, 35
calling a function, 26
function arguments, 26
function example, 28–29
function return values, 26
functions, 26
image reading, 35
m-ﬁ les, 20
matrix/vector dimensions, 23
multidimensional arrays, 35
obtaining, 3, 19
path, 25
search path, 26
startup ﬁ le, 26
MATLAB code
aspect ratio, 41, 42
audio playback, 42
correlation, 169–170
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
INDEX 
381
cumulative probability, 108–109
difference equation using ﬁ lter, 92
difference equation: pole locations, 
147
difference equation: sampled exponential 
signal, 140
difference equations, 88–92
direct DFT computation, 219
direct DFT computation using vectorized 
loop, 220
discrete frequency response, 
155–156
FFT interpolation, 233
ﬁ lter function & difference equations, 
144
FIR direct method, 287
FIR window method, 291
ﬁ rst-order difference equation, 89
Fourier series, 209
Fourier series & phase shift, 
211–212
grayscale image display, 41
grayscale image example, 79–80
Hamming window, 290
pole-zero plot, 149
probability density, 110
pseudo-random numbers, 106
random image display, 41
random number generation, 41
random number realizations, 106
sampled exponential signal, 140
stair plots, 63
stem plots, 63
waveform via direct calculation, 130
MATLAB function
addpath, 26
axis, 31, 41
bitand, 38
cat, 35–37
clear all, 20
close all, 20
colormap, 42
conj, 148
fft, 220
fgetl, 32
ﬁ lter, 92, 144
fopen, 32
fprintf, 29
fread, 34
fscanf, 32
fwrite, 35
hist, 116
ifft, 230
image, 41
imread, 42
legend, 30
load, 31
plot, 30
poly, 148
reshape, 234
residue, 346
roots, 148
save, 31
sort, 29
sound, 40, 137
stairs, 63
stem, 63
wavread, 35
zeros, 39
MATLAB function help, 20
MATLAB function lookfor, 29
MATLAB operator
for, 24
if, 24
while, 24
mean, 106
arithmetic, 107
population, 107
sample, 107
mean square, 107
median, 107
median ﬁ lter, 122
mode, 107
multidimensional arrays, 35
noise cancellation, 184
notch ﬁ lter, 278
numerical precision, 52
optimal ﬁ lter, 183
overlap-add algorithm, 310–311
overlap-save algorithm, 311–312
palette, 78–79
Parseval’s Theorem, 268
PDF, 109
periodic function, 204
poles of a system, 146
probability density function, 109
pseudo-random, 106
www.it-ebooks.info
WWW.EBOOK777.COM

free ebooks ==>   www.ebook777.com
382 
INDEX
quantization, 64
audio, 70–71
image, 68–69
SNR, 69–74
quantizer characteristic, 64
Radon transform, 189
random signals, 103
random variable generation, 120
reconstruction, 62, 293
reconstruction ﬁ lter, 62
recursive ﬁ lters, 315
sample buffers, 241
sampling, 46, 61
sampling impulses, 62
sampling terminology, 129
sampling windows, 236
Shepp-Logan head phantom, 12
Shepp-Logan phantom, 188
signal model, 105
signal processing
algorithm, 1
applications, 3
case studies, 4
deﬁ nition, 1
real-time, 2
signal-to-noise ratio, 69
sinc function, 86, 293–294
stability, 146
sunspot observations, 204
superposition, 92
system identiﬁ cation, 175
time-frequency distributions, 240
time-invariance, 92
tomography, 188
deﬁ nition, 11
transfer function, 139
two’s complement, 48
uniform distribution, 111
uniform PDF, 111
variance, 108
waveform generation
z transform, 137
direct calculation, 128
double buffering, 137
lookup table, 130
phase correction, 133
recurrence relation, 131
Wiener ﬁ lter, 184
window function
Hamming, 2243, 289
Hanning, 243
triangular, 242
z delay operator, 138
z transform examples, 140–144
zero-order hold, 61–62
www.it-ebooks.info
WWW.EBOOK777.COM


DISTRIBUTED 
AND 
PARALLEL 
DATABASES 
Volume 8-2000 
SPRINGER SCIENCE+BUSINESS MEDIA, LLC 

SECURITY OF DATA AND 
TRANSACTION PROCESSING 
edited by 
Vijay Atluri 
Rutgers University 
Pierangela Samarati 
SRI International 
A Special Issue of 
DISTRIBUTED AND PARALLEL DATABASES 
Volume 8, No. 1 (2000) 
111... " 
SPRINGER SCIENCE+BUSINESS MEDIA, LLC 

Library of Congress Cataloging-in-Publication Data 
Security of data and transaction processing / edited by Vijay Atluri,Pierangela Samarati. 
p.cm. 
"A special issue of Distributed and parallel databases, volume 8, no.1 (1999)." 
Includes bibliographical references and index. 
ISBN 978-1-4613-7009-3 
ISBN 978-1-4615-4461-6 (eBook) 
DOI 10.1007/978-1-4615-4461-6 
1. Computer security. 2. Data protection 3. Database security I.Atluri,Vijay, 1956-
II. Samarati,Pierangela 
QA76.9.A25 S435 2000 
005.8--dc21 
99-089328 
Copyright @ 2000 Springer Science+Business Media New York 
Originally published by Kluwer Academic Publishers, New York in 2000 
Softcover reprint of the hardcover 1 st edition 2000 
Ali rights reserved. No part of this publication may be reproduced, stored in a 
retrieval system or transmitted in any form or by any means, mechanical, photo-
copying, recording, or otherwise, without the prior written permission of the 
publisher, Springer Science+Business Media, LLC. 
Printed an acid-free paper. 

DISTRIBUTED 
AND 
PARALLEL 
DATABASES 
Volume 8, No.1, January 2000 
Special Issue: Security of Data and Transaction Processing 
Guest Editor's Introduction ............... Vijay Atluri and Pierangela Samarati 
Rewriting Histories: Recovering from Malicious Transactions ................. . 
· .................................... Peng Liu, Paul Ammann and Sushil Jajodia 
3 
Secure Concurrency Control in Firm Real-Time Database Systems ............. . 
· ............................................ Binto George and Jayant R. Haritsa 
37 
A Secure Agent-based Framework for Internet Trading in Mobile Computing 
Environments .... Xun Yi, Chee Kheong Siew, Xiao Feng Wang and Eiji Okamoto 
81 
Providing Security and Interoperation of Heterogeneous Systems .............. . 
· .......................... Steven Dawson, Shelly Qian and Pierangela Samarati 
115 

,~, 
Distributed and Parallel Databases, 8, 5-6 (2000) 
'II1II" © 2000 Kluwer Academic Publishers. Manufactured in The Netherlands. 
Guest Editor's Introduction 
The past two decades have witnessed a phenomenal growth in distributed and multiuser com-
puting and connectivity. Usage of computers to process sensitive and crucial information 
in governmental, military, as well as commercial sectors has been increasing considerably. 
Every activity in both public and private organizations is today depending on the correct-
ness, availability, and secrecy of the information stored or processed within a computer 
system or transmitted over a network. New technological developments and increased au-
tomation of business processes are contributing towards a more efficient and powerful way 
of managing and accessing such large quantity of data. These include the proliferation of 
the Internet and the World Wide Web, the introduction of electronic commerce and digital 
libraries, and the application of workflow systems. At the same time, however, this "evo-
lution" has also increased both the vulnerability of systems to security violations and the 
damage that such violations may cause. Consequently, security issues are today of great 
concern to both researchers and practitioners involved with data management. As a com-
plicating aspect, new technologies have also introduced new security requirements and new 
research challenges that have not been addressed before. Reacting to this increasing need 
for security, researchers and developers have contributed significantly to advancements in 
the theory, design, implementation, analysis, and application of secure computer systems 
and networks. This special issue on security of the Distributed and Parallel Databases: An 
International Journal, comprising of four research papers, is an attempt to provide a insight 
into this research. 
The first paper, "Rewriting Histories: Recovering from Malicious Transactions," by Peng 
Liu, Paul Ammann, and Sushil Jajodia, addresses the problem of providing countermeasures 
for repairing a database whose integrity has been compromised by malicious transactions. It 
proposes solutions for rewriting execution histories that back out malicious, but committed, 
transactions, while preserving the work of good transactions. The prefix of a rewritten 
history serializes exactly the set of unaffected good transactions. The paper also provides 
techniques that can extract additional good transactions from the suffix of the rewritten 
history, thus saving more good transactions than is possible with a dependency-graph based 
approach to recovery. 
The second paper, "Secure Concurrency Control in Firm Real-Time Database Systems," 
by Binto George and Jayant Haritsa, investigates the performance implications, in terms of 
killed transactions, of guaranteeing secrecy in multilevel secure real-time database systems 
supporting applications with firm deadlines. It proposes an approach to simultaneously 
use different concurrency control mechanisms for guaranteeing security while improving 
real-time performance, and an adaptive admission-control policy to provide fairness with 
respect to the distribution of killed transactions across security levels. 
The third paper, "A Secure Agent Based Framework for Internet Trading in Mobile 
Computing Environments," by Xun Yi, Siew Chee Kheong and Eiji Okamoto, proposes 

6 
ATLURI AND SAMARA TI 
a secure agent-based payment system for mobile computing on the Internet. The system, 
called SET/A+, builds on the well known SET secure payment protocol for non-mobile 
environments and makes improvement on the SET/A for mobile environments. The goal 
of SET/A+ is to protect an agent's confidential data against a malicious merchant without 
requiring, like SET/A, a secure execution environment at the merchant's server. The paper 
illustrates how this goal can be achieved through the inclusion of a verification server into 
the payment system. 
Finally, the fourth paper, "Providing Security and Interoperation of Heterogeneous Sys-
tems," by Steven Dawson, Shelly Qian and Pierangela Samarati, presents an approach to 
providing remote and distributed applications with access to multiple sources in such a way 
that security and autonomy of each source is preserved. Security requirements at each source 
are expressed through a mandatory policy, with each source labeling its data according to 
a local classification lattice. Cross lattice relationships between the application lattice and 
the lattices at each source are used to determine the visibility of application subjects. The 
paper illustrates the stating, consistency control, and enforcement of such constraints upon 
application schema definition and query execution. It also presents the system architecture, 
based on mediator and source wrappers, to provide secure interoperation. 
Many high quality papers could not be accepted due to space limitations. We would 
like to thank all the authors for submitting their research results, and the reviewers for 
their insightful reviews. We are grateful for Ahmed Elmagarmid, editor-in-chief, for his 
continuous encouragement throughout this project, and Melissa Parson and Melissa Fearon 
of Kluwer for handling the logistics of getting the special issue through the reviewing and 
publication process. 
Vijay Atluri 
Pierangeia Samarati 
2 

•
~.. 
Distributed and Parallel Databases, 8, 7-40 (2000) 
© 2000 Kluwer Academic Publishers, Manufactured in The Netherlands, 
Rewriting Histories: Recovering 
from Malicious Transactions 
PENG LIU 
pliu@umbc.edu 
Department of Information Systems, University of Maryland, Baltimore Cuunty, Baltimure, MD 212 150, USA 
PAUL AMMANN 
SUSHIL JAJODIA 
Center for Secure Information Systems, George Mason University, Fairfax, VA 22030, USA 
Recommended by: 
Vijay Atluri and Pierangela Samarati 
pammann@gmu.edu 
jajodia@gmu.edu 
Abstract. We consider recovery from malicious but committed transactions. Traditional recovery mechanisms 
do not address this problem, except for complete rollbacks, which undo the work of good transactions as well as 
malicious ones, and compensating transactions, whose utility depends on application semantics. We develop an 
algorithm that rewrites execution histories for the purpose of backing out malicious transactions. Good transactions 
that are affected, directly or indirectly, by maJicious transactions complicate the process of backing out undesirable 
transactions. We show that the prefix of a rewritten history produced by the algorithm serializes exactly the set 
of unaffected good transactions. The suffix of the rewritten history includes special state information to describe 
affected good transactions as well as maJicious transactions. We describe techniques that can extract additional 
good transactions from this latter part of a rewritten history. The latter processing saves more good transactions 
than is possible with a dependency-graph based approach to recovery. 
Keywords: 
information warfare, trusted recovery, databases, security, maJicious transactions, transaction 
histories 
1. 
Introduction 
Preventive measures sometimes faiL In the database context, some transactions that 
shouldn't commit do anyway, Undesirable committed transactions can arise from mali-
cious activity by a well-equipped attacker in many circumstances, these transactions are 
referred as malicious transactions, In tightly integrated networks, the damage caused by ma-
licious transactions can spread quickly from the initial source, In this paper, we focus on one 
aspect of planning for and responding to such damage; specifically, we develop a family of 
algorithms for rewriting execution histories that back out a set of malicious (but committed) 
transactions while preserving the work of good transactions, namely committed transac-
tions that arise from legitimate activity, Traditional recovery mechanisms do not address 
this problem, except for complete rollbacks, which undo the work of malicious transactions 
as well as good ones, and compensating transactions, whose utility depends on application 
semantics, We show that our approach is strictly better at saving good transactions than a 
dependency-graph based approach, 

8 
LTV, AMMANN AND JAJODIA 
1.1. 
Information waifare 
Experience with traditional information systems security practices (INFOSEC) has shown 
that it is very difficult to adequately anticipate the abuse and misuse to which an information 
system will be subjected in the field. The focus ofINFOSEC is prevention: security controls 
aim to prevent malicious activity thal interferes with either confidentiality, integrity, or 
availability. However, outsiders (hackers) have proved many times that security controls 
can be breached in imaginative and unanticipated ways. Further, insiders have significant 
privileges by necessity, and so are in a position to inflict damage. Finally, the dramatic 
increase in internetworking has led to a corresponding increase in the opportunities for 
outsiders to masquerade as insiders. Network-based attacks on many systems can now be 
carried out from anywhere in the world. Although mechanisms such as firewalls reduce the 
threat of outside attack, in practice such mechanisms cannot eliminate the threat without 
blocking legitimate use as well. Tn brief, strong prevention is clearly necessary, but less and 
less sufficient, to protect information resources. 
In response to problems with the INFOSEC approach, a complementary approach with 
an emphasis on survivability has emerged. J This 'information warfare' (IW) perspective 
is that not only should vigorous INFOSEC measures be taken to defend a system against 
attack, but that some attacks should be assumed to succeed, and that countermeasures to 
these successful attacks should be planned in advance. The IW perspective emphasizes the 
ability to live through and recover from attacks. 
The timeline in an IW scenario includes traditional preventive measures to harden a 
system against attack, intelligence gathering by the adversary to detect weaknesses in 
the resulting system, attack by the adversary, and finally countermeasures to the attack. 
Typical countermeasure phases are attack detection, damage confinement and assessment, 
reconfiguration, damage repair, and fault treatment to prevent future similar attacks. In this 
paper, we focus on one specific countermeasure phase to an information attack, namely the 
damage repair phase. 
Although the IW adversary may find many weaknesses in the diverse components of 
an information system, databases provide a particularly inviting target. There are several 
reasons for this. First, databases are widely used, so the scope for attack is large. Second, 
information in databases can often be changed in subtle ways that are beyond the detection 
capabilities of the typical database mechanisms such as range and integrity constraints. For 
example, repricing merchandise is an important and desirable management function, but it 
can easily be exploited for fraudulent purposes. Finally, unlike most system components, 
many databases are explicitly optimized to accommodate frequent updates. The interface 
provides the outside attacker with built in functions to implement an attack; all that is ncc-
essary is to acquire sufficient privileges, a goal experience has shown is readily achievable. 
Advanced authorization services can reduce such a threat, but never eliminate it, since in-
sider attacks are always possible, and also since system administrators are only human, and 
hence prove to making mistakes in configuring and managing authorization services. 
Integrity, availability, and (to a lesser degree) confidentiality have always been key 
database issues, and commercial databases include diverse set of mechanisms towards these 
ends. For example, access controls, integrity constraints, concurrency control, replication, 
active databases, and recovery mechanisms deal well with many kinds of mistakes and 
4 

RECOVERING FROM MALICIOUS TRANSACTIONS 
9 
errors. However, the IW attacker can easily evade some of these mechanisms and exploit 
others to further the attack. For example, access controls can be subverted by the inside 
attacker or the outside attacker who has assumed an insider's identity. Integrity constraints 
are weak at prohibiting plausible but incorrect data; classic examples are changes to dollar 
amounts in billing records or salary figures. To a concurrency control mechanism, an at-
tacker's transaction is indistinguishable from any other transaction. Automatic replication 
facilities and active database triggers can serve to spread the damage introduced by an at-
tacker at one site to many sites. Recovery mechanisms ensure that committed transactions 
appear in stable storage and provide means of rolling back a database, but no attention is 
given to distinguishing legitimate activity from malicious activity. In brief, by themselves, 
existing database mechanisms for managing integrity, availability, and confidentiality are 
inadequate for detecting, confining, and recovering from IW attacks. 
1.2. 
Contribution 
The specific problem we address in this paper is 'how can one repair a database, given that a 
set of malicious transactions has been identified.' The identification of the set of malicious 
transactions is outside the scope of this paper. In an IW context, such identification takes 
place in an earlier countermeasure phase. For example, identification of an attacker may 
lead directly to the identification of a set of malicious transactions. 
Our contribution is to provide an algorithm that rewrites an execution history so that 
malicious transactions are as near the end of the history as possible, given the read-write 
dependencies between transactions. The prefix of the rewritten history consists solely of 
good transactions; we show that this prefix is equivalent to using a write-read dependency 
graph to unwind malicious transactions and those good transactions that depend, directly 
or indirectly, on the malicious transactions. We then show how to use the latter part of the 
rewritten history to save additional good transactions. 
Although we develop these algorithms to repair a database when some malicious activ-
ity happens, our methods can be easily extended to other applications where some com-
mitted transactions may also be identified undesirable, thus have to be backed out. For 
example 
• In [19], the use of isolation is proposed to protect systems from the damage caused 
by authorized but malicious users, masqueraders, and misfeasors, where the capacity 
of intrusion detection techniques is limited. In the database context, the basic idea is 
when a user is found suspicious, his transactions are redirected to an isolated database 
version, and if the user turns out to be innocent later, the isolated database version will 
be merged into the main database version. Since these two versions may be inconsistent, 
some committed transactions may have to be backed out to ensure the consistency of the 
database . 
• During upgrades to existing systems, particularly upgrades to software. Despite efforts 
for planning and testing of upgrades, upgrade disasters occur with distressing regularity.2 
If a system communicates with the outside world, bringing the upgrade online with 
a hot standby running the old software isn't complete protection. Problems with an 
upgrade by one organization can easily affect separate, but cooperating organizations. 
5 

10 
LIU, AMMANN AND JAJODIA 
Thus an incorrect upgrade at a given organization may result in an erroneous set of 
transactions at one or more cooperating organizations. In many cases, it is not possible 
simply to defer activity, and so during the period between the introduction of an upgrade 
and the recognition of an upgrade problem, erroneous transactions at these cooperating 
organizations commit. As a result, backing out these committed erroneous transactions 
is necessary . 
• In partitioned distributed database systems, Davidson's optimistic protocol [12] allows 
transactions to be executed within each partitioned group independently with communica-
tion failures existing between partitioned groups. As a result, serial history Hi consisting 
of all transactions executing within group Pi is generated. When two partitioned groups 
PI and P2 are reconnected, HI and H2 may conflict with each other. Therefore, some 
committed transactions may have to be backed out to resolve the conflicts and ensure the 
consistency of the database . 
• In [15], J. Gray et al. state that update anywhere-any time-anyway transactional replica-
tion has unstable behavior as the workload scales up. To reduce this problem, a two-tier 
replication algorithm is proposed that allows mobile applications to propose tentative up-
date transactions that are later applied to a master copy. The drawback of the protocol is 
that every tentative transaction must be reexecuted on the base node, thus some sensitive 
transactions may have given users inaccurate information and the work of tentative trans-
actions is lost. In this situation, the strategy that when a mobile node is connected to 
the base node merges the mobile copy into the master copy may be better, however, in 
order to ensure the consistency of the master copy after the mergence, some committed 
transactions may have to be backed out. 
1.3. 
Organization 
The outline of the paper is as follows. In Section 2 we give our model for rewriting and 
repairing histories after first describing the dependencies relevant to repair. In Section 3 we 
give an algorithm to rewrite histories and show that it is equivalent to using a dependency-
graph based approach. We tum to methods to save additional good transactions in Section 4. 
In Section 5, we show how to prune a rewritten history so that a repaired history can 
be generated. We examine the relationships among the possible rewriting algorithms in 
Section 6. In Section 7, we show how to implement our rewriting methods in a realistic 
transaction processing system which is based on the Saga model [14]. Section 8 describes 
related work, and we conclude in Section 9. 
2. 
Model 
2.1. 
Assumptions 
We assume that the histories to be repaired are serializable histories generated by some 
mechanism that implements a classical transaction processing model [8]. We denote (com-
mitted) malicious or bad transactions in a history by the set B = {Bi!' Bi2 , ... , Bim}. We 
denote (committed) good transactions in a history by the set G = {Gjl , G j2 , ..• , G jn }. 
Since recovery of uncommitted transactions is addressed by standard mechanisms, we con-
sider a history Hover BUG. We define < H to be the usual partial order on BUG for such 
6 

RECOVERING FROM MALICIOUS TRANSACTIONS 
11 
a history H, namely, T; <H Tj if <H orders operations of T; before conflicting operations 
of Tj [8]. 
We assume that the concurrency control mechanism provides an explicit serial history 
H S of history H. For example, the order of first lock release provides a serialization order 
for transactions scheduled by a strict two-phase locking mechanism. We denote the total 
order on the transactions in a serial history H S by <'H' 
We assume the availability of read information for transactions in H. Only write infor-
mation is kept in logs for traditional recovery purposes, but, as later discussion makes clear, 
read information is also necessary to unwind committed transactions. Read information 
can be captured in several ways, these approaches are discussed in Section 7. 
We assume that transactions do not issue blind writes. That is, if a transaction writes 
some data, the transaction is assumed to read the value first. Although the approach in 
this paper can be adapted to blind writes, doing so complicates the presentation. Also, we 
compare the results in this paper to those obtained by a dependency-graph based approach 
to recovery that also assumes no blind writes. 
2.2. 
Syntactic dependencies 
One simple repair is to roll back a history H S until at least the first transaction in B and then 
try to reexecute transactions in G that were undone during the rollback. The drawback of 
this approach is that many good transactions may be unnecessarily undone and reexecuted. 
Consider the history HI over (BI, G2) where 
B,: r[x]w[x] 
G2 : r[y]w[y] 
HI: BI G2 
It is clear that G2 need not be undone and reexecuted since it does not conflict with B I. 
We formalize the notion that some-but not all-good transactions need to be undone and 
reexecuted in the usual way: 
Definition 1. 
Transaction Tj is dependent upon transaction T; in a history H if there exists 
a data item x such that: 
1. Tj reads x after T; has updated x; and 
2. there are no transactions that update x between the time T; updates x and Tj reads x. 
Every good transaction that is dependent upon some bad transaction potentially needs to 
be undone and reexecuted. There are also other good transactions that also need be undone 
and reexecuted. Consider the history H2 over (BI' G2, G3): 
B1: r[x]w[x] 
G2: r[x]r[y]w[y] 
G3: r[y]w[y] 
H2: BI G2 G3 
7 

12 
LIU, AMMANN AND JAJODIA 
G3 is not dependent upon B1, but it should be undone and reexecuted, because the value of 
x which G2 reads from B] may affect the value of y which G3 reads from G2• This relation 
between G3 and B 1 is captured by the transitive closure of the dependent upon relation: 
Definition 2. 
In a history, transaction T; affects transaction Tj if the ordered pair (Tj , T;) 
is in the transitive closure of the dependent upon relation. 
It is convenient to define the dependency graph for a set of transactions S in a history as 
DG(S) = (V, E) in which V is the union of S and the set of transactions that are affected 
by S. There is an edge, T; --+ Tj , in E if T; E V, Tj E (V - S), and Tj is dependent upon T;. 
Notice that there are no edges that terminate at elements of S; such edges are specifically 
excluded by the definition. As a result, every source node in DG(B) is a bad transaction, 
and every non-source node in DG(B) is a good transaction that reads some data, directly 
or indirectly, from at least one bad transaction. We refer to the transactions associated with 
the non-source nodes in DG(B) as the affected good transactions or, more briefly, as the 
affected transactions. We denote the set of affected transactions as AG. 
As an example, consider the history H3 over (B], G2 , G3, G 4 , B5 , G 6 ): 
B\: r[x]w[x] 
G 2: r[x]w[x]r[y]w[y] 
G 3: r[y]w[y] 
G4 : r[z]w[z] 
B5: r[z]w[z] 
G6: r[y ]w[y ]r[z]w[z] 
H3: Bl G2 G3 G4 B5 G6 
DG(B) is shown in figure 1. 
If a good transaction is not affected by any bad transaction (for example, G4 in H3), 
then the good transaction need not be undone and reexecuted. In other words, only the 
transactions in DG(B) potentially need be undone, and only transactions in AG potentially 
need to be reexecuted. From the recovery perspective, the goal of a dependency-graph 
based approach to recovery is to first get DG(B), then undo all these transactions. 
Figure 1. 
Dependency graph for history H3. 
8 

RECOVERING FROM MALICIOUS TRANSACTIONS 
Legend 
- - -- read-write edge 
___ write-read edge 
Figure 2. 
Zone of repair. 
13 
Zone of repair 
Figure 2 illustrates the dependency-graph based approach to backing out bad and affected 
transactions. In particular, it illustrates the importance of distinguishing between read-write 
and write-read dependencies during recovery. A read-write edge can leave the 'zone of 
repair' without causing the zone to expand. On the other hand a write-read edge potentially 
expands the zone. Note that due to the assumption of no blind writes, there are no write-write 
edges in the graph. 
In this example, a possible history H4 is 
the set AG = {AG 3 , AG7, AGs, AGll }, and the dependency-graph based recovery algo-
rithm restores the before values for all data items written by transactions in the set B U AG. 
The result is a serializable history over G - AG: 
The approach of rewriting histories developed in this paper has the advantage that it preserves 
ordering information for transactions in B U AG, thereby providing a basis for saving 
additional transactions in AG. 
2.3. 
Rewriting histories 
For a serial history HS, we augment H' with explicit database states so that the result is 
a sequence of interleaved transactions and database states. The sequence begins and ends 
with a state. The state that immediately precedes a transaction in H S is called the before 
state; the state that immediately follows a transaction in H' is called the after state. For an 
9 

14 
example, consider the augmented history 
where 
B] : if x> 0 then y:= y + z + 3 
Gz : x:= x-I 
The states associated with H; are: 
so={x=l; y=7; z=2} 
s] = {x = 1; y = 12; z = 2} 
Sz = {x = 0; y = 12; z = 2} 
LIU, AMMANN AND JAJODIA 
In rewriting histories, the general goal is either to move bad transactions towards the end of 
a history or to move good transactions towards the beginning of a history. It turns out that the 
transformations do not necessarily result in a serializable history which is conflict-equivalent 
or view-equivalent to the original history [8]. The lack of serializability is justified by the 
observation that bad transactions ultimately must be backed out anyway along with some 
or all of the affected transactions. Hence the serializability of such transactions is not a 
requirement. 
The example above helps to clarify this point. The serial history Hs is clearly not conflict-
equivalent to the serial history GzB] since there is a read-write dependency from B] to Gz. 
However, Gz is not affected by B], and simply restoring y with the appropriate value from 
the log not only repairs the damage caused by B], but preserves the effects of the good 
transaction Gz. 
However, It turns out that rewriting histories for recovery purposes requires some care 
with respect to state-equivalence of histories. Two augmented histories Hi and H2 are 
equivalent if they are over the same set of transactions and the final states are identical. 
To clarify this point, consider the above example again. After we make the transformation 
of exchanging the order of Gz and B], Hi is clearly not equivalent to the serial history 
G2B] since they result in different final states. At this situation, if Hi has more transactions 
following B) Gz, i.e., G3G4 ... Gn , then this transformation changes the before state of G3• 
As a result, after the transformation the rewritten history may not be consistent any longer 
because the precondition of some G i, 3 ~ i ~ n, may not be satisfied any more. Even if the 
rewritten history is still consistent, the behaviors and effects of G3, G4 , •.. , and Gn may 
have changed a lot, thus the original execution log may tum out to be useless. Moreover, 
the rewritten history usually can not result in the same final state, and the new final state 
is usually very difficult to get, thus semantics-based compensation is disabled. Therefore, 
keeping the equivalence of rewritten histories during a rewrite is essential to the success of 
the rewrite. 
10 

RECOVERING FROM MALICIOUS TRANSACTIONS 
15 
We approach this problem by decorating each transaction T in an augmented history H S 
with special values for read purposes by T. The decoration is facilitated by the notationjix 
which is specified below. 
Definition 3. 
A fix for transaction T; in history HS, denoted F;, is a set of variables read by 
T given values as in the original position of T in H'. That is, F; = {(Xl, vd . ... , (xn, vn)}, 
and V; is what T; read for X; in the original history. 
The notation T; F, indicates that the values read by T; for variables in F; should not come 
from the before state of T;, but from F;. 
To reduce notational clutter, we show just the variable names in F; and omit the associated 
values. 
Consider the augmented history Ht = So B] s] G2 Sz above. As discussed, the history 
with 
s3 = {x = 0; 
y = 7; 
z = 2} 
results in a different value of y in the final state, but the history 
ends in final state Sz for FI = {x}. States s] and S3 differ in the value of x; this discrepancy 
is captured by Fl, where x is associated with the value 1, which is the value Bj read for x 
in the original history Ht. 
In what follows, each transaction T; is assumed to have an associated fix F;. For ordinary 
serializable execution histories, each such fix F; = 0, the empty fix. In the example above, 
the two histories 
Ht = So Br s] G~ sz 
H4 = So G~ S3 B;x} S2 
are equivalent. 
2.4. 
Repaired histories 
Definition 4. 
Given a history H' over BUG, H: is a repaired history of H' if 
1. H: is over some subset of G, and 
2. There exists some history H; over BUG such that 
11 

16 
(a) H: is a prefix of H: and 
(b) H: and H S are equivalent. 
LID, AMMANN AND JAJODIA 
Our notion of a repaired history is that only good transactions remain (condition 1) and 
further that there is some extension to the repair that captures exactly the same transformation 
to the database state as the original history (condition 2). 
We note that the dependency-graph based approach satisfies the first part of the definition 
of a repaired history where the subset of Gis G - AG. As an example, in figure 2 history 
H~r is a repair of H~ since H~r is over {C2, C 4 , C 6 , C 9 , CIQ, C l2 } which is a subset of G 
and the necessary history H~e exists: 
H s 
C C C C C 
C 
BFJ AC FJ BFs AC F7 AC F, AC FII 
4e = 
2 
4 
6 
9 
10 
12 
1 
3 
5 
7 
g 
11 
for appropriate fixes F1, F3, Fs, F7 , Fg and FI I. Details of how to construct fixes are dis-
cussed later in the paper. 
Armed with a definition of repairs to histories, we are now ready to consider algorithms 
to construct them. 
3. 
Basic algorithm to rewrite a history 
3.1. 
Can-follow relation 
We denote the set of items read or written by a transaction T as T.readset or T.writeset, and 
the set of items read or written by a sequence of transactions R = TI T2 ... Tn as R .readset 
or R .writeset. Due to our assumption of no blind writes, R.writeset ~ R.readset. 
Definition 5. 
Transaction T can follow a sequence of transactions R if 
T.writeset n R.readset = IZl 
There are some properties of can follow: 
I. If T; .writeset is not empty, then transaction T; can not follow itself. 
2. The fact that T; can follow transaction Tj and Tj can follow transaction Tk does not 
imply that T; can follow Tk. 
3. Read-only transactions can follow any transaction. 
The can follow relation captures the notion that a transaction T can he moved to the 
right past a sequence of transactions R if no transaction in R reads from T. The can follow 
relation ensures then the cumulative etTects of the transactions in R on the database state 
are identical both before and after T is moved. The following lemma shows that the can 
follow relation can be repeatedly used to rewrite a history. 
Lemma 1. 
Transaction T canfollow a sequence of transactions R iffT can follow every 
transaction in R. 
12 

RECOVERING FROM MALICIOUS TRANSACTIONS 
17 
Proof: 
if: For every transaction Ti in R, T.writeset n ~ .readset = 0 because T can 
follow h Therefore T.writeset n R.readset = 0, so T can follow R. 
only if: By contradiction, assume there is a transaction ~ in R such that T cannot 
follow Ti, then T.writesetn ~.readset i- O. Therefore, T.writesetn R.readset i- 0, which 
contradicts the assumption that T can follow R. 
D 
3.2. 
Can-follow rewriting 
The can follow relation can be used to rewrite a history to move transactions in G - AG to 
the beginning of the history, namely, move transactions in B U AG backwards. 
Algorithm 1 
Can-Follow Rewriting 
Input: the serial history H S to be rewritten and the set B of bad transactions. 
Output: 
a rewritten history with transactions in G - AG preceding transactions in 
BUAG. 
Method: Scan forward from the first good transaction after B] until the end of HS, for 
each transaction T 
case T E B 
skip it; 
caseTEG 
if each transaction between B] and T (including B]) can follow T, then move 
T to the position immediately preceding B I. 
Algorithm 1 does not describe how to compute the fix with any transaction which has 
some transaction being moved to the left of it. The reason is that repair can simply be 
accomplished by undo. However, if we want to save some of the transactions in AG 
then we need to maintain the fix information for these transactions. Fixes are computed as 
follows: 
Lemma 2. 
Suppose transaction T can follow sequence R in history Hi' = So T F, s] R 
S2· Then for fix 
F2 = FI U (T.readset n R.writeset) 
history Hf. = So R S3 TF2 S2 is equivalent to Ht. The values associated with each data 
item in the fixes are those originally read by T. 
Proof: 
Consider some database item x E S2. x is not an element of both R. writeset and 
T.writeset since otherwise the relation T can follow R would not hold. If x is an element 
of R. writeset, then the value computed by R for x is the same in both Hi' and Hf. since R 
does not read from T. If x is an element of T. writeset, then the value computed by T for 
x is the same in both H{ and Hf. since T reads identical values for elements in T.readset 
in both histories, courtesy of fixes FI and F2, respectively. If x is not an element of either 
T.writeset or R.writeset, then the order of T and R is irrelevant to the value of x. 
D 
The correctness of Algorithm 1 is specified as follows. 
13 

18 
LID, AMMANN AND JAJODIA 
Theorem 1. 
Given a history H S , Algorithm 1 produces a history H: with a prefix H: 
such that: 
1, All and only transactions in G - AG appear in H:. 
2. H: and H S order transactions in G - AG identically. And they order transactions in 
B U AG identically. 
3. The fix associated with each transaction in H: is empty. 
4. H S and H: are equivalent. And H: is a repaired history of H'. 
Proof: 
1. We first show that when a transaction T] E G - AG is scanned every transaction between 
BI and TI is in B U AG. Assume this is not the situation and T2 is the first one between 
BI and T] which belongs to G - AG. According to the algorithm when T2 was scanned 
it should be moved to the left of BI , which is a contradiction. We second show that no 
transactions in AG will be movcd to the left of BI at the end of the algorithm. Assumc 
this is not the situation and T2 is the first one in AG. According to the definition of AG, 
when T2 is scanned there is at least one transaction between B] and T2 which can not 
follow T2, which is a contradiction. We last show that no transactions in B will be moved 
to the left of B] because they will never be moved at all. Therefore, after the rewrite all 
and only transactions in G - AG are moved to the left of B]. 
2. Since Algorithm 1 moves transactions in G - AG to the left of B] according to their 
orders in H', so they are ordered by H; and H S identically. Since transactions in B UAG 
are never moved in Algorithm 1, so they are ordered by H: and H' identically. 
3. Since there are no transactions which are moved to the left of any transaction in G - AG 
in Algorithm 1, transactions in G - AG will have empty fixes. 
4. Follows from Lemma 2 and Definition 4. 
D 
In realistic applications, although Lemma 2 gives users a sound approach to capture fixes 
in Algorithm 1, it is not efficient in many cases since whenever a transaction T; is moved 
to the left of another transaction Tj , Fj may need be augmented. A better way to compute 
fixes is as follows: 
Lemma 3. 
For any history H' , assume rewriting H S using Algorithm 1 generates a history 
H ' 
. h 
.hv H S (H S 
• 
II I 
k l'k . G 
G 
B F'] AG Fu 
BF,m 
AG Fkp 
7'1 
e wit. apreJ~ 
rF 
e typlca y 00 S 1 e. 
jl .. · 
jn 
il 
kl ... im'" 
kp' "le 
subhistory before Bi t is Hn, and assume all the fixes are computed according to Lemma 2 
during the rewriting, then the history H;', generated by replacing each non-empty fix Fi in 
H; with F: = Ti .readset - T; .writeset, is equivalent to H:. 
Proof: 
According to Theorem 1, the fix associated with cach transaction in H: is empty. 
Given a transaction T; in B U AG, for each item x in F: -
Fi , showing that the value 
of x in the before state of T; in H~' is the same as that in H' gives the proof. Assume 
G j is the first transaction which was moved to the left of Ti , then before G; was moved, 
the before state of Ti in the rewritten history is the same as that in H S because at this 
point, according to Lemma 2, the subhistory of the rewritten history which ends with the 
transaction immediately preceding T; is equivalent to the corresponding subhistory of H'. 
14 

RECOVERING FROM MALICIOUS TRANSACTIONS 
19 
After G j is moved to the left of T;, the value of x would not be changed since otherwise 
x must be in F;. Although G j might be further pushed through some other transactions in 
B U AG to the beginning of the history, the value of x in the before state of T; will not be 
changed. The reason follows from Lemma 2. 
0 
Lemma 3 enables us to separate computing fixed from transforming histories. Fixes can 
be computed after all of the transformations. Based on Lemma 3, the fix of transaction T; 
can be captured in two ways: one is to first get the read and write sets of T;, then compute 
T; .readset - T;. writeset; the other is to let each transaction T; write the set T; .readset -
T;.writeset as a record to the database when it is executed, then when we rewrite H' all of 
the fixes can be gotten directly from the database. 
3.3. 
Significance of Algorithm 1 
The majorresuit ofthis section is an equivalence theorem between the effect of a dependency-
graph based algorithm and the history produced by Algorithm 1. The dependency-graph 
based algorithm computes the set B.writeset U AG.writeset and restores the values of all 
elements in this set. In particular, the theorem shows that the optimizations in the following 
section are strict improvements over the dependency-graph based algorithm. 
Theorem 2. 
Given H", let H: be the serial history produced by eliminating all trans-
actions in B U AG as in the dependency-graph based algorithm. Given H", let H;' be the 
result of Algorithm 1. Then H: is a prefix of H:. 
Proof: 
Direct corollary of Theorem 1. 
o 
4. 
Saving additional good transactions 
In this section, we show how to integrate the notion of commutativity with Algorithm 1 to 
save not only the transactions in G - AG, but potentially transactions in AG as well. 
4.1. 
Motivating example 
Consider the following history: 
HH: B1 G 2 G 3 
B1: if u > 10 then x := x + 100, y := y - 20 
G2: u := u - 20 
G3: x := x + 10, z := z + 30 
According to Algorithm 1, which rewrites based on can follow, G 3 needs to be undone since 
it reads from B1 and hence is an element of AG. The result of Algorithm I is the history 
H: = G 2Bl u } G3. Note that G3 commutes backward through Blu } for any value of u,3 and 
15 

20 
LIU, AMMANN AND JAJODIA 
so an equivalent history is GZG3BJul. Compensation for Bjul can be applied directly to 
this history, but an undo approach requires more care. Suppose we decide to undo B] by 
restoring the before values for x and y from the log entries for B. After B is undone the 
value of u is unchanged because only G] updates u. The value of z is unchanged because 
only G3 updates z. The effect of G3 on x is wiped out because both G3 and B update x, 
and after B is undone x no longer reflects the effects of G 3. However x can be repaired by 
re-executing the corresponding part of G3'S code, that is, x = x + 10, and the cumulative 
effect is that of history GZG3. We call this last step an undo-repair action. Both the undo 
approach and the compensation approach to repair are discussed in detail in Section 5. 
The presence of fixes for transactions limits the extent to which commutativity can be 
applied. We illustrate this point with an example, and then define a more restrictive notion 
of commutativity called can precede that takes fixes into account. 
H9: So T} S} Tz Sz T3 S3 
T]: if y > 200 then x := x + 100 else x := x * 2 
Tz: y:= y + 100 
T3: if y > 200 then x := x - 10 else x := x/2 
T} can follow T2 with fix F] = {y} for T]. Although T3 commutes backward through T], T3 
F 
F 
does not commute backward through T} I, because the value of x produced by T} I depends 
on the value of y in the fix F]. For example, if the initial value of x is 100 and fix value 
of y is 150, then the final value of x in history T2 Ttl T3 is 190, but the final value of x in 
history T2T3Tt' is 180. 
The example shows that sometimes a fix can interfere with the commutativity of trans-
actions. This motivates our definition of can precede: 
Definition 6. 
A transaction T2 can precede a transaction T] for fix F if for any assignment 
of values to the variables in F and for any state So E S on which Tt Tz is defined, 
1. T2Tt is defined on So, and 
2. The same final state is produced by Tt T2 and T2 Tt. 
4.2. 
Can-Follow and Can-Precede rewriting 
We present a repair algorithm which integrates both can-follow and can-precede. 
Algorithm 2 
Can-Follow and Can-Precede Rewriting 
Input: the history H S to be repaired. 
Output: the repaired history H:. 
Method: 
Scan H S forward from the first good transaction after B} until the end of HS, 
for each transaction T 
16 
case T E B 
skip it; 
caseTEG 

RECOVERING FROM MALICIOUS TRANSACTIONS 
21 
if for each transaction T' between B] and T(including B]), either T' can follow 
T or T can precede T', then move T to the position immediately preceding B]. 
As T is pushed through each such T' between B] and T to the left of B] 
if T' can follow T, then push T to the left of T' and 
modulate the fix of T' correspondingly according to Lemma 2; 
else push T to the left of T'. 
The correctness of Algorithm 2 is specified as follows. 
Theorem 3. 
Given a history H', Algorithm 2 produces a history H: with a prefix H;' 
such that: 
1. Every transaction in G - AG appears in H;. 
2. H: and H' order transactions in H; identically. And they order transactions in H: - H; 
identically. 
3. The fix associated with each transaction in H; is empty. 
4. H S and H: are equivalent. And H; is a repaired history of H'. 
Proof: 
The proof of statements (1), (2), and (3) is similar to that of Theorem 1. (4) follows 
from Lemma 2, Definition 6 and Definition 4. 
0 
In Algorithm 1, Lemma 3 provides an efficient way to compute fixes. However, Lemma 3 
may not hold for Algorithm 2 if the system does not have the following property. 
Property 1. 
Transaction Tj can precede transaction Tj for a fix Fj only if (Tj .readset -
Tj.writeset - Fj ) n Tj.writeset = 0 and (Tj.readset - Tj.writeset) n Ii.writeset = 0. 
It should be noticed that Property 1 is not a strict requirement, and it usually holds 
for most of the transaction processing systems. The reason is: if Tj writes an item x in 
Ii .readset - Tj • writeset - Fj , then x can have different values in the before states of Tj in 
sequences Tt'Tj and Tj -r;F, respectively. Since x is not in Fj , Tj can read different values of 
x in the two sequences. Since the value of x typically affects the values of some other items 
updated by Ii, the two sequences usually can not generate the same final state. For similar 
reasons, if (Tj .readset - Tj .writeset) n Tj .writeset i= 0, then T/' Tj and Tj T/; usually can 
not gcnerate the same final state. 
Lemma 4. 
Lemma 3 holds for Algorithm 2 if the system has Property 1. 
Proof: 
The proof is similar to that of Lemma 3 except the situation when Tj is moved to 
the left of Ii based on the relation that Tj can precede Ii. At this point, for each item x in 
F/ - Fj , since the system has Property 1, Tj will not write x, so the value of x in F; is still 
the same as that in the before state of Ii after the rewrite. This completes the proof. 
0 
17 

22 
LIU, AMMANN AND JAJODIA 
4.3. 
Invert and Cover 
In this section, we introduce two semantic relationships between transactions, namely, 
Invert and Cover, and show how they can be exploited to enhance repair. 
If transaction T2 inverts TI, then any history ofthe form: SO ... T1 T2 ... is equivalent to the 
same history with TI T2 omitted; if T2 covers T" then any history of the form: so ... T1 T2 ... 
is equivalent to the same history with TI omitted. If T2 covers TI, then T2 covers Ttl for 
any F" but this is not the case for invert. 
Definition 7. 
Let P and Q be two sequences of transactions. Q inverts P if for any state 
So such that history So P Q is feasible, Q(P(so» = so. 
Definition 8. 
Let P and Q be two sequences of transactions. Q covers P if for any state 
So such that history So P Q is feasible, Q(P(so)) = Q(so). 
The rewriting algorithm which exploits these two relations is described below. 
Algorithm 3 
Can-Follow, Can-Precede, Cover, and Invert Rewriting 
Input: the history H' to be repaired. 
Output: the repaired history H:. 
Method: Scan H S forward from the first good transaction after B, until the end of H" 
for each transaction T 
case T E B 
skip it; 
caseTEG 
if for each transaction T' between B, and T(including B,), either T' 
can follow T, or T can precede T', or T inverts T', or T covers T', 
then move T to the position immediately preceding B I . As T is pushed through 
each T' between BI and T to the left of BI 
if T covers T', then remove T' from the history; 
elseif T' can follow T, then push T to the left of T' and 
modulate the fix of T' correspondingly according to Lemma 2; 
elseif T can precede T', then push T to the left of T'; 
else remove both T and T' from the history. 
For similar reasons, Lemma 3 can also be exploited to capture fixes in Algorithm 3 if the 
system has Property 1. The correctness of Algorithm 3 is specified as follows. The proof 
is similar to Theorem 1 and Theorem 3, thus omitted. 
Theorem 4. 
Given a history H', Algorithm 3 produces a history H; with a prefix H: 
such that: 
1. H: and H S order transactions in H: identically. And they order transactions in H; - H: 
identically. 
2. The fix associated with each transaction in H; is empty. 
3. Every transaction in H: is in H'. 
4. The final states of H S and H; are identical. And H: is a repaired history of H'. 
18 

RECOVERING FROM MALICIOUS TRANSACTIONS 
23 
5. 
Pruning rewritten histories 
After a rewritten history H: with a prefix H:, which is the repaired history, is generated 
from HS, we need to prune H: such that the effects of all the transactions in H: - H: are 
removed. Pruning H: generates H;. If H: is produced by Algorithm 1, then the pruning 
can be easily done by undoing each transaction in H; -
H~'. However, if H: is produced 
by Algorithm 2 or Algorithm 3, undo does not give the pruning in most cases. 
In this section, two pruning approaches are presented. The compensation approach re-
moves the efTect of each transaction T/, in H: - H: by executing the fixed compensating 
transaction of Ti , however, compensating transactions may not be specified in some systems. 
The undo approach prunes H: by building and executing a specific undo-repair action for 
each affected transaction in H:. It is a syntactic approach, but it imposes some restrictions 
on transaction programs. 
5.1. 
The compensation approach 
We denote the compensating transaction of transaction T, as ~-I [13, 14,241. Ti- I semanti-
cally undoes the effect of T,. It is reasonable to assume that Ti-I.writeset <; T, .writeset, and 
for simplicity we further assume that every transaction T, has a compensating transaction. 
After Algorithm 2 or Algorithm 3, a typical rewritten history H: with a prefix H: looks 
like (note that Bi! could be covered or inverted, and H; can also end with a bad one): 
G 
AG 
G 
AG 
B Fd AGFh(HI) 
BF,m 
AG FhP Th 
bh' t 
b" 
B Fd ' H S 
jl .. · 
hi .. • 
jq... 
hk 
il 
h(k+I)'" im'" 
hp' 
esu 
IS ory elore 
il IS 
r' 
Based on H:, compensation is a simple way to get the repaired history H:. However, 
executing the compensating transaction sequence AG;;-; ... Bi-:nI ... AG;;-(~+I) Bill on the final 
state of H S can not generates H: in most cases because the transactions we need to com-
pensate are usually associated with a non-empty fix. Fixes must be taken into account for 
the compensation to be correct. 
Definition 9. 
The fixed compensating transaction of T/' , denoted ~(-1.F,), is the regular 
compensating transaction of T, (denoted ~-I) associated with the same fix Fi . 
The effects of Ti f~ can be removed by executing ~ (-I, F,), this is justified by the following 
lemma. 
Lemma 5. 
Transaction T;F, can be fix compensated, that is, for every consistent state Sl 
on which T/' is defined, ~(-I.F')(T/' (.\'1) = SI, if F; n T,.writeset = 0. 
Proof: 
Since F; n T,.writeset = 0, Ti-I.writeset <; Ti.writeset, so F; n Ti-1.writeset = 
0. Therefore, neither T; nor T;-l will update any item in Fi . Let S2 = ~F, (.II)' For each 
item x in Fi we replace the values of x in states SI and S2 with the value of x in F, thus 
two new states are generated (denoted s; and s~ respectively). It is clear that ~-I (s~) = .I;. 
Since the differences between Ti- I (s~) and ~(-I.F'\S2) are only with the values of the items 
in Fi which are neither updated by ~-I, nor updated by T/- I.F,), so T/-I.f~) (S2) = SI. This 
completes the proof. 
0 
19 

24 
LIU, AMMANN AND JAJODIA 
A rewritten history H: can be fix compensated if every transaction in H: can be fix 
compensated. Lemma 5 shows that every H: produced by Algorithm 2 or Algorithm 3 can 
be fix compensated because for each transaction ~ in H: which is associated with a non-
empty fix F;, F; n T;. writeset = fZJ always holds. The pruning algorithm by compensation 
therefore is straightforward: based on the final state of H', executing the fixed compensating 
transaction for each transaction in H: - H: in the reverse order as they are in H'. 
5.2. 
The undo approach 
As stated above, after Algorithm 2 or Algorithm 3, a typical rewritten history H: looks 
l'k . G 
AG 
G 
AG 
BFd AG Fh (k+l) 
BF,m 
AG Fhp 
A 
h 
. H 
d' 
1 e. 
jl... 
hi ... 
jq... 
hk;1 
h(k+I)'" 
im ... 
hp' 
S S own III 
8, un mng 
transactions in H: - H: can not generate H: in most cases. However, building and executing 
the undo-repair actions forthe affected transactions in H:, namely AG hi, ... , AGhk. after 
these undo operations can generate H:. For example, in Hg, executing the undo-repair 
action, x = x + 10, for G3 after B is undone can produce the effect of history G 2 G 3• 
To build the undo-repair actions for AGhl, ... , AGhb we need to do two things: 
1. Abstract the code for each undo-repair action from the source code of the corresponding 
affected transaction. 
2. Assign appropriate values for some specific data items accessed by these undo-repair 
actions. 
Our algorithm described below is based on the following assumptions about transactions: 
• a transaction is composed of a sequence of statements, each of which is either: 
- An operation; 
- A conditional statement of the form: if c then SSI else SS2, where SSI and SS2 are 
sequences of statements, and c is a predicate; 
• each statement can update at most one data item; 
• each data item is updated only once in a transaction; 
Algorithm 4 
Build Undo-repair Actions 
Input: an affected transaction AGk. 
Output: the undo-repair action URAk for AGk . 
Method: 
1. Copy the codes of AGk to URA k • Assign URA k with the same input parameters and 
the same values associated with them as AGk . 
2. Parse URA k . For each statement to be scanned 
case it is a read statement, keep it; 
20 
case it is an update statement of the form: x := f(x, YI, Y2, ... , Yn) where f 
specifies the function of the statement, YI, ... , Yn are the data items used 
in the statement. Some input parameters may also be used in the statement, 
but they are not explicitly stated here. 

RECOVERING FROM MALICIOUS TRANSACTIONS 
if x has not been updated by any other transaction in B U AG 
Remove the statement from URAk ; 
elseif x has not been updated by any transaction in B U AG 
which precedes AGk in H S 
Replace the statement with: x := AGk.afterstate.x, 
that is, get the value of x rrum the after state of AGk in H S ; 
else for each Yi (including x) 
if Yi has not been updated by any preceding statement and has not been 
updated by any transaction in B U AG which precedes AGk in H S 
Bind Yi with AGk·heforestate.y; 
25 
3. Reparse URAk. Remove every read statement which reads some item never used in an 
update statement of URA b or reads some item Y used in one or more update statements but 
Y is bound with a value in these statements. 
It should be noticed that when we execute an undo-repair action URAb for each update 
statement x = f (x, YI , Y2, ... , Yn) of URAk, if Yi is not bound then we get the value of Yi 
from the current database state, otherwise, the bound value should be used. 
The correctness of the undo approach is specified as follows. 
Theorem 5. 
For any rewritten history H: generated by Algorithm 2 or Algorithm 3, 
after all transactions in H: - H; are undone, executing the undo-repair actions which are 
generated by Algorithm 5.2 for the affected transactions in H;, in the same order as their 
corresponding affected transactions are in H;', produces the same effect of H,s. 
Proof: 
Showing that each item x updated by an transaction in H: is restored to the value 
as generated by H; after the repair gives the proof. 
If x has never been updated by any transaction in B U AG, then the value of x will be 
correctly restored because an unaffected transaction G i can only read items from other 
unaffected transactions thus G i 's updates will not be affected by transactions in B U AG. 
Otherwise, assume x has been updated by k transactions in B U AG, that is, Til, ... , Tik. 
Tip <k T;q if p < q. Note that after x has been updated by Til, x will not then be updated 
by any unaffected transaction. If k = I, that is, there is only one such transaction. At this 
point, if Til is in H; - H: then after the undoes the value of x will be correctly restored; 
otherwise, Ti I is in H;. Since H: is equivalent to H' , su the value of x in the final state of 
H: is the same as that in the after state of T; I in H'. Hence in Algorithm 4 the corresponding 
update statement is removed. 
When k > 1, if no such transaction is in H: then after the undoes the value of x will be 
correctly restored. Otherwise, assume Tjl is in H:, then when URA jl is executed, x := 
Tjl.afterstate.x, according to Algorithm 4. This restores the value of x to that generated 
by the subhistory of H; which ends with Tjl' because in rewriting when Tjl is moved into 
H:, the subhistory HI of H S which ends with Tjl is equivalent to the subhistory H2 of the 
rewritten history at that time which ends with the transaction immediately preceding Tjl 
before the move, and Tj 1 is the last transaction in H2 that updates x. 
Assume TjI (/ > 1) is in H;, if there is another such transaction Tjm in H; such that 
:s m < I and no other such transactions stay between Tjm and Tjl, then in the update 
21 

26 
LIU, AMMANN AND JAJODIA 
statement x := f ex, YI, Y2, ... , Yn) of URA jl, the value of x for read purpose should be got 
from the state after URA jm is executed; otherwise, there is no such Tjm , thus transactions 
Tj I •... , Tj(l_I) will all be undone, hence the value of x in the above statement should be 
got from the state after Tj I is undone. 
As for Yi in the above update statement, if Yi has been updated by a preceding statement 
in URA jl, then the updated value should be used. Otherwise, if Yi has been updated by some 
transaction in B U AG which precedes Tjl in HS, then according to the above discussion, 
the value of Yi should be got from the state before URA jI is executed; Otherwise, the value 
of Yi should be got from the before state of Tjl in H'. At this situation, getting the value 
of Y; from the state before URA fl is executed can not ensure the correctness because it is 
possible that there is a transaction Ti such that T; updates Yi, ~ follows Tjl in HS, ~ is in 
B U AG and Ti is in H;. At this point, the value of Yi updated by T; will not be undone. 
Since the values of x, Yl, Y2 . ... , Yn in the above statement are correctly captured, so the 
above statement can correctly restore the value of x to that generated by the subhistory of 
H: which ends with Tjl. By induction on t, 1 .:s I .:s k, the above claim holds. 
0 
6, 
Relationships between rewriting algorithms 
Rewriting can save more good transactions than is possible with a dependency-graph based 
approach to recovery. For a history H S to be repaired, we will let DGR(H') and CFR(H S ) 
represent the sets of saved transactions after H S is repaired using a dependency-graph based 
approach and can-follow rewriting (Algorithm 1), respectively. FPR(HS) and FPCI(HS) 
will be used to represent the sets of saved transactions after H S is repaired using can-follow 
and can-precede rewriting (Algorithm 2) and can-follow, can-precede, cover and invert 
rewriting (Algorithm 3), respectively. 
Theorem 2 shows that for any history H S, DGR(HS) = CFR(HS). 
Theorem 6. 
For any history HS, CFR(H') ~ FPR(H S ) ~ FPCI(H S ). The converse is 
not, generally, true. 
Proof: 
Follows from Algorithm 1, Algorithm 2, and Algorithm 3. 
o 
Commutativity can be directly used to rewrite histories without being integrated with 
can-follow rewriting. Let CR(HS) and CBTR(H S ) represent the sets of saved transactions 
after H S is repaired using the two rewriting algorithms which are based on the commute 
relation and the commutes backward through relation between transactions, respectively. 
These two algorithms can be easily adapted from Algorithm 1 by checking the commute 
and commutes-backward-through relation between transactions respectively, instead of 
can-follow. 
Theorem 7. 
For any history H S, CR(HS) ~ CBTR(HS). The converse is not, generally, 
true. 
Proof: 
Follows from the definitions of commute and commutes backward through. 
0 
22 

RECOVERING FROM MALICIOUS TRANSACTIONS 
27 
Theorem 8. 
3 H 5 , CFR(H 5 ) n CBTR(H') i= 0 and each is not included in the other; 
3 H', CFR(H 5 ) n CR(H S ) i= 0 and each is not included in the other; 
Proof: 
Consider the history 
HIO: So B] s] G2 .1'2 G 3 .13 
B]: if y > 200 then x := x + 10 
G2: if y > 200 then x := x + 30 
G3: y := y + 100 
It is clear that CFR(H{o) = {G 3 }; CBTR(Hi'o) = CR(Hto) = {G2). This completes the 
~~ 
D 
Theorem 9. 
If the system has Property 1, then 
1. VH', CBTR(W) So; FPR(W) 
2. 3H', CBTR(HS) c FPR(HS) 
Proof: 
Given a history HS, showing that T; E FPR(H S) holds for each transaction T; E 
CBTR(HS) gives the proof. We prove this by induction on k where Tk is the kst transaction 
moved into CBTR(HS). 
Induction base: (k = 1) We want to show that TI E FPR(H S ). If there are no transactions 
between B] and T] which are in FPR(H S ), then TI will be moved into FPR(H S ) according to 
Algorithm 2 because TI can precede every transaction Tf between B. and Tj owing to the fact 
that Tl commutes backward through Tj . Otherwise, there must be some transaction Tj with 
a non-empty fix Fj staying between B j and T] (including B I ) in the rewritten history when 
Tj is scanned in Algorithm 2. Here we assume that Fj is captured by Lemma 2. At this point, 
assume Tj cannotprecede T:J , then Fj n (Tj.readset- Tj.writeset) i= 0 because otherwise 
Tj can precede Tj 1 (The reason is: for every state So on which T/' TI is defined, replacing 
So with another state SI where the value of each item x in So n Fj is replaced with x's value 
in Fj . Then TPT] is defined on .1']. According to Property 1, since Tj can precede Tr (Note 
that T] commutes backward through Tj ), so (Tj.readset - Tj.writeset) n T].writeset = 0. 
Since Fj So; (Tj .readset - Tj .writeset) according to Lemma 2, so Fj n T].writeset = 0. 
So TI will not read or update any item in Fj . Therefore, Tt'Tj(so) = TFT,(sj), and 
T, TF; (so) = T, T0(sj). Since TI commutes backward through Tj , so TJ0T, (s]) = T] TJ0(s]). 
J 
F 
J 
F 
F 
. 
Therefore, Tj 'T,(so) = T,Tj '(so), so T] can precede Tj 1). Therefore, 3x, such that, 
x E Fj n (T,.readset - T,.writeset). Since x E Fj , so according to Algorithm 2 there must 
be a transaction Tp, such that Tp is now in FPR(H'), and x E Tp. writeset. Otherwise, x will 
not be put into Fj by Lemma 2. Hence Tp.writeset n (Tj.readset - T •. writeset) i= 0. This 
conflicts with Property 1 since TI commutes backward through Tp thus T. can precede T:. 
So the assumption that T, cannot precede T:l does not hold. Therefore, T, can precede 
T:'. So TI can precede every transaction between B, and Tj which has a non-empty fix. 
Since T, commutes backward through all the other transactions between B, and T], so T. 
will be moved into FPR(HS). 
23 

28 
DGR 
CBTR 
CPR 
I 
CR I 
FPR 
I 
FPCI 
All Transactions in H s 
Figure 3. 
Relationships among repair approaches. 
LIU, AMMANN AND JAJODIA 
DGR--set of transactions saved by a 
dependency-graph based approach 
CFR--set of transactions saved by 
can-follow rewriting 
CR--set of transactions saved by commute 
rewriting 
CBTR--set of transactions saved by 
commutes-backward-through rewriting 
FPR--set of transactions saved by 
can-follow and can-precede rewriting 
FPCI--set of transactions saved by can-follow. 
can-precede. cover and invert rewriting 
Induction hypothesis: for each 1 .:s k .:s n, if Tk E CBTR(H S ), then Tk E FPR(H S ). 
Induction Step: Let k = n + 1, then when Tk is scanned in both algorithms, every 
transaction Tj , which is between Bland Tk in the rewritten history generated by Algorithm 2 
at that time, is between BJ and Tk in the rewritten history generated by the commutes-
backward-through rewriting algorithm. Therefore, Tk commutes backward through every 
such Tj . For the same reason as in the induction base step, we know that Tk will be moved 
into FPR(H S ). 
Therefore, statement 1 holds. Consider history H IO , it is clear that FPR(Hto)= {G 2 , G3 }; 
CBTR(Hfo) = {G 2 }. So statement 2 holds. 
D 
In summary, after a history H S is repaired, the relationships among DOR(H S ), CFR(H S ), 
FPR(H S ), FPCI(H S ), CR(H S ) and CBTR(H") are shown in figure 3. Here we assume that 
the system has Property 1. 
7. 
Implementing the repair model on top of sagas 
In this section, we will evaluate the feasibility of our repair model by integrating it with the 
Saga model [14]. 
7.1. 
The Saga Model 
The Saga Model is a practical transaction processing model addressing long duration trans-
actions which can be implemented on top of an existing DBMS without modifying the 
DBMS internals at all. A saga consists of a collection of saga transactions (or steps), each 
of which maintains database consistency. However any partial execution of the saga is 
undesirable; either all the transactions in a saga complete successfully or compensating 
transactions should be run to amend for the partial execution of the saga. Thus corre-
sponding to every transaction in the saga, except the last one, a compensating transaction is 
specified. The compensating transaction semantically undoes the effect of the corresponding 
transaction. 
24 

RECOVERING FROM MALICIOUS TRANSACTIONS 
29 
The Saga Model is suitable for our repair model to be implemented on top of it because 
it supports compensation inherently. For example, a compensating transaction is specified 
for each transaction, except the last one, in a saga; and when a saga transaction Tij ends, the 
end-transaction call will include the identification of the compensating transaction of Tij 
which includes the name and entry point of the compensating program, plus any parameters 
that the compensating transaction may need. 
By viewing each normal duration transaction and each long duration transaction which 
can not be specified as a multi-step saga, as a specific saga that consists of only one saga 
transaction, we can get an unified view of transactions in the systems where the saga model 
is implemented. By adding the compensating transaction for the last step in each saga, we 
can get all the necessary compensating transactions to do repair. 
In addition, the saga model has the following two features which allow for optimization 
in rewriting a history. 
Consistency Property: the execution of each saga transaction (step) maintains database 
consistency. 
Compensation Property: during the lifetime of a saga,4 no matter how the saga is inter-
leaved with other sagas, any step in the saga which is successfully executed, if having not 
been compensated, can be compensated by executing the corresponding compensating 
transaction at the end of the growing history. 
7.2. 
Repair a history of sagas 
The Compensation Property implies that in a history to be repaired whenever a saga is iden-
tified as a bad one, we can rewrite the history to move only the last step, instead of every 
step, of the saga to the end of the history. In this way, substantial rewriting and pruning work 
can be saved. The optimization based on can-follow rewriting (Algorithm 1) is specified in 
the the following algorithm. 
Algorithm 5 
Rewrite a history of sagas by can-follow rewriting 
Input: the serial history H S to be rewritten and the set B of bad sagas. 
Output: 
a rewritten history H: with a prefix H,' which consists of only good saga 
transactions. 
Method: 
Scan forward from the first good saga transaction after Bll until the end of 
H S , for each step ~j (of saga Si) 
case Si E B 
skip it; 
case Si E G 
if there is a step of Si which stays between Bland Tij 
Skip Tij; 
elseif the final step Tpn of every saga S p which stays between 
B1 and ~j (including B j ) can follow Tij 
Move Tij to the position which immediately precedes B j • As Tij is pushed 
through each such T pn , augment Fpn according to Lemma 2. 
25 

30 
LIU, AMMANN AND JAJODIA 
The integrated repair algorithm using Algorithm 5 to rewrite a history and the compen-
sation approach to prune the rewritten history is specified as follows, 
Algorithm 6 
Repair a history of sagas by can-follow rewriting 
Input: the serial history H S to be repaired 
Output: a repaired history H;' which consists of only good saga transactions, 
Method: 
1. Rewrite H S using Algorithm 5,5 
2. Do compensation from the end to the beginning of H: until BI is compensated. When 
the final step Tpn of a saga S I' is to be compensated 
F· 
T(-I.Fp,) 
T Fp" 
Th 
d 
d . 
f 
• 
Irst, execute 
pn 
to compensate 
I'n' 
e co es an mput parameters 0 
TJ~;;I.Fp,) are got from the identification of the compensating transaction of Tpn in-
cluded in the end-transaction call of Tl'n' Note that Fpn has already been computed 
after H S was rewritten . 
• Second, execute the sequence T;(,,~~i, ... , T;;1.0) to compensate all the other steps 
of S I' which are not in H:. The codes and input parameters of these compensating 
. 
. h 
f T(-1.Fp,) 
transactIOns are got m t e same way as 0 
pn 
. 
The correctness of Algorithm 6 is specified as follows. 
Theorem 10. 
Algorithm 6 is correct in the sense that H: is consistent after step 1, and 
the repair results in the same state as generated by re-executing H:. 
Proof: 
It is clear that in Algorithm 5 a step Tij will not be moved into H: unless every 
step between Til and Tij can be moved into H:. For a step ~j, if Si is a good saga, and 
each step between Til and Tij (including Tij ) can be moved into H:, then we say T,j is an 
unaffected step, otherwise, we say ~j is an affected step. 
We propose another approach to repair H S which is clearly correct. It works as follows: 
Scan H S backward from the end to the beginning: 
If b d fi 
I t 
B
· 
t 
t th 
B(-I.F.,) B(-1.0) 
B(-U1l 
th 
• 
a a 
na s ep 
in IS me, execu e 
e sequence 
in 
'i(n-I)"'" 
iJ 
on 
e 
final state of the current history. This can remove the effects of Bi from the current history 
because at this point all the steps to the right of Bin are unaffected steps. All the bad 
or affected steps to the right of Bin have already been compensated. So Bin can follow 
every step following it, thus B;n can be moved to the end of the current history without 
changing the final state of the current history if F;n is computed according to Lemma 2, 
therefore, according to the Compensation Property, after Bi~' is compensated executing 
(-10) 
B(-I0) 
h 
h 
B;(n-'I) ' ... , 
il . 
can compensate t e ot er steps . 
• If an affected final step T;n is met, assume Tip is the last unaffected step in Si, execute 
h 
T (-I F,,) T(-10) 
T(-10) F 
"1 
h 
b 
h' 
t e sequence in' 
'i(n-'1) , ... , 
;(1'+1)' 
or SImi ar reasons to tea ove case, t IS 
can remove the effects of all the affected steps of Si. 
26 

RECOVERING FROM MALICIOUS TRANSACTIONS 
31 
It is clear that the above approach results in H:. So H: is consistent. Since the above 
approach executes the same set of fixed compensating transactions on the final state of H S 
as Algorithm 6, and it executes these fixed compensating transactions in the same order, so 
Algorithm 6 results in the same state as generated by re-executing H:. 
0 
Algorithm 2 and Algorithm 3 can also be adapted to rewrite a history of sagas. The adapted 
algorithms are specified as follows. For brevity, and to highlight the differences between 
these algorithms, we describe only the modifications to Algorithm 2 and to Algorithm 3, 
respectively. 
Algorithm 7 
Rewrite a history of sagas by can-follow and can-precede rewriting 
Method: 
Scan H S forward from the first good step after Bll until the end of H S , for 
each step Tij 
case Si E G 
if there is a step of Si which stays between B] and Tij 
Skip Ti); 
elseif the final step Tpn of every saga Sp which stays between BI and Ti) 
(including Bd can follow Ti), or Ti) can precede T:::n 
Move Ti) to the position which immediately precedes BI . 
Algorithm 8 
Rewrite a history of sagas by can-follow, can-precede, cover and invert 
rewriting 
Method: 
Scan H S forward from the first good step after B11 until the end of H S , for 
each step Tij 
case Si E G 
if there is a step of Si which stays between B] and Ti) 
Skip Ti}; 
elseif the final step Tpn of every saga S p which stays between Bland Tij 
(including BI ) can follow Ti}, or Ti) can precede T;'i', 
T. 
T Fpn 
T.' 
TFpn 
or i) covers 
pn ,or ij mverts 
pn 
Move Ti) to the position which immediately precedes B] . 
The correctness of the repair based on Algorithm 7 or Algorithm 8 is specified in the 
following theorem. The proof is similar to that of Theorem 10, thus omitted. 
Theorem 11. 
The repair based on Algorithm 7 is correct in the sense that Theorem 10 still 
holds even if the rewriting step (step 1) of Algorithm 6 is done by Algorithm 7. The repair 
based on Algorithm 8 is correct in the sense that Theorem 10 still holds after Algorithm 6 
is modified as follows: 
• The rewriting step (step 1) is done by Algorithm 8; 
• In step 2, when an affected step Ti(n-I) of a saRa Si is scanned6 , assume Tip is the last 
.1+: 
d 
. S 
h 
T(-1,0) 
T(-1,0) 
unaJJecte step In 
i, execute t e sequence i(n-l)"'" 
i(p+l)' 
27 

32 
LIU, AMMANN AND JAJODlA 
7.3. 
Detecting can-follow, can-precede, cover and invert relationships 
between transactions 
In Section 7.2, the repair based on Algorithm 5, Algorithm 7 and Algorithm 8 cannot be 
enforced without first capturing the can-follow, can-precede, cover, and invert relationships 
between saga transactions. 
Given a history of sagas, the can-follow relationships between the saga transactions in the 
history depend on the readset -writeset relationships between these transactions. The write set 
of a transaction Ti can be got from the traditional log where every write operation is recorded. 
However, the read information of T; we can get from the logs for traditional recovery 
purposes such as physical logs, physiological logs, and logical logs [16], is usually not 
enough to generate the read set. Therefore, the efficient maintenance of read information is 
a critical issue. In particular, there is a tradeoff between the extra cost we need to pay besides 
that of traditional recovery facilities and the guaranteed availability of read information. 
The read information can be captured in several ways, for example 
• Augment the write log to incorporate read information. There are basically two ways: 
one is appending the read record rT;, x] to the log every time when Ti reads an item x. 
The other way is first keeping the set of items read by T; in another place until the time 
when T; is going to commit. At this point, the read set of T; can be forced to the log as 
one record. 
Although keeping read information in the log will not cause more forced I/O, it does 
consume more storage. Another problem with the approach lies in the fact that almost 
all present database systems keep only update(write) information in the log. Thus adding 
read records to the log may cause the redesign of the current recovery mechanisms. 
• Extract read sets from the profiles and input arguments oftransactions. Compared with the 
read log approach, when transaction profiles (or codes) are available, each transaction just 
needs to store its input parameters, which are often much smaller in size than the read set. 
More important, instead of putting the input parameters in the log, each transaction can 
store the parameters in a specific user database, thus the repair module can be completely 
isolated from the traditional recovery module. In this way, our repair model can be 
implemented on top of the Saga model without modifying the internals of the DBMS on 
which the Saga model is implemented. 
This approach captures read information without the need to modify DBMS internals. 
However, it usually can only achieve a complete repair, but not an exact repair. That is, 
the effects of all bad transactions will be removed, but the effects of some unaffected 
good transactions may sometimes be removed also since in many situations the approach 
can only get an approximate read set. Interested readers can refer to for more details of 
this approach. 
• Although traditional logging only keeps write information, more and more read infor-
mation can be extracted from the log, particularly when more operation semantics are 
kept in the logs. Traditional physical(value) logging keeps the before and after images 
of physical database objects(i.e., pages), so we only know that some page is read. In 
addition, a page is normally too large a unit to achieve a fine repair. Physiological logging 
keeps only the update to a record(tuple) within one and only one page, so we know that 
28 

RECOVERING FROM MALICIOUS TRANSACTIONS 
33 
this record should be in the read set, which is much finer than physical logging. Logical 
logging keeps more operation semantics than the other two logging approaches. Con-
ceptually logical logs can keep track of all the read information of a transaction, though 
this is not supported by current database systems. However, logical logging attracts 
substantial industrial and research interests. In system R, SQL statements are put into 
the log as logical records; In [27], logical logs can be a function, like x=sum (x, y), and 
swap (x, y) etc. In both situations, we get more read information than other logging 
methods. 
In long duration transaction models [14, 37], or in multilevel transaction models [26, 
38], it is possible to extract the read information of transaction (subtransaction) T from its 
compensation log records, where the action of T' s compensating transaction is recorded. 
The can-precede, cover, and invert relationships between transactions are based on the 
semantics of transactions, and they can be captured in a similar way to commutativity [23, 
28, 36, 40], and recoverability [7]. In order to capture these relationships, the profile (or 
code) and input arguments of each transaction must be available. In the Saga model, several 
possible solutions to the problem of saving code reliably are proposed [14], therefore, these 
relationships can be reliably captured in the Saga model. 
For a canned system with limited number of transaction classes and fixed code for each 
transaction class, the can-follow, can-precede, cover, and invert relationships between saga 
transactions can be detected according to the corresponding relationships between transac-
tion classes. Although detecting these relationships between two transaction classes usually 
needs more effort than detecting these relationships between two transactions, after this is 
done with all the transaction classes, detecting these relationships between transactions of 
these classes can be much easier in many situations. 
For example, in a bank a deposit transaction (denoted dep(a;, m» which deposits m 
amount of money into account a; can follow a withdraw transaction (denoted wit(aj, n» 
which withdraws n amount of money from account a j only if they access different accounts, 
that is, a; #- a j. Therefore, given the can-follow relationship between the deposit transaction 
class and the withdraw transaction class, the can-follow relationship between a dcposit 
transaction and a withdraw transaction can be detected without the need to check the 
readset-writeset relationship between the two transactions, checking their input parameters 
is enough. 
7.4. 
Fix information maintenance 
It is clear that Lemma 3 can he used in Algorithm 5, Algorithm 7 and Algorithm 8, to 
capture fixes. For a transaction 'Fi, there are two methods to get Tt .readset -
T;. writeset: 
one is to first get the readset and writcset of Tt after an execution history is generated 
using the approaches proposed in Section 7.3, then compute Tt .readset - Tt .writeset; the 
other is what we have proposed in Section 3, that is, let each transaction T; write the set 
T; .readset - T;. writeset as a record to the database when it is executed, then when we rewrite 
H S all the fixes can be directly got from the database. 
It should be noticed that in the situations where the read and write sets of T; have to 
be firstly captured in order to detect the can-follow relationships between T; and some 
other transactions, the first method is more efficient; In contrast, when all the necessary 
29 

34 
LID, AMMANN AND JAJODIA 
can-follow relationships between ~ and other transactions can be detected without the need 
to check the readset-writeset relationships between ~ and these transactions, for example, 
when these relationships can be directly got from the can-follow relationships between the 
corresponding transaction classes, the second method is more efficient. 
8. 
Related work 
Database recovery mechanisms are not designed to deal with the recovery from undesirable 
but committed transactions. Traditional recovery mechanisms [8] based on physical or logi-
cal logs guarantee the ACID properties of transactions-Atomicity, Consistency, Isolation, 
and Durability-in the face of process, transaction, system and media failures. In partic-
ular, the last of these properties ensure that traditional recovery mechanisms never undo 
committed transactions. However, the fact that a transaction commits does not guarantee 
that its effects are desirable. Specifically, a committed transaction may reflect inappropriate 
and/or malicious activity. 
There are two common approaches to handling the problem of undoing committed 
transactions: rollback and compensation. The rollback approach is simply to roll back 
all activity--desirable as well as undesirable-to a point believed to be free of damage. 
Such an approach may be used to recover from inadvertent as well as malicious damage. 
For example, users typically restore files with backup copies in the event of either a disk 
crash or a vims attack. In the database context, checkpoints serve a similar function of 
providing stable, consistent snapshots of the database. The rollback approach is effective, 
but expensive, in that all of the desirable work between the time of the backup and the time 
of recovery is lost. Keeping this window of vulnerability acceptably low incurs a substan-
tial cost in maintaining frequent backups or checkpoints, although there are algorithms for 
efficiently establishing snapshots on-the-fly [3, 31,33]. 
The compensation approach [13,14] seeks to undo either committed transactions or com-
mitted steps in long-duration or nested transactions [24]. There are two kinds of compen-
sation: action-oriented and effect-oriented [24, 26, 38, 39]. Action-oriented compensation 
for a transaction or step ~ compensates only the actions of T;. Effect-oriented compen-
sation for a transaction or step T; compensates not only the actions of T;, but also the 
actions that are affected by T;. For example, consider a database system that deals with 
transactions that represent purchasing of goods. The effects of a purchasing transaction Tl 
might have triggered a dependent transaction T2 that issued an order to the supplier in an 
attempt to replenish the inventory of the sold goods. In this situation, the action-oriented 
compensating transaction for Tl will just cancel the purchasing; but the effect-oriented 
compensating transaction for Tl will cancel the order from the supplier as well. Although a 
variety of types of compensation are possible, all of them require semantic knowledge of the 
application. 
The notion of commutativity, either of operations [23, 28, 40] or of transactions [36], 
has been well exploited to enhance concurrency in semantics-driven concurrency control. 
There are several types of commutativity. In operation level, for example, two operations 
O[ and 02 commute forward [40] if for any state s in which 01 and O2 are both defined, 
02( 01 (s» = O[ (02(S»; O2 commutes backward through [28] O[ if for any states in which 
30 

RECOVERING FROM MALICIOUS TRANSACTIONS 
3S 
0 102 is defined. O2 (0 1 (s» = 0 1 (02 (s»; 0 1 and O2 commute backward [28, 40] if each 
commutes backward through the other. In transaction level, for example, two transactions 
commute [36] if any interleaving of the actions of the two transactions for which both 
transaction commit yields the same final state; Two transactionsfailure commute [36] if they 
commute, and if they can both succeed then a unilateral abort by either transaction cannot 
cause the other to abort. Our notation can precede is adapted from the commutes backward 
through notation for the purpose of taking advantage of transaction level commutativity. 
In [7], semantics of operations on abstract data types are used to define recoverability, 
which is a weaker notion than commutativity. recoverability is a more general notion than 
canfollow in capturing the semantics between two operations or transactions, but can follow 
is more suitable for rewriting histories. recoverability is applied to operations on abstract 
data types but can follow is applied to transactions. recoverability is defined based on the 
return value of operations, and thus a purely semantic notion; but can follow is defined 
based on the intersections of read and write sets between two transactions. 
Korth, Levy, and Silberschatz [24] address the recovery from undesirable but committed 
transaction. The authors build a formal specification model for compensating transactions 
which they show can be effectively used for recovery. In their model, a variety of types of 
correct compensation can be defined. A compensating transaction, whose type ranging from 
traditional undo, at one extreme, to application-dependent, special-purpose compensating 
transactions, at the other extreme, is specified by some constraints which every compensat-
ing transaction must adhere. Different types of compensation are identified by the notion of 
compensation soundness. A history X consisting of T, the compensating-for transaction; 
CT, the compensating transaction; and dep(T), a set of transactions dependent upon T, is 
sound if it is equivalent to some history of only the transactions in dep(T). 
Though a compensating transaction in our model can be specified by their model, our 
notion of a repaired history is more suitable for rewriting histories than the notion of 
sound history, since the constraint that compensating transactions can only be applied to 
the final state of a history greatly decreases the possibility of finding a sound history, even 
if commutativity is fully exploited. We can get a feasible history by rewriting the original 
history based on can follow, can precede, invert and cover. The resulting history augmented 
with the corresponding undo-repair actions or fixed compensating transactions yields the 
desired repair. 
9. 
Discussion and conclusion 
9.1. 
Discussion 
9.1.1. Relevant security contexts. 
Our repair model can be applied to many kinds of 
secure database systems to enhance their survivability. However, the main factors on which 
the applicability of our model to a secure database system is dependent, such as (I) the 
characteristics of the database, i.e., whether it is single-version or multiversion, (2) the 
concurrency control protocol and the characteristics ofthe histories produced by it, and (3) 
the recovery protocol and the characteristics of the logs produced by it, are closely relevant 
to the security model and architecture of the system. 
31 

36 
LTU, AMMANN AND JAJODIA 
For a single-level secure database system where every subject (transaction) and object 
(data item) are within the same security class, traditional concurrency control protocols such 
as two-phase locking (2PL), and recovery protocols such as write-ahead logging (WAL), 
can be directly used without causing any security policy violations, no matter which kind 
of security model (i.e., access-matrix model [25], role-based access control model [35], 
type-based access control model [34], or flexible access-control model [21]) is enforced. 
Since serializable histories are generated by most of the current single-level systems, so our 
repair model can be directly applied to single-level systems in most cases. However, there 
are some systems where each data item has multiple versions, and one-copy serializable 
histories are generated instead. Since an one-copy serializable history is view equivalent to 
a serial single-version history [8], our model can be used to repair the one-copy serializable 
history by rewriting the equivalent serial history. However, it should be noticed that pruning 
a rewritten history in multiversion databases is usually more complicated because during 
pruning we need to decide for a (dirty) data item which version should be read, which 
version should be updated, and which version should be discarded (i.e., the versions created 
by bad transactions can just be discarded). Detailed pruning algorithms are out of the scope 
of the paper. 
For a multilevel secure (MLS) database system, traditional concurrency control and re-
covery protocols, however, are usually not enough to satisfy security requirements [6], 
especially, they can cause signaling channels from high level processes to low level pro-
cesses. Therefore, secure transaction processing is required. Most of the recent research 
and development in secure concurrency control can be categorized into two different areas: 
one based on kernelized architecture and the other based on replicated architecture. These 
two are among the number of architectures proposed by the Woods Hole study group [9] 
to build multilevel secure DBMSs with existing DBMS technology instead of building a 
trusted DBMS from scratch. 
For kemelized architecture, several kinds of secure concurrency control protocols are 
proposed: (I) In [20, 29], several secure lock-based protocols are proposed. Although 
they do not always produce serializable schedules, our repair model can be directly applied 
to every serializable history generated by them. Extending our model to repair those non-
serializable schedules is out of the scope of the paper. (2) In [I], two secure timestamp-
based protocols are proposed. Although they produce only serializable histories to which 
our model can be directly applied, they are prone to starvation. In [17], a single-level 
timestamp-based scheduler is proposed which is secure and free of starvation. Although 
it produces one-copy serializable histories, our model can still be directly used to rewrite 
these histories (the reason is mentioned above). (3) In [5, 6, 17], three weaker notions 
of correctness, namely, levelwise serializability, one-item read serializability, and pairwise 
serializability, are proposed to be used as alternative for one-copy serializability such that 
the nature of integrity constraints in MLS databases can be exploited to improve the amount 
of concurrency. Extending our model to repair level wise, one-item read, and/or pairwise 
serializable histories is out of the scope of the paper. 
For replicated architecture, several secure concurrency control protocols are proposed in 
[10, 11, 18,30]. Since they all produce one-copy serializable histories, so our model can 
be directly applied to rewrite these histories. 
32 

RECOVERING FROM MALICIOUS TRANSACTIONS 
37 
In [22], a scheduler is proposed which is secure and produces one-copy serializable 
histories to which our model can be applied. However, it uses a multilevel scheduler which, 
therefore, has to be trusted, thus it is only suitable for the trusted subject architecture. 
Since in our repair model serial orders among transactions are captured from the log, 
so the applicability of our model is affected by logging protocols. In [32], a multilevel 
secure log manager is proposed to eliminate such covert channels as insert channels and 
flush channels which are caused by traditional logging protocols. Although Logical Log 
Sequence Numbers (LLSN) instead of physical Log Sequence Numbers (LSN) are provided 
in [32] to eliminate insert channels, we can still extract serial orders from the log because 
records of transactions within different security classes are still kept in the same log, and 
LLSNs can be translated to physical LSNs internally by the log manager. Moreover, since 
the mechanisms proposed to eliminate flush channels will not change the structure of the 
log, so our model can be directly applied to a system with such a log manager. 
9.1.2. Other issues. 
One criticism of the applicability of the method may be that if a bad 
transaction B; is detected too late, that is, if the latency time of B; is too long, then there 
can be too many affected good transactions to deal with, especially when they have caused 
further effects to the real world. For example, some real world decisions could be based on 
these affected transactions. At this situation, 'manual' recovery actions may be necessary. 
We counter this augment by noting that the latency time of B; is usually related to the 
amount of transactions affected by B;. The more transactions affected by B;, the more 
proofs of B; 's malicious actions can be collected by the intrusion detector, hence the shorter 
the latency time of Bi . Therefore, even if the latency time of Bi is very long, the amount of 
transactions affected by Bi may not be too large in many circumstances. At this situation, 
the algorithm may need more time since it needs to scan a long history, but the pruning may 
still be a short process if most of the transactions in the history are unaffected. Although 
the compensation approach may not be practical when the history is very long and the 
codes for compensating transactions have to be kept in the log, it can be used in almost 
all canned systems, which are very general in real world where the codes for transactions 
and compensating transactions are fixed for each transaction class. As the techniques of 
intrusion detection are advanced, the latency time of a bad transaction should become 
shorter, so our repair model will apply to more situations. 
As to the criticism that manual recovery actions can be necessary, note that when damage 
has been caused, the effects of these affected transactions to the real world are already 
there. No matter whether the history is repaired or not, some action to compensate these 
undesirable effects is required. In the real world, such manual recovery actions are basically 
unavoidable. Therefore, repairing the database such that a consistent database state where 
no effects of bad transactions are there could be generated can be viewed as a separate 
issue from manual recovery. In addition, our rewriting methods can help users to assess 
the degree of damages because B U AG can be identified. Therefore, the security admin-
istrator can know on which transactions (or on which customers) such manual recovery 
actions should be enforced. 
33 

38 
LIU, AMMANN AND JAJODIA 
9.2. 
Conclusion 
Tn an IW scenario it is necessary to undo committed malicious transactions for the pur-
pose of damage repair. Traditional recovery methods have the disadvantage of wiping out 
much good work along with the bad, and compensation methods are heavily dependent on 
application semantics. 
In this paper we developed the notion of rewriting execution histories for the purpose 
of removing the effects of a set of bad transactions and the affected good transactions 
that depend on the bad transactions. The fact that the transactions being moved during 
the rewriting are subsequently unwound greatly increases the flexibility of the rewriting 
methods. We developed a basic rewriting method that can unwind exactly the set B U AG, 
and then developed additional rewrites incorporating commutativity, inverses, and covers 
to save further transactions in AG. It is shown that our approach is strictly better at saving 
good transactions than a dependency-graph based approach. And in most situations, our 
approach is strictly better at saving good transactions than an approach which is based on 
commutativity only. It is also shown that besides recovery from malicious transactions, our 
approach can also be extended to may other applications such as malicious user isolation, 
system upgrades, optimistic replication protocols, and replicated mobile databases. 
Acknowledgment 
Liu, Ammann, Jajodia were partially supported by Rome Laboratory, Air Force Material 
Command, USAF, under agreement number F30602-97 -1-0139. 
Notes 
1. For a recent summary with an emphasis on the database context, see [4]. 
2. For some more spectacular examples, see Peter Neumann's RISKS digest in the newsgroup news: comp . risks 
orthe archive ftp: //ftp.sri.com/risks. 
3. We adapt the notation of commutativity from [28,40]. Transaction T2 commutes backward through transaction 
T\ if for any state s on which T\ T2 is defined, T2(T\ (s)) = T\ (T2(S»; Tl and T2 commute if each commutes 
backward through the other. Note that one-sided commutativity (i.e., commutes backward through) is enough 
for our purpose. 
4. The lifetime ofa saga begins when the saga is initiated, and ends when the saga terminates (commits or aborts). 
5. It is possible that in H; one part of a saga S, is in H;' and the other part of S; is in H; ~ H;'. 
6. This may happen because T;n may have already covered or inverted. 
References 
1. P. Ammann and S. Jajodia, "A timestamp ordering algorithm for secure, single-version, multi-level databases," 
in Database Security, V: Status and Prospects, C. Landwehr and S. Jajodia (Eds.), Amsterdam, North Holland, 
1992, pp. 23-25. 
2. P. Ammann, S. Jajodia, and P. Liu, "Recovery from malicious transactions," Technical report, George Mason 
University, 1998. http://www.isse.gmu.edul~pliulpapers/dynamic.ps. 
3. P. Ammann, S. Jajodia, and P. Mavuluri, "On the fly reading of entire databases," IEEE Transactions on 
Knowledge and Data Engineering, vol. 7, no. 5, pp. 834-838, October 1995. 
34 

RECOVERING FROM MALICIOUS TRANSACTIONS 
39 
4. P. Ammann. S. Jajodia, CD. McCollum, and B.T. Blaustein. "Surviving information warfare attacks on 
databases," in Proceedings of the IEEE Symposium on Security and Privacy, Oakland, CA, May 1997, 
pp.164-174. 
5. V. Atluri, S. Jajodia, and E. Bertino, "Alternative correctness criteria for concurrent execution of transactions 
in multilevel secure databases," IEEE Transactions on Knowledge and Data Engineering, vol. 8, no. 5, pp. 839-
854, October 1996. 
6. V. Atluri, S. Jajodia, and E. Bertino. 'Transaction processing in multilevel secure databases with kernelized 
architecture: Challenges and solutions," IEEE Transactions on Knowledge and Data Engineering, vol. 9, 
no. 5,pp. 697-708, 1997. 
7. B.R. Badrinath and Ramarnritham Krithi, "Semantics-based concurrency control: Beyond commutativity," 
ACM Transactions on Database Systems, vol. 17, no. I, pp. 163-199, March 1992. 
8. P.A. Bernstein, V. Hadzilacos, and N. Goodman, Concurrency Control and Recovery in Database Systems, 
Addison-Wesley: Reading, MA, 1987. 
9. Committee on Multilevel Data Management Security, Air Force Studies Board, aud National Research Coun-
cil. Multilevel Data Management Security. National Academy Press: Washington, DC, March 1983. 
10. O. Costich, "Transaction processing using an untrusted scheduler in a multilevel secure database with repli-
cated architecture," in Database Security, V: Status and Prospects, C. Landwehr and S. Jajodia (Eds.). 
Amsterdam. North Holland, 1992, pp. 173-189. 
II. O. Costich and J. McDermott, "A multilevel transaction problem for multilevel secure database systems and 
its solution for the replicated architecture," in Proceedings of the IEEE Symposium on Security and Privacy, 
Oakland, CA, 1992, pp. 192-203. 
12. S.B. Davidson, "Optimism and consistency in partitioned distributed database systems," ACM Transactions 
on Database Systems, vol. 9, no. 3, pp. 456-581, September 1984. 
13. H. Garcia-Molina, "Using semantic knowledge for transaction processing in a distributed database;' ACM 
Transactions on Database Systems, vol. 8, no. 2, pp. 186-213, June 1983. 
14. H. Garcia-Molina and K. Salem, "Sagas," in Proceedings of ACM-SIGMOD International Conference on 
Management of Data, San Francisco, CA, 1987, pp. 249-259. 
15. J. Gray, P. Helland, P. O'Neil, and D. Shasha, "The dangers of replication and a solution," in Proceedings of 
ACM-SIGMOD International Conference on Management of Data, Montreal, Canada, 1996, pp. 173-182. 
16. J. Gray and A. Reuter, Transaction Processing: Concepts and Techniques, Morgan Kaufmann Publishers, 
1993. 
17. S. Jajodia and V. Atluri, "Alternative correctness criteria for concurrent execution of transactions in multilevel 
secure databases," in Proceedings of the IEEE Symposium on Security and Privacy, Oakland. CA, 1992, 
pp. 216-224. 
18. S. Jajodia and B. Kogau, "Transaction processing in multilevel secure databases using replicated architecture," 
in Proceedings of the IEEE Symposium on Security and Privacy, Oakland, CA, 1990, pp. 360--368. 
19. S. Jajodia, P. Liu, and C.D. McCollum, "Application-level isolation to cope with malicious database users," in 
Proceedings of the 14th Annual Computer Security Application Conference, Phoenix, AZ, December 1998, 
pp.73-82. 
20. S. Jajodia, L. Mancini, and I. Ray, "Secure locking protocols for multilevel database management systems," 
in Database Security X: Status and Prospects, P. Samarati and R. Sandhu (Eds.), London: Chapman & Hall, 
1997. 
21. S. Jajodia, P. Samarati, and V.S. Subrahmanian, "A logical language for expressing authorizations," in Pro-
ceedings of the IEEE Symposium on Security and Privacy, Oakland, CA, 1997, pp. 31-42. 
22. T.F Keefe and W.T. Tsai, "Multi version concurrency control for multilevel secure database systems," in 
Proceedings of the IEEE Symposium on Security and Privacy. Oakland, CA, 1990, pp. 369-383. 
23. H.F. Korth, "Locking primitives in a database system," Journal of the ACM, vol. 30, no. I, pp. 55-79, January 
1983. 
24. H.F. Korth, E. Levy, and A. Silberschatz, "A formal approach to recovery by compensating transactions," in 
Proceedings of the International Conference on Very Large Databases, Brisbane, Australia, 1990, pp. 95-106. 
25. B.W. Lampson, "Protection," ACM Operating Systems Review, vol. 8, no.!, pp. 18-24, January 1974. 
26. D.B. Lomet, "MLR: A recovery method for multi-level systems," in Proceedings of ACM-SIGMOD Interna-
tional Conference on Management of Data, San Diego, CA, June 1992, pp. 185-194. 
35 

40 
LIU, AMMANN AND JAJODIA 
27. D. Lomet and M.R. Tuttle, "Redo recovery after system crashes," in Recovery Mechanisms in Database 
Systems, V Kumar and M. Hsu (Eds.), chap. 6. Prentice Hall PTR, 1998. 
28. N. Lynch, M. Merritt, W. Weihl, and A. Fekete, Atomic Transactions. Morgan Kaufmann, 1994. 
29. J. McDermott and S. Jajodia, "Orange locking: Channel-free database concurrency control," in Database 
Security, VI: Status and Prospects, B.M. Thuraisingham and c.E. Landwehr (Eds.), Amsterdam, North 
Holland, 1993, pp. 267-284. 
30. J. McDermott, S. Jajodia, and R. Sandhu, "A single-level scheduler for replicated architecture for multilevel 
secure databases," in Proceedings of the 7th Annual Computer Security Applications Conference, San Antonio, 
TX, 1991, pp. 2-11. 
31. C. Mohan, H. Pirahesh, and R. Lorie, "Efficient and flexible methods for transient versioning of records 
to avoid locking by read-only transactions," in Proceedings of ACM SIGMOD International Conference on 
Management of Data, San Diego, CA, June 1992, pp. 124-133. 
32. YR. Pesati, T.F. Keefe, and S. Pal, "The design and implementation of a multilevel secure log manager:' in 
Proceedings of the IEEE Symposium on Security and Privacy, Oakland, CA, 1997, pp. 55-64. 
33. C. Pu, "On-the-f1y, incremental, consistent reading of entire databases," Algorithmica, vol. 1, no. 3, pp. 271-
287, October 1986. 
34. R.S. Sandhu, "The typed access matrix model," in Proceedings of the IEEE Symposium on Security and 
Privacy, Los Alamitos, CA, 1992, pp. 122-136. 
35. R.S. Sandhu, EJ. Coyne, H.L. Feinstein, and C.E. Youman, "Role-based access control models," IEEE 
Computer, vol. 2, pp. 38-47, February 1996. 
36. M. Stonebraker, R. Katz, D. Patterson, and J. Ousterhout, "The design of XPRS," in Proceedings of the 
International Conference on Very Large Databases, Los Angeles, CA, 1988. pp. 318-330. 
37. H. Wachter and A. Reuter, "The contract model," in Database Transaction Models for Advanced Applications, 
A. Elmagarmid (Ed.), Morgan Kaufmann Publishers, 1991, pp. 219-263. 
38. G. Weikum, C. Hasse, P. Broessler, and P. Muth, "Multi-level recovery," in Proceedings of the Ninth ACM 
SIGACT-SIGMOD-SIGART Symposium of Principles of Database Systems, Nashville, Tenn, April 1990, 
pp. 109-123. 
39. G. Weikum and H.-J. Schek, "Concepts and applications of multilevel transactions and open nested transac-
tions," in Database Transaction Models for Advanced Applications, A.K. Elmagarmid (Ed.), chap. 13. Morgan 
Kaufmann Publishers, 1992. 
40. W.E. Weihl, "Commutativity-ba,ed concurrency control for abstract data types," IEEE Transactions on Com-
puters, vol. 37, no. 12, pp. 1488-1505, December 1988. 
36 

•• 
Distributed and Parallel Databases, 8, 41-83 (2000) 
''II1II" © 2000 Kluwer Academic Publishers. Manufactured in The Netherlands. 
Secure Concurrency Control in Firm Real-Time 
Database Systems* 
BINTO GEORGE 
JAY ANT R. HARITSA 
Database Systems Lab, Indian Institute of Science, Bangalore-560012, India 
Recommended by: 
Vijay Atluri and Pierangela Samarati 
binto@dsl.serc.iisc.ernet.in 
haritsa@dsl.serc.iisc.ernet.in 
Abstract. 
Many real-time database applications arise in electronic financial services, safety-critical installations 
and military systems where enforcing security is crucial to the success of the enterprise. For real-time database 
systems supporting applications with firm deadlines, we investigate here the performance implications, in terms 
of killed transactions, of guaranteeing multilevel secrecy. In particular, we focus on the concurrency control (CC) 
aspects of this issue. 
Our main contributions are the following: First, we identify which among the previously proposed real-time 
CC protocols are capable of providing covert-channel-free security. Second, using a detailed simulation model, 
we profile the real-time performance of a representative set of these secure CC protocols for a variety of security-
classified workloads and system configurations. Our experiments show that a prioritized optimistic CC protocol, 
OPT-WAIT, provides the best overall performance. Third, we propose and evaluate a novel "dual-CC" approach 
thal allows the real-time database system to simultaneously use different CC mechanisms for guaranteeing security 
and for improving real-time performance. By appropriately choosing these different mechanisms, concurrency 
control protocols that provide even better performancc than OPT-WAIT are designed. Finally, we propose and 
evaluate GUARD, an adaptive admission-control policy designed to provide fairness with respect to the distribution 
of killed transactions across security levels. Our experiments show that GUARD efficiently provides close to ideal 
fairness for real-time applications that can tolerate covert channel bandwidths of upto one bit per second. 
Keywords: 
real-time database, covert channels, concurrency control, firm deadlines, fairness, performance 
evaluation 
1. 
Introduction 
A Real-Time Database System (RTDBS) is a transaction processing system that is designed 
to handle workloads where transactions have individual timing constraints. Typically, the 
time constraint is expressed in the form of a completion deadline, that is. the application 
submitting the transaction would like it to be completed before a certain time in the future. In 
addition, from the application's performance perspective, a transaction that completes just 
before its deadline is no different to one that finishes much earlier. Therefore. in contrast to 
conventional DBMS where transaction throughput or response time is typically the primary 
* A partial and preliminary version of the results presented here appeared earlier in Secure Transaction Processing 
in Firm Real-Time Database Systems, Proc. of ACM SIGMOD IntI. Conf. on Management of Data, Tucson, 
Arizona, USA, May 1997. 

42 
GEORGE AND HARITSA 
performance metric, performance in an RTDBS is usually measured in terms of the ability 
of the system to complete transactions before their deadlines expire. 
Many RTDBS applications arise in electronic financial services, safety-critical instal-
lations and military systems where enforcing security is crucial to the success of the en-
terprise. For example, consider the environment of an electronic (open bid) auction on 
the World-Wide-Web with online auctioneers and bidders. Typically, the auction database 
contains "secret" information such as bidders personal details including private keys, credit-
worthiness and past bidding patterns; the purchase price and ownership history of the items 
that are being auctioned; the list of "in-house bidders"-these are bidders planted by the 
auction house to provoke other bidders by artificially hiking the maximum bid; etc. The 
database also contains "public" information such as bidder public keys and authentication 
certificates; the starting bid price, the minimum bid increment and the time for delivery 
for each item; the sequence and state of bids for items currently under auction; etc. It is 
expected that the secret information is known only to the auctioneers whereas the public 
information is available to both bidders and auctioneers. 
In the above environment, the auction service provider faces a problem of three dimen-
sional complexity: (I) There is a considerable amount of data to be consulted, processed 
and updated, and while doing so the database consistency should be maintained; (2) There 
are time constraints associated with various operations-for example, a bid is valid only if 
registered in the database within a pre-specified time period after submission (in the Flash 
Auction at http://wwwjirstauctiun.com. bids that arrive more than five minutes after the 
previous bid is registered are invalidated); (3) During every stage of the bidding process, 
data security must be ensured-unauthorized access to the secret information by bidders 
may help them gain unfair and financially lucrative advantages over other competitors. 
In this paper, we focus on the design of information systems that can simultaneously 
and effectively meet the above three challenges, that is, on the design of Secure Real-Time 
Database Systems (SRTDBS). Our study is conducted in the context of real-time appli-
cations with "firm-deadlines" [19]. For such applications, completing a transaction after 
its deadline has expired is of no utility and may even be harmful. Therefore, transactions 
that miss their deadlines are "killed", that is, immediately aborted and discarded from the 
system without being executed to completion. Accordingly, the performance metric is the 
percentage of killed transactions. l 
Our choice of firm-deadline applications is based on the observation that many real-time 
applications belong to this category. For example, in the 1-800 telephone service, a system 
responds with a "circuits are busy" message if a connection cannot be made before the 
deadline. Similarly, most Web-based services employ "stateless" communication protocols 
with timeout features. 
1.1. 
Security mechanisms 
For a DBMS to be considered secure, a number of requirements have been identified in the 
literature [7]. Among these, secrecy, that is, the prevention of unauthorized knowledge of 
secret data, is an especially important requirement for RTDBS due to the sensitive nature 
38 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
43 
of their application domains. In this paper, we focus exclusively on this issue and in the 
sequel use the term security synonymously with secrecy. 
Security violations in an RTDBS can occur if, for example, information from the "se-
cret" database is transferred by corrupt high security transactions to the "public" database 
where they are read by conspiring low security transactions. Such direct violations can be 
eliminated by implementing the classical BeIl-LaPadula security model [24] which impuses 
restrictions on the data accesses permitted to transactions, based on their security levels. 
The Bell-LaPadula model is not sufficient, however, to protect from "covert channels". 
A covert channel is an indirect means by which a high security transaction can transfer 
information to a low security transaction [23]. For example, if a low security transaction 
requests access to an exclusive resource, it will be delayed if the resource is already held 
by a high security transaction, otherwise it will be granted the resource immediately. The 
presence or absence of the delay can be used as a "signaling" or encoding mechanism by a 
high security transaction passing secret information to the low security transaction. Note 
that, from the system perspective, nothing "obviously illegal" has been done in this process 
by the conspiring transactions. 
Covert channels that use the DBMS's physical resources such as the CPU or the disk as 
the medium for passing on information can be tackled by introducing "noise" in the form 
of dummy transactions that make use of these resources (similar to the "pump" scheme 
proposed in [21]). However, this approach is impractical for covert channels that use data 
as the medium (for example, presence or absence of a lock on a pre-determined data item). 
This is because, unlike physical resources which are typically few in number, the number of 
data items is usually enormous, especially in a DBMS. In fact, in heavily loaded systems, 
noise at the physical resources may be generated "for free", but this will probably never be 
the case for data since it is trivial to insert an additional data item that is of relevance only to 
the conspiring transactions. Therefore, explicitly making data access covert-channel-free 
is more vital than doing the same for resource access. 
Covert channels based on data can be prevented by ensuring that low security transactions 
do not "see" high security transactions-this notion is formalized in [IS] as non-interference, 
that is, low security transactions should not be able to distinguish between the presence or 
absence of high security transactions. This can be implemented, for example, by providing 
higher priority to low security transactions whenever a conflict occurs between a low 
security transaction and a high security transaction. From a system perspective, it translates 
to implementing a concurrency control (CC) mechanism that supports the non-interference 
feature. In this paper, we investigate the design and performance implications of secure CC 
in the context of a firm-deadline RTDBS. 
1.2. 
Design challenges 
A variety of challenging problems arise when we attempt to integrate security into the 
RTDBS framework, as described below: 
1. An SRTDBS has to simultaneously satisfy two requirements, namely, provide security 
and minimize the number of killed transactions. Unfortunately, the mechanisms for 
39 

44 
GEORGE AND HARITSA 
achieving the individual goals often work at cross-purposes [16]. In an RTDBS, high 
priority is usually given to transactions with earlier deadlines in order to help their timely 
completion. On the other hand, in secure DBMS, low security transactions are given 
high priority in order to avoid covert channels (as described above). Now consider the 
situation wherein a high security process submits a transaction with a tight deadline in 
an SRTDBS. In this case, priority assignment becomes difficult since assigning a high 
priority may cause a security violation whereas assigning a low priority may result in a 
missed deadline. 
2. Although a variety of real-time CC protocols (e.g. [1,4, 19, 27]) have been proposed 
during the last decade, not all of them may be amenable to supporting the non-interference 
requirement. 
3. Apart from the noise approach mentioned earlier, covert channels can also be eliminated 
by static resource allocation policies, wherein each transaction security class has a set 
of pre-assigned resources. This approach is taken, for example, in replicated secure 
architectures wherein a multi-level secure DBMS is built by "stacking" single-level 
database systems. As pointed out in [6], the drawback of such strategies, however, is 
that they may result in an inefficient and inflexible use of resources. 
Yet another possibility is to use "snapshot" protocols (e.g. [5]), wherein two ver-
sions of data items are provided-for data of a given security level, higher security 
transactions access the older version whereas transactions of the same level access the 
current version. A problem with this approach, however, is that higher security transac-
tions, especially long-lived ones, have to base their results on arbitrarily stale data [6]. 
In fact, a performance study in [10] shows instances where the highest security level 
transactions read data items that have been subsequently overwritten by.fifteen other 
transactions. 
The above approaches are especially problematic in the real-time context since they 
may result in: (a) A large increase in the number of killed transactions due to poor 
resource utilization; or (b) Incorrect results since real-time applications typically respond 
to current situations and therefore should not utilize stale data. Due to these reasons, the 
challenge in the RTDBS domain is to design "dynamic" and "one-copy" policies that 
are demonstrably secure. 
4. A major problem arising out of the preferential treatment of low security transactions is 
that of "fairness"-a disproportionately large fraction of the high security transactions 
miss their deadlines and are killed. 
Note that this is an especially problematic issue because it is the "VIPs", that is, the 
high security transactions, that are being discriminated against in favor of the "common-
folk", that is, the low security transactions. 
Unfortunately, designing dynamic mechanisms for achieving fairness that are com-
pletely free of covert channels appears to be fundamentally impossible [22]. The issue 
then is whether it is possible to design fair systems while still guaranteeing that thc 
information leakage bandwidth (from covert channels) is within acceptable levels. 
In summary, for all of the above reasons, making a real-time CC manager implementation 
to be secure involves significant design complexity. 
40 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
45 
1.3. 
Contributions 
We have conducted a detailed study on designing CC managers that address the challenges 
described above, and report on the results here. Our main contributions are the following: 
1. We identify which among the previously proposed real-time CC protocols are capable 
of providing protection against covert channels, that is, which protocols can support the 
concept of non-interference. 
2. Using a detailed simulation model of a firm-deadline RTDBS, we profile the real-time 
performance of a representative set of secure CC protocols. Our simulations consider a 
variety of security-classified transaction workloads and system configurations. To isolate 
and quantify the performance effects of supporting covert channel security, we also eval-
uate the performance of the CC protocols in the context of a baseline system that prevents 
direct unauthorized access, but not covert channels (that is, it only supports the Bell-
LaPadula restrictions). Our experimental results indicate that a prioritized optimistic CC 
protocol, OPT-WAIT [19], provides the best overall performance. 
3. We evaluate the effectiveness of a novel dual-CC approach to secure transaction con-
currency control wherein different CC mechanisms are simultaneously used for guaran-
teeing security and for improving real-time performance, respectively. In particular, we 
investigate the performance of various combinations of CC mechanisms for resolving 
inter-security-level and intra-security-level data conflicts. Our results show that one of 
these combinations, S2PL-WAIT, which is composed of the S2PL [33] and OPT-WAIT 
algorithms, performs even better than OPT-WAIT. 
4. We present GUARD, an adaptive admission-control policy designed to achieve fairness 
in terms of the distribution of the killed transactions across the various security levels, 
while remaining within the information leakage bandwidth limits specified in the US 
military's "Orange Book" [11J, which defines security standards. Experimental evalua-
tion of GUARD shows it to provide close to ideal fairness with very little deterioration 
in the overall real-time performance. 
To the best of our knowledge, the above results represent the first detailed study of real-time 
database security in the firm-deadline context. 
1.4. 
Organization 
The remainder of this paper is organized as follows: The security model employed in 
our study is described in Section 2. Issues related to incorporating security in real-time 
CC protocols are discussed in Section 3 and a representative set of secure CC protocols 
are described in Section 4. Our new dual approach to secure real-time CC is presented in 
Section 5. The performance model is described in Section 6, and the results of the simulation 
experiments are highlighted in Section 7. Fairness issues are addressed in Section 8. Related 
work on real-time database security is reviewed in Section 9. Finally, in Section 10, we 
present the conclusions of our study. 
41 

46 
GEORGE AND HARITSA 
2. 
Security model 
As mentioned in the Introduction, our focus in this paper is on ensuring that secret informa-
tion is not made available to unauthorized users. Most secure DBMS attempt to achieve this 
goal by incorporating access control mechanisms based on the well-known Bell-LaPadula 
model [24]. This model is specified in terms of subjects and objects. An object is a data 
item, whereas a subject is a process that requests access to an object. Each object in the 
system has a classification level (e.g., Secret, Classified, Public, etc.) based on the secu-
rity requirement. Similarly, each subject has a corresponding clearance level based on the 
degree to which it is trusted by the system. 
The Bell-LaPadula model imposes two restrictions on all data accesses: 
1. A subject is allowed read access to an object only if the former's clearance is higher 
than or identical to the latter's classification. 
2. A subject is allowed write access to an object only if the former's clearance is identical 
to or lower than the latter's classification. 
The Bell-LaPadula conditions effectively enforce a "read below, write above" constraint 
on transaction data accesses (an example is shown in figure 1), and thereby prevent direct 
unauthorized access to secure data. They are not sufficient, however, to protect from covert 
channels [23], as described in the Introduction. 
There are two flavors of covert channels: storage channels and timing channels [2]. 
Storage channels include all mediums that allow the (direct or indirect) writing of a storage 
location by a high security transaction and the subsequent access of this storage location by a 
low security transaction-the failure or success of this access, or the value obtained from the 
access, can be modulated to signal secret information. Timing channels, on the other hand, 
include all mediums that allow a high security clearance transaction to signal information to 
a low security clearance transaction by modulating its own use of system resources in such 
a way that the change in response time observed by the low security clearance transaction 
TRANSACfION 
CLEARANCE 
SECRET 
PUBLIC 
Figure 1. 
Bell-LaPadula access restrictions. 
42 
RIW 
DATA 
CLASSIACA TION 
SECRET 
PUBLIC 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
47 
provides information. In this paper, we focus our attention exclusively on timing channels, 
since storage channels can be handled by incorporating additional access restrictions [14]. 
For tackling (timing) covert channels, we use the non-interference formalism described 
in [15] wherein low security transactions do not "see" high security transactions. 
Finally, to facilitate understanding of the performance results, we only consider en-
vironments wherein the security levels are fully-ordered-in general, huwever, security 
hierarchies may be partially ordered, typically in the form of lattices. 
2.1. 
Orange security 
For many real-time applications, security is an "all-or-nothing" issue, that is, it is a cor-
rectness criterion. In such "full-secure" applications, metrics such as the number of killed 
transactions or the fairness across transaction clearance levels are secondary pelformance 
issues. However, there are also applications for whom it is acceptable to have well-defined 
bounded-bandwidth covert channels in exchange for performance improvement [2]. For 
example, the US military's security standards, which are defined in the so-called "Orange 
Book" [11], specify that covert channels with bandwidth of less than one bit per second 
are typically acceptable-we will hereafter use the term "orange-secure" to refer to such 
applications. 
The underlying premise of "orange-secure" applications is that covert channels based 
on such low bandwidths are acceptable because: (a) These channels will take a long time 
to transfer a significant amount of information; (b) By the time the secret information is 
transferred it may well be obsolete; (c) The time taken to transfer information is long 
enough to considerably increase the possibility of detection; and (d) The performance or 
implementation cost associated with eliminating the covert channel is much higher than the 
cost of any leaked information. 
In this study, we consider the design of CC protocols initially for full-secure real-time 
applications, and then in Section 8, we extend our scope to orange-secure applications. 
3. Integrating security and concurrency control 
In this section, we discuss the issues involved in integrating security and concurrency control 
mechanisms in an RTDBS. 
3.1. 
Priority assignment 
As mentioned in the Introduction, assigning priorities in an SRTDBS is rendered difficult due 
to having to satisfy multiple functionality requirements. Given the paramount importance 
of security, the database system is forced to assign transaction priorities based primarily on 
clearance levels and only secondarily on deadlines. In particular, priorities are assigned as 
a vector P = (LEVEL, INTRA), where LEVEL is the transaction clearance level and INTRA is 
the value assigned by the priority mechanism used within the level. Clearance levels are 
numbered from one upwards, with one corresponding to the lowest security level. Further, 
43 

48 
GEORGE AND HARITSA 
priority comparisons are made in lexicographic order with lower priority values implying 
higher priority. 
With the above scheme, transactions at a lower clearance have higher priority than all 
transactions at a higher clearance, a necessary condition for non-interference. For the intra-
level priority mechanism, any priority assignment that results in good real-time performance 
can be used. For example, the classical Earliest Deadline assignment [28], wherein transac-
tions with earlier deadlines have higher priority than transactions with later deadlines. For 
this choice, the priority vector would be P = (LEVEL, DEADLlNE)-this priority assignment 
is used in most of our experimental evaluations. 
3.2. 
Supporting non-interference 
In conjunction with the above priority assignment policy, it would seem at first glance that, in 
principle, any real-time CC protocol could be used in an SRTDBS and that the actual choice 
of protocol would be based only on the relative performance of these protocols. However, 
not all the previously proposed real-time CC algorithms are amenable to supporting secu-
rity requirements. For example, consider the 2PL Wait Promote algorithm proposed in [1]: 
This protocol, which is based on 2PL, incorporates a priority inheritance mechanism [31] 
wherein, whenever a requester blocks behind a lower-priority lock holder, the lock holder's 
priority is promoted to that of the requester. In other words, the lock holder inherits 
the priority of the lock requester. The basic idea here is to reduce the blocking time of 
high priority transactions by increasing the priority of conflicting low priority lock holders 
(these low priority transactions now execute faster and therefore release their locks earlier). 
The Wait Promote approach is not suitable for SRTDBS. This is because it permits 
the blocking of high priority transactions by low priority transactions which violates the 
requirement of non-interference between the transactions of different security levels. 
To generalize the above observation, a real-time CC protocol that permits, to even a limited 
extent, high priority transactions to be adversely affected by low priority transactions, a 
phenomenon known as priority inversion in the real-time literature [31], cannot be used in a 
secure RTDBS. Apart from Wait Promote, other examples of real-time CC algorithms that 
fall into this category include 2PL-CR [1], 2PL-OSIBI [4] and WAIT-50 [19]. 
4. 
Secure CC protocols 
Tn this section, we describe the CC protocols that are evaluated in our study. These include 
two well-known real-time protocols: 2PL-HP [1], which is based on locking, and OPT-
WAIT [19], which is based on optimistic concurrency control. Both these protocols are 
completely free from priority inversion and can therefore be used to resolve conflicts in an 
SRTDBS. 
Apart from the above real-time algorithms, we also consider S2PL [33], a recently pro-
posed secure locking-based protocol that does not include any real-time-specific features. 
We include it here, however, for the following reasons: First, it serves as a baseline against 
which to compare the real-time CC algorithms. Second, we use it in one of the "dual-CC" 
44 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
49 
protocols evaluated later in this study. Finally, it has been used in some of the previous 
research work on SRTDBS (these are discussed in Section 9). 
We have also evaluated OPT-SACRIFICE [19], another real-time optimistic CC protocol, 
in our performance study. However, since it performed worse than OPT-WAIT in all of 
our experiments, we do not include it here-the results are available in [14]. We have 
also not included SOPT, a secure optimistic protocol proposed in [9, 34], since its basic 
structure is similar to OPT-SACRIFICE and, in addition, it lacks real-time characteristics 
as all priorities are based purely on transaction security levels. 
In the remainder of this section, we describe the 2PL-HP, OPT-WAIT and S2PL protocols. 
4.1. 
2PL High Priority 
The 2PL High Priority (2PL-HP) scheme [1] modifies the classical strict two-phase locking 
protocol (2PL) [12] by incorporating a priority conflict resolution scheme which ensures 
that high priority transactions are not delayed by low priority transactions. In 2PL-HP, 
when a transaction requests a lock on a data item that is held by one or more higher priority 
transactions in a conflicting lock mode, the requesting transaction waits for the item to be 
released (the wait queue for a data item is managed in priority order). On the other hand, 
if the data item is held by only lower priority transactions in a conflicting lock mode, the 
lower priority transactions are restarted and the requesting transaction is granted the desired 
lock. 2 Note that 2PL-HP is inherently deadlock-free if priorities are assigned uniquely (as 
is usually the case in RTDBS). 
4.2. 
OPT-WAIT 
The OPT-WAIT algorithm [19] modifies the classical forward (or broadcast) optimistic CC 
protocol (OPT) [30] by incorporating a priority wait mechanism. Here, a transaction that 
reaches validation and finds higher priority transactions in its conflict set is "put on the 
shelf", that is, it is made to wait and not allowed to commit immediately. This gives the 
higher priority transactions a chance to make their deadlines first. While a transaction is 
waiting on the shelf, it is possible that it may be restarted due to the commit of one of the 
conflicting higher priority transactions. If at any time during its shelf period, the waiting 
transaction finds no higher priority transactions remaining in its conflict set, it is committed, 
restarting in the process the lower priority transactions (if any) in its conflict set. 
4.3. 
S2PL 
A secure locking-based protocol called Secure 2PL (S2PL) was recently proposed in [33]. 
The basic principle behind Secure 2PL is to try to simulate the execution of conventional 
2PL without blocking the actions of low security transactions by high security clearance 
transactions. This is accomplished by providing a new lock type called vi rtuallock, which is 
used by low security transactions that develop conflicts with high security transactions. The 
actions corresponding to setting of virtual locks are implemented on private versions of the 
45 

50 
GEORGE AND HARITSA 
data item (similar to optimistic CC). When the conflicting high security transaction commits 
and releases the data item, the virtual lock of the low security transaction is upgraded to a 
real lock and the operation is performed on the original data item. To complete this scheme, 
an additional lock type called dependent virtual lock is required apart from maintaining, 
for each executing transaction Ti, lists of the active transactions that precede or follow Ti 
in the serialization order. The complete details are given in [33]. 
In our implementation of S2PL, we have had to make some modifications since the 
algorithm (as described in [33]) does not eliminate interference under all circumstances-
the details of the security loopholes and our fixes for these loopholes are given in Appendix B. 
5. 
The dual-CC approach to secure concurrency control 
In this section, we move on to discussing our new dual-CC approach to secure real-time 
concurrency control. Our design is based on the observation that in the secure environment 
there are two categories of conflicts: inter-level and intra-level. Inter-level conflicts are 
data conflicts between transactions belonging to different security clearance levels whereas 
intra-level conflicts are data conflicts between transactions of the same level. The important 
point to note here is that only inter-level conflicts can result in security violations, not intra-
level conflicts. This opens up the possibility of using different CC strategies to resolve 
the different types of conflicts. In particular, we can think of constructing mechanisms 
such that inter-level conflicts are resolved in a secure manner while intra-level conflicts are 
resolved in a timely manner. For example, S2PL could be used for inter-level conflicts 
while OPT-WAIT could be used to resolve intra-level conflicts.3 
The advantage of the dual-CC approach, pictorially shown in figure 2, is that the RTDBS 
can maximize the real-time performance, by appropriate choice of intra-level CC protocol, 
without sacrificing security. This is in marked contrast to the tradeoff approach suggested 
in [10] which requires the application to compromise on security in order to achieve enhanced 
real-time performance (a more detailed assessment of the tradeoff approach is presented in 
Section 9). Another advantage of the dual-CC approach is that the separation of security 
and timeliness concerns makes it possible to use even un secure real-time CC algorithms 
(e.g., Wait-Promote, WAIT-50) for resolving intra-level conflicts! The dual-CC approach 
therefore empowers the use, even in the secure RTDBS domain, of the rich set of real-time 
CC algorithms developed during the last decade. 
5.1. 
Ensuring serializability 
At first glance, it may appear that concurrently using multiple CC mechanisms could result 
in violation of the transaction serializability requirement. This could happen, for example, 
if the serial orders enforced by the individual mechanisms were to be different. A detailed 
study of a generalized version of this problem is presented in [37], wherein the transac-
tion workload consists of a mix of transaction classes and the objective is to allow each 
transaction class to utilize its preferred CC mechanism. They propose a database system 
architecture wherein intra-class conflicts are handled by the class's preferred CC mana-
ger while inter-class conflicts are handled by a new software module called the Master 
46 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
51 
Dual Approach 
Intra-Conflicts 
Inter -Conflicts 
I 
Real-Time CC 
I 
Secure CC 
Figure 2. 
Dual approach. 
Concurrency Controller (MCC) that interfaces between the transaction manager and the 
multiple CC managers. The MCC itself implements a complete concurrency control mech-
anism and ensures a single global serialization order in the entire database system by using 
a Global Ordering Scheme [37]. 
For our study, we adapt the above architecture to the secure real-time environment. In 
our implementation, each security level has a Local Concurrency Controller (LCC) that 
resolves intra-level data conflicts. The LCCs can use any high performance real-time CC 
protocol without risking security violations. The Master Concurrency Control mechanism, 
however, is chosen to be a secure protocol and is responsible for handling all inter-level data 
conflicts. Figure 3 shows an example ofthis implementation for a two security level system. 
The proof of correctness and other details of our adaptation of the MCC architecture are 
given in Appendix A. 
5.2. 
Dual-CC protocols 
We have developed and evaluated three protocols, based on the dual-CC approach: WAIT-
HP, HP-WAIT and S2PL-WAIT. These protocols are described below: 
WAIT-HP: Inter-level data conflicts are resolved by OPT-WAIT and intra-level conflicts 
are resolved by 2PL-HP. 
47 

52 
GEORGE AND HARITSA 
LCC 
(SECRET) 
M 
C 
C 
LCC 
(PUBLIC) 
Figure 3. 
MCC architecture. 
HP-WAIT: This protocol is the "mirror" of WAIT-HP-inter-level data conflicts are re-
solved by 2PL-HP while intra-level conflicts are resolved by OPT-WAIT. 
S2PL-WAIT: Inter-level data conflicts are resolved by S2PL and intra-level conflicts are 
resolved by OPT-WAIT. The scheme operates as follows: After the intra-level conflicts 
are resolved by OPT-WAIT, the transaction enters the "virtual commit" state, and subse-
quently the inter-level data conflicts are resolved by S2PL. After all conflicts are resolved, 
a transaction "really" commits. Recall that S2PL is not a restart-oriented algorithm and 
allows a high security transaction to continue regardless of the virtual commit of a low 
security conflicting transaction. 
6. 
Simulation model 
In the previous sections, we presented a variety of secure CC protocols. To evaluate the 
real-time performance of these algorithms, we developed a detailed simulation model of 
a firm-deadline RTDBS, similar to that described in [19]. A summary of the key model 
parameters is given in Table 1. 
The model, shown in figure 4, has six components: a Database that models the data 
and its layout; a Source that generates the transaction workload; a Transaction Manager 
that models the execution of transactions; a Resource Manager that models the hardware 
resources; a Concurrency Control CCC) Manager that controls access to shared data; and 
a Sink that gathers statistics on transactions exiting the system. A separate instance of the 
CC Manager is created for each of the CC protocols evaluated in our experiments. 
6.1. 
Database model 
The database is modeled as a collection of DBSize pages that are uniformly randomly dis-
tributed across all of the system disks. The database is equally partitioned into ClassLevels 
48 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
Table I. 
Simulation model parameters, 
Parameter 
Meaning 
DBSize 
Number of pages in the database 
ClassLevels 
N umber of Classification Levels 
NumCPUs 
Number of processors 
NumDisks 
Number of disks 
PageCPU 
CPU time for processing a data page 
PageDisk 
Disk service time for a data page 
CCReqCPU 
Concurrency Control Overhead 
ArrivalRate 
Transaction arrival rate (per second) 
ClearLevels 
Number of Clearance Levels 
SlackFactor 
Slack Factor in Deadline Assignment 
TransSize 
Average transaction size (in pages) 
WriteProb 
Page write probability 
SOURCE 
TRANSACTION MANAGER 
CONCURRENCY CONTROL 
MANAGER 
Figure 4. 
Simulation model. 
Default value 
1000 
2 (Secret, Public) 
10 
20 
10 ms 
20 ms 
I ms 
0-100 
2 (Secret, Public) 
4.0 
16 
0.5 
RESOURCE 
MANAGER 
53 
security classification levels (for example, if the database has 1000 pages and the number 
of classifications is 5, pages I through 200 belong to levell, pages 20 I through 400 belong 
to level 2, and so on). 
6.2. 
System model 
The system consists of a shared-memory multiprocessor DBMS operating on disk-resident 
data (for simplicity, we assume that all data is accessed from disk and buffer pool 
49 

54 
GEORGE AND HARITSA 
considerations are therefore ignored). The physical resources of the database system con-
sist of NumCPUs processors and NumDisks disks. There is a single common queue for the 
CPUs and the service discipline is Pre-emptive Resume, with preemptions being based on 
transaction priorities. Each of the disks has its own queue and is scheduled according to a 
Head-Of-Line (HOL) policy, with the request queue being ordered by transaction priority. 
The PageCPU and PageDisk parameters capture the CPU and disk processing times per 
data page, respectively. For concurrency control requests, the associated CPU processing 
overhead is specified by the CCReqCPU parameter. 
6.3. 
Workload model 
Transactions are generated in a Poisson stream with rate ArrivalRate and each transaction 
has an associated security clearance level and a firm completion deadline. A transaction is 
equally likely to belong to any of the ClearLevels security clearance levels. (For simplicity, 
we assume in this study that the categories (e.g., Secret, Public) for data classification 
and transaction clearance are identical). Deadlines are assigned using the formula DT = 
AT + SlackFactor * RI' , where DT, AT and RT are the deadline, arrival time and resource 
time, respectively, of transaction T. The resource time is the total service time at the 
resources that the transaction requires for its data processing. The SlackFactor parameter 
is a constant that provides control over the tightness/slackness of transaction deadlines. 
If a transaction has not completed by its deadline, it is immediately killed (aborted and 
discarded from the system). 
An important point to note here is that while the workload generator utilizes information 
about transaction resource requirements in assigning deadlines, we do not assume that this 
information is available to the SRTDBS itself since such knowledge is usually hard to come 
by in practical environments. 
Each transaction consists of a sequence of page read and page write accesses. The 
number of pages accessed by a transaction varies uniformly between half and one- and-
a-half times the value of TransSize. The WriteProb paramcter determines the probability 
that a transaction operation is a write. Due to security reasons, each transaction can only 
access data from a specific segment of the database, and page requests are generated by 
uniformly randomly sampling (without replacement) from the database over this segment's 
range. The permitted access range is determined by both the security clearance level of the 
transaction and the desired operation (read or write), and is according to the Bell-LaPadula 
specifications: a transaction cannot read (resp. write) pages that are classified higher (resp. 
lower) than its own clearance level. A transaction that is restarted due to a data conflict 
has the same clearance level and makes the same sequence of data accesses as its original 
incarnation. 
6.4. 
Transaction execution 
A transaction read access involves a CC request to get access permission, followed by a 
disk I/O to read the page, followed by a period of CPU usage for processing the page. Write 
requests are handled similarly except for their disk I/O-their disk activity is deferred until 
50 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
55 
the transaction has committed. We assume that the RTDBS has sufficient buffer space to 
allow the retention of updates unti I commit time. 
6.5. 
Priority assignment 
In general, the transaction priority assignment used at all the RTDBS components is p = 
(LEVEL, DEADLINE), based on the discussion in Section 3.1.4 The only exception is for the 
DIRECT environment, described below. 
6.5.1. The DIRECT environment. 
To help isolate and understand the performance cost 
that occurs due to having to eliminate covert channels, we have also simulated the per-
formance achievable in the absence of covert channel security requirements. That is, the 
performance achievable if only Bell-LaPadula conditions had to be satisfied. For this sce-
nario, a priority assignment of (p = DEADLINE) is used. In the following experiments, 
we will refer to the performance achievable under this scenario as DIRECT since the 
Bell-LaPadula conditions prevent direct unauthorized access to secret data. 
6.6. 
Performance metrics 
The primary performance metric of our experiments is KillPercent, which is the percentage 
of input transactions that the system is unable to complete before their deadlines. We 
compute this percentage also on a per-clearance-level basis. KillPercent values in the range 
of 0 to 20 percent are taken to represent system performance under "normal" loads, while 
KillPercent values in the range of 20 percent to 100 percent represent system performance 
under "heavy" loads [19].5 
An additional performance metric is ClassFairness which captures how evenly the killed 
transactions are spread across the various clearance levels. This is computed, for each class 
i as the ratio lOO-KillPercent(i). With this formulation a protocol is ideally fair if the fairness 
, 
100-KlllPercent 
' 
value is 1.0 for all classes. Fairness values greater than one and lesser than one indicate 
positive bias and negative bias, respectively. 
6.7. 
Default parameter settings 
The default settings used in our experiments for the workload and system parameters are 
listed in Table 1. These settings were chosen to ensure significant data contention and 
resource contention in the system, thus helping to bring out the performance differen-
ces between the various CC protocols. They are also consistent with the values used in earlier 
RTDBS studies, facilitating comparison with their results. While the absolute performance 
profiles of the CC protocols would, of course, change if alternative parameter settings are 
used, we expect that the relative performance will remain qualitatively similar since the 
model parameters are not protocol-specific. 
51 

56 
GEORGE AND HARITSA 
7. Experiments and results 
Using the real-time database model described in the previous section, we evaluated the 
performance of the CC protocols presented in Sections 4 and 5 for a variety of security-
classified transaction workloads and system configurations. The simulator was written 
using the Simscript II.5 discrete event simulation language [25]. In this section, we present 
the performance results from our simulation experiments. Due to space constraints, we 
present results for only a set of representative experiments here-the others are available 
in [14]. In our discussion, only statistically significant differences are considered-all the 
performance graphs show mean values that have relative half-widths about the mean of less 
than 10 percent at the 90 percent confidence level, with each experiment having been run 
until at least 10000 transactions were processed by the system. 
The simulator was instrumented to generate several other statistics, including resource 
utilizations, number of restarts, etc. These secondary measures help to explain the perfor-
mance behavior of the CC protocols under various workload and system conditions. 
7.1. 
Experiment 1: Resource and data contention 
The settings of the workload parameters and system parameters for our first experiment are 
the default values listed in Table I, resulting in a system with two security levels: Secret 
and Public, and significant levels of both resource and data contention. For this system, 
figure 5(a) and (b) show the KillPercent behavior as a function of the overall transaction 
arrival rate.6 In figure 5(a), the overall kill percentages of the fully secure algorithms and 
their DIRECT (Bell-LaPadula) counterparts is profiled. We see here that at normal loads 
the performance of the secure algorithms is worse than that of their DIRECT counterparts. 
In contrast, under heavy loads the performance of the secure algorithms is actually better 
than that of the DIRECT algorithms. The reason for this is that while the Earliest Deadline 
priority assignment is excellent for a set of tasks that can be completed before their deadlines, 
it becomes progressively worse as the task set overloads its capacity [20]. In this situation, 
the secure protocols feature of grouping the transactions into prioritized levels means that 
Earliest Deadline is operational within smaller sets of transactions, leading to improved 
performance at higher loads. In summary, although elimination of covert channels results 
in performance degradation at normal loads, it reduces the kill percentage under heavy 
loads. 
Focusing on the secure real-time algorithms, we observe first in Figure 5(a) that the 
performance of 2PL-HP is significantly worse than that of OPT-WAIT. In fact, 2PL-HP's 
performance is no better than that of S2PL which, as mentioned in Section 4, is a non-
real-time protocol! The poor performance of 2PL-HP is primarily because of its "wasted 
restarts" problem, which was its main drawback in unsecure real-time CC also [19]: A 
transaction may be restarted by a higher priority transaction that later misses its dead-
line. This means that the restart did not result in the higher priority transaction meeting 
its deadline. In addition, it may cause the lower priority transaction to miss its dead-
line as well, apart from wasting the resources invested in the transaction prior to its 
restart. 
52 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
57 
Figure Sa: Overall Kill Percent 
Figure Sb: Level Kill Percent 
100 
100 
10 - -/-&---::1 
/ -0--
" /-
(3 
~-
80 
80 
l?f I 
III 
II 
II 
PI 
I 
a" 
I 
I 
I lSi 
I 
E 60 
E 60 
I I 
X 
.0 
Q) 
Q) 
~ 
~ 
fiJI 
I 
lSI 
Q) 
Q) 
I I 
I 
.0' 
0.. 40 
0.. 40 
0 1111 
I I8i 
S2 
S2 
I I 
I.-
0-
p~ .. 
x 
20 
20 
I 
.0 
,eJ 
Jo-' 
.x· 
. .x . 
o 
20 
40 
60 
80 
100 
o 
20 
40 
60 
80 
100 
2 
II 1.S 
I 
0 
~ 
1:: 
<tI 
(il 
Q) 
II: o.S 
o 
~ 0.8 
en 
en 
~ 0.6 
'm 
u. 0.4 
0.2 
Arrival Rate -> 
Figure Sc: Level restart ratio 
I 9 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure Se: Faimess 
IDEAL 
-1!1 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure 5. 
Resource and data contention. 
1.2 
o 
Arrival Rate -> 
Figure Sd: Secret restart ratio 
ICl 
/ 
.... CSl 
p.. 
\ 
0; 
0 
\ 
. 
\ 
~ 
/' -~ 
\\ 
/ 
" 
\ 
)< 
.\ 
\ 
X x·.x. 
'.x. 
\ 
x 
··x. 0 .. ,., \~ 
".""" 
·x· 
20 
40 
60 
80 
Arrival Rate -> 
LEGEND 
x OPT-WAIT 
Fig. Sa 
o 2PL - HP 
Direct 
100 
III S2PL 
-
Secure 
Fig. Sb, Sc, Se 
Fig. Sd 
Public 
Intra 
Secret 
Inter 
53 

58 
GEORGE AND HARITSA 
The effect of the wasted restarts problem is magnified in the secure domain for the 
following reason: In unsecure real-time CC, a transaction that is close to its deadline would 
usually not be restarted since it would have high priority. However, in the secure model, 
where the transaction level is also a factor in the priority assignment, Secret transactions 
that are close to their deadlines may still be restarted due to data conflict with a Public 
transaction. 
Moving on to OPT-WAIT, we find that it provides the best performance across the entire 
loading range. Further, there is a change of performance behavior in the secure environment 
in that the gap between its performance and that of 2PL-HP is more than that observed 
for unsecure real-time CC [19]. This is due to the access pattern restrictions of the Bell-
LaPadula model: The definition of conflict in forward optimistic CC is that a conflict 
exists between the validating transaction V and an executing transaction E if and only if 
the intersection of the write set of V and the current read set of E is non-empty. For the 
LaPadula model, where blind writes are permitted due to the "read-below, write-above" 
paradigm, optimistic algorithms will correctly conclude that there is no conflict between 
items that are in the intersection of the write set of V and the write set of E but not in the 
read set of E. In fact, it is easy to see that a validating Secret transaction will never have 
conflicts with executing Public transactions in this model. Note that for 2PL-HP, however, 
blind-writes can unnecessarily result in write-write conflicts and cause either blocking or 
restarts, thereby further deteriorating its performance. 
Finally, considering S2PL, we find that it manages to perform on par with 2PL-HP in spite 
of not being deadline cognizant. This is due to its "optimistic-like" feature of virtual commit, 
which considerably reduces the amount of blocking associated with 2PL. This phenomenon 
is similar to that seen in [19], wherein a conventional (non-real-time) optimistic protocol 
performed better than locking-based real-time protocols. 
In figure 5(b), we present the kill percentages of the various CC protocols on a per-
security-level basis. This graph clearly shows how the high-security Secret transaction class 
(dashed lines) suffers much more than the Public transaction class (dotted lines) to satisfy 
the goal of avoiding covert channels. Figure 5(c) provides statistics about the corresponding 
breakup of the "restarts ratio" (the average number of restarts of a transaction) on a level 
basis. We see here that Secret transactions are restarted much more often than Public 
transactions under normal loads. Under heavy loads the number of restarts decrease for 
Secret transactions since resource contention, rather than data contention, becomes the 
more dominant reason for these transactions missing their deadlines. 
In figure 5(d), we present a different view of the transaction restarts picture. Here, the 
restarts of Secret transactions are categorized into those caused by Public transactions (i.e., 
inter-level restarts) and those caused by Secret transactions (i.e., intra-level restarts). Note 
that this breakup is meaningful only for Secret transactions since all restarts are intra-level 
for Public transactions. The graph clearly shows that Secret transactions suffer more from 
inter-level conflicts (dashed lines) than from intra-level conflicts (dotted lines) over most 
of the loading range. 
Finally, in figure 5(e), we plot the fairness factor of each CC protocol for the Secret 
transaction class. We observe that at light loads when virtually all transactions make their 
deadlines, all the CC protocols are (trivially) fair. As the loading increases, however, the 
54 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
59 
protocols become increasingly unfair since they selectively miss the deadlines of Secret 
transactions to accommodate the Public transactions. With regard to the relative fairness 
of the secure real-time algorithms, the graph clearly shows that OPT-WAIT provides much 
better fairness than 2PL-HP, and that OPT-WAIT is the best overall. It may seem in figure 5( e) 
that, at high loads, S2PL is more fair than OPT-WAIT. Note, however, that this is really 
a "pseudo-fairness" since it arises out of S2PL, due to its non-real-time nature, missing a 
large fraction of the Public transaction deadlines, rather than out of meeting the deadlines 
of more Secret transactions. 
In summary, for the workload and system configuration considered in this experiment, 
OPT-WAIT provides the lowest kill percentage, both on an overall basis and on a per-level 
basis, and the best overall fairness. 
7.2. 
Experiment 2: Pure data contention 
The goal of our next experiment was to isolate the influence of data contention on the 
performance of the CC protocols. For this experiment, therefore, the resources were made 
"infinite", that is, there is no queueing for these resources [3].7 
The performance results for this system configuration are presented in figure 6(a)-(c). We 
observe in these figures that the differences in the relative performance of the various proto-
cols increases as compared to those seen in the previous experiment. The overall (figure 6(a» 
as well as the per-level (figure 6(b» kill percentages of OPT-WAIT are considerably better 
than those of 2PL-HP. Here, OPT-WAIT does better than 2PL-HP for two reasons: First, 
the basic wasted restarts problem of 2PL-HP occurs here too and is magnified due to the 
higher level of data contention. Second, the blocking component of 2PL-HP reduces the 
number of transactions that are making progress in their execution. This blocking causes an 
increase in the average number of transactions in the system, thus generating more conflicts 
and a greater number of restarts. With OPT-WAIT, however, transactions are never blocked 
for data access. 
Finally, in figure 6(c), which profiles the fairness factors of the protocols, OPT-WAIT is 
almost ideally fair over most of the loading range since it misses very few deadlines overall. 
In contrast, 2PL-HP and S2PL are noticeably unfair with increasing loading levels. 
In the next set of experiments in this section, we do not present S2PL's performance 
since it performed significantly worse than OPT-WAIT in all of these experiments. We 
will, however, return later to S2PL as part of a dual-CC protocol in Experiment 7, which 
evaluates the dual-CC approach. 
7.3. 
Experiment 3: The effect of write probability 
This experiment was conducted to profile the effect of the write probability on the real-time 
performance for three representative transaction arrival rates: LOW (arrival rate = 15), 
MEDIUM (arrival rate = 50) and HIGH (arrival rate = 80). For each of these arrival rates, 
the write probability was varied from 0.0 to 1.0 in steps of 0.1 and the resulting performance 
was observed. 
55 

60 
100 
80 
" 
I 
"E 60 
Q) 
~ 
Q) 
c... 40 
:i! 
20 
o 
~ 0.8 
UJ 
UJ 
~ 0.6 
'ro 
u.. 0.4 
0.2 
Figure 6a: Overall Kill Percent 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure 6c: Fairness 
IDEAL 
- -
~- --x 
"€l_ --0 
OL---~--~----~--~----
o 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure 6. 
Pure data contention. 
100 
80 
" 
I 
"E 60 
Q) 
~ 
Q) 
c... 40 
S2 
20 
o 
GEORGE AND HARITSA 
Figure 6b: Level Kill Percent 
20 
40 
60 
80 
100 
Arrival Rate -> 
LEGEND 
x OPT-WAIT 
o 2PL- HP 
181 S2PL 
Fig.6a 
Direct 
Secure 
Fig. 6b, 6c 
Public 
Secret 
Figure 7 shows the KiliPercent values for MEDIUM loads (we omit the HIGH and LOW 
performance graphs for brevity as the results are qualitatively similar). The locking protocol 
2PL-HP shows poor performance at all write probability levels as compared to the OPT-
WATT protocol and this difference is magnified at high write probability levels. For OPT-
WAIT, since the inter-level writes are blind writes (due to Bell-LaPadula conditions), write-
write conflicts do not appear in the conflict set as explained in Experiment 1. Therefore, at 
high write probabilities, when most of the operations are writes rather than reads, the data 
conflicts in fact reduce in frequency. This benefit is not realized by 2PL-HP, however, as 
discussed in Experiment 1. 
7.4. 
Experiment 4: Restricted write model 
In many secure systems, the Bell-LaPadula model of "read below, write above" is further 
restricted to allow only "read below", that is, blind writes by low security transactions to 
high security data are disalluwed. We conducted experiments to evaluate the performance 
56 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
100 
80 
60 
E 
~ 
(I) 
n. 
~ 
LEGEND 
x OPT-WAIT 
o 2PL-HP 
~~-------0~.2--------0~.-4--------0~.6--------0~.8--------~ 
Write Probability 
Figure 7. 
Effect of write probability (MEDIUM arrival rate). 
61 
behavior of the CC protocols under this Restricted Write model and a sample result is 
shown in figure 8(a)-(c) for the same environment as that modeled by Experiment 1. In 
these figures, we observe thatthe KillPercent behavior (figure 8(a) and (b» is similar to that 
of Experiment 1. A noticeable difference, however, is that the Public kill percentages are 
now higher for all the protocols whereas the Secret kill percentages are lower, especially 
under light loads. The reasons for this behavior are as follows: 
• The increase in missed Public deadlines is due to the increased intra-level data conflicts 
arising out ofthe confinement of Public writes to the Public partition of the database. This 
is further confirmed in figure 8(c), where we observe that the Public restart ratios--for 
all the protocols-are considerably higher as compared to their corresponding values in 
Experiment 1. 
• In contrast, the Secret performance improves, especially under light loads, because its 
inter-level data conflicts are reduced since inter-level writes are disallowed. At high loads, 
however, the improvement becomes negligible because the physical resources are mainly 
hogged by the Public transactions and therefore Secret transactions miss their deadlines 
due to resource contention, rather than data contention. 
7.5. 
Experiment 5: SRTDBS versus MULTI-CLASS RTDBS 
There have been quite a few studies of RTDBS systems (e.g., [10]) where the input is 
composed of a set of disjoint prioritized transaction classes. An SRTDBS is similar to 
such a generic multi-class RTDBS with the security levels in the SRTDBS corresponding 
57 

62 
100 
80 
E 
(]) 60 
e 
(]) 
a. 
~ 40 
20 
o 
2.5 
2 
o 
~ 1.5 
t 
<1l 
1ii 
(]) a: 
0.5 
o 
Figure 8a: Overall Kill Percent 
20 
40 
60 
80 
100 
Arrival Rate 
Figure 8c: Level Restart Ratio 
. 0' 
o . 
S:"'Q 
0.' 
\ 
/0 
\ .. X 
.. ·x. 
(/) 
. !is 
,;j 
.-:>(' ' x  
~";2' \.: 'b.,. 
, 
.... 
x- _ '--
20 
40 
60 
80 
100 
Arrival Rate 
Figure 11. 
Restricted write model. 
100 
80 
E 
(]) 60 
e 
(]) a. 
~ 40 
20 
o 
GEORGE AND HARITSA 
Figure 8b: Level Kill Percent 
~;;~ 
I 
,¢ 
, I 
9 I 
o' 
/ 
I 
.' 
/ 
I q 
o cf 
/.~ 
o .' / 
/ ci / 
o . / 
/ q 
.' 
o X.x··X 
20 
40 
.X 
60 
Arrival Rate 
LEGEND 
x 
.x 
80 
100 
-- Overall 
x OPT-WAIT 
.. Public 
0 2PL - HP 
- - - - Secret 
to the transaction classes in the multi-class environment. A major difference, however, 
is that the SRTDBS enforces the Bell-LaPadula conditions for data access, whereas the 
entire database is usually accessible to all classes in the multi-class environment. We 
conducted an experiment to evaluate the relative behavior of the SRTDBS and MULTI-
CLASS systems for the environment modeled in Experiment 1. The priority assignment 
for the MULTI-CLASS system is the same as that of the SRTDBS, namely, P = (LEVEL, 
DEADLINE). 
The results of this experiment are shown in figure 9. We see here that, qualitatively, 
the SRTDBS performance is very similar to the MULTI-CLASS performance. Interest-
ingly, however, the quantitative performance of the SRTDBS is virtually identical to that 
of MULTI-CLASS for the Public transactions and visibly better with regard to the Secret 
transactions. The reason for this behavior is as follows: 
• For the Public transactions, the conflict level in the SRTDBS case is proportional to 
R~~£2, where Rand W /2 are the proportion of Public reads and writes to the Public 
partition, and Dj2 is the Public partition size. Note that Public transactions do not have 
conflicts in the Secret partition due to the writes being blind writes. 
58 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
100 
80 
...... c: 60 
Q) 
~ 
Q) 
a.. 
~ 40 
20 
o 
@ . .:;c-:,,"':;"..H 
0'> x..:/ 
. /0 :x: 
(1/ 
.'1 
Icj 
.II 
/' / 
'/.1 
./ 
(J)/ 
IX 
/1 
1Cj> 
¢I 
'1 
.', 
I(J) 
Xi 
'1 
l~ 
20 
40 
60 
Arrival Rate 
80 
LEGEND 
MULn-CLASS - ""bI~ 
MUl TI-CLASS - Secret 
SRTDBS - Public 
SRTDBS - Secret 
x OPT-WAIT 
o2Pl-HP 
Figure 9. 
SRTDBS versus MULTI-CLASS. 
63 
!8! 
100 
In the MULTI-CLASS environment, the above computation works out to R~W since 
all Public reads and writes are over the entire database. 
Since the two conflict levels are the same, the performance of the Public transactions 
does not alter noticeably between the SRTDBS and MULTI-CLASS systems . 
• For the Secret transactions, Secret writes conflict only among themselves in the SRTDBS 
environment. In the MULTI-CLASS environment, however, the Secret writes conflict 
with the Public reads and writes in the Public partition, resulting in increased conflict 
levels. This results in the performance of the SRTDBS system being superior to that of 
the MULTI-CLASS system. 
7.6. 
Experiment 6: Five security levels 
The previous experiments modeled a two-security-level system. We now investigate how 
the performance behavior scales with the number of security levels by modeling a system 
with jive security levels: TopSecret, Secret, Confidential, Classified and Public. The 
results for this experiment are shown in figure lO(a)-(e). 
In figure lO(a), we see that there is a greater difference between the performance of 
the secure algorithms and their DIRECT counterparts at normal loads, as compared to the 
59 

64 
A 
I 
100 
80 
E 60 
Q) 
~ 
Q) 
a. 40 
~ 
20 
Figure 10a: Overall Kill Percent 
o 
~ 
~ 
00 
00 
100 
Arrival Rate -> 
Figure 10c: Level Kill Percent(2PL-HP) 
100 
80 
A 
I 
E 60 
Q) 
~ 
Q) a. 40 
::.: 
20 
0 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure 10e: Fairness (2PL-HP) 
2 
1.5 
A 
I 
(/J 
(/J 
Q) 
E 
.(ij 
LL 
O~--~--~----~--~--~ 
o 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure 10. 
Five security levels. 
60 
GEORGE AND HARITSA 
Figure 10b: Level Kill Percent(OPT -WAIT) 
100 
A 
I 
80 
E 60 
~ 
Q) 
a. 40 
::.: 
A 
I 
gj 
Q) 
E 
.(ij 
LL 
20 
o 
2 
1.5 
0.5 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure 10d: Fairness (OPT-WAIT) 
OL---~--~--~--~--~ 
o 
~ 
~ 
00 
00 
100 
Arrival Rate -> 
LEGEND 
Fig.10a 
Fig. 10b - 10e 
x OPT-WAIT + Public 
o 2PL-HP 
lIE Classified 
Direct 
Secure 
o Confidential 
x Secret 
Ell Top Secret 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
6S 
equivalent two-level experiment (Experiment 1). This is because in a five-level system, 
priority is much more level-based than deadline-based, thereby denying Earliest Deadline 
its ability to complete most transactions in a feasible set under normal loads. Under heavy 
loads, however, the smaller sizes of the transaction sets in each level results in Earliest 
Deadline performing well for the low security transactions. (This feature of Earliest Deadline 
was used in the Adaptive Earliest Deadline scheduling algurithm described in L20j where 
transactions are split into prioritized groups with the size of the highest priority group set 
equal to an estimate of the maximum number of transactions that could be successfully 
completed by Earliest Deadline.) 
In figure lOeb) and (c), we plot the KillPercent on a per-security-Ievel basis for OPT-
WAIT and 2PL-HP, respectively. These graphs clearly show the extent to which the kill 
percentages are skewed among the various transaction security levels, with the Top Secret 
class having the most number of killed transactions deadlines and the Public class having 
the least. The graphs also show that OPT-WAlT's performance is better than that of2PL-HP 
for every transaction security level. 
In figure lO(d) and (e), the fairness factors for the top four security levels (Top Secret, 
Secret, Confidential and Classified) are plotted on a per-security-Ievel basis for OPT-WATT 
and 2PL-HP, respectively. These figures clearly show that as the loading factor increases, 
progressively more and more security classes become discriminated against by the lowest 
security class (Public). We also find that OPT-WAlT's performance is more fair than that 
of 2PL-HP for every transaction security level. 
In summary, just as in the two-security-Ievel experiment, we find that OPT-WAIT provides 
the lowest kill percentage, both on an overall basis and on a per-level basis, and the maximum 
fairness (this observation regarding OPT-WAIT is true also with regard to the S2PL protocol 
whose results were not presented here). 
7.7. 
Experiment 7: Dual-CC approach 
In the final experiment of this section, we evaluate the performance of our new dual-CC 
approach to secure real-time CC where inter-level conflicts are handled by one protocol 
while intra-level conflicts are handled by a different protocol. In figure II(a)-(c) we show 
the performance of the three dual protocols (WAIT-HP, HP-WAIT and S2PL-WAIT) for the 
baseline environment. For the sake of comparison, the performance of a pure OPT-WAIT 
protocol is also shown in these graphs. 
Focusing our attention on the WAIT-HP and HP-WAIT dual-CC protocols, we first ob-
serve in figure 11 (a), which compares the overall kill percentages of the protocols, that the 
performance of both these approaches is considerably worse than that of the pure OPT-
WAIT protocol. The reason that OPT-WAIT remains the best among them is that 2PL-HP 
is a wasteful algorithm, as seen in the previous experiments, and therefore "dilutes" the 
effect of OPT-WAIT in both the dual-CC protocols. 
We also observe in figure ll(a) that the performance of WAIT-HP is worse than that 
of HP-WAIT throughout the loading range. The reason for this is that, for the workload 
modeled in this experiment, the number of intra-level conflicts are significantly more than the 
number of inter-level conflicts. Therefore, the algorithm which is used to handle intra-class 
61 

66 
Figure 11 a: Overall Kill Percent 
100 
80 
1\ I 
E 60 
Q) e 
Q) 
a.. 40 
52 
20 
o 
20 
40 
60 
80 
100 
10.8 
en 
en 
~ 0.6 
.Cij 
u.. 0.4 
0.2 
Arrival Rate-> 
Figure 11 c: Fairness 
IDEAL 
0~--~--~--~---5==~ 
o 
20 
40 
60 
80 
100 
Arrival Rate -> 
Figure 11. Dual-CC approach. 
100 
80 
1\ I 
E 60 
Q) e 
Q) 
a.. 40 
52 
20 
o 
GEORGE AND HARITSA 
Figure 11 b: Level Kill Percent 
20 
40 
60 
Arrival Rate -> 
LEGEND 
80 
100 
x OPT-WAIT 
Overall 
+ WAIT -HP 
Public 
0 HP-WAIT 
Secret 
• S2PL-WAIT 
conflicts has more effect on the overall kill percentage than the protocol used to handle 
inter-class conflicts. In WAIT-HP, it is 2PL-HP which handles intra-class conflicts and this 
results in worse performance than that of HP-WAIT, which uses OPT-WAIT to handle this 
category of conflicts. 
The kill percentages of the protocols on a per-security-Ievel basis is provided in 
figure II (b) and the fairness factors are shown in figure 11(c). An interesting feature in 
these graphs is that at high loads, the fairness of WAIT-HP is greater than that of OPT-
WAIT-this is the first time in all the experiments discussed so far that a real-time protocol 
has improved on OPT-WAIT's fairness performance. The reason that this happens is the 
following: In WAIT-HP, due to 2PL-HP being used for intra-class conflicts, many of the 
Public transactions are so busy "fighting" each other that they don't ever reach the end of 
their execution, which is when the OPT-WAIT policy of checking for inter-class conflicts 
comes into play. Therefore, Secret transactions suffer much less restarts from the Public 
transactions. This is clearly seen in figure 11 (b) where the kill percentage of the Public 
62 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
67 
transactions for WAIT-HP is quite high as compared to the corresponding numbers for the 
other protocols. 
Moving on to the S2PL-WAIT dual-CC protocol, we find that, unlike the other two dual 
protocols, it performs better than OPT-WAIT, especially at lower loading levels. For ex-
ample, at an arrival rate of 40 transactions per second, S2PL-WAIT more than halves the 
kill percentage suffered by OPT-WAIT (figure II(a». The reason that this combination 
works well is that Secure 2PL handles inter-class conflicts without resorting to restarts un-
like in WAIT-HP or HP-WAIT. This means that Secret transactions suffer much less in this 
environment (as confirmed in figure 11(b) and (c». At the same time, using OPT-WAIT 
for handling intra-class conflicts helps to derive the inherent good real-time performance 
associated with this protocol. At high loads, S2PL-WATT performs almost identically to 
OPT-WAIT because, in this region, the primary reason for transactions missing their dead-
lines is resource contention, rather than data contention-therefore, the virtual commit 
feature of S2PL rarely provides the intended benefits. 
In summary, this experiment shows that by carefully choosing the right combination 
of protocols in the dual-CC approach, we can design CC algorithms that provide even 
better kill percent and fairness performance than OPT-WAIT. This highlights the power and 
flexibility that is provided by the dual-CC approach. In fact, it may be possible to develop 
dual protocols that perform even better than S2PL-WAIT by appropriately choosing the 
constituent protocols. 
7.8. 
Other experiments 
We conducted several other experiments to explore various regions of the workload space. In 
particular, we evaluated the sensitivity of the results to the database size, number of security 
levels, deadline slack factor, data access patterns, etc. The complete details and results of 
these experiments are available in [14]. Our general observation was that the relative perfor-
mance behaviors of the protocols in these other experiments remained qualitatively similar 
to those seen in the experiments described here. That is, OPT-WAIT performed the best 
among the individual protocols, while S2PL-WAIT provided the best overall performance 
with respect to both the individual protocols and the dual-CC protocols. 
8. 
Class fairness 
In the previous section, we showed that by proper choice of concurrency control mechanism, 
it is possible to improve the real-time performance without violating security. However, even 
with the S2PL-WAIT protocol, which provided the best overall real-time performance, a 
marked lack of fairness was observed with respect to the high security classes, especially 
for finite resource environments. We address the fairness issue in this section. 
Policies for ensuring fairness can be categorized into static and dynamic categories. 
Static mechanisms such as resource reservation can provide fairness while still maintaining 
full security. However, they may considerably degrade the real-time performance due to 
resource wastage arising out of their inability to readjust themselves to varying workload 
conditions. 
63 

68 
GEORGE AND HARITSA 
Dynamic mechanisms, on the other hand, have the ability to adapt to workload vari-
ations. However, as observed in [22], providing fairness in a workload adaptive manner 
without incurring covert channels appears to be fundamentally impossible since the dynamic 
faimess-inducing mechanism can itselfbecome the medium of information compromise. For 
example, consider the following situation: A conspiring Secret process submits a workload 
such that the Secret class performance degrades. The fairness mechanism subsequently tries 
to improve the Secret class performance by allocating more resources for the Secret class. 
A collaborating Public transaction could now feel the reduction in the availability of system 
resources and thereby sense the presence of the Secret process in the system. Therefore, 
this mechanism could be exploited for covert signaling. 
In summary, for full-secure applications, unfairness can only be mitigated to an extent by 
a judicious choice of CC protocol, but not completely eliminated. Therefore, we move on 
to considering in the remainder of this section the more tractable problem of whether it is 
possible to dynamically provide fairness while guaranteeing Orange Security (i.e. covert-
channel bandwidth of less than one bit per second). In particular, we present GUARD, an 
adaptive admission-control policy that attempts to achieve this goal. 
S.l. 
The GUARD admission control policy 
For ease of exposition, we will assume for now that there are only two security levels: 
Secret and Public. Later, in Section 8.2, we extend the GUARD design to an arbitrary 
number of security levels. 
Forthe two-level environment, the GUARD admission control policy is shown in figure 12. 
The basic idea in the policy is that based on the imbalance in the transaction kill percentages 
of the Secret and Public classes, the admission of Public transactions into the system is pe-
riodically controlled every T seconds (the setting of T is discussed below). The FairFactor 
variable, which is the ratio of the kill percentages for the Public and Secret classes, captures 
Initial condition: Admit[Public] = 1.0, KillPercent[Public] 
0.0, 
KillPercent[Secret] 
0.0 
LOOP: 
if KillPercent[Secret] > 5.0 then 
else 
FairFactor = KillPercent[Public] / KillPercent[Secret] 
if (FairFactor < 0.95) then 
Admit[Public] = Admit[Public] * 0.95 
else if (FairFactor > 1.05) then 
Admit[Public] = Admit[Public] * 1.05 
Admit[Public] = 1.0 
sleep T seconds 
measure KillPercent[Public] and KillPercent[Secret] 
goto statement LOOP 
Figure 12. 
The GUARD admission control policy (two level). 
64 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
69 
the degree of unfairness during the last observation period. Ideally the FairFactor should 
be l.0 and so, if there is a significant imbalance (FairFactor < 0.95 or FairFactor > 1.05), 
what is done is to decrease or increase the admit probability of the Public transactions 
accordingly. 8 The increase or decrease margin has been set to a nominal 5 percent. The 
hope is that this mechanism will eventually result in the multiprogramming level of the 
Public transactions reaching a value that ensures that the Secret transactions are not unduly 
harmed. Finally, to ensure that the admission control becomes operative only when the Se-
cret transaction class is experiencing sustained missed deadlines, a threshold kill percentage 
of 5 percent is included. 
One danger associated with admission control policies is that, although providing fairness, 
they may cause unnecessary increases in the kill percentages of both the Public and Secret 
transactions as compared to the corresponding values without admission control. Such 
"pseudo-fairness" had also been earlier observed with the S2PL CC protocol in Experiment 1 
of Section 7. However, as will be borne out in the experimental results described later in this 
section, the GUARD policy does not suffer from this shortcoming-it evenly redistributes 
the "pain" without really increasing its overall magnitude. 
8.1.1. Guaranteeing orange security. In the GUARD policy, covert channels can arise due 
to the admission control mechanism. For example, a corrupt Secret user can, by modulating 
the outcomes (increase, decrease, or constant) of the Admit[Public] computation, signal 
information to collaborating Public users. Corresponding to each computation, log23 = 1.6 
bits of information can be transmitted (based on information-theoretic considerations [32]) 
and since the computation is made once every T seconds, the total channel bandwidth is 
l.6/ T bits per second. By setting this equal to I bit per second, the condition for being 
orange-secure, we get Tmin = l.6 seconds to be the minimum re-computation period. 
8.2. 
Extension to N security levels 
The GUARD policy described above assumed a two security-level system. It can be easily 
extended to the generic N security level scenario, as shown in figure 13. Note that the 
channel capacity in the general case is log23N-1 = l.6(N -
1) bits and therefore Tmin = 
(N -
1) * 1.6 seconds. 
8.3. 
GUARD implementation 
The GUARD implementation comprises of a sensory mechanism and a regulatory mech-
anism. The sensory mechanism operates once every S seconds and calculates the kill per-
centage of each security level over the last observation period. These observations are 
continuously made available to the regulatory mechanism which operates once every T 
seconds. The regulatory mechanism computes a geometrically weighted combination of 
the sensed values received in the last T seconds9 and uses this value to determine the 
desired multi-programming level for each of the security levels. The sensory mechanism 
operates once every S seconds while the regulatory mechanism is scheduled once every T 
seconds. The T value is set such that orange security is guaranteed as per the discussion 
65 

70 
GEORGE AND HARITSA 
Initial condition: Admit[l:N] 
LOOP: 
for i = 1 to N-l 
do 
1.0, KillPercent[l:N] 
if overallKillPercent > 5.0 then 
do 
0.0 
FairFactor = KillPercent[i] / overallKillPercent 
if (FairFactor < 0.95) then 
else 
done 
Admit[i] = Admit[i] * 0.95 
else if (FairFactor > 1.05) then 
Admit[i] = Admit[i] * 1.05 
done 
Admi t [i) 
1. 0 
sleep T seconds 
measure KillPercent[l:N] 
goto statement LOOP 
Figure 13. 
The general GUARD admission control policy. 
in Section 8.1.1, while S is set such that a reasonable number of samples are taken within 
each regulatory period-for example, the heuristic followed in our experiments is to set 
S = T/16. 
8.4. 
Experiments 
We conducted experiments to evaluate the performance of the GUARD admission policy 
and present a representative set of results here. In the following experiments, we compare 
two systems: 
FULL: A full secure SRTDBS equipped with the S2PL-WAIT CC manager 
ORANGE: An orange secure SRTDBS that is equipped with the S2PL-WAIT CC manager 
and the GUARD admission controller. 
Note that the FULL system was already evaluated in Section 7-we reproduce its results 
here for the purpose of comparison. Further, we show only the results for the S2PL-WAIT 
CC manager here since, as per the evaluation presented in Section 7, it performed the best 
among all the CC protocols that we considered. The results for the other CC protocols, 
which were qualitatively similar, are available in [14]. 
8.4.1. Experiment 8: Two security levels. 
In our first experiment, we evaluate the perfor-
mance of FULL and ORANGE for a two security-level enviroment. For this experiment, 
the GUARD policy parameters, Sand T, are set to O.ls and 1.6s, respectively. 
66 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
71 
The results of this experiment are shown in figure 14(a)-(c), which capture the overall 
kill percent, the level-wise kill percent, and the class fairness, respectively. In these figures 
we see that the ORANGE system achieves close to ideal fairness. Further, at low loads, 
ORANGE has only a small increase of the overall kill percentage with respect to FULL, 
whereas at heavy loads, it actually does slightly better. This is explained as follows: At 
low loads, due to the inherent lag involved in the feedback process, the GUARD policy 
sometimes tends to be over-conservative, preventing Public transactions from entering even 
when strictly not necessary, and thereby increasing the Public kill percentage. In contrast, 
under heavy loads, being conservative does less harm than being too liberal, and therefore 
its performance shows minor improvement over that of FULL. 
In summary, the results of this experiment show that the ORANGE system achieves its 
fairness very efficiently and that the GUARD admission control component functions as 
intended in its design. 
100 
80 
c 
Q) 60 
~ 
Q) 
a.. 
~ 40 
20 
0.8 
II) 
8l 
E 0.6 
. (ij 
u. 
0.4 
0.2 
Figure 14a: Overall Kill Percent 
20 
40 
60 
80 
Arrival Rate 
Figure 14c: Fairness 
20 
\ 
40 
\ 
~ 
\ 
\ ... 
60 
Arrival Rate 
IDEAL 
-€)- -
-0 
, , .. 
80 
100 
Figure 14. Two security levels. 
100 
80 
C 
Q) 
60 
~ 
Q) 
a.. 
~ 40 
20 
Figure 14b: Level Kill Percent 
20 
40 
60 
/ 
/ 
Arrival Rate 
LEGEND 
80 
Overall 
• FULL 
Public 
0 ORANGE 
Secret 
100 
67 

72 
Figure 15a: Overall Kill Percent 
100 
80 
E 
0> 60 
~ 
0> 
Q. 
~ 40 
20 
20 
40 
60 
80 
100 
Arrival Rate 
Figure 15c: Fairness (ORANGE) 
2 
1.5 
(J) 
(J) 
IDEAL 
0> E 
'ijj 
u. 
0.5 
0 
20 
40 
60 
80 
100 
Arrival Rate 
Figure 15. 
Five security levels. 
(J) 
(J) 
0> 
E 
~ 
2 
1.5 
GEORGE AND HARITSA 
Figure 15b: Fairness (FULL) 
20 
40 
60 
80 
100 
Arrival Rate 
LEGEND 
Fig. 15a 
Fig. 15b, 15c 
• FULL 
+ Public 
o ORANGE 
lIE Classified 
o Confidential 
x Secret 
EI) Top Secret 
8.4.2. Experiment 9: Five security levels. 
In this experiment. we compare the FULL and 
ORANGE systems for a five-security-Ievel environment. The results of this experiment are 
shown in figure 15(a)-(c). We see in these figures that the ORANGE system provides, at only 
a marginal performance cost, reasonably close to ideal fairness characteristics throughout 
the loading range in this many-security-level environment as well. 
9. 
Related work 
The design of secure CC protocols in the context of conventional DBMS has been inves-
tigated by several research groups (see [6, 38] for surveys). In comparison, little attention 
has been given to developing secure CC protocols for real-time database systems. The only 
work that we are aware of in this area is a series of papers by Son et al. [10, 29, 34-36]. 
In particular, a concurrency control protocol called Adaptive 2PL (A2PL) that attempts to 
balance the dual requirements of security and timeliness was presented in [10, 34, 35]. 
In their scheme, transactions dynamically choose between an un secure version of 2PL-HP 
68 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
73 
and the secure S2PL described in Section 4. The goal of the A2PL protocol is to tradeoff 
security for real-time performance with the tradeoff depending on the state of the system 
and the application's requirements. lo In contrast, in our work, we have assumed that full 
(or orange) security is afundamental requirement and that it is not permissible to improve 
the real-time performance at the cost of security. 
There are also a few design and implementation difficulties associated with the A2PL 
protocol: 
1. In A2PL, the decision to use 2PL-HP or S2PL is a function of the system state and the 
application requirements. The priority assignments in these protocols are independent 
and may therefore work at cross purposes. For example, consider the case where a 
tight-deadline Secret transaction requests a data item held by a slack-deadline Public 
transaction. Assume that the A2PL protocol chooses the real-time option to resolve this 
conflict and therefore grants the lock to the requester aborting the Public holder. Assume 
that another slack-deadline Public transaction now makes a conflicting request on the 
same data item. If the A2PL protocol decides to follow the secure option to resolve 
this conflict, the Secret holder will have to be restarted. This means that a transaction 
that was earlier assisted based on deadline considerations now gets hindered on security 
considerations, negating the priority assignment objective. 
2. Since the A2PL protocol takes only data conflicts into account, it cannot control the 
conflicts at other resources. Therefore, here again, a transaction that was assisted to 
make its deadline at the data may be hindered at the physical resources. In contrast, our 
GUARD policy applies a system-wide control that does not depend on the location of 
the performance bottleneck. 
3. For applications where full security is a fundamental requirement, the A2PL protocol 
reduces to plain S2PL, which does not have any real-time features. 
In [29], a concurrency control protocol that ensures both security and timeliness is pro-
posed. For this scheme, however, the RTDBS is required to maintain two copies of each 
data item. Further, transactions are required to obtain all their data locks before starting 
execution (i.e., conservative locking). These requirements limit the applicability of the pro-
tocol. In our work, we have considered more general database environments where all data 
items are single-copy and transactions acquire data locks dynamically. 
Another feature of their work is that it is primarily addressed towards "soft-deadline" 
applications, that is, real-time applications in which there is value to completing tasks even 
after their deadlines have expired. In contrast, we have concentrated on firm-deadline ap-
plications. The type of deadline has a significant impact on both the performance evaluation 
model and on the interpretation of the results, as observed earlier for (un secure) real-time 
transaction concurrency control [1, 19]. 
10. 
Conclusions 
In this paper, we have quantitatively investigated the performance implications of main-
taining covert-channel-free security in a firm-deadline real-time database system. Unlike 
previous studies, which used a tradeoff approach between security and timeliness, we 
69 

74 
GEORGE AND HARITSA 
have considered full (or orange) security to be a correctness requirement. In comparison, 
the number of killed transactions is a peiformance issue. Therefore, our study investigates 
the problem of how to minimize the number of mi ssed transaction deadlines without com-
promising the desired level of security. To the best of our knowledge, this is the first detailed 
study of real-time database security in the firm-deadline context. 
We first identified that, in order to salisfy the requirement of non-interference, only those 
real-time concurrency control protocols that are free from priority inversion can be used 
in a secure RTDBS. This requirement ruled out several previously proposed real-time CC 
protocols, including algorithms such as 2PL Wait Promote [1] and WAIT-50 [19]. 
Then, using a detailed simulation model of a firm-deadline RTDBS, we studied the 
relative performance of the secure versions of the 2PL-HP and OPT-WAIT real-time con-
currency control algorithms; a non-real-time secure algorithm, S2PL, was also included in 
the evaluation suite. The performance of these algorithms was also evaluated for a baseline 
system where only direct unauthorized access, but not covert channels, is prevented. 
Our experiments showed that, under normal loads, the overall kill percent of the secure 
system is worse than that of the direct system, whereas under heavy loads, it is the other 
way around. Within the secure system, the performance of high-security transactions was 
significantly worse than that of the low-security transactions. Among the secure concur-
rency control protocols, OPT-WAIT performed best in minimizing the kill percentages on 
both an overall basis and on a per-level basis. Moreover, it exhibited the maximum degree 
of fairness. These results show that OPT-WAIT, which provided excellent performance in 
traditional (unsecure) real-time concurrency control [19], continues to perform well even 
in the secure real-time domain. 
We also proposed a novel dual-CC approach to secure concurrency control wherein dif-
ferent concurrency control algorithms are used to resolve inter-level conflicts and intra-level 
conflicts. A global serialization order was ensured, in spite of having multiple CC algo-
rithms operating simultaneously, by using the MCC system architecture described in [37]. 
The dual-CC combinations of HP-WAIT and WAIT-HP were implemented and evaluated-
they both generally perform worse than pure OPT-WAIT. However, the dual combination of 
S2PL-WAIT performs better than OPT-WAIT, especially at lower kill percent levels. This 
is because S2PL is a non-restart-oriented algorithm unlike both OPT-WAIT and 2PL-HP, 
and therefore ensures reduction of the harm done to high-security transactions. 
Another advantage of the dual-CC approach, not exploited here, is that the separation 
of sccurity and timeliness concerns makes it possible to use even unsecure real-time CC 
algorithms (e.g., Wait-Promote, WAIT-50) for resolving intra-level conflicts. The dual-CC 
approach therefore empowers the use, even in the secure RTDBS domain, of the rich set of 
rcal-timc CC algorithms developed during the last decade. 
A fundamental problem associated with the elimination of covert channels is that thc 
system becomes biased against the high security transactions. To address this problem, we 
introduced the GUARD admission control policy which implemented a simple feedback-
based transaction admission control mechanism. The bandwidth of the covert channel in-
troduced by this mechanism was bounded by appropriately setting the feedback period. 
Experimental evaluation of an orange-secure version of GUARD indicated that it provides 
close to ideal fairness at little cost to the overall real-time performance. 
70 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
75 
In closing, we suggest that SRTDBS designers may find the S2PL-WAIT CC protocol 
and the GUARD admission control policy to be attractive choices for achieving high-
performance secure real-time transaction processing. 
10.1. 
Future work 
Our goal in this paper was to take a first step in providing effective transaction processing 
solutions for real-time database applications with security requirements. We hope that it 
will lead to further activity in this area, especially with regard to extending our solutions, 
which were derived for a specific security and real-time framework, to more general en-
vironments. Important issues in this regard include supporting partially-ordered security 
frameworks such as lattices, evaluating performance profiles of snapshot-based concurrency 
control algorithms, assessing the tradeoffs involved in replicated architectures, incorporat-
ing security requirements such as data integrity and availability, and handling real-time 
applications with soft deadlines. In addition, solutions for other transaction processing 
components including buffer management and transaction recovery need to be designed. 
Appendix A: Master concurrency control 
We present here the Global Ordering Scheme [37] used to ensure a single global serializa-
tion order in CC protocols based on the dual-CC approach described in Section 5. In this 
scheme, the Local CC managers (LCCs) are required to keep track of the local serialization 
order of transactions and to communicate this to the master concurrency controller (MCC) 
whenever requested. The LCCs communicate the local serialization order using the notion 
of serialization numbers which is defined shortly. The MCC then ensures that a single global 
serial order is maintained for the entire DBMS. 
Definition 1. 
A serialization number sequence for a set of transactions, with respect to a 
schedule S, is a sequence of numbers such that for any pair of transactions Ti and Tj , if m 
and n indicate the serialization numbers of Ti and Tj respectively, then the following holds: 
m < n =} :3 a serial schedule conflict equivalent to S, in which Ti finishes before Tj • 
The local serialization order of transactions at a LCC is communicated to the MCC by 
augmenting the interface presented by the LCCs to the MCC in the following fashion: Each 
LCC provides a function called Prepare_to_Commit. This function, when invoked with a 
transaction identifier, responds with an indication of whether or not the transaction can be 
committed. If it can be committed, the serialization number of the transaction with respect 
to the global schedule restricted to transactions of that class alone is also returned by the 
LCC of that class. The serialization numbers returned by a LCC are distinct and are never 
reused by a LCC, irrespective of the outcome of a transaction. 
The MCC invokes the function Prepare_to_Commit when it gets the CommiLTrans call 
from the Transaction Manager. The returned serialization number is then checked for validity 
(explained shortly) and the transaction is committed or aborted based on the outcome of 
this test. We will need a few definitions before outlining the validity test. 
71 

76 
GEORGE AND HARITSA 
Definition 2. 
Let S be a schedule of a set oftransactions T produced by a DBMS employing 
precisely one CC policy, say M. A serialization Junction, ser, for M is a function that maps 
every transaction in T to one of its operations such that the following holds: S is conflict 
equivalent to S', where S' is the serial schedule in which ~ occurs before Tj if and only if 
ser(Tj ) occurs before ser(Tj ) in S. 
All CC algorithms either inherently have serialization functions, or they can be introduced 
externally [13]. In particular, for locking protocols and optimistic algorithms, the function 
that maps every transaction to its commit operation is a serialization function, whereas for 
timestamp algorithms, the function is associated with the begin operations of transactions. 
Definition 3. 
The serialization time stamp (STS) of ~ is the time at which ser(~) for the 
CC policy at MCC is received by the MCC. 
The MCC uses the STS to define the serialization order at the MCC while the LCCs com-
municate their local order through serialization numbers returned by the Prepare_to_Commit 
call. The MCC now ensures that the serialization orders at the MCC and the LCCs agree by 
maintaining the following data structures: The MCC maintains a serialization time stamp 
(STS) for each transaction. In addition to STS, the MCC also maintains for each transac-
tion a low water mark (LWM) and a high water mark (HWM). The LWM and HWM of 
Tj determines the range in which its serialization number, as returned by the LCC, can lie. 
When a transaction is initiated, its LWM and HWM are initialized to 0 and 00 respectively. 
They are updated as follows whenever a transaction ~ commits: 
• For all Tj such that Tj belongs to the same class as Tj and Tj is after Tj in MCC's serial 
order (i.e., STS(Tj ) < STS(Tj » and Tj conflicts with Tj , set LWM(Tj ) = max(LWM(T;), 
serialization number of Tj) . 
• For all Tj such that Tj belongs to the same class as ~ and T; is prior (in MCC's serial 
order) to ~, Ci.e., STS(Tj ) > STSCTj » set HWMCTj ) = min(HWM(Tj ) , serialization 
number of T;). 
We now describe the test used for checking the validity of the serialization number returned 
in response to a Prepare_to_Commit call. Based on the result of the test, the transaction is 
committed or aborted. 
Commit Test: The MCC allows a transaction to be committed only if its serialization 
number, as returned by the LCe lies between its LWM and HWM. 
We will now prove that the commit test ensures that a single global order is maintained. 
Theorem 1. If there exists an edge ~ --7 T; in the precedence graph corresponding to 
a schedule produced by any combination of CC algorithms at MCC and the slave LCCs, 
then STS(Tj) < STS(Tj ). 
72 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
Proof: 
We will assume STS(Ti) > STS(Tj ) and derive a contradiction. 
Case 1: ~ and Tj belong to the same class. 
Case 1.1: Ti commits before Tj 
77 
Since STS(Ti) > STS(Tj ), Tj is prior (in MCC's serial order) Ti and hence HWM(Tj ) 
is set to some value which is smaller than or equal to the serialization number of ~ when 
Ti commits. Since Ti ---+ Tj , the serialization number of Tj returned by the LCC will be 
greater than that of Ti. So it exceeds HWM(Tj ) and hence the MCC will not allow it to 
commit. 
Case 1.2: Tj commits before ~ 
We know that LWM(Ti) will be set to some value which is greater than or equal to the 
serialization number of Tj when Tj commits because by our assumption ~ is after (in 
MCC's serial order) Tj and ~ conflicts with Tj . But Ti ---+ Ti and hence the serialization 
number of ~ returned by the LCC is smaller than that of Tj . It is less than LWM(~) and 
MCC will not allow it to commit. 
Case 2: ~ and Tj belong to different classes. 
By the definition of STS, if ~ ---+ Tj , then STS(Ti) < STS(Tj ) because inter-class conflicts 
are regulated by the CC algorithm at the MCC, which will produce serializable schedules 
~~ 
0 
Appendix B: 
Security loopholes in the S2PL protocol 
We describe here the Secure 2PL (S2PL) algorithm proposed in r9, 331, and the security 
loopholes associated with this algorithm. 
B.l. 
The S2PL protocol description 
We have reproduced below their protocol exactly as it was originally proposed except that, 
for clarity, we have modified the description at some places. The following notation is used 
in the description: queue[x] is a queue of operations, each of which could be holding a 
virtual lock VpLock, a dependent virtual lock DVpLock, a real lock pLock, or waiting to set 
a lock p Wait. An operation p could be a read r or a write wand an action is represented 
in the form Pi[X]. The queue is not a strict FIFO queue, since elements can be inserted in 
the middle, but it can be considered a priority queue ordered by the hefore relation between 
transactions. S L(Ti ) is the security access class of the transaction ~. 
When a request for lock P is submitted to the S2PL CC protocol, action is taken as per 
the following algorithm: 
Casel (p = read) /\ 3(wLock(~, x) V VwLock(h x) V DVwLock(Ti. x)) 
Read value of x written by Wi [x]; 
73 

78 
GEORGE AND HARITSA 
Case2 «p = write) A 3rLock(T;, x» 
Upgrade rLock(Ti. x) to wLock(Tio x); 
Case3 3j«qLock(Tj, x) v VqLock(Tj , x) v DVqLock(Tj, x) 
v Wait(Tj , x» A(j =j:. i) A «p = write) v (q = write))) 
pos := length(queue[x]); 
while (pos > 0) 
Tj +-- transaction at queue[x ].pos; 
if (operation (queue[x ].pos) conflicts with p) 
if(T; E before (Tj )) 
else 
VT3«T3 E before(Tj ) ) A (T3 E after(Tj» 
A (SL(T3) > SL(T;) A (SL(T3) > SL(T)) 
abort T3; /* value security violation */ 
if(lock(queue[x].pos) is a real lock) /* deadlock */ 
call victim selection routine; 
if(victim = Tj) exit; /* exit from routine */ 
else continue; /* to start of while loop */ 
else /* if(Tj fj before(Tj )) */ 
if(SL(T;) = SL(Tj» 
if(Tj has not committed) 
Insert(queue, pWait(Tj, x), pos); 
else 
if(p = read) readwj[x]; 
Insert(queue[x], pLock(T;, x), pos); 
elseif«p = write) A (3z, VwLock (Tj, Z») 
Insert(queue, DVwLock(Tj, x), pos); 
/* Insert after pos */ 
dependent(wj[Z]):= Wj[X]; 
else if(SL(T;) < SL(Tj» 
Insert(queue, VwLock(T;, x),pos); 
else 
Insert(queue,pWait(Tj , x),pos); 
before(T;) := before(T;) U Tj U before(T;); 
after(Tj ) := after(Tj ) U Tj U after(T;); 
exit; /* exit from routine */ 
pos : = pos - 1 ; 
endwhile /* end of while loop */ 
if(pos = 0) Insert(queue[x], pLock(T;, x), pos); 
In [9], a proof is given to show that the above algorithm satisfies all the conditions for a secure 
interference-free scheduler [22], namely, Value Security, Delay Security and Recovery 
Security. These conditions are described below: 
74 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
79 
Value Security: A system is Value Secure if the value of objects accessed by a subject are 
not changed by actions of subjects belonging to higher clearance levels. 
Delay Security: A system is Delay Secure if the delay experienced by a subject is not 
affected by actions of subjects having higher clearance levels. 
Recovery Security: A system is Recovery Secure if the occurrence of a transaction restart 
appears the same for a subject regardless of the presence or absence of higher level 
subjects in the system. 
In the remainder of this section, we analyze the S2PL protocol and point out the circum-
stances under which itfails to ensure complete security. For ease of understanding, we use 
transaction execution schedules. A schedule is a temporal log of the operations performed 
by the system for the transactions concurrently executing in the system. A transaction with 
identifier i and security clearance level C L is represented in the schedule by Ti (C L). Corre-
sponding to every transaction appearing in the schedule, there is a list of actions of the form 
Pi [x] where p is an action the transaction ~ performs on the data item x. An action p can 
appear in the schedule in two forms, r or w, which stands for read and write, respectively. 
For example, the read performed on a data item x by transaction ~ is represented as ri [x]. 
Similarly, a write performed on the data item x by transaction ~ is represented as Wi [x). 
The actions in the schedule occur in left to right order. If some actions appear in the same 
vertical column, it means they can occur simultaneously (i.e., there is no ordering defined 
among those operations). 
B.2. 
Recovery security loophole 
An operational sequence by which value security in S2PL could be violated and the method 
to prevent such violations is explained in [9], and is reproduced below: 
Transactions 
Activity 
TI (Secret) 
rl[Y] 
rl [x) 
cl 
T2 (Confidential) 
r2[Z) 
c2 
T3 (Classified) 
W3[X) 
W3[Z) 
c3 
T4(Public) 
1V4[Y) 
W4[Z) 
c4 
In the above schedule, value security is violated as follows: The Secret transaction Tj 
secures a read lock on data item y. Subsequently, the Public transaction T4 submits a write 
request on y. Since the Tj already holds a lock on y, Tj is set before T4 in the serialization 
order. Similarly T3 is set before Tj in the serialization order because T3 is already holding 
a write lock on data item x when Tj requests x. When T3 requests a write lock on data 
item z T4 already has a dependent virtual lock on it. Since Tj is before T4 and T3 is before 
Tj, the transaction T3 is allowed to overtake T4 in queue[z). The value of z read by the 
Confidential transaction T2 will be the value written by T4. Had Tj not been present in the 
system, T2 would have read the value written by T3• Since it is possible for a higher security 
75 

80 
GEORGE AND HARITSA 
level transaction to alter the value read by a lower security level transaction, if the above 
schedule is allowed to execute, value security may be violated. 
The way in which S2PL deals with the above kind of value security violations is to abort 
all transactions before T4 and after T3 whose security level is higher than T3 and T4. So, in 
the above example TJ is aborted and the value security is maintained. However, this scheme 
of aborts can itself lead to recovery security violations, as described below: 
Transactions 
Activity 
7u(TopSecret) 
rota] 
role] 
cO 
TI (Secret) 
rl[b] 
wI[a] 
r2[d] 
cl 
T2 (Confidential) 
r2ld] 
c2 
T3 (Classified) 
W3[C] 
w3[d] 
c3 
T4(Public) 
w4[b] 
w4[dj 
c4 
Consider the above schedule. Here when T4 requests for a write lock on b, TJ already 
holds a read lock on the same item. Therefore, TJ is set to before T4. When TJ requests a 
write lock on data item a, To is already holding a read lock on it, so To is set to before TJ 
in the serialization order. When To requests for a lock on data item c, T3 is already holding 
a lock on it and therefore, T3 is set before To in the serialization order. Later T4 performs 
a dependent virtual write on d and commits. Now when T3 requests a write lock on d, T3 
is allowed to supersede T4 since the former comes earlier in the serialization order. This 
would not have happened if either To were absent or TJ were absent. According to the S2PL 
protocol, To and TJ are both aborted to ensure value security. But then this opens another 
covert channel as follows: 
Imagine the case where To wants to send some information to T1, it could collaborate with 
TJ, Tz, T3, and T4 and create the above schedule. Faced with this schedule, S2PL promptly 
detects value security violation and aborts both TJ and To. Since TJ is also aborted along 
with To, it is a security violation since had To been absent, TJ would not have aborted. 
B.3. 
Delay security loophole 
An operational sequence by which delay security in S2PL could be violated is presented 
below: 
Transactions 
TI (Secret) 
T2 (Confidential) 
T3 (Classified) 
Activity 
In the above schedule, TJ gets a read lock on data item a and later Tz requests for a write 
lock on a. Since the Secret transaction TI already holds a read lock on data item a, a virtual 
76 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
81 
lock (non-blocking) is granted for T2. Now the Classified transaction T3 obtains a write lock 
on data item b. Later, when Tz requests a lock on data item b, since it already has secured 
a virtual lock (for data item a), it is granted a dependent virtual lock (again, non-blocking). 
Suppose T, was not present in the system, wz[b 1 of transaction Tz would have blocked 
until T3 releases its lock on data item b. Because of the presence of T" Tz performs Wz [b 1 
quicker than otherwise. This is a delay security violation since Tz completes earlier due to 
the presence of a higher security transaction. 
B.4. 
Non-serial schedules 
Apart from the above security-related problems, S2PL may also result in non-serial sched-
ules, as explained below: 
Transactions 
Activity 
T, (Secret) 
rj [e] 
T2 (Confidential) 
r2lhl 
W2[C] 
w2[d] 
T3 (Classified) 
r3[a] 
1I)3[h] 
T4(Public) 
11)4 [a] 
1I)4[dj 
In the above schedule, when T4 requests a write lock on the data item a, T3 has already 
obtained a read lock on a, so T3 is set before T4 in the serialization order and T3 is added to 
the beforelist of T4. Also, T4 is added to the afterlist of T3. For similar reasons, T3 is added 
to the afterlist of T2 and T2 is added to the beforelist of T3. Then T2 obtains a dependent 
virtual lock on data item d after which T4 requests for a write lock on d. Now, when T4 
makes the request for d, T2 is not in the beforelist of T4 and therefore it is set after T2. 
This is incorrect because Tz is before T4 in the serialization order (in fact, the afterlist of T2 
contains T4). Therefore, there could be a serializability violation in the S2PL protocol. 
B.5. 
Fixesfor the S2PL loopholes 
To address the above problems, we have implemented the following fixes in our implemen-
tation of the S2PL protocol: 
1. When the conditions for a potential value security violation are satisfied because of the 
actions of a set of higher security level transactions, the highest security level transaction 
in the set is aborted. This step is repeated until the conditions for value security violation 
are completely removed (or the set becomes empty). 
2. A dependent virtual write operation on a data item is made to wait if a conflicting (real or 
virtual) lock is held on the data item by another transaction of the same or lower security 
level. 
3. Instead of every transaction having its own beforelist and afterlist, a common serialization 
graph is maintained for determining the serialization order. 
77 

82 
GEORGE AND HARITSA 
Acknowledgments 
This work was supported in part by a research grant from the Dept. of Science and 
Technology, Govt. of India. 
Notes 
1. Or equivalently, the percentage of missed transaction deadlines. 
2. A new reader joins a group of lock-holding readers only if its priority is higher than that of all the waiting 
writers. 
3. A similar, but unrelated, dual-protocol strategy has been used earlier for the purpose of task scheduling in the 
Secure Alpha project [16], 
4. The non-interference method is used for eliminating covert channels at the physical resources also. 
5. A long-term operating region where the kill percentage is high is obviously unrealistic for a viable RTDBS. 
Exercising the system to high kill levels (as in our experiments), however, provides valuable information on 
the response of the algorithms to brief periods of stress loading. 
6. The physical explanation for why the KillPercent graphs show a non-linear "S-shaped" behavior have been 
identified early on in the RTDBS literature, including queueing-theory-based analyses in [17,18]. 
7. While abundant resources are usually not to be expected in conventional DBMS. they may be more common 
in RTDBS environments since many real-time systems are sized to handle transient heavy loading. This 
directly relates to the application domain of RTDBSs, where functionality, rather than cost, is often the 
driving consideration. 
8. No restrictions are placed on the admission of Secret transactions. 
9. This is similar to the system load computation method used in the Unix operating system [26]. 
10. The tradeoff approach, and alternative schemes to implement the tradeoff, have also been considered in the 
Secure Alpha project [16], which investigated the interactions between security and timeliness in the context 
of a distributed real-time operating system. 
References 
I. R. Abbott and H. Garcia-Molina, "Scheduling real-time transactions: A performance evaluation," ACM Trans. 
on Database Systems, vol. 17, no. 3, pp. 513-560, September 1992. 
2. M. Abrams, S. Jajodia, and H. Podell, Information Security, IEEE Computer Society Press, 1995. 
3. R. Agrawal, M. Carey, and M. Livny, "Concurrency control performance modeling: Alternatives and impli-
cations," ACM Trans. ou Database Systems, vol. 12, no. 4, December 1987. 
4. D. Agrawal, A. EI Abbadi, and R. Jeffers, "Using delayed commitment in locking protocols for real-time 
databases," in Proc. of ACM SIGMOD Conf., June 1992. 
5. P. Amman, F. Jaeckle, and S. Jajodia, "A two-snapshot algorithm for concurrency control in secure multilevel 
databases," in Proc. of IEEE Symp. on Security and Privacy, 1992. 
6. V. Atluri, S. Jajodia, T. Keefe, C. McCollum, and R. Mukkamala, "Multilevel secure transaction processing: 
Status and prospects," in Database Security, X: Status and Prospects, P. Samarati and R. Sandhu (Eds.), 
Chapman & Hall, 1997. 
7. S. Castano, M. Fugini, G. Martella, and P. Samarati, Database Security, Addison-Wesley, 1995. 
8. A. Dalla, S. Mukherjee, P. Konana, L Viguier, and A. Bajaj, "Multiclass transaction scheduling and overload 
management in firm real-time database systems," Information Systems, vol. 21, no. I, March 1996. 
9. R. David, "Secure concurrency control," Master's thesis, Univ. of Virginia, May 1993. 
10. R. David, S. Son, and R. Mukkamala, "Supporting security requirements in multilevel real-time databases," 
in Proc. of IEEE Symp. on Security and Privacy, May 1995. 
II. "DOD Trusted Computer System Evaluation Criteria," Department of Defense Standard, DoD 5200.28-STD, 
December 1985. 
12. K. Eswaran, J. Gray, R. Lorie, and L Traiger, "The notions of consistency and predicate locks in a database 
system," Comm. of ACM, vol. 19, no. 11, pp. 624--633, November 1976. 
78 

SECURE CONCURRENCY CONTROL IN FIRM RTDBS 
83 
13. D. Georgakopoulous, M. Rusinkiewicz, and A. Sheth, "On seri alizability of multi database transactions through 
forced local conflicts," in Proc. of 7th IEEE IntI. Conf. on Data Engineering, 1991. 
14. B. George, "Secure real-time transaction processiug," Ph.D. thesis, Indian Institute of Science, December 
1998. 
IS. J. Goguen and J. Meseguer, "Security policy and security models," in Proc. of IEEE Symp. on Security and 
Privacy, 1982. 
16. I. Greenberg, P. Boucher, R. Clark, E. Jensen, T. Lunt, P. Neuman, and D. Wells, "The secure alpha study 
(final summary report)," Tech. Report ELIN AOI2, SRI International, June 1993. 
17. 1. Haritsa, "Performance analysis of real-time database systems," in Proc. of 10th IEEE IntI. Conf. on Data 
Engineering, February 1994. 
18. 1. Haritsa, M. Carey, and M. Livny, "On being optimistic about real-time constraints," in Proc. of 9th ACM 
Symp. on Principles of Database Systems, April 1990. 
19. J. Haritsa, M. Carey, and M. Livny, "Data access scheduling in finn real-time database systems," IntI. Journal 
of Real-Time Systems, vol. 4, no. 3,1992. 
20. J. Haritsa, M. Livny, and M. Carey, "Earliest deadline scheduling for real-time database systems," Proc. of 
12th IEEE Real-Time Systems Symp., December 1991. 
21. M. Kang and 1. Moskowitz, "A pump for rapid, reliable, secure communication," in Proc. 1st ACM Conf. on 
Computer and Communications Security, November 1994. 
22. T. Keefe, W. Tsai, and J. Srivastava, "Multilevel secure database concurrency control," in Proc. of 6th IEEE 
IntI. ConI'. on Data Engineering, February 1990. 
23. W. Lampson, "A note on the confinement problem," Comm. of ACM, vol. 16, no. 10, pp. 613-615, Octoher 
1973. 
24. L. LaPadula and D. Bell, "Secure computer systems: Unified exposition and multics interpretation," The 
Mitre Corp" March 1976, 
25, A,M. Law and C.S, Lamey, Introduction to simulation using SIMSCRIPT 11.5, CACI Products Company, La 
Jolla, Calif., 1984. 
26. S.J. Leffler, M.K. McKusick, M,T. Karels, and 1.S. Quarterman, The Design and Implementation of 4.3 BSD 
UNIX Operating System, Addison-Wesley, 1989. 
27. Y. Lin and S. Son, "Concurrency control in real-time database systems by dynamic adjustment of serialization 
order," in Proc. of II th IEEE Real-Time Systems Symp., December 1990. 
28. C.L. Liu and J.W. Layland, "Scheduling algorithms for multiprogramming in a hard real-time environment," 
Journal of the ACM, vol. 20, no. I, 1973. 
29. R. Mukkamala and S. Son, "A secure concurrency control protocol for real-time databases," inProc. of Annual 
IF!P WG 11.3 Conference of Database Security, August 1995. 
30. I. Robinson, "Design of concurrency control protocols for transaction processing systems," Ph.D. thesis, 
Computer Sciences Dept., Carnegie Mellon University, 1982. 
31. L. Sha, R, Rajkumar, and J. Lehoczky, "Priority inheritance protocols: An approach to real-time synchro-
nization," Tech. Rep. CMU-CS-87-181, Depts. of CS, ECE and Statistics, Carnegie Mellon University, 
1987. 
32. C. Shannon, "A mathematical theory of communications," Bell Syst. Tech, J., vol. 27, no, 4, pp. 623-656, 
October 1948. 
33, S, Son and R. David, "Design and analysis of a secure two-phase locking protocol," in Proc. ofIntl. Computer 
Software and Applications Conf., November 1994. 
34. S. Son, R. David, and B. Thuraisingham, "An adaptive policy for improved timeliness in secure database 
systems," in Proc. of Annual IFIP WG 11.3 Conference of Database Security, August 1995. 
35, S, Son, R. David, and B. Thuraisingham, "Improving timeliness in real-time secure database systems," 
SIGMOD Record (Special Issue on Real-Time Database Systems), vol. 25, no. I, pp. 29-33, March 1996, 
36. S. Son and B. Thuraisingham, "Towards a multilevel secure database management system for real-time 
applications," in Proc. of IEEE Workshop on Real-Time Applications, May 1993. 
37, S. Thomas, S. Seshadri, and J. Haritsa, "Integrating standard transactions in real-time database systems," 
Information Systems, vol. 21, no. I, March 1996. 
38, B, Thuraisingham and H, Ko, "Concurrency control in trusted database management systems: A survey," 
SIGMOD Record, vol. 22, no. 4, December 1993. 
79 

•• 
Distributed and Parallel Databases, 8, 85-117 (2000) 
''II1II" © 2000 Kluwer Academic Publishers. Manufactured in The Netherlands. 
A Secure Agent-based Framework for Internet 
Trading in Mobile Computing Environments 
XUNYI 
CHEE KHEONG SlEW 
XIAO FENG WANG 
EIJI OKAMOTO 
leiS, School of EEE, Nanyang Technological University, Nanyang Avenue, Singapore 639798 
Recommended by: 
Vijay Atluri and Pierangela Samarati 
exyi@ntu.edu.sg 
Abstract. 
Most of the current Internet trading frameworks, in particular their negotiation and payment phases, are 
intended for customers frequently connected to the Internet during an entire transaction. This requirement cannot 
be easily met in the high communication cost and/or low bandwidth settings, typically found in mobile computing 
environments. Based on the software agent paradigm, a new secure agent-based framework for Internet trading 
in mobile computing environments is proposed in this paper. The framework is composed of two new protocols. 
One is the agent-based auction-like negotiation protocol, another is the agent-based payment protocol. Both of 
them are dedicated to solve the trade problems of Internet trading in mobile computing environments and ensured 
to be safe by cryptographic technologies. The combination of the two secure protocols constitutes an integrative 
solution for Internet trading in mobile computing environments. 
Keywords: 
electronic commerce, mobile agent, auction-like negotiation, secure electronic transaction (SET), 
signcryption 
1. 
Introduction 
In recent years, the Internet are becoming an increasingly important channel for retail 
commerce as well as business to business transactions. Large mainstream companies are 
setting up shops on the Internet not merely to have a presence, but to actually make 
sales online. Some specialty operations exist that do all of their business on the Internet 
such as Amazon.com (http://www.amazon,com), and NECX Direct (http://www.necx.com), 
FirstAuction (http://www.firstauction.com) and etc. Various recent studies by analysts from 
Nielsen, Forrestor and IDe have shown that the number of web buyers, sellers and trans-
actions are growing at rapid pace, The number of people buying on the Web is expected to 
increase from 18 million in December 1997 to 128 million in 2002, representing more than 
USD400 billion worth of commerce transactions. 
Wireless networks have been known explosive growth over the last few years, reflecting 
a world in which it is important to be active regardless of location. As the Internet becomes 
more and more important for business transactions, it is natural to expect that wireless 
technology will be used to connect to this global network. The possibility of having all the 
resources and benefits offered on the Internet available while being away from home or the 

86 
YI, SIEW, WANG AND OKAMOTO 
office is particularly attractive. In mobile computing environments it is desirable to have 
all the facilities usually found on the Internet, including the possibility of acquiring and 
paying for products and services. 
The kind of mobility is usually based on portable devices with limited computing capacity 
and/or limited connectivity. For example, computing capable mobile phones (e.g. the Nokia 
9000 Communicator [19]), PDAs (e.g. 3Com's PalmPilot [7]), Handheld PCs (e.g. Psion 
Series 5 [20]), up to notebooks, connected to the Internet through a modem attached to a 
cellular phone (or with an internal GSM modem). In view of the conditions under which 
mobile computing takes place, including low bandwidth, poor connectivity and the high 
cost of connect time, it becomes ditlicult and expensive to handle long, connected sessions, 
which require the transfer of large amounts of data. 
Most of the current Internet trading framework, in particular their negotiation and pay-
ment phases, are intended for users frequently connected to the Internet. This requirement 
cannot be easily satisfied in mobile computing environments. For example, in an error-prone 
environment such as GSM or any other used for mobile communications, a user shopping 
on the Internet and trying to pay using SET compliant software [28] may experience several 
connectivity problems during the payment operation. Even with recovery mechanisms, it is 
easy to imagine how frustrating it can be for the customer to deal with a series of connection 
interruptions, let alone the accumulation of state information both in the wallet and in the 
merchant's server, in order to let the transaction proceed. Even if it eventually does succeed, 
its overall cost would probably have been too high. 
Furthermore, the potential of the Internet for truly transforming commerce is largely 
unrealized to date. Electronic purchases are still largely non-automated. While information 
about different products and vendors is more easily accessible and orders and payments can 
be dealt with electronically, a human buyer is still responsible for collecting and interpreting 
information on merchants and products, making decisions on merchants and products and 
finally entering purchase and payment information. 
Software agent technologies otIer a new paradigm for trading on the Internet. It can 
be used to automate several of the most time consuming stages of the buying process. 
Unlike "traditional" software, software agents are personalized, continuously running and 
semi-autonomous [16]. As a mobile, flexible and autonomous small program unit, mobile 
agents can roam a network, collect and analyze the information from servers on the Internet, 
negotiate with servers, make decisions where to buy and even make automated payments 
on behalf of customers. These qualities are conducive for optimizing the whole buying 
experience and revolutionizing commerce as we know it today [18]. However, customers 
are wary about employing agents to trade on behalf of them, largely because of concerns 
about unknown risks they may face. The key to alleviating many of these concerns-to 
mitigating the risks-is security issue of agents. 
In order to run, a mobile agent has to expose its code and data to the host environment 
which supplies the means for it to run. Therefore, the host is easy to decompose agent code, 
scan his data, tamper and even kill the agent. The confidential data in a mobile agent, such 
as negotiation strategies and credit card information, cannot keep secret to the host. 
So far, one of the best solutions to agent-based negotiation for goods sale is through 
auction. The most attracting character of auction is its open and simple framework. It is 
82 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
87 
unnecessary for a negotiation agent to keep its negotiation strategies secret to a merchant 
host. The research on an agent-based auction-like negotiation protocol can be found in 
reference [36]. This protocol allows negotiation agents dynamically to decide their route 
across the Internet in merchant hosts. Therefore, a merchant host may alter, to his advantage, 
the next stop on the agent traveling agenda. In this paper, by ordaining the list of merchants 
in negotiation agents, we propose another new secure agent-based auction-like negotiation 
protocol for Internet trading in mobile environments, which has the following particular 
features: (1) negotiation for agent-based trading is performed through a novel pattern of 
electronic auction. (2) negotiation results in merchant servers are ensured to be valid with 
their signatures. (3) malicious behaviors can be detected and the breeder can be dug out by 
the help of sociological factors. 
SET/A r22], guided by the SET rules and based on the mobile agent paradigm, has 
recently been developed to meet the requirements of Internet payment in mobile computing 
environments. In order to protect agent's confidential data (i.e., credit card information) 
against the potentially malicious merchants, SET/A has to rely on a secure agent execution 
environment, such as in a tamper-proof environment [31] or a secure coprocessor [33], 
located at merchant servers. In our opinion, this solution is high cost for merchants and 
the required security is not easy to ensure. In this paper, we propose another secure agent-
based payment protocol, namely SET/A+, which can remove the limitation of the security 
of the agent's execution environment at the merchant server by adding a trust verification 
center in the payment system for mobile computing. SET/A+ is able to ensure the same 
levcl of security as SET, providing an alternative means of online payment using the SET 
protocol. 
In addition, considering the characteristics of payment agents, an effective signcryption 
scheme binding encryption to digital signature is proposed as the underlying signature and 
encryption schemes of the SET/A+ protocol. By this signcryption, the private signature 
key is not only used to sign, but also to specify a symmetric key by which a message is 
encrypted. 
By combining the above protocols, we finally propose an integrative solution for agent-
based Internet trading in mobile computing environments. 
The remainder of this paper is organized as follows. Section 2 introduces background 
knowledge necessary to describe a new secure agent-based framework. Section 3 presents 
a new agent-based auction-like negotiation protocol. Section 4 proposes a new agent-based 
payment protocol. Section 5 combines the agent-based auction-like negotiation protocol 
and the agent-based payment protocol to constitute an integrative agent-based framework 
for Internet trading in mobile computing environments. Security issues and performance of 
the proposed framework are analyzed in Section 6 and Section 7 respectively; Conclusions 
are drawn in the last section. 
2. 
Background knowledge 
In this section, we introduce the background knowledge necessary to describe a secure 
agent-based framework for Internet trading in mobile computing environments. 
83 

88 
YI, SIEW, WANG AND OKAMOTO 
2.1. 
Difficulties of internet trading in mobile computing environments 
There are several issues regarding the conditions in which mobile computing takes place. 
In the context of this paper, we are mainly concerned with the following: 
1. Low bandwidth and poor connectivity-terrestrial wireless network protocols like GSM 
or satellite-based systems like IRIDIUM [12], typically offer bandwidths in the range 
of 2,400 bps to 9,600 bps (although there are claims for much larger bandwidths with 
broadband satellite systems in the future [17]). On the other hand, the connectivity based 
on these systems is generally of low quality, with high error rates. These factors make 
it difficult to handle long, connected sessions, transferring large amounts of data. 
2. High cost-using a cellular phone or a satellite-based connection is generally more 
expensive than through a traditional telephone carrier or ISDN. On the other hand, poor 
connectivity raises costs, since it leads to longer online sessions. 
In mobile computing, bandwidth capacity is thus proportionally inverse to the cost, and 
this fact has to be taken into account when designing applications for these environments. 
What is needed is some kind of asynchronous mode of operation, in which the customer 
can send the purchase request, disconnect, and later re-connect to receive the response 
from the merchant. This reasoning seems to suggest a typical message-passing (RPC-like) 
mechanism, but this is not suitable, for two reasons: 
1. Three of the five steps of the purchase request transaction in the SET protocol are 
executed on the cardholder's side, so there would have to be two messages: one to send 
the request, and the other to send the 01 and the digital envelope, after receiving the 
first response from the merchant. Since the cardholder may be disconnected from an 
arbitrarily long period after sending the first request, the whole transaction would have 
to wait for this intermediate synchronization to happen. 
2. On the other hand, if there would be a way to avoid this step and send a single message, 
this would mean disclosing sensitive information (e.g. negotiation strategy and account 
number) and letting it be used by a remote server in an unpredictable way. 
This means that it is necessary to reduce the customer's role to two steps, the initial 
purchase request and the final receipt of the response. The request sends all the necessary 
information to complete the transaction successfully, but in such a way that the remote 
system can never take control of it. This clearly demands an agent-based mechanism. 
The customer may send an entity (the agent) with enough information for processing the 
entire transaction and, at the same time, capable of hiding the sensitive data from the outside. 
2.2. 
Mobile agents 
Software agents are probably one of the fastest growing areas of information technology. 
They are being used, and touted for applications as diverse as personalized information 
management, electronic commerce, computer games and etc. An agent can be thought of 
84 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
89 
as a computer program that simulates a human relationship by doing something that another 
person could do for you [26]. An agent is a self-contained program capable of controlling 
its own decision making and acting, based on its perception of its environment, in pursuit 
of one or more objectives [14]. 
More than one type of agent is possible. In its simplest form it is a software object that 
sifts through large amounts of data and present a subset of this data as useful information 
to another agent, user or system. An example of this is an agent that read and analyze all 
incoming e-mail, and route it to an appropriate department or another agent for reply [32]. 
These types of agents are called static agents. 
Mobile agents, owned by a user or another software element, are capable of migrating 
from one computer to another to execute a set of tasks on behalf of its owner. The agents 
would typically gather and analyze data from a multitude of nodes on the network, and 
present a subset of this data as information to a user, agent or system. Mobile agents are 
said to be autonomous, in the sense that they can make their own decisions while away from 
their host. This implies that a mobile agent (agent, for short) is not just a piece of data being 
transferred between systems, but may also carry some logic (i.e. code) and state, which 
enables it to perform some part of its tasks in one system, migrate to another and continue 
its work there. 
Agent technology has received growing interest from the research community and has 
matured significantly in the last few years [23, 29]. However, the number of applications 
using this technology is still small. Electronic commerce is generally seen as one of the 
most promising application fields for mobile agents. For example, a buying agent leaves 
its host with the mission of querying several vendors about a certain product, determines 
which one offers the lowest price (or some other kind of preferred feature), buys the product 
from that one and pays for it. Clearly there is a perception that agents are suitable for this 
kind of activity and that the ability to pay is one of the desired properties they should have. 
A major concern is always how to do this in a secure way, in particular without revealing 
confidential information to the outside world. 
The mobile agent security can be split into two broad areas [5, 6]. The first involves 
the protection of host nodes from destructive mobile agents while the second involves the 
protection of mobile agents from destructive hosts. Telescript [30] has been developed to 
allow the safe interaction between mobile agents and the host, to control access to system 
supplied resources and to provide authentication facilities, communication privacy and 
system level authorization. Both Java [9] and Safe-Tel [3] also provide similar security 
features. These approaches can effectively protect host nodes against destructive mobile 
agents, but appear forceless in preventing mobile agents from malicious hosts because in 
order to run, a mobile agent has to expose its data and code to the host environment which 
supplics thc means for him to run. Therefore, the host is easy to decompose agent code, 
scan his data, tamper and even kill the agent. 
In order for a mobile agent to trade on behalf of human, there are a few requirements that 
the agent must fulfill: 
1. Small-sized-since we're assuming low and/or expensive bandwidth, we have to mini 
mize the time it takes to send the agent. On the other hand, transport media with low 
85 

90 
YI, SIEW, WANG AND OKAMOTO 
reliability make it difficult to transmit long streams of data. For example, a reliable 
transport protocol like TCP would require many re-transmissions [II], Clearly, the agent 
should be as small as possible for better performance and smaller costs. 
2. Survive inside hostile execution environments-the customer wants to be sure that the 
agent will be able to complete the transaction as expected, without being disturbed by 
any external factor. This requires a relative degree of tolerance to merchants' server 
faults, as well as immunity to attacks trying to make the agent take unwanted decisions 
or perform unwanted actions. 
3. Hide confidential information-one of the main purposes of SET is to offer an appropriate 
level of security, so that the customer can be sure that none of the confidential data (card 
number, expiry date, etc.) is disclosed to any unauthorized party. 
In this paper, we will examine mobile agents that have the ability to buy goods on behalf 
of its owner. 
2.3. 
Agent-based auction-like negotiation 
Negotiation is an important part of Internet trading. In their survey paper on automatic 
negotiation, Beam and Segev [1] define automatic negotiation as the process by which two 
or more parties multilaterally bargain resources for mutual intended gain, using the tools 
and techniques of electronic commerce. 
So far, one of the best solutions to automatic negotiation for goods sale is through auction. 
Auctions are usually used in the cases when the auctioneer wants to sell an item and get the 
highest possible payment for it while the bidders want to acquire the items at the lowest pos-
sible price. The most attracting character of auction is its open and simple framework. The 
research on electronic negotiation through Internet-based auction can be found in reference 
[2]. Up to now, some electronic auction houses are already established on the web, such 
as, On sale (http://www.onsale.com), FirstAuction (http://www.firstauction.com),ZAuction 
(http://www.zauction.com), Dealdeal (http://www.dealdeal.com) and Ubid (http://www. 
ubid.com). They post on-sale goods on the web page every day, customers can bid for 
them via the web browsers freely. 
Although there is an "auction fever" which tends to take auction as a panacea for shopping 
and selling, a closer look at its characteristics, however, reveals its hostility towards retail 
commerce. Guttman and Maes [10] pointed out problems with current online auctions such 
as "winner's curse" and low performances. 
Nowadays, consumers are much more in the driver's seat in the online market than in 
the physical-world market. This is largely due to the dramatic reduction of search costs. 
This increases the competition among retailers and forces them to positively differen-
tiate themselves in value dimensions other than price. However, instead of merchants 
competing for consumer patronage, traditional online retail auctions force consumers to 
compete with one another for a specific merchant offering. This brings in the "winner's 
curse", which pushes up the winning bid above the product's market valuation. On the 
other hand, retailers often care less about profit on any given transaction and care more 
about long-term profitability. Customers' loss through "winner's curse" actually destroys 
86 

SECURE AGENT-BASED INTER"IET TRADING FOR MOBILE COMPUTING 
91 
the relationship between retailers and customers, thus damaging the retailers' benefits in 
the long run. 
Furthermore, the traditional online auctions have long delay between starting auctions 
and purchasing the product. This retards a large number of impatient or time-constrained 
consumers. In fact, the English and Yankee auction protocols are usually implemented over 
the Internet for several days. Since only the highest bidder of an auction can purchase the 
auctioned goods, the rest of the bidders (the majority) have to endure the long fruitless 
delays. Unlike the vendue of works of art, this is quite annoying in retail commerce 
The development of mobile agents may provide some opportunities to improve auction 
performance. Chavez and Maes suggested dispatching agents to a centralized salesroom 
to conduct the auction locally [4]. This causes security concerns. In order for an agent 
to run, it must expose its data and code to the host resources. Therefore, if the auctioneer 
conspires with the owner of the salesroom, the auctioneer can manipulate the auction to his 
advantage. If the mobile agent's negotiation strategy is known to the host, it may be at a 
significant disadvantage. Suppose the merchant's host knows that the buyer's negotiation 
strategy is to accept all offers under a certain (unknown) threshold value. The merchant can 
begin at a price greater than the threshold value and repeatedly offer the buyer a penny less 
each time, until the buyer's threshold value is reached, at which point the (worst possible, 
for the buyer) deal is made. This results in the bidding agents to suffer losses. 
Considering the difficulty for a negotiation agent to hide negotiation strategies and the 
open and simple framework of auction, an agent-based auction-like negotiation protocol 
for Internet trading has been proposed in [36]. Different from traditional auction models, a 
negotiation agent of this protocol acts as a mobile auctioneer while online retailers bid for 
low price. It is strategically analogous to English auction. This in fact eliminates the "winner 
curse". In order to combine information gathering and negotiation process together, this 
protocol allows the negotiation agent dynamically to decide their route across the Internet 
in merchant hosts. Therefore, a merchant host may alter, to his advantage, the next stop on 
the agent traveling agenda or a few merchant hosts collude so that the negotiation agent 
only negotiate among them. 
In this paper, by restricting the negotiation agent always to roam in a specified list of 
merchant servers, we will propose another new secure agent-based auction-like negotiation 
protocol for Internet trading in mobile environments. 
2.4. 
Agent-based payment protocol 
In this section, we firstly describe the SET protocol and then an agent-based payment 
protocol for mobile computing environments. 
2.4.1. The SET protocol. Almost every Internet user has heard of credit card fraud, per-
formed by hackers eavesdropping on connections used to send the data-despite the fact 
that very few of those attacks have actually succeeded. Even the deployment of secure 
servers, based on protocols such as SSL or S-HTTP, is not enough, since the credit card 
information is deposited on the server, where it can easily be read by anyone with access 
to it (even if not authorized). 
87 

92 
YI, SlEW, WANG AND OKAMOTO 
The concern about protecting the user's credit card information lead VISA and Master-
Card, in association with major software and cryptography companies, to the development 
of the SET protocol [28]. The SET protocol is composed of several kinds of transactions, 
ranging from cardholder registration, merchant registration, to purchase requests, to pay-
ment authorization and payment capture. The participants of these phases are as follows: 
1. Cardholder-a customer using a payment card that has been issued by a certificate 
authority. 
2. Issuer-a financial institution that establishes an account for a cardholder and issues 
the payment card. The Issuer guarantees payment for authorized transactions using 
the payment card in accordance with the payment card brand regulations and local 
legislation. 
3. Merchant-a merchant offering goods for sale or providing services in exchange for 
payment. A merchant that accepts payment cards must have a relationship with an 
Acquirer. 
4. Acquirer-the financial institution that establishes an account with a merchant and pro-
cesses payment card authorization and payments. 
5. Payment gateway-a device operated by an Acquirer or a designated third party that 
processes merchant payment messages, including payment instruction from cardholders. 
On the assumption that cardholders and merchants have registered and obtained their 
certificates from the Issuer. The purchase request phase can be depicted in figure 1. 
C 
M 
PG 
request 
Cs(M), Ck(PG) 
Cs(C), 01, EpclK, PI} 
EI'G{K, PI} 
Authorization 
response, Cs (M) 
Figure 1. 
SET purchase request transaction. 
88 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
93 
SET uses two distinct asymmetric key pairs, one for key-exchange (whose public key 
is contained in certificate Ck), which is used for encrypting and decrypting operations, 
and another for signature (in certificate Cs ), used for creation and verification of digital 
signatures. Therefore, each SET participant possesses two kinds of certificates, one for 
each key pair type. A merchant will have a pair of keys for each card brand it accepts. 
The procedure of purchase request transaction (shown in figure 1) is described as follows: 
Step 1. A cardholder (C), looks at a catalog (printed in paper, supplied in a CD-ROM or 
available online on the Web) provided by a merchant (M) and, after deciding to purchase 
something, sends a request to the merchant's server. The request includes the description 
of the services or the quantities of the goods, the terms of the order, and the brand of the 
credit card that will be used for payment. 
Step 2. The merchant receives the request and sends back its own signature certificate 
Cs(M), and the key-exchange certificate Ck(PC) of a payment gateway (PG). The mer-
chant also sends a unique identifier, assigned to this transaction. 
Step 3. The cardholder (i.e., his or her software) verifies the certificates by traversing the 
trust chain to the root key (the public signature key of a certificate authority (CA» so as 
to assures the authenticity and integrity of the data (the merchant had digitally signed it), 
and creates two pieces of information: 
• The Order Information (01), containing control information verified by the merchant to 
validate the order, card brand and bank identification. The or also includes a digest of 
the order description, which includes the amount of the transaction and other elements 
such as quantity, size and price of the items ordered, shipping and billing addresses, 
etc. This data, not included in the 01, will be processed outside the scope of the SET 
protocol. 
• The Payment Instructions (PI), containing the amount of the transaction, the card 
account number and expiration date, instructions for installment payments (if that's 
the case) and a couple of secret values to prevent guessing and dictionary attacks on the 
data, among other elements. The PI is encrypted with a randomly generated symmetric 
keyK. 
Both elements will contain the transaction identifier and are dually signed, so they can later 
be linked together by the payment gateway. Then, the encrypted PI (i.e., X K [PI]), and the 
key (K) used to encrypt it are encrypted into a digital envelope, denoted as Epc(K, PI) 
(=Xpcf Klil X dPI], where "II" represents the concatenation of two data blocks), using 
the payment gateway's public key. Finally, the 01 and the digital envelope are sent to the 
merchant, along with the cardholder's signature certificate Cs(C). 
Step 4. The merchant verifies the cardholder certificate and the dual signature on the 01. 
The request is then processed, which includes forwarding the digital envelope to the 
payment gateway, for authorization (the details of this operation are outside the scope of 
this description). After processing the order, the merchant generates and signs a purchase 
response, and sends it to the cardholder along with its signature certificate. If the payment 
was authorized, the merchant will fulfill the order, by delivering the products bought by 
the cardholder. 
89 

94 
YI, SlEW, WANG AND OKAMOTO 
Step 5. The cardholder verifies the merchant signature certificate, checks the digital signa-
ture of the response, and takes any appropriate actions based on its contents. 
The software responsible for the cardholder's side of the protocol manages a data structure 
called a digital wallet, where sensitive data like certificates, private keys and payment card 
information are kept, usually in encrypted files. The merchant will have a more complex 
system composed of several parts, doing different jobs: managing the dialog with cardhold-
ers, signing messages and verifying signatures and certificates with CAs, asking payment 
gateways for payment authorizations, and so on. 
SET provides important properties like authentication of the participants, non-repudiation, 
data integrity and confidentiality. Each player knows only what is strictly necessary for his 
or her role. 
2.4.2. The SETIA purchase request. 
SET/A [22], guided by the SET rules and based 
on the mobile agent paradigm, has recently been developed to meet the requirements of 
Internet payment in mobile computing environments. The cardholder registration, mer-
chant registration, payment authorization and payment capture of SET/A are the same as 
SET. The only change is in the purchase request transaction which can be depicted in 
figure 2. 
The process of SET/A purchase request transaction is described as follows: 
Step 1. As before, the cardholder C chooses a merchant and builds a request with the same 
elements as in the original SET request. Then, an agent, ACC), is sent to the merchant's 
C 
A (C) 
M 
PG 
A(C){request, Cs(C), X?[OLP l-<latall, -
-
-
-
-
-
--, 
I 
I 
I 
request 
I 
I 
C,(C), Ck(PG) 
I 
I 
I 
I 
I 
I 
C,(C),Ol 
I 
I 
EpG1K, PI} 
I EpG{K, PI} 
I 
I 
I 
I 
authorization 
I 
I 
I 
response 
I 
I 
Cs(M) 
I 
A(C){response} 
L 
-
-
-
-
-
-
---.J 
Figure 2, 
SET/A purchase request transaction, 
90 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
95 
server, carrying the request, the cardholder's signature certificate, the account information 
and other data needed to compose an 01 and aP] (0]]] _data). Clearly, this data has to be 
protected somehow, but SET/ A have not given a detailed proposal to deal with it and just 
assume it was encrypted with someone's key (X?[OLPLdata]). To our understanding, 
X? should be the public key of the secure agent execution environment located at a 
merchant server. 
Step 2. After arriving at the merchant's server, the agent sends (now locally) the request to 
the merchant M (i.e., its order processing software). 
Step 3. The merchant returns a signed message with its signature certificate Cs (M), the 
payment gateway key-exchange certificate Ck(PG), and the transaction identifier to the 
agent. 
Step 4. The agent verifies the certificates and the signature, creates the 01 and the PI 
and generates a dual signature of them. Next, it generates a random symmetric key K 
and uses it to encrypt PI. Finally, the payment gateway's public key is used to create 
the digital envelope E pG , containing the encrypted P] (i.e., XK[P]]), and the key K. 
This digital envelope, the 01 and the cardholder's certificate Cs (C) are then sent to the 
merchant. 
Step 5. The merchant verifies the certificate and the dual signature on the 01, and then 
proceeds as described above in Step 4 of the SET purchase request (see Section 2.4.1), 
sending a signed response along with its signature certificate. 
Step 6. The agent receives the response, verifies the certificate and the signature, and 
migrates back to the cardholder's computer. 
Step 7. The agent arrives at the cardholder's host, carrying the response from the merchant. 
The cardholder's software then proceeds as in SET's final step. 
Protecting confidential information, such as credit card information, in an agent from 
hostile environments is a major research issue in mobile agent security. The SET/A protocol 
suggests running the agent in a tamper-proof environment [31] or a secure coprocessor [33], 
to protect the agent against malicious merchants. That means that the agent would migrate 
to a protected (hardware) environment, securely attached to the merchant's server, and 
all the confidential data would be handled inside this environment. This solution would 
increase the security level at the cost of an additional investment in hardware from the 
merchant. 
Another approach suitable for SET/A to protect the agent-"software alternative", in 
which the agent executes inside the merchant server without any hardware protection, 
requires some kind of wrapping to hide the secret data, for example, taking some initial 
steps to execute hidden computations [24,25]. 
However, for SET/A, whether running the agent in a tamper-proof environment or a 
secure coprocessor or taking some initial steps to execute hidden computations needs to 
know the public key of the secure agent execution environment of a merchant in advance 
so that the agent can safely bring the confidential information, such as credit card informa-
tion, into it. If a cardholder gets the public key by sending a request to the merchant and 
receiving it from a reply, SET/A almost has not reduced the SET computational burden on 
the cardholder's side. 
91 

96 
YI, SlEW, WANG AND OKAMOTO 
2,5, 
Signcryption 
Signcryption is a new paradigm in public key cryptography. A remarkable property of 
a signcryption scheme is that it fulfills both the functions of public key encryption and 
digital signature, with a cost significantly smaller than that required by signature-then-
encryption. The explanation how to achieve the cost (signature & encryption) much less 
than the cost(signature) + cost(encryption) can be found in reference [37]. 
Clearly, mobile agents should be as small as possible for better performance and smaller 
costs. In view of it, a new signcryption protocol is proposed here for a participant of our 
framework to sign a message and distribute a symmetric key with his private signature key. 
The procedure of signing a message can be described as follows: 
l. As the Digital Signature Standard (DSS) [27], the signcryption protocol chooses three 
parameters (p, q, g), where p is a largc prime, q is a large prime factor of p -
1, g = 
h(p-l)/q (mod p) withh being an integer satisfying I < h < p-I andh(p-l)/q (mod p) > 
l. 
2. Each participant in our framework is required to generate a pair of signature public-secret 
keys. The pair of signature public-secret keys of an entity A is (YA, XA), where YA = gXA 
(mod p) and x A is a secret key chosen randomly from GF(q)*. 
3. The message (M) required to sign is hashed with an one-way 2n-bit hash function (H) 
which is based on a n-bit block cipher with 2n-bit key (such as IDEA [15] and the 
encryption algorithm in [34]) and shown in figure 3. In figure 3, Mi, G; and Hi are n-bit 
integers; E denotes a n-bit block cipher using 2n-bit key; EB presents bitwise exclusive-or 
of n-bit blocks while tEl indicates addition modulo 2n of n-bit integers. The detail hash 
process can be found in literature [35]. 
4. The digital signature of an entity A on a message (M) is composed of two numbers s 
and t which are defined as 
s = (gr(mod p»(mod q) 
t = -s . XA - H(M) . r(mod q) 
where r is a random number chosen from GF(q)*. 
(1) 
(2) 
Given (M*, s', t*), one can verify whether (s*, t*) is indeed a genuine signature of the 
Figure 3. 
Computational graph for the hash round function h. 
92 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
entity A on M* only by checking the following equation: 
gf' . Y~' . s*H(M')(mod p) = 1 
97 
(3) 
One can accept the digital signature of the entity A on the message M* if the above 
equation holds. This conclusion is based on the following theorem. 
Theorem 1. If sand t are computed as formulae (1) and (2) respectively, then Eq. (3) 
holds. 
Proof: 
From Eq. (2), we can deduce 
gf = gC-S-XA-HCM).r)(mod q)(mod p) 
= YAs . s-H(M)(mod p) 
After moving the right term to the left, we can obtain the Eq. (3). 
o 
The above signature scheme is used for the certificate authority (CA) to issue a signature 
certificate for the entity A in our framework as follows: 
The certificate authority (CA) adopts the certificate format of X.S09 [13] to construct the 
signature certificate message M A which may contain such information as certificate serial 
number, validity period, the public key (YA) of the entity A, the public key (YeA) of CA, 
etc., and then sign MA in accordance with the above signature scheme. 
When the entity A sends its signature on a message (M), i.e., (s, t), to the entity B, he can 
not provide the plaintext of M, but encrypt the message M with a block cipher (E) (which 
is the same as the underlying block cipher in the hash function shown in figure 3) and 
provide the entity B with the ciphertext of M, i.e., Ek(M), where k is randomly generated 
and encapsulated in the following form: 
(-r·s-x,·f)(modq) k( 
d 
) 
Z = YB' 
.. mo p 
(4) 
where (r, s, t) is the same as that in Eqs. (1) and (2). 
After the entity B receives (s, t, z) from the entity A, it can retrieve k from z on basis of 
the following theorem. 
Theorem 2. Ifz is computed asformula (4), then the following equation holds: 
SXB'S • y~8"t . Z = k(mod p) 
Proof: 
sXB'S . y~B"t . Z 
= gr,xB's . gXA ·XB·t . Z 
(r,s+xA'I) 
=YB 
. z 
=k(mod p) 
(5) 
93 

98 
YI, SIEW, WANG AND OKAMOTO 
The Eq. (5) means that the entity B can retrieve k if (s, t, z) and the entity A's certificate are 
given. With the secret-key k of the block cipher E, the entity B can decrypt the encrypted 
message, then hash the message and verify the signature of entity A on the message M 
according to the Eq. (3). 
D 
3. 
Description of the proposed agent-based auction-like negotiation protocol 
In this section, we will firstly deal with the structure of mobile negotiation agents and the 
process of auction-like negotiation. 
3.1. 
Structure of mobile negotiation agent 
In our framework, a mobile negotiation agent is defined as a software element (program, 
procedure, object, etc.), owned by a buyer who gets access to Internet by mobile devices, 
capable of migrating from one computer to another to execute negotiation task on behalf 
of its owner. The basic structure of a mobile negotiation agent can be divided into seven 
distinct sections as follows: 
l. Descriptor-the illustration of the agent's structure and format, the transaction identifier, 
encryption algorithm identifier, hash function identifier, signature algorithm identifier 
and etc. 
2. Certificate-the certificate of the agent's owner. 
3. Code-the program executed in an agent execution environment to fulfill negotiation 
mission. 
4. Data-the data including a list of merchants which the agent will visit, some possible 
error actions (the actions the agent should take should an error occur while the agent runs 
on the environment supplied by a server. Some possible error actions include discarding 
the agent, delivering an error notification to a specified address, routing the agent to a 
server) and etc. 
5. Time stamp-the time when the agent launches from the buyer. 
6. Signature-the signature of the buyer on the previous five sections. The buyer signs the 
agent in accordance with Eqs. (I) and (2) in Section 2.5. The signature can be verified 
by Eg. (3). 
7. State-the section for merchants to post their bids and signatures. 
The structure of a mobile negotiation agent can be illustrated in figure 4. 
Note that the representation of the code and data is outside of the scope of this paper. 
Here we focus on the negotiation process. 
3.2. 
Generation of mobile negotiation agent 
When a buyer wants to purchase something, he firstly looks at a catalog (printed in paper, 
supplied in a CD-ROM or available online on the Web) provided by merchants and chooses 
94 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
99 
Time 
Descriptor 
Certificate 
Code 
Data 
Signature 
State 
Stamp 
Figure 4. 
The structure of a mohile negotiation agent. 
a list of merchants with whom the buyer wants to negotiate to make a good deal. Then he 
creates a mobile negotiation agent in accordance with the structure as shown in figure 4. 
Finally the agent is motivated and migrates to the nearest merchant server on Internet 
according to the route specified in the data section of the agent. 
3.3. 
Verification of mobile negotiation agent 
When the mobile negotiation agent enters a merchant server first time, the server will 
automatically perform the following common verification procedure: 
I. Verifying the agent certificate with the signature public key of the certificate authority 
(i.e., YeA), supposed to be known by each participate of our framework. 
2. Checking the validity of Time Stamp in the agent and verifying the signature of the buyer 
on the agent with the signature public key of the buyer (i.e., YB) which is included in the 
certificate of the agent. 
3. Verifying the previous merchant's certificate and signature on the state of the agent in 
the above same way. 
If without problem, the merchant server supplies an agent execution environment for the 
agent to run its code. Otherwise, it must ask the previous server repeatedly to send the agent 
until it receives the correct agent or deliver a error notification to an emergency server. 
When the mobile negotiation agent re-enters a merchant server, the server do not need to 
perform the verification procedure (1) and (2), but only (3). 
In addition, merchant servers always need to keep the states of the passing agent and the 
previous merchant's signatures as non-repudiation evidences for a short period. Of course, 
merchant servers can automatically clean the data after the period. 
3.4. 
Agent-based auction-like negotiation process 
Our agent-based auction-like negotiation protocol for Internet trading in mobile computing 
environments is similar to the English Auction and the common values model (bidder's 
valuation is influenced by other's). In this protocol, the flow of the negotiation agent among 
the list of merchant servers Sl, S2, .... sn can be illustrated in figure 5. 
In figure 5, "~' denotes the negotiation agent. In addition, we use symbol "Bid~J" to 
represent the bid offered by SJ during the k round bid of auction. 
The negotiation process is described as follows: 
95 

100 
YI, SlEW, WANG AND OKAMOTO 
Buyer 
Merchant(l) 
Merchant(2) 
Merchant(3) 
Merchant(i) 
Merchant(n) 
0 
0 
0 
0 
0 
.. 
.. ---
0 
Sl 
S2 
S3 
sn 
0 
B 
Figure 5. 
Agent-based auction-like negotiation protocol. 
Generation of Initial Minimum Bid and Minimum Bid Decrement: The initial minimum 
bid (MBo) and minimum bid decrement (MBD) are two important parameters for auction-
like negotiation. They can be assigned by two ways: one is by the buyer, another is by the 
first bidder who wants to sell the goods. According to conventions, our negotiation protocol 
adopts the first way to assign MBD and MBo (put into the data of the agent). 
First Round Bid of Auction: In general, we suppose the negotiation agent A having 
fulfilled its task in Si-I and roaming into Si. 
The merchant server S firstly performs the verification procedure as Section 3.3. If 
without problem, it supplies an agent execution environment for the agent to run. 
When the agent run its code, it automatically tells Si (1) what goods it wants to buy and 
(2) the current lowest bid BidL . Then it asks Si to bid, Le., at what price the merchant can 
sell the goods. If Sl offers its bid Bidl" the negotiation agent will create a negotiation result 
NR1, including: 
• The transaction identifier found in the descriptor of the negotiation agent; 
• The round number of auction which is 1 in this case; 
• Bidl, which should be less than BidL (at least one minimum bid decrement); 
• The current time which acts as the time stamp. 
If Si wants to waive bidding, the place of the bid in the above contents will be a waiving 
declaration, indicating at which price it gives up. 
Afterward, the agent requires the merchant Si to sign NRl, (the signature is denoted 
as Sign(NR1,)) and then updates its state by attaching (1) Si'S certificate, (2) NR1, and 
(3) Sign (NR1J ) to the previous state. 
96 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
101 
s; 's Certificate 
S;'s 
Previous State 
NR1, 
Signature 
Sign(NR~,) 
on State 
Figure 6. 
The state of agent in the merchant server Si. 
SI 's Certificate S2' s Certificate 
sn 's Certificate 
sn's 
Descriptor 
NRl, 
NRl2 
... 
NR1, 
Signature 
Sign(NR~,) 
Sign(NR~,) 
Sign(NR1") 
on State 
Figure 7. 
The first round negotiation results. 
Furthermore, the agent demands Si to sign the new state of the agent and attaches the 
signature behind the state (shown in figure 6). 
Finally, the agent duplicates itself, keep one copy in the server, and migrates another to 
some merchant server in the list of merchant servers which have not been visited in the first 
round bid of auction. 
In this way, the negotiation agent roams one by one through the whole list of merchant 
servers. The last merchant server sn in the first round bid of auction sends a series of 
negotiation results of this round bid (shown in figure 7) back to the first merchant server 
SI. At this time, the first round bid of auction terminates. 
The kth Round Bid of Auction (k = 2,3, .. .): The k-th round bid of auction is almost 
same as the first round. The changes lie in: 
1. The negotiation agent never re-visit the merchant servers which have waived bidding. 
2. The message transferred among merchant servers is not the whole body of the negotiation 
agent, but its state with its descriptor identifying the agent (as shown in figure 7). 
3. Instead of attaching in the state of the negotiation agent like the first round, the negotiation 
result NR~, in the merchant server Si only replace the last round negotiation result NR~-: 1. 
On basis of the above discussion, we can obtain the following conclusion: 
Theorem 3. 
Assume Bid~m" Bid~m2' Bid~~, Bid~'m be significant, then the following in-
equalities hold. 
if m2 > m1 
if k2 > kl 
(6) 
(7) 
With the increase of the round number of auction, the lowest bid (Bidd will become 
less and less. Because the initial minimum bid (MBo) is limited and the minimum bid 
97 

102 
YI, SIEW, WANG AND OKAMOTO 
decrement (MBD) is greater than 0, this auction cannot repeat forever. There is always a 
bidder winning the auction after a few rounds, i.e., except from one bidder, all the others 
declare to waive bidding. In figure 5, we suppose the winner is the merchant server Si. 
Therefore, the negotiation agent residing in Si will send the final negotiation results like 
figure 7 to the buyer. 
3.5. 
Negotiation result check 
When the buyer gets access to Internet again, he can receive the negotiation results by using 
a mechanism similar the one used by cellular phone operators to deliver SMS messages 
when the user re-connects. 
From the negotiation results collected by the mobile negotiation agent, the buyer B can 
firstly verify each merchant's certificate and then its signature on the relevant negotiation 
result. If except from one, alllhe other merchants declare to waive bidding in some round 
auction, the auction-like negotiation results can be regarded as reliable. The agent-based 
auction-like negotiation is successfully fulfilled in the time. 
In the following procedure, the buyer may create a payment agent and migrate it to the 
winning merchant to perform the payment task. 
4. 
Description of the SET/A+ protocol 
In Section 2.4.2, we introduced an agent-based payment system-SET/A. We also point 
out that SET/A depends on a secure execution environment or hidden computation on 
the merchant's server to protect an agent's confidential data (i.e., credit card information) 
against a malicious merchant. In our opinion, the solution is high cost for merchants and 
the required security is not easy to ensure. 
In this section, we propose another agent-based payment system for mobile computing 
on Internet, namely SET/A+, which removes the limitation of the security of the agent's 
execution environment on merchant's server by adding a trust verification center in the 
payment system for mobile computing. 
SET/A+ is designed to be as compatible with SET as possible, only requiring significant 
modifications on the cardholder's side. The merchant software could remain unchanged, 
since its interaction with the agent is mostly the same as it would be with the cardholder. 
The only exception is that it must be aware that now it's talking to an entity residing in a 
host other than the cardholder's. 
4.1. 
Structure of payment agent in the SETIA+ protocol 
In the SET/A+ protocol, the structure of a payment agent is almost same as that of a 
negotiation agent shown in figure 4. The differences between them lie in the code, data and 
state. The code in the payment agent dedicates to fulfill certain payment mission. The data 
in the payment agent will include the order information, the payment information described 
in Section 2.4.1 and etc. The state of figure 4 is put together with code in SETI A+. 
98 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
103 
C 
TVC 
A(C) 
M 
PG 
A(Request) 
,-
-
-
-
-
-
-
-, 
I 
Initiate Request 
I 
I 
I 
Verification 
I 
I 
Request 
I 
Initiate Response 
I 
I 
I 
I 
I 
Verification 
I 
Purchase Request 
I 
Response 
Authorization 
I 
I 
Request 
I 
I 
I 
I 
I 
I Authorization 
I 
Purchase Response 
I 
Response 
A(Response) 
L 
-
-
-
-
-
-
.-J 
Figure 8. 
SET/A+ purchase request transaction. 
4.2. 
Purchase request of the SETIA+ protocol 
A purchase request under SET/A+ has a few more steps than in SET, since a payment agent 
has to be generated in a cardholder and to be sent to the merchant's server and return to the 
cardholder when the transaction is done. The process of a SET/A+ purchase request can 
be depicted in figure 8. 
Different from SET/A, we introduce a trust verification center (TVC) in our proposal. The 
TVC will charge cardholders or merchants for providing verification service. Therefore, 
it is reasonable for the TVC to be added in the payment system. In addition, we assume 
that public signature key of the TVC, i.e., YTVcC= gXTVC) is known to each cardholder. 
Under the above assumptions, the procedure of the SET/A+ protocol can be described 
as follows: 
Generation of Payment Agent: As SET/A, a cardholder (C), inspects a catalog (printed 
in paper, supplied on a CD-ROM or available online on the Web) of a merchant (M), 
after deciding to purchase something, creates a payment agent in accordance with figure 4, 
i.e., 
1. The cardholder's signature certificate (Cs(C)) is put into the certificate portion of the 
payment agent. 
2. The code and state portion are filled by program. The description of this program is out 
of the range of this paper. 
99 

104 
YI, SIEW, WANG AND OKAMOTO 
3. The order information (01), the encrypted payment instructions (PI), the digest of PI 
(i.e., H(PI», the information for the TVC, the dual signature of the cardholder on 01 and 
PI, and etc., are put into the data portion respectively. 
• The PJ is encrypted with a randomly generated symmetric key k, i.e. Ek(P!), where 
the bit length of k must be shorter than that of the prime p. 
• Both OJ and PI contain the unique transaction identifier Ie assigned by the cardholder 
C. 
• The information for the trust verification center contains: 
s = (gr(mod p»(mod q) 
t = -s . Xe -
H(MTVc) . r(mod q) 
z = Yiv~s~xC"t)(mod q) • (k + Ie· T + R)(mod p) 
w = E(k+Ic.f+R)(mod2,)(MTVc) 
(8) 
(9) 
(10) 
(11) 
where r is a random number chosen from GF(q)*, Xe is the private signature key of the 
cardholder, YTVe is the public signature key of the trust verification center, M yve is the 
message for the TVC, R is a random number chosen from GF(p)*, I is the key bit length 
of the block cipher (E) and T is the current time. 
• It should be noted that R is also put into the date portion of the payment agent. MTVe 
contains the transaction identifier Ie, the name of the merchant host from which the 
cardholder wants to order goods and the time stamp T. 
• The dual signature of the cardholder on 01 and PI is the signature of the cardholder on 
the message H[H(O!) II H(P!)]. The dual signature is denoted as SigndH(H(O!) II 
H(P!)]. 
4. The current time T is put into the time stamp portion. 
5. The signature of the cardholder on the message from descriptor to time stamp in the 
payment agent is put into the signature portion. 
Finally, a payment agent is sent to a merchant's server. 
Authenticating Payment Agent: After arriving at the merchant's server, the agent hands 
in its signature certificate and signature on the agent so that the merchant's server can verify 
the certificate and the signature with Eq. (3) by using: 
• The public signature key of the Issuer (YeA) which is known to each participant in the 
payment system; 
• The public signature key of the cardholder (Ye) which is known from the cardholder's 
signature certificate. 
If no problem, the merchant's server supplies an agent execution environment for the 
agent to run. 
Initial Request: The agent resides in the environment and sends (now locally) an initiate 
request to the merchant M. The message indicates which payment card brand will be used for 
100 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
105 
the transaction and which trust verification center will be used for certificate and signature 
verifications (i.e., Cs (TVC» and requests a copy of the gateway's certificate. 
Initial Response: When the merchant receives the request, it assigns a unique transaction 
identifier 1M to the message. It then provides the agent with an initial response, which 
ranges over the merchant signature certificate (Cs (M» and payment gateway key-exchange 
certificate (Ck(PG» that correspond to the payment card brand indicated by the agent and 
the transaction identifier 1M . The initial response takes the signcryption form as formulae 
(I), (2), (4) and Ek (*). 
Verification Request: The agent contacts the trust verification center by transmitting a 
verification request, which is composed of Cs(C), C,(M), the information for the TVC, 
i.e., (01', t. z, w) shown in (8)-(11), and the initial response from the merchant. 
After verifying CsCC), the TVC retrieves the symmetric key in the same way as Theorem 
2, i.e., 
SXTVCS • y~TVct • Z 
== grOxTVCs • gXCXTVCt . Z 
(r·s+xc t ) 
= YTVC 
. Z 
= (k + Ic . T + R)(mod p) 
Then the TVC decrypts w with the secret key (k + Ic . t + R)(mod 21) to obtain MTVc , 
verifies the signature of the cardholder on MTVc and keeps (MTVc , s, t) as a charge evidence. 
In the way, the TVC retrieves CkCPG) and 1M from the initial response of the merchant 
after verifying CAM) and the signature of the merchant on the initial response. 
Finally, after the TVC verifies Ck CPG), he replies the agent with a confirmation which 
includes: 
PGp«k + Ic . T + R) + 1M , T) 
where PGp is the payment gateway's public key-exchange key obtained from the key-
exchange certificate Ck(PG). 
Purchase Request: After receiving the above confirmation, the agent creates the digital 
envelope E pG for the payment gateway, containing the following items: 
(12) 
The purchase request including 
C,(C), 01, H(P!) , SigndH(H(O/) II H(PI»], E pG . 
is then sent to the merchant. 
Authorization Request: After checking the certificate Cs (C), in order to ensure that the 
order has not been tampered with in transit and thatit was signed using the cardholder private 
signature key, the merchant verifies the dual signature of the cardholder in the following 
way: 
101 

106 
YI, SlEW, WANG AND OKAMOTO 
• Because 01 is known to the merchant, he can compute the digest of 01, i,e" H(OI); 
• Because H (PI) is known to the merchant, he can calculate the digest of H (OI) II H (PI), 
i.e., H[H(OI) II H(P!)]; 
• The merchant can finally check the signature of the cardholder on H[H(OI) II H(PJ)] 
with Eq. (3). 
The purchase request is then processed, which includes forwarding the information 
Cs(C), E pG , H(O/), SigndH(H(OI) II H(PI»)]) 
to the payment gateway, for authorization. 
Authorization Response: If Ie, I M, Rand T are compatible, the payment gateway should 
be able to obtain correct k and then PI from E pG . If 01 and PI match, the dual signature 
can be verified with H(OI), PI and SigndH(H(O!) II H(P!)] in the similar way as in the 
former step. If no problem, it ensures that the PI has not been tampered with in transit and 
that it was signed using the cardholder private signature key. 
Next, the PG formats and sends an authorization request to the Issuer via a payment 
system. Upon receiving an authorization response, the PG generates and digitally signs an 
authorization request message, which includes the Issuer's response and a copy of the PG 
signature certificate. Then the PG sends the authorization response to the merchant. 
Purchase Response: This step is the same as that of the SET protocol. After the 01 
has been processed, the merchant software generates and digital signs a purchase response 
message, which indicates that the cardholder's order has been received by the merchant. 
The response is then transmitted to the agent. 
If the payment is authorized, the merchant will fulfill the order by shipping the goods or 
performing the services indicated in the order. 
Agent Return: After obtaining the purchase response from the merchant server, the agent 
puts the signature certificate of the merchant, purchase response and the digital signature 
of the merchant on the purchase response into its body and then migrates back to the 
cardholder's device. The cardholder's software then proceeds as in SET's final step. 
4.3. 
Extended application of the SETIA+ protocol 
SET/A+ is not only suitable for the payment system in which single merchant is involved, 
but also eligible for the payment system with multi-merchants. For example, a cardholder 
wants to order a few goods from different merchants respectively at one time, or to try 
different merchants until the goods is ordered. To fulfill this mission, it is necessary for the 
cardholder to specify a list of merchants in the information (MTVe) for the trust verification 
center and the corresponding roam route through the Internet and some transaction identifiers 
for the agent. After traversing the scheduled merchant servers, the agent brings a series of 
purchase responses back to the cardholder device. 
Next, we give a simple example to explain the extended application of the SET/A+ 
protocol. Let us imagine that a cardholder wants to order a salable goods and he knows 
that a series of merchants sell the salable goods by inspecting a catalog (printed in paper, 
102 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
lO7 
supplied on a CD-ROM or available online on the Web) of merchants, but he does not 
affirm which merchant has sold out the salable goods. Of course, he is reluctant to see his 
payment agent back without a useful purchase response from a visited merchant. In this 
case, the cardholder can specify a list of merchants visited by the agent in the data portion 
of the agent. When the agent is told that the salable goods is out of stock by a merchant, it 
will automatically roam to next merchant on the list. Until the agent finds a merchanl who 
holds the salable goods for sale, true SET/A+ protocol begins. 
The main reason why SET/A+ is most suitable for multi-merchant payment system is 
that no fixed public keys of merchants or secure agent execution environment are required 
when generating a payment agent. 
5. 
Description of the proposed agent-based framework for internet trading 
in mobile computing environments 
In this section, we consider combining the proposed agent-based auction-like negotiation 
protocol and payment protocol together to obtain an integrative solution for Internet trading 
in mobile computing environments. The integrative solution can be depicted in figure 9. 
The agent-based framework for Internet trading in mobile computing environments 
shown in figure 9 can be described as follows: 
Generation of Payment Agent (PA): The buyer creates a payment agent on basis of the 
SET/A+ protocol. Different from SET/A+, the buyer in the proposed framework will 
directly send a payment agent to the trusted verification center (TVC). Therefore, there 
are differences between the current payment agent and the previous payment agent. The 
differences lie in: 
1. R can not been put into the date section of the current payment agent. 
Buyer 
Merchant(i) 
TVC 
@ 
~@- ... -@-
Negotiation Results 
@ 
Response 
B 
Figure 9. 
Agent-based framework for Internet trading. 
103 

108 
YI, SlEW, WANG AND OKAMOTO 
2. The current MTVc contains the transaction identifier Ic and the time stamp T, but instead 
of the name of the exact merchant host from which the cardholder will order goods, a list 
of merchant hosts from which the cardholder probably order goods is included in M TVC' 
In addition, the current MTVc should include the highest price (HP) that the cardholder 
can accept to buy the goods. 
Generation of the Negotiation Agent (NA): The negotiation agent in the proposed frame-
work is almost the same as that in the previous negotiation protocol. A little modification is 
adding R and the order information (01) in the data section of the current negotiation agent. 
Flow of Mobile Agents in the Proposed Framework: The flow of mobile agents in the 
proposed framework can be described in the following steps: 
1. On basis of figure 9, the payment agent is sent to the trusted verification center. After 
arriving at the TVe, the agent hands in its signature certificate and signature on the agent 
to the TVe for verification. 
If no problem, the TVe retrieves (k + Ie' T + R) and decrypts the MTVc in the way of 
verification request procedure of SETf A+, then supplies an agent execution environment 
for the payment agent to run, i.e., waiting for the negotiation results from the negotiation 
agent. 
2. The negotiation agent from the buyer roams the list of merchants to negotiate the price of 
the goods by using the proposed agent-based auction-like negotiation protocol. After the 
negotiation process is completed, the negotiation agent in the winning merchant server 
(supposed to be Si) submits the negotiation results along with a verification request which 
contains a unique transaction identifier 1M specified by the merchant to this trade, the 
merchant signature certificate (Cs(M» and payment gateway key-exchange certificate 
(Ck(PG» that correspond to the payment card brand specified by the cardholder in the 
data section of the negotiation agent. 
3. The payment agent checks the negotiation results just like the buyer to do in Section 3 
on behalf of the buyer. If the lowest bid (Bidd of the negotiation results is lower than 
that of the highest price (HP) which the buyer can accept to buy this goods, the payment 
agent verifies Ck(PG) and creates a message which includes: 
Ek(PJ), PGp«k + Ie' T + R) + 1M , T), H(PI), SigndH(H(O!) II H(P!)] 
and sends it to the negotiation agent residing in the merchant server Si. 
4. After receiving the above confirmation, the negotiation agent in Si turns to the payment 
agent. It creates the digital envelope EpG for the payment gateway in the way of (12). 
Then the purchase request including 
Cs(C), 01, H(PI), SigndH(H(O!) II H(P!)], EpG 
is sent to the merchant. 
5. The other steps, such as authorization request, authorization response, and purchase 
response, are the same as the SETfA+. The purchase response is directly sent to the 
buyer or the cardholder. 
104 

SECURE AGENT-BASED INTERc"lET TRADING FOR MOBILE COMPUTING 
109 
6. 
Security issues 
In this section, we will firstly analyze the security of the proposed signcryption scheme 
and then discuss how the agent-based auction-like negotiation protocol protects negotiation 
results against potentially malicious merchants and how the SET/A+ protocol protects the 
cardholder payment information against potentially hostile merchants. Finally, we will 
simply explain the security of the proposed agent-based framework for Internet trading in 
mobile computing environments. 
6.1. 
Security of the signcryption scheme 
We now consider that a signer signs a message by adopting the signature scheme of the 
new signcryption scheme described in Section 2.5. For example, the entity A signs M by 
generating a pair (s, t) according to Eqs. (1) and (2). Because of the participation of the 
entity A's secret key in the formation of the digital signature (s, t) of the entity A on M, an 
attacker can not directly use the Eqs. (1) and (2) to forge a valid signature pair (s, t). 
A direct method by which an attacker may try is choosing a random number s (or t) and 
then solving out t (or s) from the Eq. (3) to forge a valid pair (s, t). However, the difficulty 
is at least equal to that of computing discrete logarithm over finite fields. 
In view of the above analysis, we conclude that: 
Theorem 4. 
The difficulty of breaking the signature scheme of the new signcryption 
scheme is at least equivalent to the difficulty of computing discrete logarithm over finite 
fields. 
We now take the key distribution between an entity A and an entity B as an example. 
Except the entity A and the entity B, the others do not know how to compute y~-r,s-xA ·t)(mod q) 
(mod p) (or SXS"S . y~B·t (mod p» because at least one of the entity A's or the entity B's 
private signature keys is definitely required in the computation. Therefore, the others can 
not separate the shared secret key k between entity A and entity B from z like the process 
in Theorem 2. The idea is similar to the scheme proposed by EI Gamal [8], in which 
message M is sent to an entity B in the form (gr, M . y~), where r is a random number, 
where YB = gXB(mod p). It suffices for the sender to know YB, whereas B can recover 
M by computing first g-r,xB. An eavesdropper faces the problem of computing discrete 
logarithms. 
On basis of the above analysis, we conclude that: 
Theorem 5. 
The difficulty of breaking the key distribution scheme of the new signcryption 
scheme is equivalent to the diffiCUlty of breaking the El Gamal scheme. 
6.2. 
Protection of negotiation results against malicious environments 
In order for a negotiation agent to run, it must expose its code and data to the host environment 
which supplies the means for that agent to run. Therefore, the host can always scan the 
105 

llO 
YJ, SIEW, WANG AND OKAMOTO 
agent for infonnation, alter the agent state and code, even kill the agent. Thus, the trade 
agent is unprotected from the host. 
Current consensus is that it is computationally impossible to protect mobile agents from 
malicious hosts. Instead of tackling the problem from a computational (difficult) point of 
view, current research [21] is looking at sociological means of enforcing good host behavior. 
In order to enforce good merchant behavior, we need firstly to establish some rules for the 
merchants to run negotiation agents. 
Rules for Merchants to Run Negotiation Agent: These rules can be outlined as follows: 
1. Whether merchants bid or waive, they must sign their bids or waiving declarations and 
put their decisions into the state of the negotiation agent. 
2. Sending the negotiation agent to the next server on the list of merchant servers which 
have not been visited in this round bid of auction until the next server declares correctly 
to receive the agent is the duty of merchants. If the next server rejects to receive the agent 
or no reply returns, the current server should send an error notification to an emergency 
server. 
3. Each merchant must keep the state of a passing negotiation agent from the previous 
server and the signature of the previous server on the state in each stage as no-repUdiation 
evidence for a period. 
4. Merchants are prohibited to tamper other merchants' negotiation results. 
As the sociological laws and regulations, the above rules can not ensure that it is im-
possible for merchants to break a law. However, we are always able to detect the kind of 
irregularities and finally dig out the breeder. If a merchant is accused as not honest, trading 
agents will never visit the merchant server again. It will result in decrease ofthe merchant's 
retail profit. 
Protection of Negotiation Results to Be Tampered: If the payment agent residing in the 
trusted verification center finds any problem when it checks the negotiation results or it 
receives an error notification from the negotiation agent, it will notice the TVC and the 
TVC will perfonns the following procedure to dig out the breeder: 
1. TVC informs all the merchants on the list of merchants in the negotiation agent that an 
error occurs and it is probably caused by malicious behaviors and asks them to commit 
their non-repudiation evidences. 
2. Those merchants who observe the negotiation rules certainly prefer to provide their true 
non-repudiation evidences to the TVC so as to prove them guiltless. The merchant who 
breaks the negotiation rules fears to do like this. But he have to do, otherwise he will be 
doubted as a malicious merchant in the beginning. 
3. After the TVC obtains all the states of the negotiation agent and the signatures of 
merchants on the states, it can recover the agent in each stage. 
4. Suppose the state of the agent in SJ is State~ (where k represents the bid round number 
of auction), then State~ and State~_l have the following relation: 
(13) 
106 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
III 
where" +--" means using the latter term to replace the relevant portion in the former 
term. 
If the TVC have not found any problem before sj, it can judge the merchant sj by 
checking whether the above equation holds. In Eq. (13), State] (including NR~J II 
Sign(NR~'J» and State1_1 are provided by Sj+l and sj respectively, both of them are 
provided by sj and Sj-l and ensured to be true with their signatures. 
In addition, the final winner of the auction may take a strategy against our negotiation 
protocol. At first, he offers his bid in very low level so that the other bidders have to waive 
and sign a waiving declaration. At last, he alters his bid into a higher level and signs it. 
This attack cannot succeed in our negotiation protocol because each waiving bidder has 
included the current lowest price in his waiving declaration. The bid of the final winner 
must be the lowest of all bids. 
On basis of the above discussion, we can conclude the following theorem: 
Theorem 6. 
If the negotiation rules (l )-(4) can be set up in the agent-based auction-like 
negotiation protocol, any malicious behavior can be detected and the breeder can be dug 
out. 
6.3. 
Protection of payment information against hostile environment 
In the Internet payment system, one obvious concern is to protect the user's critical data, 
in particular the credit card information. SET's use of the dual signature mechanism and 
the encryption of the PI and account information (into a digital envelope with the payment 
gateway's public key-exchange key) ensure the necessary privacy of critical data. 
Slightly Different from SET, both SETI A and SETI A+ dedicate to apply agent technology 
to overcome the difficulties faced by SET in mobile environment. Besides the consideration 
of protection of user's credit card information against the malicious host, it is required to 
think of security issues of mobile agents in the SET/A+ protocol. 
For SETI A+ to be able to ensure the same level of protection as SET, without modifying 
SET too much, it must be possible for the agent to carry classified information without 
having to disclose it to the wrong entities. Also, the generation of the symmetric key k in 
SET/A+ has to be performed in such a way that no one other than the cardholder and the 
payment gateway has knowledge of it. 
In accordance with the above considerations, in SET/A+ protocol, the cardholder is 
asked randomly to choose his own symmetric key k and personally encrypts the payment 
information with the key before the agent leaves the cardholder. Besides the cardholder and 
the payment gateway, no other participants can know the symmetric key k if we suppose 
that the trust verification center and the merchant never collude. The two reasons are as 
follows: 
• Although the TVC can retrieve (k + Ie . T + R) with its private signature key, it has 
no knowledge of k in view of the existence of random number R. R is kept in the 
107 

112 
YI, SlEW, WANG AND OKAMOTO 
agent execution environment located at merchant server and is never sent to the TVC. 
Furthermore, the TVC has no access to the order information. 
• Although the merchant can obtain the payment information encrypted with k, he can not 
retrieve either (k + Ie· T + R) from z or (k + Ie . T + R + 1M . T) from the envelope 
Epc;. Therefore, the agent do not disclose the payment information to the merchant in 
SET/A+ protocol. 
In view of the above reasons, we conclude: 
Theorem 7. 
Under the assumption that the trust verification center and the merchant never 
collude, the SETIA + protocol ensures the same level of protection as the SET protocol. 
6.4. 
Protection of merchant servers against malicious mobile agents 
A mobile agent is unique in that its code is executed by a server. Thus an executing agent 
has automatic access to some of a server resources. With this level of access agents can 
mount attacks by propagating viruses, worms and Trojan horses, impersonating other users 
and mounting denial of service attack. The standard approach to this problem is to reject 
all unknown code from entry into servers. It is not a viable solution in a mobile agent 
environment. Both Telescript [301 and Safe-Tcl [3] offer approaches to solve the problem. 
In the proposed agent-based framework for Internet trading, before a merchant server 
supplies an agent execution environment for a visiting agent to run its code, the server will 
verify the signature. Once any problem occurs when the server runs the code on an agent 
execution environment, the owner of the agent is probably malicious and will be accused. 
6.5. 
Security of the proposed agent-basedframeworkfor Internet trading 
The proposed agent-based framework for Internet trading in mobile environments is based 
on the agent-based auction-like negotiation protocol and the SET/A+ payment protocol. 
Therefore, its security is determined by the security of the two protocols. 
The negotiation phase of the proposed framework is completely the same as the agent-
based auction-like negotiation protocol. Therefore, the security of this phase is also the 
same as the protocol. 
Although the payment phase of the proposed framework is slight different from the 
SETI A+ protocol, their design criterion is same, i.e., do not disclose classified information 
to the wrong entities. For example, although the TVC can retrieve (k + Ie· T + R) with its 
private signature key, it has no knowledge of k in view of the existence of random number 
R. R is kept in the agent execution environment located at a merchant server and is never 
sent to the TVC. Therefore, the security of this phase is the same as that of the SET/A+ 
protocol. 
7. 
Performance analysis of the agent-based framework for Internet trading 
We have designed the agent-based framework with mobile computing in mind, especially 
focused on the two factors we have been pointing at as determinant: low bandwidth and 
108 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
113 
expensive connectlVlty. Our proposal of adopting an asynchronous computing model is a 
natural way to overcome those limitations. 
In this section, we discuss the advantages and identify possible scenarios in which the 
usage of the proposed agent -based framework designed for disconnected settings is the best 
way, or the only one that makes sense economically, to do shopping on the Internet in a 
mobile computing environments. 
7.1. 
Processing capacity 
PDAs and mobile phones have limited processing capacity, and can hardly handle processor-
demanding activities, such as those involving cryptography (key generation, encryption 
and signature generation and verification). Therefore, if one wants to use online shopping 
software on this kind of devices, either the CPU capacity has to be increased, or the load 
has to be transferred to an external machine. 
The proposed agent-based framework solves this problem by (I) moving complete nego-
tiation process to merchant servers and (2) adding a trust verification center on the Internet 
and doing part of cryptographic work at the TVC (or, at least, outside the cardholder's de-
vice). Therefore, this proposal is able to remove the computational burden from the user's 
device, which can be disconnected while the transaction is occurring. 
There is still the need to generate a pair of signature keys on the cardholder's side, but this 
belongs to another phase in the framework (the generation of the cardholder's certificates), 
not covered in this paper. Nevertheless, the keys and certificates could be generated in the 
cardholder's workstation and securely transferred to the less powerful device. 
7.2. 
Minimizing costs 
One of the problems of Internet trading is the relative high cost of small-value transactions. 
Using a mobile device to access the Internet and perform a small-value purchase, setting 
up the connection and maintaining it opened for as long as a SET-based transaction takes 
place, may have a cost similar (if not higher) to the value of the transaction itself. Having 
to pay the double of the price of a product, however low it may be, is enough to discourage 
customers to use this kind of connectivity for their shopping. 
Operations like certificate verification may be unacceptably inefficient when performed 
using a slow and expensive connection. Eliminating the need for this kind of operations with 
the payment protocol in the proposed agent-based framework contributes significantly for 
minimizing the customer costs. In addition, the payment protocol omits the key-exchange 
operation in the cardholder side and adopts the same block cipher in hash function and 
encryption operation. This purpose is to save storage in the mobile device. 
On the merchant's side, having an agent operating inside its server means having to 
let one additional certificate verifications be originated from it. But it is logical to expect 
that the merchant has good connectivity to the Internet, good enough so that a few more 
messages exchanged with the trust verification center and other merchant servers will cause 
a negligible raise in its costs, if any at all. But if that would make a substantial difference, 
the merchant could always charge a little extra over the prices of goods paid for with the 
109 

114 
YI, SlEW, WANG AND OKAMOTO 
proposed agent-based framework. So, given that the proposal does not increase costs, there 
is no loss in supporting this framework. On the other hand, it can be quite rewarding, from 
a marketing point of view. 
7.3. 
Improvement to the traditional online retail auctions 
The proposed agent -based auction-like negotiation protocol improves the traditional auction 
protocols such as Onsale (http://www.onsale.com), FirstAuction (http://www.firstauction. 
com) and etc. from the following sides: 
1. Traditional online retail auctions force consumers to compete with one another for 
a specific merchant offering. This brings in "winner curse" (winning bid is greater 
than the product market valuation) which in tum hurts the benefit of retailer himself. 
Instead of consumers competing with one another for a specific merchant offering, the 
proposed agent-based auction-like negotiation protocol forces merchants to compete for 
consumer patronage. In this way, our proposal completely overcomes "winner curse" 
which benefits the retailer in the long run. This makes the auction appears more forgiving 
than the traditional counterparts. 
2. In traditional online retail auctions, the long delay between starting auctions and purchas-
ing the product retards a large number of impatient or time-constrained consumers. In 
the proposed agent-based auction-like negotiation protocol, a negotiation agent, working 
on behalf of consumer, acts as a mobile auctioneer, while online retailers bid for low 
price. Because the agent actively finds potential bidders in a list of merchants and does not 
wait for a critical mass of bidders to complete auction, the auction efficiency is certainly 
much higher than that of traditional online retail auction. This also means the time con-
suming during the auction in our proposal is much shorter than that in traditional online 
retail auction. By limiting the negotiation agent to roam in a small or large range of mer-
chant servers, the hurry buyer may let agent return early with somewhat higher purchase 
price, while patient one may keep agent searching until locating a satisfying bargain. 
The auction process therefore can cater to the different requirements from customers. 
3. Traditional online retail auctions have brought communication intensifying: during the 
online auction, the salesroom server will become the center of the information exchange. 
This increases the communication load of certain network segment and the process load 
of the server. By contrast, the proposed agent-based auction-like negotiation decreases 
communications during the auction. Since most communications happen among differ-
ent servers, the loads are also balanced to the different network segments. This improves 
the network performance as a whole. In addition, the communication load in the cus-
tomer side is decreased to minimum, i.e., one time for delivering the negotiation agent 
and another for receiving the purchase response. Therefore, it is most suitable for the 
customer in mobile computing environments. 
7.4. 
Comparison of the SETIA+ with SET 
By comparing with SET, the merchant software in SET/A+ remains unchanged except 
providing an agent execution environment for a payment agent to run. The payment gateway 
110 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
115 
software in SETI A+ only needs to make slight modification when retrieving the symmetric 
key k. The main difference between SET/A+ and SET lies in the cardholder software. 
SET/A+ is not only suitable for payment in mobile computing environment, but also eli-
gible for online payment on the Internet. Let us imagine the situation in which a cardholder 
tries to order a few goods from different merchants on the Internet or order salable goods 
until a purchase is fulfilled. To satisfy this requirement, the cardholder in SET/A+ only 
need to send an agent to the Internet and receive the purchase responses brought back by 
the agent. It totals to two communications for the cardholder. However, if using SET, it 
will need much more communications for the cardholder than using SET/A+. 
As a consequence, the applications of SET/A+ will decrease the communication load for 
a cardholder to minimum. This is a notable characteristic of SET/A+. 
7.5. 
Comparison of the SETIA+ with the SETIA 
Although both SET/A+ and SET/A are agent-based payment systems, their required sup-
ports are distinct. In SET/A, each merchant on the Internet need to provide a secure agent 
execution environment for agents to run. This solution would increase the security level 
at the cost of an additional investment in hardware from the merchant. In addition, unless 
the secure agent environments owned by distinct merchants have same public keys, SET/A 
needs to know them firstly before sending an agent out. 
SET/A+ removes the limitation of secure agent execution environment in SET/A by 
adding a trust verification center in the payment system. It only needs to know the public 
signature key of the trust verification center. 
8. 
Conclusion 
The recent burgeoning of new communications technologies and, in particular, the Internet 
explosion has brought electronic commerce to the brink of widespread deployment. How-
ever, businesses are wary about treading beyond that brink, largely because of concerns 
about unknown risks they may face. The key to alleviating many of these concerns-to 
mitigating the risks-is security. 
Although the traditional auction protocols such as Onsale (http://www.onsaIe.com) and 
FirstAuction (http://www.firstauction.com) can easily deal with security issues, they suffer 
from some other problems such as "winner's curse" and low performances. It makes online 
auction less forgiving than would be expected in retail shopping. 
In this paper, we have proposed a new secure agent-based auction-like negotiation pro-
tocol for Internet trading in mobile environments, whose features lie in: (1) negotiation for 
agent-based trading is performed through a novel pattern of electronic auction. (2) negotia-
tion results in merchant servers are ensured to be valid with their signatures. (3) malicious 
behaviors can be detected and the breeder can be dug out by the help of sociological factors. 
SET is expected to gain wide acceptance as a secure Internet payment system since it 
combines the well-known credit card payment method with an elaborated security protocol. 
It is aimed at providing the necessary security through the authentication of the participants 
in a commercial transaction, as well as confidentiality of financial information. The fact that 
111 

116 
YI, SlEW, WANG AND OKA~OTO 
SET was developed by the major credit card companies is yet another factor contributing 
to its acceptance. However, SET is a very complex and "heavy" protocol, and if from the 
cardholder's point of view it may be generally simple to understand and use, its complexity 
may prove it unsuitable for some computational environments. 
In view of this, SET/A, based on the SET protocol and the mobile agent model, has 
recently been proposed. In order to protect an agent from a potentially malicious environ-
ment, SET/A depends on a secure execution environment in the merchant's server for an 
agent to run. However, the required security is not easy to ensure. 
In order to remove the limitation of the security of the agent execution environment, a 
novel secure agent-based payment system for mobile computing on the Internet (namely 
SET/A+) has been proposed in this paper. By adding a trust verification center into the 
payment system, SET/A+ is able to ensure the same level of security as SET, providing an 
alternative means of online payment using the SET protocol. 
Finally, by combining the proposed agent-based auction-like negotiation protocol and 
the SET/A+, we obtain an integrative solution for Internet trading-A secure agent-based 
framework for Internet trading in mobile computing environments. 
We are also interested in keeping the agent as intelligent and autonomous as possible, 
allowing it to make its own decisions (even if very simple) when needed. As part of our 
future work, we intend to use Aglet language to implement the agent-based framework 
for Internet trading, with agents capable of negotiating with their hosts on basis of the 
knowledge they carry as they migrate from one server to another and pay for goods on 
behalf of their owner. 
Acknowledgment 
The author would like to express appreciation to the anonymous reviewers for their valuable 
comments. 
References 
1. C. Beam and A. Segev, "Electronic catalogs and negotiations," CITM Working Paper 96-WP-IOI6, available 
at http://haas.berkeley.edu/citm/wp-1 OI6-summary.html. 
2. C. Beam, A. Segev, and J.G. Shanthikumar, "Electronic negotiation through Internet-based auction." CITM 
working paper 96-WP-10I6, Haas School, Berkeley, 1996. 
3. N. Borenstein, .. Email with a mind of its own: The Safe-Tel language for enabled mail," IFIP WG 65 
Conference, Barcelona, May, 1994, North Holland, Amsterdam, 1994. 
4. A. Chavez and P. Maes, "Kasbah: An agent marketplace for buying and selling goods," in Proceedings of the 
First International Conference on the Practical Application ofIntelligent Agents and Multi-Agent Technology, 
London, UK, April 1996. 
5. D. Chess, B. Grosof, C. Harrison, D. Levine, C. Parris, and G. Tsudik, "Itinerant agents for mobile computing," 
Technical Report, IBM TJ. Watson Research Center, NY, October 1995. 
6. D. Chess, C. Harrison, and A. Kershenbaum, "Mobile agents: are they a good idea," Technical Report, IBM 
TJ. Watson Research Center, NY, March 1995. 
7. 3Com Corporation, Palmpilot, http://www.3com.com/palm. 
8. T. EIGamal, "A public key cryptosystem and a signature scheme based on discrete logarithm," IEEE Trans. 
Info. Theory, vol. IT-31, no. 4, pp. 468-472, July 1985. 
112 

SECURE AGENT-BASED INTERNET TRADING FOR MOBILE COMPUTING 
117 
9. 1. Gosling and H. McGilton. "The Java language environment." Sun Microsystems white paper, 1995. 
10. R.H. Guttman and P. Maes, "Agent-mediated integrative negotiation for retail electronic commerce:' Proceed-
ings of Workshop on Agent Mediated Electronic Trading, Minneapolis, Minnesota, USA, May 199R. 
II. L. Hurst, "MCK: mobile communication kernel," Dagstuhl Seminar on Mobile Software Agents, October 
1997. 
12. IRIDIUM LLC, The IRIDIUM system, http://www.iridium.comlsystemlsystem.html. 
13. ISO/LEe 8696-8 Information Technology-Open System Interconnection-The Directory: Authentication 
framework, 1993. 
14. N. Jennings and M. Wooldridge, "Software agents," IEEE Review, January 1996. 
15. X.J. Lai and J.L. Massey, "A proposal for a new block encryption standard," Advances in Cryptology, Proc. 
of EUROCRYPT' 90, Lecture Notes in Computer Science, vol. 473, pp. 389-404,1991. 
16. P. Maes, "Agents that reduce work and information overload," Communications of the ACM, vol. 37, no. 7, 
pp. 31-40, 146, ACM Press, July 1994. 
17. 1. Montgomery, "The orbiting Internet: Fiber in the sky," Byte, vol. 22, no. II, November 1997. 
18. A. Moukas, R. Guttman, and P. Maes, "Agent-mediated electronic commerce: an MIT media laboratory 
perspective," to appear in Proceedings of the International Conference on Electronic Commerce. 
19. Nokia, Nokia 9000 communicator, http://www.nokia.comlcom9000/n9000.html. 
20. Psion, Psion series 5 handheld computer, http://www.psion.co.uk/seriesS. 
21. L. Rasmusson and S. Janson, "Simulated social control for secure Internet commerce," in New Security 
Paradigms'96, ACM Press, September 1996. 
22. Artur Romao and Miguel Mira da Silva, "An agent-based secure Internet payment system for mobile com-
puting," TrEC'98, Hamburg, Germany, 3-5 June 1998, LNCS. vol. 1402, Springer. 
23. K. Rothermel and R. Popescu-Zeletin (Eds.), "Mobile agents," Lecture Notes in Computer Science, vol. 1219, 
Springer, April 1997. 
24. T. Sander and C. Tschudin, "Towards mobile cryptography," Technical Report TR-97-049, International 
Computer Science Institute, November 1997. 
25. T. Sander and c.F. Tschudin, "Protecting mobile agent against malicious hosts," Mobile Agents and Security, 
LNCS 1419, Springer-Verlag, 1998. 
26. T. Selker, "A teaching agent that learns," Communications of the ACM, vol. 37, no. 7,1994. 
27. "The digital signature standard," Communications of the ACM, vol. 35, no. 7, pp. 36-40,1992. 
28. Visa International and MasterCard International, Secure electronic transaction (SET) specification, Version 
1.0, May 1997. 
29. J. Vitek and C. Tschudin (Eds.), "Mobile object systems-towards the programmable Internet," Lecture Notes 
on Computer Science, vol. 1222, Springer, July 1996. 
30. J. White, "Telescript technology: the foundation of the electronic market," General Magic white paper, 1995. 
31. U. Wilhelm and X. Defago, "Objects proteges cryptographiquement," in Proceedings ofRenPar'97, Lausanne, 
Switzerland, May 1997. 
32. L. Wirthman, "Gradient DCE has sign-on feature," PC Week, March 1996. 
33. B. Yee, "A sanctuary for mobile agents," in Proceedings of the DARPA Workshop on Foundations for Secure 
Mobile Code, Monterey, CA, USA, March 1997. 
34. X. Yi, "On design and analysis of a new block cipher," Proceedings of 1996 Asian Computing Science 
Conference, Asian'96, Singapore, LNCS, vol. 1179, Spring-Verlag, December 1996. 
3S. X. Yi and K.Y. Lam, "Hash function based on block cipher," IEE Electronics Letters, vol. 33, no. 23, 1997. 
36. X. Yi, X.F. Wang, K.Y. Lam, E. Okamoto, and D. Frank Hus, "A secure auction-like negotiation protocol 
for agent-based Internet trading," in Proceedings of 17th IEEE Symposium on Reliable Distributed Systems, 
Purdue University, IN, USA, 20-23 October 1998. 
37. Y. Zheng, "Digital signcryption or how to achieve cost(signature & encryption) «cost(signature) + 
cost(encryption)," Advances in Cryptology-Crypto'97, Lecture Notes in Computer Science, vol. 1294, 
pp. 165-179, Springer-Verlag, 1997. 
113 

•.... 
Distributed and Parallel Databases 8,119-145 (2000) 
"III' © 2000 Kluwer Academic Publishers. Manufactured in The Netherlands. 
Providing Security and Interoperation 
of Heterogeneous Systems* 
STEVEN DAWSON 
Computer Science Laboratory, SRI International, Menlo Park, CA 94025, USA 
SHELLYQIAN 
SecureSoft, Inc., 275 Shoreline Dr., Suite 520, Redwood Shores, CA 94065, USA 
PTERANGELA SAMARATI 
Universita di Milano, Dip. Scienze Informazione, Polo di Crema, 26013 Crema, Italy 
Recommended by: Vijay At1uri and Pierangela Samarati 
dawson@cs1.sri.com 
sqian@securesoft.com 
samarati@dsi.unimi.it 
Abstract. Interoperation and information sharing among databases independently developed and maintained by 
different organizations is today a pressing need, if not a practice. Governmental, military, financial, medical, and 
private institutions are more and more required to become part of a distributed infrastructure and selectively share 
their data with other organizations. This sharing process inevitably opens the local system to new vulnerabilities 
and enlarges the space of possible threats to the data and resources it maintains. As a complicating factor, in 
general, data sources are heterogeneous both in the data models they adopt and in the security models by which 
protection requirements are stated. We present a modeling and architectural solution to the problem of providing 
interoperation while preserving autonomy and security of the local sources based on the use of wrappers and a 
mediator. A wrapper associated with each source provides a uniform data interface and a mapping between the 
source's security lattice and other lattices. The mediator processes global access requests by interfacing applications 
and data sources. The combination of wrappers and mediator thus provides a uniform data model interface and 
allows the mapping between restrictions stated by the different security policies. We describe the practical 
application of these ideas to the problem of trusted interoperation of health care databases, targeted to enforcing 
security in distributed applications referring to independent heterogeneous sources protected by mandatory policy 
restrictions. We describe the architecture and operation of the system developed, and describe the tasks of the 
different components. 
Keywords: 
secure interoperation, mandatory access control, query processing 
I, Introduction 
Interoperation of database systems independently developed and evolved is a pressing need. 
Organizations in both the commercial and military sectors are faced with the problem of in-
tegrating independent data sources and accessing them as if they were a single system. First 
initiated within the context of federated architectures [20], interoperation proposals have 
* A preliminary version of this paper appeared under the title "Secure Interoperation of Heterogeneous Systems: A 
Mediator-Based Approach," in Pmc. of the IFfP 14th International Conference on Information Security (SEC'98), 
Vienna-Budapest, 31 August-2 September", 1998 [8J. 

120 
DAWSON, QIAN AND SAMARATI 
been more recently developed with reference to mediator-based architectures [23]. Unlike 
federated databases, approaches based on mediation do not require the specification and 
management of a federated schema or multi database language. They therefore remove pre-
vious impediments to true interoperation and automation [19]. The interest in and success 
of mediated architectures is witnessed by a large number of research projects and prototypes 
under development (e.g., [4, 14, 17]). Most attention has been devoted, however, to data 
and query management issues in interoperation, while less attention has been devoted to 
the protection of information [25]. On the one hand this may reflect a natural relationship 
between the problems, since successful protection of information requires precise knowl-
edge of how information is managed. On the other hand, security is a basic requirement for 
interoperation. We can easily imagine that no organization would make its data available 
for interoperation without a guarantee that the information will be protected as required. 
The potential loss of control over the data made available for interoperation, and, possibly, 
the fear of compromising the security of all the information managed can definitely affect 
the development of shared infrastructure for real-world applications. Effective information 
sharing and interoperation can happen only if the holders of the different databases have 
assurance that access constraints on information they own or manage will be respected. 
The major requirements of integration/mediation efforts with respect to security issues can 
be summarized as follows: 
• Transparent access. U scrs of the application should be able to access information from 
any or all of the sources in a uniform and consistent way. Specialized knowledge of the 
individual sources should not be required. 
• Autonomy. Application users must be permitted access to information that they can 
access from any individual source. Furthermore, the sources must continue to function 
after integration as they did prior to integration. 
• Security. Users of the application must be denied access to any information that they 
cannot access from any source individually. In other words, local security policies must 
be respected. 
The satisfaction of these requirements is often complicated by the different protection 
characteristics of the component systems. Different systems may enforce different access 
control policies and/or require different constraints to be satisfied in order to allow access 
to data. This heterogeneity and the potential conflicts that may result need to be resolved 
to allow interoperation while compromising neither the autonomy nor the security of the 
individual components. 
Previous proposals addressing secure interoperation issues framed the problem within 
a federated database context, where a global schema, possibly under control of a central 
authority, is defined on the local data sources [10, 12, 15]. Moreover, access control is 
generally assumed to be regulated by discretionary policies, where access decisions are 
taken with respect to authorizations stated by users. Although mandatory security has been 
investigated, and some interoperation aspects have been addressed [11, 21], no system 
providing the function ali ties above has been presented. 
In this paper we consider the problem of securing information upon interoperation in 
the context of a mediator-based distributed system where access to data is enforced by 
116 

SECURITY AND INTEROPERA nON OF HETEROGENEOUS SYSTEMS 
121 
mandatory policies, and the security lattices at the different data sources may differ. Our 
goal is to allow data sources to interoperate and to make their data available to external 
applications in such a way that their autonomy and security are not compromised. By the 
assumption of a mediator-based architecture, our approach requires neither the definition 
of a global schema nor the existence of a global centralized authority. Each application 
can define its own local (virtual) database. Schema specification and security constraints 
are maintained locally at each source/application. The consideration of mandatory-based 
policies, in contrast to discretionary ones, seems appropriate since it avoids the burden of 
dealing with identities. Indeed, while it appears to be appropriate and practically feasible 
to require applications and sources to know about security classifications with which in-
formation they store or use may be labeled, it appears rather impractical to impose similar 
requirements on identities. The necessity of maintaining information about users' identities 
managed by other sources or applications, which can also become meaningless at the local 
level, would compromise flexibility and introduce a considerable management burden. A 
mandatory policy requires instead only knowledge of the security lattices and the specifi-
cation of how the levels in them relate. Data classification and clearance assignments can 
be performed locally, according to the requirements of autonomy and security. Moreover, 
once the relationships between the different policies have been specified, new databases (or 
tables in them) can be made available to applications without need for further specifications. 
Our approach to secure interoperation is based on the use of a mediator [23] and wrappers. 
A wrapper for each source provides a uniform data model interface to the system, resolving 
the problem of data model heterogeneity. The wrapper also provides mappings between 
security levels of application subjects and security levels of the local lattice. The mediator 
interfaces the applications and the data source wrappers. It provides for the definition of 
mappings between the application and the data source security lattices, and for ensuring 
their consistency. It also provides tools for the definition of a virtual application schema 
in terms of the schemas of local data sources. The mediator provides interoperation by 
processing every application query for global access control, query processing, and access 
retrieval at the data sources. This paper describes how the schema and security specifications 
are stated at the data sources and, through the mediator, at the applications. We present 
the architecture of the mediator-based system and discuss the tasks ofthe different software 
components and their interaction during system operation. To make the discussion of the 
system more concrete, we present our approach in the context of a hypothetical secure 
information integration effort. The goal of the effort is to develop a trusted mediated 
application, called Medlnfo, which integrates three separate and independent sources. Two 
of the sources, referred to as Clinic and Hospital, are multilevel secure (MLS) relational 
databases containing (actual) data from two units of a health care network. The third source 
is the well-known Medline medical research citations database. Medline was developed 
and is maintained by the (United States) National Library of Medicine and is accessible 
via the World Wide Web. Each of the sources has its own schema, semantics, security 
policy, query language, and data model. While Medlnfo has both a schema and a security 
policy, it has no data of its own, and thus can be viewed as a virtual database. The data for 
Medlnfo are supplied by the three sources participating in the application. Figure 1 depicts 
the Medlnfo application. 
117 

122 
DAWSON, QIAN AND SAMARATI 
)L2l Medlnfo 
t 
Application 
(virtual database) 
I 
Mediator 
I 
~r§~ 
Clinic 
Medline 
Hospital 
Figure 1. 
Medlnfo application. 
The remainder of this paper is organized as follows. Section 2 illustrates the specification 
of the data source and application security policies and of the mappings between them. 
Section 3 discusses the definition and labeling of the schema at the data sources. Section 4 
illustrates how the virtual application schema is defined and how the security levels of 
the virtual relations are determined. It also illustrates how the relationships defining the 
application schema are used in query processing. Section 5 illustrates how the access control 
on application queries is executed by both the mediator and the data sources. Section 6 
illustrates the system architecture, while Section 7 illustrates the content of the knowledge 
base core of the mediator process. Section 8 illustrates the different steps in the mediation 
process, and Section 9 gives an example of secure mediation of a query. Section 10 discusses 
related work. Concluding remarks appear in Section 11. 
2. 
Security policy specifications 
We assume that access control at each source and application is regulated by a mandatory 
policy. Mandatory policies govern the access by subjects to the information on the basis 
of classifications, or security levels, assigned to subjects (clearance) and to data objects. 
Levels are partially ordered according to a dominance (=:':) relationship, forming a lattice. 
With respect to information secrecy, access is regulated by the no-read-down and no-write-
up principles, according to which a subject can read only information classified at a level 
dominated by the subject's level and write only information classified at a level dominated 
by the subject's level rll. 
2.1. 
Application and source security lattices 
At each source and application x, a security lattice.ex = (Lx, =:':x) is defined stating the 
security levels used at the source/application and the dominance relationships between 
118 

SECURITY AND INTEROPERA TION OF HETEROGENEOUS SYSTEMS 
123 
pub 
Medline 
sys 
mld 
uL 
Clinic 
Figure 2. 
Security lattices. 
~ 
~s 
p~ 
/Ins 
~ 
Hospital 
pry 
Medlnfo Application 
them. The security lattice at a source specifies the classifications that can be assigned to the 
information stored at the source (and made available for interoperation) and the clearances 
that subjects accessing the source can be assigned. The security lattice defined at each 
application defines the possible clearances that subjects of the applications can assume and 
the possible classifications that can be assigned to the (virtual) relations of the application 
schema. 
Figure 2 illustrates possible security lattices for the application and sources of our running 
example. The lattices are interpreted as follows. Medline is a publicly accessible source 
and thus has a trivial security lattice consisting ofthe single level pub, applied to all objects. 
The clinic uses a simple security lattice with three levels: sys, which is used to protect the 
most sensitive data in the database; med, which labels clinical data; and une, representing 
unclassified data. The hospital employs a more elaborate lattice. The highest level is hsp. 
The next two lower levels, eli and ins, are used to label clinical- and insurance-related data, 
respectively. Level eli/ins represents the intersection of eli and ins, and reflects the need to 
provide access to certain data to both eli- and ins-classified subjects. Level pro corresponds 
to provider-sensitive data, and une to unclassified data. The Medlnfo application uses a 
lattice of intermediate complexity. The top level is hmo, with lower levels eli and fin 
representing the partition of data into clinical and financial categories. The lowest level is 
pry (Private)-the application does not support unclassified users. 
2.2. 
Cross-lattice relationships 
The security lattice defined at each application defines the possible clearances that subjects 
of the application can assume and the possible classifications that can be assigned to the 
(virtual) relations of the application schema. Users connecting to the application will have 
knowledge of these classifications only. They do not need any knowledge of the security 
lattices of the sources. Since the application provides access to the different data sources, 
to determine visibility of the application subjects on the data at each source, subjects' 
clearances must be mapped into corresponding clearances at the local sources. For this 
purpose the mediator requires, for each application, the specification of how the security 
levels of the application security lattice "map to" the security levels of each source that 
119 

124 
DAWSON, QIAN AND SAMARATI 
Medlnfo Application 
A------~ 
, 
/ 
I 
' 
/ 
..... 
m ~///// 
~ 
PVClirns 
u C 
pub 
u C 
Clinic 
Medline 
Hospital 
Figure 3. 
Security lattices and mappings. 
the application may need to access. Such mappings are specified by means of dominance 
relationships from the security levels of the application to the security levels of the source. 
More precisely, for each application A, the mapping is specified as a set of dominance 
relationships of the form (li 2:: l j), where li E L A is a security level in the application security 
lattice and l j E L SJ is a security level in the lattice of some source Sj. In the following, we 
refer to such relationships as cross-lattice relationships, in contrast to the lattice relationships 
(or edges) locally specified at the application or at the sources. Figure 3 illustrates an example 
of cross-lattice relationships between the lattice of the Medlnfo application and those of 
the three data sources introduced in figure 2. Cross-lattice relationships are represented by 
dashed arrows. In the following, we denote by Map A the set of cross-lattice relationships 
specified for the security levels of application A. We use the notation Ii 2::x Ij to denote 
relationships specified within an individual lattice .ex. We write Ii > x I j if Ii 2::x I j and 
Ii =1= I j. We use the notation Ii 2:: I j to denote a dominance relationship holding in an 
individual lattice (application or source) or between levels of the application and levels 
in a source either explicitly (i.e., belonging to Map) or because of the combination of the 
cross-lattice and lattice relationships. For instance, with reference to the lattices and cross-
lattice relationships of figure 3, med 2:: unc, hmo 2:: sys, and hmo 2:: unc; also, hmo 2:: sys E 
MaPMedlnfo' 
Notice that the cross-lattice relationships specify which levels of the applications domi-
nate which levels of the data sources and are unidirectional. No dominance relationship is 
specified between the levels of the sources and those of the applications. This is a character-
istic of the problem under consideration where, given a subject connected at a security level 
in the application lattice, we need to retrieve all information visible to him, that is whose 
(source) level is dominated by the level of the subject. In the processing of queries from a 
specific application, only the cross-lattice relationships between the application lattice and 
the source lattices and the relationships in the individual lattices involved are considered. 
Indeed, these relationships are the only ones needed to determine the visibility of subjects 
submitting queries. 
120 

SECURITY AND INTEROPERATION OF HETEROGENEOUS SYSTEMS 
125 
2.3. 
Correctness of the specifications 
The mappings between levels of the application and levels of the sources are used to 
determine the (application) security level to be assigned to the virtual application relations 
and the (source) security levels to be assigned to a requesting application subject for local 
access control by the source. For these translations between levels to be correct, we require 
the cross-dominance relationships specified for each application to be consistent and absent 
from ambiguities and redundancies. 
As already stated, cross-lattice relationships are unidirectional: from applications to 
sources. However, since each source could also serve as an application, dominance rela-
tionships between two lattices can be specified in both directions, leading to the possibility 
of cycles. In such cases, the mediator must guarantee consistency in the combination of dif-
ferent relationships. Within an individual security lattice, the consistency property requires 
that no two levels in the lattice dominate each other, that is, forming a cycle. Intuitively, a 
cycle would permit an illegal (from lower to higher levels) flow of information. Our consis-
tency property naturally extends this requirement to the consideration of multiple lattices 
and cross-lattice relationships between them. Given a set of individually consistent lattices, 
consistency requires the combination of interlattice and cross-lattice relationships not to 
introduce any dominance relationships between levels in a lattice Lx that is not already 
present in the lattice Lx itself. 
Intuitively, an inconsistency corresponds to a path of dominance relationships from a 
levell; to a levell) in a lattice Lx, which is not dominated by l; according to the order C::x 
specified in the lattice. This concept is made clear by the following property. 
Property 1 (Consistency). 
For all lattices Lx = (Lx, C::x), and levels li, l) 
E Lx 
l; c:: I j =} l; C::x l). 
Figure 4(a) illustrates an example of inconsistent specifications. The figure contains 
three sets of inconsistent specifications. The first inconsistency is due to the presence of the 
mappings eli c:: eli from Medlnfo to Hospital and eli/ins c:: hmo from Hospital to Medlnfo. 
Medlnfo Application 
Medlnfo Application 
Medlnfo Application 
A 
A 
/~/ 
~/: 
" 
, 
aya 
" 
, 
8Y· 
" 
, 
[ 
, 
_---prY r ~ , 
'--
, 
I 
, .  
.... 
, 
" 
m;: 
m 
I 
" 
, 
, 
, 
\ ; 
u c 
U C 
!Ub 
~'--
/' 
~-- A 
f~--
___ /f 
---" 
Ii 
--
--::.,. 
" 
aya ::: 
pry 
--~ \, 
yo 
~
' 
~ ~ 
m 
~--------- ___ PVCI 
ns 
u c u e  
Clinic 
Hosp~al 
Clinic 
Clinic 
Medline 
(a) 
(b) 
(c) 
Figure 4. 
An example of inconsistent (a). ambiguous (b), and redundant (c) relationships. 
121 

126 
DAWSON, QIAN ANDSAMARATI 
Such mappings, together with the dominance re1ationshps within the lattice, would form 
a cycle of the dominance relationship involving two levels in a individual lattice. Such a 
cycle would allow eli subjects to indirectly acquire hmo-classified information. The second 
inconsistency is similar and is due to the presence of the mappings eli ~ eli from Medlnfo 
to Hospital and eli ~ fin from Hospital to Medlnfo. The combination of such mappings 
would imply a dominance relationship between eli and fin within the Medlnfo lattice, thus 
indirectly allowing, through the mediation of Hospital, the flow of information from fin 
to eli, violating the policy of the application. Finally, the third inconsistency is due to the 
presence of the dominance relationship between med in Clinic and eli in Medii ne, between 
eli in Medline and eli in Hospital, and between pro in Hospital and med in Clinic. The 
combination of such relationships together with the interlattice relationship between eli and 
pro would allow a hospital subject classified pro access to eli-labeled data, clearly violating 
the policy of the hospital. Note that inconsistencies correspond to paths from a level to a 
level that it does not dominate (either greater or incomparable). For instance, in figure 4(a) 
the existence of the two opposite dominance relationships between eli in Medline and rued 
in Clinic is not to be considered an inconsistency. Their semantics is merely that the two 
security levels are equivalent. 
The consistency property is the only constraint we need to ensure that no unauthorized 
information flow will occur. For the correctness of the security-level translations we also re-
quire cross-lattice relationships not to be ambiguous or redundant, as stated by the following 
properties. 
Property 2 (Nonambiguity). 
For all applications A, sources S, and levels Ii, Ij E LA, 
lu, Iv E Ls : li > A Ij , lu >s Iv. (li ~ lv) E MaPA =} (Ij ~ lu) tI MapA' 
The nonambiguity property states that a level I j that is dominated by a level Ii in the 
application cannot map, in a source, to a level that dominates a level to which li maps. 
Intuitively, the fact that Ii dominates Ii in the application means that Ii has visibility on 
more data than Ii' and such a situation should be respected in the mapping. The rejection 
of ambiguous specifications ensures that the schema classification and access control obey 
this principle. Figure 4(b) illustrates an example of ambiguous specifications where the 
dominance relationship between eli and prv in the application lattice appears reversed on 
the levels to which they map. According to such (ambiguous) mapping, lower-level pry 
subjects would have a larger visibility on the source data than higher-level eli subjects. 
Also, eli cleared users could augment their visibility by connecting at a lower level. Such a 
situation is obviously ambiguous. It is worth noticing that the mappings are ambiguous but 
in themselves not inconsistent since all the constraints stated by the interlattice and cross-
lattice relationships can simultaneously hold. Note instead that it is completely legitimate 
for two levels in a dominance relationship in the application lattice to map to incomparable 
levels in a source lattice. For instance, in figure 3 both fin ~ ins and pro ~ pry belong to 
MapA' The meaning of such relationships is that subjects at level fin can view, at Hospital, 
information classified pro (through the mapping specified for pry), as well as information 
classified ins. It is also completely legitimate for two incomparable levels in the application 
lattice to map to two levels in a dominance relationship in a source lattice. For instance, 
122 

SECURITY AND INTEROPERATION OF HETEROGENEOUS SYSTEMS 
127 
with reference to figure 2, eli of the application could be defined to dominate eli in Hospital, 
while fin could be defined to dominate pro. 
Property 3 (Non redundancy). 
For all applications A and sources S the following two 
conditions must be satisfied: 
(i) Forallli, lj E LA, lu E Ls: li >A Ij, (Ii ~lu) E MaPA =} (lj -:::.Iu) fj. MapA-
(ii) For all Ii E LA, lu,lv E Ls: III >s lv, (Ii ~Iu) E MapA =} (Ii ~lv) fj.MaPA-
The nonredundancy property states that (i) two levels in a dominance relationship in the 
application cannot be defined to map to the same level in a source lattice, and (ii) each level 
in the application lattice cannot map to two levels in a source lattice such that one of the 
source levels dominates the other. In other words, redundancy happens when there is both a 
direct and an indirect dominance relationship between a pair of security levels. Figure 4(a) 
illustrates an example of ambiguous specifications. There is a cross-lattice edge from both 
fin and pry in the application to pub in Medline. Although one of the relationships is 
implied by the other, and therefore holds, the direct edge from fin to pub is redundant. If it 
is intended that both fin and pry should map to pub, the cross-lattice edge from fin to pub is 
redundant and should be removed. Analogously, the mapping from eli to unc is redundant 
given the mapping eli to med. Like ambiguity, redundancy does not cause any improper 
information flow but it would introduce unnecessary complications and ambiguity in the 
translation process. 
Note that the nonredundancy constraint does not forbid two levels of the application to 
map indirectly to the same level, which is completely legitimate. For instance, in figure 3, 
both fin and pry in the application would translate to pub in Medline. It is also possible for 
an application level to directly or indirectly dominate more than one level in a source lattice 
if the levels are incomparable; that is, none of them dominates the others. This may happen, 
for instance, when a security level at the application dominates two levels at a source but 
it does not dominate their least upper bound. For instance, with reference to the lattices of 
figure 2, level fin could be defined as dominating (mapping) to both pro and ins in Hospital. 
It is worth noticing that the cross-lattice relationships are not required to be complete. 
That is, it is not necessary that an explicit dominance relationship be specified for each 
level in the classification lattice of the application and some level of each data source. 
For instance, in figure 3 no direct relationship is specified for the fin level of the Medlnfo 
application with respect to levels of the classification lattice of Clinic. The relationships 
between fin and the levels of Clinic are derived from those specified for pry. This situation 
reflects the fact that application subjects cleared at fin and those cleared at pry have the same 
access privileges to the information in source Clinic (both map to unc). In this example, a 
dominance for fin w.r.t. the levels of Clinic exists because of the cross-lattice relationship 
specified for prY. Note, however, that since no completeness property is required, it can 
happen that a security level in an application does not dominate any level in a data source. 
This is completely legitimate and reflects a situation where a security level of the application 
does not have any access to the information maintained at the data source. For instance, 
consider a situation like that in figure 3 but where there is no edge between pry of Medlnfo 
and unc of Clinic. Such a framework reffects a situation in which subjects cleared fin and pry 
are not allowed to see any information and should then be given no access to data of Clinic 
123 

128 
DAWSON, QIAN AND SAMARATl 
(queries by them will be blocked by the mediator and will not even be passed to Clinic). 
Note also that some levels of a source may have no levels in the application that dominate 
them. This reflects a situation where the source does not make available for interoperation 
information classified at certain levels. For instance, consider again the situation of figure 3 
and suppose that there is no edge between hmo of the application and sys of Clinic. In this 
case, no level at the application has visibility on information classified at sys at the Clinic, 
which can then be accessed only through direct connection by cleared users. 
Before closing this section, we note that in a framework where all sources and applications 
have a bottom element intended to denote "public" access (Le., all subjects are cleared for 
access to some data), completeness of the specifications can be enforced by the automatic 
specification of a dominance relationship between the minimal element of the application 
lattice (when no relationship is explicitly stated for it) and the minimal elements of the 
lattices of the sources. For the sake of generality, we do not make this assumption and 
require the specifications to adhere only to the properties stated earlier. 
3. Source schema definition and labeling 
Each source provides and maintains some data. We make no assumptions on the underlying 
data models used at the sources. For instance, with respect to our sample integration effort, 
Clinic and Hospital are MLS relational databases containing (actual) data from two units of 
a health care network. Thc mcdical research citations database is instead accessible through 
the World Wide Web via an HTML interface. A wrapper at each source provides a uniform 
relational interface between the source and the mediator. 
We also do not make any assumptions on the labeling policy applied at each source. 
This allows the data sources to maintain complete autonomy. For example, some sources 
may classify information at the table level, while other sources may provide classification 
at the attribute or element level. Regardless of the specific classification policy applied 
at a source, each object (relation produced by the wrapper) at the source has associated 
a security level. In case of finer-grained classification, the level assigned to each object 
will be the greatest lower bound of the security levels of the information in the object, 
as is common practice in mandatory security policies [9]. As described in Section 5, the 
security level associated with the object is the level that will be considered by the mediator 
to filter access requests to the object. Finer-grained access control (within the object) will 
be enforced at the source itself. 
4. 
Application schema definition and labeling 
4.1. 
Application/resource relationships 
At each application, a virtual database is defined using information collected from the 
different sources. The relationships between application and resource schemas are described 
in a general form that relates application queries to resource queries. This means that 
a relationship description may establish a correspondence between an arbitrary join of 
relations in the application schema and an arbitrary join of relations in a resource schema. 
In other words, relationships between application relations and resource relations are many 
124 

SECURITY A~D INTEROPERATION OF HETEROGENEOUS SYSTEMS 
129 
Labels 
Rules 
Application_query 
Resource_query 
{prY} 
RI: 
patient, patienLprivate 
+-' 
H.patients[unc] 
{ eli} 
R2: 
patient 
+-' 
C.patients[med] 
{prY} 
R3: 
physician 
+-' 
C.physicians[une] 
{pry} 
R 4 : 
physician ,ph..specia Ity 
+-' 
H .providers[pro] 
{eli} 
R5: 
event 
+-' 
c.events[med]' C.physicians[une] 
{eli} 
R6: 
event 
+-' 
H.providers[pro], H.events[eli] 
{eli} 
R7: 
visit 
+-' 
C.location[med]' C.visit[med] 
{eli, fin} 
R8: 
visit 
+-' 
H.visit[clijins] 
{prY} 
R9: 
research 
+-' 
M .pu blication [pub], M .author[pub] 
{ eli} 
RlO: 
research 
+-' 
c.events[med], C.physicians[unc] 
{ eli} 
R1I : 
research 
+-' 
H.providers[pro], H.events[eli] 
{fin} 
R I2 : 
ins_physician 
+-' 
H .providers[pro], H .insurance[ins] 
Figure 5. 
Sample Medlnfo application and resource relation~hips. 
to many. The relationship descriptions are created by a person with knowledge of both the 
application and the database resource with which it is related. Once created, the descriptions 
are stored in the knowledge base for use in the transformation process, as described in 
Section 8. 
The relationships are expressed by rules of the form 
application_query +-" resource_query 
where application_query and resource_query are essentially bodies of Datalog queries for-
mulated in terms ofthe application and resource schemas, respectively, and all the predicates 
in resource_query refer to relations of a single source. These rules can be understood as "the 
query application....query in the application schema corresponds to the query resourccquery 
in the resource schema (for some particular resource)", or, in other words, "resource_query 
provides some answers for application_query". Figure 5 illustrates an example of rela-
tionships between the Medlnfo application schema and the three database sources. The 
relationships are shown in an abstract form in which the attributes of the relations have 
been omitted. The meaning of the security levels appearing between square brackets and 
reported in column Labels is described in Section 4.3. The first rule in figure 5 states that 
a query on the relation H. patients in the hospital database provides answers for a query on 
relations Patient and PatienLprivate in the application. Answers for a query on applica-
tion relation Patient are also provided by a query on clinic relation C.Patients, as stated by 
the second rule. The detailed form of the first two relationship rules would appear as follows: 
R]: patient(Id,Ln,Fn,Adr,Dob,Ms,Sex), patienLprivate(Jd,Ssn,Race) +-" 
H. patients(Id,Ln,F n,Adr,Ssn,Dob,Ms,Sex,Race) 
R2 : patient(ld,Ln,Fn,Adr,Dob,Ms,Sex) +-" C.patients(ld,Ln,Fn,Adr,Dob,Ms,Sex) 
Note that more than one relation can appear in the right-hand side of a rule-rule Rs 
is one example. The intuitive meaning of this situation is that application_query contains 
125 

130 
DAWSON, QIAN AND SAMARATI 
information produced by the join of the relations appearing in resource_query. Also, differ-
ent rules may be defined with the same relation on the left-hand side (application_query). 
For instance, rules Rs and R6 both produce information for queries on virtual relation event. 
In this case the semantics is that the information in such relation is the union (in relational 
database terms) of the information collected through the different rules. Note the distinction 
between joining and unioning of the source relations. 
The application/resource relationships are used by the mediator to translate, by means 
of a process known as query folding, queries submitted to the application into queries on 
the sources. Intuitively, each relationship defines a possible way in which a query can be 
folded by the mediator and therefore a possible way in which some results can be obtained 
from the data sources, as illustrated in the following section. 
4.2. 
Query folding 
Before proceeding with the application schema labeling and system operation, it is useful 
to briefly illustrate how the resource relationships are used in the mediation process. 
The mediator processes queries with a query folding approach. Using the resource de-
scriptions stored in the knowledge base, query folding attempts to rewrite a given application 
query (expressed in the mediation language) into queries on available database resources. 
The folding process involves finding a suitable resource replacement for each literal in the 
body of the application query. If such replacements can be found, and if the conjunction of 
the replacement literals is consistent with the semantics of the original query, the rewritten 
query is called a complete folding, or, simply, afolding ofthe original query. The folding 
algorithm used in the prototype mediation system is guaranteed to find all complete foldings 
of a given application query. This means that the mediator will extract from the participating 
databases the maximum number of answers for the query. We illustrate the basic concept 
of the query folding approach through some examples. We refer the reader to [6,7, 18] for 
details. 
Consider the first two relationship rules of figure 5, whose detailed form has been given 
in Section 4.1, and consider the following simple query on the (virtual) relation Patient in 
the Medlnfo application: 
SELECT last_name, first_name FROM Patient 
After translation into the mediation language, the query becomes 
q1(Ln, Fn) :- patient(ld, Ln, Fn, Adr, Dob, Ms, Sex) 
which is then transformed into the two different queries 
q1a(Ln, Fn) :- H.patients(Id, Ln, Fn, Adr, Ssn, Dob, Ms, Sex, Race) 
q1b(Ln, Fn) :- C.patients(ld, Ln, Fn, Adr, Dob, Ms, Sex) 
to be forwarded to Hospital (query q1a) and Clinic (query q1b). 
126 

SECURITY AND INTEROPERATION OF HETEROGENEOUS SYSTEMS 
131 
Note that foldings need not have such a simple correspondence between query and 
individual resources. To illustrate consider the following extension of the query above: 
SELECT last_name, first_name, race 
FROM Patient, Patient_private 
WHERE Patient.id = Patient_private.id 
with translation into the mediation language 
q2(Ln, Fn, Race) :- patient(ld, Ln, Fn, Adr, Dob, Ms, Sex), 
patienLprivate(ld, Ssn, Race) 
This query requests the attribute Race from the logical relation PatienLprivate. Examining 
the rules for Patient, we see that this additional information is maintained in the hospital 
database in relation H.patients, but the clinic database has no corresponding information. 
Nevertheless, if the two databases contain some common patient information, the informa-
tion missing from C.patients might be filled in by H.patients. This possibility is reflected 
in the queries produced by the folding algorithm: 
q2a(Ln, Fn, Race) :-
H.patients(ld, Ln, Fn, Adr, Ssn, Dob. Ms, Sex, Race) 
q2b(Ln,Fn,Race) :-
C.patients(ld, Ln, Fn, Adr, Dob, Ms, Sex), 
H.patients(ldO, LnO, FnO, AdrO, SsnO. DobO, MsO. SexO, Race), 
Id = IdO 
In the first folding (query q2a) the relation H.patients alone is used to provide answers 
for q2. The second folding (query q2b) will attempt to supply answers through ajoin of 
H.patients with C.patients. Based on the resource descriptions, queries q2a and q2b are 
the only foldings that may provide answers to query q2. 
Having clarified how rules are used in the query processing to produce the view of the 
virtual relation for the application subject, we are now ready to discuss the security labeling 
of the application schema. 
4.3. 
Application schema labeling 
The security levels of the application schema are derived from the security levels assigned to 
the objects at the sources. Classification of the application schema is obtained by classifying 
relationship rules as follows. For each rule, the levels of the relations appearing in the right-
hand side of the rule are considered. According to the semantics of the cross-lattice mapping, 
a (source) relation classified at levell; should be accessible to all subjects classified at a level 
I j in the application that dominates I; (according to the lattice and cross-lattice relationships). 
Levell; of the source relation must then be translated into the set Ali of minimal levels in the 
127 

132 
DAWSON, QIAN AND SAMARATI 
application lattice that dominate Ii. Note that Ali is a set, since multiple minimal levels may 
exist that dominate Ii. For instance, in figure 3, the set of minimal levels in the application 
that dominate eli/ins in Hospital is {eli,fin} (meaning the relation can be accessed by subjects 
classified eli or above as well as by subjects classified fin or above). If the rule contains only 
one relation, this is the set of levels to be assigned to the rule. If the rule contains more than 
one relation, we need instead to determine the application levels that have visibility on all 
the relations in the right-hand side. These are the levels that dominate a level in each Ali, for 
alII; occurring in the right-hand side. Such levels are computed by calculating the cartesian 
product of all Ali and substituting each set in the cartesian product with the least upper bound 
of the levels appearing in the set. Nonminimal elements are then eliminated from the set of 
levels so produced. To illustrate, consider rule Rs in figure 5, where the levels of the source 
relations are reported within square brackets, and the lattices and cross-lattice relationships 
of figure 3. Relations c.events and C.physicians are labeled med and une, which map to 
sets {eli} and {prv}, respectively. Their cartesian product contains only one element, which 
is {e1i,prv}, whose least upper bound is {eli}. Consider instead rule R~. Relation H .visit is 
labeled eli/ins, which, as already discussed, translates in the application to the set of levels 
{e1i,fin}. Finally, consider rule R 12 • Level pro translates to {prY}, while ins translates to 
{fin}. The cartesian product contains only the set {prv,fin} whose least upper bound is 
fin. Note that, since an application level can dominate some levels in the source but not 
their least upper bound (see Section 2.3), it is important to first translate the source levels 
into application levels and then compute the least upper bound. Computing the least upper 
bound prior to translation could overclassify information, forbidding subjects to acccss 
information for which they have sufficient clearance. For instance, consider again rule R 12 . 
The least upper bound between the levels appearing in the right-hand side is hsp in the 
source, which would translate to hmo in the application, thus not allowing fin visibility to 
the information produced by the rule. The security levels associated with the rules are used 
at access control time to determine the dependencies that can be evaluated in the folding 
process and the queries to be forwarded to the sources for producing the information to be 
returned to a subject at a given level. In particular, each (virtual) relation may be expressed 
through different relationship rules; that is, it may appear on the left-hand side of different 
relationship rules. All such rules may have different classifications associated with them. 
This reflects the fact that not all subjects are cleared for all the data that can be retrieved 
through a query. The higher the subject's clearance the greater the number of rules applicable 
to the subject. For instance, with reference to query q2 illustrated in Section 4.2 and the 
classifications of figure 3, the folding algorithm will produce both queries q2a and q2b 
for subjects at level eli and hmo. It will produce only query q2a for subjects at level pry 
and fin. 
5. 
Access control 
Access control is enforced at both the application (mediator) and source levels. This "hy-
brid" approach, in contrast to mediator-only or source-only controls, ensures autonomy and 
security of the sources while avoiding unnecessary processing and forwarding of queries. 
In fact, with local control the sources will always control every request and possibly enforce 
128 

SECURITY AND TNTEROPERA nON OF HETEROGENEOUS SYSTEMS 
133 
finer grained restrictions, while with mediator control queries are filtered, and only the (lo-
cal) queries that can potentially retrieve information for which the requesting subject has 
clearance are generated and forwarded to the local sources. 
At the application level, the mediator controls access to the virtual relations by limiting 
the use of the rules that determine how an application query can be answered. (Note that 
since we address the problem of providing global visibility of information, we consider that 
only read accesses are requested at the application level, while write accesses are carried 
out locally at each source. This assumption reflects the real-world application of the kind 
of system under consideration.) Suppose that an application user connects to the system 
as a subject at level L and issues a query to the mediator. The query is formulated entirely 
in terms of the application schema, and hence references only the virtual relations of the 
application. For each relation referenced in the query, the mediator determines the set of 
rules on which the subject has visibility, that is, with at least one level dominated by the 
level of the subject. If the set of rules accessible by a subject for a given relation is empty, 
access to that relation is definitely prohibited, and the query is rejected. Otherwise, the 
mediator uses the accessible rules to attempt to answer the subject's query, according to 
the query folding process illustrated in Section 4.2. As an example, consider a subject with 
level prv who issues a query on relation research. Figure 5 contains three rules for research, 
the first labeled {prv}, and the second and third labeled {eli}. The mediator will use only 
the first rule in attempting to answer the query, since the last two rules should not be visible 
to the subject (prv does not dominate eli). Note, however, that access to rules does not, by 
itself, ensure that a query will be answered. Even if rules are accessible for each relation 
referenced in the subject's query, the mediator may still fail to find a way to answer the 
query [18J. In this case, the query is not rejected, but instead an empty set of answers is 
returned. 
At each source, access control may be enforced at a finer level than is currently supported 
by the application, depending on the capabilities of the particular source. Any source query 
generated by the mediator from an application query is issued to the (wrapper of the) source 
and labeled with the security level of the subject who issued the application query. The 
wrapper translates the subject level into the corresponding levels for the source and forwards 
the query to the source. This translation consists of determining the set of maximal levels 
in the source lattice dominated by the subject's level. For example, suppose an application 
subject operating at given level fin issues a query, and that this query results in a query 
to the hospital database. There are two maximal labels in the hospital lattice dominated 
by fin: ins and pro. Hence, two access requests are issued to the source: one at ins and 
one at pro. Note that in case of rules joining relations at incomparable levels, single-level 
requests will be submitted on each of the relation and not on their join, which will be 
computed afterwards. This to guarantee a proper view on the data to application subjects 
whose level dominates incomparable levels involved in a join but not their least upper 
bound. For instance, with reference to a request by a fin subject on relation ins_physician 
(rule R12), the request at level ins will return the data from H.insurance and the request 
at level pro will return the data from H_providers. Once data have been gathered, the 
join will be computed. Note instead that, according to the fact that joins are labeled with 
the least upper bound of the relations involved, none of the two levels would have been 
129 

134 
DAWSON, QIAN AND SAMARATI 
returned any information on a request on the join. This approach, which works well when 
the subject is restricted to the use of one level at the time, would not be appropriate in our 
context where an application level can map to multiple incomparable levels and should be 
therefore guaranteed the visibility on the union of the information visible to each of them. 
The computation of the join afterwards allow us to make use of the access control system 
at the source, working under the stated assumption, without compromising visibility of the 
information. Although application-level access control is sufficient to ensure that source-
level queries resulting from an application query will not be rejected by the source, it is not 
strong enough to allow source-level access control to be bypassed. For example, a source 
may permit tuple-level labeling of data. In such cases, either the source must enforce this 
finer level of access control, or the mediator (or wrapper) must filter out answers based 
on the requesting subject's level and the labeling of the data. Autonomy, efficiency, and 
assurance considerations argue in favor of maintaining source-level access control. 
6. 
The system architecture 
The mediator system has been implemented and demonstrated using the Medlnfo appli-
cation [5]. The core mediator components, as well as the translators and wrappers, are 
implemented in ANSI C. The translators are modules that can either be operated as stand-
alone programs or be linked directly with the core mediator components. Each wrapper 
is a stand-alone program that is called as needed by the mediator. The mediator itself runs 
as a single Unix process, and each database query issued by the mediator is managed by 
a subprocess of the mediator that calls the appropriate wrapper program. Communication 
between any wrapper process and its parent process is carried out using POSIX standard 
pipes to maximize portability of the mediator system. A limited amount of scripting code 
(e.g., Perl) is used to interface certain stand-alone components and for CGI processing. The 
two relational sources Clinic and Hospital are Trusted Oracle databases maintained on a 
Trusted Solaris platform. The Medline database is maintained separately (by the National 
Library of Medicine) and made available via the Internet. 
The overall architecture is illustrated in figure 6. The dotted areas delimit the application, 
the mediator, and the data sources. The cylinders represent information maintained, while 
rectangles represent software components. 
7. 
Knowledge base 
The knowledge base component maintains information regarding schemas and security poli-
cies of participating databases and the relationships between the application and each of the 
participating databases. The query transformer, the query language/data model translators, 
and the security policy translators all rely on information maintained in the knowledge base. 
Thus, the knowledge base is central to the mediator. Population of the knowledge base with 
meaningful and consistent knowledge of the sources and their relationships to the appli-
cation is critical to the construction of a successful mediated application. In a sense, an 
instance of the mediation architecture with an empty knowledge base can be viewed as 
130 

SECURITY AND INTEROPERATION OF HETEROGENEOUS SYSTEMS 
135 
Application 
Generator/Optimizer 
Query Dispatcher 
f-----I Wrapper C : 
SecUri~ 
P~licyJ 
Clinic 
'········M~dli~ 
Figure 6. 
Mediation architecture. 
seCUri~ : 
P~iCYJ : 
m 
Integrator/Administrator 
/ 
Cross~attice relalionships 
Knowledge Base 
1----.--1 Wrapper H 
SecUri~ 
P~IiCYJ 
...... H,;,;pii;"i' ..... 
a mediator "template" or "framework", which upon construction of the knowledge base 
becomes a mediator. 
The knowledge base is populated and maintained by an administrator interacting with 
the knowledge base editor. The main activities supported by a knowledge base editor are 
• Description of source schemas, language constraints, and representation structures 
131 

136 
DAWSON, QIAN AND SAMARATI 
• Specification of relationships between source schemas, in the form of mappings that relate 
queries answerable by one source to queries answerable by another source or application 
In addition to the knowledge required for semantic mediation, the knowledge base con-
tains information on security policies and their relationships for trusted interoperation. A 
security policy editor is provided that enables the administrator to perform the following 
security-related activities: 
• Description of security policies of the mediated application and data sources 
• Specification of mappings between the security policy of one source and that of another 
source or application 
• Identification and resolution of potential security violations that may result from inter-
operation 
Perhaps the most important function of the security policy editor is the identification of 
potential security violations. The editor allows the integrator/administrator to specify secu-
rity relationships one by one. After the addition of each relationship, the editor determines 
whether the added relationship would introduce a security violation, identifying also all 
relationships involved in the potential violation. The administrator can then decide to with-
draw the relationship that causes the violation or remove one or more relationships until the 
violation is corrected. Only then is the addition of a new relationship permitted. 
8. 
The secure mediation process 
At a high level, the mediation process consists of the following steps: 
1. Translate a subject's application query from the query language of the application (e.g., 
HTML) into the mediation language. 
2. Transform the application query (now expressed in the mediation language) into (possi-
bly multiple) target database queries (still in mediator language) including only relations 
for which the subject has sufficient clearance (mediator access control). 
3. Generate and optimize a global execution plan for target queries. 
4. Generate local query plans for target queries. 
5. Execute query plans. 
(A) Translate target queries from mediator language to database language. 
(B) Issue target queries on databases. Wrappers at databases determine appropriate 
clearance levels for queries by invoking the security policy translator and pass them 
on for local access control. 
6. Process results and return answers to the subject query. 
The user interface simply provides a means for a user to issue queries in terms of the 
application schema and to view the answers to that query provided by the mediator. The 
HTMLlCGI interface provides a graphical interface (via a Web browser) to the mediated 
132 

SECURITY AND INTEROPERA TION OF HETEROGENEOUS SYSTEMS 
l37 
Patient Information 
You may select one or more of the following patient attributes. 
For certain attributes you may restrict the information returned to 
specific values or ranges of values. Note that these attributes need not 
also be selected. For example, you may request the names and addresses 
of patients within a certain range of ID numbers. 
IfLastna:me 
r First name 
.J Street address 
.J City 
.JState 
.JZip 
f' Date of birth Range: 
.JSex 
"" Marital status 
""Race 
~ Male ~ Female 
Figure 7. 
Patient query form. 
(datefonnat DD-MMM-YV) 
application. The user logs in at a certain clearance level and then may query the system 
by entering SQL queries into a simple HTML fill-out-form interface like that illustrated in 
figure 7. A CGI (Common Gateway Interface) script recasts the HTML query in SQL and 
forwards it to a translator. The user views results formatted as HTML tables. 
The remainder of this section describes the main software modules participating in the 
secure query processing and their operation. 
8.1. 
Query translator 
The first stage in the mediation process is the translation of an application query into the 
mediation language. In the prototype system, an application query is expressed (after being 
recast from HTML) in a subset of SQL (restricted to conjunctive, or select-project-join 
l33 

138 
DAWSON, QIAN AND SAMARATI 
queries). After translation into the mediation language, the user's query is forwarded to the 
query transformer, which is the primary module of the mediator core. 
Translation is also used in a later stage of mediation, during the execution of queries 
generated by the mediator from an application query. Here the queries must be translated 
from the mediation language into the query language of the target database. Of the three 
databases mediated by the demonstration system, Clinic and Hospital use SQL as the query 
language, while Medline uses HTML as its query language. Thus, the prototype system 
includes modules for translating between the mediation language and SQL and for translat-
ing between the mediation language and Medline's HTML encoding. Normally, Medline 
is accessed via a Web browser by filling out an HTML form. Access to Medline from the 
mediator is achieved by generating an HTML query that is equivalent to the query gener-
ated by the form interface. Thus, Medline remains truly autonomous~its interface need 
not be modified to enable access from the mediator. Indeed, Medline would be unable to 
distinguish mediator accesses from Web browser accesses. 
8.2. 
Query transformer 
The query transformer attempts to rewrite a given query, formulated in terms of the appli-
cation schema, into one or more queries that can be answered by one or more participating 
databases. To accomplish this rewriting, the mediator performs the query folding process 
described in Section 4.2 subject to restrictions enforced by the access control as discussed in 
Section 5. Using descriptions of the relationships between application and target database 
schemas and the labeling specified, query folding rewrites a query on the application schema 
into a set of queries on one or more database schemas, if possible. Each query in the re-
sulting set of transformed queries is guaranteed to yield a subset of the answers to the 
original query. Furthermore, the set of transformed queries produced by query folding is 
complete, in the sense that the set will yield the maximum amount of information (from the 
participating databases) relevant to the original query. 
8.3. 
Query plan generator/optimizer 
If query transformation is successful, we have a set of queries to be issued to one or 
more databases. Some of the queries may be answerable completely by one database. For 
example, queries q1a, q1b, and q2a (from Section 4.2) are all single-database queries. 
Others may involve multiple databases, for example, query q2b, which involves both the 
hospital database and the clinic database. Individual queries involving multiple databases 
must be broken down, or decomposed, into subqueries that can be answered by individual 
databases. Some portions of a query may not be answerable by any database source. 
Such portions include joins between subqueries on different sources and the evaluation 
of selection conditions not supported by a source. These parts of the query must then be 
evaluated by the mediator. 
8.3.1. Global query plan. The details of how the set of transformed queries will be evalu-
ated are represented in a global query plan. The global plan specifies the order of evaluation 
134 

SECURITY AND INTEROPERATION OF HETEROGENEOUS SYSTEMS 
139 
of individual queries in the set, how the individual queries are broken down, if necessary, 
into subqueries, and what processing remains to be done by the mediator. A query plan 
consists of a sequence of subplans. Each subplan is made up of a group of query plans. 
In principle, all the queries produced in the transformation stage can be evaluated in par-
allel, since each query is independent of the others. In practice, the ability to evaluate 
the queries in parallel depends on the capabilities of the database resources (since several 
queries may need to be issued to the same database source) and the communication, storage, 
and processing resources available to the mediator. In the prototype system, the degree of 
actual parallel evaluation is determined at the plan execution stage (query dispatch and 
result processing). Hence, the global plan is generated to represent the maximum amount 
of parallelism possible in principle. This implies that the global plan will consist of a single 
group of query plans, one for each query in the transformed set. Generation of an evaluation 
plan for each query in the transformed set involves the following steps: (1) Identification 
of the database source for each literal in the body of the query, (2) decomposition of the 
query into single-source queries, and (3) formulation of a query for mediator processing, if 
necessary. 
Consider query q2b given in Section 4.2. Using resource information maintained in 
the mediator's knowledge base, the plan generator identifies the first literal in the query as 
a reference to the medical center database, while the second literal refers to the hospital 
database. The final literal, Id = IdO, expresses a join condition between relations from 
different databases and must be evaluated by the mediator. Thus, the query is decomposed 
into the following two single-source queries: 
q2bl(ld, Ln, Fn, Adr, Dab, Ms, Sex) :-
C.patients(ld, Ln, Fn, Adr, Dab, Ms, Sex) 
q2b2(ldO, LnO, FnO, AdrO, SsnO, DobO, MsO, SexO, Race) :-
H.patients(ldO, LnO, FnO, AdrO, SsnO, DobO, MsO, SexO, Race) 
and a mediator-evaluated query to perform the join and return answers to the original query: 
q2b3(Ln, Fn, Race) :-
q2bl(ld, Ln, Fn, Adr, Dab, Ms, Sex), 
q2b2(ldO, LnO, FnO, AdrO, SsnO, DobO, MsO, SexO, Race), 
Id = IdO 
Queries q2bl and q2b2 may be evaluated in parallel, while the evaluation of q2b3 must 
wait for the evaluation of q2bl and q2b2 to complete. 
8.3.2. Optimization. When a mediator system incorporates databases distributed over a 
wide area (e.g., the Internet), a major factor in the cost of mediation is the amount of network 
traffic between the mediator and databases. If the global query plan is generated naively, its 
execution may result in queries on individual databases that yield much more data, and hence 
result in much more network traffic, than necessary. In the prototype system, optimizations 
135 

140 
DAWSON, QIAN AND SAMARATI 
are performed that eliminate two principal sources of unnecessary network traffic, both of 
which result from the decomposition of multiple-source queries into single-source queries. 
Optimization is performed through projection and cross-product elimination. Cross-product 
elimination reconfigures the queries so to eliminate all unnecessary (i.e., not required for 
the computation of the result) cross products. Projection eliminates from the query all those 
attributes not necessary for the computation of the result so that only the data for necessary 
attributes are retrieved. A necessary attribute is one that is part of the projection (occurs 
in the head of the query) or participates in a join or selection condition. For instance, the 
optimized form of queries q2bl and q2b2 of Section 8.3.1 is 
q2bl(Jd,Ln,Fn) :- C.patients(Jd,Ln,Fn,Adr,Dob,Ms,Sex) 
q2b2(JdO,Race) :- H .patients(ldO,LnO,FnO,AdrO,SsnO,DobO,MsO,SexO,Race) 
since Ln, Fn, and Race are the projected attributes, while Id and IdO are involved in a join. 
8.3.3. Local query plan. In a heterogeneous environment, not all databases will necessarily 
have the same query evaluation capabilities. For example, some databases may not be able 
to evaluate certain built-in predicates (e.g., arithmetic comparisons) supported by other 
databases or by the application. In addition, some databases may have certain constraints 
on how queries may be formulated; for example, an attribute may be required to have a 
value supplied in the query (input only), or may not permit a value to be supplied in the 
query (output only). 
To ensure that individual source queries can be evaluated, a local query plan is generated 
for each such query in the global plan. Each local query plan can be viewed as a refinement 
of a source query node in the global plan. In the simplest case, the capabilities of the source 
are sufficient, the query meets the requirements of the source, and hence no refinement is 
needed. When the query contains (built-in) predicates not evaluable by the source, the query 
must be decomposed into a query evaluable by the source (if possible) and the remainder 
that must be processed by the mediator. If the local plan generator determines that the 
(possibly decomposed) query does not meet the constraints of the source, the mediator can 
avoid sending the query to the source and instead supply a null answer to the query. Note 
that, after local query plan generation, no local query optimization is attempted. Local 
query plans represent queries entirely answerable by a single database source, and thus 
optimization of the queries is left to the respective databases. 
8.4. 
Query dispatcher and result processor 
The plan execution component of the mediator consists of two subcomponents: a query 
dispatcher and a result processor. 
The query dispatcher is responsible for issuing the individual queries contained in the 
global query plan to the appropriate database sources or to the mediator's internal query 
processor. The order of evaluation is specified in the global plan, and the query dispatcher 
may issue the queries in any manner that satisfies that order. In particular, queries that are 
grouped together in the plan for parallel evaluation may be evaluated in any arbitrary order, 
subject to the constraints of the database sources (for multiple simultaneous connections) 
136 

SECURITY AND INTEROPERATION OF HETEROGENEOUS SYSTEMS 
141 
and the mediator's own internal resources. In the demonstration prototype, the dispatcher 
issues grouped queries in an arbitrary sequential order. Before any database query can 
be issued, it must be translated from the mediation language into the database's query 
language. Using information supplied in each local query plan, the dispatcher identifies 
the appropriate translator and calls it. The dispatcher then sends the translated query to the 
source specified in the plan. In the case of a mediator query, no translation is required, and 
the query is sent directly to the mediator's internal query processor. 
The result processor is responsible for combining the answers returned from database 
sources and formatting the processed answers for return to the user (the issuer of the origi-
nal application query). The processing necessary for combining the answers to individual 
database queries is specified by the query plan in the form of mediator queries. Recall 
that this processing includes computing joins of relations from different databases as well 
as the evaluation of built-ins that the query processors of some databases may not handle. 
Thus, the mediator contains a query processor capable of evaluating select-project-join 
queries. For return of the results to the application user, the mediator can be configured 
in either of two ways. In one configuration, the result processor computes the union of 
all answers to the application query and returns them as a single set. In the alternate 
configuration, the mediator returns the answers to each query in the set produced by query 
transformation (folding) separately, along with an indication of the source(s) of the informa-
tion. 
8.5. 
Wrappers and local access controllers 
Each database incorporated into a mediated system must have a wrapper module with which 
it communicates with the mediator. The purpose of a wrapper is to accept queries from the 
mediator, forward these queries to the query processor of the database, accept answers back 
from the database, and return these results back to the mediator. In a trusted environment, a 
wrapper must also provide a security policy translator to ensure consistent and meaningful 
use of security information in the mediation process. 
Local access control is enforced independently by the source access control system. As 
already discussed, we do not make any assumption on the local access control system, 
apart from the fact that a mandatory (lattice-based) policy is considered and that a security 
level is assigned to each object made available for interoperation. For the correctness of 
possible joins between data returned from multiple sources (see Section S.3.1) we require 
no polyinstantiation to appear in the result returned from a local query. This is consistent 
with the fact that the application subject is returned the data he can access according to his 
clearance but with no label attached. Consideration of polyinstantiation and its management 
at the global level would require a more sophisticated and complex mapping management 
across lattices, which, however, does not seem needed for the kind of applications under 
consideration. 
Note that the assumption of the use of a lattice base policy allows also for sources that 
do not enforce any access control, since these can be represented as a lattice with a single 
node that is dominated by the bottom element of the application lattice. Source Medii ne of 
our application is an example of this. 
137 

142 
DAWSON, QIAN AND SAMARATI 
9. 
An example of query securely mediated 
We now illustrate an example of a secure query mediation process, Suppose that a user 
connects to the application as a subject at level L, He then issues query Q to the mediated 
application via the "HTML Interface" (figure 6). For concreteness, suppose that L is eli, 
the lattices and the relationships between them are as illustrated in figure 3, and that Q is a 
query requesting research information on hypertension. Q would proceed as follows: 
1. Query Q is passed from the HTML Interface into a translator that converts Q, expressed 
in HTML, into Q', expressed in the mediation language. For our example query, Q' is 
expressed as a query on relation research. 
2. From query Q' (with level eli), the query transformer uses the knowledge base to deter-
mine a mediated query Q~. In general, Q~ may be a set of queries to sources C, M, and 
H. Each query in Q~ remains at security level L. For the example query Q', there are 
three relevant relationship rules (figure 5) for research. The transformer uses all three 
rules, since the query's level, eli, dominates the only level of all of them. Query Q~ is 
a union of three queries: one, Q~, is a join on M .publication, and M .author; another, 
Q~, is a join on c.events and C.physicians; and the third, Q'e, is ajoin on H.providers 
and H.events. 
3. Mediated query Q~ is passed to the query plan generator/optimizer, which computes an 
evaluation plan P for the component queries of Q~. Each of the three queries resulting 
from transformation in the example is a single-source query with no dependencies on 
the others. A simple plan allows Q~, Q~, and Q'e to be evaluated simultaneously at 
sources C, M, and H, respectively. 
4. Plan P is sent to the query dispatcher, which sends Q~, Q~, and Q'lf to the translators 
for sources C, M, and H, respectively. 
5. Queries Q~, Q:w' and Q'e are translated, respectively, into Qe, QM, and QH, which 
are then forwarded, respectively, to wrappers C, M, and H. 
6. Wrapper C translates security level L into security level Le for source C and forwards 
Qe to source C at security level Le, resulting in data Dc, which is passed back to the 
translator. Wrappers M and H do likewise for QM and QH, respectively. In particular, 
Le is med, LM is pub, and LH is eli. 
7. Answers Dc, D M, and D H are translated back into the mediation language, resulting in 
answers D~, D~, and D~, which are sent to the result processor. 
8. The result processor computes the union of answers D~, D~, and D~, and forwards it 
back to the subject. 
10. Related work 
Previous work on providing secure interoperation has mainly been performed within the 
context of federated database systems, characterized by the fact that a global schema, possi-
bly under the control of a certain authority, can be assumed, and on which access restrictions 
can be specified or derived. Also, access control has generally been based on discretionary 
policies. The different proposals have addressed the various problems in this context, such 
as management of global vs. local identities and their authentication [12], enforcement 
of different policies by the sources [13], definition of a global database and top-down 
138 

SECURITY AND INTEROPERA nON OF HETEROGENEOUS SYSTEMS 
143 
derivation (from the global to the local level) of authorizations [22], and specification and 
bottom-up derivation (from the local to the global level) of authorizations and consideration 
of administrative privileges [10]. All these approaches are clearly inapplicable in our con-
text. Multilevel security issues, always within the federated database context, have been 
addressed in [16, 21 J. In r 16] all sources are assumed to use the object -oriented data model 
and to apply the security orderings (i.e., the lattice at all sources is the same). Security is 
carried out via association of security levels to messages exchanged between the sources 
upon interoperation and to objects upon migration (relocation) from one source to another. 
In [21] sources represent a horizontal partition of a global DBMS, and their security lattices 
are all subsets of a predefined totally ordered set. Their approach to query processing is 
therefore not applicable in our context. 
The problem of combining security specifications of different interoperating systems 
while ensuring that no security breaches occur has been addressed in [3, 11]. In particu-
lar, [11] considers generic security specifications expressed in terms of permissions and 
restrictions. They characterize properties that must be satisfied for the composition not to 
compromise security and study the complexity of ensuring their satisfaction. The work 
in [3] specifically considers combination of mandatory policies, where the goal is to define 
an ordered set combining different security orderings in such a way that a set of (cross-
set) constraints among them is satisfied. Cross-set constraints are specified in both positive 
(ii ::: I j) and negative (ii t I j) form. The paper characterizes the problem of combin-
ing specifications and illustrates algorithms for the computation of the global order with a 
logic programming approach and a graph-based approach. The advantages of considering a 
mediator-based architecture, where no global ordering needs to be computed, and a simpler 
form of constraints (positive form) suffices, making the problem simpler in our context. 
Specifically targeted to providing security in a mediator-based interoperation architecture 
is the work in [2], where access control is based on credentials or roles. However, the paper 
addresses mainly aspects of authentication and secure communication with emphasis on 
protocols and encryption and anonymity enforcement. Credential- and role-based controls 
are also addressed in [26] in the context of a generic distributed infrastructure. Such controls 
are complementary to mandatory lattice-based controls considered by us. Another approach 
to providing security under interoperation [24, 25 J involves dynamic checking and possible 
sanitization of answers (based on content) before they are returned to the subject. At present, 
this approach relies on manual intervention for sanitization. Nevertheless, this approach can 
provide complementary security services within our architecture, and can be viewed as an 
additional component positioned between the result processor and the user interface. 
11. Conclusions 
We have presented a system that allows multilevel secure data sources to share their data 
and make them available to external applications in such a way that autonomy and security 
of each data source are respected. We have illustrated how the different schema and security 
constraints are specified at the data sources and at the applications, and how application 
queries are processed for access control and security constraint enforcement. We have also 
illustrated and discussed the architecture of the system we have built to provide secure 
interoperation. 
139 

144 
DAWSON, QIAN AND SAMARATI 
The architecture proposed in this paper provides the basis for secure interoperation and 
could be extended in several directions. The mediator and the wrappers can be enhanced 
to consider non-mandatory policies. For instance, discretionary policies, possibly group-
based rather than identity-based, could be considered. The access control modules could 
be enhanced to support management of credentials. In such contexts administrative issues 
regulating the specification and enforcement of access restrictions at the global level will 
need to be investigated. The problem of managing conflicting and redundant information, 
or access restrictions, from different sources should also be investigated. 
Acknowledgments 
This work was performed while all the authors were with SRI International and was sup-
ported in part by National Science Foundation under grant ECS-94-22688 and by DARPA! 
Rome Laboratory under contracts F30602-94-C-0198 and F30602-96-C-0337. 
References 
1. D.E. Bell and L.1. LaPadula, "Secure computer systems: Unified exposition and multies interpretation;' 
Technical Report, The Mitre Corp., 1974. 
2. J. Biskup, U. Flegel, and Y. Karabulut, "Secure mediation: Requirements and design," in Database Security 
XII: Status and Prospects, Sushil lajodia (Ed.), Kluwer, 1999. 
3. P. Bonatti, M.L. Sapino, and VS. Subrahmanian, "Merging heterogeneous security orderings," in Proc. 4th 
European Symp. on Research in Computer Security (ESORICS 96), Rome, Italy, September 1996. 
4. KS. Candan, S. lajodia, and VS. Subrahmanian, "Secure mediated databases," in Proc. 12th International 
Conference on Data Engineering (lCDE '96) New Orleans, Lousiana, February 1996. 
5. S. Dawson, "Optimization techniques for trusted semantic interoperation," Technical Report, SRI Interna-
tional, November 1997. 
6. S. Dawson, J. Gryz, and X. Qian, "Query folding with functional dependencies," Technical Report, SRI 
International, 1996. 
7. S. Dawson and X. Qian, "Query mediation for trusted database interoperation," in Proc. 1997 DoD Database 
Colloquium, San Diego, CA, September 1997. 
8. S. Dawson, S. Qian, and P. Samarati, "Secure interoperation of heterogeneous systems: A mediator-based 
approach," in Proc. of the IFIP 14th International Conference on Information Security (SEC'98), Vienna-
Budapest, 31 August-2 September, 1998. 
9. D.E. Denning, T.P. Lunt, R. Schell, M. Heckman, and S. Shockley, "Secure distributed data view (SeaView)-
the SeaView formal security policy model," Technical Report, SRI International, July 1987. 
10. S. De Capitani di Vimercati and P. Samarati, "Authorization specification and enforcement in federated 
database systems," Journal of Computer Security, vol. 5, no. 2, pp. 155-188, 1997. 
II. L. Gong and X. Qian, "Computational issues in secure interoperation," IEEE Transactions on Software 
Engineering, vol. 22, no. 1, pp. 43-52, January 1996. 
12. D. Jonscher and KR. Dittrich, "An approach for building secure database federations," in Proc. 20th VLDB 
Conference, Santiago, Chile, 1994. 
13. D. 10nscher and KR. Dittrich. "Argos-A configurable access control subsystem which can propagate access 
rights," in Proc. 9th IFIP Working Conference on Database Security, Rensselaerville, New York, August 1995. 
14. A.Y. Levy, A. Rajaraman, and J.1. Ordille, "Querying heterogeneous information sources using source de-
scriptions," in Proc. of the 22nd International Conference on Very Large Databases (VLDB'96), Mumbay, 
India, September 1996, pp. 25J-262. 
IS. M. Morgenstern, T.P. Lunt, B. Thuraisingham, and D.L. Spooner. Security issues in federated database 
systems: Panel contributions, in Database Security, V: Status and Prospects, C. E. Landwehr and S. Jajodia 
(Eds.), IFIP, Shepherds Town, West Virginia, 1992, pp. 131-148. 
140 

SECURITY AND INTEROPERA TION OF HETEROGENEOUS SYSTEMS 
145 
16. M.S. Olivier, "A multilevel secure federated database," in Proc. 9th IFIP Working Conference on Database 
Security, Rensselaerville, New York, August 1995, pp. 23-38. 
17. Y. Papakostantantinou, S. Abiteboul, and H. Garcia-Molina, "Object fusion in mediator systems," in Proc. 
22nd International Conference on Very Large Databases (VLDB'96), Murnbay, India, September 1996. 
18. X. Qian, "Query folding," in Proc. Twelfth International Conference on Data Engineering, 1996, pp. 48-55. 
19. X. Qian and T. Lunt, "Semantic interoperation: A query mediation approach:' Technical Report TR 94-02, 
SRI International, 1994. 
20. A.P. Sheth and J.A. Larson, "Federated database systems for managing distributed, heterogeneous, and auto-
nomous databases," ACM Computing Surveys, vol. 22, no. 3, 1990, pp. 183-236. 
21. B. Thuraisingham and H.H. Rubinovitz, "Multilevel security issues in distributed database management 
systems III," Computers & Security, vol. 11, pp. 661-674, 1992. 
22. c.Y Wang and D.L. Spooner, "Access control in a heterogeneous distributed database management system," in 
IEEE 6th Symp. un Reliability in Distributed Software and Database Systems, Williamsburg, 1987, pp. 84-92. 
23. G. Wiederhold, "Mediators in the architecture offuture information systems;' IEEE Computer, vol. 25, no. 3, 
March 1992, pp. 38--49. 
24. G. Wiederhold, M. Bilello, and C. Donahue, "Web implementation of a security mediator for medical 
databases," in Database Security XI: Status and Prospects, T.Y Lin and S. Qian (Eds.), Chapman & Hall, 
1998, pp. 60--72. 
25. G. Wiederhold, M. Bilello, V. Sarathy, and X. Qian, "A security mediator for health care information," in 
Proc. 1996 AMIA Conference, Journal of the AMIA, Washington, DC, October 1998, pp. 120-124. 
26. M. Winslett, N. Ching, V. Jones, and Slepchin, "Using digital credentials on the world wide web," Journal of 
Computer Security, vol. 5, no. 3, pp. 255-267, 1997. 
141 


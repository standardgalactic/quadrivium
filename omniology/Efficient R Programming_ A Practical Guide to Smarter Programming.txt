Eﬃcient R programming
Colin Gillespie and Robin Lovelace
2016-06-03
www.allitebooks.com

2
www.allitebooks.com

Contents
Welcome to Eﬃcient R Programming
7
Package Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
Preface
9
1
Introduction
11
1.1
Who this book is for . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.2
What is eﬃciency? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.3
Why eﬃciency? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.4
What is eﬃcient R programming?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.5
Touch typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.6
Benchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.7
Proﬁling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2
Eﬃcient set-up
17
2.1
Top 5 tips for an eﬃcient R set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Operating system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.3
R version
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.4
R startup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.5
RStudio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.6
BLAS and alternative R interpreters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3
Eﬃcient programming
39
3.1
General advice
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.2
Communicating with the user . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.3
Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.4
S3 objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.5
Caching variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.6
The byte compiler
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3
www.allitebooks.com

4
CONTENTS
4
Eﬃcient workﬂow
57
4.1
Project planning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4.2
Package selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.3
Importing data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.4
Tidying data with tidyr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.5
Data processing with dplyr
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.6
Data processing with data.table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.7
Publication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
5
Eﬃcient data carpentry
81
6
Eﬃcient visualisation
83
6.1
Rough outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
6.2
Cairo type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
7
Eﬃcient performance
85
7.1
Eﬃcient base R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
7.2
Code proﬁling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
7.3
Parallel computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
7.4
Rcpp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
8
Eﬃcient hardware
103
8.1
Top 5 tips for eﬃcient hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
8.2
Background: what is a byte?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
8.3
Random access memory: RAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
8.4
Hard drives: HDD vs SSD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
8.5
Operating systems: 32-bit or 64-bit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
8.6
Central processing unit (CPU)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
8.7
Cloud computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
9
Eﬃcient Collaboration
111
9.1
Coding style . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
9.2
Version control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
9.3
Refactoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
www.allitebooks.com

CONTENTS
5
10 Eﬃcient Learning
117
10.1 Using R Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
10.2 Reading R source code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
10.3 Learning online . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
10.4 Online resources
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
10.5 Conferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
10.6 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
10.7 Look at the source code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
www.allitebooks.com

6
CONTENTS
www.allitebooks.com

Welcome to Eﬃcient R Programming
This is the online home of the O’Reilly book: Eﬃcient R programming. Pull requests and general comments
are welcome.
To build the book:
1. Install the latest version of R
• If you are using RStudio, make sure that’s up-to-date as well
2. Install the book dependencies.
devtools::install_github("csgillespie/efficientR")
3. Clone the eﬃcientR repo
4. If you are using RStudio, open index.Rmd and click Knit.
• Alternatively, use the bundled Makefile
Package Dependencies
The book depends on the following packages:
7
www.allitebooks.com

8
CONTENTS
Name
Title
assertive.reﬂection
Assertions for Checking the State of R
benchmarkme
Crowd Sourced System Benchmarks
bookdown
Authoring Books with R Markdown
cranlogs
Download Logs from the ’RStudio’ ’CRAN’ Mirror
data.table
Extension of Data.frame
devtools
Tools to Make Developing R Packages Easier
DiagrammeR
Create Graph Diagrams and Flowcharts Using R
dplyr
A Grammar of Data Manipulation
drat
Drat R Archive Template
eﬃcient
Becoming an Eﬃcient R Programmer
formatR
Format R Code Automatically
fortunes
R Fortunes
geosphere
Spherical Trigonometry
ggplot2
An Implementation of the Grammar of Graphics
ggplot2movies
Movies Data
knitr
A General-Purpose Package for Dynamic Report Generation in R
lubridate
Make Dealing with Dates a Little Easier
microbenchmark
Accurate Timing Functions
profvis
Interactive Visualizations for Proﬁling R Code
pryr
Tools for Computing on the Language
readr
Read Tabular Data
tidyr
Easily Tidy Data with ‘spread()‘ and ‘gather()‘ Functions
www.allitebooks.com

Preface
Eﬃcient R Programming is about increasing the amount of work you can do with R in a given amount of
time. It’s about both computational and programmer eﬃciency. There are many excellent R resources about
topic areas such as visualisation (e.g. Chang 2012), data science (e.g. Grolemund and Wickham 2016) and
package development (e.g. Wickham 2015). There are even more resources on how to use R in particular
domains, including Bayesian Statistics, Machine Learning and Geographic Information Systems. However,
there are very few uniﬁed resources on how to simply make R work eﬀectively. Hints, tips and decades of
community knowledge on the subject are scattered across hundreds of internet pages, email threads and
discussion forums, making it challenging for R users to understand how to write eﬃcient code.
In our teaching we have found that this issue applies to beginners and experienced users alike. Whether it’s
a question of understanding how to use R’s vector objects to avoid for loops, knowing how to set-up your
.Rprofile and .Renviron ﬁles or the ability to harness R’s excellent C++ interface to do the ‘heavy lifting’,
the concept of eﬃciency is key. The book aims to distill tips, warnings and ‘tricks of the trade’ down into a
single, cohesive whole that will provide a useful resource to R programmers of all stripes for years to come.
The content of the book reﬂects the questions that our students, from a range of disciplines, skill levels and
industries, have asked over the years to make their R work faster. How to set-up my system optimally for R
programming work? How can one apply general principles from Computer Science (such as do not repeat
yourself, DRY) to the speciﬁcs of an R script? How can R code be incorporated into an eﬃcient workﬂow,
including project inception, collaboration and write-up? And how can one learn quickly how to use new
packages and functions?
The book answers each of these questions, and more, in 10 self-contained chapters. Each chapter starts simple
and gets progressively more advanced, so there is something for everyone in each. While the more advanced
topics such as parallel programming and C++ may not be immediately relevant to R beginners, the book
helps to navigate R’s famously steep learning curve with a commitment to starting slow and building on
strong foundations. Thus even experienced R users are likely to ﬁnd previously hidden gems of advice in the
early parts of the chapters. “Why did no one tell me that before?” is a common exclamation we have heard
while teaching this material.
Eﬃcient programming should not be seen as an optional extra and the importance of eﬃciency grows with
the size of projects and datasets. In fact, this book was devised while we were teaching a course on ‘R for
Big Data’: it quickly became apparent that if you want to work with large datasets, your code must work
eﬃciently. Even if you work with small datasets, eﬃcient code, that is both fast to write and run is a vital
component of successful R projects. We found that the concept of eﬃcient programming is important to
all branches of the R community. Whether you are a sporadic user of R (e.g. for its unbeatable range of
statistical packages), looking to develop a package, or working on a large collaborative project in which
eﬃciency is mission-critical, code eﬃciency will have a major impact on your productivity.
Ultimately eﬃciency is about getting more output for less work input. To take the analogy of a car, would
you rather drive 1000 km on a single tank (or a single charge of your batteries) or refuel a heavy, clunky and
ugly car every 50 km? In the same way, eﬃcient R code is better than ineﬃcient R code in almost every way:
it is easier to read, write, run, share and maintain. This book cannot provide all the answers about how to
produce such code but it certainly can provide ideas, example code and tips to make a start in the right
direction of travel.
9
www.allitebooks.com

10
CONTENTS
www.allitebooks.com

Chapter 1
Introduction
1.1
Who this book is for
This book is for anyone who wants to make their R code faster to type, faster to run and more scalable.
These considerations generally come after learning the very basics of R for data analysis: we assume you are
either accustomed to R or proﬁcient at programming in other languages, although the book could still be of
use for beginners. Thus the book should be of use to three groups, albeit in diﬀerent ways:
• For programmers with little R knowledge this book will help you navigate the quirks of R to
make it work eﬃciently: it is easy to write slow R code if you treat as if were another language.
• For R users who have little experience of programming this book will show you many concepts
and ‘tricks of the trade’, some of which are borrowed from Computer Science, that will make your work
more time eﬀective.
• A R beginner, you should probably read this book in parallel with other R resources such as the
numerous, vignettes, tutorials and online articles that the R community has produced. At a bare
minimum you should have R installed on your computer (see section 2.3 for information on how best to
install R on new computers).
1.2
What is eﬃciency?
In everyday life eﬃciency roughly means ‘working well’. An eﬃcient vehicle goes far without guzzling gas. An
eﬃcient worker gets the job done fast without stress. And an eﬃcient light shines bright with a minimum of
energy consumption. In this ﬁnal sense, eﬃciency (η) has a formal deﬁnition as the ratio of work done (W
e.g. light output) over eﬀort (Q energy consumption):
η = W
Q
In the context of computer programming eﬃciency can be deﬁned narrowly or broadly. The narrow sense,
algorithmic eﬃciency refers to the way a particular task is undertaken. This concept dates back to the very
origins of computing, as illustrated by the following quote by Lovelace (1842) in her notes on the work of
Charles Babbage, one of the pioneers of early computing:
In almost every computation a great variety of arrangements for the succession of the processes is
possible, and various considerations must inﬂuence the selections amongst them for the purposes
11

12
CHAPTER 1. INTRODUCTION
of a calculating engine. One essential object is to choose that arrangement which shall tend to
reduce to a minimum the time necessary for completing the calculation.
The issue of having a ‘great variety’ of ways to solve a problem has not gone away with the invention of
advanced computer languages: R is notorious for allowing users to solve problems in many ways, and this
notoriety has only grown with the proliferation of community contributed package. In this book we want to
focus on the best way of solving problems, from an eﬃciency perspective.
The second, broader deﬁnition of eﬃcient computing is productivity. This is the amount of useful work a
person (not a computer) can do per unit time. It may be possible to rewrite your codebase in C to make it
100 times faster. But if this takes 100 human hours it may not be worth it. Computers can chug away day
and night. People cannot. Human productivity the subject of Chapter 4.
By the end of this book you should know how to write R code that is eﬃcient from both algorithmic and
productivity perspectives. Eﬃcient code is also concise, elegant and easy to maintain, vital when working on
large projects.
1.3
Why eﬃciency?
Computers are always getting more powerful. Does this not reduce the need for eﬃcient computing? The
answer is simple: in an age of Big Data and stagnating computer clockspeeds (see Chapter 8), computational
bottlenecks are more likely than ever before to hamper your work. An eﬃcient programmer can “solve more
complex tasks, ask more ambitious questions, and include more sophisticated analyses in their research”
(Visser et al. 2015).
A concrete example illustrates the importance of eﬃciency in mission critical situations. Robin was working
on a tight contract for the UK’s Department for Transport, to build the Propensity to Cycle Tool, an online
application which had to be ready for national deployment in less than 4 months. To help his workﬂow he
developed a function, line2route() in the stplanr to batch process calls to the (cyclestreets.net) API. But
after a few thousand routes the code slowed to a standstill. Yet hundreds of thousands were needed. This
endangered the contract. After eliminating internet connection issues, it was found that the slowdown was
due to a bug in line2route(): it suﬀered from the ‘vector growing problem’, discussed in Section 3.1.1.
The solution was simple. A single commit made line2route() more than ten times faster and substantially
shorter. This potentially saved the project from failure. The moral of this story is that eﬃcient programming
is not merely a desirable skill: it can be essential.
1.4
What is eﬃcient R programming?
Eﬃcient R programming is the implementation of eﬃcient programming practices in R. All languages are
diﬀerent, so eﬃcient R code does not look like eﬃcient code in another language. Many packages have been
optimised for performance so, for some operations, acheiving maximum computational eﬃciency may simply
be a case of selecting the appropriate package and using it correctly. There are many ways to get the same
result in R, and some are very slow. Therefore not writing slow code should be prioritized over writing fast
code.
Returning to the analogy of the two cars sketched in the preface, eﬃcient R programming for some use cases
can simply mean trading in your heavy and gas guzzling hummer for a normal hatchback. The search for
optimal performance often has diminishing returns so it is important to ﬁnd bottlenecks in your code to
prioritise work for maximum increases in computational eﬃcency.

1.5. TOUCH TYPING
13
1.5
Touch typing
The other side of the eﬃciency coin is programmer eﬃciency. There are many things that will help increase
the productivity of yourself and your collaborators, not least following the advice of Janert (2010) to ‘think
more work less’. The evidence suggests that good diet, physical activity, plenty of sleep and a healthy work-life
balance can all boost your speed and eﬀectiveness at work (Jensen 2011; Pereira et al. 2015; Grant, Wallace,
and Spurgeon 2013).
While we recommend the reader to reﬂect on this evidence and their own well-being, this is not a self help
book. It is about programming. However, there is one non-programming skill that can have a huge impact
on productivity: touch typing. This skill can be relatively painless to learn, and can have a huge impact on
your ability to write, modify and test R code quickly. Learning to touch type properly will pay oﬀin small
increments throughout the rest of your programming life (of course, the beneﬁts are not constrained to R
programming).
The key diﬀerence between a touch typist and someone who constantly looks back at the keyboard, or who
uses only two or three ﬁngers for all letters is hand placement. Touch typing involves positioning your hands
on the keyboard with each ﬁnger of both hands touching or hovering over a speciﬁc letter (Figure 1.1). This
takes time and some discipline to learn. Fortunately there are many resources that will help you get in the
habit of touch typing early, including open source software projects Klavaro and TypeFaster.
Figure 1.1: The starting position for touch typing, with the ﬁngers over the ‘home keys’. Source: Wikipedia
under the Creative Commons license.
1.6
Benchmarking
Benchmarking is the process of testing the performance of speciﬁc operations repeatedly. Modifying things
from one benchmark to the next and recording the results after changing things allows experimentation to

14
CHAPTER 1. INTRODUCTION
see which bits of code are fastest. Benchmarking is important in the eﬃcient programmer’s toolkit: you may
think that your code is faster than mine but benchmarking allows you to prove it.
* `system.time()`
* `microbenchmark` and `rbenchmark`
The microbenchmark package runs a test many times (by default 1000), enabling the user to detect
microsecond diﬀerence in code performance.
1.6.1
Benchmarking example
A good example is testing diﬀerent methods to look-up an element of a dataframe.
library("microbenchmark")
df = data.frame(v = 1:4, name = c(letters[1:4]))
microbenchmark(
df[3, 2],
df$name[3],
df[3, 'v']
)
#> Unit: microseconds
#>
expr
min
lq mean median
uq
max neval
#>
df[3, 2] 22.8 24.2 27.6
24.8 25.5 201.3
100
#>
df$name[3] 16.2 17.6 19.9
18.9 19.7
62.4
100
#>
df[3, "v"] 14.8 15.9 17.0
16.5 17.0
30.0
100
The results show that seemingly arbitrary changes to how R code is written can aﬀect the eﬃciency of
computation. Without benchmarking, these diﬀerences would be very hard to detect.
1.7
Proﬁling
Benchmarking generally tests execution time of one function against another. Proﬁling, on the other hand, is
about testing large chunks of code.
It is diﬃcult to over-emphasise the importance of proﬁling for eﬃcient R programming. Without a proﬁle of
what took longest, you will have only a vague idea of why your code is taking so long to run. The example
below (which generates Figure 1.3 an image of ice-sheet retreat from 1985 to 2015) shows how proﬁling can
be used to identify bottlenecks in your R scripts:
library("profvis")
profvis(expr = {
# Stage 1: load packages
library("rnoaa")
library("ggplot2")
# Stage 2: load and process data
out = readRDS("data/out-ice.Rds")
df = dplyr::rbind_all(out, id = "Year")
# Stage 3: visualise output

1.7. PROFILING
15
ggplot(df, aes(long, lat, group = paste(group, Year))) +
geom_path(aes(colour = Year))
ggsave("figures/icesheet-test.png")
}, interval = 0.01, prof_output = "ice-prof")
The result of this proﬁling exercise are displayed in Figure 1.2.
Figure 1.2: Proﬁling results of loading and plotting NASA data on icesheet retreat.

16
CHAPTER 1. INTRODUCTION
Figure 1.3: Visualisation of North Pole icesheet decline, generated using the code proﬁled using the profvis
package.

Chapter 2
Eﬃcient set-up
An eﬃcient computer set-up is analogous to a well-tuned vehicle: its components work in harmony, it is
well-serviced, and it is fast. This chapter describes the software decisions that will enable a productive
workﬂow. Starting with the basics and moving to progressively more advanced topics, we explore how the
operating system, R version, startup ﬁles and IDE can make your R work faster (though IDE could be seen
as basic need for eﬃcient programming). Ensuring correct conﬁguration of these elements will have knock-on
beneﬁts in many aspects of your R workﬂow. That’s why we cover them at this early stage (hardware, the
other fundamental consideration, is covered in the next chapter). By the end of this chapter you should
understand how to set-up your computer and R installation (skip to section 2.3 if R is not already installed
on your computer) for optimal computational and programmer eﬃciency. It covers the following topics:
• R and the operating systems: system monitoring on Linux, Mac and Windows
• R version: how to keep your base R installation and packages up-to-date
• R start-up: how and why to adjust your .Rprofile and .Renviron ﬁles
• RStudio: an integrated development environment (IDE) to boost your programming productivity
• BLAS and alternative R interpreters: looks at ways to make R faster
For lazy readers, and to provide a taster of what’s to come, we begin with our ‘top 5’ tips for an eﬃcient R
set-up. It is important to understand that eﬃcient programming is not simply the result of following a recipe
of tips: understanding is vital for knowing when to use a memorised solution to a problem and when to go
back to ﬁrst principles. Thinking about and understanding R in depth, e.g. by reading this chapter carefully,
will make eﬃciency second nature in your R workﬂow.
2.1
Top 5 tips for an eﬃcient R set-up
• Use system monitoring to identify bottlenecks in your hardware/code
• Keep your R installation and packages up-to-date
• Make use of RStudio’s powerful autocompletion capabilities and shortcuts
• Store API keys in the .Renviron ﬁle
• Use BLAS if your R number crunching is too slow
2.2
Operating system
R works on all three consumer operating systems (OS) (Linux, Mac and Windows) as well as the server-
orientated Solaris OS. R is predominantly platform-independent, meaning that it should behave in the same
17

18
CHAPTER 2. EFFICIENT SET-UP
way on each of these platforms. This is partly facilitated by CRAN tests which ensure that R packages work
on all OSs mentioned above. There are some operating system-speciﬁc quirks that may inﬂuence the choice
of OS and how it is set-up for R programming in the long-term. Basic system information can be queried
from within R using Sys.info(), as illustrated below for a selection its output:
Sys.info()
##
sysname
##
"Linux"
##
release
##
"4.2.0-35-generic"
##
machine
##
"x86_64"
##
user
##
"robin"
Translated into English, this means that R is running on a 64 bit (x86_64) Linux distribution (kernel version
4.2.0-35-generic) and that the current user is robin. Four other pieces of information (not shown) are
also produced by the command, the meaning of which is well documented in ?Sys.info.
Pro tip. The assertive.reﬂection package can be used to report additional information about
your computer’s operating system and R set-up with functions for asserting operating system and
other system characteristics. The assert_* functions work by testing the truth of the statement
and erroring if the statement is untrue. On a Linux system assert_is_linux() will run silently,
whereas assert_is_solaris will cause an error. The package can also test for IDE you are using
(e.g. assert_is_rstudio()), the capabilities of R (assert_r_has_libcurl_capability etc.),
and what OS tools are available (e.g. assert_r_can_compile_code). These functions can be
useful for running code that designed only to run on one type of set-up.
2.2.1
Operating system and resource monitoring
Minor diﬀerences aside,1 R’s computational eﬃciency is broadly the same across diﬀerent operating systems.
This is important as it means the techniques will, in general, work equally well on diﬀerent OSs. Beyond the
32 vs 64 bit issue (covered in the next chapter) and process forking (covered in Chapter 6) the main issue
for many will be user friendliness and compatibility other programs used alongside R for work. Changing
operating system can be a time consuming process so our advice is usually to stick to whatever OS you are
most comfortable with.
Some packages (e.g. those that must be compiled and that depend on external libraries) are best installed at
the operating system level (i.e. not using install.packages) on Linux systems. On Debian-based operating
systems such as Ubuntu, these are named with the preﬁx r-cran- (see Section 2.4).
Regardless of your operating system, it is good practice to track how system resources (primarily CPU
and RAM use) respond when running time-consuming or RAM-intensive tasks. If you only process small
datasets, system monitoring may not be necessary but when handling datasets at the limits of your computer’s
resources, it can be a useful tool for identifying bottlenecks, such as when you are running low on RAM.
1Benchmarking conducted for a presentation “R on Diﬀerent Platforms” at useR 2006 found that R was marginally faster
on Windows than Linux set-ups. Similar results were reported in an academic paper, with R completing statistical analyses
faster on a Linux than Mac OS’s (Sekhon 2006). In 2015 Revolution R supported these results with slightly faster run times for
certain benchmarks on Ubuntu than Mac systems. The data from the benchmarkme package also suggests that running code
under the Linux OS is faster.

2.2. OPERATING SYSTEM
19
Alongside R proﬁling functions such as profvis (see Section XXX), system monitoring can help identify
performance bottlenecks and opportunities for making tasks run faster.
A common use case for system monitoring of R processes is to identify how much RAM is being used and
whether more is needed (covered in Chapter 3). System monitors also report the percentage of CPU resource
allocated over time. On modern multi-threaded CPUs, many tasks will use only a fraction of the available
CPU resource because R is by default a single-threaded program (see Chapter 6 on parallel programming).
Monitoring CPU load in this context can be useful for identifying whether R is running in parallel (see Figure
2.1).
Figure 2.1: Output from a system monitor (gnome-system-monitor running on Ubuntu) showing the
resources consumed by running the code presented in the second of the Exercises at the end of this section.
The ﬁrst increases RAM use, the second is single-threaded and the third is multi-threaded.
System monitoring is a complex topic that spills over into system administration and server management.
Fortunately there are many tools designed to ease monitoring all major operating systems.
• On Linux, the shell command top displays key resource use ﬁgures for most distributions. htop and
Gnome’s System Monitor (gnome-system-monitor, see Figure 2.1) are more reﬁned alternatives
which use command-line and graphical user interfaces respectively. A number of options such as nethogs
monitor internet usage.
• On Windows the Task Manager provides key information on RAM and CPU use by process. This
can be started in modern Windows versions by typing Ctrl-Alt-Del or by clicking the task bar and
‘Start Task Manager’.
• On Mac the Activity Monitor provides similar functionality. This can be initiated form the Utilities
folder in Launchpad.
Exercises
1. What is the exact version of your computer’s operating system?
2. Start an activity monitor then type and execute the following code. How do the results on your system
compare to those presented in Figure 2-1?
r
# 1: Create large dataset
X = data.frame(matrix(rnorm(1e8), nrow = 1e7))
#
2: Find the median of each column using a single core
r1 = lapply(X, median)
# 3:
Find the median of each column using many cores
r2 = parallel::mclapply(X, median) #
runs in serial on Windows
3. What do you notice regarding CPU usage, RAM and system time, during and after each of the three
operations?

20
CHAPTER 2. EFFICIENT SET-UP
4. Bonus question: how would the results change depending on operating system?
2.3
R version
It is important to be aware that R is an evolving software project, whose behaviour changes over time. This
applies to an even greater extent to packages, which occassionally change substantially from one release to
the next. For most use cases it we recommend always using the most up-to-date version of R and packages,
so you have the latest code. In some circumstances (e.g. on a production server) you may alternatively want
to use speciﬁc versions which have been tested, to ensure stability. Keeping packages up-to-date is desirable
because new code tends to be more eﬃcient, intuitive, robust and feature rich. This section explains how.
Previous R versions can be installed from CRAN’s archive or previous R releases. The binary versions
for all OSs can be found at cran.r-project.org/bin/. To download binary versions for Ubuntu ‘Wily’, for
example, see cran.r-project.org/bin/linux/ubuntu/wily/. To ‘pin’ speciﬁc versions of R packages you can
use the packrat package. For more on pinning R versions and R packages see articles on RStudio’s website
Using-Diﬀerent-Versions-of-R and rstudio.github.io/packrat/.
2.3.1
Installing R
The method of installing R varies for Windows, Linux and Mac.
On Windows, a single .exe ﬁle (hosted at cran.r-project.org/bin/windows/base/) will install the base R
package.
On a Mac, the latest version should be installed by downloading the .pkg ﬁles hosted at cran.r-
project.org/bin/macosx/.
On Debian-based systems adding the CRAN repository in the format. The following bash command will add
the repository to /etc/apt/sources.list and keep your operating system updated with the latest version
of R:
apt-add-repository https://cran.rstudio.com/bin/linux/ubuntu
In the above code cran.rstudio.com is the ‘mirror’ from which r-base and other r- packages can be
installed using the apt system. The following two commands, for example, would install the base R package
(a ‘barebones’ install) and the package rcurl, which has an external dependency:
apt-get install r-cran-base # install base R
apt-get isntall r-cran-rcurl # install the rcurl package
R also works on FreeBSD and other Unix-based systems.2
Once R is installed it should be kept up-to-date.
2.3.2
Updating R
R is a mature and stable language so well-written code in base R should work on most versions. However, it
is important to keep your R version relatively up-to-date, because:
• Bug ﬁxes are introduced in each version, making errors less likely;
2See jason-french.com/blog/2013/03/11/installing-r-in-linux/ for more information on installing R on a variety of Linux
distributions.

2.3. R VERSION
21
• Performance enhancements are made from one version to the next, meaning your code may run faster
in later versions;
• Many R packages only work on recent versions on R.
Release notes with details on each of these issues are hosted at cran.r-project.org/src/base/NEWS. R release
versions have 3 components corresponding to major.minor.patch changes. Generally 2 or 3 patches are
released before the next minor increment - each ‘patch’ is released roughly every 3 months. R 3.2, for example,
has consisted of 3 versions: 3.2.0, 3.2.1 and 3.2.2.
• On Ubuntu-based systems, new versions of R should be automatically detected through the software
management system, and can be installed with apt-get upgrade.
• On Mac, the latest version should be installed by the user from the .pkg ﬁles mentioned above.
• On Windows installr package makes updating easy:
# check and install the latest R version
installr::updateR()
For information about changes to expect in the next version, you can subscribe to the R’s NEWS RSS feed:
developer.r-project.org/blosxom.cgi/R-devel/NEWS/index.rss. It’s a good way of keeping up-to-date.
2.3.3
Installing R packages
Large projects may need several packages to be installed. In this case, the required packages can be installed
at once. Using the example of packages for handling spatial data, this can be done quickly and concisely with
the following code:
pkgs = c("raster", "leaflet", "rgeos") # package names
install.packages(pkgs)
In the above code all the required packages are installed with two not three lines, reducing typing. Note that
we can now re-use the pkgs object to load them all:
inst = lapply(pkgs, library, character.only = TRUE) # load them
In the above code library(pkg[i]) is executed for every package stored in the text string vector. We use
library here instead of require because the former produces an error if the package is not available.
Loading all packages at the beginning of a script is good practice as it ensures all dependencies have been
installed before time is spent executing code. Storing package names in a character vector object such as
pkgs is also useful because it allows us to refer back to them again and again.
2.3.4
Installing R packages with dependencies
Some packages have external dependencies (i.e. they call libraries outside R). On Unix-like systems, these
are best installed onto the operating system, bypassing install.packages. This will ensure the necessary
dependencies are installed and setup correctly alongside the R package. On Debian-based distributions such
as Ubuntu, for example, packages with names starting with r-cran- can be search for and installed as follows
(see cran.r-project.org/bin/linux/ubuntu/ for a list of these):

22
CHAPTER 2. EFFICIENT SET-UP
apt-cache search r-cran- # search for available cran Debian packages
sudo apt-get-install r-cran-rgdal # install the rgdal package (with dependencies)
On Windows the installr package helps manage and update R packages with system-level dependencies. For
example the Rtools package for compiling C/C++ code on Window can be installed with the following
command:
installr::install.rtools()
2.3.5
Updating R packages
An eﬃcient R set-up will contain up-to-date packages. This can be done for all packages with:
update.packages() # update installed CRAN packages
The default for this function is for the ask argument to be set to TRUE, giving control over what is downloaded
onto your system. This is generally desirable as updating dozens of large packages can consume a large
proportion of available system resources.
To update packages automatically, you can add the line update.packages(ask = FALSE) to your
.Rprofile startup ﬁle (see the next section for more on .Rprofile). Thanks to Richard Cotton
for this tip.
An even more interactive method for updating packages in R is provided by RStudio via Tools > Check for
Package Updates. Many such time saving tricks are enabled by RStudio, as described in a subsequent section.
Next (after the exercises) we take a look at how to conﬁgure R using start-up ﬁles.
Exercises
1. What version of R are you using? Is it the most up-to-date?
2. Do any of your packages need updating?
2.4
R startup
Every time R starts a number of things happen. It can be useful to understand this startup process, so you
can make R work the way you want it, fast. This section explains how.
2.4.1
R startup arguments
The arguments passed to the R startup command (typically simply R from a shell environment) determine
what happens. The following arguments are particularly important from an eﬃciency perspective:
• --no-environ tells R to only look for startup ﬁles in the current working directory. (Do not worry if
you don’t understand what this means at present: it will become clear as the later in the section.)

2.4. R STARTUP
23
• --no-restore tells R not to load any .RData ﬁles knocking around in the current working directory.
• --no-save tells R not to ask the user if they want to save objects saved in RAM when the session is
ended with q().
Adding each of these will make R load slightly faster, and mean that slightly less user input is needed when
you quit. R’s default setting of loading data from the last session automatically is potentially problematic in
this context. See An Introduction to R, Appendix B, for more startup arguments.
Some of R’s startup arguments can be controlled interactively in RStudio. See the online help ﬁle
Customizing RStudio for more on this.
2.4.2
An overview of R’s startup ﬁles
There are two special ﬁles, .Renviron and .Rprofile, which determine how R performs for the duration of
the session. These are summarised in the bullet points below we go into more detail on each in the subsequent
sections.
• The primary purpose of .Renviron is to set environment variables. These are settings that relate to the
operating system for telling where to ﬁnd external programs and the contents of user-speciﬁc variables
that other users should not have access to such as API key, small text strings used to verify the user
when interacting web services.
• .Rprofile is a plain text ﬁle (which is always called .Rprofile, hence its name) that simply runs
lines of R code every time R starts. If you want R to check for package updates each time it starts (as
explained in the previous section), you simply add the relevant line somewhere in this ﬁle.
When R starts (unless it was launched with --no-environ) it ﬁrst searches for .Renviron and then .Rprofile,
in that order. Although .Renviron is searched for ﬁrst, we will look at .Rprofile ﬁrst as it is simpler and
for many set-up tasks more frequently userful. Both ﬁles can exist in three directories on your computer.
2.4.3
The location of startup ﬁles
Confusingly, multiple versions of these ﬁles can exist on the same computer, only one of which will be used
per session. Note also that these ﬁles should only be changed with caution and if you know what you are
doing. This is because they can make your R version behave diﬀerently to other R installations, potentially
reducing the reproducibility of your code.
Files in three folders are important in this process:
• R_HOME, the directory in which R is installed. The etc sub-directory can contain start-up ﬁles read
early on in the start-up process. Find out where your R_HOME is with the R.home() command.
• HOME, the user’s home directory.
Typically this is /home/username on Unix machines or
C:\Users\username on Windows (since Windows 7).
Ask R where your home directory with,
Sys.getenv("HOME").
• R’s current working directory. This is reported by getwd().

24
CHAPTER 2. EFFICIENT SET-UP
It is important to know the location of the .Rprofile and .Renviron set-up ﬁles that are being used out of
these three options. R only uses one .Rprofile and one .Renviron in any session: if you have a .Rprofile
ﬁle in your current project, R will ignore .Rprofile in R_HOME and HOME. Likewise, .Rprofile in HOME
overrides .Rprofile in R_HOME. The same applies to .Renviron: you should remember that adding project
speciﬁc environment variables with .Renviron will de-activate other .Renviron ﬁles.
To create a project-speciﬁc start-up script, simply create a .Rprofile ﬁle in the project’s root directory and
start adding R code, e.g. via file.edit(".Rprofile"). Remember that this will make .Rprofile in the
home directory be ignored. The following commands will open your .Rprofile from within an R editor:
file.edit(file.path("~", ".Rprofile")) # edit .Rprofile in HOME
file.edit(".Rprofile") # edit project specific .Rprofile
Note that editing the .Renviron ﬁle in the same locations will have the same eﬀect. The following code will
create a user speciﬁc .Renviron ﬁle (where API keys and other cross-project environment variables can be
stored), without overwriting any existing ﬁle.
user_renviron = path.expand(file.path("~", ".Renviron"))
if(!file.exists(user_renviron)) # check to see if the file already exists
file.create(user_renviron)
file.edit(user_renviron) # open with another text editor if this fails
The pathological package can help ﬁnd where .Rprofile and .Renviron ﬁles are located on your
system, thanks to the os_path() function. The output of example(startup) is also instructive.
The location, contents and uses of each is outlined in more detail below.
2.4.4
The .Rprofile ﬁle
By default, R looks for and runs .Rprofile ﬁles in the three locations described above, in a speciﬁc order.
.Rprofile ﬁles are simply R scripts that run each time R runs and they can be found within R_HOME, HOME
and the project’s home directory, found with getwd(). To check if you have a site-wide .Rprofile, which
will run for all users on start-up, run:
site_path = R.home(component = "home")
fname = file.path(site_path, "etc", "Rprofile.site")
file.exists(fname)
The above code checks for the presence of Rprofile.site in that directory. As outlined above, the .Rprofile
located in your home directory is user-speciﬁc. Again, we can test whether this ﬁle exists using
file.exists("~/.Rprofile")
We can use R to create and edit .Rprofile (warning: do not overwrite your previous .Rprofile - we suggest
you try project-speciﬁc .Rprofile ﬁrst):

2.4. R STARTUP
25
if(!file.exists("~/.Rprofile")) # only create if not already there
file.create("~/.Rprofile")
# (don't overwrite it)
file.edit("~/.Rprofile")
2.4.5
An example .Rprofile ﬁle
The example below provides a taster of what goes into .Rprofile. Note that this is simply a usual R script,
but with an unusual name. The best way to understand what is going on is to create this same script, save it
as .Rprofile in your current working directory and then restart your R session to observer what changes.
To restart your R session from within RStudio you can click Session > Restart R or use the keyboard
shortcut Ctrl+Shift+F10.
# A fun welcome message
message("Hi Robin, welcome to R")
# Customise the R prompt that prefixes every command
# (use " " for a blank prompt)
options(prompt = "R4geo> ")
# Don't convert text strings to factors with base read functions
options(stringsAsFactors = FALSE)
To quickly explain each line of code: the ﬁrst simply prints a message in the console each time a new R
session is started. The latter two modify options used to change R’s behavior, ﬁrst to change the prompt in
the console (set to R> by default) and second to ensure that unwanted factor variables are not created when
read.csv and other functions derived from read.table are used to load external data into R. Note that
simply adding more lines the .Rprofile will set more features. An important aspect of .Rprofile (and
.Renviron) is that each line is run once and only once for each R session. That means that the options set
within .Rprofile can easily be changed during the session. The following command run mid-session, for
example, will return the default prompt:
options(prompt = "> ")
More details on these, and other potentially useful .Rprofile options are described subsequently. For more
suggestions of useful startup settings, see Examples in help("Startup") and online resources such as those
at statmethods.net. The help pages for R options (accessible with ?options) are also worth a read before
writing you own .Rprofile.
Ever been frustrated by unwanted + symbols that prevent copyied and pasted multi-line functions from
working? These potentially annoying +s can be erradicated by adding options(continue = "
") to your
.Rprofile.
2.4.5.1
Setting options
The function options, used above, contains a number of default settings. Typing options() provides a
good indication of what be conﬁgured. Because options() are often related to personal preference (with
few implications for reproducibility), that you will want for many your R sessions, .Rprofile in your home
directory or in your project’s folder are sensible places to set them. Other illustrative options are shown
below:
options(prompt="R> ", digits=4, show.signif.stars=FALSE)
This changes three default options in a single line.

26
CHAPTER 2. EFFICIENT SET-UP
• The R prompt, from the boring > to the exciting R>.
• The number of digits displayed.
• Removing the stars after signiﬁcant p-values.
Try to avoid adding options to the start-up ﬁle that make your code non-portable. The stringsAsFactors
= FALSE argument used above, for example, to your start-up script has knock-on eﬀects for read.table and
related functions including read.csv, making them convert text strings into characters rather than into
factors as is default. This may be useful for you, but can make your code less portable, so be warned.
2.4.5.2
Setting the CRAN mirror
To avoid setting the CRAN mirror each time you run install.packages you can permanently set the mirror
in your .Rprofile.
# `local` creates a new, empty environment
# This avoids polluting .GlobalEnv with the object r
local({
r = getOption("repos")
r["CRAN"] = "https://cran.rstudio.com/"
options(repos = r)
})
The RStudio mirror is a virtual machine run by Amazon’s EC2 service, and it syncs with the main CRAN
mirror in Austria once per day. Since RStudio is using Amazon’s CloudFront, the repository is automatically
distributed around the world, so no matter where you are in the world, the data doesn’t need to travel very
far, and is therefore fast to download.
2.4.5.3
The fortunes package
This section illustrate what .Rprofile does with reference to a package that was developed for fun. The
code below could easily be altered to automatically connect to a database, or ensure that the latest packages
have been downloaded.
The fortunes package contains a number of memorable quotes that the community has collected over many
years, called R fortunes. Each fortune has a number. To get fortune number 50, for example, enter
fortunes::fortune(50)
#>
#> To paraphrase provocatively, 'machine learning is statistics minus any
#> checking of models and assumptions'.
#>
-- Brian D. Ripley (about the difference between machine learning and
#>
statistics)
#>
useR! 2004, Vienna (May 2004)
It is easy to make R print out one of these nuggets of truth each time you start a session, by adding the
following to ~/.Rprofile:
if(interactive())
try(fortunes::fortune(), silent=TRUE)
The interactive function tests whether R is being used interactively in a terminal. The fortune function
is called within try. If the fortunes package is not available, we avoid raising an error and move on. By
using :: we avoid adding the fortunes package to our list of attached packages.

2.4. R STARTUP
27
Typing search(), gives the list of attached packages. By using fortunes::fortune() we avoid
adding the fortunes package to that list.
The function .Last, if it exists in the .Rprofile, is always run at the end of the session. We can use it
to install the fortunes package if needed. To load the package, we use require, since if the package isn’t
installed, the require function returns FALSE and raises a warning.
.Last = function() {
cond = suppressWarnings(!require(fortunes, quietly=TRUE))
if(cond)
try(install.packages("fortunes"), silent=TRUE)
message("Goodbye at ", date(), "\n")
}
2.4.5.4
Useful functions
You can use .Rprofile deﬁne new ‘helper’ functions or redeﬁne existing ones so they’re faster to type. For
example, we could load the following two functions for examining data frames:
# ht == headtail
ht = function(d, n=6) rbind(head(d, n), tail(d, n))
# Show the first 5 rows & first 5 columns of a data frame
hh = function(d) d[1:5, 1:5]
and a function for setting a nice plotting window:
setnicepar = function(mar = c(3, 3, 2, 1), mgp = c(2, 0.4, 0),
tck = -0.01, cex.axis = 0.9,
las = 1, mfrow = c(1, 1), ...) {
par(mar = mar, mgp = mgp, tck = tck, cex.axis = cex.axis,
las = las, mfrow = mfrow, ...)
}
Note that these functions are for personal use and are unlikely to interfere with code from other people. For
this reason even if you use a certain package every day, we don’t recommend loading it in your .Rprofile.
Shortening long function names for interactive (but not reproducible code writing). If you frequently use
View(), for example, you may be able to save time by referring to it in abbreviated form. This is illustrated
below to make it faster to view datasets (although with IDE-driven autocompletion, outlined in the next
section, the time savings is less.)
v = utils::View
Also beware the dangers of loading many functions by default: it may make your code less portable. Another
potentially useful setting to change in .Rprofile is R’s current working directory.
If you want R to
automatically set the working directory to the R folder of your project, for example, one would add the
following line of code to the project-speciﬁc .Rprofile:

28
CHAPTER 2. EFFICIENT SET-UP
setwd("R")
2.4.5.5
Creating hidden environments with .Rproﬁle
Beyond making your code less portable, another downside of putting functions in your .Rprofile is that it
can clutter-up your work space: when you run the ls() command, your .Rprofile functions will appear.
Also if you run rm(list=ls()), your functions will be deleted. One neat trick to overcome this issue is to
use hidden objects and environments. When an object name starts with ., by default it doesn’t appear in
the output of the ls() function
.obj = 1
".obj" %in% ls()
#> [1] FALSE
This concept also works with environments. In the .Rprofile ﬁle we can create a hidden environment
.env = new.env()
and then add functions to this environment
.env$ht = function(d, n = 6) rbind(head(d, n), tail(d, n))
At the end of the .Rprofile ﬁle, we use attach, which makes it possible to refer to objects in the environment
by their names alone.
attach(.env)
2.4.6
The .Renviron ﬁle
The .Renviron ﬁle is used to store system variables. It follows a similar start-up routine to the .Rprofile
ﬁle: R ﬁrst looks for a global .Renviron ﬁle, then for local versions. A typical use of the .Renviron ﬁle is to
specify the R_LIBS path, which determines where new packages are installed:
# Linux
R_LIBS=~/R/library
# Windows
R_LIBS=C:/R/library
After setting this, install.packages saves packages in the directory speciﬁed by R_LIBS. The location of
this directory can be referred back to subsequently as follows:
Sys.getenv("R_LIBS_USER")
#> [1] "/home/travis/R/Library"
All currently stored environment variables can be seen by calling Sys.getenv() with no arguments. Note
that many environment variables are already pre-set and do not need to be speciﬁed in .Renviron. HOME,
for example, which can be seen with Sys.getenv('HOME'), is taken from the operating system’s list of
environment variables. A list of the most important environment variables that can aﬀect R’s behaviour is
documented in the little known help page help("environment variables").
To set or unset environment variable for the duration of a session, use the following commands:

2.4. R STARTUP
29
Sys.setenv("TEST" = "test-string") # set an environment variable for the session
Sys.unsetenv("TEST") # unset it
Another common use of .Renviron is to store API keys and authentication tokens that will be available from
one session to another.3 A common use case is setting the ‘envvar’ GITHUB_PAT, which will be detected by the
devtools package via the fuction github_pat(). To take another example, the following line in .Renviron
sets the ZEIT_KEY environment variable which is used in the diezeit package:
ZEIT_KEY=PUT_YOUR_KEY_HERE
You will need to sign-in and start a new R session for the environment variable (accessed by Sys.getenv) to
be visible. To test if the example API key has been successfully added as an environment variable, run the
following:
Sys.getenv("ZEIT_KEY")
Use of the .Renviron ﬁle for storing settings such as library paths and API keys is eﬃcient because it reduces
the need to update your settings for every R session. Furthermore, the same .Renviron ﬁle will work across
diﬀerent platforms so keep it stored safely.
2.4.6.1
Example .Renviron ﬁle
My .Renviron ﬁle has grown over the years. I often switch between my desktop and laptop computers, so to
maintain a consistent working environment, I have the same .Renviron ﬁle on all of my machines. As well
as containing an R_LIBS entry and some API keys, my .Renviron has a few other lines:
• TMPDIR=/data/R_tmp/. When R is running, it creates temporary copies. On my work machine, the
default directory is a network drive.
• R_COMPILE_PKGS=3. Byte compile all packages (covered in Chapter 3).
• R_LIBS_SITE=/usr/lib/R/site-library:/usr/lib/R/library I explicitly state where to look for
packages. My University has a site-wide directory that contains out of date packages. I want to avoiding
using this directory.
• R_DEFAULT_PACKAGES=utils,grDevices,graphics,stats,methods. Explicitly state the packages to
load. Note I don’t load the datasets package, but I ensure that methods is always loaded. Due to
historical reasons, the methods package isn’t loaded by default in certain applications, e.g. Rscript.
Exercises
1. What are the three locations where the startup ﬁles are stored? Where are these locations on your
computer?
2. For each location, does a .Rprofile or .Renviron ﬁle exist?
3. Create a .Rprofile ﬁle in your current working directory that prints the message Happy efficient R
programming each time you start R at this location.
4. What happens to the startup ﬁles in R_HOME if you create them in HOME or local project directories?
3See vignette("api-packages") from the httr package for more on this.

30
CHAPTER 2. EFFICIENT SET-UP
2.5
RStudio
RStudio is an Integrated Development Environment (IDE) for R. It makes life easy for R users and developers
with its intuitive and ﬂexible interface. RStudio encourages good programming practice. Through its wide
range of features RStudio can help make you a more eﬃcient and productive R programmer. RStudio can,
for example, greatly reduce the amount of time spent remembering and typing function names thanks to
intelligent autocompletion. Some of the most important features of RStudio include:
• Flexible window pane layouts to optimise use of screen space and enable fast interactive visual feed-back.
• Intelligent auto-completion of function names, packages and R objects.
• A wide range of keyboard shortcuts.
• Visual display of objects, including a searchable data display table.
• Real-time code checking and error detection.
• Menus to install and update packages.
• Project management and integration with version control.
The above list of features should make it clear that a well set-up IDE can be as important as a well set-up R
installation for becoming an eﬃcient R programmer.4 As with R itself, the best way to learn about RStudio
is by using it. It is therefore worth reading through this section in parallel with using RStudio to boost your
productivity.
2.5.1
Installing and updating RStudio
RStudio can be installed from the RStudio website rstudio.com and is available for all major operating systems.
Updating RStudio is simple: click on Help > Check for Updates in the menu. For fast and eﬃcient work
keyboard shortcuts should be used wherever possible, reducing the reliance on the mouse. RStudio has many
keyboard shortcuts that will help with this. To get into good habits early, try accessing the RStudio Update
interface without touching the mouse. On Linux and Windows dropdown menus are activated with the Alt
button, so the menu item can be found with:
Alt+H U
On Mac it works diﬀerently. Cmd+? should activate a search across menu items, allowing the same operation
can be achieved with:
Cmd+? update
Note: in RStudio the keyboard shortcuts diﬀer between Linux and Windows versions on one hand and Mac
on the other. In this section we generally only use the Windows/Linux shortcut keys for brevity. The Mac
equivalent is usually found by simply replacing Ctrl and Alt with the Mac-speciﬁc Cmd button.
2.5.2
Window pane layout
RStudio has four main window ‘panes’ (see Figure 2.2), each of which serves a range of purposes:
• The Source pane, for editing, saving, and dispatching R code to the console (top left). Note that this
pane does not exist by default when you start RStudio: it appears when you open an R script, e.g. via
File -> New File -> R Script. A common task in this pane is to send code on the current line to the
console, via Ctrl-Enter (or Cmd-Enter on Mac).
4Other open source R IDEs exist, including RKWard, Tinn-R and JGR. emacs is another popular software environment.
However, it has a very steep learning curve.
www.allitebooks.com

2.5. RSTUDIO
31
• The Console pane. Any code entered here is processed by R, line by line. This pane is ideal for
interactively testing ideas before saving the ﬁnal results in the Source pane above.
• The Environment pane (top right) contains information about the current objects loaded in the
workspace including their class, dimension (if they are a data frame) and name. This pane also contains
tabbed sub-panes with a searchable history that was dispatched to the console and (if applicable to the
project) Build and Git options.
• The Files pane (bottom right) contains a simple ﬁle browser, a Plots tab, Help and Package tabs and
a Viewer for visualising interactive R output such as those produced by the leaﬂet package and HTML
‘widgets’.
Figure 2.2: RStudio Panels
Using each of the panels eﬀectively and navigating between them quickly is a skill that will develop over time,
and will only improve with practice.
2.5.3
Exercises
You are developing a project to visualise data. Test out the multi-panel RStudio workﬂow by following the
steps below:
1. Create a new folder for the input data using the Files pane.
2. Type in downl in the Source pane and hit Enter to make the function download.file()
autocomplete.
Then type ", which will autocomplete to "", paste the URL of a ﬁle to down-
load (e.g.
https://www.census.gov/2010census/csv/pop_change.csv) and a ﬁle name (e.g.
pop_change.csv).
3. Execute the full command with Ctrl-Enter:

32
CHAPTER 2. EFFICIENT SET-UP
download.file("https://www.census.gov/2010census/csv/pop_change.csv",
"data/pop_change.csv")
4. Write and execute a command to read-in the data, such as
pop_change = read.csv("data/pop_change.csv", skip = 2)
5. Use the Environment pane to click on the data object pop_change. Note that this runs the command
View(pop_change), which launches an interactive data explore pane in the top left panel (see Figure
2-3).
Figure 2.3: The data viewing tab in RStudio.
6. Use the Console to test diﬀerent plot commands to visualise the data, saving the code you want to
keep back into the Source pane, as pop_change.R.
7. Use the Plots tab in the Files pane to scroll through past plots. Save the best using the Export
dropdown button.
The above example shows understanding of these panes and how to use them interactively can help with the
speed and productivity of you R programming. Further, there are a number of RStudio settings that can
help ensure that it works for your needs.
2.5.4
RStudio options
A range of Project Options and Global Options are available in RStudio from the Tools menu (accessible
in Linux and Windows from the keyboard via Alt+T). Most of these are self-explanatory but it is worth
mentioning a few that can boost your programming eﬃciency:
• GIT/SVN project settings allow RStudio to provide a graphical interface to your version control system,
described in Chapter XX.

2.5. RSTUDIO
33
• R version settings allow RStudio to ‘point’ to diﬀerent R versions/interpreters, which may be faster for
some projects.
• Restore .RData: Unticking this default preventing loading previously creating R objects. This will
make starting R quicker and also reduce the change of getting bugs due to previously created objects.
• Code editing options can make RStudio adapt to your coding style, for example, by preventing the
autocompletion of braces, which some experienced programmers may ﬁnd annoying. Enabling Vim
mode makes RStudio act as a (partial) Vim emulator.
• Diagnostic settings can make RStudio more eﬃcient by adding additional diagnostics or by removing
diagnostics if they are slowing down your work. This may be an issue for people using RStudio to
analyse large datasets on older low-spec computers.
• Appearance: if you are struggling to see the source code, changing the default font size may make
you a more eﬃcient programmer by reducing the time overheads associated with squinting at the
screen. Other options in this area relate more to aesthetics, which are also important because feeling
comfortable in your programming environment can boost productivity.
2.5.5
Autocompletion
R provides some basic autocompletion functionality. Typing the beginning of a function name, for example
rn (short for rnorm()), and hitting Tab twice will result in the full function names associated with this text
string being printed. In this case two options would be displayed: rnbinom and rnorm, providing a useful
reminder to the user about what is available. The same applies to ﬁle names enclosed in quote marks: typing
te in the console in a project which contains a ﬁle called test.R should result in the full name "test.R"
being auto-completed. RStudio builds on this functionality and takes it to a new level.
The default settings for autocompletion in RStudio work well. They are intuitive and are likely to
work well for many users, especially beginners. However, RStudio’s autocompletion options can be
modﬁﬁed, but navigating to Tools > Global Options > Code > Completion in RStudio’s
top level menu.
Instead of only auto completing options when Tab is pressed, RStudio auto completes them at any point.
Building on the previous example, RStudio’s autocompletion triggers when the ﬁrst three characters are typed:
rno. The same functionality works when only the ﬁrst characters are typed, followed by Tab: automatic
auto-completion does not replace Tab autocompletion but supplements it. Note that in RStudio two more
options are provided to the user after entering rn Tab compared with entering the same text into base R’s
console described in the previous paragraph: RNGkind and RNGversion. This illustrates that RStudio’s
autocompletion functionality is not case sensitive in the same way that R is. This is a good thing because R
has no consistent function name style!
RStudio also has more intelligent auto-completion of objects and ﬁle names than R’s built-in command line.
To test this functionality, try typing US, followed by the Tab key. After pressing down until USArrests is
selected, press Enter so it autocompletes. Finally, typing $ should leave the following text on the screen and
the four columns should be shown in a drop-down box, ready for you to select the variable of interest with
the down arrow.
USArrests$ # a dropdown menu of columns should appear in RStudio
To take a more complex example, variable names stored in the data slot of the class SpatialPolygonsDataFrame
(a class deﬁned by the foundational spatial package sp) are referred to in the long form spdf@data$varname.5
5‘Slots’ are elements of an object (speciﬁcally, S4 objects) analogous to a column in a data.frame but referred to with @ not $.

34
CHAPTER 2. EFFICIENT SET-UP
In this case spdf is the object name, data is the slot and varname is the variable name. RStudio makes such
S4 objects easier to use by enabling autocompletion of the short form spdf$varname. Another example is
RStudio’s ability to ﬁnd ﬁles hidden away in sub-folders. Typing "te will ﬁnd test.R even if it is located in
a sub-folder such as R/test.R. There are a number of other clever auto-completion tricks that can boost R’s
productivity when using RStudio which are best found by experimenting and hitting Tab frequently during
your R programming work.
2.5.6
Keyboard shortcuts
RStudio has many useful shortcuts that can help make your programming more eﬃcient by reducing the
need to reach for the mouse and point and click your way around code and RStudio. These can be viewed by
using a little known but extremely useful keyboard shortcut (this can also be accessed via the Tools menu).
Alt+Shift+K
This will display the default shortcuts in RStudio. It is worth spending time identifying which of these
could be useful in your work and practising interacting with RStudio rapidly with minimal reliance on the
mouse. The power of these autocompletion capabilities can be further enhanced by setting your own keyboard
shortcuts. However, as with setting .Rprofile and .Renviron settings, this risks reducing the portability
of your workﬂow. To set your own RStudio keyboard shortcuts, navigate to Tools > Modify Keyboard
Shortcuts.
Some more useful shortcuts are listed below. There are many more gems to ﬁnd that could boost your R
writing productivity:
• Ctrl+Z/Shift+Z: Undo/Redo.
• Ctrl+Enter: Execute the current line or code selection in the Source pane.
• Ctrl+Alt+R: Execute all the R code in the currently open ﬁle in the Source pane.
• Ctrl+Left/Right: Navigate code quickly, word by word.
• Home/End: Navigate to the beginning/end of the current line.
• Alt+Shift+Up/Down: Duplicate the current line up or down.
• Ctrl+D: Delete the current line.
2.5.7
Object display and output table
It is useful to know what is in your current R environment. This information can be revealed with ls(), but
this function only provides object names. RStudio provides an eﬃcient mechanism to show currently loaded
objects, and their details, in real-time: the Environment tab in the top right corner. It makes sense to keep
an eye on which objects are loaded and to delete objects that are no longer useful. Doing so will minimise
the probability of confusion in your workﬂow (e.g. by using the wrong version of an object) and reduce the
amount of RAM R needs. The details provided in the Environment tab include the object’s dimension and
some additional details depending on the object’s class (e.g. size in MB for large datasets).
A very useful feature of RStudio is its advanced viewing functionality. This is triggered either by executing
View(object) or by double clicking on the object name in the Environment tab. Although you cannot edit
data in the Viewer (this should be considered a good thing from a data integrity perspective), recent versions
of RStudio provide an eﬃcient search mechanism to rapidly ﬁlter and view the records that are of most
interest (see Figure 2-3 above).

2.5. RSTUDIO
35
2.5.8
Project management
In the far top-right of RStudio there is a diminutive drop-down menu illustrated with R inside a transparent
box. This menu may be small and simple, but it is hugely eﬃcient in terms of organising large, complex and
long-term projects.
The idea of RStudio projects is that the bulk of R programming work is part of a wider task, which will
likely consist of input data, R code, graphical and numerical outputs and documents describing the work. It
is possible to scatter each of these elements at random across your hard-discs but this is not recommended.
Instead, the concept of projects encourages reproducible working, such that anyone who opens the particular
project folder that you are working from should be able to repeat your analyses and replicate your results.
It is therefore highly recommended that you use projects to organise your work. It could save hours in the
long-run. Organizing data, code and outputs also makes sense from a portability perspective: if you copy the
folder (e.g. via GitHub) your can work on it from any computer without worrying about having the right ﬁles
on your current machine. These tasks are implemented using RStudio’s simple project system, in which the
following things happen each time you open an existing project:
• The working directory automatically switches to the project’s folder. This enables data and script ﬁles
to be referred to using relative ﬁle paths, which are much shorter than absolute ﬁle paths. This means
that switching directory using setwd(), a common source of error for R users, is rarely if ever needed.
• The last previously open ﬁle is loaded into the Source pane. The history of R commands executed in
previous sessions is also loaded into the History tab. This assists with continuity between one session
and the next.
• The File tab displays the associated ﬁles and folders in the project, allowing you to quickly ﬁnd your
previous work.
• Any settings associated with the project, such as Git settings, are loaded. This assists with collaboration
and project-speciﬁc set-up.
Each project is diﬀerent but most contain input data, R code and outputs. To keep things tidy, we recommend
a sub-directory structure resembling the following:
project/
- README.Rmd # Project description
- set-up.R
# Required packages
- R/ # For R code
- input # Data files
- graphics/
- output/ # Results
Proper use of projects ensures that all R source ﬁles are neatly stashed in one folder with a meaningful
structure. This way data and documentation can be found where one would expect them. Under this system
ﬁgures and project outputs are ‘ﬁrst class citizens’ within the project’s design, each with their own folder.
Another approach to project management is to treat projects as R packages. This is not recommended for
most use cases, as it places restrictions on where you can put ﬁles. However, if the aim is code development
and sharing, creating a small R package may be the way forward, even if you never intend to submit it on
CRAN. Creating R packages is easier than ever before, as documented in (Cotton 2013) and, more recently
(Wickham 2015). The devtools package help manage R’s quirks, making the process much less painful. If
you use GitHub, the advantage of this approach is that anyone should be able to reproduce your working
using devtools::install_github("username/projectname"), although the administrative overheads of
creating an entire package for each small project will outweigh the beneﬁts for many.

36
CHAPTER 2. EFFICIENT SET-UP
Note that a set-up.R or even a .Rprofile ﬁle in the project’s root directory enable project-speciﬁc settings
to be loaded each time people work on the project. As described in the previous section, .Rprofile can
be used to tweak how R works at start-up. It is also a portable way to manage R’s conﬁguration on a
project-by-project basis.
2.5.9
Exercises
1. Try modifying the look and appearance of your RStudio setup.
2. What is the keyboard shortcut to show the other shortcut? (Hint: it begins with Alt+Shift on Linux
and Windows.)
3. Try as many of the shortcuts revealed by the previous step as you like. Write down the ones that you
think will save you time, perhaps on a post-it note to go on your computer.
2.6
BLAS and alternative R interpreters
In this section we cover a few system-level options available to speed-up R’s performance. Note that for
many applications stability rather than speed is a priority, so these should only be considered if a) you have
exhausted options for writing your R code more eﬃciently and b) you are conﬁdent tweaking system-level
settings. This should therefore be seen as an advanced section: if you are not interested in speeding-up base
R, feel free to skip to the next section of hardware.
Many statistical algorithms manipulate matrices. R uses the Basic Linear Algebra System (BLAS) framework
for linear algebra operations. Whenever we carry out a matrix operation, such as transpose or ﬁnding the
inverse, we use the underlying BLAS library. By switching to a diﬀerent BLAS library, it may be possible to
speed-up your R code. Changing your BLAS library is straightforward if you are using Linux, but can be
tricky for Windows users.
The two open source alternative BLAS libraries are ATLAS and OpenBLAS. The Intel MKL is another
implementation, designed for Intel processors by Intel and used in Revolution R (described in the next section)
but it requires licensing fees. The MKL library is provided with the Revolution analytics system. Depending
on your application, by switching you BLAS library, linear algebra operations can run several times faster
than with the base BLAS routines.
If you use Linux, you can ﬁnd whether you have a BLAS library setting with the following function, from
benchmarkme:
library("benchmarkme")
get_linear_algebra()
2.6.1
Testing performance gains from BLAS
As an illustrative test of the performance gains oﬀered by BLAS, the following test was run on a new laptop
running Ubuntu 15.10 on a 6th generation Core i7 processor, before and after OpenBLAS was installedv6
res = benchmark_std() # run a suit of tests to test R's performance
It was found that the installation of OpenBLAS led to a 2-fold speed-up (from around 150 to 70 seconds).
The majority of the speed gain was from the matrix algebra tests, as can be seen in ﬁgure 2.4. Note that the
6OpenBLAS was installed on the computer via sudo apt-get install libopenblas-base, which automatically detected and
used by R.

2.6. BLAS AND ALTERNATIVE R INTERPRETERS
37
results of such tests are highly dependent on the particularities of each computer. However, it clearly shows
that ‘programming’ benchmarks (e.g. the calculation of 3,500,000 Fibonacci numbers) are now much faster,
whereas matrix calculations and functions receive a substantial speed boost. This demonstrates that the
speed-up you can expect from BLAS depends heavily on the type of computations you are undertaking.
0
3
6
9
12
Programming
Matrix calculation
Matrix function
Benchmark
Elapsed Time
Optimised
Standard
Performance gains with BLAS
Figure 2.4: Performance gains obtained changing the underlying BLAS library (tests from benchmark_std()).
2.6.2
Revolution R
Revolution R is the main software product oﬀered by Revolution Analytics, which was recently purchased by
Microsoft, implying it has substantial development resources. It is “100%” compatible with R“, supporting
all available packages through the MRAN package repository. Revolution R provides faster performance on
for certain functions than base R, through its use of MKL, an implementation of BLAS (as described above).
Revolution R is available as a free and open source product, ‘Revolution R Open’ (RRO), and is reported to
be faster than base R installations.7
Additional benchmarks reported by Eddelbuettel (2010) show the MKL implementations of R used in RRO
and the commercial edition to be substantially faster than the reference case.
2.6.3
Other interpreters
The R language can be separated from the R interpreter. The former refers to the meaning of R commands,
the latter refers to how the computer executes the commands. Alternative interpreters have been developed
to try to make R faster and, while promising, none of the following options has fully taken oﬀ.
7See brodrigues.co/2014/11/11/benchmarks-r-blas-atlas-rro/, which ﬁnds Revolution R to be marginally faster than R using
OpenBLAS and ATLAS BLAS implementations and Faster BLAS in R, which does not.

38
CHAPTER 2. EFFICIENT SET-UP
• Rho (previously called CXXR, short for C++), a re-implementation of the R interpretter for speed and
eﬃciency. Of the new interpreters, this is the one that has the most recent development activity (as of
April 2016).
• pqrR (pretty quick R) is a new version of the R interpreter. One major downside, is that it is based
on R-2.15.0. The developer (Radford Neal) has made many improvements, some of which have now
been incorporated into base R. pqR is an open-source project licensed under the GPL. One notable
improvement in pqR is that it is able to do some numeric computations in parallel with each other, and
with other operations of the interpreter, on systems with multiple processors or processor cores.
• Renjin reimplements the R interpreter in Java, so it can run on the Java Virtual Machine (JVM). Since
R will be pure Java, it can run anywhere.
• Tibco created a C++ based interpreter called TERR.
• Oracle also oﬀer an R-interpreter that uses Intel’s mathematics library and therefore achieves a higher
performance without changing R’s core.
At the time of writing, switching interpreters is something to consider carefully. But in the future, it may
become more routine.
In this context it is also worth mentioning Julia. This is a fast language and interpreter which aims to provide
“a high-level, high-performance dynamic programming language for technical computing” that will be familiar
to R and Python users.
2.6.4
Useful BLAS/benchmarking resources
• The gcbd package benchmarks performance of a few standard linear algebra operations across a number
of diﬀerent BLAS libraries as well as a GPU implementation. It has an excellent vignette summarising
the results.
• Brett Klamer provides a nice comparison of ATLAS, OpenBLAS and Intel MKL BLAS libraries. He
also gives a description of how to install the diﬀerent libraries.
• The oﬃcial R manual section on BLAS.
2.6.5
Exercises
1. What BLAS system is your version of R using?

Chapter 3
Eﬃcient programming
Many people who use R would not describe themselves as “programmers”. Instead they have advanced
domain level knowledge, but little formal programming training. This chapter comes from this point of view;
someone who has uses standard R data structures, such as vectors and data frames, but lacks formal training.
In this chapter we will discuss “big picture” programming techniques. General R programming techniques
about optimising your code, before describing idiomatic programming structures. We conclude the chapter
by examining relatively easy ways of speeding up code using the compiler package and multiple CPUs.
3.1
General advice
C and Fortran demand more from the programmer. The coder must declare the type of every variable they
use and has the burdensome responsible of memory management. The payback is that it allows the compiler
to better predict how the program behaves and so make clever optimisations.
The wikipedia page on compiler opimisations gives a nice overview of standard optimisation
techniques (https://en.wikipedia.org/wiki/Optimizing_compiler).
In R we don’t (tend to) worry about data types. However this means that it’s possible to write R programs
that are incredibly slow. While optimisations such as going parallel can double speed, poor code can easily
run 100’s of times slower. If you spend any time programming in R, then (Burns 2011) should be considered
essential reading.
Ultimately calling an R function always ends up calling some underlying C/Fortran code. For example the
base R function runif only contains a single line that consists of a call to C_runif.
function (n, min = 0, max = 1)
.Call(C_runif, n, min, max)
The golden rule in R programming is to access the underlying C/Fortran routines as quickly as possible;
the fewer functions calls required to achieve this, the better. For example, suppose x is a standard vector of
length n. Then
39

40
CHAPTER 3. EFFICIENT PROGRAMMING
x = x + 1
involves a single function call to the + function. Whereas the for loop
for(i in 1:n)
x[i] = x[i] + 1
has
• n function calls to +;
• n function calls to the [ function;
• n function calls to the [<- function (used in the assignment operation);
• A function call to for and to the : operator.
It isn’t that the for loop is slow, rather it is because we have many more function calls. Each individual
function call is quick, but the total combination is slow.
Everything in R is a function call. When we execute 1 + 1, we are actually executing +(1, 1).
Exercise
Use the microbenchmark package to compare the vectorised construct x = x + 1, to the for loop version.
Try varying the size of the input vector.
3.1.1
Memory allocation
Another general technique is to be careful with memory allocation. If possible pre-allocate your vector then
ﬁll in the values.
You should also consider pre-allocating memory for data frames and lists. Never grow an object. A
good rule of thumb is to compare your objects before and after a for loop; have they increased in
length?
Let’s consider three methods of creating a sequence of numbers. Method 1 creates an empty vector and
grows the object
method1 = function(n) {
vec = NULL # Or vec = c()
for(i in 1:n)
vec = c(vec, i)
vec
}
Method 2 creates an object of the ﬁnal length and then changes the values in the object by subscripting:

3.1. GENERAL ADVICE
41
method2 = function(n) {
vec = numeric(n)
for(i in 1:n)
vec[i] = i
vec
}
Method 3 directly creates the ﬁnal object
method3 = function(n) 1:n
To compare the three methods we use the microbenchmark function from the previous chapter
microbenchmark(times = 100, unit="s",
method1(n), method2(n), method3(n))
The table below shows the timing in seconds on my machine for these three methods for a selection of values
of n. The relationships for varying n are all roughly linear on a log-log scale, but the timings between methods
are drastically diﬀerent. Notice that the timings are no longer trivial. When n = 107, method 1 takes around
an hour whilst method 2 takes 2 seconds and method 3 is almost instantaneous. Remember the golden rule;
access the underlying C/Fortran code as quickly as possible.
Table 3.1: Time in seconds to create sequences. When n = 107,
method 1 takes around an hour while the other methods take less
than 3 seconds.
n
Method 1
Method 2
Method 3
105
0.21
0.02
0.00
106
25.50
0.22
0.00
107
3827.00
2.21
0.00
3.1.2
Vectorised code
The vector is one of the key data types in R, with many functions oﬀering a vectorised version. For example,
the code
x = runif(n) + 1
performs two vectorised operations. First runif returns n random numbers. Second we add 1 to each element
of the vector. In general it is a good idea to exploit vectorised functions. Consider this piece of R code that
calculates the sum of log(x)
log_sum = 0
for(i in 1:length(x))
log_sum = logsum + log(x[i])
Using 1:length(x) can lead to hard-to-ﬁnd bugs when x has length zero. Instead use seq_along(x)
or seq_len(length(x)).

42
CHAPTER 3. EFFICIENT PROGRAMMING
This code could easily be vectorised via
log_sum = sum(log(x))
Writing code this way has a number of beneﬁts.
• It’s faster. When n = 107 the “R way” is about forty times faster.
• It’s neater.
• It doesn’t contain a bug when x is of length 0.
Exercises
Time the two methods for calculating the log sum. Try diﬀerent values of n.
Example: Monte-Carlo integration
It’s also important to make full use of R functions that use vectors. For example, suppose we wish to estimate
the integral
Z 1
0
x2dx
using a Monte-Carlo method. Essentially, we throw darts at the curve and count the number of darts that
fall below the curve (as in 3.1).
Monte Carlo Integration
1. Initialise: hits = 0
2. for i in 1:N
3.
Generate two random numbers, U1, U2, between 0 and 1
4.
If U2 < U 2
1 , then hits = hits + 1
5. end for
6. Area estimate = hits/N
Implementing this Monte-Carlo algorithm in R would typically lead to something like:
monte_carlo = function(N){
hits = 0
for(i in 1:N)
{
u1 = runif(1); u2 = runif(1)
if(u1^2 > u2)
hits = hits + 1
}
return(hits/N)
}
In R this takes a few seconds
N = 500000
system.time(monte_carlo(N))
#>
user
system elapsed
#>
3.023
0.003
3.027
In contrast a more R-centric approach would be

3.1. GENERAL ADVICE
43
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
f(x)
Miss
Hit
Monte−Carlo Integration
Figure 3.1: Example of Monte-Carlo integration. To estimate the area under the curve throw random points
at the graph and count the number of points that lie under the curve.
monte_carlo_vec = function(N) sum(runif(N)^2 > runif(N))/N
The monte_carlo_vec function contains (at least) four aspects of vectorisation
• The runif function call is now fully vectorised;
• We raise entire vectors to a power via ˆ;
• Comparisons using > are vectorised;
• Using sum is quicker than an equivalent for loop.
The function monte_carlo_vec is around 30 times faster than monte_carlo.
Exercise
Verify that monte_carlo_vec is faster than monte_carlo. How does this relate to the number of darts,
i.e. the size of N, that is used
3.1.3
Type consistency
When programming it is helpful if the return value from a function always takes the same form. Unfortunately,
not all of base R functions follow this idiom. For example the functions sapply and [.data.frame aren’t
type consistent
two_cols = data.frame(x = 1:5, y = letters[1:5])
zero_cols = data.frame()
sapply(two_cols, class)
# a character vector

44
CHAPTER 3. EFFICIENT PROGRAMMING
sapply(zero_cols, class) # a list
two_cols[, 1:2]
# a data.frame
two_cols[, 1]
# an integer vector
This can cause unexpected problems. The functions lapply and vapply are type consistent. Likewise
dplyr::select and dplyr:filter. The purrr package has some type consistent alternatives to base R
functions. For example, map_dbl etc. to replace Map and flatten_df to replace unlist.
Exercises
1. Rewrite the sapply function calls above using vapply to ensure type consistency.
2. How would you make subsetting data frames with [ type consistent? Hint: look at the drop argument.
3.2
Communicating with the user
When we create a function we often want the function to give feedback on the current state. For example,
are there missing arguments or has a numerical calculation failed. There are three main techniques of
communicating with the user.
3.2.1
Fatal errors: stop
Fatal errors are raised by calling the stop, i.e. execution is terminated. When stop is called, there is no way
for a function to continue. For instance, when we generate random numbers using rnorm the ﬁrst argument
is the n, if the number of observations to return less than 1, an error is raised.
Errors can be caught using try and tryCatch. For example,
## silent suppresses the error message
good = try(1 + 1, silent=TRUE)
bad = try(1 + "1", silent=TRUE)
When we inspect the objects, good just contains the number 2:
good
#> [1] 2
However, bad is a character string with class try-error and an condition attribute that contains
bad
#> [1] "Error in 1 + \"1\" : non-numeric argument to binary operator\n"
#> attr(,"class")
#> [1] "try-error"
#> attr(,"condition")
#> <simpleError in 1 + "1": non-numeric argument to binary operator>
We can use this information in a standard conditional statement

3.2. COMMUNICATING WITH THE USER
45
if(class(bad) == "try-error")
## Do something
Further details on error handling, as well as some excellent advice on general debugging techniques, are given
in (Wickham 2014a).
3.2.2
Warnings: warning
Warnings are generated using the warning function.
When a warning is raised, it indicates potential
problems. For example, mean(NULL) returns NA and also raises a warning. Warnings can be hidden using
suppressWarnings:
suppressWarnings(mean(NULL))
#> [1] NA
In general, it is good practice to solve the underlying cause of the warning message, instead of evading the
issue via suppressWarning.
3.2.3
Informative output: message and cat
To give informative output, use the message function. In my package for ﬁtting powerlaws, I use messages
to give the user an estimate of expected run time. Similar to warnings, messages can be suppressed with
suppressMessages.
Another function used for printing messages is cat.
In general cat should only be used in actual
print/show methods, e.g. look at the function deﬁnition of the S3 print method for difftime objects,
getS3method("print", "difftime").
3.2.4
Example: Retrieving a web resource
In some cases it isn’t clear what should be returned, a message, a NA value or an error. For example, suppose
we wish to download data from a web site.
The function GET from the httr package can be used to download a webpage. When the we attempt to
download data, the function also obtains an HTML status code. If the page exists, the function will return
text (with the correct status code), e.g.
GET("http://google.com/") # Status: 200
If the url is incorrect, e.g. a broken link or is a redirect, then we can interogate the output and use the Status
to determine what to do next.
GET("http://google.com/made_up") # Status: 404
By examining the error code, the user has complete ﬂexibility. However if the web-page doesn’t exist
GET("http://google1.com/")

46
CHAPTER 3. EFFICIENT PROGRAMMING
or if we don’t have an internet connection, then GET raises an error via stop. Instead of raising an error the
author could have returned NA (the web-page isn’t available). However for the typically use case, it’s not
clear how returning NA would be helpful.
The install_github function in the **devtools* package either installs the package or raises an error. When
an error is raised, the function uses a combination of message and stop to indicate the source of an error, e.g.
devtools::install_github("bad/package")
#> Downloading GitHub repo bad/package@master
#> from URL https://api.github.com/repos/bad/package/zipball/master
#>
Error in curl::curl_fetch_memory(url, handle = handle) :
#>
Couldn't resolve host name
With the key idea being that if a package isn’t able to be installed, future calculations will break; so it’s
optimal to raise an error as soon as possible.
Exercises
The stop function has an argument call. that indicates if the function call should be part of the error
message. Create a function and experiment with this option.
3.2.5
Invisible returns
The invisible function allows you to return a temporarily invisible copy of an object. This is particularly
useful for functions that return values which can be assigned, but are not printed when they are not assigned.
For example suppose we have a function that plots the data and ﬁts a straight line
regression_plot = function(x, y, ...) {
plot(x, y, ...)
model = lm(y ~ x)
abline(model)
invisible(model)
}
When the function is called, a scatter graph is plotted with the line of best ﬁt, but the output is invisible.
However when we assign the function to an object, i.e. out = regression_plot(x, y) the variable out
contains the output of the lm call.
Another example is hist. Typically we don’t want anything displayed in the console when we call the function
hist(x)
However if we assign the output to an object, out = hist(x), the object out is actually a list containing,
inter alia, information on the mid-points, breaks and counts.
3.3
Factors
Factors are much maligned objects. While at times they are awkward, they do have their uses. A factor is
used to store categorical variables. This data type is unique to R (or at least not common among programming
languages). Often categorical variables get stored as 1, 2, 3, 4, and 5, with associated documentation elsewhere
that explains what each number means. This is clearly a pain. Alternatively we store the data as a character

3.3. FACTORS
47
vector. While this is ﬁne, the semantics are wrong because it doesn’t convey that this is a categorical variable.
It’s not sensible to say that you should always or never use factors, since factors have both positive and
negative features. Instead we need to examine each case individually. As a guide of when it’s appropriate to
use factors, consider the following examples.
3.3.1
Example: Months of the year
Suppose our data set relates to months of the year
m = c("January", "December", "March")
If we sort m in the usual way, sort(m), we perform standard alpha-numeric ordering; placing December ﬁrst.
This is technically correct, but not that helpful. We can use factors to remedy this problem by specifying the
admissible levels
# month.name contains the 12 months
fac_m = factor(m, levels=month.name)
sort(fac_m)
#> [1] January
March
December
#> 12 Levels: January February March April May June July August ... December
3.3.2
Example: Graphics
Factors are used for ordering in graphics. For instance, suppose we have a data set where the variable type,
takes one of three values, small, medium and large. Clearly there is an ordering. Using a standard boxplot
call, boxplot(y ~ type), would create a boxplot where the x-axis was alphabetically ordered. By converting
type into factor, we can easily specify the correct ordering.
boxplot(y ~ type)
boxplot(y ~ factor(type, levels=c("Small", "Medium", "Large")))
3.3.3
Example: Analysis of variance
Analysis of variance (ANOVA) is a type of statistical model that is used to determine diﬀerences among group
means, while taken into account other factors. The function aov is used to ﬁt standard analysis of variance
models. Potential catastrophic bugs arise when a variable is numeric, but in reality is a categorical variable.
Consider the npk dataset on the growth of peas that comes with R. The column block, indicates the block
(typically a nuisance parameter) eﬀect. This column takes values 1 to 5, but has been carefully coded as a
factor. Using the aov function to estimate the block eﬀect, we get
aov(yield ~ block, npk)
#> Call:
#>
aov(formula = yield ~ block, data = npk)
#>
#> Terms:
#>
block Residuals
#> Sum of Squares
343
533
#> Deg. of Freedom
5
18
#>
#> Residual standard error: 5.44
#> Estimated effects may be unbalanced

48
CHAPTER 3. EFFICIENT PROGRAMMING
If we repeat the analysis, but change block to a numeric data type we get diﬀerent (and incorrect) results
aov(yield ~ as.numeric(block), npk)
#> Call:
#>
aov(formula = yield ~ as.numeric(block), data = npk)
#>
#> Terms:
#>
as.numeric(block) Residuals
#> Sum of Squares
22
854
#> Deg. of Freedom
1
22
#>
#> Residual standard error: 6.23
#> Estimated effects may be unbalanced
When we pass a numeric variable, the aov function is interpreting this variable as continuous, and ﬁts a
regression line.
3.3.4
Example: data input
Most users interact with factors via the read.csv function where character columns are automatically
converted to factors. This feature can be irritating if our data is messy and we want to clean and recode
variables. Typically when reading in data via read.csv, we use the stringsAsFactors=FALSE argument.
Although this argument can add in the global options() list and placed in the .Rprofile, this
leads to non-portable code, so should be avoided.
3.3.5
Example: Factors are not character vectors
Although factors look similar to character vectors, they are actually integers. This leads to initially surprising
behaviour
x = 4:6
c(x)
#> [1] 4 5 6
c(factor(x))
#> [1] 1 2 3
In this case the c function is using the underlying integer representation of the factor.
Overall factors are useful, but can lead to unwanted side-eﬀects if we are not careful. Used at the right time
and place, factors can lead to simpler code.
Exercise
Factors are slightly more space eﬃcient than characters. Create a character vector and corresponding factor
and use pryr::object_size to calculate the space needed for each object.

3.4. S3 OBJECTS
49
3.4
S3 objects
R has three built-in object oriented (OO) systems. These systems diﬀer in how classes and methods are
deﬁned. The easiest and oldest system is the S3 system. S3 refers to the third version of S. The syntax of
R is largely based on this version of S. In R there has never been S1 and S2 classes. The other two OO
frameworks are S4 classes (used mainly in bioconductor packages) and reference classes.
There are also packages that also provide additional OO frameworks, such as proto, R6 and R.oo.
If you are new to OO in R, then S3 is the place to start.
In this section we will just discuss the S3 system since that is the most popular. The S3 system implements a
generic-function object oriented (OO) system. This type of OO is diﬀerent to the message-passing style of Java
and C++. In a message-passing framework, messages/methods are sent to objects and the object determines
which function to call, e.g. normal.rand(1). The S3 class system is diﬀerent. In S3, the generic function
decides which method to call - it would have the form rand(normal, 1). By using an OO framework, we
avoid an explosion of exposed functions, such as, rand_normal, rand_uniform, rand_poisson and instead
have a single function call rand that passes the object to the correct function.
The S3 system is based on the class of an object. In S3, a class is just an attribute which can be determined
with the class function.
class(USArrests)
#> [1] "data.frame"
The S3 system can be used to great eﬀect. When we pass an object to a generic function, the function
ﬁrst examines the class of the object, and then dispatches the object to another method. For example, the
summary function is a S3 generic function
functionBody("summary")
#> UseMethod("summary")
Note that the only operational line is UseMethod("summary"). This handles the method dispatch based
on the object’s class.
So when summary(USArrests) is executed, the generic summary function passes
USArrests to the function summary.data.frame. If the function summary.data.frame does not exist, then
summary.default is called (if it exists). If neither function exist, an error is raised.
This simple message passage mechanism enables us to quickly create our own functions. Consider the distance
object:
dist_usa = dist(USArrests)
The dist_usa object has class dist. To visualise the distances, we create an image method. First we’ll check
if the existing image function is generic, via
## In R3.3, a new function isS3stdGeneric is going to be introduced.
functionBody("image")
#> UseMethod("image")
Since image is already a generic method, we just have to create a speciﬁc dist method

50
CHAPTER 3. EFFICIENT PROGRAMMING
euclidean
Figure 3.2: S3 image method for data of class ‘dist‘.
image.dist = function(x, ...) {
x_mat = as.matrix(x)
image(x_mat, main=attr(x, "method"), ...)
}
The ... argument allows us to pass arguments to the main image method, such as axes (see ﬁgure 3.2.
Many S3 methods work in the same way as the simple image.dist function created above: the object is
manipulated into a standard format, then passed to the standard method. Creating S3 methods for standard
functions such as summary, mean, and plot provides a nice uniform interface to a wide variety of data types.
Exercises
A data frame is just an R list, with class data.frame.
1. Use a combination of unclass and str on a data frame to conﬁrm that it is indeed a list.
2. Use the function length on a data frame. What is return? Why?
3.5
Caching variables
A straightforward method for speeding up code is to calculate objects once and reuse the value when necessary.
This could be as simple with replacing log(x) in multiple function calls with the object log_x that is deﬁned
once and reused. This small saving in time quickly multiplies when the cached variable is used inside a for
loop.
A more advanced form of caching is to use the memoise package. If a function is called multiple times
with the same input, it may be possible to speed things up by keeping a cache of known answers that it can
retrieve. The memoise package allows us easily store the value of function call and returns the cached result
www.allitebooks.com

3.5. CACHING VARIABLES
51
when the function is called again with the same arguments. This package trades oﬀmemory versus speed,
since the memoised function stores all previous inputs and outputs. To cache a function, we simply pass the
function to the memoise function.
The classic memoise example is the factorial function. Another example is to limit use to a web resource.
For example, suppose we are developing a shiny (an interactive graphic) application where the user can ﬁt
regression line to data. The user can remove points and reﬁt the line. An example function would be
# Argument indicates row to remove
plot_mpg = function(row_to_remove) {
data(mpg, package="ggplot2")
mpg = mpg[-row_to_remove,]
plot(mpg$cty, mpg$hwy)
lines(lowess(mpg$cty, mpg$hwy), col=2)
}
We can use memoise speed up by caching results. A quick benchmark
library("memoise")
m_plot_mpg = memoise(plot_mpg)
microbenchmark(times=10, unit="ms", m_plot_mpg(10), plot_mpg(10))
#> Unit: milliseconds
#>
expr
min
lq
mean median
uq
max neval cld
#>
m_plot_mpg(10)
0.04 4e-02
0.07
8e-02 8e-02
0.1
10
a
#>
plot_mpg(10) 40.20 1e+02 95.52
1e+02 1e+02 107.1
10
b
suggests that we can obtain a 100-fold speed-up.
Exercise
Construct a box plot of timings for the standard plotting function and the memoised version.
3.5.1
Function closures
The following section is meant to provide an introduction to function closures with example use
cases. See [@Wickham2014] for a detailed introduction.
More advanced caching is available using function closures. A closure in R is an object that contains functions
bound to the environment the closure was created in. Technically all functions in R have this property, but
we use the term function closure to denote functions where the environment is not in .GlobalEnv. One of
the environments associated with a function is known as the enclosing environment, that is, where was the
function created. We can determine the enclosing environment using environment:
environment(plot_mpg)
#> <environment: R_GlobalEnv>
The plot_mpg function’s enclosing environment is the .GlobalEnv. This is important for variable scope,
i.e. where should be look for a particular object. Consider the function f

52
CHAPTER 3. EFFICIENT PROGRAMMING
f = function() {
x = 5
function() x
}
When we call the function f, the object returned is a function. While the enclosing environment of f is
.GlobalEnv, the enclosing environment of the returned function is something diﬀerent
g = f()
environment(g)
#> <environment: 0x169ad38>
When we call this new function g,
x = 10
g()
#> [1] 5
The value returned is obtained from environment(g) not from the .GlobalEnv. This persistent environment
allows us to cache variables between function calls.
The operator <<- makes R search through the parent environments for an existing deﬁntion. If such
a variable is found (and its binding is not locked) then its value is redeﬁned, otherwise assignment
takes place in the global environment.
The simple_counter function exploits this feature to enable variable caching
simple_counter = function() {
no = 0
function() {
no <<- no + 1
no
}
}
When we call the simple_counter function, we retain object values between function calls
sc = simple_counter()
sc()
#> [1] 1
sc()
#> [1] 2
The key points of the simple_counter function are
• The simple_counter function returns a function;
• The enclosing environment of sc is not .GlobalEnv, instead it’s the binding environment of sc;
• The function sc has an environment that can be used to store/cache values;
• The operator <<- is used to alter the object no in the sc environment.

3.5. CACHING VARIABLES
53
Example
We can exploit function closures to simplify our code. Suppose we wished to simulate a games of Snakes and
Ladders. We have function that handles the logic of landing on a snake
check_snake = function(square) {
switch(as.character(square),
'16'=6,
'49'=12, '47'=26, '56'=48, '62'=19,
'64'=60, '87'=24, '93'=73, '96'=76, '98'=78,
square)
}
If we then wanted to determine how often we landed on a Snake, we could use a function closure to easily
keep track of the counter.
check_snake = function() {
no_of_snakes = 0
function(square) {
new_square = switch(as.character(square),
'16'=6,
'49'=12, '47'=26, '56'=48, '62'=19,
'64'=60, '87'=24, '93'=73, '96'=76, '98'=78,
square)
no_of_snakes <<- no_of_snakes + (new_square != square)
new_square
}
}
Keeping the variable no_of_snakes attached to the check_snake function, enables us to have cleaner code.
Exercise
The following function implements a stop-watch function
stop_watch = function() {
start_time = stop_time = NULL
start = function() start_time <<- Sys.time()
stop = function() {
stop_time <<- Sys.time()
difftime(stop_time, start_time)
}
list(start=start, stop=stop)
}
watch = stop_watch()
It contains two functions. One function for starting the timer
watch$start()
the other for stopping the timer

54
CHAPTER 3. EFFICIENT PROGRAMMING
watch$stop()
Many stop-watches have the ability to measure not only your overall time but also you individual laps. Add
a lap function to the stop_watch function that will record individual times, while still keeping track of the
overall time.
3.6
The byte compiler
The compiler package, written by R Core member Luke Tierney has been part of R since version 2.13.0.
The compiler package allows R functions to be compiled, resulting in a byte code version that may run
faster1. The compilation process eliminates a number of costly operations the interpreter has to perform,
such as variable lookup.
Since R 2.14.0, all of the standard functions and packages in base R are pre-compiled into byte-code. This is
illustrated by the base function mean:
getFunction("mean")
#> function (x, ...)
#> UseMethod("mean")
#> <bytecode: 0x388da70>
#> <environment: namespace:base>
The third line contains the bytecode of the function. This means that the compiler package has translated
the R function into another language that can be interpreted by a very fast interpreter. Amazingly the
compiler package is almost entirely pure R, with just a few C support routines.
3.6.1
Example: the mean function
The compiler package comes with R, so we just need to load the package in the usual way
library("compiler")
Next we create an ineﬃcient function for calculating the mean. This function takes in a vector, calculates the
length and then updates the m variable.
mean_r = function(x) {
m = 0
n = length(x)
for(i in seq_len(n))
m = m + x[i]/n
m
}
This is clearly a bad function and we should just mean function, but it’s a useful comparison. Compiling the
function is straightforward
cmp_mean_r = cmpfun(mean_r)
1The authors have yet to ﬁnd a situation where byte compiled code runs signiﬁcantly slower.

3.6. THE BYTE COMPILER
55
Then we use the microbenchmark function to compare the three variants
# Generate some data
x = rnorm(1000)
microbenchmark(times=10, unit="ms", # milliseconds
mean_r(x), cmp_mean_r(x), mean(x))
#> Unit: milliseconds
#>
expr
min
lq
mean median
uq
max neval cld
#>
mean_r(x) 0.358 0.361 0.370
0.363 0.367 0.43
10
c
#>
cmp_mean_r(x) 0.050 0.051 0.052
0.051 0.051 0.07
10
b
#>
mean(x) 0.005 0.005 0.008
0.007 0.008 0.03
10 a
The compiled function is around seven times faster than the uncompiled function. Of course the native mean
function is faster, but compiling does make a signiﬁcant diﬀerence (ﬁgure 3.3).
Sample size
Relative Timings
0
50
100
150
200
102
103
104
Compiled vs Non−Compiled
Pure R
Compiled R
mean
Figure 3.3: Comparsion of mean functions.
3.6.2
Compiling code
There are a number of ways to compile code. The easiest is to compile individual functions using cmpfun,
but this obviously doesn’t scale. If you create a package, you can automatically compile the package on
installation by adding
ByteCompile: true
to the DESCRIPTION ﬁle. Most R packages installed using install.packages are not compiled. We can
enable (or force) packages to be compiled by starting R with the environment variable R_COMPILE_PKGS set
to a positive integer value and specify that we install the package from source, i.e.

56
CHAPTER 3. EFFICIENT PROGRAMMING
## Windows users will need Rtools
install.packages("ggplot2", type="source")
A ﬁnal option to use just-in-time (JIT) compilation. The enableJIT function disables JIT compilation if the
argument is 0. Arguments 1, 2, or 3 implement diﬀerent levels of optimisation. JIT can also be enabled by
setting the environment variable R_ENABLE_JIT, to one of these values.
I always set the compile level to the maximum value of 3.

Chapter 4
Eﬃcient workﬂow
Eﬃcient programming is an important and sometimes vital skill for generating the correct result, on time.
Yet coding is only one part of a wider skillset needed for successful project outcomes which involve R. In this
context we deﬁne ‘workﬂow’ as the sum of practices, habits and systems that enable productivity.1 To some
extent workﬂow is about personal preferences. Everyone’s mind works diﬀerently so the most appropriate
workﬂow varies from person to person and from one project to the next. Project management practices will
also vary depending the scale and type of the project.
Scale is a vital consideration because the importance of project management increases in a non linear fashion
with the number of people involved. Writing a ‘one-oﬀ’ R script requires no project management. Working
with several developers to deliver a mission critical application for central government requires regular
meetings, division of labour and a project management system for tracking progress, issues and priorities.2
Project management structures also depend on the type of project. The typology below (thanks to Richard
Cotton) demonstrate the links between project type and project management requirements.
• Data analysis. Here you are trying to explore datasets to discover something interesting/answer some
questions. The emphasis is on speed of manipulating your data to generate interest results. Formality
is less important in this type of project. Sometimes this analysis project may only be part of a larger
project (the data may have to be created in a lab, for example). How the data analysts interact with
the rest of the team may be as important for the project’s success as how they interact with each other.
• Package creation. Here you want to create code that can be reused across projects, possibly by people
whose use case you don’t know (if you make it publicly available). The emphasis in this case will be on
clarity of user interface and documentation, meaning style and code review are important. Robustness
and testing are important in this type of project too.
• Reporting and publishing. Here you are writing a report or journal paper or book. The level of formality
varies depending upon the audience, but you have additional worries like how much code it takes to
arrive at the conclusions, and how much output does the code create.
• Software applications. This could range from a simple Shiny app to R being embedded in the server of
a much larger piece of software. Either way, since there is limited opportunity for human interaction,
the emphasis is on robust code and gracefully dealing with failure.
Based on these observations we recommend trying diﬀerent working practices to discover which works best
for you and the teams you work with.3
1The Oxford Dictionary’s deﬁnition of workﬂow is similar, with a more industrial feel: “The sequence of industrial,
administrative, or other processes through which a piece of work passes from initiation to completion.”
2A number of dedicated project management systems are now available to assist with this task. These include (in rough
ascending order of scale and complexity): the web browser add-on ZenHub, “the ﬁrst and only project management suite that
works natively within GitHub”; the web-based and easy-to-use Trello; and the fully featured enterprise scale open source project
management system OpenProject.
3The importance of workﬂow has not gone unnoticed by the R community and there are a number of diﬀerent suggestions to
57

58
CHAPTER 4. EFFICIENT WORKFLOW
There are, however, concrete steps that can be taken to improve workﬂow in most projects that involve R
programming. Learning them will, in the long-run, improve productivity and reproducibility. With these
motivations in mind, the purpose of this chapter is simple: to highlight some key ingredients of an eﬃcient
R workﬂow. It builds on the concept of an R/RStudio project, introduced in Chapter 2, and is ordered
chronologically throughout the stages involved in a typical project’s lifespan, from its inception to publication:
• Project planning. This should happen before any code has been written, to avoid time wasted using a
mistaken analysis strategy.
• Package selection. After planning your project you should identify which packages are most suitable
to get the work done quickly and eﬀectively. With rapid increases in the number and performance of
packages (*_join from dplyr, for example, is often more appropriate than merge), it is more important
than ever to consider the range of options at the outset.
• Importing data. This can depend on external packages and represent a time-consuming and computa-
tional bottle-neck that prevents progress.
• Tidying data. This critical stage results in datasets that are convenient for analysis and processing,
with implications for the eﬃciency of all subsequent stages (Wickham 2014b).
• Data processing. This stage involves manipulating data to help answer hypotheses and draw conclusions.
The focus of this section is on dplyr and data.table, which make data analysis code fast to type and
fast to run.
• Publication. This ﬁnal stage is relevant if you want your R code to be useful for others in the long
term. To this end Section 4.7 touches on documentation using knitr and the much stricter approach to
code publication of package development.
4.1
Project planning
Good programmers embarking on a complex project will rarely just start typing code. Instead, they will
plan the steps needed to complete the task as eﬃciently as possible: “smart preparation minimizes work”
(Berkun 2005). Although search engines are useful for identifying the appropriate strategy, the trail-and-error
approach — typing code at random and Googling the inevitable error messages — is usually highly ineﬃcient.
Strategic thinking is necessary.
The best place to start may in fact be pen and blank sheet of paper, allowing you to sketch out your
ideas before you begin. Project planning is a creative process not always well-suited to the linear logic of
computing.4 It involves considering the project’s aims in the context of available resources (e.g. computational
and programmer resources), project scope, timescales and suitable software. And these things should be
considered together. To take one example, is it worth the investment of time needed to learn a particular
R package which is not essential to completing the project but which will make the code run faster? Does
it make more sense to hire another programmer or invest in more computational resources to complete an
urgent deadline?
Minutes spent thinking through such issues before writing a single line can save hours in the future. This is
emphasised in books such as Berkun (2005) and PMBoK (2000) and useful online resources such those by
teamgantt.com and lasa.org.uk, which focus exclusively on project planning. This section is not intended
to replace such guides. Instead, the aim here is to condense some of the most important lessons from this
literature in the context of typical R projects (i.e. which involve data analysis, modelling and visualisation).
boost R productivity. Rob Hyndman, for example, advocates the strategy of using four self-contained scripts to break up R
work into manageable chunks: load.R, clean.R, func.R and do.R.
4A number of programs have been developed to assist project management and planning, however. These include ProjectLibre
and GanttProject.

4.1. PROJECT PLANNING
59
4.1.1
‘Chunking’ your work
Once a project overview has been devised and stored, in mind (for small projects, if you trust that as storage
medium!) or written, a plan with a time-line can be drawn-up. The up-to-date visualisation of this plan can
be a powerful reminder to yourself and collaborators of progress on the project so far. More importantly the
timeline provides an overview of what needs to be done next. Setting start dates and deadlines for each task
will help prioritise the work and ensure you are on track. Breaking a large project into smaller chunks is
highly recommended, making huge, complex tasks more achievable and modular PMBoK (2000). ‘Chunking’
the work will also make collaboration easier, as we shall see in Chapter 5.
Level of Activity
Planning
Programming
Write−up
Start
End
Key Project Phases
Time
Figure 4.1: Schematic illustrations of key project phases and levels of activity over time, based on PMBoK
(2000).
The tasks that a project should be split into will depend the nature of the work and the phases illustrated in
Figure 4.1 represent a rough starting point, not a template and the ‘programming’ phase will usually need to
be split into at least ‘data tidying’, ‘processing’, and ‘visualisation’.
4.1.2
Making your workﬂow SMART
A more rigorous (but potentially onerous) way to project plan is to divide the work into a series of objectives
and tracking their progress throughout the project’s duration. One way to check if an objective is appropriate
for action and review is by using the SMART criteria.
• Speciﬁc: is the objective clearly deﬁned and self-contained?
• Measurable: is there a clear indication of its completion?
• Attainable: can the target be achieved?
• Realistic: have suﬃcient resources been allocated to the task?
• Time-bound: is there an associated completion date or milestone?
If the answer to each of these questions is ‘yes’, the task is likely to be suitable to include in the project’s
plan. Note that this does not mean all project plans need to be uniform. A project plan can take many forms,

60
CHAPTER 4. EFFICIENT WORKFLOW
including a short document, a Gantt chart (see Figure 4.2 or simply a clear vision of the project’s steps in
mind.
Figure 4.2: A Gantt chart created using DiagrammeR illustrating the steps needed to complete this book
at an early stage of its development.
4.1.3
R packages for planning
A number of R packages can assist with this process of formalising and visualising the project plan, including:5
• the plan package, which provides basic tools to create burndown charts (which concisely show whether
a project is on-time or not) and Gantt charts.
• plotrix, a general purpose plotting package, provides basic Gantt chart plotting functionality. Enter
example(gantt.chart) for details.
• DiagrammeR, a new package for creating network graphs and other schematic diagrams in R. This
package provides an R interface to simple ﬂow-chart ﬁle formats such as mermaid and GraphViz.
The small example below (which provides the basis for creating charts like Figure 4.2 illustrates how
DiagrammeR can take simple text inputs to create informative up-to-date Gantt charts. Such charts can
greatly help with the planning and task management of long and complex R projects, as long as they do not
take away valuable programming time from core project objectives. {#DiagrammeR}
5For a more comprehensive discussion of Gantt charts in R, please refer to stackoverﬂow.com/questions/3550341.

4.2. PACKAGE SELECTION
61
library("DiagrammeR")
# Define the Gantt chart and plot the result (not shown)
mermaid("gantt
Section Initiation
Planning
:a1, 2016-01-01, 10d
Data processing
:after a1
, 30d")
In the above code gantt deﬁnes the subsequent data layout. Section refers to the project’s section (useful
for large projects, with milestones) and each new line refers to a discrete task. Planning, for example, has
the code a, begins on the ﬁrst day of 2016 and lasts for 10 days. See knsv.github.io/mermaid/gantt.html for
more detailed documentation.
4.1.4
Exercises
1. What are the three most important work ‘chunks’ of your current R project?
2. What is the meaning of ‘SMART’ objectives (see Making your workﬂow SMART).
3. Run the code chunk at the end of this section to see the output.
4. Bonus exercise: modify this code to create a basic Gantt chart of an R project you are working on.
4.2
Package selection
A good example of the importance of prior planning to minimise eﬀort is package selection. An ineﬃcient,
poorly supported or simply outdated package can waste hours. When a more appropriate alternative is
available this waste can be prevented by prior planning. There are many poor packages on CRAN and much
duplication so it’s easy to go wrong. Just because a certain package can solve a particular problem, doesn’t
mean that it should.
However, used well, packages can greatly improve productivity. Due to the conservative nature of base R
development, which rightly prioritises stability over innovation, much of the innovation and performance gains
in the ‘R ecosystem’ has occurred in recent years in the packages. The increased ease of package development
(Wickham 2015) and interfacing with other languages (e.g. Eddelbuettel et al. 2011) has accelerated their
number, quality and eﬃciency. An additional factor has been the growth in collaboration and peer review
in package development, driven by code-sharing websites such as GitHub and online communities such as
ROpenSci for peer reviewing code.
Performance, stability and ease of use should be high on the priority list when choosing which package to
use. Another more subtle factor is that some packages work better together than others. The ‘R package
ecosystem’ is composed of interrelated package. Knowing something of these inter-dependencies can help
select a ‘package suite’ when the project demands a number of diverse yet interrelated programming tasks.
The ‘hadleyverse’, for example, contains many interrelated packages that work well together, such as readr,
tidyr, and dplyr.6 These may be used together to read-in, tidy and then process the data, as outlined in
the subsequent sections.
There is no ‘hard and fast’ rule about which package you should use and new packages are emerging all
the time. The ultimate test will be empirical evidence: does it get the job done on your data? However,
the following criteria should provide a good indication of whether a package is worth an investment of your
precious time, or even installing on your computer:
6An excellent overview of the ‘hadleyverse’ and its beneﬁts is available from barryrowlingson.github.io/hadleyverse.

62
CHAPTER 4. EFFICIENT WORKFLOW
• Is it mature? The more time a package is available, the more time it will have for obvious bugs to be
ironed out. The age of a package on CRAN can be seen from its Archive page on CRAN. We can see
from cran.r-project.org/src/contrib/Archive/ggplot2/, for example, that ggplot2 was ﬁrst released on
the 10th June 2007 and that it has had 28 releases. The most recent of these at the time of writing was
ggplot2 2.0.0: reaching 1 or 2 in the ﬁrst digit of package versions is usually an indication from the
package author that the package has reached a high level of stability.
• Is it actively developed? It is a good sign if packages are frequently updated. A frequently updated
package will have its latest version ‘published’ recently on CRAN. The CRAN package page for ggplot2,
for example, said Published: 2015-12-18, less than a month old at the time of writing.
• Is it well documented? This is not only an indication of how much thought, care and attention
has gone into the package. It also has a direct impact on its ease of use. Using a poorly documented
package can be ineﬃcient due to the hours spent trying to work out how to use it! To check if the
package is well documented, look at the help pages associated with its key functions (e.g. ?ggplot),
try the examples (e.g. example(ggplot)) and search for package vignettes (e.g. vignette(package =
"ggplot2")).
• Is it well used? This can be seen by searching for the package name online. Most packages that
have a strong user base will produce thousands of results when typed into a generic search engine such
as Google’s. More speciﬁc (and potentially useful) indications of use will narrow down the search to
particular users. A package widely used by the programming community will likely visible GitHub. At
the time of writing a search for ggplot2 on GitHub yielded over 400 repositories and almost 200,000
matches in committed code! Likewise, a package that has been adopted for use in academia will tend
to be mentioned in Google Scholar (again, ggplot2 scores extremely well in this measure, with over
5000 hits).
An article in simplystats discusses this issue with reference to the proliferation of GitHub packages (those
that are not available on CRAN). In this context well-regarded and experienced package creators and ‘indirect
data’ such as amount of GitHub activity are also highlighted as reasons to trust a package.
4.3
Importing data
Before reading in data, it is worth considering a general principle for reproducible data management: never
modify raw data ﬁles. Raw data should be seen as read-only, and contain information about its provenance.
Keeping the original ﬁle name and including a comment about its origin are a couple of ways to improve
reproducibility, even when the data are not publicly available. This is illustrated below with functions
download.file7 and unzip to download and unzip the dataset. This illustrates how R can automate
processes that are often performed manually, e.g. through the graphical user interface of a web browser. The
result of the code below is data stored neatly in the data directory ready to be read-in (note part of the
dataset is also stored in the eﬃcient package).
The data downloaded below is a multi-table dataset on Dutch naval expeditions used with permission
from the CWI Database Architectures Group and described more fully at monetdb.org. From this
dataset we primarily use the ‘voyages’ table with lists Dutch shipping expeditions by their date of
departure.
7Since R 3.2.3 the base function download.file() can be used to download from secure (https://) connections on any
operating system.

4.3. IMPORTING DATA
63
url = "https://www.monetdb.org/sites/default/files/voc_tsvs.zip"
download.file(url, "voc_tsvs.zip") # download file
unzip("voc_tsvs.zip", exdir = "data") # unzip files
file.remove("voc_tsvs.zip") # tidy up by removing the zip file
To avoid the ﬁle download stage, many functions for reading in data can accept urls and read
directly from the internet. This is illustrated below for read.csv():
url = "https://www.osha.gov/dep/fatcat/FatalitiesFY10.csv"
df = read.csv(url)
There are now many R packages to assist with the download and import of data. The organisation ROpenSci
supports a number of these. The example below illustrates this using the WDI package (not supported by
ROpenSci) which accesses the World Bank’s World Development Indicators:
library("WDI") # load the WDI library (must be installed)
WDIsearch("CO2") # search for data on a topic
df = WDI(indicator = "EN.CO2.TRAN.ZS" ) # import data
There will be situations where you cannot download the data directly or when the data cannot be made
available. In this case, simply providing a comment relating to the data’s origin (e.g. # Downloaded from
http://example.com) before referring to the dataset can greatly improve the utility of the code to yourself
and others.
4.3.1
Fast data reading
There is often more than one way to read data into R. Even a simple .csv ﬁle can be imported using a range
of methods, with implications for computational eﬃciency. This section looks at three approaches: base
R’s plain text reading functions such as read.delim, which are derived from read.table; the data.table
approach, which uses the function fread; and the newer readr package which provides read_csv and other
read_ functions such as read_tsv.
Note that a function ‘derived from’ another in this context means that it calls another function.
The functions such as read.csv and read.delim in fact are wrappers for the more generic function
read.table. This can be seen in the source code of read.csv, for example, which shows that the
function is roughly the equivalent of read.table(file, header = TRUE, sep = ",".
Although this section is focussed on reading text ﬁles, it demonstrate the wider principle that the speed and
ﬂexibility advantages of additional read functions can be oﬀset by the disadvantages of addition package
dependency (in terms of complexity and maintaining the code) for small datasets. The real beneﬁts kick
in on large datasets. Of course, there are some data types that require a certain package to load in R: the
readstata13 package, for example, was developed solely to read in .dta ﬁles generated by versions of Stata 13
and above.
Figure 4.3 demonstrates that the relative performance gains of the data.table and readr approaches increase
with data size, especially so for data with many rows. Below around 1 MB read.delim is actually faster

64
CHAPTER 4. EFFICIENT WORKFLOW
than read_csv while fread is much faster than both, although these savings are likely to be inconsequential
for such small datasets.
For ﬁles beyond 100 MB in size fread and read_csv can be expected to be around 5 times faster than
read.delim. This eﬃciency gain may be inconsequential for a one-oﬀﬁle of 100 MB running on a fast
computer (which still take less than a minute with read.csv), but could represent an important speed-up if
you frequently load large text ﬁles.
No of columns: 2
No of columns: 20
No of columns: 200
0
5
10
15
1
10
1
10
1
10
File size (MB)
Relative time
base
fread
readr
Figure 4.3: Benchmarks of base, data.table and readr functions for reading csv ﬁles. The facets ranging
from 2 to 200 represent the number of columns in the csv ﬁle..
When tested on a large (4 GB) .csv ﬁle it was found that fread and read_csv were almost identical in load
times and that read.csv took around 5 times longer. This consumed more than 10 GB of RAM, making it
unsuitable to run on many computers (see Section 8.3 for more on memory). Note that both readr and base
methods can be made signiﬁcantly faster by pre-specifying the column types at the outset, e.g.
read.csv(file_name, colClasses = c("numeric", "numeric"))
See the associated help ﬁles for further details.
In some cases with R programming there is a trade-oﬀbetween speed and robustness. This is illustrated
below with reference to diﬀerences in how readr, data.table and base R approaches handle unexpected
values. Table 4.1 shows that read_tsv is around 3 times faster, re-enforcing the point that the beneﬁts of
eﬃcient functions increase with dataset size (made with Figure 4.3). This is a small (1 MB) dataset: the
relative diﬀerence between fread and read_ functions will tend to decrease as dataset size increases.
library("microbenchmark")
library("readr")
library("data.table")
fname = system.file("extdata/voc_voyages.tsv", package = "efficient")
res_v = microbenchmark(times = 10,

4.3. IMPORTING DATA
65
Table 4.1: Execution time of base, **readr** and **data.table** functions for reading in a 1 MB dataset
relative to the mean execution time of ‘fread‘, around 0.02 seconds on a modern computer.
Function
min
mean
max
base_read
10.7
11.1
11.4
readr_read
3.4
4.7
13.5
dt_fread
1.0
1.0
1.0
base_read = voyages_base <- read.delim(fname),
readr_read = voyages_readr <- read_tsv(fname),
dt_fread = voyages_dt <- fread(fname))
The benchmark above produces warning messages (not shown) for the read_tsv and fread functions
but not the slowest base function read.delim. An exploration of these functions can shed light on the
speed/robustness trade-oﬀ.
• The readr function read_csv generates a warning for row 2841 in the built variable. This is because
read_*() decides what class each variable is based on the ﬁrst 1000 rows, rather than all rows, as base
read.* functions do.
As illustrated by printing the result for the row which generated a warning, the read_tsv output is more
sensible than the read.delim output: read.delim coerced the date ﬁeld into a factor based on a single entry
which is a text. read_tsv coerced the variable into a numeric vector, as illustrated below.
class(voyages_base$built) # coerced to a factor
#> [1] "factor"
class(voyages_readr$built) # numeric based on first 1000 rows
#> [1] "numeric"
voyages_base$built[2841] # contains the text responsible for coercion
#> [1] 1721-01-01
#> 182 Levels:
1 784 1,86 1135 1594 1600 1612 1613 1614 1615 1619 ... taken 1672
voyages_readr$built[2841] # an NA: text cannot be converted to numeric
#> [1] NA
• The data.table function fread generates 5 warning messages stating that columns 2, 4, 9, 10 and
11 were Bumped to type character on data row ..., with the oﬀending rows printed in place of
.... Instead of changing the oﬀending values to NA, as readr does for the built column (9), fread
automatically converts any columns it thought of as numeric into characters.
To summarise, the diﬀerences between base, readr and data.table functions for reading in data go beyond
code execution times. The functions read_csv and fread boost speed partially at the expense of robustness
because they decide column classes based on a small sample of available data. The similarities and diﬀerences
between the approaches are summarised for the Dutch shipping data (described in a note at the beginning of
this section) in Table 4.2.
Table 4.2 shows 4 main similarities and diﬀerences between the three read types of read function:
• For uniform data such as the ‘number’ variable in Table 4.2, all reading methods yield the same result
(integer in this case).
• For columns that are obviously characters such as ‘boatname’, the base method results in factors (unless
stringsAsFactors is set to TRUE) whereas fread and read_csv functions return characters.

66
CHAPTER 4. EFFICIENT WORKFLOW
Table 4.2: Execution time of base, **readr** and **data.table** functions for reading in a 1 MB dataset
Function
number
boatname
built
departure_date
base_read
integer
factor
factor
factor
readr_read
integer
character
numeric
Date
dt_fread
integer
character
character
character
• For columns in which the ﬁrst 1000 rows are of one type but which contain anomalies, such as ‘built’
and ‘departure_data’ in the shipping example, fread coerces the result to characters. read_csv and
siblings, by contrast, keep the class that is correct for the ﬁrst 1000 rows and sets the anomalous records
to NA. This is illustrated in 4.2, where read_tsv produces a numeric class for the ‘built’ variable,
ignoring the non numeric text in row 2841.
• read_* functions generate objects of class tbl_df, an extension of the data.frame, as discussed in
Section 4.5. fread generates objects of class data.table. These can be used as standard data frames
but diﬀer subtly in their behaviour.
The wider point associated with these tests is that functions that save time can also lead to additional
considerations or complexities your workﬂow. Taking a look at what is going on ‘under the hood’ of fast
functions to increase speed, as we have done in this section, can help understand the knock-on consequences
of choosing fast functions over slower functions from base R.
4.3.2
Preprocessing outside R
There are circumstances when datasets become too large to read directly into R. Reading in 4 GB text
ﬁle using the functions tested above, for example, consumed all available RAM on an 16 GB machine! To
overcome the limitation that R reads all data directly into RAM, external stream processing tools can be
used to preprocess large text ﬁles. The following command, using the shell command split, for example,
would break a large multi GB ﬁle many one GB chunks, each of which is more manageable for R:
split -b100m bigfile.csv
The result is a series of ﬁles, set to 100 MB each with the -b100m argument in the above code. By default
these will be called xaa, xab and which could be read in one chunk at a time (e.g. using read.csv, fread or
read_csv, described in the previous section) without crashing most modern computers.
Splitting a large ﬁle into individual chunks may allow it to be read into R. This is not an eﬃcient way to
import large datasets, however, because it results in a non-random sample of the data this way. A more
eﬃcient way to work with very large datasets is via databases.
4.3.3
Working with databases
Instead of loading all the data into RAM, as R does, databases query data from the hard-disk. This can
allow a subset of a very large dataset to be deﬁned and read into R quickly, without having to load it ﬁrst.
R can connect to databases in a number of ways. The most mature of these is via the RODBC package, which
sets up links to external databases using the Open Database Connectivity (ODBC) API. The functionality of
RODBC is described in the package’s vignette, accessed with vignette("RODBC"). RODBC connects to
‘traditional’ databases such as MySQL, PostgreSQL, Oracle and SQLite.
The function used to set-up a connection to an external database with RODBC is odbcConnect, which takes
Data Source Name (dsn =), User ID (uid =) and password (pwd) as required arguments.

4.4. TIDYING DATA WITH TIDYR
67
Be sure never to release your password by entering it directly into the command. Instead, we
recommend saving sensitive information such as database passwords and API keys in .Renviron,
described in 2.4.6. Assuming you had saved your password as the environment variable PSWRD, you
could enter pwd = Sys.getenv("PSWRD") to minimise the risk of exposing your password through
accidentally releasing the code or your session history.
Recently there has been a shift to the ‘noSQL’ approach for storing large datasets. This is illustrated by
the emergence and uptake of software such as MongoDB and Apache Cassandra, which have R interfaces
via packages mongolite and RJDBC, which can connect to Apache Cassandra data stores and any source
compliant with the Java Database Connectivity (JDBC) API.
MonetDB is a recent alternative to traditional and noSQL approaches which oﬀers substantial eﬃciency
advantages for handling large datasets (Kersten et al. 2011). A tutorial on the MonetDB website provides an
excellent introduction to handling databases from within R. A new development showcased in this tutorial is
the ability to interact with databases using exactly the same syntax used to interact with R objects stored in
RAM. This innovation was made possible by dplyr, an R library for data processing that aims to provide a
uniﬁed ‘front end’ to perform a wide range of analysis task on datasets using a variety of ‘back ends’ which
do the number crunching. This is one of the main advantages of dplyr (see Section 4.5).
4.4
Tidying data with tidyr
A key skill in data analysis is understanding the structure of datasets and being able to ‘reshape’ them. This
is important from a workﬂow eﬃciency perspective: more than half of a data analyst’s time can be spent
re-formatting datasets (Wickham 2014b). Converting data into a ‘tidy’ form is also advantageous from a
computational eﬃciency perspective: it is usually faster to run analysis and plotting commands on a few large
vectors than many short vectors. This is illustrated by Tables 4.3 and 4.4, provided by Wickham (2014b).
These tables may look diﬀerent, but they contain precisely the same information. Column names in the ‘ﬂat’
form in Table 4.3 became a new variable in the ‘long’ form in Table 4.4. According to the concept of ‘tidy
data’, the long form is correct. Note that ‘correct’ here is used in the context of data analysis and graphical
visualisation. For tabular presentation the ‘wide’ or ‘untidy’ form may be better.
Tidy data has the following characteristics (Wickham 2014b):
1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.
Because there is only one observational unit in the example (religions), it can be described in a single table.
Large and complex datasets are usually represented by multiple tables, with unique identiﬁers or ‘keys’ to
join them together (Codd 1979).
Two common operations facilitated by tidyr are gathering and splitting columns.
• Gathering: this means making ‘wide’ tables ‘long’ by converting column names to a new variable. This
is done is done with the function gather (the inverse of which is spread) , as illustrated in Table 4.3
and Table 4.4 and in the code block below:

68
CHAPTER 4. EFFICIENT WORKFLOW
Table 4.3: First 6 rows of the aggregated ’pew’ dataset from Wickham (2014a) in an ’untidy’ form.
religion
<$10k
$10–20k
$20–30k
Agnostic
27
34
60
Atheist
12
27
37
Buddhist
27
21
30
Table 4.4: First 3 and last rows of the ’tidied’ Pew dataset.
religion
Income
Count
Agnostic
<$10k
27
Atheist
<$10k
12
Buddhist
<$10k
27
...
...
...
Unaﬃliated
>150k
258
library("tidyr")
raw = read_csv("data/pew.csv") # read in the 'wide' dataset
dim(raw)
#> [1] 18 10
rawt = gather(raw, Income, Count, -religion)
dim(rawt)
#> [1] 162
3
rawt[1:3,]
#> Source: local data frame [3 x 3]
#>
#>
religion Income Count
#>
(chr)
(chr) (int)
#> 1 Agnostic
<$10k
27
#> 2
Atheist
<$10k
12
#> 3 Buddhist
<$10k
27
Note that the dimensions of the data change from having 10 observations across 18 columns to 162
rows in only 3 columns. Note that when we print the object rawt[1:3,], the class of each variable
is given (chr, fctr, int refer to character, factor and integer classes, respectively). This is because
read_csv uses the tbl class from the dplyr package (described below).
• Splitting: this means taking a variable that is really two variables combined and creating two separate
columns from it. A classic example is age-sex variables (e.g. m0-10 and f0-15 to represent males and
females in the 0 to 10 age band). Splitting such variables can be done with separate, as illustrated in
Table 4.5 and 4.6.
agesex = c("m0-10", "f0-10") # create compound variable
n = c(3, 5) # create a value for each observation
df = data.frame(agesex, n) # create a data frame
separate(df, agesex, c("sex", "age"), 1)
#>
sex
age n

4.5. DATA PROCESSING WITH DPLYR
69
Table 4.5: Joined age and sex variables in one column
agesex
n
m0-10
3
f0-10
5
Table 4.6: Age and sex variables separated by the funtion ‘separate‘.
sex
age
n
m
0-10
3
f
0-10
5
#> 1
m 0-10 3
#> 2
f 0-10 5
There are other tidying operations that tidyr can perform, as described in the package’s vignette
(vignette("tidy-data")).
Data manipulation is a large topic with major potential implications for
eﬃciency, and there is an entire book on the subject (Spector 2008).
4.5
Data processing with dplyr
Tidy data is easier and often faster to process than messy data. As with many aspects of R programming
there are many ways to process a dataset, some more eﬃcient than others. Following our own advice, we
have selected a package for data processing early on (see Section 4.2): dplyr. This package, which rougly
means ‘data pliers’ or ‘plyr’ (another R package) for large datasets, has a number of advantages compared
with base R and data.table approaches to data processing:
• dplyr is fast to run and intuitive to type
• dplyr works well with tidy data, as described above
• dplyr works well with databases, providing eﬃciency gains on large datasets
We will illustrate the functioning of dplyr with reference to a dataset on economic equality provided by the
World Bank. This is loaded in the following code block:
library("readr")
fname = system.file("extdata/world-bank-ineq.csv", package = "efficient")
idata = read_csv(fname)
idata # print the dataset
dplyr is much faster than base implementations of various operations, but it has the potential to be even
faster, as parallelisation is planned and the multidplyr package, a parallel backend for dplyr, is under
development.
You should not be expecting to learn the dplyr package in one sitting: the package is large and can be seen
as a language in its own right. Following the ‘walk before you run’ principle, we’ll start simple, by ﬁltering
and aggregating rows, building on the previous section on tidying data.

70
CHAPTER 4. EFFICIENT WORKFLOW
4.5.1
Renaming columns
Renaming data columns is a common task that can make writing code faster by using short, intuitive names.
The dplyr function rename() makes this easy.
Note in this code block the variable name is surrounded by back-quotes (). This allows R to refer to
column names that are non-standard. Note also the syntax:renametakes thedata.frameas the
first object and then creates new variables by specifyingnew_variable_name = original_name‘.
library("dplyr")
idata = rename(idata, Country = `Country Name`)
To rename multiple columns the variable names are simply separated by commas. The base R and dplyr
way of doing this is illustrated for clarity.
# The dplyr way (rename two variables)
idata = rename(idata,
top10 = `Income share held by highest 10% [SI.DST.10TH.10]`,
bot10 = `Income share held by lowest 10% [SI.DST.FRST.10]`)
# The base R way (rename five variables)
names(idata)[5:9] = c("top10", "bot10", "gini", "b40_cons", "gdp_percap")
Now we have usefully renamed the object we save the result for future reference:
saveRDS(idata, "data/idata-renamed.Rds")
4.5.2
Changing column classes
The class of R objects is critical to performance. If a class is incorrectly speciﬁed (e.g. if numbers are treated
as factors or characters) this will lead to incorrect results. The class of all columns in a data.frame can be
queried using the function sapply(), as illustrated below, with the inequality data loaded previously.
idata = readRDS("data/idata-renamed.Rds")
sapply(idata, class)
#>
Country Country Code
Year
Year Code
top10
#>
"character"
"character"
"integer"
"character"
"character"
#>
bot10
gini
b40_cons
gdp_percap
#>
"character"
"character"
"character"
"character"
This shows that although we loaded the data correctly all columns are seen by R as characters. This means
we cannot perform numerical calculations on the dataset: mean(idata$gini) fails.
Visual inspection of the data (e.g. via View(idata)) clearly shows that all columns except for 1 to 4
(“Country”, “Country Code”, “Year” and “Year Code”) should be numeric. We can re-assign the classes of
the numeric variables one-by one:
idata$gini = as.numeric(idata$gini)
#> Warning: NAs introduced by coercion
mean(idata$gini, na.rm = TRUE) # now the mean is calculated
#> [1] 40.5
However, the purpose of programming languages is to automate tasks and reduce typing. The following
code chunk re-classiﬁes all of the numeric variables using data.matrix(), which converts a data.frame to a
numeric matrix:

4.5. DATA PROCESSING WITH DPLYR
71
id = 5:9 # column ids to change
idata[id] = data.matrix(idata[id])
sapply(idata, class)
#>
Country Country Code
Year
Year Code
top10
#>
"character"
"character"
"integer"
"character"
"numeric"
#>
bot10
gini
b40_cons
gdp_percap
#>
"numeric"
"numeric"
"numeric"
"numeric"
As is so often the case with R, there are many ways to solve the problem. Below is a one-liner using unlist()
which converts list objects into vectors:
idata[id] = as.numeric(unlist(idata[id]))
Another one-liner to acheive the same result uses dplyr’s mutate_each function:
idata = mutate_each(idata, funs(as.numeric), id)
As with other operations there are other ways of achieving the same result in R, including the use of loops
via apply() and for(). These are shown in the chapter’s source code.
4.5.3
Filtering rows
The standard way to subset data by rows in R is with square brackets, for example:
aus1 = idata[idata$Country == "Australia",]
dplyr oﬀers an alternative and more ﬂexible way of ﬁltering data, using filter().
aus2 = filter(idata, Country == "Australia")
In addition to being more ﬂexible (see ?filter), filter is slightly faster than base R’s notation.8 Note that
dplyr does not use the $ symbol: it knows that that Country is a variable of idata: the ﬁrst argument
of dplyr functions usually a data.frame, and subsequent in this context variable names can be treated as
vector objects.9
There are dplyr equivalents of many base R functions but these usually work slightly diﬀerently. The dplyr
equivalent of aggregate, for example is to use the grouping function group_by in combination with the
general purpose function summarise (not to be confused with summary in base R), as we shall see in Section
4.5.5. For consistency, however, we next look at ﬁltering columns.
4.5.4
Filtering columns
Large datasets often contain much worthless or blank information.
This consumes RAM and reduces
computational eﬃciency. Being able to focus quickly only on the variables of interest becomes especially
important when handling large datasets.
Imagine that we have a text ﬁle called miniaa which is large enough to consume most of your computer’s
RAM. We can load it with the following command:
8Note that filter is also the name of a function used in the base stats library. Usually packages avoid using names already
taken in base R but this is an exception.
9Note that this syntax is a deﬁning feature of dplyr and many of its functions work in the same way. Later we’ll learn how
this syntax can be used alongside the %>% ‘pipe’ command to write clear data manipulation commands.

72
CHAPTER 4. EFFICIENT WORKFLOW
fname = system.file("extdata/miniaa", package = "efficient")
df = read.csv(fname) # load imaginary large data
dim(df)
#> [1]
9 329
Note that the data frame has 329 columns, and imagine it has millions of rows, instead of 9. That’s a lot of
variables. Do we need them all? It’s worth taking a glimpse at this dataset to ﬁnd out:
glimpse(df)
#> $ NPI
(int) 1679576722, ...
#> $ Entity Type Code
(int) 1, 1, 2,
...
#> $ Replacement NPI
(lgl) NA, NA, NA, ...
#> ...
Looking at the output, it becomes clear that the majority of the variables only contain NA. To clean the giant
dataset, removing the empty columns, we need to identify which variables these are.
# Identify the variable which are all NA
all_na = sapply(df, function(x) all(is.na(x)))
summary(all_na) # summary of the results
#>
Mode
FALSE
TRUE
NA's
#> logical
96
233
0
df1 = df[!all_na] # subset the dataframe
The new df object has fewer than a third of the original columns. Another way to save storage space, beyond
removing the superﬂuous columns, is to save the dataset in R’s binary data format:
saveRDS(df1, "data/miniaa.Rds")
4.5.4.1
Exercises
1. How much space was saved by reducing the number of columns? (Hint: use object.size().)
2. How many times smaller is the .Rds ﬁle saved above compared with the .csv ﬁle?
(Hint: use
file.size().)
4.5.5
Data aggregation
Data aggregation is the process of creating summaries of data based on a grouping variable. The end result
usually has the same number of rows as there are groups. Because aggregation is a way of condensing datasets
it can be a very useful technique for making sense of large datasets. The following code ﬁnds the number
of unique countries (country being the grouping variable) from the ‘GHG’ dataset stored in the eﬃcient
package.
The GHG dataset used in the subsequent code reports the amount of greenhouse gas emissions
emitted by country and by year for the major economic sectors. It was provided by the World
Resources Institute and is available in raw form from their website: wri.org/resources/data-sets/.

4.5. DATA PROCESSING WITH DPLYR
73
fname = system.file("extdata/ghg-ems.csv", package = "efficient")
df = read.csv(fname)
names(df)
#> [1] "X"
#> [2] "Country"
#> [3] "Year"
#> [4] "Electricity.Heat..CO2...MtCO2."
#> [5] "Manufacturing.Construction..CO2...MtCO2."
#> [6] "Transportation..CO2...MtCO2."
#> [7] "Other.Fuel.Combustion..CO2...MtCO2."
#> [8] "Fugitive.Emissions..CO2...MtCO2."
nrow(df)
#> [1] 7896
length(unique(df$Country))
#> [1] 188
Note that while there are almost 8000 rows, there are less than 200 countries. Referring back to Section 4.5.1,
the next stage should be to rename the columns so they are more convenient to work with. Having checked
the verbose column names, this can be done in base R using the following command:
names(df)[4:8] = c("ECO2", "MCO2", "TCO2", "OCO2", "FCO2")
After the variable names have been updated, we can aggregate.10
e_ems = aggregate(df$ECO2, list(df$Country), mean, na.rm
= TRUE)
nrow(e_ems)
#> [1] 188
Note that the resulting data frame now has the same number of rows as there are countries: the aggregation
has successfully reduced the number of rows we need to deal with. Now it is easier to ﬁnd out per-country
statistics, such as the three lowest emitters from electricity production:
head(e_ems[order(e_ems$x),], 3)
#>
Group.1
x
#> 77
Iceland 0.0179
#> 121
Nepal 0.0233
#> 18
Benin 0.0464
Another way to specify the by argument is with the tilde (~). The following command creates the same
object as e_ems, but with less typing.
e_ems = aggregate(ECO2 ~ Country, df, mean, na.rm
= TRUE)
To aggregate the dataset using dplyr package one would divide the task in two: to group the dataset ﬁrst
and then to summarise, as illustrated below:
10Note the ﬁrst argument in the function is the vector we’re aiming to aggregate and the second is the grouping variable (in
this case Countries). A quirk of R is that the grouping variable must be supplied as a list. Next we’ll see a way of writing this
that is neater.

74
CHAPTER 4. EFFICIENT WORKFLOW
library("dplyr")
group_by(df, Country) %>%
summarise(mean_eco2 = mean(ECO2, na.rm
= TRUE))
#> Source: local data frame [188 x 2]
#>
#>
Country mean_eco2
#>
(fctr)
(dbl)
#> 1
Afghanistan
NaN
#> 2
Albania
0.641
#> 3
Algeria
23.015
#> 4
Angola
0.791
#> 5
Antigua & Barbuda
NaN
#> ..
...
...
countries = group_by(idata, Country)
summarise(countries, gini = mean(gini, na.rm
= TRUE))
#> Source: local data frame [176 x 2]
#>
#>
Country
gini
#>
(chr) (dbl)
#> 1
Afghanistan
NaN
#> 2
Albania
30.4
#> 3
Algeria
37.8
#> 4
Angola
50.6
#> 5
Argentina
48.1
#> ..
...
...
Note that summarise is highly versatile, and can be used to return a customised range of summary statistics:
summarise(countries,
# number of rows per country
obs = n(),
med_t10 = median(top10, na.rm
= TRUE),
# standard deviation
sdev = sd(gini, na.rm
= TRUE),
# number with gini > 30
n30 = sum(gini > 30, na.rm
= TRUE),
sdn30 = sd(gini[ gini > 30 ], na.rm
= TRUE),
# range
dif = max(gini, na.rm
= TRUE) - min(gini, na.rm
= TRUE)
)
#> Source: local data frame [176 x 7]
#>
#>
Country
obs med_t10
sdev
n30
sdn30
dif
#>
(chr) (int)
(dbl) (dbl) (int)
(dbl) (dbl)
#> 1
Afghanistan
40
NA
NaN
0
NA
NA
#> 2
Albania
40
24.4
1.25
3
0.364
2.78
#> 3
Algeria
40
29.8
3.44
2
3.437
4.86
#> 4
Angola
40
38.6 11.30
2 11.300 15.98
#> 5
Argentina
40
36.3
3.18
23
3.182 11.00
#> ..
...
...
...
...
...
...
...
To showcase the power of summarise used on a grouped_df, the above code reports a wide range of customised
summary statistics per country:

4.5. DATA PROCESSING WITH DPLYR
75
• the number of rows in each country group
• standard deviation of gini indices
• median proportion of income earned by the top 10%
• the number of years in which the gini index was greater than 30
• the standard deviation of gini index values over 30
• the range of gini index values reported for each country.
4.5.5.1
Exercises
1. Referring back to Section 4.5.1, rename the variables 4 to 8 using the dplyr function rename. Follow
the pattern ECO2, MCO2 etc.
2. Explore dplyr’s documentation, starting with the introductory vignette, accessed by entering
vignette("introduction").
3. Test additional dplyr ‘verbs’ on the idata dataset. (More vignette names can be discovered by typing
vignette(package = "dplyr").)
4.5.6
Chaining operations
Another interesting feature of dplyr is its ability to chain operations together. This overcomes one of the
aesthetic issues with R code: you can end end-up with very long commands with many functions nested
inside each other to answer relatively simple questions.
What were, on average, the 5 most unequal years for countries containing the letter g?
Here’s how chains work to organise the analysis in a logical step-by-step manner:
idata %>%
filter(grepl("g", Country)) %>%
group_by(Year) %>%
summarise(gini = mean(gini, na.rm
= TRUE)) %>%
arrange(desc(gini)) %>%
top_n(n = 5)
#> Selecting by gini
#> Source: local data frame [5 x 2]
#>
#>
Year
gini
#>
(int) (dbl)
#> 1
1980
46.9
#> 2
1993
46.0
#> 3
2013
44.5
#> 4
1981
43.6
#> 5
2012
43.6
The above function consists of 6 stages, each of which corresponds to a new line and dplyr function:
1. Filter-out the countries we’re interested in (any selection criteria could be used in place of grepl("g",
Country)).
2. Group the output by year.
3. Summarise, for each year, the mean gini index.
4. Arrange the results by average gini index

76
CHAPTER 4. EFFICIENT WORKFLOW
5. Select only the top 5 most unequal years.
To see why this method is preferable to the nested function approach, take a look at the latter. Even after
indenting properly it looks terrible and is almost impossible to understand!
top_n(
arrange(
summarise(
group_by(
filter(idata, grepl("g", Country)),
Year),
gini = mean(gini, na.rm
= TRUE)),
desc(gini)),
n = 5)
This section has provided only a taster of what is possible dplyr and why it makes sense from code writing
and computational eﬃciency perspectives. For a more detailed account of data processing with R using this
approach we recommend R for Data Science (Grolemund and Wickham 2016).
4.6
Data processing with data.table
data.table is a mature package for fast data processing that presents an alternative to dplyr. There is
some controversy about which is more appropriate for diﬀerent tasks11 so it should be stated at the outset
that dplyr and data.table are not mutually exclusive competitors. Both are excellent packages and the
important thing from an eﬃciency perspective is that they can help speed up data processing tasks.
The foundational object class of data.table is the data.table.
Like dplyr’s tbl_df, data.table’s
data.table objects behave in the same was as the base data.frame class.
However the data.table
paradigm has some unique features that make it highly computationally eﬃcient for many common tasks in
data analysis. Building on subsetting methods using [ and filter() presented in Section 4.5.4, we’ll see
data.tables’s unique approach to subsetting. Like base R data.table uses square brackets but you do not
need to refer to the object name inside the brackets:
library("data.table")
idata = readRDS("data/idata-renamed.Rds")
idata_dt = data.table(idata) # convert to data.table class
aus3a = idata_dt[Country == "Australia"]
To boost performance, one can set ‘keys’. These are ‘supercharged rownames’ which order the table based on
one or more variables. This allows a binary search algorithm to subset the rows of interest, which is much,
much faster than the vector scan approach used in base R (see vignette("datatable-keys-fast-subset")).
data.table uses the key values for subsetting by default so the variable does not need to be mentioned again.
Instead, using keys, the search criteria is provided as a list (invoked below with the concise .() syntax below).
setkey(idata_dt, Country)
aus3b = idata_dt[.("Australia")]
The result is the same, so why add the extra stage of setting the key? The reason is that this one-oﬀsorting
operation can lead to substantial performance gains in situations where repeatedly subsetting rows on large
datasets consumes a large proportion of computational time in your workﬂow. This is illustrated in Figure
4.4, which compares 4 methods of subsetting incrementally larger versions of the idata dataset.
11One question on the stackoverﬂow website titled ‘data.table vs dplyr’ illustrates this controversey and delves into the
philosophy underlying each approach.

4.7. PUBLICATION
77
Time (Relative to the fastest)
Size (MB)
base
dplyr
DT:Key
DT
0
400
0
20
40
60
80
Subsetting comparison
Figure 4.4: Benchmark illustrating the performance gains to be expected for diﬀerent dataset sizes.
Figure 4.4 demonstrates that data.table is much faster than base R and dplyr at subsetting. As with
using external packages to read in data (see Section 4.3.1), the relative beneﬁts of data.table improve with
dataset size, approaching a ~70 fold improvement on base R and a ~50 fold improvement on dplyr as the
dataset size reaches half a Gigabyte. Interestingly, even the ‘non key’ implementation of data.table subset
method is faster than the alternatives: this is because data.table creates a key internally by default before
subsetting. The process of creating the key accounts for the ~10 fold speed-up in cases where the key has
been pre-generated.
This section has introduced data.table as a complimentary approach to base and dplyr methods for data
processing and illustrated the performance gains of using keys for subsetting tables. data.table is a mature
and powerful package which uses clever computational principles implemented in C to provide eﬃcient
methods for a number of other operations for data analysis. These include highly eﬃcient data reshaping,
dataset merging (also known as joining, as with left_join in dplyr) and grouping. These are explained in
the vignettes datatable-intro and datatable-reshape. The datatable-reference-semantics vignette
explains data.table’s unique syntax.
4.7
Publication
The ﬁnal stage in a typical project workﬂow is publication. This could be a report containing graphics
produced by R, an online platform for exploring results or well-documented code that colleagues can use to
improve their workﬂow. In every case the programming principles of reproducibility, modularity and DRY
discussed in Chapter 3 will make your publications faster to write, easier to maintain and more useful to
others.
Instead of attempting a comprehensive treatment of the topic we will touch brieﬂy on a couple of ways of
documenting your work in R: dynamic reports and R packages. There is a wealth of material on each of
these online. A wealth of online resources exists on each of these; to avoid duplication of eﬀort the focus is on
documentation from a workﬂow eﬃciency perspective.

78
CHAPTER 4. EFFICIENT WORKFLOW
4.7.1
Dynamic documents with knitr
When writing a report using R outputs a typical workﬂow has historically been to 1) do the analysis 2) save
the resulting graphics and record the main results outside the R project and 3) open a program unrelated to
R such as LibreOﬃce to import and communicate the results in prose. This is ineﬃcient: it makes updating
and maintaining the outputs diﬃcult (when the data changes, steps 1 to 3 will have to be done again) and
there is an overhead involved in jumping between incompatible computing environments.
To overcome this ineﬃciency in the documentation of R outputs the knitr package was developed. Used in
conjunction with RStudio and building on a version of Markdown that accepts R code (RMarkdown, saved
as .Rmd ﬁles) knitr allows for documents to be generated automatically. Results are generated on the ﬂy by
including ‘code chunks’ such as that illustrated below:
(1:5)^2
#> [1]
1
4
9 16 25
The resulting output is evaluated each time the document is compiled. To tell knitr that (1:5)ˆ2 is R code
that needs to be evaluated, it must by preceded by “‘{r} on the line before the R code, and “‘ at the end of
the chunk. When you adapt to this workﬂow it is highly eﬃcient, especially as RStudio provides a number of
shortcuts that make it easy to create and modify code chunks. When the data or analysis code changes, the
results will be updated in the document automatically. This can save hours of ﬁddly copying and pasting of
R output between diﬀerent programs.
Furthermore dynamic documents written in RMarkdown can compile into a range of output formats including
html, pdf and Microsoft’s docx. There is a wealth of information on the details of dynamic report writing
that is not worth replicating here. Key references are RStudio’s excellent website on RMarkdown hosted at
rmarkdown.rstudio.com/ and, for a more detailed account of dynamic documents with R, (Xie 2015).
4.7.2
R packages
A strict approach to project management and workﬂow is treating your projects as R packages. This is good
practice in terms of learning to correctly document your code, store example data, and even (via vignettes)
ensure reproducibility. This approach to R workﬂow is appropriate for managing complex projects which
repeatedly use the same routines which can be converted into functions. Creating project packages can
provide foundation for generalising your code for use by others, e.g. via publication on GitHub or CRAN.
The number of essential elements of R packages diﬀerentiate them from other R projects. Three of these are
outlined below from an eﬃciency perspective.
• The DESCRIPTION ﬁle contains key information about the package, including which packages are required
for the code contained in your package to work, e.g. using Imports:. This is eﬃcient because it means
that anyone who installs your package will automatically install the other packages that it depends on.
• The R/ folder contains all the R code that deﬁnes your package’s functions. Placing your code in a
single place and encouraging you to make your code modular in this way can greatly reduce duplication
of code on large projects. Furthermore the documentation of R packages through Roxygen tags such as
#' This function does this... makes it easy for others to use your work.
• The data/ folder contains example code for demonstrating to others how the functions work and
transporting datasets that will be frequently used in your workﬂow. Data can be added automatically
to your package project using the devtools package, with devtools::use_data(). This can increase
eﬃciency by providing a way of distributing small to medium sized datasets and making them available
when the package is loaded with the function data('data_set_name').

4.7. PUBLICATION
79
As with dynamic documents, package development is a large topic. For small ‘one-oﬀ’ projects the time taken
in learning how to set-up a package may not be worth the savings. However packages provide a rigourous
way of storing code, data and documentation that can greatly boost productivity in the long-run. For more
on R packages see (Wickham 2015).

80
CHAPTER 4. EFFICIENT WORKFLOW

Chapter 5
Eﬃcient data carpentry
81

82
CHAPTER 5. EFFICIENT DATA CARPENTRY

Chapter 6
Eﬃcient visualisation
6.1
Rough outline
• Eﬃcient base graphics
– Set up par function
– Custom palettes
• ggplot2
• Interactive graphics
– plotly, shiny, leaﬂet, htmlwidgets
6.2
Cairo type
These are just some thoughts/observations
When you use cairo to save plots, we get anti-aliasing. The standard graphic devices, png,and jpeg save
graphics as either c("cairo", "Xlib", "quartz"). By default, it goes for getOption("bitmapType").
Unix-alike OSs, such as Linux, the default is cairo. Under Windows, the default is NULL. This results in the
use of X11
For example, compare
set.seed(1)
x = rnorm(100)
png("fig1.png", type="cairo")
plot(x)
dev.off()
png("fig2.png", type="Xlib")
plot(x)
dev.off()
• Can you just force type="cairo" in Windows - probably not.
• If you use the cairo package under Windows, can you then access type="cairo". Or do you need to
use the cairo:: functions.
• No idea about Macs.
83

84
CHAPTER 6. EFFICIENT VISUALISATION
Search the web brings up a few out of date pages:
• http://gforge.se/2013/02/exporting-nice-plots-in-r/

Chapter 7
Eﬃcient performance
Donald Knuth is a famous American computer scientist who developed a number of the key alogorithms that
we used today. On the subject of optimisation he give this advice.
The real problem is that programmers have spent far too much time worrying about eﬃciency in
the wrong places and at the wrong times; premature optimization is the root of all evil (or at
least most of it) in programming.
That’s why in the previous chapters we’ve focused on tools and your working environment to increase eﬃciency.
These are (relatively) easy ways of saving time, that once implemented, work for future projects. In this
chapter we are going to consider code optimisation. Simply stated, making your program run faster.
In many scernios, a code is running slowly due to a couple of key bottlenecks. By speciﬁcally targetting these
key areas, we may be able to speed up the entire program.
In this chapter we will consider a variety of code tweaks that can eke out an extra speed boost. We’ll begin
with general hints and tips about using base R. Then look at code proﬁling to determine where our eﬀorts
should be focused. We conclude by looking at Rcpp, an eﬃcient way of incorparting C++ code into an R
analysis.
7.1
Eﬃcient base R
In R there is often more than one way to solve a problem. In this section we highlight standard tricks or
alternative methods that will improve performance.
The if vs ifelse functions
The ifelse function
ifelse(test, yes, no)
is a vectorised version of the standard control-ﬂow if function. The return value of ifelse is ﬁlled with
elements from the yes and no arguments that are determined by whether the element of test is TRUE or
FALSE.
If test has length 1, then the standard if(test) yes else no is more eﬃcient.
85

86
CHAPTER 7. EFFICIENT PERFORMANCE
Sorting and ordering
Sorting a vector a single vector is relatively quick; sorting a vector of length 107 takes around 0.01 seconds.
If you only sort a vector once at the top of a script, then don’t worry too much about this. However if you
sorting inside a loop, or in a shiny application, then it can be worthwhile thinking about how to optimise this
operation.
There are currently three sorting algorithms, c("shell", "quick", "radix") that can be speciﬁed in the
sort function; with radix being a new addition to R 3.3. Typically the radix (the non-default option) is
optimal for most situations.
Another useful trick is to partially order the results. For example, if you only want to display the top ten
results, then use the partial argument, i.e. sort(x, partial=1:10)
Reversing elements
The rev function provides a reversed version of its argument. If you wish to sort in increasing order, you
should use the more eﬃcient sort(x, decreasing=TRUE) instead of rev(sort(x)).
Which indices are TRUE
To determine which index of a vector or array are TRUE, we would typically use the which function. If we
want to ﬁnd the index of just the minimum or maximum value, i.e. which(x == min(x)) then use the more
eﬃcient which.min/which.max variants.
Converting factors to numerics
A factor is just a vector of integers with associated levels. Occasionally we want to convert a factor into its
numerical equivalent. The most eﬃcient way of doing this (especially for long factors) is
as.numeric(levels(f))[f]
where f is the factor.
String concatenation
To concatenate strings use the paste function
paste("A", "B")
The separation character is speciﬁed via the sep argument.
The function paste0(..., collapse) is
equivalent to paste(..., sep = "", collapse), but is slightly more eﬃcient.
Logical AND and OR
The logical AND (&) and OR (|) operators are vectorised functions and are typically used whenever we
perform subsetting operations. For example

7.1. EFFICIENT BASE R
87
x < 0.4 | x > 0.6
#> [1]
TRUE FALSE
TRUE
When R executes the above comparison, it will always calculate x > 0.6 regardless of the value of x < 0.4.
In contrast, the non-vectorised version, &&, only executes the second component if needed. This is eﬃcient
and leads to more neater code, e.g.
## read.csv is only executed if the file exists
file.exists("data.csv") && read.csv("data.csv")
#> [1] FALSE
However care must be taken not to use && or || on vectors since it will give the incorrect answer
x < 0.4 || x > 0.6
#> [1] TRUE
Row and column operations
In data analysis we often want to apply a function to each column or row of a data set. For example,
we might want to calculation the column or row sums. The apply function makes this type of operation
straightforward.
## Second argument: 1 -> rows. 2 -> columns
apply(data_set, 1, function_name)
There are optimised functions for calculating row and columns sums/means, rowSums, colSums, rowMeans
and colMeans that should be used whenever possible.
7.1.1
is.na and anyNA
To test for an missing values, whether a vector (or other obeject) contains missing values, NA’s, we use the
is.na function. Often we are interested in whether the vector contains any missing values. In this case,
anyNA(x) is usually more eﬃcient than any(is.na(x)).
Matrices
A matrix is similar to a data frame: it is a two dimensional object and sub-setting and other functions work
in the expected way. However all matrix elements must have the same type. Matrices tend to be used during
statistical calculations. Calculating the line of best ﬁt using the lm() function, internally converts the data
to a matrix before calculating the results; any characters are thus recoded as numeric dummy variables.
Matrices are generally faster than data frames. The datasets ex_mat and ex_df from the eﬃcient package
each have 1000 rows and 100 columns. They contain the same random numbers. However selecting rows
from a data frame is around 150 times slower than a matrix.
data(ex_mat, ex_df, package="efficient")
microbenchmark(times=100, unit="ms",
ex_mat[1,], ex_df[1,])

88
CHAPTER 7. EFFICIENT PERFORMANCE
Use the data.matrix function to eﬃciently convert a data frame into a matrix.
The integer data type
Numbers in R are usually stored in double-precision ﬂoating-point format - see Braun and Murdoch (2007)
and Goldberg (1991). The term ‘double’ refers to the fact that on 32 bit systems (for which the format
was developed) two memory locations are used to store a single number. Each double-precision number is
accurate to around 17 decimal places.
When comparing ﬂoating point numbers we should be particularly careful,
since y =
sqrt(2)*sqrt(2) is not exactly 2, instead it’s almost 2. Using sprintf("%.17f", y) will give
you the true value of y (to 17 decimal places).
There is also another data type, called an integer. Integers primarily exist to be passed to C or Fortran code.
Typically we don’t worry about creating integers, however they are occasionally used to optimise sub-setting
operations. When we subset a data frame or matrix, we are interacting with C code. For example, if we look
at the arguments for the head function
args(head.matrix)
#> function (x, n = 6L, ...)
#> NULL
Using the : operator automatically creates a vector of integers.
The default argument is 6L (the L, is short for Literal and is used to create an integer). Since this function is
being called by almost everyone that uses R, this low level optimisation is useful. To illustrate the speed
increase, suppose we are selecting the ﬁrst 100 rows from a data frame (clock_speed, from the eﬃcient
package). The speed increase is illustrated below, using the microbenchmark package:
s_int = 1:100; s_double = seq(1, 100, 1.0)
microbenchmark(clock_speed[s_int, 2L], clock_speed[s_double, 2.0], times=1000000)
## Unit: microseconds
## expr
min
lq
mean median
uq
max neval cld
## clock_speed[s_int, 2L] 11.79 13.43 15.30
13.81 14.22 87979 1e+06
a
## clock_speed[s_double, 2] 12.79 14.37 16.04
14.76 15.18 21964 1e+06
b
The above result shows that using integers is slightly faster, but probably not worth worrying about.
Integers are more space eﬃcient. If we compare size of a integer vector to a standard numeric vector

7.1. EFFICIENT BASE R
89
pryr::object_size(1:10000)
#> 40 kB
pryr::object_size(y = seq(1, 10000, by=1.0))
#> 80 kB
The integer version is (approximately) half the size. However most mathematical operations will convert the
integer vector into a standard numerical vector, e.g.
is.integer(1L + 1)
#> [1] FALSE
Sparse matrices
Another eﬃcient data structure is a sparse matrix. This is simply a matrix where most of the elements are
zero. Conversely, if most elements are non-zero, the matrix is considered dense. The proportion of non-zero
elements is called the sparsity. Large sparse matrices often crop up when performing numerical calculations.
Typically, our data isn’t sparse but the resulting data structures we create may be sparse. There are a number
of techniques/methods used to store sparse matrices. Methods for creating sparse matrices can be found
in the Matrix package[ˆTechnically this isn’t in base R, it’s a recommend package.]. For this dist object,
since the structure is regular.
As an example, suppose we have a large matrix where the diagonial elements are non-zero
library("Matrix")
#>
#> Attaching package: 'Matrix'
#> The following object is masked from 'package:tidyr':
#>
#>
expand
N = 10000
sp = sparseMatrix(1:N, 1:N, x = 1)
m = diag(1, N, N)
Both objects contain the same information, but the data is stored diﬀerently. The matrix object stores each
individual element, while the sparse matrix object only stores the location of the non-zero elements. This is
much more memory eﬃcient
pryr::object_size(sp)
#> 161 kB
pryr::object_size(m)
#> 800 MB
Exercises
1. Create a vector x. Benchmark any(is.na(x)) against anyNA. Do the results vary with the size of the
vector.
2. Examine the following function deﬁnitions to give you an idea of how integers are used.
• tail.matrix
• lm.

90
CHAPTER 7. EFFICIENT PERFORMANCE
1. How does the function seq.int, which was used in the tail.matrix function, diﬀer to the standard
seq function?
A related memory saving idea is to replace logical vectors with vectors from the bit package
which take up just over 16th of the space (but you can’t use NAs).
1. Contruct a matrix of integers and a matrix of numerics. Using pryr::object_size, compare the
objects.
7.2
Code proﬁling
A circumstance that happens all too often, is that we simply want our code to run faster. In some cases it’s
obvious where the bottle neck lies. Other times we have to rely on our intuition. One major drawback of
relying on our intuition is that we could be wrong and we end up pointlessly optimising the wrong piece of
code. To make slow code run faster, we ﬁrst need to determine where the slow code lives.
The Rprof function is built-in tool for proﬁling the execution of R expressions. At regular time intervals, the
proﬁler stops the R interpreter, records the current function call stack, and saves the information to a ﬁle.
The results from Rprof are stochastic. Each time we run a function R, the conditions have changed. Hence,
each time you proﬁle your code, the result will be slightly diﬀerent.
Unfortunately Rprof is not user friendly. The profvis package provides an interactive graphical interface for
visualising data data from Rprof.
7.2.1
Getting started with profvis
The profvis package can be installed in the usual way
install.packages("profvis")
As a simple example, we will use the movies data set, which contain information on around 60,000 movies
from the IMDB. First, we’ll select movies that are classed as comedies, then plot year vs movies rating, and
draw a local polynomial regression line to pick out the trend. The main function from the profvis package,
is profvis which proﬁles the code and creates an interactive HTML page. The ﬁrst argument of profvis is
the R expression of interest.
library("profvis")
profvis({
data(movies, package="ggplot2movies")
movies = movies[movies$Comedy==1, ]
plot(movies$year, movies$rating)
model = loess(rating ~ year, data=movies)
j = order(movies$year)
lines(movies$year[j], model$fitted[j])
})
The above code provides an interactive HTML page (ﬁgure ??).
On the left side is the code and on
the right is a ﬂame graph (horizontal direction is time in milliseconds and the vertical direction is the call stack).

7.2. CODE PROFILING
91
The left hand panel tells the amount of time spent on each line of code. We see that majority of time is
spent calculating the loess smoothing line. The bottom line of the right panel also highlights that most
of the execution time is spent on the loess function. Travelling up the function, we see that loess calls
simpleLoess which in turn calls .C function.
The conclusion from this graph, is that if optimisation were required, it would make sense to focus on the
loess and possibly the order functions calls.
7.2.2
Example: Monopoly Simulation
Monopoly is a board games that originated in the United States over 100 years ago. The object of the game
is to go round the board and purchase squares (properties). If other players land on your properties they
have to pay a tax. The player with the most money at the end of the game, wins. To make things more
interesting, there are Chance and Community Chest squares. If you land on one of these squares, you draw
card, which may send to you to other parts of the board. The other special square, is Jail. One way of
entering Jail is to roll three successive doubles.
The eﬃcient package contains a simulate monopoly function that attempts to ﬁnd the probability of landing
on a certain square using a Monte-Carlo algorithm. The entire code is around 100 lines long. In order for
profvis to fully proﬁle the code, the eﬃcient package needs to be installed from source
## args is also a valid argument for install.packages
devtools::install_github("csgillespie/efficient_pkg", args="--with-keep.source")
The function can then be proﬁled via
library("efficient")
profvis(SimulateMonopoly(10000))
to get

92
CHAPTER 7. EFFICIENT PERFORMANCE
The output from profvis shows that the vast majority of time is spent in the move function. In Monopoly
rolling a double (a pair of 1’s, 2’s, . . . , 6’s) is special:
• Roll two dice (total1): total_score = total1;
• If you get a double, roll again (total2) and total_score = total1 + total2;
• If you get a double, roll again (total3) and total_score = total1 + total2 + total3;
• If roll three is a double, Go To Jail, otherwise move total_score.
The move function spends around 50% of the time is spent creating a data frame, 25% time calculating
row sums, and the remainder on a comparison operations. This piece of code can be optimised fairly easily
(will still retaining the same overall structure) by incorporating the following improvements[ˆSolutions are
available in the eﬃcient package vignette.]:
• Use a matrix instead of a data frame;
• Sample 6 numbers at once, instead of two groups of three;
• Switch from apply to rowSums;
• Use && in the if condition.
Implementing this features, results in the following code.
move = function(current) {
## data.frame -> matrix
rolls = matrix(sample(1:6, 6, replace=TRUE), ncol=2)
Total = rowSums(rolls)
# apply -> rowSums
IsDouble = rolls[,1] == rolls[,2]
## & -> &&
if(IsDouble[1] && IsDouble[2] && IsDouble[3]) {
current = 11#Go To Jail
} else if(IsDouble[1] && IsDouble[2]) {
current = current + sum(Total[1:3])
} else if(IsDouble[1]) {
current = current + Total[1:2]
} else {

7.3. PARALLEL COMPUTING
93
current = Total[1]
}
current
}
which gives a 25-fold speed improvement.
Exercise
The move function uses a vectorised solution. Whenever we move, we always roll six dice, then examine the
outcome and determine the number of doubles. However, this is potentially wasteful, since the probability of
getting one double is 1/6 and two doubles is 1/36. Another method is too only roll additional dice if and
when they are needed. Implement and time this solution.
7.3
Parallel computing
This chapter provides a brief foray into the word of parallel computing and only looks at shared memory
systems. The idea is to give a ﬂavour of what is possible, instead of covering all possible varities. For a fuller
account, see (McCallum and Weston 2011).
In recent R versions (since R 2.14.0) the parallel package comes pre-installed with base R. The parallel
package must still be loaded before use however, and you must determine the number of available cores
manually, as illustrated below.
library("parallel")
no_of_cores = detectCores()
The computer used to compile the published version of this book chapter has r no_of_cores CPUs/Cores.
7.3.1
Parallel versions of apply functions
The most commonly used parallel applications are parallelised replacements of lapply, sapply and apply.
The parallel implementations and their arguments are shown below.
parLapply(cl, x, FUN, ...)
parApply(cl = NULL, X, MARGIN, FUN, ...)
parSapply(cl = NULL, X, FUN, ..., simplify = TRUE, USE.NAMES = TRUE)
The key point is that there is very little diﬀerence in arguments between parLapply and apply, so the barrier
to using (this form) of parallel computing is low. Each function above has an argument cl, which is created
by a makeCluster call. This function, amongst other things, speciﬁes the number of processors to use.
7.3.2
Example: Snakes and Ladders
Parallel computing is ideal for Monte-Carlo simulations. Each core independently simulates a realisation from
model. At the end, we gather up the results. In the eﬃcient package, there is a function that simulates
a single game of Snakes and Ladders - snakes_ladders()[ˆThe idea for this example came to one of the
authors after a particularly long and dull game of Snakes and Ladders with his son.]
If we wanted to simulate N games we could use sapply

94
CHAPTER 7. EFFICIENT PERFORMANCE
N = 10^4
sapply(1:N, snakes_ladders)
Rewriting this code to make use of the parallel package is straightforward. We begin by making a cluster
object
library("parallel")
Then swap sapply for parSapply
parSapply(cl, 1:N, snakes_ladders)
before stopping the cluster
stopCluster(cl)
On my computer I get a four-fold speed-up.
7.3.3
Exit functions with care
We should always call stopCluster to free resources when we are ﬁnished with the cluster object. However
if the parallel code is within function, it’s possible that function ends as the results of an error and so
stopCluster is ommitted.
The on.exit function handles this problem with the minimum of fuss; regardless how the function ends,
on.exit is always called. In the context of parallel programming we will have something similar to
simulate = function(cores) {
cl = makeCluster(cores)
on.exit(stopCluster(cl))
## Do something
}
Another common use of on.exit is in conjunction with the par function. If you use par to change
graphical parameters within a function, on.exit ensures these parameters are reset to their previous
value when the function ends.
7.3.4
Process forking
Another way of running code in parallel is to use the mclapply and mcmapply functions, i.e.
## This will run on Windows, but will only use 1 core
mclapply(1:2, snakes_ladders)
These functions use forking, that is creating a new copy of a process running on the CPU. However Windows
does not support this low-level functionality in the way that Linux does. If I’m writing code that is only for
me, then I use mclapply since I avoid starting and stopping clusters. However if the code is to be shared
with a windows user, I use makeCluster since it is cross-platform.

7.4. RCPP
95
7.4
Rcpp
Sometimes R is just slow. You’ve tried every trick you know, and your code is still crawling along. At this
point you could consider rewriting key parts of your code in another, faster language. R has interfaces to
other languages via packages, such as Rcpp, rJava, rPython and recently V8. These provide R interfaces
to C++, Java, Python and JavaScript respectively. Rcpp is the most popular of these (Figure 7.1).
Time
Downloads per day
R 3.0
R 3.1
R 3.2
1
10
100
1000
10000
2013
2016
The rise of Rcpp
rPython
Rcpp
V8
rJava
Figure 7.1: Downloads per day from the RStudio CRAN mirror of packages that provide R interfaces to
other languages.
C++ is a modern, fast and very well-supported language with libraries for performing many kinds of
computational task. Rcpp makes incorporating C++ code into your R workﬂow easily.
Although C/Fortran routines can be used using the .Call function this is not recommended: using .Call
can be a painful experience. Rcpp provides a friendly API (Application Program Interface) that lets you
write high-performance code, bypassing R’s tricky C API. Typical bottlenecks that C++ addresses are loops
and recursive functions.
C++ is a powerful programming language about which entire books have been written. This chapter therefore
is focussed on getting started and provide a ﬂavour of what is possible. It is structured as follows. After
ensuring that your computer is set-up for Rcpp in Section 7.4.1, we proceed by creating a simple C++
function, to show how C++ compares with R (Section 7.4.2). This is converted into an R function using
cppFunction() in Section 7.4.3. The remainder of the chapter explains C++ data types (Section 7.4.4),
illustrates how to source C++ code directly (Section 7.4.5) and explains vectors (Section 7.4.6) Rcpp sugar
(Section 7.4.7) and ﬁnally provides guidance on further resources on the subject (Section 7.4.8).
7.4.1
Pre-requisites
To write and compile C++ functions, you need a working C++ compiler. The installation method depends
on your operating system:

96
CHAPTER 7. EFFICIENT PERFORMANCE
• Linux: A compiler should already be installed. Otherwise, install r-base and a compiler will be installed
as a dependency.
• Macs: Install Xcode.
• Windows: Install Rtools. Make sure you select the version that corresponds to your version of R.
The code in this chapter was generated using version 0.12.5 of Rcpp. The latest version can be installed
from CRAN:
install.packages("Rcpp")
Rcpp is well documented, as illustrated by package’s CRAN page. In addition to its popularity, many other
packages depend on Rcpp, which can be seen by looking at the Reverse Imports section.
To check that you have everything needed for this chapter, run the following piece of code from the course R
package:
efficient::test_rcpp()
#> Everything seems fine
7.4.2
A simple C++ function
A C++ function is similar to an R function: you pass a set of inputs to function, some code is run, a single
object is returned. However there are some key diﬀerences.
1. In the C++ function each line must be terminated with ; In R, we use ; only when we have multiple
statements on the same line.
2. We must declare object types in the C++ version. In particular we need to declare the types of the
function arguments, return value and any intermediate objects we create.
3. The function must have an explicit return statement. Similar to R, there can be multiple returns, but
the function will terminate when it hits it’s ﬁrst return statement.
4. You do not use assignment when creating a function.
5. Object assignment must use = sign. The <- operator isn’t valid.
6. One line comments can be created using //. Multi-line comments are created using /*...*/
Suppose we want to create a function that adds two numbers together. In R this would be a simple one line
aﬀair:
add_r = function(x, y) x + y
In C++ it is a bit more long winded
/* Return type double
* Two arguments, also doubles
*/
double add_c(double x, double y) {
double value = x + y;
return value;
}
If we were writing a C++ programme we would also need another function called main. We would then
compile the code to obtain an executable. The executable is platform dependent. The beauty of using Rcpp
is that it makes it very easy to call C++ functions from R and the user doesn’t have to worry about the
platform, or compilers or the R/C++ interface.

7.4. RCPP
97
Table 7.1: Overview of key C++ object types.
Type
Description
char
A single character.
int
An integer.
ﬂoat
A single precision ﬂoating point number.
double
A double-precision ﬂoating point number.
void
A valueless quantity.
7.4.3
The cppFunction command
Load the Rcpp package using the usual library function call:
library("Rcpp")
Pass the C++ function created in the previous section as a text string argument to cppFunction:
cppFunction('
double add_c(double x, double y) {
double value = x + y;
return value;
}
')
Rcpp will magically compile the C++ code and construct a function that bridges the gap between R and
C++. After running the above code, we now have access to the add_c function
add_c
#> function (x, y)
#> .Primitive(".Call")(<pointer: 0x2b5803f348a0>, x, y)
We can call the add_c function in the usual way
add_c(1, 2)
#> [1] 3
and we don’t have to worry about compilers. Also, if you include this function in a package, users don’t have
to worry about any of the Rcpp magic. It just works.
7.4.4
C++ data types
The most basic type of variable is an integer, int. An int variable can store a value in the range −32768 to
+32767. (There is also something called an unsigned int, which goes from 0 to 65, 535). There are also long
int data types, which range from 0 to 231 −1.}. To store ﬂoating point numbers, there are single precision
numbers, float and double precision numbers, double. A double takes twice as much memory as a float.
For single characters, we use the char data type.
A pointer object is a variable that points to an area of memory that has been given a name. Pointers are
a very powerful, but primitive facility contained in the C++ language. They are very useful since rather
than passing large objects around, we pass a pointer to the memory location; rather than pass the house,
we just give the address. We won’t use pointers in this chapter, but mention them for completeness. Table
@ref{tab:cpptypes} gives an overview.

98
CHAPTER 7. EFFICIENT PERFORMANCE
7.4.5
The sourceCpp function
The cppFunction is great for getting small examples up and running. But it is better practice to put your C++
code in a separate ﬁle (with ﬁle extension cpp) and use the function call sourceCpp("path/to/file.cpp")
to compile them. However we need to include a few headers at the top of the ﬁle. The ﬁrst line we add gives
us access to the Rcpp functions. The ﬁle Rcpp.h contains a list of function and class deﬁnitions supplied by
Rcpp. This ﬁle will be located where Rcpp is installed. Alternatively you can view it online. The include
line
#include <Rcpp.h>
causes the compiler to replace that lines with the contents of the named source ﬁle. This means that
we can access the functions deﬁned by Rcpp.
To access the Rcpp functions we would have to type
Rcpp::function_1. To avoid typing Rcpp::, we use the namespace facility
using namespace Rcpp;
Now we can just type function_1; this is the same concept that R uses for managing function name collisions
when loading packages. Above each function we want to export/use in R, we add the tag
// [[Rcpp::export]]
This would give the complete ﬁle
#include <Rcpp.h>
using namespace Rcpp;
// [[Rcpp::export]]
double add_c(double x, double y) {
double value = x + y;
return value;
}
There are two main beneﬁts with putting your C++ functions in separate ﬁles. First, we have the beneﬁt of
syntax highlighting (RStudio has great support for C++ editing). Second, it’s easier to make syntax errors
when the switching between R and C++ in the same ﬁle. To save space, we we’ll omit the headers for the
remainder of the chapter.
7.4.6
Vectors and loops
Let’s now consider a slightly more complicated example. Here we want to write our own function that
calculates the mean. This is just an illustrative example: R’s version is much better and more robust to scale
diﬀerences in our data. For comparison, let’s create a corresponding R function - this is the same function we
used in chapter @ref{programming}. The function takes a single vector x as input, and returns the mean
value, m:
mean_r = function(x) {
m = 0
n = length(x)
for(i in 1:n)
m = m + x[i]/n
m
}

7.4. RCPP
99
This is a very bad R function. We should just use the base function mean for real world applications. However
the purpose of mean_r is to provide a comparison for the C++ version, which we will write in a similar way.
In this example, we will let Rcpp smooth the interface between C++ and R by using the NumericVector data
type. This Rcpp data type mirrors the R vector object type. Other common classes are: IntegerVector,
CharacterVector, and LogicalVector.
In the C++ version of the mean function, we specify the arguments types: x (NumericVector) and the
return value (double). The C++ version of the mean function is a few lines longer. Almost always, the
corresponding C++ version will be, possibly much, longer.
double mean_c(NumericVector x) {
int i;
int n = x.size();
double mean = 0;
for(i=0; i<n; i++) {
mean = mean + x[i]/n;
}
return mean;
}
To use the C++ function we need to source the ﬁle (remember to put the necessary headers in).
sourceCpp("src/mean_c.cpp")
Although the C++ version is similar, there are a few crucial diﬀerences.
1. We use the .size() method to ﬁnd the length of x
2. The for loop has a more complicated syntax.
for (variable initialization; condition; variable update ) {
// Code to execute
}
3. C++ provides operators to modify variables in place. So i++ increases the value of i by 1. Similarly,
we could rewrite part of the loop as
mean += x[i]/n;
The above code adds x[i]/n to the value of mean. Other similar operators are -=, *=, /= and i--.
4. A C++ vector starts at 0 not 1
To compare the C++ and R functions, we’ll generate some normal random numbers for the comparison
x = rnorm(1e4)
Then call the microbenchmark function (results plotted in ﬁgure 7.2).

100
CHAPTER 7. EFFICIENT PERFORMANCE
## com_mean_r is the compiled version of mean_r
z = microbenchmark(
mean(x), mean_r(x),
com_mean_r(x), mean_c(x),
times=1000
)
In this simple example, the Rcpp variant is around 100 times faster than the corresponding R version. This
sort of speed-up is not uncommon when switching to an Rcpp solution.
10−3
10−2
10−1
100
101
Base
Rcpp
Byte compiled R
Pure R
Elapsed Time (secs)
Performance Gains with Rcpp
Figure 7.2: Comparison of mean functions.
Exercises
Consider the following piece of code
double test1() {
double a = 1.0 / 81;
double b = 0;
for (int i = 0; i < 729; ++ i)
b = b + a;
return b;
}
1. Save the function test1 in a separate ﬁle. Make sure it works.
2. Write a similar function in R and compare the speed of the C++ and R versions.
3. Create a function called test2 where the double variables have been replaced by float. Do you still
get the correct answer?
www.allitebooks.com

7.4. RCPP
101
4. Change b = b + a to b += a to make you code more C like.
5. (Hard) What’s the diﬀerence between i++ and ++i?
7.4.6.1
Matrices
Each vector type has a corresponding matrix equivalent: NumericMatrix, IntegerMatrix, CharacterMatrix
and LogicalMatrix. We use these types in a similar way to how we used NumericVector’s. The main
diﬀerences are:
• When we initialise, we need specify the number of rows and columns
// 10 rows, 5 columns
NumericMatrix mat(10, 5);
// Length 10
NumericVector v(10);
• We subset using (), i.e. mat(5, 4).
• The ﬁrst view in a matrix is mat(0, 0) - remember indexes start with 0.
• To determine the number of rows and columns, we use the .nrow() and .ncol() methods.
7.4.7
C++ with sugar on top
Rcpp sugar brings a higher-level of abstraction to C++ code written using the Rcpp API. What this means
in practice is that we can write C++ code in the style of R. For example, suppose we wanted to ﬁnd the
squared diﬀerence of two vectors; a residual in regression. In R, we would we would use
sq_diff_r = function(x, y) (x - y)^2
Rewriting the function in standard C++, we would more eﬀort. Assuming we have the correct headers, we
would try something like
NumericVector res_c(NumericVector x, NumericVector y) {
int i;
int n = x.size();
NumericVector residuals(n);
for(i=0; i<n; i++) {
residuals[i] = pow(x[i] - y[i], 2);
}
return residuals;
}
With Rcpp sugar we can rewrite this code to be more succinct and have more of an R feel:
NumericVector res_sugar(NumericVector x, NumericVector y) {
return pow(x-y, 2);
}
In the above C++ code, the pow function and x-y are valid due to Rcpp sugar. Other functions that are
available include the d/q/p/r statistical functions, such as rnorm and pnorm. The sweetened versions aren’t
usually faster than the C++ version, but typically there’s very little diﬀerence between the two. However
with the sugared variety, the code is shorter and is constantly being improved.

102
CHAPTER 7. EFFICIENT PERFORMANCE
Exercises
1. Construct an R version, res_r and compare the three function variants.
2. In the above example, res_sugar is faster than res_c. Do you know why?
7.4.8
Rcpp resources
The aim of this section was to provide an introduction to Rcpp. One of the selling features of Rcpp is that
there is a great deal of documentation available.
• The Rcpp website;
• The original Journal of Statistical Software paper describing Rcpp (Eddelbuettel and François (2011))
and the follow-up book (Eddelbuettel (2013));
• A very readable chapter on Rcpp that goes into a bit more detail than this section (Wickham (2014a));
• The Rcpp section on the stackoverﬂow website. Questions are often answered by the Rcpp authors.

Chapter 8
Eﬃcient hardware
This chapter is odd for a book on R programming. It contains very little code, and yet the chapter has the
potential to speed up your algorithms by orders of magnitude. This chapter considers the impact that your
computer has on your time.
Your hardware is crucial. It will not only determine how fast you can solve your problem, but also whether
you can even tackle the problem of interest. This is because everything is loaded in RAM. Of course, being a
more powerful computer costs money. The goal is to help you decide whether the beneﬁts of upgrading your
hardware are worth that extra cost.
We’ll begin this chapter with an background section on computer storage and memory and how it is measured.
Then we consider individual computer components, before concluding with renting machines in the cloud.
8.1
Top 5 tips for eﬃcient hardware
• Use the package benchmarkme to assess your CPUs number crunching ability; is it worth upgrading
your hardware.
• If possible, add more RAM.
• Double check that you have installed a 64-bit version of R.
• Cloud computing is a cost eﬀective way of obtaining more compute power.
• A solid state drives typically won’t have much impact on the speed of your R code, but will increase
your overall productivity since I/0 is much faster.
8.2
Background: what is a byte?
A computer cannot store “numbers” or “letters”. The only thing a computer can store and work with is bits.
A bit is binary, it is either a 0 or a 1. In fact from a physics perspective, a bit is just a blip of electricity that
either is or isn’t there.
In the past the ASCII character set dominated computing. This set deﬁnes 128 characters including 0 to 9,
upper and lower case alpha-numeric and a few control characters such as a new line. To store these characters
required 7 bits since 27 = 128, but 8 bits were typically used for performance reasons. Table 3.1 gives the
binary representation of the ﬁrst few characters.
Bit representation
Character
01000001
A
01000010
B
103

104
CHAPTER 8. EFFICIENT HARDWARE
Bit representation
Character
01000011
C
01000100
D
01000101
E
01010010
R
Table 3.1: The bit representation of a few ASCII characters.
The limitation of only having 256 characters led to the development of Unicode, a standard framework aimed
at creating a single character set for every reasonable writing system. Typically, Unicode characters require
sixteen bits of storage.
Eight bits is one byte, or ASCII character. So two ASCII characters would use two bytes or 16 bits. A pure
text document containing 100 characters would use 100 bytes (800 bits). Note that mark-up, such as font
information or meta-data, can impose a substantial memory overhead: an empty .docx ﬁle requires about
3, 700 bytes of storage.
When computer scientists ﬁrst started to think about computer memory, they noticed that 210 = 1024 ≃103
and 220 = 1, 048, 576 ≃106, so they adopted the short hand of kilo- and mega-bytes. Of course, everyone
knew that it was just a short hand, and it was really a binary power. When computers became more wide
spread, foolish people like you and me just assumed that kilo actually meant 103 bytes.
Fortunately the IEEE Standards Board intervened and created conventional, internationally adopted deﬁnitions
of the International System of Units (SI) preﬁxes. So a kilobyte (kB) is 103 = 1000 bytes and a megabyte
(MB) is 106 bytes or 103 kilobytes (see table 3.2). A petabyte is approximately 100 million drawers ﬁlled
with text. Astonishingly Google processes around 20 petabytes of data every day.
Factor
Name
Symbol
Origin
Derivation
210
kibi
Ki
Kilobinary:
(210)1
220
mebi
Mi
Megabinary:
(210)2
230
gibi
Gi
Gigabinary:
(210)3
240
tebi
Ti
Terabinary:
(210)4
250
pebi
Pi
Petabinary:
(210)5
Table 3.2: Data conversion table. Credit: http://physics.nist.gov/cuu/Units/binary.html
Even though there is now an agreed standard for discussing memory, that doesn’t mean that everyone follows
it. Microsoft Windows, for example, uses 1MB to mean 220B. Even more confusing the capacity of a 1.44MB
ﬂoppy disk is a mixture, 1MB = 103 × 210B.
8.3
Random access memory: RAM
Random access memory (RAM) is a type of computer memory that can be accessed randomly: any byte
of memory can be accessed without touching the preceding bytes. RAM is found in computers, phones,
tablets and even printers. The amount of RAM R has access to is incredibly important. Since R loads
objects into RAM, the amount of RAM you have available can limit the size of data set you can analysis.

8.3. RANDOM ACCESS MEMORY: RAM
105
Even if the original data set is relatively small, your analysis can generate large objects. For example, suppose
we want to perform standard cluster analysis. The built-in data set USAarrests, is a data frame with 50
rows and 4 columns. Each row corresponds to a state in the USA
head(USArrests, 3)
#>
Murder Assault UrbanPop Rape
#> Alabama
13.2
236
58 21.2
#> Alaska
10.0
263
48 44.5
#> Arizona
8.1
294
80 31.0
If we want to group states that have similar crime statistics, a standard ﬁrst step is to calculate the distance
or similarity matrix
d = dist(USArrests)
When we inspect the object size of the original data set and the distance object using the pryr package
pryr::object_size(USArrests)
#> 5.23 kB
pryr::object_size(d)
#> 14.3 kB
The distance object d is actually a vector that contains the distances in the upper triangular region.
we have managed to create an object that is three times larger than the original data set. In fact the object d
is a symmetric n × n matrix, where n is the number of rows in USAarrests. Clearly, as n increases the size
of d increases at rate O(n2). So if our original data set contained 10, 000 records, the associated distance
matrix would contain almost 108 values. Of course since the matrix is symmetric, this corresponds to around
50 million unique values.
A rough rule of thumb is that your RAM should be three times the size of your data set.
Another beneﬁt of having increasing the amount of onboard RAM is that the ‘garbage collector’, a process
that runs periodically to free-up system memory occupied by R, is called less often (we will cover this in
more detail in chapter XXX).
It is straightforward to determine how much RAM you have. Under Windows,

106
CHAPTER 8. EFFICIENT HARDWARE
1. Clicking the Start button picture of the Start button, then right-clicking Computer. Next click on
Properties.
2. In the System section, you can see the amount of RAM your computer has next to the Installed
memory (RAM) section. Windows reports how much RAM it can use, not the amount installed. This is
only an issue if you are using a 32-bit version of Windows.
In Mac, click the Apple menu. Select About This Mac and a window appears with the relevant information.
On almost all Unix-based OSs, you can ﬁnd out how much RAM you have using the code vmstat, whilst
on all Linux distributions, you can use the command free. Using this in conjunction with the -h tag will
provide the answer in human readable format, as illustrated below for a 16 GB machine:
$ free -h
total
used
free
Mem:
15G
4.0G
11G
It is sometimes possible to increase your computer’s RAM. On the computer motherboard, there are typically
2 or 4 RAM or memory slots. If you have free slots, then you can add more RAM. However, it is common
that all slots are already taken. This means that to upgrade your computer’s memory, some or all of the
RAM will have to be removed. To go from 8GB to 16GB, for example, you may have to discard the two 4GB
RAM cards and replace them with two 8GB cards. Increasing your laptop/desktop from 4GB to 16GB or
32GB is cheap and should deﬁnitely be considered. As R Core member Uwe Ligges states,
fortunes::fortune(192)
#>
#> RAM is cheap and thinking hurts.
#>
-- Uwe Ligges (about memory requirements in R)
#>
R-help (June 2007)
It is a testament to the design of R that it is still relevant and its popularity is growing. Ross Ihaka, one of
the originators of the R programming language, made a throw-away comment in 2003:
fortunes::fortune(21)
#>
#> I seem to recall that we were targetting 512k Macintoshes. In our dreams
#> we might have seen 16Mb Sun.
#>
-- Ross Ihaka (in reply to the question whether R&R thought when they
#>
started out that they would see R using 16G memory on a dual Opteron
#>
computer)
#>
R-help (November 2003)
Considering that a standard smart phone now contains 1GB of RAM, the fact that R was designed for “basic”
computers, but can scale across clusters is impressive. R’s origins on computers with limited resources helps
explain its eﬃciency at dealing with large datasets.
Exercises
The following two exercises aim to help you determine if it is worthwhile upgrading your RAM.
1. R loads everything into memory, i.e. your computers RAM. How much RAM does your computer you
have?
2. Using your preferred search engine, how much does it cost (in pounds) to double the amount of available
RAM on your system?

8.4. HARD DRIVES: HDD VS SSD
107
8.4
Hard drives: HDD vs SSD
You are using R because you want to analyse data. The data is typically stored on your hard drive; but not
all hard drives are equal. Unless you have a fairly expensive laptop your computer probably has a standard
hard disk drive (HDD). HDDs were ﬁrst introduced by IBM in 1956. Data is stored using magnetism on a
rotating platter, as shown in Figure 8.1. The faster the platter spins, the faster the HDD can perform. Many
laptop drives spin at either 5400RPM (Revolutions per Minute) or 7200RPM. The major advantage of HDDs
is that they are cheap, making a 1TB laptop standard.
In the authors’ experience, having an SSD drive doesn’t make much diﬀerence to R. However, the
reduction in boot time and general tasks makes an SSD drive a wonderful purchase.
Figure 8.1: A standard 2.5" hard drive, found in most laptops. Credit: https://en.wikipedia.org/wiki/Hard/
_disk/_drive
Solid state drives (SSDs) can be thought of as large, but more sophisticated versions of USB sticks. They have
no moving parts and information is stored in microchips. Since there are no moving parts, reading/writing is
much quicker. SSDs have other beneﬁts: they are quieter, allow faster boot time (no ‘spin up’ time) and
require less power (more battery life).
The read/write speed for a standard HDD is usually in the region of 50 −120MB/s (usually closer to 50MB).
For SSDs, speeds are typically over 200MB/s. For top-of-the-range models this can approach 500MB/s. If
you’re wondering, read/write speeds for RAM is around 2 −20GB/s. So at best SSDs are at least one order
of magnitude slower than RAM, but still faster than standard HDDs.
If you are unsure what type of hard drive you have, then time how long your computer takes to
reach the log-in screen. If it is less then ﬁve seconds, you probably have a SSD. There are links on
the book’s website detailing more precise methods for each OS.
8.5
Operating systems: 32-bit or 64-bit
R comes in two versions: 32-bit and 64-bit. Your operating system also comes in two versions, 32-bit and
64-bit. Ideally you want 64-bit versions of both R and the operating system. Using a 32-bit version of either

108
CHAPTER 8. EFFICIENT HARDWARE
has severe limitations on the amount of RAM R can access. So when we suggest that you should just buy
more RAM, this assumes that you are using a 64-bit operating system, with a 64-bit version of R.
If you are using an OS version from the last ﬁve years, it is unlikely to be 32-bit OS.
A 32-bit machine can access at most only 4GB of RAM. Although some CPUs oﬀer solutions to this limitation,
if you are running a 32-bit operating system, then R is limited to around 3GB RAM. If you are running a
64-bit operating system, but only a 32-bit version of R, then you have access to slightly more memory (but
not much). Modern systems should run a 64-bit operating system, with a 64-bit version of R. Your memory
limit is now measured as 8 terabytes for Windows machines and 128TB for Unix-based OSs. An easy method
for determining if you are running a 64-bit version of R is to run
.Machine$sizeof.pointer
which will return 8 if you a running a 64-bit version of R.
To ﬁnd precise details consult the R help pages help("Memory-limits") and help("Memory").
Exercises
These exercises aim to condense the previous section into the key points.
1. Are you using 32-bit or 64-bit version of R?
2. If you are using Windows, what are the results of running the command memory.limit()?
8.6
Central processing unit (CPU)
The central processing unit (CPU), or the processor, is the brains of a computer. The CPU is responsible for
performing numerical calculations. The faster the processor, the faster R will run. The clock speed (or clock
rate, measured in hertz) is frequency with which the CPU executes instructions. The faster the clock speed,
the more instructions a CPU can execute in a section. CPU clock speed for a single CPU has been fairly
static in the last couple of years, hovering around 3.4GHz (see ﬁgure 8.2).
Unfortunately we can’t simply use clock speeds to compare CPUs, since the internal architecture of a CPU
plays a crucial role in determining the CPU performance. The R package benchmarkme provides functions
for benchmarking your system and contains data from previous benchmarks. Figure 8.3 shows the relative
performance for over 150 CPUs.
Running the benchmarks and comparing your CPU to others is straightforward. First load the package
library("benchmarkme")
Then run the benchmarks and plot via
res = benchmark_std()
plot(res)
# Upload your benchmarks for future users
upload_results(res)
You get the model speciﬁcations of the top CPUs using get_datatable(res).

8.6. CENTRAL PROCESSING UNIT (CPU)
109
Clock speed (MHz)
1980
1985
1990
1995
2000
2005
2010
100
101
102
103
104
3.4 GHz
CPU Clock Speed
Figure 8.2: CPU clock speed. The data for this ﬁgure was collected from web-forum and wikipedia. It is
intended to indicate general trends in CPU speed.
Rank
Relative Time
0
50
100
150
1
2
10
20
100
Intel Atom @ 1.66GHz
CPU Benchmarks
Figure 8.3: CPU benchmarks from the R package, benchmarkme. Each point represents an individual
CPU result.

110
CHAPTER 8. EFFICIENT HARDWARE
8.7
Cloud computing
Cloud computing uses networks of remote servers, instead of a local computer, to store and analyse data. It
is now becoming increasingly popular to rent cloud computing resources.
8.7.1
Amazon EC2
Amazon Elastic Compute Cloud (EC2) is one of a number of providers of this service. EC2 makes it (relatively)
easy to run R instances in the cloud. Users can conﬁgure the operating system, CPU, hard drive type, the
amount of RAM and where your project is physically located.
If you want to run a server in the Amazon EC2 cloud, you have to select the system you are going to boot
up. There are a vast array of pre-packaged system images. Some of these images are just basic operating
systems, such as Debian or Ubuntu, which require further conﬁguration. There is also an Amazon machine
image that speciﬁcally targets R and RStudio.
Exercise
To assess whether you should consider cloud computing, how much does it cost to rent a machine comparable
to your laptop in the cloud?

Chapter 9
Eﬃcient Collaboration
Large projects inevitably involve many people.
This poses risks but also opportunities for improving
computational eﬃciency and productivity, especially if project collaborators are reading and committing code.
This chapter provides guidance on how to minimise the risks and maximise the beneﬁts of collaborative R
programming.
Collaborative working has a number of beneﬁts. A team with a diverse skill set is usually stronger than
a team with a very narrow focus. It makes sense to specialize: clearly deﬁning roles such as statistician,
front-end developer, system administrator and project manager will make your team stronger. Even if you are
working alone, dividing the work into discrete branches in this way can be useful, as discussed in Chapter 4.
Collaborative programming provides an opportunity for people to review each other’s code. This can be
encouraged by using a uniform style with many comments as described in Section 9.1. Like using a clear
style in human language, following a style guide has the additional advantage of making your code more
understandable to others.
When working on complex programming projects with multiple inter-dependencies version control is essential.
Even on small projects tracking the progress of your project’s code-base has many advantages and makes
collaboration much easier. Fortunately it is now easier than ever before to integrate version control into your
project, using RStudio’s interface to the version control software git and online code sharing websites such
as GitHub. This is the subject of Section 9.2.
The ﬁnal section, 9.3, addresses the question of how to respond when you ﬁnd ineﬃcient code. Refactoring is
the process of re-writing poorly written or scripts so they are faster, more comprehensible, more portable and
easier to maintain.
9.1
Coding style
To be a successful programmer you need to use a consistent programming style. There is no single ‘correct’
style. To some extent good style is subjective and down to personal taste. There are, however, general
principles that most programmers agree on, such as:
• Use modular code
• Comment your code
• Don’t Repeat Yourself (DRY)
• Be concise, clear and consistent
Good coding style will make you more eﬃcient even if you are the only person who reads it. When your code
is read by multiple readers or you are developing code with co-workers, having a consistent style is even more
111

112
CHAPTER 9. EFFICIENT COLLABORATION
important. There are a number of R style guides online that are broadly similar, including one by Google and
one by Hadley Whickham. The style followed in this book is based on a combination of Hadley Wickham’s
guide and our own preferences (we follow Yihui Xie in preferring = to <- for assignment, for example).
In-line with the principle of automation (automate any task that can save time by automating), the easiest
way to improve your code is to ask your computer to do it, using RStudio.
9.1.1
Reformatting code with RStudio
RStudio can automatically clean up poorly indented and formatted code. To do this, select the lines that
need to be formatted (e.g. via Ctrl+A to select the entire script) then automatically indent it with Ctrl+I.
The shortcut Ctrl+Shift+A will reformat the code, adding spaces for maximum readability. An example is
provided below.
# Poorly indented/formatted code
if(!exists("x")){
x=c(3,5)
y=x[2]}
This code chunk works but is not pleasant to read. RStudio automatically indents the code after the if
statement as follows.
# Automatically indented code (Ctrl+I in RStudio)
if(!exists("x")){
x=c(3,5)
y=x[2]}
This is a start, but it’s still not easy to read. This can be ﬁxed in RStudio as illustrated below (these options
can be seen in the Code menu, accessed with Alt+C on Windows/Linux computers).
# Automatically reformat the code (Ctrl+Shift+A in RStudio)
if(!exists("x")) {
x = c(3, 5)
y = x[2]
}
Note that some aspects of style are subjective: we would not leave a space after the if and ).
9.1.2
File names
File names should use the .R extension and should be lower case (e.g. load.R). Avoid spaces. Use a dash or
underscore to separate words.
# Good names
normalise.R
load.R
# Bad names
Normalise.r
load data.R

9.1. CODING STYLE
113
9.1.3
Loading packages
Library function calls should be at the top of your script. When loading an essential package, use library
instead of require since a missing package will then raise an error. If a package isn’t essential, use require
and appropriately capture the warning raised. Package names should be surrounded with speech marks.
# Good
library("ggplot2")
# Bad
library(ggplot2)
Avoid listing every package you may need, instead just include the packages you actually use. If you ﬁnd that
you are loading a large number of packages, consider putting all packages in a ﬁle called packages.R and
using source appropriately.
9.1.4
Commenting
Avoid using plain English to explain standard R code
# Setting x equal to 1
x = 1
Instead comments should explain at a higher level of abstraction the programmer’s intention (McConnell
2004). Each comment line should begin with a single hash (#), followed by a space. Comments can be toggled
(turned on and oﬀ) in this way with Ctl+Shift+C in RStudio. The double hash (##) can be reserved for R
output. If you follow your comment with four dashes (# ----) RStudio will enable code folding until the
next instance of this.
9.1.5
Object names
Function and variable names should be lower case, with an underscore (_) separating words. Unless you are
creating an S3 object, avoid using a . in the name. Create names that are concise, but still have meaning.
In functions the required arguments should always be ﬁrst, followed by optional arguments, with the special
... argument coming last. If your argument has a boolean value, use TRUE/FALSE instead of T/F for clarity.
It’s tempting to use T/F as shortcuts. But it is easy to accidently redeﬁne these variables, e.g. F =
10. R raises an error if you try to redeﬁne TRUE/FALSE
While it’s possible to write arguments that depend on other arguments, try to avoid using this idiom as it
makes understanding the default behaviour harder to understand. Typically it’s easier to set an argument to
have a default value of NULL and check its value using is.null than by using missing. Avoid using names
of existing functions. Do not, for example, use data as a variable name.
9.1.6
Example package
The lubridate package is a good example of a package that has a consistent naming system, to make it easy
for users to guess its features and behaviour. Dates are encoded in a variety of ways, but the lubridate
package has a neat set of functions consisting of the three letters, year, month and day. For example,

114
CHAPTER 9. EFFICIENT COLLABORATION
library("lubridate")
#>
#> Attaching package: 'lubridate'
#> The following objects are masked from 'package:data.table':
#>
#>
hour, mday, month, quarter, wday, week, yday, year
#> The following object is masked from 'package:base':
#>
#>
date
ymd("2012-01-02")
dmy("02-01-2012")
mdy("01-02-2012")
9.1.7
Assignment
The two most common ways of assigning objects to values in R is with <- and =. In most (but not all)
contexts, they can be used interchangeably. Regardless of which operator you prefer, consistency is key,
particularly when working in a group. In this book we use the = operator for assignment, as it’s faster to
type and more consistent with other languages.
The one place where a diﬀerence occurs is during function calls. Consider the following piece of code used for
timing random number generation
system.time(expr1 <- rnorm(10e5))
system.time(expr2 = rnorm(10e5)) # error
The ﬁrst lines will run correctly and create a variable called expr1. The second line will raise an error. When
we use = in a function call, it changes from an assignment operator to an argument passing operator. For
further information about assignment, see ?assignOps.
To prevent = being interpreted as an argument passing operator, you can enclose the function call in curly
braces:
system.time({expr3 = rnorm(10e5)})
#>
user
system elapsed
#>
0.083
0.004
0.088
9.1.8
Spacing
Consistent spacing is an easy way of making your code more readable. Even a simple command such as x =
x + 1 takes a bit more time to understand when the spacing is removed, i.e. x=x+1. You should add a space
around the operators +, -, \ and *. Include a space around the assignment operators, <- and =. Additionally,
add a space around any comparison operators such as == and <. The latter rule helps avoid bugs
# Bug. x now equals 1
x[x<-1]
# Correct. Selecting values less than -1
x[x < -1]
The exceptions to the space rule are :, :: and :::, as well as $ and @ symbols for selecting sub-parts of
objects. As with English, add a space after a comma, e.g.

9.2. VERSION CONTROL
115
cpu_speed[year > 1990, ]
9.1.9
Indentation
Use two spaces to indent code. Never mix tabs and spaces. RStudio can automatically convert the tab
character to spaces (see Tools -> Global options -> Code).
9.1.10
Curly braces
Consider the following code:
# Bad style, fails
if(x < 5)
{
y}
else {
x}
Typing this straight into R will result in an error. An opening curly brace, { should not go on its own line
and should always be followed by a line break. A closing curly brace should always go on its own line (unless
it’s followed by an else, in which case the else should go on its own line). The code inside a curly braces
should be indented (and RStudio will enforce this rule), as shown below.
# Good style
if(x < 5){
x
} else {
y
}
#> [1] 1
Be consistent with one line control statements. Some people prefer to avoid using braces:
# No braces
if(x < 5)
x else
y
#> [1] 1
9.1.11
Exercises
Look at the diﬀerence between your style and RStudio’s based on a representative R script that you have
written (see Section 9.1). What are the similarities? What are the diﬀerences? Are you consistent? Write
these down and think about how you can use the results to improve your coding style.
9.2
Version control
9.3
Refactoring

116
CHAPTER 9. EFFICIENT COLLABORATION

Chapter 10
Eﬃcient Learning
As with any vibrant open source software community, R is fast moving. This can be disorientating because it
means that you can never ‘ﬁnish’ learning R. On the other hand it can make R a fascinating subject: there is
always more to learn. Even experienced R users keep ﬁnding new functionality that helps solve problems
quicker and more elegantly and that can be really satisfying. Therefore learning how to learn is one of the
most important skills you will learn if you want to learn R in depth and for the long-term. We emphasise
depth of learning because it is more eﬃcient to learn something properly than to Google it repeatedly every
time we forget how it works.
This chapter equips you with concepts and tips that will accelerate the transition from an R hacker to an
R programmer. This inevitably involves eﬀective use of R’s help, reading R source code and use of online
material.
10.1
Using R Help
All functions have help ﬁles. For example, to see the help ﬁle for plot, just type:
?plot
Note: this is the same as help("plot"). Note that the resulting help page is divided into many sections.
The example section is very helpful in showing precisely how the function works. You can either copy and
paste the code, or actually run the example code using the example command:.
example(plot)
Another useful section in the help ﬁle is See Also:. In the plot help ﬁle, it gives pointers to 3d plotting.
To look for help about a certain topic rather than a speciﬁc function use ??topic, which is analogous to
?function. To search for information about regression in all installed packages, for example, use the following
command:
??regression
Note that this is shorthand help.search("regression").
To search more speciﬁcally for objects the appropos function can be useful. To search for all objects and
functions in the current workspace containing the text string lm, for example, one would enter:
117

118
CHAPTER 10. EFFICIENT LEARNING
#>
[1] ".__C__anova.glm"
".__C__anova.glm.null" ".__C__diagonalMatrix"
#>
[4] ".__C__generalMatrix"
".__C__glm"
".__C__glm.null"
#>
[7] ".__C__lm"
".__C__lMatrix"
".__C__mlm"
#> [10] ".__C__optionalMethod" ".__T__colMeans:base"
".colMeans"
#> [13] ".lm.fit"
"bm_matrix_cal_lm"
"colMeans"
#> [16] "colMeans"
"confint.lm"
"contr.helmert"
#> [19] "dummy.coef.lm"
"getAllMethods"
"glm"
#> [22] "glm.control"
"glm.fit"
"KalmanForecast"
#> [25] "KalmanLike"
"KalmanRun"
"KalmanSmooth"
#> [28] "kappa.lm"
"lm"
"lm.fit"
#> [31] "lm.influence"
"lm.wfit"
"model.matrix.lm"
#> [34] "nlm"
"nlminb"
"predict.glm"
#> [37] "predict.lm"
"residuals.glm"
"residuals.lm"
#> [40] "summary.glm"
"summary.lm"
Sometimes a package will contain vignettes. To browse any vignettes associated with a particular package,
we can use the handy function
browseVignettes(package = "benchmarkme")
10.2
Reading R source code
10.3
Learning online
10.3.1
Reproducible example
Asking questions on stackoverﬂow and R-help is hard. Your question should contain just enough information
that you problem is clear and can be reproducibed, while at the same time avoid unnecessary details.
Fortunately, there is a SO question - How to make a great R reproducible example? - that provides excellent
guidence!
10.3.2
Minimal data set
What is the smallest data set you can construct that will reproduce your issue? Your actualy data set may
contain 105 rows and 104 columns, but to get your idea across, you might only need 5 rows and 3 columns.
Making small example data sets is easy. For example, to create a data frame with two numeric columns and
a column of characters we use
set.seed(1)
example_df = data.frame(x=rnorm(5), y=rnorm(5), z=sample(LETTERS, 5))
Note the call to set.seed that ensures anyone who runs the code will get the same random number stream.
Alternatively, you use one of the many data sets that come with R - library(help="datasets").
If creating an example data set isn’t possible, then use dput on your actual data set. This will create an
ASCII text representation of the object that will enable anyone to recreate the object
dput(example_df)
#> structure(list(x = c(-0.626453810742332, 0.183643324222082, -0.835628612410047,
#> 1.59528080213779, 0.329507771815361), y = c(-0.820468384118015,

10.4. ONLINE RESOURCES
119
#> 0.487429052428485, 0.738324705129217, 0.575781351653492, -0.305388387156356
#> ), z = structure(c(4L, 2L, 3L, 1L, 5L), .Label = c("C", "F",
#> "P", "Y", "Z"), class = "factor")), .Names = c("x", "y", "z"), row.names = c(NA,
#> -5L), class = "data.frame")
10.3.3
Minimal example
What you should not do, is simply copy and paste your entire function into your question. It’s unlikely that
your entire function doesn’t work, so just simplify it the bare minimum. For example,
10.4
Online resources
When asking a question, here are a few pointers:
• Make your example reproducible.
• Clearly state your problem. Don’t confuse a statistical problem with an R problem.
• Read a few other questions to learn the format of the site.
• People aren’t under any obligation to answer your question!
10.4.1
Stackoverﬂow
The number one place on the internet for getting help on programming is Stackoverﬂow. This website provides
a platform for asking and answering questions. Through site membership, questions and answers are voted up
or down. Users of Stackoverﬂow earn reputation points when their question or answer is up-voted. Anyone
(with enough reputation) can edit a question or answer. This helps answers remain relevant.
Questions are tagged. The R questions can be found under the R tag. Each tag has a page describing the
tag. The R page contains links to Oﬃcial documentation, free resources, and various other links
Members of the Stackoverﬂow R community have tagged, using r-faq, a few question that often crop up.
• How to search for R materials.

120
CHAPTER 10. EFFICIENT LEARNING
10.4.2
Mailing lists: help, dev, package
10.4.3
r-bloggers
10.4.4
twitter: #rstats
10.5
Conferences
10.5.1
useR!
10.5.2
Local groups
10.6
Code
10.7
Look at the source code
* e.g. `NCOL`
* Learn from well known packages
* git version of R
* Monitor changes in the NEWS
10.7.1
R-journal and Journal of Statistical Software
10.7.2
The manuals
Those two should be mentioned somewhere as further reading, as those are one of the best reference manuals
about the language and its proper use.
• (R-lang)[https://cran.r-project.org/doc/manuals/r-release/R-lang.html]
• (R-exts)[https://cran.r-project.org/doc/manuals/r-release/R-exts.html]
Berkun, Scott. 2005. The Art of Project Management. O’Reilly.
Braun, John, and Duncan J Murdoch. 2007. A First Course in Statistical Programming with R. Vol. 25.
Cambridge University Press Cambridge.
Burns, Patrick. 2011. The R Inferno. Lulu.com.
Chang, Winston. 2012. R Graphics Cookbook. O’Reilly Media.
Codd, E. F. 1979. “Extending the database relational model to capture more meaning.” ACM Transactions
on Database Systems 4 (4): 397–434. doi:10.1145/320107.320109.
Cotton, Richard. 2013. Learning R. O’Reilly Media.
Eddelbuettel, Dirk. 2010. “Benchmarking Single-and Multi-Core BLAS Implementations and GPUs for Use
with R.” Mathematica.
———. 2013. Seamless R and C++ Integration with Rcpp. Springer.
Eddelbuettel, Dirk, and Romain François. 2011. “Rcpp: Seamless R and C++ Integration.” Journal of
Statistical Software 40 (8): 1–18.
Eddelbuettel, Dirk, Romain François, J. Allaire, John Chambers, Douglas Bates, and Kevin Ushey. 2011.

10.7. LOOK AT THE SOURCE CODE
121
“Rcpp: Seamless R and C++ Integration.” Journal of Statistical Software 40 (8): 1–18.
Goldberg, David. 1991. “What Every Computer Scientist Should Know About Floating-Point Arithmetic.”
ACM Computing Surveys (CSUR) 23 (1). ACM: 5–48.
Grant, Christine A, Louise M Wallace, and Peter C Spurgeon. 2013. “An Exploration of the Psychological
Factors Aﬀecting Remote E-Worker’s Job Eﬀectiveness, Well-Being and Work-Life Balance.” Employee
Relations 35 (5). Emerald Group Publishing Limited: 527–46.
Grolemund, Garrett, and Hadley Wickham. 2016. R for Data Science. 1 edition. O’Reilly Media.
Janert, Philipp K. 2010. Data Analysis with Open Source Tools. “ O’Reilly Media, Inc.”
Jensen, Jørgen Dejgård. 2011. “Can Worksite Nutritional Interventions Improve Productivity and Firm
Proﬁtability? A Literature Review.” Perspectives in Public Health 131 (4). SAGE Publications: 184–92.
Kersten, Martin L, Stratos Idreos, Stefan Manegold, Erietta Liarou, and others. 2011. “The Researcher’s
Guide to the Data Deluge: Querying a Scientiﬁc Database in Just a Few Seconds.” PVLDB Challenges and
Visions 3.
Lovelace, Ada Countess. 1842. “Translator’s Notes to an Article on Babbage’s Analytical Engine.” Scientiﬁc
Memoirs 3: 691–731.
McCallum, Ethan, and Stephen Weston. 2011. Parallel R. O’Reilly Media.
McConnell, Steve. 2004. Code Complete. Pearson Education.
Pereira, Michelle Jessica, Brooke Kaye Coombes, Tracy Anne Comans, and Venerina Johnston. 2015. “The
Impact of Onsite Workplace Health-Enhancing Physical Activity Interventions on Worker Productivity: A
Systematic Review.” Occupational and Environmental Medicine 72 (6). BMJ Publishing Group Ltd: 401–12.
PMBoK, A. 2000. “Guide to the Project Management Body of Knowledge.” Project Management Institute,
Pennsylvania USA.
Sekhon, Jasjeet S. 2006. “The Art of Benchmarking: Evaluating the Performance of R on Linux and OS X.”
The Political Methodologist 14 (1): 15–19.
Spector, Phil. 2008. Data Manipulation with R. Springer Science & Business Media.
Visser, Marco D., Sean M. McMahon, Cory Merow, Philip M. Dixon, Sydne Record, and Eelke Jongejans.
2015.
“Speeding Up Ecological and Evolutionary Computations in R; Essentials of High Performance
Computing for Biologists.” Edited by Francis Ouellette. PLOS Computational Biology 11 (3): e1004140.
doi:10.1371/journal.pcbi.1004140.
Wickham, Hadley. 2014a. Advanced R. CRC Press.
———. 2014b. “Tidy Data.” The Journal of Statistical Software 14 (5).
———. 2015. R Packages. O’Reilly Media.
Xie, Yihui. 2015. Dynamic Documents with R and Knitr. Vol. 29. CRC Press.



DIGITAL
SIGNAL
PROCESSING
with selected topics
ADAPTIVE SYSTEMS
TIME-FREQUENCY ANALYSIS
SPARSE SIGNAL PROCESSING
Ljubiša Stankovi´c
2015

2
Library of Congress Cataloging-in-Publication Data
Library of Congress Control Number: 2015912465
ISBN-13: 978-1514179987
ISBN-10: 1514179989
c⃝2015 Ljubiša Stankovi´c, All Rights Reserved
Printed by CreateSpace Independent Publishing Platform,
An Amazon.com Company
North Charleston, South Carolina, USA.
Available from Amazon.com and other online and bookstores
All right reserved. Printed and bounded in the United States of America.
No part of this book may be reproduced or utilized in any form or by any
means, electronic or mechanical, including photocopying, recording, or by
any information storage and retrieval system, without permission in writing
from the copyright holder.

Ljubiša Stankovi´c
Digital Signal Processing
3
To
my parents
Božo and Cana,
my wife Snežana,
and our
Irena, Isidora, and Nikola.

4

Contents
I
Review
19
Chapter 1
Continuous-Time Signals and Systems
21
1.1
Continuous-Time Signals
21
1.2
Periodic Signals and Fourier Series
24
1.2.1
Fourier Series of Real-Valued Signals
28
1.2.2
Linear Systems
33
1.3
Fourier Transform
35
1.3.1
Fourier Transform and Linear Time-Invariant
Systems
37
1.3.2
Properties of the Fourier Transform
37
1.3.3
Relationship Between the Fourier Series and
the Fourier Transform
40
1.4
Fourier Transform and Stationary Phase Method
42
1.5
Laplace Transform
48
1.5.1
Linear Systems Described by Differential Equa-
tions
51
1.5.2
Table of the Laplace Transform
52
1.6
Butterworth Filter
53
II
Discrete Signals and Systems
57
Chapter 2
Discrete-Time Signals and Transforms
59
2.1
Discrete-Time Signals
59
2.1.1
Discrete-Time Systems
64
2.2
Fourier Transform of Discrete-Time Signals
67
2.2.1
Properties
69
2.2.2
Spectral Energy and Power Density
74
5

6
Contents
2.3
Sampling Theorem in the Time Domain
75
2.4
Problems
80
2.5
Solutions
85
2.6
Exercise
105
Chapter 3
Discrete Fourier Transform
107
3.1
DFT Deﬁnition
107
3.2
DFT Properties
113
3.3
Zero-Padding and Interpolation
118
3.4
Relation among the Fourier Representations
122
3.5
Fast Fourier Transform
124
3.6
Sampling of Periodic Signals
130
3.7
Analysis of a Sinusoid by Using the DFT
133
3.7.1
Leakage Effect
134
3.7.2
Displacement
137
3.8
Discrete Cosine and Sine Transforms
140
3.9
Discrete Walsh-Hadamard and Haar Transforms
145
3.9.1
Discrete Walsh-Hadamard Transform
148
3.9.2
Discrete Haar Wavelet Transform
152
3.10 Problems
156
3.11 Solutions
158
3.12 Exercise
167
Chapter 4
z-Transform
169
4.1
Deﬁnition of the z-transform
169
4.2
Properties of the z-transform
171
4.2.1
Linearity
171
4.2.2
Time-Shift
171
4.2.3
Multiplication by exponential signal: Modulation172
4.2.4
Differentiation
172
4.2.5
Convolution in time
173
4.2.6
Table of the z-transform
173
4.2.7
Initial and Stationary State Signal Value
174
4.3
Inverse z-transform
174
4.3.1
Direct Power Series Expansion
174
4.3.2
Theorem of Residues Based Inversion
178
4.4
Discrete systems and the z-transform
180
4.5
Difference equations
183
4.5.1
Solution Based on the z-transform
183
4.5.2
Solution of Difference Equations in the Time
Domain
186
4.6
Relation of the z-transform to other Transforms
191

Ljubiša Stankovi´c
Digital Signal Processing
7
4.7
Problems
193
4.8
Solutions
197
4.9
Exercise
213
Chapter 5
From Continuous to Discrete Systems
217
5.1
Impulse Invariance Method
218
5.2
Matched z-transform method
223
5.3
Differentiation and Integration
226
5.4
Bilinear Transform
230
5.5
Discrete Filters Design
236
5.5.1
Lowpass ﬁlters
236
5.5.2
Highpass Filters
242
5.5.3
Bandpass Filters
244
5.5.4
Allpass Systems - System Stabilization
246
5.5.5
Inverse and Minimum Phase Systems
247
5.6
Problems
251
5.7
Solutions
254
5.8
Exercise
264
Chapter 6
Realization of Discrete Systems
267
6.1
Realization of IIR systems
267
6.1.1
Direct realization I
268
6.1.2
Direct realization II
268
6.1.3
Sensitivity of the System Poles/Zeros to Errors
in Coefﬁcients
271
6.1.4
Cascade Realization
276
6.1.5
Parallel realization
280
6.1.6
Inverse realization
283
6.2
FIR Systems and their Realizations
284
6.2.1
Linear Phase Systems and Group Delay
285
6.2.2
Windows
287
6.2.3
Design of FIR System in the Frequency Domain 291
6.2.4
Realizations of FIR system
293
6.3
Problems
298
6.4
Solutions
302
6.5
Exercise
314
Chapter 7
Discrete-Time Random Signals
319
7.1
Basic Statistical Deﬁnitions
319
7.1.1
Expected Value
319
7.1.2
Probability and Probability Density Function
326
7.1.3
Median
329

8
Contents
7.1.4
Variance
331
7.2
Second-Order Statistics
336
7.2.1
Correlation and Covariance
336
7.2.2
Stationarity and Ergodicity
337
7.2.3
Power Spectral Density
338
7.3
Noise
340
7.3.1
Uniform Noise
340
7.3.2
Binary Noise
341
7.3.3
Gaussian Noise
344
7.3.4
Complex Gaussian Noise and Rayleigh Distri-
bution
349
7.3.5
Impulsive Noises
350
7.3.6
Noisy Signals
352
7.4
Discrete Fourier Transform of Noisy Signals
352
7.4.1
Detection of a Sinusoidal Signal Frequency
356
7.5
Linear Systems and Random Signals
360
7.5.1
Spectral Estimation of Narrowband Signals
366
7.6
Detection and Matched Filter
368
7.6.1
Matched Filter
369
7.7
Optimal Wiener Filter
372
7.8
Quantization effects
376
7.8.1
Input signal quantization
377
7.8.2
Quantization of the results
382
7.9
Problems
394
7.10 Solutions
400
7.11 Exercise
418
III
Selected Topics
421
Chapter 8
Adaptive Systems
423
8.1
Introduction
423
8.2
Linear Adaptive Adder
427
8.2.1
Error Signal
429
8.2.2
Autocorrelation Matrix Eigenvalues and Eigen-
vectors
438
8.2.3
Error Signal Analysis
443
8.2.4
Orthogonality Principle
445
8.3
Steepest Descend Method
446
8.4
LMS Algorithm
457
8.4.1
Convergence of the LMS algorithm
458
8.5
LMS Application Examples
460

Ljubiša Stankovi´c
Digital Signal Processing
9
8.5.1
Identiﬁcation of Unknown System
460
8.5.2
Noise Cancellation
464
8.5.3
Sinusoidal Disturbance Cancellation
466
8.5.4
Signal Prediction
468
8.5.5
Adaptive Antenna Arrays
473
8.5.6
Acoustic Echo Cancellation
478
8.6
Variations on the LMS Algorithm
481
8.6.1
Sign LMS
481
8.6.2
Block LMS
482
8.6.3
Normalized LMS Algorithm
483
8.6.4
LMS with Variable Step Size
485
8.6.5
Complex LMS
487
8.7
RLS Algorithm
489
8.8
Adaptive Recursive Systems
493
8.9
From the LMS algorithm to the Kalman ﬁlters
495
8.10 Neural Networks
500
8.10.1 Neuron
502
8.10.2 Network Function
502
8.10.3 Activation Function
503
8.10.4 Neural Network Topology
505
8.10.5 Network with Supervised Learning
507
8.10.6 One-Layer Network with Binary Output - Per-
ceptron
508
8.10.7 One-Layer Neural Network with Continuous
Output
512
8.10.8 Multilayer Neural Networks
515
8.10.9 Neural Networks with Unsupervised Learning 518
8.10.10 Voting Machines
519
Chapter 9
Time-Frequency Analysis
521
9.1
Short-Time Fourier Transform
522
9.2
Windows
529
9.2.1
Rectangular Window
529
9.2.2
Triangular (Bartlett) Window
530
9.2.3
Hann(ing) Window
531
9.2.4
Hamming Window
533
9.2.5
Blackman and Kaiser Windows
534
9.2.6
Discrete Form and Realizations of the STFT
535
9.2.7
Recursive STFT Realization
541
9.2.8
Filter Bank STFT Implementation
542
9.2.9
Signal Reconstruction from the Discrete STFT
546

10
Contents
9.2.10 Time-Varying Windows
556
9.2.11 Frequency-Varying Window
567
9.2.12 Hybrid Time-Frequency-Varying Windows
569
9.3
Wavelet Transform
569
9.3.1
Filter Bank and Discrete Wavelet
574
9.3.2
S-Transform
607
9.4
Local Polynomial Fourier Transform
610
9.4.1
Fractional Fourier Transform with Relation to
the LPFT
613
9.5
High-Resolution STFT
614
9.5.1
Capon’s STFT
614
9.5.2
MUSIC STFT
618
9.5.3
Capon’s LPFT
620
9.6
Wigner Distribution
622
9.6.1
Auto-Terms and Cross-Terms in the Wigner
Distribution
627
9.6.2
Wigner Distribution Properties
631
9.6.3
Pseudo and Smoothed Wigner Distribution
636
9.6.4
Discrete Pseudo Wigner Distribution
639
9.6.5
From the STFT to the Wigner Distribution via
S-Method
647
9.7
General Quadratic Time-Frequency Distributions
653
9.7.1
Reduced Interference Distributions
657
9.7.2
Kernel Decomposition Method
662
Chapter 10 Sparse Signal Processing
665
10.1 Illustrative Examples
666
10.2 Sparsity and Reduced Set of Samples/Observations
676
10.3 Transformation Matrix Parameters
680
10.3.1 Unitary Matrix
682
10.3.2 Isometry and Restricted Isometry Property
683
10.3.3 Coherence
684
10.3.4 Restricted Isometry and Coherence
687
10.3.5 Restricted Isometry and Eigenvalues
691
10.3.6 Unique Reconstruction Condition and RIP
701
10.3.7 Rank and Spark of a Matrix
704
10.3.8 Spark and the Solution Uniqueness
706
10.4 Norm-Zero Based Reconstruction
710
10.4.1 Direct Combinatorial Search
710
10.4.2 Pseudoinverse matrix
713
10.4.3 Estimation of Unknown Positions
714

Ljubiša Stankovi´c
Digital Signal Processing
11
10.4.4 Unavailable/Missing Samples Noise in Initial
Estimation
717
10.4.5 Iterative Procedure
727
10.4.6 Inﬂuence of Additive Input Noise
729
10.4.7 Nonsparse Signal Reconstruction
733
10.5 Norm-One Based Reconstruction
735
10.5.1 Illustrations in the Signal Domain
737
10.5.2 Illustration in the Sparsity Domain
742
10.5.3 Equivalence of the Norm-Zero and Norm-One
Based Minimization Solutions
756
10.6 Median Based Formulation
760
10.7 Norm-one Based Reconstruction Algorithms
766
10.7.1 LASSO- Minimization
767
10.7.2 Signal Domain Reconstruction with a Gradient
Algorithm
770
10.8 On the Uniqueness of the DFT of Sparse Signals
782
10.9 Indirect Measurements/Sampling
791
10.10Processing of Sparse Signals with Impulsive Noise
801
10.10.1 Direct Search Procedure
802
10.10.2 Criteria for Selecting Samples
803
10.10.3 Uniqueness of the Obtained Solution
806
10.11Image Reconstruction
808
Index
811
About the Author
820

12
Contents

Preface
T
HIS book is a result of author’s thirty-three years of experience in
teaching and research in signal processing. It is written for students
and engineers as a ﬁrst book in digital signal processing, assuming
that a reader is familiar with the basic mathematics, including integrals, dif-
ferential calculus, and linear algebra. Although a review of continuous-time
analysis is presented in the ﬁrst chapter, a prerequisite for the presented
content is a basic knowledge about continuous-time signal processing.
The book consists of three parts. After an introductory review part, the
basic principles of digital signal processing are presented within Part two of
the book. This part starts with Chapter two which deals with basic deﬁni-
tions, transforms, and properties of discrete-time signals. The sampling the-
orem, providing essential relation between continuous-time and discrete-
time signals, is presented in this chapter as well. Discrete Fourier transform
and its applications to signal processing are the topic of the third chapter.
Other common discrete transforms, like Cosine, Sine, Walsh-Hadamard,
and Haar are also presented in this chapter. The z-transform, as a power-
ful tool for analysis of discrete-time systems, is the topic of Chapter four.
Various methods for transforming a continuous-time system into a corre-
sponding discrete-time system are derived and illustrated in Chapter ﬁve.
Chapter six is dedicated to the forms of discrete-time system realizations.
Basic deﬁnitions and properties of random discrete-time signals are given
in Chapter six. Systems to process random discrete-time signals are consid-
ered in this chapter as well. Chapter six concludes with a short study of
quantization effects.
The presentation is supported by numerous illustrations and exam-
ples. Chapters within Part two are followed by a number of solved and
unsolved problems for practice. Theory is explained in a simple way with
a necessary mathematical rigor. The book provides simple examples and
13

14
Preface
explanations for each presented transform, method, algorithm or approach.
Sophisticated results in signal processing theory are illustrated by simple
numerical examples.
Part three of the book contains few selected topics in digital signal
processing: adaptive discrete-time systems, time-frequency signal analysis,
and processing of discrete-time sparse signals. This part could be studied
within an advanced course in digital signal processing, following the basic
course. Some parts from the selected topics may be included in tailoring a
more extensive ﬁrst course in digital signal processing as well.
The author would like to thank colleagues: prof. Zdravko Uskokovi´c,
prof. Srdjan Stankovi´c, prof. Igor Djurovi´c, prof. Veselin Ivanovi´c, prof.
Miloš Dakovi´c, prof. Božo Krstaji´c, prof. Vesna Popovi´c-Bugarin, prof. Slo-
bodan Djukanovi´c, prof. Irena Orovi´c, dr. Nikola Žari´c, dr Marko Sime-
unovi´c, and M.Sc. Predrag Rakovi´c for careful reading of the initial version
of this book and for many comments that helped to improve the presenta-
tion.
The author thanks the colleagues that helped in preparing the special
topics part of the book. Many thanks to Miloš Dakovi´c who coauthored all
three chapters of Part three of this book and to other coauthors of chapters
in this part: Thayaparan Thayananthan, Srdjan Stankovi´c, and Irena Orovi´c.
Special thanks to M.Sc. Miloš Brajovi´c and M.Sc. Stefan Vujovi´c for their
careful double-check of the presented theory and examples, numerous com-
ments, and for the help in proofreading the ﬁnal version of the book.
London,
July 2013 - July 2015.
Author

Introduction
S
IGNAL is a physical process, mathematical function, or any other
physical or symbolic representation of an information. Signal theory
and processing are the areas dealing with the efﬁcient generation,
description, transformation, transmission, reception, and interpretation of
information. In the beginning, the most common physical processes used
for these purposes were the electric signals, for example, varying current
or electromagnetic waves. Signal theory is most commonly studied within
electrical engineering. Signal theory theory are strongly related to the ap-
plied mathematics and information theory. Examples of signals include
speech, music, image, video, medical, biological, geophysical, sonar, radar,
biomedical, car engine, ﬁnancial, and molecular data. In terms of signal gen-
eration, the main topics are in sensing, acquisition, synthesis, and reproduc-
tion of information. Various mathematical transforms, representations, and
algorithms are used for describing signals. Signal transformations are a set
of methods for decomposition, ﬁltering, estimation and detection. Modu-
lation, demodulation, detection, coding, and compression are the most im-
portant aspects of the signal transmission. In the process of interpretation,
various approaches may be used, including adaptive and learning-based
tools and analysis.
Mathematically, signals are presented by functions of one or more vari-
ables. Examples of one-dimensional signals are speech and music signals.
A typical example of a two-dimensional signal is an image while video se-
quence is a sample of a three-dimensional signal. Some signals, for example,
geophysical, medical, biological, radar, or sonar, may be represented and
interpreted as one-dimensional, two-dimensional, or multidimensional.
Signals may be continuous functions of independent variables, for ex-
ample, functions of time and/or space. Independent variables may also
be discrete, with the signal values being deﬁned only over an ordered set
15

16
Introduction
0
5
10
15
0
0.2
0.4
0.6
0.8
1
t
x(t)
continuous
0
5
10
15
0
0.2
0.4
0.6
0.8
1
n
x(n)
discrete-time
0
5
10
15
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
n
xd(n)
digital
0000
0001
0010
0011
0100
0101
0110
0111
1000
1001
1010
1011
1100
1101
1110
1111
Figure 1
Illustration of a continuous signal and its discrete-time and digital version.
of discrete independent variable values. This is a discrete-time signal. The
discrete-time signals, after being stored in a general computer or special-
purpose hardware, are discretized (quantized) in amplitude as well, so that
they can be memorized within the registers of a ﬁnite length. These kinds
of signals are referred to as digital signals, Fig.1. A continuous-time and
continuous amplitude (analog) signal is transformed into a discrete-time
and discrete-amplitude (digital) signal by using analog-to-digital (A/D)
converters, Fig.2. Their processing is known as digital signal processing. In
modern systems, the amplitude quantization errors are very small. Com-
mon A/D converters are with sampling frequency of up to megasample
(some even up to few gigasample) per second with 8 to 24 bits of resolu-
tion in amplitude. The digital signals are usually mathematically treated as
continuous (nondiscretized) in amplitude, while the quantization error is
studied, if needed, as a small disturbance in processing, reduced to a noise
in the input signal. Digital signals are transformed back into analog form by
digital-to-analog (D/A) converters.
According to the nature of their behavior, all signals could be deter-
ministic or stochastic. For deterministic signals, the values are known in the
past and future, while the stochastic signals are described by probabilistic
methods. The deterministic signals are commonly used for theoretical de-
scription, analysis, and syntheses of systems for signal processing.

Ljubiša Stankovi´c
Digital Signal Processing
17
A/D
D/A
x(t)
y(n)
x(n)
h(n)
y(t)
DIGITAL SYSTEM
y(t)
x(t)
ha(t)
ANALOG SYSTEM
Figure 2
Illustration of an analog and a digital system used to process an analog signal.
Advantages of processing signals in digital form are in their ﬂexibility
and adaptability with possibilities ranging up to our imagination to imple-
ment a transformation with an algorithm on a computer. The time required
for processing in real time (all calculations have to be completed between
two signal samples) is a limitation as compared to the analog systems that
are limited with a physical delay of electrical components and circuits only.

18
Introduction

Part I
Review
19


Chapter 1
Continuous-Time Signals and Systems
M
OST of discrete-time signals are obtained by sampling continuous-
time signals. In many applications, the result of signal process-
ing is presented and interpreted in the continuous-time domain.
Throughout the course of digital signal processing, the results will be dis-
cussed and related to the continuous-time forms of signals and their param-
eters. This is the reason why the ﬁrst chapter is dedicated to a review of
signals and transforms in the continuous-time domain. This review will be
of help in establishing proper correspondence and notation for the presen-
tation that follows in the next chapters.
1.1
CONTINUOUS-TIME SIGNALS
One-dimensional signals, represented by a function of time as a continuous
independent variable, are referred to as continuous-time signals (continu-
ous signals). Some simple forms of deterministic continuous-time signals
are presented next.
The unit-step signal (Heaviside function) is deﬁned by
u(t) =
!
1, for t ≥0
0, for t < 0 .
(1.1)
In the Heaviside function deﬁnition, the value of u(0) = 1/2 is also used.
Note that the independent variable t is continuous, while the signal itself is
not a continuous function. It has a discontinuity at t = 0.
The boxcar signal (rectangular window) is formed as b(t) = u(t +
1/2) −u(t −1/2), that is, b(t) = 1 for −1/2 ≤t < 1/2 and b(t) = 0 else-
where. A signal obtained by multiplying the unit-step signal by t is called
the ramp signal, with notation R(t) = tu(t).
21

22
Continuous-Time Signals and Systems
The impulse signal (or delta function) is deﬁned as
δ(t) = 0, for t ̸= 0
and
∞
"
−∞
δ(t)dt = 1.
(1.2)
The impulse signal is equal to 0 everywhere, except at t = 0, where it
assumes an inﬁnite value, so that its area is 1. From the deﬁnition of
the impulse signal, it follows δ(at) = δ(t)/|a|. This function cannot be
implemented in real-world systems due to its inﬁnitely short duration and
inﬁnitely large amplitude at t = 0.
In theory, any signal can be expressed by using the impulse signal, as
x(t) =
∞
"
−∞
x(t −τ)δ(τ)dτ =
∞
"
−∞
x(τ)δ(t −τ)dτ.
(1.3)
Using the previous relation, it is possible to relate the unit-step signal
and the impulse signal,
u(t) =
∞
"
−∞
δ(τ)u(t −τ)dτ =
t
"
−∞
δ(τ)dτ
or
du(t)
dt
= δ(t).
(1.4)
A sinusoidal signal, with amplitude A, frequency Ω0, and initial phase
ϕ, is a signal of the form
x(t) = Asin(Ω0t + ϕ).
(1.5)
This signal is periodic in time, since it satisﬁes the periodicity condition
x(t + T) = x(t).
(1.6)
In this case, the period is T = 2π/Ω0.
A signal periodic with a basic period T could also be considered as
periodic with periods kT, where k is an integer.
A complex sinusoidal signal
x(t) = Aej(Ω0t+ϕ) = Acos(Ω0t + ϕ) + jAsin(Ω0t + ϕ)
(1.7)
is also periodic with period T = 2π/Ω0. Fig. 1.1 depicts basic continuous-
time signals.

Ljubiša Stankovi´c
Digital Signal Processing
23
-4
-2
0
2
4
-1
0
1
u(t)
  (a)
-4
-2
0
2
4
-1
0
1
δ(t)
  (b)
-4
-2
0
2
4
-1
0
1
t
b(t)
  (c)
-4
-2
0
2
4
-1
0
1
t
sin(πt)
  (d)
Figure 1.1
Continuous-time signals: (a) unit-step signal, (b) impulse signal, (c) boxcar signal,
and (d) sinusoidal signal.
Example 1.1. Find the period of a signal
x(t) =
N
∑
n=0
AnejnΩ0t.
⋆This signal consists of N + 1 components. The constant component A0
can be considered as periodic with any period. The remaining components
A1ejΩ0t, A2ej2Ω0t, A3ej3Ω0t,..., ANejNΩ0t are periodic with periods, T1 =
2π/Ω0, T2 = 2π/(2Ω0), T3 = 2π/(3Ω0), ...., TN = 2π/(NΩ0), respectively. A
sum of periodic signals is periodic with the period being equal to the small-
est time interval T containing all of the periods T1, T2, T3,..., TN an integer
number of times. In this case, it is T = 2π/Ω0.
Example 1.2. Find the periods of signals: x1(t) = sin(2πt/36), x2(t) = cos(4πt/15+
2), x3(t) = exp(j0.1t), x4(t) = x1(t) + x2(t), and x5(t) = x1(t) + x3(t).
⋆Periods are calculated according to (1.6). For x1(t) the period follows
from 2πT1/36 = 2π, as T1 = 36. Similarly, T2 = 15/2 and T3 = 20π. The
period of x4(t) is the smallest interval containing T1 and T2. It is T4 = 180 (5
periods of x1(t) and 24 periods of x2(t)). For signal x5(t), when the periods of
components are T1 = 36 and T3 = 20π, there is no common interval T5 such
that the periods T1 and T3 are contained an integer number of times. Thus,
the signal x5(t) is not periodic.
Some parameters that can be used to describe a signal are:

24
Continuous-Time Signals and Systems
• Maximum absolute value (magnitude) of a signal
Mx =
max
−∞<t<∞|x(t)|,
(1.8)
• Signal energy
Ex =
∞
"
−∞
|x(t)|2 dt,
(1.9)
• Signal instantaneous power
Px(t) = |x(t)|2 .
(1.10)
The average signal power is deﬁned by
PAV = lim
T→∞
1
2T
T
"
−T
|x(t)|2 dt.
The average power is a time average of energy. Energy signals are
signals with a ﬁnite energy, while power signals have ﬁnite and nonzero
power. The average signal power of energy signals is zero.
1.2
PERIODIC SIGNALS AND FOURIER SERIES
Consider a periodic signal x(t) with a period T. It can be expressed as a sum
of weighted periodic complex sinusoidal functions ej2πnt/T, −∞< n < ∞,
x(t) = ··· + X−1e−j2πt/T + X0e−j0 + X1ej2πt/T + ···
(1.11)
=
∞
∑
n=−∞
Xnej2πnt/T
if the Dirichlet conditions are met: (1) the signal x(t) has a ﬁnite number of
discontinuities within the period T; (2) it has a ﬁnite average value in the
period T; and (3) the signal has a ﬁnite number of maxima and minima.
Since the signal analysis deals with real-world physical signals, rather than
with mathematical generalizations, these conditions are almost always met.

Ljubiša Stankovi´c
Digital Signal Processing
25
The set of basis functions {ej2πnt/T : −∞< n < ∞}, is an orthonormal
set of functions since their inner product is
#
ej2πmt/T,ej2πnt/T$
= 1
T
T/2
"
−T/2
ej2πmt/Te−j2πnt/Tdt
=
%
1
for m = n
sin(π(m−n))
π(m−n)
= 0
for m ̸= n .
It means that the inner product of any two different basis functions is zero
(orthogonal set), while the inner product of a function with itself is 1 (normal
set). In the case of orthonormal set of basis functions, it is easy to show that
the weighting coefﬁcients Xn can be calculated as projections of x(t) onto
the basis functions ej2πnt/T,
Xn =
#
x(t),ej2πnt/T$
= 1
T
T/2
"
−T/2
x(t)e−j2πnt/Tdt.
(1.12)
This relation follows after a simple multiplication of the right and left sides
of (1.11) by e−j2πmt/T and an integration within the period 1
T
& T/2
−T/2 (·)dt.
Example 1.3. Show that the Fourier series coefﬁcients Xn of a periodic signal x(t)
can be obtained by minimizing the mean square error between the signal and
∑N
n=−N Xnej2πnt/T within the period T.
⋆The mean square value of error
e(t) = x(t) −
N
∑
n=−N
Xnej2πnt/T,
within the period, is
I = 1
T
T/2
"
−T/2
'''''x(t) −
N
∑
n=−N
Xnej2πnt/T
'''''
2
dt.
From ∂I/∂X∗m = 0 follows
1
T
T/2
"
−T/2
e−j2πmt/T
(
x(t) −
N
∑
n=−N
Xnej2πnt/T
)
dt = 0
Xm = 1
T
T/2
"
−T/2
x(t)e−j2πmt/Tdt.
(1.13)

26
Continuous-Time Signals and Systems
Note: A derivative of complex function F(z) = u + jv = u(x,y) + jv(x,y) with
z = x + jy is deﬁned by
∂F(z)
∂z
=
* ∂
∂x −j ∂
∂y
+
F(x,y),
∂F(z)
∂z∗
=
* ∂
∂x + j ∂
∂y
+
F(x,y).
Often a half of these values is used in the deﬁnition, what does not change
our results.
In order to prove the form ∂I/∂X∗m = 0 let us denote Xm by z, all terms
in x(t) −∑N
n=−N Xnej2πnt/T = f (z) that not depend on z = x + jy = Xm by
a + jb and ej2πnt/T = ejα, then we have to show that
∂F(z)
∂z∗
= ∂| f (z)|2
∂z∗
= 2e−jα f (z).
In our case
| f (z)|2 =
'''a + jb + ejα(x + jy)
'''
= (a + xcosα −ysinα)2 + (b + xsinα + ycosα)2
For the minimization of a function of two variables x and y we need partial
derivatives
∂| f (z)|2
∂x
= 2cosα(a + xcosα −ysinα)+
(1.14)
2sinα(b + xsinα + ycosα)
= 2Re{e−jα f (z)}
and
∂| f (z)|2
∂y
= 2Im{e−jα f (z)}.
(1.15)
Therefore, all calculations with two real-valued equations (1.14) and (1.15)
are the same as using one complex valued relation
∂| f (z)|2
∂x
+ j ∂| f (z)|2
∂y
=
* ∂
∂x + j ∂
∂y
+
F(x,y) = ∂F(z)
∂z∗.
Since the signal and the basis functions are periodic with period T, in
all previous integrals, we can use
1
T
T/2
"
−T/2
x(t)e−j2πnt/Tdt = 1
T
T/2+Λ
"
−T/2+Λ
x(t)e−j2πnt/Tdt
(1.16)

Ljubiša Stankovi´c
Digital Signal Processing
27
where Λ is an arbitrary constant.
The signal expansion (1.11) is known as the Fourier series, and the
coefﬁcients Xn are the Fourier series coefﬁcients.
Example 1.4. Calculate the Fourier series coefﬁcients of a periodic signal x(t) =
cos2(πt/4). What will be the coefﬁcient values if period T = 8 is assumed?
⋆The signal x(t) can be written as x(t) = (1 + cos(πt/2))/2. The
period is T = 4. Assuming that the Fourier series coefﬁcients are calculated
with T = 4, after transforming the signal into (1.11) form, we get
x(t) = 1
4e−jπt/2 + 1
2 + 1
4ejπt/2.
The Fourier series coefﬁcients are recognized as X−1 = 1/4, X0 = 1/2 and
X1 = 1/4 (without the calculation deﬁned by (1.12)). Other coefﬁcients are
equal to zero. In the above transformation, the relation cos(πt/2) = (ejπt/2 +
e−jπt/2)/2 is used. If the period T = 8 is used, then the signal is decomposed
into complex sinusoids ej2πnt/8 = ejπnt/4,(1.11). The signal can be written as
x(t) = 1
4e−j2πt/4 + 1
2 + 1
4ej2πt/4.
(1.17)
Thus, comparing the signal deﬁnition with the basis functions ejπnt/4, we
may write X−2 = 1/4, X0 = 1/2, and X2 = 1/4. Other coefﬁcients are equal
to zero.
Example 1.5. Calculate the Fourier series coefﬁcients of a periodic signal x(t)
deﬁned as
x(t) =
∞
∑
n=−∞
x0(t + 2n)
with
x0(t) = u(t + 1/4) −u(t −1/4).
(1.18)
⋆The signal x(t) is a periodic extension of x0(t), with period T = 2.
This signal is equal to 1 for −1/4 ≤t < 1/4, within its basic period. Thus,
Xn = 1
2
1/4
"
−1/4
1e−j2πnt/2dt = sin(πn/4)
πn
,
(1.19)
with X0 = 1/4. Values of Xn are presented in Fig. 1.2.
The signal x(t) can be reconstructed by using the Fourier series (1.11).
In calculations, a ﬁnite number of terms denoted by M should be used,
xM(t) =
M
∑
n=−M
Xnejπnt.
The reconstructed signal, with M = 1,2,6, and 30, is shown in Fig. 1.3.

28
Continuous-Time Signals and Systems
-1/4
-1
1/4
1
t
x(t)
n
Xn
0
Figure 1.2
Periodic signal (left) and its Fourier series coefﬁcients (right).
-2
-1
0
1
2
-0.5
0
0.5
1
1.5
t
x1(t)
  (a)
-2
-1
0
1
2
-0.5
0
0.5
1
1.5
t
x2(t)
  (b)
-2
-1
0
1
2
-0.5
0
0.5
1
1.5
t
x6(t)
  (c)
-2
-1
0
1
2
-0.5
0
0.5
1
1.5
t
x30(t)
  (d)
Figure 1.3
Illustration of signal reconstruction by using a ﬁnite Fourier series with: (a)
coefﬁcients Xn within −1 ≤n ≤1, (b) coefﬁcients Xn within −2 ≤n ≤2, (c) coefﬁcients Xn
within −6 ≤n ≤6, and (d) coefﬁcients Xn within −30 ≤n ≤30.
1.2.1
Fourier Series of Real-Valued Signals
For a real-valued signal x(t) the Fourier series coefﬁcients can be written as
Xn = 1
T
T/2
"
−T/2
x(t)cos(2πnt
T
)dt −j 1
T
T/2
"
−T/2
x(t)sin(2πnt
T
)dt
= An −jBn
2
.
(1.20)

Ljubiša Stankovi´c
Digital Signal Processing
29
where An/2 and −Bn/2 are real and imaginary part of Xn. Since Xn = X∗−n
holds for real-valued signals, the values of An and Bn are
An = Xn + X−n = 2
T
T/2
"
−T/2
x(t)cos(2πnt
T
)dt,
Bn = Xn −X−n
−j
= 2
T
T/2
"
−T/2
x(t)sin(2πnt
T
)dt.
(1.21)
The Fourier series form for real-valued signals is
x(n) =
−1
∑
n=−∞
Xnej2πnt/T + X0 +
∞
∑
n=1
Xnej2πnt/T
= X0 +
∞
∑
n=1
(Xnej2πnt/T + X−ne−j2πnt/T)
(1.22)
= A0
2 +
∞
∑
n=1
An cos(2πnt
T
) +
∞
∑
n=1
Bn sin(2πnt
T
)
(1.23)
with |Xn| =
,
A2n + B2n/2. For real-valued signals the integrals in (1.21),
corresponding to An and Bn, are even and odd functions with respect to
n. Therefore, it is possible to calculate
Hn = 1
T
T/2
"
−T/2
x(t)
-
cos(2πnt
T
) + sin(2πnt
T
)
.
dt
(1.24)
and to get
An = Hn + H−n
Bn = Hn −H−n.
The coefﬁcients calculated by (1.24) are the Hartley series coefﬁcients. For a
real-valued and even signal x(t) = x(−t) this transform reduces to
Cn = 1
T
T/2
"
−T/2
x(t)cos(2πnt
T
)dt
corresponding to the Fourier cosine series coefﬁcients . Similar expression is
obtained for an odd real-valued signal x(n) when the Fourier series reduces
to the Fourier sine series coefﬁcients.

30
Continuous-Time Signals and Systems
Example 1.6. For a signal
x(t) = t[u(t) −u(t −1/2)]
we are interested in its reconstruction based on the Fourier series coefﬁcients.
The rate of coefﬁcients convergence may depend on the way how the periodic
extension of this signal is formed.
(a) Calculate the Fourier series of the original signal periodically ex-
tended with period T = 1/2,
xp(t) =
∞
∑
n=−∞
x(t + 1
2n).
Write the reconstruction formula with M Fourier series coefﬁcients.
(b) What are the Fourier transform coefﬁcients and the reconstruction
formula for
xp(t) =
∞
∑
n=−∞
x(t + n),
when the period is T = 1.
(c) The signal is ﬁrst extended with its reversed version
xc(t) = x(t) + x(1 −t)
and then periodically extended with period T = 1. Find the Fourier series
coefﬁcients and the reconstruction formula.
(d) Comment the coefﬁcients convergence in all cases.
⋆(a) The Fourier series coefﬁcients of this signal are
Xn =
1
1/2
1/2
"
0
te−j2πn/(1/2)tdt =
1
−j4πn
with X0 = 1/4. The reconstructed signal with M coefﬁcients is
xM(t) = 1
4 +
M
∑
n=1
-
−
1
4jπn ej4πnt +
1
4jπn e−j4πnt
.
= 1
4 −
M
∑
n=1
sin(4πnt)
2πn
.
The reconstructed signal for some values of M is presented in Fig.1.4.
(b) The Fourier series coefﬁcients of the signal extended with period 1
are
Xn = 1
1
1/2
"
0
te−j2πntdt =
1
−j2πn te−j2πnt
''''
1/2
0
+
1
(2πn)2 e−j2πnt'''
1/2
0
= (−1)n
−j4πn + (−1)n −1
(2πn)2
,

Ljubiša Stankovi´c
Digital Signal Processing
31
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x1(t)
  (a)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x2(t)
  (b)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x6(t)
  (c)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x30(t)
  (d)
Figure 1.4
Reconstruction of a signal using the Fourier series. Reconstructed signal is denoted
by xM(t), where M indicates the number of coefﬁcients used in reconstruction.
with X0 = 1/8. Note that the relation between the Fourier coefﬁcients in (a)
and (b) is 2X(b)
2n = X(a)
n . The reconstruction is presented in Fig.1.5.
(c) For the signal xc(t) extended with its reversed version follows
Cn =
1/2
"
0
te−j2πntdt +
1
"
1/2
(1 −t)e−j2πntdt
= 2
1/2
"
0
tcos(2πnt)dt = (−1)n −1
2π2n2
with C0 = 1/4. The reconstruction formula is
xM(t) = 1
4 −2
M
∑
n=1
1 −(−1)n
2π2n2
cos(2πnt)
= 1
4 −2
M
∑
n=1
1
π2(2n −1)2 cos(2π(2n −1)t).
The reconstructed signal in this case is presented in Fig.1.6.
(d) The coefﬁcients convergence in cases (a) and (b) is of order 1/n
while the convergence in the last case (c) is of order 1/n2. The best signal
reconstruction with the given number of coefﬁcients will be achieved in
the case (c). Also for a given reconstruction error we will need the smallest

32
Continuous-Time Signals and Systems
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x1(t)
  (a)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x2(t)
  (b)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x6(t)
  (c)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x30(t)
  (d)
Figure 1.5
Reconstruction of a periodic signal, with a zero interval extension before using the
Fourier series.
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x1(t)
  (a)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x2(t)
  (b)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x6(t)
  (c)
-1
-0.5
0
0.5
1
0
0.2
0.4
0.6
t
x30(t)
  (d)
Figure 1.6
Reconstruction of a periodic signal after an even extension before using the Fourier
series (cosine Fourier series).

Ljubiša Stankovi´c
Digital Signal Processing
33
number of reconstruction terms M in case (c). This kind of signal extension
will be later used as a basis for a deﬁnition of the so called cosine signal
transforms.
1.2.2
Linear Systems
A system transforms one signal (input signal) into another signal (output
signal). Assume that x(t) is the input signal. The system transformation will
be denoted by an operator T{◦}. The output signal can be written as
y(t) = T{x(t)}.
(1.25)
A system is linear if, for any two signals x1(t) and x2(t) and arbitrary
constants a1 and a2, it holds
y(t) = T{a1x1(t) + a2x2(t)} = a1T{x1(t)} + a2T{x2(t)}.
(1.26)
We say that a system is time-invariant if its properties and parameters do
not change in time. For a time-invariant system, it holds:
if y(t) = T{x(t)}, then
T{x(t −t0)} = y(t −t0),
(1.27)
for any t0.
Linear time-invariant (LTI) systems are fully described by their re-
sponse to the impulse signal. If we know the impulse response of these
systems,
h(t) = T{δ(t)},
then for arbitrary signal x(t) at the input, the output can be calculated, by
using (1.3), as
y(t) = T{x(t)} = T
⎧
⎨
⎩
∞
"
−∞
x(τ)δ(t −τ)dτ
⎫
⎬
⎭
Linearity
=
∞
"
−∞
x(τ)T{δ(t −τ)}dτ Time−invariance
=
∞
"
−∞
x(τ)h(t −τ)dτ.
The last integral is of particular signiﬁcance. It is called a convolution in
time of x(t) and h(t). Its notation is
y(t) = x(t) ∗t h(t) =
∞
"
−∞
x(τ)h(t −τ)dτ.
(1.28)

34
Continuous-Time Signals and Systems
The convolution is a commutative operation
x(t) ∗t h(t) = h(t) ∗t x(t).
(1.29)
Example 1.7. Find a convolution of signals x(t) = u(t + 1) −u(t −1) and h(t) =
e−tu(t).
⋆By using the convolution deﬁnition, we get
y(t) =
∞
"
−∞
x(τ)h(t −τ)dτ =
1
"
−1
1 · e−(t−τ)u(t −τ)dτ
= −
t−1
"
t+1
e−λu(λ)dλ =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
& t+1
t−1 e−λdλ = e−t(e −1/e),
for t ≥1
& t+1
0
e−λdλ = 1 −e−(t+1),
for −1 ≤t < 1
0
for t < −1.
A system is causal if there is no response before the input signal
appears. For causal systems h(t) = 0 for t < 0. In general signals that satisfy
the property that they may be an impulse response of a causal system may
be referred to as causal signals.
A system is stable if any input signal with a ﬁnite magnitude Mx =
max−∞<t<∞|x(t)| produces an output y(t) whose values are ﬁnite, |y(t)| <
∞. Sufﬁcient condition that a linear time-invariant system is stable is
∞
"
−∞
|h(τ)|dτ < ∞
(1.30)
since
|y(t)| = |
∞
"
−∞
x(t −τ)h(τ)dτ| ≤
∞
"
−∞
|x(t −τ)h(τ)|dτ
=
∞
"
−∞
|x(t −τ)||h(τ)|dτ ≤Mx
∞
"
−∞
|h(τ)|dτ < ∞,
if (1.30) holds.
It can be shown that the absolute value integrability of the impulse
response is the necessary condition for a linear time-invariant system to be
stable as well.

Ljubiša Stankovi´c
Digital Signal Processing
35
1.3
FOURIER TRANSFORM
The Fourier series has been introduced and presented for periodic signals,
with a period T. Assume now that we extend the period to inﬁnity, while not
changing the signal. This case corresponds to the analysis of an aperiodic
signal x(t). Its transform, the Fourier series coefﬁcients normalized by the
period, is given by
lim
T→∞XnT = lim
T→∞
T/2
"
−T/2
x(t)e−j2πnt/Tdt =
∞
"
−∞
x(t)e−jΩtdt
(1.31)
with 2π/T = ∆Ω→dΩ(being inﬁnitesimal) and 2πn/T →Ωbecoming a
continuous variable, as T →∞and −∞< n < ∞.
The function X(Ω), deﬁned by
X(Ω) =
∞
"
−∞
x(t)e−jΩtdt,
(1.32)
is called the Fourier transform (FT) of a signal x(t). For the Fourier trans-
form existence it is sufﬁcient that a signal is absolutely integrable. There
are some signals that do not satisfy this condition, whose Fourier transform
exists in a form of generalized functions, such as delta function.
The inverse Fourier transform (IFT) can be obtained by multiplying
both sides of (1.32) by ejΩτ and integrating over Ω,
∞
"
−∞
X(Ω)ejΩτdΩ=
∞
"
−∞
∞
"
−∞
x(t)ejΩ(τ−t)dtdΩ.
Using the fact that
∞
"
−∞
ejΩ(τ−t)dΩ= 2πδ(τ −t)
we get the inverse Fourier transform
x(t) = 1
2π
∞
"
−∞
X(Ω)ejΩtdΩ.
(1.33)

36
Continuous-Time Signals and Systems
Example 1.8. Calculate the Fourier transform of x(t) = Ae−atu(t), a > 0.
⋆According to the Fourier transform deﬁnition we get
X(Ω) =
∞
"
0
Ae−ate−jΩtdt =
A
(a + jΩ).
Example 1.9. Find the Fourier transform of
x(t) = sign(t) =
⎧
⎨
⎩
1
for t > 0
0
for t = 0
−1
for t < 0
.
(1.34)
⋆Since a direct calculation of the Fourier transform for this signal is
not possible, let us consider the signal
xa(t) =
⎧
⎨
⎩
e−at
for t > 0
0
for t = 0
−eat
for t < 0
where a > 0 is a real-valued constant. It is obvious that
lim
a→0xa(t) = x(t).
The Fourier transform of x(t) can be obtained as
X(Ω) = lim
a→0Xa(Ω),
where
Xa(Ω) =
0
"
−∞
−eate−jΩtdt +
∞
"
0
e−ate−jΩtdt =
2Ω
ja2 + jΩ2 .
(1.35)
It results in
X(Ω) = 2
jΩ.
(1.36)
Based on the deﬁnitions of the Fourier transform and the inverse
Fourier transform, it is easy to conclude that the duality property holds:
If X(Ω) is the Fourier transform of x(t), then the Fourier transform of
X(t) is 2πx(−Ω)
X(Ω) = FT{x(t)}
2πx(−Ω) = FT{X(t)},
(1.37)
where FT{◦} denotes the Fourier transform operator.

Ljubiša Stankovi´c
Digital Signal Processing
37
Example 1.10. Find the Fourier transform of δ(t), x(t) = 1 and u(t).
⋆The Fourier transform of δ(t) is
FT{δ(t)} =
∞
"
−∞
δ(t)e−jΩtdt = 1.
(1.38)
According to the duality property,
FT{1} = 2πδ(Ω).
Finally,
FT{u(t)} = FT
!sign(t) + 1
2
6
= 1
jΩ+ πδ(Ω).
(1.39)
1.3.1
Fourier Transform and Linear Time-Invariant Systems
Consider a linear, time-invariant system with an impulse response h(t) and
the input signal x(t) = Aej(Ω0t+ϕ). The output signal is
y(t) = x(t) ∗t h(t) =
∞
"
−∞
Aej(Ω0(t−τ)+ϕ)h(τ)dτ
= Aej(Ω0t+ϕ)
∞
"
−∞
h(τ)e−jΩ0τdτ = H(Ω0)x(t),
(1.40)
where
H(Ω) =
∞
"
−∞
h(t)e−jΩtdt
(1.41)
is the Fourier transform of h(t). The linear time-invariant system does not
change the form of an input complex harmonic signal Aej(Ω0t+ϕ). It remains
complex harmonic signal after passing through the system, with the same
frequency Ω0. The amplitude of the input signal x(t) is changed for |H(Ω0)|
and the phase is changed for arg{H(Ω0)}.
1.3.2
Properties of the Fourier Transform
The Fourier transform satisﬁes the following properties:

38
Continuous-Time Signals and Systems
1. Linearity
FT{a1x1(t) + a2x2(t)} = a1X1(Ω) + a2X2(Ω),
(1.42)
where X1(Ω) and X2(Ω) are the Fourier transforms of signals x1(t) and
x2(t), separately.
2. Realness
The Fourier transform of a signal is real (i.e., X∗(Ω) = X(Ω)), if
x∗(−t) = x(t),
since
X∗(Ω) =
∞
"
−∞
x∗(t)ejΩtdt t→−t
=
∞
"
−∞
x∗(−t)e−jΩtdt = X(Ω),
(1.43)
if x∗(−t) = x(t).
3. Modulation
FT{x(t)ejΩ0t} =
∞
"
−∞
x(t)ejΩ0te−jΩtdt = X(Ω−Ω0)
(1.44)
FT{2x(t)cos(Ω0t)} = X(Ω−Ω0) + X(Ω+ Ω0).
4. Shift in time
FT{x(t −t0)} =
∞
"
−∞
x(t −t0)e−jΩtdt = X(Ω)e−jt0Ω.
(1.45)
5. Time-scaling
FT{x(at)} =
∞
"
−∞
x(at)e−jΩtdt = 1
|a| X(Ω
a ).
(1.46)
6. Convolution
FT{x(t) ∗t h(t)} =
∞
"
−∞
∞
"
−∞
x(τ)h(t −τ)e−jΩtdτdt
(1.47)
t−τ→u
=
∞
"
−∞
∞
"
−∞
x(τ)h(u)e−jΩ(τ+u)dτdu = X(Ω)H(Ω).

Ljubiša Stankovi´c
Digital Signal Processing
39
7. Multiplication
FT{x(t)h(t)} = 1
2π
∞
"
−∞
x(t)
∞
"
−∞
H(θ)ejθtdθe−jΩtdt
(1.48)
= 1
2π
∞
"
−∞
H(θ)X(Ω−θ)dθ = X(Ω) ∗ΩH(Ω) = H(Ω) ∗ΩX(Ω).
Convolution in frequency domain is denoted by ∗Ωwith a factor of 1/2π
being included.
8. Parseval’s theorem
∞
"
−∞
x(t)y∗(t)dt = 1
2π
∞
"
−∞
X(Ω)Y∗(Ω)dΩ
(1.49)
∞
"
−∞
|x(t)|2 dt = 1
2π
∞
"
−∞
|X(Ω)|2 dΩ.
9. Differentiation
FT
!dx(t)
dt
6
= FT
⎧
⎨
⎩
d
dt
⎛
⎝1
2π
∞
"
−∞
X(Ω)ejΩtdΩ
⎞
⎠
⎫
⎬
⎭= jΩX(Ω).
(1.50)
10. Integration
The Fourier transform of
t
"
−∞
x(τ)dτ
can be calculated as the Fourier transform of
x(t) ∗t u(t) =
∞
"
−∞
x(τ)u(t −τ)dτ =
t
"
−∞
x(τ)dτ.
Then,
FT
⎧
⎨
⎩
t
"
−∞
x(τ)dτ
⎫
⎬
⎭= FT{x(t)}FT{u(t)} =
(1.51)
X(Ω)
* 1
jΩ+ πδ(Ω)
+
= 1
jΩX(Ω) + πX(0)δ(Ω).

40
Continuous-Time Signals and Systems
11. An analytic part of a signal x(t), whose Fourier transform is X(Ω), is a
signal with the Fourier transform deﬁned by
Xa(Ω) =
⎧
⎨
⎩
2X(Ω)
for Ω> 0
X(0)
for Ω= 0
0
for Ω< 0
.
(1.52)
It can be written as
Xa(Ω) = X(Ω) + X(Ω)sign(Ω) = X(Ω) + jXh(Ω)
(1.53)
where Xh(Ω) is the Fourier transform of the Hilbert transform of the signal
x(t). From Example 1.9 with the signal x(t) = sign(t) and the duality prop-
erty of the Fourier transform pair, obviously the inverse Fourier transform
of sign(Ω) is j/(πt). Therefore, the analytic part of a signal, in the time
domain, reads as
xa(t) = x(t) + jxh(t) = x(t) + x(t) ∗t
j
πt = x(t) + j 1
π
∞
"
−∞
p.v.
x(τ)
t −τ dτ.
(1.54)
where p.v. stands for Cauchy principal value of the considered integral.
1.3.3
Relationship Between the Fourier Series and the Fourier Transform
Consider an aperiodic signal x(t), with the Fourier transform X(Ω). As-
sume that the signal is of a limited duration (i.e., x(t) = 0 for |t| > T0/2).
Then,
X(Ω) =
T0/2
"
−T0/2
x(t)e−jΩtdt.
(1.55)
If we make a periodic extension of x(t), with a period T, we get a signal
xp(t) =
∞
∑
n=−∞
x(t + nT).
The periodic signal xp(t) can be expanded into Fourier series with the
coefﬁcients
Xn = 1
T
T/2
"
−T/2
xp(t)e−j2πnt/Tdt.
(1.56)

Ljubiša Stankovi´c
Digital Signal Processing
41
If T > T0 it is easy to conclude that
T/2
"
−T/2
xp(t)e−j2πnt/Tdt =
T0/2
"
−T0/2
x(t)e−jΩtdt|Ω=2πn/T
or
Xn = 1
T X(Ω)|Ω=2πn/T .
(1.57)
It means that the Fourier series coefﬁcients are the samples of the Fourier
transform, divided by T. The only condition in the derivation of this relation
is that the signal duration is shorter than the period of periodic extension
(i.e., T > T0). The sampling interval in frequency is
∆Ω= 2π
T , ∆Ω< 2π
T0
.
It should be smaller than 2π/T0, where T0 is the signal x(t) duration. This
is a form of the sampling theorem in the frequency domain. The sampling
theorem in the time domain will be discussed later.
In order to write the Fourier series coefﬁcients in the Fourier transform
form, note that a periodic signal xp(t), formed by a periodic extension of
x(t) with period T, can be written as
xp(t) =
∞
∑
n=−∞
x(t + nT) = x(t) ∗t
∞
∑
n=−∞
δ(t + nT).
(1.58)
The Fourier transform of this periodic signal is
Xp(Ω) = FT
%
x(t) ∗t
∞
∑
n=−∞
δ(t + nT)
;
(1.59)
= X(Ω) · 2π
T
∞
∑
n=−∞
δ
*
Ω−2π
T n
+
= 2π
T
∞
∑
n=−∞
X
*2π
T n
+
δ
*
Ω−2π
T n
+
since
FT
%
∞
∑
n=−∞
δ(t + nT)
;
=
∞
∑
n=−∞
∞
"
−∞
δ(t + nT)e−jΩtdt
=
∞
∑
n=−∞
ejΩnT = 2π
T
∞
∑
n=−∞
δ
*
Ω−2π
T n
+
.
(1.60)

42
Continuous-Time Signals and Systems
The Fourier transform of a periodic signal is a series of generalized
impulse signals at Ω= 2πn/T with weighting factors X( 2π
T n)/T being
equal to the Fourier series coefﬁcients Xn. The relation between periodic
generalized impulse signals in the time and frequency domain will be
explained (derived) later, (see Example 2.8).
1.4
FOURIER TRANSFORM AND STATIONARY PHASE METHOD
When a signal
x(t) = A(t)ejφ(t)
(1.61)
is not of a simple analytic form, it may be possible, in some cases, to obtain
an approximative expression for its Fourier transform by using the method
of stationary phase.
The method of stationary phase states that if the phase function φ(t) is
monotonous and the amplitude A(t) is sufﬁciently smooth function, then
∞
"
−∞
A(t)ejφ(t)e−jΩtdt ≃A(t0)ejφ(t0)e−jΩt0
<
2πj
|φ′′(t0)| ,
(1.62)
where t0 is the solution of
φ′(t0) = Ω.
The most signiﬁcant contribution to the integral on the left side of
(1.62) comes from the region where the phase φ(t) −Ωt of the exponential
function exp(j(φ(t) −Ωt)) is stationary in time, since the contribution of
the intervals with fast varying φ(t) −Ωt tends to zero. It means that locally
around an instant t the signal behaves as exp(j(φ′(t)t). Value
Ωi(t) = φ′(t)
is called instantaneous frequency of a signal. Around the stationary phase
instant t0 holds
d(φ(t) −Ωt)
dt
|t=t0
= 0
φ′(t0) −Ω= 0.
Around this point the phase can be expanded into a Taylor series as
φ(t) −Ωt = [φ(t0) −Ωt0] + [φ′(t0) −Ω] + 1
2φ′′(t0)t2 + ...

Ljubiša Stankovi´c
Digital Signal Processing
43
Since φ′(t0) −Ω= 0 the integral (1.62) is
∞
"
−∞
A(t)ej(φ(t)−Ωt)dt ∼= A(t0)ej(φ(t0)−Ωt0)
∞
"
−∞
ej 1
2 φ′′(t0)t2dt
where A(t) ∼= A(t0) is also used. With
∞
"
−∞
ej 1
2 at2dt =
<
2πj
|a|
the stationary phase approximation follows.
If the equation φ′(t0) = Ωhas two (or more) solutions t+
0 and t−
0 , then
the integral on the left side of (1.62) is equal to the sum of functions at both
(or more) stationary phase points. Finally, this relation holds for φ′′(t0) ̸= 0.
If φ′′(t0) = 0, then similar analysis may be performed, using the lowest
nonzero phase derivative at the stationary phase point.
Example 1.11. Consider signal
x(t) = exp(−(t2 −1)t2)exp(j4πt2 + j10πt).
Find its Fourier transform approximation by using the stationary phase
method.
⋆According to the stationary phase method,
8πt0 + 10π = Ω
t0 = Ω−10π
8π
and
φ′′(t0) = 8π.
(1.63)
The amplitude of X(Ω) is
|X(Ω)| ≃A(t0)
'''''
<
2π
φ′′(t0)
''''' = exp(−(t2
0 −1)t2
0)
=
2π
8π
= 1
2 exp
(
−
>* Ω−10π
8π
+2
−1
?* Ω−10π
8π
+2)
(1.64)
The signal, stationary phase approximation of the Fourier transform and the
numerical value of the Fourier transform amplitudes are shown in Fig.1.7

44
Continuous-Time Signals and Systems
-4
-3
-2
-1
0
1
2
3
4
-1
0
1
t
x(t)
-100
-80
-60
-40
-20
0
20
40
60
80
100
0
0.5
1
 Ω
|X(Ω)|
Stationary phase method
-100
-80
-60
-40
-20
0
20
40
60
80
100
0
0.5
1
 Ω
|X(Ω)|
Numeric calculation
Figure 1.7
The signal (top), along with the stationary phase method approximation of its
Fourier transform and the Fourier transform obtained by a numeric calculation (bottom).
Example 1.12. Consider a frequency-modulated signal
x(t) = A(t)exp(jat2N).
where A(t) is a slow-varying non-negative function. Find its Fourier trans-
form approximation by using the stationary phase method.
⋆According to the stationary phase method, we get that the stationary
phase point is 2Nat2N−1
0
= Ωwith
t0 =
* Ω
2Na
+1/(2N−1)

Ljubiša Stankovi´c
Digital Signal Processing
45
and
φ′′(t0) = 2N(2N −1)a
* Ω
2Na
+(2N−2)/(2N−1)
.
(1.65)
The amplitude and phase of X(Ω), according to (1.62), are
|X(Ω)|2 ≃A2(t0)
''''
2π
φ′′(t0)
''''
(1.66)
= A2(
* Ω
2Na
+1/(2N−1)
)
'''''
2π
(2N −1)Ω
* Ω
2aN
+1/(2N−1)'''''
arg{X(Ω)} ≃φ(t0) −Ωt0 + π/4 = (1 −2N)
2N
Ω
* Ω
2aN
+1/(2N−1)
+ π/4
for a large value of a.
For N = 1 and A(t) = 1, we get |X(Ω)|2 = |π/a| and arg{X(Ω)} =
−Ω2/(4a) + π/4.
The method of stationary phase may be deﬁned in the frequency
domain as well. For a Fourier transform
X(Ω) = B(Ω)ejθ(Ω)
(1.67)
the method of stationary phase states that if the Fourier transform phase
function θ(t) is monotonous and the amplitude B(t) is sufﬁciently smooth
function, then
x(t) = 1
2π
∞
"
−∞
B(Ω)ejθ(Ω)ejΩtdΩ≃B(Ω0)ejθ(Ω0)ejΩ0t
<
j
2π |θ′′(Ω0)| , (1.68)
where Ω0 is the solution of
−θ′(Ω0) = t,
and
tg = −θ′(Ω)
is the group delay.
Example 1.13. Consider a system with transfer function
H(Ω) = exp(−Ω2)exp(−jaΩ2/2 −jbΩ).
Find its impulse response by using the stationary phase method.

46
Continuous-Time Signals and Systems
⋆According to the stationary phase method,
aΩ0 + b = t
Ω0 = t −b
a
and
θ′′(Ω0) = −a.
The impulse response is
h(t)) ≃exp(−Ω2
0)e−jaΩ2
0/2−jbΩ0+jΩ0t
<
j
2π|θ′′(Ω0)|
= exp(−
* t −b
a
+2
)ej((t−b)2/(2a)+π/4)
=
1
2πa.
The signal amplitude is delayed for b. The second order parameter a in
the phase function scales time axis of the impulse response. This is an
undesirable effect in common systems.
Example 1.14. For a system with frequency response H(Ω) = |H(Ω)|ej0 the im-
pulse response is h(t). Find the impulse response of the systems with transfer
functions shown in Fig.1.8 with:
(a) Ha(Ω) = |H(Ω)|e−j4Ω,
(b) Hb(Ω) = |H(Ω)|e−j2πΩ2, and
(c) Hc(Ω) = |H(Ω)|
@
3
4 + 1
4 cos(2πΩ2)
A
ej0.
⋆(a) The impulse response is
ha(t) = 1
2π
∞
"
−∞
H(Ω)e−j4ΩejΩt = h(t −4).
It is delayed with respect to h(t) for t0 = 4.
(b) In this case
hb(t) = 1
2π
∞
"
−∞
H(Ω)e−j2πΩ2ejΩtdΩ.
The group delay is tg = −θ′(Ω) = 4πΩ. According to the stationary phase
method, by replacing Ωwith t/(4π), we get
hb(t) = H
* t
4π
+
ej(t2/8π+π/4)
=
1
8π2 .

Ljubiša Stankovi´c
Digital Signal Processing
47
Ω
|Ha(jΩ)|
Ω
|Hb(jΩ)|
Ω
|Hc(jΩ)|
Ω
arg{Ha(jΩ)}
Ω
arg{Hb(jΩ)}
Ω
arg{Hc(jΩ)}
 t
|ha(t)|=|h(t-4)|
 t
|hb(t)|
 t
|hc(t)|
Figure 1.8
Frequency response of systems (amplitude, top row, and phase, middle row) with
corresponding impulse responses (amplitude - bottom row).

48
Continuous-Time Signals and Systems
Nonlinear group delay causes time scaling and form change of the impulse
response. Note: Check that & ∞
−∞h2(t)dt = & ∞
−∞h2
b(t)dt. The impulse response
is calculated numerically as well. The agreement with the approximative
result is high, Fig.1.8(third row, middle column)
(c) Since 2cos(2πΩ2) = exp(j2πΩ2) + exp(−j2πΩ2)
hc(t) = 3
4 h(t) + 1
8 hb(t) + 1
8 hb(−t)
= 3
4 h(t) + 1
4 hb(t).
Here, fast variations of the amplitude result in a two-component impulse
response, one being proportional to the impulse response from case (a) and
the other proportional to the form from (b).
1.5
LAPLACE TRANSFORM
The Fourier transform could be considered as a special case of the Laplace
transform. At the beginning, Fourier’s work was even not published as an
original contribution due to this fact. The Laplace transform is deﬁned by
X(s) = L{x(t)} =
∞
"
−∞
x(t)e−stdt,
(1.69)
where s = σ + jΩis a complex number. It is obvious that the Fourier
transform is the value of a Laplace transform along the imaginary axis, σ = 0
or s = jΩ. This form of the Laplace transform is also known as the bilateral
Laplace transform (in contrast to unilateral one, where the integration limits
are from 0−to ∞).
Example 1.15. Calculate the Laplace transform of x(t) = e−atu(t).
⋆According to the deﬁnition
X(s) =
∞
"
0
e−ate−stdt = −
e−(s+a)t'''
∞
0
s + a
=
1
s + a
if
lim
t→∞e−(s+a)t = 0
or σ + a > 0, that is, σ > −a. Therefore, the region of convergence of this
Laplace transform is the region where σ > −a. The point s = −a is the pole of

Ljubiša Stankovi´c
Digital Signal Processing
49
the Laplace transform. The region of convergence is limited by a vertical line
in the complex s-plane, passing through a pole.
The Laplace transform may be considered as a Fourier transform of a
signal x(t) multiplied by exp(−σt), with varying parameter σ,
FT{x(t)e−σt} =
∞
"
−∞
x(t)e−σte−jΩtdt =
∞
"
−∞
x(t)e−stdt = X(s).
(1.70)
In this way, we may calculate the Laplace transform of functions that
are not absolutely integrable (i.e., do not satisfy condition for the Fourier
transform existence, & ∞
−∞|x(t)|dt < ∞) In these cases, for some values of
σ, the new signal x(t)e−σt may be absolutely integrable and the Laplace
transform could exist. In the previous example, the Fourier transform does
not exist for a < 0, while for a = 0 it exists in the generalized functions
sense only. Laplace transform of the considered signal always exists, with
the region of convergence σ > −a. If a > 0, then the region of convergence
σ > −a includes the line σ = 0, meaning that the Fourier transform exists.
The inverse Laplace transform is
x(t) =
1
2πj lim
T→∞
γ+jT
"
γ−jT
X(s)estds
where the integration is performed along a path in the region of conver-
gence of X(s).
Example 1.16. Consider a signal x(t) such that x(t) = 0 for |t| > T (time-limited
signal). Its Fourier transform is X(Ω). Derive the relation to calculate the
Laplace transform X(s) for any σ within the region of convergence, based
on the value of X(Ω).
⋆Based on X(Ω) the signal values are x(t) =
1
2π
& ∞
−∞X(Ω)ejΩtdΩ. The
Laplace transform is
X(s) =
T
"
−T
⎛
⎝1
2π
∞
"
−∞
X(Ω)ejΩtdΩ
⎞
⎠e−stdt
= 1
2π
∞
"
−∞
X(Ω)
T
"
−T
e−st+jΩtdtdΩ= 1
π
∞
"
−∞
X(Ω)sinh((jΩ−s)T)
jΩ−s
dΩ.
(1.71)
within the region of convergence.

50
Continuous-Time Signals and Systems
Properties of the Laplace transform may easily be generalized from
those presented for the Fourier transform, like for example
L{ax(t) + by(t)} = aL{x(t)} + bL{y(t)} = aX(s) + bY(s),
L{x(t) ∗t h(t)} = L{x(t)}L{h(t)} = X(s)H(s).
Since the Laplace transform will be used to describe linear systems de-
scribed by linear differential equations we will consider only the relation of
the signal derivatives with the corresponding forms in the Laplace domain.
In general the Laplace transform of the ﬁrst derivative dx(t)/d(t) of a signal
x(t) is
∞
"
−∞
dx(t)
dt
e−stdt = s
∞
"
−∞
x(t)e−stdt = sX(s).
This relation follows by integration in part of the ﬁrst integral, with the
assumption that the values of x(t)e−st are zero as t →±∞.
In many applications it has been assumed that the systems are causal
with corresponding causal signals used in calculations. In these cases x(t) =
0 for t < 0, i.e., x(t) = x(t)u(t). Then the so called one-sided Laplace trans-
form (unilateral Laplace transform) is used. Its deﬁnition is
X(s) =
∞
"
0
x(t)e−stdt.
When dealing with the derivatives of causal signals we have to take
care about possible discontinuity at t = 0.
In general the ﬁrst derivative of the function x(t)u(t) is
d(x(t)u(t))
dt
= dx(t)
dt
u(t) + x(0)δ(t).
The Laplace transform of the ﬁrst derivative of a causal signal is
∞
"
0
dx(t)
dt
e−stdt = x(t)e−st|∞
0 + s
∞
"
0
x(t)e−stdt = sX(s) −x(0).
Value of signal at t = 0, denoted by x(0), is the initial condition.

Ljubiša Stankovi´c
Digital Signal Processing
51
These relations can easily be generalized to higher order derivatives
∞
"
0
dnx(t)
dtn
e−stdt = sn
∞
"
0
x(t)e−stdt −sn−1x(0) −sn−2x′(0) −... −x(n−1)(0)
= snX(s) −sn−1x(0) −sn−2x′(0) + ... −x(n−1)(0).
The Laplace transform of an integral of x(t) is
L{
t
"
0
x(τ)dτ} = L{u(t) ∗t x(t)} = 1
s X(s)},
since L{u(t)} = & ∞
0 e−stdt = 1/s.
The initial and ﬁnal values of the signal are x(0) = lims→∞sX(s) and
x(∞) = lims→0 sX(s), respectively.
1.5.1
Linear Systems Described by Differential Equations
After we have established the relation between the Laplace transform and
signals derivatives we may use it to analyze the systems described by
differential equations. Consider a causal system
aN
dNy(t)
dtN
+ ... + a1
dy(t)
dt
+ a0y(t) = bM
dMx(t)
dtn
+ ... + b1
dx(t)
dt
+ b0x(t)
with the initial conditions x(0) = x′(0) = x(n−1)(0) = 0. The Laplace trans-
form of both sides of this differential equation is
aNsNY(s) + ... + a1sY(s) + a0Y(s) = bMsMX(s) + ... + b1sX(s) + b0X(s).
Transfer function of this system is of the form
H(s) = Y(s)
X(s) = bMsM + ... + b1s + b0
aNsN + ... + a1s + a0
.
Example 1.17.
A causal system is described by the differential equation
d2y(t)
dt2
+ 3dy(t)
dt
+ 2y(t) = x(t)
with the initial conditions y′(0) = 1 and y(0) = 0. Find the system output y(t)
for x(t) = e−4tu(t).

52
Continuous-Time Signals and Systems
⋆The Laplace transform of both sides is
[s2Y(s) −sy(0) −y′(0)] + 3[sY(s) −y(0)] + 2Y(s) = X(s)
or
Y(s)(s2 + 3s + 2) = X(s) + sy(0) + y′(0) + 3y(0).
With X(s) = 1/(s + 4) follows
Y(s) =
s + 5
(s + 4)(s2 + 3s + 2) =
A1
s + 4 +
A2
s + 1 +
A3
s + 2.
The coefﬁcients Ai are obtained from
Ai = (s −si)Y(s)|s=si .
For example,
A1 = (s + 4)
s + 5
(s + 4)(s2 + 3s + 2) |s=−4
= 1/6.
The other two coefﬁcients are A2 = −3/2 and A3 = 4/3.
The output signal is
y(t) = 1
6e−4tu(t) −3
2e−2tu(t) + 4
3e−tu(t).
1.5.2
Table of the Laplace Transform
Signal x(t)
Laplace transform X(s)
δ(t)
1
u(t)
1/s
eatu(t)
1
s−a
tu(t)
1/s2
eat cos(Ω0t)u(t)
s−a
(s−a)2+Ω2
0
eat sin(Ω0t)u(t)
Ω0
(s−a)2+Ω2
0
teatu(t)
1
(s−a)2
x′(t)u(t)
sX(s) −x(0)
tx(t)u(t)
−dX(s)/ds
x(t)u(t)/t
& ∞
s F(s)ds
eatx(t)u(t)
X(s −a)
x(t) ∗u(t) = & t
0 x(t)dt
X(s)/s

Ljubiša Stankovi´c
Digital Signal Processing
53
Ω
|H(jΩ)|2
N=2
Ω
|H(jΩ)|2
N=4
Ω
|H(jΩ)|2
N=32
Figure 1.9
Squared amplitude of the frequency response of a Butterworth ﬁlter of order N.
1.6
BUTTERWORTH FILTER
The most common processing systems in communications and signal pro-
cessing are ﬁlters, used to selectively pass a part of the input signal in the
frequency domain and to reduce possible interferences. The basic form is a
lowpass ﬁlter. Here we will present a simple Butterworth lowpass ﬁlter.
The squared frequency response of the Butterworth lowpass ﬁlter is
|H(jΩ)|2 =
1
1 +
B
Ω
Ωc
C2N .
It is shown in Fig.1.9. This ﬁlter deﬁnition contains two parameters. Order of
the ﬁlter is N. It is a measure of the transition sharpness from the passband
to the stopband region. For N →∞the amplitude form of an ideal lowpass
ﬁlter is achieved. The second parameter is the critical frequency. At Ω= Ωc
we get |H(jΩc)|2 = |H(0)|2 /2 = 1/2, corresponding to −3[dB] gain for any
ﬁlter order N.
The squared frequency response may be written as
H(jΩ)H(−jΩ) =
1
1 +
B jΩ
jΩc
C2N
H(s)H(−s) =
1
1 +
B
s
jΩc
C2N for s = jΩ.

54
Continuous-Time Signals and Systems
Re{s}
Im{s}
N=3
Re{s}
Im{s}
N=4
Re{s}
Im{s}
N=5
Figure 1.10
Poles of a stable Butterworth ﬁlter for N = 3, N = 4, and N = 5.
Poles of the product of transfer functions H(s)H(−s) are
* sk
jΩc
+2N
= −1 = ej(2πk+π)
sk = Ωcej(2πk+π)/2N+jπ/2 for k = 0,1,2,...,2N −1.
Poles of the Butterworth ﬁlter are located on a circle whose radius is Ωc at
the angles
αk = 2πk + π
2N
+ π
2 for k = 0,1,2,...,2N −1.
For a given ﬁlter order N and frequency Ωc the only remaining decision is
to select a half of the poles sk that belong to H(s) and to declare that the
remaining half of the poles belong to H(−s). Since we want that a ﬁlter is
stable then we chose the poles
s0,s1,...,sN−1
within the left side of the s plane, where Re{s} < 0, i.e., π/2 < αk < 3π/2.
The symmetric poles with Re{s} > 0 are the poles of H(−s). They are not
used in the ﬁlter design.
Example 1.18.
Design a lowpass Butterworth ﬁlter with:
(a) N = 3 with Ωc = 1,
(b) N = 4 with Ωc = 3.
⋆(a) Poles for N = 3 with Ωc = 1 have the phases
αk = 2πk + π
6
+ π
2 , for k = 0,1,2.

Ljubiša Stankovi´c
Digital Signal Processing
55
Their values are
s0 = cos(2π
3 ) + jsin(2π
3 ) = −1
2 + j
√
3
2
s1 = cos(2π
3 + π
3 ) + jsin(2π
3 + π
3 ) = −1
s2 = cos(2π
3 + 2π
3 ) + jsin(2π
3 + 2π
3 ) = −1
2 −j
√
3
2
with
H(s) =
c
(s + 1
2 −j
√
3
2 )(s + 1
2 + j
√
3
2 )(s + 1)
=
1
(s2 + s + 1)(s + 1)
where c = 1 is used to make H(0) = 1.
(b) Poles for N = 4 with Ωc = 3 are at angles
αk = 2πk + π
8
+ π
2 , for k = 0,1,2,3.
Their values are
s0 = 3cos( π
2 + π
8 ) + j3sin( π
2 + π
8 )
s1 = 3cos( π
2 + 3π
8 ) + j3sin( π
2 + 3π
8 )
s2 = 3cos( π
2 + 5π
8 ) + j3sin( π
2 + 5π
8 )
s3 = 3cos( π
2 + 7π
8 ) + j3sin( π
2 + 7π
8 )
with
H(s) =
c
(s2 + 2.296s + 9)(s2 + 5.543s + 9)
=
9
(s2 + 2.296s + 9)(s2 + 5.543s + 9)
where c = 9 is used to make H(0) = 1.
In practice we usually do not know the ﬁlter order, but its passband
frequency Ωp and stopband frequency Ωs, with a maximal attenuation in
the passband ap[dB] and a minimal attenuation in the stopband ap[dB], as
shown in Fig.1.11. Based on these values we can calculate the order N and
the critical frequency Ωc needed for a ﬁlter design.

56
Continuous-Time Signals and Systems
Ω
|H(jΩ)|2
Ωp
Ωs
Ap
As
1
Figure 1.11
Speciﬁcation of a Butterworth ﬁlter parameters in the passband and stopband.
The relations for N and Ωc are
1
1 +
B Ωp
Ωc
C2N ≥A2
p
(1.72)
1
1 +
B
Ωs
Ωc
C2N ≤A2
s.
Using equality in both relations, it follows
N = 1
2
ln( 1
A2p −1) −ln( 1
A2s −1)
lnΩp −lnΩs
.
Nearest greater integer is assumed for the ﬁlter order N. Then we can
use any of the relations in (1.72) with equality sign to calculate Ωc. If we
choose the ﬁrst one then Ωc will satisfy
''H(jΩp)
''2 = A2
p, while if we use
the second relation the value of Ωc will satisfy |H(jΩs)|2 = A2
s. These two
values differ. However both of them are within the deﬁned criteria for the
transfer function.
The relation
a = 20log A
or A = 10a/20 should be used for the attenuation given in [dB] .
All other ﬁlter forms, like passband and highpass, may be obtained
from a lowpass ﬁlter with appropriate signal modulations. These modula-
tions will be discussed for discrete-time ﬁlter forms in Chapter V.

Part II
Discrete Signals and Systems
57


Chapter 2
Discrete-Time Signals and Transforms
T
HE ﬁrst step in numerical processing of signals is in their discretiza-
tion in time. A continuous-time signal is converted into a sequence
of numbers, deﬁning the discrete-time signal. The basic deﬁnitions
of discrete-time signals and their transforms are presented in this chapter.
The key fact in the conversion from a continuous-time signal into a sequence
of numbers is that these two signal representations are equivalent under cer-
tain conditions. The discrete-time signal may contain the same information
as the original continuous-time signal. The sampling theorem is fundamen-
tal for this relation between two signal forms. It is presented in this chapter,
after basic deﬁnitions of discrete-time signals and systems are introduced.
2.1
DISCRETE-TIME SIGNALS
Discrete-time signals (discrete signals) are represented in a form of an or-
dered set of numbers {x(n)}. Commonly, they are obtained by sampling
continuous-time signals. There exist discrete-time signals whose indepen-
dent variable is inherently discrete in nature as well.
In the case that a discrete-time signal is obtained by sampling a
continuous-time signal, we can write (Fig. 2.1),
x(n) = x(t)|t=n∆t ∆t.
(2.1)
Discrete-time signals are deﬁned for an integer value of the argument
n. We will use the same notation for continuous-time and discrete-time
signals, x(t) and x(n). However, we hope that this will not cause any
confusion since we will use different sets of variables, for example, t and
τ for continuous time and n and m for discrete time. Also, we hope that the
59

60
Discrete-Time Signals and Transforms
t
x(t)
Δt
n
x(n) = x(t) Δt t = nΔt
Figure 2.1
Signal discretization: continuous-time signal (left) and corresponding discrete-
time signal (right).
context will always be clear, so that there is no doubt what kind of signal is
considered. Notation x[n] is sometimes used in literature for discrete-time
signals, instead of x(n).
Examples of discrete-time signals are presented next.
The discrete-time impulse signal is deﬁned by
δ(n) =
!
1, for n = 0
0, for n ̸= 0 .
(2.2)
It is presented in Fig. 2.2.
In contrast to the continuous-time impulse signal, that cannot be prac-
tically implemented and used, the discrete-time unit impulse is a signal that
can easily be implemented and used in realizations. In mathematical nota-
tion, this signal corresponds to the Kronecker delta function
δm,n =
!
1, for m = n
0, for m ̸= n.
(2.3)
Any discrete-time signal can be written in a form of a sum of shifted
and weighted discrete-time impulses,
x(n) =
∞
∑
k=−∞
x(k)δ(n −k),
(2.4)
as illustrated in Fig.2.3.
The discrete unit-step signal is deﬁned by
u(n) =
!
1, for n ≥0
0, for n < 0 .
(2.5)

Ljubiša Stankovi´c
Digital Signal Processing
61
-10
0
10
-1
0
1
t
x(n)=u(n)
  (a)
-10
0
10
-1
0
1
n
δ(n)
  (b)
-10
0
10
-1
0
1
n
x(n)=b(n)
  (c)
-10
0
10
-1
0
1
n
sin(nπ/4)
  (d)
Figure 2.2
Illustration of discrete-time signals: (a) unit-step function, (b) discrete-time im-
pulse signal, (c) boxcar signal b(n) = u(n + 2) −u(n −3), and (d) discrete-time sinusoid.
-5
0
5
-4
-2
0
2
4
n
x(n)
-5
0
5
-4
-2
0
2
4
n
 -2 δ (n+2)
-5
0
5
-4
-2
0
2
4
n
 3δ(n)
-5
0
5
-4
-2
0
2
4
n
 - δ(n-1 )
Figure 2.3
Signal x(n) along with corresponding discrete-time impulses.

62
Discrete-Time Signals and Transforms
The discrete-time impulse and the unit-step signal are related as
δ(n) = u(n) −u(n −1)
u(n) =
n
∑
k=−∞
δ(k).
The discrete-time complex sinusoidal signal is deﬁned by
x(n) = Aej(ω0n+ϕ) = Acos(ω0n + ϕ) + jAsin(ω0n + ϕ).
(2.6)
A discrete-time signal is periodic if there exists an integer N such that
x(n + N) = x(n).
(2.7)
Smallest positive integer N that satisﬁes this equation is called the
period of the discrete-time signal x(n). Note that the signal x(n) with
a period N is also periodic with any integer multiple of N. Some basic
discrete-time signals are presented in Fig. 2.2.
Example 2.1.
Check the periodicity of discrete-time signals x1(n) = sin(2πn/36),
x2(n) = cos(4πn/15 + 2), x3(n) = exp(j0.1n), x4(n) = x1(n) + x2(n), and
x5(n) = x1(n) + x3(n).
⋆Period of the discrete-time signal x1(n) = sin(2πn/36) is obtained
from 2πN1/36 = 2πk, where k is an integer. It is N1 = 36, for k = 1. The period
N2 follows from 4πN2/15 = 2πk as N2 = 15 with k = 2. Period of signal x3(n)
should be calculated from 0.1N3 = 2πk. Obviously, there is no integer k such
that N3 is an integer. This signal is not periodic. The same holds for x5(n). The
period of x4(n) is a common period for signals x1(n) and x2(n) with N1 = 36
and N2 = 15. It is N4 = 180.
A discrete-time signal is even if
x(n) = x(−n).
For an odd signal holds
x(n) = −x(−n).
Example 2.2.
Show that a discrete-time signal may be written as a sum
x(n) = xe(n) + xo(n)
where xe(n) and xo(n) are its even and odd part, respectively.

Ljubiša Stankovi´c
Digital Signal Processing
63
⋆For a signal x(n) we can form its even and odd part as
xe(n) = x(n) + x(−n)
2
xo(n) = x(n) −x(−n)
2
.
Summing these two parts, the signal x(n) is reconstructed. Note that xo(0) =
0.
A signal is Hermitian if
x(n) = x∗(−n).
Magnitude of a discrete-time signal is deﬁned as the maximal value of
the signal amplitude
Mx =
max
−∞<n<∞|x(n)|.
Energy of discrete-time signals is deﬁned by
Ex =
∞
∑
n=−∞
|x(n)|2 .
(2.8)
The instantaneous power of x(n) is Px(n) = |x(n)|2 , while the average
signal power is
PAV = lim
N→∞
1
2N + 1
N
∑
n=−N
|x(n)|2 =
#
|x(n)|2$
,
(2.9)
where
#
|x(n)|2$
is used to denote an average over large number of signal
values, as N →∞. The average power of signals with a ﬁnite energy (energy
signals) is PAV = 0. For power signals (when 0 < PAV < ∞) the energy is
inﬁnite, Ex →∞.
Example 2.3.
The energy of signal x(n) is Ex = 10. The energy of its even part is
Exe = 3. Find the energy of its odd part.
⋆The energy of signal is
Ex =
∞
∑
n=−∞
|x(n)|2 =
∞
∑
n=−∞
|xe(n) + xo(n)|2
=
∞
∑
n=−∞
[xe(n) + xo(n)][xe(n) + xo(n)]∗
=
∞
∑
n=−∞
|xe(n)|2 +
∞
∑
n=−∞
|xo(n)|2 +
∞
∑
n=−∞
[xo(n)x∗
e (n) + xe(n)x∗
o(n)].

64
Discrete-Time Signals and Transforms
The terms xo(n)x∗e (n) and xe(n)x∗o(n) in the last sum correspond to odd
signals
xo(−n)x∗
e (−n) = −xo(n)x∗
e (n)
xe(−n)x∗
o(−n) = −xe(n)x∗
o(n).
Their sum is zero,
∞
∑
n=−∞
xo(n)x∗
e (n) =
∞
∑
n=−∞
xe(n)x∗
0(n) = 0.
For the signals xe(n) and xo(n), satisfying the previous relation, we say that
they are orthogonal.
Therefore, for the energies Ex, Exe, and Exo, holds
Ex = Exe + Exo.
Obviously Ex0 = Ex −Exe = 7.
2.1.1
Discrete-Time Systems
Discrete-time (discrete) system transforms one discrete-time signal (input)
into the other (output signal)
y(n) = T{x(n)}.
(2.10)
A discrete system T{·} is linear if for any two signals x1(n) and x2(n) and
any two constants a1 and a2 holds
y(n) = T{a1x1(n) + a2x2(n)} = a1T{x1(n)} + a2T{x2(n)}.
(2.11)
A discrete system is time-invariant if for
y(n) = T{x(n)}
(2.12)
holds
T{x(n −n0)} = y(n −n0),
for any t0.
For any input signal x(n) the signal at the output of a linear time-
invariant discrete system can be calculated if we know the output to the
impulse signal. The output to the impulse signal, h(n) = T{δ(n)}, is the
impulse response.

Ljubiša Stankovi´c
Digital Signal Processing
65
-4
-2
0
2
4
6
8
-2
-1
0
1
2
3
x(n)
-4
-2
0
2
4
6
8
-2
-1
0
1
2
3
h(n)
Figure 2.4
Input signal and impulse response.
The output to an input signal x(n) is
y(n) = T{x(n)} = T
%
∞
∑
k=−∞
x(k)δ(n −k)
;
.
For a linear time-invariant discrete system we get
y(n) =
∞
∑
k=−∞
x(k)T{δ(n −k)} =
∞
∑
k=−∞
x(k)h(n −k).
(2.13)
This is a discrete-time convolution. Its notation is
x(n) ∗n h(n) =
∞
∑
k=−∞
x(k)h(n −k).
(2.14)
Discrete-time convolution is a commutative operation,
x(n) ∗n h(n) = h(n) ∗n x(n).
(2.15)
Example 2.4. Calculate discrete-time convolution of signals x(n) and h(n) shown
in Fig. 2.4.
⋆By deﬁnition, according to Fig. 2.5, we have
y(0) =
∞
∑
k=−∞
x(k)h(−k) = 1 −1 + 2 = 2,
y(1) =
∞
∑
k=−∞
x(k)h(1 −k) = −1 −1 + 1 + 4 = 3.
In a similar way y(−2) = 2, y(−1) = −1, y(2) = 6, y(3) = 2, y(4) = −1,
y(5) = −1, and y(n) = 0, for all other n. The convolution y(n) is shown in
Fig. 2.6.

66
Discrete-Time Signals and Transforms
-4
-2
0
2
4
6
8
-2
-1
0
1
2
3
n
x(k)
-4
-2
0
2
4
6
8
-2
-1
0
1
2
3
n
h(-k )
-4
-2
0
2
4
6
8
-2
-1
0
1
2
3
n
h(2- k)
-4
-2
0
2
4
6
8
-2
-1
0
1
2
3
n
h(1- k)
Figure 2.5
Signals for the output y(0), y(1), and y(2) calculation.
-4
-2
0
2
4
6
8
-2
0
2
4
6
8
n
y(n)
Figure 2.6
Resulting output signal y(n).
Example 2.5. Calculate the convolution of signals x(n) = n[u(n) −u(n −10)] and
h(n) = u(n).
⋆The convolution is
y(n) =
∞
∑
k=−∞
k[u(k) −u(k −10)]u(n −k) =
=
∑
0≤k≤9 and k≤n
k =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
n
∑
k=0
k = n n+1
2
for
0 ≤n ≤9
9
∑
k=0
k = 45
for
n > 9
= n n + 1
2
[u(n) −u(n −10)] + 45u(n −10).

Ljubiša Stankovi´c
Digital Signal Processing
67
Example 2.6.
If the response of a linear time-invariant system to the unit-step is
y(n) = T{u(n)} = e−nu(n) ﬁnd the impulse response h(n) of this system.
⋆The impulse response is
h(n) = T{δ(n)} = T{u(n) −u(n −1)} = T{u(n)} −T{u(n −1)}
= e−nu(n) −e−(n−1)u(n −1) = e−n[u(n) −e u(n −1)].
A discrete system is causal if there is no response before the input
signal appears. For causal discrete systems h(n) = 0 for n < 0 holds. For a
signal that may be an impulse response of a causal system we say that it is
a causal signal or one-sided signal.
A discrete system is stable if any input signal with ﬁnite magnitude
Mx = max−∞<t<∞|x(n)| produces the output y(n) whose values are ﬁnite,
|y(n)| < ∞. A discrete linear time-invariant system is stable if
∞
∑
m=−∞
|h(m)| < ∞.
(2.16)
The output of a linear time-invariant system is
|y(n)| = |
∞
∑
m=−∞
x(n −m)h(m)| ≤
∞
∑
m=−∞
|x(n −m)||h(m)|
≤Mx
∞
∑
m=−∞
|h(m)|.
Therefore |y(n)| < ∞if (2.16) holds. It can be shown that the absolute sum
convergence of the impulse response is the necessary condition for a linear
time-invariant discrete system to be stable as well.
2.2
FOURIER TRANSFORM OF DISCRETE-TIME SIGNALS
The Fourier transform of a discrete-time signal is deﬁned by
X(ejω) =
∞
∑
n=−∞
x(n)e−jωn.
(2.17)
Notation X(ejω) is used to emphasize the fact that it is a periodic function
of the normalized frequency ω. The period is 2π.

68
Discrete-Time Signals and Transforms
In order to establish the relation between the Fourier transform of
discrete-time signals and the Fourier transform of continuous-time signals,
X(Ω) =
∞
"
−∞
x(t)e−jΩtdt,
we will write an approximation of the Fourier transform of continuous-time
signal according to the rectangular rule of numerical integration,
X(Ω) ∼=
∞
∑
n=−∞
x(n∆t)e−jΩn∆t∆t.
(2.18)
By using the notation
x(n∆t)∆t −→x(n)
Ω∆t −→ω,
(2.19)
the previous approximation can be written as
∞
∑
n=−∞
x(n)e−jωn = X(ejω).
(2.20)
This is the Fourier transform of the discrete-time signal x(n).
Later we will show that, under certain conditions, the Fourier trans-
form X(ejω) of discrete-time signals is not just an approximation of the
Fourier transform X(Ω) of continuous-time signals, but the equality holds
(i.e., X(Ω) = X(ejω)) with Ω∆t = ω and −π ≤ω < π.
The inverse Fourier transform of discrete-time signals is obtained by
multiplying both sides of (2.20) by ejωm and integrating them over a period
of X(ejω)
∞
∑
n=−∞
x(n)
π
"
−π
e−jω(n−m)dω =
π
"
−π
X(ejω)ejωmdω.
Since
π
"
−π
e−jω(n−m)dω = 2sin((n −m)π)
(n −m)
= 2πδ(n −m),
we get
x(n) = 1
2π
π
"
−π
X(ejω)ejωndω.
(2.21)

Ljubiša Stankovi´c
Digital Signal Processing
69
Example 2.7. Find the Fourier transform of the discrete-time signal
x(n) = Ae−α|n|
where α > 0 is a real constant.
⋆The Fourier transform of this signal is
X(ejω) = A +
−1
∑
n=−∞
Aeαn−jωn +
∞
∑
n=1
Ae−αn−jωn
= A
(
1 +
ejω−α
1 −ejω−α +
e−jω−α
1 −e−jω−α
)
= A
1 −e−2α
1 −2e−α cos(ω) + e−2α .
(2.22)
Example 2.8. Find the inverse Fourier transform of a discrete-time signal if
X(ejω) = 2πδ(ω) for −π ≤ω < π and X(ejω) = 2π ∑∞
k=−∞δ(ω + 2kπ) for
any ω.
⋆By deﬁnition
x(n) = 1
2π
π
"
−π
2πδ(ω)ejωndω = 1.
Therefore, the Fourier transform of signal x(n) = 1 is
∞
∑
n=−∞
e−jωn = 2π
∞
∑
k=−∞
δ(ω + 2kπ).
(2.23)
The equivalent form in the continuous-time domain is obtained (by using
ω = ΩT and δ(TΩ) = δ(Ω)/T) as
∞
∑
n=−∞
ejΩnT = 2π
∞
∑
k=−∞
δ(ΩT + 2kπ) = 2π
T
∞
∑
k=−∞
δ(Ω+ 2kπ/T).
(2.24)
2.2.1
Properties
The Fourier transform of discrete-time signals is linear
FT{ax(n) + by(n)} = aX(ejω) + bY(ejω),
(2.25)
where X(ejω) and Y(ejω) are the Fourier transforms of the discrete-time
signals x(n) and y(n), respectively.

70
Discrete-Time Signals and Transforms
With respect to the signal shift and modulation the Fourier transform
of discrete-time signals behaves in the same way as the Fourier transform
of continuous-time signals,
FT{x(n −n0)} = X(ejω)ejn0ω
(2.26)
and
FT{x(n)ejω0n} = X(ej(ω−ω0)).
(2.27)
Example 2.9. The Fourier transform of a discrete-time signal x(n) is X(ejω).
Find the Fourier transform of y(n) = x(2n).
⋆For y(n) = x(2n) the Fourier transform is
FT{x(2n)} =
∞
∑
n=−∞
x(2n)e−jωn
=
∞
∑
n=−∞
x(n) + (−1)nx(n)
2
e−jωn/2
= 1
2
∞
∑
n=−∞
[x(n) + e−jnπx(n)]e−jωn/2
= 1
2[X(ejω/2) + X(ej(ω/2+π))] = 1
2[X(ejω/2) + X(ej(ω+2π)/2)].
(2.28)
The period of this Fourier transform is 2π. Period of X(ejω/2) is 4π.
Example 2.10. Calculate the Fourier transform of the discrete-time signal (rectan-
gular window),
wR(n) = u(N + n) −u(n −N −1).
(2.29)
Write the Fourier transform of a Hann(ing) window
wH(n) = 1
2 [1 + cos(nπ/N)][u(N + n) −u(n −N −1)].
⋆By deﬁnition
WR(ejω) =
N
∑
n=−N
e−jωn = ejωN 1 −e−jω(2N+1)
1 −e−jω
= sin(ω 2N+1
2
)
sin(ω/2) .
(2.30)
The Fourier transform of the Hann(ing) window can easily be written as
WH(ejω) = 1
2
N
∑
n=−N
*
1 + 1
2ejnπ/N + 1
2e−jnπ/N
+
e−jωn =
(2.31)
= sin(ω 2N+1
2
)
2sin(ω/2) + sin((ω −π
N ) 2N+1
2
)
4sin((ω −π
N )/2) + sin((ω + π
N ) 2N+1
2
)
4sin((ω + π
N )/2) .
(2.32)

Ljubiša Stankovi´c
Digital Signal Processing
71
-10
0
10
0
0.5
1
n
wR(n)
N=8
 ω
WR(ejω)
π
- π
-10
0
10
0
0.5
1
n
wR(n)
N=4
 ω
WR(ejω)
π
- π
-10
0
10
0
0.5
1
n
wH(n)
N=8
 ω
WH(ejω)
π
- π
Figure 2.7
Discrete-time signal in a form of rectangular window of the widths 2N + 1 = 9 and
2N + 1 = 17 samples (top and middle), and a Hann(ing) window with 2N + 1 = 17 (bottom).
The time domain values are on the left while the Fourier transforms of these discrete-time
signals are on the right.
As the window width increases in the time domain the main lobe width in
the Fourier domain is narrowing. The ﬁrst zero value of the Fourier transform
of a rectangular window is at ω(2N + 1)/2 = π, i.e., at ω = 2π/(2N + 1)
where 2N + 1 is the signal duration. In the case of a Hann(ing) window
the main lobe is wider as compared to the rectangular window of the same
width, but its convergence is much faster with very reduced oscillations in
the Fourier transform, Fig.2.7.

72
Discrete-Time Signals and Transforms
The Fourier transform of a convolution of discrete-time signals,
FT{x(n) ∗n h(n)} =
∞
∑
n=−∞
∞
∑
k=−∞
x(k)h(n −k)e−jnω = X(ejω)H(ejω), (2.33)
is equal to the product of the Fourier transforms of corresponding discrete-
time signals.
The Fourier transform of the impulse response
H(ejω) =
∞
∑
n=−∞
h(n)e−jωn
is called frequency response of a discrete linear time-invariant system.
Example 2.11. Find the output of a discrete linear time-invariant system with
frequency response H(ejω) if the input signals are:
(a) x(n) = Aejω0n and (b) x(n) = Acos(ω0n + ϕ). What is the output if
the impulse response h(n) is real-valued?
⋆(a) The output signal to the input x(n) is
y(n) =
∞
∑
k=−∞
h(k)x(n −k) =
∞
∑
k=−∞
h(k)Aejω0(n−k)
= Aejω0n
∞
∑
k=−∞
h(k)Ae−jω0k = Aejω0nH(ejω0)
= A
'''H(ejω0)
'''ej(ω0n+arg{H(ejω0}).
(b) The input signal can be written as
x(n) = Acos(ω0n + ϕ) = A
2 ej(ω0n+ϕ) + A
2 e−j(ω0n+ϕ).
According to the linearity property, the output is
y(n) = A
2
'''H(ejω0)
'''ej(ω0n+ϕ)+jarg{H(ejω0)}
+ A
2
'''H(e−jω0)
'''e−j(ω0n+ϕ)+jarg{H(e−jω0)}.
For a real-valued impulse response holds
H(ejω) = H∗(e−jω)

Ljubiša Stankovi´c
Digital Signal Processing
73
and
H(ejω) =
∞
∑
n=−∞
h(n)cos(ωn) + j
∞
∑
n=−∞
h(n)sin(ωn)
'''H(ejω)
'''
2
=
'''H(e−jω)
'''
2
arg{H(ejω) = arctan
! ∑∞
n=−∞h(n)sin(ωn)
∑∞
n=−∞h(n)cos(ωn)
6
= −arg{H(e−jω).
The output for a real-valued impulse response is
y(n) = A
'''H(ejω0)
'''cos(ω0n + ϕ + arg{H(ejω0)).
Example 2.12. Find the impulse response of an ideal discrete-time differentiator
H(ejω) = jω for −π ≤ω < π.
⋆The impulse response is
h(n) = 1
2π
π
"
−π
jωejωndω =
j
2π
π
"
−π
ωcos(ωn)dω −1
2π
π
"
−π
ωsin(ωn)dω
= −1
π
π
"
0
ωsin(ωn)dω = 1
π ω cos(ωn)
n
''''
π
0
+ 1
πn
π
"
0
cos(ωn)dω.
Since the last integral is equal to 0 the ﬁnal result is
h(n) = cos(πn)
n
= (−1)n
n
for n ̸= 0 and h(n) = 0 for n = 0. Using samples n = ±1, ±2, ...,±N the
approximation of the frequency response is
HN(ejω) =
N
∑
n=−N
h(n)e−jωn = 2j
N
∑
n=1
(−1)n−1 sin(ωn)
n
.
Note that this system is not causal.
The Fourier transform of a product of discrete-time signals is equal to
the convolution of the Fourier transforms in frequency,
FT{x(n)h(n)} = 1
2π
π
"
−π
X(ejθ)H(ej(ω−θ))dθ = X(ejω) ∗ω H(ejω).
(2.34)
This convolution is periodic with period 2π (circular convolution).

74
Discrete-Time Signals and Transforms
2.2.2
Spectral Energy and Power Density
Parseval’s theorem for discrete-time signals reads
∞
∑
n=−∞
x(n)y∗(n) =
∞
∑
n=−∞
1
2π
π
"
−π
X(ejω)ejωny∗(n)dω
(2.35)
= 1
2π
π
"
−π
X(ejω)
(
∞
∑
n=−∞
(e−jωny(n))∗
)
dω = 1
2π
π
"
−π
X(ejω)Y∗(ejω)dω.
For a signal x(n) Parseval’s theorem is
∞
∑
n=−∞
|x(n)|2 = 1
2π
π
"
−π
'''X(ejω)
'''
2
dω = Ex.
Function
''X(ejω)
''2 is the spectral energy density of signal x(n).
Since the average power of a signal x(n) is deﬁned by
PAV = lim
N→∞
1
2N + 1
N
∑
n=−N
|x(n)|2 ,
(2.36)
its power spectral density may be deﬁned as
Pxx(ejω) = lim
N→∞
1
2N + 1
'''XN(ejω)
'''
2
,
(2.37)
where the Fourier transform of x(n) within −N ≤n ≤N is denoted by
XN(ejω). The power spectral density can be written as
Pxx(ejω) = lim
N→∞
1
2N + 1
N
∑
n=−N
N
∑
m=−N
x(n)x∗(m)e−jω(n−m).
(2.38)
For a very speciﬁc signal x(n) = Aej(ω0n+ϕ) which satisﬁes r(k) =
x(n)x∗(n −k) = A2ejω0k the power spectral density is
Pxx(ejω) = lim
N→∞
1
2N + 1
2N
∑
k=−2N
(2N + 1 −|k|)r(k)e−jωk
since the value r(0), for n −m = k = 0, appears 2N + 1 times along the
diagonal in n,m domain in (2.38). The value for n −m = k = ±1 appears 2N

Ljubiša Stankovi´c
Digital Signal Processing
75
times, and so on. The value r(k), for n −m = k, appears 2N + 1 −|k| times
in double summation (2.38). Note that Pxx(ejω) in this case is the Fourier
transform of r(k) multiplied by a Bartlett window 1 −|k|/(2N + 1).
2.3
SAMPLING THEOREM IN THE TIME DOMAIN
Continuous-time signal x(t), whose Fourier transform X(Ω) is limited with
Ωm = 2π fm, where fm is a frequency in [Hz], while Ωm is frequency in
[rad/s], i.e.,
X(Ω) = 0 for |Ω| > Ωm,
(2.39)
can be reconstructed, for any t, based on samples taken with a sampling
interval ∆t,
x(n) = x(n∆t)∆t,
such that
∆t < π
Ωm
=
1
2fm
.
Now we will prove this statement. Since we have assumed a limited
frequency duration of X(Ω) we can make its periodic extension
Xp(Ω) =
∞
∑
m=−∞
X(Ω+ 2Ω0m)
(2.40)
with a period 2Ω0. It is very important to note that Xp(Ω) = X(Ω) for
|Ω| < Ω0 if
Ω0 > Ωm.
In this case, it is possible to make transformation from X(Ω) to Xp(Ω) and
back without losing any information.
Of course, that would not be the case if Ω0 > Ωm did not hold. By
periodic extension of X(Ω), in that case, overlapping (aliasing) would have
occurred in Xp(Ω). It would not be reversible. Then it would not be possible
to recover X(Ω) from Xp(Ω). The periodic extension is illustrated in Fig. 2.8.
The periodic function Xp(Ω) can be expanded into Fourier series with
coefﬁcients
X−n =
1
2Ω0
Ω0
"
−Ω0
Xp(Ω)ejπΩn/Ω0dΩ=
1
2Ω0
∞
"
−∞
X(Ω)ejπΩn/Ω0dΩ.
The integration limits are extended to the inﬁnity since X(Ω) = Xp(Ω)
within the basic period interval and X(Ω) = 0 outside this interval.

76
Discrete-Time Signals and Transforms
Ω
X(Ω)
- Ωm
Ωm
Xp(Ω) = X(Ω)
- Ω0 < Ω < Ω0
- Ω0
Ω0
Ω
- Ωm
Ωm
Figure 2.8
The Fourier transform of a signal, with X(Ω) = 0 for |Ω| > Ωm (top) and its
periodically extended version, with period 2Ω0 > 2Ωm (bottom).
The inverse Fourier transform of continuous-time signal is
x(t) = 1
2π
∞
"
−∞
X(Ω)ejΩtdΩ.
(2.41)
By comparing the last two equations, we easily conclude that
X−n = π
Ω0
x(t)|t=πn/Ω0 = x(n∆t)∆t
(2.42)
∆t = π
Ω0
,
meaning that the Fourier series coefﬁcients of the periodically extended
Fourier transform of X(Ω) are the samples of the signal, taken with the
sampling interval ∆t = π/Ω0.
Therefore, the samples of a signal and the periodically extended
Fourier transform are the Fourier series pair
X−n = x(n∆t)∆t ←→Xp(Ω) =
∞
∑
m=−∞
X(Ω+ 2Ω0m)
(2.43)
with ∆t = π/Ω0.

Ljubiša Stankovi´c
Digital Signal Processing
77
The reconstruction formula for x(t) then follows from
x(t) = 1
2π
∞
"
−∞
X(Ω)ejΩtdΩ= 1
2π
Ω0
"
−Ω0
Xp(Ω)ejΩtdΩ
(2.44)
= 1
2π
Ω0
"
−Ω0
(
∞
∑
n=−∞
XnejπnΩ/Ω0
)
ejΩtdΩ
= 1
2π
Ω0
"
−Ω0
(
∞
∑
n=−∞
x(−n∆t)∆tejπnΩ/Ω0
)
ejΩtdΩ
as
x(t) =
∞
∑
n=−∞
x(n∆t)sin( π
∆t(t −n∆t))
π
∆t(t −n∆t)
.
(2.45)
The signal x(t), for any t, is expressed in terms of its samples x(n∆t).
Example 2.13. The last relation can be used to prove that X(Ω) = X(ejω) with
Ω∆t = ω and |ω| < π for the signals sampled at the rate satisfying the
sampling theorem.
⋆Starting from
X(Ω) =
∞
"
−∞
x(t)e−jΩtdt
the signal x(t), satisfying the sampling theorem, can by written in terms of
samples, according to the third row of (2.44), as
x(t) = 1
2π
Ω0
"
−Ω0
(
∞
∑
n=−∞
x(n∆t)∆te−j∆tnθ
)
ejθtdθ.
It follows
X(Ω) =
∞
"
−∞
1
2π
Ω0
"
−Ω0
(
∞
∑
n=−∞
x(n∆t)∆te−j∆tnθ
)
ejθtdθe−jΩtdt
=
∞
∑
n=−∞
x(n∆t)∆t
Ω0
"
−Ω0
δ(θ −Ω)e−j∆tnθdθ
=
∞
∑
n=−∞
x(n∆t)∆te−j∆tnΩfor |Ω| < Ω0
(2.46)

78
Discrete-Time Signals and Transforms
resulting in
X(Ω) =
∞
∑
n=−∞
x(n)e−jωn for |ω| < π
with ω = Ω∆t and x(n) = x(n∆t)∆t.
Example 2.14. If the highest frequency in a signal x(t) is Ωm1 and the highest
frequency in a signal y(t) is Ωm2 what should be the sampling interval for the
signal x(t)y(t) and for the signal x(t −t1)y∗(t −t2)? The highest frequency
Ωm in a signal is used in the sense that the Fourier transform of the signal is
zero for |Ω| > Ωm.
⋆The Fourier transform of a product x(t)y(t) is a convolution of the
Fourier transforms X(Ω) and Y(Ω). Since these functions are of limited
duration |Ω| < Ωm1 and |Ω| < Ωm2, respectively, in general their convolution
is limited to the interval |Ω| < Ωm1 + Ωm2. Therefore, the sampling interval
for y(t) should be
∆t <
π
Ωm1 + Ωm2
.
Shifts in time and complex conjugate operation do not change the Fourier
transform width. Therefore the conclusion remains the same for x(t −
t1)y∗(t −t2).
Example 2.15. If the signal
x(t) = e−|t|
is sampled with ∆t = 0.1, write the Fourier transform of the obtained discrete-
time signal: (a) by a periodical extension of the continuous-time Fourier
transform and (b) by a direct calculation based on the discrete-time signal.
Comment on the expected error due to the discretization.
⋆The Fourier transform of this signal is
X(Ω) =
∞
"
−∞
x(t)e−jΩtdt =
0
"
−∞
ete−jΩtdt +
∞
"
0
e−te−jΩtdt
=
1
1 −jΩ+
1
1 + jΩ=
2
1 + Ω2 .
(2.47)
After sampling with ∆t = 0.1, the Fourier transform is periodically extended
with period 2Ω0 = 2π/∆t = 20π.
(a) The periodic Fourier transform is
Xp(Ω) = ... +
2
1 + (Ω+ 20π)2 +
2
1 + Ω2 +
2
1 + (Ω−20π)2 + ...

Ljubiša Stankovi´c
Digital Signal Processing
79
Thus, the value of Xp(Ω) at the period ending points ±10π will approxi-
mately be Xp(±10π) = 2/(1 + 100π2) ∼= 0.002. Comparing with the maxi-
mum value Xp(0) = 2, it means that the expected error due to the discretiza-
tion of this signal (since it does not strictly satisfy the sampling theorem) will
be of a 0.1% order.
(b) The discrete-time signal obtained by sampling x(t) = exp(−|t|)
with ∆t = 0.1 is x(n) = 0.1e−0.1|n|. Its Fourier transform is already calculated
with A = 0.1 and α = 0.1, eq.(2.22). The result is
X(ejω) = 0.1
1 −e−0.2
1 −2e−0.1 cos(ω) + e−0.2 .
(2.48)
Therefore, the exact value of an inﬁnite sum in Xp(Ω) is X(ejω) with ω =
Ω∆t = 0.1Ω
Xp(Ω) =
∞
∑
k=−∞
2
1 + (Ω+ 20kπ)2 = 0.1
1 −e−0.2
1 −2e−0.1 cos(0.1Ω) + e−0.2 .
Note that in this way we solve an interesting mathematical problem of
ﬁnding a sum of an inﬁnite series.
For Ω= 0, the original value of the Fourier transform is X(0) = 2.
In the signal that could be reconstructed based on the discretized signal
Xp(0) = 0.1(1 + e−0.1)/(1 −e−0.1) = 2.00167. The increase of 0.00167 is due
to periods overlapping. It manifests as an aliasing error in X(0). The value of
error corresponds to our previous conclusion of about a 0.1% error order.
Example 2.16. A continuous-time signal x(t) = cos(25πt + π/4) + sin(50πt −
π/3) is sampled with a step ∆t = 1/100 and a discrete-time signal x(n) =
x(n∆t)∆t is formed. The discrete-time signal is processed using the system
whose impulse response is
h(n) = 1
2δ(n) + 1
4δ(n −2) + 1
4δ(n + 2)
Find the output signal y(n) and the corresponding continuous-time signal
ya(t) as the result of convolution.
⋆The discrete-time input signal is
x(n) = [cos(nπ/4 + π/4) + sin(nπ/2 −π/3)]∆t
with the Fourier transform
X(ejω) = π
100
∞
∑
k=−∞
[δ(ω + π
4 + 2kπ)e−jπ/4 + δ(ω −π
4 + 2kπ)ejπ/4]
+ π
100
∞
∑
k=−∞
[δ(ω + π
2 + 2kπ)e−jπ/6 + δ(ω −π
2 + 2kπ)ejπ/6].

80
Discrete-Time Signals and Transforms
The frequency response of the discrete system is
H(ejω) =
∞
∑
n=−∞
h(n)e−jωn = 1
2(1 + cos(2ω)).
The frequency response values at the frequencies of X(ejω), within the basic
period −π ≤ω < π, are H(e±jπ/4) = 1/2 and H(e±jπ/2) = 0. Therefore the
output signal Fourier transforms is
Y(ejω) = H(ejω)X(ejω) = π
200[δ(ω + π
4 )e−jπ/4 + δ(ω −π
4 )ejπ/4]
for −π ≤ω < π.
The output discrete-time signal is
y(n) = 1
2 cos(nπ/4 + π/4)∆t
corresponding to the continuous-time signal
y(t) = 1
2 cos(n π
4∆t + π/4) = 1
2 cos(25πt + π/4).
Find the output signal for h(n) = ∑2
i=−2 δ(n −i).
2.4
PROBLEMS
Problem 2.1. Check the periodicity and ﬁnd the period of signals:
(a) x(n) = sin(2πn/32), (b) x(n) = cos(9πn/82), (c) x(n) = ejn/32, and (d)
x(n) = sin(πn/5) + cos(5πn/6) −sin(πn/4).
Problem 2.2. Check the linearity and time-invariance of the discrete system
described by equation
y(n) = x(n) + 2.
Problem 2.3. The output of a linear time-invariant discrete system to the
input signal x(n) = u(n) is y(n) = 2−nu(n). Find the impulse response h(n).
Is the system stable?
Problem 2.4. Find the convolution
y(n) = x(n) ∗x(n)
for x(n) = u(n) −u(n −5).

Ljubiša Stankovi´c
Digital Signal Processing
81
-2
0
2
4
6
8
0
2
4
6
8
10
12
h(n) = h1(n)*h2(n)*h2(n)
-2
0
2
4
6
8
-2
-1
0
1
2
3
x(n)
Figure 2.9
Problem 2.7, impulse response h(n) (left) and Problem 2.14, discrete signal x(n)
(right).
Problem 2.5. Find the convolution of signals x(n) = e−|n| and h(n) = u(n +
5) −u(n −6).
Problem 2.6. A discrete system consists of systems with impulse responses
h1(n) = e−anu(n), h2(n) = e−bnu(n), and h3(n) = u(n). Find the impulse
response of the resulting system for:
(a) Systems h1(n), h2(n), and h3(n) connected in parallel,
(b) System h1(n) connected in parallel with a cascade of systems h2(n)
and h3(n).
Problem 2.7. Consider three causal linear time-invariant systems in cas-
cade. Impulse responses of these systems are h1(n), h2(n), and h2(n), re-
spectively. The impulse response of the second and the third system is
h2(n) = u(n) −u(n −2), while the impulse response of the whole system,
h(n) = h1(n) ∗n h2(n) ∗n h2(n),
is shown in Fig. 2.9 (left).
Find h1(n) and y(n) = h(n) ∗n x(n), with x(n) = δ(n) −δ(n −1).
Problem 2.8. Find the output of discrete system whose impulse response is
h(n) = ne−n/2u(n)
to the input signal
x(n) = 5sin(πn/5) −3cos(πn/3 + π/6).
Find
S =
∞
∑
n=0
ne−n/2.

82
Discrete-Time Signals and Transforms
Problem 2.9. Find the Fourier transform of:
(a) x(n) = u(n),
(b) x(n) = 2cos(ω0n)u(n), and
(c) y(n) = ∑∞
k=−∞x(n + kN) if the Fourier transform of discrete-time
signal x(n) is X(ejω).
Problem 2.10. In implementing an approximation of a signal derivative we
may use a system with impulse response
h(n) = a[δ(n + 1) −δ(n −1)] + b[δ(n + 2) −δ(n −2)].
Find the constants a and b such that
H(ejω) ∼= jω for small ω,
i.e.,
dH(ejω)
dω
''''
ω=0
= j and d2H(ejω)
dω2
''''
ω=0
= 0.
Problem 2.11. Find the Fourier transform of the following discrete-time
signal (triangular window)
wT(n) =
*
1 −
|n|
N + 1
+
[u(n + N) −u(n −N −1)].
with N being an even number.
Problem 2.12. Find the value of integral
I = 1
2π
π
"
−π
sin2((N + 1)ω/2)
sin2(ω/2)
dω.
Problem 2.13. A window is formed as
w(n) = wH(n + N) + wH(n) + wH(n −N)
where wH(n) is the Hann(ing) window
wH(n) = 1
2 [1 + cos(nπ/N)][u(N + n) −u(n −N −1)].
Plot the window w(n) and express its Fourier transform as a function of the
Fourier transform of the Hann(ing) window WH(ejω). Generalize the results
for
w(n) =
K
∑
k=−K
wH(n + kN).

Ljubiša Stankovi´c
Digital Signal Processing
83
Problem 2.14. A discrete-time signal x(n) is given in Fig. 2.9 (right). Without
calculating its Fourier transform X(ejω) ﬁnd
X(ej0),
X(ejπ),
π
"
−π
X(ejω)dω,
π
"
−π
'''X(ejω)
'''
2
dω,
and a signal whose Fourier transform is the real part of X(ejω), denoted by
Re{X(ejω)}.
Problem 2.15. Find the Fourier transform of discrete-time signal
y(n) = ne−n/4u(n).
Using this Fourier transform ﬁnd the center of gravity of signal x(n) =
e−n/4u(n) deﬁned by
ng =
∞
∑
n=−∞
nx(n)
∞
∑
n=−∞
x(n)
.
Problem 2.16. Impulse response of a discrete system is given as:
(a)
h(n) = sin(nπ/3)
nπ
,with h(0) = 1/3,
(b)
h(n) = sin2(nπ/3)
(nπ)2
,
(c)
h(n) = sin((n −2)π/4)
(n −2)π
.
Show that the frequency response of the system with h(n) = sin(nπ/3)/nπ
is H(ejω) = 1 for |ω| ≤π/3 and H(ejω) = 0 for π/3 < |ω| < π. Find the
frequency responses in other two cases. Find the systems output to the input
signal x(n) = sin(nπ/6).
Problem 2.17. A continuous-time signal x(t) = cos(20πt + π/4) + sin(90πt)
is sampled with a step ∆t and a discrete-time signal x(n) = x(n∆t)∆t is
formed. The signal is convolved with h(n) = sin(nπ/2)/(nπ). (a) What is
the result of this convolution for ∆t = 1/100? (b) If the signal is sampled
with ∆t = 1/50 what is the output signal? (c) Find the result of convolution
for ∆t = 3/100.

84
Discrete-Time Signals and Transforms
Problem 2.18. Analytic part xa(n) of a discrete-time signal x(n) is deﬁned
in the frequency domain by
Xa(ejω) =
⎧
⎨
⎩
2X(ejω)
for
0 < ω < π
X(ejω)
for
ω = 0
0
for
−π ≤ω < 0
.
In the time domain the analytic part can be written as
xa(n) = x(n) + jxh(n),
where xh(n) is the Hilbert transform of x(n). Find the impulse response of
the system that transforms a signal x(n) into its Hilbert transform (Hilbert
transformer).
Problem 2.19. The Fourier transform of a continuous signal x(t) is nonzero
only within 3Ω1 < Ω< 5Ω1. Find the maximum possible sampling interval
∆t such that the signal can be reconstructed based on the samples x(n∆t).
Problem 2.20. For a signal whose Fourier transform is zero for frequencies
Ω≥Ωm = 2π fm = π/∆t show that
x(t) =
∞
"
−∞
x(τ)sin(π(t −τ)/∆t)
π(t −τ)
dτ.
Write a discrete-time version of this relation.
Problem 2.21. Sampling of a signal is done twice, with the sampling interval
∆t = 2π/Ωm that is twice larger than the sampling interval required by the
sampling theorem (∆t = π/Ωm is required). After ﬁrst sampling process,
the discrete-time signal x1(n) = ∆tx(n∆t) is formed, while after the second
sampling process signal x2(n) = ∆tx(n∆t + a) is formed. Show that we can
reconstruct continuous-time signal x(t) based on x1(n) and x2(n) if a ̸= k∆t,
that is, if samples x1(n) and x2(n) do not overlap in continuous-time.
Problem 2.22. In general, a sinusoidal signal x(t) = Asin(Ω0t + ϕ) is de-
scribed with three parameters A,Ω0 and ϕ. Thus, generally speaking, three
points of x(t) would be sufﬁcient to ﬁnd three signal parameters. If we know
the signal x(t) at t = t0, t = t0 + ∆t and t = t0 −∆t what is the relation
and conditions to reconstruct, for example, Ω0, which is usually the most
important parameter of a sinusoid?

Ljubiša Stankovi´c
Digital Signal Processing
85
Problem 2.23. Show that the relation among the amplitudes of a signal
x(n) and its even and odd parts xe(n) = [x(n) + x(−n)]/2 and xo(n) =
[x(n) −x(−n)]/2 is
As(n) ≤|xe(n)| + |xo(n)| ≤
√
2As(n)
with As(n) > 0 deﬁned by A2
s(n) =
@
|x(n)|2 + |x(−n)|2A
/2.
2.5
SOLUTIONS
Solution 2.1. (a) Calculate x(n + N) = sin(2π(n + N)/32). For 2πN/32 =
2kπ, k = 1,2,..., x(n + N) = x(n) holds. The smallest integer N satisfying the
previous condition is N = 32 with k = 1. The period of signal is N = 32.
(b) For this signal x(n + N) = cos(9πn/82 + 9πN/82) = x(n) for
9πN/82 = 2kπ, k = 1,2,.... The period follows from N = 164k/9. The period
of this signal is N = 164 for k = 9.
(c) In this case x(n + N) = ej(n/32+N/32). The relation N/32 = 2kπ,
k = 1,2,..., produces N = 64kπ. This is not an integer for any k, meaning
that the signal is not periodic.
(d) The periods of signal components are obtained from N1 = 10k,
N2 = 12k/5, and N3 = 8k. The smallest value of N when N1 = N2 = N3 = N
is N = 120 containing 12 periods of sin(πn/5), 50 periods of cos(5πn/6),
and 15 periods of sin(πn/4).
Solution 2.2. For linearity we have to check the system output to the linear
combination of input signals x1(n) and x2(n),
T{a1x1(n) + a2x2(n)} = a1x1(n) + a2x2(n) + 2.
This not equal to
a1y1(n) + a2y2(n) = a1x(n) + 2a1 + a2x2(n) + 2a2.
System is not linear.
This system is time-invariant since
T{x(n−N) = x(n −N) + 2 = y(n −N).
Solution 2.3. The impulse response is deﬁned by
h(n) = T{δ(n)}.

86
Discrete-Time Signals and Transforms
It can be written as
h(n) = T{u(n) −u(n −1)}.
For a linear time-invariant discrete system holds
h(n) = T{u(n)} −T{u(n −1)}.
In this case it means
h(n) = T{x(n)} −T{x(n −1)}
= y(n) −y(n −1) = 2−nu(n) −2−(n−1)u(n −1)
= δ(n) + 2−nu(n −1) −2−(n−1)u(n −1)
= δ(n) + 2−n(1 −2)u(n −1) = δ(n) −2−nu(n −1).
For this system
∞
∑
n=−∞
|h(n)| = 1 +
∞
∑
n=1
2−n = 1 +
2−1
1 −2−1 = 2.
The system is stable since the sum of absolute values of impulse response is
ﬁnite.
Solution 2.4. The convolution is calculated sample by sample as
y(0) =
∞
∑
k=−∞
x(k)x(−k) = x(0)x(0) = 1
y(1) =
∞
∑
k=−∞
x(k)x(1 −k) = x(0)x(1) + x(1)x(0) = 2
y(−1) =
∞
∑
k=−∞
x(k)x(−1 −k) = 0
y(2) =
∞
∑
k=−∞
x(k)x(2 −k) = 3
...
The calculation, along with the ﬁnal result y(n), is presented in
Fig.2.10.

Ljubiša Stankovi´c
Digital Signal Processing
87
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
k
x(k)
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
k
x(-k )
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
k
x(1-k )
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
k
x(2-k )
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
k
x(-1-k )
-15
-10
-5
0
5
10
15
0
2
4
6
n
x(n)* x(n)
Figure 2.10
Illustration of a discrete-time signal convolution calculation.
Solution 2.5. Based on the convolution deﬁnition
y(n) = x(n) ∗h(n) =
∞
∑
k=−∞
x(k)h(n −k) =
(2.49)
=
∞
∑
k=−∞
e−|k|(u((n −k) + 5) −u((n −k) −6))
with
u((n −k) + 5) =
!
1, for k ≤n + 5
0, for k > n + 5
and
u((n −k) −6) =
!
1, for k ≤n −6
0, for k > n −6
we get
(u((n −k) + 5) −u((n −k) −6)) =
!
1, for n −6 < k ≤n + 5
0, elsewhere.

88
Discrete-Time Signals and Transforms
The inﬁnite sum in (2.49) reduces to the terms for n −5 ≤k ≤n + 5
y(n) =
n+5
∑
k=n−5
e−|k|.
Since
|k| =
!
k, for k ≥0
−k, for k < 0 ,
we have three cases:
1) For n + 5 ≤0, i.e., n ≤−5, we have k ≤0 for all terms. Therefore |k| = −k,
y(n) =
n+5
∑
k=n−5
ek = en−5 1 −e11
1 −e = en e−5 −e6
1 −e
= en e0.5
e0.5
e−5.5 −e5.5
e−0.5 −e0.5 = en sinh5.5
sinh0.5
2) For n −5 ≥0, the lowest k = n −5 is greater than 0. Then k ≥0 for all
terms and |k| = k with
y(n) =
n+5
∑
k=n−5
e−k = e−n+5 1 −e−11
1 −e−1 = e−n e5 −e−6
1 −e−1
= e−n e−0.5
e−0.5
e5.5 −e−5.5
e0.5 −e−0.5 = e−n sinh5.5
sinh0.5.
3) For −5 < n < 5, index k can assume positive and negative values. The
convolution is split into two sums as
y(n) =
n+5
∑
k=n−5
e−|k| =
−1
∑
k=n−5
ek +
n+5
∑
k=0
e−k =
5−n
∑
k=1
e−k +
n+5
∑
k=0
e−k
= e−1 1 −e−(5−n)
1 −e−1
+ 1 −e−(n+6)
1 −e−1
=
= e−1/2
1 −en−5
e1/2 −e−1/2 + e1/2 1 −e−(n+6)
e1/2 −e−1/2
=
1
e0.5 −e−0.5 (e−0.5 −en−5.5 + e0.5 −e−n−5.5) =
= −e−5.5(en + e−n) + e−0.5 + e0.5
e0.5 −e−0.5
= cosh0.5 −e−5.5 cosh(n)
sinh0.5
.

Ljubiša Stankovi´c
Digital Signal Processing
89
Finally we can write
y(n) =
⎧
⎪
⎨
⎪
⎩
e−|n| sinh5.5
sinh0.5
for |n| ≥5
cosh0.5−e−5.5 cosh(n)
sinh0.5
for |n| < 5.
Solution 2.6. (a) For a parallel connection of systems
y(n) = y1(n) + y2(n) + y3(n)
=
∞
∑
k=−∞
h1(k)x(n −k) +
∞
∑
k=−∞
h2(k)x(n −k) +
∞
∑
k=−∞
h3(k)x(n −k)
=
∞
∑
k=−∞
[h1(k) + h2(k) + h3(k)]x(n −k).
The resulting impulse response is
h(n) = h1(k) + h2(k) + h3(k)
= [e−an + e−bn + 1]u(n).
(b) For a cascade of systems with h2(n) and h3(n) holds
y2(n) =
∞
∑
k=−∞
h2(k)x(n −k) =
∞
∑
k=−∞
h2(n −k)x(k)
y3(n) =
∞
∑
m=−∞
h3(m)y2(n −m) =
∞
∑
m=−∞
h3(m)
∞
∑
k=−∞
h2(n −m −k)x(k)
=
∞
∑
k=−∞
∞
∑
m=−∞
h3(m)h2(n −m −k)x(k) =
∞
∑
k=−∞
h23(n −k)x(k)
where
h23(n) =
∞
∑
m=−∞
h3(m)h2(n −m) = h2(n) ∗h3(n).
The impulse response of the whole system is
h(n) = h1(n) + h23(n) = h1(n) + h2(n) ∗h3(n),
with
h2(n) ∗h3(n) =
∞
∑
m=−∞
e−b(n−m)u(n −m)u(m)
= u(n)
n
∑
m=0
e−b(n−m) = e−bn 1 −eb(n+1)
1 −eb
u(n) = e−bn −eb
1 −eb
u(n).

90
Discrete-Time Signals and Transforms
Solution 2.7. Since we know h2(n), we can calculate
h2(n) ∗n h2(n) = δ(n) + 2δ(n −1) + δ(n −2).
Therefore, the total impulse response
h(n) = h1(n) ∗n [h2(n) ∗n h2(n)]
= h1(n) + 2h1(n −1) + h1(n −2)
h1(n) = h(n) −2h1(n −1) −h1(n −2).
From the last relation it follows h1(n) = 0 for n < 0, h1(0) = h(0) = 1,
h1(1) = h(1) −2h1(0) = 3, h1(2) = h(2) −2h1(1) −h1(0) = 3, h1(3) = 2,
h1(4) = 1, h1(5) = 0, and h1(n) = 0 for n > 5.
Output to x(n) = δ(n) −δ(n −1) can be easily calculated as
y(n) = h(n) −h(n −1).
Solution 2.8. Instead of a direct convolution we will calculate the frequency
response of discrete system as
H(ejω) =
∞
∑
n=−∞
h(n)e−jωn.
Find ﬁrst the transform of e−n/2u(n),
H1(ejω) =
∞
∑
n=0
e−n/2e−jωn =
1
1 −e−(1/2+jω)
and differentiate both sides with respect to ω
−j
∞
∑
n=0
ne−n/2e−jωn =
−je−(1/2+jω)
(1 −e−(1/2+jω))2 .
It follows
H(ejω) =
∞
∑
n=0
ne−n/2e−jωn =
e−(1/2+jω)
(1 −e−(1/2+jω))2 .
The output for a real-valued h(n) is
y(n) = 5
'''H(ejπ/10)
'''sin(πn/5 + arg{H(ejπ/10})
−3
'''H(ejπ/6)
'''cos(πn/3 + π/6 +
'''H(ejπ/6)
''')
=14.1587sin(πn/5 −1.1481)
−5.7339cos(πn/3 + π/6 −1.6605).

Ljubiša Stankovi´c
Digital Signal Processing
91
Value of the sum S is
S =
∞
∑
n=0
ne−n/2 = H(ej0) =
√e
(√e −1)2 .
Solution 2.9. (a) The unit step signal can be written as
x(n) = u(n) = lim
a→0
-1
2e−anu(n) + 1
2 −1
2e−anu(−n −1)
.
= lim
a→0xa(n).
The Fourier transform of xa(n) is
Xa(ejω) =
∞
∑
n=−∞
-1
2e−anu(n) + 1
2 −1
2e−anu(−n −1)
.
e−jωn
=
1
2
1 −e−a−jω +
∞
∑
k=−∞
πδ(ω + 2kπ) −
1
2ea+jω
1 −ea+jω
X(ejω) = lim
a→0Xa(ejω) =
1
1 −e−jω +
∞
∑
k=−∞
πδ(ω + 2kπ).
The result from (2.23) is used to transform the constant signal equal to 1/2.
(b) This signal is
x(n) = 2cos(ω0n)u(n) = (ejω0n + e−jω0n)u(n).
Its Fourier transform is
X(ejω) =
1
1 −e−j(ω−ω0) +
∞
∑
k=−∞
πδ(ω −ω0 + 2kπ)
+
1
1 −e−j(ω+ω0) +
∞
∑
k=−∞
πδ(ω + ω0 + 2kπ)
= 2
1 −e−jω cos(ω0)
1 −2cos(ω0)e−jω + e−j2ω
+
∞
∑
k=−∞
π [δ(ω −ω0 + 2kπ) + δ(ω + ω0 + 2kπ)].
(c) For a periodic signal y(n) the Fourier transform is
Y(ejω) =
∞
∑
k=−∞
∞
∑
n=−∞
x(n + kN)e−jωn =
∞
∑
k=−∞
X(ejω)ejωkN
= X(ejω)
∞
∑
k=−∞
ejωkN.

92
Discrete-Time Signals and Transforms
Using (2.23) we get
Y(ejω) = X(ejω)2π
∞
∑
k=−∞
δ(ωN + 2kπ) = X(ejω)2π
N
∞
∑
k=−∞
δ(ω + 2kπ
N ).
Solution 2.10. For the impulse response h(n) the frequency response is
H(ejω) = 2ajsin(ω) + 2jbsin(2ω).
The ﬁrst derivative of H(ejω) at ω = 0 is
dH(ejω)
dω
''''
ω=0
= 2aj + 4jb = j,
while the second derivative at ω = 0 is
d2H(ejω)
dω2
''''
ω=0
= −2aj −8jb = 0.
The constants a and b follow from the system
a + 2b = 1/2
a + 4b = 0
as b = −1/4 and a = 1 with the impulse response
h(n) = δ(n + 1) −δ(n −1) −1
4(δ(n + 2) −δ(n −2)).
Solution 2.11. Note that
wT(n) =
1
N + 1wR(n) ∗n wR(n)
where wR(n) = u(n + N/2) −u(n −N/2 −1) is the rectangular window.
Since
WR(ejω) = sin(ω N+1
2 )
sin(ω/2) ,
we have
WT(ejω) =
1
N + 1WR(ejω)WR(ejω) =
1
N + 1
sin2(ω N+1
2 )
sin2(ω/2) .

Ljubiša Stankovi´c
Digital Signal Processing
93
Solution 2.12. The integral represents the energy of a discrete-time signal
with Fourier transform
X(ejω) = sin(ω N+1
2 )
sin(ω/2) .
This signal is the rectangular window, x(n) = u(n + N/2) −u(n −N/2 −1).
Its energy is
I = 1
2π
π
"
−π
sin2((N + 1)ω/2)
sin2(ω/2)
dω =
N/2
∑
n=−N/2
x2(n) =
N/2
∑
n=−N/2
1 = N + 1.
This integral is also equal to wT(0) multiplied by N + 1.
Solution 2.13. The Hann(ing) window
wH(n) = 1
2 [1 + cos(nπ/N)][u(N + n) −u(n −N −1)].
is of duration −N ≤n ≤N −1. Thus the windows wH(n) and wH(n −N)
overlap within 0 ≤n ≤N −1. Within this interval the new window is
w(n) = wH(n) + wH(n −N)
= 1
2 [1 + cos(nπ/N)] + 1
2 [1 + cos((n −N)π/N)]
= 1 + 1
2 cos(nπ/N) + 1
2 cos(nπ/N −π) = 1.
The same holds for −N ≤n ≤−1 when
w(n) = wH(n + N) + wH(n) = 1.
The resulting window is
w(n) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0
for
n < −2N
1
2 [1 −cos(nπ/N)]
for
−2N + 1 ≤n ≤−N + 1
1
for
−N ≤n ≤N −1
1
2 [1 −cos(nπ/N)]
for
N ≤n ≤2N −1
0
for
n > 2N −1
since 1
2 [1 + cos((n ± N)π/N) = 1
2 [1 −cos(nπ/N)]. The Fourier transform
of the resulting window, in terms of the Fourier transform of the Hann(ing)

94
Discrete-Time Signals and Transforms
window WH(ejω), is
W(ejω) = WH(ejω)e−jωN + WH(ejω) + WH(ejω)ejωN
= WH(ejω)[1 + 2cos(ωN)].
For
w(n) =
K
∑
k=−K
wH(n + kN)
we get
w(n) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0
for
n < −(K + 1)N
1
2
D
1 + cos((n + KN) π
N )
E
for
−(K + 1)N + 1 ≤n ≤−KN+1
1
for
−KN ≤n ≤KN −1
1
2
D
1 + cos((n −KN) π
N )
E
for
KN ≤n ≤(K + 1)N −1
0
for
n > (K + 1)N −1
with
W(ejω) = WH(ejω)
K
∑
k=−K
e−jωkN = ejωKN 1 −e−jω(2K+1)N
1 −e−jωN
= WH(ejω)sin(ω(2K + 1)N/2)
sin(ωN/2)
.
Similar results hold for the Hamming and triangular window. The results
can be generalized for shifts of N/2, N/4,...
For very large K the second term variations in W(ejω) are much faster
than the variations of WH(ejω). Thus, for large K the Fourier transform
W(ejω) approaches to the Fourier transform of a rectangular window of the
width (2K + 1)N.
Solution 2.14. Based on the deﬁnition of the Fourier transform of discrete-
time signals,
X(ej0) =
∞
∑
n=−∞
x(n) = 7,
X(ejπ) =
∞
∑
n=−∞
x(n)(−1)n = 1,
π
"
−π
X(ejω)dω = 2πx(0) = 4π,

Ljubiša Stankovi´c
Digital Signal Processing
95
π
"
−π
'''X(ejω)
'''
2
dω = 2π
∞
∑
n=−∞
|x(n)|2 = 30π.
Finally, X(ejω) = Re{X(ejω)} + jIm{X(ejω)} and X∗(ejω) = Re{X(ejω)} −
jIm{X(ejω)}. Thus,
Re{X(ejω)} = 1
2
B
X(ejω) + X∗(ejω)
C
.
The inverse Fourier transform of Re{X(ejω)} is
y(n) = 1
2(x(n) + x∗(−n)).
Solution 2.15. The Fourier transform of y(n) is
Y(ejω) =
∞
∑
n=−∞
ne−n/4u(n)e−jωn = j d
dω
>
∞
∑
n=0
e−n/4e−jωn
?
= j d
dω
1
1 −e−1/4−jω =
e−1/4−jω
(1 −e−1/4−jω)2 .
The center of gravity of x(n) = e−n/4u(n) is
ng =
∞
∑
n=−∞
nx(n)
∞
∑
n=−∞
x(n)
= Y(ej0)
X(ej0) =
e−1/4−jω
(1−e−1/4−jω)2 |ω=0
1
1−e−1/4−jω |ω=0
=
1
e1/4 −1 = 3.52.
Solution 2.16. (a) The inverse Fourier transform of
H(ejω) =
!
1
for
|ω| ≤π/3
0
for
π/3 < |ω| < π
is
h(n) = 1
2π
π/3
"
−π/3
ejωndω = ejωn
2jπn
''''
π/3
−π/3
= sin(πn/3)
πn
.
The value of frequency response at the input signal frequency ω = ±π/6 is
H(e±jπ/6) = 1. The output signal is, y(n) = sin(nπ/6).
(b) The frequency response, in this case, is H(ejω) ∗ω H(ejω), resulting in
y(n) = 0.25sin(nπ/6).
(c) Output signal in this case is y(n) = sin((n −2)π/6) = sin(nπ/6 −π/3).

96
Discrete-Time Signals and Transforms
Solution 2.17. For the signal
x(t) = cos(20πt + π/4) + sin(90πt),
corresponding discrete-time signal is
x(n) = cos(20πn∆t + π/4)∆t + sin(90πn∆t)∆t.
(a) For ∆t = 1/100
x(n) = cos(0.2πn + π/4)/100 + sin(0.9πn)/100
with the Fourier transform
X(ejω) = π
100
∞
∑
k=−∞
[δ(ω −0.2π + 2kπ)ejπ/4 + δ(ω + 0.2π + 2kπ)e−jπ/4]
+
π
j100
∞
∑
k=−∞
[δ(ω −0.9π + 2kπ) −δ(ω + 0.9π + 2kπ)].
Since the Fourier transform of h(n) = sin(nπ/2)/(nπ) is H(ejω) = 1 for
|ω| ≤π/2 and H(ejω) = 0 for π/2 < |ω| < π, the result of a convolution
is equal to the output of system with transfer function H(ejω) to the input
signal x(n). In this case
x(n) = cos(0.2πn + π/4)/100.
Continuous-time signal corresponding to the output discrete-time signal is
y(t) = cos(20πt + π/4), Fig.2.11(top).
(b) If the signal is sampled with ∆t = 1/50 the discrete-time signal is
x(n) = cos(0.4πn + π/4)/50 + sin(1.8πn)/50,
with the Fourier transform
X(ejω) = π
50
∞
∑
k=−∞
[δ(ω −0.4π + 2kπ)ejπ/4 + δ(ω + 0.4π + 2kπ)e−jπ/4]
+ π
j50
∞
∑
k=−∞
[δ(ω −1.8π + 2kπ) −δ(ω + 1.8π + 2kπ)].

Ljubiša Stankovi´c
Digital Signal Processing
97
-2 π
- π
π
2π
- π/2
π/2
3π/2
0
ω
H(ejω), X(ejω)
1
-2 π
- π
π
2π
- π/2
π/2
3π/2
0
ω
H(ejω), X(ejω)
1
-2 π
- π
π
2π
- π/2
π/2
3π/2
0
ω
H(ejω), X(ejω)
1
Figure 2.11
Illustration of the system output with various sampling intervals (a)-(c).
The Fourier transform components within −π ≤ω < π are
X(ejω) = π
50[δ(ω −0.4π)ejπ/4 + δ(ω + 0.4π)e−jπ/4]
+ π
j50[δ(ω −1.8π + 2π) −δ(ω + 1.8π −2π)]
= π
50[δ(ω −0.4π)ejπ/4 + δ(ω + 0.4π)e−jπ/4]
+ π
j50[δ(ω + 0.2π) −δ(ω −0.2π)].
The result of convolution is x(n) = cos(0.4πn + π/4)/50 −sin(0.2πn)/50.
Corresponding continuous-time signal is x(t)=cos(20πt+π/4)−sin(10πt).
The component −sin(10πt) does not correspond to any frequency in the
input signal, , Fig.2.11(middle). This effect is illustrated in Fig.2.12.
(c) For ∆t = 3/100
x(n) = 3cos(0.6πn + π/4)/100 + 3sin(2.7πn)/100.

98
Discrete-Time Signals and Transforms
n
x(n)
Figure 2.12
Illustration of the aliasing caused frequency change, from signal sin(90πt) to
signal −sin(10πt).
The Fourier transform components within −π ≤ω < π are
X(ejω) = 3π
100[δ(ω −0.6π)ejπ/4 + δ(ω + 0.6π)e−jπ/4]
+ 3π
j100[δ(ω −2.7π + 2π) −δ(ω + 2.7π −2π)].
The result of convolution is y(n) = 0, , Fig.2.11(bottom).
Solution 2.18. The Fourier transform of an analytic part of a signal is
Xa(ejω) =
⎧
⎨
⎩
2X(ejω)
for
0 < ω < π
X(ejω)
for
ω = 0
0
for
−π ≤ω < 0
= X(ejω) + sign(ω)(X(ejω) = X(ejω) + Xh(ejω).
The frequency response of the discrete Hilbert transformer is
H(ejω) =
⎧
⎨
⎩
1
for
0 < ω < π
0
for
ω = 0
−1
for
−π ≤ω < 0
= sign(ω)
for −π ≤ω < π. The impulse response is
h(n) =
π
"
−π
sign(ω)ejωndω = 2sin2(πn/2)
πn
.

Ljubiša Stankovi´c
Digital Signal Processing
99
-2 π
- π
π
2π
0
ω
H(ejω)
1
n
h(n)
0
2/π
Figure 2.13
Frequency and impulse response of the discrete-time Hilbert transformer.
-3 -2 -1
0
1
2
3
4
5
6
7
0
0.5
1
1.5
Ω/Ω1
X(Ω)
-3 -2 -1
0
1
2
3
4
5
6
7
0
0.5
1
1.5
Ω/Ω1
Xp(Ω)
Figure 2.14
Problem 2.19: illustration of the Fourier transform periodic extension.
For n = 0 the impulse response is h(0) = 0, Fig.2.13.
Solution 2.19. By a direct application of the sampling theorem we could
conclude that the sampling interval should be related to the maximum
frequency 5Ω1 as ∆t = π/(5Ω1), corresponding to the periodical extension
of the Fourier transform X(Ω) with period 10Ω1. However, in this case,
there is no need to use such a large period in order to achieve that two
periods do not overlap. It is sufﬁcient to use the period equal to 2Ω1, as
shown in Fig. 2.14. We will be able to reconstruct the signal, with some
additional processing.
It is obvious that after signal sampling with ∆t = π/Ω1 (periodic ex-
tension of Fourier transform with 2Ω1) the basic period −Ω1 < Ω< Ω1 will
contain the original Fourier transform shifted for 4Ω1. The reconstructed
signal is
x(t) = ej4Ω1t
∞
∑
n=−∞
x(n∆t)sin(π(t −n∆t)/∆t)
π(t −n∆t)/∆t
with ∆t = π/Ω1.

100
Discrete-Time Signals and Transforms
Solution 2.20. For signal whose Fourier transform is zero for frequencies
Ω≥Ωm = 2π fm = π/∆t hods
X(Ω) = X(Ω)H(Ω)
where
H(Ω) =
!
1
for
|Ω| < π/∆t
0
for
|Ω| ≥π/∆t
.
The impulse response of H(Ω) is
h(t) = 1
2π
π/∆t
&
−π/∆t
ejΩtdΩ= sin(πt/∆t)
πt
.
Then x(t) = x(t) ∗h(t) produces
x(t) =
∞
"
−∞
x(τ)h(t −τ)dτ =
∞
"
−∞
x(τ)sin(π(t −τ)/∆t)
π(t −τ)
dτ.
In order to write this relation in discrete-time form note that
X(Ω) = Xp(Ω)H(Ω)
(2.50)
holds if the Fourier transform of signal X(Ω) is periodically extended with
period 2 π
∆t ≥2Ωm to produce
X(Ω) ∗Ω
∞
∑
k=−∞
2πδ
*
Ω−2π
∆t k
+
= Xp(Ω).
Convolution in the frequency domain corresponds to the product of signals
in the time domain
x(t)
∞
∑
n=−∞
δ(t + n∆t)∆t = IFT{Xp(Ω)} = xp(t).
(2.51)
Relation (1.60)
2π
∆t
∞
∑
k=−∞
δ
*
Ω−2π
∆t k
+
= FT
%
∞
∑
n=−∞
δ(t + n∆t)
;
= FT
%
∞
∑
n=−∞
δ(t −n∆t)
;
is used.

Ljubiša Stankovi´c
Digital Signal Processing
101
From (2.50) and then (2.51) follows
x(t) = xp(t) ∗t h(t) =
∞
"
−∞
x(τ)
∞
∑
n=−∞
δ(τ −n∆t)h(t −τ)∆tdτ
=
∞
∑
n=−∞
x(n∆t)h(t −n∆t)∆t =
∞
∑
n=−∞
x(n∆t)sin( π
∆t(t −n∆t))
π
∆t(t −n∆t)
.
(2.52)
The convergence of function sin(t)/t is very slow.
The previous derivation provides a possibility that a smooth transition
of H(Ω) is used for Ωm ≤|Ω| ≤Ωm + ∆Ωm. This region of smooth changes
from H(Ω) = 1 for |Ω| < Ωm to H(Ω) = 0 for |Ω| ≥Ωm + ∆Ωm improves
the convergence of h(t), Fig.2.15. The sampling step should be (Ωm +
∆Ωm
2 ) = π/∆t so that the periodic extension of X(Ω)H(Ω) does not include
overlapped X(Ω) values. The impulse response h(t) can be then used in the
reconstruction formula
x(t) =
∞
∑
n=−∞
x(n∆t)h(t −n∆t),
with a reduction of the sampling interval to ∆t = π/(Ωm + ∆Ωm
2 ) with
respect to ∆t = π/Ωm.
Solution 2.21. The Fourier transforms of discrete-time signals, in continu-
ous frequency notation, are periodically extended versions of X(Ω) with the
period 2π/∆t,
X1(Ω) =
∞
∑
n−−∞
X(Ω+ 2πn/∆t),
X2(Ω) =
∞
∑
n−−∞
X(Ω+ 2πn/∆t)ej(Ω+2πn/∆t)a.
Within the basic period (considering positive frequencies 0 ≤Ω< Ωm), only
two periods overlap
X1(Ω) = X(Ω) + X(Ω−2π/∆t),
X2(Ω) = X(Ω)ejΩa + X(Ω−2π/∆t)ej(Ω−2π/∆t)a.
The second term X(Ω−2π/∆t) in these relations is the overlapped period
(aliasing) that should be eliminated using these two equations. The original

102
Discrete-Time Signals and Transforms
Ω
H(Ω)
X(Ω)
- Ωm
Ωm
Xp(Ω) = X(Ω)
- Ω0 < Ω < Ω0
- Ω0
Ω0
Ω
- Ωm
Ωm
H(Ω)
X(Ω)
Xp(Ω) = X(Ω)
- Ω0 < Ω < Ω0
- Ω0
Ω0
Ω
- Ωm
Ωm
X(Ω)
Figure 2.15
Smoothed ﬁlter in the sampling theorem illustration (ﬁrst two graphs) versus
original sampling theorem relation within ﬁltering framework.
signal’s Fourier transform X(Ω) follows as
X(Ω) = X1(Ω)e−j2πa/∆t −X2(Ω)e−jΩa
e−j2πa/∆t −1
for a ̸= k∆t.
Similarly for negative frequencies, within the basic period −Ωm < Ω< 0,
follows
X(Ω) = X1(Ω)ej2πa/∆t −X2(Ω)e−jΩa
ej2πa/∆t −1
for a ̸= k∆t.

Ljubiša Stankovi´c
Digital Signal Processing
103
Therefore, the signal can be reconstructed from two independent
discrete-time signals undersampled with factor of two. A similar result
could be derived for N independently sampled, N times undersampled sig-
nals.
Solution 2.22. It is easy to show that
x(t0 + ∆t) + x(t0 −∆t)
2x(t0)
= Asin(Ω0t0 + ϕ + Ω0∆t) + Asin(Ω0t0 + ϕ −Ω0∆t)
2Asin(Ω0t0 + ϕ)
= 2sin(Ω0t0 + ϕ)cos(Ω0∆t)
2sin(Ω0t0 + ϕ)
= cos(Ω0∆t),
with
Ω0 = 1
∆t arccos
* x(t0 + ∆t) + x(t0 −∆t)
2x(t0)
+
.
The condition for a unique solution is that the argument of cosine is 0 ≤
Ω0∆t ≤π, limiting the approach to small values of ∆t.
In addition, here we will discuss the discrete complex-valued signal.
For a complex sinusoid x(n) = Aexp(j2πk0n/N + φ0), with available two
samples x(n1) = Aexp(jϕ(n1)) and x(n2) = Aexp(jϕ(n2)), from
x(n1)
x(n2) = exp(j2πk0(n1 −n2)/N)
follows
2πk0(n1 −n2)/N = ϕ(n1) −ϕ(n2) + 2kπ,
where k is an arbitrary integer. Then
k0 = ϕ(n1) −ϕ(n2)
2π(n1 −n2) N +
k
n1 −n2
N.
(2.53)
Let us analyze the ambiguous term kN/(n1 −n2) role in the determination
of k0. For n1 −n2 = 1, this term is kN, meaning that any frequency k0
would be ambiguous with kN. Any value k0 + kN for k ̸= 0, in this case,
will be outside the basic period 0 ≤k ≤N −1. Thus, we may ﬁnd k0
in a unique way, within 0 ≤k0 ≤N −1. However, for n1 −n2 = L > 1,
the terms kN/(n1 −n2) = kN/L produce shifts within the frequency basic
period. Then several possible solutions for the frequency k0 are obtained.
For example, for N = 16 and k0 = 5 if we use n1 = 1 and n2 = 5, a possible

104
Discrete-Time Signals and Transforms
solution of (2.53) is k0 = 5, but also
k0 = 5 + 16k/4,
or k0 = 9, k0 = 13, and k0 = 1, for k0 within 0 ≤k0 ≤15, are possible solutions
for frequency.
Solution 2.23. For the absolute values of even and odd part it holds
|xe(n)|2 + |xo(n)|2 =
''''
x(n) + x(−n)
2
''''
2
+
''''
x(n) −x(−n)
2
''''
2
.
From this relation follows
|xe(n)|2 + |xo(n)|2 = |x(n)|2 + |x(−n)|2
2
= A2
s(n).
Obviously |xe(n)|2 ≤A2
s(n) and |xo(n)|2 ≤A2
s(n). Replacing |xo(n)| =
F
A2s(n) −|xe(n)|2 into |xe(n)| + |xo(n)| we get
|xe(n)| + |xo(n)| = |xe(n)| +
F
A2s(n) −|xe(n)|2 .
Now we have to check the function
f (χ) = χ +
F
A2s(n) −χ2
for variable χ within 0 ≤χ ≤|As(n)|. Variable χ stands for |xe(n)|. By dif-
ferentiating f (χ) with respect to χ we get d f (χ)/dχ = 1 −χ/
,
A2s(n) −χ2.
The stationary point is obtained from d f (χ)/dχ = 0 for χ = As(n)/
√
2.
The derivative d f (χ)/dχ is positive for χ < As(n)/
√
2 and negative for
χ > As(n)/
√
2. Thus the stationary point is the position of the function
maximum. The maximal function value is
√
2As(n) since
|xe(n)| + |xo(n)| ≤As(n)
√
2
+
=
A2s(n) −A2s(n)
2
=
√
2As(n).
The minimal value is achieved at the interval ending points for χ = 0 or
χ = As(n), producing
As(n) ≤|xe(n)| + |xo(n)| ≤
√
2As(n).

Ljubiša Stankovi´c
Digital Signal Processing
105
2.6
EXERCISE
Exercise 2.1. Calculate the convolution of signals x(n) = n[u(n) −u(n −3)]
and h(n) = δ(n + 1) + 2δ(n) −δ(n −2).
Exercise 2.2. Find the convolution of signals x(n) = e−|n| and h(n) = u(3 −
n)u(3 + n).
Exercise 2.3. The output of a linear time-invariant discrete system to the
input signal x(n) = u(n) is y(n) = ( 1
3n + n)u(n). Find the impulse response
h(n). Is the system stable?
Exercise 2.4. For signal x(n) = nu(5 −n)u(n + 5) ﬁnd the values of X(ej0),
X(ejπ), & π
−π X(ejω)dω, and & π
−π |X(ejω)|2dω without the Fourier transform
calculation. Check the results by calculating the Fourier transform.
Exercise 2.5. For a signal x(n) at an instant m a signal y(n) = x(m −
n)x∗(m + n) is formed. Show that the Fourier transform of y(n) is real-
valued. What is the Fourier transform of y(n) if x(n) = Aexp(jan2/4 +
j2ω0n)? Find the Fourier transform of z(m) = x(m −n)x∗(m + n) for a given
n.
Note: The Fourier transform of y(n) is the Wigner distribution of x(n)
for a given m, while the Fourier transform of z(m) is the Ambiguity function
of x(n) for a given n.
Exercise 2.6. For a signal x(n) with Fourier transform X(ejω) ﬁnd the
Fourier transform of x(2n). Find the Fourier transform of y1(2n) = x(2n)
and y1(2n + 1) = 0. What is the Fourier transform of x(2n + 1) and what is
the Fourier transform of y2(2n) = 0 and y2(2n + 1) = x(2n + 1). Check the
result by showing that Y1(ejω) + Y2(ejω) = X(ejω).
Exercise 2.7. For a real-valued signal ﬁnd the relation between the Fourier
transform of signal X(ejω) and the Hartley transform
H(ejω) =
∞
∑
n=−∞
x(n)[cos(ωn) + sin(ωn)].
Write this relation if the signal is real-valued and even, x(n) = x(−n).
Exercise 2.8. Systems with impulse responses h1(n), h2(n) and h3(n) are
connected in cascade. If the impulse responses h2(n) = h3(n) = u(n) −
u(n −2) and the resulting impulse response is h(n) = δ(n) + 5δ(n −1) +
10δ(n −2) + 11δ(n −3) + 8δ(n −4) + 4δ(n −5) + δ(n −6). Find the impulse
response h1(n).

106
Discrete-Time Signals and Transforms
Exercise 2.9. Continuous-time signal x(t) = sin(100πt) + cos(180πt) +
sin(200πt + π/4) is sampled with ∆t = 1/125 and used as an input to the
system with transfer function H(ejω) = 1 for |ω| < 3π/4 and H(ejω) = 0
for |ω| ≥3π/4. What is the discrete-time output of this system? What is
the corresponding continuous-time output signal? What should be the sam-
pling interval so that the continuous-time output signal y(t) is equal to the
input signal x(t)?

Chapter 3
Discrete Fourier Transform
D
ISCRETE-TIME signals can be processed on digital computers in the
time domain. Their Fourier transform is a function of continuous
frequency. For numeric processing of discrete-time signals in the
frequency domain their Fourier transform should be discretized as well.
Discretization in the frequency domain will enable numeric processing of
discrete-time signals in both time and frequency domain.
3.1
DFT DEFINITION
The discrete Fourier transform (DFT) is deﬁned by
DFT{x(n)} = X(k) =
N−1
∑
n=0
x(n)e−j2πkn/N
(3.1)
for k = 0,1,2,..., N −1.
In order to establish the relation between the DFT with the Fourier
transform of discrete-time signals, consider a discrete-time signal x(n) of
limited duration. Assume that nonzero samples of x(n) are within 0 ≤n ≤
N0 −1. Its Fourier transform is
X(ejω) =
N0−1
∑
n=0
x(n)e−jωn.
The DFT values can be considered as the frequency domain samples of the
Fourier transform of discrete-time signals, taken at ∆ω = 2π/N. There are
N frequency samples within the period −π ≤ω < π,
X(k) = X(ej2πk/N) = X(ejω)
'''
ω=k∆ω=2πk/N .
(3.2)
107

108
Discrete Fourier Transform
n
x(n) = x(t) Δt t = nΔt
N0
0
x(n)
n
N
0
xp(n)
Figure 3.1
Periodic extension of a discrete-time signal.
In order to examine how the Fourier Transform sampling in the fre-
quency domain inﬂuences the signal in the time domain, we will form a
periodic extension of x(n) with a period N ≥N0, Fig.3.1.
With N being greater or equal to the signal duration N0, we will
be able to reconstruct the original signal x(n) from its periodic extension
xp(n). Furthermore, we will assume that the periodic signal xp(n) is formed
from the samples of periodic continuous-time signal xp(t) with a period T
(corresponding to N signal samples within the period, T = N∆t). Its Fourier
series coefﬁcients are
Xk = 1
T
T
"
0
xp(t)e−j2πkt/Tdt.
Assuming that the sampling theorem is satisﬁed, the integral can be re-
placed by a sum (in the sense of Example 2.13)
Xk = 1
T
N−1
∑
n=0
x(n∆t)e−j2πkn∆t/T∆t

Ljubiša Stankovi´c
Digital Signal Processing
109
with xp(t) = x(t) within 0 ≤t < T. Using T/∆t = N, x(n∆t)∆t = x(n) and
X(k) = TXk this sum can be written as
X(k) =
N−1
∑
n=0
x(n)e−j2πkn/N.
(3.3)
Therefore, the relation between the DFT and the Fourier series coefﬁ-
cients is
X(k) = TXk.
(3.4)
Sampling the Fourier transform of a discrete-time signal corresponds to
the periodical extension of the original discrete-time signal in time by the
period N. The period N in time is equal to the number of samples of the
Fourier transform within one period in frequency. We can conclude that this
periodic extension in time (discretization in frequency) will not inﬂuence
the possibility to recover the original signal if the original discrete-time
signal duration was not longer than N (the number of samples in the Fourier
transform of discrete-time signal).
The inverse DFT is obtained by multiplying both sides of the DFT
deﬁnition (3.1) by ej2πkm/N and summing over k
N−1
∑
k=0
X(k)ej2πmk/N =
N−1
∑
n=0
x(n)
N−1
∑
k=0
ej2πk(m−n)/N
with
N−1
∑
k=0
ej2πk(m−n)/N =
1 −ej2π(m−n)
1 −ej2π(m−n)/N = Nδ(m −n),
for 0 ≤m,n ≤N −1. The inverse discrete Fourier transform (IDFT) of signal
x(n) is
x(n) = 1
N
N−1
∑
k=0
X(k)ej2πnk/N.
(3.5)
for 0 ≤n ≤N −1.
The signal calculated by using the IDFT is, by deﬁnition, periodic with
the period N since
x(n + N) = 1
N
N−1
∑
k=0
X(k)ej2π(n+N)k/N = x(n).
Therefore the DFT of a signal x(n) calculated using the signal samples
within 0 ≤n ≤N −1 assumes that the signal x(n) is periodically extended

110
Discrete Fourier Transform
with period N as
IDFT{DFT{x(n)}} =
∞
∑
m=−∞
x(n + mN)
with
∞
∑
m=−∞
x(n + mN) = x(n) for 0 ≤n ≤N −1.
The values of this periodical extension within the basic period are equal to
x(n). This is a circular extension of signal x(n). The following notations are
also used for this kind of the signal x(n) extension
IDFT{DFT{x(n)}} = x(n mod N) = x((n))N.
The original aperiodic signal is then
x(n) = IDFT{DFT{x(n)}}(u(n) −u(n −N)),
assuming that the initial DFT was calculated for signal samples x(n) within
0 ≤n ≤N −1.
In literature it is quite common to use the same notation for both x(n)
and IDFT{DFT{x(n)}} having in mind that any DFT calculation with N
signal samples implicitly assumes a periodic extension of the original signal
x(n) with period N. Thus, we will use this kind of notation, except in the
cases when we want to emphasize a difference in the results when the
inherent periodicity in the signal (when the DFT is used) is not properly
taken into account.
Example 3.1. For the signals x(n) = 2cos(2πn/8) for 0 ≤n ≤7 and x(n) =
2cos(2πn/16) for 0 ≤n ≤7 plot the periodic signals IDFT{DFT{x(n)}} with
N = 8 without calculating the DFTs.
Example 3.2. ⋆The periodic extensions of these signals resulting from
IDFT{DFT{x(n)}} =
∞
∑
m=−∞
x(n + 8m)
are shown in Fig.3.2.
Example 3.3. For a signal x(n) whose values are x(0) = 1, x(1) = 1/2, x(2) = −1,
and x(3) = 1/2 ﬁnd the DFT with N = 4. What is the IDFT for n = −2?

Ljubiša Stankovi´c
Digital Signal Processing
111
N=8
0
n
x(n)
N=8
0
n
 ...x(n-N)+x(n)+x(n+N)+..
N=8
0
n
x(n)
N=8
0
n
 ...x(n-N)+x(n)+x(n+N)+..
Figure 3.2
Signals x(n) = 2cos(2πn/8) for 0 ≤n ≤7 (left) and x(n) = 2cos(2πn/16) for
0 ≤n ≤7 (right) along with their periodic extensions IDFT{DFT{x(n)}} with N = 8.
⋆The DFT of this signal is
X(k) =
3
∑
n=0
x(n)e−j2πnk/4 = 1 + 1
2e−j2πk/4 −e−jπk + 1
2ej2πk/4
= 1 + (−1)k+1 + cos(2πk/4).
The IDFT is
x(n) = 1
4
3
∑
k=0
[1 + cos(2πk/4) + (−1)k+1]ej2πnk/4,
for 0 ≤n ≤3. The DFT and IDFT inherently assume the signal and its Fourier
transform periodicity. Thus the result for n = −2 is
x(−2) = 1
4
3
∑
k=0
X(k)ej2π(−2) k
4 = 1
4
3
∑
k=0
X(k)ej2π(4−2) k
4 = x(4 −2) = x(2) = −1.
Example 3.4. Assume that there is a routine to calculate the DFT of x(n) for
0 ≤n ≤N −1 as X(k) = DFT{x(n)} = R{x(n)}. How to use it to calculate the
DFT of a signal x(n) whose values are given within −N/2 ≤n ≤N/2 −1?
⋆A periodic extension of the signal x(n) is assumed when the DFT
is calculated. It means that in the DFT calculation the signal x(n), deﬁned
within −N/2 ≤n ≤N/2 −1, will be extended with the period N. Here, we

112
Discrete Fourier Transform
have a routine to calculate the DFT of a signal using samples within 0 ≤n ≤
N −1. Samples of the periodic extension of x(n) within 0 ≤n ≤N −1 should
be used in the calculation. They are xs(n) = x(n) within 0 ≤n ≤N/2 −1 and
xs(n) = x(n −N) for N/2 ≤n ≤N −1. Then the DFT is obtained as
DFT{x(n)} = DFT{x(ns)} = R{xs(n)}
x(n) = IDFT{DFT{xs(n)}}(u(n + N/2) −u(n −N/2)).
Here, we have used the property that for a signal y(n) periodic with a period
N holds ∑N−1
n=0 y(n) = ∑M+N−1
n=M
y(n) for any M (Generalize the result for the
DFT calculation and inversion for a signal x(n) deﬁned within M ≤n ≤
M + N −1, using the given routine R{x(n)}).
In a matrix form, the DFT can be written as
⎡
⎢⎢⎢⎣
X(0)
X(1)
...
X(N −1)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎢⎣
1
1
···
1
1
e−j 2π
N
···
e−j 2π(N−1)
N
...
...
...
...
1
e−j 2π(N−1)
N
···
e−j 2π(N−1)(N−1)
N
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
x(0)
x(1)
...
x(N −1)
⎤
⎥⎥⎥⎦
(3.6)
or
X = Wx,
(3.7)
where X and x are the vectors containing the signal and its DFT values
X=[X(0) X(1) ... X(N −1)]T
x=[x(0) x(1) ... x(N −1)]T,
respectively, while W is the discrete Fourier transform matrix with coefﬁ-
cients
W =
⎡
⎢⎢⎢⎣
1
1
···
1
1
W1
N
···
WN−1
N
...
...
...
...
1
W(N−1)
N
···
W(N−1)(N−1)
N
⎤
⎥⎥⎥⎦,
(3.8)
where
Wk
N = e−j2πk/N
is used to simplify the notation, especially in graphical illustrations.
The number of additions to calculate a DFT is N −1 for each X(k)
in (3.1). Since there are N DFT coefﬁcients the total number of additions is

Ljubiša Stankovi´c
Digital Signal Processing
113
N(N −1). From the matrix from (3.6) we can see that the multiplications
are not needed for calculation of X(0). There is non need for multiplication
in the ﬁrst term of each coefﬁcient calculation as well. If we neglect the fact
that some other terms in matrix (3.6) may also assume values 1, −1, j, or −j
then the number of multiplications is (N −1)2. The order of the number of
multiplications and the number of additions for the DFT calculation is N2.
The inverse DFT in a matrix form is
x = W−1X,
(3.9)
with W−1 = 1
N W∗, where ∗denotes complex-conjugate operation. The same
calculation complexity analysis holds for the inverse DFT as for the DFT.
3.2
DFT PROPERTIES
Most of the DFT properties can be derived in the same way as in the Fourier
transform and the Fourier transform of discrete-time signals.
1. Consider a signal x(n) shifted in time x(n −n0). If the DFT of signal
x(n) is X(k) = DFT{x(n)} then X(k)e−j2πkn0/N will represent a signal
IDFT{X(k)e−j2πkn0/N} = 1
N
N−1
∑
k=0
X(k)e−j2πkn0/Nej 2π
N kn
= 1
N
N−1
∑
k=0
X(k)ej 2π
N k(n−n0) = x(n −n0).
(3.10)
Here x(n −n0) is the signal obtained when x(n) is periodically ex-
tended with N ﬁrst and then this periodic signal is shifted for n0.
The basic period of the original signal x(n) is now within n0 ≤n ≤
N −n0 −1.
This kind of shift in periodic signals, used in the above relation,
is also referred to as a circular shift. Thus, with the circular shift
DFT{x(n −n0)} = X(k)e−j2πkn0/N.
(3.11)
2. For a modulated signal x(n)ej2πnk0/N we easily get
DFT
M
x(n)ej2πnk0/NN
= X(k −k0).
(3.12)

114
Discrete Fourier Transform
3. The DFT is real-valued if
x∗(n) = x(N −n).
For a real-valued DFT holds
X(k) = X∗(k)
or
N−1
∑
n=0
x(n)e−j2πnk/N =
N−1
∑
n=0
x∗(n)ej2πnk/N =
N−1
∑
n=0
x∗(N −n)ej2π(N−n)k/N,
where x∗(N)ej2πNk/N = x∗(0)ej2π0k/N is used. Since ej2πn(N−n)/N =
e−j2πnk/N we get
N−1
∑
n=0
x(n)e−j2πnk/N =
N−1
∑
n=0
x∗(N −n)e−j2πnk/N.
It means that if X(k) = X∗(k) then x∗(n) = x(N −n) = x(−n).
In the same way for a real-valued signal x(n) the DFT satisﬁes
X∗(k) = X(N −k).
4. Parseval’s theorem for discrete-time periodic signals relates the en-
ergy in the time and the frequency domain
N−1
∑
n=0
|x(n)|2 = 1
N2
N−1
∑
n=0
N−1
∑
k1=0
N−1
∑
k2=0
X(k1)X∗(k2)ej2πn(k1−k2)/N
= 1
N2
N−1
∑
k1=0
N−1
∑
k2=0
X(k1)X∗(k2)Nδ(k1 −k2) = 1
N
N−1
∑
k=0
|X(k)|2 .
5. Convolution of two periodic signals x(n) and h(n), whose period is
N, is deﬁned by
y(n) =
N−1
∑
m=0
x(m)h(n −m).
The DFT of this signal is
Y(k) = DFT{y(n)} =
N−1
∑
n=0
N−1
∑
m=0
x(m)h(n −m)e−j2πnk/N = X(k)H(k).
(3.13)

Ljubiša Stankovi´c
Digital Signal Processing
115
Thus, the DFT of a convolution of two periodic signals is equal to
the product of DFTs of individual signals. Since the convolution is
performed on periodic signals (the DFT inherently assumes signals
periodicity), a circular shift of signals is assumed in the calculation.
This kind of convolution is called circular convolution.
Relation (3.13) indicates that we can calculate convolution of two
aperiodic discrete-time signals of a limited duration in the following
way:
• Calculate DFTs of x(n) and h(n) and obtain X(k) and H(k). At
this point, we inherently make periodic extension of x(n) and
h(n) with period N.
• Multiply these two DFTs to obtain DFT of the output signal
Y(k) = X(k)H(k).
• Calculate the inverse DFT to get the convolution
y(n) = IDFT{Y(k)}.
This procedure looks more complex than the direct calculation
of convolution by deﬁnition. However, due to very efﬁcient and fast
routines for the DFT and the IDFT calculation, this way of calculation
could be more efﬁcient than the direct one.
In using this procedure, we have to take care about the length of
signals and their DFTs that assume periodic extension.
Example 3.5. Consider a discrete-time signal
x(n) = u(n) −u(n −5).
Calculate the convolution x(n) ∗x(n). Extend signals with period N = 7 and
calculate the circular convolution (corresponding to the DFT based convolu-
tion calculation with N = 7, which is longer than the signal duration). Com-
pare the results. What value of N should be used for the period so that the
direct convolution corresponds to one period of the circular convolution?
⋆Signal x(n) and its reversed version x(−n), along with the
shifted signal used in the convolution calculation, are presented in Fig.3.3.
In the circular (DFT) calculation, for example, at n = 0, the con-
volution value is
xp(n) ∗xp(n) =
6
∑
m=0
xp(m)xp(0 −m) = 1 + 1 + 1 = 3.
In addition to the term x(0)x(0) = 1 which exists in the aperiodic convolution,
two terms for m = 3 and m = 4 appeared due to the periodic extension of

116
Discrete Fourier Transform
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
x(n)
-15
-10
-5
0
5
10
15
0
2
4
6
n
x(n)* x(n)
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
xp(m)
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
xp(-m )
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
xp(-m+1 )
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
xp(-m+5 )
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
xp(-m+3 )
-15
-10
-5
0
5
10
15
0
2
4
6
n
xp(n)* xp(n)
Figure 3.3
Illustration of the discrete-time signal convolution and circular convolution for
signals whose length is 5 and the circular convolution is calculated with N = 7.
the signal. They made that the circular convolution value differs from the
convolution of original aperiodic signals. The same situation occurred for
n = 1 and n = 2. For n = 3,4, and 5 the correct result for aperiodic convolution
is obtained using circular convolution. It could be concluded that if the signal
in circular convolution were separated by at least two more zero values (if the
period N were N ≥9) this difference would not occur, Fig.3.4 for N = 9. Then
one period of circular convolution 0 ≤n ≤N −1 would correspond to the
original aperiodic convolution.
If a signal x(n) is of length M, then we can calculate its DFT with
any N ≥M, so that the signal will not overlap with its periods, added

Ljubiša Stankovi´c
Digital Signal Processing
117
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
xp(m)
n
xp(-m )
-15
-10
-5
0
5
10
15
-0.5
0
0.5
1
1.5
n
xp(-m+8 )
-15
-10
-5
0
5
10
15
0
2
4
6
n
xp(n)* xp(n)
Figure 3.4
Illustration of the discrete-time signal circular convolution for signals whose
length is 5 and the circular convolution is calculated with N = 9.
using the DFT. If a signal h(n) is of length L, then we can calculate
its DFT with any N ≥L. However, if we want to use their DFTs for
a convolution calculation (to use circular convolution), then from the
previous example we see that the length of convolution y(n) is M +
L −1. Therefore, for the DFT-based calculation of y(n), we have to use
at least N ≥M + L −1. It means that both DFTs, X(k) and H(k), whose
product results in Y(k), must be at least of N ≥M + L −1 duration.
Otherwise, aliasing (overlapping of the periods) will appear. Then
the circular convolution calculated in this way would not correspond
(within the basic period) to the convolution of the original discrete-
time (aperiodic) signals.
Duration of the input signal x(n) may be much longer that the dura-
tion of the impulse response h(n). For example, an input signal may have
tens of thousands of samples, while the impulse response of a discrete sys-
tem duration is, for example, tens of samples, M ≫L. A direct convolution
would be calculated (after ﬁrst L −1 output samples) as
y(n) =
n
∑
m=n−L+1
x(m)h(n −m).
For each output sample, L multiplications would be used. For a direct DFT
application in the convolution calculation we should wait until the end of
the signal and then zero-pad both the input signal and the impulse response

118
Discrete Fourier Transform
up to M + L −1. This kind of calculation is not efﬁcient. Instead of using
a direct DFT calculation, the signal is split into nonoverlapping sequences
whose duration N is of the order of impulse response duration L,
x(n) =
K−1
∑
k=0
xk(n),
where xk(n) = x(n)[u(n −kN) −u(n −(k + 1)N] and M = KN (the input
signal can always be zero-padded up to the nearest KN duration, where K
is an integer). The output signal is
y(n) =
K−1
∑
k=0
(
n
∑
m=n−L+1
xk(m)h(n −m)
)
=
K−1
∑
k=0
yk(n).
(3.14)
For the convolutions yk(n) = xk(n) ∗n h(n) calculation the signals xk(n)
and h(n) should be of duration N + L −1 only. These convolutions can be
calculated after each N ≪M input signal samples. The output sequence
yk(n) duration is N + L −1. Since yk(n), k = 0,1,...,K −1, are calculated
with step N in time, they overlap, although the input signals xk(n) are
nonoverlapping. For two successive yk(n) and yk+1(n) and L ≤N, L −1
samples within kN + N ≤n < kN + N + L −1 overlap. This should be taken
into account, by summing the overlapped output samples in y(n), after the
individual convolutions yk(n) = xk(n) ∗n h(n) are calculated using the DFTs,
Fig.3.5.
3.3
ZERO-PADDING AND INTERPOLATION
The basic period of the DFT X(k), calculated for k = 0,1,2,..., N −1, should
be considered as having two parts: one part for 0 ≤k ≤N/2 −1, that
corresponds to the positive frequencies
ω = 2π
N k or Ω= 2π
N∆tk, for 0 ≤k ≤N/2 −1,
(3.15)
and the other part being a shifted version of the negative frequencies (in the
original aperiodic signal)
ω = 2π
N (k −N) or Ω= 2π
N∆t(k −N), for N/2 ≤k ≤N −1.
(3.16)
Illustration of the frequency correspondence to the frequency index in the
DFT is given in Fig.3.6

Ljubiša Stankovi´c
Digital Signal Processing
119
n
0
x(n)
n
0
h(n)
n
0
x1(n)
n
0
x2(n)
n
0
x3(n)
n
0
y1(n)
n
0
y2(n)
n
0
y3(n)
n
0
y(n)
Figure 3.5
Illustration of the convolution calculation when the input signal duration is much
longer then the duration of the system impulse response.

120
Discrete Fourier Transform
N/2-1
-N/2
0
k
Ω=2πk/(NΔt)
X(Ω)|Ω=2πk/(NΔt)
N
0
X(k)
k
Figure 3.6
Relation between the frequency in continuous-time and the DFT frequency index.
We have seen that the DFT of a signal whose duration is limited to
M samples can be calculated by using any N ≥M. In practice, this means
that we can add (use) as many zeros, after the nonzero signal x(n) values,
as we like. By doing this, we increase the calculation complexity, but we
also increase the number of samples within the same frequency range of the
Fourier transform.
If we recall that
X(k) = X(ejω)|ω=k∆ω=2πk/N = X(Ω)|Ω=k∆Ω=2πk/(N∆t) ,
(3.17)
holds in the case when the sampling theorem is satisﬁed, then we see that by
increasing N in the DFT calculation, the density of sampling (interpolation)
in the Fourier transform of the original signal increases. The DFT interpo-
lation by zero padding the signal in the time domain is illustrated in Fig.
3.7.
The same holds for the frequency domain. If we calculate DFT with N
samples and then add, for example, N zeros after the region corresponding
to the highest frequencies, then by the IDFT of this 2N point DFT, we will
interpolate the original signal in time. All zero values in the frequency
domain should be inserted between two parts (regions) of the original DFT
corresponding to positive and negative frequencies.

Ljubiša Stankovi´c
Digital Signal Processing
121
n
x(n)
k
X(k)
n
x(n)
k
X(k)
n
x(n)
k
X(k)
Figure 3.7
Discrete-time signal and its DFT (top two subplots). Discrete-time signal zero-
padded and its DFT interpolated (two subplots in the middle). Zero-padding (interpolation)
factor was 2. Discrete-time signal zero-padded and its DFT interpolated (two bottom subplots).
Zero-padding (interpolation) factor was 4. According to the duality property, the same holds if
X(k) were signal in the discrete-time and x(−n) was its Fourier transform.

122
Discrete Fourier Transform
Example 3.6. The Hann(ing) window for a signal within −N/2 ≤n ≤N/2 −1, is
w(n) = 1
2[1 + cos(2πn
N )],
for −N/2 ≤n ≤N/2 −1.
(3.18)
If the original signal values are within 0 ≤n ≤N −1 then the Hann(ing)
window form is
w(n) = 1
2[1 −cos(2πn
N )],
for 0 ≤n ≤N −1.
(3.19)
Present the zero-padded forms of Hann(ing) windows with 2N samples.
⋆The zero-padded form of the Hann(ing) windows used for window-
ing data within the intervals −N/2 ≤n ≤N/2 −1 and 0 ≤n ≤N −1 are
shown in Fig.3.8. The DFTs of windows (3.18) and (3.19) are W(k) = N[δ(k) +
δ(k −1)/2 + δ(k + 1)/2]/2 and W(k) = N[δ(k) −δ(k −1)/2 −δ(k + 1)/2]/2,
respectively. After the presented zero-padding the window DFT realness
property wpz(n) = wpz(n −2N) is preserved (for an even N in the case
−N/2 ≤n ≤N/2 −1 and for an odd N for data within 0 ≤n ≤N −1).
3.4
RELATION AMONG THE FOURIER REPRESENTATIONS
Presentation of the DFT will be concluded with an illustration (Fig. 3.9) of
the relation among four forms of the Fourier domain signal representations
for the cases of:
1. Continuous-time aperiodic signal (Fourier transform):
x(t) =
1
2π
∞
"
−∞
X(Ω)ejΩtdΩ,
X(Ω) =
∞
"
−∞
x(t)e−jΩtdt.
2. Continuous-time periodic signal (Fourier series):
xp(t) =
∞
∑
m=−∞
x(t + mT)
xp(t) =
∞
∑
n=−∞
Xnej2πnt/T,
Xn = 1
T
T/2
"
−T/2
x(t)e−j2πnt/Tdt,
Xn = 1
T X(Ω)|Ω=2πn/T .

Ljubiša Stankovi´c
Digital Signal Processing
123
n
N/2-1
-N/2
0
w(n)
n
N
0
wp(n)
n
N
2N
0
wp(n)
n
N
0
w(n)
n
N
0
wp(n)
n
2N
0
wp(n)
Figure 3.8
Zero-padding of the Hann(ing) windows used to window data within −N/2 ≤
n ≤N/2 −1 and 0 ≤n ≤N −1.

124
Discrete Fourier Transform
If the periodic signal is formed by a periodic extension of an aperiodic
signal x(t) then there is no signal overlapping (aliasing) in the periodic
signal if the original aperiodic signal duration is shorter than the
extension period T.
3. Discrete-time aperiodic signal (Fourier transform of discrete-time
signals)
x(n) = x(n∆t)∆t,
x(n) =
1
2π
∞
"
−∞
X(ejω)ejωtdω,
X(ejω) =
∞
∑
n=−∞
x(n)e−jωn,
X(ejω) =
∞
∑
m=−∞
X(Ω+ m2π
∆t )|Ω=ω/∆t .
The Fourier transform of the discrete-time signal is a periodic exten-
sion X(ejω), ω = Ω∆t, of the Fourier transform X(Ω) of a continuous-
time signal. There is no overlapping (aliasing) if the width of the
Fourier transform of the original continuous-time signal is shorter
than the extension period 2π/∆t.
4. Discrete-time periodic signal (discrete Fourier transform)
xp(n) =
∞
∑
m=−∞
x(n + mN) = xp(t)|t=n∆t ,
xp(n) = 1
N
N−1
∑
k=0
X(k)ej2πnk/N,
X(k) =
N−1
∑
n=0
x(n)e−j2πnk/N,
X(k) = X(ejω)|ω=2πk/N = X(Ω)|Ω=2πk/(N∆t) = TXk.
In the periodic discrete-time signal xp(n) it has been assumed that
there is no overlapping of the original aperiodic discrete-time signal
x(n) samples, i.e, that its duration is shorter than the period N, x(n) =
xp(n) for 0 ≤n ≤N −1.
3.5
FAST FOURIER TRANSFORM
Algorithms that provide efﬁcient calculation of the DFT, with a reduced
number of arithmetic operations, are called the fast Fourier transform (FFT).
A uniﬁed approach to the DFT and the inverse DFT, (3.5), is used. The only

Ljubiša Stankovi´c
Digital Signal Processing
125
t
x(t)
x(t)
Ω
X(Ω)
X(Ω)
n
x(n)
x(n) = x(t) Δt t = nΔt
ω
X(e jω )
X(e jω ) = X(Ω) Ω = ω/Δt
- π ≤ ω < π
- π
π
t
xp(t)
- T/2
T/2
xp(t) = x(t)
- T/2 ≤ t < T/2
n
Xn
Xn = X(Ω)/T Ω = 2πn/T
n
xp(n)
xp(n) = x(n)
- N/2 ≤ n < N/2
- N/2
N/2
k
X(k)
- N/2 ≤ k < N/2
X(k) = X(e j2πk/T ) = TXk
- N/2
N/2
Figure 3.9
Aperiodic continuous-time signal and its Fourier transform (ﬁrst row). Discrete-
time signal and its Fourier transform (second row). Periodic continuous-time signal and its
Fourier series coefﬁcients (third row). Periodic discrete-time signal and its discrete Fourier
transform (DFT), (fourth row).

126
Discrete Fourier Transform
differences between the DFT and inverse DFT calculation are in the sign of
the exponent and the division of the ﬁnal result by N.
Here we will present an algorithm based on splitting the signal x(n),
with N samples, into two signals x(n) for 0 ≤n ≤N/2 −1 and x(n) for
N/2 ≤n ≤N −1, whose duration is N/2. It is assumed that N is an even
number. By deﬁnition, a DFT of a signal with N samples is
DFTN{x(n))} = X(k) =
N−1
∑
n=0
x(n)e−j2πnk/N
=
N/2−1
∑
n=0
x(n)e−j2πnk/N +
N−1
∑
n=N/2
x(n)e−j2πnk/N
=
N/2−1
∑
n=0
@
x(n) + x(n + N/2)(−1)kA
e−j2πnk/N
since e−j2π(n+N/2)k/N = e−j2πnk/Ne−jπk = (−1)ke−j2πnk/N.
For an even number k = 2r we have
DFTN/2{g(n)} = X(2r) =
N/2−1
∑
n=0
g(n)e−j2πnr/(N/2)
with
g(n) = x(n) + x(n + N/2).
For an odd number k = 2r + 1, follows
DFTN/2{h(n)} = X(2r + 1) =
N/2−1
∑
n=0
h(n)e−j2πnr/(N/2)
where
h(n) = (x(n) −x(n + N/2))e−j2πn/N.
In this way, we split one DFT of N elements into two DFTs of N/2
elements. Having in mind that the direct calculation of a DFT with N
elements requires an order of N2 operations, it means that we will reduce
the calculation complexity, since N2 > (N/2)2 + (N/2)2. An illustration of
this calculation, with N = 8, is shown in Fig. 3.10. We can continue and
split N/2 DFTs into N/4 DFTs, and so on. A complete calculation scheme is
shown in Fig. 3.11. We can conclude that in the FFT algorithms an order of
N log2 N of operations is required. Here it is assumed that log2 N = p is an
integer, i.e., N = 2p. This a decimation-in-frequency algorithm.

Ljubiša Stankovi´c
Digital Signal Processing
127
 x(7) 
  X(7)
 x(6) 
  X(5)
 x(5) 
  X(3)
 x(4) 
  X(1)
 x(3) 
  X(6)
 x(2) 
  X(4)
 x(1) 
  X(2)
 x(0) 
  X(0)
 W 3
8  
 W 2
8  
 W 1
8  
 W 0
8  
-1
-1
-1
-1
DFT
4
DFT
4
Figure 3.10
DFT of length 8 calculation using two DFTs of length 4.
 x(7) 
  X(7)
 x(6) 
  X(3)
 x(5) 
  X(5)
 x(4) 
  X(1)
 x(3) 
  X(6)
 x(2) 
  X(2)
 x(1) 
  X(4)
 x(0) 
  X(0)
 W 3
8  
-1
 W 2
8  
-1
 W 1
8  
-1
 W 0
8  
-1
 W 2
8  
-1
 W 0
8  
-1
 W 2
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
Figure 3.11
FFT calculation scheme obtained by decimation-in-frequency for N = 8.

128
Discrete Fourier Transform
If we want to be precise the number of additions is exactly
Nadditions = N log2 N.
For the number of multiplications we can see that in the ﬁrst stage there are
(N/2 −1) multiplications. In the second stage there are 2(N/4 −1) mul-
tiplications. In the next stage would be 4(N/8 −1) multiplications. Finally
in the last stage would be 2p−1 B
N
2p −1
C
= N
2 ( N
N −1) = 0 multiplications
(N = 2p or p = log2 N). The total number of multiplications, in this algo-
rithm, is
Nmultiplicat. =
* N
2 −1
+
+ 2
* N
4 −1
+
+ 4
* N
8 −1
+
+ ... + 2p−1
* N
2p −1
+
= N
2 −1 + N
2 −2 + N
2 −4 + ... + N
2 −N
2
= N
2 p −(1 + 2 + 22 + ... + 2p−1) = N
2 p −1 −2p
1 −2
= N
2 log2 N −(N −1) = N
2 [log2 N −2] + 1.
If the multiplications by j and −j were excluded the number of multiplica-
tions would be additionally reduced.
Example 3.7. Consider a signal x(n) within 0 ≤n ≤N −1. Assume that N is an
even number. Show that the DFT of x(n) can be calculated as two DFTs, one
using the even samples of x(n) and the other using odd samples of x(n).
⋆By deﬁnition
X(k) =
N−1
∑
n=0
x(n)e−j2πkn/N
=
N/2−1
∑
m=0
x(2m)e−j2πk2m/N +
N/2−1
∑
m=0
x(2m + 1)e−j2πk(2m+1)/N
=
N/2−1
∑
m=0
xe(m)e−j2πkm/(N/2) + e−j2πk/N
N/2−1
∑
m=0
xo(m)e−j2πkm/(N/2),
(3.20)
where xe(m) = x(2m) and xo(m) = x(2m + 1) are even and odd samples of the
signal, respectively. Thus, a DFT of N elements is split into two DFTs of N/2
elements. Two DFTs of N/2 elements require an order of 2(N/2)2 = N2/2
operations. It is less than N2. In this way, if N/2 is an even number, we can
continue and split two DFTs of N/2 elements into four DFTs of N/4 elements,
and so on. This is a decimation-in-time algorithm, Fig.3.12.

Ljubiša Stankovi´c
Digital Signal Processing
129
  X(7) 
  x(7) 
  X(6) 
  x(3) 
  X(5) 
  x(5) 
  X(4) 
  x(1) 
  X(3) 
  x(6) 
  X(2) 
  x(2) 
  X(1) 
  x(4) 
  X(0) 
  x(0) 
 W 3
8  
-1
 W 2
8  
-1
 W 1
8  
-1
 W 0
8  
-1
 W 2
8  
-1
 W 0
8  
-1
 W 2
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
Figure 3.12
Decimation-in-time FFT algorithm for N = 8.
Example 3.8. Consider a signal x(n) within 0 ≤n ≤N −1. Assume that N = 3M.
Show that the DFT of x(n) can be calculated using three DFTs of M samples.
⋆The DFT of x(n) is
X(k) =
3M−1
∑
n=0
x(n)e−j2πkn/(3M)
=
M−1
∑
m=0
x(m)e−j2πkm/(3M) +
2M−1
∑
m=M
x(m)e−j2πkm/(3M) +
3M−1
∑
m=2M
x(m)e−j2πkm/(3M)
=
M−1
∑
m=0
@
x(m) + x(m + M)e−j 2πkM
3M + x(m + 2M)e−j 2πk2M
3M
A
e−j 2πmk
3M .
Now we can consider three cases for frequency index k
X(3k) =
M−1
∑
m=0
g(n)e−j2πmk/M
with g(n) = x(m) + x(m + M) + x(m + 2M)
X(3k + 1) =
M−1
∑
m=0
r(n)e−j2πmk/M

130
Discrete Fourier Transform
with r(n) =
D
x(m) + ax(m + M) + a2x(m + 3M)
E
e−j2πm/(3M),
X(3k + 2) =
M−1
∑
m=0
p(n)e−j2πmk/M
with p(n) =
D
x(m) + a2x(m + M) + ax(m + 3M)
E
e−j2π2m/(3M), where a =
e−j2π/3. Thus, a DFT of N = 3M elements is split into three DFTs of N/3 = M
elements. Three DFTs of N/3 elements require an order of 3(N/3)2 = N2/3
operations. If, for example, M = N/3 is an even number, we can continue
and split three DFTs of N/3 elements into six DFTs of N/6 elements, and so
on.
3.6
SAMPLING OF PERIODIC SIGNALS
A periodic signal x(t), with a period T, can be reconstructed if its Fourier se-
ries is with limited number of nonzero coefﬁcients so that Xk = 0 for k > km
corresponding to frequencies greater than Ωm = 2πkm/T. The periodic sig-
nal can be reconstructed from the samples taken at ∆t < π/Ωm = 1/(2fm).
The number of samples within the period is N = T/∆t.
The reconstructed signal is
x(t) =
N−1
∑
n=0
x(n∆t)
sin[(n −t
∆t)π]
N sin[(n −t
∆t)π/N]
for and odd N and
x(t) =
N−1
∑
n=0
x(n∆t)ej(n−t/∆t)π/N
sin[(n −t
∆t)π]
N sin[(n −t
∆t)π/N]
for an even N.
Example 3.9. Samples of a signal x(t) are taken with step ∆t = 1. Obtained discrete-
time values are x(n) = [0, 2.8284, −2, 2.8284, 0, −2.8284, 2, −2.8284] for 0 ≤
n ≤N −1 with N = 8. Assuming that the signal satisﬁes the sampling
theorem ﬁnd its value at t = 1.5. Check the accuracy if the original signal
values were known, x(t) = 3sin(3πt/4) + sin(πt/4).
⋆Using the reconstruction formula for an even N we get
x(1.5) =
7
∑
n=0
x(n)ej(n−1.5)π/8
sin[(n −1.5)π]
8sin[(n −1.5)π/8] = −0.2242.

Ljubiša Stankovi´c
Digital Signal Processing
131
0
2
4
6
8
-4
-2
0
2
4
x(t),   x(n)  with  Δt=1
time
Figure 3.13
Periodic signal reconstructed from its samples at ∆t = 1.
This result is equal to the original signal value. Calculation is repeated with
0 ≤t ≤8, with step 0.01. The reconstructed values of x(t) are presented in
Fig.3.13.
In order to prove the sampling theorem of periodic signals write the
signal in a form of the Fourier series
x(t) =
km
∑
k=−km
Xkej2πkt/T.
(3.21)
Using N samples of x(t) within the signal period (assuming that N is an
odd number), i.e., by sampling the signal at ∆t = T/N, we get
x(n∆t) =
km
∑
k=−km
Xkej2πkn/N.
With (N −1)/2 ≥km we can write
x(n∆t)∆t = T
N
km
∑
k=−km
Xkej2πkn/N = T
N
(N−1)/2
∑
k=−(N−1)/2
Xkej2πkn/N.

132
Discrete Fourier Transform
With x(n∆t)∆t = x(n) and TXk = X(k) this form reduces to the DFT and the
inverse DFT
x(n) = 1
N
(N−1)/2
∑
k=−(N−1)/2
X(k)ej2πkn/N,
X(k) =
N−1
∑
n=0
x(n)−j2πkn/N.
Substituting the Fourier series coefﬁcients Xk, expressed in terms of X(k)
and x(n), into signal (3.21), with km = (N −1)/2, we get
x(t) = 1
T
N−1
2
∑
k=−N−1
2
N−1
∑
n=0
x(n)e−j2πk n
N ej2πk t
T = 1
N
N−1
∑
n=0
N−1
2
∑
k=−N−1
2
x(n∆t)ej2πk( t
T −n
N )
= 1
N
N−1
∑
n=0
x(n∆t)e−j2π( t
T −n
N )(N−1)/2 1 −ej2π( t
T −n
N )N
1 −ej2π( t
T −n
N )
=
N−1
∑
n=0
x(n∆t)
sin[ π
∆t(t −n∆t)]
N sin[ π
N∆t(t −n∆t)].
This is the reconstruction formula that can be used to calculate x(t) for any
t based on the signal values at x(n∆t) with ∆t < π/Ωm = 1/(2fm).
In a similar way the reconstruction formula for an even number of
samples N can be obtained.
The sampling theorem reconstruction formula of aperiodic signals
follows as a special case as N →∞, since for a small argument
sin[ π
N∆t(t −n∆t)] →
π
N∆t(t −n∆t)
and
x(t) →
∞
∑
n=−∞
x(n∆t)sin[ π
∆t(t −n∆t)]
π
∆t(t −n∆t)
.
Example 3.10. For a signal x(t) whose period is T it is known that the signal has
components corresponding to the nonzero Fourier series coefﬁcients at k1, k2,
..., kK. What is the minimal number of signal samples needed to reconstruct
the signal? What condition the sampling instants and the frequencies should
satisfy for the reconstruction?
⋆The signal x(t) can be reconstructed by using the Fourier series
(1.11). In calculations, a ﬁnite number of K nonzero terms will be used,
x(t) =
K
∑
m=1
Xkmej2πkmt/T.

Ljubiša Stankovi´c
Digital Signal Processing
133
Since there are K unknown values Xk1, Xk2,...,XkK the minimal number of
equations to calculate their values is K. The equations are written for K time
instants
K
∑
m=1
Xkmej2πkmti/T = x(ti), for i = 1,2,...,K
or
Xk1ej2πk1t1/T + Xk2ej2πk2t1/T + ... + XkKej2πkKt1/T = x(t1)
Xk1ej2πk1t2/T + Xk2ej2πk2t2/T + ... + XkKej2πkKt2/T = x(t2)
...
Xk1ej2πk1tK/T + Xk2ej2πk2tK/T + ... + XkKej2πkKtK/T = x(tK).
In a matrix from
ΦX= y,
X = Φ−1y
where
X = [Xk1 Xk2 ... XkK]T,
y = [x(t1) x(t2) ... x(tK)]T
Φ =
⎡
⎢⎢⎣
ej2πk1t1/T
ej2πk2t1/T
...
ej2πkKt1/T
ej2πk1t2/T
ej2πk2t2/T
...
ej2πkKt2/T
...
...
...
...
ej2πk1tK/T
ej2πk2tK/T
...
ej2πkKtK/T
⎤
⎥⎥⎦
The reconstruction condition is det∥Φ∦= 0 for selected time instants ti and
given frequency indices ki.
3.7
ANALYSIS OF A SINUSOID BY USING THE DFT
Analysis and estimation of frequency and amplitude of pure sinusoidal
signals is of great importance in many applications.
Consider a simple continuous-time sinusoidal signal
x(t) = AejΩ0t
(3.22)
whose Fourier transform is X(Ω) = 2πAδ(Ω−Ω0). The whole signal en-
ergy is concentrated just in one frequency point at Ω= Ω0. Obviously, the
position of maximum is equal to the signal frequency. For this operation we
will use the notation
Ω0 = arg
!
max
−∞<Ω<∞|X(Ω)|
6
.
(3.23)

134
Discrete Fourier Transform
Assume that the signal x(t) is sampled with ∆t. The discrete-time form
of this signal is
x(n) = Aejω0n∆t,
with ω0 = Ω0∆t.
In order to compute the DFT of this signal, we will assume a value of
N and calculate
X(k) =
N−1
∑
n=0
Aejω0ne−j2πnk/N∆t.
In general, the DFT is of the form
X(k) = A
N−1
∑
n=0
ejω0ne−j2πnk/N∆t = A 1 −ejω0Ne−j2πk
1 −ejω0e−j2πk/N ∆t
(3.24)
= Aej((N−1)(ω0−2πk/N)/2) sin(N(ω0 −2πk/N)/2)
sin((ω0 −2πk/N)/2) ∆t
(3.25)
|X(k)| = |A|
''''
sin(N(ω0 −2πk/N)/2)
sin((ω0 −2πk/N)/2)
''''∆t.
(3.26)
3.7.1
Leakage Effect
Two cases may appear in the analysis of a sinusoidal signal:
1. The signal frequency is such that
ω0 = 2πk0/N
or Ω0 = 2πk0/(N∆t), where 0 ≤k0 ≤N −1 is an integer. The discretization
step ∆t is such that it is contained an integer number of times within the
signal period T0 = 2π/Ω0. Then
X(k) = A
N−1
∑
n=0
ej2πk0n/Ne−j2πnk/N∆t = NAδ(k −k0)∆t.
(3.27)
Obviously we can ﬁnd the signal frequency index from
k0 = arg{
max
0≤k≤N−1|X(k)|}.
(3.28)
Frequency is calculated as Ω0 = 2πk0/(N∆t) for 0 ≤k0 ≤N/2 −1 and
Ω0 = 2π(k0 −N)/(N∆t) for N/2 ≤k0 ≤N −1. This case is illustrated in
Fig. 3.14, top row. Noisy signals will be considered later in the book.

Ljubiša Stankovi´c
Digital Signal Processing
135
n
x(n)
k
X(k)
n
x(n)
k
X(k)
Figure 3.14
Sinusoid x(n) = cos(8πn/64) and its DFT with N = 64 (top row) and sinusoid
x(n) = cos(8.8πn/64) and its DFT absolute value, with N = 64 (bottom row).
Estimation of amplitude is easy in this case as
A =
1
N∆t X(k0).
2. In reality, we never know the signal period (or Ω0) in advance (if we knew
it, then this analysis would not be needed). So, it is highly unlikely to have a
case with frequency on the grid, when Ω0 = 2πk0/(N∆t) as in Fig. 3.14, top
row. More common is the case illustrated in Fig. 3.14, bottom row, when the
true signal frequency does not correspond to any DFT sample position. A
simple sinusoidal signal produces then DFT components at all frequencies.
This effect is known as leakage effect.
Estimation of frequency, based on
ˆk0 = arg
!
max
0≤k≤N−1
''''
sin(N(ω0 −2πk/N)/2)
sin((ω0 −2πk/N)/2)
''''
6
,
will produce an estimation error
e = Ω0 −2π
N∆t
ˆk0.
The estimation error could be up to a half of the discretization period in
frequency, ∆Ω= 2π/(N∆t),
−π
N∆t ≤e <
π
N∆t
and
2π
N∆t
ˆk0 −
π
N∆t ≤Ω0 < 2π
N∆t
ˆk0 +
π
N∆t.
(3.29)

136
Discrete Fourier Transform
Two ways to improve the estimation will be described here.
1. The simplest way to reduce the estimation error is to increase the number
of samples and to reduce the discretization interval in frequency ∆Ω=
2π/(N∆t). This could be achieved by appropriate zero-padding in the time
domain, before the DFT calculation (corresponding to the interpolation in
the frequency domain). This way increases the calculation complexity.
2. The other way is based on a window function application in the DFT
calculation
X(k) =
N−1
∑
n=0
w(n)Aejω0ne−j2πnk/N∆t = W
B
ej( 2πk
N −ω0)C
∆t,
where W(ejω) is the Fourier transform of the window function. Windows,
like for example Hann(ing) or Hamming window, smooth the transition and
reduce discontinuities at the ending calculation points that cause leakage. A
simple realization with, for example, the Hann(ing) window (relation (2.31)
and Fig.2.7)
w(n) = 1
2 [1 −cos(2nπ/N)][u(n) −u(n −N −1)].
adjusted to the time interval 0 ≤n ≤N −1, produces
XH(k) =
N−1
∑
n=0
1
2 [1 −cos(2nπ/N)] Aejω0ne−j2πnk/N∆t
= A
2
N−1
∑
n=0
-
1 −1
2ej2nπ/N −1
2e−j2nπ/N
.
Aejω0ne−j2πnk/N∆t
= 1
2
-
XR(k) −1
2XR(k −1) −1
2XR(k + 1)
.
,
where XR(k) would be the DFT if the rectangular window were used. It is
deﬁned by (3.24). The DFT of sinusoids on the grid and outside of the grid,
multiplied by a Hann(ing) window, are shown in Fig.3.15.
The leakage effect is reduced. However the DFT is spread over two
additional consecutive samples even in the case when the frequency is on
the DFT grid, Fig.3.15(top). In this case the amplitude is estimated as
A =
1
N∆t[X(k0) + X(k0 + 1) + X(k0 −1)].

Ljubiša Stankovi´c
Digital Signal Processing
137
n
x(n)w(n)
k
XH(k)
n
x(n)w(n)
k
XH(k)
Figure 3.15
Sinusoid x(n) = cos(8πn/64) multiplied by a Hann(ing) window and its DFT
with N = 64 (top row) and sinusoid x(n) = cos(8.8πn/64) multiplied by a Hann(ing) window
and its DFT absolute value, with N = 64 (bottom row).
3.7.2
Displacement
A relation of the maximum DFT value with the few surrounding values of
the windowed DFT is used to calculate correction, the displacement bin of
the estimated frequency. If we apply a window function w(n) in the DFT
calculation, we get
X(k) = W
B
ej( 2πk
N −ω0)C
∆t.
For a given window function it is possible to derive the exact displacement
formula.
Instead of deriving an exact formula for each window form, here we
will present an approach that combines the interpolation and a general
ﬁtting polynomial form. It can be used with any window.
We can always interpolate the DFT, so that there are several DFT
samples within the main lobe. Then for any symmetric window we can
approximate the Fourier transform around the maximum by a quadratic
function (in analog domain X(Ω) = aΩ2 + bΩ+ c). Let us denote the largest
sample, following from
ˆk0 = arg
!
max
0≤k≤N−1|X(k)|
6
,

138
Discrete Fourier Transform
by
X0 = |X(ˆk0)|
and two neighboring samples by
X−1 = |X(ˆk0 −1)|
X1 = |X(ˆk0 + 1)|.
By using the Lagrange polynomial interpolation of the second-order, at a
point x = d, taking the bin index as the independent variable k−1 = −1,
k0 = 0, k1 = 1, with the function values at these points being denoted by
X−1, X0 and X1, we have the Lagrange second-order polynomial
X(ˆk0 + d) = X−1
(d −0)(d −1)
(−1 −0)(−1 −1) + X0
(d + 1)(d −1)
(0 + 1)(0 −1) + X1
(d −0)(d + 1)
(1 −0)(1 + 1)
= d2[−X0 + X−1/2 + X1/2)] + d[X1 −X−1]/2 + X0.
(3.30)
This function reaches maximum at
∂X(ˆk0 + d)/∂d = 0,
resulting in the displacement bin for the frequency correction
d = 0.5
|X(ˆk0 + 1)| −|X(ˆk0 −1)|
2|X(ˆk0)| −|X(ˆk0 + 1)| −|X(ˆk0 −1)|
,
(3.31)
with frequency as in (3.32). The displacement procedure is illustrated in Fig.
3.16.
Thus, the best frequency estimation is
Ω0 = 2π
N∆t(ˆk0 + d)
(3.32)
for 0 ≤ˆk0 ≤N/2 −1 and Ω0 =
2π
N∆t((ˆk0 + d) −N) for N/2 ≤ˆk0 ≤N −1.
Example 3.11. A sinusoidal signal x(t) = Aexp(jΩ0t) is sampled with a sampling
interval ∆t = 1/128 and N0 = 64 samples are considered. Prior to the DFT
calculation, the signal is zero padded four times. The DFT maximum is
detected at ˆk0 = 95. The maximum DFT value is X(95) = 0.9. Neighboring
values are X(96) = 0.7 and X(94) = 0.3. Calculate the displacement bin d and
estimate the value of Ω0.

Ljubiša Stankovi´c
Digital Signal Processing
139
Ω, k
X(Ω), X(k)
X(0)
X(-1)
X(1)
d
Ω, k
X(Ω), X(k)
Figure 3.16
Illustration of the displacement bin correction for a true maximum position
calculation based on three neighboring values (full range – left and zoomed graph – right)
.
⋆The displacement bin value is
d = 0.5
0.7 −0.3
1.8 −0.7 −0.3 = 0.25.
The total number of samples in the DFT calculation was N = 4N0 = 256,
meaning that the value ˆk0 = 95 is within the ﬁrst half of the samples (cor-
responding to positive frequency Ω0). Therefore, we can use (3.32) for the
frequency calculation
Ω0 = 2π
N∆t (ˆk0 + d) = 95.25π.
It is possible to derive the exact displacement formula for some win-
dows, based on their Fourier transform function. For example, for the
Hann(ing) window the exact displacement formula is
dH =
1.5[|X(ˆk0 + 1)| −|X(ˆk0 −1)|]
|X(ˆk0 −1)|
B
1 + |X(ˆk0+1)|
|X(ˆk0)|
C
+ |X(ˆk0)| + |X(ˆk0 + 1)|
.
(3.33)
After the displacement is calculated the signal can also be modulated
for the displacement frequency shift in order to produce a signal with the
frequency on the frequency grid. This is especially important if we expect
that the signal contains much smaller higher order harmonics that were
masked with strong values of the dominant harmonic. If we detected that
the k0th harmonic is dominant and displaced for d then this harmonic
should be removed from a signal modulated by the displacement frequency.
The DFT of the new signal is used for the analysis of the second largest
harmonic and so on.

140
Discrete Fourier Transform
3.8
DISCRETE COSINE AND SINE TRANSFORMS
The DFT of signal satisﬁes many desirable properties. Also its calculation is
simple and efﬁcient using the FFT algorithm. With the DFT calculation the
signal periodic extension is assumed and embedded in the discrete trans-
form. However, in the DFT case the periodic signal extension will, in gen-
eral, introduce signiﬁcant signal change (corresponding to discontinuities in
continuous time) at the period ending points Fig.3.17 (ﬁrst and second row).
This change will signiﬁcantly worsen the DFT coefﬁcients convergence and
increases number of coefﬁcients in signal reconstruction. In order to reduce
this effect and to improve convergence of the signal transform coefﬁcients
the signal could be extended in an appropriate way.
The discrete cosine transforms (DCT) and discrete sine transforms
(DST) are used to analyze real-valued discrete signals, periodically extended
to produce even or odd signal forms, respectively. However, this extension
is not straightforward for discrete-time signals. Consider a discrete-time
signal of duration N, when x(n) assumes nonzero values for 0 ≤n ≤N −1.
If we try with a direct extension (using all signal values) and form a periodic
signal y(n), whose basic period is of duration 2N, as
y(n) =
!
x(n)
for
0 ≤n ≤N −1
x(2N −n −1)
for
N ≤n ≤2N −1
the obtained signal is not even, Fig.3.17(third row). It is obvious that y(n)
does not satisfy the condition y(n) = y(−n) = y(2N −n), required for a
real-valued DFT.
The same holds for an odd extension, Fig.3.17(fourth row),
y(n) =
!
x(n)
for
0 ≤n ≤N −1
−x(2N −n −1)
for
N ≤n ≤2N −1 .
Thus we have not achieved one of our goals to have a real-valued transform
after a real-valued signal periodic extension. However from Fig.3.17(third
and fourth row) we can see that the signals y(n) are even (or odd) with
respect to the vertical line at n = −1/2. Thus, if we add zeros between each
sample of y(n) and assume that the position which was at n = −1/2 in the
initial signal is the new coordinate origin n = 0 in the new signal z(n), then
these signals will be even and odd, respectively, Fig.3.17(last two rows).
This is just one of possible extensions to make the original discrete-
time signal even (or odd). Several forms of the DCT and DST are deﬁned
based on other ways of getting an even (odd) signal extension.
The most commonly used is the so called DCT-II or just DCT. It will
be presented here. Signal extension for this transform corresponds to the

Ljubiša Stankovi´c
Digital Signal Processing
141
n
x(n)
n
x(n)
x(n-N )
x(n+N)
n
x(n)
y(n)
x(2N-n-1 )
x(-n-1 )
x(n)
y(n)
- x(2N-n-1 )
- x(-n-1 )
n
n
z(n)
n
z(n)
Figure 3.17
Illustration of a signal x(n), its periodic extension corresponding to the DFT, an
even and odd discrete-time signal extension corresponding to the DCT and DST of type II.
already described one, Fig.3.17. If no form of the DCT is referred to in its
name, then it is assumed that DCT-II form is used. The DCT deﬁnition is
C(k) =
N−1
∑
n=0
2x(n)cos(π(2n + 1)
2N
k),
0 ≤k ≤N −1.
There are two main advantages of this transform over the standard DFT
calculation. The DCT coefﬁcients are real-valued for a real-valued signal.

142
Discrete Fourier Transform
This transform can produce a better energy concentration than the DFT. In
order to understand why a better energy concentration can be obtained we
will compare the DCT to the standard DFT
X(k) =
N−1
∑
n=0
x(n)e−j2πnk/N),
0 ≤k ≤N −1
calculation procedures for a real-valued signal x(n) with N samples.
In the DCT calculation it is assumed that the signal is extended as an
even function, by creating a sequence
y(n) =
!
x(n)
for
0 ≤n ≤N −1
x(2N −n −1)
for
N ≤n ≤2N −1 .
This extension eliminates the signal discontinuity at the period ending
points. Thus, in general the Fourier transform of such a signal will converge
faster, requiring fewer coefﬁcients in the reconstruction.
A zero value is then inserted between each pair of samples and a 4N
even signal z(n) is formed
z(2n + 1) = y(n),
z(2n) = 0.
The 4N-sample DFT of z(n) is calculated
XC(k) = DFT{z(n)} =
4N−1
∑
n=0
z(n)e−j2πnk/(4N)
=
2N−1
∑
n=0
z(2n + 1)e−j2π(2n+1)k/(4N) =
2N−1
∑
n=0
y(n)e−j2π(2n+1)k/(4N)
=
N−1
∑
n=0
2x(n)cos(2π(2n + 1)k
4N
) = C(k).
Only N terms of the transform are used and the DCT values are obtained.
Since the basis functions are orthogonal the inverse DCT is obtained
by multiplying both sides of the DCT by cos( 2π(2m+1)k
4N
) and summing over
0 ≤k ≤N −1,
N−1
∑
n=0
2x(n)
N−1
∑
k=0
wk cos(2π(2n + 1)k
4N
)cos(2π(2m + 1)k
4N
)
=
N−1
∑
k=0
wkC(k)cos(2π(2m + 1)k
4N
),

Ljubiša Stankovi´c
Digital Signal Processing
143
where w0 = 1/2 and wn = 1 for n ̸= 1. Since
N−1
∑
k=0
wk cos(2π(2n + 1)k
4N
)cos(2π(2m + 1)k
4N
) = N
2 δ(m −n)
we get
x(n) = 1
N
N−1
∑
k=0
wkC(k)cos(2π(2n + 1)k
4N
).
(3.34)
A symmetric relation, with the same coefﬁcients in the time and fre-
quency domain, is
C(k) = vk
N−1
∑
n=0
x(n)cos(2π(2n + 1)k
4N
)
x(n) =
N−1
∑
k=0
vkC(k)cos(2π(2n + 1)k
4N
),
where v0 = √
1/N and vn = √
2/N for n ̸= 1.
In a similar way the discrete sine transforms are deﬁned. The most
common form is the DST of type II (DST-II), whose deﬁnition is
S(k) =
N−1
∑
n=0
2x(n)sin(2π(2n + 1)
2N
(k + 1))
for ≤k ≤N −1. Its relation to the DFT can be established by creating a
sequence
y(n) =
!
x(n)
for
0 ≤n ≤N −1
−x(2N −n −1)
for
N ≤n ≤2N −1
Zero values are inserted and a signal z(n) is formed
z(2n + 1) = y(n)
z(2n) = 0.
Again a 4N-sample DFT is calculated
XS(k) =
4N−1
∑
n=0
z(n)e−j2πnk/(4N) =
2N−1
∑
n=0
y(n)e−j2π(2n+1)k/(4N)
= Im
%
N−1
∑
n=0
2jx(n)sin(2π(2n + 1)k
4N
)
;
= S(k),

144
Discrete Fourier Transform
n
x(n)
k
X(k)
n
[x(n) x(n)]
k
X2(k)
n
y(n)
k
C(k)
Figure 3.18
Illustration of the DCT calculation.
with N terms of the transform being used. The DST is the imaginary part of
this DFT.
Example 3.12. Consider a signal
x(n) = cos(2π(2n + 1)/64) + 0.75cos(7π(2n + 1)/64).
Calculate its DFT with N = 32. Plot the periodic extension of this signal. Plot
the even extension y(n) of x(n). Calculate the DFT (the DCT) of such a signal
and discuss the results.
⋆Signal x(n), along with its extended versions and corresponding
transforms, is presented in Fig.3.18. Better energy concentration in the DCT
is due to the introduced symmetry in y(n). The artiﬁcial discontinuity in the
DFT, which causes its slow convergence, is eliminated in the DCT.
By using periodic extensions in cosine transform the convolution
property of the DFT is lost. Thus, this kind of transforms may be used for a
signal reconstruction and compression but not in the realization of discrete
systems, unless they are properly related to the corresponding DFT values
(see Problem 3.10).

Ljubiša Stankovi´c
Digital Signal Processing
145
Example 3.13. For signal
x(n) = sin(2π(2n + 1)/64) −0.5cos(7π(2n + 1)/64).
Calculate its DFT with N = 32. Plot the periodic extension of this signal. Plot
even and odd extensions y(n) of x(n). Calculate the DCT and DST. Comment
the results.
⋆Signal with its periodic extensions, corresponding to the DFT, DCT,
and DST, is presented in Fig.3.19(left), as x(n), [x(n) x(n)], yc(n), and ys(n),
respectively. The corresponding transforms are shown in the right side of
this ﬁgure. Note that the convergence of the DFT and DCT is similar. Here
the DST converges faster, since its extension is smoother.
3.9
DISCRETE WALSH-HADAMARD AND HAAR TRANSFORMS
Here we will present two discrete signal transform that can be calculated
without using multiplications. One of them will be used to explain the basic
principle of the wavelet transform calculation as well.
Let us consider a two-sample signal x(n), with N = 2. The correspond-
ing two-sample DFT is
X(k) =
1
∑
n=0
x(n)e−j2πnk/2 = x(0) + (−1)kx(1).
It can be calculated without using multiplications, X(0) = x(0) + x(1) and
X(1) = x(0) −x(1). Now we can show that it is possible to deﬁne basis
functions for any signal duration in such a way that the multiplications
are not used in the signal transformation. These transform values will be
denoted by H(k). For two-sample signal case
H(0) = x(0) + x(1), for k = 0 and
H(1) = x(0) −x(1), for k = 1.
The whole frequency interval is represented by a low-frequency value X(0)
and a high-frequency value X(1). In a matrix form
-
H(0)
H(1)
.
=
-
1
1
1
−1
.-
x(0)
x(1)
.
.
(3.35)
The transformation matrix is
T2 =
-
1
1
1
−1
.
.
(3.36)

146
Discrete Fourier Transform
n
x(n)
k
X(k)
n
[x(n) x(n)]
k
X2(k)
n
ys(n)
k
S(k)
n
yc(n)
k
C(k)
Figure 3.19
Signal and its periodic extensions, corresponding to: the DFT (second row), the
cosine transform (third row), and the sine transform (fourth row). Positive frequencies for the
DFT are shown.
Example 3.14. For the signal shown in Fig. 3.20 calculate the two-sample The DFT
for each pair of signal samples
Hn(0) = yL(n) = x(2n) + x(2n + 1)
Hn(1) = yH(n) = x(2n) −x(2n + 1)
for 0 ≤n ≤N/2 −1. Discuss the results.
⋆The values of Hn(0) = yL(n) and Hn(1) = yH(n) are calculated and
are presented in Fig. 3.20. Signal yL(n) is a low-frequency, smoothed version
of the original signal, while the signal yH(n) contains the details that are
lost in the smoothed version yL(n). The original signal values may easily

Ljubiša Stankovi´c
Digital Signal Processing
147
n
x(n)
n
yL(n)
n
yH(n)
Figure 3.20
Original signal x(n) and its two-sample lowpas part yL(n) and highpass part
yH(n).
be reconstructed from Hn(0) = yL(n) and Hn(1) = yH(n) as
-
x(2n)
x(2n + 1)
.
= 1
2
- 1
1
1
−1
.- Hn(0)
Hn(1)
.
for 0 ≤n ≤N/2 −1.
In some cases the smoothed version yL(n), with a half of the samples of
the original signal, (3.20), is quite good representative of the original signal,
so there is no need to use corrections. Note that for many instants correction
is zero as well.
There are two possibilities to continue and apply the two-point DFT
scheme to a signal with N samples. One of them is in further splitting of
both yL(n) and yH(n) into their low and highpass parts. It leads to a discrete
Walsh-Hadamard transform Fig.3.21. In the other case the splitting is done
for the lowpass part only, while the highpass correction is kept as it is.
It leads to the Haar wavelet transform, Fig.3.22. These two forms will be
explained in details next.

148
Discrete Fourier Transform
n
x(n)
n
yL(n)
n
yH(n)
n
yLL(n)
n
yLH(n)
n
yHL(n)
n
yHH(n)
Figure 3.21
Illustration of the procedure leading to the Walsh-Hadamard transform calcula-
tion.
3.9.1
Discrete Walsh-Hadamard Transform
Let us continue the idea of splitting both (lowpass and highpass) parts of the
signal and deﬁne a transformation of a four-sample signal. For this signal
form two auxiliary two-sample signals yL(n) and yH(n) as
yL(0) = x(0) + x(1), yL(1) = x(2) + x(3)
(3.37)
yH(0) = x(0) −x(1), yH(1) = x(2) −x(3).
(3.38)
They represent low-frequency and high-frequency parts of the pairs: x(0),
x(1) and x(2), x(3) of two-sample signals. The lowpass part of the auxiliary

Ljubiša Stankovi´c
Digital Signal Processing
149
n
x(n)
n
yL(n)
n
yH(n)
n
yLL(n)
n
yLH(n)
Figure 3.22
Illustration of the procedure leading to the Haar wavelet transform calculation.
two-sample lowpass signal yL(n) is
H(0) = yL(0) + yL(1) = x(0) + x(1) + x(2) + x(3).
The highpass part of the auxiliary two-sample lowpass signal yL(n) is
H(1) = yL(0) −yL(1) = x(0) + x(1) −x(2) −x(3).
Then we calculate the lowpass part of the auxiliary highpass signal as
H(3) = yH(0) + yH(1) = x(0) −x(1) + x(2) −x(3).
Finally the highpass part of the auxiliary highpass signal is
H(4) = yH(0) −yH(1) = x(0) −x(1) −x(2) + x(3).

150
Discrete Fourier Transform
The transformation matrix, for the case of four-sample transform, is
⎡
⎢⎢⎣
H(0)
H(1)
H(2)
H(3)
⎤
⎥⎥⎦=
⎡
⎢⎢⎢⎢⎣
-
1
1
1
−1
.-
yL(0)
yL(1)
.
- - - - - - - - - - - - - - - - - - -
-
1
1
1
−1
.-
yH(0)
yH(1)
.
⎤
⎥⎥⎥⎥⎦
.
(3.39)
By replacing the values of yL(n) and yH(n) with signal values x(n),
we get the transformation equation
⎡
⎢⎢⎣
H(0)
H(1)
H(2)
H(3)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
1
1
1
1
1
−1
−1
1
−1
1
−1
1
−1
−1
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
⎤
⎥⎥⎦= T4
⎡
⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
⎤
⎥⎥⎦,
(3.40)
with the transformation matrix T4.
The next step would be in grouping two four-sample transforms into
an eight-sample-based analysis. The transformation equation in the case of
eight signal samples is
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
H(0)
H(1)
H(2)
H(3)
H(4)
H(5)
H(6)
H(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
1
1
1 −1 −1 −1 −1
1
1 −1 −1
1
1 −1 −1
1
1 −1 −1 −1 −1
1
1
1 −1
1 −1
1 −1
1 −1
1 −1
1 −1 −1
1 −1
1
1 −1 −1
1
1 −1 −1
1
1 −1 −1
1 −1
1
1 −1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
= T8
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(3.41)
where the transformation matrix is denoted by T8.
The inverse Walsh-Hadamard transform is x = T−1
8 H = 1
8TT
8 H.
The four-sample transformation matrix could be written as
T4 =
>
T2 ⊗
D
1
1
E
T2 ⊗
D
1
−1
E
?
=
>
T2 ⊗[T2(1,:)]
T2 ⊗[T2(2,:)]
?
where ⊗denotes Kronecker multiplication of two submatrices in T2 (its
rows) with T2, deﬁned by (3.36). Notation T2(i,:) is used for the ith row
of T2. The transformation matrix of order N is obtained by a Kronecker

Ljubiša Stankovi´c
Digital Signal Processing
151
product of N/2-order transformation matrix rows and T2,
TN =
⎡
⎢⎢⎢⎢⎣
T2 ⊗[TN/2(1,:)]
T2 ⊗[TN/2(2,:)]
...
T2 ⊗[TN/2(N/2,:)]
⎤
⎥⎥⎥⎥⎦
.
(3.42)
In this way, although we started from a two-point DFT, in splitting
the frequency domain, we did not obtain the Fourier transform of a signal,
but a form of the Walsh-Hadamard transform. In ordering the coefﬁcients
(matrix rows) in our example, we followed the frequency region order from
the Fourier domain (for example, in the four-sample case, low-low, low-
high, high-low, and high-high frequency region).
Three ways of ordering transform coefﬁcients in the Walsh-Hadamard
transform (ordering or transformation matrix rows) are used. They produce
the same result with different coefﬁcients order and different recursive
formulae for constructing transformation matrices. The presented way of
ordering coefﬁcients, as in (3.41), is known as the Walsh transform with
dyadic ordering . It will be used in examples and denoted as the Walsh-
Hadamard transform.
The Hadamard transform would correspond to the so called natural
ordering of rows from the transformation matrix T8,
H8 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1 −1
1 −1
1 −1
1 −1
1
1 −1 −1
1
1 −1 −1
1 −1 −1
1
1 −1 −1
1
1
1
1
1 −1 −1 −1 −1
1 −1
1 −1 −1
1 −1
1
1
1 −1 −1 −1 −1
1
1
1 −1 −1
1 −1
1
1 −1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
It would correspond to [H(0), H(4), H(2), H(6), H(1), H(5), H(3), H(7)]T
order of coefﬁcients in the Walsh transform with dyadic ordering (3.41).
Recursive construction of a Hadamard transform matrix H2N is easy
using the Kronecker product of T2 deﬁned by (3.36) and HN,
H2N = T2 ⊗HN =
-
HN
HN
HN
−HN
.
.
Order [H(0), H(1), H(3), H(2), H(6), H(7), H(5), H(4)]T in (3.41) would
correspond to a Walsh transform with sequency ordering.
Calculation of the Walsh-Hadamard transforms requires only addi-
tions. For an N-order transform the number of additions is (N −1)N.

152
Discrete Fourier Transform
3.9.2
Discrete Haar Wavelet Transform
Consider again two pairs of signal samples, x(0),x(1) and x(2),x(3). The
high frequency parts of these pairs are calculated as yH(n) = x(2n) −x(2n +
1), for n = 0,1. They are used in the Haar transform without any further
modiﬁcation. Since they represent highpass Haar transform coefﬁcients
they will be denoted, in this case, by W(2) = yH(0) = x(0) −x(1) and
W(3) = yH(1) = x(2) −x(3). The lowpass coefﬁcients of these pairs are
yL(0) = x(0) + x(1) and yL(1) = x(2) + x(3). The highpass and lowpass
parts of these signals are calculated as yLH(0) = [x(0) + x(1)] −[x(2) + x(3)]
and yLL(0) = [x(0) + x(1)] + [x(2) + x(3)]. For a four-sample signal the
transformation ends here with W(1) = yLH(0) and W(0) = yLL(0). Note
that the order of coefﬁcients is such that the lowest frequency coefﬁcient
corresponds to the transform index k = 0. Matrix form for a four-sample
signal is
⎡
⎢⎢⎣
W(0)
W(1)
W(2)
W(3)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
1
1
1
1
1
−1
−1
1
−1
0
0
0
0
1
−1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
⎤
⎥⎥⎦.
For an eight-sample signal the highpass coefﬁcients would be kept with-
out further modiﬁcation in each step (scale), while for the lowpass parts of
signal their highpass and lowpass parts would be calculated. The transfor-
mation matrix in the case of a signal with eight samples is
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
W(0)
W(1)
W(2)
W(3)
W(4)
W(5)
W(6)
W(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
1
1
1 −1 −1 −1 −1
1
1 −1 −1
0
0
0
0
0
0
0
0
1
1 −1 −1
1 −1
0
0
0
0
0
0
0
0
1 −1
0
0
0
0
0
0
0
0
1 −1
0
0
0
0
0
0
0
0
1 −1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(3.43)
This is the Haar transform or Haar wavelet transform of a signal with eight
samples.
The Haar transform is useful in the analysis of signals when we can
expect that in a slow-varying signal there are few details.
The Haar wavelet transform is computationally very efﬁcient. The
efﬁciency comes from the fact that the Haar wavelet transform almost does
not transform the signal at high frequencies. It leaves it almost as it is, using
a very simple two-sample transform. For lower frequencies the number of
operations is increased.

Ljubiša Stankovi´c
Digital Signal Processing
153
In speciﬁc, for the highest N/2 coefﬁcients the Haar transform does
only one addition (of two signal values) for each coefﬁcient. For next N/4
coefﬁcients the Haar wavelet uses 4 signal values with 3 additions and so
on. The total number of additions is for a Haar transform is
Nadditions = N
2 (2 −1) + N
4 (4 −1) + N
8 (8 −1) + ... + N
N (N −1).
For N of the form N = 2m we can write
Nadditions = N log2 N −N(1
2 + 1
22 + 1
23 + ... + 1
2m )
= N log2 N −N 1
2
1 −1
2m
1 −1
2
= N log2 N −(N −1) = N [log2 N −1] + 1.
This is the same order of additions as in the FFT algorithms.
Example 3.15. Consider a signal
x(n) = [2,2,12,−8,2,2,2,2,−3,−3,−3,−3,3,−9,−3,−3].
Calculate its Haar and Walsh-Hadamard transform with N = 16. Discuss the
results.
⋆Signal x(n) is presented in Fig.3.23. In full analogy with (3.43) a
Haar transformation matrix of order N = 16 is formed. For example, higher
coefﬁcients are just two-sample signal transforms,
W(k) = x(2(k −8)) −x(1 + 2(k −8)),
k = 8,9,...,15.
Although there are some short duration pulses (x(2),x(3),x(13)), the Haar
transform coefﬁcients W(2), W(3), ...,W(8), W(10), W(11),W(12),W(13),
W(15) are zero-valued, Fig.3.23. This is the result of its property to decom-
pose the high frequency signal region into short duration (two-sample) basis
functions. Then a short duration pulse is contained in high frequency part
of only one Haar coefﬁcient. That is not the case in the Fourier transform
(or Walsh-Hadamard transform) where a single delta pulse will cause that
all coefﬁcients are nonzero, Fig.3.24. Transformation matrix T16 is obtained
from T8 using (3.42).
Property that high-frequency coefﬁcients are well localized in time and
they represent a short duration signal components is used in image compres-
sion where adding high frequency coefﬁcients adds details into an image,
with important property that one detail in the image corresponds to one (a
few) nonzero coefﬁcient. Reconstruction with the Haar transform with dif-
ferent number of coefﬁcients is presented in Fig.3.23. As explained it can be
considered as "a zooming" a signal toward the details when the higher fre-
quency coefﬁcients are added. Since a half of the coefﬁcients are zero-valued

154
Discrete Fourier Transform
n
x(n)
k
W(k)
n
x0(n)
n
x0-1 (n)
n
x0-1,9 (n)
k
x0-1,9,14 (n)
Figure 3.23
Signal x(n) and its discrete Haar transform H(k). Reconstructed signals: using
H(0) presented by x0(n), using two coefﬁcients H(0) and H(1) denoted by x0−1(n), using
H(0), H(1), and H(9) denoted by x0−1,9(n), and using H(0), H(1), H(9), and H(14) denoted
by x0−1,9,14(n). Vertical axes scales for the signal and transform are different.
n
x(n)
k
H(k)
Figure 3.24
Signal x(n) and its Walsh-Hadamard transform HD(k).
a signiﬁcant compression ratio can be achieved by storing or transmitting
the nonzero coefﬁcients only. This is a basic idea for multiresolution wavelet
based image representations and compression.

Ljubiša Stankovi´c
Digital Signal Processing
155
n
x(n)
k
W(k)
k
H(k)
n
x(n)
k
W(k)
k
H(k)
Figure 3.25
The Haar wavelet transform (second row) and the Walsh-Hadamard transform
(third row) for high frequency long duration signals (ﬁrst row). Vertical axes scales for the
signal and transform are different.
Example 3.16. For long duration signals with high-frequency components:
(a) x(n) = [1,−1,1,−1,1,−1,1,−1,1,−1,1,−1,1,−1] and
(b) x(n) = [2,0,−2,0,0,−2,0,2,0,2,0,−2,−2,0,2,0], calculate the Haar
wavelet transform and the Walsh-Hadamard transform with N = 16.
⋆The Haar wavelet transform and the Walsh-Hadamard transform are
shown in Fig.3.25. We can see that for a signal of long duration on high
frequencies the number of nonzero coefﬁcients in the Haar wavelet trans-
form is large. Just one such component in the Walsh-Hadamard transform
can require a half of the available coefﬁcients in the Haar wavelet transform,
Fig.3.25(left). In addition to the fact that a much smaller number of coef-
ﬁcients is used for the Walsh-Hadamard transform based reconstruction, a
very large number of coefﬁcients in the Haar wavelet transform reconstruc-
tion may annul its calculation complexity advantage in this case.

156
Discrete Fourier Transform
3.10
PROBLEMS
Problem 3.1. Calculate the DFT of signals using the smallest possible value
of N: a) x(n) = δ(n), b) x(n) = δ(n) + δ(n −1) −2jδ(n −2) + 2jδ(n −3) +
δ(n −4), and c) x(n) = an(u(n) −u(n −10)).
Problem 3.2. If the signals g(n) and f (n) are real-valued show that their
DFTs, G(k) and F(k), can be obtained from the DFT Y(k) of the signal
y(n) = g(n) + jh(n).
Problem 3.3. The relationship between the DFT index and the continuous
signal frequency is given by
Ω=
%
2πk/(N∆t)
for 0 ≤k ≤N/2 −1
2π(k −N)/(N∆t)
for N/2 ≤k ≤N −1.
This mapping is achieved in programs by using shift functions. Show that
the shift will not be necessary if we use the signal x(n)(−1)n. The DFT
values of this signal will start from the lowest negative frequency, toward
the highest positive frequency.
Problem 3.4. If the DFT of signal x(n) with period N is X(k) ﬁnd the DFT
of signals
y(n) =
!
x(n)
for
n = 2m
0
for
n = 2m + 1
and
z(n) =
!
0
for
n = 2m
x(n)
for
n = 2m + 1
.
Problem 3.5. Using the DFT ﬁnd a convolution of signals x(n) and h(n)
whose nonzero values are x(0) = 1, x(1) = −1 and h(0) = 2, h(1) = −1,
h(2) = 2.
Problem 3.6. Find a circular convolution of x(n) = ej4πn/N + sin(2πn/N)
and h(n) = cos(4πn/N) + ej2πn/N within the common period of signals.
Problem 3.7. Find the signal whose DFT is Y(k) = |X(k)|2 and X(k) is the
DFT of x(n) = u(n) −u(n −3) with period N = 10.
Problem 3.8. What is the relation between the discrete Hartley transform
(DHT) of real-valued signals
H(k) =
N−1
∑
n=0
x(n)
*
cos 2πnk
N
+ sin 2πnk
N
+

Ljubiša Stankovi´c
Digital Signal Processing
157
and the DFT? Express the DHT in terms of the DFT and the DFT in terms of
the DHT.
Problem 3.9. Show that the DCT of a signal x(n) with N samples, deﬁned
by
C(k) =
N−1
∑
n=0
2x(n)cos(2πk
2N (n + 1
2))
can be calculated using an N-sample DFT of the signal
y(n) =
!
2x(2n)
for
0 ≤n ≤N/2 −1
2x(2N −2n −1)
for
N/2 ≤n ≤N −1
as
C(k) = Re{e−j πk
2N
N−1
∑
n=0
y(n)e−j 2πk
N n} = Re{e−j πk
2N DFT{y(n)}}.
Problem 3.10. A real-valued signal x(n) of a duration shorter than N,
deﬁned for 0 ≤n ≤N −1, has the Fourier transform X(k). A signal y(n)
is formed as
y(n) =
!
2x(n)
for
0 ≤n ≤N −1
0
for
N ≤n ≤2N −1
,
(3.44)
with the DFT Y(k), then a signal z(n) is formed using
z(2n + 1) = y(n)
z(2n) = 0.
(a) What are the real and imaginary parts of Z(k) = DFT{z(n)}? How
they are related to the DCT and DST of x(n)? (b) The signal x(n) is applied
as an input to a system with impulse response h(n) such that h(n) is of
duration shorter than N, deﬁned within 0 ≤n ≤N −1, and x(n) ∗n h(n) is
also within 0 ≤n ≤N −1. The DCT of the output signal is calculated. How
it is related to the DCT and DST of x(n)?
Problem 3.11. Consider a signal x(n) whose duration is N, with nonzero
values within the interval 0 ≤n ≤N −1. Deﬁne a system with the output
yk(n + (N −1)) =
N−1
∑
m=0
x(n + m)e−j2πmk/N

158
Discrete Fourier Transform
so that its value yk(N −1) at the last instant of the signal duration is equal
to the DFT of signal, for a given k,
yk(N −1) =
N−1
∑
m=0
x(m)e−j2πmk/N = DFT{x(n)} = X(k).
Note that the system is causal since yk(n) uses only x(n) at instant n and
previous instants.
Show that the output signal yk(n) is related to previous output value
yk(n −1) by the equation
yk(n) = ej2πk/Nyk(n −1) + ej2πk/N[x(n) −x(n −N)].
This equation can be used for a recursive DFT calculation.
Problem 3.12. Show that the discrete Hartley transform (DHT) coefﬁcients
of a signal x(n) with an even number of samples N can be calculated, for an
even frequency index k = 2r, as a DHT with N/2 samples.
Problem 3.13. Find the DFT of signal x(n) = exp(j4π
√
3n/N), for n =
0,1,..., N −1 with N = 16. If the DFT is interpolated four times (signal zero-
padded), ﬁnd the displacement bin, estimate the frequency, and compare it
with the true frequency value. What is the displacement bin if the general
formula is applied without interpolation?
3.11
SOLUTIONS
Solution 3.1. The DFT assumes that the signals are periodic. In order to
calculate the DFT we have to assume a period of signals ﬁrst. Period N
should be greater or equal to the duration of signal, so that the signal values
do not overlap. Larger values of N will increase the density of the frequency
domain samples, but will also increase the computation time.
a) For this signal any N ≥1 is acceptable, producing
X(k) = 1,
k = 0,1,..., N −1,
with period N.
b) We may use any N ≥5. Using N = 5 we get:
X(k) =
5−1
∑
n=0
x(n)e−j2πnk/5 = 1 + e−j2πk/5 −2je−j4πk/5 + j2e−j6πk/5 + e−j8πk/5
= 1 + 2cos(2πk/5) −4sin(4πk/5).

Ljubiša Stankovi´c
Digital Signal Processing
159
c) For a period N ≥10
X(k) =
9
∑
n=0
(ae−j2πk/N)n = 1 −a10e−j2πk(10/N)
1 −ae−j2πk/N
.
Solution 3.2. From y(n) = g(n) + j f (n) the real and imaginary parts g(n)
and f (n) can be obtained as
g(n) = y(n) + y∗(n)
2
, and f (n) = y(n) −y∗(n)
2j
.
Since the DFT of y∗(n) is equal to
DFT{y∗(n)} =
N−1
∑
n=0
y∗(n)e−j2πnk/N =
(
N−1
∑
n=0
y(n)ej2πnk/N
)∗
with ej2πnk/N = ej2πn(k−N)/N = e−j2πn(N−k)/N, it follows
DFT{y∗(n)} = Y∗(N −k).
Then the DFTs of signals g(n) and f (n) are
G(k) = Y(k) + Y∗(N −k)
2
and F(k) = Y(k) −Y∗(N −k)
2j
.
Solution 3.3. The DFT of x(n)(−1)n is
X1(k) =
N−1
∑
n=0
x(n)(−1)ne−j2πnk/N.
For 0 ≤k ≤N/2 −1
X1(k) =
N−1
∑
n=0
x(n)e−jπne−j2πnk/N =
N−1
∑
n=0
x(n)e−j2πn(k+N/2)/N = X(k + N
2 ).
For N/2 ≤k ≤N −1
X1(k) =
N−1
∑
n=0
x(n)ejπne−j2πnk/N =
N−1
∑
n=0
x(n)e−j2πn(k−N/2)/N = X(k −N
2 ).

160
Discrete Fourier Transform
Solution 3.4. The DFT of signal y(n) is
Y(k) =
N−1
∑
n=0
y(n)e−j2πnk/N =
N−1
∑
n=0
[x(n) + (−1)nx(n)]e−j2πnk/N
=
N−1
∑
n=0
[x(n) + x(n)e−jπnN/N]e−j2πnk/N = X(k) + X(k + N
2 )
with X(k + N/2) = X(k −N/2) for k > N/2.
For z(n) the DFT is
Z(k) =
N−1
∑
n=0
z(n)e−j2πnk/N =
N−1
∑
n=0
[x(n) −(−1)nx(n)]e−j2πnk/N
= X(k) −X(k + N
2 ).
Obviously Y(k) + Z(k) = X(k).
Solution 3.5. For the convolution calculation, using the DFT, the minimal
number N is N = K + L −1 = 4, where K = 2 is the duration of x(n) and
L = 3 is the duration of h(n). With N = 4 follows
X(k) = 1 −e−j2πk/4
H(k) = 2 −e−j2πk/4 + 2e−j4πk/4
Y(k) = X(k)H(k) = (1 −e−j2πk/4)(2 −e−j2πk/4 + 2e−j4πk/4)
= 2 −3e−j2πk/4 + 3e−j4πk/4 −2e−j6πk/4.
The signal is
y(n) = IDFT{Y(k)} = 2δ(n) −3δ(n −1) + 3δ(n −2) −2δ(n −3).
Solution 3.6. The circular convolution of y(n) = x(n) ∗h(n) has the DFT
Y(k) = X(k)H(k) with
X(k) =
N−1
∑
n=0
[ej4πn/N + 1
2jej2πn/N −1
2je−j2πn/N]e−j2πnk/N
= Nδ(k −2) + N
2j δ(k −1) −N
2j δ(k + 1)

Ljubiša Stankovi´c
Digital Signal Processing
161
and
H(k) =
N−1
∑
n=0
[1
2ej4πn/N + 1
2e−j4πn/N + ej2πn/N]e−j2πnk/N
= N
2 δ(k −2) + N
2 δ(k + 2) + Nδ(k −1).
The value of Y(k) is
Y(k) = N2
2 δ(k −2) + N2
2j δ(k −1).
The inverse DFT is
y(n) = N
2 ej4πn/N + N
2j ej2πn/N.
Solution 3.7. The DFT can be written as Y(k) = X(k)X∗(k) with
y(n) = IDFT{X(k)} ∗n IDFT{X∗(k)}.
Since
IDFT{X∗(k)} =
N−1
∑
k=0
X∗(k)ej2πnk/N =
(
N−1
∑
k=0
X(k)e−j2πnk/N
)∗
=
(
N−1
∑
k=0
X(k)ej2πk(N−n)/N
)∗
= x∗(N −n)
we get
y(n) = (x(n))10 ∗n (x∗(10 −n))10
= (u(n) −u(n −3))10 ∗n (u(10 −n) −u(7 −n))10
= (δ(n + 2) + 2δ(n + 1) + 3δ(n) + 2δ(n −1) + δ(n −2))10
where (x(n))N indicates that the signal is periodically extended with N.
Solution 3.8. For a real-valued signal holds
X(k) =
N−1
∑
n=0
[x(n)cos 2πnk
N
−jx(n)sin 2πnk
N
]
X(N −k) =
N−1
∑
n=0
[x(n)cos 2πnk
N
+ jx(n)sin 2πnk
N
].

162
Discrete Fourier Transform
Thus,
N−1
∑
n=0
x(n)cos 2πnk
N
= X(k) + X(N −k)
2
= H(k) + H(N −k)
2
N−1
∑
n=0
x(n)sin 2πnk
N
= X(N −k) −X(k)
2j
= H(k) −H(N −k)
2
.
The DHT can be calculated as a sum of these terms,
2H(k) = X(k) + X(N −k) −j[X(N −k) −X(k)].
The DFT is obtained using the DHT in the same way as
2X(k) = H(k) + H(N −k) −j[H(k) −H(N −k)].
Solution 3.9. We can split the DCT sum into an even and odd part
C(k) =
N−1
∑
n=0
2x(n)cos(2πk
2N (n + 1
2)) =
N/2−1
∑
n=0
2x(2n)cos(2πk
2N (2n + 1
2)) +
N/2−1
∑
n=0
2x(2n + 1)cos(2πk
2N (2n + 1 + 1
2)).
By reverting the summation index in the second sum using n = N/2 −1 −m
the summation in m is from m = N/2 −1 for n = 0 down to m = 0 for
n = N/2 −1. Then
N/2−1
∑
n=0
2x(2n + 1)cos(2πk
2N (2n + 1 + 1
2))
=
N/2−1
∑
m=0
2x(N −2m −1)cos(2πk
2N (N −2m −1 + 1
2)).
Shifting now the summation index in this sum for N/2 + m = n follows
N/2−1
∑
m=0
2x(N −2m −1)cos(2πk
2N (N −2m −1 + 1
2))
=
N−1
∑
n=N/2
2x(2N −2n −1)cos(2πk
2N (2N −2n −1
2)).

Ljubiša Stankovi´c
Digital Signal Processing
163
Now we can go back to the DCT and to replace the second sum, to get
C(k) =
N/2−1
∑
n=0
2x(2n)cos(2πk
2N (2n + 1
2))
+
N−1
∑
n=N/2
2x(2N −2n −1)cos(2πk
2N (2n + 1
2)) =
N−1
∑
n=0
y(n)cos(2πk
2N (2n + 1
2))
with cos( 2πk
2N (2N −2n −1
2)) = cos( 2πk
2N (2n + 1
2)) and
y(n) =
!
2x(2n)
for
0 ≤n ≤N/2 −1
2x(2N −2n −1)
for
N/2 ≤n ≤N −1
or
C(k) = Re{
N−1
∑
n=0
y(n)e−j 2πk
2N (2n+ 1
2 )} = Re{e−j πk
2N DFT{y(n)}}.
Solution 3.10. (a) For the signal z(n) we can write
DFT{z(n)} =
4N−1
∑
n=0
z(n)e−j2πnk/(4N) =
2N−1
∑
n=0
z(2n + 1)e−j2π(2n+1)k/(4N)
=
2N−1
∑
n=0
y(n)e−j2π(2n+1)k/(4N) =
N−1
∑
n=0
2x(n)e−j2π(2n+1)k/(4N).
The real and imaginary parts of DFT{z(n)} are
Re{DFT{z(n)}} =
N−1
∑
n=0
2x(n)cos(2π(2n + 1)k
4N
) = C(k)
Im{DFT{z(n)}} = −
N−1
∑
n=0
2x(n)sin(2π(2n + 1)k
4N
) = −S(k)
DFT{z(n)} = C(k) −jS(k),
and
Z(k) = DFT{z(n)} = e−j2πk/(4N)
N−1
∑
n=0
2x(n)e−j2πnk/(2N)
Z(k)ejπk/(2N) = Y(k) = 2X(k/2).
Note that X(k/2) is just a notation for 2X( k
2) = Y(k), where Y(k) =
DFT{y(n)} and y(n) is zero-padded version of 2x(n) deﬁned by (3.44).

164
Discrete Fourier Transform
b) If the signal x(n) is input to a system then the DCT is calculated for
xh(n) = x(n) ∗n h(n)
Xh(k) = X(k)H(k).
It has been assumed that all x(n), h(n), and x(n) ∗n h(n) are zero-valued
outside 0 ≤n ≤N −1 (it means that the duration of x(n) and h(n) should
be such that their convolution is within 0 ≤n ≤N −1) . Then for a signal
zh(n) related to xh(n) = x(n) ∗n h(n) in the same way as z(n) to x(n) in a)
we can write
DFT{zh(n)}ejπk/(2N) = 2Xh( k
2) = 2X( k
2)H( k
2) = Y(k)H( k
2).
Then
Ch(k) = DCT{xh(n)} = Re{Y(k)H( k
2)e−jπk/(2N)}
= Re{Y(k)e−jπk/(4N)}Re{H( k
2)} −Im{Y(k)e−jπk/(4N)}Im{H( k
2)}
= C(k)Re{H( k
2)} + S(k)Im{H( k
2)}.
The system output is x(n) ∗n h(n) = xh(n) = IDCT{Ch)k)}, (3.34). Transform
H(k/2) is the DFT of zero-padded h(n) with factor 2. Only ﬁrst half of the
DFT samples are then used.
Solution 3.11. For the signal yk(n) we may write
yk(n) =
N−1
∑
m=0
x(n −N + 1 + m)e−j2πmk/N.
Now let us shift the summation
yk(n) =
N
∑
m=1
x(n −N + m)e−j2π(m−1)k/N = ej 2π
N k
N
∑
m=1
x(n −N + m)e−j2πmk/N
= ej 2π
N k[
N−1
∑
m=0
x(n −N + m)e−j2πmk/N −x(n −N)e−j2π0k/N + x(n)e−j2πNk/N]
= ej2πk/N[yk(n −1) −x(n −N) + x(n)].
For 0 ≤n ≤N −1
yk(n) = ej2πk/N[yk(n −1) + x(n)]

Ljubiša Stankovi´c
Digital Signal Processing
165
since x(n −N) = 0. This proves the problem statement.
If the signal x(n) continues as a periodic signal after n = 0,
xp(n) =
∞
∑
l=0
x(n −lN)
(3.45)
then, for n ≥N, it holds xp(n −N) = xp(n) and yk(n) = ej2πk/Nyk(n −1),
yk(n) =
⎧
⎨
⎩
0
for
n < 0
yk(n) = ej2πk/N[yk(n −1) + x(n)]
for
0 ≤n ≤N −1
yk(n) = ej2πk/Nyk(n −1)
for
n ≥N
for xp(n) deﬁned by (3.45).
Solution 3.12. For k = 2r the DHT can be written as
H(2r) =
N/2−1
∑
n=0
x(n)
@
cos 2πrn
N/2 + sin 2πrn
N/2
A
+
N−1
∑
n=N/2
x(n)
@
cos 2πrn
N/2 + sin 2πrn
N/2
A
=
N/2−1
∑
n=0
(x(n) + x(n + N/2))
@
cos 2πrn
N/2 + sin 2πrn
N/2
A
.
Therefore H(2r) is
H(2r) =
N/2−1
∑
n=0
g(n)
-
cos 2πrn
N/2 + sin 2πrn
N/2
.
where g(n) = x(n) + x(n + N/2). This is a DHT of g(n) with N/2 samples.
Note: For odd frequency indices k = 2r + 1 we can write
H(2r + 1) =
N−1
∑
n=0
x(n)
-
cos 2π(2r + 1)n
N
+ sin 2π(2r + 1)n
N
.
.
After some lengthy, but straightforward transformations, we get
H(2r + 1) =
N/2−1
∑
n=0
f (n)
-
cos 2πnr
N/2 + sin 2πnr
N/2
.
where
f (n) = [x(n) −x(n + N
2 )]cos 2πn
N
+ [x( N
2 −n) −x(N −n)]sin 2πn
N .
This is again a DHT of a signal f (n) with N/2 samples.

166
Discrete Fourier Transform
Solution 3.13. The DFT is
|X(k)| = |
15
∑
n=0
ej2π(2
√
3−k)n/16| =
''''''
sin
B
π(2
√
3 −k)
C
sin
B
π(2
√
3 −k)/16
C
''''''
,
with
(3.46)
|X| = (1.5799, 2.1361, 3.5045, 10.9192, 9.4607, 3.3454,
2.0805, 1.5530, 1.2781, 1.1225, 1.0362, 0.99781, 0.9992, 1.0406, 1.1310, 1.2929),
where |X| is the vector whose elements are the DFT values |X(k)|, k =
0,1,...,15. Maximal value is at k = 3, meaning that the frequency estimation
without displacement bin would be (2π · 3)/16 = 1.1781, while the true
frequency is (2π · 2
√
3)/16 = 1.3603. The error is 13.4%.
For the zero-padded signal (interpolated DFT), with a factor of 4,
|X(k)| = |
15
∑
n=0
ej4π
√
3n/16e−j2πnk/64| =|
15
∑
n=0
ej2π(8
√
3−k)n/64|
=
''''''
sin
B
π(8
√
3 −k)/4
C
sin
B
π(8
√
3 −k)/64
C
''''''
.
Maximal value is obtained for k =
@
8
√
3
A
= 14, where
@
8
√
3
A
denotes the
nearest integer value. Then
|X(14)| =
''''''
sin
B
π(8
√
3 −14)/4
C
sin
B
π(8
√
3 −14)/64
C
''''''
= 15.9662,
|X(15)| =
''''''
sin
B
π(8
√
3 −15)/4
C
sin
B
π(8
√
3 −15)/64
C
''''''
= 13.9412
|X(13)| =
''''''
sin
B
π(8
√
3 −13)/4
C
sin
B
π(8
√
3 −13)/64
C
''''''
= 14.8249,
and the displacement bin is
d = 0.5
|X(15)| −|X(13)|
2|X(14)| −|X(15)| −|X(15)| = −0.1395.
The true frequency index would be 8
√
3 = 13.8564, with the true frequency
2π · 13.8564/64 = 1.3603. The correct value of frequency index is shifted

Ljubiša Stankovi´c
Digital Signal Processing
167
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
-3
-2
-1
0
1
2
3
4
 x( n)
 n
Figure 3.26
Discrete signal x(n) (Exercise 3.4)
from the nearest integer k = 14 (on the frequency grid) for 14 −13.8564 =
−0.1436, when the interpolation is done. Thus, the obtained displacement
bin value −0.1395 is close to the true shift value −0.1436. The estimated
frequency, using the displacement bin, is 1.3608. As compared to the true
frequency the error is 0.03%.
If the displacement formula is applied on the DFT values, without
interpolation, we would get d = 0.3356, while 2
√
3 = 3.4641 is displaced
from the nearest integer for 0.4641.
3.12
EXERCISE
Exercise 3.1. Find the DFT of x(n) = δ(n) −δ(n −3) with N = 4 and N = 8.
Exercise 3.2. Calculate the DFT of signal x(n) = sin(nπ/4) for 0 ≤n < N
with N = 8 and N = 16.
Exercise 3.3. For a real-valued signal the DFT is calculated with N = 8 and
the following DFT values are known: X(0) = 1, X(2) = 2 −j, X(5) = j,
X(7) = 3. Find the remaining values. What are the values of x(0) and
∑7
n=0 x(n)?
Exercise 3.4. Signal x(n) is presented in Fig. 3.26. Find X(0), X(4), and X(8),
where X(k) is the DFT of x(n) calculated with N = 16.
Exercise 3.5. Prove that for an arbitrary real-valued signal x(n), deﬁned for
0 ≤n < N, where N is an even integer, the DFT value X(N/2) is real-valued.

168
Discrete Fourier Transform
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
-3
-2
-1
0
1
2
3
4
 X( k)
 k
Figure 3.27
DFT of the discrete signal x(n) (Exercise 3.6)
Exercise 3.6. Consider a signal x(n) whose DFT values X(k), calculated
with N = 16, are presented in Fig. 3.27.
1. Find the DFT of signal y1(n) = x(n) + (−1)nx(n).
2. Find the DFT of signal y2(n) = x(n) −(−1)nx(n).
3. Find the DFT of signal y3(n) = x(n) ∗x(N −n) where ∗denotes a
circular convolution with period N.
4. Find the DFT of signal
y4(n) =
%
x(n)
for n ̸= 8
0
for n = 8.
5. Find x(0).
6. Calculate ∑15
n=0 x(n).
7. Calculate ∑15
n=0 |x(n)|2.
8. Calculate ∑15
n=0(−1)nx(n).
Exercise 3.7. Prove that if |x(n)| ≤A for 0 ≤n < N then |X(k)| ≤NA for
any k, where X(k) is the DFT of x(n) calculated with N points.
Exercise 3.8. Prove that if ∑N−1
n=0 |x(n)| ≤A then ∑N−1
k=0 |X(k)| ≤NA where
X(k) is the DFT of x(n) calculated with N points.

Chapter 4
z-Transform
T
HE Fourier transform of discrete signals and the DFT are used for
direct signal processing and calculations. A transform that gener-
alizes these transforms, in the same way as the Laplace transform
generalizes the Fourier transform of continuous signals, is the z-transform.
This transform provides an efﬁcient tool for qualitative analysis and design
of the discrete systems.
4.1
DEFINITION OF THE Z-TRANSFORM
The Fourier transform of a discrete-time signal x(n) can be considered as a
special case of the z-transform deﬁned by
X(z) =
∞
∑
n=−∞
x(n)z−n,
(4.1)
where z = rexp(jω) is a complex number. Value of the z-transform along
the unit circle |z| = 1 or z = exp(jω) is equal to the Fourier transform of
discrete-time signals.
The z-transform, in general, converges only for some values of the
complex argument z. The region of values of z where X(z) is ﬁnite is the
region of convergence (ROC) of the z-transform.
Example 4.1. Consider a discrete-time signal
x(n) = anu(n) + bnu(n),
where a and b are complex numbers, |a| < |b|. Find the z-transform of this
signal and its region of convergence.
169

170
z-Transform
a
Re{z}
Im{z}
b
Re{z}
Im{z}
b
a
Re{z}
Im{z}
Figure 4.1
Regions of convergence (gray area)
⋆The z-transform of x(n) is
X(z) =
∞
∑
n=0
(anz−n + bnz−n) =
∞
∑
n=0
(a/z)n +
∞
∑
n=0
(b/z)n
=
1
1 −a/z +
1
1 −b/z =
z
z −a +
z
z −b.
Inﬁnite geometric series with progression coefﬁcient (a/z) converges for
|a/z| < 1. Thus, the region of convergence for the ﬁrst part of the z-transform
is |z| > |a|. The other series converges for |b/z| < 1, i.e., for |z| > |b|. The
resulting transform is ﬁnite if both parts are ﬁnite (or do not cancel out to
produce a ﬁnite value). Since |a| < |b|, the region of convergence for X(z) is
|z| > |b|, Fig.4.1.
Example 4.2. Consider a discrete-time signal
x(n) = anu(n −1) −bnu(−n −1) + 2δ(n −2),
where a and b are complex numbers, |b| > |a|. Find the z-transform of x(n)
and its region of convergence.
⋆The z-transform is
X(z) =
∞
∑
n=1
anz−n −
−1
∑
n=−∞
bnz−n + 2z−2 =
∞
∑
n=1
anz−n −
∞
∑
n=1
b−nzn + 2z−2
=
a/z
1 −a/z −
z/b
1 −z/b + 2z−2 =
a
z −a +
z
z −b + 2z−2.
Inﬁnite geometric series with progression coefﬁcient (a/z) converges for
|a/z| < 1. The other series converges for |z/b| < 1. The last term has pole
at z = 0. Since |b| > |a| the region of convergence is |a| < |z| < |b|, Fig.4.2.
Note that in this example and the previous one two different signals
bnu(n) and −bnu(−n −1) produced the same z-transform Xb(z) = z/(z −b),
but with different regions of convergence.

Ljubiša Stankovi´c
Digital Signal Processing
171
a
Re{z}
Im{z}
b
Re{z}
Im{z}
b
a
Re{z}
Im{z}
Figure 4.2
Regions of convergence (gray area)
4.2
PROPERTIES OF THE Z-TRANSFORM
4.2.1
Linearity
The z-transform is linear since
Z{ax(n) + by(n)} =
∞
∑
n=−∞
[ax(n) + by(n)]z−n = aX(z) + bY(z)
with the region of convergence being at least the intersection of the regions
of convergence of X(z) and Y(z). In special cases the region can be larger
than the intersection of the regions of convergence of X(z) and Y(z) if
some poles, deﬁning the region of convergence, cancel out in the linear
combination of transforms.
4.2.2
Time-Shift
For a shifted signal x(n −n0) the z-transform is
Z{x(n −n0)} =
∞
∑
n=−∞
x(n −n0)z−n =
∞
∑
n=−∞
x(n)z−(n+n0) = X(z)z−n0.
Additional pole at z = 0 is introduced for n0 > 0. The region of convergence
is the same except for z = 0 or z →∞, depending on the value of n0.
Example 4.3. For a causal signal x(n) = x(n)u(n) ﬁnd the z-transform of x(n +
n0)u(n), for n0 ≥0.

172
z-Transform
⋆The signal x(n + n0)u(n) has a z-transform
Z{x(n + n0)u(n)} =
∞
∑
n=0
x(n + n0)z−n =
∞
∑
n=0
x(n + n0)z−(n+n0)zn0
= zn0
>
∞
∑
n=0
x(n)z−n −x(0) −x(1)z−1 −... −x(n0 −1)z−n0+1
?
= zn0
@
X(z) −x(0) −x(1)z−1 −... −x(n0 −1)z−n0+1A
.
For n0 = 1 follows Z{x(n + 1)u(n)} = zX(z) −x(0). Note that for this signal
x(n + n0)u(n) ̸= x(n + n0)u(n + n0).
4.2.3
Multiplication by exponential signal: Modulation
For a signal multiplied by an exponential signal the z-transform is
Z{anx(n)} =
∞
∑
n=−∞
x(n)anz−n = X(z
a),
with region of convergence being scaled by |a|. In a special case when
a = ejω0, the z-transform plane is just rotated
Z{ejω0nx(n)} =
∞
∑
n=−∞
x(n)ejω0nz−n = X(ze−jω0)
with the same region of convergence as X(z).
4.2.4
Differentiation
Consider the z-transform of a causal signal x(n)
X(z) =
∞
∑
n=0
x(n)z−n and dX(z)
dz
=
∞
∑
n=0
−nx(n)z−n−1.
We can conclude that
Z{nx(n)u(n)} = −zdX(z)
dz
.
This kind of the z-transform derivations can be generalized to
Z{n(n −1)...(n −N −1)x(n)u(n)} = (−1)NzN dNX(z)
dzN
.

Ljubiša Stankovi´c
Digital Signal Processing
173
4.2.5
Convolution in time
The z-transform of a convolution of signals x(n) and y(n) is
Z{x(n) ∗y(n)} = Z{
∞
∑
m=−∞
x(m)y(n −m)}
=
∞
∑
n=−∞
∞
∑
m=−∞
x(m)y(n −m)z−n =
∞
∑
l=−∞
∞
∑
m=−∞
x(m)y(l)z−m−l = X(z)Y(z)
with the region of convergence being at least the intersection of the re-
gions of convergence of X(z) and Y(z). In the case of a product of two z-
transforms it may happen that some poles are canceled out causing that the
resulting region of convergence is larger than the intersection of the individ-
ual regions of convergence.
4.2.6
Table of the z-transform
Signal x(n)
z-transform X(z)
δ(n)
1
u(n)
z
1−z , |z| > |1|
anu(n)
z
a−z , |z| > |a|
nan−1u(n)
−z
(a−z)2 , |z| > |a|
−anu(−n −1)
z
a−z, |z| < |a|
anx(n)
X(z/a)
a|n|, |a| < 1
z(1−a2)
(z−a)(1−az), |a| < |z| < |1/a|
x(n −n0)
z−n0X(z)
nx(n)u(n)
−zdX(z)/dz
n(n −1)x(n)u(n)
z2d2X(z)/dz2
cos(ω0n)u(n)
1−z−1 cos(ω0)
1−2z−1 cos(ω0)+z−2
sin(ω0n)u(n)
1−z−1 sin(ω0)
1−2z−1 cos(ω0)+z−2
1
n!u(n)
exp(z)
[x(n)u(n)] ∗u(n) = ∑n
m=−∞x(m)
z
z−1X(z)

174
z-Transform
4.2.7
Initial and Stationary State Signal Value
The initial value of a causal signal may be calculated as
x(0) = lim
z→∞X(z).
(4.2)
According to the z-transform deﬁnition all terms with z−n vanishes as
z →∞. The term which does not depend on z follows then. It is the term
with x(0).
The stationary state value of a causal signal x(n) is
lim
n→∞x(n) = lim
z→1(z −1)X(z).
(4.3)
This relation follows from
Z{x(n + 1)u(n))} −Z{x(n)u(n))} = zX(z) −x(0) −X(z)
Z{x(n + 1)u(n))} −Z{x(n)u(n))} = lim
N→∞
z→1
>
N
∑
n=0
x(n + 1)z−n −
N
∑
n=0
x(n)z−n
?
= lim
N→∞[x(N + 1) −x(0)].
Thus,
lim
N→∞[x(N + 1) −x(0)] = zX(z) −x(0) −X(z),
produces the stationary state value (4.3).
4.3
INVERSE Z-TRANSFORM
4.3.1
Direct Power Series Expansion
Most common approach to the z-transform inversion is based on a direct
expansion of the given transform into power series with respect to z−1
within the region of convergence. After the z-transform is expanded into
series
X(z) =
∞
∑
n=−∞
Xnz−n
the signal is identiﬁed as x(n) = Xn for −∞< n < ∞.

Ljubiša Stankovi´c
Digital Signal Processing
175
In general various techniques may be used to expand a function into
power series. Most of the cases in signal processing, after some transforma-
tions, reduce to a simple form of an inﬁnite geometric series
1
1 −q = 1 + q + q2 + ... =
∞
∑
n=0
qn
for |q| < 1.
Example 4.4. For the z-transform
X(z) =
1
1 −1
2z−1 +
1
1 −3z
identify possible regions of convergence and ﬁnd the inverse z-transform for
each of them.
⋆Obviously the z-transform has the poles z1 = 1/2 and z2 = 1/3. Since
there are no poles in the region of convergence there are three possibilities
to deﬁne the region of convergence: 1) |z| > 1/2, 2) 1/3 < |z| < 1/2, and 3)
|z| < 1/3. The signals are obtained by using power series expansion for each
case.
1) For the region of convergence |z| > 1/2 the z-transform should be
written in the form
X(z) =
1
1 −1
2z
+
1
−3z(1 −1
3z)
.
Now we have two sums of the geometric series
1
1 −1
2z
=
∞
∑
n=0
* 1
2z
+n
=
∞
∑
n=0
1
2n z−n for
''''
1
2z
'''' < 1 or |z| > 1
2
1
1 −1
3z
=
∞
∑
n=0
* 1
3z
+n
=
∞
∑
n=0
1
3n z−n for
''''
1
3z
'''' < 1 or |z| > 1
3.
Both of these sums converge for |z| > 1/2. The resulting power series expan-
sion of X(z) is
X(z) =
∞
∑
n=0
1
2n z−n −1
3z
∞
∑
n=0
1
3n z−n
=
∞
∑
n=0
1
2n z−n −
∞
∑
n=1
1
3n z−n.
The inverse z-transform, for this region of convergence, is
x(n) = 1
2n u(n) −1
3n u(n −1).

176
z-Transform
2) For 1/3 < |z| < 1/2 the z-transform should be written in the form
X(z) = −2z
1 −2z +
1
−3z(1 −1
3z)
.
The corresponding geometric series are
1
1 −2z =
∞
∑
n=0
(2z)n =
0
∑
n=−∞
2−nz−n for |2z| < 1 or |z| < 1
2
1
1 −1
3z
=
∞
∑
n=0
* 1
3z
+n
=
∞
∑
n=0
1
3n z−n for
''''
1
3z
'''' < 1 or |z| > 1
3.
They converge for 1/3 < |z| < 1/2. The resulting power series expansion is
X(z) = −2z
0
∑
n=−∞
2−nz−n −1
3z
∞
∑
n=0
1
3n z−n
= −
−1
∑
n=−∞
1
2n z−n −
∞
∑
n=1
1
3n z−n.
The inverse z-transform for this region of convergence is
x(n) = −1
2n u(−n −1) −1
3n u(n −1).
3) For |z| < 1/3 we can write
X(z) = −2z
1 −2z +
1
1 −3z.
The corresponding geometric series are
1
1 −2z =
∞
∑
n=0
(2z)n =
0
∑
n=−∞
2−nz−n for |2z| < 1 or |z| < 1
2
1
1 −3z =
∞
∑
n=0
(3z)n =
0
∑
n=−∞
3−nz−n for |3z| < 1 or |z| < 1
3.
Both series converge for |z| < 1/3. The expansion is
X(z) = −2z
0
∑
n=−∞
2−nz−n +
0
∑
n=−∞
3−nz−n
= −
−1
∑
n=−∞
1
2n z−n +
0
∑
n=−∞
1
3n z−n.
The inverse z-transform, in this case, is
x(n) = −1
2n u(−n −1) + 1
3n u(−n).

Ljubiša Stankovi´c
Digital Signal Processing
177
Example 4.5. For the z-transform
X(z) = ea/z
identify the region of convergence and ﬁnd the inverse z-transform.
⋆Expanding ea/z into a complex Taylor (Laurant) series
X(z) = ea/z = 1 + (a/z) + 1
2! (a/z)2 + 1
3! (a/z)3 + ...
follows
x(n) = δ(n) + aδ(n −1) + 1
2! a2δ(n −2) + 1
3! a3δ(n −3)+
= an 1
n! u(n).
The series converges for any z except z = 0.
Example 4.6. For the z-transform
X(z) =
z2 + 1
(z −1/2)(z2 −3z/4 + 1/8)
ﬁnd the signal x(n) if the region of convergence is |z| > 1/2.
⋆The denominator of X(z) will be rewritten in the form
X(z) =
z2 + 1
(z −1/2)(z −z1)(z −z2) =
z2 + 1
(z −1/2)2(z −1/4)
where z1 = 1/2 and z2 = 1/4. Writing X(z) in the form of partial fractions
X(z) =
A
(z −1
2)2 +
B
z −1
2
+
C
z −1
4
the coefﬁcients A, B, and C follow from
(z2 + 1)
(z −1
2)2(z −1
4) = A(z −1
4) + B(z −1
2)(z −1
4) + C(z −1
2)2
(z −1
2)2(z −1
4)
or from
(z2 + 1) = A(z −1
4) + B(z −1
2)(z −1
4) + C(z −1
2)2.
(4.4)
For z = 1/4 we get 17/16 = C/16 or C = 17. Value of z = 1/2 gives
(1
4 + 1) = A(1
2 −1
4)

178
z-Transform
and A = 5 is obtained. Finally if the highest order coefﬁcients in the relation
(4.4) with z2 are equated
z2 = Bz2 + Cz2
we get 1 = B + C, producing B = −16. The z-transform is
X(z) =
5
(z −1
2)2 + −16
z −1
2
+
17
z −1/4.
For the region of convergence |z| > 1/2 and a parameter |a| ≤1/2 holds
1
z −a =
1
z(1 −a
z ) = z−1(1 + az−1 + a2z−2 + ...) =
∞
∑
n=1
an−1z−n.
Differentiating both sides of the previous equation with respect to a we get
d
da (
1
z −a ) =
1
(z −a)2 =
∞
∑
n=2
(n −1)an−2z−n
Using this relation with a = 1/2 the inverse z-transform of X(z) is
x(n) = 5n −1
2n−2 u(n −2) −16
1
2n−1 u(n −1) + 17
1
4n−1 u(n −1).
Note: In general, the relation
1
(z −a)m+1 = 1
m!
dm
dam (
1
z −a ) =
= 1
m!
dm
dam
(
∞
∑
n=1
an−1z−n
)
= (n −1)(n −2)..(n −m)
m!
∞
∑
n=1
an−m−1z−n
produces the inverse z-transform
x(n) = (n −1)(n −2)..(n −m)
m!
an−m−1u(n)
= (n −1)(n −2)..(n −m)
m!
an−m−1u(n −m −1)
=
* n
m
+
an−m−1u(n −m −1).
4.3.2
Theorem of Residues Based Inversion
In general the inversion is calculated by using the Cauchy relation from the
complex analysis
1
2πj
O
C
zm−1dz = δ(m),

Ljubiša Stankovi´c
Digital Signal Processing
179
where C is any closed contour line within the region of convergence. The
complex plane origin is within the contour. By multiplying both sides of
X(z) by zm−1, after integration along the closed contour within the region
of convergence we get
1
2πj
O
C
zm−1X(z)dz =
∞
∑
n=−∞
1
2πj
O
C
zm−1x(n)z−ndz = x(m).
The integral is calculated by using the theorem of residues
x(n) =
1
2πj
O
C
zn−1X(z)dz = ∑
zi
%
1
(k −1)!
d(k−1)[zn−1X(z)(z −zi)k]
dzk−1
|z=zi
;
,
where zi are the poles of zn−1X(z) within the integration contour C that is
in the region of convergence and k is the pole order. If the signal is causal,
n ≥0, and all poles of zn−1X(z) within contour C are simple (ﬁrst-order
poles with k = 1) then, for a given instant n,
x(n) = ∑
zi
M
[zn−1X(z)(z −zi)]|z=zi
N
.
(4.5)
Example 4.7. For the z-transform
X(z) =
2z + 3
(z −1/2)(z −1/4)
ﬁnd a causal signal x(n).
⋆According to the residuum theorem for n ≥1
x(n) = ∑
zi
M
[zn−1X(z)(z −zi)]|z=zi
N
=
zn−1(2z + 3)
(z −1
2)(z −1
4) (z −1
2)|z=1/2 + zn−1(2z + 3)
(z −1
2)(z −1
4) (z −1
4)|z=1/4
=
1
2n−1 4
1
4
+
1
4n−1 7
2
−1
4
= 16
1
2n−1 −14
1
4n−1 .
For n = 0 additional pole at z = 0 exists
x(0) =
z−1(2z + 3)
(z −1
2)(z −1
4)
z|z=0 +
z−1(2z + 3)
(z −1
2)(z −1
4) (z −1
2)|z=1/2
+
z−1(2z + 3)
(z −1
2)(z −1
4) (z −1
4)|z=1/4 = 0.

180
z-Transform
An easy way to get x(0) is x(0) = limz→∞X(z).
The resulting inverse z-transform is
x(n) = 16
1
2n−1 u(n −1) −14
1
4n−1 u(n −1).
It has been assumed that the signal is causal. Using the theorem of
residuum prove that x(n) = 0 for n < 0 with |z| > 1/2.
Hint: Since for each n < 0 there is a pole at z = 0 of the order n + 1, to
avoid different derivatives for each n we can make a substitution of variables
z = 1/p, with dz = −dp/p2. New region of convergence in the complex
plane p will be p < 2. All poles are now outside this region and outside the
integration contour, producing the zero-valued integral.
4.4
DISCRETE SYSTEMS AND THE Z-TRANSFORM
For a linear time-invariant discrete system described by
y(n) = x(n) ∗h(n) =
∞
∑
m=−∞
x(m)h(n −m)
the z-transform is given by
Y(z) = X(z)H(z).
The output signal z-transform is obtained by multiplying the input signal
z-transform by the transfer function
H(z) =
∞
∑
n=−∞
h(n)z−n.
It is possible to relate two important properties of a system with the transfer
function properties.
The system is stable if
∞
∑
m=−∞
|h(m)| < ∞.
It means that the z-transform exists at |z| = 1, i.e., that the circle
|z| = 1

Ljubiša Stankovi´c
Digital Signal Processing
181
b
a
1
Re{z}
Im{z}
a
1
c
Re{z}
Im{z}
b
a
1
Re{z}
Im{z}
-10
0
10
0
20
40
60
h1(n)
-10
0
10
0
0.5
1
1.5
2
h2(n)
-10
0
10
0
1
2
3
4
h3(n)
Figure 4.3
Regions of convergence (gray) with corresponding signals. Poles are denoted by
"x".
belongs to the region of convergence for a stable system.
The system is causal if h(n) = 0 for n < 0. Since H(z) = h(0) +
h(1)z−1 + h(2)z−2 + ... it is obvious that z →∞belongs to the region of
convergence for a causal system.
From the previous two properties we can conclude that a linear time-
invariant system is stable and causal if the unit circle |z| = 1 and z →∞
belong to the region of convergence. Since there are no poles within the
region of convergence one may conclude that a transfer function H(z) may
correspond to a stable and causal system only if all of its poles are inside the
unit circle.
Example 4.8. For the systems whose transfer functions are
H1(z) =
1
(z −1/3)(z −3/2), |z| > 3/2
H2(z) =
1
z(z −1/3)(z −3/2), 1/3 < |z| < 3/2
H3(z) =
1
(z −1/3)(z −3/4), |z| > 3/4
plot the regions of convergence and discuss the stability and causality. Find
and plot the impulse response for each case.

182
z-Transform
⋆The regions of convergence are shown in Fig.4.3. The system de-
scribed by H1(z) is causal but not stable. The system H2(z) is stable but not
causal, while the system H3(z) is both stable and causal. Their impulse re-
sponses are presented in Fig.4.3 as well.
Amplitude of the frequency response (gain) of a discrete system is
related to the transfer function as
|H(ejω)| = |H(z)||z=ejω.
Consider a discrete system whose transfer function assumes the form of a
ratio of two polynomials
H(z)= B0 + B1z−1 +...+ BMz−M
A0 + A1z−1 +...+ ANz−N = B0
A0
zN−M (z −z01)(z −z02)...(z −z0M)
(z −zp1)(z −zp2)...(z −zpN)
where z0i are zeros and zpi are poles of the transfer function. For the
amplitude of frequency response we my write
|H(ejω)| =
''''
B0
A0
''''
TO1 TO2 ...TOM
TP1 TP2 ...TPN
where TOi are the distances from point T at a given frequency z = ejω to
zero Oi at z0i. Distances from point T to poles Pi at zpi are denoted by TPi.
Example 4.9. Plot the frequency response of the causal notch ﬁlter with the transfer
function
H(z) =
z −ejπ/3
z −0.95ejπ/3
⋆The transfer functions calculation is illustrated in Fig.4.4. Its value is
|H(ejω)| = TO1
TP1
where O1 is positioned at z01 = ejπ/3 and pole P1 is at zp1 = 0.95ejπ/3. For
any point T at z = ejω, ω ̸= π/3, the distances TO1 and TP1 from T to O1 and
from T to P1 are almost the same, TO1 ∼= TP1. Then |H(z)||z=ejω ∼= 1 except
at ω = π/3, when TO1 = 0 and TP1 ̸= 0 resulting in |H(z)||z=ejπ/3 = 0. The
frequency response |H(ejω)| is shown in Fig.4.4.

Ljubiša Stankovi´c
Digital Signal Processing
183
ω
π/3
Re{z}
Im{z}
O1
P1
T
-2
0
2
0
0.5
1
1.5
2
|H(ejω)|
ω
π/3
Figure 4.4
Poles and zeros of a ﬁrst-order notch ﬁlter (left). The frequency response of this
notch ﬁlter (right).
4.5
DIFFERENCE EQUATIONS
An important class of discrete systems can be described by difference equa-
tions. They are obtained by converting corresponding differential equations
or by describing an intrinsically discrete system relating the input and out-
put signal in a recursive way. A general form of a linear difference equation
with constant coefﬁcients, that relates the output signal at an instant n with
the input signal x(n) and the previous input and output samples, is
y(n)+A1y(n−1) +...+ ANy(n−N) = B0x(n)+B1x(n−1) +...+ BMx(n−M).
4.5.1
Solution Based on the z-transform
The z-transform of the linear difference equation, assuming zero-valued
initial conditions, is
[1 + A1z−1 + ... + ANz−N]Y(z) = [B0 + B1z−1 + ... + BMz−M]X(z),
since Z{x(n −i)} = X(z)z−i and Z{y(n −k)} = Y(z)z−k. The solution y(n)
of the difference equation is obtained as an inverse z-transform of
Y(z) = B0 + B1z−1 + ... + BMz−M
1 + A1z−1 + ... + ANz−N X(z).

184
z-Transform
Example 4.10. A causal discrete system is described by the difference equation
y(n) −5
6y(n −1) + 1
6y(n −2) = x(n).
(4.6)
If the input signal is x(n) = 1/4nu(n) ﬁnd the output signal.
⋆The z-transform domain form of the system is
Y(z) =
1
1 −1
2z−1 + 1
6z−2 X(z).
The z-transform of the input signal is X(z) = 1/(1 −1
4z−1) for |z| > 1/4. The
output signal z-transform is
Y(z) =
z3
(z −1
2)(z −1
3)(z −1
4)
.
For a causal system the region of convergence is |z| > 1/2. The output signal
is the inverse z-transform of Y(z). For n > 0 it is
y(n) =
∑
zi=1/2,1/3,1/4
M
[zn−1Y(z)(z −zi)]|z=zi
N
=
zn+2
(z −1
3)(z −1
4) |z=1/2
+
zn+2
(z −1
2)(z −1
4) |z=1/3
+
zn+2
(z −1
2)(z −1
3) |z=1/4
= 6 1
2n −8
3n + 3
4n .
For n = 0 there is no pole at z = 0. Thus, the above expressions hold for n = 0
as well. The output signal is
y(n) =
- 6
2n −8
3n + 3
4n
.
u(n).
Note: This kind of solution assumes the initial values from the system causal-
ity and x(n) as y(0) = x(0) = 1 and y(1) −5y(0)/6 = x(1), i.e., y(1) =
13/12.
Example 4.11. A ﬁrst-order causal discrete system is described by the following
difference equation
y(n) + A1y(n −1) = B0x(n) + B1x(n −1).
(4.7)
Find its impulse response and discuss its behavior in terms of the system
coefﬁcients.

Ljubiša Stankovi´c
Digital Signal Processing
185
⋆For the impulse response calculation the input signal is x(n) = δ(n)
with X(z) = 1. Then we have
(1 + A1z−1)Y(z) = (B0 + B1z−1)
Y(z) = B0 + B1z−1
1 + A1z−1 .
The pole of this system is z = −A1. The are two possibilities for the region
of convergence |z| > |A1| and |z| < |A1|. For a causal system the region of
convergence is |z| > |A1|. Thus, the z-transform Y(z) can be expanded into a
geometric series with q = A1z−1 = (A1/z) < 1
Y(z) =
B
B0 + B1z−1CB
1 −A1z−1 + A2
1z−2 −A3
1z−3 + ... + (−A1z−1)n + ...
C
= B0 + B0
∞
∑
n=1
(−A1)nz−n + B1
∞
∑
n=1
(−A1)(n−1)z−n
with
y(n) = B0δ(n) + (−A1)n−1(−A1B0 + B1)u(n −1).
We can conclude that, in general, the impulse response has an inﬁnite dura-
tion for any A1 ̸= 0. It is a result of the recursive relation between the output
y(n) and its previous value(s) y(n −1). This kind of systems are referred to
as inﬁnite impulse response (IIR) systems or recursive systems. If the value
of coefﬁcient A1 is A1 = 0 then there is no recursion and
y(n) = B0δ(n) + B1δ(n −1).
Then we have a system with a ﬁnite impulse response (FIR). This kind of
system produces an output to a signal x(n) as
y(n) = B0x(n) + B1x(n −1).
They are called moving average (MA) systems. Systems without recursion
are always stable since a ﬁnite sum of ﬁnite signal values is always ﬁnite.
Systems that would contain only x(n) and the output recursions, in this
case,
y(n) + A1y(n −1) = B0x(n)
are auto-regressive (AR) systems or all pole systems. This kind of systems
could be unstable, due to recursion. In our case the system is obviously
unstable if |A1| > 1. Systems (4.7) are in general auto-regressive moving
average (ARMA) systems.

186
z-Transform
If the region of convergence were |z| < |A1| then the function Y(z)
would be expanded into series with q = z/A1 < 1 as
Y(z) =
B0 + B1z−1
A1z−1(z/A1 + 1) =
* B0
A1
z + B1
A1
+ ∞
∑
n=0
(−A−1
1 z)n
= B0
0
∑
n=−∞
(−A1)n−1z−(n−1) + B1
A1
0
∑
n=−∞
(−A1)nz−n
= B0
−1
∑
n=−∞
(−A1)nz−n + B1
A1
0
∑
n=−∞
(−A1)nz−n
with
y(n) = B0(−A1)nu(−n −1) + B1
A1
(−A1)nu(−n).
This system would be stable if |1/A1| < 1 and unstable if |1/A1| > 1, having
in mind that y(n) is nonzero for n < 0. This is an anticausal system since it
has impulse response satisfying h(n) = 0 for n ≥1.
Here, we have just introduced the notions. These systems will be
considered in Chapter 5 in details.
4.5.2
Solution of Difference Equations in the Time Domain
A direct way to solve a linear difference equation with constant coefﬁcients
of the form
y(n) + A1y(n −1) + ... + ANy(n −N) = x(n)
(4.8)
in the time domain will be described next.
A homogeneous part of this difference equation is
y(n) + A1y(n −1) + ... + ANy(n −N) = 0.
(4.9)
Solution for the homogeneous equation is of the form
yi(n) = Ciλn
i .
Replacing yi(n) into (4.9), the characteristic polynomial equation follows
Ciλn
i + CiA1λn−1
i
+ ... + CiANλn−N
i
= 0,
or
λN
i + A1λN−1
i
+ ... + AN = 0.
This is a polynomial of the Nth order. In general, it has N solutions λi,
i = 1,2,..., N. All functions yi(n) = λn
i , i = 1,2,..., N are the solutions of

Ljubiša Stankovi´c
Digital Signal Processing
187
equation (4.9). Since the equation is linear, a linear combination of these
solutions,
yh(n) =
N
∑
i=1
Ciλn
i
is also a solution of the homogeneous equation (4.9). This solution is called
homogeneous part of the solution of (4.8).
Next a particular solution yp(n), corresponding to the form of input
signal x(n), should be found using the form of x(n). The solution of equa-
tion (4.8) is then
y(n) = yh(n) + yp(n).
The constants Ci, i = 1,2,..., N are calculated based on initial conditions
y(i −1), i = 1,2,..., N.
Example 4.12. Find the output of a causal discrete system
y(n) −5
6y(n −1) + 1
6y(n −2) = x(n)
(4.10)
to the input signal x(n) = (n + 11/6)u(n) by solving the difference equation
in the discrete-time domain. The initial conditions are y(0) = 1 and y(1) = 5.
⋆Solution of the homogeneous part of (4.10)
y(n) −5
6y(n −1) + 1
6y(n −2) = 0
is of the form yi(n) = Ciλn
i . Its replacement into the equation results in the
characteristic polynomial
λ2
i −5
6λi + 1
6 = 0,
producing λ1 = 1/2 and λ2 = 1/3. The homogeneous part of the solution is
yh(n) = C1
1
2n + C2
1
3n .
Since x(n) is a linear function of n, a particular solution is of the form
yp(n) = An + B. Replacing yp(n) into (4.10) we obtain
yp(n) −5
6yp(n −1) + 1
6yp(n −2) = n + 11/6
An + B −5
6(An −A + B) + 1
6(An −2A + B) = n + 11/6,
and A = 3, B = 1 follow. The solution of (4.10) is a sum of homogeneous and
particular solutions,
y(n) = yh(n) + yp(n) = C1
1
2n + C2
1
3n + 3n + 1.

188
z-Transform
Using the initial conditions
y(0) = C1 + C2 + 1 = 1
y(1) = C1
2 + C2
3 + 4 = 5
the constants C1 = 6 and C2 = −6 follow. The ﬁnal solution is
y(n) =
- 6
2n −6
3n + 3n + 1
.
u(n).
Note: The z-transform based solution would assume y(0) = x(0) =
11/6 and y(1) = 5y(0)/6 + x(1) = 157/36. The solution with the initial
conditions y(0) = 1 and y(1) = 5 could be obtained from this solution with
appropriate changes of the ﬁrst two samples of the input signal in order to
take into account the previous system state and to produce the given initial
conditions y(0) = 1 and y(1) = 5 .
If multiple polynomial roots are obtained, for example λi = λi+1, then
yi(n) = λn
i and yi+1(n) = nλn
i .
Example 4.13. Goertzel algorithm: Show that a discrete-time signal
y(n) = ej(2πk0n/N+ϕ)
is a solution of the homogeneous difference equation
y(n) −ej2πk0/Ny(n −1) = 0.
(4.11)
Consider a periodic signal x(n) with a period N and its DFT values X(k),
x(n) = 1
N
N−1
∑
k=0
X(k)ej2πnk/N.
(4.12)
If the signal within one of its periods, for 0 ≤n ≤N −1, is applied as the
input to the system described by difference equation (4.11) show that the
output signal at n = N −1 is equal to the DFT of signal at frequency k = k0,
i.e.,
y(N −1) = X(k0).
⋆For the signal y(n) holds
y(n) = ej(2πk0n/N+ϕ) = ej(2πk0(n−1+1)/N+ϕ)
= ej(2πk0/N)y(n −1).
Consider now the case when the input signal x(n) is applied to the system.
Since the system is linear, consider one component of the input signal (4.12)
xk(n) = 1
N X(k)ej2πnk/N,

Ljubiša Stankovi´c
Digital Signal Processing
189
for an arbitrary 0 ≤k ≤N −1. Then the difference equation for this input
signal reads
yk(n) −ej2πk0/Nyk(n −1) = xk(n)
Yk(z) =
Z{xk(n)}
1 −ej2πk0/Nz−1 .
(4.13)
The z-transform of xk(n), for 0 ≤n ≤N −1, is
Z{xk(n)} = Z{ 1
N X(k)ej2πnk/N}
(4.14)
= 1
N X(k)
N−1
∑
n=0
ej2πnk/Nz−n = 1
N X(k) 1 −ej2πkz−N
1 −ej2πk/Nz−1 .
The transform Z{xk(n)}, for a given k, has zeros at
zN
0 = ej2πk+j2lπ, l = 0,1,2,...,N −1
or
z0 = ej2π(k+l)/N,l = 0,1,2,..., N −1.
Note that the zero
z0 = ej2πk/N, obtained for l = 0
is canceled with the pole zp = ej2πkn/N in (4.14). Therefore the remaining
zeros are at
z0 = ej2π(k+l)/N, l = 1,2,...,N −1
The output z-transform Yk(z), deﬁned by (4.13), has a pole at
zp = ej2πk0/N
- If k ̸= k0 then one of zeros z0 = ej2π(k+l)/N, l = 1,2,...,N −1 will coincide
with the pole zp = ej2πk0/N and will cancel it. Thus for k ̸= k0 the function
Yk(z) will not have any poles. Then
yk(N −1) =
1
2πj
O
C
zN−2Yk(z)dz = 0
(4.15)
since there are no poles, Fig.4.5.
- If k = k0 then the pole at k = k0 is already canceled in Z{xk(n)}
and zp = ej2πk0/N remains as a pole of Y(z). In this case the signal value
at n = N −1 is equal to the residuum of function in (4.15) at the pole
zp = ej2πk0/N, relation (4.5),
yk0(N −1) = zN−2Yk0(z)(z −ej2πk0/N)
'''
z=ej2πk0/N
= zN−1 1
N X(k0) 1 −ej2πk0z−N
1 −ej2πk0/Nz−1
'''''
z=ej2πk0/N
= 1
N X(k0)
lim
z→ej2πk0/N
zN −ej2πk0
z −ej2πk0/N = X(k0).

190
z-Transform
Re{z}
Im{z}
1/(1-e j2π k
0n/Nz-1 ), k≠ k0
z=ej2πk
0/N
k0≠ k
Re{z}
Im{z}
1/(1-e j2π k
0n/Nz-1 ), k=k0
z=ej2πk/N
k0=k
Re{z}
Im{z}
Z {xk(n)}
z=ej2πk/N
Figure 4.5
Zeros and the pole in Z{xk(n)} (left), the pole in 1/(1 −ej2πk0n/Nz−1) for k ̸= k0
(middle), and the pole in 1/(1 −ej2πk0n/Nz−1) for k = k0 (right). Illustration is for N = 16.
Therefore the output of the system , at n = N −1, is
yk(N −1) = X(k)δ(k −k0).
Note: The difference relation
y(n) −ej2πk0n/Ny(n −1) = x(n)
(4.16)
with the z-transform domain form
Y(z) =
X(z)
1 −ej2πk0n/Nz−1
is often extended to
Y(z) =
X(z)
1 −ej2πk0n/Nz−1
1 −e−j2πk0n/Nz−1
1 −e−j2πk0n/Nz−1
Y(z) =
1 −e−j2πk0n/Nz−1
1 −2cos(2πk0n/N)z−1 + z−2 X(z)
In the discrete-time domain the system
y(n) −2cos(2πk0/N)y(n −1) + y(n −2) = x(n) −e−j2πk0n/Nx(n −1)
(4.17)
is called Goertzel algorithm for the DFT calculation at a given single fre-
quency X(k0).
It is interesting to note that the computation of (4.17) is more efﬁcient
than the computation of (4.16). For the calculation of (4.16), for one k0, we
need one complex multiplication (4 real multiplications) and one complex
addition (2 real additions). For N instants and one k0 we need 4N real
multiplications and 2N real additions. For the calculation of (4.17)we can use

Ljubiša Stankovi´c
Digital Signal Processing
191
linear property and calculate only
y1(n) −2cos(2πk0/N)y1(n −1) + y1(n −2) = x(n)
(4.18)
at each instant. It requires a multiplication of complex signal with a real
coefﬁcient. It means 2 real multiplications for each instant or 2N in total for
N instants. The resulting output, at the instant N −1, is
y(N −1) = T{x(N −1)} −e−j2πk0(N−1)/NT{x(N −1)}
= y1(N) −ej2πk0y1(N −1).
It requires just one additional complex multiplication for the last instant
and for one frequency. The total number of multiplications is 2N + 4. It
is reduced with respect to the previously needed 4N real multiplications.
The total number of additions is 4N + 2. It is increased. However the time
needed for a multiplication is much longer than the time needed for an
addition. Thus, the overall efﬁciency is improved. The efﬁciency is even more
improved having in mind that (4.18) is the same for calculation of X(k0) and
X(−k0) = X(N −k0).
4.6
RELATION OF THE Z-TRANSFORM TO OTHER TRANSFORMS
By sampling a signal x(t), the Laplace transform integral can be approxi-
mated by a sum
X(s) =
∞
"
−∞
x(t)e−stdt ∼=
∞
∑
n=−∞
x(n∆t)e−sn∆t∆t =
∞
∑
n=−∞
x(n)e−sn∆t
with x(n) = x(n∆t)∆t. Comparing this relation with the z-transform deﬁni-
tion we can conclude that the Laplace transform of x(t) corresponds to the
z-transform of its samples with
z = exp(s∆t),
that is,
X(s) ↔X(z)|z=exp(s∆t) .
(4.19)
A point s = σ + jΩfrom the Laplace domain maps into the point
z = rejω with r = eσ∆t and ω = Ω∆t. Points from the left half-plane in the
s domain, σ < 0, map to the interior of unit circle in the z domain, r < 1.

192
z-Transform
According to the sampling theorem, for the Laplace transform of
discrete-time signal holds X(s)|σ=0 = X(jΩ) = X(j(Ω+ 2kπ/∆t)).
The Fourier transform of a discrete-time signal is
X(ejω) = X(z)|z=ejω =
∞
∑
n=−∞
x(n)z−n
|z=ejω .
Example 4.14. A causal discrete-time signal x(n) has the Fourier transform X(ejω).
Write its z-transform in terms of the Fourier transform of the discrete-time
signal, i.e., write the z-transform value based on its values on the unit circle.
⋆The signal can be expressed in term of its Fourier transform as
x(n) = 1
2π
π
"
−π
X(ejω)ejωndω
X(z) =
∞
∑
n=0
x(n)z−n = 1
2π
π
"
−π
X(ejω)
∞
∑
n=0
ejωnz−ndω
= 1
2π
π
"
−π
X(ejω)
1 −ejωz−1 dω,
for |z| > 1.
The DFT of discrete-time signal with N nonzero samples is
X(k) = X(ejω)|ω=2πk/N = X(z)|z=ej2πk/N =
N−1
∑
n=0
x(n)z−n
|z=ej2πk/N .
Example 4.15. Consider a discrete-time signal with N samples different from zero
within 0 ≤n ≤N −1. Show that all values of X(z), for any z, can be calculated
based on its N samples on the unit circle in the z-plane.
⋆If the signal has N nonzero samples, then it can be expressed in term
of its DFT as
X(k) =
N−1
∑
n=0
x(n)e−j2πnk/N and x(n) = 1
N
N−1
∑
k=0
X(k)ej2πnk/N.
Thus, the z-transform of x(n), using only the values of the IDFT where the
original signal is nonzero, 0 ≤n ≤N −1,
X(z) = 1
N
N−1
∑
k=0
N−1
∑
n=0
X(k)ej2πnk/Nz−n = 1
N
N−1
∑
k=0
1 −z−Nej2πk
1 −z−1ej2πk/N X(k)

Ljubiša Stankovi´c
Digital Signal Processing
193
Re{s}=σ
Im{s}=Ω
0
π/Δt
- π/Δt
0
Re{z}
Im{z}
z=ejω
Re{z}
Im{z}
N=16
z=ej2π k/16
Figure 4.6
Illustration of the z-transform relation with the Laplace transform (left), the Fourier
transform of discrete signals (middle), and the DFT (right).
with X(k) = X(z) at z = exp(j2πk/N),k = 0,1,2,..., N −1.
For a periodic signal, including all periods in the z-transform calcula-
tion, holds
X(z) = 1
N
N−1
∑
k=0
∞
∑
n=0
X(k)ej2πnk/Nz−n = 1
N
N−1
∑
k=0
1
1 −z−1ej2πk/N X(k).
4.7
PROBLEMS
Problem 4.1. Find the z-transform and the region of convergence for the
following signals:
(a) x(n) = δ(n −2),
(b) x(n) = a|n|u(n),
(c) x(n) = 1
2n u(n) + 1
3n u(n)
Problem 4.2. Find the z-transform and the region of convergence for the
following signals:
(a) x(n) = δ(n + 1) + δ(n) + δ(n −1),
(b) x(n) = 1
2n [u(n) −u(n −10)].
Problem 4.3. Using the z-transfrom property that
Y(z) = −zdX(z)
dz
corresponds to
y(n) = nx(n)u(n)

194
z-Transform
in the discrete-time domain, with the same region of convergence for X(z)
and Y(z), ﬁnd a causal signal whose z-transform is
(a) X(z) = ea/z, |z| > 0.
(b) X(z) = ln(1 + az−1), |z| > |a|.
Problem 4.4. (a) How the z-transform of x(−n) is related to the z-transform
of x(n)?
(b) If the signal x(n) is real-valued show that its z-transfrom satisﬁes
X(z) = X∗(z∗).
Problem 4.5. If X(z) is the z-transform of a signal x(n) ﬁnd the z-transform
of
y(n) =
∞
∑
k=−∞
x(k)x(n + k).
Problem 4.6. Find the inverse z-transform of
X(z) =
1
2 −3z,
|z| > 2
3.
Problem 4.7. The z-transform of a causal signal x(n) is
X(z) =
z + 1
(2z −1)(3z + 2).
Find the signal x(n).
Problem 4.8. The transfer function of a discrete system is
H(z) =
3 −5
6z−1
(1 −1
4z−1)(1 −1
3z−1)
.
Find the impulse response if:
(a) System is stable,
(b) Region of convergence is 1
4 < |z| < 1
3,
(c) System is anticausal.
Problem 4.9. For the z-transform
H(z) =
1
(1 −4z)( 1
4 −
√
3
2 z + z2)
identify possible regions of convergence. In each case comment stability and
causality of the system whose transfer function is H(z). What is the output
of the stable system to the input x(n) = 2cos(nπ/2)?

Ljubiša Stankovi´c
Digital Signal Processing
195
Problem 4.10. Find the impulse response of a causal system whose transfer
function is
H(z) =
z + 2
(z −2)z2 .
Problem 4.11. Find the inverse z-transform of
X(z) =
z2
z2 + 1.
Problem 4.12. The system is described by a difference equation
y(n) −y(n−1) + 5
16y(n−2) −1
16y(n−3) = 3x(n) −5
4x(n−1) + 3
16x(n−2).
Find the impulse response of a causal system.
Problem 4.13. Show that the system deﬁned by
y(n) = x(n) −3
4x(n −1) + 1
8x(n −2)
has a ﬁnite output duration for an inﬁnite duration input x(n) = 1/4nu(n) .
Problem 4.14. A linear time-invariant system has impulse response
h(n) = 1/3nu(n).
Using the z-transform ﬁnd the output to the input signal x(n) = u(n) −
u(n −6) .
Problem 4.15. Find the output of a causal discrete system
y(n) −11
6 y(n −1) + 1
2y(n −2) = 2x(n) −3
2x(n −1)
if the input signal is x(n) = δ(n) −3
2δ(n −1).
Problem 4.16. Solve the difference equation using the z-transform
x(n + 2) + 3x(n + 1) + 2x(n) = 0
with the initial condition x(0) = 0 and x(1) = 1. Signal x(n) is causal.

196
z-Transform
Problem 4.17. Solve the difference equation
x(n + 1) = x(n) + an
using the z-transform with the initial condition x(0) = 0.
Problem 4.18. Find the output of a causal discrete system
y(n) −
√
2
2 y(n −1) + 1
4y(n −2) = x(n)
(4.20)
to the input signal x(n) =
1
3n u(n) by a direct solution of the differential
equation in the discrete-time domain and by using the z-transform. The
initial conditions are y(n) = 0 for n < 0.
Problem 4.19. The ﬁrst backward difference is deﬁned as
∇x(n) = x(n) −x(n −1),
and the mth backward difference is deﬁned by
∇mx(n) = ∇m−1x(n) −∇m−1x(n −1).
The ﬁrst forward difference is
∆x(n) = x(n + 1) −x(n),
with the mth forward difference being
∆mx(n) = ∆m−1x(n + 1) −∆m−1x(n).
Find the z-transforms of these differences.
Problem 4.20. Based on the poles-zero geometry plot the amplitude of the
frequency response of system
y(n) = x(n) −
√
2x(n −1) + x(n −2) + r
√
2y(n −1) −r2y(n −2)
for r = 0.99. Based on the frequency response, ﬁnd approximative values of
the output signal if the input is a continuous-time signal
x(t) = 2cos(10πt) −sin(15πt) + 0.5ej20πt
sampled at ∆t = 1/60.

Ljubiša Stankovi´c
Digital Signal Processing
197
Problem 4.21. Plot the frequency response of the discrete system (comb
ﬁlter)
H(z) = 1 −z−N
1 −rz−N
with r = 0.9999 and r1/N ∼= 1. Show that this system has the same transfer
function as
H(z) = (1 −z−2)
(1 −r2z−2)
N/2−1
∏
k=1
1 −2cos(2kπ/N)z−1 + z−2
1 −2rcos(2kπ/N)z−1 + z−2 .
4.8
SOLUTIONS
Solution 4.1. (a) The z-transform is
X(z) =
∞
∑
n=−∞
δ(n −2)z−n = z−2
for any z ̸= 0.
(b) For this signal
X(z) =
∞
∑
n=−∞
a|n|z−n =
−1
∑
n=−∞
a−nz−n +
∞
∑
n=0
anz−n =
(1 −a2)z
(1 −az)(z −a)
for |z| < 1/a and |z| > a. If |a| < 1 then the region of convergence is
a < |z| < 1/a.
(c) In this case
X(z) =
∞
∑
n=0
1
2n z−n +
∞
∑
n=0
1
3n z−n =
1
1 −1
2z−1 +
1
1 −1
3z−1
X(z) =
2 −5
6z−1
(1 −1
2z−1)(1 −1
3z−1) =
z(2z −5
6)
(z −1
2)(z −1
3)
for |z| > 1/2 and |z| > 1/3. The region of convergence is |z| > 1/2.
Solution 4.2. (a) The z-transform is
X(z) =
∞
∑
n=−∞
(δ(n + 1) + δ(n) + δ(n −1))z−n =
= z + 1 + z−1 = z + 1 + 1
z.

198
z-Transform
Re{z}
Im{z}
pole-zero cancellation at z=1/2
z=ej2π/10/2
z=1/2
Figure 4.7
Pole-zero cancellation at z = 1/2.
The region of convergence excludes z = 0 and z −→∞.
(b) For x(n) = 1
2n [u(n) −u(n −10)] we know that
u(n) −u(n −10) =
!
1, n = 0,1,...,9
0, elsewhere.
The z-transform is
X(z) =
∞
∑
n=−∞
x(n)z−n =
9
∑
n=0
1
2n z−n =
9
∑
n=0
(2z)−n = 1 −(2z)−10
1 −(2z)−1 =
= z−10
z−1
z10 −( 1
2)10
z −1
2
= z10 −( 1
2)10
z9(z −1
2)
The expression for X(z) is written in this way in order to ﬁnd the region of
convergence, observing the zero-pole locations in the z-plane, Fig.4.7. Poles
are at zp1 = 0 and zp2 = 1/2. Zeros are z0i = ej2iπ/10/2, Fig.4.7. Since the z-
transform has a zero at z0 = 1/2, it will cancel out the pole zp2 = 1/2. The
resulting region of convergence will include the whole z plane, except the
point at z = 0.
Solution 4.3. (a) For X(z) = ea/z holds
−zdX(z)
dz
= z a
z2 ea/z = a
z X(z)

Ljubiša Stankovi´c
Digital Signal Processing
199
The inverse z-transform of left and right side of this equation is
nx(n)u(n) = ax(n −1)u(n)
since Z[nx(n)] = −z dX(z)
dz
and z−1X(z) = Z[x(n −1)]. It means that
x(n) = a
n x(n −1)
for n > 0. According to the initial value theorem
x(0) = lim
z→∞X(z) = 1.
It means that
x(1) = a, x(2) = a2
2 , x(3) = a3
2 · 3,...
or
x(n) = an
n! u(n).
(b) For X(z) = ln(1 + az−1)
Y(z) = −zdX(z)
dz
= −zd(ln(1 + az−1))
dz
= z
az−2
1 + az−1 =
az−1
1 + az−1 .
Therefore
Z[nx(n)] = −zdX(z)
dz
=
az−1
1 + az−1
nx(n) = a(−a)n−1u(n −1),
producing
x(n) = −(−a)n
n
u(n −1).
Solution 4.4. (a) The z-transform of signal x(−n) is
X1(z) =
∞
∑
n=−∞
x(−n)z−n.
With a substitution −n = m it follows
X1(z) =
∞
∑
m=−∞
x(n)zm = X(1/z).

200
z-Transform
The region of convergence is complementary to the one of the original
signal. If the region of convergence for x(n) is |z| > a, then the region of
convergence for x(−n) is |z| < a .
(b) For a real-valued signal holds x∗(n) = x(n). Then we can write
X∗(z∗) as
X∗(z∗) =
∞
∑
n=−∞
x∗(n)
P(z∗)−nQ∗.
Since (z∗)−n = (z−n)∗we get
X∗(z∗) =
∞
∑
n=−∞
x∗(n)z−n =
∞
∑
n=−∞
x(n)z−n = X(z),
for a real-valued signal x(n).
Solution 4.5. From
Y(z) =
∞
∑
n=−∞
y(n)z−n =
∞
∑
n=−∞
∞
∑
k=−∞
x(k)x(n + k)z−n,
using the substitution n + k = m, follows
Y(z) = X(z)X(1
z ).
Solution 4.6. A direct expansion of the given transform into power series,
within the region of convergence, will be used. In order to ﬁnd the signal
x(n) whose z-transform is X(z) =
1
2−3z, it should be written in a form of
power series with respect to z−1. Since the condition
'' 3z
2
'' < 1 does not
correspond to the region of convergence given in the problem formulation
we have to rewrite X(z) as
X(z) = −1
3z
1
1 −2
3z
.
Now the condition
'' 2
3z
'' < 1, that is |z| > 2
3, corresponds to the problem for-
mulation region of convergence. In order to obtain the inverse z-transform,
write
X(z) = −1
3z
1
1 −2
3z
= −1
3z X1(z),
where
X1(z) =
1
1 −2
3z
.

Ljubiša Stankovi´c
Digital Signal Processing
201
For X1(z) holds
X1(z) =
∞
∑
n=0
* 2
3z
+n
=
∞
∑
n=0
*2
3
+n
z−n.
It can be concluded that X(z) can be written as
X(z) = −1
3z
∞
∑
n=0
*2
3
+n
z−n.
Comparing the z-transform deﬁnition
X(z) =
∞
∑
n=−∞
x(n)z−n
(4.21)
and the last expression it follows
X(z) = −1
3
∞
∑
n=0
*2
3
+n
z−nz−1 =
= −1
3
∞
∑
n=0
*2
3
+n
z−(n+1).
With the substitution n →n + 1 we get
X(z) = −1
3
∞
∑
n=1
*2
3
+n−1
z−n.
Finally, comparing this result with (4.21) we get
x(n) =
⎧
⎨
⎩
−1
3
P 2
3
Qn−1 , for n = 1,2,...,∞
0
,
elsewhere,
,
or
x(n) = −1
3
*2
3
+n−1
u(n −1).
Solution 4.7. Since the signal is causal the region of convergence is outside
the pole with the largest radius (outside the circle passing through this pole).

202
z-Transform
Poles of the z-transform are
zp1 = 1
2
and zp2 = −2
3.
The region of convergence is |z| > 2
3.
The z-transform is
X(z) =
z + 1
(2z −1)(3z + 2) =
A
2z −1 +
B
3z + 2
A = 3
7,
B = −1
7.
The terms in X(z) should be written in such a way that they represent sums
of geometric series for the given region of convergence. From the solution
of the previous problem, we conclude that
X(z) = A
2z
1
1 −1
2z
+ B
3z
1
1 + 2
3z
.
Now we can write
A
2z
1
1 −1
2z
= A
2z
∞
∑
n=0
*1
2
+n
z−n = A
2
∞
∑
n=0
*1
2
+n
z−n−1,
|z| > 1
2
and
B
3z
1
1 + 2
3z
= B
3z
∞
∑
n=0
*
−2
3
+n
z−n = B
3
∞
∑
n=0
*
−2
3
+n
z−n−1,
|z| > 2
3.
The z-transform, with m = n + 1, assumes the form
X(z) = A
2
∞
∑
m=1
*1
2
+m−1
z−m + B
3
∞
∑
m=1
*
−2
3
+m−1
z−m.
Replacing the values for A and B it follows
X(z) = 3
7
∞
∑
m=1
*1
2
+m
z−m + 1
14
∞
∑
m=1
*
−2
3
+m
z−m.
The signal x(n) is obtained by comparing this transform with the z-
transform deﬁnition,
x(n) =
*3
7
*1
2
+n
+ 1
14
*
−2
3
+n+
u(n −1).

Ljubiša Stankovi´c
Digital Signal Processing
203
Solution 4.8. The transfer function may be written as
H(z) =
3 −5
6z−1
(1 −1
4z−1)(1 −1
3z−1) =
A
1 −1
4z−1 +
B
1 −1
3z−1
with A = 1,
B = 2.
(a) The region of convergence must contain |z| = 1, for a stable system. It is
|z| > 1
3.
From
H(z) =
1
1 −1
4z−1 +
2
1 −1
3z−1 =
=
∞
∑
n=0
*1
4
+n
z−n + 2
∞
∑
n=0
*1
3
+n
z−n,
|z| > 1
3 and |z| > 1
4
the impulse response is obtained as
h(n) = (4−n + 2 × 3−n)u(n).
(b) The region of convergence is 1
4 < |z| < 1
3. The ﬁrst term in H(z) is the
same as in (a), since it converges for |z| > 1
4. It corresponds to the signal
4−nu(n). The second term must be rewritten in such a way that its geometric
series converges for |z| < 1
3. Then
2
1 −1
3z−1 = −2
3z
1 −3z = −2
∞
∑
n=1
(3z)n
=
m=−n −2
−1
∑
m=−∞
(3z)−m with |z| < 1
3.
Signal corresponding to this z-transform is −2 × 3−nu(−n −1). Then the
impulse response of the system with the region of convergence 1
4 < |z| < 1
3
is obtained in the form
h(n) = 4−nu(n) −2 × 3−nu(−n −1).
c) For an anticausal system the region of convergence is |z| < 1
4. Now the
second term in H(z) is the same as in (b). For |z| < 1
4 the ﬁrst term in H(z)
should be written as:
1
1 −1
4z−1 = −
4z
1 −4z = −
∞
∑
n=1
(4z)n
=
m=−n −
−1
∑
m=−∞
(4z)−m with |z| < 1
4.

204
z-Transform
The signal corresponding to this term is −4−nu(−n −1). The impulse
response of the anticausal discrete system with given transfer function is
h(n) = −4−nu(−n −1) −2 × 3−nu(−n −1).
Solution 4.9. The z-transform
H(z) =
1
(1 −4z)( 1
4 −
√
3
2 z + z2)
can be written as
H(z) =
1
(1 −4z)(z −
√
3
4 + j 1
4)(z −
√
3
4 −j 1
4)
with poles z1 = 1/4, z2 =
√
3
4 −j 1
4, and z3 =
√
3
4 + j 1
4. Since |z2| = |z3| = 1/2
possible regions of convergence are: 1) |z| < 1/4, 2) 1/4 < |z| < 1/2, and 3)
|z| > 1/2. In the ﬁrst two cases the system is neither causal nor stable, while
in the third case the system is causal and stable since |z| = 1 and |z| →∞
belong to the region of convergence.
The output to x(n) = 2cos(nπ/2) = 1 + cos(nπ) = 1 + (−1)n is y(n) =
H(ejω)|ω=0 × 1 + H(ejω)|ω=π × (−1)n = H(z)|z=1 + H(z)|z=−1 (−1)n =
−0.8681 + 0.0945(−1)n.
Solution 4.10. The transfer function can be written as
H(z) =
z + 2
z2(z −2) =
A
z −2 + B
z + C
z2 .
Multiplying both sides by z2(z −2) yields
Az2 + Bz(z −2) + C(z −2) = z + 2
(A + B)z2 + (−2B + C) −2C = z + 2.
The coefﬁcients follow from
A + B = 0
−2B + C = 1
−2C = 2,
as A = 1, B = −1, and C = −1. The transfer function is
H(z) =
z−1
1 −2z−1 −1
z2 −1
z.

Ljubiša Stankovi´c
Digital Signal Processing
205
The region of convergence for a causal system is |z| > 2. The inverse z-
transform for a causal system is the system impulse response
h(n) = 2n−1u(n −1) −δ(n −2) −δ(n −1) = δ(n −2) + 2n−1u(n −3).
The system is not stable.
Solution 4.11. The z-transform X(z) can be written in the form
X(z) =
z2
z2 + 1 =
1
2z
z + j +
1
2z
z −j.
For the region of convergence deﬁned by |z| > 1 the signal is causal and
x(n) = 1
2[1 + (−1)n]jnu(n) = 1
2[1 + (−1)n]ejπn/2u(n).
For n = 4k, where k ≥0 is an integer, x(n) = 1 , while for n = 4k + 2 the
signal values are x(n) = −1. For other n the signal is x(n) = 0.
For |z| < 1 the inverse z-transform is
x(n) = −1
2[1 + (−1)n]jnu(−n −1).
Solution 4.12. The transfer function of this system is
H(z) =
3 −5
4z−1 + 3
16z−2
1 −z−1 + 5
16z−2 −1
32z−3 =
3 −5
4z−1 + 3
16z−2
(1 −1
2z−1 + 1
16z−2)(1 −1
2z−1)
=
1
1 −1
4z−1 +
1
B
1 −1
4z−1
C2 +
1
(1 −1
2z−1)
.
For a causal system the region of convergence is outside of the pole z = 1/2,
that is |z| > 1/2. Since
1
B
1 −1
4z−1
C2 = d
da
*
z
1 −az−1
+''''
a=1/4
= d
da
∞
∑
n=0
anz−(n−1)
'''''
a=1/4
=
∞
∑
n=0
nan−1z−(n−1)
'''''
a=1/4
=
∞
∑
n=0
(n + 1) 1
4n z−n,
the inverse z-transform is
h(n) = 1
4n u(n) + (n + 1) 1
4n u(n) + 1
2n u(n).

206
z-Transform
Solution 4.13. The transfer function of the system deﬁned by
y(n) = x(n) −3
4x(n −1) + 1
8x(n −2)
is
H(z) = 1 −3
4z−1 + 1
8z−2.
The z-transform of the input signal x(n) = 1/4nu(n) is
X(z) =
1
1 −1
4z−1 ,
with the region of convergence |z| > 1/4. The output signal z-transform is
Y(z) = H(z)X(z) = (1 −1
2z−1)(1 −1
4z−1)
(1 −1
4z−1)
= 1 −1
2z−1.
Its inverse is a ﬁnite duration output signal
y(n) = δ(n) −δ(n −1)/2.
Solution 4.14. The system transfer function is
H(z) =
1
1 −1
3z−1
and the input signal z-transform is
X(z) = 1 + z−1 + z−2 + z−3 + z−4 + z−5 = 1 −z−6
1 −z−1 .
The z-transform of the output signal is
Y(z) =
1 −z−6
(1 −z−1)(1 −1/3z−1) = Y1(z) −Y1(z)z−6
with
Y1(z) =
1
(1 −z−1)(1 −1/3z−1) =
3/2
1 −z−1 −
1/2
1 −1
3z−1 .
Its inverse is
y1(n) =
-3
2 −1
2
*1
3
+n.
u(n).

Ljubiša Stankovi´c
Digital Signal Processing
207
1/3
Re{z}
Im{z}
3/2
1/3
3/2
Re{z}
Im{z}
3/2
1/3
Re{z}
Im{z}
Figure 4.8
Poles and zeros of the system (left), input signal z-transform (middle), and the
z-transform of the output signal (right).
Thus the system output is
y(n) =
-3
2 −1
2
*1
3
+n.
u(n) −
>
3
2 −1
2
*1
3
+n−6?
u(n −6).
Solution 4.15. The transfer function is obtained from
Y(z)(1 −11
6 z−1 + 1
2z−2) = X(z)(2 −3
2z−1)
as
H(z) =
2 −3
2z−1
1 −11
6 z−1 + 1
2z−2 .
The poles are at zp1 = 1/3 and zp2 = 3/2 with the region of convergence
|z| > 3/2. It means that the system is not stable, Fig.4.8.
The z-transform of the input signal is
X(z) = 1 −3
2z−1 for |z| > 0.
The output signal transform is
Y(z) =
2 −3
2z−1
1 −11
6 z−1 + 1
2z−2
*
1 −3
2z−1
+
= 2 −3
2z−1
1 −1
3z−1 .
The output signal transform does not have a pole z = 3/2 since this pole is
canceled out. The output signal is
y(n) = 1
3n u(n) −3
2
1
3n−1 u(n −1).

208
z-Transform
Solution 4.16. The z-transform of signal x(n + 2) is
X2(z) = z2X(z) −z2x(0) −zx(1)
while for x(n + 1) the transform is
X1(z) = zX(z) −zx(0).
The z-transform domain form is
z2X(z) −z2x(0) −zx(1) + 3zX(z) −3zx(0) + 2X(z) = 0
with
X(z) =
z
z2 + 3z + 2 =
1
1 + z−1 −
1
1 + 2z−1 .
The inverse z-transform of X(z) is
x(n) = [(−1)n −(−2)n]u(n).
Solution 4.17. The z-transforms of the left and right side of the equation are
zX(z) −zx(0) = X(z) +
z
z −a
X(z) =
z
(z −a)(z −1) =
1
1 −a
-
1
z −1 −
a
z −a
.
.
The inverse z-transform is
x(n) =
1
1 −a [u(n −1) −anu(n −1)] = 1 −an
1 −a u(n −1)
or
x(n) =
n−1
∑
k=0
ak, n > 0.
Solution 4.18. For a direct solution in the discrete-time domain we assume
a solution of the homogenous part of the equation
y(n) −
√
2
2 y(n −1) + 1
4y(n −2) = 0
(4.22)
in the form yi(n) = Ciλn
i . The characteristic polynomial is
λ2 −
√
2
2 λ + 1
4 = 0

Ljubiša Stankovi´c
Digital Signal Processing
209
with λ1,2 =
√
2
4 ± j
√
2
4 . The homogenous solution is
yh(n) = C1(
√
2
4 + j
√
2
4 )n + C2(
√
2
4 −j
√
2
4 )n
= C1
1
2n ejnπ/4 + C2
1
2n e−jnπ/4.
A particular solution is of the input signal x(n) = 1
3n u(n) form. It is yp(n) =
A 1
3n u(n). The constant A is obtained by replacing this signal into (4.20)
A 1
3n −
√
2
2 A
1
3n−1 + 1
4 A
1
3n−2 = 1
3n
A(1 −3
√
2
2
+ 9
4) = 1.
Its value is A = 0.886. The general solution is
y(n) = yh(n) + yp(n) = C1
1
2n ejnπ/4 + C2
1
2n e−jnπ/4 + 0.886 1
3n .
Since the system is causal with y(n) = 0 for n < 0 then the constants C1
and C2 may be obtained from the initial condition following from y(n) −
√
2
2 y(n −1) + 1
4y(n −2) = x(n) as y(0) = x(0) = 1 and y(1) =
√
2
2 y(0) +
x(1) =
√
2
2 + 1
3,
C1 + C2 + 0.886 = 1
(4.23)
C1(
√
2
2 + j
√
2
2 )/2 + C2(
√
2
2 −j
√
2
2 )/2 + 0.8861
3 =
√
2
2 + 1
3,
as C1 = 0.057 −j0.9967 = 0.9984exp(−j1.5137) = C∗
2. The ﬁnal solution is
y(n) = 2 × 0.9984 1
2n cos(nπ/4 −1.5137) + 0.886 1
3n .
For the z-domain we write
Y(z) −
√
2
2 Y(z)z−1 + 1
4Y(z)z−2 = X(z)
with
Y(z) =
1
1 −
√
2
2 z−1 + 1
4z−2
1
1 −1
3z−1

210
z-Transform
with
Y(z) =
z3
(z −(
√
2
4 + j
√
2
4 ))(z −(
√
2
4 −j
√
2
4 ))(z −1
3)
Using, for example, the residual value based inversion of the z-transform,
y(n) =
∑
z1,2,3=
√
2
4 ±j
√
2
4 ,1/3
M
[zn−1Y(z)(z −zi)]|z=zi
N
= zn+2
1
(z −
√
2−j
√
2
4
)(z −1
3)
'''''' √
2+j
√
2
4
+ zn+2
1
(z −
√
2+j
√
2
4
)(z −1
3)
''''''
z=
√
2−j
√
2
4
+zn+2
1
(z −
√
2+j
√
2
4
)(z −
√
2−j
√
2
4
)
''''''
z=1/3
=
1
j
√
2
2
(√
2 + j
√
2
4
)n+2
1
√
2+j
√
2
4
−1
3
−
1
j
√
2
2
(√
2 −j
√
2
4
)n+2
1
√
2−j
√
2
4
−1
3
+
1
3n+2
1
( 1
9 −1
3
√
2
2 + 1
4)
=
1
2n+2 ej(n+2)π/4
−j
√
2
√
2+j
√
2
4
−1
3
+
1
2n+2 e−j(n+2)π/4
j
√
2
√
2−j
√
2
4
−1
3
+ 0.886 1
3n
= 1
2n ejnπ/4
√
2
√
2 + j
√
2 −4
3
+ 1
2n e−jnπ/4
√
2
√
2 −j
√
2 −4
3
+ 0.886 1
3n
= 2 × 0.9984 1
2n cos(nπ/4 −1.5137) + 0.886 1
3n ,
for n ≥1. For n = 0 there is no additional pole at z = 0 the previous result
holds for n ≥0.
Solution 4.19. The z-transform of the ﬁrst backward difference is
Z[∇x(n)] = Z[x(n)] −Z[x(n −1)] = (1 −z−1)X(z).
The second backward difference may be written as
∇2x(n) = ∇[∇x(n)] = ∇[x(n) −x(n −1)] = ∇x(n) −∇x(n −1)
= x(n) −2x(n −1) + x(n −2).

Ljubiša Stankovi´c
Digital Signal Processing
211
Its z-transform is
Z[∇2x(n)] = (1 −z−1)2X(z).
In the same way we get
Z[∇mx(n)] = (1 −z−1)mX(z).
The z-transform of the ﬁrst forward difference is
Z[∆x(n)] = Z[x(n + 1) −x(n)] = zX(z) −zx(0) −X(z)
= (z −1)X(z) −zx(0).
The second forward difference is
Z[∆2x(n)] = x(n + 2) −2x(n + 1) + x(n)
with the z-transform
Z[∆2x(n)] = (z −1)2X(z) −z(z −1)x(0) −z∆x(0).
In a recursive way, the z-transform of the mth forward difference is
Z[∆mx(n)] = (z −1)mX(z) −z
m−1
∑
j=0
(z −1)m−j−1∆jx(0).
Solution 4.20. The transfer function of this system is
H(z) =
1 −
√
2z−1 + z−2
1 −r
√
2z−1 + r2z−2 = [1 −(
√
2
2 + j
√
2
2 )z−1][1 −(
√
2
2 −j
√
2
2 )z−1]
[1 −r(
√
2
2 + j
√
2
2 )z−1][1 −r(
√
2
2 −j
√
2
2 )z−1]
= [z −(
√
2
2 + j
√
2
2 )][z −(
√
2
2 −j
√
2
2 )]
[z −r(
√
2
2 + j
√
2
2 )][z −r(
√
2
2 −j
√
2
2 )]
The zeros and poles are z01,02 =
√
2
2 ± j
√
2
2 and zp1,p2 = r
√
2
2 ± jr
√
2
2 .They are
located as in Fig.4.9.
The amplitude of the frequency response is
|H(ejω)| =
''''
B0
A0
''''
TO1 TO2
TP1 TP2
= TO1 TO2
TP1 TP2
.
The values of TP1 and TO1, and TP2 and TO2, are almost the same for
any ω except ω = ±π/4 where the distance to the transfer function zero is

212
z-Transform
O1
P1
T
P2
O2
Re{z}
Im{z}
-2
0
2
0
0.5
1
1.5
2
|H(ejω)|
ω
- π/4
π/4
Figure 4.9
Location of zeros and poles for a second order system.
0, while the distance to the corresponding pole is small but ﬁnite. Based on
this analysis the amplitude of frequency response is presented in Fig.4.9.
The input discrete-time signal is
x(n) = x(n∆t)n∆t = [2cos(πn/6) −sin(πn/4) + 0.5ejπn/3]/60.
This system will ﬁlter out signal components at ω = ±π/4. The output
discrete-time signal is
y(n) = [2cos(nπ/6) + 0.5ejnπ/3]/60.
Corresponding continuous-time output signal is
y(t) = 2cos(10πt) + 0.5ej20πt.
Solution 4.21. The zeros of the system are
z−N
o
= 1 = e−j2πm
zom = ej2πm/N, m = 0,1,..., N −1
Similarly, the poles are zmp = r1/Nej2πm/N, m = 0,1,..., N −1. The frequency
response of the comb ﬁlter is
H(z) =
N−1
∏
m=0
z −zom
z −zpm
=
N−1
∏
m=0
z −ej2πm/N
z −r1/Nej2πm/N .

Ljubiša Stankovi´c
Digital Signal Processing
213
With r = 0.9999 and r1/N ∼= 1 follows
|H(ejω)| ∼= 1 for z ̸= ej2πm/N
|H(ejω)| = 0 for z = ej2πm/N.
The same holds for
H(z) = (1 −z−1)(1 + z−1)
(1 −rz−1)(1 + rz−1)
N/2−1
∏
k=1
1 −2cos(2kπ/N)z−1 + z−2
1 −2rcos(2kπ/N)z−1 + r2z−2
since for 1 ≤k ≤N/2 −1 we can group the terms
(1 −e2kπ/Nz−1)(1 −e2(N−k)π/Nz−1)
(1 −re2kπ/Nz−1)(1 −re2(N−k)π/Nz−1) =
1 −2cos(2kπ/N)z−1 + z−2
1 −2rcos(2kπ/N)z−1 + r2z−2 .
4.9
EXERCISE
Exercise 4.1. Find the z-transform and the region of convergence for the
following signals:
(a) x(n) = δ(n −3) −δ(n + 3),
(b) x(n) = u(n) −u(n −20) + 3δ(n),
(c) x(n) = 1/3|n| + 1/2nu(n),
(d) x(n) = 3nu(−n) + 2−nu(n),
(e) x(n) = n(1/3)nu(n).
(f) x(n) = cos(n π
2 ).
Exercise 4.2. Find the z-transform and the region of convergence for the
signals:
(a) x(n) = 3nu(n) −(−2)nu(n) + n2u(n).
(b) x(n) = ∑n
k=0 2k3n−k,
(c) x(n) = ∑n
k=0 3k.
Exercise 4.3. Find the inverse z-transform of:
(a) X(z) = z−8
1−z + 3, if X(z) is the z-transform of a causal signal x(n).
(b) X(z) =
z+2
(z−2)z2 , if X(z) is the z-transform of a causal signal x(n).
(c) X(z) = 6z2+3z−2
6z2−5z+1, if X(z) is the z-transform of an unlimited-duration
signal x(n). Find ∑∞
n=−∞x(n) in this case.
Exercise 4.4. Find the inverse z-transforms of:
(a) X(z) =
z5(5z−3)
(3z−1)(2z−4), if x(n) is causal,

214
z-Transform
(b) Y(z) = X( z
2), for a causal signal y(n),
(c) Y(z) = z−2X(z), for a causal signal y(n).
Exercise 4.5. Find the inverse z-transforms of X(z) = cosh(az) and X(z) =
sinh(az).
Exercise 4.6. If X(z) is the z-transform of a signal x(n), with the region of
convergence |z| > 1
2, ﬁnd the z-transforms for the following signals:
(a) y(n) = x(n) −x(n −1),
(b) y(n) =
∞
∑
k=−∞
x(n −kN), where N is an integer,
(c) y(n) = x(n) ∗x(−n), where ∗denotes convolution.
(d) ﬁnd the signal whose z-transform is Y(z) = d
dz X(z).
Exercise 4.7. If X(z) is the z-transform of a signal x(n) ﬁnd the z-transform
of
y(n) =
∞
∑
k=−∞
x∗(n −k)x(n + k).
Exercise 4.8. For the z-transform
H(z) =
(2 −z)
(1 −4z)(1 −3z)
identify possible regions of convergence and ﬁnd the inverse z-transform
for each of them. For each case comment stability and causality. What is the
output of the stable system to x(n) = 1 + (−1)n?
Exercise 4.9. Find the output of a causal discrete system
y(n) −3
4y(n −1) + 1
8y(n −2) = x(n).
(4.24)
to the input signal x(n) = nu(n) by:
(a) a direct solution in the time domain.
(b) using the z-transform.
The initial conditions are y(n) = 0 for n < 0, that is y(0) = x(0) = 0 and
y(1) = 3y(0)/4 + x(1) = 1.
Exercise 4.10. A causal discrete system is described by the difference equa-
tion
y(n) −5
6y(n −1) + 1
6y(n −2) = x(n).
(4.25)
If the input signal is x(n) = 1/4nu(n) ﬁnd the output signal if the initial
value of the output was y(0) = 2.

Ljubiša Stankovi´c
Digital Signal Processing
215
Hint: Since y(0) does not follow from (4.25) obviously the system
output was "preloaded" before the input is applied. This fact can be taken
into account by changing the input signal at n = 0 to produce the initial
output. It is x(n) = 1/4nu(n) + δ(n). Now the initial conditions are y(0) = 2
and y(1) = 5/3 + 1/4 = 23/12 and we can apply the z-transform with this
new input signal.
Exercise 4.11. Solve the difference equation using the z-transform
x(n + 2) −1
2x(n + 1) + x(n) = 0
with initial condition x(0) = 0 and x(1) = 1/2. The signal x(n) is causal.
Exercise 4.12. Using the basic trigonometric transformations show that a
real-valued signal y(n) = cos(2πk0n/N + ϕ) is a solution of the homoge-
neous difference equation
y(n) −2cos(2πk0/N)y(n −1) + y(n −2) = 0.
with similar conclusions as in the complex-valued signal case.
Exercise 4.13. For the system
H(z) = (1 −z−1)(1 + z−1)
(1 −rz−1)(1 + rz−1)
3
∏
k=1
1 −2cos(2kπ/8)z−1 + z−2
1 −2rcos(2kπ/8)z−1 + z−2
and r = 0.9999 plot the amplitude of the frequency response and ﬁnd the
output to the signal
x(n) = cos(nπ/3 + π/4) + sin(nπ/2) + (−1)n.

216
z-Transform

Chapter 5
From Continuous to Discrete Systems
T
RANSFORMATION of continuous-time systems into corresponding
discrete-time systems is of high importance. Some discrete-time sys-
tems are designed and realized in order to replace or perform as
equivalents of continuous-time systems. It is quite common to design a
continuous-time system with desired properties, since the designing pro-
cedures in this domain are simpler and well developed. In the next step
the obtained continuous-time system is transformed into an appropriate
discrete-time system.
Consider an Nth order linear continuous-time system described by a
differential equation with constant coefﬁcients
aN
dNy(t)
dtN
+ ... + a1
dy(t)
dt
+ a0y(t) = bM
dMx(t)
dtn
+ ... + b1
dx(t)
dt
+ b0x(t).
The Laplace transform domain equation for this system is
[aNsN + ... + a1s + a0]Y(s) = [bMsM + ... + b1s + b0]X(s),
assuming zero-valued initial conditions. The topic of this chapter is to ﬁnd
a corresponding discrete-time system, described by
A0y(n) + A1y(n −1) + ... + ANy(n −N)
= B0x(n) + B1x(n −1) + ... + BMx(n −M).
The z-transform domain form of this system is
[A0 + A1z−1 + ... + ANz−N]Y(z) = [B0 + B1z−1 + ... + BMz−M]X(z).
There are several approaches to establish a relation between continuous-
time and discrete-time systems represented by their impulse responses or
transfer functions.
217

218
From Continuous to Discrete Systems
t
h(t)
Δt
n
h(n) = h(t) Δt t = nΔt
Figure 5.1
Sampling of the impulse response for the impulse invariance method.
5.1
IMPULSE INVARIANCE METHOD
A natural approach to transform a continuous-time system into a discrete-
time system is based on the relation between the impulse responses of these
systems. Assume that the impulse response of the continuous-time system is
hc(t). The impulse response h(n) of the corresponding discrete-time system,
according to this approach, is equal to the samples of hc(t),
h(n) = hc(n∆t)∆t.
Obviously this relation can be used only if the sampling theorem is satisﬁed
for the sampling interval ∆t. It means that the frequency response of the
continuous-time system satisﬁes the condition
H(Ω) = FT{hc(t)} = 0
for
|Ω| > Ωm
and ∆t < π/Ωm. Otherwise the discrete-time version will not correspond to
the continuous-time version of the frequency response. Here, the discrete-
time system frequency response is related to a periodically extended form
of the continuous-time system frequency response H(Ω) as
∞
∑
k=−∞
H(Ω+ 2kπ/∆t) = H(ejω),
Ω= ω/∆t.
Transfer function of the continuous-time system may be written as
H(s) = aNsN + ... + a1s + a0
bMsM + ... + b1s + b0
=
k1
s −s1
+
k2
s −s2
+ ··· +
kM
s −sM
,
(5.1)

Ljubiša Stankovi´c
Digital Signal Processing
219
where only simple poles of the transfer function are assumed. The case of
multiple poles will be discussed later. The inverse Laplace transform of a
causal system, described by the previous transfer function, is
hc(t) = k1es1tu(t) + k2es2tu(t) + ··· + kMesMtu(t).
The impulse response of the corresponding discrete-time system is equal to
the the samples of hc(t),
h(n) = hc(n∆t)∆t = [k1∆tes1n∆tu(n) + k2∆tes2n∆tu(n) + ...+ kM∆tesMn∆tu(n)],
since u(n∆t) = u(n). The z-transform of the impulse response h(n) of the
discrete-time system is
H(z) =
k1∆t
1 −es1∆tz−1 +
k2∆t
1 −es2∆tz−1 + ··· +
kM∆t
1 −esM∆tz−1 .
(5.2)
By comparing (5.1) and (5.2) it can be concluded that the terms in the
transfer functions are transformed from the continuous-time to the discrete-
time case as
ki
s −si
→
ki∆t
1 −esi∆tz−1 .
(5.3)
If a multiple pole, of an (m + 1)th order, exists in the continuous-time
system transfer function then it holds
ki
(s −si)m+1 = 1
m!
dm
dsm
i
ki
s −si
.
A term in the discrete-time system, corresponding to this continuous-time
system term, is
1
m!
dm
dsm
i
ki
s −si
→1
m!
dm
dsm
i
!
ki∆t
1 −esi∆tz−1
6
.
(5.4)
In the impulse invariance method the poles are mapped according to
si →esi∆t.
This mapping relation does not hold for zeros, Fig.5.2.
In the case when the continuous-time impulse response hc(t) has a
discontinuity at t = 0, i.e., when hc(t)|t=−0 ̸= hc(t)|t=+0 then the previous

220
From Continuous to Discrete Systems
-j2 π/Δt
-j π/Δt
jπ/Δt
j2π/Δt
Re{s}
Im{s}
s=jΩ
1
z=ejω
Re{z}
Im{z}
Figure 5.2
Illustration of the impulse invariance method mapping.
forms assume that the discrete-time impulse response h(n) = hc(t)|t=+0. Re-
mind that the theory of Fourier transforms in this case states that the inverse
Fourier transform IFT{H(jΩ)} = hc(t) where the signal hc(t) is continuous
and IFT{H(jΩ)} =
P
hc(t)|t=−0 + hc(t)|t=+0
Q
/2 at the discontinuity points,
in this case at t = 0. The special case of discontinuity at t = 0 can be easily
detected by mapping H(s) into H(z) and by checking, for a causal system,
is the following relation satisﬁed
0 = hc(t)|t=−0 = hc(t)|t=+0 = h(n)|n=0 = lim
z→∞H(z).
If limz→∞H(z) ̸= 0 then a discontinuity existed and we should use
h(0) = lim
z→∞H(z)/2
since hc(t)|t=−0 = 0 and hc(t)|t=+0 ∆t = limz→∞H(z). The resulting fre-
quency response is
H(z) −lim
z→∞H(z)/2.
Example 5.1. A continuous-time system has a transfer function of the form
H(s) =
s + 3
2
s2 + 3
2s + 1
2
.
What is the corresponding discrete-time system according to the impulse
invariance method with ∆t = 1?

Ljubiša Stankovi´c
Digital Signal Processing
221
⋆The transfer function should be written as
H(s) =
s + 3
2
(s + 1)(s + 1
2) =
k1
s + 1 +
k2
s + 1
2
with
k1 = H(s)(s + 1)|s=−1 = −1,
k2 = H(s)(s + 1
2)
''''
s=−1/2
= 2.
Thus, we get
H(s) = −1
s + 1 +
2
s + 1
2
.
According to (5.3) the discrete-time system is
H(z) =
−1
1 −e−1z−1 +
2
1 −e−1/2z−1 .
Since limz→∞H(z) = 1 obviously there is a discontinuity in the impulse
response and the resulting transfer function should be corrected as
H(z) =
−1
1 −e−1z−1 +
2
1 −e−1/2z−1 −1/2.
Impulse and frequency responses of the systems with uncorrected and cor-
rected discontinuity effect are presented in Fig.5.3.
Example 5.2. A continuous-time system has a transfer function of the form
H(s) =
(1 −3s/2)
(6s2 + 5s + 1)(s + 1)2 .
What is the corresponding discrete-time system according to the impulse
invariance method with ∆t = 1?
⋆The transfer function should be written as
H(s) =
1 −3s/2
6(s + 1
2)(s + 1
3)(s + 1)2
=
k1
s + 1
2
+
k2
s + 1
3
+
k3
(s + 1)2 +
k4
s + 1
with
k1 = H(s)(s + 1/2)|s=−1/2 = −7,
k2 = 27/8,

222
From Continuous to Discrete Systems
-5
0
5
10
15
0
0.5
1
1.5  hc(t)
 h(n)
-5
0
5
10
15
0
0.5
1
1.5  hc(t)
 h(n)
-2
0
2
0
1
2
3
4
|H(jΩ)|
|H(ejω)|
-2
0
2
0
1
2
3
4
|H(jΩ)|
|H(ejω)|
Figure 5.3
Impulse responses of systems in continuous and discrete-time domains (top). Am-
plitude of the frequency response of systems in continuous and discrete-time domains (bot-
tom). System without discontinuity correction (left) and system with discontinuity correction
(right).
k3 = H(s)(s + 1)2'''
s=−1 = 5/4.
The coefﬁcient k4 follows, for example, from
H(0) = 1 = 2k1 + 3k2 + k3 + k4,
as
k4 = 29/8.
Thus, we get
H(s) =
−7
s + 1
2
+ 27/8
s + 1
3
+
5/4
(s + 1)2 + 29/8
s + 1.
According to (5.3) and (5.4) the discrete-time system is
H(z) =
−7
1 −e−1/2z−1 +
27/8
1 −e−1/3z−1
+ d
dsi
{
5/4
1 −esiz−1 }
''''
si=−1
+
29/8
1 −e−1z−1
=
−7z
z −e−1/2 +
27z/8
z −e−1/3 + 5e−1z/4
(z −e−1)2 + 29z/8
z −e−1 .

Ljubiša Stankovi´c
Digital Signal Processing
223
-1
2/3
2/3
Re{s}
Im{s}
s=jΩ
1
z=ejω
Re{z}
Im{z}
1.9894
Figure 5.4
Pole-zero locations in the s-domain and the z-domain using the impulse invariance
method.
Since h(0) = limz→∞H(z) = 0 there no need to consider possible impulse
response correction due to discontinuity.
Writing the transfer function in the form
H(z) = −
0.0341z(z −1.9894)(z + 0.3259)
(z −0.7165)(z −0.6065)(z −0.3679)2
we can easily see that the poles are mapped according to spi →espi∆t, Fig.5.4,
while there is no direct correspondence among zeros of the transfer functions.
Impulse responses of continuous-time system and discrete-time system are
presented in Fig.5.5.
5.2
MATCHED Z-TRANSFORM METHOD
The matched z-transform method is based on a discrete-time approximation
of the Laplace transform derived in the previous chapter as
X(s) =
∞
"
−∞
x(t)e−stdt ∼=
∞
∑
n=−∞
x(n)e−sn∆t = X(z)|z=es∆t .
This approximation leads to a relation between the Laplace domain and the
z-domain in the form of
z = es∆t.

224
From Continuous to Discrete Systems
0
5
10
15
20
25
30
35
40
-0.1
0
0.1
0.2
0.3
 hc(t),  h(n)
-3
-2
-1
0
1
2
3
0
0.5
1
 |H(jΩ)|,  |H(ejω)|
-3
-2
-1
0
1
2
3
10
-2
10
-1
10
0
10
1
 20log|H(jΩ)|
 20log|H(ejω)|
Figure 5.5
Impulse responses of systems in continuous and discrete-time domains (top).
Amplitude of the frequency response of systems in continuous and discrete-time domains
(middle). Amplitude of the frequency response of systems in continuous and discrete-time
domains in logarithmic scale (bottom).
If we use this relation to map all zeros and poles of a continuous system
transfer function
H(s) = bMsM + ... + b1s + b0
aNsN + ... + a1s + a0
= bM(s −s01)(s −s02)...(s −s0M)
aN(s −sp1)(s −sp2)...(s −spN)
into the corresponding z-plane locations
z0i = es0i∆t
zpi = espi∆t,

Ljubiša Stankovi´c
Digital Signal Processing
225
-j2 π/Δt
-j π/Δt
jπ/Δt
j2π/Δt
Re{s}
Im{s}
s=jΩ
1
z=ejω
Re{z}
Im{z}
Figure 5.6
Illustration of the zeros and poles mapping in the matched z−transform method.
the matched z-transform method of the system follows. The discrete-time
system transfer function is
H(z) = C (z −es01∆t)(z −es02∆t)...(z −es0M∆t)
(z −esp1∆t)(z −esp2∆t)...(z −espN∆t)
.
Constant C follows from the amplitude condition. For example, it can be
calculated from H(s)|s=0 = H(z)|z=1 .
Example 5.3. For the continuous-time system with a transfer function of the form
H(s) =
1 −s
8s2 + 6s + 1
ﬁnd the corresponding discrete-time system according to the matched z-
transform method and ∆t = 1?
⋆The transfer function of discrete-time system is obtained from
H(s) =
1 −s
8(s + 1
2)(s + 1
4)
,
using the mapping z0i = es0i∆t and zpi = espi∆t, as
H(z) = k
z −e
8(z −e−1/2)(z −e−1/4).
Since H(s)|s=0 = 1 if we want that H(z)|z=ej0 = 1 then k = −1/2.4678 =
−0.4052.

226
From Continuous to Discrete Systems
5.3
DIFFERENTIATION AND INTEGRATION
The ﬁrst-order backward difference is a common method to approximate
the ﬁrst-order derivative of a continuous-time signal
y(t) = dx(t)
dt
y(n∆t) ∼= x(n∆t) −x((n −1)∆t)
∆t
.
The Laplace transform domain of the continuous-time ﬁrst derivative is
Y(s) = sX(s).
(5.5)
In the discrete-time domain, with y(n) = y(n∆t)∆t and x(n) = x(n∆t)∆t,
this derivative approximation results in the ﬁrst-order linear difference
equation
y(n) = x(n) −x(n −1)
∆t
.
In the z-transform domain this equation is
Y(z) = 1 −z−1
∆t
X(z).
(5.6)
Based on (5.5) and (5.6) we can conclude that a mapping of the correspond-
ing differentiation operators from the continuous-time to the discrete-time
domain is
s = 1 −z−1
∆t
.
(5.7)
With a normalized discretization step ∆t = 1 this mapping is of the form
s = 1 −z−1.
The same result could be obtained by considering a rectangular rule
approximation of a continuous-time integral
y(t) =
t
"
−∞
x(t)dt ∼=
t−∆t
"
−∞
x(t)dt + x(n∆t)∆t.
At an instant t = n∆t, the value of integral can be approximated as
y(n∆t) ∼= y(n∆t −∆t) + x(n∆t)∆t.

Ljubiša Stankovi´c
Digital Signal Processing
227
In the discrete-time domain this relation reads
y(n) = y(n −1) + x(n)∆t.
The Laplace and the z-transform domain forms of the previous integral
equations are
Y(s) = 1
s X(s)
Y(z) =
∆t
1 −z−1 X(z).
The same mapping of the z-plane to the s-plane as in (5.7) follows.
Consider the imaginary axis from the s-plane (the Fourier transform
line). According to (5.7) the mapping, with ∆t = 1, is deﬁned by
1 −s →z−1.
(5.8)
Now we will consider the region that corresponds to the imaginary axis and
the left semi-plane of the s-domain (containing poles of a stable system),
Fig.5.7(left). The aim is to ﬁnd the corresponding region in the z-domain.
If we start from the s-domain and the region in Fig.5.7(left), the ﬁrst
mapping is to reverse the s-domain to −s and shift it for +1, as
1 −s →p.
The corresponding domain, after this mapping, is shown in Fig.5.7(middle).
The next step is to map the region from p-domain into the z-domain,
according to (5.8), as
p →z−1.
By denoting Re{z} = x and Im{z} = y we get that the line Re{p} = 1
in the p−domain, corresponding to the imaginary axis in the s-plane, is
transformed into the z-domain according to
Re{p} = Re{1
z }
1 = Re{
1
x + jy}
1 = Re{
1
x + jy
x −jy
x −jy}

228
From Continuous to Discrete Systems
Re{s}
Im{s}
s=0+jΩ
1-s → p
Re{p}
Im{p}
p=1
p→ z-1
1
z=ejω
Re{z}
Im{z}
Figure 5.7
Illustration of the differentiation based mapping of the left s−semi-plane with the
imaginary axis (left), translated and reversed p−domain (middle), and the z−domain (right).
resulting in
1 =
x
x2 + y2
or in
(x −1
2)2 + y2 =
*1
2
+2
.
(5.9)
Therefore, the imaginary axis in the s-plane is mapped onto a circle deﬁned
by (5.9), Fig.5.7(right) in the z-plane. From the mapping relation 1 −s →z−1
it is easy to conclude that the origin s = 0 + j0 maps into z = 1 and that
s = 0 ± j∞maps into z = ±0, according to 1/(1 −s) →z.
Mapping of the imaginary axis into z-domain can also be analyzed
from
σ + jΩ→1 −(rejω)−1
∆t
= 1 −r−1 cosω
∆t
+ jr−1
∆t sinω.
For σ = 0 follows
1 −r−1 cosω = 0
(5.10)
r = cosω,
with
Ω= r−1
∆t sinω = tanω
∆t .
Obviously ω = 0 maps to Ω= 0 (with Ω∼= ω/∆t for small ω), and ω =
±π/2 maps into Ω→±∞.
Thus, the whole imaginary axis maps onto
−π/2 ≤ω ≤π/2. These values of ω could be used within the basic period.
Relation (5.10), with −π/2 ≤ω ≤π/2, is a circle deﬁned by (5.9) if we

Ljubiša Stankovi´c
Digital Signal Processing
229
replace r =
,
x2 + y2 and cosω = x/
,
x2 + y2 with σ < 0 (semi-plane with
negative real values) being mapped into r < cosω (interior of unit circle).
Example 5.4. A continuous-time system is described by a differential equation
y′′(t) + 3
4y′(t) + 1
8y(t) = x(t),
with zero initial conditions and the transfer function
H(s) =
1
s2 + 3
4s + 1
8
.
What is the corresponding transfer function of a discrete-time system using
the ﬁrst-order backward difference approximation with ∆t = 1/2? What is
the solution of the differential equation for x(t) = u(t). Compare it with the
solution of difference equation y(n) with ∆t = 1/8.
⋆A discrete-time system transfer function is obtained by replacing
s =
P
1 −z−1Q
/∆t in H(s) as
H(z) =
1
B
1−z−1
∆t
C2
+ 3
4
1−z−1
∆t
+ 1
8
=
(∆t)2
1 + 3
4∆t + 1
8 (∆t)2 −[2 + 3
4∆t]z−1 + z−2
with
y(n) = B0x(n) + A1y(n −1) + A2y(n −2)
B0 =
(∆t)2
1 + 3
4∆t + 1
8 (∆t)2 = 0.1778
A1 =
[2 + 3
4∆t]
1 + 3
4∆t + 1
8 (∆t)2 = 1.6889
A2 = −
1
1 + 3
4∆t + 1
8 (∆t)2 = −0.7111,
where ∆t = 1/2. For x(t) = u(t) in the continuous-time case
Y(s) = H(s)X(s) =
1
s(s2 + 3
4s + 1
8)
= 8
s +
8
s + 1
2
−
16
s + 1
4
with
y(t) = [8 + 8e−t/2 −16e−t/4]u(t).
The results of the difference equation for y(n) are compared with the exact
solution y(t) in Fig.5.8. The agreement is high. It could be additionally
improved by reducing the sampling interval, for example, to ∆t = 1/8.

230
From Continuous to Discrete Systems
0
5
10
15
0
2
4
6
8
10
 y(t),  y(n)
Figure 5.8
Exact solution of the difference equation y(t) in solid line and the discrete-time
system output y(n) in large dots for ∆t = 1/2 and in small dots for ∆t = 1/8..
5.4
BILINEAR TRANSFORM
In the case of a differentiator based mapping the imaginary axis in the
s−domain, corresponding to the Fourier transform values, has been mapped
onto a circle with radius 1/2 and the center at z = 1/2 in the z−domain. It
does not correspond to the Fourier transform of discrete-time signals posi-
tion in the z−plane, that is along |z| = 1. A transformation that will map the
imaginary axis from the s−domain onto the unit circle in the z−domain is
presented next.
Consider numerical integration by using the trapezoid rule
y(t) =
t
"
−∞
x(t)dt ∼=
t−∆t
"
−∞
x(t)dt + x(n∆t) + x((n −1)∆t)
2
∆t
y(n) = y(n −1) + x(n) + x(n −1)
2
∆t.
In the Laplace and the z-transform domain, these relations have the forms
Y(s) = 1
s X(s)
Y(z) = ∆t
2
1 + z−1
1 −z−1 X(z).

Ljubiša Stankovi´c
Digital Signal Processing
231
Mapping from the s−domain to the z−domain follows as
s →2
∆t
1 −z−1
1 + z−1 .
(5.11)
In the complex analysis this mapping is known as a bilinear transform.
Within the derivatives framework the bilinear transform can be under-
stood as the following derivative approximation. Consider the ﬁrst-order
backward derivative approximation
y(n) = x(n) −x(n −1).
The same signal samples can used for the ﬁrst-order forward derivative
approximation
y(n −1) = x(n) −x(n −1).
If we assume that the difference x(n) −x(n −1) ﬁts better to the mean
of y(n) and y(n −1) than to any single one of them, then the derivative
approximation by using the difference equation
y(n) + y(n −1)
2
= x(n) −x(n −1),
produces the bilinear transform.
In order to prove that the imaginary axis in the s−domain corresponds
to the unit circle in the z−domain we may simply replace z = ejω into (5.11)
and obtain
21 −e−jω
1 + e−jω = 2ejω/2 −e−jω/2
ejω/2 + e−jω/2 = 2jtan(ω
2 ) →s∆t.
For s = σ + jΩfollows
σ = 0
Ω= 2
∆t tan(ω
2 ).
Therefore, the unit circle z = ejω maps onto the imaginary axis σ = 0.
The frequency points ω = 0 and ω = ±π map into Ω= 0 and Ω→±∞,
respectively. The linearity of frequency mapping Ω→ω is lost. It holds for
small values of ω only
Ω= 2
∆t tan(ω
2 ) ∼= ω
∆t, for |ω| ≪1.

232
From Continuous to Discrete Systems
From
z = 1 + s∆t
2
1 −s∆t
2
|z| =
F
(1 + σ∆t
2 )2 + ( Ω∆t
2 )2
F
(1 −σ∆t
2 )2 + ( Ω∆t
2 )2
it may easily be concluded that σ < 0 maps into |z| < 1, since 1 + σ∆t
2
<
1 −σ∆t
2
for σ < 0.
The bilinear transform mapping can be derived by using a series of
complex plane mappings. Since
z = 1 + s∆t
2
1 −s∆t
2
=
2
1 −s∆t
2
−1,
we can write
1 −s∆t
2
→p1,
1
p1
→p2,
2p2 −1 →z.
This series of mappings from the s-domain to the z-domain is illustrated in
Fig.5.9, with ∆t = 1. The fact that 1
p1 →p2 maps the line Re{p1} = 1 into the
circle (x −1
2)2 + y2 =
B
1
2
C2
in p2-domain is proven in the previous section.
Since the bilinear transform introduces a nonlinear transformation
of the frequency axis from the continuous-time domain to the discrete-
time domain, Ω=
2
∆t tan( ω
2 ), in system design it is necessary to take into
account this nonlinearity. Usually it is done by pre-modifying the projected
important frequency values from the analog domain Ωc →2
∆t tan( ωd
2 ), and
ωd →Ωd∆t. The frequencies Ωd will, after the bilinear transformation, be
returned to the desired values Ωc = ωc/∆t.
Example 5.5. A continuous-time system
H(s) =
2QΩ1
s2 + 2Ω1Qs + Ω2
1 + Q2 +
2QΩ2
s2 + 2Ω2Qs + Ω2
2 + Q2
is designed to pass the signal
x(t) = A1 cos(Ω1t + ϕ1) + A2 cos(Ω2t + ϕ2).

Ljubiša Stankovi´c
Digital Signal Processing
233
Re{s}
Im{s}
s=0+jΩ
Re{p1}
Im{p1}
p=1
p1
-1 → p2
1
2p2-1 → z
Re{p2}
Im{p2}
1
z=ejω
Re{z}
Im{z}
Figure 5.9
Bilinear mapping illustration trough a series of elementary complex plane map-
pings.
and to stop all other possible signal components. The parameters are Q =
0.01, Ω1 = π/4, and Ω2 = 3π/5. The signal is sampled with ∆t = 1 and
the discrete-time signal x(n) is formed. Using the bilinear transform, design
the discrete system that corresponds to the continuous-time system with the
transfer function H(s).
⋆For the beginning just use the bilinear transform relation
s →21 −z−1
1 + z−1
(5.12)
and map H(s) to HB(z) without any pre-modiﬁcation. The result is presented
in the ﬁrst two subplots of Fig.5.10. The discrete frequencies are shifted since
the bilinear transform (5.12) made a nonlinear frequency mapping from the

234
From Continuous to Discrete Systems
continuous-time to discrete-time domain, according to
Ω= 2tan( ω
2 ).
Thus, obviously, the system HB(z) is not a system that will ﬁlter the corre-
sponding frequencies in x(n) in the same way as H(s) ﬁlters x(t).
In order to correct the shift introduced by the bilinear transform map-
ping the continuous-time system should be pre-modiﬁed as
Hd(s) =
2QΩ1d
s2 + 2Ω1dQs + Ω2
1d + Q2 +
2QΩ2d
s2 + 2Ω2dQs + Ω2
2d + Q2
with
Ω1d = 2
∆t tan( Ω1∆t
2
) = 0.8284 = 0.2637π
Ω2d = 2
∆t tan( Ω2∆t
2
) = 2.7528 = 0.8762π.
We see that the shift of Ω1 = 0.25π to Ω1d = 0.2637π is small since the bilin-
ear transform frequency mapping for small frequency values is almost linear.
However for Ω2 = 0.6π the shift to Ω2d = 0.8762π is signiﬁcant due to a high
nonlinearity of mapping in that region. The modiﬁed system Hd(s) is pre-
sented in subplot 3 of Fig.5.10. Next, by using the bilinear transform mapping
s →2 1−z−1
1+z−1 the modiﬁed frequencies will map to the desired ones ω1 = Ω1∆t
and ω2 = Ω2∆t. The obtained discrete-time system transfer function
H(z) =
2QΩ1d
B
2 1−z−1
1+z−1
C2
+ 4Ω1dQ 1−z−1
1+z−1 + Ω2
1d + Q2
+
+
2QΩ2d
B
2 1−z−1
1+z−1
C2
+ 4Ω2dQ 1−z−1
1+z−1 + Ω2
2d + Q2
=
0.016569
B
2 1−z−1
1+z−1
C2
+ 0.0331375 1−z−1
1+z−1 + 0.68641
+
0.0551
B
2 1−z−1
1+z−1
C2
+ 0.1101 1−z−1
1+z−1 + 7.5778
=
0.016569(1 + z−1)2
4.65327z−2 −6.6272z−1 + 4.7195
+
0.0551(1 + z−1)2
11.4677z−2 + 7.1556z−1 + 11.6879
=
0.003567(1 + z−1)2
(z−1 −1.0071ej0.25π)(z−1 −1.0071e−j0.25π)
+
0.0048(1 + z−1)2
(z−1 −1.0096ej0.6π)(z−1 −1.0096e−j0.6π)

Ljubiša Stankovi´c
Digital Signal Processing
235
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
H(s)
 Ω1
 Ω2
 s → 2(1-z -1 )/(1+z-1 )
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
HB(z)
frequency ω/π or ΩΔt/π
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
Hd(s)
modified  H(s)
 Ω1d=2tan(Ω1Δt/2)/Δt
 Ω2d=2tan(Ω2Δt/2)/Δt
 s → 2(1-z -1 )/(1+z-1 )
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
H(z)
frequency ω/π or ΩΔt/π
 ω1=Ω1Δt
 ω2=Ω2Δt
Figure 5.10
Amplitude of the continuous-time system with transfer function H(s) and the
amplitude of the transfer function HB(z) of the discrete-time system obtained by the bilinear
transform (ﬁrst two subplots). A premodiﬁed system to take into account the frequency map-
ping nonlinearity in the bilinear transform Hd(s) and the amplitude of the transfer function
H(z) of the discrete-time system obtained by the bilinear transform of Hd(s) (last two subplots).
is shown in subplot 4 of Fig.5.10.
This is the desired discrete-time system corresponding to the continuous-
time system in subplot 1 of this ﬁgure. In calculations the coefﬁcients are
rounded to four decimal places.
Comparison of the mapping methods presented in this section is
summarized in the next table.

236
From Continuous to Discrete Systems
Method
Fourier transform
H(s)|s=jΩ→H(z)|z=ejω
Sampling
theorem
condition
Impulse Invariance
Yes, Ω= ω/∆t
Yes
Matched z-transform
No
No
First-oder difference
No
No
Bilinear transform
Yes, Ω= tan(ω/2)
∆t/2
No
5.5
DISCRETE FILTERS DESIGN
5.5.1
Lowpass ﬁlters
An ideal discrete lowpass ﬁlter is deﬁned by the frequency response
H(ejω) =
!
1
for |ω| < ωc
0
for ωc < |ω| < π .
The frequency response is periodic in ω with period 2π.
The implementation of an ideal lowpass ﬁlter in the DFT domain is
obvious, by multiplying all DFT coefﬁcients corresponding to ωc < |ω| < π
by zero. In on-line implementations in the discrete-time domain the ideal
ﬁlter should be approximated by a corresponding transfer function form
that can be implemented, since the impulse response h(n) = 2sin(ωcn)/n
of the ideal ﬁlter is noncausal signal with a slow convergence.
There are several methods to approximate the ideal lowpass ﬁlter
frequency response. One of them is the Butterworth approximation. Some of
commonly used approximations are Chebyshev and elliptic forms as well.
A lowpass ﬁlter of the Butterworth type is shown in Fig.5.11, along
with the ideal one.
ω
|H(ejω)|2
- π
π
-2 π
2π
0
ω
|H(ejω)|2
- π
π
-2 π
ωc
- ωc
2π
1
Figure 5.11
Lowpass ﬁlter frequency response: ideal case (left) and Butterworth type (right).

Ljubiša Stankovi´c
Digital Signal Processing
237
Example 5.6. Implement a Butterworth discrete ﬁlter of order N = 4 with a critical
frequency corresponding to the continuous domain ﬁlter with the critical
frequency fc = 4[kHz] and the sampling interval ∆t = 31.25[µsec]. Using:
(a) The impulse invariance method and
(b) the bilinear transform.
⋆Note that the discrete-time frequency is ωc = Ωc∆t = 2π fc∆t = π/4.
The poles of the fourth order Butterworth ﬁlter in the continuous-time
domain (Chapter I, Subsection 1.6) are
s0 = Ωc
@
cos( π
2 + π
8 ) + jsin( π
2 + π
8 )
A
= Ωc(−0.3827 + j0.9239)
s1 = Ωc
-
cos( π
2 + 3π
8 ) + jsin( π
2 + 3π
8 )
.
= Ωc(−0.9239 + j0.3827)
s2 = Ωc
-
cos( π
2 + 5π
8 ) + jsin( π
2 + 5π
8 )
.
= Ωc(−0.9239 −j0.3827)
s3 = Ωc
-
cos( π
2 + 7π
8 ) + jsin( π
2 + 7π
8 )
.
= Ωc(−0.3827 −j0.9239).
The transfer function is
H(s) =
Ω4c
(s2 + 0.7654Ωcs + Ω2c)(s2 + 1.8478Ωcs + Ω2c).
(5.13)
(a) For the impulse invariance method the transfer function (5.13) should be
written in the form
H(s) =
k0
s −s0
+
k1
s −s1
+
k2
s −s2
+
k3
s −s3
,
with
ki = H(s)(s −si)|s=si
k0 = (−0.3628 + j0.1503)/∆t,
k1 = (0.3628 −j0.8758)/∆t,
k2 = (0.3628 + j0.8758)/∆t,
k3 = (−0.3628 −j0.1503)/∆t.
Using the impulse invariance method we get the transfer function of the
discrete-time fourth order Butterworth ﬁlter
H(z) =
k0∆t
1 −es0∆tz−1 +
k1∆t
1 −es1∆tz−1 +
k2∆t
1 −es2∆tz−1 +
k3∆t
1 −es3∆tz−1
=
−0.3628 + j0.1503
1 −eωc(−0.3827+j0.9239)z−1 +
0.3628 −j0.8758
1 −eωc(−0.9239+j0.3827)z−1
+
0.3628 + j0.8758
1 −eωc(−0.9239−j0.3827)z−1 +
−0.3628 −j0.1503
1 −eωc(−0.3827−j0.9239)z−1 .

238
From Continuous to Discrete Systems
It can be seen that the discrete-time ﬁlter is a function of ωc. Thus, for a
given continuous domain frequency and sampling interval, it is possible to
calculate ωc = Ωc∆t and to to use this frequency in the continuous ﬁlter
design with normalized ∆t = 1. Replacing the value for ωc = π/4 we get
H(z) =
−0.3628 + j0.1503
1 −(0.5539 + j0.4913)z−1 +
−0.3628 −j0.1503
1 −(0.5539 −j0.4913)z−1
+
0.3628 −j0.8758
1 −(0.4623 + j0.1433)z−1 +
0.3628 + j0.8758
1 −(0.4623 −j0.1433)z−1 .
Grouping the complex-conjugate terms, a system form with real-valued co-
efﬁcients is obtained
H(z) =
−0.7256 + 0.2542z−1
1 −1.1078z−1 + 0.5482z−2 +
0.7256 −0.084z−1
1 −0.9246z−1 + 0.2343z−2 .
(b) For the bilinear transform the critical frequency ωc has to be pre-
modiﬁed according to
Ωd = 2
∆t tan( ωc
2 ) = 0.8284
∆t
.
Then the frequency Ωd is used for the design in (5.13) instead Ωc. It will be
transformed back to Ωc = ωc/∆t after the bilinear transform is used. Using
substitutions s →2
∆t
1−z−1
1+z−1 and
ωd = Ωd∆t = 0.8284
in (5.13) the transfer function follows as
H(z) =
ωd4
[4( 1−z−1
1+z−1 )2 + 2ωd0.7654 1−z−1
1+z−1 + ω2
d][4( 1−z−1
1+z−1 )2 + 2ωd1.8478 1−z−1
1+z−1 + ω2
d]
=
0.4710
[4( 1−z−1
1+z−1 )2 + 1.2626 1−z−1
1+z−1 + 0.6863][4( 1−z−1
1+z−1 )2 + 3.0481 1−z−1
1+z−1 + 0.6863]
=
0.4710
P
1 + z−1Q4
P
3.4237z−2 −6.6274z−1 + 5.9484
QP
1.6382z−2 −6.6274z−1 + 7.7704
Q
=
0.084
P
1 + z−1Q4
P
z−2 −1.9357z−1 + 1.7343
QP
z−2 −4.0455z−1 + 4.7433
Q
= 0.084z−4 + 0.336z−3 + 0.504z−2 + 0.336z−1 + 0.084
z−4 −5.9810z−3 + 14.3z−2 −16.1977z−1 + 8.2263 .
The transfer function (amplitude and phase) of the continuous-time
ﬁlter and the discrete-time ﬁlters obtained by using the impulse invariance
method and the bilinear transform are presented in Fig.5.12, within one

Ljubiša Stankovi´c
Digital Signal Processing
239
0
0.5
1
1.5
ω
|H(ejω|
- π
- π/2
π/2
π
0
ω
arg{H(ejω}
- π
- π/2
π/2
π
0
- π
0
π
Figure 5.12
Amplitude and phase of the fourth order Butterworth ﬁlter frequency response
obtained by using the impulse invariance method and bilinear transform.
frequency period. Agreement between amplitude and phase functions is
high. The difference equation describing this Butterworth ﬁlter is
y(n) = 1.969y(n −1) −1.7383y(n −2) + 0.7271y(n −3) −0.1216y(n −4)
+0.0102x(n) + 0.0408x(n −1) + 0.0613x(n −2)
+0.0408x(n −3) + 0.0102x(n −4).
In calculations the coefﬁcients are rounded to four decimal places, what
may cause small quantization error (that will be discussed within the next
chapter).
Example 5.7. Design a continuous-time lowpass ﬁlter whose parameters are:
- passband frequency Ωp = 2π fp, fp = 3 kHz,
- stopband frequency Ωs = 2π fs, fs = 6 kHz,
- maximal attenuation in the passband ap = −2 dB, and
- minimal attenuation in the stopband as = −15 dB.
Find the corresponding discrete-time ﬁlter using the bilinear transform
and ∆t = 0.05 × 10−3 sec.
⋆The maximal attenuation in the passband and the minimal attenua-
tion in the stopband are
ap = 20log(Ap)
Ap = 10ap/20 = 0.7943
As = 10as/20 = 0.1778.
The relations for the ﬁlter order N and frequency Ωc are (Chapter I, Subsec-
tion 1.6)
1
1 +
B Ωp
Ωc
C2N ≥A2
p,
1
1 +
B
Ωs
Ωc
C2N ≤A2
s.
(5.14)

240
From Continuous to Discrete Systems
Using the equality in both of these relations it follows
N = 1
2
ln
1
A2p −1
1
A2s −1
ln Ωp
Ωs
= 2.8551.
The ﬁrst greater integer is assumed for the ﬁlter order as
N = 3.
Then we can use any relation in (5.14) with equality sign in order to calculate
Ωc. For the ﬁrst one, the value of Ωc will be such to satisfy
''H(jΩp)
''2 = A2p.
Then
Ωc =
Ωp
2N
F 1
A2p −1
= 2π × 3.2805 kHz,
ωc = Ωc∆t = 1.0306.
Poles of the Butterworth ﬁlter in the continuous domain are
sk = Ωcej(2πk+π)/6+jπ/2, k = 0,1,2
s0 = 2π × 3.2805
-
cos 2π
3 + jsin 2π
3
.
× 103
s1 = −2π × 3.2805 × 103
s2 = 2π × 3.2805
-
cos 2π
3 −jsin 2π
3
.
× 103.
The transfer function is
H(s) =
(2π3.2805 × 103)3
(s + 2π3.2805 × 103)(s2 + 2π3.2805 × 103s + (2π3.2805 × 103)2).
In the design we will not use this transfer function.
For the bilinear transform we have to pre-modify the frequency Ωc so
that it will be returned back to the desired value when the bilinear transform
is applied. This frequency is
Ωd = 2
∆t tan( ωc
2 ) = 2π × 3.6054 kHz = 1.1327
∆t
.
The modiﬁed transfer function in the continuous domain is
Hd(s) =
(2π3.6054 × 103)3
(s + 2π3.6054 × 103)(s2 + 2π3.6054 × 103s + (2π3.6054 × 103)2).
The discrete-time Butterworth ﬁlter transfer function H(z) follows with
s = 2
∆t
1 −z−1
1 + z−1

Ljubiša Stankovi´c
Digital Signal Processing
241
as
H(z) =
1.13273
(2 1−z−1
1+z−1 + 1.1327)(
B
2 1−z−1
1+z−1
C2
+ 2.2653 1−z−1
1+z−1 + 1.13272)
=
1.4533(1 + z−1)3
(−0.8673z−1 + 3.1327)(3.0177z−2 −5.434z−1 + 7.54)
= −0.5553z−3 −1.6658z−2 −1.6658z−1 −0.5553
z−3 −5.4127z−2 + 9.0028z−1 −9.0249
= 0.0615z3 + 0.1846z2 + 0.1846z + 0.0615
z3 −0.9975z2 + 0.5998z −0.1108
.
The corresponding difference equation is
y(n) = 0.9975y(n −1) −0.5998y(n −2) + 0.1108y(n −3)
+ 0.0615x(n) + 0.1846x(n −1) + 0.1846x(n −2) + 0.0615x(n −3).
Example 5.8. A continuous-time signal
x(t) = 8cos(22π
3 t) + 4sin(πt) + 4cos(8π
3 t + π
4 )
is sampled with ∆t = 1/4. The discrete-time signal is passed through an ideal
lowpass ﬁlter with frequency ωc = π/3. Find the output signal. What is the
corresponding continuous-time output signal?
⋆The discrete-time signal is
x(n) = 2cos(11π
6 n) + sin( π
4 n) + cos(2π
3 n + π
4 )
with the Fourier transform
X(ejω) = 2π
∞
∑
k=−∞
[δ(ω −11π
6
+ 2kπ) + δ(ω + 11π
6
+ 2kπ)]
+ π
j
∞
∑
k=−∞
[δ(ω −π
4 + 2kπ) −δ(ω + π
4 + 2kπ)]
+ π
∞
∑
k=−∞
[δ(ω −2π
3 + 2kπ)ejπ/4 + δ(ω + 2π
3 + 2kπ)e−jπ/4].
Within the basic period −π ≤ω ≤π the Fourier transform value is
X(ejω) = 2π[δ(ω −11π
6
+ 2π) + δ(ω + 11π
6
−2π)]
+ π
j [δ(ω −π
4 ) −δ(ω + π
4 )]
+ π[δ(ω −2π
3 )ejπ/4 + δ(ω + 2π
3 )e−jπ/4].

242
From Continuous to Discrete Systems
In addition to the last two components that have frequencies corre-
sponding to the analog signal there is the ﬁrst component
2π[δ(ω −11π
6
+ 12π
6 ) + δ(ω + 11π
6
−12π
6 )]
corresponding to
x1(n) = 2cos( π
6 n).
The lowpass ﬁlter output is
y(n) = 2cos( π
6 n) + sin( π
4 n).
It corresponds to the continuous-time signal
y(t) = 8cos( π
6 t) + 4sin(πt).
One component at frequency ω = 2π/3 > π/3 is ﬁltered out. The component
at ω = π/4 is unchanged. One more component appeared at ω = π/6 due to
the periodic extension of the Fourier transform of a discrete-time signal.
In general a signal component x(t) = exp(jΩ0t), Ω0 < 0, with a sam-
pling interval ∆t such that
Kπ ≤Ω0∆t < (K + 1)π
will, after sampling, result into a component within the basic period of the
Fourier transform of discrete-time signal, corresponding to the continuous
signal at exp(j(Ω0t −K
∆t πt)
This effect is known as aliasing. The most
obvious visual effect is when a wheel rotating with f0 = 25 [Hz], Ω0 = 50π, is
sampled in a video sequence at ∆t = 1/50 [sec]. Then Ω0∆t = π corresponds
to exp(j(Ω0t −50πt)) = ej0, i.e., the wheel looks as a static (nonmoving)
object.
5.5.2
Highpass Filters
Highpass ﬁlters can be obtained by transforming corresponding continuous-
time ﬁlters into the discrete-time domain. In the discrete-time domain a
highpass ﬁlter frequency response is equal to a lowpass ﬁlter response
shifted in frequency for π, Fig.5.13,
HH(ejω) = H(ej(ω−π)).
A frequency shift corresponds to the impulse response modulation
hH(n) = ejπnh(n) = (−1)nh(n).

Ljubiša Stankovi´c
Digital Signal Processing
243
ω
|HH(ejω)|2
- π
π
-2 π
2π
0
ω
|H(ejω)|2
- π
π
-2 π
ωc
- ωc
2π
1
Figure 5.13
Highpass ﬁlter as a shifted version of the lowpass ﬁlter.
(-1) n
(-1) n
×
×
x(n)
y(n)
h(n)
Figure 5.14
Highpass ﬁlter realization using lowpass ﬁlter.
Thus, if we have a lowpass ﬁlter, the corresponding highpass ﬁlter is ob-
tained by multiplying the impulse response values h(n) by (−1)n. The out-
put of a highpass ﬁlter to any input signal x(n) is, Fig.5.14,
y(n) = x(n) ∗n hH(n) =
∞
∑
m=−∞
x(m)(−1)n−mh(n −m)
= (−1)n
∞
∑
m=−∞
(−1)mx(m)h(n −m) = (−1)n × [(−1)nx(n)] ∗n h(n) (5.15)
Example 5.9. For the lowpass Butterworth discrete-time ﬁlter
H(z) =
0.1236
P
1 + z−1Q4
P
z−2 −1.9389z−1 + 1.7420
QP
z−2 −4.0790z−1 + 4.7686
Q
from Fig.5.15 plot the frequency response if z is replaced by −z.
⋆The impulse response is obtained by changing the sign for each
other sample in h(n). In the z-transform deﬁnition that means using (−z)−n
instead of z−n. The frequency response of
HH(z) =
0.1236
P
1 −z−1Q4
P
z−2 + 1.9389z−1 + 1.7420
QP
z−2 + 4.0790z−1 + 4.7686
Q
is shown in Fig.5.15.

244
From Continuous to Discrete Systems
0
0.5
1
1.5
ω
|H(ejω|
- π
- π/2
π/2
π
0
0
0.5
1
1.5
ω
|HH(ejω|
- π
- π/2
π/2
π
0
Figure 5.15
Amplitude of frequency response of a lowpass Butterworth ﬁlter (left) and a ﬁlter
obtained from the lowpass Butterworth ﬁlter when z is replaced by −z (right).
ω
|HB(ejω)|2
- π
π
-2 π
2π
0
ω0
- ω0
ω
|H(ejω)|2
- π
π
-2 π
ωc
- ωc
2π
1
Figure 5.16
Bandpass ﬁlter as shifted version of a lowpass ﬁlter.
5.5.3
Bandpass Filters
A bandpass ﬁlter is obtained from a lowpas ﬁlter by shifting its frequency
response for ω0 and −ω0, as shown in Fig.5.16. The frequency response is
HB(ejω) = H(ej(ω−ω0)) + H(ej(ω+ω0)).
In the discrete-time domain this frequency shift corresponds to
hB(n) = ejω0nh(n) + e−jω0nh(n) = 2cos(ω0n)h(n).

Ljubiša Stankovi´c
Digital Signal Processing
245
cos(ω0n)
2cos(ω0n)
×
×
h(n)
sin(ω0n)
2sin(ω0n)
×
×
x(n)
y(n)
h(n)
Figure 5.17
Bandpass system realization using corresponding lowpass systems and signal
modulation.
In general for an input signal x(n) the output of a bandpass ﬁlter is
y(n) = hB(n) ∗x(n) =
∞
∑
m=−∞
hB(m)x(n−m) = 2
∞
∑
m=−∞
cos(ω0m)h(m)x(n−m)
= 2
∞
∑
m=−∞
cos(ω0n + ω0m −ω0n)h(m)x(n −m)
=2
∞
∑
m=−∞
[cos(ω0n)cos(ω0m−ω0n)−sin(ω0n)sin(ω0m−ω0n)]h(m)x(n−m)
= 2cos(ω0n)
∞
∑
m=−∞
cos(ω0(n −m))x(n −m)h(m)
+2sin(ω0n)
∞
∑
m=−∞
sin(ω0(n −m))x(n −m)h(m).
Finally we may write the output of a bandpass ﬁlter as a function of the
lowpass impulse response as
y(n) = 2cos(ω0n){[cos(ω0n)x(n)] ∗h(n)}
+ 2sin(ω0n){[sin(ω0n)x(n)] ∗h(n)}.
This relation leads to a realization of a bandpass ﬁlter using lowpass ﬁlters,
as shown in Fig.5.17.

246
From Continuous to Discrete Systems
5.5.4
Allpass Systems - System Stabilization
A system (ﬁlter) with unit (constant) amplitude of the frequency response is
HA(z) = z−1 −ae−jθ
1 −aejθz−1 = 1 −zae−jθ
z −aejθ
=
z −1
aejθ
1 −1
ae−jθz
e−j2θ,
where 0 < a < 1 and θ is an arbitrary phase. For this system
'''HA(ejω)
''' = 1.
To prove the statement consider
'''HA(ejω)
''' =
''''
e−jω −ae−jθ
1 −aejθe−jω
'''' =
'''''
ej(θ−ω) −a
1 −aejθe−jω
'''''
=
<
(cos(θ −ω) −a)2 + sin2(θ −ω)
(1 −acos(θ −ω))2 + a2 sin2(θ −ω) =
<
a2 −2acos(θ −ω) + 1
1 −2acos(θ −ω) + a2 = 1.
Example 5.10. Given a system
H(z) =
z + 2
(z −1
2)(z −1
3)(z −2)
.
System cannot be causal and stable since there is a pole at z = 2. Deﬁne an
allpass system to be connected to H(z) in cascade such that the resulting sys-
tem is causal and stable, with the same amplitude of the frequency response
as H(z).
⋆The system is
Hs(z) = H(z)HA(z) =
z + 2
(z −1
2)(z −1
3)(z −2)
z −1
a ejθ
1 −1
a e−jθz
e−j2θ.
For a = 1/2 and θ = 0 we get
Hs(z) =
z + 2
(z −1
2)(z −1
3)(z −2)
z −2
1 −2z
= −
z + 2
2(z −1
2)2(z −1
3)
.
This system has the same frequency response amplitude as the initial system
'''Hs(ejω)
''' =
'''H(ejω)HA(ejω)
''' =
'''H(ejω)
'''.

Ljubiša Stankovi´c
Digital Signal Processing
247
The allpass system can be generalized to the form
HA(z) = z−1 −a1e−jθ1z−1
1 −a1ejθ1z−1
z−1 −a2e−jθ2z−1
1 −a2ejθ2z−1
...z−1 −aNe−jθNz−1
1 −aNejθNz−1
where 0 < ai < 1 and θi, i = 1,2,..., N are arbitrary constants and phases. The
resulting frequency response amplitude is
'''HA(ejω)
''' = 1.
This system can be used for multiple poles cancellation and phase correc-
tion.
5.5.5
Inverse and Minimum Phase Systems
An inverse system to the system H(z) is deﬁned as
Hi(z) =
1
H(z).
It is obvious that
H(z)Hi(z) = 1
h(n) ∗hi(n) = δ(n).
This kind of system can be used to reverse the signal distortion. For ex-
ample, assume that the Fourier transform of a signal x(n) is distorted dur-
ing transmission by a transfer function H(z), i.e., the received signal z-
transform is R(z) = H(z)X(z). In that case the distortion can be compen-
sated by processing the received signal using the inverse system. The output
signal is obtained as
Y(z) =
1
H(z) R(z) = X(z).
The system Hi(z) = 1/H(z) should be stable as well. It means that the poles
of the inverse system should be within the unit circle. The poles of the
inverse system are equal to the zeros of H(z).
The system H(z) whose both poles and zeros are within the unit circle
is called a minimum phase system.

248
From Continuous to Discrete Systems
Example 5.11. (a) Which of these two systems
H1(z) = z2 + z −5
16
z2 + z + 3
16
H2(z) = z2 −z + 3
16
z2 + z + 3
16
is a minimum phase system?
(b) If the amplitude of the Fourier transform of the discrete-time re-
ceived signal is distorted as R(z) = H1(z)X(z) what is a stable and causal
system HD(z) that will produce
'''Y(ejω)
''' =
'''X(ejω)
''' at its output if the input
is the received signal r(n)?
⋆a) The systems can be written as
H1(z) = (z −1
4)(z + 5
4)
(z + 1
4)(z + 3
4)
H2(z) = (z −1
4)(z −3
4)
(z + 1
4)(z + 3
4)
The ﬁrst system is causal and stable for the region of convergence |z| > 3/4.
However one of its zeros is at |z| = 5/4 > 1 and the system is not a minimum
phase system, since its causal inverse form is not stable. The second system is
causal and stable. The same holds for its inverse since all poles of the inverse
system are within |z| < 1. Thus, the system H2(z) is a minimal phase system.
(b) In this case
R(z) = z2 + z −5
16
z2 + z + 3
16
X(z) = (z −1
4)(z + 5
4)
(z + 1
4)(z + 3
4)
X(z).
An inverse system to H1(z) cannot be used since it will not be stable. How-
ever the inverse can be stabilized with an allpass system HA(z) so that the
amplitude is not changed
Y(z) = R(z)
1
H1(z) HA(z) = H1(z)X(z)
1
H1(z) HA(z)
where
HA(z) = z + 5
4
1 + 5
4z
and
HD(z) =
1
H1(z) HA(z) = (z + 1
4)(z + 3
4)
(z −1
4)(z + 5
4)
(z + 5
4)
(1 + 5
4z)
= (z + 1
4)(z + 3
4)
(z −1
4)(1 + 5
4z)

Ljubiša Stankovi´c
Digital Signal Processing
249
This system is stable and causal and will produce
'''Y(ejω)
''' =
'''X(ejω)
'''.
If a system is the minimum phase system (with all poles and zeros
within |z| < 1) then this system has a minimum group delay out of all
systems with the same amplitude of the frequency response. Thus, any
nonminimum phase system will have a more negative phase compared
to the minimum phase system. The negative part of the phase is called
the phase-lag function. The name minimum phase system comes from the
minimum phase-lag function.
In order to prove this statement consider a system H(z) with the sam-
ple amplitude of the frequency response as a nonminimum phase system
Hmin(z). Its frequency response can be written as
H(z) = Hmin(z)HA(z) = Hmin(z)z−1 −ae−jθ
1 −aejθz−1
Here we assumed the ﬁrst-order allpass system without any loss of gener-
ality, since the same proof can be used for any number of allpass systems
that multiply Hmin(z). Since 0 < a1 < 1 and the system Hmin(z) is stable the
system H(z) has a zero at |z| = 1/a1 > 1.
The phases of the system are related as
arg{H(ejω)} = arg{Hmin(ejω)} + arg{HA(ejω)}.
The phase of allpass system is
arg{HA(ejω)} = arg{e−jω −ae−jθ
1 −aejθe−jω }
= arg{e−jω 1 −ae−jθejω
1 −aejθe−jω } = −ω + arg{1 −ae−jθejω}
−arg{1 −aejθe−jω} = −ω −2arctan
asin(ω −θ)
1 −acos(ω −θ).
Its derivative (group delay) is
τgA(ω) = −darg{HA(ejω)}
dω
= 1 + 2
acos(ω −θ) −a2
1 −2acos(ω −θ) + a2
=
1 −a2
1 −2acos(ω −θ) + a2 =
1 −a2
''1 −aej(ω−θ)''2 .

250
From Continuous to Discrete Systems
Since a < 1 then the group delay is always positive and
τg(ω) = τgmin(ω) + τgA(ω)
τg(ω) ≥τgmin(ω),
with τg(ω) and τgmin(ω) being the phase derivatives (group delays) of
systems H(z) and Hmin(z), respectively.
The phase behavior of all pass system is
arg{HA(ej0)} = arg{1 −ae−jθ
1 −aejθ } = 0
(5.16)
arg{HA(ejω)} = −
ω
"
0
τg(ω)dω ≤0
(5.17)
since τg(ω) > 0 for 0 ≤ω < π.
We can conclude that the minimum phase systems satisfy the following
conditions.
1. A minimum phase system is system of minimum group delay out
of the systems with the same amplitude of frequency response. A system
containing one or more allpass parts with uncompensated zeros outside of
the unit circle will have larger delay than the system which does not contain
zeros outside the unit circle.
2. The phase of a minimal phase system will be lower than the phase
of any other system with the same amplitude of frequency response since,
according to (5.17),
arg{H(ejω) = arg{Hmin(ejω)} + arg{HA(ejω)}
≤arg{Hmin(ejω)}.
This proves the fact that the phase of any system arg{H(ejω) is always
lower than the phase of minimal phase system arg{Hmin(ejω)}, having the
same amplitude of the frequency response.
3. Since the group delay is minimal we can conclude that
n
∑
m=0
|hmin(m)|2 ≥
n
∑
m=0
|h(m)|2
This relation may be proven in a similar way like minimal phase property,
by considering the outputs of a minimum phase system and a system
H(z) = Hmin(z)HA(z).

Ljubiša Stankovi´c
Digital Signal Processing
251
Example 5.12. A system has absolute squared amplitude of the frequency response
equal to
'''H(ejω)
'''
2
=
B
2cos(ω) + 5
2
C2
(12cos(ω) + 13)(24cos(ω) + 25)
Find the corresponding minimal phase system.
⋆For the system we can write
'''H(ejω)
'''
2
= H(ejω)H∗(ejω) = H(ejω)H(e−jω)
In the z−domain the system with this amplitude of the frequency response
(with real-valued coefﬁcients) satisﬁes
H(z)H∗( 1
z∗)
''''
z=ejω = H(z)H(1
z )
''''
z=ejω =
'''H(ejω)
'''
2
= H(ejω)H(e−jω).
In this sense
'''H(ejω)
'''
2
=
B
ejω + e−jω + 5
2
C2
(6ejω + 6e−jω + 13)(12ejω + 12e−jω + 25)
and
H(z)H(1
z ) =
B
z + 5
2 + z−1C2
(6z + 13 + 6z−1)(12z + 25 + 12z−1)
=
B
z2 + 5
2z + 1
C2
(6z2 + 13z + 6)(12z2 + 25z + 12)
= 1
36
(z + 2)2(z + 1
2)2
(z + 2
3)(z + 3
2)(z + 3
4)(z + 4
3) = 1
36
( 1
z + 1
2)2(z + 1
2)2
(z + 2
3)( 1
z + 2
3)(z + 3
4)( 1
z + 3
4)
.
The minimum phase system, with the desired amplitude of the frequency
response, is a part of H(z)H∗( 1
z∗) with zeros and poles inside the unit circle
H(z) = 1
6
(z + 1
2)2
(z + 2
3)(z + 3
4)
.
The other poles and zeros then belong to H∗(1/z∗).
5.6
PROBLEMS
Problem 5.1. An RLC circuit transfer function is
H(s) =
1
LC
s2 + s R
L +
1
LC

252
From Continuous to Discrete Systems
with R/L = 8 and 1/(LC) = 25. Find the difference equation describing
the corresponding discrete-time system obtained by the impulse invariance
method. What is the impulse response of the discrete-time system. Use
∆t = 1.
Problem 5.2. Could the method of impulse invariance be used to map the
system
H(s) = s2 −3s + 3
s2 + 3s + 3
to the discrete-time domain. What is the corresponding discrete-time system
obtained by the bilinear transform with ∆t = 1?
Problem 5.3. A continuous-time system is described by a differential equa-
tion
y′′(t) + 3
2y′(t) + 1
2y(t) = x(t)
with zero initial conditions. What is the corresponding transfer function
of discrete-time system using the ﬁrst-order backward difference approxi-
mation with ∆t = 1/10? Write the difference equation of the system whose
output approximates the output of the continuous-time system.
Problem 5.4. Transfer function of a continuous-time system is
H(s) = −
2s
s2 + 2s + 2.
What is the corresponding discrete-time system using the invariance im-
pulse method and the bilinear transform with ∆t = 1?
Problem 5.5. A continuous-time system has a transfer function of the form
H(s) =
(1 + 4s)
(s + 1/2)(s + 1)3 .
What is the corresponding discrete-time system according to:
(a) the impulse invariance method,
(b) the bilinear transform,
(c) the matched z-transform?
Use ∆t = 1.
Problem 5.6. A continuous-time system
H(s) =
2QΩ1
s2 + 2Ω1Qs + Ω2
1 + Q2

Ljubiša Stankovi´c
Digital Signal Processing
253
is designed to pass the signal
x(t) = A1 cos(Ω1t + ϕ1)
and to stop all other possible signal components. The parameters are Q =
0.01, Ω1 = π/2. The signal is sampled with ∆t = 1 and a discrete-time signal
x(n) is formed. Using bilinear transform design the discrete system that
corresponds to the continuous-time system with transfer function H(s).
Problem 5.7. (a) By using the bilinear transform ﬁnd the transfer function of
the second-order Butterworth ﬁlter with fac = 4kHz. The sampling interval
is ∆t = 50µsec.
(b) Translate the discrete-time transfer function to obtain a highpass ﬁlter.
Find its corresponding critical frequency in the continuous-time domain.
Problem 5.8. Design a discrete-time lowpass Butterworth ﬁlter for the sam-
pling frequency 1/∆t = 10 kHz. The passband should be from 0 to 1 kHz,
maximal attenuation in the passband should be 3 dB and the attenuation
should be more than 10 dB for frequencies above 2 kHz.
Problem 5.9. Using the impulse invariance method design a Butterworth
ﬁlter with the passband frequency ωp = 0.1π and stopband frequency
ωn = 0.3π in the discrete domain. Maximal attenuation in the passband
region should be less than 2dB, and the minimal attenuation in the stopband
should be 20dB.
Problem 5.10. Highpass ﬁlter can be obtained from a lowpass by using
HH(s) = H(1/s). Using the bilinear transform with ∆t = 2 we can trans-
form the continuous-time domain function into discrete domain using the
relation s = (z −1)/(z + 1). If we have a design of a lowpass ﬁlter how to
change its coefﬁcients in order to get a highpass ﬁlter.
Problem 5.11. For ﬁltering of a continuous-time signal a discrete-time ﬁlter
is used. Find the corresponding continuous-time ﬁlter frequencies if the
discrete-time ﬁlter is: a) a lowpass with ωp = 0.15π, b) bandpass within
0.2π ≤ω ≤0.25π, c) a highpass with ωp = 0.35. Consider cases when
∆t = 0.001s and ∆t = 0.1s.
What should be the starting frequencies to design these systems in the
continuous-time domain if the impulse invariance method is used and what
are the design frequencies if the bilinear transform is used?
Problem 5.12. A transfer function of the ﬁrst-order lowpass system is
H(z) =
1 −α
1 −αz−1 .

254
From Continuous to Discrete Systems
Find the corresponding bandpass system transfer function with frequency
shifts for ±ωc.
Problem 5.13. Using allpass system ﬁnd stable systems with the same
amplitude of the frequency response as the systems:
(a)
H1(z) = 2 −3z−1 + 2z−2
1 −4z−1 + 4z−2
(b)
H2(z) =
z
(4 −z)(1/3 −z).
Problem 5.14. The z-transform
R(z) = (z −1
4)(z−1 −1
4)(z + 1
2)(z−1 + 1
2)
(z + 4
5)(z−1 + 4
5)(z −3
7)(z−1 −3
7)
can can be written as
R(z) = H(z)H∗( 1
z∗).
Find H(z) for the minimum phase system.
Problem 5.15. A signal x(n) has passed trough a media whose inﬂuence
can be described by the transfer function
H(z) = (4 −z)(1/3 −z)(z2 −
√
2z + 1
4)
z −1
2
.
Signal r(n) is obtained. Find a causal and stable system to process r(n) in
order to obtain output signal y(n) such that
''Y(ejω)
'' =
''X(ejω)
''.
5.7
SOLUTIONS
Solution 5.1. For this system we can write
H(s) =
1
LC
s2 + s R
L +
1
LC
=
25
s2 + 8s + 25
=
25
(s + 4 + 3j)(s + 4 −j3)

Ljubiša Stankovi´c
Digital Signal Processing
255
H(s) =
j 25
6
s + 4 + j3 +
−j 25
6
s + 4 −j3.
The poles are mapped using
si →zi = esi.
The discrete-time system is
H(z) =
j 25
6
1 −e−(4+j3)z−1 +
−j 25
6
1 −e−(4−j3)z−1
=
25
3 e−4z−1 sin3
1 −2e−4 cos3z−1 + e−8z−2 ,
with corresponding difference equation
y(n) = 25
3 e−4 sin(3)x(n −1) + 2e−4 cos(3)y(n −1) −e−8y(n −2).
The output signal values can be calculated for any input signal using
this difference equation. For x(n) = δ(n) the impulse response would follow.
The impulse response can be obtained in a closed form from
H(z) = j25
6
∞
∑
n=0
e−(4+j3)nz−n −j25
6
∞
∑
n=0
e−(4−j3)nz−n
as
h(n) = 25
6 e−4n(je−j3n −jej3n)u(n) =
= 25
3 e−4n sin(3n)u(n).
Solution 5.2. The system is not of lowpass type. For s →∞we get H(s) →1.
Thus, the impulse invariance method cannot be used. The bilinear trans-
form can be used. It produces
H(z) =
4 (1−z−1)2
(1+z−1)2 −6 1−z−1
1+z−1 + 3
4 (1−z−1)2
(1+z−1)2 + 6 1−z−1
1+z−1 + 3
= 13z−2 −2z−1 + 1
z−2 −2z−1 + 13 .

256
From Continuous to Discrete Systems
Solution 5.3. For the system
y′′(t) + 3
2y′(t) + 1
2y(t) = x(t)
the transfer function is
H(s) =
1
s2 + 3
2s + 1
2
.
Corresponding discrete system is obtained using
s →1 −z−1
∆t
= 10(1 −z−1)
as
H(z) =
1
100(1 −z−1)2 + 3
210(1 −z−1) + 1
2
=
1
100z−2 −215z−1 + 231
2
.
The difference equation of this system is
y(n) =
2
231x(n) + 430
231y(n −1) −200
231y(n −2).
Solution 5.4. The transfer function can be written as
H(s) = −
1 + j
s + 1 −j −
1 −j
s + 1 + j.
Using the invariance impulse method it follows
H(z) = −2 −2(cos(1) + sin(1))e−1z−1
1 −2cos(1)e−1z−1 + e−2z−2 .
The bilinear transform produces
H(z) = −2
1 −z−2
5 −2z−1 + z−2 .
Solution 5.5. (a) The transfer function
H(s) =
(1 + 4s)
(s + 1/2)(s + 1)3

Ljubiša Stankovi´c
Digital Signal Processing
257
is written to a form appropriate for the impulse invariance method
H(s) =
k1
s + 1/2 +
k2
(s + 1) +
k3
(s + 1)2 +
k4
(s + 1)3
with k1 = H(s)(s + 1/2)|s=−1/2 = −8 and k4 = H(s)(s + 1)3''
s=−1 = 6. By
equating the coefﬁcients with s3 to 0 we get the relation k1 + k2 = 0. Similar
relation follows for the coefﬁcients with s2 as 3k1 + 5k2/2 + k3 = 0 or
k1/2 + k3 = 0. Then k2 = 8 and k3 = 4. With
ki
s −si
→
ki
1 −esiz−1
and
1
m!
dm
dsm
i
ki
s −si
→1
m!
dm
dsm
i
{
ki
1 −esiz−1 }
we get the discrete system
H(z) =
−8
1 −e−1/2z−1 +
8
1 −e−1z−1
+ d
ds1
*
4
1 −es1z−1
+''''
s1=−1
+ d2
ds2
1
*
6
1 −es1z−1
+''''
s1=−1
=
−8
1 −e−1/2z−1 +
8
1 −e−1z−1 +
4e−1z−1
(1 −e−1z−1)2 + 3e−2z−2 + 3e−1z−1
(1 −e−1z−1)3
= −5.83819z−3 −9.68722z−2 + 22.0531z−1
(z−1 −e)3(z−1 −e1/2)
(b) Discrete system obtained using the bilinear transform is
H(z) =
(1 + 8 1−z−1
1+z−1 )
(2 1−z−1
1+z−1 + 1/2)(2 1−z−1
1+z−1 + 1)3
= −14z−4 −24z−3 + 12z−2 + 40z−1 + 18
3z−4 −32z−3 + 126z−2 −216z−1 + 135.
(c) The matched z-transform produces
H(z) =
4(1 −e−1/4z−1)
P
1 −e−1/2z−1Q(1 −e−1z−1)3 .

258
From Continuous to Discrete Systems
Solution 5.6. Since we use the bilinear transform we have to pre-modify the
system according to
Ωd = 2
∆t tan(Ω1∆t
2
) = 2.0 = 0.6366π.
The frequency value is shifted from Ω1 = 0.5π to Ωd = 0.6366π. The modi-
ﬁed system is
Hd(s) =
2QΩd
s2 + 2ΩdQs + Ω2
d + Q2 .
Now using s = 2 1−z−1
1+z−1 the corresponding discrete- system is obtained,
H(z) =
2QΩd
B
2 1−z−1
1+z−1
C2
+ 2ΩdQ
B
2 1−z−1
1+z−1
C
+ Ω2
d + Q2
.
The bilinear transform returns the pre-modiﬁed frequency to the desired
one.
Solution 5.7. The poles of H(s)H(−s) for a continuous-time second order
(N = 2) Butterworth ﬁlter are
sk = Ωcej(2πk+π)/2N+jπ/2 = 2π fcej(2πk+π)/4+jπ/2,
where fc = 2
∆t tan(2π fac∆t/2)/(2π) = 4.6253 kHz. With k = 0,1,2,3 follows
sk = 2π fc(±
√
2
2 ± j
√
2
2 ).
For a stable system the poles satisfy Re{sp} < 0, thus
s1,2 = 2π fc(−
√
2
2 ± j
√
2
2 ).
The transfer function H(s) is
Ha(s) =
s1s2
(s −s1)(s −s2) =
4π2 f 2
c
s2 + 2π fc
√
2s + 4π2 f 2c
.
Using the bilinear transform with ∆t = 50 · 10−6 we get the corresponding
discrete system transfer function
H(z) =
1.0548(1 + z−1)2
5.1066 −1.8874z−1 + z−2 .

Ljubiša Stankovi´c
Digital Signal Processing
259
This ﬁlter has −3 dB attenuation at ω = 0.4π corresponding to Ω=
0.4π/∆t = 2π × 4 × 103.
b) The discrete highpass ﬁlter is obtained by a shift corresponding to
Hh(ejω) = H(ej(ω+π)). It corresponds to the impulse response modulation
hh(n) = (−1)nh(n) or substitution of z by −z in the transfer function,
H(z) =
1.0548(1 −z−1)2
5.1066 + 1.8874z−1 + z−2 .
The critical frequency of highpass ﬁlter is ωc = 0.6π or fac = 6 kHz.
Solution 5.8. For the continuous-time system the design frequencies are
fp = 1 kHz
fs = 2 kHz.
They correspond to
Ωp = 2π103 rad/s
Ωs = 4π 103 rad/s.
The discrete-time frequencies are obtained from ω = Ω∆t = Ω/104 as
ωp = 0.2π
ωs = 0.4π.
The frequencies for the ﬁlter design, that will be mapped to ωs and ωp by
using the bilinear transform, are
Ωpd = 2
∆t tan(0.2π/2) = 0.6498
∆t
Ωsd = 2
∆t tan(0.4π/2) = 1.4531
∆t
.
The ﬁlter order follows from
N = 1
2
log 1−100.1ap
1−100.1as
log
Ωpd
Ωsd
= 1.368.
We assume N = 2.

260
From Continuous to Discrete Systems
Since the frequency for −3 dB attenuation is given the design critical
frequency is
Ωcd = Ωpd = 0.6498
∆t
.
The poles of the ﬁlter transfer function are
sp1/2 = 0.6498
∆t
(−
√
2
2 ± j
√
2
2 )
with the transfer function
H(s) =
sp1sp2
(s −sp1)(s −sp2) =
1
∆t2 0.4223
s2 + 0.919s 1
∆t + 0.4223 1
∆t2
.
Mapping this system into the discrete-time domain by using the bilinear
transform,
s = 2
∆t
1 −z−1
1 + z−1 ,
produces
H(z) =
0.067569(1 + z−1)2
1 −1.14216z−1 + 0.412441z−2 .
Solution 5.9. The Butterworth ﬁlter order is
N = 1
2
log 1−100.1ap
1−100.1as
log Ωp
Ωs
= 2.335.
with Ωp = ωp/∆t, Ωs = ωs/∆t, and ∆t = 1. Assume N = 3.
Critical frequency Ωc, where the amplitude of the frequency response
is attenuated for 3 dB, is
Ωc =
Ωp
2N,
100.1ap −1
= 0.109345π = 0.3435.
The transfer function H(s) poles are
sp2/3 = −0.17175 ± j0.29748
sp1 = −Ωc = −0.3435.

Ljubiša Stankovi´c
Digital Signal Processing
261
The transfer function form is
H(s) =
−sp1sp2sp3
(s −sp1)(s −sp2)(s −sp3) =
0.0405
(s + 0.3435)(s3 + 0.3435s + 0.1178
=
k1
s −sp1
+
k2
s −sp2
+
k3
s −sp3
=
0.3435
s + 0.3435 −
0.17175 −j0.09916
s + 0.17175 + j0.29748 −
0.17175 + j0.09916
s + 0.17175 −j0.29748.
The coefﬁcients ki are calculated from
ki = H(s)(s −spi)
'''s=spi .
Using the impulse invariance method, mapping from the continuous-time
domain to the discrete-time domain, is done according to
ki
s −spi
→
∆tki
1 −espi∆tz−1 .
The discrete-time system transfer function is
H(z) =
−0.0253z−2 −0.0318z−1
−1.98774 + 4.61093z−1 −3.68033z−2 + z−3 .
Solution 5.10. The transfer function is
HH(s) = H(1
s )
with s = 2
∆t
1−z−1
1+z−1 = 2
∆t
z−1
z+1 and ∆t = 2. Corresponding lowpass ﬁlter would
be
HL(z) = H(s)|s= z−1
z+1 = H(z −1
z + 1).
The discrete highpass ﬁlter is
HH(z) = HH(s)|s= z−1
z+1 = H(1
s )
''''
s= z−1
z+1
HH(z) = H(z + 1
z −1).
Obviously HH(z) = HL(−z). It means that a discrete highpass system can be
realized by replacing z with −z in the transfer function. For ∆t ̸= 2 a scaling
is present as well.

262
From Continuous to Discrete Systems
Solution 5.11. a) The mapping with ∆t = 0.001 s produces a lowpass ﬁlter
with Ωp = ωp/∆t = 150πrad/s. For ∆t = 0.1 s the frequency is Ωp =
ωp/∆t = 1.5πrad/s.
b) For ∆t = 0.001 s a bandpass ﬁlter is obtained for the range 200π
rad/s ≤Ω≤250π rad/s, while ∆t = 0.1 s produces a bandpass ﬁlter with
2π rad/s ≤Ω≤2.5π rad/s.
c) For ∆t = 0.001 s a highpass ﬁlter has the frequency Ωp = 350rad/s,
while for ∆t = 0.1 s the highpass ﬁlter has critical frequency Ωp = 3.5rad/s.
For the impulse invariance method starting design frequencies should
be equal to the calculated analog frequencies. If the bilinear transform
is used calculated analog frequencies Ωp should be pre-modiﬁed to Ωm
according to Ωm = 2
∆t tan Ωp∆t
2
.
Solution 5.12. The impulse response of the passband ﬁlter is hB(n) =
2h(n)cos(ωcn). The z-transform of the impulse response is
HB(z) =
∞
∑
n=−∞
2h(n)cos(ωcn)z−n =
∞
∑
n=−∞
h(n)(e−jωcz)−n +
∞
∑
n=−∞
h(n)(ejωcz)−n
HB(z) = H(e−jωcz) + H(ejωcz) = 2(1 −α)(1 −αcosωcz−1)
1 −2αcosωcz−1 + α2z−2 .
Solution 5.13. The causal system
H1(z) = 2 −3z−1 + 2z−2
(1 −2z−1)2
is not stable since it has a second-order pole at z = 2. This system may be
stabilized, keeping the same amplitude of the frequency response, using a
second-order allpass system with zero at z = 2
HA(z) =
(
z−1 −1
2
1 −1
2z−1
)2
.
The new system has a transfer function
H1(z) = 2 −3z−1 + 2z−2
(z−1 −2)2
.
Causal system H2(z) has a pole at z = 4. It can be stabilized by using allpass
system
HA(z) = z−1 −1
4
1 −1
4z−1 = 4 −z
4z −1.

Ljubiša Stankovi´c
Digital Signal Processing
263
The transfer function of a stable system is
H2(z) =
z
(4z −1)(1/3 −z).
Solution 5.14. For the z-transform
R(z) = (z −1
4)(z−1 −1
4)(z + 1
2)(z−1 + 1
2)
(z + 4
5)(z−1 + 4
5)(z −3
7)(z−1 −3
7)
and
R(z) = H(z)H∗( 1
z∗).
the minimum phase system is a part of R(z) whose all zeros and poles
are inside the unit circle, meaning that H(z) system and its inverse system
1/H(z) can be a causal and stable. Therefore,
H(z) = (z −1
4)(z + 1
2)
(z + 4
5)(z −3
7)
.
It is easy to check that H∗( 1
z∗) is equal to he remaining terms in R(z), since
H∗( 1
z∗) = ( 1
z∗−1
4)∗( 1
z∗+ 1
2)∗
( 1
z∗+ 4
5)∗( 1
z∗−3
7)∗= (z−1 −1
4)(z−1 + 1
2)
(z−1 + 4
5)(z−1 −3
7)
.
Here we used, for example, ( 1
z∗−1
4)∗=
B
1
z∗
C∗
−1
4 = 1
z −1
4.
Solution 5.15. The received signal should be processed by the inverse
system
Hi(z) =
1
H(z) =
z −1
2
(4 −z)(1/3 −z)(z2 −
√
2z + 1
4)
.
However this system has two poles outside the unit circle since
Hi(z) =
z −1
2
(4 −z)(1/3 −z)(z −1.2071)(z −0.2071).
These poles have to be compensated, keeping the same amplitude, by using
two ﬁrst-order allpass systems. The resulting system transfer function is
Hi(z) z −4
1 −4z
z −1.2071
1 −1.2071z
=
z −1
2
(1/3 −z)(z −0.2071)(1 −4z)(1 −1.2071z).

264
From Continuous to Discrete Systems
5.8
EXERCISE
Exercise 5.1. Transfer function of a continuous-time system is
H(s) =
(s + 2)
4s2 + s + 1.
What is the corresponding discrete-time system obtained with ∆t = 1 by
using the impulse invariance method and the bilinear transform.
Exercise 5.2. A continuous system is described by a differential equation
y′′(t) + 6y′(t) −y(t) = x(t) + 1
2x′(t)
with zero initial conditions. What is the corresponding transfer function
of a discrete system obtained by using the ﬁrst-order backward difference
approximation with ∆t = 1?
Exercise 5.3. (a) A continuous system
H(s) =
2QΩ0
s2 + 2Ω0Qs + Ω2
0 + Q2
with Q = 0.01 is designed to pass the signal
x(t) = Acos(Ω0t + ϕ)
for Ω0 = 3π/4 and to stop all other possible signal components.
The signal is sampled with ∆t = 1 and a discrete-time signal x(n) is
formed. Using the bilinear transform, design a discrete system that corre-
sponds to the continuous system with transfer function H(s).
(b) What is the output r(n) of the obtained discrete-time system to the
samples y(n) of the analog signal
y(t) = 1 + 2sin(250πt) −cos(2750πt) + 2sin(750πt)
sampled with the sampling interval ∆t = 10−3 s. What would be the corre-
sponding continuous-time output signal after an ideal D/A converter.
Exercise 5.4. (a) By using the bilinear transform ﬁnd the transfer function
of a third-order Butterworth ﬁlter with fac = 3.4 kHz. The sampling step is
∆t = 40 µsec.
(b) Translate the discrete transfer function to obtain a bandpass system
with corresponding central frequency fac = 12.5 kHz in the continuous
domain.

Ljubiša Stankovi´c
Digital Signal Processing
265
Exercise 5.5. Design a continuous lowpass ﬁlter whose parameters are:
- passband frequency Ωp = 2π fp, fp = 3.5 kHz,
- stopband frequency Ωs = 2π fs, fs = 6 kHz,
- maximal attenuation in passband ap = 2 dB, and
- minimal attenuation in the stopband as = 16 dB.
Find the corresponding discrete-time ﬁlter using:
(a) the impulse invariance method and
(b) the bilinear transform,
with ∆t = 0.05 × 10−3 sec.
(c) Write the corresponding highpass ﬁlter transfer functions, obtained
by a frequency shift in the discrete domain for π, for both cases.
Exercise 5.6. Using allpass system ﬁnd a stable and causal system with the
same amplitude of the frequency response as the systems:
H1(z) = 2 −5z−1 + 2z−2
1 −4z−1 + z−2 ,
H2(z) =
z −1
(2 −z)(1/4 −z).
Exercise 5.7. The z-transform
R(z) = (z −1
3)(z−1 −1
3)
(z + 1
2)(z−1 + 1
2)
can can be written as
R(z) = H(z)H∗( 1
z∗).
Find H(z) for the minimum phase system. If h(n) is the impulse response
of H(z) and h1(n) is the impulse response of
H1(z) = H(z)z−1 −a1e−jθ1
1 −a1ejθ1z−1
show that |h(0)| ≤|h1(0)| for any θ1 and |a1| < 1. All systems are causal.
Exercise 5.8. A signal x(n) has passed trough a media whose inﬂuence can
be described by the transfer function
H(z) = (1 −z/3)(1 −5z)(z2 −z + 3
4)
z2 −2/3
and the signal r(n) = x(n) ∗h(n) is obtained. Find a causal and stable system
to process r(n) in order to obtain
''Y(ejω)
'' =
''X(ejω)
''.

266
From Continuous to Discrete Systems

Chapter 6
Realization of Discrete Systems
L
INEAR discrete-time systems may, in general, be described by a differ-
ence equation relating the output signal with the input signal at the
considered instant and the previous values of the output and input
signal. The transfer function can be written in various forms producing dif-
ferent system realizations. Some of them will be presented next. Symbols
that are used in the realizations are presented in Fig.6.1.
6.1
REALIZATION OF IIR SYSTEMS
A system that includes recursions of the output signal values results in an
inﬁnite impulse response (IIR). These systems will be presented ﬁrst.
x(n)
a
ax(n)
x(n)
x(n)
x(n)
x(n)
x(n-1)
z -1
x(n)
y(n)
x(n)+y(n)
+
x(n)
y(n)
-
x(n)- y(n)
+
x(n)
y(n)
x(n)y(n)
×
Figure 6.1
Symbols and their function in the realization of discrete-time systems.
267

268
Realization of Discrete Systems
z-1
z-1
x(n-1)
x(n-2)
y(n-1)
y(n-2)
B1
A1
B0
+
+
z-1
z-1
B2
A2
z-1
z-1
x(n)
y(n)
+
+
Figure 6.2
Direct form implementation of a second order system.
6.1.1
Direct realization I
Consider a discrete system described by a linear difference equation
y(n) = A1y(n −1) + ... + ANy(n −N)
(6.1)
+ B0x(n) + B1x(n −1) + ... + BMx(n −M).
A second-order system, as a special case, will be presented ﬁrst. Its imple-
mentation is shown in Fig.6.2.
A general system described by (6.1) can be implemented as in Fig.6.3.
This form is a direct realization I of a discrete-time system.
6.1.2
Direct realization II
Direct realization I, presented in Fig.6.3, consists of two blocks in cascade.
The ﬁrst block realizes
y1(n) = B0x(n) + B1x(n −1) + ... + BMx(n −M)
and the second block corresponds to the recursive relation
y(n) = A1y(n −1) + ... + ANy(n −N) + y1(n).
These two blocks have transfer functions
H1(z) = B0 + B1z−1 + ... + BMz−M

Ljubiša Stankovi´c
Digital Signal Processing
269
z-1
z-1
B1
A1
B0
 y1(n)
+
+
B2
A2
z-1
z-1
BM
AN
z-1
z-1
x(n)
y(n)
+
+
+
+
Figure 6.3
Direct form I implementation of a discrete-time system.
and
H2(z) =
1
1 −A1z−1 −... −ANz−N .
The overall transfer function is
H(z) = H1(z)H2(z) = H2(z)H1(z).
It means that these two blocks can interchange their positions. After the
positions are interchanged, then by using the same delay systems, we get
the resulting system in the direct realization II form, presented in Fig.6.4.
This system uses a reduced number of delay blocks in the realization.
Example 6.1. Find the transfer function of a discrete system presented in Fig.6.5.
⋆The system can be recognized as a direct realization II form. After
its blocks are separated and interchanged the system in a form presented in
Fig.6.6 is obtained.
The output of the ﬁrst block is
y1(n) = x(n) −1
2 x(n −1) + 1
3 x(n −2).
(6.2)
Its transfer function is
H1(z) = 1 −1
2z−1 + 1
3z−2.

270
Realization of Discrete Systems
z-1
z-1
B1
A1
B0
+
+
B2
A2
z-1
z-1
BM
AN
z-1
z-1
x(n)
y(n)
+
+
+
+
Figure 6.4
Direct realization II of a discrete-time system.
z-1
z-1
-1/2
 
 
+
+
1/3
1/2
z-1
z-1
-1/6
z-1
x(n)
y(n)
+
+
Figure 6.5
A discrete-time system.
The output of the second block is described by the following difference
equation
y(n) = 1
2y(n −2) −1
6y(n −3) + y1(n).
(6.3)
The transfer function of this block is
H2(z) =
1
1 −1
2z−2 + 1
6z−3 .

Ljubiša Stankovi´c
Digital Signal Processing
271
z-1
z-1
-1/2
 
1/3
1/2
z-1
z-1
-1/6
z-1
x(n)
y(n)
+
+
+
+
 y1(n)
Figure 6.6
System with interchanged blocks.
The difference equation for the whole system is obtained after y1(n) from
(6.2) is replaced into (6.3)
y(n) = 1
2y(n −2) −1
6y(n −3) + x(n) −1
2 x(n −1) + 1
3 x(n −2).
The system transfer function is
H(z) = H1(z)H2(z) = 1 −1
2z−1 + 1
3z−2
1 −1
2z−2 + 1
6z−3 .
6.1.3
Sensitivity of the System Poles/Zeros to Errors in Coefﬁcients
Systems with a large number of elements in a recursion may be sensitive to
the errors due to the coefﬁcients deviations. Deviations of the coefﬁcients
from the true values are caused by ﬁnite order registers used to memorize
them in a computer. Inﬂuence of the ﬁnite register lengths to the signal and
system realization will be studied later, as a part of random disturbance.
Here, we will only consider inﬂuence of this effect to the system coefﬁcients
since it may inﬂuence the way how to realize a discrete system.
For the ﬁrst-order system with a real-valued pole
H(z) =
1
1 + A1z−1 =
1
1 −zp1z−1

272
Realization of Discrete Systems
the error in coefﬁcient A1 is the same as the error in the system pole zp1. If
the coefﬁcient is quantized with a step ∆then the error in the pole location
is of order ∆. The same holds for the system zeros.
For a second-order system with real-valued coefﬁcients and a pair of
complex-conjugated poles
H(z) =
1
1 + A1z−1 + A2z−2 =
1
(1 −zp1z−1)(1 −zp2z−1)
the relation between the coefﬁcients and the real and imaginary parts of the
poles zp1/2 = xp ± jyp is
H(z) =
1
1 −2xpz−1 + (x2p + y2p)z−2
A1 = −2xp
A2 = x2
p + y2
p.
The error in coefﬁcient A1 deﬁnes the error in the real part of poles xp.
When the coefﬁcient A2 assumes discrete values A2 = m∆, with A1 ∼
xp = n∆then the imaginary part of poles may assume the values yp =
±
F
A2 −x2p = ±
√
m∆−n2∆2 with n2 ≤mN. For small n, i.e., for small real
part of a pole, yp = ±
√
∆m. For N discretization levels, assuming that the
poles are within the unit circle x2
p + y2
p ≤1, the ﬁrst discretization step is
changed from 1/N order to 1/
√
N order. The error, in this case, could be
signiﬁcantly increased. The changes in yp due to the discretization of A2
may be large.
The quantization of xp and yp as a result of quantization of −A1/2
and A2 = x2
p + y2
p is shown in Fig.6.7 for the case of N = 16 and N = 32
quantization levels. We see that the error in yp, when it assumes small
values, can be very large. We can conclude that the poles close to the unit
circle with larger imaginary values yp are less sensitive to the errors. The
highest error could appear if a second order real-valued pole (with yp = 0)
were implemented by using a second order system.
We have concluded that the poles close to the real axis (small yp)
are sensitive to the error in coefﬁcients even in the second order systems.
The sensitivity increases with the system order, since the higher powers in
polynomial increase the maximal possible error.
Consider a general form of a polynomial in the transfer function,
written in two forms
P(z) = zM + zM−1A1 + ... + AM

Ljubiša Stankovi´c
Digital Signal Processing
273
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
xp=Re{zp }
yp=Im{zp }
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
xp=Re{zp }
yp=Im{zp }
Figure 6.7
Quantization of the real and imaginary parts xp = Re{zp} and yp = Im{zp} of
poles (zeros) as a result of the quantization in 16 levels (left) and 32 levels (right) of the
coefﬁcients A1 = −2xp and A2 = x2
p + y2
p.
and
P(z) = (z −z1)(z −z2)...(z −zM).
If the coefﬁcients A1, A2,..., AM are changed for small ∆A1, ∆A2,...,∆AM
(due to quantization) then the pole position (without loss of generality and
for notation simplicity consider the pole z1) is changed for
∆z1 ∼=
- ∂z1
∂A1
∆A1 + ∂z1
∂A2
∆A2 + ... + ∂z1
∂AM
∆AM
.
|z=z1
.
(6.4)
Since there is no a direct relation between z1 and A1 we will ﬁnd ∂z1/∂Ai
using
∂P(z)
∂Ai
|z=z1
= ∂P(z)
∂z1
∂z1
∂Ai |z=z1
.
From this relation it follows
∂z1
∂Ai |z=z1
=
∂P(z)
∂Ai |z=z1
∂P(z)
∂z1 |z=z1
=
zM−i
1
−(z1 −z2)(z1 −z3)...(z1 −zM).
The coefﬁcients ∂z1/∂Ai|z=z1 could be large, especially in the case when
there are close poles, with a small distance (zi −zk).

274
Realization of Discrete Systems
Example 6.2. Consider a discrete system
H(z) =
1
P(z)
with
P(z) = (z −12
27)(z −7
29)(z −111
132)(z −95
101)
∼= (z −0.4444)(z −0.2414)(z −0.8409)(z −0.9406)
In the realization of this system the coefﬁcients are rounded to two decimal
positions, with absolute error up to 0.005. Find the poles of the system with
rounded coefﬁcients.
⋆The system denominator is
P(z) ∼= z4 −2.4673z3 + 2.1200z2 −0.7336z + 0.0849.
With rounded coefﬁcients to two decimal positions we get
ˆP(z) = z4 −2.47z3 + 2.12z2 −0.73z + 0.08
with poles
ˆP(z) = (z −0.2045)(z −0.5370)(0.7285)(z −1).
The poles of the function with rounded coefﬁcients can differ signiﬁcantly
from the original pole values. Maximal error in poles is 0.115. One pole is on
the unit circle making the system with rounded coefﬁcients unstable, in this
case.
Note that if the system is written as a product of the ﬁrst-order func-
tions in the denominator and each pole value is rounded to two decimals
H(z) =
1
(z −7
29)(z −12
27)(z −111
132)(z −95
101)
P(z) ∼= (z −0.24)(z −0.44)(z −0.84)(z −0.94)
the poles will differ from the original ones for no more than 0.005.
If the poles are grouped into the second-order terms (what should be
done if the coefﬁcients were complex-conjugate in order to avoid calculation
with complex valued coefﬁcients), then
P(z) ∼= (z −0.6858z + 0.1073)(z −1.7815z + 0.7910).
If the coefﬁcients are rounded to two decimal positions
ˆP(z) = (z −0.69z + 0.11)(z −1.78z + 0.79)

Ljubiša Stankovi´c
Digital Signal Processing
275
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Re{z}
P(z)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Re{z}
P1(z)P2(z)
Figure 6.8
Poles for a system with errors in coefﬁcients
we will get
ˆP(z) = (z −0.25)(z −0.44)(z −0.8442)(z −0.9358)
with maximal error of 0.01.
The pole values are illustrated in Fig.6.8.
The sensitivity analysis for this example can be done for each pole.
Assume that the poles are denoted as z1 = 12/27, z2 = 7/29, z3 = 111/132
and z4 = 95/101. Then
(z1 −z2)(z1 −z3)(z1 −z4) = 0.0399
∂z1
∂A1 |z=z1
=
z4−1
1
−(z1 −z2)(z1 −z3)(z1 −z4) = −2.1979
∂z1
∂A2 |z=z1
=
z4−2
1
−(z1 −z2)(z1 −z3)(z1 −z4) = −4.9452,
∂z1
∂A3 |z=z1
= −11.1267,
∂z1
∂A4 |z=z1
= −25.0350

276
Realization of Discrete Systems
with the errors in the coefﬁcients
∆A1 = −2.4673 −(−2.47) = 0.0027,
∆A2 = 2.12 −2.12 = 0,
∆A3 = −0.7336 −(−0.73) = −0.0036,
∆A4 = 0.0849 −0.08 = 0.0049.
Replacing these values into (6.4) the approximation of the error is
∆z1 ∼= 0.0878.
The true error is ∆z1 = 0.0926. A small difference is due to the linear approx-
imation, assuming small ∆Ai. The obtained result is a good estimate of an
order of error for the pole z1. The error in z1 is about 18.5 time greater than
the maximal error in the coefﬁcients Ai, that is of order 0.005.
6.1.4
Cascade Realization
A transfer function of discrete-time system might be written as a product of
the ﬁrst-order subsystems
H(z) = k 1 −zo0z−1
1 −zp0z−1 × 1 −zo1z−1
1 −zp1z−1 ... × 1 −zoNz−1
1 −zpNz−1 .
Commonly real-valued signals are processed and the poles and zeros in the
transfer function are in complex-conjugated pairs. In that case it is better to
group these pairs into second order systems to avoid complex calculations.
The transfer function is of the form
H(z) = B00 + B10z−1 + B20z−2
1 −A10z−1 −A20z−2 × ... × B0K + B1Kz−1 + B2Kz−2
1 −A1Kz−1 −A2Kz−2
= H0(z)H1(z)...HK(z),
where
Hi(z) = B0i + B1iz−1 + B2iz−2
1 −A1iz−1 −A2iz−2
are second-order systems with real-valued coefﬁcients. The whole system
may be realized as a cascade of lower-order (ﬁrst or second-order) systems,
Fig.6.9. Of course, if there are some real-valued poles then there is no need
to group them. It is better to keep the realization order of the subsystems as
low as possible.

Ljubiša Stankovi´c
Digital Signal Processing
277
z-1
z-1
B10
A10
B00
+
+
B20
A20
z-1
z-1
z-1
z-1
B1K
A1K
B0K
+
+
B2K
A2K
z-1
z-1
x(n)
y(n)
+
+
+
+
Figure 6.9
Cascade realization of a discrete-time system.
H(z)
+
-
y(n)
x(n)
H(z)
r(n)
Figure 6.10
System with a feedback.
In the realization the second-order subsystems are commonly used.
It is possible to realize these second-order subsystems using the ﬁrst-order
systems with real-valued coefﬁcients xpL and ypL that are real and imag-
inary parts of the complex-conjugated pair of poles, zpL = xpL ± jypL, re-
spectively. To this aim consider ﬁrst an example.
Example 6.3. Find the transfer function of a system with a feedback shown in
Fig.6.10.
⋆The z-transform of the signal at the output of adder is
R(z) = X(z) −H(z)Y(z).
The output signal z-transform is
Y(z) = H(z)R(z) = H(z)X(z) −H2(z)Y(z).
The transfer function of this system is
He(z) = Y(z)
X(z) =
H(z)
1 + H2(z).

278
Realization of Discrete Systems
H(z)
+
-
y(n)
x(n)
H(z)
H2(z)
z -1
+
+
xp
Figure 6.11
Complete second-order subsystem with complex-conjugate pair of poles realized
using the ﬁrst-order systems.
Let us now consider a realization of the second-order subsystem of the
form
Qi(z) =
ypLz−1
1 + A1iz−1 + A2iz−2 .
Using the real and imaginary parts of the poles zpL = xpL + jypL the transfer
function can be expressed as
Qi(z) =
ypLz−1
1 −2xpLz−1 + x2
pLz−2 + y2
pLz−2
=
ypLz−1
(1 −xpLz−1)2 + y2
pLz−2
= ypLz−1
1
(1 −xpLz−1)2
1
1 +
*
ypLz−1
1−xpLz−1
+2
= H(z)H2(z)
1 + H2(z)
where
H(z) =
ypLz−1
1 −xpLz−1
and
H2(z) =
1
1 −xpLz−1 .
Therefore the second-order system can be implemented as in Fig.6.11, by
using the ﬁrst-order systems as in Fig.6.12. In this case there is no grouping
of the coefﬁcients into second or higher order polynomial.

Ljubiša Stankovi´c
Digital Signal Processing
279
xp
+
+
y(n)
x(n)
z -1
H(z)
yp
Figure 6.12
First-order system for the realization of the second-order system with complex-
conjugate pair of poles.
The error in one coefﬁcient (real or imaginary part of a pole) does not
inﬂuence the other coefﬁcients. However if an error in the signal calculation
happens in one cascade, then it will propagate as an input to the following
cascades. In that sense it would be the best to order cascades in such a way
that the lowest probability of an error appears in early cascades. From the
analysis of error we can conclude that the cascades with the poles and zeros
close to the origin are more sensitive to the error and should be used in later
stages.
Example 6.4. For the system
H(z) =
1.4533(1 + z−1)3
(−0.8673z−1 + 3.1327)(3.0177z−2 −5.434z−1 + 7.54)
= 0.0615
1 + z−1
1 −0.2769z−1 ×
1 + 2z−1 + z−2
1 −0.7207z−1 + 0.4002z−2
present the cascade realization using:
(a) both the ﬁrst and the second-order systems;
(b) the ﬁrst-order systems with real-valued coefﬁcients only.
⋆(a) Realization of the system H(z) when both the ﬁrst and the
second-order subsystems can used is done according to system transfer func-
tion as in Fig.6.13.
(b) For the ﬁrst-order systems the realization should be done based on
H(z) = 0.0615
1 + z−1
1 −0.2769z−1 × (1 + z−1) × (1 + z−1)
×
1
1 −0.7207z−1 + 0.4002z−2 ,

280
Realization of Discrete Systems
z-1
z-1
0.2769
1
+
+
z-1
z-1
2
0.7207
+
+
1
-0.4002
0.0615
z-1
z-1
x(n)
y(n)
+
+
Figure 6.13
Cascade realization of a system.
with
1
1 −0.7207z−1 + 0.4002z−2
=
1
(1 −(0.3603 + j0.5199)z−1)(1 −(0.3603 −j0.5199)z−1)
=
1
1 −2 × 0.3603z−1 + 0.36032z−2 + 0.51992z−2
=
1
0.51992z−2 + (1 −0.3603z−1)2 =
1
(1 −0.3603z−1)2
1
1 + (
0.5199z−1
1−0.3603z−1 )2 .
In this way the system can be written and realized in terms of the ﬁrst-order
subsystems
H(z) = 0.0615
1 + z−1
1 −0.2769z−1 ×
1 + z−1
1 −0.3603z−1
×
1 + z−1
1 −0.3603z−1 ×
1
1 +
0.5199z−1
1−0.3603z−1 ×
0.5199z−1
1−0.3603z−1
.
6.1.5
Parallel realization
This realization is implemented based on a transfer function written in the
form
H(z) = B00 + B10z−1 + B20z−2
1 −A10z−1 −A20z−2 + ... + B0K + B1Kz−1 + B2Kz−2
1 −A1Kz−1 −A2Kz−2
= H0(z) + H1(z) + ... + HK(z).

Ljubiša Stankovi´c
Digital Signal Processing
281
z-1
z-1
0.2769
+
+
z-1
z-1
0.3603
+
+
0.0615
x(n)
y(n)
z-1
z-1
0.3603
0.3603
0.3603
yp
yp
yp=0.5199
+
+
+
+
+
z-1
z-1
Figure 6.14
Discrete-time system realized using ﬁrst-order subsystems.
In the case of a parallel realization the error in one subsystem does not
inﬂuence the other subsystems. If an error in the signal calculation appears
in one parallel subsystem, then it will inﬂuence the output signal, but will
not inﬂuence the outputs of other parallel subsystems.
Example 6.5. For the system
H(z) =
−0.7256 + 0.2542z−1
1 −1.1078z−1 + 0.5482z−2 +
0.7256 −0.084z−1
1 −0.9246z−1 + 0.2343z−2
present a parallel and a cascade realization using the second-order subsys-
tems.
⋆Parallel realization follows directly from the system transfer function
deﬁnition. It is presented in Fig.6.16.
For the cascade realization the system transfer function should be
written in a form of the product of second-order transfer functions,

282
Realization of Discrete Systems
z-1
z-1
B10
A10
B00
+
+
B20
A20
z-1
z-1
z-1
z-1
B11
A11
B01
+
+
B21
A21
z-1
z-1
z-1
z-1
B1K
A1K
B0K
+
+
B2K
A2K
z-1
z-1
x(n)
y(n)
+
+
+
+
+
+
+
Figure 6.15
Parallel realization of a discrete-time system.

Ljubiša Stankovi´c
Digital Signal Processing
283
z-1
z-1
-0.7256
0.2542
1.1078
+
+
-0.5482
z-1
z-1
z-1
z-1
0.7256
-0.084
 
0.9246
-0.2343
+
+
z-1
z-1
x(n)
y(n)
Figure 6.16
Parallel realization of a discrete-time system.
H(z) =
0.0373z−1 + 0.0858z−2 + 0.0135z−3
P
1 −1.1078z−1 + 0.5482z−2QP
1 −0.9246z−1 + 0.2343z−2Q
=
z−1
1 −1.1078z−1 + 0.5482z−2
×0.0373 + 0.0858z−1 + 0.0135z−2
1 −0.9246z−1 + 0.2343z−2
.
Cascade realization is presented in Fig.6.17.
6.1.6
Inverse realization
For each of the previous realization an inverse form may be implemented by
switching the input and the output signal and changing the ﬂow directions
of the signal. As an example consider the direct realization II from Fig.6.4.
This realization, with separated delay circuits is shown in Fig.6.18. Its in-
verse form is presented in Fig.6.19. It is easy to conclude that the inverse
realization of the direct realization II has the same transfer function as the
direct realization I. Since both realization I and realization II have the same

284
Realization of Discrete Systems
z-1
z-1
1
1.1078
+
+
-0.5482
z-1
z-1
z-1
z-1
0.0373
0.0858
0.9246
+
+
0.0135
-0.2343
z-1
z-1
x(n)
y(n)
+
+
Figure 6.17
Cascade realization of a discrete system.
z-1
z-1
B1
B0
A1
B2
A2
z-1
z-1
BM
AN
z-1
z-1
+
+
+
+
+
+
x(n)
y(n)
Figure 6.18
Direct realization II with separated delay circuits.
transfer functions it follows that the inverse realization has the same transfer
function as the original realization.
6.2
FIR SYSTEMS AND THEIR REALIZATIONS
In general, transfer functions of discrete systems are obtained in the form
of a ratio of two polynomials. The polynomial in the transfer function
denominator deﬁnes poles. In the time domain it means a recursive relation,
relating the output signal at the current instant with the previous output
signal values. Realization of this kind of system is efﬁcient, as described

Ljubiša Stankovi´c
Digital Signal Processing
285
z-1
z-1
B1
B0
A1
B2
A2
z-1
z-1
BM
AN
z-1
z-1
y(n)
x(n)
+
+
+
+
+
+
Figure 6.19
Inverse realization of the direct realization II.
in the pervious section. Systems that would not have recursions, when the
output signal is a linear combination of the input signal and its delayed
versions only,
y(n) = B0x(n) + B1x(n −1) + ... + BMx(n −M)
are the FIR systems. These systems are always stable. The FIR systems can
also have a linear phase.
6.2.1
Linear Phase Systems and Group Delay
In an implementation of a discrete system it is important to modify the
amplitude of the Fourier transform of the input signal in a desired way. At
the same time we should take care about the phase function changes in the
input signal. In an ideal case of signal ﬁltering the phase function should
remain the same, meaning a zero-phase transfer function. A linear phase
form of the transfer function
arg{H(ejω} = arctan{Im{H(ejω}
Re{H(ejω} } = −ωq
(6.5)
is also acceptable in these systems. They will have a constant group delay
τg = −d(arg{H(ejω})
dω
= q

286
Realization of Discrete Systems
and will not distort the impulse response with respect to the zero-phase
system. The impulse response will only be delayed in time for q.
Example 6.6. Consider an input signal of the form
x(n) =
M
∑
m=1
Amej(ωmn+θm).
After passing through a system with frequency response H(ejω) this signal is
changed to
y(n) =
M
∑
m=1
Am|H(ejωm)|ej(ωmn+θm+arg{H(ejωm }).
In general the phase of each signal component is changed in a different
way for arg{H(ejωm}, causing the signal distortion due to different delays
corresponding to different frequencies. If the phase function of the frequency
response is linear then all signal component phases are changed in the same
way for arg{H(ejωm} = −ωmq. They corresponding to a constant delay for
all components. A delayed signal, without distortion, is obtained
y(n) =
M
∑
m=1
Ai|H(ejωm)|ej(ωm(n−q)+θm).
In the case of a linear phase arg{H(ejω} = −ωq the phase delay
τϕ = −arg{H(ejω}
ω
= q
and the group delay τg are the same. In general, the group delay and the
phase delays are different. The group delay, as notion dual to the instanta-
neous frequency, is introduced and discussed in the ﬁrst chapter.
Consider a system with a real-valued impulse response h(n). Its fre-
quency response is
H(ejω) =
N−1
∑
n=0
h(n)e−jωn =
N−1
∑
n=0
h(n)cos(ωn) −j
N−1
∑
n=0
h(n)sin(ωn).
(6.6)
Combining the linear phase condition (6.5) with form (6.6), we get
−tan(ωq) = Im{H(ejω}
Re{H(ejω} = −∑N−1
n=0 h(n)sin(ωn)
∑N−1
n=0 h(n)cos(ωn)
,

Ljubiša Stankovi´c
Digital Signal Processing
287
or
N−1
∑
n=0
h(n)[sin(ωq)cos(ωn) −cos(ωq)sin(ωn)] = 0.
The last equation can be written as
N−1
∑
n=0
h(n)sin(ω(n −q)) = 0.
(6.7)
The middle point of interval where h(n) ̸= 0 is n = (N −1)/2. If q = (N −
1)/2, then sin(ω(n −q)) is an odd function with respect to n = (N −1)/2.
The summation (6.7) is zero if the impulse response h(n) is an even function
with respect to n = (N −1)/2. Hence, the solution of (6.7) is
q = N −1
2
h(n) = h(N −1 −n), 0 ≤n ≤N −1.
Since the Fourier transform is unique, this is the unique solution for the
linear phase condition. It is illustrated for an even and odd N in Fig.6.20.
From the symmetry condition it is easy to conclude that there is no a causal
linear phase system with inﬁnite impulse response.
6.2.2
Windows
When a system obtained from the design procedure is an IIR system and
the requirement is to implement it as an FIR system, in order to get a linear
phase or to guaranty the system stability (when small changes of coefﬁcients
are possible), then the most obvious way is to truncate the desired impulse
response hd(n) of the resulting IIR system. The impulse response of the FIR
system is
h(n) =
!
hd(n)
for
0 ≤n ≤N −1
0
elsewhere.
This form can be written as
h(n) = hd(n)w(n),
where
w(n) =
!
1
for
0 ≤n ≤N −1
0
elsewhere

288
Realization of Discrete Systems
0
16
32
n
h(n)
 N=32
 q=16
0
16.5
33
n
h(n)
 N=33
 q=16.5
Figure 6.20
Impulse response of a system with a linear phase for an even and odd N.
is the rectangular window function. In the Fourier domain the desired im-
pulse response truncation by a window function will mean a convolution of
the desired frequency response with the frequency response of the window
function
H(ejω) = Hd(ejω) ∗W(ejω).
Since the rectangular window function has a Fourier transform of the
form
W(ejω) =
N−1
∑
n=0
e−jωn = e−jω(N−1)/2 sin(ωN/2)
sin(ω/2)
its convergence is slow with signiﬁcant oscillations. It will cause oscilla-
tions in the resulting frequency response H(ejω), Fig.6.21. By increasing the
number of samples N the convergence speed will increase. However the
oscillations amplitude will remain the same, Figs.6.21 (d) and (f). Even with
N →∞the amplitude oscillations will remain, Figs.6.21 (b). This effect is
called the Gibbs phenomenon.
Example 6.7. A desired frequency response of a system is Hd(ejω) with the IIR
hd(n) for −∞< n < ∞. Find the FIR system impulse response hc(n) that
approximates the desired transfer function with a minimal mean absolute
squared error.

Ljubiša Stankovi´c
Digital Signal Processing
289
⋆The mean squared absolute error is
e2 = 1
2π
π
"
−π
'''Hd(ejω) −Hc(ejω)
'''
2
dω.
According to Parseval’s theorem
e2 = 1
2π
π
"
−π
'''Hd(ejω) −Hc(ejω)
'''
2
dω =
∞
∑
n=−∞
|hd(n) −hc(n)|2
Without loss of generality, assume that the most signiﬁcant values of hd(n)
are within −N/2 ≤n ≤N/2 −1. The impulse response hc(n) can assume
nonzero values only within −N/2 ≤n ≤N/2 −1. Therefore,
e2 =
N/2−1
∑
n=−N/2
|hd(n) −hc(n)|2 +
−N/2−1
∑
n=−∞
|hd(n)|2 +
∞
∑
n=N/2
|hd(n)|2 .
Since the last two terms are hc(n) independent and all three terms are
non negative, the error e2 is minimal if
hc(n) = hd(n), −N/2 ≤n ≤N/2 −1.
If we want to have a causal realization of the FIR system then
h(n) = hc(n −N/2).
A shift in time does not change the amplitude of the desired frequency
response, since
'''H(ejω)
''' =
'''Hc(ejω)
'''.
In order to reduce the oscillations in frequency response amplitude
other windows are introduced. They are presented within the introductory
chapters, trough the examples. Here we will list the basic windows (for
more details see Section 9.2).
Triangular (Bartlett) window is deﬁned as
w(n) =
%
1 −|n+1−N/2|
N/2
for
0 ≤n ≤N −1
0
elsewhere
.
Avoiding window discontinuities at the ending points the convergence
of its transform is improved. Since this window may be considered as a
convolution of two rectangular windows
w(n) =
1
N/2[u(n) −u(n −N/2)] ∗n [u(n) −u(n −N/2)]

290
Realization of Discrete Systems
-60
-40
-20
0
20
40
60
0
0.2
n
hd(n)
  (a)
0
0
0.5
1
 ω
Hd( ejω )
- π/4
π/4
- π
π
   (b)
-40
-20
0
20
40
60
0
0.2
n
h(n)
  (c)
0
0
0.5
1
 ω
 |H( ejω )|
- π/4
π/4
- π
π
   (d)
-20
0
20
40
60
0
0.2
n
h(n)
  (e)
0
0
0.5
1
 ω
 |H( ejω )|
- π/4
π/4
- π
π
   (f)
-20
0
20
40
60
0
0.2
n
h(n)
  (g)
0
0
0.5
1
 ω
 |H( ejω )|
- π/4
π/4
- π
π
   (h)
Figure 6.21
Impulse response of a FIR system obtained by truncating the desired IIR response
(a), (b) using two rectangular window of different widths (c)-(f), and using a Hann(ing)
window (g),(h).

Ljubiša Stankovi´c
Digital Signal Processing
291
its Fourier transform is a product of corresponding rectangular window
Fourier transforms
W(ejω) =
1
N/2e−jω(N/2−1) sin2(ωN/4)
sin2(ω/2) .
Hann(ing) window deﬁned by
w(n) =
!
1
2
P
1 + cos((n −N/2) 2π
N
Q
for
0 ≤n ≤N −1
0
elsewhere
would be continuous in the continuous-time domain. In that domain its ﬁrst
derivative would be continuous as well. Thus, its Fourier domain conver-
gence is further improved with respect to the rectangular and the Bartlett
windows. The Fourier transform of this window is related to the Fourier
transform of the rectangular window as W(ejω)/2 + W(ej(ω+2π/N)/4 +
W(ej(ω−2π/N)/4.
Hamming window is a slight modiﬁcation of the Hann(ing) window
w(n) =
!
0.52 + 0.48cos((n −N/2) 2π
N
for
0 ≤n ≤N −1
0
elsewhere.
It loses the continuity property (in the continuous-time domain). Its con-
vergence for very large values of ω will be slower than in the Hann(ing)
window case. However, as it will be shown later, its coefﬁcients are derived
in such a way that the ﬁrst side-lobe is canceled out at its mid point. Then
the immediate convergence, after the main lobe, is much better than in the
Hann(ing) window case.
Other windows are derived with various constraints. Some of them
will be reviewed in Part three of this book as well.
6.2.3
Design of FIR System in the Frequency Domain
Suppose that the desired system frequency response is given in the fre-
quency domain. If we want to get an N point FIR system that approximates
the desired frequency response then it can be obtained by sampling the fre-
quency response Hd(ejω) at
ω = 2π
N k, k = 0,1,2,..., N −1
H(k) = Hd(ejω)|ω=2πk/N
h(n) = IDFT{H(k)}.

292
Realization of Discrete Systems
-2
0
2
-0.5
0
0.5
1
1.5
 ω
Hd(ejω),  Hd(k)
n
h(n)
-2
0
2
-0.5
0
0.5
1
1.5
 ω
H(ejω),  H(k)
-2
0
2
-0.5
0
0.5
1
1.5
 ω
Hd(ejω),  Hd(k)
n
h(n)
-2
0
2
-0.5
0
0.5
1
1.5
 ω
H(ejω),  H(k)
Figure 6.22
Realization of a FIR system with N samples in time, obtained by sampling the
desired frequency response with N samples. A direct sampling (left) and the sampling with
smoothed transition (right),
This procedure is illustrated on a lowpass ﬁlter design, Fig.6.23. Note
that at the discontinuity points high oscillation will occur in the resulting
H(ejω). The oscillations can be avoided by smoothing the transition inter-
vals. Smoothing by a Hann(ing) window in the frequency domain is shown
in Fig.6.23.

Ljubiša Stankovi´c
Digital Signal Processing
293
-16 -12
-8
-4
0
4
8
12
0
0.25
0.5
 k
WH(k)
n
w(n)
Figure 6.23
A Hann(ing) window for smoothing the frequency response in the frequency
domain (left) and in the time domain (right).
z-1
z-1
z-1
y(n)
 h(0)
 h(1)
 h(2)
 h(N-1)
+
+
+
x(n)
x(n-1)
x(n-2)
x(n-N+1)
Figure 6.24
Direct realization of a FIR system.
6.2.4
Realizations of FIR system
The FIR systems can be realized in the same way as the IIR systems pre-
sented in the previous section, without using the recursive coefﬁcients. A
common way of presenting a direct realization of FIR system is shown in
Fig.6.24. It is often referred to as an adder with weighted coefﬁcients h(n).
A realization of liner phase FIR system that uses the coefﬁcients sym-
metry h(0) = h(N −1), h(2) = h(N −2),... is shown in Fig.6.25.
Realization of a frequency sampled FIR ﬁlter may be done using the
relation between the z−transform and the DFT of a signal.
If we want to realize a FIR system with N nonzero samples, then it
can be expressed in term of the DFT of frequency response (samples of the
transfer function H(z) along the unit circle) as follows. For a FIR ﬁlter we

294
Realization of Discrete Systems
z-1
z-1
z-1
z-1
z-1
z-1
y(n)
 h(0)
 h(1)
 h(2)
 h(N/2-1)
+
+
+
+
+
+
+
z-1
x(n)
x(n-1)
x(n-2)
x(n-(N/2-1))
x(n-N+1)
Figure 6.25
Direct realization of a FIR system with a linear phase.
may write
H(k) =
N−1
∑
n=0
h(n)e−j2πnk/N
h(n) = 1
N
N−1
∑
k=0
H(k)ej2πnk/N.
Then the transfer function H(z) using the values of h(n), 0 ≤n ≤N −1, is
H(z) = 1
N
N−1
∑
k=0
N−1
∑
n=0
H(k)ej2πnk/Nz−n = 1
N
N−1
∑
k=0
1 −z−Nej2πk
1 −z−1ej2πk/N H(k)
with H(k) = H(z) for z = exp(j2πk/N),k = 0,1,2,..., N −1.
Example 6.8. For a system whose impulse response is the Hamming window
function of the length N = 32 present the FIR ﬁlter based realization.
⋆For the Hamming window with N = 32 the impulse response is
given by
h(n) = 0.52 + 0.48cos((n −16) π
16)
0 ≤n ≤31.

Ljubiša Stankovi´c
Digital Signal Processing
295
The DFT values are H(0) = 0.52 × 32, H(1) = −0.24 × 32, H(31) = H(−1) =
−0.24 × 32 and H(k) = 0 for other k within 0 ≤k ≤31. Therefore
H(z) = 1
32
1 −z−32
1 −z−1 H(0)−1
32
1 −z−32ej2π
1 −z−1ej2π/32 H(1)−1
32
1 −z−32e−j2π
1 −z−1e−j2π/32 H(31)
= 1
32(1 −z−32)
(
H(0)
1 −z−1 −2H(1)(1 −cos(π/16)z−1)
1 −2cos(π/16)z−1 + z−2
)
.
This is a cascade of
H1(z) = (1 −z−32)/32
and a system H2(z) + H3(z) where
H2(z) = H(0)/(1 −z−1)
and
H3(z) = −2H(1)
1 −cos(π/16)z−1
1 −2cos(π/16)z−1 + z−2 .
Example 6.9. For a system whose frequency response Hd(jΩ) in the continuous-
time domain is
Hd(jΩ) = π −|Ω|
for |Ω| ≤π, with corresponding Hd(ejω) in the discrete-time domain (∆t = 1
is assumed, Fig.6.26) ﬁnd the FIR ﬁlter impulse response with N = 7 and
N = 8 using:
(a) Sampling the desired frequency response Hd(ejω) in the frequency
domain.
(b) Calculating hd(n) = IFT{Hd(ejω)} and taking its N the most signif-
icant values, h(n) = hd(n) for −N/2 ≤n ≤N/2 −1 and h(n) = 0 elsewhere.
(c) Comment the error in both cases.
⋆(a) The sampling in frequency domain is illustrated in Fig.6.26. The
values of the FIR system, in this case, are the samples of Hd(ejω),
H(k) = Hd(ejω)
'''
ω=2πk/N =
! π(1 −2 k
N )
for
0 ≤k < N/2
π(2 k
N −1)
for
N/2 ≤k ≤N −1
.
The sampling is illustrated in the second row of Fig.6.26 for N = 7 and N = 8.
Impulse response of the FIR ﬁlter is
h(n) = IDFT{H(k)}
= 1
N
N−1
∑
k=0
H(k)ej2πnk/N.

296
Realization of Discrete Systems
For N = 7
h(n) = π
7 + 10π
49 cos(2π
7 n) + 6π
49 cos(22π
7 n) + 2π
49 cos(32π
7 n)
0 ≤n ≤6.
For N = 8
h(n) = π
8 + 3π
16 cos(2π
8 n) + π
8 cos(22π
8 n) + π
16 cos(32π
8 n)
0 ≤n ≤7.
It is shown in Fig.6.26 (third row). The frequency response of the FIR ﬁlter is
H(ejω) = FT{h(n)}.
Its values are equal to the desired frequency response at the sampling points
H(ejω)
'''
ω=2πk/N = Hd(ejω)
'''
ω=2πk/N .
Outside these point the frequency responses signiﬁcantly differ (calculate,
for example the values H(ej0), H(ejπ/2), and H(ejπ)). Here, there is no sig-
niﬁcant discontinuity in the frequency response. It means that the frequency
response smoothing, by using a window (Hann(ing) or Hamming window
in the time domain), would not improve the result.
(b) The impulse response of the desired system is
hd(n) = IFT{Hd(ejω)} = 1
2π
π
"
−π
(π −|ω|)ejωndω
= 2
2π
π
"
0
(π −ω)cos(ωn)dω =
= 1 −cos(nπ)
πn2
.
Using the ﬁrst N = 7 samples in the time domain we get
h(n) =
%
1−cos(nπ)
πn2
for
−3 ≤n ≤3
0
elsewhere.
or for N = 8
h(n) =
%
1−cos(nπ)
πn2
for
−4 ≤n ≤3
0
elsewhere.
.
The frequency response of this FIR ﬁlter is
H(ejω) = FT{h(n)}.

Ljubiša Stankovi´c
Digital Signal Processing
297
-5
0
5
0
1
2
3
- π
π
Hd(jΩ)
-5
0
5
0
1
2
3
- π
π
Hd(ejω)
-5
0
5
0
1
2
3
H(k),   N=7
0
2
4
6
-1
0
1
2
h(n),   N=7
-5
0
5
0
1
2
3
H(ejω),   N=7
-5
0
5
0
1
2
3
H(k),   N=8
0
2
4
6
-1
0
1
2
h(n),   N=8
-5
0
5
0
1
2
3
H(ejω),   N=8
Figure 6.26
Design of a FIR ﬁlter by frequency sampling of the desired frequency response.
It is shown in Fig.6.27.
(c) The error in frequency sampling (a) is zero at the desired frequency
points. However, since the frequency response is equal to the samples of
the impulse response of an inﬁnite duration there will be aliasing of the
impulse response, resulting in the error outside the sampling points. For the
case of windowing the impulse response (b), the aliasing in the frequency
response is avoided since the impulse response is truncated. However, the
truncation causes an error in the resulting frequency response. In this case
the error distribution is not the same as in case (a). The mean square error Er

298
Realization of Discrete Systems
-15
-10
-5
0
5
10
15
-1
0
1
2 hd(n)
-4
-2
0
2
4
0
0.5
1
1.5
2
h(n),   N=7
-5
0
5
0
1
2
3
H(ejω),   N=7
-4
-2
0
2
4
0
0.5
1
1.5
2
h(n),   N=8
-5
0
5
0
1
2
3
H(ejω),   N=8
Figure 6.27
Design of a FIR ﬁlter by windowing the impulse response of an IIR ﬁlter.
is calculated and presented in Fig.6.28, along with the errors in the absolute
value of the frequency responses. As expected from the theory, the impulse
response truncation produced lower mean square error in the estimation.
6.3
PROBLEMS
Problem 6.1. For the system whose transfer function is
H(z) =
16(z + 1)z2
(4z2 −2z + 1)(4z + 3)
plot the cascade, parallel and direct realization.

Ljubiša Stankovi´c
Digital Signal Processing
299
0
1
2
3
4
5
6
-0.2
-0.1
0
0.1
0.2
Er= 0.008092
0
1
2
3
4
5
6
-0.2
-0.1
0
0.1
0.2
Er= 0.0018945
Figure 6.28
Error in the case of the frequency response sampling (top) and the IIR impulse
response truncation (bottom), along with the corresponding mean square error (Er) value.
Problem 6.2. Given a discrete system with
y(n) = x(n) + x(n −1) + x(n −2) + y(n −1) −y(n −2) −3y(n −3).
Plot the direct realization I and II, parallel and cascade realization.
Problem 6.3. Find the transfer function of a discrete system presented in
Fig.6.29.
Problem 6.4. Find the transfer function of a discrete system presented in
Fig.6.30.
Problem 6.5. For the system
H(z) = 1 −0.2z−1 + 0.02z−2
1 −1.7z−1 + 1.285z−2
1 −1.8z−1 + 1.45z−2
1 −0.1z−1 + 0.125z−2 ,
present a cascade realization. Order the system so that the subsystem which
is less sensitive to possible quantization comes ﬁrst.
Problem 6.6. If the transfer function of a system is
H(z) =
4z2
4z2 −2z + 1
4z + 4
4z + 3,

300
Realization of Discrete Systems
z-1
z-1
1/2
2
+
+
+
+
-1/3
1/3
z-1
z-1
z-1
z-1
1/3
+
+
-1/4
z-1
z-1
x(n)
y(n)
+
+
Figure 6.29
Discrete-time system
z-1
z-1
z-1
z-1
1/2
2
+
+
+
+
+
-1/3
1/3
z-1
z-1
z-1
z-1
1/3
+
+
-1/4
z-1
z-1
x(n)
y(n)
+
+
Figure 6.30
Discrete-time system
plot the cascade and parallel realization. Write down the difference equation
which describes this system.
Problem 6.7. For the system deﬁned by the transfer function
H(z) =
1 + z−2
1 + 2z−1 + 2z−2 + z−3
plot the cascade realization.
Problem 6.8. System is deﬁned by
y(n) + 1
4y(n −1) + w(n) + 1
2w(n −1) = 2
3x(n)

Ljubiša Stankovi´c
Digital Signal Processing
301
z-1
z-1
+
+
+
x(n)
-r sinθ
rcosθ
rcosθ
rsinθ
y(n)
 H1 (  z )
 H2 (  z )
Figure 6.31
Discrete-time system
y(n) −5
4y(n −1) + 2w(n) −2w(n −1) = −5
3x(n),
where x(n) is the input signal, y(n) is the output, and w(n) is a signal within
the system. What is the frequency and impulse response of the system?
Problem 6.9. For the system presented in Fig.6.31 ﬁnd the transfer function.
Problem 6.10. Show that the FIR system
H(z) = 1 + 2z −z2 + 4z3 −z4 + 2z5 + z6
z6
has a linear phase function. Find its group delay.
Problem 6.11. Let h(n) be an impulse response of a causal system with the
Fourier transform H(ejω). A real-valued output signal y1(n) = x(n) ∗h(n)
of this system is reversed, r(n) = y1(−n), and passed through the same
system, resulting in the output signal y2(n) = r(n) ∗h(n). The ﬁnal output
is reversed again y(n) = y2(−n). Find the phase of the frequency response
function of the overall system.
Problem 6.12. For a system whose frequency response in the continuous-
time domain is
Hd(jΩ) =
⎧
⎨
⎩
2
for
|ω| < π
2
1
for
π
2 < |ω| < 3π
4
0
elsewhere,

302
Realization of Discrete Systems
z-1
z-1
-1/4
 
1/8
z-1
-3/16
z-1
x(n)
y(n)
+
+
+
+
Figure 6.32
Direct realization I of a discrete-time system.
with the corresponding Hd(ejω) in the discrete-time domain obtained with
∆t = 1, ﬁnd the FIR ﬁlter impulse response with N = 15 and N = 14 using:
(a) Sampling the desired frequency response Hd(ejω) in the frequency
domain,
(b) Calculating hd(n) = IFT{Hd(ejω)} and taking its N the most signif-
icant values, h(n) = hd(n) for −N/2 ≤n ≤N/2 −1 and h(n) = 0 elsewhere.
(c) Comment the sources of error in both cases.
6.4
SOLUTIONS
Solution 6.1. In order to plot the direct form of realization, transfer function
should be written in a form suitable for this type of realization,
H(z) =
16(z + 1)z2
(4z2 −2z + 1)(4z + 3) =
1 + z−1
(1 −1
2z−1 + 1
4z−2)(1 + 3
4z−1)
=
1 + z−1
1 + 1
4z−1 −1
8z−2 + 3
16z−3 .
According to the previous relation, direct realization form I and II
follows. They are presented in Fig.6.32 and Fig.6.33, respectively.

Ljubiša Stankovi´c
Digital Signal Processing
303
z-1
z-1
-1/4
 
+
+
1/8
z-1
z-1
-3/16
z-1
x(n)
y(n)
+
+
Figure 6.33
Direct realization II of a discrete-time system.
z-1
z-1
1/2
+
+
+
-1/4
z-1
z-1
z-1
-3/4
+
x(n)
y(n)
Figure 6.34
Cascade realization of a discrete-time system.
For a cascade realization, the transfer function is written as
H(z) =
1 + z−1
(1 −1
2z−1 + 1
4z−2)(1 + 3
4z−1)
=
1 + z−1
1 −1
2z−1 + 1
4z−2
1
1 + 3
4z−1 = H1(z)H2(z).
The cascade realization, implemented as a product of two blocks, will have
the form as shown in Fig.6.34.

304
Realization of Discrete Systems
z-1
z-1
1/19
22/19
1/2
+
+
+
-1/4
z-1
z-1
z-1
z-1
-3/19
-3/4
+
x(n)
y(n)
Figure 6.35
Parallel realization of a discrete-time system.
In order to plot a parallel realization, the transfer function should be
written in a suitable form for this type of realization,
H(z) =
1 + z−1
(1 −1
2z−1 + 1
4z−2)(1 + 3
4z−1) =
Az−1 + B
1 −1
2z−1 + 1
4z−2 +
C
1 + 3
4z−1 .
Calculating the coefﬁcients A = 1/19, B = 22/19 and C = −3/19, we get
H(z) =
22
19 + 1
19z−1
1 −1
2z−1 + 1
4z−2 +
−3
19
1 + 3
4z−1 .
It is used to plot the parallel realization, Fig.6.35.
Solution 6.2. Using the z-transform properties, the given difference equa-
tion can be written as
Y(z) = X(z) + X(z)z−1 + X(z)z−2 + Y(z)z−1 −Y(z)z−2 −3Y(z)z−3.
According to the deﬁnition of transfer function, follows:
H(z) = Y(z)
X(z) =
1 + z−1 + z−2
1 −z−1 + z−2 + 3z−3 .
Direct realizations I and II, presented in Fig.6.36 and Fig.6.37, respectively,
follow from the previous equation.

Ljubiša Stankovi´c
Digital Signal Processing
305
z-1
z-1
 
+
+
-1
z-1
z-1
1
-3
z-1
x(n)
y(n)
x(n-1)
y(n-1)
y(n-2)
x(n-2)
y(n-3)
+
+
+
Figure 6.36
Direct realization I of a discrete-time system.
z-1
z-1
 
+
+
-1
z-1
z-1
1
-3
z-1
x(n)
y(n)
+
+
+
Figure 6.37
Direct realization II of a discrete-time system.
For a cascade realization, the transfer function should be written as a
product of two blocks
H(z) =
1 + z−1 + z−2
1 −2z−1 + 3z−2
1
1 + z−1 = H1(z)H2(z).
This form is suitable for the cascade realization given in Fig.6.38.
For a parallel realization, we will write the transfer function as
H(z) =
1
6
1 + z−1 +
1
2z−1 + 5
6
1 −2z−1 + 3z−2 .

306
Realization of Discrete Systems
z-1
z-1
2
+
+
+
+
-3
z-1
z-1
z-1
-1
+
x(n)
y(n)
Figure 6.38
Cascade realization of a discrete-time system.
Its realization is now straightforward.
Solution 6.3. The system can be recognized as a cascade realization form. It
can be written as a product of two blocks
H(z) = H1(z)H2(z).
where H1(z) denotes the ﬁrst block. It can be considered as a direct realiza-
tion II, with
y1(n) = 2y1(n −1) + 1
3y1(n −2) + x(n) + 1
2x(n −1) −1
3x(n −2),
presented in Fig.6.39. Using the z-transform properties, its transfer function
is
H1(z) = Y1(z)
X(z) = 1 + 1
2z−1 −1
3z−2
1 −2z−1 −1
3z−2 .
Now consider the second block whose transfer function is H2(z). This
block can be considered as a parallel realization of two blocks, H2(z) =
H21(z) + H22(z) where
H21(z) = 1.
The second transfer function is the transfer function corresponding to a
direct realization II, of a subsystem described by
y2(n) = y2(n −1) + y2(n −2) + x1(n) + 1
3x1(n −1) −1
4x1(n −2).
Thus, the transfer function of this subsystem is
H22(z) = Y2(z)
X1(z) = 1 + 1
3z−1 −1
4z−2
1 −z−1 −z−2 .

Ljubiša Stankovi´c
Digital Signal Processing
307
z-1
z-1
1/2
2
y1(n)
x1(n)
+
+
+
+
-1/3
1/3
z-1
z-1
z-1
z-1
1/3
+
+
+
-1/4
z-1
z-1
x(n)
y(n)
y2(n)
+
+
Figure 6.39
A discrete-time system.
It means that
H2(z) = H21(z) + H22(z) = 1 + 1 + 1
3z−1 −1
4z−2
1 −z−1 −z−2 .
The transfer function of the whole system is
H(z) = H1(z)H2(z) = 1 + 1
2z−1 −1
3z−2
1 −2z−1 −1
3z−2
(
1 + 1 + 1
3z−1 −1
4z−2
1 −z−1 −z−2
)
.
Solution 6.4. This realization can be considered as a cascade realization of
two blocks H1(z) and H2(z),
H(z) = H1(z)H2(z).
First block is a direct realization II, whose transfer function is
H1(z) = 1 + ( 1
2 + 1)z−1 −1
3z−2
1 −2z−1 −1
3z−2
.
Previous relation holds since the upper delay block (above the obvious
direct realization II block) has the same input and output as the ﬁrst delay
block below it.
The block with transfer function H2(z) can be considered as a parallel
realization of two blocks, similarly as in previous example, with, H21(z) and
H22(z), deﬁned by
H21(z) = 1 + 1
3z−1 −1
4z−2
1 −z−1 −z−2 ,

308
Realization of Discrete Systems
and
H22(z) = z−1.
Hence, the transfer function of the right block is
H2(z) = H21(z) + H22(z) = 1 + 1
3z−1 −1
4z−2
1 −z−1 −z−2
+ z−1.
Now, the resulting transfer function can be written in the form
H(z) = H1(z)H2(z) =
= 1 + ( 1
2 + 1)z−1 −1
3z−2
1 −2z−1 −1
3z−2
(
1 + 1
3z−1 −1
4z−2
1 −z−1 −z−2
+ z−1
)
.
Solution 6.5. The transfer function can be written as
H(z) = H1(z)H2(z).
It can be expressed, having in mind roots of the numerator and denominator
polynomials, as
H(z) =
P
1 −(0.1 + j0.1)z−1QP
1 + (0.1 + j0.1)z−1Q
(1−(0.85−j0.75)z−1)(1+(0.85−j0.75)z−1)
×
P
1 −(0.9 + j0.8)z−1QP
1 + (0.9 −j0.8)z−1Q
(1 −(0.05 −j0.1)z−1)(1 + (0.05 + j0.1)z−1).
The subsystems should be positioned as
H1(z) = 1 −1.8z−1 + 1.45z−2
1 −1.7z−1 + 1.285z−2
H2(z) = 1 −0.2z−1 + 0.02z−2
1 −0.1z−1 + 0.125z−2
since the zero-pole pairs with small values of imaginary parts should come
later. They are more sensitive to the quantization of coefﬁcients and they
will more probably cause this kind of error. Larger imaginary parts of roots
are less sensitive to these effects. The cascade realization is presented in
Fig.6.40.
Solution 6.6. For a cascade realization, form of the transfer function is
H(z) =
1
1 −1
2z−1 + 1
4z−2
1 + z−1
1 + 3
4z−1 .

Ljubiša Stankovi´c
Digital Signal Processing
309
z-1
z-1
-1.8
1.7
+
+
+
+
1.45
-1.285
z-1
z-1
z-1
z-1
-0.2
0.1
+
+
0.02
-0.125
z-1
z-1
x(n)
y(n)
+
+
Figure 6.40
Cascade realization less sensitive to possible quantization error
z-1
z-1
-3/4
+
+
z-1
z-1
1/2
+
-1/4
z-1
z-1
x(n)
y(n)
+
Figure 6.41
A cascade realization of a system
Its realization is presented in Fig.6.41
For a parallel realization, the transfer function can be written as
H(z) =
22
19 + 1
19z−1
1 −1
2z−1 + 1
4z−2 + −3
19z−1
1 + 3
4z−1 .
This realization is shown in Fig.6.42.
The transfer function can be written in the form
H(z) =
1 + z−1
1 + 1
4z−1 −1
8z−2 + 3
16z−3 .
A difference equation describing this system is
y(n) = x(n) + x(n −1) −1
4y(n −1) + 1
8y(n −2) −3
16y(n −3).

310
Realization of Discrete Systems
z-1
z-1
1/19
22/19
1/2
+
+
-1/4
z-1
z-1
z-1
z-1
-3/19
-3/4
+
x(n)
y(n)
+
Figure 6.42
Parallel realization of a discrete-time system.
Solution 6.7. The transfer function form corresponding to a cascade real-
ization is
H(z) =
(1 + z−2)
(z−1 + 1)(1 + z−1 + z−2).
In order the use the smallest number of delay circuits, it can be ex-
pressed in the form
H(z) = H1(z)H2(z) =
1
(1 + z−1)
(1 + z−2)
(1 + z−1 + z−2).
This form corresponds to the cascade realization presented in Fig.6.43.
Solution 6.8. The z-transforms of these equations are
Y(z)(1 + 1
4z−1) + W(z)(1 + 1
2z−1) = 2
3X(z)
Y(z)(1 −5
4z−1) + 2W(z)(1 −z−1) = −5
3X(z).
By eliminating W(z) we get
Y(z)[(2 + 1
2z−1)(1 −z−1) −(1 −5
4z−1)(1 + 1
2z−1)]
= X(z)[4
3(1 −z−1) + 5
3(1 + 1
2z−1)].

Ljubiša Stankovi´c
Digital Signal Processing
311
z-1
z-1
-1
+
z-1
z-1
-1
+
+
-1
z-1
z-1
x(n)
y(n)
+
Figure 6.43
Cascade realization of a discrete-time system.
The transfer function is
H(z) = Y(z)
X(z) =
3 −1
2z−1
1 −3
4z−1 + 1
8z−2 ,
with the difference equation describing this system
y(n) −3
4y(n −1) + 1
8y(n −2) = 3x(n) −1
2x(n −1).
The frequency response is
H(ejω) =
3 −1
2e−jω
1 −3
4e−jω + 1
8e−j2ω .
Based on
H(z) = Y(z)
X(z) =
3 −1
2z−1
1 −3
4z−1 + 1
8z−2 =
4
1 −1
2z−1 −
1
1 −1
4z−1 ,
the impulse response is
h(n) = [4(1/2)n −(1/4)n]u(n).
Solution 6.9. The transfer function of subsystem denoted by H1(z) follows
from
y(n) = rsinθx1(n −1) + rcosθy(n −1)

312
Realization of Discrete Systems
where x1(n) is the input to this subsystem. Its transfer function is
H1(z) = Y(z)
X1(z) =
z−1rsinθ
1 −rcosθz−1 .
The transfer function of the other subsystem is
H2(z) = −
z−1rsinθ
1 −rcosθz−1 .
For the feedback holds
H1(z)(X(z) + Y(z)H2(z)) = Y(z).
It produces
H(z) = Y(z)
X(z) =
H1(z)
1 −H1(z)H2(z) = z−1rsinθ(1 −rcosθz−1)
1 −2rcosθz−1 + r2z−2 .
Solution 6.10. The system impulse response is
h(n) = δ(n) + 2δ(n−1) −δ(n−2) + 4δ(n−3) −δ(n−4) + 2δ(n−5) + δ(n−6).
It satisﬁes the property
h(n) = h(N −1 −n), 0 ≤n ≤N −1
with N = 7, which implies phase function linearity. Thus, the group delay q
is
q = N −1
2
= 3.
Solution 6.11. We have that:
Y1(ejω) = H(ejω)X(ejω)
R(ejω) = Y∗
1 (ejω) = H∗(ejω)X∗(ejω)
Y2(ejω) = R(ejω)H(ejω) = H∗(ejω)H(ejω)X∗(ejω)
Y(ejω) = Y∗
2 (ejω) = H(ejω)H∗(ejω)X(ejω).
So we get
Y(ejω) = |H(ejω)|2X(ejω).
Obviously, the phase function of the system is equal to zero, for all ω.

Ljubiša Stankovi´c
Digital Signal Processing
313
Solution 6.12. (a) Values of the FIR ﬁlter, obtained by sampling frequency
response in the frequency domain are
H(k) = Hd(ejω)
'''
ω=2πnk/N .
This sampling is illustrated in the second row of Fig.6.44 for N = 15 and
N = 14. The impulse response of the FIR ﬁlter is calculated as
h(n) = IDFT{H(k)}
= 1
N
N−1
∑
k=0
H(k)ej2πnk/N.
It is shown in Fig.6.44 (third row). Frequency response of the FIR ﬁlter is
H(ejω) = FT{h(n)}.
Its values are equal to the desired frequency response at the sampling points
H(ejω)
'''
ω=2πk/N = Hd(ejω)
'''
ω=2πk/N .
(b) The impulse response of the desired system is
hd(n) = IFT{Hd(ejω)} = sin(nπ/2)
πn
+ sin(3nπ/4)
πn
.
Using the ﬁrst N = 15 samples in the discrete-time domain we get
h(n) =
!
hd(n)
for
−7 ≤n ≤7
0
elsewhere
or for N = 16
h(n) =
!
hd(n)
for
−8 ≤n ≤7
0
elsewhere.
The frequency response of this FIR ﬁlter is
H(ejω) = DFT{h(n)}.
It is shown in Fig.6.45.
(c) The errors along with the mean square absolute errors Er are
presented in Fig.6.46.

314
Realization of Discrete Systems
-5
0
5
0
1
2
3
- π
π
Hd(jΩ)
-5
0
5
0
1
2
3
- π
π
Hd(ejω)
-5
0
5
0
1
2
3
H(k),   N=15
-5
0
5
-1
0
1
2
h(n),   N=15
-5
0
5
0
1
2
3
H(ejω),   N=15
-5
0
5
0
1
2
3
H(k),   N=14
-6
-4
-2
0
2
4
6
-1
0
1
2
h(n),   N=14
-5
0
5
0
1
2
3
H(ejω),   N=14
Figure 6.44
Design of a FIR ﬁlter by frequency sampling of the desired frequency response.
6.5
EXERCISE
Exercise 6.1. Given a discrete system with
y(n) = x(n) −x(n −1) + x(n −2) + 1
2y(n −1) −1
3y(n −2) −1
4y(n −3),
plot the direct realization I and II, parallel and cascade realization.

Ljubiša Stankovi´c
Digital Signal Processing
315
-15
-10
-5
0
5
10
15
-1
0
1
2 hd(n)
-10
-5
0
5
10
-0.5
0
0.5
1
1.5
h(n),   N=15
-5
0
5
0
1
2
3
H(ejω),   N=15
-10
-5
0
5
10
-0.5
0
0.5
1
1.5
h(n),   N=14
-5
0
5
0
1
2
3
H(ejω),   N=14
Figure 6.45
FIR ﬁlter design using N the most signiﬁcant values of the impulse response.
Exercise 6.2. For a system whose transfer function is
H(z) =
z2 −2
(z −1)(z −2)
plot the direct I and II realization, cascade realization, and parallel realiza-
tion.
Exercise 6.3. For a system whose transfer function is
H(z) =
3z−2 + 6
z−3 −2z−2 + 3z−1 −6
a) plot the direct realizations I and II, the cascade realization, and the parallel
realization.
b) Find ∑∞
n=−∞h(n), where h(n) is the impulse response of the system.

316
Realization of Discrete Systems
0
1
2
3
4
5
6
-0.5
0
0.5
Er= 0.037954
0
1
2
3
4
5
6
-0.5
0
0.5
Er= 0.028921
Figure 6.46
Error in the case of the frequency response sampling (top) and the IIR impulse
response truncation (bottom), along with the corresponding mean square error (Er) value.
Exercise 6.4. Find the impulse response of the discrete system presented in
Fig.6.47.
Exercise 6.5. Using the impulse invariance method with the sampling step
∆t = 0.1, transform the analog system given with the transfer function
H(s) =
1 + 5s
8 + 2s + 5s2
into discrete, and plot the direct and cascade realization of the system. Is the
obtained discrete system stable?
Exercise 6.6. Using the bilinear transform with the sampling step ∆t = 1,
transform the system given with the transfer function
H(s) =
2 + s
8 + 2s + 5s2
into discrete, and plot the direct and cascade realization of the system. Is the
obtained discrete system stable?
Exercise 6.7. Using the bilinear transform, with the sampling step ∆t = 0.2
transform the analog system given with the transfer function
H(s) =
3s + 6
(s + 1)(s + 3)

Ljubiša Stankovi´c
Digital Signal Processing
317
z-1
z-1
-1
4
+
+
-5
z-1
z-1
z-1
z-1
2
0
1/2
+
+
x(n)
y(n)
Figure 6.47
Discrete-time system.
-5
0
5
0
1
2
3
- π
π
- π/2
π/2
Hd(jΩ)
-5
0
5
0
1
2
3
- π
π
- π/2
π/2
Hd(ejω)
Figure 6.48
Desired system in the continuous-time and discrete-time domains.
into discrete, and plot the direct realization II of the discrete system.
Exercise 6.8. For a system whose frequency response in the continuous-time
domain is
Hd(jΩ) =
%
2 −|Ω|
π/2
for
|ω| < π
2
0
elsewhere
with the corresponding Hd(ejω) in the discrete-time domain obtained for
∆t = 1, and presented in Fig.6.48, ﬁnd the FIR ﬁlter impulse response with
N = 7 and N = 8 using:

318
Realization of Discrete Systems
(a) Sampling the desired frequency response Hd(ejω) in the frequency
domain,
(b) Calculating hd(n) = IFT{Hd(ejω)} and taking its N the most signif-
icant values, h(n) = hd(n) for −N/2 ≤n ≤N/2 −1 and h(n) = 0 elsewhere.
(c) Comment the sources of error in both cases.

Chapter 7
Discrete-Time Random Signals
R
ANDOM signals cannot be described by simple mathematical func-
tions. Their values are not known in advance. These signals can be
described by stochastic tools only. Here we will restrict the analysis
to the discrete-time random signals. The ﬁrst-order and the second-order
statistics will be considered.
7.1
BASIC STATISTICAL DEFINITIONS
7.1.1
Expected Value
The ﬁrst-order statistics is the starting point in describing random signals.
The expected value, or the mean value, of a random signal is one of its basic
parameters. If we have a set of signal samples,
{x(n)},
n = 1,2,..., N,
(7.1)
the mean value of this set of signal values is calculated as
µx = 1
N (x(1) + x(2) + ... + x(N)).
Example 7.1. Consider a random signal x(n) whose one realization is given
in Table 7.1. Find the mean value of this signal. Find how many samples of
the signal are within the intervals [1,10], [11,20],...,[91,100]. Plot the number
of occurrences of signal x(n) samples within these intervals as a function of
the interval range.
⋆The realization of signal x(n) deﬁned in Table 7.1 is presented in
Fig.7.1.
319

320
Discrete-Time Random Signals
Table 7.1
A realization of random signal
54
62
58
51
70
43
99
52
57
57
56
53
38
61
28
69
87
41
72
72
23
26
66
47
69
71
69
81
68
68
31
55
52
23
60
34
83
39
66
66
37
12
54
42
67
95
89
67
42
42
35
55
54
55
49
77
18
64
73
73
67
56
42
66
50
47
49
25
50
50
61
84
48
67
71
74
35
59
60
60
40
77
52
63
57
42
44
64
36
36
66
39
50
31
11
75
45
62
60
60
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
110
120
x(n)
mean(x)
Figure 7.1
A realization of random signal x(n).
The mean value of all signal samples is
µx =
1
100
100
∑
n=1
x(n) = 55.76.

Ljubiša Stankovi´c
Digital Signal Processing
321
0
10
20
30
40
50
60
70
80
90
100
0
5
10
15
20
25
Figure 7.2
Histogram of random signal x(n) with 10 intervals [10i + 1,10i + 10], i = 0,1,2,...,9.
From Table 7.1 or the graph in Fig. 7.1 we can count that, for example,
there is no a signal sample whose value is within the interval [1,10]. Within
[11,20] there are two signal samples (x(42) = 12 and x(95) = 11). In a similar
way, the number of signal samples within other intervals are counted and
presented in Fig.7.2. This kind of random signal presentation is called a
histogram of x(n), with deﬁned intervals.
Example 7.2. For the signal x(n) from the previous example assume that a new
random signal y(n) is formed as
y(n) = int
! x(n) + 5
10
6
,
where int{◦} denotes the nearest integer. It means that y(n) = 1 for 1 ≤
x(n) ≤10, y(n) = 2 for 11 ≤x(n) ≤20, ..., y(n) = i for 10(i −1) + 1 ≤x(n) ≤
10i up to i = 10. Plot the new signal y(n). What is the set of possible values of
y(n). Present on a graph how many times each of the possible values of y(n)
appeared in this signal realization. Find the mean value of the new signal
y(n) and discuss the result.
⋆The signal y(n) is shown in Fig.7.3. This signal assumes values from
the set {2,3,4,5,6,7,8,9,10}.
For the signal y(n), instead of histogram we can plot a diagram of the
number of occurrences of each value that y(n) can assume. It is presented in

322
Discrete-Time Random Signals
0
10
20
30
40
50
60
70
80
90
100
0
1
2
3
4
5
6
7
8
9
10
11
y(n)
mean(y)
Figure 7.3
Random signal y(n).
Fig.7.4. The mean value of y(n) is
µy =
1
100
100
∑
n=1
y(n) = 6.13.
The mean value can also be written, by grouping the same values of y(n), as
µy =
1
100(1 · n1 + 2 · n2 + 3 · n3 + ... + 10 · n10) =
= 1 · n1
N + 2 · n2
N + 3 · n3
N + ... + 10 · n10
N ,
where N = 100 is the total number of signal values and ni is the number
showing how many times each of the values i appeared in y(n). If there is a
sufﬁcient number of occurrences for each outcome value i then
Py(i) = ni
N
can be considered as the probability that the value i appears. In that sense
µy = 1 · Py(1) + 2 · Py(2) + 3 · Py(3) + ... + 10 · Py(10)
=
10
∑
i=1
y(i)Py(i)

Ljubiša Stankovi´c
Digital Signal Processing
323
0
1
2
3
4
5
6
7
8
9
10
0
5
10
15
20
25
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
Py(i)
Figure 7.4
Number of appearances of each possible value of y(n) (left) and the probabilities
that the random signal y(n) takes a value i = 1,2,...,10 (right).
with
10
∑
i=1
Py(i) = 1.
Values of probability Py(i) are shown in Fig.7.4.
In general, the mean for each signal sample could be different. For
example, if the signal values represent the highest daily temperature during
a year then the mean value is highly dependent on the considered sample.
In order to calculate the mean value of temperature, we have to have several
realizations of these random signals (measurements over M years), denoted
by {xi(n)}, where argument n = 1,2,3,... is the cardinal number of the day
within a year and i = 1,2,..., M is the index of realization (year index). The
mean value is then calculated as
µx(n) = 1
M(x1(n) + x2(n) + ... + xM(n)) = 1
M
M
∑
i=1
xi(n),
(7.2)
for each n. In this case we have a set (a signal) of mean values {µx(n)}, for
n = 1,2,...,365.
Example 7.3. Consider a signal x(n) with realizations given in Table 7.2. Its values
are equal to the monthly average of maximal daily temperatures in a city
measured from year 2001 to 2015. Find the mean temperature for each month
over the considered period of years. What is the mean value of temperature
over all months and years? What is the mean temperature for each year?

324
Discrete-Time Random Signals
Table 7.2
Average of maximal temperatures value within months over 15 years, 2001-2015.
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
10
4
18
17
22
29
30
28
27
17
17
5
6
7
11
23
22
32
35
33
22
26
22
8
10
11
10
16
21
26
32
31
23
19
17
4
3
11
13
19
22
26
34
29
26
22
12
9
7
10
13
21
27
29
30
34
24
20
16
11
7
11
17
17
27
25
37
34
33
22
14
14
7
12
13
19
23
32
34
38
21
21
12
10
12
5
9
20
21
37
34
34
27
22
20
7
7
12
13
23
27
33
29
31
25
21
6
11
8
12
10
17
27
33
38
32
23
20
15
9
8
10
13
24
23
33
33
31
27
21
16
8
4
6
15
18
25
26
27
33
23
23
13
11
3
6
16
17
27
28
30
32
29
24
12
10
11
12
14
18
22
29
34
34
23
21
20
11
6
13
8
22
22
29
30
34
23
18
15
8
⋆The signal for years 2001 to 2007 is presented in Fig.7.5. The mean
temperature for the nth month, over the considered years, is
µx(n) = 1
15
15
∑
i=1
x20i(n),
where the notation 20i is symbolic in the sense, 2001, 2002, ... 2015, for
i = 01,02,...,15. The mean-value signal µx(n) is presented in the last subplot
of Fig. 7.5. The mean value over all months and years is
µx =
1
15 · 12
12
∑
n=1
15
∑
i=1
x20i(n) = 19.84.
The mean value for each of the considered years is
µx(20i) = 1
12
12
∑
n=1
x20i(n).

Ljubiša Stankovi´c
Digital Signal Processing
325
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45 x2001(n)
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45 x2002(n)
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45 x2003(n)
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45 x2004(n)
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45 x2005(n)
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45 x2006(n)
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45 x2007(n)
1
2
3
4
5
6
7
8
9
10 11 12
-5
5
15
25
35
45
µx(n)
Figure 7.5
Several realizations of a random signal x20i(n), for i = 01,02,...,07 and the mean
value µx(n) for each sample (month) over 15 available realizations.

326
Discrete-Time Random Signals
7.1.2
Probability and Probability Density Function
If the probabilistic description of a random signal is known, then we can
calculate the mean value and other parameters of random signals. For the
ﬁrst-order statistics calculation, it is sufﬁcient to know the probabilities or
the probability density function.
If a random signal assumes only discrete values in amplitude {ξ1,ξ2,...},
then we deal with probabilities,
Probability{x(n) = ξi} = Px(n)(ξi).
(7.3)
Probability function Px(n)(ξ) satisﬁes the following properties:
1) 0 ≤Px(n)(ξ) ≤1 for any ξ.
2) For the events x(n) = ξi and x(n) = ξj, i ̸= j, which exclude each
other
Probability
R
x(n) = ξi or x(n) = ξj
S = Px(n)(ξi) + Px(n)(ξj).
3) The sum of probabilities that x(n) takes any value ξi over the set A
of all possible values of ξ is a certain event. Its probability is 1,
∑
ξ∈A
Px(n)(ξ) = 1.
An impossible event has the probability 0.
If x(n) and x(m) are statistically independent random samples then
Probability
R
x(n) = ξi and x(m) = ξj
S = Px(n)(ξi)Px(m)(ξj).
An example of a signal when the probabilities are calculated after the
experiment (a posteriori) is already presented within the ﬁrst example. A
posteriori probability that the signal x(n) assumes value ξi is deﬁned as a
ratio of the number Nξi of appearances of the event x(n) = ξi and the total
number of signal values (experiments) N
Px(n)(ξi) = Nξi
N
for a sufﬁciently large N and Nξi.
In some cases it is possible to ﬁnd the probability of an event before the
experiment is performed. For example, if a signal is equal to the numbers
appearing in die tossing, then the signal may assume one of the values from
the set ξi ∈{1,2,3,4,5,6}. In this case, the probability of each event is known
in advance (a priori). It is P(ξi) = 1/6.

Ljubiša Stankovi´c
Digital Signal Processing
327
Example 7.4. Consider a random signal whose values are equal to the numbers ap-
pearing in a die tossing. The set of possible signal values is ξi ∈{1,2,3,4,5,6}.
Find
Probability{x(n) = 2 or x(n) = 5}
and
Probability{x(n) = 2 and x(n + 1) = 5}.
⋆Events that x(n) = 2 and x(n) = 5 are obviously mutually exclusive. Thus,
Probability{x(n) = 2 or x(n) = 5} = Px(n)(2) + Px(n)(5) = 1
6 + 1
6 = 1
3.
The events that x(n) = 2 and x(n + 1) = 5 are statistically independent. In
this case
Probability{x(n) = 2 and x(n + 1) = 5} = Px(n)(2)Px(n)(5) = 1
6
1
6 = 1
36.
Example 7.5. Assume that a signal x(n) length is N and that the number of samples
disturbed by an extremely high noise is I. The observation set of signal
samples is taken as a set of M < N randomly positioned signal samples. What
is the probability that within M randomly selected signal samples there are
no samples affected by the high noise? If N = 128, I = 16, and M = 32 ﬁnd
how many sets of M samples without high noise can be expected in 1000
realizations (trials).
⋆Probability that the ﬁrst randomly chosen sample is not affected by
the high noise could be calculated as a priori probability,
P(1) = N −I
N
since there are N samples in total and N −I of them are noise-free. Probability
that the ﬁrst randomly chosen sample is not affected by high noise and that,
at the same time, the second randomly chosen sample is not affected by high
noise is equal to to the product of their probabilities,
P(2) = N −I
N
N −1 −I
N −1
.
Here we used so called conditional probability property stating that
the probability that both events A and B occur is
Probability{A and B} = P(A)P(B/A),
where P(A) is the probability that event A occurs, while P(B/A) denotes the
probability that event B occurs subject to the condition that event A already
occurred.

328
Discrete-Time Random Signals
Then we continue the process of random samples selection. In the same
way we can calculate the probability that all of M randomly chosen samples
are not affected by the high noise as
P(M) =
M−1
∏
i=0
N −I −i
N −i
.
For N = 128, I = 16, and M = 32 we get
P(32) = 0.0112.
It means that if we repeat the whole procedure 1000 times (1000 realizations)
we can expect
P(32) × 1000 = 11.2,
i.e., about 11 realizations when none of M signal samples is disturbed by the
high noise.
The mean value is calculated as a sum over the set of possible ampli-
tudes, weighted by the corresponding probabilities,
µx(n) = E{x(n)} =
∞
∑
i=1
ξiPx(n)(ξi).
(7.4)
If a random signal can assume continuous values in amplitude then
we cannot deﬁne a probability that one exact signal amplitude value is
assumed. In that case the probability density function px(n)(ξ) is used. It
deﬁnes the probability that the nth signal sample x(n) takes a value within
an inﬁnitesimally small interval dξ around ξ,
Probability{ξ ≤x(n) < ξ + dξ)} = px(n)(ξ)dξ.
(7.5)
Properties of the probability density function are:
1) It is nonnegative, px(n)(ξ) ≥0 for any ξ
2) Since Probability{−∞< x(n) < ∞} = 1, then
∞
"
−∞
px(n)(ξ)dξ = 1.
The probability of an event that a value of signal x(n) is within a ≤x(n) < b
is
Probability{a ≤x(n) < b} =
a
"
b
px(n)(ξ)dξ.

Ljubiša Stankovi´c
Digital Signal Processing
329
Cumulative distribution F(χ) function is the probability that a signal x(n)
value is lower than χ,
F(χ) = Probability{x(n) < χ} =
χ
"
−∞
px(n)(ξ)dξ.
Obviously limχ→−∞F(χ) = 0, limχ→+∞F(χ) = 1, and F(a) ≥F(b) if a > b.
Note that
px(n)(ξ) = dF(ξ)
dξ
.
The expected value of a random variable x(n) in terms of the proba-
bility density function, is
µx(n) = E{x(n)} =
∞
"
−∞
ξpx(n)(ξ)dξ.
(7.6)
7.1.3
Median
In addition to the mean value, a median is used for description of a set of
random values. The median is a value in the middle of the set, after the
members of the set are sorted. If we denote the sorted values of x(n) as s(n)
s(n) = sort{x(n)}, n = 1,2,..., N
then the median value is
median{x(n)} = s
* N + 1
2
+
, for an odd N.
If N is an even number then the median is deﬁned as the mean value of two
samples nearest to (N −1)/2,
median{x(n)} =
s
B
N
2
C
+ s
B
N
2 + 1
C
2
, for an even N.
The median will not be inﬂuenced by a possible small number of big outliers
(signal values being signiﬁcantly different from the values of the rest of
data).

330
Discrete-Time Random Signals
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
110
120
sort(x)
median(x)
Figure 7.6
Sorted values and the median of x(n).
Example 7.6. Find the median of sets
(a) A = {−1,1,−2,4,6,−9,0}, (b) B = {−1,1,−1367,4,35,−9,0}, and (c)
of the signal x(n) from Example 7.1.
⋆(a) After sorting the values in set A we get A = {−9,−2,−1,0,1,4,6}.
Thus, median(A) = 0. (b) In a similar way median(B) = 0. The mean values
of these data would signiﬁcantly differ. (c) The sorted values of x(n) are
presented in Fig. 7.6. Since the number of samples of signal x(n) is N = 100
there is no single sample in the middle of the sorted sequence. The middle
is between sorted samples 50 and 51. Thus the median in this situation is
deﬁned as the mean value of the 50th and 51st sorted sample.
In some cases the number of big outliers is small. Thus the median will
neglect many signal values that could produce a good estimate of the mean
value. In that cases, the best choice would be to use not only the mid-value
in the sorted signal, but several samples of the signal around its median and
to calculate their mean, for odd N, as
LSmean{x(n)} =
1
2L + 1
L
∑
i=−L
s
* N + 1
2
+ i
+
.
With L = (N −1)/2 all signal values are used and LSmean{x(n)} is the
standard mean of a signal. With L = 0 the value of LSmean{x(n)} is the

Ljubiša Stankovi´c
Digital Signal Processing
331
standard median. In general, this way of signal parameters estimation is the
L-statistics based estimation.
7.1.4
Variance
For random signals that take values from a discrete set, with known proba-
bilities, the variance is deﬁned as
σ2
x(n) = E{|x(n) −µx(n)|2}
= ∑
ξ
'''ξ −µx(n)
'''
2
Px(n)(ξ).
For a random signal x(n) whose values are available in M realizations the
variance can be estimated as a mean square deviation of the signal values
from their corresponding mean values µx(n),
σ2
x(n) = 1
M
B
|x1(n) −µx(n)|2 + ... + |xM(n) −µx(n)|2C
.
The standard deviation is a square root of the variance. The standard
deviation can be estimated as a square root of the mean of squares of the
centered data,
σx(n) =
=
1
M
B
|x1(n) −µx(n)|2 + ... + |xM(n) −µx(n)|2C
.
(7.7)
For a small number of samples, this estimate tends to produce lower values
of the standard deviation. Thus, an adjusted version, the sample standard
deviation, is also used. It reads
σx(n) =
=
1
M −1
B
|(x1(n) −µx(n))|2 + ... + |xM(n) −µx(n)|2C
.
This form conﬁrms the fact that in the case when only one sample is
available, M = 1, we should not be able to estimate the standard deviation.
For the case of random signals whose amplitude is continuous the
variance, in terms of the probability density function px(n)(ξ), is
σ2
x(n) =
∞
"
−∞
'''ζ −µx(n)
'''
2
px(n)(ξ)dξ.

332
Discrete-Time Random Signals
Table 7.3
Random signal z(n)
55
57
56
54
59
52
66
54
56
56
55
55
51
56
48
59
63
52
59
59
47
48
58
53
58
59
59
61
58
58
49
55
54
47
56
50
62
51
58
58
50
44
55
50
58
58
63
58
52
52
50
55
55
55
53
60
46
57
59
59
58
55
58
58
54
53
54
48
54
54
57
62
53
58
59
60
50
56
56
56
51
60
54
57
55
52
52
57
50
50
58
51
54
49
44
60
52
57
56
56
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
110
120
z(n)
mean(z)
Figure 7.7
Random signal z(n).
Example 7.7. For the signal x(n) from Example 7.1 calculate the mean and vari-
ance. Compare it with the mean and variance of the signal z(n) given in Table
7.3.

Ljubiša Stankovi´c
Digital Signal Processing
333
⋆The mean value and variance for signal x(n) are µx = 55.76 and
σ2x = 314.3863. The standard deviation is σx = 17.7309. It is a measure of signal
value deviations from the mean value. For the signal z(n) the mean value
is µz = 55.14 (very close to µx), while the variance is σ2z = 18.7277 and the
standard deviation is σz = 4.3275. Deviations of z(n) from the mean value
are much smaller. If signals x(n) and z(n) were measurements of the same
physical value, then the individual measurements from z(n) would be much
more reliable than the individual measurements from x(n).
Example 7.8. A random signal x(n) can take values from the set {0,1,2,3,4,5}. It
is known that for k = 1,2,3,4 the probability of x(n) = k is twice higher than
the probability of x(n) = k + 1. Find the probabilities P{x(n) = k}. Find the
mean value and variance of signal.
⋆Assume that P{x(n) = 5} = A. Then the probabilities that x(n) takes
a value k are
k
0
1
2
3
4
5
P{x(n) = k}
32A
16A
8A
4A
2A
A
Constant A can be found from ∑k P{x(n) = k} = 1. It results in A = 1/63.
Now we have
µx(n) = ∑
k
kP{x(n) = k} = 19
21
σ2
x(n) = ∑
k
*
k −19
21
+2
P{x(n) = k} = 626
441.
Example 7.9. Consider a real-valued random signal x(n) with samples whose
values are uniformly distributed over interval −1 ≤x(n) ≤1. a) Find the
mean value and variance of the signal samples. b) Signal y(n) is obtained as
y(n) = x2(n). Find the mean value and variance of signal y(n).
⋆Since the random signal x(n) is uniformly distributed, its probability
density function is of the form
px(n)(ξ) =
! A
for |ξ| ≤1
0
for |ξ| > 1 .
Constant A = 1/2 is obtained from & ∞
−∞px(n)(ξ)dξ = 1. Now we have
µx(n) =
∞
"
−∞
ξpx(n)(ξ)dξ =
1
"
−1
1
2ξdξ = 0
σ2
x(n) =
∞
"
−∞
(ξ −µx(n))2px(n)(ξ)dξ =
1
"
−1
1
2ξ2dξ = 1
3.

334
Discrete-Time Random Signals
The probability that y(n) is not higher than ξ is
Fy(ξ) = P{y(n) ≤ξ} = P{x2(n) ≤ξ} = P{−
,
ξ < x(n) ≤
,
ξ}
=
⎧
⎪
⎨
⎪
⎩
0
for ξ ≤0
& √ξ
−√ξ px(n)(ξ)dξ
for 0 < ξ < 1
1
for ξ ≥1
=
⎧
⎨
⎩
0
for ξ ≤0
√ξ
for 0 < ξ < 1
1
for ξ ≥1
since y(n) ≤ξ when x2(n) ≤ξ. The probability density function is
py(n)(ξ) = dF(ξ)
dξ
=
%
1
2√ξ
for 0 < ξ ≤1
0
otherwise.
The mean value and variance of signal y(n) are
µy(n) =
1
"
0
ξ
1
2√ξ dξ = 1
3
σ2
y(n) =
1
"
0
(ξ −1
3)2
1
2√ξ dξ = 4
45.
Note: Generalize for z(n) = f (x(n)).
As an introduction to the second-order statistics consider two signals
x(n) and y(n) with continuous amplitude values. Probability that the nth
signal sample x(n) takes a value within ξ ≤x(n) < ξ + dξ and that y(m)
takes a value within ζ ≤y(m) < ζ + dζ is
Probability{ξ ≤x(n) < ξ + dξ),ζ ≤y(m) < ζ + dζ)} = px(n),y(m)(ξ,ζ)dξdζ,
where px(n),y(m)(ξ,ζ) is the joint probability density function. The probabil-
ity of an event a ≤x(n) < b and c ≤y(m) < d is
Probability{a ≤x(n) < b,c ≤y(m) < d} =
a
"
b
d
"
c
px(n),y(m)(ξ,ζ)dξdζ.
For mutually independent signals px(n),y(m)(ξ,ζ) = px(n)(ξ)py(m)(ζ). A spe-
cial case of the previous relations is obtained when y(m) = x(m).
Example 7.10. Signal x(n) is deﬁned as x(n) = a(n) + b(n) + c(n) where a(n),
b(n), and c(n) are mutually independent random signals with a uniform
probability density function over the range [−1,1). Find the probability
density function of signal x(n), its mean µx, and variance σ2x.

Ljubiša Stankovi´c
Digital Signal Processing
335
⋆Consider a sum of two independent random signals s(n) = a(n) +
b(n). The probability that s(n) = a(n) + b(n) < θ can be calculated from the
joint probability distribution of a(n) and b(n) as
F(θ) = P{s(n) < θ}
= Probability{−∞< a(n) < ∞,−∞< a(n) + b(n) ≤a < θ}
=
∞
"
−∞
θ−ζ
"
−∞
pa(n),b(n)(ξ,ζ)dξdζ =
∞
"
−∞
pb(n)(ζ)
θ−ζ
"
−∞
pa(n)(ξ)dξdζ.
Now we can calculate the probability density function of s(n) as a derivative
ps(n)(θ) = dF(θ)
dθ
=
∞
"
−∞
pb(n)(ζ) d
dθ
θ−ζ
"
−∞
pa(n)(ξ)dξdζ
=
∞
"
−∞
pb(n)(ζ)pa(n)(θ −ζ)dζ = pb(n)(θ) ∗θ pa(n)(θ),
meaning that the probability density function of a sum of two independent
random variables is a convolution of the individual probability density func-
tions. In a similar way we can include the third signal and obtain
px(n)(θ) = pc(n)(θ) ∗θ pb(n)(θ) ∗θ pa(n)(θ),
px(n)(θ) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
(θ+3)2
16
for −3 ≤θ ≤−1
3−θ2
8
for −1 < θ ≤1
(θ−3)2
16
for 1 < θ ≤3
0
for |θ| > 3
.
The mean value and variance can be calculated from px(n)(θ), or in direct
way, as
µx = E{x(n)} = E{a(n)} + E{b(n)} + E{c(n)} = 0
σ2
x = E{(x(n) −µx)2} = E{(a(n) + b(n) + c(n))2}
= E{a(n)2} + E{b(n)2} + E{c(n)2} + 2(µaµb + µaµc + µbµc)
= 1
3 + 1
3 + 1
3 = 1.

336
Discrete-Time Random Signals
7.2
SECOND-ORDER STATISTICS
7.2.1
Correlation and Covariance
Second-order statistics deals with two samples of random signals.
For a signal {xi(n)}, n = 1,2,..., N and i = 1,2,..., M, being the number
of realizations of this signal, the autocorrelation function is deﬁned by
rxx(n,m) = E{x(n)x∗(m)} = 1
M
M
∑
i=1
xi(n)x∗
i (m).
(7.8)
If the probability that a real-valued random signal x(n) assumes a
value ξ1 and that x(m) assumes ξ2 is Px(n),x(m)(ξ1,ξ2) then
rxx(n,m) = ∑
ξ1 ∑
ξ2
ξ1ξ2Px(n),x(m)(ξ1,ξ2).
(7.9)
For a real-valued random signal with continuous amplitudes and the
second-order probability density function px(n),x(m)(ξ1,ξ2), the autocorrela-
tion is
rxx(n,m) =
∞
"
−∞
ξ1ξ2px(n),x(m)(ξ1,ξ2)dξ1dξ2.
(7.10)
If the real-valued random variables x(n) and x(m) are statistically
independent, then px(n),x(m)(ξ1,ξ2) = px(n)(ξ1)px(m)(ξ2) and rxx(n,m) =
µx(n)µx(m).
The autocovariance function is deﬁned by
cxx(n,m) = E{(x(n) −µx(n))(x(m) −µx(m))∗}
= 1
M
M
∑
i=1
(xi(n) −µx(n))(xi(m) −µx(m))∗.
(7.11)
It may be easily shown that
cxx(n,m) = E{(x(n) −µx(n))(x(m) −µx(m))∗} = rxx(n,m) −µx(n)µ∗
x(m).
Value of the autocovariance for m = n is the variance
σ2
x(n) = E{|x(n) −µx(n)|2} = rxx(n,n) −|µx(n)|2 .
(7.12)

Ljubiša Stankovi´c
Digital Signal Processing
337
The cross-correlation and the cross-covariance of two signals x(n) and
y(n) are deﬁned as
rxy(n,m) = E{x(n)y∗(m)}
and
cxy(n,m) = E{(x(n) −µx(n))(y(m) −µy(m))∗}
(7.13)
= rxy(n,m) −µx(n)µ∗
y(m).
7.2.2
Stationarity and Ergodicity
Signals whose ﬁrst-order and second-order statistics are invariant to a shift
in time are called wide sense stationary (WSS) signals. For the WSS signals
holds
µx(n) = E{x(n)} = µx
rxx(n,m) = E{x(n)x∗(m)} = rxx(n −m).
(7.14)
A signal is stationary in the strict sense (SSS) if all order statistics are
invariant to a shift in time. The relations introduced for the second-order
statistics may be extended to the higher-order statistics. For example, the
third-order moment of a signal x(n) is deﬁned by
Mxxx(n,m,l) = E{x(n)x∗(m)x∗(l)}.
(7.15)
For stationary signals it assumes the form
Mxxx(m,l) = E{x(n)x∗(n −m)x∗(n −l)}.
In order to calculate the third-order moment we should know the third-
order statistics, like the third-order probability Px(n),x(m),x(l)(ξ1,ξ2,ξ3) or
probability density function.
For a random process, as collection of all realizations of a random sig-
nal along with its probabilistic description, we say that it is ergodic if its
parameters can be estimated by averaging over time instead of over real-
izations. The process is ergodic in parameter β if that particular parameter
can be estimated by averaging over time instead of over realizations. If a
random signal x(n) is a realization of a process ergodic in mean then
µx(n) = lim
M→∞
1
M(x1(n) + x2(n) + ... + xM(n))
= lim
N→∞
1
N (xi(n) + xi(n −1) + ... + xi(n −N + 1)).

338
Discrete-Time Random Signals
7.2.3
Power Spectral Density
For stationary signals the autocorrelation function is
rxx(n) = E{x(n + m)x∗(m)} = rxx(n).
The Fourier transform of the autocorrelation function of a WSS signal is the
power spectral density
Sxx(ejω) =
∞
∑
n=−∞
rxx(n)e−jωn
(7.16)
rxx(n) = 1
2π
π
"
−π
Sxx(ejω)ejωndω.
(7.17)
Integral of Sxx(ejω) over frequency,
1
2π
π
"
−π
Sxx(ejω)dω = rxx(0) = E{|x(n)|2},
(7.18)
is equal to the average power of the random signal.
Example 7.11. Find the mean, autocorrelation, and power spectral density of the
random signal
x(n) =
K
∑
k=1
akej(ωkn+θk),
where θk are random variables uniformly distributed over −π < θk ≤π.
All random variables are statistically independent. Frequencies ωk are −π <
ωk ≤π for each k.
⋆The mean value is
µx =
K
∑
k=1
akE{ej(ωkn+θk)} =
K
∑
k=1
ak
π
"
−π
1
2π ej(ωkn+θk)dθk = 0.
The autocorrelation is
rxx(n) = E{
K
∑
k=1
akej(ωk(n+m)+θk)
K
∑
k=1
ake−j(ωkm+θk)} =
K
∑
k=1
a2
kejωkn,
while the power spectral density for −π < ω ≤π is
Sxx(ejω) = FT{rxx(n)} = 2π
K
∑
k=1
a2
kδ(ω −ωk).

Ljubiša Stankovi´c
Digital Signal Processing
339
Remind that the average signal power of a signal x(n) has been de-
ﬁned as
PAV = lim
N→∞
1
2N + 1
N
∑
n=−N
|x(n)|2 =
#
|x(n)|2$
.
This relation leads to another deﬁnition of the power spectral density
of random discrete-time signals
Pxx(ejω) = lim
N→∞
1
2N + 1E{
'''XN(ejω)
'''
2
}
(7.19)
= lim
N→∞
1
2N + 1E{
'''''
N
∑
n=−N
x(n)e−jωn
'''''
2
}.
Different notation is used since the previous two deﬁnitions, (7.16) and
(7.19) of power spectral density, will not produce the same result, in general.
We can write
Pxx(ejω) = lim
N→∞
1
2N + 1E{
N
∑
m=−N
N
∑
n=−N
x(m)x∗(n)e−jω(m−n)}.
For a stationary signal
Pxx(ejω) = lim
N→∞
1
2N + 1
N
∑
m=−N
N
∑
n=−N
rxx(m −n)e−jω(m−n).
Double summation is performed within a square in the two-dimensional
domain deﬁned by −N ≤m ≤N,
−N ≤n ≤N. Since the terms within
double sum are functions of (m −n) only, then the summation could be
performed along the lines where (m −n) = k is constant. For (m −n) = k = 0
the summation line is the main diagonal of area −N ≤m ≤N, −N ≤n ≤N.
Along this diagonal there are 2N + 1 points where rxx(m −n)e−jω(m−n) =
rxx(0). For the nearest subdiagonals of −N ≤m ≤N,
−N ≤n ≤N
when (m −n) = k = ±1 there are 2N points where rxx(m −n)e−jω(m−n) =
rxx(±1)e±jω. For arbitrary lines (m −n) = ±k, with |k| ≤2N, there are
2N + 1 −|k| terms with rxx(m −n)e−jω(m−n) = rxx(±k)e±jkω. It means that
we can write
Pxx(ejω) = lim
N→∞
1
2N + 1
2N
∑
k=−2N
(2N + 1 −|k|)rxx(k)e−jωk
= lim
N→∞
2N
∑
k=−2N
(1 −
|k|
2N + 1)rxx(k)e−jωk = lim
N→∞
2N
∑
k=−2N
wB(k)rxx(k)e−jωk.

340
Discrete-Time Random Signals
Function wB(k) corresponds to a Bartlett window over the calculation in-
terval. If the values of autocorrelation function rxx(k) are such that the sec-
ond part of the sum ∑k |k|/(2N + 1)rxx(k)e−jωk is negligible as compared to
∑k rxx(k)e−jωk then
Pxx(ejω) = lim
N→∞
2N
∑
k=−2N
rxx(k)e−jωk = FT{rxx(n)} = Sxx(ejω).
This is true for rxx(k) = Cδ(k). Otherwise Pxx(ejω) is a smoothed version
of Sxx(ejω). Note that Pxx(ejω) is always nonnegative, by deﬁnition (for a
numeric illustration see Example 7.23).
7.3
NOISE
In many applications, the desired signal is disturbed by various forms of
random signals, caused by numerous factors in the signal sensing, trans-
mission, and/or processing. Often, a cumulative inﬂuence of these factors,
disturbing useful signal, is described by an equivalent random signal, called
noise. In most cases we will use a notation ε(n) for these kinds of signals.
They model a random, multiple source, disturbance.
A noise is said to be white if its values are uncorrelated
rεε(n,m) = σ2
ε δ(n −m)
(7.20)
Sεε(ejω) = FT{rxx(n)} = σ2
ε .
Spectral density of this kind of noise is constant (like it is the case in the
white light). If this property is not satisﬁed, then the power spectral density
is not constant. Such a noise is referred to as colored.
Regarding to the distribution of noise ε(n) amplitudes the most com-
mon types of noise in signal processing are: uniform, binary, Gaussian, and
impulsive noise.
7.3.1
Uniform Noise
The uniform noise is a signal with the probability density function
pε(n)(ξ) = 1
∆,
for −∆/2 ≤ξ < ∆/2
(7.21)

Ljubiša Stankovi´c
Digital Signal Processing
341
0
10
20
30
40
50
60
-1.5
-1
-0.5
0
0.5
1
1.5
0
0.5
1
1.5
-1.5
-1
-0.5
0
0.5
1
1.5
px( ξ)
Figure 7.8
A realization of uniform noise (left) with probability density function (right) with
∆= 0.5.
and pε(n)(ξ) = 0 elsewhere, Fig.7.8. Its variance is
σ2
ε =
∆/2
"
−∆/2
ξ2pε(n)(ξ)dξ = ∆2
12 .
This kind of noise is used to model rounding errors in the amplitude
quantization of a signal. It indicates that all errors within −∆/2 ≤ξ < ∆/2
are equally probable.
7.3.2
Binary Noise
Random binary sequence, or binary noise, is a stochastic signal which
randomly assumes one of two ﬁxed signal values. Assume that the noise
ε(n) values are, for example, {−1,1} and that the probability that ε(n)
assumes value 1 is p. The mean of this noise is
µε = ∑
ξ=−1,1
ξPx(ξ) = (−1)(1 −p) + 1 · p = 2p −1.
The variance is
σ2
ε = ∑
ξ=−1,1
(ξ −µε)2Px(ξ) = 4p(1 −p).
A special case is when the values from the set {−1,1} are equally probable,
that is when p = 1/2. Then we get µε = 0 and σ2
ε = 1.

342
Discrete-Time Random Signals
Example 7.12. Consider a set of N →∞balls. Equal number of balls is marked with
1 (or white) and 0 (or black). A random signal x(n) corresponds to drawing of
four balls in a row. It has four values x(0), x(1), x(2), and x(3). Signal values
x(n) are equal to the marks on the drawn balls. Write all possible realizations
of x(n). If k is the number of appearances of value 1 in the signal, write the
probabilities for each value of k.
⋆Signal realizations, with the number k being the number of appear-
ances of digit 1 in each signal realization, are given in the next table.
x(0)
x(1)
x(2)
x(2)
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
k
0
1
1
2
1
2
2
3
1
2
2
3
2
3
3
4
Possible values of k are 0,1,2,3,4 with corresponding probabilities
P(0) = 1 ·
B
1
2
1
2
1
2
1
2
C
,
P(1) = 4 ·
B
1
2
1
2
1
2
C
1
2,
P(2) = 6 ·
B
1
2
1
2
C
1
2
1
2,
P(3) = 4 ·
B
1
2
C
1
2
1
2
1
2, and
P(4) = 1 · 1
2
1
2
1
2
1
2.
These probabilities can be considered as the terms of a binomial expression
(a + b)4
=
B
4
0
C
a4 +
B
4
1
C
a3b +
B
4
2
C
a2b2 +
B
4
3
C
ab3 +
B
4
4
C
b4
with a = 1/2 and b = 1/2. For the case when N is a ﬁnite number see Problem
7.6.
An interesting form of the random variable that can assume only two
possible values {−1,1} or {No,Yes} or {A,B} is the binomial random
variable. It has been introduced through the previous simple example. In
general, if a signal x(n) assumes value B from the set {A,B} with probability
p, then the probability that there is exactly k values of B in a sequence of N
samples of x(n) is
P(k) =
B
N
k
C
pk(1 −p)N−k
=
N!
k!(N −k)! pk(1 −p)N−k.
This is a binomial coefﬁcients form.

Ljubiša Stankovi´c
Digital Signal Processing
343
The expected value of the number of appearances of event B in N
samples, denoted by y, is
µy = E{y} =
N
∑
k=0
kP(k)
=
N
∑
k=0
k
N!
k!(N −k)! pk(1 −p)N−k.
Since the ﬁrst term in summation is 0 we will shift the summation for one
and reindex it to
µy = E{y} =
N−1
∑
k=0
(k + 1)
N(N −1)!
(k + 1)!((N −(k + 1))! pk+1(1 −p)N−(k+1)
= Np
N−1
∑
k=0
(N −1)!
k!((N −1) −k)! pk(1 −p)(N−1)−k.
The sum in the last expression is equal to
1 = (p + (1 −p))N−1 =
N−1
∑
k=0
B
N−1
k
C
pk(1 −p)(N−1)−k
=
N−1
∑
k=0
(N −1)!
k!((N −1) −k)! pk(1 −p)(N−1)−k
resulting, with p + (1 −p) = 1, into
µy = E{y} = Np.
As we could write from the beginning , the expected value of the number
of appearances of an event B, whose probability is p, in N realizations is
E{y} = Np. This derivation was performed not only to prove this fact, but
it will lead us to the next step in deriving the variance of the event y, by
using the expected value of the product of y and y −1,
E{y(y −1)} =
N
∑
k=0
k(k −1)P(k)
=
N
∑
k=0
k(k −1)
N!
k!(N −k)! pk(1 −p)N−k.

344
Discrete-Time Random Signals
Since the ﬁrst two terms are 0 we can reindex the summation into
E{y(y −1)} =
N−2
∑
k=0
(k + 2)(k + 1)
N!
(k + 2)!(N −2 −k)! pk+2(1 −p)N−2−k
= N(N −1)p2
N−2
∑
k=0
(N −2)!
k!(N −2 −k)! pk(1 −p)N−2−k.
The relation
N−2
∑
k=0
(N −2)!
k!(N −2 −k)! pk(1 −p)N−2−k = (p + (1 −p))N−2 = 1
is used to get
E{y(y −1)} = N(N −1)p2.
The variance of y follows from
σ2
y = E{y2} −(E{y})2
= E{y(y −1)} + E{y} −(E{y})2
= Np(1 −p).
Therefore, in a sequence of N values of signal x(n) that can assume values
{A,B} the mean value and variance of appearances of B divided by N will
be
µy = Np
N = p
σ2
y = Np(1 −p)
N2
= p(1 −p)
N
Increasing the number of the total values N the variance will be lower and
a ﬁnite set x(n) will produce a more reliable mean value p.
7.3.3
Gaussian Noise
The Gaussian (normal) noise is used to model a disturbance caused by many
small independent factors. Namely, the central limit theorem states that a
sum of a large number of statistically independent random variables, with
any distribution, obeys to the Gaussian (normal) distribution.
The Gaussian zero-mean noise has the probability density function
pε(n)(ξ) =
1
σε
√
2π
e−ξ2/(2σ2ε ).
(7.22)

Ljubiša Stankovi´c
Digital Signal Processing
345
0
10
20
30
40
50
60
-3
-2
-1
0
1
2
3 ε(n)
0
0.25
0.5
-3
-2
-1
0
1
2
3
px( ξ)
Figure 7.9
A realization of Gaussian noise (left) with probability density function (right).
Variance of this noise is σ2
ε . It is left to reader to prove this by evaluating
corresponding integral. For the Gaussian noise with mean µ and variance
σ2
ε we can use notation N (µ,σ2
ε ).
The probability that the amplitude of a zero-mean Gaussian random
variable takes a value smaller than λ is
Probability{|ε(n)| < λ} =
1
σε
√
2π
λ
"
−λ
e−ξ2/(2σ2ε )dξ = erf
*
λ
√
2σε
+
(7.23)
where
erf(λ) =
2
√π
λ
"
0
e−ξ2dξ
is the error function.
Commonly used probabilities that the absolute value of the noise is
within the standard deviation, two standard deviations (two-sigma rule),
or three standard deviations are:
Probability{−σε < ε(n) < σε} = erf(1/
√
2) = 0.6827,
(7.24)
Probability{−2σε < ε(n) < 2σε} = erf(
√
2) = 0.9545,
Probability{−3σε < ε(n) < 3σε} = erf(3/
√
2) = 0.9973.
Example 7.13. Given 12 measurements of a Gaussian zero-mean noise {−0.7519,
1.5163, −0.0326, −0.4251, 0.5894, −0.0628, −2.0220, −0.9821, 0.6125, −0.0549,
−1.1187, 1.6360}, estimate the probability that the absolute value of this noise
will be smaller than 2.5.

346
Discrete-Time Random Signals
-4
-3
-2
-1
0
1
2
3
4
0
0.1
0.2
0.3
0.4
0.5
px( ξ)
Figure 7.10
Probability density function with intervals corresponding to −σε < ε(n) < σε,
−2σε < ε(n) < 2σε, and −3σε < ε(n) < 3σε. Value of σε = 1 is used.
⋆The standard deviation of this noise could be estimated by using (7.7)
with µ = 0 and N = 12. It is σ = 1.031. Thus, the absolute value of this noise
will be smaller than 2.5 with probability
P =
1
1.031
√
2π
2.5
"
−2.5
e−ξ2/(2·1.0312)dξ = erf(2.5/(
√
2 · 1.031)) = 0.9847.
Example 7.14. Consider a signal s(n) = Aδ(n −n0) and a zero-mean Gaussian
noise ε(n) with variance σ2ε within the interval 0 ≤n ≤N −1, where n0 is a
constant integer within 0 ≤n0 ≤N −1. Find the probability of event A that
a maximum value of x(n) = s(n) + ε(n) is obtained at n = n0.
⋆Probability density function for any sample x(n), n ̸= n0, is
px(n),n̸=n0(ξ) =
1
σε
√
2π
e−ξ2/(2σ2
ε ).
The probability that any of these samples is smaller than a value of λ could
be deﬁned by using (7.23)
P−(λ) = Probability{x(n) < λ,n ̸= n0}
Probability{x(n) < 0,n ̸= n0} + Probability{0 ≤x(n) < λ,n ̸= n0}
= 0.5 + 0.5erf(λ/(
√
2σε)).
Since the random variables x(n), 0 ≤n ≤N −1,n ̸= n0, are statistically
independent, then the probability that all of them are smaller than a value

Ljubiša Stankovi´c
Digital Signal Processing
347
of λ is
P−
N−1(λ) = Probability{All N −1 values of x(n) < λ,n ̸= n0}
=
@
0.5 + 0.5erf(λ/(
√
2σε))
AN−1
.
The probability density function of the sample x(n0) is a Gaussian function
with the mean value A,
px(n0)(ξ) =
1
σε
√
2π
e−(ξ−A)2/(2σ2
ε ).
The probability that the random variable x(n0) takes a value around λ,
λ ≤x(n0) < λ + dλ, is
P+
n0(λ) = Probability{λ ≤x(n0) < λ + dλ} =
1
σε
√
2π
e−(ξ−A)2/(2σ2
ε )dλ (7.25)
The probability that all values of x(n),0 ≤n ≤N −1,n ̸= n0 are smaller than
λ and that, at the same time, λ ≤x(n0) < λ + dλ is
PA(λ) = P−
N−1(λ)P+
n0(λ) =
-
0.5 + 0.5erf
*
λ
√
2σε
+.N−1
1
σε
√
2π
e−(ξ−A)2/(2σ2
ε )dλ,
while the total probability that all x(n), 0 ≤n ≤N −1,n ̸= n0 are bellow
x(n0) is an integral over all possible values of λ
PA =
∞
"
−∞
PA(λ) =
∞
"
−∞
-
0.5 + 0.5erf
*
λ
√
2σε
+.N−1
1
σε
√
2π
e−(ξ−A)2/(2σ2
ε )dλ.
(7.26)
Example 7.15. Random signal x(n) is a Gaussian noise with the mean µx = 1
and variance σ2
x = 1. A random sequence y(n) is obtained by omitting
samples from signal x(n) that are either negative or higher that 1. Find the
probability density function of sequence y(n). Find its µy and σy.
⋆The probability density function for the sequence y(n) is
py(n)(ζ) =
%
B
1
√
2π e−(ζ−1)2
2
for 0 < ζ ≤1
0
otherwise
Constant B can be calculated from & ∞
−∞py(n)(ζ)dζ = 1, resulting in
B = 2/erf( 1
√
2
).

348
Discrete-Time Random Signals
Now we have
µy(n) =
1
"
0
ζ
2
erf( 1
√
2)
1
√
2π
e−(ζ−1)2
2
dζ = 1 −
√
2(1 −e−1/2)
√πerf( 1
√
2)
≈0.54
σ2
y(n) =
1
"
0
(ζ −µy(n))2
2
erf(
√
2)
1
√
2π
e−(ζ−1)2
2
dζ ≈0.08.
Example 7.16. Consider a random signal x(n) that can assume values
{No, Yes} with probabilities 1 −p and p. If a random realization of this
signal is available with N = 1000 samples and we obtained that the event
Yes appeared 555 times ﬁnd the interval where the true p will be with
probability of 0.95. Denote by y the number of observed Yes values divided
by N. We can assume that the mean value estimates for various realizations
are Gaussian distributed.
⋆This is a binomial random variable with the mean p and the variance
σ2
y = p(1 −p)
N
∼=
555
1000(1 −555
1000)
1000
= 0.2470
1000
σy = 0.0157.
Therefore the estimated value
ˆp = 555
1000
is within the range
ˆp = 0.555 ∈
D
p −2σy, p + 2σy
E
= [p −0.0314, p + 0.0314]
with probability 0.95, i.e.,
−0.0314 ≤0.555 −p ≤0.0314
|0.555 −p| ≤0.0314.
with the same probability. The true value is around 55.5% within 3.14%
range (from 52.36% to 58.64%) with probability 0.95. By increasing the value
of N we can reduce the margin of estimation error. However, about 1000
values are commonly used for various opinion poll estimations.

Ljubiša Stankovi´c
Digital Signal Processing
349
7.3.4
Complex Gaussian Noise and Rayleigh Distribution
In many application the complex-valued Gaussian noise is used as a model
for disturbance. Its form is
ε(n) = εr(n) + jεi(n)
where εr(n) and εi(n) are real-valued Gaussian noises. Commonly it is
assumed that they are zero-mean, independent, with identical distributions
(i.i.d.), and variance σ2/2.
The mean value of this noise is
µε = E{ε(n)} = E{εr(n)} + jE{εi(n)} = 0 + j0.
The variance is
σ2
ε = E{|ε(n)|2} = E{ε(n)ε∗(n)}
= E{εr(n)εr(n)} + E{εi(n)εi(n)} + j(E{εi(n)εr(n)} −E{εr(n)εi(n)}}
= E{εr(n)εr(n)} + E{εi(n)εi(n)} = σ2.
The amplitude of Gaussian noise |ε(n)| is an important parameter in
many detection problems. The probability density function of the complex-
Gaussian noise amplitude is
p|ε(n)|(ξ) = 2ξ
σ2 e−ξ2/σ2u(ξ).
The probability density function p|ε(n)|(ξ) is called the Rayleigh distribu-
tion.
In order to prove the previous relation consider the probability den-
sity function of εr(n) and εi(n). Since they are independent and equally
distributed then
pεrεi(ξ,ζ) = pεr(ξ)pεi(ζ) =
1
σ2π e−(ξ2+ς2)/σ2.
The probability that |ε(n)| =
F
ε2r(n) + ε2
i (n) < χ is
P{
F
ε2r(n) + ε2
i (n) < χ} =
""
ξ2+ς2<χ2
pεrεi(ξ,ζ)dξdζ
=
1
σ2π
""
ξ2+ς2<χ2
e−(ξ2+ς2)/σ2dξdζ.

350
Discrete-Time Random Signals
With ξ = ρcosα and ζ = ρcosα (the Jacobian of the polar coordinate trans-
formation is J = |ρ|) we get
P{
F
ε2r(n) + ε2
i (n) < χ} =
1
σ2π
χ
"
0
2π
"
0
e−ρ2/σ2ρdρdα
= 2
σ2
χ
"
0
e−ρ2/σ2ρdρdα =
χ2/σ2
"
0
e−λdλ = (1 −e−χ2/σ2)u(χ) = F|ε(n)|(χ).
The probability density function is
p|ε(n)|(ξ) =
dF|ε(n)|(ξ)
dξ
= 2ξ
σ2 e−ξ2/σ2u(ξ).
(7.27)
Example 7.17. A random signal is deﬁned as y(n) = |ε(n)|, where ε(n) is
the Gaussian complex zero-mean i.i.d. noise with variance σ2. What is the
probability that y(n) ≥A? Calculate this probability for A = 2 and σ2 = 1.
⋆The probability density function for sequence y(n) is
py(x) = 2ξ
σ2 e−ξ2
σ2 u(ξ)
The probability that y(n) ≥A is
P{ξ > A} = 1 −P{ξ ≤A} = e−A2
σ2 .
For A = 2 and σ2 = 1 we get P{ξ > A} ≈0.0183.
7.3.5
Impulsive Noises
This noise is used to model disturbances when strong impulses occur more
often than in the case of a Gaussian noise. Due to possible stronger pulses,
their probability density function decay toward ±∞is slower than in the
case of Gaussian noise.
The Laplacian noise has the probability density function
pε(n)(ξ) = 1
2αe−|ξ|/α.

Ljubiša Stankovi´c
Digital Signal Processing
351
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
0.2
0.4
0.6
pε(ξ)
Laplacian distribution
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
0.2
0.4
0.6
pε(ξ)
Gaussian distribution
Figure 7.11
The Gaussian and Laplacian noise histograms (with 10000 realizations), with
corresponding probability density function (dots).
It decays much slower as |ξ| increases than in the Gaussian noise case.
The Laplacian noise can be generated as
ε(n) = ε1(n)ε2(n) + ε3(n)ε4(n)
where εi(n), i = 1,2,3,4 are real-valued Gaussian independent zero-mean
noises, Fig.7.11 (for variance see Problem 7.13).
The impulsive noise could be distributed in other ways, like, for
example, the Cauchy distributed noise, whose probability density function
is
pε(n)(ξ) =
1
π(1 + ξ2).
The Cauchy distributed noise ε(n) is a random signal that can be obtained
as a ratio of two independent Gaussian random signals ε1(n) and ε2(n), i.e.,

352
Discrete-Time Random Signals
as
ε(n) = ε1(n)
ε2(n).
7.3.6
Noisy Signals
In the case of noisy signals the noise could added to the signal s(n). Then
we have
x(n) = s(n) + ε(n).
This is an additive noise. For a deterministic signal s(n)
E{x(n)} = E{s(n) + ε(n)} = s(n) + µε(n),
E{|x(n) −µε(n)|2} = σ2
ε (n).
Noise can also be multiplicative, when
x(n) = (1 + ε(n))s(n).
In this case
E{x(n)} = E{s(n) + ε(n)s(n)} = s(n)(1 + µε(n)),
E{|x(n) −µε(n)|2} = |s(n)|2 σ2
ε (n).
Both the mean and the variance are signal dependent in the case of multi-
plicative noise.
7.4
DISCRETE FOURIER TRANSFORM OF NOISY SIGNALS
Consider a noisy signal
x(n) = s(n) + ε(n)
(7.28)
where s(n) is a deterministic useful signal and ε(n) is an additive noise. The
DFT of this signal is
X(k) =
N−1
∑
n=0
(s(n) + ε(n))e−j2πkn/N = S(k) + Ξ(k).
(7.29)
The mean value of X(k) is
E{X(k)} =
N−1
∑
n=0
s(n)e−j2πkn/N +
N−1
∑
n=0
E{ε(n)}e−j2πkn/N = S(k) + DFT{µε(n)}.

Ljubiša Stankovi´c
Digital Signal Processing
353
In the case of a zero-mean noise ε(n), when µε(n) = 0, follows
µX(k) = E{X(k)} = S(k).
(7.30)
The variance of X(k), for a zero-mean noise, is
σ2
X(k) = E{|X(k) −µX(k)|2} = E{X(k)X∗(k) −S(k)S∗(k)}
=
N−1
∑
n1=0
N−1
∑
n2=0
E{(s(n1) + ε(n1))(s∗(n2) + ε∗(n2))}e−j2πk(n1−n2)/N
−
N−1
∑
n1=0
N−1
∑
n2=0
s(n1)s∗(n2)e−j2πk(n1−n2)/N
=
N−1
∑
n1=0
N−1
∑
n2=0
E{ε(n1)ε∗(n2)}e−j2πk(n1−n2)/N.
(7.31)
For a white noise, with the autocorrelation
rεε(n1,n2) = E{ε(n1)ε∗(n2)} = σ2
ε δ(n1 −n2),
we get
σ2
X(k) = σ2
ε N.
(7.32)
If the deterministic signal is a complex sinusoid,
s(n) = Aej2πk0n/N,
(7.33)
with a frequency adjusted to the grid ω0 = 2πk0/N, then its DFT is
S(k) = ANδ(k −k0).
Peak signal-to-noise ratio, being relevant parameter for the DFT based
estimation of frequency, is
PSNRout = maxk |S(k)|2
σ2
X
= A2N2
σ2ε N = A2
σ2ε
N.
(7.34)
It increases as N increases. We have expected this result since the signal
values are added in phase, increasing the DFT amplitude N times (its power
N2 times), while the noise values are added in power. Noise inﬂuence to
the DFT of a real-valued sinusoid s(n) = Acos(2πk0n/N) = (Aej2πk0n/N +
Ae−j2πk0n/N)/2 is illustrated in Fig. 7.12.

354
Discrete-Time Random Signals
n
x(n)
k
X(k)
n
x(n)
k
X(k)
Figure 7.12
Illustration of a signal x(n) = cos(6πn/64) and its DFT (top row); the same signal
corrupted with additive zero-mean real-valued Gaussian noise of variance σ2
ε = 1/4, along
with its DFT (bottom row).
The input signal-to-noise ratio (SNR) for signal 7.33 is
SNRin = Ex
Eε
=
N−1
∑
n=0
|x(n)|2
N−1
∑
n=0
E
M
|ε(n)|2N = NA2
Nσ2ε
= A2
σ2ε
.
(7.35)
If the maximal DFT value is detected then only its value could be used for
the signal reconstruction (equivalent to the notch ﬁlter at k = k0 being used).
The DFT of output signal is then
Y(k) = X(k)δ(k −k0).
The output signal in the discrete-time domain is
y(n) = 1
N
N−1
∑
n=0
Y(k)ej2πkn/N = 1
N X(k0)ej2πk0n/N.
Since X(k0) = AN + Ξ(k0), according to (7.29) and (7.32), where Ξ(k) is a
noise with variance σ2
ε N, we get
y(n) = Aej2πk0n/N + Ξ(k0)
N
ej2πk0n/N = x(n) + εX(n).

Ljubiša Stankovi´c
Digital Signal Processing
355
The output signal-to-noise ratio is
SNRout = Ex
EεX
=
N−1
∑
n=0
|x(n)|2
N−1
∑
n=0
E
!''' Ξ(k0)
N ej2πk0n/N
'''
26
= NA2
N Nσ2ε
N2
= N A2
σ2ε
= N · SNRin.
Taking 10log(◦) of both sides we get the signal-to-noise ratio relation in dB,
SNRout[dB] = 10log N + SNRin[dB].
(7.36)
Example 7.18. If the DFT of a noisy signal s(n) + ε(n) is calculated using a window
function w(n), ﬁnd its mean and variance. Noise is white, rεε = σ2ε δ(n), with
zero-mean.
⋆Here,
X(k) =
N−1
∑
n=0
w(n)[s(n) + ε(n)]e−j2πkn/N.
For this DFT, the mean value is
µX(k) = E{X(k)} =
N−1
∑
n=0
w(n)s(n)e−j2πkn/N = W(k) ∗k S(k)
where W(k) = DFT{w(n)}.
The variance of X(k) is
σ2
XX(k) =
N−1
∑
n1=0
N−1
∑
n2=0
w(n1)w∗(n2)σ2
ε δ(n1 −n2)e−j2πk(n1−n2)/N
= σ2
ε
N−1
∑
n=0
|w(n)|2 = σ2
ε Ew,
(7.37)
where Ew is the window energy.
Example 7.19. The DFT deﬁnition, for a given frequency index k, can be under-
stood as
X(k) =
N−1
∑
n=0
(s(n) + ε(n))e−j2πkn/N
= N
mean
n=0,1,...,N−1
M
(s(n) + ε(n))e−j2πkn/NN
(7.38)

356
Discrete-Time Random Signals
Based on the deﬁnition of median, discuss when the DFT estimation
XR(k) = N median
n=0,1,...,N−1Re
M
(s(n) + ε(n))e−j2πkn/NN
(7.39)
+ jN median
n=0,1,...,N−1Im
M
(s(n) + ε(n))e−j2πkn/NN
can produce better results than (7.38). Calculate the value X(0) using (7.38)
and estimate it by (7.39) for s(n) = exp(j4πn/N) with N = 8 and noise
ε(n) = 2001δ(n) −204δ(n −3). Which one is closer to the noise-free DFT
value?
⋆If we can expect strong impulsive noise then the mean value will be
highly sensitive to this noise. The median based calculation is less sensitive
to strong impulsive noise. For the given signal
s(n) = exp(jπn/2) = [1, j,−1,−j,1, j,−1,−j]
and noise ε(n) the value of X(0) is
X(0) = 0 + 2001 −204 = 805.
The median-based estimation is
XR(0) = 8 median
n=0,1,..,N−1{2002,0,−1,−204,1,0,−1,0}+
(7.40)
j8 median
n=0,1,..,N−1{0,1,0,−1,0,1,0,−1} = 0 + j0.
Obviously the median-based estimate is not inﬂuenced by this impulsive
noise. In this case it produced better estimate of the noise-free DFT.
7.4.1
Detection of a Sinusoidal Signal Frequency
Consider a set of data x(n), for 0 ≤n ≤N −1. Assume that this set of data
are noisy samples of signal s(n) = Aej2πk0n/N. Additive noise ε(n) is white,
complex-valued Gaussian with zero-mean independent real and imaginary
parts and variance σ2
ε . The aim is to ﬁnd the signal s(n) parameters from
the noisy observations x(n). Since the signal form is known we look for
a solution of the same form, using the model bej2πkn/N where b and k are
parameters that have to determined, and α = {b,k} is the set of parameters.
Parameter b is complex-valued. It includes amplitude and initial phase of
the model. For each value of x(n) we may deﬁne an error as a difference of
the given value x(n) and the assumed model, at the considered instant n,
e(n,α) = x(n) −bej2πkn/N.
(7.41)

Ljubiša Stankovi´c
Digital Signal Processing
357
Since the noise is Gaussian, the probability density function of the error is
p(e(n,α)) =
1
2πσ2ε
e−|e(n,α)|2/(2σ2ε ).
The joint probability density function for all samples from the data set is
equal to the product of individual probability density functions
pe(e(0,α),e(1,α),...,e(N −1,α)) =
1
(2πσ2ε )N e−∑N−1
n=0 |e(n,α)|2/(2σ2ε ).
The maximum-likelihood solution for parameters α = {b,k} in obtained
by maximizing this function for given values of x(n). Maximization of
pe(e(0,α),e(1,α),...,e(N −1,α)) is the same as the minimization of the total
square error,
ϵ(α) =
N−1
∑
n=0
|e(n,α)|2 =
N−1
∑
n=0
'''x(n) −bej2πkn/N'''
2
.
(7.42)
The solution of this problem is obtained from ∂ϵ(α)/∂b∗= 0 (see Example
1.3). It is in the form of a standard DFT of signal x(n),
b = 1
N
N−1
∑
n=0
x(n)e−j2πkn/N = mean
M
x(n)e−j2πkn/NN
= 1
N X(k).
A speciﬁc value of parameter k that minimizes ϵ(α) and gives the estimate
of the signal frequency index k0 is obtained by replacing the obtained b back
into relation (7.42) deﬁning ϵ(α),
ϵ(α) =
N−1
∑
n=0
|x(n) −bej2πkn/N|2 =
(
N−1
∑
n=0
|x(n)|2
)
−N|b|2.
Minimal value of ϵ(α) is achieved when |b|2 (or |X(k)|2) is maximal,
ˆk0 = arg{max|X(k)|2} = arg{max|X(k)|}.
If there is no noise |x(n)| = A , ˆk0 = k0, b = A or X(k0) = NA, and ϵ(k0) = 0.
The same approach can be used for a signal s(n) = Aejω0n. Assuming
the solution in the form bejωn, the Fourier transform of discrete-time signals
would follow.

358
Discrete-Time Random Signals
If the additive noise were, for example, Laplacian then the probability
density function would be p(e(n,α)) =
1
2σε e−|e(n,α)|/σε, and the solution of
ϵ(α) = ∑N−1
n=0 |e(n,α)| minimization would follow from
X(k) = Nmedian
M
x(n)e−j2πkn/NN
.
Note that the absolute value of error can be written as
|e(n,α)| =
'''x(n) −bej2πkn/N''' =
'''x(n)e−j2πkn/N −b
'''.
Minimization of a sum of this kind of terms is discussed in (10.72).
Now we will analyze the signal frequency estimation for a single
component sinusoidal signal s(n) with unknown discrete frequency ω0 =
2πk0/N using the DFT. Since a frequency on the frequency grid is assumed
this case can be understood as a frequency position detection. Available
observations of the signal are
x(n) = s(n) + ε(n), for 0 ≤n ≤N −1,
where ε(n) is a complex zero mean Gaussian white noise with independent
real and imaginary parts, with variance σ2
ε . Its DFT is
X(k) =
N−1
∑
n=0
(s(n) + ε(n))e−j2πkn/N = NAδ(k −k0) + Ξ(k),
with σ2
X(k) = σ2
ε N and E{Ξ(k)} = 0. The real and imaginary parts of the DFT
X(k0) at the signal position k = k0 are Gaussian random variables, with total
variance σ2
ε N, or
N (NA,σ2
ε N/2),
N (0,σ2
ε N/2),
(7.43)
respectively, where a real-valued A is assumed without any loss of general-
ity.
Real and imaginary parts of the noise only DFT values X(k) for k ̸= k0
are zero-mean random variables with the same variance
N (0,σ2
ε N/2).
Next, we will ﬁnd the probability that a DFT value of noise at any
k ̸= k0 is higher than the signal DFT value at k = k0. This case corresponds

Ljubiša Stankovi´c
Digital Signal Processing
359
to a false detection of the signal frequency position, resulting in an arbitrary
large and uniform estimation error (within the considered frequency range).
The probability density function for the absolute DFT values outside
the signal frequency is Rayleigh-distributed (7.27)
q(ξ) = 2ξ
σ2ε N e−ξ2/(σ2ε N),ξ ≥0.
The DFT at a noise only position takes a value greater than Ξ, with proba-
bility
Q(Ξ) =
∞
"
Ξ
2ξ
σ2ε N e−ξ2/(σ2ε N)dξ = exp(−Ξ2
σ2ε N ).
(7.44)
The probability that a DFT of noise only is lower than Ξ is [1 −Q(Ξ)]. The
total number of noise only points in the DFT is M = N −1. The probability
that M independent DFT noise only values are lower than Ξ is [1 −Q(Ξ)]M.
Probability that at least one of M DFT noise only values is greater than Ξ, is
G(Ξ) = 1 −[1 −Q(Ξ)]M .
(7.45)
The probability density function for the absolute DFT values at the
position of the signal (whose real and imaginary parts are described by
(7.43)) is Rice-distributed
p(ξ) = 2ξ
σ2ε N e−(ξ2+N2A2)/(σ2ε N)I0(2NAξ/(σ2
ε N)), ξ ≥0,
(7.46)
where I0(ξ) is the zero-order modiﬁed Bessel function (for A = 0, when
I0(0) = 1 the Rayleigh distribution is obtained).
When a noise only DFT value surpasses the DFT signal value, then an
error in estimation occurs. To calculate this probability, consider the absolute
DFT value of a signal at and around ξ. The DFT value at the signal position
is within ξ and ξ + dξ with the probability p(ξ)dξ , where p(ξ) is deﬁned
by (7.46). The probability that at least one of M DFT noise only values is
above ξ in amplitude is G(ξ) = 1 −[1 −Q(ξ)]M. Thus, the probability that
the absolute DFT signal component value is within ξ and ξ + dξ and that at
least one of the absolute DFT noise only values exceeds the DFT signal value
is G(ξ)p(ξ)dξ. Considering all possible values of ξ, from (7.44) and (7.45), it

360
Discrete-Time Random Signals
follows that the probability of the wrong signal frequency detection is
PE
=
∞
"
0
G(ξ)p(ξ)dξ =
∞
"
0
(
1 −
-
1 −exp(−ξ2
σ2ε N )
.M)
× 2ξ
σ2ε N e−(ξ2+N2A2)/(σ2ε N)I0(2NAξ/(σ2
ε N))dξ.
(7.47)
Approximation of this expression can be calculated by assuming that
the DFT of the signal component is not random and that it is equal to
NA (positioned at the mean value of the signals DFT). The form of error
probability is then very simple
PE ∼= 1 −
-
1 −exp(−NA2
σ2ε N )
.M
.
(7.48)
This expression can be used for a simple rough approximative analysis.
Analysis can easily be generalized to the case with K signal compo-
nents, s(n) = ∑K
k=1 Akejωkn.
In many cases, the discrete frequency of the deterministic signal does
not satisfy the relation ω0 = 2πk0/N, where k0 is an integer. In these cases,
when ω0 ̸= 2πk0/N, the frequency estimation result can be improved , for
example, by zero-padding before the Fourier transform calculation or using
ﬁner grid around the detected maximum. Comments on the estimation of
signal frequency outside the grid are given in Chapter III as well.
7.5
LINEAR SYSTEMS AND RANDOM SIGNALS
If a random signal x(n) passes through a linear time-invariant system, with
an impulse response h(n), then the mean value of the output signal y(n) is
µy(n) = E{y(n)} =
∞
∑
k=−∞
h(k)E{x(n −k)}
(7.49)
=
∞
∑
k=−∞
h(k)µx(n −k) = h(n) ∗n µx(n).
(7.50)
For a stationary signal
µy = µx
∞
∑
k=−∞
h(k) = µxH(ej0).
(7.51)

Ljubiša Stankovi´c
Digital Signal Processing
361
The cross-correlation of the output and input signal is
ryx(n,m) = E{y(n)x∗(m)} =
∞
∑
k=−∞
E{x(k)x∗(m)}h(n −k)
=
∞
∑
k=−∞
rxx(k,m)h(n −k).
(7.52)
For a stationary signal, with n −m = l and k −m = p, we get
ryx(l) =
∞
∑
p=−∞
rxx(p)h(l −p) = rxx(l) ∗l h(l).
The z-transform of both sides gives
Ryx(z) = Rxx(z)H(z).
The cross-correlation of the input and output signal is
rxy(n,m) = E{x(n)y∗(m)} =
∞
∑
k=−∞
E{x(n)x∗(k)}h∗(m −k)
=
∞
∑
k=−∞
rxx(n,k)h∗(m −k).
(7.53)
For a stationary signal, with n −m = l and n −k = p, we get
rxy(l) =
∞
∑
p=−∞
rxx(p)h∗(p −l).
The z-transform of both sides are
∞
∑
l=−∞
rxy(l)z−l =
∞
∑
l=−∞
∞
∑
p=−∞
rxx(p)h∗(p −l)z−l
=
∞
∑
k=−∞
∞
∑
p=−∞
rxx(p)h∗(k)z−p B
z−1C−k
Rxy(z) = Rxx(z)H∗( 1
z∗).
If we calculate the Fourier transform of both sides, we get
Sxy(ejω) = Sxx(ejω)H∗(ejω).
(7.54)

362
Discrete-Time Random Signals
Similarly, starting from
ryy(n,m) = E{y(n)y∗(m)}
=
∞
∑
k=−∞
∞
∑
l=−∞
E{x(l)x∗(k)}h(n −l)h∗(m −k),
(7.55)
after some straightforward calculations, we get the relation
Ryy(z) = Rxx(z)H(z)H∗( 1
z∗).
The Fourier transform of output signal autocorrelation function is
Syy(ejω) = Sxx(ejω)
'''H(ejω)
'''
2
,
(7.56)
proving that Sxx(ejω) is indeed a power density function. By taking a
narrow-pass ﬁlter with unit amplitude
''H(ejω)
''2 = 1 for ω0 ≤ω < ω0 + dω,
we will get the spectral density of signal x(n) for that small frequency range.
Example 7.20. A linear time-invariant system is deﬁned by
y(n) = x(n) + ax(n −1) + a2x(n −2).
The input signal is a zero-mean white noise ε(n) with variance σ2ε . Find the
cross-correlation of the input and output signal and the autocorrelation of
the output signal. For a = −1 ﬁnd the power spectral density of the output
signal.
⋆The system transfer function is
H(z) = 1 + az−1 + a2z−2.
Since the input signal is a white noise of variance σ2ε its autocorrelation, by
deﬁnition, is
rxx(n) = rεε(n) = σ2
ε δ(n).
The power spectral density of the input signal is
Sxx(ω) =
∞
∑
n=−∞
rxx(n)e−jωn = σ2
ε .
The z-transform of the input signal autocorrelation function is
Rxx(z) =
∞
∑
n=−∞
rxx(n)z−n = σ2
ε
∞
∑
n=−∞
δ(n)z−n = σ2
ε .

Ljubiša Stankovi´c
Digital Signal Processing
363
The z-transform of the autocorrelation function of the output signal, for linear
time-invariant system, is
Ryy(z) = Rxx(z)H(z)H∗(1/z∗)
= σ2
ε
@
1 + a2 + a4 + a(1 + a2)(z + z−1) + a2(z2 + z−2)
A
.
The autocorrelation function of the output signal is equal to the inverse z-
transform of Ryy(z),
ryy(n) = σ2
ε (1 + a2 + a4)δ(n) + σ2
ε a(1 + a2)(δ(n + 1) + δ(n −1))
+ σ2
ε a2(δ(n + 2) + δ(n −2)).
The power spectral density of the output signal is
Syy(ω) = Ryy(ejω)
= σ2
ε (1 + a2 + a4 + 2a(1 + a2)cosω + 2a2 cos(2ω)),
while the z-transform of the cross-correlation of the input and output signal
is
Ryx(z) = H(z)Rxx(z) = (1 + az−1 + a2z−2)σ2
ε .
Its inverse z-transform is the cross-correlation,
ryx(n) = σ2
ε (δ(n) + aδ(n −1) + a2δ(n −2)).
For a = −1 the power spectral density function of the output signal is
Syy(ω) = σ2
ε (3 −4cosω + 2cos(2ω))
= σ2
ε (1 −4cosω + 4cos2 ω) = σ2
ε (1 −2cosω)2.
Example 7.21. For a discrete-time system deﬁned by
y(n) −1.3y(n −1) + 0.36y(n −2) = x(n)
with the input signal x(n) = ε(n), µε = 0 and rεε(n) = δ(n), ﬁnd:
a) Mean value µy(n) and autocorrelation ryy(n) of the output signal,
b) Power spectral density functions Syy(ω) and Syx(ω).
⋆a) The mean value of output signal is
µy = µxH(ej0) = µεH(ej0) = 0.
The z-transform of the output signal autocorrelation is
Ryy(z) = Rxx(z)H(z)H(1/z)

364
Discrete-Time Random Signals
since H(z) is the z-transform of a real-valued signal. The autocorrelation of
the input signal is
Rxx(z) = 1.
The transfer function has the form
H(z) =
1
1 −1.3z−1 + 0.36z−2
=
1
(1 −0.9z−1)(1 −0.4z−1).
Therefore, the autocorrelation of the output signal is
Ryy(z) =
1
(1 −0.9z−1)(1 −0.4z−1)(1 −0.9z)(1 −0.4z)
or
Ryy(z) = 25
8
-
z
(z −0.4)(z −1/0.4) −
z
(z −0.9)(z −1/0.9)
.
.
The inverse z-transform of Ryy(z) is
ryy(n) = 25
8
-
(0.9)|n| 0.9
0.19 −(0.4)|n| 0.4
0.84
.
.
b) The power spectral density of the output signal is obtained as
Syy(ω) = Ryy(z)|z=ejω =
1
(1.16 −0.8cosω)(1.81 −1.8cosω),
while the cross-power spectral density function Syx(ω) can be deﬁned as the
value of Ryx(z) at z = ejω
Syx(ω) = Ryx(z)|z=ejω = H(z)Rxx(z)|z=ejω
=
1
1 −1.3cosω + 0.36cos2ω + j(1.3sinω −0.36sin2ω).
Example 7.22. A white noise ε(n) with variance σ2ε and zero mean is an input to
a linear time-invariant system. If the impulse response of the system is h(n)
show that
E{x(n)y(n)} = h(0)σ2
ε
and
σ2
y = σ2
ε
∞
∑
n=−∞
|h(n)|2 = σ2
ε Eh,
where y(n) is the output of this system.

Ljubiša Stankovi´c
Digital Signal Processing
365
⋆The mean value of the product of input and output signal is
E{x(n)y(n)} = E
%
∞
∑
k=−∞
h(k)x(n)x(n −k)
;
.
Since the impulse response is a deterministic signal
E{x(n)y(n)} =
∞
∑
k=−∞
h(k)E{x(n)x(n −k)} =
∞
∑
k=−∞
h(k)rxx(k)
and
rxx(n) = σ2
ε δ(n)
we get
E{x(n)y(n)} =
∞
∑
k=−∞
h(k)σ2
ε δ(k) = h(0)σ2
ε .
The variance of output signal is deﬁned by
σ2
y = E{y(n)y∗(n)} −E{y(n)} E{y∗(n)}
or
σ2y = E
%
∞
∑
k=−∞
h(k)x(n −k)
∞
∑
k=−∞
h∗(k)x∗(n −k)
;
−
−E
%
∞
∑
k=−∞
h(k)x(n −k)
;
E
%
∞
∑
k=−∞
h∗(k)x∗(n −k)
;
.
The output signal is zero-mean signal,
E{y(n)} = E{y∗(n)} =
∞
∑
k=−∞
h(k)E{x(n −k)} = 0.
Thus, we get
σ2
y =
∞
∑
k=−∞
∞
∑
l=−∞
h(k)h∗(l)E{x(n −k)x∗(n −l)}
=
∞
∑
k=−∞
∞
∑
l=−∞
h(k)h∗(l)rxx(l −k).
Since rxx(n) = σ2ε δ(n) , i.e., rxx(l −k) = σ2ε δ(l −k) , only the terms with l = k
remain in the double summation expression for the variance σ2y, producing
σ2
y = σ2
ε
∞
∑
k=−∞
|h(k)|2 = σ2
ε Eh.

366
Discrete-Time Random Signals
7.5.1
Spectral Estimation of Narrowband Signals
A narrowband random signal with Np components around frequencies ω1,
ω2, and ωNp can be considered, from a spectral point of view, as an output
of a system whose transfer function is of the form
H(z) =
G
(1 −r1ejω1z−1)(1 −r2ejω2z−1)...(1 −rNpejωNp z−1)
=
G
1 + a1z−1 + a2z−2 + ... + aNpz−Np .
when the input is a white noise. The amplitudes of the poles ri are inside
(and close to) the unit circle. The discrete-time domain description of this
system is
y(n) + a1y(n −1) + a2y(n −2) + ... + aNpy(n −Np) = Gx(n),
where x(n) is a white noise with variance σ2
x = 1, autocorrelation rxx(k) =
δ(k), and spectral energy density Sxx(ω) = 1. For a given narrowband
random signal y(n), the task is to ﬁnd coefﬁcients ai and G.
The autocorrelation of the output signal is obtained after the multipli-
cation of the difference equation by y(n + k),
y(n + k)y(n) + a1y(n + k)y(n −1) + ... + aNpy(n + k)y(n −Np)
= Gy(n + k)x(n),
and expected value calculation,
E{y(n + k)y(n) + a1y(n + k)y(n −1) + ... + aNpy(n + k)y(n −Np)}
= E{Gy(n + k)x(n)}.
For k = 0 it follows
ryy(0) + a1ryy(0 −1) + a2ryy(0 −2) + ... + aNpryy(0 −Np) = G2.
For k > 0 we get
ryy(k) + a1ryy(k −1) + a2ryy(k −2) + ... + aNpryy(k −Np) = 0.

Ljubiša Stankovi´c
Digital Signal Processing
367
The previous equations are known as the Yule-Walk equations. The matrix
form of this system is
⎡
⎢⎢⎣
ryy(0)
ryy(1)
...
ryy(Np)
ryy(1)
ryy(0)
...
ryy(Np −1)
...
...
...
...
ryy(Np)
ryy(Np −1)
...
ryy(0)
⎤
⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
1
a1
a2
...
aNp
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
G2
0
0
...
0
⎤
⎥⎥⎥⎥⎦
.
(7.57)
The system is solved for unknown system coefﬁcients [a0, a1, a2,...,aNp] with
G = 1. Then the coefﬁcients are normalized as [a0, a1, a2,...,aNp]/a0 with
G = 1/a0. The spectral energy density of y(n) follows with Sxx(ω) = 1 as
Syy(ω) =
'''''
G
1 + a1e−jω + a2e−j2ω + ... + aNpe−jNpω
'''''
2
.
(7.58)
This is the autoregressive (AR) spectral estimation.
Note that the autocorrelation functions can be estimated, for real-
valued y(n), deﬁned within 0 ≤n ≤N −1, as
ryy(k) =
1
N −k
N−1−k
∑
n=0
y(n + k)y(n)
for 0 ≤k ≤N −1,
(7.59)
and ryy(k) = ryy(−k) for −N + 1 ≤k < 0. These values are then used in
(7.57) for the autoregressive spectral estimation.
Next we will comment the estimated autocorrelation within the basic
deﬁnition of the power spectral density, Section 7.2.3. Relation (7.59) corre-
sponds to the unbiased estimation of the autocorrelation function. Power
spectral density, according to (7.17), is calculated as Syy(ω) = FT{ryy(k)}.
Since the autocorrelation estimates for a large k use only a small
number of signal samples in averaging, they are not reliable. It is common
to apply a triangular (Bartlett) window function (w(k) = (N −|k|)/N) to
reduce the weight of these estimates in the Fourier transform calculation
w(k)ryy(k) = w(k)
1
N −k
N−1−k
∑
n=0
y(n + k)y(n)
= (N −k)
N
1
N −k
N−1−k
∑
n=0
y(n + k)y(n)
= 1
N
N−1−k
∑
n=0
y(n + k)y(n)
(7.60)

368
Discrete-Time Random Signals
for 0 ≤k ≤N −1. Since the window is used this autocorrelation function
estimate is biased. The Fourier transform of biased autocorrelation func-
tion w(k)ryy(k) = (1 −|k|/N)ryy(k) is the power spectral density Pyy(ω) =
FT{(1 −|k|/N)ryy(k)} deﬁned by (7.19).
Example 7.23. Consider a random signal
y(n) = 2cos(0.95n + ϕ1) +
√
2sin(1.05n + ϕ2) + 0.5
within 0 ≤n ≤127, where ϕ1 and ϕ2 are random variables. Plot the power
spectral density calculated using:
(a) The Fourier transform of ryy(k)
Syy(ω) = FT{ryy(k)} =
N−1
∑
k=−N+1
ryy(k)e−jωk.
(b) The Fourier transform of signal
Pyy(ω) = 1
N
'''''
N−1
∑
n=0
y(n)e−jωn
'''''
2
.
This form corresponds to FT{wB(k)ryy(k)} where wB(k) is a Bartlett window
whose width is equal to the width of the autocorrelation function.
(c) Applying the Fourier transform to K = 7 shorter intervals of signal
of duration M = 32 with step R = M/2
Yi(ejω) = 1
M
M−1
∑
n=0
y(iR + n)e−jωn
for i = 0,1,...,6 and averaging the power spectral density over these intervals
(Welch periodogram)
SA
yy(ω) = 1
K
K−1
∑
i=0
'''Yi(ejω)
'''
2
.
(d) Using (7.58) with appropriately estimated coefﬁcients ai and G
using (7.57) and (7.59).
⋆The results are shown in Fig.7.13., in order from (a) to (d).
7.6
DETECTION AND MATCHED FILTER
Detection of an unknown deterministic signal in a high noise environment is
of crucial interest in many real-world applications. In this case the problem

Ljubiša Stankovi´c
Digital Signal Processing
369
-3
-2
-1
0
1
2
3
-0.5
0
0.5
1
(a)
-3
-2
-1
0
1
2
3
-0.5
0
0.5
1
(b)
-3
-2
-1
0
1
2
3
-0.5
0
0.5
1
(c)
-3
-2
-1
0
1
2
3
-0.5
0
0.5
1
(d)
Figure 7.13
Spectral analysis of sinusoidal signals with random phases (normalized values).
is in testing the hypothesis
H0 : Signal is not present in the observed noisy signal
(7.61)
H1 : Signal is present in the observed noisy signal
Here we will present the of detection of a known signal in a white
noise using the matched ﬁlter.
7.6.1
Matched Filter
Consider a general signal form
x(n) = s(n) + ε(n),

370
Discrete-Time Random Signals
where s(n) is a known function with the Fourier transform S(ejω) and ε(n)
is a white noise with power spectral density σ2
ε . The problem is to ﬁnd a
system with a maximal output if the input x(n) contains the signal s(n).
The output signal is used to test the hypothesis H1: presence of the signal
s(n) in x(n).
The output of a system with impulse response h(n), with the fre-
quency response H(ejω), to the signal x(n) is of the form
y(n) = ys(n) + yε(n)
where ys(n) and yε(n) are the system outputs to the inputs s(n) and ε(n),
respectively. For the output signal ys(n) holds
Ys(ejω) = H(ejω)S(ejω).
Power spectral density of ys(n) is
'''Ys(ejω)
'''
2
=
'''H(ejω)
'''
2 '''S(ejω)
'''
2
.
The power of output noise is
E{|yε(n)|2} = 1
2π
π
"
−π
'''H(ejω)
'''
2
σ2
ε dω.
The output signal y(n), at an instant n0, is
ys(n0) = 1
2π
π
"
−π
H(ejω)S(ejω)ejωn0dω
|ys(n0)|2 =
''''''
1
2π
π
"
−π
H(ejω)S(ejω)ejωn0dω
''''''
2
.
The aim is to maximize the output signal at an instant n0 if the input signal
contains s(n). According to Schwartz’s inequality (for its discrete form see
Section 10.3.3)
''''''
1
2π
π
"
−π
H(ejω)S(ejω)ejωn0dω
''''''
≤1
2π
π
"
−π
'''S(ejω)
'''
2
dω 1
2π
π
"
−π
'''H(ejω)
'''
2
dω,

Ljubiša Stankovi´c
Digital Signal Processing
371
the peak output signal-to-noise ratio is
PSNR =
|ys(n0)|2
E{|yε(n)|2}
≤
1
2π
π
"
−π
''S(ejω)
''2 dω 1
2π
π
"
−π
''H(ejω)
''2 dω
1
2π
π
"
−π
''H(ejω)
''2 σ2ε dω
.
This ratio is maximal when the equality sign holds
PSNRmax =
1
2πσ2ε
π
"
−π
'''S(ejω)
'''
2
dω = Es
σ2ε
.
The maximal ratio in Schwartz’s inequality is achieved for
H(ejω) = kS∗(ejω)e−jωn0.
In the time domain the impulse response is
h(n) = ks∗(n0 −n).
This system is called matched ﬁlter. Its impulse response is matched
to the signal form. It maximizes the ratio of the output signal and the noise.
Thus, it is used in the detection, i.e., used to make a decision if the known
signal s(n) exists in the noisy signal x(n).
The matched ﬁlter is illustrated on detection of a chirp signal
s(n) = e−2(n/128)2 cos(8π(n/128)2 + πn/8)
in a Gaussian white noise of variance σ2
ε = 1. The output of the matched
ﬁlter is calculated for n0 = 0 by using the known signal as
y(n) = x(n) ∗n s(−n).
Two cases are presented in Fig.7.14: 1) When the input signal contains s(n)
and 2) when the input signal does not contain s(n). We can see that the
output of the matched ﬁlter has an easily detectable peak at n = 0 for the
case then the input signal contains s(n). There is no such a peak in y(n)
when the input signal x(n) is noise only.

372
Discrete-Time Random Signals
-2
-1
0
1
2
-1
0
1
s(n)
-2
-1
0
1
2
-5
0
5
x(n)=s(n)+ε(n)
-2
-1
0
1
2
-5
0
5
x(n)=ε(n)
-2
-1
0
1
2
-50
0
50
100
y(n)
-2
-1
0
1
2
-50
0
50
100
y(n)
Figure 7.14
Illustration of the matched ﬁlter: Signal s(n). Input noisy signal x(n) = s(n) + ε(n)
containing signal s(n). Input signal x(n) = ε(n) does not contain signal s(n). Corresponding
outputs from the matched ﬁlter y(n) = x(n) ∗s(−n) are presented bellow the input signal
subplots.
7.7
OPTIMAL WIENER FILTER
Assume that the input signal is x(n) and that it contains an information
about the desired signal d(n). The output signal is y(n) = h(n) ∗n x(n).
The task here is to ﬁnd the impulse response h(n) of system such that the
difference of the desired signal and the output signal, denoted as error
e(n) = d(n) −y(n),

Ljubiša Stankovi´c
Digital Signal Processing
373
is minimal in the mean square sense, i.e.,
h(n) = min
h(n){E{|e(n)|2}}.
The mean square error is
E{|e(n)|2} = E{
'''''d(n) −
∞
∑
m=−∞
h(m)x(n −m)
'''''
2
}.
The minimal value is obtained from
∂E{|e(n)|2}
∂h∗(k)
= E
%
2
(
d(n) −
∞
∑
m=−∞
h(m)x(n −m)
)
x∗(n −k)
;
= 0. (7.62)
This relation states that expected value of the product of error signal e(n) =
d(n) −y(n) and the input signal x∗(n −k) is zero
E{2e(n)x∗(n −k)} = 0
for any k. For signals satisfying this relation we say that they are normal to
each other.
Relation (7.62) can be written as
E
%
∞
∑
m=−∞
h(m)x(n −m)x∗(n −k)
;
= E{d(n)x∗(n −k)}
or
∞
∑
m=−∞
h(m)rxx(k −m) = rdx(k).
Taking the z-transform of both sides we get
H(z)Rxx(z) = Rdx(z).
Transfer function of the optimal ﬁlter is
H(z) = Rdx(z)
Rxx(z).
For a special case when the input signal is the desired signal d(n) with an
additive noise
x(n) = d(n) + ε(n)

374
Discrete-Time Random Signals
where ε(n) is uncorrelated with the desired signal, the optimal Wiener
ﬁltering relation follows
H(z) =
Rdd(z)
Rdd(z) + Rεε(z)
since
rdx(k) = E{d(n)x∗(n −k)}
= E{d(n)[d∗(n −k) + ε∗(n −k)]}
= rdd(k).
Here we used E{d(n)ε∗(n −k)} = 0 since d(n) and ε(n) are uncorrelated.
Also
rxx(k) = E{[d(n) + ε(n)][d∗(n −k) + ε∗(n −k)]}
= rdd(k) + rεε(k).
The frequency response of the optimal ﬁlter is
H(ejω) =
Sdd(ω)
Sdd(ω) + Sεε(ω).
Example 7.24. A signal x(n) = d(n) + ε(n) is processed by an optimal ﬁlter. Power
spectral density of d(n) is Sdd(ω). If the signal d(n) and the additive noise
ε(n), whose power spectral density is Sεε(ω), are independent ﬁnd the output
signal-to-noise ratio.
⋆For this signal and noise, according to (7.56), we have
Syy(ejω) =
'''H(ejω)
'''
2
Sxx(ejω)
Syy(ejω) =
''''
Sdd(ω)
Sdd(ω) + Sεε(ω)
''''
2
Sxx(ejω)
=
S2
dd(ω)
Sdd(ω) + Sεε(ω)
since Sxx(ejω) = Sdd(ω) + Sεε(ω). The output signal-to-noise ratio is
SNR =
1
2π
& π
−π Sdd(ω)
'''H(ejω)
'''
2
dω
1
2π
& π
−π Sεε(ω)
''H(ejω)
''2 dω
.

Ljubiša Stankovi´c
Digital Signal Processing
375
Note that the input signal-to-noise ratio is
SNRi =
1
2π
& π
−π Sdd(ω)dω
1
2π
& π
−π Sεε(ω)dω
.
The optimal prediction system follows with the input signal x(n) =
d(n −1) + ε(n −1) and the desired signal d(n). Transfer function of the
optimal predictor is obtained from
rdx(k) = E{d(n)x∗(n −k)}
= E{d(n)[d∗(n −1 −k) + ε∗(n −1 −k)]} = rdd(k + 1)
and
rxx(k) = E{[d(n −1) + ε(n −1)][d∗(n −1 −k) + ε∗(n −1 −k)]}
= rdd(k) + rεε(k).
as
H(z) =
zSdd(z)
Sdd(z) + Sεε(z)
since
∞
∑
k=−∞
rdd(k + 1)z−k =
∞
∑
k=−∞
rdd(k)z−k+1 = zSdd(z).
The optimal smoothing is the case when the desired signal is d(n) and
we can use its future value(s). It follows with x(n) = d(n + 1) + ε(n + 1) as
H(z) =
z−1Sdd(z)
Sdd(z) + Sεε(z).
Example 7.25. The input signal is x(n) = s(n) + ε(n), where d(n) = s(n) is the
desired signal and ε(n) is a noise. If the autocorrelation functions of the signal
and noise are rss(n) = 4−|n| and rεε(n) = 2δ(n), respectively, and the cross-
correlation of the signal and noise is rsε(n) = δ(n), design the optimal ﬁlter.
⋆The optimal ﬁlter transfer function is
H(z) = Rdx(z)
Rxx(z)
where are
Rdx(z) = Rss(z) + Rsε(z)

376
Discrete-Time Random Signals
Rxx(z) = Rss(z) + 2Rsε(z) + Rεε(z).
Based on the correlation functions we can calculate the z-transforms
Rss(z) =
∞
∑
n=−∞
rss(n)z−n =
∞
∑
n=−∞
4−|n|z−n =
−1
∑
n=−∞
4nz−n +
∞
∑
n=0
4−nz−n =
=
z/4
1 −z/4 +
1
1 −1/(4z) =
−3.75z
(z −0.25)(z −4)
and
Rsε(z) = 1
Rεε(z) = 2.
The transfer function of the optimal ﬁlter is
H(z) =
Rss(z) + Rsε(z)
Rss(z) + 2Rsε(z) + Rεε(z) = 0.25z2 −2z + 0.25
z2 −5.1875z + 1 .
The optimal systems with FIR ﬁlters will be presented within the
introductory part of the chapter dealing with adaptive discrete systems.
7.8
QUANTIZATION EFFECTS
In order to process continuous signals using computers they have to be con-
verted into numbers stored into registers of a ﬁnite precision. Continuous
signals are transformed into digital by using analog-to-digital (A/D) con-
verters. A continuous-time signal is converted into a discrete-time signal by
taking samples of the continuous-time signal at discrete-time instants
x(n) = x(n∆t)∆t.
Next the discrete-time signal, with continuous amplitudes, is converted into
a digital signal
xQ(n) = Q[x(n)]
with discrete-valued amplitudes (quantization). This process is illustrated
in Fig.7.15. Error caused by this process is called quantization noise.
Quantization noise inﬂuences results of signal processing in several
ways:
-Input signal quantization error, described by an additive quantization
noise. This inﬂuence (additive input noise that depends on quantization

Ljubiša Stankovi´c
Digital Signal Processing
377
0
5
10
15
0
0.2
0.4
0.6
0.8
1
t
x(t)
continuous
0
5
10
15
0
0.2
0.4
0.6
0.8
1
n
x(n)
discrete-time
0
5
10
15
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
n
xd(n)
digital
0000
0001
0010
0011
0100
0101
0110
0111
1000
1001
1010
1011
1100
1101
1110
1111
Figure 7.15
Illustration of a continuous signal and its discrete-time and digital version.
step ∆) can be modeled as uniform noise with values between −∆/2 and
∆/2.
-Quantization of the results of arithmetic operations. It depends on the
way how the calculations are performed.
-Quantization of the coefﬁcients in the algorithm. Usually this kind
of error is neglected in analysis since it is deterministic (comments on the
errors in the coefﬁcients are given in the chapter dealing with realizations
of discrete systems).
In order to make appropriate analysis, common assumptions are:
1) random variables corresponding to the quantization errors are un-
correlated, i.e., the quantization error is a white noise process with a uniform
distribution,
2) the error sources are uncorrelated with one another, and
3) all the errors are uncorrelated with the input signal and, conse-
quently, with all signals in the system.
7.8.1
Input signal quantization
For registers with b bits the digital signal values xQ(n) are coded into binary
format.

378
Discrete-Time Random Signals
Assume that registers with b bits are used and that all input signals
are normalized to the range 0 ≤x(n) < 1. The binary numbers are written
within the register as
a−1
a−2
a−3
...
a−b .
The value of xQ(n) is
xQ(n) = a−12−1 + a−22−2 + ... + a−b2−b.
The maximal number that can be written within this format is 0.111...11
representing 1 −2−(b+1). Common number of bits b ranges from 8 to 24.
For reducing the signal number of digits to b bits rounding or trun-
cation is used. An example of quantization with b = 4 bits is presented in
Fig.7.15, where the maximal value of xd(n) = xQ(n) is denoted by 1111
meaning 2−1 + 2−2 + 2−3 + 2−4 = 15/16.
For the case with positive an negative numbers, one extra bit is used
for the sign. The registers are now with b + 1 bits. The ﬁrst bit is the sign bit
and the remaining b bits represent the signal absolute value
s
a−1
a−2
a−3
...
a−b
.
In computers negative numbers are commonly represented in a comple-
ment of 2 form.
In order to distinguish these two cases we will use register of length b
meaning no sign bit exists and register of length b + 1, where the sign bit is
used.
Example 7.26. In a register with b = 8 bits, the binary number xQ(n)
1
0
1
1
0
0
1
0
has the decimal value
xQ(n) = 1 · 2−1 + 1 · 2−3 + 1 · 2−4 + 1 · 2−7 = 89
128 = 0.6953.
The decimal point assumes position just before the ﬁrst digit. The values of
xQ(n) in this register are
0 ≤xQ(n) ≤255
256
with the quantization step 1/256.

Ljubiša Stankovi´c
Digital Signal Processing
379
The quantization error is a difference in the amplitude
e(n) = x(n) −xQ(n).
For rounding, the maximum absolute error can be a half of the last digit
weight
−1
22−b ≤x(n) −xQ(n) < 1
22−b
−1
2∆≤x(n) −xQ(n) < 1
2∆
where
∆= 2−b.
We can also write
|e(n)| ≤2−(b+1) = 1
2∆.
In the example from Fig.7.15, obviously the quantization step is 2−4 = 1/16
and the error is within |e(n)| ≤1
2
1
16.
The error values are equally probable within the deﬁned interval. Its
probability density function is
pe(ξ) =
⎧
⎨
⎩
1
∆
for
−1
2∆≤ξ < 1
2∆
0
elsewhere.
,
The quantization error of signal x(n) may be described as an additive
uniform white noise.
Mean of the quantization error, with rounding, is
µe = E{e(n)} =
∆/2
"
−∆/2
ξpe(ξ)dξ = 0.
Its variance is
σ2
e =
∆/2
"
−∆/2
1
∆(ξ −µe)2dξ = 1
12∆2.
When the truncation is used, the error is within
0 ≤x(n) −xQ(n) < ∆

380
Discrete-Time Random Signals
or
0 ≤e(n) < ∆
with mean value
µe = E{e(n)} = ∆
2
and variance
σ2
e =
∆
"
0
1
∆(ξ −∆
2 )2dξ = 1
12∆2.
Example 7.27. The DFT of a signal x(n) is calculated by using its quantized
version
xQ(n) = Q[x(n)] = x(n) + e(n).
Quantization is done in an A/D convertor with b + 1 = 8 bits using round-
ing. The DFT is calculated on a high precision computer with N = 1024
signal samples. Find the mean and variance of the calculated DFT.
⋆The DFT of quantized signal is
XQ(k) =
N−1
∑
n=0
[x(n) + e(n)]e−j2πkn/N.
Its mean is
µXQ(k) = E{XQ(k)} =
N−1
∑
n=0
x(n)e−j2πkn/N = X(k).
The variance is
σ2
XQ(k) =
N−1
∑
n1=0
N−1
∑
n2=0
σ2
e δ(n1 −n2)e−j2πk(n1−n2)/N
= σ2
e N = 1
12∆2N = 1
122−2bN
= 1
122−14N = 1
122−14210 =
1
192.
The noise in the DFT is a sum of many independent noises from the
input signal and coefﬁcients. Thus it is Gaussian distributed with standard
deviation σXQ = 0.072. It may signiﬁcantly inﬂuence the signal DFT values,
especially if they are not well concentrated or if there are signal components
with small amplitudes.

Ljubiša Stankovi´c
Digital Signal Processing
381
Example 7.28. How the input quantization error inﬂuences the results of:
(a) Weighted sum
Xs =
N−1
∑
n=0
anx(n)
(b) Product
XP =
N−1
∏
n=0
x(n).
⋆If the quantized values xQ(n) = Q[x(n)] = x(n) + e(n) of signal x(n)
are used in calculation instead of the signal true values then:
(a) The estimator of a weighted sum is
ˆXs =
N−1
∑
n=0
anxQ(n) =
N−1
∑
n=0
anx(n) +
N−1
∑
n=0
ane(n).
Obviously the total error is
eXs =
N−1
∑
n=0
ane(n).
It is Gaussian distributed since there are many small errors e(n). It has been
assumed that the weighting coefﬁcients are such that they allow many signal
values to inﬂuence result with similar weights.
The mean value is
µXs = E{eXs} =
N−1
∑
n=0
anE{e(n)} = 0,
for rounding. The variance is
σ2
Xs =
N−1
∑
n=0
a2
nvar{e(n)} = 1
12∆2
N−1
∑
n=0
a2
n.
(b) The estimator of the product is
ˆXP =
N−1
∏
n=0
(x(n) + e(n)).
Assuming that the individual errors are small so that all higher order error
terms containing e(n)e(m), e(n)e(m)e(l),... could be neglected we get
ˆXP ∼=
N−1
∏
n=0
x(n) +
N−1
∑
m=0
N−1
∏
n=0
n̸=m
x(n)e(m).

382
Discrete-Time Random Signals
The quantization effect caused error is
eXP =
N−1
∑
m=0
N−1
∏
n=0
n̸=m
x(n)e(m).
It is interesting to note that the relative error is additive since
rXP = eXP
XP
=
N−1
∑
m=0
N−1
∏
n=0
n̸=m
x(n)e(m)
N−1
∏
n=0
x(n)
=
N−1
∑
m=0
e(m)
x(m) =
N−1
∑
m=0
rx(m).
The mean value is zero if rounding is used. The variance is signal dependent,
σ2
Xp =
N−1
∑
m=0
N−1
∏
n=0
n̸=m
x2(n)var{e(n)} = 1
12∆2
N−1
∑
m=0
N−1
∏
n=0
n̸=m
x2(n).
7.8.2
Quantization of the results
In the quantization of results after the basic arithmetic operations are per-
formed we can distinguish two cases. One is with ﬁxed point arithmetic. In
that case the register assumes that the decimal point is at the ﬁxed place. All
data are written with respect to this position. In the ﬂoating point arithmetic
numbers are written in the sign-mantissa-exponent format. The quantiza-
tion error is then produced on mantissa only.
7.8.2.1
Fixed point arithmetic
Fixed point arithmetic assumes that the decimal point position is ﬁxed.
Common assumption is that the all input values and the mid-results, in this
case, are normalized so that 0 ≤x(n) < 1 or −1 < x(n) < 1 if sign bit is used.
In multiplications, the result of a multiplication
xQ(n)xQ(m)
will, in general, produce a result of 2b digits. It should be quantized in the
same way as the input signal
Q[xQ(n)xQ(m)] = xQ(n)xQ(m) + e(n,m)

Ljubiša Stankovi´c
Digital Signal Processing
383
where e(n,m) is the quantization error satisfying all the previous properties
with
−1
2∆≤e(m,n) ≤1
2∆.
Example 7.29. Find the mean of quantization error for
r(n) =
N−1
∑
m=0
x(n + m)x(n −m)
where x(n) is quantized and the product of signals is quantized as well to b
bits. Assume that the signal values are such that their additions will not cause
overﬂow.
⋆For this calculation the model is
ˆr(n) =
N−1
∑
m=0
D
xQ(n + m)xQ(n −m) + e(n + m,n −m)
E
=
N−1
∑
m=0
{[x(n + m) + e(n + m)][(x(n −m) + e(n −m)] + e(n + m,n −m)}.
The mean value is
E{ˆr(n)} =
N−1
∑
m=0
x(n + m)x(n −m) + E{
N−1
∑
m=0
e(n + m)e(n −m)}
= r(n) + E{e2(n)} = r(n) + 1
12∆2,
since it is assumed that errors for two different signal samples are not
correlated E{e(n + m)e(n −m)} = 0 for m ̸= 0 and the signal and errors are
not correlated, E{x(n + m)e(n −m)} = 0 for any m and n.
In general the additions cause quantization error as well. Namely in
adding two values 0 ≤x(n) < 1 the result could be greater than 1. In order
to avoid the overﬂow the input values are shifted in the register to the left
(appropriately divided), causing quantization error.
I the case that complex-valued numbers are used in calculation then
the quantization of real and imaginary parts is done separately,
xQ(n) = Q[x(n)] = Q[Re{x(n)} + jIm{Q[x(n)]}] = x(n) + er(n) + jei(n).
Since the real and imaginary part are independent, with the same variance,
the variance of quantization error for a complex-valued signal is
σ2
e = 2 1
12∆2 = 1
6∆2.

384
Discrete-Time Random Signals
For the additions the variance is doubled as well.
In case of multiplications one complex-valued multiplication requires
four real-valued multiplications, introducing four errors. The quantization
variance of a complex-valued multiplication is
σ2
e = 4 1
12∆2 = 1
3∆2.
If the values of a signal x(n) are not small we have to ensure that no
overﬂow occurs during the calculations using the ﬁxed point arithmetic.
Consider a real-valued random white signal whose samples are within
−1 < x(n) < 1, with variance σ2
x. Registers of b + 1 bits are assumed, with
one bit being used for the sign. As an example consider the mean value
calculation
XN = 1
N
N−1
∑
n=0
x(n).
We have to be sure that an overﬂow will not occur during the mean value
calculation. All sums should stay within the interval (−1,1).
One approach to calculate XN is in dividing the input signal values by
N and summing them
XN = x(0)
N
+ x(1)
N
+ ... + x(N −1)
N
.
Then we are sure that no result will be outside the interval (−1,1). By divid-
ing the signal samples by N an additive quantization noise is introduced,
ˆXN = x(0)
N
+ e(0) + x(1)
N
+ e(1) + ... + x(N −1)
N
+ e(N −1).
Variance of the equivalent noise e(0) + e(1) + ··· + e(N −1) is
σ2
e = 1
12∆2N = 1
122−2bN.
Since the variance of x(n)/N is σ2
x/N2, the variance of ˆXN is
σ2
XN = N σ2
x
N2 + 1
12∆2N.
Ratio of variances corresponding to the signal and noise in the result is
SNR =
N σ2x
N2
1
12∆2N = 1
N2
σ2
x
1
12∆2 = 1
N2
σ2
x
1
122−2b

Ljubiša Stankovi´c
Digital Signal Processing
385
or in [dB]
SNR = 10log( 1
N2
σ2
x
1
122−2b )
= 20logσx −20log N −20log2−b + 10log(12)
= 20logσx −20 log2 N
log2 10 −20log2 2−b
log2 10 + 10.8
= 20logσx −6.02(m −b) + 10.8,
where N = 2m. Obviously increasing the number of samples N to 2N will
keep the same SNR if b is increased for one bit, since (m + 1 −(b + 1)) =
m −b.
Another way to calculate the mean is in performing the summation
step by step, according to the scheme presented, for example for N = 8, as
XN =
x(0)
2
+ x(1)
2
2
+
x(2)
2
+ x(3)
2
2
2
+
x(4)
2
+ x(5)
2
2
+
x(6)
2
+ x(7)
2
2
2
.
Here two adjunct signal values x(n) are divided by 1/2 ﬁrst. They are
added then, avoiding possible overﬂow. The error in one step is
x(n)
2
+ e(n) + x(n + 1)
2
+ e(n + 1) = x(n) + x(n + 1)
2
+ e(2)
n .
The error
e(2)
n
= e(n) + e(n + 1)
has the variance
var
M
e(2)
n
N
= 1
12∆2 + 1
12∆2 = 1
6∆2.
After each division by 2 the result is shifted in the register to the right and
a quantization error is created. Thus the error model, due to the addition
quantization, is
ˆXN =
x(0)
2 + x(1)
2 +e(2)
0
2
+
x(2)
2 + x(3)
2 +e(2)
2
2
+ e(4)
0
2
(7.63)
+
x(4)
2 + x(5)
2 +e(2)
4
2
+
x(6)
2 + x(7)
2 +e(2)
6
2
+ e(4)
4
2
+ e(8)
0

386
Discrete-Time Random Signals
= x(0)
N
+ x(1)
N
+ ... + x(N −1)
N
+ e(2)
0
N/2 + e(2)
2
N/2 + ... + e(2)
N−2
N/2 +
+ e(4)
0
N/4 + ... + e(4)
N−4
N/4 +
....
+ e(N)
0
N/N .
The variance of all qunatization noises is the same
σ2
e = 1
6∆2 = 1
62−2b.
Note that the noises in the ﬁrst stage are divided by N/2, due to divisions
by 2 in the next stages of summation. Their variance is reduced for N2/4.
The value of variance of errors in these stages is
var{ e(2)
0
N/2 + e(2)
2
N/2 + ... + e(2)
N−2
N/2 } = 1
6∆2
1
N2/4
N
2 = 1
6∆2 2
N
var{ e(4)
0
N/4 + ... + e(4)
N−4
N/4 } = 1
6∆2
1
N2/16
N
4 = 1
6∆2 4
N
...
var{ e(N)
0
N/N } = 1
6∆2
1
N2/N2
N
N = 1
6∆2 N
N = 1
6∆2 2m
N .
The total variance of ˆXN is
σ2
XN = N σ2
x
N2 + 1
6∆2 2
N + 1
6∆2 4
N + ... + 1
6∆2 2m
N
(7.64)
= σ2
x
N + 1
6∆2 2
N (1 + 2 + ... + 2m−1) = σ2
x
N + 1
6∆2 2
N
1 −2m
1 −2
= σ2
x
N + 1
6∆2 2
N (N −1) = σ2
x
N + 1
3∆2(1 −1
N ).

Ljubiša Stankovi´c
Digital Signal Processing
387
Ratio of the variances, corresponding to the output signal-to-noise ratio, is
SNR =
σ2x
N
1
3∆2(1 −1
N ). =
σ2
x
1
3∆2(N −1)
∼= 1
N
σ2
x
1
32−2b = 3σ2
x22(b−m/2).
Signiﬁcant improvement (for an order of N) is obtained using this way of
the summation, instead of the direct one. In dB the ratio is
SNR ∼= 10log
P
3σ2
x22(b−m/2)Q = 20logσx −6.02(m
2 −b) + 4.8.
If the signal values were complex then 2−2b/12 would be changed to
2−2b/6.
The previous results are common in literature. They are derived as-
suming that the variances of the errors are the same and obtained assum-
ing unform nature of the quantization errors. However these results differ
from the ones obtained by statistical analysis. The reason is in the quan-
tization error distribution and variance. Namely, after the high precision
signal x(n) is divided by 2 and stored into b + 1 bit registers, the errors
in x(n)/2 + e(n) are unform with −∆/2 ≤e(n) < ∆/2. When these val-
ues are stored into registers, then in each next stage when we calculate
[x(n)/2 + e(n)] + [x(n + 1)/2 + e(n + 1)]/2 the input values x(n)/2 + e(n)
and x(n + 1)/2 + e(n + 1) are already stored in the b + 1 bit registers. Di-
vision by 2 is just a one bit shift to the right. This shift cases one bit error.
Therefore this one bit error is discrete in amplitude
ed ∈{−∆/2,0,∆/2},
with probabilities Pd(±∆/2) = 1/4 and Pd(0) = 1/2. Mean value of this
error kind of is zero, provided that the rounding is done in such a way that it
takes values ±∆/2 with equal probability (various tie-breaking algorithms
for rounding exist). Its variance is
var
M
e(i)
n
N
= 2var{ed} = 2[1
4(−∆
2 )2 + 1
4(∆
2 )2] = 1
4∆2, for i > 2.
The total variance of ˆXN is then of form
σ2
XN = N σ2
x
N2 + 1
6∆2 2
N + 1
4∆2 4
N + ... + 1
4∆2 2m
N = σ2
x
N + 1
2∆2(1 −4
3N ),
instead of (7.64). Signal-to-noise ratio is
SNR =
σ2x
N
1
2∆2(1 −
4
3N )
∼= 2σ2
x22(b−m/2).

388
Discrete-Time Random Signals
The previous analysis corresponds to the calculation of the DFT coefﬁ-
cient X(0) when the input signal is a random uniform signal whose values
are −1 < x(n) < 1 with variance σ2
x. A model for a coefﬁcient X(k), with all
quantization errors included, is
ˆX(k) = 1
N
N−1
∑
n=0
M
[x(n) + ei(n)]Wnk
N + em(n)
N
=
N−1
∑
n=0
y(n),
where ei(n) is the input signal quantization error and em(n) is the multipli-
cation quantization error. The variances for complex-valued signals are
var{ei(n)} = 2 1
12∆2 = 1
6∆2,
var{em(n)} = 4 1
12∆2 = 1
3∆2.
In addition, we have to provide that additions do not produce an overﬂow.
If we use the calculation scheme, presented for N = 8, as
ˆX(k) =
y(0)
2 + y(1)
2 +e(2)
0
2
+
y(2)
2 + y(3)
2 +e(2)
2
2
+ e(4)
0
2
+
y(4)
2 + y(5)
2 +e(2)
4
2
+
y(6)
2 + y(7)
1 +e(2)
6
2
+ e(4)
4
2
+ e(8)
0 ,
then in each addition the terms should be divided by 2. This division
introduces a quantization error. In the ﬁrst step
y(n)
2
+ e(n) + y(n + 1)
2
+ e(n + 1) = 1
2{[x(n) + ei(n)]Wnk
N + em(n)+
[x(n + 1) + ei(n + 1)]W(n+1)k
N
+ em(n + 1)} + e(n) + e(n + 1).
The total error in this step is
e(2)
n
= ei(n)Wnk
N + em(n) + ei(n+1)W(n+1)k
N
+ em(n+1)
2
+ e(n) + e(n + 1)
with variance
var{e(2)
n } = 1
4
*1
6∆2 + 1
3∆2 + 1
6∆2 + 1
3∆2
+
+ 21
6∆2 = 7
12∆2.
In all other steps, within the errors e(4)
0
to e(N)
0
, just the addition errors
appear. Their variance, for complex-valued terms, is
var{e(i)
n } = 21
6∆2.

Ljubiša Stankovi´c
Digital Signal Processing
389
Therefore, the variance of
ˆXN = x(0)
N
+ x(1)
N
+ ... + x(N −1)
N
+ e(2)
0
N/2 + e(2)
2
N/2 + ... + e(2)
N−2
N/2 +
+ e(4)
0
N/4 + ... + e(4)
N−4
N/4 + .... + e(N)
0
N/N
(7.65)
is obtained using
var{ e(2)
0
N/2 + e(2)
2
N/2 + ... + e(2)
N−2
N/2 } = 7
12∆2
1
N2/4
N
2 = 7
12∆2 2
N
var{ e(4)
0
N/4 + ... + e(4)
N−4
N/4 } = 1
3∆2
1
N2/16
N
4 = 1
3∆2 4
N
...
var{ e(N)
0
N/N } = 1
3∆2
1
N2/N2
N
N = 1
3∆2 2m
N .
The total variance of ˆXN is
σ2
XN = σ2
x
N + 1
3∆2 2
N (3
4 + 1 + 2 + ... + 2m−1)
= σ2
x
N + 2
3∆2 N −1
4
N
∼= σ2
x
N + 2
3∆2
with
SNR = 20logσx −6.02(m
2 −b) + 1.76.
If the described discrete nature of the quantization error amplitude,
after the ﬁrst quatization step, is taken into account (provided that the
rounding is done in such a way that the error takes values ±∆/2 with equal
probability), then with
var
M
e(i)
n
N
= 4var{ed} = 1
2∆2,
for i > 2, the variance of ˆXN follows
σ2
XN = σ2
x
N + ∆2
N (7
6 + 2 + 4 + ... + 2m−1)
= σ2
x
N + ∆2 N −5
4
N
∼= σ2
x
N + ∆2.

390
Discrete-Time Random Signals
If the FFT is calculated using the ﬁxed point arithmetic and the signal
is uniform, distributed within −1 < x(n) < 1 with variance σ2
x, then in order
to avoid an overﬂow the signal could be divided at the input with N and
the standard FFT could be used, as in Fig.7.16.
An improvement in the SNR can be achieved if the scaling is done
not to the input signal x(n) by N but by 1/2 in each butterﬂy, as shown in
Fig.7.17. The improvement is here due to the fact that the quantization errors
appearing in the early butterﬂy stages are divided by 1/2 and reduced at the
output as in (7.63). Improvement of an order of N is obtained in the output
signal-to-noise ratio.
7.8.2.2
Floating point arithmetic
Fixed point arithmetic is simple, but could be inefﬁcient if the signal values
within wide range of amplitudes may be expected. For example, if we can
expect signal values
xQ(n1) = 1011111110101.010
xQ(n2) = 0.0000000000110101
then obviously ﬁxed point arithmetic would require large registers so that
both values can be stored without loosing their signiﬁcant digits. However,
we can represent these signal values into the exponential form as
xQ(n1) = 1.011111110101010 × 212
xQ(n2) = 1.10101 × 2−11
The exponential format of numbers is then written within the register in the
following format
sn
se
e1
e2
e3
e4
e5
e6
e7
m−1
m−2
m−3
...
m−b
where:
sn is the sign of number (1 for positive number and 0 for negative
number)
se is the sign of exponent (1 for positive exponent and 0 for negative
exponent)
e1e2...e7 is the binary format of exponent, and
m−1m−2...m−b is the mantissa, assuming that the integer value is
always 1, it is omitted.

Ljubiša Stankovi´c
Digital Signal Processing
391
 x(7)/N 
  X(7)
 x(6)/N 
  X(3)
 x(5)/N 
  X(5)
 x(4)/N 
  X(1)
 x(3)/N 
  X(6)
 x(2)/N 
  X(2)
 x(1)/N 
  X(4)
 x(0)/N 
  X(0)
 W 3
8  
-1
 W 2
8  
-1
 W 1
8  
-1
 W 0
8  
-1
 W 2
8  
-1
 W 0
8  
-1
 W 2
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
 W 0
8  
-1
Figure 7.16
FFT calculation scheme obtained by decimation in frequency for N = 8 with
signal being divided by N in order to avoid overﬂow when the ﬁxed point arithmetic is used.
 x(7) 
  X(7)
 x(6) 
  X(3)
 x(5) 
  X(5)
 x(4) 
  X(1)
 x(3) 
  X(6)
 x(2) 
  X(2)
 x(1) 
  X(4)
 x(0) 
  X(0)
 1/2 
 1/2 
 1/2 
 1/2 
 1/2 
1/2 
 1/2 
1/2 
 1/2 
1/2 
 1/2 
1/2 
 W 3
8  
-1/2
 W 2
8  
-1/2
 W 1
8  
-1/2
 W 0
8  
-1/2
 W 2
8  
-1/2
1/2
1/2
1/2
1/2
1/2
1/2
 W 0
8  
-1/2
1/2
1/2
1/2
1/2
1/2
1/2
 W 2
8  
-1/2
 W 0
8  
-1/2
 W 0
8  
-1/2
1/2
1/2
1/2
 W 0
8  
-1/2
1/2
1/2
1/2
 W 0
8  
-1/2
1/2
1/2
1/2
 W 0
8  
-1/2
1/2
1/2
1/2
Figure 7.17
FFT calculation scheme obtained by decimation in frequency for N = 8 with
signal being divided in each butterﬂy by 1/2 in order to avoid overﬂow when the ﬁxed point
arithmetic is used.

392
Discrete-Time Random Signals
Within this format, the previous signal value xQ(n1), with a register of
19 bits in total, is
1
1
0
0
0
1
1
0
0
0
1
1
1
1
1
1
1
0
1
while xQ(n2) is
1
0
0
0
0
1
0
1
1
1
0
1
0
1
0
0
0
0
0 .
If the exponent cannot be written within the deﬁned number of bits (here
7) the computer has to stop the calculation and indicate "overﬂow", that
is, the number cannot ﬁt into the register. For mantissa the values are just
rounded to the available number of bits. In the implementations based on
the ﬂoating-point arithmetic, the quantization affects the mantissa only. The
relative error in mantissa is again
|e(n)| ≤2−(b+1) = 1
2∆.
The error in signal is multiplied by the exponent. Since we can say that the
exponent value is of the signal order, we can write
Q[x(n)] = xQ(n) = x(n) + e(n)x(n) = x(n)(1 + e(n)).
The error behaves here as a multiplicative uniform noise. Thus, for the
ﬂoating-point representation, multiplicative errors appear.
The ﬂoating-point additions also produce the quantization errors,
which are represented by a multiplicative noise. During additions the num-
ber of bits may increase. This increase in the number of bits requires man-
tissa shift, what causes multiplicative error.
In addition to the IEEE standard when the total number of bits is 32
(23 for mantissa and 7 for exponent) we will mention two standard formats
for the telephone signal coding. The µ-law pulse-coded modulation (PCM)
is used in the North America and the A-law PCM is used in European tele-
phone networks. They use 8-bit representations with a sign bit, 3 exponent
bits, and 4 mantissa bits
s
e1
e2
e3
m1
m2
m3
m4 .
The µ-law encoding takes a 14-bit signed signal value (its two’s complement
representation) as input, adds 33 (binary 100001) and converts it to an 8 bit
value. The encoding formula in the µ-law is
(−1)s @
2e+1(m + 16.5) −33
A
.

Ljubiša Stankovi´c
Digital Signal Processing
393
This is a 14-bit signed integer from −8031 to +8031.
The sign bit s is set to 1 if the input sample is negative. it is set to 0 if
the input sample is positive. Number 0 is written as
0
0
0
0
0
0
0
0 .
As an example consider the positive numbers from +1 to +30. They
are written as +21(m + 16.5) −33 with 15 quantization step 2 (starting
from m = 1 to m = 15). Then the numbers from +31 to +94 are written
as +22(m + 16.5) −33 with 16 quantization steps 4 (with m from 0 to 15).
The last interval for positive numbers is from +4063 to +8158 written as
+28(m + 16.5) −33 with 16 quantization intervals (with m from 0 to 15)
of 256. The range of input values is from −8159 to +8159 (±213) with the
minimum step size 2 for the smallest amplitudes.
Compression function corresponding to this format of signal 0 ≤|x| ≤
1 is
F(x) = sign(x)ln(1 + µ|x|)
ln(1 + µ) .
with µ = 255.
Example 7.30. Write the number a = 456 in the binary µ-law format.
⋆The number to be represented by 2e+1(m + 16.5) is 456 + 33 = 489.
The mantissa range is 0 ≤m ≤15. It means that the exponent (e + 1) should
be such that
0 + 16.5 ≤489
2e+1 ≤15 + 16.5
for the range 16.5 ≤m + 16.5 ≤31.5. It is easy to conclude that 489/16 =
30.5625, meaning e + 1 = 4 with m + 16.5 = 30.5625. The nearest integer value
of m is m = 14. Therefore ˆa = 23+1 × (14 + 16.5) −33 = 455 is the nearest µ-
law format number to a. The binary form is
0
0
1
1
1
1
1
0 .
Quantization step for this range of numbers is 24 = 16. It means that the
previous possible number is 439, while the next possible number would be
471. It is the last number with 2e+1 = 16.
Example 7.31. Write a model for calculation of
r(n,m) = x(n + m)x(n −m)
if the quantization error is caused by ﬂoating point registers with b bits for
mantissa. What is the mean value? Write the model for
y(n) = x(n) + x(n + 1).

394
Discrete-Time Random Signals
Signals are real-valued.
⋆For this calculation the model is
ˆr(n,m) = x(n + m)(1+ e(n + m))x(n −m)(1+ e(n −m))(1+ e(n + m,n −m)).
The mean value is
E{ˆr(n)} = x(n + m)x(n −m) + E{e(n + m)e(n −m)}
= r(n) + E{e2(n)}δ(m) = r(n) + 1
12∆2δ(m).
For y(n) the model is
ˆy(n) = [x(n)(1 + e(n)) + x(n + 1)(1 + e(n + 1))](1 + e(n,n + 1))
where e(n,n + 1) is the is the multiplicative noise modeling the addition error.
7.9
PROBLEMS
Problem 7.1. Signal x20i(n), for i = 01,02,..,15, is the monthly average of
maximal daily temperatures in a city measured from year 2001 to 2015.
Values are given in Table 7.2. If we can assume that the signal for individual
month is Gaussian ﬁnd the probability that the average of maximal daily
temperatures: (a) in January is lower than 2, (b) in January is higher than 12.
Problem 7.2. Cumulative probability distribution function F(χ) is given as
F(χ) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0
χ ≤0
χ/2
0 < χ ≤1
1/2
1 < χ ≤2
(χ −1)/2
2 < χ ≤3
1
χ > 3.
Find the probability density function p(ξ) and the probability that x(n) <
2.5.
Problem 7.3. Probability density function of x(n) is
px(ξ) = ae−b|ξ|, −∞< ξ < ∞,
where a and b are constants. Find the relation between a and b. What is the
cumulative probability distribution function for a = 1?

Ljubiša Stankovi´c
Digital Signal Processing
395
Problem 7.4. Random signal x(n) has a probability density function
px(ξ) = λ
2 e−λ|ξ|, λ > 0.
Find the mean and variance of x(n).
Problem 7.5. Joint probability density function of signals x(n) and y(n) is
pxy(ξ,ζ) =
!
kξe−ξ(ζ+1)
0 ≤ξ < ∞0 ≤ζ < ∞
0
elsewhere.
Find the value of constant k.
Problem 7.6. A set of N = 10 balls is considered, with equal number of
balls being marked with 1 (or white) and 0 (or black). A random signal x(n)
corresponds to drawing four balls in a row. It has four values x(0), x(1),
x(2), and x(3). Signal values are equal to the number on randomly drawn
ball. If k is the number of values 0 that appear in the signal (number of black
balls), write the probability for k = 0. Generalize the result for N ballas and
M signal samples.
Problem 7.7. Random signal x(n) is such that x(n) = 0 with probability
0.8. In all other cases x(n) is Gaussian random variable with mean 3 and
variance 2. Find the mean and variance of x(n).
Problem 7.8. Signal ε(n) is a Gaussian noise with mean µε = 0 and variance
σ2
ε . Find probability that |ε(n)| > A. If the signal length is N = 2000 ﬁnd the
expected number of samples with amplitudes higher than A = 10 assuming
that σ2
ε = 2. What is the result for A = 4 and σ2
ε = 2.
Problem 7.9. Random signal x(n) is a Gaussian noise with mean 0 and
variance σ2
x. Signal has a large number N of samples. Random sequence
y(n) is obtained by using M samples from signal x(n) with the lowest
amplitudes. Find µy and σy.
Problem 7.10. Random signal x(n) is a Gaussian noise with mean 0 and
variance σ2
x. Random sequence y(n) is obtained by omitting samples from
signal x(n) whose amplitude is higher than A. Find the probability density
function of sequence y(n). Find µy and σy.
Problem 7.11. Signal samples x(n) are such that
x(n) =
!
A + ε(n)
for n ∈Nx
ε(n)
otherwise

396
Discrete-Time Random Signals
where ε(n) is a Gaussian noise with mean µε = 0 and variance σ2
ε , A > 0
is a constant and Nx is nonempty set of discrete time instants. A threshold
based criterion is used to detect if an arbitrary time instant n belongs to the
set Nx
n ∈Nx
if x(n) > T,
where T is threshold. Find threshold T if the probability of false detection is
0.01.
Problem 7.12. Signal x(n) is a random Gaussian sequence with mean
µx = 5 and variance σ2
x = 1. Signal y(n) is a random Gaussian sequence,
independent from x(n), with mean µy = 1 and variance σ2
y = 1. If we
consider N = 1000 samples of these signals ﬁnd the expected number of
time instants where x(n) > y(n) holds.
Problem 7.13. Let x(n) and y(n) be independent real-valued white Gaus-
sian random variables with means µx = µy = 0 and variances σ2
x and σ2
y.
Show that the random variable
z = 1
M
M
∑
n=1
x(n)y(n)
has the variance
σ2
z = 1
M σ2
xσ2
y.
Problem 7.14. A random signal ε(n) is stationary and Cauchy distributed
with probability density function
pε(n)(ξ) =
a
1 + ξ2 .
Find the coefﬁcient a, mean, and variance of signal.
Problem 7.15. A causal system is deﬁned by
y(n) = x(n) + 0.5y(n −1).
Input signal is x(n) = aδ(n) with a random amplitude a. Random variable a
is uniformly distributed within the interval from 4 to 5. Find the mean and
autocorrelation of the output signal. Is the output signal WSS?
Problem 7.16. Consider a Hilbert transformer with the impulse response
h(n) =
%
2
π
sin2(nπ/2)
n
,
n ̸= 0
0,
n = 0 .

Ljubiša Stankovi´c
Digital Signal Processing
397
Input signal is a white noise with variance 1.
a) Find the autocorrelation function of the output signal.
b) Find the cross-correlation of the input and output signal. Show that
it is an antisymmetric function.
c) Find the autocorrelation and the power spectral density function of
an analytic signal εa(n) = ε(n) + jεh(n), where εh(n) = ε(n) ∗n h(n).
Problem 7.17. Consider a causal system
y(n) −ay(n −1) = x(n).
If the input signal is white noise x(n) = ε(n), with the autocorrelation
function rεε(n) = σ2
ε δ(n), ﬁnd the autocorrelation and the power spectral
density of the output signal.
Problem 7.18. Consider a linear time-invariant system whose input is
x(n) = ε(n)u(n)
and the impulse response is
h(n) = anu(n),
where ε(n) is a stationary real-valued noise with mean µε and autocorrela-
tion rεε(n,m) = σ2
ε δ(n −m) + µ2
ε. Find the mean and variance of the output
signal.
Problem 7.19. Find the mean, autocorrelation, and power spectral density
of random signal
x(n) = ε(n) +
N
∑
k=1
akej(ωkn+θk),
where ε(n) is a stationary real-valued noise with mean µε and autocorre-
lation rεε(n,m) = σ2
ε δ(n −m) + µ2
ε and θk are random variables uniformly
distributed over −π < θk ≤π. All random variables are statistically inde-
pendent.
Problem 7.20. Find a stable optimal ﬁlter if the correlation functions for the
signal and noise are rss(n) = 0.25|n| , rsε(n) = 0 and rεε(n) = δ(n). Discuss
ﬁlter causality.
Problem 7.21. Calculate the DFT value X(2) for x(n) = exp(j4πn/N) with
N = 8 and noise ε(n) = 2001δ(n) −204δ(n −3) using
X(k) =
N−1
∑
n=0
(s(n) + ε(n))e−j2πkn/N

398
Discrete-Time Random Signals
and estimate the DFT using
XR(k) = N median
n=0,1,..,N−1Re
M
(s(n) + ε(n))e−j2πkn/NN
+ jN median
n=0,1,..,N−1Im
M
(s(n) + ε(n))e−j2πkn/NN
.
Problem 7.22. The spectrogram is one of the most commonly used tools in
time-frequency analysis. Its form is
Sx(n,k) =
'''''
N−1
∑
i=0
x(n + i)w(i)e−j 2π
N ik
'''''
2
where the signal is x(n) = s(n) + ε(n), with s(n) being the desired deter-
ministic signal and ε(n) a complex-valued, zero-mean white Gaussian noise
with variance σ2
ε and independent and identically distributed (i.i.d.) real
and imaginary parts. Window function is w(i). Using a rectangular window
of the width N ﬁnd:
a) the mean of Sx(n,k),
b) the variance of Sx(n,k).
Note: For a Gaussian random signal ε(n), it holds
E{ε(l)ε∗(m)ε∗(n)ε(p)} = E{ε(l)ε∗(m)}E{ε∗(n)ε(p)}
+E{ε(l)ε∗(n)}E{ε∗(m)ε(p)} + E{ε(l)ε(p)}E{ε∗(m)ε∗(n)}.
Problem 7.23. The basic time-frequency distribution is the Wigner distribu-
tion, whose discrete-time form reads
Wx(n,ω) =
L
∑
k=−L
x(n + k)x∗(n −k)e−j2ωk
where the signal is x(n) = s(n) + ε(n), with s(n) being the desired deter-
ministic signal and ε(n) complex-valued, zero-mean white Gaussian noise
with variance σ2
ε and independent and identically distributed (i.i.d.) real
and imaginary parts. Find:
a) the mean value of Wx(n,ω),
b) the variance of Wx(n,ω).
Use the previous problem note. Write the variance form for an FM
signal when |s(n)| = A.
Problem 7.24. A random signal s(n) carries an information. Its autocorre-
lation function is rss(n) = 4(0.5)|n|. A noise with variance of autocorrelation
rεε(n) = 2δ(n) is added to the signal. Find the optimal ﬁlter for:

Ljubiša Stankovi´c
Digital Signal Processing
399
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
Sdd (ejω)
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
Sεε (ejω)
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
H(ejω)
Figure 7.18
Power spectral densities of the signal
''S(ejω)
''2 and input noise Sεε(ejω) along
with the frequency response of an optimal ﬁlter H(ejω).
a) d(n) = s(n) - optimal ﬁltering,
b) d(n) = s(n −1) - optimal smoothing,
c) d(n) = s(n + 1) - optimal prediction.
Problem 7.25. Design an optimal ﬁlter if the autocorrelation function of the
signal is rss(n) = 3(0.9)|n|. The autocorrelation of noise is rεε(n) = 4δ(n),
while the cross–correlation of the signal and noise is rsε(n) = 2δ(n) .
Problem 7.26. The power spectral densities of the signal Sdd(ejω) and input
noise Sεε(ejω) are given in Fig.7.18. Show that the frequency response of the
optimal ﬁlter H(ejω) is presented in Fig.7.18(bottom). Find the SNR at the
input and output of the optimal ﬁlter.

400
Discrete-Time Random Signals
Problem 7.27. Find the mean of quantization error of the Wigner distribu-
tion (its pseudo form over-sampled in frequency)
Wx(n,k) =
N−1
∑
m=0
x(n + m)x(n −m)e−j2πmk/N
where x(n) is real-valued quantized signal. The product of signals is quan-
tized to b bits as well. Neglect the quantization of the coefﬁcients e−j2πmk/N
and the quantization of their products with the signal.
7.10
SOLUTIONS
Solution 7.1. (a) The mean value for January, Table 7.2, is
µx(1) = 7.2667.
The standard deviation for January, calculated over 15 years, is σx(1) =
2.7115. Probability that the average maximal temperature in January is
lower than 2 is
P(x(1) < 2) =
2
"
−∞
1
σx(1)
√
2π
e
−(ξ−µx(1))2
2σ2x(1)
dξ =
= 0.5
-
1 −erf(7.2667 −2
2.7115
√
2
)
.
= 0.0260.
It means that this even will occur once in about 40 years.
(b) The average maximal temperature is higher than 12 with probabil-
ity
P(x(1) > 12) =
∞
"
12
1
σx(1)
√
2π
e
−(ξ−µx(1))2
2σ2x(1)
dξ =
0.5
-
1 −erf(12 −7.2667
2.7115
√
2
)
.
= 0.0404.
It means that this will happen once in about 25 years.

Ljubiša Stankovi´c
Digital Signal Processing
401
Solution 7.2. For the cumulative probability distribution function
F(χ) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0
χ ≤0
χ/2
0 < χ ≤1
1/2
1 < χ ≤2
(χ −1)/2
2 < χ ≤3
1
χ > 3
the probability density function is the derivative of F(χ),
px(ξ) = dF(ξ)
dξ
=
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0
ξ ≤0
1/2
0 < ξ ≤1
0
1 < ξ ≤2
1/2
2 < ξ ≤3
0
ξ > 3.
The probability of x(n) < 2.5 is P(x(n) < 2.5) = F(2.5) = 0.75.
Solution 7.3. Integral of the probability density function over (−∞,∞) is
" ∞
−∞px(ξ)dξ = 1.
Therefore
" ∞
−∞ae−b|ξ|dξ = a
-" 0
−∞ebξdξ +
" ∞
0
e−bξdξ
.
= 2a
b = 1,
resulting in b = 2a.
For a = 1 the probability density function is px(ξ) = e−2|ξ| for −∞<
ξ < ∞. The probability distribution function is
Fx(χ) =
" χ
−∞px(ξ)dξ =
" χ
−∞e2ξdξ = e2χ
2
for −∞< χ < 0, and
Fx(χ) = 1
2 +
" χ
0 pϵ(ξ)dξ = 1
2 +
" χ
0 e−2ξdξ = 1 −e−2χ
2
for 0 < χ < ∞.

402
Discrete-Time Random Signals
Solution 7.4. The mean is
Ex =
" ∞
−∞ξpx(ξ)dξ =
" 0
−∞ξeλξdξ +
" ∞
0
ξe−λξdξ
= −
" ∞
0
ξe−λξdξ +
" ∞
0
ξe−λξdξ = 0.
The variance of x is obtained from
σ2
x(n) = E
M
|x(n) −E{x(n)}|2N
= E{x2(n)} =
" ∞
−∞ξ2px(ξ)dξ = 2
λ2 .
Solution 7.5. Since
" ∞
−∞
" ∞
−∞pxy(ξ,ζ)dξdζ = 1
" ∞
0
" ∞
0
kxe−ξ(ζ+1)dξdζ =
" ∞
0
kξe−ξdξ
" ∞
0
e−ξζdζ
=
" ∞
0
kξe−ξ 1
ξ dξ =
" ∞
0
ke−ξdξ = k
the value of constant k is 1.
Solution 7.6. There are 5 out of 10 black balls. Probability that x(0) = 0 is
P0 = 5
10.
If the ﬁrst ball was 0 then we have 9 balls for the second draw, with 4 balls
marked with 0. The probability that x(1) = 0 if x(0) = 0 is
P1 = 4
9.
If x(0) = 0 and x(1) = 0 then there are 8 remaining balls with 3 of them being
marked with 0. The probability that x(2) = 0, with x(0) = 0 and x(1) = 0, is
P2 = 3
8.
The probability for k = 0 is
P(k = 0) = 5
10
4
9
3
8
2
7.

Ljubiša Stankovi´c
Digital Signal Processing
403
In general, if there were N balls and we considered M signal samples
(drawings), the probability P(k = 0) would be
P(k = 0) =
M−1
∏
i=0
N/2 −i
N −i .
Solution 7.7. Let us ﬁnd probability that x(n) < ξ for arbitrary ξ. Consider
the case when ξ < 0,
P{x(n) < ξ} = 0.2
2
*
1 + erf
*ξ −3
2
++
.
It has been taken into account that the considered sample is Gaussian (with
probability 0.2), along with the probability that the sample value is smaller
than ξ.
For ξ > 0 we should take into account that the signal assumes x(n) = 0
with probability 80% as well as that in the remaining 20% cases, Gaussian
random value could be smaller than ξ. So we get
P{x(n) < ξ} = 0.8 + 0.2
2
*
1 + erf
*ξ −3
2
++
.
Now we have
P{x(n) < ξ} =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0.2
2
B
1 + erf
B
ξ−3
2
CC
forξ < 0
0.8 + 0.2
2
B
1 + erf
B
ξ−3
2
CC
for ξ > 0
This function has a discontinuity at ξ = 0. It is not differentiable at this
point as well. Derivative of P{x(n) < ξ} can be expressed in a form of the
generalized functions (Dirac delta function) as
d
dξ P{x(n) < ξ} = py(n)(ξ) = 0.2
2√π e−(ξ−3)2
4
+ 0.8δ(ξ).
The mean and variance are
µy(n) =
∞
"
−∞
ξpy(n)(ξ)dξ = 0.2 × 3 + 0.8 × 0 = 0.6
σ2
y(n) =
∞
"
−∞
(ξ −0.6)2py(n)(ξ)dξ = 0.2 × 7.76 + 0.8 × (0.6)2 = 1.84.

404
Discrete-Time Random Signals
Solution 7.8. Probability that |ε(n)| > A is
P{|ε(n)| > A} = P{ε(n) < −A} + P{ε(n) > A}
=
−A
"
−∞
1
σε
√
2π
e
−ζ2
2σ2ε dζ +
∞
"
A
1
σε
√
2π
e
−ζ2
2σ2ε dζ
= 1 −erf
*
A
√
2σε
+
.
For A = 10 and σ2
ε = 2 we get
P{|ε(n)| > 10} = 1 −erf(5) ≈1.5 × 10−12.
For N = 2000 the expected number of samples with amplitude above A is
P{|ε(n)| > 10} × 2000 ≈3 × 10−9 ≈0. It means that we do not expect any
sample with amplitude higher than 10.
For A = 3 we have
P{|ε(n)| > A} = 1 −erf(2) ≈4.7 × 10−3
with 2000 × 4.7 × 10−3 = 9.4 ≈9 samples among considered 2000 assuming
an amplitude higher than 3.
Solution 7.9. If we are in position to use a reduced set of signal samples
for processing, then the ideal scenario would be to eliminate signal samples
with higher noise values and to keep for processing the samples with lower
noise values. For the case of N signal samples and signal processing based
on M samples we can ﬁnd the interval of amplitudes A for the lowest M
noisy samples. The probability that |x(n)| < Aσε is
P{|x(n)| < Aσε} =
1
σε
√
2π
Aσε
"
−Aσε
e−ξ2/(2σ2ε )dξ.
Since we use M out of N samples this probability should be equal to M/N,
1
√
2π
A
"
−A
e−ξ2/2dξ = erf
* A
√
2
+
= M
N .
The calculation of A value is easily related to the inverse erf(x) function
denoted by erﬁnv(x). For a given M/N, the amplitude is A =
√
2erﬁnv( M
N ).

Ljubiša Stankovi´c
Digital Signal Processing
405
For example, for M = N/2 a half of the lowest noise samples will be within
the interval [−0.6745σε, 0.6745σε] since A =
√
2erﬁnv(0.5) = 0.6745.
The probability density function of the new noise is
py(ξ) =
%
k
σε
√
2π e−ξ2/(2σ2ε )
for
|ξ| < Aσε
0
for
|ξ| ≥Aσε.
The constant k is obtained from the condition that
∞&
−∞
py(ξ)dξ = 1. It is
k = N/M.
The variance of this new noise, formed from the Gaussian noise after
the largest N −M values are removed, is much lower than the variance of
the whole noise. It is
σ2
y =
N
M
σε
√
2π
√
2erﬁnv( M
N )σε
"
−
√
2erﬁnv( M
N )σε
ξ2e−ξ2/(2σ2ε )dξ.
(7.66)
Solution 7.10. The probability density function for sequence y(n) is
py(n)(ζ) =
⎧
⎨
⎩
B
1
σx
√
2π e
−(ζ)2
2σ2x
for −A < ζ ≤A
0
otherwise.
Constant B can be calculated from & ∞
−∞py(n)(ζ)dζ = 1. Its value is
B = 1/erf
*
A
σx
√
2
+
.
Now we have
µy(n) = 0
σ2
y(n) =
A
"
−A
ζ2
1
erf
B
A
σx
√
2
C
1
σx
√
2π
e
−(ζ)2
2σ2x dζ
= σ2
x
⎛
⎜
⎝1 −A
σx
√
2e
−A2
2σ2x
√πerf
B
A
σx
√
2
C
⎞
⎟
⎠.

406
Discrete-Time Random Signals
By denoting β = A/(
√
2σx), the variance σ2
y(n) can be written as
σ2
y(n) = σ2
x
(
1 −2β
e−β2
√πerf(β)
)
.
Solution 7.11. False detection means that we make a wrong decision by
classifying instant n into set Nx. The probability is
PF = P{ε(n) > T} = 1
2 −1
2 erf
*
T
√
2σε
+
Now we can ﬁnd T as
T =
√
2σε erﬁnv(1 −2PF) ≈2.33σε
where erﬁnv(·) is the inverse erf function. Note that the threshold does not
depend on A.
Solution 7.12. The joint probability distribution is
px(n),y(n)(ξ,ζ) = px(n)(ξ)py(n)(ζ)
since signals are mutually independent. Probability that x(n) > y(n) can be
obtained by integrating px(n),y(n)(ξ,ζ) over the region ξ > ζ. It is
P{x(n) > y(n)} =
∞
"
−∞
1
√
2π
e−(ξ−5)2
2
ξ
"
−∞
1
√
2π
e−(ζ−1)2
2
dζdξ ≈0.99766.
For 1000 instants we expect that x(n) > y(n) is satisﬁed in about 998
instants.
Solution 7.13. Since the variable
z = 1
M
M
∑
n=1
x(n)y(n)

Ljubiša Stankovi´c
Digital Signal Processing
407
is also of zero-mean then its variance is
σ2
z = E[z2] = E
>
1
M
M
∑
n=1
x(n)y(n) 1
M
M
∑
m=1
x(m)y(m)
?
=
1
M2
M
∑
n=1
M
∑
m=1
E[x(n)y(n)x(m)y(m)] =
1
M2
M
∑
n=1
M
∑
m=1
E[x(n)x(m)]E[y(n)y(m)]
=
1
M2
M
∑
n=1
E[x2(n)]E[y2(n)] =
1
M2
M
∑
n=1
σ2
xσ2
y = 1
M σ2
xσ2
y.
Solution 7.14. Probability that the random variable is within −∞< ξ < ∞
is
1 =
∞
"
−∞
pε(n)(ξ)dξ =
∞
"
−∞
a
1 + ξ2 dξ = a arctan(ξ)|∞
−∞= aπ,
resulting in a = 1/π. The mean value is
µε = 1
π
∞
"
−∞
ξ
1 + ξ2 dξ = 0,
while the variance
σε = 1
π
∞
"
−∞
ξ2
1 + ξ2 dξ →∞
does not exist. This noise belongs to the class of impulsive, heavy tailed,
noises.
Solution 7.15. The transfer function of a causal system is
H(z) =
1
1 −0.5z−1 .
The z-transform of the input signal x(n) is
X(z) =
∞
∑
n=−∞
x(n)z−n =
∞
∑
n=−∞
aδ(n)z−n = a.
The z-transform of the output signal is
Y(z) = H(z)X(z) =
a
1 −0.5z−1 , |z| > 1/2.

408
Discrete-Time Random Signals
Using the power series expansion of Y(z) we can write
Y(z) = a
∞
∑
n=0
(1/2)nz−n.
The output signal is
y(n) = a · 2−nu(n).
It has been assumed that the random variable a is uniform within [4,5]. Its
probability density function is
pa(ξ) =
!
1,
ξ ∈[4,5]
0,
elsewhere.
The mean value and autocorrelation of the output signal y(n) are
µy(n) = E{y(n)} =
" ∞
−∞y(n)p(a)da = 9 · 2−(n+1)u(n)
ryy(n,m) = E{y(n)y∗(m)} = 61
3 2−(n+m)u(n)u(m).
The output signal y(n) is not WSS.
Solution 7.16. a) The autocorrelation function of the input signal is
rxx(n) = rεε(n) = δ(n).
Its z-transform and power spectral density are
Rxx(z) =
∞
∑
n=−∞
rxx(n)z−n = 1
Sxx(ω) = 1.
The power spectral density of the output signal is
Syy(ω) = Ryy(ejω) = Sxx(ω)
'''H(ejω)
'''
2
= 1, for ω ̸= 0.
The inverse Fourier transform produces the autocorrelation function
ryy(n) = rεhεh(n) = 1
2π
" π
−π Syy(ω)ejωndω = δ(n).

Ljubiša Stankovi´c
Digital Signal Processing
409
b) The z-transform of the cross-correlation of input and output signal y(n) =
ε(n) ∗h(n) = εh(n), is
Rxy(z) = Rxx(z)H(z).
For z = ejω we get
Rεεh(ejω) = Sεε(ω)H(ejω) = H(ejω),
resulting in
rεεh(n) = h(n) =
%
2
π
sin2(nπ/2)
n
,
n ̸= 0
0,
n = 0.
It is easy to conclude that the cross-correlation function is antisymmetric
rxy(−n) = −rxy(n).
c) The analytic part of signal x(n) = ε(n) is
xa(n) = εa(n) = x(n) + jxh(n) = x(n) + j
∞
∑
k=−∞
h(k)x(n −k).
The Fourier transform of both sides produces
Xa(ejω) = X(ejω) + jH(ejω)X(ejω).
If we divide both sides by X(ejω) we get
Xa(ejω)
X(ejω) = Ha(ejω) = 1 + jH(ejω) = 1 + sgn(ω)
=
⎧
⎨
⎩
2,
1,
0,
ω > 0
ω = 0
ω < 0.
The power spectral density of the output signal is
Sεaεa(ω) =
'''Ha(ejω)
'''
2
Sεε(ω) =
'''Ha(ejω)
'''
2
=
⎧
⎨
⎩
4,
1,
0,
ω > 0
ω = 0
ω < 0

410
Discrete-Time Random Signals
with the autocorrelation function of εa(n),
rεaεa(n) = 1
2π
" π
−π Sεaεa(ω)e−jωndω =
⎧
⎨
⎩
−4
jnπ,
for odd n
0,
for even n
Solution 7.17. The power spectral density of the input signal is
Sxx(ω) =
∞
∑
n=−∞
rxx(n)e−jωn =
∞
∑
n=−∞
rεε(n)e−jωn = σ2
ε .
The transfer function is
H(z) =
1
1 −az−1 =
z
z −a
with the impulse response h(n) = anu(n) . It is real-valued. The z-transform
of the output signal autocorrelation function is
Ryy(z) = H(z)H(1/z)Rxx(z) =
z
(z −a)(1 −az)σ2
ε .
The inverse z-transform results in the autocorrelation function of y(n)
ryy(n) =
a|n|
1 −a2 σ2
ε .
Power spectral density of the output signal is
Syy(ω) = Ryy(ejω) =
σ2
ε
(1 −ae−jω)(1 −aejω) =
σ2
ε
1 −2acosω + a2 .
Solution 7.18. The mean of y(n) is
µy(n) = E
%
∞
∑
k=−∞
h(k)x(n −k)
;
=
∞
∑
k=0
akE{ε(n −k)}u(n −k)
=
n
∑
k=0
akµε = µε
1 −an+1
1 −a
u(n).
The variance is
σ2
y(n) = E
MP
y(n) −µy(n)
Q2N
= E{y2(n)} −µ2
y(n)
=
n
∑
k1=0
n
∑
k2=0
ak1ak2E{ε(n −k1)ε(n −k2)}u(n) −
*
µε
1 −an+1
1 −a
+2
u(n).

Ljubiša Stankovi´c
Digital Signal Processing
411
Since E{ε(n −k1)ε(n −k2)} = σ2
ε δ(k1 −k2) + µ2
ε, we get
σ2
y(n) = σ2
ε
1 −a2(n+1)
1 −a2
u(n).
Solution 7.19. The mean value is
µx = µε +
N
∑
k=1
akE{ej(ωkn+θk)} = µε,
since
E{ej(ωkn+θk)} = 1
2π
π
"
−π
ej(ωkn+θk)dθk = 0.
The autocorrelation is
rxx(n) = σ2
ε δ(n) + µ2
ε +
N
∑
k=1
a2
kejωkn,
while the power spectral density for −π < ω ≤π is
Sxx(ejω) = FT{rxx(n)} = σ2
ε + 2πµ2
εδ(ω) + 2π
N
∑
k=1
a2
kδ(ω −ωk).
Solution 7.20. For the optimal ﬁltering d(n) = s(n). The cross-correlation of
the input and desired signal is
rdx(n) = E{d(k)x(k −n)} = E{s(k)[s∗(k −n) + ε∗(k −n)]}
= rss(n) = 0.25|n|.
Its z-transform is
Rdx(z) = Rss(z) =
−15z/4
(z −1/4)(z −4).
The input signal autocorrelation is
rxx(n) = rss(n) + rεε(n) = 0.25|n| + δ(n),
with the z-transform
Rxx(z) =
−15z/4
(z −1/4)(z −4) + 1 =
z2 −8z + 1
(z −1/4)(z −4).

412
Discrete-Time Random Signals
The optimal ﬁlter transfer function is
H(z) = Rdx(z)
Rxx(z) = Rss(z)
Rxx(z) =
−15z/4
z2 −8z + 1.
A stable system requires the region of convergence 0.127 < |z| < 7.873. It is
not causal.
Solution 7.21. By direct calculation, in noisy case, we obtain
X(2) = 2001 + j204
and
XR(k) = 8.
Note that the noise-free DFT value X(2) is 8.
Solution 7.22. With a rectangular window the spectrogram reads
Sx(n,k) =
'''''
N−1
∑
i=0
x(n + i)e−j 2π
N ik
'''''
2
=
N−1
∑
i1=0
N−1
∑
i2=0
x(n + i1)x∗(n + i2)e−j 2π
N (i1−i2)k.
a) The mean value of spectrogram is
E{Sx(n,k)} = E{
N−1
∑
i1=0
N−1
∑
i2=0
x(n + i1)x∗(n + i2)e−j 2π
N (i1−i2)k}
=
N−1
∑
i1=0
N−1
∑
i2=0
E{x(n + i1)x∗(n + i2)}e−j 2π
N (i1−i2)k.
Using the fact that the signal s(n) is deterministic and the noise ε(n) is zero-
mean white stationary we get
E{Sx(n,k)} =
N−1
∑
i1=0
N−1
∑
i2=0
s(n + i1)s∗(n + i2)e−j 2π
N (i1−i2)k
+
N−1
∑
i1=0
N−1
∑
i2=0
E{ε(n + i1)ε∗(n + i2)}e−j 2π
N (i1−i2)k
or
E{Sx(n,k)} = Ss(n,k) + σ2
ε
N−1
∑
i1=0
N−1
∑
i2=0
δ(i1 −i2)e−j 2π
N (i1−i2)k
= Ss(n,k) + σ2
ε
N−1
∑
i=0
1 = Ss(n,k) + Nσ2
ε

Ljubiša Stankovi´c
Digital Signal Processing
413
since for the noise holds
rεε(i) = E{ε(n + i)ε
∗(i)} = σ2
ε δ(i).
b) The variance of Sx(n,k) is
σ2 = E{Sx(n,k)S∗
x(n,k)} −E{Sx(n,k)}E{S∗
x(n,k)}.
The ﬁrst term can be written as
E{Sx(n,k)S∗
x(n,k)} =
N−1
∑
i1=0
N−1
∑
i2=0
N−1
∑
i3=0
N−1
∑
i4=0
E{x(n + i1)x∗(n + i2)
× x∗(n + i3)x(n + i4)}e−j 2π
N (i1−i2−i3+i4)k
where
E{x(n + i1)x∗(n + i2)x∗(n + i3)x(n + i4)}
= s(n + i1)s∗(n + i2)s∗(n + i3)s(n + i4)
+ s(n + i1)s∗(n + i2)rεε(i4 −i3) + s(n + i1)s∗(n + i3)rεε(i4 −i2)
+ s∗(n + i2)s(n + i4)rεε(i1 −i3) + s∗(n + i3)s(n + i4)rεε(i1 −i2)
+ E{ε(n + i1)ε∗(n + i2)ε∗(n + i3)ε(n + i4)}.
The facts that odd order moments of a Gaussian zero-mean noise are zero
and rεε∗(k) = rε∗ε(k) = 0 for a complex-valued noise with i.i.d. are used.
According to the relation from the note it holds
E{ε(n + i1)ε∗(n + i2)ε∗(n + i3)ε(n + i4)}
= rεε(i1 −i2)rεε(i4 −i3) + rεε(i1 −i3)rεε(i4 −i2).
After few straightforward transformations, we get
E{Sx(n,k)S∗
x(n,k)} = S2
s(n,k)
+σ2
ε
N−1
∑
i1=0
N−1
∑
i2=0
N−1
∑
i3=0
N−1
∑
i4=0
[s(n + i1)s∗(n + i2)δ(i4 −i3)
+s(n + i1)s∗(n + i3)δ(i4 −i2) + s(n + i4)s∗(n + i2)δ(i1 −i3)
+s(n + i4)s∗(n + i3)δ(i1 −i2)]e−j 2π
N (i1−i2−i3+i4)k
+σ4
ε
N−1
∑
i1=0
N−1
∑
i2=0
N−1
∑
i3=0
N−1
∑
i4=0
[δ(i1 −i2)δ(i4 −i3)
+δ(i1 −i3)δ(i4 −i2)]e−j 2π
N (i1−i2−i3+i4)k.

414
Discrete-Time Random Signals
The ﬁnal form of the variance is
σ2 = S2
s(n,k) + 4Nσ2
ε Ss(n,k) + 2N2σ4
ε −(Ss(n,k) + Nσ2
ε )2
= 2Nσ2
ε Ss(n,k) + N2σ4
ε .
The variance is proportional to the signal spectrogram values Ss(n,k).
Solution 7.23. a) Mean value of the Wigner distribution of x(n) is
E{Wx(n,ω)} =
L
∑
k=−L
E{x(n + k)x∗(n −k)}e−j2ωk.
Signal is deterministic and not correlated with the white noise ε(n),
E{x(n + k)x∗(n −k)} = s(n + k)s∗(n −k) + rεε(2k),
where rεε(2k) is the autocorrelation function of the additive noise ε(n). The
noise variance is σ2
ε . Then
E{x(n + k)x∗(n −k)} = s(n + k)s∗(n −k) + σ2
ε δ(2k).
Final form of the mean value of Wx(n,ω) is
E{Wx(n,ω)} = Ws(n,ω) + σ2
ε .
b) Variance of Wxx(n,ω) follows from
σ2 = E{Wx(n,ω)W∗
x (n,ω)} −E{Wx(n,ω)}E{W∗
x (n,ω)}.
The ﬁrst term can be written as
E{Wx(n,ω)W∗
x (n,ω)} =
=
L
∑
k1=−L
L
∑
k2=−L
E{x(n + k1)x∗(n −k1)x∗(n + k2)x(n −k2)}e−j2ω(k1−k2).
In the case of a Gaussian zero-mean white stationary noise complex-valued
noise with i.i.d. real and imaginary parts rεε∗(k) = rε∗ε(k) = 0 we can write
E{x(n + k1)x∗(n −k1)x∗(n + k2)x(n −k2)}
= s(n + k1)s∗(n −k1)s∗(n + k2)s(n −k2) + s(n + k1)s∗(n −k1)rεε(−2k2)
+ s(n + k1)s∗(n + k2)rεε(k2 −k1) + s∗(n −k1)s(n −k2)rεε(k1 −k2)
+ s∗(n + k2)s(n −k2)rεε(2k1) + rεε(2k1)rεε(−2k2) + r2
εε(k1 −k2).

Ljubiša Stankovi´c
Digital Signal Processing
415
The note from the previous problem is used. Since
E{Wx(n,ω)}E{W∗
x (n,ω)} = W2
s (n,ω) + 2σ2
ε Ws(n,ω) + σ4
ε
the Wigner distribution variance is
σ2 =
L
∑
k1=−L
L
∑
k1=−L
[s(n + k1)s∗(n + k2)rεε(k2 −k1) + r2
εε(k1 −k2)
+s∗(n−k1)s(n−k2)rεε(k1 −k2)]e−jω(k1−k2) = σ2
ε
L
∑
k=−L
(2|s(n + k)|2+σ2
ε ).
For an FM signal σ2 = σ2
ε (2L + 1)(2A2 + σ2
ε ). This variance is constant.
Solution 7.24. Signal s(n) and noise ε(n) are not correlated. Then
rxx(n) = rss(n) + rεε(n) = 4(0.5)|n| + 2δ(n)
Rxx(z) = 4
1
1 −0.5z−1 + 4
0.5z
1 −0.5z + 1 =
3z
(2z −1)(2 −z) + 1.
(7.67)
a) For the optimal ﬁltering d(n) = s(n). The cross-correlation of the desired
and input signal is
rdx(n) = E{d(k)x(n −k)} = E{s(k)[s(k −n) + ε(k −n)]} = rss(n) = 4(0.5)|n|.
The optimal ﬁlter transfer function is
H(z) = Rdx(z)
Rxx(z) =
3z
(2z−1)(2−z)
3z
(2z−1)(2−z) + 1 =
3z
−2z2 + 8z −2.
b) For optimal smoothing d(n) = s(n −1) with
rdx(n) = E{d(k)x(n −k)} = E{s(k −1)[s(k −n) + ε(k −n)]} = rss(n −1)
and
Rdx(z) =
∞
∑
n=−∞
4(0.5)|n−1|z−n = zRss(z) =
3z2
(2z −1)(2 −z)
follows
H(z) =
3z2
−2z2 + 8z −2.

416
Discrete-Time Random Signals
c) In the case of prediction d(n) = s(n + 1) and
rdx(n) = E{d(k)x(n −k)} = E{s(k + 1)[s(k −n) + ε(k −n)]} = rss(n + 1),
Rdx(z) =
∞
∑
n=−∞
4(0.5)|n+1|z−n = z−1Rss(z) =
3
(2z −1)(2 −z)
with
H(z) =
3
−2z2 + 8z −2.
Solution 7.25. For the optimal ﬁlter d(n) = s(n). The correlation functions
are
rxx(n) = E{x(k)x∗(k −n)} = E{(s(k) + ε(k))(s∗(k −n) + ε∗(k −n))}
= rss(n) + 2rsε(n) + rεε(n) = 3(0.9)|n| + 8δ(n)
and
rdx(n) = E{s(k)[s∗(k −n) + ε∗(k −n)]}=rss(n)+rsε(n) = 3(0.9)|n| +2δ(n).
Calculation of the z-transforms and ﬁlter transfer function is left to the
reader.
Solution 7.26. The power spectral densities of the signal and the input noise
are
Sdd(ejω) =
!
1 −|ω/2|
for
|ω/2| < 1
0
elsewhere
and
Sεε(ejω) =
!
1 −||ω| −2|
for
|ω −2| < 1
0
elsewhere
.
The optimal ﬁlter frequency response is
H(ejω) =
Sdd(ejω)
Sdd(ejω) + Sεε(ejω).
For 0 ≤ω < π we get
H(ejω) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
for
ω ≤1
2−ω
ω
for
1 < ω ≤2
0
for
2 < ω ≤3
1
for
3 < ω < π

Ljubiša Stankovi´c
Digital Signal Processing
417
since for 1 < ω ≤2 holds
H(ejω) =
1 −ω/2
1 −ω/2 + (1 −|ω −2|)
=
1 −ω/2
1 −ω/2 + (1 + (ω −2)) = 2 −ω
ω
.
The result for −π ≤ω < 0 is symmetric. It is shown in Fig.7.18(bottom). The
input SNR is
SNRi = Es
Eε
= 2
2 = 1
or 0 [dB]. The output SNR is
SNRo =
1
2π
& π
−π Sdd(ejω)
''H(ejω)
''2 dω
1
2π
& π
−π Sεε(ejω)2 ''H(ejω)
''2 dω
= 3/2 + 2& 2
1 (1 −ω
2 )
'' 2−ω
ω
''2 dω
2& 2
1 (1 + (ω −2))
'' 2−ω
ω
''2 dω
= 10 −12ln2
16ln2 −11 = 18.6181
or 12.7 [dB].
Solution 7.27. For this calculation the model is
V
Wx(n,k) =
N−1
∑
m=0
D
xQ(n + m)xQ(n −m) + e(n + m,n −m)
E
e−j2πmk/N
=
N−1
∑
m=0
{[x(n + m) + e(n + m)][(x(n −m) + e(n −m)]+
+ e(n + m,n −m)}e−j2πmk/N.
The mean value is
E{ V
Wx(n,k)} =
N−1
∑
m=0
x(n + m)x(n −m)e−j2πmk/N
+E{
N−1
∑
m=0
e(n + m)e(n −m)e−j2πmk/N} = Wx(n,k) + 1
12∆2.
It has been assumed that the errors in two different signal samples are not
correlated E{e(n + m)e(n −m)} = 0 for m ̸= 0 and that the signal and error
are not correlated, E{x(n + m)e(n −m)} = 0 for any m and n.

418
Discrete-Time Random Signals
7.11
EXERCISE
Exercise 7.1. Signal x20i(n) is equal to the monthly average of maximal daily
temperatures in a city measured from year 2001 to 2015. If we can assume
that the signal for an individual month is Gaussian ﬁnd the probability that
the average of maximal temperatures: (a) in July is lower than 25, (b) in
August is higher than 39.
Exercise 7.2. Random signal x(n) is such that x(n) = x1(n) with probability
p. In all other cases x(n) is x2(n). If the mean and variance of x1(n) and
x2(n) are µx1, σ2
x1 and µx2, σ2
x2, respectively, ﬁnd the mean and the variance
of x(n).
Result: µx = pµx1 + (1 −p)µx2 and
σ2
x = p
@
E{x2
1(n)} −µ2
x
A
+ (1 −p)
@
E{x2
2(n)} −µ2
x
A
= p[σ2
x1 + µ2
x1 −µ2
x] + (1 −p)[σ2
x2 + µ2
x2 −µ2
x]
= pσ2
x1 + (1 −p)σ2
x2 + p(1 −p)(µx1 −µx2)2.
Exercise 7.3. Find the mean and variance of a white uniform noise whose
values are within the interval −a ≤x(n) ≤a. If that signal is an input to
the FIR system with impulse response h(n) = 1 for 1 ≤n ≤N and h(n) = 0
elsewhere, ﬁnd the mean and variance of the output signal.
Exercise 7.4. Consider a signal x(n) equal to the Gaussian zero-mean noise
with variance σ2
ε . A new noise y(n) is formed by using the values of x(n)
lower than median value. Find the mean and variance of this new noise
y(n). Result: σ2
y = 0.1426σ2
ε .
Exercise 7.5. A causal system is deﬁned by
y(n) −1
2y(n −1) = x(n).
The input signal is the causal part of a white noise ε(n)
x(n) = ε(n)u(n)
where µε = 0 and rεε(n) = σ2
ε δ(n). Find the mean value and the autocorrela-
tion ryy(n,m) of the output signal. What is the cross-correlation between the
input and output signal ryx(n,m). Show that for n →∞the output signal
tends to a WSS signal.

Ljubiša Stankovi´c
Digital Signal Processing
419
Exercise 7.6. (a) Calculate the DFT value X(4) for x(n) = exp(j4πn/N) with
N = 16.
(b) Calculate the DFT of a noisy signal x(n) + ε(n), where the noise is
ε(n) = 1001δ(n) −899δ(n −3) + 561δ(n −11) −32δ(n −14).
(c) Estimate the DFT using noisy signal x(n) + ε(n) and
XR(k) = N median
n=0,1,..,N−1Re
M
(x(n) + ε(n))e−j2πkn/NN
+ jN median
n=0,1,..,N−1Im
M
(x(n) + ε(n))e−j2πkn/NN
.
Discuss the results.
Exercise 7.7. The power spectral densities of the signal Sdd(ejω) and input
noise Sεε(ejω) are given in Fig.7.19 for two cases. One on the left subplots
and the other on the right subplots. Show that the frequency response of the
optimal ﬁlter H(ejω) is presented in Fig.7.19(bottom subplot for both cases
of signal and noise). Find the SNR at the input and output of the optimal
ﬁlter in both cases.
Exercise 7.8. Find the transfer function of an optimal ﬁlter for the signal
x(n) = s(n) + ε(n), where ε(n) is a white noise with the autocorrelation
rεε(n) = Nδ(n), and s(n) is a random signal obtained as the output of the
ﬁrst-order linear system to a white noise with the autocorrelation rss(n) =
a|n|, 0 < a < 1. Signal and noise are not correlated.
Exercise 7.9. A random signal s(n) carries an information. Its autocorre-
lation function is rss(n) =
1
4|n| . A noise with the autocorrelation rεε(n) =
0.5δ(n) is added to the signal. Find the optimal ﬁlter for:
a) d(n) = s(n) - optimal ﬁltering,
b) d(n) = s(n −1) - optimal smoothing,
c) d(n) = s(n + 1) - optimal prediction.
Exercise 7.10. Find the power spectral densities of signals whose autocor-
relation functions are:
a) rxx(n) = δ(n) + 2cos(0.πn) ,
b) rxx(n) = −4δ(n + 1) + 7δ(n) −4δ(n −1) .
(c) rxx(n) = 2acos(ω0n) +
∞
∑
k=0
σ2(1/2)kδ(n −k)

420
Discrete-Time Random Signals
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
Sdd (ejω)
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
Sεε (ejω)
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
H(ejω)
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
Sdd (ejω)
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
Sεε (ejω)
-3
-2
-1
0
1
2
3
0
0.5
1
1.5
H(ejω)
Figure 7.19
Power spectral densities of the signal
''S(ejω)
''2 and input noise Sεε(ejω) along
with the frequency response of an optimal ﬁlter H(ejω). Two cases are presented, one on the
left subplots and the other on the right subplots.

Part III
Selected Topics
421


Chapter 8
Adaptive Systems
8.1
INTRODUCTION
Classic systems for signal processing are designed to satisfy properties de-
ﬁned in advance. Their parameters are time-invariant. Adaptive systems
change their parameters or form, in order to achieve the best possible per-
formance. These systems are characterized by ability to observe variations
in the input signal behavior and to react to these changes by adapting their
parameters in order to improve the desired performance of the output sig-
nal. Adaptive systems have the ability to "learn" so that they can appropri-
ately adapt the performance when the system environment is changed. By
deﬁnition the adaptive systems are time-variant. These systems are often
nonlinear as well. These two facts make the design and analysis of adaptive
systems more difﬁcult than in the case of classical time-invariant systems.
Adaptive systems are the topic of this chapter.
Consider an adaptive system with one input and one output signal,
as in Figure 8.1. In addition to the algorithm that transforms the input
signal to the output signal, the adaptive system have a part that tracks
the system performance and implements appropriate system changes. This
control system takes into account the input signal, the output signal, and
some additional information that can help in making a decision on how the
system parameters should change.
Architecture of an adaptive system whose task is to transform the
input signal in such a way that it is as close to a reference (desired) signal
________________________________________
Authors: Ljubiša Stankovi´c, Miloš Dakovi´c
423

424
Adaptive Systems
adaptive
system
adaptation
rule
input
signal
output
signal
other
data
Figure 8.1
General adaptive system
adaptive
system
input
signal
output
signal
desired
output
error signal
x(n)
y(n)
e(n)
d(n)
+
-
Figure 8.2
Adaptive system
as possible is presented in Figure 8.2. Deﬁnition of the reference signal
will be crucial and speciﬁc for each application. It will be described later
in this chapter. Difference between the output and the reference signal is
used as a measure of the system performance quality. This difference is
called the error signal. It plays an important role in adaptation of the system
parameters.
As in the case of classic systems, the adaptive systems may be imple-
mented with a ﬁnite or inﬁnite impulse response. They can be linear and
nonlinear. One of the crucial problems in the adaptive systems with inﬁnite
impulse response is to provide their stability. The control part must take
into account that the system is conﬁgured in such a way that its parameters
stay all time within the region that guarantees system stability. In the case
of recursive systems this task is not simple, as it will be illustrated by the
next example.

Ljubiša Stankovi´c
Digital Signal Processing
425
Example 8.1. Consider an adaptive system deﬁned by the difference relation
y(n) = a x(n) + b x(n −1) + cy(n −1) + dy(n −2)
where a,b,c and d are real-valued adaptive (changing) parameters of the
system. What is the range for the parameters a,b,c and d so that the system is
stable?
⋆For a given set of parameters, during the time interval when these
parameters do not change, the system can be considered as a time-invariant
system. Its transfer function is
H(z) =
a + bz−1
1 −cz−1 −dz−2 .
The stability condition requires that the system poles are within the
unit circle. The pole values are
z1,2 = c ±
√
c2 + 4d
2
.
Consider two cases:
Case I: Poles are complex-valued, that is c2 + 4d < 0. In this case the parame-
ter d must be negative. In addition it has to satisfy the inequality |c| < 2
√
−d.
Poles of the system are
z1,2 = c
2 ± j
√
−c2 −4d
2
.
They are within the unit circle if |z1,2| < 1, meaning
√
−d < 1 or −1 < d < 0.
Parameters a and b do not inﬂuence the system stability. They can take
any value. The conditions that the system is stable in this case are
d > −1
|c| < 2
√
−d.
Case II: Poles of the system are real-valued, c2 + 4d ≥0. The poles are
z1 = c +
√
c2 + 4d
2
,
z2 = c −
√
c2 + 4d
2
.
The stability condition is
|z1| < 1
and
|z2| < 1
or
2d + c2 ± c
,
4d + c2 < 2.

426
Adaptive Systems
 c
 d
real
complex
-3
-2
-1
0
1
2
3
-2
-1
0
1
2
Figure 8.3
Region of system coefﬁcient values where the system is stable.
In this case the system is stable if
c2 + 4d ≥0
2d + c2 + c
,
4d + c2 < 2
2d + c2 −c
,
4d + c2 < 2
The ﬁnal solution is a set of parameters c and d that satisﬁes the
conditions deﬁned within Case I or Case II. The region of parameters c and
d values, when the system is stable, is presented in Figure 8.3. Region when
the system has complex-valued poles is presented by dark-gray region, while
the lighter region is the convergence region for two real-valued poles. From
Figure 8.3 we can note that the system stability conditions reduce to
|d| < 1
|c| < 1 −d.
The stability conditions are derived assuming that the coefﬁcients do
not change within the considered time interval. In the case when the param-
eter d assumes absolute value greater than 1 the system will remain stable.
It would be quite complex to establish the stability region if time-varying
nature of the parameters (within the considered interval) would be taken into
account. It is outside of the scope of this textbook.

Ljubiša Stankovi´c
Digital Signal Processing
427
Example 8.2. Consider an adaptive system deﬁned by
y(n) = h0(n) x(n) + h1(n) x(n −1) + ... + hN−1(n) x(n −(N −1))
=
N−1
∑
i=0
hi(n)x(n −i)
where hi(n) for i = 0,1,..., N −1 are real-valued time-varying parameters
of the system. What are the allowed values of parameters hi(n) for a stable
system?
⋆A system is stable if for an arbitrary input signal x(n), with a
bounded amplitude |x(n)| < Ax for any n, there is a constant Ay such that
the output signal is also bounded |y(n)| < Ay for any n. In this example it
means
|y(n)| =
'''''
N−1
∑
i=0
hi(n)x(n −k)
''''' ≤
N−1
∑
i=0
|hi(n)||x(n −i)| < Ax
N−1
∑
i=0
|hi(n)|
A constant Ay exist if the system coefﬁcients are limited, |hi(n)| < Ah
for any i and n. Then
|y(n)| < Ay = NAxAh
and the system is stable.
The proof of system stability in this case is simpler that in the case of
the recursive system from previous example. The stability condition is also
simple here. It is sufﬁcient that the system coefﬁcients are bounded.
8.2
LINEAR ADAPTIVE ADDER
Basic structure in an adaptive system is a linear adder (ﬁnite impulse
response system). The output signal y(n) is a linear combination of the
input signal x(n) in the considered instant n and its N −1 previous values
x(n −1),x(n −2),...,x(n −N + 1)
y(n) = h0x(n) + h1x(n −1) + ··· + hN−1x(n −N + 1)
=
N−1
∑
i=0
hix(n −i)
Description and analysis of this system is quite simple. The system is linear.
In addition, the system with ﬁnite impulse response is always stable, for
any ﬁnite coefﬁcient values. Finally, the realization of these systems is very
simple. In the case of adaptive systems the coefﬁcients hi change their values

428
Adaptive Systems
z-1
z-1
z-1
y(n)
h0(n)
h1(n)
h2(n)
hN-1 (n)
+
+
+
x(n)
Figure 8.4
Adaptive linear adder.
in time. This simple system is called linear adaptive adder. Taking into
account time-variant nature of the coefﬁcients the system is described by
y(n) = h0(n)x(n) + h1(n)x(n −1) + ··· + hN−1(n)x(n −N + 1)
=
N−1
∑
i=0
hi(n)x(n −i).
The adaptation process consists of an appropriately deﬁned algorithm for
change of the coefﬁcients hi(n) values with the aim to achieve the desired
system performance. Based on Example 8.2 we can conclude that this sys-
tem is stable if all coefﬁcient values are bounded. Realization of the linear
adaptive adder is presented in Figure 8.4. Time-variant vectors
X(n) =
⎡
⎢⎢⎢⎣
x(n)
x(n −1)
...
x(n −N + 1)
⎤
⎥⎥⎥⎦
N×1
H(n) =
⎡
⎢⎢⎢⎣
h0(n)
h1(n)
...
hN−1(n)
⎤
⎥⎥⎥⎦
N×1
will be introduced for description and analysis of this system. Vector X(n)
commonly consists of the current value of the input x(n) and its N −1 past
values, while elements of vector H(n) are the system coefﬁcients hi(n) in
the current instant n. The output signal can be written as a product of these
two vectors
y(n) = XT(n)H(n) = HT(n)X(n)
(8.1)
where (·)T denotes the vector transpose operation. The output y(n) is a
scalar.
In general the input vector X(n) may not be formed using the de-
layed samples of the input signal x(n). It can be understood, in gen-
eral case, as a vector whose elements are N independent input signals

Ljubiša Stankovi´c
Digital Signal Processing
429
+
...
x0(n)
x1(n)
xN-1 (n)
h0(n)
h1(n)
hN-1 (n)
y(n)
Figure 8.5
Adaptive linear combinator.
x0(n),x1(n),... xN−1(n),
X(n) =
⎡
⎢⎢⎢⎣
x0(n)
x1(n)
...
xN−1(n)
⎤
⎥⎥⎥⎦
N×1
.
This system has N inputs and one output (multiple input single output
system - MISO system). It is called a linear adaptive combinator, Figure 8.5.
The linear adaptive adder is just a special case of linear adaptive combinator
with xi(n) = x(n −i) for i = 0,1,..., N −1.
8.2.1
Error Signal
Block diagram of an adaptive system is presented in Figure 8.2. Input signal
is denoted by x(n), while the output signal is y(n). The reference (desired)
signal is denoted by d(n). The error signal e(n) is equal to the difference
between the reference signal d(n) and the output signal y(n),
e(n) = d(n) −y(n).
In adaptive systems the ultimate goal of the adaptation (learning)
process is to adjust system coefﬁcients so that the output signal is as close as
possible to the reference signal. In an ideal case y(n) ≡d(n) should hold,
when e(n) = 0. The adaptation process can be described as a process of
system parameters modiﬁcation in order to achieve this goal.
First step in the deﬁnition of an appropriate algorithm for the system
parameters modiﬁcation is in deﬁning a measure of the output signal and
the reference signal difference (similarity). The expected value of error e(n)
is not a good choice for this measure. We will illustrate this fact on a simple

430
Adaptive Systems
example. Assume that the following error is obtained in 6 consecutive
measurements: [0,0,0,0,0,0] in the ﬁrst realization, [−20,20,−20,20,−20,20]
in the second realization, and [0.1,0.1,0.1,0.1,0.1,0.1] in the third realization.
The average value of the error signal in the ﬁrst two realizations is 0 while
in the third one is 0.1. It would lead to the conclusion that both the ﬁrst
and the second realization achieved good system performance. At the same
time it would mean that the third realization produced the worst results.
Obviously this is a wrong conclusion.1
Commonly used measure of the deviation of output signal from the
reference signal is the mean square error (MSE),
ε = E[e2(n)],
where E[·] denotes the expected value. For the previous example with 6
values of error we get: ε = 0 for the ﬁrst case, ε = 400 in the second case,
and ε = 0.01 in the third case. We see that this kind of measure meets our
expectation about the measure behavior.
In general a function J(e) is used to deﬁne the deviation of the error
signal e(n) from the ideal case. This is a cost function. It should be nonneg-
ative. It should also have a minimum where the error signal achieves its
lowest possible value (in ideal case 0), while local minima should not exist.
From the previous illustration we can conclude that one possible form of
the cost function is the mean square error function
JMSE(e) = E[e2(n)].
In practical realizations this function can be estimated using an average of
L most recent values of the error signal
JLS(e) = 1
L
L−1
∑
k=0
e2(n −k).
This measure corresponds to the least square (LS) criterion in the analysis.
Consider now the square error signal in the linear adaptive adder
e2(n) = (d(n) −y(n))2 =
B
d(n) −HT(n)X(n)
C2
=
= d2(n) −2d(n)HT(n)X(n) + HT(n)X(n)XT(n)H(n)
1
A simple modiﬁcation of the expected value of error that would produce the correct
conclusion would be the expected absolute value of error |e(n)|. However, the absolute
value is not differentiable function (at e(n) = 0). The algorithms for its minimization would
be complex. Therefore it will not be used here (it will be the main form of minimization
function in the chapter dealing with sparse signals).

Ljubiša Stankovi´c
Digital Signal Processing
431
In the mean square error ε = E[e2(n)] calculation we should take into
account that the signals d(n) and x(n) are random, while the coefﬁcients
of the system H(n) are deterministic variables
ε = E[e2(n)] =
= E[d2(n) −2d(n)HT(n)X(n) + HT(n)X(n)XT(n)H(n)] =
= E[d2(n)] −2HT(n)E[d(n)X(n)] + HT(n)E[X(n)XT(n)]H(n).
(8.2)
The linearity property of the expected value operator E[·] is used, E[A +
B] = E[A] + E[B] and E[k · A] = k · E[A] where A and B are random variables
and k is a constant.
The mean square error (8.2) will be analyzed, with the assumption that
signals x(n) and d(n) are stationary random processes. The ﬁrst term in (8.2)
E[d2(n)] = σ2
d
is equal to the variance σ2
d of reference signal d(n). Next term is
E[d(n)X(n)] = E[d(n)
⎡
⎢⎢⎢⎣
x(n)
x(n −1)
...
x(n −N + 1)
⎤
⎥⎥⎥⎦] =
⎡
⎢⎢⎢⎣
E[d(n)x(n)]
E[d(n)x(n −1)]
...
E[d(n)x(n −N + 1)]
⎤
⎥⎥⎥⎦.
Its elements E[d(n)x(m)] are the cross-correlations of the reference and in-
put signals. They will be denoted by rdx(n,m) = E[d(n)x(m)]. For station-
ary random signals rdx(n,m) is a function of time index difference only,
E[d(n)x(m)] = rdx(n −m). The previous relation can be rewritten in the form
E[d(n)X(n)] =
⎡
⎢⎢⎢⎣
rdx(n,n)
rdx(n,n −1)
...
rdx(n,n −N + 1)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
rdx(0)
rdx(1)
...
rdx(N −1)
⎤
⎥⎥⎥⎦= rdx.
(8.3)
The elements of cross-correlation vector rdx = E[d(n)X(n)] do not depend
on the considered instant n.

432
Adaptive Systems
The last term in (8.2) is of the form
X(n)XT(n) =
⎡
⎢⎢⎢⎣
x(n)
x(n −1)
...
x(n −N + 1)
⎤
⎥⎥⎥⎦
D
x(n)
x(n −1)
...
x(n −N + 1)E =
⎡
⎢⎢⎢⎣
x(n)x(n)
x(n)x(n −1)
···
x(n)x(n−N+1)
x(n −1)x(n)
x(n −1)x(n −1)
···
x(n −1)x(n−N+1)
...
...
...
...
x(n−N+1)x(n)
x(n−N+1)x(n −1)
···
x(n−N+1)x(n−N+1)
⎤
⎥⎥⎥⎦.
The expected value of this expression is
E[X(n)XT(n)] = R(n)=
⎡
⎢⎢⎢⎣
rxx(n,n)
rxx(n,n −1)
···
rxx(n,n−N+1)
rxx(n −1,n)
rxx(n −1,n −1)
···
rxx(n −1,n−N+1)
...
...
...
...
rxx(n−N+1,n)
rxx(n−N+1,n −1)
···
rxx(n−N+1,n−N+1)
⎤
⎥⎥⎥⎦.
For a stationary random signal x(n)
E[X(n)XT(n)] = R =
(8.4)
⎡
⎢⎢⎢⎣
rxx(0)
rxx(1)
···
rxx(N −1)
rxx(1)
rxx(0)
···
rxx(N −2)
...
...
...
...
rxx(N −1)
rxx(N −2)
···
rxx(0)
⎤
⎥⎥⎥⎦
In the derivation of this relation, the autocorrelation function rxx(n,m) =
E[x(n)x(m)] is used. In the case of stationary signal rxx(n,m) = rxx(n −m).
The autocorrelation function of real-valued signals is even rxx(−n) = rxx(n).
The autocorrelation matrix R does not depend on the current instant n.
The mean square error can now be written in the form
ε = σ2
d −2HT(n)rdx + HT(n)RH(n)
(8.5)
The task of an adaptive system is to ﬁnd the coefﬁcients in vector H(n) that
will produce the minimal mean square error ε. In (8.5) we have a vector
rdx of cross-correlations between the reference and input signal and the

Ljubiša Stankovi´c
Digital Signal Processing
433
autocorrelation matrix of the input signal R. If the statistical behavior of
these signals are known then we can ﬁnd rdx and R. If that is not the case the
autocorrelation matrix elements can be estimated by averaging over time of
the input signal values
rxx(i) = E[x(n)x(n −i)] = 1
M
M−1
∑
k=0
x(n −k)x(n −k −i)
(8.6)
where i = 0,1,..., N −1. This estimation is derived with the assumption
that the random process
x(n) is ergodic and that the mean value over
time is equal to the mean value over different realizations. Increasing the
number of terms M in averaging produces a better estimation. However
the computational complexity is increased, as well the required duration
of signal is increased. Note that in (8.6) the signal samples x(n),x(n −
1),...,x(n −(M −1) −i) are used. For 0 ≤i ≤N −1 it means that the signal
has to be available from n −(M + N −2) to n.
Consider a simple example of a linear adaptive adder of the second
order (N = 2). Then
X(n) =
-
x(n)
x(n −1)
.
H(n) =
-
h0(n)
h1(n)
.
.
The mean square error (according to (8.5)) is
ε = σ2
d −2
D
h0
h1
E-
rdx(0)
rdx(1)
.
+
D
h0
h1
E-
rxx(0)
rxx(1)
rxx(1)
rxx(0)
.-
h0
h1
.
=
= σ2
d −2rdx(0)h0 −2rdx(1)h1 + rxx(0)h2
0 + 2rxx(1)h0h1 + rxx(0)h2
1.
(8.7)
Indices n are omitted from the coefﬁcients h0(n) and h1(n) for notation
simplicity. The last relation has a simple geometric interpretation. The mean
square error ε is presented as a function of two variables h0 and h1 in Figure
8.6. From this ﬁgure (the same as from relation (8.7)) we may conclude that
the error function is a paraboloid. The paraboloid has a minimum at (h∗
0,h∗
1).
Our aim is to ﬁnd these values, meaning that the system is adjusted to the
values corresponding to the minimal measure of error.
The minimization process can be done in a numerical or analytical
way. Analytic way is based on the calculation of stationary points of a multi-
dimensional function using partial derivatives. The most common numeric
way of solving this problem is in using iterative procedures. Starting from
an arbitrary point (representing parameters of the system), the direction of
cost function decrease is determined using ﬁnite differences. System param-
eters are corrected and the process is repeated until the minimum is found
with a desired accuracy.

434
Adaptive Systems
-2
0
2
4
6
-2
0
2
4
6
0
20
40
60
80
h0
 (h0
*,h1
*) 
h1
 ε(h0,h1) 
h1
h0
 (h0
*,h1
*) 
 ε(h0,h1) 
-2
0
2
4
6
-2
0
2
4
6
Figure 8.6
Mean square error ε as a function of the system coefﬁcients h0 and h1. The optimal
coefﬁcient values are denoted by h∗
0 and h∗
1.
In the considered case we have a paraboloid (convex function) with a
unique solution for the extreme value of ε. It will be determined by using the
partial derivatives of ε with respect to the variables h0 and h1. The position
(h∗
0,h∗
1) of minimal ε will be found by equating these derivatives to zero.
The derivatives are
∂ε
∂h0
= −2rdx(0) + 2rxx(0)h0 + 2rxx(1)h1
∂ε
∂h1
= −2rdx(1) + 2rxx(1)h0 + 2rxx(0)h1.
From ∂ε/∂h0 = 0 and ∂ε/∂h1 = 0 follows
rxx(0)h0 + rxx(1)h1 = rdx(0)
rxx(1)h0 + rxx(0)h1 = rdx(1),
or in matrix form
RH = rdx.
The solution of this matrix equation
H∗= R−1rdx.
produces the optimal system coefﬁcients, denoted by h∗
0 and h∗
1. This is the
Wienner optimal ﬁlter.

Ljubiša Stankovi´c
Digital Signal Processing
435
In order to simplify notation, a symbolic „differentiation with respect to
a vector” is introduced as
∂ε
∂HT = 0.
Partial derivatives of ε with respect to each element of vector H are denoted
by ∂ε/∂HT. Each derivative is independently equated to zero. When the
vector H has N elements then ∂ε/∂HT is a system of N equations ∂ε/∂hi,
i = 0,1,2,..., M −1. Since ε is a function of N variables h0,h1,...,hN−1 it
means that ∂ε/∂HT is a gradient of ε, ∇ε = gradient(ε) = ∂ε/∂HT.
Using this simpliﬁed symbolic notation we can analyze an Nth order
system. By differentiating (8.5) with respect to the system coefﬁcients we
get
∂ε
∂HT = −2rdx + 2RH
or
H∗= R−1rdx.
(8.8)
This kind of calculation of the optimal system coefﬁcients requires
statistical parameters of the input signal (in R) and its cross-correlation with
the reference signal (in rdx). In addition, it requires an inversion of a matrix
of order N which is numerically demanding operation (inversion of a matrix
of order N requires an order of O
P
N3Q
operations).
Minimal value of the mean square error is
εmin = σ2
d −2H∗Trdx + H∗TRH∗
= σ2
d −2(H∗)Trdx + (H∗)Trdx = σ2
d −(H∗)Trdx.
(8.9)
Example 8.3. Find the stationary point of the error function
ε(h0,h1) = 2 + 3h0 + 2h1 + 5h0h1 + 3h2
0 + 4h2
1
and prove that it is a minimum of this function.
⋆The partial derivatives of ε(h0,h1) with respect to h0 and h1 are
∂ε
∂h0
= 6h0 + 5h1 + 3
∂ε
∂h1
= 8h1 + 5h0 + 2.
They are equal to zero for
6h0 + 5h1 = −3
8h1 + 5h0 = −2.

436
Adaptive Systems
This system produces the solution
h∗
0 = −14
23
and
h∗
1 = 3
23.
It is a stationary point of ε(h0,h1). The stationary point can be minimum,
maximum or neither of this two (just a saddle point). To check what kind of
stationary point is the previous solution (h∗
0,h∗
1) we have to ﬁnd the second
order partial derivatives of ε(h0,h1). They are
∂2ε
∂h2
0
= 6
∂2ε
∂h0∂h1
= 5
∂2ε
∂h2
1
= 8
The stationary point is a minimum of the function if for h0 = h∗
0 and h1 = h∗
1
holds
∂2ε
∂h2
0
> 0
and
∂2ε
∂h2
0
∂2ε
∂h2
1
>
*
∂2ε
∂h0∂h1
+2
.
In the considered case these inequalities hold (6 > 0 and 6 · 8 > 52). Therefore,
the function ε(h0,h1) has a minimum at h∗
0 = −14
23 and h∗
1 = 3
23. The minimum
value is
ε(h∗
0,h∗
1) = 28
23.
Example 8.4. The input signal x(n) is a zero-mean white noise with variance 1.
The reference signal is d(n) = 1
2 x(n −2). Find the optimal coefﬁcients of the
fourth order system.
⋆The optimal coefﬁcients are the solution of
H∗= R−1rdx,
where R is the autocorrelation function of the input signal, equal to
R =
⎡
⎢⎢⎣
rxx(0)
rxx(1)
rxx(2)
rxx(3)
rxx(1)
rxx(0)
rxx(1)
rxx(2)
rxx(2)
rxx(1)
rxx(0)
rxx(1)
rxx(3)
rxx(2)
rxx(1)
rxx(0)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎤
⎥⎥⎦
since rxx(n) = δ(n). Cross-correlation function rdx(i) is deﬁned by
rdx(i) = E[d(n)x(n −i)] = E[1
2 x(n −2)x(n −i)] = 1
2rxx(i −2) = 1
2δ(i −2).
Therefore
rdx=
D
0
0
1
2
0
ET .

Ljubiša Stankovi´c
Digital Signal Processing
437
The optimal coefﬁcients are
H∗=
⎡
⎢⎢⎣
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎤
⎥⎥⎦
−1
·
⎡
⎢⎢⎣
0
0
1
2
0
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
0
0
1
2
0
⎤
⎥⎥⎦.
Indeed, if we set these optimal coefﬁcients we get the output signal in
the form
y(n) = 0 · x(n) + 0 · x(n −1) + 1
2 x(n −2) + 0 · x(n −3) = 1
2 x(n −2) = d(n).
The output signal is equal to the reference signal, with zero error.
Example 8.5. Signal x(n) is observed. The autocorrelation function values rxx(0) =
1, rxx(1) = 0.8, rxx(2) = 0.4 and rxx(3) = 0.1 are obtained by averaging. Find
the parameters of the optimal system that will predict values of x(n) one-step
ahead. The reference signal is d(n) = x(n + 1). Find the ﬁrst and second order
system, with N = 1 and N = 2. In both cases calculate the value of minimal
error εmin.
⋆For the ﬁrst-order system N = 1 we have H = [h0], R = [rxx(0)] = 1
and rdx = [E[d(n)x(n)]] = [E[x(n + 1)x(n)]] = [rxx(1)] = 0.8. The optimal
value of system coefﬁcient h0 is
H∗= h∗
0 = R−1rdx = 1 · 0.8 = 0.8
For the minimal error value εmin calculation we need the value of
σ2
d = E[d(n)d(n)] = E[x(n + 1)x(n + 1)] = rxx(0) = 1.
Based on (8.9) we get
εmin = σ2
d −(H∗)Trdx = 1 −0.8 · 0.8 = 0.36.
This is the mean square error in one-step prediction using a ﬁrst-order
system.
For the second order system (N = 2) we get
R =
-rxx(0)
rxx(1)
rxx(1)
rxx(0)
.
=
- 1
0.8
0.8
1
.
rdx =
-
E[d(n)x(n)]
E[d(n)x(n −1)]
.
=
-
E[x(n + 1)x(n)]
E[x(n + 1)x(n −1)]
.
=
-rxx(1)
rxx(2)
.
=
-0.8
0.4
.
.

438
Adaptive Systems
Optimal values of the system coefﬁcients are
H∗=
-h∗
0
h∗
1
.
= R−1rdx=
- 1
0.8
0.8
1
.−1 -0.8
0.4
.
=
- 4
3
−2
3
.
.
The minimal value of ε is
εmin = σ2
d −(H∗)Trdx = 1 −
D 4
3 −2
3
E-0.8
0.4
.
= 0.2.
It is signiﬁcantly lower than in the ﬁrst-order system.
Note that the optimal ﬁrst-order system is described by the relation
y(n) = 0.8x(n),
while the second order system has the form
y(n) = 4
3 x(n) −2
3 x(n −1).
Note that by using data given in the example it was possible to calcu-
late the coefﬁcients of a third-order (N = 3) one-step ahead prediction system
as well.
8.2.2
Autocorrelation Matrix Eigenvalues and Eigenvectors
Consider a square matrix R of order N. A nontrivial vector q is an eigenvec-
tor of matrix R if there exists a scalar λ such that
Rq = λq.
(8.10)
The scalar λ is an eigenvalue of R.
For a zero vector q = 0 relation (8.10) is met for any λ. This trivial
solution is not of our interest. Note that if q1 is an eigenvector of matrix R
then the vector q2 = αq1 is also an eigenvector of the considered matrix R
(with the same eigenvalue λ) for an arbitrary scalar α. Since the deﬁnition
of eigenvector has this kind of ambiguity, it is commonly assumed that the
eigenvector is normalized (having unit intensity) and deﬁned as
q0 =
q
||q||
where ||q|| is the intensity of q deﬁned as
||q|| =
F
q2
0 + q2
1 + ··· + q2
N−1.

Ljubiša Stankovi´c
Digital Signal Processing
439
Calculation of the eigenvalues and eigenvectors is based on their
deﬁnition (8.10),
Rq = λq
Rq −λq= 0
(R −λI)q= 0
(8.11)
where I is an identity matrix of order N (diagonal matrix with 1 on diago-
nal). In this way we have obtained a system of N homogeneous equations
with N unknown elements (coordinates) of vector q. Since we are inter-
ested in nontrivial solutions only, then the determinant of (R −λI) should
be equal to zero,
det(R −λI) = 0.
This determinant is an Nth order polynomial with variable λ. This polyno-
mial is called characteristic polynomial of matrix R. Roots of this polynomial
are the eigenvalues of matrix R. It is known that the Nth order polynomial
has N roots. An eigenvector is the associated to each eigenvalue.
Denote by λi roots of the characteristic polynomial and assume that
all roots are of the ﬁrst-order. Then the rank of matrix R −λI is N −1. This
implicates that the space of solutions is of dimension one (space of vectors q
satisfying (8.11)). It means that only one eigenvector q0i corresponds to the
eigenvalue λi. In the case that λi is a root of kth order then the rank of matrix
R −λI is for k lower than the order of matrix. The space of solutions is a k-
dimensional space deﬁned by k linear independent vectors q1i,q2i,...,qki.
Then there are k independent vectors corresponding to one λi. Note that
vectors q1i,q2i,...,qki are not unique here even with an additional condition
that they are orthonormal.
The eigenvalues of the autocorrelation matrix are real-valued and
positive. This follows from the symmetry property of the autocorrelation
matrix. Function rxx(m) is symmetric with maximum at m = 0. For positive
m holds rxx(m) ≥rxx(m + 1). It means that the eigenvalues of R are positive.
Example 8.6. Consider matrix R deﬁned by
R =
- 1
0.9
0.9
1
.
.
Find its eigenvalues and eigenvectors.

440
Adaptive Systems
⋆The characteristic polynomial of matrix R is
det(R −λI) = det(
- 1
0.9
0.9
1
.
−λ
-1
0
0
1
.
) =
= det(
-1 −λ
0.9
0.9
1 −λ
.
) =
= (1 −λ)2 −0.81 = λ2 −2λ + 0.19.
The roots of this polynomial are the eigenvalues λ0 = 1.9 and λ1 = 0.1. It is
common to index the eigenvalues starting from the highest absolute value
into descending order.
Next we will ﬁnd the eigenvectors. The eigenvector q0 corresponding to
λ0 = 1.9 follows from (8.11)
(R −λ0I)q0=0
-1 −λ0
0.9
0.9
1 −λ0
.-q00
q01
.
= 0
-−0.9
0.9
0.9
−0.9
.-q00
q01
.
= 0.
We have q00 = q01 = α where α is an arbitrary scalar. The eigenvector q0 is
q0 =
-q00
q01
.
=
-α
α
.
.
Scalar α is determined so that the intensity of q0 is 1,
F
q2
00 + q2
01 =
√
2α2, as
α = 1/
√
2. The ﬁnal form of q0 is
q0 =
> 1
√
2
1
√
2
?
The eigenvector corresponding to λ = λ1 is obtained in the same way from
(8.11)
(R −λ1I)q1=0
-1 −λ1
0.9
0.9
1 −λ1
.-q10
q11
.
= 0
-0.9
0.9
0.9
0.9
.-q10
q11
.
= 0.
It follows that q10 = −q11 = α with α = 1/
√
2. Vector q1 assumes the form
q1 =
>
1
√
2
−1
√
2
?

Ljubiša Stankovi´c
Digital Signal Processing
441
In the previous example we have calculated eigenvectors q0, q1 and
eigenvalues λ0, λ1 from the equations
Rq0 = λ0q0
Rq1 = λ1q1.
These two equations can be written in one matrix equation as
R
D
q0
q1
E =
D
q0
q1
E-
λ0
0
0
λ1
.
.
Introducing the matrix notation for eigenvectors and eigenvalues as
Q =
D
q0
q1
E =
-
q00
q10
q01
q11
.
Λ =
-
λ0
0
0
λ1
.
we may write
RQ = QΛ
or
R = QΛQ−1
Λ=Q−1RQ.
Matrix Q contains the eigenvectors as its columns. This matrix is orthonor-
mal matrix, Q−1 = QT. Then we can write
R = QΛQT
Λ=QTRQ
The same matrix relations can be written for any order N of autocorrelation
matrix R.
Example 8.7. For the autocorrelation matrix R deﬁned by
R =
⎡
⎣
3
1
1
1
3
1
1
1
3
⎤
⎦
ﬁnd the eigenvalues and eigenvectors and write matrices Q and Λ.

442
Adaptive Systems
⋆The characteristic polynomial of matrix R is
det(R −λI) = det(
⎡
⎣
3 −λ
1
1
1
3 −λ
1
1
1
3 −λ
⎤
⎦) =
= −λ3 + 9λ2 −24λ + 20.
Roots of this polynomial are λ0 = 5 and λ1,2 = 2. The root λ = 2 is of order 2.
The eigenvector q0 corresponding to λ0 = 5 follows from (8.11) as
(R −λ0I)q0=0
⎡
⎣
−2
1
1
1
−2
1
1
1
−2
⎤
⎦
⎡
⎣
q00
q01
q02
⎤
⎦= 0.
Since the rank of the system matrix is 2 the system does not have a unique
solution. One equation is omitted. Solving two remaining equations for two
unknowns we get q00 = q01 = q02 = α, where α is an arbitrary scalar. The
solution is
q0 =
⎡
⎣
q00
q01
q02
⎤
⎦=
⎡
⎣
α
α
α
⎤
⎦.
Value of scalar α is found in such a way to normalize the intensity of q0. It
follows that α = 1/
√
3, or
q0 =
⎡
⎢⎢⎣
1
√
3
1
√
3
1
√
3
⎤
⎥⎥⎦.
For the second and third eigenvector we use λ = 2
(R −2I)q2=0
⎡
⎣
1
1
1
1
1
1
1
1
1
⎤
⎦
⎡
⎣
q10
q11
q12
⎤
⎦= 0.
The system reduces to one equation only
q10 + q11 + q12 = 0,
with the solution q12 = −q10 −q11. Therefore we may take two variables
q10 = α and q11 = β as arbitrary. The solution is
q =
⎡
⎣
α
β
−α −β
⎤
⎦.

Ljubiša Stankovi´c
Digital Signal Processing
443
We are interested in orthogonal vectors. For the second vector we will use
α = β. After normalization we get
q1 =
⎡
⎢⎢⎣
1
√
6
1
√
6
−2
√
6
⎤
⎥⎥⎦
The third vector should be orthogonal to q1, meaning that their scalar prod-
uct is zero,
⟨q,q1⟩=
1
√
6
α + 1
√
6
β −2
√
6(−α −β) = 0
−α −β = 0
α = −β.
With α = −β after normalization, the third eigenvector is obtained in the form
q2 =
⎡
⎢⎣
−1
√
2
1
√
2
0
⎤
⎥⎦.
Matrices Q and Λ contain the eigenvectors and eigenvalues, respectively,
Q =
⎡
⎢⎢⎣
1
√
3
1
√
6
−1
√
2
1
√
3
1
√
6
1
√
2
1
√
3
−2
√
6
0
⎤
⎥⎥⎦
Λ =
⎡
⎣
5
0
0
0
2
0
0
0
2
⎤
⎦
It is easy to check that QTQ = I and R = QΛQT.
8.2.3
Error Signal Analysis
The mean square error of a linear adaptive adder has been deﬁned by (8.5)
as
ε = σ2
d −2HT(n)rdx + HT(n)RH(n).
(8.12)
Its minimization produced the optimal coefﬁcients
H∗= R−1rdx.
The minimal value of the mean square error is obtained for H(n) = H∗in
(8.12) as
εmin = σ2
d −2(H∗)Trdx + (H∗)TRH∗=
= σ2
d −2(H∗)Trdx + (H∗)Trdx =
= σ2
d −(H∗)Trdx = σ2
d −(H∗)TRH∗
(8.13)

444
Adaptive Systems
The error (8.12) can be expressed in terms of εmin, the autocorrelation matrix
R, and optimal coefﬁcients H∗. The value of σ2
d is calculated using (8.13) and
replaced in (8.12),
ε = εmin + (H∗)TRH∗−2HTrdx + HTRH.
Time index n is omitted to simplify the notation. Since the cross-correlation
vector rdx is equal to rdx = RH∗, it follows
ε = εmin + (H∗)TRH∗−2HTRH∗+ HTRH
= εmin + (H∗)TRH∗−HTRH∗−HTRH∗+ HTRH
= εmin +
B
(H∗)T −HTC
RH∗−HTR(H∗−H)
= εmin + (H∗−H)T RH∗−HTR(H∗−H).
Note that (H∗−H)T RH∗and HTR(H∗−H) are scalars, when
(H∗−H)T RH∗=
B
(H∗−H)T RH∗CT
= (H∗)TRT (H∗−H) = (H∗)TR(H∗−H),
with RT = R. The mean square error can be written as
ε = εmin + (H∗)TR(H∗−H) −HTR(H∗−H)
= εmin +
B
(H∗)T −HTC
R(H∗−H)
= εmin + (H −H∗)TR(H −H∗).
Using the autocorrelation matrix R expressed in terms of its eigenvalues
and eigenvectors R = QΛQT we get
ε = εmin + (H −H∗)TQΛQT(H −H∗).
Let us introduce new coefﬁcients V deﬁned by
V = QT(H −H∗).
In the new coordinate system whose coordinates are elements of vector V
the minimum of mean square error is achieved for V = 0 (since V = 0 for

Ljubiša Stankovi´c
Digital Signal Processing
445
H = H∗). The mean square error can be written as
ε = εmin + VTΛV.
In order to illustrate this relation, consider a two-dimensional case
(N = 2), when the mean square error is
ε = εmin +
D
v0
v1
E-
λ0
0
0
λ1
.-
v0
v1
.
ε = εmin + v2
0λ0 + v2
1λ1.
This equation can be rewritten in the form
v2
0
ε−εmin
λ0
+
v2
1
ε−εmin
λ1
= 1
representing (for a constant ε) an ellipse in the coordinate system v0,v1. The
center of ellipse is at (0,0). The main axes of the ellipse coincide with the
coordinate axes. The eigenvalues deﬁne the ellipse semi-major and semi-
minor axis. This conclusion is in agreement with the previous ﬁndings that
the mean square error function in two-dimensional case is a paraboloid in
the coordinate system h0,h1. Graphical representation of the mean square
error in v0,v1 and h0,h1 systems is presented in Fig. 8.7.
8.2.4
Orthogonality Principle
Two random signals x1(n) and x2(n) are orthogonal if
E[x1(n)x2(n)] = 0
holds. When the system coefﬁcients assume their optimal values H∗=
R−1rdx the error signal e(n) is orthogonal to the output signal y(n) and
to all components of the input signal vector X(n). In order to prove this
property consider the orthogonality of e(n) to X(n) for H∗= R−1rdx with
y(n) = XT(n)H∗,
E[e(n)X(n)] = E[(d(n) −y(n))X(n)] = E[d(n)X(n)] −E[X(n)y(n)] =
= rdx −E[X(n)XT(n)H∗] = rdx−E[X(n)XT(n)]R−1rdx=
= rdx−RR−1rdx= rdx −rdx = 0
Deﬁnitions E[X(n)XT(n)] = R and E[d(n)X(n)] = rdx are used here.

446
Adaptive Systems
h1
h0
h*
1
h*
0
v0
v1
Figure 8.7
Coordinate system change by translation origin to the optimal point and by using
coordinate axes deﬁned by eigenvectors of the autocorrelation matrix
Orthogonality of e(n) to y(n) directly follows from the orthogonality
of the error signal to the input signal, since
E[e(n)y(n)] = E[(H∗)TX(n)e(n)] = (H∗)TE[e(n)X(n)] = 0.
8.3
STEEPEST DESCEND METHOD
Consider the optimal ﬁlter whose coefﬁcients are obtained by minimizing
the mean square error
ε = E[e2(n)].
This minimization resulted in the matrix equation (system of linear equa-
tions)
RH = rdx
Its solution produces the optimal values of the coefﬁcients of the adaptive
system. In order to avoid inversion of matrix R the solution of this system
of linear equations may be obtained using an iterative method. One of the
iterative methods is the steepest descend method. It will be presented next.
This method consists of an arbitrary initial value H0 of system coef-
ﬁcients vector in the ﬁrst step (usually H0 = 0). The coefﬁcients are then
modiﬁed in an iterative way toward the minimum of mean square error ε.

Ljubiša Stankovi´c
Digital Signal Processing
447
Direction of the steepest accent of function ε is deﬁned by the gradient of
this function ∇ε. The elements of the gradient vector ∇ε are the derivatives
of ε with respect to the variables (coefﬁcients) Hi, ∂ε/∂Hi. Vector form of
these derivatives is ∇ε = ∂ε/∂H. The steepest descend is in the direction
opposite to the gradient. It is −∇ε. The ﬁrst iteration step is
H1 = H0 + µ
2 (−∂ε
∂H)
''''
H=H0
where µ/2 deﬁnes the step in the steepest descend direction. In general the
iterations are deﬁned by
Hn+1 = Hn + µ
2 (−∂ε
∂H)
''''
H=Hn
for n = 0,1,... . A common stopping criterion for the iterative procedure is
deﬁned by using the difference of two consecutive iteration Hn+1 −Hn. If
the coefﬁcients in two iterations are sufﬁciently close, the iteration process
is stopped.
The gradient vector can be written as
∂ε
∂H = −2rdx + 2RH.
The iterative relation is then
Hn+1 = Hn + µ(rdx −RHn).
(8.14)
The iterative algorithm step µ deﬁnes the value of coefﬁcients change
in the direction of the steepest descend. Its choice is crucial for the itera-
tive algorithm performance. Too small values of step µ will guarantee the
convergence of the iterative algorithm, but at the expense of a very large
number of iterations. Larger values of µ will reduce the number of iterations
to reach the optimal solution. Too large values of µ could lead to the iterative
algorithm divergence.
The steepest descend method is illustrated on a second order adaptive
system in Figure 8.8. Contour lines represent the mean square error value in
the space of system coefﬁcients h0, h1. The optimal coefﬁcient values h∗
0, h∗
1
should be obtained as a result of the iterative algorithm. Optimization using
the steepest descend algorithm, with the different steps µ, is considered. The
smallest step µ is used in Figure 8.8(a). The initial value of the system coef-
ﬁcients is denoted by 0 position in the coordinate system h0, h1. Positions of

448
Adaptive Systems
h1
h0
h*
1
h*
0
0
1
2
3
(a)
h1
h0
h*
1
h*
0
0
1
2
(b)
h1
h0
h*
1
h*
0
0
1
2
3
(c)
Figure 8.8
Steepest descent method illustration. Smallest step µ is used in case (a), larger step
is presented in case (b), and the largest µ corresponds to case (c). The steepest descend method
converges in cases (a) and (b), while it diverges in case (c). Contour plot of error function is
presented in all cases. Iterations are marked with dots and numbers 0,1,2,..., where 0 is the
starting iteration.
the coefﬁcient values in the next iterations are denoted by 1,2,3,...We can
see that the iterative procedure converges toward the optimal coefﬁcient
values h∗
0, h∗
1. A larger step µ is used in the case presented in Figure 8.8(b).
The iterative algorithm convergence is faster than in the previous case. In
the third case, presented in Figure 8.8(c), a very large step µ is used. The step
is here too large and the iterative algorithm does not converge to the optimal
coefﬁcient values. Note that the convergence in all of these cases does not
depend on the initial position (initial value of the system coefﬁcients).
The range of step µ values when the steepest descend iterative algo-
rithm converges can be determined in an analytical way. The optimal coefﬁ-
cient values are obtained as a result of the equation RH∗= rdx. Consider the
deviation of the system coefﬁcients vector Hn+1 in (n + 1)th iteration from

Ljubiša Stankovi´c
Digital Signal Processing
449
the optimal value H∗. The deviation is
Hn+1 −H∗= Hn + µ(RH∗−RHn) −H∗
= (I −µR)(Hn −H∗).
Relation (8.14) is used. The deviation (Hn+1 −H∗) is equal to the deviation
value in the previous iteration (Hn −H∗) multiplied by the matrix (I −µR).
It is easy to relate now the deviation in the nth iteration with the initial
deviation
Hn −H∗= (I −µR)n(H0 −H∗).
The autocorrelation matrix R can be written as R = QΛQT, where Q is the
matrix of eigenvectors (which are orthonormal, Q−1 = QT and QTQ = I)
and Λ is the eigenvalue matrix. The deviation in the nth iteration assumes
the form
Hn −H∗= Q(I −µΛ)nQT(H0 −H∗).
(8.15)
Matrix (I −µΛ)n is a diagonal matrix with elements (1 −µλk)n, k =
0,1,..., N −1, where N is the order of matrix R, i.e., the order of the system.
All matrices on the right side of equation (8.15) do not depend on the itera-
tion index n. The deviation Hn −H∗will tend to zero for n →∞(meaning
that the iterative algorithm will converge toward H∗) if the absolute values
of all elements on the diagonal of matrix (I −µΛ) are smaller than 1. In that
case the matrix (I −µΛ)n tends to zero valued matrix when the number of
iterations increase, for n →∞. Therefore, the convergence condition is that
the inequalities
|1 −µλk| < 1
hold for all k = 0,1,..., N −1. These inequalities reduce to −1 < 1 −µλk < 1
or 0 < µ < 2/λk for all k. Finally, the steepest descend method converges if
the step µ satisﬁes the condition
µ <
2
λmax
(8.16)
where λmax is the largest eigenvalue of the autocorrelation matrix R.
Calculation of eigenvalues requires a lengthy numerical procedure.
Note that a rough estimate of the maximal step µ can be obtained using
a simple relation
µ <
2
Tr[R]
since λmax < Tr[R]. Here Tr[R] denotes trace of matrix R. It is equal to the
sum of the matrix R elements on the diagonal. It can be easily calculated as
Tr[R] = Nrxx(0) = NE
@
|x(n)|2A
= Ex, where Ex is the energy of input signal.

450
Adaptive Systems
Condition (8.16) guarantees that the coefﬁcients deviation will tend
to zero. However, it does not tell anything about the rate of convergence.
Let the eigenvalues of the autocorrelation matrix R
be indexed into a
nonincreasing order λ0 ≥λ1 ≥... ≥λN−1 and µ < 2/λ0. The total square
deviation of all coefﬁcients can be calculated using (8.15). Its value is
EH = ∥Hn −H∗∥2
2 = (Hn −H∗)T(Hn −H∗)
=
B
(H0 −H∗)TQ(I −µΛ)nQTCB
Q(I −µΛ)nQT(H0 −H∗)
C
= (H0 −H∗)TQ(I −µΛ)2nQT(H0 −H∗).
We can conclude that the total square deviation is a linear combination of
the terms
P(1 −µλi)2Qn, i.e.,
EH = A0
B
(1 −µλ0)2Cn
+ A1
B
(1 −µλ1)2Cn
+ ... + AN−1
B
(1 −µλN−1)2Cn
where the coefﬁcients A0, A1,..., AN−1 are independent from the iteration
index n. Values of (1 −µλi)2 are within the interval from 0 to 1 (since we
assume that the convergence condition is meet). The terms with smaller
values of (1 −µλi)2 converge faster as the iteration index n increases. Now
we will ﬁnd a value of step µ that will take into account convergence of
all coefﬁcients. For small values of µ the term with the smallest eigenvalue
P(1 −µλN−1)2Qn is dominant. For step µ close to its upper bound value the
term
P(1 −µλ0)2Qn is dominant. The best choice for µ will be its value when
these two terms are equal,
(1 −µλ0)2 = (1 −µλN−1)2
µ(λ2
0 −λ2
N−1) = 2(λ0 −λN−1)
µ =
2
λ0 + λN−1
.
In this case, for k = 0,1,..., N −1 holds
(1 −µλ0)2 ≥(1 −µλk)2
(1 −µλ0)2 −(1 −µλk)2 ≥0
µ(λ0 −λk)(2 −µ(λ0 + λk)) ≥0
2µ(λ0 −λk)
*
λ0 + λk
λ0 + λN−1
−1
+
≥0
2µ(λ0 −λk)
* λk −λN−1
λ0 + λN−1
+
≥0,

Ljubiša Stankovi´c
Digital Signal Processing
451
since µ(λ0 −λk) ≥0 having in mind that λ0 is the largest eigenvalue
and that λk ≥λN−1, where λN−1 is the smallest eigenvalue. This relation
reafﬁrms the approach to consider only the behavior of the ﬁrst and the
last term in EH. The optimal value of step µ, from the point of view of the
coefﬁcients deviation, is
µopt =
2
λmax + λmin
where λmax = λ0 is the largest and λmin = λN−1 is the smallest eigenvalue
of the input autocorrelation matrix.
In a special case, if all eigenvalues are equal, λ0 = λ1 = ... = λN−1 = λ,
the relation for optimal step µ produces
µopt = 1
λ.
The square deviation of the coefﬁcients (for n > 0) is
EH = A00n + A10n + ... + AN−10n = 0.
In means that the steepest descend method, in this special case, will reach
the optimal system coefﬁcients H∗in one iteration step.
Example 8.8. Consider autocorrelation of an input signal, as in 8.6
R =
- 1
0.9
0.9
1
.
.
Assume that the cross-correlation vector of the input and reference signal is
rdx =
-3.8
1.9
.
.
Find the optimal system coefﬁcients using the inverse of autocorrelation
matrix. Find adaptive system coefﬁcients using the steepest descend method.
Use 3 different step values: µ1 = 0.5, µ2 = 1, and µ3 = 1.5. In all cases ﬁnd the
number of iterations to achieve the square deviation value of the coefﬁcients
∥Hn+1 −H∗∥2 lower than 0.01. The initial value of H0 is the zero-vector.
⋆The optimal coefﬁcients are
H∗= R−1rdx=
- 1
0.9
0.9
1
.−1 -3.8
1.9
.
=
- 11
−8
.
.

452
Adaptive Systems
Using the steepest descend method, starting from H0 = 0, we get
H1 = H0 + µ1(rdx −RH0) =
-0
0
.
+ 0.5
-3.8
1.9
.
=
- 1.9
0.95
.
H2 = H1 + µ1(rdx −RH1) =
- 1.9
0.95
.
+ 0.5
- 1.045
−0.76
.
=
-2.4225
0.47
.
H3 =
-2.855
0.145
.
,
...,
H141 =
- 10.993
−7.993
.
After 141 iterations the norm of the coefﬁcients deviation is below 0.01.
Using a larger step, µ2 = 1, we get
H1 =
-3.8
1.9
.
H2 =
- 2.09
−1.52
.
H3 =
-5.168
0.019
.
...
H69 =
- 10.994
−7.992
.
The required precision is achieved in 69 iterations.
For the step value µ3 = 1.5 it follows
H1 =
- 5.7
2.85
.
H2 =
- −1
−6.27
.
... H5 =
-39.3
28.7
.
... H10 =
-−695
−710
.
...
The iterative algorithm in this case diverges.
Note that the eigenvalues of the autocorrelation matrix R are λ0 = 1.9
and λ1 = 0.1. The bound for the step is
µ <
2
λmax
= 2
1.9 ≈1.0526.
This condition is satisﬁed for the ﬁrst two cases. In the third case the conver-
gence condition is not met. The optimal value of the step is
µopt =
2
λmin + λmax
= 1.
This is the reason why the second case produced required precision in a lower
number of iterations than the ﬁrst case.
Example 8.9. Consider an adaptive system of the second order, described by the
difference equation
y(n) = h0(n) x(n) + h1(n) x(n −1)
where h0(n) and h1(n) are real-valued varying system parameters. The in-
put signal x(n) is stationary with the autocorrelation function rxx(m) =
5δ(m) + 3δ(m2 −1). The reference signal is d(n) with the cross-correlation
between the input and reference signal rdx(m) = δ(m) + 1
2δ(m −1). System
is adapted by using the steepest descend method with step µ. The initial
conditions for the system coefﬁcients are h0(0) = 0 and h1(0) = 0. Find the

Ljubiša Stankovi´c
Digital Signal Processing
453
optimal system coefﬁcients in the sense of minimal mean square error, where
the error is e(n) = d(n) −y(n). Find the coefﬁcient values as a function of
the iteration (time) index n. Find the range for the step µ when the coefﬁ-
cients converge toward the optimal values. For the cases when the system
coefﬁcients converge ﬁnd the number of iterations when the mean square
deviation of the coefﬁcients from the optimal values will be lower than 10−6.
⋆System is of the second order. Its autocorrelation matrix and cross-
correlation vector are
R=
-rxx(0)
rxx(1)
rxx(1)
rxx(0)
.
=
-5
3
3
5
.
rdx =
-rdx(0)
rdx(1)
.
=
-1
1
2
.
.
The inverse of R is
R−1= 1
16
- 5
−3
−3
5
.
,
with the optimal coefﬁcients of the system
H∗= R−1rdx
-h∗
0
h∗
1
.
= 1
16
- 5
−3
−3
5
.
·
-1
1
2
.
=
-
7
32
−1
32
.
.
In order to get the coefﬁcients h0(n) and h1(n) as a function of the
iteration (time) index n we will use the iteration relation for the steepest
descend method
Hn+1 = Hn + µ(rdx −RHn)
with the initial condition H0 = [0 0]T, or
-h0(n + 1)
h1(n + 1)
.
=
-h0(n)
h1(n)
.
+ µ
*-1
1
2
.
−
-5
3
3
5
.-h0(n)
h1(n)
.+
.
The system of equations is
h0(n + 1) = h0(n) + µ(1 −5h0(n) −3h1(n))
h1(n + 1) = h1(n) + µ(1 −3h0(n) −5h1(n)).
Expressing h1(n) from the ﬁrst equation
h1(n) = 1 −5µ
3µ
h0(n) −1
3µ h0(n + 1) + 1
3
and replacing it into the second equation, to get
1 −5µ
3µ
h0(n + 1) −1
3µ h0(n + 2) + 1
3 =
= (1 −5µ)
*1 −5µ
3µ
h0(n) −1
3µ h0(n + 1) + 1
3
+
+ µ −3µh0(n).

454
Adaptive Systems
This is a recursive (difference) relation for h0(n). It can be written as
h0(n + 2) −2(1 −5µ)h0(n + 1) + (1 −8µ)(1 −2µ)h0(n) = µ(1 −3µ)
with initial conditions h0(0) = 0, h0(1) = h0(1) + µ(1 −5h0(1) −3h1(1)) = µ.
The solution of this equation is
h0(n) = −3
32(1 −8µ)n −1
8(1 −2µ)n + 7
32.
From the relationship between h1(n) and h0(n) follows
h1(n) = −3
32(1 −8µ)n + 1
8(1 −2µ)n −1
32.
Consider limit values
lim
n→∞h0(n)
and
lim
n→∞h1(n).
They are ﬁnite if
|1 −8µ| < 1
and
|1 −2µ| < 1.
Using positive value of the step µ we get µ < 1/4. For this value of step, the
limit values are equal to the optimal system coefﬁcient values. For µ > 1/4
the coefﬁcients tend to inﬁnity. In the limit case µ = 1/4, for a large n (so that
the term with (1 −2µ)n can be neglected) the coefﬁcients are approximately
equal to
h0(n) = 7
32 −3
32(−1)n
h1(n) = −1
32 −3
32(−1)n.
They assume oscillatory form, with oscillations around the optimal values of
the system coefﬁcients.
The number of iterations needed to get the mean square deviation of
the coefﬁcients bellow 10−6 follows from
(h0(n) −h∗
0)2 + (h1(n) −h∗
1)2
2
< 10−6
or
9
1024(1 −8µ)2n + 1
64(1 −2µ)2n < 10−6.
This inequality does not have a closed form solution. For a given step µ
(0 < µ < 1/4) the minimal number of iterations n can be found in a numerical
way. Solutions for some possible values of step µ are given as
µ
0.01
0.1
0.15
0.18
0.19
0.2
0.21
0.22
0.24
0.248
n
239
22
14
11
11
10
12
17
55
282

Ljubiša Stankovi´c
Digital Signal Processing
455
From this table we can conclude that small values of step µ should not be
used since the convergence is very slow. Based on the values from the table
we can conclude that the optimal step is around µ = 0.2. Next we will ﬁnd
this value based on the analytical consideration of the coefﬁcients. Assume
an arbitrary value of variable n and use the equality for the mean square error
9
1024(1 −8µ)2n + 1
64(1 −2µ)2n = 10−6
9
1024
B
(1 −8µ)2Cn
+ 1
64
B
(1 −2µ)2Cn
= 10−6.
This formula provides the relation between n and µ. Finding the value of µ
that produces minimal n is not simple. Note that the left side of the previous
equation consists of two positive terms. Assume that, for a sufﬁciently large
n, the terms are of the same order. It results in
(1 −8µ)2 = (1 −2µ)2
µ(5µ −1) = 0
or µ = 0.2. For this value of step µ the number of iterations is
n = log 1024
25 10−6
log 9
25
≈9.888 ≈10
These values of µ and n correspond to the numerically obtained ones,
presented in the table. For µ < 0.2 the second term dominates in the mean
square deviation relation. The number of iterations can then be determined
as
1
64
B
(1 −2µ)2Cn
= 10−6
n = log(64 · 10−6)
log((1 −2µ)2).
For µ = 0.15 we get n ≈13.537. This result is in agreement with the numerical
one obtained for n = 14. For µ > 0.2 the ﬁrst term is dominant and
n = log(64 · 10−6)
log((1 −8µ)2).
For µ = 0.22 value n = 17.594 follows. It corresponds to the numerical result
n = 17.
Example 8.10. Analyze the convergence of the steepest descend method using the
eigenvalues of the autocorrelation matrix from the previous example.
⋆The autocorrelation matrix is
R =
-5
3
3
5
.
.

456
Adaptive Systems
Its eigenvalues follow from det(R −λI) = 0 as
''''
5 −λ
3
3
5 −λ
'''' = 0
(5 −λ)2 −9 = 0
λ0 = 8
λ1 = 2
The steepest descend method converges for
µ <
2
λmax
= 2
8 = 1
4 = 0.25.
The optimal rate of convergence is achieved for
µ =
2
λmax + λmin
=
2
8 + 2 = 1
5 = 0.2.
Example 8.11. The autocorrelation function of an input signal is rxx(m) = 2−|m|.
The cross-correlation of the reference and the input signal is rdx(m) =
2δ(m) + δ(m −1). Adaptive system is of order N = 3. The coefﬁcients are
adapted using the steepest descend method. Find the optimal coefﬁcient val-
ues, as well as the bound and optimal value for the step µ.
⋆The autocorrelation matrix and cross-correlation vector are
R =
⎡
⎣
1
1
2
1
4
1
2
1
1
2
1
4
1
2
1
⎤
⎦
and
rdx =
⎡
⎣
2
1
0
⎤
⎦.
The optimal system coefﬁcient values are
H∗= R−1rdx =
⎡
⎣
2
1
3
−2
3
⎤
⎦.
The eigenvalues of the autocorrelation matrix are the roots of characteristic
polynomial
''''''
1 −λ
1
2
1
4
1
2
1 −λ
1
2
1
4
1
2
1 −λ
''''''
= 0
(1 −λ)3 + 2
16 −9
16(1 −λ) = 0.

Ljubiša Stankovi´c
Digital Signal Processing
457
Their values are
λ0 = 3
4 = 0.75
λ1 = 9 −
√
33
8
≈0.407
λ2 = 9 +
√
33
8
≈1.843.
The steepest descend method converges with a step
µ <
2
λmax
=
16
9 +
√
33 ≈1.085.
Optimal rate of convergence is achieved if
µ =
2
λmax + λmin
=
2
9−
√
33
8
+ 9+
√
33
8
= 8
9 ≈0.889.
8.4
LMS ALGORITHM
Consider ﬁrst the steepest descend method with iterative adaptation of
system coefﬁcients. Denote the iteration index by k. In general, the input
and reference signals d(n) and x(n) are not stationary. Their statistical
parameters may change with the current instant n. As a consequence, the
coefﬁcients of the adaptive system change in time. The iterative procedure
is then preformed for each instant n according to
Hk+1(n) = Hk(n) + µ(rdx(n) −R(n)Hk(n))
where rdx(n) and R(n) are deﬁned as the expectations
rdx(n) = E[d(n)X(n)]
R(n) = E[X(n)XT(n)].
Statistical properties of the signals are not fast-varying. For each next instant
n we may use the system coefﬁcients obtained at the previous instant n −1
(in K iterations) as the initial values
H0(n) = HK(n −1).

458
Adaptive Systems
Assume that only one iteration is done for each time instant n. With K = 1 it
follows
H1(n) = H0(n) + µ(rdx(n) −R(n)H0(n))
H1(n) = H1(n −1) + µ(rdx(n) −R(n)H1(n −1))
H1(n + 1) = H1(n) + µ(rdx(n + 1) −R(n + 1)H1(n)).
For notation simplicity, the index denoting the number of iterations will be
omitted (since it has been assumed that it is 1). Then we can write
H(n + 1) = H(n) + µ(rdx(n + 1) −R(n + 1)H(n)).
In the LMS algorithm the autocorrelation matrix R(n + 1) and the cross-
correlation vector rdx(n + 1) are approximated by their instantaneous values
rdx(n + 1) ≈rdx(n) ≈d(n)X(n)
R(n + 1) ≈R(n) ≈X(n)XT(n).
Using this approximation the iteration formulae are
H(n + 1) = H(n) + µ(d(n)X(n) −X(n)XT(n)H(n))
(8.17)
= H(n) + µX(n)(d(n) −XT(n)H(n)).
With y(n) = XT(n)H(n) it follows
H(n + 1) = H(n) + µ(d(n) −y(n))X(n).
(8.18)
Difference d(n) −y(n) is the error signal e(n). A common LMS algorithm
form reads
H(n + 1) = H(n) + µe(n)X(n)
(8.19)
In each time instant the coefﬁcients of adaptive system are changed with
respect to their previous values in the direction of input signal vector X(n).
Intensity of the change is determined by the step µ and the error signal at
the previous instant e(n).
For a system of order N the LMS algorithm is numerically very efﬁ-
cient. At each instant n it needs N + 1 multiplication and N additions.
8.4.1
Convergence of the LMS algorithm
Consider a stationary signals when matrix R(n) and vector rdx(n) are time
invariant. Then the LMS algorithm converges „in mean” toward the optimal

Ljubiša Stankovi´c
Digital Signal Processing
459
system coefﬁcient values H∗
lim
n→∞E[H(n)] = H∗
under the same conditions as in the steepest descend case. The step µ in the
LMS algorithm should be such that
µ <
2
λmax
where λmax denotes the maximal eigenvalue of the autocorrelation matrix
R. It can be easily proven by considering the expected value of the adaptive
system coefﬁcients
E[H(n + 1)] = E[H(n) + µe(n)X(n)] =
= E[H(n)] + µE[X(n)(d(n) −XT(n)H(n))] =
= E[H(n)] + µE[d(n)X(n)] −µE[X(n)XT(n)H(n)] =
= E[H(n)] + µrdx −µE[X(n)XT(n)H(n)].
Assume that the expected value E[H(n)], for a sufﬁciently large n, does not
depend on n as well as that X(n) and H(n) are mutually independent. Then,
with E[H(n + 1)] = E[H(n)] = HLMS, it follows
HLMS = HLMS + µrdx −µE[X(n)XT(n)]E[H(n)]
or
HLMS = HLMS + µrdx −µRHLMS.
From this relation we get
µRHLMS = µrdx
RHLMS = rdx
HLMS = R−1rdx
HLMS = H∗.
This proves the statement that the LMS algorithm coefﬁcients converge „in
mean” to the optimal system coefﬁcient values.
The convergence in mean does not mean that the LMS achieves the
optimal value in the stationary state. If there is a smallest difference between
the reference and the output signal it will cause the coefﬁcients ﬂuctuation.

460
Adaptive Systems
In addition convergence in mean does not guarantee that the results will
converge to the same values. It can be shown that the LMS algorithm will
converge with ﬁnite variations of the coefﬁcients and the error if the step µ
satisﬁes a more conservative bound
µ <
2
∑N
k=1 λk
than the bound µ < 2/λmax requited for the convergence „in mean”. It is
known that the sum of the eigenvalues is equal to the trace of matrix R.
As it has been stated for the steepest descend method, the trace can easily
calculated as Tr[R] = Nrxx(0) = NE
@
|x(n)|2A
= Ex, where Ex is input signal
energy.
8.5
LMS APPLICATION EXAMPLES
8.5.1
Identiﬁcation of Unknown System
Consider the problem of unknown system identiﬁcation. A way to solve this
problem is in using adaptive system with the same input as the input to the
unknown system. The unknown system output is used as a reference signal
d(n) in the adaptive system. If the unknown system can be described by
d(n) = a0x(n) + a1x(n −1) + ... + aM−1x(n −1)
then the Nthe order adaptive system, with output signal
y(n) = HT(n)X(n) = [h0(n) h1(n) ...hN−1(n)]
⎡
⎢⎢⎢⎣
x(n)
x(n −1)
...
x(n −N + 1)
⎤
⎥⎥⎥⎦,
can adapt its coefﬁcients, through the iterative procedure, in such a way
that y(n) is as close as possible to d(n). In an ideal case, with N ≥M, it is
possible to obtain limn→∞H(n) = [a0 a1...aM−1 0 ... 0]. In that case e(n) = 0.
The system is identiﬁed when the error is equal to zero. The identiﬁcation
of an unknown system is illustrated in Figure 8.9.
If the unknown system is an inﬁnite impulse
response (recursive)
system or if the order of ﬁnite impulse response system is greater than the
adaptive system order, then we will get an approximation of the unknown
system, in the sense of minimal mean square error. The error signal will not
vanish as n increases.

Ljubiša Stankovi´c
Digital Signal Processing
461
adaptive
system
unknown
system
x(n)
y(n)
e(n)
d(n)
+
-
Figure 8.9
Identiﬁcation of unknown system.
Example 8.12. Consider a system with transfer function
H(z) = 3 + 2z−1 −z−2 + z−3.
It has been assumed that this signal is unknown. Identiﬁcation of this system
is done using an adaptive system of order N = 3. The identiﬁcation process
is repeated with an adaptive system of order N = 5. The input to the system
x(n) is Gaussian zero-mean white noise with variance σ2x = 1. The step
µ = 0.05 is used in the adaptive algorithm. Comment the results.
⋆For the input signal x(n) the reference signal is d(n) = 3x(n) +
2x(n −1) −x(n −2) + x(n −3). This reference signal is used in the adaptive
system of order N = 3 implemented as
y(n) = HT(n)X(n) = h0(n)x(n) + h1(n)x(n −1) + h2(n)x(n −1).
The adaptive coefﬁcients are calculated using H(n + 1) = H(n) + µ(d(n) −
y(n))X(n) with H(0) = 0 and µ = 0.05. The results of simulation (error signal
e(n) and coefﬁcients H(n) = [h0(n) h1(n) h2(n)]T) are presented in Figure
8.10(top).
The error signal does not vanish with N = 3. The adaptive system
cannot identify the system by varying its coefﬁcients. The reason is obvious.
The unknown system is of order 4 and we have tried to identify it with
an adaptive system of order 3. After about 100 iterations the error signal
and the coefﬁcients assume a state with random variations and do not
produce stationary values. At the end of adaptation interval the coefﬁcients
are h0(200) = 3.16, h1(200) = 1.79 and h2(200) = −0.99. The average values
of these coefﬁcients, calculated for n = 100,101,...,200, are
¯h0 =
1
101
200
∑
n=100
h0(n) = 2.72
¯h1 = 2.03
¯h2 = −0.92.

462
Adaptive Systems
0
50
100
150
200
-6
-4
-2
0
2
4
6
time index n
error signal e(n)
(a) 
0
50
100
150
200
-2
-1
0
1
2
3
4
time index n
coefficients hk(n)
h0(n)
h1(n)
h2(n)
(b) 
0
50
100
150
200
-6
-4
-2
0
2
4
6
time index n
error signal e(n)
(c) 
0
50
100
150
200
-2
-1
0
1
2
3
4
time index n
coefficients hk(n)
h0(n)
h1(n)
h2(n)
h3(n)
h4(n)
(d) 
Figure 8.10
Identiﬁcation of unknown system from Example 8.12. System order is N = 3 (a-
b), and N = 5 (c-d). The error signal is presented on the left and the system coefﬁcients on the
right.
They are close to the true values of ﬁrst three system coefﬁcients (3, 2, and
−1), meaning that the LMS algorithm in this case follows the true values „in
mean”.
For the ﬁfth order adaptive system (N = 5), after about 100 iterations,
the error signal is almost 0. The adaptive system has identiﬁed the unknown
system. The ﬁnal coefﬁcient values in this case are
h0(200) = 2.9999
h1(200) = 1.9999
h2(200) = −1
h3(200) = 0.9999
h4(200) = 0.
The last coefﬁcient h4(200) = 0, as expected, since the identiﬁcation of a
fourth order system has been done by a ﬁfth order adaptive system.

Ljubiša Stankovi´c
Digital Signal Processing
463
0
50
100
150
200
-3
-2
-1
0
1
2
3
time index n
error signal e(n)
(a) 
0
50
100
150
200
-2
-1
0
1
time index n
coefficients hk(n)
0
1
2
3
4
(b) 
0
50
100
150
200
-3
-2
-1
0
1
2
3
time index n
error signal e(n)
(c) 
0
50
100
150
200
-2
-1
0
1
time index n
coefficients hk(n)
0
1
2
3
4
5
6
7
8
9
(d) 
Figure 8.11
Identiﬁcation of unknown system from Example 8.13. System order is N = 5
(a) and (b), and N = 10 (c) and (d). The error signal is presented on the left and the system
coefﬁcients on the right. System coefﬁcients hk(n) are labeled with k.
Example 8.13. Repeat the simulation from 8.12 for the case of unknown system
whose transfer function is
H(z) =
1 −11
8 z−1
1 + 1
4z−1 −15
64z−2 .
Use the step µ = 0.05 and the adaptive systems of order N = 5 and N = 10.
⋆In this case the unknown system is a system with an inﬁnite impulse
response. In theory, we should have an adaptive system with very large
(inﬁnite) order to identify this system. The identiﬁcation results with the
adaptive systems of order N = 5 and order N = 10 are shown in Figure 8.11.
We can see that the system with order N = 10 reduces the error to a small
value, achieving a good approximation of the unknown system.

464
Adaptive Systems
adaptive
system
e(n)
x(n)
y(n)
  d(n)
interference
only
signal of interest
with interference
output
+
-
Figure 8.12
Adaptive system for interference removal.
8.5.2
Noise Cancellation
Consider a setup where the signal of interest s(n) is acquired in a strong
noise environment ε(n). There are many such situations is real life (cockpit
and pilot voice in a plane, hands-free conversation in a car, speaking on
microphone in a loud noise environment are some of the examples). Assume
that in addition to the noisy signal we are in position to record (for example,
by a another microphone) one more signal, far from the desired signal. In
that case the other signal η(n) will contain noise only. This noise is highly
correlated with the noise ε(n) in the desired noisy signal. In such situations
an adaptive system, as in Figure 8.12, will be able to cancel out (signiﬁcantly
reduce) the noise in the desired signal.
The input signal in this case is the noise only signal η(n), while the
reference signal is s(n) + ε(n). Note that the noises η(n) and ε(n) are highly
correlated, since they have a common source. However the noises are not
the same, since they propagate through different pats, including possibility
of reﬂections (from the walls or other objects). The desired signal s(n) is
statistically independent from the noise. The adaptive system, in ideal case,
will try make the output signal y(n) as close as possible to the reference
signal s(n) + ε(n). Since its input η(n) is correlated to ε(n) it may achieve its
cancellation when ε(n) = HT(n)X(n). Then the error is equal to the desired
signal, e(n) = d(n) −y(n) = s(n) + (ε(n) −HT(n)X(n)) = s(n).
Example 8.14. Consider a simple setup when we will be in a position to follow the
system behavior in an intuitive way. Assume that the input signal η(n) is a
white zero-mean Gaussian noise with variance σ2η = 1. The desired signal is of
the form s(n) = cos(2πn/512) + 0.5sin(2πn/256 + π/3), with 0 ≤n ≤5000.
The noise at the position of signal s(n) is ε(n) = 0.5η(n) −0.7η(n −1). Find
the optimal coefﬁcients and then the error signal at the output of an LMS

Ljubiša Stankovi´c
Digital Signal Processing
465
based adaptive system from Figure 8.12. Comment the result with respect to
the LMS step µ.
⋆A second-order adaptive system with the input
X(n) = [η(n) η(n −1)]T
will be used. The adaptive system output is y(n) = HT(n)X(n) = h0(n)η(n) +
h1(n)η(n −1). The reference signal is d(n) = s(n)+ ε(n) The input signal
autocorrelation matrix and the cross-correlation vector of the input and
reference signal are
R=
- rηη(0)
rηη(1)
rηη(1)
rηη(0)
.
=
- 1
0
0
1
.
and
rdx = rεη
- 0.5rηη(0)
−0.7rηη(0)
.
=
- 0.5
−0.7
.
.
The optimal coefﬁcient values are
H∗= R−1rdx =
- 0.5
−0.7
.
,
producing the output y(n) = h∗
0η(n) + h∗
1η(n −1) = 0.5η(n) −0.7η(n −1), as
expected. The error signal is then e(n) = d(n) −y(n) = s(n).
Next the LMS algorithm is used in the adaptation, at each time instant
n, as H(n + 1) = H(n) + µe(n)X(n) with H(0) = 0. For large n the error
value will not vanish since, in an ideal case e(n) = s(n). Therefore the system
coefﬁcients H(n + 1) will ﬂuctuate with µe(n)X(n) ̸= 0. In means that, in
order to reduce these ﬂuctuations, the step µ should be much lower than its
bound µ < 2/λmax = 2 required by the convergence condition. The results
with µ = 0.01 and µ = 0.001 are presented in Figure 8.13.
Example 8.15. Consider a signal s(n) embedded in high noise ε(n). The signal
acquisition is done using two microphones. One close to the source of s(n)
and the other far from this source.
Signal s(n) is modelled as a nonstationary zero-mean Gaussian noise
with variance σ2s (n) = 3sin(πn/100)4. Signal ε(n) is a stationary zero-mean
white Gaussian noise with variance σ2ε = 300. The noise at the input to the
ﬁrst and the second microphone is modiﬁed by the system transfer functions
H1(z) = 1 + 0.5z−1 + 0.2z−2 −0.2z−3 + 0.1z−4
H2(z) = 1 −0.2z−1 + 0.1z−2,
respectively. Using an adaptive system of order N = 10 reduce (eliminate)
the noise from the signal recorded by the ﬁrst microphone. Experimentally
ﬁnd the value of step µ so that the signal-to-noise ratio is about 10dB.

466
Adaptive Systems
0
2000
4000
-3
-2
-1
0
1
2
3
time index n
error signal e(n)
0
2000
4000
-1
-0.5
0
0.5
time index n
coefficients hk(n)
0
2000
4000
-3
-2
-1
0
1
2
3
time index n
error signal e(n)
0
2000
4000
-1
-0.5
0
0.5
time index n
coefficients hk(n)
Figure 8.13
Simulation results for Example 8.14 – Adaptive system for noise cancelation.
System coefﬁcients are given in upper subplots. Lower subplots present error signal and target
signal (black line).
⋆The simulation results are presented in Figure 8.14. Presented are
the adaptive system coefﬁcients hk(n), error signal e(n) and the signal s(n)
for 0 ≤n ≤500. The step µ = 0.00005 is used. Note that the system cannot
produce the error signal equal to zero since error signal contains signal s(n).
Smaller value of step µ enables lower variations of the coefﬁcients and a
closer approach to the optimal values. In the staring iteration the noise ε(n)
is dominant in the error signal. As the coefﬁcients of the system approach to
their optimal value the error reduces and assumes the values closer to the
desired signal s(n). Experimenting with various step values we concluded
that µ = 0.00005 can achieve the required signal-to-noise ratio
8.5.3
Sinusoidal Disturbance Cancellation
In many application a desired signal is disturbed by a sinusoidal interfer-
ence of unknown frequency. A system for adaptive cancellation of such

Ljubiša Stankovi´c
Digital Signal Processing
467
0
50
100
150
200
250
300
350
400
450
500
-20
-10
0
10
20
time index n
error signal e(n)
0
50
100
150
200
250
300
350
400
450
500
-0.2
0
0.2
0.4
0.6
0.8
1
time index n
coefficients hk(n)
0
1
2
3
4
5
6
7
8
9
Figure 8.14
Simulation results for Example 8.15 – Adaptive system for noise cancelation.
System coefﬁcients are given in upper subplot. Lower subplot presents error signal (gray line)
and target signal (black line).
interference is presented in Figure 8.15. In this case the measured signal
contains a sinusoidal interference Acos(ω0n + ϕ), in addition to the desired
signal s(n). The adaptive system is set in such a way that the reference signal
d(n) is the measured signal x(n), while the input signal to the adaptive sys-
tem is a delayed version of the measured signal x(n −M). For a sufﬁciently
large delay M we may assume that the desired signal s(n) is not correlated,
i.e., that its autocorrelation function is rss(m) = 0 for |m| ≥M. The reference
signal d(n) will have two components. One component corresponding to
the desired signal s(n), which is not correlated with the input x(n −M).
The other component is the sinusoidal signal which is correlated for any
delay M. The adaptive system will (in an ideal case) be able to adjust its
parameters to remove correlated component (in this case the sinusoidal in-
terference). Then we will get e(n) = d(n) −y(n) = s(n).
In an ideal case the adaptive system should adjust its coefﬁcients to be-
have as an inverse notch ﬁlter at ω = ±ω0, i.e.,
''H(ejω)
'' =
''DFT[h∗
k]
'' = 1 for

468
Adaptive Systems
adaptive
system
z- M
e(n)
input
x(n)
y(n)
  d(n)
output
e(n)
+
-
Figure 8.15
Adaptive system for sinusoidal interference removal.
ω = ±ω0 and H(ejω) = 0 elsewhere. Such a system would produce y(n) ∼=
Acos(ω0n + ϕ) if the input signal is x(n) = s(n) + Acos(ω0n + ϕ), where
s(n) is a wide-band signal. Then d(n) −y(n) ∼= x(n) −Acos(ω0n + ϕ) =
s(n).
Example 8.16. Consider an adaptive system for the sinusoidal interference cancel-
lation, with input signal
x(n) = s(n) +
√
200cos
B
4π n
32
C
,
where s(n) is a stationary zero-mean Gaussian white noise with autocorre-
lation rss(m) = δ(m) + 0.25δ(m −1). Simulate the adaptive system of order
N = 32 to eliminate the sinusoidal interference. Use the delay M = 3 and step
µ = 0.00002.
⋆Results of the simulation are presented in Fig. 8.16. The input signal
x(n) is shown in Fig. 8.16(a), and the desired signal s(n) in Fig. 8.16(b). The
adaptive coefﬁcients are presented in Fig. 8.16(c). The output signal error e(n)
is given in Fig. 8.16(d). We can conclude that after about 100 iterations the
output signal is close to s(n), with a delay of M = 3 samples.
8.5.4
Signal Prediction
Adaptive system conﬁguration to predict one step ahead signal value is
presented in Figure 8.17. The input signal x(n) is used as the reference signal
d(n) = x(n). The aim is to obtain this value of signal using its past samples
x(n −1), x(n −2), ..., x(n −M). The input signal vector is of the form
X(n) =
⎡
⎢⎢⎢⎣
x(n −1)
x(n −2)
...
x(n −N)
⎤
⎥⎥⎥⎦

Ljubiša Stankovi´c
Digital Signal Processing
469
0
50
100
150
200
-10
-5
0
5
10
time index n
error signal e(n)
(d) 
0
50
100
150
200
-0.1
-0.05
0
0.05
0.1
time index n
(c) 
coefficients hk(n)
0
50
100
150
200
-20
-10
0
10
20
time index n
input signal x(n)
(a) 
0
50
100
150
200
-10
-5
0
5
10
time index n
signal s(n)
(b) 
-2
0
2
0
0.2
0.4
0.6
0.8
1
Fourier transfom of hk(200)
frequency ω
(e)
Figure 8.16
Simulation results for Example 8.16. Signal with sinusoidal interference (a), signal
without interference (b), system coefﬁcients (c), output signal (d), and Fourier transform of the
ﬁnal system coefﬁcients, hk(200), k = 0,1,2,...N −1, (e).
with y(n −1) = HT(n)X(n). The error signal is formed as
e(n) = d(n) −HT(n)X(n) = x(n) −y(n −1).
If the adaptive system is able to adjust its coefﬁcients so that the error is
small, with y(n −1) ≈d(n) = x(n) then its output will predict the next

470
Adaptive Systems
adaptive
system
e(n)
y(n-1)
z-1
y(n)
x(n) 
output
+
-
Figure 8.17
Adaptive prediction.
signal value
y(n) ≈x(n + 1).
Consider a signal described by
x(n) = a1x(n −1) + a2x(n −2) + ... + aMx(n −M) + ε(n)
where ε(n) is a zero-mean white noise with variance σ2
ε . We may expect that
the optimal coefﬁcients for one step ahead prediction should be
H∗= [a1 a2 aM 0 ... 0]T
for an adaptive system whose order is N > M with
y(n) = h∗
0x(n −1) + h∗
1x(n −2) + ... + h∗
N−1x(n −N).
The prediction error will depend on the ratio of the recursive part of signal
x(n) and random part ε(n). For large n the error value will not vanish since,
in an ideal case e(n) = ε(n). The system coefﬁcients H(n + 1) will ﬂuctuate
with µe(n)X(n) ̸= 0 causing so called excessive mean square error. In order
to reduce these ﬂuctuations (this kind of error), the step µ should be much
lower than its bound µ < 2/λmax = 2 required by the convergence condition.
The excessive means square error is proportional to the signal energy and
the algorithm step, EMSE = µEx/2.
Example 8.17. Consider a third order adaptive system for signal prediction. As-
sume that the signal x(n) is random signal with autocorrelation function
rxx(m) = σ2x(m)δ(m). Find the output signal, assuming that the adaptive
system has adjusted its coefﬁcients is such a way that they are equal to the
optimal ones.

Ljubiša Stankovi´c
Digital Signal Processing
471
⋆Samples of the input signal are uncorrelated. Autocorrelation matrix
of the input signal X(n) = [x(n −1) x(n −2) x(n −3)]T is
R(n) =
⎡
⎣
σ2x(n)
0
0
0
σ2x(n −1)
0
0
0
σ2x(n −2)
⎤
⎦.
Cross-correlation between the input and reference signal d(n) = x(n) is
rdx(n) =
⎡
⎣
E[x(n)x(n −1)]
E[x(n)x(n −2)]
E[x(n)x(n −3)]
⎤
⎦=
⎡
⎣
0
0
0
⎤
⎦
Optimal coefﬁcient values are H∗(n) = R−1(n)rdx(n) = 0. It means that the
output signal is zero.
Example 8.18. Assume that in the previous example the input signal is stationary
with autocorrelation rxx(m) = 2−|m|. Find the optimal coefﬁcient values and
the form of optimal predictor.
⋆Autocorrelation matrix of the input signal and its cross-correlation
vector with the reference signal are
R =
⎡
⎣
1
1
2
1
4
1
2
1
1
2
1
4
1
2
1
⎤
⎦
rdx =
⎡
⎣
E[x(n)x(n −1)]
E[x(n)x(n −2)]
E[x(n)x(n −3)]
⎤
⎦=
⎡
⎣
rxx(1)
rxx(2)
rxx(3)
⎤
⎦
⎡
⎣
1
21
41
8
⎤
⎦
The optimal coefﬁcient values are
H∗= R−1rdx =
⎡
⎣
1
1
2
1
4
1
2
1
1
2
1
4
1
2
1
⎤
⎦
−1 ⎡
⎣
1
21
41
8
⎤
⎦=
⎡
⎣
1
2
0
0
⎤
⎦
The output signal, predicting one step ahead the input signal value is
y(n) = ˆx(n + 1) = 1
2 x(n).
Example 8.19. Consider a signal
x(n) = −0.1x(n −1) + 0.72x(n −2) + ε(n)

472
Adaptive Systems
where ε(n) is a zero-mean white noise with variance σ2ε = 0.5. Find optimal
values of the system coefﬁcients for one step ahead prediction of a second
order adaptive system. Plot the adaptation coefﬁcients for the second order
LMS algorithm with µ = 0.1 and µ = 0.01. Calculate and plot average of the
prediction square error in 100 realizations in dB for both cases. What is the
convergence bound for µ.
Repeat the calculation for x(n) = 1
2 x(n −1) + ε(n) and the ﬁrst-order
adaptive system.
⋆For the optimal values of the adaptive prediction system we have to
ﬁnd autocorrelation matrix of the input signal vector, in this case of X(n) =
[x(n −1) x(n −2)]T. Signal x(n) is obtained as the output of a recursive
system whose input is ε(n) and the transfer function is
H(z) =
1
1 + 0.1z−1 −0.72z−2 =
9
17
1 + 0.9z−1 +
8
17
1 −0.8z−1 .
Its impulse response is
h(n) = [ 9
17(−0.9)n + 8
17(0.8)n]u(n).
Therefore the signal x(n) can be written as
x(n) = h(n) ∗ε(n).
The autocorrelation function of x(n) is
rxx(m) = E[x(n + m)x(n)]
= E[
∞
∑
k1=0
∞
∑
k2=0
ε(n + m −k1)h(k1)ε(n −k2)h(k2)]
= σ2
ε
∞
∑
k=0
h(k)h(k −m)
since rεε(m) = σ2ε δ(m). Thus, we have
rxx(0) = σ2
ε
∞
∑
k=0
h2(k) = 1.19,
rxx(1) = σ2
ε
∞
∑
k=0
h(k)h(k −1) = −0.425,
and rxx(2) = 0.8993. The optimal coefﬁcient values are
H∗=
- 1.19
−0.425
−0.425
1.19
.−1 -−0.425
0.8993
.
=
-−0.1
0.72
.

Ljubiša Stankovi´c
Digital Signal Processing
473
since
R(n)=E
@
X(n)XT(n)
A
= E
@
[x(n −1) x(n −2)]T [x(n −1) x(n −2)]
A
=
-rxx(0)
rxx(1)
rxx(1)
rxx(0)
.
and rdx(n) = E[d(n)X(n)] = [rxx(1) rxx(2)]T .
Adaptive system for prediction is implemented using the LMS al-
gorithm, H(n + 1) = H(n) + µe(n)X(n) with H(0) = 0. Input vector is
X(n) = [x(n −1) x(n −2)]T , while the error signal has the form e(n) =
d(n) −HT(n)X(n) with d(n) = x(n) and H(n) = [h0(n) h1(n]T. The results
obtained in each iteration (time index) are presented in Figure 8.18. The re-
sults are averaged over 500 independent realizations. The eigenvalues of ma-
trix R are λ0 = 1.6150 and λ1 = 0.7650. The bound for convergence "in mean"
is µ < 2/λmax = 1.2384. As expected, much lower values of step µ should be
used to reduce random ﬂuctuations of the coefﬁcients.
8.5.5
Adaptive Antenna Arrays
Antenna array (system) is a set of antennas distributed in space to produce
desired performance. Commonly the antennas are located along a line, with
equal consecutive distances, forming a uniform linear antenna array. Signals
are received by individual antennas with different delays, being dependent
on the antennas location. The received signals from each antenna are com-
bined to achieve the antenna array performance. The most common way
to combine signals from different antennas is to add up their appropriately
weighted values. This is done by a linear combinator, Figure 8.5. It produces
a weighted sum of signals obtained from individual antennas.
When the linear combinator changes its weighting coefﬁcients in time,
then it performs a task of an adaptive system. Various tasks can be required
from the adaptive combinator. One possible requirement is that the antenna
array ampliﬁes a signal from a speciﬁc direction (being a known direction
of desired signal). A task could be to ﬁnd the direction of signal arrivals
(DOA) and to track them in time if they change. In some cases, in addition
to the desired signal, the antenna system receives one or more undesired
interferences from different directions. The task of an adaptive system could
be to cancel out or to suppress the interferences.
Consider a linear uniform antenna array with individual antennas
being spaced at l. Assume that a direction of arrival of signal is deﬁned
by angle θ as well that the signal form at the ﬁrst antenna is
r0(t) = s(t)ejω0t,

474
Adaptive Systems
0
500
1000
-6
-4
-2
0
2
time index n
10loge2(n) [dB]
0
500
1000
-0.8
-0.6
-0.4
-0.2
0
0.2
time index n
h0(n),  h1(n)
0
500
1000
-6
-4
-2
0
2
time index n
10loge2(n) [dB]
0
500
1000
-0.8
-0.6
-0.4
-0.2
0
0.2
time index n
h0(n),  h1(n)
Figure 8.18
Coefﬁcients and error in the prediction setup of the second order adaptive LMS
algorithm for µ = 0.1 (left) and µ = 0.01 (right). The results are averaged over 500 realizations.
where s(t) is a narrowband signal and ω0 is its carrier frequency. Signal
arrives to other antennas with a delay. The front of incident wave has to
pass the path l cos(θ) to arrive at the second antenna. Since the propagation
speed is the speed of light c, time delay from the ﬁrst to the second antenna
is
td = l cos(θ)/c.
The same delay holds for each next antenna, since the antenna array is
uniform. The signal at (k + 1)th antenna is
rk(t) = s
*
t −kl cos(θ)
c
+
ejω0
B
t−k l cos(θ)
c
C
.

Ljubiša Stankovi´c
Digital Signal Processing
475
Since the signal s(t) is narrowband, meaning that its amplitude variations
are slow, we may write
s
*
t −kl cos(θ)
c
+
∼= s(t).
Including this fact, the signal at the (k + 1)th antenna assumes the form
rk(t) = s(t)ejω0te−j ω0
c kl cos(θ) = s(t)ejω0te−j2π l
λ kcos(θ),
where λ is wavelength of the propagating wave λ = 2πc/ω0.
We can conclude that the signals at different antennas differ in phase
only. It means that it is possible to deﬁne an appropriate linear combination
of these signals so that they may add up in phase for a given incident angle
θ, or to cancel out for some other incident angles θ.
The input signals to the linear combinator are demodulated by e−jω0t
and discretized to form
xk(n) = rk(t)e−jω0t'''
t=n∆t = s(t)e−j2π l
λ kcos(θ)'''
t=n∆t .
This system is presented in Figure 8.19. The output of linear combinator is
y(n) =
N−1
∑
k=0
hk(n)xk(n) = s(n∆t)
N−1
∑
k=0
hk(n)e−j2π l
λ kcos(θ)
= s(n∆t)
N−1
∑
k=0
hk(n)e−jωk = s(n∆t)FTk[hk(n)]|ω=2π l
λ cos(θ)
= s(n∆t)HT(n)a(ω)|ω=2π l
λ cos(θ)
where
HT(n) = [h0(n) h1(n)...hN−1(n)]
a(ω) = [1 e−jωk e−jω2k...e−jω(N−1)]T
ω = 2π l
λ cos(θ)
The output signal y(n) is equal to the input signal s(n∆t) multiplied by
the Fourier transform of the coefﬁcients hk(n), k = 0,1,..., N −1 at ω =
2π l
λ cos(θ).

476
Adaptive Systems
+
-
e(n)
y(n)
l 
antenna
array
adaptive
system
θ
incident
wave
+
...
...
...
x0(n)
x1(n)
xN-1 (n)
xref(n)
h0(n)
h1(n)
hN-1 (n)
Figure 8.19
Uniform antenna array with adaptive system for interference rejection.
Now we will consider an adaptive setup of this system with the aim
to cancel out input interference signals. Assume that several waves with
incident angles θ1, θ2,...,θP arrives to this antenna array. The input signal to
each antenna is then
xk(n) =
P
∑
p=1
sp(n∆t)e−j2π l
λ kcos(θp) =
P
∑
p=1
sp(n∆t)e−jωpk
with ωp = 2π l
λ cos(θp). The output of linear combinator is
y(n) =
N−1
∑
k=0
hk(n)xk(n) =
P
∑
p=1
sp(n∆t)
(
N−1
∑
k=0
hk(n)e−jωpk
)
.
It is a sum of the Fourier transforms of FTk[hk(n)] at the frequencies ωp =
2π l
λ cos(θp) multiplied with amplitudes sp(n∆t). If we want to cancel out
all input signals then the weighting coefﬁcients should be adjusted in such
a way that their Fourier transform is zero (notch ﬁlter) at the frequencies
corresponding to the directions of arrivals of the considered signals.
Consider now N antennas in the array and use an additional antenna
for the reference signal
d(n) =
P
∑
p=1
sp(n∆t)e−j2π l
λ N cos(θp).
The adaptive LMS based system is implemented using
H(n + 1) = H(n) + µe(n)X∗(n)

Ljubiša Stankovi´c
Digital Signal Processing
477
with H(0) = 0 (see Subsection 8.6.5). Input vector is
X(n) = [x0(n) x1(n)...xN−1(n)]T .
The reference signal is the output of reference antenna d(n) = xN(n). With
e(n) = d(n) −HT(n)X(n) and d(n) = xN(n) we can write
e(n) =
@
−HT(n) 1
A@
XT(n) xN(n)
AT
All system coefﬁcients, including d(n), are
[−H(n) 1] = −[h0(n) h1(n)...hN−1(n) −1]T .
They will approach the values such that their Fourier transform is a notch
ﬁlter form like function. Then all the input signals will be canceled out
and the error e(n) will be zero-valued (assuming that the order of system
is appropriate for the number of input signal from different directions). It
was assumed that the desired signal was not present (switched off) during
the adaptation process, otherwise it would be canceled out as well. When
the system ends adaptation we can then switch on out desired signal from
a direction that does not correspond to one of interferences. It will pass
through the system, while all interfering signals are canceled out.
This kind of system is simulated using an adaptive system of order
N = 10, with four interfering signals with the directions or arrival θ1 = 30◦,
θ2 = 75◦, θ3 = 90◦and θ4 = 120◦. Note that the ability to cancel out a number
of disturbances depends on the system order and the positions of the angles
of arrivals. With for example, 10 coefﬁcients we will not be able to achieve
an arbitrary number of arbitrary positioned zeros in its Fourier transform.
The antenna system gain is
A(θ) =
''''
y(n)
s(n∆t)
'''' =
'''HT(n)a(ω)|ω=2π l
λ cos(θ)
'''
=
'''FTk[hk(n)]|ω=2π l
λ cos(θ)
'''
or in decibels
a(θ) = 20log10 A(θ)
[dB].
It is calculated for angles 0◦≤θ ≤180◦and presented in Figure 8.20. The
antenna system is adjusted to cancel out the interference (gain of the system
is here below −25dB). Signals from other directions will pass unattenuated
through this system, with a gain of about 5dB. A radiation plot of this
system is presented in 8.21.

478
Adaptive Systems
0
15
30
45
60
75
90
105
120
135
150
165
180
-30
-25
-20
-15
-10
-5
0
5
10
incident angle [degrees]
antenna gain [dB]
Figure 8.20
Antenna system gain for various incident angles. Iterference incident angles are
marked with arrows.
θ=30o
θ=90o
θ=75o
θ=120o
antenna array
Figure 8.21
Radiation plot of the antenna system.
8.5.6
Acoustic Echo Cancellation
In this case the input to microphone is an acoustic signal. This is the desired
signal in adaptive system. In addition to this signal there are interference
signals coming from speakers. These signals come to the microphone over
direct path and one or more reﬂected paths. Adaptive system has the task

Ljubiša Stankovi´c
Digital Signal Processing
479
adaptive
system
speaker
microphone
voice
direct path
reflection
e(n)
y(n)
input 
x(n) 
 output
 e(n)
  d(n)
+
-
Figure 8.22
Adaptive system for acoustic echo cancelation.
to cancel out the inﬂuence of this interference. The system for adaptive
acoustic echo cancellation is presented in Figure 8.22. This kind of adaptive
systems is used in hands-free devices and in the systems for audio commu-
nication over internet.
Example 8.20. Consider a system as in Figure 8.22. Assume that the signal from
microphone is sampled with frequency fs = 11025 Hz. Speed of the acoustic
signal propagation is c = 330 m
s . Speaker is at the distance r0 = 27 cm from the
microphone, meaning that the direct component reaches microphone with
a delay of fsr0/c ≈9 samples. The system is in a room whose dimensions
are such that the reﬂected components passing paths longer than 3m can
be neglected. From this fact we can conclude that the maximal delay is
100 samples. The intensity of reﬂected components is inversely proportional
to the propagation path. With these assumptions the impulse response of
the system that transfers a signal x(n) from the speaker to the input in
microphone can be modelled as
hecho(n) =
⎧
⎨
⎩
1
for n = 9
wn
n
for 10 ≤n ≤100
0
for other values of n
where wn are uncorrelated zero-mean Gaussian variables of variance 1. Sig-
nal x(n) is modeled as a zero-mean Gaussian white noise with variance
σ2x = 25. The acoustic signal s(n) is modeled as nonstationary zero-mean
Gaussian random process with variance σ2s (n) = 3sin4 P nπ
250
Q
.
⋆Results of simulation of this system are presented in Figure 8.23.
An LMS adaptive system of order N = 100 is used with step µ = 0.00005.

480
Adaptive Systems
0
250
500
1500
1750
2000 3000
3250
3500
-10
0
10
time index n
voice
signal s(n)
0
250
500
1500
1750
2000 3000
3250
3500
-10
0
10
time index n
microphone
signal d(n)
0
250
500
1500
1750
2000 3000
3250
3500
-10
0
10
time index n
output
signal e(n)
Figure 8.23
Acoustic echo cancelation example.
Acoustic signal is presented. Signal at the output from microphone that
contains components from the speaker is presented as well. The output
signal in this case is equal to the error signal. These signals are shown for
0 ≤n < 500, for 1500 ≤n < 2000, and for 3000 ≤n < 3500. The output signal,
after the adaptation process, clearly contains acoustic signal, what is not case
at the microphone input. The ratio of the acoustic signal and the total echo
of signals at the microphone input is −14dB, while in the error signal the
ratio of the acoustic signal and the remaining echo signals is 12dB. Total
improvement is about 26dB.
System behavior as a function of the step µ is presented in Figure 8.24.
Rejection of the echo signals in dB is used as a parameter for the system
description. We can see that smaller step µ produces higher echo rejection,
but with a slower convergence of the algorithm.

Ljubiša Stankovi´c
Digital Signal Processing
481
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
5
10
15
20
25
30
35
time [s]
echo rejection ratio [dB]
µ= 0.00050
µ= 0.00020
µ= 0.00005
µ= 0.00001
Figure 8.24
Echo rejection ratio for various steps µ.
8.6
VARIATIONS ON THE LMS ALGORITHM
8.6.1
Sign LMS
The LMS algorithm is numerically efﬁcient. In each iteration only N + 1
multiplications and N additions should be performed. The step parameter
µ can be assumed so that it does not require multiplications, but a shift only
(in decimal system its form 10−b requires shift only, while in the binary sys-
tem 2−b is a shift operation, where b is an integer). In this way the number
of multiplications will be reduced to N in each iteration. Multiplications (as
computationaly more demanding arithmetic operation than the additions)
may be completely avoided if we use the error value just to ﬁnd the direction
of change of the adaptive coefﬁcients. If the error is replaced by its sign only,
then we have a sign LMS algorithm,
H(n + 1) = H(n) + µsign(e(n))X(n).
This form, with appropriate µ, reduces the number of arithmetic operations
to one addition for each coefﬁcient in each iteration.
Another variant of the LMS with reduced number of multiplications is
to apply the sign operator to the input vector X(n). Then we get the signed-
regressor LMS form
H(n + 1) = H(n) + µe(n)sign(X(n)).

482
Adaptive Systems
The number of arithmetic operations is the same as in the sign LMS algo-
rithm.
For an arbitrary step µ it is still possible to avoid multiplications by
applying sign functions to both the error and the signal vector X(n). The
sign-sign LMS is deﬁned by
H(n + 1) = H(n) + µsign(e(n))sign(X(n)).
Note that the change of system coefﬁcients in each iteration is ±µ. It pre-
vents the system from achieving stationary state (coefﬁcients oscillate). To
avoid this effect it is possible to deﬁne a sign function with a "dead zone" as
signD(α) = sign(α −D) + sign(α + D)
2
.
Function signD(α) is equal to −1 for α < −D, and 1 for α > D, while it is 0
for |α| < D. Value of this function at the discontinuity points is 1/2.
Example 8.21. Consider adaptive system described in 8.12 (page 461). Simulate the
system using signed error LMS, signed regressor LMS, and sign-sign LMS.
Use the adaptive system of order N = 5 with step µ = 0.05.
⋆Simulation results are presented in Figure 8.25. We can conclude that
the convergence is slower when the sign is applied to the error function (in
both cases) than in the case when the sign is applied to the signal vector. If the
error signal is kept in its original form, when the error approaches to zero the
system coefﬁcients approach to their stationary values, without oscillations.
This is not the case for sign error LMS form.
8.6.2
Block LMS
The block LMS differs from the standard in the sense that the coefﬁcients
are not modiﬁed at each instant n, but after each K instants. Time index is of
the form n = pK + m, where K is the block length, p is the block index,
and m is the index of a sample within a block 0 ≤m < K. For adaptive
systems with large N the computation time of standard LMS algorithm can
be reduced by using the block LMS. They can be implemented with the FFT
algorithms (fast block LMS algorithm). Coefﬁcients are adapted in such a
way that all coefﬁcient modiﬁcations within a block are added up and the
ﬁnal coefﬁcients modiﬁcation is done according to
H(p + 1) = H(p) + µ
K−1
∑
m=0
e(pK + m)X(pK + m).

Ljubiša Stankovi´c
Digital Signal Processing
483
0
100
200
-5
0
5
(a) 
 error signals e(n) 
Signed error LMS
0
100
200
-2
0
2
4
0
1
2
3
4
time index n
(d) 
coefficients hk(n)
0
100
200
-5
0
5
(b) 
Signed regressor LMS
0
100
200
-2
0
2
4
0
1
2
3
4
time index n
(e) 
0
100
200
-5
0
5
(c) 
Sign-sign LMS
0
100
200
-2
0
2
4
0
1
2
3
4
time index n
(f) 
Figure 8.25
Simulation results for Example 8.21.
Example 8.22. Consider the system from Example 8.15 (page 465). Simulations will
be repeated with the block LMS using the block size K = 50.
⋆The results of simulation are presented in Figure 8.26. Note that
the coefﬁcients change at the end of each block. Deviations of the adaptive
system coefﬁcients are lower than in the LMS algorithm. The input signal-to-
noise ratio is −25dB, while this ratio at the output is 11dB.
8.6.3
Normalized LMS Algorithm
Deviations of the adaptive system coefﬁcients in stationary state (when the
error does not vanish) depend on the algorithm step and signal energy.
The idea in normalized LMS algorithms is to reduce the step µ in order
to compensate the inﬂuence of signal energy. The step in normalized LMS
algorithm is
µ =
µnorm
1 + XT(n)X(n)
where the normalization is done with the input signal energy ∥X(n)∥2
2 =
XT(n)X(n) within the considered N signal samples. Value 1 is added in
denominator to avoid the problem with small signal energy. The adaptation

484
Adaptive Systems
0
50
100
150
200
250
300
350
400
450
500
-20
-10
0
10
20
time index n
error signal e(n)
0
50
100
150
200
250
300
350
400
450
500
-0.2
0
0.2
0.4
0.6
0.8
1
time index n
coefficients hk(n)
0
1
2
3
4
5
6
7
8
9
Figure 8.26
Block LMS simulation for Example 8.22.
relation is
H(n + 1) = H(n) +
µnorm
1 + XT(n)X(n) e(n)X(n)
The convergence bound for the step is
µnorm < 2
In applications, the value µnorm = 0.5 is commonly used.
A generalization of the normalized LMS is
µ =
µnorm
α + XT(n)X(n)
(8.20)
where α is a small constant, α ≪XT(n)X(n).

Ljubiša Stankovi´c
Digital Signal Processing
485
8.6.4
LMS with Variable Step Size
The basic idea of a variable step size (VSS) variant of the LMS algorithm is to
change the step µ during iterations. The step should not be too large to cause
divergence, but also not too small so that it cannot detect possible changes
in the coefﬁcients. Various variable step size algorithms have been derived
in literature, using the error signal, the input signal, the reference signal, and
the output signal in the considered and previous instants. Previous values
of the step µ are also used in the algorithms.
The normalized LMS, in the case that the signal energy changes during
the considered interval, may be considered as a variable step size LMS. A
form of the normalized LMS was used to deﬁne an interesting and efﬁcient
variable step size algorithm called generalized normalized gradient descent
(GNGD) algorithm, with appropriate adaptation of coefﬁcient α in (8.20).
It takes into account most of above-mentioned signals. Adaptation formula
for coefﬁcient α is derived in the form
α(n + 1) = α(n) −ρµe(n)e(n −1)XT(n)X(n −1)
(e(n −1) + XT(n)X(n))2
(8.21)
where e(n) = d(n) −HT(n)X(n) is the error signal and ρ is a constant ρ < 1.
A simple form of the variable step size LMS can be obtained using
the standard LMS calculated with two (or several) step values µ. One µmin
should be sufﬁciently small that the coefﬁcient deviations in the steady state
are small and the other µmax sufﬁciently large that the convergence is fast
when the change of the coefﬁcients is detected. The crucial decision in this
simple algorithm is when to use the LMS with the small step and when the
LMS with the large step. One of possible criteria is based on the energy of
error signal. If the error increases the algorithm will switch to the larger step,
after few instants, when the energy of error exceeds the threshold α.
The transition period in this case could be much shorter if the variable
step size LMS based on the weighting coefﬁcients bias-variance trade-off is
used. In this algorithm the difference between the coefﬁcients is compared
with the expected standard deviation of the coefﬁcients (with a constant κ)
for considered steps µ,
|hk(n,µmin) −hk(n,µmax)| ≷κ(σµmin + σµmax).
If the difference is small (within conﬁdence interval with few (σµmin +
σµmax)) then the system is assumed to be in the stationary state and small
step µmin should be used. Otherwise the system is in a transition and the
large step µmax should be used. Standard deviations of the coefﬁcients can
be calculated based on the input signal energy and the used steps µ.

486
Adaptive Systems
Example 8.23. Consider the system from Example 8.15 (page 465). The simulation
will be repeated with the variable step size LMS. In this case it has been
assumed that at the instant n = 200 transfer function H1(z) changes to
H1(z)|for n≥200 = 1 + z−1 + 0.2z−2 + 0.75z−3 −0.1z−4.
Consider two ways of the step µ changes. In the ﬁrst case use
µ(n) = αµmin + Ee(n)µmax
α + Ee(n)
where µmin = 0.00005, µmax = 0.0005 and α = 25, while Ee(n) is an average
energy (power) of the signal in previous instants K = 50
Ee(n) = 1
K
n
∑
k=n−K+1
e2(n)
In the second case use only two steps µmin and µmax the switching criterion
Ee(n) ≷α.
⋆The result of simulation are presented in Figure 8.27. At the begin-
ning, the algorithm uses maximal possible step size µmax. Then the step de-
creases. At n = 200 there is an abrupt change in the considered system and
the adaptive system adjusts its step to the new circumstances.
The results using the second way of step size change, using only
two steps µmin and µmax, are presented in Figure 8.28. On the coefﬁcients
plot, a gray shade indicates the region where the system uses larger step
µ(n) = µmax. Within the remaining time intervals the lower value of step
µ(n) = µmin is used.
Example 8.24. A system is deﬁned by
y(n) = x(n) + 0.8x(n −1)
where x(n) is a zero-mean Gaussian random signal with variance σ2x = 0.6.
Using the constant step LMS with µ = 1 and µ = 0.1 identify the system.
Compare the identiﬁcation result with the normalized LMS (8.20) using
adaptive α deﬁned by the generalized normalized gradient descent (GNGD)
algorithm, (8.21).
⋆Result of the identiﬁcation in the form of square error 10log |e(n)|2
[dB], averaged over 100 realizations, is presented in Figure 8.29. We can
clearly see the difference in the rate of convergence. For the constant step
LMS with µ = 1 we already see an increase in the error due to large step,
meaning that further step increase could lead to algorithm instability, since
2/Tr[R] = 2/(2σ2x) = 1.6667..

Ljubiša Stankovi´c
Digital Signal Processing
487
0
50
100
150
200
250
300
350
400
450
500
-20
-10
0
10
20
time index n
e(n) and s(n)
0
50
100
150
200
250
300
350
400
450
500
0
0.5
1
time index n
coefficients hk(n)
0
0
1
1
2
2
3
3
4
4
5
5
0
50
100
150
200
250
300
350
400
450
500
0
1
2
3
4
5
x 10
-4
time index n
µmin
µmax
time varying step µ(n)
Figure 8.27
LMS algorithm with variable step (Example 8.23, ﬁrst case).
8.6.5
Complex LMS
When the input signal x(n) (or/and the coefﬁcients of an adaptive system
h(n), or its reference signal d(n)) are complex-valued then the complex
LMS algorithm should be used. In this case the square absolute value is
minimized. The error function is
ε = |e(n)|2 = e(n)e∗(n)
where (·)∗denoted complex-conjugate value.

488
Adaptive Systems
0
50
100
150
200
250
300
350
400
450
500
-20
-10
0
10
20
time index n
e(n) and s(n)
0
50
100
150
200
250
300
350
400
450
500
0
0.5
1
time index n
coefficients hk(n)
0
0
1
1
2
2
3
3
4
4
5
5
Figure 8.28
LMS algorithm with variable step (Example 8.23, second case).
0
100
200
300
400
500
600
700
800
900
1000
-300
-250
-200
-150
-100
-50
0
1
2
3
time index n
average error 10loge2(n) [dB]
1
2
3
1
2
3
1 Adaptive  GNGD
2 LMS with  µ=1
3 LMS with  µ=0.1
Figure 8.29
Averaged square error in dB for the constant LMS with µ = 0.1 and µ = 1 and the
variable step size generalized normalized gradient descent (GNGD) algorithm.

Ljubiša Stankovi´c
Digital Signal Processing
489
It is easy to show that in this case the adaptation formula reads
H(n + 1) = H(n) + µe(n)X∗(n).
In the implementation of the complex LMS a special attention should be
paid to the transpose operation of the complex-valued matrices and vectors.
There are two ways of transposing the complex-valued matrix or vector.
The standard transpose only XT(n) and the Hermitian transpose XH(n) =
P
XT(n)
Q∗. In the case of Hermitian transpose, in addition to the standard
transpose, each element is conjugated as well. In the complex LMS the
output signal is calculated using
y(n) = HT(n)X(n) = XT(n)H(n)
where (·)T is standard transpose.
It is important to note that the complex LMS is used in adaptive
antenna arrays in subsection 8.5.5.
8.7
RLS ALGORITHM
The main drawback of the LMS algorithm is in stochastic approximation
of the expected value of error function gradient by its instantaneous value
e(n)X(n). This is the reason why, in some applications, there is a need for
better gradient approximation, resulting in better algorithm convergence.
Of course, this requirement will lead to the increased computational com-
plexity. One such algorithm is the recursive least square (RLS) algorithm.
The basic idea is in better gradient approximation, keeping the number of
arithmetic operation as low as possible.
The error
e(i|n) = d(i) −y(i|n) = d(i) −XT(i)H(n)
is deﬁned as an error that would be obtained in the ith instant if we would
use the coefﬁcients from the nth instant. At the considered instant n we
have n of such errors e(i|n), i = 1,2,...,n. The total error can be calculated
as a sum of squared values of these errors. It also important to include a
forgetting factor which will weight the most recent errors with higher values
that the older errors. An obvious way to deﬁne such an error function is
e(n) =
n
∑
i=1
λn−ie2(i|n)

490
Adaptive Systems
where λ is the forgetting factor, a positive number smaller than 1. In practice
values 0.95 < λ < 0.995 are used.
Now we can ﬁnd the adaptive system coefﬁcients H(n) producing
minimal error function e(n). They follow from
∂e(n)
∂H(n) = −2
n
∑
i=1
λn−ie(i|n)X(i) = 0.
By replacing e(i|n) we get
n
∑
i=1
λn−i(d(i) −XT(i)H(n))X(i) = 0
or
n
∑
i=1
λn−id(i)X(i) =
n
∑
i=1
λn−iX(i)XT(i)H(n)
˜rdx(n) = ˜R(n)H(n).
This solution is similar as in the optimal ﬁlter case. The difference is that
the cross-correlation vector ˜rdx(n) and the autocorrelation matrix ˜R(n) are
obtained by a weighted averaging
˜rdx(n) =
n
∑
i=1
λn−id(i)X(i)
˜R(n) =
n
∑
i=1
λn−iX(i)XT(i).
The coefﬁcient values are
H(n) = ˜R−1(n)˜rdx(n).
Relation between the coefﬁcients at instant n with the coefﬁcients at instant
n −1 will be obtained from
H(n) = ˜R−1(n)˜rdx(n)
H(n −1) = ˜R−1(n −1)˜rdx(n −1).
In order to ﬁnd the relation between H(n) and H(n −1) we have to ﬁnd a
relation between ˜R−1(n) and ˜R−1(n −1) and between ˜rdx(n) and ˜rdx(n −1).

Ljubiša Stankovi´c
Digital Signal Processing
491
By deﬁnition
˜R(n) =
n
∑
i=1
λn−iX(i)XT(i)
= λ
n−1
∑
i=1
λ(n−1)−iX(i)XT(i) + X(n)XT(n) = λ ˜R(n −1) + X(n)XT(n).
The inverse matrix ˜R−1(n) relation is need for a recursion. Using the matrix
inversion formula for
A = B + abT,
where A and B are square matrices of order N, and a and b are vector
columns with N elements, we have
A−1 = B−1 −B−1a(1 + bTB−1a)−1bTB−1.
Note that (1 + bTB−1a)−1 is a scalar. Applying this formula on ˜R(n) =
λ ˜R(n −1) + X(n)XT(n) we get
˜R−1(n) = 1
λ
˜R−1(n −1)
−1
λ
˜R−1(n −1)X(n)
*
1 + XT(n) 1
λ
˜R−1(n −1)X(n)
+−1
XT(n) 1
λ
˜R−1(n −1).
Using the notation
µ(n) = XT(n) ˜R−1(n −1)X(n)
it follows
˜R−1(n) = 1
λ
˜R−1(n −1) −
1
λ(λ + µ(n))
˜R−1(n −1)X(n)XT(n) ˜R−1(n −1).
Denoting the inverse matrix ˜R−1(n) by C(n)
C(n) = ˜R−1(n)
and introducing
g(n) = C(n −1)X(n)
λ + µ(n)

492
Adaptive Systems
we get
µ(n) = XT(n)C(n −1)X(n)
C(n) = 1
λC(n −1) −1
λg(n)XT(n)C(n −1).
The relation between vectors ˜rdx(n) and ˜rdx(n −1) is obtained from
˜rdx(n) =
n
∑
i=1
λn−id(i)X(i)
= λ
n−1
∑
i=1
λ(n−1)−id(i)X(i) + X(n)d(n) = λ˜rdx(n −1) + X(n)d(n).
Now we can write
H(n) = ˜R−1(n)˜rdx(n) = C(n)˜rdx(n) =
= 1
λ
B
C(n −1) −g(n)XT(n)C(n −1)
C
(λ˜rdx(n −1) + X(n)d(n)) =
= C(n −1)˜rdx(n −1) −g(n)XT(n)C(n −1)˜rdx(n −1)+
+ 1
λC(n −1)X(n)d(n) −1
λg(n)XT(n)C(n −1)X(n)d(n)
or
H(n) = H(n −1) −g(n)XT(n)H(n −1)
+ 1
λg(n)(λ + µ(n))d(n) −1
λg(n)µ(n)d(n)
= H(n −1) −g(n)XT(n)H(n −1) + g(n)d(n)
= H(n −1) + g(n)
B
d(n) −XT(n)H(n −1)
C
.
Finally the RLS formula is obtained as
H(n) = H(n −1) + g(n)e(n|n −1).
In the initial iteration H(0) = 0 is used, while for the initial matrix C(0) =
R−1(0) a matrix δI is used, where I is an identity matrix and δ ≫1.
Example 8.25. Consider the system from Example 8.15 (page 465). The simulations
will be repeated using the RLS with λ = 0.99.
⋆Results obtained using the RLS algorithm are presented in Figure
8.30. The convergence is faster than in the case of any LMS algorithm variant
in the previous examples. In about 10 iterations the coefﬁcients achieve their
exact values. In the stationary state the variation of coefﬁcients is small.

Ljubiša Stankovi´c
Digital Signal Processing
493
0
20
40
60
80
100
120
140
160
180
200
-20
-10
0
10
20
time index n
error signal e(n)
0
20
40
60
80
100
120
140
160
180
200
0
0.5
1
time index n
coefficients hk(n)
0
1
2
3
4
5
6
7
8
9
Figure 8.30
RLS algorithm example.
8.8
ADAPTIVE RECURSIVE SYSTEMS
For recursive systems (with inﬁnite impulse response) value of the output
signal at the nth instant depends on the input signal at the nth and previous
N −1 instants, x(n), x(n −1), x(n −2), ...,x(n −N + 1). Output signal
depends also on the previous output signal values y(n −1), y(n −2),...,
y(n −L),
y(n) =
N−1
∑
k=0
ak(n)x(n −k) +
L
∑
k=1
bk(n)y(n −k)
The vector notations
X(n) =
D
x(n)
x(n −1)
...
x(n −N + 1)ET
Y(n) =
D
y(n −1)
y(n −2)
...
y(n −L)
ET

494
Adaptive Systems
A(n) =
D
a0(n)
a1(n)
...
aN−1(n)ET
B(n) =
D
b1(n)
b2(n)
...
bL(n)ET
will be used along with their generalized forms
U(n) =
-
X(n)
Y(n)
.
W(n) =
-
A(n)
B(n)
.
.
The output signal can now be written as
y(n) = WT(n)U(n).
An adaptive system should iterate coefﬁcients using
W(n + 1) = W(n) −µ
2 grad(e2(n)),
where the error signal is
e(n) = d(n) −y(n).
The gradient vector of the error function can be written as
grad(e2(n)) = −2e(n)G(n),
where G(n) denotes a vector whose elements are
G(n) =
@
∂y(n)
∂a0
∂y(n)
∂a1
...
∂y(n)
∂aN−1
∂y(n)
∂b1
...
∂y(n)
∂bL
A
.
The adaptation rule is now
W(n + 1) = W(n) + µe(n)G(n).
Derivatives of the output signal with respect to al and bl are
αl(n) = ∂y(n)
∂al
= x(n −l) +
L
∑
k=1
bk(n)αl(n −k)
βl(n) = ∂y(n)
∂bl
= y(n −l) +
L
∑
k=1
bk(n)βl(n −k).

Ljubiša Stankovi´c
Digital Signal Processing
495
In a vector notation
G(n) = U(n) +
L
∑
k=1
bk(n)G(n −k).
Generalization of this algorithm is obtained if different steps µ are
used for different coordinates of the error vector. Then instead of step µ
a diagonal matrix of steps is used
M = diag(µ1,µ2,...,µN+L).
Special attention in the adaptive recursive systems has to be paid to
the system stability. It requires additional constraints on coefﬁcients bk(n).
Example 8.26. Consider identiﬁcation of system from Example 8.13 (page 462).
Here this system will be identiﬁed by using an adaptive recursive system
with N = 2 and L = 2. Step µ for the coefﬁcients in the numerator of the
transfer function is 0.025 while its value is 0.005 for the coefﬁcients in de-
nominator. Other parameters are the same as in Example 8.13.
⋆The simulation results are presented in Figure 8.31.
8.9
FROM THE LMS ALGORITHM TO THE KALMAN FILTERS
Consider the problem of unknown system identiﬁcation, where the
unknown system has the same input signal x(n) as the adaptive system,
and the output of the unknown system is used as a reference signal d(n) =
XT(n)H∗+ ν(n) in the adaptive system. Here ν(n) denotes zero-mean Gaus-
sian measurement noise, with variance σ2
ν, and it is assumed that it is not
correlated with other variables. It has been shown that the cost function
JMSE(n) = E
D
e2(n)
E
can be used to deﬁne the deviation of the error signal
from the ideal case. Minimization of this function provides the optimal sys-
tem parameters in form of the Wiener optimal ﬁlter. It is also proved that
LMS algorithm converges "in mean" toward the optimal system coefﬁcient
values H∗. In the unknown system identiﬁcation framework, the optimal
coefﬁcient values are equal to the coefﬁcients of the uknown system. In each
_____________________________________________________
This Section presents analysis from: D. P. Mandic, S. Kanna and A. G. Constantinides, "On
the Intrinsic Relationship Between the Least Mean Square (LMS) and Kalman Filters", IEEE
Signal Processing Magazine, preprint, Nov. 2015. Adapted for this book by M. Brajovi´c.

496
Adaptive Systems
0
20
40
60
80
100
120
140
160
180
200
-3
-2
-1
0
1
2
3
time index n
error signal e(n)
0
20
40
60
80
100
120
140
160
180
200
-1.5
-1
-0.5
0
0.5
1
1.5
time index n
coefficients ak(n) and bk(n)
a0(n)
a1(n)
b1(n)
b2(n)
Figure 8.31
Identiﬁcation of an unknown system (from Example 8.26) using the adaptive
recursive system.
time instant the adaptive system coefﬁcients are changed following the rule
H(n + 1) = H(n) + µ(n)e(n)X(n).
(8.22)
by changing previous values of system coefﬁcients in the direction of input
signal vector X(n). Since the LMS algorithm employs stochastic gradient
descent to minimize the cost function JMSE, it performs locally optimal
steps, but not the globally optimal shortest path to the solution, which
especially slows the convergence of algorithm in the case of correlated data.
The step µ(n) which can be in general time-dependent (class of variable
step LMS algorithms) controls the magnitude of adaptation steps, but not
the direction. In order to be able to follow the shortest path to the optimal
solution, i.e. to control the direction among the amplitude of adaptation
steps, the scalar step size µ(n) can be replaced by a positive deﬁnite matrix
G(n), which introduces more degrees of freedom in adaptation steps. This

Ljubiša Stankovi´c
Digital Signal Processing
497
is the ﬁrst step towards Kalman ﬁlters, and (8.22) now becomes
H(n + 1) = H(n) + G(n)e(n)X(n) = H(n) + g(n)e(n).
(8.23)
Previous recursion is also known as generalized LMS algorithm. Since
the unknown system identiﬁcation framework is considered, instead of er-
ror e(n) = d(n) −y(n) = d(n) −XT(n)H(n) the weight error vector deﬁned
as the deviation of adaptive coefﬁcients H(n) from the optimal coefﬁcients
H∗
ˆH(n) = H∗−H(n)
(8.24)
can be introduced, and based on it, we can deﬁne a measure of how closely
adaptive system coefﬁcients H(n) approach the optimal solution H∗. This
measure is the mean square deviation (MSD) and it is given with
JMSD(n) = E
MWW ˆH(n)
WW2N
(8.25)
Note that it is assumed that the unknown system is deterministic and non-
stationary. Since the weight error vector can be related with the system
output error e(n) with:
e(n) = XT(n)H∗+ ν(n) −XT(n)H(n) = XT(n) ˆH(n) + ν(n),
(8.26)
a relation between JMSE and JMSD can be found indicating that the min-
imization of MSD also corresponds to the minimization of MSE. For the
simplicity of derivation we will assume that X(n) is deterministic which is
a common assumption in Kalman ﬁltering literature, although it is usually
treated as a zero-mean process with autocorrelation matrix R in the con-
text of adaptive systems. If we introduce the weight error covariance matrix
P(n) =E
R ˆH(n) ˆHT(n)
S
, in order to perform the minimization of JMSD, start-
ing from (8.23) a recursive relation for the matrix P(n) is established
H∗−H(n + 1) = H∗−H(n) −G(n)X(n)(XT(n) ˆH(n) + ν(n))
ˆH(n + 1) = ˆH(n) −g(n)XT(n) ˆH(n) −g(n)ν(n)
ˆH(n + 1) ˆHT(n + 1) =
B
ˆH(n) −g(n)XT(n) ˆH(n) −g(n)ν(n)
C
B
ˆH(n) −g(n)XT(n) ˆH(n) −g(n)ν(n)
CT
P(n + 1)=P(n)−
B
P(n)X(n)gT(n) + g(n)XT(n)P(n)
C
+ g(n)gT(n)
B
XT(n)P(n)X(n) + ν(n)
C
.

498
Adaptive Systems
By taking expectations of both sides, previously multiplied with their
transposes, and having in mind that tr
R
P(n)X(n)gT(n)
S = gT(n)P(n)X(n)
this leads us to the MSD recursion of the form
JMSD(n + 1) = JMSD(n) −2gT(n)P(n)X(n) + ∥g(n)∥2 (XT(n)P(n)X(n) + σ2
ν).
The optimal learning gain vector g(n) which provides the control over
both direction and amplitude of adaptation steps in (8.23) is obtained by
solving ∂JMSD(n + 1)/∂g(n) = 0 as
g(n) = G(n)e(n) =
P(n)X(n)
XT(n)P(n)X(n) + σ2ν
,
(8.27)
which is known as the Kalman gain. Besides the calculation of (8.27), the
Kalman ﬁlter which estimates the optimal time-invariant and deterministic
coefﬁcients for each time instant also includes the coefﬁcients adjustment
H(n + 1) = H(n) + g(n)(d(n) −XT(n)H(n)),
as well as the weight error covariance matrix update
P(n + 1) = P(n) −g(n)XT(n)P(n).
(8.28)
Note that previous algorithm steps for σ2
ν = 1 can be related with the
RLS algorithm equations.
A generalization of the previous approach assumes time-varying and
stochastic weight vector H∗(n)
H∗(n + 1) = F(n)H∗(n) + q(n),
(8.29)
d(n) = XT(n)H∗(n) + ν(n)
(8.30)
with q(n) being a zero-mean Gaussian stochastic process with covariance
matrix Q = E
R
q(n)qT(n)
S
, and F(n) being a known matrix which describes
the system changes over time (state-transition matrix). It is assumed that the
measurement noise ν(n) is also uncorrelated with q(n). In the framework of
the general Kalman ﬁlter, the coefﬁcient vector is updated using the current
state estimate denoted with H(n|n), while the prediction of its next state
is denoted with H(n + 1|n). The prediction step is needed for tracking the
time-varying error surface. The coefﬁcients are updated by
H(n|n) = H(n|n −1) + g(n)(d(n) −XT(n)H(n|n))
(8.31)

Ljubiša Stankovi´c
Digital Signal Processing
499
while the coefﬁcients prediction is obtained with
H(n + 1|n) = F(n)H(n|n).
(8.32)
Note that the same deﬁnition of the weight error vector ˆH(n|n) =
H∗−H(n|n) holds, as well as for weight error covariance matrix
P(n|n) =E
M
ˆH(n|n) ˆHT(n|n)
N
.
The weight error covariance matrix is updated in the same manner as for
the time-invariant deterministic case
P(n|n) = P(n|n −1) −g(n)XT(n)P(n|n −1),
(8.33)
with the respect to the new index notation. The general Kalman ﬁlter also
includes the prediction step of weight error matrix which easily follows
from its deﬁnition
P(n + 1|n) =E
M
ˆH(n + 1|n) ˆHT(n + 1|n)
N
= F(n)P(n|n)FT(n) + Q. (8.34)
Similarly to the time-invariant deterministic case, the Kalman gain which
minimizes MSD is obtained in the following form
g(n) = G(n)e(n) =
P(n|n −1)X(n)
XT(n)P(n|n −1)X(n) + σ2ν
.
(8.35)
Example 8.27. Consider the problem of identiﬁcation of unknown time-invariant
deterministic system with two coefﬁcients h0 = 3 and h1 = 4 using the stan-
dard LMS algorithm and Kalman ﬁlter (for stationary system identiﬁcation),
with N = 2. The input signal is colored noise x(n) = 5w(n) + 3.4w(n −1) +
w(n −2), where w(n) is a zero mean white noise with variance σ2w = 1. The
step µ = 0.0005 is used for the LMS algorithm. Show the convergence paths
on the MSE contour plot. After how many iterations the Kalman ﬁlter ap-
proaches the optimal solution?
⋆The convergence paths on the MSE contour are shown in Fig. 8.32.
Numbers on the Kalman ﬁlter path indicate that the optimal solution is
obtained after only two iterations.

500
Adaptive Systems
0
1 (h0
*,h1
*)
h0
h1
ε(h0,h1)
 
 
0
1 (h0
*,h1
*)
0
1
2
3
4
5
6
0
1
2
3
4
5
6
LMS convergence path
Kalman filter convergence path
Figure 8.32
Convergence paths of the LMS algorithm and Kalman ﬁlter in the problem of
identiﬁcation of unknown time-invariant deterministic system. Contour lines are the projec-
tions of the MSE surface on the coefﬁcients plane.
8.10
NEURAL NETWORKS
Artiﬁcial neural networks, or just neural networks, represent a simpliﬁed
mathematical model of biological systems. In such systems a distributed
parallel data processing is performed, in contrast to the common engineer-
ing systems that are designed for a sequential data processing. Common
computer systems are based on well deﬁned algorithms that are executed
in a sequential order, while in the neural networks a learning period is re-
quired to achieve their satisfactory response to the input data. Correspon-
dence with biological systems, that also require learning, is evident. Com-
monly, a trained network continues to learn and adapt to the new situations
during the exploitation. It means that the process of learning does not end

Ljubiša Stankovi´c
Digital Signal Processing
501
neural
network
inputs
outputs
Figure 8.33
Neural network illustration.
with the training period. It continues through the whole functioning of neu-
ral network.
Neural network can be deﬁned as an artiﬁcial cell system capable of
accepting, memorizing and applying empirical knowledge. The knowledge
here means that the neural network can respond to an input from the en-
vironment in an appropriate way. Neural network is connected to the envi-
ronment in two ways: through the inputs where the environment inﬂuences
the network and through the outputs where the network responses to envi-
ronment, as it is illustrated in Figure 8.33.
The basic element in a neural network is neuron. It is the elementary
unit for a distributed signal processing in a neural network. A full function-
ality of neural networks is achieved using large number of interconnected
neurons. Connections among neurons are one-directional (the outputs from
one neuron can be used as inputs to the other neuron). They are called
synapses, in analogy with the biological systems.
Possible applications of neural networks include almost all aspects of
modern life, text and speech recognition, optimization of a communication
channel, ﬁnancial forecasts, detection of a fraud credit card usage, are just a
few examples.
Of course, there are many situations when a usage of neural networks
is not justiﬁed. In many cases our knowledge about the system, that we
want to control or observe, is sufﬁcient and complete so the problem can be
solved using classical algorithms, with sequential processing on common
computers.
An ideal system for neural networks realization would use indepen-
dent systems for hardware realization of each neuron. Then the distributed
processing would be most efﬁcient. In the cases of monoprocessor comput-
ers, high efﬁciency is achieved by using very fast sequential data processing.
Typical examples are computer programs for recognition of a scanned text.

502
Adaptive Systems
x1(n)
x2(n)
xN(n)
inputs 
...
y(n)
output
(a)
x1(n)
x2(n)
xN(n)
...
u(n)
y(n)
(b)
f
activation
function
network
function
Figure 8.34
Neuron schematic symbol (a) and the model based on network and activation
functions (b).
8.10.1
Neuron
The ﬁrst step in a neuron design is to deﬁne its inputs and outputs. In
biological systems the input and output signals to a neuron are electric
potential that can be modelled by real numbers. The same principle is used
in artiﬁcial neurons. Illustration of a neuron is given in Figure 8.34(a) for
the case when it has N inputs (x1(n),x2(n),...,xN(n)) and one output y(n).
Index n may be a time index, but it can also be understood as a cardinal
number that identiﬁes the input and output index of a neuron.
Neuron represents and algorithm that transforms N input data into
one output signal. It is common to split this algorithm into two parts: 1)
combinatorial process that transforms N input data to one output value
u(n) and 2) the process that produces output signals y(n) based on the
value of u(n). This two-phase model of a neuron is presented in Figure
8.34(b). The algorithm/rule to produce u(n) is called the network function,
while the second part which determines the output value is the activation
function.
Neuron knowledge is accumulated and contained in the way how the
input data are combined, i.e., in the network function.
8.10.2
Network Function
The basic task of the network function is to combine the input data. The
simplest way of combining N input signals is in their linear weighed com-
bination with coefﬁcients wi, i = 1,2,..., N. This is a linear network function.
Because of it simplicity, this type of function is commonly used in neurons.

Ljubiša Stankovi´c
Digital Signal Processing
503
Examples of network function are given in the table.
Name
Network function
Linear form
u(n) =
N
∑
i=1
wixi(n) + θ
Linear form (II order)
u(n) =
N
∑
i=1
N
∑
k=1
wikxi(n)xk(n) + θ
Product form
u(n) =
N
∏
i=1
xwi
i (n)
The values of network function commonly depends not only on the
input data, but also on the current state of the neuron. This state is modeled
by a real variable θ, called bias or threshold. Neuron model with a nonzero
bias and a linear activation function may be simpliﬁed if we introduce an
additional input x0 in addition to the existing N inputs. It will be assumed
that this additional input is always set to x0 = 1. Then the bias can be
modelled with a coefﬁcient w0 and the neuron considered as a zero-bias
neuron. This kind of simpliﬁcation will be used in the presentation that
follows.
The knowledge (as a way to transform input data to output signal) is
contained in the values of the coefﬁcients wi (or wik) of the network function.
8.10.3
Activation Function
The activation function transform the output value from the network func-
tion to an acceptable output value. A common requirement is that the out-
put values have limited range. Thus, most of the activation functions have
a bounded interval of real numbers as its codomain, like for example, [0,1]
or [−1,1] or a set of binary digits. Forms of commonly used activation func-
tions are presented in table. The most important functions from this set are
the unipolar threshold function and the unipolar sigmoid. Some of the acti-
vation functions are presented in Figure 8.35 as well.

504
Adaptive Systems
u
f(u)  
unipolar sigmoid
u
f(u)  
unipolar threshold function
u
f(u)  
Gaussian function
u
f(u)  
bipolar sigmoid
u
f(u)  
bipolar threshold function
u
f(u)  
limiter
Figure 8.35
Neuron activation functions.
Function
Formula
Linear
f (u) = u
Linear with a limiter
f (u) =
⎧
⎨
⎩
1
za
u > 1
u
za
−1 ≤u ≤1
−1
za
u < −1
Threshold function (unipolar)
f (u) =
!
1
za
u > 0
0
za
u < 0
Threshold function (bipolar)
f (u) =
!
1
za
u > 0
−1
za
u < 0
Sigmoid (unipolar)
f (u) =
1
1 + exp(−u)
Sigmoid (bipolar)
f (u) =
2
1 + exp(−2u) −1
Inverse tangent function
f (u) = 2
π arctan(u)
Gauss function
f (u) = exp
B (u−m)2
σ2
C
In literature hyperbole tangent function is used as well. It is equal the
bipolar sigmoid.

Ljubiša Stankovi´c
Digital Signal Processing
505
x1(n)
x2(n)
y(n)
(a)
x1(n)
x2(n)
y(n)
(b)
Figure 8.36
Neural network topology: acyclic (a) and cyclic (b).
8.10.4
Neural Network Topology
A number of neurons is connected to each other and to input signal within a
neural network in order to get a network output. Conﬁguration of the signal
ﬂow can be presented by a graph where the neurons are nodes of the graph
and the signal ﬂow is represented by the directed edges. A special category
are the input edges (its start is not a neuron but a node that represents
acquisition of data outside of the neural network - input node) and the
output edges (its end is not a neuron but a node that represents output
information forwarded to the environment - output node).
From the point of view of topology the neural networks can be classi-
ﬁed in various ways. If a neural network contains a closed loop in the graph
then it is a cyclic (recurrent) neural network. The network containing no
closed loop is an acyclic (feed-forward) neural network. Examples of cyclic
and acyclic networks are presented in Figure 8.36. Cyclic or recursive neu-
ral networks are dynamic nonlinear systems (with memory) whose design,
learning, and implementation is complex, mainly because of the nonlinear
nature of individual neuron activation functions . In acyclic networks there
are no closed loops so that the input information pass to the network output
through a ﬁnite number of neurons.
A special category of the acyclic neural networks are layer networks,
where the neurons can be divided into disjunctive subsets (layers). The
output data from one layer are the input data to other layer. Neurons
from the ﬁrst layer get the information from the network input, while the
neurons from the last layer produce output information from the network.
The simplest cases are neural networks with one or two layers of neurons. In
multilayer conﬁgurations it is assumed that the inputs to the mth layer are
the outputs from the (m −1)th layer of neurons. This approach simpliﬁes

506
Adaptive Systems
x1(n) 
x2(n) 
  y1(n)
  y2(n)
  y3(n)
input
layer
output
layer
Figure 8.37
Single layer network example.
x1(n) 
x2(n) 
  y1(n)
input
layer
output
layer
hidden
layer I
hidden
layer II
hidden
layer III
Figure 8.38
Four layer neural network example.
mathematical analysis of the neural networks. In situation when the state of
one neuron is not inﬂuenced by all input values to that layer, the inputs
without inﬂuence are modelled with
zero weighting coefﬁcients in the
network function of this neuron.
In layer networks it is common to introduce the zeroth (input) layer
of neurons where there is no data processing. Its function is to forward the
input data to the ﬁrst layer. The last layer is the output layer. An example of
one-layer neural network is presented in Figure 8.37. An example of neural
network with four layers is shown in Figure 8.38. This network consists of
three hidden layers (I, II, and III) with 3, 2 and 4 neurons, respectively, and
the output layer with one neuron.

Ljubiša Stankovi´c
Digital Signal Processing
507
8.10.5
Network with Supervised Learning
The network adapts for desired functioning trough a process of training.
The training is implemented using the following algorithm:
1. Data for the network training are acquired. This data consists of
the input-output pairs. The output data are assumed, estimated or
obtained trough experiments. This set of training data pairs if ﬁnite.
Denote the number of available input-output pairs by K.
2. The network is initiated, commonly by using random parameters of
neurons (if an a priori information about the range of their values does
not exist). After the initialization, the iterative training procedure is
implemented as follows:
(a) One input-output pair of data is considered. The output of the
neural network is calculated. The output value is compared with
the desired (given) output value. If the output from the neural
network is equal to the desired output value (or sufﬁciently
close to this value) then no correction in the network is done.
Otherwise when the result of comparison is not satisfactory, the
parameters of neural network are corrected to get a better result
in the considered case.
(b) Next pair of the input-output data is considered. The calculation,
comparison, and correction process is repeated.
(c) This cycle of training ends when all data available for training are
used. This whole cycle is called one epoch.
(d) The result achieved by the neural network in the previous epoch
of training is analyzed. If the output of the neural network has
been satisfactory for each individual pair of the data then the
training is ﬁnished. If the result is not satisfactory, then another
epoch of training is needed. Go back to 2a and repeat all previous
steps.
3. Neural network is ready for use. A testing of the trained neural
network can now be done. A common way of testing is in omitting
some pairs of the available input-output data in the training procedure
and in using them after the training process is completed, to test the
neural network accuracy.

508
Adaptive Systems
The iterative algorithm for training of neural network (steps 2a–2d)
does not necessary converges. Algorithm is usually implemented by im-
posing a maximal number of epochs. In the case that the result is not sat-
isfactory (neural network does not produce accurate results after training)
the training process may be repeated with new initialization of the network
parameters. This is one of the reasons why the random initial values of the
parameters are used.
One of the main problems in the neural networks training is the
way how to modify the parameters when we are not satisﬁed with the
results. Several networks will be analyzed next, where this problem will
be addressed. In some cases just a small random correction of parameters
can improve results, when the process of parameter changes is locked in a
local minimum.
8.10.6
One-Layer Network with Binary Output - Perceptron
Perceptron as the oldest simple form of neural networks. Perceptron has
to establish if the input data have a desired property or not. It produces a
binary output. For example, the input data may be scanned pixels of one
letter in the text. From neural network we expect a simple answer is the
scanned letter, for example letter „A” or not. Since the output is a logical
variable, an obvious choice for the neuron activation function is a function
with binary output (bipolar or unipolar threshold function). Assume that
unipolar function is used as the activation function
f (u) =
!
1
za
u > 0
0
za
u < 0 .
The neuron network function, in the case of perceptron, is a linear form
u(n) =
N
∑
k=1
wkxk(n)
(8.36)
where it has been assumed that the neuron has N input data. The weighting
coefﬁcients wk represent „knowledge” that the network should get through
the training procedure. This knowledge will be then used in real situations.
The vector notation is
X(n) =
⎡
⎢⎢⎢⎣
x1(n)
x2(n)
...
xN(n)
⎤
⎥⎥⎥⎦
N×1
W =
⎡
⎢⎢⎢⎣
w1
w2
...
wN
⎤
⎥⎥⎥⎦
N×1
.

Ljubiša Stankovi´c
Digital Signal Processing
509
The network function can be written as
u(n) = WTX(n)
= XT(n)W.
The neuron output is
y(n) = f (u(n)) = f
B
WTX(n)
C
.
Perceptron basic form consists of one neuron only. In topological sense
it is acyclic network. It is one layer neural network. In a similar way it
is possible to deﬁne perceptron with more than one neuron. In that case
we have a neural network with M output signals. Network can decide if
the input data contain some out of M properties that we are interested in.
An example of such a network would be a perceptron which, based on the
pixels of a scanned letter, decides which of the letters „A”, „B” or „C” was
scanned. Note than in this case we may obtain more than one 1 at the output
(network, for example has recognized that the input data correspond to the
letter „A” and letter „B”). In the considered case it means that the network
was not able to recognize the letter. However, in some cases the output data
can be deﬁned in such a way that more than one property are satisﬁed for
one set of input data. For example, in letter recognition, the results may be:
„scanned letter contains horizontal line”, „scanned letter contains vertical
line” and „scanned letter contains oblique” and to train the neural network
to recognize these properties of a scanned letter.
The network output can be written as a column vector with M ele-
ments,
Y(n) =
⎡
⎢⎢⎢⎣
y1(n)
y2(n)
...
yM(n)
⎤
⎥⎥⎥⎦
M×1
.
The weighting coefﬁcients of individual neurons Wl, l = 1,2,..., M can be
written in a matrix form
W =
D
W1
W2
···
WM
E
N×M
=
⎡
⎢⎢⎢⎣
w11
w21
···
wM1
w12
w22
···
wM2
...
...
...
...
w1N
w2N
···
wMN
⎤
⎥⎥⎥⎦
N×M
.

510
Adaptive Systems
The relation between output and input can be written as
Y(n) = f
B
WTX(n)
C
.
Consider again
simple case of perceptron with one neuron. In the
training process, when for a given input X(n) the network produces output
y(n), its has to be compared with desired value d(n). Possible cases are
y(n) = d(n), when no correction of the network coefﬁcients W is done, and
y(n) ̸= d(n) when the coefﬁcients are modiﬁed as
Wnew = Wold + ∆W.
The correction ∆W should be done in such a way that it increases the
possibility to get a desired output for the considered input data. In this case
the output may take only one of two binary values: 0 for u(n) = WTX(n) < 0
or 1 for u(n) = WTX(n) > 0. Assume that the desired value of the output
is 1 and that we obtained 0. It means WTX(n) < 0 holds, while it should
be WTX(n) > 0. The value of WTX(n) should be increased to increase the
possibility of getting the desired output. In the case that the desired output
is 0 and the output from the network is 1, using a similar reasoning we can
conclude that WTX(n) has to be decreased. A way to implement the desired
coefﬁcients modiﬁcation is
Wnew = Wold + µ(d(n) −y(n))X(n).
In the case when d(n) = 1 and y(n) = 0 it means that Wnew = Wold + µX(n)
or
WT
newX(n) = WT
oldX(n) + µXT(n)X(n)
= WT
oldX(n) + µ||X(n)||2
2,
where ||X(n)||2
2 is the squared norm two of vector X(n) (sum of its squared
elements). The value of WTX(n) is increased for µ||X(n)||2
2, what was the
aim. If d(n) = 0 and y(n) = 1 then Wnew = Wold −µX(n) holds, meaning
that WTX(n) is reduced for µ||X(n)||2
2.
The coefﬁcient µ is the learning coefﬁcient. It is positive. The choice
of parameter µ value is of great importance for the rate of convergence
and learning process of the network. Larger values may reduce the learning
period, but also may inﬂuence the convergence of the training process.
Example 8.28. Consider a one-neuron neural network. Assume that the activation
function of the neuron is unipolar threshold function and that the neuron

Ljubiša Stankovi´c
Digital Signal Processing
511
is biased. The network has three inputs and one output. Set of data for the
neural network training is
X =
⎡
⎣
1
1
0
0
0
1
1
0
1
1
0
0
0
1
1
0
1
0
⎤
⎦
D =
D
1
1
0
0
0
0
E
where matrix X contains the input data and vector D consists of desired
outputs from the neural network for the considered input data values. Train
the neural network with µ = 0.5.
⋆Since the neuron is biased one more input will be introduced. Its
input value is always 1. After this modiﬁcation the matrix of input data is
X =
⎡
⎢⎢⎣
1
1
1
1
1
1
1
1
0
0
0
1
1
0
1
1
0
0
0
1
1
0
1
0
⎤
⎥⎥⎦.
Initial values of weighting coefﬁcients are random, for example,
W =
⎡
⎢⎢⎣
w0
w1
w2
w3
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−1
1
1
0
⎤
⎥⎥⎦.
Now we can start the ﬁrst epoch of training process. We will use
all input-output data pairs and calculate the output y(n) from the neural
network. The output y(n) will be compared with the desired value d(n) and
the coefﬁcients W will be appropriately modiﬁed for each pair of data.
For the ﬁrst par of data we have
y(1) = f
B
WTX(1)
C
= f (
D−1
1
1
0
E
⎡
⎢⎢⎣
1
1
1
0
⎤
⎥⎥⎦) = 1.
Since d(1) = 1 the error d(n) −y(n) is 0 and the coefﬁcients are not modiﬁed.
For the second pair of data
y(2) = f
B
WTX(2)
C
= 0.
The desired value is d(2) = 1. Since the error is not zero, the coefﬁcients
should be modiﬁed as
Wnew = Wold + µ(d(2) −y(2))X(2) =
⎡
⎢⎢⎣
−1
1
1
0
⎤
⎥⎥⎦+ 0.5
⎡
⎢⎢⎣
1
1
0
1
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−0.5
1.5
1
0.5
⎤
⎥⎥⎦.

512
Adaptive Systems
Next pair of input-output data is used. After all data pairs are used, the ﬁrst
epoch of training is ﬁnished. Nonzero error appeared in three out of six data
pairs. The ﬁnal value of the coefﬁcients, after the ﬁrst training epoch, is
Wepoch 1 =
D−1.5
1
0.5
0
ET .
With this initial value, the second epoch of training is completed, using
the same input-output pairs of data. After the second epoch nonzero error
appeared two times. The ﬁnal values of the coefﬁcients, after the second
epoch, are
Wepoch 2 =
D−1.5
1
1
0
ET .
The process is continued in the third epoch. In the ﬁfth epoch we came
to the situation that the neural network has made no error. It means that the
training is completed and that more epochs are not needed. The ﬁnal values
of the coefﬁcients are
W =
D−1.5
1.5
0.5
0.5
ET .
8.10.7
One-Layer Neural Network with Continuous Output
In this kind of neural networks the output signal is not binary, but a real
number (usually within the interval from 0 to 1). It may be interpreted as
a probability that the input data contain or do not contain certain property.
In general any interval of real numbers can be a codomain of the output
function. The main difference from the perceptron is that we do not require
that the neural network achieves an exact precision y(n) −d(n) = 0. In this
case the aim to get a small error in the processing of input results.
Since the output variable is continuous, the activation function should
have such a property as well. Consider, for example, the unipolar sigmoid
activation function
f (u) =
1
1 + e−u .
A simple way to quantify the difference of the output signal from the
desired signal is to use the square error
ε(n) = 1
2(d(n) −y(n))2
where constant 1/2 is introduced to simplify the notation in the period of
the neural network training process. The goal is to minimize the square er-
ror. This minimization can be done in various ways: using steepest descent

Ljubiša Stankovi´c
Digital Signal Processing
513
method, conjugate gradient method, Newton method, are some of these
methods. We will use the steepest descend method in the correction of the
neural network coefﬁcients.
Consider a network with N inputs. The input data vector is X(n) and
the desired output is d(n). The network output signal is obtained as
y(n) = f (u(n)) = f
B
WTX(n)
C
= f
(
n
∑
k=1
wkxk(n)
)
with the square error
ε(n) = 1
2(d(n) −y(n))2 = 1
2
(
d(n) −f
(
n
∑
k=1
wkxk(n)
))2
.
(8.37)
This error is a function of the network coefﬁcients wk. Using the steepest
descend method the coefﬁcients modiﬁcation is done as
wk,(new) = wk,(old) −µ∂ε(n)
∂wk
or
Wnew = Wold −µ∂ε(n)
∂W
where ∂ε(n)/∂W is the gradient of error function.
Derivatives can be calculated from (8.37) as
∂ε(n)
∂wk
= −(d(n) −y(n))∂y(n)
∂wk
= −(d(n) −y(n)) f ′
(
n
∑
k=1
wkxk(n)
)
xk(n).
For the unipolar sigmoid activation functions we have
f ′(u) = d
du
1
1 + e−u = −
−e−u
(1 + e−u)2 =
1
1 + e−u
e−u
1 + e−u =
= f (u)
*
1 −
1
1 + e−u
+
= f (u)(1 −f (u)).
Therefore
∂ε(n)
∂wk
= −(d(n) −y(n))y(n)(1 −y(n)) xk(n),
where
f
(
n
∑
k=1
wkxk(n)
)
= y(n)

514
Adaptive Systems
is used. The training rule (correction of the coefﬁcients) is
wk,(new) = wk,(old) + µ(d(n) −y(n))y(n)(1 −y(n)) xk(n)
or in vector form
Wnew = Wold + µ(d(n) −y(n))y(n)(1 −y(n))X(n).
In is common to denote (d(n) −y(n))y(n)(1 −y(n)) by δn so that the
training rule can be written as
Wnew = Wold + µδnX(n).
This rule is called delta-rule. Note that the letter δ is also used for Dirac delta
pulse in some chapters of the book. These two values do not have anything
in common.
For the activation function in the form of bipolar sigmoid
f (u) =
2
1 + e−2u −1 = 1 −e−2u
1 + e−2u
we would have
f ′(u) =
4e−2u
(1 + e−2u)2 = (1 + e−2u)2 + 4e−2u −(1 + e−2u)2
(1 + e−2u)2
= 1 −(1 + 2e−2u + e−4u) −4e−u
(1 + e−2u)2
= 1 −(1 −e−2u)2
(1 + e−u)2 = 1 −f 2(u)
and the value of δn would be
δn = (d(n) −y(n))(1 −y2(n)).
Example 8.29. Neural network consists of one unbiased neuron with two input
signals and a sigmoid activation function. Input values are random numbers
from the interval [0,1]. Available are K = 30 input-output pairs of data.
Training of of the neural network should be done in 30 epochs with µ = 2.
Data for network training are obtained as a set of 30 input values of x1
and x2. They are assumed as random numbers from the interval from 0 to 1
with a uniform probability density function. For each training pair of random

Ljubiša Stankovi´c
Digital Signal Processing
515
numbers x1 and x2 the desired output data is calculated using the formula
d = 1
2 +
x1 −2x2
3 + x2
1 + 3x2
2
.
Find the total square error after the ﬁrst, second, ﬁfth and thirtieth
epoch. What are the coefﬁcient values at the end of training process? If the
input values x1 = 0.1 and x2 = 0.8 are applied to the network after the training
process is completed ﬁnd the output value y and compare it with the desired
result d calculated using the formula.
⋆Coefﬁcients of the neuron are w1 and w2. With the sigmoid activation
function the coefﬁcient corrections are
-w1
w2
.
new
=
-w1
w2
.
old
+ µ(d(n) −y(n))y(n)(1 −y(n))
-x1(n)
x2(n)
.
,
where index n assumes values from 1 to 30 within one epoch. It denotes the
index of the input-output pair of data. The output y is calculated using
y(n) = f
B
WTX(n)
C
= f (w1x1(n) + w2x2(n)).
Initial coefﬁcient values are randomly chosen.
The training process is implemented on a computer and the following
results are obtained: Total square error after the ﬁrst epoch of training is
0.4266. After the second training epoch the total error is reduced to 0.1062.
The total error after the ﬁfth epoch is 0.0336, while its value at the end of
the training process (after 30 epochs) is 0.0170. The ﬁnal values of the neuron
coefﬁcients are
w1 = 1.0455
w2 = −1.9401.
For the input data x1 = 0.1 and x2 = 0.8 we get
y = f (w1x1 + w2x2) = 0.1904
d = 1
2 +
x1 −2x2
3 + x2
1 + 3x2
2
= 0.1957.
The error is small. The task for neural network in this example was to ﬁnd a
complex, nonlinear relation between the input and output data.
8.10.8
Multilayer Neural Networks
The multilayer neural networks are characterized by at least one hidden
layer, the layer whose values do not appear as the network output. During
the training process available are the input-output pairs of data, while
the data for hidden layers are not known. On of the approaches to train

516
Adaptive Systems
this kind of neural network is so called „error backpropagation learning”
method. In this method the known error at the output layer of neurons is
transformed into the error at the output of the previous neuron layer.
This algorithm will be illustrated on an example of a neural network
with two layers and one output. The considered neural network has N
inputs, M neurons in the hidden layer and one neuron in output layer. The
unipolar sigmoid is assumed as the activation function, while the network
function is a linear combination of the inputs. Consider one element from
the training data set pairs (X(n),d(n)). The outputs of the hidden layer are
denoted by U(n) and calculated as
U(n) = f
B
WTX(n)
C
,
where WN×M is the matrix of neuron weighting coefﬁcients in the hidden
layer, and U(n) is a vector column with M output values of the hidden layer.
The neural network output is
y(n) = f
B
VTU(n)
C
= f
B
VT f
B
WTX(n)
CC
where VM×1 is the vector of weighting coefﬁcients of the output neuron.
The square error is
ε(n) = 1
2(d(n) −y(n))2.
The desired output value is d(n). Coefﬁcients V are modiﬁed in the same
way as in the previous case with the one-layer neural network with contin-
uous output,
vm,(new) = vm,(old) −µ∂ε(n)
∂vm
where
∂ε(n)
∂vm
= −(d(n) −y(n)) f ′ B
VTU(n)
C
um(n)
= −(d(n) −y(n))y(n)(1 −y(n))um(n).
The mth element of vector U(n) is denoted by um(n).
The ﬁnal modiﬁcation relations are
Vnew = Vold + µ(d(n) −y(n))y(n)(1 −y(n))U(n)
Vnew = Vold + µδn U(n)

Ljubiša Stankovi´c
Digital Signal Processing
517
where δn = (d(n) −y(n))y(n)(1 −y(n)) is the training rule.
Consider now kth neuron in the hidden layer. Coefﬁcients of this neu-
ron are the elements of kth column of matrix W, denoted by Wk. Coefﬁcients
of this neuron are modiﬁed as
wpk,(new) = wpk,(old) −µ∂ε(n)
∂wpk
where
∂ε(n)
∂wpk
= −(d(n) −y(n)) f ′ B
VT f
B
WTX(n)
CC
vk f ′ B
WT
k X(n)
C
xp(n).
The pth element of vector X(n) is denoted by xp(n), while the kth element
of vector V is vk. Taking into account that uk(n) = f
P
WT
k X(n)
Q
we get
∂ε(n)
∂wpk
= −(d(n) −y(n))y(n)(1 −y(n))vk [uk(n)(1 −uk(n))] xp(n).
Coefﬁcients modiﬁcation rule for this neuron is
wpk,(new) = wpk,(old) + µ(d(n) −y(n))y(n)(1 −y(n))vk [uk(n)(1 −uk(n))]
× xp(n) = wpk,(old) + µ δn2 vk [uk(n)(1 −uk(n))] xp(n)
where δn2 denotes the learning rule for the considered layer of neurons. In
vector form we can write
Wk,(new) = Wk,(old) + µδn2vk [uk(n)(1 −uk(n))] X(n).
This is the modiﬁcation formula for all coefﬁcients of one neuron in the
hidden layer. The modiﬁcation can be generalized to all neurons in the
hidden layer
W(new) = W(old) + µδn2 X(n) [V. ∗U(n). ∗(1 −U(n))]T ,
where .∗denotes the element-by-element multiplication, while 1 is the
vector of the same dimension as U(n) whose elements are equal to 1.
The described procedure can be generalized for neural networks with
more than two layers. The basic principle is that based on the error in
one layer, the coefﬁcients are modiﬁed in this layer and then in all layers
before the considered layer. It means that the inﬂuence of the output error
is transferred in an inverse way (backpropagated) to the correction of the
coefﬁcients of the layers of neurons.

518
Adaptive Systems
Example 8.30. Consider a two-layer neural network with two neurons in the
hidden layer and one neuron in the output layer. The activation function for
all neurons is the unipolar sigmoid. The task for this neural network is to ﬁnd
unknown relation between the input and output data. Step µ = 5 is used in
the training process. The data for the training are formed as in Example 8.29,
i.e., as a set of K = 30 input data x1 and x2 that are uniformly distributed
random numbers from the interval 0 ≤x1 ≤1, 0 ≤x2 ≤1. For each training
input value of x1 and x2 the desired signal is calculated as
d = 1
2 +
x1 −2x2
3 + x2
1 + 3x2
2
.
Find the total square error after 10th, 100th, and 300th epoch. What
are the coefﬁcients of the neurons after the training process? If the values of
x1 = 0.1 and x2 = 0.8 input the trained neural network ﬁnd the output y and
compare it with the desired result d.
⋆The training process is implement on a computer and the following
results are obtained: Total square error after 10 epochs of training is 0.1503.
After 100 epochs the total square error is reduced to 0.0036, while the squared
error after 300 epochs is 0.0003. The ﬁnal coefﬁcient values in the hidden and
output layers, W and V, are
W =
-−0.2911
1.8297
3.4435
−0.6945
.
V =
-−2.6173
2.5889
.
.
For the input data x1 = 0.1 and x2 = 0.8 we get
y = f
*
VT f
*
WT
-0.1
0.8
.++
= 0.1978
d = 1
2 +
x1 −2x2
3 + x2
1 + 3x2
2
= 0.1957.
The error is very small. As expected this result is better than in the case of
one-layer neural network (Example 8.29). However, the calculation process
is signiﬁcantly more demanding.
8.10.9
Neural Networks with Unsupervised Learning
Consider an example of one-layer neural network with N neurons and two
input data. The input data will be here interpreted as the coordinates of
points in plane. If the input data (coordinates of points) exhibit property
of being grouped in certain regions of the two-dimensional plane (regions
are deﬁned by straight lines passing through the origin), then we can ask
the neural network to ﬁnd the group to which an arbitrary input data pair

Ljubiša Stankovi´c
Digital Signal Processing
519
(point) belongs. In an ideal case the number of groups (categories) is known
and equal to the number of neurons N. The training process for a neural
network reduces to the selection of the neuron with the highest output
(assume that it is the neuron with index k) and to the modiﬁcation of its
coefﬁcients using
Wk,(new) = Wk,(old) + µ
B
X(n) −Wk,(old)
C
After the training process we may expect that each of the neurons
recognizes one category (belonging to one group) of the input signals. If
an uncategorized input signal appears it means that the estimation of the
number of neurons is not good. It should be increased and the training
process should be continued. When two neuron adjust to the same category,
then they produce the same result and one of them can be eliminated. In this
way, we may avoid the assumption that the number of categories (groups)
or neurons N is known in advance.
Example 8.31. Consider a neural network with two input data and 3 neurons. The
task of neural network is to classify the input data in one of three categories.
Each neuron corresponds to one category. The classiﬁcation decision is made
by choosing the neuron with the highest output. Activation function is a
bipolar sigmoid.
Simulate the neural network in the case when the input data belongs to
one of thee categories with equal probability. Data from the ﬁrst category are
pairs of Gaussian random variables with probability density function whose
means are ¯x1 = 0 and ¯x2 = 4 and variances are σ2x1 = 4 and σ2x2 = 0.25. For the
data from the second category the mean values and variances of Gaussian
variables are ¯x1 = 4, ¯x2 = −2, σ2x1 = 1 and σ2x2 = 4. In the third category are
the input data with ¯x1 = −4 and ¯x2 = −2, σ2x1 = 1 and σ2x2 = 1. during the
training process the step µ = 0.5 is used.
⋆Results achieved by neural network after 10 and 100 pairs of input
data are presented in Figure 8.39. The categories are indicated with different
colors. Learning process of the neural network in the input classiﬁcation of
data is fast.
8.10.10
Voting Machines
Voting machines are special forms of the application of neural networks.
Two basic forms of the voting machines are used: neural network ensemble
and mixture of experts.
Voting machines of the neural network ensemble type consists of
several neural networks which are independently designed and trained

520
Adaptive Systems
x1
x2
10 input data points
-8
-6
-4
-2
0
2
4
6
8
-8
-6
-4
-2
0
2
4
6
8
x1
x2
100 input data points
-8
-6
-4
-2
0
2
4
6
8
-8
-6
-4
-2
0
2
4
6
8
Figure 8.39
Example of unsupervised training of a neural network. Input data are classiﬁed
into three categories. Regions obtained by the neural network after 10 and 100 input data, are
presented in different colors in the plane of input data.
and a control network that interprets the output data from these neural
networks. All networks are trained to solve the same kind of problem,
meaning that the same data are used as input in all of them, while the control
network decides about the ﬁnal result, for example using the principle of
majority of votes.
The mixture of experts is a set of neural networks, where each of them
is trained to process one type of the input data. Control network, in this
case, has to choose one or more experts (neural networks) which are trained
for the type of data that appears the input.

Chapter 9
Time-Frequency Analysis
The Fourier transform provides a unique mapping of a signal from the
time domain to the frequency domain. The frequency domain representa-
tion provides the signal’s spectral content. Although the phase characteristic
of the Fourier transform contains information about the time distribution of
the spectral content, it is very difﬁcult to use this information. Therefore, one
may say that the Fourier transform is practically useless for this purpose,
i.e., that the Fourier transform does not provide a time distribution of the
spectral components.
Depending on problems encountered in practice, various representa-
tions have been proposed to analyze non-stationary signals in order to pro-
vide time-varying spectral description. The ﬁeld of the time-frequency sig-
nal analysis deals with these representations of non-stationary signals and
their properties. Time-frequency representations may roughly be classiﬁed
as linear, quadratic, and higher order representations.
Linear time-frequency representations exhibit linearity, i.e., the repre-
sentation of a linear combination of signals equals the linear combination
of the individual representations. From this class, the most important one
is the short-time Fourier transform (STFT) and its variations. The energetic
version of the STFT is called spectrogram. It is the most frequently used tool
in time-frequency signal analysis.
The second class of time-frequency representations are the quadratic
ones. The most interesting representations of this class are those which pro-
vide a distribution of signal energy in the time-frequency plane. They will be
referred to as distributions. The concept of a distribution is borrowed from
_________________________________________________
Authors: Ljubiša Stankovi´c, Miloš Dakovi´c, Thayaparan Thayananthan
521

522
Time-Frequency Analysis
t
τ
τ
x(t) 
w(τ) 
x(t+τ)w(τ) 
t
Figure 9.1
Illustration of the signal localization in the STFT calculation.
the probability theory, although there is a fundamental difference. For ex-
ample, in time-frequency analysis, distributions may take negative values.
Other possible domains for quadratic signal representations are the ambi-
guity domain, the time-lag domain and the frequency-Doppler frequency
domain. In order to improve time-frequency representation various higher-
order distributions have been deﬁned as well.
9.1
SHORT-TIME FOURIER TRANSFORM
The idea behind the short-time Fourier transform (STFT) is to apply the
Fourier transform to a portion of the original signal, obtained by introduc-
ing a sliding window function w(t) to localize the analyzed signal x(t). The
Fourier transform is calculated for the localized part of the signal. It pro-
duces the spectral content of the portion of the analyzed signal within the
time interval deﬁned by the width of the window function. The STFT (a
time-frequency representation of the signal) is then obtained by sliding the
window along the signal. Illustration of the STFT calculation is presented in
Fig.9.1.
Analytic formulation of the STFT is
STFT(t,Ω) =
∞
"
−∞
x(t + τ)w(τ)e−jΩτ dτ.
(9.1)
From (9.1) it is apparent that the STFT actually represents the Fourier
transform of a signal x(t), truncated by the window w(τ) centered at

Ljubiša Stankovi´c
Digital Signal Processing
523
instant t (see Fig. 9.1). From the deﬁnition, it is clear that the STFT satisﬁes
properties inherited from the Fourier transform (e.g., linearity).
By denoting xt(τ) = x(t + τ) we can conclude that the STFT is the
Fourier transform of the signal xt(τ)w(τ),
STFT(t,Ω) = FTτ{xt(τ)w(τ)}.
Another form of the STFT, with the same time-frequency performance,
is
STFTII(t,Ω) =
∞
"
−∞
x(τ)w∗(τ −t)e−jΩτdτ
(9.2)
where w∗(t) denotes the conjugated window function.
It is obvious that deﬁnitions (9.1) and (9.2) differ only in phase, i.e.,
STFTII(t,Ω) = e−jΩtSTFT(t,Ω) for real valued windows w(τ). We will
mainly use the ﬁrst STFT form.
Example 9.1. To illustrate the STFT application, let us perform the time-frequency
analysis of the following signal
x(t) = δ(t −t1) + δ(t −t2) + ejΩ1t + ejΩ2t.
(9.3)
The STFT of this signal equals
STFT(t,Ω) = w(t1 −t)e−jΩ(t1−t) + w(t2 −t)e−jΩ(t2−t)
+ W(Ω−Ω1)ejΩ1t + W(Ω−Ω2)ejΩ2t,
(9.4)
where W(Ω) is the Fourier transform of the used window. The STFT is de-
picted in Fig. 9.2 for various window lengths, along with the ideal represen-
tation. A wide window w(t) in the time domain is characterized by a narrow
Fourier transform W(Ω) and vice versa. Inﬂuence of the window to the re-
sults will be studied later.
Example 9.2. The STFT of signal
x(t) = ejat2
(9.5)
can be approximately calculated for a large a, by using the method of station-
ary phase. Find its form and the relation for the optimal window w(τ) width,
assuming that the window is nonzero for |τ| < T .

524
Time-Frequency Analysis
t1
t2
Ω1
Ω2
Ω
t
 (a)
STFTwide(t,Ω)
t1
t2
Ω1
Ω2
Ω
t
 (b)
STFTnarrow(t,Ω)
t1
t2
Ω1
Ω2
Ω
t
 (c)
STFToptimal(t,Ω)
t1
t2
Ω1
Ω2
Ω
t
 (d)
Ideal TFR(t,Ω)
Figure 9.2
Time-frequency representation of the sum of two delta pulses and two sinusoids
obtained by using (a) wide window, (b) narrow window (c) medium width window and (d)
ideal time-frequency representation.
⋆Applying the stationary phase method (1.62), we get
STFT(t,Ω) =
∞
"
−∞
eja(t+τ)2w(τ)e−jΩτdτ
T
"
−T
eja(t+τ)2w(τ)e−jΩτdτ ≃ejat2ej(2at−Ω)τ0ejaτ2
0 w(τ0)
=
2πj
2a
= ejat2e−j(2at−Ω)2/4aw
* Ω−2at
2a
+=
πj
a
(9.6)
since
2a(t + τ0) = Ω.

Ljubiša Stankovi´c
Digital Signal Processing
525
Note that the STFT absolute value reduces to
|STFT(t,Ω)| ≃
''''w
* Ω−2at
2a
+''''
=
π
a .
(9.7)
In this case, the width of |STFT(t,Ω)| along frequency does not decrease with
the increase of the window w(τ) width. The width of |STFT(Ω,t)| around the
central frequency Ω= 2at is
D = 4aT,
where 2T is the window width in the time domain. Note that this relation
holds for a wide window w(τ), such that the stationary phase method may
be applied. If the window is narrow with respect to the phase variations of
the signal, the STFT width is deﬁned by the width of the Fourier transform
of window. It is proportional to 1/T. Thus, the overall STFT width could
be approximated by a sum of the frequency variation caused width and the
window’s Fourier transform width, that is,
Do = 4aT + 2c
T ,
(9.8)
where c is a constant deﬁned by the window shape (by using the main lobe
as the window width, it will be shown later that c = 2π for a rectangular
window or c = 4π for a Hann(ing) window). This relation corresponds to
the STFT calculated as a convolution of an appropriately scaled time domain
window whose width is |τ| < 2aT and the frequency domain form of window
W(Ω). The approximation is checked against the exact STFT calculated by
deﬁnition. The agreement is almost complete, Fig.9.3. Therefore, there is a
window width T producing the narrowest possible STFT for this signal. It is
obtained by equating the derivative of the overall width to zero,
4a −2c
T2 = 0,
which results in
To =
=
c
2a.
(9.9)
As expected, for a sinusoid, a →0, To →∞. This is just an approximation
of the optimal window, since for narrow windows we may not apply the
stationary phase method (the term 4aT is then much smaller than 2c/T and
may be neglected anyway).
Note that for a = 1/2, when the instantaneous frequency is a symmetry
line for the time and the frequency axis
2 −2c
T2 = 0 or 2T = 2c
T ,
meaning that the optimal window should have the widths equal in the time-
domain 2T and in the frequency domain 2c/T (main lobe width).

526
Time-Frequency Analysis
Exact absolute STFT value
frequency Ω
log of the window width   log2(T)
-1
0
1
1
2
3
4
5
6
7
8
9
10
STFT approximation
frequency Ω
log of the window width   log2(T)
-1
0
1
1
2
3
4
5
6
7
8
9
10
Figure 9.3
Exact absolute STFT value of a linear FM signal at t = 0 for various window
widths T = 2,4,8,16,..,1024 (left) and its approximation calculated as an appropriately scaled
convolution of the time and frequency domain window w(τ) (right).
The STFT can be expressed in terms of the signal’s Fourier transform
STFT(t,Ω) = 1
2π
∞
"
−∞
∞
"
−∞
X(θ)ej(t+τ)θ w(τ)e−jΩτ dθ dτ
= 1
2π
∞
"
−∞
X(θ)W(Ω−θ)ejtθdθ =
@
X(Ω)ejtΩA
∗ΩW(Ω). (9.10)
where ∗Ωdenotes convolution in Ω. It may be interpreted as an inverse
Fourier transform of the frequency localized version of X(Ω), with localiza-
tion window W(Ω) = FT{w(τ)}.
The energetic version of the STFT, called the spectrogram, is deﬁned
by
SPEC(t,Ω) =| STFT(t,Ω) |2
=
''''''
∞
"
−∞
x(τ)w∗(τ −t)e−jΩτdτ
''''''
2
=
''''''
∞
"
−∞
x(t + τ)w∗(τ)e−jΩτdτ
''''''
2
.
Obviously, linearity property is lost in the spectrogram.

Ljubiša Stankovi´c
Digital Signal Processing
527
t
x1(t)
 (a)
Ω
|X1(Ω)|
 (c)
t
x2(t)
 (b)
Ω
|X2(Ω)|
 (d)
Figure 9.4
Two different signals x1(t) ̸= x2(t) with the same amplitudes of their Fourier
transforms, i.e., |X1(Ω)| = |X2(Ω)|.
Example 9.3. For illustration consider two different signals x1(t) and x2(t) pro-
ducing the same amplitude of the Fourier transform, Fig. 9.4,
x1(t) = sin
*
122π t
128
+
−cos
(
42π t
128 −16
11π
* t −128
64
+2)
−1.2cos
(
94π t
128 −2π
* t −128
64
+2
−π
* t −120
64
+3)
e−( t−140
75 )
2
−1.6cos
(
15π t
128 −2π
* t −50
64
+2)
e−( t−50
16 )
2
(9.11)
x2(t) = x1(255 −t).
Their spectrograms are presented in Fig.9.5. From the spectrograms we can
follow time variations of the spectral content. The signals obviously consist
of one constant high frequency component, one linear frequency component
(in the ﬁrst signal with increasing frequency as time progresses, and in the
second signal with decreasing frequency), and two chirps (one appearing at
different time instants and the other having different frequency variations).
The signal can be obtained from the STFT calculated at an instant t0 as
x(t0 + τ) =
1
w(τ)
∞
"
−∞
STFT(t0,Ω)e−jΩτ dτ.

528
Time-Frequency Analysis
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
 (a)
Ω
SPEC1(t,Ω)
t
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
 (b)
Ω
SPEC2(t,Ω)
t
Figure 9.5
Spectrograms of the signals presented in Fig.9.4.
This relation can be theoretically used for the signal within the region
w(τ) ̸= 0. In practice it is used within the region of signiﬁcant window w(τ)
values.
If the window is shifted for R, for each next STFT calculation, then a
set of values
x(t0 + iR + τ)w(τ) =
∞
"
−∞
STFT(t0 + iR,Ω)e−jΩτ dτ

Ljubiša Stankovi´c
Digital Signal Processing
529
is obtained. If the value of step R is smaller than the window duration then
the same signal value is used within two (several) windows. Using a change
of variables iR + τ = λ and summing over all overlapping windows we get
x(t0 + λ)∑
i
w(λ −iR) = ∑
i
∞
"
−∞
STFT(t0 + iR,Ω)e−jΩλejΩiR dλ.
Values of i in the summation are such that for a given λ and R the value of
iR −λ = τ is within the window w(τ).
If the sum of shifted versions of the windows is constant (without loss
of generality assume equal to 1), ∑
i
w(τ −iR) = 1, then
x(t0 + λ) = ∑
i
∞
"
−∞
STFT(t0 + iR,Ω)e−jΩλejΩiR dλ
for any λ. Condition ∑
i
w(τ −iR) = 1 means that a periodic extension of
the window, with period R, is constant. Periodic extension of a continuous
signal corresponds to the sampling of the window Fourier transform at
Ω= 2π
R n in the Fourier domain, (1.59). It means that W( 2π
R n) = 0 when
n ̸= 0 for ∑
i
w(λ −iR) = 1.
9.2
WINDOWS
The window function plays a crucial role in the localization of the signal
in the time-frequency plane. The most commonly used windows will be
presented next.
9.2.1
Rectangular Window
The simplest window is the rectangular one, deﬁned by
w(τ) =
!
1
for |τ| < T
0
elsewhere
(9.12)
whose Fourier transform is
WR(Ω) =
T
"
−T
e−jΩτdτ = 2sin(ΩT)
Ω
.
(9.13)

530
Time-Frequency Analysis
The rectangular window function has very strong and oscillatory
sidelobes in the frequency domain, since the function sin(ΩT)/Ωconverges
very slowly, toward zero, in Ωas Ω→±∞. Slow convergence in the Fourier
domain is caused by a signiﬁcant discontinuity in time domain, at t = ±T.
The mainlobe width of WR(Ω) is dΩ= 2π/T. In order to enhance signal
localization in the frequency domain, other window functions have been
introduced.
The discrete-time form of the rectangular window is
w(n) = u(n + N/2) −u(n −N/2)
with the Fourier transform
W(ejω) =
N/2−1
∑
n=−N/2
e−jωn = sin(ωN/2)
sin(ω/2) .
9.2.2
Triangular (Bartlett) Window
It is deﬁned by
w(τ) =
!
1 −|τ/T|
for |τ| < T
0
elsewhere.
(9.14)
It could be considered as a convolution of the rectangular window of
duration T with itself, since
[u(t + T/2) −u(t −T/2)] ∗t [u(t + T/2) −u(t −T/2)]
= (1 −|τ/T|)[u(t + T) −u(t −T)].
The Fourier transform of the triangular window is a product of two Fourier
transforms of the rectangular window of the width T,
WT(Ω) = 4sin2(ΩT/2)
Ω2
.
(9.15)
Convergence of this function toward zero as Ω→±∞is of the 1/Ω2 order.
It is a continuous function of time, with discontinuities in the ﬁrst derivative
at t = 0 and t = ±T. The mainlobe of this window function is twice wider
in the frequency domain than in the rectangular window case. Its width
follows from ΩT/2 = π as dΩ= 4π/T.
The discrete-time form is
w(n) =
-
1 −2|n|
N
.
[u(n + N/2) −u(n −N/2)].

Ljubiša Stankovi´c
Digital Signal Processing
531
In the frequency domain its form is
W(ejω) =
N/2−1
∑
n=−N/2
-
1 −2|n|
N
.
e−jωn = sin2(ωN/4)
sin2(ω/2) .
9.2.3
Hann(ing) Window
This window is of the form
w(τ) =
!
0.5(1 + cos(πτ/T))
for |τ| < T
0
elsewhere.
(9.16)
Since
cos(πτ/T) = [exp(jπτ/T) + exp(−jπτ/T)]/2, the Fourier
transform of this window is related to the Fourier transform of the rect-
angular window of the same width as
WH(Ω) = 1
2WR(Ω) + 1
4WR(Ω−π/T) + 1
4WR(Ω+ π/T)
=
π2 sin(ΩT)
Ω(π2 −Ω2T2).
(9.17)
The function WH(Ω) decays in frequency as Ω3, much faster than WR(Ω).
The discrete-time domain form is
w(n) = 0.5
-
1 + cos
*2πn
N
+.
[u(n + N/2) −u(n −N/2)]
with the DFT of the form
W(k) = N
2 δ(k) + N
4 δ(k + 1) + N
4 δ(k −1).
If the window is used on the data set from 0 to N −1 then
w(n) = 0.5
-
1 −cos
*2πn
N
+.
[u(n) −u(n −N)]
W(k) = N
2 δ(k) −N
4 δ(k + 1) −N
4 δ(k −1).
If a signal is multiplied by the Hann(ing) window the previous relation
also implies the relationship between the DFTs of the signal x(n) calculated
using the rectangular and Hann(ing) windows. The DFT of windowed

532
Time-Frequency Analysis
signal is moving average (smoothed) form of the original signal,
DFT{x(n)w(n)} = 1
N DFT{x(n)} ∗k DFT{w(n)}
= 1
4X(k + 1) + 1
2X(k) + 1
4X(k −1)
Example 9.4. Find the window that will correspond to the frequency smoothing
(X(k + 1) + X(k) + X(k −1))/3, i.e., to
DFT{x(n)w(n)} = 1
N DFT{x(n)} ∗k DFT{w(n)}
= 1
3 X(k + 1) + 1
3 X(k) + 1
3 X(k −1).
⋆The DFT of this window is
W(k) = N
3 δ(k) + N
3 δ(k + 1) + N
3 δ(k −1).
In the discrete-time domain the window form is
w(n) = 1
3
-
1 + 2cos
*2πn
N
+.
[u(n) −u(n −N)].
Example 9.5. Find the formula to calculate the STFT with a Hann(ing) window, if
the STFT calculated with a rectangular window is known.
⋆From the frequency domain STFT deﬁnition
STFT(t,Ω) = 1
2π
∞
"
−∞
X(θ)W(Ω−θ)ejtθdθ
easily follows that, if we use the window,
WH(Ω) = 1
2WR(Ω) + 1
4WR(Ω−π/T) + 1
4WR(Ω+ π/T),
then
STFTH(t,Ω) = 1
2STFTR(t,Ω)
(9.18)
+1
4STFTR
B
t,Ω−π
T
C
+ 1
4STFTR
B
t,Ω+ π
T
C
.
(9.19)

Ljubiša Stankovi´c
Digital Signal Processing
533
For the Hann(ing) window w(τ) of the width 2T, we may roughly
assume that its Fourier transform WH(Ω) is nonzero within the main lattice
| Ω|< 2π/T only, since the sidelobes decay very fast. Then we may write
dΩ= 4π/T. It means that the STFT is nonzero valued in the shaded regions
in Fig. 9.2.
We see that the duration in time of the STFT of a delta pulse is equal
to the widow width dt = 2T. The STFTs of two delta pulses (very short du-
ration signals) do not overlap in time-frequency domain if their distance
is greater than the window duration |t1 −t2| > dt. Then, these two pulses
can be resolved. Thus, the window width is here a measure of time res-
olution. Since the Fourier transform of the Hann(ing) window converges
fast, we can roughly assume that a measure of duration in frequency is
the width of its mainlobe, dΩ= 4π/T. Then we may say that the Fourier
transforms of two sinusoidal signals do not overlap in frequency if the con-
dition |Ω1 −Ω2| > dΩholds. It is important to observe that the product of
the window durations in time and frequency is a constant. In this example,
considering time domain duration of the Hann(ing) window and the width
of its mainlobe in the frequency domain, this product is dtdΩ= 8π. There-
fore, if we improve the resolution in the time domain dt, by decreasing T, we
inherently increase the value of dΩin the frequency domain. This essentially
prevents us from achieving the ideal resolution (dt = 0 and dΩ= 0) in both
domains. A general formulation of this principle, stating that the product of
effective window durations in time and in frequency cannot be arbitrarily
small, will be presented later.
9.2.4
Hamming Window
This window has the form
w(τ) =
!
0.54 + 0.46cos(πτ/T))
for |τ| < T
0
elsewhere.
(9.20)
A similar relation between the Hamming and the rectangular window
transforms holds, as in the case of Hann(ing) window.
The Hamming window was derived starting from
w(τ) = a + (1 −a)cos(πτ/T))
within |τ| < T, with
W(Ω) = a2sin(ΩT)
Ω
+ (1 −a)
*sin((Ω−π/T)T)
Ω−π/T
+ sin((Ω+ π/T)T)
Ω+ π/T
+
.

534
Time-Frequency Analysis
If we choose such a value of a to cancel out the second sidelobe at its
maximum (i.e., at ΩT ∼= 2.5π) then we get
0 = 2aT
2.5π −(1 −a)
*
T
1.5π +
T
3.5π
+
resulting in
a = 25/46 ∼= 0.54.
(9.21)
This window has several sidelobes, next to the mainlobe, lower than the
previous two windows. However, since it is not continuous at t = ±T, its
decay in frequency, as Ω→±∞, is not fast. Note that we let the mainlobe
to be twice wider than in the rectangular window case, so we cancel out not
the ﬁrst but the second sidelobe, at its maximum.
The discrete-time domain form is
w(n) =
-
0.54 + 0.46cos
*2πn
N
+.
[u(n + N/2) −u(n −N/2)]
with
W(k) = 0.54Nδ(k) + 0.23Nδ(k + 1) + 0.23Nδ(k −1).
9.2.5
Blackman and Kaiser Windows
In some applications it is crucial that the sidelobes are suppressed, as much
as possible. This is achieved by using windows of more complicated forms,
like the Blackman window. It is deﬁned by
w(τ) =
!
0.42 + 0.5cos(πτ/T) + 0.08cos(2πτ/T)
for |τ| < T
0
elsewhere.
(9.22)
This window is derived from
w(τ) = a0 + a1 cos(πτ/T) + a2 cos(2πτ/T)
with a0 + a1 + a2 = 1 and canceling out the Fourier transform values W(Ω)
at the positions of the third and the fourth sidelobe maxima (i.e., at ΩT ∼=
3.5π and ΩT ∼= 4.5π). Here, we let the mainlobe to be three times wider than
in the rectangular window case, so we cancel out not the ﬁrst nor the second
but the third and fourth sidelobes, at their maxima.

Ljubiša Stankovi´c
Digital Signal Processing
535
The discrete-time and frequency domain forms are
w(n) =
-
0.42 + 0.5cos
*2πn
N
+
+ 0.08cos
*4πn
N
+.
[u(n + N
2 ) −u(n −N
2 )]
W(k) = [0.42δ(k) + 0.25(δ(k + 1) + δ(k −1)) + 0.04(δ(k + 2) + δ(k −2))]N.
Further reduction of the sidelobes can be achieved by, for example,
the Kaiser (Kaiser-Bessel) window. It is an approximation to a restricted
time duration function with minimum energy outside the mainlobe. This
window is deﬁned by using the zero-order Bessel functions, with a localiza-
tion parameter. It has the ability to keep the maximum energy within the
mainlobe, while minimizing the sidelobe energy. The sidelobe level can be
as low −70 dB, as compared to the mainlobe, and even lower. This kind of
window is used in the analysis of signals with signiﬁcantly different ampli-
tudes, when the sidelobe of one component can be much higher than the
amplitude of the mainlobe of other components.
These are just a few of the windows used in signal processing. Some
windows, along with the corresponding Fourier transforms, are presented
in Fig. 9.6.
Example 9.6. Calculate the STFT at t = 0 with a Hamming and Blackman
window of the signals x1(t) = 2cos(4πt/T) + 2cos(12πt/T) and x2(t) =
2cos(4πt/T) + 0.001cos(64πt/T) with T = 128 using in numerical calcula-
tion ∆t = 1. Comment the results.
⋆The STFT at t = 0 is shown in Fig.9.7. The resolution of close compo-
nents in x1(t) is better with the Hann(ing) than with the Blackman window,
since the main lobe of the Blackman window is wider. Small signal in x2(t) is
visible in the STFT with the Blackman window since its side-lobes are much
lower than in the Hamming window.
9.2.6
Discrete Form and Realizations of the STFT
In numerical calculations the integral form of the STFT should be dis-
cretized. By sampling the signal with sampling interval ∆t we get
STFT(t,Ω) =
∞
"
−∞
x(t + τ)w(τ)e−jΩτdτ
≃
∞
∑
m=−∞
x((n + m)∆t)w(m∆t)e−jm∆tΩ∆t.

536
Time-Frequency Analysis
τ
w(τ)
Ω
W(Ω)
Ω
10 log|W(Ω)|
τ
w(τ)
Ω
W(Ω)
Ω
10 log|W(Ω)|
τ
w(τ)
Ω
W(Ω)
Ω
10 log|W(Ω)|
τ
w(τ)
Ω
W(Ω)
Ω
10 log|W(Ω)|
τ
w(τ)
Ω
W(Ω)
Ω
10 log|W(Ω)|
Figure 9.6
Windows in the time and frequency domains: rectangular window (ﬁrst row),
triangular (Bartlett) window (second row), Hann(ing) window (third row), Hamming window
(fourth row), and Blackman window (ﬁfth row).

Ljubiša Stankovi´c
Digital Signal Processing
537
-1
-0.5
0
0.5
1
10
-5
10
0
frequency Ω
|STFT(0,Ω)|
-1
-0.5
0
0.5
1
10
-5
10
0
frequency Ω
|STFT(0,Ω)|
-1
-0.5
0
0.5
1
10
-5
10
0
frequency Ω
|STFT(0,Ω)|
-1
-0.5
0
0.5
1
10
-5
10
0
frequency Ω
|STFT(0,Ω)|
Figure 9.7
The STFT at n = 0 calculated using the Hamming window (left) and the Blackman
window (right) of signals x1(n) (top) and signal x2(n) (bottom).
By denoting
x(n) = x(n∆t)∆t
and normalizing the frequency Ωby ∆t, ω = ∆tΩ, we get the time-discrete
form of the STFT as
STFT(n,ω) =
∞
∑
m=−∞
w(m)x(n + m)e−jmω.
(9.23)
We will use the same notation for continuous-time and discrete-time signals,
x(t) and x(n). However, we hope that this will not cause any confusion since
we will use different sets of variables, for example t and τ for continuous
time and n and m for discrete time. Also, we hope that the context will be
always clear, so that there is no doubt what kind of signal is considered.

538
Time-Frequency Analysis
It is important to note that STFT(n,ω) is periodic in frequency with
period 2π. The relation between the analog and the discrete-time form is
STFT(n,ω) =
∞
∑
k=−∞
STFT(n∆t,Ω+ 2kΩ0) with ω = ∆tΩ.
The sampling interval ∆t is related to the period in frequency as ∆t = π/Ω0.
According to the sampling theorem, in order to avoid the overlapping of the
STFT periods (aliasing), we should take
∆t = π
Ω0
≤π
Ωm
where Ωm is the maximal frequency in the STFT. Strictly speaking, the
windowed signal x(t + τ)w(τ) is time limited, thus it is not frequency
limited. Theoretically, there is no maximal frequency since the width of
the window’s Fourier transform is inﬁnite. However, in practice we can
always assume that the value of spectral content of x(t + τ)w(τ) above
frequency Ωm, i.e., for |Ω| > Ωm, can be neglected, and that overlapping
of the frequency content above Ωm does not degrade the basic frequency
period.
The discretization in frequency should be done by a number of sam-
ples greater than or equal to the window length N. If we assume that the
number of discrete frequency points is equal to the window length, then
STFT(n,k) = STFT(n,ω)|ω= 2π
N k =
N/2−1
∑
m=−N/2
w(m)x(n + m)e−j2πmk/N (9.24)
and it can be efﬁciently calculated using the fast DFT routines
STFT(n,k) = DFTm{w(m)x(n + m)},
for a given instant n. When the DFT routines with indices from 0 to N −1
are used, then a shifted version of w(m)x(n + m) should be formed for the
calculation for N/2 ≤m ≤N −1. It is obtained as w(m −N)x(n + m −N),
since in the DFT calculation periodicity of the signal w(m)x(n + m), with
period N, is inherently assumed.
Example 9.7. Consider a signal with M = 16 samples, x(0), x(1),...., x(15), write
a matrix form for the calculation of a four-sample STFT. Present nonoverlap-
ping and overlapping cases of the STFT calculation.

Ljubiša Stankovi´c
Digital Signal Processing
539
⋆For the calculation of (9.24) with N = 4, when k = −2,−1,0,1, for
given instant n, the following matrix notation can be used
⎡
⎢⎢⎣
STFT(n,−2)
STFT(n,−1)
STFT(n,0)
STFT(n,1)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
W4
4
W2
4
1
W−2
4
W2
4
W1
4
1
W−1
4
1
1
1
1
W−2
4
W−1
4
1
W1
4
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x(n −2)
x(n −1)
x(n)
x(n + 1)
⎤
⎥⎥⎦
or
STFT(n) = W4x(n)
with STFT(n) = [STFT(n,−2) STFT(n,−1) STFT(n,0) STFT(n,1)]T, x(n) =
[x(n −2) x(n −1) x(n) x(n + 1)]T, and W4 is the DFT matrix of order
four with elements Wmk
4
= exp(−j2πmk/N). Here a rectangular window
is assumed. Including the window function, the previous relation can be
written as
STFT(n)= W4H4x(n),
with
H4 =
⎡
⎢⎢⎣
w(−2)
0
0
0
0
w(−1)
0
0
0
0
w(0)
0
0
0
0
w(1)
⎤
⎥⎥⎦
being a diagonal matrix whose elements are the window values w(m),
H4 =diag(w(m)), m = −2,−1,0,1 and
W4H4 =
⎡
⎢⎢⎣
w(−2)W4
4
w(−1)W2
4
w(0)
w(1)W−2
4
w(−2)W2
4
w(−1)W1
4
w(0)
w(1)W−1
4
w(−2)
w(−1)
w(0)
w(1)
w(−2)W−2
4
w(−1)W−1
4
w(0)
w(1)W1
4
⎤
⎥⎥⎦.
All STFT values for the nonoverlapping case are obtained as
STFT = W4H4
⎡
⎢⎢⎣
x(0)
x(4)
x(8)
x(12)
x(1)
x(5)
x(9)
x(13)
x(2)
x(6)
x(10)
x(14)
x(3)
x(7)
x(11)
x(15)
⎤
⎥⎥⎦= W4H4X4,4
where STFT is a matrix of the STFT values with columns corresponding to
the calculation instants and the rows to the frequencies. This matrix is of the
form
STFT =
D
STFTM(0)
STFTM(M)
...
STFTM(N −M)
E
=
⎡
⎢⎢⎣
STFT(2,−2)
STFT(6,−2)
STFT(10,−2)
STFT(14,−2)
STFT(2,−1)
STFT(6,−1)
STFT(10,−1)
STFT(14,−1)
STFT(2,0)
STFT(6,0)
STFT(10,0)
STFT(14,0)
STFT(2,1)
STFT(6,1)
STFT(10,1)
STFT(14,1)
⎤
⎥⎥⎦.

540
Time-Frequency Analysis
Matrix X4,4 is formed of by using four successive signal values in each
column. Notation XN,R will be used to denote the signal matrix with columns
containing N signal values and the difference of the ﬁrst signal value indices
in the successive columns is R. For R = N the nonoverlapping calculation is
performed.
For a STFT calculation with overlapping R < N, for example with the
time step in the STFT calculation R = 1, we get
STFT = H4W4
⎡
⎢⎢⎣
x(0)
x(1)
x(2)
...
x(10)
x(11)
x(12)
x(1)
x(2)
x(3)
...
x(11)
x(12)
x(13)
x(2)
x(3)
x(4)
...
x(12)
x(13)
x(14)
x(3)
x(4)
x(5)
...
x(13)
x(14)
x(15)
⎤
⎥⎥⎦
STFT =W4H4X4,1.
The step R deﬁnes the difference of arguments in two neighboring columns.
In the ﬁrst case the difference of arguments in two neighboring columns was
4 (time step in the STFT calculation was R = 4 equal to the window width,
meaning nonoverlapped calculation). In the second example difference is
R = 1 < 4, meaning the overlapped STFT calculation. Note that the window
function HN and the DFT matrix WN remain the same for both cases.
Example 9.8. Consider a signal
x(t) = e−t2e−j6πt2−j32πt + e−4(t−1)2ej16πt2+j160πt.
Assuming that the values of the signal with amplitudes bellow 1/e4 could be
neglected, ﬁnd the sampling rate for the STFT-based analysis of this signal.
Write the approximate spectrogram expression for the Hann(ing) window of
N = 32 samples in the analysis. What signal will be presented in the time-
frequency plane, within the basic frequency period, if the signal is sampled
at ∆t = 1/128?
⋆The time interval, with signiﬁcant signal content, for the ﬁrst signal
component is −2 ≤t ≤2, with the frequency content within −56π ≤Ω≤
−8π, since the instantaneous frequency is Ω(t) = −12πt −32π. For the
second component these intervals are 0 ≤t ≤2 and 160π ≤Ω≤224π. The
maximal frequency in the signal is Ωm = 224π. Here we have to take into
account possible spreading of the spectrum caused by the lag window. Its
width in the time domain is dt = 2T = N∆t = 32∆t. Width of the mainlobe
in frequency domain dw is deﬁned by 32dw∆t = 4π, or Ωw = π/(8∆t).
Thus, taking the sampling interval ∆t = 1/256, we will satisfy the sampling
theorem condition in the worst instant case, since π/(Ωm + dw) = 1/256.
In the case of the Hann(ing) window with N = 32 and ∆t = 1/256,
the lag interval is N∆t = 1/8. We will assume that the amplitude variations
within the window are small, that is, w(τ)e−(t+τ)2 ∼= w(τ)e−t2 for −1/16 <
τ ≤1/16. Then, according to the stationary phase method, we can write the

Ljubiša Stankovi´c
Digital Signal Processing
541
STFT approximation,
|STFT(t,Ω)|2 = 1
6e−2t2w2 B
Ω+12πt+32π
12π
C
+ 1
32e−8(t−1)2w2 B
Ω−32πt−160π
32π
C
with t = n/256 and Ω= 256ω within −π ≤ω < π.
In the case of ∆t = 1/128 the signal will be periodically extended with
period 2Ω0 = 256π. The basic period will be for −128π ≤Ω< 128π. It means
that the ﬁrst component will remain unchanged within the basic period,
while the second component is outside the basic period. However, its replica
shifted for one period to the left, that is, for −256π, will be within the basic
period. It will be located within 160π −256π ≤Ω≤224π −256π, that is,
within −96π ≤Ω≤−32π. Thus, the signal represented by the STFT in this
case will correspond to
xr(t) = e−t2e−j6πt2−j32πt + e−4(t−1)2ej16πt2+j(160−256)πt,
with approximation,
|STFT(t,Ω)|2 = 1
6e−2t2w2 B
Ω+12πt+32π
12π
C
+ 1
32e−8(t−1)2w2 B
Ω−32πt−96π
32π
C
,
(9.25)
with t = n/128 and Ω= 128ω within −π ≤ω < π or −128π ≤Ω< 128π.
9.2.7
Recursive STFT Realization
For the rectangular window, the STFT values at an instant n can be calcu-
lated recursively from the STFT values at n −1, as
STFTR(n,k) = [x(n + N/2 −1) −x(n −N/2 −1)](−1)kej2πk/N
+ STFTR(n −1,k)ej2πk/N.
This recursive formula follows easily from the STFT deﬁnition (9.24).
For other window forms, the STFT can be obtained from the STFT
obtained by using the rectangular window. For example, according to (9.18)
the STFT with Hann(ing) window STFTH(n,k) is related to the STFT with
rectangular window STFTR(n,k) as
STFTH(n,k) = 1
2STFTR(n,k) + 1
4STFTR(n,k −1) + 1
4STFTR(n,k + 1).
This recursive calculation is important for hardware implementation of
the STFT and other related time-frequency representations (e.g., the higher
order representations implementations based on the STFT).

542
Time-Frequency Analysis
+
STFTR(n,k)
STFTR(n,k-1)
STFTR(n,k+1)
a-1
a1
a0
STFTH(n,k)
STFTR(n,k)
+
+
z- N
z-1
-1
(-1) k
e j2kπ/N
x(n+N/2-1)
Figure 9.8
Recursive implementation of the STFT for the rectangular and other windows.
A system for the recursive implementation of the STFT is shown in
Fig. 9.8. The STFT obtained by using the rectangular window is denoted by
STFTR(n,k), Fig.9.8, while the values of coefﬁcients are
(a−1,a0,a1) = (1
4, 1
2, 1
4),
(a−1,a0,a1) = (0.23,0.54,0.23),
(a−2,a−1,a0,a1,a2) = (0.04,0.25,0.42,0.25,0.04)
for the Hann(ing), Hamming and Blackman windows, respectively.
Note that in general instead of multiplying the signal by the previous
window functions, for each calculation instant n, the STFT matrix STFT can
be calculated without window multiplication (using a rectangular window).
The STFT matrix for the Hann(ing) window, for example, is obtained as
STFTH = 0.5STFT+0.25STFT↓+ 0.25STFT↑, where STFT↓and STFT↑
are the STFT matrices with circularly shifted rows down and up for one
position, respectively.
9.2.8
Filter Bank STFT Implementation
According to (9.1), the STFT can be written as a convolution

Ljubiša Stankovi´c
Digital Signal Processing
543
x(n)
w(n)
STFT(n,0)
↓R
w(n) e j2πn/N
STFT(n,1)
↓R
w(n) e j2πn(N-1)/ N
STFT(n,N-1)
↓R
. . .
Figure 9.9
Filter bank realization of the STFT
STFT(t,Ω) =
∞
"
−∞
x(t + τ)w(τ)e−jΩτ dτ
=
∞
"
−∞
x(t −τ)w(τ)ejΩτdτ = x(t) ∗t
@
w(t)ejΩtA
where an even, real valued, window function is assumed, w(τ) = w(−τ).
For a discrete set of frequencies Ωk = k∆Ω= 2πk/(N∆t), k = 0,1,2,..., N −1,
and discrete values of signal, we get that the discrete STFT, (9.24), is an
output of the ﬁlter bank with impulse responses
STFT(n,k) = x(n) ∗n
@
w(n)ej2πkn/NA
= x(n) ∗n hk(n)
hk(n) = w(n)ej2πkn/N
k = 0,1,..., N −1
what is illustrated in Fig.9.9. The next STFT can be calculated with time step
R∆t, meaning downsampling in time with factor 1 ≤R ≤N. Two special
cases are: no downsampling, R = 1, and nonoverlapping calculation, R = N.
Inﬂuence of R to the signal reconstruction will be discussed later.

544
Time-Frequency Analysis
9.2.8.1
Overlapping windows
Nonoverlapping cases are important and easy for analysis. They also keep
the number of the STFT coefﬁcients equal to the number of the signal sam-
ples. However, the STFT is commonly calculated using overlapping win-
dows. There are several reasons for introducing overlapped STFT repre-
sentations. Rectangular windows have poor localization in the frequency
domain. The localization is improved by other window forms. In the case of
nonrectangular windows some of the signal samples are weighted in such
a way that their contribution to the ﬁnal representation is small. Then we
want to use additional STFT with a window positioned in such a way that
these samples contribute more to the STFT calculation. Also, in the param-
eters estimation and detection the task is to achieve the best possible esti-
mation or detection for each time instant instead of using interpolations for
the skipped instants when the STFT with a big step (equal to the window
width) is calculated. Commonly, the overlapped STFTs are calculated using,
for example, rectangular, Hann(ing), Hamming, Bartlett, Kaiser, or Black-
man window of a constant window width N with steps N/2, N/4, N/8,...
in time. Computational cost is increased in the overlapped STFTs since more
STFTs are calculated. A way of composing STFTs calculated with a rectan-
gular window into a STFT with, for example, the Hann(ing), Hamming, or
Blackman window, is presented in Fig.9.8.
If a signal x(n) is of duration M, in some cases in addition to the
overlapping in time, an interpolation in frequency is done, for example up
to the DFT grid with M samples. The overlapped and interpolated STFT of
this signal is calculated, using a window w(m) whose width is N ≤M, as
STFTN(n,k) =
N/2−1
∑
m=−N/2
w(m)x(n + m)e−j2πmk/M
n = N/2 + 1, N/2 + 2,.., M −N/2
k = −M/2,−M/2 + 1,...,−1,0,1,..., M/2 −1.
Example 9.9. The STFT calculation of a signal whose frequency changes linearly is
done by using a rectangular window. Signal samples within 0 ≤n ≤M −1
with M = 64 were available. The nonoverlapping STFT of this signal is
calculated with a rectangular window of the width N = 8 and presented in
Fig.9.10. The nonoverlapping STFT values obtained by using the rectangular
window are shifted in frequency, scaled, and added up, Fig. 9.11, to produce
the STFT with a Hamming window, Fig. 9.12.
The STFT calculation for the same linear FM signal will be repeated for
the overlapping STFT with step R = 1. Results for the rectangular and Ham-
ming window (obtained by a simple matrix calculation from the rectangular

Ljubiša Stankovi´c
Digital Signal Processing
545
0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
32
34
36
38
40
42
44
46
48
50
52
54
56
58
60
62
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
-32
-31
-30
-29
-28
-27
-26
-25
-24
-23
-22
-21
-20
-19
-18
-17
-16
-15
-14
-13
-12
-11
-10
-9
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
S8(4,-4)
S8(4,-3)
S8(4,-2)
S8(4,-1)
S8(4,0)
S8(4,1)
S8(4,2)
S8(4,3)
S8(12,-4)
S8(12,-3)
S8(12,-2)
S8(12,-1)
S8(12,0)
S8(12,1)
S8(12,2)
S8(12,3)
S8(20,-4)
S8(20,-3)
S8(20,-2)
S8(20,-1)
S8(20,0)
S8(20,1)
S8(20,2)
S8(20,3)
S8(28,-4)
S8(28,-3)
S8(28,-2)
S8(28,-1)
S8(28,0)
S8(28,1)
S8(28,2)
S8(28,3)
S8(36,-4)
S8(36,-3)
S8(36,-2)
S8(36,-1)
S8(36,0)
S8(36,1)
S8(36,2)
S8(36,3)
S8(44,-4)
S8(44,-3)
S8(44,-2)
S8(44,-1)
S8(44,0)
S8(44,1)
S8(44,2)
S8(44,3)
S8(52,-4)
S8(52,-3)
S8(52,-2)
S8(52,-1)
S8(52,0)
S8(52,1)
S8(52,2)
S8(52,3)
S8(60,-4)
S8(60,-3)
S8(60,-2)
S8(60,-1)
S8(60,0)
S8(60,1)
S8(60,2)
S8(60,3)
STFT with rectangular window
Figure 9.10
The STFT of a linear FM signal x(n) calculates using a rectangular window of the
width N = 8.
window case) are presented in Fig.9.13. Three window widths are used here.
The same procedure is repeated with the windows zero padded up to the
widest used window (interpolation in frequency). The results are presented
in Fig.9.14. Note that regarding to the amount of information all these ﬁg-
ures do not differ from the basic time-frequency representation presented in
Fig.9.10.

546
Time-Frequency Analysis
STFTR(n,k-1)
0.23 STFTR(n,k-1)
+
STFTR(n,k)
0.54 STFTR(n,k)
+
= STFTH(n,k)
STFTR(n,k+1)
0.23 STFTR(n,k+1)
Figure 9.11
The STFT of a linear FM signal calculated using a rectangular window (from the
previous ﬁgure), along with its frequency shifted versions STFTR(n,k −1) and STFTR(n,k −
1). Their weighted sum produces the STFT of the same signal with a Hamming window
STFTH(n,k).
9.2.9
Signal Reconstruction from the Discrete STFT
Signal reconstruction from non-overlapping STFT values is obvious for a
rectangular window. A simple illustration is presented in Fig.9.15. Win-
dowed signal values are reconstructed from the STFTs by a simple inversion
of each STFT
STFT(n) = WNHwx(n)
Hwx(n) = IDFT{STFT(n)} = W−1
N STFT(n)
where Hw is a diagonal matrix with the window values as its elements,
Hw = diag(w(m)).
Example 9.10. Consider a signal with M = 16 samples, x(0), x(1),...., x(16). Write
a matrix form for the signal inversion using a four-sample STFT (N = 16)
calculated with the rectangular and a Hann(ing) window: (a) Without over-
lapping, R = 16. (b) With a time step in the STFT calculation of R = 2.
⋆(a) For the nonoverlapping case the STFT calculation is done accord-
ing to:
STFT = W4H4
⎡
⎢⎢⎣
x(0)
x(4)
x(8)
x(12)
x(1)
x(5)
x(9)
x(13)
x(2)
x(6)
x(10)
x(14)
x(3)
x(7)
x(11)
x(15)
⎤
⎥⎥⎦.
with H4 =diag([w(−2) w(−1) w(0) w(1)]) and W4 is the corresponding four
sample DFT matrix.

Ljubiša Stankovi´c
Digital Signal Processing
547
0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
32
34
36
38
40
42
44
46
48
50
52
54
56
58
60
62
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
37
39
41
43
45
47
49
51
53
55
57
59
61
63
-32
-31
-30
-29
-28
-27
-26
-25
-24
-23
-22
-21
-20
-19
-18
-17
-16
-15
-14
-13
-12
-11
-10
-9
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
S8(4,-4)
S8(4,-3)
S8(4,-2)
S8(4,-1)
S8(4,0)
S8(4,1)
S8(4,2)
S8(4,3)
S8(12,-4)
S8(12,-3)
S8(12,-2)
S8(12,-1)
S8(12,0)
S8(12,1)
S8(12,2)
S8(12,3)
S8(20,-4)
S8(20,-3)
S8(20,-2)
S8(20,-1)
S8(20,0)
S8(20,1)
S8(20,2)
S8(20,3)
S8(28,-4)
S8(28,-3)
S8(28,-2)
S8(28,-1)
S8(28,0)
S8(28,1)
S8(28,2)
S8(28,3)
S8(36,-4)
S8(36,-3)
S8(36,-2)
S8(36,-1)
S8(36,0)
S8(36,1)
S8(36,2)
S8(36,3)
S8(44,-4)
S8(44,-3)
S8(44,-2)
S8(44,-1)
S8(44,0)
S8(44,1)
S8(44,2)
S8(44,3)
S8(52,-4)
S8(52,-3)
S8(52,-2)
S8(52,-1)
S8(52,0)
S8(52,1)
S8(52,2)
S8(52,3)
S8(60,-4)
S8(60,-3)
S8(60,-2)
S8(60,-1)
S8(60,0)
S8(60,1)
S8(60,2)
S8(60,3)
STFT with Hamming window
Figure 9.12
The STFT of a linear FM signal x(n) calculated using the Hamming window with
N = 8. Calculation is illustrated in the previous ﬁgure.
The inversion relation is
⎡
⎢⎢⎣
x(0)
x(4)
x(8)
x(12)
x(1)
x(5)
x(9)
x(13)
x(2)
x(6)
x(10)
x(14)
x(3)
x(7)
x(11)
x(15)
⎤
⎥⎥⎦= H−1
4 W−1
4 STFT
where the elements of diagonal matrix H−1
4
are proportional to 1/w(m),
H−1
4
=diag([1/w(−2) 1/w(−1) 1/w(0) 1/w(1)]). If a rectangular window
is used in the STFT calculation then H−1
4
= I4 is unity matrix and this kind of

548
Time-Frequency Analysis
STFT with rectangular window, N=48
STFT with Hamming window, N=48
STFT with rectangular window, N=16
STFT with Hamming window, N=16
STFT with rectangular window, N=8
STFT with Hamming window, N=8
Figure 9.13
Time-frequency analysis of a linear frequency modulated signal with overlapping
windows of various widths. Time step in the STFT calculation is R = 1.

Ljubiša Stankovi´c
Digital Signal Processing
549
STFT with rectangular window, N=48
STFT with Hamming window, N=48
STFT with rectangular window, N=16
STFT with Hamming window, N=16
STFT with rectangular window, N=8
STFT with Hamming window, N=8
Figure 9.14
Time-frequency analysis of a linear frequency modulated signal with overlapping
windows of various widths. Time step in the STFT calculation is R = 1. For each window
width the frequency axis is interpolated (signal in time is zero padded) up to the total number
of available signal samples M = 64.

550
Time-Frequency Analysis
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S4(2,-2)
S4(2,-1)
S4(2,0)
S4(2,1)
S4(6,-2)
S4(6,-1)
S4(6,0)
S4(6,1)
S4(10,-2)
S4(10,-1)
S4(10,0)
S4(10,1)
S4(14,-2)
S4(14,-1)
S4(14,0)
S4(14,1)
STFT(2,k)
x(0), x(1), x(2), x(3)
x(2+m)w(m)=
IDFT{STFT(2,k)}
m=-2,-1,0,1
STFT(6,k)
x(4), x(5), x(6), x(7)
x(6+m)w(m)=
IDFT{STFT(6,k)}
m=-2,-1,0,1
STFT(10,k)
x(8), x(9),x(10),x(11)
x(10+m)w(m)=
IDFT{STFT(10,k)}
m=-2,-1,0,1
STFT(14,k)
x(12),x(13),x(14),x(15)
x(14+m)w(m)=
IDFT{STFT(14,k)}
m=-2,-1,0,1
Figure 9.15
Illustration of the signal reconstruction from the STFT with nonoverlapping
windows.

Ljubiša Stankovi´c
Digital Signal Processing
551
calculation can be used. However if a nonrectangular window is used then
some of the window values are quite small. The signal value is then obtained
by multiplying the inverse DFT with large values 1/w(m). This kind of
division with small values is very imprecise, if any noise in the reconstructed
signal is expected. In the Hann(ing) window case the ending point is even
zero-valued, so 1/w(m) does not exist.
(b) The STFT calculation is done with overlapping with step R = 2,
Fig.9.16. For N = 4 and calculation step R = 2 the STFT calculation corre-
sponds to
STFT = W4H4
⎡
⎢⎢⎣
0
x(0)
x(2)
x(4)
x(6)
x(8)
x(10) x(12) x(14)
0
x(1)
x(3)
x(5)
x(7)
x(9)
x(11) x(13) x(15)
x(0)
x(2)
x(4)
x(6)
x(8)
x(10) x(12) x(14) 0
x(1)
x(3)
x(5)
x(7)
x(9)
x(11) x(13) x(15) 0
⎤
⎥⎥⎦
The inversion is
W−1
4 STFT = H4X =
⎡
⎢⎢⎣
0
x(0)w(−2)
x(2)w(−2)
x(4)w(−2)
...
x(14)w(−2)
0
x(1)w(−1)
x(3)w(−1)
x(5)w(−1)
...
x(15)w(−1)
x(0)w(0)
x(2)w(0)
x(4)w(0)
x(6)w(0)
...
0
x(1)w(1)
x(3)w(1)
x(5)w(1)
x(7)w(1)
...
0
⎤
⎥⎥⎦
where X is the matrix with signal elements. The window matrix is left
on the right side, since in general it may be not invertible. By calculating
W−1
4 STFT
we can then recombine the signal values. For example, the
element producing x(0)w(0) in the ﬁrst column is combined with the element
producing x(0)w(−2) in the second column to get x(0)w(0) + x(0)w(−2) =
x(0), since for the Hann(ing) window of the width N holds w(n) + w(n −
N/2) = 1. The same is done for other signal values in the matrix obtained
after inversion,
x(0)w(0) + x(0)w(−2) = x(0)
x(1)w(1) + x(1)w(−1) = x(1)
x(2)w(0) + x(1)w(−2) = x(2)
...
x(15)w(1) + x(15)w(−1) = x(15)
Note that the same relation would hold for a triangular window, while for a
Hamming window a similar relation would hold, with w(n) + w(n −N/2) =
1.08. The results should be corrected in that case, by a constant factor of 1.08.
Illustration of the STFT calculation for an arbitrary window width N
at n = n0 is presented in Fig.9.16. Its inversion produces x(n0 + m)w(m) =
IDFT{STFTN(n0,k)}. Consider the pervious STFT value in the case of
nonoverlapping windows. It would be STFTN(n0 −N,k). Its inverse
IDFT{STFTN(n0 −N,k)} = x(n0 −N + m)w(m)

552
Time-Frequency Analysis
is also presented in Fig.9.16. As it can be seen, by combining these two inverse
transforms we would get signal with very low values around n = n0 −N/˙2.
If one more STFT is calculated at n = n0 −N/2 and its inverse combined with
previous two it will improve the signal presentation within the overlapping
region n0 −N ≤n < n0. In addition for the most of common windows
w(m −N) + w(m −N/2) + w(m) = 1 (or a constant) within 0 ≤m < N
meaning that the sum of overlapped inverse STFTs, as in Fig.9.16, will give
the original signal within n0 −N ≤n < n0.
In general, let us consider the STFT calculation with overlapping
windows. Assume that the STFTs are calculated with a step 1 ≤R ≤N
in time. Available STFT values are
...
STFT(n0 −2R),
STFT(n0 −R),
(9.26)
STFT(n0),
STFT(n0 + R),
STFT(n0 + 2R),
...
Based on the available STFT values (9.26), the windowed signal values can
be reconstructed as
Hwx(n0 + iR) = W−1
N STFT(n0 + iR),
i = ... −2,−1,0,1,2,...
For m = −N/2,−N/2 + 1,..., N/2 −1 we get signal values x(n0 + iR + m)
w(m)x(n0 + iR + m) = 1
N
N/2−1
∑
k=−N/2
STFT(n0 + iR,k)ej2πmk/N.
(9.27)
Since R < N we we will get the same signal value within different STFT, for
different i. For example, for N = 8, R = 2 and n0 = 0 we will get the value
x(0) for m = 0 and i = 0, but also for m = −2 and i = 1 or m = 2 and i = −1,
and son on. Then in the reconstruction we should use all these values to get
the most reliable reconstruction.
Let us reindex the reconstructed signal values (9.27) by substitution
m = l −iR
w(l −iR)x(n0 + l) = 1
N
N/2−1
∑
k=−N/2
STFT(n0 + iR,k)ej2πlk/Ne−j2πiRk/N
−N/2 ≤l −iR ≤N/2 −1.

Ljubiša Stankovi´c
Digital Signal Processing
553
n
m
m
x(n) 
w(m) 
x(n0+m)w(m) 
w(m) 
x(n0- N+m)w(m) 
n0
n0-N
n0-N/2
w(m) 
x(n0- N/2+m)w(m) 
m
n
x(n)w(n-n 0+N/2) 
n 
x(n)w(n-n 0+N)+x(n)w(n-n 0+N/2)+x(n)w(n-n 0) 
Figure 9.16
Illustration of the STFT calculation with windows overlapping in order to pro-
duce an inverse STFT whose sum will give the original signal within n0 −N ≤n < n0.

554
Time-Frequency Analysis
If R < N then a value of signal x(n0 + l) will be obtained by inverting
w(l)x(n0 + l) = 1
N
N/2−1
∑
k=−N/2
STFT(n0,k)ej2πlk/N
but also it will be obtained within the inversions
...
w(l −2R)x(n0 + l) = 1
N
N/2−1
∑
k=−N/2
STFT(n0 + 2R,k)ej2πlk/Ne−j2π2Rk/N
w(l −R)x(n0 + l) = 1
N
N/2−1
∑
k=−N/2
STFT(n0 + R,k)ej2πlk/Ne−j2πRk/N
w(l + R)x(n0 + l) = 1
N
N/2−1
∑
k=−N/2
STFT(n0 −R,k)ej2πlk/Nej2πRk/N
w(l + 2R)x(n0 + l) = 1
N
N/2−1
∑
k=−N/2
STFT(n0 −2R,k)ej2πlk/Nej2π2Rk/N
...
as far as w(l −2iR), for i = 0,±1,±2,... is within
−N/2 ≤l −2iR < N/2.
By summing all reconstructions over i satisfying −N/2 ≤l −iR ≤N/2 −1
we get the reconstructed signal x(n0 + l). It is undistorted (up to a constant)
if
c(l) = ∑
i
w(l −iR) = const. = C
(9.28)
since
∑
i
w(l −iR)x(n0 + l) = Cx(n0 + l)
for any n0 and l. Note that ∑i w(l −iR) is a periodic extension of w(l) with
a period R. If W(ejω) is the Fourier transform of w(l) then the Fourier
transform of its periodic extension is equal to the samples of W(ejω) at
ω = 2πk/R. The condition (9.28) is equivalent to
W(ej2πk/R) = CNδ(k) for k = 0,1,...,R −1.
Special cases:

Ljubiša Stankovi´c
Digital Signal Processing
555
 x(n)
 x(n-7) 
w(-4) 
w(-4)  x(n-7) 
  STFT(n-3,7)
↓
 x(n-6) 
w(-3) 
w(-3)  x(n-6) 
z-1
  STFT(n-3,6)
↓
 x(n-5) 
w(-2) 
w(-2)  x(n-5) 
z-1
  STFT(n-3,5)
↓
 x(n-4) 
w(-1) 
w(-1)  x(n-4) 
z-1
  STFT(n-3,4)
↓
 x(n-3) 
w(0) 
w(0) x(n-3) 
z-1
  STFT(n-3,3)
↓
 x(n-2) 
w(1) 
w(1) x(n-2) 
z-1
  STFT(n-3,2)
↓
 x(n-1) 
w(2) 
w(2) x(n-1) 
z-1
  STFT(n-3,1)
↓
 x(n-0) 
w(3) 
w(3) x(n-0) 
z-1
  STFT(n-3,0)
↓
R=N/2=4
N/2
STFT
(DFT)
  ..., STFT(n-7,k),  STFT(n-3,k),  STFT(n+1,k), ...
IDFT
z-4
+
 x(n-7) 
z-4
+
 x(n-6) 
z-4
+
 x(n-5) 
z-4
+  x(n-4) 
Figure 9.17
Signal reconstruction from the STFT for the case N = 8, when the STFT is
calculated with step R = N/2 = 4 and the window satisﬁes w(m) + w(m −N/2) = 1. This
is the case for the rectangular, Hann(ing), Blackman and triangular windows. The same holds
for the Hamming window up to a constant scaling factor of 1.08.
1. For R = N (nonoverlapping), relation (9.28) is satisﬁed for the rectan-
gular window, only.
2. For a half of the overlapping period, R = N/2, condition (9.28) is met
for the rectangular, Hann(ing), Hamming, and triangular window.
Realization in this case for N = 8 and R = N/2 = 4 is presented in
Fig.9.17. Signal values with a delay of N/2 = 4 samples are obtained at
the exit. The STFT calculation process is repeated after each 4 samples,
producing blocks of 4 signal samples at the output.
3. The same holds for R = N/2, N/4, N/8, if the values of R are integers.
4. For R = 1, (the STFT calculation in each available time instant), any
window satisﬁes the inversion relation. In this case we may also use a

556
Time-Frequency Analysis
simple reconstruction formula, Fig.9.18
1
N
N/2−1
∑
k=−N/2
STFT(n,k) = 1
N
N/2−1
∑
m=−N/2
(
w(m)x(n + m)
N/2−1
∑
k=−N/2
e−j2πmk/N
)
= w(0)x(n).
Very efﬁcient realizations, for this case, are the recursive ones, instead
of the direct DFT calculation, Fig.9.8.
In analysis of non-stationary signals our primary interest is not in
signal reconstruction with the fewest number of calculation points. Rather,
we are interested in tracking signals’ non-stationary parameters, like for
example, instantaneous frequency. These parameters may signiﬁcantly vary
between neighboring time instants n and n + 1. Quasi-stationarity of signal
within R samples (implicitly assumed when down-sampling by factor of R
is done) in this case is not a good starting point for the analysis. Here, we
have to use the time-frequency analysis of signal at each instant n, without
any down-sampling.
9.2.10
Time-Varying Windows
In general, varying window widths could be used for different time-
frequency points. When Ni changes with ni we have the case of a time-
varying window. Assuming a rectangular window we can write,
STFTNi(ni,k) =
Ni/2−1
∑
m=−Ni/2
x(ni + m)e−j 2π
Ni mk
(9.29)
Notation STFTNi(n,k) means that the STFT is calculated using signal sam-
ples within the window [ni −Ni/2,ni + Ni/2 −1] for −Ni/2 ≤k ≤Ni/2 −1,
corresponding to an even number of Ni discrete frequencies from −π to π.
For an odd Ni, the summation limits are ±(Ni −1)/2. Let us restate that
a wide window includes signal samples over a wide time interval, losing
the possibility to detect fast changes in time, but achieving high frequency
resolution. A narrow window in the STFT will track time changes, but with
a low resolution in frequency. Two extreme cases are Ni = 1 when
STFT1(n,k) = x(n)
and Ni = M when
STFTM(n,k) = X(k),

Ljubiša Stankovi´c
Digital Signal Processing
557
 x(n)
 x(n-7) 
w(-4) 
  STFT(n-3,7)
 x(n-6) 
w(-3) 
z-1
  STFT(n-3,6)
 x(n-5) 
w(-2) 
z-1
  STFT(n-3,5)
 x(n-4) 
w(-1) 
z-1
  STFT(n-3,4)
 x(n-3) 
w(0) 
z-1
  STFT(n-3,3)
 x(n-2) 
w(1) 
z-1
  STFT(n-3,2)
 x(n-1) 
w(2) 
z-1
  STFT(n-3,1)
 x(n-0) 
w(3) 
z-1
  STFT(n-3,0)
STFT
(DFT)
+
 x(n-3)
 1/(Nw(0))
Figure 9.18
Signal reconstruction when the STFT is calculated with step R = 1.
where M is the total number of all available signal samples and X(k) =
DFT{x(n)}.
In vector notation
STFTNi(ni) = WNixNi(ni),
where STFTNi(ni) and xNi(ni) are column vectors. Their elements are
STFTNi(ni,k), k = −Ni/2,..., Ni/2 −1 and x(ni + m), m = −Ni/2,..., Ni/2 −
1, respectively
STFTNi(ni) = [STFTNi(ni,−Ni/2) ...STFTNi(ni, Ni/2 −1)]T
xNi(ni) = [x(ni −Ni/2) ...x(ni + Ni/2 −1)]T.
Matrix WNi is an Ni × Ni DFT matrix with elements
WNi(m,k) = exp(−j2πmk/Ni),

558
Time-Frequency Analysis
where m is the column index and k is the row index of the matrix. The STFT
value STFTNi(ni,k) is presented as a block in the time-frequency plane of
the width Ni in the time direction, covering all time instants [ni −Ni/2,ni +
Ni/2 −1] used in its calculation. The frequency axis can be labeled with the
DFT indices p = −M/2,..., M/2 −1 corresponding to the DFT frequencies
2πp/M (dots in Fig.9.19). With respect to this axis labeling, the block
STFTNi(ni,k) will be positioned at the frequency 2πk/Ni = 2π(kM/Ni)/M,
i.e., at p = kM/Ni. The block width in frequency is M/Ni DFT samples.
Therefore the block area in time and DFT frequency is always equal to
the number of all available signal samples M as shown in Fig.9.19 where
M = 16.
Example 9.11. Consider a signal x(n) with M = 16 samples. Write the expression
for calculation of the STFT value STFT4(2,1) with a rectangular window.
Indicate graphically the region of time instants used in the calculation and
the frequency range in terms of the DFT frequency values included in the
calculation of STFT4(2,1)?
⋆The STFT value STFT4(2,1) is:
STFT4(2,1) =
1
∑
m=−2
x(2 + m)e−j 2π
4 m.
It uses discrete-time samples of x(n) within
−2 ≤2 + m < 1
0 ≤n ≤3.
The frequency term is exp(−j2πm/4). For the DFT of a signal with M = 16
X(k) =
15
∑
n=0
x(n)e−j 2π
16 mk
k = −8,−7,... −1,0,1,...,6,7
this frequency would correspond to the term exp(−j2π4m/16). Therefore
k = 1 corresponds to the frequency p = 4 in the DFT. Since the whole fre-
quency range −π ≤ω < π in the case of Ni = 4 is covered with 4 STFT val-
ues STFT4(2,−2), STFT4(2,−1), STFT4(2,0), and STFT4(2,1) and the same
frequency range in the DFT has 16 frequency samples, it means that each
STFT value calculated with Ni = 4 corresponds to a range of frequencies cor-
responding to 4 DFT values,
k = −2, corresponds to p = −8,−7,−6,−5
k = −1, corresponds to p = −4,−3,−2,−1
k = 0, corresponds to p = 0,1,2,3
k = 1, corresponds to p = 4,5,6,7.

Ljubiša Stankovi´c
Digital Signal Processing
559
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S4(2,-2)
S4(2,-1)
S4(2,0)
S4(2,1)
S4(6,-2)
S4(6,-1)
S4(6,0)
S4(6,1)
S4(10,-2)
S4(10,-1)
S4(10,0)
S4(10,1)
S4(14,-2)
S4(14,-1)
S4(14,0)
S4(14,1)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S2(1,-1)
S2(1,0)
S4(4,-2)
S4(4,-1)
S4(4,0)
S4(4,1)
S4(8,-2)
S4(8,-1)
S4(8,0)
S4(8,1)
S2(11,-1)
S2(11,0)
S2(13,-1)
S2(13,0)
S1(14,0)
S1(15,0)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S2(1,-1)
S2(1,0)
S2(3,-1)
S2(3,0)
S2(5,-1)
S2(5,0)
S2(7,-1)
S2(7,0)
S2(9,-1)
S2(9,0)
S2(11,-1)
S2(11,0)
S2(13,-1)
S2(13,0)
S2(15,-1)
S2(15,0)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S2(1,-1)
S2(1,0)
S2(3,-1)
S2(3,0)
S4(6,-2)
S4(6,-1)
S4(6,0)
S4(6,1)
S8(12,-4)
S8(12,-3)
S8(12,-2)
S8(12,-1)
S8(12,0)
S8(12,1)
S8(12,2)
S8(12,3)
Figure 9.19
The nonoverlapping STFTs with: (a) constant window of the width N = 4, (b)
constant window of the width N = 2, (c)-(d) time-varying windows. Time index is presented
on the horizontal axis, while the DFT frequency index is shown on the vertical axis (the STFT
is denoted by S for notation simplicity).
This discrete-time and the DFT frequency region, 0 ≤n ≤3 and 4 ≤p ≤7, is
represented by a square denoted by S4(2,1) in Fig.9.19(a).
In a nonoverlapping STFT, covering all signal samples
x =[x(0),x(1),...,x(M −1)]T
with STFTNi(ni), the STFT should be calculated at n0 = N0/2, n1 = N0 +
N1/2, n2 = N0 + N1 + N2/2,..., nK = M −NK/2. A matrix form for all STFT

560
Time-Frequency Analysis
values is
STFT =
⎡
⎢⎢⎢⎣
WN0
0
···
0
0
WN1
···
0
...
...
...
...
0
0
···
WNK
⎤
⎥⎥⎥⎦x
STFT= ˜Wx = ˜WW−1
M X,
(9.30)
where STFT is a column vector containing all STFT vectors STFTNi(ni),
i = 0,1,..., K, X = WMx is a DFT of the whole signal x(n), while ˜W is a block
matrix (M × M) formed from the smaller DFT matrices WN0, WN1, ...,WNK,
as in (9.29). Since the time-varying nonoverlapping STFT corresponds to a
decimation-in-time DFT scheme, its calculation is more efﬁcient than the
DFT calculation of the whole signal. Illustration of time-varying window
STFTs is shown in Fig.9.19(c), (d). For a signal with M samples, there
is a large number of possible nonoverlapping STFTs with a time-varying
window Ni ∈{1,2,3,..., M}. The exact number will be derived later.
Example 9.12. Consider a signal x(n) with M = 16 samples, whose values are
x = [0.5, 0.5, −0.25, j0.25, 0.25, −j0.25, −0.25, 0.25, −0.25, 0.25, 0.5, 0.5,
−j0.5, j0.5, 0, −1]. Some of its nonoverlapping STFTs are calculated according
to (9.29) and shown in Fig.9.19. Different representations can be compared
based on the concentration measures, for example,
µ[STFTN(n,k)] = ∑
n ∑
k
|STFTN(n,k)| = ∥STFT∥1 .
The best STFT representation, in this sense, would be the one with the small-
est µ[STFTN(n,k)]. For the considered signal and its four representations
shown in Fig.9.19 the best representation, according to this criterion, is the
one shown in Fig.9.19(b).
Example 9.13. Consider a signal x(n) with M = 8 samples. Its values are x(0) = 0,
x(1) = 1, x(2) = 1/2, x(3) = −1/2, x(4) = 1/4, x(5) = −j/4, x(6) = −1/4,
and x(7) = j/4.
(a) Calculate the STFTs of this signal with rectangular window of the
widths N = 1, N = 2, N = 4. Use the following STFT deﬁnition
STFTN(n,k) =
N/2−1
∑
m=−N/2
x(n + m)e−j2πmk/N.
For an odd N, the summation limits are ±(N −1)/2. Calculate STFT1(n,k)
for n = 0,1,2,3,4,5,6,7, then STFT2(n,k) for n = 1,3,5,7, then STFT4(n,k)
for n = 2,6
and STFT8(n,k)
for n = 4. For frequency axis use notation
k = 0,1,2,3,4,5,6,7.

Ljubiša Stankovi´c
Digital Signal Processing
561
(b) Assuming that time-varying approach is used in the nonoverlap-
ping STFT calculation, ﬁnd the total number of possible representations.
(c) Calculate the concentration measure for each of the cases in (b)
and ﬁnd the representation (nonoverlapping combination of previous STFTs)
when the signal is represented with the smallest number of coefﬁcients. Does
it correspond to the minimum of µ[STFT(n,k)]?
⋆(a) The STFT values are:
– for N = 1
STFT1(n,0) = x(n), for all n = 0,1,2,3,4,5,6,7;
– for N = 2
STFT2(n,0) = x(n) + x(n −1)
STFT2(1,0) = 1,
STFT2(3,0) = 0,
STFT2(5,0) = (1 −j)/4,
STFT2(7,0) = (−1 + j)/4
STFT2(n,1) = x(n) −x(n −1)
STFT2(1,1) = 1,
STFT2(3,1) = −1,
STFT2(5,1) = (−1 −j)/4,
STFT2(7,1) = (1 + j)/4
– for N = 4 and n = 2, 6
STFT4(n,0) = x(n −2) + x(n −1) + x(n) + x(n + 1)
STFT4(2,0) = 1,
STFT4(6,0) = 0
STFT4(n,1) = −x(n −2) + jx(n −1) + x(n) −jx(n + 1)
STFT4(2,1) = (1 + 3j)/2,
STFT4(6,1) = 0
STFT4(n,2) = x(n −2) −x(n −1) + x(n) −x(n + 1)
STFT4(2,2) = 0,
STFT4(6,2) = 0,
STFT4(n,3) = −x(n −2) −jx(n −1) + x(n) + jx(n + 1)
STFT4(2,3) = (1 −3j)/2,
STFT4(6,3) = −1

562
Time-Frequency Analysis
M=4.41
M=4.60
M=4.60
M=4.79
M=3.41
M=4.00
M=4.19
M=4.19
M=4.38
M=3.00, Optimal
M=5.41
M=5.60
M=5.60
M=5.79
M=4.41
M=5.00
M=5.19
M=5.19
M=5.38
M=4.00
M=5.51
M=5.70
M=5.70
M=5.89
M=4.51
Figure 9.20
Time-frequency representation in various lattices (grid-lines are shown), with
concentration measure M = µ[SPEC(n,k)] value. The optimal representation, with respect
to this measure, is presented with thicker gridlines. Time axis is n = 0,1,2,3,4,5,6,7 and the
frequency axis is k = 0,1,2,3,4,5,6,7.
(b) Now we have to make all possible nonoverlapping combinations of these
transforms and to calculate the concentration measure for each of them. Total
number of combinations is 25. The absolute STFT values are shown in Fig.
9.20, along with measure
µ[STFT(n,k)] = ∑n∑k |STFT(n,k)|
for each case. (c) By measuring the concentration for all of them, we will get

Ljubiša Stankovi´c
Digital Signal Processing
563
 frequency
 time
0
1
2
3
4
5
0
π/4
π/2
3π/4
π
Figure 9.21
Areas in the time-frequency plane.
that the optimal combination, to cover the time-frequency plane, is
STFT1(0,0) = x(0) = 0
STFT1(1,0) = x(1) = 1
STFT2(3,1) = x(3) −x(2) = −1
STFT2(3,0) = x(3) + x(2) = 0
STFT4(6,0) = x(4) + x(5) + x(6) + x(7) = 0
STFT4(6,1) = −x(4) + jx(5) + x(6) −jx(7) = 0
STFT4(6,2) = x(4) −x(5) + x(6) −x(7) = 0
STFT4(6,3) = −x(4) −jx(5) + x(6) + jx(7) = −1
with just three nonzero transformation coefﬁcients. It corresponds to the
minimum of µ[SPEC(n,k)].
In this case there is an algorithm for efﬁcient optimal lattice determina-
tion, based on two regions consideration, starting from lattices 1, 19, and 25
from the Fig. 9.20, corresponding to the constant window widths of N = 1,
N = 2, and N = 4 samples.
Example 9.14. Discrete signal x(n) for n = 0,1,2,3,4,5 is considered. Time-
frequency plane is divided as presented in Fig. 9.21.
(a) Denote each region in the ﬁgure by appropriate coefﬁcient STFTNi(n,k), where
N is window length, n is the time index, and k is the frequency index.
(b) Write relations for coefﬁcients calculation and write transformation matrix T.
(c) By using the transformation matrix, ﬁnd STFT values if signal samples are
x(0) = 2, x(1) = −2, x(2) = 4, x(3) =
√
3, x(4) = −
√
3, x(5) = 0.

564
Time-Frequency Analysis
 frequency
 time
0
1
2
3
4
5
0
π/4
π/2
3π/4
π
STFT2(1,0)
STFT2(1,1)
STFT3(4,0)
STFT3(4,1)
STFT3(4,2)
STFT1(2,0)
Figure 9.22
Denoted areas in the time-frequency plane.
(d) If the STFT coefﬁcients for signal y(n) are
STFT2(1,0) = 4,
STFT2(1,1) = 0
STFT1(2,0) = 1,
STFT3(4,0) = 0
STFT3(4,1) = 3,
STFT3(4,2) = 3
ﬁnd the signal samples y(n).
⋆(a) Denoted areas are presented in Fig. 9.22. (b) The STFT values are
obtained using
STFTN(n,k) =
(N−1)/2−1
∑
m=−(N−1)/2
x(n + m)e−j2πmk/N or
STFTN(n,k) =
N/2−1
∑
m=−N/2
x(n + m)e−j2πmk/N
for and odd and even number of samples N, respectively. It follows
STFT2(1,0) = x(0) + x(1)
STFT2(1,1) = −x(0) + x(1)
STFT1(2,0) = x(2)

Ljubiša Stankovi´c
Digital Signal Processing
565
STFT3(4,0) = x(3) + x(4) + x(5)
STFT3(4,1) = −1 + j
√
3
2
x(3) + x(4) + −1 −j
√
3
2
x(5)
STFT3(4,2) = −1 −j
√
3
2
x(3) + x(4) + −1 + j
√
3
2
x(5).
The transformation matrix (where the STFT coefﬁcients are arranged into
column vector S) is
T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
0
0
0
−1
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
1
1
0
0
0
−1+j
√
3
2
1
−1−j
√
3
2
0
0
0
−1−j
√
3
2
1
−1+j
√
3
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(c) The STFT coefﬁcients are
S =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
1
0
0
0
0
−1
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
1
1
0
0
0
−1+j
√
3
2
1
−1−j
√
3
2
0
0
0
−1−j
√
3
2
1
−1+j
√
3
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎣
2
−2
4
√
3
−
√
3
0
⎤
⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
−4
4
0
−3+j3
√
3
2
3−j3
√
3
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(d) The signal samples y(n) are obtained as T−1S resulting in
D
y(5)
y(4)
y(3)
y(2)
y(1)
y(0)
ET
=
D
2
2
1
−1
2
−1
ET .
Example 9.15. A discrete signal x(n) is considered for 0 ≤n < M. Find the number
of the STFTs of this signal with time-varying windows.
(a) Consider arbitrary window widths from 1 to M.
(b) Consider dyadic windows, that is, windows whose width is 2m,
where m is an integer, such that 2m ≤M. In this case ﬁnd the number of time-
varying window STFTs for M = 1,2,3,...,15,16.
⋆(a) Let us analyze the problem recursively. Denote by F(M) the
number of STFTs for a signal with M samples. It is obvious that F(1) = 1,
that is, for one-sample signal there is only one STFT (signal sample itself).
If M > 1, we can use window with widths k = 1,2,... M, as the ﬁrst analysis
window. Now let us analyze remaining (M −k) samples in all possible ways,
so we can write a recursive relation for the total number of the STFTs. If

566
Time-Frequency Analysis
the ﬁrst window is one-sample window, then the number of the STFTs is
F(M −1). When the ﬁrst window is a two-sample window, then the total
number of the STFTs is F(M −2), and so on, until the ﬁrst window is the M-
sample window, when F(M −M) = 1. Thus, the total number of the STFTs
for all cases is
F(M) = F(M −1) + F(M −2) + ... + F(1) + 1
We can introduce F(0) = 1 (meaning that if there are no signal samples we
have only one way to calculate time-varying window STFT) and obtain
F(M) = F(M −1) + F(M −2) + ... F(1) + F(0) =
M
∑
k=1
F(M −k)
Now, for M > 1 we can write
F(M −1) =
M−1
∑
k=1
F(M −1 −k) =
M
∑
k=2
F(M −k)
and
F(M) −F(M −1) =
M
∑
k=1
F(M −k) −
M
∑
k=2
F(M −k) = F(M −1)
F(M) = 2F(M −1).
resulting in F(M) = 2M−1.
(b) In a similar way, following the previous analysis, we can write
F(M) = F(M −20) + F(M −21) + F(M −22) + ··· + F(M −2m)
=
⌊log2 M⌋
∑
m=0
F(M −2m)
where ⌊log2 M⌋is an integer part of log2 M. Here we cannot write a simple
recurrent relation as in the previous case. It is obvious that F(1) = 1. We can
also assume that F(0) = 1. By unfolding recurrence we will get
F(2) = F(1) + F(0) = 2
F(3) = F(2) + F(1) = 3
F(4) = F(3) + F(2) + F(0) = 6
...
The results are presented in the table
M
1
2
3
4
5
6
7
8
F(M)
1
2
3
6
10
18
31
56
M
9
10
11
12
13
14
15
16
F(M)
98
174
306
542
956
1690
2983
5272 .

Ljubiša Stankovi´c
Digital Signal Processing
567
Note that the approximative formula
F(M) ≈
@
1.0366 · (1.7664)M−1A
where [·] is an integer part of the argument, holds, with relative error smaller
then 0.4% for 1 ≤M ≤1024. For example, for M = 16 we have 5272 different
ways to split time-frequency plane into non-overlapping time-frequency
regions.
9.2.11
Frequency-Varying Window
The STFT may use frequency-varying window as well. For a given DFT
frequency pi the window width in time is constant, Fig.9.23
STFTNi(n,ki) =
Ni/2−1
∑
m=−Ni/2
w(m)x(n + m)e−j 2π
Ni mki.
For example, value of STFT4(2,−1) is
STFT4(2,−1) =
2−1
∑
m=−2
x(2 + m)e−j2πm(−1)/4.
It position in the time-frequency plane is shown in 9.23(left).
For the signal used to illustrate the frequency-varying STFT in 9.23,
the best concentration (out of the presented four) is the one shown in the
last subplot. Optimization can be done in the same way as in the case of
time-varying windows.
The STFT can be calculated by using the signal’s DFT instead of the
signal. There is a direct relation between the time and the frequency domain
STFT via coefﬁcients of the form exp(j2πnk/M). A dual form of the STFT
is:
STFT(n,k) = 1
M
M−1
∑
i=0
P(i)X(k + i)ej2πin/M,
(9.31)
STFTM(k) = W−1
M P−1
M X(k).
Frequency domain window P(i) may be of frequency varying width. This
form is dual to the time-varying form. Forms corresponding to frequency
varying windows, dual to the ones for the time-varying windows, can be

568
Time-Frequency Analysis
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S4(2,-2)
S4(6,-2)
S4(10,-2)
S4(14,-2)
S4(2,-1)
S4(6,-1)
S4(10,-1)
S4(14,-1)
S2(1,0)
S2(3,0)
S2(5,0)
S2(7,0)
S2(9,0)
S2(11,0)
S2(13,0)
S2(15,0)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S16(8,-8)
S16(8,-7)
S8(4,-3)
S8(12,-3)
S4(2,-1)
S4(6,-1)
S4(10,-1)
S4(14,-1)
S2(1,0)
S2(3,0)
S2(5,0)
S2(7,0)
S2(9,0)
S2(11,0)
S2(13,0)
S2(15,0)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S2(1,-1)
S2(3,-1)
S2(5,-1)
S2(7,-1)
S2(9,-1)
S2(11,-1)
S2(13,-1)
S2(15,-1)
S16(8,0)
S16(8,1)
S8(4,1)
S8(12,1)
S4(2,1)
S4(6,1)
S4(10,1)
S4(14,1)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
S8(4,-4)
S8(12,-4)
S4(2,-1)
S4(6,-1)
S4(10,-1)
S4(14,-1)
S4(2,0)
S4(6,0)
S4(10,0)
S4(14,0)
S8(4,1)
S8(12,1)
S8(4,2)
S8(12,2)
S16(8,6)
S16(8,7)
Figure 9.23
Time-frequency analysis with the STFT using frequency-varying windows.
easily deﬁned, for example, for a rectangular frequency domain window, as
STFT =
⎡
⎢⎢⎢⎢⎣
W−1
N0
0
···
0
0
W−1
N1
···
0
...
...
...
...
0
0
···
W−1
NK
⎤
⎥⎥⎥⎥⎦
X,
(9.32)
where X = [X(0), X(1), ...,X(M −1)]T is the DFT vector.

Ljubiša Stankovi´c
Digital Signal Processing
569
9.2.12
Hybrid Time-Frequency-Varying Windows
In general, spectral content of signal changes in time and frequency in an ar-
bitrary manner. Combining time-varying and frequency-varying windows
we get hybrid time–frequency-varying windows with STFTN(i,l)(ni,kl),
STFTN(i,l)(ni,kl) =
N(i,l) /2−1
∑
m=−N(i,l) /2
w(i,l)(m)x(ni + m)e
−j
2π
N(i,l)
mkl
(9.33)
For a graphical representation of the STFT with varying windows, the cor-
responding STFT value should be assigned to each instant n = 0,1,..., M −1
and each DFT frequency p = −M/2,−M/2 + 1,..., M/2 −1 within a block.
In the case of a hybrid time–frequency-varying window the matrix form is
obtained from the deﬁnition for each STFT value. For example, for the STFT
calculated as in Fig.9.24, for each STFT value an expression based on (9.33)
should be written. Then the resulting matrix STFT can be formed.
There are several methods in the literature that adapt windows or
basis functions to the signal form for each time instant or even for every
considered time and frequency point in the time-frequency plane. Selection
of the most appropriate form of the basis functions (windows) for each time-
frequency point includes a criterion for selecting the optimal window width
(basis function scale) for each point.
9.3
WAVELET TRANSFORM
The ﬁrst form of functions having the basic property of wavelets was used
by Haar at the beginning of the twentieth century. At the beginning of
1980’s, Morlet introduced a form of basis functions for analysis of seismic
signals, naming them “wavelets”. Theory of wavelets was linked to the
image processing by Mallat in the following years. In late 1980s Daubechies
presented a whole new class of wavelets that can be implemented in a
simple way, by using digital ﬁltering ideas. The most important applications
of the wavelets are found in image processing and compression, pattern
recognition and signal denoising. Here, we will only link the basics of the
wavelet transform to the time-frequency analysis.
Common STFT is characterized by a constant window and constant
time and frequency resolutions for both low and high frequencies. The ba-
sic idea behind the wavelet transform, as it was originally introduced by
Morlet, was to vary the resolution with scale (being related to frequency)

570
Time-Frequency Analysis
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
-8 
-7 
-6 
-5 
-4 
-3 
-2 
-1 
0 
1 
2 
3 
4 
5 
6 
7 
 frequency
 time
STFT2(15,-1)
STFT2(13,-1)
STFT4(2,-2)
STFT4(6,-2)
STFT4(10,-2)
STFT4(10,-1)
STFT4(2,1)
STFT4(6,1)
STFT8(4,-2)
STFT8(4,-1)
STFT8(4,1)
STFT8(12,1)
STFT8(12,2)
STFT8(12,3)
STFT16(8,0)
STFT16(8,1)
Figure 9.24
A time-frequency varying grid in the STFT calculation.
in such a way that a high frequency resolution is obtained for signal com-
ponents at low frequencies, whereas a high time resolution is obtained for
signal at high frequency components. This kind of resolution change could
be relevant for some practical applications, like for example seismic signals.
It is achieved by introducing a frequency variable window width. Window
width is decreased as frequency increases.
The basis functions in the STFT are
STFTII(t,Ω0) =
∞
"
−∞
x(τ)w(τ −t)e−jΩ0τdτ
=
#
x(τ),w(τ −t)e−jΩ0τ$
= ⟨x(τ),h∗(τ −t)⟩=
∞
"
−∞
x(τ)h∗(τ −t)dτ
where h(τ −t) = w(τ −t)ejΩ0τ is a a band-pass signal. It is obtained when
a real-valued window w(τ −t) is modulated by ejΩ0τ.

Ljubiša Stankovi´c
Digital Signal Processing
571
When the above idea about wavelet transform is translated into the
mathematical form and related to the STFT, one gets the deﬁnition of a
continuous wavelet transform
WT(t,a) =
1
,
|a|
∞
"
−∞
x(τ)h∗(τ −t
a
)dτ
(9.34)
where h(t) is a band-pass signal, and the parameter a is the scale. This
transform produces a time-scale, rather than the time-frequency signal rep-
resentation. For the Morlet wavelet the relation between the scale and the
frequency is a = Ω0/Ω. In order to establish a strong formal relationship
between the wavelet transform and the STFT, we will choose the basic Mor-
let wavelet h(t) in the form
h(t) = w(t)ejΩ0t
(9.35)
where w(t) is a window function and Ω0 is a constant frequency. For the
Morlet wavelet we have a modulated Gaussian function
h(t) =
=
1
2π e−αt2ejΩ0t
where the values of α and Ω0 are chosen such that the ratio of h(0) and
the ﬁrst maximum is 1/2, Ω0 = 2π√
α/ln2. From the deﬁnition of h(t) it
is obvious that small Ω(i.e., large a) corresponds to a wide wavelet, i.e., a
wide window, and vice versa. The basic idea of the wavelet transform and
its comparison with the STFT is illustrated in Fig. 9.25.
Substitution of (9.35) into (9.34) leads to a continuous wavelet trans-
form form suitable for a direct comparison with the STFT:
WT(t,a) =
1
,
|a|
∞
"
−∞
x(τ)w∗(τ −t
a
)e−jΩ0 τ−t
a dτ.
(9.36)
From the ﬁlter theory point of view the wavelet transform, for a given
scale a, could be considered as the output of system with impulse response
h∗(−t/a)
,
|a|, i.e.,
WT(t,a) = x(t) ∗t h∗(−t/a)
F
|a|,
where ∗t denotes a convolution in time. Similarly the STFT, for a given Ω,
may be considered as STFTII(t,Ω) = x(t) ∗t [w∗(−t)ejΩt]. If we consider

572
Time-Frequency Analysis
t
a=2
Wavelet expansion functions
 (a)
t
a=1
 (c)
t
a=1/2
 (e)
t
STFT expansion functions
Ω=Ω0/2
 (b)
t
Ω=Ω0
 (d)
t
Ω=2Ω0
 (f)
Figure 9.25
Expansion functions for the wavelet transform (left) and the short-time Fourier
transform (right). Top row presents high scale (low frequency), middle row is for medium scale
(medium frequency) and bottom row is for low scale (high frequency).
these two band-pass ﬁlters from the bandwidth point of view we can see
that, in the case of STFT, the ﬁltering is done by a system whose impulse
response w∗(−t)ejΩt has a constant bandwidth, being equal to the width of
the Fourier transform of w(t).
Constant Q-Factor Transform: The quality factor Q for a band-pass ﬁlter,
as measure of the ﬁlter selectivity, is deﬁned as
Q = Central Frequency
Bandwidth
In the STFT the bandwidth is constant, equal to the window Fourier trans-
form width, Bw. Thus, factor Q is proportional to the considered frequency,
Q = Ω
Bw
.
In the case of the wavelet transform the bandwidth of impulse response is
the width of the Fourier transform of w(t/a). It is equal to B0/a, where B0
is the constant bandwidth corresponding to the mother wavelet (wavelet in

Ljubiša Stankovi´c
Digital Signal Processing
573
t1
t2
Ω1
Ω2
Ω
t
WT(t,Ω)
 (a)
t1
t2
Ω1
Ω2
Ω
t
STFT(t,Ω)
 (b)
Figure 9.26
Illustration of the wavelet transform (a) of a sum of two delta pulses and two
sinusiods compared with STFT (b)
scale a = 1). It follows
Q =
Ω
B0/a = Ω0
B0
= const.
Therefore, the continuous wavelet transform corresponds to the passing a
signal through a series of band-pass ﬁlters centered at Ω, with constant
factor Q. Again we can conclude that the ﬁltering, that produces Wavelet
transform, results in a small bandwidth (high frequency resolution and low
time resolution) at low frequencies and wide bandwidth (low frequency and
high time resolution) at high frequencies.
Example 9.16. Find the wavelet transform of signal (9.3)
x(t) = δ(t −t1) + δ(t −t2) + ejΩ1t + ejΩ2t.
(9.37)
⋆Its continuous wavelet transform is
WT(t,a) =
1
,
|a|
@
w((t1 −t)/a)e−jΩ0(t1−t)/a + w((t2 −t)/a)e−jΩ0(t2−t)/aA
+
F
|a|
@
ejΩ1tW[a(Ω0/a −Ω1)] + ejΩ2tW[a(Ω0/a −Ω2)]
A
.
(9.38)
where w(t) is a real-valued function. The transform (9.38) has nonzero values
in the region depicted in Fig. 9.26(a).
In analogy with spectrogram, the scalogram is deﬁned as the squared
magnitude of a wavelet transform:
SCAL(t,a) =| WT(t,a) |2 .
(9.39)

574
Time-Frequency Analysis
The scalogram obviously loses the linearity property, and ﬁts into the cate-
gory of quadratic transforms.
9.3.1
Filter Bank and Discrete Wavelet
This analysis will start by splitting the signal’s spectral content into its high
frequency and low frequency part. Within the STFT framework, this can be
achieved by a two sample rectangular window
w(n) = δ(n) + δ(n + 1),
with N = 2. A two-sample window STFT is
STFT(n,0) =
1
√
2
1
∑
m=0
x(n + m)e−j0
= 1
√
2 (x(n) + x(n + 1)) = xL(n),
(9.40)
for k = 0, corresponding to low frequency ω = 0 and
xH(n) =
1
√
2 (x(n) −x(n + 1))
(9.41)
for k = 1 corresponding to high frequency ω = π. A time-shifted (anticausal)
version of the STFT
STFT(n,k) =
1
√
N
N−1
∑
m=0
x(n + m)e−j2πkm/N
is used, instead of STFT(n,k) = ∑N/2−1
m=−N/2 x(n + m)e−j2πkm/N in order to
remain within the common wavelet literature notation. For the same reason
the STFT is scaled by
√
N (a form when the DFT and IDFT have the same
factor 1/
√
N).
This kind of signal analysis leads to the Haar (wavelet) transform. In
the Haar wavelet transform the high-frequency part, xH(n) is not processed
any more. It is kept with this (high) two-samples resolution in time. The res-
olution in time of xH(n,1) is just slightly (two-times) lower than the original
signal sampling interval. The lowpass part xL(n) = (x(n) + x(n + 1))/
√
2
will be further processed. After the signal samples x(n) and x(n + 1) are
processed using (9.40) and (9.41), then next two samples x(n + 2) and
x(n + 3) are analyzed. The highpass part is again calculated xH(n + 2) =

Ljubiša Stankovi´c
Digital Signal Processing
575
(x(n + 2) −x(n + 3))/
√
2 and kept as it is. Lowpass part xL(n + 2) =
(x(n + 2) + x(n + 3))/
√
2 is considered as a new signal, along with its cor-
responding previous sample xL(n).
Spectral content of the lowpass part of signal is divided, in the same
way, into its low and high frequency part,
xLL(n) =
1
√
2 (xL(n) + xL(n + 2))
= 1
2 [x(n) + x(n + 1) + x(n + 2) + x(n + 3)]
xLH(n) =
1
√
2 (xL(n) −xL(n + 2))
= 1
2 [x(n) + x(n + 1) −[x(n + 2) + x(n + 3)]].
The highpass part xLH(n) is left with resolution four in time, while the
lowpass part is further processed in the same way, by dividing spectral
content of xLL(n) and xLL(n + 4) into its low and high frequency part. This
process is continued until the full length of signal is achieved. The Haar
wavelet transformation matrix in the case of signal with 8 samples is
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
√
2W1(0, H)
√
2W1(2, H)
√
2W1(4, H)
√
2W1(6, H)
2W2(0, H)
2W2(4, H)
2
√
2W4(0, H)
2
√
2W4(0, L)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −1
0
0
0
0
0
0
0
0
1 −1
0
0
0
0
0
0
0
0
1 −1
0
0
0
0
0
0
0
0
1 −1
1
1 −1 −1
0
0
0
0
0
0
0
0
1
1 −1 −1
1
1
1
1 −1 −1 −1 −1
1
1
1
1
1
1
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(9.42)
This kind of signal transformation was introduced by Haar more than a century
ago . In this notation scale a = 1 values of the wavelet coefﬁcients W1(2n, H)
are equal to the highpass part of signal calculated using two samples,
W1(2n, H) = xH(2n). The scale a = 2 wavelet coefﬁcients are W2(4n, H) =
xLH(4n). In scale a = 4 there is only one highpass and one lowpass coef-
ﬁcient at n = 0, W4(8n, H) = xLLH(8n) and W4(8n, L) = xLLL(8n). In this
way any length of signal N = 2m can be decomposed into Haar wavelet
coefﬁcients.
The Haar wavelet transform has a property that its highpass coefﬁ-
cients are equal to zero if the analyzed signal is constant within the analyzed
time interval, for considered scale. If signal has large number of constant

576
Time-Frequency Analysis
value samples within the analyzed time intervals, then many Haar wavelet
transform coefﬁcients are zero valued. They can be omitted in signal storage
or transmission. In recovery their values are assumed as zeros and the orig-
inal signal is obtained. The same can be done in the case of noisy signals,
when all coefﬁcients bellow an assumed level of noise can be zero-valued
and the signal-to-noise ratio in the reconstructed signal improved.
9.3.1.1
Lowpass and Highpass Filtering and Downsampling
Although the presented Haar wavelet analysis is quite simple we will use
it as an example to introduce the ﬁlter bank framework of the wavelet
transform. Obvious results from the Haar wavelet will be used to introduce
other wavelet forms. For the Haar wavelet calculation two signals xL(n) and
xH(n) are formed according to (9.40) and (9.41), based on the input signal
x(n). Transfer functions of the discrete-time systems producing these two
signals are
HL(z) =
1
√
2 (1 + z)
(9.43)
HH(z) =
1
√
2 (1 −z).
Frequency responses of these systems assume the form
HL(ejω) =
1
√
2
B
1 + ejωC
HH(ejω) =
1
√
2
B
1 −ejωC
with amplitude characteristics
''HL(ejω)
'' =
√
2|cos(ω/2)|, and
''HH(ejω)
'' =
√
2|sin(ω/2)|, presented in Fig.9.27. As expected, they represent a quite
rough forms of lowpass and highpass ﬁlters. In general, this principle
is kept for all wavelet transforms. The basic goal for all of them is to
split the frequency content of a signal into its lowpass part and highpass
part providing, in addition, a possibility of simple and efﬁcient signal
reconstruction.
After the values representing lowpass and highpass part of signal
are obtained, next values of the signals xL(n) = [x(n) + x(n + 1)]/
√
2
and xH(n) = [x(n) −x(n + 1)]/
√
2 are calculated after one time instant is
skipped. Therefore the output signal is downsampled by factor of two. The

Ljubiša Stankovi´c
Digital Signal Processing
577
-3
-2
-1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
|HH(ejω)|=|DFT{ψ1(n)}|
|HL(ejω)|=|DFT{φ1(n)}|
|HL(ejω)|2+|HH(ejω)|2=2
Figure 9.27
Amplitude of the Fourier transform of basic Haar wavelet and scale function
divided by
√
2.
new downsampled signals will be denoted by
sL(n) = xL(2n)
sH(n) = xH(2n).
(9.44)
Downsampling of a signal x(n) to get the signal y(n) = x(2n) is
described in the z-transform domain by the function
Y(z) = 1
2X(z1/2) + 1
2X(−z1/2).
This relation can easily be veriﬁed using the z-transform deﬁnition
X(z) =
∞
∑
n=−∞
x(n)z−n
X(z1/2) + X(−z1/2) =
∞
∑
n=−∞
x(n)[(z−1/2)n + (−z−1/2)n] =
∞
∑
n=−∞
2x(2n)z−n
Z{x(2n))} = Y(z) = 1
2X(z1/2) + 1
2X(−z1/2).
(9.45)
For the signals sL(n) = xL(2n) and sH(n) = xH(2n) the system implementa-
tion is presented in Fig.9.28.

578
Time-Frequency Analysis
HH(z)
2
↓
x(n)
X(z)
[X(z1/2)HH(z1/2+X(z-1/2 )HH(z-1/2 )]/2
HL(z)
2
↓
[X(z1/2)HH(z1/2+X(z-1/2 )HH(z-1/2 )]/2
Figure 9.28
Signal ﬁltering by a low pass and a high pass ﬁlter followed by downsaampling
by 2.
If the signals sL(n)and sH(n) are passed through the lowpass and
highpass ﬁlters HL(z) and HH(z) and then downsampled,
SL(z) = 1
2 HL(z1/2)X(z1/2) + 1
2 HL(−z1/2)X(−z1/2)
SH(z) = 1
2 HH(z1/2)X(z1/2) + 1
2 HH(−z1/2)X(−z1/2)
hold.
9.3.1.2
Upsampling
Let us assume that we are not going to transform the signals sL(n) and
sH(n) any more. The only goal is to reconstruct the signal x(n) based on its
downsampled lowpass and highpass part signals sL(n) and sH(n). The ﬁrst
step in the signal reconstruction is to restore the original sampling interval
of the discrete-time signal. It is done by upsampling the signals sL(n) and
sH(n).
Upsampling of a signal x(n) is described by
y(n) = [...x(−2), 0, x(−1), 0, x(0), 0, x(1),0, x(2), 0, ...].
Its z-transform domain form is
Y(z) = X(z2),

Ljubiša Stankovi´c
Digital Signal Processing
579
since
X(z2) =
∞
∑
n=−∞
x(n)z−2n = ...x(−1)z2 + 0 · z1 + x(0) + 0 · z−1 + x(1)z−2 + ....
(9.46)
Upsampling of a signal x(n) is deﬁned by
y(n) =
!
x(n/2)
for
even n
0
for
odd n
= Z−1{X(z2))}.
If a signal x(n) is downsampled ﬁrst and then upsampled, the result-
ing signal transform is
Y(z) = 1
2X(
B
z1/2C2
) + 1
2X(−
B
z1/2C2
)
Y(z) = 1
2X(z) + 1
2X(−z).
(9.47)
In the Fourier domain it means Y(ejω) = (X
P
ejωQ + X
B
ej(ω+π)C
. This form
indicates that an aliasing component X
B
ej(ω+π)C
appeared in this process.
9.3.1.3
Reconstruction Condition
In general, when the signal is downsampled and upsampled the aliasing
appears since the component X(−z) exists in addition to the original signal
X(z) in (9.47). The upsampled versions of signals sL(n) and sH(n) should
be appropriately ﬁltered and combined in order to eliminate aliasing. The
conditions to avoid the aliasing in the reconstructed signal will be studied
next.
In the reconstruction process the signals are upsampled (SL(z) →
SL(z2) and SH(z) →SH(z2)) and passed through the reconstruction ﬁlters
GL(z) and GL(z) before being added up to form the output signal, Fig.9.29.

580
Time-Frequency Analysis
HH(z)
2
↓
x(n)
X(z)
SH(z)
HL(z)
2
↓
SL(z)
GH(z)
2
↑
GL(z)
2
↑
y(n)
Y(z)
+
Figure 9.29
One stage of the ﬁlter bank with reconstruction, corresponding to the one stage
of the wavelet transform realization.
The output signal transforms are
YL(z) = SL(z2)GL(z) = [1
2 HL(z)X(z) + 1
2 HL(−z)X(−z)]GL(z)
YH(z) = SH(z2)GH(z) = [1
2 HH(z)X(z) + 1
2 HH(−z)X(−z)]GH(z)
Y(z) = YL(z) + YH(z)
= [1
2 HL(z)GL(z) + 1
2 HH(z)GH(z)]X(z)
+[1
2 HL(−z)GL(z) + 1
2 HH(−z)GH(z)]X(−z).
Condition for alias-free reconstruction is
Y(z) = X(z).
It means that
HL(z)GL(z) + HH(z)GH(z) = 2
(9.48)
HL(−z)GL(z) + HH(−z)GH(z) = 0.
(9.49)
These are general conditions for a correct (alias-free) signal reconstruction.

Ljubiša Stankovi´c
Digital Signal Processing
581
Based on the reconstruction conditions we can show that the lowpass
ﬁlters satisfy
HL(z)GL(z) + HL(−z)GL(−z) = 2
(9.50)
P(z) + P(−z) = 2,
(9.51)
where P(z) = HL(z)GL(z).
From (9.49) we may write
GH(z) = HL(−z)GL(z)
HH(−z)
HH(z) = HL(z)GL(−z)
GH(−z)
.
Second expression is obtained from (9.49) with z being replaced by −z,
when HL(z)GL(−z) + HH(z)GH(−z) = 0. Substituting these values into
(9.48) we get
HL(z)GL(z) + HL(−z)GL(z)
HH(−z)
HL(z)GL(−z)
GH(−z)
= 2
or
HL(z)GL(z)
HH(−z)GH(−z) [HH(−z)GH(−z) + HL(−z)GL(−z)] = 2.
Since the expression within the brackets is equal to 2 (reconstruction condi-
tion (9.48) with z being replaced by −z) then
HL(z)GL(z)
HH(−z)GH(−z) = 1
(9.52)
and (9.50) follows with
HH(z)GH(z) = HL(−z)GL(−z).
In the Fourier transform domain the reconstruction conditions are
HL(ejω)GL(ejω) + HH(ejω)GH(ejω) = 2
(9.53)
HL(−ejω)GL(ejω) + HH(−ejω)GH(ejω) = 0.

582
Time-Frequency Analysis
9.3.1.4
Orthogonality Conditions
The wavelet transform is calculated using downsampling by a factor 2. One
of the basic requirements that will be imposed to the ﬁlter impulse response
for an efﬁcient signal reconstruction is that it is orthogonal to its shifted
version with step 2 (and its multiples). In addition the wavelet functions in
different scales should be orthogonal. Orthogonality of wavelet function in
different scales will be discussed later. The orthogonality condition for the
impulse response is
⟨hL(m),hL(m −2n)⟩= δ(n)
(9.54)
∑
m
hL(m)hL(m −2n) = δ(n).
For the Haar wavelet transform this condition is obviously satisﬁed. In
general, for wavelet transforms when the duration of impulse response
hL(n) is greater than two, the previous relation can be understood as a
downsampled convolution of hL(n) and hL(−n)
r(n) = hL(n) ∗hL(−n) = ∑
m
hL(m)hL(m −n),
Z{r(n))} = HL(z)HL(z−1)
FT{r(n))} =
'''HL(ejω)
'''
2
.
The Fourier transform of the downsampled convolution, for real-valued
hL(n) is, (9.45)
FT{r(2n))} = 1
2
'''HL(ejω/2)
'''
2
+ 1
2
'''HL(−ejω/2)
'''
2
.
From r(2n) = δ(n) follows
'''HL(ejω)
'''
2
+
'''HL(−ejω)
'''
2
= 2.
The impulse response is orthogonal, in the sense of (9.54), if the frequency
response satisﬁes
'''HL(ejω)
'''
2
+
'''HL(ej(ω+π))
'''
2
= 2.

Ljubiša Stankovi´c
Digital Signal Processing
583
Time domain form of relation (9.50) is
hL(n) ∗gL(n) + [(−1)nhL(n)] ∗[(−1)ngL(n)] = 2δ(n)
∑
m
hL(m)gL(n −m) + ∑
m
(−1)nhL(m)gL(n −m) = 2δ(n)
∑
m
hL(m)gL(2n −m) = δ(n).
If the impulse response hL(n) is orthogonal, as in (9.54), then the last relation
is satisﬁed for
gL(n) = hL(−n).
In the z-domain it holds
GL(z) = HL(z−1)
and we may write (9.48) in the form
GL(z)GL(z−1) + GL(−z)GL(−z−1) = 2
(9.55)
or P(z) + P(−z) = 2 with P(z) = GL(z)GL(z−1). Relation (9.48) may also
written for HL(z) as well
HL(z)HL(z−1) + HL(−z)HL(−z−1) = 2.
9.3.1.5
FIR Filter and Orthogonality Condition
Consider a lowpass anticausal FIR ﬁlter of the form
hL(n) =
K−1
∑
k=0
hkδ(n + k)
and the corresponding causal reconstruction ﬁlter
gL(n) = hL(−n) =
K−1
∑
k=0
hkδ(n −k)
GL(ejω) = HL(e−jω)

584
Time-Frequency Analysis
If the highpass ﬁlters are obtained from corresponding lowpass ﬁlters by
reversal, in addition to common multiplication by (−1)n, then
gH(n) = (−1)ngL(K −n)
GH(ejω) =
K
∑
n=0
gH(n)e−jωn =
K
∑
n=0
(−1)ngL(K −n)e−jωn
=
K
∑
m=0
(−1)K−mgL(m)e−jω(K−m) = (−1)Ke−jωK
K
∑
m=0
ejπmgL(m)e−j(−ω)m
= −e−jωKGL(e−j(ω−π)) = −e−jωKGL(−e−jω)
or
GH(ejω) = −e−jωKGL(−e−jω) = −e−jωKHL(−ejω)
for GL(ejω) = HL(e−jω). Similar relation holds for the anticausal hH(n)
impulse response
hH(n) = (−1)nhL(−K −n).
HH(ejω) =
0
∑
n=−K
hH(n)e−jωn =
0
∑
n=−K
(−1)nhL(−n −K)e−jωn
=
0
∑
m=−K
(−1)−K−mhL(m)ejω(m+K) = −ejωKHL(−e−jω)
The reconstruction conditions are satisﬁed since, according to (9.48) and
(9.52), a relation corresponding to
HH(z)GH(z) = HL(−z)GL(−z)
holds in the Fourier domain
HH(ejω)GH(ejω) =
@
−ejωKHL(−e−jω)
A@
−e−jωKHL(−ejω)
A
= HL(−e−jω)HL(−ejω) = GL(−ejω)HL(−ejω).
In this way all ﬁlters are expressed in terms of GL(ejω) or HL(ejω).
For example, if GL(ejω) is obtained using (9.55), with appropriate
design conditions, then
HL(ejω) = GL(e−jω)
GH(ejω) = −e−jωKGL(−e−jω)
HH(ejω) = −ejωKGL(−ejω).
(9.56)

Ljubiša Stankovi´c
Digital Signal Processing
585
Note that the following symmetry of the frequency response amplitude
functions holds
'''HL(ejω)
''' =
'''GL(e−jω)
''' =
'''HH(ej(ω+π))
''' =
'''HH(e−j(ω+π))
'''.
The highpass and lowpass response orthogonality
∑
m
hL(m)hH(m −2n) = 0
∑
m
gL(m)gH(m −2n) = 0
(9.57)
is also satisﬁed with these forms of transfer functions for any n. Since
Z{hL(n) ∗hH(−n)} = HL(z)HH(z−1)
and Z{hL(2n) ∗hH(−2n)} = 0, in the Fourier domain this relation assumes
the form
HL(ejω)HH(e−jω) + HL(−ejω)HH(−e−jω) = 0.
This identity follows from the second relation in (9.53)
HL(−ejω)GL(ejω) + HH(−ejω)GH(ejω) = 0
with HH(−ejω) = ejωKHL(e−jω), GH(ejω) = −e−jωKGL(−e−jω),
and HL(ejω) = GL(e−jω) as
GL(−e−jω)GL(ejω) −ejωKGL(ejω)e−jωKGL(−e−jω) = 0.
9.3.1.6
Haar Wavelet Implementation
The condition that the reconstruction ﬁlter GL(z) has zero value at z = ejπ =
−1 means that its form is GL(z) = a(1 + z−1). This form without additional
requirements would produce a = 1/
√
2 from the reconstruction relation
GL(z)GL(z−1) + GL(−z)GL(−z−1) = 2. The time domain ﬁlter form is
gL(n) =
1
√
2 [δ(n) + δ(n −1)].
It corresponds to the Haar wavelet. All other ﬁlter functions can be deﬁned
using gL(n) or GL(ejω).

586
Time-Frequency Analysis
The same result would be obtained starting from the ﬁlter transfer
functions for the Haar wavelet already introduced as
HL(z) =
1
√
2 (1 + z)
HH(z) =
1
√
2 (1 −z).
The reconstruction ﬁlters are obtained from (9.48)-(9.49)
1
√
2
(1 + z)GL(z) + 1
√
2
(1 −z)GH(z) = 2
1
√
2
(1 −z)GL(z) + 1
√
2
(1 + z)GH(z) = 0
as
GL(z) =
1
√
2
B
1 + z−1C
(9.58)
GH(z) =
1
√
2
B
1 −z−1C
with
gL(n) =
1
√
2
δ(n) + 1
√
2
δ(n −1)
(9.59)
gH(n) =
1
√
2
δ(n) −1
√
2
δ(n −1).
The values impulse responses in the Haar wavelet transform (relations
(9.43) and (9.59)) are:
n
√
2hL(n)
√
2hH(n)
n
√
2gL(n)
√
2gH(n)
0
1
1
0
1
1
−1
1
−1
1
1
−1
A detailed time domain ﬁlter bank implementation of the reconstruc-
tion process in the Haar wavelet case is described. The reconstruction is
implemented in two steps:
1) The signals sL(n) and sH(n) from (9.44) are upsampled, according
to (9.46), as
rL(n) = [sL(0) 0 sL(1) 0 sL(2) 0 ...sL(N −1) 0]
rH(n) = [sH(0) 0 sH(1) 0 sH(2) 0 ...sH(N −1) 0]

Ljubiša Stankovi´c
Digital Signal Processing
587
These signals are then passed trough the reconstruction ﬁlters. A sum of the
outputs from these ﬁlters is
y(n) = rL(n) ∗gL(n) + rH(n) ∗gH(n)
=
1
√
2
rL(n) + 1
√
2
rL(n −1) + 1
√
2
rH(n) −1
√
2
rH(n −1)
= 1
√
2 [xL(0) 0 xL(2) 0 xL(4)....0 xL(2N −2) 0]+
+ 1
√
2 [0 xL(0) 0 xL(2)....0 xL(2N −2)]
+ 1
√
2 [xH(0) 0 xH(2) 0 xH(4)....0 xH(2N −2) 0]
−1
√
2 [0 xH(0) 0 xH(2)....0 xH(2N −2)].
where sL(n) = xL(2n) and sH(n) = xH(2n). From the previous relation
follows
y(0) =
1
√
2 [xL(0) + xH(0)] = x(0)
y(1) =
1
√
2 [xL(0) −xH(0)] = x(1)
...
y(2n) =
1
√
2 [xL(2n) + xH(2n)] = x(2n)
y(2n + 1) =
1
√
2 [xL(2n) −xH(2n)] = x(2n + 1).
A system for implementation of the Haar wavelet transform of a signal
with eight samples is presented in Fig.9.30. It corresponds to the matrix form
realization (9.42).
Example 9.17. For a signal x(n) = [1, 1, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0] calculate
the Haar wavelet transform coefﬁcients, with their appropriate placement in
the time-frequency plane corresponding to a signal with M = 16 samples.
⋆The wavelet transform of a signal with M = 16 samples after the
stage a = 1 is shown in Fig.9.31(a). The whole frequency range is divided
into two subregions, denoted by L and H within the coefﬁcients W1(n, L) =
[x(n) + x(n + 1)]/
√
2 and W1(n, H) = [x(n) −x(n −1)]/
√
2 calculated at
instants n = 0,2,3,6,8,10,12,14. In the second stage (a = 2) the highpass re-
gion is not transformed, while the lowpass part s2(n) = W1(2n, L) is divided

588
Time-Frequency Analysis
discrete-time   n
0
1
2
3
4
5
6
7
HH(z)
W1(0,H)
W1(2,H)
W1(4,H)
W1(6,H)
W2(0,H)
W2(4,H)
W4(0,H)
W4(0,L)
2
↓
x(n)
HL(z)
↓
2
HH(z)
2
↓
HL(z)
↓
2
HH(z)
↓
2
HL(z)
↓
2
first stage
second stage
third stage
scale a=1
scale a=2
scale a=3
Figure 9.30
Filter bank for the wavelet transform realization
into its lowpass and highpass region W2(n, L) = [s2(n) + s2(n + 1)]/
√
2 and
W2(n, H) = [s2(n) −s2(n + 1)]/
√
2, respectively, Fig.9.31(b). The same calcu-
lation is performed in the third and fourth stage, Fig.9.31(c) - (d).
9.3.1.7
Daubechies D4 Wavelet Transform
The Haar wavelet has the duration of impulse response equal to two. In
one stage, it corresponds to a two-sample STFT calculated using a rectan-
gular window. Its Fourier transform presented in Fig.9.27 is quite rough
approximation of a lowpass and highpass ﬁlter. In order to improve ﬁlter
performance, an increase of the number of ﬁlter coefﬁcients should be done.
A fourth order FIR system will be considered. The impulse response of an-
ticausal fourth order FIR ﬁlter is hL(n) = [hL(0),hL(−1),hL(−2),hL(−3)] =
[h0, h1, h2, h3].

Ljubiša Stankovi´c
Digital Signal Processing
589
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
W1(0,L)
W1(2,L)
W1(4,L)
W1(6,L)
W1(8,L)
W1(10,L)
W1(12,L)
W1(14,L)
W1(0,H)
W1(2,H)
W1(4,H)
W1(6,H)
W1(8,H)
W1(10,H)
W1(12,H)
W1(14,H)
(a)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
W3(0,L)
W3(8,L)
W3(0,H)
W3(8,H)
W2(0,H)
W2(4,H)
W2(8,H)
W2(12,H)
W1(0,H)
W1(2,H)
W1(4,H)
W1(6,H)
W1(8,H)
W1(10,H)
W1(12,H)
W1(14,H)
(c)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
W2(0,L)
W2(4,L)
W2(8,L)
W2(12,L)
W2(0,H)
W2(4,H)
W2(8,H)
W2(12,H)
W1(0,H)
W1(2,H)
W1(4,H)
W1(6,H)
W1(8,H)
W1(10,H)
W1(12,H)
W1(14,H)
(b)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
W4(0,L)
W4(0,H)
W3(0,H)
W3(8,H)
W2(0,H)
W2(4,H)
W2(8,H)
W2(12,H)
W1(0,H)
W1(2,H)
W1(4,H)
W1(6,H)
W1(8,H)
W1(10,H)
W1(12,H)
W1(14,H)
(d)
Figure 9.31
Wavelet transform of a signal with M = 16 samples at the output of stages 1, 2, 3
and 4, respectively. Notation Wa(n, H) is used for the highpass value of coefﬁcient after stage
(scale of) a at an instant n. Notation Wa(n, L) is used for the lowpass value of coefﬁcient after
stage (scale of) a at an instant n.
If the highpass and reconstruction ﬁlter coefﬁcients are chosen such
that
n
hL(n)
hH(n)
n
gL(n)
gH(n)
0
h0
h3
0
h0
h3
−1
h1
−h2
1
h1
−h2
−2
h2
h1
2
h2
h1
−3
h3
−h0
3
h3
−h0
.
(9.60)
then relation (9.56) is satisﬁed with K = 3, since hL(n) = gL(−n), gH(n) =
(−1)ngL(3 −n), and hH(n) = (−1)ngL(n + 3).

590
Time-Frequency Analysis
The reconstruction conditions
HL(z)GL(z) + HH(z)GH(z) = 2
HL(−z)GL(z) + HH(−z)GH(z) = 0
are satisﬁed if
h2
0 + h2
1 + h2
2 + h2
3 = 1.
Using the z-transform of the corresponding ﬁlters, it follows
HL(z)GL(z) + HH(z)GH(z)
=
B
h0 + h1z + h2z2 + h3z3CB
h0 + h1z−1 + h2z−2 + h3z−3C
+
B
−h0z3 + h1z2 −h2z + h3
CB
−h0z−3 + h1z−2 −h2z−1 + h3
C
= 2(h2
0 + h2
1 + h2
2 + h2
3) = 2
and
HL(−z)GL(z) + HH(−z)GH(z)
=
B
h0 −h1z + h2z2 −h3z3CB
h0 + h1z−1 + h2z−2 + h3z−3C
+
B
h0z3 + h1z2 + h2z + h3
CB
−h0z−3 + h1z−2 −h2z−1 + h3
C
= 0.
For the calculation of impulse response values h0, h1, h2, h3 of a fourth
order system (9.60) four independent equations (conditions) are needed.
We already have three conditions. The ﬁlter has to satisfy zero-frequency
condition HL(ej0) =
√
2, high-frequency condition HL(ejπ) = 0 and the
reconstruction condition h2
0 + h2
1 + h2
2 + h2
3 = 1. Therefore one more condition
is needed. In the Daubechies D4 wavelet derivation the fourth condition is
imposed so that the derivative of the ﬁlter transfer function at ω = π is equal
to zero
dHL(ejω)
dω
''''
ω=π
= 0.
This condition, meaning a smooth approach to zero-value at ω = π, also
guarantees that the output of high-pass ﬁlter HH(−z) to the linear input
signal, x(n) = an + b, will be zero. This will be illustrated later. Now we

Ljubiša Stankovi´c
Digital Signal Processing
591
have a system of four equations,
h0 + h1 + h2 + h3 =
√
2 from HL(ej0) =
√
2
h2
0 + h2
1 + h2
2 + h2
3 = 1 reconstruction condition
h0 −h1 + h2 −h3 = 0 from HL(ejπ) = 0
−h1 + 2h2 −3h3 = 0 from dHL(ejω)
dω
''''
ω=π
= 0.
Its solution produces the fourth order Daubechies wavelet coefﬁcients (D4)
n
hL(n)
hH(n)
n
gL(n)
gH(n)
0
1+
√
3
4
√
2
1−
√
3
4
√
2
0
1+
√
3
4
√
2
1−
√
3
4
√
2
−1
3+
√
3
4
√
2
−3−
√
3
4
√
2
1
3+
√
3
4
√
2
−3−
√
3
4
√
2
−2
3−
√
3
4
√
2
3+
√
3
4
√
2
2
3−
√
3
4
√
2
3+
√
3
4
√
2
−3
1−
√
3
4
√
2
−1+
√
3
4
√
2
3
1−
√
3
4
√
2
−1+
√
3
4
√
2
Note that this is just one of possible symmetric solutions of the previous
system of equations, Fig.9.32.
The reconstruction conditions for the fourth order FIR ﬁlter
HL(ejω) = h0 + h1ejω + h2ej2ω + h3ej3ω
with Daubechies wavelet coefﬁcients (D4) can also be checked in a graphical
way by calculating
'''HL(ejω)
'''
2
+
'''HL(ej(ω+π))
'''
2
= 2
HL(ej(ω+π))H∗
L(ejω) + HL(ejω)H∗
L(ej(ω+π)) = 0.
From Fig.9.33, we can see that it is much better approximation of low and
high pass ﬁlters than in the Haar wavelet case, Fig.9.27.
Another way to derive Daubechies wavelet coefﬁcients (D4) is in using
relation (9.55)
P(z) + P(−z) = 2
with
P(z) = GL(z)HL(z) = GL(z)GL(z−1)
Condition imposed on the transfer function GL(z) in D4 wavelet is that its
value and the value of its ﬁrst derivative at z = −1 are zero-valued (smooth

592
Time-Frequency Analysis
-4
-3
-2
-1
0
1
2
3
4
-0.5
0
0.5
1
time n
 gL(n)
-4
-3
-2
-1
0
1
2
3
4
-0.5
0
0.5
1
time n
 gH(n)
-4
-3
-2
-1
0
1
2
3
4
-0.5
0
0.5
1
time n
 hL(n)
-4
-3
-2
-1
0
1
2
3
4
-0.5
0
0.5
1
time n
 hH(n)
Figure 9.32
Impulse responses of the D4 ﬁlters.
-3
-2
-1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
|HH(ejω)|=|DFT{ψ1(n)}|
|HL(ejω)|=|DFT{φ1(n)}|
|HL(ejω)|2+|HH(ejω)|2=2
Figure 9.33
Amplitude of the Fourier transform of basic Daubechies D4 wavelet and scale
function.

Ljubiša Stankovi´c
Digital Signal Processing
593
approach to the highpass zero value)
GL(ejω)
'''
ω=π = 0
dGL(ejω)
dω
''''
ω=π
= 0.
Then GL(z) must contain a factor of the form
P
1 + z−1Q2. Since the ﬁlter
order must be even (K must be odd), taking into account that
P
1 + z−1Q2
would produce a FIR system with 3 nonzero coefﬁcients, then we have to
add at least one factor of the form a(1 + z1z−1) to GL(z). Thus, the lowest
order FIR ﬁlter with an even number of (nonzero) impulse response values
is
GL(z) =
B
1 + z−1C2
a(1 + z1z−1)
with
P(z) =
B
1 + z−1C2 B
1 + z1C2
R(z)
where
R(z) =
@
a(1 + z1z−1)
A@
a(1 + z1z1)
A
= z0z−1 + b + z0z.
Using
P(z) + P(−z) = 2
only the terms with even exponents of z will remain in P(z) + P(−z)
producing
(4z0 + b)z2 + 8z0 + 6b + (4z0 + b)z−1 = 1
8z0 + 6b = 1
4z0 + b = 0
The solution is z0 = −1/16 and b = 1/4. It produces az1 = z0 = −1/16 and
a2 + z2
1 = b = 1/4 with
a =
1
4
√
2
B
1 +
√
3
C
and z1 = 1 −
√
3
1 +
√
3
and
R(z) =
*
1
4
√
2
+2 B
1 +
√
3 +
B
1 −
√
3
C
z−1CB
1 +
√
3 +
B
1 −
√
3
C
z1C
.

594
Time-Frequency Analysis
The reconstruction ﬁlter transfer function is
GL(z) =
1
4
√
2
(1 + z−1)2 B
1 +
√
3 +
B
1 −
√
3
C
z−1C
with
gL(n) =
1
4
√
2
[
B
1 +
√
3
C
δ(n) +
B
3 +
√
3
C
δ(n −1)
+
B
3 −
√
3
C
δ(n −2) +
B
1 −
√
3
C
δ(n −3)].
All other impulse responses follow from this one (as in the presented table).
Example 9.18. Consider a signal that is a linear function of time
x(n) = an + b.
Show that the condition
−hL(−1) + 2hL(−2) −3hL(−3) = 0 following from dHL(ejω)
dω
'''''
ω=π
= 0
is equivalent to the condition that highpass coefﬁcients (output from HH(ejω))
are zero-valued, Fig.9.33. Show that the lowpass coefﬁcients remain a linear
function of time.
⋆The highpass coefﬁcients after the ﬁrst stage W1(2n, H) are obtained
by downsampling W1(n, H) whose form is
W1(n, H) = x(n) ∗hH(n)
= x(n)hH(0) + x(n + 1)hH(−1) + x(n + 2)hH(−2) + x(n + 3)hH(−3)
= x(n)h3 −x(n + 1)h2 + x(n + 2)h1 −x(n + 3)h0
= (an + b)h3 −((n + 1)a + b)h2 + ((n + 2)a + b)h1 −((n + 3)a + b)h0
= (a(n + 3) + b)(−h0 + h1 −h2 + h3) −a(h1 −2h2 + 3h3) = 0
if
−h0 + h1 −h2 + h3 = 0 and
h1 −2h2 + 3h3 = 0.
The lowpass coefﬁcients are obtained by downsampling
W1(n, L) = x(n) ∗hL(n)
= x(n)h0 + x(n + 1)h1 + x(n + 2)h2 + x(n + 3)h3
= (an + b)h0 + ((n + 1)a + b)h1 + ((n + 2)a + b)h2 + ((n + 3)a + b)h3
= (an + b)(h0 + h1 + h2 + h3) + a(h1 + 2h2 + 3h3)
= a1n + b1

Ljubiša Stankovi´c
Digital Signal Processing
595
where a1 =
√
2a and b1 =
√
2b + 0.8966a.
Thus we may consider that the highpass D4 coefﬁcients will indicate
the deviation of the signal from a linear function x(n) = an + b. In the ﬁrst
stage the coefﬁcients will indicate the deviation from the linear function
within four samples. In the next stage the equivalent length of wavelet is
doubled. The highpass coefﬁcient in this stage will indicate the deviation
of the signal from the linear function within doubled number of signal
samples, and so on. This a signiﬁcant difference from the STFT nature that
is derived based on the Fourier transform and the signal decomposition and
tracking its frequency content.
Example 9.19. Show that with the conditions
h0 + h1 + h2 + h3 =
√
2 from HL(ej0) =
√
2
−h0 + h1 −h2 + h3 = 0 from HL(ejπ) = 0
the reconstruction condition
h2
0 + h2
1 + h2
2 + h2
3 = 1
is equivalent to the orthogonality property of the impulse response and its
shifted version for step 2
h0
h1
h2
h3
0
0
0
0
0
0
h0
h1
h2
h3
0
0
given by
h2h0 + h3h1 = 0.
⋆If we write the sum of squares of the ﬁrst two equations follows
2(h2
0 + h2
1 + h2
2 + h2
3) + 4h0h2 + 4h1h3 = 2.
Therefore, the conditions
h2
0 + h2
1 + h2
2 + h2
3 = 1
and
h0h2 + h1h3 = 0
follow from each other if h0 + h1 + h2 + h3 =
√
2 and −h0 + h1 −h2 + h3 = 0
are assumed.

596
Time-Frequency Analysis
The matrix for the D4 wavelet transform calculation in the ﬁrst stage
is of the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
W1(0, L)
W1(0, H)
W1(2, L)
W1(2, H)
W1(4, L)
W1(4, H)
W1(6, L)
W1(6, H)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
h0
h1
h2
h3
0
0
0
0
h3
−h2
h1
−h0
0
0
0
0
0
0
h0
h1
h2
h3
0
0
0
0
h3
−h2
h1
−h0
0
0
0
0
0
0
h0
h1
h2
h3
0
0
0
0
h3
−h2
h1
−h0
h2
h3
0
0
0
0
h0
h1
h1
−h0
0
0
0
0
h3
−h2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(9.61)
In the ﬁrst row of transformation matrix the coefﬁcients corresponds to
hL(n), while the second row corresponds to hH(n). The ﬁrst row produces
D4 scaling function, while the second row produces D4 wavelet function.
The coefﬁcients are shifted for 2 in next rows. As it has been described
in the Hann(ing) window reconstruction case, the calculation should be
performed in a circular manner, assuming signal periodicity. That is why
the coefﬁcients are circularly shifted in the last two rows.
Example 9.20. Consider a signal x(n) = 64 −|n −64| within 0 ≤n ≤128. How
many nonzero coefﬁcients will be in the ﬁrst stage of the wavelet transform
calculation using D4 wavelet functions. Assume that the signal can appropri-
ately be extended so that the boundary effects can be neglected.
⋆In the ﬁrst stage all highpass coefﬁcients corresponding to linear
four-sample intervals will be zero. It means that out of 64 high pass coef-
ﬁcients (calculated with step two in time) only one nonzero coefﬁcient will
exist, calculated for n = 62, including nonlinear interval 62 ≤n ≤65. It means
that almost a half of the coefﬁcients can be omitted in transmission or stor-
age, corresponding to 50% compression ratio. In the DFT analysis this would
correspond to a signal with a half of (the high frequency) spectrum being
equal to zero. In the wavelet analysis this process would be continued with
additional savings in next stages of the wavelet transform coefﬁcients calcu-
lation. It also means that if there is some noise in the signal, we can ﬁlter
out all zero-valued coefﬁcients using an appropriate threshold. For this kind
of signal (piecewise linear function of time) we will be able to improve the
signal-to-noise ratio for about 3 dB in just one wavelet stage.
Example 9.21. For the signal x(n) = δ(n −7) deﬁned within 0 ≤n ≤15 calcu-
late the wavelet transform coefﬁcients using the D4 wavelet/scale function.
Repeat the same calculation for the signal x(n) = 2cos(16πn/N) + 1 with
0 ≤n ≤N −1 with N = 16.

Ljubiša Stankovi´c
Digital Signal Processing
597
⋆The wavelet coefﬁcients in the ﬁrst stage (scale a = 1, see also
Fig.9.30) are
W1(2n, H) = x(2n)hH(0) + x(2n + 1)hH(−1)
+x(2n + 2)hH(−2) + x(2n + 3)hH(−3)
= x(2n)h3 −x(2n + 1)h2 + x(2n + 2)h1 −x(2n + 3)h0
with
[h3,h2,h1,h0] = [1 −
√
3
4
√
2
, 3 −
√
3
4
√
2
, 3 +
√
3
4
√
2
, 1 +
√
3
4
√
2
].
In speciﬁc, W1(0, H) = 0, W1(2, H) = 0, W1(4, H) = −0.4830, W1(6, H) =
−0.2241, W1(8, H) = 0, W1(10, H) = 0, W1(12, H) = 0, and W1(14, H) = 0.
The lowpass part of the ﬁrst stage values
s2(n) = W1(2n, L) = x(2n)h0 + x(2n + 1)h1 + x(2n + 2)h2 + x(2n + 3)h3
are W1(0, L) = 0, W1(2, L) = 0, W1(4, L) = −0.1294, W1(6, L) = 0.8365, W1(8, L) =
0, W1(10, L) = 0, W1(12, L) = 0, and W1(14, L) = 0. Values of s2(n) are deﬁned
for 0 ≤n ≤7 as s2(n) = −0.1294δ(n −2) + 0.8365δ(n −3). This signal is the
input to the next stage (scale a = 2). The highpass output of the stage two is
W2(4n, H) = s2(n)h3 −s2(n + 1)h2 + s2(n + 2)h1 −s2(n + 3)h0.
The values of W2(4n, H) are: W2(0, H) = −0.5123, W2(4, H) = −0.1708,
W2(8, H) = 0, and W2(12, H) = 0. The lowpass values at this stage at the input
to the next stage (a = 3) calculation
s3(n) = W2(4n, L) = s2(n)h0 + s2(n + 1)h1 + s2(n + 2)h2 + s2(n + 3)h3.
They are W2(0, L) = −0.1373, W2(4, L) = 0.6373, W2(8, L) = 0, and W2(12, L) =
0.
Since there is only 4 samples in s3(n) this is the last calculation. The
coefﬁcients in this stage are W3(0, H) = −0.1251, W3(8, H) = −0.4226 and
W3(0, L) = 0.4668, W3(8, L) = −0.1132. The absolute value of the wavelet
transform of x(n) with D4 wavelet function is shown in Fig.9.34.
For the signal x(n) = 2cos(2π8n/N) + 1 with 0 ≤n ≤N −1 with
N = 16 the same calculation is done. Here it is important to point out that
the circular convolutions should be used. The wavelet transform coefﬁcients
are W1(2n, L) = 1.4142 and W1(2n, H) = 2.8284. Values in the next stage are
W2(2n, H) = 0 and W2(2n, L) = 2. The third stage values are W3(2n, H) = 0
and W3(2n, L) = 2.8284. Compare these results with Fig. 9.26(a). Since the
impulse response duration is 4 and the step is 2 this could be considered as a
kind of signal analysis with overlapping.

598
Time-Frequency Analysis
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
W3(0,L)
W3(8,L)
W3(0,H)
W3(8,H)
W2(0,H)
W2(4,H)
W2(8,H)
W2(12,H)
W1(0,H)
W1(2,H)
W1(4,H)
W1(6,H)
W1(8,H)
W1(10,H)
W1(12,H)
W1(14,H)
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
W3(0,L)
W3(8,L)
W3(0,H)
W3(8,H)
W2(0,H)
W2(4,H)
W2(8,H)
W2(12,H)
W1(0,H)
W1(2,H)
W1(4,H)
W1(6,H)
W1(8,H)
W1(10,H)
W1(12,H)
W1(14,H)
Figure 9.34
Daubechies D4 wavelet transform (absolute value) of the signal x(n) = δ(n −7)
using N = 16 signal samples, 0 ≤n ≤N −1 (left). The Daubechies D4 wavelet transform
(absolute value) of the signal x(n) = 2cos(2π8n/N) + 1, 0 ≤n ≤N −1, with N = 16 (right).
The inverse matrix for the D4 wavelet transform for a signal with
N = 8 samples would be calculated from the lowest level in this case
for a = 2 with coefﬁcients W2(0, L), W2(0, H), W2(4, L), and W2(4, H). The
lowpass part of signal at level a = 1 would be reconstructed using
⎡
⎢⎢⎣
W1(0, L)
W1(2, L)
W1(4, L)
W1(6, L)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
h0
h3
h2
h1
h1
−h2
h3
−h0
h2
h1
h0
h3
h3
−h0
h1
−h2
⎤
⎥⎥⎦
⎡
⎢⎢⎣
W2(0, L)
W2(0, H)
W2(4, L)
W2(4, H)
⎤
⎥⎥⎦.
After the lowpass part W1(0, L), W1(2, L), W1(4, L), and W1(6, L) are recon-
structed, they are used with wavelet coefﬁcients from this stage W1(0, H),
W1(2, H), W1(4, H), and W1(6, H) to reconstruct the signal as
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
x(4)
x(5)
x(6)
x(7)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
h0
h3
0
0
0
0
h2
h1
h1
−h2
0
0
0
0
h3
−h0
h2
h1
h0
h3
0
0
0
0
h3
−h0
h1
−h2
0
0
0
0
0
0
h2
h1
h0
h3
0
0
0
0
h3
−h0
h1
−h2
0
0
0
0
0
0
h2
h1
h0
h3
0
0
0
0
h3
−h0
h1
−h2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
W1(0, L)
W1(0, H)
W1(2, L)
W1(2, H)
W1(4, L)
W1(4, H)
W1(6, L)
W1(6, H)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(9.62)

Ljubiša Stankovi´c
Digital Signal Processing
599
This procedure can be continued for signal of length N = 16 with one more
stage. Additional stage would be added for N = 32 and so on.
Example 9.22. For the Wavelet transform from the previous example ﬁnd its
inverse (reconstruct the signal).
⋆The inversion is done backwards. From W3(0, H), W3(0, L),W3(8, H),
W3(8, L) we get signal s3(n) or W2(2n, L) as
⎡
⎢⎢⎣
W2(0, L)
W2(4, L)
W2(8, L)
W2(12, L)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
h0
h3
h2
h1
h1
−h2
h3
−h0
h2
h1
h0
h3
h3
−h0
h1
−h2
⎤
⎥⎥⎦
⎡
⎢⎢⎣
W3(0, L)
W3(0, H)
W3(8, L)
W3(8, H)
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
h0
h3
h2
h1
h1
−h2
h3
−h0
h2
h1
h0
h3
h3
−h0
h1
−h2
⎤
⎥⎥⎦
⎡
⎢⎢⎣
0.4668
−0.1251
−0.1132
−0.4226
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
−0.1373
0.6373
0
0
⎤
⎥⎥⎦.
Then W2(4n, L) = s3(n) are used with the wavelet coefﬁcients W2(4n, H) to
reconstruct W1(2n, L) or s2(n) using
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
W1(0, L)
W1(2, L)
W1(4, L)
W1(6, L)
W1(8, L)
W1(10, L)
W1(12, L)
W1(14, L)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
h0
h3
0
0
0
0
h2
h1
h1
−h2
0
0
0
0
h3
−h0
h2
h1
h0
h3
0
0
0
0
h3
−h0
h1
−h2
0
0
0
0
0
0
h2
h1
h0
h3
0
0
0
0
h3
−h0
h1
−h2
0
0
0
0
0
0
h2
h1
h0
h3
0
0
0
0
h3
−h0
h1
−h2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
W2(0, L)
W2(0, H)
W2(4, L)
W2(4, H)
W2(8, L)
W2(8, H)
W2(12, L)
W2(12, H)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The obtained values W1(n, L) with the wavelet coefﬁcients W1(n, H) are used
to reconstruct the original signal x(n). The transformation matrix in this case
is of 16 × 16 order and it is formed using the same structure as the previous
transformation matrix.
9.3.1.8
Daubechies D4 Wavelet Functions in Different Scales
Although the wavelet realization can be performed using the same basic
function presented in the previous section, here we will consider the equiva-
lent wavelet function hH(n) and equivalent scale function hL(n) in different
scales. To this aim we will analyze the reconstruction part of the system. As-
sume that in the wavelet analysis of a signal only one coefﬁcient is nonzero.
Also assume that this nonzero coefﬁcient is at the exit of all lowpass ﬁl-
ters structure. It means that the signal is equal to the basic scale function in

600
Time-Frequency Analysis
GL(z)
 δ (n)
GH(z)
0
GL(z)
2
↑
GH(z)
2
↑
GL(z)
2
↑
GH(z)
2
↑
 φ0(n)=hL(n)
 φ1(n)
 φ2(n)
Figure 9.35
Calculation of the upsampled scale function.
the wavelet analysis. The scale function can be found in an inverse way, by
reconstructing signal corresponding to this delta pulse like transform. The
system of reconstruction ﬁlters is shown in Fig.9.35. Note that this case and
coefﬁcient in the Haar transform would correspond to W4(0, L) = 1 in (9.42)
or in Fig.9.30. The reconstruction process consists of signal upsampling and
passing it trough the reconstruction stages. For example, the output of the
third reconstruction stage has the z-transform
Φ2(z) = GL(z)GL(z2)GL(z4).
In the time domain the reconstruction is performed as
φ0(n) = δ(n) ∗gL(n) = gL(n)
φ1(n) = [φ0(0) 0 φ0(1) 0 φ0(2) 0 φ0(3)] ∗gL(n)
φ2(n) = [φ1(0) 0 φ1(1) 0 ... φ1(8) 0 φ1(9)] ∗gL(n)
....
φa+1(n) = ∑
p
φa(p)gL(n −2p)
where gL(n) is the four sample impulse response (Daubechies D4 coefﬁ-
cients). Duration of the scale function φ1(n) is (4 + 3) + 4 −1 = 10 samples,
while the duration of φ2(n) is 19 + 4 −1 = 22 samples. The scale function for

Ljubiša Stankovi´c
Digital Signal Processing
601
GL(z)
 δ (n)
0
GH(z)
GL(z)
2
↑
GH(z)
2
↑
GL(z)
2
↑
GH(z)
2
↑
 ψ0(n)=hH(n)
 ψ1(n)
 ψ2(n)
Figure 9.36
Calculation of the upsampled wavelet function
different scales a (exists of different reconstruction stages) are is presented
in Fig.9.37. Normalized values φa(n)2(a+1)/2 are presented. The amplitudes
are scaled by 2(a+1)/2 in order to keep their values within the same range
for various a.
In a similar way the wavelet function ψ(n) is calculated. The mother
wavelet is obtained in the wavelet analysis of a signal when only one
nonzero coefﬁcient exists at the highpass of the lowest level of the signal
analysis. To reconstruct the mother wavelet the reconstruction system as
in Fig.9.36 is used. The values of ψ(n) are calculated: using the values of
gH(n) at the ﬁrst input, upsampling it and passing trough the reconstruction
system with gL(n), to obtain ψ1(n) and repeating this procedure for the next
steps. The resulting z-transform is:
Ψ(z) = GH(z)GL(z2)GL(z4).
In the Haar transform (9.42) and Fig.9.30 this case would correspond to
W4(0, H) = 1.

602
Time-Frequency Analysis
Calculation in the time of the wavelet function in different scales is
done using
ψ0(n) = δ(n) ∗gH(n) = gH(n)
ψ1(n) = [ψ1(0) 0 ψ1(1) 0 ψ1(2) 0 ψ1(3)] ∗gL(n)
ψ2(n) = [ψ2(0) 0 ψ2(1) 0 ... ψ2(8) 0 ψ2(9)] ∗gL(n)
....
ψa+1(n) = ∑
p
ψa(p)gL(n −2p)
Different scales of the wavelet function, are presented in Fig.9.37.
Values are normalized using ψa(n)2(a+1)/2.
Wavelet function are orthogonal in different scales, with correspond-
ing steps, as well. For example, it is easy to show that
⟨ψ0(n −2m),ψ1(n)⟩= 0
since
⟨ψ0(n −2m),ψ1(n)⟩= ∑
p
gH(p)
(
∑
n
gH(n −2m)gL(n −2p)
)
= 0
for any p and m according to (9.57).
Note that the wavelet and scale function in the last row are plotted
as the continuous functions. The continuous wavelet transform (CWT) is
calculated by using the discretized versions of the continuous functions.
However in contrast to the discrete wavelet transform whose step in time
and scale change is strictly deﬁned, the continuous wavelet transform can
be used with various steps and scale functions.
Example 9.23. In order to illustrate the procedure it has been repeated for
the Haar wavelet when gL(n) = [1 1] and gH(n) = [1 −1]. The results are
presented in Fig.9.38.
9.3.1.9
Daubechies D6 Wavelet Transform
The results derived for Daubechies D4 wavelet transform can be extended
to higher order polynomial functions. Consider a sixth order FIR system
hL(n) = [hL(0),hL(−1),hL(−2),hL(−3),hL(−4),hL(−5)]
= [h0, h1, h2, h3, h4, h5].

Ljubiša Stankovi´c
Digital Signal Processing
603
0
10
20
30
40
-1
0
1
Daubechies scaling function D4
0
10
20
30
40
-1
0
1
0
10
20
30
40
-1
0
1
Daubechies wavelet D4
0
10
20
30
40
-1
0
1
0
10
20
30
40
-1
0
1
0
10
20
30
40
-1
0
1
0
10
20
30
40
-1
0
1
0
10
20
30
40
-1
0
1
0
1
2
3
-1
0
1
0
1
2
3
-1
0
1
Figure 9.37
The Daubechies D4 wavelet scale function and wavelet calculated using the ﬁlter
bank relation in different scales: a = 0 (ﬁrst row), a = 1 (second row), a = 2 (third row), a = 3
(fourth row), a = 10 (fourth row-approximation of a continuous domain). The amplitudes are
scaled by 2(a+1)/2 to keep them within the same range. Values ψa(n)2(a+1)/2 and φa(n)2(a+1)/2
are presented.

604
Time-Frequency Analysis
0
5
10
15
-1
0
1
Haar scaling function
0
5
10
15
-1
0
1
0
5
10
15
-1
0
1
0
5
10
15
-1
0
1
0
5
10
15
-1
0
1
Haar wavelet
0
5
10
15
-1
0
1
0
5
10
15
-1
0
1
0
5
10
15
-1
0
1
Figure 9.38
The Haar wavelet scale function and wavelet calculated using the ﬁlter bank
relation in different scales. Values are normalized 2(a+1)/2.
In addition to the conditions HL(ej0) =
√
2 and HL(ejπ) = 0, written as
h0 + h1 + h2 + h3 + h4 + h5 =
√
2
h0 −h1 + h2 −h3 + h4 −h5 = 0,

Ljubiša Stankovi´c
Digital Signal Processing
605
the orthogonality conditions
h0h2 + h1h3 + h2h4 + h3h5 = 0
h0h4 + h1h5 = 0,
are added. Since the ﬁlter order is 6 then two orthogonality conditions must
be used. One for shift 2 and the other for shift 4.
The linear signal cancellation condition is again used as
−h1 + 2h2 −3h3 + 4h4 −5h5 = 0.
The ﬁnal condition in the Daubechies D6 wavelet transform is that the
quadratic signal cancellation is achieved for highpass ﬁlter, meaning
d2HL(ejω)
dω2
''''
ω=π
=
d2 B
∑5
n=0 hnejωnC
dω2
''''''
ω=π
= −
5
∑
n=0
n2hnejωn
'''''
ω=π
= 0.
This condition is of the form
−h1 + 22h2 −32h3 + 42h4 −52h5 = 0
From the set of ﬁve equations the Daubechies D6 wavelet transform
coefﬁcients are obtained as
hL(n) = [1.1411,0.4705,0.6504,0.0498,−0.1208,−0.1909].
This is one of possible symmetric solutions of the previous system. From
the deﬁnition it is obvious that the highpass coefﬁcients will be zero as far
as the signal is of quadratic nature within the considered interval. These
coefﬁcients can be used as a measure of the signal deviation from the
quadratic form in each scale.
Implementation is the same as in the case of Haar or D4 wavelet
transform. Only difference is in the ﬁlter coefﬁcients form.
This form can be also derived from the reconstruction conditions
and the fact that the transfer function GL(z) contains a factor of the form
(1 + z−1)3 since z = −1 is its third order zero, according to the assumptions.
9.3.1.10
Coifﬂet Transform
In the Daubechies D6 wavelet transform the last condition is introduced so
that the output of high-pass ﬁlter is zero when the input signal is quadratic.

606
Time-Frequency Analysis
Another way to form ﬁlter coefﬁcients for a six sample wavelet is to intro-
duce the condition that the ﬁrst moment of the scale function is zero, instead
of the second order moment of the wavelet function. In this case symmetric
form of coefﬁcients should be used in the deﬁnition
hL(−2) + hL(−1) + hL(0) + hL(1) + hL(2) + hL(3) =
√
2
h2
L(−2) + h2
L(−1) + h2
L(0) + h2
L(1) + h2
L(2) + h2
L(3) = 1
−2hL(−2) + hL(−1) −hL(1) + 2hL(2) −3hL(3) = 0
hL(−2)hL(0) + hL(−1)hL(1) + hL(0)hL(2) + hL(1)hL(3) = 0
hL(−2)hL(2) + hL(−1)hL(3) = 0.
The ﬁrst-order moment of hL(n) is
−2hL(−2) −hL(−1) + hL(1) + 2hL(2) + 3hL(3) = 0
This is so called sixth order coifﬂet transform. Its coefﬁcients are
h(−2) = (
√
2 −
√
14)/32,
h(−1) = (−11
√
2 +
√
14)/32,
h(0) = (7
√
2 +
√
14)/16,
h(1) = (−
√
2 −
√
14)/16,
h(2) = (
√
2 −
√
14)/32,
h(3) = (−3
√
2 +
√
14)/32.
9.3.1.11
Discrete Wavelet Transform - STFT
Originally the wavelet transform was introduced by Morlet as a frequency
varying STFT. Its aim was to analyze spectrum of the signal with varying
resolution in time and frequency. Higher resolution in frequency was re-
quired at low frequencies, while at high frequencies high resolution in time
was the aim, for speciﬁc analyzed seismic signals.
The Daubechies D4 wavelet/scale function is derived from the con-
dition that the highpass coefﬁcients of a signal with linear change in time
(x(n) = an + b) are zero-valued. Higher order Daubechies wavelet/scale
functions are derived by increasing the order of the signal polynomial
changes. Frequency of a signal does not play any direct role in the discrete-
wavelet transform deﬁnition using Daubechies functions. In this sense it

Ljubiša Stankovi´c
Digital Signal Processing
607
would be easier to relate the wavelet transform to the linear (D4) and higher
order interpolations of functions (signals), within the intervals of various
lengths (corresponding to various wavelet transform scales), than to the
spectral analysis where the harmonic basis functions play the central role.
Example 9.24. Consider a signal x(n) with M = 16 samples, 0 ≤n ≤M −1. Write
the Daubechies D4 wavelet transform based decomposition of this signal that
will divide the frequency axis into four equal regions.
⋆In the STFT a 4−point (N−point) signal would be used to calculate
4 (or N) coefﬁcients of the frequency plane. The wavelet transform divides
the time-frequency plane into two regions (high and low) regardless of the
number of the signal values (wavelet transform coefﬁcients) being used.
If the Haar wavelet is used in Fig.9.39 then by dividing both highpass
bands and lowpass bands in the same way the short-time Walsh-Hadamard
transform with 4-sample nonoverlapping calculation would be obtained. In
the cases of Daubechies 4D wavelet transform, a kind of short time analysis
with the Daubechies functions would be obtained. For the Daubechies D4
function the scale 2 functions:
φ1(n) = hLL(n) = [hL(0) 0 hL(1) 0 hL(2) 0 hL(3)] ∗hL(n)
(9.63)
ϕ1(n) = hLH(n) = [hH(0) 0 hH(1) 0 hH(2) 0 hH(3)] ∗hL(n)
ψ1(n) = hHL(n) = [hL(0) 0 hL(1) 0 hL(2) 0 hH(3)] ∗hH(n)
κ1(n) = hHH(n) = [hH(0) 0 hH(1) 0 hH(2) 0 hH(3)] ∗hH(n)
(9.64)
would be used to calculate W(4n,0), W(4n,1), W(4n,2),
and W(4n,3),
Fig.9.40. The asymmetry of the frequency regions is visible.
Note that the STFT analysis of this case, with a Hann(ing) window
of N = 8 and calculation step R = 4 will result in the same number of
instants, however the frequency range will be divided in 8 regions, having
a ﬁner grid. This grid is redundant with respect to the signal and to the
wavelet transform. Both, the signal and the wavelet transform have 16 values
(coefﬁcients).
9.3.2
S-Transform
The S-transform (the Stockwell transform) is conceptually a combination of
the STFT analysis and wavelet analysis. It employs a common window, as
in the STFT, with a frequency variable length as in the wavelet transform.
The frequency-dependent window function produces a higher frequency
resolution at lower frequencies, while at higher frequencies sharper time
localization can be achieved, the same as in the continuous wavelet case.
For a signal x(t) it reads as

608
Time-Frequency Analysis
discrete-time   n
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15
HH(z)
W(0,3)
W(0,2)
W(0,2)
W(0,0)
W(4,3)
W(4,2)
W(4,2)
W(4,0)
W(8,3)
W(8,2)
W(8,2)
W(8,0)
W(12,3)
W(12,2)
W(12,2)
W(12,0)
2
↓
x(n)
HH(z)
2
↓
HL(z)
2
↓
HL(z)
2
↓
HH(z)
2
↓
HL(z)
2
↓
Figure 9.39
Full coverage of the time-frequency plane using the ﬁlter bank calculation and
systems with impulse responses corresponding to the wavelet transformation.
Sc(t,Ω) =
|Ω|
(2π)3/2
+∞
"
−∞
x(τ)e−(τ−t)2Ω2
8π2
e−jΩτdτ,
(9.65)
with substitutions τ −t →τ, the above equation can be rewritten as follows
Sc(t,Ω) = |Ω|e−jΩt
(2π)3/2
+∞
"
−∞
x(t + τ)e−τ2Ω2
8π2 e−jΩτdτ.
(9.66)
For the window function of form
w(τ,Ω) =
|Ω|
(2π)3/2 e−τ2Ω2
8π2 ,
(9.67)
the deﬁnition of the continuous S-transform can be rewritten as follows
Sc(t,Ω) = e−jΩt
+∞
"
−∞
x(t + τ)w(τ,Ω)e−jΩτdτ.
(9.68)

Ljubiša Stankovi´c
Digital Signal Processing
609
0
2
4
6
8
10
-1
0
1
Daubechies functions D4
-1
-0.5
0
0.5
1
0
2
4
Spectral form of Daubechies functions D4
0
2
4
6
8
10
-1
0
1
-1
-0.5
0
0.5
1
0
2
4
0
2
4
6
8
10
-1
0
1
-1
-0.5
0
0.5
1
0
2
4
0
2
4
6
8
10
-1
0
1
-1
-0.5
0
0.5
1
0
2
4
Figure 9.40
Daubechies functions: Scaling function (ﬁrst row), Mother wavelet function
(second row), Function producing the low-frequency part in the second stage of the high
frequency part in the ﬁrst stage (third), Function producing the high-frequency part in the
second stage of the high frequency part in the ﬁrst stage (fourth). Time domain forms of the
functions are left while its spectral content is shown on the right.
A discretization over τ of (9.68) results in the discrete form of S-transform
Sd(t,Ω) = e−jΩt∑
n
x(t + n∆t)w(n∆t,Ω)e−jΩn∆t∆t.
(9.69)
It may be considered as a STFT with frequency-varying window.

610
Time-Frequency Analysis
9.4
LOCAL POLYNOMIAL FOURIER TRANSFORM
After the presentation of the wavelet transform we will shift back our
attention to the frequency of the signal, rather than to its amplitude values.
There are signals whose instantaneous frequency variations are known up
to an unknown set of parameters. For example, many signals could be
expressed as polynomial-phase signals
x(t) = Aej(Ω0t+a1t2+a2t3+···+aNtN+1)
where the parameters Ω0,a1,a2,...,aN are unknown.
For nonstationary
signals, this approach may be used if the nonstationary signal could be
considered as a polynomial phase signal within the analysis window. In
that case, the local polynomial Fourier transform (LPFT) may be used. It is
deﬁned as
LPFTΩ1,Ω2,...,ΩN(t,Ω) =
∞
"
−∞
x(t + τ)w(τ)e−j(Ωτ+Ω1τ2+Ω2τ3+···+ΩNτN+1)dτ.
(9.70)
In general, parameters Ω1,Ω2,...,ΩN could be time dependent, that is, for
each time instant t, the set of optimal parameters could be different.
Realization of the LPFT reduces to the local signal x(t + τ) demodula-
tion by e−j(Ω1τ2+Ω2τ3+···+ΩNτN+1) followed by the STFT calculation.
Example 9.25. Consider the second-order polynomial-phase signal
x(t) = ej(Ω0t+a1t2).
Show that its LPFT could be completely concentrated along the instantaneous
frequency.
⋆Its LPFT has the form
LPFTΩ1(t,Ω) =
∞
"
−∞
x(t + τ)w(τ)e−j(Ωτ+Ω1τ2)dτ
= ej(Ω0t+a1t2)
∞
"
−∞
w(τ)e−j(Ω−Ω0−2a1t)τe−j(Ω1−a1)τ2dτ.
(9.71)
For Ω1 = a1, the second-order phase term does not introduce any distortion
to the local polynomial spectrogram,
''LPFTΩ1=a1(t,Ω)
''2 = |W(Ω−Ω0 −2a1t)|2,

Ljubiša Stankovi´c
Digital Signal Processing
611
with respect to the spectrogram of a sinusoid with constant frequency. For a
wide window w(τ), like in the case of the STFT of a pure sinusoid, we achieve
high concentration.
The LPFT could be considered as the Fourier transform of win-
dowed signal demodulated with exp(−j(Ω1τ2 + Ω2τ3 + ··· + ΩNτN+1)).
Thus, if we are interested in signal ﬁltering, we can ﬁnd the coefﬁcients
Ω1,Ω2,...,ΩN, demodulate the signal by multiplying it with exp(−j(Ω1τ2 +
Ω2τ3 + ··· + ΩNτN+1)) and use a standard ﬁlter for almost a pure sinu-
soid. In general, we can extend this approach to any signal x(t) = ejφ(t)
by estimating its phase φ(t) with Xφ(t) (using the instantaneous frequency
estimation that will be discussed later) and ﬁltering demodulated signal
x(t)exp(−jXφ(t)) by a lowpass ﬁlter. The resulting signal is obtained when
the ﬁltered signal is returned back to the original frequencies, by modula-
tion with exp(jXφ(t)).
Example 9.26. Consider the ﬁrst-order LPFT of a signal x(t). Show that the second-
order moments of the LPFT could be calculated based on the windowed
signal moment, windowed signal’s Fourier transform moment and one more
LPFT moment for any Ω1 in (9.70), for example for Ω1 = 1.
⋆The second-order moment of the ﬁrst-order LPFT,
LPFTΩ1(t,Ω) =
∞
"
−∞
xt(τ)e−j(Ωτ+Ω1τ2)dτ,
deﬁned by
MΩ1 = 1
2π
∞
"
−∞
Ω2 ''LPFTΩ1(t,Ω)
''2 dΩ
(9.72)
is equal to
MΩ1 =
∞
"
−∞
''''''
d
B
xt(τ)e−jΩ1τ2C
dτ
''''''
2
dτ,
since the LPFT could be considered as the Fourier transform of xt(τ)e−jΩ1τ2,
that is, LPFTΩ1(t,Ω) = FT{xt(τ)e−jΩ1τ2}, and the Parseval’s theorem is used.
After the derivative calculation
MΩ1 =
∞
"
−∞
''''
dxt(τ)
dτ
−j2Ω1τxt(τ)
''''
2
dτ =
∞
"
−∞
(
''''
dxt(τ)
dτ
''''
2
+ j2Ω1τx∗
t (τ) dxt(τ)
dτ
−j2Ω1τxt(τ) dx∗
t (τ)
dτ
+ |2Ω1τxt(τ)|2)dτ.

612
Time-Frequency Analysis
We can recognize some of the terms in the last line, as
M0 =
∞
"
−∞
''''
dxt(τ)
dτ
''''
2
dτ = 1
2π
∞
"
−∞
Ω2 ''LPFTΩ1=0(t,Ω)
''2 dΩ.
This is the moment of Xt(Ω) = FT{xt(τ)}, since the integral of |dxt(τ)/dτ|2
over τ is equal to the integral of |jΩXt(Ω)|2 over Ω, according to Parseval’s
theorem. Also, we can see that the last term in MΩ1 contains the signal
moment,
mx =
∞
"
−∞
τ2 |xt(τ)|2 dτ,
(9.73)
multiplied by 4Ω2
1. Then, it is easy to conclude that
MΩ1 −M0 −4mxΩ2
1 = Ω1
∞
"
−∞
*
j2τx∗
t (τ) d[xt(τ)]
dτ
−j2τxt(τ) d[x∗
t (τ)]
dτ
+
dτ.
Note that the last integral does not depend on parameter Ω1. Thus, the
relation among the LPFT moments at any two Ω1, for example, Ω1 = a and
an arbitrary Ω1, easily follows as the ratio
MΩ1=a −M0 −4a2mx
MΩ1 −M0 −4Ω2
1mx
=
a
Ω1
.
(9.74)
With a = 1, by leaving the notation for an arbitrary Ω1 unchanged, we get
M1 −M0 −4mx
MΩ1 −M0 −4Ω2
1mx
= 1
Ω1
,
(9.75)
with M1 = MΩ1=1.
Obviously, the second-order moment, for any Ω1, can be expressed as
a function of other three moments. In this case the relation reads
MΩ1 = 4Ω2
1mx + Ω1(M1 −M0 −4mx) + M0.
Example 9.27. Find the position and the value of the second-order moment min-
imum of the LPFT, based on the windowed signal moment, the windowed
signal’s Fourier transform moment, and the LPFT moment for Ω1 = 1.
⋆The minimal value of the second-order moment (meaning the best
concentrated LPFT in the sense of the duration measures) could be calculated
from
dMΩ1
dΩ1
= 0

Ljubiša Stankovi´c
Digital Signal Processing
613
as
Ω1 = −M1 −M0 −4mx
8mx
.
Since mx > 0 this is a minimum of the function MΩ1. Thus, in general, there
is no need for a direct search for the best concentrated LPFT over all possible
values of Ω1. It can be found based on three moments.
The value of MΩ1 is
MΩ1 = M0 −(M1 −M0 −4mx)2
16mx
.
(9.76)
Note that any two moments, instead of M0 and M1, could be used in
the derivation.
The fractional Fourier transform easily reduces to the ﬁrst-order LPFT.
9.4.1
Fractional Fourier Transform with Relation to the LPFT
The fractional Fourier transform (FRFT) for an angle α (α ̸= kπ) is deﬁned
as
Xα(u) =
∞
"
−∞
x(τ)Kα(u,τ)dτ,
(9.77)
where
Kα(u,τ) =
=
1 −jcotα
2π
ej(u2/2)cotαej(τ2/2)cotαe−juτcscα.
(9.78)
It can be considered as a rotation of signal in the time-frequency plane for
an angle α. Its inverse can be considered as a rotation for angle −α
x(t) =
∞
"
−∞
Xα(u)K−α(u,t)du.
Special cases of the FRFT reduce to: X0(u) = x(u) and Xπ/2(u) = X(u)/
√
2π,
that is, the signal and its Fourier transform.
The windowed FRFT is
Xw,α(t,u) =
F
1−jcotα
2π
ej(u2/2)cotα
∞
"
−∞
x(t + τ)w(τ)ej(τ2/2)cotαe−juτcscαdτ.
(9.79)
Relation between the windowed FRFT and the ﬁrst-order LPFT is
Xw,α(t,u) =
=
1 −jcotα
2π
ej(u2/2)cotαLPFTΩ1(t,Ω)
(9.80)

614
Time-Frequency Analysis
where Ω1 = cot(α)/2 and Ω= ucsc(α). Thus, all results can be easily
converted from the ﬁrst-order LPFT to the windowed FRFT, and vice versa.
That is the reason why we will not present a detailed analysis for this
transform after the LPFT has been presented.
By using a window, local forms of the FRFT are introduced as:
STFTα(u,v) =
∞
"
−∞
Xα(u + τ)w(τ)e−jvτdτ
(9.81)
STFTα(u,v) =
∞
"
−∞
x(t + τ)w(τ)Kα(u,τ)dτ
(9.82)
meaning that the lag truncation could be applied after signal rotation or
prior to the rotation. Results are similar. A similar relation for the moments,
like (9.75) in the case of LPFT, could be derived here. It states that any FRFT
moment can be calculated if we know just any three of its moments.
9.5
HIGH-RESOLUTION STFT
High-resolution techniques are developed for efﬁcient processing and sepa-
ration of very close sinusoidal signals (in array signal processing, separation
of sources with very close DOAs). Among these techniques the most widely
used are Capon’s method, MUSIC, and ESPRIT. The formulation of high-
resolution techniques could be extended to the time-frequency representa-
tions. Here we will present a simple formulation of the STFT and the LPFT
within Capon’s method framework.
9.5.1
Capon’s STFT
Here we will present the STFT formulation in a common array signal-
processing notation. The STFT of a discrete time signal x(n) in (causal)
notation
STFT(ω,n) = 1
N
N−1
∑
n=0
x(n + m)e−jωm

Ljubiša Stankovi´c
Digital Signal Processing
615
can be written as
STFT(ω,n) = ˆsω(n) = hHx(n) = 1
N aH(ω)x(n)
aH(ω) = [1 e−iω e−iω2...e−iω(N−1)]
(9.83)
x(n) = [x(n) x(n + 1) x(n + 2)... x(n + N −1)]T,
where T denotes the transpose operation, and H denotes the conjugate and
transpose (Hermitian) operation. Normalization of the STFT with N is done,
as in the robust signal analysis.
The average power of the output signal ˆsω(n), over M samples (ergod-
icity over M samples around n is assumed), for a frequency ω, is
P(ω) = 1
M ∑
n
|ˆsω(n)|2
(9.84)
= 1
N2 aH(ω) 1
M ∑
n
[x(n)xH(n)]a(ω) = 1
N2 aH(ω) ˆRxa(ω),
where ˆRx is the matrix deﬁned by
ˆRx = 1
M ∑
n
x(n)xH(n).
The standard STFT (9.83) can be derived based on the following con-
sideration. Find h as a solution of the problem
min
h {hHh}
subject to hHa(ω) = 1.
(9.85)
This minimization problem will be explained through the next example.
Example 9.28. Show that the output power of the ﬁlter producing s(n) = hHx(n)
is minimized for the input x(n) = Aa(ω) + ε(n), with respect the input white
noise ε(n), whose autocorrelation function is ˆRε = ρI if hHh is minimal
subject to hHa(ω) = 1.
⋆The output for the noise only is sε(n) = hHε(n), while its average
power is
1
M ∑
n
|hHε(n)|2 = 1
M ∑
n
hHε(n)εH(n)h
= hH
(
1
M ∑
n
ε(n)εH(n)
)
h =ρhHh.

616
Time-Frequency Analysis
Minimization of hHh is therefore equivalent to the output white noise power
minimization.
The condition hHa(ω) = 1 means that the input in form of a sinusoid
Aa(ω), at frequency ω, should not be changed, that is, if x(n) = Aa(ω), then
hHx(n) = hHAa(ω) = A.
Thus, the condition hHa(ω) = 1 means that the estimate is unbiased with
respect to input sinusoidal signal with amplitude A.
The solution of minimization problem (9.85) is
∂
∂hH {hHh + λ(hHa(ω) −1)} = 0
subject to hHa(ω) = 1,
2h = −λa(ω)
subject to hHa(ω) = 1
resulting in
h =
a(ω)
aH(ω)a(ω) = 1
N a(ω)
(9.86)
and the estimate (9.83), which is the standard STFT, follows.
Consider now a different optimization problem, deﬁned by
min
h { 1
M ∑
n
|hHx(n)|2}
subject to hHa(ω) = 1.
(9.87)
Two points are emphasized in this optimization problem. First, the
weights are selected to minimize the average power
1
M ∑n |hHx(n)|2 of
the output signal of the ﬁlter. It means that the ﬁlter should give the best
possible suppression of all components of signals-plus-noise components of
the observations as well as a suppression of the components of the desired
signal for all time-instants (minimization of the power of y(n)). Second, by
setting the condition hHa(ω) = 1, in the considered time instant n the signal
amplitude is preserved at the output.
The optimization problem can be rewritten in the form
min
h { 1
M ∑
n
hHx(n)xH(n)h}
subject hHa(ω) = 1.
By denoting
ˆRx = 1
M ∑
n
x(n)xH(n),
we get

Ljubiša Stankovi´c
Digital Signal Processing
617
min
h {hH ˆRxh}
subject to hHa(ω) = 1.
The constrained minimization
∂
∂hH {hH ˆRxh + λ(hHa(ω) −1)} = 0
subject to hHa(ω) = 1.
gives the solution
h = −ˆR−1
x
λa(ω)
2
subject to hHa(ω) = 1.
(9.88)
The solution can be written in the form
ˆh =
ˆR−1
x a(ω)
aH(ω) ˆR−1
x a(ω)
,
(9.89)
where
ˆRx = 1
M ∑
n
x(n)xH(n).
(9.90)
The output signal power, in these cases, corresponds to Capon’s form
of the STFT, deﬁned by
SCapon(ω) = 1
M ∑
n
|hHx(n)|2 = hH ˆRxh
(9.91)
=
(
ˆR−1
x a(ω)
aH(ω) ˆR−1
x a(ω)
)H
ˆRx
ˆR−1
x a(ω)
aH(ω) ˆR−1
x a(ω)
(9.92)
=
1
aH(ω) ˆR−1
x a(ω)
.
(9.93)
Note that aH(ω) ˆR−1
x a(ω) is a real valued scalar. Along with (9.90), we
can use a sliding window estimate of the autocorrelation matrix in the form
ˆRx(n) =
1
K + 1
n+K/2
∑
p=n−K/2
x(p)xH(p),
(9.94)
where K is a parameter deﬁning the width of a symmetric sliding window.
Inserting ˆRx(n,K) instead of ˆRx in (9.91) gives the STFT with weights mini-
mizing the output power in (9.87), for the observations in the neighborhood
of the time instant of interest n.

618
Time-Frequency Analysis
The mean value of this power function, calculated in the neighborhood
of the time n over the window used in (9.94), gives an averaged Capon’s
STFT as follows
SCapon(n,ω) =
1
aH(ω) ˆR−1
x (n)a(ω)
.
(9.95)
where n indicates the time instant of the interest and the mean is calculated
over the observations y(n) in the corresponding window.
In the realization the autocorrelation function is regularized by a unity
matrix I thus, we use
ˆR(n) =
1
K + 1
n+K/2
∑
p=n−K/2
x(p)xH(p) + ρI.
(9.96)
instead of ˆRx(n) for the inverse calculation in (9.95) and (9.91).
9.5.2
MUSIC STFT
In the MUSIC formulation of the high resolution STFT the eigenvalue
decomposition of the autocorrelation matrix (9.96) is used as
ˆR(n) =
1
K + 1
n+K/2
∑
p=n−K/2
x(p)xH(p) + ρI = VH(n)Λ(n)V(n),
ˆR−1(n) = VH(n)Λ−1(n)V(n).
Note that the Capon spectrogram, using eigenvalues and eigenvectors of
the autocorrelation matrix, can be written as
SCapon(n,ω) =
1
aH(ω)VH(n)Λ−1(n)V(n)a(ω)
=
1
N
∑
k=1
1
λk |STFTk(n,ω)|2
where
STFTk(n,ω) = aH(ω)vk(n)
is the STFT of the kth eigenvector (column) of the autocorrelation matrix
ˆR(n), corresponding to the eigenvalue λk. If the signal has N −M com-
ponents then the ﬁrst N −M largest eigenvalues λk (corresponding to the

Ljubiša Stankovi´c
Digital Signal Processing
619
smallest values 1/λk) will represent the signal space (components), and the
remaining M eigenvalues will correspond to the noise space (represented
by ρI in the deﬁnition of autocorelation matrix ˆR(n)).
If a frequency ω corresponds to a signal component, then all eigenvec-
tors corresponding to the noise space will be orthogonal to that harmonic,
being represented by aH(ω). It means that the spectrograms of all noise
space only components will be very small at the frequencies corresponding
to the signal frequencies.
The MUSIC STFT is deﬁned based on this fact. It is calculated using
the eigenvectors corresponding to noise space, as
SMUSIC(n,ω) =
1
aH(ω)VH
MVMa(ω) =
1
N
∑
k=N−M+1
|STFTk(n,ω)|2
,
(9.97)
where VM is the eigenvector matrix containing only M eigenvectors corre-
sponding to the M lowest eigenvalues in Λ, representing the space of noise.
In this case the signal has N −M components corresponding to the largest
eigenvalues. A special case with M = 1 is the Pisarenko method.
Example 9.29. Calculate high resolution forms of the spectrogram for two-
component signal whose frequencies ω0 + ∆ω and ω0 −∆ω may be con-
sidered as constants around the instant of interest n = 128,
x(n) = exp(jn(ω0 + ∆ω)) + exp(jn(ω0 −∆ω)),
ω0 = 1 and ∆ω = 0.05.
In the STFT calculation use a rectangular window of the width N = 16.
Use 15 samples for averaging (estimation) of the autocorrelation matrix,
as well as its regularization by a 0.0001 · I (corresponding to noise signal
x(n) + ε(n), where ε(n) is complex white noise with variance σ2ε = 0.0001).
Assume that signal samples needed for autocorrelation function estimation
are also available.
⋆Signal values around n = 128 are considered. The STFT is calculated
using N = 16 signal samples
x(128) = [x(128) x(129) x(130)... x(143)]T
and a rectangular window. The mainlobe with of this window is D = 4π/N =
π/4 = 0.7854. Its will not be able to resolve two components closer than
2∆ω ∼D/2 = 0.3927. Considered ∆ω = 0.05 is well below this limit. The
STFT is interpolated in frequency up to 2048 samples. The result is shown in
Fig. 9.41(a). Next the autocorrelation matrix
ˆR(128) = 1
15
128+7
∑
p=128−7
x(p)xH(p) + 0.00001 · I

620
Time-Frequency Analysis
is estimated using the signal vectors x(p) = [x(p) x(p + 1) x(p + 2)... x(p +
15)]. Note that values of signal from x(128 −7) for p = 128 −7 up to p =
128 + 7 + 15 are needed for this calculation. Values of vector
a(ω) = [1 eiω eiω2...eiω(N−1)]T
are calculated at the frequencies of interest ω = 2πk/2048, for k = 0,1,2,...,1023.
The Capon’s STFT is then
SCapon(128,ω) =
1
aH(ω) ˆR−1(128)a(ω) =
1
16
∑
k=1
1
λk |STFTk(n,ω)|2
.
Its value is presented in Fig. 9.41(b),(d).
The MUSIC spectrogram is obtained by calculating the eigenvectors of
ˆR(128) and using only N −2 eigenvectors corresponding to the noise space
eigenvalues of this matrix (there are 2 signal components)
SMUSIC (n,ω) =
1
aH(ω)VH
14V14a(ω) =
1
16
∑
k=3
|STFTk(n,ω)|2
where V14 is a 14 × 16 matrix containing 14 eigenvectors vk(n), k = 3,4,...16,
corresponding to the noise space (2 eigenvectors corresponding to two largest
eigenvalues, being the signal space, are omitted). The STFT of eigenvector
vk(n) is denoted by STFTk(n,ω). The MUSIC spectrogram is presented in
Fig. 9.41(c),(e).
The case corresponding to one eigenvector being used in the spectro-
gram |STFT16(n,ω)|2 (a form of Pisarenko spectrogram, when only the low-
est eigenvector is considered as the noise space) is presented in Fig. 9.41(f).
Note that in the case of Pisarenko spectrogram it is sufﬁcient (and required
by its deﬁnition) to use only N = 3 window width (number of components
plus one).
Normalized values of all spectrograms are presented in Fig. 9.41.
9.5.3
Capon’s LPFT
With varying coefﬁcients or appropriate signal multiplication, before the
STFT calculation, a local polynomial version of Capon’s transform could be
deﬁned. For example, for a linear frequency-modulated signal of the form
x(n) = Aej(α0n2+ω0n+ϕ0)

Ljubiša Stankovi´c
Digital Signal Processing
621
0
1
2
3
0
0.5
1
Ω
 (a)
Spectrogram
0
1
2
3
0
0.5
1
Ω
 (b)
Capon spectrogram (normalized)
0.95
1
1.05
10
-4
10
-3
10
-2
10
-1
Ω
 (d)
Capon spectrogram (zoomed log scale)
0
1
2
3
0
0.5
1
Ω
 (d)
MUSIC spectrogram (normalized)
0.95
1
1.05
10
-4
10
-3
10
-2
10
-1
Ω
 (e)
MUSIC spectrogram (zoomed log scale)
0.95
1
1.05
10
-4
10
-3
10
-2
10
-1
Ω
 (f)
Pisarenko spectrogram (zoomed log scale)
Figure 9.41
(a) The standard STFT using a rectangular window N = 16. The STFT is interpo-
lated in frequency up to 2048 samples. (b) Capon’s spectrogram calculated in 2048 frequency
points. (c) MUSIC spectrogram calculated in 2048 frequency points. (d) Capon’s spectrogram
zoomed to the signal components. (e) MUSIC spectrogram zoomed to the signal components.
(f) Pisarenko spectrogram zoomed to the signal components.
we should use (9.95) or (9.91) with a signal of the form
ˆRx(n,K,α) =
1
K + 1
n+K/2
∑
p=n−K/2
xα(p)xH
a (p)
with xα(p) = x(p)e−jαp2,
with α as a parameter. The high-resolution form of the LPFT can be used for
efﬁcient processing of close linear frequency-modulated signals, with the
same rate within the considered interval.

622
Time-Frequency Analysis
t
Ω
 (a)
-500
0
500
-0.5
0
0.5
t
Ω
 (c)
-500
0
500
-0.5
0
0.5
t
Ω
 (b)
-500
0
500
-0.5
0
0.5
t
Ω
 (d)
-500
0
500
-0.5
0
0.5
Figure 9.42
(a) The standard STFT, (b) the LPFT, (c) Capon’s STFT, and (d) Capon’s LPFT-
based representations of two close almost linear frequency-modulated signals.
Example 9.30. The Capon LPFT form is illustrated on an example with a signal
with two close components
x(t) = exp(j128πt(0.55 −t/2) + j5πt3) + exp(j128πt(0.45 −t/2) + j5πt3),
that in addition to the linear frequency-modulated contained a small disturb-
ing cubic phase term. The considered time interval was −1 ≤t ≤1 −∆t with
∆t = 2/512, ρ = 0.5, K = 30, and the frequency domain is interpolated eight
times. The standard STFT, LPFT, Capon’s STFT, and Capon’s LPFT-based rep-
resentations are presented in Fig. 9.42.
In general, higher-order polynomial or any other nonstationary signal,
with appropriate parametrization, can be analyzed in the same way.
9.6
WIGNER DISTRIBUTION
The dimensions of the STFT blocks (resolutions) are determined by the
window width. The best STFT for a signal would be the one whose win-
dow form ﬁts the best to the signal’s time-frequency content. Consider,
for example, an important and simple signal such as a linear frequency

Ljubiša Stankovi´c
Digital Signal Processing
623
Optimal STFT with a Hann window
Wigner distribution with a Hann window
Figure 9.43
Optimal STFT (absolute value, calculated with optimal window width) and the
Wigner distribution of a linear frequency modulated signal.
modulated (LFM) chirp. For simplicity of analysis assume that its instan-
taneous frequency (IF) coincides with the time-frequency plane diagonal.
It is obvious that, due to symmetry, both time and frequency resolution
are equally important. Therefore, the best STFT would be the one calcu-
lated by using a constant window whose (equivalent) widths are equal
in time and frequency domain. With such a window both resolutions will
be the same. However, these resolutions could be unacceptably low for
many applications. It means that the STFT, including all of its possible time
and/or frequency-varying window forms, would be unacceptable as a time-
frequency representation of this signal. The overlapping STFT could be used
for better signal tracking, without any effect on the resolution.
A way to improve time-frequency representation of this signal is in
transforming the signal into a sinusoid whose constant frequency is equal to
the instantaneous frequency value of the linear frequency modulated signal
at the considered instant. Then, a wide window can be used, with a high
frequency resolution. The obtained result is valid for the considered instant
only and the signal transformation procedure should be repeated for each
instant of interest.
A simple way to introduce this kind of signal representation is pre-
sented. Consider an LFM signal,
x(t) = Aexp(jφ(t)) = Aexp(j(at2/2 + bt + c)).

624
Time-Frequency Analysis
Its instantaneous frequency changes in time as
Ωi(t) = dφ(t)/dt = at + b.
One of the goals of time-frequency analysis is to obtain a function that will
(in an ideal case) fully concentrate the signal power along its instantaneous
frequency. The ideal representation would be
I(t,Ω) = 2πA2δ(Ω−Ωi(t)).
For a quadratic function φ(t), it is known that
τ dφ(t)
dt
= φ(t + τ
2 ) −φ(t −τ
2 )
= τ(at + b) = τΩi(t).
This property can easily be converted into an ideal time-frequency repre-
sentation for the linear frequency modulated signal by using
FTτ{x(t + τ/2)x∗(t −τ/2)} =
FTτ{A2ejΩi(t)τ} = 2πA2δ(Ω−Ωi(t)).
The Fourier transform of x(t + τ/2)x∗(t −τ/2) over τ, for a given t, is called
the Wigner distribution. It is deﬁned as
WD(t,Ω) =
∞
"
−∞
x(t + τ/2)x∗(t −τ/2)e−jΩτdτ.
(9.98)
The Wigner distribution is originally introduced in quantum mechanics.
The illustration of the Wigner distribution calculation is presented in Fig.
9.44.
Expressing x(t) in terms of X(Ω) and substituting it into (9.98) we get
WD(t,Ω) = 1
2π
∞
"
−∞
X(Ω+ θ/2)X∗(Ω−θ/2)ejθtdθ
(9.99)
what represents a deﬁnition of the Wigner distribution in the frequency
domain.

Ljubiša Stankovi´c
Digital Signal Processing
625
t
  x(t)
 considered instant t
τ
  x(t+τ/2)
τ
  x(t- τ/2)
τ
  x(t+τ/2)x*(t- τ/2)
Ω
  FT{x(t+τ/2)x*(t- τ/2)}
Ω
t  
 WD(t,Ω) 
Figure 9.44
Illustration of the Wigner distribution calculation, for a considered time instant t.
Real values of a linear frequency modulated signal (linear chirp) are presented.
It is easy to show that the Wigner distribution satisﬁes the marginal
properties. From the Wigner distribution deﬁnition, it follows
x(t + τ/2)x∗(t −τ/2) = IFT{WD(t,Ω)} = 1
2π
∞
"
−∞
WD(t,Ω)ejΩτdΩ(9.100)
which, for τ = 0, produces (9.126)
|x(t)|2 = 1
2π
∞
"
−∞
WD(t,Ω)dΩ.
(9.101)
Based on the deﬁnition of the Wigner distribution in the frequency domain,
(9.99), one may easily prove the fulﬁllment of the frequency marginal.
Example 9.31. Find the Wigner distribution of signals: (a) x(t) = δ(t −t1) and (b)
x(t) = exp(jΩ1t).

626
Time-Frequency Analysis
⋆The Wigner distribution of signal x(t) = δ(t −t1) is
WD(t,Ω) =
∞
"
−∞
δ(t −t1 + τ/2)δ(t −t1 −τ/2)e−jΩτdτ
= 2δ(2(t −t1))e−j2Ω(t−t1) = δ(t −t1),
since |a|δ(at)x(t) = δ(t)x(0). From the Wigner distribution deﬁnition in
terms of the Fourier transform, for x(t) = exp(jΩ1t) with X(Ω) = 2πδ(Ω−
Ω1), follows
WD(t,Ω) = 2πδ(Ω−Ω1).
A high concentration of time-frequency representation for both of these
signals is achieved. Note that this fact does not mean that we will be able
to achieve an arbitrary high concentration simultaneously, in a point, in the
time-frequency domain.
Example 9.32. Consider a linear frequency modulated signal, x(t) = Aejbt2/2. Find
its Wigner distribution.
⋆In this case we have
x(t + τ/2)x∗(t −τ/2) = |A|2ejbtτ
with
WD(t,Ω) = 2π|A|2δ(Ω−bt).
Again, a high concentration along the instantaneous frequency in the time-
frequency plane may be achieved for the linear frequency modulated signals.
These two examples demonstrate that the Wigner distribution can
provide superior time-frequency representation of one-component signal,
in comparison to the STFT.
Example 9.33. Calculate the Wigner distribution for a linear frequency modulated
signal, with Gaussian amplitude (Gaussian chirp signal)
x(t) = Ae−at2/2ej(bt2/2+ct).
⋆For the chirp signal, the local autocorrelation function reads as
R(t,τ) = x(t + τ/2)x∗(t −τ/2) = |A|2e−at2e−aτ2/4ejbtτ+jcτ.
The Wigner distribution is obtained as the Fourier transform of R(t,τ),
WD(t,Ω) = 2|A|2e−at2=
π
a e−(Ω−bt−c)2
a
.
(9.102)

Ljubiša Stankovi´c
Digital Signal Processing
627
The Wigner distribution from the previous example is obtained with c = 0
and a →0, since 2√
π/ae−Ω2/a →2πδ(Ω) as a →0.
The Wigner distribution of the Gaussian chirp signal is always positive,
as it could be expected from a distribution introduced with the aim to
represent local density of signal energy. Unfortunately, this is the only signal
when the Wigner distribution is always positive, for any point in the time-
frequency plane (t,Ω). This drawback is not the only reason why the study of
time-frequency distributions does not end with the Wigner distribution.
9.6.1
Auto-Terms and Cross-Terms in the Wigner Distribution
For the multi-component signal
x(t) =
M
∑
m=1
xm(t)
the Wigner distribution has the form
WD(t,Ω) =
M
∑
m=1
M
∑
n=1
∞
"
−∞
xm
B
t + τ
2
C
x∗
n
B
t −τ
2
C
e−jΩτdτ.
Besides the auto-terms
WDat(t,Ω) =
M
∑
m=1
∞
"
−∞
xm(t + τ
2 )x∗
m(t −τ
2 )e−jΩτdτ,
the Wigner distribution contains a signiﬁcant number of cross-terms,
WDct(t,Ω) =
M
∑
m=1
M
∑
n=1
n̸=m
∞
"
−∞
xm(t + τ
2 )x∗
n(t −τ
2 )e−jΩτdτ.
Usually, they are not desirable in the time-frequency signal analysis. Cross-
terms can mask the presence of auto-terms, which makes the Wigner distri-
bution unsuitable for the time-frequency analysis of signals.
For a two-component signal with auto-terms located around (t1,Ω1)
and (t2,Ω2) (see Fig.9.45) the oscillatory cross-terms are located around
((t1 + t2)/2,(Ω1 + Ω2)/2).
Example 9.34. Analyze auto-terms and cross-terms for two-component signal of
the form
x(t) = e−1
2 (t−t1)2ejΩ1t + e−1
2 (t+t1)2e−jΩ1t

628
Time-Frequency Analysis
0
t1
t2
Ω1 
Ω2 
t
Ω 
Auto-term
Auto-term
Oscillatory
cross-term
Figure 9.45
Wigner distribution of two component signal.
⋆In this case we have
WD(t,Ω) = 2√
πe−(t−t1)2−(Ω−Ω1)2 + 2√
πe−(t+t1)2−(Ω+Ω1)2
+ 4√
πe−t2−Ω2 cos(2t1Ω−2Ω1t)
where the ﬁrst and second terms represent auto-terms while the third term
is a cross-term. Note that the cross-term is oscillatory in both directions. The
oscillation rate along the time axis is proportional to the frequency distance
between components 2Ω1, while the oscillation rate along frequency axis is
proportional to the distance in time of components, 2t1. The oscillatory nature
of cross-terms will be used for their suppression.
To analyze auto-terms and cross-terms, the well-known ambiguity
function can be used as well. It is deﬁned as:
AF(θ,τ) =
∞
"
−∞
x
B
t + τ
2
C
x∗B
t −τ
2
C
e−jθtdt.
(9.103)
It is already a classical tool in optics as well as in radar and sonar signal
analysis.

Ljubiša Stankovi´c
Digital Signal Processing
629
The ambiguity function and the Wigner distribution form a two-
dimensional Fourier transform pair
AF(θ,τ) = FT2D
t,Ω{WD(t,Ω)},
WD(t,Ω) = 1
2π
∞
"
−∞
∞
"
−∞
⎡
⎣
∞
"
−∞
x(u + τ
2 )x∗(u −τ
2 )e−jθudu
⎤
⎦ejθt−jΩτdτdθ,
where the integration over frequency related variable θ assumes factor
1/(2π) and the positive sign in the exponent exp(jθt).
Consider a signal whose components are limited in time to
xm(t) ̸= 0
only for
|t −tm| < Tm.
In the ambiguity (θ,τ) domain we have xm(t + τ/2)x∗
m(t −τ/2) ̸= 0 only
for
−Tm < t −tm + τ/2 < Tm
−Tm < t −tm −τ/2 < Tm.
It means that xm(t + τ/2)x∗
m(t −τ/2) is located within |τ| < 2Tm, i.e.,
around the θ-axis independently of the signal’s position tm. Cross-term
between signal’s m-th and n-th component is located within |τ + tn −tm| <
Tm + Tn. It is dislocated from τ = 0 for two components that do not occur
simultaneously, i.e., when tm ̸= tn.
From the frequency domain deﬁnition of the Wigner distribution a
corresponding ambiguity function form follows
AF(θ,τ) = 1
2π
∞
"
−∞
X
*
Ω+ θ
2
+
X∗
*
Ω−θ
2
+
ejΩτdΩ.
(9.104)
From this form we can conclude that the auto-terms of the components,
limited in frequency to Xm(Ω) ̸= 0 only for |Ω−Ωm| < Wm, are located
in the ambiguity domain around τ-axis within the region |θ/2| < Wm. The
cross-terms are within
|θ + Ωn −Ωm| < Wm + Wn,
where Ωm and Ωn are the frequencies around which the Fourier transform
of each component lies.

630
Time-Frequency Analysis
0 
0
θ2
θ1
τ1 
τ2 
θ
τ  
Cross-term
Cross-term
Auto-terms
| AF (θ,τ) |
Figure 9.46
Auto and cross-terms for two-component signal in the ambiguity domain.
Therefore, all auto-terms are located along and around the ambiguity
domain axis. The cross-terms, for the components which do not overlap in
the time and frequency, simultaneously, are dislocated from the ambiguity
axes, Fig. 9.46. This property will be used in the deﬁnition of the reduced
interference time-frequency distributions.
The ambiguity function of a four-component signal consisting of two
Gaussian pulses, one sinusoidal and one linear frequency modulated com-
ponent is presented in 9.47.
Example 9.35. Let us consider signals of the form
x1(t) = e−1
2 t2
x2(t) = e−1
2 (t−t1)2ejΩ1t + e−1
2 (t+t1)2e−jΩ1t
The ambiguity function of x1(t) is
AFx1(θ,τ) = √
πe−1
4 τ2−1
4 θ2
while the ambiguity function of two-component signal x2(t) is
AFx2(θ,τ) = √
πe−1
4 τ2−1
4 θ2ejΩ1τe−jt1θ + √
πe−1
4 τ2−1
4 θ2e−jΩ1τejt1θ+
√
πe−1
4 (τ−2t1)2−1
4 (θ−2Ω1)2 + √
πe−1
4 (τ+2t1)2−1
4 (θ+2Ω1)2

Ljubiša Stankovi´c
Digital Signal Processing
631
-3
-2
-1
0
1
2
3
-100
-50
0
50
100
θ
AF(θ,τ)
τ
Figure 9.47
Ambiguity function of signal from Fig.9.4
In the ambiguity domain (θ,τ) auto-terms are located around (0,0) while
cross-terms are located around (2Ω1,2t1) and (−2Ω1,−2t1) as presented in
Fig. 9.46.
9.6.2
Wigner Distribution Properties
A list of the properties satisﬁed by the Wigner distribution follows. The
obvious ones will be just stated, while the proofs will be given for more
complex ones. In the case when the Wigner distributions of more than one
signal are considered, the signal will be added as an index in the Wigner
distribution notation. Otherwise signal x(t) is assumed, as a default signal
in the notation.
P1 – Realness
For any signal holds,
WD∗(t,Ω) = WD(t,Ω).
P2 – Time-shift property
The Wigner distribution of a signal shifted in time
y(t) = x(t −t0),
is
WDy(t,Ω) = WDx(t −t0,Ω).

632
Time-Frequency Analysis
P3 – Frequency shift property
For a modulated signal
y(t) = x(t)ejΩ0t,
we have
WDy(t,Ω) = WDx(t,Ω−Ω0).
P4 – Time marginal property
1
2π
∞
"
−∞
WD(t,Ω)dΩ= |x(t)|2.
P5 – Frequency marginal property
∞
"
−∞
WD(t,Ω)dt = |X(Ω)|2.
P6 – Time moments property
1
2π
∞
"
−∞
∞
"
−∞
tnWD(t,Ω)dtdΩ=
∞
"
−∞
tn|x(t)|2dt.
⋆This property follows from
1
2π
& ∞
−∞WD(t,Ω)dΩ= |x(t)|2.
P7 -Frequency moments property
∞
"
−∞
∞
"
−∞
ΩnWD(t,Ω)dΩdt =
∞
"
−∞
Ωn|X(Ω)|2dΩ.
P8 – Scaling
For a scaled version of the signal
y(t) =
F
|a|x(at), a ̸= 0,
the Wigner distribution reads
WDy(t,Ω) = WDx(at,Ω/a).

Ljubiša Stankovi´c
Digital Signal Processing
633
P9 – Instantaneous frequency property
For x(t) = A(t)ejφ(t)
& ∞
−∞ΩWD(t,Ω)dΩ
& ∞
−∞WD(t,Ω)dΩ
= Ωi(t) = d
dt arg[x(t)] = φ′(t).
(9.105)
⋆In order to prove this property, we will use the derivative of the
inverse Fourier transform of the Wigner distribution
d[x(t + τ/2)x∗(t −τ/2)]
dτ
= 1
2π
∞
"
−∞
jΩWD(t,Ω)ejΩτdΩ
with x(t) = A(t)ejφ(t), calculated at τ = 0. It results in
j
2π
∞
"
−∞
ΩWD(t,Ω)dΩ= 1
2[x′(t)x∗(t) −x(t)x∗′(t)] = jφ′(t)A2(t).
With the frequency marginal property & ∞
−∞WD(t,Ω)dΩ= 2πA2(t), this
property follows.
P10 – Group delay
For signal whose Fourier transform is of the form X(Ω) = |X(Ω)|ejΦ(Ω),
the group delay tg(Ω) = −Φ′(Ω) is
& ∞
−∞tWD(t,Ω)dt
& ∞
−∞WD(t,Ω)dt = tg(Ω) = −d
dΩarg[X(Ω)] = −Φ′(Ω).
The proof is the same as in the instantaneous frequency case, using the
frequency domain relations.
P11 – Time constraint
If x(t) = 0 for t outside [t1,t2], then WD(t,Ω) = 0 for t outside [t1,t2].
⋆The Wigner distribution is a function of x(t + τ/2)x∗(t −τ/2). If
x(t) = 0 for t outside [t1,t2] then x(t + τ/2)x∗(t −τ/2) is different from
zero within
t1 ≤t + τ/2 ≤t2
and
t1 ≤t −τ/2 ≤t2.
The range of values of t deﬁned by the previous inequalities is t1 ≤t ≤
t2.
P12 – Frequency constraint

634
Time-Frequency Analysis
If X(Ω) = 0 for Ωoutside [Ω1,Ω2], then, also WD(t,Ω) = 0 for Ωoutside
[Ω1,Ω2].
P13 – Convolution
WDy(t,Ω) =
∞
"
−∞
WDh(t −τ,Ω)WDx(τ,Ω)dτ.
for
y(t) =
∞
"
−∞
h(t −τ)x(τ)dτ,
P14 – Product
WDy(t,Ω) = 1
2π
∞
"
−∞
WDh(t,Ω−v)WDx(t,v)dv
for
y(t) = h(t)x(t).
⋆The local autocorrelation of y(t) is h(t + τ/2)h∗(t −τ/2)x(t +
τ/2)x∗(t −τ/2). Thus, the Wigner distribution of y(t) is the Fourier trans-
form of the product of local autocorrelations h(t + τ/2)h∗(t −τ/2) and
x(t + τ/2)x∗(t −τ/2). It is a convolution in frequency of the corresponding
Wigner distributions of h(t) and x(t). Property P13 could be proven in the
same way using the Fourier transforms of signals h(t) and x(t).
P15 – Fourier transform property
WDy(t,Ω) = WDx(−Ω/c,ct)
(9.106)
for
y(t) =
F
|c|/(2π)X(ct), c ̸= 0.
⋆Here the signal y(t) is equal to the scaled version of the Fourier
transform of signal x(t),
WDy(t,Ω) = |c|
2π
∞
"
−∞
X
B
ct + cτ
2
C
X∗B
ct −cτ
2
C
e−jΩτdτ
= 1
2π
∞
"
−∞
X
*
ct + θ
2
+
X∗
*
ct −θ
2
+
ej(−Ω/c)θdθ.
(9.107)

Ljubiša Stankovi´c
Digital Signal Processing
635
Comparing (9.99) to (9.98), with ct →Ωand (−Ω/c) →t, we get
WDy(t,Ω) =
∞
"
−∞
x
*
−Ω
c + τ
2
+
x∗
*
−Ω
c −τ
2
+
e−jctτdτ = WDx
*
−Ω
c ,ct
+
.
P16 – Chirp convolution
WDy(t,Ω) = WDx
*
t −Ω
c ,Ω
+
(9.108)
for
y(t) = x(t) ∗
F
|c|ejct2/2.
⋆With Y(Ω) = FT{x(t) ∗t
,
|c|ejct2/2} =
,
2πjX(Ω)e−jΩ2/(2c) and the
signal’s Fourier transform-based deﬁnition of the Wigner distribution, proof
of this property reduces to the next one.
P17 – Chirp product
WDy(t,Ω) = WDx(t,Ω−ct)
for
y(t) = x(t)ejct2/2.
⋆The Wigner distribution of y(t) is
∞
"
−∞
x
B
t + τ
2
C
ejc(t+τ/2)2/2x∗B
t −τ
2
C
e−jc(t−τ/2)2/2e−jΩτdτ
=
∞
"
−∞
x
B
t + τ
2
C
x∗B
t −τ
2
C
ejctτe−jΩτdτ = WDx(t,Ω−ct).
(9.109)
P18 – Moyal property
1
2π
∞
"
−∞
∞
"
−∞
WDx(t,Ω)WDy(t,Ω)dtdΩ=
''''''
∞
"
−∞
x(t)y(t)dt
''''''
2
.
(9.110)

636
Time-Frequency Analysis
⋆This property follows from
1
2π
∞
"
−∞
∞
"
−∞
∞
"
−∞
x
B
t + τ1
2
C
x∗B
t −τ1
2
C
y
B
t + τ2
2
C
y∗B
t −τ2
2
C
×
∞
"
−∞
e−jΩτ1e−jΩτ2dΩdτ1dτ2dt
=
∞
"
−∞
∞
"
−∞
x
B
t + τ
2
C
x∗B
t −τ
2
C
y
B
t −τ
2
C
y∗B
t + τ
2
C
dτdt.
With t + τ/2 = u and t −τ/2 = v, we get
=
∞
"
−∞
x(u)y∗(u)du
∞
"
−∞
x∗(v)y(v)dv =
''''''
∞
"
−∞
x(t)y(t)dt
''''''
2
.
9.6.3
Pseudo and Smoothed Wigner Distribution
In practical realizations of the Wigner distribution, we are constrained with
a ﬁnite time lag τ. A pseudo form of the Wigner distribution is then used. It
is deﬁned as
PWD(t,Ω) =
∞
"
−∞
w(τ/2)w∗(−τ/2)x(t + τ/2)x∗(t −τ/2)e−jΩτdτ (9.111)
where window w(τ) localizes the considered lag interval. If w(0) = 1, the
pseudo Wigner distribution satisﬁes the time marginal property. Note that
the pseudo Wigner distribution is smoothed in the frequency direction with
respect to the Wigner distribution
PWD(t,Ω) = 1
2π
∞
"
−∞
WD(t,θ)We(Ω−θ)dθ
where We(Ω) is a Fourier transform of w(τ/2)w∗(−τ/2).
The pseudo Wigner distribution example for multi-component signals
is presented in Fig.9.48. The pseudo Wigner distribution example for multi-
component signals is presented in Fig.9.48. Mono-component case with

Ljubiša Stankovi´c
Digital Signal Processing
637
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
 (a)
Ω
PWD1(t,Ω)
t
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
 (b)
Ω
PWD2(t,Ω)
t
Figure 9.48
Pseudo Wigner distribution of a signals from Fig.9.4
sinusoidally frequency modulated signal is presented in Fig.9.49. Note that
signiﬁcant inner interferences are present.
Monocomponent case with sinusoidally frequency modulated signal
is presented in Fig.9.49. Note that signiﬁcant inner interferences are present.
Example 9.36. For a sinusoidally frequency modulated signal
x(t)= exp(−j32cos(πt/64))
calculate an approximate value of the pseudo Wigner distribution with a
window w(τ) of the width T = 2.

638
Time-Frequency Analysis
0
1
2
3
-100
-50
0
50
100
 (a)
Ω
PWD(t,Ω)
t
0
1
2
3
-100
-50
0
50
100
 (b)
Ω
PWD(t,Ω)
t
Figure 9.49
Pseudo Wigner distribution for sinusoidally frequency modulated signal. Narrow
window (left) and wide window (right).
⋆The pseudo Wigner distribution of this signal is
PWD(Ω,t) =
2
"
−2
ej32cos(π(t−τ/2)/64)e−j32cos(π(t−τ/2)/64)w(τ)e−jΩτdτ.
By using the Taylor expansion
cos
B
πt/64 ± πτ
128
C
= cos(πt/64) ∓π
128 sin(πt/64)τ
−
B π
128
C2
cos(πt/64) τ2
2 +
B π
128
C3
sin(πt/64)
τ3
1,2
6 ,
with |τ1,2| ≤2 in the Taylor series reminder, we get
WD(Ω,t) =
2
"
−2
ejπ/2sin(πt/64)τej32 π3
1283 sin(πt/64)
τ3
1 +τ3
2
6
w(τ)e−jΩτdτ.
Obviously,
'''256 π3
128 sin(πt/64) τ3
1 +τ3
2
6
''' ≤0.081, since |τ1,2| ≤2. Thus, we may
write
PWD(Ω,t) ∼= W(Ω−π/2sin(πt/64)),
where W(Ω) is the Fourier transform of window w8(τ). For a Hann(ing)
window this approximation holds for wider windows as well, since its values
toward the ending points are small, meaning that the effective window width
is lower than the window width itself.

Ljubiša Stankovi´c
Digital Signal Processing
639
9.6.4
Discrete Pseudo Wigner Distribution
If the signal in (9.111) is discretized in τ with a sampling interval ∆t, then a
sum instead of an integral is formed. The pseudo Wigner distribution of a
discrete-lag signal, for a given time instant t, is given by
PWD(t,Ω) =
∞
∑
m=−∞
w
B
m ∆t
2
C
w∗B
−m ∆t
2
C
x
B
t+m ∆t
2
C
x∗B
t−m ∆t
2
C
e−jmΩ∆t∆t.
(9.112)
Sampling in τ with ∆t = π/Ω0, Ω0 > Ωm corresponds to the sampling of
signal x(t + τ/2) in τ/2 with ∆t/2 = π/(2Ω0).
The discrete-lag pseudo Wigner distribution is the Fourier transform
of signal
R(t,m) = w
*
m∆t
2
+
w∗
*
−m∆t
2
+
x
*
t + m∆t
2
+
x∗
*
t −m∆t
2
+
∆t.
For a given instant t, it can be written as
PWD(t,ω) =
∞
∑
m=−∞
R(t,m)e−jmω
with ω = Ω∆t. If the sampling interval satisﬁes the sampling theorem, then
the sum in (9.112) is equal to the integral form (9.111).
A discrete form of the pseudo Wigner distribution, with N + 1 samples
and ω = 2πk/(N + 1), for a given time instant t, is
PWD(t,k) =
N/2
∑
m=−N/2
R(t,m)e−j2πmk/(N+1).
Here, N/2 is an integer. This distribution could be calculated by using the
standard DFT routines.
For discrete-time instants t = n∆t, introducing the notation
R(n∆t,m∆t)
= w
*
m∆t
2
+
w∗
*
−m∆t
2
+
x
*
n∆t + m∆t
2
+
x∗
*
n∆t −m∆t
2
+
∆t
R(n,m) = w
Bm
2
C
w∗B
−m
2
C
x
B
n + m
2
C
x∗B
n −m
2
C
,

640
Time-Frequency Analysis
the discrete-time and discrete-lag pseudo Wigner distribution can be written
as
PWD(n,ω) =
∞
∑
m=−∞
w
Bm
2
C
w∗B
−m
2
C
x
B
n + m
2
C
x∗B
n −m
2
C
e−jmω.
(9.113)
Notation x(n + m/2), for given n and m, should be understood as the
signal value at the instant x((n + m/2)∆t). In this notation, the discrete-time
pseudo Wigner distribution is periodic in ω with period 2π.
Since various discretization steps are used (here and in open litera-
ture), we will provide a relation of discrete indexes to the continuous time
and frequency, for each deﬁnition, as
PWD(t,Ω)|t=n∆t, Ω=
2πk
(N+1)∆t = PWD
*
n∆t,
2πk
(N + 1)∆t
+
→PWD(n,k).
The sign →could be understood as the equality sign in the sense of sam-
pling theorem (Example 2.13). Otherwise it should be considered as a corre-
spondence sign. The discrete form of (9.111), with N + 1 samples, is
PWD
*
n∆t,
2πk
(N + 1)∆t
+
→PWD(n,k)
PWD(n,k) =
N/2
∑
m=−N/2
w
Bm
2
C
w∗B
−m
2
C
x
B
n + m
2
C
x∗B
n −m
2
C
e−j2πkm/(N+1),
where N/2 is an integer, −N/2 ≤k ≤N/2 and ω = Ω∆t = 2πk/(N + 1) or
Ω= 2πk/((N + 1)∆t).
In order to avoid different sampling intervals in time and lag in the
discrete Wigner distribution deﬁnition, the discrete Wigner distribution can
be oversampled in time, as it has been done in lag. It means that the same
sampling interval ∆t/2, for both time and lag axes, can be used. Then, we
can write
R
*
n∆t
2 ,m∆t
+
→R(n,m)
R
*
n∆t
2 ,m∆t
+
= w
*
m∆t
2
+
w∗
*
−m∆t
2
+
x
*
n∆t
2 +m∆t
2
+
x∗
*
n∆t
2 −m∆t
2
+
∆t
R(n,m) = w(m)w∗(−m)x(n + m)x∗(n −m)

Ljubiša Stankovi´c
Digital Signal Processing
641
The discrete-time and discrete-lag pseudo Wigner distribution, in this case,
is of the form
PWD(n,ω) = 2
∞
∑
m=−∞
w(m)w∗(−m)x(n + m)x∗(n −m)e−j2mω.
(9.114)
It corresponds to the continuous-time pseudo Wigner distribution (9.111)
with substitution τ/2 →τ
PWD(t,Ω) = 2
∞
"
−∞
w(τ)w∗(−τ)x(t + τ)x∗(t −τ)e−j2Ωτdτ.
The discrete pseudo Wigner distribution is given here by
PWD
*n∆t
2 ,
4πk
(N + 1)∆t
+
→PWD(n,k)
PWD(n,k) =
N/2
∑
m=−N/2
w(m)w∗(−m)x(n + m)x∗(n −m)e−j4πmk/(N+1)
(9.115)
for −N/2 ≤2k ≤N/2. Since, the standard DFT routines are commonly
used for the pseudo Wigner distribution calculation, we may use every
other (2k) sample in (9.115) or oversample the pseudo Wigner distribution
in frequency (as it has been done in time). Then,
PWD
*n∆t
2 ,
2πk
(N + 1)∆t
+
→PWD(n,k)
PWD(n,k) =
N/2
∑
m=−N/2
w(m)w∗(−m)x(n + m)x∗(n −m)e−j2πmk/(N+1).
(9.116)
This discrete pseudo Wigner distribution, oversampled in both time and in
frequency by factor of 2, has ﬁner time-frequency grid, producing smaller
time-frequency estimation errors at the expense of the calculation complex-
ity.
Example 9.37. Signal x(t) = exp(j31πt2) is considered within −1 ≤t ≤1. Find the
sampling interval of signal for discrete pseudo Wigner distribution calcula-
tion. If the rectangular window of the width N + 1 = 31 is used in analysis,
ﬁnd the pseudo Wigner distribution values and estimate the instantaneous
frequency at t = 0.5 based on the discrete pseudo Wigner distribution.

642
Time-Frequency Analysis
⋆For this signal the instantaneous frequency is Ωi(t) = 62πt. It is
within the range −62π ≤Ωi(t) ≤62π. Thus, we may approximately as-
sume that the maximal frequency is Ωm = 62π.The sampling interval for the
Fourier transform would be ∆t ≤1/62. For the direct pseudo Wigner distri-
bution calculation, it should be twice smaller, ∆t/2 ≤1/124. Therefore, the
discrete version of the pseudo Wigner distribution, normalized with 2
√
∆t,
at t = 0.5 or n = 62, is (9.115)
PWD(n,k) =
15
∑
m=−15
ej31π((n+m)/124)2e−j31π((n−m)/124)2e−j4πmk/31
=
15
∑
m=−15
ejπmn/124e−j4πmk/31 = sin( π
8 (n −16k))
sin( π
248(n −16k)).
The argument k, when the pseudo Wigner distribution reaches maximum for
n = 62, follows from 62 −16k = 0 as
ˆk = arg
!
max
k
PWD(n,k)
6
=
-62
16
.
= 4,
where [·] stands for the nearest integer. Obviously, the exact instantaneous
frequency is not on the discrete frequency grid. The estimated value of the in-
stantaneous frequency at t = 1/2 is ˆΩ= 4πˆk/((N + 1)∆t) = 16π/(31/62) =
32π. The true value is Ωi(1/2) = 31π. When the true frequency is not on
the grid, the estimation can be improved by using the interpolation or dis-
placement bin, as explained in Chapter 1. The frequency sampling inter-
val is ∆Ω= 4π/((N + 1)∆t) = 8π, with maximal estimation absolute error
∆Ω/2 = 4π.
If we used the standard DFT routine (9.116) with N + 1 = 31 and all
available frequency samples, we would get
PWD(n,k) = DFT31
M
ej31π((n+m)/124)2e−j31π((n−m)/124)2N
=
15
∑
m=−15
ej31π((n+m)/124)2e−j31π((n−m)/124)2e−j2πmk/31 = sin( π
8 (n −8k))
sin( π
248(n −8k)).
The maximum would be at ˆk = 8, with the estimated frequency
ˆΩ=
2πˆk/((N + 1)∆t). Thus, ˆΩ= 32π, as expected. By this calculation, the fre-
quency sampling interval is ∆Ω= 2π/((N + 1)∆t) = 4π, with the maximal
estimation absolute error ∆Ω/2 = 2π.
By using an odd number of samples N + 1 in the previous deﬁnitions,
the symmetry of the product x(n + m)x∗(n −m) is preserved in the summa-
tion. However, when an even number of samples is used, that is not the case.
To illustrate this effect, consider a simple example of signal, for n = 0, with

Ljubiša Stankovi´c
Digital Signal Processing
643
N = 4 samples. Then, four values of the signal x(m), used in calculation, are
x(m)
x(−2)
x(−1)
x(0)
x(1)
x(−m)
x(1)
x(0)
x(−1)
x(−2) .
So, in forming the local autocorrelation function, there are several possibili-
ties. One is to omit sample x(−2) and to use an odd number of samples, in
this case as well. Also, it is possible to periodically extend the signal and to
form the product based on
x(m)
···
x(1)
x(−2)
x(−1)
x(0)
x(1)
x(−2)
x(−1)
x(−m)
··· x(−1)
x(−2)
x(1)
x(0)
x(−1)
x(−2)
x(1)
we(m)
···
0
0
we(1)
w0(0)
we(1)
0
0
Here we can use four product terms, but with the ﬁrst one formed as
x(−2)x∗(−2), that is, as x(−N/2)x∗(−N/2). When a lag window with zero
ending value is used (for example, a Hann(ing) window), this term does not
make any inﬂuence to the result. The used lag window must also follow the
symmetry, for example we(m) = cos2(πm/N), when,
PWD
*n∆t
2 , 2πk
N∆t
+
→PWD(n,k)
PWD(n,k) =
N/2−1
∑
m=−N/2
we(m)x(n + m)x∗(n −m)e−j2πmk/N
=
N/2−1
∑
m=−N/2+1
we(m)x(n + m)x∗(n −m)e−j2πmk/N,
since we(−N/2) = 0. However, if the window is nonzero at the ending point
m = −N/2, this term will result in a kind of aliased distribution.
In order to introduce another way of the discrete Wigner distribution
calculation, with an even number of samples, consider again the continuous
form of the Wigner distribution of a signal with a limited duration. Assume
that the signal is sampled in such a way that the sampling theorem can be
applied and the equality sign used (Example 2.13). Then, the integral may

644
Time-Frequency Analysis
be replaced by a sum
WD(t,Ω) =
N
∑
m=−N
x(t + m ∆t
2 )x∗(t −m ∆t
2 )e−jmΩ∆t∆t
=
N/2
∑
m=−N/2
x(t + 2m ∆t
2 )x∗(t −2m ∆t
2 )e−j2mΩ∆t∆t
+
N/2−1
∑
m=−N/2
x(t + (2m + 1) ∆t
2 )x∗(t −(2m + 1) ∆t
2 )e−j(2m+1)Ω∆t∆t.
(9.117)
The initial sum is split into its even and odd terms part. Now, let us assume
that the signal is sampled in such a way that twice wider sampling interval
∆t is also sufﬁcient to obtain the Wigner distribution (by using every other
signal sample). Then, for the ﬁrst sum (with an odd number of samples)
holds,
N/2
∑
m=−N/2
x(t + m∆t)x∗(t −m∆t)e−j2mΩ∆t∆t = 1
2WD(t,Ω).
The factor 1/2 comes from the sampling interval. Now, from (9.117) follows
N/2−1
∑
m=−N/2
x(t + (2m + 1) ∆t
2 )x∗(t −(2m + 1) ∆t
2 )e−j(2m+1)Ω∆t∆t = 1
2WD(t,Ω).
(9.118)
This is just the discrete Wigner distribution with an even number of sam-
ples. If we denote
x(t + (2m + 1) ∆t
2 ) = x(t + m∆t + ∆t
2 ) = xe(t + m∆t)
x(n∆t + m∆t + ∆t
2 )
√
2∆t = xe(n + m)
then
x(t −m∆t −∆t
2 ) = x(t −m∆t + ∆t
2 −∆t)
x(n∆t −m∆t + ∆t
2 −∆t)
√
2∆t = xe(n −m −1).
The summation terms, for example for n = 0, are of the form
xe(m)
...
xe(−2)
xe(−1)
xe(0)
xe(1)
...
xe(−m −1)
...
xe(1)
xe(0)
xe(−1)
xe(−2)
... .

Ljubiša Stankovi´c
Digital Signal Processing
645
They would produce a modulated version of the pseudo Wigner distribu-
tion, due to the shift of a half of the sampling interval. However, this shift
can be corrected as (9.118)
WD(t,Ω) = e−jΩ∆t
N/2−1
∑
m=−N/2
xe(t + m∆t)x∗
e (t −m∆t −∆t)e−j2mΩ∆t(2∆t)
for any t and Ω(having in mind the sampling theorem). Thus, we may also
write
WD
*
n∆t, πk
N∆t
+
→WD(n,k)
WD(n,k) = e−jπk/N
N/2−1
∑
m=−N/2
xe(n + m)x∗
e (n −m −1)e−j2πmk/N.
(9.119)
In MATLAB notation, relation (9.6.4) can be implemented, as follows.
The signal values are
x+
n = [xe(n −N/2), xe(n −N/2 + 1),...,xe(n + N/2 −1)],
x−
n = [x∗
e (n + N/2 −1), x∗
e (n + N/2 −2),...,x∗
e (n −N/2)].
The vector of Wigner distribution values, for a given n and k, is
WD(n,k)=e−jπk/N
!
x+
n ∗
B
x−
n . ∗e−jπkm/NCT6
,
where e−jπkm/N is the vector with elements e−jπkm/N, for −N/2 ≤m ≤
N/2 −1, ∗is the matrix multiplication and .∗denotes the vector multiplica-
tion term by term.
Thus, in the case of an even number of samples, the discrete Wigner
distribution of a signal xe(n), calculated according to (9.6.4), corresponds to
the original signal x(t) related to xe(n) as
xe(n) ↔x(n∆t + ∆t/2)
√
2∆t.

646
Time-Frequency Analysis
To check this statement, consider the time marginal property of this distri-
bution. It is
1
N
N/2−1
∑
k=−N/2
WD(n,k)
=
N/2−1
∑
m=−N/2
(
xe(n + m)x∗
e (n −m −1) 1
N
N/2−1
∑
k=−N/2
e−j(2m+1)πk/N
)
=
N/2−1
∑
m=−N/2
(
xe(n + m)x∗
e (n −m −1) 1
N ej(2m+1)π/2 1 −e−j(2m+1)π
1 −e−j(2m+1)π/N
)
=
N/2−1
∑
m=−N/2
(xe(n + m)x∗
e (n −m −1)δ(2m + 1)) =
''''xe(n −1
2)
''''
2
= |x(n∆t)|2 (2∆t),
for |2m + 1| < N.
Since for any signal y(n) and its DFT holds
DFTN/2{y(n) + y(n + N/2)} = Y(2k),
where
Y(k) = DFTN{y(n)},
the pseudo Wigner distribution (9.6.4), without frequency ovesampling, in
the case of an even N, can be calculated as
WD
*
n∆t, 2πk
N∆t
+
→WD(n,k)
WD(n,k) = e−jπk/(N/2)
N/4−1
∑
m=−N/4
(R(n,m) + R(n,m + N/2))e−j2πmk/(N/2)
where
R(n,m) = xe(n + m)x∗
e (n −m −1).
Periodicity in m, for a given n, with period N is assumed in R(n,m), that
is, R(n,m + N) = R(n,m) = R(n,m −N). It is needed to calculate R(n,m +
N/2) for −N/4 ≤m ≤N/4 −1 using R(n,m) for −N/2 ≤m ≤N/2 −1
only.
In the case of real-valued signals, in order to avoid the need for
oversampling, as well as to eliminate cross-terms (that will be discussed
later) between positive and negative frequency components, their analytic
part is used in calculations.

Ljubiša Stankovi´c
Digital Signal Processing
647
9.6.5
From the STFT to the Wigner Distribution via S-Method
The pseudo Wigner distribution can be calculated as
PWD(t,Ω) = 1
π
∞
"
−∞
STFT(t,Ω+ θ)STFT∗(t,Ω−θ)dθ.
(9.120)
Where STFT is deﬁned as
STFT(t,Ω) =
∞
"
−∞
x(t + τ)w(τ)e−jΩτdτ.
(9.121)
This can be proven by substituting (9.121) into (9.120).
Relation (9.120) has led to the deﬁnition of a time-frequency distribu-
tion
SM(t,Ω) = 1
π
LP
"
−LP
P(θ)STFT(t,Ω+ θ)STFT∗(t,Ω−θ)dθ,
(9.122)
where P(θ) is a ﬁnite frequency domain window (we also assume rectangu-
lar form), P(θ) = 0 for |θ| > LP. Distribution obtained in this way is referred
to as the S-method. Two special cases are: the spectrogram P(θ) = πδ(θ) and
the pseudo Wigner distribution P(θ) = 1.
The S-method can produce a representation of a multi-component sig-
nal such that the distribution of each component is its Wigner distribution,
avoiding cross-terms, if the STFTs of the components do not overlap in time-
frequency plane.
Consider a signal
x(t) =
M
∑
m=1
xm(t)
where xm(t) are monocomponent signals. Assume that the STFT of each
component lies inside the region Dm(t,Ω), m = 1,2,..., M and assume that
regions Dm(t,Ω) do not overlap. Denote the length of the m-th region along
Ω, for a given t, by 2Bm(t), and its central frequency by Ω0m(t). Under this
assumptions the S-method of x(t) produces the sum of the pseudo Wigner
distributions of each signal component
SMx(t,Ω) =
M
∑
m=1
PWDxm(t,Ω),
(9.123)

648
Time-Frequency Analysis
if the width of the rectangular window P(θ), for a point (t,Ω), is deﬁned by
LP(t,Ω) =
!
Bm(t) −|Ω−Ω0m(t)| for (t,Ω) ∈Dm(t,Ω)
0 elsewhere.
To prove this consider a point (t,Ω) inside a region Dm(t,Ω). The integra-
tion interval in (9.122), for the m-th signal component is symmetrical with
respect to θ = 0. It is deﬁned by the smallest absolute value of θ for which
Ω+ θ or Ω−θ falls outside Dm(t,Ω), i.e.,
|Ω± θ −Ω0m(t)| ≥Bm(t).
For Ω> Ω0m(t) and positive θ, the integration limit is reached for θ =
Bm(t) −(Ω−Ω0m(t)). For Ω< Ω0m(t) and positive θ, the limit is reached
for θ = Bm(t) + (Ω−Ω0m(t)). Thus, having in mind the interval symmetry,
an integration limit which produces the same value of integral (9.122) as the
value of (9.120), over the region Dm(t,Ω), is given by LP(t,Ω). Therefore,
for (t,Ω) ∈Dm(t,Ω) we have SMx(t,Ω) = PWDxm(t,Ω). Since regions
Dm(t,Ω) do not overlap we have
SMx(t,Ω) =
M
∑
m=1
PWDxm(t,Ω).
Note that any window P(θ) with constant width
LP ≥max
(t,Ω){LP(t,Ω)}
produces SMx(t, f ) = ∑M
m=1 PWDxm(t,Ω), if the regions Dm(t,Ω) for m =
1,2,.., M, are at least 2LP apart along the frequency axis,
''Ω0p(t) −Ω0q(t)
'' >
Bp(t) + Bq(t) + 2LP, for each p, q and t. This is the S-method with constant
window width. The best choice of LP is the value when P(θ) is wide
enough to enable complete integration over the auto-terms, but narrower
than the distance between the auto-terms, in order to avoid the cross-terms.
If two components overlap for some time instants t, then the cross-term will
appear, but only between these two components and for that time instants.
A discrete form of the S-method (9.122) reads
SML(n,k) =
L
∑
i=−L
SN(n,k + i)S∗
N(n,k −i)
for P(i) = 1, −L ≤i ≤L (a weighted form P(i) = 1/(2L + 1) could be used).
A recursive relation for the S-method calculation is
SML(n,k) = SML−1(n,k) + 2Re[SN(n,k + L)S∗
N(n,k −L)],
(9.124)

Ljubiša Stankovi´c
Digital Signal Processing
649
The spectrogram is the initial distribution SM0(n,k) = |SN(n,k)|2 and
2Re[SN(n,k + i)S∗
N(n,k −i)], i = 1, 2,..., L are the correction terms. Changing
parameter L we can start from the spectrogram (L = 0) and gradually make
the transition toward the pseudo Wigner distribution by increasing L.
For the S-method realization we have to implement the STFT ﬁrst,
based either on the FFT routines or recursive approaches suitable for hard-
ware realizations. After we get the STFT we have to “correct” the ob-
tained values, according to (9.124), by adding few “correction” terms to the
spectrogram values. Note that S-method is one of the rare quadratic time-
frequency distributions allowing easy hardware realization, based on the
hardware realization of the STFT, presented in the ﬁrst part, and its “correc-
tion” according to (9.124). There is no need for analytic signal since the cross-
terms between negative and positive frequency components are removed in
the same way as are the other cross-terms. If we take that STFT(n,k) = 0
outside the basic period, i.e., when k < −N/2 or k > N/2 −1, then there
is no aliasing when the STFT is alias-free (in this way we can calculate the
alias-free Wigner distribution by taking L = N/2 in (9.124)). The calcula-
tion in (9.124) can be performed for the whole matrix of the S-method and
the STFT. This can signiﬁcantly save time in some matrix based calculation
tools.
There are two ways to implement summation in the S-method. The
ﬁrst one is with a constant L. Theoretically, in order to get the Wigner
distribution for each individual component, the number of correcting terms
L should be such that 2L is equal to the width of the widest auto-term. This
will guarantee cross-terms free distribution for all components which are at
least 2L frequency samples apart.
The second way to implement the S-method is with a time-frequency
dependent L = L(n,k). The summation, for each point (n,k), is performed
as long as the absolute values of SN(n,k + i) and S∗
N(n,k −i) for that (n,k)
are above an assumed reference level (established, for example, as a few
percents of the STFT maximal value). Here, we start with the spectrogram,
L = 0. Consider the correction term SN(n,k + i)S∗
N(n,k −i) with i = 1. If the
STFT values are above the reference level then it is included in summation.
The next term, with i = 2 is considered in the same way, and so on. The
summation is stopped when a STFT in a correcting term is below the
reference level. This procedure will guarantee cross-terms free distribution
for components that do not overlap in the STFT.
Example 9.38.
A signal consisting of three LFM components,
x(n) =
3
∑
i=1
Ai exp(jaiπn/32 + jbiπn2/1024),

650
Time-Frequency Analysis
with
(a1,a2,a3) = (−21,−1,20)
and
(b1,b2,b3) = (2,−0.75,−2.8),
is considered at the instant n = 0. The IFs of the signal components are ki = ai,
while the normalized squared amplitudes of the components are indicated by
dotted lines in Fig.9.50. An ideal time-frequency representation of this signal,
at n = 0, would be
I(0,k) = A2
1δ(k −k1) + A2
2δ(k −k2) + A2
3δ(k −k3).
The starting STFT, with the corresponding spectrogram, obtained by using
the cosine window of the width N = 64 is shown in Fig.9.50(a),(b). The ﬁrst
correction term is presented in Fig.9.50(c). The result of summing the spec-
trogram with the ﬁrst correction term is the S-method with L = 1, Fig.9.50(d).
The second correction term (Fig.9.50(e)) when added to SM1(0,k), produces
the S-method with L = 2, Fig.9.50(f). The S-methods for L = 3, 5, and 8, end-
ing with the Wigner distribution (L = 31) are presented in Fig.9.50(g)-(j). Just
a few correction terms are sufﬁcient in this case to achieve a high concen-
tration. The cross-terms start appearing at L = 8 and increase as L increases
toward the Wigner distribution. They make the Wigner distribution almost
useless, since they cover a great part of the frequency range, including some
signal components (Fig.9.50(j)). The optimal number of correction terms L is
the one that produces the best S-method concentration (sparsity), using the
ℓ1/2-norm of the spectrogram and the S-method (corresponding to the ℓ1-
norm of the STFT). In this case the best concentrated S-method is detected for
L = 5. The spectrogram is the initial distribution SM0(n,k) = |SN(n,k)|2 and
2Re[SN(n,k + i)S∗
N(n,k −i)], i = 1, 2,..., L are the correction terms. Consider-
ing the parameter L as a frame index, we can make a video of the transition
from the spectrogram to the Wigner distribution.
Example 9.39. The adaptive S-method realization will be illustrated on a ﬁve-
component signal x(t) deﬁned for 0 ≤t < 1 and sampled with ∆t = 1/256.
The Hamming window of the width Tw = 1/2 (128 samples) is used for STFT
calculation. The spectrogram is presented in Fig.9.51(a), while the S-method
with the constant Ld = 3 is shown in Fig.9.51(b). The concentration improve-
ment with respect to the case Ld = 0, Fig.9.51(a), is evident. Further increasing
of Ld would improve the concentration, but the cross-terms would also ap-
pear. Small changes are noticeable between the components with constant
instantaneous frequency and between quadratic and constant instantaneous
frequency component. An improved concentration, without cross-terms, can
be achieved by using the variable window width Ld. The regions Di(n,k),
determining the summation limit Ld(n,k) for each point (n,k), are obtained
by imposing the reference level corresponding to 0.14% of its maximal value

Ljubiša Stankovi´c
Digital Signal Processing
651
STFT
|SN(0,k)|
 (a)
+
-32
-16
0
16
31
k
|SN(0,k)|2=SM0(0,k)
 (b)
2Re[SN(0,k+1) SN
*(0,k-1)]
first correction term
 (c)
+
-32
-16
0
16
31
k
SM1(0,k)
 (b)+(c)=(d)
 (d)
second correction term
2Re[SN(0,k+2) SN
*(0,k-2)]
 (e)
-32
-16
0
16
31
k
SM2(0,k)
 (d)+(e)=(f)
 (f)
SM3(0,k)
 (g)
-32
-16
0
16
31
SM5(0,k)
k
 (h)
SM6(0,k)
 (i)
SM8(0,k)
 (j)
SM9(0,k)
 (k)
-32
-16
0
16
31
SM31(0,k)=WD(0,k)
k
 (l)
Figure 9.50
Analysis of a signal consisting of three LFM components (at the instant n = 0).
(a) The STFT with a cosine window of the width N = 64. (b) The spectrogram. (c) The ﬁrst
correction term. (d) The S-method (SM) with one correction term. (e) The second correction
term. (f) The S-method with two correction terms. (g) The S-method with three correction
terms. (h) The S-method with ﬁve correction terms. (i) The S-method with six correction terms.
(j) The S-method with eight correction terms.(k) The S-method with nine correction terms. (l)
The Wigner distribution (the S-method with L = 31 correction term).

652
Time-Frequency Analysis
0
200
400
600
800
-1
-0.5
0
0.5
1
 (d)
Ω
t
0
200
400
600
800
-1
-0.5
0
0.5
1
 (b)
Ω
t
0
200
400
600
800
-1
-0.5
0
0.5
1
 (a)
Ω
t
Ω
t
  (c)
0
200
400
600
800
-1
-0.5
0
0.5
1
Figure 9.51
Time-frequency analysis of a multi-component signal: a) Spectrogram, b) The S-
method with a constant window, with LP = 3, c) Regions of support for the S-method with
a variable window width calculation, corresponding to Q2 = 725, d) The S-method with the
variable window width calculated using regions in c).
at that time instant n. They are deﬁned as:
Di(n,k) =
!
1 when |STFTxi(n,k)|2 ≥Rn
0 elsewhere
and presented in Fig.9.51(c). White regions mean that the value of spectro-
gram is below 0.14% of its maximal value at that time instant n, meaning
that the concentration improvement is not performed at these points. The
signal dependent S-method is given in Fig.9.51(d). The method sensitivity,
with respect to the reference level is low.

Ljubiša Stankovi´c
Digital Signal Processing
653
9.7
GENERAL QUADRATIC TIME-FREQUENCY DISTRIBUTIONS
In order to provide additional insight into the ﬁeld of joint time-frequency
analysis, as well as to improve concentration of time-frequency represen-
tation, energy distributions of signals were introduced. We have already
mentioned the spectrogram which belongs to this class of representations
and is a straightforward extension of the STFT. Here, we will discuss other
distributions and their generalizations.
The basics condition for the deﬁnition of time-frequency energy dis-
tributions is that a two-dimensional function of time and frequency P(t,Ω)
represents the energy density of a signal in the time-frequency plane. Thus,
the signal energy associated with the small time and frequency intervals ∆t
and ∆Ω, respectively, would be
Signal energy within [Ω+ ∆Ω,t + ∆t] = P(t,Ω)∆Ω∆t.
However, point by point deﬁnition of time-frequency energy densities in
the time-frequency plane is not possible, since the uncertainty principle pre-
vents us from deﬁning concept of energy at a speciﬁc instant and frequency.
This is the reason why some more general conditions are being considered
to derive time-frequency distributions of a signal. Namely, one requires that
the integral of P(t,Ω) over Ω, for a particular instant of time should be equal
to the instantaneous power of the signal |x(t)|2, while the integral over time
for a particular frequency should be equal to the spectral energy density
|X(Ω)|2. These conditions are known as marginal conditions or marginal
properties of time-frequency distributions.
Therefore, it is desirable that an energetic time-frequency distribution
of a signal x(t) satisﬁes:
– Energy property
1
2π
∞
"
−∞
∞
"
−∞
P(t,Ω)dΩdt = Ex,
(9.125)
– Time marginal properties
1
2π
∞
"
−∞
P(t,Ω)dΩ= |x(t)|2, and
(9.126)

654
Time-Frequency Analysis
|x(t)|2  
|X(Ω)|2
t    
Ω 
t    
Ω 
P(t,Ω) 
Integration over Ω
Integration over t
Figure 9.52
Illustration of the marginal properties
– Frequency marginal property
∞
"
−∞
P(t,Ω)dt = |X(Ω)|2,
(9.127)
where Ex denotes the energy of x(t). It is obvious that if either one of
marginal properties (9.126), (9.127) is fulﬁlled, so is the energy property.
Note that relations (9.125), (9.126) and (9.127), do not reveal any informa-
tion about the local distribution of energy at a point (t,Ω). The marginal
properties are illustrated in Fig. 9.52.
Next we will introduce some distributions satisfying these properties.
Time and frequency marginal properties (9.126) and (9.127) may be
considered as the projections of the distribution P(t,Ω) along the time
and frequency axes, i.e., as the Radon transform of P(t,Ω) along these
two directions. It is known that the Fourier transform of the projection
of a two-dimensional function on a given line is equal to the value of
the two-dimensional Fourier transform of P(t,Ω), denoted by AF(θ,τ),
along the same direction (inverse Radon transform property). Therefore, if
P(t,Ω) satisﬁes marginal properties then any other function having two-
dimensional Fourier transform equals to AF(θ,τ) along the axes lines θ = 0
and τ = 0, and arbitrary values elsewhere, will satisfy marginal properties,
Fig. 9.53.
Assuming that the Wigner distribution is a basic distribution which
satisﬁes the marginal properties (any other distribution satisfying marginal
properties can be used as the basic one), then any other distribution with

Ljubiša Stankovi´c
Digital Signal Processing
655
|x(t)|2  
|X(Ω)|2
t    
Ω 
t    
Ω 
P(t,Ω) 
Integration over Ω
Integration over t
AF(τ,θ)
τ
θ 
FT [ |x(t)|2 ]
FT [ | X(Ω)|2 ]
2D FT
Figure 9.53
Marginal properties and their relation to the ambiguity function.
two-dimensional Fourier transform
AFg(θ,τ) = c(θ,τ)FT2D
t,Ω{WD(t,Ω)} = c(θ,τ)AF(θ,τ)
(9.128)
where c(0,τ) = 1 and c(θ,0) = 1, satisﬁes marginal properties as well.
The inverse two-dimensional Fourier transform of AFg(θ,τ) produces
the Cohen class of distributions, introduced from quantum mechanics into
the time-frequency analysis by Claasen and Mecklenbäuker, in the form
CD(t,Ω) = 1
2π
∞
"
−∞
∞
"
−∞
∞
"
−∞
c(θ,τ)x(u + τ/2)x∗(u −τ/2)ejθt−jΩτ−jθududτdθ
(9.129)
where c(θ,τ) is called the kernel in the ambiguity domain.
Alternatively, the frequency domain deﬁnition of the Cohen class of
distributions is
CD(t,Ω) =
1
(2π)2
∞
"
−∞
∞
"
−∞
∞
"
−∞
X(u −θ/2)X∗(u + θ/2)c(θ,τ)ejθt−jτΩ+jτududτdθ.
(9.130)
Various distributions can be obtained by altering the kernel function
c(θ,τ). For example, c(θ,τ) = 1 produces the Wigner distribution, while for
c(θ,τ) = ejθτ/2 the Rihaczek distribution follows.

656
Time-Frequency Analysis
The Cohen class of distributions, deﬁned in the ambiguity domain:
CD(t,Ω) = 1
2π
∞
"
−∞
∞
"
−∞
c(θ,τ)AF(θ,τ)ejθt−jΩτ dτ dθ
(9.131)
can be written in other domains, as well. The time-lag domain form is
obtained from (9.129), after integration on θ, as:
CD(t,Ω) =
∞
"
−∞
∞
"
−∞
cT(t −u,τ)x(u + τ/2)x∗(u −τ/2)e−jΩτ dτ du.
(9.132)
The frequency-Doppler frequency domain form follows from (9.130), after
integration on τ, as:
CD(t,Ω) =
1
(2π)2
∞
"
−∞
∞
"
−∞
CΩ(θ,Ω−u)X(u + θ/2)X∗(u −θ/2)ejθt dθ du.
(9.133)
Finally, the time-frequency domain form is obtained as a two-dimensional
convolution of the two-dimensional Fourier transforms, from (9.131), as:
CD(t,Ω) = 1
2π
∞
"
−∞
∞
"
−∞
Π(t −u,Ω−ξ)WD(u,ξ)dudξ.
(9.134)
Kernel functions in the respective time-lag, Doppler frequency-frequency
and time-frequency domains are related to the ambiguity domain kernel
c(θ,τ) as:
cT(t,τ) = 1
2π
∞
"
−∞
c(θ,τ)ejθtdθ
(9.135)
CΩ(θ,Ω) =
∞
"
−∞
c(θ,τ)e−jΩτdτ
(9.136)
Π(t,Ω) = 1
2π
∞
"
−∞
∞
"
−∞
c(θ,τ)ejθt−jΩτ dτ dθ.
(9.137)
According to (9.134) all distributions from the Cohen class may be consid-
ered as 2D ﬁltered versions of the Wigner distribution. Although any dis-
tribution could be taken as a basis for the Cohen class derivation, the form
with the Wigner distribution is used because it is the best concentrated dis-
tribution from the Cohen class with the signal independent kernels.

Ljubiša Stankovi´c
Digital Signal Processing
657
9.7.1
Reduced Interference Distributions
The analysis performed on ambiguity function and Cohen class of time-
frequency distributions leads to the conclusion that the cross-terms may
be suppressed or eliminated, if a kernel c(θ,τ) is a function of a two-
dimensional low-pass type. In order to preserve the marginal properties
c(θ,τ) values along the axis should be c(θ,0) = 1 and c(0,τ) = 1.
Choi and Williams exploited one of the possibilities deﬁning the dis-
tribution with the kernel of the form
c(θ,τ) = e−θ2τ2/σ2.
The parameter σ controls the slope of the kernel function which affects the
inﬂuence of cross-terms. Small σ causes the elimination of cross-terms but
it should not be too small because, for the ﬁnite width of the auto-terms
around θ and τ coordinates, the kernel will cause their distortion, as well.
Thus, there should be a trade-off in the selection of σ.
Here we will mention some other interesting kernel functions, produc-
ing corresponding distributions, Fig. 9.54.
Born-Jordan distribution
c(θ,τ) = sin( θτ
2 )
θτ
2
,
Zhao-Atlas-Marks distribution
c(θ,τ) = w(τ)|τ| sin( θτ
2 )
θτ
2
,
Sinc distribution
c(θ,τ) = rect(θτ
α ) =
!
1
for |θτ/α| < 1/2
0
otherwise
Butterworth distribution
c(θ,τ) =
1
1 + ( θτ
θcτc )2N ,
where w(τ) is a function corresponding to a lag window and α, N, θc and τc
are constants in the above kernel deﬁnitions.

658
Time-Frequency Analysis
-2
0
2
-100
-50
0
50
100
 (a)
θ
c(θ,τ)
τ
-2
0
2
-100
-50
0
50
100
 (b)
θ
c(θ,τ)
τ
-2
0
2
-100
-50
0
50
100
 (c)
θ
c(θ,τ)
τ
-2
0
2
-100
-50
0
50
100
 (d)
θ
c(θ,τ)
τ
Figure 9.54
Kernel functions for: Choi-Williams distribution, Born-Jordan distribution, Sinc
distribution and Zhao-Atlas-Marks distribution.
The spectrogram belongs to this class of distributions. Its kernel in
(θ,τ) domain is the ambiguity function of the window
c(θ,τ) =
∞
"
−∞
w
B
t −τ
2
C
w
B
t + τ
2
C
e−jθtdt = AFw(θ,τ).
Since the Cohen class is linear with respect to the kernel, it is easy to
conclude that a distribution from the Cohen class is positive if its kernel

Ljubiša Stankovi´c
Digital Signal Processing
659
can be written as
c(θ,τ) =
M
∑
i=1
aiAFwi(θ,τ),
where ai ≥0, i = 1,2,..., M.
There are several ways for calculation of the reduced interference dis-
tributions from the Cohen class. The ﬁrst method is based on the ambiguity
function (9.131):
1. Calculation of the ambiguity function,
2. Multiplication with the kernel,
3. Calculation of the inverse two-dimensional Fourier transform of this
product.
The reduced interference distribution may also be calculated by using
(9.132) or (9.134) with appropriate kernel transformations deﬁned by (9.135)
and (9.137). All these methods assume signal oversampling in order to avoid
aliasing effects. Figure 9.55 presents the ambiguity function along with ker-
nel (Choi-Williams). Figure 9.56(a) presents Choi-Williams distribution cal-
culated according to the presented procedure. In order to reduce high side
lobes of the rectangular window, the Choi-Williams distribution is also cal-
culated with the Hann(ing) window in the kernel deﬁnition c(θ,τ)w(τ) and
presented in Fig. 9.56(b). The pseudo Wigner distribution with Hann(ing)
window is shown in Fig. 9.48.
For the discrete-time signals. there are several ways to calculate a
reduced interference distributions from the Cohen class, based on (9.131),
(9.132), (9.133), or (9.134).
The kernel functions are usually deﬁned in the Doppler-lag domain
(θ,τ). Thus, here we should use (9.131) with the ambiguity function of a
discrete-time signal
AF(θ,m∆t) =
∞
∑
p=−∞
x
*
p∆t + m∆t
2
+
x∗
*
p∆t −m∆t
2
+
e−jpθ∆t∆t.
The signal should be sampled as in the Wigner distribution case. For a given
lag instant m, the ambiguity function can be calculated by using the stan-
dard DFT routines. Another way to calculate the ambiguity function is just
to take the inverse two-dimensional transform of the Wigner distribution.
Note that the corresponding transformation pairs are time ↔Doppler and
lag ↔frequency, that is, t ↔θ and τ ↔Ω. The relation between discretiza-
tion values in the Fourier transform pairs (considered interval, sampling

660
Time-Frequency Analysis
θ
τ
AF(θ,τ) and CW kernel
-3
-2
-1
0
1
2
3
-100
-50
0
50
100
Figure 9.55
Ambiguity function for signal from Fig.9.4 with the Choi-Williams kernel
interval in time ∆t, number of samples N, sampling interval in frequency
∆Ω= 2π/(N∆t)) is discussed in Chapter 1.
The generalized ambiguity function is obtained as
AFg(l∆θ,m∆t) = c(l∆θ,m∆t)AF(l∆θ,m∆t)
(9.138)
= c(l∆θ,m∆t)
∞
∑
p=−∞
x
*
p∆t + m∆t
2
+
x∗
*
p∆t −m∆t
2
+
e−jl∆θ p∆t∆t,
while a distribution, with kernel c(θ,τ) is the two-dimensional inverse
Fourier transform in the form
CD(n∆t,k∆Ω) = 1
2π
∞
∑
l=−∞
∞
∑
m=−∞
AFg(l∆θ,m∆t)e−jkm∆t∆Ωejnl∆θ∆t∆t∆θ.
In this notation we can calculate
CD(n,k) = IDFT2D
l,m
R
AFg(l,m)
S
where the values of AFg(l,m) are calculated according to (9.138).

Ljubiša Stankovi´c
Digital Signal Processing
661
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
 (b)
Ω
CWD(t,Ω)
t
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
 (a)
Ω
CWD(t,Ω)
t
Figure 9.56
Choi-Williams distribution: (a) direct calculation, (b) calculation with the kernel
multiplied by a Hann(ing) lag window.
In the time-lag domain, the discrete-time form reads
CD(n∆t,k∆Ω) =
∞
∑
p=−∞
∞
∑
m=−∞
cT(n∆t −p∆t,m∆t)
×x
*
p∆t + m∆t
2
+
x∗
*
p∆t −m∆t
2
+
e−jkm∆t∆Ω(∆t)2
(9.139)

662
Time-Frequency Analysis
with
cT(n∆t −p∆t,m∆t) = 1
2π
∞
∑
l=−∞
c(l∆θ,m∆t)ejnl∆θ∆te−jlp∆θ∆t∆θ.
For the discrete-time signals, it is common to write and use the Cohen
class of distributions in the form
CD(n,ω) =
∞
∑
p=−∞
∞
∑
m=−∞
cT(n −p,m)x(p + m)x∗(p −m)e−j2mω,
(9.140)
where
x(p + m)x∗(p −m) = x
*
(p + m)∆t
2
+
x∗
*
(p −m)∆t
2
+
∆t
cT(n −p,m) = cT
*
(n −p)∆t
2 ,m∆t
+ ∆t
2
CD(n,ω) →CD
*
n∆t
2 ,Ω∆t
+
.
Here we should mention that the presented kernel functions are of
inﬁnite duration along the coordinate axis in (θ,τ) thus, they should be
limited in calculations. Their transforms exist in a generalized sense only.
9.7.2
Kernel Decomposition Method
Distributions from the Cohen class can be calculated by using decomposi-
tion of the kernel function in the time-lag domain. Starting from
CD(t,Ω) =
∞
"
−∞
∞
"
−∞
cT(t −u,τ)x(u + τ/2)x∗(u −τ/2)e−jΩτdτdu
with substitutions u + τ/2 = t + v1 and u −τ/2 = t + v2 we get t −u =
−(v1 + v2)/2 and τ = v1 −v2, resulting in
CD(t,Ω) =
∞
"
−∞
∞
"
−∞
cT(−v1 + v2
2
,v1 −v2)x(t + v1)x∗(t + v2)e−jΩ(v1−v2)dv1dv2
The discrete-time version of the Cohen class of distribution can be written,
as
CD(n,ω) =∑
n1 ∑
n2
cT
*
−n1+n2
2
,n1−n2
+
[x(n+n1)e−jωn1][x(n+n2)e−jωn2]∗.

Ljubiša Stankovi´c
Digital Signal Processing
663
Assuming that C is a square matrix of ﬁnite dimension, with elements:
C(n1,n2) = cT
*
−n1 + n2
2
,n1 −n2
+
we can write
CD(n,ω) = xnCxH
n
where xn is a vector with elements x(n + n1)e−jωn1. We can now perform
the eigenvalue decomposition, ﬁnding solutions of det(C −λI) = 0 and
determining eigenvectors matrix Q that satisﬁes QQH = I and
C = QΛQH,
where Λ is a diagonal matrix containing the eigenvalues. It results in
CD(n,ω) = (xnQ)Λ(xnQ)H
Then, it is easy to conclude that the Cohen class of distribution can be
written as a sum of spectrograms:
CD(n,ω) = ∑
i
λi|STFTqi(n,ω)|2
where λi represents eigenvalues, while qi are corresponding eigenvectors
of C, i.e., columns of Q, used as windows in the STFT calculations.
Example 9.40. A four-component real-valued signal with M = 384 samples is
considered. Its STFT is calculated with a Hann(ing) window of the width
N = 128 with a step of 4 samples. The spectrogram (L = 0) is shown in
Fig.9.57(a). The alias-free Wigner distribution (L = N/2) is presented in Fig.
9.57(b). The Choi-Williams distribution of analytic signal is shown in Fig.
9.57(c). Its cross-terms are smoothed by the kernel, that also spreads the auto-
term of the LFM signal and chirps. The S-method with L = 10 is shown in
Fig. 9.57(d). For graphical presentation, the distributions are interpolated by
a factor of 2. In all cases the pure sinusoidal signal is well concentrated. In
the Wigner distribution and the SM the same concentration is achieved for
the LFM signal.

664
Time-Frequency Analysis
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
Ω
SPEC(t,Ω)
a)
t
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
Ω
WD(t,Ω)
b)
t
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
Ω
CWD(t,Ω)
c)
t
0
0.5
1
1.5
2
2.5
3
0
50
100
150
200
250
Ω
SM(t,Ω)
d)
t
Figure 9.57
Time-frequency representation of a four component signal: (a) the spectrogram,
(b) the Wigner distribution, (c) the Choi-Williams distribution, and (d) the S-method.

Chapter 10
Sparse Signal Processing
A discrete-time signal can be transformed into other domains using
different signal transformations. Some signals that cover the whole consid-
ered interval in one domain could be sparse in a transformation domain, i.e.,
could be located within a few nonzero coefﬁcients. Compressive sensing is
a ﬁeld dealing with a model for data acquisition including the problem of
sparse signal recovery from a reduced set of observations. A reduced set
of observations can be a result of a desire to sense a sparse signal with
the lowest possible number of measurements/observations (compressive
sensing). It can also be a result of a physical or measurement unavailability
to take a complete set of observations. Since the signal samples are linear
combinations of the signal transformation coefﬁcients they could be consid-
ered as the observations of a sparse signal in the transformation domain. In
applications it could also happen that some arbitrarily positioned samples
of a signal are so heavily corrupted by disturbances that it is better to omit
them and consider as unavailable in the analysis and to try to reconstruct
the signal with a reduced set of samples. Although the reduced set of ob-
servations/samples appears in the ﬁrst case as a result of user strategy to
compress the information, while in the next two cases the reduced set of
samples is not a result of user intention, all of them can be considered within
the uniﬁed framework. Under some conditions, a full reconstruction of a
sparse signal can be performed with a reduced set of observations/samples,
as in the case if a complete set of samples/observations were available. A
priori information about the nature of the analyzed signal, i.e., its sparsity
in a known transformation domain, must be used in this analysis. Sparsity
_________________________________________________
Authors: Ljubiša Stankovi´c, Miloš Dakovi´c, Srdjan Stankovi´c, Irena Orovi´c
665

666
Sparse Signal Processing
is the main requirement that should be satisﬁed in order to efﬁciently apply
the compressive sensing methods for sparse signal reconstruction.
The topic of this chapter is to analyze the signals that are sparse in
one of the common transformations domains. The DFT is used as a study
case. The compressive sensing results and algorithms are presented and
used only as a tool to solve engineering problems, involving sparse signals.
10.1
ILLUSTRATIVE EXAMPLES
Before we start the analysis we will describe few widely known examples
that can be interpreted and solved within the context of sparse signal
processing and compressive sensing.
Consider a large set of real numbers X(0), X(1),...,X(N −1). Assume
that only one of them is nonzero (or different from a common and known
expected value). We do not know either its position or its value. The aim is to
ﬁnd the position and the value of this number. This case can easily be related
to many real life examples when we have to ﬁnd one sample which differs
from other N −1 samples. The nonzero value (or the difference from the
expected value) will be denoted by X(i). A direct way to ﬁnd the position
of nonzero (different) sample would be to perform up to N measurements
and compare each of them with zero (the expected) value. However, if N
is very large and there is only one nonzero (different than expected) sample
we can get the result in just a few observations/measurements. A procedure
for the reduced number of observations/measurements is described next.
Take random numbers as weighting coefﬁcients ai, i = 0,1,2,..., N −1,
for each sample. Measure the total value of all N weighted samples, with
weights ai, from the set. Since only one is different from the common and
known expected value m (or from zero) we will get the total measured value
M = a1m + a2m + ... + ai(m + X(i)) + ... + aNm.
From this measured value M subtract the expected value MT = (a1 + a2 +
... + aN)m. The obtained value of this observation/measurement, denoted
by y(0), is
y(0) = M −MT =
N−1
∑
k=0
akX(k) = aiX(i),
since nonzero value in the space of X(0), X(1),...,X(N −1) is at one position
only, X(k) = X(i)δ(k −i).
As an illustration consider a set of N bags. Assume that only one bag
contains all false coins of a weight m + X(i). It is different from the known

Ljubiša Stankovi´c
Digital Signal Processing
667
1
2
3
N
a1
a2
a3
aN
+
One bag with false coins
M-M T= a1 m+a2 m+...+ai (m+X(i))+...+aN m
 -(a 1 +a2 +...+ai +...+aN )m=aiX(i)
 i=?,  X(i)=?
Two bags with false coins
M-M T= a1 m+...+ai (m+X(i))+...+ak (m+X(k))+...+aN m
 -(a 1 +a2 +...+ai +...+aN )m=aiX(i)+akX(k)
 i=?, k=?  X(i)=?, X(k)=?
Figure 10.1
There are N bags with coins. One of them, at an unknown position, contains false
coins. False coins differ from the true ones in mass for unknown X(i) = ∆m. The mass of the
true coins is m. Set of coins for measurement is formed using a1 coins from the ﬁrst bag, a2 coins
from the second bag, an so on. The total measured value is M = a1m + ... + ai(m + X(i)) + ... +
aNm. The difference of this value from the case if all coins were true is M −MT. Equation for
the case with one and two bags with false coins are presented (left and right).
weight m of true coins. The goal is to ﬁnd the position and the difference
in weight of false coins. From each of N bags we will take ai, i = 1,2,...N,
coins. Number of coins from the ith bag is denoted by ai. The total measured
weight of all coins from N bags is M, Fig.10.1.
After the expected value is subtracted the observation/measurement
y(0) is obtained
y(0) =
N−1
∑
k=0
X(k)ψk(0),
(10.1)
where the weighting coefﬁcients for this measurement are denoted by
ψk(0) = ak, k = 0,1,..., N −1. In the space of unknowns (variables) X(0),
X(1),...,X(N −1) this equation represents an N-dimensional hyperplane.
We know that only one unknown X(k) is nonzero at the unknown position

668
Sparse Signal Processing
k = i. Any cross-section of hyperplane (10.1) with any of coordinate axes
could be a solution of our problem. Assuming that a single X(k) is nonzero
a solution will exist for any k. Thus, one measurement would produce N
possible single nonzero values equal to
X(k) = y(0)/ψk(0),
ψk(0) ̸= 0, k = 0,1,2,..., N −1.
As expected, from one measurement we are not able to solve the problem
and to ﬁnd the position and the value of nonzero sample.
If we perform one more measurement y(1) with another set of weight-
ing coefﬁcients ψk(1), k = 0,1,..., N −1, and get measured value y(1) =
X(i)ψi(1) the result will be a hyperplane
y(1) =
N−1
∑
k=0
X(k)ψk(1).
This measurement will produce a new set of possible solutions for each X(k)
as
X(k) = y(1)/ψk(0), k = 0,1,2,..., N −1.
If these two hyperplanes (sets of solutions) produce only one common value
X(i) = y(0)/ψi(0) = y(1)/ψi(1).
then it is the solution of our problem.
As an example consider N = 5 sets of coins. The common weight of
true coins is 2. In the ﬁrst measurement we use ψi(0) = ai = i coins from
each set. The total weight of coins is M = 31. It is obtained by measuring (1+
2 + 3 + 4 + 5)2 + iX(i) = M, where X(i) is the unknown weight difference
of false coins. It means that iX(i) = 1, since all true coins would produce
(1 + 2 + 3 + 4 + 5)2 = 30. If the false coins were in the ﬁrst set the weight
difference would be X(1) = 1/1 = 1, if they were in the second set then
X(2) = 1/2, and so on, X(3) = 1/3, X(4) = 1/4, X(5) = 1/5. False coins can
be in any of ﬁve sets. Perform one more measurement with ψi(1) = ai = i2
coins from each set. Total measured weight is now M = 113. It is obtained as
M = 2(12 + 22 + 32 + 42 + 52) + i2X(i) = 113. Obviously i2X(i) = 3. Again
if the false coins were in the ﬁrst set then X(1) = 3/1, the second would
produce X(2) = 3/22 = 3/4, and so on, X(3) = 3/32 = 1/3, X(4) = 3/16,
X(5) = 3/25. The solution satisfying both equations is X(3) = 1/3. Thus,
false coins are in the third set. Their weight is 2 + 1/3 = 7/3. Note that we
would not be able to solve the problem with two measurements if we got
two values X(i) and X(k) for i ̸= k satisfying both equations.

Ljubiša Stankovi´c
Digital Signal Processing
669
In a matrix form these two measurements are
-
y(0)
y(1)
.
=
-
ψ0(0)
ψ1(0)
...
ψN−1(0)
ψ0(1)
ψ1(1)
...
ψN−1(1)
.
⎡
⎢⎢⎣
X(0)
X(1)
...
X(N −1)
⎤
⎥⎥⎦
y = AX
where A is the matrix of coefﬁcients (measurement matrix)
A =
-
ψ0(0)
ψ1(0)
...
ψN−1(0)
ψ0(1)
ψ1(1)
...
ψN−1(1)
.
and y are observations/measurements of sparse variable X.
Common value for two measurements X(i) = y(0)/ψi(0) and X(i) =
y(1)/ψi(1) is unique if
ψi(0)ψk(1) −ψi(1)ψk(0) ̸= 0
for any i ̸= k.
In order to prove this statement assume that two different solutions
X(i) and X(k), for the case of one nonzero coefﬁcient, satisfy the same
measurement hyperplane equations
ψi(0)X(i) = y(0), ψi(1)X(i) = y(1)
and
ψk(0)X(k) = y(0),
ψk(1)X(k) = y(1).
Then
ψi(0)X(i) = ψk(0)X(k)
and
ψi(1)X(i) = ψk(1)X(k).
If we divide these two equations we get
ψi(0)/ψi(1) = ψk(0)/ψk(1)
or ψi(0)ψk(1) −ψi(1)ψk(0) = 0. This is contrary to the assumption that
ψi(0)ψk(1) −ψi(1)ψk(0) ̸= 0.
The same conclusion can be made considering matrix form relations
for X(i) and X(k). If both of them may satisfy the same two measurements

670
Sparse Signal Processing
then
-
y(0)
y(1)
.
=
-
ψi(0)
ψk(0)
ψi(1)
ψk(1)
.-
X(i)
0
.
-
y(0)
y(1)
.
=
-
ψi(0)
ψk(0)
ψi(1)
ψk(1)
.-
0
X(k)
.
.
(10.2)
Subtraction of the previous matrix equations results in
-
ψi(0)
ψk(0)
ψi(1)
ψk(1)
.-
X(i)
−X(k)
.
= 0.
For ψi(0)ψk(1) −ψi(1)ψk(0) ̸= 0 follows X(i) = X(k) = 0. Therefore two
different nonzero solutions X(i) and X(k) in this case cannot exist. This
concludes the proof that the solution is unique if
ψi(0)ψk(1) −ψi(1)ψk(0) = det
-
ψi(0)
ψk(0)
ψi(1)
ψk(1)
.
̸= 0
for any i ̸= k. It also means that rank(A2) = 2 for any A2 being a 2 × 2 sub-
matrix of the matrix of coefﬁcients (measurement matrix) A. For additional
illustration of this simple problem see Section 10.5.2.
In numerical and practical applications we would not be satisﬁed,
if for example ψi(0)ψk(1) −ψi(1)ψk(0) ̸= 0 but ψi(0)ψk(1) −ψi(1)ψk(0) =
ε close to zero. In this case the theoretical condition for a unique solution
would be satisﬁed, however the analysis and possible inversion would be
highly sensitive to any kind of noise, including quantization noise. Thus,
a practical requirement is that the determinant is not just different from
zero, but that it sufﬁciently differs from zero so that an inversion stability
and robustness to a noise is achieved. Inversion stability for a matrix B is
commonly described by the condition number of matrix
cond{B} = λmax
λmin
where λmax and λmin are the largest and the smallest eigenvalue of matrix
B (when BHB = BBH)1. The inversion stability worsens as λmin approaches
to zero (when λmin is small as compared to λmax). For stable and robust
1
The value of determinant of matrix B is equal to the product of its eigenvalues, det {B} =
λ1λ2...λN, where N is the order of square matrix B. Note that the condition number can
be interpreted as a ratio of the norms-two (square roots of energies) of noise ε and signal x
after and before inversion y + yε = B−1(x+ε). This number is always greater or equal to 1.
The best value for this ratio is achieved when λmin is close to λmax.

Ljubiša Stankovi´c
Digital Signal Processing
671
calculations a requirement
λmax
λmin
≤1 + δ
is imposed, with a nonnegative constant δ being sufﬁciently small. In our
example this condition should hold for any submatrix A2 = B.
The previous experiment can be repeated assuming two nonzero val-
ues X(i) and X(k), Fig.10.1(right). In the case of two nonzero coefﬁcients,
two measurements
y(0) =
N−1
∑
l=0
X(l)ψl(0) = X(i)ψi(0) + X(k)ψk(0)
(10.3)
y(1) =
N−1
∑
l=0
X(l)ψl(1) = X(i)ψi(1) + X(k)ψk(1)
will result in X(i) and X(k) for any i and k. They are the solution of a system
with two equations and two unknowns. Therefore, with two measurements
we cannot get a result of the problem and ﬁnd the positions and the values
of nonzero coefﬁcients. If two more measurements are performed then an
additional system with two equations
y(2) = X(i)ψi(2) + X(k)ψk(2)
(10.4)
y(3) = X(i)ψi(3) + X(k)ψk(3)
is formed. Two systems of two equations (10.3) and (10.4) could be solved
for X(i) and X(k) for each combination of i and k. If these two systems
produce only one common solution pair X(i) and X(k) then this pair is the
solution of our problem. As in the case of one nonzero coefﬁcient, we may
show that the sufﬁcient condition for a unique solution is
det
⎡
⎢⎢⎣
ψk1(0)
ψk2(0)
ψk3(0)
ψk4(0)
ψk1(1)
ψk2(1)
ψk3(1)
ψk4(1)
ψk1(2)
ψk2(2)
ψk3(2)
ψk4(2)
ψk1(3)
ψk2(3)
ψk3(3)
ψk4(3)
⎤
⎥⎥⎦̸= 0
(10.5)
for any k1, k2, k3 and k4 or rank(A4) = 4 for any A4, where A4 is a
4 × 4 submatrix of the matrix of coefﬁcients A. In numeric realizations, the
condition is cond{A4} ≤1 + δ with sufﬁciently small δ for all A4. Suppose
that (10.5) holds and that two pairs of solutions of the problem X(k1),X(k2)

672
Sparse Signal Processing
and X(k3),X(k4) exist. Then
⎡
⎢⎢⎣
y(0)
y(1)
y(2)
y(3)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
ψk1(0)
ψk2(0)
ψk3(0)
ψk4(0)
ψk1(1)
ψk2(1)
ψk3(1)
ψk4(1)
ψk1(2)
ψk2(2)
ψk3(2)
ψk4(2)
ψk1(3)
ψk2(3)
ψk3(3)
ψk4(3)
⎤
⎥⎥⎦
⎡
⎢⎢⎣
X(k1)
X(k2)
0
0
⎤
⎥⎥⎦
and
⎡
⎢⎢⎣
y(0)
y(1)
y(2)
y(3)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
ψk1(0)
ψk2(0)
ψk3(0)
ψk4(0)
ψk1(1)
ψk2(1)
ψk3(1)
ψk4(1)
ψk1(2)
ψk2(2)
ψk3(2)
ψk4(2)
ψk1(3)
ψk2(3)
ψk3(3)
ψk4(3)
⎤
⎥⎥⎦
⎡
⎢⎢⎣
0
0
X(k3)
X(k4)
⎤
⎥⎥⎦.
By subtracting of these two systems we get
0 =
⎡
⎢⎢⎣
ψk1(0)
ψk2(0)
ψk3(0)
ψk4(0)
ψk1(1)
ψk2(1)
ψk3(1)
ψk4(1)
ψk1(2)
ψk2(2)
ψk3(2)
ψk4(2)
ψk1(3)
ψk2(3)
ψk3(3)
ψk4(3)
⎤
⎥⎥⎦
⎡
⎢⎢⎣
X(k1)
X(k2)
−X(k3)
−X(k4)
⎤
⎥⎥⎦.
Since (10.5) holds then X(k1) = X(k2) = X(k3) = X(k4) = 0, meaning that
the assumption about two independent pairs of solutions with two nonzero
coefﬁcients is not possible.
This approach to solve a problem (and to check the solution unique-
ness) is illustrative, however not computationally feasible. For example, for
a simple case with N = 1024 and just two nonzero coefﬁcients, in order to
ﬁnd a solution we have to solve two times systems of equations (10.3) and
(10.4) for each possible combination of i and k and to compare their solu-
tions. Total number of combinations of two indices out of the total number
of N indices is
*N
2
+
∼106.
In order to check the solution uniqueness we should calculate a determinant
value for all combinations of four indices k1, k2, k3 and k4 out the set of N
values. The number of determinants is (N
4) ∼1012. If one determinant of the
forth order is calculated in 10−5 [sec], then more than 5 days are needed
to calculate all determinants for this quite simple case of two nonzero
coefﬁcients.
As a next example consider a signal described by a weighted sum of
K harmonics from a set of possible oscillatory functions ej2πkn/N, k = 0, 1, 2,

Ljubiša Stankovi´c
Digital Signal Processing
673
..., N −1,
x(n) = A1ej2πk1n/N + A2ej2πk2n/N + ... + AKej2πkKn/N,
with K ≪N. In the DFT domain this signal will be sparse with X(k) =
DFT{x(n)} having only few nonzero values at k = ki, i = 1,2,...,K. Accord-
ing to the sampling theorem the sampling of this kind of signals should be
adjusted to the maximal expected signal frequency k = max{k1,k2,...,kK}.
For an arbitrary set of frequencies, it means that we should adjust sampling
in time to the maximal possible frequency k = N −1 and to use the full set of
N signal values/measurements at n = 0,1,2,..., N in order to avoid aliasing.
However, if we know that the signal consists of only K ≪N functions
with unknown amplitudes, then regardless of their frequencies, the signal
can be fully reconstructed from a reduced set of samples. Samples can be
considered as weighted measurements of the sparse function X(k),
y(0) = x(n1) =
N−1
∑
k=0
X(k)ψk(n1)
with the weighting coefﬁcients ψk(n1) = exp(j2πn1k/N)/N. The previous
relation is the IDFT. Now a similar analysis like in the previous illustrative
example can be performed, assuming for example K = 1 or K = 2. We can
ﬁnd position and value of nonzero X(k) using just a few of signal samples
y(i). This model corresponds to many signals in real life. For example, in
the Doppler-radar systems the speed of a radar target is transformed into
a frequency of a sinusoidal signal. Since the returned signal contains only
one or just a few of targets, the signal representing target velocity is a sparse
signal in the DFT domain. It can be reconstructed from fewer number of
samples than the total number of radar return signal samples N, Fig.10.2.
The signal model with complex-valued sinusoids is speciﬁc and very
important in engineering applications. We will focus most of our presen-
tation to this model. To illustrate complexity of the problem we will dis-
cus the simplest possible case consisting of one complex sinusoid at a fre-
quency k0. Within the previous framework it means that we consider a
case with only one nonzero DFT coefﬁcient at an unknown frequency in-
dex k0. Assume that two samples/observations x(n1) = Aexp(j2πk0n1/N)
and x(n2) = Aexp(j2πk0n2/N) of this signal are available. Note that signal
amplitude A is complex-valued and includes the initial phase. In order to
ﬁnd the unknown position (frequency index) form the ratio
x(n1)
x(n2) = exp(j2πk0(n1 −n2)/N).

674
Sparse Signal Processing
0
20
40
60
-2
-1
0
1
2  x(n)
Target velocities transformed
into a dense signal
(b)
0
20
40
60
-2
-1
0
1
2  y(n)
Mesurements of x(n)
(c)
0
20
40
60
0
20
40
60  Y(k)
DFT of y(n)
before reconstruction
(d)
0
20
40
60
0
20
40
60  X(k)
Two target velocities
within 64 bins range
(a)
Figure 10.2
(a) Signal in the frequency domain, where it is sparse (velocities of two targets
in Doppler radar signal). (b) Signal in the time domain, where it is dense. (c) Reduced set of
measurements (samples) and (d) its DFT before reconstruction, calculated using the available
samples only. Real parts of signals are presented.
From this relation follows
2πk0(n1 −n2)/N = arg{x(n1)} −arg{x(n2)} + 2kπ,
where k is an arbitrary integer. Then
k0 = arg{x(n1)} −arg{x(n2)}
2π(n1 −n2)
N +
k
n1 −n2
N.
(10.6)
Let us analyze the ambiguous term kN/(n1 −n2) role in the determination
of k0. For n1 −n2 = 1, this term is kN, meaning that any frequency k0
would be ambiguous with kN. Any value k0 + kN for k ̸= 0, in this case,
will be outside the basic period 0 ≤k ≤N −1. Thus, we may ﬁnd k0
in a unique way, within 0 ≤k0 ≤N −1. However, for |n1 −n2| = L > 1,
the terms kN/(n1 −n2) = kN/L produce shifts within the frequency basic
period. Then several possible solutions for the frequency k0 are obtained.
For example, for a signal with N = 16 and k0 = 5 if we use n1 = 1 and n2 = 5,
a possible solution of (10.6) is k0 = 5, but also
k0 = 5 + 16k/4,

Ljubiša Stankovi´c
Digital Signal Processing
675
or k0 = 9, k0 = 13, and k0 = 1 are possible solutions for frequency within
0 ≤k0 ≤15. Therefore, for |n1 −n2| = L > 1 more than two samples are
needed to resolve this ambiguity. An interesting case with nonuniform
(random) sampling positions ti, x(ti) = Aexp(jω0ti), will be discussed at
the end of this chapter.
The ﬁnal illustrative example is based on Schepp-Logan phantom in
computed tomography. This example was the one that attracted signiﬁcant
engineering community attention to the compressive sensing ﬁeld and ini-
tiated many other applications. Consider a simpliﬁed, one-dimensional ver-
sion of this model, along the line p(k) = P(200,k), Fig.10.3(white horizontal
line). Its derivative X(k) = p(k) −p(k −1) is sparse, Fig.10.3(left, middle).
From the computed tomography it is known that just some and nonun-
formly positioned values of the Fourier transforms of the image are avail-
able. In one dimension we may say that the available values correspond to
the samples of the Fourier transform of X(k), i.e. to x(ξ) = FT{X(k)}, at
some nonuniform positions of ξ. Thus, the problem here is to reconstruct
the sparse image p(k) (producing X(k)) from a reduced number of arbitrary
positioned samples of x(ξ). Note that in reality the DFT values are available
at the polar coordinate system, with their number and distance changing
with the projection angle. Also in two dimensions the difference opera-
tor p(k) −p(k −1) would be replaced by corresponding two-dimensional
difference relation (approximation of the gradient vector intensity, or total
variation, corresponding to one-dimension form
F
|p(k) −p(k −1)|2). In
traditional computed tomography reconstruction algorithms, based on the
Fourier transform inversion, the main idea is to reconstruct fast changing
x(ξ) = FT{X(k)} on a ﬁne rectangular grid, using nearest or linear or spline
interpolations. The sparsity of p(k) (producing X(k)) has not been taken into
account in these approaches.
In some applications there could be an indirect linear relation between
the sample/measurements and the sparsity domain. These cases can be
solved in a similar way as in the case of a direct relation, as it will be shown
at the end of this chapter.
In this chapter we will mainly use the DFT as the domain of signal
sparsity, since it plays the central role in engineering applications. Note
that in the compressive sensing theory random measurement matrices are
mainly used. The topic of this chapter is to analyze the signals that are
sparse in one of common transformations domains (DFT is used as a study
case). The compressive sensing results and algorithms are used as a tool to
solve this kind of problems, involving sparse signals.

676
Sparse Signal Processing
0
100
200
300
400
500
0
0.5
1
 p(k)=P(200,k)
0
100
200
300
400
500
-1
0
1
 X(k)=p(k)-p(k-1)
-2
0
2
-1
0
1  x(ξ)=FT[X(k)]
Figure 10.3
Shepp-Logan model for the computed tomography reconstruction (left), along
with its slice along indicated line (right-top), its derivative (right-middle) and its Fourier
transform.
10.2
SPARSITY AND REDUCED SET OF SAMPLES/OBSERVATIONS
Consider a signal x(n) and its transformation domain coefﬁcients X(k),
x(n) =
N−1
∑
k=0
X(k)ψk(n)
or
x= ΨX,
where Ψ is the transformation matrix with elements ψk(n), x is the signal
vector column, and X is the transformation coefﬁcients vector column. A
signal is sparse in the transformation domain if the number of nonzero
transform coefﬁcients K is much lower than the number of the original
signal samples N, i.e., if
X(k) = 0

Ljubiša Stankovi´c
Digital Signal Processing
677
for
k /∈{k1,k2,...,kK} = K,
The number of nonzero samples is
∥X∥0 = card{X} = K,
where
∥X∥0 =
N−1
∑
k=0
|X(k)|0
and card{X} is the notation for the number of nonzero transformation
coefﬁcients in X. Counting the nonzero coefﬁcients in a signal representation
can be achieved by using the so called ℓ0-norm denoted by ∥X∥0. This form
is referred to as the ℓ0-norm (norm-zero) although it does not satisfy norm
properties. By deﬁnition |X(k)|0 = 0 for |X(k)| = 0 and |X(k)|0 = 1
for
|X(k)| ̸= 0.
A signal x(n), whose transformation coefﬁcients are X(k), is sparse in
this transformation domain if
card{X} = K ≪N.
For linear signal transforms the signal can be written as a linear combination
of the sparse domain coefﬁcients X(k)
x(n) =
∑
k∈{k1,k2,...,kK}
X(k)ψk(n).
(10.7)
A signal sample can be considered as a measurement/observation of linear
combination of values X(k). Topic of this chapter is to show that a signal of
sparsity K in a transformation domain can be reconstructed from a reduced
set of M samples/observations. In the reconstruction, two approaches are
possible:
-In the ﬁrst approach the reconstruction process is done by reconstruct-
ing sparse transform coefﬁcients X(k). By reconstructing all sparse coefﬁ-
cients X(k) we would be able to reconstruct all signal values x(n) and have
a complete signal x(n) for all 0 ≤n ≤N −1.
-We may also reconstruct x(n) samples/measurements up to the com-
plete set of data, using the available samples/measurements and the spar-
sity of coefﬁcients X(k). Then all N coefﬁcients X(k) can easily be calculated
from the full set of signal samples/measurements. This approach, involving
complete set of samples, can be used with common signal transformation
matrices when a well deﬁned complete set of samples exists.

678
Sparse Signal Processing
0
20
40
60
80
100
120
-2
-1
0
1
2  x(n)
0
20
40
60
80
100
120
-2
-1
0
1
2  y(n)
Figure 10.4
Signal x(n) and available samples y(n).
Assume that samples of x(n) are available at some random positions
ni ∈M ={n1,n2,...,nM}⊂N = {0,1,2,3,..., N −1}.
Here N = {0,1,2,3,..., N −1} is the set of all samples of a signal x(n)
and M ={n1,n2,...,nM} is its random subset with M elements, M ≤N.
Case when the samples/measurements are taken at random instants ti of a
continuous signal x(t), will be considered later as well. The available signal
values are denoted by vector y, Fig.10.4,
y = [x(n1), x(n2), ...,x(nM)]T.
The available samples (measurements of a linear combination of X(k))
deﬁned by (10.7), for ni ∈M ={n1,n2,...,nM}, can be written as a system of

Ljubiša Stankovi´c
Digital Signal Processing
679
M equations
⎡
⎢⎢⎣
x(n1)
x(n2)
...
x(nM)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
ψ0(n1)
ψ1(n1)
ψN−1(n1)
ψ0(n2)
ψ1(n2)
ψN−1(n2)
...
...
...
ψ0(nM)
ψ1(nM)
ψN−1(nM)
⎤
⎥⎥⎦
⎡
⎢⎢⎣
X(0)
X(0)
...
X(N −1)
⎤
⎥⎥⎦
or
y = AX
where A is the M × N matrix of measurements/observations/available
signal samples.
The fact that the signal is sparse with X(k) = 0 for k /∈{k1,k2,...,kK} =
K is not included in the measurement matrix A since the positions of
the nonzero values are unknown. If the knowledge that X(k) = 0 for k /∈
{k1,k2,...,kK} = K were included then a reduced observation matrix would
be obtained as
⎡
⎢⎢⎣
x(n1)
x(n2)
...
x(nM)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
ψk1(n1)
ψk2(n1)
ψkK(n1)
ψk1(n2)
ψk2(n2)
ψkK(n2)
...
...
...
ψk1(nM)
ψk2(nM)
ψkK(nM)
⎤
⎥⎥⎦
⎡
⎢⎢⎣
X(k1)
X(k2)
...
X(kK)
⎤
⎥⎥⎦
or
y = AKXK.
Matrix AK would be formed if we knew the positions of nonzero samples
k ∈{k1,k2,...,kK} = K. It would follow from the measurement matrix A by
omitting the columns corresponding to the zero-valued coefﬁcients X(k).
Assuming that there are K nonzero coefﬁcients X(k), out of the total
number of N values, the total number of possible different matrices AK is
equal to the number of combinations with K out of N. It is equal to (N
K).
In the common signal transform cases (like the DFT) the set of miss-
ing/unavailable samples can be deﬁned as well
yc= [x(nM+1), x(nM+2), ...,x(nN)]T.
The union of sets y and yc is a set containing all signal samples (complete
set of samples/measurements). If x is the complete set of samples then
x = y ∪yc.
The signal of interest x(n) can also be measured in an indirect way.
It is common to assume that each indirect measurement f (n) is a linear

680
Sparse Signal Processing
combination of signal samples
f (i) = bi1x(0) + bi2x(1) + ... + biNx(N −1),
(10.8)
where i = 0,1,..., M −1. In matrix form the relation between signal samples
x(n) and M indirect measurements is given by
f = BMx.
Since the signal is related to its sparsity domain by x = ΨX, the measure-
ments are related to the sparsity domain form of signal as
f = BMΨX = AX,
where
A = BMΨ.
An example of indirect measurements is a linear signal transform
f (i) =
N−1
∑
m=0
h(i −m)x(m)
with bim = h(i −m −1). In this case samples of the output signal (a trans-
form of original signal) are the measurements, while the sparsity domain
is the transformation domain of the input (original) signal. All linear signal
transforms can be considered within this framework.
10.3
TRANSFORMATION MATRIX PARAMETERS
Consider a discrete-time signal x(n) of duration N, for 0 ≤n ≤N −1
within a transformation domain deﬁned by the basis functions set ψk(n),
k = 0,1,..., N
x(n) =
N−1
∑
k=0
X(k)ψk(n)
with
X(k) =
N−1
∑
n=0
x(n)ϕk(n).
In a matrix form
x= ΨX
and
X = Φx.

Ljubiša Stankovi´c
Digital Signal Processing
681
For the DFT matrix the elements of matrices Ψ and Φ are
ψk(n) = 1
N W−k
N = 1
N ej2πk/N
ϕk(n) = Wk
N = e−j2πk/N,
i.e., Φ = WN and Ψ = W−1
N = 1
N WH
N. The elements of matrix WN are Wk
N
and WH
N is the Hermitian transpose of WN.
Other signal transformation matrices can be considered in the same
way as the DFT. In the case of common signal transforms the measurement
matrix A is formed from the complete N × N transformation matrix Ψ by
omitting the rows corresponding to the unavailable samples. When matrix
Ψ is the inverse DFT matrix then the measurement matrix A is a partial IDFT
matrix.
The normalized form of the partial DFT matrix would have functions
ψk(n) =
1
√
M
W−k
N =
1
√
M
ej2πk/N
so that its energy over M measurements (energy of a column of measure-
ments matrix A) is
⟨ψk,ψ∗
k⟩=
M
∑
i=1
|ψk(ni)|2 = 1.
For the common DFT matrix ψk(n) = W−k
N /N with
⟨ψk,ψ∗
k⟩= N.
In the introductory example we have mentioned a multiplication of
measurements with random numbers. This kind of matrices play central
role in compressive sensing theory. For example, zero-mean independent
random Gaussian variables
ψk(n) = N (0, 1
N )
can be used as basis functions. Variance of the Gaussian random variable
is σ2
ε = 1/N so that a sum of N variables (corresponding to the number of
measurements) has unity variance. This kind of basis functions is orthonor-
mal in mean since
E{ψ2
k(n)} = 1
and
E{ψi(n)ψk(n)} = 0, for i ̸= k.

682
Sparse Signal Processing
Bernoulli random matrix, whose elements take the value 1/
√
N and
−1/
√
N, is also used in compressive sensing. An interesting class of mea-
surement matrices is called structured random matrices. One type of such
matrices is obtained by random sampling of functions that have a sparse
expansion in terms of an orthonormal system. The partial DFT matrix is
one of such examples. The randomness is a result of the random sampling
positions. Another more complex example of such sampling and structured
random matrix will be presented on the case of nonuniform sampling of
signal x(t) and the DFT transform as its sparsity domain, by the end of this
chapter.
10.3.1
Unitary Matrix
A linear transformation matrix Ψ is unitary matrix if it satisﬁes the property
⟨ΨX,ΨY⟩=EΨ ⟨X,Y⟩
where ⟨X,Y⟩denotes scalar product of two N-dimensional vectors
⟨X,Y⟩= X(0)Y∗(0) + X(1)Y∗(1) + ... + X(N −1)Y∗(N −1)
and EΨ is the energy of ψ
EΨ = ⟨ψk,ψ∗
k ⟩= |ψk(0)|2 + |ψk(1)|2 + ... + |ψk(N −1)|2 .
It is assumed that EΨ is the same for any k. For normal basis functions,
EΨ = 1 by deﬁnition, and
⟨ΨX,ΨY⟩=⟨X,Y⟩.
In the DFT case the basis would be normal if we used
ψk(n) =
1
√
N
W−k
N =
1
√
N
ej2πk/N
ϕk(n) =
1
√
N
Wk
N =
1
√
N
e−j2πk/N.
For the notation common in signal processing
ψk(n) = 1
N W−k
N = 1
N ej2πk/N.
Then EΨ = 1/N in the DFT case. Note that the unitary property in the DFT
is just Parseval’s theorem, since ΨX = x and ΨY = y. With EΨ = 1/N the

Ljubiša Stankovi´c
Digital Signal Processing
683
relation
⟨ΨX,ΨY⟩=EΨ ⟨X,Y⟩
results in
N−1
∑
n=0
x(n)y∗(n) = 1
N
N−1
∑
k=0
X(k)Y∗(k).
10.3.2
Isometry and Restricted Isometry Property
A transformation matrix Ψ satisﬁes the isometry property if it preserves the
vector intensity in the N-dimensional space, i.e., if
∥ΨX∥2
2 = EΨ ∥X∥2
2
(10.9)
where ∥X∥2
2 is deﬁned as
∥X∥2
2 = |X(0)|2 + |X(1)|2 + ... + |X(N −1)|2 .
For a transformation matrix Ψ which satisﬁes the isometry property the
following relation holds
1
EΨ ∥ΨX∥2
2 −∥X∥2
2
∥X∥2
2
= 0.
A transformation matrix Ψ satisﬁes the restricted isometry property
(RIP) with a constant δ if
'''''
1
EΨ ∥ΨX∥2
2 −∥X∥2
2
∥X∥2
2
''''' ≤δ.
(10.10)
Here the isometry condition that
1
EΨ ∥ΨX∥2
2 −∥X∥2
2 = 0 is relaxed to the
condition that the relative absolute value of
1
EΨ ∥ΨX∥2
2 −∥X∥2
2 is sufﬁciently
small as compared to the signal energy, i.e., that it is within 0 ≤δ < 1 range.
The restricted isometry constant is a measure of how much the transform
matrix Ψ differ from an isometry transform (10.9).
If δ = 1 the isometry does not hold since for ∥X∥2
2 ̸= 0 we have
1
EΨ ∥ΨX∥2
2 = 0. It means that some nonzero coordinates of X are projected
to zero-values in
1
EΨ ∥ΨX∥2
2.

684
Sparse Signal Processing
For an M × N matrix A the restricted isometry property is satisﬁed
with δ if
'''''
1
EA ∥AX∥2
2 −∥X∥2
2
∥X∥2
2
''''' ≤δ.
(10.11)
For a K−sparse signal the restricted isometry is satisﬁed with δK if
'''''
1
EA ∥AKXK∥2
2 −∥XK∥2
2
∥XK∥2
2
''''' ≤δK
for all possible (earlier described) submatrices AK of matrix A. The re-
stricted isometry property must hold for any sparsity lower than K as well.
The restricted isometry property can be written as
−∥XK∥2
2 δK ≤1
EA
∥AKXK∥2
2 −∥XK∥2
2 ≤∥XK∥2
2 δK
or
1 −δK ≤
1
EA ∥AKXK∥2
2
∥XK∥2
2
≤1 + δK.
for 0 ≤δK < 1. For δK = 0 the isometry property holds for AK.
10.3.3
Coherence
The coherence index of a matrix A is deﬁned as a maximal absolute value
of normalized scalar product of its two columns
µ = max|µ(m,k)|, for m ̸= k
where
µ(m,k) =
1
∑M
i=1 |ψk(ni)|2
M
∑
i=1
ψm(ni)ψ∗
k(ni) = 1
EA
⟨ψm,ψ∗
k ⟩
(10.12)
and ψk is the kth column of matrix A with EA =
Y
ψk,ψ∗
k
Z
. This index plays
an important role in the analysis of measurement matrices.
The coherence index cannot be arbitrary small for an M × N matrix A
(M < N). The Welch upper bound relation holds
µ ≥
<
N −M
M(N −1).
(10.13)

Ljubiša Stankovi´c
Digital Signal Processing
685
The Welch limit for matrix A, whose columns have energy EA, will be
proven next.
Denote the elements of matrix
1
EA AHA by b(m,k). By deﬁnition, the
trace of this matrix is a sum of its diagonal elements,
Trace{ 1
EA
AHA} =
N
∑
m=1
b(m,m) = N.
Trace and energy are related to the eigenvalues λi of
1
EA AHA as
Trace{ 1
EA
AHA} =
M
∑
i=1
λi
WWWW
1
EA
AHA
WWWW
2
2
=
N
∑
m=1
N
∑
k=1
|b(m,k)|2 =
M
∑
i=1
λ2
i .
We may write
N2 =
B
Trace{AHA}
C2
=
(
M
∑
i=1
λi
)2
≤M
M
∑
i=1
λ2
i = M
WWWW
1
EA
AHA
WWWW
2
2
.
(10.14)
Schwartz’s inequality
(λ1 + λ2 + ... + λM)2
λ2
1 + λ2
2 + ... + λ2
M
≤M.
(10.15)
is used. Since the elements b(m,k) are equal to the scalar products (10.12) of
columns ψ∗
m(ni) and ψk(ni) then
WWWW
1
EA
AHA
WWWW
2
2
=
M
∑
m=1
M
∑
k=1
|b(m,k)|2 =
M
∑
m=1
M
∑
k=1
''''
1
EA
⟨ψ∗
m,ψk⟩
''''
2
=
M
∑
m=1
M
∑
k=1
|µ(m,k)|2 ≤(N + N(M −1)µ2).
(10.16)
with µ(m,m) = 1 and |µ(m,k)| ≤µ for m ̸= k. Relation (10.13) follows, from
N2 ≤M(N + N(N −1)µ2).

686
Sparse Signal Processing
The equality holds for matrices that form an equiangular tight frame.
From the presented proof for the Welch bound we can see that two inequal-
ities in (10.14) and (10.16) become equalities if
λ1 = λ2 = ... = λM
|⟨ψ∗
m,ψk⟩| = µ for any m,k.
A matrix satisfying these properties is an equiangular tight frame. The par-
tial DFT matrix with a large number of columns for some speciﬁc combi-
nations of rows can be treated as a rough approximation of an equiangular
tight frame.
As it will be seen later by using a measurement matrix with a smallest
possible µ we will be able to reconstruct the signal of a largest possible
sparsity K, with a given number of measurements M. It is interesting to note
that a number of optimization procedures, in order to ﬁnd the best possible
measurement matrix, uses the minimization of this parameter.
Example 10.1. Since Schwartz’s inequality for discrete-time signals
(
M
∑
n=1
x(n)y(n)
)2
≤
M
∑
n=1
x2(n)
M
∑
n=1
y2(n).
will be used few more times (in various forms), within this chapter, we will
present its proof here. Note that with y(n) = 1 and x(n) = λn it produces
(10.15). Previous inequality easily follows from
0 ≤
M
∑
n=1
M
∑
m=1
(x(n)y(m) −x(m)y(n))2
=
M
∑
n=1
M
∑
m=1
x2(n)y2(m) −2
M
∑
n=1
M
∑
m=1
x(n)y(n)x(m)y(m) +
M
∑
n=1
M
∑
m=1
x2(m)y2(n).
Since the ﬁrst and last sums are equal, Schwartz’s inequality follows from
2
M
∑
n=1
x2(n)
M
∑
m=1
y2(m) −2
(
M
∑
n=1
x(n)y(n)
)2
≥0.
With y(n) = 1 and x(n) = |x(n)| Schwartz’s inequality can also be written as
(
M
∑
n=1
|x(n)|
)2
≤M
M
∑
n=1
|x(n)|2
(10.17)
or
∥x∥1 ≤
√
M∥x∥2
or
∥x∥2 ≥
1
√
M ∥x∥1

Ljubiša Stankovi´c
Digital Signal Processing
687
with
∥x∥1 =
M
∑
n=1
|x(n)| and ∥x∥2 =
[
\
\
]
M
∑
n=1
|x(n)|2.
Equality in this relation holds when |x(n)| = Cy(n) = C, i.e., for |x(1)| =
|x(2)| = ... = |x(M)|.
For a K sparse vector X holds
(
K
∑
i=1
|X(ki)|
)2
≤K
K
∑
k=1
|X(ki)|2
∥X∥2 ≥
1
√
K ∥X∥1 .
10.3.4
Restricted Isometry and Coherence
For a measurement matrix A we may write
∥AX∥2
2 = ∥y∥2
2 = |y(0)|2 + |y(1)|2 + ... + |y(M −1)|2
= |x(n1)|2 + |x(n2)|2 + ... + |x(nM)|2
=
'''''
N−1
∑
k=0
ψn1(k)X(k)
'''''
2
+
'''''
N−1
∑
k=0
ψn2(k)X(k)
'''''
2
+ ... +
'''''
N−1
∑
k=0
ψnM(k)X(k)
'''''
2
=
(
M
∑
i=1
|ψni(0)|2
)
|X(0)|2 + ... +
(
M
∑
i=1
|ψni(N −1)|2
)
|X(N −1)|2
+
N−1
∑
k1=0
N−1
∑
k2=k1+1
2Re
%
X(k1)X∗(k2)
M
∑
i=1
ψni(k1)ψ∗
ni(k2)
;
.
(10.18)
Using EA = ∑M
i=1 |ψni(k)|2 and µ(k1,k2) =
1
EA ∑M
i=1 ψni(k1)ψ∗
ni(k2) we
get
∥AX∥2
2 = EA
N−1
∑
k=0
|X(k)|2 +
N−1
∑
k1=0
N−1
∑
k2=k1+1
2Re{X(k1)X∗(k2)µ(k1,k2)EΨ}.
(10.19)
Since the restricted isometry property reads
''''
1
EA
∥AX∥2
2 −∥X∥2
2
'''' ≤δK ∥X∥2
2 ,
(10.20)

688
Sparse Signal Processing
the value of restricted isometry constant δK, for an arbitrary signal X(k), is
δK = max
'''''
∑N−1
k1=0 ∑N−1
k2=k1+1 2Re{X(k1)X∗(k2)µ(k1,k2)}
∑N−1
k=0 |X(k)|2
'''''.
(10.21)
Value on the right side of inequality is highly signal dependent. We will ﬁnd
an estimate of its bound. Since
|Re{X(k1)X∗(k2)µ(k1,k2)}| ≤|X(k1)X∗(k2)||µ(k1,k2)|
≤µ|X(k1)X∗(k2)|
we can write
δK ≤µmax
%
∑N−1
k1=0 ∑N−1
i=k2+1 2|X(k1)X∗(k2)|
∑N−1
k=0 |X(k)|2
;
.
For example, for sparsity K = 2 only two speciﬁc values of X(k) are
nonzero. Assume that their positions are k = k1 and k = k2
∥A2X∥2
2 = EA
N−1
∑
k=0
|X(k)|2 + 2Re{X(k1)X∗(k2)µ(k1,k2)EΨ}.
Then we get
δ2 ≤2µmax
'''''
X(k1)X∗(k2)
|X(k1)|2 + |X(k2)|2
''''' = µ
(10.22)
since
|X(k1)|2 + |X(k2)|2
|X(k1)X∗(k2)|
≥2.
(10.23)
The maximal value in Schwartz’s inequality (10.22) is achieved for |X(k1)| =
|X(k2)| and µ = max|µ(k1,k2)|. Inequality (10.23) easily reduces to the well
known inequality
(a + 1
a) ≥2
for a > 0 and (a + 1
a) = 2 for a = 1. Since the limit value may be achieved for
a speciﬁc signal, if our aim is that (10.20) holds for any signal, we may write
δ2 = µ.

Ljubiša Stankovi´c
Digital Signal Processing
689
For K = 3 inequality (10.20) and (10.21) assume the form
'''''
1
EA ∥A3X∥2
2 −∥X∥2
2
∥X∥2
2
''''' ≤2µ|X(k1)X∗(k2)| + |X(k1)X∗(k3)| + |X(k2)X∗(k3)|
|X(k1)|2 + |X(k2)|2 + |X(k3)|2
=
(
(|X(k1)| + |X(k2)| + |X(k3)|)2
|X(k1)|2 + |X(k2)|2 + |X(k3)|2 −1
)
µ ≤(3 −1)µ = 2µ.
where Schwartz’s inequality (10.17) is used with M = 3 and x(n) = |X(kn)|.
Therefore
δ3 ≤(K −1)δ2 = 2µ.
The maximal value of the signal dependent term is achieved if |X(k1)| =
|X(k2)| = |X(k3)|.
The results can easily be generalized to any K ≥2 using
2
K
∑
i=1
K
∑
j=i+1
''X(ki)X∗(kj)
''
K
∑
i=1
|X(ki)|2
=
* K
∑
i=1
|X(ki)|
+2
K
∑
i=1
|X(ki)|2
−1 ≤(K −1).
and Schwartz’s inequality (10.17) with x(n) = |X(kn)|.
The restricted isometry constant inequality is
δK ≤(K −1)δ2 = (K −1)µ.
Equality holds for |X(k1)| = |X(k2)| = ... = |X(kK)|.
Since δK ≤(K −1)δ2 = (K −1)µ then the matrix satisﬁes the restricted
isometry relation with
''''
1
EA
∥AX∥2
2 −∥X∥2
2
'''' ≤(K −1)µ∥X∥2
2 .
In general, it does not mean that that there is no lower values of bound
δK such that the restricted isometry inequality is satisﬁed. This is just an
estimate of the upper bound value of the constant δK. Equality could be
checked by examining the imposed inequality conditions.

690
Sparse Signal Processing
For the DFT matrix with |ψni(k)| =
'''ej2πk/N/N
''' = 1/N and EA =
M/N2 from (10.19) we get
∥AX∥2
2 = M
N2
N−1
∑
k=0
|X(k)|2
(10.24)
+ 1
N2
N−1
∑
k1=0
N−1
∑
k2=k1+1
2Re
%
X(k1)X∗(k2)
M
∑
i=1
ej2πni(k1−k2)/N
;
.
Introducing notation
α =
1
M∥X∥2
2
N−1
∑
k1=0
N−1
∑
k2=k1+1
2Re
%
X(k1)X∗(k2)
M
∑
i=1
ej2πni(k1−k2)/N
;
(10.25)
we can write
N2
M ∥AX∥2
2 = ∥X∥2
2 + α∥X∥2
2 .
(10.26)
For M = N it is easy to check that the isometry property
N ∥AX∥2
2 = ∥X∥2
2
holds since ∑M
i=1 ej2πni(k1−k2)/N = 0 for M = N and ni ∈M = N = {0,1,..., N −
1}.
The restricted isometry property is satisﬁed with δ = max|α|.
Example 10.2. The value of α, for signals with sparsity K ≥2 in the DFT domain,
can be related to the average power of available samples/measurements y(n)
deﬁned by Py = ∑M
i=1 |x(ni)|2 /M = ∥AX∥2
2 /M and the average power of
signal Px = ∑N−1
n=1 |x(n)|2 /N = ∥X∥2
2 /N2. The constant α from (10.26) can
be written in the form
α =
N2
M ∥AX∥2
2 −∥X∥2
2
∥X∥2
2
=
1
M ∥AX∥2
2 −
1
N2 ∥X∥2
2
1
N2 ∥X∥2
2
= Py
Px
−1.
If all samples are used (M = N) then Py = Px and α = 0. The isometry
property holds for any signal sparsity, as expected. For an arbitrary M < N
the restricted isometry property will not hold if Py = 0 can be obtained for
any combination of M out of N signal samples. Then |α| = 1.
The uncertainty principle for a discrete signal x(n) and its DFT states
that a product of the number of nonzero samples in the time domain (N −
Nz) and the number of nonzero values in the frequency domain K is always

Ljubiša Stankovi´c
Digital Signal Processing
691
greater or equal to the total number of signal samples N. It can be written as
K(N −Nz) ≥N,
where Nz is the number of zero-valued signal samples in the time domain.
The number of zero values in a signal is such that N −Nz ≥N/K or Nz ≤
N −N/K. For a unique reconstruction of a signal whose sparsity is K we
should be able to reconstruct a signal of sparsity 2K using any combination
of M available signal samples. Then Nz ≤N −N/(2K) should hold. It means
that the number of available samples should be greater than the maximal
number of zero signal values (to avoid the event Py = 0 with probability 1),
that is
M > Nz = N 2K −1
2K
samples are needed so that Py ̸= 0 for any combination of M available signal
samples/measurements. For example, for K = 1 the unique solution can be
obtained with M > N/2 signal samples, for K = 2 with M > 3N/4, for K = 4
using M > 7N/8, and so on. If the number of missing samples is denoted by
Q = N −M then the condition for a unique reconstruction of signal with
sparsity K using any M < N signal samples requires that the number of
unavailable samples Q satisﬁes N −Q > N(1 −1/(2K)) or
K < N
2Q =
N
2(N −M).
This is an interesting and simple, but pessimistic bound. It will be discussed
again at the end of this chapter.
10.3.5
Restricted Isometry and Eigenvalues
The restricted isometry property is satisﬁed for a K sparse signal, with a
restricted isometry constant δK, if the inequality
''''
1
EA
∥AKXK∥2
2 −∥XK∥2
2
'''' ≤δK ∥XK∥2
2
holds for any combination of K out of N columns AK of measurement matrix
A whose order is M × N. Note that (for real-valued A)
1
EA ∥AKXK∥2
2
∥XK∥2
2
=
1
EA XT
KAT
KAKXK
XT
KXK
.
According to the standard matrix norm relation
dmin ≤∥BX∥2
2
∥X∥2
2
= XTBTBX
∥B∥2
2
≤dmax,

692
Sparse Signal Processing
where dmin and dmax denote the minimal and the maximal eigenvalue of
Gram matrix BTB. Eigenvalues of Gram matrix are real and nonnegative.
In our case
BTB = 1
EA
AT
KAK.
Using this inequality we can write
dmin ≤
1
EA ∥AKXK∥2
2
∥XK∥2
2
≤dmax
1 −δmin ≤
1
EA ∥AKXK∥2
2
∥XK∥2
2
≤1 + δmax
where constants δmin and δmax are deﬁned by δmin = 1 −dmin, δmax =
dmax −1. A symmetric form of the restricted isometry property is commonly
used with
δK = max{δmin,δmax}.
A symmetric restricted isometry property inequality
1 −δK ≤
1
EA ∥AKXK∥2
2
∥XK∥2
2
≤1 + δK
is obtained. It can be related to the condition number of matrix
1
EA AT
KAK
deﬁned by
cond
! 1
EA
AT
KAK
6
= dmax
dmin
Since
1 −δK ≤dmin ≤dmax ≤1 + δK
it means
cond
! 1
EA
AT
KAK
6
≤1 + δK
1 −δK
.
Small values of δK, close to 0, mean robust and stable invertibility of Gram
matrix. In theory 0 ≤δK < 1 is sufﬁcient.
If the eigenvalues of matrix
1
EA AT
KAK are denoted by di then, by
deﬁnition,
det( 1
EA
AT
KAK −diI) = 0

Ljubiša Stankovi´c
Digital Signal Processing
693
The eigenvalues λi of a matrix
1
EA AT
KAK−I by deﬁnition satisfy
det(
- 1
EA
AT
KAK −I
.
−λiI) = 0
det( 1
EA
AT
KAK−(λi + 1)I) = 0.
Relation between the eigenvalues of
1
EA AT
KAK and
1
EA AT
KAK−I is
λi = di −1.
In a symmetric case the restricted isometry property bounds δmin, δmax
are symmetric for small sparsity, while for large sparsity the value δmax
dominates. It is common to calculate δK = δmax or
δK = dmax −1 = λmax = max
!
eig
* 1
EA
AT
KAK−I
+6
,
(10.27)
where λmax is the maximal eigenvalue of
1
EA AT
KAK−I for all (N
K) combina-
tion of {k1,k2,...,kK} ⊂{0,1,2,..., N −1}. The restricted isometry property is
satisﬁed with δK = λmax. Then δK is calculated as the maximal value over all
possible realizations of matrix AK from matrix A (including all matrices of
order lower than K). This calculation is an NP hard problem.
Example 10.3. Gaussian zero-mean random signals are used as the measurement
basis functions
ψk(n) = N (0, 1
M ).
These functions are used to weight measurements of a sparse signal X(k)
of sparsity K. The total number of samples is N = 2048.
The number of
measurements is M = 1024. Form the measurements matrix A whose di-
mension is M × N. Then form 10,000 random realizations of AK for K =
8,16,32,256,1024 (using K randomly positioned columns of A). Calculate all
eigenvalues of AT
KAK (Wishart matrix) and of AT
KAK−I. Estimate the largest
and the lowest value for each case and estimate the restricted isometry con-
stant.
⋆Fig.10.5 shows the histograms (normalized) of the eigenvalues for
N = 2048, M = 1024 and K = 8,16,32,256,1024. The limits ﬁt well with the
expected mean values of the bounds
E{dmax(M,K)} =
(
1 +
=
K
M
)2
(10.28)
E{dmin(M,K)} =
(
1 −
=
K
M
)2
,

694
Sparse Signal Processing
derived in literature for a large M. Dashed thick vertical lines indicate the
values
√
2 −1 and −(
√
2 −1) for λ. Later it will be shown that these
limits play an important role in the deﬁnition of a sufﬁciently small δK. The
absolute reconstruction limit δK = 1 is achieved ﬁrst with E{dmax(M,K)} =
*
1 +
F
K
M
+2
= 2 or
F
K
M ≤
√
2 −1 for K ≤0.1716M. We can see than the
case K = 16 is the last one whose eigenvalues in 10,000 realizations are
within limits, meaning that M = 1024 observations are sufﬁcient for unique
reconstruction (in the sense of these limits) of K = 8 sparse signal (for a K
sparse signal the reconstruction requires that all limits and constants are
satisﬁed for a 2K sparse signal). Note that the presented values are only
the mean values. Values dmax(M,K) and dmin(M,K) are random variables.
Minimal and maximal values obtained in 10,000 realizations are given in the
table.
Limit
√
2 −1 in λ or
√
2 in d is achieved using (10.28) for K = 0.0358M.
For M = 1024 its value is K = 36.6. Therefore this kind of bounds estimate
is optimistic. The value of the bound determined by the mean value is
lower than the maximal value based bound of a random variable, as we can
see from the table. Calculation of the bounds with satisfactory probability,
taking into account stochastic nature of eigenvalue limits, may be found in
literature.
K = 8,
λmin = −0.24,
λmax = 0.27,
K = 16,
λmin = −0.30,
λmax = 0.35,
K = 24,
λmin = −0.34,
λmax = 0.41,
K = 32,
λmin = −0.37,
λmax = 0.48,
K = 64,
λmin = −0.47,
λmax = 0.65,
K = 128,
λmin = −0.60,
λmax = 0.91,
K = 256,
λmin = −0.76,
λmax = 1.32,
K = 1024,
λmin = −0.98,
λmax = 3.08.
Limit cases for K/M ≪1 and for the case K = M easily follow.
Example 10.4. Write the full DFT transformation matrix for a signal of N = 8
samples.
(a) Show that it satisﬁes the unitary and isometry property (restricted
isometry property with δ = 0).
(b) Write the measurement matrix A if the number of available signal
samples/measurements in time domain is M = 6.
(c) If the sparsity in the DFT domain is K = 2 what is the form of the
submatrix A2 and the isometry constant δ2.
(d) Write δ2 in terms of coherence index µ.
(e) Consider cases with K = 3 and K = 4. Comment the results.

Ljubiša Stankovi´c
Digital Signal Processing
695
-1
-0.5
0
0.5
1
0
1
2
 pλ (ξ)
K=8
0
0.5
1
1.5
2
0
1
2
 pd (ξ)
K=8
-1
-0.5
0
0.5
1
0
1
2
 pλ (ξ)
K=16
0
0.5
1
1.5
2
0
1
2
 pd (ξ)
K=16
-1
-0.5
0
0.5
1
0
1
2
 pλ (ξ)
K=32
0
0.5
1
1.5
2
0
1
2
 pd (ξ)
K=32
-1
-0.5
0
0.5
1
0
1
2
 pλ (ξ)
K=64
0
0.5
1
1.5
2
0
1
2
 pd (ξ)
K=64
-1
0
1
2
0
1
2
3
 pλ (ξ)
K=256
0
1
2
3
0
1
2
3
 pd (ξ)
K=256
-1
0
1
2
3
0
5
10
15  pλ (ξ)
K=1024
0
1
2
3
4
0
5
10
15  pd (ξ)
K=1024
Figure 10.5
Histograms (normalized) of the eigenvalues of AT
KAK (Wishart matrix) and
AT
KAK−I matrix for N = 2048, M = 1024 and K = 8,16,32,256,1024. Dashed thick vertical lines
show the limits
√
2 −1 and −(
√
2 −1) sufﬁcient for unique K/2 signal reconstruction.

696
Sparse Signal Processing
⋆(a) For the DFT the transformation matrix is
x= ΨX
x = [x(0),x(1),x(2),x(3),x(4),x(5),x(6),x(7)]T
X = [X(0),X(1),X(2),X(3),X(4),X(5),X(6),X(7)]T
Ψ=1
8
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
1
W1
8
W2
8
W3
8
W4
8
W5
8
W6
8
W7
8
1
W2
8
W4
8
W6
8
W8
8
W10
8
W12
8
W14
8
1
W3
8
W6
8
W9
8
W12
8
W15
8
W18
8
W21
8
1
W4
8
W8
8
W12
8
W16
8
W20
8
W24
8
W28
8
1
W5
8
W10
8
W15
8
W20
8
W25
8
W30
8
W35
8
1
W6
8
W12
8
W18
8
W24
8
W30
8
W36
8
W42
8
1
W7
8
W14
8
W21
8
W28
8
W35
8
W42
8
W49
8
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
∗
where ∗denotes complex conjugate and Wnk
8
= exp(−j2πnk/8). The trans-
formation matrix W is unitary matrix according Parseval’s theorem
⟨ΨX,ΨY⟩=EΨ ⟨X,Y⟩
N−1
∑
n=0
x(n)y∗(n) = 1
N
N−1
∑
k=0
X(k)Y∗(k)
The isometry property follows from the unitary property of Ψ as
⟨ΨX,ΨX⟩=EΨ ⟨X,X⟩
∥ΨX∥2
2 = 1
N ∥X∥2
2
N−1
∑
n=0
|x(n)|2 = 1
N
N−1
∑
k=0
|X(k)|2 .
As expected for the full DFT matrix, the isometry property is satisﬁed, since
'''''
N ∥ΨX∥2
2 −∥X∥2
2
∥X∥2
2
''''' ≤δ with δ = 0.
(b) For M = 6 random samples/measurements at
ni ∈{n1,n2,n3,n4,n5,n6} = M
⊂N = {0,1,2,3,4,5,6,7}
the available signal values are
y = [x(n1),x(n2),x(n3),x(n4),x(n5),x(n6)]T

Ljubiša Stankovi´c
Digital Signal Processing
697
with
y = AX
A = 1
8
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
Wn1
8
W2n1
8
W3n1
8
W4n1
8
W5n1
8
W6n1
8
W7n1
8
1
Wn2
8
W2n2
8
W3n2
8
W4n2
8
W5n2
8
W6n2
8
W7n2
8
1
Wn3
8
W2n3
8
W3n3
8
W4n3
8
W5n3
8
W6n3
8
W7n3
8
1
Wn4
8
W2n4
8
W3n4
8
W4n4
8
W5n4
8
W6n4
8
W7n4
8
1
Wn5
8
W2n5
8
W3n5
8
W4n5
8
W5n5
8
W6n5
8
W7n5
8
1
Wn6
8
W2n6
8
W3n6
8
W4n6
8
W5n6
8
W6n6
8
W7n6
8
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
∗
where * denotes complex-conjugate.
(c) A submatrix of A of the order K = 2, for two arbitrary nonzero
coefﬁcients at k1 and k2, is
A2 = 1
8
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
Wn1k1
8
Wn1k2
8
Wn2k1
8
Wn2k2
8
Wn3k1
8
Wn3k2
8
Wn4k1
8
Wn4k2
8
Wn5k1
8
Wn5k2
8
Wn6k1
8
Wn6k2
8
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
∗
For the isometry property this matrix is a special case of (10.24) using only
k = k1 and k = k2,
∥A2X∥2
2 = 6
64
B
|X(k1)|2 + |X(k2)|2C
+ 2
64 Re
M
X(k1)X∗(k2)∑6
i=1W−nik1
8
Wnik2
8
N
64
6 ∥AX∥2
2 −
B
|X(k2)|2 + |X(k2)|2C
|X(k1)|2 + |X(k2)|2
=
2
6 Re
!
X(k1)X∗(k2)
M
∑
i=1
W−nik1
8
Wnik2
8
6
|X(k1)|2 + |X(k2)|2
(d) Using the coherence deﬁnition
µ(k1,k2) =
#
ψk1,ψ∗
k2
$
#
ψk1,ψ∗
k1
$
for the DFT we get
µ(k1,k2) = 1
6
6
∑
i=1
W−nik1
8
Wnik2
8
= 1
6
6
∑
i=1
ej2πni(k1−k2)/8.
Maximal value of |µ(k1,k2)| is the coherence index
µ = max
k1,k2,
k1̸=k2
|µ(k1,k2)|.
(10.29)

698
Sparse Signal Processing
The restricted isometry constant is
δ2 =
''''''
64
6 ∥AX∥2
2 −
B
|X(k2)|2 + |X(k2)|2C
|X(k2)|2 + |X(k2)|2
''''''
= max
'''''
2Re{X(k1)X∗(k2)µ(k1,k2)}
|X(k1)|2 + |X(k2)|2
'''''
≤2|X(k1)X∗(k2)|
'''''
µ(k1,k2)
|X(k1)|2 + |X(k2)|2
''''' ≤µ
The maximal value in this inequality is achieved for |X(k1)| = |X(k2)| and
max|µ(k1,k2)| = µ. Having in mind inequality for Re{X(k1)X∗(k2)µ(k1,k2)}
the overall maximum is achieved for |X(k1)| = |X(k2)| with
arg{X(k1)X∗(k2)µ(k1,k2)} = rπ/2
where r is an integer.
To comment the results consider the least mean square solution of
system
A2X = y
AH
2 A2X = AH
2 y
X =
B
AH
2 A2
C−1
AH
2 y
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
Wn1k1
8
Wn1k2
8
Wn2k1
8
Wn2k2
8
Wn3k1
8
Wn3k2
8
Wn4k1
8
Wn4k2
8
Wn5k1
8
Wn5k2
8
Wn6k1
8
Wn6k2
8
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
T ⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
Wn1k1
8
Wn1k2
8
Wn2k1
8
Wn2k2
8
Wn3k1
8
Wn3k2
8
Wn4k1
8
Wn4k2
8
Wn5k1
8
Wn5k2
8
Wn6k1
8
Wn6k2
8
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
∗⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
−1
X0
where
X0 = NAH
2 y
(10.30)
and AH
2 =
P
A∗
2
QT. Then by multiplying AH
2 A2 we get
X =
-
M
Mµ∗(k1,k2)
Mµ(k1,k2)
M
.−1
X0,
(10.31)
with
µ(k1,k2) = 1
M
M
∑
i=1
ej2πni(k1−k2)/N.

Ljubiša Stankovi´c
Digital Signal Processing
699
The determinant of the system is
det
-
M
Mµ∗(k1,k2)
Mµ(k1,k2)
M
.
= M2(1 −|µ(k1,k2)|2).
Obviously if
µ(k1,k2) = 1
M
M
∑
i=1
ej2πni(k1−k2)/N = ±1
when
ρ2 = µ = max|µ(i,k)| = 1
the system does not have a (unique) solution. It means that measurements
y(n) are not independent and that during the projection of the N dimensional
space of the sparse vector X to the space of dimension M < N by the
linear transformation AX = y the information about one of the two nonzero
coordinates is lost, i.e. it is projected to zero and can not be recovered.
The inversion robustness in (10.31) is the highest when µ(k1,k2) = 0.
The reconstruction is done in this case using the identity matrix. For values
of µ(k1,k2) increasing toward 1 the determinant value M2(1 −µ2(k1,k2))
reduces. It means the results in the reconstruction are multiplied by 1/(M2 −
M2µ2(k1,k2)). If there is noise in the measurements y, i.e., in the initial
estimate X0 = AHy, then the noise in the reconstruction will be increased,
meaning degradation of the signal-to-noise ratio. Therefore the values of
ρ2 = max|µ(i,k)| close to 1 are not desirable in the reconstruction, although
in theory, the reconstruction is possible. Reduction of the value of isometry
constant ρK toward zero will be of crucial importance in the application of
some reconstruction algorithms that will be presented later.
The values of
µ(k1,k2) = 1
M
M
∑
i=1
ej2πni(k1−k2)/N
(10.32)
for the DFT matrix are calculated for all possible (k1,k2) and presented in
Fig.10.6. The coherence index value is equal to the maximal absolute value
of µ(k1,k2). Signals of sparsity K = 2 (top), K = 3 (middle), and K = 4
(bottom) are considered for all possible positions of the available samples
ni and nonzero coefﬁcients ki. The restricted isometry constant for this signal
with N = 8 samples and M = 6 observations (available samples) at ni for
i = 1,2,3,4,5,6 is also calculated. The restricted isometry property constant
δK is calculated by using eigenvalues of the matrix Λ = eig
B
1
6AT
KAK−I
C
for
all possible nonzero positions of X(k), as in (10.27). Then, for example for
K = 2, δ(k1,k2) = λmax = max{Λ} is calculated for each possible AK. Finally
δ2 = maxk1,k2 δ(k1,k2). Note that in this case equality in δK ≤(K −1)µ holds
for all K, where µ = maxk1,k2 |µ(k1,k2)|, Fig.10.6.
(e) The calculation is done for K = 3 and K = 4 as well. The restricted
isometry property is not satisﬁed for matrix AK in the case K = 4. The

700
Sparse Signal Processing
0
0.2
0.4
0.6
0.8
1
0
500
1000
 N=8, K=4, M=6
Histogram of max|µ(ki,kj)|
 for all k1 , k2, k3, k4,  and ni, i=1,2,3,4,5,6
0
0.2
0.4
0.6
0.8
1
0
200
400
600
 N=8, K=4, M=6
Histogram of δ(k1,k2,k3,k4)
 for all k1 , k2, k3, k4,  and ni, i=1,2,3,4,5,6
δ4=1
0
0.2
0.4
0.6
0.8
1
0
100
200
300
 N=8, K=2, M=6
Histogram of max|µ(ki,kj)|
 for all k1 , k2,  and ni, i=1,2,3,4,5,6
0
0.2
0.4
0.6
0.8
1
0
100
200
300
 N=8, K=2, M=6
Histogram of δ(k1,k2)
 for all k1 , k2,  and ni, i=1,2,3,4,5,6
δ2=0.333
0
0.2
0.4
0.6
0.8
1
0
200
400
600
800
 N=8, K=3, M=6
Histogram of max|µ(ki,kj)|
 for all k1 , k2, k3,  and ni, i=1,2,3,4,5,6
0
0.2
0.4
0.6
0.8
1
0
200
400
600
800
1000
1200
 N=8, K=3, M=6
Histogram of δ(k1,k2,k3)
 for all k1 , k2, k3,  and ni, i=1,2,3,4,5,6
δ3=0.667
Figure 10.6
The coherence index value and the restricted isometry constant for signal with
N = 8 samples and M = 6 observations (available samples) at ni for i = 1,2,3,4,5,6. Signals of
sparsity K = 2 (top), K = 3 (middle), and K = 4 (bottom) are considered for all possible positions
of the available samples ni and nonzero coefﬁcients ki. The DFT is the transformation matrix.

Ljubiša Stankovi´c
Digital Signal Processing
701
maximal value of isometry constant is ρ4 = 1. It is interesting to note that
this value of isometry constant (when the isometry property does not hold)
is obtained for missing samples n7 = 1 and n8 = 5 (available samples x(ni) are
at positions ni = 0,2,3,4,6,7 for the nonzero positions of X(k) at k1 = 1, k2 = 3,
k3 = 5, and k4 = 7 or at k1 = 0, k2 = 2, k3 = 4, and k4 = 6. The same result
is obtained for missing samples n7 = 3 and n8 = 7 and the same nonzero
positions of the DFT coefﬁcients X(k). Having in mind the symmetry the
total number of realizations when the restricted isometry property does not
hold is 8 out of 1960 realizations. The probability that the restricted isometry
property is lost with an arbitrary signal of length N = 8, whose sparsity is
K = 4, with M = 6 observations is 0.0041.
10.3.6
Unique Reconstruction Condition and RIP
From the introductory examples we have seen that for a signal of sparsity
K = 1, two samples/measurements may produce full reconstruction. We
have also shown that any two samples/measurements may not be sufﬁ-
cient. The solution is unique if the determinant of any second order linear
system of equations, for these measurements, is nonzero
det
-
ψi(0)
ϕk(0)
ψi(1)
ϕk(1)
.
̸= 0
for all i ̸= k. In the case with K = 1 if there is more than M = 2 measurements
then at least any two of them should satisfy this condition. In the same way
we have concluded in the introductory analysis that for the case of sparsity
K = 2 at least four samples/measurements. The solution is unique if the
determinant of system for any signal of sparsity K = 4 is nonzero, (10.5). In
this way we can make a generalization for a K sparse signal. Obviously we
should have at least M ≥2K samples/measurements
y = AX
(10.33)
They are sufﬁcient for a unique reconstruction of a K sparse signal if M sam-
ples/measurements are independent in such a way that any 2K dimensional
sparse signal can be reconstructed from them, i.e., that all determinants of
2K order are nonzero
det(A2K) =det
⎡
⎢⎢⎣
ψk1(n1)
ψk2(n1)
...
ψk2K(n1)
ψk1(n2)
ψk2(n2)
...
ψk2K(n2)
...
...
...
....
ψk1(n2K)
ψk2(n2K)
...
ψk2K(n2K)
⎤
⎥⎥⎦̸= 0
(10.34)

702
Sparse Signal Processing
for at least one combination of available measurements {n1,n2,...,n2K} ⊂
{n1,n2,...,nM}, with M ≥2K, and any combination of {k1,k2,...,k2K} ⊂
{0,1,2,..., N −1}. Note that the number of 2K class combinations of N
elements is very large. It is ( N
2K).
Since our task is to check if there is 2K independent measurements, it
means that we want to check if the rank of matrix A2K is rank(A2K) = 2K.
There is no need for combinations over ni (to form a quadratic matrix
2K × 2K from M × 2K matrix) if M > 2K since the rank of M × 2K matrix
A2K can be checked by checking the rank of a 2K × 2K matrix AT
2KA2K using
rank(A2K) = rank(AT
2KA2K).
Matrix AT
2KA2K is the Gram matrix of A2K. For matrices A2K with complex
elements the conjugate transpose (Hermitian matrix) is used AH
2KA2K. A
way to check if the rank of AT
2KA2K is 2K is to calculate and check
det(AT
2KA2K) = d1d2...d2K ̸= 0
where d1d2...d2K are eigenvalues of AT
2KA2K. All eigenvalues di of symmetric
matrix AT
2KA2K
di = eig(AT
2KA2K)
are nonnegative. Rank of AT
2KA2K is 2K if the minimal eigenvalue of AT
2KA2K
is
dmin > 0.
This should be satisﬁed for all combinations of {k1,k2,...,k2K} ⊂{0,1,..., N −
1}. It means that the reconstruction will be achieved if
min
% 1
EA ∥A2KX2K∥2
2
∥X2K∥2
2
;
= dmin > 0.
For a practical matrix invertibility and robustness, commonly the condition
number
cond
! 1
EA
AT
2KA2K
6
= dmax
dmin
is used. The restricted isometry in this sense means
1 −δ2K ≤
1
EA ∥A2KX2K∥2
2
∥X2K∥2
2
≤1 + δ2K

Ljubiša Stankovi´c
Digital Signal Processing
703
with δ2K = max{1 −dmin,dmax −1} and 1 −δ2K ≤dmin ≤dmax ≤1 + δ2K,
cond
! 1
EA
AT
2KA2K
6
≤1 + δ2K
1 −δ2K
.
It means that the invertibility is possible, in theory, if 0 ≤δ2K < 1.
The previous analysis can be summarized by the next statement. If X
is a K sparse vector of dimension N then it can uniquely be reconstructed
from a reduced set of M samples/measurements y = AX if the measurement
matrix A is such that its submatrices A2K satisfy 2K restricted isometry
property with constant 0 ≤δ2K < 1 for all combinations of 2K out of N
columns.
This statement will be proven by contradiction. Assume that X is a K
sparse and the system of samples/measurements y = AX does not produce
a unique solution. It means that two different signals X and H of sparsity K
may satisfy the same measurements equation
y = AX and y = AH.
Then
(AX −AH)=0
A(X −H)=0.
The vector X −H is in general 2K sparse since it contains K nonzero ele-
ments of X and K different nonzero elements of H. Within the restricted
isometry property context it means
'''''
1
EA ∥A(X −H)∥2
2 −∥X −H∥2
2
∥X −H∥2
2
''''' ≤δ2K.
Since A(X −H)= 0 it follows δ2K = 1. If
0 ≤δ2K < 1
then ∥A(X −H)∥2
2 ̸= 0 meaning AX = y and AH = y is not possible for two
different vectors of sparsity K if δ2K < 1. This condition should be satisﬁed
for all combinations of {k1,k2,...,k2K} ⊂{0,1,2,..., N −1}.
Reconstruction with δ2K close to 1 should be avoided due to high inﬂu-
ence of possible noise in data. Small values of δ2K, close to 0, means robust
and stable invertibility. Imposing any speciﬁc limit for condition number

704
Sparse Signal Processing
is equivalent to imposing the restricted isometry property with a speciﬁc
constant δ2K. For example, the requirement that cond
M
1
EA AT
2KA2K
N
< 2 is
the same as
0 ≤δ2K < 1/3.
10.3.7
Rank and Spark of a Matrix
Consider a matrix A with M rows containing N ≥M elements (N columns).
The rank of matrix A is equal to the largest number of independent columns
(rows). Obviously for a nonzero matrix
1 ≤rank{A} ≤M.
The spark of matrix A is the smallest number of dependent columns (rows).
By deﬁnition if one column contains all zero elements then spark {A} = 1.
In general,
2 ≤spark{A} ≤M + 1.
Example 10.5. Find the rank and spark of matrix
A =
⎡
⎣
1
0
1
3
2
2
0
2
3
2
1
1
−1
4
2
1
−1
3
⎤
⎦
The rank of matrix A is rank{A} = 3 since we may easily check that the
determinant of a matrix formed using ﬁrst three columns of A is nonzero. If
that determinant was zero, then before concluding that rank of A is lower
than 3 we should try with all possible combinations of columns. If all com-
binations of 3 columns were dependent, then we should check if rank {A} =
rank
M
AATN
= 2 by forming all possible 2 × 2 submatrices. If any of them has
a nonzero determinant then the rank would be 2, otherwise the rank would
be one when only one nonzero element of matrix A exists.
There are several methods for calculation of rank of matrix without
combinatorial search. The rank calculation can be simpliﬁed using the fact
that rank{A} = rank
M
AATN
. The only one matrix
AAT =
⎡
⎣
19
13
8
13
19
18
8
18
32
⎤
⎦
should be checked for the possible rank 3. Note also that det
M
AATN
=
λ1λ2λ3 where {λ1,λ2,λ3} = eig
M
AATN
. Therefore for rank
M
AATN
= 3 all

Ljubiša Stankovi´c
Digital Signal Processing
705
eigenvalues should be different from zero. In addition, the rank is equal to
the number of nonzero elements of {λ1,λ2,λ3}.
For spark we have to ﬁnd minimal number of dependent columns.
-Obviously there is no all zero column, thus
spark{A} > 1.
-The ﬁrst possible solution is spark{A} = 2. The spark of matrix A is 2 if
there are two dependent columns. Two columns are dependent if there is
linear relation between their elements. It means that spark {A} = 2 if there is
any par of two columns such that its rank is equal to one, i.e., that they are
proportional to each other. Here we have to check rank of all combinations
of two columns. the total number of combinations is (6
2) = 15. We have
calculated all combinations and found that for all of them rank was 2. It
means that spark{A} > 2.
-Next we have to check all possible combinations of three columns.
There are (6
3) = 20 combinations of 3 columns. We have calculated rank of all
20 combinations of columns and found that there is a dependent combination
of columns. Namely, the fourth column of A is obtained by multiplying the
ﬁrst column by 3 and adding it to the second column. Since we have found
at least one dependent combination of columns, further search is not needed.
Therefore the lowest number of dependent columns is 3. It means that
spark{A} = 3.
If there were not dependent columns in this calculation using combinations
of 3 columns, then the spark would be spark{A} = M + 1 = 4 by deﬁnition.
Relation between the rank and spark can be established based on
following consideration. If a matrix has spark {A} = 3 it means that all
combinations of two columns are independent and that there is at least
one dependent combination of three columns. Since the rank of matrix
rank{A} = 2 if at least one combination of two columns are independent
then obviously if spark{A} = 3 then rank{A} ≥2. In general if
spark{A} = p
then it means that all combinations of p −1 columns are independent
(including all combination of lower than p −1 number of columns). Since
the rank requires that at least one combination of p columns is independent
then
rank{A} ≥p −1.

706
Sparse Signal Processing
It means that
rank{A} ≥spark{A} −1
spark{A} ≤rank{A} + 1.
If a matrix A has M rows and N ≥M columns then 0 ≤rank{A} ≤M
and 1 ≤spark{A} ≤M + 1. In a special case of orthogonal square matrix
N = M, by deﬁnition spark{A} →∞. The sense of this deﬁnition will be
clariﬁed later.
10.3.8
Spark and the Solution Uniqueness
The spark of measurement matrix is used for very simple deﬁnition of the
existence of the sparsest solution of a minimization problem
min∥X∥0
subject to
y = AX.
If the vector X is of sparsity K, with
∥X∥0 = K
then if
K < 1
2spark{A}
the solution X is unique.
In order to prove this statement consider a matrix A whose spark is
spark{A}. Then for a sparse vector X of sparsity K = spark{A} obviously
there exists such a combination of nonzero elements in X so that they
coincide with the dependent columns. Then we can obtain
AX = 0.
This property is used for the spark deﬁnition as well
min∥X∥0 such that AX = 0.
Note that for any X of sparsity K < spark{A} the relation AX = 0 will
not hold, since nonzero signal elements of X cannot produce a zero result
when multiplied by columns which are independent. Since K < spark{A}
it means that in all cases K columns are independent.
The proof that K < 1
2spark{A} means that X, being solution of AX = y,
is unique, will be based on contradiction.

Ljubiša Stankovi´c
Digital Signal Processing
707
Assume that X is a solution and that it satisﬁes K < 1
2spark{A} but
that there is another solution H such that AH = y which is also sparse with
sparsity lower than the sparsity of X, i.e., lower than 1
2spark{A}.
Since
AH = AX = y
A(H −X) = 0
then
spark{A} = min∥H −X∥0 such that A(X −H) = 0.
or
spark{A} = ∥H −X∥0 ≤∥H∥0 + ∥X∥0
spark{A} −∥H∥0 ≤∥X∥0 .
If there is another solution H such that ∥H∥0 < 1
2spark{A} then from
the last inequality follows ∥X∥0 > 1
2spark{A}. This is a contradiction to
the assumption that both solutions H and X have sparsity lower than
1
2spark{A}.
The spark of matrix can be related to the coherence of matrix. The
relation is
spark{A} = 1 +
1
µ(A)
where µ(A) (or just µ) is the coherence index of matrix A. The proof is based
on the quadratic norm positivity of the matrix ATA.
The coherence index value is (10.29)
µ(A) = max
i̸=k |µ(i,k)| = max
'''''
Y
ψi,ψ∗
k
Z
Y
ψi,ψ∗
i
Z
'''''.
where ψi are columns of matrix A. It is assumed that all columns are of equal
energy 1
M
Y
ψi,ψ∗
i
Z = 1.
The maximal possible value of spark is spark {A} = M + 1 when there
is no dependent columns. Then
K < 1
2 (M + 1).
For K sparse signal we must have at least
M ≥2K.

708
Sparse Signal Processing
Note that for random matrices we have
spark{A} = M + 1
with a very high probability. However, in the cases of noisy signals or ap-
proximately sparse signals, more robust calculations are required increasing
the number of required observations. For a quadratic and orthogonal matrix
A the coherence index is µ(A) = 0 and for that matrix spark{A} →∞, by
deﬁnition.
For the illustrative example from the beginning of this chapter we
had a condition that one false bag can be discovered if we performed two
measurements
-
y(0)
y(1)
.
=
-
ψ0(0)
ψ1(0)
...
ψN−1(0)
ψ0(1)
ψ1(1)
...
ψN−1(1)
.
⎡
⎢⎢⎣
X(0)
X(1)
...
X(N −1)
⎤
⎥⎥⎦
y = AX
such that ψi(0)ϕk(1) −ψi(1)ϕk(0) ̸= 0 for any combination of columns i and
k. It means that two columns are not dependent, i.e., that
ψi(0)
ψk(0) = ψi(1)
ψk(1) does not hold for any i ̸= k.
Assuming that there is no an all zero column then spark {A} = 3 meaning
that a signal X of sparsity K < 1
2 (2 + 1) can be recovered.
Within this framework we can now consider the case with three mea-
surements
⎡
⎣
y(0)
y(1)
y(2)
⎤
⎦=
⎡
⎣
ψ0(0)
ψ1(0)
...
ψN−1(0)
ψ0(1)
ψ1(1)
...
ψN−1(1)
ψ0(2)
ψ1(2)
...
ψN−1(2)
⎤
⎦
⎡
⎢⎢⎣
X(0)
X(1)
...
X(N −1)
⎤
⎥⎥⎦
y = AX.
Then one nonzero value of X can be recovered if the spark of A is
spark{A} ≥3. Since the spark in this case can assume a value up to 4, the
value of spark is equal or greater than 3 if any combination of two columns
are not dependent. The condition that spark {A} ̸= 2 is
ψi(0)
ψk(0) = ψi(1)
ψk(1) = ψi(2)
ψk(2) does not hold for any i ̸= k.

Ljubiša Stankovi´c
Digital Signal Processing
709
In the notation of the determinants it means that det
-
ψi(0)
ϕk(0)
ψi(1)
ϕk(1)
.
̸= 0
or det
-
ψi(1)
ϕk(1)
ψi(2)
ϕk(2)
.
̸= 0. In the terminology of a matrix rank it means
that rank{A2} = 2 for any submatrix A2 of two columns of A. The matrix
A2 has two columns and M rows. For the rank calculation there is no need
for combinations over rows since
rank{A2} = rank
M
AT
2 A2
N
,
where AT
2 A2 is 2 × 2 matrix. Still all combinations over different columns
should be checked. There are (N
2) = N(N −1)/2 of them.
Checking a spark of order p is an NP hard problem since all combina-
tion of p out of N elements should be checked.
Example 10.6. Gaussian zero-mean random signals are used as the measurement
basis functions
ψk(n) = N (0, 1
M ).
These functions are used to weight measurements of a sparse signal X(k)
whose sparsity is K and the total number of samples is N = 2048. The number
of measurements is M = 1024. Using the coherence relations estimate the
largest value of K so that the measurements matrix satisﬁes the restricted
isometry property with δ2K < 0.41 with a probability of 0.9999.
⋆The coherence index for columns k1 and k2 is
µ(k1,k2) =
M
∑
i=1
ψk1(ni)ψ∗
k2(ni).
Since the variance of ψk1(ni) is 1/M then EA = 1. For Gaussian variables the
variance of random variable µ(k1,k2) is
σ2 = Mσ2
ψσ2
ψ = M 1
M
1
M = 1/M
(see Problem 7.13). As a sum of large number of random variables the
resulting variable µ(k1,k2) can be considered as Gaussian with variance
σ2 = 1/M. Since δ2K ≤(2K −1)µ where µ = max|µ(k1,k2)| then using the
equality δ2K = (2K −1)µ in the estimation for a given δ2K then all absolute
values of µ(k1,k2) should satisfy
|µ(k1,k2)| ≤µ =
δ2K
(2K −1)
with a high probability P = erf(S/
√
2), following
S = µ
σ =
√
Mδ2K
(2K −1)

710
Sparse Signal Processing
sigma rule. In order to ﬁnd P (and corresponding S) note that there are (N
2)
different values of µ(k1,k2). Assuming that they are independent
Pr{max|µ(k1,k2)| ≤
√
Mδ2K
(2K −1) } =
B
erf(S/
√
2)
C(N
2)
for (N
2 ) = 2038 · 1024 ∼106 the value S = 6.5 will produce the above proba-
bility of order 0.9999. It means
(2K −1) =
√
Mδ2K
S
= 2.02.
The largest value of K according to this analysis is K = 1. This is a
very pessimistic estimation, as compared to the analysis in Fig.10.5. There
we could expect a unique reconstruction, with the same probability, for
K = 16/2 = 8. Note that here M =
S
δ2K (2K −1)2 holds. Calculations closer
to the expected results are derived in literature.
Welsh bound
µ ≥
<
N −M
M(N −1)
and the restricted isometry property with
δK = (K −1)
<
N −M
M(N −1)
for M ≪N lead to (K −1)2 = δ2
KM. This leads to K = O(M1/2) what is much
lower than the theoretically known reconstruction limit being of order of
M/ln(N/M).
10.4
NORM-ZERO BASED RECONSTRUCTION
Although the ℓ0-norm cannot be used in the direct minimization, the algo-
rithms based on the assumption that some coefﬁcients X(k) are equal to
zero, and the minimization of the number of remaining nonzero coefﬁcients
that can reconstruct sparse signal, may efﬁciently be used.
10.4.1
Direct Combinatorial Search
The reconstruction process can be formulated as ﬁnding the positions and
the values of K nonzero coefﬁcients X(k) of a sparse signal (or all signal x(n)

Ljubiša Stankovi´c
Digital Signal Processing
711
values) using a reduced set of signal values x(ni),
ni ∈M = {n1,n2,...,nM} ⊂{0,1,2,..., N −1}
such that
min∥X∥0 subject to y = AX
where ∥X∥0 = card{X} = K. Consider a discrete-time signal x(n). Signal is
sparse in a transformation domain deﬁned by the basis functions set ψk(n),
k = 0,1,..., N −1. The number of nonzero transform coefﬁcients K is much
lower than the number of the original signal samples N, i.e., X(k) = 0 for
k /∈{k1,k2,...,kK} = K,
K ≪N. A signal
x(n) =
∑
k∈{k1,k2,...,kK}
X(k)ψk(n).
(10.35)
of sparsity K can be reconstructed from M samples, where M ≤N.
In
the case of signal x(n) which is sparse in the transformation domain there
are K nonzero unknown values X(k1), X(k2),...,X(kK). Other transform
coefﬁcients X(k), for k /∈{k1,k2,...,kK} = K, are zero-valued.
Just for the beginning assume that the transformation coefﬁcient posi-
tions {k1, k2, ..., kK} are known. Then the minimal number of equations to
ﬁnd the unknown coefﬁcients (and to calculate signal x(n) for any n) is K.
The equations are written for at least K time instants ni, i = 1,2,..., M ≥K,
where the signal is available/measured,
∑
k∈K
X(k)ψk(ni) = x(ni), for i = 1,2,..., M ≥K.
(10.36)
In a matrix form this system of equations is
AKXK= y,
(10.37)
where XK is the vector of unknown nonzero coefﬁcients values (at the
known positions) and y is the vector of available signal samples,
XK = [X(k1) X(k2) ... X(kK)]T
(10.38)
y = [x(n1) x(n2) ... x(nM)]T
AK =
⎡
⎢⎢⎣
ψk1(n1)
ψk2(n1)
...
ψkK(n1)
ψk1(n2)
ψk2(n2)
...
ψkK(n2)
...
...
...
....
ψk1(nK)
ψk2(nK)
...
ψkK(nK)
⎤
⎥⎥⎦.
(10.39)

712
Sparse Signal Processing
Matrix AK is the measurements matrix A with the columns corresponding
to the zero-valued transform coefﬁcients k /∈{k1, k2, ..., kK} being excluded.
For a given set {k1,k2,...,kK} = K the coefﬁcients reconstruction condition
can be easily formulated as the condition that system (10.37) has a (unique)
solution, i.e., that there are K independent equations,
rank(AK) = K.
Note that this condition does not guarantee that another set {k1,k2,...,kK} =
K can also have a (unique) solution, for the same set of available samples.
The uniqueness of solution is considered within the previous subsections.
It requires rank(A2K) = 2K for any submatrix A2K of the measurements
matrix A. It will be addressed for the DFT case again later in this chapter.
System (10.36) is used with K ≪M ≤N. Its solution, in the mean
squared sense, follows from the minimization of difference of the available
signal values and the values produced by inverse transform of the recon-
structed coefﬁcients, minX(k)
R
e2S
where
e2 = ∑
n∈M
'''''y(n) −∑
k∈K
X(k)ψk(n)
'''''
2
=
= (y −AKXK)H (y −AKXK) = ∥y∥2
2 −2XH
K AH
K y + XH
K AH
K AKXK
(10.40)
or
min
M
(y −AKXK)H (y −AKXK)
N
where exponent H denotes the Hermitian conjugate. The derivative over
X∗(p) is (Chapter I, equation (1.13))
∂e2
∂X∗(p) = 2 ∑
n∈M
(y(n) −∑
k∈K
X(k)ψk(n))ψ∗
p(n).
The minimum of quadratic form error is reached for
∑
n∈M
ψ∗
p(n)y(n) = ∑
n∈M ∑
k∈K
ψk(n)ψ∗
p(n)X(k)
for p = 0,1,..., N −1.
In matrix form this system of equations reads
AH
K y = AH
K AKXK.

Ljubiša Stankovi´c
Digital Signal Processing
713
Its solution is
XK=
B
AH
K AK
C−1
AH
K y.
(10.41)
It can be obtained by a symbolic vector derivation of (10.40) as
∂e2
∂XH
K
= −2AH
K y + 2AH
K AKXK = 0.
If we do not know the positions of the nonzero values X(k) for k ∈{k1,
k2, ..., kK} = K then all possible combinations of {k1, k2, ..., kK} ⊂N should
be tested. There are (N
K) of them. It is not computationally feasible problem.
Thus we must try to ﬁnd a method to estimate {k1, k2, ..., kK} in order to
recover values of X(k).
10.4.2
Pseudoinverse matrix
In (10.41) we have used a form of the pseudoinverse of a matrix. In general,
if AHA is invertible then the pseudoinverse of matrix A is deﬁned by
pinv(A) =
B
AHA
C−1
AH.
This is the left pseudoinverse since
pinv(A)A = I
In the case that AAH is invertible the pseudoinverse is deﬁned by
pinv(A) = AH B
AAHC−1
.
It is the right pseudoinverse since
Apinv(A) = I.
For the considered matrices, for an M × N matrix A with M < N, the
matrix AAH of dimension M × M can be invertible (the highest possible
rank of an M × N matrix, with M < N, is M). For a matrix AK whose
dimension is M × K, with K < M, the matrix AHA of dimension K × K can
be invertible.
For a system of equations AX = y if there is one solution
X0 = pinv(A)y

714
Sparse Signal Processing
then all solutions are
X =pinv(A)y + [I−pinv(A)A]z
(10.42)
where z is an arbitrary vector.
For invertible AHA holds pinv(A)A = I and the solution
X =pinv(A)y =
B
AHA
C−1
AHy
is unique.
For invertible AAH we have an indeterminate system. All solutions
can be written in form (10.42) with arbitrary z. It can be easily shown that,
in this case, by using the norm-two ( ℓ2-norm) minimization
min∥X∥2 subject to AX = y
the solution is X =pinv(A)y = AH B
AAHC−1
y with z = 0.
For the DFT analysis, the signal corresponding to X = AH B
AAHC−1
y,
would be
xR = ΨX = ΨAH B
AAHC−1
y =
-
A
Ac
.
AH B
AAHC−1
y
=
⎡
⎢⎣
AAH B
AAHC−1
y
AcAH B
AAHC−1
y
⎤
⎥⎦=
-
y
0
.
,
(10.43)
where A is the measurement matrix and Ac is its complement to Ψ so that
Ψ =
-
A
Ac
.
. This is the result expected from Parseval’s theorem.
10.4.3
Estimation of Unknown Positions
Solution of the minimization problem, assuming that the positions of the
nonzero signal coefﬁcients in the sparse domain are known, is presented
in the previous two subsections. The next step is to estimate the coefﬁcient
positions, using the available samples. A simple way is to try to estimate the
positions based on signal samples that are available, ignoring unavailable
samples. This kind of transform estimate is
ˆX(k) = ∑
n∈M
x(n)ϕk(n),
(10.44)

Ljubiša Stankovi´c
Digital Signal Processing
715
where for the DFT ϕk(n) = exp(−j2πnk/N) and n ∈M = {n1,n2,...,nM}.
Since ϕk(n) = Nψ∗
k(n) this relation can be written as (10.30)
ˆX = NAHy
where A is the measurement matrix. With K ≪M ≪N the coefﬁcients
ˆX(k), calculated with M samples, are random variables. Note that using
(10.44) in calculation is the same as assuming that the values of unavailable
samples x(n), n /∈M, is zero. This kind of calculation corresponds to the
result (10.43) that would be achieved for the signal transform if ℓ2-norm is
used in minimization.
Algorithm
A simple and computationally efﬁcient algorithm, for signal recovery,
can now be implemented as follows:
(i) Calculate the initial transform estimate ˆX(k) by using the avail-
able/remaining signal values
ˆX(k) = ∑
n∈M
x(n)ϕk(n)
(10.45)
or ˆX=NAHy.
(ii) Set the transform values X(k) to zero at all positions k except the
highest ones. Alternative:
(ii) Set the transform values X(k) to zero at all positions k where this
initial estimate ˆX(k) is below a threshold Tr,
X(k) = 0 for k ̸= ki, i = 1,2,..., ˆK
ki = arg{
'' ˆX(k)
'' > Tr}.
This criterion is not sensitive to Tr as far as all nonzero positions of the
original transform are detected ( ˆX(k) is above the threshold) and the total
number ˆK of transform values in ˆX(k) above the threshold is lower than the
number of available samples, i.e., K ≤ˆK ≤M.
All ˆK −K transform values that are zero in the original signal will be
found as zero-valued.
(iii) The unknown nonzero (including ˆK −K zero valued) transform
coefﬁcients could be then easily calculated by solving the set of M equations
for available instants n ∈M, at the detected nonzero candidate positions ki,
i = 1,2,..., ˆK,
ˆK
∑
i=1
X(ki)ψki(n) = x(n), for n ∈M.

716
Sparse Signal Processing
This system of the form
AKXK= y
is now reduced to the problem with known positions of non zero coefﬁcients
(considered in the previous section). It is solved in the least square sense as
(10.41)
XK =
B
AH
K AK
C−1
AH
K y.
(10.46)
The reconstructed coefﬁcients X(ki), i = 1,2,..., ˆK, (denoted by vector XK) are
exact, for all frequencies. If some transform coefﬁcients, whose true value
should be zero, are included (when K < ˆK) the resulting system will produce
their correct (zero) values.
Comments: In general, a simple strategy can be used by assuming that
ˆK = M and by setting to zero value only the smallest N −M transform
coefﬁcients in ˆX(k). System (10.36) is then a system of M linear equations
with ˆK = M unknown transform values X(ki). If the algorithm fails to detect
a component the procedure can be repeated after the detected components
are reconstructed and removed. This simple strategy is very efﬁcient and
if there is no input noise. Large ˆK, close or equal to M, will increase the
probability that full signal recovery is achieved in one step. It will be shown
later that in the case of an additive (even small) input noise in all signal
samples a reduction of the number ˆK as close to the true signal sparsity K as
possible will improve the signal-to-noise ratio.
Example 10.7. Consider a discrete signal
x(n) = 1.2ej2πn/16+jπ/4 + 1.5ej14πn/16−jπ/3 + 1.7ej12πn/16,
for 0 ≤n ≤15, sparse in the DFT domain since only three DFT values are
different than zero. Assume now that its samples x(2), x(4), x(11), and x(14)
are not available. Show that, in this case, the exact DFT reconstruction may
be achieved by: (1) Calculating the initial DFT estimate by setting unavailable
sample values to zero
ˆX(k) = ∑
n∈M
x(n)ej2πkn/16=16AHy,
where
n ∈M = {0,1,3,5,6,7,8,9,10,12,13,15}.
(2) Detecting, for example K = 3 positions of maximal DFT values, k1, k2, and
k3, and (3) calculating the reconstructed DFT values at k1, k2, and k3 from
system
3
∑
i=1
X(ki)ej2πkin/16 = x(n),

Ljubiša Stankovi´c
Digital Signal Processing
717
where n ∈M = {0,1,3,5,6,7,8,9,10,12,13,15} are the instants where the
signal is available.
⋆The discrete-time signal x(n), with 0 ≤n ≤15 is shown in Fig. 10.7.
The signal is sparse in the DFT domain since only three DFT values are
different than zero (Fig. 10.7(second row)). The CS signal, with missing
samples x(2), x(4), x(11), and x(14), being set to 0 for the initial DFT
estimation, is shown in Fig. 10.7 (third row). The DFT of the signal, with
missing values being set to 0, is calculated and presented in Fig. 10.7 (fourth
row). There are three DFT values, at k1 = 1, k2 = 6, and k3 = 7
K = {1,6,7}
above the assumed threshold, for example, at level of 11. The rest of the DFT
values is set to 0. This is justiﬁed by using the assumption that the signal is
sparse. Now, we form a set of equations, for these frequencies k1 = 1, k2 = 6,
and k3 = 7 as
3
∑
i=1
X(ki)ej2πkin/16 = x(n),
where n ∈M = {0,1,3,5,6,7,8,9,10,12,13,15} are the instants where the
signal is available. Since there are more equations than unknowns, the system
AKXK= y is solved using XK =
P
AH
K AK
Q−1 AH
K y. The obtained reconstructed
values are exact, for all frequencies k, as in Fig. 10.7(second row). They are
shown in Fig. 10.7 (ﬁfth row).
If the threshold was lower, for example at 7, then six DFT values at
positions
K = {1,6,7,12,14,15}
are above the assumed threshold. The system with six unknowns
6
∑
i=1
X(ki)ej2πkin/16 = x(n),
where n ∈M = {0,1,3,5,6,7,8,9,10,12,13,15} will produce the same values
for X(1), X(6), and X(7) while the values X(12) = X(14) = X(15) = 0 will be
obtained.
If the threshold is high to include the strongest signal component only,
then the solution is obtained through an iterative procedure described later,
after noise analysis.
10.4.4
Unavailable/Missing Samples Noise in Initial Estimation
The initial DFT calculation (10.44) is done assuming zero-valued missing
samples. The initial calculation quality has a crucial importance for success-
ful signal recovery. With a large number of randomly positioned missing

718
Sparse Signal Processing
0
5
10
15
-4
-2
0
2
4
Original signal
0
5
10
15
0
10
20
30
DFT of original signal
0
5
10
15
-4
-2
0
2
4
Signal with 4 missing samples 
0
5
10
15
0
10
20
30
DFT of signal with
4 missing samples set to 0
threshold for reconstruction
0
5
10
15
0
10
20
30
Reconstructed DFT 
on detected frequencies
Figure 10.7
Original signal in the discrete-time domain (ﬁrst row); the DFT of the original
signal (second row); signal with four missing samples at n = 2, 4,11, and 14 set to zero (third
row); the DFT of signal with missing values being set to 0 (fourth row). The reconstructed
signal assuming that the DFT contains components only at frequencies where the initial DFT is
above threshold (ﬁfth row). Absolute values of the DFT and real part of signal are shown.
samples, the missing samples manifest themselves as a noise in this initial
transform. For a sparse signal of the form
x(n) =
K
∑
p=1
Apej2πnkp/N,

Ljubiša Stankovi´c
Digital Signal Processing
719
the initial DFT is calculated using n ∈M = {n1,n2,...,nM}, as NAHy, or
X(k) = ∑
n∈M
x(n)e−j2πnk/N = ∑
n∈M
K
∑
p=1
Ape−j2πn(k−kp)/N.
(10.47)
We can distinguish two cases: (1) For k = ki ∈{k1,k2,...,kK} then, with
M = card(M),
X(ki) = AiM + ∑
n∈M
K
∑
p=1,p̸=i
Ape−j2πn(ki−kp)/N.
The value of
Ξ = ∑
n∈M
K
∑
p=1,p̸=i
Ape−j2πn(ki−kp)/N
(10.48)
with a random set M = {n1,n2,...,nM}, for 1 ≪M ≪N, can be considered
as a random variable. Its mean value over different realizations of available
samples (different realizations of sets M) is E{Ξ} = 0. The mean value of
X(ki) is
E{X(ki)} = AiM.
(2) For k /∈{k1,k2,...,kK} the mean value of (10.47) is
E{X(k)} = 0.
The mean value of (10.47) for any k is of the form
E{X(k)} = M
K
∑
p=1
Apδ(k −kp).
The variance of signal transform is
σ2
N(k) = var(X(k)) =
K
∑
p=1
A2
pM N −M
N −1
D
1 −δ(k −kp)
E
.
(10.49)
This relation will be derived next. To simplify notation the variance of X(k)
will be calculated for K = 1 with k ̸= k1. The variance is deﬁned by
var{X(k)} = E
%
∑
n∈M ∑
m∈M
|A1|2 e−j2πm(k−k1)/Nej2πn(k−k1)/N
;
(10.50)
= ∑
m∈M
E
%
|A1|2 +
∑
n∈M,n̸=m
|A1|2 e−j2πm(k−k1)/Nej2πn(k−k1)/N
;

720
Sparse Signal Processing
Obviously,
E
%
∑
n∈M
|A1|2
;
= |A1|2 M.
Full set of signal samples would produce the DFT of original signal. It
means that all variables ej2πn(k−k1)/N are not statistically independent for
(k −k1) ̸= 0. They satisfy
e−j2πm(k−k1)/N
N−1
∑
n=0
ej2πn(k−k1)/N = 0
since the sum over all discrete-time instants is deterministic and X(k) = 0
for k ̸= k1. Its expectation is
N−1
∑
n=0
E{e−j2πm(k−k1)/Nej2πn(k−k1)/N} = 0
(10.51)
Since all values ej2πn(k−k1)/N (with random n) are equally distributed we
may write their expected value over many realizations of different sets M
as
E{e−j2πm(k−k1)/Nej2πn(k−k1)/N} = B, for n ̸= m,
(10.52)
E{e−j2πn(k−k1)/Nej2πn(k−k1)/N} = 1 for n = m.
From (10.51) and (10.52) follows
(N −1)B + 1 = 0.
Now we can easily calculate terms in (10.50)
E
%
∑
n∈M,n̸=m
|A1|2 e−j2πm(k−k1)/Nej2πn(k−k1)/N
;
= |A1|2 (M −1)B = |A1|2 (M −1)
*
−
1
N −1
+
.
Finally, the variance of X(k), for k ̸= k1, is
σ2
N(k) = var(X(k)) = |A1|2 M
*
1 + (M −1)
*
−
1
N −1
++
= |A1|2 M N −M
N −1 .

Ljubiša Stankovi´c
Digital Signal Processing
721
Of course, for k = k1 we get
σ2
N(k1) = 0
since all terms in X(k) are summed in phase with no random variation. The
ratio of signal amplitude X(k1) and standard deviation σN(k) for k ̸= k1 is
the crucial parameter for a correct signal detection. Its value is
σN(k)
|X(k1)| =
<
N −M
M(N −1).
For small M, when (N −M)/(N −1) ∼1 we have σN(k)/|X(k1)| =
√
M.
For M ≪N a rough approximation var(X(k)) = |A1|2 M follows. It corre-
sponds to the assumption of statistically independent variables.
Note that the variance in a multicomponent signal with K > 1 is sum
of the variance of individual components at all frequencies k
σ2
N(k) = M N −M
N −1
K
∑
p=1
''Ap
''2 ,
(10.53)
except at ki ∈{k1,k2,...,kK} when the values are lower for |Ai|2 M N−M
N−1
σ2
N(ki) = M N −M
N −1
K
∑
p=1
p̸=i
''Ap
''2 ,
since all ith component values are then added up in phase at k = ki, without
random variations.
According to the central limit theorem, for 1 ≪M ≪N the real and
imaginary parts of the DFT values for noise only positions k /∈{k1,k2,...,kK}
can be described by Gaussian distribution, N (0,σ2
N/2) with zero-mean and
variance σ2
N = σ2
N(k). Real and imaginary parts of the DFT value, at the
pth signal component position kp ∈{k1,k2,...,kK}, can be described by the
Gaussian distributions
N (MRe{Ap},σ2
Sp/2), and
N (MIm{Ap},σ2
Sp/2),
(10.54)
respectively, where
σ2
Sp = σ2
N −A2
pM N −M
N −1 ,
according to (10.49).

722
Sparse Signal Processing
Example 10.8. Consider a three-component signal
x(t) = A1 exp(j2πk1t/N) + A2 exp(j2πk2t/N) + A3 exp(j2πk3t/N) (10.55)
with A1 = 1, A2 = 0.75, A3 = 0.25, {k1,k2,...,kK} = {58,117,21}, within 0 ≤t ≤
256. With t = n∆t, ∆t = 1 and N = 257 the signal is sparse in the DFT domain.
Random realizations of the initial DFT (10.45) are given in Fig.10.8, for several
values of the number of available samples M. We can see that a low value of
M does not provide possibility to detect the signal component positions. All
three components are visible for larger values of M. When signal frequencies
are detected then the signal is recovered using (10.41) and the available
samples in the discrete-time domain at ni ∈{n1,n2,...,nM}, with detected
frequencies {k1,k2,...,kK}. Obviously from a noisy observation of the DFT
1
128
257
0
4
8
12
16
M=16
1
128
257
0
16
32
48
64
M=64
1
128
257
0
32
64
96
128
M=128
1
128
257
0
48
96
144
192
M=192
1
128
257
0
56
112
168
224
M=224
1
128
257
0
64
128
192
256
M=257
frequency
signal transform
Figure 10.8
Initial DFT of a signal with various number of available samples M. Available M
samples are a random subset of N samples taken according to the sampling theorem interval.
Dots represent the original signal DFT values, scaled with M/N to match the mean value of
the DFT calculated using a reduced set of signal samples. The DFT values are presented as a
function of the frequency index.
we can distinguish two cases: 1) When the number of available samples is
large and all components are above a threshold that can be calculated based
on (10.49). Then all signal frequencies will be distinguishable as peaks in the
DFT. 2) If the number of available samples is low or there are components
with much lower amplitudes then the largest component is detected and
estimated ﬁrst. It is subtracted from the signal. The next one is detected and

Ljubiša Stankovi´c
Digital Signal Processing
723
the signal is estimated using the frequency from this and the previous step(s).
The estimated two components are subtracted from the original signal. The
frequency of next components is detected, and the process with estimation
and subtraction is continued until the energy is negligible. This iterative
procedure will be the topic of next subsection.
Example 10.9. For a discrete-time signal
x(n) = ej2πk1n/N + 1
2ej2πk2n/N + 1
4ej2πk3n/N,
with N = 64 the DFT is calculated using a random set of M = 16 samples.
Calculation is performed with 105 random realizations with randomly po-
sitioned M samples and random values of k1,k2, and k3. Histogram of the
DFT values, at a noise only position k /∈{k1,k2,k3} and at the signal compo-
nent k = k1 position, is presented in Fig.10.9 (left). Histogram of the initial
DFT real part is shown, along with the corresponding Gaussian functions
N (0, 21
16
N−M
N−1 ) and N (M, 5
16
N−M
N−1 ), shown by dots conﬁrming (10.54). The
same calculation is repeated with M = 64, Fig.10.9(right). Note that the vari-
ance factor 21/16 = 1 + (1/2)2 + (1/4)2 follows from (10.53).
We can see that the mean value of the Gaussian variable X(k) can
be used for the signal component position detection. Also the variance is
different for noise only and the signal component positions. It can also be
used for the signal position detection.
In the case with M = 16 the histograms are close to each other, meaning
that there is a small probability that a signal component is missdetected.
Histograms are well separated in the case with M = 64. It means that the
signal components will be detected with an extremely high probability in
this case. Calculation of the detection probability is straightforward with the
assumed probability density functions.
The spark based relation can be obtained within the framework of
previous analysis if we assume that the noises in (10.48) due to missing
samples, coming from different components of the same (unity) amplitude
Ai, are added up with the same phase to produce
X(k) = ∑
n∈M
K
∑
p=1
e−j2πn(k−kp)/N = K ∑
n∈M
e−j2πn(k−kp)/N
at some frequency k /∈{k1,k2,...,kK}. Random variable ∑n∈M e−j2πn(k−kp)/N
(since n ∈M is random) should also assume its maximal possible value
(calculated over all possible kp and all possible positions k, k ̸= kp). The
maximal possible value of this variable is related to the coherence index

724
Sparse Signal Processing
-5
0
5
10
15
20
0
0.1
0.2
pℜ{X(k)}(ξ)
M=16
Distribution of noise only DFT, ℜ{X(k)}
-5
0
5
10
15
20
0
0.1
0.2
pℜ{X(k
1)}(ξ)
Distribution of signal DFT, ℜ{X(k1)}
M=16
-20
0
20
40
60
80
0
0.05
0.1
0.15
0.2
pℜ{X(k)}(ξ)
M=64
Distribution of noise only DFT, ℜ{X(k)}
-20
0
20
40
60
80
0
0.05
0.1
0.15
0.2
pℜ{X(k
1)}(ξ)
Distribution of signal DFT, ℜ{X(k1)}
M=64
Figure 10.9
Histograms and Gaussian probability density functions for the signal and noise
only positions in the initial DFT for a three-component signal with N = 128 and M = 16 (left)
and M = 64 (right). The histograms are calculated in 105 random realizations of M available
samples and random signal frequency positions.
of the partial DFT matrix as (10.32)
µ = max
''µ(k,kp)
'' = 1
M max
k,kp
''''' ∑
n∈M
e−j2πn(k−kp)/N
'''''.
It means that maximal possible value of this variable is µM. It should
also be assumed that (K −1) remaining noise components (due to missing
samples) at the component position k = kp assume the same maximal value
µM and that all of them are subtracted in phase from the signal mean
value M at k = kp. Condition for the correct detection of a component
position at k = kp is then such that the minimal possible amplitude of the
component M −Mµ(K −1) is greater than the maximal possible noise MµK
at k /∈{k1,k2,...,kK}, i.e.,
M −Mµ(K −1) > MµK

Ljubiša Stankovi´c
Digital Signal Processing
725
or
K < 1
2(1 + 1/µ).
According to several very unlikely assumptions that have been made, we
can state that this is a very pessimistic bound for K. Therefore, for a high
degree of randomness, a probabilistic approach may be more suitable for
the analysis than the spark based relation.
This kind analysis will be repeated on the case of Gaussian real-valued
random matrix. In this case there is no complete set of measurements. This
analysis then can be considered as a reduced set of measurements analysis.
In this case
X =
B
ATA
C−1
ATy
can be again considered as
X =
B
ATA
C−1
X0
where
X0 = ATy
is the initial estimation. It uses available reduced set of M measurements y
to calculate N values of X0. Its value is the same as if a complete transfor-
mation matrix existed and all values of the missing measurements (to com-
plete set of measurements) were considered as zero. If the initial estimation
X0 = ATy can produce correct positions of nonzero values in a K-sparse X
then the solution will be straightforward using only nonzero values of X
denoted by XK and corresponding measurements submatrix AK as
XK =
B
AT
KAK
C−1
AT
Ky.
Assume that the measured signal is
x(n) =
K
∑
i=1
X(ki)ψki(n) =
K
∑
i=1
Aiψki(n)
with the elements of y being x(n) for n ∈M and ki ∈{k1,k2,...,kK}. Then the
elements of X0 = ATy are
X0(k) =
K
∑
i=1
Ai ∑
n∈M
ψk(n)ψki(n).

726
Sparse Signal Processing
Obviously
E{X0(k)} = 0 for k ̸= ki
E{X0(k)} = Ai for k = ki
since E
R
∑n∈M ψ2
k(n)
S = 1. For k ̸= ki
∑
n∈M
ψk(n)ψki(n) ≤µ
by deﬁnition, where µ is the coherence.
In the worst case, assuming ∑n∈M ψ2
k(n) = 1 and Ai = 1 for all ki, the
smallest possible value at k = ki would be obtained if all (K −1) components
assume the lowest possible value −µ
X0(ki) = 1 −(K −1)µ
The signal components should assume this lowest possible case and it
should be greater than the highest possible value at a k ̸= ki
X0(k) = Kµ.
It should hold
1 −(K −1)µ > Kµ
K < 1
2
*
1 + 1
µ
+
.
We can easily see why the coherence index based limit in Example 10.6,
produced very conservative estimate. It calculates the sparsity limit as-
suming that an order of K Gaussian variables ∑n∈M ψk(n)ψki(n) assume,
at the same time, maximal upper limit and that (K −1) variables assume
at the same time lower limit −µ. The eigenvalue based calculation does
not make such an assumption. Therefore it is closer to the expected be-
havior, although it also assumes a speciﬁc, the worst case, signal form.
(Note: Show that any other A1 ≥A2 ≥... ≥AK ≥0 will produce more re-
laxed condition than when all amplitudes are equal A1 −µ(A2 + ... + AK) >
µ(A1 + A2 + ... + AK)).
A realistic and very simpliﬁed probabilistic approach would be based
on:
(1) Variance of K random variables ∑n∈M ψk(n)ψki(n) corresponding
to signal components ki in the worst case is K/M.

Ljubiša Stankovi´c
Digital Signal Processing
727
(2) Variance of N −K random variables ∑n∈M ψk(n)ψki(n) for the
positions ki not corresponding to signal components is (K −1)/M.
(3) Distance between the mean values of signal components and noise
only components is 1.
(4) Probability density function of these two classes of random vari-
ables should be well separated. For example, if √
K/M ≤1/6 then
3
=
(K −1) 1
M + 3
=
K
M < 1
with 3-sigma rule Gaussian distributions guarantees probabilities of 1 −
10−4 order that these distributions will not overlap (any of N −K signal
free samples with at least one of K signal samples). It means
K ≤M
36
For M = 1024 we get
K ≤28
or 2K < 28 for the unique solution, corresponding to Fig.10.5.
10.4.5
Iterative Procedure
If components with very different amplitudes exist and the number of
available samples is not large, then the iterative procedure should be used.
This procedure could be implemented as follows. The largest component is
detected and estimated ﬁrst. It is subtracted from the signal. The next one
is detected and the signal is estimated using the frequency from this and
the previous step(s). The estimated two components are subtracted from
the original signal. The frequency of next components is detected, and the
process of estimations and subtractions is continued until the energy of the
remaining signal is negligible or bellow an expected additive noise level.
Algorithm
(i) Calculate the initial transform estimate ˆX1(k) by using the avail-
able/remaining signal values x1(n) = x(n)
ˆX1(k) = ∑
n∈M
x(n)ϕk(n)
Set the transform values ˆX(k) to zero at all positions k except the
highest one at k = k1,

728
Sparse Signal Processing
ˆK1= {k1}. Set the counter to r = 1.
Form the matrix A1 using the available samples in time n ∈NA and
detected index k ∈ˆK1, with one nonzero component. Calculate the estimate
of the transformation coefﬁcient at k = k1
ˆX1 =
B
AH
1 A1
C−1
AH
1 y.
Calculate the signal estimation (as the inverse DFT)
ˆx1(n) = ˆX1(k1)ψk1(n), for n ∈M
and check
ϵ = ∑n∈M |x(n) −ˆx1(n)|2
∑n∈M |x(n)|2
.
If, for example ϵ < 10−5, stop the calculation and use x(n) = ˆx1(n). If not
then go to the next step.
(ii) Set the counter to r = r + 1. Form a signal
er(n) = x(n) −ˆxr−1(n),
at the available sample positions and calculate the transform
ˆEr(k) = ∑
n∈M
er(n)ϕk(n).
Set the transform values ˆEr(k) to zero at all positions k except the highest
one at k = kr. Form the set of r indices, using union of the previous maxima
positions and the detected position, as
ˆKr= { ˆKr−1,kr}.
Form matrix Ar using the available samples in time n ∈M and detected ˆKr
indices k ∈ˆKr. Calculate the estimate of Kr transformation coefﬁcients
ˆXKr =
B
AH
r Ar
C−1
AH
r y.
Calculate the signal
ˆxr(n) = ∑
ˆKr
i=1 ˆXr(ki)ψki(n), for n ∈M

Ljubiša Stankovi´c
Digital Signal Processing
729
and check
ϵ = ∑n∈M |x(n) −ˆxr(n)|2
∑n∈M |x(n)|2
.
If, for example ϵ < 10−5, stop the calculation and use
x(n) = ˆxr(n).
Else repeat step (ii).
Example 10.10. Signal
x(n) = sin(12π n
N + π
4 ) + 0.7cos(40π n
N + π
3 ) −0.4
with N = 64 is shown in Fig.10.10. Small number of samples is available
M = 16 with different signal amplitudes, making one-step recovery impos-
sible. The available signal samples y(n) are shown in Fig.10.10(second row,
left). The iterative procedure is used and for the detected DFT positions dur-
ing the iterations the recovered signal is calculated according to the presented
algorithm. The recovered DFT values in the rth iteration are denoted as Xr(k)
and presented in Fig.10.10. After ﬁrst iteration the strongest component is
detected and its amplitude estimated. At this stage other components be-
have as noise (this will be analyzed later in this section) and make amplitude
value inaccurate. Accuracy improves as the number of detected components
increases in next iterations. After ﬁve steps the agreement between the re-
constructed signal and the available signal samples was complete. Then the
algorithm is stopped. The DFT of the recovered signal is presented as X5(k)
in the last subplot of Fig.10.10. Its agreement with the DFT of the original
signal, Fig.10.10 (ﬁrst row, right) is complete.
10.4.6
Inﬂuence of Additive Input Noise
Assume now that an input additive noise ε(n) exists in the available signal
samples. Note that the noise due to missing samples inﬂuences the results
in the sense of the possibility to recover the signal. When the recovery is
achieved the result accuracy is related to the input additive noise in signal
samples and the number of available samples as it will be shown next.
The reconstruction equations (10.36) for noisy samples are
x(n) + ε(n) =
K
∑
i=1
X(ki)ψki(n), for n ∈M.
for the detected indices k = {k1,k2,...,kK}. Matrix form of these equations is
y+ε = AKXK.

730
Sparse Signal Processing
0
20
40
60
-2
-1
0
1
2
x(n)
0
20
40
60
-2
-1
0
1
2
y(n)
-20
0
20
0
10
20
30
40
50
X(k)
-20
0
20
0
10
20
30
40
50
X1(k)
-20
0
20
0
10
20
30
40
50
X2(k)
-20
0
20
0
10
20
30
40
50
X3(k)
-20
0
20
0
10
20
30
40
50
X4(k)
-20
0
20
0
10
20
30
40
50
X5(k)
Figure 10.10
Iterative signal recovery
It is a system of M linear equations with K unknowns. The solution is
obtained from
AH
K (y+ε) = AH
K AKXK
XK =
B
AH
K AK
C−1
AH
K (y+ε)
XK = XKS + XKN.
(10.56)

Ljubiša Stankovi´c
Digital Signal Processing
731
Here the true signal transform coefﬁcients are
XKS =
B
AH
K AK
C−1
AH
K y,
and
XKN =
B
AH
K AK
C−1
AH
K ε
is the noise inﬂuence to the reconstructed signal coefﬁcients.
In terms of matrix norms the bound for ratio of noise in the recon-
structed and the original signal follows from
∥XKN∥2 ≤
WWWW
B
AH
K AK
C−1WWWW
2
WWWAH
K
WWW
2 ∥ε∥2
∥XKN∥2
∥ε∥2
≤
√
dmax
dmin
≤
1
1 −δK
,
1 + δK.
The fact that
WWW
P
AH
K AK
Q−1WWW
2 ≤1/dmin ≤1/(1 −δK) is used, where dmin
is the smallest eigenvalue of
WWAH
K AK
WW
2. The norm of
WWAH
K
WW
2 is equal to
FWWAH
K AK
WW
2, meaning
WWAH
K
WW
2 ≤√
dmax ≤√1 + δK, where dmax is the largest
eigenvalue of
WWAH
K AK
WW
2.
For a small noise, a simpliﬁed analysis can be performed. If all signal
samples were available, the input signal-to-noise (SNR) ratio, would be
SNRi = 10log ∑N−1
n=0 |x(n)|2
∑N−1
n=0 |ε(n)|2 = 10log Ex
Eε
.
Assume that the noise energy in the available samples is
EεA = ∑
n∈M
|ε(n)|2 .
(10.57)
The true amplitude in the signal transform at the index kp, in the case if
all signal samples were used, would be NAp, where Ap is the amplitude
of the signal component corresponding to the index kp. To compensate
the resulting transform for the known bias in amplitude when only M
available samples are used the coefﬁcient should be multiplied by N/M.
In a full recovery, a signal transform coefﬁcient is equal to the coefﬁcient
of the original signal with all signal samples being used. The noise in the
transform coefﬁcients are multiplied by the same factor. The energy of noise

732
Sparse Signal Processing
in the reconstruction algorithm is increased to E2
εAN/M2. The SNR in the
recovered signal is
SNR = 10log
∑N−1
n=0 |x(n)|2
N2
M2 ∑n∈M |ε(n)|2 .
(10.58)
Since only K out of N coefﬁcients are used in the reconstruction the
energy of the reconstruction error is reduced for the factor of K/N as well.
The energy of noise in the recovered signal is
EεR = K
N
N2
M2 ∑
n∈M
|ε(n)|2 .
The SNR in the recovered signal is
SNR = 10log
∑N−1
n=0 |x(n)|2
ˆKN
M2 ∑n∈M |ε(n)|2 .
(10.59)
Since the variances in all samples and the available samples are the same
then
1
M ∑
n∈M
|ε(n)|2 = 1
N
N−1
∑
n=0
|ε(n)|2
(10.60)
Thus, the SNR in the recovered signal is
SNR = SNRi −10log
* K
M
+
.
(10.61)
Using the number K in reconstruction as small as possible (in ideal case
equal to the signal sparsity) improves the results when a small additional
input noise exists in all signal samples
Example 10.11. This simple theoretical result is tested on the DFT and signal
x(n) = A1 exp(j2πk1n/N) + A2 exp(j2πk2n/N) + A3 exp(j2πk3n/N)
(10.62)
with A1 = 1, A2 = 0.75, A3 = 0.25, {k1,k2,k3} = {58,117,21}, within 0 ≤n ≤
N −1 = 255, with additive noise ε(n) of variance σ2ε = 1. For a random set
of M available samples the initial DFT is calculated using (10.45). Since a
large number of available samples M is used in these simulations the signal
components {k1,k2,k3} are easily detected in one step. The signal is recovered
by (10.46) for the set of available signal samples y = [x(n1) x(n2) ... x(nM)]T

Ljubiša Stankovi´c
Digital Signal Processing
733
and the detected frequencies are {k1,k2,k3}. The input SNR was SNRi =
2.6383 [dB]. The theoretical result for the output SNR, for example, M =
N/2 = 128 and K = 3, according to (10.61) is SNR = 18.88 [dB]. For statistical
check of the results, 100 random realizations of the available sample positions
are used. The statistical SNR was obtained as SNR = 18.87 [dB]. Its agreement
with the theory is high.
10.4.7
Nonsparse Signal Reconstruction
According to the results in Section 10.4.4 the missing samples can be repre-
sented by a noise inﬂuence. Assume that we use a reconstruction algorithm
for a signal of sparsity K on a signal whose DFT coefﬁcients X are not sparse
(or not sufﬁciently sparse). Denote by XK the sparse signal with K nonzero
coefﬁcients equal to the largest K coefﬁcients of X. Suppose that the number
of components K and the measurements matrix satisfy the reconstruction
conditions so that a reconstruction algorithm can detect (one by one or at
once) largest K components (A1, A2,...,AK) and perform signal reconstruc-
tion to get XR. The remaining N −K components (AK+1,AK+2,...,AN) will
be treated as a noise in these K largest components. Variance from a signal
component is |Ai|2 M(N −M)/(N −1). After reconstruction this variance
is multiplied by (N/M)2, according to the analysis in previous subsection,
producing
|Ai|2 N2
M2
M(N −M)
N −1
∼= |Ai|2 N N −M
M
.
The total energy of noise in the reconstructed K largest components XR will
be
∥XR−XK∥2
2 = KN N −M
M
N
∑
i=K+1
|Ai|2
Denoting the energy of remaining signal, when the K largest are removed
from the original signal, by
∥X −XK∥2
2 = N
N
∑
i=K+1
|Ai|2
we get
∥XR−XK∥2
2 = K N −M
M
∥X −XK∥2
2 .
If the signal is sparse, i.e., X = XK, then
∥XR−XK∥2
2 = 0.

734
Sparse Signal Processing
The same result follows if N = M. The error will be zero if a complete DFT
matrix is used in the calculation of any signal component.
Finally using Schwartz’s inequality for X −XK having N −K nonzero
elements,
∥X −XK∥2 ≤
1
√
N −K ∥X −XK∥1 ,
follows
∥XK−XR∥2 ≤
=
N −M
M
K
N −K ∥X −XK∥1 .
In the case of additive input noise with variance σ2
ε , a general expres-
sion is obtained in the form
∥XR−XK∥2
2 = K N −M
M
∥X −XK∥2
2 + K
M Nσ2
ε .
Example 10.12. Consider a nonsparse signal
x(n) = ej2πk1n/N + 0.8ej2πk2n/N + 0.77ej2πk3n/N + 0.75ej2πk4n/N
+
255
∑
i=5
*1
3
+[1+(i−5)/50]
ej2πkin/N
where ki, i = 1,2,...,255 are random frequency indices from 0 to N −1. Using
N = 257 and M = 192 the ﬁrst K = 4 components of signal are reconstructed.
The remaining 251 signal components are considered as disturbance. Recon-
struction of K = 4 largest components is done in 100 independent realizations
with different frequencies and positions of available samples. The result for
noise-free case is
SNRstat = 10log
(
∥XK∥2
2
∥XR−XK∥2
2
)
= 23.1476
SNRtheor = 10log
(
∥XK∥2
2
K N−M
M
∥X −XK∥2
2
)
= 23.1235.
Note that a closed form expression for ∥X −XK∥2
2 in SNRtheor can be obtained
since we assumed that the amplitudes of disturbing components are coefﬁ-
cients of a geometric series. One realization is presented in Fig.10.11.
In the case of additive complex-valued noise of variance σ2ε = 2 the
results are
SNRstat = 10log
(
∥XK∥2
2
∥XR−XK∥2
2
)
= 17.0593
SNRtheor = 10log
(
∥XK∥2
2
K N−M
M
∥X −XK∥2
2 + K
M Nσ2ε
)
= 17.0384.

Ljubiša Stankovi´c
Digital Signal Processing
735
50
100
150
200
250
0
50
100
150
200
250
300
350
Figure 10.11
Single realization reconstruction of K = 4 largest signal components of a non-
sparse noisy signal.
A decrease in the SNR due to noise is
∆SNRtheor = 10log
(
K N−M
M
∥X −XK∥2
2
K N−M
M
∥X −XK∥2
2 + K
M Nσ2ε
)
= −6.0851.
The simulation is repeated with M = 128 and the same noise. The SNR values
are SNRtheor = 14.3345 and SNRstat = 14.4980.
10.5
NORM-ONE BASED RECONSTRUCTION
A direct way to solve the ℓ0-norm based minimization problem
M0 = ∥X∥0 = card{X} = K,
subject to y = AX is a search over all possible combinations of nonzero
samples, starting from the lowest K = 1, until a smallest set of K nonzero
coefﬁcients satisfying y = AKXK is found. In general, this is not computa-
tionally feasible problem. Indirect methods, based on an initial estimation
of the nonzero positions, are presented in the previous section.
Note that the ℓ0-norm sparsity measure can be considered as a limit
case of the concentration measure
Mp =
N−1
∑
k=0
|X(k)|p = ∥X(k)∥p
p ,

736
Sparse Signal Processing
for p →0. We can expect that the behavior of this measure will not signif-
icantly change if p is slightly increased from 0. This kind of concentration
measure with 0 ≤p ≤1 has been used for decades in optimization of time-
frequency representations, as an alternative to measure based on the ratio
of higher order norms.
In compressive sensing the most commonly used sparsity measure
is the norm with p = 1 since it is the only convex function for p within
the interval 0 ≤p ≤1. Convex form of the measure enables application of
linear programming in the solution of the minimization problem. Thus the
minimization problem formulation with p = 1 is
min∥X∥1
subject to y = AX,
where
M1 = ∥X∥1 =
N−1
∑
k=0
|X(k)|.
Under some conditions minimization of the ℓ1-norm sparsity measure can
produce the same result as the ℓ0-norm minimization. These conditions will
be considered in this section.
Note that norms with p > 1 cannot be used. For p = 2 this measure
is equivalent with the well-known ℓ2-norm used in deﬁnitions of standard
signal transforms. In the standard signal transforms the measure with ℓ2-
norm has a minimum when the missing signal samples/measurements are
set to zero. Parserval’s theorem states that the energy of a signal in the time
domain is the same as the energy of the Fourier transform in the frequency
domain. A signal has the lowest energy when its missing samples are zero-
valued. Associating any nonzero value to the missing samples will increase
the signal energy. The same holds in the frequency domain since the energy
in the frequency domain equals to the energy in the time domain. The
minimization solution with the ℓ2-norm is therefore trivial. With this norm,
we attempt to minimize
∥X∥2
2 =
N−1
∑
k=0
|X(k)|2 .
According to Parseval’s theorem we have ∥X∥2
2 = N ∑N−1
n=0 |x(n)|2. Since
any value other than x(n) = 0 for the unavailable/missing signal samples,
would increase ∥X∥2
2, then the solution for the non-available samples, with
respect to the ℓ2-norm, are all zero values (a proof of this fact has been
presented within the section dealing with pseudoinverse matrix as well).

Ljubiša Stankovi´c
Digital Signal Processing
737
Resulting transform X(k) is then not sparse. It was the reason why this norm
was not used as a concentration measure as well.
Example 10.13. Minimization in a space with two variables x,y will be illustrated
on the cases with p = 1, p = 1/2, p = 1/4 and p = 2 using the condition
y = ax + ˙b. Note that in the case of p = 1 the result of function z = |x| + |y|
minimization subject to y = ax + ˙b is a point with minimal value of z =
|x| + |y| on the line where the surface z = |x| + |y| intersects with the plane
y = ax + ˙b (the plane y = ax + ˙b in x,y,z space is z independent). Constant
values of |x| + |y| are presented by isolines on the ﬁrst subplot of Fig.10.12.
The minimal value of z is the one where projection of y = ax + ˙b on z = 0
touches the isoline of z = |x| + |y|. All points on isolines crossing this line
correspond to larger values of z = |x| + |y| while all isolines corresponding
to lower values of z = |x| + |y| do not have a common point with the plane
y = ax + ˙b. The minimization of z = |x| + |y| with y = ax + ˙b can also be
written as
min(|x| + |y|) = min
P|x| +
''ax + ˙b
''Q
.
Since we have a sum of two piecewise linear functions |x| and |ax + b| its
minimum is either at x = 0 or at ax + b = 0 for |a| < 1 or |a| > 1, respectively.
Therefore the function z = |x| + |ax + b| will have a minimum at one of
these two points. For y = 0.5x + 1 the solution is (0,1) and for y = 3x −3
the solution is (1,0), Fig.10.12. The solution is the same for p = 1, p = 1/2
(when z = |x|1/2 + |ax + b|1/2), and p = 1/4. For p = 2 the solution follows
as a minimum of z = x2 + (ax + b)2. It is (−0.4,0.8) and (0.9,−0.3) for the
considered functions, respectively. This is just a mathematical illustration of
a constraint minimization. Due to its low dimensionality it cannot be deﬁned
within the measurements and sparsity framework (when for sparsity K = 1
at least two measurements are required).
10.5.1
Illustrations in the Signal Domain
Minimization of the sparsity measures will be illustrated on the signal and
its DFT in recovering missing samples.
Example 10.14. For signals x(n) given by:
(a)
x(n) = ej10πn/N + 4
5ej14πn/N,
(b)
x(n) = ej10πn/N + 1
4ej14πn/N + 1
5ej4πn/N,
for 0 ≤n ≤N −1, with N = 64 ﬁnd the DFT and the number of nonzero
coefﬁcients (sparsity) K in the DFT domain. Calculate the measure values
Mp =
N−1
∑
k=0
|X(k)|p

738
Sparse Signal Processing
-2
-1
0
1
2
-2
-1
0
1
2
y=0.5x+1
y=3x-3
|y|+|x|=z
-2
-1
0
1
2
-2
-1
0
1
2
y=0.5x+1
y=3x-3
|y|1/2+|x|1/2=z
-2
-1
0
1
2
-2
-1
0
1
2
y=0.5x+1
y=3x-3
|y|1/4+|x|1/4=z
-2
-1
0
1
2
-2
-1
0
1
2
y=0.5x+1
y=3x-3
y2+x2=z
Figure 10.12
Illustration of constrained minimization with various norms.
for p = 0, p = 1, and p = 2. Could the measure for p = 1 be used for comparing
sparsity of two different signals?
(c) If the signal sample x(2) in signal
x(n) =
!
A1ej10πn/N + A2ej14πn/N,
for
n ̸= 2, 0 ≤n ≤N −1
z
for
n = 2
may assume an arbitrary value x(2) = z, ﬁnd the value of z minimizing
each considered sparsity measure. Comment on this example why M2 = Ex
(energy) based signal measure cannot be used as a signal sparsity measure.
⋆(a) The DFT of signal x(n) is
X(k) =
N−1
∑
n=0
x(n)e−j2πnk/N = Nδ(k −5) + 4
5 Nδ(k −7).

Ljubiša Stankovi´c
Digital Signal Processing
739
The sparsity measures for this signal are
M0 = ∥X∥0 =
N−1
∑
k=0
|X(k)|0 = 2
M1 = ∥X∥1 =
N−1
∑
k=0
|X(k)|1 = N(1 + 4
5) = 9N
5 .
The measure for p = 2 is equal to the signal energy. Its value is
M2 =
N−1
∑
k=0
|X(k)|2 = N2(1 + 16
25) = 41N2
25 .
(b) For this signal x(n) the DFT is
X(k) =
N−1
∑
n=0
x(n)e−j2πnk/N = Nδ(k −5) + 1
4 Nδ(k −7) + 1
5 Nδ(k −2).
The measures are
M0 = ∥X∥0 =
N−1
∑
k=0
|X(k)|0 = 3
M1 = ∥X∥1 =
N−1
∑
k=0
|X(k)|1 = N(1 + 1
4 + 1
5) = 29N
20 .
The signal energy is
M2 =
N−1
∑
k=0
|X(k)|2 = N2(1 + 1
16 + 1
25) = 441N2
400 .
We can see that M0 counts the number of nonzero coefﬁcients. The
measure M1 cannot be used to compare sparsity of different signals since its
value in the second case is lower than in the ﬁrst case.
(c) For an arbitrary x(2) = z we can write
X(k) =
N−1
∑
n=0
@
A1ej10πn/N + A2ej14πn/NA
e−j2πnk/N
+
@
z −A1ej10π2/N −A2ej14π2/NA
e−j2π2k/N
= A1Nδ(k −5) + A2Nδ(k −7) + Z0(k),
(10.63)
with
Z0(k) =
@
z −A1ej10π2/N −A2ej14π2/NA
e−j2π2k/N = z0e−j2π2k/N.

740
Sparse Signal Processing
It is obvious that
M0 =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
N
for
Z0(k) ̸= 0 and Z0(5) ̸= −A1N and Z0(7) ̸= −A2N
N −1
for
Z0(k) ̸= 0 and (Z0(5) = −A1N or Z0(7) = −A2N)
N −2
for
Z0(k) ̸= 0 and (Z0(5) = −A1N and Z0(7) = −A2N)
2
for
Z0(k) = 0, i.e., for z = A1ej10π2/N + A2ej14π2/N.
Minimal value of M0 is achieved for |Z0(k)| = 0 when
z = x(2) = A1ej10π2/N + A2ej14π2/N.
Therefore the ℓ0-norm based measure minimization recovers missing
signal sample in such a way to complete the form of a signal with smallest
number of complex sinusoids, producing the minimal count of nonzero DFT
coefﬁcients. The smallest value of N when measure M0 can be used to
produce z = x(2) must satisfy N −2 > 2. Then the value of M0 in the fourth
line will be smaller than the value of M0 in the third line. In this case it means
N ≥5.
For the ℓ1-norm based sparsity measure, from
X(k) = A1Nδ(k −5) + A2Nδ(k −7) + Z0(k),
follows
M1 = |A1N + Z0(5)| + |A2N + Z0(7)| +
N−2
∑
k=0
k̸=5,k̸=7
|Z0(k)|
=
'''A1N + z0e−j2π10/N''' +
'''A2N + z0e−j2π14/N''' + (N −2)|z0|
We know that in the case of correct solution for |z0| = 0
M1 = |A1| N + |A2| N.
Therefore in any other case when |z0| ̸= 0
'''A1N + z0e−j2π10/N''' +
'''A2N + z0e−j2π14/N''' + (N −2)|z0| > |A1| N + |A2| N
(10.64)
should hold, including the case when the phases of A1N and z0e−j2π10/N and
the phases of A2N and z0e−j2π14/N are opposite. This is the worst case since
the terms
'''A1N + z0e−j2π10/N''' and
'''A2N + z0e−j2π14/N''', corresponding to
signal components are minimal. In the worst case
|A1N| −
'''z0e−j2π10/N''' + |A2N| −
'''z0e−j2π14/N''' + (N−2)|z0| > |A1| N + |A2| N
should also hold. It reduces to
(N −4)|z0| > 0

Ljubiša Stankovi´c
Digital Signal Processing
741
for any |z0| ̸= 0. 2The minimization result |z0| = 0 is the same as in the ℓ0-
norm based measure if N ≥5. The minimal requirement for this reconstruc-
tion is N = 5. The number of available samples is M = 4 and the signal spar-
sity is K = 2.
Note that the condition for ℓ0-norm to fail for N = 4 was Z0(5) =
z0e−j2π10/N = −A1N and Z0(7) = z0e−j2π14/N = −A2N. It means that A1 =
A2e−j2π4/N should hold. In the ℓ1-norm the phases of A1N and z0e−j2π10/N
and the phases of A2N and z0e−j2π14/N should only be opposite, in the worst
case. The condition for the ℓ0-norm to fail is just a special case of the ℓ1-norm
condition with |A1| = |A2| = |z0|/N. If the condition for the ℓ0-norm to fail
is satisﬁed then the condition for the ℓ1-norm to fail is satisﬁed as well. This
conclusion, drown from a very speciﬁc example, will be generalized later.
For the energy
M2 = NEx =
N−1
∑
k=0
|X(k)|2 = N
N−1
∑
n=0
|x(n)|2 = N(
N−1
∑
n=0
n̸=2
|x(n)|2 + |x(2)|2).
Since the value of ∑N−1
n=0,n̸=2 |x(n)|2 is constant (the available samples are exact
and that they should not be changed) then the value of M2 is minimal if
|x(2)| = |z| = 0.
Therefore in the ℓ2-norm (or energy) based minimization the missing sample
will be set in such a way to produce the minimal energy. That is zero value
of the missing samples/measurements. The reconstructed DFT using M2
minimization is
X(k) = A1Nδ(k−5) + A2Nδ(k−7) +
@
−A1ej10π2/N −A2ej14π2/NA
e−j2π2k/N.
In general X(k) ̸= 0 for all k, (10.63).
The simplest illustrative reconstruction can be performed based on a
direct search over all unavailable/missing sample values, by minimizing
the sparsity measure. This method is not computationally feasible except
for very small number of missing samples. We will use it just two illustrate
the sparsity measures behavior.
Consider a complete set of signal samples {x(1),x(2),...,x(N −1)}. If
M of them y= [x(n1), x(n2), ...,x(nM)] are available, the missing N −M
samples are yc=[x(nM+1), x(nM+2), ...,x(nN)] with x= y ∪yc. The values
2
Note that in any other than the worst case the condition would be (N −4)|z0| + α > 0
where
''A1N + z0e−j2π10/N'' +
''A2N + z0e−j2π14/N'' = |A1N| −
''z0e−j2π10/N'' + |A2N| −
''z0e−j2π14/N'' + α, where α is a positive constant. This condition is less restrictive for |z0|
than the worst case when α = 0.

742
Sparse Signal Processing
of missing samples are the solution of the sparsity measure minimization
min
yc
N−1
∑
k=0
|X(k)|p subject to y= AX
Since this is a direct search method then any valid sparsity measure can
be used. From the available samples we can estimate the range limits for
the missing samples A. For example, A = max|x(ni)|, i = 1,2,..., M. In the
direct search approach we can vary each missing sample value from −A to
A with a step ∆x = 2A/(L −1), where L is the number of considered values
within the selected range. It is obvious that the reconstruction error in each
sample is limited by the step 2A/(L −1) used in the direct search. Number
of the analyzed values for N −M coefﬁcients (variables) is L(N−M). For any
reasonable accuracy the value of L is large and the number of calculations
L(N−M) is extremely large. One possible approach to reduce the number
of calculations in the direct search is to use a large step (small L) for the
ﬁrst (rough) estimation, then to reduce the step around the rough estimate
of unavailable/missing values x(nM+1), x(nM+2),..., x(nN). This procedure
can be repeated several times, until the desired accuracy is achieved.
Example 10.15. Consider a discrete signal
x(n) = cos(2πn/N) + 0.5sin(8πn/N) + 0.4cos(30πn/N + π/3) −0.8
(10.65)
for n = 0,1,..., N −1, and N = 256 is the number of signal samples. The case
of two missing samples x(nN−1) and x(nN) is presented. The direct search
is performed over a wide range [−3,3] with a step of 0.01. Sparsity measure
Mp is calculated for p = 0, p = 1/2, p = 1, and p = 2. Results for Mp/N
are shown in Fig. 10.13. The measure minimum is located on the true sample
values for p ≤1 (norms ℓ1 and lower). The measure minimum for p > 1 (ℓ2
norm, for p = 2) is not located at the true signal values, as expected.
Note that p ≤1 produces accurate position of the sparsity measure at
the missing sample positions. For ℓ0-norm the value of measure is constant
and equal to N everywhere, except at the exact values of the missing samples.
For p = 2 the measure with ℓ2-norm has a minimum when the missing signal
samples are set to zero, which is not the solution of this problem.
10.5.2
Illustration in the Sparsity Domain
In general we have a reduced set of M measurements/samples of a sparse
signal with N samples y = AX. In the minimization process the values of
X(k) can be considered as variables. One measurement/sample deﬁned by
ψ0(ni)X(0) + ψ1(ni)X(1) + ... + ψN−1(ni)X(N −1) = y(i)

Ljubiša Stankovi´c
Digital Signal Processing
743
-3
-2
-1
0
1
-3
-2
-1
0
1
2
2.5
3
3.5
4
4.5
5
x(nN-1 )
x(nN)
-3
-2
-1
0
1
-3
-2
-1
0
1
2
2.5
3
3.5
4
4.5
5
x(nN-1 )
x(nN)
-3
-2
-1
0
1
-3
-2
-1
0
1
2
2.5
3
3.5
4
4.5
5
x(nN-1 )
x(nN)
-3
-2
-1
0
1
-3
-2
-1
0
1
2
2.5
3
3.5
4
4.5
5
x(nN-1 )
x(nN)
Figure 10.13
Measure as a function of two missing sample values yc(0) = x(nN−1) and
yc(1) = x(nN) corresponding to various norms. True values of missing samples are presented
with lines. For the presentation all measures are normalized to the interval from 2.5 to 4.9.
represents an N−dimensional hyperplane with variables X(k) and con-
stants ψk(ni) and y(i). For graphical illustrations we will assume real-valued
functions and signals.
Consider a signal in the transformation domain X(k) with three pos-
sible values, N = 3 and k = 0,1,2. Assume that X(k) is sparse with sparsity

744
Sparse Signal Processing
K = 1. It means that only one coefﬁcient X(k) is nonzero. Using measure-
ments with functions ψk(ni), k = 0, 1,2, and ni ∈{0,1,2} we get a plane in
three-dimensional space
ψ0(ni)X(0) + ψ1(ni)X(1) + ψ2(ni)X(2) = y(i).
The solution will be illustrated in the space of variables X(0), X(1), and
X(2).
Consider one measurement denoted by
ψ0(0)X(0) + ψ1(0)X(1) + ψ2(0)X(2) = y(0).
(10.66)
To simplify notation it has been assumed that ni = i. This is a plane in the
three-dimensional space of variables X(0), X(1), and X(2). Since the signal
is sparse with K = 1 it means that only one X(k) value is nonzero. The
solution of problem is an intersection of plane (10.66) with a coordinate axis.
In general plane (10.66) has three intersections with coordinate axes. Thus
there are three possible solutions. The problem cannot be solved using only
one measurement, Fig.10.14(a).
If we add one more measurement then two planes of variables X(0),
X(1), and X(2) are obtained
ψ0(0)X(0) + ψ1(0)X(1) + ψ2(0)X(2) = y(0)
(10.67)
ψ0(1)X(0) + ψ1(1)X(1) + ψ2(1)X(2) = y(1).
In general, these two planes intersect along a line in the three-dimensional
space. If there is only one common intersection point with coordinate axes,
for both planes, then it is the solution of our problem, Fig.10.14(b).
However, in special cases two planes (measurements/samples) may
not be sufﬁcient to get a unique solution:
-First special case is when two planes (10.67) intersect along a line
passing through two of the possible solutions (intersecting with two coor-
dinate axes at the common points). It means that the intersection line lies
in one of the coordinate planes, Fig.10.14(c). Then these two planes are not
sufﬁcient to ﬁnd a unique solution.
-Second special case is when the planes intersect along the axis, con-
taining the solution. In that case the origin (with trivial solution with spar-
sity 0) will be a possible (undesired) solution as well.
-Finally if planes (10.67) coincide then all coefﬁcients of one plane are
just scaled versions of the coefﬁcients in the other equation, Fig.10.14(d).
In this case the second measurement does not introduce any additional
information with respect to the already existing measurement.

Ljubiša Stankovi´c
Digital Signal Processing
745
Figure 10.14
Illustration of solution for N = 3 and K = 1 for various possible cases.
Consider the direction vector p of the measurements line deﬁned by
the system of planes (10.67). It is normal to vectors of the planes deﬁned
by (ψ0(0),ψ1(0),ψ2(0)) and (ψ0(1),ψ1(1),ψ2(1)). Vector p coordinates are
equal to the vector product
p =
''''''
iX(0)
iX(1)
iX(2)
ψ0(0)
ψ1(0)
ψ2(0)
ψ0(1)
ψ1(1)
ψ2(1)
''''''
,
where iX(k) are unity vectors along coordinate axes representing X(k). For
sparsity K = 1 the solution is unique if the measurements line is not within

746
Sparse Signal Processing
any of coordinate planes. The components of vector p
pX(0) = ψ1(0)ψ2(1) −ψ2(0)ψ1(1)
pX(1) = ψ0(0)ψ2(1) −ψ2(0)ψ0(1)
pX(2) = ψ0(0)ψ1(1) −ψ1(0)ψ0(1)
must be such that
min
M'''pX(0)
''',
'''pX(1)
''',
'''pX(2)
'''
N
> 0.
Then the measurements line of system will (10.67) not lie in one of the
coordinate planes, meaning that the solution is unique, Fig.10.14. Note that
the values of vector p components are equal to the determinants of the
system presented and discussed in the ﬁrst illustrative example, (10.2).
In the ℓ0-norm based minimization, the task is to solve
min∥X∥0 =
N−1
∑
k=0
|X(k)|0 subject to y = AX
Therefore in the original N dimensional space the solution is on the inter-
section of the maximal possible number of N−dimensional hyperplanes of
the form X(kK+1) = 0, X(kK+2) = 0,..., X(kN) = 0 satisfying all available
equations/conditions
ψl1(ni)X1 + ψl2(ni)X2 + ... + ψlK(ni)XK = y(i),
i = 1,2,,..., M.
When the number of zero values of X(k) is maximal then the number of its
nonzero values (the sparsity) is minimal.
Example 10.16. Find the minimal sparsity solution for measurements
0.3617X(0) −0.4942X(1) + 0.3611X(2) = −0.4550
−0.2991X(0) −0.4967X(1) + 0.4052X(2) = −0.5105
using combinatorial approach and ℓ0 sparsity measure.
⋆Start with possible sparsity K = 1. Then we ﬁnd solutions of these
equations with all possible combinations with one nonzero coefﬁcient: {X(0),
X(1) = 0, X(2) = 0}, {X(0) = 0, X(1), X(2) = 0}, and {X(0) = 0, X(1) = 0,
X(2)}. For each of these combinations we get a solution of the ﬁrst and the
second equation. The solution which is the same for the ﬁrst and second
equation is {X(0) = 0, X(1) = 0, X(2) = −1.2600}. It is the solution of the
problem. Signal is of sparsity card{X(k)} = 1.

Ljubiša Stankovi´c
Digital Signal Processing
747
In general, direct optimization using the ℓ0-norm is combinatorial NP-
hard problem. For a signal with N samples, whose sparsity is K, the number
of combinations is (N
K). This is the reason why convex formulation of the
problem is done using the ℓ1-norm
min∥X∥1 =
N−1
∑
k=0
|X(k)|
subject to y = AX.
The problem is solved by minimizing
z = ∥X∥1 = |X(0)| + |X(1)| + |X(2)| + ... + |X(N −1)|
subject to the set of given hyper-planes (measurements)
ψl1(ni)X1 + ψl2(ni)X2 + ... + ψlK(ni)XK = y(i),
for i = 1,2,..., M.
For the graphical illustration we use the three-dimensional signal with
transformation coefﬁcients X(k), k = 0, 1, 2. We will also assume that the
sparsity is K = 1 and that M = 2 measurements/samples are available. In
this case we minimize
z = |X(0)| + |X(1)| + |X(2)|
subject to two available measurements (assuming ni = i)
ψ0(0)X(0) + ψ1(0)X(1) + ψ2(0)X(2) = y(0)
(10.68)
ψ0(1)X(0) + ψ1(1)X(1) + ψ2(1)X(2) = y(1).
The available measurements in this case represent a line (measurements
line) in the considered space. The solution is obtained by ﬁnding the mini-
mal value of z when the three-dimensional ℓ1-norm "ball"
z = |X(0)| + |X(1)| + |X(2)|
has a common point with line (10.68), Fig.10.15 (left). Since the sparsity
K = 1 is assumed, intersection of the measurements line is at the corner
of the ℓ1-norm "ball". Considering the values of minimization function z =
|X(0)| + |X(1)| + |X(2)| along the line (10.68) its minimum will be achieved
at the corner, which is a sparse solution of the problem. It is important to
note that, in this case, the solution is the same as if we used minimization

748
Sparse Signal Processing
Figure 10.15
Illustration of solution with norm-one and norm-1/4 (close to norm-zero) for a
three dimensional case. In lower graphics a view from the direction where the measurements
line and norm-1/4 ball are touching is presented.
of the norm close to the ℓ0-norm, for example, z = |X(0)|1/4 + |X(1)|1/4 +
|X(2)|1/4, presented in Fig.10.15 (right).
The ℓ0-norm and ℓ1-norm based minimizations can produce the same
result. Illustration of the conditions that have to be satisﬁed by measure-
ments line to get the same result with these two norms will be presented on
the three-dimensional case. For the illustration assume that the solution of
the problem is X(0) = z0 ̸= 0. In addition to the condition required by the
ℓ0-norm that the measurements line does not lie within the planes X(1) = 0

Ljubiša Stankovi´c
Digital Signal Processing
749
or X(2) = 0,
|ψ0(0)ψ2(1) −ψ2(0)ψ0(1)| > 0 and |ψ0(0)ψ1(1) −ψ1(0)ψ0(1)| > 0
the measurements line in the ℓ1-norm case should not have such a direction
to intersect with (go thought) the ℓ1-norm "ball" |X(0)| + |X(1)| + |X(3)| =
z0. Therefore, in the worst case the measurements line should intersect the
plane X(0) = 0 just outside the thick line |X(1)| + |X(2)| = z0. If a part
of line is in the ﬁrst octant then it means that it should pass above the
line |X(1)| + |X(2)| = z0, Fig.10.16. Several possible measurements lines are
presented in Fig.10.16 (top-left). Their intersections with X(0) = 0 plane are
denoted by numbers from 1 to 7. For the measurements lines presented by
2, 3 or 4, the ℓ1-norm minimization will produce the correct result for X(k).
It is (X(0),0,0). Line 1 is the critical case when z = |X(0)| + |X(1)| + |X(3)|
is constant along whole line within the ﬁrst octant (any value within this
interval can be the minimization solution). Value of z = |X(0)| + |X(1)| +
|X(3)| will not be minimal at (X(0),0,0) for lines 5, 6 and 7. The ℓ1-norm
function assumes lower values along these lines than at (X(0),0,0) point, as
the line penetrate into the ℓ1-norm "ball".
A uniﬁed condition for all possible nonzero values of X(k), is that the
direction of the measurements line has such direction vectors pX(0), pX(1),
and pX(2) that its minimal coordinate along any of axes X(k) is such that it
passes above the minimization ℓ1-norm "ball". It means
'''pX(0)
''' +
'''pX(1)
''' +
'''pX(2)
''' −max{
'''pX(0)
''',
'''pX(1)
''',
'''pX(2)
'''}
max{
'''pX(0)
''',
'''pX(1)
''',
'''pX(2)
'''}
> 1.
For example, for
'''pX(0)
''' = max{
'''pX(0)
''',
'''pX(1)
''',
'''pX(2)
'''} we get
'''pX(1)
'''
'''pX(0)
'''
+
'''pX(2)
'''
'''pX(0)
'''
> 1, and
'''pX(1)
''' ̸= 0,
'''pX(2)
''' ̸= 0.
If this relation is satisﬁed for the worst case, it means that it holds for other
directions as well. Then the line should pass trough X(0) = 0 outside the
region indicated by thick lines in Fig.10.16. It includes lines 2,3, and 4. The
imposed condition is still very close to line 1. If the measurements line is
close to line 1 it would be sensitive to even a small noise.

750
Sparse Signal Processing
In the case of the ℓ1/2-norm based measure
z = |X(0)|1/2 + |X(1)|1/2 + |X(3)|1/2
minimization we can see from illustration in Fig.10.16 that some measure-
ments which did not produce the correct result with ℓ1-norm will be able to
reconstruct the original sparse signal values (line 5). However lines 6 and 7
will not produce the correct sparse result even with the ℓ1/2-norm. In this
case the line direction will be considered with respect to the thick line in
Fig.10.16 (middle) deﬁned by
[
\
\
\
]
'''pX(1)
'''
'''pX(0)
'''
+
[
\
\
\
]
'''pX(2)
'''
'''pX(0)
'''
> 1.
The same analysis with direction lines is repeated with a function
being closer to ℓ0-norm
z = |X(0)|1/5 + |X(1)|1/5 + |X(3)|1/5 .
For a norm close to the ℓ0-norm the condition reduces to the discussed case
when all direction coordinates should be slightly greater than zero
5
[
\
\
\
]
'''pX(1)
'''
'''pX(0)
'''
+
5
[
\
\
\
]
'''pX(2)
'''
'''pX(0)
'''
> 1.
In this case all measurements corresponding to lines 1-6 will produce correct
result. The measurement 7 is the only one which will not produce the correct
sparse solution, Fig.10.16 (bottom).
An ideal measurements line would correspond to the case when full
isometry is preserved, i.e., when
1 −δ2 ≤∥A2X2∥2
2
∥X2∥2
2
≤1 + δ2
with δ2 = 0. It has been assumed that columns of A are normalized with
EA = 1. Then
dmax = max
%
eig∥A2X2∥2
2
∥X2∥2
2
;
= dmin = min
%
eig∥A2X2∥2
2
∥X2∥2
2
;
= 1.

Ljubiša Stankovi´c
Digital Signal Processing
751
X(1)
 4
 2
 3
 1
p = 1
 5
 6
X(2)
 7
X(0)
X(1)
 4
 2
 3
 1
p = 0.5
 5
 6
X(2)
 7
X(0)
X(1)
 4
 2
 3
 1
p = 0.2
 5
 6
X(2)
 7
X(0)
X(1)
p = 1
X(2)
X(0)
X(1)
p = 0.5
X(2)
X(0)
X(1)
p = 0.2
X(2)
X(0)
Figure 10.16
Minimization function |X(0)| + |X(1)| + |X(3)| = z0 in the ﬁrst coordinate
system octant (X(0),X(1),X(3) > 0) thick lines.
A dot at (0,1,1) surrounded with a gray
rectangular region belongs to the ideal measurement line.
It means that all eigenvalues of AT
2 A2, for any combination of two columns,
are 1 ≤di ≤1. All eigenvalues of A2 are then ±√di = ±1. Since the determi-
nant of A2 (direction vector coordinates for three-dimensional case) is equal
to the product of eigenvalues of A2, it means pX(i) = ±1 or
'''pX(i)
''' = 1. The
ideal case, corresponding to (0,1,1) point in the ﬁrst octant, is presented by
the dot in Fig.10.16 and will be used as a reference in examples.

752
Sparse Signal Processing
Figure 10.17
Minimization using the l1-norm with the solution illustration for the case when
the measurements line crosses through the l1-norm "ball".
Example 10.17. The previous relations are tested on K = 1 sparse signal with N = 3
possible values of X(k) using two measurements with random Gaussian
coefﬁcients ψk(n) = N (0,1/2). Reconstruction mean square error for each
of 1000 realizations, classiﬁed using the measurements line directions, is
presented.
-In 791 random realizations we had the case that the measurements
line direction is outside the ℓ1-norm "ball". The error in reconstruction using
the ℓ1-norm minimization for the measurements line directions outside the
ℓ1-norm "ball" is shown in Fig.10.18 (top). We see that for all cases with the
measurements line directions outside the ℓ1-norm "ball" the reconstruction is
successful, with a small (computer precision) error.

Ljubiša Stankovi´c
Digital Signal Processing
753
0
100
200
300
400
500
600
700
10
-30
10
-20
10
-10
10
0
Reconstruction square error with l1  and directions outside l1 "ball"
0
50
100
150
200
10
-30
10
-20
10
-10
10
0
Reconstruction square error with l1 and directions through l1 "ball"
0
50
100
150
200
10
-30
10
-20
10
-10
10
0
Reconstruction square error with l1/2 and directions through l1 "ball"
0
50
100
150
200
10
-30
10
-20
10
-10
10
0
Reconstruction square error with l1/4 and directions through l1 "ball"
Figure 10.18
Reconstruction square error in 1000 realizations, classiﬁed using the data line
direction: Error using l1 minimization for directions outside the l1 "ball" (top). Error using l1
minimization for directions through the l1 "ball" (second). Error using l1/2 minimization for
directions through the l1 "ball" (third). Error using l1/4 minimization for directions through the
l1 "ball" (bottom).

754
Sparse Signal Processing
-In 209 random realization we had the case that the measurements line
direction is crossing the ℓ1-norm "ball", Fig, 10.17. In all these cases the ℓ1-
norm based reconstruction was not successful. Error using ℓ1 minimization
for directions through the ℓ1-norm "ball" is presented in Fig.10.18 (second).
-All 209 random realizations (when the measurements line direction
is crossing the ℓ1-norm "ball") are also considered by using the l1/2-norm
minimization. Then many of the measurements lines crossing the ℓ1-norm
"ball" will not be crossing the ℓ1/2-norm "ball". Recovery results for the
directions crossing the ℓ1-norm "ball" by using the ℓ1/2-norm minimization
are presented in Fig.10.18 (third). As expected many full recovery realizations
are achieved.
-Finally all 209 random realizations when the measurements line di-
rection is crossing the ℓ1-norm "ball" are considered by using the ℓ1/4-norm
minimization. Error using the ℓ1/4 minimization for directions through the
ℓ1-norm "ball" is given in Fig.10.18 (bottom). All cases are successfully re-
covered since the ℓ1/4-norm is close to the ℓ0-norm. It would fail in a low
probable case when the measurements line would pass trough (or would be
very close to) one of the coordinate planes.
-Two speciﬁc examples of measurements (illustrating the reconstruc-
tion calculation) with directions inside and outside "ball" will be given in
detail next. For the direction outside the ℓ1-norm "ball" the measurement
1.3681X(0) −1.1171X(1) −1.9446X(2) = 2.4502
−0.3370X(0) −1.2624X(1) −0.0207X(2) = 0.0261
is considered. Minimization is done by expressing X(0) and X(1) from
the measurement equations in terms of X(2) and then by minimizing z =
|X(0)| + |X(1)| + |X(2)|. It is now a function of one variable X(2) only.
Minimization is done in a numeric way. The result is X(0) = 0, X(1) = 0,
and X(2) = −1.2600.
An example of a measurement that produces a direction through the
ℓ1-norm "ball" is
0.3617X(0) −0.4942X(1) + 0.3611X(2) = −0.4550
−0.2991X(0) −0.4967X(1) + 0.4052X(2) = −0.5105.
Minimization of z = |X(0)| + |X(1)| + |X(2)| produces X(0) = 0.0802, X(1) =
0.9800, and X(2) = 0.0007. If the measure z = |X(0)|1/2 + |X(1)|1/2 + |X(2)|1/2
is used in the minimization (in a numeric way) it will produce the correct
result (the same X(k) as in the previous measurement), when the measure-
ments line was outside the ℓ1-norm ball.

Ljubiša Stankovi´c
Digital Signal Processing
755
In some applications a random Gaussian measurement matrix
A =
⎡
⎢⎢⎣
ψ0(n1)
ψ1(n1)
ψN−1(n1)
ψ0(n2)
ψ1(n2)
ψN−1(n2)
...
...
...
ψ0(nM)
ψ1(nM)
ψN−1(nM)
⎤
⎥⎥⎦
is normalized so that the energy of each column is ∥ψi∥2
2 = 1. In that case
randomness is reduced and ψi(nm) can be considered as coordinates of an
M-dimensional vector ψi whose ending points are on the M-dimensional
unity sphere. This condition can change behavior of the measurement ma-
trix.
Example 10.18. For the normal set of measurement coefﬁcients (when the column
energies are normalized)
ψ2
k(0) + ψ2
k(1) = 1, k = 0,1,2
the transformation can be written as
∥A2X∥2
2 = |ψi(0)X(i) + ψk(0)X(k)|2 + |ψi(1)X(i) + ψk(1)X(k)|2
=
B
|ψi(0)|2 + |ψi(1)|2C
|X(i)|2 +
B
|ψk(0)|2 + |ψk(1)|2C
|X(k)|2
+2[ψi(0)ψk(0) + ψi(1)ψk(1)] X(i)X(k)
and
∥A2X∥2
2 −∥X∥2
2
∥X∥2
2
= 2[ψi(0)ψk(0) + ψi(1)ψk(1)] X(i)X(k)
∥X∥2
2
≤ψi(0)ψk(0) + ψi(1)ψk(1) = µ(i,k)
since 2X(i)X(k)/∥X∥2
2 ≤1, as shown in (10.23). For the normal set of coefﬁ-
cients we have
µ2(i,k) + p2
X(l) = 1
since
[ψi(0)ψk(0) + ψi(1)ψk(1)]2 + [ψi(0)ψk(1) −ψk(0)ψi(1)]2
ψ2
i (0)
@
ψ2
k(0) + ψ2
k(1)
A
+ ψ2
i (1)
@
ψ2
k(0) + ψ2
k(1)
A
= 1
for i ̸= k ̸= l and i,k,l ∈{0,1,2}. Therefore the condition
δ2 = max{|µ(i,k)|} < 1
is equivalent to
min
M'''pX(0)
''',
'''pX(1)
''',
'''pX(2)
'''
N
> 0.

756
Sparse Signal Processing
In this case, it can be shown that
'''pX(i)
''' +
'''pX(k)
''' ≥
'''pX(l)
'''
for any i,k,l. It means that the normalized matrix (for the three-dimensional
case) will always satisfy the condition that the ℓ1-norm and the ℓ0-norm
solutions are the same (measurements lines are always outside the ℓ1-norm
"ball").
10.5.3
Equivalence of the Norm-Zero and Norm-One Based Minimiza-
tion Solutions
Consider an N-dimensional vector X whose sparsity is K and its M mea-
surements y = AX. The measurements matrix A is an M × N matrix, with
K < M ≤N. A reconstruction of vector X can be achieved from a reduced set
of samples/measurements using the sparsity measures minimization. The
ℓ0-norm based solution of sparsity measure minimization
min∥X∥0
subject to y = AX
recovers K sparse vector X from a reduced set of measurements if the
measurements matrix A satisﬁes the restricted isometry property for a 2K
sparse vector
1 −δ2K ≤
1
EA ∥A2KX2K∥2
2
∥X2K∥2
2
≤1 + δ2K
with
0 ≤δ2K < 1.
The ℓ1-norm based minimization
min∥X∥1
subject to y = AX
(10.69)
produces the same result as the ℓ0-norm based minimization if the restricted
isometry property is satisﬁed with the constant
0 ≤δ2K <
√
2 −1.
Note that other possible upper bounds on the isometry constant have been
derived in literature. Illustration of the reason why the restricted isometry
condition has to be more strict in the ℓ1-norm based minimization than in
the ℓ0-norm is presented in the previous section. Proof is outside of the
mathematical tools used in this book.

Ljubiša Stankovi´c
Digital Signal Processing
757
If the signal X is not sparse then the solution of minimization problem
(10.69) denoted by XR will satisfy
∥XR−X∥2 ≤C0
∥XK−X∥1
√
K
(10.70)
where XK is K sparse signal whose nonzero values are equal to K largest
values of X. If the signal X is of sparsity K then ∥XK−X∥2 = 0 and XR= X.
Note that according to Schwartz’s inequality ∥XK−X∥1
√
K
≤∥XK−X∥2.
Example 10.19. Consider a signal with coefﬁcients X = [X0 a b] where |b| < |a| <
X0. Consider M = 2 measurements with idealized measurements line when
δ2K = 0 (in real cases δ2K can be small but nor zero), deﬁned by
X(0) −X0
−1
= X(1) −a
1
= X(2) −b
1
= t
Find the result of minimization problem (10.69) as a function of a and b.
⋆Replacing X(0) = X0 −t, X(1) = a + t and X(2) = b + t, where t is
the line parameter, we get the value of minimization function z = ∥X∥1 along
the measurements line in the form
z = |X0 −t| + |a + t| + |b + t|.
Minimum of this function is at
t0 = median{X0,−a,−b}
since the function z increases both right and left from t0. It increases with
rate 1 until the ﬁrst of X0,−a,−b is reached left and right, and then increases
toward +∞as t tends toward ±∞. More details about median based mini-
mization will be given in the next subsection.
Illustration is presented in Fig.10.19 with
X0 = 2/3, a = 2/9, and b = −1/9
when
t0 = median{2/3,−2/9,1/9} = 1/9 = −b
with
XR(0) = X0 + b, XR(1) = a −b and XR(2) = 0
XR(0) = 5/9, XR(1) = 1/3 and XR(2) = 0
It means that the solution is a signal XR with reduced sparsity as compared
to the original X. The data line will touch the ℓ1-norm "ball" somewhere on
the edges. Since all edges are within the coordinate planes, it means that

758
Sparse Signal Processing
minimization (10.69) will reduce the sparsity to at least K = 2 for any X = [X0
a b]. We can see that value X0 = 2/3 = 6/9 is reduced to XR(0) = 5/9 and
XR(1) = 1/3 = 3/9, while XR(2) = 0.
The absolute error in X(0) and X(1) is equal to b. Energy of error is
proportional to the energy of the reduced coordinates
∥XR−X∥2 =
F
(X0 −(X0 + b))2 + (a −(a −b))2 + (b −0)2
= |b|
√
3 ≤C0
∥XK−X∥1
√
K
= C0
|X0 + b −X0| + |a −(a −b)| + |0 −0|
√
2
= C02 |b|
√
2
.
The equality holds here with C0 = √
3/2. For a = 0 and b = 0 the solution is
X = [X0 0 0] as expected.
In the case of noisy measurements when
∥y −AX∥2 ≤ϵ
then
∥XR−X∥2 ≤C0
∥XK−X∥1
√
K
+ C1ϵ
where C0 and C1 are constants depending on δ2K.
Example 10.20. For Examples 10.3 and 10.4 estimate the maximal signal sparsity
when the solutions using the ℓ1-norm based minimization and the ℓ0-norm
based minimization are the same.
⋆The restricted isometry property is satisﬁed with ρK = λmax <
√
2 −1
for K = 24 in Example 10.3. It means that the uniqueness is guarantied for
signals of sparsity K/2 = 12. Note that this is a statistical estimate in 10000
realizations. The true bound is slightly lower.
In the case of the DFT matrix in Example 10.4 the restricted isometry
property was satisﬁed with ρK <
√
2 −1 for K = 2 only, meaning that in the
recovery we can guarantee the same solution for sparsity K = 1 only, with
M = 6 out of N = 8 samples.
The order of signal sparsity K such that the signal can be recovered
using M measurements/samples has been derived in literature as
K < C
M
log(N/M)
for Gaussian measurement matrix
K < C
M
log6 N
for partial DFT matrix

Ljubiša Stankovi´c
Digital Signal Processing
759
Figure 10.19
Minimization using the ℓ1-norm and the solution illustration for the case when
the measurements line corresponds to noisy data.
where C are (different) constants.
More conservative bounds would be obtained if we used the spark
deﬁnition. Then two minimization formulations produce the same solution
if
K < 1
2spark(A) = 1
2(1 + 1
µ)
where µ is the coherence index of the measurement matrix A. It is easy to
show that this condition is the same as the requirements that the measure-
ment matrix A (its A2K submatrices) with coherence µ satisﬁes the restricted
isometry property with δ2K = (2K −1)µ
''''
1
EA
∥AX∥2
2 −∥X∥2
2
'''' ≤(2K −1)µ∥X∥2
2 .

760
Sparse Signal Processing
The condition that
δ2K = (2K −1)µ < 1
is the same as K < 1
2(1 + 1/µ). Note that δ2K = (2K −1)µ is just a bound
of δ2K. For a matrix A there could be a lower and less restrictive constant
satisfying the restricted isometry property.
In an ideal case the matrix
P
AT
KAK
Q−1 should be identity matrix for any
combination of K columns. It means that the lines are with vector coordinate
1 in each direction. Reconstruction condition would be always satisﬁed. The
transformation y = AX would correspond to a rotation on a sphere with all
axis 1. Each X(0), X(1), X(2) would be transformed as y = AX keeping its
amplitude. Since this is not the case then the transform y = AX will change
amplitudes in addition to the rotation. For matrix A (not square matrix) the
maximal gain of vector X is obtained in a direction deﬁned by the maximal
eigenvector. In reality
XK =
B
AT
KAK
C−1
AT
Ky =
B
AT
KAK
C−1
X
0
∥XK∥2
2 ≤
1
d2
min
∥X0∥2
2
with d2
min = (1 −δk)2. The condition δ2 <
√
2 −1 would here mean that
1/d2
min > 0.343. It has been assumed that EA = 1.
10.6
MEDIAN BASED FORMULATION
Illustrative explanation of the ℓ1-norm based minimization can be presented
on a slightly more general case with N-dimensional signal. Consider the
case with M = N −1 measurements. All coefﬁcients X(k) can be expressed
as a function of one coefﬁcient, for example. X(0). For common signal
transforms, when a complete set of measurements exists, there is only one
degree of freedom in the minimization and the measurements are on a line
in the N dimensional space. First assume ideal directions of measurement
line
pX(0) = pX(1) = ... = pX(N−1) = 1.

Ljubiša Stankovi´c
Digital Signal Processing
761
Then
X(1) = X(0) −b1
X(2) = X(0) −b2
...
X(N −1) = X(0) −bN−1
(10.71)
where bi are unknown coefﬁcients. Cost function for minimization is
z = ∥X∥1 = |X(0)| + |X(0) −b1| + ... + |X(0) −bN−1|
(10.72)
The solution of this minimization problem is the median
|X(0)| = arg{min{z}} = median{0,b1,...,bN−1}.
Then for any value of coefﬁcients bi at least one X(k) will be equal to zero,
since at least one of the elements in z is zero. It means that the solution will
be of sparsity K = N −1 at least.
In order to prove that the median produces position of (10.72) mini-
mum assume that the total number of terms N is an odd number. Function
z in (10.72) is a sum of the functions of form |x −a|. The rate (derivative) of
these functions is +1 for x > a and −1 for x < a. If there are N terms, as in
(10.72), then the rate of function z will be +N for x →∞. Going now back
from x →∞toward the term with largest shift, the rate will remain +N.
At the position of the largest shift, the rate of this term will change from
+1 to −1 meaning that the overall rate of z will be reduced to +(N −2).
By passing each term, the rate will be reduced for additional factor of 2. It
means that after the kth term the rate will be (N −2k). The rate of z will
change its sign when (N −2k) = −1. This will be the position of function z
minimum. It is k = (N + 1)/2 and it corresponds to the middle coefﬁcient
positions, i.e., to the median of coefﬁcients (shifts).
Example 10.21. As an example consider the case with N = 7 and M = 6 measure-
ments AX = y producing an ideal line in a seven-dimensional space of the
form (10.71). with b1 = 0.7, b2 = 0.2, b3 = −0.5, b4 = 1, b5 = 0.8, and b6 = −0.9.
For the data presented in Fig.10.20 the solution is |X(0)| = arg{min{z}} =
median{0,0.7,0.2,−0.5,1,0.8,−0.9} = 0.2 with the coefﬁcient corresponding
to X(2) = X(0) −0.2 = 0 being equal to zero.
If the signal sparsity is K < N/2 then there will exist more than
N/2 values bi = b such that |X(0) −bi| = 0. The solution of minimization
problem then will not depend on other bk ̸= bi = b and will be unique

762
Sparse Signal Processing
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Functions  |x-x 1|,  |x-x 2|, ..., |x-x 7|
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
3
4
5
6
7
8
9
 z=|x-x 1|+|x-x 2|+...+|x-x 7|
 arg{min{z}}= median{x1,x2,x3,x4,x5,x6,x7}
 dz/dx=N=7
 dz/dx=5
 dz/dx=3
 dz/dx=1
 dz/dx=-1
 dz/dx=-3
 dz/dx=-5
 dz/dx=-N=-7
Figure 10.20
Median as the solution of minimization problem.

Ljubiša Stankovi´c
Digital Signal Processing
763
arg{min{z}} = median{0,b1,...,bi,bi,...,bi,...,bN−1} = bi = b. Therefore for
one missing sample M = N −1 the solution is unique for signals whose
sparsity is K < N/2.
If the directions are not ideal but pX(0) = a0, pX(1) = a1,..., pX(N−1) =
aN−1 then a form corresponding to the weighted median appears. For
N = 2P + 1 a weighted median produces the same result as the unweighted
median if a sum of the smallest P + 1 coefﬁcient values is greater than a sum
of its P largest values
P
∑
i=0
|ai| >
N−1
∑
i=P+1
|ai|.
For P = 1 we get
|a0| + |a1| + |a2| −max{|a0|,|a1|,|a2|}
max{|a0|,|a1|,|a2|}
> 1.
This relation corresponds to the thick line for the ℓ1-norm in Fig.10.16(top)
with |a0| =
'''pX(0)
''' = max{|a0|,|a1|,|a2|} and
'''pX(1)
''' +
'''pX(2)
'''
'''pX(0)
'''
> 1.
Consider next the case when two degrees of freedom exist (with M =
N −2 measurements). All coefﬁcients X(k) can be expressed as a function
of, for example, X(0) and X(1) as
X(2) = a2,0X(0) + a2,1X(1) −b2,
...
X(N −1) = aN−1,0X(0) + aN−1,1X(1) −bN−1.
Then
z = ∥X(k)∥1 = |X(0)| + |X(1)| + |a20X(0) + a21X(1) −b2|
+ ... + |aN−1,0X(0) + aN−1,1X(1) −bN−1|
The solution of the minimization problem is a two-dimensional median. It
is a point in X(0),X(1) plane such that a sum of absolute distances from the

764
Sparse Signal Processing
lines
X(0) = 0
X(1) = 0
a2,0X(0) + a2,1X(1) −b2 = 0
...
aN−1,0X(0) + aN−1,1X(1) −bN−1 = 0
is minimal3. Median here is not so simple as in the one-dimensional case.
Various algorithms have been proposed for multidimensional (multivariate
or spatial) median form. Note that by crossing a line ai,0X(0) + ai,1X(1) −
xi = 0 we will always either increase or reduce the rate of the function z, as
in one dimensional case.
An illustration of signal with N = 6 is presented in Fig.10.21. Value of
z is presented, along with measurements lines, for the case of two degrees
of freedom (two dimensional variable space). From this ﬁgure we can see
that the number of measurements is M = 4 and the sparsity of signal is
K = 2 since the distance of the function z minimum point from four planes
is 0. There are two nonzero distances (to the thick black lines) meaning that
there are two nonzero coefﬁcients X(k). It is interesting that in this case the
marginal median (minimization along axes X(0) and X(1) independently
would produce the same result, since one of the zero values is on the axis).
For any value of variables at least two X(k) will be equal to zero,
since at least two of the elements in z are zero. It means that, in general,
the solution will be of sparsity K = N −2 at least.
In the case of M measurements the system
AX = y
contains M equations with N unknowns. It means that there are N −M
free variables, while M can be calculated based on the free variables. Let us
3
Distance of a plane ax + by + c = 0 from a point x0,y0 is
d0 = |ax0 + by0 + c|
√
a2 + b2
= |ax0 + by0 + c|
if
√
a2 + b2 = 1.

Ljubiša Stankovi´c
Digital Signal Processing
765
 z(x,y)=|x|+|y|+|y+2x-2 |+|0.7y+0.5x-0.5 |+|y+1.6x+1.4|+|0.3y-0.5x+0.5 |
 median2(z(x,y))=(1,0)
-1.5
-1
-0.5
0
0.5
1
1.5
-1.5
-1
-0.5
0
0.5
1
1.5
Figure 10.21
Illustration of a two-dimensional median.
denote M unknowns X(k) by vector XM. Then it can be written
⎡
⎢⎢⎣
x(n1)
x(n2)
...
x(nM)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
ψ0(n1)
ψ1(n1)
ψN−M−1(n1)
ψ0(n2)
ψ1(n2)
ψN−M−1(n2)
...
...
...
ψ0(nM)
ψ1(nM)
ψN−M−1(nM)
⎤
⎥⎥⎦
⎡
⎢⎢⎣
X(0)
X(1)
...
X(N −M −1)
⎤
⎥⎥⎦
+
⎡
⎢⎢⎣
ψN−M(n1)
ψN−M+1(n1)
ψN−1(n1)
ψN−M(n2)
ψN−M+1(n2)
ψN−1(n2)
...
...
...
ψN−M(nM)
ψN−M+1(nM)
ψM−1(nM)
⎤
⎥⎥⎦
⎡
⎢⎢⎣
X(N −M)
X(N −M + 1)
...
X(N −1)
⎤
⎥⎥⎦

766
Sparse Signal Processing
y= BN−MX0,N−M−1+CMXN−M,N−1
XN−M,N−1 = C−1
M y −C−1
M BN−MX0,N−M−1
where X0,N−M−1 is the vector of free variables X(0), X(1),...,X(N −M −
1), with corresponding measurement coefﬁcients C−1
N−MBN−M. Then the
minimization problem can be written as
z = ∥X(k)∥1 = |X(0)| + |X(1)| + ... + |X(N −M −1)|
+
WWWC−1
M y −C−1
M BN−MX0,N−M−1
WWW
1 .
It is reduced to (N −M)-dimensional median formulation over variables
X(0), X(1),..., X(N −M −1). Note that the multidimensional median cal-
culation is not simple as in the case of one-dimensional problem.
10.7
NORM-ONE BASED RECONSTRUCTION ALGORITHMS
In the ℓ1-norm based reconstructions the problem is formulated as
min∥X∥1
subject to y = AX
where ∥X∥1 = ∑N−1
k=0 |X(k)|. The problem can be formulated in Lagrangian
form
F(X) =∥y −AX∥2
2 + λ∥X∥1
where F(X) is the function to be minimized.
Reformulation of the problem in a constrained form reads
min∥y −AX∥2
2
subject to
∥X∥1 < ε
or
min∥X∥1
subject to
∥y −AX∥2
2 < ε,
where ε is sufﬁciently small parameter.
There are many ways to solve the stated problem, based on the con-
strained or Lagrangian form. Many of them are developed within the regres-
sion theory. Here we will present just one of them, based on the least abso-
lute selection and shrinkage operator (LASSO) formulation and Lagrangian
minimization form.

Ljubiša Stankovi´c
Digital Signal Processing
767
10.7.1
LASSO- Minimization
The ℓ1-norm based minimization can be formulated as the minimization of
y −AX with a condition imposed on X.
The standard ridge formulation within the regression framework
would minimize the error
∥y −AX∥2
2 = (y −AX)T (y −AX)
= ∥y∥2
2 −XTATy −yTAX + XTATAX
subject to the minimal energy values of X, i.e. subject to ∥X∥2
2. The mini-
mization of the ridge constraint problem can be reformulated in Lagrangian
form using a parameter λ as
X =argmin
X
M
∥y −AX∥2
2 + λ∥X∥2
2
N
.
Minimization of
F(X) =∥y −AX∥2
2 + λ∥X∥2
2
= ∥y∥2
2 −XTATy −yTAX + XTATAX+λXTX
can be obtained in a closed form using the symbolic derivative operator
∂F(X)
∂XT
= −2ATy + 2ATAX + 2λX = 0
as
Xridge =
B
ATA + Iλ
C−1
ATy.
Parameter λ balances the error and constraint. Its inclusion makes that the
inversion is nonsingular even if ATA is singular. Real valued matrix A is
assumed, otherwise Hermitian conjugate and transpose AH would be used.
The standard ridge regression minimizes the energy of solution X(k)
and not its sparsity, Fig.10.22. That is the reason while the ℓ1-norm con-
straint is introduced in the cost function
F(X) =∥y −AX∥2
2 + λ∥X∥1
= ∥y∥2
2 −XTATy −yTAX + XTATAX+λXTsign{X}
with the LASSO minimization problem formulation
X =argmin
X
M
∥y −AX∥2
2 + λ∥X∥1
N
.

768
Sparse Signal Processing
-1
0
1
-1
-0.5
0
0.5
1
|X(0)|2+|X(1)|2
-1
0
1
-1
-0.5
0
0.5
1
|X(0)|+|X(1)|
-1
0
1
-1
-0.5
0
0.5
1
|X(0)|1/4+|X(1)|1/4
Figure 10.22
Minimization with constraint: in ridge regression (left), LASSO regression (mid-
dle), and the ℓ1/4-norm being a function closer to the ℓ0-norm .
Function ∥X∥1 promotes sparsity. It produces the same results (under cer-
tain conditions) as if ∥X∥p, with p close to 0, is used, Fig.10.22.
10.7.1.1
Iterative Calculation
The minimization problem with the ℓ1-norm constraint does not have a
close form solution. It is solved in iterative ways. In order to deﬁne an
iterative procedure we will add a nonnegative term, having zero value at
the solution Xs of the problem,
G(X) = (X −Xs)T(αI −ATA)(X −Xs),
to the function F(X). This term will not change the minimization solution.
New function is
H(X) =F(X) + (X −Xs)T(αI −ATA)(X −Xs).
where α is such that the added term is always nonnegative. It means α >
λmax, where λmax is the largest eigenvector of ATA. Gradient of H(X) is
∇H(X)=∂H(X)
∂XT
= −2ATy + 2ATAX+λsign{X} + 2(αI −ATA)(X −Xs).
Solution of ∇H(X) = 0 is
−ATy+λ
2 sign{X}−(αI −ATA)Xs + αX = 0
X+ λ
2αsign{X} = 1
αAT(y −AXs) + Xs.

Ljubiša Stankovi´c
Digital Signal Processing
769
Corresponding iterative relation is of the form
Xs+1+ λ
2αsign{Xs+1} = 1
2αAT(y −AXs) + Xs.
Note that the solution of scalar equation
x + λsign(x) = y
is obtained using soft-thresholding rule deﬁned by a function soft(y,λ) as
x = soft(y,λ) =
⎧
⎨
⎩
y + λ
for
y < −λ
0
for
|y| ≤λ
y −λ
for
y > λ
.
or
soft(y,λ) = sign(y)max{0,|y| −λ}.
The same rule can be applied to each coordinate of vector Xs+1,
Xs+1=soft( 1
αAT(y −AXs) + Xs, λ
2α)
(10.73)
or
X(k)s+1=soft( 1
α (a(k) −b(k))+X(k)s, λ
2α)
where a(k) and b(k) are coordinates of vectors a and b deﬁned by a = ATy
and b = ATAXs.
This is the iterative soft-thresholding algorithm (ISTA) for LASSO
minimization. It can be easily modiﬁed to improve convergence to fast ISTA
(FISTA). Note that this is just one of possible solutions of the minimization
problem with the ℓ1-norm.
The Lagrangian constant λ is a balance between the error and the
ℓ1-norm value, while α = 2max
R
eig{ATA}
S
is commonly used. The al-
gorithms that solve this kind of problem are implemented as functions
X =lasso(A,y).
Example 10.22. Measurement matrix A is formed as a Gaussian random matrix
of the size 40 × 60. Since there are 40 measurements the random variable
N(0,σ2) with σ2 = 1/40 is used. The original sparse signal of the total
length N = 60 is X(k) = δ(k −5) + 0.5δ(k −12) + 0.9δ(k −31) −0.75δ(k −
45) in the transformation domain. It is measured with a matrix A with
40 measurements stored in vector y. All 60 signal values are reconstructed
using these 40 measurements y and the matrix A, in 1000 iterations. In

770
Sparse Signal Processing
0
10
20
30
40
50
60
-1
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
1
sparse signal X(k)
index k
λ=0.01
0
10
20
30
40
50
60
-1
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
1
sparse signal X(k)
index k
λ=0.0001
Figure 10.23
A sparse signal with N = 60 and K = 4 reconstructed using a reduced set of
M = 40 observations and LASSO iterative algorithm. The results for λ = 0.01 and λ = 0.0001
are presented.
the initial iteration X0 = 0 is used. Then for each next s the new values
of X are calculated using (10.73), given data y and matrix A. Value of α =
2max
R
eig{ATA}
S
is used. The results for λ = 0.01 and λ = 0.0001 are
presented in Fig.10.23. For very small λ = 0.0001 the result is not sparse, since
the constraint is too weak.
10.7.2
Signal Domain Reconstruction with a Gradient Algorithm
It is shown that the sparse signal reconstruction can be formulated as a con-
strained minimization problem. The sparsity measure is minimized having
in mind constraints deﬁned by available samples/measurements). If a com-
plete set of samples/measurements can be deﬁned then the signal recon-
struction can be formulated as a minimization problem where the missing
samples/measurements yc are considered as minimization variables, while
available samples/measurements y remain unchanged. The simplest way
to solve this problem is in changing all missing samples within the range
of their possible values and then to select the combination of their values
which produced the minimal sparsity measure. This method has been illus-
trated in Example 10.15 on a signal with two missing samples. However,
when the number of missing samples is large, then a direct search over all
missing sample values cannot be used due to its high calculation complex-
ity.
Minimization of the sparsity measure M can be implemented with
gradient descent (or steepest descent) method instead of using a direct

Ljubiša Stankovi´c
Digital Signal Processing
771
search over missing sample values. Minimum sparsity measure position is
determined through an iterative procedure
y(m+1)
c
= y(m)
c
−α ∂M
∂yc
''''
yc=y(m)
c
where y(m)
c
is the vector of missing samples in the mth iteration and M is
the sparsity measure. Gradient of sparsity measure calculated at yc = y(m)
c
is
denoted by ∂M/∂yc|yc=y(m)
c
, while α is the iteration step. For the algorithm
convergence a convex measure function is required.
A signal x(n) that is sparse in a transformation domain X(k) =
T{x(n)} is used for illustration. As in Example 10.15 it has been assumed
that two samples x(nN−1) and x(nN) are not available, yc = (x(nN−1),
x(nN)). Signal xa(n) is formed. Its values at the available sample positions
y = (x(n1), x(n2), ..., x(nM)), M = N −2, are considered as constants. Sam-
ples x(nN−1) and x(nN) at the positions q1 = nN−1 and q2 = nN are con-
sidered as variables. For various values of x(nN−1) and x(nN) the sparsity
measure of xa(n) is calculated as M = ∥T[xa(n)]∥1 = ∥Xa∥1 and presented
in Fig. 10.24, along with illustration of the gradient ∂M/∂yc|yc=0 coordi-
nates at x(nN−1) = 0, x(nN) = 0 .
Consider a signal x(n) with available samples at n ∈M. Signal is
sparse in a transformation domain X(k) = T{x(n)}. The DFT will be used
as a study case, X(k) = DFT[x(n)].
As the initial estimate of reconstructed signal x(0)
a
we will use values
that would follow as a result of the ℓ2-norm based minimization of the signal
transform. Values of x(0)
a
are
x(0)
a (n) =
!
0
for missing samples, n ∈NQ
x(n)
for available samples, n ∈M
where NQ is the set of missing sample positions. The available samples are
considered as constants, while the missing samples are changed through
iterations. Denote by x(m)
a
the values of the signal reconstructed after m
iterations. The minimization process can be described as
min∥Xa∥1
subject to x(m)
a
(n)=x(n) for n ∈M
where Xa(k) = DFT[X(m)
a
(n)]. Since the task is to ﬁnd the position of func-
tion z = ∥Xa∥1 minimum, trough an iterative procedure, the relation for

772
Sparse Signal Processing
-3
-2
-1
0
1
-3
-2
-1
0
1
2
2.5
3
3.5
4
4.5
5
x(nN-1  )
|| Xa ||1
x(nN )
Figure 10.24
Sparsity measure function in the case of two unavailable signal sam-
ples yc = (x(nN−1),x(nN)) with corresponding gradient. Available samples are y =
(x(n1),x(n2),...,x(nN−2)).
missing samples calculation can be deﬁned by using the gradient of sparsity
measure
y(m+1)
c
= y(m)
c
−α 1
N
∂∥Xa∥1
∂yc
''''
yc=y(m)
c
(10.74)
where y(m)
c
is the vector of variables (missing signal sample values) in the
mth iteration, Fig. 10.13. Factor 1/N is introduced for the DFT analysis
so that coefﬁcients X(k) are equal to the signal amplitudes in time. The
coordinates of gradient vector g(ni) = ∂∥Xa∥1 /(∂ycN) in the mth iteration
can be estimated using ﬁnite differences of the sparsity measure calculated
for each variable (missing sample) ni ∈NQ
g(ni) = ∥X+
a ∥1 −∥X−
a ∥1
2∆N

Ljubiša Stankovi´c
Digital Signal Processing
773
where
X+
a (k) = T{x+
a (n)}
X−
a (k) = T{x−
a (n)}
and
x+
a (n) = x(m)
a
(n) + ∆δ(n −ni)
x−
a (n) = x(m)
a
(n) −∆δ(n −ni).
For ni ∈M there are no changes of the signal values, g(ni) = 0. A parameter
for ﬁnite difference calculation is denoted by ∆. All g(n) values form vector
denoted by Gm with elements Gm(n). The minimum of sparsity measure
is obtained when all unavailable samples are equal to the values of the
original signal values, i.e., when the signal is reconstructed (assuming that
the recovery conditions are satisﬁed).
10.7.2.1
Finite Difference Step
Before presenting the algorithm, the basic idea and parameters in (10.74)
will be discussed. Assume ﬁrst a simple case when a single signal sample at
n0 ∈NQ is not available, with card{M} = N −1. This sample is considered
as variable. It may assume an arbitrary signal value xa(n0) = x(n0) + z(n0),
where z(n0) is a variable representing shift from the true signal value at n0.
In order to estimate the ﬁnite difference of the sparsity measure
∥Xa∥1 =
N−1
∑
k=0
|Xa(k)|,
due to the change of variable z(n0), form the signals
x+
a (n) = x(n) + (z(n) + ∆)δ(n −n0)
x−
a (n) = x(n) + (z(n) −∆)δ(n −n0),
where ∆is a parameter. The ﬁnite difference of the sparsity measure is
g(n0) = ∥X+
a ∥1 −∥X−
a ∥1
2N∆
.

774
Sparse Signal Processing
The pulses δ(n −n0) are uniformly spread over all frequencies in the DFT
domain. Then
X+
a (k) = X(k) + (z(n0) + ∆)ej2πn0k/N
X−
a (k) = X(k) + (z(n0) −∆)ej2πn0k/N
holds. Since the signal is sparse (K ≪N) in a rough analysis we may neglect
changes in a few nonzero values of X(k). We may approximately write
WWX+
a
WW
1 =
N−1
∑
k=0
''X+
a (k)
'' ∼= µ + |z(n0) + ∆| N
WWX−
a
WW
1 =
N−1
∑
k=0
''X−
a (k)
'' ∼= µ + |z(n0) −∆| N,
where µ = ∥X∥1 is the sparsity measure of the original signal x(n). There-
fore the gradient approximation of the sparsity measure ∥Xa∥1 along the
direction of variable z(n0) is
g(n0) = ∥X+
a ∥1 −∥X−
a ∥1
2N∆
∼= |z(n0) + ∆| −|z(n0) −∆|
2∆
.
For deviations from the true signal value smaller than the step |z(n0)| < ∆
we get
g(n0) ∼= z(n0)
∆
∼z(n0).
(10.75)
It means that the gradient value can be used as an indicator of the signal
value deviation from the correct value (this property will be later used
for detection of impulsive noise in signal samples as well). For a large
|z(n0)| > ∆
g(n1) ∼= 1
2sign(z(n0)).
(10.76)
In that case the gradient assumes correct direction toward minimum posi-
tions, with a deviation independent intensity.
In order to analyze the inﬂuence of ∆to the solution precision, when
z(n0) is very small, assume that we have obtained the exact solution and
that the change of sparsity measure is tested on the change of sample x(n0)
for ±∆. Then for a signal x(n) = ∑K
i=1 Aiej2πn0ki/N of sparsity K the DFTs of

Ljubiša Stankovi´c
Digital Signal Processing
775
x+
a (n) = x(n) + ∆δ(n −n0) and x−
a (n) = x(n) −∆δ(n −n0) are
WWX+
a
WW
1 =
K
∑
i=1
'''Ai + ∆e−j2πn0ki/N''' + (N −K)∆
WWX−
a
WW
1 =
K
∑
i=1
'''Ai −∆e−j2πn0ki/N''' + (N −K)∆.
For the worst case analysis, assume that Ai are in phase with e−j2πn0ki/N
and ∆≤|Ai| when
WWX+
a
WW
1 =
K
∑
i=1
|Ai| + K∆+ (N −K)∆= µ + N∆
WWX−
a
WW
1 =
K
∑
i=1
|Ai| −K∆+ (N −K)∆= µ + (N −2K)∆.
where µ = ∥X∥1. Therefore g(n0) = (∥X+
a ∥1 −∥X−
a ∥1)/(2N∆) ̸= 0. The
correct signal value will not be a stationary state. The algorithm will move
the solution from x(n0) to x(n0) + b in order to produce g(n0) = 0 in the
stationary point. Then ∥X+
a ∥1 = µ + N(∆−b) is equal to ∥X−
a ∥1 = µ0 + (N −
2K)(∆+ b). It means that the stationary point will be biased. The worst case
bias b follows from
N(∆−b) = (N −2K)(∆+ b)
(10.77)
b =
K
N −K ∆∼= K
N ∆for K ≪N.
(10.78)
The bias upper limit can be reduced by using very small ∆. However,
calculation with a small ∆would be time consuming (with many iterations).
Efﬁcient implementation can be done by using ∆of an order of signal
amplitude in the initial iteration. When the algorithm reaches a stationary
point, with a given ∆, the value of mean squared error will assume its almost
constant value. The error will be changing the gradient direction around
correct point only, for almost π. This fact may be used as an indicator
to reduce the step ∆, in order to approach the signal true value with a
given precision. For example, if the signal amplitudes are of order of 1 and
K/N = 0.1 taking ∆= 1 in the ﬁrst iteration will produce the solution with
a precision better than 20 [dB]. Then, the step ∆should be reduced, for
example to ∆= 0.1. A precision better than 40 [dB] would be obtained, and
so on.
Through simulation study it has been concluded that appropriate step
parameter value in (10.74) is related to the ﬁnite difference step as α = 2∆.

776
Sparse Signal Processing
10.7.2.2
Algorithm
The presented analysis is used as a basic idea for the algorithm summarized
as follows:
Step 0:) Set m = 0 and form the initial signal estimate x(0)
a (n) deﬁned for n ∈
N as
x(0)
a (n) =
!
0
for missing samples, n ∈NQ
x(n)
for available samples, n ∈M ,
(10.79)
where N = {0,1,..., N −1} and NQ= N\M is the complement of M with
respect to N. The initial value for an algorithm parameter ∆is estimated as
∆= max
n∈M |x(n)|.
(10.80)
Step 1: Set xp(n) = x(m)
a
(n). This signal is used in Step 3 in order to estimate
reconstruction precision.
Step 2.1: Set m = m + 1. For each missing sample at ni ∈NQ form signals
x+
a (n) and x−
a (n):
x+
a (n) = x(m)
a
(n) + ∆δ(n −ni)
x−
a (n) = x(m)
a
(n) −∆δ(n −ni).
(10.81)
Step 2.2: Estimate differential of the signal transform measure
g(ni) = ∑N−1
k=0 |X+
a (k)| −∑N−1
k=0 |X−
a (k)|
2N∆
(10.82)
where X+
a (k) = T{x+
a (n)} and X−
a (k) = T{x−
a (n)} are transforms of x+
a (n)
and x−
a (n).
Step 2.3: Form a gradient vector Gm with the same length as the signal. At
the positions of available samples n ∈M, this vector has value Gm(n) = 0.
At the positions of missing samples n ∈NQ its values are Gm(n) = g(n),
calculated by (10.82).
Step 2.4: Correct the values of estimated signal ya(n) iteratively by
x(m)
a
(n) = x(m−1)
a
(n) −αGm(n),
(10.83)
where the step parameter α = 2∆is commonly used.
Step 2.5: Calculate angle βm between successive gradients as
βm = arccos
∑N−1
k=0 Gm−1(k)Gm(k)
F
∑N−1
k=0 G2
m−1(k)
F
∑N−1
k=0 G2m(k)

Ljubiša Stankovi´c
Digital Signal Processing
777
If angle βm is lower than 170◦and the maximal allowed number of iterations
is not reached (m < mmax) go to Step 2.1.
Step 3: If the maximal allowed number of iterations is reached stop the
algorithm. Otherwise calculate
Tr = 10log10
∑n∈NQ |xp(n) −x(m)
a
(n)|2
∑n∈NQ |x(m)
a
(n)|2
.
Value of Tr is an estimate of the reconstruction error to signal ratio, calcu-
lated for missing samples only. If Tr is above the required precision thresh-
old (for example, if Tr > −100dB), the calculation procedure should be re-
peated with smaller ∆. For example, set new ∆value as ∆/
√
10 or ∆/10 and
go to Step 1.
Step 4: Reconstruction with the required precision is obtained in m itera-
tions or the maximal allowed number of iterations is reached. The recon-
structed signal is xR(n) = x(m)
a
(n).
By performing presented iterative procedure, the missing values will
converge to the true signal values, producing the minimal concentration
measure in the transformation domain.
- The inputs to the algorithm are the signal length N, the set of
available samples M, the available signal values x(ni), ni ∈M, the required
precision Tmax, and maximal number of iterations.
- Instead of calculating signals (10.81) and their transforms for each
ni ∈NQ we can calculate
''X+
a (k)
'' =
'''X(m)
a
(k) + ∆Dni(k)
'''
''X−
a (k)
'' =
'''X(m)
a
(k) −∆Dni(k)
'''
with X(m)
a
(k) = T{x(m)
a
(n)} and Dni(k) = T{δ(n −ni)} = exp(−j2πnik/N),
for the DFT and each ni ∈M. Since Dni(k) are independent of the iteration
number m they can be calculated independently from the DFT of the signal.
Example 10.23. Consider a signal
x(n) = 3sin(20π n
N )
with N = 8. Missing samples are n ∈NQ = {1,6}. The signal is reconstructed
using a simpliﬁed gradient based algorithm using Step 0 to Step 2.4, from
(10.79) to (10.83), in 60 iterations. The initial algorithm parameter ∆= 1 and

778
Sparse Signal Processing
3.5
3.5
4
4
4
4.5
4.5
4.5
5
5
5.5
5.5
6
-4
-3
-2
-1
0
1
-1
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
Figure 10.25
Illustration of a signal reconstruction using adaptive gradient algorithm.
the initial value of missing samples x(1) = 0 and x(6) = 0 are used. The values
of missing samples in the ﬁrst 20 iterations are presented by dots (connected
by a line) in Fig.10.25. After about 6 iterations the algorithm with ∆= 1
does not signiﬁcantly change the missing sample values (zoomed changes
are shown in lower subplot within the ﬁgure). Close to the stationary point
obtained for ∆= 1 the gradient coordinates are almost zero-valued (with
direction changes for almost π), since the measures are on the contour with
almost the same measure (circles). After the step is reduced to ∆= 0.1 in
the 20th iteration, the algorithm resumes its fast approach toward the exact
value, until a new stationary state. With a new change of ∆to ∆= 0.01 the
approach is again continued.
The stationary state bias for ∆= 1 is lower than K
N ∆= 1/4 (it corre-
sponds to the bias caused MSE lower than 15.5 [dB]). By each reduction of ∆
to ∆/10 the bias caused MSE will be lower for 20 [dB]. The reconstruction re-
sult and the MSE for the estimated missing values x(1) and x(6) is presented
in Fig.10.26.
The calculation is repeated with the signal
x(n) = 3sin(20π n
N ) + 2cos(60π n
N ) + 0.5sin(46π n
N )
and N = 32. Missing samples are n ∈NQ = {2,4,5,7,9,13,17,19,24,26,28,31}.
The result for this case is shown in Fig.10.27.

Ljubiša Stankovi´c
Digital Signal Processing
779
0
2
4
6
8
-3
0
3
Original signal x(n)
0
2
4
6
8
-3
0
3
Available samples
0
2
4
6
8
-3
0
3
Reconstructed signal in  60  iteratons
time n
0
20
40
60
-80
-60
-40
-20
0
iteration
Reconstruted MSE in [dB]
Figure 10.26
Gradient-based reconstruction of a sparse signal.
10.7.2.3
Comments on the Algorithm
- In a gradient-based algorithm, a possible divergence is related to the
algorithm behavior for large steps ∆. Small steps inﬂuence the rate of the
algorithm approach to the solution only (assuming that it exists). Here, we
will examine the algorithm behavior for a large value of step ∆. We can write
''X+
a (k)
'' −
''X−
a (k)
'' =
'''X(m)
a
(k) + ∆Dni(k)
''' −
'''X(m)
a
(k) −∆Dni(k)
'''
= ∆|Dni(k)|
('''''1 + X(m)
a
(k)
∆Dni(k)
''''' −
'''''1 −X(m)
a
(k)
∆Dni(k)
'''''
)
.
Considering the complex number a = X(m)
a
(k)/(∆Dni(k)), with |a| ≪1 for
a large ∆, from the problem geometry it is easy to show that the following
bounds hold 0 ≤||1 + a| −|1 −a|| ≤2|a|. Exact value of this expression
depends on the phase of a. Therefore,
0 ≤
''''X+
a (k)
'' −
''X−
a (k)
'''' ≤2
'''X(m)
a
(k)
'''.
Lower limit 0 is obtained if a is imaginary-valued, while the upper limit
2
'''X(m)
a
(k)
''' follows if a is real-valued.
It means that the value of the ﬁnite difference |X+
a (k)| −|X−
a (k)|, that
is used to correct the missing signal samples, does not depend on the value
of the step ∆if ∆is large. The missing signal values will be adapted for

780
Sparse Signal Processing
0
10
20
30
-4
-2
0
2
4
Original signal x(n)
0
10
20
30
-4
-2
0
2
4
Available samples
0
10
20
30
-4
-2
0
2
4
Reconstructed signal in  5  iteratons
0
10
20
30
-4
-2
0
2
4
Reconstructed signal in  15  iteratons
0
10
20
30
-4
-2
0
2
4
Reconstructed signal in  60  iteratons
time n
0
20
40
60
-80
-60
-40
-20
0
iteration
Reconstruted MSE in [dB]
Figure 10.27
Gradient-based reconstruction of a sparse signal.
a value independent on ∆in that case. The values of missing samples will
oscillate within the range of the original signal values of order
'''X(m)
a
(k)
'''/N,
until ∆is reduced in the iterations below the signal magnitude. Then the
missing samples will start approaching to the position of the sparsity mea-
sure minimum. The initial values will be arbitrary changed within the signal
amplitude order as far as ∆is too large. It will not inﬂuence further conver-
gence of the algorithm, when the step ∆assumes appropriate values.
- Since two successive gradient vectors are required to calculate the
gradient angle βm, it is calculated starting from the second iteration for each
∆.
- Algorithm output is the reconstructed signal xR(n), n = 0,1,..., N −1.
- Other signal transforms can be used instead of the DFT. The only re-
quirement is that signal is sparse in that transform domain (two-dimensional
DCT will be presented later).

Ljubiša Stankovi´c
Digital Signal Processing
781
Example 10.24. Consider a signal
x(t) =
K/2
∑
i=1
Ai cos(2πtki/T + ϕi),
(10.84)
with t = n∆t, ∆t = 1, and the total number of samples N = T/∆t. The
sparsity parameter K is changed from K = 2 to K = N/2. The amplitudes
Ai, frequencies ki, and phases ϕi are taken randomly. Amplitude values
are modeled as Gaussian random variables with variance 1, the frequency
indices assume random numbers within 1 ≤ki ≤N −1, and the phases
assume uniform random values within 0 ≤φi ≤2π, in each realization. The
reconstruction is performed by using 100 realizations for each K with random
sets of missing Q = N −M samples in each realization. The reconstructed
signals xR(n) are obtained. The results are presented in Fig.10.28 in a form of
the signal-to-reconstruction-error ratio (SRR) in [dB]
SRR = 10log
∑N−1
n=0 |x(n)|2
∑N−1
n=0 |x(n) −xR(n)|2 .
(10.85)
Bright colors indicate the region where the algorithm had fully recovered
missing samples in all realizations, while dark colors indicate the region
where the algorithm could not recover missing samples in any realization.
In the transition region for M slightly greater than 2K we have cases when
the signal recovery is not achieved and the cases of full signal recovery. The
simulations are done for N = 128 and for N = 64, Fig.10.28(a),(b). A stopping
criterion for the accuracy of 120 [dB] is used. It corresponds to a precision
in the recovered signal of an input samples precision if they are acquired by
a 20-bit A/D converter. The case with N = 64 is repeated with an additive
input Gaussian noise such that the input signal-to-noise ratio is 20 [dB] in
each realization Fig.10.28(c). The reconstruction error in this case is limited
by the input signal-to-noise value. The number of iterations to achieve the
required precision is presented in Fig.10.28(d). We can see that the number
of iterations is well bellow 100 for the most important region where the
reconstruction was achieved in all realizations (high values of M and small
value of K, M ≫K).The number of iterations is quite small in the region
where the reconstruction can be achieved.
An illustration of the algorithm performance regarding to the SRR and
the gradient angle βm in one realization, with K = 6, is presented in Fig.10.29.
The algorithm reached 120 [dB] accuracy in 47 iterations. From the gradient
angle graph we see that the algorithm step is reduced to ∆/
√
10 →∆in about
each 4 iterations. According to (10.77) the expected MSE improvement by
each reduction of ∆is 20log(
√
10) = 10 [dB].

782
Sparse Signal Processing
available samples M
sparsity K
Signal-to-reconstruction error
 
 
(a)
20
40
60
80
100 120
10
20
30
40
50
60
[dB]
0
20
40
60
80
100
120
available samples M
sparsity K
Signal-to-reconstruction error
 
 
(b)
20
40
60
5
10
15
20
25
30
[dB]
0
20
40
60
80
100
120
available samples M
sparsity K
Signal-to-noise ratio
 
 
(c)
20
40
60
5
10
15
20
25
30
[dB]
0
5
10
15
available samples M
sparsity K
Number of iterations
 
 
(d)
20
40
60
5
10
15
20
25
30
[n]
100
200
300
400
Figure 10.28
Signal-to-reconstruction-error (SRR) averaged over 100 realizations for various
sparsity K and number of available samples M: (a) The total number of samples is N = 128. (b)
The total number of samples is N = 64. (c) With a Gaussian noise in the input signal, SNR = 20
[dB] and N = 64. (d) Number of iterations to reach the solution with the deﬁned precision.
10.8
ON THE UNIQUENESS OF THE DFT OF SPARSE SIGNALS
In general, the reconstructed signal uniqueness is guarantied if the restricted
isometry property is used and checked with appropriate isometry constants.
However, two problems exist in the implementation of this approach. For
a speciﬁc measurement matrix it produces quite conservative bounds. In
practice it would produce a large number of false alarms for nonuniqueness.
In addition, uniqueness check with the restricted isometry property requires
a combinatorial approach, which is an NP hard problem.

Ljubiša Stankovi´c
Digital Signal Processing
783
0
10
20
30
40
50
0
50
100
150
200
successive gradient angles
iteration
 βm
0
10
20
30
40
50
0
50
100
SSR [dB]
iteration
Figure 10.29
Angle between successive gradient estimations
βm
and the signal-to-
reconstruction-error ratio (SRR) as a function of the number of iterations in the algorithm for
one signal realization with 6 nonzero DFT coefﬁcients and M = 64.
In the adaptive gradient-based method the missing samples (measure-
ments) are considered as the minimization variables. The available samples
values are known and ﬁxed. The number of variables in the minimization
process is equal to the number of missing samples/measurements in the ob-
servation domain. This approach is possible when the common signal trans-
forms are the domains of signal sparsity. Then the missing and available
samples/measurements form a complete set of samples/measurements.
The DFT will be considered here as the signal sparsity domain. The
solution uniqueness is deﬁned in the sense that the variation of the missing
sample values cannot produce another signal of the same sparsity. In the
case when the signal is already reconstructed then the uniqueness is checked
in the sense that there is no other signal of the same or lower sparsity with
the same set of available samples.
Consider a signal x(n) with n ∈N={0,1,2,...., N −1}. Assume that
Q of its samples at the positions qm ∈NQ = {q1,q2,....,qQ} are miss-
ing/omitted. The signal is sparse in the DFT domain, with sparsity K. The
reconstruction goal is to get x(n), for all n ∈N using available samples at

784
Sparse Signal Processing
n ∈M = N\NQ. A new signal of the form
xa(n) = x(n) + z(n)
will be analyzed here. For the available signal positions n ∈M the value of
z(n) is ﬁxed z(n) = 0, while z(n) may take arbitrary value at the positions of
missing samples n = qm ∈NQ = {q1,q2,....,qQ}. If x(n) is a K sparse signal
then the DFT of xa(n) is
Xa(k) = X(k) + Z(k)
= N
K
∑
i=1
Aiδ(k −k0i) +
Q
∑
m=1
z(qm)e−j2πqmk/N.
Positions of nonzero values in X(k) are k0i ∈K = {k01,k02,....,k0K} with
amplitudes X(k0i) = NAi. The values of missing samples of xa(n) = x(n) +
z(n) for n ∈NQ are considered as variables. The goal of reconstruction
process is to get xa(n) = x(n), or z(n) = 0 for all n ∈N. This goal should be
achieved by minimizing a sparsity measure of the signal transform Xa(k).
Existence of the unique solution of this problem depends on the number of
missing samples, their positions, and the signal form.
If a signal with the transform X(k) of sparsity K is obtained using a re-
construction method, with a set of missing samples, then the reconstruction
X(k) is unique if there is no other signal of the same or lower sparsity that
satisﬁes the same set of available samples (using the same set of missing
samples as variables).
Example 10.25. Consider the simplest case of one missing sample at position
n = q. The signal sparsity is K. Signal reconstruction is based on xa(n) =
x(n) + zδ(n −q) where z indicates an arbitrary deviation from the true signal
value, since the missing sample x(q) is considered as variable. The DFT of
xa(n) is
Xa(k) = N
K
∑
i=1
Aiδ(k −k0i) + ze−j2πkq/N.
The number of nonzero DFT coefﬁcients is
card{Xa} = ∥Xa∥0 =
K
∑
i=1
'''NAi + ze−j2πk0iq/N'''
0
+
N
∑
i=K+1
|z|0

Ljubiša Stankovi´c
Digital Signal Processing
785
Possible sparsity of Xa(k) is
∥Xa∥0 =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
N
for
|z| ̸= 0 and z ̸= −NAiej2πk0iq/N for any i
N −1
for
|z| ̸= 0 and z = −NAiej2πk0iq/N for one i only
...
...
...
N −K
for
|z| ̸= 0 and z = −NAiej2πk0iq/N for i = 1,..,K
K
for
|z| = 0.
(10.86)
With just one missing value and arbitrary signal, the minimum of ∥Xa∥0 is
achieved at |z| = 0 only if the signal sparsity is lower than the lowest possible
sparsity with |z| ̸= 0,
K < N −K.
It means K < N/2. For K = N/2 the last two rows of (10.86) will produce the
same result N −K = N/2 and K = N/2. In that case the minimum of ∥Xa∥0
is not unique. Note that this is true only if the considered signal x(n) has a
very speciﬁc form
A1ej2πk01q/N = A2ej2πk02q/N = A3ej2πk03q/N = ... = AKej2πk0Kq/N = C. (10.87)
In reality the case that all components have equal amplitudes |A1| = |A2| =
|A3| = ... = |AK| and that the missing sample position q is such that
arg{A1} + 2πk01q/N = arg{A2} + 2πk02q/N = ... = arg{AK} + 2πk0Kq/N
(10.88)
is a zero probability event.
It is interesting to note that if the last two conditions are satisﬁed by a
signal x(n) then the DFT coefﬁcients from (10.87) are the frequency domain
samples of a harmonic signal Bexp(ej2πkq/N), at k ∈{k01,k02,....,k0s}. Its IDFT
is a delta pulse with the group delay at the position of missing sample
IDFT{Bexp(ej2πkq/N)} = Bδ(n −q).
Example 10.26. Consider a signal x(n) with N = 32 and two missing samples at
qm ∈NQ = {3,19}.
Signal sparsity is K. In order to simplify the notation assume that one DFT
value of the reconstructed signal is X(5) = 2.
(a) Show that the limit for sparsity K (when we can claim that the
reconstructed sparse signal is unique, assuming that all signal amplitudes
may assume arbitrary values) is K < 8.
(b) What are the properties that a signal must satisfy in the limit case
K = 8 so that the solution is not unique?
(c) What is the sparsity limit if the missing samples are at qm ∈NQ =
{5,9}?

786
Sparse Signal Processing
(d) Repeat the analysis for qm ∈NQ = {4,7}.
(e) What is the sparsity limit if the missing samples are qm ∈NQ =
{3,4,19}?
⋆(a) Consider Xa(k) = X(k) + Z(k) with z(n) assuming nonzero val-
ues z3 and z19 at n = 3 and n = 19, respectively. The DFT Z(k) of z(n) is equal
to
Z(k) = z3e−j2π3k/32 + z19e−j2π19k/32 = e−j2π3k/32 B
z3 + (−1)kz19
C
k = 0,1,...,31.
In the worst case for the minimization Z(k) should have maximal possible
number of zeros and they should remain in Xa(k) = X(k) + Z(k). We con-
clude that either z3 = z19 or z3 = −z19 should hold (when sparsity of Z(k) is
16), otherwise the sparsity of Z(k) would be 32. In addition, in the worst case
nonzero values of Z(k) could cancel out all K components including assumed
X(5) = 2. Therefore the maximal number of zeros in Xa(k) with nonzero z(n)
is 16 + K. The sparsity of Xa(k) is then 32 −(16 + K). It should be greater than
the sparsity K of the correct solution when all z(n) = 0 and Xa(k) = X(k). It
means
32 −(16 + K) > K
should hold. This completes the proof that K < 8 should hold.
(b) Since z3 = z19 would produce Z(2k + 1) = 0 it would not be able
to cancel X(5). Therefore for the worst case analysis we must use z3 = −z19
with
Z(5) = e−j2π15/32 (z3 −z19) = −X(5) = −2.
It means z3 = −z19 = −ej2π15/32 and
Z(k) =
%
−2e−j2π(3k−15)/32
for odd k
0
for even k.
In order to cancel all nonzero values of X(k), all of them must be located at
odd positions (where Z(k) is nonzero)
X(k) ̸= 0 for k ∈{5,k02,k03,k04,k05,k06,k07,k08}.
The values of X(k) must be of opposite sign and equal amplitude to the
corresponding (determined) values of Z(k)
X(k0i) = −Z(k0i) = 2e−j2π(3k0i−15)/32
for i = 2,3,...,8
resulting in
X(k) =
%
2e−j2π(3k−15)/32
for k ∈{5,k02,k03,k04,k05,k06,k07,k08}
0
elsewhere.
(10.89)

Ljubiša Stankovi´c
Digital Signal Processing
787
In this case sparsity of X(k) + Z(k) is 8, the same as the sparsity of X(k).
Two solutions of our minimization problem are signal x(n) = IDFT[X(k)] and
x(n) + z(n) where
z(n) = IDFT[Z(k)] = δ(n −3) −δ(n −19).
Both of these signals have the same sparsity K = 8 and satisfy the same set
of available samples. However, if the sampled signal x(n) is not the signal of
very speciﬁc from (10.89) then the solution of sparsity K = 8 will be unique
for a given set of available samples. Then z(n) = δ(n −3) −δ(n −19) will not
be able to cancel all 8 DFT values of signal and the sparsity of X(k) + Z(k)
will be 8 only for z(n) = 0, producing correct unique solution. Signal Y(k) =
−Z(2k −1) is Y(k) = 2e−j2π(3(2k−1)−15)/32 = 2e−j2π(3k−9)/16. It is periodic
with period N/Q = 16. Group delay of this signal is n0 = 3 with period 16.
Therefore within n = 0,1,...,31 group delays n0 = 3 and n0 + 16 = 19 of Y(k)
correspond to the missing sample positions. The signal must have the form
X(k0m) ∈{2e−j2π(3k−9)/16 | k = 0,1,..., N
Q −1}, with k = 3 corresponding to
k0m = 2k −1 = 5 producing X(5) = 2.
(c) Inﬂuence of missing samples highly depends on their positions. If
the missing samples are at qm ∈NQ = {5,9} then
Z(k) = z5e−j2π5k/32 + z9e−j2π9k/32 = e−j2π5k/32 B
z5 + e−j2πk/8z9
C
.
Maximal number of zero values in Z(k) is now 4. Assuming that nonzero
values can cancel out all signal nonzero samples, maximal number of zeros
in X(k) + Z(k) is 4 + K with sparsity (N −K −4). It should be greater than
the signal sparsity K. Thus in this case uniqueness condition is K > 28 −K,
or K > 18.
(d) In the case of qm ∈NQ = {4,7} two nonzero variables z(4) and z(7)
can produce only one zero value in Z(k) since
Z(k) = z4e−j2π4k/32 + z7e−j2π7k/32 = 0
for z4 = −z7e−j2π3k/32. In addition, all K signal nonzero values X(k) can be
cancel out. Then the uniqueness relation is N −1 −K > K.
(e) If the missing samples are qm ∈NQ = {3,4,19} then this case may
be considered as a case with three variables producing two nonzero values
in Z(k), but also it can be considered as {3,19} ∪{4}, when z(4) = 0 and two
variables z(3) and z(19) deﬁne sparsity as in (a). The second case is worse,
meaning that it deﬁnes the resulting sparsity K < 8.
The analysis presented in the previous two examples can be general-
ized, taking into account the positions and number of missing samples, to a
simple uniqueness test as follows:
Test 1: Consider a signal x(n) that is sparse in the DFT domain with
unknown sparsity. Assume that the signal length is N = 2r samples and that Q

788
Sparse Signal Processing
samples are missing at the instants qm ∈NQ. Assume that the reconstruction
is performed and that the DFT of reconstructed signal is of sparsity K. The
reconstruction result is unique if the inequality
K < N −
max
h=0,1,...,r−1
M
2h (Q2h −1)
N
−K
holds. Integers Q2h are calculated as
Q2h =
max
b=0,1,...,2h−1
{card{q : q ∈NQ and mod(q,2h) = b}}
(10.90)
Example 10.27. Consider a signal with N = 25 = 32 and Q = 9 missing samples at
qm ∈NQ = {2,3,8,13,19,22,23,28,30}.
Using Test 1 we will ﬁnd the sparsity limit K when we are able to claim that
the reconstructed sparse signal is unique for any signal form.
-For h = 0 we use Q20 = Q and get 20 (Q20 −1) −1 = (Q −1) −1 = 9.
-For h = 1, the number Q21 is the greater value of
card{q : q ∈NQ and mod(q,2) = 0} = card{2,8,22,28,30} = 5
card
R
q : q ∈NQ and mod(q,2) = 1
S = card{3,13,19,23} = 4,
i.e., the maximal number of even or odd positions of missing samples. Thus
Q21 = max{5,4} = 5 with 21 (Q21 −1) = 8.
-Next Q22 is calculated as the maximal number of missing samples
whose distance is multiple of 4. For various initial counting positions b =
0,1,2,3 the numbers of missing samples with distance being multiple of 4 are
2,1,3, and 3, respectively. Then Q22 = max{2,1,3,3} = 3 with 22(Q2h −1) = 8.
-For Q23 the number of missing samples at distances being multiple
of 8 are found for various b = 0,1,2,3,4,5,6,7. The value of Q23 is 2 with
23(Q23 −1) = 8.
-Finally we have two samples at distance 16 (samples at the positions
q2 = 3 and q5 = q2 + N/2) producing Q24 = Q16 = 2 with 24(2 −1) = 16.
The reconstructed signal of sparsity K is unique if
K < N −
max
h=0,1,2,3,4
M
2h (Q2h −1)
N
−K
K < 32 −max{9,8,8,8,16} −K
K < 32 −16 −K
or
K < 8.

Ljubiša Stankovi´c
Digital Signal Processing
789
Test 1 considers general signal form. It includes the case when the
amplitudes of signal components are related to each other and related to
the missing sample positions. The speciﬁc signal form required by Test 1, to
reach its bound, is analyzed in the example. Since this kind of relation is a
zero-probability event, the condition obtained by neglecting the probability
that the signal values are dependent to each other and related to missing
sample positions at the same time is presented next.
C1: Assume that the amplitudes of signal components in Test 1 are arbitrary
with arbitrary phases so that the case when all of them can be related to the values
deﬁned by using the missing sample positions is a zero-probability event. The
reconstruction result is not unique if the inequality
K ≥N −
max
h=0,1,...,r−1
M
2h (Q2h −1)
N
−1
holds. Integers Q2h are calculated in the same way as in the Test1.
Example 10.28. Consider a signal with N = 25 = 32 and Q = 9 missing samples at
qm ∈NQ = {2,3,8,13,19,22,23,28,30}.
The sparsity limit K when we are able to claim that the reconstructed sparse
signal is not unique is
K ≥N −
max
h=0,1,2,3,4
M
2h (Q2h −1)
N
−1
K ≥32 −max{9,8,8,8,16} −1
K ≥15.
Corollary C1 provides the uniqueness test for the given positions
of unavailable samples. In the cases with h > 0 it exploits the periodic
structure of the transformation matrix of missing samples. The periodical
form assumes that the positions of possible zero values in Z(k) do not
interfere with the signal nonzero value positions. This is possible in the
worst case analysis.
Test 2: Consider a signal x(n) that is sparse in the DFT domain with
unknown sparsity. Assume that the signal length is N = 2r samples and that Q
samples are missing at the instants qm ∈NQ. Also assume that the reconstruction
is performed and that the DFT of reconstructed signal is of sparsity K. Assume
that the positions of the reconstructed nonzero values in the DFT are k0i ∈K =
{k01,k02,....,k0s} Reconstruction result is unique if the inequality
K < N −
max
h=0,1,...,r−1
M
2h (Q2h −1) −K + 2S2r−h
N

790
Sparse Signal Processing
holds. Integers Q2h and S2r−h are calculated as
Q2h =
max
b=0,1,...,2h−1
{card{q : q ∈NQ and mod(q,2h) = b}}
S2r−h =
Q2h−1
∑
l=1
Ph(l)
Ph(l) =
sort
b=0,1,...,2r−h−1
{card{k : k ∈K and mod(k,2r−h) = b}}
where Ph(1) ≤Ph(2) ≤... ≤Ph(2r−h).
Note: For S2r−h = 0 this Test reduces to the Test 1. For the DFT values
equally distributed over all positions this Test produces result close to
K ≥N −Q.
C2: Assume that the positions of the reconstructed nonzero values in the DFT
are k0i ∈K = {k01,k02,....,k0s}. Assume that the amplitudes of signal components
Test 2 are arbitrary with arbitrary phases so that the case when all of them can
be related to the values deﬁned by using the missing sample positions is a zero-
probability event. Reconstruction result is not unique if the inequality
K ≥N −
max
h=0,1,...,r−1
M
2h (Q2h −1) −1 + S2r−h
N
holds. Integers Q2h and S2r−h are calculated as in the Test 2. The case when all of
signal components can be related to the values deﬁned by using the missing sample
positions is considered here.
Example 10.29. Consider a signal with N = 32 and Q = 9 missing samples at
qm ∈NQ = {2,3,8,13,19,22,23,28,30}.
Assume that with these missing samples we have reconstructed signals with
nonzero DFT values at the positions
a)
K = {1,3,5,7,9,11,13,15,17,21,23,25,27,29,31},
b)
K = {1,3,5,9,13,17,21,29,31,2,4,8,12,16,20,24,30}.
By testing these two signals we get the following decisions. According to
Test 1 we cannot claim uniqueness in either of these cases since K = 15
in the ﬁrst case and K = 17 in the second case. Both are greater than the
Test1 bound K < 8. The same holds for Corollary C1 since both are K ≥15.
By testing these results with Test 2 we get that in case a) the solution is
nonunique. It is due to very speciﬁc form of the reconstructed signal with all
components being found at the odd frequency positions. Since the sparsity
was deﬁned by periodicity 16 in qm ∈NQ, then variations of two signal

Ljubiša Stankovi´c
Digital Signal Processing
791
samples z(q2 = 3) and z(q5 = 19) can produce a signal X(k) + Z(k) with
lower sparsity than the reconstructed signal. These two samples, as variables,
are able to produce many (N/2) zero values in Z(k) either at odd or even
positions in frequency. In this case they are at even positions of X(k) + Z(k).
However, in signal b) that is not the case. Nonzero values are distributed
over both even and odd frequency positions. Although sparsity of this signal
is K = 17 the reconstruction is unique. The distribution of nonzero values in
the reconstructed X(k) is such that by varying two samples z(q2 = 3) and
z(q5 = 19) we cannot produce a signal X(k) + Z(k) of lower sparsity with
nonzero z(q2 = 3) and z(q5 = 19). The limit in this case is deﬁned by the
lower periodicity in z(q) than N/2. Thus, if we obtain this signal using a
reconstruction algorithm the solution is unique.
Example 10.30. Consider a signal with N = 1024 and Q = 512 missing samples at
qm ∈NQ = {0,2,4,...1022}. The reconstructed signal is at the frequencies: a)
K = {3}, b) K = {3,515}. We can easily check that in all cases with Test 1,
Corollary C1 and Test 2, the reconstruction is nonunique although K = 1 or
K = 2 is much smaller than the available number of samples N −Q = 512. The
answer is obtained almost immediately, since the computational complexity
of Test 1, Corollary C1 and Test 2, is of order O(N).
10.9
INDIRECT MEASUREMENTS/SAMPLING
In some applications a signal x(n) is measured in an indirect way. Consider
the case when each measurement f (n) is a linear combination of all signal
samples
f (i) = bi1x(0) + bi2x(1) + ... + biNx(N −1)
(10.91)
with i = 0,1,..., M −1. In this case the reconstructed signal is x =[x(0) x(1) ...
x(N −1)]T or its sparse transform X. Matrix relation between signal samples
x(n) and M indirect measurements f (i) is
f = BMx
where elements of vector f are f (i) and the elements of M × N matrix BM
are bin. For the transformation x = ΨX the sparsity domain to measurements
domain relation is
f = BMΨX.
Minimization problem is deﬁned by
min∥X∥1 subject to f = BMΨX = AX,

792
Sparse Signal Processing
where
A = BMΨ.
As a simple study case for this kind of measurements consider a
discrete-time signal x(n) obtained by sampling a continuous-time signal
x(t) at nonuniform (or random) positions. Using the results presented in
this chapter we can state that if the signal x(t) satisﬁes the sampling theorem
and its DFT is sparse, then the signal can be reconstructed from a reduced set
of samples x(ti) at {t1,t2,...,tM} not corresponding to the sampling theorem
positions.
Since the DFT is used in the analysis, we can assume that the con-
tinuous time signal is periodically extended with a period T. According to
the sampling theorem, the period T is related to the number of samples N,
the sampling interval ∆t, and the maximal frequency Ωm as Ωm = π/∆t =
πN/T. The continuous-time signal can be written as an inverse Fourier se-
ries
x(t) =
N/2−1
∑
k=−N/2
Xkej2πkt/T,
(10.92)
with the Fourier series coefﬁcients being related to the DFT as XkN = X(k) =
DFT[x(n)] and x(n) = x(n∆t). The discrete-time index n corresponds to
the continuous-time instant t = n∆t. Discrete-frequency indices are k ∈
{−N/2,...,−1,0,1,..., N/2 −1}. Any signal value can be reconstructed from
the samples taken according to the sampling theorem, 3.6
x(t) =
N−1
∑
n=0
x(n)ej(n−t/∆t)π/N
sin[(n −t
∆t)π]
N sin[(n −t
∆t)π/N].
(10.93)
This relation holds for an even N. Similar relation can be written for an odd
N, Section 3.6.
For a sparse x(n) in the DFT domain, the number K of nonzero
transform coefﬁcients X(k) is much lower than the number of the original
signal samples N within T, K ≪N, i.e., X(k) = NXk = 0 for k /∈{k1, k2, ...,
kK}. A signal
x(t) =
K
∑
i=1
Xkiej2πkit/T.
(10.94)
of sparsity K can be reconstructed from a reduced set of M samples if the
recovery conditions are met.
Consider a random set of possible nonuniform sampling instants
{t1,t2,...,tN},
ti = i∆t + νi,
(10.95)

Ljubiša Stankovi´c
Digital Signal Processing
793
where, for example, νi is a uniform random variable −∆t/2 ≤νi ≤∆t/2.
Here tni denotes a time instant, while in the uniform sampling the discrete-
time index ni has been used to indicate instant corresponding to ni∆t.
Assume that a set of M signal samples are available
f =[x(tn1),x(tn2),...,x(tnM)]T
at instants
tni ∈TA = {tn1,tn2,...,tnM}.
being a random subset of {t1,t2,...,tN}, with tni = ni∆t + νni. The measure-
ments matrix relation is, (10.92)
⎡
⎢⎢⎣
x(tn1)
x(tn2)
...
x(tnM)
⎤
⎥⎥⎦=
⎡
⎢⎢⎢⎣
e−j2πNtn1/(2T)
...
ej2π(N−2)tn1/(2T)
e−j2πNtn2/(2T)
...
ej2π(N−2)tn2/(2T)
...
...
...
e−j2πNtnM /(2T)
...
ej2π(N−2)tnM /(2T)
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎣
X−N/2
X−N/2+1
...
XN/2−1
⎤
⎥⎥⎦
(10.96)
f= AX
The analysis presented in this chapter can be used to solve this problem and
calculate sparse coefﬁcients Xk from the reduced set of observations f. The
measurements matrix in this case is a structured random matrix.
The nonzero positions of the Fourier transform coefﬁcients can be
estimated using the available measurements only
X0 = AHf
or
X0(k) = NX0,k = ∑
tni ∈TA
x(tni)e−j2πktni /T
(10.97)
Note that a sparse signal X(k) with components at k /∈{k1, k2, ..., kK} can
be written in form (10.94). For a frequency k = kp and the signal component
Xkp exp(j2πkpt/T) all terms in (10.97) will be the same
Xkpej2πkptni /Te−j2πktni /T'''
k=kp = Xkp.
Therefore, the mean value of estimator (10.97), using M instants tni, is
E{X(k)} = M
K
∑
p=1
Xkpδ(k −kp).

794
Sparse Signal Processing
The variance of this estimator is different from the case when the avail-
able signal samples were at the sampling theorem positions. The condition
that a value of the DFT coefﬁcient at k ̸= kp is zero (with zero variance) if
M = N samples are used, does not hold any more. The total variance can be
estimated as a simple sum of variances
var{X(k)} =
K
∑
p=1
X2
kp M
D
1 −δ(k −kp)
E
.
(10.98)
For small M we have (N −M)/(N −1) ∼= 1 and expressions (10.49) and
(10.98) produce similar results.
In reconstruction we may use the estimated maxima of X(k) obtained
from (10.97). If K positions of nonzero coefﬁcients are correctly estimated
then their values (elements of vector XK) follow from (10.96) as
f= AKXK
XK =
B
AH
K AK
C−1
AH
K f =
B
AH
K AK
C−1
X0K.
Example 10.31. Some of the random realizations of the initial DFT (10.97) for
signal (10.55) are given in Fig.10.30. In contrast to the partial DFT matrix case,
the variance of the estimator (10.97) does not tend to zero as M approaches
to N. However, we can see that the signal frequencies can be detected and
used to recover the signal using (10.37) and (10.39) with known time instants
ti ∈{tn1,tn2,...,tnM} and detected frequencies {k1,k2,...,kK}.
The results for several random realization and nonuniform sampling of
signal (10.55), with recalculated signal values at the sampling theorem posi-
tions, are shown in Fig.10.31. As the number of available samples approaches
to the total number of samples N the reconstructed DFT is again noise-free,
Fig.10.31. For the signal deﬁned by (10.55) the variance of initial DFT is cal-
culated in 100 random realizations of the sets of available samples for the
cases of when the signal is sampled according to the sampling theorem and
for nonuniform sampling without and with recalculation. The results for the
variance is presented in Fig. 10.32. From Fig.10.32 we can conclude that the
recalculation is not efﬁcient for a small number of available samples, when
M ≪N. In that case even slightly worse results are obtained than without re-
calculation, what could be expected, since the recalculated signal with many
inserted zeros is not sparse any more. For a large number of available sam-
ples (in Fig.10.32 for M > 5N/8) the recalculation produces better results,
approaching to the sparse signal without any deviation, for N = M.
The problem with indirect measurements can also be reformulated
using the DFT framework results. If the signal values were available at

Ljubiša Stankovi´c
Digital Signal Processing
795
1
128
257
0
4
8
12
16
M=16
1
128
257
0
16
32
48
64
M=64
1
128
257
0
32
64
96
128
M=128
1
128
257
0
48
96
144
192
M=192
1
128
257
0
56
112
168
224
M=224
1
128
257
0
64
128
192
256
M=257
frequency
signal transform
Figure 10.30
DFT of a signal with various number of available samples M. Available M
samples are taken at random positions within 0 ≤ti ≤T. Dots represent the original signal
DFT values, scaled with M/N to match the mean value of the DFT calculated using a reduced
set of samples signal.
ti ∈TA for M = N the signal values at the sampling theorem positions
could be recovered from this set of available samples, denoted by fN. The
transformation matrix relating samples taken at ti with the signal values at
the sampling theorem positions, according to (10.93), is
⎡
⎢⎢⎣
x(t1)
x(t2)
...
x(tN)
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
b11
b12
...
b1N
b21
b22
...
b2N
...
...
...
...
bN1
bN2
...
bNN
⎤
⎥⎥⎦
⎡
⎢⎢⎣
x(0)
x(1)
...
x(N −1)
⎤
⎥⎥⎦
fN= BNx
and
x = B−1
N fN.
(10.99)
with
bij =
sin[(j −tni/∆t)π]
N sin[(j −tni/∆t)π/N]ej(j−tni /∆t)π/N
If a reduced set of available samples is used we know just M < N
of signal samples/measurements (10.91). Each available sample is a linear

796
Sparse Signal Processing
1
128
257
0
4
8
12
16
M=16
1
128
257
0
16
32
48
64
M=64
1
128
257
0
32
64
96
128
M=128
1
128
257
0
48
96
144
192
M=192
1
128
257
0
56
112
168
224
M=224
1
128
257
0
64
128
192
256
M=257
frequency
signal transform
Figure 10.31
DFT of a signal with various number of available samples M. Available M
samples are a random subset of N nonuniform samples taken at random positions within the
sampling theorem interval. Dots represent the original signal DFT values, scaled with M/N to
match the mean value of the DFT calculated using a reduced set of samples signal.
combination of all signal samples taken at the sampling theorem rate
f (i) = x(tni) = bi1x(0) + bi2x(1) + ... + biNx(N −1)
tni ∈TA = {tn1,tn2,...,tnM}.
For the initial calculation the values at unavailable positions ti /∈TA are
assumed to be zero
yc=[x(tnM+1),x(tnM+2),...,x(tnN)].
Their positions are assumed at the sampling theorem instants, tni = ni∆t for
tni /∈TA, since they are not known anyway,
ni∆t = tni /∈TA.
An illustration for N = 8 is presented in Fig.10.33. The available samples are
f =[x(t0) x(t2) x(t3) x(t4) x(t6) x(t7)]T with assumed unavailable samples
yc = [x(1) x(5)]. They form a complete observation vector for the initial

Ljubiša Stankovi´c
Digital Signal Processing
797
64
128
192
256
0
50
100
150
200
250
300
350
400
450
variance
number of available samples
Figure 10.32
Variance of the DFT for three methods of sampling and various number of
available samples M. (1)-line with marks "x": Available samples a subset of samples taken
at the sampling theorem grid (solid line-theory, marks "x"-statistics). (2)-line with marks
"o": Randomly positioned M samples taken within 0 ≤ti ≤T (solid line-theory, marks "o"-
statistics). (3)-marks "+": Nonuniform randomly shifted samples from the sampling theorem
grid. (4)-marks "*": Nonuniform randomly shifted available samples being recalculated on the
sampling theorem grid.
iteration
x(0)
a
= [x(t0) x(1) x(t2) x(t3) x(t4) x(5) x(t6) x(t7)]T
= [x(t0) 0 x(t2) x(t3) x(t4) 0 x(t6) x(t7)]T .
Using the recalculation procedure, the problem can be solved using
the gradient algorithm as well.
The missing samples yc=[x(nM+1),x(nM+2),...,x(nN)]
ni = qi ∈NQ = {q1,q2,....,qQ} = {nM+1,nM+2,....,nN},
are considered as variables.
The adaptive gradient algorithm is used as follows:

798
Sparse Signal Processing
0
1
2
3
4
5
6
7
8
-4
-2
0
2
4
x(t ),   for   t=tni
time
x(t0)
x(1)
x(t2)
x(t3)
x(t4)
x(5)
x(t6)
x(t7)
Figure 10.33
Illustration of indirect (nonunform) sampling with N = 8 and M = 6 available
samples x(t0), x(t2), x(t3), x(t4), x(t6), x(t7), and two unavailable samples whose positions
are assumed at x(∆t) = x(1) and x(5∆t) = x(5).
-For each missing signal sample position qi ∈NQ the gradient of the
sparsity measure is estimated forming the signal (in the (m + 1)th iteration)
x+
a (qi) = x(m)
a
(qi) + ∆
x−
a (qi) = x(m)
a
(qi) −∆.
The available samples x(tni), tni ∈TA = {tn1,tn2,...,tnM} are unchanged.
Since the sparsity domain is the DFT of signal x =[x(0),x(1),...,x(N −1)]
then the signals x+
a and x−
a are used to recalculate corresponding signals at
the sampling theorem positions x1 and x2 according to (10.99)
x1= B−1
N x+
a
and
x2= B−1
N x+
a
Sparsity minimization using the DFT of these signals X1(k) = DFT[x1(n)]
and X2(k) = DFT[x2(n)], with the estimation of the sparsity measure gradi-
ent
g(qi) = ∑N−1
k=0 |X1(k)| −∑N−1
k=0 |X2(k)|
2N∆
(10.100)
reduces this problem to the problem with the sampling at the sampling
theorem rate. The reconstruction is then based on the same procedure using
the steps (10.82)-(10.83) from the presented algorithm.

Ljubiša Stankovi´c
Digital Signal Processing
799
Example 10.32. Consider the signal deﬁned by (10.84) with M samples at instants
tni ∈TA = {tn1,tn2,...,tnM}
where tni = ni∆t + νni and νni is a uniform random variable −∆t/2 ≤νni ≤
∆t/2. Similar results for the SRR and the average number of iterations, for
various M and sparsity K, are obtained as in Fig.10.28. They will not be re-
peated. A particular realization with K = 6 nonzero DFT coefﬁcients, out of
N = 128, and a number of available samples M = 16 within the transition
region, when the recovery is not always obtained, is considered. The realiza-
tions, when the recovery conditions, for a given signal and for some of the
considered sets of available samples, are met, can be detected. The criterion
for detection of a sparse signal after reconstruction is the measure of signal
sparsity. In this case measures closer to the ℓ0-norm should be used. For ex-
ample, with ℓ1/4-form in the case of a nonsparse reconstruction all transform
coefﬁcients are nonzero with ∑N−1
k=0 |X(k)/N|1/4 ∼N. For a full recovery of a
sparse signal the number of nonzero coefﬁcients (the measure value) is much
lower since K ≪N.
Among 100 performed realizations a possible sparse recovery event is
detected when the described sparsity measure of the result is much lower
than N. The set of DFT coefﬁcient positions for the detected sparse signal
is K = {22, 35, 59, 69, 93, 106}. This sparse reconstruction is checked for
uniqueness using the Test 1. The missing samples are from the set qm ∈NQ.
It is a set difference of all samples N={n |0 ≤n ≤127} and
M = {7, 14, 18, 21, 34, 37, 51, 69, 79, 82, 89, 90, 99, 100, 113, 117}.
For h = 0,1,...,r −1 = 6 corresponding values of Q2h and S2r−h, deﬁned in Test
1, are calculated. Their values are:
h
0
1
2
3
4
5
6
Q2h
112
58
31
16
8
4
2
S27−h
0
0
4
5
4
4
2
.
Note that Q20 = 112 is the total number of missing samples, while Q21 is
obtained by counting odd and even samples in NQ and taking higher number
of these two. Since there are 54 samples at odd positions and 58 samples at
even positions, it means that Q21 = 58.
For h = 2 there are 31 missing sample qm ∈NQ with mod(qm,4) = 0, 26
missing samples with mod(qm,4) = 1, 27 missing samples with mod(qm,4) =
2, and 28 missing samples with mod(qm,4) = 3, resulting in
Q22 = max{31,26,27,28} = 31,
and so on. We can easily conclude that samples x(1) and x(65) are missing,
meaning that Q64 assumes its maximal possible value Q64 = 2.

800
Sparse Signal Processing
Similar counting is done to get S27−h. For example,
S27−6 = S21 =
Q64−1
∑
l=1
P6(l) =
1
∑
l=1
P6(l) = P6(1),
where array P6(l) is obtained by sorting number of even and odd elements in
K. Since there are 2 even and 4 odd elements P6(1) = 2 and P6(2) = 4 resulting
in S21 = 2.
As expected this set of 112 missing samples NQ does not guarantee
a unique solution for an arbitrary signal of sparsity K = 6. By using the
Test 1 with S2r−h = 0 and Q2h presented in the previous table we easily get
that the solution uniqueness for this set NQ and arbitrary signal requires
K < 4. However, for the speciﬁc available signal values, a sparse signal
is reconstructed in this case, with nonzero coefﬁcients at K = {22, 35, 59,
69, 93, 106}. The uniqueness then means that starting from this signal we
cannot ﬁnd another signal of the same sparsity by varying the missing signal
samples positioned at n ∈NQ. Test 1 then gives the answer that this speciﬁc
recovered signal xR(n), with speciﬁc missing sample values and positions
NQ, is unique. It means that starting from xR(n) we cannot get another
signal of the same or lower sparsity by varying the missing samples only. The
reconstructed signal is presented in Fig.10.34. The signal-to-reconstruction-
error ratio deﬁned by (10.85), calculated for all signal samples, is SRR =
111.08 dB. It corresponds to the deﬁned reconstruction algorithm precision
of about 100 dB.
In addition to the considered case two obvious cases in the uniqueness
analysis may appear: 1) when both, the reconstructed signal and the worst
case analysis produce a unique solution using the set of missing samples NQ,
and 2) when both of them produce a result stating that a signal with certain
sparsity cannot be reconstructed in a unique way with NQ. 3) Finally, it is
interesting to mention that there exists a third case when the set of missing
samples can provide a unique reconstruction of sparse signal (satisfying
unique reconstruction condition if it were possible to use ℓ0-norm in the
minimization process), however the ℓ1-norm based minimization does not
satisfy the additional restricted isometry property constraints to produce this
solution (the same solution as the one which would be produced by the ℓ0-
norm). This case will be detected in a correct way using the presented Test
1. It will indicate that a unique solution is possible using NQ, while if the
ℓ1-norm based minimization did not produce this solution as a result of the
reconstruction algorithm, the speciﬁc reconstructed signal will not satisfy the
uniqueness condition.
In the considered complex-valued signal case a sample variation is
done in four directions ±∆± j∆. The estimated gradient vector is complex-
valued.

Ljubiša Stankovi´c
Digital Signal Processing
801
0
16
32
48
64
80
96
112
-7.5
0
7.5
 x(t), xR(n)
time
original and reconstructed signal on the sampling interval grid
0
16
32
48
64
80
96
112
-7.5
0
7.5
 x(ti )
time
available randomly positioned signal samples
Figure 10.34
Available randomly positioned samples x(ti) (dots) of a sparse signal x(t)
(top). Reconstructed signal xR(n) at the sampling theorem positions (crosses) along with the
available samples (dots) (bottom). Continuous-time signal x(t) is presented by solid line.
10.10
PROCESSING OF SPARSE SIGNALS WITH IMPULSIVE NOISE
Processing of signals corrupted with impulsive noise is common situa-
tion in practical applications. Consider a discrete signal x(n), 0 ≤n ≤N −1
which is sparse with sparsity K in, for example DFT domain. Assume that
I samples of the signal x(n) at unknown positions n ∈NQ are corrupted
with impulsive noise ε(n), while the other samples at positions n ∈M are
uncorrupted. The noise ε(n) can be then modeled as: ε(n) = 0 for n ∈M and
ε(n) assumes arbitrary values for n ∈NQ.
If the corrupted samples are considered as unavailable, it is obvious
that the original signal can be reconstructed if a sufﬁcient number of uncor-
rupted samples exists. In this formulation, uncorrupted signal samples are
considered as available observations/measurements.
_____________________________________________________
This Section presents results from: L. Stankovic, M. Dakovic and S. Vujovic, "Reconstruc-
tion of Sparse Signals in Impulsive Disturbance Environments", preprint, 2014. Adapted for
this book by S. Vujovi´c.

802
Sparse Signal Processing
10.10.1
Direct Search Procedure
Very simple and intuitive idea is used ﬁrst to address the problem of this
kind of noise elimination. A random set of M signal samples is used and
considered as available samples/measurements. The number of available
samples should be sufﬁciently large so that the signal of assumed sparsity
K can be reconstructed. Signal is then reconstructed. If nonnoisy samples are
selected then a sparse signal will be obtained. Detection of a sparse signal
reconstruction event is done by measuring sparsity of the obtained signal.
By using a sparsity measure close to l0-norm the reconstruction realizations
containing disturbed samples, will produce nonsparse signal with the value
of sparsity measure close to the total number of samples N. In the case when
only the uncorrupted samples are used in the reconstruction, the sparsity
measure value is of order K, which is much lower than the total number of
samples N. The measure of form
M{X(k)} =
N−1
∑
k=0
|X(k)/N|p ,
(10.101)
can be used with a small p so that its behavior is similar to the l0-norm. In the
calculation with a ﬁnite precision, a sparse recovery will produce very small
(but nonzero) transformation coefﬁcients values X(k) at the positions where
they should be zero. Value of p should be such that |X(k)|p at these positions
is much lower than the value of |X(k)|p at the original nonzero signal
positions. Robustness to small but non-zero values in X(k) is achieved using
p slightly greater than zero, for example p = 1/4. A threshold Tµ within
K < Tµ < N can be used in order to detect a sparse reconstruction event.
Now we will estimate the probability that all samples from a ran-
domly chosen subset are uncorrupted. The total number of samples in this
randomly chosen subset is M, at the positions n ∈M. Probability that the
ﬁrst randomly chosen sample is not affected by the described disturbance
is (N −I)/N since there are N samples in total and N −I of them are un-
corrupted. Similarly, the probability that both the ﬁrst and second chosen
samples are not affected by disturbance is N−I
N
N−I−1
N−1 . In general, probabil-
ity that all of M randomly chosen samples at the positions n ∈M are not
affected by a disturbance is
P(M, N) =
M−1
∏
i=0
N −I −i
N −i
.
(10.102)
The probability P(M, N) decreases as the number of terms in the product
increases, since
N−I−i
N−i
< 1. In order to improve probability of a sparse

Ljubiša Stankovi´c
Digital Signal Processing
803
recovery event, it is important to keep the number of samples M in the
observation set M as low as possible, while satisfying the reconstruction
condition. For a ﬁxed number of pulses I, the expected number of random
realizations to achieve at least one sparse recovery event using a subset of
M samples is 1/P(M, N).
Example 10.33. Consider a N = 128 samples of signal
x(n) =
l
∑
i=1
Ai cos(2πkin/N + φi)
(10.103)
which is sparse in DFT domain with sparsity K = 2l = 6. A 1000 realizations
of reconstructions are performed, and for each realization, the amplitudes,
frequencies, and phases of the signal were taken randomly within 1 ≤Ai ≤2,
1 ≤ki ≤63 and 0 ≤φi ≤2π. Signal x(n) is corrupted by an impulsive noise
ε(n), which is expected in about 12% of the signal values, corresponding
to I = 15 corrupted signal samples, Fig.10.35(a). Since the signal sparsity is
K = 6, the full recovery will be possible, with a high probability, if we use
M = 32 samples in reconstruction, assuming that all of M = 32 samples are
not affected by disturbance (Fig.10.28). Using (10.102) we can calculate proba-
bility that none of M = 32 randomly chosen samples will be affected by noise.
It is P(32,128) = 0.0099. It means that we can expect an order of 10 full recov-
ery realizations in 1000 trials. Impulsive noise used in this example is of the
form ε(n) = ε1(n)/ε2(n) + ε3(n)/ε4(n) + 10ε5(n) where εi(n), i = 1,2,3,4,5,
for n ∈NQ are the unit variance Gaussian noises. It is important to note that
the results do not depend on the disturbance amplitude values or their distri-
bution. The l1-norm is used as sparsity measure in the reconstruction process.
As we can see from Fig.10.35(b) there are some realization of sparse
signal recovery corresponding to the algorithm precision value of SRR. The
measure of reconstructed signal sparsity (10.101) is a criterion for sparse sig-
nal recovery detection, Fig.10.35(d). The sparsity measure values for the cases
when a nonsparse signal is recovered is much higher than K. Low values of
sparsity measure correspond to high SRR, Fig.10.35(b). In Fig.10.35(c), the
realization with the smallest sparsity measure is used to reconstruct the sig-
nal x(n). From Figs.10.35(b) and (d) we can conclude that there are 3 full
sparse signal recoveries in 200 random realizations. The reconstructed sig-
nal is xR(n). In reality we need only one full recovery realization. Calcula-
tion should be stopped when the sparsity measure threshold is reached ﬁrst
time.
10.10.2
Criteria for Selecting Samples
The presented direct search procedure can be used on signal with a small
number of corrupted samples since a number of the random realizations

804
Sparse Signal Processing
0
200
 SRR [dB]
 (b)
realization index
0
200
0
20
40
60 M{X(k)}
 
 (d)
realization index
0
50
100
-20
-10
0
10
20
 x(n)+ε(n) 
 (a)
time
0
50
100
-10
-5
0
5
10  x(n), xR(n)
 (c)
time
Figure 10.35
Reconstruction of a signal with I = 15 out of N = 128 samples being affected
by an impulsive disturbance. In each realization 96 randomly chosen samples are removed.
Total number of realizations is 200. a) The available corrupted signal; b) The SRR for each of
200 realizations; c) The original (black line) and the reconstructed (dots) signal for the best
realization; d) The sparsity measure for each of 200 realizations.
required to have an uncorrupted subset of signal samples increases with
the number of corrupted samples.
10.10.2.1
L-statistics Based Elimination
In some applications the impulsive noise is much stronger than the signal.
The trimmed L-statistics can be used to eliminate the corrupted signal
samples, without any search procedure. The values of signal samples x(n)
are ordered into a nonincreasing sequence
|x(n1)| ≥... ≥|x(ni)| ≥|x(ni+1)| ≥... ≥|x(nN)|.
(10.104)
If strong impulsive noise components exist, well above the signal level, then
very large absolute values of signal samples should be omitted as corrupted.

Ljubiša Stankovi´c
Digital Signal Processing
805
After these samples are removed then the remaining M < N samples
y = {x(n1),...,x(nM)}.
are used as the available observations in the signal reconstruction. The
number of omitted samples or the threshold for signal samples elimination
are studied within the robust analysis.
The L-statistics and the direct search methods can be combined. Some
of the corrupted samples may be eliminated based on their values using
the L-statistics, while a small number of corrupted samples at unknown
positions can be found by a random selection of subsets. This approach is
based on a priori knowledge of impulsive disturbance.
10.10.2.2
Iterative procedure
A criterion that will mark some signal samples as probably more corrupted
than the others is presented next. In this process, no particular distribution
or number of corrupted samples is assumed.
Consider a corrupted signal xε(n) = x(n) + ε(n). For each time instant
we will form two signals x+
a (n) = xε(n) + ∆δ(n −m) and x−
a (n) = xε(n) −
∆δ(n −m), where m = 0,..., N −1. Then, a difference of measure values is
calculated as
g(m) =
N−1
∑
k=0
''X+
a (k)
'' −
N−1
∑
k=0
''X−
a (k)
'',
(10.105)
where X+
a (k) = DFT[x+
a (n)] and X−
a (k) = DFT[x−
a (n)]. For a large step size
∆, according to (10.75) g(m) ∼ε(m). The signal samples at the positions
where the value |g(m)| is highest are eliminated and considered as un-
available in the reconstruction. However, in the remaining samples there
could also exist some samples corrupted with noise. In order to remove
the remaining noisy samples, two procedures can be used. First one is to
apply direct search over the remaining samples. The second one is based
on repeating the previous difference of measures based elimination in an
iterative way.
Algorithm:
Step 0: At the beginning, denote with NQ set of signal sample po-
sitions which are selected with criterion (large |g(m)|). Most of them are
heavily corrupted by noise. The goal is to locate remaining corrupted sam-
ples. The set of remaining sample positions M is a set complement of NQ.
Step 1: For each sample in M perform reconstruction under the as-
sumption that the considered sample is also unavailable, i.e. that it belongs
to NQ. Comparing concentration measure of reconstructed signals before

806
Sparse Signal Processing
and after adding each considered samples to NQ, we can ﬁnd candidates
for corrupted samples. This analysis is a consequence of the fact that the re-
moval of a sample with a high noise will signiﬁcantly improve sparsity mea-
sure of the recovered signal. Sparsity measure will almost not be changed
by removing a sample with low/no noise.
Step 2: Choose few r = 1,2,3, or 4 samples from M after whose
removal the best improvement in measure value was produced in Step 1.
Move them from M to NQ and repeat Steps 1-2 with the new NQ and M.
Comment: For r = 1 only the sample causing maximal measure change
will be moved from one to another set. In order to make procedure more
efﬁcient we may remove r = 2 or r = 3 samples since the procedure detects
few the largest changes with a high reliability. At the end of It iterations, in
total Itr samples with remaining noise could be removed.
Example 10.34. Consider N = 128 samples of a signal deﬁned by (10.84) with
sparsity K = 10. The disturbance of form ε(n) = 40(ε1(n) −0.5) + 40(ε2(n) −
0.5) is used, where ε1(n) and ε2(n) are white uniform noises. The number
of samples affected by a disturbance is I = 64. This kind of disturbance is
chosen since a large number of its values are within the signal amplitude
range at the positions where the disturbance exits. The iterative removal
procedure is used for reconstruction, with r = 4 samples being added to
the set of unavailable/corrupted samples NQ in each iteration. In an ideal
case if there is no miss-detection, all corrupted samples from this example
will be removed in 64/4 = 16 iterations. The more realistic scenario is that
there exist few missdetections. Results of reconstruction are presented in Fig.
10.36. The numbers of omitted samples in the realizations were Q = 72 for the
considered signals. It means that just a few miss-detections existed. In Fig.
10.37 are presented disturbance values, in the order as they were detected
and omitted by the algorithm. Note that the algorithm followed quite well
the signiﬁcance order of the disturbance in their omission. In order to more
accurately check this procedure, the same process is repeated 100 times with
arbitrary signal amplitudes and frequencies. In all realizations, all corrupted
samples where among 72 samples selected by the criterion.
10.10.3
Uniqueness of the Obtained Solution
After a sparse signal is reconstructed from a reduced set of samples its
uniqueness should be conﬁrmed. In theory, even if the reconstructed signal
corresponds to the original signal at the instants of available samples, it still
does not mean that there does not exist another signal satisfying the same
set of available samples.

Ljubiša Stankovi´c
Digital Signal Processing
807
0
16
32
48
64
80
-50
0
50
100
150
 SRR [dB] 
 K=10
 (a)
removed samples
0
16
32
48
64
80
0
50
100
150
removed samples
 (b)
M{X(k)} 
K=10 
Figure 10.36
Reconstruction of a sparse signal when corrupted samples are removed by using
the criterion in iterative way. In each iteration r = 4 samples are removed. a) The SRR during
the iterations. b) The sparsity measure during the iterations for a signal of sparsity K = 10.
20
40
60
80
100
120
0
10
20
30
40
2A   
sorting index
disturbance samples  
K=10  
Figure 10.37
Disturbance values in the signal, sorted according to the introduced signiﬁcance
criterion, with signal range in amplitude 2A.
Here we will illustrate the presented uniqueness test on the signal
from the last example, whose sparsity is K = 10 and the algorithm has
removed Q = 72 out of N = 128 samples. Using the theorem for the speciﬁc
set of removed samples NQ we obtained the sparsity limit K < 16. It means
that the reconstruction is unique.
For the same number of missing samples the theorem is run 100,000
times with arbitrary possible distribution of Q = 72 removed sample posi-
tions. The probability that a signal with sparsity K is unique, with randomly
removed Q = 72 samples is presented in Fig.10.38. Probability that the worst

808
Sparse Signal Processing
0
5
10
15
20
25
30
35
10
-4
10
-3
10
-2
10
-1
10
0
sparsity K
uniqueness probability, Q=72 out of N=128
Figure 10.38
Sparsity limit probability distribution for the worst possible case of signal with
Q = 72 out of N = 128 samples in 100,000 random realizations.
case signal with sparsity K = 10 is unique for Q = 72 is
Probability[K = 10 is unique, with Q = 72] = 0.8723.
10.11
IMAGE RECONSTRUCTION
The gradient based algorithm is applied on the image x(n,m). As the
transformation domain the two-dimensional DCT (in symmetric form) will
be used
C(k,l) = vkvl
N−1
∑
m=0
N−1
∑
n=0
x(m,n)cos
*2π(2m + 1)k
4N
+
cos
*2π(2n + 1)l
4N
+
,
where v0 = √
1/N and vk = √
1/N for k ̸= 0. Assume that random set of
pixels is available (not corrupted) at (n,m) ∈M. The goal is to reconstruct
_________________________________________
This section is written by Isidora Stankovi´c.

Ljubiša Stankovi´c
Digital Signal Processing
809
unavailable pixels. In order to apply a CS reconstruction algorithm, the
image sparsity is assumed in the DCT domain. The DCT of an image
is usually calculated by using 8x8 blocks. Most of the common images
could be considered as sparse in the DCT domain without any additional
processing. If we want to be sure that the original image, which will be
processed, is sparse we can pre-process it by calculating the DCT of its
8x8 blocks and set the lowest amplitude coefﬁcients to zero. By making
the image sparse in the DCT domain we will not make a notable visual
difference with respect to the original image.
Using the available pixels (measurements), an initial image is formed.
It assumes the original image values at the positions of available pixels,
while the missing pixels are set to zero (or arbitrary) value. This new image
is deﬁned as
x(0)
a (m,n) =
!
x(m,n)
for
(n,m) ∈M
0
for
(n,m) ∈NQ
Note that for the missing pixels any value within the possible image
values range can be assumed in the initial step. The algorithm will recon-
struct the true image values at these positions. For graphical representation
of missing pixels the value 255 corresponding to a white pixel or 0 will be
used. Then the corrupted pixels are black or white pixels, Fig. 10.39.
For each missing sample signals x+
a (m,n) and x+
a (m,n) are formed:
x+
a (m,n) = x(p)(m,n) + ∆δ(m −mi,n −ni)
x−
a (m,n) = x(p)(m,n) −∆δ(m −mi,n −ni).
(10.106)
The ﬁnite difference of the signal transform measure is calculated
g(mi,ni) = ∥C+
a (k,l)∥1 −∥C−
a (k,l)∥1
2∆
(10.107)
where C+
a (k,l) = DCT[x+
a (m,n)] and C−
a (k,l) = DCT[x−
a (m,n)].
A gradient matrix Gm,n is of the same size as the image. At the
positions of available samples (n,m) ∈M, this matrix has zero value, Gm,n =
0. At the missing sample positions n ∈NQ its values are Gm,n = g(m,n),
calculated using (10.107).
The image values are corrected iteratively as
x(p)
a (m,n) = x(p−1)
a
(m,n) −2∆Gm,n.
(10.108)
The change of step ∆and the stopping criterion are the same as in one-
dimensional case. The results in 50 iterations are shown in Fig. 10.39. Re-
constructed image after 1, 3, and 50 iterations are presented.

810
Sparse Signal Processing
Noisy image
Reconstructed image.  Iteration:   1
Reconstructed image.  Iteration:   3
Reconstructed image.  Iteration:  50
Figure 10.39
Reconstruction of image using the gradient-based algorithm.

Index
Adaptive reconstruction, 771
Adaptive systems, 423
Allpass system, 246
Ambiguity function, 629
Analog signals, 16
Analytic part, 40
Antenna array, 473
Anticausal systems, 186
Attenuation, 56
Auto-regressive (AR), 185
Autocorrelation function, 336
Autocovariance function, 336
Backward difference, 226
Bandpass ﬁlter, 244
Bilinear transform, 230
Binomial random variable, 342
Blackman window, 534
Block LMS algorithm, 482
Born-Jordan distribution, 657
Butterworth ﬁlter, 53, 240
discrete-time, 237
Capon’s method, 614
local polynomial Fourier transform (LPFT),
620
short-time Fourier transform (STFT),
618
Cascade realization, 276
Causal system, 34, 67
Causal systems, 181
Central limit theorem, 344
Characteristic polynomial, 186
Choi-Willimas distribution, 663
Cohen class of distributions, 655
discrete form, 660
kernel decomposition, 662
Coherence, 684, 687
Complex sinusoidal signal, 22
Compressive sensing, 666
Continuous signals, 21
Convolution
circular, 115
continuous, 33
discrete-time, 65
in frequency domain, 39, 73
Cosine series, 29
Derivative
complex function, 26
Difference equation, 183, 186
Differential equation, 51
Differentiator, 73
Digital signals, 16
Direct realization I, 268
Direct realization II, 268
Dirichlet conditions, 24
Discrete Cosine transform (DCT), 140
Discrete Fourier transform (DFT), 107, 124
Discrete Hartley transform (DCT), 165
Discrete pseudo Wigner distribution, 639
Discrete Sine transform (DST), 143
Discrete system, 64
Discrete-time signals (discrete signals), 59
Displacement, 137
Downsampling, 576
Duality property, 36
Eigenvalues, 438
Eigenvectors, 438
Energy, 24, 63
Equiangular Tight Frame, 686
Ergodicity, 337
811

812
Index
Error function, 345
Error signal
adaptive system, 429
Fast Fourier transform, 126
decimaton-in-frequency, 126
decimaton-in-time, 128
Finite impulse response (FIR), 185
frequency domain design, 291
realization, 284
First-order statistics, 319
Fixed point arithmetic, 382
Floating point arithmetic, 390
IEEE standard, 392
mu-law and A-law, 392
Fourier series, 27, 41, 122
Fourier transform, 35, 41, 122
matrix, 112
of discrete-time signals, 67, 124
properties, 37
signum function, 37
Fractional Fourier transform, 613
relation to the LPFT, 614
windowed, 614
Frequency estimation, 136
Goertzel algorithm, 188
Gradient, 768
Gramm matrix, 692
Group delay, 46, 250, 285, 633
Haar transform, 152, 575
Hadamard transform, 151
Hamming window, 533
Hann(ing) window, 70, 531
Hartley series, 29
Highpass ﬁlter, 242
Hilbert transform, 40
Homogeneous equation, 187
Image reconstruction, 809
Impulse invariance method, 218
Impulse signal
continuous (delta function), 22
discrete-time, 60
Indirect measurements, 792
Inﬁnite impulse response (IIR), 185
Initial condition
continuous, 50
Instantaneous frequency, 42, 633
Interpolation, 120
Inverse system, 247
Isometry, 683
ISTA algorithm, 769
Kalman ﬁlter, 495
Kronecker delta function, 60
L-statistics, 331
Lagrangian, 766
Laplace transform, 48
LASSO minimization, 767
Leakage effect, 135
Linear adaptive adder, 427
Linear phase systems, 285
Linear system, 33
LMS algorithm, 457
antenna systems, 473
block, 482
complex, 487
convergence, 459
echo cacellation, 479
identiﬁcation, 460
noise cancelation, 464
prediction, 470
sign, 481
sinusoidal disturbance, 467
variable step, 485
Local polynomial Fourier transform, 610
moments, 611
relation to fractional Fourier transform,
613
Lowpass ﬁlter, 236
Magnitude, 24
Marginal properties, 653
Matched ﬁlter, 369
Matched z-transform method, 223
Measurement Matrix, 679
Bernoulli Random, 682
Gaussian Random, 681
Indirect, 680
Partial DFT, 681
Structured Random, 682, 793
Median, 329, 356
Minimum phase system, 247
Moments
LPFT, 612
Morlet wavelet, 571
Moving average (MA), 185

Ljubiša Stankovi´c
Digital Signal Processing
813
Moyal property, 635
MUSIC, 618
Narrowband signals
spectral estimation, 366
Neural networks, 501
activation function, 503
acyclic, 506
continuous output, 512
cyclic, 506
error backpropagation, 516
layer, 506
multilayer, 516
network function, 503
perceptron, 508
supervised, 507
unsupervised, 519
voting mashines, 519
Neuron, 502
Noise, 340
binary, 341
compex Gaussian, 349
Gaussian, 344
Laplacian, 350
missing samples, 718
reconstruction, 729
unform, 340
Noisy signal
Fourier transform, 352
Norm zero, 710
Norm-one, 735
ball, 748
Norm-zero, 677
Notch ﬁlter, 182, 212
Optimal ﬁlter, 372, 434
Orthogonality principle, 445
Overﬂow, 384
Parallel realization, 280
Parseval’s theorem, 74, 114, 682
Perceptron, 508
Period of a discrete signal, 62
Pisarenko method, 619
Power, 24, 63
Power spectral density, 338, 362
Probability, 326
density function, 326
Probability density function, 328
Quantization, 376
Random signals, 319
Rank of matrix, 704
Rayleigh distribution
unform, 349
Reconstruction uniqueness, 783
Reconstruction uniquness, 787
Rectangular window, 70
Recursive systems
adaptive, 493
Reduced interference distributions
discrete form, 657
Region of convergence, 170
Resolution, 533
Restricted isometry, 683, 687
constant, 683, 688
eigenvalues, 691
uniqueness, 701
Ridge regression, 767
RLS algorithm
variable step, 489
S-method, 647
S-transform (the Stockwell transform), 607
Sampling
nonunform, 792
Sampling theorem, 101
for periodic signals, 130
in the frequency domain, 41
in the time domain, 77
Schwartz’s inequality, 686
Second-order statistics, 336
Sensitivity of system, 271
Short-time Fourier transform (STFT), 522
discrete, 538
discrete-time, 535
ﬁlter bank, 542
frequency-varying, 567
hybrid, 569
inversion, 529, 546
optimal window, 525
optimisation, 560
overlapping, 544
recursive, 541
time-varying, 556
Sign LMS algorithm, 481
Sinc distribution
discrete form, 657
Soft-thresholding, 769

814
Index
Spark of matrix, 704
coherence, 760
uniqueness, 706
Sparse signals, 666
Sparsity, 676
Stable system, 34, 67
Stable systems, 181
Standard deviation, 331
Stationary phase method, 42
Stationary signals, 337
Steepest descend method, 446
Taylor series, 42
Trace of matrix, 449
Unit step signal
continuous (Heaviside function), 21
discrete-time, 60
Unitary matrix, 682
Upsampling, 578
Variance, 331
Voting mashines, 519
Walsh-Hadamard transform, 151
Wavelet transform, 569
Coifﬂet, 606
Daubechies D4, 588
Daubechies D6, 602
ﬁlter bank, 574
Haar, 585
orthogonality, 582
reconstruction condition, 579
scale function, 600
wavelet function, 600
Welch bound, 684
Welch periodogram, 368
Wide sense stationary signals, 337
Wiener ﬁlter, 372
Wigner distribution, 623, 656
auto-terms, 627
cross-terms, 627
discrete form, 639
properties, 631
pseudo, 636
S-method, 647
smoothed, 637
Window, 529
Bartlett (triangular), 289, 530
Blackman, 534
Hamming, 291, 533
Hann(ing), 291
Hann(ing) (Hann), 531
Kaiser, 535
rectangular, 529
Windows, 287
Yule-Walk equation, 367
z-transform, 169, 191
inverse, 174
Zero-padding, 120
Zhao-Atlas-Marks distribution
discrete form, 657

Bibliography
[1] S. T. Alexander, Adaptive Signal Processing: Theory and Applications,
Springer-Verlag, 1986.
[2] R. Allred, Digital Filters for Everyone, CreateSpace Independent Publish-
ing Platform; 2 edition, 2013.
[3] M. Amin, Compressive Sensing for Urban Radar, CRC Press, 2014.
[4] A. Antoniou, Digital Signal Processing: Signals, Systems, and Filters,
McGraw-Hill Education, 2005.
[5] F. Auger and F. Hlawatsch, Time-Frequency Analysis: Concepts and Meth-
ods, Wiley-ISTE, 2008.
[6] D. Blandford and J. Parr, Introduction to Digital Signal Processing, Pren-
tice Hall, 2012.
[7] B. Boashash, Time-Frequency Signal Analysis and Processing: A Compre-
hensive Reference, Second Edition, Academic Press, 2015.
[8] P. Bremaud, Mathematical Principles of Signal Processing: Fourier and
Wavelet Analysis, Springer, 2002.
[9] S. A. Broughton, Discrete Fourier Analysis and Wavelets: Applications
to Signal and Image Processing, Wiley-Interscience, 2008.
[10] L. F. Chaparro, Signals and Systems using MATLAB, Academic Press,
2011.
[11] C. T. Chen, Signals and Systems: A Fresh Look, CreateSpace Independent
Publishing Platform, 2011.
815

816
Bibliography
[12] V. Chen, D. Tahmoush, and W. J. Miceli, Radar Micro-Doppler Signature-
Processing and Applications, The Institution of Engineering and Technol-
ogy (IET), 2014.
[13] L. Cohen, Time-frequency Analysis, Prentice-Hall, 1995.
[14] A. G. Constantinides, System Function of Discrete-Time Systems, Aca-
demic Press, 2001.
[15] M. Davenport, Digital Signal Processing, Kindle edition, 2011.
[16] P. S. R. Diniz, E. A. B. da Silva, and S. L. Netto, Digital Signal Processing:
System Analysis and Design , Cambridge University Press; 2 edition,
2010.
[17] I. Daubechies, Ten Lectures on Wavelets, SIAM, 1992.
[18] M. Elad, Sparse and Redundant Representations: From Theory to Applica-
tions in Signal and Image Processing, Springer, 2010.
[19] Y. C. Eldar and G. Kutyniok Compressed Sensing: Theory and Applications,
Cambridge University Press, 2012.
[20] M. E. Frerking, Digital Signal Processing in Communication Systems,
Springer, 1994.
[21] P. Flandrin, Time-Frequency/Time-Scale Analysis, Academic Press, 1999.
[22] R. C. Gonzalez and R. E. Woods, Digital Image Processing, Prentice Hall,
2007.
[23] N. Hamdy, Applied Signal Processing: Concepts, Circuits, and Systems,
CRC Press, 2008.
[24] M. H. Hayes, Schaums Outline of Digital Signal Processing, 2nd Edition,
Schaum’s Outline Series, 2011.
[25] S. Haykin, Least-Mean-Square Adaptive Filters, Wiley-Interscience, 2003.
[26] S. Haykin and B. Van Veen, Signals and Systems, Wiley, 2002.
[27] E. Ifeachor and B. Jervis, Digital Signal Processing: A Practical Approach,
Prentice Hall; 2 edition, 2001.
[28] V. K. Ingle and J. G. Proakis, Digital Signal Processing Using MATLAB,
CL Engineering; 3 edition, 2011.

Ljubiša Stankovi´c
Digital Signal Processing
817
[29] S. Kay, Fundamentals of Statistical Signal Processing, Volume I: Estimation
Theory, Prentice Hall, 1993.
[30] S. Kay, Fundamentals of Statistical Signal Processing, Volume II: Detection
Theory, Prentice Hall, 1998.
[31] E. Kudeki and D. C. Munson Jr., Analog Signals and Systems, Prentice
Hall, 2008.
[32] S. M. Kuo, B. H. Lee, and W. Tian, Real-Time Digital Signal Process-
ing: Fundamentals, Implementations and Applications, Wiley, third edition,
2013.
[33] B. P. Lathi and Z. Ding, Modern Digital and Analog Communication
Systems , Oxford University Press, 2009.
[34] B. P. Lathi and R. A. Green, Essentials of Digital Signal Processing, Cam-
bridge University Press, 2014.
[35] S. R. Laxpati and V. Goncharoff, Practical Signal Processing and its Appli-
cations, Part I, CreateSpace Independent Publishing Platform, 2013.
[36] R. G. Lyons, Understanding Digital Signal Processing, Pearson, 2010.
[37] S. Mallat, A Wavelet Tour of Signal Processing, Academic Press, second
edition, 1999.
[38] D. P. Mandi´c and S. L. Goh, Complex Valued Nonlinear Adaptive Filters:
Noncircularity, Widely Linear and Neural Models, John Wiley & Sons,
2009.
[39] D. P. Mandi´c, M. Golz, A. Kuh, D. Obradovic, and T. Tanaka, Signal
Processing Techniques for Knowledge Extraction and Information Fusion,
Springer, 2008.
[40] D. G. Manolakis and V. K. Ingle, Applied Digital Signal Processing: Theory
and Practice, Cambridge University Press, 2011.
[41] D. G. Manolakis, D. Manolakis, V. K. Ingle, S. M. and Kogon, Statisti-
cal and Adaptive Signal Processing: Spectral Estimation, Signal Modeling,
Adaptive Filtering and Array Processing, Artech House, 2005.
[42] J. H. McClellan, R. W. Schafer, and M. A. Yoder, Signal Processing First,
Prentice Hall, 2003.
[43] S. K. Mitra, Digital Signal Processing, McGraw-Hill, 2010.

818
Bibliography
[44] B. Mulgrew, P. Grant, and J. Thompson, Digital Signal Processing: Con-
cepts and Applications, Palgrave Macmillan; 2nd Edition edition, 2002.
[45] R. Newbold, Practical Applications in Digital Signal Processing, Prentice
Hall, 2012.
[46] A. V. Oppenheim and A. S. Willsky, Signals and Systems, Prentice Hall,
2008.
[47] A. V. Oppenheim and R. W. Schafer, Discrete-Time Signal Processing,
Prentice Hall, 2009.
[48] A. V. Oppenheim and G. C. Verghese, Signals, Systems and Inference,
Prentice Hall, 2015.
[49] A. Papulis, Signal Analysis, McGraw-Hill, 1997
[50] C. L. Phillips, J. Parr, and E. Riskin, Signals, Systems, and Transforms,
Prentice Hall, 2013.
[51] B. Porat, A Course in Digital Signal Processing, Wiley, 1996.
[52] J. G. Proakis and D. G. Manolakis, Digital Signal Processing: Principles,
Algorithms, and Applications, Pearson; 4 edition, 2013.
[53] A. Quinquis, Digital Signal Processing Using Matlab, Wiley-ISTE, 2008.
[54] L. R. Rabiner and B. Gold, Theory and Application of Digital Signal
Processing, Prentice Hall, 1975.
[55] M. A. Richards, Fundamentals of Radar Signal Processing, McGraw-Hill
Education, second edition, 2014.
[56] M. J. Roberts, Signals and Systems: Analysis Using Transform Methods &
MATLAB, McGraw-Hill Education, 2011.
[57] A. H. Sayed, Adaptive Filters, Wiley-IEEE, 2008.
[58] L. L. Scharf, Statistical Signal Processing: Detection, Estimation, and Time
Series Analysis, Prentice Hall, 1991.
[59] M. Soumekh, Synthetic Aperture Radar Signal Processing with MATLAB
Algorithms, Wiley-Interscience, 1999.
[60] L. Stankovi´c, Digital Signal Processing, Naucna knjiga, Beograd, Second
edition 1989 (in Montenegrin/Serbian/Croat/Bosnian).

Ljubiša Stankovi´c
Digital Signal Processing
819
[61] L. Stankovi´c, M. Dakovi´c, and T. Thayaparan, Time-frequency Signal
Analysis with Applications, Artech House, Boston, 2013.
[62] L.
Stankovi´c
and
I.
Djurovi´c,
Solved
Problems
in
Digital
Sig-
nal
Processing,
University
of
Montenegro,
1998
(in
Montene-
grin/Serbian/Croat/Bosnian).
[63] S. Stankovi´c, I. Orovi´c, E. Sejdi´c, Multimedia Signals and Systems,
Springer, second edition, 2015.
[64] H. Stark and J. W. Woods, Probability and Random Processes with Applica-
tions to Signal Processing, Prentice Hall, third edition, 2001.
[65] L. Tan and J. Jiang, Digital Signal Processing: Fundamentals and Applica-
tions, Academic Press; 2 edition , 2013.
[66] S. Theodoridis and R. Chellappa, eds., Academic Press Library in Signal
Processing, Vols.1-4, Academic Press, 2013.
[67] A. Uncini, Fundamentals of Adaptive Signal Processing (Signals and Com-
munication Technology), Springer, 2014.
[68] M.Vetterli and J.Kovaˇcevi´c, Wavelets and Subband Coding, CreateSpace
Independent Publishing Platform, 2013.
[69] M. Vetterli, J. Kovaˇcevi´c, and V. K. Goyal, Foundations of Signal Process-
ing, Cambridge University Press, 2014.
[70] B. Widrow and D. Stearns, Adaptive Signal Processing, Prentice Hall,
1985.

About the Author
Ljubiša Stankovi´c was born in Montenegro on June 1, 1960. He received
a BSc degree in electrical engineering from the University of Montenegro
in 1982 with the award as the best student at the University. As a student
he won several competitions in mathematics in Montenegro and former
Yugoslavia. He received an MSc degree in communications from the Uni-
versity of Belgrade, and a PhD in theory of electromagnetic waves from the
University of Montenegro in 1988. As a Fulbright grantee, he spent 1984-
1985 academic year at the Worcester Polytechnic Institute, Worcester, MA.
Since 1982, he has been on the faculty at the University of Montenegro,
where he has been a full professor since 1995. In 1997-1999, he was on leave
at the Ruhr University Bochum, Germany, supported by the Alexander von
Humboldt Foundation. At the beginning of 2001, he was at the Technische
Universiteit Eindhoven, The Netherlands, as a visiting professor. During
the period of 2003-2008, he was the rector of the University of Montenegro.
He was ambassador of Montenegro to the United Kingdom, Iceland, and
Ireland 2011-2015. During his stay in United Kingdom he was a visiting aca-
demic at the Imperial College London, 2013-2014. His current interests are in
signal processing. He published about 400 technical papers, more than 140
of them in the leading journals. Stankovi´c received the highest state award
of Montenegro in 1997 for scientiﬁc achievements.
Stankovi´c was an associate editor of the IEEE Transactions on Image
Processing, an associate editor of the IEEE Signal Processing Letters, and an
associate editor of the IEEE Transactions on Signal Processing. Stankovi´c is
a member of the Editorial Board of Signal Processing. He is a member of the
National Academy of Sciences and Arts of Montenegro (CANU) since 1996
and a member of the European Academy of Sciences and Arts. Stankovi´c is
a Fellow of the IEEE for contributions to time-frequency signal analysis.
820


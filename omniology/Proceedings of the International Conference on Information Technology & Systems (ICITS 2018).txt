Advances in Intelligent Systems and Computing 721
Álvaro Rocha
Teresa Guarda   Editors
Proceedings of the 
International Conference 
on Information 
Technology & Systems 
(ICITS 2018)

Advances in Intelligent Systems and Computing
Volume 721
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Advances in Intelligent Systems and Computing” contains publications on theory,
applications, and design methods of Intelligent Systems and Intelligent Computing. Virtually
all disciplines such as engineering, natural sciences, computer and information science, ICT,
economics, business, e-commerce, environment, healthcare, life science are covered. The list
of topics spans all the areas of modern intelligent systems and computing.
The publications within “Advances in Intelligent Systems and Computing” are primarily
textbooks and proceedings of important conferences, symposia and congresses. They cover
signiﬁcant recent developments in the ﬁeld, both of a foundational and applicable character.
An important characteristic feature of the series is the short publication time and world-wide
distribution. This permits a rapid and broad dissemination of research results.
Advisory Board
Chairman
Nikhil R. Pal, Indian Statistical Institute, Kolkata, India
e-mail: nikhil@isical.ac.in
Members
Rafael Bello Perez, Universidad Central “Marta Abreu” de Las Villas, Santa Clara, Cuba
e-mail: rbellop@uclv.edu.cu
Emilio S. Corchado, University of Salamanca, Salamanca, Spain
e-mail: escorchado@usal.es
Hani Hagras, University of Essex, Colchester, UK
e-mail: hani@essex.ac.uk
László T. Kóczy, Széchenyi István University, Győr, Hungary
e-mail: koczy@sze.hu
Vladik Kreinovich, University of Texas at El Paso, El Paso, USA
e-mail: vladik@utep.edu
Chin-Teng Lin, National Chiao Tung University, Hsinchu, Taiwan
e-mail: ctlin@mail.nctu.edu.tw
Jie Lu, University of Technology, Sydney, Australia
e-mail: Jie.Lu@uts.edu.au
Patricia Melin, Tijuana Institute of Technology, Tijuana, Mexico
e-mail: epmelin@hafsamx.org
Nadia Nedjah, State University of Rio de Janeiro, Rio de Janeiro, Brazil
e-mail: nadia@eng.uerj.br
Ngoc Thanh Nguyen, Wroclaw University of Technology, Wroclaw, Poland
e-mail: Ngoc-Thanh.Nguyen@pwr.edu.pl
Jun Wang, The Chinese University of Hong Kong, Shatin, Hong Kong
e-mail: jwang@mae.cuhk.edu.hk
More information about this series at http://www.springer.com/series/11156

Álvaro Rocha
• Teresa Guarda
Editors
Proceedings of the
International Conference
on Information
Technology & Systems
(ICITS 2018)
123

Editors
Álvaro Rocha
DEI/FCT
Universidade de Coimbra
Coimbra
Portugal
Teresa Guarda
Systems Department
Universidad Estatal Península de Santa
Elena
La Libertad
Ecuador
ISSN 2194-5357
ISSN 2194-5365
(electronic)
Advances in Intelligent Systems and Computing
ISBN 978-3-319-73449-1
ISBN 978-3-319-73450-7
(eBook)
https://doi.org/10.1007/978-3-319-73450-7
Library of Congress Control Number: 2017963760
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This book contains a selection of papers accepted for presentation and discussion at
the 2018 International Conference on Information Technology & Systems
(ICITS’18). This conference had the support of the State University of Península de
Santa Elena, IEEE Systems, Man, and Cybernetics Society, and AISTI (Iberian
Association for Information Systems and Technologies). It took place at Libertad,
Península de Santa Elena, Ecuador, January 11–13, 2017.
The 2018 International Conference on Information Technology & Systems
(ICITS’18) is an international forum for researchers and practitioners to present and
discuss the most recent innovations, trends, results, experiences, and concerns in the
several perspectives of Information Technology & Systems.
The Program Committee of ICITS’18 was composed of a multidisciplinary
group of 110 experts and those who are intimately concerned with Information
Systems and Technologies. They have had the responsibility for evaluating, in a
“double-blind review” process, the papers received for each of the main themes
proposed for the conference: (A) Information and Knowledge Management;
(B) Organizational Models and Information Systems; (C) Software and Systems
Modeling;
(D)
Software
Systems,
Architectures,
Applications,
and
Tools;
(E) Multimedia Systems and Applications; (F) Computer Networks, Mobility, and
Pervasive Systems; (G) Intelligent and Decision Support Systems; (H) Big Data
Analytics
and
Applications;
(I)
Human–Computer
Interaction;
(J)
Ethics,
Computers & Security; (K) Health Informatics; (L) Information Technologies in
Education.
ICITS’18 also included several workshop sessions taking place in parallel with
the conference ones. They were sessions of the WMETACOM 2018—1st
Workshop on Media, Applied Technology and Communication.
ICITS’18 received about 200 contributions from 31 countries around the world.
The papers accepted for presentation and discussion at the conference are published
by Springer (this book) and by AISTI and will be submitted for indexing by ISI,
EI-Compendex, SCOPUS, DBLP, and/or Google Scholar, among others.
v

We acknowledge all of those that contributed to the staging of ICITS’18 (au-
thors, committees, workshop organizers, and sponsors). We deeply appreciate their
involvement and support that was crucial for the success of ICITS’18.
January 2017
Álvaro Rocha
Teresa Guarda
vi
Preface

Organization
Conference
Organizing Committee
Datzania Villao
State University of Santa Elena Peninsula, Ecuador
Shendry Rosero Vasquez
State University of Santa Elena Peninsula, Ecuador
Teresa Guarda
State University of Santa Elena Peninsula, Ecuador
Scientiﬁc Committee
Álvaro Rocha (Chair)
University of Coimbra, Portugal
Abderrazak Sebaa
Bejaia University, Algeria
Adefemi Falase
Ajayi Crowther University, Nigeria
Abdulmotaleb El Saddik
University of Ottawa, Canada
Alexandru Vulpe
University Politehnica of Bucharest, Romania
Amal Al Ali
University of Sharjah, United Arab Emirates
André da Silva
IFSP and NIED/UNICAMP, Brazil
André Marcos Silva
University Adventist of São Paulo, Brazil
Andrés Melgar
Pontiﬁcia Universidad Católica del Perú, Peru
Angeles Quezada
Universidad Autonoma de Baja California, Mexico
Ania Cravero
University de La Frontera, Chile
Antonio Jara
HES-SO, Switzerland
Antonio Osorio
University of Minho, Portugal
Arnulfo Alanis Garza
Tijuana Institute of Technology, Mexico
Anushia Inthiran
University of Canterbury, New Zealand
Azeddine Chikh
University of Tlemcen, Algeria
Borja Bordel
Universidad Politécnica de Madrid, Spain
Carlos Carreto
Polytechnic of Guarda, Portugal
Carlos Grilo
Polytechnic of Leiria, Portugal
Dalila Durães
Technical University of Madrid, Spain
vii

Dália Filipa Liberato
ESHT/IPP, Portugal
Daniela Benalcázar
Universidad Técnica de Ambato, Ecuador
Dante Carrizo
Universidad de Atacama, Chile
Diego Marcillo
Universidad de las Fuerzas Armadas ESPE, Ecuador
Diego Ordóñez-Camacho
Universidad Tecnológica Equinoccial, Ecuador
Eddie Galarza
Universidad de las Fuerzas Armadas, Ecuador
Edison Loza-Aguirre
Escuela Politécnica Nacional, Ecuador
Eduardo Albuquerque
Federal University of Goiás, Brazil
Efraín R. Fonseca C.
Universidad de las Fuerzas Armadas ESPE, Ecuador
Egils Ginters
Riga Technical University, Latvia
Enrique Carrera
Universidad de las Fuerzas Armadas ESPE, Ecuador
Ewaryst Tkacz
Silesian University of Technology, Poland
Fabio Gomes Rocha
Tiradentes University, Brazil
Felix Blazquez Lozano
University of A Coruña, Spain
Filipa Ferraz
University of Minho, Portugal
Filipe Sá
Câmara Municipal de Penacova, Portugal
Francisco Andrade
University of Minho, Portugal
Franklim Silva
Universidad de las Fuerzas Armadas, Ecuador
Gabriel Pestana
Universidade Europeia, Portugal
George Suciu
BEIA, Romania
Gregory O’Hare
University College Dublin, Ireland
Hector Florez
Universidad Distrital Francisco Jose de Caldas,
Colombia
Henrique Lopes Cardoso
University of Porto, Portugal
Ildeberto Rodello
University of São Paulo
Isabel Pedrosa
Coimbra Business School - ISCAC, Portugal
Ivan Puentes Rivera
University of Vigo, Spain
Jan Kubicek
Faculty of Electrical Engineering and Computer
Science VŠB-TUO, Czech Republic
João Paulo Pereira
Polytechnic of Bragança, Portugal
João Vidal de Carvalho
ISCAP/IPP, Portugal
Jonathas Cruz
IFPI, Brazil
José Araújo
SAP, Portugal
Jose Ignacio
Rodrigues Molano
Universidade Distrital Francisco Jose Caldas,
Colombia
José Luís Silva
ISCTE-IUL and Madeira-ITI, Portugal
Juan Jesus Ojeda
University of Almeria, Spain
Juan M. Ferreira
Senate, Paraguay
Júlio Menezes Jr.
Federal University of Pernambuco, Brazil
Justyna Trojanowska
Poznan University of Technology, Poland
Korhan Gunel
Adnan Menderes University, Turkey
Laura Varela-Candamio
University of A Coruña, Spain
Leandro Flórez Aristizábal
Antonio Jose Camacho University Institute,
Colombia
Leonardo Botega
UNIVEM, Brazil
viii
Organization

Mafalda Teles Roxo
INESC TEC, Portugal
Magdalena Diering
Poznan University of Technology, Poland
Manuel Monteiro
Hospital Particular São Lucas, Portugal
Marcelo Mendonça
Teixeira
Federal Rural University of Pernambuco, Brazil
Marcia Bayas
Universidad Estatal Peninsula de Santa Elena,
Ecuador
Marciele Berger
University of Minho, Portugal
Maria José Sousa
University of Coimbra, Portugal
Maria Koziri
University of Thessaly, Greece
María Pilar Mareca López
Universidad Politécnica de Madrid, Spain
María Teresa
García-Álvarez
University of A Coruna, Spain
Maristela Holanda
University of Brasilia, Brazil
Martin Kyselak
University of Defence, Czech Republic
Michele Della Ventura
Music Academy “Studio Musica,” Italy
Miguel Angel
Manso Callejo
Universidad Politécnica de Madrid, Spain
Mohamed Abouzeid
Innovations for High Performance
Microelectronics IHP, Germany
Monica Leba
University of Petrosani, Romania
Nadjet Kamel
University Ferhat Abbas Setif 1, Algeria
Nikolai Prokopyev
Russian Academy of Sciences, Russia
Nikolaos Giannakeas
Technology Educational Institute of Epirus, Greece
Niranjan S K
JSS Science and Technology University, Mysore,
India
Nomusa Dlodlo
Namibia University of Science and Technology,
Namibia
Pablo Alejandro Quezada
Sarmiento
Universidad Internacional del Ecuador, Ecuador
Pedro Fernandes
de Oliveira Gomes
State University of Maringá, Brazil
Pedro Liberato
ESHT/IPP, Portugal
Pedro Nogueira
LIACC, Portugal
Ramayah T.
Universiti Sains Malaysia, Malaysia
Ramon Alcarria
Universidad Politécnica de Madrid, Spain
Robson Lemos
Federal University of Santa Catarina, Brazil
Prabhat Mahanti
University of New Brunswick, Canada
Said Achchab
Mohammed V University in Rabat, Morocco
Samanta Patricia
Cueva Carrión
Universidad Técnica Particular de Loja, Ecuador
Sandra Patricia
Cano Mazuera
San Buenaventura Cali University, Colombia
Sampsa Rauti
University of Turku, Finland
Santoso Wibowo
CQUniversity, Australia
Organization
ix

Saulo Barbará Oliveira
Universidade Federal Rural do Rio de Janeiro, Brazil
Sergio Luján-Mora
University of Alicante, Spain
Shuai Zhao
MediaTek Inc. USA, USA
Simona Riurean
University of Petrosani, Romania
Songjie Wei
Nanjing University of Science and Technology,
China
Stanley Lima
Technische Universität Dresden, Germany
Sylvie Ratté
École de Technologie Supérieure, Canada
Tariq Ahamed Ahanger
Prince Sattam Bin Abdulaziz University,
Saudi Arabia
Teresa Guarda
State University of Santa Elena Peninsula, Ecuador
Thanasis Loukopoulos
University of Thessaly, Greece
Victor Georgiev
Russian Academy of Sciences, Russia
Victor Villar
Tiradentes University, Brazil
Ville Leppänen
University of Turku, Finland
Vladislav Gorbunov
Russian Academy of Sciences, Russia
x
Organization

WMETACOM 2018 – 1st Workshop on Media,
Applied Technology and Communication
Organizing Committee
Andrea Mila Maldonado
Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra, Ecuador
Francisco Campos Freire
Universidade de Santiago de Compostela, Spain
Iván Puentes Rivera
Universidade de Vigo, Spain
María Fannery
Suárez Berrío
Directora Académica Pontiﬁcia Universidad
Católica del Ecuador Sede Ibarra, Ecuador
María Josefa
Rubio Gómez
Prorrectora Pontiﬁcia Universidad Católica del
Ecuador Sede Ibarra, Ecuador
Mónica López-Golán
METACOM research group from Pontiﬁcia
Universidad Católica del Ecuador Sede Ibarra,
Ecuador
Nancy Ulloa Erazo
METACOM research group from Pontiﬁcia
Universidad Católica del Ecuador Sede Ibarra,
Ecuador
Paulo Carlos López-López
METACOM research group from Pontiﬁcia
Universidad Católica del Ecuador Sede Ibarra,
Ecuador
Tania Aguilera Bravo
Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra, Ecuador
Xosé López García
Universidad de Santiago de Compostela, Spain
Scientiﬁc Committee
Ana Belén Fernández
Souto
Universidade de Vigo, Spain
Andrea Mila
Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra, Ecuador
Alba Silva Rodríguez
Universidade de Santiago de Compostela, Spain
Carlos Toural Bran
Universidade de Santiago de Compostela, Spain
xi

Clide Rodríguez Vázquez
Universidade de A Coruña, Spain
Eva Sánchez Amboage
Universidade de A Coruña, Spain
Francisco Campos Freire
Universidade de Santiago de Compostela, Spain
Félix Blázquez Lozano
Universidade de A Coruña, Spain
Iván Puentes Rivera
Universidade de Vigo, Spain
Magdalena
Rodríguez Fernández
Universidade de A Coruña, Spain
Miguel Túñez López
Universidade de Santiago de Compostela, Spain
Mónica López Golán
Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra, Ecuador
Nancy Ulloa Erazo
Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra, Ecuador
Óscar Juanatey Boga
Universidade de A Coruña, Spain
Paulo Carlos López
Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra, Ecuador
Tania Aguilera Bravo
Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra, Ecuador
Valentín Alejandro
Martínez Fernández
Universidade de A Coruña, Spain
Xosé Rúas Araújo
Universidade de Vigo, Spain
Xosé López García
Universidade de Santiago de Compostela, Spain
xii
WMETACOM 2018

Contents
Organizational Models and Information Systems
Rationalization of Organizational Processes: The Case
of the Institute of Applied Social Sciences of
Rio de Janeiro Federal Rural University . . . . . . . . . . . . . . . . . . . . . . . .
3
Elisangela Costa, Saulo Barbará de Oliveira,
and Daniel Ribeiro de Oliveira
Beneﬁts of Process Simulation Software – The Case of a Brazilian
Mixed Public-Private Company . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
Ricardo Luiz Schiavo do Nascimento, Saulo Barbará de Oliveira,
and Aparecida Laino Entriel
IT Service Management Using COBIT Enablers: The Case
of Brazilian National Institute of Cancer . . . . . . . . . . . . . . . . . . . . . . . .
20
Sandro Luís Freire de Castro Silva, Saulo Barbará de Oliveira,
Marcos Azevedo Benac, Antônio Augusto Gonçalves,
and Carlos Henrique Fernandes Martins
Towards a Forensic Analysis of Mobile Devices Using Android . . . . . . .
30
Estevan Gomez-Torres, Oswaldo Moscoso-Zea, Nelson Herrera Herrera,
and Sergio Lujan-Mora
Process-Based Project Management for Implementation
of an ERP System at a Brazilian Teaching Institution . . . . . . . . . . . . . .
40
Ada Guagliardi Faria, Saulo Barbará de Oliveira,
and Fábio Carlos Macêdo
A Conceptual Framework for the Implantation of Enterprise
Applications in Small and Medium Enterprises (SMEs) . . . . . . . . . . . . .
50
Irving Reascos and João Alvaro Carvalho
Model for Selecting Software Development Methodology . . . . . . . . . . . .
62
Lizeth Chandi, Catarina Silva, Tatiana Gualotuña, and Danilo Martinez
xiii

Information and Knowledge Management
Smart Cities Semantics and Data Models . . . . . . . . . . . . . . . . . . . . . . . .
77
Antonio J. Jara, Martin Serrano, Andrea Gómez, David Fernández,
Germán Molina, Yann Bocchi, and Ramon Alcarria
The Information Technologies in the Competitiveness
of the Tourism Sector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
Pedro Liberato, Dália Liberato, António Abreu, Elisa Alén-González,
and Álvaro Rocha
A Fuzzy Classiﬁer-Based Penetration Testing
for Web Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
J. K. Alhassan, Sanjay Misra, A. Umar, Rytis Maskeliūnas,
Robertas Damaševičius, and Adewole Adewumi
Comparative Evaluation of Mobile Forensic Tools . . . . . . . . . . . . . . . . .
105
J. K. Alhassan, R. T. Oguntoye, Sanjay Misra, Adewole Adewumi,
Rytis Maskeliūnas, and Robertas Damaševičius
Cloud Based Simple Employee Management Information System:
A Model for African Small and Medium Enterprises . . . . . . . . . . . . . . .
115
Isaac U. Oduh, Sanjay Misra, Robertas Damaševičius,
and Rytis Maskeliūnas
Inexpensive Marketing Tools for SMEs . . . . . . . . . . . . . . . . . . . . . . . . .
129
José Avelino Vitor, Teresa Guarda, Maria Fernanda Augusto,
Marcelo Leon, Datzania Villao, Luis Mazon,
and Yovany Salazar Estrada
Big Data, the Next Step in the Evolution
of Educational Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
W. Villegas-Ch, Sergio Luján-Mora, Diego Buenaño-Fernandez,
and X. Palacios-Pacheco
Method for Emotion Corpus Validation from the Consensual
Identiﬁcation of Patterns in Alzheimer’s Patients . . . . . . . . . . . . . . . . . .
148
Pablo Gómez, Alexandra González-Eras, and Pablo Torres Carrión
Web Prosumers: The Intangible Wealth of Education
and Economics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
Emanuel Bohórquez, Teresa Guarda, Humberto Peña, Marcelo León,
William Caiche, and José Villao
Analysis of Correspondences Applied to Vehicle Plates
Using Descriptors in Visible Spectrum . . . . . . . . . . . . . . . . . . . . . . . . . .
170
Shendry Rosero and Alberto Jimenez
xiv
Contents

Alignment of Software Project Management with the Business
Strategy in VSEs: Model and Evaluation . . . . . . . . . . . . . . . . . . . . . . . .
178
Carlos Montenegro and Geovani Barragán
Proposal to Implementation Time-Driven Activity Based Costing
(TDABC) for Calculation of Surgical Procedure Costs
of a Medium-Sized Teaching Hospital . . . . . . . . . . . . . . . . . . . . . . . . . .
191
Michele Mendes Hiath Silva and Saulo Barbará de Oliveira
Best Practices and Pitfalls in Open Source Hardware . . . . . . . . . . . . . .
200
Manuel Moritz, Tobias Redlich, and Jens Wulfsberg
Best Practice in Advanced Enterprise Knowledge Engineering . . . . . . .
211
Matteo Sasgratella and Alberto Polzonetti
Production Flow Improvement in a Textile Industry . . . . . . . . . . . . . . .
224
Jose J. Lopes, Maria L. R. Varela, Justyna Trojanowska,
and Jose Machado
Marketing Knowledge Management Model . . . . . . . . . . . . . . . . . . . . . .
234
Teresa Guarda, Maria Fernanda Augusto, Marcelo León,
Hugo Pérez, Washington Torres, Walter Orozco,
and Jacqueline Bacilio
Participative Sensing in Noise Mapping: An Environmental
Management System Model for the Province of Santa Elena . . . . . . . . .
242
Teresa Guarda, Marcelo León, Maria Fernanda Augusto, Hugo Pérez,
Johnny Chavarria, Walter Orozco, and Jaime Orozco
Using Experimental Material Management Tools in Experimental
Replication: A Systematic Mapping Study . . . . . . . . . . . . . . . . . . . . . . .
252
Edison Espinosa, Juan M. Ferreira, and Henry Chanatasig
Unveiling Unbalance on Sustainable Supply Chain Research:
Did We Forget Something? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
Edison Loza-Aguirre, Marco Segura Morales, Henry N. Roa,
and Carlos Montenegro Armas
Industry Knowledge Management Model 4.0 . . . . . . . . . . . . . . . . . . . . .
275
José Ignacio Rodríguez-Molano, Leonardo Emiro Contreras-Bravo,
and Edwin Rivas-Trujillo
Smartphone-Based Vehicle Emission Estimation . . . . . . . . . . . . . . . . . .
284
M. Cerón, M. Fernández-Carmona, C. Urdiales, and F. Sandoval
An Information Visualization Engine for Situational-Awareness
in Health Insurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
294
Flávio Epifânio and Gabriel Pestana
Contents
xv

ECOPPA: Extensible Context Ontology for Persuasive
Physical-Activity Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
Mohamad Hoda, Valeh Montaghami, Hussein Al Osman,
and Abdulmotaleb El Saddik
Creating Predictive Models for Forecasting the Accident Rate
in Mountain Roads Using VANETs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319
Borja Bordel, Ramón Alcarria, Gianluca Rizzo, and Antonio Jara
Intelligent and Decision Support Systems
Improving Game Modeling for the Quoridor Game State
Using Graph Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
Daniel Sanchez and Hector Florez
A Model of Self-oscillations in Relay Outputs Control Systems
with Elements of Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . .
343
R. H. Rovira, V. M. Duvoboi, M. S. Yukhimchuk, M. M. Bayas,
and W. D. Torres
Computer Vision-Based Method for Automatic Detection of Crop
Rows in Potato Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
Iván García-Santillán, Diego Peluffo-Ordoñez, Víctor Caranqui,
Marco Pusdá, Fernando Garrido, and Pedro Granda
A Finger-vein Biometric System Based on Textural Features. . . . . . . . .
367
Enrique V. Carrera, Santiago Izurieta, and Ricardo Carrera
Multi-level Skew Correction Approach for Hand Written
Kannada Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
376
H. C. Vinod and S. K. Niranjan
Semi-automatic Determination of Geometrical Properties of Short
Natural Fibers in Biocomposites by Digital Image Processing . . . . . . . .
387
Victoria Mera-Moya, Jorge I. Fajardo, Iális C. de Paula Junior,
Leslie Bustamante, Luis J. Cruz, and Thiago Barros
ExperTI: A Knowledge Based System for Intelligent Service Desks
Using Free Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
397
Alejandro Bello, Andrés Melgar, and Daniel Pizarro
Mapping the Global Offshoring Network Through
the Panama Papers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
David Dominguez, Odette Pantoja, and Mario González
Comparing Different Data Fusion Strategies for
Cancer Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
417
Katarzyna Pojda, Michał Jakubczak, Sebastian Student,
Andrzej Świerniak, and Krzysztof Fujarewicz
xvi
Contents

A Recommender System Based on Cognitive Map
for Smart Classrooms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
Jose Aguilar, Priscila Valdiviezo-Diaz, and Guido Riofrio
Strategy to Develop a Digital Public Health Observatory
Integrating Business Intelligence and Visual Analytics . . . . . . . . . . . . . .
443
Leidy Alexandra Lozano and Maria del Pilar Villamil
Using Machine Learning for Sentiment and Social Inﬂuence Analysis
in Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
453
Emmanuel Awuni Kolog, Calkin Suero Montero, and Tapani Toivonen
Big Data Analytics and Applications
Big Data Applications in Cancer Research: A Case Study
at the Brazilian National Cancer Institute . . . . . . . . . . . . . . . . . . . . . . .
467
Antônio Augusto Gonçalves, Carlos Henrique Fernandes Martins,
José Geraldo Pereira Barbosa, and Sandro Luís Freire de Castro Silva
New Diagnostic Tool for Patients Suffering from Noncommunicable
Diseases (NCDs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
476
Wojciech Oleksy, Zbigniew Budzianowski, Ewaryst Tkacz,
and Małgorzata Garbacik
Detection of Genetic Aberrations in Cancer Driving Signaling
Pathways Based on Joint Analysis of Heterogeneous
Genomics Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
Roman Jaksik and Krzysztof Fujarewicz
Ethics, Computers and Security
Cookie Scout: An Analytic Model for Prevention of Cross-Site
Scripting (XSS) Using a Cookie Classiﬁer . . . . . . . . . . . . . . . . . . . . . . .
497
Germán Eduardo Rodríguez, Diego Eduardo Benavides, Jenny Torres,
Pamela Flores, and Walter Fuertes
An Empirical Evaluation of Open Source in Telecommunications
Software Development: The Good, the Bad, and the Ugly . . . . . . . . . . .
508
Rolando P. Reyes Ch., Efraín R. Fonseca C., John W. Castro,
Hugo Pérez Vaca, and Manolo Paredes Calderón
Wearable Technology, Privacy Issues . . . . . . . . . . . . . . . . . . . . . . . . . . .
518
Pablo Saa, Oswaldo Moscoso-Zea, and Sergio Lujan-Mora
Human-Computer Interaction
Older Adults’ Perception of Online Health Webpages Using Eye
Tracking Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
Anushia Inthiran and Robert D. Macredie
Contents
xvii

Method for Accessibility Assessment of Online Content Editors. . . . . . .
538
Tania Acosta, Patricia Acosta-Vargas, Luis Salvador-Ullauri,
and Sergio Luján-Mora
An Approach to Mobile Serious Games Accessibility Assessment
for People with Hearing Impairments . . . . . . . . . . . . . . . . . . . . . . . . . .
552
Angel Jaramillo-Alcázar and Sergio Luján-Mora
Real Time Driver Drowsiness Detection Based on Driver’s Face
Image Behavior Using a System of Human Computer Interaction
Implemented in a Smartphone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
563
Eddie E. Galarza, Fabricio D. Egas, Franklin M. Silva, Paola M. Velasco,
and Eddie D. Galarza
Interactive System Using Beaglebone Black with LINUX Debian
for Its Application in Industrial Processes . . . . . . . . . . . . . . . . . . . . . . .
573
Marco Pilatásig, Franklin Silva, Galo Chacón, Víctor Tapia,
John Espinoza, Esteban X. Castellanos, Lucia Guerrero,
and Jessy Espinosa
Interactive System for Monitoring and Control of a Flow Station
Using LabVIEW. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
583
Jorge Buele, John Espinoza, Marco Pilatásig, Franklin Silva,
Alexandra Chuquitarco, Jenny Tigse, Jessy Espinosa,
and Lucía Guerrero
Interactive System for Hands and Wrist Rehabilitation . . . . . . . . . . . . .
593
Marco Pilatásig, Jenny Tigse, Alexandra Chuquitarco, Pablo Pilatásig,
Edwin Pruna, Andrés Acurio, Jorge Buele, and Ivón Escobar
Toward a Combined Method for Evaluation of Web Accessibility . . . . .
602
Patricia Acosta-Vargas, Sergio Luján-Mora, Tania Acosta,
and Luis Salvador-Ullauri
Educational Math Game for Stimulation of Children
with Dyscalculia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
614
Pablo Torres-Carrión, Christian Sarmiento-Guerrero,
Juan Carlos Torres-Diaz, and Luis Barba-Guamán
Intelligent Tutoring Based on a Context-Aware Dialogue
in a Procedural Training Environment . . . . . . . . . . . . . . . . . . . . . . . . .
624
José Paladines and Jaime Ramírez
Communities of Language Learners: Mobility, Usability
and Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
631
Fernanda Maria Pereira Freire, André Constantino da Silva,
and Isaque Miguel Pires
xviii
Contents

Moving Beyond Limitations: Evaluating the Quality
of Android Apps in Spanish for People with Disability . . . . . . . . . . . . .
640
Andrés Larco, Cesar Yanez, Carlos Montenegro,
and Sergio Luján-Mora
Software Systems, Architectures, Applications and Tools
Analysis and Implementation of ETL System for Unmanned
Aerial Vehicles (UAV) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
653
Wilson Medina-Pazmiño, Aníbal Jara-Olmedo, Cristian Tasiguano-Pozo,
and José M. Lavín
Analysis of the Interaction on the Web Through Social Networks
(Twitter, Facebook, Instagram) Case Study: Economic Sectors
with Higher Incomes in Ecuador . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
663
Mariuxi Tejada-Castro, Maritza Aguirre-Munizaga,
Vanessa Vergara-Lozano, Mayra Garzon-Goya, and Evelyn Solís-Avilés
Proposal of a Supply Chain Architecture Immersed
in the Industry 4.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
677
Jose Ignacio Rodriguez, Monica Blanco, and Karen Gonzalez
Open Source Web Software Architecture Components for Geographic
Information Systems in the Last 5 Years: A Systematic
Mapping Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
688
Alvaro Uyaguari, Edison Espinosa-Gallardo,
Santiago P. Jácome-Guerrero, Patricio Espinel, Cristian F. Cabezas,
Gloria I. Arias Almeida, and Frankz Alberto Carrera Calderón
Ethnographic Study on Practices of the Software Development
Industry in Chile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
700
Dante Carrizo and Andrés Alfaro
Geolocation Applied to Emergency Care Systems
for Priority Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
710
A. José Sánchez, L. Lídice Haz, B. Datzania Villao,
and G. Washington Torres
MORPHY: A Multiobjective Software Tool for Phylogenetic
Inference of Protein Coded Sequences . . . . . . . . . . . . . . . . . . . . . . . . . .
719
Cristian Zambrano-Vega, Antonio J. Nebro, José F. Aldana Montes,
and Byron Oviedo
Adaptive Harris Corner Detector Evaluated
with Cross-Spectral Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
732
Patricia L. Suárez, Angel D. Sappa, and Boris X. Vintimilla
Contents
xix

JavaScript Middleware for Mobile Agents Support on Desktop
and Mobile Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
745
Carlos Silva, Nuno Costa, Carlos Grilo, and Jorge Veloz
Analyzing UAV-Based Remote Sensing and WSN Support
for Data Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
756
Ramón Alcarria, Borja Bordel, Miguel Ángel Manso, Teresa Iturrioz,
and Marina Pérez
Continuous Speech Recognition and Identiﬁcation
of the Speaker System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
767
Diego Guffanti, Danilo Martínez, José Paladines, and Andrea Sarmiento
Competitive Intelligence Using Domain Ontologies on Facebook
of Telecommunications Companies of Peru . . . . . . . . . . . . . . . . . . . . . .
777
Geraldo Colchado and Andrés Melgar
Sustainability Performance Evaluation of Groundwater
Remediation Technologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
788
Santoso Wibowo and Srimannarayana Grandhi
Mobile Application to Encourage Local Tourism
with Context-Aware Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
796
Carlos A. Silva, Renato Toasa, Juan Guevara, H. David Martinez,
and Javier Vargas
Modelled Testbeds: Visualizing and Augmenting Physical
Testbeds with Virtual Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
804
Stephane Kundig, Constantinos Marios Angelopoulos, and Jose Rolim
Proposal for an Integrated Framework for Mobile
Applications Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
813
Danilo Martínez, Xavier Ferré, and Diego Marcillo
Computer Networks, Mobility and Pervasive Systems
An Adaptive-Bounds Band-Pass Moving-Average Filter
to Increase Precision on Distance Estimation from Bluetooth RSSI . . . .
823
Diego Ordóñez-Camacho and Edwin Cabrera-Goyes
Health Informatics
Towards a Framework to Enable Semantic Interoperability of Data
in Heterogeneous Health Information Systems in Namibian Public
Hospitals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
835
Nikodemus Angula and Nomusa Dlodlo
xx
Contents

Automatic Extraction and Aggregation of Diseases
from Clinical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
846
Ruth Reátegui and Sylvie Ratté
eHealth Applications in Portuguese Hospitals: A Continuous
Benchmarking with European Hospitals . . . . . . . . . . . . . . . . . . . . . . . .
856
João Vidal Carvalho, Álvaro Rocha, and António Abreu
Innovation Process in Cancer Treatment: The Implementation
of Picture Archiving and Communication System (PACS)
at Brazilian National Cancer Institute . . . . . . . . . . . . . . . . . . . . . . . . . .
867
Antônio Augusto Gonçalves, Carlos Henrique Fernandes Martins,
José Geraldo Pereira Barbosa, and Sandro Luís Freire de Castro Silva
The Higher-Order Spectra as a Tool for Assessing the Progress
in Rehabilitation of Patients After Ischemic Brain Stroke . . . . . . . . . . .
874
Ewaryst Tkacz, Zbigniew Budzianowski, and Wojciech Oleksy
Developing and Testing an Application to Assess the Impact
of Smartphone Usage on Well-Being and Performance Outcomes
of Student-Athletes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
883
Poppy DesClouds, Fedwa Laamarti, Natalie Durand-Bush,
and Abdulmotaleb El Saddik
Multisensory Virtual Game with Use of the Device Leap Motion
to Improve the Lack of Attention in Children of 7–12 Years
with ADHD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
897
David Chilcañán Capelo, Milton Escobar Sánchez,
Jhonatan Salazar Hurtado, and Daniela Benalcázar Chicaiza
Development and Improvement of the Visomotriz Coordination:
Virtual Game of Learning and Using the Sphero Haptic Device
for Alpha Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
907
David Chilcañán Capelo, Milton Escobar Sánchez,
Chrystian López Hidalgo, and Daniela Benalcázar Chicaiza
Information Technologies in Education
Experiential Education: Creation of a Business Game to Enhance
Learning of Business Administration Students . . . . . . . . . . . . . . . . . . . .
919
Eduardo de Oliveira Ormond, Gustavo Olivares,
and Saulo Barbará de Oliveira
Educational Computing Resources Applied to the Teaching
of Manufacturing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
927
Leonardo Emiro Contreras Bravo, Jose Ignacio Rodriguez Molano,
and Edwin Rivas Trujillo
Contents
xxi

Recommendation Systems in Education: A Systematic
Mapping Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
937
Abdon Carrera Rivera, Mariela Tapia-Leon, and Sergio Lujan-Mora
Looking for Usability and Functionality Issues: A Case Study . . . . . . . .
948
Karina Jiménes, Jhonny Pincay, Mónica Villavicencio,
and Alberto Jiménez
Determinants of ICT Integration in Teaching Secondary School
Agriculture: Experience of Southern Africa (Swaziland) . . . . . . . . . . . .
959
Nomsa M. Mndzebele, Mzomba Nelson Dludlu,
and Comfort B. S. Mndebele
Media, Applied Technology and Communication
Design of a Recommender System for Intelligent Classrooms Based
on Multiagent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
973
Dulce Rivero-Albarrán, Francklin Rivas-Echeverria, Laura Guerra,
Brian Arellano, and Stalin Arciniegas
The Visual Speech and Creativity in Advertising Impressed
in Ecuador in Daily “El Comercio” Between 1908 and 1950 . . . . . . . . .
983
Marco López-Paredes
Inﬂuence of Social Networks from Cellphones to Choose
Restaurants, Salinas – 2016 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
992
Homero Rodríguez, Jeyco Macías, Néstor Montalván,
and René Garzozi
Digital Feedback and Academic Resilience . . . . . . . . . . . . . . . . . . . . . . . 1004
Laura Guerra, Dulce Rivero, Stalin Arciniegas, and Santiago Quishpe
Mobile Learning: Challenging the Current Educational Model
of Communication Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1014
Verónica Yépez-Reyes
Competencies and Indicators for a Productive
Digital Communication. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1022
Laura Guerra, Stalin Arciniegas, Luis David Narváez,
and Francisca Grimon
Transparency and Participation in Public Service Television
Broadcasters: The American Southern Cone . . . . . . . . . . . . . . . . . . . . . 1033
Paulo Carlos López-López, Mónica López-Golán,
and Iván Puentes-Rivera
xxii
Contents

Knowledge Based of an Expert System Using the Horizontal
Analysis for Financial Statements of National, Private TV Companies
in Ecuador . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1044
Ana Cecilia Vaca-Tapia, Francisco Campos Freire,
Francklin Iván Rivas-Echeverría, and Johnny Alejandro Aragón-Puetate
The Use of Facebook in Community Radio: A Quantitative
Analysis of the Andean Community of Nations . . . . . . . . . . . . . . . . . . . 1055
Viviana Galarza-Ligña, Amparo Reascos-Trujillo,
and Stalin Rivera-Imbaquingo
The Interaction Gap: From the Bit to the Resurgence of a New
Information and Communication System . . . . . . . . . . . . . . . . . . . . . . . . 1065
Carmelo Márquez-Domínguez, Nancy Ulloa-Erazo,
and Yalitza Therly Ramos-Gil
The Inﬂuence of New Technologies on University
Radio in Ecuador . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1076
Ana Culqui Medina and Elizabeth Granda Sánchez
The Press in the Context of the Andean Community
of Nations (CAN): Without Sustainable Monetization
in the Digital Economy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1084
Yalitza Ramos-Gil, Carmelo Márquez-Domínguez,
and Aldo Romero-Ortega
Ecuador, the Non-communication: Postdrama or Performance? . . . . . . 1094
Miguel Ángel Orosa and Aldo Romero-Ortega
Media Processes of Communicational Management, Information
Transparency and the Incidence of TIC . . . . . . . . . . . . . . . . . . . . . . . . . 1104
Nancy Ulloa-Erazo and Álvaro Cevallos Ramírez
Radio: A Didactic Meaningful Strategy, for Strengthening
the Oral Communicative Competence in the English Language . . . . . . . 1115
María Fernanda Ibadango-Tabango,
Armida Mariela Montenegro-Cevallos,
and Luz Marina Rodríguez-Cisneros
The Communication in English from an Educational Perspective
and Its Relationship with the Competence-Based Teaching Proﬁle . . . . 1125
Brenda Gutierrez-Franco, Armida Mariela Montenegro-Cevallos,
and Hazel Machado-Rosales
The Diffusion of Public Policies on Technical Training
for the Textile and Clothing Industry in Ecuador . . . . . . . . . . . . . . . . . 1135
Tania Aguilera, Andrea Mila, Daniela Batallas, and Giovannina Torres
Contents
xxiii

Open Government and Citizen Participation in the Web Portals
of Ecuador GADM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1146
Patricia Henríquez-Coronel, Jennifer Bravo-Loor, Enrique Díaz-Barrera,
and Yosselin Vélez-Romero
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1157
xxiv
Contents

Organizational Models and Information
Systems

Rationalization of Organizational Processes:
The Case of the Institute of Applied Social
Sciences of Rio de Janeiro Federal
Rural University
Elisangela Costa(&), Saulo Barbará de Oliveira,
and Daniel Ribeiro de Oliveira
Universidade Federal Rural do Rio de Janeiro, Rodovia BR 465
KM7-Seropédica, Rio de Janeiro 23890-000, Brazil
egcostaufrrj@gmail.com
Abstract. Processes are instruments for systematization of working activities
that facilitate management and add value to the organization. This study
describes results of mapping the processes of the Institute of Applied Social
Sciences of Rio de Janeiro Federal Rural University (UFRRJ). The study is
qualitative, exploratory and interventionist in nature. The data were collected by
consulting the literature and documents and conducting interviews. We sought
to ascertain the most suitable techniques and methods for the objectives of the
study, and through interviews with managers and other staff members it was
possible to assess the initial situation of the organization, its bottlenecks and
critical points, and from the knowledge obtained, to implement improvements in
the process. The results allow concluding that the mapping of processes con-
tributed by enabling the sector to identify and better visualize the processes,
identify failures, standardize activities and improve the quality of the services
rendered.
Keywords: Mapping  Processes  Performance improvement
Public organization
1
Introduction
A process is a set of activities carried out to generate a result, product or service. The
processes in organizations of any type or size are means to rationalize activities, guide
strategic actions and implement improvements in working routines. The objective of a
process is to facilitate aggregation of value to the tangible and intangible assets of the
organization.
In studying process management, modeling reveals the operational ﬂow and its
interplay with processes, allowing the organization to clearly see the strong and weak
points, gain a better understanding of processes and increase performance, be it in
business or any other sector [1]. In general, mapping consists of the graphical repre-
sentation of a process, to allow clearer visualization and more detailed analysis. This, in
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_1

turn, facilitates identiﬁcation of failures and application of measures to rectify them and
improve overall performance.
The Institute of Applied Social Sciences of Rio de Janeiro Federal Rural University,
covered by this study, faced a critical situation due to lack of a standardized adminis-
trative routine and duly mapped business processes for hiring new professor, one of its
most important processes. In this respect, in 2015, 30% of the public examinations
realized were annulled due to appeals by candidates regarding ﬂaws.
The entire process is executed by the academic departments, which indicate to the
human resources sector the members of the board of examiners, and together with an
administrative secretary, deﬁne the steps of the selective process. However, the absence
of standardization of the administrative routine and appointment of the secretary wound
up generating a bottleneck in the process, since other administrative routines exist that
must be concluded before, during and after the competitive exam. In this respect, the
failure to name a secretary undermined the whole process, contributing to the occur-
rence of failures and the respective nulliﬁcation of the selection process in many cases.
Another factor that interfered in the progress of the process occurred after all the
selection steps, because paperwork was sent to sectors that no longer had any say over
the outcome of the process, generating unnecessary bureaucracy and delaying the
conclusion of the process, thus compromising the hiring of new professors.
The objective of the article can be synthesized by the following question: How does
the Process Management contribute to the improvement of the quality of the selection
process for professors of an institute inside a university? In this article, a speciﬁc
educational institute known as the Institute of Applied Social Sciences (ICSA) was
studied aiming at the improvement of its services.
The article is structured in four sections besides this introduction: theoretical
framework; methodology; case study (process and post-mapping beneﬁts); and ﬁnal
considerations.
2
Theoretical Framework
2.1
Process Management
Process Management can be deﬁned as a “system or model for organizational man-
agement”, with the aim of managing organizations with focus on processes [2].
Organizations try to work with focus on processes when they need to execute a
sequence of activities that consume time and resources in the production of goods or
provision of services, to create value for clients and the organization. However, only
recently, with popularization of business process management (BPM), have organi-
zations started to explicitly systematize and formalize their processes [3].
An organization managed according to processes no longer operates through a
vertical hierarchical structure, but instead by means of matrix structures and multi-
functional teams, with focus on business processes [4]. Seen in this light, process
management is a management model that strives to manage the organization with focus
on the processes that generate value for clients [5]. It also contributes to ongoing
improvement of the organizational performance. Therefore, an organization that wants
4
E. Costa et al.

to pursue continuous improvement should ﬁrst identify and then classify its processes,
seeking to learn which are the most important and which are the most critical. The
classiﬁcation can be done by applying a reference model, such as the process classi-
ﬁcation framework (PCF) – an architecture developed by the American Productivity &
Quality Center (APQC) [5] that also enables subsequent benchmarking.
Although various PCF formats exist, all of them are decomposed into: (a) process
categories; (b) process groups; (c) processes; (d) activities. The PCFs are divided into
two process levels: operating processes, which are the most important to the company,
also called business processes; and management and support processes, which as the
name indicates provide support to the operating processes, as shown in Fig. 1.
By adopting this type of architecture, organizations can gain a horizontal view of
their activities, in place of the traditional vertical hierarchical vision. This allows them
to optimize the entire decision-making process, since the horizontal vision of the
processes reduces the power islands within organizations [7].
Fig. 1. PCF architecture Source: [6]
Rationalization of Organizational Processes
5

2.2
Process Management
The process modeling method (process mapping) is a way to coordinate and guide
efforts to analyze the organization in relation to its processes. The method adopted in
this study consisted of eight integrated steps that compose a system for analysis,
structured as follows: (a) analysis of requirements; (b) construction of the model;
(c) analysis of processes; (d) simulation; (e) reengineering; (f) documentation of the
partial and ﬁnal results/products; (g) disclosure; (h) management of processes –
meaning monitoring them [2]. Additionally, to model the processes, we used the
BPMN (Business Process Model and Notation) technique, one of the most widely used
notations for this type of activity [8].
Among the tools utilized to model processes are: (a) process maps; (b) the SIPOC
model (Supplier, Input, Process, Output, Customer), which describes the input and
output elements as well as the performance and customer indicators of the process;
(c) block diagrams, which generically represent the processes and their connections;
(d) business process diagrams (BPDs) or ﬂowcharts, which graphically depict the
processes, detail the logical order in which they occur and allow visualization of
actions and deviations [9].
When the problems identiﬁed involve execution of operations, BPDs are the best
tools to use to visualize, analyze and correct failures. Through graphical illustration of
the ﬂows of processes, it is possible to identify, determine and if necessary eliminate
steps carried out within a particular process [10].
3
Methodology
To attain the proposed objective, we used the qualitative research method, to analyze
the contributions of process mapping aiming to improve the main administrative
activities of ICSA/UFRRJ. For this purpose we conducted individual interviews with
the administrative staff (a total of 15 technical-administrative staff members of ICSA.
An interview script with semi-structured questions served to guide the ﬁeld research.
Before this, we carried out a bibliographical study of reputable magazines, books,
articles
and
websites,
and
examined
ofﬁcial
documents
made
available
by
ICSA/UFRRJ. The study had an exploratory, descriptive and interventionist nature.
4
The Study
The case examined here is one of the administrative processes of the Institute of
Applied Social Sciences (ICSA) of Rio de Janeiro Federal Rural University (UFRRJ),
founded over a hundred years ago.
The process of periodic public examinations to hire new full-time professor was
indicated by the interviewees as the most critical administrative process. Based on this
information, we carried out an analysis of this process together with staff members of the
university’s Personnel Department. This investigation conﬁrmed the existence of prob-
lems, causing the submission of administrative appeals and even lawsuits, the outcome of
6
E. Costa et al.

which required the annulment of 30% of the selective hiring processes realized by ICSA in
2015. After this ﬁnding, we mapped the process with the participation of the 15 staff
members who had been interviewed, all of them directly involved in the process.
Once the step of gathering information through the interviews was ﬁnished, the
next step was to model the process in the “as is” phase, ascertaining all the organi-
zational levels necessary for realization.
This analysis showed that some tasks were overlooked by the managers in charge,
thus needing attention because this inattention interfered directly in the result of the
process. The following were the main problems found: failure to choose someone to act
as secretary of the competitive examination process, causing serious disorganization in
the performance of the related activities, in turn causing errors that on some occasions
resulted in annulment; unnecessary circulation of documents to administrative instan-
ces that no longer had any involvement in deciding the outcome of the examination,
delaying the process; and lack of periodic training of staff members, so they could stay
abreast of the modiﬁcations made in the process by the higher authorities, hampering
communication and generating the need for rework.
Furthermore, the analysis of the data collected allowed identifying that collective
design of the ﬂows of the process, as well as clear working instructions, are of fun-
damental importance to provide transparency to managers about the bureaucratic
aspects of the process and show the dependence relations between the sectors involved.
The realization that acceptable results could only be attained with the coordinated
action of each party involved was a very important institutional gain for ICSA, because
it clearly showed that the conduct of all those involved has a direct effect on the results
obtained. This allowed those involved to understand the importance of their respective
roles in performing each activity of the process, resulting in a marked improvement in
the services rendered and the organizational climate.
After the discussion with all the people involved, we prepared a plan for improve-
ments, which became part of the redesign of the process, modeled in the “to be” format.
The chart below summarizes the improvements implemented for the teacher hiring
process of ICSA, allowing perception of the results achieved.
BEFORE (As Is)
AFTER (To Be)
There was a failure to appoint a secretary to
oversee the hiring process
A secretary is now named by the members of
the department that is requesting the hiring of
a new professor, or by the director of the
Institute
There was an absence of standardization of
the administrative steps of the process
The technical-administrative staff have
established a checklist of the necessary
documentation and the steps required to
carry out the process
There was no formality in the request for a
hiring process: it was done by email,
memorandum, among other means
A form was created to solicit the opening of a
competitive exam, indicating the members of
the board of examiners, the proﬁle of the
position, and the date and time of the
examination
(continued)
Rationalization of Organizational Processes
7

(continued)
BEFORE (As Is)
AFTER (To Be)
About 30% of the competitive exams to ﬁll
professorships were annulled
No processes have been nulliﬁed since the
implementation of the improvements
There was no training for the
technical-administrative staff
There is a project for ongoing training of the
technical-administrative staff members
There was slowness in the process: it used to
take an average of 45 days to be completed
The hiring process takes an average of 30
days to be completed
The BPD of the ﬁnal Hiring Process can be accessed in the following electronic
address: http://institutos.ufrrj.br/icsa/2017/10/17/diagramas-de-processos-de-negocios-
icsa-concurso-publico-para-cargo-professor-efetivo/.
5
Conclusion
The adoption of process mapping by ICSA provided a number of improvements in
execution of the activities by the administrative staff. The visualization of the process,
besides being a transparency beneﬁt per se, led to other beneﬁts, such as more
streamlined execution of the process, as demonstrated by the reduction of the average
time to conclude the hiring process, from 45 to 30 days. It was also possible to
standardize and systematize the examination procedures, giving greater consistency to
the information provided to the users of the internal services, reducing problems and
sticking points. Another important gain for the management of ICSA was the provision
of regular training to technical-administrative staff members, necessary to develop
better administrative routines, as well as a sensation of valorization of their work.
However, the beneﬁt mentioned the most by those involved in validating the new
process, involving adoption of the process mapping method, was the full visualization
of the process by all those involved in its execution. This has enabled discussing the
process in detail and preparing proposals for improvements, making it easier to plan,
monitor and control the process from start to ﬁnish. With this, other beneﬁts can be
mentioned, such as: reduced operational complexity; better quality of the service
provided; identiﬁcation and elimination of activities with low added value; identiﬁ-
cation and elimination of repetitive and unnecessary tasks; rationalization of merely
bureaucratic tasks; and more agile communication about activities and processes.
Now, with the process under good control, and with the clarity of the beneﬁts
obtained, it is possible to introduce a cycle of ongoing improvement, and to take
advantage of the lessons learned to repeat the work in other processes of the Institute.
8
E. Costa et al.

References
1. Campos, R.A., Lima, S.M.P.: Mapeamento de Processos: Importância para as organizações
(2012)
2. Oliveira, S.B. (org.): Gestão por processos - Fundamentos, técnicas e modelos de
implementação: foco no sistema de gestão de qualidade com base na ISO 9000:2000,
2nd edn. Qualitymark, Rio de Janeiro (2012)
3. Trigo, A.; Belfo, F.; Estebanez, R.: Accounting Information Systems: evolving towards a
business process oriented accounting. Procedia Comput. Sci., 988–994, October 2016
4. De Sordi, J.O.: Gestão por processos: uma abordagem da moderna administração. 3rd edn.
Saraiva, São Paulo (2012) (Revised and Updated)
5. Ebinger, M., Madritsch, T.: A Classiﬁcation framework for facilities and real estate
management –– the built environment management model (BEM2). Facil. Real Estate
Manag. 30(5/6), 185–198 (2012). https://doi.org/10.1108/02632771211208477
6. APQC.
Process
Classiﬁcation
Framework,
https://www.apqc.org/knowledge-base/
documents/apqc-process-classiﬁcation-framework-pcf-cross-industry-pdf-version-611.
Accessed 17 Oct 2017
7. Oliveira, S.B., Motta, R.A.M., Altemar S.: A gestão por processos e a interface humana:
identiﬁcando, descrevendo e classiﬁcando os processos de gestão de pessoas. In: V Simpósio
de Excelência em Gestão de Tecnologia (2008)
8. Martinho, R., Domingos, D., Varajão J.: CF4BPMN: a BPMN extension for controlled
ﬂexibility in business processes. In: Conference on Enterprise Information Systems,
International Conference on Project Management, Conference on Health and Social Care
Information Systems and Technologies, CENTERIS/PROJMAN/HCIST 2015, 7–9 October
2015 (2015). Procedia Comput. Sci. 64, 1232–1239 (2015)
9. Oliveira, S.B.: Insights para a formulação de questões de pesquisa. In: OLIVEIRA, Saulo
Barbará de (Org). Análise e melhoria de processos de negócios, pp. 251–259. Atlas,
São Paulo (2012)
10. Llatas, M. (org.): OSM: Organização, sistema e Métodos. 1st edn. Pearson Prentice Hall,
São Paulo (2012)
Rationalization of Organizational Processes
9

Beneﬁts of Process Simulation Software – The
Case of a Brazilian Mixed Public-Private
Company
Ricardo Luiz Schiavo do Nascimento1(&),
Saulo Barbará de Oliveira1, and Aparecida Laino Entriel2
1 Federal Rural University of Rio de Janeiro, BR – 437 – Km 7,
Seropedica 23897-000, Brazil
ricardo@frontin.com.br, saulobarbara@gmail.com
2 Federal Fluminense University, R. Passo da Pátria, 152-470 – São Domingos,
Niteroi 24210-240, Brazil
aparecidalaino@hotmail.com
Abstract. This paper presents an evaluation of the use of mapping and process
simulation software as a tool for value mapping, by means of analyzing the
costs, time frames and resources involved in each activity that composes the
process of procuring information and communication technology resources of a
company with mixed public-private ownership in the Brazilian electric power
sector. We mapped the company’s processes using the AS IS model and its
simulation, and based on the results of the simulation we suggested the nec-
essary corrections and adjustments. The simulation and analysis based on the
TO BE model contributed to the visualization of new bottlenecks and problems,
providing support for continuous improvements. The use of the ASTS&I model
enabled running simulations before implementing the changes in the process,
helping us to identify the possible problems by means of the performance
indicators. The ﬁnal result was redesign of the process and the generation of the
TO BE model, which allows the evaluation and improvement of processes in the
company studied before its practical implementation.
Keywords: Simulation  Simulation software  Value mapping
Improvement of processes
1
Introduction
Organizations adopt different management styles, according to their bylaws and the
laws that govern them. The system of managing the procurement processes of an
organization is a target to discover opportunities for improvements, according to dif-
ferent process management models, although each model presents advantages and
disadvantages. In the Brazilian public sector there are many so called mixed-ownership
companies, meaning corporations in which the government owns at least 50% plus one
of the total shares. The others are listed for trading on stock exchanges in Brazil and/or
abroad. These organizations have characteristics of private companies, because they are
subject to normal commercial laws on buying and selling products and services.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_2

However, the negotiation with suppliers is governed by speciﬁc legislation, which on
the one hand aims to guarantee legality and transparency, but on the other winds up
hampering their administrative processes with a set of rules and standards that limit
their ability to compete with companies in the private sector.
The Brazilian Constitution, promulgated in 1988, states in its article 37 the prin-
ciples that the public administration must obey: legality, impersonality, morality,
publicity and efﬁciency [1]. These principles are in particular evidence due to the
country’s current situation, where improvement of economic performance is seen as
critical. An example of this was the enactment of Law 12,783 in 2013, which reduced
electricity rates paid for generation by an average of 20.2%, conditional on renewal of
generation and transmission concessions [2]. That law, however, did not take into
consideration the installed capacity of companies in the electricity sector and their
ability to react to the changes, causing a negative cash ﬂow situation and losses for
three consecutive years.
This context caused management difﬁculties for all organizations subject to Law
12,783/13, easily be seen by analyzing the annual management reports and ﬁnancial
statements for the past ﬁve years of the company studied here, which in particular
reveals the difﬁculty in managing budgets for investments in information and com-
munication technology (ICT) resources. This difﬁculty motivated the development of
this study, with the focus on assessing the process of procuring ICT resources to ﬁnd a
solution by proposing ways to improve its efﬁciency.
Public organizations in particular must efﬁciently manage their resources, since
they not only have the responsibility of obtaining positive ﬁnancial results, but also
must serve society as a whole. In this respect, the mission of the company studied here
is: “To act in the energy markets in integrated, proﬁtable and sustainable form”. This
efﬁciency is measured by the relation between the result attained and the resources used
[3], so that improving an organization’s efﬁciency means obtaining better results with
fewer resources, i.e., producing more with lower costs.
Over the years, several tools have been developed to manage costs, such as
activity-based costing (ABC), cost-volume-proﬁt analysis, and total quality manage-
ment [4]. However, considering the way the implementation of these methods is
managed in practice, the results often fall short of expectations. The main objective of
this study was to assess, in a mixed-ownership company in the Brazilian electric power
sector, the applicability of process simulation as a tool to map value, using a speciﬁc
computer program for this purpose. In this respect, we ﬁrst analyzed the theoretical
underpinnings of the theme and then developed a model to support the value mapping
of the processes of purchasing ICT resources of the company in question.
Whereas process-based management helps an organization stand out from its
competitors leading to a competitive advantage, our intention is to contribute to the
advancement of studies and methods to assess process ﬂows, by means of a practical
approach that permits the evaluation and improvement of processes before their
implementation.
The article is divided into four sections besides this introduction: theoretical
foundation; methodology employed; practical application of the model; and ﬁnal
considerations.
Beneﬁts of Process Simulation Software
11

2
Theoretical Foundation
2.1
Process Modeling
Modeling of processes can be deﬁned as a set of instructions to construct graphs
combined with rules that deﬁne the design of a process. Although there are various
types of competing notations applied in process modeling tools, each with its speciﬁc
features [5], the Business Process Modeling and Notation (BPMN) technique is among
those most often used these days [6–8], the reason we chose it to model the process
studied in this work.
The objective of mapping the ﬂow of value of a current process (AS IS) is to identify
the activities that aggregate value and the sources of losses of this process, so they can be
eliminated by implementing a new value ﬂow in a future state (TO BE) [9]. Just as is
done in business processes, the current state (AS IS) is ﬁrst mapped. When this is
concluded, the mapping of the future state (TO BE) begins, incorporating the
improvements identiﬁed and eliminating bottlenecks and losses, i.e., factors or elements
that do not add value to the current processes [10].
Value and its respective ﬂow are directly linked to the lean enterprise idea [11]. The
deﬁnition developed is very similar to the deﬁnition of a process itself, namely the set
of actions necessary to move a product through all its transformation ﬂows, from raw
material to ﬁnal customer [10]. Mapping the value ﬂow is related to mapping a process.
Just as for notation techniques, a variety of mapping tools exist, and process analyst
must ﬁnd the one that can be best adapted to the reality observed, and assess its
adaptation to the use of the tools at hand. Here we used Bizagi Modeler, because of is
existing use in the organization studied and our familiarity with it.
2.2
Process Simulations
Simulations are similar to bench tests. By using computer programs it is possible to
support the design of new processes or analyze the performance of existing ones.
Conducting simulations allows estimating time frames, monetary values, types of
events and frequency of occurrences, among other performance factors. Process sim-
ulation programs are available in various versions, and some modeling programs also
have integrated simulation resources [12], as is the case of Bizagi Modeler.
Simulation is also used as an experimental laboratory to assess the real effects
regarding conditions, alternatives and possible courses of an action. Starting from this
principle, simulation contribute to the understanding, redesign and analysis of processes,
providing estimates of the impacts of changes and assessing their performance [13].
Therefore, simulation was one of the tools used to assess the performance of the
procurement process, identify problems and implement improvements. Table 1 presents
results that can be obtained through simulations.
2.3
Process Improvement
Before a company improves the experience of its customers with its products and
services, it needs to check internally whether the demands for resources are being met,
12
R. L. S. do Nascimento et al.

i.e., whether the internal processes are sufﬁciently organized to maintain harmony
among the ﬁrm’s various areas [14]. These veriﬁcations are possible by measuring the
level of service and experiences emerging from the process.
Operations use the ﬁrm’s resources as inputs, such as labor, materials, information,
technologies and equipment, to design, create and promote the service offered to
customers. Therefore, while a service is a process or activity, customers’ experiences
are their personal interpretation of the service process and their interaction and
involvement with it during the ﬂow, through a series of contact points [15].
In this study, to assess the improvements, it was necessary to use a monitoring
model, i.e., a model with performance indicators. Since processes need continuous
management, this requires metrics directly related to the needs of customers and the
requirements of the company, as parameters for future improvements [16]. On the other
hand, to obtain ongoing improvements, it is necessary to create a parallel between the
PDCA cycle (plan, do, check, act) and the BPM life cycle (business process man-
agement), mapping, redesigning and modeling business processes (planning), then
promoting their implementation (doing), followed by monitoring their execution
(checking) and ﬁnally introducing continuous improvements (acting) [17].
2.4
Competitive Cost Advantage
For a company to be among the most successful in the market, it must apply three
management approaches: total cost leadership, where the company that produces at the
lowest overall cost has an advantage over rivals; differentiation, meaning that com-
panies that have products with attributes that others do not offer have advantages; and
focus on determined markets, by segmenting their products to meet the desires of
speciﬁc publics [18].
From an economic standpoint, the value of a product to consumers can be repre-
sented by the price they are willing to pay to obtain the utility of the product or their
satisfaction in using it [19]. Therefore, customer satisfaction directly affects the value
attributed to the product (or service) supplied. Since the production of goods and
services adds value in each phase, it also adds costs, through the resources and time
applied to perform the activities. Allied with this, the concept of value chain analysis
Table 1. Typical examples of result variables
Objective of the model
Result variables
Measurement
Timing (cycle, waiting period, etc.), Costs, Resources, Yield,
Capacity, Bottlenecks, etc.
Visualization/Validation
Current Process, Problems, State of the System, Organizational
Relationships
Modeling/Testing
improvements
Allocation of Resources, Rationalization, Inﬂuencing/Leveraging
Technology
Development of
business models
Performance Data, Financial Data, Return on Investment
Communication
Owners of the Process, Decision Makers, Teams
Source: [13]
Beneﬁts of Process Simulation Software
13

has the objective of identifying the sources of the ﬁrm’s competitive advantage,
revising business practices to anticipate market trends, disclosing business rules, pro-
viding a complete vision of the information ﬂow within the organization and all the
relations between its processes, evaluating the proﬁtability of operations and the
positioning of products and services in the market, and ﬁnally, promoting better per-
formance of the ﬁrm’s processes [18].
In the case of cost competitiveness, there are two ways for a company to achieve
advantage. The ﬁrst is to control cost drivers and perhaps reconﬁgure the value chain,
focusing on the cost drivers of value activities, which represent a signiﬁcant portion of
total costs. The second is to adopt a more efﬁcient way to design, produce, distribute
and/or sell the product [18].
Many ﬁrms focus on regularly reconﬁguring their value chains, seeking a com-
petitive advantage in costs. However, reducing costs requires effort and constant
monitoring. Therefore, it is necessary to understand what activities should be priori-
tized to reduce costs without compromising the company’s activities. Cost drivers can
be understood as a series of factors that inﬂuence costs or operations, such as econo-
mies of scale, learning, patterns of resource deployment, linkages inside and outside the
chain, interrelations, integration, opportunity and policy [18].
Based on these foundations, it is possible do deduce that the ﬂow of value is
intimately associated with the organization of resources, be they ﬁnancial, material,
human or technological, each of which makes a contribution to aggregate value to the
product or service. In the speciﬁc case of purchasing ICT items, these can be either
products or services, each with its own speciﬁc processes. However, both have the
same objective: to satisfy the needs of clients, both internal and external.
3
Methodology
To carry out this study, we relied on methodological techniques involving both ends
and means [20, 21]. Regarding ends, this is an interventionist and applied study. It is
interventionist because we sought to intervene in the current situation for modifying it;
and applied due to the motivation to resolve a problem existing in a company in the
Brazilian electricity sector with mixed public-private ownership. Regarding means, we
used documental analysis, literature review and action research. Documents of the
company, both in printed and electronic form, were consulted to obtain information on
its current situation, while the literature review provided the theoretical framework to
support the development of a model to evaluate the internal processes.
The study counted on the participation of a focus group composed of six employees
with leadership positions in the company, for the purpose of identifying performance
problems and eliciting proposals for improvements in the process analyzed. All of them
knew and were affected directly or indirectly by the problem.
To develop the proposed model, we deﬁned steps, namely: mapping the current
situation of the company (AS IS mapping); analyzing the current state of the process
(AS IS analysis); conducting simulations to evaluate the ﬂow of the process and the
monetary values involved (AS IS simulation); analyzing possible improvements, for
subsequent implementation if found worthy (simulation analysis); including the
14
R. L. S. do Nascimento et al.

proposed improvements in the mapping, for possible implementation (TO BE map-
ping); and conducting simulations to obtain estimates of the results of the improve-
ments to be implemented in the process (TO BE simulation). Table 2 details the
procedure for development of the study.
Table 2. Typical examples of result variables
AS IS mapping
Interviews with key employees involved in the process; Study of
documents of the process; Application of the BPMN method
AS IS analysis
Evaluation of the process together with the key employees of the area;
Determination of the items of costs, resources and times involved in each
activity; Determination of the critical activities; Deﬁnition of the scenario
for simulation
AS IS
simulation
Utilization of software to map and simulate processes; Veriﬁcation of the
ﬂow of activities; Evaluation of the value ﬂow; Determination of
bottlenecks and unnecessary activities; Modeling of the results of the
process in terms of costs, resources and times
Simulation
analysis
Presentation of the result to the manager of the process; Evaluation of the
results obtained in the simulation; Development of proposals for
improvements
TO BE
mapping
Veriﬁcation of proposed improvements with key employees of the area;
Redesign of the process, introducing improvements; Application of the
BPMN method; Development of new documents of the process
TO BE
simulation
Evaluation of the improvements to be implemented; Application of the
software to map and simulate processes; Presentation of the possible
improvements to the manager of the process; Veriﬁcation of the
applicability of implementing the improvements; Preparation for
implementation of improvements
Source: Authors.
TO BE
Resources
Costs
Time
R
e
s
u
l
t
s
AS IS
Simulation
Implemen-
tation
AS IS
TO BE
Simulation
Fig. 1. ASTS&I Model Source: Authors.
Beneﬁts of Process Simulation Software
15

Because of the use of AS IS and TO BE mapping, interspersed with simulations, we
gave the model the name ASTS, and considering the implementation of the new
mapping at the end of the analytic process, we included “I” at the end of the name, to
yield ASTS&I. The meaning of this name is AS IS, Simulation, TO BE, Simulation and
Implementation. Additionally, based on the concepts presented, we developed a model
to simulate processes aimed at obtaining results in terms of costs, resources used and
time necessary for execution, as presented in Fig. 1.
4
Practical Application of the Model
The development of the proposed model included analysis of macro-processes, pro-
cesses, sub-processes and individual tasks. The main functions of this model are: (a) to
predict the results of the process in the simulation step; (b) to evaluate the use of the
resources employed in executing the process; and (c) to estimate the costs by deter-
mining the time for execution of each step of the process.
The use of the model was based on mapping and simulations to obtain the desired
results. Starting from the AS IS mapping we performed its simulation using the Bizagi
Modeler in a scenario near the reality of the company under analysis. All told, we
carried out 10 cycles, including alteration of scenarios, the resources allocated to the
process and the activities performed.
The AS IS modeling allowed identifying redundant activities and tasks involving
issuance of the documents called Contractual Scope, Technical Speciﬁcations and
Budget Report. The ﬁrst two of these documents had ﬁelds common to each other, but
were produced separately, generating conﬂicts, inconsistencies, errors or other defects.
This always-required rework, lost time and wasted resources allocated to the process.
The same thing occurred with issuance of the Budget Report and obtaining the ref-
erence price, two activities that were subsequently uniﬁed, maintaining their docu-
mentation, which enabled increasing agility in executing the process of purchasing ICT
resources and facilitating the task of auditing by oversight bodies. The AS IS modeling
as shown in https://sites.google.com/a/frontin.com.br/picture/home/ASIS.png.
The AS IS simulations revealed the inefﬁciency of the activities and tasks initially
identiﬁed as redundant and provided security for the decision to eliminate them. It also
allowed discovering the absence of a method or system for monitoring the process,
which seriously impaired its performance, indicating that the use of software to monitor
and register activities would provide further gains in terms of checking the ﬂow and
time necessary to execute tasks.
By using the ASTS&I, the mapping tools, in particular the AS IS mapping, it was
possible to develop the TO BE mapping, which allowed identifying: (a) the need for
improvements in the process of purchasing ICT resources; and (b) activities that were
candidates for automation and that could be improved only by intensifying the allo-
cation of the resources already utilized in the process under analysis. Among the
improvements incorporated to the TO BE modeling were the recommendation to use
Clarianty (a document management tool), to enable monitoring the entire process and
issuing all the documents necessary to monitor the ﬂow of the process, aiming to obtain
16
R. L. S. do Nascimento et al.

reference prices in parallel with generation of the technical speciﬁcations and other
activities, such as deﬁnition of scope, preparation of the Contractual Term Sheet, etc.
The results of the TO BE simulation as shown in https://sites.google.com/a/frontin.
com.br/picture/home/TOBE.png, allowed identifying and deﬁning performance indi-
cators to accompany the use of resources, the costs involved and times necessary to
ﬁnalize the processes. It also generated important information, such as the rate of
utilization of resources and the time for execution of the process, as well as monitoring
and evaluation of the efﬁciency of the processes studied. Table 3 shows the main
simulation results.
Using the ASTS&I model, it was possible to verify that the number of processes
ﬁnalized grow up, promoting advantages, when the costs are rated. The same was
veriﬁed with the time of processes. The rate of problem correction was considered low,
shown improvement of the effectiveness. As a ﬁnal point, the use of available resources
was reduced (people, equipment and materials), possibility to increase the quantity of
processes.
Additionally, the simulation and analysis based on the TO BE model contributed to
the visualization of new bottlenecks and problems, providing support for continuous
improvements. By collating the results shown in Table 1 it was possible to assess the
procurement process and implement improvements [16]. The use of the ASTS&I
model enabled running simulations before implementing the changes in the process,
making it possible to identify the possible problems by means of the performance
indicators [12]. The last step of the model was called implementation, since it involved
putting the structure of the model in place according to the reality of the organization
studied, including establishment of the new process/redesign and modiﬁcation of the
way of producing [18].
Table 3. Results obtained from the simulation
Name of the indicator
AS IS
TO BE
Improvement obtained
Number of processes ﬁnalized
129 processes
146 processes
13.18%
Processes requiring rework
18 processes
1 processes
94.44%
Effectiveness rate of the process 87.75%
99.3%
13.16%
Rate of problem correction
12.25%
0.7%
94.29%
Minimum time of the process
4 days, 22 h
2 days, 18 h
48.34%
Maximum time of the process
119 days, 21 h 6 days, 11 h
94.87%
Average time of the process
45 days, 19 h
4 days and 4 h 90.26%
Total cost
R$ 847,230.84 R$ 449,048.40 47.00%
Cost per ﬁnalized process
R$ 6,567.68
R$ 3,075.67
53.17%
Use of available resources
99.23%
75.74%
23.67%
Source: Authors.
Beneﬁts of Process Simulation Software
17

5
Final Considerations
The objective of this work was to assess, in a company with mixed public-private
ownership in the Brazilian electric power sector, the applicability of process simulation
as a tool for value mapping, using a speciﬁc computer program for this purpose.
The application of the ASTS&I model to the company in question enabled iden-
tifying improvements in processes, by evaluating the costs, times and resources utilized
and separating necessary and unnecessary activities by using mapping, modeling,
simulation and remapping. This in turn permitted consolidating the implementation of
improvements to boost bottom-line results. The mapping of values, using simulations,
revealed the possibility of obtaining greater agility in gathering and analyzing essential
information to support improvements of processes, such as reducing the time for
execution of activities and the associated costs, as well as lowering the rate of failures
and errors.
The development of the ASTS&I model contributed to the incorporation of mod-
eling tools and improvements of processes, allowing the implementation of the ongoing
improvement cycle, one of the greatest challenges of good management.
Based on the results obtained, we believe that the use of the ASTS&I model
promoted improvements in the process of purchasing ICT resources in the organization
studied, conﬁrming the applicability of simulation using software, such as value
mapping, before implementation of changes, meeting the objective proposed for the
study. Nevertheless, considering seeking to obtain a possible consolidation of the
model and its broader and more generalized use, to check its efﬁciency in other
business settings.
References
1. Brazil: Constituição. Constituição da República Federativa do Brasil (1988)
2. Brazil: Law No 12.783, 11 January 2013
3. ISO 9000:2015: Quality management systems - Fundamentals and vocabulary. International
Organization for Standardization, Geneva (2015)
4. Paiva, E.V., Fonseca, F., Corgozinho, P.R., Ferreira, R.M.: GESTÃO DE CUSTOS. In:
Anais do Congresso Brasileiro de Custos-ABC (1999)
5. Dijkman, R., Dumas, M., van Dongen, B., Käärik, R., Mendling, J.: Similarity of business
process models: metrics and evaluation. Inf. Syst. 36(2), 498–516 (2011)
6. OMG: Business Process Model and Notation. Object Management Group (2011)
7. Martinho, R., Domingos, D., Varajão, J.: CF4BPMN: a BPMN extension for controlled
ﬂexibility in business processes. In: Conference on Enterprise Information Systems/
International Conference on Project Management/Conference on Health and Social Care
Information Systems and Technologies, CENTERIS/PROJMAN/HCIST 2015, 7–9 October
2015, Procedia Computer Science, vol. 64, pp. 1232–1239 (2015)
8. Allanni, O., Ghannouchi, S.A.: Veriﬁcation of BPMN 2.0 process models: an event
log-based approach. In: Conference on Enterprise Information Systems/International
Conference on Project Management/Conference on Health and Social Care Information
Systems and Technologies, CENTERIS/PROJMAN/HCIST 2016, 5–7 October 2016,
Procedia Computer Science, vol. 100, pp. 1064–1070 (2016)
18
R. L. S. do Nascimento et al.

9. Walter, O.M.F.C., Zvirtes, L.: Implantação da produção enxuta em uma empresa de
compressores de ar. XXVIII ENEGEP (2008)
10. Recker, J.: Opportunities and constraints: the current struggle with BPMN. Bus. Process
Manage. J. 16(1), 181–201 (2010)
11. Greef, A.C., Freitas, M.C.D.: Fluxo enxuto de informação: um novo conceito. Perspectivas
em Ciência da Informação 17(1), 37–55 (2012)
12. Baldam, R.: Técnicas de otimização e modelagem de estado futuro. In: Oliveira, S.B. (Org.)
Análise e Modelagem de Processos de Negócios, Chapter 11, pp. 116–126. Atlas, São Paulo
(2012)
13. Braconi, J.: O papel e contribuição da simulação para a melhoria de processos. In: Oliveira,
S.B. (Org.) Análise e Modelagem de Processos de Negócios, Chapter 8, pp. 127–142. Atlas,
São Paulo (2012)
14. Botha, G.J., Van Rensburg, A.: Proposed business process improvement model with
integrated customer experience management. S. Afr. J. Ind. Eng. 21(1), 45–57 (2010)
15. Johnston, R., Kong, X.: The customer experience: a road-map for improvement. Manag.
Serv. Qual. Int. J. 21(1), 5–24 (2011)
16. Hammer, M.: What is business process management? In: Brocke, J.V., Rosemann, M. (eds.)
Handbook on Business Process Management 1, pp. 3–6. Springer, Heidelberg (2015)
17. Sganderla, K.: Planejando a implementação de melhorias com base no ciclo de gestão por
processos. In: Oliveira, S.B. (Org.) Análise e Modelagem de Processos de Negócios, Chapter 11,
pp. 169–185. Atlas, São Paulo (2012)
18. Porter, M.E.: Vantagem Competitiva. Campus, Rio de Janeiro (1992). Translated by
Elizabeth M. de Pinho Braga
19. Richins, M.L.: Valuing things: The public and private meaning of possessions. J. Consum.
Res. Chicago 21, 504–521 (1994)
20. Vergara, S.C.: Projetos e relatórios de pesquisa em administração, 8th edn. Atlas, São Paulo
(2007). 96 p.
21. Vasconcelos, J.B., Kimble, C., Carreteiro, P., Rocha, Á.: The application of knowledge
management to software evolution. Int. J. Inf. Manage. 37(1), 1499–1506 (2017)
Beneﬁts of Process Simulation Software
19

IT Service Management Using COBIT
Enablers: The Case of Brazilian National
Institute of Cancer
Sandro Luís Freire de Castro Silva1(&), Saulo Barbará de Oliveira2,
Marcos Azevedo Benac2, Antônio Augusto Gonçalves1,
and Carlos Henrique Fernandes Martins1
1 Instituto Nacional do Câncer - COAE Tecnologia da Informação,
Rua do Resende 195, Rio de Janeiro 20230-092, Brazil
sandrofreire@gmail.com
2 Universidade Federal Rural do Rio de Janeiro,
Rodovia BR 465 KM7-Seropédica, Rio de Janeiro 23890-000, Brazil
Abstract. Due to increased demand for Technology Services in Brazilian
Public Institutions, Information Technology (IT) planning activities are
increasingly being used in organizations and IT services are maintained with
goal differential in the market. The purpose of this article is to investigate how
the ITIL implementation contributes to Brazilian National Institute of Cancer
management model by using COBIT IT governance practices to measure
organizational performance. For this, a qualitative ﬁeld research was done,
through interviews aimed at evaluating the perception of the subjects investi-
gated on the research them. It was identiﬁed that the strategies converge when it
comes to the initiative to improve administrative processes. The main conclu-
sion is that ITIL and COBIT practices are equally relevant for improving the
efﬁciency of governance cycle management in the institution under study.
Keywords: IT governance  IT planning  IT service management
ITIL  COBIT
1
Introduction
Life in modern societies has become hostage to the Information Technology (IT). Little
or nothing is more possible to do without it. IT is present in the cycles of life and
organizations.
Although different characteristics of private for-proﬁt organizations, public orga-
nizations, have shown increasing concern for the establishment of effective procedures
for IT Governance and presented high dependence on IT, due to increased demand for
their services in institutions public.
Contemporary organizations are increasingly investing in mechanisms for imple-
menting Information Technology Governance. With controls mechanisms advent that
demand has been more latent. In public sector, government organizations have rec-
ognized that operative public policies for addressing the requirements of citizens
request collaboration with other public, civic, and private organizations. Additionally,
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_3

improvements like crowdsourcing and public-private partnerships increase the creation
of networks [1].
In Brazil, this reality is an important part of the public corporation’s context, it is
knowing that recent studies show that investment in technology allows gains in pro-
ductivity. In last year, the department responsible for overseeing the technology in
Brazil Government, called Tribunal de Contas da União (TCU), has conducted a survey
of the governance landscape of IT in government. That survey report, carried out with
the objective of monitoring the situation of ITG in the Federal Public Administration,
held every two years by this Court.
In 2014, the last report presented that 67% of the organizations adopt practices of
corporate governance, with 41% applying in their totality. The effort to reach a high
maturity stage often goes beyond technical barriers and affects the organization’s
business, and in some cases, may act negatively [2].
Brazilian public organizations strive to implement IT management. Computer
User’s Service Center of National Cancer Institute (INCA - a public health organiza-
tion) has been working to bring Service Management following Information Tech-
nology Infrastructure Library (ITIL) as a guide to good practices for this. For ITIL
practices into its most accurate form for the institution’s reality, that initiative is driven
by the enhancement of IT service management.
In last year CA Service Desk Management software was purchased by DTI (INCA´s
Technology Department) to optimize the ticket system and ensuring that reports of
accountability for IT infrastructure contracts are reliable. This application covers ﬁfteen
ITIL framework processes that enables the effective management of a service through its
life cycle. This tool reduces the business risk and cost associated with IT.
INCA’s service users beyond the professional question, has a differentiated factor
since qualifying the work causes that the assistance to the patients is directly affected.
This study aims to investigate how the implementation of ITIL can contribute to
INCA’s management model by using IT Governance practices to identify and measure
organizational performance.
1.1
National Cancer Institute and IT View
Brazil presents a complex scenario in cancer treatment. The occurrences and rates of
death have been increasing, around 600.000 cases every year: they are mainly
increasing when it comes to prostate cancer in men and breast cancer in women.
Researches has found that waiting lines for treatment and diagnosis have turn into
routine in many regions of the country, resulting in patients being diagnosed at
advanced stages of the disease.
The National Cancer Institute plays a multiple role in all areas of cancer prevention
and control in Brazil – prevention, epidemiological surveillance, treatment, informa-
tion, education and research. As a technical division of the Federal Government, under
the direct administration of the Ministry of Health, the Institute provides cancer care
within the Integrated Public Health System (SUS) [3].
Since 1997, INCA has made deeply changes in its organization, with the purpose of
converting its Research Center (RC) into a Technological Development Center
(TDC) that is skilled of leading Brazilian efforts towards the development of diagnostic
IT Service Management Using COBIT Enablers
21

and treatment protocols in the oncology. From this background of improvement,
Information and Communication Technologies (ICTs) have played a relevant role in
supporting innovation processes in cancer care.
Moreover, INCA formulates and coordinates public policies, develops research
activities and disseminates practices and knowledge on medical oncology. Due to its
patterns of excellence, which are comparable to the world´s major cancer care centers,
INCA has become a national and international model in cancer control. A key factor for
INCA to fulﬁll its mission is the dedication of its professional staff whose integrated
work is based on a participative management model. In 2004, this model was incor-
porated with success, involving all our employees and changing for better delivery of
services to the Brazilian population [3].
Every day, new demands has been present in volume and complexity resulting from
more speciﬁc technological solutions of the hospital area which being installed in
INCA units; growth INCA workforce with new members and employees and
automation of hospital procedures.
The requirement of a better performance for IT area is increasingly intense,
demanding the management of solutions, systems and equipment ensures the innova-
tion, safety and permanent availability of services. Consequently, it is necessary a
constant activity of prospecting, installation, maintenance, monitoring and improve-
ment of the entire technological solution which requires technical services in IT. The IT
Division (DTI), faces a challenge that goes beyond the currently capacity. The
department has been adapting the new rules of the Brazilian government, together with
the need to adapt to the emergence of new technologies.
Currently the Service Desk team attend more than six units located in Rio de
Janeiro city. These units are in addition to hospitals, research facilities, administrative,
hotels and warehouses. Therefore, control is necessary to ensure quality management
of information and control software usability is critical for this to happen.
1.2
INCA’s ITIL View
In 2016, INCA’s administrators assumed the commitment to implement the Service
Management following ITIL as a guide to good practices for this. They assume Service
Management deﬁnition as the set of skills of the organization to provide value to the
customer in the form of services [4].
In third version, ITIL has a core of conducting activities called Service Strategy
Book, which guides the other books: Service Design, Service Transition, and Service
Operation. They surround all processes in the Improvement Book Continuous Service,
considering everyone as phases of the service life cycle, the Strategy being the initial
phase of the service. Processes and functions are distributed throughout the life cycle,
as shown in the Fig. 1.
22
S. L. F. de Castro Silva et al.

To be understood, one can divide the life cycle into three groups of concepts: one of
requirements analysis and initial deﬁnition, where are the books of Strategy and
Design; another one of implantation in the productive/operational environment, where
is the book of Transition and ﬁnally, and operation and improvement in production,
where are Operations and Continuous Improvement of Services. The deﬁnition that
INCA’s IT assumes for the life cycle are described as follows:
(a) Service strategy: identifying needs and business needs that are provided by IT
services. The requirements are agreed and documented in a Service Level Package
(SLP).
(b) Service design: from the requirements is designed the IT solution in the form of
services, in all its aspects, that are documented in a service design package (SDP).
The SDP is a document of speciﬁcations and characteristics of the services.
(c) Service Transition: deals with the implementation in production. Such an
implementation is tested and monitored as well as validated. The service
knowledge management system (SKMS) is updated with information from the
production environment.
(d) Service operation: the service is maintained in operation and functioning per the
service level agreement (SLA) established to generate the expected results.
Continuous Service Improvement (CSI) identiﬁes opportunities for service
improvement.
Fig. 1. ITIL life cycle [4]
IT Service Management Using COBIT Enablers
23

Assuming an ITIL perception for INCA IT Service, DTI stipulated to implement
only ten ITIL process in Service Desk division until March 2017, that would serve as a
starting point for a continuity of the project: ITIL implementation in all sectors of the
division. To aim project goals, the COBIT enablers were applied, so that they could
serve as a basis for the elaboration of these processes. The method will be explained on
methodology chapter. The enablers are shown in Fig. 2.
In this article COBIT and ITIL will be related with complementary roles: COBIT
will serve as a pillar for an analysis of the current management model, with an IT view.
ITIL, in its turn, will collaborate by providing a reference on how to apply these
activities, how to design them in a process, deﬁne their roles in detail, and general
techniques and guidelines on how to execute the process. Although COBIT also
provides complementary publications that guide the “how to apply”, ITIL is the best
known and adopted reference for this purpose.
2
Methodology
In March 2017 was started the data collection to evaluate the relationship between
Organizational Management Model and IT Service Management. The study was
conducted with the entire team of DTI IT infrastructure, and proposed to all users who
Fig. 2. COBIT enablers [5]
24
S. L. F. de Castro Silva et al.

participate in project evaluation, once the frontline team is responsible for imple-
menting the project.
To aim the paper goals, from the point of view of IT Governance, a documental critical
analysis of the Strategic Planning was carried out. This study was completed through
interviews. For this, an interview script was used, based on an adaptation of the organi-
zational situation survey questionnaire already used in other studies of this nature [6].
The research, for diagnosis of the Organizational Management Model and IT
Service Management situation, used open unstructured interviews. The unstructured
interview, also called an in-depth interview, instead of answering the question through
several pre-formulated alternatives, seeks to obtain what the investigated subjects
considered the most relevant aspects of a problem: the descriptions of a situation of
Study [7]. It is observed that the use of this script did not exempt the freedom of the
interviewee to speak about the points in which he considered more important.
2.1
Sample
The interviews were applied to 15 users. Two participants were female and thirteen are
male. The age of the participants was between 18 and 60 years. Ten of them are
graduates in IT and ﬁve in other professions (Administration and Engineering). Two
participants are INCA´s employees and thirteen are outsourced to execute IT infras-
tructure jobs.
3
Data Analysis, Discussion and Results
At the time of the exploration of the material, seven categories of analysis could be
deﬁned. This choice was based on COBIT 5 enablers which refers to the application of
the holistic approach to IT. COBIT 5 enablers are deﬁned as:
(a) Factors that, individually and collectively, inﬂuence whether something will work –
in the case of COBIT, governance and management over enterprise IT.
(b) Driven by the goals cascade, i.e., higher-level IT-related goals deﬁne what the
different enablers should achieve.
Table 1 describes the categories and how analysis used them based on COBIT 5.
IT Service Management Using COBIT Enablers
25

The results obtained in the interviews will be discussed below.
3.1
Principles, Policies and Frameworks
Privacy and IT policies have a lot in common: both are considered important; orga-
nizations are concerned about not having them, but not everybody fully understands the
implications and consequences if they are wrongly understood [4]. This point initially
deals with the knowledge of employees about what an institutional strategy depends on
IT to be served. In this category, the data analysis showed that the respondents did not
know about any formal methodology or IT policy. In many cases, it has been reported
the use of non-formalized techniques, mainly in the maintenance in proprietary systems
and in the politics of selection and use of adopted solutions.
For this category, aspects of the institution were discussed, as well as knowledge of
strategic planning, organizational knowledge, vision, mission and values. In their
totality, the respondents did not demonstrate knowledge about these aspects, which
points to failures in the contracting process of third parties, since service providers must
know the organization in which they will provide their activities.
3.2
Processes
Regarding the processes, a great attachment to informality was observed, based on the
intrasetorial trust relationship. Reports of non-formal processes that have ten years of
Table 1. COBIT 5 enablers [5]
Category
Deﬁnition
Principles, Policies and
Frameworks
Describe an organized set of practices and activities to achieve
certain objectives and produce a set of outputs in support of
achieving overall IT-related goals
Processes
Are the key decision-making entities in an organization
Organizational Structures
Of individuals and of the organization; very often
underestimated as a success factor in governance and
management activities
Culture, Ethics and
Behavior
Are the vehicles to translate the desired behavior into practical
guidance for day-to-day management
Information
Is pervasive throughout any organization, i.e., deals with all
information produced and used by the enterprise. Information
is required for keeping the organization running and well
governed, but at the operational level, information is very often
the key product of the enterprise itself
Services, Infrastructure
and Applications
Include the infrastructure, technology and applications that
provide the enterprise with information technology processing
and services
People, Skills and
Competencies
Are linked to people and are required for successful completion
of all activities and for making correct decisions and taking
corrective actions
26
S. L. F. de Castro Silva et al.

existence, therefore, rooted in the IT culture of the researched sector, and have never
been reviewed. In contrast, the use of CA Service Desk Manager allowed many pro-
cesses to be registered and deﬁned, but it was identiﬁed that access to these platforms
should be facilitated, since it is a predominant factor for effective process management.
Some processes in the current management model interfere with the deployment of
service management. An example is a homogeneity in the service strategy.
Although INCA is a research institute that provides assistance, the management model
does not contribute to a scheduling of care, and because of this, it has difﬁculty in
prioritizing services that directly impacting patient.
3.3
Organizational Structures
Typically, teams or departments are already well deﬁned, but it is critical to clarify the
roles and functions within the framework from the point of view of the processes.
The IT position in organizational structures still suffers greatly inﬂuences from how
managers see their role in value creation. However, this vision has been changed in
recent years and managers must be aware of these changes.
Regarding organizational structures, it was identiﬁed that all employees have
knowledge about them, but almost all the interviewees reported that the current
structure is not able to meet the demands of IT services. With the analysis of the data, it
was noticed that these structures are formalized and published in the Information
Development Plan (IDP), but that do not seem to be considered by the interviewees.
3.4
Culture, Ethics and Behavior
The discussion of enabler 4, Culture, Ethics and Behavior, offers probably the most
informative content in respect to the implementation of ethics in an organization. The
practices noted in this enabler are communication, enforcement, incentives and
rewards, awareness, rules and norms, and champions.
The data analysis results also made it possible to identify great resistance in the
implementation of formal methodologies, which is related to aspects of organizational
culture and server behavior.
In the same way, both in the institutional strategy analysis and in the servers’
reports, a strong attachment to traditional and bureaucratic procedures were observed.
The intraorganizacional behavior model does not respect hierarchies, so that
technical decisions on information technology are made by professionals who are not
from this area.
3.5
Information
In this category, relevant points were observed. INCA has a highly-evolved commu-
nication structure, in which it has a communication division that operates in various
media, both print and digital. However, the analysis of the categories showed that there
are still difﬁculties in locating strategic directions. It was identiﬁed the absence of
information handling policies, as well as notes made regarding security.
IT Service Management Using COBIT Enablers
27

Information is not categorized and is not related to the existence of mechanisms for
secrecy of information.
3.6
Services, Infrastructure and Applications
Through this category of analysis, it was possible to identify that the applications made
available do not directly meet the strategic objectives of the institution. Since the
scheduling of care is a necessity of the current organizational model.
Many services in operation reported are linked to traditional administrative ser-
vices, not being developed improvement tools or that meet the objectives of institute
expansion present in the institutional goals of the institutional plan.
3.7
People, Skills and Competencies
The totality of the interviewees pointed the team integration as a strong point of the
division, but the same totality understands that need a speciﬁc qualiﬁcation. On the
other hand, the analysis of this category shows that there is a high degree of divergence
between the servers’ abilities and the competencies required to achieve the strategic
objectives.
Through the interviews, it was veriﬁed that the necessary job experience required
for the exercise of the work, are not the same as the technical specialization of the
employees of the sector.
4
Conclusion and Recommendations
The interviews made it clear that the implementation of ITIL recommended service
management practices is an initiative that involves cultural change. People will ques-
tion about having to do things different from what they have always done. Therefore,
senior management support is needed to inﬂuence change.
The analysis of the results showed that there are points requiring adjustment
between the ITIL Implementation Strategy and the organizational management model,
from the point of view of the COBIT activators as a category of analysis.
The results of the research stimulate the creation of new indicators to make accurate
measurement of the degree of alignment between IT and strategic institutional objec-
tives. It is believed that with the development of these indicators the measurement
could be performed with more precision.
Some problems were more frequently reported, such as the absence of standard
procedures and processes, although it has already been adopted a tool that manages the
IT service with a knowledge management system, which aims to formalize the routines
of intrasetorial work. This, in a way, indicates the need to review how this tool is being
used in the practices of the unit studied.
It was identiﬁed that the strategies converge when it comes to the initiative to
improve administrative processes. When this happens, people tend to be more col-
laborative. Even with operational inconsistencies, some processes have been system-
atically improved, such as meeting demands and managing software development
28
S. L. F. de Castro Silva et al.

projects. There is also convergence regarding the need to qualify the servers. The
internal training plans and the employee development proposal present in the organi-
zational plan are compatible, being applied and developed during the research process
itself.
The most important conclusion is that ITIL and COBIT are equally relevant to
accomplish this part of the governance cycle, except that the governance framework
provides indicators for processes that are not part of the service management library.
Considering the limitation of the type of research applied to the Department of
Technology of a speciﬁc hospital unit, in the present case, the National Cancer Insti-
tute, the authors suggest its replication in a greater number of ITIL user units, aiming at
the extension of the study, in order to increase the sample and to enable generalization.
References
1. Dawes, S.S., Gharawi, M.A., Burke, G.B.: Transnational public-sector knowledge networks:
knowledge and information sharing in a multi-dimensional context. Gov. Inf. Q. 29(Suppl. 1),
S112–S120 (2012). https://doi.org/10.1016/j.giq.2011.08.002
2. Tribunal de Contas da União, Relatório de Levantamento, March 2014
3. INCA - Instituto Nacional de Câncer José de Alencar Gomes da Silva, http://www.inca.gov.br.
Accessed 24 Jan 2017
4. ITIL V3, ITIL Ofﬁcial Website (2012), http://www.itil-ofﬁcialsite.com/. Accessed 1 July 2012
5. ISACA: COBIT 5: A Business Framework for The Governance and Management of
Enterprise IT. Information Systems Audit and Control Association, Rolling Meadows (2012)
6. Oliveira, S.B., Almeida Neto, M.A., Oliveira, Barbará, F.N.: Planejamento Estratégico de TI:
Formulário de Levantamento de Dados sobre a Situação geral da Organização (FLD01)
Relatório Técnico de Projeto. UFRRJ/DCAC: Seropédica – RJ (2013), http://www.slideshare.
net/sbarbara/. Accessed Jan 13 2017
7. Richardson, R.J.: Pesquisa social: métodos e técnicas, 3rd edn. Atlas, São Paulo (2007)
IT Service Management Using COBIT Enablers
29

Towards a Forensic Analysis of Mobile Devices
Using Android
Estevan Gomez-Torres1(&), Oswaldo Moscoso-Zea1(&),
Nelson Herrera Herrera1(&), and Sergio Lujan-Mora2(&)
1 Carrera de Ingeniería en Informática, Universidad Tecnológica Equinoccial,
Av. mariscal Sucre y Mariana de Jesús Quito, Quito, Ecuador
{estevan.gomez,oswaldo.moscoso,
nelson.herrera}@ute.edu.ec
2 Universidad de Alicante, Carretera de San Vicente
del Raspeig s/n, Alicante, Spain
sergio.lujan@ua.es
Abstract. The high utilization rate of mobile devices highlights the problem of
vulnerability. As a result, new cybercrime techniques are created, in response to
which new forensic techniques must be created, so we can deduct the impor-
tance of this paper. For some years now, there has been a signiﬁcant growth in
the use of mobile devices in daily life, since they allow to carry personal data in
a practical, easy and comfortable way. These data are, in many cases, the target
of malicious people, who, taking advantage of the vulnerabilities that these
devices present, are capable of illegal actions, usually for unlawful purposes.
The current research proposes, using a comparative method to allow us to
formulate a forensic analysis to mobile devices with Android operating system;
based on the chain of custody guidelines, compliance stages, and phases and to
detect ﬁndings, nonconformities, locate vulnerabilities. Based on this process we
can determine the origin of the leading causes of different types of events or
crimes committed from a mobile device. Additionally, using a decision matrix,
the best software for performing the forensic analysis is chosen and using
Balanced Scorecard, indicators are evaluated.
Keywords: Forensic analysis  Android  Chain of custody
Balanced scorecard  Mobile forensics methodologies
1
Introduction
If we analyze the current situation, we can observe that most people have as a priority
to own or use a cell phone with the initial purpose of satisfying the need to commu-
nicate, proper to every human. Thanks to the convergence of telecommunications, that
same mobile device that is used for voice communications is already providing another
series of services or functionalities. Android is the dominant operating system in
mobile devices, surpassing the 1,000 million devices, because of its versatility and the
fact of easy access to work. It has a high global demand, for that reason, it has been
evolving in its functionality, content, and features exponentially [1]. Gartner made a
prediction [2], he said that by 2016 at least 89% of business emails will be consulted
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_4

through mobile devices. Tied to this growth software companies will launch new
management platforms, services, tools, content, video conferencing, shared tasks. I.e.
every task that one can develop from a desktop computer can be done from a mobile
device [3]. In 2015 and 2016, the community of mobile device users was shaken by
two very typical cases that have left their mark on vulnerability systems and access to
information of users. It was December 2, 2015, at 10:59 am (UTC-8) at the Inland
Regional Center in San Bernardino, California [4]. A banquet of the San Bernardino
County Public Health Department; which was held in an auditorium of at least one
hundred people. It was attacked by shooters equipped with long guns and balaclavas; as
a result of which 14 people died, once perpetrated the fact, the authors ﬂed in a sports
utility vehicle. The suspects died after a shootout with the police, one of them a woman
and the other a man. As the US government wanted to access the information contained
in the phone, they imposed a legal action to force Apple to help break through the
security barriers of the phone. Apple refused to help the FBI, saying that if it did
compromise the safety of all iPhone users. The company argued that the authorities did
not understand the consequences of creating a so-called “back door” to access the
phone. The FBI acknowledged having paid to Cellebrite by that work at least 1.3
million dollars (1.1 million euros). Apple declined, arguing that it violated the privacy
of its users since a master key can be used on any device. However, the action, far from
ending the debate, fueled ﬁre, raising a series of questions about the tools used and the
future of the privacy of data contained in mobile devices.
It was August 22, 2016, in Spain [5]. While celebrating the Festivities of the
locality of A Pobra Do Caramiñal, Diana Quer is with a group of friends, but at 2:30
she decides to go home, where she spends her summer vacation with her mother and
sister. From there, her track was lost. In the following hours, beats are made without
success and soon after the case takes social dimensions. Afterward, police have tried to
investigate the mysterious disappearance of Diana. However, although the mobile
device that was found in a river near the town has been restored and operates appar-
ently regularly, two alternatives were discussed. A method to hack it, trying to avoid
that the mobile device blocks itself, or hire the Israeli company Cellebrite. The Israeli
technology, the ﬁrm that was hired by the United States FBI in the case of San
Bernardino, California 2015. According to Höbarth [4], the most logical way would be
to have used an exploit (a software tool designed to take advantage of a failure of a
computer system) that exploits a vulnerability. Thus, one could take control of it and be
able to avoid limiting the password. Another possible way to unlock it without losing
the information is to clone an image of the phone, that is, a physical copy of the device
to test passwords even when the limit of attempts is exceeded [5]. All digital infor-
mation that can be stored at mobile devices can become evidence that reveals details of
activities performed on the mobile device; these activities are investigated by
answering the same questions on which excellent forensic analysis is based: what was it
done? how was it done? what order was it made? [6]. The principal contribution of the
present article is to propose a new model for forensic analysis to mobile devices with
Android operating system. By using it, any researcher will be capable of detect ﬁnd-
ings; nonconformities; locate vulnerabilities and determine the primary causes of the
different types of crimes and events made using a mobile device were.
Towards a Forensic Analysis of Mobile Devices Using Android
31

2
Theoretical Framework
According to OWASP [7], top ten vulnerabilities in mobile applications was the subject
discussed by Tobias Zander at the conference Take off 2014 [8]. Moreover, some
aspects should be considered: (1) Weak controls on the server side. (2) Insecurity in data
storage. (3) Insufﬁcient transport layer protection. (4) Unwanted data leak. (5) Poor
authorization and authentication. (6) Cryptography Rupture. (7) Customer-side injec-
tion. (8) Security decisions: through unreliable entries. (9) Inaccurate session handling.
(10) Lack of binary protections. We have to remark that other authors have worked in
Forensic Analysis of Mobile Devices such as Vidas et al. [9], which present the ﬁrst
generic scope of forensic analysis for Android devices, introducing the analysis of
several speciﬁc devices. “Forensic Analysis in Mobile Devices”; Of Stirparo, which
generate the MobiLeak Project, the idea behind that project is to investigate existing
methodologies, and possibly develop new ones, that are used or can be used to perform
mobile application privacy assessments [10].
3
Methodology
The nature of this article is a research of a technological or applied type since it involves
the elaboration of stages and phases of a model that allows performing a forensic
analysis to mobile devices with Android Operating System, the one that will be tested
afterward at Sect. 5. In such a way, it is possible to collect information that helps to
clarify the events that have been carried out during a speciﬁed time using the mobile
device. In our previous work [11] we have deﬁned different indicators to solve the
relationship between the classiﬁcation of levels for applying forensic and the results that
are explained in this section. The indicators inventoried here are general guidelines; for
the forensic analysis; from the: (a) Perspective of the client: (5 indicators) and from the
process perspective (2 indicators); where objective and responsible are included, based
on the previous study presented by the authors [11]. (b) Customer Perspective: the goal
is to validate customer satisfaction, to analyze the number of cases closed in the month,
to analyze the incidence of service level agreements (SLAs), in addition to the per-
centage of effectiveness per client, validating the efﬁciency of the process.
3.1
Description of the Proposed Model
The model has been based on an earlier study [11], adapting this methodology to the
modern demands for mobile devices, although some authors have proposed solutions,
have not considered the state of acquisition of evidence in their solutions, nor phase of
return of evidence. Once raised the model; it goes through several tests in order to be
properly validated. The proposed model is composed of ﬁve stages and twelve phases
as we can see in Fig. 1, where the different stages and phases are shown.
Identiﬁcation and Preparation Stage: It consists of the ﬁrst three phases, the ﬁrst
phase: assignment of the case, second phase: identiﬁcation of roles and functions, and
the third phase: recognition of the organization and those involved. At this stage the
32
E. Gomez-Torres et al.

request signed by the parties concerned with the appropriate authorizations to carry out
the forensic analysis of the devices is assigned, assigning roles and functions to the
personnel who will form the group of investigators. Besides, information is collected
on the processes of the organization that was affected, as well as on personnel related to
the incident. Finally, all the electronic components provided for research are identiﬁed
and documented.
Preservation and Acquisition Stage: It is composed of fourth phase: identiﬁcation
and documentation of electrical components, ﬁfth phase: deﬁnition of hardware and
software, sixth phase: evidence assurance phase. Next, the hardware and the appli-
cations that are used for the forensic analysis are deﬁned, taking into account that the
devices provided for this process should not be manipulated and thus maintain the
admissibility of the evidence. With the objective of reproducing the scene of the event,
digital copies of the evidence must be generated according to NIST [12].
Analysis Stage: At this juncture, we have the seventh phase: which consist of evi-
dence generation, eighth phase: search of digital evidence, ninth phase: analysis of
digital evidence. Next, the search, localization, and analysis of the digital evidence
obtained in the previous stage are carried out; completing an act of delivery of the MD5
code generated from the digital evidence. In cryptography, MD5 (Message-Digest
Stage of 
Preservation and 
Adquistion
Evidence Delivery
Stage
Stage of Presentation
Stage of Analysisis
Stage of 
Identification and 
Preparation
1.-Case Assignment
Tool
2.- Role and Function
Identification Phase
3.- Recognition of
the Company and
Stakeholders Phase
4.- Identification and
of
Documentation 
Electrical components
5. -Definition of
Hardware and
software
7. - Evidence 
Generation Phase
6.-Evidence 
Assurance phase
8. – Search of 
Digital Evidence
. 
9 – Analysis of 
Digital Evidence
10. – Preparation 
of the report
11. – Results of 
Information
12. – Return of 
Evidence
Fig. 1. Phases and stages of the proposal methodology
Towards a Forensic Analysis of Mobile Devices Using Android
33

Algorithm 5, Message 5 Summary Algorithm) is a widely used 128-bit cryptographic
reduction algorithm. One of its uses is to verify that some ﬁle has not been modiﬁed.
Emphasis should be placed on the preservation, integrity, and admissibility of digital
evidence. However, MD5 was deprecated by RIPEMD-160: it is a Hash algorithm
designed to replace MD4 and MD5. It produced a 20-byte Hash (160 bits, hence its
name) and was released by its designers.
Stage of Presentation: It includes tenth phase: preparation of the report and eleventh
phase: results of information. Here a report is generated with the localized ﬁndings;
additionally, it indicates possible suggestions of closure and explanation of the impact
of the sources of information analyzed.
Evidence Delivery Stage: At this juncture, we have twelfth phase: the return of
evidence. Following this, the delivery of each one of the electrical components,
manuals, reports and all the documentation facilitates to carry out the analysis of the
raised incident. The implementation of the proposed methodological model will sup-
port the obtaining of ﬁndings, the same will be used as digital evidence in a legal
process, allowing reconstructing scenes, and criminal acts, exploring and analyzing
digital information. Always we should be taking into consideration the integrity and
conservation of the data and the processing of this kind of information.
The forensic analysis uses a forensic tool determined by a decision matrix shown in
Table 1, which helps to select appropriately so that a special selection is made, which
avoids subjectivities. Such a forensic tool will contribute to detect, locate evidence of
computer attacks, theft of information, conversations or emails, chats, browsing his-
tory, videos, the location of places visited. It should be taken into account that the
digital evidence to be handled is extremely fragile. Since only entering individual ﬁles
will change the last access date of the same. So it is important to follow appropriate
guidelines or an approximate model to avoid making mistakes or, in their absence,
minimize them and thus ensure that the admissibility of the same is guaranteed in a
judicial process.
Table 1. DAR Template
Dar Template V:1.0
Problem statement
Choose an application that allows for forensic analysis
Selection of evaluation technique:
Factors Considered According to DAR guidelines
Evaluation
We have three software applications, and we have to decide which
one use
Solution selected
We choose the one that has best points
Justiﬁcation
Choose the best tool for forensic process
Stakeholder
Forensic category
Approval reference
Forensic investigator There will be a team meeting to decide about
Forensic expert
34
E. Gomez-Torres et al.

The DAR Template (Decision Analysis and Resolution) matrix [13], which is
presented in Table 1. It has the same one that is used in our study for deciding about
software forensics appliance. It has the following characteristics: (a) problem statement,
(b) factors considered, (c) technique selected, (d) justiﬁcation, (e) evaluation, (f) solu-
tion selected, Justiﬁcation and stakeholders that provide extensive documentation for
the control and monitoring of the forensics. The purpose of (DAR), (CMMI-DEV) is to
analyze possible decisions using a formal evaluation process that evaluates identiﬁed
alternatives against established criteria [11].
4
Evaluation of Results
To decide the best option Grid Analysis has been used to establish the best choice in
choosing forensic software, described in Table 2: For calculating the Total, we mul-
tiply each point-evaluation-factor by the weight and sum each value. The different
weights included in Table 3, had been deﬁned using the scale 1 to 9. First of all, if the
application is for mobile devices, 8 points, if it has an Online manual, it has the
maximum weight because of the importance that it has for developers to work with the
application in an appropriate way, and the support that the manufacturer company has
concerning the software.
4.1
Forensic Analysis Development
Stages one and two are obviated because they are documentary issues and we move to
stage three. For development we use AFLogical OSE [14], which allows the examiner
to extract data of great importance from an Android device; the data that can be
obtained are: call logs, phone numbers of contacts, messages (MMS, SMS, MMSParts.
Table 2. Grid Analysis
Grid Analysis
Criteria
Total
I
II III IV V VI VII VIII
Alternatives A 5 2
8
4
5
6
8
9
B 2 3
3
3
3
3
3
3
133
C 1 1
1
1
3
3
2
2
88
D 3 3
3
3
3
3
2
2
122
Indicators for Alternatives, Table 2: A = Weight,
B = Oxygen Forensic Suite, C = Autopsy Forensic,
D = encase Forensic.
Indicators for Criteria: I = Parametrizable, II = For
Windows, III = For Mobile devices, IV = Multi
Platform, V = Open source, VI = Online upgrade,
VII = Support, VIII = Online Manual
Towards a Forensic Analysis of Mobile Devices Using Android
35

The forensic process continues as follows: the command adb devices, veriﬁes if the
system recognizes the device, otherwise it proceeds to install drivers according to the
device to investigate. The code used for that purpose is as follows:
Then the information is extracted with the command aﬂogical-ose, as we can see
that the tool starts the extraction of the data, the target folder is /root/AFLogical Ose
and the ﬁles have the CVS format. With this information, we proceeded to investigate
the extracted data that shows us information obtained with AFlogical Ose.
The information obtained by the software in summary is:
• Contacts Phones.csv - Stored contacts.
• Call Log Calls.csv - Calls made-received, date and time duration.
• MMS.csv - Multimedia message.
• SMS.csv - Text messages.
Now we can browse any images as well as the extracted data (such as contacts, call
logs, MMS/SMS, and device info) in CSV format.
Most valuable information is usually stored in SQLite databases as we show in
Table 3. Which shows the different types of evidence and the route; it can be located in
the Android operating system.
Once we recover the information using the software, we can decide between
starting the analysis of data that give us the text messages, the log of the calls, or
WhatsApp sent or received, etc., as seen in Fig. 2 that shows an example of the
information we get from the sms.csv ﬁle.
Table 3. Information retrieved from the phone
Evidence
Name of ﬁle
Phone book
\data\data\com.android.providers.contacts\databases\contacts2.db
SMS, MMS
messages
\data\data\com.android.providers.telephony\databases\mmssms.db
Calendar
\data\com.android.providers.calendar\databases\calendar.db
Log
\data\com.sec.android.provider.logsprovider\databases\logs.db
User’s data
\data\system\users\accounts. DB
Web-browser history
\data\data\com.android.browser\databases\browser2.db
Dictionary
\data\user\comc.android.providers.userdictionary\databases
\user_dict.db
36
E. Gomez-Torres et al.

5
Discussion
Among the most well-known tools are: open-source (Kali Linux Operating System,
Santoku Linux containing tools for forensic analysis and digital audit) commercial use
such as Oxygen Forensic Kit [15], Andriller DS7 [16], Encase [17]. In Computer
Forensics Tool Catalog [18], there is a detailed list of tools available for Forensic
Analysis on mobile devices. According to Lonsdale, [8], the steps to perform the
analysis vary depending on the range of the apparatus; this is because low-end phones
are merely limited to saving basic user information, such as: (a) Date and time of calls
made; received, and missed, (b) Date and time of received; sent, and draft text mes-
sages, (c) Recording of deleted calls, (d) Telephone contacts, (e) Log of Wap and Web
pages visited, (f) Ringtones, (g) Images and photos.
For our case, the expected results of the implementation of the forensic analysis
model
for
Android
Operating
System
is
done
using
an
Integral
Scorecard
(CMI) evaluating indicators or metrics, which we will describe in the inventory of
indicators, focused from the perspective of the Process and Customer. To validate the
effectiveness of the use of the methodological model; the signs will be monitored
month-by-month using an RGY board (Red, Green, Yellow) [6, 7].
6
Conclusions
From the analysis and application of our proposed model, we can conclude that. The
forensic analysis model for mobile devices with proposed Android Operating System
covers several stages of research. The identiﬁcation and preparation stage, preservation
and acquisition stage, stage of analysis, stage of presentation and stage of delivery of
evidence. All together support the search and analysis of digital evidence. We have
considered that as Android is an operating system of open source, in this sense, there
are vulnerabilities regarding information security.
One factor to taking into account is the fact that data safety in the company does not
reach a level of maturity that guarantees and conﬁdence to the clients on the protection
of the information, which today constitutes one of the most signiﬁcant assets in the
organizations. Another aspect to consider is: the technological advance and the increase
Fig. 2. Information we get from SMS.csv ﬁle.
Towards a Forensic Analysis of Mobile Devices Using Android
37

of electronic transactions, makes the users are more exposed to be victims of the
cybercrime and social engineering, in this sense it is necessary to continue carrying out
research in the area of computer forensics oriented to mobile devices.
It should also be considered that Forensic analysis model for mobile devices with
proposed Android Operating System is applicable for Android mobile devices, how-
ever, because of the general characteristics of the model could be extended to other
platforms such as iOS, BlackBerry, Symbian.
The strength of the proposed methodology is that: the forensic model for mobile
devices with proposed Android Operating System complies with guidelines of the
chain of custody throughout the research process, based on the scientiﬁc and technical
point of view. Also it should be considered that: mobile devices continue to increase
their capabilities with new versions, so new challenges will continue to arise for
forensic analysis; therefore, contributions and continuous research are required.
Future Research cloud be developed, in a case of requiring its application in another
platform like iOS, BlackBerry, Symbian, the tools and software to be used must be
evaluated by using decision matrix that help to select adequately in a technical way free
of subjectivities.
References
1. Yıldırım, N., Varol, A.: Android based mobile application development for web login
authentication using ﬁngerprint recognition feature. In: 2015 23th Signal Processing and
Communications Applications Conference (SIU). https://doi.org/10.1109/siu.2015.7130436
2. Venkateswara Rao, V., Chakravarthy, A.S.N.: Forensic analysis of Android mobile devices.
In: 2016 International Conference on Recent Advances and Innovations in Engineering
(ICRAIE), 23–25 December 2016. https://doi.org/10.1109/icraie.2016.7939540
3. Roger, M., Seigfried, K.: The future of computer forensics: a needs analysis survey. Cent.
Educ. Res. Inf. Assur. Secur. 23(1), 12–16 (2004)
4. Höbarth, S.: A framework for on-device privilege escalation exploit. In: de IWSSI2011, San
Francisco (2011). https://www.wibas.com/cmmi/decision-analysis-and-resolution-dar-cmmi-
dev
5. El Conﬁdencial: https://goo.gl/WjSVoF. Accessed 17 Mar 2017
6. Ting, W.: Applying the balanced score card in the team strategic performance management.
In: 2011 2nd International Conference on Artiﬁcial Intelligence, Management Science and
Electronic Commerce (AIMSEC), 8–10 August 2011. https://doi.org/10.1109/aimsec.2011.
6010534
7. OWASP: https://goo.gl/OS4fdM. Accessed 01 Feb 2017
8. Zander, T.: https://www.tobiaszander.de/. Accessed 08 Aug 2017
9. Vidas, et al.: Toward a general collection methodology for Android devices. In: The
Proceedings of the Eleventh Annual DFRWS Conference, vol. 8, pp. S14–S24, Carnegie
Mellon ECE/CyLab, USA
10. Stiparo, P., Kounelis, I.: The mobileak project: forensics methodology for mobile application
privacy assessment. In: International Conference for Internet Technology and Secured
Transactions. IEEE
11. Gómez, E, Herrera, N, Moscoso O, Guaman, P.: Propuesta de Análisis Forense para
Dispostivos Móviles con Sistema Operativo Android. https://goo.gl/MSQ1Ua
38
E. Gomez-Torres et al.

12. NIST: National Institute of Standards and Technology, 3 April 2014. http://www.nist.gov/.
Accessed 6 Apr 2014
13. The Process Group: Decision Analysis & Resolution, 18 February 2014. http://www.
processgroup.com/
14. Santoku: Santoku. Obtenido de, 17 de 03 de 2017. https://santoku-linux.com/
15. Oxigen Forensic: Oxigen Forensic Suite. http://www.oxygen-forensic.com/es/
16. Andriller Tool: https://www.andriller.com/
17. Guidances Software: Encase Forensic.https://www.guidancesoftware.com/encase-forensic?
cmpid=nav_r
18. Li, B., Wang, Y.: Study on the enterprise strategic budget management mode based on
balance scored card. In: 2010 International Conference on E-Business and E-Government
(ICEE), 7–9 May 2010. https://doi.org/10.1109/icee.2010.707
Towards a Forensic Analysis of Mobile Devices Using Android
39

Process-Based Project Management
for Implementation of an ERP System
at a Brazilian Teaching Institution
Ada Guagliardi Faria(&), Saulo Barbará de Oliveira,
and Fábio Carlos Macêdo
Universidade Federal Rural do Rio de Janeiro, Rodovia BR 465,
Km 7 – Seropédica, Rio de Janeiro 23890-000, Brazil
ada.faria@gmail.com, saulobarbara@gmail.com,
macedo.fabio@gmail.com
Abstract. One of the main challenges faced in implementing ERP systems is
how to efﬁciently manage a project of this size. This article presents the case of
the Instituto Federal de Educação, Ciência e Tecnologia do Rio de Janeiro – IFRJ,
which established a process-based project management solution to implement the
ERP system, showing the preliminary results achieved and advantages expected
with the conclusion of the program. The PMBOK method was chosen to
implement the ERP, while concepts of business process management (BPM) are
being used to support the activities carried out in the project. The adjustment and
alignment of the methods have enabled the team to implement the system con-
sidering its complexity, interconnections and the particularities of the institute.
The beneﬁts attained from the modules implemented so far are improved agility
of services, greater assertiveness in decision making e better performance, all
achieved through the adoption of integrated project management.
Keywords: ERP  Project management  Process management
1
Introduction
The Instituto Federal de Educação, Ciência e Tecnologia do Rio de Janeiro – IFRJ
(Federal Institute of Education, Science and Technology of Rio de Janeiro) is a public
teaching institution with undergraduate and graduate programs in a wide range of
technical disciplines. The Institute is formed by 16 units (the chancellery and 15 cam-
puses), and has 2,000 employees, among administrative and technical staff and teachers.
With a structure of this size, managing IFRJ requires robust systems and technology.
Before, the various administrative and academic areas faced varied problems in per-
forming their activities, making it common to ﬁnd inconsistencies in the execution of
activities still carried out manually. Information was often duplicated in systems that
were not integrated, requiring the entry of the same data in more than one application.
This duplication generated higher expenses for information technology (IT) infrastruc-
ture for storage, maintenance, backup and greater efforts to update the databases.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_5

The Institute’s main challenge was to unify its systems, through a solution able to
integrate the management of academic and administrative aspects. The solution found
was to acquire an Enterprise Resource Planning (ERP) system to administer the various
functional areas and integrate its systems and databases to enhance agility and save
money. The implementation of an ERP system is a complex project that needs to be
well planned and executed. Therefore, a project was created for implementation of the
ERP system. This study covers the project management solution that combined with
process management, created controls and connections between activities and facili-
tates communication between stakeholders to achieve efﬁcient project management and
achieve the expected results. Will be presented the strategies used, the results already
obtained and the beneﬁts expected with conclusion of the project. The chancellery, the
Institute’s central administrative ofﬁce, is the focus of the study and the source of
information that enabled a more comprehensive investigation of the organizational
context involved, including analysis of the Institute’s administrative factors and the
governmental inﬂuence, which led to deﬁnition of the form of planning, managing and
implementing the project.
The data presented in this study were collected in the unit analyzed by examination
of documents, participative observation and conduction of interviews with the people
involved. The work was supported by ﬁve members of the project team, who partic-
ipated in the period of planning, analysis and deﬁnition of execution, control and
monitoring of the activities of implementing the ERP system. The data were gathered
from January to July 2017.
2
Basic Characteristics of an ERP System
The expression enterprise resource planning involves systems that integrate a large
number of activities, through different modules [1]. ERP systems have the purpose of
integrating the essential functions of an organization [2]. The software employed
creates connectivity among the various sectors of the organization and provides
operational beneﬁts, by streamlining processes, including by storing data in a single
base. This makes information more reliable, enables the implementation of electronic
governance practices. In short, ERP systems work to integrate administrative processes
and support decision making and innovation, guided by better management of data [3].
After analyzing several systems available in the market, the SIG (Integrated
Management System), developed by Federal University of Rio Grande do Norte, was
chosen. This system was selected for meeting the full range of institutional needs in the
administrative and academic areas. Besides meeting these needs, it is based on free and
open code, covers all the school levels of IFRJ and has clear documentation for
possible corrective maintenance and evolution. The SIG has six subsystems. IFRJ
contracted the six systems, opting for the modules that meet their needs, meaning that
not all modules of these systems have been contracted.
Process-Based Project Management for Implementation of an ERP System
41

3
Planning and Implementation of the System
The SIG product, developed by Universidade Federal do Rio Grande do Norte – UFRN
(Federal University of Rio Grande do Norte) was contracted by IFRJ through a
company licensed by UFRN to implement the system in teaching institutions, through a
competitive bidding process. After the contract was signed, the project to implement
the SIG began in the second half of 2016. Because the system is based on open code
software, the implementation is accompanied by a process of technology transfer to the
information technology (IT) area and to the administrative areas of IFRJ, which will
continue the maintenance and evolution of the system after the contractual period.
The IT area is also responsible for providing the system’s infrastructure, monitoring the
technical activities and evaluating the project’s deliverables.
The project’s contract management process is based on Normative Instruction
4/2014, issued by the Brazilian government to guide the management of public pro-
curement. Management of the new system’s implementation is delicate and requires
careful attention, so as to satisfy the applicable legislation, the school calendar and the
customizations previously identiﬁed through analysis by the Institute’s user areas. For
efﬁcient integrated management of a project of this size, three types of vision are
necessary: implementation management vision; software management vision; and
contract management vision. For that purpose, initiatives based on project management
and process management were adopted by all the groups involved in implementing and
customizing the ERP system.
Establishing project management practices that are efﬁcient and effective is a
challenging task for organizations [4], this being the case of IFRJ. The Institute is using
the project management body of knowledge (PMBOK) method for project manage-
ment, while to plan the implementation of the new ERP system, it established a
program named “Integrare”. Deﬁnition of this program is based on the PMI PMBOK
Guide [5] and refers to a group of related projects, managed in coordination to obtain
strategic beneﬁts and controls that would not be available if they were managed
individually. The Integrare program is scheduled to be concluded by the end of 2019.
Of the six subsystems of the SIG, SIGAdmin has already been totally implemented,
SIGRH and SIGAA are being implemented, with ﬁve modules already delivered, and
implementation of SIPAC, SIGPP and SIGED is still pending (Fig. 1).
Fig. 1. Structure of the program – adapted
42
A. G. Faria et al.

The aim of dividing the program into projects was to designate a person to be in
charge of each project, with attribution of functions and competencies, to provide the
necessary autonomy and agility. Smaller projects are less prone to suffer failures
compared to larger projects [6]. Dividing the program into three smaller projects
allowed dividing responsibilities and distributing tasks. Thus, each project has a leader
responsible for its execution (a project manager). This structure and organization also
serves to satisfy federal legislation on the activities of the public administration.
The management of the Implementation Project is based on the document entitled
Project Management Methodology, part of the federal government’s System for
Informatization and Simpliﬁcation of Processes (SISP) [7]. This methodology is
composed of a set of good project management practices for federal entities under the
SISP and is based on the PMBOK, a globally known benchmark in project manage-
ment [7]. The Implementation Project involves all phases of conception of the SIG,
covering the deliverables in each step and stipulating how to administer them, using
formal documents, such as the Project Opening Instrument, Acceptance Instrument and
Project Analytic Structure Instrument. This project involves direct interaction with all
the user areas that will validate the delivery of the system’s modules.
The objective of the Software Project is to manage the development and delivery of
the necessary customizations of the SIG as well as the IT infrastructure to be developed
to conduct tests, ratify and implement the system and prepare the IT area to administer
the system after the implementation phase. The execution of the project is based on the
Software Process (PSW) of the SISP. The IT team of IFRJ, based on the SISP’s PSW
methodology, has responsibility for testing the modules delivered, verifying the quality
of the code developed by the contractor (service provider), validating the functionalities
and analyzing functional points. The IT team also is charged with absorbing the
knowledge about the system so as to provide support to each module after the initial
assisted operation period, when the contractor’s responsibility will end.
Finally, the Contract Project relies on the rules of Normative Instruction 4/2014,
involving description of the processes, activities and artifacts of the Model for Con-
tracting IT Solutions, with the objective of supporting the process of managing the
contracting of IT solutions [8] and the subsequent interaction with the contractor
chosen in the tender process. This project entails requesting the speciﬁc services from
the contractor, through the issuance of descriptive service orders, followed by oversight
of the services rendered and payment for the services. The Project Management
Information System (PMIS) called Redmine is utilized in this project. It is a freeware
for project management that has contributed to the success achieved so far, by sup-
porting planning and enabling control of the activities developed during the project [9].
The processes and activities of each project were modeled so as to assure that a
smooth sequential ﬂow is followed by all the people involved in the activities. The
actors responsible for the activities were deﬁned and the inputs and outputs necessary
for each process were established, along with the form of interaction of the processes of
the different projects. According to Oliveira [10], “modeling is the vision of the
company by means of constructing functional diagrams about the behavior of its
processes – mapping of processes”. The processes were designed following the
Business Process Management (BPM) method, which enables identifying, mapping
and recording the information about processes [10].
Process-Based Project Management for Implementation of an ERP System
43

4
The Beneﬁts of Structuring the Process-Based Program
Meeting deadlines, assertiveness in carrying out plans and monitoring their progress are
very important for the success of a project [9]. The creation of a program composed of
the mentioned projects enabled better control of the activities carried out in each of the
projects and the points of interaction between them. To contribute to the good man-
agement of the projects, the working team opted to detail each of the projects in
processes. A process is a set of ordered actions that are integrated to produce a speciﬁc
product or service [10]. Process management was chosen as the strategy of the Inte-
grare program and is being used to support the management of the overall effort. The
management of processes is an integrated organizational capability that encompasses a
set of routines and practices for mutual support [11]. Management of processes guides
the organization to be focused on proper development of processes [10]. It is important
for organizations to have well-deﬁned processes and to follow these processes in
managing projects [12]. Thinking based on processes should be utilized naturally in the
project management environment [13]. The adoption of process management should
promote transparency, agility and rationality in managing projects, and consequently
increase the quality and productivity of the management of public resources [14].
The objective of adopting processes for managing the Integrare program was to
make all the people involved aware of the activities deﬁned for them. This broad but
detailed knowledge of the project by specifying its activities conveys a clear vision of
the tasks of each person and those of others and of the overall ﬂow in managing the
project. The design of processes shows the necessary inputs, the outputs generated in
each process and the actors responsible for each activity. The ﬂow to be followed it
previously arranged between the parties.
Better knowledge of the activities and complete ﬂow of the work creates an
opportunity to streamline the workﬂows and also facilitates communication among the
people involved during the course of the project. They know who is responsible, what
each person needs to start his or her activities and what outputs these activities produce.
In possession of this information, the managers of each project and the team know on
whom to rely in case of the need for a decision on prioritization, corrections or changes
during the projects, etc. The project team deﬁned and created this structure for pro-
cesses to help the management of the current project, which can evolve as the program
is executed, with the possibility of using successful actions of this structure in future
projects. The process is seen as a mechanism to support learning within and between
projects [13].
Figure 2 shows the Business Processes Diagram integration among the projects.
The process expresses the routines carried out in the project and through it is
possible to obtain the learning necessary to improve management, enabling the
application of knowledge current and future projects. To create a speciﬁc place to store
the documents of the project for easy access by people interested in consulting them,
the processes deﬁned were published at the IRFJ website where they can be consulted
and used to orientation about the procedures carried out by each of the actors involved
in the project’s tasks.
44
A. G. Faria et al.

5
Results
This section presents results already reached and expected when the project is ﬁnished.
5.1
Results Already Reached
With the adoption of the project management tool Redmine, 100% of the services
rendered in the project that involve the contracted ﬁrm are allocated by using the tool.
This provides the system’s managers with a broad view of the services performed. This
has also made it possible to extract reports from the tool, facilitating monitoring and
control of the activities planned and concluded in each of the three projects. The use of
the project management tool also contributed to resource and time savings. There are
few activities in the contract project that require the printing of documents and sig-
natures, since its management is done through software, using the login of those
responsible. This represented an 80% savings in the printing of control documents
required to execute the contracts and an increase in service agility of 30%.
Fig. 2. Integration of the processes of each project.
Process-Based Project Management for Implementation of an ERP System
45

Good management of the communication with stakeholders is a fundamental factor
for the success of projects in general [15]. This communication with interested parties
was speciﬁed in the planning of the projects. Project managers can extract from
Redmine 100% service order status and use this information for the communication and
decision making of those responsible. Accordingly, the project managers must share
the decision making and create a culture of participation of stakeholders [16]. A project
committee was formed, composed of project managers and representatives of the user
areas. This was only possible after the initial experiences in conducting the project,
where feedback from the participants indicated the need for this committee. Now all the
decisions that involve alterations in the planning are discussed with these participants.
New customizations not previously contemplated must be approved by the committee.
These activities increase the involvement and responsibility of people.
The ability to understand, anticipate and ﬁnd ways to overcome the complexity of a
project is a consequence of its success [6]. A key to success is management that
delivers good results in organizational projects [17]. The division of a highly complex
project into smaller parts is contributing to its effective management. The success of a
project also depends on the perception of the parties involved [18]. In the case here, the
user areas participate directly in validation of the deliverables of the modules making
up the systems and are perceiving the improvements that the SIG has brought.
Members of the project teams who previously often found difﬁculties during planning
and execution now have opportunities to improve the way the project is executed. By
identifying the gains, the team members perceive the beneﬁts of the project manage-
ment methods and processes employed and spread this experience to other work fronts.
This type of attitude has started to alter the workplace climate positively. The efforts
that are generating the best results are already being replicated in other areas, but this is
still incipient and needs to be solidiﬁed, being needed initiatives at the strategic level so
that an improvement in climate can be consolidated.
Projects conducted by IFRJ to contract IT solutions were already common in the
past, but had never been implemented with the process management tool used in the
Integrare program. The activities in the present case have been performed better than in
past projects. This better performance can be explained by the fact that the design of the
processes is serving to support integrated management of the project, the mapped
process have facilitated the resolution of the problems that have arisen so far, reducing
the time required for adjustments in service orders delivered by the contractor by up to
30% of the time normally practiced in IFRJ IT projects. A project model based on
well-deﬁned activities is allowing a high degree of assertiveness in the management of
the Integrare program. It must be considered that the success of the project can be
facilitated by the proﬁle of the three project managers, since all already had knowledge
in project management and process management.
5.2
General Results Expected
Projects managed using the Redmine tool can serve as the base for planning new
initiatives, by creating organizational learning where previous experiences and lessons
supply inputs for new projects. Previous experiences are utilized to support the making
of future decisions [16]. The creation of ongoing improvement of the management of
46
A. G. Faria et al.

IFRJ’s projects will be based on the information obtained during previous programs
and projects. Maturity and better ability to manage projects will be developed as the
organization integrates knowledge and learning [6]. A project of this size, lasting three
years and involving all the academic and administrative areas, will leave a set of
lessons learned that can be used in forthcoming projects. Experience during the project
life cycle should be used to properly promote the collection of learning through the
lessons learned [19]. All learning is being recorded as “Lessons Learned” that are part
of the project documentation.
The public sector in Brazil needs to advance with the adoption of project man-
agement, for which purpose it needs to develop skills and competencies, such as better
knowledge acquisition, as well as new attitudes [4]. The processes serve as a base for
learning within the project and this knowledge must be improved during the project and
carried over to future projects. The parties involved need to be able, through these
processes, to create a climate of continuous learning and convey this learning, now
improved, to the projects that will be undertaken in the future. The expectation is that
the team leading the project, the other participants and representatives of the user areas
will perceive the beneﬁts that the Integrare program is generating, understand that these
gains have only been possible by adopting process and project management techniques
and that these can lead to better overall management of the Institute.
In the ﬁnal analysis, everyone’s desire is for the project to succeed with respect to
its scope, time frame and cost as well as its deliverables. It is important to consider how
managers plan projects so that project results and performance can be improved [20].
The success of the deliverables of a project involves the effectiveness of the products or
services delivered by the project not only during its execution, but afterward [21]. In
the case of the Integrare program, the system when fully implemented will hopefully
meet the Institute’s demands in the next several years, without the need for new
investments to acquire software or major maintenance of the systems for the areas the
SIG will serve.
6
Conclusion
The efforts of the managers and other staff members of IFRJ to use methods that are in
accordance with the public administration demonstrates their commitment to follow the
standards established for federal public entities, improving public administration and
thus delivering better services to society. The use of project and process management
methods is allowing IFRJ to standardize the implementation of the Integrare program,
enabling keeping its working processes aligned and preparing the necessary docu-
mentation for its management and control. It is also helping to obtain more ambitious
results, increasing the chances of success and retention of knowledge.
The actions developed in the project create institutional value, from the expanded
knowledge base, greater agility in making course corrections and better organizational
intelligence, adding value to the work being done and facilitating the analyses for
application of techniques for ongoing improvement, in search of organizational
excellence. The project had some points of ease to be planned and implemented, since
all three managers involved already had previous knowledge in project management
Process-Based Project Management for Implementation of an ERP System
47

and process management. There is a needed for investment in project management
training and processes to prevent future projects, which are developed by people who
do not have prior knowledge in project and processes management, have low perfor-
mance and unrealized results. We believe that the experience acquired and the lessons
learned during implementation of the present project and that are being properly
documented can and will be used in new projects by IFRJ and other teaching and
research institutions at the three levels of government in Brazil (federal, state and
municipal). Good knowledge of the past will prevent repetition of mistakes.
For future studies, consideration will be given to the need for development of IFRJ
staff in project and process management. As a suggestion, the results of projects based
on the good practices of PMBOK and BPM can be compared with those that did not
have this base.
References
1. Silva, L.C.S., Oliveira, B.S.: Planning and scope deﬁnition to implement ERP: the case study
of Federal Rural University of Rio de Janeiro (UFRRJ). Procedia Comput. Sci. 64, 196–203
(2015)
2. Sudhaman, P., Thangavel, C.: Efﬁciency analysis of ERP projects-software quality
perspective. Int. J. Proj. Manag. 33, 961–970 (2015)
3. Ram, J., Wu, M.L., Tagg, R.: Competitive advantage from ERP projects: examining the role
of key implementation drivers. Int. J. Proj. Manage. 32, 663–675 (2014)
4. Santos, V., Varajão, J.: PMO as a key ingredient of public sector project’ success – position
paper. Procedia Comput. Sci. 64, 1190–1199 (2015)
5. PMI – Project Management Institute. PMBOK (Project Management Body of Knowledge)
Guide. PMI (2013)
6. Alami, A.: Why do information technology projects fail? Procedia Comput. Sci. 100, 62–71
(2016)
7. MPOG - Ministério do Planejamento, Orçamento e Gestão. Metodologia de Gerenciamento
de Projetos do SISP. MGP-SISP. 2011. Version 1.0. http://www.sisp.gov.br. Accessed 21
Feb 2017
8. MPOG - Ministério do Planejamento, Orçamento e Gestão. Guia de Boas Práticas em
Contratação de Soluções de Tecnologia da Informação. 2014. Version 2.0. https://www.cti.
ufu.br. Accessed 3 Mar 2017
9. Teixeira, L., Xambre, A.R., Figueiredo, J., Alvelos, H.: Analysis and design of a project
management information system: practical case in consulting company. Procedia Comput.
Sci. 100, 171–178 (2016)
10. Oliveira, S.B.: Gestão por processos: fundamentos, técnicas e modelos de implementação:
foco no sistema de gestão de qualidade com base na ISO 9000: 2000, 2nd edn. Qualitymark,
Rio de Janeiro (2008)
11. Ng, S.C.H., Rungtusanatham, J.M., Zhao, X., Lee, T.S.: Examining process management via
the lens of explotation and exploration: Reconceptualization and scale development. Int.
J. Prod. Econ. 163, 1–15 (2015)
12. Lewis, J.: Mastering Project Management: Applying Advanced Concepts to Systems
Thinking, Control & Evaluation, Resource Allocation, 2nd edn. McGraw-Hill, New York
(2007)
48
A. G. Faria et al.

13. Chronéer, D., Backlund, F.: A holistic view on learning in project-based organizations. Proj.
Manage. J. 46, 61–74 (2015)
14. Daher, E.P., Oliveira, S.B., Silva, L.C.P., Freitas, M.S.A.M.: Improvement of processes in
the service contract payment system of Brazil’s National Cancer Institute. Procedia Comput.
Sci. 100, 693–700 (2016)
15. Gasik, S.: Are public projects different than projects in other sectors? Preliminary results of
empirical research. Procedia Comput. Sci. 100, 399–406 (2016)
16. Cunha, J.A., Moura, H.P., Vasconcellos, F.J.S.: Decision-making in software project
management. Procedia Comput. Sci. 100, 947–954 (2016)
17. Gomes, J., Romão, M.: Maturity, beneﬁts and project management shaping project success.
In: Rocha, A., Correia, A., Costanzo, S., Reis, L. (eds.) New Contributions in Information
Systems and Technologies. Advances in Intelligent Systems and Computing, vol. 353,
pp. 435–448. Springer, Cham (2015)
18. Gomes, J., Romão, M.: Improving project success: a case study using beneﬁts and project
management. Procedia Comput. Sci. 100, 489–497 (2016)
19. Amaral, A., Madalena, Araújo M., Rodrigues, C.S.: Organizational learning and knowledge
management—insights from industrial managers. In: Schlick, C., Trzcieliński, S. (eds.)
Advances in Ergonomics of Manufacturing: Managing the Enterprise of the Future.
Advances in Intelligent Systems and Computing, vol. 490, pp. 403–415. Springer, Cham
(2016)
20. Rocha L., Tereso A., Couto J.P.: Project management: evaluation of the problems in the
portuguese construction industry. In: Rocha A., Correia A., Costanzo S., Reis L. (eds) New
Contributions in Information Systems and Technologies. Advances in Intelligent Systems
and Computing, vol. 353, pp. 69–78. Springer, Cham (2015)
21. Varajão, J.: Success management as a PM knowledge area – work-in-progress. Procedia
Comput. Sci. 100, 1095–1102 (2016)
Process-Based Project Management for Implementation of an ERP System
49

A Conceptual Framework for the Implantation
of Enterprise Applications in Small
and Medium Enterprises (SMEs)
Irving Reascos1,2(&) and João Alvaro Carvalho2
1 Universidad Técnica del Norte, Ibarra, Ecuador
imreascos@utn.edu.ec
2 Universidade do Minho, Braga, Portugal
Abstract. This research addresses the implantation of ready-to-use enterprise
applications in SMEs. The objective is to develop a description of the implan-
tation process, identifying the actors involved and establishing the factors that
affect the success of the whole process and of some of its components. The
inceptive phase of the research is mainly based on a literature review. The
Technology-Organization-Environment (TOE) framework was used to classify
the inﬂuence factors already from the literature. The emerging process model
combines contributions from several authors and it encompasses three phases,
later sub-divided in fourteen stages: Pre-Implantation, Implantation and
Post-Implantation. The work described in this article corresponds to a research
phase that aims at understanding the implantation process and its major prob-
lems, in SMEs. The following stage will lead to the development of solutions
that will address those problems. The article ends with a general view of the
work to be done in future.
Keywords: SMEs  Implantation of enterprise applications
1
Introduction
In the early days of commercial computing, when one enterprise wanted to use com-
puters to support its operational or managerial activities, there was no alternative to
start a tailor-made software development project. Producing the software product itself
was then the key problem in the process of reaching computer support to enterprise
activities. The importance of having the software being appropriately used by the
enterprise’s personnel and achieving the expected beneﬁts from its use was obfuscated
by the challenge of identifying the right business and organizational requirements and
building a computer application that adequately ﬁt those requirements.
Nowadays the situation is quite different. For most business situations, enterprises
can rely on ready to use software products – enterprise applications. The key problem
now is how to implant the enterprise application in the enterprise so that it is used as
quickly as possible and the expected beneﬁts are realized (as evidenced by companies
already using the application).
The implantation of enterprise applications has its own difﬁculties and demands
speciﬁc competences to be accomplished successfully. The phenomenon attracted
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_6

attention from researchers who addressed aspects such as its course of action or the
factors that condition its success. Most of the existing studies, however, focus on large
companies, that can afford expending signiﬁcant resources in the process and can
secure the necessary IT capabilities. The implantation of enterprise applications in
small and medium enterprises (SME) has been studied to a lesser extent.
It can be argued that, due to the reduced dimension of SME, the complexity of the
implantation of enterprise applications in these type of enterprises is minor, and,
therefore, potential problems in the implantation process are of little signiﬁcance. Such
position neglects the fact that SME account for most of the enterprises in any economic
sector. Furthermore, SME lacks the IT capabilities [1–6] crucial to the successful
conduct of implantation processes, even those an at a small scale.
Implanting enterprise applications is not a minor issue. It is about bringing change
to a stable setting, redeﬁning work and social structures and altering existing power
balances. Even in SME, whose reduced dimension and complexity might lead to the
idea of a simple implantation process, but in practice the risk of failure is high. The
impact of such failures in the economy can be considerable if we take into consider-
ation the number of SME, including micro-enterprises.
The purpose of the research described in this article is to develop knowledge to be
used in enterprise applications implantation processes in SME. This knowledge must
be suitable to a wide range of situations faced by enterprises with little IT capabilities
when getting involved in processes that implicate the selection of commercially
available enterprise applications, its acquisition, installation and exploitation.
The research encompasses two phases. The ﬁrst phase aims at a better under-
standing of the phenomenon – implantation of commercial available enterprise appli-
cations in SME. Besides including an extensive review of the literature, this phase will
also include empirical studies to complement the understanding achieved through the
literature review. The second phase aims producing design knowledge of relevance for
those involved in the implantation of enterprise applications in SME.
This article is organized as follows: starts with the background on SMEs in Europe,
ready to use enterprise applications, players involved in the implantation of SMEs and
problems and factors that inﬂuence the implantation of enterprises applications in
SMEs; continued with the research plan, preliminary results are presented and ﬁnalized
with a brief discussion and proposal of work to be carried out in the future.
2
Background
2.1
SME
Micro, small and medium-sized enterprises (SMEs) play a signiﬁcant role in the European
economy. They are a major source of entrepreneurial skills, innovation and employment.
Figure 1 uses a “deformed” pyramid for presents a distribution of enterprises by size,
according to the criteria deﬁned by the European Union that involves the number of
employees and turnover annual [7]. This Figure depicts the relative proportion of enter-
prises according to their size, contributing to emphasize the importance of helping micro
and SME to take care with the way the carry out the implantationof enterprise applications
A Conceptual Framework for the Implantation of Enterprise Applications in SMEs
51

2.2
Ready to Use Enterprise Applications in SME
Along the evolution of IT usage in enterprises, there was a dramatic change from
tailor-made to ready to use software products. Nowadays there is a wide offer of ready
to use enterprise applications of IT addressing most enterprise areas. Examples include
products such as Point-of-Sales (POS), Enterprise Resource Planning (ERP), Customer
Relationship Management (CRM), Supply Chain Management (SCM), Content Man-
agement (CM), Document Management (DM). There aren’t standard names for these
products as they can appear in many conﬁgurations, providing a vast variety of
arrangements of functionality. Nevertheless, those designations are widely used in the
IT market. ERP products are a case particularly interesting as they assure basic func-
tions in an enterprise. They process business transactions, support the general ledger of
an enterprise, hold a great deal of the enterprise’s business records and execute a wide
range of operational and managerial actions upon business information.
Those products can be obtained through several diverse ways – e.g., purchase,
renting,
licensing
–
and
run
on
in-house
servers
or
on
hosting
services.
Software-as-a-Service is becoming an increasingly common way of enterprises to get
access to commercially available enterprise applications. These trend towards
Software-as-a-Service only reinforces the movement towards the use of ready to use
enterprise applications – being Commercial Off-The-Shelf (COTS) products and Ready
to Use Software Product (RUSP) two designations found in the literature for these type
of software packages.
SMEs are a natural client of ready to use enterprise applications. Several reasons
may be present to explain this preference. First, the cost. It is no surprise that the cost of
an enterprise application that addresses the essential needs of enterprises is lower than
the cost of a tailor-made enterprise application. Furthermore, ready to use enterprise
applications embrace good practices of the industry, thus providing to any enterprise
up-to-date support to generic business functions. This might be true even for speciﬁc
industrial areas, as long as enterprises do not look for competitive advantages in the
information processing domain.
Fig. 1. SMEs in Europe, based in [7]
52
I. Reascos and J. A. Carvalho

2.3
Players Involved in the Implantation of Enterprise Applications
in SMEs
The implantation of enterprise applications in SMEs typically involves more than the
two obvious players: the software-house (software developer) that produces the
enterprise application and the company that buys it. Other players include resellers of
the software product and facilitators who help the company and the software developer
to implant the applications. Figure 2 depicts these players and their relationships.
– Software developers, the entities that develop the enterprise applications; often, the
enterprise applications aren’t sold directly but through resellers;
– Software resellers, the companies that sell the enterprise applications; these
companies represent software developers, but they can sell products from different
software developers, even from competing ones; in many situations involving SME,
besides selling the enterprise applications, the Resellers gives support to its
implantations providing capabilities related to IT and managerial aspects;
– Consultants providing services that involve IT and/or managerial capabilities; the
consultants can act hired by the buying SME, the selling entity or both;
– The SME that buys the enterprise application and that goes through the implan-
tation process;
Fig. 2. Actors involved in the implantation the enterprise applications in SME
A Conceptual Framework for the Implantation of Enterprise Applications in SMEs
53

– Associations of SMEs can also play important roles in the commercialization of
enterprise applications; considering the small dimension of the buying enterprises,
their typical deﬁcit in IT and managerial capabilities and the cost of the enterprise
applications, associations of SME can develop efforts to ﬁnd solutions that suit their
members and negotiate prices and conditions for a set of enterprises that will be
more favorable than for a situation involving just one buying enterprise. associa-
tions of SME can also negotiate advantageous conditions for the involvement of
consultants; this is particularly interesting in cases where the SME are from one
same economic sector.
2.4
Factors that Inﬂuence the Implantation of Enterprises Applications
in SMEs
The successful implantation of enterprise applications is of great importance in SMEs
and isn’t free from difﬁculties [8–11]. As other processes that involve technological
innovation, or at least, the adoption of new technologies, the implantation of enterprise
applications can be described using the Technology-Organization-Environment
(TOE) framework [12]. This framework arranges the factors that inﬂuence the
implantation of enterprise applications in 3 classes: Technological, Organizational
and Environmental. Focusing on the difﬁculties that can affect the implantation
process, these factors may include:
Technological factors -the heterogeneous and incompatible infrastructure [3, 12]; the
few capability’s and technological competences of SMEs [1, 3, 9, 10, 2, 13–17]; the
complexity of the these systems [13, 18]; its ﬁt and customization in the enterprise and,
poor data quality and security [11, 19, 20].
Organizational factors - poor leadership [11, 21]; low strategic planning [4, 9]; direct
and indirect costs are poorly estimated [19, 22]; errors in the initial stages scale to the
following [1, 23, 24]; a deﬁciency in the structure of the organization and informal
processes [3, 4, 13, 17, 25–29]; lack of required resources (knowledge, skills, ﬁnance,
management, time) [9, 12, 21, 22, 29, 29–35]; low levels of management (projects,
change, risk) [15, 22]; complexity of the implantation process [17, 19, 23]; neglect of
social aspects such as user resistance [1, 11]; lack formal communication [20, 23, 29];
inadequate training and preparation of users [11, 15, 19, 20, 23, 29, 33].
Environment factors - changing government regulations [14, 2, 35, 36]; constants
market pressures [11, 12, 14, 37], difﬁculty of accessing enterprise applications and
consultants that ﬁt the enterprise proﬁle [20, 29].
Figure 3 presents a summary of the factors that inﬂuence the implantation of
enterprises applications in SMEs that result from a meta-analysis of articles reporting
empirical studies that focus on the implantation of enterprise applications in SME.
Factors have been arranged in the TOE framework.
54
I. Reascos and J. A. Carvalho

2.5
Frameworks for the Implantation of Enterprise Applications
in SMEs
Several frameworks, or models, exist in the literature that contribute to a broad
understanding of the implantation of ready-to-use enterprise applications in SMEs.
These frameworks were classiﬁed in six categories as shown in Table 1. A limitation in
these frameworks is that none of them provides a comprehensive view of the imple-
mentation process and its issues. Each framework covers speciﬁc issues of the
implantation. Some focus on support to decision-making in pre-implantation and
software selection; others are TOE-based frameworks, describing the factors that
should be considered for a successful implantation; some attempt to holistically
embrace the problem, but they turn out to be too general; yet others, address speciﬁc
aspects, such as the implantation of enterprise applications in cloud computing envi-
ronments. In any case, these frameworks will serve as a basis for the research to be
carried out.
Fig. 3. Inﬂuence factors for implantation of enterprise applications
Table 1. Frameworks for the implantation of enterprise applications in SMEs
Frameworks
References
For decision support
[38, 39]
For the pre-implantation phase [1, 40, 41]
For selection
[42]
TOE-based
[43, 44, 45, 37, 18, 46, 47]
For cloud adoption
[35]
Holistic
[48, 49, 20, 19, 50, 51, 52]
A Conceptual Framework for the Implantation of Enterprise Applications in SMEs
55

3
Research Plan
The research will be conducted using the paradigm Design Science Research
(DSR) from two perspectives, inﬂuenced by the recommendations of Gregor and
Hevner [53]: Knowledge by Understanding (Descriptive) and Knowledge as a Propose
(Prescriptive).
Figure 4 presents the proposed research plan. First, a review of the literature on the
proposed topic is carried out to detect the existing gaps. In a second stage, different
methods and research techniques will be used to understand the problem in depth
(Knowledge to understand) and ﬁnally will propose a framework or set of solutions for
the process of implantation of enterprise applications in SMEs (Knowledge as a
purpose).
At the stage of understanding the problem, a case study has been prepared for the
purpose of exploring the implantation process of an enterprise application, the prob-
lems that arise and the decisions made to overcome those problems in a medium-sized
company. Later, interviews will be conducted with stakeholders involved in the
implantation process of enterprise applications in SMEs, including the different players
Fig. 4. Research plan based in [53]
56
I. Reascos and J. A. Carvalho

mentioned in Sect. 2.3: owners or CEO of software development companies, resellers,
consultants related to the company providing software, independent consultants,
owners or CEO of SMEs. Finally, if found necessary, we will conduct a survey with
experts to uphold the process of implanting enterprise applications in SMEs.
Once the problem is understood and characterized, it will be possible to move to the
next stage - propose and evaluate solutions - thus involving knowledge as a purpose
(Prescriptive). This will lead to the development of guidelines for the implantation of
enterprise applications in SMEs. At a higher level of abstraction, those guidelines will
include three main phases: pre-implantation, implantation, and post-implantation. Each
phase should consider a set of stages according to the size and complexity of the SME,
thus helping the players involved in the process of implantation of enterprise
applications
4
Preliminary Findings
So far, we have focused on the stage of understanding the problem. The case study has
already been carried out. Now, we are working on the interviews to selected players.
This involves interviewing, transcribing and analyzing the resulting data (cf. Fig. 2).
Results so far enable the production of the ﬁrst version of a process model for the
implantation of enterprise applications in SMEs (Fig. 5).
This process has three phases and fourteen stages. In the pre-implantation phase,
which is largely done by the enterprise, six stages are identiﬁed: vision, which is what
is expected to be achieved with the enterprise application; deﬁnition of processes to
Fig. 5. Process model for the implantation of enterprise applications in SMEs
A Conceptual Framework for the Implantation of Enterprise Applications in SMEs
57

automate; identifying the main requirements that the application should have; selection
of the enterprise application of several possible alternatives; negotiate with the supplier
the cost, time of implantation, support, etc., and decide its implantation.
The implementation phase that is carried out by the application provider with the
support of the contracting company has seven phases: Supplier knows the company,
the recognition of the main activities of the company; customize the application, adjust
to the needs of the company; parameterization of the application; Migration of data,
data are debugged prior to migration or entry; the application is tested; the company’s
staff is trained and the application is ﬁnally put into production (Go-live).
The ﬁnal phase is post-implantation, and so far, the support stage has been iden-
tiﬁed, which may consist of supporting the company in the use of the application and in
later stages increase functionality.
It is understood that this process can have variants, depending on the size of the
company and on other contingency factors. As a preliminary result, it is likely that this
process model evolves in order to accommodate insights to be gained from other forms
of inquiry.
5
Discussion and Future Work
A process model for the implantation of enterprise applications in SMEs has been
elaborated
covering
three
main
phases:
pre-implantation,
implantation
and
post-implantation. The model provides an overview of the main aspects that a SME
should consider prior to the implantation of an enterprise application. The model can
also be useful for the companies that develop software, resellers and consultants, as the
model identify the stages that a SME goes through before requesting their services.
The model is now being reﬁned in order to provide a comprehensive characteri-
zation of the stages and activities in each phase, and the contingency aspect that will
support the conﬁguration of this general process model to speciﬁc situations.
Upcoming stages of the research process will take into consideration this process
model. However, it is expected that the insights to be obtained will enable the process
model to evolve either to provide more detail of its components or to adapt to per-
spectives that weren’t covered yet by the literature review or by the ﬁeld study.
Acknowledgements. This work has been partially supported by: (i) Secretaría Nacional de
Educación Superior, Ciencia, Tecnología e Innovación (SENESCYT) and Universidad Técnica del
Norte – Ecuador; (ii) COMPETE: POCI-01-0145-FEDER-007043 and FCT - Fundação para a
Ciência e Tecnologia within the Project Scope: UID/CEC/00319/2013; (iii) SmartEGOV: Har-
nessing EGOV for Smart Governance (Foundations, methods, Tools)/NORTE-01-0145-
FEDER-000037, supported by Norte Portugal Regional Operational Programme (NORTE
2020), under the PORTUGAL 2020 Partnership Agreement, through the European Regional
Development Fund (EFDR).
Note: For the graphics, we have used the icons of website - https://icons8.com/.
58
I. Reascos and J. A. Carvalho

References
1. Hustad,
E.,
Olsen,
D.H.:
Exploring
the
ERP
pre-implementation
process
in
a
small-and-medium-sized enterprise: a case study of a Norwegian retail company. In: 19th
European Conference on Information Systems, ECIS 2011 (2011)
2. Seethamraju, R.: Adoption of Software as a Service (SaaS) Enterprise Resource Planning
(ERP) Systems in Small and Medium Sized Enterprises (SMEs). Inf. Syst. Front. 17(3), 475–
492 (2015)
3. Douglas, A., Wainwright, D., Greenwood, D.: The dynamics of IT supplier relationships
with construction SMEs: a technological frames approach. In: UK Academy for Information
Systems Conference Proceedings, March 2010
4. Reicher, R., Komáromi, N., Szeghegyi, Á.: The possible success factors of introduction of
CRM system at hungarian SMEs. Acta Polytech. Hung. 12(8), 215–229 (2015)
5. Zeng, Y.-R., Wang, L., Xu, X.-H.: An integrated model to select an ERP system for Chinese
small- and medium-sized enterprise under uncertainty. Technol. Econ. Dev. Econ. 23(1), 38–
58 (2017)
6. Gonzalez, M.J., Martín, E., Buiza, G., Hidalgo, M., Beltrán, J.: Implementation of an
operations management system in eight Spanish SMEs. In: Proceedings of 2015
International
Conference
on
Industrial
Engineering
and
Systems
Management,
IEEE IESM 2015, pp. 1296–1302 (2015)
7. European Union: User Guide to the SME Deﬁnition, p. 2015. Publications Ofﬁce of the
European Union, Luxembourg (2015)
8. Čelar, S., Mudnić, E., Gotovac, S.: Interrelation between ERP modiﬁcation and modiﬁcation
scheduling: four SME case studies in Croatia. Strojniski Vestnik J. Mech. Eng. 57(1), 27–30
(2011)
9. Nguyen, T.H., Newby, M., Macaulay, M.J.: Information technology adoption in small
business: conﬁrmation of a proposed framework. J. Small Bus. Manag. 53(1), 207 (2015)
10. Nguyen, T., Newby, M., Waring, T.: Understanding Customer Relationship Management
(CRM) technology adoption in SMEs: an empirical study in the USA. In: UK Academy for
Information Systems Conference Proceedings, March 2012
11. Shaul, L., Tauber, D.: CSFs along ERP life-cycle in SMEs: a ﬁeld study. Ind. Manag. Data
Syst. 112(3), 360–384 (2012)
12. Serrano, A., Chen, H., Serrano, A.: Investigating factors affecting integration technologies
adoption in organizations. In: AMCIS 2010 Proceedings, August 2010
13. Ahmadi, S., Yeh, C.H., Martin, R.: Strategic framework for achieving readiness in
organisations to implement an ERP system. In: 19th Americas Conference on Information
Systems, AMCIS 2013 - Hyperconnected World: Anything, Anywhere, Anytime, vol. 4,
pp. 3095–3103 (2013)
14. Alshawi, S., Missi, F., Irani, Z.: Organisational, technical and data quality factors in CRM
adoption - SMEs perspective. Ind. Mark. Manag. 40(3), 376–383 (2011)
15. Deltour, F.: ERP project in SMEs: a matter of risks, a matter of competencies: a quantitative
analysis. In: ECIS 2012 Proceedings, May 2012
16. Shahawai, S., Idrus, R.: Malaysian SMEs perspective on factors affecting ERP system
adoption. In: 2011 Fifth Asia Modelling Symposium (AMS), pp. 109–113 (2011)
17. Winkelmann, A., Klose, K.: Experiences while selecting, adapting and implementing ERP
systems in SMEs: a case study. In: AMCIS 2008 Proceedings, January 2008
18. Ramdani, B., Chevers, D., Williams, D.A.: SMEs’ adoption of enterprise applications: a
technology-organisation-environment model. J. Small Bus. Enterp. Dev. 20(4), 735–753
(2013)
A Conceptual Framework for the Implantation of Enterprise Applications in SMEs
59

19. Jha, R., Hoda, M.N., Saini, A.K.: Implementing best practices in ERP for small and medium
enterprises. In: IEEE Symposium on Advanced Management of Information for Globalized
Enterprises, AMIGE 2008, pp. 1–5 (2008)
20. Sahran, S., Goni, F.A., Mukhtar, M.: ERP implementation challenges in small and medium
enterprise: a framework and case study. Adv. Mater. Res. 139–141, 1636–1639 (2010)
21. Sumner, M., Bradley, J.: CSF’s for implementing ERP within SME’s. In: AMCIS 2009
Proceedings, January 2009
22. Seethamraju, R.: Enterprise system’s characterisitics in small and medium-sized enterprises
context - a case study. In: Proceedings of the European and Mediterranean Conference on
Information Systems, EMCIS 2008 (2008)
23. Hustad, E., Olsen, D.H.: Critical issues across the ERP life cycle in small-and-medium- sized
enterprises: experiences from a multiple case study. Procedia Technol. 9, 179–188 (2013)
24. Seethamraju, R., Seethamraju, J.: Adoption of ERPs in a medium-sized enterprise - a case
study. In: ACIS 2008 Proceedings, January 2008
25. Boumediene, R., Kawalek, P.: Predicting SMEs willingness to adopt ERP, CRM, SCM &
e-procurement systems. In: ECIS 2008 Proceedings, January 2008
26. Čelar, S., Mudnić, E., Gotovac, S.: Interrelation between ERP modiﬁcation and modiﬁcation
scheduling: four SME case studies in Croatia. In: Povezava Med Spremembami Termin.
Sprememb Posl. Inf. Sist. Štiri Študije Prim. V Malih Sredn. Podjetjih Na Hrvaškem, vol. 57,
no. 1, pp. 27–30, January 2011
27. Christoﬁ, M., Nunes, M., Peng, G.: Identifying and improving deﬁcient business processes
to prepare SMEs for ERP implementation. In: UK Academy for Information Systems
Conference Proceedings 2009, March 2009
28. Leyh, C.: Which factors inﬂuence ERP implementation projects in small and medium-sized
enterprises? In: AMCIS 2014 Proceedings, June 2014
29. Sia, C.: Impact of organisational resources on implementaion of ERP by an SME ﬁrm: an
exploratory study. In: PACIS 2008 Proceedings, July 2008
30. Caldeira, M.M., Ward, J.M.: Understanding the successful adoption and use of IS/IT in
SMEs: an explanation from Portuguese manufacturing industries. Inf. Syst. J. 12(2), 121–
152 (2002)
31. Chen, H., Lee, M.W., Wilson, N.: Resource constraints related to emerging integration
technologies adoption: the case of small and medium sized enterprises. In: Association for
Information Systems - 13th Americas Conference on Information Systems, AMCIS 2007:
Reaching New Heights, vol. 7, pp. 4662–4675 (2007)
32. Buonanno, G., Faverio, P., Pigni, F., Ravarini, A., Sciuto, D., Tagliavini, M.: Factors
affecting ERP system adoption: a comparative analysis between SMEs and large companies.
J. Enterp. Inf. Manag. 18(4), 384–426 (2005)
33. Ghobakhloo, M., Hong, T.S., Sabouri, M.S., Zulkiﬂi, N.: Strategies for successful
information technology adoption in small and medium-sized enterprises. Information 3(1),
36–67 (2012)
34. Johansson, B., Laurinavičius, R., Venckauskaite, A.: ERP system procurement in SMEs –
two contrasting ways. In: AMCIS 2013 Proceedings, May 2013
35. Salim, S.: Cloud ERP adoption-a process view approach. In: PACIS 2013 Proceedings, June
2013
36. Li, M., Yu, Y., Zhao, L., Li, X.: Drivers for strategic choice of cloud computing as online
service in SMEs. In: ICIS 2012 Proceedings, December 2012
37. Shahawai, S.S., Idrus, R.: Pre-considered factors affecting ERP system adoption in
Malaysian SMEs. In: UKSim2010 - UKSim 12th International Conference on Computer
Modelling and Simulation, pp. 323–328 (2010)
60
I. Reascos and J. A. Carvalho

38. Blackwell, P., Shehab, E.M., Kay, J.M.: An effective decision-support framework for
implementing enterprise information systems within SMEs. Int. J. Prod. Res. 44(17), 3533–
3552 (2006)
39. Xie, Y., Allen, C.J., Ali, M.: An integrated decision support system for ERP implementation
in small and medium sized enterprises. J. Enterp. Inf. Manag. 27(4), 358–384 (2014)
40. Hidayanto, A.N., Hasibuan, M.A., Handayani, P.W., Sucahyo, Y.G.: Framework for
measuring ERP implementation readiness in small and medium enterprise (SME): a case
study in software developer company. J. Comput. Finl. 8(7), 1777–1782 (2013)
41. Jebreen, I., Wellington, R., MacDonell, S.G.: Packaged software implementation require-
ments engineering by small software enterprises. In: 2013 20th Asia-Paciﬁc Software
Engineering Conference (APSEC), vol. 1, pp. 50–57 (2013)
42. Ganapathy, N., Raju, J.: A framework for enterprise resource planning system selection by
small and medium enterprises. In: 2nd European Conference on Information Management
and Evaluation, ECIME 2008, pp. 193–202 (2008)
43. Olupot, C., Kituyi, G.M.: A framework for the adoption of electronic customer relationship
management information systems in developing countries. Electron. J. Inf. Syst. Dev. Ctries.
58(3), 1–19 (2013)
44. Awa, H.O., Ojiabo, O.U.: A model of adoption determinants of ERP within T-O-E
framework. Inf. Technol. People 29(4), 901–930 (2016)
45. Saedi, A.: Cloud computing adoption framework: innovation translation approach. In: 2016
3rd International Conference on Computer and Information Sciences (ICCOINS), pp. 153–
157 (2016)
46. Saini, S., Nigam, S., Misra, S.C.: Success factors for implementing ERP in SMEs in India: a
conceptual model. In: 2010 2nd IEEE International Conference on Information Management
and Engineering, pp. 165–169 (2010)
47. Sophonthummapharn, K.: The adoption of techno-relationship innovations: a framework for
electronic customer relationship management. Mark. Intell. Plan. 27(3), 380–412 (2009)
48. Xu, L.X.X., Yu, W.F., Lim, R., Hock, L.E.: A methodology for successful implementation
of ERP in smaller companies. In: Proceedings of 2010 IEEE International Conference on
Service Operations and Logistics, and Informatics, pp. 380–385 (2010)
49. Fu, K.E.: Development of a generic procedure model for the enterprise resource planning
implementation in small and medium enterprises. Proc. SICE Annu. Conf. 2010, 3523–3528
(2010)
50. Vilpola, I., Kouri, I., Vaananen-Vainio-Mattila, K.: Rescuing small and medium-sized
enterprises from inefﬁcient information systems–a multi-disciplinary method for ERP system
requirements engineering. In: 40th Annual Hawaii International Conference on System
Sciences, HICSS 2007, p. 242b (2007)
51. Wu, W.-H., Ho, C.-F., Fu, H.-P., Chang, T.-H.: SMEs implementing an industry speciﬁc
ERP model using a case study approach. J. Chin. Inst. Ind. Eng. 23(5), 423–434 (2006)
52. Lockett, N., Brown, D., Sissons, A.: Soft systems methodology in IT project management:
implementing CRM in SMEs. In: AMCIS 2006 Proceeding, December 2006
53. Gregor, S., Hevner, A.R.: Positioning and presenting design science research for maximum
impact. MIS Q. Manag. Inf. Syst. 37(2), 337–355 (2013)
A Conceptual Framework for the Implantation of Enterprise Applications in SMEs
61

Model for Selecting Software Development
Methodology
Lizeth Chandi1,2(&), Catarina Silva1, Tatiana Gualotuña2,
and Danilo Martinez2
1 School of Technology and Management,
Polytechnic Institute of Leiria, Leiria, Portugal
2152219@my.ipleiria.pt, catarina@ipleiria.pt
2 Departamento de Ciencias de La Computación, Universidad de Las Fuerzas
Armadas ESPE, Sangolquí, Ecuador
{tmgualotunia,mdmartinez}@espe.edu.ec
Abstract. Nowadays there are various software (SW) development method-
ologies. However, it may be complex to select a speciﬁc methodology, espe-
cially if project managers or leaders do not have sufﬁcient experience, or if the
project to be carried out has characteristics that they have not previously worked
with. In this paper, we consider a set of variables and criteria to propose a
model, in order to select a particular SW development methodology. It is
important to recognize that, while many factors may inﬂuence the selection of a
SW development methodology, there are key points to consider, such as
available resources, project requirements, among others. The result of the pre-
sent study is a model that guides decision making for the selection and adoption
of a SW development methodology.
Keywords: Software development methodologies  Model
1
Introduction
The SW development methodologies are diverse and software projects are becoming
ubiquitous in terms of application areas. Additionally, technologies are rapidly
evolving and project managers are faced with the necessity of rapidly deciding on SW
methodologies with little or no speciﬁc information. Furthermore, there is a lack of
scientiﬁc documentation that reﬂects development processes and their activities [1, 7].
The objective of this research is to analyse and propose a model that guides
decision making in adopting a SW development methodology. The proposed model is
based on matrix multiplication, intensive operation of fundamental calculation used in
many algorithms in scientiﬁc computation [2].
Operations with matrices are used in different areas, speciﬁcally in computing they
are widely applied in coding, digital signal processing, image processing, graphics, and
robotics, various algorithms and control models, among others. The matrices and the
operations that can be performed with them are in fact the central engine of several
algorithms [2].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_7

The model we propose in this paper starts by deﬁning a set of criteria (character-
istics of SW development methodologies) quantiﬁed per methodology, constituting the
ﬁrst matrix. The second matrix is the result of the evaluation of several criteria related
to the requirements/needs of the speciﬁc project to be carried out. Hence, it is advisable
to carry out this evaluation with the project leader or project manager to get full
knowledge about the project. Finally, the multiplication of the two previously obtained
matrices is carried out. Once these values are obtained, the total vertical sum of the
values (per column) is executed. The totals obtained from the sum for each column,
allow visualizing the methodologies that are better accommodated to what the project
requires, with the highest score being mainly recommended. Subsequently, the deﬁ-
nition of criteria that allow the quantitative qualiﬁcation by SW development
methodology is carried out.
This study also contributes to better understanding the SW development method-
ologies selected, since it requires a previous analysis of them, in order to synthesize
their important and relevant characteristics, and then quantify them.
2
Related Work
In this section, we present the studies that have approached the use of models to select a
SW development methodology.
Verma et al. [3] proposed a rule based expert system as a base for software
engineers in the selection of the best software development methodology for a project.
The proposed systems is block-based. The ﬁrst block represents the software devel-
opment methodologies. The next block represents selection criteria, ﬁnal score,
dynamic priority allocation and results. The result block present blocks with check
boxes and the priorities can be assigned between 0 and 10. After making the entire
selection, a “Decide” button is pressed and the result is displayed in the result text box
with the selected model. The “View Heuristic Report” button gives the entire report
with situational analysis. Their future work will include more software development
methodologies and more selection parameters.
Mahapatra and Goswami, identiﬁed in [4] different project characteristics that
inﬂuence the decision of selecting the most appropriate software development
methodology for a speciﬁc project, based on the literature review of different software
development methodologies (i.e. waterfall, prototype, iterative, spiral, RAD, XP). The
selection of the project characteristics is based on requirement analysis, status of
development team, users’ participation, project type and associated risk.
In [5], an analysis of factors that inﬂuence the decision of choosing the most
suitable software development methodology for a project is presented. Three popular
software development methodologies are considered: RUP, XP, and RAD. The ﬁnd-
ings of the analysis provide information regarding which methodology is best to be
used depending on the level of each factor for a speciﬁc project.
In the following, we present a model proposed, which makes use of matrices, the
product between them and speciﬁc qualiﬁcations of a project to be evaluated given by
the representative of the organization that will develop the SW, to guide which soft-
ware development methodology is best suitable to a speciﬁc project.
Model for Selecting Software Development Methodology
63

3
Methodology
We propose a model with the objective of selecting a SW development methodology
that meets the requirements of the SW project to be developed. We start by introducing
the deﬁnition criteria: development methodologies and selection criteria.
3.1
Deﬁnition Criteria
In this section, the analysis of the criteria to be considered for the selection of a
development methodology is carried out, allowing to make the best use of the available
resources for the realization of a SW project [5].
i. Development Methodologies
The selection of the methodologies to be evaluated was carried out under the results
obtained in the case of study carried out in “Mobile application development process:
A practical experience” [1], in which the following methodologies were used in aca-
demic and industry contexts: RUP, RAD, SCRUM, and XP.
ii. Selection Criteria
Once the methodologies to be evaluated are deﬁned, the following criteria are
considered:
(a) Budget: the available budget with which a project for its development has a direct
inﬂuence on the methodology to be selected, since those large-scale projects
require a greater number of resources. The ideal of this criterion of selection is that
it does not demand excessive expenses.
(b) Delivery time: ideally, every project should be carried out and delivered as soon
as possible; however, there are activities inherent to the methodologies, such as
the delivery of formal documentation, the holding of periodic meetings, among
others, which may extend or shorten times. This is why the delivery time of the
project is decisive as to the methodology to be selected. Agile methodologies, as
its name suggests, are known to have cycles or fast phases, so they are considered
ideal when carrying out a project that presents a limited time.
(c) Documentation: Formal documentation can be generated in both agile and tradi-
tional methodologies; however, for the latter, the rigor or extension of documents
surpasses those presented in agile methodologies. Although documentation may be
omitted for certain projects, according to their development team and the condi-
tions agreed with the client, others cannot leave aside having documented supports.
(d) Human resource: In addition to the number of people who make up a development
team, it is important to consider the role of each one of the members. Furthermore,
within a project, according to the methodology, not only the presence of the
development team but also the customer and even end users may be necessary.
Being recommendable the presence of all the parts for all phases of project
activity. Therefore, it is advisable to identify the availability of personnel that is
required in the short and long term, how important it is and in this way to be able
to select which methodology allows covering these particularities.
64
L. Chandi et al.

(e) Project dimension: Identifying the dimension of a project immediately may not be
a simple task, however, it is vital to know its dimension, so that it can align
several criteria, such as resources that are required both human and time, to its
time the budget, among others; and thus opt for the methodology that corresponds
to the identiﬁed. In this criterion, to a greater dimension of the project, greater
need of resources, reason why the “ideal” are projects of medium or small
dimension.
(f) Adaptability: It is important to mention that in the course of the development of a
project, signiﬁcant changes may occur, and much depends on the project so that
the adaptability that a methodology has to those changes is considered important.
Although every project is susceptible to improvement, certain projects within its
development present constant changes, for which the ﬂexibility that the
methodology can provide is of paramount importance.
3.2
Development of Criteria for SW Development Methodologies
The methodologies selected have multiple characteristics which, when deployed under
different factors, may become advantages as well as disadvantages. Then, we present
the differences that stand out in each of the development methodologies according to
previously deﬁned criteria (Table 1, 2, 3, 4, 5, 6 and 7) [5].
Table 1. SW development methodologies and budget available
Criteria
SW development methodologies
RAD
AUP
SCRUM
XP
Budget
available
Requires a
disciplined
team in cost
management
Demand
low
costs
Does not demand
expenditure on
personnel and
resources for the
development of
projects
Does not demand
excessive expenditure
on personnel and
resources for the
development of
projects
Table 2. SW development methodologies and delivery time
Criteria
SW development methodologies
RAD
AUP
SCRUM
XP
Delivery
time
The delivery time,
although not
allowed to be
extensive, is
shortened as much
as possible
Being an agile
methodology, the
delivery time will
not be extensive,
however it is
established
without drastic
cuts of time
It stands out
for its drastic
reduction of
the times of
development
The delivery time
is shortened as
much as possible,
in order to
comply with one
of the
characteristics of
agile
methodologies
Model for Selecting Software Development Methodology
65

Table 3. SW development methodologies and time boxing
Criteria
SW development methodologies
RAD
AUP
SCRUM
XP
Time
boxing
Secondary
functions are
removed as
needed to meet
the calendar
Secondary
functions are
deleted only if
calendar
compliance is
required.
Secondary
functions are
removed as
needed to meet
the calendar
Secondary
functions are
deleted only if
calendar
compliance is
required
Table 4. SW development methodologies and adaptability
Criteria
SW development methodologies
RAD
AUP
SCRUM
XP
Adaptability,
response to
changes
Not every
application is
suitable for RAD.
The system must
be modular,
otherwise building
the components
for RAD will
bring difﬁculties
Accept that
requirements
change, even
in late stages
of
development
Flexibility
in response
to changes.
Great
adaptability
to change
It stands out for its
great adaptability
to changes. It
gives more value
to the response to
change than to
following a plan
Table 5. SW development methodologies and documentation
Criteria
SW development methodologies
RAD
AUP
SCRUM
XP
Required
documentation
Produces the
necessary
documentation
to facilitate the
future
development
and
maintenance
Everything is
described
concisely
using little
documentation
The little
documentation
that this
methodology
offers is the
result code of
the different
iterations
It lacks the
handling of a
formal
documenta-tion.
The scant
documentation
that this
methodology
presents is
usually source
code
66
L. Chandi et al.

In this section, each methodology was characterized according to previously
established criteria, and based on these qualitative characteristics, we will obtain the
quantitative ones that will give rise to the ﬁrst matrix that is required. Then, based on
the behaviour that each methodology presents against each criterion, will be carried out
respective quantiﬁcation.
3.3
Quantiﬁcation of Criteria by SW Development Methodology
In the present section, the obtained matrix allows to visualize the degree of compliance
of each methodology with respect to each criterion, the qualiﬁcation being values
Table 6. SW development methodologies and human resource
Criteria
SW development methodologies
RAD
AUP
SCRUM
XP
Required
human
resource
Small teams of
professionals, in
case of medium
projects. Large
projects will
require more HR.
Fundamental
involvement of
users/customers
Business
leaders and
developers
work together
on a daily basis
throughout the
project
Methodology
that less
personnel needs,
it has very few
roles and its size
grows depending
on the
programming
group that is
recommended to
be conformed by
5 to 10 people.
Scrum has the
client as part of
the team in the
different
deliveries of
Sprints
It has numerous
roles for the
control of the
processes in the
different
iterations and the
number of
people can
increase
depending on the
size of the group
of programmers,
even so, the total
number of
members is not
recommended
over 15 people
Table 7. SW development methodologies and project dimension
Criteria
SW development methodologies
RAD
AUP
SCRUM
XP
Project
dimension
They can be large
medium-sized projects,
however, when it comes
to large projects, they
require enough human
resources to create the
right number of teams
Mainly
projects of
medium
dimension or
small
Focused
mainly on
projects not
too
extensive
Oriented
mainly to
non-extensive
projects
Model for Selecting Software Development Methodology
67

between 1 and 5. The lower value (min. 1) represents a poor or undesired fulﬁlment of
certain criterion, while the highest value (maximum 5) constitutes the best or ideal
fulﬁlment of said criterion (Table 8).
3.4
Evaluation of the General Requirements of the Project
In this phase of the proposed model, it is fundamental the participation of the project
leader, who will be in charge of evaluating characteristics of the SW project to carry
out, even for which a SW development methodology is required, regarding the criteria
established. Before the questions that cover the criteria to be evaluated, we obtain
afﬁrmative and negative answers, assigning a value of one and zero respectively,
according to the needs that the project presents (Table 9).
3.5
Matrix Multiplication
Matrices are instruments of algebra that facilitate the ordering and handling of data, and
appear in situations speciﬁc to the Social, Economic and Biological Sciences [6]. In the
proposed model, once the necessary data are obtained from the two matrices, a product
of these matrices is made, between the matrices A and B, quantitative criteria and
evaluative criteria (numerical results of Tables 8 and 9), respectively.
Table 8. Quantitative criteria in relation to methodologies
#
Criteria
SW development
methodologies
RAD AUP SCRUM XP
1
Budget available
3
4
5
4
2
Project dimension
5
4
3
3
3
Delivery time
4
4
5
5
4
Time boxing
5
4
5
4
5
Required documentation
3
3
1
2
6
Required human resource:
6.1 Developers
3
3
1
2
6.2 Clients
4
3
4
3
7
Adaptability, response to changes 1
4
5
5
8
Iterative
5
5
5
5
68
L. Chandi et al.

3.6
Obtaining Results
Once the matrix multiplication described in the previous point is performed, the
resulting values are summed, column by column; obtaining in this way the score of
each software development methodology based on the criteria established by the
project leader or project leader. With the values obtained, sorted in ascending order, is
possible to visualize which methodology presents the highest value, being this one that
is better adapted to the project to be carried out and for which the methodology is
sought. In addition, if one could not opt for the best rated methodology; the following
values are available, which the higher the value the better the adaptation will present.
Table 9. Evaluative criteria: application case
#
Evaluative criteria
Yes (1)/No (0)
1
Is the budget for the project limited?
1
2
Can the dimension of the project be considered large?
1
3
Is it necessary to deliver the project in a short period, in relation to the
size of the project?
0
4
Are they willing to eliminate secondary SW functions in order to
comply with the project schedule?
1
5
Is it necessary for the project to generate robust documentation?
1
6
Is a robust and comprehensive development team required in the
project?
0
6
Is it necessary in the project to accompany the client in the process or
several stages of it?
1
7
Can the project to be developed can present signiﬁcant changes at any
stage or stage of the project?
1
8
Is it required to carry out iterations in the project development?
0
Model for Selecting Software Development Methodology
69

4
Implementation
Next, the proposed model is developed within a possible scenario. To begin the
development of the model, a project leader was contacted, responsible for carrying out
the development of a mobile application, which, when answering questions about his
project, allowed us to deﬁne the evaluative criteria of the software product, which
become the initial or basic requirements.
The project leader will develop a mobile application that allows online purchases
(e-commerce), with the following particularities. The project itself is extensive, has a
limited budget, and the delivery time of it is not limited. However, they would be
willing to leave behind the development of extra features or non-essential comple-
mentary to meet the times/schedule of the project. Robust documentation, the presence
of the client, and the adaptability of the methodology to be adopted in this project are
totally required and necessary. While, you do not need a robust and large development
team, nor is it critical with iterations in the process of developing the mobile app
(Table 9).
Once the evaluative data of the project are obtained, the product of matrices is
carried out, based on the quantitative criteria deﬁned by development methodology in
the proposed model and the evaluative criteria (Tables 8 and 9, matrices A and B
respectively). Finally, after the matrix product (matrix C) and the vertical summation of
the values by methodology, the resulting values are obtained (Table 10).
In the present scenario, we have as a result: SCRUM: 23, AUP: 22, RAD: 21 and
XP: 18. It is thus evident:
• It is a question of the development team and/or leader of the project, the selection of
a methodology or another, who should take into account that the higher the score
presents a methodology, the greater the coupling to the requirements of the project
will have. Therefore, the totals of the methodologies function as indicators of their
coupling with the evaluated project.
• The methodology that mostly covers the requirements of the described project
(application case) is SCRUM, and although this methodology may or may not be
adopted by the development team. An additional list of methodologies is given with
a respective order, allowing visualizing which are coupled of best to not so rec-
ommended way, being in this case punctual: AUP, RAD and XP.
70
L. Chandi et al.

• The strengths of the methodology recommended in the ﬁrst instance can be high-
lighted. Scrum allows a short delivery time, does not require excessive expenses,
and is achieved with an affordable budget for the project. Additionally, Scrum
performs time boxing, and adapts to changes at any point in the SW development
process, being ﬂexible in its entirety.
5
Discussion
In this section, we present the main ﬁnding of the study and highlight the limitations
that may threaten the validity of the study and examine the implications for research in
this ﬁeld.
The main ﬁnding in the present research is that although the proposed model may
be a reference for the selection of a software development methodology, it is worth
mentioning that there are other inﬂuential factors when selecting a methodology. Such
factors may be speciﬁc needs not included in the criteria presented in the proposed
model, given that technological projects possess qualities that are inherent to the
environment in which they are developed.
The main challenge of the study is the evaluation of a limited number of
methodologies, so that in the future adding methodologies and/or methods would allow
a favourable variety to be used for those who use this proposed model, obtaining a wide
range of recommendations.
Furthermore, a major challenge of the proposed model is to have a limited number
of evaluative criteria, as well as SW development methodologies. A greater number of
criteria are required and these are as speciﬁc as possible, so that the evaluation of the
methodologies is greater and complete. Likewise, increasing and improving the eval-
Table 10. QUANTITATIVE CRITERIA IN RELATION TO METHODOLOGIES
#
Criteria
SW DEVELOPMENT
METHODOLOGIES
RAD AUP SCRUM XP
1
Budget available
3
4
5
4
2
Project dimension
5
4
3
3
3
Delivery time
4
4
5
5
4
Time boxing
5
4
5
4
5
Required documentation
3
3
1
2
6
Required human resource:
6.1 Developers
3
3
1
2
6.2 Clients
4
3
4
3
7
Adaptability, response to changes 1
4
5
5
8
Iterative
5
5
5
5
TOTAL
21
22
23
18
Model for Selecting Software Development Methodology
71

uation criteria of the project to be developed, qualiﬁed by the project leader or project
manager, would help to give results that are even more complete.
Moreover, the ﬁnding mentioned above can be considered as a possible future
research topic, and better, if more methodologies and a greater number of evaluation
criteria are considered for them, so that the range of options is larger and therefore more
complete.
The ﬁrst ambitious implication is the formalization of the proposed model, making
it possible for the model to be used by those who wish to guide the methodology that is
appropriate for the development of their project based on the main and outstanding
characteristics of the project.
A second implication is the investigation of mobile applications characteristics, in
order to ﬁnd criteria that are related to this type of software with the development
methodologies. In order to guide in the selection of a methodology for the development
of mobile applications speciﬁcally, taking advantage of the features and functionalities
of mobile devices such as screen size, sensors, GPS, cameras, among others.
6
Conclusions
Since there are many development software methodologies, one of the challenges faced
by software developers is to decide which methodology to apply in each speciﬁc
software project. In our present work, we present an approach to help with this issue by
comparing four different software development methodologies based on project
speciﬁc characteristics. The methodologies selected were the result of a previous
research case study, in which the units of analysis, both in industrial and academic
contexts, used them as base for implementation.
It is not possible to determine a particular methodology as better than all others for
all purposes. A methodology of software development will be advisable and better
adapted to a project according to the characteristics of the project. Selecting the
methodology depends on different project characteristics. Therefore, although the
model may guide the selection of a methodology, an expert, considering the factors that
the model does not contemplate and still may have importance in the organization or in
the project to be developed, should take the ﬁnal decision.
References
1. Chandi, L., Silva, C., Martínez, D., Gualotuña, T.: Mobile application development process :
a practical experience (2017)
2. Mishra, A., Yadav, H., Rani, S., Saxena, S.: A review of different methods for matrix
multiplication based on FPGA. Int. J. VLSI and Embed. Syst. 5, 747–751 (2014)
3. Verma, J., Bansal, S., Pandey, H.: Develop framework for selecting best software
development methodology. Int. J. Sci. Eng. Res. 5(4), 1067–1070 (2014)
4. Mahapatra, H., Goswami, B.: Selection of software development methodology (SDM): a
comparative approach. Int. J. Adv. Res. Comput. Sci. Softw. Eng. 5(3), 4 (2015)
72
L. Chandi et al.

5. Ben-Zahia, M.A., Jaluta, I.: Criteria for selecting software development models. In: Global
Summit on Computer and Information Technology, pp. 1–6 (2014)
6. Dy, C.: Matrix Algebra and Applications, pp. 173–256
7. Vasconcelos, J.B., Kimble, C., Carreteiro, P., Rocha, Á.: The application of knowledge
management to software evolution. Int. J. Inf. Manage. 37(1), 1499–1506 (2017)
Model for Selecting Software Development Methodology
73

Information and Knowledge
Management

Smart Cities Semantics and Data Models
Antonio J. Jara1(&), Martin Serrano2, Andrea Gómez4,
David Fernández4, Germán Molina3, Yann Bocchi1,
and Ramon Alcarria4
1 Mobility Lab, University of Applied Sciences Western Switzerland (HES-SO),
Technopole. 3, 3960 Sierre, Switzerland
{antonio.jara,yann.bocchi}@hevs.ch
2 Insight Research Centre, National University of Ireland Galway (NUIG),
Galway, Ireland
martin.serrano@insight-centre.com
3 HOP Ubiquitous S.L. (HOPU), Luis Buñuel. 6, Ceutí, 30562 Murcia, Spain
german@hopu.eu
4 Escuela Técnica Superior de Ingeniería en Telefomunicaciones,
Universidad Politécnica de Madrid (UPM), Madrid, Spain
andrea@hopu.eu, ralcaria@upm.es
Abstract. Data models and semantics are a key aspect for the valorization of
data in cross-domain applications and to obtain knowledge/insights beyond the
original applications (vertical use cases). An important role of Big Data and a
key fundament of its success is this capacity to discover and extract new
knowledge beyond the original use of data, in order to learn, optimize processes
and understand the hidden rules of our world. This works presents the different
data models from standardization bodies such as IEEE PAR2530, ITU-T
FG DPM, ETSI ISG CIM and oneM2M, W3C SSN, OMA LwM2M etc. An
analysis and comparative among all of them and also the opportunities to link
them in order to guarantee that we can obtain the major value through
co-operation among cities and different departments. This work is contextual-
ized in the principles from the Open and Agile Smart Cites (OASC) and linked
initiatives focused on data management cross-cities and large scale pilots.
Keywords: Smart cities  Data models  Internet of things
Semantics  ETSI ISG CIM  ITU-T  oneM2M  FIWARE
Open and agile smart cities  OASC
1
Introduction
Smart Cities present an opportunity for valorizing digitalized services and Internet of
Things (IoT) infrastructure with the creation of disruptive and innovative services and
solutions. These innovations through exploitation of data correlation, open data and
data analytics will deal with cities challenges such as environmental sustainability,
citizens’ engagement, economic growth and citizens’ mobility. This work carries out an
analysis and discussion around the standardization and data modelling actions that are
working towards deﬁning a common semantic descriptions for smart city data to enable
cross-domain and advanced services development in smart cities.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_8

For this purpose, we have identiﬁed the main activities from IEEE, ETSI, ITU-T,
W3C and OMA present examples of data that can be collected from cities, discuss
issues around this data and put forward some preliminary thoughts for creating a
semantic description model [1] to describe and help discover, index and query smart
city data.
The Table 1 presents a summary of the key datasets that we can ﬁnd in a Smart City
based on the datasets from CityPulse [2] and FIESTA-IoT [3].
Table 1. Different types of data models in a Smart City.
Category
Description
Publisher (Data
owner)
Data access
Mobility
City Maps (Roads, street
names, Points of Interests
(POIs), stations, etc.)
Government,
Private
Companies and
Organizations
OpenStreetMap (Open Data),
APIs accessible (Google Maps)
and proprietary/closed GIS such
as ESRI
Public transport schedules
Government and
Private
Companies
Open Data or Web-accessible
content
Transport authority updates
and events (road status, road
works…)
Government and
Private
Companies
Waze (APIs accessible),
Web-accessible data and Open
Data
Car Parks/Parking meters
Government and
Private
Companies
APIs accessible and Open Data
Trafﬁc (Number of vehicles
passing between two points
and speed)
Government and
Private
Companies
Private data, APIs accessible and
Open Data
Opportunistic and Crowd
monitoring
Government,
Private
Companies and
Organizations
Nomadic Sensing (NOSE)
Environmental
data
Air Quality (Air Quality
Index - AQI, Particles
concentration and gases
concentration)
Government
Private data, APIs accessible and
Open Data
Temperature/Humidity
Government
Private data, APIs accessible and
Open Data
Noise level/Noise maps
Government
Private data, APIs accessible and
Open Data
Tourism
Points of Interest, Time
schedule, events, museums,
bar/discos etc.
Cultural Groups
APIs and Web available data
Demographic
Population, Crowd
Monitoring, Ages, Cultural
Level, Economical Level
Government and
organizations
Open Data
Co-creation
(Citizens
engagement)
Opinions, decisions, surveys,
etc.
Government and
organizations
Siidi (Organicity)
78
A. J. Jara et al.

2
Smart Cities Semantics and Data Models
This section introduces different available data models used in the majority of smart
cities deployments and European actions such as FIESTA-IoT, Synchronicity,
Organicity, CityPulse etc.
2.1
M3-Lite
The M3-lite is a taxonomy that enables testbeds to semantically annotate the IoT data
produced by heterogeneous devices and store them in a federated datastore such as
FIESTA-IoT. In this taxonomy, we classify devices, the domain of interests (health,
smart home, smart kitchen, environmental monitoring, etc.), phenomena and unit of
measurements.
2.2
W3C SSN
W3C SSN describes not only sensor device capabilities, but also organises the sensors
into systems and describes processes that model sensor operations and can work across
multiple domains. The goal is to correlate measurement data with capabilities of
sensors (and sensor systems), however the descriptions about observation and mea-
surement data are generic and cannot be used to annotate the data with domain
knowledge - speciﬁc to applications. Therefore, SSN by itself cannot be used to
describe smart city services (scenarios) in detail, as each service has its own quality
requirements, relies on its own set of sensors, has different demands on data ownership
(security, privacy concerns) etc.
Previous research has suggested building a linked-data approach for stream
annotation [4]. According to this approach, external domain knowledge about the data
can be provided on request - and can be speciﬁc per service rendered (e.g. quality
description, sensor capabilities, etc.). The model proposed in [5] describes some basic,
common attributes on the data stream but delegates details about the speciﬁc streams to
other models (linked-data models).
2.3
oneM2M Ontology SAREF Ontology
The oneM2M base ontology and the SAREF ontology are described in the oneM2M
TS00122 and ETSI TS1032643. OneM2M base ontology aims to provide a high level
ontology for the IoT market in order to provide a minimal set of common knowledge
that enables the cross-domain syntactic and semantic interoperability. oneM2M
ontology is very abstract and general, thereby oneM2M expects external ontologies that
describe a speciﬁc domain of interest in a more detailed way to be mapped to the
oneM2M base ontology. Additionally, oneM2M deﬁnes how to internetwork between
devices and things from different domains is enabled.
For the speciﬁc domains, it is where ETSI ISG Context Information Management,
ETSI ISG Smart Cities data models, and other widely accepted ontologies such as
SAREF plays a crucial role. These ontologies are described in the coming two sections.
As an example of this base ontology and the speciﬁc domains one, we can explore the
Smart Cities Semantics and Data Models
79

following Figure, where oneM2M plays a key role as an interconnection with
telecommunications infrastructure, and other information systems. However, ETSI ISG
CIM offers all the extensions for the Context management required to develop Smart
Apps and provide contextualized Open Data (Fig. 1).
2.4
SAREF Ontology
SAREF aims to provide a common knowledge for the domain of Smart Appliance,
especially on the energy consuming aspect. Compared to oneM2M base ontology, it is
less high level and more applicable to describe devices. The mapping between
oneM2M base ontology and SAREF is performed by oneM2M.
2.5
FIWARE and ETSI ISG CIM
ETSI ISG Context Information Management (ETSI ISG CIM). FIWARE initiative and
platform through the Orion Context Broker component identiﬁed a key market need for
IoT and Smart Cities; it is the management of context in a scalable and standardized
way. For this purpose, FIWARE deﬁned on the one hand, OMA NGSI interfaces to
offer a homogeneous access to data, and on the other hand, a set of data models being
standardized by ETSI ISG CIM.
Context Information provides the meta-data structure for sensors measurement and
also other data feeds from video, social media etc. Even when context is very simple to
understand by human being, in order to provide artiﬁcial intelligence capabilities to
smart systems, it is crucial to formalize and provide much more details about the
context and make it available in conjunction with the data. A Context Information
Fig. 1. ETSI ISG CIM as an example of extension and specialization for oneM2M base
ontology towards the User Applications and re-use of data, which extends to oneM2M which is
mainly focused on the interconnection among platforms and devices.
80
A. J. Jara et al.

Management (CIM) system acts as a clearing-house for publishing, discovering,
monitoring and maintaining data according to relevant contexts for smart applications.
“ETSI ISG CIM will specify protocols running on top of IoT platforms and
allowing exchange of data together with its context, this includes what is described by
the data, what was measured, when, where, by what, the time of validity, ownership,
and others. That will dramatically extend the interoperability of applications, helping
smart cities to integrate their existing services and enable new third-party services”, as
stated by the ETSI ISG CIM convenor, Lindsay Frost. ETSI ISG CIM has been focused
on developing speciﬁcations for a common context information management API, data
publication platforms and standard data models. A practical example is presented in the
coming Fig. 2.
2.6
OMA LwM2M and IPSO Smart Objects
IPSO Application Framework from the IPSO Alliance deﬁnes in collaboration with
OMA LwM2M, a set of RESTFul interfaces for the deﬁnition and management of
resource lists, batch, sensors, parameter, actuators and binding tables of resources. For
example, the semantic IPSO Application Framework has chosen SenML over JSON
with the usage of the Uniﬁed Code for Units of Measure (UCUM). These initial
semantic capabilities allow avoiding the initial mistakes from CoAP such as the use of
inappropriate unit codes such as 23 °C for temperature, when it is according to the
UCUM standard means velocity of light, and consequently this should be 23 CEL.
The current semantic capabilities from IPSO Application Framework are very basic
in order to offer a very simple and lightweight solution. In details, the protocol over
which IPSO Smart Object are deﬁned is OMA LwM2M.
OMA LwM2M Device Management is a protocol for device management, the use
this protocol in M2M requires efﬁcient message formats and transport replacement such
Fig. 2. ETSI ISG CIM domain-speciﬁc domain mapping of the base model [6, 7].
Smart Cities Semantics and Data Models
81

as CoAP, and Core Link Format. For that reason, Lightweight OMA DM has chosen
CoAP to provide the core functionalities of HTTP (GET, PUT, POST, DELETE
commands) in a reduced footprint.
OMA LwM2M offers key functionalities for the device management such as
remote ﬁrmware upgrade, remote diagnostics, information reporting (read data, write
data and subscribe to events) and it focuses on providing mechanisms for asynchronous
and synchronous communication, store, forward and caching mechanism for opti-
mizing the communication, and security with mechanisms to provide two way
authentication and secure communication channels.
A detailed example of OMA LwM2M objects are presented in the references [8, 9]
for the Smart Spot device, where several sensors focused on air quality, noise, tem-
perature, humidity, crowd monitoring and connectivity management are properly
modelled.
3
Analysis and Discussion
The main challenge that arise for the IoT is to make a proper usage and exploitation of
the IoT potential to build more powerful applications and services.
The support for heterogeneous and legacy devices integration is being integrated
thanks to the IoT context brokers and middlewares that enable the interfacing of
heterogeneous protocols through a homogenized and harmonized interface. Addition-
ally, these entities such as context brokers are enabling the capacity to integrate more
details about context that facilitates the exploitation of the data and content provided by
the sensors with knowledge engineering technologies.
For that reason, the current steps for the IoT are focused on the importance of
metadata to build intelligent solutions; and there is where emerging context-aware
systems such as the proposed by ETSI ISG CIM will play a key role, and where the
platforms such as FIWARE via the Orion Context Broker and the several implemen-
tations of oneM2M such as OpenMTC and OM2M are supporting this data integration
and data brokering as the core element.
A very relevant feature is the alignment that is happening between ETSI ISG CIM
with ETSI oneM2M, at the same time that ETSI oneM2M with OMA LwM2M, and the
IPSO Smart Objects and OMA LwM2M.
Thereby, we are ﬁnding a proper ecosystem deﬁnition where oneM2M, FIWARE,
ETSI ISG CIM and OMA LwM2M are co-existing and playing a clear complementary
role.
In details, OMA LwM2M is supported by the oneM2M, which provides an
international initiative that will play a very relevant role to propose the standards for the
syntactic and semantic information.
oneM2M deﬁnes the abstraction layers, using the same format. This will ease the
creation of the higher-layers for the IoT and M2M that enables a high-level modeling of
real world entities, development of applications, and ﬁnally huge quantities of data
collection. oneM2M will also offer support and solutions to facilitate the development
of vertical industries and new markets.
82
A. J. Jara et al.

However, oneM2M has play a very clever and smart approach relying on ETSI ISG
CIM and FIWARE for the development and interfacing of the Smart Applications and
data exploitation, i.e., the interconnection with mobile and Web Apps developers.
At the same way, oneM2M is also relying on OMA LwM2M for the devices
management and a more native interconnection with the devices addressing the
physical features, sensors conﬁguration, ﬁne tuning, and calibration etc. as required
several times for the proper use of the sensors, its conﬁguration, and maintenance.
oneM2M is extending and coordinating with other institutions such as HGI,
Broadband Forum, OSGi, Continua Alliance for healthcare devices, ZigBee Alliance
for smart metering devices etc. Therefore, they are presenting a very inclusive
approach.
The following Figure presents our vision of oneM2M integration for the physical
devices with a support for OMA LwM2M for the devices management, in conjunction
with FIWARE with a support for ETSI ISG CIM for the Apps interfacing, and also
support for Open Data (CKAN) and external applications (Fig. 3).
4
Conclusions
The market is moving from vertical solutions where the sensors are stove-piped (one
device per application) to speciﬁc platforms for its application in pre-deﬁned use cases
towards a more open market, where the sensors will be re-used, shared and accessed by
a wide range of different applications.
Fig. 3. FIWARE and oneM2M cooperation in conjunction with ETSI ISG CIM and OMA
LwM2M.
Smart Cities Semantics and Data Models
83

The pending challenges cover the development of tools and protocols for dynamic
interoperability, semantic discovery reasoners, mechanisms to re-adapt devices in case
of change of context, ontologies repository, and in general toolkits that allows the
semantic integration and exploitation from the IoT.
Thereby, the semantics will be managed through the different phases inside of a use
case, their heterogeneous devices integrated, and they will be deﬁned interfaces among
the different components involved. In addition, these interfaces are based on very agile
and ﬂexible technologies such as RESTFul architecture, which supports a resource-
oriented solution, simplifying and optimizing resource manipulations for a broad range
of devices and solutions that enabled a quick and efﬁcient application development.
The market is also moving from proprietary and complex protocols to open
approaches such as HTTP, OMA LwM2M, CoAP (IPSO), oneM2M (OpenMTC and
OM2M), and MQTT (Eclipse Foundation).
The current status and evolution of the IoT in the Smart Cities market is driven by
the semantic and context-aware data models. The data exploitation of the data is the
next step after of the provisioning of architectures and solutions with context-
awareness. Interoperability and semantic-annotated models deﬁnitely increase the
re-usability of the IoT resources outside the use cases and scope in which they were
originally deployed and designed. This is a key need for Smart Cities use-cases and
emerging IoT markets focused on large scale pilots. An example of data opportunities
is Synchronicity http://synchronicity-iot.eu, a data marketplace for Smart Cities.
Acknowledgments. This work is sponsored by HES-SO University. This work has been carried out
in collaboration with the National University of Ireland Galway (NUIG) in the framework of the
European Project FIESTA-IoT (Grant Agreement CNECT-ICT-643943) and the experiment
FINETUNE; a the same time this work has included results carried out in collaboration with
SmartSDK (Grant Agreement 723174) and SynchroniCity (Grant Agreement 732240). Finally,
Andrea Gómez collaboration is supported as part of the scholarship (Industrial Doctorate) by UCAM.
References
1. Bischof, S., Karapantelakis, A., Nechifor, C., Sheth, A.P., Mileo, A., Barnaghi, P.: Semantic
Modelling of Smart City Data. Kno.e.sis Publications. The Ohio Center of Excellence in
Knowledge-Enabled Computing (Kno.e.sis) (2014). http://corescholar.libraries.wright.edu/
knoesis/572
2. CityPulse: EU FP7 CityPulse Project, Grant No. 603095. http://www.ict-citypulse.eu
3. FIESTA-IoT: EU H2020 FIESTA-IoT project “Federated Interoperable Semantic IoT/cloud
Testbeds and Applications”, Grant No. 643943 (2017)
4. Barnaghi, P., Wang, W., Dong, L., Wang, C.: A linked-data model for semantic sensor
streams. In: 2013 IEEE and Internet of Things (iThings/CPSCom), IEEE International
Conference on Green Computing and Communications (GreenCom), and IEEE Cyber,
Physical and Social Computing, pp. 468–475 (2013)
5. Compton, M., Barnaghi, P., Bermudez, L., Garcıa-Castro, R., Corcho, O., Cox, S., Graybeal,
J., Hauswirth, M., Henson, C., Herzog, A., Huang, V., Janowicz, K., Kelsey, W.D., Phuoc, D.
L., Lefort, L., Leggieri, M., Neuhaus, H., Nikolov, A., Page, K., Passant, A., Sheth, A.,
Taylor, K.: The SSN ontology of the W3C semantic sensor network incubator group. Web
Semant. Sci. Serv. Agents World Wide Web 17, 25–32 (2012)
84
A. J. Jara et al.

6. ETSI ISG CIM: Context Information Management. https://portal.etsi.org/tb.aspx?tbid=
854&SubTB=854
7. Jara, A.J., et al.: An analysis of context-aware data models for smart cities: towards ﬁware and
etsi cim emerging data model. Int. Arch. Photogram. Remote Sens. Spat. Inf. Sci. 42 (2017)
8. HOP Ubiquitous: Homard overview and OMA LwM2M (2017). https://storage.googleapis.
com/softﬁre/homard%20speciﬁcations.pdf
9. HOP Ubiquitous: Smart Spot Firmware (2017). https://storage.googleapis.com/softﬁre/smart-
spot-speciﬁcations%20ﬁrmware-063d.pdf
Smart Cities Semantics and Data Models
85

The Information Technologies
in the Competitiveness of the Tourism Sector
Pedro Liberato1(&), Dália Liberato1, António Abreu2,
Elisa Alén-González3, and Álvaro Rocha4
1 School of Hospitality and Tourism, Polytechnic Institute of Porto,
Porto, Portugal
{pedrolib,dalialib}@esht.ipp.pt
2 Porto Accounting and Business School, Polytechnic Institute of Porto,
Porto, Portugal
aabreu@iscap.ipp.pt
3 Faculty of Business Sciences and Tourism, University of Vigo, Vigo, Spain
alen@uvigo.es
4 Informatics Engineering Department, Coimbra University, Coimbra, Portugal
amrocha@dei.uc.pt
Abstract. This article seeks to reﬂect on the changes in tourism in the digi-
talization era. Regarding the tourism industry, the development of technology
and the internet have been an asset. The adoption of information and commu-
nication technologies has led to changes in the way of communication with the
individual or institutional clients and enables the adoption of innovative busi-
ness models and electronic sales channels of tourism products. It is crucial the
information about demand/tourists, tourist destinations, amenities, availability,
pricing, geographic information and weather, supply and transport, information
about companies, intermediaries and competitors; trends in the tourism market,
prices, products and tourist packages, Marketing Organization of tourist desti-
nations, trends in the industry, size and nature of tourism, as well as policies and
development plans. The role of the ICT in tourism has become an essential tool
in today’s world of quick information.
Keywords: Information and communication technologies  Internet
Tourism  E-tourism
1
Introduction
To many countries, tourism plays an important role in generating revenue for the
nation. Tourism is a major export industry for many countries and cities [40] and as an
important and necessary industry, it is considered a sector of the economy that can
beneﬁt from the various technological resources available [23]. Despite all the political,
economic and social instability, people continue to feel like traveling. Tourism is an
industry with its own characteristics, with great importance in the economy of many
countries, including Portugal [32], based on its high potential for generating income
and employment, contributing to the increase in the gross domestic product (GDP) of
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_9

each country [36]. The use of Information and Communication Technologies can
provide competitive advantages in promotion, as well as strengthen the strategies and
operations of the tourism industry [10].
[39] evidences that tourism adds a variety of services that can beneﬁt from Infor-
mation and Communication Technologies such as: accommodation services that
include hotels, apartments, residences and villages; catering services, including
restaurants and cafés; transport services including trains, buses, taxis and aviation
companies; tourist support services such as insurance companies and exchange ofﬁces;
recreational services such as sports centers, swimming pools, marinas and golf courses;
attractions services such as museums, parks, gardens, monuments and conference
centers; entertainment services including casinos, cinemas, clubs and theaters.
Also [39] consider crucial information about demand/tourists, tourist destinations,
amenities, availability, pricing, geographic information and weather, supply and
transport, information about companies, intermediaries and competitors; trends in the
tourism market, amenities, availability, prices, package tours, Marketing Organization
of tourist destinations, trends in the industry, size and nature of tourism, as well as
policies and development plans.
On the other hand, [34] highlights the link between information and communication
technologies and the tourist activity, saying that “(…) contrary to what may seem at
ﬁrst sight, tourism and ICT can be regarded as two sides of the same coin”. [11] point
out that a diversity of suppliers throughout the supply chain are now able to form a
direct connection with customers through digital platforms, providing visitors more
familiar with them, the ability to create personalized trips.
If we make an historical analysis about tourism development, it is closely linked to
technological evolution. Thus, from [34] perspective, communication and information
tools are essential to tourism activity, and it is fundamental that tourism service pro-
viders consider the contemporary tourism “ecosystem”, formulating strategies for the
sector and for tourism destinations [11], because, while some tourists prefer the con-
venience of group trips organized in packagings, others hope to get more interactive
and personalized services [21].
This analysis seeks to understand the role of Information and Communication
Technologies and the Internet in most of the activities associated with the tourism
sector.
2
Technological Development: The Internet
As [8] points out, Information and Communication Technologies have a strong
inﬂuence on people’s daily lives. The current society is somehow linked to a concept of
knowledge-based economy, a “(…) economy that is focused on knowledge and
information as bases of production, productivity and competitiveness” [14]. Essential,
today, is to carry out a correct use, research, storage and processing of information,
making it essential to learn how to deal with technology and with all the information
that is available. People have to follow properly the innovations towards effective
integration into the labor market and services. On the other hand, and as [42] concern,
wealth creation must be the relationship between individuals and institutions, as well as
The Information Technologies in the Competitiveness
87

the ability to manage the existing means and resources in the territory. Technological
development has contributed to several changes that occur in society and in people’s
behaviors, particularly in how we communicate, how people behave and interact in
society [15], as well as in the way we seek products, services and information [11].
[37] refer that information and communication technologies have not only changed
the way people conduct their activities, but also the mobilization of material and
immaterial resources, the way in which generates wealth and how negotiated oppor-
tunities are created and expanded. [41] emphasize that “the internet, today, represents
one of the main information and communication technologies. This new tool works,
through thousands of interconnected computers, around the world, enabling data
exchange and information provided in a large network”. To [6] the internet, with
regard, speciﬁcally, to tourism, has changed profoundly the way tourists access to
information, plan their trips, make reservations, and share their travel experiences. The
internet and other interactive technologies, in addition to causing changes in the
behavior of people drive changes in the market, in particular the tourism market,
enabling the global distribution of tourist services [4]. Technologies use makes the
market more competitive and more accessible to the user [45].
[37], assumes that the internet and tourism are ideal partners. The internet meets the
needs of the tourists of the 21st century, increasingly demanding, informed and
sophisticated. The internet reduces geographical distances between tourism companies
and customers, which may contribute to greater ﬂexibility, mobility, elasticity and
efﬁciency in business.
As says [1] the Internet enables a wide social network (virtual), linking the various
subjects by the most diverse forms, astonishing speed and in most cases, a synchronous
interface, giving a new concept of social interaction. By reducing geographical dis-
tances between tourism companies and customers, creates greater ﬂexibility, mobility,
elasticity and efﬁciency in business. The Internet is a strategic resource and [16] “(…) it
will be enough for the agents and promoters of the hotel sector to become aware that
the Internet, being a useful window for promotion and dissemination, is an ideal space
for conducting business [47]. In order to take full advantage of this communication
tool, agents and promoters in the hotel sector must centralize their core business by
making proposals, on the internet, based on ﬂexibility models, speed, utility and
imagination [44, 48]. [16] point out that companies that provide tourism services, when
setting up their own websites have the possibility to establish direct contacts with
consumers, which will help to increase sales. On the other hand, [31] emphasize that
the Internet allows tourism providers to be in the same place as their clients or potential
clients, and to understand their attitudes, needs, interests, choices and requirements.
Technologies promote the exchange of information which is essential for tourism [2].
[7] refers that tourism activity generates a signiﬁcant amount of information that has a
strategic importance and value to the business. This means that the information must be
treated as a strategic element of organizational/institutional planning. Using the internet
tourists have immediate access to useful information, varied destinations, and the
possibility of making reservations in an easier and faster way. Particular attention
should be given to changes in market needs, triggered by technological innovations
[12] and, especially to a new market resource, the mobility and ubiquity allowed by the
spread of smartphones and the emergence of QR codes that contextualize the mobile
88
P. Liberato et al.

applications and services and renew the discussion about the importance of the des-
tination strategy.
2.1
The Emergence of E-Tourism
Tourism companies will gain competitive advantage if they are able to maximize their
proﬁts by improving their services in order to achieve a greater degree of consumer
satisfaction. In this technological context, the destination is understood to be a variety
of individual products and opportunities for experiences. Regarding to the tourism
industry, the development of technology and the internet have been an asset, since
according to [13] contributed to several changes that seek to improve the offer in terms
of services. The Internet and other interactive multimedia platforms contribute to the
promotion of tourism by enhancing transformations in the tourism industry structure
[10]. Experts at the 19th World Travel Monitor Forum (ITB Berlin, 2011) point out that
tourism needs to be online, beyond traditional marketing strategies. They also add
(19th World Travel Monitor Forum, ITB Berlin, 2011) that people worldwide prefer to
use the internet to book their trips. In the 20th Monitor Forum WorldTravel (ITB
Berlin, 2012) concluded that people seek to take advantage of the latest technologies in
terms of information and to purchase products and services. It is also noted that the
purchase of travel and other tourism products is extremely popular online. The report
Future Traveller Tribes 2030, Understanding Tomorrow Traveller, published by
Amadeus Traveller Trend Observatory in April 2015, refers that travel trends, in the
next few years will be determined primarily by the intensive use of technologies social,
cultural criteria, the convenience in travel management and trip, the lack of time and
luxury.
For many tourists, technology represents an opportunity to actively participate in
the destination activities and to take part personally in the construction of their own
experience [38]. Likewise, they place special emphasis on sharing their experience with
other tourists and residents, and are therefore willing to activate conversation processes
through social media with the destination using electronic devices [14], with their
family, friends or anonymous users [8, 37]. In this sense, it has been shown that the
most valued experiences are those co-created with tourists and supported by high levels
of technology [46]. As argued by [37], ICTs are extremely useful because they facil-
itate encounters between tourists and the destination, and improve the experiential
process in time and space.
Tourism companies, feeling the need to adapt to technological development,
included the technologies in their business processes, and thus emerged the e-Tourism.
According to [9] e-Tourism represents all aspects of tourism that involve and promote
the integration of information and communication technologies, revolutionizing the
strategic relations of tourism organizations and all of its stakeholders [13]. The concept
of e-Tourism encloses all functions of business such as e-commerce, email marketing,
electronic production, as well as the electronic strategy, electronic planning and
management for all sectors in the tourism industry [13], grouping, also according to the
same authors, three main areas: business management, information systems and tour-
ism management. [11] highlight that e-tourism is a result of the scanning of all pro-
cesses and the value chain of the tourism sector, in particular travel, hotel and
The Information Technologies in the Competitiveness
89

restaurant management. E-tourism, is also the result of the fact that we live in the age of
wireless communication and that tourists use their mobile devices with internet access
before, during and after their trips [29].
[17] refer to the electronic or virtual agency that allows users to [17] “search, plan
and make all the reservations they wish from a computer terminal, allowing the issuing
airline tickets, reservations in hotels, rent a car, choice of cruises, among others, with
all partners associated with such virtual agencies”, whereas [24] consider very
important the existence of applications for smartphones that offer tourists a wide variety
of services such as audio guides, road maps, interactive info on transport, tariffs,
cultural agendas, among others, as a strategy to support the decision on the choice of
destination.
2.2
Big Data
Companies face today, many challenges, such as the storage and treatment of numerous
and varied data generated in the course of its activities. According to [26], innovation,
business model transformation, globalization and personalization services, observed
nowadays, has contributed to the increase of data generated. Globalization has not only
trade and even the way of working, but also the variety of data format. Organizations
need to be effective, provide to their client a quickly and efﬁciently service. To do this
they need to understand and meet the needs of its customers. In addition, companies
must take into account new data sources generated by social networks, mobile devices
and sensors [26]. However, they need and access a lot of information, but they do not
always get value from the same face of their large quantity and lack of structuring [51].
It is in this context that Big Data allows the access and analysis of large amounts of
data. Thus, analytical intelligence makes it possible for companies to become more
competitive [18]. Big Data is not a “thing”, but a dynamic activity that crosses many IT
frontiers” [22], constituting itself as a large data set, whose challenge is storage,
research, sharing the visualization and analysis of the information that is generated,
understanding [50] Big Data as a collection of complex and voluminous databases that
do not allow simple operations effectively with management systems of traditional
database. For INTEL (2012), Big Data Analytics is a technologic strategy that allows
companies to more intensely and accurately perceive customers by analyzing patterns
and correlations, enabling tourism companies to gain more advantages and become
more competitive. Cloud computing enables small and medium businesses to imple-
ment Big Data technology.
By the end of the 20th century and the beginning of the 21st century, and as a result
of the inﬂuence of the Internet and e-commerce, a great quantity and variety of data
was instantly produced as users transmitted it through the World Wide Web. [19], point
out that the amount of data that may be of interest and can be used by both business and
people increases every day.
Faced with such a large amount of data, the most complex becomes its analysis and
understanding. [30] or [33] report that data analysis will allow a better understanding of
customers, markets, competitors, products, the market environment, the impact of
technologies or suppliers. We now beneﬁt from other transformations resulting from
the ubiquity of mobile devices, the use of cloud computing and the connection of
90
P. Liberato et al.

various everyday devices, dubbed the “internet of things” [16], which centralize data
production for a continuous ﬂow.
2.3
Internet of Things (IoT)
With the increasing use of smartphones and tablets, increased the number of devices
connected to the Internet [28]. The type of communication known as the Internet of things
appears linked to a worldwide network of interconnected objects [25]. The Internet of
Things (IoT) covers the use of networks, sensors and cloud computing [5, 27, 41, 42],
allowing the link between physical objects and computers with the internet. According to
[4] the Internet of things allows the development of a large number of applications in
various areas and environments, integrating objects such as mobile phones, sensors, and
other devices that interact with each other. In the Internet of things the objects work in
smart spaces, using intelligent interfaces to relate and communicate with various envi-
ronments [45]. These concepts are formed by ubiquitous computing, pervasive com-
puting and intelligent environments [20, 49]. [42] also agree that the ubiquitous
computing has become invisible to the end user computing, and pervasive computing
suggests that user’s access to information and computer resources regardless of the
location or device used. [20] consider that in smart environments, devices can interact
with the processes. In turn, [43] report that in the environment of the Internet of things,
objects acquire naturally three features: intelligence, connectivity and interaction.
The development of new technologies will enable the use of smart devices in
everyday life. With the Internet of Things, numerous objects will be linked together,
with increased trafﬁc volume, and data storage capacity [43].
2.4
Virtual Reality and Augmented Reality
Virtual reality consists of a three-dimensional, computer generated medium that allows
the user to see, interact and manifest in an environment outside reality. Depending on
the interactivity provided may be immersive (based on use, for example helmets) or
non-immersive (based on the use of monitors). Got a lot of notoriety with Second Life.
Augmented reality is a technology derived from virtual reality that consists of
superimposing digital information to real-world images, that is, it does not completely
emerge in a virtual world where it cannot see what is around it: it is a supplement to the
reality and not a substitution of it. Can also be seen as a middle ground between Virtual
reality (fully synthetic) and Telepresence (quite real), according to [35]. [3, 4] states
that the main difference of virtual reality regarding augmented reality lies in the fact
that in augmented reality we have the possibility to visualize objects and graphics in a
real environment while also allowing virtual reality although in an environment sep-
arated from reality.
Given the great potential of this technology in several areas and tourism in par-
ticular, there is now a large proliferation of applications for mobile devices (Apps),
with augmented reality, applied in museums, monuments, galleries, open spaces and
tourism attractions in general, where objects can be “augmented” and complemented in
real time with diverse information (text, images, three-dimensional animations, audio
or video).
The Information Technologies in the Competitiveness
91

2.5
Location-Based Services (LBS)
The user’s location-based services (Location Based Services-LBS) use GPS technology
and enable the development of applications that allow to implement new models of
mobility. Are example, trafﬁc management systems, navigation systems and informa-
tion to the user on the move. The same concept applied with a proximity criterion refers
to the beacon. The Beacon is a small device that connects with other electronic devices
and POS (point-of-sale system). In the same physical space, for example a shop, such
may be connected with a system of payment (e.g. Paypal). Thus, an individual with a
mobile device, for example a smartphone with Bluetooth will receive notiﬁcations as
long as it is in the range of the beacon.
3
Final Considerations
Agents that use the internet become more competitive because they are chosen by users
who prefer to opt for destinations, make hotel reservations more independently, faster
and cheaper.
Given the overall context of promoting use of technologies in search of information
about destinations and tourism enterprises, will be decisive for the tourism sector
management, investments in the professionalization of human resources with special
emphasis on the enhancement of technological skills; the continuous appreciation of
the mark – ofﬁcial product, ofﬁcial portal, for the submission of quality companies in
the region, with a permanent concern for transparency with regard to information
provided to the tourist/visitor.
A shared vision should be developed between the citizen who lives in the city, the
tourist that visits it, destination management, and the different stakeholders, in order to
add value to the citizen and the visitor. Destination Management Organizations (DMO’s)
must provide citizens and tourists with a collaborative platform that allows bi-directional
communication between the public administration and citizens or tourists/visitors.
In order to value the tourist experience at the destination, free internet should be
promoted in public and private places, associated with products that incorporate the use
of technology and digital media in a context of diversiﬁed offer.
References
1. Amadeus Traveller Trend Observatory: Future Traveller Tribes 2030: Understanding
tomorrow’s
Travel
(2015).
http://www.amadeus.com/documents/future-traveller-tribes-
2030/travel-report-future-traveller-tribes-2030.pdf
2. Anjos, E., Souza, F., Ramos, K.: Novas Tecnologias e Turismo: um estudo do site Vai
Brasil. Caderno Virtual de Turismo 6(4) (2006)
3. Atzori, L., Iera, A., Morabito, G.: The internet of things: a survey. Comput. Netw. 54(15),
2787–2805 (2010)
4. Avelar, E., et al.: Arquitetura de Comunicação para Cidades Inteligentes: Uma proposta
heterogénea, extensível e de baixo custo. Universidade Federal de Pernambuco (UFPE),
Recife (2010)
92
P. Liberato et al.

5. Azuma, R.: A survey of augmented reality. Teleoper. Virtual Environ. 6(4), 355–385 (1997)
6. Bilgihan, A., Barreda, A., Okumus, F., Nusair, K.: Consumer perception of knowledge-
sharing in travel-related online social networks. Tour. Manag. 52(2), 287–296 (2016)
7. Bissoli, M.: Planejamento Turístico Municipal com Suporte em Sistemas de Informação.
Futura, São Paulo (1999)
8. Brejla, P., Gilbert, D.: An exploratory use of web content analysis to understand cruise
tourism services. Int. J. Tour. Res. 16(2), 157–168 (2014)
9. Buhalis, D.: eTourism: Information Technology for Strategic Tourism Management. Pearson
(Financial Times/Prentice Hall), London (2003)
10. Buhalis, D.: Information Technology for small and medium-sized tourism enterprises. In:
Keller, P., Bieger, T. (eds.) The Future of Small and Medium Sized Enterprises in Tourism,
AIEST Congress 2004, Jordan, Editions AIEST, pp. 235–258. St-Gallen, Switzerland
(2004). ISBN 3952172359
11. Buhalis, D., Flouri, E.: Wireless technologies for tourism destinations. In: Frew, A. (ed.)
Information and Communications Technologies in Tourism, ENTER 2004 Proceedings,
pp. 27–38. Springer, Wien (2004). ISBN 3211206698
12. Buhalis, D., Law, R.: Progress in tourism management: twenty years on and 10 years after
the internet. The state of eTourism research. Tour. Manag. 29(4), 609–623 (2008)
13. Buhalis, D., Jun, S.: eTourism. Good Fellow Publishers Limited, Oxford (2011)
14. Buonincontri, P., Micera, R.: The experience co-creation in smart tourism destinations: a
multiple case analysis of european destinations. J. Inf. Technol. Tour. 16, 285–315 (2016)
15. Castells, M.: La Ciudad de la nueva economía, La Factoría, 12 (2000)
16. Castells, M.: A Sociedade em Rede. Editora Paz e terra S/A, São Paulo (2002)
17. Costa, J.; Águas, P.; Rita, P.: Tendências Internacionais em Turismo, 2.ª edn. Lisboa, Lidel
(2004)
18. Coutinho, L., Sarti, F.: Nota Técnica Parcial: tecnologia da informação aplicada ao turismo.
Centro de Gestão e Estudos Estratégicos. Ministério do Turismo (2007)
19. Davenport, T., Barth, P., Bean, R.: How big data is different. Harvard Bus. Rev. (2012)
20. Demchenko, Y., Grosso, P., De Laat, C., Membrey, P.: Addressing big data issues in
scientiﬁc data infrastructure. In: 2013 International Conference on Collaboration Technolo-
gies and Systems (CTS). IEEE (2013)
21. Dohr, A., Modre-Osprian, R., Drobics, M., Hayn., D., Schreier, G.: The Internet of Things
for ambient assisted living. In: Seventh International Conference on Information Technol-
ogy. IEEE (2010)
22. Forsyth, L., Dwyer, P., Rao, P.: The price competitiveness of travel and tourism: a
comparison of 19 destinations. Tour. Manag. 21(1), 9–22 (2000)
23. Gantz, W.: Reﬂections on communication and sport: on fanship and social relationships.
Commun. Sport 1(2), 176–187 (2012)
24. García-Crespo, A., Chamizo, J., Rivera, I., Mencke, M., Colomo-Palacios, R., Gómez-Ber-
bís, J.M.: SPETA: social pervasive e-tourism advisor. Telematics Inform. 26, 306–315
(2009)
25. Ji Hoon, P., Cheolhan, L., Changsok, Y., Yoonjae, N.: Int. J. Inf. Manage. 36(6) (2016)
26. Jiang, Y., Zhang L. Wang, L.: Wireless sensor networks and the Internet of Things. Int.
J. Distrib. Sens. Netw. 1–7 (2016). Research Center for Mobile Computing, Tsinghua
University, Institute of Microelectronics, Tsinghua University, China
27. Krishnan, K.: Data Warehousing in the Age of Big Data. Newnes, Boston (2013)
28. Kurose, J., Atzori, L., Lera, A., Morabito, G.: The Internet of Things: a survey. Comput.
Netw. 54(15), 2787–2805 (2010)
29. Kurose, J.F., Ross, K.W.: Redes de computadores e a Internet: uma abordagem top-down,
5th edn. Addison Wesley, São Paulo (2010)
The Information Technologies in the Competitiveness
93

30. Langelund, S.: Mobile travel. Tour. Hosp. Res. 7, 284–286 (2007)
31. Marchand, D., Peppard, J.: Why IT Fumbles Analytics. Harvard Bus. Rev. 91, 104–112
(2013)
32. Maurer, C., Schaich, S.: Online customer reviews used as complaint management tool. Inf.
Commun. Technol. Tour. 2011, 499–511 (2011)
33. Maurer, C., Wiegmann, R.: Effectiveness of advertising on social network sites: a case study
on Facebook. In: Law, R., Fuchs, M., Ricci, F. (eds.) Information and Communication
Technologies in Tourism 2011. Springer, Vienna (2011)
34. Mayer-Schönberger, V., Cukier, K.: Big Data: A Revolution That Will Transform How We
Live, Work, and Think (2014). ISBN-10: 0544227751
35. Milgram, P.: A taxonomy of mixed reality visual displays. IEICE Trans. Inf. Syst. E77-D,
1321–1329 (1994)
36. Mendonça, F.: A Promoção de Destinos Turísticos na Internet – O Algarve e os seus
Concorrentes – Uma análise comparativa. Dissertação de Mestrado em Gestão de Sistema de
Informação, Universidade de Évora: Évora, (2002)
37. Neuhofer, B., Buhalis, D., Ladkin, A.: Conceptualising technology enhanced destination
experiences. J. Destination Mark. Manage. 1(1), 36–46 (2012)
38. OMT: E-Business for Tourism: Practical Guidelines for Tourism Destinations and business.
OMT, Madrid (2001)
39. Prebensen, N., Vitterson, J., Dahl, T.: Value co-creation signiﬁcance of tourist resources.
Ann. Tour. Res. 42, 240–261 (2013)
40. Ramos, C., Rodrigues, P.M., Perna, F.: Sistemas e Tecnologias de Informação no Setor do
Turismo. J. Tour. Dev. 12, 21–32 (2009)
41. Ramos, C.M.: Sistemas de informação para a gestão turística. Tour. Manage. Stud. 6, 107–
116 (2010)
42. Romão, J., Leeuwen, E., Neuts, B., Nijkamp, P.: Tourist loyalty and urban e-services: a
comparison of behavioral impacts in Leipzig and Amsterdam. J. Urban Technol. 22(2),
85–101 (2015)
43. Sant’anna, A., Jardim, G.: Turismo on-line: oportunidades e desaﬁos em um novo cenário
proﬁssional. Observatório de Inovação do Turismo. Revista Acadêmica 2(3) (2007)
44. Serrano, A., Gonçalves, F., Neto, P.: Cidades e Territórios do Conhecimento – Um novo
referencial para a competitividade. Edições Sílabo, Lisboa (2005)
45. Tan, L., Wang, N.: Future internet: the Internet of Things. In: 3rd International Conference
on Advanced Computer Theory and Engineering, vol. 5, pp. 376–380 (2010)
46. Tussyadiah, L., Fesenmaier, D.: Mediating tourist experiences access to places via shared
videos. Ann. J. Res. 36(1), 24–40 (2009)
47. Vector21.Com: A Hotelaria Portuguesa na Internet – 2º Relatório Portugal Insite/Plano21.
Com (2008). http://www.vector21.com/pd/estudosmercado/[3-11-2008]
48. Vicentin, I., Hoppen, N.: Tecnologia aplicada aos negócios de Turismo no Brasil. Turismo
Visão e Ação, 4, 11ª edn, pp. 79–105 (2002)
49. Vicentini, A., Ferreira, G., Lorenzi, F., Augustin, I.: Arquitetura de um sistema de
informação pervasivo para auxílio às atividades clínicas. Revista Brasileira de Computação
Aplicada. Passo Fundo 2(2), 69–80 (2010)
50. Vieira, M., Figueiredo, J., Liberatti, G., Viebrantz, A.: Bancos de Dados NoSQL: Conceitos,
Ferramentas, Linguagens e Estudos de Casos no Contexto de Big Data (2012). http://data.
ime.usp.br/sbbd2012/artigos/pdfs/sbbd_min_01.pdf
51. Zikopoulos, P.C., Eaton, C., Zikopoulos, P.: Understanding Big Data: Analytics for
enterprise Class Hadoop and Streaming Data. McGraw-Hill Professional (2011)
94
P. Liberato et al.

A Fuzzy Classiﬁer-Based Penetration Testing
for Web Applications
J. K. Alhassan1(&), Sanjay Misra2, A. Umar1, Rytis Maskeliūnas3,
Robertas Damaševičius3, and Adewole Adewumi2
1 Federal University of Technology, Minna, Nigeria
jkalhassan@futminna.edu.ng,
umarabdulkadir@hotmail.com
2 Covenant University, Ota, Nigeria
{sanjay.misra,
adewole.adewumi}@covenantuniversity.edu.ng
3 Kaunas University of Technology, Kaunas, Lithuania
{rytis.maskeliunas,robertas.damasevicius}@ktu.lt
Abstract. The biggest challenge of Web application is the inestimable losses
arising from security ﬂaws. Two approaches were advanced by a number of
scholars to provide security to Web space. One of such approach is vulnerability
assessment, which is a conscious effort to isolate, identify and recognize
potentials vulnerabilities exploited by attackers. The second being the estimation
and determination of level of risks/threats posed to Web applications by vul-
nerabilities obvious to the developer (or tester); this is generally referred to as
penetration testing. Recently, there is Vulnerability Assessment and Penetration
Testing (VAPT) that combined these two schemes to improve safety and effec-
tively combat the menace of attackers on Web applications. This paper proposed
Fuzzy Classiﬁer-based Vulnerability and Assessment Testing (FCVAPT) model
to provide security for sensitive data/information in Web applications. Cross Site
Scripting (XSS) and Structured Query Language (SQL) injections were selected
for evaluation of proposed FCVAPT model. FCVAPT model’s classiﬁcation
performance for MSE, MAPE and RMSE were 33.33, 14.81% and 5.77%
respectively. FCVAPT is considerably effective for detecting vulnerability and
ascertaining the nature of threats/risks available to Web applications.
Keywords: Vulnerabilities assessment  Penetration testing
Fuzzy classiﬁer-based  Web applications
1
Introduction
The number of active users of the Internet and Web applications is increasing over the
years [1]. Web applications and services interface with the Internet resulting in higher
degree of security risks. Consequently, the Web application server cannot be ignored,
because, Web applications are completely open to global audience. Presently, there
exist several approaches for managing security risk for Web applications such as
ﬁrewall (hardening), defensive coding, monitoring and auditing [2]. Recently, majority
of vulnerability assessment methods make use of classiﬁcation techniques including
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_10

artiﬁcial neural networks, fuzzy control systems and expert systems. In practice, ethical
hackers or security experts across the globe usually examine security breaches and
possible loopholes by means of Vulnerability Assessment and Penetration Testing
(VAPT). VAPT provides an effective means of securing cyber assets [3].
It is believed that, there are over 72 million hosts on the Internet (www.isc.org/ds).
By extension, these potentially mean that there could be billions of people within the
network neighborhood making the security of network and web application users very
difﬁcult to be guarantee. This is due to the vastness of all sorts of people involved. An
attacker needs not to be smart or skillful in order to carry out an attack on a speciﬁc
target. Majority of bigger organizations and users of common computer networks
focused investment through direct acquisition of appropriate security methods for
computers and network systems.
The study adopted fuzzy classiﬁer in order to categorize the presence of attack.
Since fuzzy allow one to work in an uncertain and confusing situation or even an
incomplete problem. Although, there are two types of fuzzy framework systems, the
study adopted fuzzy inference rule as a platform for its rule base. This research
undertook penetration testing of a certain Web application.
2
Related Literature
The use of Web application has increased over times as more services became accessible
on the web. Day-by-day, there are more businesses adopting Web applications as means
of carrying out transactions. Consequent upon this, the number of attacks on web
applications increased tremendously. Studies have shown that the web applications
continue to be the main object of attackers, which impacts on data, reputation or
ﬁnancial losses. But, many kinds of countermeasures are available to safeguard systems
against attacks such as Intrusion detection system, ﬁrewall, and defensive coding [1].
2.1
Computer Vulnerability Analysis
A vulnerability analysis is the process of identifying and quantifying vulnerabilities in
an environment. It is an in-depth evaluation of organization’s posture, indicating
weaknesses as well as providing the appropriate mitigation procedures required to
either eliminate those weaknesses or reduce them to an acceptable level of risk [4].
Once a network is secured by fully patching it and deploying antivirus solutions,
hackers might still be able to exploit a number of misconﬁgurations.
Some vulnerability scanners look for signs of known malware based on the com-
puter’s behaviour rather than actually scanning the ﬁles for known malware signatures.
In some cases, this approach can help uncover issues that an antivirus might miss,
especially if that malware is being protected by a rootkit. There are vulnerabilities
caused by software. Some web services contain known exploits that allow a malicious
attacker to use that script as a gateway to send emails, potentially using an organization
to launch spam runs; SQL injection exploits might allow an attacker to get hold of
usernames and passwords, or inserting his own username, or even to run code remo-
tely. Likewise, the use of applications with known vulnerabilities can open an
96
J. K. Alhassan et al.

organization to targeted attacks. Malicious hackers might try to send people malicious
payloads targeted at these vulnerable applications that, when triggered, would run the
code the hacker would have embedded in the payload sent.
2.2
Web Vulnerability Assessment
[5] developed a Pixy, which is tool providing static analysis proﬁciencies for the
purpose of detecting weaknesses in Web applications. Pixy is developed using the
model of open source to enable it discover cross-site scripting weaknesses in PHP
scripts. The PHP is prominent for building Web applications thereby causing serious
concerns to security consultants. Data ﬂow analysis concerned by experts as means of
isolating vulnerabilities. Pixy and six other open sources Hypertext Preprocessor
(PHP) solution were used to evaluate vulnerabilities in Web applications. PhpMyAd-
min, PhpNuke and Gallery, 36 renowned susceptibilities were remodelled with 27
False Positives. In Simple PHP Blog, Serendipity and Yapig, 15 unacquainted vul-
nerabilities were identiﬁed as having 16 False Positives. However, the Pixy is inef-
fective for object orientated cases.
[6] developed model called Tainted Mode, which is the prominent input validation
for detecting internal vulnerabilities. The author implemented the internal data ﬂows
assessment using classical tainted mode model. The information obtained from
dynamic analysis is realized through computerized penetration test.
[7] introduced a scanner to discover injection weaknesses. This system conduct
searches on websites in order to automatically identify XSS and SQL injection vul-
nerabilities. The two key components of this system are scanner and spider. Spider
traverses the site and search for input points. Scanner begins with injection test and
response exploration, which is made up of response analyst and rules author. The
author uses VMware work station ACE comprising two hosts for the Web server and
the defence server. The system was developed using PHP5 and MySQL, while, the
attacks were executed by cURL module.
[8] proposed state violation attacks detection using WebScrab tool called BLOCK,
which is a typical stateless application system. Also, the application behaviour model is
attained from the client and application interactions (that is, the associations between
responses, Web requests and session variables). BLOCK scheme uses two key aspects
for identifying state violation based attacks. The training stage models the preferred
behaviour through observation of structure of Web request/response, and the variable
values corresponding to the session attacks free execution. Identiﬁcation stage uses the
attained model to test each inbound Web request and outbound response, and violation
identiﬁed is noted.
[9] worked security challenges of Web services because of its distributed and open
nature. In general, Web services are prone to cross-site scripting (XSS) attacks by
exploiting vulnerabilities. The technique was developed based on fault injection and
penetration testing to act out XSS attack across Web services. The author combines
with Security Tokens and Web Service Security to effectually identify the sender and
thereafter; offer access control authorization to the SOAP messages exchanged. The
author conducted the penetration testing using soapUI vulnerability scanner tool, which
A Fuzzy Classiﬁer-Based Penetration Testing for Web Applications
97

involves analysis of the behaviour in a speciﬁc situation for new errors or faults on
Web services with WSInject.
[10] introduced an automatic black-box tool for recognize reﬂected XSS and
keeping XSS weaknesses in Web applications. The tool depends on interactions of
users to conduct its test more efﬁciently. Firstly, the records of user interactions are
made. Thereafter, changes are effected on these interactions in case of attacks. Then,
the entire transaction is started all over again on the system. The performance is
evaluated and compared with Brup Spider, Spider, Acunetix and w3af on three
applications of the Django framework from several setups. The outcomes reveal that
the method can recognize more bugs than the listed open source and commercial tools.
2.3
Web Penetration Testing
Penetration testing is a notable practice for assessing a computer system security. At the
start of 1970s, the Department of Defence (DoD) for the ﬁrst time used the technique to
demonstrate the security vulnerabilities in computer systems; thereafter it begun a
project to develop programs to mitigate the exploitation of identiﬁed weaknesses and
make systems more safe. On the whole, penetration tests are carried out by institutions
in order to ensure the safety of services and information systems but isolate recognized
security vulnerability to guard against dubious exploitation by the users [11].
Large data services corporations are generally worried about guarding information
and sensitive data. The penetration test runs checks on the security schemes available in
companies by means of numerous attacks simulations. The outcomes of this process is
to determine areas of fresh infrastructure, application of software updates, installation
of software, modiﬁcation of user policies and application of security patches. The
beneﬁts of utilizing penetration testing include: prioritization of security risks, infor-
mation protection, Financial Loss and security challenges [12].
Penetration test is a process of examining the security of computer or network
systems through simulation of professional hacker attacks. The operations of hacker
and penetration examiner are similar in many ways except that penetration testing is
achieved with a certiﬁed professional undertaking a deal with an enterprise. The out-
comes are presented as published and circulated reports. The aim of penetration test is
to enhance data and information security. The reports of penetration testing comprise
series of security information and weaknesses which are conﬁdential and undisclosed
to unauthorized individuals until all defects are corrected completely.
3
Methodology
The proposed system architecture comprises the test bed for antecedents, rule applier
and consequents, and the checker as shown in Figure 1. The overall structure of the
Web application vulnerability model is broken into three main components namely: the
input, the FCVAPT Processing (or Test Bed) and the output as shown in Figure 1.
The FCVAPT is the most complex component of proposed fuzzy classiﬁer-based
98
J. K. Alhassan et al.

VAPT because the codes, syntax, test bed and datasets are created and stored in order
to assist the security experts and developers to ascertain the correct status of the Web
applications and rate of susceptibility.
In Fig. 1, the input component collects the necessary dataset about known and
unknown susceptibilities available on Web application referred to as the penetration
stage. On the other hand, the VAPT processing or Test Bed is the assessment stage of
the proposed technique which is concerned with the detection, recognition and clas-
siﬁcation of the deﬁned and undeﬁned vulnerabilities of the Web application under
consideration.
3.1
Modeling of Threat Penetration Assessments
The risks/threats are normalized to fall within the range of 0–1 because, classiﬁcation
processes easily understand binary representations of entities. Using a scale of 0–1 for
the rating score in each category in which the score of 0 depicts the least possible
chance with smallest damage, and reverse is the case of rating score of 1 for Web
application. Therefore, risk assignment of Web application is with the range of 0–1 in
that higher values show the severity of risk to the Web application as shown in Eq. 1.
vj ¼ 1
q
X
q
i¼1
ki :
ð1Þ
Fig. 1. Structure of fuzzy classiﬁer-based penetration testing
A Fuzzy Classiﬁer-Based Penetration Testing for Web Applications
99

where,
v is the vector for risk assessment score ratings for web application (such as worst or
good), for j = 1, 2, 3, … q
q is the total number of categories for risk analyzed and assessed,
k is the unique risk assessed and analysed components,
j is the matric of estimation or calculations for web application risk levels,
i is individual risk components measured to give the ﬁnal risk score of the web
application, which are damage, reproducibility, exploitability, users affected and
discoverability, for i = 1, 2, 3, … q.
Upon substitution of the various components described in Eq. 1, which are damage,
reproducibility, exploitability, users affected and discoverability, then, it can be
expressed as given by Eq. 2:
v ¼ 1
q  k1 þ k2 þ k3 þ    þ kq1 þ kq:
ð2Þ
where,
q is the vector of individual assessment and analysis components, for i = 1, 2, 3, … q.
v is vector of scores for low, fair or high rates and 0, 0.5 or 1 respectively.
k is individual vulnerability threats assessed.
3.2
Experimental Setup
The encoded signatures/features of the two selected Web application threats (or ante-
cedents) are used to develop the truth table for expected fuzzy Rules Base as presented
in Table 1.
In Table 1, the maximum number of rules generated is determined by 3-by-3
membership degrees for the two Web application threats, which serve as antecedent A,
antecedent B, and consequent C representing the vulnerability index. The membership
function are built on the basis of three conditions for the antecedents such as Little,
Table 1. The expected fuzzy decision rules indices for the penetration testing
Rule Antecedent A Antecedent B Consequent C
1
1
1
1
2
1
0.5
1
3
1
0
1
4
0.5
1
1
5
0.5
0.5
1
6
0.5
0
0.5
7
0
1
1
8
0
0.5
0.5
9
0
0
0
100
J. K. Alhassan et al.

Much and Biggest. These degrees are arbitrary used to depict different conditions for
the two antecedents, whereas the consequent can be described as Lowest, Cautious and
Highest impact of vulnerability in Web applications understudies.
3.3
Performance Metrics
This paper adopts four metrics in validating the Fuzzy Classiﬁer VAPT model in Web
applications. These are the relative absolute error, mean absolute percentage error,
mean square error and root mean square errors performance given by Eqs. 3, 4, 5 and 6.
The Relative Absolute Error (RAE) estimates the rate of incorrectness or deviation
of penetration testing value as compared to expected value scores given by Eq. 3:
RAE ¼ x  ^x
x

  100%
ð3Þ
MSE ¼ 1
n
Xn
1 x  ^x
ð
Þ2
ð4Þ
MAPE ¼ 1
n
Xn
1
x  ^x
j
j
x

 100%
ð
Þ
ð5Þ
RMSE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n
Xn
1 x  ^x
ð
Þ2
r
ð6Þ
where,
x is the actual or expected penetration test outcome,
^x is the predicted penetration test outcome,
n is sample size obtained.
4
Results and Discussion
The test bed is generated from the unique signatures encoded for XSS and SQL
injections, which represented the antecedent for the classiﬁcation machine deployed for
the VAPT experiment setup I. The XSS signatures and their encoding produces the ﬁrst
antecedent (A). Similarly, the SQL injection signatures and their unique encoded
schemes are used to generate the second antecedent (B) stored in the Repository and
utilized by the Checker for the developed VAPT model developed.
The Rules Base signiﬁcant component of the vulnerability assessment and pene-
tration testing which effectively maps the two antecedents to produce logically unique
consequents (that is, assesses threats/risks levels based on identiﬁed Web applications
vulnerabilities. The antecedents were generated from the signatures values for SQL
injections and XSS attacks propagated by malicious programmers or users. The fuzzy
classiﬁer is used to achieve this mapping mechanisms of antecedents and consequent,
AND-logic operations is introduced as shown in Figure 2.
A Fuzzy Classiﬁer-Based Penetration Testing for Web Applications
101

In Fig. 2, the threats/risks recognition pattern is relatively stable for conditions
between [0.60–0.80]. The recognition error rates for model when tested with 30% of
datasets showed stability in the lines of curve for the variable C. The RAE is calculated
to be 25% by means of Eq. 3. This implies about 75% precision for analyzed datasets
during training of Checker.
This study developed a Vulnerability Assessment and Penetration Testing model by
hybridizing fuzzy and intelligent classiﬁers whose outcomes are analyzed in Table 2.
In Table 2, the RAE determines the extent of closeness of observed outcomes to
expected outcomes of the FCVAPT model. This reveals 14.81% deviation of observed
variables from actual variables in the outputs realized in FCVAPT model setup. The
results of other evaluation parameters signiﬁcantly improved for the fuzzy classiﬁer
when deployed for vulnerability assessment and penetration testing. The performance of
FCVAPT model calculated for MSE, MAPE and RMSE are 33.33%, 14.81% and 5.77%
Fig. 2. The 3D representation of the variables A, B and C for VAPT model
Table 2. Fuzzy classiﬁer based models outputs
Evaluation metric (%) FCVAPT
RAE
14.815
MSE
33.333
MAPE
14.815
RMSE
5.774
102
J. K. Alhassan et al.

respectively. The implication of these results is that FCVAPT model is capable of
detecting vulnerability and estimating threats/risks level associated effectively with Web
applications at a particular time.
5
Conclusion
This work identiﬁed two most prevalent threats/risks to Web application, which are
propagated through malicious insertion of SQL and XSS injection codes. These SQL
and XSS injections are discovered to be most dangerous security threats to Web
applications due to their ability to be easily deployed malicious along genuine state-
ments on different components such as system, application, operational, network, and
physical.
This work proposed a FCVAPT model by introducing intelligent learning scheme
known as fuzzy classiﬁer. The model is developed to detect vulnerability in Web
applications and concisely state threat or penetration level for recognized cases. The
outcomes revealed that the RAE 14.81% deviation for established values of variables
considered. Again, FCVAPT model’s classiﬁcation performance for MSE, MAPE and
RMSE were 33.33, 14.81% and 5.77% respectively. These results imply that the model
is effective for recognizing vulnerability and deﬁne the nature of threats/risks available
to Web applications during VAPT.
Acknowledgements. We acknowledge the support and sponsorship provided by Covenant
University through the Centre for Research, Innovation and Discovery (CUCRID).
References
1. Doshi, J., Trivedi, B.: Comparison of vulnerability assessment and penetration testing. Int.
J. Appl. Inf. Syst. 8(6), 51–54 (2015)
2. Ruse, M.E.: Modelling checking techniques for vulnerability analysis of web applications.
Unpublished Ph.D. thesis, Department of Computer Science, Iowa State University, Ames,
USA, pp. 1–90 (2013)
3. Aghariya, T.: Security testing on web application. Unpublished M.Eng. thesis, Department
of Software Engineering, Charles Darwin University, Darwin, Australia, pp. 1–93 (2015)
4. Samant, N.: Automated penetration testing. Unpublished M.Sc. thesis, Department of
Computer Science, San Jose State University, California, USA, pp. 1–69 (2011)
5. Jovanovic, N., Kruegel, K., Kirda, E.: Precise alias analysis for static detection of web
application vulnerabilities. In: Proceedings of Programming Languages and Analysis for
Security, New York, pp. 27–36. ACM press (2006)
6. Petukhov, A., Kozlov, D.: Detecting security vulnerabilities in web applications using
dynamic analysis with penetration testing. Computing Systems Lab, Department of
Computer Science, Moscow State University, pp. 1–120 (2008)
7. Chen, J.M., Wu, C.L.: An automated vulnerability scanner for injection attack based on
injection point. In: Proceedings of International Computer Symposium, pp. 113–118 (2010)
8. Li, X., Xue, Y.: BLOCK: a black-box approach for detection of state violation attacks
towards web applications. In: Proceedings of the 27th Annual Computer Security
Applications Conference, pp. 247–256 (2011)
A Fuzzy Classiﬁer-Based Penetration Testing for Web Applications
103

9. Salas, M.I.P., Martins, E.: Security testing methodology for vulnerabilities detection of XSS
in Web Services and WS-Security. Electron. Notes Theor. Comput. Sci. 302, 133–154
(2014)
10. McAllister, S., Kirda, E., Kruegel, C.: Leveraging user interactions for in-depth testing of
web applications. In: Proceedings of Recent Advances in Intrusion Detection, pp. 191–210.
Springer, Berlin, Heidelberg (2008)
11. Doupe, A.L.: Advanced automated web application vulnerability analysis. Unpublished Ph.
D. thesis, Department of Computer Science, University of California, Santa Barbara, USA,
pp. 1–227 (2014)
12. Shelly, D.A.: Using a web server test bed to analyse the limitations of web application
vulnerability scanners. Unpublished M.Sc. thesis, Department of Computer Engineering,
Virginia Polytechnic Institute and State University, Blacksburg, USA, pp. 1–98 (2010)
104
J. K. Alhassan et al.

Comparative Evaluation of Mobile
Forensic Tools
J. K. Alhassan1(&), R. T. Oguntoye1, Sanjay Misra2,
Adewole Adewumi2, Rytis Maskeliūnas3,
and Robertas Damaševičius3
1 Federal University of Technology, Minna, Nigeria
jkalhassan@futminna.edu.ng
2 Covenant University, Otta, Nigeria
ssopam@gmail.com
3 Kaunas University of Technology, Kaunas, Lithuania
robertas.damasevicius@ktu.lt
Abstract. The rapid rise in the technology today has brought to limelight
mobile devices which are now being used as a tool to commit crime. Therefore,
proper steps need to be ensured for Conﬁdentiality, Integrity, Authenticity and
legal acquisition of any form of digital evidence from the mobile devices. This
study evaluates some mobile forensic tools that were developed mainly for
mobile devices memory and SIM cards. An experiment was designed with ﬁve
android phones with different Operating System. Four tools were used to ﬁnd
out the capability and efﬁciency of the tools when used on the sampled phones.
This would help the forensic investigator to know the type of tools that will be
suitable for each phone to be investigated for acquiring digital evidence. The
evaluation result showed that AccessData FTK imager and Paraben device
seizure performs better than Encase and Mobiledit. The experimental result
shows that, Encase could detect the unallocated space on the mobile deice but
could retrieve an deleted data.
Keywords: Mobile  Mobile phone  Smartphone  Forensics
Digital investigation  Digital evidence
1
Introduction
Currently, one of the major tools in this world is mobile devices with high storage
capability that allows mobile device to store huge amount of data, which includes a rich
set of personally identiﬁable data [1]. They have numerous functions and they contain
sensitive personal information. The rates of crime committed by mobile devices are
increasing daily, and there is a need to have evidence of such in the court of law. In
acquiring such evidence, authenticity, integrity and consistency of such evidence must be
taken care of in the process of acquisition, [2]. Mobile forensic is used to access erased
information from the phone without any alteration and can serve as satisfactory evidence
in the court of law [3]. The sensitive personal information in mobile devices is used by the
criminals with aid of software on such devices like Operating System (OS) for instance
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_11

Android and iOS. Malware are developed to threaten the personal information on such
mobile devices thus, there is a need for mobile forensic to ﬁght such malwares [4]. The
process of recovering of digital evidence from mobile devices is referred to as digital
forensic. This process does not cause alteration to the information nor the content of such
mobile devices, [4]. The use of scientiﬁc technique in ﬁnding, removing, evaluating data
and presentation of evidence that can be used in the court of law is referred to forensics,
[5]. In the case of mobile forensic, it includes the techniques trailed in acquiring, ana-
lyzing, preserving mobile data and reporting Subscriber Identity Module (SIM) cards and
phone memory [6]. The increasing upgrade rate of mobile apps, hardware and operating
systems (OS) has made forensic investigating very complex and highly challenging. In
addition, researchers of mobile forensics has shown tremendous interests in this area [1].
Therefore, this study gave a comparative performance analysis of the most widely used
mobile forensic tools for acquiring erased data from mobile phone. The remaining part of
this paper is organized as follows; literature review, mobile phone evidence guide,
methodology, results and discussion, analysis, and conclusion.
2
Literature Review
The use of systematic procedures in identifying, analyzing, interpreting, documenting
and presenting digital evidence acquired from digital source to carefully plan the events
in a criminal offence is known as Digital Forensic Investigation [7]. However, the
changes in mobile phones hardware and operating system are due to the difference in
functionality of the product designed by the developers. Hence, an effective forensic
investigator must understand the phone operating system and hardware in order to
develop an efﬁcient and compatible tool [8].
[9] presented a comparative survey for android forensic tools, the paper analyzed
different tools and techniques used in android forensic and concluded that forensic tools
such as OYGEN are enriced with several features and device supports. [10] performed
a comparative analysis for the different commercial mobile device forensic tools with
open source mobile forensic tools using the cross-device and test-driven approach. The
study concluded that commercial tools are more superior in speed and accuracy during
data extraction and analysis than the open source tools. [11] presented a performance
measurement analysis on Firefox OS for mobile forensic data Acquisition. The analysis
was done on ﬁve existing mobile forensic tools. The study concluded that Mobiledit
detected the Operating System (OS) running on the mobile device and could also only
access pictures from removable memory.
[12] carried out a comparative evaluation on two mobile devices Samsung HTC
(Desire 300) and Galaxy (GT-S5300) using ﬁve trial versions of mobile forensic tools.
In conclusion, Mobiledit and Encase v4.2 provided evidentiary report related to the
SIM card while AccessData FTK Imager could not access any information. [10]
analysed some mobile forensic tools for retrieving evidentiary information from mobile
phones. Two mobile forensic tools were evaluated for reliability and accuracy using
two mobile devices. The evaluation result shows that XRY 5.0 perform better than the
UFED Physical Pro1.1.3.8.
106
J. K. Alhassan et al.

[12] presented a smartphone forensic on Nokia E5-00 mobile phone. The study was
done using four mobile forensic tool on Nokia E5-00 mobile phone. The result shows
that the forensic toolkit could not retrieve the erased evidence from the Nokia E5-00
phone. [13] analyzed a smartphone forensic on a crime using WhatsApp messages.
Two forensic tools were analyzed and the result concluded that WhatsApp message are
not cellular network dependent only but also Wi-Fi or wireless network as well.
2.1
Mobile Phone Evidence Guide
The United States Department of Justice enumerated some mobile phone evidence
which act as a set of rules to the United State Secret service on whether to turn or off a
mobile device when conducting an investigation [13]. The following are a set of rules
guiding the turning on or off of a mobile device:
1. If phone is met “ON”, then do not turn it “TURN OFF”.
2. “TURNING IT OFF” could activate authentication pattern feature.
3. “NOTE” and “PHOTOGRAPH” all information displayed on the screen.
4. If phone is met “TURNED OFF”, do not “TURN ON”.
5. Evidence could be altered or modiﬁed, when device met “OFF” is “TURNED
ON”..
6. “Alert forensic Expert” immediately you get hold of the mobile devices.
7. Call “1-800-LAWBUST” if “No expert is available” they are available 24 h in a
week.
8. Ensure you get the manual of the mobile phones.
In order to acquire proofs/evidence from a mobile phone, some recommendations
must be adhere to however; there are some pitfalls when using such recommendations.
The National Institute of Justice (NIJ) under United States Department of Justice listed
some evidentiary document which includes: Calendars/information, phone book, text
messages, electronic serial number, password, caller identiﬁcation information, voice
mail, e-mail, memos and web browsers [13] as crucial information in the court of law.
In addition, some evidence is considered as miscellaneous such as the mobile phone
cables, cloning equipment, applications on Symbian, mobile linux and windows mobile
phone may also contain information of evidence value that is not included in the
recommendation. Symbian and windows mobile devices are used for executing mal-
ware code such as Trojans and viruses that are transferred through the use Bluetooth
technology. Therefore, it is important that every malicious application present on
mobile phones should be considered as evidentiary value [14–16].
3
Methodology
This section discusses the various materials used for this research study. However,
quite a number of existing researches have outlined methodology to adopt for mobile
forensics investigation. The materials adopted for this research work are divided into
two parts:
Comparative Evaluation of Mobile Forensic Tools
107

1. Hardware devices:
(a) Fly Fly IQ4503, OS 4.4.2 kitkat, processor 1.20 GHz
(b) Three SIM cards: MTN, Airtel and Etisalat
(c) Samsung Galaxy (GT-S5300), OS 2.3.6 Gingerbread, processor 832 MHz
ARM11.
(d) Tecno L3, OS 4.1 Jelly bean, processor 1.0 GHz.
(e) USB Cable.
(f) Tecno phantom A7, OS 4.4.2 kitkat.
(g) HP650 laptop, running on Windows 8, 64 bits OS.
(h) Tecno M7, OS 4.2.2 jelly beans, processor 1.3 GHz dual core
2. Software devices:
(a) Mobiledit Forensic v8.6. (Trial)
(b) AccessData FTK Imager v3.2.0.
(c) Paraben Device Seizure v7.5.
(d) Encase v6.18.1
This study focuses on data acquired from both phone and SIM memories using ﬁve
different operation systems such as Ginger Bread, jelly bean and Kitkat on ﬁve android
phones. Consequently, in this study external memory was not in use, it was removed
from the device.
3.1
Procedure Used for Acquiring Data
For the purpose of this study, ﬁve mobile phones were collected from ﬁve different
users with a newly bought SIMs card inserted to each mobile phone. The ﬁve phones
were formatted to restore factory settings while USB debugging was enabled for device
visibility. The same mobile data evidence was generated on each phones for a ﬁve
days’ duration where the ﬁrst day data was collected on Tecno L3 using ﬁve mobile
forensic tools. Tecno A7 was analyzed on the second day using the mobile forensic
tools to collect erased data while the third day analysis was done on Fly Fly IQ4503.
Tecno M7 was also analyzed on the mobile forensic tools on the fourth day, while
Samsung Galaxy Pocket GS5300 was analyzed on the ﬁfth day using the same data
created on all the ﬁve android phones. However, the generated data were gradually
deleted from the mobile devices. The mobile evidence (data) gathered during these ﬁve
days was transferred via the use of Bluetooth and ﬂash share to the android phones.
Table 1 shows the type of data and number of data that were generated.
Table 1. Mobile data evidence creation.
Data
Number of data generated
Pictures
80
Contacts
80
Videos
35
Audio
50
Message
15
Documents 15
108
J. K. Alhassan et al.

Pictures were generated by using the phone camera to take snapshots while others
data such as Audio and video ﬁles were generated from HTC Incredible S via Flash
share. Contacts were inputted manually into the phone memory and SIM card with text
messages sent from different phones and from each mobile phone. After the experi-
mental environment was completely setup the airplane mode was enabled to avoid any
communications into the mobile phone.
4
Results and Discussion
In this section, evaluation outcome for the different mobile forensic tools when ana-
lyzed on the mobile devices are discussed.
4.1
Mobiledit
The evaluation of the ﬁve mobile devices on Mobiledit forensic tool gave extremely
important information from the SIM card and the mobile phone memory. Examples of
information acquired from each mobile phone include International Mobile Equipment
Identiﬁcation (IMEI) number, International Mobile Subscriber Identity (IMSI) for each
registered SIMs and Integrated Circuit Card ID (ICCID) or SIM Serial Number for
enrolled SIM cards. A typical example of acquired information collected from exam-
ining Tecno L3 is shown on Table 2.
Mobile evidence (data) analysed in this study on the mobile forensic tools include:
Contacts, pictures, SMS, audio ﬁles, video ﬁles and documents. This evidence was
gradually deleted from the mobile phones and the investigation result shows that
Mobiledit could not retrieve the erased evidence from mobile the phone, therefore, it
can be said that Mobiledit tool can only be useful to forensic investigator for backing
up information before evidence is being tampered with. Tables 3 and 4 shows the
evaluation result for Tecno L3 and Tecno phantom A7 respectively.
Table 3 and 4 shows the evaluation result of Tecno L3 and Tecno phantom A7 on
the four different mobile forensic tools. It can be deduced from the table that Mobiledit
and Encase could not retrieve any of the erased evidence from the mobile device while
Access FTK Imager and Paraben device Seizure was able to retrieve some erased
evidence such as pictures, audio, video and document.
Table 2. Information acquired on Tecno L3 mobile phone
Type of
Phone
IMSI
ICCID
IMEI
USB port
Techno L3
621300107418186 89234010002214161948 861350022780164 8B56F7AC
Comparative Evaluation of Mobile Forensic Tools
109

4.2
AccessData FTK Imager
From the experiment, AccessData FTK imager was able to detect, retrieve, analyze,
report ﬁndings and save digital evidence (data) from phone memory for court vali-
dation. On contrary, this mobile forensic tool could not detect or retrieve data from the
SIM card. Figure 1 shows the Interface of the captured Disk Image.
Figure 1 shows that AccessData FTK Imager was able to detect unallocated and
slack spaces and could also retrieve all erased data on the phone memory with retrieved
data starred to show the duration and date when such data was erased from the phone
memory.
4.3
Encase Forensic
In this study, the Encase mobile forensics tool was only able to access the mobile
device that is the phone memory and not the SIM card memory. Therefore, information
about the SIM card could not be detected using this mobile forensic tool. In addition,
the unallocated space on the phone device was detected but could not retrieve any
deleted data.
Table 3. Performance evaluation result of Tecno L3
Mobile data evidence Encase Mobiledit Paraben Device Seizure Access FTK Imager
Unallocated Space
✓
✓
✓
✓
Audio
✓
✓
Pictures
✓
✓
Video
✓
✓
Document
✓
✓
Contacts
Messages(SMS)
Table 4. Performance Evaluation result for Tecno phantom A7
Mobile data evidence Encase Mobiledit Paraben Device Seizure Access FTK Imager
Unallocated Space
✓
✓
✓
✓
Audio
✓
✓
Pictures
✓
✓
Video
✓
✓
Document
✓
✓
Contacts
Messages(SMS)
110
J. K. Alhassan et al.

4.4
Paraben Device Seizure Result
Paraben device seizure is an effective mobile forensic tool with effective access to
phone memory when connected to an android phone. This tool can access contacts and
other multimedia data that was erased from the mobile device.
Mobiledit provided information about the SIM while AccessData FTK Imager,
Encase and Paraben Device Seizure could not retrieve any of the erased data. This is
because AccessData FTK Imager, Encase and Paraben Device Seizure are used for
obtaining data on the mobile device and not to the SIM memory.
This performance analysis shows that none of the mobile forensic tools could
retrieve erased contacts and messages on the ﬁve different android phones evaluated.
However, three of the mobile forensic tools which include: AccessData, FTK Imager
and Paraben Device Seizure, were capable of effectively retrieving erased data such as
deleted pictures, audios, videos, document, unallocated space and slack space from the
mobile phone.
Based on existing literatures, it can be said that there is no forensic tool which has
the capability of retrieving all the types of data on different categories of mobile device.
Thus, the type of evidence data required from the mobile devices can be determine by
the type of analysis to be adopted, and the appropriate forensic tool(s) needed to carry
out the analysis.
Fig. 1. Interface of the captured disk image
Comparative Evaluation of Mobile Forensic Tools
111

5
Conclusion
In this research study, a comparative analysis of four mobile forensic tools were carried
out on ﬁve android phones using different operating systems. The result evaluated from
this study shows that AccessData FTK Imager and Paraben device Seizure mobile
forensic tools presented a better result than the Encase and Mobiledit. In addition,
AccessData FTK Imager and Paraben could retrieve erased data such as videos, music,
pictures, document from the phone memory but do not have access to the SIM card.
While Encase only indicated that a device was connected no deleted data was retrieved
and Mobiledit only showed the status of the phones and some basic information on the
SIM card such as IMEI, ICCID, IMSI etc.
Therefore, the need for effective and efﬁcient forensic tools for the purpose of
evidentiary data from mobile devices cannot be overemphasized. For a court of law to
successful prosecute a suspect who decided to erase all evidence to a crime committed
from his mobile device, there must be an appropriate and reliable evidence of the
erased data. AccessData FTK Imager and Paraben device Seizure mobile forensic tools
can effectively be used for that purpose.
Acknowledgements. We acknowledge the support and sponsorship provided by Covenant
University through the Centre for Research, Innovation and Discovery (CUCRID).
112
J. K. Alhassan et al.

Appendix: Some of the Graphic Pictures of the Tool
Comparative Evaluation of Mobile Forensic Tools
113

References
1. Azfar, A., Choo, K.K., Liu, L.: International Conference on Multimedia Tools Applcation.
Springer (2016)
2. Lin, C., Peng, C.: Research of digital evidence forensics standard operating procedure with
comparison and analysis based on smart phone. In: Broadband and Wireless Computing
Communication and Application (BWCCA), pp. 386–391. IEEE Xplore, Taiwan (2011)
3. Alghaﬁ, J., Martin, M.: Forensic data acquistion methods for mobile phones. 2012 Inter-
national Conference on Internet Technology and Secured Transactions, pp. 265–269. IEEE,
United Arab. Emirates (2012)
4. Chandran, A.: Investigating and analysing malicious events in Android application. Int.
J. Eng. Sci. Res. Technol., 1462–1467 (2013)
5. Ntantogian, A., Marinakis, X.: Discovering authentication credentials in volatile memory
android mobile devices. Mob. Forensics, 110–117 (2013)
6. Farjamfar, A., Mahmod, U.: A review on mobile device’s digital forensic process models.
Res. J. Appl. Sci. Eng. Technol. 8(3), 358–366 (2014)
7. Lai, Y., Lin, A.: Design and implementation of mobile forensic tool for android smart phone
through cloud computing. In: Design and Implementation of Mobile Forensic Tool, pp. 196–
203 (2011)
8. Kamble, J.: Digital forensic investigation procedure. Int. J. Adv. Res. Sci. Eng. IJARSE 4,
157–168 (2015)
9. Aniar, R.R., Anshul, K.K., Leesha, A.: Anroid phone forensics: tools and techniques. In:
International Conference on Computing, Communication and Automation, ICCCA 2016
(2016)
10. Padmanabhan, R., Lobo, K., Ghelani, M., Sujan, D., Mahesh, S.: Comparative analysis of
commercial and open source mobile device forensic tools. IEEE (2016)
11. Osho, O., Ohida, A.: Comparative evaluation of mobile forensic tools. I. J. Inf. Technol.
Comput. Sci., pp. 74–83 (2016)
12. Yusof, M., Abdullah, D.: Performance measurement for mobile forensic data acquisition in
ﬁrefox OS. Int. J. Cyber-Secur. Digital Forensics (IJCSDF), 130–140 (2014)
13. Kubi, S., Saleem, P.: Evaluation of some tools for extracting e-evidence from mobile
devices. I. J. Inf. Technol. Comput. Sci., 64–73 (2011)
14. Mohtasebi, A., Dehghantanha, G., Broujerdi, H.: Smartphone foresics: a case study with
Nokia E5-00 mobile phone. Int. J. Digital Inf. Wireless Commun. 1(3), 651–655 (2011)
15. Al Mutawa, N., Baggili, I., Marrington, A.: Forensic analysis of social networking
applications on mobile devices. Digital Invest. 9, S24–S33 (2012)
16. Ahmed, R., Dharaskar, R.V.: Mobile forensics: an overview, tools, future trends and
challenges from law enforcement perspective. In: 6th International Conference on
E-Governance,
ICEG,
Emerging
Technologies
in
E-Government,
M-Government,
pp. 312–323 (2008)
114
J. K. Alhassan et al.

Cloud Based Simple Employee Management
Information System: A Model for African
Small and Medium Enterprises
Isaac U. Oduh1(&), Sanjay Misra1, Robertas Damaševičius2,
and Rytis Maskeliūnas2
1 Covenant University, Ota, Nigeria
ufuomaoduh@gmail.com,
sanjay.misra@covenantuniversity.edu.ng
2 Kaunas University of Technology, Kaunas, Lithuania
robertas.damasevicius@ktu.lt
Abstract. Cloud computing is gradually becoming accepted in different sectors
and businesses are beginning to adopt the concept’s shared infrastructure and
applications. The aim of this paper is to design and implement a simple pro-
totype of a cloud based application for SMEs to manage their human resources
challenges. The choice of an Employee Information Management System
(EIMS) is due to the fact that one of the basic challenges of SMEs is human
resource management and how to effectively manage employee information.
The prototype developed in this study is targeted at the Software as a Service
(SaaS) layer of the cloud framework. It leverages on existing cloud platform
providers to deliver four core modules, which include: payroll management,
record management, leave management and staff appraisal. Among the things
this study seeks to achieve is a cheaper and cost effective solution to some basic
issues affecting human resource management in SMEs.
Keywords: Management information systems  Small and medium enterprises
Cloud computing  Web applications
1
Introduction
In this modern day of technological advancement, people are beginning to leverage on
cloud computing infrastructure because it delivers on certain features like scalability
and cost-effectiveness. Cloud computing also has some other effective deliverables like
security, inﬁnite storage, low cost and multiple user access on certain resources like
ﬁles and applications on the cloud infrastructure. The cloud-computing concept has
been accumulating a relatively high level of consideration as another computing
standard in the provision of adaptable and on-request services, infrastructures, plat-
forms and cloud based software [1]. Cloud computing brings forth a way through
which information processing can be done through a virtual means [2] and as a rising
technological innovation is a major stride in development and deployment of an
expanding number of appropriated applications [3, 4].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_12

The NIST deﬁnition of cloud computing is a model for empowering omnipresent,
advantageous, on-request access to a mutual pool of conﬁgurable resources that can be
quickly provisioned and discharged with insigniﬁcant administration exertion [5]. It is
also reaching a popularity level among community of cloud users by offering a variety
of resources. Cloud computing platforms [6] for example, those given by Microsoft,
Amazon, Google, IBM and Hewlett Packard, let engineers convey applications
crosswise over PCs facilitated by a focal association. Engineers acquire the upsides of
an organized platform, without committing assets to conﬁguration, constructing and
keeping up of the system [7]. The advantage of CC apart from the obvious and most
fundamental ones being lower costs, provisioning of assets and remote availability. It
brings down cost by evacuating the weight of investing capital consumption by the
organization of renting physical framework from an outsider supplier [8] Thus of
nature of adaptability of distributed computing, organizations can without much of a
stretch get to more assets from cloud suppliers when they are attempting to extend The
remote availability empowers access to cloud administrations from anyplace whenever.
So as to increase most extreme level advantages of these previously mentioned
advantages, the services or assets ought to be ideally designated to the applications
running on the cloud.
As indicated by [4] distributed computing at its essential clariﬁcation is an accu-
mulation of applications and other resources or services accessible from a decentralized
system of servers. Taking a gander at the development of distributed computing, [9]
proposed that the advancement begun amid the 1980s from mind boggling and
expanded underlying foundations of information technology (IT) division. At the
approach of the web and system advancements in the 1990s, customers could connect
up with different customers and servers to trade data and records (reports) and fur-
thermore utilize remote applications [10]. To this end, there was not at present a typical
server for sharing data on a worldwide stage. This prompted improvement of cloud PC
which depicted an IT framework which conveys an arrangement of helpful, on request
and conﬁgurable registering administrations and assets to customers over a system in a
self-beneﬁting nature that was not subject to gadget and area specialized co-op con-
nection. The distribution of cloud computing services is depicted in Fig. 1.
The expression “cloud” has for quite some time been utilized as an allegory for the
web, and there are numerous prevalent applications, which are being used today that
are actually powered by cloud services. Social media applications, online email
Fig. 1. Distribution of cloud computing services
116
I. U. Oduh et al.

applications Gmail etc. and even shared systems like Skype are applications that keep
running on the cloud [6]. Most businesses are looking to stay globally competitive and
thus they are shifting to cloud based infrastructure for all their business applications for
the day-to-day transactions.
Therefore looking at the points above its only reasonable to leverage on this
existing platforms when developing business solutions to solve certain business
problem as it takes the network management burden from the developer and is allowed
to focus primarily on the process involved developing the solutions.
The continuing part of this work is structured in this way: section two looks at an
overview of CC and its reference architecture. Related works are presented in Sect. 3.
Section four discusses the design of the application and section ﬁve contains conclu-
sions with further research directions.
2
Cloud Computing
2.1
Overview
The National institute of Standards and technology (NIST) deﬁnes cloud computing as
“a model for empowering omnipresent, advantageous, on-request access to a mutual
pool of conﬁgurable resources that can be quickly provisioned and discharged with
insigniﬁcant administration exertion” [11]. NIST speciﬁes ﬁve characteristics of cloud
computing which include on request self-service, broad network access, resource
pooling, rapid elasticity, and measured services. The architecture of the cloud is
depicted in Fig. 2.
Fig. 2. Architecture of the cloud
Cloud Based Simple Employee Management Information System
117

2.2
Cloud Computing Architecture
These are:
a. Software as a service (SaaS): this is application conveyance framework, which
gives access to applications through the web as an online service [12].
b. Platform as a Service (PaaS): this constitutes the middle layer where applications are
developed and provide a deployment platform for running software in the cloud [12].
c. Infrastructure as a Service (IaaS): constitutes the most prevalent market portion of
cloud computing. IaaS solutions bring every one of the advantages of hardware
virtualization [14].
2.3
Cloud Computing Deployment Models
These are:
i. Public Cloud: the cloud infrastructure is possessed by the cloud specialist co-op.
The cloud framework exists in the premises of the cloud supplier. Overall pop-
ulation can get to the cloud beneﬁts on a for-every-per-utilize model or technique.
The clients are apportioned the assets in the cloud on-request. The assets are given
on powerful premise over the web. SMEs can proﬁt to awesome degree from
utilizing public clouds [12]. Points of interest incorporate area autonomy,
cost-adequacy, dependability, adaptability etc. [13].
ii. Private Cloud: this model is with the end goal that it is possessed exclusively by
the organization. The organization itself or an outsider can oversee it. The private
cloud can exist on premises or off premises [15].
iii. Hybrid Cloud: Comprise two or more cloud frameworks. Each of them stays as novel
elements however are connected together by institutionalized or restrictive tech-
nology. This innovation empowers information and application compactness [5].
iv. Community Cloud: the cloud framework in a group cloud is shared by a few
organizations which have shared It is by and large overseen by the organizations in
the group or an outsider and can be available either on-premises or off-premises [5].
3
Review of Related Works
This paper centers on developing a management information system for the sole
purpose of managing staff information in SMEs. Management information systems
have been applied to so many domains. Here are some related implementations:
The study in [16] introduced an agent technology into the management of domestic
storage, which leveraged on the self-governance, reactivity of Agent in acknowledging
connection as a technique for enterprises to achieve viable capacity administration.
Study [17] designed a web based VOIP recording management system in order to help
data management issues in monitoring and recording telephone calls and management
of such data. The system comprises of multiple modules including management of
device, channel, call records, etc. [18] developed an intranet based management system
for patient and appointment management. This comprised of a secure graphical
118
I. U. Oduh et al.

interface and process automation facilities. The authors had the following outcomes in
mind undertaking this project, which were user satisfaction, time saving as well as
process automation. Another of such management system was developed in [19], this
time it was a record management for medical images, which was to serve as an easy to
use solution for dermatologists to perform basic storage, retrieval, analysis and col-
laborative sharing of ﬁles and images that describe or show skin lesions. The system
was design to handle other information like patient history and other relevant
meta-data. The system allows the users to store different levels of images together with
metadata ﬁelds for the most relevant skin features. The interface was developed using
responsive web technologies, which makes the system highly accessible on mobile
platforms. Also [20] developed e-health TABLET (Technology Assisted Boards for
Local government unit Efﬁciency and Transparency) that was to coordinate existing
health information systems to achieve the goal of a uniﬁed health information man-
agement system and to likewise make a transparency layer at the local government. The
web based system comprised of an electronic medical record sub-system, which con-
tains
patient
information
module
and
diagnosis
information
module,
the
requests/approval sub-system and a dashboard for data-visualization purposes. [21],
decided to develop a student information management system for a tertiary institution.
This application manages a wide range of information from starting enlistment to
deﬁnite graduation, program information, participation data, funds and examinations.
[22] developed a pediatric electronic health record management system as part of an
ongoing mandate to implement an interoperable national health information network.
This system provides certain capabilities such as scheduling, billing, medical personnel
interviews etc. [23] developed as part of an e-healthcare information system (HIS) a
Radiology Information System using RDBMS technologies comprising of organized,
semi-structure and unstructured information. This was hosted on a cloud platform,
hybrid to be exact which is a combination of both private cloud for storing private
information of patients and public cloud for storing public information. The study in
[24] developed an information system for managing data and resources about theses of
bachelors and masters students, which gives a convenient way of information retrieval.
The study in [25] designed and implemented a management information system for a
client laboratory to assist in collecting, managing and analyzing ABR data for research.
The ﬁnal application was deployed to a cloud platform and this allows scientist to enjoy
attributes of ﬂexibility and reliability in terms of data retrieval and analysis. In [26] an
application called DigiMAPS (Digitization of maternal and post-natal system) was
deployed which aims at providing an information system for health record and other
related information which will help health care stakeholders like hospitals and gov-
ernment ministries in Indonesia to efﬁciently manage administrative processes, and in
collection of clinical information than conventional paper-based reporting, this will in
turn minimize cases of reporting errors. The study in [27] looked at safety practices and
developed a management system to effectively handle safety and emergency operations
by developing a web based information management system to maintain safety
information in labs, workshops and ofﬁces in organizations. The study in [28] devel-
oped an information system that helps in location-based allocation of technicians. This
system helps in prompt response to customer requests. This application allocates based
on proximity of the technician to the customer. The information contained in this
Cloud Based Simple Employee Management Information System
119

system include location data, mobile device numbers, it uses a Google Map API to
effectively see the location in real time of the customer and then the technician is
dispatched to that location. The study in [29] designed and developed an application to
support material ﬂow and management processes in educational institutions. This
application is supported on smart phones to aid in record keeping of school inventory.
This system also accepts the recording of receipts of transactions from suppliers,
loan/return of items and use/discarding of materials. The system has a backend and
central repository database. The study in [30] developed a management information
system to help small businesses to manage warehousing tasks like managing storage,
stocktaking, garage shift and tasks this was due to the fact that market WMS (Ware-
house Management System) software are expensive, complex and have operational
difﬁculty for the small business.
4
Methodology and System Design
This is a software solution hence the SimpleEIMS is integrated into the cloud design
framework by developing it at the SaaS layer. The design framework of the proposed
system uses a layered approach as shown in Fig. 3.
The consumers are devices that will use the SimpleEIMS system. Such devices
include cloud-enabled devices like tablets, smartphones and PCs.
Fig. 3. Cloud Architecture of the SimpleEIMS system
120
I. U. Oduh et al.

The SimpleEIMS is a cloud-based application designed by using web-based
technologies to help SMEs to solve data management issues without having to incur
costs of developing in house applications. The main idea is to design a system that can
be accessed on local systems unique to each organization. It shows a system that
manages all information of staff in an organization such as personal information,
ﬁnance, appraisal systems. It was also aimed to replace paper-based forms with new
electronic management for the organization’s human resource team. The SimpleEIMS
has 4 sub-systems or modules, which are record management, leave management, staff
appraisal and ﬁnancial management modules. The Record Management module: This
module forms the basics of the whole system, in the sense that all other subsystems are
dependent on this particular module. This module is used for storing basic information
like personal bio, departmental information and the information stored here can be used
of advance data mining purposes or advanced search or targeted marketing by other
third party companies although with due permissions. The Leave Management module:
This is a really simple module used to process applications for leave by employees and
the employees can get feedback on their leave application status without the hassle of
having to push paper forms. The Staff Appraisal Module: This module is used in
running periodical assessment of employees and as such can be used as a form of
feedback mechanism for the workforce in the organization. The Finance Module: This
module is primarily concerned with ﬁnancial record management for employees. The
module is used to process monthly payment information.
The Cloud Platform Providers provide language runtimes and data storage services
for their platform. Some popular service providers include Microsoft with their Win-
dows Azure platform and Google with its Google App Engine platform. With Win-
dows Azure, applications can be built using any language, framework or tool
particularly Python, Java, Ruby, Node.js and PHP.
Fig. 4. Database decomposition of Simple EIMS
Cloud Based Simple Employee Management Information System
121

The Cloud Infrastructure provided by the cloud providers like Microsoft and
Google support applications with their respective platforms by cloud infrastructure,
which includes networks servers and storage facilities. The database decomposition of
Simple EIMS is given in Fig. 4.
Figure 5 further provides an abstraction of the classes and associations that take
place in Simple EIMS.
5
Implementation
The system was implemented using web technologies, which include HTML, CSS,
PHP, and JavaScript.
The login page is depicted in Fig. 6. It gives room for role based access comprising
of Admin Login and Staff Login.
Employee
1…...1
Leave Information
1……… 1
Employee Records
Financial Information
1……*n
Financial Records
1 ……. *n
User 
User Records
Account
1 …….. *n
Fig. 5. Classes and association in SimpleEIMS
122
I. U. Oduh et al.

The Administrator Module: This handles all relevant administrator and human
resource tasks in the organization as depicted in Fig. 7.
Staff Record Module: This is used to manage records of employees; it contains
information divided into three categories, which are Personal, Business or Ofﬁcial and
login information. The administrator can perform basic Create, Read, Update and
Fig. 6. Login page for the App
Fig. 7. Administrator screen
Cloud Based Simple Employee Management Information System
123

Delete (CRUD) operations on all of this data. Figure 8 shows the form for adding a
new employee information while Fig. 9 shows the interface for both searching and
viewing existing employee records.
The Financial Module: is tasked with management and processing of staff ﬁnancial
information including salary processing. This module is only accessible to a user with
admin privileges as depicted in Fig. 10.
Fig. 8. Admin/AddEmployee
Fig. 9. Admin/Employee Data
124
I. U. Oduh et al.

The Appraisal Module: This is used to perform regular appraisal of staff. It is a
date-based appraisal, which means appraisals can be updated (Fig. 11).
The Leave Module: This is used to respond to leave requests by staff members. It
keeps track of each employee’s leave requests. This includes: the description, duration
and status as depicted in Fig. 12.
Fig. 10. Admin/Finance Module
Fig. 11. Admin/New Appraisal
Fig. 12. Admin/Leave Module
Cloud Based Simple Employee Management Information System
125

6
Conclusion
The cloud solution to developing a SimpleEIMS (Simple Employee Information
Management System) adopted in this paper brings to bare great beneﬁts to SMEs in
managing Human Resource operations. It will help them to overcome high cost of
developing, deploying and maintaining technology infrastructure within the organi-
zation. Also, the SMEs can easily change their scaling preferences i.e. whether to scale
down or scale up the resources depending on the volume of demand. This platform will
allow the organizations to focus more attention on core activities to boost efﬁciency and
productivity in workﬂow and business operations rather than maintaining servers or
systems. The application needs to be tested, so as part of future work or studies, more
data and scalability issues will be discussed. Also on a larger scale, there is intention by
the authors to develop more prototype enterprise applications at cheaper rates for access
and use for SMEs who wish to explore cloud computing technology paradigm.
A reason to adopt this system is the fact that the modules have been simpliﬁed,
making it easier to be understood by different level of users. Knowing fully well that
the target population is a mixture of educated and semi educated users. In Nigeria there
is really no standard for adopting technological solutions but this work I believe can be
set up or adopted to stand as a guide to develop cloud solutions seeing that it’s a cheap
way to access simple but effective tools for management.
Acknowledgements. We acknowledge the support and sponsorship provided by Covenant
University through the Centre for Research, Innovation and Discovery (CUCRID).
References
1. Srinivasan, A., Quadir, A.M., Vijayakumar, V.: Era of cloud computing: a new insight to
hybrid cloud. Procedia Comput. Sci. 50, 42–51 (2015)
2. Frost and Sullivan, in Market Insight (2011)
3. Li, J., Qiu, M., Jian-Wei, N., Chen, Y., Zhong, M.: Adaptive resource allocation for
preemptable jobs in cloud systems. In: 10th International Conference on Intelligent System
Design and Application, pp. 31–36 (2011)
4. Mohiuddin, A., Abu, S.M., Raju, C., Mustaq, A., Mahmudul, H.R.M.: Advanced survey on
cloud computing and state-of-the-art research issues. IJCSI Int. J. Comput. Sci. Issues 9(1),
201–207 (2012)
5. Mell, P., Grance, T.:The NIST Deﬁnition of Cloud Computing, NIST Special Publication
800-145, Department of Commerce, pp. 2–3 (2011)
6. Pandaba, P., Prafulla, B.K., Ray, B.N.B.: Modiﬁed round robin algorithm for resource
allocation in cloud computing. Procedia Comput. Sci. 85, 878–890 (2016). International
Conference on Computational Modeling and Security (CMS 2016)
7. Hien: Automatic virtual resource management for service hosting platforms. In: Cloud,
pp. 1–8 (2009)
8. Vouk, M.A.: Cloud computing - issues, research and implementations. J. Comput. Inf.
Technol. - CIT 16(4), 235–246 (2008)
9. Alali, A.F., Chia-Lun, Y.: Cloud computing: overview and risk analysis. J. Inf. Syst. 26, 13–33
(2012)
126
I. U. Oduh et al.

10. Talal, H.N., Quan, S.Z., Sherali, Z., Jian, Y.: Trust management of services in cloud
environments: obstacles and solutions. ACM Comput. Surv. 46(1), 12–30 (2013)
11. Rani, K.B., Rani, P.B., Vinaya, B.: Cloud computing and inter-clouds - types, topologies and
research issues. Procedia Comp. Sci. 50, 24–29 (2015)
12. Rajkumar, B., Christian, V., Selvi, T.S.: Mastering Cloud Computing, pp. 112–117.
McGraw Hill, Waltham (2013)
13. Cloud
Computing
Tutorial
-
TutorialsPoint.
https://www.tutorialspoint.com/cloud_
computing/cloud_computing_overview.htm
14. Sosinsky, B.: Cloud Computing Bible, pp. 6–9. Wiley Publishing Inc, Indianapolis (2011)
15. Kelly, K. : A cloud book for the cloud, November 2007. http://kk.org/thetechnium/a-
cloudbook-for/
16. Zhongnan, Y.F.: Development of inventory management system. In: 2010 2nd IEEE
International Conference on Information Management and Engineering, Chengdu, pp. 207–
210 (2010)
17. Gao, F., Gao, Y., Li, M.: Design and implementation of web-based voip recording
management system. In: 2009 First International Conference on Information Science and
Engineering, pp. 2300–2303 (2009)
18. Marinos, S., Nikolopoulos, P., Pavlopoulos, S.: A WEB-based patient record and
appointment management system. In: 1999 IEEE Engineering in Medicine and Biology
21st Annual Conference and the 1999 Annual Fall Meeting of the Biomedical Engineering
Society, Atlanta (1999)
19. Tóth, J., Bartha, L., Szabó, T., Lázár, I., Harangi, B., Hajdu, A.: An online application for
storing, analyzing, and sharing dermatological data. In: 6th IEEE International Conference
on Cognitive Infocommunications (CogInfoCom), pp. 339–342 (2015)
20. Estuar, M.R., et al.: eHealth tablet: a developing country perspective in managing the
development and deployment of a mobile - cloud electronic medical record for local
government units. In: IEEE 15th International Conference on Mobile Data Management,
Brisbane, QLD, pp. 313–316 (2014)
21. Alshareef, A., Alkilany, A., Alweshah, M., Bakar, A.A.: Toward a student information
system for Sebha University, Libya. In: Fifth International Conference on the Innovative
Computing Technology (INTECH 2015), pp. 34–39 (2015)
22. Ginsburg, M.: Pediatric electronic health record interface design: the pedone system. In:
2007 40th Annual Hawaii International Conference on System Sciences, HICSS 2007,
p. 139 (2007)
23. Weider, D.Y., Kollipara, M., Penmetsa, R., Elliadka, S.: A distributed storage solution for
cloud based e-Healthcare Information System. In: IEEE 15th International Conference on
e-Health Networking, Applications and Services (Healthcom 2013), pp. 476–480 (2013)
24. Wisnicka, J., Wojtowski, M., Slusarczyk, K.: Web based system for archive and
management theses. In: Proceedings of the International Conference Mixed Design of
Integrated Circuits and System, pp. 801–803 (2006)
25. Chaczko, Z., Kirkpatrick, S., Braun, R.: A web-based solution to collect, manage and
analyse Auditory Brainstem Response data. In: 12th International Conference on Informa-
tion Technology Based Higher Education and Training (ITHET), pp. 1–4 (2013)
26. Faried, A., et al: Mother and children health reporting system: innovative information system
application in the rural West Bandung Area, Indonesia, by using multimodal communica-
tions systems. In: 4th International Conference on Instrumentation, Communications,
Information Technology, and Biomedical Engineering, pp. 202–207 (2015)
27. Sarode, D.M., et al.: A Web based workplace layout and material information system for
safety management. In: 2nd International Conference on Reliability, Safety and Hazard -
Risk-Based Technologies and Physics-of-Failure Methods (ICRESH), pp. 529–534 (2010)
Cloud Based Simple Employee Management Information System
127

28. Delman, X., Shibeshi, Z., Scott, M.: Development of a location based service for technician
allocation. In: IST-Africa Week Conference, pp. 1–8 (2016)
29. Vladimirou, A., Kokkinaki, A.: Design and development of school assets management
system. In: 36th International Convention on Information and Communication Technology,
Electronics and Microelectronics (MIPRO), pp. 587–590 (2013)
30. Zhou, C., Fei, Q.: Warehouse management system development base on open source web
framework. In: International Conference on Industrial Informatics - Computing Technology,
Intelligent Technology, Industrial Information Integration, pp. 65–68 (2016)
128
I. U. Oduh et al.

Inexpensive Marketing Tools for SMEs
José Avelino Vitor1,2, Teresa Guarda3,4,5(&),
Maria Fernanda Augusto3, Marcelo Leon3,4, Datzania Villao4,
Luis Mazon4, and Yovany Salazar Estrada6
1 Instituto Universitário da Maia, Maia, Portugal
javemor@iiporto.com
2 Instituto Politécnico da Maia, Maia, Portugal
3 Universidad de las Fuerzas Armadas-ESPE, Sangolqui, Quito, Ecuador
tguarda@gmail.com, mfg.augusto@gmail.com,
marceloleon11@hotmail.com
4 Universidad Estatal Península de Santa Elena – UPSE, La Libertad, Ecuador
datzaniavillao@gmail.com, luismazon86@gmail.com
5 Algoritmi Centre, Minho University, Braga, Portugal
6 Universidad Nacional de Loja, Loja, Ecuador
ysalazarec2002@yahoo.es
Abstract. Today small and medium-sized enterprises (SMEs) play a key role in
the economy and are considered the engines of global economic growth. In
today’s environment of mature economies, stagnant markets and ﬁerce com-
petition, consumers are increasingly informed and demanding personalized
treatment and products and services that meet their needs. In this context, SMEs
can remain in the market, and maintain a competitive advantage, if they are able
to respond to customers’ needs in a timely manner. That is possible if supported
by the appropriate information systems and information technologies. Actually,
many SMEs are far from accessing all the available data, because they have
neither the knowledge nor ﬁnancial capacity to acquire tools that allow you to
extract knowledge from your internal and external databases. However, is
possible by combining a database that provides behavioral information from
your prospects and combining that data with the spatial information of those
customers. This joint allows a comprehensive analysis that is possible through
the use of segmentations techniques, which supports marketing campaigns in an
effective way, promoting visibility in the market, and allowing acquiring or
maintaining a strategic positioning, using inexpensive tools.
Keywords: Competitive advantage  Database marketing  GeoMarketing
RFM model  Costumer segmentation
1
Introduction
Small and medium sized enterprises can gain a competitive advantage and create sus-
tainable business by adopting information technologies (IT) and information systems.
Sales and marketing will only be successful if supported by good databases (DB). IT
properly harnessed, can be competitive advantage of a company over its competitors.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_13

Companies need to develop competitive advantages based on an adequate use of IT,
which are an essential element of success in today’s competitive global market. This is
fundamental for SMEs, whose survival depends on the adequate use of IT.
Marketing decisions, which aim to deﬁne the best strategic plans to address the
market, choosing the best advertising campaign, select the segment and the type of
product to offer, should and must result from an analysis of information and data
available. We can say that the basic principles of marketing are applicable to large and
small businesses. Marketing in small businesses can be categorized as: culture (analysis
of consumer needs); strategy (development to enhance actual and potential market
position); and tactics (analysis of the 4Ps - Product, Price, Place, Promotion - to
inﬂuence the performance and growth) [1]. Some authors have referred the inability of
SMEs to make strategic marketing decisions [2, 3], because of the way an owner
\manager acts, they make most decisions on their own, and respond to current oppor-
tunities, so decision making occurs according to personal and business priorities at any
given time.
At the moment it is fundamental for any company to know in detail its action
market. However, this is only possible by combining the database\Database Marketing
(DBM), that provides behavioral information from your prospects and combining that
data with the spatial information of those customers. This joint allows a much more
comprehensive analysis and is made possible through the segmentation techniques.
The competitiveness resulting from globalization and also the evolution of infor-
mation technology has scariﬁed the market view, since the extent of competition is no
longer limited by the geographical space of a city, state or country, but rather has
competitors for the whole world to a click away. To survive in the global market, with
an increasingly and aggressive competitiveness, focusing on the customer is becoming
a key factor for SME’s, and customer retention is very important because of their
limited resources.
This paper is organized as follows: after this introductory part is presented the
literature review, based in Database Marketing and customers segmentation using the
RFM model and GeoMarketing (GM). In the 3th section, is combining the DBM that
provides behavioral information from prospects; with GM, providing data with the
spatial information of customers; in order to provide a comprehensive analysis through
the use of segmentations techniques, supporting marketing campaigns in an effective
way, promoting visibility in the market, and allowing acquiring or maintaining a
strategic positioning, using inexpensive tools. In the last section, we draw some
conclusions.
2
Background
Over the last decades, organizations have increasingly come to trust on technology to
support communication and information processing in almost all areas of their oper-
ations. Marketers and others related to marketing function have been seeking the best
way to introduce such information and communication technology successfully into
their business.
130
J. A. Vitor et al.

The contemporary marketing practices framework [4] and the resulting empirical
research ﬁndings highlight the information and communication technology challenges
for marketers during the 1990s and until today [5].
Organizations should to develop their strategies in order to gain competitive
advantage over their competitors. Competitive advantage can be understood as seeking
unique opportunities that will give the company a strong competitive position. The
markets become more competitive and many companies understand and recognize the
importance of retaining current customers. The beneﬁts associated with customer
loyalty are widely recognized within business. Singh [6] suggest that customer loyalty
is rapidly becoming, “the marketplace currency of the twenty-ﬁrst century”. This is a
commonly held view in the academic ﬁeld, which supports the need for strategist’s
businessmen and marketers to adopt a customer-centric vision [7, 8].
2.1
Database Marketing
The Database Marketing (DBM) is an essential part of marketing in most organizations.
The basis of the DBM is that at least part of the communication of organizations with
their customers is direct [9]. We can say that the DBM is normally covered by classical
statistical inference, which may fail when data are complex, multidimensional, and
incomplete.
Nowadays database marketing approach is differentiated by the fact that much more
data is maintained in databases, and that the data are used in more sophisticated ways.
There are various deﬁnitions of DBM, with different approaches or perspectives,
showing some improvement over the concepts [10]. In marketing perspective, the
DBM is an interactive approach to marketing communications, which uses addressable
communication media, such as telephone, mail and Internet [11], or DBM is a strategy
that is based on the premise that not all customers or prospects are equal, and the
collection, maintenance and analysis of detailed information about customers and
prospects, marketers can modify their marketing strategies.
In a simple way, the DBM involves gathering information about past, current and
potential customers, to build a database that improve the marketing effort. The infor-
mation includes: demographics, what the consumer likes and dislikes, tastes, pur-
chasing behavior and lifestyle [12].
With the advancement of information technology, in terms of processing speed, and
in terms of storage space, the ﬂow of data in organizations has grown exponentially,
suggesting different approaches to the DBM. Generally, it is the art of using the data
collected, to create new ideas to make money [13], or add other costumer information
in a database (lifestyle, transaction history, and others), and use these information as the
basis for customer loyalty programs to facilitate contacts and to enable future marketing
planning, [14]. The DBM can be set to collect, store and use the maximum of useful
knowledge about customers and prospects, to their beneﬁt and proﬁt.
DBM is a marketing tool oriented to databases, increasingly the focus of the
strategies of the organizations [15]. All deﬁnitions have in common a main idea, the
DBM is the process that uses the data stored in database marketing, in order to extract
relevant information to support marketing decisions and activities by understanding
customers, which will satisfy their needs and anticipate their desires.
Inexpensive Marketing Tools for SMEs
131

2.2
RFM Model
There are several direct marketing response models using consumer data, among them,
one of the classic models, known as RFM model, this model identify customer
behavior [16], determining the probability of consumers responding to a direct mar-
keting promotion based on the recency of the last purchase, the frequency of purchases
over the past years, and the monetary value of a customer’s purchase history [17], and
it’s a good model to SME’s.
Peter Hardie et al., presented a stochastic model to estimate the customer life time
value, using as explanatory the RFM variables. Colombo introduced a simple
stochastic model based on RFM to respond to what customers should a organization
focus to make a product offering [18]. Both studies have in common the same moti-
vational principle: customer behavioral measures are key indicators to predict future
behavior of these [19].
The RFM model is a model used to analyze and predict customer behavior [20].
RFM model is the most frequently adopted segmentation technique, focuses on the
three behavioral variables of recency, frequency, and monetary value, and is used to
segment customers using information related to recency, frequency, and monetary
value (which are combined into a three digit RFM code, covering ﬁve equal quintiles).
Recency represents the time period since the last purchase; frequency is the number of
purchases in a given period of time; while monetary is the amount of money spent in
this time period [21]. These variables can be used to segmenting customer’s behavior
from databases. These three variables are considered powerful predictors of future
behavior and form the basis of database marketing. Recency enables the prediction of
future value, while frequency and monetary value enable the estimation of the current
value. The combination of these three dimensions (RFM) allows combined analysis of
current and future customer value. The higher the RFM score, more probable it is for a
customer to respond to a marketing action.
Companies can maximize the return of campaigns and minimize marketing costs if
they know to which customers should send promotions. These customers can therefore
be considered of greater value to the company because their past behavior indicates a
positive intention to maintain these relationships.
The RFM model, allows the quantiﬁcation of customer behavior through the
development of a quantitative framework and allocating customers, of behavioral
patterns. It is through this attribution of behavioral patterns and subsequent grouping
segments. It is possible to perform an economic feasibility analysis at the level of future
promotional investments [24]. Companies use RFM analysis to determine whether and
how to invest in their direct marketing customers [25]. The RFM model only works
with DB\DBM of existing customers. The basis for its operation will be the purchase
history of each customer. The analysis of each variable of the RFM method is done
separately. In the case of recency, consumers who have bought recently are more
probability to respond to a new offer than someone who has bought long time ago.
Frequency in different types of business may use other frequencies, as we can use the
average number of purchases per year, the number of calls made per month in the case
of a telephone operator. The frequency represents the number of iterations between the
customer and the company in a given period. There are two possible approaches:
132
J. A. Vitor et al.

the exclusion of all clients who began to be outside the period and the weighted average
period of time that is as customer. For the case of monetary value, the database before
the data are separated into quintiles must be ordered by the total value of purchases per
year, per month or more depending on the timeline indicated for the business. Then
ordered quintiles from 5 (spend more) to 1 (spend less).
Thereafter, variables are grouped together and used to segment DBM in RFM cells.
If we choose for each RFM attribute, 5 classes, with the numbers 1 to 5, we obtain 125
different classiﬁcations. Thus the 555 customer is a very recent customer, very frequent
and with a high volume of purchases, whereas a customer 111 is little recent, infrequent
and with low volume of purchases. There is a possibility of hierarchizing the classi-
ﬁcations, following the valuation that is common to give the integers.
The higher the RFM score, the more proﬁtable the customer is to the business both
now and in future. High RFM customers and probably are most likely to continue to
purchase and visit, and to respond to marketing promotions. Low RFM score cus-
tomers, are the least likely to respond to promotions. No doubt, those high RFM
customers represent future business potential, because the customers are willing and
interested in doing business with you. With RFM we can decide who to promote to and
predict the response rate and increase customer loyalty and proﬁtability.
The RFM model revealed interesting customer segments that could be targeted
using appropriately designed marketing campaigns. We believe that model will provide
signiﬁcant business value. This model is known and appreciated for its simplicity, since
it can be used without requires specialized statistical software, and also their results are
easily understood by users.
2.3
GeoMarketing
GeoMarketing is a marketing approach that comes from joining the disciplines of
marketing with geography. GM brings a new dimension, the spatial dimension, to the
study and analysis of the socioeconomic, behavioral, demographic and statistical
phenomena of a given market, phenomena that have always been present in studies and
marketing strategies [22].
GM is the application of Geographic Information Systems (GIS) to the ﬁeld of
business. In general terms, it is the integration of geographical concepts into the
marketing environment, such as areas of operation, distribution and location of sales
points. GM is a branch of geography coupled with concepts of marketplace, based on
the processing of geographic information that allows the concatenation, organization
and manipulation of data referring to users and research from a geographic point of
view [23].
GM combines several variables, such as social information (such as age, sex and
level of education), economic information (such as wage level and market potential),
geography and plants). This combination allows a better understanding of the reality of
the market and companies can take full advantage of this situation as several layers of
customer information in a given geographical area allow access to a greater knowledge
of their current customers and potential. Thus, it is possible to verify the inﬂuence of a
certain location on the competitors, consumption activities and on the variables of the
marketing-mix. Geography has the function of giving the coordinates to the marketing,
Inexpensive Marketing Tools for SMEs
133

coordinates that allow a company to know better its market, namely who buys and
where it buys, how often it does, among others. Thus, as each market segment is
identiﬁed and delimited on the map, it is also identiﬁed which sites that have the
greatest potential of consumption are the service or product in question.
The applicability and importance of GM as a business tool are relevant. In fact, it
has an enormous breadth of application and is an important component of the mar-
keting strategy of companies in different sectors (food, transport, banking, insurance,
hotels, among others).
To truly be used as a methodology to support decision making, GM must incor-
porate into spatial analysis several qualitative variables derived from the understanding
of human geography.
This technique allows a more accurate understanding of the connections between
consumers and physical space, whose interaction provides innumerable innovative
marketing possibilities for companies [23].
GM is therefore a tool that is extremely useful for managers, helping them to make
decisions and making the identiﬁcation of opportunities and threats to their business
more efﬁcient. In addition, it is possible to state that GM techniques contribute to a
better allocation of available resources; preventing issues such as the lack of demand,
poor public acceptance or excessive competition; and to optimize organizational
results, since it is able to identify the ideal environment for business development and,
in the case of franchises, target regions with a coverage gap, which ends up repre-
senting a cost for a network of stores.
3
Combining DBM and GM
The basis for successful marketing strategy is the identiﬁcation of speciﬁc customer
groups that have homogeneous characteristics in detail. In order to identify these
segments, a very large number of information is necessary to know the particularities of
each group and to be able to satisfy their needs. The targeting process is long and
complex because it requires the conﬁrmation that the segments exist, the determination
of their characteristics and location; from this information it is possible to devise ways
of allocating each customer in the correct segment.
The knowledge about consumers retained in DBM, plus the GM information on
where and how these customers are graphically placed in the market, is critical to
implementing a marketing approach. There is a growing need for a more accurate
understanding of the market, which manifests itself through increasing and speciﬁc
segmentations. This segmentation is due to the gradual fragmentation of the population
and the need to deﬁne a differentiated strategy for each segment [23].
By combining DBM that provides behavioral information from your prospects and
combining GM with data from the spatial information of those customers, it´s possible
to perform a comprehensive analysis and is possible through the use of segmentations
techniques; which supports marketing campaigns in an effective way, promoting vis-
ibility in the market, and allowing acquiring or maintaining a strategic positioning.
Although the range of applications from GIS to business issues is quite broad, the
structure of the market-leading programs makes them particularly well suited for DBM.
134
J. A. Vitor et al.

They integrate three types of ﬁles: DB\DBM, geographic ﬁles and point ﬁles. The
geographic ﬁles contain the geographical entities (areas, lines, points) deﬁned by their
coordinates (latitude, longitude), and serve to produce the maps themselves, forming
the most critical and expensive part of the system. Point ﬁles are a hybrid of the ﬁrst
two. Point ﬁles include information associated with point locations that are not durable
geographic entities. The classic case would be a customer database (DB\DBM). These,
once properly geocoded from the costumer address, postal code or other attribute, can
be placed on the map. In addition, the data available are associated with location and
can be manipulated taking this information into account. The interaction of these three
large blocks allows the assembly of maps, the application of colors, patterns and
symbols (to represent different types of data simultaneously) and the performance of
several aggregation, disaggregation and statistical calculations.
The typical end product can be an analysis of the potential market, segmentation,
location of customer bases and prospects, or branch location. It can also be the planning
and projection of responses to campaigns or the projection of market trends, any of the
numerous studies where location is important.
Currently there some open source tools available to SMEs, such as QGIS [26] and
GVSIG [27]. QGIS is currently the reference in open source GIS software, customizable
with a multitude of plugins and with the power of the OpenGEO suite. GVSIG is also
very complete. With these two desktop GIS it´s possible perform analysis of geo-
graphical coverage, market penetration, demand potential, among others, like in the
most common applications of GM. But if we need more arsenal of spatial analysis, we
can go through Spatial Datamining and test its excellent available tools for commercial
territories, regionalization, spatial clustering, and spatial interpolation. Another example
is Geoda [28], also has excellent resources for space econometrics, spatial regression,
and integration with statistical software. These tools also allow SME´s to spatially
analyze the data about clients stored in the DBM. Even is possible have free online
geocoding resources, to assign geographic codes to customers.
4
Conclusions
Nowadays, to survive in the global market, with an increasingly and aggressive
competitiveness, focusing on the customer is becoming a key factor for SME’s, and
customer retention is increasingly important, conditioned by SME’s limited resources.
By joining the concepts of geography and marketing, we have GeoMarketing,
which allows study the relations between the territory or space and the strategies and
policies of Marketing. Being the territory the space where the company, its customers,
suppliers and distribution points are located.
GIS integrate DBM, geographic ﬁles and point ﬁles. Combining the customer
segmented extract from DBM using RFM model, with GM geographic ﬁles, its pos-
sible return the geocoded from costumer address, postal code or other attribute, can be
placed on the map. In addition, the data available are associated with location and can
be manipulated taking this information into account. The interaction of these ﬁles
(DMB, geographic ﬁles and point ﬁles) allows the assembly of maps, the application of
colors, patterns and symbols (to represent different types of data simultaneously) and
Inexpensive Marketing Tools for SMEs
135

the performance of several aggregation, disaggregation and statistical calculations. The
typical result can be an analysis of the potential market, segmentation, location of
customer bases and prospects, or branch location. It can also be the planning and
projection of responses to campaigns or the projection of market trends, any of the
numerous studies where location is important.
This work provides a comprehensive review of a combining application of DBM
with GM, using open source tools, that helps marketers visualize and quickly identify
important customer segments, and to develop the marketing effective strategy; allowing
acquiring or maintaining a strategic positioning; being a good and innovative strategy
for SMEs.
References
1. Romano, C., Ratnatunga, J.: The role of marketing: its impact on small enterprise research.
Eur. J. Market. 29, 9–30 (1995)
2. Culkin, N., Smith, D.: An emotional business: a guide to understanding the motivations of
small business decision takers. Qual. Market Res. Int. J. 3, 145–157 (2000)
3. Kotler, P.: Marketing Management. Prentice-Hall, Upper Saddle River (2000)
4. Coviello, N., Brodie, R., Danaher, P., Johnston, W.J.: How ﬁrms relate to their markets: an
empirical examination of contemporary marketing practices. J. Market. 66, 33–46 (2002)
5. Brodie, R., Winklhofer, H., Coviello, N., Johnston, W.: Is e-marketing coming of age? An
examination of the penetration of e-marketing and ﬁrm performance. J. Interact. Market.
21, 2–21 (2007)
6. Singh, J., Sirdeshmukh, D.: Agency and trust mechanisms in relational exchange. J. Market.
66, 5–37 (2000)
7. Venkateswaran, R.: A customer satisﬁed in not a customer retained. Indian Inst. Manage.
Bangalore Manage. Rev. 3, 120–130 (2003)
8. Kandampully, J.: Service quality to service loyalty: a relationship which goes beyond
customer services. Total Qual. Manage. 9, 431–443 (1998)
9. Bond, A., Foss, B., Patron, M.: Consumer Insight: How to Use Data e Market Research to
Get Closer to Your Customer. Kogen, London (2004)
10. Detlev, Z., Dholakia, N.: Whose identity is it anyway? Consumer representation in the age of
database marketing. J. Macromarket. 24(1), 31–43 (2004)
11. Hughes, A.: Strategic Database Marketing. Probus Publishing Company, Chicago (1994)
12. Gama, M.: Database marketing, age-old customer savvy gets an algorithmic boost. Medical
Industry Information Report (1997)
13. Tucker, M.: Fresh dough. Datamation (1997)
14. Fletcher, K., Deans, K.: The structure and content of the marketing information system: a
guide for management. Market. Intell. Plan. 6, 27–35 (1998)
15. Cross, R., Janet, S.: Retailers move toward new customer relations. Direct Market. J. 57,
20–22 (2004)
16. Chan, C.: Online auction customer segmentation using a neural network model. Int. J. Appl.
Sci. Eng. 3, 101–109 (2005)
17. McCarty, J., Hastak, M.: Segmentation approaches in data-mining: a comparison of RFM,
CHAID, and logistic regression. J. Bus. Res. 60, 656–662 (2007)
18. Fader, P., Hardie, B., Lee, K.: RFM and CLV: using iso-value curves for customer base
analysis. J. Market. Res. 42, 415–430 (2005)
136
J. A. Vitor et al.

19. Colombo, R., Weina, J.: A stochastic RFM model. J. Interact. Market. 13, 2–12 (1999)
20. Yeh, I., Yang, K.J., Ting, T.M.: Knowledge discovery on RFM model using Bernoulli
sequence. Expert Syst. Appl. 36, 5866–5871 (2009)
21. Wang, C.H.: Apply robust segmentation to the service industry using kernel induced fuzzy
clustering techniques. Expert Syst. Appl. 37, 8395–8400 (2010)
22. Hughes, A.M.: Strategic Database Marketing. McGraw–Hill, Chicago (2000)
23. Venkatesan, R., Kumar, V.: A customer lifetime value framework for customer selection and
resource allocation strategy. J. Market. 68, 106–125 (2004)
24. Lai, P., So, F., Chan, K.: Spatial Epidemiological Approaches in Disease Mapping and
Analysis. CRC Press, Boca Raton (2008)
25. Mennecke, B.: Understanding the role of geographic information technologies in business:
applications and research directions. J. Geogr. Inf. Decis. Anal. 1(1), 44–68 (1997)
26. QGIS: QGIS - The Leading Open Source Desktop GIS. http://www.qgis.org/
27. gvSIG: gvSIG: Technologies and open source software solutions for working with
geographic data. In: gvSIG Association. http://www.gvsig.com/en/products
28. The University of Chicago: Software of the Center for Spatial Data Science. Geoda. https://
spatial.uchicago.edu/software
Inexpensive Marketing Tools for SMEs
137

Big Data, the Next Step in the Evolution
of Educational Data Analysis
W. Villegas-Ch1(&), Sergio Luján-Mora2,
Diego Buenaño-Fernandez1, and X. Palacios-Pacheco3
1 Facultad de Ingeniería y Ciencias Agropecuarias,
Universidad de Las Américas, Quito, Ecuador
{william.villegas,diego.buenano}@udla.edu.ec
2 Departamento de Lenguajes y Sistemas Informáticos,
Universidad de Alicante, Alicante, Spain
sergio.lujan@ua.es
3 Departamento de Sistemas, Universidad Internacional del Ecuador,
Quito, Ecuador
xpalacio@uide.edu.ec
Abstract. This paper presents an analysis of new concepts such as big data,
smart data and a data lake. It is to sought integrate learning management systems
with these platforms and contribute to education by making it personalised and
of quality. For this study, the data and needs of a university in Ecuador have
been considered. This university has set its goals to the discovery of patterns,
using data mining techniques applied to cubes generated in a data warehouse.
However, the institution wants to integrate all the systems and sensors that
contribute to the educational development of the student. Integrating more
systems into the data warehouse has compromised the veracity of the data and
the processing capabilities have been surpassed by the volume of data. The
paper proposes the use of one of the platforms analysed and its tools to generate
knowledge and to help the students to learn.
Keywords: Analysis of data  Big data  Data lake  Data mining
Data warehouse  Smart data
1
Introduction
Education currently uses learning platforms, information and communications tech-
nology (ICT) to manage learning. The aim of this integration of pedagogy and ICT is to
create learning methods that are accessible and used by students. The integration uses,
as its main tool, the learning management system (LMS) [1]. LMSs have become the
main repository of student performance information. In order to take advantage of this
information, data mining techniques are used to obtain patterns in student performance
[17]. For this work the Moodle platform of a university in Ecuador is used. In this
university, the use of Moodle has been institutionalised and policies of use have been
created for a standard management of the courses of each one of the teachers. The
Moodle platform has been customised according to the policies and needs presented by
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_14

the academic department in charge of evaluating student learning [7]. The customi-
sation of the platform is based on the integration of modules and links that allow the
proper management of academic resources which students can use for the development
of activities. The large amount of data generated has surpassed the analysis capabilities
to be processed in a conventional way. To supplement this processing, it is necessary to
work with new concepts such as big data, smart data, data lake and data mining. The
convergence of these concepts with the educational systems will allow evaluation of
the data and transform it into useful information. This paper deﬁnes which of the
platforms analysed, based on the needs of an educational institution, gives greater
beneﬁts for decision making and contributes to the improvement of education.
The work is composed as follows: Sect. 2 presents the concepts used in the
development of this work; Sect. 3 determines the method used for the analysis of big
data, smart data and the data lake; Sect. 4 presents the analysis of results to choose the
platform that gives the solution to the problems raised, and Sect. 5 presents the con-
clusions that have been reached from the work done.
2
Preliminary Concepts
This article takes into account several key concepts that help the management and
application of different analyses that help clarify new processes within the educational
ﬁeld.
2.1
Big Data
Big data is born from data sets or combinations of data sets whose size, complexity
(variability) and speed of growth (velocity) make it difﬁcult to capture, manage, process
or analyse using conventional technologies and tools [10]. Big data requires a com-
bination of different tools such as relational and non-relational databases [5]. It makes
use of analytical tools to transform data into value information. Big data analysis is
done on hundreds or thousands of blade servers. The tasks are distributed in intelligent
networks of parallel processing that allow the use of analyses in a real time to cus-
tomise, segment, optimise prices and relate in with customers [6].
Big data allows the management and analysis of huge volumes of data that cannot
be processed in a conventional way. The size used to determine if a dataset is con-
sidered big data is not deﬁned and keeps changing over time [9]. However, as a
benchmark, analysts and professionals refer to datasets ranging from 30–50 TB to
several petabytes.
Big data is a complex data set; this is mainly due to the unstructured nature of much
of the data generated by modern technologies. These technologies are: web logs, radio
frequency identiﬁcation and built-in sensors in devices, vehicles, Internet searches,
social networks, smart phones, GPS devices and call centre records.
Organizations have handled large volumes of data for a long time and have
developed data warehouses and powerful analytical tools. These tools allow the ade-
quate handling of large volumes of data [2]. The goal of big data is to turn the data into
information that facilitates decision making in real time. However, more than a matter
Big Data, the Next Step in the Evolution
139

of size, it is a business opportunity. Organizations use big data to understand the
proﬁle, needs and feelings of their customers regarding the products or services offered.
In order to use big data effectively, it must integrate structured data from a conventional
business application, such as enterprise resource planning (ERP) or customer rela-
tionship management (CRM).
2.2
Smart Data
The concept smart data appears following the big data and focuses on processing data,
in order to convert them into statistics. These statistics serve to ﬁnd the content of
higher value (separates the useful from the useless), smart data is a complex data ﬁlter
[5]; it is a concept that revolves around mass information management but only of the
one that has a real value.
To understand the smart data easily, the big data would be information gathering,
processing and ﬁltering. The smart data would act once all that processed information
is available and use mathematical formulae to convert data into “axiomatic” responses
on a market [4].
2.3
Data Lake
A data lake is a storage repository that contains a large amount of raw data and is kept
there until needed [8]. A data lake is a concept close to data warehouse that allows for
the storage and processing of large volumes of data. They are used to collect raw data
before the data sets pass into a production analytical environment, such as a data
warehouse [11].
The main beneﬁt of a data lake is the centralisation of disparate content sources.
Once assembled, these sources can be combined and processed using big data (searches
and analyses). The disparate content sources often contain conﬁdential information that
will require the implementation of appropriate security measures in the data lake.
The content of the data lake can be normalized and enriched. This may include
extracting metadata, format conversion, augmentation, entity extraction, crosslinking,
aggregation, de-normalization or indexing. Scattered users around the world can have
ﬂexible access to a data lake and its content from anywhere. Accessibility increases
re-use of the content and helps the organisation gather the data needed more easily to
drive business decisions.
2.4
Data Mining
Data mining is the analysis stage of knowledge discovery in databases (KDD) [3]. It is
a ﬁeld of statistics and computer science that attempts to uncover patterns in large
volumes of data. It uses the methods of artiﬁcial intelligence, machine learning,
statistics and database systems. The general objective of the data mining process is to
extract information from a set of data and transform it into useful information. It ﬁnds
repetitive patterns, trends or rules that explain the behaviour of data in a given context.
The data are the raw materials, the user attributes some special meaning to them and
they become information; the specialists elaborate or manage a model, so that the
140
W. Villegas-Ch et al.

interpretation that arises between the information and that model represents an added
value, which is called knowledge.
3
Method
For the development of this work, the analyses of big data, smart data and the data lake
have been undertaken. The objective is to specify how these three concepts can be
applied to educational institutions and which will help to improve education and the
generation of knowledge. For this analysis, technical data such as data volume, his-
torical management needs, pre-processing of data, etc. are considered.
Once the results are obtained, a method is proposed that helps to discover which of
the platforms provides the best solution for the needs of the knowledge analysis within
the LMS. The tool should detect patterns in student behaviour that help deﬁne
strategies for resource improvement, as well as educational activities provided on LMS
platforms.
3.1
Description of the Problem
For this work we analyse the data of a university in Ecuador. Since 2010, this edu-
cational institution has worked with the Moodle platform as an e-learning tool. The
institution has eight thousand students in different programs. The students, to ﬁnish
their programs, must culminate ten educational periods. Each period consists of four
months and on average six subjects are given. The use of the Moodle platform is
obligatory in support of the teaching-learning activity. For the control of the use of the
platform, the administrators generate reports of the activities performed at the end of
each period.
At the beginning of each educational period, a new virtual classroom is generated
for each subject within the LMS. The generation of virtual classrooms has the purpose
of removing the virtual classrooms from past periods and, in such a way, maintains
backups and records of the data. With the passage of time, standards have improved the
use of the platform, in the same way that modules have been created to help teacher
management. These improvements have allowed the creation of new techniques and
resources that help the development of the learning in the students. Currently the
platform has links to different websites, multimedia material and games. It has even
been integrated with tools that allow virtual tutorials, as well as systems that allow the
detection of similarity between documents or any ﬁle that is on the internet.
The registry of activities in the platform has made it the most important repository
that handles educational data within the institution. The constant growth of data volume
has forced to implement a data warehouse for data analysis. The analysis is based on
the conventional data mining application adapted from business analysis to data cubes.
Data mining has revealed patterns in the behaviour of educational data and improved
learning techniques. However, the data warehouse tool has fulﬁlled its life cycle and
exceeded the processing capabilities compromising the accuracy of the results. To
solve these problems, there needs to be an improvement in the design of the data
Big Data, the Next Step in the Evolution
141

warehouse or, in turn, to apply one of the new trends in data analysis and to take
advantage of the information that has been generated during this time.
For our work we consider the following data from an Ecuadorian university. For the
calculation, the number of hours that an average student with 7/10 grades devotes to the
Moodle platform has been considered, as Table 1 shows.
Table 2 calculates the average storage utilization used by each student over the
course of a year. These data are considered important for analysis because they give us
an exact ﬁgure on the consumption that has been generated in the database of the
Moodle platform. The calculation has been made with the known data that are: the 5
TB of storage used from 2010 to 2017 and the 8000 students with which it counts, the
institution until the indicated year. The result is 0.62 GB for each student since 2010
then we have detailed the storage consumption of each student per year.
The amount of storage shown in Table 2 indicates that it is a very low growth.
However, due to data analysis needs, academic authorities have proposed increasing
the management of the LMS platform and generating data by 50% with respect to
megabyte used per year and per student.
3.2
Characteristics of Big Data, Smart Data and the Data Lake
Table 3 shows a comparison of the characteristics of big data, smart data and the data
lake. It has taken as a reference the capacity of these platforms for data management
where big data handles a large volume of data [14]. The Smart data, with respect to
capacity, acts as a data ﬁlter taking only the most important ones and, on those, applies
the techniques of data analysis. The data lake, with its capacity, shares the character-
istics of a data warehouse with the advantage that it does not need a process of
extraction, transformation and load (ETL) that is in charge of a pre-processing of the
data [18].
Table 1. Calculation of hours of use of the Moodle platform
# hours of an average student on
platform per week
Days per
week
# weeks
per period
# of periods
per year
Total hours
per year
5.5
5
16
2
880
Total number of students
8000
7040000
Table 2. Calculation of average Moodle storage by year and student
Megabyte used per year and
per student
Gigabyte consumed by students
since 2010
Terabyte consumed
since 2010
89.3 MB
0.62 GB
5 TB
142
W. Villegas-Ch et al.

With regard to the sources each of these supports, all three have similar charac-
teristics, they feed on structured or unstructured datasets. Another feature that is
important is the cost of storage where the big data has the highest cost due to the large
volume of data and the infrastructure it supports for data acquisition. On the other hand,
smart data, acting as a data ﬁlter, manages the storage cost better. Finally, a data lake is
based on technologies that allow the storage of raw data and then apply incrementally
the structure, as deﬁned by the analytical requirements.
So far, we have detailed the characteristics of each of the data platforms which are
important for the current situation of our platform, Moodle. The storage of the LMS of
the university that has been used so far, since 2010, is 5 TB. The concept of big data is
not considered as a ﬁxed parameter from which it can be adopted as a big data platform.
However, we will adopt as a reference a starting point of 30 TB [14]. If we consider the
level of processing, speed and analysis, it is an advantage to use big data. However
smart data and the data lake that have been considered in this work can also offer these
characteristics without oversizing the resources. Therefore, categorisation with respect
to storage is sufﬁcient to rule out the adoption of big data for the needs of the
institution.
The need is to convert the data into useful information. Considering this objective,
we can better analyse both the adoption of smart data and a data lake. We begin this
analysis by indicating that, in the case study of the Moodle platform, data mining
techniques have been applied with the help of a data warehouse and the generation of
cubes in the past. However, the life cycle of this technique has come to an end. With
this consideration, at present the University has an original Moodle repository
(MySQL) and a repository with clean data in an SQL database engine. In addition to
these data sources should be considered external sources that are spreadsheets, plain
text and even independent databases. These databases may contain relevant information
from several of the courses and have been handled internally by teachers either as
learning activities or records. As an additional point, the big data in our case is
oversized by the volume of data available on the LMS platform, but we can beneﬁt
from the tools that big data offers for the analysis of the information.
3.3
Processing in Smart Data and a Data Lake
The smart data is not focused on storing or processing information, but on extracting
value from it. This is where the human factor, the business knowledge and the expertise
are most relevant. This task is achievable by data scientists who know the data they
have, what they need to know and how to obtain it [13]. The questions to be answered
in the analysis are: what do we do with all this volume?, what is the relevant
Table 3. Analyses of datasets
Platform
Capacity
Sources
Storage cost
Big data
Large dataset
Complex, structured or
unstructured data set
High
Smart data It is a data ﬁlter
Raw data
Medium
Data lake
Improved version of the data warehouse Raw data
Low
Big Data, the Next Step in the Evolution
143

information of all that we have collected?, what level of aggregation is needed?. With
regard to speed, we must know precisely what actions make sense in real time, which in
near-real time and which can be performed every hour, every day, every month or
every year. It does not make sense to analyse the information every second if we can
only act every twenty-four hours, we should simply store it to analyse after.
In data lake the access to the original information is direct and reduces the inter-
mediate steps for its processing [12]. Sometimes, when a record is deleted, it may not
be needed immediately, but after a while. Data that may not be useful today may be
needed after a few months or even years. A data lake marks the differences as a
non-pre-processed data storage system. However, it is necessary to consider a higher
cost, both of technical means and of professional proﬁles that are able to manage it.
Table 4 describes the parameters evaluated in the smart data and data lake plat-
forms. The storage parameter refers to the capacity of the platform to provide the user
with the storage of the data. The smart data does not act as a storage platform because it
is focused on extracting useful information. Their accuracy depends on the analysis of
the data scientists in determining the exact times for the execution of processes. The
data lake, on its own, acts as a data repository since its processing uses raw data, its
operation is based on the conservation of the data. For cost parameters at the technical
level, it is considered that the smart data does not need a great infrastructure since it
uses the operational data sources to extract the information; by contrast, the data lake,
requires greater infrastructure for data storage as well as systems for processing. In
human cost, the two platforms require highly trained analysts and data scientists with
extensive knowledge of the business and the data generated.
4
Analysis of Results
The analysis made in Sect. 3 gives us a broader picture of the tool that we can use
considering the needs that are presented by the educational institution used as a case
study. It is worth mentioning that any change considered for improvement in education
should be attached to the economic reality of the institution. With this clariﬁcation, the
tool that is sought must meet the technical and quality requirements, as well as the
technical and human costs.
In the ﬁrst analysis carried out in Sect. 3.2, big data has been discarded: although its
functionality is broad and applicable to each organization often it is not an optimal
solution, since it is possible to oversize the utility of the organizations technical
resources. Excessive resources allocated to the system will affect implementation costs;
however, we can make use of the analysis tools for our process.
In reviewing the needs as explained in Sect. 3.1, we note that the LMS used
manages a data warehouse. This data store can be reused as an external source and be
Table 4. Technical analysis of smart data and the data lake
Platform
Storage Prosecution
Technical costs Human cost
Smart data Low
High availability Medium
High
Data lake
High
High availability High
High
144
W. Villegas-Ch et al.

managed by both a smart data as a data lake. The results indicate that the data lake
provides advantages to the educational environment if it meets several characteristics,
such as high availability in storage resources. In return, it offers us the conservation of
data which, if not needed at this moment, but it may be important in the future. With
this option, the analysis of several sensors or systems that help the discovery of trends
or patterns of the students can be integrated. For example, whether it is necessary to
increase the consumption of coffee in the period of examinations, or how many times a
student enters the university, the data collected from his access card. The qualities
offered by a data lake are interesting and give long-term advantages. However, at the
moment our study only covers the LMS platform, so our application focuses on the use
of smart data by reducing costs.
The use of smart data focuses on how we can integrate our data warehouse into its
processing. The ideal for this tool is that it can make use of the cubes that are available in
the current system without the need to process the information. It will simply extract the
value of it into the process. Keeping data that has gone through a previous processing
ensures the accuracy of data in the same way as it reduces processing. Another feature of
smart data is the conﬁguration of processes at speciﬁc times. For example, every 24 h,
once a month or every 4 months, depending on the need of the organization.
The beneﬁts offered by data mining for the analysis of data will be used, because
this converges, without any problem, with the data and information generated by smart
data. In this work, we do not perform an analysis of the algorithms to be used.
However, from experience of the authors in previous works [15, 16], it can be men-
tioned that both the search algorithm and the cluster have sufﬁcient characteristics to
solve our needs.
On average one student generates 90 Mb per year, this volume of information is
only from the use of the Moodle platform. The amount of information is very low in
consideration of common enterprises, but this ﬁgure will increase if more sensors and
systems are integrated into our analysis platform. The description of the problem
mentions the need for integration of the systems that the university manages in order to
carry out an in-depth analysis of the students. These systems manage the student’s
attendance, ﬁnancial situation, qualiﬁcations, even there are printing systems that will
indicate the trend in each student’s reading.
5
Conclusions
This work includes new concepts that can be considered as a component that helps to
improve education through the use of information and communication technologies.
What has been sought during this development is to qualify the various platforms based
on the needs of a particular educational institution considering, as the main base, the
multiple sources of data.
Most organizations currently seek to take advantage of the information that is
generated daily in the interaction with customers, deﬁning what their interests are and
being able to generate more proﬁts based on these statistics. The same concept can be
replicated in the educational ﬁeld and, in this way, a personalised education can be
offered based on the characteristics or patterns presented by each individual student.
Big Data, the Next Step in the Evolution
145

The use of data mining on educational platforms every day has greater depth.
However, it is important that the evaluation environment goes beyond an LMS. It is
important that all the sensors or systems surrounding the student converge into one, so
that the trends, problems or help that each student requires may be detected in a timely
manner. This processing capacity exceeds the typical data warehouse so we have
considered it important to scale to other types of tools.
Using a smart data will have the ability to analyse these systems in depth and
establish patterns that tell us how the learning outcomes of a speciﬁc student can be
improved. At the moment, a test on the operation of a smart data has been carried out
using Microsoft power BI tool. The results will be presented in a future work since the
data obtained are in validation stage. The power BI tool allows an ad hoc analysis and
until now, four systems that control the student’s activity when he or she is at university
have been integrated into the test.
References
1. Dalsgaard, Ch.: Social software: e-learning beyond learning management systems. Eur.
J. Open Distance E-Learn. 9(2), 1–7 (2006)
2. Davenport, T.H., Barth, P., Bean, R.: How big data is different. MIT Sloan Manage. Rev. 54
(1), 4346 (2012). https://search.proquest.com/docview/1124397830?accountid=33194
3. Fayyad, U., Piatetsky-Shapiro, G., Smyth, P.: From data mining to knowledge discovery in
databases. AI Mag. 17(3), 37 (1996)
4. Higdon, S.J., Devost, D., Higdon, J., Brandl, B., Houck, J., Hall, P., Green, J.: The SMART
data analysis package for the infrared spectrograph* on the spitzer space telescope. Publ.
Astron. Soc. Pac. 116(824), 975 (2004)
5. Lavalle, S., Lesser, E., Shocley, R.: Big data, analytics and the path from insights to value.
MIT Sloan Manage. Rev. 52(2), 21 (2011)
6. Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., Byers, A.: Big data:
the next frontier for innovation, competition, and productivity, pp. 27–36 (2011). http://
www.mckinsey.com/business-functions/digital-mckinsey/our-insights/big-data-the-next-
frontier-for-innovation
7. Dougiamas, M., Taylor, P.: Moodle: using learning communities to create an open source
course management system. In: Proceedings of ED-MEDIA World Conference on
Educational Multimedia, Hypermedia and Telecommunications, pp. 171–178. Association
for the Advancement of Computing in Education, Honolulu (2003)
8. O’leary, D.: Embedding AI and crowdsourcing in the big data lake. IEEE Intell. Syst. 29(5),
70–73 (2014)
9. Sagiroglu, S., Sinanc, D.: Big data: a review. In: International Conference on Collaboration
Technologies and Systems (CTS), pp. 42–47 (2013)
10. Snijders, C., Matzat, U., Reips, U.: Big Data: big gaps of knowledge in the ﬁeld of internet
science. Int. J. Internet Sci. 7(1), 1–5 (2012)
11. Terrizzano, I.G., Schwarz, P.M., Roth, M., Colino, J.E.: Data wrangling: the challenging
Yourney from the wild to the lake. In: Conference on Innovative Data Systems Research
(CIDR), pp. 1–9 (2015)
146
W. Villegas-Ch et al.

12. Thusoo, A., Shao, Z., Anthony, S., Borthakur, D., Jain, N., Sen Sarma, J., Liu, H.: Data
warehousing and analytics infrastructure at Facebook. In: Proceedings of the 2010
ACM SIGMOD International Conference on Management of data, pp. 1013–1020. ACM
(2010)
13. Trautsch, F., Herbold, S., Makedonski, P., Grabowski. J.: Adressing problems with external
validity of repository mining studies through a smart data platform. In: Proceedings of the
13th International Conference on Mining Software Repositories MSR, pp. 97–108. ACM
(2016)
14. Villars, R.L., Carl, W., Matthew, E.: Big data: what it is and why you should care. White
Paper IDC 14, 1–14 (2011)
15. Villegas-Ch, W., Luján-Mora, S.: Systematic review of evidence on data mining applied to
LMS platforms for improving e-learning. In: International Technology, Education and
Development Conference (INTED), pp. 6537–6545 (2017)
16. Villegas-Ch, W., Luján-Mora, S.: Analysis of data mining techniques applied to LMS for
personalized education. In: World Engineering Education Conference (EDUNINE),
pp. 85–89. IEEE (2017)
17. Walker, J.S.: Big data: a revolution that will transform how we live, work, and think. Int.
J. Advertising 33(1), 181–183 (2014)
18. Widom, J.: Research problems in data warehousing. In: Proceedings of the Fourth
International Conference on Information and Knowledge Management (CIKM), pp. 25–30
(1995)
Big Data, the Next Step in the Evolution
147

Method for Emotion Corpus Validation
from the Consensual Identiﬁcation of Patterns
in Alzheimer’s Patients
Pablo Gómez(&), Alexandra González-Eras(&),
and Pablo Torres Carrión(&)
Universidad Técnica Particular de Loja, San Cayetano Alto Loja, Loja, Ecuador
{pfgomez1,acgonzalez,pvtorres}@utpl.edu.ec
Abstract. The present research proposes a method for the construction of a
corpus for the early detection of Alzheimer’s, by identifying basic patterns of
emotions on video, from the collection of patient information in the form of
videos, analysis and identiﬁcation of emotions based on facial expressions and
ﬁnally validation by two statistical measures: weighted Kappa and Kappa.
Applying the method on a corpus of 40 videos, an average score of 0.60
obtained in the Kappa index and 0.67 in the weighted Kappa index, which
indicates a good agreement among the observers, and provides encouraging
results for the use of the corpus in automatic learning, on the patterns of
emotions that allow the detection of Alzheimer’s disease.
Keywords: Emotion analysis Alzheimer’s disease  Kappa index
Weighted Kappa index  Alzheimer’s detection
1
Introduction
Research related to mental illness is gaining importance for society. The World Health
Organization indicates that “it is essential to conduct studies to adults over 60 years of
age because they are more vulnerable to suffering a type of dementia at a certain point in
their lives” [1]. Alzheimer’s is a type of dementia that in the elderly population causes
difﬁculties in the development of activities, directly affecting memory, thinking, ori-
entation, and learning ability and how to express themselves. In that sense, observation
of the emotions that Alzheimer’s patients express when they carry out the activities
affected by the disease, is a viable diagnostic option, instead of expensive and invasive
methods that are currently the only way to obtain information about the disease.
Consequently, it is necessary to undertake the development of reliable sources
about the emotions, expressed by Alzheimer patients in their daily activities, with the
following purposes:
1. To use them as input in the development of other investigations, oriented towards
the creation of environments and technological applications that promote the quality
of life of both people with the disease and their caregivers.
2. To establish non-invasive methods for collecting disease information from patients,
to identify patterns that allow the study of disease progression.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_15

To date, there are no corpus or datasets that identify basic emotions of people with
Alzheimer’s, so the purpose of this project is precisely build a corpus of videos of
Alzheimer’s patients, to identify patterns of emotions, which allows to research on the
disease.
In the present work, we propose a method for creating a dataset of videos of
Alzheimer’s patients, in whom emotions are recognized and validated by statistical
methods, ensuring the consistency and reliability of identiﬁcation. For the construction
of the dataset, using interview videos to analyze the emotions experienced by patients,
two observers responsible for labeling the emotions manually analyzed each video and
independently, using in both cases a window time of twenty seconds. Them, Kappa and
weighted Kappa index are used to represent the proportion of agreement observed after
eliminating the agreement by chance; thus establishing the reliability of the corpus and
ensuring its use in other research studies related to the Alzheimer’s disease.
2
Related Works
There are previous studies, where corpus constructed through observer annotation and
the use of the Kappa index for the study of pharmacological substances and their
drug-drug interactions, with results between 0.55 and 0.72 [2]. In the analysis of textual
information, a corpus of medical phrases constructed from 10000 abstracts annotated
by experts, validated by the Kappa index with an average score of 0.62 indicating a
substantial agreement [3].
In other works the recognition of patterns of emotions based on polarity in videos
of Alzheimer’s patients is performed by manual annotation by experts, and the use of
human emotion detection (HER) software; achieving an agreement between annota-
tions, according to the weighted Kappa index of 0.7 [4]. In [5] CSIS, a software that
collects information from cancer medical reports of 710 patients, of which 179 have
been evaluated by two independent pathologists to measure the agreement between the
human expert and the software.
In conclusion, the use of Kappa and Kappa weighted indexes is considered ade-
quate for the treatment of subjective evaluation results of emotion patterns, obtained
from observation of annotators [15].
3
Methodology
The proposed methodology allows the identiﬁcation of emotions through videos with
Alzheimer patients for the construction of a corpus of videos. For this reason, we have
worked with four phases: Phase I part of the research design, Phase II video analysis
and Phase III methods are used statistics for corpus validation (Fig. 1).
Method for Emotion Corpus Validation
149

3.1
Phase I: Research Design
The research design establishes a set of activities to follow sequentially to achieve the
research objective [13]. With this purpose, activities deﬁned such as sample size, collection
of information, data collection and identiﬁcation of variables, which detailed below.
3.1.1
Sample Size
The sample comprise 40 videos, corresponding to interviews of elderly people
belonging to care centers, private individuals and interviews with the elderly published
on YouTube. The distribution of the videos according to their source detailed below.
Physical sources
• Centro del Día San José (Loja): 4
• Centro del Adulto mayor (Catamayo): 2
• Centro del Adulto mayor (Vilcabamba): 2
• Fundación del Adulto mayor (Quito): 2
• Ciudad de Loja: 1
• Ciudad de San Pedro de la Bendita: 1
Digital sources
• YouTube: 28
3.1.2
Collection of Information
In order to create the conditions for the collection, criteria established for each source of
information. In the case of physical sources, a team is formed by, the interviewer, a
medical or psychology professional and a camera operator. On the side of the inter-
viewee, elderly people considered according to their age, their disease level and the
written consent given by the patient or a family member to participate in the experiment.
Fig. 1. Corpus development methodology.
150
P. Gómez et al.

Additionally, requirements are set for the preparation of the interview site, such as a
family space for the interviewee, with adequate lighting and acoustics and the presence
of the caregiver or relative with whom the elder has a greater afﬁnity.
For the digital sources, the search criteria are “interview Alzheimer’s people”,
obtaining 3990 results; and “Alzheimer interviews”, obtaining 530.00 results. Then two
scorers select videos that comply with requirements such as videos with free license
(that is to say published in the platform to be used without any restriction on the part of
the author), with a duration of 2 and 6 min, and in which the interviewee appears in
frontal position. As a result, a number of 28 valid videos obtained.
3.1.3
Obtaining the Data
In the case of physical sources, an interview conducted with each person for a duration
of ﬁve minutes, where the interviewer used a questionnaire based on the Folstein Test,
in order to obtain visual answers to the questions raised and thus detect the expressions
of the interviewee. Table 1 presents the structure of the questionnaire, where it is
observed that the selected classiﬁcations of the test: general information, temporal
orientation and spatial orientation are sufﬁcient to evaluate the cognitive level of
patients and provide a brief analysis of their mental state [6]; also that the number of
questions selected is appropriate to the duration of the videos.
For the digital sources, videos were downloaded from the YouTube platform,
following criteria such as: quality of the interview, i.e. the responses of the interviewee
are similar to those obtained with the questions of the questionnaire; and, the quality of
the video ﬁle that must be high, in order to clearly appreciate the face and expressions
of the interview.
Table 1. Questionnaire structure for interviews.
Questionnaire
Answers
Informative data
What’s your name?
How old are you?
What year were you born?
What are your children’s names?
What are their siblings named?
Temporary orientation What year are we in?
On what day of the week?
What month of the year?
Spatial orientation
In what place do we ﬁnd ourselves?
On what ﬂoor?
What city?
In what country?
Method for Emotion Corpus Validation
151

3.1.4
Variable Identiﬁcation
For the analysis process, the following groups of variables are established:
Informative. That in the case of physical sources gather personal information of the
interviewee, such as name, age, geographical location, among others; while in digital
sources, in addition to those mentioned data on origin and copyright.
Annotations. They refer to the recognition of the emotion expressed by the interviewee
at a speciﬁc time of the interview. The annotations are made on segments of the video
Table 2. Description of basic emotions used in identiﬁcation.
Expression
Emotion
Description
Joy
Express joy in your face when has a 
sense of well-being and security.
Feeling cheerful is a positive emotion.
.
Surprise
The person reflects in his face startled, 
astonished. This basic emotion arises 
when we are not prepared and something 
comes up suddenly.
Sadness
The patient expresses in his face 
solitude, pessimism in general sadness
manifested by loss or rejection.
Fear
When it is in danger, what it produces
anxiety, insecurity and uncertainty.
Anger
Emotion that a person experiences
when you feel harmed or when
we see that someone does something
unjust to another person.
Disguss
Emotion 
that 
produces 
escape 
or 
avoidance responses.
Contempt
Emotion that expresses disrespect to 
another 
person 
through 
a 
deal 
derogatory.
152
P. Gómez et al.

(windows of 20 s), which are analyzed to detect emotions according to the descriptions
proposed by [14] and detailed in Table 2 [7]. It also establishes the deﬁnition of
“Neutral”, for those expressions that are not within those mentioned in the table.
3.2
Phase II: Video Analysis
Video analysis performed both on physical and digital sources, including the following
steps:
Pre-processing. Where each video was edited, in order to eliminate those temporary
spaces, where the interviewer does not have a conversation with the interviewee.
Figure 2 presents the development of this process, using VideoPad1 tool. In this article,
the interviewee’s face was covered, to protect his identity.
Identiﬁcation of Emotions. The pre-processed videos analyzed by two observers who
for each video establish temporary windows of 20 s, within which they identify the
emotions based on the facial expressions of the interviewee, according to the
descriptions presented in Table 2. In case of no recognize the expression; the observer
classiﬁes it as “Neutral”. Each observer performs this process individually, which ends
when the observer has checked every second of the video of all the established tem-
porary windows, and has labeled with an emotion type all existing expressions.
Fig. 2. Video pre-processing
1 Video Pad, available online on http://www.nchsoftware.com/videopad/index.html.
Method for Emotion Corpus Validation
153

Creation of the Corpus. The corpus includes a set of multidimensional characteris-
tics, collected in the different phases of the methodology, and explained below:
• A collection of digital ﬁles of interview videos was established, tagged with a
unique identiﬁer.
• A general register is established, where each row corresponds to a video, which is
assigned a unique identiﬁer, along with the informative variables.
• For each observer a second record was created, where each row corresponds to a
video, recognized by its identiﬁer and each cell contains the emotions found during
the identiﬁcation process.
• Each observer creates a third record, where each row corresponds to a video, each
column corresponds to one type of emotion and each cell stores the frequency of
occurrence of emotion in the video.
3.3
Fase III: Corpus Evaluation
The evaluation of the corpus allows measuring the level of agreement of the annota-
tions made by the observers, for this, the following steps were established:
• Contingency Matrix. Constructed by each video, where, the observations of the
two observers were crossed in order to obtain the matches between them. Figure 3
presents a contingency matrix, where it was observed that, for each type of emotion,
the annotations made by Observer 1 are located in the rows, while the observer 2
annotations are located in the columns; the diagonal of the matrix represents the
number of times that, for a type of emotion, the two observers agree.
• Kappa Index Calculation. Cohen (1960) introduces the Kappa coefﬁcient, which
represents the ratio according to or after eliminating the agreement by pure chance.
To calculate the Kappa coefﬁcient in any problem of nominal scale between two
judges, there are two relevant quantities, expressed in Eq. 1 [8].
kappa ¼
P Po  P Pe
1  P Pe
ð1Þ
Fig. 3. Contingency matrix for the case of Video01
154
P. Gómez et al.

Where:
Po. Indicate the proportion of units in which observers agreed. Equation 2 repre-
sents the proportion of agreements observed.
Po ¼
X
k
i¼1
Pij
N
ð2Þ
Pe. It is the proportion of units from which an agreement is expected by chance (3).
Pe ¼
X
k
i¼1
Pi þ  Pj þ
N2
ð3Þ
• Weighted Kappa Index Calculation. Cohen (1968) introduced a Kappa extension
called a weighted Kappa statistic that is denoted by the symbol Kw, is a measure in
which weights are assigned in each disagreement [9]. To calculate the weighted
Kappa statistic the same as part of the Kappa index equation as shown below:
Kw ¼ Pow  Pew
1  Pew
ð4Þ
Where:
Pow It is the weighted observed agreement given by the product sum of all the
records in the contingency table by the weights assigned to each record and divided by
the total number of subjects N being evaluated (5).
Pow ¼
X
k
i¼1
X
k
j¼1
wij
nij
N


ð5Þ
Pew. Agreement proportion expected by weighted pure chance, given by the pro-
duct of the marginal borders of the contingency table, corresponding to the cells that
have the same weight, these products are added and multiplied by the weight. The same
procedure is performed with all the cells of the table, to then obtain a total value and
divide it by the square of the total number of subjects N.
Pew ¼
X
k
i¼1
X
k
j¼1
wij
Ai
N

 Bi
N


ð6Þ
• Allocation of Quadratic Weights. The assignment of the weights was considered
using a linear system and later a system that is the most used called quadratic or
biquadratic system [10, 11] (7).
Method for Emotion Corpus Validation
155

w ¼ 1  i  j
j
j2
k  1
ð7Þ
• Valuation Scale. Landis and Koch (1977) proposed a Kappa index rating scale
divided into six classiﬁcations to facilitate their interpretation. The values of k go
from 0.00 to 1.00, being 0.00 the value where there is more disagreement and 1.00
the interval where there is greater agreement among the evaluators. Their classiﬁ-
cation indicates that the Kappa index can be Poor, when their value is (0.00), Light,
when their value oscillates between 0.01 to 0.20, Acceptable (0.21 to 0.40),
Moderate (0.41–0.60), Good (0.61 to 0.80) and Very Good (0.81 to 1.00) [12].
4
Results
Applying the Kappa and weighted Kappa index to the sample of the 40 videos showed
that the mean of the Kappa index is 0.60 and the mean of the weighted Kappa index is
0.67. Therefore, it concluded that the index concordance force Kappa is Moderate, and
the strength of concordance of the weighted Kappa index is good. By applying the
weighted Kappa index and assigning the quadratic weights, we can conclude that the
value of the weighted Kappa index increases relative to the Kappa index (Fig. 4).
It is also observed that the results of the Kappa index reﬂect that the largest number
of videos in the corpus have a concordance in the annotations, located in the Moderate
(0.6) and Good (0.8) levels of the Landis scale and Koch (Fig. 5).
Fig. 4. Comparison of Kappa and Kappa weighted index scores
156
P. Gómez et al.

Fig. 5. Kappa index results
Fig. 6. Weighted Kappa index results
Method for Emotion Corpus Validation
157

It is also observed that the results of the weighted Kappa index reﬂect that the
largest number of videos in the corpus have a concordance in the annotations, located
in the Moderate (0.6) and Good (0.8) levels of the Landis scale and Koch (Fig. 6).
5
Conclusions and Future Works
The present work presents a method for corpus validation, identifying basic patterns of
emotions on video and evaluating by weighted Kappa and Kappa. Applying the
method on a corpus of 40 videos, we obtain levels of agreement of Moderate and Good
in the scale of Landis and Koch, which provides encouraging results for the use of the
corpus in automatic learning, on the patterns of emotions related with emotions in
Alzheimer’s patients.
The following steps are directed towards the extension of the experiment to another
collection of videos to obtain a corpus with a greater level of agreement, allowing
experimentation in the recognition of emotion patterns and the use of corpus in research
on detection and learning of Alzheimer’s disease characteristics.
References
1. OMS: Alzheimer’s report. http://www.who.int/mediacentre/factsheets/fs362/es/7
2. Herrero-Zazo, M., Segura-Bedmar, I., Martínez, P., Declerck, T.: The DDI corpus: an
annotated corpus with pharmacological substances and drug–drug interactions. J. Biomed.
Inf. 46(5), 914–920 (2013)
3. Kim, S.N., Martinez, D., Cavedon, L., Yencken, L.: Automatic classiﬁcation of sentences to
support evidence based medicine. BMC Bioinform. 12(2), 1 (2011)
4. Narváez, M.: Análisis y reconocimiento de la expresión facial de la emoción en video de
personas con demencia. http://dspace.utpl.edu.ec/handle/123456789/15600
5. McCowan, I.A., Moore, D.C., Nguyen, A.N., Bowman, R.V., Clarke, B.E., Duhig, E.E.,
Fry, M.J.: Collection of cáncer stage data by classifying free-text medical reports. J. Am.
Med. Inf. Assoc. 14(6), 736–745 (2007)
6. Folstein, M., Folstein, S., McHugh, P.: A practical state method for. J. Psychiatr. Res.
12, 189–198 (1975)
7. Ekman, P., Friesen, W.V.: Facial Action Coding System. Consulting Psychologists Press,
Palo Alto (1977)
8. Cohen, J.: A coefﬁcient of agreement for nominal scale. Educ. Psychol. Meas. 20, 37–46
(1960)
9. Cohen, J.: Weighted Kappa: nominal scale agreement provision for scaled disagreement or
partial credit. Psychol. Bull. 70(4), 213 (1968)
10. Ruiz, A., Morillo, L.: Epidemiologia clínica aplicada: Investigación clínica aplicada. In:
Bogotá: Médica Internacional (2004)
11. Díaz, I., Sidorov, G., Suárez, S. (n.d.): Creación y evaluación de un diccionario marcado con
emociones y ponderado para el español (2004)
158
P. Gómez et al.

12. Landis, J.R., Koch, G.G.: The measurement of observer agreement for categorical data.
Biometrics 33(1), 159–174 (1977). https://doi.org/10.2307/2529310
13. Bravo, S., Bravo, R.S.: Técnicas de investigación social: teoría y ejercicios. Thomson (2003)
14. Ekman, P., Friesen, W.V.: Unmasking the Face: A Guide to Recognizing Emotions from
Facial Clues. Malor Books, Cambridge (2003)
15. Manning, C.D., Hinrich, S.: Foundations of statistical Natural Language Processing. MIT
Press, Cambridge (1999)
Method for Emotion Corpus Validation
159

Web Prosumers: The Intangible Wealth
of Education and Economics
Emanuel Bohórquez1(&), Teresa Guarda1,2, Humberto Peña1,
Marcelo León1, William Caiche1, and José Villao1
1 Universidad Estatal Península de Santa Elena – UPSE, Av. Principal La
Libertad, Santa Elena, Ecuador
ema_bohorquez@hotmail.com, tguarda@gmail.com,
hcpr100271@gmail.com, marceloleon@hotmail.com,
caichewilliam@yahoo.com, jvillaov@hotmail.com
2 Universidad de Las Fuerzas Armadas – ESPE, Sangolquí, Quito, Ecuador
Abstract. The present research focuses on the prosume of the third wave, being
an issue of worldwide interest and known as the intangible wealth of the
economy, which has originated from the Internet, where users have the possi-
bility of not only being passive consumers, but also to become active consumers
or called prosumers. In this context, we intend investigate the causes that
originate the low production of content in learning websites and social networks
in the cantons of La Libertad and Salinas. The study use a quantitative approach
and an descriptive research, concluding in the lack of educational policies and
the weak culture in the use of the web by governments and higher education
institutions, as well as the lack of interest and of knowledge in contributing or
producing on the web by young people.
Keywords: Web prosume  Prosumers  Learning websites  Social networks
1
Introduction
The blow of the third wave is a subject with a great depth and interest, being considered
as the intangible wealth of the economy. Globalization and technological advances in
information and communication, complemented with the internet, are offering a great
opportunity to humanity to be able to communicate, express, opine, see and feel of a
different way, giving to the people the possibility to change the role of passive con-
sumer to active consumer (web prosumer). Having as problematic, the existence of a
low generation of prosume in the web in La Libertad, and Salinas cantones.
Highlighting contributions, Tofﬂer argues that the prosumer, who dominated
societies in the ﬁrst wave, reappears to establish them as the center of economic action,
but on a high technology basis, on the basis of the third wave. To increase the
“productivity” of the prosumer, governments need to focus scientiﬁc and technological
research on the prosume [1].
This research is structured as follows: the second section presents the arguments that
support the web prosume and its relation with the business and university development.
Subsequently, in the third section, the study case is presented, and the respective results.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_16

Finally, the conclusions are presented, and some proposals are made with the aim to help
increase the prosume generation in the web; and limitations and future lines of research
are presented.
2
Web Prosumer
2.1
The Prosume of the Third Wave
To be able to understand the term prosume of the third wave, Tofﬂer (1980) mentions
that a more manifest way of considering the economy is to estimate it composed of two
sectors [1]: Sector A comprises all unpaid work performed directly by individuals, their
families or their communities. Meanwhile; sector B comprises all production of goods or
services for sale or exchange through the exchange network, or market. All this brings us
back to the millions of individuals who are beginning to perform services for themselves
that others have been doing for them. So what these people move some of the output
from sector B to sector A, from the visible economy that the economists supervise, to the
ghost economy they have forgotten, in other words, they are “prosuming” and they are
not alone. But whatever their meaning for social organization, they include a basic shift
from the role of passive consumer to active prosumer, and therefore also have economic
signiﬁcance. However, dependent on the market and still interrelated with it, they are
transporting the activity from sector B of the economy to sector A, from the exchange
sector to the prosume sector. And this favorable movement is not the only force of this
kind, some of the world richest and largest companies are also, for their own economic
and technological reasons, accelerating the rise of the prosumer.
On the other hand, the third wave is very important to non-economic and
non-technological interests. It makes us take a look at education with new eyes.
Education for everyone is fundamental to development. Thus, when the colonial
powers introduced formal education in Africa, India and other parts of the world of the
ﬁrst wave, factory-style schools or miniature imitations was reproduced, and in the
smallest range of their own elite schools. The second wave educational models are
being questioned everywhere.
The third wave deﬁes the typical notion of the second wave that education is
necessarily develop in a classroom. In modern life, we need to combine learning with
work, political struggle, community service and even the game. All our conventional
ideas about education need to be re-examined in both rich and poor countries [1].
Consequently, according to Tofﬂer’s (1980) contributions, civilization of the third
wave, the most basic raw material of all and one that can never be exhausted is
information, including imagination.
Through imagination and information, many of these current exhaustible resources
will be replaced, although all too often this substitution will be accompanied by dra-
matic oscillations and shakes. As information becomes more important than ever, the
new generation will structure education, redeﬁne scientiﬁc research and communica-
tion media.
In addition, Valverde & González state that new technologies and digital resources
are fully integrated into the day-to-day life of today’s society. So we can mention that,
Web Prosumers: The Intangible Wealth of Education and Economics
161

among their different functions, these resources are a source of information, and with
them and through the internet, the path to this information about any subject is easy and
can be accessed from anywhere. Therefore, if this reﬂection is transferred to the
educational context, these resources and tools are also fully integrated into the students’
lives, from the earliest ages, and are an essential part of their leisure and social
relations [2].
Finally, in the year 1955, where for the ﬁrst time the administrative staff gave
numerical notice to the workers, it originated in the US the third wave, quickly spread
to all the countries of high technology and entered into alliance with the old economies
of the second wave, relating mainly to the generation of the computer, as well as with
the commercial ﬂights of reactors, and other innovations of relevance [3].
2.2
Web Prosume for Business and University Development
Martínez, Cabecinhas, & Loscertales (2011), state that the most important of Infor-
mation and Communication Technologies (ICTs) is the Internet, which has had
enormous dominance in society. The Internet has led to a real social revolution with its
widespread use. It has made the exchange of knowledge and information, allowing
access to this resource at any place and at any time. The Internet allows us to carry out
any type of activity traditionally associated with other technologies, and also allows us
to interact with it in a passive way, as well as with other means such as television, or to
do it in a more active way. Now the user can interact with such media unlimitedly [4].
Thus, we can highlight that in recent years there was been a boom in the use of
hypermedia materials in the instruction of science, getting to assign to these materials
the role of catalysts of a change in teaching, and is able to help the shortcomings of
textbooks in terms of interactivity, dynamism and three-dimensionality [5].
At the same time, the concept of prosumer, deﬁned by José Octavio Islas (2008),
was anticipated technologies. Marshal McLuhan and Barrington Nevitt (1972) asserted
in his book “Take Today”, that in today’s world technology electronics would access or
allow consumers to simultaneously hold the roles of producer and consumer of digital
content, thereby making it possible to question the role of technology; and if this would
be the one that has driven the consumer or user to become an Internet prosumer [6].
In fact, prosumist activities are part of the so-called “hidden economy”, also known
as “invisible”; that is, non-monetary. Despite the condition of not being remunerated,
the execution of these activities generates some kind of welfare and/or beneﬁt, for the
different agents that develop them and so for society [7].
For Gómez et al., the prosume of the third wave is immersed in the development of
the universities, the university faces digital native classrooms that request a new type of
education [8]. The new technological tools (social networks, blogs, video platforms,
and others) have given them the power to share, create, inform and communicate,
becoming a primary element in their lives. In this way, the applications or social media
arising from Web 2.0, involve the active participation of users, becoming both pro-
ducers and consumers. Some of its own characteristics, such as collaboration, free
dissemination of information or generation of own contents for the foundation of
knowledge have been applied immediately to the educational ﬁeld [9].
162
E. Bohórquez et al.

With Web 2.0 a new dynamic of production-publication-consumption of contents
was born in all the spaces of Internet. According to Anderson (2007), the Web 2.0 are
presented with a mark that gives priority to the contents generated by the users, the
sharing of data and content and the collaboration [10].
Nevertheless, the third-wave prosum is also involved in business development,
Magdaleno & García are mentioned that prosumers would do more than a simple
personalization of the products and services consumed [11]; would add value
throughout the product life cycle, starting with idea and design and reaching beyond
the original idea to create innovative secondary markets.
Monetary economy and its non-monetary counterpart, form together the system of
wealth creation, and once united, a new fact is evident, and the monetary system is
going to expand dramatically. But what we do without money, will have a growing
impact on what we do with money. Prosumers are the anonymous heroes of the
economy that is falling, according to Garcia (2012) criteria [12].
Hence, prosumers are buyers of goods, despite the fact that their work is not
economically compensated, they sell products, services and knowledge and thus add
value when they act without an economic purpose and promote innovation, by gen-
erating knowledge and distribute the workforce [13].
Finally, the prosumers are the hypertex producers, the need to express themselves
and communicate with their peers is the engine that leads these young people to create
content from and for the network [14, 17], in addition the navigation form or com-
munication, are deﬁned in two classes: the common user, that is subdivided in Chatters
and Surfers and the hipertex producer or prosumer, in third place are the computer and
cognitive resources that they use; according to the use of different programming
techniques or the mode of action, we can ﬁnd that the hypertext producer, are subdi-
vided into: hackers, webmasters, fotologgers and forum users. In addition, Tapscott &
Williams (2008) mentions that a new prosumer model exists, where customers par-
ticipate in the creation of products in an active and permanent way, it is known as
MASHUP [15].
3
Study Case
The main purpose of this research is to identify the causes that lead to a low generation
of prosume on the web in learning websites and social networks, as well as to know
characteristics and preferences about the use of ICTs by the market segment of uni-
versity students.
In this sense, a research was predominated by a quantitative approach and a
descriptive and explanatory type of study, because the characteristics of young
third-wave prosumers are described by means of graphs and by explaining the
cause-effect relationship of the third wave and why a low generation of prosume on the
web in Canton La Libertad and Salinas.
The population selected for the present investigation, are persons between 15 and
24 years of age from Canton La Libertad and Salinas, which corresponds to 31,116
people. The sample of the study corresponds to 379 young people who study in the
university and they become the potential consumers of Internet of the Canton La
Web Prosumers: The Intangible Wealth of Education and Economics
163

Libertad and Salinas, for which a margin of error of 5% and a conﬁdence level of 95%
was considered.
A survey was used as a research instrument, with which it was possible to respond
to the objectives of the present study. The survey consisted of 23 direct closed polit-
omic questions, using variables from previous studies about the prosume in the web in
the commercial and educational ﬁeld. In addition, it is composed of 2 parts, one with 10
items of socio-demographic variables and uses of technologies and another with 13
items with variables of the use of social network and learning web portals, among
which are mentioned: Twitter, Facebook, Instagram, YouTube, In Linked In, Google+,
Blogger, Wikipedia, SlideShare and Scribd.
In addition, we used the cause-effect diagram, also known as Ishikawa diagram or
ﬁshbone diagram, which is a graphic representation showing the relationship between
an effect (problem) and all the possible causes that cause it, allowing identiﬁcation and
classify them for analysis. It should be mentioned that the problem can come from
different areas, such as production, social phenomena, health, organization, etc. In this
way, the problem (effect) that is the existence of a low generation of prosume in the
web in the Canton La Libertad and Salinas was deﬁned clearly, later described the
causes that could be inﬂuencing the problem and regrouped them in four main cate-
gories are: social, time, users and technology, then the diagram of conformity to what
was detailed by García (2006) [16] and ﬁnally they analyzed the inﬂuence that the
causes have on the problem to obtain solutions that would allow to increase this
invisible wealth of the economy.
Applying the described methodology, we have obtained the characteristics and
preferences that web prosumers have in learning websites and social networks.
It is noted that a large majority of users have access from the home with 46.4%,
followed by 22.7% who mention having access to the internet from their mobile phone,
while 21.4% indicate access from the university and only 6.3% access from a cyber
(Fig. 1). We can analyze that the main place where respondents always access the
internet is from their homes, possibly because that is where they have a computer and
could be more comfortable to surf in the web.
Fig. 1. Place where they access the Internet
164
E. Bohórquez et al.

Figure 2 shows users’ opinions about how they are considered in relation to the use
of new technologies. In this way, 42.1% considered innovative, followed by 31.7%
who say it is traditional/conservative. 16.7% of them mention being totally traditional/
conservative. As mentioned above, we can realize that there is almost equality between
users in how they are considered in relation to the use of new technologies, because
half of them tend to be considered as traditional while the other half are considered as
innovators.
Figure 3 shows the criterio of qualiﬁcation given to the use of Internet by the users,
the reason why the students generally use the Internet, in this way it is concluded that
9.1% and 16.9% use it a lot for the socialization and study respectively; it can also be
mentioned that 16.9%, 13.8% and 11.5% use it normally for socialization, study and
work respectively and ﬁnally it can be pointed out that with 7.6% and 6.3% use it little
Fig. 2. New technologies use.
Fig. 3. Criteria of qualiﬁcation: use of Internet.
Web Prosumers: The Intangible Wealth of Education and Economics
165

and nothing respectively for work activities. Analyzing the data and having as reference
the criteria between much and normal, the main use that users use the Internet is for study,
referring mainly to the fact of searching and downloading information (consuming).
As for the reasons why users have not been able to probe on the web whether in
social networks or learning websites, in this way, it can be evidenced that the vast
majority of them with a 55. 0% would like to produce content, but the reason they do
not do it is that it consumes them a lot of time; followed by a 21.8% that mentions not
being interested in creating or uploading information on the web; and ﬁnally 18.7%
indicate who do not know how to do it (produce on the web) (Fig. 4).
Fig. 4. Difﬁculties in producing-consuming: on learning websites and social networks.
Fig. 5. Cause-effect diagram.
166
E. Bohórquez et al.

Thus, once the main characteristics of third-wave prosumers have been described,
the cause-effect diagram is shown (Fig. 5), where the independent variables known as
causes are deﬁned, which will provide the necessary information of why young uni-
versity students have a low production on the web in both learning websites and social
networks.
4
Conclusions and Recommendations
The vertiginous change in information and communication technologies has allowed a
scenario in which only a few had the possibility of interacting, to another scenario in
which many have the opportunity to produce and share their knowledge for the rest of
the world.
The present research has reviewed the characteristics and preferences that univer-
sity students have at the time of consuming and/or producing on the web, which has
helped us to establish the various causes that cause a low generation of prosume of the
third wave both on social networks and on learning websites. It is possible to conclude
that there is little production or contribution on the web by young university students,
highlighting as causes: lack of interest; and the lack of knowledge in producing on the
web.
In order to accept that web portals form an essential part of the tools of the future
for active and collaborative learning, it is necessary to establish an awareness of these
web portals as educational resources. Its essential that the governments establish
educational policies that include and promote educational policies aimed at the
inclusion and teaching of web tools, together with the support of higher education
institutions, through the teaching faculty motivating to its students to explore with an
educational purpose. In this sense, teachers should increase the tasks of searching for
educational content on learning websites, and promote the development of production
and participation habits, which can be raised through the idea of publishing on the web
the best contents performed by the students, or competitions between universities to
reward the best content productions. Consequently, universities should leverage tech-
nology and the internet to libraries and laboratories, to count on ﬂexible schedules, so
that students have the possibility to consume and produce content on learning websites.
In a globalized world, where young people represent the future of mankind, it
becomes predominant to effectively combine the production and consumption of
educational and social content, being the reference to get students and users in general
motivated (consumer) and later (producer-publisher) in educational or cultural works,
as well as to interact with the contribution of designs and improvements to the products
and/or services offered by companies, and in this way increase this intangible wealth
(knowledge) of the economy.
Web Prosumers: The Intangible Wealth of Education and Economics
167

The present research has some limitations, the socio-economic and cultural factors
of the study population, which, in our opinion, may limit the possibility of generalizing
the results in other contexts. Future research could identify the causes that makes
difﬁcult to generate presume on the web, in different markets and socio-economic and
cultural levels. Likewise, we hope that the proposals to increase the intangible wealth
of the economy will be analyzed and implemented by the governments of the different
countries and implemented and promoted by higher education institutions and com-
panies or businesses, thereby justifying the results of this research.
References
1. Tofﬂer, A.: La Tercera Ola. Plaza & Janes S.A, Bogotá (1980)
2. Valverde, D., González, J.: Búsqueda y selección de información en recursos digitales:
Percepciones de alumnos de Física y Química de Educación Secundaria Obligatoria y
Bachillerato sobre Wikipedia. Revista Eureka sobre Enseñanza y Divulgación de las
Ciencias, 13(1), 67–83 (2016)
3. Letichevsky, S.: La Deslumbrante Percepción de Alvin Tofﬂer. In: El Catoblepas. http://goo.
gl/GU1D85. Accessed 01 Nov 2004
4. Martínez, R., Cabecinhas, R., Loscertales, F.: University senior students on the web.
Comunicar 19, 89–98 (2011)
5. Garritz, A.: La Enseñanza de la Ciencia en una Sociedad con Incertidumbre y Cambios
Acelerados. Enseñanza de las Ciencias, 28(3), 315–326 (2010)
6. González, R.: El prosumidor de Internet, Santiago de Chile (2013)
7. Ortegón Clavijo, C., Serna Mendoza, C.: Desarrollo ciudadano. Una aproximación
conceptual y metodológica (2015)
8. Gómez, M., Roses, S., Farias, P.: El uso académico de las redes sociales en universitarios.
Comunicar, 28(3), 315–326 (2012)
9. De Haro, J.: Educar para la comunicación y la cooperación social. Redes sociales en
educación, 27, 203–216 (2010)
10. Sarsa, J.: El perﬁl prosumidor de los estudiantes en la web 2.0. J. Educ. Teach. Trainers, 5
(2), 74–87 (2014)
11. Magdaleno, M., García, J.: Crowdsourcing: la descentralización del conocimiento y su
impacto en los modelos productivos y de negocio. Cuadernos de gestión, 14(2), 33–50
(2014). Oviedo
12. Gracia, C.: El pensamiento un camino aún por recorrer, Bogotá (2012)
13. Bocanegra, C.: Reseña de “La revolución de la riqueza” de Alvin Tofﬂer y Heidi Tofﬂer.
Región y Sociedad, XXI(44) 241–246 (2009)
14. Medina, D.: Tendencias y estilos de vida del universitario a partir de su consumo cotidiano
(marcas y productos) desde sus habitaciones (principales universidades de Cali) Segundo
estudio redes sociales virtuales y blogs, herramientas para Insight, Santiago de Cali (2013)
15. Tapscott, D., Williams, A.: Wikinomics: How Mass Collaboration Changes Everything.
Portfolio, New York (2008)
168
E. Bohórquez et al.

16. García, N.: Técnicas para evaluar alternativas. Aula de la farmacia: revista profesional de
formación continuada, 25–31 (2006)
17. Salvador, P., Rocha, Á.: An assessment of content quality in websites of basic and secondary
portuguese schools. In: Rocha, Á., Correia, A., Tan, F., Stroetmann, K. (eds) New
Perspectives in Information Systems and Technologies, vol. 1. Advances in Intelligent
Systems and Computing, vol. 275. Springer, Cham (2014)
Web Prosumers: The Intangible Wealth of Education and Economics
169

Analysis of Correspondences Applied
to Vehicle Plates Using Descriptors in Visible
Spectrum
Shendry Rosero(&) and Alberto Jimenez
Escuela Superior Politécnica del Litoral, Vía Perimetral 5, Guayaquil, Ecuador
{shrosero,ajimenez}@espol.edu.ec
Abstract. Recognition of regions is consolidated as a branch of the computer
vision that so far no limitation, new developments are being developed every
day methods that allow more or less precision; distinguish points, areas and
elements of interest both in photographs as well as video. There are currently
several comparative studies which focus on analysis of descriptors on images
without regions, so the proposal of this study intended to show a comparative
analysis of the methods and algorithms that are more robust with respect to
descriptors focused on the detection of license plates, in the end we obtain
values of robustness that they compare with studies of correspondence previous
ones.
Keywords: Descriptors  Detectors  SIFT  SURF  ORB  BRISK
BRIEF  FAST  AST  MSER  FREAK
1
Introduction
The quality of the detection and description of characteristics and areas of interest in
images is determined by the descriptor and detector used the same that can be classiﬁed
independence of its invariability on certain factors such as rotation, scale changes or
amount of noise that may be present in the image.
Descriptors such as Scale-Invariant Feature Transform (SIFT) proposed by Lowe
[1] and considered one of the oldest, Maximally Stable Extremal Regions (MSER)
proposed by Matas et al. [2], Features From Accelerated Segment Test (FAST)1
proposed by Rosten and Drummond [3], Speeded-Up Robust Features (SURF) pro-
posed by Bay et al. [4], Center Surround Extremas For Realtime Feature Detection And
Matching (Censure) proposed by Agrawal et al. [5], Binary Robust Independent Ele-
mentary Features (BRIEF) developed by Calonder et al. [6], Binary Robust Invariant
Scalable Key-points (BRISK) proposed by Leutenegger et al. [7], Oriented FAST and
Rotated BRIEF (ORB) by Rublee et al. [8], and the most recent Fast Retina Key point
(FREAK) by Alahi et al. [9]. They are examples of the diversity of techniques that
could be used for the detection of regions.
Previous comparative studies [11, 12] have been applied to objects in general
analyzing the amount of correspondences that can be detected in images subject to
variations.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_17

The present study proposes a comparative analysis of four descriptors: SIFT,
SURF, BRISK and ORB applied to the detection of vehicle plates for it was considered
a set of images belonging to a L. Dlagnekov and S. Belongie, “UCSD/Calit2 Car
License Plate, Make and Model Database,” from http://vision.ucsd.edu/car_data.html
which consisted of captures of license plates of 291 parked cars that were subjected to
different changes with respect to rotation, scale and addition of noise, with a work
dataset of 22396 images.
The study is divided into three sections, Introduction (with subsections A, and B),
Methodology and Presentation of results.
2
Methodology
The present study proposes a comparative and quantitative analysis of four descriptors:
SIFT, SURF, BRISK and ORB applied to the detection of vehicle plates. We con-
sidered a set of images belonging to L. Dlagnekov and S. Belongie, “UCSD/Calit2 Car
License Plate, Make and Model Database,” downloaded by permission of the owner
from http://vision.ucsd.edu/car_data.html. The block of pre-processed images belonged
to captures of license plates of 291 cars parked in JPG format with a size of 400  220
pixels, which were applied changes with respect to rotation, scale, addition of salt and
pepper, as well as Gaussian noise and a Gaussian combination for Blur, totaling 22
transformations that resulted in a work dataset of 22396 images.
The procedure performed on each image consisted of two phases: a simple pre-
processing phase through the application of Matlab that allowed to transform each
original image into an image with differences in size, scale and changes in the amount
of noise present in the image (see Fig. 1).
Fig. 1. Example of variations of car image with respect to rotation, scale and addition of noise
Analysis of Correspondences Applied to Vehicle Plates
171

A second phase was applied by using Python and OpenCv for the execution of
descriptors such as SIFT, SURF, BRISK and ORB, prior to obtaining the same
key-points that were compared for each original image in order to obtain the number of
correspondences by each descriptor with [11, 12]
C ¼ # of correct correspondences
# of correspondences
ð1Þ
Where “C” is the number of accounted correspondences.
3
Results
After the execution of the algorithm of processing and evaluation of correspondences
between images, it can be observed in Tables 1 and 2, that the ORB descriptor is
presented as the most robust with respect to the other descriptors considering Rotation,
Blur, application of Salt and pepper, and Gaussian noise; while for size changes, the
SIFT descriptor is presented as robust, for scale changes greater than 30%.
Graphically, these same results can be seen in Figs. 2, 3, 4, and 5.
Table 1. Comparative table of the correspondences between images with respect to applied
descriptors
Rotate
Blur
30
45
60
75
90
110
160
180
1
2
SURF
21.44 21.96 21.93 30.2
85.73 25.67 25.98 77.59 62.32 31.82
SIFT
59.13 59.46 58.28 56.15 80.92 57.5
57.12 72.96 49.83 21.14
BRISK 39.88 41.12 39.88 43.27 62.92 40.21 36.65 52.23 38.27 10.12
ORB
65.99 66.45 65.66 67.51 88.03 66.82 66.86 85.43 71.21 33.83
Table 2. Comparative table of the correspondences between images with respect to applied
descriptors
Salt & Pepper
Gaussian
Scale
0.1
0.15
0.2
0.1
0.2
0.3
0.4
30%
70%
130% 170%
SURF
21.03 14.87 11.07 36.45 28.98 20.39 13.29 16.64 42.12 57.51 53.67
SIFT
27.82 20.43 16.28 41.25 35.61 28.41 21.05 16.46 52.96 73.12 70.98
BRISK 17.8
12.39
9.59 31.99 25.3
16.73
9.24
4.98 26.6
34.9
35.91
ORB
60.31 60.26 60.15 69.15 62.93 55.61 48.18
0.43 45.89 69.87 69.32
172
S. Rosero and A. Jimenez

Fig. 2. Analysis of correspondences with respect to changes of rotation in the image, in relation
to the original image.
Fig. 3. Analysis of correspondences with respect to scale changes in the image, in relation to the
original image.
Fig. 4. Comparisons between descriptors for noise addition by blurring (BLUR)
Analysis of Correspondences Applied to Vehicle Plates
173

4
Discussions
Considering the classiﬁcation made by Dibyendu Mukherjee et al. [10], as shown in
Table 3. In which a slight comparison of the descriptors versus their function as
detectors was selected for the present study to: BRISK, ORB, SIFT and SURF for its
characteristic of both descriptor and detector.
According to the conclusions made by Mukherjee et al. [10], who experimented
with the performance of feature detectors and descriptors through various image
transformations such as blurring, rotation, scaling, and changes of point of view by
degrees, slight differences were obtained with the present study since they consider
SIFT, (with its own descriptor) as the most robust. Mentioned result agrees with the
study of Ricaurte et al. [11, 12], whose approach is presented in Table 4, which differs
from the present study whose results show ORB as more robust for rotation, blurring
and gaussian changes; coinciding only in the SIFTs for the variations of scale.
The difference can be marked by aspects such as the quality of the images and the
scenarios shown by these, consider that the results shown in this writing are limited to a
region of interest that applies to the vehicle’s license plate.
Fig. 5. Comparisons between descriptors for addition of Gaussian noise
Table 3. Comparison between algorithms whose characteristics deﬁne it as a detector and a
descriptor, taken from “A comparative experimental study of image feature detectors and
descriptors” by Dibyendu Mukherjee et al.
BRISK ORB SIFT SURF
Detector
Yes
Yes
Yes
Yes
Descriptor Yes
Yes
Yes
Yes
174
S. Rosero and A. Jimenez

5
Conclusions
The process of recognition of regions of interest related to the location of license plates
in vehicles is affected by several factors, including the possible movement of the
vehicle in the horizontal plane (moving away and approaching) and in the vertical
plane, adding to it possible noises present in the cameras at the moment of capturing the
image to be processed.
Hence the present study was concerned with estimating the correspondences of the
main descriptors with importance in variations of size, rotations and inclusions of
noise; once the study is completed, it is presented to ORB and SIFT as robust for
changes of rotation and scale respectively, while SURF appears as one of the weakest
when obtaining correspondences for the case of rotations and noises. The data collect
for BRISK instead are diverse.
Once determined the potential descriptors to be used, alternatives can be proposed
to perform the search of the signature of the car license plate in the image, while the
interpretation of the numbers would require the application of neural networks, since
the methods that resort to detectors and descriptors and their invariance to the
parameters mentioned above, would focus more on the detection of the quadrant of the
plate, take into account that the next step is the detection of the numerical record and
this maintains some design challenges among which are:
The differentiation that there are between the types of plate by function of the
automobile and the color that represents them, the size of the plate (variation by
country) and the diversity of resolutions of surveillance cameras used. In this sense, it is
recommended as future work the studies on the automatic detection of plates based on
“The Algorithm for Cars License Plates Segmentation” [13] that rests on algorithms of
segmentation of plates on a complex background, invariant to its size, contrast and
positioning of the image, as well as methods such as “A cognitive and video-based
approach for multinational License Plate Recognition” [14] that would allow overcome
the challenge of design mentioned above on the difference of sizes.
In addition to this, options such as “Multi-scaled license plate detection based on
the label-moveable maximal MSER clique” [15] as it considers several lighting situ-
ations, and similar proposals such as “An Adaptive Vehicle License Plate Detection at
Higher Matching Degree” [16], which also applies multiple segmentation methods as it
considers several lighting situations, and similar proposals such as “An Adaptive
Vehicle License Plate Detection at Higher Matching Degree” [16], which also applies
methods of multiscale segmentation.
Table 4. Comparison of descriptions of Ricaurte descriptors in visible spectrum, with the
present study
Effect
Ricaurte et al. Rosero, Jimenez
Rotate
SIFT
ORB
Scale
SIFT
SIFT
Blur
SURF
ORB
Gaussian ORB
ORB
Analysis of Correspondences Applied to Vehicle Plates
175

On the other hand, there are proposals such as “Ensemble Haar and MB-LBP
Features for License Plate Detection” [17] as combinations of weights and rules using
characteristics of Haar and MB-LBP, which presents very good results when recog-
nizing the quadrant the plate.
In addition, we suggest approaches such as “License Plate Extraction Using
Spiking Neural Networks”, Advances in Neural Networks: Computational and Theo-
retical Issues [18], and Robust scene text detection with convolution neural network
induced MSER trees [19], among the most representative proposals.
References
1. Lowe, D.G.: Object recognition from local scale-invariant features. In: The Proceedings of
the Seventh IEEE International Conference on Computer Vision, vol. 2. IEEE (1999)
2. Matas, J., Chum, O., Urban, M., Pajdla, T.: Robust wide baseline stereo from maximally
stable extremal regions. In: Proceedings of British Machine Vision Conference, pp. 384–396
(2002)
3. Rosten, E., Drummond, T.: Machine learning for high-speed corner detection. In:
Proceedings of European Conference on Computer Vision, pp. 430–443 (2006)
4. Bay, H., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features. Comput. Vis.
Image Underst. (CVIU) 110(3), 346–359 (2008)
5. Agrawal, M., Konolige, K., Blas, M.: CenSurE: center surround extremas for realtime
feature detection and matching. In: Proceedings of European Conference on Computer
Vision, pp. 102–115 (2008)
6. Calonder, M., Lepetit, V., Strecha, C., Fua, P.: BRIEF: binary robust independent
elementary features. In: Proceedings of the European Conference on Computer Vision,
pp. 778–792 (2010)
7. Leutenegger, S., Chli, M., Siegwart, R.: BRISK: Binary Robust Invariant Scalable
Keypoints. In: Proceedings of IEEE International Conference on Computer Vision,
pp. 2548–2555 (2011)
8. Rublee, E., Rabaud, V., Konolige, K., Bradski, G.: ORB: an efﬁcient alternative to SIFT or
SURF. In: Proceedings of IEEE International Conference on Computer Vision, pp. 2564–
2571 (2011)
9. Alahi, A., Ortiz, R., Vandergheynst, P.: Freak: fast retina keypoint. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 510–517 (2012)
10. Mukherjee, D., Jonathan, Q.M., Wang, W., Wang, G.: A comparative experimental study of
image feature detectors and descriptors. Mach. Vis. Appl. 26(4), 443–466 (2015)
11. Ricaurte, P., Chilán, C., Aguilera-Carrasco, C.A., Vintimilla, B.X., Sappa, A.D.: Feature
point descriptors: infrared and visible spectra. Sensors (Switzerland) 14(2), 3690–3701
(2014)
12. Ricaurte, P., Chilán, C.: Correspondencia de características utilizando esquemas clásicos en
el espectro visible (2012)
13. Kryachko, A.A., Timofeev, B.S., Motyko, A.A.: The Algorithm for Cars License Plates
Segmentation 2 Physical Aspects of the License Plate Recognition Systems, pp. 577–590
14. Thome, N., Vacavant, A., Robinault, L., Miguet, S.: A cognitive and video-based approach
for multinational license plate recognition. Mach. Vis. Appl. 22(2), 389–407 (2011)
15. Gu, Q., Yang, J., Kong, L., Cui, G.: Multi-scaled license plate detection based on the
label-moveable maximal MSER clique. Opt. Rev. 22(4), 669–678 (2015)
176
S. Rosero and A. Jimenez

16. Prates, R.C., Schwartz, W.R., Menotti, D.: An Adaptive Vehicle License Plate Detection at
Higher Matching Degree, pp. 1–8 (2014)
17. Pan, Q., Shen, J., Yang, W., Sun, C.: Ensemble Haar and MB-LBP features for license plate
detection. Lecture Notes in Computer Science (Including Subseries Lecture Notes in
Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), vol. 7751, pp. 223–230 (2013)
18. Borghese, N.A., Lanzi, P.L., Mainetti, R., Pirovano, M., Surer, E.: Advances in neural
networks: computational and theoretical issues. Smart Innov. Syst. Technol. 37, 243–251
(2015)
19. Huang, W., Qiao, Y., Tang, X.: Robust scene text detection with convolution neural network
induced MSER trees. Lecture Notes in Computer Science (Including Subseries Lecture
Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), vol. 8692, no. Part 4,
pp. 497–511(2014)
Analysis of Correspondences Applied to Vehicle Plates
177

Alignment of Software Project Management
with the Business Strategy in VSEs: Model
and Evaluation
Carlos Montenegro1(&) and Geovani Barragán2
1 Escuela Politécnica Nacional, Quito, Ecuador
carlos.montenegro@epn.edu.ec
2 INCOTEC Services, Quito, Ecuador
geovani.barragan@incotecservices.com
Abstract. Using DSR approach, this paper proposes the design and evaluation
of a model type artifact, for the alignment of software project management with
business strategy in Very Small Entities (VSE). The model design phase
establishes the alignment strategy and processes to value generation, as well as
their indicators, using IT best practices combination. The validation phase is
done through the model application in a case study, with three software projects
with different levels of complexity and progress, into an Ecuadorian VSE. The
Case Study evidences the following factors with the most signiﬁcant effect:
value generation assurance, application of alignment processes and procedures;
and, early adoption of the model. So, the work contributes providing a practical
tool for the practitioners and on the expansion of the existing body of knowledge
concerning the alignment of Software Projects with the Business Strategy.
Keywords: Software project management  Value generation
Alignment model  Very small entities (VSE)  DSR
1
Introduction
The importance of alignment between business and information technology (IT) for
generating business value, has been emphasized in many research. According to
Wagner and Moshtaf [1], IT does not automatically create business value; people create
value by using it. Moreover, this is the point where business-IT alignment becomes an
issue. Regarding business value through IT, the work of Martínez [2] ﬁnds that the
current measure of value is focused on processes associated with the value chain.
According to Gutierrez [3], the IT strategy implementation is realized through Infor-
mation Systems (IS) projects, mainly using a traditional project management approach.
However, the traditional project management is task oriented in a temporary organi-
zation; so, according to Biorn and Saeed [4], the projects rarely fail due to the technical
problems but majorly fail due to organizational challenges.
About project management, Muhammad [5] consider it as an essential means to
implement a corporate strategy, but how it happens in practice is rarely the subject of
detailed examination. Srivannaboon and Milosevic [6] recognize the strategic
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_18

importance of project management, which impacts on the adaptation of the business
strategy, derivated of signiﬁcant threats detected due to environmental changes.
In turn, Biorn and Saeed [4] and Srivannaboon [7] pose, as the primary objective of
the execution of a project, the fulﬁllment of one or several organizational strategies.
Also, Srivannaboon mentions that although projects are the core building blocks of
organizational strategy in many companies, project management is not often recog-
nized as a functional strategy and is rarely perceived as a business process, achieving a
project management/business strategy alignment even more challenging.
The described context shows that the alignment of project management with
business strategy can be seen as an organization critical success factor, which has not
been treated in detail. Therefore, the understanding of the meaning and the adequate
processes and mechanisms implementation, constitute a research opportunity that
contributes to cover the current gap in the Business-IT Alignment issue for Software
projects.
2
Related Work
According to Marinho et al. [8], successful projects generate beneﬁts, increasing the
possibility of creating value in their businesses. The research of Hjelmbrekke et al. [9]
reveals that the business model of the projects design team focuses on efﬁciency rather
than on the client’s strategic objectives; this entails a need for project governance
functions. According to Westphal et al. [10], the assessment of potential value gen-
eration is regarded as a collaborative process, and collaboration capabilities are
required.
Serra and Kunc [11] consider that organizations fail to implement their strategies,
despite the use of project management techniques, programs, and portfolios. According
to this study, the Beneﬁts Realization Management (BRM) ensures the implementation
of the most valuable initiatives.
For Too and Weaver [12], a framework is required that guides organizations in
developing effective project governance to optimize project management. Kaiser et al.
[13] state that project portfolio management (PPM) is a technique commonly used to
align a portfolio of projects with strategic objectives.
Concerning the management frameworks, PMBOK [14] relates the creation of
value with the generation of utility or wealth over a period. COBIT 5 [15] presents
Value Creation as the primary goal of IT Governance, through Maximization of
Beneﬁts, and Optimization of Risks and Resources. MSF [16] provides a set of ﬂexible
concepts, models, and practices for the planning, development, and management of
technology projects, focusing on process models and work teams. Finally, COBIT
mapping [17] describes the mapping of its processes with PMBOK.
Regarding Software Project Management, the work of Shefﬁeld and Lemétayer [18]
reveals that the development agility is inﬂuenced by a factor of the project environment
(organizational culture) and a factor of the project (empowerment of the project team).
Ebert [19] insists, among others factors, on the standardized product lifecycle gover-
nance, and on the Portfolio Management.
Alignment of Software Project Management
179

For Very Small Entities (VSE), O’Connor and Laporte [20, 21] deal with project
management based on the ISO/IEC 29110 standard [22]. The ISO/IEC 29110 state that
project management must use the client software work order to develop the project
plan. The evaluation and control tasks compare the actual plan execution, and activities
are realized to correct deviations and/or incorporate changes to the project plan. The
project closing provides the software conﬁguration and obtains the acceptance of the
client to formalize the process. de Vasconcelos et al. [23] consider that in VSE,
planning, and communication between stakeholders is crucial for effective collabora-
tion through different stages of software development.
It is notorious the existence of various factors to the alignment of projects and
business strategy. This study uses a different context, through the Value generation and
delivery. From IT practice perspective, a mechanism to IT value delivery is the ade-
quate implementation of management/governance processes using standards and
guidelines, whose use promises to improve quality and contributes to requirements
compliance [15, 24, 25]. According to Oud [26], the existence of a vast number of
standards and guidelines enables organizations to use one in particular, or choose
several of them in a given situation; and since there is no universal model, it is
necessary to combine models that complement and/or to improve the details together.
3
Methodology
Design Science (DSR) is a research approach developed through the last decade
[27–29]. It has been used in several domains: Information Systems [30], Business
Processes Management [31] and IT Management [32], among others. According to
Hevner and Chatterjee [28], DSR constitutes a pragmatic research paradigm that
encourages the creation of innovative artifacts to solve real-world problems. Thus,
DSR combines a focus on IT artifacts with a high priority for its relevance in the
application domain.
DSR follow a two-phase approach: Design and Evaluation. The criterion for model
design as a search process is a top-down design, where, as relevant, successively
speciﬁc models and best practices for the new artifact, are included. In the Evaluation
Phase, as a DSR option [27], the observational method of Case Study is used.
Also, DSR provides a stage of feedback for the designed artifact [27, 30]. For effect,
contributed the qualitative technique of Participant Observation: the experiences
observer-as-participant were taken and analyzed [33, 34]. One researcher on partial
time acted as a participant in the generation and explanation of the model, and in
planning implementation activities; and, additionally, one researcher on full time,
served as an observer in decisions of application and the operational activities, carried
out jointly with the Company PMO personnel.
180
C. Montenegro and G. Barragán

4
Design Phase: Software Project Management Model
While it is true that COBIT, PMBOK, and MSF are essential sources of Value Gen-
eration concept, a more global deﬁnition is needed to guide the identiﬁcation of its
mechanisms. With this aim, is established an analogy with the work of Webb related to
IT Governance deﬁnition [35]. Based on the references Willmot [36], Martínez [2],
Ruedas [37], PMBOK [14], COBIT5 [15] and MSF [16], are considered twenty-four
aspects/terms referenced by at least three sources. Combining them with an approach of
coincidences and complementarities, the referential deﬁnition of Value Generation is:
“Software Project Management generates Measurable Organizational Value (MOV),
through tangible and intangible elements aligned with the Business Strategy, the
Government Goals, and the Objectives; by meeting the needs of stakeholders; gener-
ating and/or maximizing beneﬁts; optimizing risks and resources, within a period”.
As a strategy to implement the deﬁned value generation of Software Project
Management, the integration and combination of COBIT 5, PMBOK, and MSF have
been considered. The Model Phases are equivalent with the Project Management
phases of ISO/IEC 29110, and MSF, excepting the Post Phase (Support, Maintenance,
Monitoring, and Evaluation), that assure the value generation, and it is measured by the
MOV compliance.
The application of the model can start in any phase, preferably in the Vision and
Planning of the Project. Given the spiral paradigm with iterations, if the project is
already in advanced stages, the application can be started at the next Iteration/ Phase.
Table 1 summarizes the model processes, tasks, and activities; the abbreviations
and numerals correspond to the original documentation of the frameworks.
4.1
Indicators Deﬁnition
For the measurement and analysis of the Value Generation of Software Projects, a set
of quantitative indicators are deﬁned, on a scale from 0 to 1; and, Qualitative indicators,
with levels Bad, Fair, Good, Very Good, Excellent.
Project Management Indicators. Table 2 summarizes the Project Management
indicators, taking as reference the model phases. They are additional to standard Earned
Value.
Model Management Indicators. Table 3 shows a set of indicators to evaluate and
analyze the model outcomes, based on the current state and the results of the projects.
For the satisfactory alignment of the management model, all indicators should have
at least the accepted value and the majority close to the ideal value.
Alignment of Software Project Management
181

Table 1. Summary of processes of the software project management model
Phase
Process/Task/Activity
COBIT5
PMBOK
MSF
Vision
a. Business Case
elaboration
Business
Case
i. Ensure alignment with
Strategic Objectives
Goal
Cascade
ii. Establishment of the
framework
EDM01
iii. Ensuring transparency
toward stakeholders
EDM05
iv. Establish Measurable
Organizational Value
(MOV)
MOV
Business
Value
v. Risk identiﬁcation and
analysis
EDM03
b. Business Case
Evaluation
vi. Analysis of alternatives
• Meet the stakeholder
needs
Principle 1
13.1
vii. Selecting an alternative
c. Requirements
Compilation
5.2
d. Vision and scope
deﬁnition
5.3
Vision, Scope,
Requirements,
Risk
e. Project Constitution
Minutes
4.1
Minutes
Planning
a. Create WBS
5.4
b. Deﬁne and organizing
the activities and resources
EDM04
6.2, 6.3
c. Estimate times and
resources
6.4, 6.5
d. Prepare the schedule
6.6
Project
schedule
i. Functional deliveries
deﬁnition
Spiral
paradigm
ii. Milestone setting
Processes
Model
e. Risk Management Plan
11.1
f. Stakeholder Management
Plan
13.2
g. Budget preparation
7.2, 7.3
h. Project management Plan
4.2
(continued)
182
C. Montenegro and G. Barragán

Table 1. (continued)
Phase
Process/Task/Activity
COBIT5
PMBOK
MSF
Development
• Project execution
a. Lead the Project work
4.3, 9.2,
9.4
b. Stakeholders
participation
13.3
c. Technical and functional
documentation
Technical
documentation
Stabilizatión
a. Scope validation
4.5, 5.5
Change control
b. Assurance and quality
control
8.2, 8.3
Test and
Changes
documentation
c. Version approval
Technical
documentation
Deployment
a. Deployment Plan
Deployment
Plan
b. Version approval
Minutes
Project/phase
closure
a. Close and start new
phase or project
4.6
i. Delivery and project
completion
Minutes
ii. Analysis of not
contemplated requirements
Record
iii. New
Phase/Requirements?
(Beginning of a new phase)
Vision and
Scope
Support and
maintenance
a. Support, adjustments,
corrections according to
Vision and Scope
Adjustments
register
All phases
a. Monitoring (Applies to
development, stabilization,
deployment)
i. Monitor and control the
scope, schedule, costs
4.4, 5.6,
6.7, 7.4
ii. Risk monitor and control
EDM03
11.6
Agility, Risk
and Change
Control
b. Learned lessons
documentation
Learned
lessons
Evaluation
a. Verify MOV compliance
X
Alignment of Software Project Management
183

Table 2. Project management indicators
Phase
Indicator
Vision
AIBS (Alignment Index with the Business Strategy) = #Project Business
Objectives/#Organization Strategic Objectives
Ideal Value 1, Accepted Value >= 0.8
VGI (Value Generation Index). It is a Qualitative index that measures the project
MOV statement, concerning the organization value deﬁnition.
Ideal Value: Excellent, Accepted Value: Good
Development
For monitoring and control in these phases, the indicators established by the
standard Earned Value are used
Stabilization
Deployment
FAI (Functional Acceptance Index) = #Functional Requirements Fulﬁlled/#Total
of functional requirements
Ideal Value 1, Accepted Value >= 0.8
TAI (Technical Acceptance Index) = #Technical Requirements Fulﬁlled/#Total of
Technical Requirements
Ideal Value 1, Accepted Value >= 0.8
Support and
maintenance
SII (Supported incident index): Number of supported incidents per month.
Ideal Value 0, Accepted Value <= 10
Evaluation
CI_MOV (MOV compliance index) = (MOV reached/MOV raised)  100%.
If CI_MOV < 70%, Failed Project. If 70% <= CI_MOV < = 90%, Partial success
If CI_MOV > 90%, Successful project
Table 3. Model management indicators
Indicator
Descriptión
IPABS
Index of Projects Aligned
with the Business Strategy
#Aligned Projects/#Total Evaluated Projects
Ideal Value 1, Accepted Value >= 0.9
IPGV
Index of Projects that
Generate Value
#Projects that generate Value/#Total Executed
Projects. Ideal Value 1, Accepted Value >= 0.9
SPI
Successful Projects Index
#Successful Projects/#Total Executed Projects
Ideal Value 1, Accepted Value >= 0.9
FPI
Failed Projects Index
#Failed Projects/#Total Executed Projects
Ideal Value 0, Accepted Value <= 0.1
PIT
Project Index on time
#On time Projects/#Total Projects Under
Execution
Ideal Value 1, Accepted Value >= 0.7
DPI
Delayed Projects Index
#Delayed Projects/#Total Executed Projects
Ideal Value 0, Accepted Value <= 3
RML
Risk Management Level
It indicates the level that reaches the Risk
Management with the model application
Ideal Value 1, Accepted Value >= 0.8
PML
Project Management Level
It indicates, in general, the level that reaches the
Project Management with the model application.
Ideal Value 1, Accepted Value >= 0.8
184
C. Montenegro and G. Barragán

5
Evaluation Phase: Case Study
INCOTEC Services is an Ecuadorian private VSE dedicated to the development of
software products and services. It is specialized in the ﬁnancial services development,
especially the processing of collections and payments with high volumes of informa-
tion. It maintains a PMO, which assigns ofﬁcials to support each project development.
As a component of the Strategic Plan, the company has established its deﬁnition of
Value Generation, through Software projects. The deﬁnition is the following: “Soft-
ware Project Management generates a MOV in a period, applying a ﬂexible and
dynamic model that ensures tangible and intangible elements are aligned with the
Government Goals and Objectives established in the Strategic Plan, contributing to the
satisfaction of needs and/or generating beneﬁts for our stakeholders, through the Risks
and Resources optimization” [38].
For the model application, a set of pilot projects is selected (Table 4). The size and
complexity are established based on the criteria by O’Connor and Laporte [21]. Also,
projects were taken in different phases of execution to verify the impact of the model
according to the stage in which its application begins. Likewise, projects of different
size and complexity are taken to validate that the model can be applied to various types
of software projects.
The application process of the model is summarized in Fig. 1, which shows inputs,
general process steps, controls, and outputs.
The Indicators are a signiﬁcant factor since they allow the monitoring and control
of the effects, impacts, and results, obtained in the model application process. The agile
documentation (based on standardized templates) and the learned lessons registration,
facilitate the continuous improvement process application; in the Case Study, the
baseline of learned lessons did not exist.
Table 4. Pilot projects for case study
Projects
A
B
C
Name
Electronic banking Basic services payments Massive business payments
Current phase Planning
Vision
Development
Team
10
8
6
Delivery time 10 months
6 months
4 months
Final users
700.000
300.000
50.000
Budget ($)
500.000
300.000
150.000
Project size
Big
Middle
Middle
Alignment of Software Project Management
185

5.1
Results
After the ﬁrst three months of the model implementation, the 2016 report of the PMO,
based on Earned Value indicators, mentions “… these three months has allowed us an
average compliance of 87% of the planned scope and schedule, compared to the
traditional 70%, registered in the company” [39]. It is also emphasized that “The
application of the model indicators, have achieved an effective control and a constant
monitoring of both the management of the model in general and the state and the
results that are obtained on each project.”
At the end of the six months of application of the model, the report recommends,
“Select three additional pilot projects and continue with the application of the software
project management model, for the additional six months, applying the ﬁrst lessons
learned during this initial stage” [40].
The fact is explained because of the alignment focus, evidenced by its Alignment
value, AIBS (0.95); and, in the value generation, IGV (1). Likewise, it can be observed
that project C has the lowest indicators, even below the acceptable minimum limit
(0.7), mainly because the Vision and Planning phases are not executed in the initial
iteration (Table 5). For this reasons, the average MOV compliance index (CI_MOV)
shows a partial success.
CONTROLS
Model Indicators
INPUT
Organization Environment 
description               
Stakeholders needs 
Strategic Plan, Goals and 
business Objectives                
Value generation definition   
Learned Lessons       
PROCESS
1. Execution and 
Control of the 
model processes 
2.Evaluation based on 
the model 
management 
indicators.
3.Implementation 
results analysis
OUTPUT
Success /Failure Level
Strengths and weaknesses 
Adaptations to the environment
Continuous improvement
Fig. 1. The model application process
Table 5. Case study projects management indicators
Indicators A
B
C
Average
AIBS
0.85 0.95 0.80 0.87
VGI
0.85 1.00 0.80 0.88
FAI
0.87 0.92 0.75 0.85
TAI
0.85 0.93 0.75 0.84
SII
0.84 0.88 0.67 0.80
CI_MOV
0.90 0.95 0.80 0.88
186
C. Montenegro and G. Barragán

On the other hand, Table 6 shows the summary of the model management indi-
cators. PIT and DPI indicators have not the accepted values, showing an improvement
opportunity for the following iterations. In this case, the C project is delayed and out of
time.
Table 7 shows a comparison a set of picked Indicators that, for the case study are
considered as signiﬁcant. They show the current alignment improvement due to the
model application in six months.
In summary, according to the described results, the model application demonstrates
his feasibility and utility. Other Case Study operational details can be found in the work
of Barragán [41].
6
Conclusions and Future Work
As DSR establishes, the designed model aided to the problem and solution under-
standing. In this sense, the selected case study with multiple projects has allowed
investigating a phenomenon within a real corporative context and is cataloged as a
representative to inferring generalizable knowledge to related cases. Accordingly, in the
Case Study, projects of different size and complexity are taken to validate the model.
Table 6. Model management indicators, over the period
Indicators
Value
Index of projects aligned with the business strategy IPABS 1.00
Index of projects that generate value
IPGV
1.00
Successful projects index
SPI
1.00
Failed projects index
FPI
0.00
On time project index
PIT
0.67
Delayed projects index
DPI
0.33
Risk management level
RML
0.80
Project management level
PML
0.84
Table 7. Selected indicators: Initial vs. Current
Issue
Indicator Initial Current
Strategic govern alignment AIBS
0.7
0.87
Value generation
VGI
0.6
0.88
Process and procedures
TAI
0.7
0.85
Project management
FAI
0.8
0.84
Risk management
SII
0.8
0.80
0.72
0.848
Alignment of Software Project Management
187

The model Indicators, as well as scales, the ideal values, and the accepted values,
should be taken as a reference and each organization can establish its parameters,
according to its environment, speciﬁc policies and/or needs. Likewise, the approach to
generating value that a particular project brings to the organization can be measured
regarding the expected beneﬁts, using the model indicators, mainly the MOV com-
pliance. Besides, if the model is implemented from the stages of visioning and plan-
ning, favorable values are obtained for the following stages, and even if it is adopted in
the intermediate development stages.
Considering the Indicators values and the related process and procedures, the Case
Study evidences the following primary factors with the most signiﬁcant effect in
alignment the business strategy with the software projects management: (i) Assurance
of the Value Generation, through the model application; (ii) Application of alignment
processes and procedures, mainly through the Goal Cascade mechanism; and,
(iii) Early adoption of the model.
Although the case study shows the applicability of the model in VSE, it can be
applied to medium and large companies, adding a higher formality in the processes and
procedures.
Finally, the research is related to studies concerning software governance.
According to Manteli et al. [42], the Software Development Governance is an
emerging ﬁeld of research, under the umbrella of IT governance. Baars and Jansen [43]
consider that many organizations do not adequately measure, compare or analyze their
governance policies for the software ecosystem management; and without sufﬁcient
governance knowledge, cannot optimally perform as critical players in their business.
These backgrounds evidence a current and future research opportunity, to which this
study contributes to a formal model for alignment of Software Projects with the
Business Strategy. Besides, this work contributes a practical tool for the IT and soft-
ware projects practitioners.
References
1. Wagner, H., Moshtaf, J.: Individual IT roles in business-IT alignment and IT governance. In:
49th Hawaii International Conference on System Sciences (2016)
2. Martínez, D.: Valor de los Recursos de TI desde el enfoque basado en Competencias - Un
Mapa Sistemático de Revisión de la Literatura. Universidad Politecnica de Valencia,
Valencia (2014)
3. Gutierrez, A.: Alignment of information systems’ projects with business strategy: evolution
of thinking and practice. In: Regent’s Working Papers in Business & Management 2014
(2014)
4. Biørn, K., Saeed, M.: The link between organizational strategy and projects. Norwegian
University of Science and Technology (2014)
5. Muhammad, S.: Aligning Project Management with Corporate Strategy. KTH Royal
Institute of Technology - Industrial Engineering and Management, Stockholm (2015)
6. Srivannaboon, S., Milosevic, D.: A two-way inﬂuence between business strategy and project
management. Int. J. Proj. Manage. 24(6), 493–505 (2016)
7. Srivannaboon, S.: Linking project management with business strategy. In: PMI Global
Congress Proceedings, Seattle, Washington (2016)
188
C. Montenegro and G. Barragán

8. Marinho, M., Sampaio, S., Moura, H.: Uncertainties in software projects management. In:
2014 XL Latin American IEEE Computing Conference (CLEI), Montevideo (2014)
9. Hjelmbrekke, H., Klakegg, O., Lohne, J.: Governing value creation in construction project: a
new model. Int. J. Managing Proj. Bus. 10(1), 60–83 (2017)
10. Westphal, I., Eschenbächer, J., Vedovato, D.: Collaborative assessment of potential value
generation in development projects. In: Camarinha-Matos, L.M., Boucher, X., Afsarmanesh,
H. (eds.) Collaborative Networks for a Sustainable World. PRO-VE 2010. IFIP Advances in
Information and Communication Technology, Berlin, Heidelberg (2010)
11. Serra, C.E.M., Kunc, M.: Beneﬁts realisation management and its inﬂuence on project
success and on the execution of business strategies. Int. J. Proj. Manage. 33(1), 53–66 (2015)
12. Too, E.G., Weaver, P.: The management of project management: a conceptual framework
for project governance. Int. J. Proj. Manage. 32(8), 1382–1394 (2014)
13. Kaiser, M.G., El Arbi, F., Ahlemann, F.: Successful project portfolio management beyond
project selection techniques: understanding the role of structural alignment. Int. J. Proj.
Manage. 33(1), 126–139 (2015)
14. Project Management Institute. A Guide to the Project Management Body of Knowledge
(PMBOK® Guide), Fifth ed. (2013)
15. ISACA, COBIT 5.0. A business framework for governance and management of IT (2012)
16. MICROSOFT, Microsoft Solutions Framework version 6.0 Overview (2014)
17. ISACA, COBIT Mapping Overview of International IT Guidance, 3rd ed., pp. 58–82 (2013)
18. Shefﬁeld, J., Lemétayer, J.: Factors associated with the software development agility of. Int.
J. Proj. Manage. 31(3), 459–472 (2013)
19. Ebert, C.: Software product management. IEEE Softw. 31(3), 21–24 (2014)
20. O’Connor, R.V., Laporte, C.Y.: The evolution of the ISO/IEC 29110 set of standards and
guides. Int. J. Inf. Technol. Syst. Approach (IJITSA) 10(1), 1–21 (2017)
21. O’Connor, R.V., Laporte, C.Y.: Software project management in very small entities with
ISO/IEC 29110. In: Winkler, D., O’Connor, R.V., Messnarz, R. (eds.) Systems, Software,
and Services Process Improvement (EuroSPI 2012). Communications in Computer and
Information Science, Berlin, Heidelberg (2012)
22. ISO IEC, ISO/IEC TR 29110-1 Systems and software engineering — Lifecycle proﬁles for
Very Small Entities (VSEs) (2016)
23. de Vasconcelos, J., Krimble, C., Carreteiro, P., Rocha, A.: The application of knowledge
management to software evolution. Int. J. Inf. Manage. 37(1), 1499–1506 (2017)
24. Gerke, K., Tamm, G.: Continuous quality improvement of IT processes based on reference
models and process mining. In: Americas Conference on Information Systems (2009)
25. Siviy, J., Kirwan, P., Morley, J., Marino, L.: Maximizing Your Process Improvement ROI
through Harmonization. SEI, Carnegie Mellon University (2008)
26. Oud, E.: The value to IT of using international standards. Inf. Syst. Control J. 3, 35–39
(2005)
27. Hevner, A., Ram, S., March, S., Park, J.: Design science in information systems research.
MIS Q. 28(1), 75–105 (2004)
28. Hevner, A., Chatterjee, S.: Design Research in Information Systems. Springer Publishing,
New York (2010)
29. Gregor, S., Hevner, A.: Positioning and presenting design science research for maximum
impact. MIS Q. 37, 337–355 (2013)
30. Peffers, K., Rothenberger, M., Kuechler, B.: Design science research in information systems.
Advances in theory and practice. In: Peffers, K., Rothenberger, M., Kuechler, B. (eds.) 7th
International Conference on DESRIST 2012. Springer, Heidelberg (2012)
Alignment of Software Project Management
189

31. Chiarini, M., VanderMeer, D., Rothenberger, M., Gupta, A., Yoon, V.: Advancing the
impact of design science: moving from theory to practice. In: DESRIST 2014. Springer,
Miami (2014)
32. Helfer, M., Donnellan, B.: Practical aspects of design science. In: Helfer, M., Donnellan, B.
(eds.) European Design Science Symposium, (EDSS2011). Springer, Heidelberg (2012)
33. Given, L.M. (ed.): The SAGE Encyclopedia of Qualitative Research Methods. SAGE
Publications, Los Angeles (2008)
34. Mason, J.: Qualitative Researching. SAGE Publications, London (2002)
35. Webb, P., Pollard, C., Ridley, G.: Attempting to deﬁne IT governance wisdom or folly? In:
39th International Conference on System Sciences, Hawaii (2016)
36. Willmott, P.: The do-or-die questions boards should ask about technology (2013). http://
www.mckinsey.com/business-functions/digital-mckinsey/our-insights/. Accessed 2016
37. Ruedas, J.G.: Dirigir las Tecnologías de la Información de una gran Organización Pública
Quién y Cómo - Entre la Tradición y la Innovación. Instituto Español de Estudios
Estratégicos, p. 16, 19 Noviembre (2013)
38. INCOTEC Services, Plan Estratégico Empresarial INCOTEC Services 2016–2020, Quito
(2016)
39. Pintado, P.: Informe Anual Gestión de Proyectos INCOTEC Services 2016, Quito (2016)
40. Pintado, P.: Informe Gestión de Proyectos INCOTEC Services 1er Trimestre 2017, Quito
(2017)
41. Barragán, G.: Alineamiento de la Gestión de Proyectos de TI con la estrategia de Negocio y
la generación de Valor. Escuela Politécnica Nacional, Quito (2017)
42. Manteli, C., van den Hooff, B., Tang, A., van Vliet, H.: The impact of multi-site software
governance on knowledge management. In: Global Software Engineering (ICGSE), 15–18
August (2013)
43. Baars, A., Jansen, S.: A framework for software ecosystem governance. In: International
Conference of Software Business (ICSOB), Berlin (2012)
190
C. Montenegro and G. Barragán

Proposal to Implementation Time-Driven
Activity Based Costing (TDABC)
for Calculation of Surgical Procedure Costs
of a Medium-Sized Teaching Hospital
Michele Mendes Hiath Silva(&) and Saulo Barbará de Oliveira
Universidade Federal Rural do Rio de Janeiro, Rodovia BR 465 KM7,
23890-000 Seropédica, Rio de Janeiro, Brazil
michelehiath@yahoo.com.br
Abstract. Hospitals have several distinct services besides in-patient and
out-patient health care. This service diversity leads to huge complexity and to
high cost of the hospital management. However, speciﬁc characteristics of
health services, such as the diversity of professionals and their autonomy, the
variety of procedures, the different types of information and often the lack of an
integrated system of information, make cost calculation a big challenge. In
Brazil, there are only a few hospitals that can calculate their costs and utilize
them as a tool for effective management. This study presents the preliminary
results of TDABC use for calculating the costs of surgical procedures at a
medium-sized hospital, as well as to use business process analysis and modeling
for its implementation. The results attained so far indicate that TDABC is an
adequate method for calculation and effective control of costs and a tool for
improving processes as well.
Keywords: Hospital costs  TDABC  Business process management
1
Introduction
A hospital needs professionalized management and deﬁnition of institutional processes
that promote growth in a context of rapid changes, in which adaptation to the new
reality is a constant need [1]. Hence, knowing and managing costs is an immediate and
strategic necessity for organizations, especially for hospitals, which have high costs.
Health services have speciﬁc characteristics, such as diversity, variability and
unpredictability regarding the time factor. Those characteristics make accurate
knowledge and control of costs, and consequently processes, indispensable [2]. Cost
management of a hospital goes beyond the accounting nature required by legislation,
given the new market demands that require current and precise strategic information to
support decision making. This new condition must consider aspects related to man-
agement of operational costs, the involvement of managers and teams, the formulation
or prices and analysis of results [3].
Despite the relevance, few hospitals calculate their costs in a systematic way.
The absence of standardized information about hospital costs is a critical factor to the
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_19

health sector. This problem limits the deﬁnition and choice of management indicators,
with a negative repercussion on public health policies and actions, principally in the
system’s organization and in the forms and amounts of payments [4].
Without suitable determination of costs, it is not possible to assess the efﬁciency of
using resources for operation of the organization. This situation generates negative
results both for service providers and clients, be they public or private [5].
The objective of this article is to present the preliminary results of implementing a
method to determine the costs of surgical procedures at a medium-sized teaching
hospital located in the mountain region of Rio de Janeiro state. This article is organized
into seven sections. The ﬁrst presents the general aspects of the situation investigated.
The second describes the cost calculation method used. The third presents the analysis
and modeling of business processes to support cost calculation. The fourth section
describes the research methodology. The ﬁfth and the sixth sections present, respec-
tively, the results and discussion, followed by the ﬁnal considerations.
2
Time-Driven Activity Based Costing (TDABC)
The TDABC method is a variation of activity-based costing (ABC), which attributes
costs to the consumption of resourced by each activity of the organization. Products and
services are consequences of those activities, where there is a cause and effect relation
between the activities and the cost [6, 7]. TDABC is an approach that is simpler to
implement than the ABC method, with the following beneﬁts: easy and quick imple-
mentation, integration with other corporate systems, or utilization of simple electronic
spreadsheets, possibility of continuous improvement of processes, and identiﬁcation of
idle capacity. Overall, it improves efﬁciency and adds value for clients [8].
The TDABC method’s main simpliﬁcation is the utilization of only two variables:
(1) the costs of consumed resources; and (2) the time for execution of activities. With
these two variables, the capacity cost rate is determined [9].
Variable 1 (numerator) is obtained from the total cost of all resources that the
activities consume, while variable 2 (denominator) is calculated from the theoretical
capacity, which means the total time of resource availability. This calculation can be
made through an arbitrary estimation approach, which presumes that the practical
capacity is 80% of the theoretical capacity, or through the analytical approach
involving real calculation of the time available for each resource [8].
TDABC allows comprehension of complex processes in a more accurate, efﬁcient
and quick way, making management of costs and decision making easier [9]. In the
health area, TDABC produces information that goes beyond purely accounting data. As
such, it contributes to the analysis and continuous improvement of care processes,
guides health policies, allows evaluating quality, supports the construction of clinical
protocols, improves efﬁciency and reduces costs [10]. Moreover, TDABC can be used
to measure and evaluate the effects of proposals for process improvements before their
implementation, making it possible to identify whether modiﬁcations will reduce costs
and improve hospital efﬁciency, therefore improving value to patients. Other advan-
tages are the identiﬁcation of operational idleness of hospitals and applicability to
complex and dynamic processes, which are characteristic of these organizations [11].
192
M. M. H. Silva and S. B. de Oliveira

3
Business Process Analysis and Modeling as Support
for Implementing Management of Costs
The vision of processes is the base for control and clearer understanding of each
activity and consequently of the resource allocation. This concept emphasizes the direct
relation between the cost based on activities and the process vision. Organizations are
like systems that have complex interactions with the internal and external environment
in which they are inserted. This vision permits identifying the different organizational
levels, facilitating the implementation of tools for control, norms and standards [12].
In the concept of activity-based costing, activities can be grouped into business
processes, enabling calculation of total cost, benchmarking and the identiﬁcation of a
process inefﬁciencies [6]. However, the ABC method requires intense efforts to map,
measure and manage the processes with maximum efﬁciency. These efforts are orga-
nizational elements for control, transformation and generation of value [12].
A method with focus on the analysis and modeling of business processes can
provide a vision of the company structured in functional diagrams that reveal the
behavior of each process. There are four modeling objectives: understanding, learning,
documentation and improvement [13]. Business process management (BPM) is
strongly affected by human action and requires a model that is ﬂexible and adaptable to
different realities. Considering these aspects, Valle and de Oliveira [13] presented a
cycle for the management of processes developed by Baldam. In this cycle there are
four steps: (1) BPM planning; (2) shaping and optimization of processes; (3) process
execution; and (4) data control and analysis. By grouping these four elements, the BPM
cycle proposed by Baldam is a suitable tool to implement TDABC.
For Cannavacciuolo [14], business processes management is the base for imple-
mentation of cost management. It makes controlling costs easier by indicating the
critical areas that need more attention from managers.
Nogueira and Castilho [15] proposed a model of cost calculation based on ABC
starting from the description, mapping and validation of the processes for managing the
solid waste generated by a surgical center. The modeling of these processes allowed
visually identifying how and when the resources are consumed. The authors concluded
that the costing based on activities associated with business process management is a
powerful tool for cost control and the decision-making process.
4
Methodology
The method used in this study was the action research, deﬁned by Thiollent [16] as a
research method suitable for social research with empirical base, whose main charac-
teristic is collective participation for the resolution of a problem. Action research
requires reﬂection, action, cooperation and collaboration among everyone involved in
the study, as well as sharing of the knowledge produced [17]. Action research
speciﬁcally focuses on the relation between two objectives: knowledge acquisition and
practical results [18]. This study’s practical objective is to ascertain the costs of surgical
procedures of a hospital, and consequently to obtain information and technical
Proposal to Implementation TDABC for Calculation
193

instruments to enable actions to improve the working processes. The objective is to
build individual and collective knowledge from the experiences of the people involved
in resolving problems, as recommended by Thiollent [16], seeking to produce and
structure knowledge related to determination of costs.
This study was carried out in four stages: (1) exploratory, in which the initial
diagnosis was carried out; (2) main stage, in which the participants were recruited and
the action plan was deﬁned; (3) action, in which the method was applied on a pilot
basis (originally envisioned to be followed by disclosure of the results, which instead
will occur in the future); and (4) evaluation, involving judging the effectiveness of the
results (also to be done in the future).
5
Preliminary Results of the Research (Pilot)
The objectives of the pilot study were: (1) to verify applicability of the BPM cycle
proposed by Baldam [13], supporting TDABC implementation; (2) to identify and
describe the activities and resources necessary for a surgical procedure; (3) to verify the
types and sources of information necessary to implement the costing method; and (4) to
evaluate the initial adherence of the TDABC method with regards to its efﬁciency,
effectiveness and control in measuring the costs of surgical procedures.
The ﬁeldwork began with the BPM steps, which lasted from November 2016 to
February 2017. The description of the process scope was done based on an interview
with the surgical center’s head nurse and general surgeon who is also the hospital’s
director. We used the Description of Processes Form according to the model suggested
by de Oliveira [12]. The form was presented to the interviewees and completed jointly
in a dynamic way. We also used an interview script that served as guide for this stage.
For this purpose, we held two meetings in January 2017. After the form was ﬁlled out,
the business process was designed using Bizagi Process Modeler software, free version
2.7.0.2. The material produced was then presented to the interviewees for their vali-
dation of the information provided at the ﬁrst meeting.
Based on the classiﬁcation of Valle and de Oliveira [13], the surgical procedure was
deﬁned as a primary process, because it is related directly with patients, and is a key
process because it has high impact on patients and hospital costs. The cost report of
2016 indicated an average monthly cost of the surgery center of US$ 191,219.43,
representing 12% of the total hospital costs.
De Souza [18] presents four initial items of data necessary for the deﬁnition of
procedure costs calculation by the ABC concept: (a) The procedures to be analyzed:
the procedures were deﬁned from the hospital production in 2016, as recorded in the
Datasus [19] database. Datasus is the national database of the Brazilian uniﬁed health
system. The data showed that videolaparoscopic cholecystectomy was the procedure
performed the most at the hospital. (b) The activities of the surgical procedure:
considering that this study’s propose was to determine the costs of a surgery procedure,
the activities directly related to the patient were mapped and listed according to the
deﬁnition of De Souza [18]. Seven activities were identiﬁed for the surgical procedure.
(c) The resources consumed by the activities: these were the human resources,
material used and consumed, equipment and medicinal gases. The materials for
194
M. M. H. Silva and S. B. de Oliveira

medical use and the medicine utilized by the patients during the surgery were con-
sidered as direct resources. In this case, the data came from the billing documents of the
42 selected procedures. (d) The guide of resources and activities: the activity guide
was the time to execute each activity. Initially we calculated, from the surgical center
statistical data, the average time for performing each of the 42 selected procedures. The
average time was 74 min, ranging from 30 to 130 min. This ﬁnding called attention to
the huge variation between the minimum and maximum, generating a recommendation
for a deeper analysis of the surgery time recording process in the future. The time of
activities was estimated considering the experience of the interviewees (Table 1).
The elements for calculating the capacity cost rate were deﬁned in the following
way: (a) Theoretical capacity deﬁnition: The theoretical capacity represents the total
available resource time. The surgical center’s resources are distributed to guarantee
operation 24 h a day, so we calculated the theoretical capacity of a month (30 days)
with 43,200 min. (b) Practical capacity deﬁnition: The practical or real capacity is the
available time in practice for the activities. For deﬁnition of practical capacity, we
utilized the arbitrary approach, that is, 80% of the theoretical capacity represents the
practical capacity, meaning 34,560 min per month available for the surgery center’s
activities. (c) Practical capacity cost: This is the total cost of the resources. We
considered only the direct costs of the surgery center. The costs for materials and
medicines were considered direct costs consumed by the patient and hence not
involving time (Table 2).
After these deﬁnitions, the capacity cost rate was calculated (Table 3):
After deﬁning the capacity cost rate, we ascertained the cost of each activity. The
sum of each activity cost resulted in the average cost of the videolaparoscopic
cholecystectomy surgery, presented in Table 4.
Table 1. Resources and time consumed by the activities
Activity
Resources
Time of activity
(minutes)
Receive patient
Human/Use and Consumption
5
Evaluate clinical conditions
Human/Use and Consumption
5
Delineate surgical strategy
Human
10
Prepare for anesthetic
induction
Human/Material and
Medicines
10
Induce anesthesia
Human/Material and
Medicines
Equipment/Medicinal Gases
10
Operate on the patient
Human/Material and
Medicines
Equipment/Medicinal Gases
40
Recovery after anesthesia
Human/Equipment
30
Proposal to Implementation TDABC for Calculation
195

We also calculated the practical capacity percentage utilized, that is, how many
minutes the surgical procedures took. The practical capacity information was taken
from the statistical report of the surgical center and consisted of the average time of all
procedures performed in 2016 (Table 5).
Table 2. Practical capacity cost of the surgical center
Code Resources
Average monthly cost (US$)
R1
Human
93,818.72
R2
Maintenance
2,376.36
R3
Medicinal gases
2,612.64
R4
Material for use and consumption 872.51
R5
Equipment
5,434.97
104,480.19
Table 3. Capacity cost rate calculation
Practical capacity cost (dollars)
104,480.19
Practical capacity (minutes)
34,560
Capacity cost rate (US$/minute) 3.02
Table 4. Activity cost calculation (dollars)
Activity
Time
(minutes)
Capacity cost rate
(US$)
Patient direct cost
(US$)
Activity total value
(UD$)
A1
5
3.02
0.00
15.11
A2
5
3.02
0.00
15.11
A3
10
3.02
0.00
30.23
A4
10
3.02
0.00
30.23
A5
10
3.02
0.00
30.23
A6
40
3.02
141.38
262.29
A7
30
3.02
0.00
90.68
TOTAL
110
TOTAL
473.88
Table 5. - Practical capacity percentage utilized by the surgical center
Capacity
Quantity (minutes)
Theoretical
43,200
Practical (80%)
34,560
Practical utilized
29,957
Practical capacity percentage utilized 87%
196
M. M. H. Silva and S. B. de Oliveira

6
Discussion
The preliminary results supported the claim of Kaplan and Anderson [8] that TDABC
is a simpliﬁed method that demands less time for implementation when compared to
ABC and that TDABC generates more detailed information, and above all, establishes
a causal relation that is fundamental for reduction of costs. In relation to the use of the
BPM cycle for implementation of the TDABC method, the initial results were positive,
even without the execution of all stages. However, it is important to mention that this
method requires a minute level of detail of the processes, with an effect on the time to
implement the TDABC.
The TDABC method was efﬁcient since during its initial process it allowed iden-
tifying the activities and the necessary resources for the execution of the surgical
procedure selected for this study. This allows analysis of how well (or not) the orga-
nization is executing its processes. Another important aspect is that even in the initial
stages it was possible to identify shortcomings and correct them. The efﬁciency aspect
was enhanced by using the BPM cycle. Regarding the ﬁnal result, the method was able
to calculate the surgical procedure cost. The results also show that the TDABC method
is an effective control tool, mostly because it identiﬁes and allocates all available
resources to each activity to be executed, allowing clearer analysis by managers.
Besides this, the information generated through the TDABC can support decisions by
hospital managers in negotiating the amounts of remuneration for the procedures, both
in the private and public sectors. Based on the vision of each process, the medical team
can reassess the activities that compose the surgical procedure, focusing on those that
generate value for the patient.
Currently the costs of the hospital studied are calculated by the absorption method,
so only the sector costs (costs centers) are identiﬁed. Since there is no cost calculation
breakdown by procedure, it was not possible to compare the cost found for the vide-
olaparoscopic cholecystectomy with previous estimates (planned for the next states of
the study).
Due to the preliminary nature of the study, we only considered the direct costs of
the surgical center and all of the resources were allocated in one way. However, there is
a need deeper analysis of these criteria. There may be a need for adjustment in this
question.
The identiﬁcation of the surgery center idleness (13%) was a positive result.
However, considering the speciﬁcities and the complexity of a surgery center, it is
possible to predict other forms of capacity cost calculation, such as distribution of
resources in a structure for elective procedures and another for urgent procedures.
As for the information necessary and its sources, it was possible to reach a partial
ﬁnding, evidencing that the quality of statistical data produced by the surgical center
needs to be improved. This represents a risk factor, since it reduces the quality of
information for decision making.
The fact that the hospital has not implemented process management model yet
posed a barrier to the initial investigations, mostly regarding modeling processes. This
type of difﬁculty is pointed out Valle and de Oliveira [11] when they afﬁrm that ABC
implementation in an organization oriented by processes is a facilitator.
Proposal to Implementation TDABC for Calculation
197

7
Final Considerations
The objective of this study was to analyze the initial procedures for the implementation
of an efﬁcient costing method for calculating the costs of surgical procedures.
The TDABC choice was based on the theoretical framework, which indicated, among
other aspects, the simplicity and speed of its implementation. The choice was also for
practical purposes based on the current status of the structuring of processes and the
computerization level of the hospital. This situation reafﬁrmed the possibility of
applying TDABC using electronic spreadsheets, although we believe a more robust
information technology solution would facilitate the process.
The study enabled showing the beneﬁts of using BPM as support for implementing
the TDABC method in the one process of the hospital. Application to managing costs
in the other processes will be carried out in the continuation of this study.
Although it was not part of the initial scope of this study, it was possible to identify
the surgery center’s idleness. Even though this aspect needs deeper analysis about the
time variable utilized, this indicator can already be used managers to optimize resource
use in actions. The study also allowed identifying and mapping the ordinary activities
of a surgical procedure, as well as its main resources. We stress that this work produced
the ﬁrst mapping of the business process in the ambit of a free public hospital. The next
stages of the study will involve expanding the sample, i.e., applying the TDABC
method to investigate other surgical procedures, incorporation of direct costs and
execution of all the steps of the Baldam cycle.
References
1. Pérez-Pineda, F., Privetera, A.E.: Guadalupano hospital: looking for sustainable growth.
J. Bus. Res. 69(9), 3848–3858 (2016)
2. Alemão, M.M., et al.: Aplicação do custeio Abc no processo de transplantes de fígado, no
Estado de Minas Gerais. Revista PRETEXTO 16(3), 77–91 (2015)
3. Matos, A.J.: Gestão de Custos Hospitalares: técnicas, análises e tomada de decisão, 3a edn.
STS, São Paulo (2002)
4. Gonçalves, M.A., Alemão, M.M., Drumond, H.A.: Estudo da utilização da informação de
custos como ferramenta de gestão em organização pública: o estudo e o Sigh-Custos.
Perspectivas em Gestão & Conhecimento 3(1), 210–226 (2013)
5. Mediha, G.O., et al.: Activity-based costing management and hospital cost in patients with
chronic obstructive pulmonary disease. Eur. J. Gen. Med. 13(2), 116–126 (2016)
6. Cogan, S.: Modelos de ABC/ABM: inclui modelos resolvidos e metodologia original de
reconciliação de dados para o ABC/ABM. Qualitymark, Rio de Janeiro (1997)
7. Kaplan, R.S., Cooper, R.: Custo e Desempenho: administre seus custos para ser mais
competitivo, 2a edn. Futura, São Paulo (2000)
8. Kaplan, R.S., Anderson, S.R.: Time-driven-activity-based costing. Harvard Bus. Rev. 82
(11), 131–138 (2004)
9. Kaplan, R.S., Anderson, S.R.: The speed reading organization. Bus. Finance 13, 39–42
(2007)
10. Chen, A., et al.: Time-driven activity based costing of total knee replacement surgery at a
London teaching hospital. Knee 22(6), 640–645 (2015)
198
M. M. H. Silva and S. B. de Oliveira

11. French, K.E., et al.: Measuring the value of process improvement initiatives in a preoperative
assessment center using time-driven activity-based costing. Healthcare 1(3–4), 136–142
(2013)
12. de Oliveira, S.B.: Gestão por processos: fundamentos, técnicas e modelos de implementação:
foco no sistema de gestão de qualidade com base na ISO 9000:2000. 2a ed. Rio de Janeiro:
Qualitymark (2012)
13. Valle, R., de Oliveira, S.B., et al.: Análise e modelagem de processos de negócio: foco na
notação BPMN. Atlas, São Paulo (2013)
14. Cannavacciuolo, L., et al.: An activity-based costing approach for detecting inefﬁciencies of
healthcare processes. Bus. Process Manage. J. 21(1), 55–79 (2014)
15. Nogueira, D.N.G., Castilho, V.: Resíduos de serviços de saúde: mapeamento de processo e
gestão de custos como estratégias para sustentabilidade em um centro cirúrgico. REGE -
Revista de Gestão 23(4), 362–374 (2016)
16. Thiollent, M.: Metodologia da Pesquisa Ação, 18a edn. Cortez, São Paulo (2011)
17. Tripp, D.: Pesquisa ação: uma introdução metodológica. Educação e Pesquisa 31, 443–466
(2005)
18. De Souza, A.A.: Gestão ﬁnanceira e de custos em hospitais. Atlas, São Paulo (2013)
19. Datasus.
Produção
Hospitalar
SIH/SUS.
Disponível
em:
http://datasus.saude.gov.br/
informacoes-de-saude/tabnet. Accessed 25 Jan 2017
Proposal to Implementation TDABC for Calculation
199

Best Practices and Pitfalls in Open Source
Hardware
Manuel Moritz(&), Tobias Redlich, and Jens Wulfsberg
Institute of Production Engineering, Helmut Schmidt University,
Holstenhofweg 85, 22043 Hamburg, Germany
{manuel.moritz,tobias.redlich,
jens.wulfsberg}@hsu-hh.de
Abstract. Easy-to-use and affordable means of production (e.g. 3D printer),
access to these technologies (e.g. in makerspaces) and powerful tools for online
product design led to the emergence of open source hardware (OSH). Like in
open source software, projects and online platforms have been evolving around
physical products from various technologies where people jointly develop and
freely share designs. We also ﬁnd businesses that sell products made from these
designs and offer complimentary services. This decentralized and collaborative
model of value creation offers new opportunities for social, economic and
ecological sustainability, but also calls for a different understanding beyond
traditional notions. Results from an exploratory study on OSH projects and
companies revealed that licensing is a critical issue, communities and partners
play a key role in the open source ecosystem and modes of value creation are
either centered around design or production in combination with a mix of
complimentary services.
Keywords: Open source hardware  Co-creation  Open production
1
Introduction
The impact of the open source software (OSS) movement on the development and
advancement of the Internet and related ICT is widely recognized [1]. Also in IT
industry, OSS plays a huge role: Google, IBM and SAP are just a few among global
players that make use of OSS and engage with communities. With recent technologies
that enable distributive and virtual product development, with online platforms and
communities for sharing ideas and CAD ﬁles as well as the rise of consumer 3D
printing and makerspaces where people have access to means of production, the spirit
of open source (online collaboration, knowledge sharing, openness etc.) also spilled
over to the domain of physical objects, namely open source hardware (OSH) or open
design (OD). The landscape of OSH covers nearly all ﬁelds of technology, e.g.
automotive, electronics, machine tools, drones.
All over the world people engage in communities, jointly develop products, share
designs and build upon each other’s ideas [2–4]. Evolving from projects, businesses
were created that capture value by selling and distributing ready-to-use products and
kits, components or by offering workshops [5, 6]. Design and documentation of these
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_20

products, however, is freely shared with anyone to study, build, adapt and sell products
based on these designs. The story of the RepRap project illustrates how an OSH project
turned into a million-dollar business and boosted technology. It started off as a com-
munity project where people jointly developed open source 3D printers [7]. MakerBot
started selling kits and ready-to-use printers that were based on the RepRap works.
Within 3 years, MakerBot sold 22,000 3D printers. Finally, MakerBot was sold to
Stratasys for more than $ 400 million as the market for 3D printers exploded [8].
The OSH movement is accompanied by a paradigm shift in industrial value cre-
ation. We observe new patterns that cannot be described with traditional economic
notions. Networking, knowledge sharing, collaboration, co-creation and decentraliza-
tion are summed up under the term bottom-up economics [9]. The range of distributed
value creation systems (for tangible, intangible and informational goods) varies from
production networks that integrate external actors through open innovation and
crowdsourcing
via
communities
of
knowledge
creation
(e.g.
Wikipedia)
to
peer2peer-production approaches and open manufacturing workshops (e.g. FabLabs)
[10]. The open source approach to hardware not only provides opportunities for
technological innovation, but also for socializing and democratizing production. For-
mer passive customers are becoming prosumers and, thus, are empowered to participate
in collaborative value creation without necessarily being dependent on companies [11].
The latter still might remain relevant, though, for providing cost-efﬁcient physical
commodities. Despite creating new business opportunities based on openness and
competition rather that intellectual monopolies, research and education as well as
developing countries with access to low cost, customizable and appropriate tech-
nologies beneﬁt from this approach [12].
The goal of this paper is to shed light on the OSH ecosystem and to ﬁnd out how
OSH projects and companies create value. Furthermore, we want to know: What is the
role of communities and who are the key partners? What about intellectual property and
licensing? We present results of an exploratory study on OSH projects and companies
to answer these questions.
2
Theoretical Background
Open source hardware (OSH) describes physical artifacts with its relevant documentation
and design (schematics, assembly instruction, bill of materials, design ﬁles, user man-
uals, source code etc.) freely available and accessible [13]. Based on this digital repre-
sentation, products can be manufactured, used, but also offered for sale by basically
anyone. According to the Open Source Hardware Association (OSHWA), anyone shall
be given the freedom to “study, modify, distribute, make, and sell the design or hardware
based on that design” [14]. Transparency enables users to have full control of the tech-
nology (e.g. study, repair, individualize). OSH product design is highly efﬁcient for
several reasons: First, the OSH community comprises an enormous number of
like-minded, but heterogeneous people from all over the world who voluntarily collab-
orate on different projects (crowdsourcing effect). Second, information can freely cir-
culate. People share ideas, learn from each other and build upon each other’s ideas [15].
Best Practices and Pitfalls in Open Source Hardware
201

Last, OSH fosters sustainability when it comes to resource efﬁciency (e.g. standardiza-
tion, peer review, crowdsourcing), technological literacy (e.g. learning, sharing, partic-
ipation), and user empowerment (e.g. repair, service, individualization) [5].
Evaluating the (economic) impact of OSH is very hard, though. Contributors in
communities are not getting paid. People who build products and use them are not
buying them from vendors. It is not possible yet to register how many products were
built from downloaded designs. So, there is no ofﬁcial registration of value that is being
generated/captured besides revenues of OSH businesses. Still, Pearce [16] applied
different evaluation methods to a simple syringe pump and found that millions of
dollars of value were generated by sharing the design.
Raasch et al. [17] studied open design (OD) projects with focus on the product
development process and found that the development in open design can be compared
to OSS development and subsequently ﬁnd the open source model valid for both,
software and hardware. Balka et al. [18] analyzed open source product development
processes and product related aspects of openness. To be open, they found, the design
of a product must be transparent, accessible and replicable. Bonvoisin et al. [19] found
that these days only 10% of OSH products meet these criteria. Raasch and Herstatt [20]
analyzed how companies capture value based on open design. They found 4 distinct
types of business models: Tier specialists, focal companies, sellers of complements and
commercial users.
In our analysis, we preferred to look at modes of value creation instead of value
capture strategies because otherwise we would ignore OSH projects that create, but not
capture value in the same sense that Wikipedia provides huge value which is not
captured/monetarized. Other questions that require further exploration of the OSH
ecosystem deal with the role and size of communities, with key partners as well as
differences in licensing especially when distinguishing between projects and companies.
Licensing is a critical issue in OSH. It is crucial for all creators and users to know the
terms and conditions. The aim of OSH licenses is to balance the initial rights of creators
of designs derived from copyright law with the principles of the open source movement
(free access, sharing etc.). Licenses make sure that ideas may freely circulate and
attribution to contributors is given. No one shall be allowed to restrict the access to OSH.
Thus, the rights of users (to copy, distribute, sell etc.) are very strong compared to
creators. Prevailing OSH licenses (CERN OHL, TAPR OHL) were derived from OSS
licenses, but there is a major difference between software and hardware: The outcome of
a software project is a program (lines of code as expression of ideas) that is subject to
copyright and, thus, give the author the “right to copy, the right to distribute, and the
right to create derivative works” [21]. Creative Commons (CC) licenses (based on
copyright) are very often used for documentation, schematics and design ﬁles. Copy-
right, however, does only cover the expression of an idea and not the idea itself (or its
implementation in a physical product). In OSH projects, this is a problem as the aim of
an OSH project is the implementation of an idea into a useful physical product [21].
Still, the schematics and designs are subject to copyright, the tangible product (and
its commercialization), on the other hand, is not. [21] Even though aspects of distri-
bution of products are considered in some licenses, it is questionable, if they would be
enforceable in case of infringement (unlike a patent). Also, some CC licenses are not
open source in the true sense as they allow restrictions regarding commercial use (e.g.
202
M. Moritz et al.

“non-commercial”, “no derivatives”). Applying for a patent is one way to control
commercial activities with physical products. However, patent law is not a feasible way
for the OS community as it contradicts the basic principles of open source (sharing,
openness, collaboration, freedom). Still, (defensive) patenting is in line with open
source when combined with an OSH license (no one else may appropriate the idea, but
anyone may use it with an open license). Another way to control the distribution of
OSH in the marketplace to some extent is to apply for a trademark (e.g. Firefox,
Arduino).
3
Methods and Data
3.1
Research Approach
The state of research on open source hardware still is nascent. Organizational and
strategic issues like licensing, modes of value creation and the role of communities
have attracted little research so far. Thus, we chose an exploratory approach to cover a
broad variety of projects/companies and study how these entities operate within the
OSH ecosystem [22, 23]. We were looking for projects and companies that claimed to
be an OSH project/company or that otherwise stated to operate with or around OSH.
Furthermore, projects/companies should be related to physical artifacts with a
minimum degree of complexity excluding for example jewelry, t-shirts or gadgets.
Data collection was carried out from November 2016 until February 2017. Most
information was found via desk research. In addition, we browsed open source data
bases, blogs and social media to ﬁnd suitable projects/companies. Whenever infor-
mation on websites was insufﬁcient, we contacted afﬁliated persons via e-mail.
3.2
Data Base
We found 78 projects and companies to be suitable for further analysis. A subdivision
between projects and companies should help us to identify different approaches.
Classiﬁcation revealed a broad spectrum of technological ﬁelds (Fig. 1). 41% of the
entities were founded in Europe, 54% in USA/Canada and 5% in Asia. Most of the
Fig. 1. Technological ﬁelds of OSH (left), purpose/reputation (right).
Best Practices and Pitfalls in Open Source Hardware
203

project/companies were set up between 2007 and 2014. The earliest project started in
1999. Since 2014 only 5 projects have been initiated.
Within the sample we found 37 projects and 41 companies. Projects could be
initiated either by individuals (56%) or by an organization (e.g. NGO) (44%). Busi-
nesses based on or around OSH are majorly proﬁt-oriented (88%), only 5 are
non-proﬁt. Looking closer at companies, we found that 9 organizations managed to
build up personnel of 100 + people which indicates regular operations. Furthermore,
projects/companies differ among their purpose and reputation. While we ﬁnd more
companies in technology and innovation (e.g. electronics, 3D-printers, cars) than in the
ﬁeld of social impact, projects can be equally found in both sections (Fig. 1).
4
Results
4.1
IP and Licensing
Open source is about sharing ideas, jointly developing products and providing
knowledge to other people and give them the opportunity to learn and build, but also to
sell products based on these ideas. Thus, the goal is to not only freely share infor-
mation, but also to guarantee that information remains free. This is where licensing
matters. Especially when companies build business models with open source, getting
licensing right is a critical issue as it guarantees the freedom to operate in the com-
mercial domain. It turns out, though, that this is not the case (Fig. 2).
Results show that, in fact, 18% of projects and companies do not use any license at
all for their products. This is critical for both: creators and users. There is no legal
protection against misuse and lock-in of knowledge (and thus exclusion of others).
Entities that use licenses (70%) majorly rely on one of the Creative Commons licenses
which proved to be very practical for digital works of artists and authors. However,
these licenses provide no effective protection whenever a physical artifact based on
copyright-protected documentation (expression) of an idea is implemented. Only 17%
use proper OSH licenses (CERN OHL, TAPR). These licenses cover not only the
digital data and documentation, but also deal with aspects regarding the physical
Fig. 2. Licensing/patenting by OSH projects and companies.
204
M. Moritz et al.

implementation, commercial use and distribution of the products (like a patent). In
addition, 5 companies applied for patents (for defensive purposes) which does not
necessarily contradict the idea of open source when anyone is granted the permission to
use the patented idea. Overall, 26 projects and 23 companies (63%) comply with the
strict notion of open source by using licenses that explicitly permit commercial use
(e.g. CC-BY-SA).
4.2
Online Communities
Online communities have always been playing a crucial role within open source
ecosystems. In fact, most of open source software projects evolved in or around a
virtually connected community of like-minded people who jointly developed software.
The same logic applies to hardware projects as digital technologies enable virtual and
online ideation, conception and development of products. Building up a community
serves various purposes: (ﬁnancial and motivational) support, testing, bug-ﬁxing,
feedback, co-development, customer base, brand recognition etc. Results show that
81% of project and companies were able to build up online commmunities around their
product(s) (Fig. 3). 11 of these gathered more than 10,000 members.
The depth of interaction with users in these online communities varies from loose
support and feedback to in-depth collaboration and co-development. Combinations of
these and different user groups within one community are very common (Fig. 4). More
than half of the projects and companies are integrating external users in product
development processes. In contrast, mere support is not very common which indicates
that users are able and willing to participate in product development.
Fig. 3. Existence and size of online communities of OSH projects and companies.
Fig. 4. Roles of users in OSH online communities.
Best Practices and Pitfalls in Open Source Hardware
205

4.3
Key Partners
The collaborative and distributive nature of OSH projects and products calls for an
(online) ecosystem of partners. First, online co-creation requires knowledge and pro-
cess management systems as well as online repositories to jointly work on projects, to
store documentation and to share information with other people. Second, crowdfunding
and PR with stakeholders (supporters, lead users, backers) plays a crucial role at early
stages of OSH projects. For companies, communication and marketing via partners to
build up and strengthen brand awareness is important. Third, “business” partners like
suppliers, distributors, vendors and FabLabs help projects and companies to set up
supply chains and sales channels.
Our results (Fig. 5) show that online platforms are very important partners for OSH
projects and companies, e.g. GitHub, Thingiverse or Wevolver. Here, project and
product documentation (instructions, bill of materials etc.), CAD-ﬁles, source code and
other information related to the project can be stored, shared and offered for download.
Another important group of partners are crowdfunding plattforms. Campaigns on
platforms like Kickstarter and Indiegogo are strong means for both, funding and
marketing of OSH projects at an early stage. Other partners comprise distributors and
vendors (e.g. Adafruit, Sparkfun) as well as suppliers of components (e.g. Arduino,
Raspberry Pi).
4.4
Modes of Value Creation
Arduino (microcontroller), Sparkfun (electronic components) and Ultimaker (3D
printer) are just a few among a group of companies that managed to build up viable
businesses with OSH. In fact, 9 out of 41 companies employ 100+ people. However,
capturing value by opening up and sharing knowledge is not easy. These organizations
need to ﬁnd a set of alternative value propositions besides traditional approaches based
on intellectual property and market power. Some are operating in technological niches
that are too small for corporate actors, others offer superior products by constantly
innovating, e.g. by direct feedback of users and customer/user integration via
co-creation. Finally, companies might gain advantage with a strong brand awareness by
hosting a user community, providing a platform for idea exchange, and/or offering
experience-based online and ofﬂine events (e.g. hackathons, idea challenges). How-
ever, open source is more than just another way of making money. It is also about
social and ecological impact, about sharing, learning and teaching and access to
technology and knowledge. A huge amount of value is being created and provided to
other people.
Fig. 5. Relevance of key partners in the OSH ecosystem.
206
M. Moritz et al.

Our results revealed two basic modes of value creation: (1) develop a product, share
the design and all relevant information so that other people may make use of it, or
(2) develop a product, share the design, manufacture/assemble the product based on
that design and sell it as kit or ready-to-use. Based on these modes, we ﬁnd a mix of
business models. One direction is to offer expertise and experience-based services and
events. Another option is to host a platform for customers for learning, exchanging
ideas and supplying components, products and raw materials.
The design mode is mostly used for early stage and hobby projects as ﬁnancial
investment is low (Fig. 6). All you need is a computer, Internet access and some design
skills. With the help of online repositories and platforms it is very easy to share
information and exchange ideas. Still, in combination with expertise (workshops,
tutorials) and platform services, business opportunities emerge. For example, Open
Source Ecology develops low-cost, modular and robust agricultural machinery.
Designs and documentation are freely shared via a wiki. The organization offers
workshops to build machines and farm applications. Another case is e-Nable, a com-
munity that develops and shares designs for 3D-printable hand protheses. Anyone may
download, adapt and print artiﬁcial hands. FabLabs that provide access to means of
production play an important role here. 30 out of 37 projects apply this mode. Only few
businesses can be found.
In the production mode, capturing value matters. The products are manufactured
and offered for sale via webshops or distributing platforms. Even if the digital repre-
sentation of the physical product is freely shared, actually making it requires resources
(time, money, materials, skills) and access to means of production. Economy of scales
are at work, so offering low-price, high-quality, and ready-to-use products creates
business opportunities. Most OSH businesses can be found here (Fig. 6). They com-
bine design, production and sale. For example, you can access and download all
relevant information about the Ultimaker 3D printer, but not many people have the time
nor the skills to build it. Sparkfun is another example: they design, manufacture and
sell electronic components on a platform. They also serve as distribution channel for
Fig. 6. Modes of value creation: design-centric (left), production-centric (right).
Best Practices and Pitfalls in Open Source Hardware
207

other related electronic parts and components, e.g. microcontrollers, 3D printers.
Additionally, they have a strong user community and offer learning materials and
workshops. Overall, we ﬁnd 80% of OSH businesses that sell their products and offer
related services.
5
Discussion and Conclusion
OSH still is an unexplored phenomenon. We found wide variety among projects and
companies that run businesses based on open source. Without being accounted, huge
value is being created by thousands of users worldwide. We also don’t know how
many people make use of OSH by converting digital designs into physical products in
makerspaces or at home, by adapting the designs for their speciﬁc needs or by sharing
their experience and, thus, helping to improve products. We know, however, that there
is a huge demand (commercial and non-proﬁt) for OSH technologies and that supply is
growing rapidly. The potential to improve the life of and empower people worldwide
who now have access to technology and means of production, who may study product
design and who want to participate in value creation is just too vast to not have an
impact on markets and industries in the future.
Still, there are some challenges to be tackled. Licensing, for example, remains a
critical issue. Companies seem to not be aware of the risk they are taking by not
properly licensing their (digital) products. Creative Commons licenses are mostly used
for the designs. However, these might not effectively protect against physical imple-
mentation and commercial distribution by violators. More research in the ﬁeld of IP
management and IP law is required to ensure freedom to operate in OSH. We also
found that communities play a key role in the OSH ecosystem. Users can act as
supporters, feedbackers and contributors. At the same time, they might become cus-
tomers or at least users of the products. Long-term collaboration with users requires a
lot of effort, especially, if distinct groups are involved. Financial backers need different
treatment than collaborators, experts another handling than potential users which might
become customers. Companies need to carefully manage their community and treat
users as valuable resources. In this realm, results from research on OSS might help to
derive the best strategies.
In addition, we had a closer look at the OSH ecosystem. Distributive and collab-
orative value creation calls for a system of actors that fulﬁll all functions of traditional
value chains. Here, platforms for ﬁle sharing, data management, idea exchange and
product design are just as important as crowdfunding platforms. The latter represent a
popular means for ﬁnancing, but may also serve marketing purposes to attract sup-
porters and co-creators and build up a community of followers. Last, we described two
modes of value creation. Projects usually tend to be design-centric. This model is very
useful for projects in technological niches or with a focus on social and sustainable
issues. The goal of these projects is to develop and freely share a blueprint. To this
stage, entry barriers are very low. Setting up a project requires few resources. It is also a
low-risk approach for start-ups. If there is a design to convert into physical artifacts and
a market need, business opportunities appear. People might not be willing to spend
time and money to build products themselves. They prefer buying ready-to-use
208
M. Moritz et al.

products or kits with quality assurance and customer service. The market for 3D
printers is very interesting here: We ﬁnd both closed and open source vendors com-
peting in the same market.
In conclusion, OSS industry is proof of concept for viable and competitive business
models based on open source. In the realm of hardware, the emergence of companies
utilizing OSH can be observed, too. However, the tipping point is not yet reached
except for some niche actors. We don’t know yet if the story of OSH will be a game
changer as it was in the case of OSS. How will traditional industry face these new
challenges? Companies like Tesla and Toyota went a step towards openness when they
announced to grant free licenses for patented technology. More research on this phe-
nomenon is required to fully understand and describe the economics of OSH.
References
1. Weber, S.: The Success of Open Source. Harvard University Press, Cambridge (2004)
2. Benkler, Y.: The Wealth of Networks: How Social Production Transforms Markets and
Freedom. Yale University Press, New Haven (2006)
3. Von Hippel, E.: Democratizing innovation: the evolving phenomenon of user innovation.
J. für Betriebswirtschaft 55(1), 63–78 (2005)
4. Anderson, C.: Makers. Nieuw Amsterdam (2013)
5. Moritz, M., Redlich, T., Wulfsberg, J.: Value creation in open-source hardware commu-
nities: case study of open source ecology. In: 2016 Portland International Conference on
Management of Engineering and Technology (PICMET). IEEE (2016)
6. Gibb, A.: Building Open Source Hardware: DIY Manufacturing for Hackers and Makers.
Pearson Education, London (2014)
7. RepRap project. http://reprap.org/
8. MakerBot. http://de.wikipedia.org/wiki/MakerBot#cite_note-4
9. Wulfsberg, J., Redlich, T., Bruhns, F.: Open production: scientiﬁc foundation for co-creative
product realization. Prod. Eng. 5(2), 127–139 (2011)
10. Redlich, T.: Open Production Gestaltungsmodell für die Wertschöpfung in der Bottom-up-
Ökonomie. Springer, Heidelberg (2010)
11. Baldwin, C., von Hippel, E.: Modeling a paradigm shift: from producer innovation to user
and open collaborative innovation. Organ. Sci. 22(6), 1399–1417 (2011)
12. Pearce, J.M.: The case for open source appropriate technology. Environ. Dev. Sustain. 14(3),
425–431 (2012)
13. Pearce, J.M.: Building research equipment with free, open-source hardware. Science 337
(6100), 1303–1304 (2012)
14. Open Source Hardware (OSHW) Deﬁnition 1.0. http://www.oshwa.org/deﬁnition/
15. Von Hippel, E.: Innovation by user communities: learning from open-source software. MIT
Sloan Manag. Rev. 42(4), 82 (2001)
16. Pearce, J.M.: Quantifying the value of open source hardware development. Mod. Econom. 6,
1–11 (2015)
17. Raasch, C., Herstatt, C., Balka, K.: On the open design of tangible goods. R&D
Management 39(4), 382–393 (2009)
18. Balka, K., Raasch, C., Herstatt, C.: How open is open source? software and beyond.
Creativity and Innovation Manag. 19(3), 248–256 (2010)
Best Practices and Pitfalls in Open Source Hardware
209

19. Bonvoisin, J., Mies, R., Stark, R., Jochem, R.: Theorie Und Praxis in der Open-Source-
Produktentwicklung. In: Wulfsberg, J., Redlich, T., Moritz, M. (eds.): Konferenzband zur 1.
Interdisziplinären Konferenz zur Zukunft der Wertschöpfung, pp. 95–108. University press
HSU, Hamburg (2016)
20. Raasch, C., Herstatt, C.: How companies capture value from open design. Int. J. Inf. Decis.
Sci. 3(1), 39–53 (2011)
21. Ackerman, J.R.: Toward open source hardware. Univ. Dayton Law. Rev. 34, 183 (2008)
22. Yin, R.K.: Case Study Research: Design and Methods. Sage publications, Thousand Oaks
(2013)
23. Edmondson, A.C., McManus, S.E.: Methodological ﬁt in management ﬁeld research. Acad.
Manag. Rev. 32(4), 1246–1264 (2007)
210
M. Moritz et al.

Best Practice in Advanced Enterprise
Knowledge Engineering
Matteo Sasgratella and Alberto Polzonetti(&)
E-Linking On Line Systems (E_LIOS) UNICAM spin-off,
62032 Camerino, MC, Italy
{matteo.sagratella,alberto.polzonetti}@e-lios.eu
Abstract. The research work addresses mainly issues related to the adoption of
models, methodologies and knowledge management tools that implement a
pervasive use of the latest technologies in Semantic Web for the improvement of
business processes and Enterprise 2.0 applications. The contribution of research
that is intended to give, consists in the identiﬁcation and deﬁnition of a uniform
and general model, a “Knowledge Enterprise Model”, the original model with
respect to the canonical approaches of enterprise architecture. All studies and
analysis are ﬁnalized and validated by deﬁning a methodology and related
software tools to support, for the improvement of processes related to the life
cycles of best practices across the enterprise.
Keywords: Knowledge extraction  Ontology  Best practice
Information retrieval
1
Introduction
The role of knowledge began to be an important resource since the 80’s because it can
create value for the enterprise. It may be deﬁned as the most signiﬁcant resource of our
time, a kind of hallmark of modern society. The increased importance of knowledge
has marked a crucial step in modern theories of economics and business [1]. This does
not mean that up to this time there was no knowledge in enterprises, but simply that his
administration was done unconsciously and without considering the importance of the
relationship between knowledge and value to the company [1].
The possession of knowledge does not ensure the enterprise to obtain the beneﬁts
that could derive from it, but it requires interaction between people who possess such
knowledge and are able to create new. So, she made space the need to capitalize on the
business knowledge fostering the acquisition, re-use, dissemination, and creation. This
has resulted in a growing interest in the concepts of knowledge engineering and from
the role of organizational memory and processes of accumulation of organizational
knowledge. The creation of social enterprise is therefore attributable to an action aimed
at managing knowledge.
They play a major role in the context of corporate knowledge and are enablers for
optimal operation of processes. The proper management of the ﬂow of knowledge
involves the maintenance and continuous updating of Best Practice, avoiding obso-
lescence, and making sure that they are the basic elements of competitiveness.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_21

Speciﬁcally, this paper deﬁnes the life cycle of Best Practice to promote the continuous
improvement of business processes, whether production or coordination.
2
Knowledge Management Flow
The knowledge is manifested in different forms in the organization [2]:
• Tacit: represented by what people know but do not know how to express in a formal
way through the normal channels of communication, typically based on writing.
Tacit knowledge is closely linked to the person, depends heavily on the context and
it’s hardly made explicit and formalized typically being much less of a practical
nature.
• Implicit: is that component of the knowledge that you cannot or do not want to
express, but of which you are aware and that you would be able to explain, for-
malize, communicate.
• Explicit: available in documentary form in the format: Structured or when stored in
enterprise databases, management systems, processes, systems for the representa-
tion of knowledge that make use of ontologies.
Semi-structured: or when stored in the web pages of corporate intranets and the
Internet (based on HTML and XML). Non- structured: when stored in textual docu-
ments of any kind used in the organization. In particular, according to Nonaka and
Takeuchi, Japanese creators of the theory of knowledge, the creation of knowledge is to
be understood as a diffusion process in which knowledge is created by individuals
within the network of systematized knowledge of the organization. From here Nonaka
proposes the model called Organizational Knowledge Conversion which presents the
process of knowledge management as a spiral in which new knowledge is created
always [3]. Figure 1 shows the spiral on the dynamics of knowledge creation based on
conversions tacit/explicit through the processes of socialization, externalization, com-
bination, and internalization.
This spiral develops along two dimensions: the ﬁrst, called “epistemological”,
concerns the interactions between tacit and explicit knowledge, the second dimension,
the “ontological”, concerns the individual and the organization. According to this
model an organization is able to create knowledge only through individuals working in
it, in the hope, therefore, a valuation and an incurrence of the most creative by inserting
them in a collaborative environment in which knowledge is created.
212
M. Sasgratella and A. Polzonetti

This process is governed by the SECI model, proposed by the same Nonaka
(Fig. 2).
The SECI model consists of four modes of knowledge conversion: socialization
(tacit to tacit), externalization (tacit to explicit), combination (explicit to explicit), and
internalization (explicit to tacit).
• Socialization is the process of sharing tacit knowledge through observation,
imitation, practice, and participation in formal and informal communities.
Fig. 1. Spiral of organizational knowledge creation
Fig. 2. SECI model
Best Practice in Advanced Enterprise Knowledge Engineering
213

The socialization process is usually preempted by the creation of a physical or virtual
space where a given community can interact on a social level.
• Externalization is the process of articulating tacit knowledge into explicit concepts.
Since tacit knowledge is highly internalized, this process is the key to knowledge
sharing and creation.
• Combination is the process of integrating concepts into a knowledge system.
• Internalization is the process of embodying explicit knowledge into tacit
knowledge.
The SECI model shows that the process of knowledge creation is cyclical, was born
at the individual and develops at a group level, ending at the organizational level. In
order for this process to take place it is necessary to implement models of social
interaction that support the realization of the SECI process, among them we can
mention, in particular, the Ba and communities of practice. The model of social
interaction called Ba (Japanese term that means “place, a place, an arena for creative
exchange”), which was introduced by the same Nonaka deﬁnes the ways in which
people communicate in order to achieve the above process of converting knowledge
from tacit to explicit. The interactions can take place physically or virtually through
special software tools. Nonaka identiﬁes different types of Ba each specialized to
support a different stage of the process SECI: Originating Ba for the phase socializa-
tion, Dialoguing Ba for the’ externalization, Systemizing Ba to implement the com-
bination and Exercising Ba for internalization [4]. The interplay between the four
categories of ba is illustrated in Fig. 3.
Another model of representation of social interactions aimed at the generation and
development of corporate knowledge is that of communities of practice, groups of
employees who share a common heritage of knowledge and interact informally,
exchanging knowledge on issues of mutual interest; information exchanges result in the
generation and sharing of new knowledge. Communities of practice are easily
Fig. 3. Four categories of ba
214
M. Sasgratella and A. Polzonetti

implemented through collaborative tools of the social network business, plus the ability
to capture and capture a portion of tacit knowledge, making it explicit.
The ICT knowledge management are the real enabler of any strategy for knowledge
management. Understanding the potential offered by the different technological solu-
tions available is crucial for their correct application: the attempt to create a manage-
ment culture of shared knowledge, based solely on an organizational approach and on
the active collaboration between people, without the presence an appropriate computer
system, leads to results necessarily partial and potentially disastrous.
3
Benchmarking and Best Practice
The analysis of business processes to maximize the performance is certainly one of the
elements that characterize the management of an organization. One possible approach
to this problem is realized by comparing the processes characterizing their business
model with those of other organizations or with new models, assessing the charac-
teristics and ensuring any improvements. This approach is called benchmarking and
has been formally deﬁned as “the search for industry best practices that lead to have
superior performance” [5]. In other words, benchmarking is an activity which consists
in “learn, share best practices and adapt to a business reality” [6].
This operating mode essentially comprises three steps:
• understand the need to improve a process;
• ﬁnd a best practice that improves the process;
• Assess extent to which this process has been improved.
The ﬁrst two steps refer to a process of continuous research typical of bench-
marking, which therefore differs from traditional analysis of competitiveness as it is a
constantly evolving, which compares business processes with all the “better” that can
be researched or proposed, both inside and outside the organization. This research has
potential limitations, since any process can be challenged by another at any time, the
more it stimulates the creativity of the business community and it is open to all possible
instances of itself, the more it is likely to ﬁnd a best practice that take the place of a
process currently in place.
The last phase involves, essentially, the veriﬁcation of the performance of the best
practices applied to the process that you are trying to improve, whereas the concept of
performance is very variable and dependent on the particular organization you are
analyzing. It seems clear that the concept of best practices is central in the whole
organization and how much goes to impact on all business processes. A best practice
can be deﬁned as “the best way currently known to implement a particular business
process”, the adjective “best” is often questioned, preferring the term good practice,
highlighting the fact that this solution is subject to a continuous cycle of monitoring
and review, which often leads to ﬁnding another best practice for the same process. It
can, therefore, afﬁrm that a best practice constitutes the operating model currently used
to implement a series of similar processes, but which must be continually questioned,
with the aim of ﬁnding another that further improve the performance of the process.
Best Practice in Advanced Enterprise Knowledge Engineering
215

Many best practices are inherent in the company’s tacit knowledge, that certain
processes are carried out under conditions resulting from the practice, but not formally
deﬁned. Therefore, in addition to managing the best practices deﬁned, it is appropriate
to prepare strategies to try to extract the tacit, perhaps preparing communities of
practice, communicate, and collaborate through social networks. The two approaches
are complementary in the sense you cannot think of extracting best practices from the
corporate tacit knowledge without good organization of best practices already
formalized.
The essence of best practices is therefore to ﬁnd a better solution to a given problem
and to share in the company, with immediate advantages of different types: [7]
• identify and replace practices “poor” and obsolete;
• minimize costs through improved productivity and efﬁciency;
• facilitate the resolution of problems through the search for solutions to similar
problems;
• ensure consistency, having the certainty that a given business process is always
done in a certain way;
• decrease the time of learning, because of the greater simplicity of creating educa-
tional materials for standardized processes;
• improve the overall quality of the products or services offered.
In the literature point to six fundamental points for the identiﬁcation and sharing of
best practices [8]:
• Identify the real needs: never lose sight of the main purpose of a best practice, or
improve a business process in order to understand where we can provide added
value to the organization. In this sense, in search of best practices should involve
primarily corporate sectors in difﬁculty or performance deﬁcit.
• Discover best practices: there are potentially many ways to ﬁnd a best practice.
A ﬁrst approach consists in observing the operating procedures that have produced
good results in other organizations and assess whether they can become best
practices for their own. A second approach is embodied in formalizing a best
practice, starting from the above company tacit knowledge, which is going to shape
a practice applied in an informal way thanks to the skills of workers. A ﬁnal
approach is to harness the creativity of the workers, who should have the oppor-
tunity to promote their own ideas to improve a process, the best insights could be
explored and eventually formalized as a best practice.
• Template: the description of the good practice is generally placed in a repository
according to a standard format. A typical template should include the following
sections:
• Title - short descriptive title (may be accompanied by a short abstract);
• Proﬁle - short sections that outline processes, function, author, keywords, etc.
• Context - where it is applicable? What problem does it solve?
• Resources - what resources and skills are needed to carry on the good practice?
• Description - what are the processes and steps involved?
216
M. Sasgratella and A. Polzonetti

• Margins for improvement - there are good performances associated with this
practice?
• Tools and techniques used
• Validate best practices. Best practice is cyclical in nature, in the sense that its actual
dowry to be “better” must be constantly challenged through precise operations
evaluation of existing best practices and validation of new ones. A usual approach is
to have a team of auditors including external experts, both internal and external to
the organization. Equally important are the input and feedback (i.e. the last bene-
ﬁciaries) of best practices.
• Dissemination and application. A database of good practices is a useful starting
point, but many organizations are essential to accompany him with a face-to-face
sharing of the knowledge of good practice. Usual ways of knowledge sharing of
good practice include: communities of practice, improvement groups, learning
events organized, etc.
• Develop and support infrastructure. It is needed to make sure that you have the
necessary infrastructure to have a proper management best practices. This infras-
tructure is generally included in the more general business knowledge, so much so
that he has often mentioned that the best practices involving both tacit and explicit
knowledge. A modern social network, an advanced DMS and a suite for semantic
knowledge management constitute the minimum equipment to implement a modern
management best practices.
There are different methods of business process management using best practices as
a driving force. The following is a list of those considered most relevant, with no
presumption of completeness:
• Six Sigma: This methodology, which was introduced in the 80 s by Motorola, is its
assumptions in the approach of Deming’s PDCA cycle, the model developed for the
improvement of quality on a long haul through the continuous reﬁnement of pro-
cesses and to an optimal use of resources. Six Sigma seeks to combine the European
trend to improvement through systematic changes (breakthrough) with the Japanese
approach to continuous improvement pursued through “baby steps”. All business
processes are analyzed, with the assertion that each process can be measured, and
that therefore it is possible to intervene with measures to improve only after you
have performed the measurements of the characteristic parameters or indicators
more representative and having analyzed the data thus obtained [9].
• TOGAF: created in 1995, is based on the Technical Architecture Framework for
Information Management (TAFIM) of the Department of Defense of the United
States of America. It is a standard infrastructure to handle the corporate structure,
with the ultimate goal of maximizing productivity and process performance.
TOGAF consists of three basic parts: the Architecture Development Method
(ADM), which describes how to get enterprise architecture speciﬁc to a particular
organization that meets certain requirements, the Enterprise Continuum, a sort of
repository of all possible assets of the company that can be taken as an example to
develop its architecture, the TOGAF Resource Base, a set guidelines, templates and
various information useful to support architects in the use of ADM [10].
Best Practice in Advanced Enterprise Knowledge Engineering
217

• eTOM: The Business Process Framework (eTOM), formalized in 2003, is a multi-
level, hierarchical view of business processes deemed necessary to achieve an
efﬁcient and proﬁtable. At a conceptual level, the framework has three main process
areas: Strategy/Infrastructure/Product, Operations and Management. The strong
point for this approach is to encourage reusing Process from different organizations
with a consequent lowering costs while improving Performance Process [11].
• ITIL: providing a broad set of best practices related to IT processes, ITIL is one of
the most widely used framework for the management of the companies or IT
departments. Its key element is the continuous evaluation and possible improvement
of services offered, both from the company point of view and from that of the client.
Published for the ﬁrst time in England between 1989 and 1995 by Her Majesty’s
Stationery Ofﬁce (HMSO) under the aegis of the Central Communications and
Telecommunications Agency (CCTA), was initially used almost exclusively in the
United Kingdom and the Netherland, from its second version, developed since
2000, its use has spread on a global scale and is probably one of the most
well-known frameworks [12].
• Kaizen: Japanese management strategy which means “continuous improvement”.
Its deﬁnition is derived, in fact, from the Japanese words “kai” which means
“continuous” or “change” and “Zen” which means “improvement”, “better” This
method encourages and supports small improvements to be done day after day, in a
continuous manner. The kaizen, initially presented by Toyota in the 80 s and
applied increasingly in the world, is based on the principle of the emergence of
corporate knowledge from below, i.e. on the understanding that the performance of
business processes can be increased only through the analysis of the proposed
improvements by workers in the ﬁeld of collaborative [13].
4
Best Practice and Business Competencies
The study of various business skills, particularly in relation to the fact that a worker
who is in possession or not, is often approached with an assessment resulting from
studies carried out in 1980 by brothers Stuart and Hubert Dreyfus faculty at the
University of California [14]. This scale describes, dividing it into ﬁve levels (Fig. 4),
the phase of acquisition of skills by the worker, through which he became adept at
grooming a given process:
Table 1 describes for each of these levels, the manner in which an employee who as
that level of competence for a given process, it relates to a best practice that describes it:
218
M. Sasgratella and A. Polzonetti

Table 1. Dreyfus model and best practice
Stage
Best practice
Novice
Requires a best practice to perform an operation.
It is able to judge the performance resulting from its application to a process.
It needs training to learn the elements described
by the best practices.
Advanced
beginner
Ability to independently perform many steps of the operation sequence on
the process.
It can judge the performance of the process.
It should be followed carefully as the increasing conﬁdence in their own
ability could lead to the belief that they can change the process, yet without
having the necessary expertise.
Competent
Knows and executes the process without the need for training.
It can judge the process in its entirety.
Can think of changes to the process, that is, new best practices, as well as
mindful of mistakes when it belonged to the previous levels.
Proﬁcient
It’s able to propose improvements to the process, or new best practices.
It’s able to establish similarity with other processes.
Must be discouraged from changing the process without following a best
practice.
Expert
Can both propose best practices that assess those proposed by others.
It should be encouraged to think more and new improvements to the
process.
Must be discouraged from changing the process without following a best
practice.
Fig. 4. Novice-to-expert scale
Best Practice in Advanced Enterprise Knowledge Engineering
219

It is evident that more experienced it becomes a worker in a particular process, the
more you run the risk that he may turn away from best practices. Minimize this risk
involves not only bind workers to follow best practices, but also entice workers with
high levels of competence in the provision of new best practices.
5
Knowledge Enterprise Model: Best Practice
From the analysis made in the previous paragraphs show the need to address two key
issues for modern organizations: knowledge and process performance. The latter is
achieved by managing the most of a particular area of business knowledge, those best
practices introduced earlier. The strength of the model is proposed, shown in Fig. 5, is
to leverage corporate knowledge to better manage the life cycle of best practices and
consequently improve the performance of business processes.
The cycle is divided into two main phases: a formal and one informal. Formal refers
to the management of best practices with regard to the nature of their improvement to a
process, in other words, this phase sees the best practices as a formal proposal for the
improvement of a business process, typically made explicit by means of special tem-
plate document. A proposal, whether it is a compilation of a new or a revision to an
existing best practice, follows a precise business process before becoming possibly
Fig. 5. Best practice life cycle
220
M. Sasgratella and A. Polzonetti

process standards. This path depends strictly on the organization, but the steps that
compose it can be traced to the following steps:
• Compile/Revision: is the creation of new best practices or bring changes to existing
BP. During this phase, an issue resolved, such resolution shall be made available to
everyone in the company, entering all the information into a template set up and
submitting to being published such best practices.
• Publish: completed the drafting process, the best practice passes being published to
undergo the procedure for ofﬁcial authorization and validation to become
operational.
• Validate: is the time in which the best practice is subjected to a control both formal
content, before being sent to ﬁnal approval.
• Authorize: is when the best practices, as well as approved in its form and content, is
emitted and thus made available to other users of the system, and in general, all
users who have the necessary access rights. Typically, is evaluated, empirically, in
some cases, the improvement in performance of which has described the process
through the implementation of best practices. This phase, like the three previous
ones, by one or more persons empowered, which can operate independently or in
collaboration with specialized tools (wiki, forum, etc.).
The phase described above only covers the steps of combining and subsequent
internalization of the SECI model that deals exclusively with the explicit knowledge,
for more in a strictly structured. To lower management best practices in the entirety of
enterprise knowledge management, it is necessary to introduce a phase called informal,
that manages to bring out instances arising from tacit knowledge and implicit. This
phase can be deﬁned by the following steps:
i. Sharing proposals for improvement: after the phase of socialization, made pos-
sible through the collaborative tools provided by the social network business, you
can enjoy the considerations expressed by workers in reference to a particular
business process. Their tacit or implicit knowledge can lead to deduce proposed
improvements resulting from the practice in carrying out a speciﬁc process or
even from simple intuitions.
ii. Analysis of proposed candidates. The proposals that emerged in the previous step
are discussed and reﬁned by the user community, thanks to the outsourcing
process. At this point the knowledge of individual workers has been clariﬁed and
is potentially available to the entire business community.
iii. Selection of proposals for improvement. The knowledge has emerged, at this
point, needs to be conveyed to the processes on which the company has need or
interest to invest.
6
Conclusions
Best practices are a powerful way to leverage corporate knowledge, increasing com-
petitiveness through continuous improvement and constant asset of the organization.
Knowledge, whether express, implied, or constructive, of the workers is used to bring
Best Practice in Advanced Enterprise Knowledge Engineering
221

continuous improvements to business processes. If explicit knowledge is easily man-
aged through a workﬂow like a document management, tacit or implicit needs to be
extracted and fed. An ideal approach is to examine the activities of the workers on the
social network business, in particular the interactions and communications made
through collaborative tools. In this way, the activities are possible externalization and
internalization to make explicit the implicit knowledge and make manifest the tacit. For
these purposes collaborative tools, typical of Web 2.0 and Enterprise 2.0 are not
characteristic enough, we need more sophisticated tools that are based on emerging
semantic technologies in the Semantic Web.
To achieve interoperability and the development of knowledge models extracted,
the challenges of the future in the following directions:
• Ontology alignment. Study, deﬁnition, and development of the approaches for
extracting approximate matching in order to harmonize heterogeneous ontology
conceptualization. The result of a matching operation is the evaluation of relation
between two ontologies. Concepts matching enable us to augment knowledge
discovery performances.
• Ontology Merging. Study, deﬁnition, and development of the approach to create
ontology from two or more source ontologies. The new ontology will unify and in
general replace the original source ontologies. By the merging of the concepts
ontology, we intend to support knowledge Extraction applications through reduc-
tion of redundancy.
References
1. Hislop, D.: Knowledge Management in Organizations: A Critical Introduction. Oxford
University Press, New York (2013)
2. Studer, R., Burghart, C., Stojanovic, N., Thanh, T., Zacharias, V.: New dimensions in
semantic knowledge management. In: Towards the Internet of Services: The THESEUS
Research Program, pp. 37–50. Springer International Publishing (2014)
3. Nonaka, L., Takeuchi, H., Umemoto, K.: A theory of organizational knowledge creation. Int.
J. Technol. Manag. 11(7–8), 833–845 (1996)
4. Oyemomi, O., Liu, S., Neaga, I.: The contribution of knowledge sharing to organizational
performance and decision making: a literature review. Decision Support Systems
IV-Information and Knowledge Management in Decision Processes, pp. 1–12. Springer,
Cham (2015)
5. Bruno, I.: Benchmarking. In: Encyclopedia of Quality of Life and Well-Being Research,
pp. 363–368. Springer, Netherlands (2014)
6. Boshyk, Y., (Ed.).: Business driven action learning: global best practices. Springer (2016)
7. Garrison, L.P., Towse, A., Briggs, A., De Pouvourville, G., Grueger, J., Mohr, P.E., Sleeper,
M.: Performance-based risk-sharing arrangements—good practices for design, implemen-
tation, and evaluation: report of the ISPOR good practices for performance-based
risk-sharing arrangements task force. Value in Health 16(5), 703–719 (2013)
8. Esteves, J.M.: An empirical identiﬁcation and categorisation of training best practices for
ERP implementation projects. Enterp. Inf. Syst. 8(6), 665–683 (2014)
222
M. Sasgratella and A. Polzonetti

9. Pyzdek, T., Keller, P.A.: The Six Sigma Handbook, p. 25. McGraw-Hill Education, New
York (2014)
10. Cabrera, A., Abad, M., Jaramillo, D., Gómez, J., Verdum, J.C.: Deﬁnition and implemen-
tation of the enterprise business layer through a business reference model, using the
architecture development method ADM-TOGAF. In: Trends and Applications in Software
Engineering, pp. 111–121. Springer, Cham (2016)
11. Priya, R.L., Giri, N.: Comparative study of various next generation network-business process
framework. Int. J. Comput. Appl. 102(4), 1–6 (2014)
12. Suhairi, K., Gaol, F.L.: The measurement of optimization performance of managed service
division with ITIL framework using statistical process control. JNW 8(3), 518–529 (2013)
13. Estácio, B., Prikladnicki, R., Morá, M., Notari, G., Caroli, P., Olchik, A.: Software kaizen:
using agile to form high-perfomance software development teams. In: Agile Confer-
ence AGILE 2014, pp. 1–10. IEEE, July 2014
14. Dreyfus, S.E., Dreyfus, H.L.: A ﬁve-stage model of the mental activities involved in directed
skill acquisition (No. ORC-80-2). California Univ Berkeley Operations Research Center
(1980)
Best Practice in Advanced Enterprise Knowledge Engineering
223

Production Flow Improvement in a Textile
Industry
Jose J. Lopes1, Maria L. R. Varela1, Justyna Trojanowska2(&),
and Jose Machado3
1 Department of Production Systems, University of Minho, Campus of Azurém,
4800-058 Guimarães, Portugal
zjzejoao@hotmail.com, leonilde@dps.uminho.pt
2 Chair of Management and Production Engineering Springer,
Poznan University of Technology, ul. Piotrowo 3, 61-138 Poznan, Poland
justyna.trojanowska@put.poznan.pl
3 Mechanical Engineering Department, University of Minho,
Campus of Azurém, 4800-058 Guimarães, Portugal
jmachado@dem.uminho.pt
Abstract. Enhance production ﬂow in a Company is of prior importance, not
just regarding the general functioning of the underlying production system itself
but also regarding all the more or less closely related issues, varying from the
warehouse organization and materials handling and space management, to
general productivity improvements to reach by a Company. In this paper is
presented a case study regarding the analysis of production ﬂow in a textile
company in Portugal, and the main improvements proposals are exposed and
brieﬂy described.
Keywords: Production ﬂow  Textile industry  Warehouse  Case study
1
Introduction
Production improvement problem is still important research problem because the
proper organization of work determines the productivity of production system. The
productivity directly translates into the proﬁt, which is the primary goal of manufac-
turing companies exists.
Especially in job-shop production system, in which similar workstations are
grouped together and ﬂow of processing materials are discontinuous because of needs
to transport them from one workstation to another, organization of production ﬂow
conduct a crucial role in a company. In this paper, the production ﬂow of a Textile
Company is improved through the application of lean philosophy.
The aim of the conducted research was to improve the production system, by
reducing or eliminating wastes identiﬁed. This required an analysis of the entire pro-
duction process, from the input of the raw material to the output of the ﬁnal products or
the semi-ﬁnished products. This study also sought to determine the maximum capacity
of each production station and further enhance this capacity in order to increase
productivity.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_22

2
Literature Review
Improvement of manufacturing processes is challenging due to the complexity of the
issue. Increasingly often, companies use decision-making support methods for this
purpose [1–3]. The literature describes many applications of the statistical methods and
tools to gain insight into the manufacturing process [4–7] or to minimize losses [8–10].
Manufacturing companies increasingly use dedicated methods and systems for mod-
eling and simulation of production processes (soft modeling and hard modeling
methods), instead of costly Trial & Error methods [11]. Recently, competitiveness of
manufacturing enterprises depends on effective management which can be supported
by lean production.
The another crucial indicator in a new industrial textiles culture named sustainable
production that strongly improve manufacturing processes is the life cycle concept. It
means that products are designed for their whole life cycle i.e. fabric transport, product
transport, packaging, production wastes, distribution, retail, and end of life waste
disposal with minimized (acceptable) inﬂuence on the environment, occupational
health and use of resources. Consequently, it is completely clariﬁed that various cost
types should be taken into account, including over the whole life of the product from
concept to end-of-life, and related to an relevant measurement unit of value [12].
According to Womack [13], Lean Production is an organizational production model
that focuses on fostering a philosophy of continuous improvement, eliminating
wastefulness in the process, reducing unnecessary costs, and thereby increasing pro-
ductivity and customer satisfaction. In other words, it is a system with fewer inputs to
achieve the same objectives as traditional mass production systems, but it offers a
greater variety of products [14].
It allows, according to Liker [15], to reach smaller cycle times through the elimi-
nation of waste. In this case, the goal is to increase the time value that is added during
the production process, reducing the time between the customer order and the ﬁnal
delivery. By eliminating the waste and the added value time, it is possible to reduce the
time of process throughput, that is, the time from when the raw material enters until it is
incorporated in the ﬁnal product.
The Lean Production concept had a greater impact when it was featured in the book
“The Machine That Changed the World” written by J.P. Womack, D. Jones and D.
Roos in 1990 to describe the Toyota Production System (TPS) developed by Toyota
[16].
TPS was initially seen as a survival strategy. Developed by Toyota Vice President
Taiichi Ohno and Shigeo Shingo [17] at that time, the intention was to maintain capital,
reduce unnecessary stocks, stop wastes, reduce production times and reduce the rigidity
of it. High investments were not foreseen and the whole business would be customer
oriented, also introducing error prevention processes [18]. This system was evaluated
positively and later implemented throughout the Company.
Production Flow Improvement in a Textile Industry
225

3
Production Flow Improvement
Since the markets are becoming more competitive, the textile J.F. Almeida, in Portugal
does not want to take the risk of being left behind. That is why it is able to suggest a
wide range of logos, names, articles and colors suitable for promotional and publicity
actions. The Company is constantly represented in several international fairs, with the
intention of making the products known to potential new customers.
3.1
Flow of Material and Production Processes
Due to the high production and response capacity that the company presents, besides
working with internal work, it also operates as service providers. He receives in the
warehouse work from his partners, but also from other customers. This raw work goes
through the same processes regardless of whether it is internal or external, differing
only at the end since the last process differentiates according to the origin of the work.
The production process steps are:
• When the raw work arrives at the warehouse, a sheet that accompanies it with the
necessary information is made. There are two separate storage areas for housework
or service provision.
• The ﬁrst production process is to open or unroll the work. Crude goods are wrapped
in tubes and must be placed in transport trolleys to facilitate the following pro-
cesses. This operation has at its disposal two machines.
• The next process is dyeing. Dyeing is equipped with 17 jets of varying capacities,
ranging from 120 kg to 920 kg. The choice of the most appropriate jet takes into
account the size and weight of the work in question. This procedure can comprise
different types of dyeing from reactive, direct, vats, desiccants, bleaching, among
others.
• After removing the work from the machines, it is discharged to the basins. At this
moment the merchandise is ﬁlled with enough water, so the next process is to
hydrate. This procedure has three machines available and removes as much water as
possible.
• Once the water is removed, the work continues in basins, being necessary to put it
back in cars. This process is very similar to the ﬁrst one and is limited to opening
the work, placing it in a transport car, with only one machine available.
• When the work enters part of the ﬁnishes, usually the ﬁrst process passes through
the tumbler. The Company has two of these machines and it is in them that a
pre-drying and lifting of the article’s ring takes place. The fact that it is a plush or
quilt can inﬂuence the choice of machine.
• After drying, the work goes through machine ‘Ramula’. There are also two avail-
able ones and they are responsible for setting the measurements and ﬁnishing the
article.
The processes described above are common to the internal article and to the pro-
vision of services. It is at this point that there are differences.
226
J. J. Lopes et al.

• If the work is for services, it can go directly to the customer after ramming or being
rolled. If the customer wishes to receive the work in a roll, the Company has three
machines available and that are chosen according to the type of article in hands.
• If the work in hands is internal, after rambling it goes to the confection. In this
sector, the goods go through the cutting machine, through the longitudinal and
transverse machines. The accessories and labels are placed, followed by packing
and shipping.
3.2
Initial Situation - Layout
This section presents the Company’s situation at the time the project was started,
focused on the dyeing and ﬁnishing of articles. All the surrounding processes were
analyzed and sketched with the intention to perceive where it was possible to intervene
and improve them, in order to improve the Company’s organization, processes, and its
production ﬂow. The layout of the Company with its material ﬂow is shown in Fig. 1.
The productive process begins with the allocation of works in the warehouse of
plush and quilt. It then passes to the machines shown in brown in Fig. 1, followed by
the jets shown in yellow. The next process is in the pink machines, the ‘hydros’, and
then the openwork in the machine is painted white. In the ﬁnishing zone the ﬁrst
process occurs in one of the tumblers, represented in blue, and then in the ‘ramulas’,
symbolized in red. As already mentioned, if the work is from the house goes to the
confection, and if it is service, is rolled in one of the three machines illustrated in green.
The layout of the company presents itself confused and not optimized, with the
work always going from one side to another. At peak times of production, there are
often conﬂicts in the path of the commodity that constantly crosses between the dif-
ferent processes.
3.3
The Suggested Improvements
In this work a set of main improvements are proposed for being implemented in the
Textile Company, mainly related to an over production system process and its pro-
duction ﬂow optimization, but also regarding the defect registration process, and the
Fig. 1. Company’s layout illustration.
Production Flow Improvement in a Textile Industry
227

multi-functionality of the workers, to reach overall improvements in the productive
system and on its productivity levels.
It is necessary to increase the actual capacity of the stations in order to bring it
closer to the theoretical maximum capacity planned by the Company. So, it is possible,
with some changes in the way employees operate, to get better results. For this, an
extensive analysis was made to the setup and transport times of all processes.
Firstly was established a list of all the operations and consequent division into
internal and external ones. The objective was to turn the internal ones, whenever
possible, in external ones to avoid the prolonged stoppage of the machines. It is
important to notice that the proposed suggestions are not always feasible. In punctual
cases of urgent orders, lack of work or material, the way of operating can be inﬂuenced.
Through the analysis carried out for analyzing the production capacity of each
station during the cycle time, it was concluded that the biggest obstacle was related to
the ‘hydros’. This situation, coupled with the preparation times that took place in this
process, led to the creation of a manual of practices. The objective was to train the
operators and sensitize them to change the way they operate so that they could increase
the productivity of its tasks.
3.4
Layout Changes
As far as the storage area is concerned, there are two different ﬂoors. The upper ﬂoor
has mostly work for confection, while the lower ﬂoor has raw material, departures for
dry cleaning, rolled and rolled work. Figure 2 shows the distribution of space in the
lower storage zone.
Starting from the machines where the work is opened, as already mentioned,
sometimes it is difﬁcult to get to the desired departures, located in zone 6. When the
work is opened, it is also difﬁcult to ﬁnd space to allocate it in the warehouse, zone 4,
while waiting to get into the ‘jets’. For rolling machines, the incoming work is allocated
to zone 5 and then reworked into zone 2 or 3.
Fig. 2. Warehouse layout improvement.
228
J. J. Lopes et al.

The proposed layout changes are intended mainly to reduce space problems, to
avoid wasting time unnecessarily and to reduce machine downtime.
Following are proposals for improvement that have in common the passage of the
rolling machines to the upper ﬂoor, the change of location of the tumbler of ﬁnishes, the
creation of a dock for freight and another for discharges, the reorganization of the
warehouse of the ﬂoor as well as an attempt to improve the ﬂow of materials.
Finishing Area. With the change of the tumbler from the ﬁnishing area to the
warehouse, the free space would be used only to place the work that will enter the jets.
In other words, the work previously stored in zone 4 would be relocated, getting much
closer to the jets. The zone 4 is approximately 18 m long and 5.40 m wide, which
makes 97.2 m2. However, all this space cannot be occupied with trolleys because it is
necessary to leave space of passage. The new area, represented in the ﬁgure next to
lilac, has approximately the same space (5  19 m) but allows a greater use because
the trolleys can be placed side by side along the 19 m, leaving still space to allocate
some in “2nd row”.
Taking into account that 1.3 m wide is enough to pass a cart, and since from the
wall on the left side of the ﬁgure to the formula are about 7.55 m, it is possible,
in situations of greater confusion, to allocate a few more carts in the zone represented in
blue, without compromising the passage to the dry cleaning.
Rolling Machines. The passage of the rolling machines upstairs has some advantages.
In the ﬁrst place, it allowed all the work that leaves the model, regardless of whether it
is internal or service, to go directly to the top ﬂoor. A space in the downstairs storage
room next to the elevator would be safeguarded to accumulate some work that could
not rise immediately, since there may be a temporary lack of upstairs space or the
elevator is not available. This space, marked in the layout of the warehouse with zone
5, is about 4.40 m wide and 25.2 m long, so it is possible to accumulate more than 20
trolleys, without compromising the passage.
The passage of the three rolling machines to the upper ﬂoor would make the work
waiting to be rolled up, as well as the work already rolled up, to stop being in the lower
warehouse, following the same path as the machines. The zone 3 of the warehouse,
reserved for the rolled work, has about 180 m2. To this space it is necessary to add a
few m2 of zone 2, since it is also used to store rolled work. Taking into account that
zone 2 is approximately 34 m2 (17  2), it is assumed that half will be for bedspreads
and another half for rollers. 180 m2 + 17 m2 = 197 m2. To this space, it is necessary to
add the area where the rolling machines are and the area where currently the matches in
raw are also free. At the total, there are another 68 m2 (4.7  14.5) of the rolling
machines and 97,2 m2 of the zone 4. It is possible to conﬁrm that this change would
release 362,2 m2 (197 m2 + 68 m2 + 97,2 m2) in the downstairs.
The space reserved for the work to be rolled (zone 5 in the initial layout) is not
counted because it will continue to be used for a similar purpose. Instead of storing the
work to be rolled up, it will pass, as previously mentioned, to store not only the work to
be rolled but also the work to be done waiting for the transport to the top ﬂoor. The new
layout of the warehouse shown in Fig. 3.
Production Flow Improvement in a Textile Industry
229

Creation of a Loading Dock and Other Discharge Dock. With the change of the
rolling machines to the top ﬂoor, all the work that leaves the model will follow, as soon
as possible, to the top ﬂoor. What is proposed is to make better use of the existence of a
quay on each ﬂoor of the company. The suggestion is to use the dock downstairs
exclusively to unload merchandise and the dock from the top ﬂoor to load. The work
already done, packed or rolled up is all on the top ﬂoor, so it is justiﬁed that it is also
loaded there, so there is no need to go back down. This alternative allows loading and
unloading simultaneously, faster and without so much space problems.
Tumbler Change. Measurements were made to ﬁnd ways to place the two tumblers as
close as possible to each other. It is understood that it is advantageous because in this
way both can be in operation resorting to only one worker, a situation that does not
currently occur. It was concluded that it is impossible to put the tumbler of the
warehouse in the area of ﬁnishes, so it was analyzed the hypothesis of putting both in
the warehouse. New tuber location is shown in Fig. 4.
Fig. 3. New layout of the warehouse.
Fig. 4. New tumbler location.
230
J. J. Lopes et al.

From the wall of the warehouse to the yellow line marking the path are about
14.2 m (5.8 + 0.3 + 5.4 + 0.3 + 2.4). The tumbler that already there occupies 5.8 m,
from the wall to the ﬁrst column. From this point to the next column are more 5.4 m.
The tumbler of ﬁnishes has, with the doors closed, about 4 m, plus a protrusion of
0.30 m wide and 14.5 long (including 2 trolleys with workmanship behind and a trolley
with work on the front). If you lean against the second column, there is a gap of 1.4 m
between each to move the trolleys. On average the carts are 1.1 m wide, so the space
seems to be sufﬁcient, however, if the doors are open, the carts cannot get through. In
the ﬁgure it is possible to check in red the location of the tumblers and blue the route
available for the carts. In this case the course of the work in the tumblers would be from
right to left.
With this solution the space represented in green (2.4  17.3 m) would be used to
place the work that is waiting to enter the tumbler, and the space of the current rolling
machines indicated in blue (4,7  14,5 m) would serve to allocate the work that was
once to the tumbler and is waiting for the formula.
3.5
Reorganization of Material Flow
With the change of the tumbler it is fundamental that both the ‘ramulas’ have to work
in the same direction. Nowadays the ‘ramulas’ work in opposite directions, that is, on
the one hand they enter work on one of them and work on the other, and on the other
side it is the opposite. This creates some confusion in the movement of trolleys. It is
most advisable that both ‘ramulas’ load the work on the side of the tumblers and that
the work leaves the side of the elevator, thus facilitating the transportation of the carts.
In the Fig. 5 is represented the new material ﬂow with the proposed changes. Thus,
the work that comes from the dry cleaning is waiting in the green zone to enter the
tumblers represented in blue. When you leave the tumbler, if necessary wait in the area
ahead (pink), until entering the ramblings, marked in red. After passing the ramps, exit
through the other side and if possible climb up the elevator to the top ﬂoor. If it is not
possible immediately, wait in the reserved area, in the gray ﬁgure.
Fig. 5. Reorganization of material ﬂow.
Production Flow Improvement in a Textile Industry
231

• The height of the dock in the top warehouse is approximately 1.20 m and may not
be compatible with some lorries, getting a little higher or lower and it is necessary to
adjust.
• The lift is 3.10  2.45 m which means that some trolleys with bedspreads may
have difﬁculty to entry.
• The change of tumbler implies a stop in its functioning never inferior to 2 weeks.
• The formula that is intended to be reversed is already relatively old, so it will
probably be more proﬁtable at the monetary level to eliminate it and put in its place
a new or almost new already in the right orientation. In this way the energy costs
would be smaller than the existing one.
4
Conclusions
During the data collection, some obstacles were observed to the normal production
ﬂow in the different stations. A confusing layout, poor utilization of station capacities
and a wrong way of operating by workers were some of the major ﬂaws detected.
A closer analysis also led to the conclusion that the lack of available space and
transport cars was one of the greatest setbacks common to all jobs. This lack of
resources was one of the reasons why production rates sometimes conducted to situ-
ations with capacity values much below the maximum capacity.
With a focus on solving the problems observed, improvements were proposed that
would help to achieve productivity improvements. Training and raising the awareness
of the workers on how best to work, ensure their satisfaction and keep them motivated,
where the ﬁrst suggestions implemented successfully. A manual of practices was
created and handed over to those in charge to help increase productivity and ﬂow at the
critical points of the system and proposed a reorganization of the layout that allowed to
free space within the production as well as a better and simpler organization of the
productive ﬂow. Other small suggestions have also been proposed and implemented
with the intention of ensuring that the product reaches the ﬁnal customer in perfect
condition and that all possible defects are detected during production. Therefore, one of
the requirements of the Company is now fulﬁlled, which is to enhance existing
resources and increase production, not neglecting the quality that it has recognized.
References
1. Rogalewicz, M., Sika, R.: Methodologies of knowledge discovery from data and data mining
methods in mechanical engineering. Manag. Prod. Eng. Rev. 7(4), 97–108 (2016)
2. Kłos, S., Patalas-Maliszewska, J.: Throughput analysis of automatic production lines based
on simulation methods. In: Jackowski, K., Burduk, R., Walkowiak, K., Wozniak, M., Yin,
H. (eds.) Intelligent Data Engineering and Automated Learning – IDEAL 2015. Lecture
Notes in Computer Science, vol. 9375, pp. 181–190. Springer, Cham (2015)
232
J. J. Lopes et al.

3. Varela, M.L.R., Ribeiro, R.A.: Distributed manufacturing scheduling based on a dynamic
multi-criteria decision model. In: Zadeh, L., Abbasov, A., Yager, R., Shahbazova, S.,
Reformat, M. (eds.) Recent Developments and New Directions in Soft Computing. Studies
in Fuzziness and Soft Computing, vol. 317, pp. 81–93. Springer, Cham (2014)
4. Bożek, M., Kujawińska, A., Rogalewicz, M., Diering, M., Gościniak, P., Hamrol, A.:
Improvement of catheter quality inspection process. In: MATEC Web of Conferences, vol.
121, p. 05002, 8th International Conference on Manufacturing Science and Education –
MSE 2017 “Trends in New Industrial Revolution” (2017). https://doi.org/10.1051/
matecconf/201712105002
5. Sika, R., Rogalewicz, M.: Demerit control chart as a decision support tool in quality control
of ductile cast-iron casting process. In: MATEC Web of Conferences, vol. 121, p. 05007, 8th
International Conference on Manufacturing Science and Education – MSE 2017 “Trends in
New Industrial Revolution” (2017). https://doi.org/10.1051/matecconf/201712105007
6. Rojek, I., Kujawińska, A., Hamrol, A., Rogalewicz, M.: Artiﬁcial neural networks as a means
for making process control charts user friendly. In: Burduk, A., Mazurkiewicz, D. (eds.)
Intelligent Systems in Production Engineering and Maintenance – ISPEM 2017. Advances in
Intelligent Systems and Computing, vol. 637, pp. 168–178. Springer, Cham (2017)
7. Santos, A.S., Varela, M.L.R., Putnik, G.D., Madureira, A.M.: Alternative approaches
analysis for scheduling in an extended manufacturing environment. In: Proceedings of
Nature and Biologically Inspired Computing, pp. 97–102 (2014)
8. Kujawińska, A., Vogt, K.: Human factors in visual quality control. Manag. Prod. Eng. Rev.
6(2), 25–31 (2015). https://doi.org/10.1515/mper-2015-0013
9. Rewers, P., Hamrol, A., Żywicki, K., Bożek, M., Kulus, W.: Production leveling as an
effective method for production ﬂow control – experience of Polish enterprises. Procedia
Eng. 182, 619–626 (2017)
10. Kujawińska, A., Diering, M., Rogalewicz, M., Żywicki, K., Hetman, Ł.: Soft modelling-based
methodology of raw material waste estimation. In: Burduk, A., Mazurkiewicz, D. (eds.)
Intelligent Systems in Production Engineering and Maintenance – ISPEM 2017. Advances in
Intelligent Systems and Computing, vol. 637. Springer, Cham (2018)
11. Sika, R., Hajkowski, J.: Synergy of modeling processes in the area of soft and hard
modeling. In: Proceedings of the 8th International Conference on Manufacturing Science
and Education (MSE 2017), Trends in New Industrial Revolution, Sibiu, MATEC Web of
Conference, vol. 121, p. 04009 (2017). https://doi.org/10.1051/matecconf/2011721105007
12. Selech, J., Joachimiak-Lechman, K., Klos, Z., et al.: Life cycle thinking in small and
medium enterprises: the results of research on the implementation of life cycle tools in Polish
SMEs—Part 3: LCC-related aspects. Int. J. Life Cycle Assess. 19, 1119 (2014). https://doi.
org/10.1007/s11367-013-0695-9
13. Womack, J.P., Jones, D.T., Roos, D.: The Machine That Changed The World: The Story of
Lean Production. Rawson Associates, New York (1990)
14. Womack, J.P., Jones, D.T.: Lean Thinking: Banish Waste and Create Wealth in Your
Corporation. Simon & Schuster, New York (1996)
15. Liker, J.K.: Becoming Lean: Inside Stories of U.S. Manufacturers. Productivity Press,
Portland (1997)
16. Yasuhiro, M.: Toyota Production System: An Integrated Approach to Just-In-Time, 3rd edn.
Engineering and Management Press, Norcross (1998)
17. Bhamu, J., Sangwan, K.S.: Lean manufacturing: literature review and research issues. Int.
J. Oper. Prod. Manag. 34(7), 876–940 (2014). https://doi.org/10.1108/ijopm-08-2012-0315
18. Aulakh, S.S., Gill, J.S.: Lean manufacturing - a practitioner’s perspective. In: IEEE
International Conference on Industrial Engineering and Engineering Management, pp. 1184–
1188 (2008). http://doi.org/10.1109/IEEM.2008.4738057
Production Flow Improvement in a Textile Industry
233

Marketing Knowledge Management Model
Teresa Guarda1,2,3(&), Maria Fernanda Augusto1,3, Marcelo León1,3,
Hugo Pérez3, Washington Torres1, Walter Orozco1,
and Jacqueline Bacilio1
1 Universidad Estatal Península de Santa Elena – UPSE, La Libertad, Ecuador
tguarda@gmail.com, mfg.augusto@gmail.com,
marceloleon11@hotmail.com, wtorresguin@gmail.com,
worozcoi@hotmail.com, jbacilio@upse.edu.ec
2 Algoritmi Centre, Minho University, Braga, Portugal
3 Universidad de las Fuerzas Armadas-ESPE, Sangolqui, Quito, Ecuador
hlperez@espe.edu.ec
Abstract. Database marketing (DBM) refers to the use of information data-
bases to support marketing activities in order to obtain useful information to
establish and maintain a proﬁtable interaction with the customer. This work
focuses the failures of traditional approaches to the database marketing,
proposing the use of techniques from artiﬁcial intelligence, in the context of
business intelligence in marketing areas. Based in literature review, it’s explored
a vision for the systemic use of methods and techniques of data mining in
projects of DBM, and proposed a conceptual model that combines DBM
activities with appropriate data mining techniques, contributing to efﬁciency and
effectiveness of database marketing projects.
Keywords: Database marketing  Data mining
Business intelligence  Knowledge discovery from databases
1
Introduction
The volume of data grows every day in organizations challenging the storage capacity
in the databases (DB) and obtaining access to these. The growing volume of data has
given rise to a new problem in several areas that collect the data automatically, created
an urgent need of new techniques and tools that can transform the data into meaningful
information and knowledge. This information is valuable for planning, management
and decision making process, which cannot easily be identiﬁed by conventional
databases management systems (DBMS). Then, Data Mining (DM) emerges as
response to this need.
The ability to capture and retain customers for many authors represents a major
challenge in the ﬁeld of marketing in organizations [1–3]. With the advances in
information and communication technology, organizations can obtain and store
transactional and demographic data of customers at reasonable cost [2].
The challenge is how to extract important knowledge from these large databases in
order to gain competitive advantage [4]. In many organizations, the use of databases
stills complex and sometimes not available, not only because DBMS require
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_23

knowledge relevant, but also because the data is not ready to be used outside of the
purposes of these DBMS.
Currently, organizations have a clearer understanding of the importance of cus-
tomer data, and critical decisions of decision models in business intelligence, are being
built on the analysis of such data. The emphasis on the management of customer
relations makes the function of the marketing an ideal area of application that can
greatly beneﬁt from using the tools of Data Mining (DM) for decision support in the
context of Business Intelligence.
When using DBM, organizations can identify valuable customers, predict future
behavior, and make decisions based on knowledge, or by calculating the statistical
model development database queries for marketing. However, this approach is not
structured and there is a need for a uniﬁed view to guide marketers in their search for
relevant knowledge. This includes understanding of customer preferences and behavior
through the analysis of their data. Many researches have been done in this direction,
and DM techniques have been used successfully in several areas such as bankruptcy
prediction [5], fraud detection [6], critical care medicine [7, 21] and engineering,
among other areas. In fact, the old model of design-build-sell is being replaced by the
model sell-build-redesign; the oriented management model for the product was
replaced by customer-oriented. The traditional process of mass marketing is being
challenged by the approach to marketing one-to-one.
Thedeﬁnition ofDBMas astrategy for marketingsupporthas changed signiﬁcantlyin
recent years. The current approach is based on estimated models of response to customer
segmentationforsubmissionofoffers.Thesemodelsaccuratelyestimatetheprobabilityof
a client responding to a speciﬁc supply and can signiﬁcantly increase the response rate of a
product offered. Their use in supporting marketing decisions highlights issues of interest,
such as the management of customer relations, marketing, interactive real-time customer
proﬁles and managing cross-organizational knowledge [8].
Have been published some of the contributions to overcome these constraints,
particularly with regard to data manipulation [9], and aspects of the data quality [10],
among others. There are still many aspects that remain unresolved, such as data
integration and pre-processing in marketing activities.
Most contributions in the ﬁeld of DBM refer to simple methods of use in speciﬁc
cases, such as management of customer relationships [8], activities of cross-selling and
up-selling [11], or analysis the shopping basket [12], or else to a speciﬁc set of tech-
niques to improve speciﬁc outcomes, for example, segmentation, or one-to-one mar-
keting activities [13].
In order to help marketers to make use of knowledge obtained through the approach
of Knowledge Discovery in Databases (KDD) in their marketing activities and improve
their results, we propose a model for systematic and efﬁcient integration of involved
processes.
This paper is organized as follows: after this introductory part, we present a general
description of DBM and relevant issues concerning to KDD process, marketing
activities and objectives of data mining; in the third section, is presented a model that
integrates marketing activities with DM techniques; ﬁnally we draw some conclusions.
Marketing Knowledge Management Model
235

2
Database Marketing and KDD
2.1
Database Marketing
The DBM is the set of processes that allows using the data stored in the internal and
external databases, with the objective of extracting relevant information to support the
decision-making of the marketers in the marketing activities, giving a clear vision of
the needs of the clients, and thus anticipate their desires. In this paper, the DBM is
presented as the use of technology databases to support marketing activities, while the
marketing databases are referred as the database system itself. Coopers & Lybrand
proposed three levels of DBM in order to better organize these concepts [5]:
– Direct marketing: organizations manage lists and conduct performance reviews of
basic promotions;
– Marketing customer relationship: companies apply, adapted a more sophisticated
approach, and technological tools to manage your customer relationships;
– Relationship management customer-focused: customer information guide business
decisions across the enterprise, enabling resellers to talk directly with individual
customers and thereby ensure a relationship of loyalty.
DBM is deﬁned as the creation of a database of customers and prospects that
enables organizations to communicate with each in a personalized way [15]. There
others that consider DBM as a way to use information on consumers for the purpose of
increasing the effectiveness and efﬁciency of marketing activities [13]. Finally, DBM
can be deﬁned as the use of customer information with beneﬁts for both the organi-
zation and the customer [16].
All these deﬁnitions emphasize the technologies of databases to support marketing
activities, and impose the deﬁnition of DBM, a set of processes based on marketing
databases for analysis and data exploration, seeking new knowledge [17].
2.2
Knowledge Discovery in Databases
KDD refers to the process of discovering knowledge from data stored in databases,
culminated with the implementation of DM techniques.
In a systemic approach to knowledge acquisition, can be deﬁned through the
system changes that allow re-do in the future tasks more effectively and efﬁciently. In
view of mathematics, knowledge acquisition can be seen as the perception of data sets
[18]. The acquisition of knowledge becomes feasible to rely on the experience and
being supported by the understanding of the data set.
KDD aims to develop methods and techniques for extracting high-level knowledge
from information stored in databases. It can be deﬁned as the process that allows
identifying patterns and/or models that are potentially useful and understandable.
According to Fayyad et al. (1996), the term KDD was created in 1989 as a ref-
erence to the broad process of ﬁnding knowledge in data. KDD refers to any process of
ﬁnding useful data knowledge, while DM refers to the application of algorithms to
extract models from the data. Until 1995, many authors considered the terms KDD and
DM as synonyms. The KDD process is a set of continuous activities that share the
knowledge discovered from databases [19].
236
T. Guarda et al.

The process of knowledge acquisition is usually composed of the following general
steps: (1) selection; (2) pre-processing; (3) transformation; (4) data mining and (5) in-
terpretation and evaluation.
In the ﬁrst step, selection, once the scope and objectives of the process have been
deﬁned, the data are collected with characteristics that are considered useful. The next
step, pre-processing, is characterized by the cleaning of noise and errors (wrong data
and omissions); and non-relevant data are eliminated. The transformation step is
characterized by the search for the most important characteristics of the data in order to
reduce the number of variables or modify the form of a given variable, making the data
extremely organized. The fourth step, DM, algorithms are applied algorithms to dis-
cover patterns in the data, using the methods and techniques for extraction of data
patterns. Finally, the interpretation and evaluation step, where the patterns identiﬁed by
the system are interpreted as knowledge, which will be visualized to make possible its
interpretation and evaluation, being possible to resume the process in any of the pre-
vious steps for a new iteration.
2.3
Data Mining Objectives
The term Knowledge Discovery in Databases refers the process of discovering
knowledge from data stored in databases, this process culminated in the implementa-
tion of DM techniques.
DM is one of the components with more notoriety of KDD with which is often
confused. The knowledge discovery in databases allows you to transform data into
quality information, allowing making strategic decisions for the best performance of
organizations, facing the growing competition and globalization of the market. DM
refers to the extraction of non-trivial identiﬁcation of patterns in the data valid, new and
potentially useful and understandable from the data in databases [19].
The main difference between the DM and other data analysis tools, it’s how they
explore the relationships between data. While the various analysis tools available you
build hypotheses about speciﬁc relationships, it may corroborate or refute them through
the outputs produced by the tool used, the process of DM is responsible for creating
hypotheses, providing greater speed, range and reliability to the results.
The DM aims to build data models. There are many algorithms available, each with
speciﬁc characteristics. DM main activities are [20]:
– Estimation modeling: These models are built starting from the set of input data
(independent variables) to the output values (dependent variables) which can be
developed in two ways depending on the type of output:
– Classiﬁcation: learning a function that allows you to associate with each data
object a of a ﬁnite set of classes and pre-deﬁned user (example customer proﬁle);
– Regression: learning a function that “maps” each object in a data value con-
tinuous (example value of the transaction).
– Descriptive modeling: discover groups or categories of data objects that share
similarities and helps in the description of data sets provides a space (example
customer segments);
– Dependencies modeling: is a model that describes dependencies or associations
between certain relevant data objects (example content of the order of the market
basket analysis);
Marketing Knowledge Management Model
237

– Deviations modeling: tries to detect the most signiﬁcant deviations from mea-
surements and/or past behavior considered as a reference (example fraud detection).
The selection of data mining activities is directly dependent on the marketing
objectives initially set.
2.4
Data Mining Models
Marketing activities refer to the exchange of products and services, being driven by
marketing goals. There is an important set of questions to which marketing activities
must be capable of responding:
(1) Who do I achieve?
(2) Achieve them with What?
(3) When should I do it?
(4) Which promotional channel should I use?
(5) How should be promoting?
The DBM is a process-oriented for marketing objectives, which will determine the
whole process of gathering information. From here, and adopting the above model, it is
possible to suggest at least one task of DM for each objective. Find the “How” means
using DM techniques to segment the likely responses, repeated users, acquisition tar-
gets, customers with increasing costs, and potential defectors. The question “What”
suggests ﬁnding the key characteristics of customers with more value for the company.
This can be achieved by analyzing the data on products and consumer behavior. A set
of prediction activities are associated to the question “How” (example: How many
clients may come to leave the company). The time activity on marketing activities is
represented by the question “When”, which includes all marketing activities that refer
to temporal tasks (example: when the company must send promotional emails to
customers). The question “What” is one of the key word most used in the deﬁnition of
marketing activities, due to the selection of the characteristics associated (example
based on the analysis of the market basket, marketers want to know which products are
associated). By their nature all marketing questions include some prediction of its
results, so it is possible to assign each a DM prediction. The descriptive models of DM
are better to answer the questions “Who” and “How”, not only for its characteristics of
classiﬁcation but also by the type of desired results. The dependency analysis models
have a wide application in marketing activities and may be included in these objectives
“When”, “Who” and “What”. Finally, the model variance analysis can be used to
answer questions of marketing “How”, “When” and “Who”.
3
Marketing Knowledge Management Model
Data mining may be useful in addressing the questions of marketing “Who”, “What”
and “When”. On the other hand, DM is not enough by itself, requiring a set of related
activities to ensure quality results.
238
T. Guarda et al.

An approach to the development of DBM must adopt a certain set of steps to follow
and requirements to fulﬁll. We presents a proposed framework for exploration of the
concepts and features of the KDD process and its intersection with marketing activities
and questions related to the integration of data mining models.
The database marketing model (see Fig. 1) has three phases: Information Gathering,
Knowledge Discovery, Evaluation and Implementation. Initially, the data are collected
from various sources, external sources, internal and market research. After data
Fig. 1. Marketing knowledge management model.
Marketing Knowledge Management Model
239

registration and analysis the marketing database is created in order to support the whole
process of KD.
KDD process is part of the proposed second phase, and includes a set of steps that
allow the data to extract new knowledge from databases of marketing:
– Selection of data: consists in selecting a subset of data on which the algorithms used
in modeling will work;
– Preprocessing: and data transformation, the selected data is processed appropriately
for the extraction process of knowledge [2];
– Modeling: crucial step in which various techniques are applied to discover poten-
tially useful patterns. These techniques are used to achieve the initial objectives
such as the segmentation and classiﬁcation.
In the third phase, the evaluation and implementation, refers to the integration of
the knowledge obtained from the KDD process models in marketing database. Since
the answers to marketing issues supported by these models.
4
Conclusions
In this work, we used a KDD approach to DBM projects, looking for the systemati-
zation of the whole process, in order to facilitate its use in support of marketing
activities. In today’s customer-centric business environment, it is our belief that there is
need for a deep understanding of the use of DM and KDD as a decision support for
marketing.
To this end, we showed how the DM can be integrated into the model of knowledge
management marketing. The availability of large volumes of data, enabled by modern
information technology, one of the main problems is to ﬁlter, sort, and process, analyze
and manage these data, in order to extract the relevant information to the user. The
growth in size and number of existing databases is much superior to the human capacity
to analyze the data using traditional tools, which creates the need and opportunity for use
of DM tools. With the shift from mass marketing to relationship marketing one-to-one,
one area that could beneﬁt greatly from DM is its own marketing function.
The systematic application of data mining techniques enriches the process of
knowledge management and it provides marketers a better understanding of their
customers, allowing provide them a better service. For us, it is also clear that Web
technology will have a major impact on the practice of data mining and knowledge
management, and will present interesting challenges for future research in information
systems.
References
1. Lusch, R., Vargo, S.: The Service-Dominant Logic of Marketing: Dialog, Debate, and
Directions. Routledge, Abingdon (2014)
2. Khodakarami, F., Chan, Y.: Exploring the role of customer relationship management
(CRM) systems in customer knowledge creation. Inf. Manag. 51(1), 27–42 (2014)
240
T. Guarda et al.

3. Wedel, M., Kannan, P.: Marketing analytics for data-rich environments. J. Mark. 80(6), 97–
121 (2016)
4. Kasemsap, K.: The role of data mining for business intelligence in knowledge management.
In: Integration of Data Mining in Business Intelligence Systems, pp. 12–33 (2015)
5. Kingyens, A., Paradi, J., Tam, F.: Bankruptcy prediction of companies in the retail-apparel
industry using data envelopment analysis. In: Advances in Efﬁciency and Productivity,
pp. 299–329. Springer International Publishing (2016)
6. Wheeler, R., Aitken, S.: Multiple algorithms for fraud detection. Knowl.-Based Syst. 13(2),
93–99 (2004)
7. Silva, A., Cortez, P., Santos, M., Gomes, L., Neves, J.: Multiple organ failure diagnosis
using adverse events and neural networks. In: Proceedings of 6th International Conference
on Enterprise Information Systems – ICEIS, vol. 2 (2004)
8. Shaw, M., Subramaniam, C., Tan, G., Welge, M.: Knowledge management and data mining
for marketing. Decis. Support Syst. 31, 127–137 (2001)
9. Guarda, T., Augusto, M.F., L., Sousa, A., Silva, C., Costa, A.: Database Marketing Tools for
SMEs (2014)
10. Lee, J.: A study on the data mining preprocessing tool for efﬁcient database marketing.
J. Digital Convergence 12(11), 257–264 (2014)
11. Oliveira, T., Coelho, V., Souza, M., Boava, D., Boava, F., Coelho, I., Coelho, B.: A hybrid
variable neighborhood search algorithm for targeted offers in direct marketing. Electron.
Notes Discrete Math. 47, 205–212 (2015)
12. Aguinis, H., Forcum, L., Joo, H.: Using market basket analysis in management research.
J. Manag. 39(7), 1799–1824 (2013)
13. McDonald, M., Wilson, H.: Marketing Plans: How to Prepare Them, How to Proﬁt from
Them. Wiley, Chichester (2016)
14. Consulting, C.: Database Marketing Standards for the Retail Industry. Retail Target
Marketing System Inc (1996)
15. Babin, B., Zikmund, W.: Exploring Marketing Research. Cengage Learning, Boston (2015)
16. Rodriguez, M., Peterson, R.M., Ajjan, H.: CRM/social media technology: impact on
customer orientation process and organizational sales performance. In: Ideas in Marketing:
Finding the New and Polishing the Old. Springer, Cham (2015)
17. Pinto, F., Guarda, T., Gago, P.: A framework proposal for ontologies usage in marketing
databases. In: International Conference on Model and Data Engineering, pp. 31–41. Springer
(2011)
18. Mining, W.: Data Mining: Concepts and Techniques. Morgan Kaufmann, Burlington (2006)
19. Piatetsky-Shapiro, G. (ed.): Advances in Knowledge Discovery and Data Mining
Uthurusamy, 21st edn. AAAI Press, Menlo Park (1996)
20. Elder, J.: Handbook of Statistical Analysis and Data Mining Applications. Academic Press,
Cambridge (2009)
21. Gonçalves, J., Faria, B.M., Reis, L.P., Carvalho, V., Rocha, Á.: Data mining and electronic
devices applied to quality of life related to health data. In: 2015 10th Iberian Conference on
Information Systems and Technologies (CISTI), pp. 1–4. IEEE (2015)
Marketing Knowledge Management Model
241

Participative Sensing in Noise Mapping:
An Environmental Management System Model
for the Province of Santa Elena
Teresa Guarda1,2,3(&), Marcelo León1, Maria Fernanda Augusto1,
Hugo Pérez3, Johnny Chavarria1, Walter Orozco1, and Jaime Orozco1
1 Universidad Estatal Península de Santa Elena – UPSE, La Libertad, Ecuador
tguarda@gmail.com, marceloleon11@hotmail.com,
mfg.augusto@gmail.com,
johnny_chavarria@consultant.com,
worozcoi@hotmail.com, jorozco@upse.edu.ec
2 Algoritmi Centre, Minho University, Braga, Portugal
3 Universidad de las Fuerzas Armadas-ESPE, Sangolqui, Quito, Ecuador
hlperez@espe.edu.ec
Abstract. Noise pollution is a common problem in urban environments.
Although the authorities in some cities have launched campaigns to monitor the
problem, the resulting maps are not always easily accessible and are generally
not detailed enough to understand the temporal and spatial variations in noise to
which people are exposed. In this context, information technologies and systems
are an added value in improving the mapping of noise pollution. The objective
of this article is solve a gap in the province of Santa Elena, the absence of noise
map in urban areas, and consequently the lack of knowledge of the impact of
noise pollution on the population. This paper presents a model for participative
sensing in noise pollution mapping system using a smartphone, allowing citi-
zens to contribute to the collection of noise data, thus participating in a more
sustainable environmental management.
Keywords: Noise pollution  Environment mapping
Participative sensing networks
1
Introduction
Nowadays, we are living the effects of the environmental impact caused by the tech-
nological advance, which originates contaminating sources that affect health, and
personal performance. In this context, it is urgent to draw up the noise map of the urban
areas of the province of Santa Elena.
According to the Ecuadorian Ministry of the Environment (MAE), “Noise is one of
the main sources of annoyance for the population and the environment, causing health
problems and altering the natural conditions of ecosystems. This type of pollution
occurs on a larger scale in large urban conglomerates, often leading to physiological
and psychological injuries that are often detected when the damage is irreversible” [1].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_24

In Europe, despite Directive 2002/49/EC on the assessment and management of
environmental noise, which is a key legislative instrument for protecting citizens from
excessive noise pollution, noise pollution remains a major environmental health
problem. It has been scientiﬁcally proven that prolonged exposure to high levels of
noise pollution can have serious health effects in areas controlled by the brain and the
human endocrine system, including cardiovascular disease, sleep disorders and dis-
comfort (a feeling of discomfort that affects the well-being general). According to the
World Health Organization (WHO), in Europe, among environmental causes, noise
pollution generates a burden of morbidity only outweighed by air pollution [2].
The noise from road trafﬁc currently represents one of the major sources of envi-
ronmental pollution. Several studies have already been done on the effects of noise
from different types of trafﬁc in the population. There are guidelines that seek to control
and reduce sound pressure levels to acceptable standards in order to protect the health
of urban residents. Culturally, throughout human evolution, the noise aspect is not
considered when implementing a new road or an urban center.
The present work has as main objective to propose an Environmental Management
System (EMS) model for collecting noise data on a large scale with active citizen
participation, thus facilitating the transfer of knowledge at the level of environmental
management, and promoting public awareness of the adverse effects of exposure to
excessive noise levels.
With the participation of citizens using their mobile phones as noise sensors, a low
cost solution is created for the public administration, allowing the rooting of their daily
environment, participating in the creation of collective noise maps through the sharing
of geographic and noise data.
This article is structured in four sections. After this introductory part several con-
tents related to the central theme of the study are presented, namely: noise and health
effects, participative sensing, and Smartphones as mobile sensors. In the third section,
it’s proposed an Environmental Management System (EMS) model for the province of
Santa Elena. Finally, in the last section we draw some conclusions about the impor-
tance of using EMS model.
2
Background
2.1
Noise
By deﬁnition, the noise is any unwanted or unpleasant sound to receiver. Both sound
and noise are a vibratory phenomenon that is propagated through a medium and
detectable by the ear, its subjective distinction depending on the individual’s percep-
tion. Noise is a sound that induces physiological/psychological damage [2].
Sound can be deﬁned as a disturbance propagating in the form of a longitudinal
wave, by means of an elastic medium which may be gas, liquid or solid. The propa-
gation speed is dependent on the characteristics of the medium. This disturbance is the
result of a vibratory movement generated by a vibrating surface, or by the turbulent
ﬂow of a ﬂuid. The movement produces small pressure changes in the surrounding
environment that provoke the stimulation of the auditory system, resulting in the
Participative Sensing in Noise Mapping
243

perception of sound by the ear. A sound is classiﬁed as audible when within the normal
audible frequency range of 20 Hz to 20 kHz [3].
A sound wave consists of a pure tone, it is a sinusoidal wave composed of a single
frequency and can be characterized by various physical quantities, namely amplitude,
wavelength, frequency, period and propagation speed [4]. In turn, the amplitude of the
pressure changes corresponds to the difference between the maximum and minimum
values, around the atmospheric pressure. This pressure ﬂuctuation is called sound
pressure and is expressed in Pascal (Pa). The amplitude of the pressure ﬂuctuations can
be described by the maximum amplitude pressure (pM) or the square-mean-root
(RMS) of amplitude (prms). The number of cycles of pressure variation in the medium
per unit of time (seconds) corresponds to the frequency (f) of a sound wave and is
expressed in Hertz (Hz). The decibel scale (dB), in which noises are measured, is a
logarithmic scale. The noise intensity increases twice as the sound level rises. The
intensity, coupled with the duration of exposure, is what determines the risk of effects
on living beings.
There are several instruments that can be used for noise measurements, for the
measurement of sound physical properties. The instrument chosen will depend on the
purpose of the measurement, the characteristics of the noise in question and the type of
analysis that is required. The main instruments are the soundmeter/noisemeter and the
dosimeter, the ﬁrst one being the most used [5]. The purpose of the dosimeter is to
measure personal exposure to noise, and is essentially used to assess the risk of hearing
damage as a result of exposure of individuals to high-intensity noise [6].
Measuring instruments are subject to various standards, such as ANSI S1.4 (issued
by the American National Standards Institute (ANSI)) and IEC 61672 (issued by the
International Electrotechnical Commission (IEC)).
The standards divide the soundmeters into categories according to their precision:
type 1, high precision instruments; type 2, general purpose instruments; type 3,
instruments with the sole purpose of being used in studies with lower precision
requirements. Being the category recommended by the ofﬁcial entities is the type 2 [7].
Environmental noise is considered to be the noise produced by various human
activities, such as transport, industry, among others, and to which the population is
exposed (Directive 2002/49/EC). Transport media are the main source of ambient noise,
especially in urban environments. In the European Union, approximately 40% of the
population is exposed to road trafﬁc noise with an equivalent noise level above 55 dBA
in the daytime period and 20% of the population is exposed to levels above 65 dBA [2].
2.2
Noise Health Effects
Health problems caused by excessive noise are only perceived after a long period of
exposure and when damage to the body is already serious. Environmental noise causes
various health ailments such as cardiovascular, hormonal and stress problems. It also
causes communication difﬁculties, concentration and detrimental moments of rest
(interference in sleep), extremely important for the physical and mental restoration of
the body.
Noise is an inherent phenomenon in a civilized world, its impact being greatest in
urban environments. Its effect on the population is negative insofar as it contributes to
244
T. Guarda et al.

the deterioration of the quality of life of the individuals and can lead to several health
problems in the short and long term.
Environmental noise has already been recognized as a serious public health
problem by the WHO. For this reason, has established noise-related guidelines based
on their health effects, which translate into sound levels above which health and/or
human well-being impacts are expected to occur, recommending that [8]:
– within the bedroom the individual should be exposed to an LAeq level of less than
30 dBA at night so that a good quality sleep can be provided;
– within classrooms an LAeq of less than 35 dBA, so that good teaching and learning
conditions are provided;
– in outdoor housing areas, an LAeq not greater than 55 dBA during daytime and
nighttime periods so as not to be uncomfortable.
The International Program on Chemical Safety presented a deﬁnition of the harmful
effect of noise. This effect translates into a change in the morphology and physiology of
an organism resulting in deterioration of functional capacity, impairment of ability to
compensate for additional stress or increased susceptibility of the organism to adverse
effects from other environmental factors. The effects of noise on health are thus trig-
gered by physiological mechanisms [8].
The effects of ambient noise on health can be classiﬁed into two categories, hearing
effects and non-hearing effects. The ﬁrst category concerns the effects of exposure to
noise in the human auditory system. Non-auditory effects correspond to all effects on
health and well-being, caused by exposure to noise, except for effects on the organ
responsible for hearing and effects caused by the masking of auditory information.
Concerning the hearing effects, one of the main consequences of prolonged expo-
sure to noise in the auditory system is hearing loss, which results from damage to the ear
because it is sensitive to loud noises. This sensitivity particularly occurs at the level of
the outer hair cells, which may be damaged or even destroyed before the inner hair cells
suffer comparable damage [2]. Regarding the non-hearing effects, excessive noise
exposure has other negative effects throughout human body with effects to the physical,
mental and emotional levels. Other effects include disturbances in sleep, stress, car-
diovascular problems, ringing in the ears and reduction of professional performance,
leading to problems of concentration, difﬁculties in communication and nerves. By
disrupting communication and carrying out activities, it creates discomfort which in turn
can lead to stress responses and subsequently symptoms and illness. However, noise
may also have a direct negative effect on health, which is dependent on some acoustic
characteristics of noise, such as its intensity, frequency, complexity and duration [9].
2.3
Participative Sensing
Participative sensing is a person-centered participatory technique, enabling the col-
lection of environmental data with high granularity in space and time [10].
Environmental monitoring with the inclusion of the communities (citizens) in the
process, besides being a form of participation in the management of natural resources,
allows for a more sustainable management, particularly at the level of monitoring.
Involving citizens is a cost-effective way to obtain environmental information [11].
Participative Sensing in Noise Mapping
245

Participative sensing application to be successful must have a good balance
between the elements: technology, data and people. In the case of environmental
monitoring, the data has a particular importance. Globally, there is an increasing
number of activists and citizens who adhere to participatory technologies to face local
pollution problems [12].
2.4
Smartphones as Mobile Sensors
Smartphones have the characteristics of personal computers, having evolved to become
devices with computing, sensing and communication capabilities.
A smartphone is thus deﬁned as a mobile phone that includes software that can be
modiﬁed and updated by the user, allowing the sharing of information with other
external systems through mobile networks. Mobile phones are currently the main
means of communication and mobile computing, containing several built-in sensors
(WiFi, camera, microphone, GPS, accelerometer, among others), which allows its
application in sensing tasks, They have great potential as sensors, allowing the col-
lection, processing and dissemination of data on a large scale [13].
Mobile sensing provides some advantages compared to static wireless sensor net-
works, particularly in urban areas. Mobile sensing represents a basically null imple-
mentation cost, and the great mobility associated with the users of these, allows a large
spatiotemporal coverage in certain areas, which would have high costs in the case of a
network of static sensors.
3
Participative Sensing in Noise Mapping
The MAE has issued the regulations on ‘Permissible limits of ambient noise levels for
ﬁxed sources, mobile sources, and vibrations’ and the ‘Airport Noise Standard’, which
is presented in Annex 5 and Annex 9 of Book VI of the Text Uniﬁed Ministry of
Environment (TULSMA) [14], which establish the maximum permissible noise limits
for ﬁxed sources (buildings, industries, airport facilities and others), and mobile sources
such as motor vehicles. According to Book VI of the Uniﬁed Text of Secondary Leg-
islation of the Ministry of the Environment [14], “the levels of sound pressure equiv-
alent, NPSeq, expressed in decibels, in weight with scale A, obtained from the emission
of a ﬁxed source emitting noise, cannot exceed the values that are set in Table 1.
Table 1. Maximum permissible Noise Levels according to Land Use.
Equivalent
sound pressure
level NPS eq
[db(A)]
Type of area according to land use
Hospital and
Educational
area
Residential
area
Mixed
residential
zone
Shopping
area
Mixed
commercial
zone
Industrial
zone
From 06h00 to
20h00
45
50
55
60
65
70
From 20h00 to
06h00
35
40
45
50
55
65
246
T. Guarda et al.

Although extremely strict measures are applied by the European Community to
protect the health of the population and to guarantee their quality of life in Europe, it is
estimated that about 80 million people are affected with noise levels above 65 dB (A) and
170 million between 55 and 65 dB (A) which is the level where people start to feel more
disturbed and that noise from urban trafﬁc exposes the majority of the population (about
90%) to levels of sound pressure above 65 dB (A).
The diagnosis of acoustic pollution in the province of Santa Elena is fundamental to
control some pathologies caused by excessive noise, as is the case of acoustic trauma,
which consists of the deterioration of hearing in the acute tones. Noises that exceed
90 dB kill the cells of the ear.
The target populations of this study are the inhabitants of the urban areas of the
cantons of La Libertad, Salinas and Santa Elena.
3.1
Description of the Current Situation
At the moment there is no information to know the reality of environmental noise in
urban areas of the province of Santa Elena. In this context, it is urgent to draw up the
noise map of the urban area.
Its intended incorporate the EMS model that starts with the initiatives of the ter-
ritories, in accordance with the Territorial Planning Plan (PDOT) for proper manage-
ment of the environment, decrease in some cases and alternatives of local development
much more balanced with the rights of nature inscribed in the constitution of Ecuador.
For the administration of the proposal it is considered the province of Santa Elena,
urban zone; it involves three municipalities in question. Determining by micro regional
zones of the territories, or “Urban Parishes of Santa Elena Province” should be linked
to the decentralized autonomous governments (GAD parochial) of Salinas: Carlos
Espinoza Larrea, General Alberto Enriquez Gallo, Vicente Rocafuerte, Santa Pink; La
Libertad: unique urban parish; Santa Elena: Ballenita and Santa Elena.
The province of Santa Elena is divided into three cantons: La Libertad, Salinas and
Santa Elena. Based on the statistics of population census (2010) carried out by the INEC
[15], in the province of Santa Elena there are 308,693 inhabitants, of which 51% are men
and 49% are women: La Libertad with 31% of the population of the province (31% men
and 32% women); Salinas with 22% of the population of province (23% men and 22%
women); Santa Elena with 47% of the population (47% men and 47% women).
The reference population of this study is the entire population of the three
municipalities of the province of Santa Elena. According to the projections of the INEC
[16], the reference population has the following demographic distribution for the year
2017: 51% of the population are male, and the remaining 49% are female; of which
33.1% with 0–14 years, 60.9% with 15–64 years and 6.0% with 65 or more.
3.2
Environmental Management System Model
The EMS system consists of the following elements: a mobile application (NoiseTube),
a page on a social network (Facebook) and a Web application. The mobile application
is associated with a smartphone, to perform measurements of noise levels; the second
Participative Sensing in Noise Mapping
247

component aims to promote the discussion around the theme, and recruit volunteers;
ﬁnally, the Web application, based on Google Maps, aims to visualize the noise levels
map.
NoiseTube it’s a mobile participatory sensing application that aims to make a
simple smartphone a low cost portable soundmeter. It is embedded in an ambient noise
evaluation system called NoiseTube, which includes a community-based web memory,
as well as the mobile application. The application is currently available for three mobile
operating systems, the Java ME, Android and Apple iOS platforms. It can be down-
loaded through the NoiseTube Website or the mobile app stores associated with each
mobile platform. Your use does not imply a user registration on the Website, this
operation is necessary only for the submission of data in the associated community
memory. The goal of the NoiseTube project is to enable citizens to contribute to the
measurement of noise pollution. Under normal conditions, measurements are made by
specialized people, at speciﬁc sites, leading to the following problems: the fact that the
measurements are carried out in speciﬁc sites, sometimes very distant, leads to the
production of sometimes ‘wrong’ pollution maps [17].
The objective is to use mobile equipment that is present in the daily life of a large
part of the population, in the task of measuring noise levels, preferably associated with
ambient noise. It is intended that the citizen perform measurements in the most varied
activities that constitute his daily routine, such as during his trip to work. Thus, no
additional effort is imposed on the individual, requiring measurements to be made in
speciﬁc environments.
The noise mapping process aims to assess noise exposure resulting from the
presence of noise sources such as cities, highways, railways and industrial units in a
particular area. In this way the resulting map spatially highlights the spatial variation of
ambient noise levels (Fig. 1) [18, 19].
In EMS model (Fig. 2), citizens or participants with Apps NoiseTube Mobile
installed on the system, voluntarily contribute to the collection of environmental data
(noise level) through their smartphones. NoiseTube will automatically georeference the
Fig. 1. Noise map (an example). The noise level in decibels:
80+;
75–80;
70–75;
65–70;
60–65;
55–60;
50–55;
45–50;
40–45;
35–40;
0–35.
248
T. Guarda et al.

noise level measurements, allowing associate the noise level with the geographic
coordinates. The information is obtained through the GPS receiver incorporated in the
equipment, being transmitted together with the measurement data. The Apps provides a
mapping interface, which shows noise levels in the form of a map on Google Maps.
The data is then stored, and the participant can choose to use one of the following
conﬁguration options in Apps:
(1) Submit the data directly to the NoiseTube Mobile community memory, needing to
be registered on the associated website and have an internet connection, so that it
is feasible to send data in real time;
(2) Store data on mobile equipment;
(3) Do not store any measurement log.
The sending process ensures transmission of the collected data to the application
server through the communication infrastructures available on the participant’s
smartphone.
In the page associated to the monitoring system, the incorporation of a web
mapping application based on Google Maps is required to view the measured noise
levels. This component is a gadget associated to Google Maps, it is a gadget that allows
view maps created through the search and viewing service of maps and satellite images
of the planet Earth, Google Maps.
NoiseTube Mobile stores data from measurements in an XML extension. Using a
spreadsheet application, the ﬁle is converted to “txt” format with text, separated by tab;
Google Fusion Tables can be used to map tabulated data from Google Maps and also
store, view and share them; in this case, the publication of the created map, using an
HTML code that can be incorporated into a Website associated with the monitoring
system. Any changes that are made to the Fusion Tables database will be automatically
updated on the Website.
Fig. 2. EMS model.
Participative Sensing in Noise Mapping
249

The system requires members of the municipality, or other elements, to be
responsible for the implementation, management and maintenance of the entire system,
which means that a system administrator is required.
Users of the system can consult the data collected from the system, according to
their objectives. We can include in the users group the participants, the system
administrator and the general public. Participants may have an interest in querying the
collected data; in turn the system administrators can check and evaluate the contri-
butions made.
4
Conclusions
To solve a gap in the province of Santa Elena, the absence of noise map in urban areas,
and consequently the lack of knowledge of the impact of noise pollution on the pop-
ulation, its intended incorporate the EMS model that starts with the initiatives of the
territories, in accordance with the Territorial Planning Plan (PDOT) for proper man-
agement of the environment. The active participation of citizens and the existence of
monitoring data on environmental parameters are two of the essential aspects of a good
environmental management. Then, it’s essential that the municipalities of Salinas, La
Libertad and Santa Elena deﬁne a strategy to raise citizens’ awareness of the envi-
ronment and to recruit participants to the collection of environmental data.
Attracting and retaining a sufﬁcient number of users is a key factor in the success of
any mobile sensing system. This condition assumes greater importance in participatory
sensing.
To obtain results as accurately as possible, it is important that the application
NoiseTube be calibrated against a soundmeter. Ideally this calibration should be per-
formed individually for each model of smartphone, since different devices have
microphones with different properties. However such a scenario is not feasible, so
ofﬁcially the application is calibrated for a limited number of smartphone models.
We can conclude that the implementation of the monitoring system would be
feasible because high percentages were obtained in the possession of smartphones and
the frequent use of mobile applications. Together with the receptivity of citizens to
participate in the monitoring tasks, there are conditions for a successful implementation
of the described system in relation to the number of potential users.
References
1. Ministerio del Ambiente. Ecuador le dice “NO AL RUIDO” http://www.ambiente.gob.ec/
hoy-ecuador-le-dice-no-al-ruido/. Accessed 2017
2. Heutschi, K.: Lecture Notes on Acoustics I. Swiss Federal Institute of Technology, ETH,
Institute for Signal- and Informationprocessing, ISI, Zurich (2013)
3. Goelzer, B., Hansen, C., Sehrndt, G.: Occupational exposure to noise: evaluation, prevention
and control. World Health Organisation (2001)
4. Bies, D., Hansen, C.: Engineering Noise Control: Theory and Practice. CRC Press, Boca
Raton (2009)
250
T. Guarda et al.

5. Fahy, F.: Sound and Structural Vibration: Radiation, Transmission and Response. Academic
Press, London (2012)
6. Clark, W., Saunders, S.: Assessment of noise exposures for pre-term infants during air
transport to neonatal intensive care units using iPhone sound meter apps. J. Acoust. Soc.
Am. 139(4), 2036 (2016)
7. WHO: Guidelines for Community Noise. World Health Organization, Suiça (1999)
8. Hays, J., McCawley, M., Shonkoff, S.: Public health implications of environmental noise
associated with unconventional oil and gas development. Sci. Total Environ. 580, 448–456
(2017)
9. Oldenburg, J., Griskewicz, M.P.: Participatory Healthcare: A Person-Centered Approach to
Healthcare Transformation. CRC Press, Boca Raton (2016)
10. Koontz, T., Thomas, C.: What do we know and need to know about the environmental
outcomes of collaborative management? Public Adm. Rev. 66(s1), 111–121 (2006)
11. D’Hondt, E., Stevens, M., Jacobs, A.: Participatory noise mapping works! An evaluation of
participatory sensing as an alternative to standard techniques for environmental monitoring.
Pervasive Mob. Comput. 9(5), 681–694 (2013)
12. Khan, W., Xiang, Y., Aalsalem, M., Arshad, Q.: Mobile phone sensing systems: a survey.
IEEE Commun. Surv. Tutor. 15(1), 402–427 (2013)
13. Ministerio de Ambiente: TULSMA, Legislación Secundaria, T.U. del Ministerio del
Ambiente. Libro VI, Ministerio de Ambiente, Quito, Ecuador (2015)
14. INEC: Resultados del Censo 2010 de Población y vivienda en el Ecuador. Instituto Nacional
de Estadistica y Censos (2010)
15. INEC: Compendio Estadistico 2014. INEC (2014)
16. Drosatos,
G.,
Efraimidis,
P.,
Athanasiadis,
I.,
D’Hondt,
E.,
Stevens,
M.:
A
privacy-preserving cloud computing system for creating participatory noise maps. In: IEEE
Computer Software and Applications Conference (COMPSAC), vol. 3 (2012)
17. Rey Gozalo, G., Barrigón Morillas, J.: Analysis of sampling methodologies for noise
pollution assessment and the impact on the population. Int. J. Environ. Res. Public Health 13
(5), 490 (2016)
18. EPA: Guidance Note for Strategic Noise Mapping. Environmental Protection Agency,
Ireland (2011)
19. WHO: Burden of disease from environmental noise. Quantiﬁcation of healthy life years lost
in Europe. World Health Organization (2011)
Participative Sensing in Noise Mapping
251

Using Experimental Material Management
Tools in Experimental Replication:
A Systematic Mapping Study
Edison Espinosa1(&), Juan M. Ferreira2, and Henry Chanatasig3
1 Universidad de las Fuerzas Armadas, ESPE, Quito, Ecuador
egespinosa1@espe.edu.ec
2 Universidad Nacional de Asunción, FPUNA, Asunción, Paraguay
jmferreira1978@gmail.com
3 Universidad Técnica de Cotopaxi, Cotopaxi, Ecuador
henry.chanatasig@utc.edu.ec
Abstract. Context: In experimental software engineering (ESE), experimental
replication is applied to validate results of an experiment. Much information and
one version of the experimental materials are required for replication. Prior to
the replication execution, all or part of the materials may require changes,
producing new or modiﬁed versions of these, which should be incorporated into
the material of the original experiment. There is a direct relationship between the
increase in the number of replications with the increase of versions of the
experimental material, which commonly causes confusion and disorder for
experimental administrators. Objective: The aim of this paper is to conduct a
mapping study to locate articles about the use of experimental material man-
agement tools in experimental replication in ESE. Method: We applied the
mapping study to search, analyze and select published papers from reported
replications. Results: We analyzed a total of 592 articles published from 1998 to
2014, 24 of them have been pre-selected and 4 have ﬁnally been selected.
Conclusion: The results show the limited existence of articles on this subject. In
addition, our analysis has allowed to identify that most of them suffer from
problems in versions management for both replication and experimental mate-
rial. These data provide information of interest to start a research about adoption
of the paradigm of software conﬁguration management inside the management
of the experimental material in ESE.
Keywords: Experimental software engineering  Experimental replication
Experimental material  Experimental material management
1
Introduction
In ESE, replication is used to validate the empirical ﬁndings published by researchers
[1, 2]. Lots of information and materials about experiment is required to perform a
replication [2]. Before the execution of the experimental replication, all or part of the
materials may require changes producing new or modiﬁed versions of these. After the
execution of the experimental replication, all or part of the materials is expected to be
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_25

incorporated in the material of the original experiment. The increase of the number of
replications of the original experiment is directly related to the increase of the versions
of the experimental material. This commonly generates confusion and disorder into the
management where having multiple versions of the experimental materials and repli-
cations of an experiment becomes a complex problem [3]. The experimental material
management is an open research in ESE and has a remarkable importance as several
authors have shown [4–6]. The literature allows to identify the evolution of the
instruments used in the transmission of information to perform an experimental
replication. Firstly, the reports of experiments have just been used. Replication pack-
ages took in place later on and now we have supportive environments for experi-
mentation including the use of repositories of experimental material and tools for
activities support of the experimental process [7].
The paper is structured as follows. In Sect. 2, the methodology, the research
questions, the process of planning and execution are described. The results and relevant
aspects of selected articles are discussed in Sects. 3 and 4. Threats to validity are
discussed in Sect. 5. Finally, in Sect. 6, the ﬁndings and future works are given.
2
Method
The aim of this paper is to conduct a mapping study to locate articles where the use of
material management tools for supporting the experimental process is evidenced. So,
we ask these research questions:
• RQ1: What do web applications manage experimental materials using infrastruc-
ture, repositories and packages?
• RQ2: What do web applications provide support for the experimental replication
process?
We focus the scope of this research about articles related to the use of experimental
material management tools in replication of experiments until July 2014.
The question RQ1 seeks to locate articles that support the experimental material
management in infrastructure, repositories and packages, for which it is necessary to
get evidence of the features in the tools to conduct activities, storage, retrieval, mon-
itoring, and trace of the experimental materials. Meanwhile, the RQ2 seeks to identify
features that allow: (1) to obtain a speciﬁc version of materials together with the
protocol that allows the execution of the replication, (2) to incorporate new versions of
replications and materials into the existing and, (3) to identify experimental materials
required to perform a replication.
2.1
Planning
We follow Petersen recommendations [8] to carry out a systematic mapping study. The
steps are:
• Selection of article sources
• Deﬁnition of the searching string
Using Experimental Material Management Tools
253

• Deﬁnition of the inclusion and exclusion criteria
• Pre-selection of articles
• Selection of articles
The selection of article sources is based on the following rules: (1) criteria deﬁ-
nition for sources selection to locate articles that come from reliable sources such as
international conferences, journals, book chapters and articles available on web (2) the
data sources are Scopus and IEEE.
As proposed by Beecham [9], the searching string is built from the research
question posed with the PICOC strategy. Searching strings are structured by: experi-
mental material, replication packages, infrastructure, repository, support and materials
management. More detail of the searching strings are shown in Table 1.
The research questions are used to deﬁne the inclusion and exclusion criteria. In the
activity of pre-selection of articles, the inclusion criteria allows to include articles in
which the use of terms in the searching string is evidenced in sections: title, abstract
and keywords. In addition, exclusion criteria rule out the articles in which the terms of
the searching string are used but they do not describe the content of the use of
experimental material management tools. For the cases where the abstract does not
provide the necessary information to determine their pre-selection we will proceed to
read the introduction section.
In the selection activity, a full reading of the article is done looking for evidence
inside their content with information about web applications that manage replications
and experimental materials.
2.2
Execution
The total number of analyzed articles was 592, 428 from Scopus and 164 from IEEE, of
which 20 were pre-selected from SCOPUS and 4 from IEEE by ﬁltering repeated
articles.
The total of articles found in digital libraries in pre-selection and selection activities
are also shown in Table 2. Note that 6 common articles were found during
pre-selection, while just one common article was found in two libraries during
selection.
254
E. Espinosa et al.

Table 1. Deﬁnition of searching substrings
String name
Context
Deﬁnition
SRCHMaterial
Experimental
Material
Software Engineering AND Experiment OR
Empirical OR Empirical Study OR Empirical
Evaluation OR Experimentation OR
Experimental Comparison OR Experimental
Analysis OR Experimental Evidence OR
Experimental Setting OR Empirical
Data AND Experimental Materials
Replication OR Experimental Materials OR
Experimental Artifacts OR Experimental
Object OR Experiment Materials
SRCHPackage
Replication
Packages
Software Engineering AND Experiment OR
Empirical OR Empirical Study OR Empirical
Evaluation OR Experimentation OR
Experimental Comparison OR Experimental
Analysis OR Experimental Evidence OR
Experimental Setting OR Empirical
Data AND Experimental Package OR Lab
Package OR Laboratory Package OR
Replication Package
SRCHInfrastructure
Infrastructure
Software Engineering AND Experiment OR
Empirical OR Empirical Study OR Empirical
Evaluation OR Experimentation OR
Experimental Comparison OR Experimental
Analysis OR Experimental Evidence OR
Experimental Setting OR Empirical
Data AND Infrastructure OR Infrastructure
Replication
SRCHSupport
Replication Process
Support
Software Engineering AND Experiment OR
Empirical OR Empirical Study OR Empirical
Evaluation OR Experimentation OR
Experimental Comparison OR Experimental
Analysis OR Experimental Evidence OR
Experimental Setting OR Empirical
Data AND Replication Support
SRCHManagement
Applications that
Manage Materials
and Replications
Software Engineering AND Experiment OR
Empirical OR Empirical Study OR Empirical
Evaluation OR Experimentation OR
Experimental Comparison OR Experimental
Analysis OR Experimental Evidence OR
Experimental Setting OR Empirical
Data AND Replication Management OR
Experimental Management OR Experimental
Material Management OR Experimental
Material Sharing
Using Experimental Material Management Tools
255

Finally, after the selection process, 20 articles were excluded from the 24
pre-selected because the criteria was not met. Generally, excluded studies do not show
any URL where the use of experimental material management tools in replications is
evidenced. The included and excluded articles from mapping are shown in Table 3.
Table 2. Pre-selected articles and precision with searching strings
Engine
String
Articles
matching
Pre-selected
articles
Repeated
articles
Precision
Scopus
SRCHMaterial
7
2 [JG12],
[MCDdO06]
28,57%
SRCHRepository
208
5 [LT09],
[Din03],
[GMD + 10],
[JN03], [BLS98]
2.40%
SRCHInfrastructure
154
7 [DER05],
[DER04],
[TdSNB08],
[SCVJ08],
[LvM97],
[UATBK10],
[SGC11]
4,54%
SRCHPackage
9
2 [MCS + 06],
[SMB + 04]
22,22%
SRCHSupport
31
4 [BSL99],
[RWM97],
[HC06], [MN11]
12,90%
SRCHManagement
19
0
0,00%
Total Scopus
428
20
0
IEEE
SRCHMaterial
3
0
0,00%
SRCHRepository
65
1 [VZJ03]
1 [LT09]
1,53%
SRCHInfrastructure
56
0
3
[DER04],
[LT09]
[TdSNB08]
0,00%
SRCHPackage
2
1 [SBC + 02]
50,00%
SRCHSupport
16
0
2
[BSL99],
[MN11]
0,00%
SRCHManagement
22
2 [JLj09],
[RSCGP06]
9.00%
Total IEEE
164
4
6
Total
592
24
6
256
E. Espinosa et al.

3
Results
• RQ1: What web applications manage experimental materials using infrastructure,
repositories and packages?
As shown in Table 3, four articles were selected and they correspond to infras-
tructure and packages, where the use of tools for the experimental materials manage-
ment is evidenced. Speciﬁcally, studies by Do [5] and Shull [6] correspond to
infrastructure, whereas by Basili [10, 11] apply to developed packages. Finally, no
articles on repositories that manage experimental materials were found in this mapping
studying.
• RQ2: What web applications provide support for the experimental replication
process?
In the selected articles, as a result of the functionality analysis of the tools to
support the replication process in ESE, we have identiﬁed generic shortcomings:
(1) most studies do not allow to add information about new or modiﬁed versions of
experimental materials to stored existing material in the system (2) they do not display
information to view the status and trace the materials and applications within the
research and (3) no information for generating variants for new replication is observed
in any of the studies. Finally, the total number of analyzed studies up to this date is
shown in Fig. 1.
Table 3. Detail of included and excluded articles
#
Pre-selected articles
Excluded
articles
Reason
1,2,3,4,5,6,7,8
[JG12], [MCDdO06], [LT09], [Din03],
[GMD + 10], [JN03], [BLS98], [VZJ03]
Yes
No
evidence
for URL
9
[DER05]
No
10,11,12,13,14,15
[DER04], [TdSNB08], [SCVJ08],
[LvM97], [UATBK10], [SGC11]
No
evidence
for URL
16
[MCS + 06]
No
17
[SMB + 04]
No
18
[SBC + 02]
Yes
No
evidence
for URL
19
[BSL99]
No
20, 21, 22, 23, 24
[RWM97], [HC06], [MN11], [JLj09],
[RSCGP06]
Yes
No
evidence
for URL
Using Experimental Material Management Tools
257

4
Discussion
The mapping study about the use of experimental material management tools in
replications has allowed to locate and analyze a large number of articles. In addition,
we could remove a high number of articles for failing to meet the established criteria.
The total amount of articles matched with all search strings for each engine is
shown in Table 2. Thus, it has been found 428 articles in Scopus, whereas 164 in IEEE.
In addition, the number of common articles in both databases has been six. The largest
number of articles of Scopus in relation to IEEE is because Scopus includes IEEE,
ACM and others databases. This is evidenced by the existence of common articles.
Four articles were found and analyzed. We believe that these articles have been
found because of experimental managers proposed and developed tools to manage
replications and experimental material to encourage and promote the performing of
replications that allows them to validate the generated knowledge. Furthermore, the
analysis of these studies has revealed that they are consolidated researches since there
are several reports generated from some decades ago and that they still exist.
Regarding the high number of excluded articles, as shown in Table 3, there are two
main reasons. The ﬁrst one is that in the pre-selection of articles we sought to exclude
those studies where the terms of the searching strings are used but their use is not
appropriate to the context of experimentation, speciﬁcally we sought references about
the management of experimental materials and not about product for software devel-
opment which is commonly found in the excluded articles. The second reason relates to
the selection activity which has excluded studies where there is not available Web
address. As an example we quote [JG12] in Table 3.
The features of the experimental material management together with support for the
replication process is discussed below. The app designed and built by Do [5] supports
controlled experiments for testing and regression testing, including, according to the
authors, artifacts (experimental material) that allow to perform controlled experiments
and replications together with documentation of the processes used to select, organize
and conﬁgure artifacts and also tools helping to carry out these processes automatically.
The repository is online but for private use and several experimental materials are
available on web. Regarding experimental material features for experimenters, the
materials managed by the app include programs, versions, test cases, failures, and
scripts.
Fig. 1. Analyzed studies in literature
258
E. Espinosa et al.

The app built by Shull [6], is a joint effort by the Maryland University from United
States and São Carlos, Rio de Janeiro and Salvador Universities from Brazil. According
to its authors, it is a common repository for storing laboratory packages, workshops,
materials and documents associated with the experimentation project. The app manages
two experimental packages, the ﬁrst one is an experiment comparing the functional
techniques, ﬂow control, data ﬂow testing, mutation testing, and reading by step-wise
abstraction and the second one is an experiment comparing PBR and checklist tech-
niques. Packages that manages the app are available on web and they are publicly
accessible. Experimental materials in each package can be found with no restrictions on
the web. The app displays a list of materials for each of the experiments in a
chronological order, which may be partially saved (by material) in pdf ﬁle type or
completely (includes all materials) in a rar packaged ﬁle type by default. Regarding the
experimental material features for experimenters, the materials from both experiments
include: procedures for replication, theoretical and practical materials for training,
instructions for applying techniques, initial and ﬁnal forms and feedback forms of the
experiment.
The app developed by Basili [10, 11], manages a set of experimental materials, it is
publicly accessible, it is active and is freely available on web. In terms of its structure, it
consists of hypertext links associated with experimental materials used to carry out
activities of the replication process and those apparently are used by interested
experimenters as a guideline to replicate the experiment. Experimental materials
include: training and executing material of the experiment and a technical report of the
base experiment execution. In relation to the features of the experimental material,
experimental materials are available in the app and for each study techniques of the
experiment, they include: training materials, requirements speciﬁcation documents,
surveys and data collection forms. Finally, as a result of the analysis of the selected
articles, we have identiﬁed that they suffer from problems in managing replications and
experimental materials to carry out a replication process. Speciﬁcally, we have found
some limitations in the apps to support experimental replication process: (1) most
studies do not allow to add information about new or modiﬁed versions of experimental
materials to stored existing in the system (2) they do not display information to view
the status and trace of materials and applications within research and (3) no information
for generating variants for new replication is observed in any of the studies.
5
Threats to Validity
The main threat to the validity of this research is the use of databases: SCOPUS and
IEEE. This could represent only a subset of all studies taking into account primary
studies that have been identiﬁed and they probably are limited to publications (very
high standing journals and conferences). This could skew the results and, consequently,
lead to erroneous conclusions. However, bias threat is very unlikely to materialize. On
the one hand, Scopus is the largest database of abstracts and references from scientiﬁc
literature and contains indexed publications of another databases such as IEEE, ACM,
Springer and Elsevier. On the other hand, IEEE produces more than 30% of the
published research in the world. Therefore, coverage is wide, and primary studies were
Using Experimental Material Management Tools
259

really published by several of the above editors. A secondary threat to validity is related
to the keywords used for study selection. We used terms and synonyms that are
commonly used in the research community in ESE to build searching strings. We did
this thanks to a baseline study carried out on articles suggested by experts who know
and have experience of experimentation and replication in ESE.
6
Conclusions and Future Works
We have conducted a systematic mapping study in which we seek to identify the
number, publications type and technical reports available about the use of experimental
material management tools in experimental replication. We can conclude that there are
few studies on this subject, speciﬁcally, four articles have been found. Furthermore, an
analysis has discovered that such articles mostly suffer from problems in the versions
management of both replication and materials to carry out a replication process.
Finally, the experimental materials management represents a novel and little worked
framework, and we have provided useful information as a starting point for beginning a
research of the adoption of the software conﬁguration management paradigm for
experimental material management in ESE.
In this mapping study, a substantial effort was made to select a group of articles
from a lot of publications. We believe that future expansions and updates will beneﬁt
from our efforts, because these extensions will only have to apply the searching pro-
cedure published after 2014. So, we expect to make continuous updates to this mapping
study using or improving the research protocol described in this article.
Acknowledgments. The authors thank the University of the Armed Forces (ESPE-L), the
Polytechnic School - National University of Asuncion (FPUNA), the Ministry of Higher Edu-
cation on Science, Technology and Innovation (SENESCYT) and Empirical Software Engi-
neering Research Group (GrISE) for supporting the development of this work.
Appendix: List of Pre-selected Articles
[BLS98] Victor R. Basili, Filippo Lanubile and Forrest Shull. Investigating mainte-
nance processes in a framework-based environment. In 1998 International Conference
on Software Maintenance, ICSM 1998, Bethesda, Maryland, USA, November 16-19,
1998, pages 256–264, 1998.
[BSL99] V.R. Basili, F. Shull, and F. Lanubile. Building knowledge through
families of experiments. Software Engineering, IEEE Transactions on, 25(4):456–473,
Jul 1999.
[DER04] Hyunsook Do, Sebastian G. Elbaum, and Gregg Rothermel. Infrastructure
support for controlled experimentation with software testing and regression testing
techniques. In 2004 International Symposium on Empirical Software Engineering
(ISESE 2004), 19-20 August 2004, Redondo Beach, CA, USA, pages 60–70, 2004.
260
E. Espinosa et al.

[DER05] Hyunsook Do, Sebastian Elbaum, and Gregg Rothermel. Supporting
controlled experimentation with testing techniques: An infrastructure and its potential
impact. Empirical Softw Eng., 10(4):405–435, October 2005.
[Din03] T. Dingsoyr. An empirical study of an informal knowledge repository in a
medium-sized software consulting company. In Software Engineering, 2003. Pro-
ceedings. 25th International Conference on, pages 84–92, May 2003.
[GMD + 10] Mark Grechanik, Collin McMillan, Luca De-Ferrari, Marco Comi,
Stefano Crespi, Denys Poshyvanyk, Chen Fu, Qing Xie, and Carlo Ghezzi. An
empirical investigation into a large-scale java open source code repository. In Pro-
ceedings of the 2010 ACM-IEEE International Symposium on Empirical Software
Engineering and Measurement, ESEM’10, pages 11:1–11:10, New York, USA, 2010.
ACM.
[HC06] Lulu He and Jeffrey Carver. Pbr vs. checklist: A replication in the n-fold
inspection context. In Proceedings of the 2006 ACM/IEEE International Symposium
on Empirical Software Engineering, ISESE’06, pages 95–104, New York, NY, USA,
2006. ACM.
[JG12] Natalia Juristo and Omar S. Gómez. Replication of software engineering
experiments. In Bertrand Meyer and Martin Nordio, editors, Empirical Software
Engineering and Veriﬁcation, pages 60–88. Springer-Verlag, Berlin, Heidelberg, 2012.
[JLj09] Wang Jun and Liu Lan-juan. A study of design ﬂow of management
experiments based on software engineering. In INC, IMS and IDC, 2009. NCM’09.
Fifth International Joint Conference on, pages 1096–1101, Aug 2009.
[JN03] Andreas Jedlitschka and Markus Nick. Software engineering knowledge
repositories. In Reidar Conradi and AlfInge Wang, editors, Empirical Methods and
Studies in Software Engineering, volume 2765 of Lecture Notes in Computer Science,
pages 55–80. Springer Berlin Heidelberg, 2003.
[LT09] V.P. Lopes and G.H. Travassos. Knowledge repository structure of an
experimental software engineering environment. In Software Engineering, 2009.
SBES’09. XXIII Brazilian Symposium on, pages 32–42, Oct 2009.
[LvM97] Steve Lang and Anneliese von Mayrhauser. Building a research infras-
tructure for program comprehension observations. In 5th International Workshop on
Program Comprehension (WPC ‘97), May 28-30, 1997-Dearborn, USA, pages
165-169, 1997.
[MCDdO06] Manoel Mendonca, Daniela Cruzes, Josemeire Dias, and Maria
Cristina Ferreira de Oliveira. Using observational pilot studies to test and improve lab
packages. In Proceedings of the 2006 ACM/IEEE International Symposium on
Empirical Software Engineering, ISESE ‘06, pages 48{57, New York, NY, USA, 2006.
ACM.
[MCS + 06] Jose Carlos Maldonado, Jeffrey Carver, Forrest Shull, Sandra
Camargo Pinto Ferraz Fabbri, Emerson Doria, Luciana Andreia Fondazzi Martimiano,
Manoel G. Mendon¸ca, and Victor R. Basili. Perspective-based reading: A replicated
experiment focused on individual reviewer effectiveness. Empirical Software Engi-
neering, 11(1):119–142, 2006.
[MN11] S. Matsumoto and M. Nakamura. Service oriented framework for mining
software repository. In Software Measurement, 2011 Joint Conference of the 21st Int’l
Using Experimental Material Management Tools
261

Workshop on and 6th Int’l Conference on Software Process and Product Measurement
(IWSM- MENSURA), pages 13–19, Nov 2011.
[RSCGP06] D. Rodriguez, M.A. Sicilia, J.J. Cuadrado- Gallego, and D. Pfahl.
e-learning in project management using simulation models: A case study based on the
replication of an experiment. Education, IEEE Transactions on, 49(4):451–463, Nov
2006.
[RWM97] Marc Roper, Murray Wood, and James Miller. An empirical evaluation
of defect detection techniques. Inf. Softw. Technol., 39(11):763-775, June 1997.
[SBC + 02] Forrest Shull, Victor Basili, Jeffrey Carver, Jose C. Maldonado,
Guilherme Horta Travassos, Manoel Mendon¸ca, and Sandra Fabbri. Replicating
software engineering experiments: Addressing the tacit knowledge problem. In Pro-
ceedings of the 2002 International Symposium on Empirical Software Engineering,
ISESE’02, pages 7–, Washington, DC, USA, 2002. IEEE Computer Society.
[SCVJ08] Forrest J. Shull, Jeffrey C. Carver, Sira Vegas, and Natalia Juristo. The
role of replications in empirical software engineering. Empirical Softw. Engg., 13
(2):211–218, April 2008.
[SGC11] Lilian Passos Scatalon, Rogerio Eduardo Garcia, and Ronaldo Celso
Messias Correia. Packaging controlled experiments using an evolutionary approach
based on ontology(s). In Proceedings of the 23rd International Conference on Software
Engineering & Knowledge Engineering (SEKE’2011), Eden Roc Renaissance, Miami
Beach, USA, July 7-9, 2011, pages 408–413, 2011.
[SMB + 04] Forrest Shull, Manoel G. Mendonca a, Victor Basili, Jeffrey Carver,
José C. Maldonado, Sandra Fabbri, GuilhermeHorta Travassos, and MariaCristina
Ferreira. Knowledge-sharing issues in experimental software engineering. Empirical
Software Engineering, 9(1-2):111{137, 2004.
[TdSNB08] G.H. Travassos, P.S.M. dos Santos, P.G.M. Neto, and J. Biolchini. An
environment to support large scale experimentation in software engineering. In Engi-
neering of Complex Computer Systems, 2008. ICECCS 2008. 13th IEEE International
Conference on, pages 193– 202, March 2008.
[UATBK10] Florian Uhlig, Mohammad Al-Turany, Denis Bertini, and Darmstadt]
Karabowicz, Radek [GSI. Software Development Infrastructure for the FAIR Experi-
ments, 10 2010. https://www-alt.gsi.de/documents/ DOC-2010-Nov-199.html.
[VZJ03] P. Vitharana, F.M. Zahedi, and H. Jain. Knowledge-based repository
scheme for storing and retrieving business components: a theoretical design and an
empirical analysis. Software Engineering, IEEE Transactions on, 29(7):649–664, July
2003.
References
1. Daly, J., Brooks, A., Miller, J., Roper, M., Wood, M.: Veriﬁcation of results in software
maintenance through external replication. In: 1994 Proceedings of International Conference
on Software Maintenance, pp. 50–57 (1994)
2. Schmidt, S.: Shall we really do it again? the powerful concept of replication is neglected in
the social sciences. Rev. Gen. Psychol. 13, 90–100 (2009)
262
E. Espinosa et al.

3. Mendonca, M.G., Maldonado, J.C., de Oliveira, M.C.F., Carver, J., Fabbri, S.C.P.F., Shull,
F., Travassos, G.H., Hohn, E.N., Basili, V.R.: A framework for software engineering
experimental replications. In: 13th IEEE International Conference on Engineering of
Complex Computer Systems 2008 ICECCS 2008, pp. 203–212 (2008)
4. Shull, F., Basili, V., Carver, J., Maldonado, J.C., Travassos, G.H., Mendonça, M., Fabbri, S.:
Replicating software engineering experiments: addressing the tacit knowledge problem. In:
Proceedings of 2002 International Symposium on Empirical Software Engineering 2002,
pp. 7–16 (2002)
5. Do, H., Elbaum, S., Rothermel, G.: Supporting controlled experimentation with testing
techniques: an infrastructure and its potential impact. Empir. Softw. Eng. 10, 405–435
(2005)
6. Shull, F., Mendoncça, M.G., Basili, V., Carver, J., Maldonado, J.C., Fabbri, S., Travassos,
G.H., Ferreira, M.C.: Knowledge-sharing issues in experimental software engineering.
Empir. Softw. Eng. 9, 111–137 (2004)
7. Vegas, S., Juristo, N., Moreno, A., Solari, M., Letelier, P.: Analysis of the inﬂuence of
communication between researchers on experiment replication. In: International Symposium
on Empiricial Software Engineering, p. 28 (2006)
8. Petersen, K., Feldt, R., Mujtaba, S., Mattsson, M.: Systematic mapping studies in software
engineering. In: 12th International Conference on Evaluation and Assessment in Software
Engineering (2008)
9. Beecham, S., Baddoo, N., Hall, T., Robinson, H., Sharp, H.: Protocol for a systematic
literature review of motivation in software engineering (2006)
10. Basili, V.R., Shull, F., Lanubile, F.: Building knowledge through families of experiments.
IEEE Trans. Softw. Eng. 25, 456–473 (1999)
11. Maldonado, J.C., Carver, J., Shull, F., Fabbri, S., Dória, E., Martimiano, L., Mendonça, M.,
Basili, V.: Perspective-based reading: a replicated experiment focused on individual
reviewer effectiveness. Empir. Softw. Eng. 11, 119–142 (2006)
Using Experimental Material Management Tools
263

Unveiling Unbalance on Sustainable Supply
Chain Research: Did We Forget Something?
Edison Loza-Aguirre1,2(&), Marco Segura Morales1, Henry N. Roa3,
and Carlos Montenegro Armas1
1 Facultad de Ingeniería en Sistemas, Escuela Politécnica Nacional,
Ladrón de Guevara, E11-253, P.O. Box 17-01-2759, Quito, Ecuador
{edison.loza,marco.segura,
carlos.montenegro}@epn.edu.ec
2 CERAG FRE 3748, CNRS/UGA, 150, rue de la Chimie,
BP 47, 38040 Grenoble Cedex 9, France
lozaedison@univ-grenoble-alpes.fr
3 Facultad de Ingeniería, Pontiﬁcia Universidad Católica del Ecuador,
Av. 12 de Octubre 1076 y Vicente Ramón Roca, Quito, Ecuador
hnroa@puce.edu.ec
Abstract. Even though it has been theorized that initiatives on sustainable
development should pursue an equilibrium among social, environmental and
economic dimensions, several studies have pointed that an unbalance exists
regarding the consideration of the three dimensions. However, there is little
evidence to support such unbalance. Thus, in this article, we propose a tool to
determinate sustainable dimensions balance by representing sustainable efforts
according to their orientation. To test our tool, we reviewed about ten years of
literature from top tier journals dealing with Sustainable Supply Chain issues to
establish the sustainable efforts undertaken. Our results visually unveil unbal-
ance on research in this ﬁeld and show that most research is oriented to envi-
ronmental and economic aspects, leaving social issues aside.
Keywords: Monte Carlo method  Sustainable development
Sustainable supply chain  Logistics  Transport
1
Introduction
The introduction of sustainable development (SD) inside an organization is transversal
because its adoption affects almost all business areas. In this connection, the logistics
and supply chain activities are primarily concerned not only for traditional association
with environmental pollution but also because of their potential to propose solutions in
terms of sustainability [1–3]. Thus, this interaction between SD and logistics activities
originated a new application area named sustainable logistics or, in a more complex
and larger view, sustainable supply chain (SSC).
The scientiﬁc community has nearly followed the growth of the newcomer disci-
pline. However, as an emerging research area, the study of SSC has not yet a consensus
framework and even the implications of this notion are neither stable nor clear [4, 5].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_26

From a theoretical perspective, a SSC “performs well on both traditional measures of
proﬁt and loss as well as on an expanded conceptualization of performance that
includes social and natural dimensions” [6]. This deﬁnition, which is inspired by the
principles of Elkington’s triple bottom line [7], establishes a new perspective of the
traditional economic-only view of performance in supply chains and puts in evidence
the needs of balance between the three dimensions of SD – i.e. environmental, social
and economic. Nonetheless, several academics have pointed that an unbalance exists
regarding the consideration of the three dimensions [4, 5, 8]. Even when this unbalance
has been suggested, no evidence has been offered to support this claim. Thus, in this
article, we propose a tool for representing sustainable efforts according to their ori-
entation. The aim of this tool is to determinate if there are sustainable dimensions that
are more privileged than others. To test the tool, we reviewed about ten years of
literature from top tier journals in the ﬁelds of Supply Chain Management and Oper-
ations to establish the sustainable efforts undertaken.
In this article, ﬁrst, we present the different trends used in literature to study the
integration of SD issues in logistics and supply chain activities. Then, we present a
description of the developed tool for representing sustainable efforts in the three
dimensions of SD. The results of the evaluation of the tool are presented in Sect. 4, and
discussed in Sect. 5. Finally, conclusions are presented.
2
From a Partial to a “Truly” Sustainable Supply Chain
Perspective
As mentioned earlier, there is not consensus about the integration of SD in the domain
of supply chain management. This integration has been discussed in the literature
through three trends, which differ from one another by focusing on different dimen-
sions of SD (Fig. 1). The main considerations of these approaches are detailed below.
Fig. 1. Different trends used in academic literature for studying the integration of SD in supply
chain management.
Unveiling Unbalance on Sustainable Supply Chain Research
265

2.1
Green Supply Chain Management
Most extended efforts to introduce the sustainability issues in supply chain manage-
ment considerations have been oriented “to green the supply chain”. In this regard,
environmental management and supply chain management were coupled together to
originate the notion of Green Supply Chain Management1 (GSCM) [3] as a conﬁr-
mation that a global view of the supply chain is more adequate to address environ-
mental factors than local optimizations [9]. An important aspect of GSCM thinking is
that activities that are oriented to reduce the ecological impact are, at the same time,
intended to become a source of economic proﬁt [10, 11]. In this context, environmental
and economic dimensions of sustainability are directly concerned.
2.2
Logistics Social Responsibility
The GSCM approach is centered in environmental and economic issues but fails to
consider the social dimension of SD. In this regard, some research has been conducted
to propose a more complete approach that also considers social. Those works fall in the
group of what is known as Logistics Social Responsibility (LSR)2.
As their name signals, LSR approaches are based on the precepts of Corporate
Social Responsibility. Thus, LSR focuses in ﬁve dimensions: environment, safety,
human rights, diversity and philanthropy [12]. Nonetheless, until the last few years, the
dominant tendency was to study those aspects as standalone [13]. The above deﬁnitions
represent an effort to position social and environmental issues at the center of the
debate. In addition, even when Murphy and Poist [14] explicitly include economic
importance, authors seem to forget this dimension in their empirical investigation [4].
Nowadays, however, research works that deal with LSR concerns are not numerous.
2.3
Sustainable Supply Chain Management
As we presented, research on LSR does not include explicitly economic aspects. This
omission could suppose a no desirable situation as some authors considers that any social
or environmental initiative cannot be everlasting without economic success [15, 16]. This
consideration resulted on the notion of Sustainable Supply Chain Management (SSCM),
which refers to “the management of material, information and capital ﬂows as well as
cooperation among companies along the supply chain while taking goals from all three
dimensions of SD into account which are derived from customer and stakeholder
requirements” [2].
Carter and Rogers [4] proposed a framework for SSCM based on Elkington’s
Tripple-Botton Line [7]. In this framework, all social and environmental activities that
can harm or not help the economic dimension must be placed outside of the zone where
1 Some authors, as Handﬁeld et al. [17], use the term Environmental Supply Chain Management
instead of GSCM. All the same, both terms are equivalents.
2 Some authors, as Ciliberti et al. [18], use the term Social Responsible Supply Chain Management
instead of Logistics Social Responsibility. Others, as Murphy and Poist [14], use the term Socially
Responsible Logistics. All the same, cited terms are equivalents.
266
E. Loza-Aguirre et al.

the economic dimension overlaps the other two. The “best” area, the intersection of
three dimensions, is the “truly sustainable” area, where activities balance performance
in environmental, social and economic aspects. It is presumed that organizations
seeking success in integrating SD on their supply chain activities should pace their
efforts on initiatives falling inside the intersection of the three dimensions of SD.
3
Proposing a Tool to Represent Sustainable Efforts by Their
SD Orientation
3.1
Representation Principle
We developed a tool to represent visually the efforts undertaken to integrate SD
according to the dimension to which each effort belongs (social, economic or envi-
ronmental), but also if they belong to any intersection of two or all three dimensions.
To develop our tool, we took the Elkington’s sustainability representation [7] as a basis
(Fig. 2).
We propose to represent quantitatively the initiatives on sustainability as a circle
area or an intersection circle area. This circle areas represent values corresponding to
ﬁnancial investments or any kind of countable items – e.g. articles, protects, words.
Accordingly, initiatives on SD must be classiﬁed previously based on their orientation.
For instance, if some initiatives involve only environmental issues, these initiatives
must be classiﬁed as “only environmental”. Otherwise, if the initiatives involve two or
three SD dimensions (e.g. environmental and social), these initiatives must be classiﬁed
as “environmental and social”.
3.2
Tool Implementation
In this research, we followed a design science approach [19]. The fundamental prin-
ciple of design science research is that both knowledge and understanding of a design
Fig. 2. The triple bottom line for representing SD [7]
Unveiling Unbalance on Sustainable Supply Chain Research
267

problem, as well as the solution of the problem itself, are acquired through the con-
struction of an artifact [20]. In our case, the resulting artifact corresponds to a tool for
representing sustainable efforts according to their orientation. The implemented tool
considers three circles, each one of them representing the Economic, Social and
Environmental dimensions, with an area proportional to the number of components
identiﬁed for each category. The implementation is based on a Monte Carlo approach
to calculate the area of a surface, which can be a whole circle or the shared area
between two or more circles.
As depicted in the Fig. 3, the process starts with a random generation of points
which can be used to determine surface areas and to change the position of the circles
on the screen. In the next step, we determine the appropriate distance between the
Economic and Social circles, based on their number of components and therefore their
total and shared areas. Then, we start a series of iterations where we assign a random
position to the Environmental circle and we calculate the error as the difference
between the shared areas between the three circles and the expected value for those
areas. The next step in the process consists in determining the position for the Envi-
ronmental circle, based on the lowest error achieved in the previous set of iterations.
Finally, we draw the three circles on the screen with a different set of color for each one
of them and their shared areas.
Fig. 3. Process for calculating representation of SD dimensions.
268
E. Loza-Aguirre et al.

4
Evaluating the SSC Research Orientation
The purpose of this evaluation is to use the tool to identify if there are sustainable
dimensions that are more privileged than others by reviewing about ten years (2002–
2012) of literature from top tier journals in the ﬁelds of Supply Chain Management and
Operations. These ﬁelds were chosen because they have been traditionally associated
with the research in supply chain management, logistics, and transport. we reviewed
from top tier journals dealing with Sustainable Supply Chain. We truncated the period
of our review due to contractual embargo periods on our databases subscriptions.
To identify the top-tier journals in the selected ﬁelds, we conducted a review in
several journal rankings, retaining the journals that were ranked at one the two higher
level on at least one ranking. Since our research was conﬁned to articles published
before 2012, we consulted the standing rankings at that time. Thus, the rankings used
for this evaluation are listed below:
• The journal ranking of the Center for Advanced Studies in Management and
Economics (CEFAGE) from the University of Évora. 2nd Edition 2009–2011.
• The journal ranking of the National Centre for Scientiﬁc Research. Classiﬁcation of
journals in economics and management 2011.
• The Erasmus Research Institute of Management (ERIM) journal list 2011–2015
from the Erasmus University of Rotterdam.
• The Association of Business Schools (ABS) Academic Journal Quality Guide
version 4 published in 2010.
• The ranking of journals VHB-JOURQUAL 2.1 published in 2011 by the German
Academic Association for Business Research.
• The Australian Business Deans Council Journal Ratings List 2010.
• The Excellence in Research for Australia (ERA) 2010 Ranked Journal List from the
Australian Research Council and the 2011 adjusted ERA Rankings List from the
University of Queensland Business School (UQBS).
• The 2011 review of journal rankings for transport, logistics and supply chain
management from the Institute of Transport and Logistics of the University of
Sidney.
• The 2011 ranking of scientiﬁc management journals of the National Foundation for
Companies Management Academic Education (FNEGE).
For testing the pertinence and validity of our journal selection approach, we
reviewed the quartile indicator of each journal on the SCImago (and SCOPUS) Journal
Rank (SJR). We validated the journals that were classed on the quartiles Q1 and Q2 in
2012. Accordingly, we retained six Production and Operations journals and seven
Supply Chain and Logistics journals3:
Production and Operations Journals
• International Journal of Operations and Production Management
• International Journal of Production Economics
3 The full results can be provided by request.
Unveiling Unbalance on Sustainable Supply Chain Research
269

• International Journal of Production Research
• Journal of Operations Management
• Production and Operations Management
• Production Planning and Control
Supply Chain and Logistics Journals
• International Journal of Logistics Management
• International Journal of Logistics: Research and Applications
• International Journal of Physical Distribution & Logistics Management
• Journal of Business Logistics
• Journal of Supply Chain Management
• Supply Chain Management: An International Journal
• Transportation Research Part E
Then, we collected the articles of these journals for our review. We systematically
applied the following ﬁlters on the databases where the selected journal was present.
We conducted these queries in the title, keywords and abstract ﬁelds:
• Sustainable AND supply chain
• Sustainable AND logistics
• Green AND supply chain
• Green AND logistics
• Sustainability AND supply chain
• Sustainability AND logistics
• Social AND sustainable AND supply chain
• Social AND sustainable AND logistics
• Social AND sustainability AND supply chain
• Social AND sustainability AND logistics
• Social AND responsibility AND supply chain
• Social AND responsibility AND logistics
Using these research parameters, we retained 193 articles4. To analyze the orien-
tation of each article in terms of the three dimensions of sustainability – i.e. environ-
mental, social and economic. Each article was carefully read and coded according to
the main orientation of their problematic. For coding, we use the coding scheme
presented in Appendix. Since some articles deal with quite a few problems at the same
time, an article could be coded in one or more dimensions. The results of the coding are
presented in Table 1.
To represent unbalance graphically, we used the results from coding as inputs to the
tool we developed. With a total of 1,000,000 points to calculate the area of each circle,
and 100,000 random positions for the Environmental circle, the minimum error
achieved was 1.9118, and the resulting graphical representation is depicted in the ﬁgure
on the right side of the Fig. 4.
Figure 4 shows graphically this imbalance between the three dimensions of sus-
tainability in supply management research. In the left side of the ﬁgure, the traditional
4 The full list can be provided by request.
270
E. Loza-Aguirre et al.

representation of SD is presented. In this representation, “truly” sustainability is
reached in the intersection area of three dimensions. In the right part of the ﬁgure, the
representation obtained, in this research, visually let us evidence the dimensional
reduction presumption in SSC research. First, most of research is oriented to the
intersection of environmental and economic aspects. Second, social issues are the less
studied from the three aspects. Finally, even when the sustainability area – the inter-
section of three dimensions – looks interestingly important, less than half of articles in
this area (8 of 20) is empirical in nature, the others are theoretical contributions.
Table 1. Number of articles by category from coding.
Totals by category
Only environmental
7
Only economic
21
Only social
8
Environmental + economic
123
Environmental + social
5
Economic + social
9
Economic + environmental + Social
20
Total
193
Fig. 4. Comparison between Elkington’s sustainability model and orientations of research in
SSC obtained from this research.
Unveiling Unbalance on Sustainable Supply Chain Research
271

5
Discussion and Conclusions
In this article, we proposed a tool for representing sustainable efforts according to their
orientation. The aim was to determinate if there are sustainable dimensions that are
more privileged than others. Our results suggest that academic research on SSC are
mainly concerned with a GSC view (environmental and economic concerns) rather than
with a “truly” SSC perspective, which also integrates social concerns. Even though
theoretical contributions on the subject have called for balancing social, economic and
environmental concerns [4, 6, 16], our results report that social is, by far, the dimension
of SD that has received less attention in comparison to the other two.
Besides our results, originated from the academic world, some questions emerge
about how SD issues are understood in practice by organizations: Is there a balance in
practice? Is it important to pursuit a “truly” SSC? And what are the drivers and barriers
for balancing SSC efforts? These questions should be at the origin of further research
conducted within a private context. Since the tool can represent not only coding units,
such articles or words, but monetary values, it can become a useful mechanism to
represent and analyze investments in SD. These studies could also evaluate their
robustness and utility of the tool in practice when organizations use it to evaluate their
efforts on SD.
Even though our results are limited only to a small spectrum of SD concerns, those
of logistics and transports, further research could be interested to expand these results
on context others than SSC. Our study was also limited by access to databases. Another
study can also be addressed to analyze the evolution of the research orientation in the
last ﬁve years. Finally, further research could be interested to improve the algorithms
used in this study. Another venue lies on the use of text mining techniques to analyze
textual corpus without laborious and time consuming human intervention. In this sense,
methods to ﬁnd frequencies or topic analysis could become new input to feed the
proposed tool in this research.
Appendix: Coding Scheme
See Table 2.
272
E. Loza-Aguirre et al.

References
1. Carter, C.R., Easton, P.L.: Sustainable supply chain management: evolution and future
directions. Int. J. Phys. Distr. Log. 41(1), 46–62 (2011)
2. Seuring, S., Müller, M.: Core issues in sustainable supply chain management - a Delphi
study. Bus. Strateg. Environ. 17(8), 455–466 (2008)
3. Srivastava, S.K.: Green supply-chain management: a state-of-the-art literature review. Int.
J. Manag. Rev. 9(1), 53–80 (2007)
4. Carter, C.R., Rogers, D.S.: A framework of sustainable supply chain management: moving
toward new theory. Int. J. Phys. Distr. Log. 38(5), 360–387 (2008)
5. Pagell, M., Shevchenko, A.: Why research in sustainable supply chain management should
have no future. J. Supply Chain Manag. 50(1), 44–55 (2014)
6. Pagell, M., Wu, Z.: Building a more complete theory of sustainable supply chain
management using case studies of 10 exemplars. J. Supply Chain Manag. 45(2), 37–56
(2009)
7. Elkington, J.: Cannibals with Forks: The Triple Bottom Line of 21st Century Business. New
Society Publishers, Gabriola Island (1998)
8. Yawar, S.A., Seuring, S.: Management of social issues in supply chains: a literature review
exploring social issues, actions and performance outcomes. J. Bus. Ethics 141(3), 621–643
(2017)
Table 2. Coding scheme for classifying articles according to their SD orientation. An article is
coded within a SD dimension if its main subject is listed on the ‘Themes’ column. Since some
articles deal with quite a few problems at the same time, an article could be coded in one or more
dimensions.
SD dimension Themes
Environment
Energy conservation
Environmental friendly transport
Environmental measurement and evaluation
Gas emissions reduction
Green manufacturing
Green product design
Green purchasing and supply
Green urban logistics
Reverse logistics
Waste management
Social
Ethics
Human rights
Philanthropy
Quality of life
Local communities
Safety
Diversity
Economic
Cost reductions
Sustainable competitive advantage
Financial revenues
Unveiling Unbalance on Sustainable Supply Chain Research
273

9. Koh, S.C.L., Gunasekaran, A., Tseng, C.S.: Cross-tier ripple and indirect effects of directives
WEEE and RoHS on greening a supply chain. Int. J. Prod. Econ. 140(1), 305–317 (2012)
10. Constantini, V., Crespi, F., Marin, G., Paglialung, E.: Eco-innovation, sustainable supply
chains and environmental performance in European industries. J. Clean. Prod. 155(2), 141–
154 (2017)
11. Rao, P., Holt, D.: Do green supply chains lead to competitiveness and economic
performance? Int. J. Oper. Prod. Manag. 25(9), 898–916 (2005)
12. Carter, C.R., Jennings, M.M.: Logistics social responsibility: an integrative framework.
J. Bus. Logist. 23(1), 145–180 (2002)
13. Carter, C.R., Jennings, M.M.: Social responsibility and supply chain relationships.
Transp. Res. E-Log. 38(1), 37–52 (2002)
14. Murphy, P.R., Poist, R.F.: Socially responsible logistics: an exploratory study. Transp. J. 41
(4), 23–35 (2002)
15. Frota Neto, J.Q., Bloemhof-Ruwaard, J.M., Van Nunen, J.A.E.E., Van Heck, E.: Designing
and evaluating sustainable logistics networks. Int. J. Prod. Econ. 111(2), 195–208 (2008)
16. Seuring, S., Müller, M.: From a literature review to a conceptual framework for sustainable
supply chain management. J. Clean. Prod. 16(15), 1699–1710 (2008)
17. Handﬁeld, R., Sroufe, R., Walton, S.: Integrating environmental management and supply
chain strategies. Bus. Strateg. Environ. 14(1), 1–19 (2005)
18. Ciliberti, F., Pontrandolfo, P., Scozzi, B.: Logistics social responsibility: standard adoption
and practices in Italian companies. Int. J. Prod. Econ. 113, 88–106 (2008)
19. March, S.T., Smith, G.F.: Design and natural science research on information technology.
Decis. Support Syst. 15, 251–266 (1995)
20. Hevner, A.R., March, S.T., Park, J., Ram, S.: Design science in information systems
research. MIS Quart. 28(1), 75–105 (2004)
274
E. Loza-Aguirre et al.

Industry Knowledge Management Model 4.0
José Ignacio Rodríguez-Molano(&),
Leonardo Emiro Contreras-Bravo, and Edwin Rivas-Trujillo
Universidad Distrital Francisco José de Caldas, Bogotá, Colombia
{jirodriguez,lecontrerasb,erivas}@udistrital.edu.co
Abstract. This article describe the actual situation for the platforms designed
for the IoT (internet of Things) through the denominated industry 4.0 presenting
related concepts. Based on the previous information we will do a knowledge
management model for the industry 4.0 acknowledging the role of the used
platforms on industry.
Keywords: Industry 4.0  Internet of things  Platforms IoT  Big data
Knowledge management
1
Introduction
It is important to highlight the application of technology in global scale for ICT
(Information & Communication Technologies) industry. The Industry’s future is
coined by “4.0 Industry” concept [1] which means, that through automatization and the
interconnection of all the activities and processes of the company, Internet of Things
concept is developed, where each time more devices and tools are equipped to the
companies with sensors able to collect large amounts of data that have to be gathered,
processed and storage in cloud through tools like big data.
The decisions taken are the ones that deﬁne differential aspects stabilising the path
followed by a company or an enterprise [2], those decisions rely upon tools of
acquisition, collection, organization and analysis of information of the scenery of study.
Which is why, this article claims to show up the current state of the IoT existing
platforms in the studied area with their deﬁnition, characteristics and differential factors
of the applications in several countries and regions around the globe such as success
cases, and how these platforms help to make decisions that determinate the future of
companies in the current society of information and knowledge management [3, 27].
For the reader’s convenience, this article is organized as follows: in the ﬁrst section,
general introduction is developed, then second section shows the deﬁnitions and cur-
rent state of key concepts. After that, third section regards the characteristics of each
consulted IoT platforms, followed by fourth section describing the development of
platforms worldwide, and for Colombian case in ﬁfth section. Finally, last section
presents conclusions disaggregate from the paper content.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_27

2
Conceptualization
2.1
Industry 4.0
The industry 4.0 is considered as the designation for the fourth industrial revolution,
distinctive of the current trend of data exchange and automation in the technology
industry. It includes cyber-physical systems, internet of things (IoT) and cloud
computing [4].
Regarding Cheng, Liu and Qiang words’ [5] industry 4.0 describes industry future as
the establishment of internet and technological information based on interactive plat-
forms, integrating each time more factors of scientiﬁc production becoming it in a more
automated and connected industry. In addition to custom manufacturing, the changes
done and the new standard, industry 4.0 incorporates information and manufacturing
technology, it means that it is the fourth industrial revolution dominated by intelligent
manufacturing. What in brief results let industry 4.0 as intelligent factories [6].
As mentioned before, industry 4.0 is based on cyber-physical systems [5], that
connect virtual spaces with physical realities, integrating computing, communication
and storage capacity, in real time, reaching limits of reliability, stability, security and
acceptable efﬁciency.
Core concept of the cyber physical system is the 3C (Computation, Communication
and Control) to achieve interaction between the real world and information in real time
through feedback loops of engagement between computational and physical processes.
All that, in order to increase or expand new functions, providing real time sensing,
dynamic control and information feedback, among other characteristics. The complete
cycle reﬂects perfectly as an industry 4.0 example, relying on IoT technology, a variety
of sensors and actuators are used to trace the vital sign data in actual time and transmit
it to data center through the wireless networks technologies to establish electronic ﬁles
of manipulation.
2.2
Internet of Things
Taking an overall vision, IoT is the acronym that makes reference to each object that
has its own virtual identity and the potential capacity to integrate and interact inde-
pendently in a network with any other individual or either between machines [7].
According with Cisco’s concept, internet of things is simply the point in time, when
more things or objects than people get connected to internet.
It is a priority for asset-intensive companies to use their equipment to the maximum
and optimally to become more proﬁtable. Increasing the performance depends on
several factors as the operation, design, maintenance’s plans and the operational
context, modeling of factors will allow better control over the asset. However, this is
only possible if accurate information that allows to characterize the equipment, and that
is where the IIoT (Industrial Internet of Things) concept appears as the use of IoT
technology in industry, which has three phenomena that characterize it, as show in
Table 1:
276
J. I. Rodríguez-Molano et al.

2.3
Big Data
In short words, big data can be deﬁned as the management of a variety, volume and
speed of data transferred, often used by the industry as a tool to automatically track the
performance of IT systems and their behavior, and to innovate in business strategies
and improve overall operational efﬁciency [8]. Substantially, big data are massive
amounts of data that accumulate over time, difﬁcult to analyze and manage using
common database management tools. In the same way Zdnet, interprets big data as
tools, processes and procedures that allow an organization to create, manipulate and
manage huge datasets and storage facilities, adding the concept of physical space to the
Big Data dimension [9].
3
Industry 4.0 Platforms
Nowadays, the success of a company that aims to implement industry 4.0 in its pro-
cesses will depend of company’s innovation capacity [10] that goes hand in hand with
interaction with other companies, through development platforms that make it easier
for other companies to develop this philosophy, this paper will show the deﬁnitions,
characteristics and advantages of each platform gives to the organization from its own
development, taking into account that the platforms described here follow the logic of
the ranking of worldwide demand.
3.1
Splunk
Deﬁnition: Splunk transforms machine generated data into valuable information,
enabling companies to be more productive, proﬁtable and secure [11].
Relation with Big Data: within the beneﬁts offered by Splunk, big data plays an
important role, because it allows to obtain and process the data generated by machines
with one of the most complex and fastest growing areas of big data, containing a record
of all transactions, client behaviors, sensor readings, machine behaviors, security
threats, fraudulent activities and so on, it is where Splunk helps discover the hidden
value of all data generated by the machines, providing a uniﬁed method of organizing
and extracting real time information from huge amounts of machine data from prac-
tically any source: Websites, enterprise applications, social networking platforms,
application servers, hypervisors, sensors, traditional databases, and warehouses open
source data.
Table 1. IIoT phenomena.
Miniaturization It makes that server’s components become smaller, providing connection of
anything, anywhere
Infrastructure
Mobile telephony’s infrastructure is no longer limited
Proliferation
Of applications and services that use the data generated of IoT
Industry Knowledge Management Model 4.0
277

IoT and IIoT Relation: it provides real time information on sensors, devices and
industrial systems such as SCADA by providing a ﬂexible and versatile platform for
machine data generated by all sources connected in current networks.
3.2
Anella Industrial 4.0
Deﬁnition: This platform was born in Cataluña and developed by technology centers
and local industries with the aim of providing a secure, stable and accessible framework
for the exchange of cloud-IT services as a global solution to the expansion of the IIoT.
This is a digital transformation tool that through a marketplace facilitates digitization of
processes and industrial companies will be able to access to a catalog of digital services
for the industry. It gives efﬁcient, secure and scalable access to standard software
solutions [12].
3.3
Xompass Faas
Deﬁnition: This is an end-to-end solution that adds Edge Intelligence to thousands of
assets in mining, water, power, oil, gas and power using cloud power [13].
Description: Platform oriented in three senses that facilitate to the company man-
agement of its operation from the advantages of the internet, big data, cloud computing
and IIoT, allowing:
• Rapid deployment, all assets can be connected in seconds, integrating them with
existing control systems beyond the control layer, and is instantly available for
implementation at every time.
• Productivity improvement, the power of cloud and the new data management
intelligence make it possible, to make better decisions in real time, which allow to
operate assets efﬁciently, reducing product losses.
• Downtime reduction, it is easier to avoid downtime with cloud-enabled predictive
asset behavior models, by consolidating ﬁeld data source, maintenance workforce is
reduced, resulting in lower cost of preventive and corrective maintenance.
3.4
Microsoft Azure
Deﬁnition: Microsoft’s Azure is an open and ﬂexible cloud platform to quickly build,
deploy, and manage applications across a global network of Microsoft-managed dat-
acenters. It provides the foundation for business and consumer applications that deliver
a consistent way for people to store and share information easily and securely in the
cloud, and access it on any device from any location [14].
Description: Microsoft Azure works through three tools and developers which include
the plataforms functions:
• IaaS: Services oriented to the user, in order to have full control of virtual infras-
tructure. It includes everything related to servers (virtual machines), where to
278
J. I. Rodríguez-Molano et al.

choose operating system (Windows Server, Linux, Oracle, Open Logic, etc.),
number of processing cores, RAM capacity or virtual disks.
• PaaS: Platform which is already created by Azure and it is also managed by itself.
PaaS escalates, develops and manages the customer needs through the apps.
• SaaS: Services where infrastructure and platform remain hide under an abstraction’s
cape.
3.5
Watson IoT Platform
Deﬁnition: It allows to make sense to data to optimize operations, manage assets,
rethink products and services, and transform customer experience [15].
Description: See Table 2.
4
Avant-Grade Countries in 4.0 Industry
4.1
Germany
Following Industry 4.0 essence in Germany is “Internet + Manufacturing” [16]. For
this reason, Germany is an example of initiatives’ deployment around Industry 4.0 and
the development for those technologies make possible digital transformation, com-
bining the exemplary participation of public administrations with the active role of
companies, universities and research centers, in order to generate industry 4.0
ecosystems. Example of this collaborative ecosystem are the projects German National
Academy of Science and Engineering (ACATECH) [5].
Fraunhofer Institute of Industrial Engineering and automation in Germany predicts
that in the country, industry 4.0 can improve productivity of 20 to 30% in 2025 [17].
Table 2. Watson IoT platform solutions.
Analyze Natural Language Processing: Automatically binds spoken words with meaning and
intention of user
Automatic learning: Automates data processing and classiﬁes data according to
priorities
Image and video analysis: Monitor unstructured video channel data and image
snapshots to identify scenes and patterns in video data
Text Analysis: Survey unstructured textual data
Connect Easily connection of chip devices to smart devices to their applications and industry
solutions
Manage
Management of information and integration of external data
Manage risk and collect information from IoT’s environment using sophisticated
dashboards and alerts
Insert
Enter data from other data sources and platforms, to increase devices’ data and
perform additional analysis
Develop Develop a variety of cloud services as data track record
Industry Knowledge Management Model 4.0
279

In summary, data on application’s process for industry 4.0 represent an investment of
40 billion euros per year with a forecast of digital transformation over 80% of
industries by 2021, an efﬁciency increase of 18%, a cost reduction around 13% and a
potential business growth of 425 billion euros in 2025 [18].
4.2
China
For Asian country, the transitional program “Created in China” preliminary to the
program “Made in China” as the complete transformation [5], stablish that manufac-
ture’s future based on interactive platforms, internet and information technologies are
going to be increasingly integrated to scientiﬁc and automated production factors, to
networks creation and intellectuality added to standard of personalized manufacturing.
In 2015, Chinese government promulgated “Made in China 2025” program, the Chinese
version of Industry 4.0, which will be on the stage of history, in future development,
directly related to the development of China’s industrial economy [5].
In this way, digitization is an appropriate trampoline for China. According with
ofﬁcial Chinese estimations, Industry 4.0 can increase China’s productivity by 25 to
30% and reduce unforeseen production losses in a 60% [19]. Moreover, from Chinese
perspective the per capita income could be on par with countries such as United States,
Germany and Japan, in accordance with a study of the Chinese Academy of Engi-
neering (CAE) by the year 2045 thanks to the correct and the total implementation of
the fourth industrial revolution [20].
5
Industry 4.0 in Colombia
5.1
Contextualization
For Colombia, which at Latin American level has the fourth position in Open Data [21].
The capital city, Bogota is also at the top part of the three most intelligent cities in Latin
America, along with Sao Paulo and Mexico City [22]. However, this regional posi-
tioning is not the guarantor of an “industrial revolution” considering the magnitudes of
the industry. Colombia has just opened its ﬁrst public-private research and innovation
center at IoT, the Center for Excellence and Internet Acquisition of Things in which ﬁve
national universities, eight national companies, three multinational technology com-
panies (Intel, Hewlett-Packard Enterprise and Microsoft), Colciencias and the ICT
Ministry to join forces to create developments in this technological ﬁeld [23].
The business sector agrees that successful organizations are those that manage their
digital universe to analyze behaviors, based on real data and using the concepts of
“cloud computing”, with which they can quickly access the information to make
decisions [24] Which is basically the newness of Industry 4.0. While the state holds
that technology is at hand, and that the way to use it has to be changed, it must pass
from e-government to digital government. The big problem that Colombia has is that
private companies do not use the technology properly, they think it’s just having a web
page, but it is transforming business model. Then, companies do not innovate using
280
J. I. Rodríguez-Molano et al.

technology, because technology is the tool not the goal [25]. This concludes that much
remains to be done, recognizing the progress made.
The challenge then is to understand that the development of Colombian industry
towards a more intelligent and automated industry is everyone’s task and that the way
to achieve that is integrating government, companies and the civil population in a
perfectly related network, in which government must create suitable and optimum
conditions to ensure industry’s sustainability and support the entrepreneurship char-
acterized for its innovation and technology leverage to create value, all that in order that
government improves life’s quality (allowing access to education for the entire pop-
ulation). Finally, dare of citizens and companies is to be ethical regarding the good
acting and committing to generate shared value to society, realizing that ICT are not a
luxury but a necessity to remain the market.
5.2
IoT Platforms Used in Colombia
As goal to accomplish the objectives of the study, a survey was made for thirteen
(13) companies, both private and public, with activity in the country and most of them
enterprises with Industry 4.0 platforms, for glimpsing the general overview of the real
position of Colombia as named previously described. Companies consulted cover ﬁelds
as food production, manufacturing, mining, health, entertainment, services, sales,
marketing, and collection and control of customs and taxes. Of total respondents, it was
found that only 23% of them do not use any platform to analyze data’s company,
although those recognize Industry 4.0 concept, while 47% assert to use two or more
platforms simultaneously in different business areas.
Hence it is necessary to identify currently used platforms by the country companies
in order to verify top platforms presence described before, their versatility and the ﬁeld
of application (large or small scale within the internal structure of business).
At this point and taking into account the percentage of companies that use any data
analysis platform, 77% of the sample claimed to use collection and analysis of data
platforms with solid implementations, but that of these implementations only 30.7%
correspond to any of the platforms named in this review, reason why each company
was asked remaining 69.3%, the platform used by them, aiming to cover a greater
depth regarding the type of platform, its characterization and why these platforms were
preferred in comparison to top platforms of this paper.
Nevertheless, 86% of these platforms are non-license, buffering ﬁnancial expenses
of paying a license periodically, according with data processing type preferred by each
company. From survey results, there can be distinguished tendency of the companies to
understand Industry 4.0 platforms are a simple support for processes, rather than a tool
of decision making at managerial levels.
As mentioned, 30.7% of the 13 companies surveyed, that reported the use of the top
named platforms return the next values, from ﬁve (5) of those companies just three
(3) of them use two of the platforms.
Industry Knowledge Management Model 4.0
281

5.3
Implementation Barriers
Although it was previously said that at Latin American level the country stands out for
the gradual reception of the most innovative business theories, not only in the case of
platforms of Industry 4.0, but also of Big Data and business intelligence, among others
[26]. Country evolution towards these tools of innovation and competitiveness is
constrained by the high economic licenses’ costs of the best products (according with
the ranking on global statistics [27], and change resistance of which organizations are
victims in adaptation processes [28].
6
Concluding Remarks
It is important to mention that companies that are leveraging their business model with
this kind of tool are B2B (Business to business) companies, those are too recent
creation and are focused in generating value with services that improve the manage-
ment indicators of large business companies. Those are characterized because their
founders are young people, who have mostly worked in large companies and have seen
their speciﬁc needs, making the decision to solve those needs by building service.
Industrial revolutions throughout history have been characterized by a support,
even late from the state for its total implementation, Colombian case of Industry 4.0
requires a harmonized ensemble of public sector headed by the state, businessmen and
academy.
Commitment of the companies in investments related to Industry 4.0 platforms in
the country should be understood as company’s evolution and the increase of the value
added to all the products, not only as a fashion industry or a mere global trend.
Open source tools are especially beneﬁcial in Latin America because those are open
projects, compatible with open data sources and attractive cost/beneﬁt. On the other
hand, licensed tools outstrip security platform correlation themes, technologies and
algorithm capabilities to perform data analysis associated with security of technology
platforms.
Dissemination, exhibition, consultation and research of technological tools that
help to the industrial development in general, are necessary to achieve the same
development, this article is a sample and an attempt of it.
References
1. Botia, D., Patiño, R., Ospina, E., et al.: Implementación de un Sistema Domótico Basado en
una Plataforma de Internet de las Cosas. Décima Quinta Conferencia Iberoamericana en
Sistemas. Cibernética e Informática (2016)
2. Devia, M., Gutiérrez, T.M.: Inteligencia de Negocios y su relación con la cultura
Colombiana, Bogotá, pp. 1–8 (2016)
3. Rivoir, A.: La Sociedad de la Información y el Conocimiento hacia: un paradigma complejo.
Plan CEIBAL – MEC, pp. 1–4 (2015)
4. Huba, M., Kozak, S.: From E-learning to Industry 4.0. IEEE Xplore (2014)
282
J. I. Rodríguez-Molano et al.

5. Cheng, G., Liu, L., Qiang, X., Liu, Y.: Industry 4.0 development and application of
intelligent manufacturing. In: International Conference on Information System and Artiﬁcial
Intelligence (2016)
6. Baygin, M., Yetis, H., Karakose, M., et al.: An Effect Analysis of Industry 4.0 to Higher.
IEEE Xplore (2016)
7. Everlet, A., Pastor, J.: Introducción al internet de las cosas (2013). https://www.carriots.com/
newFrontend/img-carriots/press_room/Construyendo_un_proyecto_de_IOT.pdf
8. Xinhua, H., Jing, W., Yasong, L.: Big Data-as-a-Service: Deﬁnition and Architecture (2013).
http://ieeexplore.ieee.org.bdigital.udistrital.edu.co:8080/stamp/stamp.jsp?arnumber=
6820472
9. Zdnet: Big Data. Zdnet (2017)
10. Shamim, S., Cang, S., Yu, H., et al.: Management Approaches for Industry 4.0. A human
resource management perspective (2016)
11. Splunk: Splunk (2017). https://www.splunk.com/es_es/solutions/solution-areas.html
12. Anella: Anella (2017). http://anellaindustrial.cat/portfolio/esi/
13. Xompass: Xompass (2017). http://www.xompass.com/)
14. Microsoft: Microsoft Azure (2017). https://azure.microsoft.com/es-es/overview/what-is-
azure/
15. IBM: IBM Plataform (2017). https://www.ibm.com/internet-of-things/platform/watson-iot-
platform
16. Lasi, H., Fettke, P., Kemper, H., et al.: Industry 4.0. Bus. Inf. Syst. Eng. 64 (2014)
17. Astarloa, A., Bidarte, U., Jimenez, J., et al.: Intelligent gateway for Industry 4.0-compliant
production lines. Euskal Herriko Unibertsitatea (2016)
18. Kovar, J., Mouralova, K., Ksica, F., Kroupa, J., et al.: Virtual Reality in Context of Industry
4.0. Proposed Projects at Brno University of Technology (2017)
19. Wübbeke, J., Conrad, B.: MERICS (2017). http://www.merics.org/ﬁleadmin/templates/
download/china-monitor/China_Monitor_No_23_en.pdf
20. Gobierno de España: La Transformación Digital de la Industria Española. Ministerio de
Industria, Energía y Turismo (2014)
21. Global Open Data Index: Tracking the state of government open data (2017). http://index.
okfn.org
22. Corporación Colombia Digital: Estrategias para transformar los municipios en entornos más
inteligentes, seguros y sostenibles (2017). https://colombiadigital.net/actualidad/noticias/
item/9422-estrategias-para-transformar-los-municipios-en-entornos-mas-inteligentes-
seguros-y-sostenibles.html.23
23. Gobierno de la Republica de Colombia: Internet of Thing (2017). http://www.mintic.gov.co/
portal/604/w3-article-6165.html
24. Oracle: Oracle Latinoamérica (2017). https://www.oracle.com/lad/corporate/contact/index.
html
25. Nokia: Explore best practices of real-life smart city IoT applications (2017). https://pages.
nokia.com/2170.What.Are.Cities.Doing.to.Be.Smart.html
26. Martínez, J.: La inteligencia de negocios como herramienta para la toma de decisiones
estratégicas en las empresas. Análisis de su aplicabilidad en el contexto. Universidad
Nacional de Colombia (2010)
27. Capterra: Top Big Data Software Products (2017). http://www.capterra.com/big-data-
software/
28. Lueth, L., Patsioura, C., Williams, D., et al.: The current state of data analytics usage in
industrial companies. In: Industrial Analytics, pp. 38–49 (2016)
Industry Knowledge Management Model 4.0
283

Smartphone-Based Vehicle
Emission Estimation
M. Cerón, M. Fernández-Carmona, C. Urdiales(&), and F. Sandoval
ISIS Group, ETSI Telecomunicación, Universidad de Málaga,
29071 Málaga, Spain
{acurdiales,Sandoval}@uma.es
Abstract. Intelligent Transportation Systems are expected to contribute to
reduce vehicle emissions. Solutions include user behavior changes, which are
typically vehicle sharing programs and driving habits changes. To change
driving habits, people need personalized feedback on their vehicle emissions in
the routes they complete. In this work, we present a smartphone based system
that returns a geopositioned emission estimation on the ﬂy. Our estimator is
based on the ARTEMIS project results. The system has been successfully tested
in different routes in Malaga (Spain) in highway, rural and urban areas.
Keywords: Smartphone application  Intelligent Transportation Systems
Driving habits  Vehicle emission
1
Introduction
Recent studies have stated that transport related emissions are responsible for a sig-
niﬁcant share of environmental pollution and greenhouse gas (GHG) [9]. Furthermore,
half of all road transport emissions are the result of trafﬁc in urban areas. Motorized
private transport accounts for 40% of the GHG emissions of the total road transport
sector and up to 70% of other pollutants stemming from transport. The ill effects of
pollution on health are well reported: up to 1.3 millions deaths each year, specially
involving children and elderly people.
While a large number of studies contemplate the impact of Intelligent Trans-
portation Systems (ITS) on areas like road safety, trafﬁc management or intelligent
vehicles, only a small number address the potential of ITS for reducing GHG emissions
in qualitative or quantitative terms, so there is still lack of consolidated empirical
evidence on the subject [4]. Developed countries should reduce their emissions by 60–
80% over 1990–2050 and ITS are meant to help to this respect. Improved logistics -e.g.
in-cab communication systems, [16] vehicle tracking systems, sat-nav, warehouse, eet
management- could result in a 16% reduction in transport emissions and a 27%
reduction in storage emissions. It has been reported that ITS-driven applications across
logistics could achieve a reduction in total global emissions of 1.52 Gt CO2 [17].
However, private transport is harder to address. Most works on the subject either focus
on technical changes -e.g. cleaner cars or smart trafﬁc management- or behavioral
changes -e.g. increased use of public transportation or car sharing programs- to cope
with the problem, although it has been suggested that the best choice would be a
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_28

combination of both approaches [5]: although major reductions in GHG emissions will
depend on cleaner cars and fuels, without behavioral change, the increase in CO2
emissions from expected travel growth would outweigh the possible savings from
changes in technology [18].
Urban trafﬁc management and control (UTMC) approaches rely on cameras and
sensors to control access to jammed areas and track ﬂows of vehicles. UTMC is
typically enforced by law, but there are personalized approaches to the problem. These
approaches typically rely on advising the driver on which routes to use in order to avoid
trafﬁc jams [12], e.g. ASSIST-V (BMV). Much research on this area focuses on
communication protocols, like DAB (Digital Audio Broadcasting) or TPEG (Transport
Protocol Experts Group), and also on city-wide mesh sensor networks (e.g. Crossbow).
However, pollution is not limited to trafﬁc jams. It would be interesting to detect
high-emission driving habits under far more frequent, non-excepcional situations.
Car and bike sharing programs and Demand Management Systems (DMS) conform
a well-known approach to behavioral change [13]. However, these approaches only
provide information on trafﬁc as a whole, not personalized feed-back for users to
quantitatively estimate potential beneﬁt for society and for themselves, e,g. fuel saving,
pollution reduction, etc. Information on the impact of these approaches is only avail-
able after a signiﬁcant time period and typically gathered by means of (potentially
subjective) questionnaires [14].
In order to obtain immediate feedback on how a given user is driving, cars need to
include on board sensors, processing unit(s) and some kind of human computer
interface (HCI). Thus, fuel consumption and/or emissions can be monitored and
feedback can be used to optimize driving style and vehicle behavior. There are several
approaches to acquire instant driving information from users -e.g. pressure on breaks or
face gestures- in order to provide assistance to the driver when help is needed [11], but
they are mostly focused on safety issues.
The main goals of the present work are: (i) to study the dynamic impact of driver
behavior and chosen routes on emissions; and (ii) to provide feedback for behavior
correction. This goal can be decomposed into the following ones:
– To experimentally validate theoretical data provided by ofﬁcial reports using
vehicle on board sensors.
– To develop a methodology to automatically detect highly pollutant driving
behaviors and routes.
– To provide personal feedback to drivers so that they can change their driving habits
if necessary.
We propose to use a smartphone to support the whole system, as it has been
reported that smartphones will most likely be the most crucial tool in the next decade to
motivate behavior changes [6]. It needs to be noted that no common framework
architecture for ITS systems exists, so maximizing the potential of ITS for reducing
emissions will depend signiﬁcantly on interoperability. Current cars are equipped with
OBD (On Board Diagnosis) systems that provide information about a number of
vehicle parameters. Originally, it was not easy to access a car OBD, specially on the y.
However, nowadays there are electronic systems that tap into the OBD and return
information via standard communication protocols. Furthermore, these devices have
Smartphone-Based Vehicle Emission Estimation
285

become progressively cheaper. Speciﬁcally, we will use a ELM327 V1.4 B g, which
extracts the OBD information via Bluetooth (BT) and, hence, easy to capture using any
existing smartphone. Additionally, the smartphone GPS will be used to extract the
vehicle location and geoposition all data. The mobile phone will analyze the sensor
information, plot it using a GIS -in our case, Google Map- and offer feedback to users.
All these data will be used to validate a general existing emission model and obtain all
estimations for the user behavior and route on the go. It can be noted that this approach
follows the four main trajectories deﬁned for the next generation of ICT:
– Networked, mobile, seamless and scalable, offering the capability to be always best
connected anytime, anywhere and to anything.
– Embedded into the things of everyday life in a way that is either invisible to the user
or brings new form-ﬁtting solutions.
– Intelligent and personalized, and therefore more centered on the user and their
needs.
– Rich in content and experiences and in visual and multimodal interaction.
2
Estimation of Emissions: The ARTEMIS Project
There are two main approaches to estimate how much a vehicle pollutes: (i) to carry
on-board Portable Emissions Measurement Systems (PEMS) [7]; or (ii) to estimate the
emissions from related parameters via system modelling [8]. PEMS based analysis is
usually more reliable, but it requires a very specialized hardware that limits its prac-
ticality for general application. Modeling techniques can be either statistically-based [1]
or model-based [8, 10, 15]. Statistical approaches require ﬁtting of an analytical model,
so they may be affected by errors derived from ill data ﬁt. Modal based approaches are
not affected by these errors, but they involve a signiﬁcant amount of data averaging
and, hence, they might not be adequate for micro scale applications or individual
vehicles analysis. This problem is (partially) solved by introducing simpliﬁed physical
models that extrapolate required data from additional parameters. For example,
vehicle-speciﬁc power (VSP) -a function of speed, acceleration, and road grade- is
usually a good predictor of vehicle fuel use. Some of these simple models can be
derived from large scale experiments and used later at smaller scale tests. In this work,
we rely on this third approach, based on the results of project ARTEMIS.
Project ARTEMIS [3] gathered a data base of 2800 cars and 27000 emission
calculations to derive models on emissions in Europe roads depending on different
driving environment. There have been previous efforts in the ﬁeld, like CORIANAI,
COPERT (I-IV), EMET, MODEM, COST and MEET. ARTEMIS is based on driving
cycles, which correspond to distinct driving behavior.
2.1
Deﬁning a Driving Cycle
A (new) driving cycle is deﬁned each time a vehicle changes from a speciﬁc driving
environment to another, e.g. urban to rural areas. Speciﬁcally, ARTEMIS considers 3
kind of cycles: urban driving, rural driving and motorway driving, which are deﬁned
according to vehicle speed and stop durations [2].
286
M. Cerón et al.

Although somewhat arbitrary, this deﬁnition returned quite homogeneous cycle
groups in ARTEMIS tests, where urban trips, rural trips and motorway trips reportedly
had average stop durations of 28, 10 and 5% of total trip time and driving speeds of 30,
55 and 98 km/h, respectively.
Cycles can be further divided into sub cycles depending on trafﬁc conditions.
ARTEMIS detected 12 subcycles (Table 1). Given this classiﬁcation, driving cycles in
a running experiment can be automatically labelled.
Depending on the driving cycle and whether the vehicle is a diesel or a petrol
model, emissions are affected by a different set of parameters [3].
Diesel Vehicle
– Urban cycle: (i) all pollutes increase with strong acceleration -average, frequency
and acceleration time-; (ii) HC and CO increase at high speeds (60–100 km/h) and
strong accelerations; (iii) HC grows with the number of stops and CO2 decreases at
higher speeds.
Table 1. Classiﬁcation of driving cycles (ARTEMIS project)
Cycle of driving conditions % of
total
mileage
Running
speed
(km/h)
Average
speed
(km/h)
Stop
duration
(%)
Stop
rate
(stp/km)
Aver.
positive
accel. (m/s2)
1
Congested
urban
High stop
duration
3.7
25.9
10.2
60.8
3.9
0.87
2
5.9
23.6
15.9
32.7
3
0.81
3
Low
steady
speeds
2.4
16.5
13.2
19.5
3.4
0.67
4
Free-ﬂow
urban
5.1
28
26.1
6.7
0.97
0.65
5
Unsteady
speeds
12.2
35.6
32.3
9.1
0.98
0.81
6
Secondary
roads
Unsteady
speeds
10.8
52.2
48.8
6.6
0.41
0.75
7
8.8
45.5
43.8
3.7
0.39
0.63
8
Steady
speeds
7.2
65
64
1.5
0.15
0.55
9
Main roads Unsteady
speeds
11.8
75
72.5
3.3
0.15
0.67
10
6.2
86.1
85.7
0.4
0.04
0.48
11 Motorways Unsteady
speeds
10.4
115.6
114.9
0.7
0.03
0.53
12
15.6
123.8
123.7
0.1
0.01
0.4
Smartphone-Based Vehicle Emission Estimation
287

– Rural cycle: (ii) all pollutes grow with acceleration -average, frequency and
acceleration time-; CO2, HC and NOx grow with the frequency and duration of
stops; CO2 and NOx decrease when speed grows.
– Highway cycle: (i) all pollutes grow with accelerations at high speeds (120–
140 km/h).
Petrol Vehicle
– Urban cycle: (i) all pollutes increase with stop frequency and duration; (ii) all
pollutes but CO decrease when speed grows; CO grows at high speed (60–
100 km/h); (iii) NOx and CO2 grow with stop frequency and acceleration.
– Rural cycle: (ii) all pollutes increase with stop frequency and duration; (ii) all
pollutes decrease when speed grows and increase for low speeds (20–40 km/h) and
positive accelerations; CO is sensitive to large accelerations/decelerations.
– Highway cycle: (i) NOx and CO2 grow at high speeds (120–140 km/h) and
acceleration and decrease at medium speed (60–100 km/h); (ii) CO grows at
medium/low speeds and also with stop frequency and acceleration; it decreases at
low speeds.
In brief, pollutes depend on speed, acceleration, number and duration of stops. CO
follows a pattern different from the rest of the pollutants, that loosely depend on the
same factors and behave similarly. The estimators for each pollutant, cycle and vehicle
type are fully reported in [3]. They depend on driving behavior, so we can not estimate
emissions in given route can be unless the driving conditions and driver behavior are
known. We propose to extract these parameters on the y using a smartphone and then
use gathered data to estimate emissions and validate the estimation using OBD
information.
2.2
Parameter Acquisition
The ARTEMIS project has been chosen for this study because all required input
parameters can be directly obtained on the y using a smartphone in a vehicle.
Speciﬁcally, we need to obtain the following parameters for each detected driving
cycle: (i) total distance; (ii) number of stops and stop rate; and (iii) number of (sig-
niﬁcant) acceleration per km. Distance (D) is obtained as the sum of distances between
consecutive readings and vehicle acceleration can be approximated as:
acceleration ¼ Dspeed
Dt
ð1Þ
3
The OBD System: Integration with a Smartphone
Major car manufacturers and government agencies report that Ecodriving could lead to
signiﬁcant reductions in emissions and fuel saving4. Recommendations include actions
like Maintaining a Steady Speed Using Highest Gear Possible, Decelerating Smoothly,
Shifting to a Higher Gear as Soon as Possible, etc. Many drivers are not aware that they
288
M. Cerón et al.

do not drive efﬁciently. Hence, personalized feedback is important to learn about bad
habits and consider on greener alternatives to everyday routes. Eco-driving parameters
may be extracted from the OBD and geopositioned using a smartphone. We will use
the OBD to check the ARTEMIS estimations match Eco-driving guidelines.
Traditionally, OBDs could be accessed via an appropriate bus connector, usually by
specialized staff. In this work we use a basic Android smartphone (with GPS and
Bluetooth) and a ELM327 Bluetooth Version 1.4 (communication mode 1) to abstract
the low-level protocol via a UART. ELM327 privides: (i) fully supports every OBDII
protocol; (ii) presents a low power consumption mode; (ii) it is widely available; and
(iv) it is very affordable.
Most OBDs deliver parameters like Engine load (%), Engine Temperature (oC),
Fuel Pressure (KPa), Engine RPM (rpm), Vehicle speed (km/h), Air Intake Temper-
ature (oC), Mass Air Flow (MAF) or Ambient Temperature (oC). Unfortunately, several
parameters of interest are often not provided by every OBD, e.g. Air/Fuel ratio (A/F)
(%), Fuel Level Intake or Fuel Consumption (Km/l). Emissions are related to vehicle
fuel consumption, that we can estimate using vehicle speed and MAF. We need the A/F
as well for our calculation. Since this parameter is not provided by most OBD, we have
to assume that in modern vehicles the proportion is close to ideal, i.e. the ratio is close
to 1. Finally, we approximate fuel density (FD) as 680 g/l and 850 g/l for petrol and
diesel motors respectively. Then, fuel consumption can be approximated as:
Fuelcons ¼ A=F  14:7  FD  speed
MAF  3600
ð2Þ
4
Experiments and Results
In this section, we will compare emissions for di_erent routes joining the same starting
and destination point in Malaga (Spain). All data in trajectories in this section has been
gathered by different drivers from real routes in Malaga. Every route may include
different driving cycles. Trafﬁc conditions, i.e. subcycles, depend on the hour and date.
For fairness, in this section we have chosen three routes across the city that include all 3
types of cycles (Fig. 1). All presented results correspond to the same user, i.e. same
driving habits, and routes selected presented similar trafﬁc conditions.
Figure 2a shows the different cycles detected in all three routes. As commented,
these routes were completed by same driver under similar trafﬁc conditions, so that
driving behavior related to habits are the same in all three cases and, hence, differences
mostly depend on routes. It can be observed that every cycle deﬁned in ARTEMIS is
included at least in one route. As expected, speed is higher in highway routes and there
are less stops, that are more frequent in urban cycles. Routes A, B and C in these tests
took 12.3, 15.8 min and 21.6 min, with 58.3%, 69.6% and 100% of urban cycles,
respectively.
Figure 2b shows the acceleration in routes A, B and C. Routes B and C resent more
variations and these variations have larger magnitude than route A. Furthermore, it can
be observed that these plots are correlated with the RPM changes in Fig. 2a, i.e. larger
Smartphone-Based Vehicle Emission Estimation
289

accelerations match large rpm changes. This was to be expected: in urban cycles,
drivers need to change gears more frequently, whereas in highway cycles, speed is
consistently larger. In these areas -mostly in urban and rural cycles- the motor load
grows and, consequently, fuel consumption is larger. This expectation can be checked
by estimating fuel consumption for each route using the OBD and Eq. 2. Results are
presented in Fig. 2b It can be observed that plots for urban and rural cycles present
larger variations, which reportedly lead to higher emissions. Indeed, according to
Eco-driving advice (Sect. 3), route A should be the least pollutant. This expectation is
going to be checked using the ARTEMIS estimators.
ARTEMIS requires the number of stops and accelerations per cycle (Tables 2 and 3).
No stops were detected in highway nor rural cycles. Urban cycle 1 has the larger number of
stops in routes A and B, probably because in both cases it is close to the city center. Route B
has the highest number of accelerations per km in all urban cycles, possibly because it
follows the main city road from east to west (dense trafﬁc and many trafﬁc lights).
Fig. 1. Test routes from origin to destination
Fig. 2. OBD parameters in routers A, B and C: (a) RPM; (b) estimated fuel consumption
290
M. Cerón et al.

Our geopositioned data is stored in KML for visualization. The height of each
location corresponds to the vehicle speed and color represents emissions (green = low
emissions; red = high emissions). Since all emissions except CO present similar
behavior, we only present results for CO in Fig. 3a and for other pollutants in Fig. 3b.
In both cases, route C is consistently the most polluting one.
Table 2. Stops per km in routes A, B and C
Urban cycle 1 Urban cycle 2
Route A 5.2 stops/km
2.07 stops/km
Route B 4.63 stops/km 1.64 stops/km
Route C 2.72 stops/km —
Table 3. Accelerations per km in routes A, B and C
Urban cycle 1 Highway cycle Rural cycle Urban cycle 2
Route A 7.2 acc/km
0.41 acc/km
—
4.1 acc/km
Route B 14.41 acc/km
—
1.5 acc/km
13.5 acc/km
Route C 12.55 acc/km
—
—
—
Fig. 3. Emissions in routes A, B and C: (a) CO; (b) others
Smartphone-Based Vehicle Emission Estimation
291

Results on routes A and B depend on the type of emission: route A is the least
pollutant for CO -specially in the highway cycle-, whereas route B is greener for the
other pollutants. Results are coherent with our ARTEMIS-based estimation. In this test,
behavior analysis points out that the driver most pollutant factor is a tendency to
accelerate/decelerate sharply. Hence, it would be advisable to use highway cycles as
much as possible.
5
Conclusions and Future Work
This work has presented a smartphone application to estimate emissions on the y based
on the ARTEMIS methodology [3]. It visually offers personalized advice on greener
driving behaviors are routes to adapt driving habits if necessary. The system was tested
in Malaga under different trafﬁc conditions. We validated estimations using OBD
parameters. We checked that behaviors like frequent changes in acceleration, using low
gears or a high number of stops corresponded to the areas of largest emissions pre-
dicted by the application. As expected, urban cycles typically present the highest
emissions.
Future work will focus on learning the most pollutant habits and visited city spots
for each speciﬁc user to propose greener routes a priori. To do so, we plan to produce
personalized “emission” maps, annotated with which kind of behavior and what
emission costs can be expected at different locations.
Acknowledgements. This work has been supported by the Spanish Ministerio de Educacion y
Ciencia (MEC), Project n. TEC2014-56256-C2-1-P and Universidad de Malaga.
References
1. Ahn, K., Rakha, H., Trani, A., Van Aerde, M.: Estimating vehicle fuel consumption and
emissions based on instantaneous speed and acceleration levels. J. Transp. Eng. 128(2), 182–
190 (2002)
2. André, M.: The ARTEMIS European driving cycles for measuring car pollutant emissions.
Sci. Total Environ. 334–335, 73–84 (2004)
3. André, M., Keller, M., Sjödin, Å., Gadrat, M.: The Artemis European tools for estimating the
transport pollutant emissions. In: Proceedings of 18th International Emission Inventories
Conference, vol. 1, p. 10 (2009)
4. Bell, M.C.: Environmental factors in intelligent transport systems. In: IEEE Proceedings of
Intelligent Transport Systems, vol. 153, pp. 113–128 (2006)
5. DG Enterprise & Industry European Commision. The potential of Intelligent Transport
Systems for reducing road transport related greenhouse gas emissions. Technical Report
December, European Commission, DG Enterprise & Industry (2009)
6. Fogg, B.J., Eckles, D.: Mobile Persuasion: 20 Perspectives on the Future of Behavior
Change. Stanford Captology Media, Stanford (2007)
7. Frey, H.C., Zhang, K., Rouphail, N.M.: Vehicle-speciﬁc emissions modeling based upon
on-road measurements. Environ. Sci. Technol. 44(9), 3594–3600 (2010)
8. Joumard, R., Rapone, M., Andre, M.: Analysis of the cars pollutant emissions as regards
driving cycles and kinematic parameters (2006)
292
M. Cerón et al.

9. Lutsey, N., Sperling, D.: Greenhouse gas mitigation supply curve for the United States for
transport versus other sectors. Transp. Res. Part D Transp. Environ. 14(3), 222–229 (2009)
10. U.S. Environmental Protection Agency Ofﬁce of Transportation and Air Quality.
A Roadmap to MOVES 2004. Technical Report March (2005)
11. Rakotonirainy, A., Tay, R.S.: In-vehicle ambient intelligence transport systems: towards an
integrated research. In: 7th International IEEE Conference on Intelligent Transportation
Systems, pp. 648–651, Washingon DC, USA (2004)
12. Ridwan, M.: Fuzzy preference based trafﬁc assignment problem. Transp. Res. Part C Emerg.
Technol. 12(3–4), 209–233 (2004)
13. Shaheen, S.A., Mallery, M.A., Kingsley, K.J.: Personal vehicle sharing services in North
America. Res. Transp. Bus. Manag. 3, 71–81 (2012)
14. Susan, A., Shaheen, S., Martin, E.W., Finson, R.S.: Ecodriving and carbon footprinting:
understanding how public education can reduce greenhouse gas emissions and fuel use.
Technical report (2012)
15. Silva, C.M., Farias, T.L., Frey, H.C., Rouphail, N.M.: Evaluation of numerical models for
simulation of real-world hot-stabilized fuel consumption and emissions of gasoline
light-duty vehicles. Transp. Res. Part D Transp. Environ. 11(5), 377–385 (2006)
16. Review of evidence on health aspects of air pollution. REVIHAAP Project. World Health
Organization (WHO) Regional Ofﬁce for Europe (2013)
17. The Climate Group and the Global eSustainability Initiative (GeSI): SMART 2020: Enabling
the Low Carbon Economy in the Information Age (2008)
18. VIBAT, Visioning and Backcasting for UK Transport Policy, Bartlett School of Planning,
University College London. (http://www.vibat.org)
Smartphone-Based Vehicle Emission Estimation
293

An Information Visualization Engine
for Situational-Awareness in Health Insurance
Flávio Epifânio1(&) and Gabriel Pestana2
1 Future Healthcare, Lisbon, Portugal
ﬂavio.epifanio@future-healthcare.pt
2 INOV, Lisbon, Portugal
gabriel.pestana@inov.pt
Abstract. Nowadays the number of chronically idled patients has increased
greatly because of unhealthy behaviors. Consequently, healthcare costs related
to chronicle medical conditions have also increased. Health insurance players
face new challenges on how to prevent disease burdens and how to contribute
for a better quality of life and wellbeing for chronic patients. Keeping these
patients aware about events requiring their immediate attention assumes there-
fore a key relevance, contributing simultaneously to promote an active partici-
pation of healthcare professionals in monitoring the health status and wellbeing
of each chronic patient. This paper presents an information visualization engine
for situational-awareness as a way to proactively monitor unhealthy risk
behaviors. A metadata model was designed following a user-centric approach
and addressing information visualization requirements to streamline the visual
representation of events in a collaborative decision making environment. The
paper presents the outcomes of a prototype conﬁgured to demonstrate how the
proposed model can be applied in health insurance ﬁeld.
Keywords: Information visualization  Chronic patients  Risk indicators
Gamiﬁcation  Spatio-temporal data-sensemaking  Situational-awareness
1
Introduction
Today, modern sedentary lives and people’s unhealthy behaviors are directly related
with health risk factors, such as high blood pressure, sedentary lifestyle, tobacco
consumption, obesity and high cholesterol, are responsible for the increased risk of
morbidity and chronic diseases [1]. Accordingly to the World Health Organization, in
2015, 70% of the overall total deaths occurred due to chronic diseases; it is also
estimated that two-thirds of current healthcare cost’s increase are related to chronic
diseases [2].
Such unhealthy behaviors represent a high risk to healthcare players, such as health
insurance providers. In this domain, providers need to address unhealthy behaviors as a
risk factor that needs to be monitored to prevent disease burdens, improve quality of
life and wellbeing [3].
The challenge faced by health insurance programs is to be aware whenever the
insured person presents a deviant behavior. The use of non-invasive wearable devices
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_29

enabled the introduction of innovative and low-cost monitoring mechanisms,
addressing the surveillance of unhealthy behaviors, and simultaneously acting as a
self-awareness mechanism to mitigate the risk of adverse clinical situations. Peoples’
daily activity can easily be tracked and automatically analyze incoming data in
real-time.
When a suspicious situation is detected, the system triggers an event to inform all
intervenient actors (based on the level-of-concern, i.e., event severity), creating in this
way a collaborative dataﬂow to keep all actors well informed. Although, data analysis
processes are not covered in this paper, because it assumes that data are already
processed and are ready to be visualized, the addressed challenges consider how events
and data should be presented to the user. The approach is to use Information Visual-
ization principles as a way to streamline visual representation of critical data and in
optimizing human interventions, mitigating simultaneously the risk impact and con-
tributing to improve patient healthcare and wellbeing [4].
The Information Visualization ﬁeld has a relevant role in providing value to the
existing data where sensemaking ﬁts into the whole process of working with vast
volumes of datasets [5]. This is particularly relevant when the user relies on the system
situational-awareness capabilities to be aware about events requiring an immediate
attention. In this case, information has to be presented in an understandable way,
providing quick insights, and minimizing the user effort to interpret and understand the
reported information (e.g. triggering alert messages whenever a reported measurements
exceeds thresholds deﬁned based on the user clinical proﬁle). The study of clinical
indicators’ thresholds is outside of the scope of this work. It’s assumed that they were
deﬁned according to the client clinical proﬁle, health insurance program characteristics
and conditions agreed between the client and the health insurance provider.
Situational-awareness capabilities provide a way to keep the user well informed
about deviant behaviors, which need to be rectiﬁed to reduce the risk of events, or to
motivate the user by presenting rewards regarding healthy behaviors. Such motiva-
tional aspects can be managed using gamiﬁcation techniques [3]. On the other hand, for
healthcare professionals, data related with risky situations is associated to the
level-of-concern of patients’ health-outcomes (e.g. patient health status degradation or
non-fulﬁllment of prescribed objectives) [4].
The solution to support a context awareness and spatio-temporal data-sensemaking
in health insurance required the implementation of an iterative, user-centric design
process with an agile feedback cycle. The system behavior and information visual-
ization artifacts were conﬁgured based on the user proﬁle, empowering the user in
building personalized dashboards. These interactive dashboards support data integra-
tion, data aggregation, ﬁltering and searching capabilities, streamlining the exploration
and analysis of data, providing the user with the ability to repeatedly redeﬁne goals as
new information insight is acquired [6].
The contributions of this work consists in (1) modeling health insurance challenges
regarding chronic patients and health risk behaviors, (2) design of an information
visualization engine framework to promote situational-awareness in risk situations,
(3) and implementation of a prototype for the case-study.
An Information Visualization Engine for Situational-Awareness
295

The remainder of the paper is structured as follows. Section 2 presents a literature
review on the intersection between information visualization, situational-awareness and
gamiﬁcation research ﬁelds, taking in consideration the health insurance case-study.
Section 3 discusses the proposed information visualization engine data model and
architecture. Section 4 presents the case-study in health insurance, regarding a health
management program. Finally, in Sect. 5, conclusions are presented and future research
directions are pointed out.
2
Related Work
2.1
Information Visualization Framework
The research ﬁeld of Information Visualization provides informational artifacts
addressing a graphical representation of data in order to enhance human cognition [5].
It also contributes to streamline the detection of patterns, increasing perceptual infer-
ence and improving understanding of the data. When combine with other research
areas, such as gamiﬁcation, information visualization techniques has a relevant role in
providing value to the existing data, improving self-awareness and information inter-
action using, for instance, Dashboard viewers [7].
Interactive and personalized dashboards are an Information Visualization technique
to transform datasets into relevant information, supporting the decision making process
and dynamically triggering alerts [8]. In a dashboard, data are organized to be moni-
tored and visualized at a glance, making them the ideal communication mechanism for
situational-awareness [9]. Taking advantage of different representations (e.g. colors,
geometry, size) and visualization elements (e.g. charts, tables or indicators) the dash-
board can alert and point the user to those situations requiring an immediate attention,
improving the ability to take quick actions based on informed decisions [10].
In this domain, the end-user based on the user proﬁle and goals should have the
possibility to autonomously customize the Dashboard views or personalize the
graphical elements without requiring any programming skills [11]. To improve inter-
actions, the end-user should be able to operate with aggregations, ﬁlters and other
operators to search and drill-down to more granular data [4]. The schema presented in
Fig. 1 outlines the reference model used to build the Information Visualization solution
presented in this paper.
Based on the way users interpret and interact with visual elements in a Dashboard,
the reference model [7] presents a three step process to describe how users can con-
ﬁgure visual elements in a Dashboard to retrieve insights. (1) raw data is transformed
into structured data, tables representing data relations and attributes, for ease of data
manipulation, (2) data structures are then combined with visual elements and corre-
sponding (graphical) properties to build visual structures (3) render visualizations. Each
visual structure can serve one or more visualizations, allowing users to analyze data
from different perspectives and levels of detail.
296
F. Epifânio and G. Pestana

The reference model provided valuable inputs to the design of the domain model
associated to the information visualization engine proposed in this paper. Card et al.
model provides an understanding of the different stages and informational entities used
along the iterative process in transforming raw data into visual representations that are
relevant to the end-user. In this paper the information visualization guidelines are the
building-block to the speciﬁcation of a user-centric dashboard engine to empower the
end-user in conﬁguring and customizing the visual elements relevant for his/her
activities. Our work extends the reference model by merging gamiﬁcation techniques
with visual elements using data from the health insurance sector. Access to granular
data is restricted based on health insurance policies and by taking into account the role
and user proﬁle of the different actors.
2.2
Situational-Awareness and Gamiﬁcation
Situational-awareness addresses “knowing what is going on” providing a way to keep
the user aware of those situations requiring a rapid intervention. In the health insurance
sector, situational-awareness plays an important role [11]. This is particularly true in
insurance programs addressing the surveillance of behaviors of high-risk clients as a
way to reduce disease burdens (i.e. preventing adverse clinical situations) and simul-
taneously to promote a better quality of life and wellbeing (by reducing health risk
factors).
Supporting situational-awareness in complex domains requires the ability to present
different types of data in a comprehensive format addressing quick insights [4]. In
health insurance, when monitoring chronic patients, any awareness must be well
characterized based on the situation triggering an alert. This may include information
regarding the client health status (e.g. client’s biometric data), changes to the client
health factors (e.g. weight growth rate) or knowledge of the service quality (e.g. alert
notiﬁcation response rate). For instance, situational-awareness will inform a nutritionist
if a patient outcomes are in line with expected results (e.g. a visual dashboard with diet
indicators), or assist a client in accomplishing prescribed physical activities, with data
collected from wearable devices.
In this domain, the framework presented by Endsley [12] addresses a framework
model addressing situation awareness in three levels: (1) perception of relevant ele-
ments in the environment, (2) comprehension of the signiﬁcance of these elements
Fig. 1. Visualization reference model, adapted from [7].
An Information Visualization Engine for Situational-Awareness
297

regarding deﬁned goals, and (3) projection of future environment state providing
valuable information for decision making.
Using this model, Franklin et al. [4] conducted a research to design dashboard visu-
alizations in emergency departments or other clinical environments. They have shown that
user-centric dashboard visualizations design, supporting situation awareness, can improve
the decision-making process (and ultimately patient care) by increasing awareness, per-
ception and comprehension. Situation awareness, using colors and graphical symbols,
enabled healthcare professionals to be alerted for those situations requiring a rapid
intervention (e.g. reported measurements values above acceptable thresholds).
In a recent study [3], it was demonstrated that gamiﬁcation had a positive impact in
promoting physical activity, keep all intervenient actors well informed about abnormal
healthy or well-being situations. Gamiﬁcation uses game design elements in order to
increase user engagement to motivate desired behaviors, in non-gaming contexts,
products and services. According to [13], the design approach must start by analyzing
the ability to visualize information related to the “gamiﬁed” business, with a clear
categorization of information’s relevance, the deﬁnition of business metric’s thresholds
and the ability to identify achievements using a standard classiﬁcation.
In chronic patients, health indicators can be characterized by cost (e.g. hours
without exercise), beneﬁt (e.g. cholesterol) or on-target values (e.g. hearth rate, body
temperature) for control or risk assessment. Each type of indicators has a different
relevance based on the user’s proﬁle. The adoption of gamiﬁcation techniques, such as
recognition or rakings, can promote motivation sentiments enhancing self-awareness
regarding good behavior or achievement of intermediate goals; but for healthcare
professionals, indicators related with risk situations are more relevant. Therefore,
notiﬁcations must be conﬁgurable based on the user proﬁle/role.
Endsley [12] model is used as a guideline to address informational artifacts with
contextual information to expedite the decision making process. This spatio-temporal
dimension/context (i.e. when and where it occurred) supports the visualization of events
and provides the ability to draw users’ attention to relevant data (Endsley’s ﬁrst step -
perception step) using color, shape, or other visual attributes. It also provides a clear
sensemaking of relevant data by using indicators, thresholds and gamiﬁcation techniques
(e.g. risk behaviors or motivational targets) based on pre-deﬁned goals (Endsley’s second
step - comprehension step). Although, pure predictive analysis (Endsley’s third step -
projection step) is outside the scope of this paper, the design of informational artifacts
provides the understanding need to address what is likely to happen next.
2.3
Health Insurance Case Study
The growing demand for healthcare insurance, especially in the private sector, has
reached in 2016 a 1.3 trillion Euros in global revenues (and forecasted to double by
2025) [11]. However, the healthcare insurance market faces new challenges due to
changes in the insurance risk burden dimension. Singhal et al. [11] have analyzed a set
of healthcare risks, classifying them into eight categories: cheap routine care, pre-
ventive care, chronic care, catastrophic care due to chronic conditions, elective pro-
cedures, expensive discretionary care with no medical justiﬁcation, unpredictable
catastrophic care, and end-of-life care.
298
F. Epifânio and G. Pestana

The classic insurance business model was designed to cover unpredictable catas-
trophic expenses, providing a ﬁnancial protection to such situations. Yet, in United
States, classic insurance risk represents only 28% of health expenses [11], whereas the
largest spending reason in healthcare are derived from chronicle medical conditions [2].
The change in insurance risk burdens represents a high-risk to health insurance pro-
viders. According to the Portuguese Insurance and Pension Funds Supervisory
Authority (ASF)1, although pre-existing health conditions (e.g. chronicle diseases) of
the insured person (at contract’s issuance date) are covered by the health insurance, it is
a usual exclusion condition to ﬁgure in any health insurance contract. However, the
new insurance risk dimension represent an opportunity to develop innovative products
and services addressing consumer needs, not only providing access to best healthcare
and ﬁnancial protection, but also providing advice and guidance to prevent disease
burdens and promoting healthy behaviors.
We believe that information technologies can play here an important role. The use
of wearable devices enables clients to share day-to-day data providing health insurance
providers the means for customizing health management programs and monitor pro-
gresses, based on several factors, such as client’s proﬁle and health behaviors. Health
behaviors
surveillance,
supported
by
an
information
visualization
engine
for
situational-awareness enhancing risk-situations awareness enabling timely intervention
can provide a way to mitigate the risk of adverse clinical situations, prevent disease
burdens and promote client’s quality of life and wellbeing.
3
Information Visualization Engine
The main features of the proposed solution were designed to assure scalability and a
high level of conﬁgurability based on the user proﬁle (multi-user environment). This
means that spatio-temporal or situational awareness is mapped using graphical repre-
sentations of data (e.g. indicators) collected from heterogeneous data sources. Several
data connectivity mechanisms, such as REST, JDBC, MDX and CSV, support data
access. The assumption is that all incoming data are ready to be visualized, focusing on
how to present that information to better understand the status of the patient life quality
and well-being.
The design followed a user-centric approach, providing a solution to enhance
end-users’ ﬂexibility in the creation, customization and interaction with the informa-
tional artifacts presented at the dashboard elements (i.e. Tabs). Each artifact is
expressed as a set of customized content instances. Based on the user proﬁle/role the set
of information presented on a speciﬁc dashboard might change including the number of
events triggered to keep the user informed about those situations requiring an imme-
diate attention. Such level of awareness is mostly expressed though the conﬁguration of
Content/Indicators elements responsible to dynamically trigger notiﬁcations whenever
predeﬁned thresholds are exceeded. A dashboard can hold a set of different Content
Types, each one customized based on the user preferences and access level.
1 http://www.asf.com.pt/EN
An Information Visualization Engine for Situational-Awareness
299

3.1
Informational Artifacts of the System Domain Model
This section describes the design of the informational artifacts within the system data
model that are required to support the visualization of the data as well as to address the
system situational-awareness capabilities. The system is structured into six layers with
each layer encapsulating speciﬁc aspects of the information visualization engine. The
schema in Fig. 2 outlines this layered approach following a classical 3-tier architecture
model (i.e. presentation, business logic and data layer). The schema is complemented
with two additional layers encapsulating functionalities related to security and col-
laboration aspects. At the bottom, the schema presents a metadata layer responsible to
handle and manage the info-structure of data used to characterize some of the infor-
mational artifacts within the domain model. To streamline the association of the layers
in this schema with the informational artifacts in the system domain model, a color
code was adopted (see Fig. 3 for more details).
Metadata Layer. This layer addresses the characterization and management of the
informational artifact’s metadata in a centralized way for all reported artifacts. The
Entity artifact (see Fig. 3) encapsulates base attributes (e.g. entity version attribute) and
functionalities (e.g. adding a note to a Dashboard or to a Content is generalized by
adding a note to an Entity) responsible to characterize artifacts holding data from each
layer of the architecture model (Entity Type).
In Fig. 3, each Entity attribute is characterized by the association of a Metadata
description to a value. For example, an Indicator has a Metadata attribute named
“Type” that can have the ﬂawing values: “Beneﬁt”, “Cost” or “On-Target”, and a set of
Boolean attributes indicating if it is required, has no default value, isn’t read only, is
functional or userMd. These last two ones are particular relevant for customization
issues. When the userMd is set to true it means that the user can create and customize
existing metadata attributes, providing a ﬂexible and simple way to extend the metadata
model. When functional attribute is set to true the user is able to add functionalities to
the engine (e.g. to inject a value in a SQL query).
Each Metadata attribute can be assigned to a group, providing the ability to relate
them in different ways. The group relation is used by the information visualization
engine to address related ﬁelds in the same section/tab (e.g. in Fig. 4, adding a user
metadata to the “Weigh Evolution” content in the “Options” Metadata Group would
dynamically add that ﬁeld to the “Options” tab in the content conﬁguration view).
Fig. 2. High level overview of the model internal structure
300
F. Epifânio and G. Pestana

Data Layer. This layer addresses the informational artifacts required to manage data
connectivity and information gathering, providing the ability to retrieve and structure
data for visual mapping. The proposed model follows Card et al. [7] approach to build
structured data from raw data: DataSources provide data connectivity and access to
heterogeneous data, and DataItems provide structure to the source data in a set of
Columns with functionalities to ﬁlter and conﬁgure how to visualize the data to work
with. DataSources can be of four types: REST, JDBC, MDX and CSV and has different
metadata ﬁelds to characterizing them. The DataItem conﬁguration depends on the
DataSource type (e.g. a DataItem with a JDBC DataSource uses an SQL query to
selected data, but on a REST DataSource selects the root node). With data selected,
Columns are used to describe heterogeneous data in a structured way. For instance, to
build the contents presented in the case study, a JDBC DataSource was conﬁgured,
used to build a DataItem (see Fig. 6) that originated the creation of ﬁve Columns used
Fig. 3. Domain model of the information visualization engine
An Information Visualization Engine for Situational-Awareness
301

on further layers to build visualizations, such as the BMI metric column used to build
the Weigh Evolution chart and the BMI indicator (see Figs. 4 and 5).
Business Logic Layer. This layer addresses the management of informational artifacts
used to ﬁll the workspace of the tab elements in each conﬁgured dashboard. After
deﬁning Content’s connection to a speciﬁc GraphicalObject, a default visual structure is
presented (e.g. graphical mode or tabular mode). In the graphical mode the user can
select a set of multiple graphical elements (e.g. pie chart, donut chart, bar char, multi bar
chart, line chart, cumulative line chart, scatter chart, stacked area chart, indicator card,
combo box or list box). GraphicalObjects (visual components) offers an abstraction for
the implemented visualization mechanism (Angular-nvD32 in our prototype).
Using Content’s connection to a speciﬁc DataItem, a set of functionalities are
available to customize how the values of the data are going to be visualized. DataItems
and corresponding structural data components (i.e. Columns) are associated to Contents
using categories and series. Categories are a set of Columns used to deﬁne the “X-axis”
values and metric aggregation dimension. By deﬁning more than one category we can
conﬁgure a multi-value/level content. Series deﬁne the “Y-axis” values using a metric
(Column) and an aggregation function. The metric value is calculated based on the
aggregation function and the selected category.
Contents provide the conﬁguration of Thresholds and Alerts with a spatio-temporal
context using colors and shapes to draw users’ attention to relevant data and enhancing
information-awareness. A Threshold can raise Alerts and Notiﬁcations to “subscribing”
Users. Based on the Threshold values, events are generated and identiﬁed with different
symbols/colors.
Contents have a specialization named Indicator. The standard conﬁguration sup-
ports three types of indicators: cost, beneﬁt or on-target indicators. Base on the type of
indicator, each threshold interval should be deﬁned for the minimum and the maximum
acceptable values. For each value range an associated color is deﬁned (e.g. red, yellow,
green), providing a visual representation (i.e. color-code) to the values of the incoming
data. The access to these informational entities is based on the user proﬁle. Contents
also enable the use of gamiﬁcation techniques to support situational-awareness. For
example, the “Monthly Step Ranking” Indicator (content used as leaderboard/ranking)
and the “Weigh Evolution” Content (content providing evolution information) pre-
sented in the case study (see Figs. 4 and 5).
Presentation Layer. This layer addresses the informational artifacts required to create,
view and customize an interactive context dashboard. A dashboard provides a tab-based
structure where visualizations (ContentInstances) can be added and customized based
on the user preferences, empowering end-user’s customization and interactivity. The
separation between the Content (visual structure) deﬁnition and ContentInstance (view)
enhances reusability and collaborative work, since the same Content can have several
instances viewed by multiple users from different perspectives.
The ContentInstances in each tab can interact with the ContentInstances hold by
the same tab. Such tab-based approach combined with the user role/proﬁle provides a
2 http://krispo.github.io/angular-nvd3
302
F. Epifânio and G. Pestana

way to manage access to speciﬁc information on each tab, allowing users to analyze
and monitor data from different perspectives.
In our case study, the presented dashboard (in Figs. 4 and 5) have two different tabs
with access based on user role/proﬁle and where the same Content can be instantiated
with different conﬁgurations. For instance, another Wellbeing Staff (besides Albert)
could access the Wellbeing Metrics tab and set the Cholesterol Indicator to a line chart
evolution view (by activating Indicator’s “ShowHistory” metadata) without affecting
Albert’s view. Folders provide a tree list view of the informational entities created. The
listed information depends on user role/proﬁle.
Collaboration Layer. This Layer addresses the informational artifacts used to
enhance collaborative work, sharing information in the form of comments/annotations,
discussion threads/chats or ratings. The proposed model presents an ﬂexible solution in
collaborative work capabilities by providing the possibility of add Notes (with
Attachments) and Comments through all levels of the application, using different
informational entities, such as Dashboards, Tabs, Contents, Columns, and more.
Collaboration work is not only a capability with increased recognition in last year’s
[14] in analytic platforms, such as visual analytic dashboards, but also can improve
situational-awareness since it provides status of read (notes/comments) and ﬂagged
(notes), providing visual awareness of new comments (that may need intervention, for
example, a new medical prescription based on some indicator) or important/relevant
(ﬂagged) information enabling an memory aid. On the other hand, Ratings can be used
based on gamiﬁcation techniques to reward good behavior (e.g. physical trainer rating
5 starts to a weight evolution chart that the client will see in his dashboard).
Security Layer. This layer addresses the informational artifacts that instantiates the
user model, enabling user management and conﬁguration of access levels. The access
levels can be conﬁgured by user group/role or for speciﬁc users restricting access
privileges to different informational entities, such as dashboards, tabs, visualizations,
data structures, and more. To notice that the user informational entity, although cata-
loged in security layer, plays an important role in the presentation layer (providing the
ability to conﬁgure visualization instances content instances by user) and plays an
important role in the business logic layer (allowing the conﬁguration of different
notiﬁcations for different users).
3.2
Architecture Overview
Although this paper focuses on information visualization engine’s design model for
situational-awareness, a brief architecture overview is presented. Information visual-
ization engine follows a client-server architecture comprised of an AngularJS Java-
Script client, a Java backend server and a metadata info-structure database for engine’s
conﬁgurations. This architecture manages multi-user access based on the user mode
(user roles/proﬁles).
The server provides access to external data sources and manages the metadata
info-structure that supports the designed model using a created API. Also enables the
dispatch of external notiﬁcation alerts, based on thresholds and notiﬁcations’ conﬁg-
uration, providing an important contribution for situational-awareness.
An Information Visualization Engine for Situational-Awareness
303

The client addresses the creation of visual analytics dashboards for situational-
awareness following a layered approach with a clear separation of the responsibilities
and concerns between the application layers, providing modularity, ﬂexibility, and
testability to the implementation. Uses an event driven approach to enable
situational-awareness with informational entities representing data model state, related
with concepts like relevance or severity in a spatio-temporal dimension/context (when
and where it occurred) providing the ability to draw users’ attention to relevant data
through color, or shapes. The communication between server and client is supported by
a RESTful API with serialized and de-serialized JSON messages.
4
Case Study: Health Management Program at Future
Healthcare
In order to have a real case study to apply the information visualization engine for
situational-awareness in health insurance, we used a new health management program
(instantiated in Figs. 4 and 5) from Future Healthcare3 (FH), a company operating in
the health insurance domain. FH is a privately held international group, established in
2003, with headquarters in Lisbon and an ofﬁce in Zurich, focused in providing clients
with the best health, life and well-being conditions, through innovative health insurance
products and services enabling an easier access to private healthcare and well-being
Fig. 4. Dashboard layout for different user roles.
3 http://future-health.care/en/
304
F. Epifânio and G. Pestana

services. FH manages at Portugal a medical network with over 25.000 health providers,
including 80 private hospitals, over 3000 well-being providers (e.g. nutrition, senior
care or nursing) and over 20.000 medical doctors spread by dozens of specialties, such
as cardiology, pneumology or sports medicine.
The instantiated program (in Figs. 4 and 5) presents the layout regarding different
user roles, enabling each one to be notiﬁed about relevant events. The ﬁrst layout
instantiates the client user role, proving awareness about prescribed physical infor-
mation. The second layout represents the conﬁguration of a user role with detailed
biometric values for a speciﬁc client user. This layout models a customized health
management program addressing health insurance challenges, following a telemedicine
approach in monitoring chronic patients and health risk behaviors. The implemented
prototype makes use of an information visualization engine for situational-awareness,
enhancing user awareness in risk-situations and enabling user intervention as a way to
mitigate the risk of adverse clinical situations, prevent disease burdens and promote
customer’s quality of life and wellbeing.
Figures 4 and 5 are the visual outcomes of the health insurance scenario speciﬁed
to show the conﬁgurations performed by FH Supervisor to monitor the health behavior
of a persona. Scenario: John, a 55 years old male, diabetic (type I), stockbroker at a
multinational ﬁnancial company. John is also a smoker that likes sweet drinks, has been
experiencing some breathing difﬁculties lately. John’s biometric data are monitored by
Albert trough a telemedicine program. Albert is a health professional (a specialist
nurse) who will be aware of any event triggered by the engine regarding some risk
situation of John’s health.
Fig. 5. Dashboard layout after two weeks regarding the data reported in Fig. 4, corresponding to
a high level user awareness view.
An Information Visualization Engine for Situational-Awareness
305

Thetwo views presented inFig. 4 provide relevantinformation tomonitor and improve
John’s wellness. The ﬁrst layout represents the client’s (John) role view, composed by a
dashboard with two tabs and four content instances. The second layout is composed by a
dashboard with three tabs and six content instances, staff’s (Albert) role view.
Each indicator shows a measure/value (from the conﬁgured series), the indicator
type, speciﬁed goal (based on the value of the threshold conﬁguration with “Target
Threshold selected”) and a variation arrow (showing value variation). Values use
threshold’s color and the variation arrow makes use of green (beneﬁt) or red (cost)
color to express awareness (e.g. in Fig. 4, the “HbA1c”4 indicator shows that the
reported measurement should be improved, the yellow color outlines a shift from the
target goal and the red arrow that the tendency is negative because the measure is
increasing). The second layout shows a “Weigh Evolution” line chart with a
multi-value/level X-Axis’ values: year, week and weight. The “Weigh Evolution”
content conﬁguration is presented in Fig. 6 (snapshot at the center).
Fig. 6. Information visualization engine backofﬁce conﬁguration views of DataSources
(top) DataItems (bottom) and Contents (center).
4 HbA1c refers to glycated hemoglobin, which identiﬁes average plasma glucose concentration.
HbA1c provides a longer-term trend, similar to an average, of how high your blood sugar levels have
been over a period of time.
306
F. Epifânio and G. Pestana

When suspicious situations are detected (taking into consideration its level-of-
concern) the system triggers events to inform all intervenient stakeholders, creating a
collaborative community acting proactively to assure the person health status and
wellbeing. This alert is represented with a related symbols and colors accordingly with
the level-of concern.
For example, Fig. 4 shows a “regular” (level-of-concern) alert on “Daily Walk”
indicator through a yellow warning sign. This Indicator represents a prescribed daily
walk without stopping. An alert is generated due to the unfulﬁlled. Figure 5 shows a
“high” (level-of-concern) alert on “Hearth Rate” indicator using a red warning sign. In
the presented scenario, due to the high level-of-concern, this event generates an
immediate awareness to Albert, who is responsible to monitor John’s data.
In order to build the presented interactive dashboard, the informational entities used to
map raw data to visualizations (DataSource -> DataItem/Column -> Content ->
ContentInstance) must be created. The snapshot in Fig. 6 addresses the engine’s back-
ofﬁce conﬁguration views. On the top side is presented the Datasource, on the bottom the
DataItem and on center the Content conﬁguration views. The DataSource workspace
shows the connection conﬁguration to a database supporting the Health Management
Program and the DataItem workspace shows the conﬁguration of Weigh Metrics data
structure with ﬁve Columns, showing in particular, “BMI” Data Column with the asso-
ciated metadata ﬁelds. At the center is presented the “Weigh Evolution” Content
creation/customization workspace. Content’s workspace presents a preview of the current
conﬁgurations (providing a fast-feedback conﬁguration cycle) and two tabs to conﬁgure
Content visualization attributes and data selection/calculation using categories and Series
through DataItem’s Columns.
5
Conclusions
The paper outlines the main achievements in designing and implementing an infor-
mation visualization engine that uses gamiﬁcation techniques and visual artifacts to
streamline the way events are transmitted to the user. The engine was designed to be
used in any context, but for demonstration purposes, the system was conﬁgured to
consider requirements from the health insurance sector, in particular on how to prevent
disease burdens and promote client’s quality of life and wellbeing; interacting with
heterogeneous data sources to analyze the data and provide spacio-temporal
data-sensemaking; and conﬁgure the system to support a multichannel awareness
mechanism as a way to assure situational-awareness in health insurance.
Using the information visualization paradigm as guidelines, we presented an
expressive and complete metadata model to proactively monitor client’s unhealthy risk
behaviors, keeping the users aware about events requiring their immediate attention.
The end-user is empowered to conﬁgure and customize information artifacts to pro-
mote situational-awareness (e.g. health status and wellbeing monitor for unhealthy
behaviors) and streamline the way events and data are presented to the user in a
collaborative decision making environment, creating a global awareness contributing to
improve patient health status and wellbeing.
An Information Visualization Engine for Situational-Awareness
307

The results achieved are promising. Therefore, in terms of future work it is intended
to conduct a formal evaluation of the proposed information visualization engine within
the business context at Future Healthcare. Such study will integrate the company
strategy in providing an innovative health program. Moreover, we intend to address the
model limitations regarding embedding and dynamically publishing the dashboard
layout into external systems (e.g. webportals).
References
1. World Health Organization: World Health Statistics 2017: Monitoring Health for The SDGs
(2017)
2. Milani, R.V., Lavie, C.J.: Health care 2020: reengineering health care delivery to combat
chronic disease. Am. J. Med. 128(4), 337–343 (2015)
3. Johnson, D., Deterding, S., Kuhn, K.-A., Staneva, A., Stoyanov, S., Hides, L.: Gamiﬁcation
for health and wellbeing: a systematic review of the literature. Internet Interv. 6, 89–106
(2016)
4. Franklin, A., et al.: Dashboard visualizations: supporting real-time throughput decision-
making. J. Biomed. Inform. 71, 211–221 (2017)
5. Ware, C.: Information Visualization: Perception for Design. Elsevier (2012)
6. Smuts, M., Scholtz, B., Calitz, A.: Design guidelines for business intelligence tools for
novice users. In: Proceedings of 2015 Annual Research Conference South African Institute
of Computer Science and Information Technologists - SAICSIT 2015, pp. 1–15 (2015)
7. Card, S., Mackinlay, J., Shneiderman, B.: Information visualization. Hum. Comput. Interact.
Des. Issues Solut. Appl. 181 (2009)
8. Filonik, D.: Developing a Dashboard for Real-Time Data Stream Composition and
Visualization (2012)
9. Elias, M., Bezerianos, A.: Exploration views: understanding dashboard creation and
customization for visualization novices. In: Human Computer Interaction 2011, vol. 6949.
LNCS, no. PART 4, pp. 274–291 (2011)
10. Few, S.: Information Dashboard Design: The Effective Visual Communication of Data.
O’Reilly, Sebastopol (2006)
11. Singhal, S., et al.: Global private payors: a trillion-euro growth industry (2016)
12. Endsley, M.R.: Toward a theory of situation awareness in dynamic systems. Hum.
Factors J. Hum. Factors Ergon. Soc. 37(1), 32–64 (1995)
13. Deterding, S.: Gamiﬁcation: designing for motivation. Interactions 19(4), 14 (2012)
14. Parenteau, J., Sallam, R.L., Howson, C., Tapadinhas, J., Schlegel, K., Oestreich, T.W.:
Magic quadrant for business intelligence and analytics platforms. Gartner Research notes,
February, p. 82 (2016)
308
F. Epifânio and G. Pestana

ECOPPA: Extensible Context Ontology
for Persuasive Physical-Activity Applications
Mohamad Hoda(&), Valeh Montaghami, Hussein Al Osman,
and Abdulmotaleb El Saddik
MCRLab, School of Electrical and Computer Science,
University of Ottawa, 75 Laurier Ave, Ottawa, ON, Canada
{mhoda053,halosman,elsaddik}@uottawa.ca,
valeh.montaghamii@gmail.com
Abstract. Throughout the last decade, there has been a dramatic decline in
daily physical activity among individuals which results in numerous severe
health issues, including obesity, cardiovascular diseases, and high blood pres-
sure. Pervasive applications that promote health living present a promising
approach for the reduction or prevention of these health ailements. Context-
awareness plays an important role in the development of these technologies.
Context-aware systems can adapt to the changing context in order to better serve
their users. In this paper, we propose the Extensible Context Ontology for
Persuasive Physical-Activity Applications (ECOPPA), a formal context mod-
eling scheme for applications that promote physical activity.
Keywords: Context modeling  Ontologies  Pervasive computing
Persuasive technologies
1
Introduction
The lack of physical activity among a large swaths of the population is contributing to
many severe health issues [1, 2]. Pervasive computing technologies that encourage
exercising are showing a great promise to tackle this issue [3–5]. Effective pervasive
computing applications are context-aware which implies that applications, services, and
devices adapt their behavior to changing situations [6, 18].
Most of the existing systems that maintain a model of context use a variety of data
structures to store contextual information. These data structures fall into one of the
general context modelling categories that includes Key-Value Model, Markup Schema
Model, Graphical Modelling, Object-Oriented Modelling, Logic-Based Modelling, and
Ontology-Based Modelling [7, 8]. Strang et al. [7] evaluated context modeling methods
based on incompleteness, ambiguity, richness, quality of information, and level of
formality, and found ontology to be the most promising method for context modelling.
SEVERAL context ontologies have been developed by researchers and SOUPA
[9], COBRA-ONT [10], CONON [11], and Extendible Context Ontology [12] are
some of the most prominent ones. SOUPA models context for pervasive computing
environments and comprises two distinct ontologies: SOUPA Core which deﬁnes the
general terms in the common domains of pervasive computing, and SOUPA extensions
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_30

which deﬁnes additional concepts for supporting speciﬁc applications. COBRA-ONT
consists of a set of ontologies for modeling context in smart environments. It is
designed to be used in a broker-centric agent architecture (CoBrA) in smart spaces.
COBA-ONT is expressed by OWL and consists of three major concepts: Place, agent,
and events. CONON is divided into two sub-ontologies: upper ontology and domain
ontology. The upper ontology describes primary and universal entities which are
general in pervasive computing domains.
The above described ontologies are very general and only model the core context
entities that are common between different domains. We propose a domain speciﬁc
context ontology for modelling the general contextual aspects of pervasive computing
applications which enhance physical activity.
2
Proposed Context Ontology
We develop our ontology in two levels: the upper and the domain-speciﬁc level. The
ontology at the upper-level captures the common contextual entities that are universal
for all pervasive computing environments. The ontology at the domain-speciﬁc level
extends the upper level. It models context for the domain of technologies that promote
physical activity. It captures speciﬁc features about physical activity and the context it
happens in. It extends details about the general concepts by adding speciﬁc concepts
and properties. The ontology at the domain-level can be extended into application
ontologies. The application ontologies are used in particular health-promoting appli-
cations enhancing physical ﬁtness. Figure 1 shows the general overview of the pro-
posed ontology.
2.1
ECOPPA at the Upper Level
Context has some basic and generic entities in various pervasive computing domains;
however, detailed features about these entities are considerably different in each
domain [11]. For instance, consider location as one of the key context entities. In all
pervasive computing scenarios, there is always a person who uses a device, agent, or
Fig. 1. General overview of ECOPPA
310
M. Hoda et al.

service, and this person has a location, so changes in any of these contexts may
inﬂuence the provided context-aware service. However, location is an abstract term;
any place can be called a location. Therefore, the details and features of identifying a
location change from one domain to another. For example, for a context-aware smart
home domain, kitchen, living room, and/or bedroom are location. Alternatively, in the
context-aware vehicle domain, GPS position and coordinates are location.
At the upper level, we identify the core context entities that are shared in different
domains without providing detail information about these concepts.
2.2
ECOPPA at the Domain Level
ECOPPA at the domain level models context for applications aimed at improving
physical activity. Person, Device, Activity, and PersuasiveService are the key concepts
that play critical roles in modelling context for technologies that motivate physical
activity. Typically, these technologies deliver a PersuasiveService to a person in
regards to an activity. For example, using the PersuasiveService, a person gets moti-
vated to exercise more often. Figure 2 shows an overview of the main concepts and
their relationships.
2.2.1
Person
A person consumes persuasive services. He/she is a representative of a user in per-
vasive computing environments. A wide range of information and characteristics about
a person (e.g. name, personality, physical status and activeness) must be presented in
the ontology. They can be divided into two groups, according to the rate of change in
their values. A large part of the information about a person is static. Static aspects of a
person do not change over a long time, such as name or personality traits. However,
some of a person’s properties vary frequently or in a short amount of time, such as heart
rate or blood pressure. It is very important to distinguish between these two categories
of information from a ubiquitous systems perspective, as static data can be acquired
once and remains constant, while, dynamic information has to be collected continu-
ously through sensors as the system’s behaviour depends on this data (Fig. 3).
Personal information is one of the primary categories of contextual knowledge that
has to be captured in the model. Personal information consists of age, gender, date of
birth, contact information, and profession. Personal information does not change fre-
quently over time. Psychological characteristics consisting of personality traits, cog-
nitive traits, emotional states, and behaviours are other considerable aspects of a user
context that are modelled by the ontology. People’s personality and cognitive aspects
Fig. 2. General overview of ECOPPA at domain level
ECOPPA: Extensible Context Ontology
311

play a key role in designing persuasive technologies [13, 14] aimed at changing
people’s attitudes and behaviours. Technologies promoting physical activity are one
kind of persuasive systems that encourage healthier lifestyle. Physiological attributes,
individual goals, preferences, plans, interests, preferences, and goals are other concepts
pertaining to a user that are represented in the model.
2.2.2
Activity
The Activity section of the ontology models information about an individual’s activity
and factors impacting it. Activity is a general term for anything people do to achieve a
goal, such as eating breakfast, driving a car, taking a shower, etc. Physical and
sedentary activities in particular are extended from the Activity concept. The context of
an activity performed by a person can be distinguished by several factors consisting of
temporal, spatial, and environmental attributes. These attributes are modelled by the
ontology, since many systems enhancing physical ﬁtness require such contextual
information consisting of where, when, and in what conditions an activity is being
performed. For instance, several ﬁtness and physical activity motivating applications
have been developed for monitoring and calculating performed physical activity based
on location or temporal features. They calculate activity through the changes location,
using GPS coordinates, or temporal duration, to show whether users are progressing
towards their goals or not [5, 15] (Fig. 4).
2.2.3
Device
Device is the provider of the persuasive service. For a device to run a service or an
application, it must have some hardware and software requirements. A diverse range of
devices, including smart phones, smart TVs, and wearable devices, are available in
pervasive environments which have different capabilities and functionalities. Modelling
device speciﬁcation allows application developers to capture device limitations,
capabilities, and requirements for running services and applications.
Fig. 3. Partial deﬁnition of ECOPPA - Person
312
M. Hoda et al.

The hardware concept describes a device from a hardware point of view. Device
hardware consists of input, output hardware, and hardware resources. This speciﬁcation
is useful for checking the feasibility of running software or services on different devices
with respect to hardware capabilities and limitations.
The software concept provides a description of the software installed on the device.
The speciﬁcation includes name, vendor, version, and function of the installed soft-
ware. In a pervasive computing environment, application software on a device may
communicate with applications or invoke a service on other participating devices in the
environment. Therefore, the issue of compatibility between different software modules
may become crucial. Modelling information about software allows applications inter-
ested in service composition to understand compatibility issues. An overview of the
concepts and relationships for activity is presented in Fig. 5.
2.2.4
Persuasive Service
At an abstract level, a service is a functionality that can be consumed by users,
applications, agents, and other services. For example, consider an application that
forwards calls to a user’s voice mail when he/she is not able to answer the phone such
Fig. 4. Partial deﬁnition of ECOPPA - Activity
Fig. 5. Partial deﬁnition of ECOPPA (a) device (b) persuasive service
ECOPPA: Extensible Context Ontology
313

as when they are in a meeting. This system offers a call-forwarding service to the end
user. To supply such a call-forwarding service, the application must know the location
of the user, and subsequently, uses a location service which is offered by a software. To
enable service users, including agents and applications, to automatically discover,
invoke, and compose services, services are speciﬁed in our ontology with explicit
semantics.
Service speciﬁcation should have enough details about the service. We attempt to
represent a well-deﬁned speciﬁcation for describing services with certain functionality.
Figure 5 shows an overview of a service speciﬁcation. To model the domain related to
a service, we adopt a service ontology called OWL-s [16]. Although this ontology is
deﬁned for describing semantic web service in the semantic web, it has a standard and
general interface for specifying a service. Based on OWL-s, the three concepts of
ServiceProﬁle, ServiceModel, and ServiceGrounding can express a service in general.
ServiceProﬁle provides information about the functionality of the service as what it
does. This information can be used for service discovery. By using ServiceModel, a
description of how a service can be used is provided. This description gives an
overview of service outcomes when the service is executed and the conditions that lead
to these outcomes. ServiceGrounding describes implementation details which consist
of a transport protocol, message format and other implementation information.
In our ontology, we distinguish the persuasive service from the general service
term. In our deﬁnition, a persuasive service is a specialized context-aware service that
is offered to end users to persuade them to engage in physical activity based on the
context. Moreover, a persuasion service is also deﬁned as a kind of multimedia service
using one or multiple types of mediums such as text, audio, and/or video to deliver
persuasion content. A virtual trainer application uses the media types of video or text
and image for the persuasive services it delivers.
Furthermore, persuasion services differ from each other with respect to the per-
suasive strategies used, such as praise, reward, and cooperation. A persuasive strategy
Fig. 6. Run time reasoning performance. (a) rules run time, (b) queries run time.
314
M. Hoda et al.

is a technique used to persuade a person to do or stop certain behaviour. The appli-
cations developed based on our proposed context model can analyze and deliver a
persuasion service based on the context.
2.3
Context Reasoning
One of the signiﬁcant advantages of formal modelling is enabling inferring high-level
context from low-level context. To demonstrate the feasibility of context reasoning
based on ECOPPA, we describe context reasoning for creating high-level context from
low-level and sensor-driven context.
Consider a sedentary activity tracker, that monitors the user activity of “watching
TV” and sends a warning message when it realizes the user has been watching for three
hours long period without any physical movement. User activity, “Watching TV”, is a
high level context which cannot be directly acquired from sensors. It should be rea-
soned from context data taken from sensors which are in this case the status of pressure
sensors attached to the sofa and the TV Set. If TV Set and the status of pressure sensor
are both on, it can be reasoned that user is watching TV. This rule can be written based
on the ontology (Table 1). Moreover, the tracker has to send a warning message under
a speciﬁc context. The conditional context for triggering the action of sending a
warning message can also can be written based on the ontology (Table 2).
3
Evaluation of Reasoning Performance
We use Protégé [17] 5.0.0 to implement the ECOPPA ontology. We employ a com-
puter machine with an i5, 2.4 GHz CPU and 4.00 GB RAM to evaluate the reasoning
performance. It is important to note that the reasoning performance could be affected by
Table 1. Rule for describing sedentary activity “Watching TV”
High level context
Reasoning rule
WATCHING TV
(sedentary activity)
“Person P is Watching TV”
Persosn(?p) ^ hasStatus(sofaSensor, ON) ^ hasStatus (TVSet,
ON) -> perform(p, WATCHING_TV)
Table 2. Rule for describing the contextual conditions triggering persuasive service
Persuasive service
Description
Reasoning rule
AlarmingService – send and
alarming message to User
Mobile Phone to encourage the
user takes a break
Sedentary
activity for
more than
three hours
Person(?p) ^ perform(?p,
?activity) ^
SedentaryActivity(?activity) ^
hasTemporalDescription(?activity, ?
duration) ^ TemporalDuration
(?duration) ^
Hour(?duration, ?h) ^
swrlb:greaterThan(?h, 3)
ECOPPA: Extensible Context Ontology
315

the pre-deﬁned ontology classes. In this study, we investigate the effect of the number of
instances, instantiated from these classes, on the reasoning performance. In other words,
we evaluate the feasibility of using ontology context reasoning based on the proposed
ontology to build context-aware applications which persuade users to engage in physical
activities. We examine how the size of SWRL rules in the ontology affects the reasoning
and query response time. The ﬁrst dataset has 950 instances; the second dataset has 1900
instances (950 * 2) and so on. The tenth dataset has 9500 instances. For each dataset,
several SWRL rules (three rules) are created, and a number of queries (three queries) are
asked. Figure 8 shows an example of how rules and queries work. The time needed to
transfer the SWRL rule into the rule engine knowledge is recorded. In addition, the
average time needed to answer a query is also recorded. It is worth mentioning that
context-aware applications can use context reasoning to take decisions and react to the
contextual changes based on the new inferred rules. Moreover, context-aware appli-
cations usually respond to contextual changes quickly. However, the size of contextual
information will inﬂuence the context reasoning performance. Therefore, an increase in
the number of contextual facts in the knowledge base may slow down the inference of
new facts. As a consequence, context-aware applications may not be reactive to the
context changes over a corresponding time. Our results show that the reasoning time is
approximately linear with respect to the number of instances (Fig. 7).
Finally, we examine the reason for obtaining different results in Fig. 7. In fact, rule
transforming time -> SWRL is a language to express the facts based on the OWL.
Therefore, the facts are expressed in SWRL in the knowledge base. To deduct new
facts and add them into the knowledge base, a rule engine runs over the rules in the
knowledge base. The amount of time required to run the rule engine and reach
the stable state in the knowledge base is called rule time. This time increases with the
increase of the knowledge base. However, to answer a query, the knowledge base will
not be affected and the query time does not change.
Fig. 7. Examples of how rules and queries work. Rule 1 is used to check whether a person p is
watching TV, query 1 is used to state all the persons p who are watching TV. Rule 2 is used to
locate the location of the device, query 2 is used to state all the location of the Tablet. Rule 3 is
used to check the persuade game that is suitable for person p, query 3 is used to state the persuade
state of person Sara.
316
M. Hoda et al.

4
Conclusion
In this paper, we presented the ECOPPA ontology designed to model context for
pervasive applications aimed at persuading users to engage in physical activity. The
ontology is divided into the upper and the domain-speciﬁc level. At the upper-level, the
ontology models comment contextual entities that are relevant for most pervasive
computing applications. At the domain-level, the ontology deﬁnes concepts that are
relevant to persuasive applications that encourage physical activity. The reasoning
performance of the ontology was tested in Protégé [17] where we found that the
computational time increases almost linearly with respect to the number of instances.
References
1. Manson, J.E., Skerrett, P.J., Greenland, P., VanItallie, T.B.: The escalating pandemics of
obesity and sedentary lifestyle: a call to action for clinicians. Arch. Intern. Med. 164,
249–258 (2004)
2. Deckelbaum, R.J., Williams, C.L.: Childhood obesity: the health issue. Obes. Res. 9,
239S–243S (2001)
3. Bielik, P., Tomlein, M., Krátky, P., Mitrík, Š., Barla, M., Bieliková, M.: Move2Play: an
innovative approach to encouraging people to be more physically active. In: Proceedings of
the 2nd ACM SIGHIT International Health Informatics Symposium, pp. 61–70. ACM
(2012)
4. Toscos, T., Faber, A., An, S., Gandhi, M.P.: Chick clique: persuasive technology to motivate
teenage girls to exercise. In: CHI 2006 extended abstracts on Human factors in computing
systems, pp. 1873–1878. ACM (2006)
5. Maitland, J., Sherwood, S., Barkhuus, L., Anderson, I., Hall, M., Brown, B., Chalmers, M.,
Muller, H.: Increasing the awareness of daily activity levels with pervasive computing. In:
2006 Pervasive Health Conference and Workshops, pp. 1–9. IEEE (2006)
6. Finin, T., Joshi, A., Kagal, L., Ratsimore, O., Korolev, V., Chen, H.: Information agents for
mobile and embedded devices. In: International Workshop on Cooperative Information
Agents, pp. 264–286. Springer (2001)
7. Strang, T., Linnhoff-Popien, C.: A context modeling survey. In: Workshop Proceedings
(2004)
8. Chen, G., Kotz, D.: A survey of context-aware mobile computing research. Technical Report
TR2000-381, Department of Computer Science, Dartmouth College (2000)
9. Chen, H., Perich, F., Finin, T., Joshi, A.: SOUPA: standard ontology for ubiquitous and
pervasive applications. In: The First Annual International Conference on Mobile and
Ubiquitous Systems: Networking and Services, MOBIQUITOUS 2004, pp. 258–267. IEEE
(2004)
10. Chen, H., Finin, T., Joshi, A.: An ontology for context-aware pervasive computing
environments. Knowl. Eng. Rev. 18, 197–207 (2003)
11. Wang, X.H., Zhang, D.Q., Gu, T., Pung, H.K.: Ontology based context modeling and
reasoning using OWL. In: Proceedings of the Second IEEE Annual Conference on Pervasive
Computing and Communications Workshops, pp. 18–22. IEEE (2004)
12. Preuveneers, D., Van den Bergh, J., Wagelaar, D., Georges, A., Rigole, P., Clerckx, T.,
Berbers, Y., Coninx, K., Jonckers, V., De Bosschere, K.: Towards an extensible context
ontology for ambient intelligence. In: European Symposium on Ambient Intelligence,
pp. 148–159. Springer (2004)
ECOPPA: Extensible Context Ontology
317

13. Oinas-Kukkonen, H., Harjumaa, M.: Persuasive systems design: key issues, process model,
and system features. Commun. Assoc. Inf. Syst. 24, 28 (2009)
14. Xu, Y., Poole, E.S., Miller, A.D., Eiriksdottir, E., Kestranek, D., Catrambone, R., Mynatt, E.
D.: This is not a one-horse race: understanding player types in multiplayer pervasive health
games for youth. In: Proceedings of the ACM 2012 Conference on Computer Supported
Cooperative Work, pp. 843–852. ACM (2012)
15. Foster, D., Linehan, C., Kirman, B., Lawson, S., James, G.: Motivating physical activity at
work: using persuasive social media for competitive step counting. In: Proceedings of the
14th International Academic MindTrek Conference: Envisioning Future Media Environ-
ments, pp. 111–116. ACM (Year)
16. Martin, D., Burstein, M., Hobbs, J., Lassila, O., McDermott, D., McIlraith, S., Narayanan,
S., Paolucci, M., Parsia, B., Payne, T.: OWL-S: semantic markup for web services. W3C
member submission 22, 2007–2004 (2004)
17. Protégé. http://protege.stanford.edu/
18. Kimble, C., de Vasconcelos, J.B., Rocha, Á.: Competence management in knowledge
intensive organizations using consensual knowledge and ontologies. Inf. Syst. Front. 18(6),
1119–1130 (2016)
318
M. Hoda et al.

Creating Predictive Models for Forecasting
the Accident Rate in Mountain Roads
Using VANETs
Borja Bordel1,2(&), Ramón Alcarria1,2, Gianluca Rizzo1,
and Antonio Jara1
1 Institute of Information Systems, University of Applied Sciences Western
Switzerland (HES-SO), Sierre, Valais, Switzerland
gianluca.rizzo@hevs.ch, jara@ieee.org
2 Universidad Politécnica de Madrid, Madrid, Spain
bbordel@dit.upm.es, ramon.alcarria@upm.es
Abstract. Monitoring the road network status of an entire country in a visual
way (as traditionally) is very hard, so different mechanisms to do it in an
automatic manner have been investigated. In particular, nomadic pervasive
sensing platforms based on VANETs have been recently deployed. However,
the level of road damage is a relative variable, and it is necessary to predict the
particular impact of the same in each case, in order to prioritize the conditioning
works. Therefore, in this paper a predictive model for forecasting the accident
rate in mountain roads, considering the measures previously obtained through a
nomadic sensing environment (and through the weather ofﬁce) is deﬁned. The
model considers the type of road under study as well as different analysis scales
to perform the calculations. The model is based on Taylor’s series and multi-
variate functions. Real data related to Valais (Switzerland) road network is
employed to construct and validate the proposed model.
Keywords: Vehicular ad-hoc networks  Prediction models  Data analysis
Pervasive sensing
1
Introduction
Switzerland is the most mountainous country in Europe. Thus, many settlements,
monuments and infrastructures are only accessible by road. In fact, roads account for
roughly 90% of total transportation space in Switzerland [1]. This percentage, besides, is
continuously growing, as during the last thirty years roads have gained around 15% of
extra space. In particular, in the canton of Valais are located the highest mountains of all
Switzerland, being this region separated from its neighbors by mountain ranges crossable
by high mountain roads that are closed almost the entire winter [2]. However, in summer,
weather is warmer and temperatures reach the thirty-ﬁve degrees in some occasions.
B. Bordel and R. Alcarria—On leave from: Universidad Politécnica de Madrid, Madrid, Spain.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_31

As a consequence, in Valais, motorways only occupy 9% (approximately) of space
reserved for roads [1]. Besides, both motorways and other roads (see Fig. 1) are subject
to very variable climatic conditions, which causes rapid (and potentially serious)
damage to these infrastructures.
The orography and the high density of roads make very difﬁcult to monitor the
status of the Valais road network in a traditional way (visually, by means of workers on
the roads assessing their status). Thus, during the last years, different technologies
related to Smart City solutions have been implemented in order to address this problem.
Speciﬁcally, in Valais, the Institute of Information Systems (University of Applied
Sciences Western Switzerland) together with the Mobility Lab in Sion have developed
a Nomadic Scalable Ecosystem (NOSE) for pervasive sensing, computing and com-
munication [3], which monitors the road network status through sensor nodes
embedded into public buses (managed by La Poste Suisse). Those buses deﬁne a
Vehicular Ad Hoc Network (VANET) which offers the obtained results to the stake-
holders [4].
Data obtained from this environment, nevertheless, cannot be directly employed by
road management ofﬁces, as road damage is a relative variable: the most urgent con-
ditioning works are those which prevent the highest number of accidents. It would be
desirable, then, to predict the future accident rate (or dangerousness index, more
properly) of a road, considering the obtained data through NOSE.
Therefore, the objective of this paper is to create a predictive model for forecasting
the dangerousness index of Valais roads, considering the obtained data through NOSE.
Besides, weather information from the Meteosuisse ofﬁce was included in order to
improve the prediction accuracy. In order to do that, the different types of roads were
studied, and different analysis scales were considered (roads were divided into sections
of 1 km, 2 km, etc.). Once correlations among the available data were located, Taylor’s
series and multivariate functions were employed to deﬁne the model.
The rest of the paper is organized as follows: Sect. 2 describes the state of the art on
predictive models. Section 3 describes the main proposal, and Sect. 4 validates the
performance of the model. Finally, Sect. 5 presents some conclusions.
Fig. 1. Considered types of roads (a) motorway (b) urban road (c) mountain road
320
B. Bordel et al.

2
State of the Art on Predictive Models
Predictive models are a key technique in several ﬁelds. From medicine [5] to Cyber-
Physical Systems [6], predictive models allow improving the systems’ performance.
In relation to trafﬁc research, most works are focused on noise prediction [7].
Different mathematical expressions (usually logarithmic functions), depending on the
environment under study and the evaluated infrastructure are proposed, in order to
preview the acoustic impact of roads. Other important group of works deals with the
idea of forecasting the trafﬁc congestion. In particular, different mobility models are
proposed depending on the urban or road conﬁguration [8]. Besides, seasonal behaviors
are also modeled in order to preview bottlenecks in the road network [9]. Other articles
deﬁne statistical distributions and probabilistic models in order to predict collision
between animals and vehicles [10]; or the “en route” mental overload [11].
Recently, besides, modern data analytics [12], stochastic modeling [13] and adaptive
control [14] techniques have been also employed to deﬁne predictive model for trafﬁc
ﬂow, especially in the context of smart cities and cyber-physical systems [15]. Predictive
models for forecasting the trafﬁc evolution at real-time based on Big Data technologies
[16] have been reported as well; and application-speciﬁc predictive models to be used in
key scenarios (such as critical situations [17]) may be also found. As relevant contri-
butions, different simulators [18] and case studies about the use of trafﬁc predictive
models (e.g. Montreal [19], Taiwan [20], etc.) have been published too.
Finally, predictive trafﬁc models usually are hierarchical and consist of various
layers, where lower levels manage correlation among physical parameters and more
abstract levels are employed to predict human and long-term behaviors [21]. Although
modern learning techniques are commonly employed [22] in predictive models, pure
mathematical models allow a better understanding of the relations among the different
variables. In particular, in this work, Taylor’s series and multivariable functions are
employed.
3
Proposal
In this section, the creation of the predictive models is discussed. The creation process
includes three steps: (i) data acquisition and pre-processing, (ii) correlation analysis;
and (iii) model deﬁnition and particularization. The following subsections describe in
detail each one of these steps.
3.1
Data Acquisition and Pre-processing
The deployed NOSE infrastructure monitors around 100 km of roads in the canton of
Valais, in the districts of Sion and Hérens. Three types of roads are monitored: the A9
motorway between Saxon and Sion (30 km); urban roads in Sion (20 km) and a
mountain road between Sion and Arolla (40 km). Figure 2 shows this itinerary. This
infrastructure offers data about location, time, vehicle (bus) speed, road temperature,
water-ﬁlm height, percentage of ice, friction, dew point and general road conditions
(dry, wet, moisture, melted snow, snowy/icy).
Creating Predictive Models for Forecasting the Accident Rate
321

In order to obtain a deeper knowledge about the situation, information from the
Meteosuisse ofﬁce maybe also considered. Meteosuisse offers measures about 145
different variables (approximately) with time precision of milliseconds. For this work,
however, we are considering only four of these variables: the total thickness of
recumbent snow (in centimeters), rainfall (daily sum, in millimeters), duration of sun-
shine (hourly sum, in minutes) and the hourly solar radiation (in watts per square meter).
Data about the trafﬁc statistics (including accidents) in Valais were also obtained.
This information was employed to calculate the dangerousness index ðDIÞ of the roads
under study during the second half 2016. Basically, the dangerousness index of a road
(1) is a relative measure of the number of accidents in this road ðNaccÞ, considering the
corresponding level of risk exposure ðR, i.e. the trafﬁc volume in vehicleskm). In order
to remove random variations, temporal and spatial mean data are usually employed.
DI ¼ Nacc
R
accidents
vehicles  km


ð1Þ
A spatial database is used to store these parameters. A PostgreSQL 9.6 database was
used, with PostGIS 2.3.2 extension for enabling spatial operations. Data provided by the
sensor infrastructure is stored considering the timestamp for the data and the location
where the data was generated, storing the values in timestamp with timezone and
geometry data types. This enables the possibility of performing temporal requests (data
ranges depending on dates) and spatial requests (data ﬁltering by proximity or density).
Data coming from Meteosuisse is stored in two interrelated tables. The ﬁrst one
stores metadata information from weather stations, such as location, metrics and time of
the last update. The second one contains observations related to the station_id. Each
observation had a unique identiﬁer consisting in an auto-increment number, acting as
table key.
Fig. 2. Itinerate under study showed on the GIS tool
322
B. Bordel et al.

Data about trafﬁc statistics were also imported to the spatial database. The table
describing accidents contained accident coordinates (expressed in geometry) and ID of
the road, time of the accident (timestamp with timezone), type, description, caused
property damage, and type of injuries (death, major, and minor).
For information representation in a spatial way we used QGIS framework version
2.18.6. We obtained information in vector format from the PostGIS database and
represented it as three layers. We used Microsoft Bing as base map for the overlays.
The itinerate under study showed on the GIS tool can be seen in Fig. 2.
We have presented the information graphically, and using maps of the region to
make a selection of data according to the type of road that we deﬁned in the
introduction.
For this, we have obtained road sections of 500 m, 1 km and 2 km and we have
deﬁned for each section the type of road (motorway, urban road, and mountain road).
For the measurement of the section we have used the measurement tools provided by
QGIS. For each section, a data set has been generated with information provided by the
sensor infrastructure, information generated by Meteosuisse from the nearest weather
station (meteosuisse.station.sion), and the number and type of accidents that occurred
on those road sections. This is the input information that we introduce in our correlation
analysis process, described in Sect. 3.2.
3.2
Correlation Analysis
Performing a complete correlation analysis would imply to deﬁne a clustering method
based on a sliding window with a ﬁx length (2 km, 1 km or 500 m in our proposal).
This window should be moved through the entire itinerary under study, which should
be also divided into cells (inside which it is suppose road conditions do not change).
Cell by cell, the sliding window advances generating a set of measures whose geo-
graphical mean has to be calculated in order to remove random effects. Mean values,
thus, would be employed to calculate correlations. This procedure, however, requires
detailed information about trafﬁc volume in each road section to be available.
In this case, we are only provided with global information [2] (see Table 1), so it is
proposed a simpler analysis method. Roads are divided into ﬁxed consecutive sections
of the speciﬁed length, which are employed to obtain the geographical means of
parameters. With this information, related to the ﬁrst half of the year, we are looking for
correlation with the accident rate in the second half of the year (see Tables 2 and 3).
For this ﬁrst work, besides, data acquisition time is not considered.
Table 1. Trafﬁc ﬂow depending on the road typology
Road type
Vehicle  km in hundreds
ð
Þ Motorway Urban road Mountain road
419
119
20
Creating Predictive Models for Forecasting the Accident Rate
323

Variables whose correlation with the accident rate is under rj j ¼ 0:3 are not con-
sidered relevant for this study. In that way, as can be seen, different variables may be
relevant depending on the considered analysis scale and type of road (i.e. there is a
strong dependency on the road conﬁguration). In general, however, 1 km road sections
present a good balance and pretty acceptable results. In relation to climatic information,
as only one measuring station is placed around the study area, temporal (monthly)
correlations are obtained.
3.3
Model Deﬁnition
As can be seen, correlations between the relevant variables identiﬁed in the previous
subsection and the accident rate are, in general, around rj j ¼ 0:5. This means that none
of these variables is a direct cause of accidents, but they are involved in the accident
occurrence. A combination of several of these variables, then, should explain the
accident rate during the next temporal period (6 months). However, how these variables
are weighted and/or combined in order to cause or not an accident is unknown. In this
work, we are supposing the multivariate function F ~x
ð Þ represent this weighted com-
bination. This function, moreover and as any other function, may be expressed using
Taylor’s series (2) centered in the origin.
Table 3. Correlation between the accident rate and Meteosuisse parameters
Road type
Meteosuisse parameters
Snow
Rainfall
Sunshine Radiation
Motorway
−0.4205 −0.0155 −0.1402
−0.1991
Urban road
0.1526
−0.2699 −0.4000
−0.4459
Mountain road −0.4410 −0.4692 0.5481
0.5076
Table 2. Correlation between the accident rate and NOSE parameters
Road type 
NOSE Parameters 
Altitude 
Speed 
Temp 
Waterfilm 
Ice 
Friction 
Dew 
point 
Road 
Cond 
Motorway 
-0.0294 
-0.0758 
-0.1479 
0.082 
-0.434 
-0.325 
-0.198 
0.086 
-0.008 
0.1295 
-0.3894 
0.2677 
--- 
--- 
--- 
-0.1118 
0.5502 
0.2676 
-0.177 
0.128 
-0.081 
0.1504 
-0.371 
--- 
Urban 
road 
-0.3771 
-0.352 
0.2640 
0.053 
-0.684 
0.232 
-0.3974 
0.497 
0.3677 
0.0435 
-0.5344 
-0.0505 
--- 
0.435 
--- 
-0.0186 
0.5444 
0.0256 
-0.584 
-0.084 
0.6158 
0.0291 
-0.556 
0.0186 
Mountain 
road 
-0.8648 
-0.521 
0.6023 
0.099 
0.494 
0.043 
0.777 
0.248 
-0.580 
-0.551 
-0.593 
-0.205 
0.769 
0.531 
0 
0.4929 
0.491 
0.1168 
0.813 
0.431 
-0.187 
-0.492 
-0.208 
-0.381 
Black numbers represent road sections of 2km, red numbers 1km sections and purple results 500m sections 
 
324
B. Bordel et al.

DI ¼ F ~x
ð Þ ¼
X
1
k¼0
1
k!
X
k1 þ ... þ kd¼k
k
k1  . . .  kd


@kF ~0
 
@xk1
1  . . .  @xkd
d
xk1
1  . . .  xkd
d


ð2Þ
Where ~x is a d-dimensional vector containing the relevant variables to be consid-
ered in the predictive model.
The model, besides, may be simpliﬁed if all constant values are aggregated (3).
Furthermore, as computational solution cannot deal with inﬁnite sums, the proposed
Taylor’s series should be approximated through the sum of the ﬁrst q terms.
DI ¼
X
1
k¼0
X
k1 þ  þ kd¼k
kk1;...;kd xk1
1  . . .  xkd
d



X
q
k¼0
X
k1 þ  þ kd¼k
kk1;...;kd xk1
1  . . .  xkd
d


ð3Þ
In that way, the proposed predictive model would be totally deﬁned if the kk1;...;kd
unknown parameters were obtained. These parameters are time independent, although
an identical mathematical expression could be obtained if there were considered time
dependent. This set of parameters may be calculated considering the data collections
employed in the previous subsection, and solving the resulting equation system using
the optimization of the mean square error technique.
4
Experimental Validation
An experimental validation was designed in order to validate the proposal and analyze
the performance of the proposed models. This section describes the creation of a
particular predictive model using the previously presented information, and evaluates
the validity of the obtained predictions trying to forecast the accident rate in the second
half of 2016 (using information about the ﬁrst half).
4.1
Model Creation
The objective of this subsection is to deﬁne a predictive model to forecast the accident
rate in mountain roads, considering road segments with a length of 1 km. In order to
create this ﬁrst predictive model three variables from NOSE project were considered:
the ice percentage, the waterﬁlm height and friction. Besides, one climatic variable was
also taken into account: rainfall. In that way, four input variables were considered.
On the other hand, in order to create the model it is necessary to select the order of
the mathematical expressions. For this ﬁrst example, we selected a quadratic model
ðq ¼ 2Þ.
With these choices, the created model presents ﬁfteen unknown parameters. In
order to determine their value, thirty different segments of mountain roads were ana-
lyzed. Data about the accident rate in these segments and information about the NOSE
project (as well as climatic information from Meteosuiss) were used to calculate these
parameters kk1;...;kd (see Table 4).
Creating Predictive Models for Forecasting the Accident Rate
325

4.2
Predictions and Results
A set of new road segments (control group) was employed to evaluate the performance
of the proposed model. Using information from NOSE project and Meteosuisse the
predicted accident rate for the second half of 2016 was obtained. Results were com-
pared with the real information from the Trafﬁc Ofﬁce.
In order to evaluate the quality of the performed prediction the mean square error
was calculated and evaluated. Table 5 presents the error associated with each road
segment.
As can be seen, extreme situations are badly predicted. This fact is due to the
employed data during the creation of the predictive model. As only information about
six months was available, no representative number of samples of these situations was
considered. Future works will consider more precise predictions as a higher amount of
information should be available.
Any case, as the proposed model produces decimal numbers, while the number of
accidents is an integer, it is necessary to perform a cast between both values. Three
possibilities were considered: rounding to the nearest integer, truncation to the lower
integer, and truncation to the upper integer.
Figure 3 presents the resulting success rate in the predictions for each casting
technique.
As can be seen, rounding technique is the most success solution. Around 80% of
cases are predicted correctly. Truncation, however, presents more problems. In general,
truncation to the upper integer is most precise (around 10% more) than truncation to the
lower integer (which presents a success rate of 60%, approximately).
Table 4. Model parameters
k0;0;0;0
k1;0;0;0
k0;1;0;0
k0;0;1;0
k0;0;0;1
k1;1;0;0
k1;0;1;0
k1;0;0;1
1; 27  107 3; 1  107 1; 8  105 4; 8  103 4; 6  105 1; 91  107 118; 46
0; 4437
k0;1;1;0
k0;1;0;1
k0;0;1;1
k2;0;0;0
k0;2;0;0
k0;0;2;0
k0;0;0;2
7; 06  104 2; 33  105
5; 57  105
5; 82  103
33; 02
1; 69  104 89; 1053
Table 5. Mean square error of predictions
Accidents per road segment Less than one 1–2
Two or more Total
Error
44.26%
17.50% 58.55%
20,19%
326
B. Bordel et al.

5
Conclusions
Trafﬁc accidents are one of the most important social problems nowadays. Preventing
or reducing the accident rate is a remarkable objective of most trafﬁc management
ofﬁces. In order to do that, roads have to be maintained in the best possible condition,
however, resources are limited and decisions have to be made. In this paper it is
proposed a predictive framework and model, in order to forecast the future accident rate
in a road segment depending on a collection of variables monitored though VANETs.
The proposed model is focused on mountain roads in the canton of Valais
(Switzerland).
Results showed that the proposed model allows obtaining a general picture about
the future accident rates, although detailed and more precise predictions require
advanced solutions or more information about roads (currently, predictions present an
error of 20%).
Future works should consider information acquired during longer time periods in
order to remove all random phenomena, and more detailed geographical information
about roads.
Acknowledgments. This publication was produced in the framework of the RCSO NOSE
project. Borja Bordel has received funding from the Ministry of Education through the FPU
program (grant number FPU15/03977); and Ramón Alcarria thanks the Spanish Ministry of
Education, Culture and Sport for the professor’s mobility program (José Castillejo’s 2017 grant).
Additionally, these results were supported by the Ministry of Economy and Competitiveness
through SEMOLA project (TEC2015-68284-R) and from the Autonomous Region of Madrid
through MOSI-AGIL-CM project (grant P2013/ICE-3019, co-funded by EU Structural
Funds FSE and FEDER).
Fig. 3. Success rate depending on the casting technique
Creating Predictive Models for Forecasting the Accident Rate
327

References
1. Federal Statistical Ofﬁce (FSO): Land use in Switzerland. Results of the Swiss land use
statistics. https://www.bfs.admin.ch/bfsstatic/dam/assets/348992/master. Accessed 26 Aug
2017
2. Roads and trafﬁc 2017 - Developments, facts, ﬁgures. Annual publication of the Federal
Roads
Ofﬁce.
https://www.astra.admin.ch/astra/en/home/documentation/facts-and-ﬁgures.
html. Accessed 26 Aug 2017
3. Dupont, A., Bocchi, Y., Rizzo, G.: NOSE: a NOmadic scalable ecosystem for pervasive
sensing, computing and communication. In: International Conference on Innovative Mobile
and Internet Services in Ubiquitous Computing, pp. 884–893. Springer, Cham, July 2017
4. NOSE dashboard. https://nose-e3fe0.ﬁrebaseapp.com/app/#/dashboard/sTemp. Accessed 26
Aug 2017
5. Saqr, H.E., Pearl, D.K., Yates, A.J.: A review and predictive models of gang ioside uptake
by biological membranes. J. Neurochem. 61(2), 395–441 (1993)
6. Sánchez, B.B., Alcarria, R., De Rivera, D.S., Sánchez-Picot, A.: Predictive algorithms for
mobility and device lifecycle management in cyber-physical systems. EURASIP J. Wirel.
Commun. Network. 2016(1), 228 (2016)
7. Quartieri, J., Mastorakis, N.E., Iannone, G., Guarnaccia, C., D’ambrosio, S., Troisi, A.,
Lenza, T.L.L.: A review of trafﬁc noise predictive models. In: Recent Advances in Applied
and Theoretical Mechanics, 5th WSEAS International Conference on Applied and
Theoretical Mechanics (MECHANICS 2009) Puerto De La Cruz, Tenerife, Canary Islands,
Spain, pp. 14–16, December 2009
8. Papageorgiou, M., Diakaki, C., Dinopoulou, V., Kotsialos, A., Wang, Y.: Review of road
trafﬁc control strategies. Proc. IEEE 91(12), 2043–2067 (2003)
9. Williams, B.M., Hoel, L.A.: Modeling and forecasting vehicular trafﬁc ﬂow as a seasonal
ARIMA process: theoretical basis and empirical results. J. Transp. Eng. 129(6), 664–672
(2003)
10. Malo, J.E., Suarez, F., Diez, A.: Can we mitigate animal–vehicle accidents using predictive
models? J. Appl. Ecol. 41(4), 701–710 (2004)
11. Loft, S., Sanderson, P., Neal, A., Mooij, M.: Modeling and predicting mental workload in En
route air trafﬁc control: critical review and broader implications. Hum. Factors 49(3), 376–
399 (2007)
12. Lin, L., Li, J., Chen, F., Ye, J., Huai, J.P.: Road trafﬁc speed prediction: a probabilistic
model fusing multi-source data. IEEE Transactions on Knowledge and Data Engineering
(2017)
13. Li, L., You, S., Yang, C., Yan, B., Song, J., Chen, Z.: Driving-behavior-aware stochastic
model predictive control for plug-in hybrid electric buses. Appl. Energy 162, 868–879
(2016)
14. Massera Filho, C., Terra, M.H., Wolf, D.F.: Safe optimization of highway trafﬁc with robust
model predictive control-based cooperative adaptive cruise control. IEEE Trans. Intell.
Transp. Syst. 18, 3193–3203 (2017)
15. Bordel, B., Alcarria, R., Robles, T., Martín, D.: Cyber–physical systems: extending
pervasive sensing from control theory to the internet of things. Pervasive Mob. Comput. 40,
156–184 (2017)
16. Amini, S., Gerostathopoulos, I., Prehofer, C.: Big data analytics architecture for real-time
trafﬁc control. In: 2017 5th IEEE International Conference on Models and Technologies for
Intelligent Transportation Systems (MT-ITS), pp. 710–715. IEEE, June 2017
328
B. Bordel et al.

17. Götte, C., Keller, M., Haß, C., Bertram, T.: Model predictive planning and control applied to
critical trafﬁc situations. ATZ Worldwide 118(9), 64–69 (2016)
18. Suh, J., Yi, K., Jung, J., Lee, K., Chong, H., Ko, B.: Design and evaluation of a model
predictive vehicle control algorithm for automated driving using a vehicle trafﬁc simulator.
Control Eng. Pract. 51, 92–107 (2016)
19. Carrier, M., Apparicio, P., Séguin, A.M.: Road trafﬁc noise in Montreal and environmental
equity: what is the situation for the most vulnerable population groups? J. Transp. Geogr. 51,
1–8 (2016)
20. Wang, V.S., Lo, E.W., Liang, C.H., Chao, K.P., Bao, B.Y., Chang, T.Y.: Temporal and
spatial variations in road trafﬁc noise for different frequency components in metropolitan
Taichung. Taiwan. Environ. Pollut. 219, 174–181 (2016)
21. Pappas, G.J., Lafferriere, G., Sastry, S.: Hierarchically consistent control systems. IEEE
Trans. Autom. Control 45(6), 1144–1160 (2000)
22. Dufour, L., Genoud, D., Ladevie, B., Bezian, J.J.: Heating and hot water industrial prediction
system for residential district. In: 2016 30th International Conference on Advanced
Information Networking and Applications Workshops (WAINA), pp. 821–826. IEEE,
March 2016
Creating Predictive Models for Forecasting the Accident Rate
329

Intelligent and Decision Support Systems

Improving Game Modeling for the Quoridor
Game State Using Graph Databases
Daniel Sanchez(B) and Hector Florez
Universidad Distrital Francisco Jose de Caldas, Bogot´a, Colombia
danielssj88@gmail.com, haflorezf@udistrital.edu.co
Abstract. Artiﬁcial intelligence has gained great importance in the
last decades because based on its techniques, it is possible to make
autonomous systems. In addition, it is possible to make those systems
able to learn based on the previous interactions with users. This paper
presents one proposal for an agent to play the Quoridor game based on
some improvements in the graph model of the board. It is done by using
artiﬁcial intelligence techniques to provide the capacity to learn through
games played against users. Thus, learning is achieved through the use
of game trees, where some of the nodes are going to be stored using a
graph database. Since graph databases are one of the subgroup of the
noSQL databases, which focuses in the relation representation between
nodes, such databases are suitable for this kind of approaches.
Keywords: Game tree · Quoridor · Graph databases · Minimax
1
Introduction
The Quoridor game was designed by Mirko Marchesi and published by Gigamic
Games in 1997. It received the Mensa Mind Game award in 1997 and the Game
Of The Year in USA, France, Canada and Belgium. It has gained attention in
the research community since several studies are being made to mastering this
game. The reason is that despite its rules are simple, it has complex strategies
to study. Furthermore, the game has a big branch factor and game tree order,
which is similar to the chess game.
In the last thirty years, the studies around the game theory has increased
in order to make proposals that improve the way as an artiﬁcial agent can deal
with games with a certain complexity. Of course, the use of a strong hardware
is a really important aspect. For instance, Deep Blue was able to review explore
200.000.000 of positions per second, which means that it was able to make a
deeper search in the game tree in less time [1]. In this paper, we focus on the
model of the game state since there is no a lot of research in deﬁning its heuristics.
Graph databases are in the group of noSQL databases, which are focused on
the relations between the data stored in the database, among other facilities like
the semantic of the model, which can be shared directly to an expert (in this
case a game player).
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_32

334
D. Sanchez and H. Florez
The rest of the paper is structured as follows: Sect. 2 - explanation of the
game, Sect. 3 - an explanation of the features of graph databases, Cypher and
Neo4J, Sect. 4 - representation of the board as a graph, Sect. 5 - future work and,
ﬁnally Sect. 6 - conclusions.
2
Quoridor Game
2.1
Game Rules
The Quoridor consists of a board of 9x9 spaces and can be played by 2 or 4
players; however, in this work we just focus on 2 players, the COM and the
human player. Each player has one pawn and ten fences that can be placed
horizontally or vertically as shown in Fig. 1a. A player wins the game when he
puts its pawn in the row where the opponent pawn starts.
On the player’s turn, it is possible to either move his pawn or place one fence.
Fences can be placed anywhere on the board between the squares that the pawns
can move; however, a new fence cannot intersect an existing fence. In addition,
a fence cannot block completely a player’s access to his goal. Furthermore, the
pawn cannot move through a fence. If pawns are adjacent to each other with no
fence between them, a pawn may jump over the opponent’s pawn. This jump
must be in a straight line as shown in Fig. 1b, unless a fence is behind the other
pawn, in which case the jump must be diagonal as presented in Fig. 1c.
(a) Fence positions
(b) Jump over a
pawn
(c) Jump over a
pawn with fence
behind
Fig. 1. Quoridor Rules
2.2
Game Notation
For representing the game we are using the notation proposed in [2], which uses
an approximation for the chess algebraic notation, explained as follows [2]:
– Each square on the board is identiﬁed by a letter-number combination.
– The columns are labeled with letters a to i from left to right. The rows are
labeled with numbers 1 to 9 from top to bottom (see Fig. 2).
– The black pawn begins at square e9, while the white pawn at square e1.

Improving Game Modeling for the Quoridor Game State
335
– A pawn move is identiﬁed by the square that the pawn is moved to.
– A fence move is identiﬁed by the fence’s northwest square. This square identiﬁ-
cation is followed by h or v to identify whether the fence is placed horizontally
or vertically.
– A sequence of moves should be identiﬁed by the turn number, where a turn
is ended after both players have moved, ﬁrst black then white.
Fig. 2. Board positions notation
2.3
Game Complexity
According to Glendenning et al., [2], based on game theory, Quoridor complexity
is similar to chess and checkers and can be described as a deterministic, sequen-
tial, two-player, zero-sumgame of perfect information with a restricted outcome.
In addition, according to Mertens [1] there is two ways to measure the complexity
of the game:
1. The state-space complexity: It refers to the diﬀerent positions that may be
raised in the game. For the Quoridor, it corresponds to the possible amount
of positions for the pawns multiplied by the available positions to place the
fences. Since the amount of illegal positions are hard to calculate; then, the
upper bound is calculated. The possible pawns movements is presented in
Eq. 1:
S(p) = 81 ∗80 = 6480
(1)

336
D. Sanchez and H. Florez
Fig. 3. Positions occupied by a fence [1]
Where 81 is the amount of possible pawns movements and 80 is the possible
movements of the other pawn minus the position occupied for the ﬁrst pawn.
With this in mind, when a player places a fence on the board, both players
are enabled to place another fence in four spaces. Thus, it uses the required
space to put other 4 fences as shown in Fig. 3
Then, it is possible to put 8 fences in 8 columns horizontally, having 64
possible positions, and 8 fences in 8 rows vertically. So the possible movements
of the fences are presented in Eq. 2. Therefore, the total value of the state-
space is presented in Eq. 3.
S(f) =
20

i=0
i
j=0
(128 −4i) = 6.1582 ∗1038
(2)
Table 1. Complexity comparison
GAME
log
log
Average
Source
(state-space) (game-tree) (branching factor)
Tic tac toe
3
5
4
Nine Men’s Morris 10
50
10
[3]
Awari
12
32
3.5
[3]
Pentominoes
12
18
75
[4,5]
Connect Four
13
21
4
[3,6]
Checkers
20 or 18
31
2.8
[3,7]
Lines of Action
23
64
29
[8]
Othello
28
58
10
[3]
Xiangqi
40
150
38
[3,9,10]
Quoridor
42
162
60
[2]
Arimaa
43
402
17281
[11–13]
Chess
47
123
35
[14]
Shogi
71
226
92
[9,15]
Go (19 × 19)
170
360
250
[3,16,17]
Connect6
172
140
46000
[18]

Improving Game Modeling for the Quoridor Game State
337
S = S(p)S(f) = (6480)(6.1582 ∗1038) = 3.9905 ∗1042
(3)
2. The game-tree complexity: The total amount of games that can be played.
This is estimated by raising the average branching factor (the number of
possible game state nodes that can be derived from a game state node).
According to [2] this is 60.4 and the average game length is 91.1, so the
game-tree complexity is estimated with the Eq. 4
G = 60.491.1 = 1.788410162
(4)
A little comparison between the complexities of some games similar to Quori-
dor is presented Table 1.
3
Quoridor Game Using Graph Databases
Normally, this kind of games are treated as an adversarial search problem. In this
proposal, we use an approach to model the game trees to generate one leaf that
represents each of the states of the game. We have the MIN and MAX players,
who take turns to play, until the game ﬁnishes. Then, at the end of the game,
it makes one evaluation of the movements, in order to know which is the best
movement. In this moment, we are not focusing our attention in the algorithm
or the heuristic function to evaluate the best movement, but instead we are
focusing on representing the states of the game. It is important to mention that
this approach requires a lot of processing for generating movements and visiting
them. Then, in order to reduce the impact of high processing, we improve the
approach by using a graph database in order to store eﬃciently all states of the
game. Thus, we present graph databases and their useful characteristics for this
kind of problems.
In this work, we used Neo4j 1, which is a graph database engine. In addition,
we use Cypher 2, which is a declarative language, inspired on SQL to manage
the required operations in Graph Databases. Some features of Cypher are: (1)
matches nodes and relations in the graph database in order to extract informa-
tion or modify data, (2) allows to create, update and delete nodes, relations and
properties, and (3) manages indexes and constraints.
Furthermore, we used Python for connecting to Neo4j sending Cypher queries
through the Bolt protocol3, which is a connection-oriented protocol that uses a
compact binary encoding over TCP or web sockets for higher throughput and
lower latency4.
3.1
Relational Vs Graph Databases
According to Hunger et al., [19], a Graph database is one technology which cen-
ters its attention in the representation of the data using nodes and relationships
between those nodes, forming a graph structure.
1 https://neo4j.com/.
2 https://neo4j.com/developer/cypher-query-language/.
3 https://github.com/neo4j/neo4j-python-driver.
4 https://neo4j.com/blog/neo4j-3-0-milestone-1-release/.

338
D. Sanchez and H. Florez
Currently, data is increasing in volume, velocity and variety. As a result,
data relationships are gaining more importance than the data itself. Traditional
relational databases are not designed to capture this rich relationships. In this
case, when storing tree nodes in a relational database; then, series of JOINS are
require, which impacts considerably the system performance. On the contrary,
graph databases have better response for this kind of queries.
There are a lot of diﬀerent database options, including a set of NoSQL data
stores, but none of them are explicitly designed to handle and store relationships
like Graph databases.
Nowadays, organizations use graph database technology in a diversity of ways,
including these six most common use cases: (1) fraud detection, (2) real-time rec-
ommendation engines, (3) master data management (MDM), (4) network and
IT operations, (5) identity and access management (IAM), and (6) graph-based
search.
Graph databases give the ease of a well-done, normalized entity-relationship
diagram, which can quickly share with other domain experts with a representa-
tion almost similar than the real domain. For these reasons, we selected this type
of store system to save the Quoridor game trees. This database has a knowledge
base of all the movement possibilities of the previous games played by the appli-
cation. Consequently, the application will make the queries in order to select the
best next movement, so the eﬀort is centered to explode the potential semantic
given by the Graph databases in the relationships in order to label them with the
correct heuristic value. Thus, the load processing is target to query the node that
represents the best next movement based on the relation with the best heuristic
that the application needs. This allows showing a better semantic representation
of data and also a better semantic query over data.
4
Representation of the Quoridor Board
Several works such as [2,20,21] have evaluated the mechanisms to represent the
state of the board, concluding that the best one is to model it with an undirected
graph. Nevertheless, we consider that some of the connections between the spaces
could be directed, this might oﬀer a better representation because according to
our evaluation, there are some situations in which some directed connections are
useful. For instance, consider the situation in which a pawn needs to jump the
opponent’s pawn with a fence behind him.
Thus, we have create a bidirected graph using Neo4J. Figure 4 presents the
initial state of the game from the perspective of the black player. In this state,
the graph indicates the possible movements towards their neighbors direction
with the possibility to return to the current position. In this initial graph the
only unidirectional relation is towards the edges in the objective position.
In our approach, each node has two relations because of the nature of the
Quoridor. Thus, we store in memory the digraph using Python, saving the two
directions of the path in a dictionary structure in the case of a bidirectional
relation.

Improving Game Modeling for the Quoridor Game State
339
Fig. 4. Initial graph representation of the board.
Another example is the board state presented in Fig. 5, which is representing
the perspective of the black pawn in the graph of the Fig. 6. Since it is not
possible to place the black pawn in the white position, then that pawn is not
modeled in the graph representation.
Fig. 5. Board state when an enemy pawn has one fence behind him

340
D. Sanchez and H. Florez
Fig. 6. Generated graph when an enemy pawn has one fence behind him
Figure 7 presents a zoom of the graph focusing on the center. The graph
shows one connection from e5 to d4 (Jump1). However, from d4 is not possible
to jump to e5 because reaching the d4 position; then, the pawn can jump the
Fig. 7. Detailed generated graph when an enemy pawn has one fence behind him

Improving Game Modeling for the Quoridor Game State
341
enemy through the pawn straight to the f4 position (Jump2). In this case, it is
valuable having the representation as a directed relations in those speciﬁc edges.
In addition, albeit our own pawn is not in front of the opponent’s pawn,
the approach calculates the graph with the available jumps over the opponent’s
pawn because we hypothesized that this feature might improve the minimax
calculation. Moreover, based on the graph observation, it is possible to lead
some unexplored heuristics.
5
Future Work
There is a tradeoﬀregarding the depth explored in the game tree; thus, accu-
racy demands computational resources. We believe that graph databases might
provide a great picture of the study of the Quoridor game. In addition, graph
databases allow seeing the behavior of the search algorithms. Thus, one oppor-
tunity based on the results of this project is testing this graphs databases in
business projects. Regarding to the minimax algorithm, in the last thirty years
there have been a lot of research in this area. Nevertheless, we are evaluating
the idea to help minimax algorithms using Graph Databases.
Having in mind the big size of the game-tree of the Quoridor game, it is
not possible to save all the possible movements in the Graph Database. Hence,
we will work on saving just the eligible paths of the minimax evaluation for
one node, and the minimax value calculated in some depth. Based on that,
the following iteration could use those values to improve its evaluation podding
by avoiding the unnecessary calculation of some nodes. In addition, it would
be possible to evaluate the shortest path to the end of the game directly in
the graph database [22]. Using the concepts of the network science applied in
games, we plan to make a simpler representation of the moves in order to have
an improvement of the space used. So far, we have focused this model in the
fact that the Cypher queries can be more speciﬁc ﬁltering the values of only one
node at time and retrieve its related nodes.
6
Conclusions
Approaches based on trees are simple models applied in machine learning, but
in this case, it has a great potential to represent semantically the states of the
Quoridor game. Graph databases seems to be a good tool to apply in this kind
of scenarios.
Python is a suitable language to apply in the data analysis problems because
it has build-in several functionalities like the list comprehension to deal with
data collections in a natural way like the mathematician used to do. In addition,
the ascii-art syntax used for Cypher looks great and an easy way to create nodes
and connection using Cypher into the database on Neo4j.
The bidirected graphs help us to calculate diﬀerent paths from the current
position of the pawn and the objective row, or even the enemy pawn in order to
identify which one is in the better position in the state of the game. Moreover,

342
D. Sanchez and H. Florez
graph databases help us to create dynamically data structures at the same time
the data is added into the database, without having a static structure deﬁned
from the beginning like the relational databases.
References
1. Mertens, P.J.: A quoridor-playing agent. Bachelor thesis, Department of Knowledge
Engineering, Maastricht University (2006)
2. Glendenning, L., et al.: Mastering quoridor. Bachelor thesis, Department of Com-
puter Science, The University of New Mexico (2005)
3. Allis, L.V., et al.: Searching for Solutions in Games and Artiﬁcial Intelligence.
Wageningen, Ponsen & Looijen (1994)
4. Van Den Herik, H.J., Uiterwijk, J.W., Van Rijswijck, J.: Games solved: now and
in the future. Artif. Intell. 134(1–2), 277–311 (2002)
5. Orman, H.K.: Pentominoes: a ﬁrst player win. In: Games of No Chance, vol. 29,
pp. 339–344 (1996)
6. Tromp, J.: Johns connect four playground (1995)
7. Schaeﬀer, J., Burch, N., Bj¨ornsson, Y., Kishimoto, A., M¨uller, M., Lake, R., Lu,
P., Sutphen, S.: Checkers is solved. Science 317(5844), 1518–1522 (2007)
8. Winands, M.: Informed search in complex games. Universitaire Pers Maastricht
(2004)
9. Chen, S.J.Y.J.C., Yang, T.N., Hsu, S.C.: Computer chinese chess. ICGA J. 27(1),
3–18 (2004)
10. Park, D.: Space-state complexity of korean chess and chinese chess. arXiv preprint
arXiv:1507.06401 (2015)
11. Cox, C.J.: Analysis and implementation of the game arimaa. M. Sc. diss., Univer-
siteit Maastricht, The Netherlands (2006)
12. Wu, D.J.: Move Ranking and Evaluation in the game of Arimaa. Ph.D. thesis,
Harvard University (2011)
13. Brian, H.: A look at the arimaa branching factor (2006)
14. Shannon, C.E.: Programming a computer for playing chess. In: Computer Chess
Compendium, pp. 2–13. Springer (1988)
15. Iida, H., Sakuta, M., Rollason, J.: Computer shogi. Artif. Intell. 134(1–2), 121–144
(2002)
16. Tromp, J., Farneb¨ack, G.: Combinatorics of go. In: International Conference on
Computers and Games, pp. 84–99. Springer (2006)
17. Tromp, J.: The number of legal go positions. In: International Conference on Com-
puters and Games, pp. 183–190. Springer (2016)
18. Xu, C.M., Ma, Z., Tao, J.J., Xu, X.H.: Enhancements of proof number search
in connect6. In: Control and Decision Conference, CCDC 2009, pp. 4525–4529,
Chinese. IEEE (2009)
19. Hunger, M., Boyd, R., Lyon, W.: The Deﬁnitive Guide to Graph Databases for the
RDBMS Developer. Neo4j (2016)
20. Quinlan, J.R.: Induction of decision trees. Mach. Learn. 1(1), 81–106 (1986)
21. Tsur, G., Segev, Y.: Quoridor agent. http://www.cs.huji.ac.il/ai/projects/2012/
Quoridor/ﬁles/report.pdf
22. Castelltort, A., Laurent, A.: Fuzzy queries over NoSQL graph databases: perspec-
tives for extending the cypher language. In: International Conference on Informa-
tion Processing and Management of Uncertainty in Knowledge-Based Systems, pp.
384–395. Springer (2014)

A Model of Self-oscillations in Relay
Outputs Control Systems with Elements
of Artiﬁcial Intelligence
R. H. Rovira1(&), V. M. Duvoboi2, M. S. Yukhimchuk2,
M. M. Bayas1, and W. D. Torres1
1 State University Península de Santa Elena, 7047 Santa Elena, Ecuador
{rrovira,mbayas,wtorres}@upse.edu.ec
2 Vinnytsia National Technical University, Vinnytsia, Ukraine
v.m.dubovoy@gmail.com, umcmasha@gmail.com
Abstract. Creating high-precision and reliable control systems with elements
of artiﬁcial intelligence is a relevant problem. In existing works, this issue has
been considered. However, the important and urgent task, not yet solved, is to
reduce energy consumption in relay control systems with artiﬁcial intelligence
components (RCS AIC) without degrading their stability and quality. The goal
of this work is estimating the energy consumption in the RCS AIC and its use
for the management of the thermal facility. A model of a “smart house” heating
control system has been developed, the dependences of the energy consumption
on the system parameters have been obtained with use of Markov process model
and a method for reducing them has been proposed. An algorithm is proposed
that allows us to work out recommendations on how to change the hydraulic and
temperature parameters of the heating system and the tuning parameters of the
control system.
Keywords: Model of Auto-oscillations  Relay control system
Artiﬁcial intelligence  Smart house  Heating  Markov process
Optimization
1
Introduction
Currently, the task of automating complex processes in the manufacturing sector
requires the creation of control systems of high accuracy and reliability with the ele-
ments of artiﬁcial intelligence, which perform the function of optimal control.
A signiﬁcant number of works are devoted to the creation of control systems with
the elements of artiﬁcial intelligence. For example, the problems of applying artiﬁcial
intelligence in power system [1], home automation [2, 3] etc. are widely discussed.
The problems of constructing effective nonlinear control systems with elements of
artiﬁcial intelligence are considered in [4, 5] and many other works. An important and
urgent task, however unsolved, is reducing energy consumption in relay control sys-
tems with artiﬁcial intelligence components (RCS AIC) without degrading stability and
quality. For RCS it is characteristic the oscillatory processes of the state change of the
control object, which makes it difﬁcult to determine the characteristics of quality and
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_33

stability, and also worsens rapidity and reliability [6]. Therefore, the creation of new
RCS AIC as well as the analysis of the processes that occur in them is important.
This paper considers the problem of estimating of energy consumption in the
RCS AIC and its solution for controlling a thermal object. The structure of the
RCS AIC is shown in Fig. 1.
As an object of research and modeling, consider the heating control system of a
smart house [7].
Let’s assume for simplicity that the house consists of three heated rooms (bedroom,
living room, study) with one heat source H (heating boiler). In each room there are
temperature sensors St and motion detectors Sm ; which ﬁx the presence of people. The
measurement and detection results are processed by a fuzzy controller. The result of the
processing is the signal y, corresponding to the optimal temperature in the rooms,
taking into account the presence of people and the time of a day. At a temperature
below the optimum, the heater switches on. The comparison with the optimal value of y
is performed in a relay with hysteresis. Hysteresis is necessary to ensure the stability of
the system. As a result of hysteresis, the “heating-cooling” process has an oscillatory
character. When people move between rooms, the system will have transitional tem-
perature changes. The characteristics of transient processes depend on the settings of
the fuzzy controller and relay parameters.
The aim of this work is to develop a model of the heating control system and
methods of its optimization to ensure maximum energy efﬁciency.
To achieve the objective, it is necessary to solve the following tasks:
– Substantiate the structural and operational base of the heating fuzzy control system;
– Develop a model of such a system;
– Investigate the dependence of energy consumption on the parameters of the system
and suggest a way to reduce them.
Fig. 1. Structure of the RCS AIC
344
R. H. Rovira et al.

2
Method
2.1
Object of Modeling and Optimizing
At ﬁrst, let us consider a system with a non-fuzzy algorithm of temperature control,
which structure shown in Fig. 2(a).
Since in a system with one heater it is impossible to separately control the tem-
perature of each room, some optimal average temperature is calculated using, for
example, a genetic algorithm [8].
With a fuzzy control algorithm, the fuzzy controller FC performs the functions of
calculating the optimal temperature, the deviation and determining the moment of time
Fig. 2. Structural diagram of the optimal relay SC: (a) with a non-fuzzy controller; (b) with a
fuzzy controller. On the diagram: C - controller; FCs – Sugeno controller, FCz – Zadeh
controller, H - heater; r1 – r3 – rooms; m1 – m3 – signals of motion sensors; t10 – t30 – room
temperatures set; y- average temperature in the rooms; T – time of day.
A Model of Self-oscillations
345

of switching on/off the heater. The structural scheme of the fuzzy temperature control
system is shown in Fig. 2(b).
Due to the large dimensionality of the precondition vector of the fuzzy algorithm, a
fuzzy logical output is performed in a hierarchical scheme using two blocks:
FCs – Sugeno’s fuzzy conclusion;
FCz – Zadeh’s fuzzy calculator.
The FCs output is speciﬁed by the rules base (Table 1). The coefﬁcients of linear
dependencies are chosen taking into account the order of permanence in the rooms and
the inertia of the heating process (the schedule may vary for each family and the time
periods of the day are set individually):
– At night the inhabitants are mostly in the bedroom;
– In the morning they move from the bedroom to the living room;
– In the evening from the study to the living room.
The time T is determined with advancing Ds to the time of the change in the
temperature regime, which depends on the system inertia.
The coefﬁcients aij are chosen in accordance with the conditions:
aij ¼
amax  1;
mj ¼ 1
amax  1;
mj ¼ 0

amax ¼ amin
P
j
aij ¼ 1
8
>
>
>
>
>
<
>
>
>
>
>
:
Thus, when setting up the system, one speciﬁes:
– The time limits of the day: Tnight-morning, Tmorning-day, Tday-evening, Tevening-night.
– The desired temperature with the presence of people in the room: t10, t20, t30.
– The duration of the change in the temperature regime (the transient process - is
determined experimentally).
During the system’s operation, the following are also measured/monitored:
– Temperature in the rooms: t1, t2, t3.
Table 1. Rules base of the FCs output.
T(Δs)
m1 m2 m3 yopt
Morning 0
0
0
a11 Δt10 + a12 Δt20 + a13 Δt30
….
0
0
1
a21 Δt10 + a22 Δt20 + a23 Δt30
….
0
1
0
….
….
0
1
1
….
Day
0
0
0
an1 Δt10 + an2 Δt20 + an3 Δt30
346
R. H. Rovira et al.

– Presence of people: m1, m2, m3.
– Current time: s.
Based on the measured and set temperature and time, the optimum temperature is
determined using the Zadeh generalization principle:
yopt ¼ Yoptðt1  t10; t2  t20; t3  t30; s; T  DsÞ
and decisions are made:
u ¼
1;
y\yopt
0;
y  yopt

2.2
Model
The graph of transitions between the states of the system S (m1, m2, m3) is shown in
Fig. 3.
To develop a model of system processes, let consider the models of its elements.
The process of changing the room temperature is the result of several simultaneous
processes:
– Heat transfer from the heating element to a nearby layer of air in the room;
Fig. 3. Graph of object’s states Si, Si = [m1, m2, m3], Pij(T) – is the probability of transitions
between states
A Model of Self-oscillations
347

– The accumulation of thermal energy and an increase the average temperature in the
room;
– Heat energy transfer through the walls from the room (heat losses).
The oscillatory process of maintaining the temperature in the steady state is shown
in Fig. 4.
As noted above, the oscillations are caused by the object’s inertia and hysteresis.
The width of the hysteresis loop 2D determines the amplitude of the oscillations, and
the inertia is the period T ¼ s1 þ s2, where s1 - is the duration of heating and s2 - is
the duration of cooling.
Transfer function of the object in cooling mode is
Wt0 ¼
Wc
Ks þ Wc
ð1Þ
from which we obtain Laplace equation
L t s
ð Þ
½
 ¼ Wt0ðsÞ  L t0 s
ð Þ
½

ð2Þ
where t0 - is the temperature of the environment; L - Laplace transform.
In the heating mode we have the system transfer functions of the object:
Wt0 ¼
Wc
Ks þ Wc þ WH
WtH ¼
WH
Ks þ Wc þ WH
(
ð3Þ
Fig. 4. Stationary process of temperature change
348
R. H. Rovira et al.

from which we obtain Laplace equation
L t s
ð Þ
½
 ¼ WtHðsÞ  L tH s
ð Þ
½
  Wt0ðsÞ  L t0 s
ð Þ
½

ð4Þ
where tH - heater temperature.
Let us ﬁnd the transfer functions Wc and WH. Model of the heat transfer process
from the heating element is
dtcl
ds ¼ kHðtH  tclÞ
or
dtcl
ds þ kHtcl ¼ kHtH
ð5Þ
from which we obtain transfer function of heater
WHðsÞ ¼
1
1
kH s þ 1
ð6Þ
where tcl - is the temperature in the zone of the room closest to the heater; kH - heat
transfer coefﬁcient of the heating element.
Model of heat transfer through walls (cooling process)
dt0
cl
ds ¼ klðt0  t0
clÞ;
or
dt0
cl
ds þ klt0
cl ¼ klt0;
ð7Þ
from which we obtain cooling transfer function
WcðsÞ ¼
1
1
kl s þ 1 ;
ð8Þ
where t0
cl - the temperature in the zone of the room closest to the walls; kl - loss factor
due to heat transfer to the external environment.
Substituting (8) into (1), we obtain
Wt0ðsÞ ¼
1
K
kl s2 þ Ks þ 1
ð9Þ
whence, taking into account (2), we obtain a model in the cooling regime in the form of
a differential equation
K d2t
ds2 þ Kkl
dt
ds þ klt ¼ klt0
ð10Þ
A Model of Self-oscillations
349

Similarly, for the heating mode
Wt0 ¼
kls þ klkH
Ks3 þ Kðkl þ kHÞs2 þ ðkl þ kHÞs þ 3klkH
WtH ¼
kHs þ klkH
Ks3 þ Kðkl þ kHÞs2 þ ðkl þ kHÞs þ 3klkH
8
<
:
from which, taking (4) into account, we obtain the equation
K d3t
ds3 þ Kðkl þ kHÞ d2t
ds2 þ ðkl þ kHÞ dt
ds þ 3klkHt ¼ kH
dt0
ds  kl
dtH
ds
ð11Þ
Obviously, at the beginning of modeling process the initial conditions are zero, and
the simulation is performed ﬁrst in accordance with Eq. (11) until condition
t s1
ð
Þ ¼ y0 þ D:
ð12Þ
Next, the simulation is performed in accordance with Eq. (10) until meeting the
condition
t s2
ð
Þ ¼ y0  D;
ð13Þ
Moreover, the initial conditions correspond to the ﬁnal conditions of the previous
interval (condition of continuity). Then the simulation is performed alternately in
accordance with the models (10) and (11).
2.3
Computer Simulation
The simulation algorithm is implemented in the SciLab/Xcos system. The result is the
process of temperature change shown in Fig. 5 for different model parameters. Sim-
ulation shows that the amplitude of temperature ﬂuctuations always exceeds the hys-
teresis of the relay (shown in Fig. 5 by straight lines Y ¼ Y0  D and Y ¼ Y0 þ D).
The process of heating and maintaining the temperature consumes thermal energy.
In the “heating-cooling” cycle during cooling, the thermal energy is lost, transmitted
off-site; and during heating, apart from losses, energy is expended in raising the
temperature. If we assume that the temperature of the heater is proportional to its
power, and the energy is consumed during the heating stage, then the average energy
consumption in the cycle is:
Pav ¼ gtH
s1
s1 þ s2
ð14Þ
The developed model and the executed modeling allow to formulate the problem of
optimization of the system parameters in order to minimize energy consumption:
350
R. H. Rovira et al.

- criterion of optimization –
min Pav
½
 ¼ min gtH
s1
s1 þ s2


;
ð15Þ
- constraints –
tmax  tmin  Dmax
Pmin  gtH  Pmax
tmin  y0  tmax
8
>
<
>
:
;
ð16Þ
Fig. 5. Results of modeling the heating system in the SciLab/Xcos system
A Model of Self-oscillations
351

- optimization parameters –
kH; D; tH
f
g:
ð17Þ
In the control system of such an object, the fuzzy controller calculates the optimal
average temperature in the house yopt and compares it with the current average tem-
perature in the rooms. The result is fed to the heater on/off relay.
We estimate the consumption of thermal energy taking into account the probabil-
ities of the Pij(T) transitions between the states Si m1; m2; m3
ð
Þ and Sj m1; m2; m3
ð
Þ
(Fig. 3).
Obviously, the process of changing the state of the system is a Markov process with
a transition matrix PðsÞ ¼
Pijðs; i ¼ 1::8; j ¼ 1::8


. As the initial state can be taken
S4 m1 ¼ 1; m2 ¼ 0; m3 ¼ 0
½
 at T ¼ «night», where m1 ¼ 1 is the presence of people
in the bedroom. So the initial state probability vector is
PS0 ¼ 0; 0; 0; 1; 0; 0; 0; 0
½

and the probability vector of subsequent states
P
0
s ¼ PT
s  PðTÞ
ð18Þ
Where T - is the sign of transposition.
In transitional regimes when yopt increases, the heating step is prolonged (in-
creases s1), and when decreases yopt with cooling, the cooling stage lengthens (in-
creases s2). In connection with the asymmetry of the process ðs1 6¼ s2Þ changes in state
lead to additional energy losses.
3
Results
Based on the developed model, it is possible to propose the algorithm for modeling the
system, predicting average energy consumption and optimizing parameters:
1. Set the values of constants
2. Set current values of custom parameters
3. Set the forecast horizon smax
4. Set the system status, time of day and desired indoor
temperatures
5. Assess the current indoor temperature
6. Calculate the current average temperature y ¼ Favðt10; t20; t30Þ
7. Determine
yopt ¼ Fðm1; m2; m3; t10; t20; t30Þ
using
Sugeno’s
Fuzzy
inference
8. If yopt  y [ D we then estimate the duration of the transient
process and the energy expenditure on the transient process
9. Using model (17) estimate the period of self-oscillations
ðDs ¼ s1 þ s2Þ
352
R. H. Rovira et al.

10. s ¼ s þ Ds
11. Using
the
model
(18)
we
estimate
the
self-oscillation
parameters
12. Using (13) we estimate the average energy expenditure for the
period of self-oscillations
13. Estimate the probability of the next state
14. If s\smax, then go back to 6
15. Estimate
the
average
energy
consumption
during
the
simulation.
16. Make a step of changing the parameters in accordance with the
selected optimization method
17. Repeat steps 2-15
18. If the optimum is not reached, then go back to 16.
19. Display the results of optimization
20. end.
The proposed algorithm makes it possible to develop recom-
mendations on how to change the hydraulic and temperature
parameters of the heating system and the control parameters of
the control system.
4
Conclusion
The model of the process of self-oscillations in the heating system and methods for
optimizing the control system with a fuzzy controller makes it possible to increase the
energy efﬁciency of the system. The algorithm for modeling the energy consumption in
relay control systems of smart home with artiﬁcial intelligence components, predicting
average energy consumption and optimizing parameters based on the developed model
was proposed. It seems advisable to direct further research to develop training tools for
the fuzzy inference system in the process of practical use.
References
1. Zhao, X., Zhang, X.: Artiﬁcial intelligence applications in power system. In: Advances in
Intelligent Systems Research, vol. 133, pp. 158–161 (2016)
2. Rama Krishna, A.V.V., Sukanya Devi, Ch., RajaSneha, P.: Home automation using remote
control system. Int. J. Eng. Res. Sci. 2(9), 84–91 (2016)
3. Das, S.K., et al.: The role of prediction algorithms in the MavHome smart home architecture.
IEEE Wirel. Commun. 9(6), 77–84 (2002)
4. Jaszczak, S., Kolodziejczyk, J.: An industrial fuzzy PID autotuner based on the relay method.
In: ICAISC 2014, Part I. LNAI, vol. 8467, pp. 193–204 (2014)
5. Liu, J., Wang, W., Golnaraghi, F., Kubica, E.: A novel fuzzy framework for nonlinear system
control. Fuzzy Sets Syst. 161(21), 2746–2759 (2010). https://doi.org/10.1016/j.fss.2010.
04.009
A Model of Self-oscillations
353

6. Dubovoi, V.M., Yukhymchuk, M.S., et al. Evaluation of uncertainty of control by
measurement with logical conditions. In: Proceedings of SPIE 10031, Photonics Applications
in Astronomy, Communications, Industry, and High-Energy Physics Experiments 2016,
100314F (2016). https://doi.org/10.1117/12.2248871
7. Nacer, A., Marhic, B., Delahoche, L.: Smart home smart HEMS smart heating: an overview
of the latest products and trends. In: 2017 6th International Conference on Systems and
Control (ICSC), pp. 90–95 (2017). ISSN 2379-0067
8. Bayas, M., Dubovoy, V.: Efﬁcient resources allocation in technological processes using
genetic algorithm. Middle-East J. Sci. Res. 14(1), 01–04 (2013)
354
R. H. Rovira et al.

Computer Vision-Based Method for Automatic
Detection of Crop Rows in Potato Fields
Iván García-Santillán(&), Diego Peluffo-Ordoñez, Víctor Caranqui,
Marco Pusdá, Fernando Garrido, and Pedro Granda
Department of Software Engineering, Faculty of Applied Sciences,
Universidad Técnica del Norte, Ibarra, Ecuador
{idgarcia,dhpeluffo,vmcaranqui,mrpusda,jfgarridos,
pdgranda}@utn.edu.ec
Abstract. This work presents an adaptation and validation of a method for
automatic crop row detection from images captured in potato ﬁelds (Solanum
tuberosum) for initial growth stages based on the micro-ROI concept. The crop
row detection is a crucial aspect for autonomous guidance of agricultural
vehicles and site-speciﬁc treatments application. The images were obtained
using a color camera installed in the front of a tractor under perspective pro-
jection. There are some issues that can affect the quality of the images and the
detection procedure, among them: uncontrolled illumination in outdoor agri-
cultural environments, different plant densities, presence of weeds and gaps in
the crop rows. The adapted approach was designed to address these adverse
situations and it consists of three linked phases. The main contribution is the
ability to detect straight and curved crop rows in potato crops. The performance
was quantitatively compared against two existing methods, achieving acceptable
results in terms of accuracy and processing time.
Keywords: Crop row detection  Autonomous guidance  Image segmentation
Computer vision
1
Introduction
Artiﬁcial vision systems installed on autonomous tractors are useful tools for per-
forming crop rows detection [1] as well as site-speciﬁc treatments application [2],
including the removal of weeds that are located outside the crop rows. In such type of
vehicles, the navigation is mainly based on Global Positioning Systems (GPS) [3].
However, when small deviations occur in navigation, the detection of crop rows
becomes crucial to accomplish a correction process [4] in site. Agricultural environ-
ments are affected by several adverse situations that inﬂuence the crop rows detection
process, such as: uncontrolled lighting in outdoor agricultural environments; discon-
tinuities in crop rows due to defects in sowing or germination; weeds with green colors
similar to crops; variety of heights and volumes of plants due to the growth stages;
curved crop rows that may be created in certain irregular and/or rugged ﬁelds.
Several approaches based on image analyses (e.g. Hough transform, linear
regression, horizontal stripes, region analysis, ﬁltering, stereo vision, green pixel
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_34

accumulation) have been mainly conducted for detection of straight crop rows [1, 5, 6]
and to a lesser extent for curved crop rows [7, 8]. Indeed, some researches have opted
by combining many approaches with a prior knowledge included through different
constraints in order to decrease the algorithmic complexity, enabling them for real time
applications. Based on the trends and studies reported in recent literature, this work
aims to adapt and validate a computer vision method based on micro-ROI [9] for
automatic detection of crop rows in potato ﬁelds, superchola variety (Solanum
tuberosum), at initial stages of growth (  40 days). The adaptation of the original
method (validated in maize ﬁelds) to another type of crop (potato ﬁelds) with other
phenological and physiological characteristics constitutes the main contribution of this
research.
2
Materials and Methods
2.1
Image Set
Images were acquired in a potato ﬁeld from Ibarra-Ecuador during January and
February 2017. The ﬁeld is non-regular, exhibiting slopes up to 8°. Images were
acquired under different lighting conditions and growth stages (  40 days), as shown
in Fig. 1. Some instances of images are: (a) plants of different sizes in a sunny day,
(b) low weed presence in a dark day, and (c) discontinuities into crop rows in a clear
day. The crop rows were spaced, an average, 0.90 m and curved furrows were oriented
to the left. A total of 320 images were randomly selected for experimentation.
The images were obtained under perspective projection with a goPro Hero 3+
Black Edition color camera with a focal length of 3 mm. The camera was installed on
the front of a New Holland TD90 tractor and located at a height between 1.80 m and
2 m from the ground with an inclination between 40° and 50°, as shown in Fig. 2.
The images were stored in a JPG format with a resolution of 3000  2250 pixels
(7 Mpx). The size and location of the region of interest (ROI) were established to
detect 4 crop rows [1], containing sufﬁcient resolution for detecting objects of interest
[10]. The length of the ROI was set to 5 m (in the potato ﬁeld) and the ROI have a
resolution of 2000  650 pixels (width  length) in the projected 2D image (Fig. 2b).
Fig. 1. Instances of the image set processed by the adapted approach in potato ﬁelds, namely:
(a) plants of different sizes in a sunny day, (b) low weed presence in a dark day, and
(c) discontinuities into crop rows in a clear day.
356
I. García-Santillán et al.

The images were processed using Matlab R2015a software [11] executing on an Intel
Core i7 2.0 GHz processor (4th generation), 8 GB RAM and Windows 8.1 Pro
(64-bit).
2.2
Outline of the Method
Adapted approach is based on [9], which was validated in maize ﬁelds, and consists of
three stages: (i) image segmentation, (ii) starting point’s identiﬁcation, and (iii) crop
rows detection.
2.2.1
Image Segmentation: It stage consists of 4 steps as explained below:
(a) ROI identiﬁcation: As previously mentioned (regarding Fig. 2b), the ROI was
selected containing 4 crop rows with a suitable resolution.
(b) Identiﬁcation of greens: Given the evidence of its good results reported in lit-
erature, the index ExG (Excess Green) described in [12] was used. Its standard
formulation is given in Eq. (1). Figure 3(a) shows the ROI within a grayscale
image where ExG index was applied.
ExG ¼ 2g  r  b
ð1Þ
(c) Double thresholding: A ﬁrst thresholding process, based on [13], was applied to
separate vegetation (crop and weed) and soil. Then, a second thresholding was
Fig. 2. Acquisition process: (a) Vision system installed on tractor; (b) Location of the region of
interest (ROI) into an image
Fig. 3. Examples of ROI images: (a) ExG; (b) double thresholding; and (c) morphological
operations.
Computer Vision-Based Method for Automatic Detection of Crop Rows
357

applied to the vegetation pixels to separate the crop and weeds. Figure 3(b) shows
the ROI as a binary image containing the potato crop in white pixels, while soil in
black pixels.
(d) Morphological operations: morphological opening [14] and 5  5 majority ﬁlter
were applied to remove isolated pixels, and obtaining cleaner binary images for
further processing (Fig. 3c).
2.2.2
Starting Point’s Identiﬁcation: This stage aims to locate the starting
points on the ROI, which are the basis for searching the crop rows.
It works as follows:
(a) The ROI is divided into two same-sized horizontal strips: lower and higher (see
Fig. 4).
(b) The Hough transform (HT) [15] is applied in the lower ROI strip to identify linear
segments of white pixels. To do so, some constraints are assumed by a priori
knowledge in order to decrease the computational complexity, such as the ori-
entation of lines between −45° and 45°, four peaks in the Hough polar space (rho,
theta) with a resolution of 1 pixel and 1°, respectively.
(c) Four peaks (lines) are identiﬁed in the Hough polar space, which in turn determine
4 parameters m (slope) and b (intercept) associated with the 4 linear segments.
The points of intersection between the lines detected with TH and the lower edge
of ROI determine the initial 4 points, as can be observed in Fig. 4 (red points).
Likewise, the slope of each linear segment indicates the direction to start the
search of the plants and then the crop rows within the ROI.
2.2.3
Crop Rows Detection: This stage encompasses three processes:
(a) extraction of candidate points, (b) regression analysis
and (c) crop rows selection
(a) Extraction of candidate points: The ROI (depicted in Fig. 3c) is divided into 10
same-sized horizontal substrips (see Fig. 5), unlike the 12 substrips of the original
method [9]. This number was established after trial and error tests with the cur-
vature of the crop rows in the images considered for experiments. Thus, the lower
Fig. 4. Location of 4 stating points in the lower ROI strip.
358
I. García-Santillán et al.

and upper ROI strips contain four and six substrips, respectively. The curved lines
appear more pronounced at the top of the ROI because of the perspective pro-
jection, so the number of substrips (six) enables to capture such variability
(curvature) more precisely.
Crop rows are extracted sequentially from left to right, exploring the ten substrips
vertically from bottom to top of the ROI, based on the concept of micro-ROIs, starting
from the associated starting point (Fig. 4) and with the slope of the straight line
estimated with TH, used as a guidance (direction). Consequently, a micro-ROI deter-
mines a small rectangular region (window). The height of each micro-ROI is the same
as that of the corresponding substrip, while the width varies in each substrip due to
perspective projection, being initially ﬁxed at 160 pixels (0.30 m in the ROI base),
gradually decreasing at the rate of 6% as measured that ascends by the substrips. The
objective of each micro-ROI is to include the maximum number of potato plants,
leaving mostly weeds that are located in the inter-row spaces. Each micro-ROI is
deﬁned by four parameters [x, y, width, height], where the point (x, y) represents the
upper left corner. The parameters y, width, and height are beforehand known once the
horizontal substrips are established (Fig. 5), while x is unknown and is obtained
dynamically for each substrip as follows:
1. The ﬁrst micro-ROI is located with the base centered at the ﬁrst initial point
(Fig. 4). The geometric center of the micro-ROI is calculated [16], obtaining P1
x1; y1
ð
Þ, Fig. 6 (yellow cross). This expression is less sensitive to noise and mini-
mizes the effects of isolated pixels belonging to the weeds located in the space
between cultivation lines.
2. Following the estimated line of reference with TH (Fig. 4), three additional
micro-ROIs are placed with the bases centered at the points of intersection between
the reference line (HT) and the lower edges of the substrips, Fig. 7.
In the upper strip of the ROI, which contains six substrips (Fig. 5), each micro-ROI
is placed centered on the point of intersection between the lower edge of the corre-
sponding substrip and the estimated straight line with the four previously calculated
centroids, i.e. with the points based on record that deﬁne the trend of the crop rows. The
straight line is adjusted using least squares linear regression as follows:
Fig. 5. The ROI is divided into ten substrips of equal size. Four in the lower ROI and six in the
upper strip.
Computer Vision-Based Method for Automatic Detection of Crop Rows
359

3. The ﬁfth micro-ROI, Fig. 8, is located considering the last four centroids (P1, P2,
P3, P4), from which a straight line is ﬁtted, obtaining the corresponding slope
m1,2,3,4. The intersection point (red cross) in the next substrip (ﬁfth) is obtained
using the last centroid P4 and the slope m1,2,3,4.
Fig. 6. Location of the ﬁrst micro-ROI in the ﬁrst substrip, where the red cross is the center of
the micro-ROI at the base and the yellow cross is the centroid P1.
Fig. 7. Location of the fourth micro-ROI, where the yellow cross is the centroid P4.
Fig. 8. Location of the ﬁfth micro-ROI, where the yellow cross is the centroid P5.
360
I. García-Santillán et al.

4. The centroid P5 (yellow cross) is calculated within the micro-ROI as before.
5. Steps 3 and 4 apply to other micro-ROIs within the upper strip of the ROI, Fig. 9.
For example, if centroid P10, is considered, the line is adjusted considering the four
previous centroids (P6, P7, P8, P9) with the slope m6,7,8,9. Thus, the intersection
point in the next substrip is obtained using centroid P9 with slope m6,7,8,9.
6. Once the ten substrips are processed, a set of 10 points distributed along the ﬁrst
crop row is obtained, object of detection.
The complete procedure (steps 1–6) is repeated for other available starting points
(Fig. 4). Figure 10 shows the result of this process on the four crop rows. The can-
didate points obtained constitute the entry for the subsequent regression analysis
process.
Fig. 9. Location of the tenth micro-ROI, where the yellow cross is the centroid P10.
Fig. 10. Location of micro-ROIs (blue rectangles) along crop rows that cross the top and bottom
edges of the ROI.
Computer Vision-Based Method for Automatic Detection of Crop Rows
361

(b) Regression Analysis: With the candidate points obtained in the previous process,
polynomials of degree one (straight line) and two (quadratic curve) are adjusted
for each crop row using the least squares technique. The coefﬁcients to be esti-
mated for the straight line are the slope (m) and the intercept (b); while for
quadratic polynomial are coefﬁcients a, b and c, Eq. (2).
y ¼ mx þ b;
y ¼ ax2 þ bx þ c:
ð2Þ
Figure 11(a) shows the two polynomials (straight and quadratic) obtained for each
crop row within the ROI. The least squares technique also provides the norm of
residues (R), which is a measure of the quality of the ﬁtting. The smaller the R, the
better the adjustment performed. This measurement is used in the following process for
the selection of the best lines.
(c) Crop row selection: The selected polynomial (straight or quadratic) is the one
that best ﬁts each crop row based on the norm of residues (R), i.e. the line with the
lowest R value, Fig. 11b. As a ﬁnal result, the method detects 4 crop rows (curves
and/or straight) and mathematically modeled (Eq. 2).
The robustness of the adapted method is complemented by three veriﬁcation pro-
cedures: (i) the number of lines detected must be equal to four; (ii) the separation of
crop rows at the base of the ROI does not exceed 600 pixels, which was set by
experimentation; (iii) there is no intersection between the crop rows detected within the
ROI. In case of any anomaly, the image is rejected and next one is processed.
Regarding the crop row curvature, the method was designed for curvatures toward
left. However, when the curvature is oriented to the right (Fig. 12a), the method works
by applying to the ROI a vertical specular reﬂection operation (Fig. 12b) and horizontal
translational operations on the points identiﬁed along the crop rows (Fig. 10) [7]. In
this way, the algorithm can work with crop rows with curvatures oriented left or right,
but not both simultaneously.
Fig. 11. Polynomials obtained for each crop row within the ROI. (a) Straight and quadratic
lines; (b) selection of lines that ﬁtted better.
362
I. García-Santillán et al.

3
Results and Discussion
The performance of the adapted approach, here-named DBMR (Detection Based on
Micro-ROI) -maintaining the original nomenclature from [9]- was quantitatively
compared against 2 existing methods, which are capable of detecting both straight and
curved crop rows: (i) Template matching with Global Energy Minimization (TMGEM)
in order to detect regular patterns and determine an optimal crop model using dynamic
programming [8]; (ii) Detection for accumulation of green pixels (DAGP) for exploring
candidate alignments of green pixels that deﬁne straight and quadratic crop rows [7].
The two existing methods were selected for comparison purposes because, as far as is
known, there are no other methods available with such capabilities for detecting
straight and curved crop rows.
The CRDA measure (Crop Row Detection Accuracy), proposed in [8], was used to
evaluate the performance of the methods, whose value is between [0, 1], where the best
score is the unit. CRDA measures the coincidence between the horizontal coordinates
of each crop row, obtained by each method under evaluation (DBMR, TMGEM, and
DAGP) and the corresponding values of the ground truth. The ground truth was
manually created by an expert based on visual observation, where at least ﬁve points
were selected on each crop row and squared curves were ﬁtted using the curve ﬁtting
toolbox incorporated into Matlab [11]. In total, 320 images were randomly selected,
containing straight and curved crop rows, spaced both regularly and irregularly.
Irregular spacing of crop rows usually occurs when the tractor crosses the crop several
times, displacing soil in each pass (as can be noticed from Fig. 13d).
Figure 13 shows some examples of the processed images by the DBMR adapted
method with the respective CDRA metrics. In Fig. 13(a) and (b) with straight crop
rows; Fig. 13(c) to (f) with curved crop rows; (g) and (h) a mixture of curved (red) and
straight (blue) crop rows.
As can be noted in Fig. 13(a) and (f), some crop rows may contain discontinuities
caused by defects in planting or germination. In this study, the gaps do not exceed
1.20 m of discontinuity in the same furrow according to the vision system conﬁgu-
ration. A longer length may result in a fault in the crop row detection. In addition,
weeds are often distributed irregularly into the inter-row spaces, as can be appreciated
in Fig. 13(a) and (b). For experimental purposes, the method was tested with a low
weed level, i.e. up to 5% of weed cover according to the classiﬁcation scale proposed in
[17]. A higher level of weed may lead to incorrect crop row detection. Both factors,
Fig. 12. (a) ROI with crop rows oriented to the right in potato crops. (b) ROI after applying a
vertical specular reﬂection operation.
Computer Vision-Based Method for Automatic Detection of Crop Rows
363

crop row discontinuity and weed level affect the performance of the DBMR adapted
method (see Fig. 13a).
Table 1 shows the average percentage detection rate for the 320 analyzed images
with curvatures oriented toward left or right. From the results, it can be seen that
DBMR exceeds to the two existing methods TMGEM and DAGP in potato crops,
which is in agreement with the results reported in [9] in maize crops. The adapted
method DBMR ﬁts well to the curvature of potato crop rows using 10 substrips
(Fig. 5), unlike the 12 substrips used in [9]. This is due to the fact that the minimum
radius of curvature in potato crops was greater than those of the maize crop, which was
19 m.
Fig. 13. Examples of the set of images in potato ﬁelds containing straight and curved crop rows
detected by the DBMR adapted method.
364
I. García-Santillán et al.

The DAGP and DBMR methods were implemented in Matlab, while TMGEM in
C++. Table 2 shows the average processing time for the complete process of the
adapted method DBMR, which was 694 ms, measured in Matlab under an interpreted
programming language that is slower than a compiled (e.g. C++). So, the method
shows potential for real-time applications, which is an aspect to be studied in further
researches. The increase of execution time when running DBMR with respect to that
reported in [9] can be attributed to the fact that the potato crop is leaﬁer than maize
crop, so it requires the processing of a greater number of pixels during the involved
stages (segmentation, identiﬁcation of starting points, crop row detection).
According to the working conditions of the DBMR adapted method, its application
requires that three main input parameters are beforehand known, namely: (i) number of
crop rows to be detected, (ii) crop row concavity, and (iii) the intrinsic and extrinsic
parameters of the visual system.
Finally, as a future work, following issues are to be considered: (i) the imple-
mentation of DBMR using a compiled programming language running on a real-time
operating system and platform, following the ideas of the RHEA (2014) project.
(ii) The implementation of the DBMR method using parallel programming techniques
would be advisable in order to reduce the processing time for real-time application.
(iii) Test DBMR method over other crops that are important in the northern region of
Ecuador, such as: bean, pea and onion.
4
Conclusions
This work presents an adaptation of a computer vision method, based on micro-ROIs
[9], here-called DBMR, for detection of straight and curved crop rows (oriented toward
left or right, but not simultaneously) in potato ﬁelds, superchola variety (Solanum
tuberosum), in initial stages of growth (  40 days). Adapted approach consists of 3
stages, namely: segmentation, identiﬁcation of starting points, and crop rows detection.
Table 1. Comparison of the adapted method DBMR against TMGEM and DAGP regarding
CRDA measure, whose value is between [0, 1], being the unit the best score.
Detection rate (%) TMGEM DAGP DBMR
Mean value
0.627
0.860
0.871
Position
3
2
1
Table 2. Average execution time in milliseconds (ms) for the three methods: TMGEM, DAGP,
and DBMR adapted method.
Method
Execution time ROI size
Programming language
TMGEM 4836
640  480 C++
DAGP
785
2000  650 Matlab
DBMR
694
2000  650 Matlab
Computer Vision-Based Method for Automatic Detection of Crop Rows
365

To evaluate the performance of the algorithm, the CRDA measure is used, which
reaches 1 as highest score. The DBMR adapted method works well in the presence of
low weed levels (CRDA > 0.87) involving processing times less than 694 ms,
exhibiting a potential for real-time applications.
References
1. Gonzalez-de-Santos, P., Ribeiro, A. (eds.): Proceedings of the Second International
Conference on Robotics and Associated High-Technologies And Equipment for Agriculture
and Forestry: New Trends in Mobile Robotics, Perception and Actuation for Agriculture and
Forestry, RHEA (2014)
2. Gée, C., Bossu, J., Jones, G., Truchetet, F.: Crop/weed discrimination in perspective
agronomic images. Comput. Electron. Agric. 60(1), 49–59 (2008)
3. Emmi, L., Gonzalez-de-Soto, M., Pajares, G., Gonzalez-de-Santos, P.: New trends in
robotics for agriculture: integration and assessment of a real ﬂeet of robots. Sci.
World J. 2014, 21 pages (2014). Article ID 404059
4. Rovira-Más, F., Zhang, Q., Reid, J.F., Will, J.D.: Machine vision based automated tractor
guidance. Int. J. Smart Eng. Syst. Des. 5(4), 467–480 (2003)
5. Montalvo, M., Pajares, G., Guerrero, J.M., Romeo, J., Guijarro, M., Ribeiro, A., Cruz, J.M.:
Automatic detection of crop rows in maize ﬁelds with high weeds pressure. Expert Syst.
Appl. 39(15), 11889–11897 (2012)
6. Guerrero, J.M., Guijarro, M., Montalvo, M., Romeo, J., Emmi, L., Ribeiro, A., Pajares, G.:
Automatic expert system based on images for accuracy crop row detection in maize ﬁelds.
Expert Syst. Appl. 40(2), 656–664 (2013)
7. García-Santillán, I., Guerrero, J., Montalvo, M., Pajares, G.: Curved and straight crop row
detection by accumulation of green pixels from images in maize ﬁelds. Precision Agriculture
(2017). https://doi.org/10.1007/s11119-016-9494-1
8. Vidovic, I., Cupec, R., Hocenski, Z.: Crop row detection by global energy minimization.
Pattern Recogn. 55, 68–86 (2016)
9. García-Santillán, I., Montalvo, M., Guerrero, M., Pajares, G.: Automatic detection of curved
and straight crop rows from images in maize ﬁelds. Biosyst. Eng. 156, 61–79 (2017). https://
doi.org/10.1016/j.biosystemseng.2017.01.013
10. Pajares, G., García-Santillán, I., Campos, Y., Montalvo, M., Guerrero, J.M., Emmi, L., et al.:
Machine-vision systems selection for agricultural vehicles: a guide. J. Imag. 2, 34 (2016)
11. MathWorks, Inc. (2015). http://www.mathworks.com/products/new_products/release2015a.
html
12. Sogaard, H.T., Olsen, H.J.: Determination of crop rows by image analysis without
segmentation. Comput. Electron. Agric. 38(2), 141–158 (2003)
13. Otsu, N.: Threshold selection method from gray-level histograms. IEEE Trans. Syst. Man
Cybern. 9(1), 62–66 (1979)
14. Onyango, C.M., Marchant, J.A.: Segmentation of row crop plants from weeds using colour
and morphology. Comput. Electron. Agric. 39(3), 141–155 (2003)
15. Hough, P.: Method and means for recognizing complex patterns. Patente 3069654 (1962)
16. Gonzalez, R., Woods, R.: Digital Image Processing, 3rd edn. Pearson/Prentice Hall, Upper
Saddle River (2010)
17. Maltsev, A.I.: Weed Vegetation of the USSR and Measures of its Control. Selkhozizdat,
Leningrad-Moscow (1962). (in Russian)
366
I. García-Santillán et al.

A Finger-vein Biometric System Based
on Textural Features
Enrique V. Carrera1(B), Santiago Izurieta1, and Ricardo Carrera2
1 Departamento de El´ectrica y Electr´onica, Universidad de las Fuerzas Armadas
ESPE, Sangolqu´ı 171103, Ecuador
evcarrera@espe.edu.ec
2 Colegio de Ciencias e Ingenier´ıa, Universidad San Francisco de Quito,
Cumbay´a 170901, Ecuador
rvcarrera@estud.usfq.edu.ec
Abstract. Biometric systems are being widely used today for automated
authentication purposes. In particular, vascular biometrics or vein recog-
nition is receiving a large amount of attention because of its several
advantages related to security and convenience. However, images con-
taining vein patterns normally include more information than just those
structural arrangements. Thus, we propose a ﬁnger-vein biometric system
based exclusively on textural features to evaluate the usefulness of the
remaining information around vein patters. Textural features are obtained
through gray-level co-occurrence matrices from the wavelet detail coef-
ﬁcients belonging to ﬁnger-vein images. The evaluation of the proposed
biometric system is based on a standardized ﬁnger-vein database and its
results show favorable improvements on the ﬁnger-vein authentication
accuracy when textural features are incorporated in the biometric process.
Keywords: Finger-vein biometry · Digital image processing
Machine learning algorithms
1
Introduction
Biometric authentication provides high-level security by identifying people based
on physiological and behavioral characteristics. A biometric system operates by
acquiring biometric data from each person, extracting features and comparing
them with template sets in its internal database [1]. Diﬀerent techniques are
used for biometric authentication, such as faces, ﬁngerprints, irides, etc.
Vascular biometrics or vein recognition has received a lot of attention recently
and is viewed as a promising biometric trait [2]. Researchers have determined that
the vascular system of humans is unique for each person, diﬃcult to forge, not
aﬀected by race or skin discolorations, and does not change as people age. More-
over, ﬁnger-vein biometry is not-invasive (contactless), easy and fast for extracting
features, and only works with live bodies [3,4]. Thus, ﬁnger-vein biometric systems
could lead biometric technology in terms of security and convenience [5].
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_35

368
E. V. Carrera et al.
Since ﬁnger-vein images are captured through infrared light, the images used
for authentication purposes contains more than just vein patterns; they also
include shades produced by various thickness of the ﬁnger muscles, bones, and
tissue networks surrounding the veins (some examples of ﬁnger-vein images are
presented in Fig. 1). Thus, it is not clear yet if vein patterns are the only inter-
esting characteristics to consider in a ﬁnger-vein biometric algorithm [6].
Moreover, textural features have been proposed as important descriptors to
identify objects or regions of interest in images [7,8]. In fact, texture provides
meaningful features for describing pictorial information and humans use it to
interpret visual information. Hence, we are interested in understanding the use-
fulness of textural features as the main characteristics employed by a ﬁnger-vein
biometric system.
Based on that, we have proposed a ﬁnger-vein biometric system based exclu-
sively on textural features. No segmentation nor structural analysis of the vein
patterns is included in the biometric process. The textural features used in
this work were obtained through gray-level co-occurrence matrices from the
wavelet detail coeﬃcients belonging to ﬁnger-vein images. Furthermore, the pro-
posed biometric system was evaluated using a standardized ﬁnger-vein database,
namely SDUMLA-HMT [9], in order to determine the usefulness of textural fea-
tures for authentication tasks.
Fig. 1. Sample images in the SDUMLA-HMT [9] ﬁnger-vein database.
Results show a promising expectation for the use of textural features as
relevant characteristics to take into account in any biometric system based on
ﬁnger-vein images. The accuracy reached by our biometric system authenticating
106 people is 91.5%, while the same approach was able to distinguishes a valid
used from a bunch of intruders with an accuracy of 99.7%. In the last case, the
false acceptance rate is as lower as 0.15%.

A Finger-vein Biometric System Based on Textural Features
369
The rest of this paper is organized as follow. Section 2 describes the pro-
posed ﬁnger-vein biometric system. Section 3 presents the most signiﬁcant results
reached through our evaluation process. The implications of these results are
analyzed in Sect. 4. Finally, Sect. 5 concludes this paper.
2
Finger-vein Biometric System
An overview of the proposed ﬁnger-vein biometric system is shown in Fig. 2. Each
one of the blocks shown in that ﬁgure is explained in the following subsections.
Fig. 2. Block diagram of the ﬁnger-vein biometric system.
2.1
Image Database
There is a diﬀerence in the absorption properties of oxygenated and deoxygenated
hemoglobin in human blood to infrared illumination [10]. Thus, ﬁnger-vein images
can be obtained using infrared light and a camera sensitive to wavelengths between
700 and 1000 nm.
In order to standardize our study, we have used the publicly available
ﬁnger-vein images included in the SDUMLA-HMT database [9]. These images
were taken in 2010 at Shandong University using a device designed at Wuhan
University. 106 people, including 61 males and 45 females with age between 17
and 31, participated in the data collecting process. Each person was asked to
provide images of his/her index ﬁnger, middle ﬁnger and ring ﬁnger of both
hands, and the collection for each of the 6 ﬁngers is repeated for 6 times. Every
image is stored in BMP format with 320×240 pixels in size. The total size of the
ﬁnger-vein images is around 0.85 GB, since we have 106 × 6 × 6 = 3816 images.
Some sample images of the SDUMLA-HMT database are shown in Fig. 1.
In this work, we have only used the images corresponding to the right index
ﬁnger of each person (i.e., 106 × 6 = 636 images) for his/her authentication.
2.2
Feature Extraction
Our ﬁnger-vein biometric system extract features using two stages: a wavelet
preprocessing stage to obtain the called detail coeﬃcients, and a second stage
where textural features are computed for each image employing those detail
coeﬃcients.

370
E. V. Carrera et al.
Wavelet Preprocessing.
Each image is passed through a multilevel 2-D
stationary wavelet decomposition of level N using either an orthogonal or a
biorthogonal wavelet. This decomposition generates the coeﬃcients of approx-
imation A, and the coeﬃcients of details H, V , and D (i.e., horizontal, verti-
cal, and diagonal) for each one of the N levels. Since an over-complete discrete
wavelet transform is applied, the coeﬃcients of details are not under-sampled,
allowing us to have matrices of the same size as the original image.
As a result of this preprocessing stage, we have 3 × N matrices containing
the detail coeﬃcients of each ﬁnger-vein image.
Textural Features. Using the detail coeﬃcients (i.e., matrices H, V , D) of
each level of decomposition, we compute the gray-level co-occurrence matrix
according to M =256 gray-levels and four diﬀerent directions: 0, π/4, π/2, and
3π/4 rads. Each of these co-occurrence matrices of size M × M is transformed
to a symmetric matrix by adding it its own transposed matrix. After that, each
symmetric co-occurrence matrix is normalized dividing each element by the sum
of all its elements in the matrix.
The normalized and symmetric co-occurrence matrices, represented by p(i, j),
i, j =1, . . . , M, are then used for generating 8 textural descriptors. Table 1 sum-
marizes the textural descriptors used in this work and the operations involved
in its calculation [11]. Since each matrix of detail coeﬃcients produces 4 co-
occurrence matrices, one for each listed direction, we have a set of 4 values for
each textural descriptor associated to a particular matrix of detail coeﬃcients.
Hence, in order to reduce the total number of features, the textural descriptors
corresponding to the same detail coeﬃcients are summarized in a single value
through its mean value.
Table 1. Textural descriptors based on the symmetric co-occurrence matrix p(i, j)
where i, j =1, . . . , M.
Textural descriptor
Computing equation
Energy

i

j p(i, j)2
Contrast
M−1
n=1 n2 
i

j p(i, j)
|i −j| = n

Correlation

i

j i j p(i, j) −μxμy

/σxσy
Homogeneity

i

j[1 + (i −j)2]−1 p(i, j)
Entropy
−
i

j p(i, j) log[p(i, j)]
Cluster shade

i

j(i + j −μx −μy)3 p(i, j)
Cluster prominence

i

j(i + j −μx −μy)4 p(i, j)
Maximum probability maxi,j p(i, j)
μx = 
i

j i p(i, j)
σx = 
i

j(i −μx)2 p(i, j)
μy = 
i

j j p(i, j)
σy = 
i

j(j −μy)2 p(i, j)

A Finger-vein Biometric System Based on Textural Features
371
As a result of this stage, we have 8 mean textural descriptors for each one
of the 3 detail coeﬃcients at the N levels of decomposition. Thus, the ﬁnal
generated vector has 3×N×8 features for each ﬁnger-vein image using exclusively
the discrete wavelet transform and its gray-level co-occurrence matrices.
2.3
Classiﬁcation
The classiﬁcation of the textural descriptors of each ﬁnger-vein image allows us to
authenticate the person to which that image belongs. For that, any parametric
o non-parametric classiﬁer can be used. In this work, we used two diﬀerent
machine-learning based classiﬁers: discriminant analysis and nearest neighbor
[12]. Because of that, it was needed to split the ﬁnger-vein images in training
and test sets. From the six images collected for each person, we have randomly
selected T images to be part of the training set, while the 6−T remaining images
become part of the test set. In addition, training and test textural descriptors
were normalized using the z-score method, deﬁned as:
z = d −μd
σd
where d is a particular textural descriptor and z is the value presented to our
classiﬁers. This normalization is required since most of the machine learning
algorithms work better with mean cancellation and covariance equalization.
3
Results
We have evaluated our ﬁnger-vein biometric system varying its main parameters:
the classiﬁer algorithm, the mother wavelet and level of decomposition used in
the preprocessing stage, the size of the training and test sets, and the number
of people to authenticate. The results presented below correspond to statistical
measurements of 20 executions.
3.1
Classiﬁer Algorithm and Mother Wavelet
As mentioned before, we have evaluated two classiﬁcation algorithms in this
work [12]:
– Discriminant analysis with linear and quadratic discriminant functions: this
algorithm ﬁts a multivariate normal density to each class, with a diagonal
covariance matrix estimated for every class (i.e., linear) or stratiﬁed by class
(i.e., quadratic).
– Nearest neighbor classiﬁer (kNN): this algorithm uses the euclidean distance
and the majority rule with nearest point tie-break. It was proved in this work
that k greater than 300 produces the best accuracy results, so we have chosen
k=350.

372
E. V. Carrera et al.
In addition, we have tested decomposition ﬁlters based on the Daubechies
(i.e., haar) and Biorthogonal (i.e., bior2.2) wavelet families, both having lineal
phase. In the two cases, we have tried N =3 and N =4 decomposition levels.
For all the mentioned variations, we have computed the accuracy of our
ﬁnger-vein biometric system using 106 people, keeping T =5 randomly selected
images of each index ﬁnger in the training set and just one index-ﬁnger image
per person in the test set. Accuracy results for this particular evaluation are
shown in Table 2.
Table 2. Accuracy of the ﬁnger-vein biometric system.
Classiﬁer
Mother
Accuracy (%)
wavelet
N = 3 N = 4
Linear discriminant
bior2.2 64.2
70.8
haar
76.4
79.3
Quadratic discriminant bior2.2 35.9
39.0
haar
44.3
50.9
kNN
bior2.2 84.9
90.6
haar
87.7
91.5
As we can see, N =4 decomposition levels imply better accuracy than N =3,
and the haar mother wavelet improves slightly the accuracy of the classiﬁer with
respect to the bio2.2 wavelets. Moreover, the kNN classiﬁer is more accurate
and robust than the discriminant analysis classiﬁers. In particular, a quadratic
discriminant function is almost useless for this classiﬁcation task.
3.2
Training Set Size and Number of People
In the previous subsection, we use T =5 randomly selected images, but certainly
the number of images included in the training set will inﬂuence the accuracy of
our system. Thus, we have used T = 4 and T = 5 to compare the accuracy of
the kNN classiﬁer using N =3 and N =4 decomposition levels. Results for these
new variations of parameters are shown in Table 3.
It is clear that using T =4 randomly selected images in the training set, and
the two remaining images in the test set, is not a good option, independently of
the mother wavelet or levels of decomposition. The best result (i.e., 91.5%) is
still obtained for T =5, N =4, and the haar mother wavelet. However, note that
for T =4, the bior2.2 wavelet shows better accuracy results.
Finally, we have set up some experiments where the number of people to
authenticate was reduced. Other people, that are not valid users, are consid-
ered system intruders. Thus, besides accuracy results, we can also compute false
acceptance (FAR) and false rejection (FRR) rates. Table 4 shows these results

A Finger-vein Biometric System Based on Textural Features
373
Table 3. Accuracy of kNN as a function of the training set size.
Training samples Mother wavelet Accuracy (%)
N = 3 N = 4
T = 4
bior2.2
73.6
80.2
haar
72.6
75.5
T = 5
bior2.2
84.9
90.6
haar
87.7
91.5
Table 4. Accuracy of kNN as a function of the number of people.
Valid subjects Mother wavelet Accuracy (%) FAR/FRR (%)
T = 4 T = 5
T = 4
T = 5
1
bior2.2
99.4
99.7
0.3/0.3 0.2/0.2
haar
99.5
99.7
0.3/0.3 0.2/0.2
2
bior2.2
98.7
99.4
0.6/0.5 0.3/0.3
haar
98.9
99.4
0.5/0.5 0.3/0.3
106
bior2.2
80.2
90.6
– / –
– / –
haar
75.5
91.5
– / –
– / –
for the kNN classiﬁer using N =4 decomposition levels and T randomly selected
images in the training set.
We can observe that with a small number of users the accuracy of our bio-
metric system is pretty good (around 99%), specially with T = 5 images in the
training set, when the best accuracy is 99.7%. The critical FAR parameter is
also acceptable (≤0.3%) for T =5.
4
Discussion
All obtained results are quite evident in the sense that textural features are
important for ﬁnger-vein biometric purposes. Speciﬁcally, the proposed biometric
system extracts better textural features with the mother wavelet belonging to
the Daubechies family (i.e., haar). This behavior is consistently shown in all our
evaluations.
The biometric system is also dependent on the size of the training set. More
samples of ﬁnger veins in the training set help to improve the accuracy of clas-
siﬁers. In addition, the kNN classiﬁer showed a signiﬁcantly better performance
than those classiﬁers based on discriminant analysis.
Similarly, the number of decomposition levels used during the wavelet pre-
processing also inﬂuences the accuracy of our biometric system. A large number
of levels improves the authentication accuracy. In our case, N =4 decomposition
levels seem to be enough, since they only supply a slightly improvement over
N =3 decomposition levels.

374
E. V. Carrera et al.
Finally, a more real situation where there are a small number of valid users
and a large set of intruders presents interesting results. In this case, the obtained
accuracy and FAR values are quite acceptable, proving the usefulness of textural
features as biometric parameters.
4.1
Comparison Against Previous Works
Results presented in this paper are pretty good considering that we are only
using textural features. For instance, Trabelsi et al. [13] obtained a predictive
capacity of 87% using the same ﬁnger-vein database and algorithms for region of
interest (ROI) detection and monogenic local binary pattern recognition. In [14],
a 98.8% recognition rate is obtained using ROI detection and matching score-
level fusion on an 8-channel bank of Gabor ﬁlter. In a similar way, Lu et al.
[15] also show a 99.2% accuracy when polydirectional local line binary pattern
is used for extracting line patterns in any orientation.
Several other previous works are discussed in [3] where quite complicated
approaches imply better accuracy rates that those presented here. However, we
can see that our proposal is simpler and more generic that previous works based
on structural pattern recognition. In fact, our proposal does not require any
previous ROI detection or segmentation. Although the simplicity of our proposal,
the accuracy obtained in this work is acceptable.
5
Conclusions
This work shows that textural features are important characteristic that can be
used by ﬁnger-vein biometric systems. Although our implementation does not
include any segmentation preprocessing or structural analysis of the vein pat-
tern, obtained results are signiﬁcant to demonstrate that textural features of
ﬁnger-vein images can authenticate people. This work also proves that textural
features obtained through gray-level co-occurrence matrices from the wavelet
detail coeﬃcients are important discriminant characteristics in ﬁnger-vein bio-
metric systems.
We are planning to include other structural characteristics proposed in previ-
ous works [3–6], besides textural features, to determine the improvement of our
proposal in those approaches. Finally, we believe that textural features can be
successfully applied in several other image classiﬁcation tasks, and that is also
part of our future work.
Acknowledgment. This work was partially supported by the Universidad de las
Fuerzas Armadas ESPE under Research Grant 2015-PIC-004.

A Finger-vein Biometric System Based on Textural Features
375
References
1. Kaur, G., Singh, G., Kumar, V.: A review on biometric recognition. Int. J. BioSci.
BioTechnol. 6, 69–76 (2014)
2. Wang, P., Sun, D.: A research on palm vein recognition. In: 2016 IEEE 13th Inter-
national Conference on Signal Processing (ICSP), pp. 1347–1351. IEEE (2016)
3. Syazana-Itqan, K., Syafeeza, A., Saad, N., Hamid, N.A., Saad, W.H.B.M.: A review
of ﬁnger-vein biometrics identiﬁcation approaches. Indian J. Sci. Technol, 9 (2016)
4. Yang, L., Yang, G., Yin, Y., Xi, X.: Finger vein recognition with anatomy structure
analysis. IEEE Transactions on Circuits and Systems for Video Technology (2017)
5. Cheng, Y.C., Chen, H., Cheng, B.C.: Special point representations for reducing
data space requirements of ﬁnger-vein recognition applications. Multimedia Tools
Appl. 76, 11251–11271 (2017)
6. Bansal, K., Kaur, S.: Finger vein recognition using minutiae extraction and curve
analysis. Int. J. Sci. Res. 4, 2402–2405 (2015)
7. Beura, S., Majhi, B., Dash, R.: Mammogram classiﬁcation using two dimensional
discrete wavelet transform and gray-level co-occurrence matrix for detection of
breast cancer. Neurocomputing 154, 1–14 (2015)
8. Etehadtavakol, M., Ng, E., Chandran, V., Rabbani, H.: Separable and non-
separable discrete wavelet transform based texture features and image classiﬁcation
of breast thermograms. Infrared Phys. Technol. 61, 274–286 (2013)
9. Yin, Y., Liu, L., Sun, X.: SDUMLA-HMT: a multimodal biometric database. In:
Sun, Z., Lai, J., Chen, X., Tan, T. (eds.) CCBR 2011. LNCS, vol. 7098, pp. 260–
268. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-25449-9 33
10. Modi, S.K.: Biometrics in identity management: concepts to applications. Artech
House, Massachusetts (2011)
11. Chen, C.H.: Handbook of pattern recognition and computer vision. World Scien-
tiﬁc, New Jersey (2015)
12. Marsland, S.: Machine learning: an algorithmic perspective. CRC Press, Florida
(2015)
13. Trabelsi, R.B., Masmoudi, A.D., Masmoudi, D.S.: A new multimodal biometric
system based on ﬁnger vein and hand vein recognition. Int. J. Eng. Technol. 4,
3175 (2013)
14. Lu, Y., Yoon, S., Park, D.S.: Finger vein recognition based on matching score-level
fusion of Gabor features. J. Korean Inst. Commun. Inf. Sci. 38, 174–182 (2013)
15. Lu, Y., Xie, S.J., Yoon, S., Park, D.S.: Finger vein identiﬁcation using polydirec-
tional local line binary pattern. In: 2013 International Conference on ICT Conver-
gence (ICTC), pp. 61–65. IEEE (2013)

Multi-level Skew Correction Approach
for Hand Written Kannada Documents
H. C. Vinod1(&) and S. K. Niranjan2(&)
1 Department of Information Science and Engineering,
SJBIT, Bangalore 560060, Karnataka, India
vinod4805@gmail.com
2 Department of Master of Computer Applications, Sri Jayachamarajendra
College of Engineering, Mysore 570006, Karnataka, India
sriniranjan@yahoo.com
Abstract. During capturing documents using camera or camera phone and
scanning the documents, document skew is unavoidable. Skew in line and
paragraph of handwritten document is varying for different people. Document
analysis and character recognition efﬁciency is mainly depending on document
pre-processing, document skew correction. Document skew detection and cor-
rection is one of the difﬁcult step before document analysis. In this paper we
proposing multi-level Run-Length-Smoothed Image (RLSA) skew detection and
correction for kannada hand written document image. We have major two
sections, ﬁrst section is pre-processing techniques like Haar wavelet decom-
position, maximum gradient, 4, 8 connective Laplacian methods to extract text
region without losing data. Second section is multi-level skew detection, hori-
zontal and vertical projection proﬁle are used to detect the page boarder and
further RLSA skew detection is applied to ﬁnd skew angle and rotate document
by desired angle, this removes the document skew occurred while capturing the
image. Later, the skewed document will be further processed to remove line and
paragraph skew angle by applying RLSA skew detection technique for each
segmented line and rotate each line by desired angle. Performance of proposed
system is performed for kannada hand written documents; the experimental
results are discussed; the proposed system is encouraging.
Keywords: Haar wavelet  RLSA  Horizontal and vertical projection
Laplacian mask
1
Introduction
Document image contains different structural units such as characters/text, graphics,
tables, images, etc. The conversion of physical paper to digital form is very important
for retrieval, processing, analysis, understanding, storing, transmission and so on.
Document digitization is the process to the ﬁll gap between previous and current
technologies, and also for analysis, storing, document retrieval, document indexing,
understanding and creating documents to accessible & available through internet or
networks for future generation.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_36

The document analysis & understanding are the major two steps in document
processing. There are three stages in document analysis, they are; pre-processing,
content segmentation and classiﬁcation. Document layout, boarder, skew detection &
binarization are part of the pre-processing document analysis, various skew correction,
binarization, noise removing methods are compared [9, 10], whereas segmentation,
extraction & recognizing the document content are the part of document understanding.
During capturing documents using camera or camera phone or scanning the doc-
ument, document skew is unavoidable. Document analysis and character recognition
efﬁciency is mainly depending on document pre-processing and document skew cor-
rection. Document skew detection and correction are difﬁcult steps in document
analysis. The main applications of document processing are; image retrieval, document
storing, high-quality facsimile, digitizing documents, document content recognition,
pre-processing for OCR.
We need to check whether document contain skew or not before extracting text
from the document. If the document is skewed then skew correction has to be done.
This skew affects the performance of character recognition. Several attempts have been
made for skew detection. But many techniques are limited to printed & scanned
document images [7], and the methods can be mainly categorized into following eight
groups:
(1) The Hough transform: is a well-known technique in computer vision that has been
used to detect lines and curves in digital images. The values for parameters for all
curves of particular lines are calculated by the Hough transform to transfer
through every individual black pixel [5, 12]. Hough Transform [8] technique is
best suited for vehicle number plate skew correction. In complex background it
fails to extract the text and it demands huge space to store data in Hough plane.
(2) Principal Component Analysis (PCA): The PCA method [8] is categorized into 5
sub-modules, they are; Pre-processing, PCA, Skew Correction, 3D Correction and
Character Segmentation. PCA is suites to recognize face and to compress image
size.
(3) Method of Extreme Points: This method [6] describes the objects in rectangular
shapes in any document image such as paragraphs, text lines, tables and ﬁgures.
These objects can be conﬁned inside rectangles. Extreme point’s properties are
used to obtain the corners of the rectangle which ﬁts huge connected component
of a document image. The angle of document skew is deﬁned by the angle of the
rectangular shape objects angle.
(4) Radon Transform: The radon transforms [11] of the image is the sum of radon
transform of each pixel of the given image. Line integral is the set of 2D pro-
jection function f(x, y). Line integral are calculated by radon function from many
sources like, beams, paths and in directions.
(5) Projection Proﬁle: In projection proﬁle method, skew angles are detected by using
projection features. The skew is estimated or evaluated by the deviation or ori-
entation of a projection histogram. The rotation angle id depends on the skew
angle but the projection histograms mean deviation is maximized [4, 15].
(6) Cross Correlation: The cross-correlation method presented by Yan is based on the
relationship between two vertical lines in document image. [13] skew occurred
Multi-level Skew Correction Approach
377

due to the translation of two parallel lines, later the correlation matrix will be
constructed.
(7) Fourier Transformation: According to this Pstl [16] method, larger the density of
Fourier space direction leads to the skew angle of the document image. The
Fourier transformation techniques complexity is very high because of calculation
of density in each and every pixel of document.
(8) k-NN (k Nearest Neighbours) Clustering: The k-NN clustering technique is used
in [14]. The k-nearest centres of the neighbouring connected components are
chosen to ﬁnd the vector directions between random pairs. Later the skew angle
for a document image is calculated for the corresponding histogram peak.
Gari et al. [1] proposed a method based on extraction of Harris corner features
points and Hough transform is presented to detect skew angle of printed documents.
Brodic & Milivojevic [2] presents a text skew detection method consists of three steps.
In the ﬁrst step, it calculates the horizontal and vertical projection proﬁles. Secondly,
the rough angle will be calculated by horizontal and vertical entropy. Finally, the
smooth angles are also performed by the horizontal and vertical entropy. The calculated
entropy creates the two cost functions: horizontal and vertical. Each function represents
detected text skew angle.
Alaei et al. [3] proposes a skew estimation technique based on iterative employ-
ment of the Piece-wise Painting Algorithm (PPA) on document images. Horizontal and
vertical painted images are obtained initially by the PPA. Horizontal bands are selected
from the horizontal-painted images; the horizontal bands are the small number of the
horizontal regions. Six horizontal bands are identiﬁed; these bands are left (top), right
(bottom) and middle (middle). Selected points in each one are used in linear regression
and draws a geometric line. Best ﬁt line is selected by statistical mode and voting
approach. The document skew is detected based on the slope of the best ﬁt line and
skew corrected by desired angle. The same technique is applied repeatedly until the
skew angle is less than 1.
2
Proposed System
The proposed system consists of major two sections. Section 1 is the pre-processing. It
consists of binarizing the input kannada document image by using Haar wavelet
decomposition. Section 2 is the multilevel skew detection & correction. It consists of
document skew correction, eliminating unwanted spaces, skew correction for para-
graph and lines by using RLSA skew correction algorithm.
378
H. C. Vinod and S. K. Niranjan

2.1
Pre-processing
Binarization is the important stage in document analysis and character recognition. In
the low intensity level document image, variation in the intensity and brightness in the
document or shadow of another object is imposed on the document image will lead to
loss of vital text information, by applying Haar wavelet decomposition, we can
overcome the above problems.
Haar wavelet decomposing technique is applied to input document image to sup-
press the non-text contents, by considering Horizontal(H), Vertical(V) and Diagonal(D)
part of the decomposition with N level, where N is 1,2, 3…., N is 2 in our proposed
system, quantization is done to reduce the unwanted details and maximize the required
details. Haar wavelet transformation ﬁlter uses the high pass and low-pass ﬁlters
simultaneously.
Average of H, V & D is calculated to get the maximum text details showed in
Fig. 2 (b) (c) and (d). The imadjust is applied on averaged image (AVG), imad-
just maps the intensity values of grayscale image AVG to new values in J such that
1% of data is saturated between low and high intensities of AVG. Contrast of output
image J is increased due to this imadjust method showed in Fig. 2 (e). Further Max-
imum Gradient difference method is applied to connect the disconnected pixels by
applying thickening method as shown in Fig. 2 (g). Laplacian 4 & 8 connected
operation is used to connect horizontal, vertical & diagonal pixels shown in Fig. 2 (f).
Pre-processing with & without using Haar wavelet decomposition for complex docu-
ment image that contains shadow is shown in Fig. 2 (i) and (h) respectively.
2.2
Multi Level Skew Detection and Correction
The output image of pre-processing stage is converted to binary image by calculating
threshold automatically. It removes the small pixels group that contains less than
12-pixel width by applying bwareaopen & morphological operators like erosion,
dilation, etc. Page boarder is detected by applying horizontal and vertical projection
proﬁle. Horizontal projection will give the top & bottom pixel locations and vertical
projection gives the left and right pixel locations.
Document skew angle is detected by using Run-Length-Smoothed Image (RLSA)
skew detection algorithm Angle of tilt provided by RLSA document is rotated by the
angle speciﬁed. This eliminates skew occurred while capturing the image. In hand-
written document image tilt or skew may be present in paragraph or in line with
different angle or orient. to overcome this problem. We have overcome this challenge
by segmenting each line and RLSA skew detection algorithm for each segmented line.
By this we have eliminated the extra spaces and corrected the skew of each paragraph
and line. This proposed architecture and its pseudo code is show in Fig. 1 & Algorithm
1 respectively.
Multi-level Skew Correction Approach
379

Fig. 1. The proposed system.
380
H. C. Vinod and S. K. Niranjan

Fig. 2. (a) Input image. (b) (c) & (d) Horizontal(H), Vertical(V) & Diagonal(D) part of Haar
wavelet decomposition respectively. (e) Average of H, V & D. (f) 8-connected laplacian.
(g) Maximum gradient operation. (h) Binarization with wavelet decomposition. (i) Binarization
without wavelet decomposition.
Multi-level Skew Correction Approach
381

Algorithm 1. Multi-level Skew detection
Step 1: Input document image.
Step 2: Pre-processing:
Haar Wavelet decomposition
Average of H, V & D from wavelet decomposition
imadjust to smoothen the image. 
The Maximum gradient method, Laplacian 4 & 8 
connected operation are used to connect 
disconnected region, 
covert to binary image by Thresholding.
Step 3: Boarder detection:
Horizontal & Vertical projection profile (to 
find top, bottom, left and right co-ordinates)
Step 4: Skew detection and correction for document 
image by using RLSA Algorithm.
Step 5: Skew Detection & correction for line & 
paragraph
do
{
Line Segmentation
Find nonzero element row and column number 
(i.e. to remove space)
Copy non- zero row to another variable
Count the number of lines in page
for each line
{
If sum of row element is zero
Copy first line matrix to FL
Copy the remaining line matrix to RL
Else 
Copy only one-line matrix to FL
RL will be null
}
Detect skew for first line matrix by using RLSA 
Rotate the matrix by detected angle
} Repeat until RL is null
382
H. C. Vinod and S. K. Niranjan

3
Experimental Results
This section presents the experimental result and performance analysis. The proposed
method is implemented in MATLAB R2017a. The data set consists of 90 hand-written
kannada document images. These document samples are collected by various peoples
with different age group. These documents are captured by using hand held camera
phones with different resolution, different angle or skew variations with shadow and
without shadow on text both in indoor and outdoor environment.
Dataset consists of documents with total skew on entire document, paragraph skew
and line skew. We have considered single column document only. Each page contains
12–20 lines of text. Dataset does not contain the non-text objects like ﬁgures, graphs
etc. The Table 1 shows the actual skew and detected skew through proposed algorithm.
Figure 3 shows the input, various steps in proposed method and ﬁnal skew corrected
document image. By these Figures and Table, we can conclude the proposed system
works for both positive and negative skewed documents and results are encouraging.
The drawback of the proposed system is scaling of skew corrected lines and paragraph
as shown in Fig. 3 (d). If the text width is less or only having few words in the line will
enlarge and attach to previous paragraph or line. This is the drawback of the proposed
system as shown in Fig. 3 (d).
Table 1. Result analysis: actual angle detected by proposed system versus actual angle of skew
in original documents.
Document Number Actual Angle (°) Detected Angle (°)
Doc_1
−2
−2
Doc_3
−4
−4
Doc_5
−15
−15
Doc_10
−1
−1
Doc_15
+4
+4
Doc_20
+10
+10
Doc_25
−5
−5
Doc_30
+34
+34
Doc_40
−36
−35
Doc_50
−23
−21
Doc_55
−8
−8
Doc_60
+18
+18
Doc_70
+3
+3
Multi-level Skew Correction Approach
383

Fig. 3. (a) Input image [Doc_5]. (b) After maximum gradient difference. (c) After document
skew correction (−15°). (d) After paragraph and line skew correction by multi-level skew
correction. (e) Input image [Doc_10]. (f) After maximum gradient difference. (g) After document
skew correction (−1°). (h) After paragraph and line skew correction by multi-level skew
correction.
384
H. C. Vinod and S. K. Niranjan

4
Conclusion
In this paper we proposing multi-level Run-Length-Smoothed Image (RLSA) skew
detection & correction algorithm for kannada hand written document images. We have
major two sections. First section consists of pre-processing techniques such as Haar
wavelet decomposition, maximum gradient, 4 & 8 connective laplacian technique.
Second section consists of multi-level skew detection and boarder detection using
horizontal and vertical projection. RLSA skew detection is applied to detect skew angle
of entire document and later same algorithm is applied for each segmented line. The
performance of proposed system is encouraging on kannada hand written documents.
Percentage of skew detection for the kannada document dataset is 93.33%.
References
1. Gari, A., Khaissidi, G., Mrabti, M., Chenouni, D., El Yacoubi, M.: Skew detection and
correction based on Hough transform and Harris corners. In: 2017 International Conference
on
Wireless
Technologies,
Embedded
and
Intelligent
Systems
(WITS).
IEEE
978-1-5090-6681-0/17/$31.00 (2017)
2. Brodic, D., Milivojevic, Z.N.: Text skew detection using combined entropy algorithm. J. Inf.
Technol. Control 46(3), 308–318 (2017)
3. Alaei, A., Nagabhushan, P., Pal, U., Kimura, F.: An efﬁcient skew estimation technique for
scanned documents: an application of piece-wise painting algorithm. J. Pattern Recogni. Res.
11(1), 1–14 (2016)
4. Kolhe, S.S., Jadhao, K.Y.: Skew detection techniques used in scanned document images.
IJSRSET 2(4), 414–420 (2016). Print ISSN: 2395-1990. Online ISSN: 2394-4099
5. Ahmad, R., Rashid, S.F., Afzal, M.Z., Liwicki, M., Dengel, A., Breuel, T.: A novel skew
detection and correction approach for scanned documents. In: 12th International IAPR
Workshop on Document Analysis Systems, Santorini, Greece (2016)
6. Wagdy, M., Faye, I., Rohaya, D.: Document image skew detection and correction method
based on extreme points. In: 2014 International Conference on Computer and Information
Sciences (ICCOINS), pp. 1–5. IEEE (2014)
7. Dixit, S., Narayan, S.H., Belur, M.: Kannada text line extraction based on energy
minimization and skew correction. In: IEEE International Advance Computing Conference
(IACC). IEEE (2014)
8. Arulmozhi, K., Perumal, S.A., Priyadarsini, T., Nallaperumal, K.: Image reﬁnement using
skew angle detection and correction for Indian license plates. In: IEEE International
Conference on Computational Intelligence and Computing Research (2012)
9. Saba, T., Sulong, G., Rehman, A.: Document image analysis: issues, comparison of methods
and remaining problems. Artif. Intell. Rev. 35(2), 101–118 (2011)
10. Rehman, A., Saba, T.: Document skew estimation and correction: analysis of techniques,
common problems and possible solutions. Appl. Artif. Intell. 25, 769–787 (2011)
11. Aithal, P.K., Rajesh, G., Siddalingaswamy, P.C., Acharya, D.U.: A novel skew estimation
approach using radon transform. In: 11th International Conference on Hybrid Intelligent
Systems (HIS). IEEE (2011)
12. Manjunath Aradhya, V.N.: Document skew estimation: an approach based on wavelets. In:
Proceedings of the 2011 International Conference on Communication, Computing &
Security ICCCS 2011, Rourkela, Odisha, India. February 12–14, 2011, pp. 359–364. ACM
978-1-4503-0464-1/11/02…$10.00 (2011)
Multi-level Skew Correction Approach
385

13. Yan, H.: Skew correction of document images using interline cross-correlation. CVGIP:
Graph. Models Image Process. 55(6), 538–543 (1993)
14. O’Gorman, L.: The document spectrum for page layout analysis. IEEE Trans. Pattern Anal.
Mach. Intell. 15(11), 1162–1173 (1993)
15. Ciardiello, G., Scafur, G., Degrandi, M.T., Spada, M.R., Roccoteli, M.P.: An experimental
system for ofﬁce document
handling and text recognition. In: Proceedings of the 9th
International Conference on Pattern Recognition, Rome, Italy, November 14–17, pp. 739–
743 (1988)
16. Pstl, W.: Detection of linear oblique structure and skew scan in digitized documents. In:
Proceedings of the 8th International Conference on Pattern Recognition, Pairs, France,
October 27–31, pp. 687–689 (1986)
386
H. C. Vinod and S. K. Niranjan

Semi-automatic Determination of Geometrical
Properties of Short Natural Fibers
in Biocomposites by Digital Image Processing
Victoria Mera-Moya1, Jorge I. Fajardo2(&), Iális C. de Paula Junior1,
Leslie Bustamante3, Luis J. Cruz4, and Thiago Barros1
1 Universidade Federal do Ceará, Sobral, Ceará, Brazil
2 Universidad Politécnica Salesiana, Cuenca, Ecuador
jfajardo@ups.edu.ec
3 Doshisha University, Kyoto, Japan
4 Universidad Pontiﬁcia Bolivariana, Medellín, Colombia
Abstract. The present article proposes a method for the estimation of geo-
metrical properties of short natural ﬁbers that act as reinforcing phase in poly-
meric composites. The extraction of the image attributes is performed based on
the analysis of microscopic images taken in different sections of the material,
requiring a minimal user intervention. The proposed method estimates the ori-
entation tensor of the short ﬁbers from geometrical characteristics such as
inclination, length, width and aspect ratio, using an elliptical covering on each
ﬁber. The method validation was performed on a polypropylene composite
reinforced with 30% by weight of bamboo short ﬁbers (guadua angustifolia
species), the accuracy and precision of the proposed method proved to be
adequate. The a11 element of the orientation tensor was evaluated and the results
agree in 98% with respect to the commercial software. This technique is a fast
alternative of low cost in characterization of new materials.
Keywords: Digital image processing  Orientation tensor
Composite materials  Biomaterials
1
Introduction
In order to reduce the environmental pollution produced by synthetic materials, interest
in renewable and bio-sustainable materials has grown in recent years [1–3]. Nowadays,
it has become common the use of composite materials reinforced with glass, carbon
and Kevlar ﬁbers in industrial applications. However, the impact on the environment
have made researchers to focus their studies on natural ﬁbers. Composite materials
reinforced with natural ﬁbers can be found in nature, a clear example is wood com-
posed of cellulose (ﬁber) and lignin (resin) [4]. The mechanical properties of these
materials have been used by ancient cultures (6000 BC) in order to reinforce ceramics,
mummies and decorative objects. However, their potential was underestimated for
other applications. Composite materials with natural ﬁbers have several advantages
compared to synthetic materials such as biodegradability, low density, low energy
consumption for their manufacture, eco friendly, non-abrasive, economical, renewable,
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_37

excellent mechanical properties and can be found in the nature, moreover they are
abundant [5, 6]. From the point of view of the mechanical properties, these materials
present anisotropy which is a direct dependence of the properties with the orientation
state of the reinforcing ﬁbers. In this case, two situations can be found, the ﬁrst when
the ﬁbers are perfectly aligned and the second, when they are randomly distributed.
Due to their simplicity and low computational cost [7], the ellipses method from photo
micrographs is widely used to evaluate the orientation of short ﬁbers. Vélez et al. [8]
showed that the error associated with this method is minimal in the case of rigid short
ﬁbers. Some works have been developed with computational tools such as ModFlow,
Ansys or SolidWorks that simulate the behavior of the material through ﬁnite element
analysis to predict the ﬁber orientation. The main disadvantage of this analysis is that it
does not include properties of natural reinforcements (lignocellulosic) in the database
[5]. A big boost for the study of reinforcing ﬁbers of composite materials is the work
accomplished by Advani et al. [9], this technique reduces considerably the computa-
tional cost of the process by calculating the second and fourth order moments in this
way, the orientation tensors are completely deﬁned. Subsequently, Neves et al. [10]
studied the injection process of polycarbonate reinforced with short glass ﬁbers at 10%
of concentration in volume determining the effect exerted by the orientation of the
ﬁbers on the mechanical properties of the material. A comparison of results with a
commercial software is also shown. A new method proposed by Kim and Lee [11]
presents a simple non-destructive method using X-ray images, the orientation is deﬁned
based on the intensity of the image. Abdennadher [7] evaluates the ﬁber orientation
considering the ﬁber morphology by using a scanning electron microscopy (SEM). In
this case specialized equipment is required. Recently, a non-destructive lCT technique
with X-ray (Micro Computed Tomography) has been introduced to analyze the com-
pounds microstructure [12, 13]. The disadvantages associated with these methods lie in
the high costs and limitation in using certain natural ﬁbers. This paper aims to validate
an efﬁcient, low cost and minimal user intervention tool for characterization of short
ﬁber orientation tensor in polymeric compounds as well as its morphological proper-
ties. The manuscript has been organized as follows: Sect. 2 describes the theoretical
basis of the methods used in this work. Section 3 presents the methodology used and
ﬁnally, in Sect. 4 the results and conclusions of this work are presented.
2
Theoretical Foundation
2.1
Fiber Orientation and Tensors
Fiber orientation of composite materials allow to predict the material behavior in
micromechanical terms. The composite is stiffer and stronger in the direction of greater
orientation of the ﬁbers and weaker and more ﬂexible in the direction of lesser ori-
entation [14]. The ﬁrst works developed in this ﬁeld are based on Jeffery’s model [15],
who proposed the use of the probability distribution of orientation function (ODF) in
non-Newtonian ﬂuids. The motion equation is based on the volume control approach
and proposes a resolution technique which provides accurate responses, however the
analysis can last several days [6]. After that, a spherical harmonic approach was
388
V. Mera-Moya et al.

introduced, the method can be adapted to a ﬁnite element software for ﬂow simulation
inside the molding cavity. The results are as precise as the results using Bay’s tech-
nique, however the memory requirements make difﬁcult its use in industrial applica-
tions. Considering previous works, Advani y Tucker III [9] changed the ODF
movement focus and left the control volume analysis to adopt a new point of view
focused on the moments of orientation distribution [14]. This technique is known as
orientation tensors which allows to obtain concise answers in few seconds additionally,
it is suitable for industrial applications.
2.2
Orientation Tensors
Tensional notation is an effective way of representing the orientation state of short ﬁber
reinforced composites. It contains all the necessary information to represent the com-
pound behavior with less computational effort than the required when using statistical
orientation distribution. The orientation tensors can be deﬁned by forming didactic
products of the vector p and then integrating the product of the tensors with the
distribution function in all possible directions. The study carried out by Advani and
Tucker III [9], shows the effectiveness in using the second and fourth order tensors for
the ﬁber orientation estimation the same that are determined by Eqs. (1) and (2).
aij ¼
I
pipjw p
ð Þdp
ð1Þ
aijkl ¼
I
pipjpkplw p
ð Þdp
ð2Þ
The second-order orientation tensor with a suitable closure approximation can
precisely predict the distribution of the ﬁbers of the material [9, 16]. The tensors are
calculated from the average orientation data obtained from each ﬁber (among N
individual ﬁbers) and are represented by Eq. (3):
aij ¼
PN
n¼1 PiPjFn
PN
n¼1 Fn
ð3Þ
where Fn is a weighting function of the nth ﬁber. In total, there are nine tensors
distributed in a matrix of 3  3 where the sum of the components of the main diagonal
is the unit [10]. If the tensors are symmetrical, it is enough to calculate the value of six
tensors of the whole matrix, Eqs. (4)–(7):
a11 ¼ 1
N
X
N
n¼1
cos2hn;
ð4Þ
a12 ¼ a21 ¼ 1
N
X
N
n¼1
sen2hncos2/nsen/n;
ð5Þ
Semi-automatic Determination of Geometrical Properties
389

a13 ¼ a31 ¼ 1
N
X
N
n¼1
senhncoshncos/n;
ð6Þ
a23 ¼ a32 ¼ 1
N
X
N
n¼1
senhncos/nsen/n:
ð7Þ
where N is the total number of ﬁbers existing in the image, hn and /n are the orientation
angles in the plane and in the space of each ﬁber respectively. The a11 value represents
the ﬁber orientation in relation to the ﬂow direction, constituting in the reference
direction tensor [17], as follows: a11\0:35 in relation to the ﬂow direction, consti-
tuting in the reference direction tensor; a11 [ 0:7 orientation considered parallel to the
direction
of ﬂow
and
0:5\a11\0:6
a
random
orientation.
In
the
case
of
two-dimensional analysis with symmetric tensors, it is enough to calculate the value of
three tensors a11; a12; a22
ð
Þ. Thus, the preferred angle is represented by Eq. (8):
tan2a ¼
2a12
a11  a22
ð8Þ
The summary of the ﬁbers orientation can be shown through the orientation of an
ellipsoid, where the autovectors of the 3  3 matrix delivers information related to the
direction and the eigenvalues give the anisotropic grade of the material.
3
Methodology
3.1
Materials
The images used in this work were provided by the research group GiMaT (Research
Group on New Materials and Transformation Processes) of the Politécnica Salesiana
University, Cuenca-Ecuador. In order to obtain the composite, guadua Angustifolia
kunth (GAK) was used as natural reinforcing material and homopolymer polypropylene
H-306 as matrix. During the elaboration process, superﬁcial treatments were carried out
to guarantee the good adhesion of the components. Subsequently, the injection material
was performed as a modeling method. Normally, the variation in the orientation or
breakage of the ﬁbers occurs in this step. Microscopic images were captured using light
microscopy with a BX51M microscope and an Olympus DP 72 digital camera with 5x
magniﬁcation. In order to obtain the complete ﬁeld of analysis, a combined image
acquisition tool called MIA was used, this tool allows individual captures of adjacent
positions and their subsequent combination to form only one image of the entire ﬁeld of
analysis. Two samples were fabricated at 30% of ﬁber concentration by weight. From
each of the samples nine images were obtained to know the direction of ﬂow during the
injection process. Figure 1 shows the areas and depth of the specimen from which the
images were obtained.
390
V. Mera-Moya et al.

3.2
Pre-processing
The pre-processing corresponds to all the tools and methods applied prior to the
analysis of the ﬁbers. Initially it was necessary to determine the value of the scale
which is the number of one-dimensional pixels representing a deﬁned value, usually
1 mm. Since the top edges of the images were not horizontal and the area of analysis
does not have a standard value, it was necessary to make inclination adjustments and
crop the image.
To eliminate noise or particulate matter, some ﬁltering alternatives were evaluated.
Since it is an analysis of morphological characteristics speciﬁcally, methods such as
erosion or dilation that could modify the shape of the ﬁbers were not considered as
valid tools for this work. The application of reconstruction by erosion shows a
remarkable improvement in the results, since through erosion the ﬁbers are ﬁltered by
the size and what remains acts as a marker for the reconstruction. The selected
structuring element was size 2 diamond. Figure 2 shows the difference between the
evaluated methods. These results were decisive to select the ﬁltering method.
Subsequently, the thresholding process, which consists in determining the value of
the grayscale that will serve as a separation between what will be considered ﬁber
(white) and matrix (black) was performed. The proposed methodology is based on an
automatic thresholding process using Otsu’s method which chooses the threshold value
Fig. 1. Zones and depths of image acquisition. Zone 1 next to the ﬂow inlet and Zone 3 the
farthest. Depth (1) 1 mm from the surface, (0) at the center of the specimen and (−1) at 3 mm
from the surface.
(a)
(b)
(c)
(d)
Fig. 2. Result of ﬁltering by the techniques: (a) Original, (b) Erosion, (c) Dilation, (d) recon-
struction by erosion.
Semi-automatic Determination of Geometrical Properties
391

to minimize the intraclass variance of the thresholded black and white pixels. The
manual thresholding was used only for comparison between the algorithm and the
information from commercial software.
4
Processing
The image processing started with feature extraction. In this step, each of the objects
presents in the image were separated and the ellipses method was applied. This method
consists of determining the adjusted and inscribed ellipse to the object, using the fourth
spatial moments. The angle formed between the major and the horizontal axis deter-
mines the ﬁber orientation. Figure 3 displays examples the method applied. In order to
deﬁne the ﬁber length, the Feret diameter was calculated. This diameter represents the
distance between two parallels lines that are tangential to the projection contour. The
ﬁber width corresponds to the perpendicular line at the object orientation which was
traced from the centroid.
As the analysis is performed on ﬁbers, it is necessary to establish geometric
parameters that differentiate ﬁbers from particles. The main constraint corresponds to
the aspect ratio which is the ratio between the length and width of the ﬁbers. The
minimum value depends on the composition of each material. For this particular case,
the minimum aspect ratio was 2.5 [18]. Another restriction corresponds to the maxi-
mum limit of ﬁber length deﬁned in 80 lm according to characteristics of elaboration
of the test piece. These data may vary depending on the forming process of the
material.
The resulting objects from this ﬁltering meet pre-established conditions and were
used for the tensor analysis. Using Eqs. (4)–(7), previously described, the second-order
orientation tensor was calculated, where a11 indicates the ﬁbers orientation tendency. If
this value is greater than 0.7 it means that the ﬁbers follow the direction of the injection
ﬂow. The higher this value, the ﬁbers will be more aligned to the ﬂow direction.
Likewise, the preferred angle delivers the tilt value of all ﬁbers in the image using the
eigenvalues of the tensor matrix.
4.1
Validation
Initially, three standard images with ﬁber distributions in the ranges: −10 to 10, 80 to
100 and 35 to 145, covering all possible orientations of the ﬁbers were used. The
geometric characteristics of these images were known beforehand. Once the systematic
Fig. 3. Ellipses method application
392
V. Mera-Moya et al.

error was obtained and the correct behavior of the algorithm was veriﬁed, the images of
the composite were analyzed. The obtained results were compared with the Olympus
Stream Essentials ® software analysis, found in [18].
5
Results
5.1
Algorithm Validation in Pattern Images
Table 1 shows the comparison between the actual and calculated data of each image.
Section (a) corresponds to ﬁbers in the range −10 to 10°; section (b) has ﬁber infor-
mation in the range 80 to 100° and section (c) corresponds to ﬁbers in the range 35 to
145°.
As can be seen in the fourth column (Difference), the absolute error for all cases
was minimal. The algorithm accurately identiﬁed the number of objects existing in the
image. The values of the orientation tensors had an error rate of 1.9% in a12 and the
most important tensor a11, presented a difference of 0.05%. The results also showed
that the average of the ﬁbers orientation had an error of 2%. In terms of length and
width, the error was 3.77%. It is important to note that the obtained error values were
lower than those found in the literature speciﬁed in [18].
5.2
Analysis of PP-GAK Composite at 30% Weight Concentration
Table 2 summarizes the percentages of similarity for the test specimens A and B with
30% concentration by weight, considering as baseline the results from [18]. According
to the obtained data, the difference between the two methods is low, especially in the
a11 (98,25%), preferential angle (96,40%) and overage orientation (96,09%), which are
important parameters to determine the micromechanical behavior of the material. It is
worth notice that the main element of the tensor orientation is higher than 0.8 for each
Table 1. Comparison of the pattern image results
(a) Comparing real vs. calculated data. Standard image
−10º to 10º
Aspects
Real data Calculated Difference
Number of objects
200
200
0
Orientation average 75,70
78,40
2,69
Preferential angle
0,0990
0,0936
0,0054
a11
0,9870
0,9870
0,0000
a12
0,0017
0,0016
0,0001
a22
0,0130
0,0130
0,0000
Length
12,2000
11.8993
0,3007
Width
2,6600
2,8005
0,1405
(continued)
Semi-automatic Determination of Geometrical Properties
393

value of z. A certain asymmetry was also observed across the thickness. Generally
highly oriented zones are those regions in which the polymer matrix solidiﬁes ﬁrst,
with slight variations due to the forces produced by front advance of the ﬂow, as it was
reported by Fajardo et al. [19], using a commercial software.
Table 2. PP-GAK 30% percentage of similarity summary
Aspects
Real data Calculated Difference
Number of objects
89,68%
87,69%
88,69%
Orientation average 95,43%
96,74%
96,09%
Preferential angle
96,91%
95,89%
96,40%
a11
97,91%
98,60%
98,25%
a12
91,83%
95,52%
93,68%
a22
89,42%
92,04%
90,73%
(b) Comparing real vs. calculated data. Standard
image 80º to 100º
Aspects
Real data Calculated Difference
Number of objects
200
200
0
Orientation average 90,48
90,46
0,02
Preferential angle
0,4759
0,4560
0,0199
a11
0,0130
0,0131
0,0001
a12
−0,0080
−0,0077
0,0004
a22
0,9870
0,9869
0,0001
Length
12,0000
11.7373
0,26227
Width
2,7096
2,6600
0,0496
(c) Comparing real vs. calculated data. Standard
image 35º to 145º
Aspects
Real data Calculated Difference
Number of objects
200
200
0
Orientation average 82,02
80,01
2,01
Preferential angle
−42,4392 −43,1781
0,7389
a11
0,4939
0,4944
0,0005
a12
0,0682
0,0879
0,0197
a22
0,5061
0,5056
0,0005
Length
11,4000
11,1213
0,2787
Width
2,9600
2,7096
0,2504
Table 1. (continued)
394
V. Mera-Moya et al.

6
Conclusions
An effective method for the semi-automatic determination of morphological and geo-
metric properties of natural ﬁbers reinforcing biocomposites was developed.
Of all the evaluated aspects, a11 is the most important value since it determines the
general ﬁber orientations of the image additionally it is used to predict the microme-
chanical behavior of the material. The similarity rate in relation to the data obtained
with commercial software was 98,25%, and with the standard image 99,8%, which
guarantees a good performance of the proposed method.
The pattern image analysis allowed to evaluate the performance of the developed
algorithm and to predict the system error interval.
Pre-processing techniques are decisive in determining geometric characteristics,
especially in small objects and are suitable for precision applications as the presented in
this work. The application of reconstruction by erosion was considered adequate since
it eliminates the noise present in the image keeping the original shape of the objects.
One of the causes of the difference of values corresponds to the lack of deﬁnition of
edges that in small objects has greater inﬂuence.
All evaluated aspects in this images of PP-GAK composite show a similarity rate
higher than 90%, the values corresponding to a11 have a similarity of 98% and the
average preferred orientation angle presents a similarity higher than 96%.
The similarity rate could have been even higher if the analysis area used in the base
work had been known. According to a report with graphs of the area used, issued by the
SSE the space and inclination of the original images was assumed.
The proposed method optimizes the analysis time in comparison to the traditional
process which requires at least two stages of analysis with different softwares, one to
extract the geometric attributes of the reinforcement material and the other for the
post-processing generated by the tensor of orientation.
References
1. Zhong, Y., Kureemun, U., Tran, L.Q.N., Lee, H.P.: Natural plant ﬁber composites-
constituent properties and challenges in numerical modeling and simulations. Int. J. Appl.
Mechanics 9(4), 1750045 (2017)
2. Maache, M., Bezazi, A., Amroune, S., Scarpa, F., Dufresne, A.: Characterization of a novel
natural cellulosic ﬁber from Juncus effusus L. Carbohydr. Polym. 171, 163–172 (2017)
3. Luna, P., Lizarazo-Marriaga, J., Mariño, A.: Guadua angustifolia bamboo ﬁbers as
reinforcement of polymeric matrices: an exploratory study. Constr. Build. Mater. 116, 93–97
(2016)
4. Askeland, D.: Ciencia e Ingeniería de los Materiales, 3rd edn. Thomson, México (1998)
5. Bay, R., Tucker, C.: Fiber orientation in simple injection moldings. Part I: theory and
numerical methods. Polym. Compos. 13(4), 317–331 (1992)
6. Babatunde, A., Jack, D., Montgomery, S.: Effectiveness of recent ﬁber-interaction diffusion
models for orientation and the part stiffness predictions in injection molded short-ﬁber
reinforced composites. Compos. A: Appl. Sci. Manuf. 43(11), 1959–1970 (2012)
7. Abdennadher, A.: Injection moulding of natural ﬁber reinforced polypropylene: Process,
microestructure and properties. Ecole Nationale Supérieure des Mines de Paris, Francia
(2015)
Semi-automatic Determination of Geometrical Properties
395

8. Velez-García, G., Mazahir, S., Hofmann, J., Wapperom, P., Baird, D., Zink-Sharp, A., Kunc,
V.: Improvement in orientation measurements for short and long ﬁber injection molded
composites. In: Proceedings of the 10th Annual Automotive Composites Conference and
Exhibition, Michigan, vol. 10, no. 1, pp. 799–809 (2010)
9. Advani, S., Tucker III, C.: The use of tensors to describe and predict ﬁber orientation in short
ﬁber composites. J. Rheol. 31(8), 751–784 (1987)
10. Neves, N., Pontes, A., Pouzada, A.: Fiber contents effect on the ﬁber orientation injection
model GF/PP composite plates. SPE ANTEC (2002)
11. Kim, D., Lee, J.: Measurements of the ﬁber orientation angle in FRP by intensity method.
J. Mater. Process. Technol. 201(1–3), 755–760 (2008)
12. Thi, T., Morioka, M., Yokoyama, A., Hamanaka, S., Yamashita, K., Nonomura, C.:
Measurements of ﬁber orientation distribution in injection-molded-short-glass-ﬁber com-
posites using X-ray computed tomography. J. Mater. Process. Technol. 219, 1–9 (2015)
13. Sun, X., Lasecki, J., Zeng, D., Gan, Y., Su, X., Tao, J.: Measurements and quantitative
analysis of ﬁber orientation distribution in long ﬁber reinforced part by injection molding.
Polym. Test. 42, 168–174 (2015)
14. Blanc, R., Germain, C., Da Costa, J., Baylou, P., Cataldi, M.: Fiber orientation
measurements in composite materials. Compos. A Appl. Sci. Manuf. 37(2), 197–206 (2006)
15. Jeffery, G.: The motion of ellipsoidal particles immersed in a viscous ﬂuid. In: Proceedings
of the Royal Society A, London, vol. 102, no. 715, pp. 167–179 (1923)
16. Dunn, M., Ledbetter, H.: Micromechanically-based acoustic characterization of the ﬁber
orientation distribution function of morphologically textured short-ﬁber composites:
prediction of thermo mechanical and physical properties. Mater. Sci. Eng., A 285(1), 56–
61 (2000)
17. Eberhardt, C., Clarke, A.: Automated reconstruction of curvilinear ﬁbers from 3D datasets
acquired by X-ray microtomography. J. Microsc. 2016(1), 41–53 (2002)
18. Fajardo, J.: Determinación del estado de orientación de ﬁbras cortas, mediante proce-
samiento digital de imágenes en un compuesto termoplástico polipropileno/bambú modelado
por inyección. Dissertación (Maestría en nuevos materiales) – Universidad Pontiﬁcia
Bolivariana (2015)
19. Fajardo, J., Suarez, G., Cruz, L., Garzón, L., López, L.: Characterization of the state planar
orientation for short natural ﬁber in polymeric composites by means of the tensor orientation.
In: 5th International Conference on Advanced Materials and Systems, pp. 43–48 (2014)
396
V. Mera-Moya et al.

ExperTI: A Knowledge Based System
for Intelligent Service Desks Using Free Text
Alejandro Bello1, Andr´es Melgar1,2(B), and Daniel Pizarro3
1 Secci´on de Ingenier´ıa Inform´atica, Departamento de Ingenier´ıa,
Pontiﬁcia Universidad Cat´olica del Per´u, Lima, Peru
amelgar@pucp.edu.pe
2 Grupo de Reconocimiento de Patrones e Inteligencia Artiﬁcial Aplicada,
Pontiﬁcia Universidad Cat´olica del Per´u, Lima, Peru
3 Infobox Latinoam´erica SAC, Lima, Peru
http://inform.pucp.edu.pe/∼grpiaa/
Abstract. When many users consult service desks simultaneously, these
typically saturate. This causes the customer attention to be delayed more
than usual. To increase the amount of human agents is a costly process
for organizations. All this has motivated the design of a knowledge-based
system that automatically assists both customers and human agents at
the service desk. Web technology was used to enable clients to commu-
nicate with a software agent via chat. Techniques of Natural Language
Processing were used for the software agent to understand the customer
requests. The domain knowledge used by the software agent to under-
stand customer requests has been codiﬁed in an ontology. A rule-based
expert system (ES) was designed to perform the diagnostic task. This
paper presents a knowledge-based system allowing client to communicate
with the service desk through a chat system using free text. Evaluations
conducted with users have shown an improvement in the attention of
service desks when the software developed is used.
Keywords: Service desks · Expert systems
Natural language processing · Free text · Ontologies
1
Introduction
Service desk can be deﬁned as a communication channel between the ﬁrms and its
users [8]. This channel is made up of a set of infrastructure, processes and services
which, working together, allows that incidents and problems are analyzed and
resolved [8]. The tasks performed by the service desk are not trivial. There are
many aspects to consider in order to increase a probability of success as services
time duration, waiting delays, call registration, among others [8]. The technical
knowledge that the support staﬀmust have receives special attention [8]. It is
for this reason that the support staﬀmust be in constant training. A challenge
that the service desk faces is the high human resources turnover [7].
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_38

398
A. Bello et al.
In order to fulﬁll the demand of the service desks, it has been proposed some
works that seek to streamline the tasks performed by agents. In [14] the authors
presents a framework for analyzing IT service desk tickets on the ground of text
analytics technologies. The text pre-processing is also used in [7]. In this work,
ﬁve stages that compose the pre-processing text are applied which serves as an
initial step for further analysis of similarity between strings. Our proposal is
similar to those described above in the sense that we also use processing tech-
niques to process natural language text entered by users. The main diﬀerence is
that in our case we use an ontology to represent the domain knowledge. Basi-
cally our proposal has three goals: (i) encode the domain knowledge required for
a software agent automatically responds to user requests, (ii) establish a new
standard for the human agent where only answer queries really relevant, leaving
the trivial and repetitive ones to be answered by an ES, and (iii) as a natural
consequence, speed up the attentions at the desk service.
We present a knowledge base system composed by ﬁve layers: (i) presentation,
(ii) communication, (iii) interface, (iv) logic and (v) persistence. Experiments
with users, have shown that the proposed system has allowed increasing the
numbers of queries received, handle more requests, decrease the operating time
requests and improve the level of customer satisfaction.
The rest of the paper is organized as follows: in the second section summarizes
the background literature, makes a brief description of the ES, natural language
processing and ontologies. The third section presents, in detail, the architecture
proposed. In section four the implementation of the prototype is detailed. The
results of a survey laboratory are presented in section ﬁve. Finally in section six
conclusions and guides that can be used for future work are presented.
2
Literature Background
According to the literature review, there are two approaches to knowledge based
systems to support the service desks. The ﬁrst one is a direct communication
between the client requesting the service and the agent who attends [1,11]. The
second one is to provide the customer, through the web, a list of the problems
that could be solved with the system and then provide some diagnostics with
possible solutions, all guided by an ES [9,12].
2.1
Expert System - ES
An ES is a software that seeks to emulate the behavior of a human expert in a
speciﬁc domain knowledge. This task is performed with the aid of a knowledge
base (KB), represented in an appropriate manner and with an inference engine
that allows you to make inferences [10,13]. There are various ways of representing
knowledge [16] by rules, using fuzzy logic, ontologies, etc.

ExperTI: A Knowledge Based System
399
2.2
Natural Language Processing - NLP
NLP seeks to recognize and process (either analyzing or synthesizing) natu-
ral language in its spoken or written, for the realization of certain tasks such
as machine translation, iterative dialogues systems, analysis of opinions, etc.
[2,15,17]. Natural language processing techniques has been used to get the
lemmatization of clients’ queries. Commonly, the study of natural language is
structured into four levels of analysis: (i) Morphological analysis, (ii) Syntactic
analysis, (iii) Semantic analysis and (iv) Pragmatic analysis. The goal is to add
semantic information analysis that can provide the participants, development of
discourse itself or hypothetical information with statistical support.
2.3
Ontologies
Studer deﬁnes an ontology as a “formal, explicit speciﬁcation of a shared con-
ceptualization” [25]. Explicit means that the type of concepts used, and the
constraints on their use, are explicitly deﬁned. Formal refers to the fact that
the ontology should be machine-readable. Shared reﬂects the notion that an
ontology captures consensual knowledge, that is, it is not private of some indi-
vidual, but accepted by a group [6]. Thus, ontologies help people and machines
to communicate more eﬀectively.
3
Proposed Architecture
The proposed knowledge-based system has been built using a layer-based
architecture (see 1).
Fig. 1. Layered architecture

400
A. Bello et al.
3.1
Presentation Layer
The presentation layer is constituted by diﬀerent graphical interfaces that the
system oﬀers each of the actors in the service desk: user agent, manager and
engineer of knowledge. The user is who performs the query, the agent is who
monitors the action of the ES.
3.2
Communication Layer
The communication layer is one that allows the interaction between the client
and the agent. This is conducted through a chat server. A chat server allows
writing conversations in real time through the web. This system follows the
client-server model. A server is a program that runs all the time in background
and is generally attentive to answer the requests that are made by any client. A
client is a program that communicates with the server for any request.
3.3
Interface Layer
The interface layer is one that can process ﬂow chains using natural language
processing. The chains that are user submitted through chat client must be
processed and then being chained with the ES. The ﬁrst part of this process
is to standardize the chains, removing extra spaces, as well as tabs, lines and
other changes. A second step is to remove prepositions, articles, conjunctions,
etc. That is to say that the word must be lemmatized.
3.4
Logic Layer
The logic layer is performed by the process capable of “Reason” on the text
entered by the customer. This layer is formed by the inference engine of the
ES, once it receives an event, the system checks that there is some overlap
with some premise of the rules in the KB. When the premises are completed
of one or several rules, these are activated and triggered by priority, generating
new premises (facts) or conclusions that the system will show as a result of a
diagnosis.
3.5
Persistence Layer
The persistence layer is responsible for storing chat history sessions for each user
in a repository. These are recorded for future analysis statisticians and to increase
the KB. The KB is the main storage resource and therefore the main component
of this layer. Here are the rules written in the language chosen appropriately.

ExperTI: A Knowledge Based System
401
4
Prototype Implementation
4.1
The Web Chat
The Play Framework software has been used to implement all the backend and
frontend web chat. This is a modern open source framework designed especially
for writing web applications in Java. Below a segment of code where the chat
room is created:
public void onReceive(Object message) throws Exception{
if(message instanceof Join){
Join join = (Join)message;
if(container.getMembers().containsKey(join.uuid))
getSender().tell("This uuid is already used",getSelf());
else{
...
}
} else if(message instanceof Typing){
Typing typing = (Typing) message;
Usuario usuario = ChatRoomContainer.
usuarioPerfil.get(typing.username);
...
} else if(message instanceof Talk){
Talk talk = (Talk)message ;
...
} else if(message instanceof TalkAgent){
TalkAgent talk = (TalkAgent)message;
...
}
}
4.2
Treatment of Inﬂow
Once the corresponding module receives the ﬂow of strings entered by any user,
these must be processed. First, normalizing work string is performed, this pro-
cess is to leave the chain only with alphanumeric characters, eliminating the rest.
Additionally, multiple whitespace, line changes, tabs, single and double quotes
are replaced by a single blank. Also accents are removed. A second step is to see
if the chain needs to be lemmatized. A string does not need be lemmatized if it
is: (a) a greeting, (b) an aﬃrmation, (c) a rejection, (d) if more information is
needed or (e) there is ignorance. To ﬁnd out if one of these cases use of an devel-
oped ontology using Web Prot´eg´e with Apache Jena as inference engine. If the
chain needs to be lemmatized is passed to the application Freeling, to perform
this work. Freeling removed prepositions and articles and performs lemmatiza-
tion. For example if you receive the following chain: I like to know how can
I create a ticket? after invoking Freeling, would be obtained: like, know,
create, ticket. His chain is ready to be placed in the tree of rules of the KB.

402
A. Bello et al.
4.3
Location in the Decision Tree of the Knowledge Base
For the inference engine of the ES starts their deductions it needs to be placed
in the KB. As the knowledge base is formed by rules, it is formed by one or
more trees with many nodes. The ﬁrst point then is, if we are at the beginning
of a conversation it must ﬁnd the root node of the tree whose premises matching
the change entered by the user. The process of identifying which is the root
corresponding to the premise of the rule, presents the following diﬃculty: how
to look its corresponding node in the tree rules?
The solution adopted in this research is that the premises of the rules should
be identiﬁed with labels each of which will have a weight according to its impor-
tance. Thus when a lemmatized chain arrives, the correlation tag is sought, if
the correlation is full, there is no doubt, and that node is taken as the beginning
of a tree of rules. But if the match is not full then it should be taken the highest
number of matches that has been established. What if the number of matching
matches? then enters settle the weight of the labels, in this case the weights are
added and the one which has the greatest weight is chosen as the starting of the
tree of rules.
For example, given a KB with Node 1 and Node 2:
Node 1 →How to create trades?
Labels(weights) →create(1),trade(5)
Node 2 →How create contacts of a trade?
Labels(weights) →create(1),contact(2),trade(3)
Moreover the synonym of the word create is generate, to identify the syn-
onyms a table in a database is loaded. The customer writes: How to create
new trades? The system lemmatizes: create, new, trade. The system ﬁnds
coincidence: 2 tags in the Node 1 adds 6, and 2 labels on Node 2 totaling 4,
while having the same number of labels but because the sum of the weights of
the ﬁrst node is greater, then this will be selected.
It is clear that if all the tags associated to the nodes of the tree ES were
diﬀerent, then there would be no problems, because the only problem there is, is
for there to be a coincidence. However many tags are repeated, for that reason it
must weigh the degree of entailment of the label with the node. This weighting
or valuation should be evaluated and speciﬁed by the knowledge engineer. The
system initially assigns the value of 1 and subsequent repetitions shows the
highest value that is assigned to that time.
Finally, a second point is to analyze what happens if we are in the middle of
a conversation. In this case our goal will not be seeking the root node of a tree
of rules, but to ﬁnd out what path does it take? according to the node where it
is. In this case use of the ontology is done to identify answers like yes, of course,
no, negative, etc.

ExperTI: A Knowledge Based System
403
4.4
The Expert System and the Knowledge Base
To complete the prototype we built an ES based on rules to guide the use
of Infotrack software. Infotrack is a software designed and implemented by
INFOBOX LATINOAMERICA SAC 1 that allows to monitor the care of cus-
tomer requests represented by tickets, recording information from the generation
of ticket (request to attend some type of service) through the assignment of ticket
to the technician who will attend, logging activities attention and solution, ﬁnally
with the veriﬁcation of the realization of the service so that it could close the
ticket.
Drools expert2 has been used to implement this ES. Drools expert is a compo-
nent of the system business rules management system named drools [3–5,21–23].
It uses forward and backward chaining. A modiﬁed version of the RETE algo-
rithm called PHREAK [18] is used for its inference engine. In addition, this
software is distributed for free under the Apache license. It is written in Java
therefore its interaction in Java projects is natural.
Drools expert has been widely used on research of ESs and intelligent multi-
agents [19,20,24]. In drools, the KB is represented by rules. A rule for Infotrack
system, contained in a ﬁle with extension drl is shown below:
r u l e
” r e g l a
1”
when
h
:
Hecho ( idDiagrama == ”22” ,
pregunta == ”150” ,
respuesta == ” s i ” ,
estado == Hecho .ESTADO EN PROCESO)
then
h . setPregunta ( ” ” ) ;
h . setRespuesta ( ””
) ;
h . setConclusion (
”152”
) ;
h . setEstado ( Hecho .ESTADO FINALIZADO ) ; update ( h
) ;
end
Thereafter the constructor of the KB must be created. The ﬁle of rules is
added to this constructor. Eventually, the KB is created with this constructor,
and is loaded into memory in order for drools to be able to check if a fact matches
any rule and to place it on its agenda and then trigger some rule.
4.5
Operation of the Prototype
The client connects to a web server and chooses the subject on which he wants
to make a query. After receiving the welcome message, the user enters the string
that describes its query. After processing the text, a search is performed in a
database in order to be able to choose the starting point of the ES that initially
will receive the queries.
If the query escape the realm from the ES knowledge base, then the commu-
nication control is taken by the agent in either of the following ways: (1) Either
1 http://www.infobox.com.pe/.
2 http://www.drools.org/.

404
A. Bello et al.
the agent has realized the ES is “stuck” in any query (2) or the ES itself has
asked its intervention. Thus the agent becomes an ES attentions monitor. In
addition, the agent may intervene rebuilding the query made by the user to the
ES, or transferring the query received by the ES to another more experienced
agent. The administrator has the ability to conﬁgure the system, create users,
create proﬁles and set highs attention capabilities or minimal response times, or
alert attention.
On the other hand, the KB should be modiﬁed as queries that are not in
the KB, are resolved. Therefore, a graphical mechanism for knowledge input to
the KB is proposed. This work should be performed by an expert or knowledge
engineer or an experienced agent. This is one of the great features of ExperTI.
Lastly, the system displays statistics about connections and keep historical
records of these, in order to enrich the KB among other uses.
5
Experiments and Results
In order to test the system developed, a study using system users was performed.
For this study, 22 volunteer users were chosen. They were provided a list of
questions with answers. The questionnaire contained questions that were in the
KB and others that were not found in it.
The ﬁrst part of this experiment consisted in response the queries, but with-
out help of the knowledge based system. For this purpose four trained agents
responded the queries. In the second part, the queries were responded by the
KB system, but monitored by the agents. In both cases the test conducted was
the same. The test was run sequentially, ﬁrst without the use of ExpertTI and
then using ExpertTI. Questions were asked completely random.
The number of queries achieved, increases considerably with the use of the
proposed system. After consulting human agents on the causes of increased recep-
tion using ExpertTI, it was concluded that when using the system, human agents
had more free time. This is because the KB system was responsible for answering
many trivial and repetitive queries.
But not only increases the number of queries received, the proportion of
queries handled were also increased. Using the proposed system, the proportion
of queries answered increased by almost 3 times. After analyzing the possible
causes of this increase, it was concluded that increased attention was mainly due
to reuse of knowledge. This knowledge as “understood” by the computer, it can
use it to automatically answer requests from users.
The level of user satisfaction was also evaluated. Most customers are very sat-
isﬁed with the results of the proposed system. The number of satisﬁed customers
is doubled when the system is used. This means that improvements implemented
by the system are perceived both internally to the service desk and externally
by users treated.

ExperTI: A Knowledge Based System
405
6
Conclusions and Future Works
In this article we present ExpertTI, a knowledge-based system, which can pro-
cess queries made in free text. This system serves to support the service desk
because it allows them to signiﬁcantly increase the number of queries received,
the proportion of cases handled, and the user satisfaction. The presented system
has been implemented using a layer-based architecture.
By conducting a study with system users it has been able to conclude that
(i) the number of queries handled with ExperTI is higher than when ExperTI is
not used, (ii) when ExpertTI is used, the number of queries unattended is less
than the number of queries unattended ExperTI is not used, (iii) the response
time of the queries when ExperIT is used is lower than when it is not used and
(iv) the number of queries very satisﬁed is greater when ExperTI is used than
when it is not used.
In order to further improve the eﬃciency of the proposed system, some future
work is emerging to continue this line of technological development. The natural
language processing module could be expanded and improved, adding an infer-
ence engine to reason and generate natural language text not ﬁxed a priori, as
it is currently done, the module will be able to generate a language. This will
allow the system to deal with complex expressions on a normal conversation.
Acknowledgement. The authors wishes to thank to The National Program of
Innovation for Competitiveness and Productivity (Inn´ovate Per´u) (http://www.
innovateperu.gob.pe/), who is the government agency that funded this project with
INFOBOX LATINOAMERICA SAC Company technology solutions.
References
1. Abraham, D.M., Spangler, W.E., May, J.H.: Expertech: issues in the design and
development of an intelligent help desk system. Expert Syst. Appl. 2(4), 305–319
(1991)
2. Allen, J.F.: Natural language processing. Encyclopedia of Computer Science, pp.
1218–1222. Wiley, Chichester (2003)
3. Amador, L.: Drools Developer’s Cookbook. Packt Publishing Ltd., Birmingham
(2012)
4. Bali, M.: Drools JBoss Rules 5.0 Developer’s Guide. Packt Publishing Ltd. (2009)
5. Browne, P.: JBoss Drools Business Rules. Packt Publishing Ltd., Birmingham
(2009)
6. Calero, C., Ruiz, F., Piattini, M.: Ontologies for Software Engineering and Software
Technology. Springer, Berlin (2006)
7. De Oliveira, T.B., Nunes, F.B., Voss, G.B., Medina, R.D., De Lima, J.V.: Rec-
ommendation system for assisting the management of information technology. In:
UBICOMM 2014–8th International Conference on Mobile Ubiquitous Computing,
Systems, Services and Technologies. pp. 72–79 (2014)
8. Fenner, G., Lima, A., Souza, N.d., Moura, A., Andrade, R.: A system dynamics
model for managing service desk capacity. In: IFIP/IEEE International Symposium
on Integrated Network Management (IM), pp. 1424–1427 (2015)

406
A. Bello et al.
9. Foo, S., Hui, S.C., Leong, P.C., Liu, S.: An integrated help desk support for cus-
tomer services over the world wide web – a case study. Comput. Ind. 41(2), 129–145
(2000)
10. Giarratano, J., Riley, G.: Expert Systems: Principles and Programming, 4th edn.
Course Technology, Reading (2004)
11. Gonz´alez, L.M., Giachetti, R.E., Ramirez, G.: Knowledge management-centric help
desk: speciﬁcation and performance evaluation. Decis. Support Syst. 40(2), 389–
405 (2005)
12. Ho Kang, B., Yoshida, K., Motoda, H., Compton, P.: Help desk system with intel-
ligent interface. Appl. Artif. Intell. 11(7–8), 611–631 (1997)
13. Jackson, P.: Introduction to Expert Systems. Addison-Wesley Publishing Com-
pany, Reading (1986)
14. Jan, E.E., Chen, K.Y., Id´e, T.: Probabilistic text analytics framework for informa-
tion technology service desk tickets. In: IFIP/IEEE International Symposium on
Integrated Network Management (IM), pp. 870–873 (2015)
15. Jurafsky, D., Martin, J.H.: An Introduction to Natural Language Processing, Com-
putational Linguistics, and Speech Recognition. Prentice Hall, Upper Saddle River
(2000)
16. Liao, S.H.: Expert system methodologies and applications–a decade review from
1995 to 2004. Expert Syst. Appl. 28(1), 93–103 (2005)
17. Manning, C.D., Sch¨utze, H.: Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge (1999)
18. Merilinna, J.: A mechanism to enable spatial reasoning in JBoss drools. In: 2014
International Conference on Industrial Automation, Information and Communica-
tions Technology (IAICT), pp. 135–140, August 2014
19. Nair, M.K., Kakaraddi, S.M., Ramanarayan, K.M., Gopalakrishna, V.: Agent with
rule engine: The glue for web service oriented computing applied to network man-
agement. In: Proceedings of the 2009 IEEE International Conference on Services
Computing, SCC 2009, pp. 528–531, IEEE Computer Society, Washington (2009)
20. Park, J., Lee, H.C., Lee, M.J.: JCOOLS: A toolkit for generating context-aware
applications with JCAF and DROOLS. J. Syst. Architect. 59(9), 759–766 (2013)
21. Proctor, M.: Drools: a rule engine for complex event processing. In: Applications
of Graph Transformations with Industrial Relevance, pp. 2–2. Springer, Heidelberg
(2012)
22. Proctor, M., Neale, M., Lin, P., Frandsen, M.: Drools Documentation. JBoss.org,
Technical Report (2008)
23. Shi, J., Qiao, Y., Wang, H.: Visualizing inference process of a rule engine. In:
Proceedings of the 2011 Visual Information Communication - International Sym-
posium, VINCI 2011, pp. 10:1–10:9. ACM, New York (2011)
24. Sottara, D., Mello, P., Sartori, C., Fry, E.: Enhancing a production rule engine
with predictive models using PMML. In: Proceedings of the 2011 Workshop on
Predictive Markup Language Modeling, PMML 2011, pp. 39–47. ACM, New York
(2011)
25. Staab, S., Studer, R.: Handbook on Ontologies. Springer, Heidelberg (2010)

Mapping the Global Oﬀshoring Network
Through the Panama Papers
David Dominguez1, Odette Pantoja2, and Mario Gonz´alez3,4(B)
1 Escuela Polit´ecnica Superior, Universidad Aut´onoma de Madrid,
28049 Madrid, Spain
david.dominguez@uam.es
2 FCA, Escuela Polit´ecnica Nacional, Quito, Ecuador
odette.pantoja@epn.edu.ec
3 COPA, Department of Industrial Engineering, Universidad de los Andes,
Bogot´a, Colombia
ms.gonzalez@uniandes.edu.co
4 FICA, Universidad de las Am´ericas, Quito, Ecuador
Abstract. This works maps the oﬀshoring network between regions and
countries worldwide through the Panama Papers. The Panama Papers
2016 divulgence is the largest leak of oﬀshoring and tax avoidance docu-
mentation. The leaked documents contain 2.6 TB of information involv-
ing more than two hundred thousand of enterprises in more than two
hundreds countries. Using the Oﬀshore leaks database we related entities
around the world through diﬀerent types of relationships. These relation-
ships were used in order to build an oﬀshoring network at countries and
geographical regions scales. The network is characterized and described
using chord diagrams to map the intra and inter relation between the
countries and regions, discovering which of them are the more prominent
in the worldwide oﬀshoring scenario.
Keywords: Corruption network · Panama Papers · Oﬀshore societies
Tax havens
1
Introduction
Recently, scandals related with tax havens and company oﬀshoring have gained
public interest. In this sense, tax havens are geographic areas which oﬀer low
or zero tax rates with the purpose of encouraging foreign investments [4]. This
practice has quickly evolved in the last 25 years. The oﬀshore ﬁnancial econ-
omy is a corruption alternative to reinforce capital ﬂight and tax evasion. Under
this framework, natural persons and corporations non residents in tax havens,
accumulate ﬁnancial capital without strict regulations being all the informa-
tion protected by a secret veil that prevents the transparency of operations and
ownership [3]. The banking and commercial secrecy implies the unawareness of
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_39

408
D. Dominguez et al.
the identity of company owners, permission to designate second persons who
act as nominated directors hiding the real owners and the privilege to create
accounts which are not registered publicly [12].
In the individuals case, which are in many circumstances corrupt autocratic
oﬃcials, politicians, and even sportspeople and movie stars, they are interested
to guard their assets in havens, due to this income comes from illegal bribery
or for tax evasion purposes, the latter being the principal motive in the ﬁrms’
case [10].
In the quantitative order, about 15% of countries are tax havens, which
are characterized by being small, under 1 million of population [4]. In 2014, the
71.6% of the Fortune 500 companies (358 enterprise) had as minimum 7,622 tax-
haven subsidiaries [2]. With these subsidiaries, ﬁrms are evading around $90 USD
billions in federal taxes. There are abut 50–60 tax havens, where are located more
than the 30% of the global foreign direct investment [8]. Manufacturing compa-
nies from higher tax countries are more likely to have oﬀshore operations than
services industries [7], as well private ﬁrms have a greater tax reduction thanks
tax havens than public companies [11]. There are several implications of the use
of oﬀshore [12], within which they are detected that tax-base of non-havens coun-
tries deteriorates since the amount collected from taxes decreases. Tax evasion
generates at the same way, an additional control increasing regulation cost.
The Panama papers deal with the largest leak of documents in the history.
These documents were in the Panamanian law ﬁrm Mossack Fonseca whose main
function was to act as intermediary to create oﬀshore societies. The documents
reveal how thousands of people hid their assets in companies located in tax
haven. The 2.6 TB sized leaked documents containing information of 214,488
oﬀshore entities which connect 208 diﬀerent territories.
From the Oﬀshore leak database data, which include the Panama papers, a
network of connections between countries is built based on the relations that
appear in the database. The network is characterized in the scale of countries
and geographical regions, to discover the structure of the worldwide oﬀshoring
network. The rest of the article is organized as follows. In the following Sect. 2 we
describe the ensemble model. In Sect. 3 we present the main results and Sect. 4
concludes the paper discussing the implications of our ﬁndings.
2
The Model
In this section the proposed methodology to build the oﬀshoring networks is
introduced, starting with the data collection, and the network construction from
the data. Finally, the procedure to describe the network structure using chord
diagrams is presented.
2.1
Data Collection and Organization
The data was obtained from the Panama Papers Oﬀshore leaks Database
(https://oﬀshoreleaks.icij.org/), where the data can be downloaded as csv ﬁles.

Mapping the Global Oﬀshoring Network Through the Panama Papers
409
The data has information of entities, intermediaries, oﬃcers, addresses and edges
between them.
Entities correspond to enterprises created in tax havens. There are 495,309
entities with the following attributes: name, original name, former name, juris-
diction, jurisdiction description, company type, address, Internal id, incorpora-
tion date, inactivation date, struck oﬀdate, dorm date, status, service provider,
ibcRUC, countr codes, countries, note, valid until, node id, sourceID.
Intermediaries correspond to Law ﬁrms or other intermediaries that oﬀer
oﬀshoring services. There are 24,183 intermediaries with the following attributes:
name, internal id, address, valid until, country codes, countries, status, node id,
sourceID, note.
Oﬃcers correspond to individuals or companies with a role in the oﬀshore
entity. There are 370,873 oﬃcers with the following attributes: name, icij id,
valid until, country codes, countries, node id, sourceID, note.
Addresses corresponds to postal addresses of companies and individuals in
the database. There are 151,665 addresses and the attributes are: address, icij id,
valid until, country codes, countries, node id, sourceID, note.
The edges data contains the existing relations between all the above tables
and is product of text mining the Panama Papers. The attributes available are
node 1, rel type, node 2, sourceID, valid until, start date, end date. Here node 1
refers to the issuing entity and node 2 refers to the receiver entity. We use the
relationship (rel type) information to build the connectivity between node 1 and
node 2. There are 19 types of relationships such as shareholder, beneﬁciary of,
intermediary, president, secretary, director, to mention some of them.
A PostgreSQL database was built with the aforementioned tables, in order
to facilitate the extraction of information through database queries.
2.2
Network Construction
In order to build the network a set of database queries are performed in order
to relate pairs of countries. The weight connection between pairs of countries
is proportional to the number of appearances from the queries obtained for all
possible relations according to entities, intermediaries, oﬃcers, addresses
and edges. Inner join subqueries are performed in order to get all possible
relations between each pair of countries. Finally, a high level query is done to
aggregate all the results of similar tuples returned in the subqueries. In this way
one gets a raw value for the total number of appearances ar
ij of pairs of countries
i, j for each type of relationship r. These values are normalized between 0 and
4 for each of type of the 19 types of relationships r ∈{0, 1, . . . , 19} and for each
country i. The normalization Tr and the weight of the relationship Rij between
countries i and j is calculated as follows
Rij =

r
Tr,
Tr =
ar
ij
max(ar)max(ai) × 4, r ∈{0, 1, . . . , 19}.
(1)

410
D. Dominguez et al.
Then the network connectivity matrix Cij is built as follows:
Cij =

1, if Rij > θ,
0, otherwise.
(2)
Here, θ is related to the value of the desired quantile used to build the mesoscopic
networks obtained in the next Sect. 3.
Then, the oﬀshoring network is described by the adjacency matrix Oij =
CijRij. This adjacency matrix O is used to characterize the oﬀshoring network
structure between countries using chord diagrams.
2.3
Characterizing the Network
The network structure obtained in the previous subsection is characterized for
countries and also in mesoscopic geographical relations. A chord diagram is used
to display the intra- and inter-relationships between the blocks of the meseso-
copic network and between countries (see Sect. 3). The blocks are arranged radi-
ally around a circle and the relationships are drawn as arcs that connect the
blocks. A connection is represented as internal when it connects a country itself
or in the case of geographical regions when connects countries belonging to the
same regions. Chord diagrams are a very intuitive way to depict the structure
of networks, and have been used to describe migration ﬂows [1], and Enterprise
Sustainability Reporting Maps [5,6] to mention a few works.
To construct the chord diagrams that are presented in Figs. (1 and 2), the
strength of these internal and external connections is measured. To determine
the size of the connection, the chord diagram considers the number of links
between enterprises among the regions/countries and the size (weighted degree)
of the region/country where the connections originated.
3
Results: Mapping Oﬀshoring Networks
This section depicts the oﬀshoring network obtained in the previous section
and discuss the network structure in terms of countries and world geographical
regions using chord diagrams.
3.1
Oﬀshore Maps Relation for Regions
The principal involved regions in the oﬀshoring network are listed with their
acronyms in Table 1. These regions are America Antilles, Central America, North
America, South America, South Asia, Eastern Asia, South East Asia, Slavic
Europe, Germanic Europe, Sub-Saharan Africa, Latin Europe, Oceania and
Middle East.
Figure 1 shows the oﬀshoring network relationship structure for the 80th (left)
and 90th (right) percentiles of world geographical regions obtained from the
mesoscopic conﬁguration of the network deﬁned in the previous Sect. 2. The size

Mapping the Global Oﬀshoring Network Through the Panama Papers
411
of the segments in the chord diagram is proportional to the number of countries
in the geographical block and their weighted importance in terms of R. The
width of the links between segments indicates countries’ connections that move
from one region to another. Clearly, the links are thicker when they connect the
largest blocks and according to the weighted relationship R between countries
belonging the group. One should observe that, the chord diagrams represent the
outgoing inﬂuences as connections that have a larger gap from the departing
region/country, and the incoming inﬂuences are represented by a smaller gap
from the arriving region/country.
For example, the relationship between ASO (East Asia) and AMA (America
Antilles) indicates a strong relationship between entities belonging to the East
Asia (ASO) countries oﬀshoring to the Antilles. Similarly, the relationship between
South-East Asia (ASE) and the Antilles (AMA). This means that the Antilles is
receiving a lot of oﬀshoring operations from these regions (ASO, ASE). On the other
hand the Antilles is not exerting a big inﬂuence to other regions, but has a large
self connections, which indicates a strong presence of relationship among the coun-
tries/entities inside the region. Likewise, East Asia has large oﬀshore relationships
with companies based in Oceania (OCE); and with other entities settled in the East
Asia region. Analyzing South East Asia, their oﬀshoring entities, in addition to be
related to Antilles as was mentioned previously, are also related with Oceania and
with companies from the region. In Slavic Europe (EUS), Germanic Europe (EUG)
and Latin Europe (EUL) theirs oﬀshore entities predominantly establish relations
with ﬁrms from the same region. In Slavic Europe case, but in small amount, part
of the relations are develop with entities from South East Asia and Antilles, pre-
vailing the oﬀshoring relationship inside this region. Germanic Europe in a smaller
scale, also establish oﬀshore relations with South East Asia, Antilles and Latin
Europe.
Table 1. Regions’ acronyms
Region
Acronym Region
Acronym
Maghreb Africa
AFM
Sub-Saharan Africa AFS
America Antilles
AMA
Central America
AMC
North America
AMN
South America
AMS
South Asia
ASS
Eastern Asia
ASO
South East Asia
ASE
Slavic Europe
EUS
Germanic Europe EUG
Latin Europe
EUL
Oceania
OCE
Middle East
ORM
Oceania is a region that mainly receives oﬀshoring operations, about 70% in
comparison with the emitted operations (30%). From this 70%, a smaller part
corresponds to internal operations and the bigger one corresponds to external
companies. This means, that the most parts of the oﬀshoring entities are from

412
D. Dominguez et al.
Fig. 1. Oﬀshoring maps for world regions. Left: 80th percentile of regions. Right: 90th
percentile of regions.
regions like North America, South East Asia and Eastern Asia, with links to
Oceania by relations based in address, intermediaries, oﬃcers, among others. On
the other hand, Middle East (ORM) received operations from their owns entities,
but also their entities establish relations with South East Asia and Antilles.
Central America (AMC) establish operations with their own entities and with
Antilles, in both directions, as a receiver and as a transmitter. In North America
(AMN), more than a half of the oﬀshoring operations are between their entities
and regions like Oceania, Antilles and South Easts Asia, which means that is
a region where large oﬀshored entities are generated; where connections like
intermediaries, oﬃcers and address are established with ﬁrms or persons from
the aforementioned regions.
There are three remaining regions to be included in the analysis: South Amer-
ica (AMS), Africa Sub-Saharan (AFS) and South Asia (ASS), which in spite of
having few relationships compared to the other analyzed regions, it is worth
studying their behavior. South America are linked basically with their own enti-
ties and with Antilles, Africa Sub-Saharan are only linked it with South East
Asia; and South Asia are connected with Antilles and South East Asia.
Up to this point, is valid to highlight that from the thirteen regions involved,
only four account for the major oﬀshored operations: Antilles (the big one),
Eastern Asia, South East Asia and Oceania. Of all the studied regions, the pre-
dominant behavior is that these regions contain oﬀshored entities, which estab-
lish relationships with other companies or people from the same region or from
other regions. Only Antilles and Oceania are shown as regions where they pre-
dominantly receive operations, based on connections with intermediaries, oﬃcers,
address, among others. These internal relationships can be seen as the network
of entities to cover the trace of the receiving oﬀshoring operations from other
regions.

Mapping the Global Oﬀshoring Network Through the Panama Papers
413
3.2
Oﬀshore Maps Relation for Countries
Figure 2 shows the oﬀshoring relationship for the top 0.1 (left) and top
0.05 (right) percent of countries in the network. The countries involved are
Hong Kong, Guernsey, United Kingdom, Cyprus, Cayman Islands, Cook Islands,
China, Switzerland, Bahamas, Samoa, British Virgin Islands, Venezuela, United
States, Taiwan, Thailand, Singapore, Russia, Panama, Malaysia, Jersey, India
and Indonesia (see Table 2).
Taiwan (TWN) oﬀshore entities establish relations predominantly with Samoa
(WSM) and British Virgin Island (VGB). Samoa is an oﬀshore ﬁnance center since
1987 and the sixth most popular tax havens in the Panama Papers, which is
focused in the provision of international business companies, being their principal
market the countries from South East Asia [15]. In the Oﬀshore Leaks Database
it is possible to conﬁrm the existence of 265 oﬀshore entities in Samoa jurisdiction
linked to Taiwan [14]. British Virgin Island occupies in the representation of the
Fig. 2. Oﬀshoring maps for countries. Left: Top 0.1% (quantile (0.999)) of countries.
Right: Top 0.05% (quantile (0.9995)) of countries.
Table 2. Countries’ acronyms
Country
Acronym
Country
Acronym
Country
Acronym
Hong Kong
HKG
Guernsey
GGY
United Kingdom
GBR
Cyprus
CYP
Cayman Islands
CYM
Cook Islands
COK
China
CHN
Switzerland
CHE
Bahamas
BHS
Samoa
WSM
Jersey
JEY
Venezuela
VEN
United States
USA
Taiwan
TWN
Thailand
THA
Singapore
SGP
Russia
RUS
Panama
PAN
Malaysia
MYS
Indonesia
IDN
India
IND
British Virgin Islands
VGB

414
D. Dominguez et al.
oﬀshoring map the greater portion, and it is recognized by Mossack Fonseca’s
ﬁles as the favorite tax haven, where about 113,000 entities are registered, 1819 of
which, are linked with Taiwan. Likewise, British Virgin Island has an important
volume of operations with countries like Singapore, Malaysia, India, Indonesia,
Hong Kong and China.
Singapore (SGP), despite having a business friendly tax regime, maintains
a higher connection with British Virgin Island, as was previously mentioned,
and with Cook Island (COK) both of which are no tax territories. There are
4027 registered entities in VGB related with Singapore and 48 registered in COK.
Similarly, Singapore has reported 706 oﬀshore own entities related with itself,
by operations that are supported by ﬂexible tax politics.
Panama (PAN) is known as a popular tax haven, having predominantly inter-
nal relationships and in a minor scale with Bahamas (BHS), with 1120 oﬀshore
entities with jurisdiction in BHS linked with PAN. Malaysia (MYS), although
having a lower weight compared to the other countries analyzed, it is possi-
ble to reﬂect that their principal operations are with British Virgin Island and
Indonesia (IDN). Jersey (JEY) is another hosts of oﬀshore ﬁnance centers, where
the operations coming from this activities represent 90% of its government rev-
enues [9]. Their principal operations are related with other entities or person
from the same country.
India (IND) establish their principal oﬀshore relations with British Virgin
Island, and it has been studied that Indian entities related with to tax havens,
pay 30% less tax than another ﬁrms without these connections [13]. Indonesia
also are linked with British Virgin Island, with small relationships with another
countries like Singapore and Samoa. Likewise, Indonesia is related, as an oper-
ation receiver, from Singapore and Thailand (THA).
Hong Kong (HKG) is considered as another trendy tax haven in the Panama
Papers where the companies are looking for tax avoidance/evasion [13]. The 90%
of Hong Kong gross domestic product (GDP) comes from the service industries,
being the ﬁnancial services included one of the ﬁve most important industry ser-
vices. The principal relations detected are with British Virgin Island, in lesser
extent with Samoa and with companies and persons from the same country. It
has been found that such evaded tax origin are related with money launder-
ing and ﬁnancing of terrorist activities, with about 500 entities connected with
Hong Kong [17].
Guernsey (GGY), a Britain’s crown dependencies, is another international
ﬁnancial center which have larger operations with itself. Cook Island (COK) is
a Paciﬁc island, which “concentrates on forming trusts to protect assets from
seizure by courts, wives, husbands or creditors” [16, p. 652]. Their principal rela-
tions are with companies from the same country, and are also linked it as an
operations receiver with Singapore and United Stated (USA); been the connec-
tion with USA the larger. It is worth noting that USA maintains its largest
volume of oﬀshore operations with Cook Island, and in a lesser extent with the
Virgin Islands.

Mapping the Global Oﬀshoring Network Through the Panama Papers
415
China (CHN) has links with Hong Kong, being their ﬁrst investor, followed by
British Virgin Island [18]. China, like India, has propitiated zones with special
economic regulations with attractive tax systems in order to attract oﬀshoring
investors.
There are another countries contemplated in the Fig. 2 left, which contain
minority operations compared to the rest of the ones analyzed. One of them is
Venezuela (VEN) linked with Virgin Islands as well as Thailand (THA), which also
is connected with Indonesia. Russia (RUS), Cyprus (CYP), Switzerland (CHE) and
United Kingdom (GBR) have mainly small connections with themselves. Caymand
Island (CYM) is linked as a receiver with Taiwan and with it self, and Bahamas
(BHS) is connected with Panama and Virgin Island. Again, the self connections
can be interpreted as the internal network to cover the oﬀshoring operations
from their external investors. This internal operations are highlighted in the left
panel of Fig. 2, in contrast to the right panel where a larger variety of external
relations still can be appreciated.
Its is interesting to highlight that of the 22 countries analyzed, there are
three that cover the greater proportion of oﬀshore relationships: Virgin Island
(1st place), Hong Kong (2nd) and Singapore (3rd). It has been remarked the
high participation of Virgin Islands within the world of tax havens, being an
area highly welcomed by countries of diﬀerent regions as a ﬁnancial center. It is
also noted that most of the countries studied have relations not only with other
regions, but also with companies and entities from the same country.
4
Conclusions
We have studied the oﬀshoring relations for the principal countries and world
geographical regions. The network was built from the diﬀerent relations appear-
ing between emitting and receiving entities from where we have traced their
corresponding countries. The network of entities has been represented mesoscopi-
cally by countries and geographical regions and their intra and inter relationships
have been mapped.
As commented before, we have identiﬁed the main oﬀshoring regions/
countries and how they are related to the rest of the world. The major oﬀ-
shoring receiving regions by operations are Antilles, Eastern Asia, South East
Asia and Oceania. And the major oﬀshoring receiving countries are Virgin Island,
Hong Kong and Singapore. Also, the dimension of internal relationships is impor-
tant, since can give a measurement of the internal network used by the countries
in order to hinder the tracking of money in tax havens.
Building a network from socio-economic data, and discribing it in a meso-
scopic scale, in this case by countries and geographical regions can give inter-
esting insights to these systems. Using this approach in order to tackle socio-
economic problems is of interest in order to get a plausible description of the
structure and intra/inter relationships of the system components. As a further
work, we can apply clustering analysis to this network in order to get relation-
ships that can be linked not only to the geographical analysis done in this work.

416
D. Dominguez et al.
Also, we can apply this analysis to gender equality, LGBT, corruption data,
among others, which are of current interest in sociophysics.
Acknowledgments. This work was funded by UAM-Santander CEAL-AL/2017-08,
and UDLA-SIS.MG.17.02.
References
1. Abel, G.J., Sander, N.: Quantifying global international migration ﬂows. Science
343(6178), 1520–1522 (2014)
2. Akamah, H., Hope, O.K., Thomas, W.B.: Tax havens and disclosure aggregation.
J. Int. Bus. Stud. 1–21 (2016)
3. Christensen, J.: The looting continues: tax havens and corruption. Crit. Perspect.
Int. Bus. 7(2), 177–196 (2011)
4. Dharmapala, D., Hines, J.R.: Which countries become tax havens? J. Pub. Econ.
93(9), 1058–1068 (2009)
5. Gonz´alez, M., del Mar Alonso-Almeida, M., Avila, C., Dominguez, D.: Modeling
sustainability report scoring sequences using an attractor network. Neurocomput-
ing 168(30), 1181–1187 (2015)
6. Gonz´alez, M., del Mar Alonso-Almeida, M., Dominguez, D.: Mapping global sus-
tainability report scoring: a detailed analysis of Europe and Asia. Qual. Quant.
1–15 (2017)
7. Gonzalez-Navarro, M., Quintana-Domeque, C.: Paving streets for the poor: exper-
imental analysis of infrastructure eﬀects. Rev. Econ. Stat. 98(2), 254–267 (2016)
8. Haberly, D., W´ojcik, D.: Tax havens and the production of oﬀshore FDI: an empir-
ical analysis. J. Econ. Geogr. 15(1), 75–101 (2014)
9. Hampton, M.P., Christensen, J.: Oﬀshore pariahs? Small island economies, tax
havens, and the re-conﬁguration of global ﬁnance. World Dev. 30(9), 1657–1673
(2002)
10. Hebous, S., Lipatov, V.: A journey from a corruption port to a tax haven. J. Comp.
Econ. 42(3), 739–754 (2014)
11. Jaafar, A., Thornton, J.: Tax havens and eﬀective tax rates: an analysis of private
versus public European ﬁrms. Int. J. Account. 50(4), 435–457 (2015)
12. Jalan, A., Vaidyanathan, R.: Tax havens: conduits for corporate tax malfeasance.
J. Financ. Regul. Compliance 25(1), 86–104 (2017)
13. Michael, B., Goo, S.H.: The role of Hong Kong’s ﬁnancial regulations in improving
corporate governance standards in China: lessons from the Panama papers for
Hong Kong (2016)
14. Oﬀshore
Leaks
Database.
https://oﬀshoreleaks.icij.org/search?c=TWN&e=&
j=SAM&q=&utf8=%E2%9C%93. Accessed 30 July 2017
15. Rawlings, G., et al.: Oﬀshore ﬁnance centres: institutions of global capital and sites
of cultural practice (2005)
16. Vlcek, W.: Tax havens and sovereignty in the Paciﬁc islands (2013)
17. Wang, J.: Losing out in powering innovation: the necessity of introducing research
and development tax incentives in Hong Kong (2017)
18. Weichenrieder, A.J., Xu, F.: Are tax havens good? Implications of the crackdown
on secrecy (2015)

Comparing Diﬀerent Data Fusion Strategies
for Cancer Classiﬁcation
Katarzyna Pojda, Michal Jakubczak, Sebastian Student, Andrzej ´Swierniak,
and Krzysztof Fujarewicz(B)
Silesian University of Technology, Akademicka 16, 44-100 Gliwice, Poland
krzysztof.fujarewicz@polsl.pl
Abstract. Automatic cancer diagnosis can be performed based on dif-
ferent types of data sets. Some of them are microarray data, clinical
trial data and cytopathological data. Usually class prediction is done by
chosen classiﬁcation method, for example a machine learning algorithm,
and uses only one type of the available data. In this work an additional
predictive value of a fusion of these three types of data is examined. To
perform such research, authors upgrade and use their recently developed
Spicy system. Diﬀerent data fusion strategies have been tested on thy-
roid cancer data set. The workﬂow that has been created and the new
module of a data fusion implemented in the Spicy system allows to qual-
ify fusion of microarray data, clinical trials data and information about
the Bethesda system class as a valuable method of prediction the thyroid
nodule malignancy.
Keywords: Cancer classiﬁcation · Feature selection · Data fusion
Machine learning
1
Introduction
The proper diagnosis and treatment of a cancer are rapidly developed ﬁeld of the
research. Plenty of data could be gathered on patients for diagnostic purposes.
Besides traditional information such as gender, age or medical history, new types of
data can be obtained as the technology keeps moving forward. One of the increas-
ingly common and available in daily diagnosis type of data is molecular data.
New tools and techniques supporting decisions in treatment are still being
created, but most of them consider all types of the information as a separate and
independent, or even its exploration is narrowed down to one type of available
data. Some examples of such systems are described in [1]. Tests mentioned in
the article process only molecular data to predict malignancy of thyroid nodule.
Another technique which process only one type of data and is used for thyroid
tumor examination tumor is an assessment of the specimen taken by a ﬁne-needle
aspiration (FNA) using the Bethesda System for reporting thyroid cytopathol-
ogy. Other examples of application of microarray data to classiﬁcation of thyroid
cancer were presented for example in [2–4].
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_40

418
K. Pojda et al.
In the past years, great interest for both integrating and fusing diﬀerent
types of biological data has arisen. However, to perform such operations a system
suitable to all challenges related to data combination is required. One of them
is possessing tools adapted to process all types of considered data. In the case
of molecular data processing, the system has to be adapted to a large-scale
data analysis. The synergetic eﬀect of fusion of microarray and clinical data in
breast cancer is discussed for example in [6], or [7]. But, according to our actual
knowledge, there is a lack of a research which demonstrates results of microarray
and clinical data combination on thyroid tumor data set or discuss the eﬀect of
direct fusion of microarray data and information about the Bethesda system
class.
We use and develop the functionality of the Spicy system [8] to assess the
additional predictive value of using fused genomic, clinical and cytopathological
data in thyroid nodule malignancy prediction.
2
Concept Development
2.1
Data Fusion
According to [9], two methods of using data obtained from diﬀerent sources can
be distinguished: data integration and data fusion. The method is classiﬁed as
data integration when it uses data from several sources individually. For exam-
ple, when results from a procedure, which process one type of data, is used as
an additional ﬁltering or verifying parameter of procedure, which processes the
second type of data. Data fusion methods use all data sources in combination as
an input of the same procedure. The diﬀerence between that two approaches is
presented in Fig. 1. Of course, there is no purpose of performing data fusion or
Fig. 1. Diﬀerence between the data integration and the data fusion approaches. (a)
scheme of example data integration method, (b) scheme of data fusion method.

Comparing Diﬀerent Data Fusion Strategies
419
data integration if there is no improvement in obtained results in comparison to
the analysis performed on the single data source. The level of such improvement
should be known and assess for the speciﬁc research problem. We decided to
examine the data fusion approach ﬁrst.
2.2
Microarray Data
DNA microarrays allow to evaluate thousands of genes expression at once. The
rapid development of that technique during recent years has been observed. As
it is known that cancer is related to disorders in genes expressions, microarrays
are considered as a useful tool for better understanding mechanisms underlying
tumor genesis and cancer treatment. However, usefulness of microarrays in clas-
siﬁcation is debatable and there are diﬃculties related to their analysis. They
are much more expensive to acquire than clinical trials, which is still a serious
drawback in their usefulness in daily treatment of patients. As microarray data
consist of thousands of transcripts, their analysis could be diﬃcult and unreliable
because of so-called “curse of dimensionality”. Many standard analysis methods
do not work properly, when there are more variables than samples in data set
and may be more aﬀected by an overﬁting problem. However, microarray data
could contain information which are not possible to notice in simple clinical
predictors.
2.3
The Bethesda System
Fine-needle aspiration (FNA) is a diagnostic procedure of a tissue collection. The
sample of cellular material taken during FNA is analyzed in the cancer detection
purpose. One of the most common methods of analysis of the sampled tissue is
a cytological assessment. The Bethesda system is a reporting system for thyroid
FNA, which was introduced to facilitate eﬀective and uniform communication
in thyroid FNA interpretations [5]. It consists of 6 general diagnostic categories,
from which 5 describe suspected type of nodule and cancer risk (categories from
II to VI). Category I describes the inappropriate and nondiagnostic sample. In
fact, according to [10], in 10–30% of cases samples belong to III, IV or V Bethesda
category are indeterminate. Only samples with category II (certain benign) or
VI (certain malignant) are considered to be certain in diagnosis.
3
System Design
To assess the additional predictive value of fusion of microarray data with gene
expression levels, clinical trial data and information about the Bethesda System
class we consider ﬁve classiﬁers distinguished by its structures in dependence of
processed types of the data sets. Five combinations of available types of data
sets were used: microarray data (referred as M), fusion of microarray and clinical
data (referred as M+Cl), fusion of microarray data, clinical data and informa-
tion about the Bethesda system class (referred as M+Cl+B), clinical trial data

420
K. Pojda et al.
(referred as Cl) and fusion of clinical data and information about the Bethesda
system class (referred as Cl+B). The main goal of a task is to solve “benign or
malignant” problem. Schemes of all classiﬁer structures are presented in Fig. 2.
Performing the feature selection on the microarray data set is necessary to reduce
the large dimensionality of that type of data. First, high dimensionality could
overshadow the classiﬁcation results. Second—the data fusion block merges fea-
tures without distinguishing between types of applied data. It could lead to the
inappropriate assessment of obtained results, if dimensionalities of the data sets
are not consistent. Even if a value of prediction of the clinical data is high, it is
probably to get lost within the huge dimensionality of the microarray data set.
However, it is not known for what number of selected features accuracy of the
classiﬁer will reach the highest score. Thus, for each structure, which use the
feature selection module, inﬂuence of parameter changes on the obtained by the
classiﬁer accuracy was investigated and the best model was chosen.
Fig. 2. Five structures of created classiﬁers. (a) classiﬁer designed to process M, (b)
classiﬁer designed to process Cl, (c) classiﬁer designed to process Cl+B, (d) classiﬁer
designed to process M+Cl, (e) classiﬁer designed to process M+Cl+B.
4
System Prototyping
Described in Sect. 3 classiﬁers and operations were performed using the currently
developed by the authors, the SPICY classiﬁcation system [8], which is a part of a
bigger BioTest system [13–15]. The following functionality was added: creation
of classiﬁers which perform data fusion, testing and validation of a classiﬁer,
checking an inﬂuence of a parameter changes on results obtained by classiﬁer.
To perform data fusion and create classiﬁers 2(c), 2(d) and 2(e), we introduce
to the system a new data fusion module, which complies the scheme of data fusion
presented in Fig. 1. To select optimal number of features for the feature selection
stage we implemented a models selection tool.
Spicy is an application for supervised data analysis, created in the R
language. Thanks to the modular construction of the system, it provides a high

Comparing Diﬀerent Data Fusion Strategies
421
degree of ﬂexibility in creating various structures of classiﬁers and analysis work-
ﬂows. Regardless of chosen structure, a proper scheme of model assessment,
protected from so-called “information leak” is provided. “Information leak” is
a situation when test data are used during classiﬁer learning in any of stages
of the whole analysis pipeline. That approach is also called resubstitution and
used unconsciously can aﬀect the proper assessment of the performance of the
classiﬁer. Modules of the Spicy system are protected from “information leak” by
using separate learning and testing methods during classiﬁer assessment. Exam-
ples of inappropriate method of model testing and a correct testing method
implemented in the Spicy system are presented in Fig. 3. As resampling tech-
nique, two standard methods can be used in case of small-sample data sets:
bootstrap sampling or cross-validation.
Fig. 3. Examples of diﬀerent methods of model testing. (a) inappropriate method of
model testing, feature selection step is performed on whole available data set, (b)
correct method of model testing implemented in the Spicy system.
Also in the model selection tool the proper scheme of model comparison is
implemented. Because of modular construction of the classiﬁers, selection of the
best number of selected features is equal to creation models with that same
structure, but diﬀerent value of parameter in the feature selection stage, and
selection of the model which obtained the highest score. It is important to learn
and test each examined model with exactly that same learning and testing set
obtained with data resampling technique.
For each structure of classiﬁer which use feature selection module best clas-
siﬁer was evaluated and selected according to the scheme presented in Fig. 4.
Classiﬁers with structures presented in 2(b) and 2(c) were also evaluated using
data resampling technique, hence only single model was evaluated and all avail-
able clinical variables were used.
Each process such as model creation, selecting best model or model training
is implemented in the Spicy system as a separate tool. To create workﬂow, which
bonds together all the selected tools, the Galaxy server environment [11] is used.
Figure 5 presents Galaxy workﬂow canvas. Presented analysis pipeline is equal
to scheme presented in Fig. 4.

422
K. Pojda et al.
Fig. 4. Scheme of model evaluation and selection for one type of classiﬁer structure.
Fig. 5. Workﬂow created for model evaluation and selection for one type of classiﬁer
structure using the Galaxy Environment.
5
Results and Evaluation
Analyzed data were obtained from 121 thyroid lesion patients by the Maria
Sklodowska-Curie Memorial Cancer Center and Institute of Oncology, Gliwice
branch. Samples belong from II to VI the Bethesda system categories. 50 were
deﬁned as “benign” lesion and 71 were deﬁned as “malignant”. Learning and
testing data set consist of 97 and 24 samples, respectively.

Comparing Diﬀerent Data Fusion Strategies
423
Tissues collected during FNA were processed and analyzed using the
Aﬀymetrix GeneChip Human Transcriptome Array 2.0 (HTA 2.0) (Aﬀymetrix,
Santa Clara, CA, USA). Obtained .CEL ﬁles were normalized using RMA
method and a custom ENTREZID package (version 20.0.0) available on the
BrainArray website at [12].
To perform normalization, authors have used the R environment and modiﬁed
aﬀy package (package is also available at [12]). After deleting control probes,
32500 transcripts were obtained, from which only genes with an expression level
greater than log2(5) in at least 25% of the cases were considered. Threshold level
was calculated using expressions of the control probes. The ﬁnal microarray data
set consists of 30132 transcripts.
The available clinical variables are gender, age and information about tumor
nodule multifocality. As the Bethesda system data are ordinal, they were con-
verted into 5 binary features, according to II to VI categories.
For each classiﬁer used methods are the same: z-score normalization in nor-
malization stage and SVM classiﬁer with a linear kernel in classiﬁcation stage.
For the feature selection, the absolute value of the t-statistic was used. To exam-
ine inﬂuence of microarray data dimensionality on classiﬁcation results, the num-
ber of selected features was changed from 1 to 50 with step equal 3.
Table 1 summarizes results obtained by the best model selected for each
classiﬁers structure. Column “Bootstrap accuracy” contains accuracy of clas-
siﬁcation achieved by the best model during bootstrap model testing. Column
“Testing accuracy” contains accuracy of classiﬁcation achieved during testing
learned model on independent testing set. Column “Number of selected features”
informs about number of features selected from microarray data set during fea-
ture selection stage, for which obtained by classiﬁer accuracy was the highest
in comparison to the other models during models selection process. Because the
feature selection has been applied only for the microarray data there is no infor-
mation about number selected features for ﬁrst two fusion strategies: Cl and
Cl+B. Surprisingly, for microarray data used alone the best number of features
is 40, which looks relatively high for the data set consisting of only 121 samples.
Table 1. Results obtained for diﬀerent classiﬁer structures
Classiﬁer structure Bootstrap
accuracy [%]
Testing
accuracy [%]
Number of
selected features
Cl
58.29 ± 1.32
54.17
−
Cl+B
82.84 ± 1.06
79.92
−
M
76.32 ± 1.09
79.92
40
M+Cl
72.18 ± 1.48
79.92
7
M+Cl+B
81.75 ± 1.18
87.50
1

424
K. Pojda et al.
6
Conclusions and Future Work
The functionality of the currently developed Spicy system makes possible the
assessment of research thesis which requires machine learning and classiﬁca-
tion methods. The new workﬂow that has been created uses newly introduced
in Spicy system tools: the data fusion module and the models selection tool,
helps in assessment of the predictive value of data fusion in thyroid malignancy
prediction. As shown in Table 1, there is no clear answer about an additional
value of data fusion performed during classiﬁcation process. It seems to be more
important what type of data is fused than a fact, that the fusion was performed.
What is interesting, in the case of M+Cl fusion, the accuracy of classiﬁcation is
worse than in case of calculations performed only on microarray data set. It is
worth to consider to use rather integration of that two types of data. It is hard
to assessment results obtained by classiﬁer, which performs M+Cl+B. As the
results obtained during bootstrap testing are not better than results obtained
for Cl+B fusion, accuracy achieved on independent testing set is signiﬁcantly
higher, although only one feature was selected in feature selection stage from
microarray data set.
Besides of good results obtained by the classiﬁer which performs fusion of
all three types of data, we plan to introduce mechanisms of data integration to
examine connections hidden in the data. It is important because of two main rea-
sons: ﬁrst, to reduce information redundancy in data. Second, to reduce depen-
dencies in microarray data, which could lead to incorrect inferences about classi-
ﬁcation eﬃciency, especially when the classiﬁer is training on small sample data
set. When a distribution of some feature (e.g. age) is not the same in groups
which should be distinguished, in the feature selection stage transcripts, which
expression is correlated with that misleading feature, not the main class of sam-
ples, could be selected. It could provide false high accuracy of classiﬁcation. Only
selection of valuable transcripts, which do not duplicate information described
by a pathologist and are correlated with main class of samples, will achieve truly
high results.
Acknowledgment. This work was supported by NCBR (Polish National Centre for
Research and Development) under grant Strategmed2/267398/4/NCBR/2015 and by
Silesian University of Technology. Calculations were performed using the infrastructure
supported by the computer cluster Ziemowit (www.ziemowit.hpc.polsl.pl) funded by
the Silesian BIO-FARMA project No. POIG.02.01.00-00-166/08 and expanded in the
POIG.02.03.01-00-040/13 in the Computational Biology and Bioinformatics Labora-
tory of the Biotechnology Centre at the Silesian University of Technology.

Comparing Diﬀerent Data Fusion Strategies
425
References
1. Zhang, M., Lin, O.: Molecular testing of thyroid nodules: a review of current avail-
able tests for ﬁne-needle aspiration specimens. Arch. Pathol. Lab. Med. 140(12),
1338–1344 (2016)
2. Jarzab,
B.,
Wiench,
M.,
Fujarewicz,
K.,
Simek,
K.,
Jarzab,
M.,
Oczko-
Wojciechowska, M., Wloch, L., Czarniecka, A., Chmielik, E., Lange, D., Pawlaczek,
A., Szpak, S., Gubala, E., Swierniak, A.: Gene expression proﬁle of papillary thy-
roid cancer: sources of variability and diagnostic implications. Cancer Res. 65(4),
1587–1597 (2005)
3. Eszlinger, M., Wiench, M., Jarzab, B., Krohn, K., Beck, M., Lauter, J., Gubala, E.,
Fujarewicz, K., Swierniak, A., Paschke, R.: Meta-and reanalysis of gene expression
proﬁles of hot and cold thyroid nodules and papillary thyroid carcinoma for gene
groups. J. Clin. Endocrinol. Metab. 91(5), 1934–1942 (2006)
4. Fujarewicz, K., Jarzab, M., Eszlinger, M., Krohn, K., Paschke, R., Oczko-
Wojciechowska, M., Wiench, M., Kukulska, A., Jarzab, B., Swierniak, A.: A multi-
gene approach to diﬀerentiate papillary thyroid carcinoma from benign lesions:
gene selection using support vector machines with bootstrapping. Endocr. Relat.
Cancer 14(3), 809–826 (2007)
5. Cibas, E.S., Ali, S.Z.: The Bethesda system for reporting thyroid cytopathology.
Am. J. Clin. Pathol. 132(5), 658–665 (2009)
6. Boulesteix, A.L., Porzelius, C., Daumer, M.: Microarray-based classiﬁcation and
clinical predictors: on combined classiﬁers and additional predictive value. Bioin-
formatics 24(15), 1698–1706 (2008)
7. Thomas, M., De Brabanter, K., Suykens, J.A., De Moor, B.: Predicting breast can-
cer using an expression values weighted clinical classiﬁer. BMC Bioinform. 15(1),
411 (2014)
8. Fujarewicz, K., Student, S., Ziela´nski, T., Jakubczak, M., Pieter, J., Pojda, K.,
´Swierniak, A.: Large-scale data classiﬁcation system based on galaxy server and
protected from information leak. In: Nguyen, N., Tojo, S., Nguyen, L., Trawi´nski,
B. (eds.) Intelligent Information and Database Systems (ACIIDS 2017). Lecture
Notes in Computer Science, vol. 10192, pp. 765–773. Springer, Cham (2017)
9. Synnergren, J., Olsson, B., Gamalielsson, J.: Classiﬁcation of information fusion
methods in systems biology. Silico Biol. 9(3), 65–76 (2009)
10. Chudova, D., Wilde, J.I., Wang, E.T., Wang, H., Rabbee, N., Egidio, C.M.,
Reynolds, J., Tom, E., Pagan, M., Rigl, C.T., Friedman, L., Wang, C.C., Lanman,
R.B., Zeiger, M., Kebebew, E., Rosai, J., Fellegara, G., LiVolsi, V.A., Kennedy,
G.C.: Molecular classiﬁcation of thyroid nodules using high-dimensionality genomic
data. J. Clin. Endocrinol. Metab. 95(12), 5296–5304 (2010)
11. Afgan, E., Baker, D., van den Beek, M., Blankenberg, D., Bouvier, D., Cech, M.,
Chilton, J., Clements, D., Coraor, N., Eberhard, C., Gruning, B., Guerler, A.,
Hillman-Jackson, J., Von Kuster, G., Rasche, E., Soranzo, N., Turaga, N., Taylor,
J., Nekrutenko, A., Goecks, J.: The Galaxy platform for accessible, reproducible
and collaborative biomedical analyses: 2016 update. Nucleic Acids Res. 44, w3–w10
(2016)
12. Microarray
Lab
(2017).
http://brainarray.mbni.med.umich.edu/Brainarray/
Database/CustomCDF/CDF download.asp. Accessed 3 July

426
K. Pojda et al.
13. Bensz, W., Borys, D., Fujarewicz, K., Herok, K., Jaksik, R., Krasucki, M., Kurczyk,
A., Matusik, K., Mrozek, D., Ochab, M., Pacholczyk, M., Pieter, J., Puszynski, K.,
Psiuk-Maksymowicz, K., Student, S., Swierniak, A., Smieja, J.: Integrated system
supporting research on environment related cancers. In: Kr´ol, D., Madeyski, L.,
Nguyen, N.T. (eds.) Recent Developments in Intelligent Information and Database
Systems. Studies in Computational Intelligence, vol. 642, pp. 399–409. Springer,
Cham (2016)
14. Psiuk-Maksymowicz, K., Mrozek, D., Jaksik, R., Borys, D., Fujarewicz, K.,
Swierniak, A.: Scalability of a genomic data analysis in the biotest platform. In:
Nguyen, N.T., Tojo, S., Nguyen, L.M., Trawi´nski, B. (eds.) Intelligent Information
and Database Systems (ACIIDS 2017). Lecture Notes in Computer Science, vol.
10192, pp. 741–752. Springer, Cham (2017)
15. Psiuk-Maksymowicz, K., Placzek, A., Jaksik, R., Student, S., Borys, D., Mrozek,
D., Fujarewicz, K., ´Swierniak, A.: A holistic approach to testing biomedi-
cal hypotheses and analysis of biomedical data. In: Kozielski, S., Mrozek, D.,
Kasprowski, P., Malysiak-Mrozek, B., Kostrzewa, D. (eds.) Beyond Databases,
Architectures and Structures, BDAS 2016. Advanced Technologies for Data Mining
and Knowledge Discovery. Communications in Computer and Information Science,
vol. 613, pp. 449–462. Springer, Cham (2016).

A Recommender System Based on Cognitive
Map for Smart Classrooms
Jose Aguilar1,2(&), Priscila Valdiviezo-Diaz3(&),
and Guido Riofrio3(&)
1 CEMISID, Departamento de Computación,
Universidad de Los Andes, Mérida, Venezuela
aguilarjos@gmail.com
2 UTPL, Loja, Ecuador
3 Dpto. de Ciencias de la Computación y Electrónica,
Universidad Técnica Particular de Loja, Loja, Ecuador
{pmvaldiviezo,geriofrio}@utpl.edu.ec
Abstract. In this paper, we propose a Fuzzy Cognitive Maps (FCMs) to rec-
ommender Learning Resources in a Smart Classroom. We have proposed a
Smart Environment for a Classroom in previous works, based on Multi-agent
Systems, called SaCI. One of its agents is a recommender system of Learning
Resources. In this paper, we deﬁne this recommender system using Fuzzy
Cognitive Maps. Our recommender system exploits the knowledge, learns,
discovers new information, infers preferences, among other thing. For that, it
uses ﬁve types of knowledge from SaCI: students, learning resources, topics,
context and criticism. The performance results of our recommender system
based on FCMs are very encouraging.
Keywords: Cognitive maps  Smart Classroom  Recommender system
1
Introduction
Smart Classroom for the education is one of the challengers in the area of Ambient
Intelligent (AmI). In [1] have been deﬁned a Smart Classroom based on the multi-agent
paradigm, called SaCI (Salón de Clase Inteligente, for its acronym in Spanish). The
Smart Classroom is supported by a middleware to smart environments based on
multi-agent systems, developed in [2, 3].
Particularly, one of the software components of SaCI is the Recommender System
(RS) of learning resources. The RSs classically have been classiﬁed as content-based,
collaborative, knowledge-based, and hybrid [4]. Recently has been proposed a new
type of recommender systems, called Intelligent Recommender Systems (IRS), which
is an extension of the knowledge-based RS [5]. An IRS has an intelligent behavior
using the next set of capabilities: knowledge representation, learning capabilities, and
reasoning mechanisms.
In this paper, we propose an IRS of learning resources for SaCI, based on FCMs.
The FCMs have been used in different domains [6–8], to model systems based on the
representation of concepts that describe the main aspects of the modeled system (states,
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_41

variables or characteristics of the system), and the causal relationships between them.
FCMs use fuzzy logic theory to describe their structure and to infer answers of the map
from a given data input.
In the literature, there are some RSs of learning resources, here we present some of
them, nearby to our proposal. [9] Describes a RS to discover learning resources that go
beyond educational content, and incorporates elements such as: software applications
that support, people and events outside school linked, etc. Therefore, the RS takes into
account contextual factors when calculating the relevance of each learning resource.
[10] Presents a RS for learning resources in speciﬁc learning contexts. They consider
the particular characteristics of a learning context (topic, language, tools available, etc.)
like main criteria, and not the preferences or previous behaviors of a student. [11]
Presents a RS in an e-learning context that recommends actions to students based on
the actions of previous students. Particularly, they use web mining techniques to build a
RS that recommends online learning activities, or shortcuts to course web sites.
Additionally, there are some papers about the utilization of the FCM in the learning
domain. Two recent papers are: [12] presents a knowledge representation approach based
on FCM, of an adaptive and/or personalized tutoring system. Here, FCM is used for
knowledge representation in order to represent graphically the knowledge dependencies
that exist between the domain concepts of the learning material. Also, [13] uses the FCMs
to model the control engineering educational critical success factors. The concepts of the
FCM model are deﬁned by the aggregation of the opinions of the students.
Our approach is the ﬁrst based on the concepts about IRS deﬁned in [5], and it has
been deﬁned in detail in [14]. In this paper, we deﬁne the RS agent of SaCI based on
this RS, and we test its behavior in the context of the “online tutoring process” con-
versation of in SaCI, because it is its main conversation [15].
This paper is organized like follow: the next section presents SaCI and the RSs.
Section 3 details the implementation of the RS of learning resources for SaCI, using
FCMs. The next section presents the experiments and analyses the results.
2
SaCI
SaCI is a smart classroom proposed in [1], where its deployment environment (mid-
dleware) was proposed in [2, 3] (AmICL). SaCI proposes a smart student-centered
classroom, which supports the learning process, through collaborative devices and
applications that facilitate self-training. To do this, the smart classrooms have different
types of components: hardware (such as smart boards, projectors, cameras, etc.) and
software (Intelligent Tutoring Systems (ITS), Virtual Learning Environments (VLE),
among others). In [1] has been proposed the SaCI model that characterizes a smart
classroom, using the paradigm of Multiagent Systems. The reﬂexive middleware for
SaCI, called AmCL, is shown in Fig. 1.
AmICL has six levels. A physical layer with components that interconnect the
elements of a smart ambient (software or hardware), such as APIs, libraries, etc. That
level works directly with the operating system. The SMA management level consists of
the classic multi-agent community, deﬁned by FIPA to support the implementation of
multi-agent applications (allows the deployment of MAS) [16]. The service
428
J. Aguilar et al.

management layer has the responsibility to seek the required services in the cloud,
particularly educational. AmI physical layer represents the various devices in the
environment, represented as agents. AmI logical layer represents the different software
applications used in the educational platform, also represented as agents. The last two
layers are those that deﬁne SaCI. Finally, in the real classroom is where the devices
(sensors, smart cameras, etc.) and software (VLE, etc.) of SaCI are deployed. Partic-
ularly, the layer “AmI Logical Management Layer” (ILL) speciﬁcs all applications
(software) and individuals present at SaCI as agents, which contain metadata that
deﬁnes them.
One of the key software, in order to give the adaptive capability to SaCI, is the RS
of learning resources. In our model, it is deﬁned like an agent, belonging to ILL. In our
case, the RS based on FCMs will be this agent, to recommend learning resources for
SaCI.
3
Speciﬁcation of Our RS Based on FCM for SaCI
SaCI provide a collection of data about the learning process, which can be used by
different types of tasks and applications: recommender systems, learning analytics
tasks, etc. These data come from different sources, e.g. from the students, the institu-
tional LMS and VLE, etc. Other important remark is that SaCI can include other
information outside of it (for example, from the Internet); and can use different types of
knowledge representations: ontologies, cognitive maps, etc. Additionally, it is trans-
parent to the techniques of data processing.
In our case, we use FCM for the representation of the knowledge used by the RS
agent, in order to provide suggestions of items to students. In this case, our RS agent
combines a FCM with a recommender system to determine the learning resources to
recommend to students.
Fig. 1. Middleware AmlCI.
A Recommender System Based on Cognitive Map for Smart Classrooms
429

3.1
Recommender Agent in SaCI
In this section, we show how to specify the RS like agent in SaCI, using MASINA, a
methodology to speciﬁc multi-agent systems [16, 17].
Recommender Agent: it searches the learning resources on the local repository,
according to the student proﬁle. If it doesn’t exist, it searches on the Internet. Table 1
shows the description of this agent.
Recommendations are based on the knowledge about the student proﬁle, and about
the performance of the students in previous activities or resources; the recommender
system exploits that to recommend new items. For that, the VLE agent of SaCI
determines the subjects to learn and the students’ proﬁle, and immediately asks the RS
agent search the learning resources, which are shown into the environment according to
the planning deﬁned by the VLE. Then, the students interact with these learning
resources and the VLE monitors the work of the students. This is the “online tutoring
process” conversation of SaCI (see Table 4). Table 2 shows the basic information of
the RS agent using the agent model of MASINA [17].
Now, we specify only its main task to search educational resources (Table 3).
Finally, the “online tutoring process” conversation of SaCI is described in the
Table 4 (see [15] for more details).
In the conversation are involved different types of agents of SaCI: one device type,
and the rest of software type: VLE, RS, students, etc. Figure 2 shows the interaction
diagram of this conversation.
Table 1. Recommender agent description.
Name: Recommender agent
Position: Community of Agents that manage the Learning Resources
Description: This agent searches learning resources, according to the student proﬁles
Table 2. Basic information about the RS agent
Agent Name: RS
Components: a RS system, in our case, based on FCM
Goal: search the adequate learning resources for the students
Services: Search learning resources, Store information about the learning resources, Store
information about the repositories of learning resources
Table 3. Main task of the RS agent
Task Name: search educational resources
Objective: it searches the adequate learning resources for a given proﬁle of student
Description: Each student has a proﬁle, which is used to ﬁnd the learning resources that must
use
430
J. Aguilar et al.

SaCI adapts the online tutoring process to the requirements of a speciﬁc session
using the VLE and RS agents. This is a cyclical process that is done in each tutoring
session. At the end, VLE establishes a student’s score (evaluation), and updates the
learning proﬁle of the students in function of these results (learn) for the next session.
Table 4. “Online tutoring process” conversation of SaCI
Conversation Name: online tutoring process
Goal: help the students during their processes of learning
Agents: VLE, a smart board, RS of learning resources, an academic system, a tutor, and several
students
Beginner: tutor and students
Precondition: A new subject to be covered in the curriculum by the students
End condition: When the student ﬁnishes the interaction with the resources of learning planned
by the VLE and tutor for this session
Description: This conversation describes the different activities on the AmI to support the
online support to the students. These activities are carried out by different agents according to
their roles. For example, VLE plans the learning resources to be used during the session, and
monitors its utilization during it. Also, it demands to search the learning resources according to
the students’ proﬁle and the subject of the session
Fig. 2. Conversation “online tutoring process in SaCI” [15]
A Recommender System Based on Cognitive Map for Smart Classrooms
431

3.2
Bases of the RS Agent
Our agent is an IRS based on the FCM paradigm. According to [5], an IRS is deﬁned as
a reasoning-based approach, which uses the knowledge about users, products, domain,
etc., to generate a recommendation. The main components of an IRS are [5]:
• An extended user proﬁle (including its opinions, critics, etc.): This is an extended
model about the user, with his/her personal information (age, gender, profession,
and education), information about his/her opinions, critics, and his/her relationships
with other users (friend groups, etc.). The next table deﬁnes the main aspects of the
user proﬁle of an IRS.
• An extended item proﬁle: This model includes the description, structure, functions,
and operational information of each item. An item is described by its general or
objective characteristics, and its speciﬁc or subjective characteristics. Some of them
can be statics or dynamics. The next table deﬁnes the different attributes that
describe an item in IRS (the ﬁrst letter next to the item represents if the attribute is
static (I) or dynamic (D), and the second if the attribute is Descriptive (D), Struc-
tural (S), Functional (F) or Operational (O)):
• The context and domain knowledge: this model has the information about the
domain where the items will be used, the context where the individual is going to
make the decision, etc.
Table 5. User proﬁle in IRS
General attributes
Descriptions
Personal data
Name, address, ID, Sex Age, relationship status, personality
Physical Features
Size, weight, physical defects
User type
Student, research, etc.
Languages
Languages of the users
Some tastes,
preferences
We can ask explicitly to the user, but some of this information can be
inferred
Education
Level of the education: PhD, etc.
Occupation
Work
Socio-cultural
aspects
Behavior patterns, needs, cultural behavior
Economic aspects
Income, buying habits, etc.
Political aspects
Ideological trend, etc.
Most inﬂuential
sectors
Networks of friends, etc.
Technological skills
Intellectual
capabilities
Projects developed
Positions held
432
J. Aguilar et al.

• A critical knowledge: This is a knowledge that must be discovered. Normally, it
must be discovered using machine-learning techniques in order to obtain useful
patterns in the behavior. This class of knowledge allows representing what users
ﬁnd interesting and uninteresting (see Table 7).
3.3
Speciﬁcation of the RS Agent Based on FCM
The ﬁrst point is specifying the contextual and domain knowledge according to SaCI.
With this knowledge, we deﬁne the FCM. We keep the previous concepts deﬁned in
IRS, which follows the two standards used by SaCI, one to deﬁne the learning resource
(LOOM-IEEE [18]), and the other to deﬁne the users (students and professors) in a
learning computational platform (the IMS Global Learning [19]).
We consider these concepts, based on the standards, because they provide enough
information about the student and items, in order to have meaningful information for the
Table 7. Critical model in IRS
Attribute
Critical type
Behavior patterns Unit and compound critiques
Trends
Directional or replacement
Opinions
Directional or replacement
Preferences
Unit and compound critiques
Punctuation
Unit and compound critiques
Table 6. Item proﬁle in IRS
General attributes
Subjective attributes
Item ID (I, D)
Level of use (D, O)
Name (I. D)
Utility (D, O)
Type of item (I, D)
Punctuation/Qualiﬁcation (D, O)
Description (I, D)
Type of Problems where can be used (D, O)
Localization (I, D)
Reusability (D, O)
Author of the item (I, D)
Extensibility (D, O)
Date of elaboration (I, D)
Interoperability (D, O)
Provider (I, D)
Dimension (I, D)
Version (I, D)
Format (I, D)
Components (I, S)
Relationship between components (I, S)
Constrains (I, S)
Technical Requirements (I, S)
Goal (I, F)
Requirements cover (I, F)
A Recommender System Based on Cognitive Map for Smart Classrooms
433

recommendation. These concepts refer to the student’s characteristics of a higher edu-
cation institution, and learning resources to which they have access through the VLE.
In our FCM, each concept is represented on the map. Particularly, we propose a
FCM based on two levels (see Fig. 3, it represents the FCM developed with the FCM
Design tool [20]), where a level describes the main aspects of the phenomena studied
(in our case, the second level), and the other level FCM determines the decisions about
the process (in our case, the ﬁrst level). This multilevel approach follows the classical
structure of the multilevel FCM approaches presented in the literature [21–23].
In our case, the ﬁrst level describes the concepts that are inferred (preferences,
recommendations of learning resources, among other concepts), and represents the
knowledge discovered (gray concepts in Fig. 3). The second level represents the
concepts that describe the current situation (white concepts in Fig. 2): the information
about the items and users, the context, etc. Both levels are deﬁned according to the
models.
These concepts, and the relationship among them, are deﬁned using a FCM
learning approach based on two steps: the query to several experts, and then the
weights of the relationships are adjusted following the classical supervised learning
approach described in [6, 8]. Now, we describe each concept in each level of the FCM:
(a) First Level: At this level, there are concepts inferred from the concepts of the
second level or from concepts on the same level. Mainly, they are concepts linked to
the recommendations (our system can recommend different things, for example,
interesting items, similar items, etc.), but additionally other types of concepts about
information inferred over the products or items (for example, users opinion, or items
punctuation). In general, the attributes of the critical model belong to this level
Fig. 3. Our multilevel FCM
434
J. Aguilar et al.

(normally are inferred), and they reduce the space of candidate items. Also, the attri-
butes called “Subjective Attributes” belong to this level. The concepts on this level are
deﬁned in Table 8.
(b) Second level: At this level belong the concepts that represent the different
attributes of the user and item proﬁles, extended with the knowledge about the context
and domain (see Tables 5 and 6). They are grouped according to the knowledge that
represent (users, products, etc.). Particularly, the domain knowledge depends on the
domain of application. At this level, we use two standards to deﬁne the domain
knowledge, due to our domain of application is SaCI:
• The LOM standard to describe the learning objects, because the domain of appli-
cation is to recommend learning objects.
• The IMS Global Learning to deﬁne the users (students and professors), because
SaCI is a learning computational platform.
Similarly that a classical RS, the FCM is designed to give personalized suggestions.
However, the FCM is able to represent knowledge, to learn users’ preferences, in order
to infer the recommendations. Particularly, after the deﬁnition of the cognitive model
(concepts and relationships among them, see Fig. 3), the FCM can be instanced in
order to carry out inferences of speciﬁc recommendations. The FCM is executed until it
stabilizes, and the ﬁnal values of the ﬁrst level represent the information inferred.
According to these values is determined if the item can be recommended to this speciﬁc
user. Next section gives some examples of this inference process of the FCM.
4
Experiments
In this section, we describe the initialization of the FCM used by our RS agent, and the
behavior of the RS agent in the “online tutoring process” Conversation of SaCI.
Table 8. Inferred concepts
Attribute
Description
Item of interest
It refers to the usefulness of the item for the user
Punctuation
It refers to score or rating that a user provides to the item
Item preferred by
the user
It is the priority that a user has for the item
Preferences
It refers to user-deﬁned preferences based on its proﬁle information
Use Level
Indicator of the user interaction with the item
Usability
It refers to the ease with which user can use the item
Interoperability
It concerns whether the item can be integrated into different systems
or platforms
Similar user
It concerns whether there are other users with the same characteristics
as the active user
A Recommender System Based on Cognitive Map for Smart Classrooms
435

4.1
Initialization of the FCM Used by the RS Agent
To start using the FCM, additional to deﬁne the concepts, it is required the initialization
of the weights between them. For that, a learning process is required, which is based on
two steps:
(1) Determination of the initial values of the weights. In our case, it is the assignment
of the initial values (weights) of the relationships between concepts, carried out by
human experts. In the case of SaCI, it has been carried out by pedagogical experts.
The Fig. 4 shows these initial weight values. To obtain theses values, we have
followed the classical learning process based on experts deﬁned in [7].
(2) Adjustments of weights based on initial experiments. For this, the RS agent uses a
sample of real examples given by the VLE agent, where they know in advance for
each case, the student and the learning resource to recommend. The RS agent runs
the recommender system based on FCM, and its results are compared with the real
case, in order to determine whether the weights given by the FCM provides the
desired output, or if is necessary adjust its weights (see the procedure in [6, 8]).
Fig. 4. Initial weight matrix of the FCM of the RS agent
436
J. Aguilar et al.

4.2
Case Studies
With the previous FCM, experiments were developed in the context of the “online
tutoring process” Conversation of SaCI. In this conversation, the FCM of the RS agent
must infer if the student is interested in a given educational resource or not. In our case
study, the VLE agent asks for two things, in the ﬁrst case, it needs only to know the
preferences of a student (item preferred, its user preferences, similar users, item of
interest, among others), and in the second case, the characteristics of the learning
resources ideal for him/her (usability and interoperability). For each case, the RS agent
needs to use only some of the concepts of the FCM, to infer the previous information
(see Tables 9 and 10). For example, for the ﬁrst case, it is not necessary the concepts
linked to detailed information of a learning resource and a student. In the second case,
to infer the capabilities of the learning resources, only the student concepts about its
physical characteristics (sex, disability, etc.) and socio-cultural aspects, are interesting.
Case 1: This case study seeks to infer, based on the characteristics of a learning
resource, if a student is interested or not in a learning resource, if it has a preference for
it, if the learning resource is useful for the student, etc. The information about the
students and educational resources are given by the VLE agent of SaCI [24].
We use the FCM deﬁned by the ﬁrst experiment of the previous case, in order to
deﬁne the preferences of the student for this educational resource. For this experiment,
an example of the values of the student attributes is given in Table 11, and of the
educational resources in Table 12.
These values of the concepts are deﬁned by the RS agent, using the information
sent by the VLE agent. We can see that the goal of the item is according to the proﬁle
Table 9. Concepts about students without impacts in each experiment
Attribute
Case 1 Case 2
Personal data (name, id, sex, etc.)
Physical Features (physical disability)
User type
X
Languages
Some tastes, preferences (likes)
X
Education (academic degree)
X
Occupation
X
Socio-cultural aspects (behavior, behavior patterns)
X
Economic aspects
X
Political aspects (context)
X
Most inﬂuential sectors (networks of friends)
X
Technological skills
X
Intellectual capabilities (score intellectual)
X
Projects developed
X
Positions held (employment performed)
X
A Recommender System Based on Cognitive Map for Smart Classrooms
437

Table 10. Concepts about learning resource without impacts in each experiment
General attributes
Case 1 Case 2
Item ID (I, D)
X
Name (I. D)
X
Type of item (I, D)
Description (I, D)
X
Localization (I, D)
X
X
Author of the item (I, D)
X
Date of elaboration (I, D)
X
Provider (I, D)
X
Dimension (size) (I, D)
X
Version (I, D)
Format (I, D)
X
Components (I, S)
X
Relationship between components (I, S)
X
Constrains (I, S)
X
Technical Requirements (platform, types of problems) (I, S) X
Goal (I, F)
X
Requirements cover (topic, career, area) (I, F)
X
Table 11. Input value of the Student concepts.
Attribute
Student
Concept
value
Personal data (name, id, sex, etc.)
Juana Carrero, CI 9434553,
female, etc.
1
Physical Features (physical disability)
With a physical disability
1
User type
Genius
0.5
Languages
English
0.2
Some tastes, preferences (likes)
See video, Sport
1
Education (academic degree)
Student of Computer sciences
Engineering
1
Occupation
Student
0.8
Socio-cultural aspects (behavior,
behavior patterns)
Sport
0.8
Economic aspects
Middle class
0.5
Political aspects (context)
Free market
0.1
Most inﬂuential sectors (networks of
friends)
Its friends of sport
0.9
Technological skills
Mathematical modeled
04
Intellectual capabilities (score
intellectual)
NA
0.3
Projects developed
Data analysis
0.5
Positions held (employment performed)
Teacher Assistant
0.8
438
J. Aguilar et al.

of the student (for this reason, the concept “requirements cover” is 0.9), but the learning
resource can be used in some platforms (windows 8 and Linux), for this reason the
value of the constraints concept is 0.5. In this way are deﬁned the initial values of the
different concepts of the FCM, and the FCM starts to iterate until its convergence, using
the FCM Designer Tool [20]. Some examples of the inferences carried out by the RS
agent for different queries of the VLE agent are shown in the Table 13.
The FCM iterates until the values of the inferred concepts are stabilized. The ﬁrst
column corresponds to the results of the pair learning resource and student of Tables 11
and 12. In this case, the FCM iterates 6 times. For that example, the RS agent can
determine with the values obtained for the inferred concepts that the learning resource
can be recommended (has a good punctuation and preference), it is more or less useful
for the student, there are not similar students that have used this learning resource, and
the student will have a good level of interaction with the learning resource.
Case 2: Now, the RS agent infers the characteristics of usability and interoperability of
the learning resource. In this case, an example of the input of the pair learning resource
and student is given in Tables 14 and 15.
The RS agent can predict the characteristics of usability and interoperability of an
educational resource using only the information deﬁned in Tables 14 and 15. In this
way, it can determine if an educational resource can be interesting for a given course
Table 12. Input value of the learning resource concepts
General attributes
Educational resource
Value
Name
Power of the Data Analysis
1
Type of item
Slides
0.8
Description
This resource describes a course about the
importance of the Data Analysis: http://www.ing.
edu.ve/saberula
0.8
Constrains
Windows 8.0, Linux
0.5
Technical Requirements
(platform, types of problems)
10K Ram
0.6
Goal
Introduction to Data Analysis
0.9
Requirements cover (topic,
career, area)
Programming, Artiﬁcial Intelligence
0.9
Table 13. Results of some recommendations
Attribute inferred Recommend 1 Recommend 2 Recommend 3 Recommend 4
Item of interest
0.541
0.068
0.070
0.010
Punctuation
0.914
0.189
0.101
0.144
Preferences
0.819
0.048
0.080
0.185
Use Level
0.812
0.281
0.291
0.412
Similar user
0.353
0.520
0.201
0.111
A Recommender System Based on Cognitive Map for Smart Classrooms
439

(that can be very important in courses where there are students with physical disability).
For example, for the case of the Tables 14 and 15, the second column in Table 16
describes the inference. In this case, the RS agent infers a low usability and interop-
erability of the educational resource, for the student with the proﬁle deﬁned in
Table 14. Particularly, the RS agent infers that the educational resource is not easy to
use for the students with the proﬁle of Table 14 (usability), and cannot be integrated
into other platforms (interoperability).
5
Conclusions
In this paper, we have proposed a RS agent based on FCM for SaCI, which allows
analyzing the educational resources according to their information and the information
about the students. Particularly, the RS agent can infer different types of information,
Table 14. Student concepts in the FCM to infer the characteristics of usability and
interoperability of a learning resource
Attribute
Student
Concept
value
Personal data (name, id, sex, etc.)
Juana Carrero, CI 9434553, female,
etc.
1
Physical Features (physical
disability)
Without a physical disability
1
Table 15. Learning resource concepts in the FCM to infer the characteristics of usability and
interoperability of a learning resource.
General attributes
Educational resource Value
Type of item
Slides
0.8
Date of elaboration
Sept 2105
1
Provider
Jhon Amaya
1
Dimension (size)
10 Gb
1
Version
First version
1
Format
Pdf
1
Components
One part
1
Relationship between components
Cyclic
1
Constrains
Windows and Linux 0.5
Technical Requirements (platform, types of problems) 10K Ram
0.6
Table 16. Results of some recommendations.
Attribute inferred Inference 1 Inference 2 Inference 3
Usability
0.581
0.014
0.937
Interoperability
0.334
0.132
0.830
440
J. Aguilar et al.

what is not traditionally deﬁned by classic RS. For example, it can determine the
preferences of a student for an educational resource, predict characteristics of the
educational resources, among other things.
Our RS agent based on FCM can use the knowledge for different things (to infer or
to recommend), in different ways (to predict to explain, etc.). In this way, the RS agent
based on FCM has the capabilities of reasoning, learning and representation of diverse
knowledge. The learning capability is exploited to deﬁne the FCM. The reasoning
capability is used to infer recommendation, preferences, etc. Finally, the capability of
representation of diverse knowledge is the FCM.
In a previous work [14], the quality of the FCM has been determined in function of
some metrics, deﬁned in [4, 5], which are: Validity (explanations to allow a user to
check the validity of a recommendation); Efﬁciency (it determines if the RS reduces the
cognitive decision-making effort of the users); and Persuasiveness (it determines if the
RS can change a person’s attitude or behavior). Other works do not reach the validity,
efﬁciency or persuasive criteria [9–11]. This is a handicap, in order to be used by the
RS agent, because it need to argue its decision inside of SaCI.
Acknowledgment. Dr. Aguilar has been partially supported by the Prometeo Project of the
Ministry of Higher Education, Science, Technology and Innovation of the Republic of Ecuador.
This work has been partially supported by the UTPL Project entitled: “Medios de Gestión de
Servicios (Middleware) Inteligentes para Entornos de Aprendizaje Virtual”.
References
1. Valdiviezo, P., Cordero, J., Aguilar, J., Sánchez, M.: Conceptual design of a smart classroom
based on multiagent systems. In: International Conference on Artiﬁcial Intelligence (ICAI
2015), pp. 471–477 (2015)
2. Sánchez, M., Aguilar, J., Cordero, J., Valdiviezo, P.: A smart learning environment based on
cloud learning. Int. J. Adv. Inf. Sci. Technol. 39, 39–52 (2015)
3. Sánchez, M., Aguilar, J., Cordero, J., Valdiviezo, P.: Basic features of a reﬂective
middleware for intelligent learning environment in the cloud (IECL). In: Asia-Paciﬁc
Conference on Computer Aided System Engineering (APCASE), pp. 1–6 (2015)
4. Ricci, F., Rokach, L., Shapira, B., Kantor, P. (eds.): Recommender Systems Handbook: A
Complete Guide for Research Scientists and Practitioners. Springer, New York (2011)
5. Aguilar, J., Valdiviezo, P., Riofrio, G.: A general framework for intelligent recommender
systems. Appl. Comput. Inform. 13, 147–160 (2017). Elsevier
6. Aguilar, J.: Different dynamic causal relationship approaches for cognitive maps. Appl. Soft
Comput. 13, 271–282 (2013). Elsevier
7. Aguilar, J.: A fuzzy cognitive map based on the random neural model. Lecture Notes in
Artiﬁcial Intelligence, vol. 2070, pp. 333–338. Springer, Heidelberg (2001)
8. Aguilar, J.: Dynamic random fuzzy cognitive maps. Rev. Comput. Sist. Rev. Iberoam.
Comput. 7, 260–271 (2004)
9. Rodriguez, A.C., Gago, J.M.S., Rifón, L.E.A., Rodríguez, R.P.: A recommender system for
non-traditional educational resources: a semantic approach. J. Univ. Comput. Sci. 21,
306–325 (2015)
A Recommender System Based on Cognitive Map for Smart Classrooms
441

10. Rifon, L., Canas, A., Roris, V., Gago, J., Iglesias, M.: A recommender system for
educational resources in speciﬁc learning contexts. In: 8th International Conference on
Computer Science & Education (ICCSE), pp. 371–376 (2013)
11. Sikka, R., Dhankhar, A., Rana, C.: A survey paper on e-learning recommender system. Int.
J. Comput. Appl. 47, 27–30 (2012)
12. Chrysaﬁadi, K., Virvou, M.: A knowledge representation approach using fuzzy cognitive
maps for better navigation support in an adaptive learning system. SpringerPlus 2, 1–13
(2013)
13. Yesil, E., Ozturk, C., Dodurka, M.F., Sahin, A.: Control engineering education critical
success factors modeling via fuzzy cognitive maps. In: International Conference on
Information Technology Based Higher Education and Training, pp. 1–8 (2013)
14. Aguilar, J., Valdiviezo, P., Riofrio, G.: A fuzzy cognitive map like recommender system of
learning resources. In: Proceeding of the IEEE World Congress on Computational
Intelligence (WCCI), pp. 1539–1546 (2016)
15. Aguilar, J., Cordero, J., Chamba-Eras, L.: Speciﬁcation of a smart classroom based on agent
communities. In: Rocha, Á., Correia, A., Adeli, H., Reis, L., Mendonça Teixeira M. (eds.)
New Advances in Information Systems and Technologies. Advances in Intelligent Systems
and Computing, vol. 444, pp. 1003–1012. Springer, Cham (2016)
16. Aguilar, J., Rios, A., Hidrobo, F., Cerrada, M.: Sistemas MultiAgentes y sus aplicaciones en
Automatización Industrial, Talleres Gráﬁcos, Universidad de Los Andes (2013)
17. Aguilar, J., Besembel, I., Cerrada, M., Hidrobo, F., Narciso, F.: Una metodología para el
modelado de sistemas de ingeniería orientado a agentes. Rev. Iberoam. Intel. Artif. 12(38),
39–60 (2008)
18. LOM-IEEE standard. http://ieeeltsc.org/wg12LOM/
19. IMS Global Learning. http://www.imsglobal.org/cc/index.htm
20. Contreras, J., Aguilar, J.: The FCM designer tool. In: Glykas, M. (ed.) Fuzzy Cognitive
Maps: Advances in Theory, Methodologies, Tools and Application, pp. 71–88. Springer,
Heidelberg (2010)
21. Papageorgiou, E., Stylios, C., Groumpos, P.: An integrated two-level hierarchical system for
decision making in radiation therapy based on fuzzy cognitive maps. IEEE Trans. Biomed.
Eng. 50, 1326–1339 (2003)
22. Peng, Z., Wu, L.: Two-level fuzzy cognitive map mining for text categorization. Int. J. Digit.
Content Technol. Appl. 6, 296–302 (2012)
23. Perozo, N., Aguilar, J., Terán, O., Molina, H.: A veriﬁcation method for MASOES. IEEE
Trans. Syst. Man Cybern. Part B 43(1), 64–76 (2013). Coautores
24. Aguilar, J., Fuentes, J., Moreno, K., Dos Santos, R., Altamiranda, J., Hernández, D.:
Computational platform for the educational model based on the cloud paradigm. In: 45th
Annual Frontiers in Education (FIE) Conference, pp. 2399–2407 (2015)
442
J. Aguilar et al.

Strategy to Develop a Digital Public Health
Observatory Integrating Business Intelligence
and Visual Analytics
Leidy Alexandra Lozano(&) and Maria del Pilar Villamil
Universidad de los Andes, Bogotá, Colombia
{la.lozano51,mvillam}@uniandes.edu.co
Abstract. One of the challenges in the public health sector is to improve
individually and collectively life quality of the population. The availability of
digital tools for the manipulation and visualization of information helps to
address this challenge, due to the agility to understand the events, the facility to
analyze situations, and the support to the decision-making process based on
facts and not in assumptions. Currently, there are projects oriented to support
decision-making in the health sector. However, it is not clear how they are
developed, nor how information should be shown to facilitate the interaction of
different types of actors. These issues motivate the present work that is a strategy
for the creation of a digital public health observatory to integrate concepts, tools,
methods and methodologies of Business Intelligence and Visual Analytics. The
validation of this proposal in the National Institute of Health (Colombia) shown
the opportunity to improve support decision-making process.
Keywords: Business intelligence  Visual analytic  Health observatory
1
Introduction
The management, monitoring and improvement of the health status of the population is
a function of public health policies to provide the best measures of life quality,
well-being and population development. In particular, public health surveillance has an
essential role associated with the responsibility of the State and citizens for health
protection, consisting on the systematic and constant process to collect, analyze,
interpret and disseminate speciﬁc data related to health. This process is used to planned,
implement and evaluate public health practice [1]. In this way, the availability of
reliable and timely information is a fundamental contribution to decision-making
process as also, to achieve the proposed objectives in this area.
A digital public health observatory is focus on integrating different sources of
information such as surveys, technical reports and publications to analyze the rela-
tionship between demographic, socioeconomic and the health status of the population
in a region. In this way, information is provided to monitor health status from different
perspectives, such as environmental and social factors, health and mental states, habits,
common diseases, demographic, among others [2, 16]. It also allows to meet the goals
established in the sector’s health plans and the objectives of sustainability development
by the country
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_42

Both business intelligence (BI) and visual analytics (VA) provide mechanisms to
support organizational decision-making processes. The BI focuses on collecting,
organizing and managing information from several sources, providing updated, con-
sistent and complete data for the generation of knowledge, allowing the discovery of
patterns and trends, indicators monitoring and in general different levels of analysis [3,
4]. The VA provides interactive, easy-to-use visual interfaces that aid in the intuitive
exploration of information. It also offers options for manipulating information (ﬁlter-
ing, grouping, sorting, eliminating, reﬁning, among others), interacting easily and
quickly with it [5, 6].
The public health sector is responsible for the planning of actions, decision-making
and follow-up carried out by different organizations, such as the congress, health
institutes, among others. As a consequence, its analyses require the integration and
structuring of information from diverse heterogeneous sources created from different
perspectives, such as health centers and territorial surveys.
The availability of a public health observatory that uses the techniques and tools of
the BI and VA will allow the integration, visualization and manipulation of the
information that is collected and stored in the sector.
Right now, there are different health observatories, however, they are very static
and usually present the information in textual reports that cannot be manipulated. This
is the case of the Observatory of Public Health of Santander. Other observatories, such
as the Caldas Health Observatory, provide separate indicators that do not relate the
information to each other to facilitate their analysis. Also, applications related to health
observatories only support a speciﬁc type of analysis or functionality and decision
support tools are difﬁcult to adapt by themselves to respond to the needs of a health
observatory.
The lack of such tools in Colombia and the difﬁculty to have a clear strategy to
build them motivates this work, which is a strategy to build a Public Health Digital
Observatory.
The article continues presenting in Sect. 2 a summary of the literature review
carried out. Section 3 describes the proposed strategy and the main results obtained
from its application in a case study. Section 4 presents the evaluation made to the case
study. Finally, Sect. 5 presents the conclusions and future work.
2
State-of-the-Art
Currently there is a wide range of analysis and BI platforms offered in the market that
cover different types of analysis and have different levels of maturity classiﬁed
according to Gartner in descriptive, diagnostic, predictive and analytical analysis [7].
Most of the applications built with such analytical platforms are descriptive because the
capabilities offered, for example, reports or dashboards, are used to describe dimen-
sions and measures of a fundamental aspect of the business. However, the tendency of
most of the solutions goes towards the construction of interactive visualizations that
allow to discover new perspectives and to generate value for the companies.
There are different proposals about health observatories. They offer only static
analysis like the existing observatories in Colombia “Así vamos en salud” [8], the
444
L. A. Lozano and M. del Pilar Villamil

Public Health Observatory of Santander (OSPS) [9] and The Observatory of Quality of
Health Care [10]. However, observatories such as the Caldas Health Observatory, the
Global Health Observatory (GHO) [11] and the network of public health observatories
(PHOs) [12] offer interaction tools that allow users to explore information and perform
more comprehensive analyzes.
3
Strategy to Develop Digital Public Health Observatories
Based on the Kimball methodology [13] for the development of BI applications and a
series of methods proposed for the development of VA components, a strategy was
developed that allows the creation of digital public health observatories. As in the
Kimball methodology, the proposed strategy consists of two main stages. The ﬁrst is
the deﬁnition and global planning stage that discloses the organization’s business
processes and the requirements that can be supported by the analysis of the informa-
tion, as well as providing a road map for its development. This stage is mainly based on
the realization of strategies to collect business requirements such as the execution of
interviews where it is required to answer questions associated with the daily task
related to the analytical part, such as: What styles of analysis are made? How do they
do it and What sources do they use? With this description in mind, projects are
generated and prioritized to be developed in the second part of the strategy. Therefore,
in the second stage, called the development stage, we start working on a cyclical model
composed of ten main tasks: planning, administration, deﬁnition of business require-
ments, technology, data, applications, evaluation, deployment, maintenance and
increase.
In this project, the case study was developed at the Colombian National Institute of
Health (INS). INS is a national scientiﬁc technical institute that contributes to the
improvement of the health of Colombians through the generation of knowledge and
monitoring of public health. It consists of four lines of action: Surveillance and Risk
Analysis in Public Health, National Network of Laboratories, Research and Production.
The main results of the application of the strategy to the case study are presented
below.
3.1
Deﬁnition Stage
The main objective of this stage is to create the route to continue implementing ana-
lytical solutions that support the business processes of the organization. To do this,
three main activities are deﬁned: deﬁnition of scope, deﬁnition of business require-
ments and prioritization of the same. Initially, the scope of the project was deﬁned in
the case study: the decision was made to work only with the Public Health Surveillance
and Risk Analysis line of the INS, since its objective is directly related to information
management and its analysis for decision making. In addition, it was decided to carry
out exclusively the development of the main project that is identiﬁed at this stage by
time constraints.
For the identiﬁcation and deﬁnition of business requirements, Dr. Oscar Bernal
[14, 15], expert in public health, was interviewed. Based on this interview, and the
Strategy to Develop a Digital Public Health
445

information gathered from the study of the state of the art, three main analytical topics
were identiﬁed: (1) Understanding of the current health situation, (2) Monitoring of
objectives and (3) Control and Resource planning.
Current health: This is analyzed as the geographical approach that allows to locate
the place of origin, reported areas and focus on the social factors that are related to the
people that is affected such as age, gender, socioeconomic factors and health habits.
Monitoring the achievement of objectives: This analytical theme is composed of the
analyzes related to the fulﬁllment of indicators and objectives proposed to evaluate the
efﬁciency of the decisions taken. For example, monitoring public health events and
meeting sustainable development goals.
Control and planning of resources: These analyzes are more complex than the
previous ones and generally use algorithms and statistical models to obtain the desired
results. For example, the use of mathematical models to predict the spread of an
epidemic and to plan the vaccination campaigns or take the necessary measures.
The prioritization of the projects, considered the feasibility (number of sources of
information involved), the impact (number of analyzes required) and importance for the
organization, and with these results, a bus matrix was deﬁned, to cross the information
of Status of the current situation of Health, Monitoring and compliance of objectives
and Control and planning of resources against the dimensions person, date, event,
territory, regime and social condition.
3.2
Development Stage
Based on the prioritization obtained in the previous stage, each of these projects is
developed in an iterative way. This stage consists of 10 main activities: Planning,
deﬁnition of business requirements, technology, data, applications, evaluation,
deployment, maintenance, growth and project management.
It began with the planning of the project identiﬁed in the previous stage, called
Current State of the Health Situation of Colombia. Based on the analyzes that had been
carried out in the deﬁnition phase, it was decided to work with the information of the
cases reported to the INS of tuberculous meningitis, extra pulmonary tuberculosis,
pulmonary tuberculosis and drug resistant tuberculosis. The information was extracted
from reports of the INS during the years 2007 to 2011.
In the technology activity, it was decided to use SQL Server to create the data
warehouse and Tableau Software for the development of the application. In the data
activity, a Datamart was designed with a star schema, the fact table was called Case
Reported, with 10 tables of dimensions such as date, event, patient, among others.
In the data exploration, the following results were obtained: Initially, 52,508
records provided by the INS with information on the cases reported in the years 2007–
2011 of mycobacterial diseases were listed: pulmonary tuberculosis, extra pulmonary
tuberculosis, tuberculous meningitis and leprosy. Considering the deﬁned range, the
reported cases of Leprosy were excluded, resulting in 50,364 records, with the highest
number of records, of pulmonary tuberculosis (40,855 cases), followed by extrapul-
monary tuberculosis (8,784 cases) and tuberculous meningitis (725). Pulmonary
446
L. A. Lozano and M. del Pilar Villamil

tuberculosis cases have had a variant behavior over time since in some years the
number of cases increases from 2007 to 2008 and from 2009 to 2010 and in others it
decreases as from 2008 to 2009 and from 2010 to 2011. Cases Of extrapulmonary
tuberculosis have remained more or less stable and cases of tuberculous meningitis
have been increasing over time.
In the dispersion of data, considering the age range, it is observed that the popu-
lation most affected by tuberculous meningitis is between 15 and 60 years, extrapul-
monary tuberculosis between 15 and 70 years and pulmonary tuberculosis between 15
and more than 80 years. In addition, in all three cases it can also be seen that the
population between 0 and 4 years is also considerably affected.
In the three events, the distribution of the population in the different ethnic groups
coincides. The largest number of cases belongs to the other ethnic group, followed by
Black, Mulato and Afro Colombian, raizales, gitanos and palenqueros. As in the dis-
tribution of ethnicity, the number of events in the population group “Other Population
Groups” has a fairly large proportion with respect to the others. In the three events
analyzed, the following groups are most affected: prisoners, displaced persons,
migrants, pregnant women and the disabled.
As for the population regime, for the three events the subsidized regime has the
highest number of cases followed by the contributory regime, the non-afﬁliated, the
special regime and the exceptions.
As for the quality of the data, generally good information is available. When
analyzing the existing null values, the following results were obtained: of the 50,364
cases, there are 10,868 records without a patient’s identity document, 7 records that do
not have the patient’s age, 40,590 that do not have the occupation, 27 have an unknown
country in the country of origin, 7 have an unknown country in the country of residence
and 96 cases report to Colombia as a country of residence but the department in which
they reside is unknown.
From the execution of the application speciﬁcation phase, three applications were
deﬁned to support the selected analytical theme: report, trace and event evolution,
shown respectively in Figs. 1. All designed to be used by novice users with basic
knowledge of information technology and public health.
Based on the identiﬁed analysis tasks, three views were deﬁned, one for each
deﬁned application and described based on the Pﬁtzner taxonomy, et al. (2000).
Fig. 1. Description of identiﬁed analyzes, event evolution and event tracking.
Strategy to Develop a Digital Public Health
447

The event report view, shown in the ﬁrst image of Fig. 1, shows the reported cases
of the different events relating the characteristics of the population, thus visualizing the
health status of a population group. It seeks to answer questions such as: Which
department has the highest incidence of a selected event? What characteristics does the
population most affected by an event? It consists of a dynamic graphical interface in
which the screen is divided into three main parts: the ﬁlter zone, the main zone and the
trace zone. This distribution was made to focus the attention of the user in the main
zone and from the global vision of the information start to specify the analysis using the
ﬁlter zone. The interactivity allowed is done manually through the interaction of the
user with the mouse. Additionally, an automatic interaction is performed by inter-
connecting the different views of the information allowing all of them to be updated
when the user interacts manually. The analysis tasks used in this view are: overview,
zoom, ﬁltering, on-demand details, relationship, history, and extraction. Then, for
reasons of space, the main characteristics of the Filters area are described.
Filter area: This area is located on the left side of the screen composed of ﬁlters by
event, year, age, population group, ethnicity, gender and health regime. In this area, a
pastel color palette was used so as not to distract the attention of the user from the main
area of analysis. To present the events we used a list with simple selection because it is
not necessary to compare the different events because each one has different charac-
teristics. To represent the years, a line chart was used to see the behavior of the selected
event over time. This is important since it allows analyzing the evolution of the event.
For the representation of the age, the population group and the ethnic group a bar
diagram was used that allows to see the distribution of the population in the different
existing categories; In the three graphs, a logarithmic scale was used since some
categories have very distant values. Finally, for the representation of the regime and the
genre, two pie charts were used to facilitate the understanding of the distribution of the
population, and because there are few categories and with close ranges, the analysis
with this graph is adequate.
The evolution view of the events, shown in the middle image of Fig. 1, compares
the behavior of the events studied over time and the location of origin, visualizing their
evolution and identifying patterns of behavior over time or location. It allows you to
answer questions such as: How has the evolution of an event been through time? Is
there a period or week of the year that presents more cases? How have the behavior of
an event been compared to other departments? The analysis tasks used in this view are:
zoom, ﬁltering, details on demand, relationship and extraction. Finally, the event
tracking view, shown on the right-hand side in Fig. 1, displays the relationships
between the place of residence and event notiﬁcation, identifying the locations where
the services are provided. It allows you to answer questions such as: Where are health
services being provided? The analysis tasks used in this view are: ﬁlter, details on
demand, relationship and extraction.
448
L. A. Lozano and M. del Pilar Villamil

4
Evaluation
To validate the design of the application and to evaluate the compliance of the func-
tionality, user experience (usability) and system efﬁciency, three types of evaluation
were used: pre-controlled experiment, questionnaire and evaluation of heuristics. To do
this, a group of heuristics was initially deﬁned, the pre-experiment was designed and a
questionnaire was created to evaluate the heuristics deﬁned. The tests were done to a
group of experts in usability and another group of experts in public health. The fol-
lowing are the heuristics used, the details of the evaluation and the results obtained.
4.1
Methodology
A controlled pre-experiment was performed on 10 users where each one had to perform
predetermined analysis tasks and then answer some questions based on the heuristic
deﬁned above. The 10 users belonged to 2 groups depending on their specialties. The
ﬁrst group consisted of public health experts, directors of health surveillance entities,
experts in medical information analysis, and experts in epidemiology. The second
group consisted of computer specialists, visual analytical systems, designers, and
information visualization systems. Each of the trials was recorded for further analysis.
4.2
Results
The tests had an average duration of 45 min. Among the population evaluated, 70%
were men, the age group ranged from 28 to 52 years, 50% of the participants were
familiar with public health and 40% were constantly using software tools for analysis
of the information.
Results test manually: Of the 5 questions, only 60% of the participants answered
them all. All participants answered question 1 and 2, 90% of participants answered
question 3, 70% answered question 4 and 60% of participants answered question 5. Of
the answers obtained in question 1, Only 20% of participants had the correct answer.
The percentage of people who responded correctly to questions 2, 3, 4 and 5 was 50,
60, 40 and 40 percent respectively.
Test results with the application: Of the 5 questions, 90% of the participants
answered the whole question. From the answers obtained in question 1, 90% of the
participants had the correct answer. In questions 2, 3 and 4, 80% of respondents
answered correctly and to question 5, 70% were correct in their response. When
comparing test results, it is observed that the best results are given using the
application.
Results of heuristics questionnaire: Each of the possible answers of the heuristics
was given a value and this value was added to ﬁnd the qualiﬁcation of the application.
The results obtained are presented in Table 1. The third column of the table shows the
percentages obtained in the evaluation of the application. It can be observed that these
results are above 86%, indicating that the application in most cases satisﬁes the needs
of the user easily and effectively. The category with the lowest score was that of ease of
Strategy to Develop a Digital Public Health
449

understanding. This was because the participants considered that the size and color of
the sources used was not the most appropriate.
5
Conclusions and Future Work
BI and VA offer techniques and tools that support decision making, the ﬁrst more
focused on the management of information and the second for visualization and
interaction with said information.
Applying these techniques in the health sector, speciﬁcally in the creation of a
Public Health Digital Observatory, provides a solid basis for assisting decision makers.
In this way, it is possible to count and interact with information that is easy to access,
analyze, organize and visualize in an orderly, summarized and easy to interpret way,
reducing the possibility of errors.
The creation of the strategy described in this article, allowed characterizing the
public health sector, generating a reference framework that guides similar problems,
improving usability and the process of information analysis. Additionally, include a
strong interaction component that involves the user and gives dynamism, expressive-
ness, ﬂexibility and greater depth to the analysis, to quickly validate their hypotheses.
Finally, it allows concluding the pertinence of using an incremental strategy for the
development of the observatory that facilitates the understanding of the business and its
construction. Additionally, it was evidenced the need to contemplate as a quality
attribute the ﬂexibility of the application in terms of the execution of the analysis and
customization of the interface. This allows the user to adapt the identiﬁed analyzes to
speciﬁc needs such as choosing the parameter to be displayed between number of
cases, percentages or incidence in the different visualizations.
The application also allowed to complete a greater number of tasks in a shorter time
and with a greater percentage of certainty compared to the manual method. However, a
more formal experiment is suggested to have strong conclusions. In addition, it is
Table 1. Results of heuristics questionnaire.
Category
Heuristic
Percentage
Ease of
understanding
(385 of 450)
Visual readability (53 of 75), Ease of reading (70 of
75), Saving effort (93 of 100), Navigability (67 of 75)
and Degree of attraction (102 of 125)
86%
Ease of learning
(435 of 500)
Predictability (69 of 75), Potentiality (46 of 50),
Informative comments (21 of 25), Adaptability (42 of
50), Controllability (44 of 50), Consistency (213 out
of 250)
87%
Efﬁciency in use
(91 of 100)
Ease of help (41 of 50) and Performance of user tasks
(50 of 50)
91%
Satisfaction in use
(219 of 225)
Cognitive satisfaction (49 of 50), Emotional
satisfaction (95 of 100), Physical satisfaction (25 of
25) and Trust (50 of 50)
97%
Category
Heuristic
Percentage
450
L. A. Lozano and M. del Pilar Villamil

important to be clear about the objective user of each analysis to support decisions in
the design of the application, as well as in the selection of users who will participate in
the validation of the same.
As a future work it is proposed the revision of the methodology of contextual
design for the collection of requirements developed by Hugh Beyer and Karen
Holtzblatt [16] since it is a user-centered design process that offers interesting char-
acteristics for the collection of requirements. These elements could be included in the
strategy giving value to this stage by allowing a better understanding of the work
environment of users and their needs.
References
1. Ministerio de Salud y Protección Social, Vigilancia en Salud Pública. https://www.minsalud.
gov.co/salud/publica/epidemiologia/Paginas/vigilancia-salud-publica.aspx. último acceso 06
May 2017
2. Gómez Dantés, H., Arreola Ornelas, H., Lozano, R., Knaul, F.M.: Observatorio de la Salud:
una iniciativa para América Latina y el Caribe. Fundación Mexicana para la Salud e
Instituto CARSO para la Salud, México, D.F. (2008). http://dgces.salud.gob.mx/ocasep/
doctos/doc_09.pdf. último acceso 06 May 2017
3. Evelson, B.: Want to know what forrester’s lead data analysts are thinking about bi and the
data domain? (2010). http://blogs.forrester.com/boris_evelson/10-04-29-want_know_what_
forresters_lead_data_analysts_are_thinking_about_bi_and_data_domain
4. Bigatti, C., Grasso, M: Bi – data warehouse (2005). http://www.edutecne.utn.edu.ar/sist-
gestion-II/Apunte%20BI.pdf
5. VisualAnalytics.eu. What is visual analytics? (2012). http://www.visual-analytics.eu/faq/.
últmo acceso 06 May 2017
6. Hanrahan, P., Stolte, C., Mackinlay, J.: Selecting a visual analytics application (2009). http://
mkt.tableausoftware.com/ﬁles/selecting-visual-analytics-app.pdf. último acceso 06 May
2017
7. Schlegel, K., Sallam, R., Yuen, D., Tapadinhas, J.: Magic quadrant for business intelligence
and
analytics
platforms
(2013).
http://www.gartner.com/technology/reprints.do?id=1-
1DZLPEP&ct=130207&st=sb
8. Así vamos en salud. http://www.asivamosensalud.org/. último acceso 06 May 2017
9. Observatorio de Salud Pública de Santander. http://saludsantander.gov.co/web/. último
acceso 06 May 2017
10. Goicochea, A.: Resumiendo el “magic quad- rantfor business intelligence platforms 2012”
(2012).
https://anibalgoicochea.com/2012/02/10/magic-quadrant-for-business-intelligence-
platforms-2012/. último acceso 06 May 2017
11. Global health observatory (gho). http://www.who.int/gho/en/. último acceso 06 May 2017
12. Public health observatories (phos). http://www.apho.org.uk/resource/view.aspx?RID=39421.
último acceso 06 May 2017
13. Kimball, R., Ross, M., Thornthwaite, W., Mundy, J., Becker, B.: The data warehouse
lifecycle toolkit. 2nd edición (2008). http://www.gbv.de/dms/ilmenau/toc/545984432.PDF.
último acceso 06 May 2017
Strategy to Develop a Digital Public Health
451

14. Bernal, O.: Sistemas de información en el sector salud en Colombia. Colombia. Revista
Gerencia y Politicas De Salud, v. 10 fasc.21, pp. 85–100. ed: Editorial Gente Nueva Ltda.
(2011). ISSN 1657-7027
15. Gutierrez, C., Bernal, O.: La salud en Colombia. Logros, retos y recomendaciones. En:
Colombia 2012, v. 0, p. 576. ed: Ediciones Uniandes (2012). ISBN 978-958-695-774-8
16. Leite, P., Gonçalves, J., Teixeira, P., Rocha, Á.: A model for the evaluation of data quality in
health unit websites. Health Inform. J. 22(3), 479–495 (2016)
452
L. A. Lozano and M. del Pilar Villamil

Using Machine Learning for Sentiment
and Social Inﬂuence Analysis in Text
Emmanuel Awuni Kolog(&), Calkin Suero Montero,
and Tapani Toivonen
School of Computing, University of Eastern Finland,
P. O. Box 111, 80101 Joensuu, Finland
{emmanuk,calkins,tapani.toivonen}@uef.ﬁ
Abstract. Students’ academic achievement is largely driven by their social
phenomena, which is shaped by social inﬂuence and opinion dynamics. In this
paper, we employed a machine learning technique to detect social inﬂuence and
sentiment in text-based students’ life stories. The life stories were ﬁrst
pre-processed and clustered using k-means with euclidean distance. After that,
we identiﬁed domestic, peer and school staff as the main inﬂuences on students’
academic development. The various inﬂuences were used as class labels for
supervised classiﬁcation using SMO, MNB and J48 decision tree classiﬁers. In
addition, the stories were manually labelled with positive and negative senti-
ments. We employed 10-folds cross-validation in classifying the sentiments and
the social inﬂuences in the story corpus. The result show that peer inﬂuence is
more salient on students’ academic development followed by staff (15%) and
domestic inﬂuences (12%). However, the remaining 54% of the stories contains
unrelated social and other inﬂuences. Also, Students expressed more negative
sentiment towards academic engagement than the positive sentiments. As per
the classiﬁer performance, SMO was found to be superior over MNB and J48 in
the sentiment classiﬁcation while MNB also performed slightly better than the
SMO and J48 in the social inﬂuence analysis.
Keywords: Clustering  Sentiment  Social inﬂuence  Text classiﬁcation
Student
1
Introduction
Social phenomena are shaped by social inﬂuence and opinion dynamics [1]. Social
inﬂuence occurs when a person’s emotions, opinion, or behaviours are affected by
others [1]. With this, social inﬂuence takes many forms and can be seen in conformity,
socialization, peer pressure, obedience, leadership, persuasion, sales, and marketing
[2]. Parents, teachers and peers to students are found to be the main sources of social
inﬂuence on students towards their academic development [13–15]. In line with this,
Ganotice and King [3] found students’ higher levels of social support towards their
academic development from their parents, teachers and peers. Further, Ganotice and
King [3] have asserted, from their study, that students’ higher achievement in their test
scores is socially inﬂuenced. This implies that students’ actions, inactions, attitudes and
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_43

behaviours are inﬂuenced by others while conducting their studies [17]. Many students,
at the level of their senior high school education, do not earn income that warrant
self-dependency, rather they rely hugely on their parents and close relations for their
well-being especially in the developing economies [24].
Opinion dynamics is an important component of social phenomena that has widely
been studied in psychology. This is because of its relevance of understanding one’s
behaviour and their perception towards others in a society. According to Sirbu et al. [7]
“opinion formation is a complex process affected by the interplay of different elements,
including the individual predisposition, the inﬂuence of positive and negative peer
interaction (social networks playing a crucial role in this respect), the information each
individual is exposed to, and many others”. Opinion analysis has long been a subject of
interest for researchers in the ﬁeld of educational technology to assess and predict
students’ perception towards teaching and teaching methodologies of teachers. In this
light, we believe that the magnitude and nature of social phenomenon represents a core
component of students’ academic development. Hence, the need to explore how
computational techniques could be used to analyse social phenomenon in students’ life
stories, especially on detecting sentiment and social inﬂuence from students’ generated
content. Thus, school counsellors and administrators would be able to assist students in
their academic development through an informed decision based on students’ senti-
ments and social inﬂuence.
Trust is tacit which represents a core element for students’ relationships with others
[4, 12]. It is undeniable fact that students need to develop trust in their school coun-
sellors or people close to them, in order for them to open up their life conﬁdentialities
[20, 25]. Therefore, counsellors are obliged to give students a reason to trust them.
Kolog et al. [5, 16] and Glasheen et al. [6], in their respective studies, believe that
many students prefer to be counselled anonymously because of the lack of trust they
have in their counsellors. In this light, many students may be confronted with academic
and life threatening challenges yet decline to share with counsellors, and rather suffer in
silence. In this paper, we collected students’ life stories that may have never been
shared with their counsellors, and these stories were then developed into suitable
corpus for analysis [25]. With machine learning (ML) technique, we aim to analysis
students’ sentiments and track social inﬂuences on students’ academic development
from the life story corpus.
Generally, the aim of this study is to examine the role of social support from
parents, teachers, and peers in students’ academic achievement through their life sto-
ries. Knowing and understanding of the social phenomena of learners, counsellors,
teachers and school administrators are able help learners to manage their academic
work. For instance, if a learner experiences a sharp decline in his or her academic
performance as a result of parental inﬂuence, counsellor can alert parents and get them
involve in the learner’s development. The sentiments associated with students’ aca-
demic life stories can prompt teachers to modify the content of their teaching
methodologies or modify any existing intelligent system for teaching and learning to
suit leaners needs.
454
E. A. Kolog et al.

2
Related Works
Text mining is one of the most sought areas of artiﬁcial intelligence (AI) in recent
years. The ﬁeld has attracted a lot of attention from researchers with deferring interests.
Most notable of these areas are Natural language processing (NLP), affective com-
puting, computational linguistics and human computer interaction. Usually, these ﬁelds
employs machine learning (ML) techniques for predictions, clustering, among others.
ML is an application of artiﬁcial intelligence (AI) that provides systems the ability to
automatically learn and improve from experience without being explicitly programmed
[8]. Two different approaches of ML are involved: supervised and unsupervised. In
supervised ML, training data is required to train a classiﬁer. After that, a model is
created, capable of predicting unseen data. Example is a classiﬁcation of text docu-
ments according to predeﬁned emotion categories, such as Ekman [19] and Plutchik’s
[18] basic emotions. Unsupervised ML, on the other hand, rather looks for patterns in
an unseen data, thus training of a classiﬁer is not required.
In a related study, Lopes et al. [9] employed both supervised and unsupervised ML
algorithms for automatic classiﬁcation of sentiments from 2000 social network users.
The researchers crawled the experimented data from a social networking platform
called Scientia.Net. Scientia.Net is a social network site for academic environment with
speciﬁc content for scientists who want to share their research or advance in their work
through interaction with other researchers [9]. Two supervised classiﬁers: Multilayer
perceptron, support vector machine and two unsupervised classiﬁers: modiﬁed koho-
nen network and k-means algorithm were used in their experiment. The researchers, in
their study aimed to ﬁnd out which among the aforementioned ML algorithms per-
formed better in the classiﬁcation. With respect to the results from the supervised
classiﬁers, the researchers found SVM to outperform the multilayer perceptron. Also,
the results from the unsupervised algorithms revealed that Kohonen Network obtained
an average execution time much longer compared with K-means algorithm, but with a
lower error rate regarding the classiﬁcation.
Pang et al. [10] carried out an experiment on automatic classiﬁcation of sentiments
in text documents using ML algorithms. The researchers classiﬁed the text documents
by topic, but overall sentiment according to negative and positive sentiments. From
their experiment, three ML algorithms were used in classifying sentiments in movie
review datasets. The ML algorithms employed by the researchers are Naive Bayes,
maximum entropy, and support vector machines. The results from the ML algorithm
were compared with human-produced baselines. In the end, Pang et al. [10] found the
the ML algorithms to perform better than the human-produced baselines. However, the
researchers found the four ML algorithms to perform poorly on the sentiment classi-
ﬁcation by topic.
Altrabsheh et al. [11] used supervised ML techniques to detect ﬁne- (sentiments)
and coarse-grained emotions in student’s feedback, of which they collected the feed-
back from social media, such as twitter. The researchers classify ﬁne-grained emotions
in the tweets data by using Naive Bayes (NB), Maximum Entropy (Max. Ent.), and
support vector machines (SVM) classiﬁers. In the end, Altrabsheh et al. [11] concluded
that NB, Max. Ent. and SVM classiﬁers were superior in terms of their capabilities in
Using Machine Learning for Sentiment and Social Inﬂuence Analysis in Text
455

text classiﬁcation. While Altrabsheh et al. [11] have demonstrated the superiority of
NB, Max. Ent. and SVM in emotion analysis in text, we also employed NB, SVM and
MNB classiﬁers to show their efﬁcacies in detecting sentiment and social inﬂuence in
students’ life stories.
Social inﬂuence analysis has long been researched in diverse domains, especially in
the ﬁelds of psychology and business. Yet, to the best of our knowledge, we could not
ﬁnd studies that have employed machine learning techniques to detect social inﬂuence
in students’ text though there are a vast amount of similar studies in sentiment or
opinion analysis in text. For this reason, we believe that this paper will open up the
awareness and the need for researchers in computer science to venture. Social inﬂuence
plays an important role in students’ academic development, especially in the context of
the emotional and personal-social development.
3
Experimental Setup
This section takes a holistic view of the processes and approaches of the experiment in
this work. The source of the data used in this study is outlined in this section as well.
We have also elaborated on the various processes of clustering, annotation and clas-
siﬁcation of the life story corpus according to sentiments and social inﬂuence (SI).
Figure 1 represents the experimental process of this paper. However, detail explana-
tions of the ﬁgure are outlined in the Subsects. 3.1 to 3.4. In this study, we have
considered the inﬂuence of students’ academic development from the perspective of
their peers (PI), school staff (StI), domestic (DI) and other or no social inﬂuences
(NI) that students have expressed in their life stories.
3.1
Dataset
Students from Ghana were tasked to write about their life stories subjectively. The life
stories, in this study and as we deﬁned for the students, is concerned with the emotional
antecedents or life situations that have affected students in their academic orientation.
Fig. 1. Experimental research process
456
E. A. Kolog et al.

Given the context and the challenges associated with the inability of many students to
use ICT, we administered paper-based questionnaires for the students to express their
life situations on which students are able or not able to discuss with their school
counsellors. Students were assured that their stories would not be traced to them as they
were made not to indicate anything in the stories that might identify them. Ethically,
students were made to sign informed consent form before the stories were collected.
The collected stories were pre-processed for annotation and for further analysis. After
the initial pre-processing, the maximum number of sentences in text instances or
document was ﬁve while the minimum sentence was only one.
3.2
Cluster Analysis
Clustering is unsupervised ML technique that classiﬁes new cases based on a similarity
measure (e.g. distance function). For pre-processing of the data before the cluster
analysis, we developed a Python script that counted certain keywords from the life
story dataset. The usage of key words, such as multiple occurrences of ‘father’ or
‘mother’ in the life story indicated that the story is most likely to have domestic
inﬂuence. The Python script generated 1-dimentional input vector space for the cluster
analysis with each of input vector having the length of 3 indices: the ﬁrst index
indicated the occurrence of domestic keywords, the second index indicates the
occurrence of staff keywords and the third index indicates the occurrences of peer
keywords. Sample stories from the students with the keywords highlighted:
“For my ﬁrst day in school, I have realised that most of the teachers don’t attend to class and
this is really affecting us badly in academic work”
“I have a good relationship with my friends. Anytime I don’t understand anything in class my
friends and other mates helps me to understand”
After the pre-processing of the dataset, we used two different widely known cluster
algorithms to cluster the data. The algorithms were k-means and hierarchical cluster
algorithms, all with euclidean distance as a distance metrics. Euclidean distance
computes the root of square difference between co-ordinates of pair of objects [21]. For
instance, If X = (x1, x2, ……, xn) and Y = (y1, y2,…., yn) are two points Euclidean n-
space, then the distance (Dist.) from X to Y, or from Y to X is given by the Pytha-
gorean formula in the Eq. 3.1.
DistXY ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
m
k¼1
ðXi  YiÞ2
s
ð3:1Þ
The number of clusters were set to 4. In this light, our hypothesis before the cluster
analysis was that:
• One of the clusters would have high number of vectors that contain high number of
domestic inﬂuence attributes, such as parents, family, among others.
• One of the clusters would have high number of vectors that contain peer inﬂuence
attributes, such as friend, mate, colleagues, among others.
Using Machine Learning for Sentiment and Social Inﬂuence Analysis in Text
457

• One of the clusters would be inﬂuenced highly by their teaching or non-teaching
staff, such as teacher, counsellor, staff, among others.
• The remaining data would have low or no aforementioned attributes.
3.3
Cluster Evaluation
For the evaluation of the clustering algorithm, we used Silhouette index [22]. Silhouette
index calculates how well a dataset vector is clustered. Silhouette index varies from −1
to 1, where −1 and other low values indicate that the vectors might be in the wrong
clusters. The performance of k-means was slightly better (2.8 compared to 2.4 of
hierarchical cluster analysis algorithm). Based on the Silhouette index of k-means, we
chose to maintain the results of k-means algorithm and as we expected, the resulted
clusters contained either high domestic inﬂuence attributes, high peer inﬂuence attri-
butes, high staff attributes and attributes where all values were low (neutral inﬂuence).
For instance, one of the clusters held the input vectors [1, 2, 11] and [0, 4, 0] which
indicates higher staff inﬂuence. Another example is an input vectors of [0, 0, 4] and [0,
0, 3] which also indicates a higher peer inﬂuence. In the neutral cluster some of the
input vectors, for instance, were [0, 0, 0] and [1, 1, 0], which also indicates low
occurrences of all the inﬂuences. Finally, the labels and the clustered input vectors
created by the k-means acted as the basis of the supervised classiﬁcation process.
3.4
Annotation and Classiﬁcation
The various social inﬂuences that were identiﬁed during the cluster analysis were used
as class labels for the life story corpus. The rationale is to further apply supervised text
classiﬁcation technique on the labelled corpus. Tables 2 represents the instances
(documents) of the stories for each class labels (social inﬂuence) in the life stories.
Another objective of this study is to examine and extract the sentiments students
expressed in their life stories. With this in mind, the corpus was given out to three
selected counsellors to annotate them with positive and negative sentiments. A kappa
score of 82% was obtained, which was deemed suitable for classiﬁcation [14]. The
kappa score was obtained after a repeated exercise and consensus among the annota-
tors. Table 1 shows the number of instances (documents) identiﬁed for each class
labels in the life stories for the sentiments analysis.
WEKA software [23] was used to perform the classiﬁcation exercise. WEKA was
developed by Machine Learning Group at the University of Waikato in the New
Zealand1. It is a collection of machine learning algorithms for data mining tasks. The
algorithms can either be applied directly to a dataset or called from one’s own Java
code. WEKA contains tools for data pre-processing, classiﬁcation, regression, clus-
tering, association rules, and visualization [23]. It is also well-suited for developing
new machine learning schemes.
Two different classiﬁcation tasks was carried out in this work: social inﬂuence and
sentiment classiﬁcations. In WEKA, we employed SMO (sequential minimum
1 http://www.cs.waikato.ac.nz/ml/weka/.
458
E. A. Kolog et al.

optimisation) for SVM (support vector machine), J48 decision tree and MNB (Multi-
nomial Naïve-Bayes) classiﬁers for the classiﬁcation task. During classiﬁcation in
WEKA, 10-folds cross-validation was performed for the dataset, where the original
sample is randomly partitioned into 10 equal size subsamples. With this, the
cross-validation process is repeated 10 times (the folds), with each of the 10 sub-
samples used exactly once as the validation data. During classiﬁcation, the life story
corpus was converted into bag-of-words features, as part of the pre-processing, with
‘StringToWordVector’ ﬁlter in WEKA.
4
Results and Discussion
This section presents the results and discussion of this study. While we report and
discuss the ﬁndings of the sentiment and social inﬂuence classiﬁcation, the section also
reports the performance of the various classiﬁers used in the classiﬁcation task.
4.1
Sentiments
The results in Table 1 shows that students expressed more negative (approx. 56%)
sentiments in their life stories than positive (approx. 44%) sentiments. The difference
could be alluded to the fact that students’ sentiments towards their academic devel-
opment is mostly marred with challenges and difﬁculties. As observed from the dataset,
we can argue that many of the students understood their life stories to be their life
challenges though their understanding was partly in line with our expectations. Being
able to detect students’ social phenomena (sentiment and social inﬂuence), counsellors
and perhaps school administrators will be able to assist students in creating awareness
to the various sources of inﬂuences on students’ academic development.
Table 1. Number of documents annotated with positive, negative and neutral
Sentiment # of instances
Positive
436 (44.6%)
Negative
557 (55.7%)
Neutral
007 (0.7%)
Total
1,000 (1,00%)
Table 2. Clustered instances of the social inﬂuence
Social Inﬂuence
# of clustered instances
Domestic
119 (11.9%)
Peer
199 (19.9%)
Staff
147 (14.7%)
Neutral and others 535 (53.5%)
Total
1, 000 (100%)
Using Machine Learning for Sentiment and Social Inﬂuence Analysis in Text
459

Having established the quantitative levels of sentiments expressed by the students
in their life stories, it is important to look into the performance of the ML classiﬁers.
We used precision (P), recall (R) and f-measure (F) to report the performance of the
various classiﬁers used in this study. In this light, the various evaluation measures were
computed for each individual sentiment categories, and as well the overall weighted
performance of the classiﬁers (shown in Table 3).
As indicated in Table 3, the performance of all the classiﬁers in the classiﬁcation of
each of the sentiments were high. This implies that the machine learning techniques are
suitable for sentiment classiﬁcation of students’ life stories. By comparing the accu-
racies of each of the classiﬁers, the performance of MNB (83%) surpasses that of the
SMO (80%) and the J48 (69%) classiﬁers. Apart from the MNB classiﬁer, SMO and
J48 in classifying sentiments in the stories were below the baseline of the kappa score
from the counsellors as indicated in Sect. 3.3.
4.2
Social Inﬂuence
We also looked into the various social inﬂuences found in the students’ life stories.
From Table 2, apart from the ‘neutral and other inﬂuences’ (approx. 54%) expressed by
the students, peer inﬂuence (approx. 20%) was found to be salient inﬂuence on stu-
dents’ academic development followed by the staff (approx.15%) and domestic inﬂu-
ences (12%). This implies that students’ academic development, based on their life
stories, is mainly inﬂuenced by peers, such as course mates and friends. For instance,
while some students are inspired to learn or receive help from their friends, others may
be bullied or be discouraged by their friends while conducting their studies. Interest-
ingly, this ﬁnding is consistent with Ganotice and King [3] who also found peer support
to be salient inﬂuence on the academic engagement of students followed by their
parents and then teachers. Ganotice and King [3] conducted their study with Filipino
secondary students where they used questionnaires to collect data regarding students
perceived social inﬂuence and academic engagement. unlike this study where we used
ML techniques, Ganotice and King [3] analysed their data thematically.
To underpin the aforementioned ﬁndings, there is the need to ascertain the per-
formance of the classiﬁcation algorithms. Therefore, we evaluated the performance of
the classiﬁers with coarse-grained evaluation measures. From Table 4, the performance
of SMO in the classiﬁcation of all the categories were high except for the domestic
inﬂuence that the classiﬁer performance was low. The best performance of the MNB
Table 3. Classiﬁers performance on sentiment classiﬁcation (in %)
Sentiment
SMO
MNB
J48
P
R
F
P
R
F
P
R
F
Positive
80 76 78 86 74 79 64 70 67
Negative
81 85 83 81 91 86 75 70 72
Weighted avg. 80 80 80 83 83 82 70 69 70
460
E. A. Kolog et al.

classiﬁer is the classiﬁcation of staff and neutral inﬂuences, while the classiﬁcation of
domestic inﬂuence yielded low. J48 performance was high with “neutral” while the
staff and domestic inﬂuences were low. Overall, the level of accuracies of each of the
classiﬁers in the classiﬁcation of the social inﬂuence in the life stories are 80%, 72%
and 69% for SMO, MNB and J48 respectively. This generally implies a good per-
formance of all the classiﬁers with SMO being the best classiﬁer for the detection of
social inﬂuence in the students’ life stories.
5
Conclusion
In this paper, we used ML algorithms to analyse social phenomenon in students’ life
stories. Thus, sentiments and social inﬂuence were tracked from students’ life stories.
We used both supervised and unsupervised machine learning techniques. By the
unsupervised ML technique, the life story corpus was clustered using k-means clas-
siﬁer. This approach was to help in identifying the various clusters according to the
social inﬂuence of students on their academic development. The various social inﬂu-
ences identiﬁed from the cluster analysis were then used as class labels for further
classiﬁcation. In addition, the stories were manually annotated with positive and
negative sentiments. WEKA was employed to classify the content of the stories
according to the various categories of the sentiments and social inﬂuences of students.
Our results, with respect to the social inﬂuence, shows that students’ academic
development is largely inﬂuenced by their peers followed by their school staff and
domestic inﬂuences. The results have also shown that students expressed more of
negative sentiments than positive sentiments. With the classiﬁer evaluation, overall, the
SMO (80%) performed slightly better than J48 (72%) and the MNB (69%) classiﬁers in
terms of the social inﬂuence analysis. In the same vein, MNB (83%) was found to
perform slightly better in the sentiment analysis than the SMO (80%) and J48 (69%).
References
1. Moussaïd, M., Kämmer, J.E., Analytis, P.P., Neth, H.: Social inﬂuence and the collective
dynamics of opinion formation. PloS one 8(11), e78433 (2013)
2. Fiske, S.T.: Social Beings: Core Motives in Social Psychology. Wiley, Hoboken (2009)
Table 4. Classiﬁers performance on social inﬂuence classiﬁcation (in %)
Social inﬂuence SMO
MNB
J48
P
R
F
P
R
F
P
R
F
Domestic
67 60 63 56 24 33 69 37 48
Staff
77 65 71 66 78 58 57 45 50
Peer
85 70 77 52 46 58 77 64 70
Neutral
80 91 86 68 56 77 74 90 82
Weighted avg.
80 80 79 69 87 67 72 72 71
Using Machine Learning for Sentiment and Social Inﬂuence Analysis in Text
461

3. Ganotice, F.A., King, R.B.: Social inﬂuences on students’ academic engagement and science
achievement. Psychol. Stud. 59(1), 30–35 (2014)
4. Chevalier, M.E.: Teacher-learner relationships in adult education classrooms: the social
construction of trust. Unpublished doctoral dissertation (1995)
5. Kolog, E.A., Sutinen, E., Vanhalakka-Ruoho, M.: E-counselling implementation: students’
life stories and counselling technologies in perspective. Int. J. Educ. Dev. Inform. Commun.
Technol. 10(3), 32 (2014)
6. Glasheen, K., Campbell, M., Shochet, I.: Opportunities and challenges: school guidance
counsellors’ perceptions of counselling students online. Aust. J. Guidance Counselling 23,
222–235 (2013)
7. Sîrbu, A., Loreto, V., Servedio, V.D., Tria, F.: Opinion dynamics: models, extensions and
external effects. In: Participatory Sensing, Opinions and Collective Awareness, pp. 363–401.
Springer International Publishing (2017)
8. Munoz, A.: Machine Learning and Optimization (2014). https://www.cims.nyu.edu/
*munoz/ﬁles/ml_optimization.pdf. Accessed 02 Mar 2016
9. Lopes, L.A., Machado, V. P., Rabelo, R.D.A.: Automatic cluster labeling through Artiﬁcial
Neural Networks. In: International Joint Conference on Neural Networks(IJCNN), pp. 762–
769 (2014)
10. Pang, B., Lee, L., Vaithyanathan, S.: Thumbs up? Sentiment classiﬁcation using machine
learning techniques. In Proceedings of the ACL 2002 Conference on Empirical Methods in
Natural Language Processing, vol. 10, pp. 79–86 (2002)
11. Altrabsheh, N., Gaber, M., Cocea, M.: SA-E: sentiment analysis for education. In:
International Conference on Intelligent Decision Technologies, vol. 255, pp. 353–362, June
2013
12. Kolog, E.A., Sutinen, E., Vanhalakka-Ruoho, M., Suhonen, J., Anohah, E.: Using uniﬁed
theory of acceptance and use of technology model to predict students’ behavioral intention to
adopt and use E-counseling in Ghana. Int. J. Modern Educ. Comput. Sci. 7(11), 1 (2015)
13. Anderman, L.H., Anderman, E.M.: Social predictors of changes in students’ achievement
goal orientations. Contemp. Educ. Psychol. 24(10), 21–37 (1999)
14. Hughes, J., Kwok, O.M.: Inﬂuence of student-teacher and parent-teacher relationships on
lower achieving readers’ engagement and achievement in the primary grades. J. Educ.
Psychol. 99(1), 39 (2007)
15. Kindermann, T.: Peer group inﬂuences on students’ academic motivation. In: Handbook of
Social Inﬂuences in School Contexts: Social-Emotional, Motivation and Cognitive
Outcomes, pp. 31–47 (2016)
16. Kolog, E.A., Suero Montero, C., Sutinen, E.: Annotation agreement of emotions in text: the
inﬂuence of counsellors’ emotional state on their emotion perception. In: Proceedings of
Advanced Learning Technologies (ICALT), pp. 357–359 (2016)
17. Hughes, J., Kwok, O.M.: Inﬂuence of student-teacher and parent-teacher relationships on
lower achieving readers’ engagement and achievement in the primary grades. J. Educ.
Psychol. 99(1), 39 (2007)
18. Plutchik, R.: Emotion: Theory, Research, and Experience: Theories of Emotion, vol. 1,
p. 399. Academic, New York (1980)
19. Ekman, P.: An argument for basic emotions. Cogn. Emot. 6(3–4), 169–200 (1992)
20. Kolog, E.A., Sutinen, E., Suhonen, J., Anohah, E., Vanhalakka-Ruoho, M.: Towards
students’ behavioural intention to adopt and use E-counseling: an empirical approach of
using Uniﬁed Theory of Acceptance and Use of Technology model. In: IEEE AFRICON,
pp. 1–6 (2015)
21. Rousseeuw, P.J.: Silhouettes: a graphical aid to the interpretation and validation of cluster
analysis. Comput. Appl. Mathematics. 20, 53–65 (1987)
462
E. A. Kolog et al.

22. Kaur, D. A Comparative Study of various Distance Measures for Software fault prediction.
arXiv preprint arXiv:1411.7474 (2014)
23. Hall, M., Frank, E., Holmes, G.: Pfahringer, B., Reutemann, P., Witten, I.H.: The WEKA
Data Mining Software: An Update. SIGKDD Explorations (2009)
24. Considine, G., Zappalà, G.: Factors inﬂuencing the educational performance of students
from disadvantaged backgrounds. In: Competing Visions: Refereed Proceedings of the
National Social Policy Conference, pp. 91–107 (2002)
25. Gültekin, F., Erkan, Z., Tüzüntürk, S.: The effect of group counseling practices on trust
building among counseling trainees: from the perspective of social network analysis.
Procedia-Social Behav. Sci. 15, 2415–2420 (2011)
Using Machine Learning for Sentiment and Social Inﬂuence Analysis in Text
463

Big Data Analytics and Applications

Big Data Applications in Cancer Research:
A Case Study at the Brazilian National
Cancer Institute
Antônio Augusto Gonçalves1,2, Carlos Henrique Fernandes Martins1,
José Geraldo Pereira Barbosa2,
and Sandro Luís Freire de Castro Silva1(&)
1 Instituto Nacional do Câncer - COAE Tecnologia da Informação,
Rua do Resende 195, Rio de Janeiro 20230-092, Brazil
sandrofreire@gmail.com
2 Universidade Estácio de Sá - MADE,
Avenida Presidente Vargas 642, Rio de Janeiro 200071-001, Brazil
Abstract. The development of smart devices, Internet of Things (IOT) and
cloud computing is generating an expressive increase in the volume of public
health data that can be collected from various sources and analyzed in an
unprecedented way. The demand for Big Data analysis is increasing progres-
sively. The development of applications that support the continuing evaluation
of early detection programs is among the priorities in Brazil’s cancer control.
This paper presents the development of a big data application employed on the
management, processing and analysis of large-scale Brazilian cancer diseases
data, and the beneﬁts for cancer prevention and control derived from its
implementation.
Keywords: Big data  Cloud computing  Health informatics  IOT
1
Introduction
The Big Data concept is causing a revolution around the world. Its successful appli-
cations in business, science and health are driving radical changes in traditional
management practices. The demand for analysis using Big Data applications is
increasing in different organizations [1].
Organizations are increasingly adopting Big Data tools to perform data analysis in
order to better understand their customers, create new products and deliver more per-
sonalized services. The focus on data science and predictive analytics has generated great
research interest. However, most projects are still at an early stage with many organi-
zations developing prototypes to better understand this technology and its beneﬁts [2].
The term “big data” incorporates concepts that have been present for decades, and
their deﬁnition is progressing. The concept seems to have been resultant from a
strategic IT consulting group to manage large volumes of data with speed and variety.
In a recent review of the characterization of “Big Data”, authors have incorporated size,
complexity, and technology attributes to deﬁne the term as the storage and analysis of
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_44

large datasets using a number of techniques that include machine learning [3]. The term
applies to various areas that cover internet of things, sensors, social media and cloud.
Big Data technologies have four Vs that are related to: (1) large data volume;
(2) variety of data type; (3) high-speed data generation and updating (velocity); and
(4) value creation. The ﬁrst three Vs are associated to data collection, storage and
transfer. The last V is focused on predictive analysis using statistical methods for
knowledge extraction and decision making [4].
Big Data technologies allow a deeper, real-time insight into consumer behavior
trends and preferences. As a result, organizations are increasingly adopting these
concepts in order to better understand their customers and design customized products
and services. Organizational factors such as senior management support, openness to
change, and a strong culture-driven data analysis may contribute to the use of analytical
tools [5].
A large number of organizations are presently in the process of implementing Big
Data projects in order to obtain signiﬁcant insights from their data for improve
decision-making process. Big Data projects have fundamentally two types of chal-
lenges: manage large volumes of data and ﬁnd and combine relevant information from
different sources of knowledge of a multi-disciplinary nature [6].
Big data has been deﬁned as the gathering of complex data sets difﬁcult to manage
and process using traditional tools. Big data is based on large data sets and predictive
analysis to get insights for decision making in the healthcare sector. Although many
sectors have incorporated Big Data, in the healthcare sector its use is incipient [7].
In healthcare organizations, there are several issues that can be elucidated with Big
Data applications, such as: epidemiological surveillance, health referral systems,
sensor-based health conditions, and social networks besides to genome association
research. To solve these problems, many advanced computing tools can be applied.
The majority of Big Data projects are in their initial stages, with many companies
currently developing a strategy and road map around the subject and developing pilot
projects to better understand the technology and its potential [8].
Another application of great prospective is the personal health record systems
(PHR) that are updated in the cloud and allow users to keep track of their own health
data, most of which are still provided by health professionals. Thus, medical history
can be updated together with the patient. These systems are more than just static data
repositories, providing features through which the patient can access, manage and share
their health information with authorized health professionals by becoming active par-
ticipants in their own care [9].
Certainly, the personal health record will be useful to the patient when offering
more than just storing structured data such as name, age, diagnostic codes, laboratory
results, etc. Patients can beneﬁt if the PHR system supports the clinician’s real-time
decision-making (RTAP) processes by transforming the conventional periodic care into
a continuous communication between doctors and patients [10].
The large number of cancer cases in Brazil means that the accessibility of infor-
mation about the disease is critical for patients and clinicians involved in their care, and
those with cancer services responsibility, which opens an opportunity window for
implementing big data projects. Being in charge of the prevention and control of cancer
in Brazil, the Brazilian National Cancer Institute (INCA – Instituto Nacional de
468
A. A. Gonçalves et al.

Cancer) develops actions, campaigns and programs nationwide in compliance with the
Brazilian National Health Policy. The Institute also plays an important role in the
international arena, through technical cooperation agreements with foreign entities and
organizations, acting as an effective member of international networks of collaboration.
The development of applications that support the continuing evaluation of early
detection programs is among the priorities in Brazil’s cancer control. The strategic
importance of clinical data usage has motivated INCA in adopting electronic medical
record and implementing big data applications as decision support tools for strategic
decision making.
This paper presents the development of a big data application employed on the
management, processing and analysis of large-scale Brazilian cancer data, and the main
beneﬁts for cancer disease management derived from its implementation. As a sec-
ondary objective, this research also looks to detect the main aspects that should be
observed by Brazilian healthcare organizations that are in the ﬁrst stages of big data
development and encourage them for a rapid adoption of such solutions.
2
Big Data in Healthcare
The development of smart devices, Internet of Things (IOT) and cloud computing is
generating an expressive increase in the volume of public health data that can be
collected from various sources and analyzed in an unprecedented way. Its successful
applications in business, sciences and healthcare have radically changed their tradi-
tional practices. The demand for Big Data analysis is increasing progressively [11].
During the last decades, high volumes of data were collected in electronic medical
records to screen the health of patients, such as laboratory results, medical prescrip-
tions, clinical evolution, etc. In this environment, accessible information for
patient-oriented decision making has increased dramatically and became spread by
diverse organizations. In response to this challenge, Big Data applications are being
developed to store a large volume and variety of patient data and allow real-time access
to healthcare professionals [10].
Healthcare organizations are deﬁnitely using analytical tools to support their
ﬁnancial and operational functions. Therefore, the adoption of electronic health records
and other investments in health information technology have a great potential to
transform clinical care [12].
Healthcare organizations collect data at an incredible speed. Electronic health
records (EHRs) gather data using different approaches: structured and unstructured.
This variety can generate difﬁculties when looking for the veracity or quality assurance
of data. The Big Data projects can provide a rich source of data for analysis to increase
the capacity to offer personalized patients care [13].
Healthcare is one of the ﬁelds that need scalable and distributable applications.
Even though traditional database systems are able to handle with huge volumes of
information, both relational database management systems (RDBMS) and analytical
processing systems (OLAP) are not able to handle with real-time decision-making
processes (RTAP) [14]. Therefore, new technologies are required for projects involving
Big Data Applications in Cancer Research
469

real-time decisions and making the implementation of Big Data projects a great
opportunity [15].
These projects are of short period to solve problems for which the data can provide
answers, from hypotheses, which are then investigated iteratively through experiments
to acquire knowledge. The short-lived, well-deﬁned lifecycle allows better cost control
compared to conventional IT projects that are longer in duration and involve a con-
siderable investment of resources [16].
Not only technological innovations need to be deployed in IT projects, but the
posture of workforces needs to change so that they begin to apply the knowledge
acquired by the implemented tools. This comportment change within an organization is
often difﬁcult to achieve [17].
Much of the research on the diffusion and use of ICT has focused on conventional
IT projects typically designed to support basic operational processes in organizations.
With regard to Big Data, the literature examined still presents few studies in public
health organizations. The present study focuses on the following research question:
How does the process of adopting Big Data applications in cancer research and
treatment in Brazil occur? This research expands the scientiﬁc knowledge by identi-
fying and analyzing the factors that inﬂuenced the adoption of BIG DATA tools in the
data analysis experiments at the Brazilian National Cancer Institute (INCA).
3
Methodology
This study was carried out using qualitative research designed to present a descriptive
analysis of Big Data implementation within the National Cancer Institute (Instituto
Nacional de Cancer [INCA]). A single-case study intends to contribute to the
knowledge of organizational phenomena, presenting a contemporary description of the
application implemented, using an empirical inquiry to answer the questions what,
who, where and how [18].
The researchers conducted semi-structured interviews with six systems analysts
responsible for the development of Big Data applications in the area of Information and
Communication Technology and with ﬁve end users of the system.
The interviews had been conducted over the ﬁrst four months of 2017, based on a
script with open questions, and structured from the literature review used in the
research. All the interviews were made in the administrative units of INCA and
recorded with the agreement of the interviewees.
The data collected was examined by the categorical content analysis method, tri-
angulating the frequency in which terms present in the recorded answers occurred, with
the observations and data obtained by the researchers.
The implementation of Big Data applications at INCA was chosen as unit of
analysis in order to take advantage of the professional experience of three of the authors
that work in its Information and Communication Technology Division and due to the
fact that the organization is one of the most qualiﬁed to carry out research on oncology
in Brazil.
470
A. A. Gonçalves et al.

4
Big Data Project
To start a Big Data project, several steps are suggested as shown in Fig. 1: First, the
suitable problem should be chosen. Problems that can be solved with technologies that
handle large volumes of data in real time must be selected. Second, it is necessary to
collect data by sensors, monitors, molecular proﬁle or extract information from data-
bases and public sources. Third, it is necessary to pre-process data to obtain clean and
meaningful data.
Pre-processing of data is a critical step in the success of the Big Data project. Data
quality control is essential. The cleaned data is stored in the database for the next
analysis step. Knowledge is extracted from data processed through statistical analysis.
Finally, the analytical results are presented to the end user such as reports, online
recommendations or by decision making. Visualization of data, which uses graphs and
tables, can make analytical results easy to interpret and understand. If the results do not
make sense, you need to reshape the problem and restart the process [11].
To discover hidden relationships in the data it is necessary to develop tools for
visualizing relationships in big data sets, but visualization tools work best when sci-
entists have an idea of what to visualize in a pile of data.
A robust infrastructure must consist of a scalable hardware architecture, efﬁcient
information systems, and well-deﬁned processes for implementing Big Data projects.
The model to be developed needs to work in sync with existing IT systems; It is
therefore essential that the integration process is carried out in order to reduce any
errors [19].
Formulate
your queson
Data Collecon
Data Storage and
Transferring
Data
Analysis
Report / 
Visualizaon
Evaluaon
End
Naonal Cancer Database
Cancer Genomics
Internet
Brazilian Mortality Database
NoSQL
GEO
Infrastructure
Deep Learning
Network Analysis
R
Micro Strategy
Mortality Online Atlas
Fig. 1.
Source adapted [11].
Big Data Applications in Cancer Research
471

5
Big Data Project at Brazilian National Cancer Institute
Cancer surveillance is an essential tool of a disease control policy. Such monitoring and
evaluation should occur broadly and include instruments for the analysis of cancer
incidence, morbidity and mortality. Each of these issues plays an important role in the
surveillance system, but mortality indicators are the only ones capable of reﬂecting the
effectiveness of the interventions, since ultimately the main objective of the majority of
actions in the healthcare sector is to reduce the number of deaths.
The Brazilian National Cancer Institute (INCA) has developed the analysis of
information on cancer mortality, extracted from large data sets of the Brazilian Mor-
tality Information System (SIM). The cleaned data was stored in the database for the
next analytics steps. Knowledge was extracted from data processed through statistical
inference. This analysis has been available in real time (RTAP) as a tool of support for
cancer surveillance, in order to insight the epidemiological proﬁle and health conditions
of the Brazilian population.
Mapping spatial characteristics of cancer mortality is essential for understanding
disease incidence. Disease maps are important to visually inform health practitioners
and general community about disease distribution. This study describes the develop-
ment of an on-line atlas of cancer mortality in Brazil.
The Mortality Online Atlas has as one of its main purposes to assist public health
professionals in determining the priorities necessary for the prevention and control of
cancer. It may also be useful in identifying suspected risk situations to be explored in
analytical epidemiological studies to determine risk factors.
In the new version using Big Data techniques, INCA has developed a robust
infrastructure to provide to its users information about all cancer types. The information
was organized in national, regional, state and municipal terms. In addition, they were
classiﬁed by gender and primary tumor location. The forms of analysis range from
proportional distribution to the sophisticated statistical methodology for trend analysis.
Geographical Information Systems (GIS) and Public health mapping has been
applied to trace the geographical spread of deaths by all kinds of cancer. One example
of color maps was made based on indicator: “Total deaths by all neoplasms, by years,
by locality, in men, in the selected Brazilian states” as shown in Fig. 2.
The tools of analysis of this Atlas are introduced, accompanied by practical
examples of its use, collaborating with the dissemination of cancer mortality infor-
mation, contributing to improve the monitoring and management of the plan of actions
for the control of cancer in Brazil.
The model presents the time series of the main types of cancer of the last years
studied and allows the analysis of trends. Moreover, to assessing the risk of dying, it is
necessary for cancer surveillance, analyzing the trend in order to check pattern changes,
evaluating the effect of interventions. In fact, the most effective measure to evaluate an
intervention is its impact on the risk of dying.
Another relevant approach is to assess the risk of dying in a particular locality in
relation to the risk observed in other localities, comparing the results of the analysis. In
this Big Data application, it is possible to analyze in detail the risk of dying in different
locations.
472
A. A. Gonçalves et al.

Cancer is a disease generally associated with aging and, therefore, its involvement
at early ages may indicate the need for preventive actions. In the Model the use of more
elaborate statistical techniques allows to calculate the average number of years of life
lost of the individuals with cancer.
Frequently, when performing the analysis, it may be necessary to deepen study the
cause-and-effect relationship obtained from the previously organized models. Thus, the
Atlas On-Line of mortality is a tabulator, in which it is possible quickly get person-
alized results.
6
Conclusion
The major trend in healthcare organizations is the change of the decision-making
process based on structured data to unstructured data like images and sensor-based
devices. The rising employment of sensors is a signiﬁcant factor of home healthcare
services and telemedicine growth, meaning that the volume of data being generated
from sensors will continue to rise signiﬁcantly. This will consecutively increase the
quality of health services through more precise predictions and analysis in real time.
Nowadays, majority of healthcare organizations are developing big data projects or
are in initial stage of deployment. All of them need a suitable approach to establish the
goals and accurate expectations regarding big data. Their success will depend on the
ability to develop technical capabilities to integrate and analyze information using new
technologies and handling effective decision making through analytics.
A great revolution is in progress in healthcare sector, beginning with the massively
increased supply of information. Over the last years, healthcare organizations have
Fig. 2. Total deaths by all neoplasms, by years, by location, in men, in the selected Brazilian
states.
Big Data Applications in Cancer Research
473

been aggregating decades of clinical data into medical databases, while hospitals have
digitized their patient records.
Physicians have conventionally used essentially their knowledge when making
treatment decisions, however in the last few decades there has been a move toward
evidence-based medicine, which includes systematically revising clinical data and
making treatment decisions based on the accessible information.
Privacy remains an important issue. Even though applications can remove personal
information from electronic medical records, technicians and physicians must be
attentive for potential security threats. The Big data’s success in generating value in
healthcare sector possibly will involve changes in current polices related to the guar-
antee of patients’ conﬁdentiality. Current practices related to data access, privacy and
share has to be revised.
Big data applications have the potential to improve healthcare organizations.
Managers that are committed to innovation and open to a new vision of governance
will be the ﬁrst to harvest the beneﬁts of these tools and obtain competitive advantage.
The study of the Big Data application development at Brazilian National Cancer
Institute was important to different aspects. First, the steps presented in this project can
be used as a roadmap to be followed by healthcare organizations planning to implement
Big Data analytics projects besides encourage them for a rapid adoption of such
solutions.
Although this case study has limitations, it may act as a motivation for further
researches in the healthcare area. Big Data Applications in cancer research are still
relatively new which makes it difﬁcult to ﬁnd many academic references that provide
an overview of this innovative technology. The challenges of future researches are
related to issues of patients’ security, privacy and conﬁdentiality.
References
1. McAfee, A., Brynjolfsson, E.: Big data: the management revolution. Harvard Bus. Rev. 90,
60–66, 68 (2012)
2. Waller, M.A., Fawcett, S.E.: Data science, predictive analytics, and big data: a revolution
that will transform supply chain design and management. J. Bus. Logistics 34(2), 77–84
(2013)
3. Ward, J.S., Barker, A.: Undeﬁned By Data: A Survey of Big Data Deﬁnitions. arXiv:1309.
5821 (2013)
4. Leventhal, R.: Trend: big data. Big data analytics: from volume to value. Healthcare Inform.
30(2), 12 (2013)
5. Kiron, D., Shockley, R.: Creating business value with analytics. MIT Sloan Manage. Rev. 53
(1), 57–63 (2011)
6. Bizer, C., Boncz, P., Brodie, M.L., Erling, O.: The meaningful use of big data: four
perspectives – four challenges. SIGMOD Record 40(4), 56–60 (2011)
7. Toh, S., Platt, R.: Is size the next big thing in epidemiology? Epidemiology 24, 349–351
(2013)
8. Miele, S., Shockley, R.: Analytics: the real-world use of big data. IBM Institute for Business
Value
(2013).
http://public.dhe.ibm.com/common/ssi/ecm/en/gbe03550usen/GBE03550
USEN.PDF
474
A. A. Gonçalves et al.

9. Tang, P.C., Ash, J.S., Bates, D.W., Overhage, J.M., Sands, D.Z.: Personal health records:
deﬁnitions, beneﬁts, and strategies for overcoming barriers to adoption. J. Am. Med. Inform.
Assoc. 13, 121–126 (2006)
10. Wiesner, M., Pfeifer, D.: Health recommender systems: concepts requirements, technical
basics and challenges. Int. J. Environ. Res. Public Health 11(3), 2580–2607 (2014)
11. Huang, T., Lan, L., Fang, X., An, P., Mim, H.J., Wang, F.: Promises and challenges of big
data computing in health sciences. Big Data Res. 2, 2–11 (2015)
12. Giniat, E.: Using business intelligence for competitive advantage. Healthcare Finan.
Manage. 65(9), 142, 144, 146 (2011)
13. Chawla, N.V., Davis, D.A.: Bringing big data to personalized healthcare: a patient-centered
framework. Journal of General Internal Medicine 28, 660–665 (2013)
14. Madden, S.: From databases to big data. IEEE Internet Comput. 16(3), 4–6 (2012)
15. Biesdorf, S., Court, D., Willmott, P.: Big data: what’s your plan. McKinsey Q. 2, 40–51
(2013)
16. Marchand, D.E., Peppard, J.: Why IT fumbles analytics. Harvard Bus. Rev. 91(1–2), 1–9
(2013)
17. Ferguson, R.B.: The big deal about a big data culture and innovation. MIT Sloan Manage.
Rev. 1–4, 2012
18. Yin, R.K.: Case Study Research: Design and Methods, 15th edn. Sage, London (2014)
19. Bose, R.: Knowledge management-enabled health care management systems: capabilities,
infrastructure, and decision-support. Expert Syst. Appl. 24, 59–71 (2003)
Big Data Applications in Cancer Research
475

New Diagnostic Tool for Patients Suffering
from Noncommunicable Diseases (NCDs)
Wojciech Oleksy(&), Zbigniew Budzianowski, Ewaryst Tkacz,
and Małgorzata Garbacik
Department of Biosensors and Processing of Biomedical Signals,
Faculty of Biomedical Engineering, Silesian University of Technology,
Gliwice, Poland
{wojciech.oleksy,zbigniew.budzianowski,ewaryst.tkacz,
malgorzata.garbacik}@polsl.pl
Abstract. Electrocardiography, technique, which is an essential tool in the
diagnosis of heart disease, as well as other organs, is used by doctors for over
100 years. It is used to measure electrical activity of the heart as a function of
time and present it in digital or analogue form. Whilst the standard 12 lead ECG
is the basic clinical method of heart diagnosis it has its drawbacks. Measuring all
12 leads is often difﬁcult and impractical, most of all it restricts patient move-
ment. In 1988, Gordon Dower developed a system of quasi-orthogonal lead
called EASI, which uses only 5 electrodes in order to register standard 12 lead
ECG signals. The main goal of this work is to present a new tool using machine
learning algorithms which transforms electrocardiographic signals (ECG) per-
formed by EASI into a standard 12-channel ECG, which therefore could be an
ideal tool for diagnose of NCDs.
Keywords: EASI  ECG  Regression  Machine learning
1
Introduction
In Europe noncommunicable diseases (NCDs) are responsible for the largest share of
mortality: about 80% of deaths in 2009. Among all of them, diseases of the circulatory
system caused nearly 50% of all of deaths. This number is higher among men and it
ranges in different countries from less than 30% to more than 65% of all deaths. Those
numbers are greater in less wealthy countries, where the social awareness of the risks is
limited and access to qualiﬁed medical personnel is limited. Just for a comparison,
cancer is responsible for 20% of deaths, ranging from around 5% to more than 30% in
some countries [1]. That is why it is so important to ﬁnd new ways to increase the
availability of diagnostic methods which will assist in the detection of cardiovascular
disease. We decided to develop a device, which with help of appropriate algorithms, as
well as telemedicine, will allow more accurate or even automatic and remote diagnosis
of the patient. In 1988 Dower and his team introduced EASI ECG system, which
derives standard 12 lead ECG using only 5 electrodes [2]. The E electrode is on the
sternum while, the A and I electrodes are at the left and right mid-auxiliary lines,
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_45

respectively. The S electrode is at the sternal manubrium. The ﬁfth electrode is a
ground and is typically placed on one or the other clavicle, see Fig. 1. EASI was
proven to have high correlation with standard 12 lead ECG, as well as with
Mason-Likar 12-Lead ECG. Apart from that it is less susceptible to artefacts, it increase
mobility of patients, it is easier and faster to use because of smaller number of elec-
trodes. What is more, smaller number of electrodes reduces cost of a device [3]. The
electrodes are positioned over readily identiﬁed landmarks which can be located with
minimal variability, independent of the patient’s physique, assuring high repeatability.
The electrode placement make the chest largely unencumbered, allowing physical or
imaging examination of the heart and lungs without removing the electrodes.
2
Theoretical Background of the EASI Method
In the so called classical approach introduced by Dower, using the EASI lead con-
ﬁguration, 3 modiﬁed vectorcardiographic signals are recorded from the following
bipolar electrode pairs:
• A-I (primarily X, or horizontal vector component)
• E-S (primarily Y, or vertical vector component)
• A-S (containing X, Y, Z, the anteriorposterior component)
Fig. 1. Lead placement for EASI method.
New Diagnostic Tool for Patients Suffering from NCDs
477

Each of the 12 ECG leads is derived as a weighted linear sum of these 3 base
signals using the following formula:
L derive ¼ a  AI
ð
Þ þ b  ES
ð
Þ þ c  AS
ð
Þ
ð1Þ
where L represents any surface ECG lead and a, b, and c represent empirical coefﬁ-
cients. These coefﬁcients, developed by Dower, are positive or negative values,
accurate to 3 decimal places, which result in leads very similar to standard leads. We
decided to improve EASI ECG performance by ﬁnding new model used for 12 ECG
leads calculation. We treated the system as a black box with 4 input variables: E, A, S, I
and 12 output variables: I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6 and we
used different regression and machine learning techniques to build a model.
3
Methods Used
Our work was focused on improving Dower model by using some regression and
machine learning techniques. Several different methods were tested to ﬁnd a best ﬁtting
model, namely LARS method, Lasso method, Forward Stagewise method [4], Linear
Regression - Least Squares method [5], Least Median of Squares method [6],
Regression Pace [7], Bagging Predictors method [8], Gradient Boosting method [9],
Artiﬁcial Neural Networks - Multilayer Perceptron method [10–12] and Support Vector
Machine method [13]. Some of them produced a linear model, other created a nonlinear
one. Both models, linear and nonlinear, have different advantages and disadvantages.
Linear models, so in other word models described by set of linear equations, usually are
easier to understand and build, their total error could be easily estimated. However they
are less effective in most cases. Example of such a model is given below:
aVF ¼ 0:214  E þ 0:114  A  1:093  S þ 0:728  I  3:068
ð2Þ
aVL ¼ 0:129  E þ 0:598  S  1:680  I þ 2:304
ð3Þ
aVR ¼ 0:084  E  0:119  A þ 0:492  S þ 0:940  I þ 0:781
ð4Þ
I ¼ 0:030  E þ 0:083  A þ 0:071  S  1:740  I þ 1:004
ð5Þ
II ¼ 0:199  E þ 0:156  A  1:057  S  0:141  I  2:566
ð6Þ
III ¼ 0:229E þ 0:073A  1:129S þ 1:598I  3:570
ð7Þ
V1 ¼ 0:634E þ 0:079A þ 0:501S þ 0:493I þ 4:038
ð8Þ
V2 ¼ 1:083E  0:095A þ 0:525S  1:249I þ 13:663
ð9Þ
V3 ¼ 0:799E þ 0:280A þ 0:088S  2:311I þ 5:057
ð10Þ
478
W. Oleksy et al.

V4 ¼ 0:368E þ 1:234A þ 0:086S  1:187I  2:241
ð11Þ
V5 ¼ 0:138E þ 1:557A þ 0:086S þ 0:361I þ 0:024
ð12Þ
V6 ¼ 0:036E þ 1:255A  0:146S þ 0:706I  1:235
ð13Þ
Nonlinear models tend to be more effective, but harder to understand and build.
4
Obtained Results
Every method tested produced thousands of different models. To determine perfor-
mance of all models, for each of them correlation coefﬁcient, root mean squared error
and mean absolute error was calculated. Each model calculation was 10 fold cross
validated. All results are based on data from PhysioNet [14] database and also on
synthetic data generated using ECG->EASI model described by following set of
equations:
E ¼ 0:228  II  0:157  aVR þ 1:402  V1  0:231  V2 þ 0:638  V3
 0:310  V4  0:525  V5 þ 0:745  V6  6:911
ð14Þ
A ¼ 0:116  I þ 0:146  aVL  0:078  V1 þ 0:039  V3 þ 0:146  V5
þ 0:532  V6  0:152
ð15Þ
S ¼ 0:407  II þ 0:290  aVR  0:239  aVF  0:096  V1 þ 0:360  V2
 0:326  V3 þ 0:252  V4 þ 0:046  V5  0:131  V6 þ 3:263
ð16Þ
I ¼ 0:107  I þ 0:113  III þ 0:102  aVR  0:121  aVL  0:154  V2
þ 0:264  V3  0:171  V4 þ 0:036  V5  0:108  V6 þ 0:441
ð17Þ
Procedure of ﬁnding this model is presented in the paper Investigation Of A
Transfer Function Between Standard 12-Lead ECG And EASI ECG [15]. Calculated
models were compared with results obtained using classical Dower approach and also
with Improved EASI Coefﬁcients described in the paper “Improved EASI Coefﬁcients:
Their Derivation, Values, and Performance” [16]. From all linear and nonlinear models
obtained we have chosen one best linear and one best nonlinear for further analysis.
The best results obtained for each model are presented in Table 1 (correlation coefﬁ-
cient), Table 2 (root mean squared error) and Table 3 (mean absolute error). Each of
the obtained result was proven to be statistically signiﬁcant by Student’s t-test (with
P < 0.01).
New Diagnostic Tool for Patients Suffering from NCDs
479

Table 1. Correlation coefﬁcient.
Linear model Nonlinear model Dower’s model Field’s model
aVF 0.939
0.970
0.984
0.776
aVL 0.966
0.981
0.955
0.922
aVR 0.984
0.987
0.985
0.966
I
0.985
0.990
0.971
0.973
II
0.964
0.977
0.994
0.894
III
0.941
0.973
0.963
0.786
V1
0.990
0.996
0.882
0.849
V2
0.984
0.994
0.968
0.872
V3
0.975
0.986
0.971
0.751
V4
0.971
0.983
0.981
0.851
V5
0.992
0.995
0.977
0.970
V6
0.997
0.999
0.888
0.985
Table 2. Root mean squared error [mV].
Linear model Nonlinear model Dower’s model Field’s model
aVF 27.31
18.32
28.41
66.29
aVL 21.81
16.63
35.45
34.22
aVR 15.95
14.70
31.86
55.30
I
17.83
14.99
40.75
42.47
II
26.03
21.13
32.57
78.20
III
31.19
21.55
37.19
60.03
V1
20.83
13.78
99.24
86.42
V2
40.18
25.24
177.75
119.61
V3
46.24
34.58
120.25
141.69
V4
55.23
43.04
144.60
129.96
V5
24.55
19.49
119.93
49.90
V6
10.63
7.41
93.17
33.10
Table 3. Mean absolute error [mV].
Linear model Nonlinear model Dower’s model Field’s model
aVF 18.10
14.99
20.33
28.16
aVL 15.20
12.45
16.38
19.06
aVR 11.64
11.20
20.93
19.48
I
12.72
11.37
22.32
17.01
II
17.53
15.91
24.74
29.72
III
21.09
16.71
20.63
29.16
(continued)
480
W. Oleksy et al.

5
Hardware Implementation
Currently a work is being done on hardware implementation of obtained models. Few
hardware conﬁgurations are going to be created. First there will be a simple device with
a linear model implemented using operational ampliﬁers, transmitting ECG signals via
Bluetooth, see Fig. 3. Second device will be a more complex solution, which includes
microprocessor, with a nonlinear model implemented, transmitting ECG signal via
WiFi and Bluetooth.
Table 3. (continued)
Linear model Nonlinear model Dower’s model Field’s model
V1
10.94
10.70
45.90
37.05
V2
27.19
18.64
82.90
52.51
V3
28.18
23.42
52.76
47.57
V4
34.89
29.35
53.24
44.94
V5
14.17
13.07
43.92
21.37
V6
6.86
5.56
35.14
13.68
Sample plot obtained for some models is shown in Fig. 2.
Fig. 2. V1 lead sample plot.
New Diagnostic Tool for Patients Suffering from NCDs
481

6
Conclusions
Above results show that the best performance was obtained for the nonlinear model.
Second best model was linear model. This was the one obtained by Dower. Surpris-
ingly low performance was observed for model that uses improved EASI coefﬁcients.
Because linear model was almost as good as nonlinear one it should be considered as a
ﬁrst choice in case of hardware implementation of the device. Further work in the topic
of improving EASI ECG coefﬁcient using various regression and machine learning
techniques should be continued.
References
1. WHO Europe – The European Health Report 2012: Charting The Way To Well-Being.
http://www.euro.who.int/en/publications/abstracts/european-health-report-2012
2. Patent at http://www.google.com/patents/US4850370/
3. Redley, B.: EASI ECG monitoring vs traditional 12-lead ECG. A Review of the Literature
(2005)
4. Efron, B., Johnstone, I., Robert, T.: Least angle regression. Ann. Stat. 32, 407–499 (2004)
5. Hastie, T., Tibshirani, R., Jerome, F.: The elements of statistical learning. Data Mining,
Inference, and Prediction. Springer, New York (2013)
6. Rousseeuw, P.J.: Least median of squares regression. J. Am. Stat. Assoc. 79(388), 871–880
(1984)
7. Wang, Y., Witten, I.H.: Pace Regression Working Paper Series (1999)
Fig. 3. Prototype of the device using obtained linear model.
482
W. Oleksy et al.

8. Leo, B.: Bagging predictors. Mach. Learn. 24, 123–140 (1996)
9. Friedman, J.H.: Stochastic gradient boosting. Comput. Stat. Data Anal. 38(4), 367–378
(2002)
10. Haykin, S.: Neural Networks: A Comprehensive Foundation. Prentice Hall, Upper Saddle
River (1998)
11. Bishop, C.M.: Neural Networks for Pattern Recognition. Oxford University Press,
New York (1995)
12. Frank, E., Witten, I.H.: Data Mining. Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, Massachusetts (2005)
13. Smola, A.J., Schlkopf, B.: A tutorial on support vector regression. Stat. Comput. 14, 199–
222 (2004)
14. PhysioNet data at http://www.physionet.org/challenge/2007/data/
15. Oleksy, W., Tkacz, E.: Investigation of a transfer function between standard 12-Lead ECG
And EASI ECG. Anal. Biomed. Signals Images 20, 322–327 (2010)
16. Feild, D.Q., Feldman, C.L., Milan, H.B.: Improved EASI coefﬁcients: their derivation
values, and performance. J. Electrocardiol. 35, 22–33 (2002)
New Diagnostic Tool for Patients Suffering from NCDs
483

Detection of Genetic Aberrations in Cancer
Driving Signaling Pathways Based on Joint
Analysis of Heterogeneous Genomics Data
Roman Jaksik(&) and Krzysztof Fujarewicz
Institute of Automatic Control,
Silesian University of Technology, Gliwice, Poland
{roman.jaksik,krzysztof.fujarewicz}@polsl.pl
Abstract. Cancer results from genetic aberrations that affect multiple intra-
cellular processes, combined with evolution and clonal selection that give cancer
a signiﬁcant advantage over heathy cells. Identiﬁcation of genes and processes
that function improperly in cancer cells is a signiﬁcant challenge, necessary for
the proper understanding of cancerogenesis and for the development of suc-
cessful treatment scenarios. This paper shows the advantages of utilizing data
provided by various methods to complement the knowledge about alterations in
regulatory processes associated with cancer. Using four different experiment
types that focus on mutations and indels, gene expression, copy number vari-
ation and methylation, used to study the genome of over 2000 patients treated
for breast, thyroid and prostate cancer, we test some of the assumptions used in
cancer research associated with coverage and mutual exclusivity of alterations.
We show that individual methods do not allow to observe alterations in all
cancer related processes and that the exclusivity assumption is valid only for
individual alteration types. We additionally show the relationship between the
violation of those assumptions and clinical data, at the level of individual
patients, providing a comprehensive description of the analysis strategy used
and its possible impact on the interpretation of data.
Keywords: Signaling pathways  Sequencing  Methylation  Gene expression
Copy-number alteration  Mutual exclusivity
1
Introduction
A major problem in oncology research is that each tumor is different on a molecular
level, although they often show considerable similarity in development and treatment
response (Kandoth et al. 2013). Cancers is caused by disorders in several speciﬁc
processes, but the disorders themselves may have a different background. For example,
the process of apoptosis (suicide of a cell triggered when the DNA contains too much
damages) can be impaired by mutations in several different genes, changes in the copy
number of genes, changes in the DNA methylation proﬁle, or in the number of com-
pletely different proteins, linked with them through regulatory interactions.
In an article by Douglas Hanahan “The Hallmarks of Cancer” (Hanahan and Wein-
berg 2000) later updated in (Hanahan and Weinberg 2011), the processes required for the
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_46

development of cancer are summarized. Not one but all of them are believed to be required
for the malignant cancer to emerge. Those processes include: sustaining proliferative
signaling, evading growth suppressors, enabling replicative immortality, activating
invasion and metastasis, inducing angiogenesis, resisting cell death, genome instability
and mutations, avoiding immune destruction, tumor-promoting inﬂammation, deregu-
lating cellular energetics. The role and impact of those processes on cancerogenesis is still
under debate (Fouad and Aanei 2017; Sonnenschein and Soto 2013), however there is a
general agreement that cancer is not a result of a single mutation or other types of
alteration on the DNA level, rather than a set of various genetic and epigenetic alterations
or chromosomal aberrations combined with evolution and clonal selection.
In order to identify genes directly responsible for the disorders in those processes,
different types of studies are being conducted that focus on speciﬁc genomic mecha-
nisms. Most common study types include: detection of somatic mutations, based on
whole exome or whole genome sequencing (Kandoth et al. 2013); study of changes in
mRNA and miRNA expression levels (Jacobsen et al. 2013); detection of copy number
variations (CNV) (Shlien and Malkin 2009) and study of methylation levels to identify
hypermetylated genomic regions (Kulis and Esteller 2010). The complexity of the
methods and large amount of data which they produce makes it difﬁcult to combine the
results obtained using various experiment types, based on existing annotation and
interaction databases, in order to identify all types of alterations in speciﬁc cancer-related
processes. Current methods used to identify cancer driving genes are based on the
observation that mutations in the same pathway are often mutually exclusive (Yeang
et al. 2008), which results from another observation that cancers result from a small
number of driver mutations, distributed among various genes and signaling pathways.
The main goal of this study is to (1) Determine whether it is possible to observe
alterations in genes directly associated with all cancer related processes using knowledge
provided by various experiment types (2) Identify whether some of the alteration types
are observed more frequently among certain processes (3) determine which method is
able to identify the largest amount of alterations that can contribute to cancer (4) Test
mutual exclusivity of alterations in identiﬁed pathways using data from various study
types (5) Test whether patients with missing hallmark alterations show different phe-
notype associated features e.g. survivability after treatment. The study will be conducted
on a large number of samples representing thyroid, breast and prostate cancers.
2
Materials and Methods
2.1
Data Acquisition
Data obtained using four various methods were downloaded from The Cancer Genome
Atlas (TCGA) database for over 2000 patients diagnosed with either thyroid, breast or
prostate cancer. For each patient we downloaded gene expression, methylation, copy
number variation and mutation data. Each data type was converted to a binary table
where columns represent individual samples while rows gene-associated features. If the
feature was considered different in cancer sample from the expected value calculated
for normal cells it was than marked with a number one in the table. Additionally, for
Detection of Genetic Aberrations
485

each patient we obtained clinical data including age at diagnosis, tumor grade and
survival time since diagnosis.
2.2
Gene Expression
Transcript levels were obtained using RNA-seq by mapping reads to known human
transcripts from the ENSEMBL database. For each patient upper quantile normalized
FPKM (Fragments Per Kilobase of Exon Per Million Fragments Mapped) statistics
were obtained and combined into a single table ﬁle. Transcript was assumed to be
differentially expressed for a given patient if the results for cancer speciﬁc sample was
different from the average of control sample levels by at least two standard deviations.
Multiple transcripts per gene were uniﬁed for each patient by taking a logical OR from
all zero or one values (0 – non differentially expressed, 1 – differentially expressed).
2.3
Methylation
Methylation data were obtained using Illumina Inﬁnium HumanMethylation450 or
HumanMethylation27 BeadChips. Only beta values that represent methylation level in
a 0–1 scale and symbols of genes that are associated with those levels were extracted
from each patient-speciﬁc ﬁle. Cancer cells were assumed to be differentially methy-
lated for a given patient if they were different from the average of control sample levels
by at least two standard deviations. Results obtained from multiple probes associated
with a single gene were uniﬁed for each patient using the same strategy as in the gene
expression data.
2.4
Copy Number Variation
Copy number variation data was obtained using Affymetrix SNP6.0 microarrays. For
each patient genome coordinates and segment mean values were extracted, which
represent copy number gain or loss events. Copy numbers were considered signiﬁcantly
different if the segment mean was outside of the (0.5, –0.5) range. Regions were
associated
with
genes
using
exon
coordinates
extracted
from
the
RefSeq
(GCF_000001405.30) transcript database, partial overlaps of the regions were accepted.
2.5
Mutations
Mutations and indels (insertions and deletions) were obtained from whole exome
sequencing experiments and annotated using Variant Effect Predictor (McLaren et al.
2016). For each variant we extracted its location in the genome and symbol of gene
inside of which the variant is located. Only missense variants that result in alterations of
the protein structure were extracted for each patient from the obtained data.
2.6
Cancer Hallmarks
Database that links cancer hallmarks to human genes originates from (Knijnenburg
et al. 2015) which links Gene Ontology (Ashburner et al. 2000) and Pathway
486
R. Jaksik and K. Fujarewicz

Interaction Database (Schaefer et al. 2009) to create the relations for a total of 999
unique genes and 10 various hallmarks listed in Table 1. Some of the genes can be
included in multiple hallmarks therefore the total number of connections is 2016.
3
Results
3.1
Mutations and Indels in Cancer-Related Processes
Most of cancer studies focuses on the cancer genome alterations on the nucleotide
sequence level, looking for mutation and indels that can be found in cancer genomes
but not in the heathy cells obtained from the same patients (somatic changes). However
while looking for common features among the analyzed patients it is very rare to ﬁnd
identical mutations and even the overlap between mutated genes is relatively low and
rarely exceeds 50% of samples for a single gene. It is more important look for alter-
ations that can affect a speciﬁc process, rather than individual genes which take part in
it which brings the analysis to a higher level. However even after doing so it is very
difﬁcult to ﬁnd mutations that can affect each of the processes required by the cancer to
emerge. Figure 1 shows genomic alterations (mutations and indels) in at least one of
the genes associated with a speciﬁc process. Columns represent individual patients
divided into various cancer types. The ﬁrst conclusion that can be drawn is that despite
using information provided by the analysis of multiple genes is impossible to observe
alterations in all of the processes. However, it is worth noting that this study focuses on
genes that are directly responsible for those processes and do not take into account
genes which are connected to them through other regulatory interaction, affecting their
function. The plot also shows that there are substantial differences between the pro-
cesses and between cancer types most clearly visible in the case of prostate cancer
(PRAD) which shows the highest number of patients with unaltered processes. To test
whether differences between cancer types are signiﬁcant calculated 95% conﬁdence
intervals for the proportions of samples that have at least one damage in a particular
process (Fig. 2).
Table 1. Number of genes for each hallmark.
Hallmark
# Genes
Sustained angiogenesis
105
Tumor-promoting inﬂammation
84
Genome instability
124
Sustaining proliferative signaling
343
Evading immune destruction
150
Replicative immortality
2
Resisting cell death
404
Evading growth suppressors
182
Reprogramming energy metabolism
8
Tissue invasion and metastasis
614
Detection of Genetic Aberrations
487

Fig. 1. Relationship between individual samples (columns) and hallmarks (rows). Blue color
marks the existence of mutations and indels in at least one of the genes that takes part in a
particular process. BRCA, PRAD and THCA are breast, prostate and thyroid cancers
respectively.
Fig. 2. Percentage of samples with mutations and indels in at least one gene that belong to a
particular process. Whiskers represent 95% conﬁdence intervals for proportions.
488
R. Jaksik and K. Fujarewicz

In all cases breast cancer shows the highest number of samples that show mutations
in a particular process. This might be related to the fact that also the percentage of
samples with alterations in genes that can result in genome instability (mainly DNA
repair mechanisms) is the highest. In all cases both of the studies show that in all cancer
types the number of alterations in cancer hallmarks that can be explained by mutations
alone is relatively small, increasing the need to include data from other sources.
3.2
Genomic Alterations Observed in Various Experiment Types
The goal of this study is to determine which method is able to identify the largest
number of alterations associated with the cancer related processes. The methods studied
are based on the analysis of distinct features of the DNA therefore the difference
doesn’t result from variations in the levels of sensitivity and speciﬁcity but from their
basic assumptions and number of features that can be detected using them. The total
number of genes that are tested using each method varies, for example methylation
study is based on microarray probes designed only for speciﬁc DNA regions (CpG
islands) which are not associated with all available genes. However in this study we use
only a speciﬁc subset of genes which are were tested using all methods.
Figure 3 shows the percentage of samples in which at least one alteration was
found, associated with a speciﬁc cancer-related process, separately for each of the
experiment types. In general the largest number of alterations was found using gene
expression study which can be expected since alterations on the methylation level, copy
number variation (CNV) and to a lesser extend mutations all have impact on the gene
expression levels and not the other way around. It would seem reasonable to focus
predominantly on this method in the cancer research however changes on the gene
expression level can be considered only as end effect without the knowledge about its
cause, which in many cases is the basic question in cancer studies.
Fig. 3. Fraction of samples in which at least one cancer process-related alteration was found,
separately for each of the experiment methods.
Detection of Genetic Aberrations
489

Among the remaining methods copy number variation shows the largest number of
changes. Despite the fact that the number of such events is relatively small comparing
to mutations and methylation level changes they affect large regions of the DNA (on
average 47 thousand base pairs) and in some cases can even include more than a half of
an entire chromosome affecting the expression yield of genes that are located in them.
The ﬁgure also shows that replicative immortality and reprogramming energy meta-
bolism are underrepresented in all of the methods. This is connected to the low number
of known genes that take part in those processes (Table 1) however indirect regulation
of them might still explain considerable percentage of samples in which gene
expression alterations were observed.
3.3
Mutual Exclusivity
One of the basic assumptions of some methods used to identify cancer driving genes
and pathways is high coverage and mutual exclusivity (Yeang et al. 2008). High
coverage means that most patients have at least one mutated gene in a particular
pathway associated with individual cancer-related process. High exclusivity means that
most patients will have only one mutated gene in that process. The latter assumption
results from the observation that only few cancer driving mutations a can be observed
in an individual therefore it is very unlikely that two will appear in the same process.
To validate the exclusivity assumption for cancer hallmarks we calculated the
percentage of samples with alterations detected using all methods, omitting gene
expression study since it is closely related to factors tested by other methods. Since we
would also like to compare the data between carious cancer types we analyzed them
separately and presented the outcomes as the fraction of samples in each individual
cancer. For this reason the results presented on Fig. 4 in a form of stacked bar plots
sum up to 300%.
Figure 4 shows that in almost all cases the assumption of exclusivity is not met
when focusing on multiple detection methods. Each method alone shows a signiﬁcantly
lower number of samples with two or more alterations in the same hallmark. This
observation indicates that hallmark affecting events are not as that rare when consid-
ering other sources of data, not only mutations.
3.4
Genotype Alterations vs Phenotype
The inability to detect alterations in speciﬁc processes may result from the method
limitations and assumptions made during data processing which affect sensitivity and
speciﬁcity of the methods used to detect alterations. Due to low sensitivity we might be
unable to detect some of the alterations or the alterations are not detectable at all with
all of the selected methodologies. However another possibility is that not all of the
processes are necessary for the development of those particular tumors and they are not
altered in all patients. This however should also affect the phenotype, like the tumor
grade or the survival rate of patients. To test that hypothesis we created survival curves
after dividing patients into two groups, based on the ability to detect alterations using a
speciﬁc method.
490
R. Jaksik and K. Fujarewicz

Fig. 4. Percentage of samples with alterations in at least one two genes that belongs to a
particular process. The plot is divided into separate tumors or which the percentages were
calculated separately. Since the bars are stacked the maximum value is 300%.
Fig. 5. Survival probability of patients with missing alterations, using two speciﬁc experiment
types that focus on the detection of genome damages. Left panel corresponds CNV where the
patients were divided into two groups, ﬁrst group (blue) shows damages in all cancer hallmarks,
in the second one (gray) damages in at least one hallmark were not detected. Right side represents
somatic mutation/indel discovery and is made in a similar manner, ﬁrst group (blue) shows
damages in at less than half of cancer hallmarks, the second one (gray) shows no damages in
more than half of all hallmarks
Detection of Genetic Aberrations
491

Figure 5 shows the survival curves for two experiment types: detection of copy
number variation events and somatic mutations or indels. In both cases higher survival
probability is associated with patients for which damages were not observed in all hall-
marks. We were unable to make the same observation for gene expression and methy-
lation data suggesting that they do not have such a signiﬁcant impact on the outcomes.
The survival observation suggests that the more hallmarks are altered the more
malignant should be the cancer which is very intuitive. To test that we made a similar
comparison using tumor stage information. We were able to show that patients for
which the joint analysis of mutation/indel, CNV and methylation data shows alterations
in at least 9 out of 10 hallmarks are associated with tumors of higher stage (larger
which have grown more deeply into nearby tissue and have the capability of spreading
into lymph nodes). The difference between both sample groups was shown to be
statistically signiﬁcant (p-value = 1.810−15) (Fig. 6).
Fig. 6. Percentage of samples with alterations on the level of mutations, CNV or methylation in
at least 9 out of 10 hallmarks versus those with less than 9 alterations. The higher the stage the
more malignant is the cancer. The difference between both groups was assessed with Pearson’s
Chi-squared test (p-value is shown on the plot).
492
R. Jaksik and K. Fujarewicz

4
Conclusions
Most of the cancer oriented studies focus on speciﬁc types of genetic alterations which
as we were able to show do not provide a comprehensive view on the cancer genome,
omitting crucial information that can aid the research. By focusing on individual
methods we were unable to observe the high coverage assumption of genetic alterations
according to which most patients have at least one mutated gene in a particular
pathway. However by combining the results from four different methods nearly all
samples met that assumption, however the mutual exclusivity assumption was violated
at the same time showing that it can apply to only one type of alteration e.g. mutations.
We were also able to show that the higher is the number of hallmarks in which we
observe changes the more malignant is the tumor which is in agreement with the cancer
hallmark theory. It is also worth noting that we did not focus on all available study
types, for example we did not take into account changes in the microRNA expression
levels that can regulate gene expression of changes on the chromatin conformation
level which makes gene more or less accessible to protein binging, required for their
activation. The incorporation of additional data and information about gene relations
could improve our understanding of the processes involved in tumorigenesis which in
turn could contribute to the development of more effective anticancer therapies.
Acknowledgments. This work was supported by the Polish National Centre for Research and
Development grant 2/267398/4/NCBR/2015 and an internal grant of the Silesian University of
Technology. Calculations were carried out using the computer cluster Ziemowit (http://www.
ziemowit.hpc.polsl.pl)
funded
by
the
Silesian
BIO-FARMA
project
No.
POIG.02.01.
00-00-166/08 in the Computational Biology and Bioinformatics Laboratory of the Biotechnology
Centre at the Silesian University of Technology.
References
Ashburner, M., Ball, C.A., Blake, J.A., Botstein, D., Butler, H., Cherry, J.M., Sherlock, G.: Gene
ontology: tool for the uniﬁcation of biology. The gene ontology consortium. Nat. Genet. 25
(1), 25–29 (2000). https://doi.org/10.1038/75556
Fouad, Y.A., Aanei, C.: Revisiting the hallmarks of cancer. Am. J. Cancer Res. 7(5), 1016–1036
(2017)
Hanahan, D., Weinberg, R.A.: The hallmarks of cancer. Cell 100(1), 57–70 (2000)
Hanahan, D., Weinberg, R.A.: Hallmarks of cancer: the next generation. Cell 144(5), 646–674
(2011). https://doi.org/10.1016/j.cell.2011.02.013
Jacobsen, A., Silber, J., Harinath, G., Huse, J.T., Schultz, N., Sander, C.: Analysis of
microRNA-target interactions across diverse cancer types. Nat. Struct. Mol. Biol. 20(11),
1325–1332 (2013). https://doi.org/10.1038/nsmb.2678
Kandoth, C., McLellan, M.D., Vandin, F., Ye, K., Niu, B., Lu, C., Ding, L.: Mutational
landscape and signiﬁcance across 12 major cancer types. Nature 502(7471), 333–339 (2013).
https://doi.org/10.1038/nature12634
Knijnenburg, T.A., Bismeijer, T., Wessels, L.F., Shmulevich, I.: A multilevel pan-cancer map
links gene mutations to cancer hallmarks. Chin. J. Cancer 34(10), 439–449 (2015). https://doi.
org/10.1186/s40880-015-0050-6
Detection of Genetic Aberrations
493

Kulis, M., Esteller, M.: DNA methylation and cancer. Adv. Genet. 70, 27–56 (2010). https://doi.
org/10.1016/B978-0-12-380866-0.60002-2
McLaren, W., Gil, L., Hunt, S.E., Riat, H.S., Ritchie, G.R., Thormann, A., Cunningham, F.: The
ensembl variant effect predictor. Genome Biol. 17(1), 122 (2016). https://doi.org/10.1186/
s13059-016-0974-4
Schaefer, C.F., Anthony, K., Krupa, S., Buchoff, J., Day, M., Hannay, T., Buetow, K.H.: PID:
the pathway interaction database. Nucleic Acids Res. 37(Database issue), D674–D679 (2009).
https://doi.org/10.1093/nar/gkn653
Shlien, A., Malkin, D.: Copy number variations and cancer. Genome Med. 1(6), 62 (2009).
https://doi.org/10.1186/gm62
Sonnenschein, C., Soto, A.M.: The aging of the 2000 and 2011 Hallmarks of Cancer reviews: a
critique. J. Biosci. 38(3), 651–663 (2013)
Yeang, C.H., McCormick, F., Levine, A.: Combinatorial patterns of somatic gene mutations in
cancer. FASEB J. 22(8), 2605–2622 (2008). https://doi.org/10.1096/fj.08-108985
494
R. Jaksik and K. Fujarewicz

Ethics, Computers and Security

Cookie Scout: An Analytic Model for Prevention
of Cross-Site Scripting (XSS)
Using a Cookie Classiﬁer
Germ´an Eduardo Rodr´ıguez1(B), Diego Eduardo Benavides1,2, Jenny Torres1,
Pamela Flores1, and Walter Fuertes1,2
1 Systems Engineering Faculty, Escuela Polit´ecnica Nacional, Quito, Ecuador
{german.rodriguez,jenny.torres,pamela.flores}@epn.edu.ec
2 Universidad de las Fuerzas Armadas ESPE, Quito, Ecuador
{debenavides,wmfuertes}@espe.edu.ec
Abstract. Cross-Site Scripting (XSS) attack is a vulnerability typical of
Web applications, where malicious scripts are injected into trusted web-
sites. It allows attackers to execute scripts in the victims browser which
can hijack user sessions, deface websites, steal cookies or redirect the user
to malicious sites. This paper presents Cookie Scout, an analytical model
for preventing XSS attacks, which main goal is to classify cookies accord-
ing to their parameters. For this purpose we collect, analyse and classify
the type of traﬃc in a botnet using the Browser Exploitation Frame-
work (Beef) tool for execute attacks and malicious code remotely in a
controlled testing environment. With the parameters obtained from the
traﬃc analysis, an algorithm was designed to detect suspicious websites
based on the reputation of their cookies. The results obtained showed
that the parameters of the cookies were a good reference to determine
malicious websites.
Keywords: XSS · Hook · Beef · JavaScript · Cookies
1
Introduction
Cross-Site Scripting (XSS) is an attack carried out directly on websites. Accord-
ing to OWASP [3], a XSS ﬂaws occur whenever an application takes untrusted
data and sends it to a web browser without proper validation. These malicious
codes can be hosted on the same web server, or can be inserted through com-
plementary attacks, which means that users can inadvertently navigate through
a compromised website. The main security issues resulting from an XSS attack
are the hijack of user sessions, deface websites, redirect the user to malicious
sites, capture of keystrokes, denial of service attacks, scans of corporate net-
works, phishing attacks, or the geolocation of a user. Nevertheless, the objective
of this attack is the theft of cookies stored on the victim computer, since this
information can let the attacker to extract conﬁdential data from the user.
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_47

498
G. E. Rodr´ıguez et al.
In this scenario, the existing security solutions found in the literature are
not enough to put oﬀXSS attacks [1,2,4,5,7–10]. These solutions are classiﬁed
depending on where the control takes place: client-side, server-side, or hybrid
solutions; most of them based on software since hardware solutions require addi-
tional infrastructure. Nonetheless, these solutions have diﬀerent limitations, for
instance the need to know the programming mode of the web page, which rep-
resents a decrease in the performance of the browser; the manual deﬁnition of
blacklists or whitelists; restriction to only certain browsers; among others. Based
on the described scenario, this study propose Cookie Scout, an analytic model
to prevent XSS attacks, based on the values stored in cookie’s parameter at
the client’s computer. The main contribution of this research is to analyse the
behaviour of cookies, through their parameters, and how they could be blocked
by an algorithm that could be developed in any programming language. Our
research consists of a ﬁrst part, where the data is collected through a controlled
attack. Additionally, to perform penetration tests on web pages, the Browser
Exploitation Framework (Beef) is used. In the second part of the research, the
exchanged traﬃc is analysed.
The rest of the article proceeds as follows. Section 2 presents an overview of
the related work. Section 3 details the Cookie Scout model. It consists on the
data collection and data analysis, in order to select the parameters later used
in the modelling of the classiﬁcation algorithm. Section 4 analyses the results.
Finally, Sect. 5 presents the conclusions of this approach and the description of
future work.
2
Related Work
This section gives an overview of existing security solutions for an XSS attack.
These solutions are classiﬁed depending on where the control takes place [6]:
client-side [4,8], server-side [2], and hybrid solutions [1,5,7,9,10]. Among client-
side solutions, we have XSS Guard [4], which learns from the code which the
web application was created. This applies to any HTML request. The problem
with this solution is that, all forms of programming in the browser must be
considered, since it detects and processes the scripts of the web pages. Another
solution is noXSS [2], which contains a complete JavaScript parser. This, being
a suite with high reliability, oﬀers greater security in the navigation, but reduces
the performance of the browser. Diﬀerent techniques like that one shown in [8]
are also used, where a dynamic cookie rewrite is proposed. Through a web proxy,
cookies that are exchanged between users and web applications are automatically
rewritten. This causes client-side cookies to be overridden, and XSS attacks are
eliminated when criminals attempt to use stolen cookies. In the case of server-
side solutions, the most common are the ﬁrewalls [2]. At the application level,
these mitigate XSS attacks, preventing the attacker’s script from sending victim
identiﬁcation data. Firewalls control the ﬂow of sensitive information within the
website, blocking the diﬀerent suspicious domains. To complement its operation,
there are systems that block code-injection attacks, however, they only succeed

An Analytic Model for Prevention of Cross-Site Scripting (XSS)
499
in the reﬂected XSS attacks, but not in persistent XSS attacks. One disadvantage
of this type of ﬁlters is the low performance.
In the hybrid solutions approach, server policies are highlighted. These poli-
cies are incorporated in the web pages in order to determine what types of
scripts can be run. For example, BEEP [9] is a defense software that avoids the
injection of scripts. Another solution is Noncespaces [5], which randomizes the
preﬁxes of the XML tags in the web application, before a document is delivered
to the clients. NoScript [1] acts as an add-on for the Mozilla Firefox browser,
which serves to protect it from the client side. It acts as an extra protection,
because only allows the execution of JavaScript, Java and other plug-ins on
trusted websites that the user chooses. In [7], a proprietary tool called Noxes is
proposed. This tool is an XSS mitigation module which goal is to signiﬁcantly
reduce alarms and connection alerts, giving protection to sensitive data such as
cookies or session identiﬁers.
Table 1. Main features in XSS security solutions
Solutions
HW SW Limitations
Cookies classiﬁer
XSS Guard [4]
No
Yes Know the programming mode
No
Dynamic rewrite [8] Yes Yes Use of infrastructure
No
noXSS [2]
No
Yes Low browser performance
No
WAF [2]
Yes No Deﬁne rules and criteria to block sites No
BEEP [9]
No
Yes ——-
No
Noncespaces [5]
No
Yes ——-
No
NoScript [1]
No
Yes Deﬁnition of black/whitelists
No
Noxes [7]
No
Yes ——-
No
Authentication [10] No
Yes Use of resources for authentication
No
As shown in Table 1, we analyse of the main features in XSS security solutions
found in literature, including their limitations. Among the parameters analysed,
we have papers that used a solution with hardware requirements, it means servers
for the implementation of the proposal; and papers that used a software solution,
it means implemented in the browser itself, or as control agents to prevent XSS
attacks. As mentioned in [8], a proxy server provides a centralized analysis of
websites in the same way of a web application ﬁrewall [2], but requires main-
tenance in terms of server updates, security or hardware/software maintenance.
For software solutions, it is proposed to use add-ons, codes, agents, or rules in
the form of hand-set lists. Nevertheless, this removes its functionality since they
are proposals that only serve for certain browsers.
In the literature, no solutions were found to prevent XSS attacks using anal-
ysis of the inherent properties of cookies. Our proposal focuses on classifying
the types of cookies created by the websites visited. This proposal represents a
novel solution, since it would not be blocked at the level of web pages, but at

500
G. E. Rodr´ıguez et al.
the level of their corresponding session ﬁles. In addition, it oﬀers compatibility
for all browsers, because cookies are created independently of them.
3
Cookie Scout: An Analytic Model for Prevention
of Cross-Site Scripting (XSS)
Cookie Scout is based on the analysis of the cookies that are created when a
user visits diﬀerent websites. It deﬁnes a new approach for analyzing these sites
and avoiding XSS attacks, reducing the likelihood of a user visiting compromised
websites. The model will prevent the storage of suspicious cookies, the theft of
personal information; or the hijacking of these session ﬁles. This section has been
structured in three parts (Fig. 1), the data collection, the data analysis and the
analytical model (algorithm).
Fig. 1. Cookie Scout: an analytic model for prevention of Cross-Site Scripting
3.1
Data Collection
For the data collection two scenarios were proposed for executing the XSS attack:
(a) within a domain infrastructure, and (b) in a controlled infrastructure. For
both cases, the Beef framework is used, which commits the victims and analyses
the traﬃc they exchange with the attacker. The goal is to analyse the number
of packets sent and received, the ports used, and the behaviour of cookies. This
ﬁrst part deﬁnes the variables that will be used later in the algorithm.
Execution of the XSS attack
The successful implementation of the attack was using social engineering. A link
was conﬁgured, so that the home page of client’s browsers changed to a modiﬁed
web page with information that allow the execution of the attack. The hook was
a JavaScript code named hook.js which was hosted in the same framework.

An Analytic Model for Prevention of Cross-Site Scripting (XSS)
501
When it is executed, it calls the server where Beef is running, and begins to send
information about the victim. Once the victim computer was compromised, it
was possible to execute remote commands and additional modules against the
victims, among others, the automatic play of a sound and a pop-up asking the
need to update Adobe Flash Player plug-in. When the client clicks the web page,
it commits itself and all the machine is recorded in a tree of hooked browsers.
The attack was performed under two scenarios, in a domain infrastructure
and a controlled infrastructure. In the ﬁrst scenario a domain management server
was implemented to modify the rules that set the default homepage in the
browser. In general, the most used domain server is based on Windows Server
operating system. In the Group Policy Management settings, important URLs
can be set. In the second scenario, diﬀerent virtual machines were conﬁgured
in Windows and Linux. This scenario allowed to ﬁnd patterns in the traﬃc
that exchanged the victims and the attacking machine. In addition, a client
was used with Android operating system. The hook occurs when a user clicks
a button or link on the modiﬁed web page, and thus, the computer gets into
a zombie status and is subsequently monitored on-line by the framework. For
this test, the bait website was hosted on the attacking machine (server) in the
link: 192.168.X.A:3000/demos/basic.html. In the ﬁrst scenario, users who did
not have basic security knowledge relied on the home page of their browsers
without realizing it was modiﬁed. Wireshark and Tshark were used to collect
the exchanged traﬃc. The ﬁrst one allowed to analyse in a visual form all the
data obtained on Windows clients and the second one for the collection of text
on Linux clients. The information obtained from the cookie created by the Beef
framework is shown in Table 2.
Table 2. Cookie information
Parameter
Description
Name
BEEFHOOK
Content
ﬀMLBvDUSJufXhcihMv
Domain
192.168.10.X
Path
/
Creation date
June 2017
Expiration date
December 2030
Script accessibility Yes
Certiﬁcate
No
After performing the attack, we veriﬁed that antivirus like McAfee, Avast,
Nod32 and Norton Security, did not detect the action that produced the modiﬁed
home page in the test; no warning or alarm was generated, which proves that it
goes unnoticed. As a result, 100% of users established connection with the Beef
framework.

502
G. E. Rodr´ıguez et al.
3.2
Data Analysis
After collecting the data in the network, we can analyse:
Traﬃc between the victims and the attacker: in Table 3, the number of packets
exchanged were analysed. It was observed that this traﬃc was always higher in
comparison with the other websites visited.
Table 3. Summary of packages sent/received in the attack (A) towards each victim (V)
Victim
Total
A →V V →A
Victim 1
187
100
87
Victim 2 1024
757
447
Victim 3 2076
1043
1033
Victim 4 1098
1075
833
Used ports: as seen in Table 4, during the attack there was a variation in
ports. For the action of hooking the victims to the framework, the ports were
randomized, but sequentially. In each case, the destination port always varied
for each victim, but the source port was always 3000.
Table 4. Summary of ports used in the attack (A) on each victim (V)
Victim
Source Port (A) Destination port (V)
Victim 1 3000
1391 to 1398
Victim 2 3000
2166 to 2188
Victim 3 3000
52300 to 52305
Victim 4 3000
35616 to 35621
Identiﬁcation of cookies: a cookie was formed by a key that gives it a name
and associates an identiﬁer, which can be: created, modiﬁed or deleted; on the
client and on the server. Once created, it is sent in each HTTP request as a
parameter of this protocol. Nevertheless, certain cookies may have the same
name and belong to diﬀerent websites.
The traﬃc analysis is a point of reference to evaluate the amount of informa-
tion that a user exchanges with the sites visited. This implies the consumption
of time and the high use of resources. As seen in Table 4, the ports used to attack
the victims are random. Nowadays, there are tools that allow hiding the ports
in the execution of an attack. That is why this parameter is not included in
the design of the algorithm. With this analysis, the following parameters of the
cookie were chosen and then structured to present the algorithm:

An Analytic Model for Prevention of Cross-Site Scripting (XSS)
503
– Name: is the proper name with which the cookie is created.
– Site: is the domain or web page visited by the user.
– Creation date: is the date a user visited a web page.
– Expiration date: is the date that indicates when the cookie expires.
– Execution of commands: is a property that indicates if it allows to execute
commands or not.
3.3
Analytical Model
In this section, we propose the analytical model (algorithm) using the already
structured data, and thus, the classiﬁcation algorithm is deﬁned with the detail
of the chosen variables. To complement the cookie analysis, a test of browsing is
executed in suspicious sites in order to obtain more references about cookies and
to make a comparison with the information found. The operation of the algo-
rithm is described in Algorithm 1. It runs sequentially for each website visited
and each cookie created by it.
– Site Explorer: scans all websites visited by the client, covers the entire web
browsing information of each user.
– Cookie analyser: analyses the cookies generated by each website visited. It
should be taken into account that according to the results obtained from the
data collection, most websites generated 1, 2, 5 or even 31 cookies; therefore,
it is a requirement to analyse each website visited, in order to identify the
number of cookies created.
– Extract parameters: after analyzing the website and its cookies, the fol-
lowing parameters are extracted:
• Cookie Name - [CookieName]: it does not correspond to a known name
and can be a combination of symbols, letters, numbers or all. It is deﬁned
as a string of text.
• Website it belongs - [CookieSite]: as previously stated, a website can gen-
erate from 1 to 31 cookies. It is deﬁned as a text string.
• Accessibility - [CookieExec]: a boolean variable was identiﬁed for the exe-
cution of commands, this can be of type: hostOnly cookie, session cookie,
secure cookie or httpOnly cookie.
• Creation date - [CookieCreate]: corresponds to the day, month, year and
time the cookie was created. It is assigned a variable of type date.
• Expiration date - [CookieExpire]: corresponds to the cookie expiration
day, month, year and time. It is assigned a variable of type date.
In addition, the following variables were declared:
• [i]: to count the cookies for each website.
• [j]: to scroll through the entire list of visited websites.
• [CookieRate]: which starts with a value of 100 to qualify cookie’s reputa-
tion. This value means that it is a reliable cookie.
• [BlockSite]: is the variable that will be assigned to a website when the
cookie has a low reputation. This variable will later serve to keep track
the websites that must be blocked.

504
G. E. Rodr´ıguez et al.
– Storage parameters: is responsible for storing the variables, according
to the information of the parameters found in the websites visited, with
their respective cookies. The following variables are registered: [CookieName],
[CookieSite], [CookieExec], [CookieCreate], [CookieExpire].
– Conditions: subsequently, a set of conditions are executed:
• “if its diﬀerence is greater than 3”. This condition is executed to compare
the years of creation and expiration of cookies. 10 points are subtracted
from the variable [CookieRate] that qualiﬁes its reputation, otherwise the
next condition is executed.
• “If the [CookieExec] variable is equal to 1”. Ten points are subtracted
from the variable that qualiﬁes its reputation.
• “If the cookie name ends in “* .js”. This indicates that is JavaScript and
10 points are subtracted from the variable where its reputation is stored.
• “If the cookie’s reputation is less than or equal to 70”, a cookie that fulﬁls
the three conditions ends with a reputation of 70 points. Then, the site
where it belongs is extracted and stored in the [BlockSite] variable and
an alert will be presented to warn the user about this.
For cookies that meet one or two conditions, a gray list is created to qualify
them as suspicious. Later this information is stored in the Knowledge Base.
– Knowledge Base: is executed in a sequential way and with each cookie
analysed. Each time the algorithm analyses a website and extract the cookie
parameters, it queries its knowledge base to see if the visited website has
already been blocked or is in the list of suspicious websites. If so, it does not
execute any action, and the user is alerted in a second visit to the website.
Otherwise, return to execute the sequence of the algorithm.
4
Analysis of Results
The results obtained show that cookies can have the same name for diﬀerent
domains or websites visited. A curious fact is the number of cookies that are
generated per site. Tests reveal that a site can generate more than one cookie,
being 31 the biggest number. Among the cookies generated, there were those
that had the property “local storage”, which means that they keep a record of
when was the last time a user accessed the corresponding website. By comparing
the cookies generated by suspicious and unreliable websites it was found that
certain websites generated cookies with the name facebook.com, which means
that an attacker could steal information of credentials, if we navigate with an
open session of social networks. Other websites, more dangerous, generated a
cookie similar to the one of the framework Beef, with the name cookies.js. The
creation and expiration dates of cookies were a good reference to determine
which ones were suspicious. The analysis of websites in Fig. 2 shows that 29.41%
of the websites create cookies with a year of expiration greater than 3. The most
common years found were 2027, 2075 and 2077. Comparing with the cookie
created in the attack using the framework Beef (13 years of expiration which

An Analytic Model for Prevention of Cross-Site Scripting (XSS)
505
Algorithm 1. Cookie Classiﬁer
Require: Cookie Name CookieName : String
Require: Cookie Site CookieSite =: String
Require: Exec Commands CookieExec = 0, 1 : Boolean
Require: Creation Date CookieCreate : Date
Require: Expire Date CookieExpire : Date
Require: Number Cookie CookieNum : Numeric
Require: Total Site Visited TotalSiteV isited : Numeric
Require: Reputation CookieRate = 100 : Numeric
1: SITE EXPLORER Count all the sites visited by the user and assign the value to
the variable TotalSiteV isited
2: COOKIE analyseR Count the number of cookies per site visited and assign it to
the variable CookieNum
3: EXTRACT PARAMETERS Extract the properties of cookies created
4: STORAGE PARAMETERS Save the properties of cookies created
5: Select the following variables: CookieName; CookieSite; CookieExec;
CookieCreate; CookieExpire
6: if CookieSite is stored in the Knowledge Base then
7:
ALERT= “This site CookieSite is suspect”
8: else
9:
while j < TotalSiteV isited do
10:
Count the number of cookies per site visited and assign it to the variable
CookieNum
11:
if CookieNum > 1 then
12:
i = CookieNum
13:
repeat
14:
if CookieExpire −CookieCreate > 3 then
15:
CookieRate = CookieRate −10
16:
else
17:
if CookieExec = 1 then
18:
CookieRate = CookieRate −10
19:
else
20:
if CookieName = “ ∗.js′′ then
21:
CookieRate = CookieRate −10
22:
else
23:
i = i + 1
24:
end if
25:
end if
26:
end if
27:
until CookieNum = 1
28:
end if
29:
if CookieRate <= 70 then
30:
BloqueaSitio = CookieSite
31:
ALERT= “This site CookieSite is dangerous”
32:
end if
33:
j = j + 1
34:
end while
35: end if

506
G. E. Rodr´ıguez et al.
Fig. 2. Expiration date of cookies
created
Fig. 3. Cookies created that allow the
execution of commands
helped to deﬁne the variables CookieCreate and CookieExpire), we can say that
this property was fundamental and the basis for designing the algorithm, since
we compared the cookie that creates an ethical hacking program vs a cookie
created by a suspicious website. The average found for the expiration date of
the cookies was only 2 years. With this information, we classiﬁed as dangerous
cookies those ones that exceed 3 years of expiration.
The choice of the name is also crucial in the analysis, since it was found
that the names of all cookies have no meaning. We found names with a single
character, which opens the possibility of analysing cookies with a process, it
means each character of their names. Within the cookies created we had names
like cookies.js similar to the JavaScript ﬁle that Beef uses to hook victims. Then,
the cookie name is included as a variable, due to the extension, that is equivalent
to a script that can be exploited in the XSS attack. To conclude, cookies that
allow the execution of commands, shown in Fig. 3, are also part of the algorithm.
From this, we can analyse that from 17 websites visited, 88.24% of cookies allow
the execution of commands, and only 11.76% of websites create cookies of type
httpOnly, which does not allow the execution of XSS attacks. This parameter
can vary, and be of type hostOnly, which shows that the host that makes the
request has to match the domain of the cookie. Finally, the algorithm has a sub
process that allows forming a knowledge base, to be consulted before analysing
a visited website. This gives the added value to the proposal, since we will have
a database with all the websites visited, their cookies and their reputation level.
The algorithm would make a previous comparison before analysing new websites.
5
Conclusions and Future Work
Cookie Scout works to classify and qualify cookies, in order to give them a value
according to their reputation. It is formed by several processes; the most impor-
tant one allows to make previous and subsequent comparisons using a Knowledge
Base. The analytical model is based on a case study, generated by an attack in
controlled scenarios. The obtained results evaluate the traﬃc characteristics that
the victims exchange with their attackers, the ports used and the properties of

An Analytic Model for Prevention of Cross-Site Scripting (XSS)
507
the cookies created when a satisfactory XSS attack is executed. The results show
that 88.24% of cookies created have the property for executing commands, and
only 11.76% have the tttpOnly property, which is a feature of browsers to pre-
vent XSS attacks. As a complement, 29.41% of dangerous websites create cookies
with an expiration date of more than two years. As a future work, we highlight
the analysis of the parameter where the value of the cookie is stored, in order to
have an additional variable to improve the proposed model.
References
1. NOSCRIPT (2007), https://addons.mozilla.org/es/ﬁrefox/addon/noscript/
2. NOXSS (2007), https://addons.mozilla.org/en-us/ﬁrefox/addon/noxss/
3. Cross-site Scripting (XSS) OWASP (2017), https://www.owasp.org/index.php
4. Bisht, P., Venkatakrishnan, V.N.: XSS-GUARD: precise dynamic prevention of
cross-site scripting attacks, pp. 23–43. Springer, Heidelberg (2008)
5. Gundy, M.V., Chen, H.: Noncespaces: using randomization to defeat cross-site
scripting attacks. Comput. Secur. 31(4), 612–628 (2012)
6. Gupta, S., Gupta, B.B.: Cross-site scripting (XSS) attacks and defense mecha-
nisms: classiﬁcation and state-of-the-art. Int. J. Syst. Assur. Eng. Manag. 8(1),
512–530 (2017)
7. Kirda, E., Jovanovic, N., Kruegel, C., Vigna, G.: Client-side cross-site scripting
protection. Comput. Secur. 28(7), 592–604 (2009)
8. Putthacharoen, R., Bunyatnoparat, P.: Protecting cookies from cross site script
attacks using dynamic cookies rewriting technique. In: 13th International Con-
ference on Advanced Communication Technology (ICACT 2011), pp. 1090–1094
(2011)
9. Jim, N. Swamy, M.H.: Beep: browser-enforced embedded policies. In: 16th Inter-
national WorldWide Web Conference (WWW 2007), pp. 1090–1094 (2007)
10. Takahashi, H., Yasunaga, K., Mambo, M., Kim, K., Youm, H.Y.: Preventing abuse
of cookies stolen by XSS. In: 2013 Eighth Asia Joint Conference on Information
Security, pp. 85–89 (2013)

An Empirical Evaluation of Open Source
in Telecommunications Software Development:
The Good, the Bad, and the Ugly
Rolando P. Reyes Ch.1,4(B), Efra´ın R. Fonseca C.2, John W. Castro3,
Hugo P´erez Vaca1, and Manolo Paredes Calder´on4
1 Departamento de Seguridad y Defensa, Universidad de las Fuerzas Armadas ESPE,
Sangolqu´ı, Ecuador
hlperez@espe.edu.ec
2 Departamento de Ciencias de la Computaci´on,
Universidad de las Fuerzas Armadas ESPE, Sangolqu´ı, Ecuador
erfonseca@espe.edu.ec
3 Departamento de Ingenier´ıa Inform´atica y Ciencias de la Computaci´on,
Universidad de Atacama, Copiap´o, Chile
john.castro@uda.cl
4 Centro de Investigaci´on Cient´ıﬁca y Tecnol´ogica del Ej´ercito,
Universidad de las Fuerzas Armadas ESPE, Sangolqu´ı, Ecuador
{rpreyes1,dmparedes}@espe.edu.ec
Abstract. Software development for the communication networks
‘monitoring is usually based on Open Source software components, as
an eﬀective and low cost technological option. However, when we evalu-
ated a product developed with Open Source components, we found that
its eﬃciency is less than other similar Open Source software developed
with proprietary tools; which is unusual or at least it isn’t expected.
To the best of our knowledge this phenomenon has not been reported
in the literature. Hence, our aim was to identify the circumstances that
explain why the eﬃciency of Open Source software applications tends to
be less than Open Source applications developed with proprietary soft-
ware tools. A controlled experiment was performed at Universidad de las
Fuerzas Armadas ESPE of Ecuador to compare the performance of two
software tools for communication networks’ monitoring. A post hoc anal-
ysis reveled that some causal relationships that could explain the unex-
pected behavior of compared applications’ eﬃciency. From the statisti-
cal perspective, there is no signiﬁcant diﬀerence in eﬀectiveness between
Open Source and proprieta- ry applications for communication networks’
monitoring. Eﬃciency of the Open Source tools depends on a large extent
of software components used for their integration, which apparently is
not considered in the development of this kind of applications.
Keywords: Open Source · Software engineering · Experimentation
Empirical evaluation · Monitoring software · Communication networks
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_48

An Empirical Evaluation of Open Source in Telecommunications
509
1
Introduction
In the last years the use of Open Source tools have had an important growth,
becoming an attractive option to software development into programmers com-
munities [6]. The Open Source development has allowed programers obtain dif-
ferent source codes, change it, adapt it to their needs or make it better; within
a collaborative context given by Open Source communities [7,16]. This context
implies an agile software development [4], reducing costs concerning payment
of programmers, reducing time; but the highlight is undoubtedly the software
components reuse [8,18].
Taking into account that software components development is one of the most
representative practices within software engineering [18] and a common activity
performed by developers, appear to be necessary the personalization of Open
Source software components previous their use, due to its generality [1]. There
are several studies that enhance the software Open Source’s characteristics (e.g.:
colaboration, economic development, etc.) [9,11,12,14]; However, the eﬃciency
associated to the Open Source software components integration is generally no
referred; in other words there are very few studies that highlight this issue and
hence its validation has not already been property achieved [10,17,20].
In the context of communication networks, software development has been
oriented towards communication networks management and evaluation and
setup [3]. Most of this software is derived from Open Source software components
integration and the rest results from the use of proprietary software for devel-
opment. For the telecommunications industry is very important the eﬀective
network management; hence, the imminent possibility to reduce de eﬀectiveness
of this kind of software [10] motivated us to carried out an empirical evaluation
between OXE (software based on java, JEE architecture, code and components
Open Source distributed in diﬀerent communities) and MRTG (Multi Router
Traﬃc Grapher, a software developed with proprietary tools by Oetiker [15]).
Both tools have similar functionalities and services for networks monitoring.
This empirical evaluation was based on an experiment and three replica-
tions, which were carried out in a controlled environment. We evaluated the
eﬀectiveness of network monitoring applications through detection of speciﬁc
OID (Object ID) SNMP disconnections.
However, we found that the use of source code and components Open Source
have additional requirements of processing and memory. We conclude that
although software Open Source cover the requirements of monitoring, its code
is not always the most eﬃcient; hence, more empirical estudies could be carried
out in the Open Source communities.
The remainder of the article is structured as follows: Sect. 2 introduces the
studies related to the problem under study. The development of OXE and the
description of MRTG is detailed in Sect. 3. Section 4 presents the experimental
design. Implementation and results are described in Sect. 5. Experiment Threats
to validity are detailed in Sect. 6. Finally, conclusions, discussion and future work
are presented in Sect. 7.

510
R. P. Reyes Ch. et al.
2
Background
Open Source tools have been widely mentioned in the software development
community in recent years. Its rapid growth and particular characteristic of
collaborative work, have been engaging among programmers and considered in
some empirical studies, as shown below.
Mockus et al. [14] conducted an empirical evaluation of productivity between
Apache and Mozilla. History of source code changes, problem reports, developer
involvement, defect density, and more were considered. The ﬁndings show that
Apache and Mozilla have a low defect density in post-release environments and
high utilization productivity.
Meneely and Williams [13] correlated vulnerabilities belonging to the Open
Source kernel of Red Hat Enterprise Linux 4. The results showed that indepen-
dent developers are more likely to have vulnerabilities. However, it was found
that there could be serious vulnerabilities when many developers participate in
code management.
Adams et al. [1] studied the integration of multiple components of Debian,
Ubuntu and FreeBSD, based on the information of 384 package-versions and
the historical information of 29 years of: code changes, identiﬁed errors, release
of distributions, among others. The results demonstrated the need for further
empirical research to help organizations reuse components.
These works show that the empirical evaluation in Open Source is impor-
tant; however, many of them are focused in the beneﬁts of Open Source, or as
evaluation of their tools. We have found few empirical studies on code devel-
opment with Open Source and the empirical assessments related to eﬃciency
in integrating Open Source components from diﬀerent communities which are
generally generic in functionality are less taken into account [2,5,19]. This is the
reason why we have posed the following research question:
RQ: ¿How eﬀective are software tools developed using Open Source compo-
nents for telecommunications monitoring?
3
Open Source - Telecommunications Network
Monitoring Tools
Telecommunications networks are important. Practically it sustain the entire
communication infrastructure of the modern world. Consequently, its correct
operation is very valuable [2]. Today, more than ever, their need is increasing
when new technologies appear (e.g.: IOT) to improve the quality of life in cities
(e.g.: Smart Cities) and hence for the people (e.g.: Health).
From this point of view, network monitoring is important. There are stan-
dards such as IEEE1 or IETF2 RFCs3 that allow the management of data
1 Institute of Electrical and Electronics Engineers.
2 Internet Engineering Task Force.
3 Request for Comments.

An Empirical Evaluation of Open Source in Telecommunications
511
network traﬃc. Hence, it is necessary to use tools that allow simultaneous and
dynamic collection of data traﬃc generated by the equipment that makes up a
telecommunications network. The telecommunications network monitoring tools
perform this action. However, many of them are proprietary and therefore inac-
cessible to research in universities and laboratories. As a result, Open Source
appears to be the ideal way for software development for tools with this pur-
pose. A telecommunications network monitoring tool has at least the following
characteristics: (1) it obtains network traﬃc data through an SNMP agent, (2)
sends requests through SNMP protocol object identiﬁers (OIDs) to a network,
(3) has a dynamic data collection mechanism, and (4) generates statistics of
what is collected by creating statistical graphs.
For the development of our research, we will describe two Open Source mon-
itoring tools, MRTG and OXE. We chose MRTG due to it is a widely known
tool developed in a modular way for the Open Source community through pro-
prietary tools. OXE was choose due to it is a tool developed under Open Source
components from diﬀerent communities and for our familiarity with it. These
tools are described as follows.
3.1
MRTG Tool
It is an Open Source tool created by Oetiker [15]. Its development is based on
a modular decomposition in very coupled functional groups. MRTG was created
from its beginnings in the traditional way without the use of components and
then releasing its code. It was completely written in perl script. However, due
to performance problems, it was rewritten in C programming language in the
MRTG-2 version. This new version received many patches until to get a stable
product.
3.2
OXE (Proof of Concept)
OXE is a tool created by one of the authors (R. Reyes). It has the same func-
tionalities as the MRTG tool. The tool was created in JAVA with JEE standard
and a multilayer architecture (presentation, business, and persistence) and web
services. It was developed in three interactions under the RUP methodology. In
its business layer contains several GPL v3 Open Source components obtained on
the Internet (e.g. Net-SNMP, JMib, OpenNMS, etc.) in order to obtained the
above-mentioned characteristics. The source code could be accessed from the
following link: https://goo.gl/6H2dpU.
4
Experimental Design
We have considered the experimental reports proposed by Wohlin et al. [22]
in order to show our results. The evaluation of eﬀectiveness within monitoring
systems is necessary to know the way of monitoring based on standards IETF4.
4 Internet Engineering Task Force.

512
R. P. Reyes Ch. et al.
For this reason, some authors, who have proposed similar tools, state that fall
detection or disconnection in network traﬃc would be a suitable element for an
evaluation. Therefore, we have posed the following hypotheses:
• H0: The eﬀectiveness in detecting falls of OXE, is equal, to the eﬀectiveness
of detection of falls of MRTG.
• H1: The eﬀectiveness in detecting falls of OXE, does not equal, to the eﬀec-
tiveness of detection of falls of MRTG.
As shown in the hypotheses, our independent variables are MRTG and OXE
(see Sect. 3), whereas our dependent variable is the software’s eﬀectiveness to
detect falls or disconnections of an speciﬁc SNMP OID. In order to evaluate the
eﬀectiveness we have determined that the most appropriate metric is related to
the total number of falls or disconnections detected by the monitoring software
(NTDSM) and the total number of artiﬁcially injected drops or disconnections
(NTDIA). The following formula was established:
Effectiveness(M1) = NTDSM
NTDIA
It should be noted that an SNMP OID is the experimental unit selected
for the experiment. More speciﬁcally, it is the SNMP OID 1.3.6.1.2.1.4.3.0 cor-
responding to the “pInReceives” pertaining to standard RFC1213-MIB5 This
enable us to perform the detection of falls. The selection and assignment of
treatment subjects will not be used, because the context refers to artiﬁcially
injecting the falls or disconnections by the experimenter. Both monitoring soft-
ware should detect it independently. For that reason, our experimental task is
summarized in the application of 60 injections of drops or disconnections applied
to the SNMP OID (pInReceives) per thousandths of a second in each period of
2 min. The experiment would take about 2 h to run. The number of falls or
disconnections and the time between them will be recorded.
The experimental instruments for the controlled experimental environ-
ment consisted of two computers with the same hardware characteristics: Pro-
cessor Intel Core i5-3200U, RAM memory of 4 GB and storage of 1 TB. One
computer was implemented with Centos 5 as base operating system, in which
OXE and MRTG reside. This equipment carried out the monitoring tasks. The
second computer had Microsoft Windows OS, which was the monitored com-
puter. Both computers interconnect to the Gigabit ports of a Cisco Small Busi-
ness Smart Sg200 switch, simulating a real network of a small or medium scale
enterprise. Both application were installed in the same memory and processing
context in order to avoid the incidence of factors related to: diﬀerent hardware
(e.g.: memory paging, bus speed, etc.), hardware and network failures.
5 This RFC is deﬁned in the second version of the MIB- II for use with network
management protocols in TCP/IP-Internet-based.

An Empirical Evaluation of Open Source in Telecommunications
513
5
Execution and Results
This study consisted of an experiment and three replications. The experiment
and replications were performed at the Universidad de las Fuerzas Armadas
ESPE as described in Sect. 4. The execution consisted in recording the start
time of the experiment, the injection of drops or disconnections of thousandths
of a second for every 2 min, recording the hour, minute and second of the fall
or disconnection. This procedure was performed during 60 times. The duration
of the experiment was approximately 2 h. The replicates were executed in later
days in the same way as the experiment, one replication per day. The data col-
lection process consisted in obtaining the OXE and MRTG detection log where
the records of detected falls in the monitoring is stored. The date, hour, minute
and second are extracted. The data obtained are recorded on an electronic sheet
by crossing the hour, minute and second register of the experimenter. Correc-
tive detections were set to YES (1) and unsatisfactory as NO (0). Dichotomous
responses allowed the statistical analysis to be carried out.
Table 1. Summary of the results of the descriptive statistics of the experiment and
replications
Range
Min. Max. Sum
Mean
Standard deviation
Variance
Experiment
MRTG 1
0
1
52
0.87
0.34
0.12
OXE
1
0
1
50
0.83
0.38
0.14
Replication 1
MRTG 1
0
1
44
0.73
0.45
0.19
OXE
1
0
1
45
0.75
0.48
0.19
Replication 2
MRTG 1
0
1
53
0.88
0.32
0.11
OXE
1
0
1
50
0.83
0.38
0.14
Replication 3
MRTG 1
0
1
52
0.87
0.34
0.12
OXE
1
0
1
50
0.83
0.38
0.14
Statistical analysis with SPSS were performed based on the data obtained
about detected and undetected falls. The results of the descriptive analysis are
detailed in Table 1. Brieﬂy, it can be observed that the means of the falls detected
by MRTG are slightly higher than the mean drops detected by OXE, except in
replication 2. The same way happens with the standard deviation. As for the
variance, the values are slightly lower in MRTG due to the mean of the quadratic
deviations. A better detail of falls detected and undetected are shown in Fig. 1.
At ﬁrst sight a similarity is observed in the detection of the monitoring. Except
in replication 2 where there is a slight increase when no drops or disconnects are
detected.
Before using the statistical test, we created contingency tables to analyze
the association of detections and non-detections between the two independent
variables: OXE and MRTG. A summary of this information is shown in Table 2.
Brieﬂy, we can see that very few times the diﬀerence between disconnections

514
R. P. Reyes Ch. et al.
(a) MRTG Fall Detection
(b) OXE Fall Detection
Fig. 1. Descriptive charts of OXE and MRTG results
between MRTG and OXE. OXE almost always recognizes the disconnection
when MRTG also recognizes it. The data show that both software have approx-
imately more than 80% of them recognize the disconnects at the same instant.
Table 2. Contingency table of the experiment and replications
Experiment
Replication 1
Replication 2
Replication 3
OXE
OXE
OXE
OXE
No (0) Yes (1) No (0) Yes (1) No (0) Yes (1) No (0) Yes (1)
MRTG No (0)
2
6
5
11
2
5
2
6
20%
12%
33.3%
24.4%
20%
10%
20%
12%
Yes (1) 8
44
10
34
8
45
8
44
80%
88%
66.7%
75.6%
80%
90%
80%
88%
As a statistical technique to test our hypothesis we apply chi-square (X2) in
order to establish the independence of the data among the experimental variables
and thus to accept or reject our H0. We used a conﬁdence level of 95%, a level of
signiﬁcance of 0.05 and degree of freedom of 1, due to the context of the study. In
the Table 3 the results are shown. According to Newman-Pearson’s theories, in
all cases (experiment and all replications) the value of X2 (0.462, 0.455, 0.809,
0.462) is greater than the level of signiﬁcance proposed (0.05; whereby, it is
decided to ACCEPT the H0 and reject the H1. Thus, it is empirically stated
that the eﬀectiveness in detecting falls of OXE, is equal, to the eﬀectiveness of
detection of falls of MRTG.

An Empirical Evaluation of Open Source in Telecommunications
515
Table 3. Results of Chi square calculation
Chi
squared
(x2)
Degrees of
freedom
Level of
sig.
Conf.
level
Cont.
correction
Like. ratio Number
of cases
Experiment
0.462
1
0.05
95%
0.029
0.420
60
Replication 1 0.455
1
0.05
95%
0.114
0.441
60
Replication 2 0.809
1
0.05
95%
0.129
0.711
60
Replication 3 0.462
1
0.05
95%
0.029
0.420
60
6
Threats to Validity
We use the recommendations of threats to validity of Wohlin et al. [22] to mini-
mize the threats of internal validity, great care was taken in network connections,
using the same cable to avoid the threat of detecting false falls (false-positive)
by noise in the network. We also used a single computer where OXE and MRTG
systems were installed to avoid eﬀects introduced by the hardware that could
harm OXE and MRTG during the experiment. As for external validity we use
a controlled experimental environment based on actual interconnect equipment
and cables that are used in the industry. It also monitors a real team that has
generated actual traﬃc in order to make the results as close to reality. The con-
struct validity is minimized by the measurement of eﬃcacy in the detection of
disconnections, adequate and relevant variable for the context of telecommu-
nications. Finally, the validity of the statistical conclusion was used a suitable
statistical technique while several replications performed were suﬃciently for the
inference of the hypothesis.
(a) BoxPlot Fall detection
(b) BoxPlot no Fall detection
Fig. 2. BoxPlots

516
R. P. Reyes Ch. et al.
7
Conclusions, Discussion and Future Work
Organizations tend to integrate at least one or more Open Source components in
practice. This is common in software development. The use of components is a
main recommendation in Software Engineering to obtain a software architecture.
Now the scenario changes when we stop coding our components and start looking
for something similar in Open Source that exists on the Internet. Works such as
Ven et al. [21] warn that the beneﬁts of Open Source come together with unique
risks and challenges that go beyond a simple programmer’s “eureka” possibly
motivated by the Open Source code. Our results conﬁrm that code Open Source
can be found in good terms of eﬀectiveness; however, after a post-hoc analysis
we determined that the eﬃciency could be being aﬀected. In Fig. 2 we see that
MRTG is more eﬃcient than OXE. This was evidenced in the management
of resources (processing and memory) that OXE used during the experimental
execution. There is enough proofs to say that Open Source perfectly meets the
needs, supports faster, lower cost development, and other beneﬁts that critical
literature supports. However, not all this could be good if it would later have
to be validated or recoded. We do not know with certainty the cause for the
results obtained in our research, but we believe that the performance of tools
developed with Open Source components could need a refactoring of its code or
of its integration.
As future work we intend to make several empirical evaluations of software
development with and without Open Source, with students and professionals to
establish new eﬀects that may aﬀect the Open Source in the development of
software in the industry.
Acknowledgment. This work was supported by the “Laboratorio Industrial en Inge-
nier´ıa del Software Emp´ırica (LI2SE)”research project of the Universidad de las Fuerzas
Armadas ESPE of Ecuador.
References
1. Adams, B., Kavanagh, R., Hassan, A.E., German, D.M.: An empirical study of
integration activities in distributions of open source software. Empirical Softw.
Eng. 21(3), 960–1001 (2016)
2. Antichi, G., Shahbaz, M., Geng, Y., Zilberman, N., Covington, A., Bruyere, M.,
McKeown, N., Feamster, N., Felderman, B., Blott, M., et al.: OSNT: open source
network tester. IEEE Netw. 28(5), 6–12 (2014)
3. Bastian, M., Heymann, S., Jacomy, M., et al.: Gephi: an open source software for
exploring and manipulating networks. In: ICWSM, vol. 8, pp. 361–362 (2009)
4. Chiarani, M.C., Pianucci, I.G., Lucero, M.M.: Criterios de evaluaci´on de platafor-
mas virtuales de c´odigo abierto para ambientes de aprendizajes colaborativos. In:
VI Workshop de Investigadores en Ciencias de la Computaci´on (2004)
5. Chowdhury, S.R., Bari, M.F., Ahmed, R., Boutaba, R.: Payless: a low cost net-
work monitoring framework for software deﬁned networks. In: 2014 IEEE Network
Operations and Management Symposium (NOMS), pp. 1–9. IEEE (2014)

An Empirical Evaluation of Open Source in Telecommunications
517
6. DiBona, C., Ockman, S.: Open Sources: Voices from the Open Source Revolution.
O’Reilly Media Inc., Sebastobol (1999)
7. Frakes, W.B., Kang, K.: Software reuse research: status and future. IEEE Trans.
Software Eng. 31(7), 529–536 (2005)
8. Gaﬀney, J.E., Durek, T.A.: Software reuse key to enhanced productivity: some
quantitative models. Inf. Softw. Technol. 31(5), 258–267 (1989)
9. Hahn, J., Moon, J.Y., Zhang, C.: Emergence of new project teams from open
source software developer networks: impact of prior collaboration ties. Inf. Syst.
Res. 19(3), 369–391 (2008)
10. Hauge, Ø., Cruzes, D., Conradi, R., Velle, K., Skarpenes, T.: Risks and risk miti-
gation in open source software adoption: bridging the gap between literature and
practice. Open Source Software: New Horizons, pp. 105–118 (2010)
11. Jaaksi, A.: Experiences on product development with open source software. Inter-
national Federation for Information Processing. IFIP, vol. 234, pp. 85–96 (2007)
12. Li, H., Tesfatsion, L.: Development of open source software for power market
research: the ames test bed. J. Energy Markets 2(2), 111 (2009)
13. Meneely, A., Williams, L.: Secure open source collaboration: an empirical study
of Linus’ law. In: Proceedings of the 16th ACM Conference on Computer and
Communications Security, pp. 453–462. ACM (2009)
14. Mockus, A., Fielding, R.T., Herbsleb, J.D.: Two case studies of open source
software development: apache and mozilla. ACM Trans. Softw. Eng. Methodol.
(TOSEM) 11(3), 309–346 (2002)
15. Oetiker, T.: Monitoring your it gear: the mrtg story. IT Prof. 3(6), 44–48 (2001)
16. Paulson, J.W., Succi, G., Eberlein, A.: An empirical study of open-source and
closed-source software products. IEEE Trans. Software Eng. 30(4), 246–256 (2004)
17. Stol, K.J., Babar, M.A., Avgeriou, P., Fitzgerald, B.: A comparative study of
challenges in integrating open source software and inner source software. Inf. Softw.
Technol. 53(12), 1319–1336 (2011)
18. Szyperski, C.: Emerging component software technologies: a strategic comparison.
Softw.-Concepts Tools 19(1), 2–10 (1998)
19. Van Adrichem, N.L., Doerr, C., Kuipers, F.A.: Opennetmon: network monitoring
in openﬂow software-deﬁned networks. In: 2014 IEEE Network Operations and
Management Symposium (NOMS), pp. 1–8. IEEE (2014)
20. Ven, K., Mannaert, H.: Challenges and strategies in the use of open source software
by independent software vendors. Inf. Softw. Technol. 50(9), 991–1002 (2008)
21. Ven, K., Verelst, J., Mannaert, H.: Should you adopt open source software? IEEE
Softw. 25(3), 54–59 (2008)
22. Wohlin, C., Runeson, P., H¨ost, M., Ohlsson, M.C., Regnell, B., Wessl´en, A.: Exper-
imentation in Software Engineering. Springer, Heidelberg (2012)

Wearable Technology, Privacy Issues
Pablo Saa1(B), Oswaldo Moscoso-Zea1, and Sergio Lujan-Mora2
1 Faculty of Engineering Sciences, Universidad Tecnolgica Equinoccial,
Quito, Ecuador
psaa@ute.etu.ec
2 Department of Software and Computing Systems,
University of Alicante, Alicante, Spain
Abstract. The market of wearable devices, which includes advanced
devices such as smart watches, ﬁtness trackers, augmented and virtual
reality headsets, wearable cameras and other gadgets, is growing at a
spectacularly fast pace thanks to the acceptance by users with open arms.
However, there exist some concerns about how wearable devices could
aﬀect people’s privacy. This paper presents a literature review of privacy
issues related to wearable devices. Due to the novelty of this topic, there is
a lack of legislation and most wearable manufacturers do not respect the
privacy of their customers. The main concern is related to the potential
incorrect use of health data collected by wearable devices. Finally, from
the information reviewed, several implications to be considered by all
stakeholders are drawn.
Keywords: Wearables · Wearable technology · Privacy issue
Wearables regulations · Wearable data
1
Introduction
Advances in science and technology and the proliferation of devices connected to
the Internet, sensors and mobile apps are critical factors to an increase in smart
devices that can be attached to the human body. These types of devices are called
wearable devices. Wearable technology or wearable devices are “clothing and
accessories incorporating computer and advanced electronic technologies” [14].
Wearables started its development with the main goal to enhance the func-
tionality of clothing [11]. This development bridges the gap in making technology
pervasive into people’s daily life. Today, wearable technology is becoming pop-
ular and is growing and gaining momentum quickly for personal and business
use. A forecast expects the market to grow from 84 million units of wearables in
2015 to 245 million units in 2019, with a market to be worth $25 billion [4].
Companies like Google and Apple are taking the lead on the wearable devices
market and are oﬀering people an inﬁnity of newly and trendy devices. This
wearable market is forecast to grow by 78 % each year, until reaching 112 million
in 2018 [2]. There are many reasons for an exponential adoption of these devices
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_49

Wearable Technology, Privacy Issues
519
in the very near future. The most important reasons for these global adoption
are: low prices, high customer perceived value and low perceived risk [28].
Despite the positive aspects, sound functionality and higher expectations of
mass adoption of wearable devices, there is still some work to do. One of the
ﬁelds which need much attention before these devices reach a mature stage of
commercialization is information security, speciﬁcally risks concerning privacy
issues. Privacy has been considered one of the biggest barriers to the mass uptake
of this emerging technology, as it will be discussed along this paper. Legislation
has to be created to protect users and restrict manufacturers and companies the
use of personal and health information.
After introducing the topic of this paper in this section, the rest of the paper
is structured as follows: Sect. 2 presents a literature review on the wearable topic;
Sect. 3 summarizes an analysis about privacy violation by exploiting wearable
devices; Sect. 4 describes existing legislation and regulation; Sect. 5 presents the
implications for the adoption of this technology; ﬁnally, Sect. 6 provides conclu-
sions of the work.
2
Literature Review
The terms “wearable technology”, “wearable devices”, and “wearables” all refer
to electronic technologies or computers that are incorporated into items of cloth-
ing and accessories which can comfortably be worn on the body. These wearable
devices can perform many of the same computing tasks as mobile phones and
laptop computers [25].
Wearables are expected to provide the users certain information about health,
ﬁtness, disabilities, aging, education, enterprise, ﬁnance, transportation, gam-
ing, and music in real time. Today, some examples of wearable devices include
watches, glasses, contact lenses, smart fabrics (e-textiles), beanies (caps), head-
bands, jewelry (rings), bracelets, and hearing aid-like devices (earrings).
Before wearables were open to the consumer market, they were used in the
ﬁeld of military technology and had a considerable implication for healthcare and
medicine. Nowadays, wearables are mainly projecting their aim in the ﬁelds of
ﬁtness, health, and dietary, considering the demand of users that are interested in
acquiring these technological devices and what information they want to receive
from wearable technology [17], as is shown on Fig. 1.
As any other technology, wearables extends the capabilities of a person
through diﬀerent features, providing enhanced communication, sensing, recogni-
tion, memory, and logistical skills, such as ﬁltering phone calls or just monitoring
the health. A clear example of this kind of devices is a special vest developed by
“VivoMetrics”, which allows to accelerate treatment in patients by monitoring
their blood pressure and heart rate.
Wearable computing devices are no longer just modern and fashionable acces-
sories that complement our mobile devices, they are taking their own place in
our day to day lives, becoming an integral part of the business world. In the
article “Why Wearable Tech Will Be as Big as the Smartphone” from the Wired

520
P. Saa et al.
Fig. 1. What consumers want from wearables [17]
Journal, many of their editors agree with the idea that “A new device revolution
is at hand: Just as mobile phones and tablets displaced the once-dominant PC,
so wearable devices are poised to push smartphones aside” [27], this statement
ensures a successful growing for wearables’ business in a non-so far future. How-
ever, in the other hand, there is a social and cultural impact of wearables for the
privacy issues, especially for the health information that this devices generate.
“The distinguished capabilities of these devices are also the very reasons they
require security and privacy protections of an unprecedented scale” [7, p. 75].
2.1
Privacy Issues
Compared to the smartphones and tablets, most of the wearable devices are
designed with smaller size and readily attached on one’s body, such as bands,
watches, glasses, and so on. With the wider adoption by both consumer and
enterprise sectors, more privacy issues are also brought by wearable devices to
the people who are wearing and surrounded by those gadgets.
2.2
Health Data
As described in section two, the main features that the wearable manufacturers
are promoting are: activity tracking, ﬁtness, and dietary. However, in regards to
data collection, wearable devices go further on top of the relatively traditional
electronic portable devices [20]. Besides the common concerns on personal infor-
mation, wearable devices are also recording health data as: users’ steps, blood
pressure, heart rate, sleep pattern, and other private medical data including
dietary. All these information is more private and more sensitive than a mobile
number and an email address [8].

Wearable Technology, Privacy Issues
521
2.3
Health Data in Consumer Sector
When the data is synchronized to the dashboard through a mobile or laptop,
the users’ data collected by the wearable devices is loaded into the centralized
database maintained by the wearable manufacturer [13]. Thus, these private
data can be potentially exploited by the manufacturers in order to generate
more proﬁt without the user’s consent.
In most privacy policies of the wearable manufacturers, the possibility of
releasing the sensitive health data without users’ consent, are unequivocally
disclosed with statements. For instance, users skip statements such as “We may
share your information with third parties. . . ” [12]. This implies that the wearable
manufacturers are potentially able to abuse of users’ health information and
sell it to other commercial companies, aﬀecting users’ privacy. For example, the
consumers can intermittently receive spam from some “health advisors”, and the
health insurance companies can adjust individual’s premium based on acquired
data, without user’s awareness [21]. Also, the use of application programming
interfaces (APIs) can enable more third-party programs to access the consumers’
sensitive health data for diﬀerent purposes, while consumers have no idea about
their privacy policies and few awareness of what would happen.
In addition, according a report published in 2014 [16], 43 % of users were
unwilling to share health data with friends and family because they did not feel
comfortable sharing any information about their health condition. However, by
2016 the “The Wearable Life 2.0” report [17, p. 5] states that “Consumers are
also less likely to agree that wearable technology will make us more vulnerable
to security breaches” and that “will invade their privacy”. While results show
increased comfort levels between 2014 and 2016, there still a 25 % of consumers
that would not trust any company with personal information associated to wear-
able technology [17]. Figure 2 clearly shows how consumers are still unhappy to
trust their personal data to companies, by allowing them to capture their infor-
mation through wearable technology.
Eventually, the default settings of social share and proﬁle in the wearable
devices are still posing more privacy leaking. Fitbit, one of largest wearable
manufacturers, used to have default privacy setting as enabling the proﬁles cre-
ated by the users, to be searchable by search engines like Google, Bing, and
so on. This even includes the most intimate personal information, such as the
sexual activity recorded in the proﬁle [19]. Obviously, in this case the wearable
manufacturers did not take proper actions to protect the users’ privacy.
2.4
Health Data in Enterprise Sector
The wide adoption of wearable devices in enterprise sectors is also raising more
privacy issues. By 2018, Gartner states that “two million employees will be
required to wear health and ﬁtness tracking devices as a condition of employ-
ment” [9].
Besides being a handy companion device to the laptop and smartphone,
wearables, particularly smart watches and ﬁtness bands, are mainly applied by

522
P. Saa et al.
Fig. 2. Consumers’ non-trusted companies to capture their information through
wearables [17].
the employers to promote the staﬀwellness program. The wearable devices are
distributed to and worn by employees to track their activities, heart rate, sleep
pattern and so on, as their normal functions [26]. Then, these health data will be
collected and analyzed by the employers and aimed to help employers identify
the stress levels and fatigue levels of employees [17].
Considering these beneﬁts, wearables oﬀer a wealth of possibilities for both
employer and employee. However, employees are not interested on wearing any of
these devices, unless their employers give them any kind of incentive, or provide
the devices for free. A survey conducted in ﬁve diﬀerent countries by Price-
WaterhouseCoopers obtained the top ﬁve biggest hesitations with regards to
purchasing a wearable [17]. These results were organized in order to show the
reasons from the most to the less important. To clarify what is stated, a radar
chart (Fig. 3) is depicted to show how the price of a wearable is the main barrier
from customers at the time to decide to purchase a wearable. Then, usability
and utility follows the list, which indicates that customers will not pay a lot of
money for something they do not even know if they would use.
Finally, the complexity of a wearable itself and the privacy issues that these
devices arise, still being top concerns from employees by adopting this corpo-
rate trend (see Fig. 3). An extreme case occurred in a hi-tech oﬃce complex in
Sweden, named Epicenter. This company requires employees to be implanted a
tiny RFID chip in their hands [5]. This wearable technology furthers more than
ﬁtness bands, considering that employees are forced to implant them during
work. The health data that wearables collect would readily blur the bound-
aries between work and routine life. Therefore, potentially invade employee’s
privacy, as the non-working life is also monitored by the employers and wearable

Wearable Technology, Privacy Issues
523
Fig. 3. Consumers’ biggest hesitations with regards to purchasing a wearable [17].
manufacturers [23]. Furthermore, the legislation [8] only regulates on customer’s
personal and health information, while employees’ privacy is completely uncov-
ered, implying more serious risks.
3
Privacy Violation by Exploiting Wearable Devices
Besides the privacy concern from the consumers by wearing these technological
devices, the privacy of other people within the consumer’s proximity is also at
risk. Some wearable devices not only track the user’s own activities, they also
record what the world is like within the user’s proximity.
A good example to analyze are the Google Glasses, which raise companies’
and people’s extensive privacy concerns since its ﬁrst debut. This is mainly due
to the equipped mini camera that is constantly recording everything in front of
the screen, which actually, somehow could aﬀect indirectly others. Particularly,
if facial recognition technology is implemented to the Google Glasses, people
will be easily identiﬁed with eﬀective data processing, and all behaviors will be
recorded. Their privacy would be readily violated and people would have to be
conscious of what it means [24]. Therefore, many restaurants, bars, casinos, and
some other public places quickly banned Google Glasses [3], including Google’s
own shareholders meeting [10].
Even that Google has announced to forgo the plan of incorporating facial
recognition technology in Google Glasses [22], it would still be diﬃcult to restrain
other similar privacy inﬁltrating technology in other wearable devices [18]. For
example, the Samsung Galaxy Gear is likewise armed with a camera, yet less
obtrusive, but can be maliciously abused to invade others’ privacy.

524
P. Saa et al.
4
Legislation and Regulation
As mentioned above, by collecting the data through gadgets, wearable manufac-
turers would be able to access the users’ personal and health information, analyze
the data, and share the outcomes, generating more proﬁt. All this under their
blurred privacy policies. The API function also enables third-party integration
to access the data. A critical reason for those ambiguous privacy policies is that
currently there are no mature regulations and legislations to protect customer’s
privacy and restrict the use of personal information by the emerging wearable
technology.
A good example of a country that is working on legislation in this area is
Australia. The “Australian Privacy Principles (APP) belonging to Privacy Act
regulate the handling of personal information by Australian government agencies
and some private sector organizations” [15]. It states that an APP entity that
collects personal information for a particular purpose should not use those data
for another purpose unless the customer consent, or in certain excepted situa-
tions [15]. This seems to provide powerful principles on restricting the data trad-
ing practices. But it has not clearly explicated the emerging trend of wearable
devices, and, therefore, the uncertainty and possibility of not being applicable
still exists [6]. In addition, only those private companies and Australian govern-
ment agencies that have an annual turnover of at least 3 million AUD (Australian
Dollars) are subjected to the Privacy Act [8]. Thus, other private organizations
of small-scale may still use personal information for other purposes.
Moreover, due to the limited power of enforcement, Australian agencies can
only take actions on onshore manufacturers and service providers physically
located in Australia [6]. Since most worldwide famous wearable device manufac-
turers are headquartered in the United States (US), they may not necessarily
comply with the APP. The wearable manufacturers may hold personal infor-
mation, including the users’ health data from all over the world, and they only
have to comply the US regulations. Particularly for health information in the US,
Health Insurance Portability and Accountability Act of 1996 (HIPAA) regulates
how entities in health care area should be legally used and adequately protected.
However, likewise in Australia, the emerging of wearable devices manufactur-
ers are not clearly deﬁned in the current entity categorization [1]. This means
the wearable manufacturers are holding and processing the users’ personal and
health information in a dark-zone out of the regulations, which put users in a
high risk of privacy violation.
5
Implications
From the discussion presented above, several implications are drawn in order to
determine the most common as described in the following:
– Current regulations cannot eﬀectively restrict the behavior of processing per-
sonal and health information by those wearable device manufacturers or third-
party application providers.

Wearable Technology, Privacy Issues
525
– Regulations and legislations should be complemented to standardize the per-
sonal and health data collecting, storing and processing practices; those wear-
able companies need to be clearly identiﬁed, categorized and regulated into
the privacy regulations of each country.
– Data collection should only be approved when it’s necessary to devices func-
tions, and when it comes to sensitive data such as health information should
only be collected and used under the consent of individuals.
– Referring to the privacy regulations, information holders should also modify
their privacy policies and clearly state how personal data will be collected,
stored and the possible usage of these data in any conditions.
– Employers who are promoting workplace wearable devices should keep the
transparency in all those data related practices. Sound regulations should be
developed in each country to monitor the operations in terms of employees’
privacy.
– Regulations and legislations are always playing a passive role facing emerging
technologies and are far behind of them as it is very hard for regulations and
legislations to predict and act proactively before the technologies are born;
however, it is still necessary and feasible to reduce the gap between them, rel-
evant government agencies should be more tightly connected to technologies
and do more research and analysis.
– Collaboration between agencies and emerging technology companies plays an
important role by prioritizing customers’ needs and deﬁning clear policies
that can be easily adopted along the technology evolves.
6
Conclusions
Wearable industry is growing fast, while privacy issue is a topic that cannot
be neglected. Furthermore, the privacy issue might become a big barrier to the
adoption of wearable technology for users, as users’ awareness on privacy are
strengthened and the current condition of wearable devices represents a threat
to the users’ own privacy.
After the analysis performed in this paper, it is clearly deﬁned that the pri-
vacy issue related with wearable technology implemented on all relevant comput-
ing devices requires thorough consideration by the wearable technology industry
and the regulation parties. Even though the emerging wearable technology are
bringing beneﬁts to their lives, privacy protection should not be compromised.
Users feel the need to be protected from their information not to being shared
or leaked by any entity or third party. Therefore, the wearable manufacturers
should take necessary actions to protect the users’ privacy and the legislator
needs to catch up the pace to regulate both personal and business uses of wear-
able devices to eliminate the risks of privacy violation within compliance.

526
P. Saa et al.
References
1. Bromberg, K.H., Cranston, D.A.: Wearable technology: taking privacy issues to
heart. New York Law J. (2015)
2. Business Wire: Worldwide Wearable Computing Market Gains Momentum with
Shipments Reaching 19.2 Million in 2014 and Climbing to Nearly 112 Million
in 2018 (2014), http://www.businesswire.com/news/home/20140410005050/en/
Worldwide-Wearable-Computing-Market-Gains-Momentum-Shipments
3. Castillo, M.: Seattle restaurant bans google glass wearers (2013), https://www.
cbsnews.com/news/seattle-restaurant-bans-google-glass-wearers
4. CCS Insight: Wearables Market to Be Worth $25 Billion by 2019 (2017), http://
www.ccsinsight.com/press/company-news/2332-wearables-market-to-be-worth-
25-billion-by-2019-reveals-ccs-insight
5. Cellan-Jones, R.: Oﬃce puts chips under staﬀ’s skin (2015), http://www.bbc.com/
news/technology-31042477
6. Daly, A.: The Law and Ethics of ’Self Quantiﬁed’ Health Information: An
Australian Perspective. Int. Data Priv. Law (2015)
7. Di Pietro, R., Mancini, L.V.: Security and privacy issues of handheld and wearable
wireless devices. Commun. ACM 46(9), 74–79 (2003)
8. Federal Register of Legislation, Australian Government: Privacy act 1988 (2015),
https://www.legislation.gov.au/Details/C2015C00279
9. Gartner: Gartner Reveals Top Predictions for IT Organizations and Users for 2016
and Beyond (2015), http://www.gartner.com/newsroom/id/3143718
10. Lloyd, C.: Google Glass Banned at Company’s Own Shareholders Meeting (2013),
https://goo.gl/JhS2fe
11. Meola, A.: Wearable technology and iot wearable devices. Business Insider (2016),
http://www.businessinsider.com/wearable-technology-iot-devices-2016-8
12. Misﬁt: Privacy policy (2016), http://misﬁt.com/legal/privacy policy
13. Ng, C.: 5 Privacy Concerns about Wearable Technology (2015), https://blog.
varonis.com/5-privacy-concerns-about-wearable-technology
14. O’Donovan, T., O’Donoghue, J., Sreenan, C., Sammon, D., O’Reilly, P., O’Connor,
K.A.: A context aware wireless body area network (ban). In: 3rd International Con-
ference on Pervasive Computing Technologies for Healthcare (PervasiveHealth), pp.
1–8 (2009)
15. Oﬃce of the Australian Information Commissioner: Privacy fact sheet 17:
Australian Privacy Principles (2014), http://www.oaic.gov.au/privacy/privacy-
resources/privacy-fact-sheets/other/privacy-fact-sheet-17-australian-privacy-
principles
16. PwC Health Research Institute: Health wearables: Early days (2014), http://
www.pwc.com/us/en/health-industries/top-health-industry-issues/assets/pwc-
hri-wearable-devices.pdf
17. PwC Health Research Institute: Half of people would use a workplace smart-
watch (2016), http://pwc.blogs.com/press room/2015/04/half-of-people-would-
use-a-workplace-smartwatch-pwc-research.html
18. Ranger, S.: Google glass is just the beginning: Invisible cameras and the pri-
vacy headaches of tomorrow (2013), http://www.zdnet.com/article/google-glass-
is-just-the-beginning-invisible-cameras-and-the-privacy-headaches-of-tomorrow
19. Rao, L.: Sexual Activity Tracked By Fitbit Shows Up In Google Search
Results (2011), https://techcrunch.com/2011/07/03/sexual-activity-tracked-by-
ﬁtbit-shows-up-in-google-search-results

Wearable Technology, Privacy Issues
527
20. Sacco, A.: Fitness Trackers are Changing Online Privacy - and It’s Time to Pay
Attention
(2014),
https://www.computerworld.com/article/2491195/personal-
technology/ﬁtness-trackers-are-changing-online-privacy-and-it-s-time-to-pay-
attention.html
21. Shemkus, S.: Fitness trackers are popular among insurers and employers - but is
your data safe? The Guardian (2015)
22. Simpson, J.M.: Welcomes Death Of Google Glass, Says Internet Giant Should Not
Oﬀer “Glass 2.0” Until Privacy Issues Are Solved (2015), https://goo.gl/hc73te
23. Spicer, A., Cederstrm, C.: What companies should ask before embracing wearables.
Harvard Bus. Rev. (2015)
24. Swearingen, J.: How the Camera Doomed Google Glass (2015), https://www.
theatlantic.com/technology/archive/2015/01/how-the-camera-doomed-google-
glass/384570
25. Tehrani, K., Andrew, M.: Wearable technology and wearable devices: Everything
you need to know (2014), http://www.wearabledevices.com/what-is-a-wearable-
device
26. Tractica: Wearable Devices for Enterprise and Industrial Markets (2016), https://
www.tractica.com/research/wearable-devices-for-enterprise-and-industrial-
markets
27. Wasik, B.: Why wearable tech will be as big as the smartphone. Wired (2013)
28. Yang, H., Yu, J., Zo, H., Choi, M.: User acceptance of wearable devices: an extended
perspective of perceived value. Telematics Inform. 33(2), 256–269 (2016)

Human-Computer Interaction

Older Adults’ Perception of Online Health
Webpages Using Eye Tracking Technology
Anushia Inthiran1(&) and Robert D. Macredie2
1 Department of Accounting and Information Systems,
University of Canterbury, Christchurch, New Zealand
anushia.inthiran@canterbury.ac.nz
2 Department of Computer Science, College of Engineering
Design and Physical Sciences, Brunel University London, Uxbridge, UK
robert.macredie@brunel.ac.uk
Abstract. In this paper, we describe how older adults perceive online health
webpages, using a checklist and heatmaps to provide visualization of users’ gaze
data. Preliminary results indicate that older adults demonstrate a consistent
viewing pattern when viewing a page perceived as favorable. When viewing a
page perceived as unfavorable, older adults tended to ignore many sections of
the webpage. Results of this research study provide information in relation to
how perceived favorability inﬂuences page viewing behavior.
Keywords: Eye tracking  Health  Older adults  Perception
Web pages
1
Introduction
One popular online activity performed by older adults is looking for health information
[1], with most searches being in relation to their own healthcare [2]. However, as
‘digital immigrants’, older adults may lack the digital literacy skills of how effectively
to ‘surf the web’ [3]. Further, relatively little is, known about older adults’ perceptions
of online health webpages. Older adults are a diverse demographic group [4] and may
have different perceptions of what a good webpage should look like.
Previous research studies provide a wealth of information [5–7] but what remains
unknown is if older adults view online health webpages differently based on perceived
favorability; and if so what differences are exhibited? This study extends previous work
by speciﬁcally eliciting older adults’ perceptions of online health webpages which is an
area yet to be studied. Therefore, this study poses the question - how do older adults
view online health webpages based on their perception of the pages’ favorability?
Eye tracking data is useful in this context as it provides an objective source of
interface evaluation [8]. Data from the eye tracker provides us with an opportunity to
describe these differences in relation to viewing behavior. A higher ﬁxation count on an
area indicates that it is more important or noticeable to the user than areas with lower
ﬁxation counts [8]. We explore if older adults’ perception of webpage favorability
inﬂuences viewing behavior. This study addresses this question, providing information
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_50

on older adults’ perceptions of online health webpages, with perceptual data gathered
using an eye tracking device and visualized as heatmaps.
2
Related Work
Previous studies conducted with older adults using eye tracking data were related to
viewing behavior and the use of a particular type of search interface [5], comparisons
study of older and young adults in relation to viewing behavior of images and web
content [6] and how reading patterns of health text messages can be used to predict
recall [7]. Older adults view and use faceted search interface elements more when
searching for a severe health condition [5]. Older adults spent more time ﬁxating on
illustrations on cognitive websites in comparison to affective websites [6] and older
adults who ﬁxated more on text recalled more information in comparison to young
adults [7]. In another comparison study between young and older adults, results
indicate older adults spent more time looking at the content of the page and navigation
areas in comparison to young adults [9]. Essentially older adults looked at more parts of
a page than young adults [9].
There is yet to be eye tracking data on what older adults look at based on perceived
page favorability. This is an important issue as websites are made up of visual per-
ceptual elements which facilitate the cognitive process of viewing a website [10]. This
cognitive process forms a user’s perception of a website [10]. A positive perception of
the website meant that it has met the users’ need, so without a positive perception a
website is rendered pointless regardless of the usefulness of its content [10]. Thus,
information on viewing behavior based on perceived webpage favorability will enable
the design of online health webpages that favor older adults needs and increase search
efﬁcacy.
3
Methodology
A purposeful convenience method was used. The inclusion criteria were that partici-
pants had to be more 55 years and above. There were 31 females and 21 male par-
ticipants. Hence, an exploratory study was conducted with 52 participants, all aged
55 years or over. The average age was 60.5 (SD = 4). All participants were still
working. Participants qualiﬁcations ranged from a PhD to a college diploma. The
experiment was conducted at a university lab. None of the participants had used
MedlinePlus previously. This allowed for us to obtain a ‘fresh’ perspective on per-
ception of the page and viewing behavior. Webpages from MedlinePlus were selected
pertaining to four health issues: high blood pressure (Fig. 1a (left)); diabetic
nephropathy (Fig. 1b (right)); stroke (Fig. 2a (right)); and cancer (Fig. 2b (left)). These
four health topics represent common health issues experienced by older adults [11].
532
A. Inthiran and R. D. Macredie

3.1
Webpage Selection
The webpages in Figs. 1a and b (Set 1) and Figs. 2a and b (Set 2) are similar in terms
of page design and property. This means pages in each set used the same font size, font
type and placement of text and images. Pages in Set 1 were taken from the medical
encyclopedia page of the medical domain MedlinePlus. The webpages in Figs. 2a and
b were taken from the general information page of MedlinePlus. These pages were
generated when the queries ‘stroke’ and ‘cancer’ were entered into the search box.
Thus, participants were provided with the four webpages from the two sets to view.
While the webpages in each set were similar (allowing for focus within pages of the
set), each set had different design underpinnings. For example, in Fig. 1a and b, there
are links on the top left-hand corner. These links are not available in Figs. 2a and b.
This allows for the exploration of how these differences impacted perceived favora-
bility and in the way that participants viewed them.
3.2
Experimental Procedure and Data Collection
Participants were informed that they would be viewing four health webpages on a Tobii
TX300 screen. They were told to view and read the page for a period of 10 s - as it is
Fig. 1. (a) and (b) (Set 1). Heatmaps for the webpages on blood pressure - favorable (left) and
kidney - favorable (right).
Fig. 2. (a) and (b) (Set 2). Heatmaps for the webpages on stroke - favorable (left) and cancer -
unfavorable (right).
Older Adults’ Perception of Online Health Webpages
533

the average time spent on a webpage before deciding whether to stay on the page or to
leave [12, 13]. Participants were told to view and read the page as normally as possible.
It is acknowledged that viewing a health page and searching for health information is
usually performed with a speciﬁc search goal; however, for the purpose of this study,
the intention is to obtain a ‘ﬁrst impression’ of viewing a webpage via ﬁxation counts
rather than to fulﬁl a search goal. The webpages were presented one at a time and the
order was rotated (inter and intra set rotation) for each participant. At the end of 10 s,
participants were given a checklist (with the webpage still visible) to allow their
perceptions of the page to be gathered. The checklist was adopted from Lingard [9],
with 10 perception elements used: interesting, vibrant, simple, consistent, straightfor-
ward, clear, reliable, attractive, pleasant and progressive. These elements were selected
as they best encompass the design elements of the webpages used in this experiment.
Heatmaps generated from a total of 208 recordings were taken into the analysis stage
(52 participants*4 webpages).
3.3
Data Analysis
The perception checklist used a 5-point rating scale with notations provided at each end
of the scale. For example, interesting-boring, vibrant-pale. The scaling technique was
explained to participants prior to the start of the experiment. The 5-point rating scale
was quantiﬁed by assigning values of +2, +1, 0, −1 and −2 to the scale items.
Favorability ratings were calculated by taking the participant’s actual score (x) and
dividing it by the maximum score possible. The maximum possible score was derived
by each of the 10 impression elements receiving a score of +2. Hence, favorability
rating for each participant would range from −1 to 1. This scoring mechanism has been
used in another study [9]. Zero scores were not received for any perception element.
A positive rating indicates favorability; a negative rating indicates un-favorability. An
independent t-test was conducted to compare the mean perception ratings of the pages
judged by the participants to be favorable (F) and unfavorable (UF). As shown in
Fig. 1/Set 1, and Fig. 2/Set 2, the design for both sets of webpages is similar. Given the
design similarities across the pages, the webpages were categorized into six areas of
interest (AoI(i)-(vi)) for Set 1 whilst for Set 2 ﬁve AoI’s were identiﬁed AoI (i)-(v).
Tables 1 and 2 provide information on the AoI names and percentage ﬁxation details.
Table 1. AoI and ﬁxation percentage details for Fig. 1
AoI Name
Percentage ﬁxation
Figure 1a (F) Figure 1b (F)
i.
Links (top left)
7
7
ii.
Image
10
45
iii.
Accompanying text
30
50
iv.
Overview text
93
70
v.
Update information
5
5
vi.
Social network (top right)
0
0
534
A. Inthiran and R. D. Macredie

4
Results
Results indicate that participants found 3 of the webpages favorable and 1 webpage
unfavorable. The webpages containing information on kidney, stroke and blood
pressure were rated as favorable (Figs. 1a, b and 2a). The webpage on cancer was rated
as unfavorable (Fig. 2b). The webpage containing information on kidney was per-
ceived as the most favorable (Fig. 1b), followed by the webpages on blood pressure
and stroke. Results of the t-test provide a value of t = 8.41, p < .0001. This indicates
that there is a signiﬁcant difference between the favorable and unfavorable webpage
ratings. As the pages in each set were similar, the results are discussed based on page
sets (Set 1 – Figs. 1a and b, and Set 2 – Figs. 2a and b).
Set 1 - AoI(iv) was viewed the most, followed by AoI(iii) and AoI(ii). This indi-
cates that participants viewed text more than images. However, AoI(ii), in Fig. 1b was
viewed more that the image in Fig. 1a. AoI(i) and AoI(v) were viewed least, with
percentage ﬁxations of 7% and 5% respectively for Figs. 1a and b. AoI(vi) was not
viewed at all. Participants demonstrate a similar viewing behavior when viewing
webpages perceived as favorable.
Set 2 - participants ﬁxated on AoI(iii) the most followed by AoI(ii). The remaining
AoI’s(i, iv, v) were viewed only for Fig. 2a but not for Fig. 2b. For Fig. 2a, AoI(i) and
AoI(v) were viewed the least, with a percentage ﬁxation of 10% and 29% respectively.
Participants did not view the image in Fig. 2b, nor did they view the links at the top left
and right-hand corners of the page. Participants ignored many sections of the page
when viewing a webpage deemed unfavorable. Participants demonstrated dissimilar
behavior when viewing Figs. 2a and b.
5
Discussion and Conclusion
Participants rated the webpages in Set 1 as favorable. The ﬁxation counts, visualized as
heatmaps indicate that the viewing pattern for the favorable webpages were similar.
More attention was paid to text than to images, suggesting that older adults may prefer
textual forms of information. It is noted that the reading patterns for Figs. 1a and b
were similar. Attention was paid to all parts of the overview text (AoI(iv)). However,
the image in Fig. 1b was viewed more than that in Fig. 1a. This raises an interesting
question on the choice of image used. We postulate that the image in Fig. 1a was
Table 2. AoI and ﬁxation percentage details for Fig. 2.
AoI Name
Percentage ﬁxation
Figure 2a (F) Figure 2b (UF)
i.
Links (top left)
10
0
ii.
Function on left hand corner 33
7
iii.
Explanatory text
75
80
iv.
Image
29
0
v.
Links (top right)
10
0
Older Adults’ Perception of Online Health Webpages
535

general purpose (taking blood pressure) whereas the image in Fig. 1b was speciﬁc,
depicting a diseased and normal kidney. The top left periphery of the page (AoI(i)) was
viewed sparingly, but AoI(vi) (top right) was not viewed at all. This aspect requires
further investigation.
As for Figs. 2a and b the ﬁxation counts visualized as heatmaps indicate that there
is a difference in the way that the two webpages were viewed. When viewing a
favorable page, older adults seemed to pay attention to most aspects of the page;
however, when viewing an unfavorable page, most sections of the page were not
viewed. Speciﬁcally, the periphery of the webpage (top left, top right) were not viewed
when a page was deemed unfavorable. The next difference observed concerned AoI(iii)
and the way that text was read. As seen in Fig. 2a, the heatmap shows that attention
was paid equally to all areas of the text. However, in Fig. 2b, attention was largely paid
to the text at the top of the screen. Whilst the image in Fig. 2a was viewed, the image in
Fig. 2b was not viewed at all. This suggests the need to further investigate the effects of
using images on webpages for older adults. An interesting point to note is - older
adults’ favorability towards viewing text in comparison to images. The typical
F-pattern in relation to text reading and viewing webpage was not supported in Figs. 1
and 2, suggesting that older adults may not demonstrate this pattern in this context.
Results of this study provide preliminary information on how older adults view
webpages based on perceived favorability. There are differences in viewing behavior
amongst older adults when viewing webpages perceived as favorable and unfavorable.
Results also raise interesting questions in relation to the use of images and items at the
periphery of the webpage. We acknowledge that only 4 webpages and 52 participants
were involved in this study meaning that any interpretations have to be treated cau-
tiously, but they deﬁne interesting directions to reﬁne the work. In future work, we
intend to conduct the experiment with a wider range of webpages and a larger sample.
In the next stage of the research we also intend to obtain subjective feedback from
participants to seek to understand why older adults view webpages in speciﬁc ways.
Acknowledgement. The authors thank participants of this survey.
References
1. Schaub, H.: Why Senior Citizens Use the Internet, Brookings (2014). https://www.
brookings.edu/blog/techtank/2014/04/11/why-senior-citizens-use-the-internet/
2. Flynn, K.E., Smith, M.A., Freese, J.: When do older adults turn to the internet for health
information? Findings from the Wisconsin longitudinal study. J. Gen. Intern. Med. 21(12),
1295–1301 (2006)
3. McMillian, S., Macias, W.: Strengthening the safety net for online seniors: factors
inﬂuencing differences in health information seeking among older. J. Health Commun. 3(8),
778–792 (2008)
4. Pak, R., Price, M.M., Thatcher, J.: Age-sensitive design of online health information:
comparative usability study. J. Med. Internet Res. 11(4), e45 (2009)
5. Kules, B., Xie, B.: Older adults searching for health information in MedlinePlus – an
exploratory study of faceted online search interfaces. Proc. Assoc. Inf. Sci. Technol. 48(1),
1–10 (2011)
536
A. Inthiran and R. D. Macredie

6. Bol, N., Bergstrom, J.C.R., Smets, E.M.A., Lools, E.F., Strohl, J., van Weert, J.C.M.: Does
web design matter? Examining older adults’ attention to cognitive and affective illustrations
on cancer-related websites through eye tracking. In: Universal Access in Human-Computer
Interaction. Aging and Assistive Environments, UAHCI 2014. Lecture Notes in Computer
Science, vol 8515, pp. 15–23. Springer, Cham (2014)
7. Bol, N., Bergstrom, J.C.R., Smets, E.M.A., Lools, E.F., Strohl, J., van Weert, J.C.M.: How
are online health messages processed? Using eye tacking to predict recall of information in
younger and older adults. J. Health Commun. 21(4), 387–396 (2016)
8. Poole, A., Linell, J.B.: Eye tracking in HCI and usability research. In: Encyclopaedia of
Human Computer Interaction, Chap. 34, pp. 211–219 (2006)
9. Tullis, T.S.: Older adults and the web: lessons learned from eye tracking. In: International
Conference in Universal Access in Human Computer Interaction, pp. 1030–1039 (2007)
10. Lindgaard, G., Fernandes, G., Dudek, C., Brown, J.: Attention web designers: you have
50 ms to make a good ﬁrst impression! J. Behav. Inf. Technol. 25(2), 115–126 (2016)
11. Vann, M.R.: The 15 Most Common Health Problems for Seniors, Everyday Health (2016).
http://www.everydayhealth.com/news/most-common-health-concerns-seniors/
12. Nielsen, J.: How Long Do Users Stay on Webpages (2011). https://www.nngroup.com/
articles/how-long-do-users-stay-on-web-pages/
13. Rocha, Á.: Framework for a global quality evaluation of a website. Online Inf. Rev. 36(3),
374–382 (2012)
Older Adults’ Perception of Online Health Webpages
537

Method for Accessibility Assessment of Online
Content Editors
Tania Acosta1(&), Patricia Acosta-Vargas2, Luis Salvador-Ullauri1,
and Sergio Luján-Mora3
1 Escuela Politécnica Nacional, Quito, Ecuador
tania.acosta@epn.edu.ec, l.salvador@cec-epn.edu.ec
2 Universidad de Las Américas-UDLA, Quito, Ecuador
patricia.acosta@udla.edu.ec
3 University of Alicante, Alicante, Spain
sergio.lujan@ua.es
Abstract. This paper deﬁnes a method for evaluating the accessibility of online
content editors by considering Web Content Accessibility Guidelines 2.0
(WCAG 2.0) and part B of the Authoring Tool Accessibility Guidelines 2.0
(ATAG 2.0). The method includes 63 accessibility features that should be met
by the images, headings and tables, which are inserted through an online content
editor. The compliance of these guidelines contributes to the creation of
accessible content so that visually impaired people using assistive technologies
can easily access the content. Furthermore, the results of this study provide
criteria for those people who have the responsibility of selecting an accessible
online content editor. The proposed method has made it possible to meet the
objective set out in this research document and can be used to evaluate the
accessibility of learning management systems and course management systems.
Keywords: Accessibility  ATAG 2.0  Content editors
Content management systems  CMS  Disabilities  E-learning
Learning management systems  LMS  WCAG 2.0  W3C
1
Introduction
Currently,becauseoftheevolution oftheweb,thewaywecarry outourdailyactivities has
changed drastically [1]. Among the services offered by the web, it is the access to infor-
mation, education, communication, training, among others. These constitute services that
can be accessed from anywhere in the world if there is an internet connection [2].
One of the advantages offered by the web to the elderly or disabled people is the
possibility of improving their quality of life. However, we also have cases in which, the
use of the web has caused frustration in some people [3, 17]. One of these causes is the
impossibility of accessing the content of a web page. Nowadays, web content pro-
duction integrates content editors based on the What You See Is What You Get
(WYSIWYG) philosophy, such as TinyMCE, Atto, Standard and others.
Similarly, the Web offers the possibility of creating and maintaining content through
CMS, Learning Content Management System (LCMS), Learning Management System
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_51

(LMS) based on the ﬁeld of education [4]. These tools typically rely on platform-
independent, browser-based WYSIWYG editors and allow people to produce content
without knowledge about language, web standards or accessibility guidelines [5].
To support website accessibility, some accessibility guidelines and standards have
been proposed in recent years [6]. Initiatives, such as Web Content Accessibility
Guidelines 2.0 (WCAG 2.0) [7], and the Authoring Tool Accessibility Guidelines 2.0
(ATAG 2.0) [8], can guide developers and authors in producing reachable accessible
web content to people with disabilities [10]. Unfortunately, the lack of sufﬁcient
knowledge of developers does not allow the compliance of these guidelines [1, 6].
The research work described in this paper has these objectives, to deﬁne a method
for accessibility assessment of online content editors. To propose a set of features for
the creation of accessible web content by using online content editors and also to
present some recommendations for a better compliance of the accessibility guidelines
WCAG 2.0 and 2.0 in online content editors.
The paper is organized as follows: Sect. 2, describes related works; Sect. 3, pre-
sents the method used for the development of this research; Sect. 4, results are exposed
and discussed; ﬁnally, the conclusions and recommendations are shown in Sect. 5.
2
Related Works
Given the importance of this topic, many studies have been performed on accessibility
issues and web content editing tools. In this section, previous research papers relevant
to this study are considered and described in chronological order based on its pub-
lishing dates.
In 2009, a study performed by Web Accessibility in Mind (WebAIM) summarizes
the 12 major difﬁculties that blind users who use screen readers should overcome [9].
In 2012, a methodology to help identify and solve problems related to the accessibility
of the Course Management Systems (CMS) was presented [10]. They considers the
guidelines recommended by ATAG 1.0 [11] and WCAG 2.0. In 2014, a comparative
study of three LCMSs (Moodle 1.9.4, ATutor 1.6.2 and Sakai 2.6.0) was carried out the
research carried out by [12]. The results underscore accessibility issues, especially for
people with disabilities and older adults. Additionally, they recommend paying
attention to the accessibility issues presented by WYSIWYG editors. In 2015, an editor
prototype was designed for evaluated the accessibility of images, tables, headings and
language. The prototype is aimed at users of content creation tools, considering WCAG
2.0 and ATAG 1.0 [5]. In 2016, a study was conducted by [13]. The authors presented
16 accessibility issues were encountered while users interacted with the Moodle CMS,
considering ATAG 2.0 guidelines. Another research in 2016 identiﬁed 20 desirable
features in an HTML visual text editor for embedding accessible images [14].
The work presented in this article extends previous studies, differentiating in the
following aspects:
• A more in-depth analysis of the recommended WCAG 2.0 and ATAG 2.0 guide-
lines are performed.
• In the case of the image accessibility, the features proposed by [14] increase to 21.
Method for Accessibility Assessment of Online Content Editors
539

• For the case of content that includes accessible tables and headings, features which
comply with standards and guidelines such as ISO/IEC 24751, WCAG 2.0, and
ATAG 2.0 are proposed.
• Each of the accessibility features proposed is based on the appropriate criteria and
levels of conformity given by WCAG 2.0, as well as the success criteria established
by ATAG 2.0 with respect to accessible images.
2.1
Web Accessibility
Web accessibility means that people with some type of disability or the elderly can use
the web. Web accessibility refers to web design that will allow people to perceive,
understand, navigate and interact with the Web, contributing with content [6].
WCAG 2.0 were developed by the World Wide Web Consortium (W3C) and is a set
the recommendations for making Web content more accessible. It includes 4 principles
(perceivable, operable, understandable and robust), 12 guidelines, 61 criteria and 3
levels of conformity which includes A (less demanding but the most important), AA and
AAA (more demanding, but the least important). The guidelines provide basic goals for
making content accessible. Compliance criteria (sufﬁcient and recommended) have
techniques that are applied to the content and technology being used, as described in [7].
Authoring tools are software and services that authors (web developers, designers,
writers, etc.) use to produce web content (static web pages, dynamic web applications,
etc.) [15].
ATAG 2.0 was developed by W3C:
• Part A includes guidelines for designing web content authoring tools that are more
accessible to authors with disabilities.
• Part B includes guidelines for to support and promote the production of more
accessible web content by all authors.
ATAG 2.0 includes 4 principles in each of its parts; the principles are broken down
into 24 guidelines (13 parts A and 11 part B). The guidelines are divided into 89
compliance criteria (33 part A and 56 part B) and 3 compliance levels A, AA, and AAA
[15].
In this study, part B of ATAG 2.0 was considered, because it is focused exclusively
on the generation of accessible content.
2.2
Accessibility of Content Editors
The web content editor tools allow users to create content without previous HTML
knowledge, showing users precisely how content should appear on the screen. Addi-
tionally, web content can be created by users without the knowledge of web standards
or accessibility guidelines [5]. The recommendations published by WCAG 2.0 and
ATAG 2.0 can provide a guidance for developers in generating authoring tools that
create accessible Web content for people with disabilities. However, recent research in
some countries indicates that adherence to accessibility standards on web sites is still
low [16].
540
T. Acosta et al.

In this study, the results of the research work with respect to web content editors
presented by [9, 13], were related as shown in Tables 1 and 2. In the aforementioned
research work, the major problems of accessibility for visually impaired people have
been identiﬁed. It can be observed that the three major accessibility barriers that
visually impaired users have to overcome are related to: images, headings and tables.
For this reason, the present research proposes a method to evaluate the accessibility of
web content editors, considering such elements.
3
Method
In order to propose a set of desirable features for the generation of accessible headings,
tables and images, using online content editors, WCAG 2.0 and ATAG 2.0 guidelines
were considered. The methodology used in this study consists of four major steps as
shown in Fig. 1.
Step 1: Selection of compliance criteria and techniques of WCAG 2.0, related to
accessibility of images, headings and tables. The ﬁrst step consisted of:
1:1 Comprehensive review of WCAG 2.0.
1:2 Detailed review of the techniques of the WCAG 2.0.
Table 1. Items with a high rate of accessibility issues applied to online content editors.
No. Most problematic
items
Description
Related to web
content editors
1
CAPTCHA
Images presenting text used to verify a
human. user
N/A
2
Flash
The presence of inaccessible ﬂash content N/A
3
Ambiguous link
Links or buttons that do not make sense
N/A
4
Missing/Improper
Alt text
Images with missing or improper
descriptions .(alt text).
Applicable
5
Complex or Difﬁcult
Forms
Complex or difﬁcult forms
N/A
6
Poor Keyboard
Accessibility
Lack of keyboard accessibility
N/A
7
Unexpected Screen
Change
Screens or parts of screens that change
unexpectedly
N/A
8
Poor or Missing
Heading
Missing or improper headings
Applicable
9
Too Many Links
Too many links or navigation items
N/A
10
Complex Data Table
Complex data tables
Applicable
11
Lack or “Skip” Links Lack of “skip to main content” or “skip
navigation” links
N/A
12
Inaccessible/Missing
Search
Inaccessible or missing search
functionality
N/A
Method for Accessibility Assessment of Online Content Editors
541

Step 2: Selection of ATAG 2.0 compliance criteria for creating accessible web
content related to images, headings and tables. This step consisted of:
2:1 Detailed review of Part B of ATAG 2.0.
Table 2. Problems users faced while interacting with Moodle applied to online content editors.
No. Issues users faced while interacting with Moodle
Related to web content
editors
1
Control has no label associated with a descriptive text
N/A
2
Page refreshes without prior warning
N/A
3
User is redirected to another page without prior warning
N/A
4
Web page appearance is not uniform
N/A
5
Table is used for layout
Applicable
6
Information is communicated only through images of text
Applicable
7
Task completion procedures are difﬁcult to understand or
follow
N/A
8
English-language text appears despite the prior selection of
Spanish
N/A
9
No button appears for the cancelation of an operation
N/A
10
Screen reader cannot read table well due to a table design
problem
N/A
11
Headings are used inappropriately
N/A
12
Table has many rows, making it difﬁcult to discern overall
table structure
N/A
13
HTML default editor is not accessible
Applicable
14
Text description is incorrect
Applicable
15
No message appears to avoid or correct an error
N/A
16
Text is not read correctly by screen reader
Applicable
Step: 4
Verification of the guidelines defined in step 3.
Step: 1
Selection of compliance criteria, techniques of WCAG 
2.0, related to accessibility of images, headings and tables.
Detailed review of 
techniques for WCAG 2.0
Step: 2
Selection of ATAG 2.0 compliance criteria for 
creating accessible web content related to images, 
headers and tables.
Detailed review of ATAG 
2.0, part B
Step: 3
Definition of desirable accessibility features for online 
content editors, related to creating content that includes 
headings, tables and images.
Comprehensive review of 
WCAG 2.0
Fig. 1. Research methodology ﬂow diagram
542
T. Acosta et al.

Step 3: Deﬁnition of desirable accessibility features for online content editors,
related to the creation of content that includes headings, tables and images.
Step 4: Validation process for step 3. The step consisted of:
Each of the characteristics was veriﬁed in the text editors TinyMCE and
Atto, and based on its applicability, a feedback was made to step 3, per-
forming a debugging procedure if required.
4
Results and Discussion
As part of the implementation and validation process of the method for accessibility
assessment of online content editors, tables with appropriate features required by online
content editors to generate accessible images, headings and tables were established.
Therefore, Table 3 presents 21 features, Table 4 shows 16 and Table 5 includes 19,
supported by the success criteria, level of conformance and techniques of the WCAG
2.0 and the ATAG 2.0 required for compliance related to accessible images, heading
and tables.
Table 3. Features for accessibility assessment of images online content editors.
Features
Content generated by WCAG 2.0
User
interface
Success
criteria
Levels of
conformance
Techniques
ATAG
2.0
1
Allows inserting
alternative text from the
user interface, i.e. <img>
alt attribute
1.1.1
A.
G82, G94, G95, G143,
G144, G196, H2, H24,
H30, H35, H36, H37,
H44, H53, H65, H86, F3,
F13, F20, F30, F38, F39,
F65, F67, F71, F72.
B.2.1.1,
B.2.2.2
2
Allows editing of
alternative text from the
user interface
1.1.1.
A.
G82, G94, G95, G143,
G144, G196, H2, H24,
H30, H35, H36, H37,
H44, H53, H65, H86, F3,
F13, F20, F30, F38, F39,
F65, F67, F71, F72.
B.2.3.1,
B.2.3.2.
3
The name of the input
ﬁeld for the alternative
text represents its function
B.2.2.
4
Provides help about how
to use the alternative text
to improve accessibility
B.4.2.
5
Warns if alternative text
has not been provided
B.4.2.
(continued)
Method for Accessibility Assessment of Online Content Editors
543

Table 3. (continued)
Features
Content generated by WCAG 2.0
User
interface
Success
criteria
Levels of
conformance
Techniques
ATAG
2.0
6
Gives an indication that
the lack of alternative text
produces a decorative
image, it is not accessible
for assistive technology
users, and the attribute
Alt = “” is included in the
HTML code
B.4.2.
7
Warns about the
possibility that alternative
text does not provide the
same content or
information transmitted
by the image
B.4.2.
8
Does not generate
alternative text
automatically
1.1.1.
1.2.1.
A
A.
F30
B.1.1.1,
B.1.1.2
9
Gives an indication after
the insertion that the
alternative text should
properly describe the
image
B.3.2.1
10 Keeps the alternative text
associated with an image
when it was added to the
media library and allows
editing it
B.2.5.1
B.2.5.2
11 Allows inserting a URL
for a long description if
the image is complex
from the user interface,
i.e. <img> longdesc
attribute
1.1.1
A.
G82, G94, G143, G144,
G196, G95, H2, H24,
H30, H35, H36, H37,
H44, H53, H65, H86, F3,
F13, F20, F30, F38, F39,
F65, F67, F71, F72.
B.2.2.2.
12 Allows editing the URL
for the long description
from the user interface
1.1.1.
A.
G82, G94, G95, G143,
G144, G196, H2, H24,
H30, H35, H36, H37,
H44, H53, H65, H86, F3,
F13, F20, F30, F38, F39,
F65, F67, F71, F72.
B.2.3.1,
B.2.3.2.
(continued)
544
T. Acosta et al.

Table 3. (continued)
Features
Content generated by WCAG 2.0
User
interface
Success
criteria
Levels of
conformance
Techniques
ATAG
2.0
13 The name of the input
ﬁeld for the URL of the
long description
represents its function
B.2.2.
14 Provides help about how
to use the long description
to improve accessibility of
complex images
B.4.2.
15 Warns about possibility
that long description does
not provide the same
content or information
transmitted by the image
B.4.2.
16 Gives a suggestion about
using the long description
when the length of the
alternative text exceeds
140 characters
B.4.2.
17 Allows inserting a title for
the image from the user
interface, i.e. HTML title
attribute
1.3.1
A.
G82, G94, G143, G144,
G196, G95, H2, H24,
H30, H35, H36, H37,
H44, H53, H65, H86, F3,
F13, F20, F30, F38, F39,
F65, F67, F71, F72.
B.2.2.2.
18 Allows editing the title
from the user interface
1.1.1.
A.
G82, G94, G95, G143,
G144, G196, H2, H24,
H30, H35, H36, H37,
H44, H53, H65, H86, F3,
F13, F20, F30, F38, F39,
F65, F67, F71, F72.
B.2.3.1.
B.2.3.2.
19 The name of the input
ﬁeld for the title of the
image represents its
function
B.2.2.
20 Warns if only the title
attribute is provided and
not the alt attribute
B.4.2
21 Checks that title and alt
attributes do not repeat the
same text
B.3.1.
Method for Accessibility Assessment of Online Content Editors
545

Table 4. Features for accessibility assessment of heading in online content editors.
Features
Content generated by WCAG 2.0
User
interface
Success
criteria
WCAG
2.0
Levels of
conformance
WCAG 2.0
Techniques
WCAG 2.0
ATAG
2.0
1
Allows assigning the heading from
the user interface
1.3.1.
A.
G115, H42.
B.2.1.1.
2
Allow to edit the heading from the
user interface
1.3.1.
A.
G115, H42.
B.2.1.1.
3
Allows changing the level of heading
from the user interface
1.3.1.
A.
G115, H42.
B.2.2.1.
4
Provides help about how to use the
headings improve accessibility, and
that the heading should describe the
subject or purpose being treated in the
content
B.4.2.
5
Veriﬁes correct nesting of headings
1.3.1.
A.
G115, H42.
B.3.1.3.
2.4.1.
A.
C6, G1,
G123,
G124, H42,
H69.
2.4.6.
AA.
G130
6
Displays an error message in case of
failures nesting of heading
B.4.2.
7
Warns if a heading is not provided
B.4.2.
8
Use heading for the structure of the
page in the template
1.3.1.
A.
G115, H42.
B2.4.1.
2.4.1.
A.
C6, G1,
G123,
G124, H42,
H69.
2.4.10.
AAA.
G141, H69.
3.2.3.
AA.
G61
9
Headings other than those used for
the structure of the page in the
template are available in the headings
list
1.3.1.
A.
G115, H42.
B.2.2.1.
2.4.1.
A.
C6, G1,
G123,
G124, H42,
H69.
2.4.10.
AAA.
G141, H69.
10 Warns about an error if heading is
used to format the text
B.4.2.
11 Warns if the heading does not have a
text or contains an image without
alternative text
B.4.2.
(continued)
546
T. Acosta et al.

Table 4. (continued)
Features
Content generated by WCAG 2.0
User
interface
Success
criteria
WCAG
2.0
Levels of
conformance
WCAG 2.0
Techniques
WCAG 2.0
ATAG
2.0
12 Warns if headings are duplicated
B.4.2.
13 Allows inserting a title for the
headings from the user interface, i.e.
HTML title attribute
1.3.1
A
H89.
B.2.1.1.
14 Allows editing the title of heading
from the user interface
B.2.1.1.
15 Warns if heading is inside a heading
B.4.2.
Table 5. Features for accessibility assessment of tables in online content editors.
Features
Content generated by WCAG 2.0
User
interface
Success
criteria
Levels of
conformance
Techniques ATAG
2.0
1
Gives an indication about that it is not
advisable to use tables to layout the
website. And in the case of using the
table for layout recommends not to insert
caption, summary to the table
1.3.1.
A
H51.
B.4.2.
H39.
H63.
2
Allows inserting headings of row in the
table from the user interface
1.3.1.
A
H63.
B.2.2.2.
3
Allows editing headings of row in the
table from the user interface
1.3.1.
A
H63.
B.2.3.1.
4
Allows inserting headings of column in
the table from the user interface
1.3.1.
A
H63.
B.2.2.2.
5
Allows editing headings of column in the
table from the user interface
1.3.1.
A
H63.
B.2.3.1.
6
Provides help about how the use of
information tabular improves
accessibility
B.4.2.
7
Allows inserting the Caption in the table
from the user interface. (“Caption”
element)
1.3.1.
A
H39.
B.2.2.2.
8
Allows editing the Caption in the table
from the user interface. (“Caption”
element)
1.3.1.
A
H39.
B.2.3.1.
(continued)
Method for Accessibility Assessment of Online Content Editors
547

It is important to note that some cells in Tables 3, 4 and 5 do not contain infor-
mation because WCAG 2.0 is not applicable. In those cases, the ATAG 2.0 success
criteria apply because they are focused exclusively on authoring tools to produce
accessible content.
Table 6, lists nine general features required for the generation of accessible content
by online content editors, and which are applicable to the three elements considered in
this study. For this reason, the WCAG 2.0 guidelines are not applicable to these
characteristics.
Table 5. (continued)
Features
Content generated by WCAG 2.0
User
interface
Success
criteria
Levels of
conformance
Techniques ATAG
2.0
9
Provides help about how the use of the
caption of the table improves
accessibility, and that the caption of the
table should identify the content of the
table
B.4.2.
10 Allows inserting a table summary from
the user interface. (“Summary” attribute)
1.3.1.
A
H73.
B.2.2.2.
11 Allows editing the table summary from
the user interface. (“Summary” attribute)
1.3.1.
A
H73.
B.2.3.1.
12 Provides help about how the use of the
table summary improves accessibility of
tables, and that the summary should not
duplicate the caption
B.4.2.
13 Data cells that relate to more than one
row heading and/or column are shown to
the user such an association. (“Scope”
attribute)
1.3.1.
A
H63.
B.2.2.1.
14 Provides help about row and/or column
headings to improve accessibility of
tables (“Scope” attribute)
B.4.2.
15 Data cells that relate to more than one
row heading and/or column are shown to
the user such an association. (“id” and
“header” attributes)
1.3.1
A
H43
B.2.2.1.
16 Provides help about attributes “id” and
“header” to improve accessibility of
tables
B.4.2.
17 Allows inserting a title for the table from
the user interface
1.3.1
A
H89
B.2.1.1.
18 Allows editing a title for the table from
the user interface
1.3.1.
A
H89
B.2.3.1.
548
T. Acosta et al.

5
Conclusions
The method proposed in this article constitutes a great contribution for the evaluation
process of the accessibility features of online content editor tools, considering images,
tables and heading details which should comply with the guidelines proposed by Web
Content Accessibility Guidelines 2.0 (WCAG) 2.0 and Authoring Tool Accessibility
Guidelines 2.0 (ATAG 2.0).
The method implemented established a total of 63 accessibility features, which
represent desirable options for online content editors in order to insert elements such as
images, headings and tables accessible on web pages. These three elements were
selected since previous studies have detected that they constitute one of the main
accessibility barriers that users with visual disabilities should overcome. One limitation
of this research would be not to address all of the elements that cause accessibility
issues while creating content online.
The beneﬁt of meeting the characteristics highlighted in this research is to improve
the accessibility of content editors, thus enabling people with disabilities to access
information without difﬁculty.
The WCAG 2.0 constitutes a set of useful recommendations, which represented a
supporting pillar for the implementation of the method proposed in this research.
Table 6. Features for online content editor assessment in producing accessible content.
No.
Characteristics of online content editors
User interface
ATAG 2.0
1
The authoring tool allows to copy and paste content then any
accessibility information in the copied content is preserved
B.1.2.2.
2
The authoring tool provides accessible content support features,
which are active by default
B.4.1.1.
3
The authoring tool includes options for reactivating features that
support accessible content
B.4.1.2.
4
The authoring tool does not include options to turn off its
accessible content support features. Opposite case, the authors are
informed that this may increase the risk of content accessibility
problems
B.4.1.3.
5
All accessible content support features are at least as prominent
as features related to either invalid markup, syntax errors,
spelling errors or grammar errors
B.4.1.4
6
The authoring tool provides examples that demonstrate
accessible authoring practices
B.4.2.1.
7
The authoring tool provides instructions for using any accessible
content support features in the documentation
B.4.2.2.
8
The authoring tool provides a tutorial for an accessible authoring
process that is speciﬁc to that authoring tool
B.4.2.3.
9
The authoring tool provides index to support content creation
with accessible headings
B.4.2.4.
Method for Accessibility Assessment of Online Content Editors
549

Part B of the ATAG 2.0, supports and promotes the production of accessible web
contents; however, they are not as developed as WCAG 2.0 since they do not con-
template all the aspects that a tool could provide to the author.
Future work intends to apply the method proposed in this research to evaluate the
accessibility of the most used author tools in the learning management systems.
Another work might also expand the analysis of other elements included in online
content editors such as multimedia elements.
In order to provide equal opportunity and equal access to information published on
the web to everybody regardless of the circumstances, the authoring tools should be
designed and developed considering ATAG 2.0 and WCAG 2.0, to produce accessible
content.
References
1. Acosta, T., Luján-Mora, S.: Most common accessibility errors in websites of Ecuadorian
universities. In: International Conference on Information Systems and Computer Science,
INCISCOS, Quito, pp. 48–55 (2016)
2. Luján-Mora, S.: Por qué es importante que la web sea accessible? In: El Impacto de las
Nuevas Tecnologías (tics) en Discapacidad y Envejecimiento Activo, Tirant lo Blanch,
pp. 94–110 (2016)
3. Hassan Montero, Y.: Factores del diseño web orientado a la satisfacción y no-frustración de
uso. Rev. Esp. Doc. Cient. 29, 239–257 (2006)
4. Buzzi, M.C., Buzzi, M., Leporini, B.: Accessing e-learning systems via screen reader: an
example. In: International Conference on Human-Computer Interaction, pp. 21–30. Springer,
Heidelberg (2009)
5. Minin, H.C., Alemán, J.J., Sacramento, C., Trevisan, D.G.: A WYSIWYG editor to support
accessible web content production. In: International Conference on Universal Access in
Human-Computer Interaction, Los Angeles, pp. 221–230 (2015)
6. Luján-Mora, S.: Web accessibility among the countries of the European union: a
comparative study. Actual Probl. Sci. 1, 8–27 (2013)
7. World Wide Web Consortium, Web Content Accessibility Guidelines (WCAG) 2.0. https://
www.w3.org/TR/WCAG20/
8. World Wide Web Consortium, Authoring Tool Accessibility Guidelines (ATAG) 2.0.
https://www.w3.org/TR/ATAG20/
9. WebAIM:
Screen
Reader
User
Survey
#2
Results.
http://webaim.org/projects/
screenreadersurvey2/
10. López, J., Pascual, A., Menduiña, C., Granollers, T.: Methodology for identifying and
solving accessibility related issues in web content management system environments. In:
International Cross-Disciplinary Conference on Web Accessibility, Lyon, p. 32 (2012)
11. World Wide Web Consortium, Authoring Tool Accessibility Guidelines (ATAG) 1.0.
https://www.w3.org/TR/WAI-AUTOOLS/
12. Iglesias, A., Moreno, L., Martínez, P., Calvo, R.: Evaluating the accessibility of three
open-source learning content management systems: a comparative study. Comput. Appl.
Eng. Educ. 22, 320–328 (2014)
13. Calvo, R., Iglesias, A., Moreno, R.: Accessibility barriers for users of screen readers in the
Moodle learning content management system. Univ. Access Inf. Soc. 13, 315–327 (2014)
550
T. Acosta et al.

14. Sanchez-Gordon, S., Estévez, J., Luján-Mora, S.: Editor for accessible images in e-Learning
platforms. In: 13th Web for All Conference, Montreal, pp. 1–2 (2016)
15. Web Accessibility Initiative: Authoring Tool Accessibility Guidelines (ATAG) Overview.
https://www.w3.org/WAI/intro/atag.php
16. Meyers, J.E., Bartee, J.W.: Improvements in the signing skills of hearing parents of deaf
children. Am. Ann. Deaf 137, 257–260 (1992)
17. Rocha, Á.: Framework for a global quality evaluation of a website. Online Inf. Rev. 36(3),
374–382 (2012)
Method for Accessibility Assessment of Online Content Editors
551

An Approach to Mobile Serious Games
Accessibility Assessment for People
with Hearing Impairments
Angel Jaramillo-Alc´azar1(B) and Sergio Luj´an-Mora2
1 Facultad de Ingenier´ıas y Ciencias Agropecuarias,
Universidad de Las Am´ericas, Quito, Ecuador
angel.jaramillo@udla.edu.ec
2 Department of Software and Computing Systems,
University of Alicante, Alicante, Spain
sergio.lujan@ua.es
Abstract. Serious games allow educating in a fun way in diﬀerent areas
and its development in mobile devices has been growing since the advent
of smartphones. However, accessibility for people with hearing impair-
ments has not been considered as a necessary element in the development
of this type of video games. Unfortunately, there is no model for assessing
the accessibility of serious games for people with hearing impairments,
although some video games development companies recommend general
guidelines for the construction of these applications. This paper presents
a compilation and analysis of the current accessibility guidelines for the
development of video games for people with hearing impairments. As a
case study, an assessment of a mobile game is made to identify its level of
accessibility. An analysis tool for those who wish to develop or evaluate
serious games for people with hearing impairments on mobile devices is
proposed.
Keywords: Accessibility assessment · Accessibility guidelines
Hearing impairments · Mobile devices · Serious games
1
Introduction
The number of people worldwide with impairments is growing and it has been
developed initiatives to promote accessibility as an important feature in the
world around us, and serious games are not the exception. Serious games is a
category of video games designed with the purpose of support the educational
process [21].
Today, the video games development, as part of software industry, generated
around $91 billion worldwide in 2016. Of these, $41 billion were from the video
games market for mobile devices [25]. Due to this growth, serious games have
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_52

Mobile Serious Games Accessibility for Hearing Impaired
553
also increased in mobile platforms and have obtained positive results. This eﬀect
is due to the fact that mobile devices have advantages over ﬁxed equipment and
computers [8]. One of the main advantages of serious games is that they enable
learning through entertainment [16]. It is estimated that the development of
this kind of games will reach $5,400 million in 2020, at a compound annual
growth rate of 16.38% between 2015 and 2020 [19]. In many cases, serious games
allow teachers to apply new teaching methods [24]. This kind of video games
are designed to understand the necessities of students in the acquisition of skills,
knowledge [9] and the achieving of learning outcomes [11].
Cisco says that 563 million mobile devices and connections to the Internet
were added in 2015 [3]. According to [27], the number of worldwide mobile users
is expected to increase over 6.2 billion at the end of 2018 and than roughly 84%
of the world population will be using mobile technology. This is because people
prefer to purchase a mobile device to access the Internet instead of a computer.
In education, mobile devices oﬀer a variety of ways to learn, communicate and
collaborate [10]. For example, mobile learning (m-learning) allows users of mobile
devices to be able to learn from any device at anytime, anywhere [6].
More than one billion people live around the world with some form of dis-
ability, it is about 15% of the world population [29]. In addition, in 2010 around
360 million people worldwide had disabling hearing loss, and 32 million of these
were children [31]. These people usually live with social, educational and enter-
tainment limitations because their impairments.
Furthermore, we can deﬁne accessibility as the ability of an object to be
used in spite of the condition or disability of a person [1]. Accessibility in video
games is a factor that is beginning to be considered by software developers. How-
ever, there exist several players with hearing disabilities. It is the case of Chris
Robinson [17], known as Phoenix, who is deaf and he is a ﬁghting game player.
Another case is Adam ‘Loop’ Bahriz who is a legally blind and deaf Counter-
Strike gamer [23]. These cases exist because some video games are accessible to
people with diﬀerent disabilities.
According to [30], there are four grades of hearing impairment based on a
decibel scale representing hearing loss:
– Slight/mild hearing loss (26–40 dB).
– Moderate hearing loss (41–60 dB).
– Severe hearing loss (61–80 dB).
– Profound hearing loss (over 81 dB).
People with mild, moderate and severe hearing loss are grouped under the
term hard of hearing; whereas people with profound hearing are deaf. Both, hard
of hearing and deaf together represents the total number of cases of hearing
impairments [31].
Taking into account the large number of people with hearing impairments,
it is important to determine what accessibility parameters should be consid-
ered for the development of serious games. In previous works, we have studied
some methods that assess the mobile video games accessibility for people with

554
A. Jaramillo-Alc´azar and S. Luj´an-Mora
visual [15] and cognitive [14] impairments. Thus, in this article a method to eval-
uate the accessibility of mobile video games for people with hearing impairments
is proposed.
The rest of this article is organized as follows. In Sect. 2, we present a com-
pilation of accessibility guidelines for the development of video games for people
with hearing impairments. In Sect. 3, we propose a categorization of accessibil-
ity guidelines for mobile serious games. Next, in Sect. 4, we evaluate a mobile
serious game with the categorization proposed in Sect. 3. In Sect. 5, we discuss
the results of our evaluation. Finally, in Sect. 6 we conclude the research and we
outline our future works.
2
Accessibility Parameters
Each disability has its own particularities and therefore its own accessibility
parameters. There are accessible video games for people with hearing impair-
ments. This is the case of MusicPuzzle, a video game for encouraging active
listening among deaf and hard of hearing people [18]. Also Memosign [2] and
Robostar [22] are video games that promotes learning sign language. There
are mobile video games that have accessibility features for people with hear-
ing impairments. Her Interactive, for example, is a video game company that
includes subtitles and sometimes subtitled sounds in most of the video games
it develops. Thus, we have Nancy Drew: Ghost of Thornton Hall [12] which
includes subtitles in all of the video game’s conversations as we can see in Fig. 1.
This allows the video game to be used by deaf or hard of hearing people.
Fig. 1. Subtitles in Nancy Drew: Ghost of Thornton Hall mobile video game
This section discusses several guidelines for the development of video games
on mobile devices for people with hearing impairment. The guidelines, for the

Mobile Serious Games Accessibility for Hearing Impaired
555
most part, come from video games development groups. They focus their work
on proposing features that provide the opportunity for people with disabilities
to enjoy video games. This contribution is of great importance because it oﬀers
the possibility of learning through serious games.
2.1
Guidelines Review
Several authors describe accessibility parameters and guidelines for video games.
They analyze them from the perspectives of the disability and/or the type of
device that is used. In our research we will consider guidelines for hearing impair-
ment.
The AbleGamers Foundation explains some important accessibility features
that can be included into a video game [26]. It deﬁnes three levels of accessibility,
from a minimal scenario to an ideal one: Level one (Good), Level two (Better),
Level three (Best).
These levels cover from basic to advanced accessibility items that should be
considered in development of video games. Taking into account hearing impair-
ments, this study deﬁnes one, two and two accessibility guidelines for level one,
two and three, respectively. Further, this paper presents a section that describes
six guidelines for mobile gaming accessibility: touch, multi-touch, alternative
buttons, high contrast, colorblind options and speed settings.
On the other hand, the International Game Developers Association (IGDA)
with its Game Accessibility Special Interest Group, describes the importance
and beneﬁts of accessible games [13]. Among them, the most prominent are:
user satisfaction, reach the largest possible audience and the opportunity to
learn new skills. This paper deﬁnes nine approaches for developers who want to
provide hearing accessibility to their games.
The Game Accessibility Guidelines [7] propose a list of guidelines for the
development of video games for people with disabilities. They deﬁned three cat-
egories: basic, intermediate and advanced. These levels try to balance the number
of people who beneﬁt, the impact or diﬀerence made for those people and the
cost to implement in video games. In the case of hearing impairment, this group
deﬁnes three, ten and two guidelines for the basic, intermediate and advanced
categories, respectively. A step-by-step guide is also provided to work with these
guidelines.
The MediaLT company, in its game accessibility UPS-project, deﬁnes a list
of guidelines for the development of entertaining software for people with dis-
abilities [20]. This company determines ﬁve categories: level/progression, entries,
graphics, sounds and installation and conﬁguration, which have six, four, ﬁve,
six and four guidelines, respectively.
In [5], the authors deﬁne a set of practical design guidelines for video games in
education of deaf children. They propose three categories of guidelines: interface
environment, gameplay mechanics and educational content. In the ﬁrst category,
related to our work, the authors deﬁne nineteen guidelines.

556
A. Jaramillo-Alc´azar and S. Luj´an-Mora
An analysis of all the guidelines is made in the following subsection. Likewise,
it will be determined those that are required in the mobile devices and their
respective levels.
3
Guidelines Analysis Method
Once an overview of the diﬀerent guidelines has been given, they need to be ana-
lyzed and grouped into a new categorization. The ﬁrst step was to propose new
categories. These categories group the guidelines according to the complexity
that exists in its implementation and the beneﬁt it provides to the people with
hearing impairments. The perspective of complexity is given from the point of
view of the developer while the beneﬁt is deﬁned from the vision of the end user
of the video game.
An analysis was made considering the categories proposed by the cited
authors in the previous section. In addition, the guidelines were revised and
this led to the deﬁnition of three new categories or levels aligned with those
proposed by most of the authors reviewed. These new deﬁned categories cover
the full spectrum of guidelines that have been considered:
– Low Level - Good: It refers to simple implementation complexity and good
accessibility features.
– Medium Level - Better: It refers to a medium implementation complexity and
better accessibility features.
– High Level - Best: It refers to a high implementation complexity and the best
accessibility features.
Once the levels were deﬁned, each guideline was analyzed with the objective
of mapping them to the corresponding category. In this process, it was important
to evaluate the ease of implementation and the beneﬁt to people with disabil-
ities. With the guidelines mapped in their respective category, we proceeded
to eliminate existing duplicates. This process was done by carefully comparing
the guidelines descriptions and considering that each author handled their own
language to describe them.
After this analysis, we proceeded to verify, according to its description, the
guidelines that could be evaluated in mobile video games. This process was done
considering the characteristics and nature of the mobile devices, for example
Android and iOS accessibility features. The result of the whole process is showed
in Table 1.
Once the parameters were determined, an indicator was deﬁned for each
accessibility level. This indicator allows quantifying the accessibility of a serious
game at each of the levels proposed. Each guideline should be analyzed in the
video game and its compliance should be scored. The indicator for each level is
given by Eq. 1:
i=n

i=0
xi = x1 + x2 + . . . + xn
(1)

Mobile Serious Games Accessibility for Hearing Impaired
557
Table 1. Mobile video games guidelines per Level
Level
Video games guidelines
Mobile
guidelines
Low Level -
Good
Subtitles
Yes
Customizable fonts (color, sizes)
Yes
Switch on/oﬀgraphic elements
Yes
Use simple language
Yes
No essential information in audio alone
Yes
Adequate interface for the player age
Yes
Appropriate words-per-minute
Yes
Vibratory alerts
Yes
Easy installation
No
Medium
Level - Better
Use explicit visual feedback
Yes
Background noise to minimum during speech
Yes
Possibility for repetition
Yes
Ambient noise and information as text output
Yes
Save settings
Yes
Visual indication of who is currently speaking
Yes
Pause while text is being readed
Yes
Simple evaluation system grades
Yes
Adjustable speed and size of pointers and markers
No
Switch on/oﬀexplanations of pictures and actions
No
High Level -
Best
Simple to diﬃcult progression
Yes
Speed settings
Yes
Sound compass or voiced GPS
Yes
Provide separate volume controls
Yes
Provide a stereo/mono toggle
Yes
Adjustable sensitivity/error tolerance
Yes
In-game tutorials
Yes
Sign language
Yes
Allow several diﬀerent input and output devices
No
Support text and voice chat for multiplayer
No
Provide visual means of communicating in multiplayer
No
Keyboard navigation of all controls, visual and spoken
No
Wizard for product adaptations
No
Direct access to individual activities and/or secret areas No

558
A. Jaramillo-Alc´azar and S. Luj´an-Mora
Where n refers to the number of guidelines that are available for each category
and x takes the value of ‘1’ or ‘0’ taking into account if the game complies with
it. For a general indicator of accessibility, we propose the summation of each
level indicator as follows in Eq. 2:
j=3

j=1
yj = y1 + y2 + y3
(2)
In this case, j corresponds to the number of levels proposed (1 - low, 2 -
medium, 3 - high) and y is the value scored in each level by Eq. 1. As result, we
will have a value that will allow us to have a general accessibility rating of the
video game.
4
Serious Games Accessibility
In this work, we selected a serious game application from Google Play with
the objective of evaluating its accessibility with the guidelines proposed in the
previous section. The search string that was used is “serious games” and the
search results presented several entertainment applications of which we show
the ﬁrst ﬁve in Table 2.
Table 2. Google play serious games search result
Game
Category
Serious Games CEOE
Educational
Dead Target: Zombie Shooting
Action
Metal Soldiers 2
Action
Carreras en Bicicleta Gratis
Races
The Walking Dead: Season One Adventure
The unique mobile app oriented to the educational category is Serious
Games CEOE. This app, developed by Confederaci´on Espa˜nola de Organiza-
ciones Empresariales (CEOE), was selected considering the educational approach
of this work. This mobile game contains ﬁve serious games to promote healthy
in the daily life and especially in the workplace: Stress Management, Healthy
Life, Move It, The Oﬃce Room and Business Up [4]. This app was downloaded
from Google Play and installed in a Samsung Galaxy S6 with Android 7.0 for
its analysis.
4.1
Accessibility Assessment
Once the games were identiﬁed, the assessment was carried out considering the
guidelines for mobile devices in Table 1. Equation 1 was used to quantify the

Mobile Serious Games Accessibility for Hearing Impaired
559
accessibility of each game in each level proposed. For this, each accessibility
guideline was analyzed in each game and it was scored whether or not complies
with it. To ﬁnalize the assessment, it was necessary to determine the total value
of accessibility of the video games. Equation 2 was used and the result of all the
process is presented in Fig. 2.
Fig. 2. Serious games (CEOE) accessibility assessment
In general, each bar shows the value of accessibility reached in each level and
also, in the top of the bar with a big size, it shows the total value of accessibility
of each video game. It is important to take into account that the maximum value
of accessibility that can be reached is 24, due to the number of guidelines that
were determined, and the maximum score obtained was 8 for the Move It and
Healthy Life games.
5
Discussion of Results
Each game meets at least one of the accessibility guidelines deﬁned for mobile
devices. In the Low Level-Good, four of the ﬁve games make use of a simple
language that supports the sequence of the game. In addition, all games have
subtitles that allows to understand the diﬀerent instructions of them. We can
see an example of this feature in Fig. 3. At this same level, no game considers
vibratory alerts or font customization, whether in size, color or type.
Taking into account the Medium Level - Better, all games give the player
the possibility to repeat the stages and four of the ﬁve games allow the user to
save the settings. Also, in three games it was evidenced that they propose simple
grades of evaluation.
On the other hand, in this level no game oﬀers explicit visual feedback or
ambient noise as text output. Finally in this level, just one game presents the
option to have visual indication of who is currently speaking.

560
A. Jaramillo-Alc´azar and S. Luj´an-Mora
Fig. 3. Subtitles in healthy life serious game
In the High Level - Best, the scenario is minimal because most of the games
evaluated do not comply with the guidelines of this level. Only three games
include in-game tutorials and two oﬀer simple to diﬃcult progression.
The total value of accessibility obtained for each video game is in the range
of 2 to 8, that is, between 8.3% and 33.3% on the maximum value of possible
accessibility. The video game with the lowest value of accessibility is The Oﬃce
Room and the highest are Move It and Healthy Life. These results allow us to
understand that mobile video games in some cases lack accessibility features and
in others they comply fairly.
To ﬁnalize this analysis, we must note that the mobile serious games analyzed
are just a speciﬁc case that was taken for the evaluation of accessibility guidelines
and it is important to note that the results vary from one game to another. This
analysis aims to highlight the importance of taking this type of guidelines into
account in the development of mobile serious games. Additionally, this research
only considered guidelines that ﬁt the reality of mobile devices. The rest, which
are also important, should be considered for video games in general.
6
Conclusions and Future Work
This research aims to improve the entertainment and learning conditions of peo-
ple with hearing impairments that can not access video games because of their
condition, especially serious games. Due to the growth of this type of games,
and their contribution to the teaching process, accessibility parameters should be
considered in their design and in their implementation. The use of non-accessible
serious games goes against the Article 24 - Education of the United Nations Con-
vention on the Rights of Persons with Disabilities [28], considering that avoids
people with impairments having access to education on equal terms with a person
without disabilities.
In recent years, the use of mobile devices has increased. This has given way
to the development of a greater number of applications available to a greater
number of people. Users of mobile devices tend to demand a greater amount

Mobile Serious Games Accessibility for Hearing Impaired
561
of video games, which is a great opportunity for the serious games market. The
demand of accessible video games for people with disabilities is growing and that
is why many initiatives are deﬁning good practices for the development of video
games for this vulnerable group of people.
This study allow us to propose a method for developing accessible serious
games in mobile devices, given the appropriate features that satisfy the needs of
people with hearing impairments. On the other hand, it will be possible to test
with people who have this impairment in order to assess the eﬀectiveness of the
features of serious games identiﬁed in this work. Similar research can be done
considering other disabilities. The long-term objective is to propose a method of
analysis and development of accessible serious games for any type of disability.
There are several disabilities and this article only analyzed the hearing
impairment. That is why it is relevant to continue analyzing other disabilities
for a more general view and to be able to oﬀer greater accessibility.
References
1. Accessible University. Deﬁning Accessibility (2016). https://goo.gl/Hwr2vC
2. Bouzid, Y., Khenissi, M.A., Jemni, M.: Designing a game generator as an educa-
tional technology for the deaf learners. In: 5th International Conference on Infor-
mation Communication Technology and Accessibility, pp. 1–6 (2015)
3. Cisco. Cisco Visual Networking Index: Global Mobile Data Traﬃc Forecast Update,
2016/2021 White Paper - Cisco (2017). https://goo.gl/Qr9c4W
4. Confederaci´on Espa˜nola de Organizaciones Empresariales (CEOE) (n.d.) Serious
Games CEOE - Aplicaciones de Android en Google Play. https://goo.gl/NRAvJU
5. Dos Passos Canteri, R., S´anchez Garc´ıa, L., Amara Felipe de Souza, T., Andrade
Iatskiu, C.E.: Video games in education of deaf children. In: 17th International
Conference on Enterprise Information Systems, vol. 3, pp. 122–129 (2015)
6. Floro, N.: Mobile learning. American Society for Training & Development (2011)
7. Game Accessibility Guidelines (n.d.) A straightforward reference for inclusive game
design. https://goo.gl/Mj7gGt
8. GameLearn. The future of serious games through the lens of mobile devices (2015).
https://goo.gl/9hu2w4
9. Ghannem, A.: Characterization of serious games guided by the educational
objectives. In: Second International Conference on Technological Ecosystems for
Enhancing Multiculturality, pp. 227–233 (2014)
10. Gikas, J., Grant, M.M.: Mobile computing devices in higher education: student
perspectives on learning with cellphones, smartphones & social media. Internet
High. Educ. 19, 18–26 (2013)
11. Guill´en-Nieto, V., Aleson-Carbonell, M.: Serious games and learning eﬀectiveness:
the case of it’s a Deal! Comput. Educ. 58(1), 435–448 (2012)
12. Her Interactive. Nancy Drew Games: Ghost of Thornton Hall (2014). https://goo.
gl/8xf88K
13. International Game Developers Association (IGDA) — Game Accessibility SIG.
Accessibility in Games: Motivations and Approaches (2004). https://goo.gl/
3gUaV2
14. Jaramillo-Alc´azar, A., Luj´an-Mora, S.: Accessibility assessment of mobile serious
games for people with cognitive impairments. In: International Conference on Infor-
mation Systems and Computer Science (2017, in press)

562
A. Jaramillo-Alc´azar and S. Luj´an-Mora
15. Jaramillo-Alc´azar, A., Luj´an-Mora, S.: Mobile serious games: an accessibility
assessment for people with visual impairments. In: International Conference Tech-
nological Ecosystems for Enhancing Multiculturality (2017, in press)
16. Koster, R.: Theory of Fun for Game Design, 2nd edn. O’Reilly Media, Inc. (2013)
17. Kotaku. Deaf Gamer Founds A Fighting Games Team For Players Like Him (2017).
https://goo.gl/y5cWMQ
18. Li, Z., Wang, H.: A mobile game for encouraging active listening among deaf and
hard of hearing people: comparing the usage between mobile and desktop game
(2015). https://goo.gl/tkUC3r
19. Markets and Markets (n.d.) Serious Game Market worth $5,448.82 Million by 2020.
https://goo.gl/WbdR7t
20. MediaLT. Guidelines for the development of entertaining software for people with
multiple learning disabilities (2004). https://goo.gl/DsG9AN
21. Michael, D.R., Chen, S.L.: Serious Games: Games That Educate, Train, and
Inform. Muska & Lipman/Premier-Trade (2005)
22. ¨Ozkul, A., K¨ose, H., Yorganci, R., Ince, G.: Robostar: an interaction game with
humanoid robots for learning sign language. In: IEEE International Conference on
Robotics and Biomimetics, pp. 522–527 (2014)
23. PCGamesN. Legally deaf-blind CS:GO player Loop oﬀered pro streamer contract
after community support (2017). https://goo.gl/MwKzeJ
24. Sauv´e, L., S´en´ecal, S., Kaufman, D., Renaud, L., Leclerc, J.: The design of generic
serious game shell. In: International Conference on Information Technology Based
Higher Education and Training, pp. 1–5 (2011)
25. SUPERDATA (n.d.) SuperData Research — Games data and market research —
Market Brief — Year in Review (2016). https://goo.gl/6drfXB
26. The AbleGamers Foundation. A Practical Guide to Game Accessibility (2012).
https://goo.gl/SraJMj
27. The Radicati Group, Inc. Mobile Statistics Report, 2014-2018 (2014). https://goo.
gl/Gmk7q6
28. United Nations General Assembly. Convention on the rights of persons with dis-
abilities (2017). https://goo.gl/ZuFucZ
29. World Health Organization. World report on disability (2011)
30. World Health Organization. Grades of hearing impairment (2016). https://goo.gl/
1pWAZd
31. World Health Organization. Deafness and hearing loss (2017). https://goo.gl/
35F73p

Real Time Driver Drowsiness Detection Based
on Driver’s Face Image Behavior Using
a System of Human Computer Interaction
Implemented in a Smartphone
Eddie E. Galarza(&), Fabricio D. Egas, Franklin M. Silva,
Paola M. Velasco, and Eddie D. Galarza
Universidad de las Fuerzas Armadas - ESPE, Sangolquí, Ecuador
{eegalarza,fmsilva,pmvelasco,gqeddie}@espe.edu.ec,
fabricio_pa21@hotmail.com
Abstract. The main reason for motor vehicular accidents is the driver
drowsiness. This work shows a surveillance system developed to detect and alert
the vehicle driver about the presence of drowsiness. It is used a smartphone like
small computer with a mobile application using Android operating system to
implement the Human Computer Interaction System. For the detection of
drowsiness, the most relevant visual indicators that reﬂect the driver’s condition
are the behavior of the eyes, the lateral and frontal assent of the head and the
yawn. The system works adequately under natural lighting conditions and no
matter the use of driver accessories like glasses, hearing aids or a cap. Due to a
large number of trafﬁc accidents when driver has fallen asleep this proposal was
developed in order to prevent them by providing a non-invasive system, easy to
use and without the necessity of purchasing specialized devices. The method
gets 93.37% of drowsiness detections.
Keywords: Drowsiness detection  Artiﬁcial vision  Mobile app
PERCLOS  Face detection
1
Introduction
Sleeping is one of the basic needs of the human being, sleep lack causes the body to
react inefﬁciently, reducing both reaction time and wakefulness, also produce low
alertness and lose of concentration which reduces the ability to perform activities based
on care that is necessary in the case of driving a car.
According to many researches drowsiness is related to thousands of trafﬁc accidents
each year, the accidents produces approximately 50% of death or serious injuries [1], as
they tend to be impacts at high speed because the driver who has fallen asleep cannot
This work was supported by the Universidad de las Fuerzas Armadas, Sangolquí – Ecuador.
Eddie E. Galarza (IEEE member), Franklin M. Silva, Paola M. Velasco and Eddie D. Galarza
work at the University of the Fuerzas Armadas at the campus in Latacunga City. Fabricio D. Egas
is an Electronic Engineer graduated in that university.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_53

brake or deviate to avoid or reduce impact. To mitigate these accidents, manufacturers
have developed drowsiness detection systems that recognize signs of possible
drowsiness, alerting the driver to their condition [2].
In the research: “A smartphone-based driver safety monitoring system using data
fusion. Sensors”, Lee and Chung [3] propose a method to monitor driver safety levels
using a data fusion approach such as: eye characteristics, variation of biological signals,
temperature inside the vehicle and vehicle speed. This system is developed as an
application for an Android-based smartphone, where measuring security-related data
that does not require additional costs or additional equipment. The system has an
efﬁciency of 96% to detect that the driver is awake and 97% to detect that he is
asleep. This information allows knowing the signs that shows a sleepy driver.
In work “Detection of fatigue using Smartphone aims to use a smartphone (with
Android operating system or IOS) to detect fatigue in the driver” Roberson et al. [4]
uses the front camera of the smartphone to capture images of the driver and then uses
advanced algorithms of computer vision to detect his face and eyes. Rotation and tilting
of the head and blinking of the eyes are detected as indicators of fatigue. The smart-
phones is used to assist driver using front and rear camera [5], for drowsy driving
detection system [6], for the wavelet analysis of heart rate variability and a support
vector machine classiﬁer [7], and for identiﬁcation of dangerous driving situations [8].
The PERCLOS (Percent of the time Eyelids are CLOSed) metrics is used to
measure drowsiness in the work “Eye tracking based driver fatigue monitoring and
warning system” [9]. The system estimates with a non - parametric methods for
detecting drowsiness, the vehicle steering wheel variability is considered to determine
the amount of drowsiness because drivers makes variability greater as driver become
more drowsy. The PERCLOS metrics for alerting driver is used in [10] to detect
drowsiness in heavy vehicles, to monitor and alert the driver [11], for line departure
warnings [12] and to detect drowsiness conditions in drivers [13].
The HCI systems allows to interrelate the human being with an electronic device
(computer) which is capable of giving solutions to a great number of problems that can
affect him. The development and use of HCI has been very important, so it must be
implemented with adequate usability criteria [14] and satisfy users’ needs efﬁciently [15].
A relevant aspect is that not only sought a simple interaction also sought to assist humans
with special skills to satisfy their needs even overcoming their limitations [16–19] and
can be implemented using low cost systems [20–25]. The smart phones being mass-use
are actually a low-cost computer, if are used in an HCI would allow to massify its use and
therefore offer greater solutions to improve the quality of life of any person satisfying
their needs even if the person presents some limitation in one or more of their senses.
The objective of this work was to implement a surveillance system to the vehicular
driver based on artiﬁcial vision techniques and implemented in a smartphone in order to
detect and alert when the driver have drowsiness signs. To achieve this objective it was
analyzed other works related with detecting drowsiness in drivers, the drowsiness
symptoms in vehicle drivers; we identify the technical parameters and algorithms that
allow to process signals of the state of drowsiness. In this work we present a developed
drowsiness detection algorithm, the interface in which the state of drowsiness is dis-
played and the necessary adjust to get the correct functioning of the implemented
system.
564
E. E. Galarza et al.

Some of the aspects that are included in this work have been considered in the
investigations referred to, but it differs essentially in the use of the new systems for
digital image processing in smartphones.
1.1
Drowsiness Characteristics
Drowsiness is a physiological state with a tendency to fall asleep. Technically,
drowsiness is different from the fatigue that is the lack of willingness to continue
performing the same activity. Fatigue occurs by performing tasks that are always
performed in the same way using the same muscle groups, their repetition rate is high
and are usually performed by adopting forced postures such as monitoring a screen
[26]. A person may be fatigued without being drowsy, but conditions that produce
fatigue such as driving cars over great distances unmask the presence of physiological
drowsiness, but do not cause fatigue.
Among the effects of being sleepy we have a lowered wakefulness, reaction time,
psychomotor coordination and decreased information processing. For the driver the
main effect is the progressive withdrawal of attention in demands of road, trafﬁc and
signaling, which causes a low driving performance producing accidents [27]. People
who are drowsy have signs like frequent blinking, rubbing eyes, repeated yawning,
head tilt, and distractions are the most important among which it can mention.
1.2
Drowsiness Detection Methods
Detection methods are divided into two main groups: methods based on driver per-
formance and methods based on driver status [28]. The methods centered on driver
status are divided into two subgroups: methods that use physiological signals and
methods that use artiﬁcial vision techniques. Figure 1 shows classiﬁcation of drowsi-
ness detection methods.
Fig. 1. Drowsiness detection methods.
Real Time Driver Drowsiness Detection Based on Driver’s Face Image Behavior
565

The drowsiness detection using patterns analysis are generated based on measur-
able variables that are obtained experimentally. These variables can be speed, accel-
eration, braking, gear shifting, hand pressure on the steering wheel and the car’s path in
the road lane. This method has the disadvantage that its modeling depends on the
characteristics of each car and the way of driving that is speciﬁc to each driver.
By the use of the image processing, driver states can be determined. From the
image of the face it can be detected if the driver is awake or asleep. The drowsiness of
the driver can be determined because the driver is trying to close his eyes [29]. This
method has the advantage of not being intrusive and can be used techniques like the
template pairing technique where a driver templates is deﬁned. The technique of the
behavior of the eyes, calculates the blinking frequency and the time interval of eyes
closing in order to determine the rate of drowsiness.
One of the most used indexes to calculate the level of sleepiness is PERCLOS
(Percent of the time Eyelids are CLOSed), which measures the percentage of time a
person’s eyes are closed at 80% to 100% in a period. According to a study by Walter
Wierwille and colleagues [30], PERCLOS is among the most important real-time alert
measures for vehicle drowsiness detection systems.
Perclose ¼
Closed eyes time= closed eyes time þ open eyes time
ð
Þ
ð
Þ  100
ð1Þ
The Yawning technique is based on the driver’s yawn frequency [29]. The opening
of the driver mouth is greater when yawning than when speaking normally. The mouth
is compared with a reference point experimentally obtained by the programmer and the
number of times the driver has yawned is calculated to generate a drowsiness index.
The analysis based on changing physiological measures use sensors that measure
physiological variables of the human body to analyze states of drowsiness. These
variables are the heart rate, brain activity, heart rate variability, respiration, peripheral
skin temperature, and blood pressure [31].
1.3
Mobile Applications in Smartphones
Smartphones are electronics devices that combines the functionalities of a mobile
phone and the functionalities of a computer. The term intelligent is used commercially
to refer to the ability to use as a handheld computer, sometimes leading to a replace-
ment of a personal computer.
Smartphones are built on a mobile platform similar to the operating system of a
computer, which makes it equally vulnerable to viruses. Among the basic character-
istics of a smartphone we have that includes an operating system, it can send and
receive calls and text messages, it has multimedia services, includes basic applications
such as clock, alarm, calendar, calculator, games, digital agenda, it has internet con-
nection, includes front and rear video cameras, it can read and edit documents, it has
sensors like gyroscope, accelerometer, barometer, thermometer, etc.
Google provides a set of open source visual programming interfaces (APIs) for
mobile devices [32], the APIs can make face detection and tracking, barcode scanner,
text recognition. The API also offers information about the state of facial features like
open eyes and smiles [33].
566
E. E. Galarza et al.

2
Materials and Method
This section describes different aspects of the system considered in its implementation;
they include the functional requirements as well as the tools used and devices selected
for system testing in different study cases.
The used algorithm processes the color information present in the image, con-
verting it to grayscale. To determine the face in the image, the image is divided in sub
regions determining whether the subregion is a face or not. The use of this algorithm
means a time saving and only the subregions that contains a face are processed.
The gesture detection is done from the residual error that is modeled considering a
linear combination of facial movement models. A similar model is considered to detect
the position and inclination of the face. It includes a system that allows detecting facial
gestures in the presence of head movement.
Figure 2 shows the ﬂowchart of the system. The code used to implement the
algorithms was created taking into account the limitations that have mobile devices like
the limited content or features in the interface, slow or limited hardware and use
situations. Their success in functionality is based on the way they are designed and
optimized by company that owns Android.
A system general scheme is seen in Fig. 3 in which it is shown that the driver must
take into account the alerts presented by the system, while the smartphone is in charge
of processing the information acquired from the driver’s face in real time. The main use
case diagram is shown in Fig. 4.
The user interface of the system is presented in Fig. 5, in which two buttons are
presented that allow to show information regarding the situation of the driver, which is
necessary for research aspects. In addition, there are indicators that shows the presence
Fig. 2. Flowchart of the drowsiness detection system.
Real Time Driver Drowsiness Detection Based on Driver’s Face Image Behavior
567

Fig. 3. General scheme of the drowsiness detection system.
Fig. 4. Main use case diagram.
Fig. 5. System user interface
568
E. E. Galarza et al.

or absence of the driver’s face at the interface, open eyes, left or right face tilt, left or
right distraction, and yawn presence are included. Technical information is also pre-
sented with respect to the operating characteristics of the system that the user does not
necessarily need to know.
Subjects: To perform the system tests, 20 drivers, 10 men and 10 women of
different ages were included, each of whom was accompanied by a “co-pilot” who was
in charge of managing the controlled events of sleepiness only when the external
conditions in the road and close to the vehicle were safe.
The events to validate the system were the following: yawning detection, front and
lateral assent of the head, left and right distraction and blinking. In the process of
validating the system, the co-pilot will direct them, making the drivers repeat 10 times
the test. The number of correctly detected events, false positives, false negatives, and
the efﬁciency of the implemented system were registered.
3
Results
This section presents the results on the detection of visual indicators of drowsiness.
Collecting the data set to properly evaluate the system is a challenge, this is because
dangerous drowsiness events are not guaranteed to occur during daily driving for
application testing.
Table 1 presents the results of the detection of somnolence considering the normal
operation of the system in which the responses are obtained from each of the drivers
that were submitted to the addresses issued by the co-pilot who recorded the results.
The level of total hits on detection represents an average percentage of 93.37%.
Table 2 presents the results obtained by placing additional objects on the clothes of
the drivers, which in this case were the caps and glasses. The average percentage of hits
is 88.5%.
Table 3 presents the results obtained by considering the hair covering driver’s face.
The drivers were women and the average percentage of hits was 81.4%.
Table 1. Detection levels for drowsiness parameter under normal conditions.
Test
Number of
observations
Number of
hits
Percentage of
hits
Yawn detection
170
143
84.11%
Front nodding
200
184
92.0%
Assent of the head to the
right
200
190
95.0%
Assent of the head to the left
200
191
95.5%
Distraction to the right
200
184
92.0%
Distraction to the left
200
193
96.5%
Blink detection
200
197
98.5%
Real Time Driver Drowsiness Detection Based on Driver’s Face Image Behavior
569

Los resultados presentados indican la eﬁciencia del sistema que es de alto nivel y
son comparable e inclusive superiores a otros sistemas a los que se hace referencia en el
presente trabajo. Los menores niveles de acierto se presentan cuando los usuarios
incluyen elementos que no permiten ser identiﬁcados los gestos de la cara de manera
correcta, pero que a pesar de aquello sus niveles son satisfactorios.
The presented results indicate the efﬁciency of the system that is of higher level and
even better than other systems referred to in this work. The lower levels of accuracy
occur when users include elements that do not allow to correctly identify the face
gestures, but despite that, their levels are satisfactory.
4
Conclusions
The study has shown promising results in applying the vehicular driver surveillance
based on artiﬁcial vision techniques and implemented in a smartphone. The imple-
mented system allows an efﬁcient detection of the indicators that appear in drowsiness,
as long as the measurements are carried out under the established conditions. The
correct functioning of the system depends on these conditions.
The increase in the processing characteristics in smartphones made possible to
develop an application of artiﬁcial vision, capable of detecting the face and visual
indicators present in a person who suffers from drowsiness such as: yawning, head
movements and the state of the eyes.
The symptoms that people present during the transition between awake and asleep
are appearing as the intensity of drowsiness increases. The greater intensity of
drowsiness means a higher loss of concentration and a lower ability of driver reaction. In
development this work, the implementation of 3 levels of sleepiness allows the system to
alert the driver about their condition, not necessarily at a critical level where it may have
serious repercussions, rather at early levels where drowsiness is just emerging.
An HCI could be implemented using smartphones like shown in this work, which
would allow massify their use and therefore provides greater solutions improving the
quality of life of the people even if has specials skills.
Table 2. Detection levels for different drowsiness parameter under special conditions.
Test
Number of observations Number of hits Percentage of hits
Driver with a cab
1400
1295
92.5%
Driver with glasses 1400
1183
85.5%
Table 3. Detection levels for different drowsiness parameter considering the hair covering face.
Test
Number of
observations
Number of
hits
Percentage of
hits
Hair covering driver’s face
70
65
92.8%
Hair not covering driver’s
face
70
49
70.0%
570
E. E. Galarza et al.

References
1. World Health Organization. Road Safety: Basic Facts (2016). http://www.who.int/violence_
injury_prevention/publications/road_trafﬁc/Road_safety_media_brief_full_document.pdf
2. Ji, Q., Yang, X.: Real-time eye, gaze, and face pose tracking for monitoring driver vigilance.
Real Time Imaging 8(5), 357–377 (2002)
3. Lee, B.G., Chung, W.Y.: A smartphone-based driver safety monitoring system using data
fusion. Sensors 12(12), 17536–17552 (2012)
4. He, J., Roberson, S., Fields, B., Peng, J., Cielocha, S., Coltea, J.: Fatigue detection using
smartphones. J. Ergon. 3(3), 1–7 (2013)
5. Chang, K., Oh, B.H., Hong, K.S.: An implementation of smartphone-based driver assistance
system using front and rear camera. In: 2014 IEEE International Conference on Consumer
Electronics (ICCE), pp. 280–281. IEEE (2014)
6. Xu, L., Li, S., Bian, K., Zhao, T., Yan, W.: Sober-Drive: a smartphone-assisted drowsy
driving detection system. In: 2014 International Conference on Computing, Networking and
Communications (ICNC), pp. 398–402. IEEE (2014)
7. Li, G., Chung, W.Y.: Detection of driver drowsiness using wavelet analysis of heart rate
variability and a support vector machine classiﬁer. Sensors 13(12), 16494–16511 (2013)
8. Smirnov, A.V., Kashevnik, A., Lashkov, I., Baraniuc, O., Parfenov, V.: Smartphone-based
identiﬁcation of dangerous driving situations: algorithms and implementation. In: FRUCT,
pp. 306–313 (2016)
9. Singh, H., Bhatia, J.S., Kaur, J.: Eye tracking based driver fatigue monitoring and warning
system. In: 2010 India International Conference on Power Electronics (IICPE), pp. 1–6.
IEEE (2011)
10. Grace, R., Byrne, V.E., Bierman, D.M., Legrand, J.M., Gricourt, D., Davis, B.K.,
Staszewski, J.J., Carnahan, B.: A drowsy driver detection system for heavy vehicles. In:
Proceedings of the 17th AIAA/IEEE/SAE Digital Avionics Systems Conference, DASC,
vol. 2, pp. I36/1–I36/8. IEEE (1998)
11. Grace, R., Steward, S.: Drowsy driver monitor and warning system. In: International Driving
Symposium on Human Factors in Driver Assessment, Training and Vehicle Design, vol. 8,
pp. 201–208 (2001)
12. Kozak, K., Pohl, J., Birk, W., Greenberg, J., Artz, B., Blommer, M., Cathey, L., Curry, R.:
Evaluation of lane departure warnings for drowsy drivers. Proc. Hum. Factors Ergon. Soc.
Annu. Meet. 50(22), 2400–2404 (2006). Sage Publications, Los Angeles, CA
13. Garcia, I., Bronte, S., Bergasa, L.M., Almazán, J., Yebes, J.: Vision-based drowsiness
detector for real driving conditions. In: IEEE Intelligent Vehicles Symposium (IV), pp. 618–
623. IEEE (2012)
14. Harshul, G., Tanupriya, C., Praveen, K.: Comparison between signiﬁcance of usability and
security in HCI. In: 2017 3rd International Conference on Computational Intelligence &
Communication Technology (CICT), Ghaziabad, India, pp. 1–4. IEEE (2017)
15. Xu, Z., Qiu, X., He, J.: A novel multimedia human-computer interaction (HCI) system based
on kinect and depth image understanding. In: International Conference on Inventive
Computation Technologies (ICICT), Coimbatore, India, pp. 1–6. IEEE (2017)
16. Fernandez Montenegro, J., Argyriou, V.: Gaze estimation using EEG signals for HCI in
augmented and virtual reality headsets. In: 2016 23rd International Conference on Pattern
Recognition (ICPR), Cancun, Mexico. IEEE (2016)
Real Time Driver Drowsiness Detection Based on Driver’s Face Image Behavior
571

17. El-Shazly, E., Abdelwahab, M., Shimada, A.: Real time algorithm for efﬁcient HCI
employing features obtained from MYO sensor. In: 2016 IEEE 59th International Midwest
Symposium on Circuits and Systems (MWSCAS), Abu Dhabi, United Arab Emirates, pp. 1–
4. IEEE (2016)
18. Itkarkar, R.R., Nandi, A.V.: A survey of 2D and 3D imaging used in hand gesture
recognition for human-computer interaction (HCI). In: 2016 IEEE International WIE
Conference on Electrical and Computer Engineering (WIECON-ECE), Pune, India, pp. 188–
193. IEEE (2016)
19. Zuo, H.: Implementation of HCI software interface based on image identiﬁcation and
segmentation algorithms. In: 2016 Online International Conference on Green Engineering
and Technologies (IC-GET), Coimbatore, India, pp. 1–6. IEEE (2016)
20. Gabrielli, L., Bussolotto, M., Squartini, S.: Reducing the latency in live music transmission
with the BeagleBoard xM through resampling. In: 2014 6th European Embedded Design in
Education and Research Conference (EDERC), Milano, Italy, pp. 302–306. IEEE (2014)
21. Leboeuf-Pasquier, J., Villa, A.G., Burgos, K.H., Carr-Finch, D.: Implementation of an
embedded system on a TS7800 board for robot control. In: 2014 International Conference on
Electronics,
Communications
and
Computers
(CONIELECOMP),
Cholula,
Mexico,
pp. 135–141. IEEE (2014)
22. Toshniwal, K., Conrad, J.M.: A web-based sensor monitoring system on a Linux-based
single board computer platform. In: Proceedings of the IEEE SoutheastCon 2010
(SoutheastCon), Concord, NC, USA, pp. 371–374. IEEE (2010)
23. Jaziri, I., Chaarabi, L., Jelassi, K.: A remote DC motor control using embedded Linux and
FPGA. In: 2015 7th International Conference on Modelling, Identiﬁcation and Control
(ICMIC), Sousse, Tunisia, pp. 1–5. IEEE (2015)
24. Kang, P., Wei, Y., Wei, Z.: Control system for granary ventilation based on embedded
networking and Qt technology. In: 2017 29th Chinese Control and Decision Conference
(CCDC), Chongqing, China, pp. 2275–2280. IEEE (2017)
25. Degada, A., Savani, V.: Design and implementation of low cost, portable telemedicine
system: an embedded technology and ICT approach. In: 2015 5th Nirma University
International Conference on Engineering (NUICONE), Ahmedabad, India, pp. 1–6. IEEE
(2015)
26. Stutts, J.C., Wilkins, J.W., Vaugh, B.V.: Why do people have drowsy driving crashes? Input
from drivers who just did. AAA Foundation for Trafﬁc Safety, Washington, DC (1999)
27. Verwey, W.B., Zaidel, D.M.: Preventing drowsiness accidents by an alertness maintenance
device. Accid. Anal. Prev. 31(3), 199–211 (1999)
28. Flores, M.J., Armingol, J.M., De la Escalera, A.: Sistema avanzado de asistencia a la
conducción para la detección de la somnolencia. Rev. Iberoam. Autom. e Inform. Ind. RIAI
8(3), 216–228 (2011)
29. Fuletra, J.D., Bosamiya, D.: A survey on driver’s drowsiness detection techniques. Int.
J. Recent Innov. Trends Comput. Commun. 1(11), 816–819 (2013)
30. Dinges, D.F., Grace, R.: PERCLOS: a valid psychophysiological measure of alertness as
assessed by psychomotor vigilance. US Department of Transportation, Federal Highway
Administration, Publication Number FHWA-MCRT-98-006 (1998)
31. Jo, J., Lee, S.J., Jung, H.G., Park, K.R., Kim, J.: Vision-based method for detecting driver
drowsiness and distraction in driver monitoring system. Opt. Eng. 50(12), 127202 (2011)
32. Joyce, G., Lilley, M., Barker, T., Jefferies, A.: Mobile application tutorials: perception of
usefulness
from
an
HCI
expert
perspective.
In:
International
Conference
on
Human-Computer Interaction, pp. 302–308. Springer International Publishing, Cham (2016)
33. https://www.sitepoint.com/mobile/app-development/. Accessed 10 Sep 2017
572
E. E. Galarza et al.

Interactive System Using Beaglebone Black
with LINUX Debian for Its Application
in Industrial Processes
Marco Pilatásig(&), Franklin Silva, Galo Chacón, Víctor Tapia,
John Espinoza, Esteban X. Castellanos, Lucia Guerrero,
and Jessy Espinosa
Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador
{mapilatagsig,fmsilva,ggchacon,vatapia1,jjespiniza6,
excastellanos,leguerrero6,jjespinosa}@espe.edu.ec
Abstract. In this article it presents the development of an interactive system,
which allows the user to interact with an air ﬂow temperature control process,
implemented with two control algorithms: Proportional Integral Derivative PID
and Fuzzy control. It is intended to demonstrate the usefulness of an interactive
system in industrial applications, it is used a low-cost embedded system the
Beaglebone Black and LINUX Debian free software, also the system allows to
enter the desired control parameters and to adequately observe the behavior of
the process and the efﬁciency of the controller; for the veriﬁcation of the
interactive system, a model of air ﬂow temperature control plant was con-
structed, which is currently used as a didactic module in the Laboratory of
Process Control of the University.
Keywords: Interactive system  Beaglebone Black  Debian LINUX
PID control  Fuzzy control  Industrial process
1
Introduction
With the development of digital systems and computers, all kinds of interrelation with
those, it is done mainly through images, text and sounds, which must deliver under-
standable and relevant information, in turn the application of computers has advanced a
lot in several ﬁelds, one of them is the control in industrial processes and even though
its lack of reliability can not directly take control of variables or actions in a process, it
can easily supervise a lot of relevant information and therefore it can be the link
between the process and the user.
The applicability of the Interactive System is very important and necessary, there-
fore it must have adequate characteristics of usability [1], it design must be adequate,
friendly and satisfy the needs of users [2].
Currently the interaction with the computer has interested many researchers who
seek to make it increasingly easy for humans, there are several very interesting pro-
posals: In [3] is intended to perform an interactive system using electroencephalog-
raphy (EEG) signals to detect movement of the eyes and with them to enter the
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_54

necessary information to the system, this is very important since it is not required of
contact and is not invasive, its use would facilitate the interaction with people who
present body paralysis, in [4] it pretend the use of motion sensors of the muscles, which
would serve for people with amputated limbs, in [5] it is proposed the use of hand
gestures that would allow interaction with people who use sign language, in [6] it is
proposed the use of segmentation of images which would help all kinds of interaction
by means of movements of any part of the user’s body, in [7] the use of other senses
such as touch, taste and smell is analyzed in addition to the use of vision and hearing,
which would make the use of system more pleasant, there is also a novel proposal [8] it
consist to analyze the style of thinking of the people in order to give a different type of
interaction according to their personality, in [9] it is proposed the use of a low-cost
embedded system, a camera and an image projector to convert a sheet of simple acrylic,
in a system “touch”.
As one can see, the Interactive Systems are very important and necessary in many
and varied ﬁelds, therefore its cost will inﬂuence their use and their use will inﬂuence the
technological progress and the improvement of life quality of the members of a society.
This paper is also intended to demonstrate that a suitable Interactive System is not
necessarily expensive since it can be made using low cost elements such as embedded
systems and the use of free software. In this case the proposal is made using a Bea-
glebone Black board, which is a small inexpensive computer that can be easily con-
nected to any type of monitor and is programmed using free software Debian LINUX.
This combination of low-cost embedded systems and LINUX software have already
been used in several studies and thus proved useful, in [10] are used to reduce the latency
in live music transmission, in [11] to perform the control of a robot trying to emulate an
artiﬁcial brain, in [12] to construct a suitable system to supervise sensors whit a low cost,
in [13] for the implementation of a remote speed controller of a DC motor using PWM
signal, in [14] for a ventilation control in barns, in [15] for the implementation of a
portable low-cost telemedicine system. It can be mentioned another Beaglebone
applications with other software or the Linux application in other embedded systems
such as [16] Beaglebone to handle image-based information, [17] Beaglebone Black in
the three-phase inverter implementation, [18] Beaglebone Black to admission control in
the public transport using a card and also to show messages about the route, in [19] the
application of Debian LiINUX in another low-cost embedded system Raspberry PI for
the identiﬁcation of characteristics in newborns, and could mention many more studies.
The objective of the present work is to implement an Interactive System that allows
the user to interact with an airﬂow temperature control process, which permits to enter
the data required by the process as the desired set point, and others parameters such the
tuning constants in the case of an Proportional Integral Derivative Controller PID, and
also the behavior of the variable can be visualized which will allow to evaluate the
behavior of the applied controllers, the controllers are made using a Beaglebone Black
computer programmed with Debian LINUX software, a monitor, a keyboard and the
plant.
The present work is divided into six sections including: the introduction, Sect. 2
presents the plant for the control of air ﬂow temperature, Sect. 3 describes the con-
trollers used, Sect. 4 shows the development of algorithms, Sect. 5 presents the results
obtained and Sect. 6 the conclusions.
574
M. Pilatásig et al.

2
Plant for the Airﬂow Temperature Control
The implemented plant consists of a three-phase motor, which has both start and speed
controlled by a frequency variable inverter. Its rotor is coupled a fan for the generation
of air which circulates through a duct where there is a wire wound radiant heater that
serve to heat the air. The plant actuator is a solid state relay that regulates the current to
wire wound radiant heater and is activated by a PWM signal generated by the
Beaglebone Black board. The acquisition of the airﬂow temperature value is realized
by sensor DS18B20, which enters the value to the board for its corresponding analysis
in the PID control algorithm or with fuzzy logic algorithm. Figure 1 shows the airﬂow
temperature control plant.
For the control of the temperature using the PID controller two graphical interfaces
are realized: the ﬁrst one allows entering the data of the set point and the tuning
constants and the second one allows visualizing the behavior of the controlled variable
(airﬂow temperature). In the case of control with fuzzy logic there are also two
graphical interfaces: in the ﬁrst one you can enter the set point and in the second one
graph the behavior of the controlled variable is displayed, in Fig. 2 a block diagram of
the system is shown.
Fig. 1. Plant for the airﬂow temperature control.
Fig. 2. Block diagram of the airﬂow temperature control plant.
Interactive System Using Beaglebone Black with LINUX Debian
575

3
Controllers
The controllers used are: Proportional Integral Derivative (PID) controller and Con-
troller with Fuzzy Logic.
3.1
Proportional Integral Derivative (PID) Controllers
The PID control (proportional, integral and derivative) is one of the most used in
industry and in feedback control systems, where the control parameter u(t) is calculated
based on the error which is the difference between the set point and the process
variable. Used for applications with different variables including temperature, level,
pressure, ﬂow, and many more. It responds to the following Eq. 1 as show in [20]:
uðtÞK ¼ peðtÞ þ Kp
Ti
Z t
0
eðtÞdt þ KpTd deðtÞ
dt
ð1Þ
Where u(t) is the control parameter, Kp is the proportional constant, Ti is the
integral time, Td is the derivative time and e(t) is the error. To implement the digital
PID control it is necessary to convert Eq. 1 to a discrete representation [20] as pre-
sented in Eq. 2.
UðSÞ ¼ Kp 1 þ 1
TiS þ TdS


EðSÞ
ð2Þ
3.2
Controls with Fuzzy Logic
The control with fuzzy logic can be described as a control of very easy interpretation,
given to a group with a certain degree of belonging, it, which is better, adapted to the
real world and even can be understood and worked with our own expressions. In the
present controller to be designed a MISO system is implemented (Multiple inputs and
simple output), the inputs are; the error and the derivative of the error, and the only
output is the resulting signal from the controller (PWM).
4
Algorithms Development
4.1
Control Algorithms
The algorithms of control are developed in the 2.7 version python programming lan-
guage that has different characteristics, it offers an advantage to the programmer since it
is an interpreted programming language that forces the developer to use code lines, this
is very useful for future corrections and used in learning for different types of
programmers.
The control algorithm is structured using easy comprehension functions for future
code debugging and improvements, which basically consists of program start, data
576
M. Pilatásig et al.

acquisition using a sensor (DS18B20), PID control mathematics equation, the resulting
signal to be sent to the actuator, the variables presentation in the System (Set value,
control value), the database generation (excel) with the corresponding results analysis.
Figure 3 shows the algorithms ﬂow diagrams, both for the PID control, and for the
control with fuzzy logic.
To determine the process behavior it is necessary to observe data such as set point,
process value and control value, these were plotted thanks to the matplolib library, the
data were saved and plotted one by one with each algorithm iteration. Each data was
stored in vectors up to a total of 50 in order to avoid accumulation along the process
and thus facilitate the performance of the board, in Fig. 4 is presented the ﬂow diagram
that allows to observe the behavior for both the PID control and control with fuzzy
logic.
Also generated a very important database for the results analysis in order to take
decisions about controller efﬁciency.
4.2
Design of the Interactive System
For the developed application the following libraries were installed, in which it can
verify the operating system to be installed, in this case they were installed in Debian
Linux 7.5 and it have to consider the dependencies that each has for its correct
installation and handling.
TKINTER
With this library it will be possible to realize the graphic interfaces that serve to interact
with the user, helping the easy handling of the application.
Fig. 3. Flows chart (a) control PID and (b) fuzzy control
Interactive System Using Beaglebone Black with LINUX Debian
577

MATPLOTLIB
Matplotlib is a library of functions widely used to perform 2D and 3D work within the
python programming language, graphs can be made in which the Set point, Process
value and control value are displayed, which allows visualizing the behavior of the
controller.
SCIKIT-FUZZY
Scikit-fuzzy o skfuzzy is the library used in python to perform controllers with fuzzy
logic, a very important feature is that it allows to perform MISO (Multiple Inputs and
Simple Output) systems because it will use as input variables the error and the
derivative of the error and as output the PWM signal.
The following is the Interactive System which allow to interact with the controllers,
thanks to use of all the libraries used in a systematic way, in Fig. 5 it can see the data
entry interface for the both PID controller and for the Fuzzy control.
Fig. 4. Flowchart of the interface that allows observing the process behavior.
578
M. Pilatásig et al.

5
Test and Results
5.1
Test
The interactive system was used by 20 students of the Electronic Career sixth level, 15
men and 5 women.
In the interface of the PID control the students enter the value of set point using
keyboard and then observe the behavior of the signals both the process value and
control value; also they can tune the PID controller by entering the values of kp, Ti and
Td, in this way the student interacts with the computer in order to check the effec-
tiveness of the implemented controller.
In the Interface of the control with fuzzy logic the students can only enter the value
of the set point and observe the signals mentioned above, in the two interfaces the
process can be stopped at any moment; several tests were performed with different set
point values in ascending and descending form.
5.2
Results
After having developed the algorithms of the controllers: PID and the control with
fuzzy logic, the results are presented in a graphical interface, in Fig. 6 the response of
both the PID controller and control with fuzzy logic is observed using different types of
temperature steps.
The efﬁciency of the PID controller and the controller with fuzzy logic is clearly
observed, since at different changes of the desired value (blue signal) the controlled
variable (red signal) is able to follow them with a suitable delay for a temperature
Fig. 5. Graphic interface (a) PID control and (b) Fuzzy control
Interactive System Using Beaglebone Black with LINUX Debian
579

process which is generally slow, in the control with fuzzy logic presents a small over
impulse, in contrast the one in PID is practically null.
In [21] a control with fuzzy logic is implemented in an arduino card for the
temperature control, in which the information is presented in an LCD; in this work two
graphical interfaces are implemented that are developed by means of lines of text using
Python as programming language, so that the students interact with the computer both
easy and intuitive way.
6
Conclusions
An Interactive System is a very useful and necessary tool in many ﬁelds; one of them is
the industrial ﬁeld where it allows an efﬁcient interaction between the user and the
process.
The use of Interactive System determines a very important advance in both the
technological and the social, therefore it should be sought its massive use, this can be
done by implementing them using low cost and easy to acquire elements.
It is demonstrated in this work that a low-cost Beaglebone Black programmed with
free software is adequate to satisfy basic solutions, both for the Interactive System and
for control.
References
1. Harshul, G., Tanupriya, C., Praveen, K.: Comparison between signiﬁcance of usability and
security in HCI. In: 2017 3rd International Conference on Computational Intelligence &
Communication Technology (CICT), Ghaziabad, India, pp. 1–4. IEEE (2017)
2. Xu, Z., Qiu, X., He, J.: A novel multimedia human-computer interaction (HCI) system based
on kinect and depth image understanding. In: International Conference on Inventive
Computation Technologies (ICICT), Coimbatore, India, pp. 1–6. IEEE (2017)
3. Fernandez Montenegro, J., Argyriou, V.: Gaze estimation using EEG signals for HCI in
augmented and virtual reality headsets. In: 2016 23rd International Conference on Pattern
Recognition (ICPR), Cancun, Mexico. IEEE (2016)
4. El-Shazly, E., Abdelwahab, M., Shimada, A.: Real time algorithm for efﬁcient HCI employing
features obtained from MYO sensor. In: 2016 IEEE 59th International Midwest Symposium
on Circuits and Systems (MWSCAS), Abu Dhabi, United Arab Emirates, pp. 1–4. IEEE
(2016)
Fig. 6. Response curves (a) for PID control and (b) for fuzzy logic control
580
M. Pilatásig et al.

5. Itkarkar, R.R., Nandi, A.V.: A survey of 2D and 3D imaging used in hand gesture recognition
for human-computer interaction (HCI). In: 2016 IEEE International WIE Conference on
Electrical and Computer Engineering (WIECON-ECE), Pune, India, pp. 188–193. IEEE
(2016)
6. Zuo, H.: Implementation of HCI software interface based on image identiﬁcation and
segmentation algorithms. In: 2016 Online International Conference on Green Engineering
and Technologies (IC-GET), Coimbatore, India, pp. 1–6. IEEE (2016)
7. Obrist, M., Gatti, E., Maggioni, E., Vi, C.T., Velasco, C.: Multisensory experiences in HCI.
IEEE MultiMed. 24, 9–13 (2017)
8. Adeyemi, I.R., Razak, S.A., Salleh, M.: Individual difference for HCI systems: examining
the probability of thinking style signature in online interaction. In: 2016 4th International
Conference on User Science and Engineering (i-USEr), Melaka, Malaysia, pp. 51–56. IEEE
(2016)
9. Pimpalkar, T., Tupe-Waghmare, P.: The design of touch detection HCI system based on
Raspberry Pi module. In: 2016 IEEE International Conference on Recent Trends in
Electronics, Information & Communication Technology (RTEICT), Bangalore, India,
pp. 305–310. IEEE (2016)
10. Gabrielli, L., Bussolotto, M., Squartini, S.: Reducing the latency in live music transmission
with the BeagleBoard xM through resampling. In: 2014 6th European Embedded Design in
Education and Research Conference (EDERC), Milano, Italy, pp. 302–306. IEEE (2014)
11. Leboeuf-Pasquier, J., Villa, A.G., Burgos, K.H., Carr-Finch, D.: Implementation of an
embedded system on a TS7800 board for robot control. In: 2014 International Conference on
Electronics,
Communications
and
Computers
(CONIELECOMP),
Cholula,
Mexico,
pp. 135–141. IEEE (2014)
12. Toshniwal, K., Conrad, J.M.: A web-based sensor monitoring system on a Linux-based
single board computer platform. In: Proceedings of the IEEE SoutheastCon 2010
(SoutheastCon), Concord, NC, USA, pp. 371–374. IEEE (2010)
13. Jaziri, I., Chaarabi, L., Jelassi, K.: A remote DC motor control using Embedded Linux and
FPGA. In: 2015 7th International Conference on Modelling, Identiﬁcation and Control
(ICMIC), Sousse, Tunisia, pp. 1–5. IEEE (2015)
14. Kang, P., Wei, Y., Wei, Z.: Control system for granary ventilation based on embedded
networking and Qt technology. In: 2017 29th Chinese Control and Decision Conference
(CCDC), Chongqing, China, pp. 2275–2280. IEEE (2017)
15. Degada, A., Savani, V.: Design and implementation of low cost, portable telemedicine
system: an embedded technology and ICT approach. In: 2015 5th Nirma University
International Conference on Engineering (NUICONE), Ahmedabad, India, pp. 1–6. IEEE
(2015)
16. Siradjuddin, I., Tundung, S.P., Indah, A.S., Adhisuwignjo, S.: A real-time model based
visual servoing application for a differential drive mobile robot using Beaglebone Black
embedded system. In: 2015 IEEE International Symposium on Robotics and Intelligent
Sensors (IRIS), Langkawi, Malaysia, pp. 186–192. IEEE (2015)
17. Götz, M., Gobetti, M.W., Líbano, F.B.: A grid-tie micro-inverter software development
based on a low cost multiprocessor platform. In: 2015 Brazilian Symposium on Computing
Systems Engineering (SBESC), Foz do Iguacu, Brazil, pp. 122–127. IEEE (2015)
18. Shariff, S.U., Swamy, J.C., Seshachalam, D.: Beaglebone Black based e-system and
advertisement revenue hike scheme for Bangalore city public transportation system. In: 2016
2nd International Conference on Applied and Theoretical Computing and Communication
Technology (iCATccT), Bangalore, India, pp. 781–786. IEEE (2016)
Interactive System Using Beaglebone Black with LINUX Debian
581

19. Sivaranjani, S., Sumathi, S.: Implementation of ﬁngerprint and newborn footprint feature
extraction on Raspberry Pi. In: 2015 International Conference on Innovations in Information,
Embedded and Communication Systems (ICIIECS), Coimbatore, India, pp. 1–6. IEEE
(2015)
20. Ogata, K.: Modern Control Engineering, pp. 567–573. Prentice Hall, Madrid (2010)
21. Oltean, G., Ivanciu, L.N.: Implementation of a fuzzy logic-based embedded system for
temperature control. In: 2017 40th International Spring Seminar on Electronics Technology
(ISSE), Soﬁa, pp. 1–6 (2017)
582
M. Pilatásig et al.

Interactive System for Monitoring and Control
of a Flow Station Using LabVIEW
Jorge Buele, John Espinoza, Marco Pilatásig(&), Franklin Silva,
Alexandra Chuquitarco, Jenny Tigse, Jessy Espinosa,
and Lucía Guerrero
Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador
{jlbuele,jjespinoza6,mapilatagsig,fmsilva,
eachuquitarco,jptigse,iiespinosa,
leguerrero6}@espe.edu.ec
Abstract. This work delivers the design of an interactive system that allows
end user to interact with an intuitive, friendly and efﬁcient way within the
process control world. Using Raspberry Pi 3 with a high-level language, a fuzzy
control algorithm applied to a ﬂow station is implemented, whose information
will be sent in serial form to an interface developed in the LabVIEW software.
This interface has three windows, in which you can modify the value of the
setpoint, trends views and save historical data for the subsequent processing of
such information. This will replace the expensive conventional display screens,
with interfaces developed in computer. Besides, it is demonstrated its applica-
bility and ease of communication with low-cost embedded boards, through
experimental tests and usability surveys.
Keywords: Interactive system  Fuzzy control  Raspberry Pi
LabVIEW
1
Introduction
With the accelerated advance of technology, digital systems and the gradual migration
of database content and computer services, physical documents have been changed to
digital ﬁles [1–3]. All this led to the emergence of the term HCI (human-computer
interaction) in the 1960s, an area of interdisciplinary research oriented to the interre-
lationship modalities developed by people with a computer [4, 5]. Its application was
launched when computers reach their apogee and reach the end-user jobs and homes,
which do not have extensive computer skills. Frequently, the one who uses the com-
puter services does not directly contact the technical staff who develop the same, but
rather interacts with a computer application.
Human-machine interaction encompasses the study of the interaction between a
person and the software/hardware; as well as the adaptation of the computer systems to
people needs, since the beneﬁts are directed to them (user-centered design) [6]. That is
the reason why its applicability is wide, and can be used in any discipline that provides
the possibility of installing a computer. It engages in education, sociology, industrial
design, computer science and mainly in medicine combined with technology. In this
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_55

context, there are several research papers, for example, in [7] a virtual reality system is
detailed that uses a haptic device for the rehabilitation of upper limbs. This shows that
within the rehabilitation programs of health professionals can include such systems
providing better results, which emotionally motivate patients, as noted in [8–10]. The
continuous technological advancement in consumer electronics has opened up new
scenarios for the development of more pleasant, accessible, robust and conﬁgurable
interfaces. This makes it possible to reduce the space between human mental models
and the way in which computers, machines or robots, develop their tasks [11]. Sub-
sequently, the term Human Machine Interface (HMI) appears, to describe the user
interface in a manufacturing or process control system, which is used in various
applications. For example, in [12] an industrial process simulator is presented, where
real-time simulation of industrial automation processes is obtained, with this, users can
develop complementary programming skills. In [13], a Wireless HART training system
is presented that concedes operator interaction with the computer remotely by devel-
oping HMI interfaces. Most of these interfaces are developed in touch screens with
limitations of animations and trends commercialized by the own manufacturers at high
costs.
This paper presents the development of an interface, elaborated in LabVIEW, this
software facilitates a diversity of animations and designs that facilitate the operation
and efﬁcient performance of a person who monitors and controls a process. With this, it
is tried to increase the interest of the university students in the subject related to the
automatic control of processes. In addition to this application, an algorithm of a fuzzy
control in a Raspberry Pi 3 board has been implemented, which use free software and
communicates serially with the interface and thus present in real time the results
obtained in a ﬂow station. Figure 1 presents the proposed general diagram.
Fig. 1. General diagram of interactive system implemented.
584
J. Buele et al.

This work is divided into 5 sections, including the introduction. Section 2 presents
the methodology of the developed system; Sect. 3 presents the design of the interactive
system using LabVIEW. Section 4 shows the results obtained in the research and,
ﬁnally, the conclusions are presented in Sect. 5.
2
Methodology of the Developed System
This section describes the stages that are part of the system for the monitoring and
control of the ﬂow station, as shown in Fig. 2.
2.1
Signal Acquisition
In the ﬂow control station, a magnetic ﬂow sensor is placed inside a cylindrical storage
tank, which detects the variation of the measured variable. It emits a signal that is
received by the transmitter Rosemount 8732E type ﬂange connected in conﬁguration of
two wires, which produces a standard signal of 4 to 20 mA. This signal is introduced in
a current-to-voltage converter (I/E) that consisting of a 250 X precision resistor and
produces an analog signal of 1 to 5 V. This signal enters in the Raspberry Pi 3 board,
the same as a single-board computer (SBC), the Model B has a four-core ARMv8 CPU
that operates at 1.2 GHz [14, 15], has GPIO (General Purpose Input/Output), where the
normal GPIO are used as digital inputs/outputs operating at 3.3 V or 5 V and the
special ones are intended for UART, SPI or I2C interfaces. The difference with other
embedded boards is that has no analog inputs; an external analogue - digital converter
(ADC) must be used or connected to an interface card [16] and the 12 - bit dual -
channel A/D converter MCP3202 has been used, which is communicated via the bus of
serial peripheral interface (SPI), a synchronous protocol that operates in full duplex
mode and provides good data transmission speed through 4 signals.
2.2
Development of Algorithm
A fuzzy control system is one that offers the facility of creating a control algorithm
through linguistic variables that interpret common sense. Compared with classical
controls, diffuse variables can assume values between 0 and 1 [17].
Fig. 2. System block diagram.
Interactive System for Monitoring and Control of a Flow Station Using LabVIEW
585

The three main stages that are executed in Fuzzy control are: (i) Fuzziﬁcation, in
which the real values are taken and assigned a degree of membership according to the
knowledge base where the linguistic rules are deﬁned. (ii) Inference, this phase
establishes logical relations between the fuzzy sets of input and output. For this
application the Mamdani architecture, which consists of a series of rules of the form
IF … THEN [18], has been used to generate the desired behavior. (iii) Diffuziﬁcation,
at this stage the fuzzy values are translated to such numerical values that can be applied
to the system to be controlled. The output values are obtained by the center of area
method which is represented in (1).
yd ¼
XR
l¼1 d1lBlðdlÞ=
XR
l¼1 lBlðdlÞ
ð1Þ
Where, yd is the output, R is the number of rules, l = 1, …, R, dl is the center of
gravity of the output fuzzy set, Bl is the l-th rule.
To start the development of the algorithm, it must install an operating system in
Raspberry Pi 3, where Raspbian and Python programming language is usually used.
This was not optimal for this application and UbuntuMate was installed instead, a
version of Linux that does not require many resources and provides all its functionality.
The WiringPi library provides the necessary complements to communicate with the
Fig. 3. Program ﬂow diagram in Raspberry Pi 3
586
J. Buele et al.

MCP3202 A/D converter and the PCF8591 D/A converter via I2C communication.
Through the installation of MinGW, a G++ compiler, it can be operated with the
embedded eFLL (Embedded Fuzzy Logic Library). This library was chosen because it
allows to implement fuzzy systems in an efﬁcient and simpliﬁed way, it was written in
C++ and C, using the standard language library C “stdlib.h”. Its main advantage is that
it has no explicit restrictions on the number of Fuzzy rules, inputs or outputs, but rather
depends on the internal memory of the controller, which makes it functional for this
application. This algorithm is structured in different programming blocks as shown in
Fig. 3.
The modeling of the rules for Fuzziﬁcation and Diffuziﬁcation, as well as the
structure of the membership rules are done with the Fuzzy Logic Designer toolkit of
MATLAB which admits a preview of the control behavior before being implemented
in the board. In this case, it has worked with an input and output system (SISO), where
7 membership functions have been included in the algorithm developed.
3
Interactive System Design Using LabVIEW
The design and execution of the user interface is done in LabVIEW, a simple, intuitive,
powerful and ﬂexible software [19]. A virtual instrument platform created by National
Instruments, whose programming language is clearly graphical and permits connec-
tivity with a wide range of devices including vision, motion, signal processing, data
acquisition and in this case control [20]. For the design of the interface, users who will
interact with the application has taken into account; thus, developing windows with
ease of use and understanding, using the minimum amount of colors, choosing soft
shades for the background contrast with objects, let the students avoid tiredness or
distraction. In addition, the elements are shown in a distributed way, avoiding over-
lapping of them. This is complemented by the use of buttons with the appropriate
signage and screens for the presentation of historical data transmitted in real time from
the control board, as well as its storage in a document with .xlsx extension.
The presentation of the interactive system consists of three screens: (i) Start win-
dow, (ii) Control and trends window and (iii) History window, which can be seen in
Figs. 4, 5 and 6, respectively. In the block diagram of the Control and Trends window
of Fig. 5, a block is created exclusive to the serial communication and sends the value
of ﬂow in LPM (Set Point) that the user needs and the reading of the plot sent by the
Raspberry Pi 3 card. Figure 4 shows the block diagram that allows navigation between
the Start Window with the Control, Trends and Historic windows.
The control and trend window has three slider indicators for Set Point (SP), Process
Variable (PV) and Control Variable (CV); represented by green, blue and red
respectively, so that users can identify them quickly. Additionally, a button was created
that allows to send the value of SP towards the control algorithm that is executed in the
board; because the serial communication is question and answer, to avoid the reception
of unwanted data in the frame that is received in the developed user interface.
Interactive System for Monitoring and Control of a Flow Station Using LabVIEW
587

4
Results and Discussion
4.1
Response of the Developed Interactive System
To observe how the interactive system optimizes the relation of operator with the
control and monitoring of the physical variables of the system; a ﬂow station was used
which has a magnetic ﬂow sensor, a 1/2HP three-phase centrifugal pump and a fre-
quency inverter.
In Fig. 7, the control and trend window is shown. In this it can be observed that
when the operator makes a change in the value of the setpoint, immediately the fuzzy
controller acts and through the user interface the status of each of the process variables
can be visualized in real time. In this window, users can also interact with the history
window by means of the action buttons located at the bottom of it.
In the History window, users can save data that is produced from the control and at
the same time can be exported to an Excel ﬁle in Fig. 8. It shows all data that is saved
along the control algorithm actuation. In the same way, the interface has buttons that
allow users to navigate between the Trends and Control window and the Main window,
so that a person can make changes to the SP.
a)    
 
b)
 
Fig. 4. Start window: (a) Front panel, (b) Block diagram
588
J. Buele et al.

4.2
Interface Usability Evaluation
The interactive system was tested by 60 students from the “Universidad de las Fuerzas
Armadas ESPE Extension Latacunga”, to determine how user-friendly and optimal the
system was for people as well as to know suggestions or possible changes in the
interface. The results obtained are analyzed below: 97% of students think that the use
of a suitable interface can signiﬁcantly optimize the control of a process. 85% of
students said that the interface facilitates the monitoring and control applied at the
station. 90% said that the interface is intuitive and easy to use, on the other hand 89%
stated that the interactive system helps them to visualize the data in real time and easily
export it to Excel to create a database. Based on the results obtained it can be observed
that the development of this interface is appropriate to improve the process control.
a) 
b) 
Fig. 5. Control window and trends: (a) Front panel, (b) Block diagram
Interactive System for Monitoring and Control of a Flow Station Using LabVIEW
589

a) 
b) 
Fig. 6. History window: (a) Front panel, (b) Block diagram
Fig. 7. Interactive system developed in Labview
590
J. Buele et al.

5
Conclusions
The development of this interactive system can be used in several ﬁelds, for example,
in the industrial ﬁeld because according to surveys allows end users to monitor and
control at the same time any process in an intuitive, friendly, efﬁcient and safe way.
The choice of LabVIEW software helps to create less expensive interactive systems
and due to the variety of toolkits and indicators that it offers, it is easy to customize the
interface with which user interacts.
An interactive system was introduced that improves understanding of process
control by the ease of LabVIEW software to communicate with other devices through
serial communication and the Raspberry Pi 3 card to implement control algorithms.
References
1. Kuutti, K., Bannon, L.J.: The turn to practice in HCI: towards a research agenda. In:
Proceedings of the 32nd Annual ACM Conference on Human Factors in Computing
Systems, pp. 3543–3552. ACM, New York (2014)
2. Xu, Z., Qiu, X., He, J.: A novel multimedia human-computer interaction (HCI) system based
on kinect and depth image understanding. In: International Conference on Inventive
Computation Technologies (ICICT), Coimbatore, pp. 1–6. IEEE Press (2017)
3. Heng, S., Yunfeng, D.: Research on cooperative control of human-computer interaction tools
with high recognition rate based on neural network. In: 2014 International Conference on
Virtual Reality and Visualization (ICVRV), Shenyang, pp. 350–354. IEEE Press (2014)
4. Lindtner, S., Hertz, G.D., Dourish, P.: The design of touch detection HCI system based on
Raspberry Pi module. In: Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, Toronto, pp. 439–448. ACM (2014)
5. Koepﬂer, J., Stark, L.D., Dourish, P., Sengers, P., Shilton, K.: Values & design in HCI
education. In: Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, Toronto, pp. 127–130. ACM (2014)
Fig. 8. Historical generated by the fuzzy control
Interactive System for Monitoring and Control of a Flow Station Using LabVIEW
591

6. Pimpalkar, T., Tupe-Waghmare, P.: Emerging sites of HCI innovation: hackerspaces,
hardware startups & incubators. In: 2016 IEEE International Conference on Recent Trends in
Electronics, Information & Communication Technology (RTEICT), Bangalore, pp. 305–
310. IEEE Press (2016)
7. Pruna, E., Acurio, A., Tigse, J., Escobar, I., Pilatásig, M., Pilatásig, P.: Virtual system for
upper limbs rehabilitation in children. In: International Conference on Augmented Reality,
Virtual Reality and Computer Graphics, Ugento, pp. 107–118. Springer, Cham (2017)
8. Agrawal, S.K., Chen, X., Kim, M.J., Lee, Y.M., Cho, H.P., Park, G.J.: Feasibility study of
robot enhanced mobility in children with cerebral palsy. In: 2012 4th IEEE RAS & EMBS
International Conference on Biomedical Robotics and Biomechatronics (BioRob), Roma,
pp. 1541–1548. IEEE Press (2012)
9. Schüler, T., Drehlmann, S., Kane, F., von Piekartz, H.: Abstract virtual environment for
motor rehabilitation of stroke patients with upper limb dysfunction. A pilot study. In: 2013
International Conference on Virtual Rehabilitation (ICVR), Philadelphia, pp. 184–185. IEEE
Press (2013)
10. Lancheros-Cuesta, D.J., Marin, M.P., Saenz, Y.V.: Intelligent system (HCI) for people with
motor misabilities. In: 2015 10th Iberian Conference on Information Systems and
Technologies (CISTI), Aveiro, pp. 1–6. IEEE Press (2015)
11. Montuschi, P., Sanna, A., Lamberti, F., Paravati, G.: Human-computer interaction: present
and future trends. Comput. Now 7(9) (2014). IEEE Computer Society. http://www.computer.
org/web/computingnow/archive/september2014
12. Pruna, E., Calvopiña, J., Serna, E., Escobar, I., Freire, W., Chang, O.: Implementación de
una herramienta didáctica para el diagnóstico de válvulas de control. In: 2016 IEEE Biennial
Congress of Argentina (ARGENCON), Buenos Aires, pp. 1–6. IEEE Press (2016)
13. Escobar, I., Pruna, E., Chang, O., Navas, A., Zambrano, J., Ávila, G.: Implementation of a
wireless HART training system for upgrading industrial. IEEE Lat. Am. Trans. 14(6), 2663–
2668 (2016). IEEE Press, Mexico City
14. Oulkar, S., Bamane, R., Gulave, S., Kothawale, P.: Voice controlled home automation using
Raspberry Pi 3. Int. J. Recent Innov. Eng. Res. 2(1), 28–32 (2017)
15. Senthilkumar, G., Gopalakrishnan, K., Kumar, V.S.: Embedded image capturing system
using Raspberry Pi system. Int. J. Emerg. Trends Technol. Comput. Sci. 3(2), 213–215
(2014)
16. Jindarat, S., Wuttidittachotti, P.: Smart farm monitoring using Raspberry Pi and Arduino. In:
2015 International Conference on Computer, Communications, and Control Technology
(I4CT), Kuching, pp. 284–288. IEEE Press (2015)
17. Nolasco, J.J.M., Padilla Medina, J.A.: LabVIEW-based classic, fuzzy and neural controllers
algorithm design applied to a level control prototype. IEEE Lat. Am. Trans. 15(6), 1154–
1162 (2017). IEEE Press, Mexico City
18. Dutu, L.C., Mauris, G., Bolon, P.: A fast and accurate rule-base generation method for
Mamdani fuzzy systems. IEEE Trans. Fuzzy Syst. PP(99), 1–19 (2017). IEEE Press, United
States
19. Guo, S., Gao, J., Guo, J., Li, N.: The LabVIEW-based control system for the upper limb
rehabilitation robot. In: 2017 IEEE International Conference on Mechatronics and
Automation (ICMA), Takamatsu, pp. 1732–1737. IEEE Press (2017)
20. Guo, J., Li, N., Gao, J.: A LabVIEW-based human-computer interaction system for the
exoskeleton hand rehabilitation robot. In: 2017 IEEE International Conference on
Mechatronics and Automation (ICMA), Takamatsu, pp. 571–576. IEEE Press (2017)
592
J. Buele et al.

Interactive System for Hands
and Wrist Rehabilitation
Marco Pilatásig, Jenny Tigse, Alexandra Chuquitarco(&),
Pablo Pilatásig, Edwin Pruna, Andrés Acurio, Jorge Buele,
and Ivón Escobar
Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador
{mapilatagsig,jptigse,Eachuquitarco,pxpilatasig,
eppruna,adacurio,jlbuele,ipescobar}@espe.edu.ec
Abstract. An Interactive system is presented for the rehabilitation of hands and
wrists using the leap motion device and the Unity3D software. Two applications
were created with several movements were by programming such as ﬂexion,
wrist extension, pronation, supination and adduction. Through the interfaces the
users have immersion and perform the exercises correctly because at the end of
the game a visual and audible feedback is presented. Five people used the
system and then the SEQ usability test was applied with results of 59.6. This
indicates that the system has a good acceptance and can be used for
rehabilitation.
Keywords: LeapMotion  Unity  Virtual reality  Test SEQ
1
Introduction
The Human Computer Interaction (HCI) has presence in several ﬁelds such as image
processing, education, industry, medicine, and others. The design must to be suitable,
friendly and must satisfy the necessities of the users. Furthermore, the system must
have usability and security features [1, 2].
There are several works developed that are related to the theme. For example in [3]
is presented the 3D evaluation of hands movements with a softkinetic camera whose
result is the visualization of marked points in ﬁngertips and ﬁngers joint over a front
view. In [4] is performed a interactive control system based on hands recognition using
Labview whose result is a robot movement according the hands movements. The
system obtains data through a glove, which sends the information with a wireless
connection. In [5] is presented a Human Computer Interaction for focused video sta-
bilization that proposes a stabilization method of centered video in targets and a
movement estimation component based on tracking. In [6] is performed a multichannel
heart frequency and electrocardiography interactive system whose result is the physi-
ological signals transmission by mobile Wi-Fi networks. The information will be
uploaded in a server for the doctors can diagnose the patients. In [7] is shown an
interactive system for hands gesture recognition with a method that works through the
3D data analysis in real time. The interactive system uses a set of classiﬁcation rules to
order the number of convexity defects in gesture classes.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_56

At present, there are people with diseases that affect the lower or upper limbs. The
hands and wrists are affected, producing that people can’t perform ﬂexion and exten-
sion movements. To recover the movement the patient performs a rehabilitation with
the support of a physical therapist. Furthermore, with the technological advance the
interactive system appeared that focuses in hands rehabilitation. In [8] is presented an
interactive system for hands rehabilitation based on Labview whose result is the quick
delivery of data to the virtual ﬁngers and then implementing a synchronized movement
between a robotic hand and a virtual hand. In [9] is shown a glove design and develop
for hands rehabilitation after stroke whose result give a good control and repeatability
of the prototype that attribute to its light weight, low proﬁle, anthropomorphic shape,
satisfactory output force and angular displacement in contraction and extension
movements.
In [10] is presented a low cost instrumented glove design for hands monitoring
rehabilitation system whose result demonstrating the relationship between of
ﬂexion/extension ﬁngers movements with the forces applied on each ﬁngertip. In [11]
is shown an entertainment interface of hands and prothesis rehabilitation whose pur-
pose is doing available the physiotherapy anywhere with a computer without the
necessity to go to a therapy center. In [12] is implemented a 3D hands rehabilitation
system with a haptic device whose result obtain that neurological recovery time is less
than other methods with robots rehabilitation.
In this work is presented an interactive system for hands and wrists rehabilitation
using the Unity3D software and the leap motion device. Furthermore, the system obtain
immersion because the interfaces developed are intuitive, friendly and easy to use.
Finally, the system presents a visual and audio feedback for showing that the task was
completed correctly.
2
Methodology for System Development
In this section is presented the stages that build the implemented system as is showing
in Fig. 1.
2.1
Acquisition of Signals
Leap Motion (Optical tracking device) is using to acquire hands movements which has
two IR monochrome cameras and three infrared LEDs. The device has a hemispherical
area approximately of view and has a detection of 1 m approximately. The data are sent
to the computer with USB. The hands movements visualization is realized using the
Unity3D software, which must have its own runtime of Leap Motion. In addition, it
must have the Unity Core Assets that contains the plugins, scripts and prefabs.
2.2
Scripting Development
The Unity3D software lets to develop 2D and 3D videogames. Scripting were per-
formed by C # programming language. The scripts let the interaction between the user
594
M. Pilatásig et al.

and the computer. The scripts let the performance of functions according to the right
position get by Leap Motion device.
Therefore, elements with cube form were used to perform the task. Each element
has a different variable that let to determine if the ship object had a collision. When the
object has a contact with other in a designed position, a new variable will be activated.
This variable will add four point in total at the end and it will determine that the task
was performed correctly.
To detect the movements performed by the user with the hands, the palm and
ﬁngers detection scripts are used respectively. They will be represented like a 3D vector
with values between zero and one in X, Y and Z positions.
In addition, to move the object within the virtual environment it’s necessary to
modify the object positions. A new script was created to resolve it. This script com-
pares the data of ﬂags and according to its value causes the displacement of the object
in different positions until the user requires it.
2.3
Interactive System Design
The applications design is related with real environments. Therefore, each complement
object of the application was designed in blender software. The 3D objects designed
will be used in the interactive system.
To design each interface many elements was created in the blender software which
exported the elements with .Fbx extension to be used in Unity 3D. By joining several
elements, a very friendly and intuitive environment for the user is achieved.
The two interfaces created have the appearance of a zoo. Here the user must fulﬁll
several tasks to ﬁnish correctly with each one of them.
Interface 1: It contains four rings within the zoo. The user must move the ship around
the environment according to the hands position. The movement makes it pass in order
GameObject
Audio
GameObject
Cursor
GameObject
Visual
GameObject
GameObject
Leap Motion
Modules
Audio
UNITY 3D
INPUTS
OUTPUTS
Conexión USB 
LeapMotion
SCRIPTS
GameObject
Visual
Fig. 1. Interactive system block diagram
Interactive System for Hands and Wrist Rehabilitation
595

for each of the rings, at the end of the fourth rings will appear several particles and a
sound which will motivate and indicate to the user that their task is performed cor-
rectly. The process is shown in Fig. 2.
The hands positions was speciﬁed by setting in the scripts. It will allow the ship
object to move up, down, forward, left, right and back.
Interface 2: In the same zoo were placed arcs and a sphere which will move in relation
to the movements that the user makes correctly with the hands.
By means of scripts several hands positions have been conﬁgured, which ones that
are enable and disable if the user makes the right movement. If the position is true or
correct, the object called the sphere will begin to move forward, left, right, and back
(Fig. 3).
3
How to Use
The applications are based on the movements of the wrist to help ﬁne motor rehabil-
itation, thus giving a beneﬁt by strengthening the muscles of the hands and wrist. In the
implemented interfaces, the movements shown in Fig. 4 are used.
Interface 1 has the ﬂexion, extension, pronation and supination movements, which
will help the movement of the ship to reach the goal of the game. The game is based on
passing the ship through each of the rings distributed in the interface. The position of
the hands for this application is shown in the Fig. 5. Table 1 indicates which movement
of the ship by making the movements of the wrist.
Fig. 2. Interface 1
596
M. Pilatásig et al.

Fig. 3. Interface 2
Fig. 4. Hands movements
Interactive System for Hands and Wrist Rehabilitation
597

The second application is based on the adduction and supination movements, to
achieve the goal of the game, which is to pass the sphere through all the arcs, for which
in Table 2 shows the movements of the hand so that the sphere moves in some
direction.
4
Test y Results
4.1
Test
The test was performed with 5 users (2 girls and 3 boys) aged between 8 and 12 years
old, it was uses the following inclusion criteria: age >6 years and <13 years, have mild
and moderate Down syndrome. The exclusion criteria was: have visual deﬁciency
and/or several audible deﬁciency. Before the task execution there were and explanation
of how to use the virtual games. Figure 6 shows the user movements used in the
interactive system.
Fig. 5. Hands Position
Table 1. Ship movements.
Hand
Real movement Ship movement
Left (Palm down) Extension
Up
Flexion
Down
Pronation
Go on
Right (Palm left)
Flexion
Right
Extension
Left
Supination
Go down
Table 2. Sphere Movement
Hand Real movement Sphere movement
Left
Adduction
Left
Supination
Go on
Right Adduction
Right
Supination
Go Down
598
M. Pilatásig et al.

4.2
Results
For the validation of the system the SEQ test [13, 14] was used, which serves to
evaluate through 14 questions the efﬁciency of the system, taking into account that the
last question is open.
The mean and standard deviation of each question with N = 5 are shown in Table 3
below. According to the parameters established by this test [15], this indicates that a
system is efﬁcient if it is in the range of 40–65, it was veriﬁed that the result of 59.6
indicates that the system is efﬁcient.
Flexion
Flexion
Extension
Supination
Adduction
Extension
Pronation
Fig. 6. Interactive system used for user
Table 3. Test SEQ
Questions
N = 5
Means
SD
Q1. How much did you enjoy your experience with the system?
3,8
0,75
Q2. How much did you sense to be in the environment of the system?
3,6
0,8
Q3. How successful were you in the system?
4,8
0,4
Q4. To what extent were you able to control the system?
3,8
0,75
Q5. How real is the virtual environment of the system?
4,8
0,4
Q6. Is the information provided by the system clear?
5,0
0
Q7. Did you feel discomfort during your experience with the system?
1,0
0
Q8. Did you experience dizziness or nausea during your practice with the
system?
1,0
0
Q9. Did you experience eye discomfort during your practice with the
system?
1,2
0,4
Q10. Did you feel confused or disoriented during your experience with the
system?
1,2
0,4
Q11. Do you think that this system will be helpful for your rehabilitation?
5,0
0
(continued)
Interactive System for Hands and Wrist Rehabilitation
599

In [16] an interactive system is shown that uses the Geomagic Touch haptic device,
which allows observing the movement of a virtual object at the same time that the user
makes movements of ﬂexion and extension of wrist. While the interactive system that is
proposed allows the user to visualize the movement of hands and ﬁngers within the
virtual environment in real time. The system makes an intuitive, user-friendly and
entertaining experience for the user.
5
Conclusions y Future Works
An interactive system was implemented using the Unity software and the leap motion
device to create interactive and user-friendly interfaces with visual and audible feed-
back that allow different movements of the hand and wrist, so that the user can
rehabilitate the affected limb. Five users tested the interactive system. Then applied to
the SEQ test, obtaining the result of 59.6. This result shows that the system can be used
for rehabilitation since users enjoy when they interact with each one of the interfaces.
As a future work in this interactive system can be integrated other devices like gloves
that help rehabilitation of ﬁngers and hands and get better signal collection. Also the
system can use the Oculus Rift to increase the user immersion.
References
1. Garg, H.T., Choudhury, Kumar, P., Sabitha, S.: Comparison between signiﬁcance of
usability and security in HCI. In: 3rd International Conference on Computational
Intelligence & Communication Technology (CICT), pp. 1–4, Ghaziabad (2017)
2. Xu, Z., Qiu, X., He, J.: A novel multimedia human-computer interaction (HCI) system based
on Kinect and depth image understanding. In: International Conference on Inventive
Computation Technologies (ICICT), pp. 1–6, Coimbatore (2016)
3. Safaei, A., Wu, Q.M.J.: Evaluating 3D hand motion with a softkinetic camera. In: IEEE
International Conference on Multimedia Big Data, pp. 290–291, Beijing (2015)
4. Zhi-heng, W., Jiang-tao, C., Jin-guo, L., Zi-qi, Z.: Design of human-computer interaction
control system based on hand-gesture recognition. In: 32nd Youth Academic Annual
Conference of Chinese Association of Automation (YAC), pp. 143–147, Hefei (2017)
5. Zhu, H., You, Q., Chen, W.: Target-focused video stabilization for human computer
interaction. In: 29th Chinese Control and Decision Conference (CCDC), pp. 7688–7693,
Chongqing (2017)
Table 3. (continued)
Questions
N = 5
Means
SD
Q12. Did you ﬁnd the task difﬁcult?
1,8
0,4
Q13. Did you ﬁnd the devices of the system difﬁcult to use?
1,0
0
GLOBAL SCORE (Total)
59,6
0,33
600
M. Pilatásig et al.

6. Liou, J.C., Lin, W.C., Kong, Y.Y.: Multi-channel module of heart rate and electromyo-
graphy clinical human-computer interaction system. In: IEEE International Conference on
Consumer Electronics - Taiwan (ICCE-TW), pp. 97–98, Taipei (2017)
7. Agrawal, R., Gupta, N.: Real time hand gesture recognition for human computer interaction.
In: IEEE 6th International Conference on Advanced Computing (IACC), pp. 470–475,
Bhimavaram (2016)
8. Guo, J., Li, N., Guo, S., Gao, J.: A LabVIEW-based human-computer interaction system for
the exoskeleton hand rehabilitation robot. In: IEEE International Conference on Mecha-
tronics and Automation (ICMA), pp. 571–576, Takamatsu (2017)
9. Wang, B., McDaid, A., Biglari-Abhari, M., Aw, K.C.: Design and development of a glove
for post-stroke hand rehabilitation. In: IEEE International Conference on Advanced
Intelligent Mechatronics (AIM), Munich, pp. 1047–1051, Germany (2017)
10. Ganeson, S., Ambar, R., Jamil, M.M.A.: Design of a low-cost instrumented glove for hand
rehabilitation monitoring system. In: 6th IEEE International Conference on Control System,
Computing and Engineering (ICCSCE), pp. 189–192, Batu Ferringhi (2016)
11. Sayilgan, M.E., Kaplanoğlu, E., Atasoy, A., Kuchimov, S., Özkan, M.: Hand rehabilitation
and prosthesis training interface. In: 19th National Biomedical Engineering Meeting
(BIYOMUT), pp. 1–6. Istanbul (2015)
12. Rodriguez, A., Li, X., Yu, W.: A 3-D hand rehabilitation system using haptic device. In:
12th International Conference on Electrical Engineering, Computing Science and Automatic
Control (CCE), pp. 1–6, Mexico City (2015)
13. Fitzgerald, D., Kelly, D., Ward, T., Markham, C., Caulﬁeld, B.: Usability evaluation of
e-motion: a virtual rehabilitation system designed to demonstrate, instruct and monitor a
therapeutic exercise programme. In: Virtual Rehabilitation, pp. 144–149 (2008)
14. Kalawsky, R.S.: VRUSE–a computerised diagnostic tool: for usability evaluation of
virtual/synthetic environment systems. Appl. Ergon. 30, 11–25 (1999)
15. Gil-Gómez,
J.A.,
Gil-Gómez,
H.,
Lozano-Quilis,
J.A.,
Manzano-Hernández,
P.,
Albiol-Pérez, S., Aula-Valero, C.: SEQ: suitability evaluation questionnaire for virtual
rehabilitation systems. In: Application in a Virtual Rehabilitation system for Balance
Rehabilitation, 2013 7th International Conference on Pervasive Computing Technologies for
Healthcare and Workshops, pp. 335–338, Venice (2013)
16. Pruna, E., Acurio, A., Tigse, J., Escobar, I., Pilatásig, M., Pilatásig, P.: Virtual system for
upper limbs rehabilitation in children. In: AVR International Conference on Augmented
Reality. Virtual Reality and Computer Graphics, LNCS. Springer, pp. 107–118, Verlag
(2017)
Interactive System for Hands and Wrist Rehabilitation
601

Toward a Combined Method for Evaluation
of Web Accessibility
Patricia Acosta-Vargas1(&), Sergio Luján-Mora2, Tania Acosta3,
and Luis Salvador-Ullauri3
1 Universidad de Las Américas-UDLA, Quito, Ecuador
patricia.acosta@udla.edu.ec
2 University of Alicante, Alicante, Spain
sergio.lujan@ua.es
3 Escuela Politécnica Nacional, Quito, Ecuador
tania.acosta@epn.edu.ec, l.salvador@cec-epn.edu.ec
Abstract. This study describes the problems of web accessibility, especially for
people with disabilities, as external conditions can distort user behavior and
limit the data that can be obtained. Several studies recommend combining some
methods with each other to achieve better results. This article proposes a
combined approach, with the application of automatic and heuristic tools to
make websites more accessible. In this study, we apply the Web Site Accessi-
bility Assessment Methodology (WCAG-EM) considered in the Web Content
Accessibility Guidelines 2.0 (WCAG 2.0). From the results, we conclude that
most tested websites can achieve an acceptable level of compliance. We propose
that future work can focus on optimizing this combined approach, also this study
can serve as a guide to help develop more inclusive websites.
Keywords: Accessibility  Automatics tools  Combined method
Evaluation  Heuristics  People with disabilities  WCAG-EM
WCAG 2.0  W3C  Website
1
Introduction
Currently a signiﬁcant amount of activities worldwide revolves around the Internet [9];
through the websites, we can review the information, materials, guides, tutorials, and
conferences. The Internet every time spans more activities that can be carried out
online.
An example is University students are frequently using the Internet for educational
purposes and for other purposes [18]. There are services necessary for daily life that are
only available through the Web.
This is possible due to e-government services, by which the government must ensure
that the websites are accessible to all citizens. If this were not the case, discrimination
scenarios could be created. In some countries, web accessibility legislation forces
government and local authorities to ensure a minimum of accessibility; however, after
evaluating many websites, this has not been accomplished. With the purpose of creating
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_57

inclusive websites, some authors have proposed many methods of evaluation of web
accessibility such as automatic tools, heuristics and tests with end users [5, 10, 23].
According to the standard of the International Organization for Standardization
(ISO) 2008 9241-171, the accessibility is deﬁned as the “Ergonomics of the interaction
man-system” which provides the usability for all systems, regardless of the type of user
[1, 2]. Web accessibility is known as the ease of access to a product or service, through
the websites [14]. To achieve adequate accessibility on a website, it is recommended to
apply by the Web Content Accessibility Guidelines 2.0 (WCAG 2.0) developed by the
World Wide Web Consortium (W3C).
WCAG standards are guidelines that help to validate if web pages apply the
guidelines to their sites to make them accessible and inclusive of people with
disabilities.
With the implementation of the WCAG 2.0, we can reach all the web accessibility
requirements suggested by the standard (ISO/IEC 24751-3: 2008) Information Tech-
nology - Individualized Adaptability and Accessibility in E-learning [11].
Prior to this study, we reviewed several articles related to the evaluation of web
accessibility. One of the articles indicates that the assessment of the accessibility in
websites of universities was performed using automated tools, where it was identiﬁed
that there are major barriers to accessibility for many users [5].
Another article of the analysis of the level of accessibility of the websites of the
Ecuadorian universities recommends developing a plan to initiate processes of adap-
tation or development of their accessible websites [12].
A third article about the research carried out in government web sites points out that
there is a lot of work to do and suggests applying best practices for inclusive websites [6].
On the other hand, there are several types of research on the evaluation of websites
with combined methods, Brajnik [3] proposes a comparative test of the methods of
evaluation of web accessibility.
The research on the combined methodology for the evaluation of the web acces-
sibility proposed by [15], in which it is considered that when assessing the accessi-
bility, we are faced with three different situations. The ﬁrst consists of evaluating a site
during its development until its completion with the purpose of adjusting it with all the
accessibility guidelines. In the second case, we validate and compare the performance
of the accessibility guidelines later being adapted by developers and after his release.
Finally, in the third situation, we cannot establish anything on the site that we want to
assess, for which we are forced to go back to the initial steps to check, the actual state
of the site, which involves using the analytical assessment methods.
The rest of this article is structured as followed: Sect. 2 explains about the
accessible web content, it is presents the automatic tools; the heuristic evaluation, in
Sect. 3 presents the method used and the case study, in Sect. 4 presents the results and
discussion; ﬁnally, in Sect. 5 the conclusions obtained in the research.
Toward a Combined Method for Evaluation of Web Accessibility
603

2
Accessible Web Content
The Web Content Accessibility Guidelines 2.0 (WCAG) 2.0 were published in 2008; a
new working draft of the Web Content Accessibility Guidelines 2.1 [21] has been
released with guidelines that cover a wide range of recommendations to make web
content more accessible.
The WCAG 2.0 is intended mainly for web content, with support material for
developers, web accessibility evaluators and others who require related standard
information for web accessibility. It includes 12 guidelines organized into four prin-
ciples [21, 22]:
1. Perceptible: considers the three main organs of the senses that we need to work as
vision, hearing and feeling. It includes 22 success criteria.
2. Operable: deﬁnes the ways of management such as navigation and user interface
for people with disabilities. It includes 20 success criteria.
3. Understandable: deﬁnes the forms of correct interpretation of the content. It
includes 17 success criteria.
4. Robust: considers the compatibility with current and future technologies. It
includes two success criteria.
The standard includes 61 success criteria, organized according to three levels of
conformity [22]:
1. Level A: all the checkpoints of priority 1 are satisﬁed; it is the lowest priority. It
includes 25 success criteria.
2. Level AA: all the checkpoints of priority 1 and 2 are satisﬁed; this is medium
priority. In addition to the criteria of the priority 1, it also includes 13 other success
criteria.
3. Level AAA: all the points of veriﬁcation of priorities 1, 2 and 3 are satisﬁed; this is
the highest priority. Includes the ﬁrst two priorities and 23 additional success
criteria.
2.1
Automatic Tools
Are instruments that are responsible for a judgment on the accessibility of the page or
website evaluated, by assessing the level of conformity in accordance with the
Guidelines (WCAG 2.0, Section 508). It is based on the automatic application of
certain rules of accessibility, without the use of any artiﬁcial intelligence method and
the implementation of a mechanical approach to obtain results. Because of this, the
supervision of an expert in accessibility is required to interpret and give validity to the
results [16].
The assessment tools in some cases are likely to produce misleading results,
because the application of web accessibility may not be as accurate as we think. There
are several automatic tools for the evaluation of the accessibility of websites; among the
most used we have [7].
604
P. Acosta-Vargas et al.

• TAW: allows automatic validation of those points referenced the three accessibility
priorities as described in the WCAG 2.0standard, noting the points manually
checked. We can select the following levels: A, AA, or AAA.
• Tenon: valid URL and code, has an extension for the browser. It is a licensed tool.
Registered users can monitor full sites and have access to data on the evaluations.
• WAVE: it is available as a free service by WebAIM (Web Accessibility In Mind),
to check the level of conformance with WCAG 2.0 and Section 508. To use
WAVE, the page should be on a public server, which can be a problem if are
working on a site or product in development.
• Cynthia Says: it allows the revision of a web page through URL indicating from
the beginning if you want to revise it in accordance with the WCAG 2.0 (A, AA,
AAA) or Section 508. The report is very comprehensive; it includes the criteria and
techniques, for each level, accomplished, not accomplished, not applicable, or that
require manual review.
• eXaminator: this tool is free and available in Spanish. The validation is performed
by including the URL of the page to scan, uploading the HTML ﬁle or directly
entering the code. It takes into consideration the Accessibility Guidelines for Web
Contents 2.0 (WCAG 2.0).
• Koa11y: it is available for Windows, OSX and Linux. It validates the website
through the URL. The report can be generated in formats compatible with HTML,
CSV, JSON, Markdown or XML. It also validates according to the WCAG 2.0 (A,
AA, AAA) and Section 508.
2.2
Heuristic Evaluation
A heuristic it is deﬁned is the way to search for a solution a problem using not rigorous
methods. The heuristic of accessibility consists of obtaining the knowledge by
assumptions rather than following a set formula to provide a methodological proposal
for the development of accessible websites [8].
Heuristics does not replace knowledge; it is a typical set of assumptions detailed by
an evaluator. On the other hand, evaluators with limited knowledge in web accessibility
may encourage the use of heuristics incorrectly [17]. To perform a heuristic evaluation
on accessibility, design elements were assessed to verify compliance with the principles
of web accessibility. There are some studies on heuristics of web accessibility, for
example, The IBM1 proposal, is based on the guidelines of the WCAG 2.0 and Sec-
tion 508: provide meaningful alternatives and relevant to non-text elements; support
consistent navigation and correct labeling; allow full and efﬁcient use of the keyboard;
respect user browser settings; ensure the appropriate use of controls owners; distrust in
the color only to encode and distinguish; allow users to control possible distractions; to
enable users to understand and control the time restrictions; to ensure that the website is
compatible with assistive technology content.
1 https://www-03.ibm.com/able/guidelines/ci162/accessibility_checklist.html.
Toward a Combined Method for Evaluation of Web Accessibility
605

3
Method
The study began with the characterization of the current methods of evaluation of web
accessibility. It proposes a combined method between the automatic tools and a
heuristic process [3]. We propose three phases: evaluation of the websites with auto-
matic tools; manual veriﬁcation and heuristic evaluation.
In the ﬁrst phase, we evaluated the websites with automatics tools which are
summarized on the WebAIM questionnaire [19]. The websites are evaluated by typing
the Uniform Resource Locator (URL) to determine if it meets some of the recom-
mendations of the WCAG 2.0.
In the second phase, it is manual veriﬁcation was carried out on the sites that
achieved a better score in accordance with automatic tools and reports of web acces-
sibility. In this phase, it is suggested to have the experience of an expert in web
accessibility. We apply the Website Accessibility Conformance Evaluation Method-
ology (WCAG-EM) that considered in the Web Content Accessibility Guidelines 2.0.
Finally, in the third phase, it applies the heuristic evaluation of web accessibility
was implemented. We help each other with the questionnaire the questionnaire pro-
posal by Hassan and Martín, on the Web site heuristic evaluation guide [10]. Besides, it
is suggested to apply the barrier walkthrough, suggestions by Brajnik [3, 4].
3.1
Case Study
This study was applied to the websites of Latin American universities selected
according to the worldwide classiﬁcation of Webometrics2.
From the 3664 registered universities in Latin America, a random sample of 26
countries were considered, and within this group of countries a total of 45 websites
were evaluated.
The websites evaluated were: Argentina, Bolivia, Brazil, Chile, Colombia, Costa
Rica, Cuba, Ecuador, El Salvador, French Guyana, Grenada, Guatemala, Guyana,
Haiti, Honduras, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, Puerto Rico,
Dominican Republic, Suriname, Uruguay and Venezuela. In Table 1, the websites of
the evaluated universities were registered.
Automatic tools used in this study to evaluate the websites include WAVE3 (Web
Accessibility Evaluation Tool), and Koa11y4. A heuristic evaluation process was
applied to websites that passed to the second phase [13]. The data and analysis are
available in the repository of the Alicante University5.
In Fig. 1 the process of evaluation of web accessibility is explained in three phases:
1. Evaluation of the websites with automatic tools.
2. Manual veriﬁcation.
3. Heuristic evaluation.
2 http://www.webometrics.info/es/Latin_America_es.
3 http://wave.webaim.org/.
4 https://open-indy.github.io/Koa11y/.
5 http://hdl.handle.net/10045/68867.
606
P. Acosta-Vargas et al.

Table 1. Evaluation with automated tools WAVE and Koa11y
URL
Errors
WAVE
Alerts
WAVE
Contrast
WAVE
Errors
Koa11y
Warnings
Koa11y
Notices
Koa11y
http://www.uba.
ar/
11
11
15
21
15
227
http://www.unlp.
edu.ar/
26
94
29
0
0
0
http://www.umss.
edu.bo/
22
8
15
32
88
205
http://www.
umsa.bo/
17
5
41
66
128
273
http://www.usp.
br/
45
71
99
2
0
0
http://ufrj.br/
25
3
34
56
39
215
http://www.
uchile.cl/
23
90
77
52
129
308
http://www.uc.cl/
9
23
27
50
59
229
http://unal.edu.
co/
41
44
17
47
109
265
http://uniandes.
edu.co/
13
197
54
82
119
756
http://www.ucr.
ac.cr/
19
235
91
113
100
494
http://www.una.
ac.cr/
10
107
7
31
95
261
http://www.uh.
cu/
0
1
0
1
2
3
http://cujae.edu.
cu/
9
24
43
14
139
324
http://www.usfq.
edu.ec/
31
33
19
62
89
251
http://www.epn.
edu.ec/
29
88
30
33
41
211
http://www.ues.
edu.sv/
22
6
34
31
32
108
http://www.uca.
edu.sv/
9
2
14
27
58
109
http://www.espe-
guyane.fr/
5
14
29
38
9
63
http://www.sgu.
edu/
7
36
15
96
69
282
http://www.ufm.
edu/
48
53
160
74
113
370
http://www.uog.
edu.gy/
24
15
56
26
85
227
(continued)
Toward a Combined Method for Evaluation of Web Accessibility
607

Table 1. (continued)
URL
Errors
WAVE
Alerts
WAVE
Contrast
WAVE
Errors
Koa11y
Warnings
Koa11y
Notices
Koa11y
http://www.ueh.
edu.ht/
3
16
1
6
35
96
http://www.unah.
edu.hn/
5
27
12
12
90
202
http://www.uwi.
edu/
20
9
29
47
64
191
http://www.
mona.uwi.edu/
20
164
49
27
85
549
http://www.
unam.mx/
2
115
13
6
4
194
http://www.
cinvestav.mx/
39
50
46
70
45
318
http://www.unan.
edu.ni/
83
254
23
111
159
564
http://www.
unanleon.edu.ni/
46
29
19
53
155
224
http://www.una.
py/
4
57
10
8
57
201
http://www.uaa.
edu.py/
6
42
2
19
47
152
http://www.utp.
ac.pa/
14
38
3
7
42
137
http://www.up.
ac.pa/
23
11
1
1
0
1
http://www.pucp.
edu.pe/
13
1
3
60
125
222
http://www.
unmsm.edu.pe/
47
25
11
94
103
277
http://www.upr.
edu/
1
24
3
4
88
250
http://www.
uprrp.edu/
13
41
25
28
9
204
http://www.
pucmm.edu.do/
34
24
48
54
222
248
http://www.intec.
edu.do/
46
19
74
66
94
459
http://www.uvs.
edu/
7
45
10
24
182
308
http://www.
universidad.edu.
uy/
3
76
30
4
252
327
http://ucu.edu.uy/
48
21
98
133
46
294
(continued)
608
P. Acosta-Vargas et al.

In the ﬁrst phase, automatic tools WAVE and Kao11y were applied, and which are
summarized on the WebAIM questionnaire [19]; the 45 university sites were evaluated
by typing the Uniform Resource Locator (URL) to determine if it meets some of the
recommendations of the WCAG 2.0. The data of the 45 evaluated websites is available
in the repository of the Alicante University6. In the second phase, a manual veriﬁcation
was carried out on the sites that achieved a better score in accordance with automatic
tools and reports of web accessibility; in this phase, it is suggested to have the expe-
rience of an expert in web accessibility.
Finally, in the third phase the heuristic evaluation of web accessibility was
implemented; it is suggested to apply the method of travel of the barrier [3, 4]. A barrier
of accessibility is any condition that makes it hard for people to reach a goal by using
Table 1. (continued)
URL
Errors
WAVE
Alerts
WAVE
Contrast
WAVE
Errors
Koa11y
Warnings
Koa11y
Notices
Koa11y
http://www.ula.
ve/
5
11
101
103
45
428
http://www.ucv.
ve/
7
68
77
26
359
343
Fig. 1. Process applied in the evaluation of the web accessibility
6 http://hdl.handle.net/10045/68867.
Toward a Combined Method for Evaluation of Web Accessibility
609

the website. The barrier is a failure mode of the website, it may be related to the
category of users involved, the type of technology used, the objective and the features
of the website. The following questionnaire was applied [10]:
1. Has the font size been deﬁned in a relative way, or at least, is large enough not to
hinder the readability of the text?
2. Does the type of source, typographic effects, line width and alignment employed
make it easier to read?
3. Is there a high contrast between the font color and the background?
4. Does it include images with “alt” attributes that describe the content?
5. Is it compatible website with different browsers?
6. Is it displayed properly with different screen resolutions?
7. Can the user enjoy all the contents of the website without having to download and
install additional plug-ins?
8. Has the weight of the page been controlled?
9. Can the web page be printed without problems?
Table 2 lists the websites of the universities that present errors during the evalu-
ation process with WAVE and Koa11y Table 2 considers questions from one to nine
that were manually evaluated by a web accessibility expert. The value of one has been
assigned to the questions that comply with the parameter evaluated and zero to the
questions that do not meet such standards.
The results obtained in the reports of web accessibility were related to the Website
Accessibility Conformance Evaluation Methodology (WCAG-EM) 1.0 [20]; which
includes: a deﬁnition of the evaluation scope, explore the target website, select a
representative sample, audit the selected sample and report the evaluation ﬁndings.
4
Results and Discussion
Table 1 shows the data registered using the automatic tools WAVE and Koa11y, in the
evaluation the errors, warnings, and notices were recognized. Table 2 presents the
websites of the universities with fewer errors. The parameters that were not considered
by the automatic tools are related to the font size, font type, typographical effects, line
width and alignment, the contrast between the font color and the background, the
Table 2. Heuristic evaluation with the nine questions
URL
1 2 3 4 5 6 7 8 9 Total
http://www.uh.cu/
1 0 0 0 0 0 0 0 0 1
http://www.ueh.edu.ht/
1 1 1 0 1 1 1 1 1 8
http://www.unam.mx/
1 1 1 1 1 1 1 0 1 8
http://www.una.py/
1 1 1 0 1 1 1 0 1 7
http://www.upr.edu/
1 1 1 0 1 1 1 1 0 7
http://www.universidad.edu.uy/ 1 1 1 1 1 1 1 0 0 7
610
P. Acosta-Vargas et al.

images with the “alt” attribute, the compatibility of the website between different
browsers, the different display resolutions, the display of all the contents of the website
without the need to download and install plugins, the weight of the page and printing
problems.
By correlating the errors recorded in Fig. 1 between WAVE and Koa11y, it can be
observed that with WAVE there is an upper atypical value, while with Koa11y there
are no atypical values, in both cases, it is observed that the data is concentrated in the
ﬁrst quartile. The automatic tools help evaluate the web accessibility, but none can
check all the techniques of WCAG 2.0 and can produce false positives.
In Total from Table 2, the value of eight points corresponds to 88.9% and the seven
corresponds to 77.8% satisfaction of the parameters evaluated in the websites. To
achieve 100% compliance in the heuristic evaluation, the website must achieve nine
points out of nine, although this represents a greater time and cost (Fig. 2).
The questions considered in Table 2 allowed the evaluation of the most repre-
sentative websites of the universities that reached an acceptable level, corresponding to:
Universidad de la Habana, Université d’Etat d’Haiti, Universidad Nacional Autónoma
de México, Universidad Nacional de Asunción, Universidad de Puerto Rico y a la
Universidad de la República.
5
Conclusions
The methods used allowed expert reviews to be made, while testing with automatic
tools could not guarantee 100% of web accessibility.
Fig. 2. Diagram of the WAVE errors vs. Koa11y
Toward a Combined Method for Evaluation of Web Accessibility
611

Automated web accessibility programs may not be as accurate; therefore, it is
suggested to use automatic tools during website development, but not as a reliable
monitoring system. It is always necessary to have an evaluation from an expert in web
accessibility.
This study can serve as a guide for future works, especially for evaluating websites
in a more reliable way, by combining automatic tools with the heuristic process. In the
next phase of this study, it is recommended to include web accessibility tests with end
users.
The reports generated by the automatic tools help to take the heuristics and man-
ually review them with a web accessibility expert. The beneﬁt of applying a combined
method is to save time and improve the accessibility of websites, for a future work it is
suggested to apply a third phase with groups of users with different disabilities.
References
1. Batanero, C., Karhu, M., Holvikivi, J., Oton, S., Amado-Salvatierra, H.R.: A method to
evaluate accessibility in e-learning education systems. In: 14th IEEE International
Conference on Advanced Learning Technologies, pp. 556–560 (2014)
2. Bevan, N.: How you could beneﬁt from using ISO standards. In: Extended Abstracts of the
32nd Annual ACM Conference on Human Factors in Computing Systems, pp. 2503–2504
(2014)
3. Brajnik, G.: A comparative test of web accessibility evaluation methods. In: 10th
International ACM Special Interest Group on Accessible Computing, pp. 113–120 (2008)
4. Martín, E., Haya, P.A., Carro, R.M.: Adaptation technologies to support daily living for all.
In: User Modeling and Adaptation for Daily Routines, pp. 1–21. Springer (2013)
5. Acosta-Vargas, P., Luján-Mora, S., Salvador-Ullauri, L.: Evaluation of the web accessibility
of higher-education websites. In: 15th IEEE International Conference on Information
Technology Based Higher Education and Training (ITHET), pp. 1–6 (2016)
6. Acosta-Vargas, P., Luján-Mora, S., Salvador-Ullauri, L.: Web accessibility polices of higher
education institutions. In: 16th IEEE International Conference on Information Technology
Based Higher Education and Training (ITHET) (2017)
7. Carreras, O.: Validators and tools for accessibility and usability consultancies. http://www.
usableyaccesible.com/recurso_misvalidadores.php. Accessed: 20 Aug 2017
8. Cockton, G., Woolrych, A.: Understanding inspection methods: lessons from an assessment
of heuristic evaluation. In: People and Computers XV Interaction without Frontiers, pp. 171–
191. Springer (2001)
9. Debevc, M., Kozuh, I., Hauptman, S., Klembas, A., Lapuh, J.B., Holzinger, A.:
Using WCAG 2.0 and heuristic evaluation to evaluate accessibility in educational web
based pages. In: International Workshop on Learning Technology for Education in Cloud,
pp. 197–207. Springer (2015)
10. Hassan, Y., Martín, F.J.: Web site heuristic evaluation guide. http://www.nosolousabilidad.
com/articulos/heuristica.htm. Accessed: 10 Aug 2017
11. International Standard Organization, ISO/IEC 24751-3:2008: Information Technology -
Individualized Adaptability and Accessibility in E-learning, Education and Training - Part 3:
“Access for All” Digital Resource Description (2008)
12. Authors: Details omitted for double-blind, Fourth Author (2017)
612
P. Acosta-Vargas et al.

13. Kreijns, C.J., Kirschner, P.A., Jochems, W.: The sociability of computer-supported
collaborative learning environments. Educ. Technol. Soc. 5, 8–22 (2002)
14. Luján-Mora, S.: Accesibilidad web. http://accesibilidadweb.dlsi.ua.es/. Accessed: 16 Oct
2017
15. Luján-Mora, S., Masri, F.: Evaluation of web accessibility: a combined method. In:
Information Systems Research and Exploring Social Artifacts: Approaches and Method-
ologies, Information Science Reference, pp. 314–331 (2002)
16. Mascaraque, E.S.: Herramientas para la evaluación de la accesibilidad Web [tools for the
evaluation of web accessibility]. Documentación de las Ciencias de la Información 32, 245–
266 (2009)
17. Paddison, C., Engleﬁeld, P.: Applying heuristics to perform a rigorous accessibility
inspection in a commercial context. In: ACM International Conference on Computer
Graphics and Interactive Techniques, Computers and the Physically Handicapped No. 73–
74, pp. 126–133 (2003)
18. Rayan, A., Dadoul, A.M., Jabareen, H., Sulieman, Z., Alzayyat, A., Baker, O.: Internet use
among university students in south west bank: prevalence, advantages and disadvantages,
and association with psychological health. Int. J. Ment. Health Addict. 15(1), 118–129
(2017)
19. WebAIM,
Web
Accessibility
in
Mind.
Checklist.
http://webaim.org/standards/wcag/
checklist. Accessed: 16 Oct 2017
20. W3C, Website Accessibility Conformance Evaluation Methodology (WCAG-EM) 1.0.
https://www.w3.org/TR/WCAG-EM/#procedure, W3C Working Group Note, 10 July 2014.
Accessed: 16 Oct 2017
21. World Wide Web Consortium, W3C: Web Content Accessibility Guidelines (WCAG) 2.1.
http://www.w3.org/TR/WCAG21/. Accessed: 16 Oct 2017
22. World Wide Web Consortium, W3C: Web Content Accessibility Guidelines (WCAG) 2.0.
http://www.w3.org/TR/WCAG20/. Accessed: 16 Oct 2017
23. Rocha, Á.: Framework for a global quality evaluation of a website. Online Inf. Rev. 36(3),
374–382 (2012)
Toward a Combined Method for Evaluation of Web Accessibility
613

Educational Math Game for Stimulation
of Children with Dyscalculia
Pablo Torres-Carrión(&), Christian Sarmiento-Guerrero,
Juan Carlos Torres-Diaz, and Luis Barba-Guamán
Computer Science Department, Universidad Técnica Particular de Loja,
San Cayetano, 11 01 608 Loja, Ecuador
{pvtorres,cfsarmiento,jctorres,lrbarba}@utpl.edu.ec
Abstract. Dyscalculia is a speciﬁc learning disorder, reﬂected in an arithmetic
capacity below the expected on individuals of similar chronological age, IQ and
schooling. In order to improve these abilities, several studies agree that didactic
strategies with play activities help to improve the difﬁculties of learning to
count, to do basic mathematical calculations, to deﬁne groups of objects and in
spatial thinking. From this context, the objective is to develop an educational
game for children suffering from dyscalculia, available to the public on the
Arcade and Android platforms. For development, the agile XP methodology is
applied, in addition to GameSalad Creator and Viewer, OpenProj, Adobe
Photoshop CS2, Voice Record Pro, and C++ as the base programming lan-
guage. The application has been submitted for validation in institutions of initial
and primary education.
Keywords: Dyscalculia  Educational games  Inclusive education
Mathematics teaching  Gamiﬁcation
1
Introduction
The educational game improves teaching and learning as well the student motivation in
classes [1]; it has been demonstrated that in educational environments, game platforms
complemented with digital didactic strategies improves the memory of objects and
images, decrease response times and increase the visomotor cognitive abilities [2]; also
improves the intuitive understanding of how numbers work, how to compare and
estimate quantities in a numerical sequence and ﬁnally serves as a basis for future
research [3].
Within the scope of the basic sciences in the general curriculum of infantile and
primary education, mathematics is fundamental and continues its study until superior
education; from the decade of the 70’s of the twentieth century a series of new notions
of relation and equivalence between sets were introduced, that evolved towards a
mechanical conception, and that by the end of the century would be established as
“modern mathematics” with a domain of knowledge centered on the interaction
between the teacher and the student, where knowledge is an important manifestation in
the “games” of the school [4]. Knowing the learning styles for mathematics, allows the
teaching process to adapt to the student, and becomes a practice with signiﬁcant
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_58

improvements in learning [5]; research demonstrates how didactic resources from
mathematics textbooks achieve development of learning styles, centered on modalities
that group each style into active, reﬂective, theoretical and pragmatic [6]; in congru-
ence, from modeling and evaluation of interaction in computer-supported learning and
game-based learning achieves improvements in learning quality [7]. From this context
of video games a tool of teaching support is proposed as a complement to the didactic
strategies of the teacher.
As a general objective, it is proposed the development of an educational game for
children with dyscalculia, which is available to the public on the platforms Arcade and
Android. Several speciﬁc objectives have been established for its fulﬁllment, starting
with the identiﬁcation of the characteristics of the dyscalculia and its connection with
the didactic resources, which will allow to know in depth the learning difﬁculties that
children have with dyscalculia and to ﬁnd the best solutions to deal with such difﬁ-
culties; the second refers to the design of assessment models applied to children with
dyscalculia, and measures the degree of learning in which children with this disorder
are found; the third is the development and publication of the educational game and the
last is the evaluation of the didactic game in an application scenario, to verify the
impact that the game has on children suffering from this disorder.
The main limitation in meeting the objectives is presented with teachers and parents
who do not have enough time to give continuous support to the project and therefore
the necessary information of feedback is not reached in time, extending the times of
development and tests. Another limitation is the cognitive bias of the subject, since the
results obtained from the assessments depend on the degree of knowledge of these in
the area of learning (mathematics), varying between different contexts.
For the development of the educational game project, the agile XP development
methodology is used, which has features such as: simplicity, communication and
feedback; this methodology will be ideal for developing short-term projects. Several
applications have been used for the planning, design, development and implementation
of the application: GameSalad Creator and Viewer, OpenProj, Adobe Photoshop CS2,
Voice Record Pro, and C++ as programming language base.
2
Dyscalculia and Education
2.1
Dyscalculia
In the fourth edition of the Diagnostic and Statistical Manual of Mental Disorders
(DSM-IV), dyscalculia is exposed as a disorder of calculation, having as its main
characteristic the limitation of arithmetic capacity (measured by standardized calcula-
tion tests or mathematical reasoning individually), which is below that expected with
respect to individuals of chronological age, IQ and schooling [8]; This disorder sig-
niﬁcantly interferes with academic performance and everyday activities that require
skills with numbers and calculus.
From the biological perspective Developmental Discalculia (DD) starts from the
presupposition that the genetic syndrome is the cause of alterations in the cognitive
proﬁle related to numerical processing; a premise that has been supported by studies of
Educational Math Game for Stimulation of Children
615

discalculous twins where it is reported that the index of heritability in monocygotic
discalculous twins is 0.73 and dizygotic twins of 0.56. However, there are no subtypes
that allow to make a theoretically oriented sampling selection and to characterize the
anatomo-physiological substrate of the individuals so classiﬁed, to establish differences
with respect to the typical subjects [9]. Kosc [10] deﬁnes it as a genetic or congenital
disorder of those parts of the brain that constitute the direct anatomo-physiological
substrate of the maturation of age-appropriate mathematical skills without a simulta-
neous impairment of general mental functions. Menon [11] explains changes in
working memory in people with dyscalculia, evidencing a disruption when visualizing
that during the resolution of arithmetic problems, there are areas of the brain that are
kept blocked. In children with dyscalculia in relation to a population of control, con-
cluding that they do not properly use the resources of the viso-spatial working memory.
In this disorder, several abilities can be affected: linguistic (understanding or
naming of mathematical terms, operations or concepts and decoding of problems
written in mathematical symbols …), perceptual (recognition or reading of numerical
symbols or arithmetic signs and grouping of objects …) (correctly reproduce numbers
or ﬁgures, remember to add numbers taking and take into account operational signs
…) and mathematics (follow sequences of mathematical steps, count objects and learn
multiplication tables …) [12]. According to DSM-IV data, between 2.5% and 6.4% of
school-age children present with calculus disorders; in addition 56% of children with
reading disorder also show poor performance in math and 43% of children with math
disorder display poor reading skills [8]. Therefore, these two disorders (dyslexia and
dyscalculia) have a direct relationship, although not always visible by teachers and
parents.
Regarding typing, Novoa [13] differentiates four groups: (i) primary: speciﬁc dis-
order of the calculus attached to a brain injury; (ii) secondary: directly related to other
disorders (difﬁculties in language, little capacity for reasoning and spatio-temporal
disorientation); (iii) dysarithmetic: has problems to associate vocabulary or numerical
resolution mechanisms; (iv) spatial: difﬁculty in ordering numbers according to a
spatial structure. Kosc [10], who made the ﬁrst neuropsychological deﬁnition of
dyscalculia, typiﬁes it in: (i) verbal: difﬁculty in naming terms and mathematical
relationships; (ii) lexical: difﬁculty reading mathematical symbols; (iii) graph: difﬁculty
in writing mathematical symbols and numbers; (iv) operational: difﬁculty performing
arithmetic operations; (v) practogsnotic: difﬁculty to list, compare, manipulate math-
ematical objects (real or drawn); (vi) ideognic: difﬁculty in understanding concepts,
establishing relationships and performing mathematical operations. The video game
would apply in the classiﬁcation of Novoa to the secondary, dysarithmetic and spatial;
and from Kosc to the lexical, graphic, operational and practogsnotic.
2.2
Didactic Strategies of Video Games for Mathematics
The epistemological root of Ludication and the games is born of the homo ludens of
Huizinga [14], that exposes that the art of touching is before the culture, being one of
the most signiﬁcant aspects the fact that it is fun, and has therefore less 5 character-
istics: it is free, it is not ordinary or real life, it is indifferent to aspects of locality and
duration, demand absolute order and allows to connect with non-material interests [15].
616
P. Torres-Carrión et al.

Caillois [16] extends this view from his study of the relationship between man, game
and games, proposing game theory, social aspects about game adaptation and the
impact of games on various social changes.
In the academic ﬁeld it has been corroborated how the games help signiﬁcantly in
the learning. Prensky [17], who is one of the precursors of serious games, explains that
these are used to teach children letters, tactical and practical skills to the military,
ﬁnancial references to auditors, CAD tools to architects, among others. Navarro et al.
[18] from an experimental study shows how from the work with active games it is
possible to maintain the attention of the student, achieving improvements in the
learning outcomes and the predisposition to carry out complementary activities.
Torres-Carrión [19] demonstrates how with the help of a videogame in a tablet it has
been possible to improve the phonological awareness of children with dyslexia.
As in the previous cases, dyscalculia can be treated from day to day activities such
as setting the table, counting cars, looking for numbers while walking, making the
purchase, among others; or in family games such as dominoes, card games, UNO,
rummi, chess, puzzles, bingo, monopoly, etc. [twenty]. As Urbano explains in his blog
dedicated to dyscalculia [20], games offer several advantages to the process of learning
mathematics:
• Allows group or individual work.
• Memorization of solutions is not possible.
• It promotes motor, sensory, visual and mental abilities.
• Stimulates logical and ordered thinking.
• Promotes student interest in research.
• Stimulates the capacity for abstraction.
• Stimulates mental concentration.
3
Methodology
3.1
Agile Software Development Methodology “eXtreme Programming”
(XP)
eXtreme Programming (XP) is based on a series of rules and principles that have been
developing throughout the history of software engineering; is considered one of the
agile methodologies of development, particularly for allowing to have a version
deliverable continuously, according to the structure of its development cycle. Darwish
[21] details four base columns that support the methodology XP:
• Communication
• Feedback
• Simplicity
• Courage
and details several characteristics that enable an efﬁcient teamwork:
Educational Math Game for Stimulation of Children
617

• XP approach is designed for use in an environment of rapidly changing require-
ments. It helps to reduce the cost of changes by being more ﬂexible to changing
requirements.
• The focus on customer increases the possibility that the software produced will
actually meet the needs of the users.
• The focus on small and incremental release decreases the risks of the project by
putting functionality in the hands of the users, helping them to provide timely
feedback regarding the new software.
• Continuous testing and integration enables the team to increase the quality of the
new software.
Lassenius as editor of the book Agile Processes in Software Engineering and
Extreme Programming [22], makes an updated compilation of the advantages of XP in
the company, for the development of video games and local development; so it is
evident that despite XP has nearly two decades of application, it is still very active for
the development of software solutions.
The life cycle of an XP project consists of six moments [23]:
1. Exploration: Customers outline the user stories that are of interest to the ﬁrst pro-
duct delivery.
2. Planning: the client establishes the priority of each user history, and correspondingly,
the programmers make an estimate of the necessary effort of each one of them.
3. Iterations to Release: The Delivery Plan consists of iterations of no more than three
weeks. In the ﬁrst iteration one can try to establish a system architecture that can be
used during the rest of the project.
4. Productionizing: requires additional testing and performance reviews before the
system is moved to the client environment.
5. Maintence: while the ﬁrst version is in production, the XP project must keep the
system running while developing new iterations.
6. Death: happens when the client has no more stories to be included in the system;
requires that the customer’s needs be met in other aspects such as performance and
reliability of the system.
3.2
Software and Tools Used in the Development of the Application
In the development is made use of several software tools, to meet each of the devel-
opment needs:
• Raster Graphics Editor Adobe Photoshop CS2: for the design of personals, images,
numbers, pictures, etc.
• Voice Record Pro: to record voice notes and sounds with unlimited duration with
conﬁgurable quality; the recorded voices are in standard AAC/MP4/M4A format.
• GameSalad Creator application development platform: for the construction of the
application; does not require any coding, and ﬁts mobile operating systems like
Google’s Android or Apple’s IOS.
618
P. Torres-Carrión et al.

• GameSalad Viewer App: Connects to the GameSalad Creator desktop software. It
allows you to test, preview the GameSalad projects on the mobile device before
being published on the application distribution platforms.
• OpenProj: for project management, allowing work with Gantt and PERT diagrams.
• StarUML: for software modeling. It is an open source software modeling tool that
supports UML (Uniﬁed Modeling Language).
4
Development
4.1
Exploration
A theoretical revision of the needs of this population is carried out; in addition an
expert (teacher of mathematics) is used to develop the didactic activities and to put
them into the game. It also takes into account the perceptible and measurable char-
acteristics of an individual with this disorder [12], which represent the input to be
considered for the design of the solution:
• Symbols (often numbers) are written upside down or rotated.
• Similar-looking digits (6 and 9, 3 and 8) are confused with one another.
• Difﬁculty to take the distance between digits correctly, e.g. numbers 8 and 12 when
they appear in succession are read as 812.
• Difﬁculty in recognizing and using symbols for the four types of basic arithmetic
operations.
• Difﬁculty copying numbers or geometric ﬁgures or reproducing them from memory.
• Difﬁculty writing or reading the correct value of a number that has two or more
digits.
• Difﬁculty changing from one type of arithmetic operation to another.
• Problems to understand differences in magnitude between different numbers, for
example realizing that 93 is 4 more than 89.
• Problems to remember which steps to follow when performing a particular arith-
metic calculation, among others.
Puente [24] in her thesis difﬁculties and ICT: dyslexia, dysgraphia and dyscalculia,
presents us with resources that work some of the characteristics presented by students
with dyscalculia: sums and balloons, number rain, number-quantity relationship,
Cuisenaire rods-series, Cuisenaire rods-length, abacus, puzzles of numbers and sum of
coins; these results are based on a previous study of the observed needs and charac-
teristics of each disorder.
4.2
Planning and Iterations to Release
Thirty-four user stories are planned, which are visible in Fig. 1. These stories were
organized strategically together with the expert who was validating the results after
each iteration (Fig. 2).
Educational Math Game for Stimulation of Children
619

4.3
Productionizing
The application has been developed to run under Android platform from version 4.0
and is available for download, for Android applications in PlayStore: https://play.
google.com/store/apps/details?id=com.cfsg.discalculia and for Arcade: http://arcade.
gamesalad.com/games/143519#sthash.eidttekJ.Bka3CE5B.dpbs.
4.4
Maintance
The test of the app in each of its stories and scenes is done with GameSalad Viewer,
which connects to the software GameSalad Creator desktop, and allows to emulate
GameSalad projects on the mobile device before being published on the platforms of
distribution of the application. It is done in conjunction with the user (teacher) to check
all user stories and their goals.
.
5
Evaluation
According to the fourth speciﬁc objective, to evaluate the educational game in a real
scenario of application, it is important to mention that an evaluation was carried out on
a 9-year-old girl who suffers from this disorder with a medical history that veriﬁes it
(Fig. 3).
With the consent of the parents and under the teachers’ recommendation, the use of
the didactic game is made, measuring the variables estimated by Tere Silva [25]
and Puente [24] which reﬂect standard parameters of the learning treatment and
Fig. 1. Work planning scheme of user stories.
620
P. Torres-Carrión et al.

mathematical abilities. The student managed to comply with each of the phases of the
game, thus overcoming each of the challenges proposed in the three levels of difﬁculty.
After completing the phases of the game, the student shows improvement in the reso-
lution of math problems, according to teacher and parents’ observation, exposed in an
interview at the end of the intervention. It is not the purpose of this document to evaluate
the cognitive validity of the application, only a functional evaluation is performed; the
results of the cognitive study will be published in another study.
a.  History 1: Home
b.  History 7: Choose player
c.  History 18. Choose numbers
Fig. 2. Screenshots results of three user stories.
Educational Math Game for Stimulation of Children
621

6
Conclusions
The non-technological didactic resources most used in the dyscalculia disorder were
the basis for the development of the educational game that helps to treat the charac-
teristics that these children present. Through a real scenario of application of the
educational game, it was veriﬁed that this technological resource is an effective tool to
treat children suffering from this disorder.
Children, teachers, parents and the community in general have an educational game
available that will help improve math skills and reduce dyscalculia disorder; this
application has a friendly interface, simple and easy to use, according to the require-
ments of the user. The game will be available on platforms Arcade and Android, and is
free to download.
The XP methodology was a great help for the development of the game. As it is an
agile methodology, and have continuous versions in each iteration, the user has a
resource on which to provide feedback.
References
1. González-González, C.S., Navarro-Adelantado, V.: Métodos y técnicas para la evaluación de
la experiencia emocional de niños y niñas con videojuegos activos. In: Ponsa, P., Román,
J.A., Guasch, D. (eds.) XVI Congreso Internacional de Interacción Persona-Ordenador.
Interacción 2015, pp. 141–151. AIPO, Vilanova i la Geltrú. Barcelona, España (2015)
2. Torres-Carrión, P.: Evaluación de Estrategias de Aprendizaje con HCI Kinect en alumnos
con Síndrome de Down (2017)
3. Trujillo, K., Chamberlin, B., Wiburg, K., Armstrong, A.: Measurement in learning games
evolution: review of methodologies used in determining effectiveness of math snacks games
and animations (2016)
4. Artigue, M., Douady, R., Moreno, L., Gómez, P.: Ingeniería Didáctica en Educación
Matemática. Grupo Editorial Iberoamérica, Bogotá (1995)
5. Gallego Gil, D.J.: Los estilos de aprendizaje y la enseñanza de las matemáticas. Rev.
Comput. Educ. 19, 95–112 (2008)
Fig. 3. Evaluation of the application. Case study
622
P. Torres-Carrión et al.

6. Santaolalla, E., Gallego, D.J., Urosa, B.: Los libros de texto de matemáticas y su capacidad
para desarrollar los distintos Estilos de Aprendizaje: estudio piloto. J. Learn. Styles. 8, 78–
210 (2015)
7. Blanco-Izquierdo, F., González, C.S., Collazos, C.A.: Modelado y Evaluación de la
Interacción en el Aprendizaje CSCL y Juegos Colaborativos. Development 190, 4921 (2016)
8. Psiquiatría, A.A.: Manual Diagnóstico y Estadístico de los Trastornos Mentales DSM V.
Washingt, Arlingt (2014)
9. Pérez, N., Castro, D., Reigosa, V.: Bases Biológicas de la Discalculia del desarrollo. Rev.
Cuba. Genet Comunit. 2, 14–19 (2008)
10. Kosc, L.: Developmental dyscalculia. J. Learn. Disabil. 7, 164–177 (1974)
11. Menon, V.: Working memory in children’s math learning and its disruption in dyscalculia.
Curr. Opin. Behav. Sci. 10, 125–132 (2016)
12. Pérez, E., Bermúdez, I., Álvarez, N.D.: La discalculia, como uno de los trastornos especíﬁco
del aprendizaje. Conrado Rev. 12, 130–138 (2016). Pedagógica la Univ. Cienfuegos.
13. Novoa, A.: CÓMO SUPERAR LAS DIFICULTADES (How to overcome difﬁculties in
mathematics classes) (2015)
14. Huizinga, J.: Homo ludens: el juego como elemento de la historia. Azar, Lisboa (1943)
15. Huizinga, J.: Homo ludens: a study of the play-element in culture (1998)
16. Caillois, R.: Man, Play and Games (1961)
17. Prensky, M.: Digital natives, digital immigrants part 1. Horiz. 9, 1–6 (2001)
18. Navarro, V., Gonzalez, C.S., del Castillo, J.M., Quirce, C., Cairós, M.: Un programa
integrado juego motor-videojuego activo para desarrollar hábitos saludables. In: II Simposio
Internacional de Políticas Educativas y Buenas Prácticas TIC., Santa Cruz de Tenerife -
España (2013)
19. Torres-Carrión, P., González-González, C., Basurto-Ortiz, J., Vaca-Gallegos, S.: Enhancing
phonological awareness in children with dyslexia: application based on a computer learning
game environment. In: Proceedings of the 4th Workshop on ICTs for Improving Patients
Rehabilitation Research Techniques, pp. 121–124. ACM, New York, NY, USA (2016)
20. Urbano: Características de los estudiantes con Discalculia. http://estudiantescondiscalculia.
blogspot.com/
21. Darwish, N.R.: Towards an approach for evaluating the implementation of extreme
programming practices. Int. J. Intell. Comput. Inf. Sci. - IJICIS. 13, 55–67 (2013)
22. Lassenius, C., Dingsøyr, T., Paasivaara, M.: Agile processes, in software engineering, and
extreme programming. In: Proceedings of 16th International Conference, XP. Springer
(2015)
23. Auer, K., Miller, R.: Extreme Programming Applied. Addison-Wesley, Boston (2002)
24. Puente, A.G.: DIFICULTADES DE APRENDIZAJE Y TIC : dislexia, disgrafía y
discalculia. TFG más bien malo, pp. 1–49 (2001)
25. Silva Tere: Discalculia Tere Silva.VOB – YouTube. https://www.youtube.com/watch?v=
VVWWiuNWoQU
Educational Math Game for Stimulation of Children
623

Intelligent Tutoring Based on a Context-Aware
Dialogue in a Procedural Training
Environment
José Paladines1,2(&) and Jaime Ramírez1,2
1 Technical Sciences Faculty, Universidad Estatal del Sur de Manabí,
Jipijapa, Ecuador
jose.paladines@unesum.edu.ec
2 Computer Science School, Universidad Politécnica de Madrid, Madrid, Spain
jramirez@ﬁ.upm.es
Abstract. In this paper we present a proposal for an Intelligent Tutoring
System with context-based natural language for procedural training in a 2D/3D
virtual environment, in which each student will be trained takes into account
his/her speciﬁc features, his/her progress in the development of the task and the
environment where the task is performed. In that way, tutoring feedback will be
the result of a context-aware dialogue that will be managed through a dialogue
manager built on some of the well-known platforms currently available.
Keywords: Intelligent Tutoring System  Natural Language Processing
Context-aware dialogue  Procedural training  Virtual environment
1
Introduction
In recent years, Intelligent Tutoring Systems (ITS) have experienced remarkable
advances thanks to the use of various tools and/or techniques that provide signiﬁcant
learning gains [1]. These advances have been evidenced both in the environment for
which they are developed, and in the tutoring process itself; however, there are very
few ITSs for procedural training in which the student can interact with the system
through an avatar in a 2D/3D virtual environment, and can dialogue with the system by
using natural language.
In this sense, this document aims to present a proposal for an ITS for procedural
training in a 2D/3D virtual environment with the capacity for dialoguing based on the
context. In this way, each student will be trained through a dialogue in natural language
that takes into account his/her speciﬁc features, his/her progress in the development of
the task and the environment where the task is performed. So, tutoring feedback will be
the result of a context-aware dialogue.
The use of Natural Language Processing techniques makes learning and the
teaching process much more adaptable to the student. Therefore, to support the dia-
logue, we will use a dialogue manager built on some of the well-known platforms for
creating dialogue managers currently available. These platforms have reached a
remarkable degree of maturity for their facilities to interpret and process natural lan-
guage in different application domains.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_59

This work provides a ﬁrst step in the process of designing a system with the
features mentioned above. The remaining of this article is structured as follows: Sect. 2
presents the related work; Sect. 3 shows the proposed approach that includes a brief
analysis of some well-known platforms for the construction of dialog managers, the
main features of the proposed system and an sketch of the ITS with NLP architecture;
and ﬁnally, Sect. 4 presents the conclusions of the work.
2
Related Works
In this section, we will focus on ITSs with Natural Language Processing (NLP) for
2D/3D virtual environments. Within this group, we will distinguish between ITSs
oriented to either teaching of concepts or training in tasks (procedural knowledge).
Before presenting this group of ITSs, we will mention some ITSs for procedural
training that, despite not having NLP, are remarkable systems that can supervise the
student behavior and provide tutoring feedback. These systems, Steve [2], Lahystotrain
[3], SafeChild [4], provide a tutoring feedback that comprises: giving advice when
student makes a mistake or asks for help; and motivating him/her when he/she is idle
for a long time. In this kind of systems, tutoring messages are prefabricated explana-
tions associated with anticipated errors or hints. However, they cannot understand and
answer student’s open questions, or evaluate student’s knowledge from his/her com-
ments throughout the practice.
Concerning ITSs with NLP, ﬁrstly, we will mention some systems whose tutoring
feedback is directed to teaching concepts. CircSim [5] considered as one of the ﬁrst
ITSs with natural language dialogue, performs a syntactic analysis of the student’s
inputs. As questions only admit short answers of one or two words, its range of
response is limited, which does not induce to a complex language. Andes [6] interacts
with students using coached problem solving, the student-tutor interaction changes
according to the progress being made. Generate a hint when the student gets stuck and
asks for help, the hints are generated based on the state of the student model at the time
the student asks for help. Why2-Atlas [7] supports that students write long explanations
of simple mechanical phenomena, and uses a symbolic analysis (CARMEL) to dis-
cover their misconceptions. Autotutor [8] maintains mixed initiative dialogue in
interactions in which students can explain concepts, and then their explanations are
compared with a set of expectations (ideal answers) and misconceptions (incorrect
answers) by using Latent Semantic Analysis. Both Why2-Atlas and Autotutor help the
student to build right explanations through the dialogue, but they are different in the
employed NLP techniques for interpreting student’s utterances. Beetle II [9] imple-
ments an approach based on task-oriented dialogue systems, which is more domain
speciﬁc than the Autotutor one. The dialogue is used to support the student throughout
a “Predict-Verify-Evaluate” cycle, in which the student has to explain his/her
predictions/errors on a simulation. To guide the student, Beetle II applies tutoring
tactics that can be used in different problem-solving contexts. In addition, Beetle uses
an ontology to represent domain knowledge.
Next, we will mention the only ITSs with NLP for procedural training that can be
found in the literature to the best of our knowledge, Paco and Jacob. Paco [10] supports
Intelligent Tutoring Based on a Context-Aware Dialogue
625

tutoring actions as part of a collaborative dialogue system (built on Collagen) that uses
rules of discourse act generation. This generation is based on a task model, a student
model and the interaction with the student. Jacob [11] teaches to solve the problem of
the Towers of Hanoi in a 3D virtual environment, and the interaction with the user is
given by performing actions concerning the task. Both of these systems provide help
when asked for and positive feedback, although in the case of Jacob it is occasional. In
general, Paco and Jacob suffer from many limitations regarding natural language
dialogue; however, despite not being able to understand open questions from students,
they provide answers based on a certain level of understanding.
3
Proposed Approach
As we can see in the related work section, there is a remarkable shortage of ITSs for
procedural training in a 2D/3D virtual environment with NLP. In this sense, we pro-
pose an ITS for procedural training equipped with a dialogue manager implemented
through one of the currently available platforms.
3.1
Platforms for Creating Dialogue Managers
The platforms for the management of dialogue in natural language are reaching a high
degree of maturity in the treatment of the structures of dialogue and the interpretation of
the concepts thanks to the advanced techniques of NLP that they implement. This has
facilitated a signiﬁcant proliferation of its use in different domains.
These platforms offer a diversity of Application Programming Interface (APIs) for
its development and the integration with other external services. The most well-known
platforms are Api.ai from Google, Wit.ai from Facebook Messenger, Watson from
IBM and LUIS from Microsoft.
In general, these platforms allow the implementation of a dialog ﬂow controlled by
a workspace in which the intentions, entities and dialog nodes are deﬁned.
• Intentions represent the purpose of a user’s entry (request).
• Entities represent a term or object that is relevant to intentions and provides a
context for intention. Most platforms have predeﬁned intentions that give a greater
robustness to the application.
• The dialog is a logical tree-like conversation ﬂow that deﬁnes how the application
responds when it recognizes deﬁned intentions and entities.
These platforms recognize the inclusion of context variables, which play a very
important role in the ﬂow of dialogue, allowing the dialogue to be adjusted to the
situations of a speciﬁc scenario. Facilitating the internal and external manipulation of
the context in order to establish a communication mechanism between the external
application and the dialogue manager.
626
J. Paladines and J. Ramírez

3.2
Main Features of an ITS with NLP
An ITS with NLP must provide a suitable tutoring feedback; for this, the dialogue
manager has to implement a context-aware dialogue. To provide this kind of dialogue,
the context should contain information on the virtual environment (static and dynamic),
the student knowledge and the student’s progress in the activity to be performed. Based
on this information it will be possible to provide a personalized tutoring through a
context-aware dialogue adapted to: what the student knows; his/her avatar location in
the virtual environment at the time of the dialogue; and the phase of the procedure in
which the student is. Some examples of personalized tutoring feedbacks would be the
following ones:
• Answer questions about the location of an object in the virtual environment or how
to reach it, even when this object is far away from the student’s avatar.
• Answer questions about the next action to be done.
• Recommend learning activities to bridge knowledge gaps.
• Provide proactively clues to guide the student with the execution of a task.
• Encourage an affective dialogue in the face of student inactivity or moments of
uncertainty during the practice.
3.3
The Architecture
Based on what was explained in the previous section, in the Fig. 1 we show the
architecture of the proposal of a ITS with NLP for procedural training in a 3D/2D
virtual environment, which has as main axis the environment where the training is
performed in order to provide a personalized tutoring.
Fig. 1. Architecture of ITS with NLP for procedural training
Intelligent Tutoring Based on a Context-Aware Dialogue
627

The model has three components for supporting natural language tutoring, each one
of which fulﬁlls different functions.
The procedural training environment is the scenario that simulates the real world
where the training activities are carried out. It may be designed with 3D elements to
give more realism to the tasks that must be performed.
The Intelligent Tutoring System will integrate the modules corresponding to a
classic ITS such as the student model, tutoring model, expert model, communication
model.
Additionally, a world model will be added to the ITS to represent the characteristics
of the training environment, in order to enable an effective tutoring process. It will
contain information about the constituent aspects of the environment, related to the
avatars and 3D/2D objects of the virtual environment. In addition, this model will
represent the physical structure of the scenarios and their content so that the system can
answer a question regarding the situation of an object or how to go from one place to
another. This information will be necessary for tutoring and validating the actions
performed by the student. Another two modules that will have a key role in our
proposal will be the student model and the expert model.
Student model will contain information related to the student’s actions; his/her
movements through the virtual environment; the time he/she needed to perform a
particular action or group of actions; the questions he/she asked; the hints he/she
received from the tutor; etc. From this information student’s knowledge can be
inferred, which, in turn, it will be useful to decide the best tutoring strategy.
Expert model will specify the knowledge that must be taught to the student. As we
are addressing procedural training, this model will contain a description of the pro-
cedure to be learned.
Within the ITS, the context will be deﬁned in terms of an ontology and it will be
populated with information coming from the student model (knowledge state, activity
progress, student trace), the expert model (correct plan, next correct action) and the
world model (virtual world structure, student avatar position, object descriptions).
Then, before passing the context to the dialogue manager, the ontological represen-
tation of the context will be translated into another representation understandable by the
dialogue manager. To this end, we will adopt the student model proposed in [12].
The dialog manager will contain the deﬁnition of the dialog structure, intentions
and entities speciﬁcally intended for the training environment as well as the runtime of
the chosen platform. This component will be responsible of the communication with
the user taking into account the contextual information provided by the models that
integrate the ITS. Additionally, dialogue manager will be able to update the context
after inferring the student knowledge from his/her utterances. This updated context will
be sent back to the ITS.
The user can interact with the procedural training environment generating events
such as performed actions, actions attempts, questions, etc., which will be provided to
the ITS throughout a practice.
The following pseudocode outlines how the dialogue manager may be integrated in
the tutoring strategy applied by the ITS.
628
J. Paladines and J. Ramírez

1: Tutor gives a welcome message with possible hint
2: Wait for the student to generate an event
3: If the student attempts to do some action before the timeout
4:
If the action is correct
5:
Ask dialogue manager to give OK
6:
Let student do the action
7:
Else
8:
Ask dialogue manager to give an error message, if necessary
9:
Block the action, if necessary
10:
End if
11:
If the student seems to need a hint based on his/her student model
12:
Give dialogue manager the user event and context
13:
Give student the hint (the dialogue manager’s answer) 
adjusted to his/her knowledge state and the next right 
action
14:
End if
15: End if
16: If the student says something before the timeout
17:
Give dialogue manager the user utterance and context
18:
Show the answer of the dialogue manager to the student
19: End if
20: If the student attempts to do nothing or says nothing, and expires the 
timeout
21:
Give dialogue manager the user event and context
22:
Give a hint adjusted to his/her knowledge state
23: End if
24: Update student model and world model
25: Return to Step 2
4
Conclusion
We have presented a proposal of an ITS with NLP for procedural training that uses a
dialog manager built on some well-known platform to create this type of components.
For a better understanding of the proposal, we show the system architecture that would
cover the ITS and the dialog manager, along with a pseudocode that describes how the
dialogue manager would be integrated into the ITS tutoring strategy.
This is an ongoing work of a Ph.D. thesis. In the future we plan to reﬁne the
presented architecture by: deepening in the concept of “context”; and designing a
dialogue structure that leverages the context information to provide natural language
messages properly adapted to the student and his/her situation in the virtual
environment.
Acknowledgements. Paladines would like to acknowledge ﬁnancial support from the Univer-
sidad Estatal del Sur de Manabí.
Intelligent Tutoring Based on a Context-Aware Dialogue
629

References
1. Graesser, A.C., VanLehn, K., Rosé, C.P., Jordan, P.W., Harter, D.: Intelligent tutoring
systems with conversational dialogue. AI Mag. 22, 39–51 (2001)
2. Rickel, J., Johnson, W.L.: Animated agents for procedural training in virtual reality:
perception, cognition, and motor control. Appl. Artif. Intell. 13, 343–382 (1999)
3. Los Arcos, J., Muller, W., Fuente, O., Orúe, L., Arroyo, E., Leaznibarrutia, I., Santander, J.:
LAHYSTOTRAIN: integration of virtual environments and ITS for surgery training. In:
Gauthier, G., Frasson, C., VanLehn, K. (eds.) ITS 2000. LNCS, vol. 1839, pp. 43–52.
Springer-Verlag (2000)
4. Gu, Y., Sosnovsky, S., Ullrich, C.: SafeChild: an intelligent virtual reality environment for
training pedestrian safety skills. In: Conole, G., Klobučar, T., Rensing, C., Konert, J.,
Lavoué, É. (eds.) Design for Teaching and Learning in a Networked World: 10th European
Conference on Technology Enhanced Learning, EC-TEL 2015, Toledo, Spain, 15–18
September 2015, Proceedings, pp. 141–154. Springer International Publishing, Cham (2015)
5. Glass, M.: Processing language input in the CIRCSIM-tutor intelligent tutoring system. In:
Moore, J.D., et al. (eds.) Artiﬁcial Intelligence in Education, pp. 210–221. IOS Press, San
Antonio (2001)
6. Gertner, A.S., VanLehn, K.: Andes: a coached problem solving environment for physics. In:
Gauthier, G., Frasson, C., VanLehn, K. (eds.) Intelligent Tutoring Systems: 5th International
Conference, ITS 2000, Montréal, Canada, 19–23 June 2000, Proceedings, pp. 133–142.
Springer, Berlin, Heidelberg (2000)
7. VanLehn, K., Jordan, P.W., Rosé, C.P., Bhembe, D., Böttner, M., Gaydos, A., Makatchev,
M., Pappuswamy, U., Ringenberg, M., Roque, A., Siler, S., Srivastava, R.: The architecture
of Why2-Atlas: a coach for qualitative physics essay writing. In: Cerri, S.A., Gouardères, G.,
Paraguaçu, F. (eds.) Intelligent Tutoring Systems: 6th International Conference, ITS 2002,
Biarritz, France and San Sebastian, Spain, 2–7 June 2002, Proceedings, pp. 158–167.
Springer, Berlin, Heidelberg (2002)
8. Graesser, A., et al.: AutoTutor: an intelligent tutoring system with mixed-initiative dialogue.
IEEE Trans. Educ. 48, 612–618 (2005)
9. Dzikovska, M., Steinhauser, N., Farrow, E., Moore, J., Campbell, G.: BEETLE II: deep
natural language understanding and automatic feedback generation for intelligent tutoring in
basic electricity and electronics. Int. J. Artif. Intell. Educ. 24, 284–332 (2014)
10. Rickel, J., Lesh, N., Rich, C., Sidner, C., Gertner, A.: Collaborative discourse theory as a
foundation for tutorial dialogue. In: Proceedings of Sixth International Conference on
Intelligent Tutoring System, pp. 524–551. Springer-Verlag (2002)
11. Evers, M., Nijholt, A.: Jacob - an animated instruction agent in virtual reality. In: Tan, T.,
Shi, Y., Gao, W. (eds.) Advances in Multimodal Interfaces—ICMI 2000: Third International
Conference, Beijing, China, 14–16 October 2000, Proceedings, pp. 526–533. Springer,
Berlin, Heidelberg (2000)
12. Clemente, J., Ramírez, J., de Antonio, A.: A proposal for student modeling based on
ontologies and diagnosis rules. Expert Syst. Appl. 38, 8066–8078 (2011)
630
J. Paladines and J. Ramírez

Communities of Language Learners: Mobility,
Usability and Learning
Fernanda Maria Pereira Freire1(&), André Constantino da Silva2,
and Isaque Miguel Pires1
1 Nucleus of Informatics Applied to Education (NIED),
UNICAMP, Campinas, Brazil
ffreire@unicamp.br, isaque.prs@gmail.com
2 Federal Institute of Sao Paulo, Hortolândia, Brazil
andre.constantino@ifsp.edu.br
Abstract. Mobile Assisted Language Learning consists of using the mobile
devices with the purpose of supporting the acquisition of languages. We aim to
investigate how different people use mobile applications (apps) on their own
initiative and out of a formal educational to improve language skills (Speaking,
Listening, Reading and Writing). In this exploratory work, we present how a
particular user interacts with 18 different interlocutors, both English native and
non-native English speakers in three months using the app Tandem to improve
him/her English proﬁciency. We analyzed these interactions to ﬁnd (i) usability
issues which may interfere on the productive and satisfactory use of the app and
(ii) potential learning situations, discussing about the feasibility and the efﬁ-
ciency of mobile learning in the language acquisition. We found 4 types of
interventions that promoted some kind of learning and some difﬁculties related
to the app´s usability.
Keywords: Mobile Assisted Language Learning  m-Learning
Mobile devices and applications  Usability  Functionalities
1
Introduction
Learning other language(s) has become a necessity in the contemporary world, either to
ingress in the job market or for personal satisfaction. The technological progress in
hardware and software has been empowering the development of tools and apps. They
promote bigger dynamicity to language acquisition [1] through calls, text messaging,
sending audios, pictures and videos, with a low cost and with freedom of time and
space. The only requirement is to have a good device with good internet connection.
Smartphones have been offering mobility, allowing the contact between people in long
distance, making feasible synchronous and asynchronous dialogs.
Authors such as [2] claim that “the instantaneous communication technology,
mediated through cell phones and smartphones, has changed the way people com-
municate and interact, and the educators cannot ignore the device as possible of being
used in the educational process, in special as integrative between the involved people”.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_60

The use of technology with the purpose of supporting the acquisition of languages is
a theme of several investigations [3–7]. There are studies about the use of computers as
“learning assistants” that use systems known by CALL (Computer Assisted Language
Learning), as described by [8]. Currently, with the mobile technology development
found in smartphones, some apps known by MALL (Mobile Assisted Language
Learning) appeared supporting the mobile acquisition or Mobile Learning (m-learning).
Some researchers use the expression Mobile Pedagogy to put a spotlight to the “vital
role played by teachers” [9], beyond the learner’s mobility, contexts and settings.
Besides the improvement of technology, which enable new opportunities for lan-
guage acquisition, [10] consider that its users have a different proﬁle. According to the
author: “the educational paradigm change imposes a change to the student’s behavior,
in which the teacher does not impose him/her a study routine, but he has to assume this
role and has conscience of his actions and has responsibility for his learning. This is a
factor that contributes to this new method which will work as an open system, and it is
susceptible to variations, but in its interior will bring several advances to the acquisition
of a new language”.
Considering the literature of m-learning area, we concentrated our attention on the
way how different people use the app Tandem, on their own initiative and, out of a
formal educational context, aiming to improve the English learning1.
We present and discuss as a particular user, IP, who used the app Tandem to
interact in English with 18 different interlocutors. These interactions were registered to
identify (i) usability issues which may interfere on the productive and satisfactory use
of the app [11, 12]. The hardware and software limitations of smartphones, when
compared to computers, cannot have inﬂuence in the experience of users in m-learning
context [13]. It means that the screen size, the processing and storage capacity, the
battery’s reduced performance, etc. must not be factors that demotivate the use of apps
to learn. It was also intended to identify the (ii) potential learning situations; factors
that, we suppose, may contribute to the debate about the feasibility and the efﬁciency of
mobile learning [3] in the language acquisition.
2
Materials and Methods
This is an exploratory work, of qualitative character, that was arranged in stages. In the
beginning, there was a literature study about m-learning and the selection of the app
which would be used. Subsequently, there was the exploratory use of the app to
recognize its functions and to deﬁne the way it would be handled by IP. Tandem was
used by IP during three months to interact with other people in order to register and
analyze the data to compose the corpus of the research. We focused on the recognition
of learning situations and usability matters (eventual found problems in the interface
that can interfere on the use of the app and on the user’s satisfaction). Thereafter we
detail each one of these stages.
1 The data that will be presented are restricted only to English language, although on IP’s (abbreviation
used to identify one of the authors of this article who used Tandem) proﬁle description, there is also
Spanish language.
632
F. M. P. Freire et al.

2.1
App’s Selection
We selected, in a ﬁrst moment, smartphones’ apps that have the objective of improving
the language learning (to speak, listen, read and write), through the Apple Store,
considering that the available device for the study was an iPhone 5S. We identiﬁed the
apps Tandem, Periscope and Hellotalk on the category “education”. The three apps
were used by IP, on an exploratory way, with the intention of recognizing and knowing
the functionalities, as well as verifying the working stability. Tandem’s selection is
justiﬁed, among other reasons, for the fact of IP has found, since the beginning, bigger
reciprocity on the interactions with other registered people. Besides that, the platform
offers some functionalities that, supposedly, assist on the language acquisition, as the
statement corrector tool.
2.2
Introduction of the Smartphone App Tandem
This app is destined to people who have interest in learning and practicing other
languages such as, English, Spanish, French, German, etc. It is similar to a community
in which it is needed to apply to participate.
After downloading the app, the user needs to create a proﬁle or to enroll with
his/her Facebook account; after that, it is needed to answer three questions: “What kind
of subjects do you like to talk about?”, “What kind of people would be your perfect
partner on Tandem?” and “What are the learning goals do you want to achieve with
Tandem?”. After that, it is needed to choose the privacy options, that is, who is able to
see that particular user’s proﬁle.
Afterwards, the user waits about one hour for his acceptance on the community,
informs his/her ﬂuency level on the chosen languages and accepts the service term
which arrange the interaction between the app’s users. Once approved2, the app
automatically ﬁlters the people who share the goals that complement each other. For
example, in case the user wants to learn English and speak Portuguese, the app will
show people who have ﬂuency in English and want to learn Portuguese, although these
languages are not necessarily the native language of these people.
Tandem has many functionalities. When meeting someone it is possible to talk
through text messages, audios or even voice and video calls, besides the possibility of
sending pictures. Chat has a functionality which allows the interlocutor to correct a
receipt text message, that is, in case it is found an orthography or grammatical error, it
can be corrected to assist on the other’s learning. Another possibility is to follow
people, this way, in case the person follows back, a friendship is stablished in which the
visualization of who is online become easier.
There are 5 main tabs inside the app, “Community”, “Teachers”, “Messages”,
“Following” and “Proﬁle”. Inside the tab “Community”, it is presented the people with
whom it is possible to initiate chatting. For each person it is presented a picture, status
(online or ofﬂine), name and age, the interest topic written by this user, the languages
2 The service term have several clauses that explain how the relation between the user and the app
happens, how far the users and app developers’ obligations do, being, therefore, a rule protocol with
rights and duties of both sides.
Communities of Language Learners
633

he/she is ﬂuent and which ones he/she wants to practice. In “Teachers”, it is found
available teachers who give remunerated classes through the app. In the tab “Mes-
sages”, there are the initiated conversations. In “Following”, there are the people who
the user follows, as well as if they are online or not. Finally, in “Proﬁle”, the user sees
his/her proﬁle picture, the practicing languages and personal information. The screen
that the interaction happens allows the use of all the functionalities described.
2.3
Construction of the Research’s Corpus
The app has some ﬁlter settings, the reason why IP appears for some people and sees
certain people and not others. There are two screens of the app that allow setting
parameters to ﬁnd partners. The ﬁrst one, “ﬁlters”, is on the top of the screen “Com-
munity”, right below the name of the app, where it is possible to mark the option of
ﬁnding people who are learning the same language without being, necessarily, ﬂuent.
The intention of those who mark this option is to share and practice what they learned
of the language until the moment with someone who is also learning, therefore not
ﬂuent, being able to happen a learning cooperation between them. In case this option is
not marked, it will be visible only to people who have two languages in common on the
following arrangement: If chosen English and Spanish as language of interest and
Portuguese as ﬂuent language, consequently it will appear only people who are ﬂuent
in at least one of the interested languages and also have marked Portuguese as inter-
ested language for them.
Still on screen “ﬁlters”, there is an option of selecting the countries of where it is
wanted to ﬁnd people. If selected just one country, it will appear only people who live
in it. Right below, it is possible to check a box to appear only people that are on the
following list or that are near geographically. Other screen that allows setting the
ﬁlters’ parameters is on the tab “Proﬁle” in which it is possible to set the age range that
appears on the app.
IP’s proﬁle was set to ﬁnd the largest quantity of people. The ﬁlters were with the
minimum of restrictions; the range age chosen, for example, included people from 17
until 50 years old or more; English and Spanish were chosen as languages of interest
and Portuguese as ﬂuent language. The ﬁlters of country restriction and of ﬁnding only
people near geographically were not used.
Summarizing, we focused on the use of the app “Tandem” in smartphones with the
intention of improving the acquisition of the English language (to speak, listen, read
and write) through interactions with users from other countries. During 3 months, the
app was used by IP to interact with 18 people indicated by the system and that were,
initially, contacted by IP, whose proﬁles are described on Table 1.
Among the eighteen people, ﬁve are men and thirteen are women. Seven of them
are ﬂuent in English, twelve in Spanish, one in Swedish and one in Japanese. Although
just three interlocutors are English native speakers, a huge variety of languages
(Spanish, Japanese, Swedish) and, most of them being ﬂuent in Spanish, almost all of
the interactions happened in English, being rare the cases which there were massages in
Portuguese or Spanish.
It is needed to consider that interlocutors, due to their different levels of English
knowledge, have different conditions to correct the other’s statements. The English
634
F. M. P. Freire et al.

native ones, naturally, are able to identify and correct eventual mistakes in an easier
way and with more conﬁdence.
To expand the possibilities of interaction between the interlocutors we decided to
use constantly the app, without pre-established periods (morning, afternoon, evening)
with the purpose of decreasing possible limitations imposed by the difference of
time-zones and to use the lowest number of available ﬁlters on the app with the
intention of increasing the chances of ﬁnding a larger quantity of users.
The interactions were registered to identify, as said before, (i) occasional usability
[6] issues which may interfere on the productive use of the app, as well as (ii) potential
learning situations.
By the end of each day IP registered his impressions about the occurred interac-
tions, with the support of the dialogs’ history of the app, although it does not register
delivery and receipt time of each message. This history aims to inform how many
interactions happened per day, who was the interlocutor, and quantity and type of
dialog message (text, audio or calls). Also, it was always saved the screens of the app
that IP identiﬁed as a (potential) learning situation for one of the interlocutors on the
dialog, or as a (supposed) usability issue of the app. These registers construct the
corpus of the research and were recurrently read and analyzed with the intention of
organizing the analyzed data.
Table 1. Interlocutors’ proﬁle of IP on Tandem
Destined
abbreviationa Age Gender
Place of residence
Fluent language
i1-Q
17
Masculine Freeport, Bahamas
English
i2-A
19
Feminine
Quioto, Japan
English, Japanese
i3-S
17
Feminine
Santa Cruz de la Sierra, Bolivia
Spanish
i4-V
17
Feminine
Santa Cruz de la Sierra, Bolivia
Spanish
i5-C
18
Masculine Tegucigalpa, Honduras
English, Spanish
i6-N
24
Feminine
Johannesburg, South Africa
English
i7-M
25
Masculine Dublin, Ireland
Spanish
i8-A
31
Masculine Philippines
English
i9-C
26
Masculine Malmö, Sweden
English, Swedish
i10-D
18
Feminine
Monterrey, Mexico
Spanish
i11-P
18
Feminine
Lithuania
English
i12-P
29
Feminine
Alajuela, Costa Rica
Spanish
i13-M
25
Feminine
Lima, Peru
Spanish
i14-L
25
Feminine
California, United States
Spanish
i15-F
17
Feminine
Mexico
Spanish
i16-A
21
Feminine
Mexico City, Mexico
Spanish
i17-A
17
Feminine
Chihuahua, Mexico
Spanish
i18-A
19
Feminine
Monterrey, México
Spanish
aThe people’s identity is referred by the initials “interlocutor – nº – name initial”, for example,
“i1-Q” correspond matches to interlocutor 1 whose name is Qxxxxx.
Communities of Language Learners
635

3
Results and Discussion
Table 2 brings a general sight of the interlocutors, the kind of message (audio, text,
calls) and numbers of established interactions with these interlocutors, as well as the
number of identiﬁed learning situations along these 3 months of app’s use.
It was considered learning situations those in which, at least, one of the inter-
locutors seems to have learned something new, such as words, expressions, grammar
structures, orthography, pronunciation, etc., being through the available tools on the
app or, simply, through the interaction by sending messages. We focused only on the
language learning proper but, it is worth mentioning that, although we have not con-
sidered those situations in which there was some kind of learning about the other’s
culture, they have inﬂuence on the way the language is learned, once the learning
becomes contextualized, helping on the language acquisition.
As it was already said, we limited our study to the English language acquisition, for
this reason, the beginning of the interactions was in English and remain this way during
the next dialogs.
On Table 1, it is observed information about some interlocutors who participated of
interactions with IP. It was noticed, by the users “i14-L”, “i8-A”, “i6-N”, “i13-M”, and
“i11-P”, that there is not a direct proportion between the quantity of text messages,
audio messages and observed learning situations.
Table 2. Overview of the research’s corpus by interlocutor
Destined
abbreviation
Text
messages
Audio
messages
Calls
Pictures
Observed learning
situation
i1-Q
788
131
1
2
9
i7-M
470
19
0
1
6
i4-V
154
17
0
0
0
i12-P
118
1
0
0
0
i16-A
104
7
0
0
0
i10-D
83
29
0
0
0
i15-F
79
5
0
0
0
i17-A
50
5
0
0
0
i18-A
47
7
0
0
0
i11-P
39
9
0
0
3
i14-L
36
7
0
0
2
i8-A
27
2
1
0
1
i2-A
23
9
1
0
0
i5-C
7
0
1
0
0
i6-N
7
0
0
0
1
i13-M
7
11
0
0
1
i9-C
3
0
0
0
0
i3-S
2
0
0
0
0
636
F. M. P. Freire et al.

In relation to the potential learning situations, we observed that they may be cat-
egorized in direct and indirect. The direct are those in which it is possible to observe,
on its materiality of text messages or audio messages, an evidence of grammatical,
orthography or lexicon evidence. The indirect are those in which the speaking, reading
and writing practicing, over time, contributes to the improvement of ﬂuency and more
familiarization with the learning language. For being more visible, we will stick to the
direct situations.
3.1
About Potential Learning Situations
Considering the set of interactions with the 18 interlocutors, we veriﬁed 4 types of
interventions from one of the interlocutors during the interaction that promoted some
kind of learning: (1) use of the grammar corrector tool (Fig. 1) with 6 occurrences;
(2) grammar correction without the grammar corrector tool (Fig. 2) with 5 occurrences;
(3) deﬁnition of unknown word by one of the interlocutors (Fig. 3) with 8 occurrences
and (4) audio tool use to deﬁne unknown word and to correct the pronunciation with 4
occurrences.
Fig. 1. Use of the grammar corrector tool.
Fig. 2. Grammar correction without the grammar corrector tool.
Communities of Language Learners
637

3.2
Usability Observations
The use of the app presented some difﬁculties on its handling, such as:
• Audio message: although the tool works well, either to send or to receive messages,
it is not possible to operate the audio ﬁle to listen to its intervals. When just part of
the audio is not listened/understood, it is necessary to play it fully again.
• Message reception notiﬁcation: a message notiﬁcation appears on the lock screen of
the phone, and after unlocking it, the screen with the message is showed. However,
it is closed right after, appearing the screen with all the other interactions and,
ultimately, coming back to the screen with the just received message.
• Statement corrector: it consists of the opening of a new screen, removing the user of
the interlocution context, what may demotivate its use, besides interfering with the
speed that, generally, characterizes this kind of interaction when it happens
synchronously.
• Call via app: two of the four calls made through the app were interrupted without
the interlocutors’ wish. We suppose that it has happened due to connection errors,
since the users complained about the connection on both occasions.
The users of apps like this expect to obtain a fast answer to their actions and have
low tolerance with interface problems which interfere on its usability and do not satisfy
their needs.
Three of the interface problems presented (receipt of message notiﬁcation, gram-
matical and orthography corrector tool and calls via app) effect on users’ expectation
about the speed of use of the app and this may lead them to look for other app because
nowadays there is a big facility in substituting an app for another that has the same
function and that is executed with more efﬁciency.
4
Conclusions
We attempted to use the app constantly with mobile connection, with the purpose of
dedicating more attention to the dialogs: the faster the messages were answered, the
better was the dynamicity of the dialog, and the mobile connection makes it possible.
Nevertheless, IP’s messages were not always answered or the dialog established.
We suppose that the number of learning situations was bigger than the one regis-
tered during this research, due to the needed time took by the author to identify them, as
Fig. 3. Deﬁnition of unknown word.
638
F. M. P. Freire et al.

consequence of the multiplicity of tasks the initial app use demands: to learn its
handling and functionalities use, to establish a minimal mutual knowledge with his
pairs, among other things.
Although it has been found some difﬁculties on the handling of the app, we can say
that its use helps with the improvement of speaking, listening, reading and writing
skills of the language which is being learned, especially when the interlocutors share
the same goals and act reciprocally, highlighting the orthography of the words on the
studied corpus.
The fact of the users being from several countries, with different cultural experi-
ences, contributes to the contextualization of acquisition that happens informally,
different from the majority of the conventional ways of teaching and learning a
language.
References
1. De Souza, C.F.: Aprendizagem sem distância: tecnologia digital móvel no ensino de língua
inglesa. Texto Livre Linguagem e Tecnologia 8(1), 39–50 (2015)
2. Pereira, P.C., Pereira, R.S., Alvez, J.C.: Ambientes virtuais e mídias de comunicação,
abordando a explosão das mídias na sociedade da informação e seu impacto na
aprendizagem - o uso do WhatsApp como plataforma de m-learning. Revista Mosaico
6(1), 29–41 (2015)
3. Sung, Y., Chang, K., Liu, T.: The effects of integrating mobile devices with teaching and
learning on students’ learning performance: a meta-analysis and research synthesis. Comput.
Educ. 94, 252–275 (2016)
4. Taj, I.H., Sulan, N.B., Sipra, M.A., Ahmad, W.: Impact of mobile assisted language learning
(MALL) on EFL: a meta-analysis. Adv. Lang. Literary Stud. 7(2), 76–83 (2016)
5. Burston, J.: Twenty years of MALL project implementation: a meta-analysis of learning
outcomes. ReCALL 27(1), 4–20 (2015)
6. Gholami, J., Azarmi, G.: An introduction to mobile assisted language learning. Int.
J. Manage. IT Eng. IJMIE 2(8), 1–9 (2012)
7. Burston, J.: Realizing the potential of mobile phone technology for language learning.
IALLT J. Lang. Learn. Technol. 41(2), 56–71 (2011)
8. Leffa, V.J.: Aprendizagem de línguas mediada por computador. In: Leffa, V.J. (Org.)
Pesquisa em Linguística Aplicada - Temas e Métodos, pp. 5–30. EDUCAT, Pelotas (2006)
9. Kukulska-Hulme, A., Norris, L., Donohue, J.: Mobile pedagogy for English language
teaching: a guide for teachers. ELT Research Papers 14.07, British Council (2015)
10. Gonçalves, J.A., Silva, V.: Inglês na Palma da Mão: Possibilidades de Aprendizagem
Através dos Dispositivos Móveis Conectados à Internet. Revista de Estudos Acadêmicos de
Letras, Cáceres, pp. 49–57 (2014)
11. Norman, D.A., Nielsen, J.: Gestural interfaces: a step backward in usability. Interactions
17(5), 46–49 (2010)
12. Nielsen, J.: Usability Engineering. Morgan Kaufmann, EUA, San Francisco (1993)
13. Silva, F., Silva, J.K., Lucena, M., Gomes, A.: Requisitos para Integração entre Ambientes de
Aprendizado e m-Learning: Uma Revisão Sistemática da Literatura. In: Anais do XXVI
Simpósio Brasileiro de Informática na Educação (sbie 2015), pp. 269–278. SBC, Porto
Alegre (2015)
Communities of Language Learners
639

Moving Beyond Limitations: Evaluating
the Quality of Android Apps in Spanish
for People with Disability
Andrés Larco1(&), Cesar Yanez1, Carlos Montenegro1,
and Sergio Luján-Mora2
1 Departamento de Informática y Ciencias de la Computación,
Escuela Politécnica Nacional,
Ladrón de Guevara, E11-253, 17-01-2759, Quito, Ecuador
{andres.larco,carlos.montenegro}@epn.edu.ec,
cesaryanezv@outlook.com
2 Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante,
Carretera San Vicente del Raspeig, 03690 Alicante, Spain
sergio.lujan@ua.es
Abstract. Autism, Down syndrome and cerebral palsy, constitute the most
common disabilities from a medical point of view. In others individual ﬁelds,
the Assistive Technology use constitute a current option to improve the Quality
of Life. In this case, the free available specialized software covers several
competences of life, like language and communication, autonomy, sensorimo-
tor, social skills, mathematics, knowledge of the natural and social environment,
and digital competences. The objective of this research was to evaluate the
quality of Android apps using Mobile Application Rating Scale. A systematic
search
using
Preferred
Reporting
Items
for
Systematic
Reviews
and
Meta-Analyses was conducted using Google Play store as a base in August
2017. Evaluated apps were classiﬁed according to their respective competence
of life. The results showed that the evaluated apps needed improvements in
customization and interactivity. Also, an apps list based on Mobile Application
Rating Scale scores, useful for therapist, parents, and people with disability has
been established.
Keywords: Disability  Android apps  Apps assessment  Autism
Down syndrome  Cerebral palsy
1
Introduction
According to Lancioni and Singh [1], assistive technology are the commercial or cus-
tomized devices that help people to reduce the impact of their disability on daily
functioning and to achieve a better Quality of Life (QoL). There is an international
consensus that QoL has eight core domains. It includes emotional well-being, inter-
personal relationships, material well-being, personal development, physical well-being,
self-determination, social inclusion, and rights.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_61

By comparing persons with and without disabilities, the study of Kadijevich et al.
examined the extent to which individuals used ICT. The study found there is a high
positive correlation between the use of ICT and the quality of life improvement for
people with disabilities [2].
On the other hand, autism [3], Down syndrome [4], and cerebral palsy [5] constitute
the more common disabilities on the medical scope, and they were more studied from
assistive technology therapy use [6], including the specialized software. Wikinclusion
[7] is a web knowledge base that contains software for people with autism, Down
syndrome, cerebral palsy, deaf-blindness, and multiple sclerosis. The software was
classiﬁed as the digital ramps, augmented communication, author language, rein-
forcement software and open software. Software inside Wikinclusion can support the
following competences of life: (1) autonomy, sensorimotor and social skills; (2) lan-
guage and communication; (3) mathematics; (4) knowledge of the social and natural
environment; (5) digital competence; (6) artistic knowledge, and (7) transition to the
labor market. The software was assessed by competences of life, but it has not been
assessed according results provide an apps list with their respective MARS scores.
Android is to software quality perspective.
According to Holzinger et al. [8], metrics-based benchmarks are crucial for mea-
suring usability, particularly for special end user groups. Likewise, Ahamed et al. [9]
consider that to satisfy the end user, and vendor satisfaction, the software measurement,
and quality assurance are essential components of software-based medical systems.
There is a need to consider whether a speciﬁc quality rating scale may be needed for
apps. Attempts to develop mobile health (mHealth) evaluation criteria are often too
general, complex, or speciﬁc to a particular health domain. The only speciﬁc quality
rate tool for mobile health apps providing a multidimensional measure is Mobile App
Rating Scale (MARS) [10].
The objective of this research was to evaluate the quality of Android apps for
people with autism, Down syndrome and cerebral palsy using MARS. The reliability of
MARS scale in Android was analyzed [10], and the considered because is the most
used operating system in the world [11]. Also, the research considered apps in Spanish
because there are not apps assessment with MARS for Spanish language apps.
This document is organized as follows. Section 2 describes how Information and
Communication Technology (ICT) and mobile devices can be beneﬁcial to People with
Disability (PWD), how software quality is rated and how Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) and MARS were used on the
health scope. Section 3 describes the systematic search performed with PRISMA, how
MARS can be used to rate app quality and how data was analyzed. Section 4 contains
the results found in this research. Section 5 discusses the results, and ﬁnally, Sect. 6
contains conclusions and future work.
Moving Beyond Limitations
641

2
Background and Related Work
2.1
Mobile Devices
Mobile internet usage allows consumers access and shares information on the go.
Mobile internet data trafﬁc shows increasing projections for the future between 2016
and 2021. With the large volume of apps available, marketers started to focus on
mobile app retention instead of acquisition [12].
Google Play Store is widely used around the world, contains 2.8 million available
apps up to March 2017. Based on the fact that the majority number of apps of Google
Play Store are free, the trend is developing free apps to adapt to the current business
models in order to have revenues [13].
2.2
ICT Beneﬁts for People with Disability and Software Quality
ICT can be beneﬁcial to people with disabilities, therefore, accumulating evidence has
suggested that the role of student-directed learning and self-determination promotes
positive academic, social, and adult outcomes for students with intellectual and
developmental disabilities [14].
A mobile app with resources to help to improve communication for PWD would be
a valuable tool for families, caregivers, therapists, teachers and healthcare professionals
[15]. PWD will be able to express their needs, improve social dexterity, share infor-
mation and get involved in social routines at home, school and work, and grow their
interaction in community groups [16].
In the context of mobile apps and health scope, several reviews and evaluations
have been made using PRISMA. PRISMA is an evidence-based minimum set of items
for reporting in systematic review and meta-analyses. PRISMA focuses on the
reporting of reviews evaluating randomized trials, but can also be used as a basis for
reporting systematic review in other types of research [17]. Akter was able to identify
prominent quality characteristics for health services using mobile platforms [18]. Santo
et al. [19] made a systematical review of the medication reminder apps available in the
Australian Apple App Store and Google Play to assess their features to identify
high-quality apps.
The only speciﬁc rate tool for mobile health apps is MARS. MARS was used to
evaluate the quality of mobile apps because it is an objective (intraclass correlation
coefﬁcient [ICC] between raters = .79) and highly reliable scale (alpha = .90) [10]. It
contains four subscales. Engagement, which refers to fun, interesting, customizable,
interactive and well-targeted to the audience. Functionality, which refers to function-
ing, easy to learn, navigation, ﬂow logic, and gestural design of the app. Aesthetics,
which refers to the graphic design, overall visual appeal, color scheme, and stylistic
consistency. The last subscale, information, refer to high-quality information from a
credible source.
642
A. Larco et al.

3
Methodology
3.1
Systematic Search Criteria and Selection
A systematic search using PRISMA was conducted using Google Play store as a base
in August 2017. The search terms included autism, Down syndrome, cerebral palsy,
Education Top Free, Educational Games Top Free, Family – Education Top Free, and
Puzzle Games Top Free.
PRISMA is a publication guide to improve clarity and transparency in reporting
systematic reviews and consists of a four-phase ﬂowchart [17]. The ﬁrst phase is
identiﬁcation, the second is screening, the third one is eligibility, and the last one is
included. On identiﬁcation phase, apps by title, non-Spanish language, duplicated, and
paid apps were excluded. On screening phase, apps were removed due to irrelevant
content for autism, Down syndrome, and cerebral palsy. Applying assessment for
eligibility excluded apps based on not enough information, no longer available and no
longer working. On included phase, the remaining apps were downloaded and evalu-
ated with MARS.
3.2
Rating Tool
MARS was used to rate mobile apps, and it contains 19 items grouped by four sub-
scales: engagement (5 items), functionality (4 items), aesthetics (3 items), and infor-
mation (7 items). The average of the four subscales determines the app quality score.
The exclusion of the subjective quality subscale from the overall mean app quality
score supports the objectivity of MARS as a measure of app quality [10]. MARS items
use a Likert scale (1-Inadequate, 2-Poor, 3-Acceptable, 4-Good, 5-Excellent) [19, 20].
A template was created for data extraction following MARS scales. A team of 17
testers performed the evaluation; each tester was assigned a minimum of two appli-
cations to evaluate. Training sessions for testers were performed about the process of
how to evaluate apps using the template. Testers evaluated every app according to
MARS items and classiﬁed it according to their respective competence of life. Included
apps were evaluated in the following devices: Motorola, Sony, Huawei, LG, Samsung,
and Asus.
3.3
Data Analysis
ICC determined the interrater reliability of MARS subscales and total score. In this
research, we used the two-way mixed-effects model because the result only represents
the reliability of the speciﬁc raters involved in the reliability experiment [21]. The
Conﬁdence Interval (CI) is a type of interval estimate that was computed from the
observed data. The conﬁdence level is the frequency of possible conﬁdence intervals
that contain the true value of their corresponding parameter. The most common con-
ﬁdence level is 95% [22].
The Corrected item-total correlation is how an item is correlated with a subscale,
correlations of less than 0.35 or 0.25 are often discarded. Pearson correlation coefﬁcient
Moving Beyond Limitations
643

is a measure between sets of data and how well they are related [23]. MARS scores
were analyzed using IBM SPSS Statistics.
4
Results
A total of 3,206 android mobile apps from Google Play Store were identiﬁed. For the
ﬁnal evaluation, 73 mobile apps were obtained. Figure 1 shows the results of the
systematic review of Android apps.
Table 1 contains the percentage of evaluated apps by disability, the number of
downloads of each application and if there were updated in the last six months.
Fig. 1. A systematic review of Android apps.
Table 1. Percentage, downloads and latest update of the evaluated apps by disability.
Disability
Percentage of
evaluated apps
More than 10,000
downloads
Updated in the last six
months
Autism
47%
26.47%
55.88%
Down
syndrome
12%
22.22%
33.33%
Cerebral palsy
10%
N/A
14.28%
Multiple
disabilities
15%
36.36%
18.18%
General
16%
41.67%
16.67%
644
A. Larco et al.

Multiple disabilities concept refers to the app description, which includes at least
two of these disabilities: autism; Down syndrome, and cerebral palsy. Table 2 contains
MARS subscales values and MARS total score for each application. Total value was
calculated based on engagement, functionality, aesthetics, and information. Subse-
quently, the apps were grouped by disability and their respective competence of life.
The main competence in evaluated apps for people with autism was language and
communication with 50%. For Down syndrome apps was mathematics with 44.44%.
For cerebral palsy, was language and communication with 71.43%. Finally, for mul-
tiple disabilities apps, was language and communication with 72.73%.
Table 2. Mean score of applications.
Name
Enga-
gement
Func-
tionality
Aes-
thetics
Infor-
mation
Total 
Mean
Google
rating
Autism
Language and communication
Soyvisual
5.00
4.75
3.33
4.86
4.49
4.80
Puzzingo
3.80
4.50
5.00
4.29
4.40
4.40
Preescolar Juegos en Español
4.20
4.75
4.33
4.14
4.36
4.60
Día a Día
4.00
4.25
4.67
4.14
4.26
4.60
Autism Therapy with MITA
4.40
4.25
4.00
4.00
4.16
4.30
Picto One: Autismo
3.40
4.25
4.67
4.17
4.12
4.30
Autism
DictaPicto
3.60
4.00
4.33
4.29
4.05
4.80
INTIC Móvil
4.00
4.50
3.33
4.29
4.03
5.00
AZAHAR
3.00
4.25
3.67
4.43
3.84
4.20
HablaFácil Autismo DiegoDice
3.60
4.50
3.00
4.00
3.78
3.70
e-Mintza
4.20
3.75
3.33
3.29
3.64
3.20
ABC Autismo
3.60
3.50
4.00
3.29
3.60
4.60
Pictea · Habla con Pictogramas
2.80
4.50
4.00
3.00
3.58
4.50
Linking Igualación
2.80
4.25
3.67
2.71
3.36
4.80
Terapia de Lenguaje Autismo
3.40
3.50
2.00
4.43
3.33
450
SCAI Autismo
3.00
4.50
3.33
2.00
3.21
4.10
Proyect@ PECS
3.00
4.00
3.33
2.14
3.12
3.60
Autonomy, sensorimotor, social skills
Proyect@ Emociones 2 - Autismo
4.60
4.75
4.00
4.86
4.55
4.70
iSECUENCIAS LITE
5.00
5.00
4.00
4.00
4.50
4.20
Sígueme Móvil
5.00
4.75
3.33
4.86
4.49
3.00
Autismo Imagen Discusión
4.00
4.75
4.33
4.43
4.38
3.40
Autimo - Descubra emociones
4.00
4.50
4.67
4.14
4.33
3.60
El viaje de elisa Móvil
3.80
4.50
4.67
4.14
4.28
4.90
Niño juego de memoria ALIMENTO
4.00
4.75
4.33
3.86
4.24
3.90
Alex aprende a vestirse solo
3.60
4.50
4.33
3.57
4.00
4.00
Proyect@Habilidades
3.80
4.25
4.00
3.86
3.98
5.00
ComunicaTEA HUS / SURESTEA
3.80
4.00
3.67
4.29
3.94
4.20
PictogramAgenda
3.80
3.50
4.33
4.00
3.91
4.30
ValpoDijo - Autismo
4.00
4.25
3.67
3.57
3.87
5.00
Aprendizaje sensorial de niños
3.80
4.00
4.67
2.43
3.72
4.40
Pictogramas autismo
3.00
2.75
2.33
2.43
2.63
3.20
Gaido Autismo - Agenda Visual.
2.80
2.25
2.00
2.43
2.37
4.40
Mathematics
Desafíos mas y menos
2.80
4.75
3.67
3.71
3.73
5.00
ParejasdeMascotas
2.80
4.50
4.00
3.00
3.58
4.00
Moving Beyond Limitations
645

Table 3 contains the Corrected Item-total correlation, mean and the standard devi-
ation of the 19 items of MARS subscales for the 73 evaluated apps. Each subscale has its
own ICC, used to demonstrate the acceptable level of reliability among evaluators. If the
value is closer to one, MARS results are more reliable. Item 19 “Evidence base” was
excluded from all calculations, as it currently contains no measurable data [10].
Down syndrome
Language and communication
AprendizajexConceptos Simples
2.60
3.75
2.67
3.00
3.00
0.00
Autonomy, sensorimotor, social skills
Siluetas OA
3.60
5.00
4.00
4.57
4.29
4.00
Mathematics
El juego de los opuestos
3.60
4.75
5.00
4.57
4.48
4.00
Opuestolandia
2.40
4.50
4.00
3.43
3.58
4.00
Jugamos Todos
2.40
3.50
3.33
4.00
3.31
3.00
Down Números
2.40
3.25
3.00
2.71
2.84
5.00
Knowledge of the natural and social environment
Grupolandia
2.60
5.00
4.33
4.43
4.09
4.20
AxCS Bloque 1 4to Ciencias Naturales
2.60
2.25
1.67
2.29
2.20
5.00
AxCS Bloque 1 4to Formación Cívica y Ética
2.40
2.25
1.67
2.14
2.11
5.00
Language and communication
Cerebral palsy
Press & Say LITE: CAA, AAC
4.40
4.75
5.00
4.71
4.72
5.00
Plaphoons
3.50
4.50
4.33
4.00
4.41
4.00
Vox4all 2.0 Free
4.40
4.25
4.33
3.86
4.21
3.90
La calle de Renata
3.40
5.00
4.33
3.29
4.01
5.00
Learny PCI
3.80
4.50
3.67
3.57
3.89
4.70
Autonomy, sensorimotor, social skills
Burbujo
4.40
5.00
5.00
4.43
4.71
4.00
Bocciapp
3.00
3.75
3.00
2.71
3.12
4.70
Language and communication
Multiple Disabilities
OTTAA Project
4.60
5.00
4.67
4.43
4.67
4.80
Pictosonidos
3.40
4.50
4.00
4.29
4.09
4.50
LetMeTalk: Talker SAAC,CAA,SAC
4.20
3.50
3.67
3.86
3.81
4.40
COMUNIQUEMONOS
2.80
3.25
3.33
3.86
3.75
3.90
SAAC. Comunicación. Autismo
3.80
3.75
4.00
2.86
3.60
3.80
Conversador
3.20
3.50
3.33
3.29
3.33
3.50
Touch-Emotions
3.00
4.00
3.33
2.29
3.15
3.30
Picto Cuento
2.60
3.25
3.00
1.86
3.00
4.00
Autonomy, sensorimotor, social skills
SuperApp
3.60
4.50
4.67
4.29
4.27
5.00
Mathematics
Juegos de puzzle: Acertijos
2.80
3.75
3.67
2.14
3.09
4.30
PiktoPop: Explotar globos.
4.00
2.25
2.00
2.86
2.78
4.20
Language and communication
Tarjetas educativas en español
3.40
5.00
4.67
3.71
4.20
4.00
Jugando con las vocales
3.20
4.50
4.67
3.57
3.99
4.00
Sonigrama
3.00
4.75
3.67
4.00
3.86
4.30
JoseAprende Móvil
2.80
3.50
2.67
2.71
2.92
4.70
General
Aprender Jugando - Preescolar
3.00
3.50
2.33
2.71
2.89
4.10
Puzles para bebés
4.00
4.75
5.00
4.43
4.54
4.40
Mathematics
Memo
2.40
3.75
2.33
2.71
3.25
4.80
Números en acción
2.80
4.25
3.67
2.14
3.22
2.20
Knowledge of the natural and social environment
Sonidos de Animales
4.40
4.75
5.00
4.43
4.64
4.40
Dibugrama
2.80
4.25
4.67
3.71
3.86
3.90
TuliEmociones
2.60
4.25
3.67
3.00
3.38
3.60
Digital competences
Repe
2.80
5.00
3.33
2.43
3.39
3.70
Table 2. (continued)
646
A. Larco et al.

5
Discussion
The initial sample of 3,206 apps was reduced through PRISMA phases up to 73 apps,
that shows an important absence of apps for PWD in Spanish. PWD face many barriers
to taking advantage of the online world; Internet use offers many ways to participate in
society actively and to create alternatives to combat against exclusion in the world [24].
Regardless the disability, the main competence for the evaluated apps was language
and communication (49.32%) followed by autonomy, sensorimotor and social skills
(27.40%) since is essential for PWD in their daily activities. The lack of development
for other competences is evidenced with digital competence, which has only one
app. Autism had more quantity of apps (46.58%), however, for Down syndrome, fewer
apps were found (9.59%). Also, the research found apps focused on at least two
disabilities.
The best-rated subscale was functionality with a mean value of 4.15; the reason is
on gestural design (4.19), performance (4.16), ease of use (4.14) and navigation (4.12)
of the evaluated apps. The subscale with the lowest mean value was engagement with
3.48 due to the lack of customization (2.73) and interactivity (2.53). The MARS total
mean score of subscales had a good reliability (ICC = 0.78), which mean the work of
Table 3. Statistics of MARS items.
#
Subscale/Item
Corrected item-total correlation Mean SD
Engagement ICC = 0.74 (95% CI 0.58–0.80)
1
Entertainment
0.49
3.99
0.83
2
Interest
0.66
4.01
0.79
3
Customization
0.56
2.73
1.57
4
Interactivity
0.46
2.53
1.20
5
Target group
0.31
4.16
0.62
Functionality ICC = 0.81 (95% CI 0.73–0.87)
6
Performance
0.59
4.16
0.88
7
Ease of use
0.63
4.14
0.87
8
Navigation
0.64
4.12
0.93
9
Gestural design
0.64
4.19
0.83
Aesthetics alpha ICC = 0.83 (95% CI 0.75–0.89)
10 Layout
0.59
3.93
0.98
11 Graphics
0.72
3.66
1.03
12 Visual appeal
0.76
3.70
0.95
Information alpha ICC = 0.74 (95% CI 0.64–0.83)
13 Accuracy of app description 0.29
4.25
0.97
14 Goals
0.45
2.93
1.70
15 Quality of information
0.64
3.45
1.52
16 Quantity of information
0.71
3.11
1.55
17 Visual information
0.51
3.88
1.09
18 Credibility
0.35
3.60
1.08
Moving Beyond Limitations
647

testers was objective according to MARS items. Also, according to Corrected item-total
correlation, the accuracy of app description is the less related item with information
subscale. Similarly, the target group is the less related item with engagement subscale.
On the other hand, visual appeal item is deeper related with aesthetics subscale.
Less than a half of Android apps (30/73) had a MARS score higher than 4, which
means 41% had an acceptable quality. Based on these results, these apps can be used
by therapists, parents, and PWD. Google Play Store ratings were small correlated with
the MARS total score (Pearson correlation coefﬁcient = 0.27).
6
Conclusions and Future Work
App ratings and user reviews are subjective. Thus, Google Play Store rating is an
unreliable indicator of app quality. It should not be considered in selecting an app
because it is not focused on PWD. Nevertheless, MARS results of this research can
help therapists and parents to select apps for PWD avoiding confusion and independent
search for apps due to the non-existence of store categorizations by disability type.
There is a general absence of speciﬁc, measurable and achievable goals in the
evaluated apps alongside with customization and interactivity. These parameters are
important for improving the Quality of Life of People with Disability.
In the future, Android app developers can use the results of this research to improve
mobile app quality for PWD working closely with therapists and parents. Additionally,
future developments can merge Internet of Things (IoT) devices with apps for PWD to
improve the Quality of Life of PWD.
Future research can include apps not aimed for PWD but focused on competences
of life.
References
1. Lancioni, G.E., Singh, N.N.: Assistive technologies for improving quality of life. In:
Assistive Technologies for People with Diverse Abilities, pp. 1–20. Springer, New York
(2014)
2. Kadijevich, D., Odovic, G., Maslikovic, D.: Using ICT and quality of life: comparing
persons with and without disability (2016)
3. World Health Organization: Autism spectrum disorders. http://www.who.int/mediacentre/
factsheets/autism-spectrum-disorders/en/
4. World Health Organization: Genes and human disease. http://www.who.int/genomics/
public/geneticdiseases/en/index1.html
5. Centers for Disease Control and Prevention: Data and Statistics | Cerebral Palsy. https://
www.cdc.gov/ncbddd/cp/data.html
6. Lancioni, G.E., Sigafoos, J., O’Reilly, M.F., Singh, N.N.: Assistive Technology: Interven-
tions for Individuals with Severe/Profound and Multiple Disabilities. Springer Science &
Business Media, New York (2012)
7. Wikinclusion. http://wikinclusion.org/index.php/Página_principal
648
A. Larco et al.

8. Holzinger, A., Searle, G., Kleinberger, T., Seffah, A., Javahery, H.: Investigating usability
metrics for the design and development of applications for the elderly. In: International
Conference on Computers for Handicapped Persons, pp. 98–105. Springer (2008)
9. Ahamed, N.U., Sundaraj, K., Ahmad, R.B., Rahman, M., Ali, A.: A framework for the
development of measurement and quality assurance in software-based medical rehabilitation
systems. Procedia Eng. 41, 53–60 (2012). https://doi.org/10.1016/j.proeng.2012.07.142
10. Stoyanov, S.R., Hides, L., Kavanagh, D.J., Zelenko, O., Tjondronegoro, D., Mani, M.:
Mobile app rating scale: a new tool for assessing the quality of health mobile apps. JMIR
MHealth UHealth 3, e27 (2015). https://doi.org/10.2196/mhealth.3422
11. Desktop operating system market share Worldwide. http://gs.statcounter.com/os-market-
share/desktop/worldwide
12. Statista: Mobile Internet - Statistics & Facts. https://www.statista.com/topics/779/mobile-
internet/
13. Statista: Google Play Store: number of apps 2009–2017. https://www.statista.com/statistics/
266210/number-of-available-applications-in-the-google-play-store/
14. Agran, M., Brown, F., Hughes, C., Quirk, C., Ryndak, D.: Equity and Full Participation for
Individuals with Severe Disabilities. Paul H. Brookes, Baltimore (2014)
15. Chicano, F., Luque, G.: A mobile application and academic portal to support professionals
working with people having severe intellectual or developmental disabilities. Procedia Soc.
Behav. Sci. 237, 568–575 (2017). https://doi.org/10.1016/j.sbspro.2017.02.108
16. Pinazo, E.P., Reina, M.C.: A model to enhance interaction for people with severe intellectual
disability in healthcare, education and interpreting. Procedia Soc. Behav. Sci. 237, 1189–
1195 (2017). https://doi.org/10.1016/j.sbspro.2017.02.188
17. Liberati, A., Altman, D.G., Tetzlaff, J., Mulrow, C., Gotzsche, P.C., Ioannidis, J.P.A.,
Clarke, M., Devereaux, P.J., Kleijnen, J., Moher, D.: The PRISMA statement for reporting
systematic reviews and meta-analyses of studies that evaluate healthcare interventions:
explanation and elaboration. BMJ 339, b2700 (2009). https://doi.org/10.1136/bmj.b2700
18. Akter, S., D’Ambra, J., Ray, P.: Service quality of mHealth platforms: development and
validation of a hierarchical model using PLS. Electron. Mark. 20, 209–227 (2010)
19. Santo, K., Richtering, S.S., Chalmers, J., Thiagalingam, A., Chow, C.K., Redfern, J.: Mobile
phone apps to improve medication adherence: a systematic stepwise process to identify
high-quality apps. JMIR MHealth UHealth 4, e132 (2016)
20. Mani, M., Kavanagh, D.J., Hides, L., Stoyanov, S.R.: Review and evaluation of
mindfulness-based iPhone apps. JMIR MHealth UHealth 3, e82 (2015). https://doi.org/10.
2196/mhealth.4328
21. Koo, T.K., Li, M.Y.: A guideline of selecting and reporting intraclass correlation coefﬁcients
for reliability research. J. Chiropr. Med. 15, 155–163 (2016). https://doi.org/10.1016/j.jcm.
2016.02.012
22. Gupta, S.K.: The relevance of conﬁdence interval and P-value in inferential statistics.
Indian J. Pharmacol. 44, 143–144 (2012). https://doi.org/10.4103/0253-7613.91895
23. Andale: pearson correlation: deﬁnition and easy steps for use. http://www.statisticshowto.
com/what-is-the-pearson-correlation-coefﬁcient/
24. Dobransky, K., Hargittai, E.: Unrealized potential: exploring the digital disability divide.
Poetics 58, 18–28 (2016). https://doi.org/10.1016/j.poetic.2016.08.003
Moving Beyond Limitations
649

Software Systems, Architectures,
Applications and Tools

Analysis and Implementation of ETL System
for Unmanned Aerial Vehicles (UAV)
Wilson Medina-Pazmiño1(&), Aníbal Jara-Olmedo1(&),
Cristian Tasiguano-Pozo2(&), and José M. Lavín3(&)
1 CIDFAE, Centro de Investigación y Desarrollo de la Fuerza Aérea
Ecuatoriana, Ambato, Ecuador
wilsonmedina81@hotmail.com, ljara@fae.mil.ec
2 Escuela Politécnica Nacional, Quito, Ecuador
cristian.tasiguano@epn.edu.ec
3 Decisions and Innovation Group, Universidad Técnica de Ambato,
Ambato, Ecuador
josemaria.lavin@uta.edu.ec
Abstract. Unmanned Air Vehicles are technological tools that in recent years
have raised interest in researchers and developers for their multiple civil and
military applications. UAVs carry on board a large number of sensors and
electronic equipment to ensure optimal operation. However, several times these
devices are from different suppliers or developers making difﬁcult to integrate
them in the aircraft. To solve this integration problem, an application based on
ETL (Extraction, Transformation and Load) systems is presented. ETL system
extracts, processes and shares information from propulsion, communication,
transponder, electro-optical and energy systems. ETL systems integrate signals
with different communication protocols, providing efﬁciency and high speed in
data processing. The ETL system is developed in embedded platform with
multi-task execution capabilities and real-time information processing. ETL
system integrates equipment with different communication protocols in order to
facilitate operation, control, monitor and information record tasks of UAV.
Keywords: ETL system  UAV  Multitask  Information management
Real time  RS232  Communication protocol
1
Introduction
Unmanned Aerial Vehicles (UAVs) are systems applied in research and development
projects in civilian and military ﬁelds [1]. One of the most developed ﬁeld of UAV
applications is detection, observation and surveillance by image acquisition [2] applied
to environment, agriculture [3] and erosion monitoring [4].
The aircraft carries on board devices, equipment and sensors that perform speciﬁc
tasks to guarantee the ﬂight safety of the UAV. UAV navigation equipment executes
control algorithms for the autonomous ﬂight of the aerial vehicle [5]. Data Link sys-
tems communicates UAV with Ground Control Station [6]. Data Link equipment
transmits real-time information from electro-optical sensors [7], instrumentation
equipment [8], identiﬁcation components (transponder) and propulsion equipment.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_62

UAV on board devices need to share information even though every one of them
uses a different communications protocol [10]. In fact, electro-optical sensor needs GPS
data and control equipment requires communication with the propulsion system. Each
equipment, device or sensor on board the UAV uses its own communications protocol
[10]. These protocols are the rules of (i) semantics, (ii) synchronization and (iii) syntax
used by electronic equipment to transmit and receive information about the perfor-
mance of its components or their physical variables [18].
Timely, accurate and fast availability of information plays an important role in
monitoring and control systems of electronic devices [20]. ETL system is based on
FPGA architecture with the purpose of guaranteeing the safe delivery of the infor-
mation [25] between UAV on board components.
Implementation of the Extraction, Transformation and Load (ETL) system [11] will
allow to share the information generated by each equipment on board the UAV,
regardless of the protocol used for communications, as presented in Fig. 1.
The scheme in Fig. 1 shows the feasibility of sharing information between com-
puters using different communications protocols. ETL system centralizes the infor-
mation generated by different devices on board the UAV. After that, the information
acquired is processed and shared with all the devices.
This paper is structured as follows: Sect. 2 reviews on board ETL systems that
acquire process and share information of aircraft; Sect. 3 describes ETL system
operation mode and components; Sect. 4 tests the functionality of the ETL system; and
ﬁnally, Sect. 5 concludes this work presenting some future work that can be done.
2
UAV ETL Systems
Revision of literature starts with embedded systems [13] developed to be installed on
unmanned aircraft. In [9] the development of a prototype of information management
system and storage of data and control processes of an aircraft is presented. For
commercial off-the-shelf (COTS) devices, VPS Systems describes the stages, design
and manufacturing standards, modularity of equipment and how to integrate them into
Fig. 1. ETL system scheme.
654
W. Medina-Pazmiño et al.

an aircraft. Teams mentioned in [9, 12] concentrate their efforts in performing com-
munication tasks between on board devices using different communications protocols.
This information sharing expedites UAV tasks as the release of payload or the man-
agement of emergency parameters.
Searcher II and Heron [14] are examples of multiple application unmanned air
vehicles with data management systems (on board computers) to facilitate communi-
cations between different equipment. The on-board computers allow the aircraft several
tasks to be fulﬁlled, such as radar detection, communications relay [15] and surveil-
lance or reconnaissance with electro-optical sensors.
Examples of information exchange between different embedded systems are pre-
sented in [16]. For this case, object recognition tasks are also developed with the use of
an aerial platform. Here, the navigation system receives information from the image
recognition sensor in order to be able to make decisions of permanence or change of
aircraft coordinates. Some researches consider the most relevant aspects in this type of
systems are the capacity of the main processing card to execute control algorithms [17]
and the energy source required by the system [19]. Both aspects are related because the
power source is important due to UAV use of 5, 12, 24 DC volts, and the card must
take power from any of these voltage levels.
2.1
ETL System Architecture
The UAV components communicate themselves through differentiated protocols [10],
which already have their communication syntax deﬁned in the master slave architecture
or point-point communication with the standard RS-232 [21]. ETL system transfers
information from one component to another. Navigation and control system delivers a
time signal between 1 to 2 miliseconds (ms). ETL reads time value and converts it to an
acceleration value. 1 ms corresponds to 0% acceleration and 2 ms corresponds to 100%
acceleration. This value is coded as hexadecimal and transmitted to propulsion system to
set up the acceleration value. Transponder, GPS, electro-optical sensor and datalink
communicate by RS-232. ELT sends a request for information from each device. ETL
analyzes information received from devices and respond with parameters for each one.
The components to be coordinated aboard the UAV are presented in Fig. 2.
UAV equipment transmits complete eight bits information words [22]. The format
of the information transmitted is one start bit, eight data bits and one stop bit. The
transmission speed can be set up between 1200 bps and 115200 bps as deﬁned by the
RS-232 standard [21].
2.2
ETL System Information Management
The estimation of the amount of information transmitted per time unit is deﬁned adding
the number of bits required. The numbers of bits required are eight (8) bits per word or
byte [22], plus one (1) start bit and one more bit to stop (1). Then, it is necessary 10 bits
to transmit a word. For calculation of the amount of information transmitted per time
unit, Eq. 1 will be employed.
Analysis and Implementation of ETL System
655

nB ¼ bps
nbw  nd
ð1Þ
where:
nB ¼
Number of bits transmitted per time unit.
bps ¼
Bits per second.
nbw ¼
Number of total bits per word (10 bits).
nd ¼
Number of information bits per word (8 bits)
With a serial speed of 38400 bps Eq. (1) determines:
nB ¼ bps
nbw  nd
nB ¼ 38400½bps
10½b
 8½b]
nB ¼ 30720 [bps]
The number of bits transmitted is 30720 bits per second. By analyzing the amount
of information per time unit, it is possible to determine the time required to transmit
information between the systems on board the UAV. The bit rate is directly propor-
tional to the amount of information transmissible (at higher transmission speed more
information can be transmitted). Equation 2 is used to estimate the time required for
information transmission between two devices considering 100 bytes per each request.
t ¼ 1½sec  nbits
nB
ð2Þ
Fig. 2. UAV components block diagram for ETL system
656
W. Medina-Pazmiño et al.

where:
t ¼
Time required for information transmission.
1½sec ¼
Time unit.
nbits ¼
Number of bits to transmit.
nB ¼
Number of bits transmitted per time unit.
Each byte has 8 bits and it must be added the start bit and the stop bit. Then, one
byte requires 10 bits transmission data frame. Hence, in order to transmit 100 bytes
1000 bits must be transmitted. The estimated time to transmit the 1000 bits is deter-
mined by replacing this value in Eq. 2 and it is approximately 33 ms.
t ¼ 1½sec  1000½bits
307020½bits
t ¼ 0:033 ½sec
Additionally, physical signals generated by energy equipment must be monitored.
In order to be visualized by the UAV operator, voltage and current signals from the
energy components are acquired for the signal conditioning process [21] that includes
the signal conditioning stage and the processing stage.
Navigation and control equipment regulates the acceleration rate of the UAV
engine. The acceleration percentage of the motor is controlled by a pulse-width
modulation (PWM) signal generated with a frequency of 50 Hz and a variation in the
duty cycle Fig. 3.
Acceleration percentage varies from 0% to 100% depending on the pulse width that
oscillates between 1 and 2 ms. Minimum value of 1 ms corresponds to 0% and the
maximum value of 2 ms corresponds to 100%. The navigation and control system
delivers the analogue signal (PWM) to the ETL system. ETL system transforms the
analog signal into a digital signal by scaling the corresponding signal from 1 ms to
0x0000 and from 2 ms to 0xFFFF. Finally, this digital signal is transmitted in a serial
communication to the propulsion system that controls the UAV engine.
Fig. 3. Signal diagram of the navigation and control system for controlling the acceleration of
the propulsion system
Analysis and Implementation of ETL System
657

3
ETL System Operation Mode
The hardware-in-the-loop tool [23] of the task-programming interface was utilized for
the development and validation of the ETL system [24]. The embedded platform, in
which the ETL was developed, is made up of an FPGA technology architecture that
allows estimation of low probability of collisions and loss of information.
3.1
Allocation of Resources to Processes
Resources required by each process are allocated based on the tasks to be performed,
optimizing available resources and prioritizing the processing and operation of the ETL
system. As a starting point, the ETL establishes communication independently with the
UAV equipment. Then, physical memory locations are allocated for information
storage, decision making and information sharing required by the equipment on board
the UAV.
3.2
Information Managed by ETL System
Electronic components on board the aircraft generate analog and digital data [24] of
physical magnitudes such as temperature, fuel level, current, voltage and serial com-
munication frames. All these variables from the UAV equipment are acquired and
processed by the ETL system.
3.3
Operation Work
ETL system can work in two modes: master mode and slave mode. In master mode, the
ETL system performs information requests to other equipment on board the aircraft. In
slave mode or listening mode, ETL system is waiting to receive information from other
equipment or sensor on board the UAV. ETL system is conﬁgured in master mode for
propulsion, electro-optical and transponder systems. On the other hand, ETL is con-
ﬁgured in slave mode for GPS system and data link system.
3.4
Communication Data Frame
ETL data frame encapsulates and codes signs obtained from on board UAV. Data
frame structure is presented in Table 1.
4
Tests and Preliminary Results
To determine the capacity for extraction, treatment and loading of information among
the devices on board the UAV, tests were carried out to read out the requirement of the
analogue signal corresponding to the acceleration percentage of the Navigation and
Control (NC) component. The ETL system extracts the analog PWM signal from the
NC, calculates the pulse width of the PWM signal and transforms it to a digital value
between 0X00 corresponding to 0% acceleration and 0XFFFF corresponding to the
658
W. Medina-Pazmiño et al.

100% engine acceleration. The determined value is transmitted using RS-232 com-
munication standard to the propulsion system, which continuously informs its speed.
The NC system is able to perform control tasks on the engine through the ETL
system. The value of engine acceleration required by the NC system is 35% and the
value interpreted by the ETL system is 37%. The difference of 2% is due to the signal
ﬁltering and regeneration stage performed by the ETL system in the analog variables.
In Fig. 4 graphic results of request and response of acceleration system is presented
Similarly, the analog signals from propulsion equipment were recorded, including
the engine cylinders temperature and the number of times the shaft rotates about its
axis. Records of analog variables of propulsion system are presented in Fig. 5.
Table 1. Communication data frame structure.
Deﬁnition
Data
length
Description
Data frame start
03
bytes
Three registers present starting point of reading or writing
request
Function
01
byte
Functions deﬁned are analog data reading, digital data
reading, analog data writing, digital data writing, special
components reading and special components writing.
Special components refer to transponder systems.
Data reading or
writing address
02
bytes
Main computer possesses 03E8 memory addresses to read
or write. Reading or writing address refers to starting point.
Devices
information
exchange
N
bytes
N registers are read or written by request. Data frame length
is variable
Error detection
02
bytes
Cyclic redundancy control algorithm CRC16 is used to
determine error in communications
Fig. 4. Comparison of acceleration percentage required by the NC system vs. propulsion system
response
Analysis and Implementation of ETL System
659

5
Conclusions
This paper presents the process of development of Extraction, Transformation and
Load (ETL) system for UAV. In this case, in order to integrate devices using different
communications protocol ETL transforms signals related to propulsion system. A data
frame is structured to convert analog and digital signals into a standar data. The system
is tested with the acceleration of the propulsion system. Differences between request
and response of the systems are considered acceptable.
As future work, it is proposed to incorporate the ETL system in other Unmanned
Air Vehicles, maintaining the operating principle and making the respective modiﬁ-
cations and adaptations for ﬁxed wing aircrafts or hot air balloons.
References
1. Fahlstrom, P., Gleason, T.: Introduction to UAV Systems, 1st edn. Wiley, Hoboken, N.
J. (2013)
2. Sieberth, T., Wackrow, R., Chandler, J.: Automatic detection of blurred images in UAV
image sets. ISPRS J. Photogrammetry Rem. Sens. 122, 1–16 (2016). https://doi.org/10.1016/
j.isprsjprs.2016.09.010
3. Ribeiro-Gomes, K., Hernandez-Lopez, D., Ballesteros, R., Moreno, M.: Approximate
georeferencing and automatic blurred image detection to reduce the costs of UAV use in
environmental and agricultural applications. Biosyst. Eng. 151, 308–327 (2016). https://doi.
org/10.1016/j.biosystemseng.2016.09.014
4. d’Oleire-Oltmanns, S., Marzolff, I.: UAV derived data for the monitoring of gully erosion in
the Souss Basin, Morocco. In: European Geosciences Union General Assembly Conference,
vol. 14, p. 911 (2012)
5. Valencia-Redrovan, D., Guijarro-Rubio, O., Basantes-Montero, D., Enríquez-Champutiz,
V.: Analysis, design, and implementation of an autopilot for unmanned aircraft - UAV’s
based on fuzzy logic. In: 2015 Asia-Paciﬁc Conference on Computer Aided System
Engineering (APCASE), pp. 196–201 (2015). https://doi.org/10.1109/APCASE.2015.42
Fig. 5. Analog variables of the propulsion system during start-up and stabilization.
660
W. Medina-Pazmiño et al.

6. Medina-Pazmino, W., Jara-Olmedo, A., Valencia-Redrovan, D.: Analysis and determination
of minimum requirements for a data link communication system for unmanned aerial
vehicles- UAV’s. In: 2016 IEEE Ecuador Technical Chapters Meeting (ETCM), vol.
1 (2016). IEEE Press, New York (2016). http://dx.doi.org/10.1109/etcm.2016.7750816
7. Zwick, H.H.: Passive electro-optical remote sensors at the Canada centre for remote sensing.
Can. J. Rem. Sens. 4, 51–62 (1978) http://dx.doi.org/10.1080/07038992.1978.10854967
8. Guerrero, J., González, A., Vega, J., Tovar, L.: Instrumentation of an array of ultrasonic
sensors and data processing for unmanned aerial vehicle (UAV) for teaching the application
of the kalman ﬁlter. Procedia Comput. Sci. 75, 375–380 (2015). https://doi.org/10.1016/j.
procs.2015.12.260
9. Li-Qun, L., Hui, Z.: The research of light UAV management computer system. Int.
J. Control Autom. 8(2), 281–290 (2015). http://dx.doi.org/10.14257/ijca.2015.8.2.27
10. Electronic y Electric Industries Association of Chile.: Protocolos de comunicaciones
industriales [en línea]. Asociación de la Industria Eléctrica y Electrónica, Vicuña (Chile)
(2011)
11. Bansal, S.K.: Towards a semantic extract-transform-load (ETL) framework for big data
integration. In: 2014 IEEE International Congress on Big Data, Anchorage, AK 2014,
pp. 522–529. IEEE Press, New York (2014). https://doi.org/10.1109/BigData.Congress.
2014.82
12. McGee, W.: Three stage process speeds path to avionics system deployment. Cots J. (2016).
http://archive.cotsjournalonline.com/articles/view/106078. Accessed: 08 Jan 2017
13. Cherukuri, R.: Design and development of a project-based embedded system laboratory
using PIC 18F25K20. Am. J. Embedded Syst. Appl. 2(3), 21 (2014). https://doi.org/10.
11648/j.ajesa.20140203.12
14. Airforce-Technology on Line: Heron/Machatz 1 Unmanned Aerial Vehicle (UAV), Israel
(2017). http://www.airforce-technology.com/projects/heron-uav/. Accessed: 08 Jan 2017
15. Sboui, L., Ghazzai, H., Rezki, Z., Alouini, M.S.: Achievable rates of UAV-relayed
cooperative cognitive radio MIMO systems. In: IEEE Access, vol. 5, pp. 5190–5204. IEEE
Press, New York (2017). https://doi.org/10.1109/ACCESS.2017.2695586
16. Choi, H., Geeves, M., Alsalam, B., Gonzalez, F.: Open source computer-vision based
guidance system for UAVs on-board decision making. In: 2016 IEEE Aerospace
Conference, Big Sky, MT, pp. 1–5. IEEE Press, New York (2016). https://doi.org/10.
1109/AERO.2016.7500600
17. Chen, N.: Design of micro inertial navigation computer and data real-time displaying system
based on FPGA. Adv. Mater. Res. 919–921, 2123–2126 (2014). https://doi.org/10.4028/
www.scientiﬁc.net/AMR.919-921.2123
18. Abdallah, M., Elkeelany, O.: A survey on data acquisition systems DAQ. Int. Conf. Comput.
Eng. Inf., 240–243 (2009). https://doi.org/10.1109/ICC.2009.24
19. Li, M., Li, G., Zhong, M.: A data driven fault detection and isolation scheme for UAV ﬂight
control system. In: 2016 35th Chinese Control Conference (CCC), Chengdu, pp. 6778–6783
(2016). https://doi.org/10.1109/ChiCC.2016.7554425
20. Xia, M., Gong, L., Lyu, Y., Qi, Z., Liu, X.: Effective real-time android application auditing.
In: 2015 IEEE Symposium on Security and Privacy 2015, pp. 899–914. IEEE Press, New
York (2015). https://doi.org/10.1109/SP.2015.60
21. Maulding, G.: RS232 interface circuits for 3.3 V systems. In: Dodkin, B., Hamburger, J
(Eds.) Analog Circuit Design, pp. 849–850. Elsevier, Oxford (2015)
22. Radonjic, A., Vujicic, V.: Integer codes correcting high-density byte asymmetric errors.
IEEE Commun. Lett. 21(4), 694–697 (2017). https://doi.org/10.1109/LCOMM.2016.
2644661
Analysis and Implementation of ETL System
661

23. Guillaud, X., Faruque, M.O., Teninge, A., Hariri, A.H., Vanfretti, L., Paolone, M., Dinavahi,
V., Mitra, P., Lauss, G., Dufour, C., Forsyth, P., Srivastava, A.K., Strunz, K., Strasser, T.,
Davoudi, A.: Applications of real-time simulation technologies in power and energy
systems. IEEE Power Energy Technol Syst. J. 2(3), 103–115 (2015)
24. Nakayama, K., Cha, B., Kanaoka, Y., Isahaya, N., Omori, M., Uno, M., Takeya, J.: Organic
temperature sensors and organic analog-to-digital converters based on p-type and n-type
organic transistors. Org. Electron 36, 148–152 (2016)
25. Feng, W., Chen, X.: LED visible light communication system based on FPGA. In: 2015
IEEE Advanced Information Technology, Electronic and Automation Control Conference
(IAEAC), pp. 428–432 (2015). https://doi.org/10.1109/IAEAC.2015.7428589
662
W. Medina-Pazmiño et al.

Analysis of the Interaction on the Web
Through Social Networks (Twitter, Facebook,
Instagram) Case Study: Economic Sectors
with Higher Incomes in Ecuador
Mariuxi Tejada-Castro(&)
, Maritza Aguirre-Munizaga(&)
,
Vanessa Vergara-Lozano
, Mayra Garzon-Goya
,
and Evelyn Solís-Avilés
Escuela de Ingeniería en Computación e Informática,
Facultad de Ciencias Agrarias, Universidad Agraria del Ecuador,
Av. 25 de Julio y Pio Jaramillo, P.O. BOX 09-04-100, Guayaquil, Ecuador
mtejada@uagraria.edu.ec
Abstract. This investigation presents the architecture of a tool of social net-
works management using Web semantic techniques; it is expected to serve as a
guide for developing through free software a social networks manager. As part
of the methodology, it is done an analysis of diverse tools of important path that
have inﬂuence over the work of Social Media Management, to conclude in the
model proposed in this investigation. At the same time, this document focuses
on standing out how the three economic sectors of greater income in Ecuador
(Telecommunications, Supermarkets and Financial) use the social networks to
promote their organizations through the platforms of social media, allowing by
this way a greater conversion ratio around the creation of new and/or potential
customers. Therefore, it is demonstrated through a case study that communities
of interest online are generated in favor of companies, brands or products. For
the development of the case, metrics and intelligence analysis in Social Net-
works tools are used, getting as a result a complete detail of the inﬂuence of
social networks in these sectors and the architectural model of an application
that could be developed to increase the utilization of information technologies.
Keywords: Social networks  Media Management  Economic sectors
Ecuador  Computer interaction
1
Introduction
Internet has become an extraordinary tool for the exchange of information, and now is
even considering a strategic infrastructure for the development of the countries.
Nowadays technologies tendencies [1] induce that customers and providers make all
their transactions in a digital way, in this way the diverse platforms of social media like
Twitter [2], Facebook [3], Instagram [4] allow the creation of customers beneﬁting to
different economic sectors of Ecuador [5], it can highlighted that in this country the on
line communities of interest are formed around companies, marks or products that use
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_63

the interaction in the Web to attract customers and disseminate information about
products and services through the use of various digital marketing tools.
In spite of the popular use of social media by consumers and sellers, the empirical
investigation in Ecuador that stands out its economic values is still delayed. In this
study, we have integrated qualitative user-seller interaction data from three economic
sectors using consumer transaction data to assemble a unique group of data by sectors.
Through the study carried out in this research it can be seen that the companies use
diverse social networks to obtain commercial value [6], these platforms can be used
like another way for similar applications of e-commerce, companies are looking to fully
exploit the capacities of social media platforms to attract economic sectors with greater
incomes in Ecuador. This article supports the argument that indicates that social media
is a hybrid element of the mix of promotion, since in a traditional sense allow to the
companies communicate with their customers, whereas in a non-traditional sense allow
to the customers comment directly between them, about the products.
To identify different approaches that inﬂuence now of making implementations of
the social networks in three business sectors, we have made several metrics mea-
surements using Open Source tools that are found for free on the Web like LikeAlyzer,
SocialMention and Google Analytics.
Section two takes as reference different published research within the proposed area
to relate it to the objective of this publication, section three details the case study as the
main axis of this research highlighting how companies in Ecuador ﬂuctuate in the use
of social networks to reach their customers and with this is highlighted and affected the
amount of incomes of each of them, section four presents a manager architecture that
implemented in an application can achieve that within a tool to run various types of
measurements and recommendations based on various criteria, both quantitative and
qualitative.
2
Works Related
In the present study we have taken as reference several researches done to demonstrate
how the different businesses use tools of competitive analysis of social media to
transform the data of the social media into knowledge for decision making [8, 17],
between the advantages that are highlighted about using social networks to increase
income, it can be mentioned that entrepreneurs also use these strategies to overcome
their sales.
The use of freely available online data is increasing rapidly because companies
have seen in this social networking information an aid in gaining customer under-
standing for business strategies decisions. However, the unstructured and uncertain
character of this type of large data implies challenges, for that reason the architecture
that allows managing the quality of social media data at each stage of processing has
been designed. The proposal resembles other investigations that help improve business
664
M. Tejada-Castro et al.

decision making by providing in real time validated data for the user [9], in which it can
be observed the initial validation layer of the metadata management for large systems
of data.
There are several tools that perform an excellent management to the marketing
strategy in a company from different points of view, however, it is good to note that
there is no a free tool that includes all the beneﬁts that a company needs to manage their
campaigns and enhances the products it markets, as well as its offers [7].
This document is based on related work where it provides an overview of the
analyzes and essential methods, useful for the improvement of the business architecture
and based on the social networks approach, where it is demonstrated that the analysis of
social corporate networks, as a support system for decision making can inﬂuence in the
management of a company [8].
Figure 1 presents a general diagram of operation of the Hootsuite tool, there are
two ways of use that can be free or paid, for the freeway users can link their accounts to
different social networks and conﬁgure alerts, as well as scheduling the publicity that is
published through the manager, in this free interface you can create tabs, manage a
Web or mobile environment and schedule automatically advertising. When the user
gets a paid account has more options to manage their social networks such as campaign
monitoring and hashtag tracking, in turn the tool allows obtaining the result of the
analyzed metrics from digital marketing campaigns.
Fig. 1. Diagram of operation for the interaction with Hootsuite
Analysis of the Interaction on the Web Through Social Networks
665

3
Case of Study: Economic Sectors in Ecuador
with Higher Incomes
In order to perform the analysis of the use and interactivity in the Web through the
Social Networks of the business sector, the most signiﬁcant ones in Ecuador were taken
according to the Ekos Magazine in its Companies Ranking in 2016 [9], these sectors
are among the ﬁrst according to the total income received in the year analyzed.
The sectors chosen for the present analysis are: Retail stores (Corporación La
Favorita, Corporación el Rosado and Tiendas Industriales Asociadas TIA S.A.),
Telecommunications (Consorcio Ecuatoriano de Telecomunicaciones S.A. Conecel,
Otecel S.A., Corporación Nacional de Telecomunicaciones) and the Financials (Banco
del Pichincha, Banco del Pacíﬁco and Banco de Guayaquil).
The social networks in which we have focused are Facebook, Twitter and Insta-
gram, that according to the Ranking of Social Networks of Ecuador Web 2017 [10]
they are the most used by Ecuadorians.
Table 1 shows the business sectors of the case study that use these social platforms
to interact with their clients. In addition, they were selected because they allow content
sharing and are highly participative. Within the criterion, others social platforms were
grouped like Pinterest, YouTube, Tumblr, etc.
We can also verify their use in Table 2, which shows the adoption density based on
the number of platforms used by the three sectors, we can say that on average each
sector has used more than one social platform; we observed that the social network
Facebook has the majority of active accounts with 47% of adoption of the platform,
while Instagram and Twitter have equally 20% of adoption.
Table 1. Use of social networks by the three business sectors to September 2017
Sector of The Company/Name of
the Company
Social Networks
Twitter
Facebook
Instagram
Others
Yes
No
Yes
No
Yes
No
Yes
No
Telecommunications
Claro S.A.
X
X
X
X
Otecel S.A.
X
X
X
X
CNT
X
X
X
X
Retail trade - supermarkets
Supermaxi
X
X
X
X
Mi Comisariato
X
X
X
X
Tía S.A.
X
X
X
X
Financial
Banco del Pichincha
X
X
X
X
Banco del Pacíﬁco
X
X
X
X
Banco de Guayaquil
X
X
X
X
*According to the ofﬁcial Web pages of the companies.
666
M. Tejada-Castro et al.

By verifying in each of the social networks platforms we can observe some of the
metrics that may be of interest to companies in the sector, such as the number of
followers, number of mentions, likes to what is published, number of publications, as
we can see in Table 3, but indicators of acceptance, response rate, attraction index, etc.,
are not obtained within these platforms; metrics that can be used to measure and
manage the growth of companies within the network, that is why we have used other
Table 2. Density of use of social platforms
Sector of the company/Name of
the company
Number of applications of social networks by business
sector
Twitter
Facebook
Instagram
Others
Average
Telecommunications
3
10
3
4
1,67
Retail Trade - Supermarkets
3
4
3
0
0,83
Financial
3
4
3
0
0.83
Total 3 Business Sectors
6
14
6
4
2.50
%Of Use of Social Network in
function of active accounts by
business sector
20%
47%
20%
13%
8%
*To 13 August 2017
Table 3. Metrics obtained from the web platforms of social networks
Sector of the Company 
/ Name of the Compa-
ny 
SOCIAL NETWORKS
SOCIAL MENTION
TWITTER
FACEBOOK
INSTAGRAM
QUANTITY
# FOLLOWERS
#MENTIONS
LIKES
FOLLOWERS
# PUBLICATIONS
#FOLLOWERS
Positive Feelings
Negative Feelings
Sources Sought
Quantity of ítems in 
Sources 
Telecommunications
Claro S.A.
501.222
101
1.545.734
1.532.113
687
55.421
5 
4 
4 
117
Otecel S.A.
414.050
104
1.532.113
1.337.833
704
21.586
1 
0 
3 
94
CNT
172.627
96
476.211
479.881
N/A 
N/A 
3 
0 
3 
107
Retail Trade - Supermarkets
Supermaxi
202.000
331
565.207
561.681
531
495.000
1 
0 
2 
6 
Mi Comisariato
7.922
362
221
220.970
1.048
593.000
0 
0 
3 
86
Tía S.A. 
7.246
179
2.651
2.621
149
107.000
N/A N/A N/A N/A 
Financial
Banco del Pichincha
300.000
705
869.381
866.386
113.000
11.200
0 
1 
3 
5 
Banco del Pacífico
305.000
810
817.203
813.921
765
28.100
1 
0 
3 
21
Banco de Guayaquil
172.000
542
371.745
368.263
1.341
23.800
2 
0 
4 
83
* To 13 August 2017
Analysis of the Interaction on the Web Through Social Networks
667

tools to verify some of the metrics that by itself cannot be obtained from social
networking platforms.
In order to complement the information presented in the following table, it has been
considered to analyze each of the sectors jointly to four additional aspects taken from the
Social Mention tool, which provides metrics of the analyzed accounts. They could be
positive feelings that provides information regarding the positive mentions of the
company in the social networks, or negative feelings that allows identifying the men-
tions and negative comments of the mark in the social networks. Also searched sources
that presents the main sources that mentions the mark and the quantities of items in
sources that list the quantities of items related to the brands sought in the sources.
From this information, it was obtained that in the accounts of the telecommuni-
cations sector, Claro S.A. is the one that registers the highest values in reference to
these four metrics provided by the tool; however, Otecel and CNT metrics show that
there are no negative feelings for their brand. Concerning the Retail sector, the one that
registers most activity at the level of the metrics speciﬁcally in sources sought and
quantity of items in sources is Mi Comisariato and about Tía S.A., no data were
obtained.
In terms of the ﬁnancial sector, there was a greater activity for the Banco de
Guayaquil with activities in positive feelings, sources sought and quantity of items in
sources, the one with the least activity is Banco Pichincha. In this way, the lack of
coincidence between data that a company can extract from its networks and interpre-
tation of this data, is validated in the given contingency table, obtaining the limitations
of the tools that are presented as managers within this article, such as decoding the
content structure and presentation, as well as customer perception [11].
In the next section, we considered several metrics that are very useful to analyze the
behavior of customers in different social networks that have adopted these three
business sectors, as well as their acceptance and customers tastes. All this using 2 tools
for analysis such as SocialMention and LikeAlyzer both platforms that manage social
networks; the ﬁrst one is a Web application for searching and analyzing all the content
added by users on the internet, while LikeAlyzer is a Web page that analyses for free
Facebook pages.
3.1
Results of the Case of Study
Telecommunications
Within the telecommunications sector, the three companies belonging to this sector
were analyzed, segregating each one of the social networks:
Twitter. ID="Par24">It can be seen from Table 4 that on average this sector generates
approximately 80.846 tweets being the largest tweets owner Otecel S.A. (Movistar) on
the other hand, we can identify that the company that has more likes from its customers
is Claro S.A. with 2.903 likes; reviewing the SocialMention we can conﬁrm what is in
the Twitter Platform, since according to this tool the probability that customers speak
about the brand is 72% (Passion), the company that is most talking about on social
networks is CNT.
668
M. Tejada-Castro et al.

Facebook. It can be observed in Table 5 that on average this sector generates
approximately one million followers in this social network, being the one with the
largest number of followers Claro S.A., on the other hand, we can identify that the
company that has less followers is CNT with 479.881 followers; when reviewing the
LikeAlyzer tool we can conﬁrm that by the index of attractiveness that it has from its
customers with 74 of 100 for Claro S.A., another interesting indicator that measures
this tool is the response rate and the attractiveness with other similar brands, what gave
us that the one with the highest response rate is Otecel S.A. (Movistar) and the one that
is the most attractive with respect to the others is CNT with 56 of 100.
Table 4. Analysis of telecommunications sector by SocialMention.
Telecommunications 
TWITTER - PLATFORM
SOCIAL MENTION
TWEETS
FOLLOWING
LIKES
Lists
Strength 
Feeling 
Passion
Average by 
Mention 
Scope 
Claro S.A.
94.704 108.411 2.903 
6 
4%
0 A 0
72%
9 horas
10%
Otecel S.A.
115.841 71.255 2.197 12
1%
0 A 0
15%
13 horas
28%
CNT
31.994 
4.645 
899 
2 
8%
2 A 1
0%
6 horas
20%
Average
80.846 
61.437 2.000 
7 
4%
* To 01 September 2017
Table 5. Analysis of the telecommunications sector by LikeAlyzer
Telecommunications 
FACEBOOK 
LIKEALYZER
Likes 
Followers 
Atractiveness 
Answer Index 
Attractiveness 
with other similar 
brands 
Publications By 
Day 
Claro S.A.
1.545.734
1.532.113
74
83%
53
72
Otecel S.A.
1.532.113
1.337.833
83
100%
53
1,12
CNT
476.211
479.881
68
N/A 
56
6,02
Average
1.184.686
1.116.609
75
92%
54
26
* To 01 September 2017
Analysis of the Interaction on the Web Through Social Networks
669

Commercial Sector
The retail companies or supermarkets from which the data was collected were Mi
Comisariato, Supermaxi and Almacenes Tía; evidencing that they mainly work with
three social networks such as Twitter, Facebook and Instagram.
The research was concentrated in analyzing the case of Twitter, by observing the
Table 6 we identiﬁed that the company that has the greatest quantity of followers and
mentions is Mi Comisariato with 7.922 followers and 362 mentions.
In the case of Facebook, you can see Table 7 where the likes and followers were
veriﬁed and the company Supermaxi obtained the greatest number of likes and fol-
lowers with 565.207 and 561.681 respectively.
In the case of Instagram, publications and followers were analyzed and the com-
pany with the highest number was Mi Comisariato with 1.048 publications and 59.300
followers.
Companies that participated in this research, work with one account in each social
network Facebook, Twitter and Instagram except from Almacenes Tía that works with
2 accounts in Facebook.
Analysis of the Social Network Facebook for retail companies or supermarkets
gave the attractiveness index over 100, answer index, attractiveness with similar brands
over 100, advertising per day and likes, allows knowing the behavior of these
Table 6. Analysis of the commercial sector by Twitter
Commercial
Analysis of Twitter
Tweets Following Likes List
Supermaxi
7.068
417
2.052 N/A
Mi Comisariato 6.988
1.004
271
N/A
Tía S.A.
2.212
552
1.465 N/A
Table 7. Analysis of the commercial sector by Facebook
Commercial 
ANALYSIS OF FACEBOOK 
Atractiveness 
Answer Index
Attractiveness 
with other simi-
lar brands
Publications by 
Day 
Likes 
Supermaxi
70 
701 
39 
1,95 574.663 
Mi Comisariato
80 
784 
39 
1,68 224.208 
Tía S.A. 
60 
166 
64 
6,1 
517.329 
670
M. Tejada-Castro et al.

companies, interaction with other similar institutions and connection with users. The
analysis shows that in the attractiveness index the company Mi Comisariato obtained
80 points beating the others, in the answer index Mi Comisariato has 784 points in
comparison with the other companies, the attractiveness index with 100 similar brands,
in the advertising index per day Supermaxi gets 1,95 beating the others and in the Likes
index collects a superior score of 574.663 corresponding to the company Supermaxi.
Financial
The next section presents the analysis of each one of these social networks from the
three selected ﬁnancial sector companies, which will also describe if a complementary
tool is used to analyze these networks, their behavior and community interaction.
Twitter. We can see in Table 8 that these companies have an average of 259.000
followers on Twitter, the company with the largest number of followers is Banco del
Pacíﬁco with 305.000, and the one with the lowest quantity of followers is Banco
Guayaquil with 172.000 followers. The maximum difference between the followers of
this group of companies is 133.000. Tweets made by companies in this sector gave a
total of 124.200, the company with fewer tweets is Banco del Pacíﬁco, the one with the
greatest quantity of tweets is Banco Guayaquil, complementing this index with the one
obtained from SocialMention a direct relationship is evidenced because Banco
Guayaquil has an average by mention of 01 min, which indicates that there it has more
interaction in this social network than the other companies in this sector, being also the
ﬁnancial institution with more quantity of Likes it has a signiﬁcant difference from the
rest of up to about 1.600 Likes. On the other hand, Banco Guayaquil is one of the
companies in this sector that follows a greater number of users, also with a notable
difference of up to 15.000 users. The average retweets that were obtained from this
group of companies and their interaction in this social network is 86. The company
Table 8. Social network analysis of Twitter
Financial 
Source: Twitter
Source: Social Mention
Followers 
Tweets 
Following 
Likes 
List 
Strength 
Feeling 
Passion
Average by Mention
Retweets 
Banco Pichincha
300.000
44.600
3.676
200
n/a 
27%
0 To 0
37%
2 hours
82
Banco Del Pacífico
305.000
25.900
7.468
1.179
1 
100%
3 To 1
2%
2 min
97
Banco Guayaquil
172.000
53.700
18.900
1.869
5 
42%
12 To 0
27%
1 min
80
Analysis of the Interaction on the Web Through Social Networks
671

with the greatest strength in this social network according to the consultation made in
the last days of August 2017 with the SocialMention tool, we obtained that Banco del
Pacíﬁco is the one that presents an index of 100%, that is that in the last twenty-four
hours before the consultation this mark was being discussed in social networks with
great intensity, the one with the least strength is Banco Pichincha. From the “feeling”, it
is evident that Banco Guayaquil registers a better mean of positive and negative
mentions from 12 to 0. Regarding the “passion” that is another aspect presented in the
table, it is deﬁned as the measure of probability that people who speak of these
companies will do it again, the major values were represented by Banco Pichincha and
Banco Guayaquil.
Facebook. For the social network Facebook, information of each one of the accounts of
these companies of the ﬁnancial sector was obtained, data like: Likes, Followers, Total
Visits and LikeAlyzer was used for the rest of indexes that are presented in Table 9, in
this social network it was obtained that the average ﬁnancial sector followers on
Facebook is 682.857 and the average of likes 686.110. From the ofﬁcial page, it was
obtained that the page that has the greatest number in the record of visits is Bank
Guayaquil with 1.220 visits. Using the LikeAlyzer tool, the ﬁnancial institution that
evidences the greatest number of publications per day in Facebook is Banco Guayaquil
with an average of three publications per day, when comparing with the rest of
companies of this group the reﬂected index is an average of one publication by day.
Also, this tool allows a comparison analysis where the companies are classiﬁed
according to a ranking or score where the rating goes from 01 to 100, with 01 being the
lowest and 100 being the highest, the lowest score is from Banco Guayaquil with 67
points, the highest score is from Banco Pichincha with 77 points, if we compa
red these results with other companies in the sector the score is 53 to 60 in similar
brands and 54 to 59 in commercial banking.
Table 9. Analysis of social network Facebook
Financial 
SOURCE: FACEBOOK 
SOURCE: LIKEALYZER
Likes 
Followers 
Total of visits
Publications by 
day 
Ranking Of Comparison
Media like-
rank 
Average in 
commercial 
bank 
Similar 
brands marks 
Comparison
Banco Pichincha
869.381
866.386
N/A 
1,12
51
54
60
77
Banco Del Pacífico
817.203
813.921
680
1,34
51
54
60
73
Banco Guayaquil
371.745
368.263
1.220
3,38
51
59
53
67
672
M. Tejada-Castro et al.

Instagram
When analyzing the social network Instagram, it can be seen in Table 10 that the
company with the most publications in this network is Banco Pichincha with a sig-
niﬁcant difference with respect to the other two ﬁnancial companies that are part of the
analysis, the company with the lowest number of publications is Banco del Pacíﬁco,
however this company has the largest number of followers reaching a total of 28.100
members of its community, if the last column of the table is analyzed, the company that
most connects with its community is Banco de Guayaquil which follows 7.469
members of its community. The average followers of this sector is of 21.033 on
Instagram.
In the end, it can be concluded that, from this group of social networks, these
companies register a greater number of followers in Facebook, where some of them
have reached approximately 860.000 users, followed by the social network Twitter
with a maximum of 305.000 followers and ﬁnally Instagram with a maximum of
28.000 followers.
4
Proposed Architecture for Interaction in Social Networks
This study identiﬁes an architecture based on usability and accessibility for free to be
used in digital marketing campaigns, taking into account that today’s software systems
are characterized by the increased size, their complexity, the distribution and hetero-
geneity [12]. This research contributes to the infrastructure management through an
architecture that understands and supports the interaction between software require-
ments and social networks challenging problems in software engineering research.
Regarding to the market trend in the digital marketing, there is a lot of talk about
content marketing and marketing attraction between the consumer and seller. These
strategies are the main ones that will allow them to be applied as tools to execute
planned processes in the positioning of campaigns in social networks [13], which make
it possible to take products directly to potential customers and thus close sales
according to the conversion into planned sales, obtaining expected results in the Return
of Investment index (ROI) [14], due to the large trafﬁc of information generated in
social networks, it is very useful to use a platform that optimizes the management of
Social Media by facilitating communication with customers and processing their
requests, as well as monitoring their behavior and maintaining their loyalty.
Table 10. Analysis of social network Instagram
Financial
Instagram
Publications Followers Followed
Banco Pichincha
113.000
11.200
43
Banco Del Pacíﬁco 765
28.100
57
Banco Guayaquil
1.341
23.800
7.469
Analysis of the Interaction on the Web Through Social Networks
673

For that reason, an architecture model for a Social Media Manager System is
proposed, presenting a three-level approach to integrate trust, provenance and leaks
based on Semantic Web [15].
As social networks are virtual public places, much information can exist in them
that cannot be reliable for everyone. A necessary mechanism to qualify the next news,
reviews and opinions on a topic deﬁned by users, according to each preference and
oriented to companies’ growth, it is oriented in a single tool through the architecture
designed in this section.
This work suggests the use of diffuse linguistic terms to specify trust to other users
and proposes an algorithm to infer the trust of a person to another person who cannot
be directly connected in the conﬁdence graph of a Social Network. It provides a
classiﬁcation of an adaptive architecture related to the type of semantic technology that
they download to improve speciﬁc tasks of adaptation and users modeling.
Figure 2 shows three levels of the social network manager in the architecture of this
research, the ﬁrst level represents the number of mentions, likes, comments, followers,
among other indicators that are the basis of the strategy of market capture in digital
marketing, the second level establishes the programming phase of the processes that are
Fig. 2. Proposed architecture of social networks manager
674
M. Tejada-Castro et al.

deﬁned as the most important for a company, and as a third level the results to be given
by the manager are speciﬁed, considering as a strategic tool the use of semantic
networks by discovering the preference of the customers regardless of their location.
5
Results
Through the proposed architecture, a method, a system and an advanced product
manager is introduced to analyze social media content. The case study was used to
measure the fragments of social media data in strategic sectors of Ecuador. Using the
Semantic Web [16], the aim is to overcome the limits in the digital marketing appli-
cation, taking semantic wikis (Semantic MediaWiki), knowledge networks (Twine),
detection and reuse of integrated microcontainers (Operator, Headup, Semantic Radar),
social graphics and data APIs portability (from Google and Facebook), etc., gathers
various data to pose a completely dynamic and interactive manager.
The tool proposed in this architecture is focused on semantic networks and can be
advantageously used to analyze and understand the content of the social media message
even when only very small amounts of data are provided within each publication.
6
Conclusions and Future Works
The architecture designed in this research is expected to be a reference for future
applications and has been designed for large data systems based on other implemen-
tation architectures performed by large companies.
The impact of advertising through social media in combination of the applied
technique in digital marketing is linked to the different components, analyzed in the
case study within measurements of several metrics have been made using tools that are
Open Source and are found for free on the Web such as LikeAlyzer, SocialMention and
Goo-gle Analytics. The idea of this analysis is to demonstrate the importance of this
type of managers in the strategic impact of the use of social networks within different
economic sectors, in this case from Ecuador. In the future, it is expected to build
through this framework an application that serves as a manager for the RSS, designed
with the basis of the architecture proposed in section four of the present research.
The universal reach of social networks for all types of customers can be further
enhanced as long as the messages transmitted through them are handled correctly.
References
1. Alberti, A.M.: Future network architectures: technological challenges and trends. In: Tronco,
T. (ed.) New Network Architectures: The Path to the Future Internet, pp. 79–120. Springer,
Heidelberg (2010)
2. Alhajj, R., Rokne, J. (eds.): Twitter. In: Encyclopedia of Social Network Analysis and
Mining, p. 2253. Springer, New York (2014)
Analysis of the Interaction on the Web Through Social Networks
675

3. Ledermann, N., Schwartz, R., Abd-El-Khalick, F.: Encyclopedia of Science Education.
Springer, Dordrecht (2015)
4. Lee, C.S., Abu Bakar, N.A.B., Muhammad Dahri, R.B., Sin, S.-C.J.: Instagram this! Sharing
photos on Instagram. In: Allen, R.B., Hunter, J., Zeng, M.L. (eds.) Digital Libraries:
Providing Quality Information: 17th International Conference on Asia-Paciﬁc Digital
Libraries, ICADL 2015, Seoul, Korea, 9–12 December, 2015, Proceedings, pp. 132–141.
Springer International Publishing, Cham (2015)
5. Martín-Mayoral, F.: Estado y mercado en la historia de Ecuador. Rev. Nueva Soc. (2009)
6. Kazienko, P., Michalski, R., Palus, S.: Social network analysis as a tool for improving
enterprise architecture. In: O’Shea, J., Nguyen, N.T., Crockett, K., Howlett, R.J., Jain, L.C.
(eds.) Agent and Multi-Agent Systems: Technologies and Applications: 5th KES Interna-
tional Conference, KES-AMSTA 2011, Manchester, UK, June 29–July 1, 2011, Proceed-
ings, pp. 651–660. Springer, Heidelberg (2011)
7. Ladkin, A., Buhalis, D.: Online and social media recruitment: hospitality employer and
prospective employee considerations. Int. J. Contemp. Hosp. Manag. 28, 327–345 (2016)
8. Kazienko, P., Michalski, R., Palus, S.: Social network analysis as a tool for improving
enterprise architecture. In: O’Shea, J., Nguyen, N.T., Crockett, K., Howlett, R.J., Jain, L.C.
(eds.) Agent and Multi-Agent Systems: Technologies and Applications: 5th KES Interna-
tional Conference, KES-AMSTA 2011, Manchester, UK, June 29–July 1, 2011, Proceed-
ings, pp. 651–660. Springer, Heidelberg (2011)
9. Revista Ekos - Digital: Guía de Negocios Ecuador - Ranking mejores empresas Ecuador
10. Formación Gerencial: Ranking Redes Sociales, Sitios Web y Aplicaciones Móviles Ecuador
2017 | Formacion Gerencial BlogFormacion Gerencial Blog
11. Bracamonte, T., Bustos, B., Poblete, B., Schreck, T.: Extracting semantic knowledge from
web context for multimedia IR: a taxonomy, survey and challenges. Multimed. Tools Appl.
(2017). https://doi.org/10.1007/s11042-017-4997-y
12. George, S., Radu, C., Muneeb, A.: Social media and digital interactions using cloud services
for orienting young people in their careers (2017)
13. Van Looy, A.: Social media strategy and return on investment. In: Social Media
Management: Technologies and Strategies for Creating Business Value, pp. 49–62. Springer
International Publishing, Cham (2016)
14. Blanchard, O.: Social Media ROI. Pearson Education India, New Delhi (2011)
15. Hermida, J.M., Meliá, S., Montoyo, A., Gómez, J.: Applying model-driven engineering to
the development of rich internet applications for business intelligence (2013)
16. Breslin, J.G., Passant, A., Decker, S.: Motivation for applying semantic web technologies to
the social web. In: The Social Semantic Web, pp. 11–20. Springer, Heidelberg (2009)
17. Barão, A., de Vasconcelos, J.B., Rocha, Á., Pereira, R.: A knowledge management approach
to capture organizational learning networks. Int. J. Inf. Manage. 37(6), 735–740 (2017)
676
M. Tejada-Castro et al.

Proposal of a Supply Chain Architecture
Immersed in the Industry 4.0
Jose Ignacio Rodriguez(&), Monica Blanco(&),
and Karen Gonzalez(&)
Universidad Distrital Francisco José de Caldas, Bogota, Colombia
jirodriguezm@udistrital.edu.co,
{mjblancor,ktgonzalezr}@correo.udistrital.edu.co
Abstract. This article shows a proposal of the architecture that can be adopted
by the supply chains immersed in the industry 4.0, that its considered like the
fourth industrial revolution, where the virtual and the real world merge. The
employed methodology consists in different phases that began with the review
of the state of the investigation literature, making an exhaustive and methodical
analysis of the proposals, advances, methodologies, future investigations, results
and conclusions obtained. As second, the architecture is proposed and as third,
starting out from the architecture, a mobile application is created, ﬁnishing with
the validation of the architecture, checking the usability of the mobile appli-
cation. The mobile application was validated through a mathematical model that
measures the usability of the application. For that reason, the connection
between the sensor layer and the application layer gets validated. The present
investigation exposes the tools that offer guidelines to the supply chain to be
included in the industry 4.0 and gain competitive advantages.
Keywords: Big Data  Industry 4.0  Internet of things  Cloud computing
Supply chain
1
Introduction
The Industry 4.0 (I4) was born in a high technology strategy project of the German
government in 2011 and it is known as the fourth industrial revolution. I4 allows a
change from the “centralized” production to the “decentralized” production [1], gen-
erating a strategy to be competitive in the future [2], where the products tend to control
their own fabrication process [3]. In addition, it focuses in the optimization of the value
chains through the dynamic and autonomously controlled production, automating the
industries through the data interchange between the links of the supply chain [4].
An integral part of the future factory is the extension and expansion of the supply
chains, breaking the traditional trade barriers and transcending the four walls of just one
installation, in order to work closely with the clients, the suppliers, the industrial
organisms and the academic world. This idea of extended enterprise is fundamental for
the whole concept of industry 4.0, where the collaborative works is seen as a way to
optimize productivity [5].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_64

In this sense, the industry 4.0 is mainly based in the Internet of things (IoT), the Big
Data and the smart manufacturing [6], where the products in process, the components
and the production machines collect and share data in real time that can be used to
predict failures [7], to improve the manufacturing [3], in the decision making process
discovering weaknesses and taking in account the actual situation of the system [8].
The Industry 4.0 along with the IoT can make a great revolution in the management of
the global supply chain [4] by allowing to reach unprecedented operative efﬁciency
levels and accelerating the productivity [9].
The IoT allows the device interconnection for the acquisition of data [10], including
communication and collaboration between them to reach common objectives [11, 12],
like the resource optimization and the costs [13]. The IoT let the modern enterprises
adopt new strategies based on data, and handle the global competitive pressure easily.
However, the adoption of the IoT, increases the total volume data generated, trans-
forming them in Big Data [14], for which they are needed technologies and systems
designed to discover, collect and analyze efﬁciently different kinds of big data and
extract value from them for the organization [15]. Some advantages of the Big Data are
that it can ignore physical and geographical constraints, it can perform the connectivity
and the decentralization of the information resources and can solve the “isolated island
information” problem [16]. Narrowing the gaps between the links of the supply chain.
The cloud computing (CC) has emerged as a new paradigm to give the computation
as a utility service to board different necessities of processing [17]. CC integrates
technologies or architectures like the IoT and the Big Data to achieve a goal, offer a
platform or build a solution [18], it is being employed more and more by the companies
because it provides agility and ﬂexibility to support the supply chain operations [19]. In
the cloud computing, the users or clients see the interface of the application, due to the
use of Internet like a transport unity or a link factor between applications and hardware.
It is possible to access to the cloud computing at any time and place whit an available
internet connection [20].
Likewise, the use of the suppliers of CC has been valuable alternatives with the
objective of accelerate the learning platforms of the machines. The machine learning
(ML) performs tasks that need too much execution time and require platforms able to
decrease this times [21] when realizing a deep understanding of patterns in the data,
which help to make optimal an efﬁcient decisions, improving the performance across
the experience [22]. The main goal of the ML is the creation of a system able to give an
optimal solution when information is entered into it [23].
In summary, the machine learning algorithm starts with the analysis of a “training”
data set, to establish a function able to distinguish individual subjects between groups.
Once this is done, the model can be applied to a new data set, and the precision of the
method can be measured in this new stage [24]. The success of the machine learning in
intelligence tasks is largely due to his capacity to discover a complex structure that was
not speciﬁed before [25].
The data play an important role in the different decisions related with the enterprise
supply chain [4, 26]. The success of any business is based in the efﬁciency of the
supply chain that is responsible of create and maintain the links of different entities in a
business, that are responsible of the acquisition of raw materials for the ﬁnal delivery of
the product [15].
678
J. I. Rodriguez et al.

In this way, this article presents a proposal of an architecture for the supply chain in
the industry 4.0 context, which can be used by each one of the links of the chain to
optimize the processes in real time, besides evaluate the architecture through a mobile
application.
The rest of this work is structured as follows: In Sect. 2 we present the proposed
presentation of the architecture for the supply chain in the context of industry 4.0 and
explain each of the authors that interact in it, continues with Sect. 3 where the con-
nection between the user layers and the application for the medium of the mobile
application “ERP PARA I4” is evaluated, the conclusions and future investigations can
be found in Sect. 4.
2
Proposed Architecture
According to the studied articles, can be noted that de industry 4.0 seeks to make
manufacturing smart, that is to say it is self-conscious, self-optimized and self-
conﬁgured, in order to generate economic beneﬁts in the companies that employ it [3].
When the industry 4.0 is implemented in the supply chain, the traditional production of
scattered cells gets transformed to a fully integrated production ﬂow, automated and
optimized to achieve greater efﬁciency and closer manufacturing relationships whit the
suppliers, producers and clients.
In the Fig. 1 where identiﬁed the actors and elements that interact in the supply
chain in the industry 4.0, which allowed to establish a general framework information
of the links inside the proposed architecture and thus determine the different actors
(enterprise, business, demand, machine learning among others), that carry out the
interactions to make smart the supply chain. When a smart supply chain is generated,
the information ﬂow starts with the obtaining of date through the application of the
internet of things in all the links of the supply chain, going through the cleaning and
selection of date in the Big Data processes, and then sending this information to the
cloud computing, where the machine learning offers the optimal solutions to then
feedback the supply chain, such as demand an offer predictions, planning of distri-
bution routes, forecast of machinery failures, optimal production planning, among
others.
The Fig. 2 shows a proposed architecture for the supply chain in the industry 4.0
context, in which three layers that seek for the integration of the actors and elements are
deﬁned.
Below are described the components of the architecture inside the supply chain in
the industry 4.0 in order to establish an integral proposal that cover all the necessities of
the organizations at the time of being part of the industry 4.0.
Proposal of a Supply Chain Architecture Immersed in the Industry 4.0
679

2.1
Sensor Layer
This layer collects the data of the supply chain through different ways, including sensors
(that are in the different agents of the plant, like machines, locations and operative staff),
the business component (each link of the supply chain generates information in real
Fig. 1. Interaction cycle of actors and elements in the proposed architecture. Source: Authors
Fig. 2. Architecture proposal in the industry 4.0 context. Source: adapted from [27].
680
J. I. Rodriguez et al.

time, the social networks (where the communication and crossing of information
between clients, suppliers and organization is generated), and for last the georeferencing
elements (that take care of the location and collecting of data from the distribution
channels).
2.1.1
Intelligent Sensors
The intelligent sensors collect data and are complemented by actuators that take care of
turn orders into actions. The following sensors were used in the links of the supply chain.
• Radio frequency identiﬁcation (RFID): this technology consist in a RFID tag that
contains the unique identiﬁcation information of the product, a reader collects the
information stored in the tag and a server system stores the data [28]. This kind of
sensors facilitated the identiﬁcation and administration of inventories, the monitoring
of the company assets, the detection of the product along the supply chain (trace-
ability), machinery control (diagnosis and follow-up) and access control by proximity.
• Laser scroll sensor: allowed to make an exact measurement without contact with the
product. The laser light changes the intensity according the dimensions of the object.
• Photoelectric barrier sensor: were used to control the speciﬁc measurements of the
products.
• Load cells: was possible make a control of the weight of the products through this
sensor that transmits the information to be controlled by a weight indicator.
2.1.2
Business
In the supply chain, the communication network was carried out between several
enterprises, factories, suppliers and clients. Each section optimized his conﬁguration in
real time in function of the requirements and the condition of the section associated to
the network, creating the maximum beneﬁt with the exchange of limited resources for
all the links of the chain [3].
2.1.3
Georeferencing
The distribution of raw material and ﬁnished products is a crucial element in the supply
chain, the mission of the distribution is to get the right products to the indicated place,
in the precise moments and whit the required conditions. For the distribution data
collection, were used the following elements in the proposed architecture, that allowed
control in the logistics.
• Geographic information system (SIG): with this tool was obtained the information
about the characteristics of the places, the condition of the routes and the calculation
of optimal routes.
• Global positioning system (GPS): is a free service and a great ally in the logistics,
since it allows to the users to determine their exactly location and travel to other
place. In the proposed architecture GPS was used like a land navigation support.
2.1.4
Social Networks
The increase of the internet use promoted the use of the social networks that are con-
nections between different actors. During the last years, the analysis techniques for social
Proposal of a Supply Chain Architecture Immersed in the Industry 4.0
681

networks have acquired importance in the application and usage of data mining, pro-
viding organizations with a tool for predicting sales and market segmentation. The social
networks date were extracted applying the API Rest y Streaming API techniques [29].
2.1.5
Cloud and Sensor Controller
• Is a storage device that transfers automatically the data from the local storage to the
cloud storage. In the development of the proposed architecture was used the AWS
(Amazon Web Services) service.
• The sensors controllers are devices than store and run the control programs of the
sensors and actuators. For this case, ASC (Autonics Sensors Controllers) were used.
This controllers are compatible with a broad range of sensors.
2.2
Physical and Virtual Databases
All the collected information must be stored, for this, data bases are used, where is
safely to access the information, in the proposed architecture, two kinds of data bases
were used.
• Physical data bases: the data is stored in the enterprise server. From this server, the
date is uploaded to the cloud, what means that information is modiﬁed in real time.
• Virtual Data Bases: a backup of information was made to have access to the
information with no need of an internet connection. In this data bases, only is
possible to consult existing data for the analysis and decision making.
2.3
Network Layer
In this layer is the infrastructure that is stored in an industrial cloud. Here is where
happens the integration between the layers receiving data and supplying information to
the external applications.
2.3.1
Industrial Cloud
In order to access to the information of the supply chain from anywhere in real time, it
is necessary to have an internet connection and access to the AWS server where the
information is stored, to decentralize it and make it available to anyone who requires it.
2.4
Data Transfer
They were used the most common ICT (information and communications technologies)
applied to the supply chain: EDI (Electronic Document Interchange) and VMI (Vendor
Managed Inventory) in order to avoid information leaks and losses along the digital
thread. Equally different data sources were integrated to create a holistic vision of the
process from side to side. Besides, this integration of data includes information about
suppliers and clients. This information is relevant for the adaptation of the manufac-
turing processes [30].
682
J. I. Rodriguez et al.

2.5
User Layer
Through an external application (app), this layer shows the monitoring of raw mate-
rials, equipment failures, quality control, production planning, demand forecast and
optimal distribution routes among others, with the aid of the cloud computing, the Big
Data and the machine learning.
2.5.1
Cloud Computing
Cloud computing offers new ways to use resources when you conﬁgure them
depending on your needs [31]. AML (Amazon Machine Learning) was hired due to it
allows access to the service of software and storage form anywhere through internet.
2.5.2
Machine Learning
The machine learning for this architecture was supported in tools like: Hadoop, Flink
and Kafka, it can be supported in other techniques developed for the organization if it
requires it too.
2.5.3
Application Layer
To enter the developed application, the access is through a user account, authenticating
and authorizing the access. Through the app is possible to manage the information
during its life cycle (since the capture until being deleted). The application counts with
a technical support module too.
2.5.4
Big Data
Through the Big Data, the data collected by the sensors layer was analyzed, selected
and processed. For the development of this architecture, was used a computational
system called Apache Storm, which processes constant data ﬂows in real time.
2.5.5
Data Mining (Demand/Offer)
The exploration of the offer of raw materials data and ﬁnished products demand data
allows a comprehension of the patterns and trends of this data, in order to be processed
through the Big Data. For this architecture was used Apache Storm, that uses predictive
techniques like Bayesian methods and genetic algorithms.
2.5.6
ERP Applications
The ERP systems were used to plan the business resources from the SAP platform. This
systems typically handle the production, logistics, distribution, inventory, shipments,
bills and accounting of the company in a modular way. The developed application
counts with ﬁve modules: (a) demand forecast, (b) failure reports, (c) production
reports, (d) suppliers reports and (e) routing.
3
Proposal Revalidation
From the ﬁrst validation made to the proposal, the changes suggested by surveyed
experts were made in the user layer of the mobile application.
Proposal of a Supply Chain Architecture Immersed in the Industry 4.0
683

The architecture revalidation, was made with a test in the user layer, in which the
modiﬁed and is shown in the Fig. 3, called ERP FOR I4, applying the “usability
measurement of mobile applications model, through the attribute analysis and usability
evaluation methods” [32].
To re-evaluate the model, 7 experts with academic knowledge in supply chains and
SAP and 8 clients involved with the organization supply chain were needed, besides to
establishing the attributes, sub attributes and heuristics that are shown below:
Attributes: (a) the user understand the app functionality, (b) the proposed app helps
the user to interact with the links of the supply chain, (c) the app works in an agile way,
(d) the app handles information in real time, (e) the information provided by the app
helps the links of the supply chain.
Sub attributes: (a) the app is easy to understand, (b) the app is easy to handle, (c) the
obtained information is easy to analyze.
Heuristics: (a) the proposed app offers a fast way to get supply chain information,
(b) companies can beneﬁt from the app and (c) internet of things can beneﬁt from the app.
Once the model with the previously described properties is applied, a result of 96%
of usability is obtained, like is shown in the table below.
Fig. 3. User application interface based on the proposed architecture. Source: authors.
684
J. I. Rodriguez et al.

Whit the obtained data of the model in the mobile application, it is evident that the
ERP FOR I4 application meets the necessary parameters to validate the proposed
architecture in the interconnection of the sensor and application layers, making the
supply chain integrate to the industry 4.0 successfully.
In addition, the usability of the application grown 4% with the implementation of
the changes made to the mobile applications.
The mobile applications usability model used to evaluate the ERP FOR I4 mobile
application, gave a new usability result of usability of 96%, so it can be said that the
proposed architecture fulﬁlls his function of integrate the links of the supply chain with
the industry 4.0.
4
Conclusions and Future Work
As result of the presented investigation, it can be concluded that exists a positive
relation between the proposed architecture and the efﬁciency of the supply chain, due to
two main factors; the ﬁrst one is related with the real time interaction between the links
of the supply chain supported in the cloud computing. The second one factor is the
machine learning that allows the making of optimal and efﬁcient decisions.
The efforts made by the companies in order to create competitive advantages can be
reinforced due the immersion in the industry 4.0, however, the organizations must take
in account that the inclusion in the in industrial revolution brings with it changes in
technologies used for the collecting, analysis and transformation of the data, and this
changes mean an inversion that companies must make.
After the validation of the proposal, it was determined that the use of the developed
mobile application provides an initiative to the organizations to be part of the industry
4.0, due to the interface presented is suitable for the revision of the desired information
from any point of view of the supply chain in real time.
For future works it must be taken in account the security of the information through
encryptions that allow the conﬁdentiality of the data, protocols should be established
for the access to the information and in addition technologies should be standardized in
order to allow the integration of all the elements through different platforms. Also for
future work can be done cases of study in companies of different sectors, experiments
and simulations.
Attribute
Validation 1
Validation 2
Attribute a
95%
95%
Attribute b
88%
93%
Attribute c
94%
98%
Attribute d
91%
97%
Attribute e
87%
96%
Usability
91%
96%
Source: Authors.
Proposal of a Supply Chain Architecture Immersed in the Industry 4.0
685

References
1. Ang, J., Goh, C., Saldivar, A., Li, Y.: Energy-efﬁcient through-life smart design,
manufacturing and operation of ships in an Industry 4.0 environment. Energies 10(5), 610
(2017)
2. Mrugalska, B., Wyrwicka, M.K.: Towards lean production in Industry 4.0. Procedia Eng.
182, 466–473 (2017)
3. Qin, J., Liu, Y., Grosvenor, R.: A categorical framework of manufacturing for Industry 4.0
and beyond. Procedia CIRP 52, 173–178 (2016)
4. Jayaram, A.: Lean six sigma proposal for global supply chain management using Industry
4.0 and IIoT. In: 2016 2nd International Conference on Contemporary Computing and
Informatics, pp. 89–94 (2016)
5. International Electrotechnical Commission, Factory of the future, White Paper Future
Factory, pp. 44–47 (2015). http://www.qualitymag.com/articles/93484-stepping-up-to-the-
factory-of-the-future
6. Díez, V., Arriola, A., Val, I., Vélez, M.: Validation of RF communication systems for
Industry 4.0 through channel modeling and emulation. In: 2017 IEEE International
Workshop of Electronics, Control, Measurement, Signals and their Application to
Mechatronics (ECMSM), pp. 1–6 (2017)
7. Spendla, L., Kebisek, M., Tanuska, P., Hrcka, L.: Concept of predictive maintenance of
production systems in accordance with Industry 4.0. In: Proceedings of the 2017 IEEE 15th
International Symposium on Applied Machine Intelligence and Informatics, SAMI 2017,
pp. 405–410 (2017)
8. Meissner, H., Ilsen, R., Aurich, J.C.: Analysis of control architectures in the context of
Industry 4.0. Procedia CIRP 62, 165–169 (2017)
9. Thames, L., Schaefer, D.: Software-deﬁned cloud manufacturing for Industry 4.0.
Procedia CIRP 52, 12–17 (2016)
10. Farooq, M.J., Zhu, Q.: Secure and reconﬁgurable network design for critical information
dissemination in the Internet of Battleﬁeld Things (IoBT). In: 2017 15th International
Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks
(WiOpt), Paris, pp. 1–8 (2017)
11. Alkhabbas, F., Spalazzese, R., Davidsson, P.: Emergent conﬁgurations in the internet of
things as system of systems. In: 2017 IEEE/ACM Joint 5th International Workshop on
Software Engineering for Systems-of-Systems and 11th Workshop on Distributed Software
Development, Software Ecosystems and Systems-of-Systems (JSOS), pp. 70–71 (2017)
12. Iglesias-Urkia, M., Orive, A., Urbieta, A.: Analysis of CoAP implementations for industrial
internet of things: a survey. In: The 8th International Conference on Ambient Systems,
Networks and Technologies (ANT 2017), no. 2016 (2017)
13. Saarikko, T., Westergren, U.H., Blomquist, T.: The internet of things: are you ready for
what’s coming? Bus. Horiz. 60, 667–676 (2017). http://www.sciencedirect.com/science/
article/pii/S000768131730068X
14. Mourtzis, D., Vlachou, E., Milas, N.: Industrial big data as a result of iot adoption in
manufacturing. Procedia CIRP 55, 290–295 (2016)
15. Ghosh, D.: Big data in logistics and supply chain management - a rethinking step. In: 2015
International Symposium on Advanced Computing and Communication, pp. 168–173
(2015)
16. Khan, M., Wu, X., Xu, X., Dou, W.: Big data challenges and opportunities in the hype of
Industry 4.0. In: 2017 IEEE International Conference on Communications (ICC), pp. 1–6.
IEEE, May 2017
686
J. I. Rodriguez et al.

17. Khan, N., Al-Yasiri, A.: Identifying cloud security threats to strengthen cloud computing
adoption framework. Procedia Comput. Sci. 94, 485–490 (2016)
18. Raza, M.H., Adenola, A.F., Nafarieh, A., Robertson, W.: The slow adoption of cloud
computing and IT workforce. Procedia Comput. Sci. 52(1), 1114–1119 (2015). http://www.
sciencedirect.com/science/article/pii/S187705091500928X
19. Choi, T.-M., Shen, B.: A system of systems framework for sustainable fashion supply chain
management in the big data era. In: 2016 IEEE 14th International Conference on Industrial
Informatics, pp. 902–908 (2016)
20. Hussain, S.A., Fatima, M., Saeed, A., Raza, I., Shahzad, R.K.: Multilevel classiﬁcation of
security concerns in cloud computing. Appl. Comput. Inform. 13(1), 57–65 (2017)
21. Pop, D.: Machine Learning and Cloud Computing: Survey of Distributed and SaaS
Solutions, Institute e-Austria Timisoara, Technical report 1 (2012). https://arxiv.org/pdf/
1603.08767.pdf
22. Talwar, A., Kumar, Y.: Machine learning: an artiﬁcial intelligence methodology. Int. J. Eng.
Comput. Sci. 2(12), 3400–3405 (2013). http://www.ijecs.in/issue/v2-i12/11%20ijecs.pdf
23. Stăncioiu, A.: The Fourth Industrial Revolution, no. 1, pp. 74–79 (2017). http://bit.ly/
2wncneN
24. Lasi, H., Fettke, P., Kemper, H.-G., Feld, T., Hoffmann, M.: Industry 4.0. Bus. Inf. Syst. Eng.
6(4), 239 (2014). https://www.cgi.com/en/white-paper/Industry-4-making-your-business-
more-competitive
25. Di Deco Sampedro, J., Díaz García, J.: Estudio y aplicación de técnicas de machine learning
orientadas al ámbito médico: estimación y explicación de predicciones individuales, p. 103
(2012). https://repositorio.uam.es/handle/10486/12100
26. Chaouni Benabdellah, A., Benghabrit, A., Bouhaddou, I., Zemmouri, E.M.: Big data for
supply chain management: opportunities and challenges. Int. J. Sci. Eng. Res. 7(11),
20–25
(2016).
https://www.ijser.org/researchpaper/Big-Data-for-Supply-Chain-Manage-
ment-Opportunities-and-Challenges.pdf
27. Manuel, J., Lovelle, C., Enrique, C., Marín, M.: Metamodelo para la integración de la
internet of things y redes sociales (2014). http://di002.edv.uniovi.es/*cueva/investigacion/
tesis/Tesis-JoseIgnacio.pdf
28. Ang, J.H., Goh, C., Li, Y.: Smart design for ships in a smart product through-life and
Industry 4.0 environment. In: 2016 IEEE Congress on Evolutionary Computation, CEC
2016, pp. 5301–5308 (2016)
29. Turri, A.M., Smith, R.J., Kopp, S.W.: Privacy and RFID technology: a review of regulatory
efforts. J. Consum. Aff. 51(2), 329–354 (2017). http://onlinelibrary.wiley.com/doi/10.1111/
joca.12133/abstract
30. Asensio Blasco, E.: Aplicación de técnicas de minería de datos en redes sociales/web, p. 50
(2015). https://riunet.upv.es/handle/10251/56102
31. Ularu, E.G., Puican, F.C., Suciu, G., Vulpe, A., Todoran, G.: Mobile computing and cloud
maturity-introducing machine learning for ERP conﬁguration automation. Inform. Econ. 17
(1), 40 (2013)
32. Molano, J.I.R., Yara, E.S., Garcia, L.K.J.: Model for measuring usability of survey mobile
apps, by analysis of usability evaluation methods and attributes. In: 2015 10th Iberian
Conference on Information Systems and Technologies, CISTI 2015 (2015). http://ieeexplore.
ieee.org/document/7170420/
Proposal of a Supply Chain Architecture Immersed in the Industry 4.0
687

Open Source Web Software Architecture
Components for Geographic Information
Systems in the Last 5 Years: A Systematic
Mapping Study
Alvaro Uyaguari1(&), Edison Espinosa-Gallardo1,
Santiago P. Jácome-Guerrero1, Patricio Espinel1,
Cristian F. Cabezas1, Gloria I. Arias Almeida1,
and Frankz Alberto Carrera Calderón2
1 Universidad de las Fuerzas Armadas ESPE, Av. General Rumiñahui s/n,
171-5-231B, Sangolquí, Ecuador
{aduyaguari,egespinosa1,psjacome,jlcarrillo,
gpespenel,cacabezas5,giareas}@espe.edu.ec
2 Universidad de las Universidad Regional Autónoma de Los
Andes UNIANDES Ambato, Ambato, Ecuador
frankzcarrera@uniandes.edu.ec
Abstract. Geographic information system (GIS) is an organized integration of
hardware, software and geographic data designed to recover, store, manipulate,
analyze and to visualize geographically referenced information. In the software
architecture for enterprise systems, it is essential the paradigm of the component
orientation, which can be mature tools, modules, libraries or complete systems.
This research shows a mapping study which objective is know the state of the art
of the software architecture components for the GIS Web open source devel-
opment. The results show that 74.19% of are integrations of existing open
source components, 16.13% add tools to the software architecture, and 9.68%
integrate new methods to solve or improve algorithms that are part of the
architecture. This clearly indicates that further research is needed to experience
new tools and/or new algorithms to solve or improve existing ones.
Keywords: Web software architectures  Open source
Geographic information systems  Systematic mapping study  Components
1
Introduction
Geospatial information is the result of processing data from objects and facts with spatial
reference. It is currently an essential component of the national and international infras-
tructure for the information society. It is estimated that about 80 per cent of our daily
decisions depend on this [1], which is processed by Geographic information systems
(GIS). These are formed by a set of tools that integrates and relates various components
(users, hardware, software and processes) [2], which allow the acquisition, storage,
manipulation, analysis, visualization and dissemination of geospatial information [2].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_65

Facilitating the incorporation of social-cultural, economic and environmental aspects,
which lead to the decision-making in a more effective way [2].
GIS are characterized as closed systems, monolithic and proprietary, currently
integrate several components that allow versatile applications that work in a transparent
way. These use the World Wide Web (WWW or Web) as the primary connectivity
medium in computing distributed in it in general and in the domain of GEO processing
speciﬁcally [3, 4]. GIS focused on the data and tools deployed in a client-server
architecture, evolving to a Web service model [5], where the Web architecture not only
focuses on delivering the data but also the functionality of geo-processing (compo-
nents). This allowed to satisfy ubiquity, ease of access, ﬂexibility and risk reduction of
insulation and obsolescence. Creating broader and more comprehensive services and/or
applications [6]. Extending GIS beneﬁts widely on the Web [7].
The development of a GIS software architecture is based on speciﬁcations that
allow the management of geographic information to be regulated [8]. For example, the
Open geospatial Consortium (OGC) provides speciﬁcations for aviation, infrastructure
construction, 3d modeling, business intelligence, military defense, emergency and
disaster response management, energy, geosciences and the environment, spatial data
infrastructure and governability, mobile Internet and localization services, Web sen-
sors, research [9].
The objective of this research is to carry out a systematic mapping study (EMS), on
the open source Web software architecture for geographic information systems, which
allowed from the analysis of a set of articles, to establish the problematic and contri-
butions on the research posed.
The article is structured as follows: Sect. 2 describes the research question and the
proposed methodology. In Sect. 3, mapping planning is detailed. In Sect. 4, the exe-
cution of systematic mapping is performed. The analysis and discussion of results are
detailed in Sect. 5. Situations that threaten the validity of this investigation are pre-
sented in Sect. 6. To conclude, Sect. 7 establishes the conclusions of the investigation.
2
Research Question and Proposed Methodology
We direct the scope of this mapping to the Web technologies of geographic information
systems from the year 2012 to the month of June of 2017. The research question that is
raised is as follows:
R1Q1: What types of research and contributions have been made in the last 5 years
in the Open Source GIS Web software architecture?
The R1Q1 seeks to map the distribution of the types of research and contribution
carried out in the last 5 years in architectures of Web Open Source Computing solu-
tions that manage geographic information. A MSL (Systematic Mapping Study) is
performed covering the planning and execution activities to identify the quantity and
type of publications available on this topic. According to [10], the steps to carry out a
systematic mapping are: the deﬁnition of the research question, the search for related
articles, the analysis of results, the selection of articles, the keywording of the summary
and the extraction and mapping of the data.
Open Source Web Software Architecture Components
689

In each of these steps some results are produced that ends with the map of the
related articles on the topic raised. Article searches include a set of rules, activities,
elements and instruments, which formalize and support the activities of:
1. Selecting Item Sources.
2. Deﬁnition of the search string.
3. Deﬁnition of inclusion and exclusion criteria.
4. Preselection of articles.
5. Selection of items.
6. Data extraction of the articles.
Selecting item sources requires a set of rules to ensure the quality and quantity of
items. The rules are basically: deﬁnition of criteria for selecting sources and search
engines. In the deﬁnition of search strings, the search strings used to answer the
research question are structured. As proposed [11], the chain of search is constructed
from the question of investigation posed with the strategy PICOC and to reﬁne the
question of investigation applies the strategies proposed by [12]. The deﬁnition of the
inclusion and exclusion criteria is previously established according to a set of criteria
used during the search execution activity for the pre-selection and selection of articles.
In the pre-selection of articles we proceed to the reading of some sections of the article,
in order to identify in the text terms used in the chain of search. In the selection of
articles a complete reading of the article is made, applying the terms of the search
string. Finally, for the extraction of data from the articles, an instrument is created that
allows to collect the information related to the thematic.
3
Systematic Mapping Planning
The planning constitutes the support for the activities of organizing, executing and
delivering results of the search of articles. Below are the activities, criteria instruments
among others to carry out this activity.
The following rules were implemented for selecting item sources:
(a) Sources selection criteria:
• Articles originated in reliable sources such as magazines, scientiﬁc journals,
international conferences and book chapters.
(b) Language:
• Articles written in the English language.
(c) Search engines.
• searched articles in Scopus and IEEE.
In the activity deﬁnition of the search string is established the chain used to answer
the research question, based on the strategy PICOC: Population, Intervention, Com-
parison, Output, context [12]; Leaving the research question raised as follows:
690
A. Uyaguari et al.

• Population: Geographic information systems.
• Intervention: Software architecture.
• Context 1: Open Software.
• Context 2: Web.
• Comparison: Open source Software used in architecture.
• Output: Statistics of software components used in the Web architecture of geo-
graphic information systems.
Deﬁned the terms according to PICOC, population, intervention and context, the
structure of the chain was deﬁned. We use the term geographic information Systems
Web Open Source for the population and their corresponding synonyms according to
[9]. Software architecture was used for the intervention [8]. For the context 1 the term
open source was used, whereas for the context 2 it was employed “Web GIS” because
in a random sampling, all the articles related to the publication of geographical
information on the Web included the keyword “WebGIS or its synonym Web-GIS”.
But “Program architecture” was not included because it did not affect any aspect to
incorporate or exclude investigations.
Table 1 details the synonyms related to the terms of the research question.
At the time of establishing the chain four search substrings were established, using
logical operators between the components of PICOC to relate the terms and their
corresponding synonyms.
Table 2 shows the activities and inputs of the planning for the execution of the
search for articles related to the study object.
The proposed research question determines the criteria for inclusion and exclusion,
these criteria are used in the execution of the search for pre-selection and selection of
articles.
In the article pre-selection activity, the inclusion criteria allow you to incorporate
articles that evidence the use of search string terms in sections such as: Job title,
summary, and keywords. Otherwise the exclusion criteria allow you to discard the
articles in which the terms of the search string are used, but which do not correspond to
the context of geographic information systems Web Open Source. This activity gen-
erates a list containing the primary search studies.
In the activity of selection of articles are taken the criteria of inclusion to incor-
porate the articles in which evidence activities on the thematic of geographic infor-
mation systems Web Open Source. On the other hand, the exclusion criteria are applied
to discard the articles in which no geographic information systems are detailed Web
Open Source, for this it is done a complete reading of the articles.
Table 1. Synonyms and related search terms of articles PICOC.
PICOC
Related search terms
Synonyms
Population
Geographic information systems GIS
Intervention Software Architecture
Program architecture
Context 1
Open Source
Context 2
WebGIS
Web-GIS
Open Source Web Software Architecture Components
691

4
Execution of Systematic Mapping
In this activity are carried out the planned tasks, to ﬁnd articles related to topic raised, it
is started by entering the string (Resulting_String) in the digital bases IEEE and
Scopus. The Table 3 shows the results of execution of Resulting_String, ﬁltering the
results through the title, abstract and keywords corresponding, We have obtained 25
articles in Scopus and 15 articles in IEEE, The inclusion and exclusion criteria are
applied for preselection of articles, We have obtained 19 articles from Scopus and 12
from IEEE.
It is important to note that the column precision of the Table 3 is a percentage
relation between the number of pre-selected articles versus the number of located
articles.
After performing a previous analysis, we have excluded those articles that do not
show in their content, the application of software architecture Open Source for
Table 2. Planning activities and inputs.
No.
Activity
Planning inputs
1
Select Sources for Item
search
Sources: Magazines, book chapters, magazine articles.
Language: English.
Search engines: Scopus and IEEE
2
Deﬁnition of inclusion
and exclusion criteria
What types of research and contributions have been
made over the past 5 years in the Open Source GIS Web
software architecture?
3
Pre-selection of articles
Inclusion criteria: Evidence of the use of the terms of
the search string in the content of the article.
Exclusion criterion: No evidence of the use of the terms
of the search string in the content of the article
4
Selection of articles
Inclusion criteria: Evidence of the application of
geographic information systems Web Open Source.
Exclusion criteria: it is not evident from the application
of geographic information systems Web Open Source
5
Deﬁnition of the search
string
Resulting string = “geographic Information systems” or
“GIS”) AND “software architecture” AND “open
source” AND “Web-GIS” OR “WebGIS”)
6
Data extraction
Basic information of the selected articles
Table 3. Localized articles, pre-selected and precision in relation to the search string
Resulting_String
Platform Localized articles Pre-selected articles Precision
Scopus
25
19
61,29%
IEEE
17
12
38,71%
Total
42
31
692
A. Uyaguari et al.

Geographic Information Systems. The selected articles are detailed in reference section
of publication.
The revision of 31 primary studies have allowed their classiﬁcation according the
type of investigation, it shows in the Table 4. While the Table 5 shows the contribution
of investigation to establish the software, architecture of Geographic Information
Systems with open source tools.
From the information of the Table 4, the frequency of publication of every type of
investigation was calculated. We have used a bubble diagram to represent the inter-
connected frequencies. It shows in Fig. 1. It is an x-y dispersion diagram with bubbles
in intersection of the categories.
Table 4. Types of investigation.
Type of investigation Cases Percentage
Case study
27
87,10%
Experiment
3
9,68%
Action research
1
3,23%
Total
31
100%
Table 5. Research Contributions
Contribution
Number Percentage
Tool
5
16,13%
Tool and method 23
74,19%
Method
3
9,68%
Total
31
100%
Fig. 1. Map of the years of publication according the types of investigation in the Web GIS
Open Source Software architecture
Open Source Web Software Architecture Components
693

In Fig. 2 is evident that the major part of contributions in the study period, these are
tools and methods of implementing architectures based in reusing of open source
components and following a method established in the study.
5
Analysis and Discussion of Results
RIQ1: What types of investigations and contributions have been made in the last 5
years in the GIS Web Open Source Software Architecture?
As speciﬁed in the Table 4, 87,10% of research types in the primary studies are
study cases that in exploratory way, the industrial applied projects are studied, 9,68%
perform experiments with new Open Source components or tools, 3.23% perform an
action investigation to improve the algorithms of processing of geographic information
shown in Fig. 1.
In that ﬁgure, it does not include 2017 because the mapping has as cut-off date of
month of June 2017 then the year would not be complete and could distort the statistics.
In addition, the Fig. 1 shows that in 2013 the mayor number of research was
performed and exist an incremental tendency of investigations in ambit of this article
mostly investigations are Case Studies.
The Table 5 shows that 74,19% of studies contribute to exploring architectures based
on integration of Open source components, including an own development component in
some cases and according to a method according the software application area, 16,13%
includes in studied architectures, new components and owns for an exploratory or
experiment, 9,68% shows own methods to study and improve algorithms.
We analyzed the principal components of open source architectures explored in the
31 primary research, in the following ambits:
– Libraries of Web Clients.
– Servers of maps.
– Databases.
– Programming Language Back end.
Fig. 2. Evolution in the time of contributions
694
A. Uyaguari et al.

The Table 6 shows that the most studied libraries in software architecture are Open
Layers java script libraries and in many cases, combination with GeoExt java script
libraries.
The Table 7 shows that the GeoServer map server is the most used to be part of
architectures that integrate open source software components, for the rendering of
geographic information and generation of Web geographic services. While, the Table 8
shows that Php language with 22.58% is the most used back end language in the
studied of revised architectures, next we have Java language with 12%. The Table 9
shows that Postgresql Database, with their especial extension PostGIS, is the principal
tool in Open Source databases, for the storage and management of geographic
information.
Table 6. Libraries of Web Clients
Libraries
Amount Percentage%
Open layers
9
29,03
GeoExt y open layers
5
16,13
Adobe ﬂex
2
6,45
Leaftlet
2
6,45
Others
8
25,81
Not speciﬁed
3
9,68
Not required
2
6,45
Total
31
100
Table 7. Map Servers
Map servers
Amount Percentage%
GeoServer
16
51,61
Mapserver
8
25,81
Others
4
12,90
Not Required 3
9,68
Total
31
100
Table 8. Back end programming language
Language
Amount Percentage%
Java
4
12,90
Python
2
6,45
Php
7
22,58
.Net
2
6,45
Others
2
6,45
Not required o not speciﬁed 14
45,16
Total
31
100
Open Source Web Software Architecture Components
695

Finally, other Open source components are studied too, but in projects oriented to
speciﬁc thematic, for example geometric networks, road networks, etc.
6
Threats to Validity
The principal threats to validity of investigation are related to the bias of the selection
of studies, in other cases to possible inaccuracies in the extraction of the data; and, it
could be that we have not been able to locate all the primary studies. However,
although it is not possible to reach a full validity, the following actions have been
carried out to minimize possible inaccuracies:
We have established a proven and detailed methodology in the section “Question of
investigation and methodology” to establish the search string of the articles.
We have performed a deﬁned process of inclusion and exclusion, this process have
been analyzed for informatics experts and specialists in technologies of geographic
information systems, through the strategy of use of keywords to develop our own
scheme of classiﬁcation, process that have performed reading the abstracts, the content
of article, and searching keywords and concepts that reﬂect the contribution of the
article, that helped to deﬁne a set of categories that is representative of research context.
7
Conclusions
With the EMS presented in this article, we have identiﬁed the principal investigations
of Open Source software architecture for geographic information systems, published
from 2012 to June 2017, in digital libraries IEEE, Xplore and Scopus. This type of
mapping study is basic to facilitate the opening of investigation camp to new
researchers, because their results may be used to identify trends and gaps in the
investigation showing the enormous possibilities of working for community of
researchers of Web technologies of geographic information systems, in this context, the
investigation of software architectures of open source systems show that 74.19% are
integrations of existent Open source components, according to an established method
in the study with a development component but oriented to a speciﬁc requirement,
16.13% adds tools to the central axis of the GIS Web architecture and 9.68% integrates
new methods and algorithms to improve different aspects of architecture.
Table 9. Databases.
Databases
Amount Percentage%
Postgresql con postGIS 24
77,42
Sql express
1
3,23
Not required
2
6,45
Not speciﬁed
2
6,45
Others
1
3,23
Total
31
100
696
A. Uyaguari et al.

Hence, the need to carry out new research aimed at evaluating and improving the
components of the Open Source software architecture of a geographic information
system in a Web environment.
In addition, in the present article (Figs. 1 and 2) the evolution in the time of the
investigations and contributions in the studied area is synthesized, through which it is
evident that in the year 2013 more research was done, but However, it is also evident
that there is a clear incremental tendency of the execution of scientiﬁc studies in the
area of the present article.
Finally, the present study contributes with a systematization of components and
software tools used in the research of architectures carried out during the last 5 years,
which can be very useful not only to researchers, but also to professionals related to the
geographic information systems technologies.
References
1. Calderón, F.C.: UNIANDES y la Infraestructura de Datos Espaciales (2017)
2. IGAC, Fundamentos de Sistemas de Información Geográﬁca. CIAF (2008)
3. Anderson, G., Moreno-Sanchez, R.: Building Web-based spatial information solutions
around open speciﬁcations and open source software. Trans. GIS 7(4), 447–466 (2003)
4. Hecht, L.: Insist on interoperability. GeoWorld 15(4), 22–23 (2002)
5. Dangermond, J.: Web services and GIS. Geospatial Solut. 12(7), 56 (2002)
6. Hecht, L.: Web services are the future of geo-processing. GeoWorld 15(6), 23–24 (2002)
7. Ye, Z.: A Web-based geographical information system prototype on Portuguese traditional
food products (Doctoral dissertation) (2009)
8. González, J.T., Yáñez, C.Y.: Sistemas de Información Medio Ambiental. Netbiblo (2005)
9. Lasanta, R., Bueyo, M.: Diseño y desarrollo de un cliente Web Processing Service
(WPS) para gvSIG (2010)
10. Petersen, K., Feldt, R., Mujtaba, S., Mattsson, M.: Systematic mapping studies in software
engineering. In: 12th International Conference on Evaluation and Assessment in Software
Engineering, vol. 17, p. 1 (2008)
11. Petticrew, M., Roberts, H.: Systematic Reviews in the Social Sciences: A Practical Guide.
Wiley-Blackwell (2008)
12. Beecham, S., Baddoo, N., Hall, T., Robinson, H., Sharp, H.: Protocol for a systematic
literature review of motivation in software engineering. University of Hertfordshire (2006)
13. Bandyophadyay, M., Singh, M.P., Singh, V.: Integrated visualization of distributed spatial
databases: an open source Web-GIS approach. In: 1st International Conference on Recent
Advances in Information Technology (2012)
14. Chen, Z., Chen, N., Yang, C., Di, L.: Cloud computing enabled web processing service for
earth observation data processing. IEEE J. Sel. Topics Appl. Earth Observations Remote
Sensing 5(6), 1637–1649 (2012)
15. Nakayama, Y., Mori, S.: FOSS4G based mobile Web-GIS for ﬁeld survey in natural
environmental studies. In: 9th International Conference on Ubiquitous Intelligence and
Computing and 9th International Conference on Autonomic and Trusted Computing (2012)
16. Françoso, M.T., Costa, D.C., Valin, M.M., Amarante, R.R.: Free software for development
of Web GIS in tourism accessibility. Appl. Mech. Mater. 256–259, 2953–2956 (2013)
17. Zhou, G., Lin, J., Zheng, W.: A Web-based geographical information system for crime
mapping and decision support. In: Computational Problem-Solving (ICCP) (2012)
Open Source Web Software Architecture Components
697

18. Kehe, W., Jiabo, C., Wei, C., Chi, Z.: Design and implementation of open source WebGIS
client framework based on ﬂex. In: International Conference on Computational and
Information Sciences (2013)
19. Zheng, J., Zhang, Z., Ciepłuch, B., Winstanley, A.C., Mooney, P., Jacob, R.: A
PostGIS-based pedestrian way ﬁnding module using OpenStreetMap data. In: 2013 21st
International Conference Geoinformatics (GEOINFORMATICS) (2013)
20. Porta, J., Parapar, J., García, P., Fernandez, G., Tourino, J., Doallo, R., Onega, F., Sant, I.,
Díaz, P., Miranda, D., Crecente, R.: Web-GIS tool for the management of rural land markets:
application to the Land Bank of Galicia (NWSpain). Earth Sci. Inform. 6, 209–226 (2013)
21. Gkatzoﬂias, D., Mellios, G., Samaras, Z.: Development of a Web GIS application for
emissions inventory spatial allocation based on open source software tools. Comput. Geosci.
52(2013), 21–33 (2013)
22. Balbo, S., Boccardo, P., Dalmasso, S., Pasquali, P.: A public platform for geospatial data
sharing for disaster risk management. Int. Arch. Photogrammetry Remote Sensing Spatial
Inf. Sci. XL-5/W(3), 189–195 (2013)
23. Bessaa, B., Aissa, M.B., Amara, R., Aissa, A.B: Spatial indexing of static maps for
navigation in online GIS: application for tourism Web GIS. Int. J. Comput. Appl. Technol.
47(2/3), 189–197 (2013)
24. Mangiameli, M., Mussumeci, G.: Real time integration of ﬁeld data into a GIS platform for
the management of hydrological emergencies. Int. Arch. Photogrammetry Remote Sensing
Spatial Inf. Sci. XL-5/W(3), 153–168 (2013)
25. Cinnirella, S., D’Amore, F., Mazzetti, P., Nativi,S., Pirrone, N.: A spatial data infrastructure
for the global mercury observation system. In: E3S Web of Conferences 28001 (2013)
26. Moshi, M., Nahar, N., Rahman, R., Sakib, K.: MapBeing: an architecture for manipulating
and publishing vector data in Web based geographic information system. In: 2014 8th
International Conference on Software, Knowledge, Information Management and Applica-
tions (SKIMA) (2014)
27. Zavala-Romero, O., Ahmed, A., Chassignet, E.P., Zavala-Hidalgo, J., Eguiarte, A.F.,
Meyer-Baese, A.: An open source Java Web application to build self-contained Web GIS
sites. Environ. Model. Softw. 62, 210e220 (2014)
28. Delipetrev, B., Jonoski, A., Solomatine, D.P.: Development of a Web application for water
resources based on open source software. Comput. Geosci. 62, 35–42 (2014)
29. Duan, M., Yang, Y., Yang, H.: Educational geographic information system based on
WebGIS. In: 2013 International Conference on Information Science and Cloud Computing
Companion (2014)
30. Yusoff, N.M.R.N., Shafri, H.Z.M., Muniandy, R., Wali, I.M.: Development of obstacle
avoidance technique in Web-based geographic information system for trafﬁc management
using open source software. J. Comput. Sci. (2014)
31. Changyong, D., Huadong, G., Chunming, H., Ming, L.: An open source software and
Web-GIS based platform for airborne SAR remote sensing data management, distribution
and sharing. In: 35th International Symposium on Remote Sensing of Environment
(ISRSE35) (2015)
32. Aye, Z.C., Jaboyedoff, M., Derron, M.-H., van Westen, C.J.: Prototype of a Web-based
participative decision support platform in natural hazards and risk management. ISPRS Int.
J. GeoInf. 4, 1201–1224 (2015)
33. Han, G., Chen, J., He, C., Li, S., Wu, H., Liao, A., Peng, S.: A Web-based system for
supporting global land cover data production. ISPRS J. Photogrammetry Remote Sensing
103, 66–80 (2014)
698
A. Uyaguari et al.

34. Knörchen, A., Ketzler, G., Schneider, C.: Implementation of a near-real time cross-border
Web-mapping platform on airborne particulate matter (PM) concentration with open-source
software. Comput. Geosci. 74, 13–26 (2015)
35. Barik, R.K., Das, P.K., Lenka, R.K.: Development and implementation of SOA based SDI
model for tourism information infrastructure management Web services. In: 2016 6th
International Conference on Cloud System and Big Data Engineering (Conﬂuence) (2016)
36. Nizamuddin, Hizir, Ardiansyah, Pertiwi, D., Handayani, P.: Development of Web GIS for
information of renewable energy in Aceh Province after rehabilitation and reconstruction
process. IOP Conf. Ser. Earth Environ. Sci. 56(conference 1), 012016 (2016)
37. Sakhare, P., Mascarnes, S., Chaudhari, A.: Development of WebGIS framework for Indian
technical institutes using open source GIS tools. In: 2015 International Conference on
Computer, Communication and Control (IC4) (2015)
38. Brovelli, M.A., Fahl, F.C., Minghini, M., Molinari, M.E.: Land user and land cover maps of
Europe: a WebGIS platform. Int. Arch. Photogrammetry, Remote Sensing Spatial Inf. Sci.
XLI(B7), 913–917 (2016)
39. Mattavelli, M., Strigaro, D., Frigerio, I., Locci, F., Melis, M.T., De AMICIS, M.: The IDB:
an ice core geodatabase for paleoclimatic and glaciological analyses. Revista GEOGRAFIA
FISICA E DINAMICA QUATERNARIA artículo 1 (2016)
40. Kanthi, N.S., Purwanto, T.H.: Application of OpenStreetMap (OSM) to support the mapping
village in Indonesia. IOP Conf. Ser. Earth Environ. Sci. 47(1), 012003 (2016)
41. Lv, D., Ying, X., Gao, X., Tao, W., Cui, Y., Hua, T.: A WebGIS platform design and
implementation based on open source GIS middleware. In: 2016 24th International
Conference on Geoinformatics (2016)
42. Duruz, S., Flury, C., Matasci, G., Joerin, F., Widmer, I., Joost, S.: A WebGIS platform for
the monitoring of farm animal genetic resources (GENMON) (2017)
43. Nizamuddin, Hizir, Ardiansyah, Pertiwi, D., Handayani, P.: Development of Web GIS for
information of renewable energy in Aceh Province after rehabilitation and reconstruction
process. IOP Conf. Ser. Earth Environ. Sci. 56(conference 1), 012016 (2017)
Open Source Web Software Architecture Components
699

Ethnographic Study on Practices
of the Software Development Industry in Chile
Dante Carrizo(&) and Andrés Alfaro
Computer Science Department, University of Atacama,
Ave. Copayapu 485 Copiapó, Chile
{dante.carrizo,andres.alfaro}@uda.cl
Abstract. Research in software engineering generates results that are supposed
to be useful to practitioners. However, there is little evidence of the degree of
adoption by the development industry. This article presents an investigation
about the practices of the software industry in Chile to obtain information on its
processes, tools and methods, as well as factors of success, risk and failure in the
development of a software product. For this, an ethnographic study was carried
out by making short stays in a set of companies dedicated to the development of
software. As a result, it was obtained that the companies perform their projects
through a set of good practices, and identifying the client as a fundamental factor
in the development of their projects. The study allowed knowing the gaps
between national and international practices as a reference. This can help guide
future industry methodological strategies.
Keywords: Software development  Success and failure factors
Industry practices  Development methodologies  Techniques and tools
1
Introduction
The software industry worldwide has taken great importance in recent years. This
phenomenon created a new economic activity around the software industry that today
is considered promising worldwide [1, 9]. Chile is no stranger to this reality. This can
be seen in the data provided in [2], which show an increase in the ﬁgures of the
Information Economy in the last years. For example, Information Technology presents
a growth of 8.2% in 2016. However, despite these indicators on the performance of the
software industry in the country, there is not enough evidence on the form or style of
work that these companies have to create a software product. These statistical analyzes
[3] do not delve into Chilean industry’s problems, daily lives and reality, due to the
way in which this information is collected. In addition, there are no studies carried out
in Chile on the quality of the national software product. Through the above explained,
some problems can be detected such as the lack of concordance between theory and
international practice with national reality; and the scarcity of studies on risk factors,
success, failure and practices or processes that have software developers in Chile.
These limitations motivate the research that seeks to know how software development
companies work, providing a real overview of methods, activities, processes, tools and
factors, that condition software development, helping to obtain an overview of their
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_66

way of working. Along with this, it will provide a comparison between national and
international development, allowing to highlight differences and similarities with other
development industries, which will help to identify gaps that can be overcome for
continuous improvement of the software development process.
In Sect. 2, a review of related work will be presented. Then, in Sect. 3, the
methodology used to support the investigative work will be presented. Section 4
presents the results obtained both nationally and internationally. In Sect. 5, a brief
discussion will be held on the gaps detected between national versus international
reality and to conclude in Sect. 6 the conclusions on this research.
2
Work Related
It is important to note that the work on the characteristics of national development
companies is very scarce, especially studies focused directly on software development
processes in Chilean companies. On the other hand, in other countries, there are works
focused on this subject, one of the most important and relevant is Chaos de [4] in the
United States, which collects information on IT environments. Within Latin America
we ﬁnd the work elaborated by [5] that exposes a study carried out to Mexican
companies dedicated to developing software. Also included are some statistical results
regarding the use of Software Engineering practices in such companies. At the national
level, a study was carried out to obtain detailed information on the conditions and
characteristics of software companies, including items such as “Research and Devel-
opment”, “Human Capital”, “Processes and Tools”, among others [6]. Another work
aimed to develop risk analysis and mitigation models to help project managers identify
potential risks early, in order to take corrective actions [7]. Finally, there is a study that
surveyed different national software development companies, with the objective of
obtaining a characterization of the Chilean industry regarding developers, projects,
processes and labor climate, among other aspects [3]. The information was harvested
through different roles, therefore this information could be biased by its low reliability.
3
Methodology
In this section, the methodology with which the research was carried out will be
presented, detailing the different activities that were carried out in the development of
the study. It should be noted that the description of the different stages and activities
carried out to develop this research work is found in [8]. The methodological proposal
is based on a qualitative approach, with an ethnographic technique as the center. It
consists of 5 stages, with different steps or activities in stages, as shown in Table 1.
In the ﬁrst stage, information was collected from software development companies.
This information was searched through web pages. It should be noted that this was the
only source that gave results. Other types of contacts were made, such as societies and
universities, but they did not offer any concrete response. From this collection, more
than 120 companies with different business areas related to IT were obtained. To this
population ﬁlters were applied leaving only 45 companies, of which more than half
Ethnographic Study on Practices of the Software Development Industry
701

responded positively to our request of visit. However, visits were made only to 12
companies, due to factors such as resources, travel distances, dates and times available.
At the same time, a questionnaire was prepared representing the hypotheses of the
project. Because the information would be captured through narrations (answers to the
open questions) made by the workers themselves, they would be recorded for later
analysis. In the second phase, the participating companies were visited, interacting
through open questions, participating in team meetings, and knowing the physical
space where they work and some of the tools they use for the development of their
projects.
For the third phase, the information contained in audio ﬁles is transferred to paper.
Once this ﬁrst harvest of data has been made, the information is transferred to col-
laborative murals by ordering and classifying it into: Risk Factors, Failure Factors,
Success Factors, Work Style, Problems, Assessments, Company Data and a scheme of
the different stages to create a software product described by each of the companies.
This second classiﬁcation is guided by the Elements of Analysis, which are a set of
classiﬁed concepts (Tag) that help to give a clearer and more speciﬁc answer to the
hypotheses raised at the beginning of the project.
In the next stage, we begin to construct different proposals for improvement for the
Critical Elements found in this study, including some improvements, actions or
activities that have already been implemented by some companies.
And in the last stage, a report was created that was sent to all the participating
companies, as remuneration for the participation in the investigation. In turn, three of
these companies are selected to receive retrospectives on this research and in particular
on proposals for improvement. It is important to emphasize that this methodology was
used both nationally and internationally.
Table 1. Methodological research proposal.
Stages
Description
Steps by stage
Deﬁnition
It seeks to explain and describe how the population
or group is to be studied, how the contacts were
made, what information or data will be sought and
the form or technique to be used to collect this data
Selection and contact
of Informants
Data Class to Collect
Data collection
procedure
Camp
It begins the empirical study of the selected
population or groups, using ethnography as study
technique
Field work
Ethnographic
Technique
Recollection
Analysis of data, through, transcription,
classiﬁcation and categorization of the information
obtained
Data Reduction
Data Reconstruction
Proposal
Different possibilities and options are proposed that
seek to improve in different areas the selected
group or population
Construction of
improvement
alternatives
Evaluation
Obtain feedback and evaluate the effectiveness of
proposals
Validation of
improvement
alternatives
702
D. Carrizo and A. Alfaro

4
Results
4.1
Demographic Scope
At the national level, in relation to the number of workers, most companies have
between 2 to 10 workers and the rest between 15 and 25. It should be noted that this
may vary according to the number of projects that are being implemented. The majority
work between 5 to 10 projects in parallel. Companies describe two types of organi-
zational structures. The ﬁrst, shows only two workers, one of whom takes the lead in
the face of the customer and the other is the programmer. In the second, the number of
workers increases, identifying roles as General Manager, Project Manager or CTO,
Sales or Sales Manager, Project Managers and ﬁnally Developers, Design, Testing,
among others. In relation to the business areas, special attention is given to custom
development, where Web development and maintenance of the systems that were
created are more frequent. In most cases, a good working climate is described, a
ﬂat-linear organizational structure, highly qualiﬁed personnel, collaborative and com-
fortable spaces, with good communication between peers and a low level of profes-
sional rotation. At least three ways of accessing or working on a project are described:
the ﬁrst, and the most frequent, is when you go out to look for projects, presenting and
offering services to clients; in the second, the customer through different means gets in
touch or arrives at the company, requesting the service; and the third, are public
tenders. Finally, they identify the customer as a crucial factor when developing a
software product, where their participation stands out in meetings, project progress,
approvals, requirements or functionalities development and product development.
At the international level, companies with between 5 and 10 workers were detected
and only one of the 20 members. With respect to the number of projects running in
parallel, these vary between 5 and 20 projects, both development and maintenance. In
relation to the business areas, all develop customized software and maintenance, others
include the development of mobile applications, their own ERP, sub-contracting and
Marketing. The structure or roles in the companies visited vary between those that
deﬁne personnel for Backend, Frontend and Frontend App divided into development
departments, including management roles; those that describe roles according to their
business areas (ERP and Development); and those that use only two roles, the Project
Manager and developer. On the other hand, different ways of working are identiﬁed,
among them the ﬂexibility of time, allowing in summer and on all Fridays only work in
the morning, as well as having feedback times for projects. The companies declare
calm climates, the importance of working in teams, and the positive inﬂuence that has
had on their projects the incorporation of young people. The latter is the reason why
they prefer to hire people with little experience and train them in their methodologies of
work and development. The companies emphasize the good relationship and empathy
with the clients, especially in Post Sale. It is important to emphasize the complication
that international companies have in accessing large clients or public administration,
because large development companies cover this niche market.
Ethnographic Study on Practices of the Software Development Industry
703

4.2
Software Engineering Scope
Table 2 shows some of the tools that support the development of the projects, both to
companies at national and international level. It is important to note that most com-
panies use online tools and free of charge, reducing costs and optimizing their services.
In relation to the documentation, a document called Functional is given, which contains
the functionalities of the system, the cost of each one according to development hours,
some milestones or Sprint, together with the total value of the project. The content of
this document will depend on the magnitude of the project and the client.
At the national level, companies described between 6 to 10 stages of development
and the starting point of a software project are the initial meetings in which the client
describes their problem or the project to be developed, and from that it is carried out an
estimate of cost and time of very high level. The next stage is estimation. At this stage
the project is analyzed and a more detailed proposal on the solution of the problem is
generated. Once this proposal is accepted, some companies hold a meeting to present
the work plan and identify the participants in the project.
Then, following the development or coding of the product, at the end of the
Development stage, testing is performed, which can be through functional tests,
together with the client or as a stage totally independent of development, to subse-
quently deliver the product that in most cases is done in a piecemeal fashion. As an
international counterpart, the companies visited describe between 4 to 10 stages or
phases when developing a software project. Regardless of how the project gets in the
Table 2. Tools used for the development of software projects.
Tools
National
International
Developmental
languages
.Net, Java, Php. In few cases Ruby,
Python and framework as Larabel,
Django Rails
Php, python, Java, CSM
(Wordpress ad Blogger) Android,
Android Studio, Objective-C,
Xcode and the use of framework
and screen engines
Project
management
Pivotal Traker, Tagg, Carga Gantt
(utilizada frecuentemente),
Sistemas propios de Gestión de
Proyectos (en algunos casos)
Trello, Asana, Instagantt, Jira,
Redmine, Microsoft proyect, own
ERP and Mantis (errors
notiﬁcation)
Team
management
Slack
Slack, Team Work
Implementation
Heroku
Docker, Heroku, Fabric
Documentation
Proposal Doc, Requirements
speciﬁcation (2° option)
Proposal Doc
Versions
Control (VCS)
Git, Subversion (mainly)
Git (GitHub, Bitbucket)
Data bases
MySql, postgres
Mysql, Postgres and
Mongo/SGDB: Mariadb, Sql
Alchemy
704
D. Carrizo and A. Alfaro

hands of companies, the early stages or activities are meetings or consultancies. In
them, they know the problems or ideas of the client, as well as the ﬁrst collections of
functionalities. In addition, a rough estimate of the cost of the project is also provided.
As a second step, we analyze the feasibility of the project, what the client wants, and
the architecture of the project.
Once the different contractual documents have been sent and closed, the next stage
is the development of the product. In this activity the functionalities, development time,
milestones to the different project management tools are raised, in addition a functional
test is described. Once the coding stage is over, a series of evolutionary deliveries
begins. This stage is a period of testing between the developer and the client. And
ﬁnally, we arrive at the total delivery phase of the product, where the project enters a
period of white march, to detect the last faults.
Next, we will go on to describe in more detail, some of the most signiﬁcant stages,
when developing a software product, both nationally and internationally.
In a national context, the delivery of the product, mostly, is parceled out, delivering
a package or set of functionalities. Once these functionalities are approved, the process
is restarted, in some cases some requirements are retaken or other functionalities are
continued. After delivering the product, some companies begin a stage of Warranty,
where the customer is offered the maintenance of the product. The ﬁrst requirements
survey is done in the ﬁrst meeting with the client, in which roles include Business
Manager or Sales Manager, Project Manager. One company stated that it uses other
requirements-raising techniques such as: templates (screenshots) with a basic work-
ﬂow, user experience, edge conditions and canvas to clarify the client’s ideas.
Few companies separate or individualize the stages of development and testing.
They call it the QA phase. Small development teams are found between 1 and 3
developers at most. They manage the codes through VCS. Also detected some com-
panies that do not use an VCS, where the code is managed through emails or pendrive
and the backup is only done locally.
Most companies perform delivery by milestones, ranging from 20% to 80% of
system operability. These deliveries allow the customer to be able to test and become
familiar with the system from the start. The criterion of expert is the technique or
method used by all the companies to estimate the cost and total time of software
project. Among the factors to consider to allocate the time are: the complexity of the
functionalities, a history of previous estimates and the capabilities of its programmers.
Along with this, some companies mention a time of slack that is given to the projects to
solve changes or some eventuality in the development of the project. Companies show
ﬂexibility in the face of changes, however, this will depend on the exchange rate, since
if it is not a simple change, or another totally different functionality, then it will be
revalued and frozen until the end of the project, to generate a new version of the
system.
In relation to the international level, the requirements are raised in the different
meetings with the client (consultancies). In many cases it is a question of collecting the
functionalities of the system to be developed. It is important to emphasize that none
describes or exposes any special technique or different at the time of performing this
activity. At a different stage called the test phase, companies perform a test of system
functionalities that can be through a test plan, between peers (developers) or with the
Ethnographic Study on Practices of the Software Development Industry
705

client. It is important to emphasize that the client plays an important role and in many
cases is the main one at the stage, since he is the one who performs a more profound
system review. As for the times of this stage, most companies state that this activity
takes place between two to three weeks, depending on the type of project. In relation to
the coding stage, we describe the division of groups, both for Backend and Fronted,
which are incorporated into the different projects according to the stage in which their
development. So too, are smaller companies where everyone does everything. Among
the elements that stand out in this phase are Kickoff meetings to give indications of the
project, being the Senior who programs the data model, more complex parts of the
system and creates the structure of the project, using structures or code repositories and
in some cases, the detailed record of hours of development.
The delivery of the product is very evolutionary and is divided into different
delivery milestones, in addition, is closely related to the Testing phase because they
serve to make corrections and follow-up of the project. In addition, the implementation
of the ﬁnal version of the system is another type of delivery. This is called a white
running period or guarantee that goes from 2 to 6 months, depending on the type of
project.
The companies make their ﬁrst estimates of both cost and time in consulting or
meetings with the customer, being reﬁned and delivered in a ﬁnal budget. All the
companies studied make their estimates based on experience and in some cases add
some standard (pre-deﬁned) valuations, according to some functionalities, modules or
documents. Once this estimate is made, most companies add a few more hours to
development or slack time. Most changes consume the slack or margin of time that is
added to the project. It is important to note that these changes have nothing to do with
the stages of testing, delivery and warranty, since these phases are for the correction of
the functionalities.
Then, we will go on to describe the different ways of facing, managing and
managing the projects.
At the national level, development times are between one and three months small
projects, three to six months medium projects, and more than six months large projects.
Most development teams are made up of a programmer and a project manager who, in
addition to supervising, also program. With regard to the distribution of projects,
project managers lead more than one project at a time and one programmer can par-
ticipate in more than one project. In some cases, a programmer, depending on his level
of experience, can take a project alone.
In most companies, it is the Project Manager who relates directly to the customer,
organizing meetings, showing progress, distributes functionalities, etc. It is important to
note that the tools most used to manage a software project are the gantt chart and free or
own project management systems, which in some cases are combined. These tools load
data such as: activities, dates, delivery milestones, development times, among others.
In contrast, at international level, development times vary between one and a half to
two months; the three- to four-month projects are called large projects. In relation to the
formation of teams, in most cases they are a programmer and a Senior, although they
will depend on the size of the project. Among the different ways of working on your
projects are the development of projects with micro-service structures, customer
feedback rounds to close the design stage, offers of maintenance, among others.
706
D. Carrizo and A. Alfaro

Inrelationtoqualityassuranceatthenationallevel,ontheonehand,onlyonecompany
has an Area and Quality Manager that allows it to help project managers manage, meet
targets, review project times and prevent possible risks. The remaining companies do not
have an area or a quality management department. There is a difference between quality
and metrics, because, as mentioned above, almost all of the companies studied do not
manage quality, but some of them collect data. These data are obtained through different
tools used to manage projects or other activities such as project closure meetings.
On the other hand, at the international level, although it is understood the impor-
tance and need for companies to incorporate quality into their services, due to limi-
tations of resources and priorities, this element is not fully incorporated or not properly
executed in companies. One company says they are evaluating to be certiﬁed with
quality assurance (QA) companies. Another describes that through Timeline compli-
ance, user feedback and clients, evaluate the quality of their services. In relation to the
collection of measures, there are cases where this task is done based on captures of
sensations by the clients of how the company does its work. On the contrary, there are
companies that are constantly collecting measures in a rigorous way, which allows
them to deliver reports or reports to their customers.
4.3
Factors Scope
Among the success factors at the national level, we can emphasize constant commu-
nication, transparency and becoming an ally or partner of the client. In addition to the
detection on the client side of a role with capabilities to make technical and economic
decisions, in addition to end users who can be asked to raise requirements and will also
validate and use the system. At the international level, the success factors described by
the participating companies are, the management of expectations, customer loyalty and
tranquility, being realistic and knowing whether or not a project can be undertaken.
At the national level, low transparency, the deterioration of the relationship with the
client and the non-detection of an empowered role capable of making economic and
technical decisions are the most named risk factors for national companies. Along with
this, the poor estimation of project times, poor capture of requirements, incorporation of
more members into the development team and encapsulation of knowledge are also
referred to as risks. As a counterpart, for international companies, the risk factors to be
highlighted are the lack of management in the projects, the client, either because it has
no idea of the project or because it is a bad client, and not compliance with requirements.
No company was detected at the national level that had products that were not used
or projects canceled before time, but did identify factors of failure as the rigidity of the
project and the client. And to conclude, it is important to note that many participants
spoke of failure factors as a counterpart to success factors. This means that in order not
to fail in a software project, you have to meet the factors of success and in turn keep in
mind and prevent risk factors.
Similarly, international companies also describe failure factors as contrary to suc-
cess factors. In spite of this, some factors of failure are explained such as: bad esti-
mation,
requirements not
compiled or handled incorrectly,
inefﬁcient
project
management, professional skill level, not knowing how to sell or explain the added
value of project, and clients who do not know what they want.
Ethnographic Study on Practices of the Software Development Industry
707

5
Discussion
In relation to the demographic scope, a similarity could be observed in terms of number
of workers, number of projects, structure and organizational climate, among others.
However, some differences appear in speciﬁc points such as the most advanced
international marketing and mobile development business areas, as well as restriction
of access to public clients and greater time ﬂexibility by international companies.
In the ﬁeld of software engineering, on the one hand you can detect some simi-
larities such as: tools, with international companies being the most used and supported
in these elements, especially in project management; capturing requirements, where
meetings play a fundamental role for this activity; number of roles per project, in most
cases, a project a developer, supported by the Project Manager (National) or Senior
(International); poor quality management due to resource constraints; deliveries, carried
out in installments and milestones; estimates, made through experience. In addition to
documentation and number of stages in the development of projects.
On the other hand, the differences or advantages that the international companies
have over national ones in this same area are: testing, since this activity at international
level, is carried out separately to the coding stage, having the client a very participative
role; use of skeletons or code libraries, as well as version control tools and project
management. Along with this, at the time of estimation, the use of templates with
predeﬁned valuations of the functionalities, periods of white running and development
of projects in shorter times.
Finally, in relation to factors, on a timely basis in the factors of success, national
and international companies emphasize the good relationship with the client. In addi-
tion, national companies attach importance to the identiﬁcation of a role by the
empowered client with decision-making abilities, unlike international companies that
emphasize the idea of clarifying client limits from the outset.
For risk factors, the customer is the common element both nationally and inter-
nationally. As differences, national companies show that a bad relationship with the
client and the non-detection of a decision-making role are risk factors. As a counterpart,
at the international level, when the client is not clear the idea of the project is con-
sidered as a risk factor. Regarding the factors of failure, both nationally and interna-
tionally, the customer is highlighted as a risk in the development of a software product.
6
Conclusions
The software industry has been increasing and positioning itself worldwide, this is not
alien to the Chilean plan, where the indices show the increase in the IT industry,
especially the IT sector. Therefore, knowing how they work or develop their products
are fundamental aspects to promote and encourage the continuous improvement of this
industry. For this reason, this research was developed with the objective of getting
involved in the software development companies and knowing how to contract,
develop and deliver their software products. As a result, it was possible to detect that
companies develop their projects through a set of good practices, which can vary or
adapt according to the type of project. These processes are supported through different
708
D. Carrizo and A. Alfaro

tools that help manage, control and support the creation of a software product. The
degree of use of these tools will depend on each company, having cases in which the
level of use is minimal. Although there are variants in the form of developing a
software product, they all start with the requirements acquisition, budget delivery,
product coding and testing stage, and ﬁnally, delivery, where in most of the cases is
parceled out, although it will depend on the type of project. One of the important
aspects to note is that, although they recognized that quality management is a funda-
mental element in the development of a software product, not all companies were
carrying out this activity or task, accusing them of lacking resources. Faced with the
issue of resources, all companies stated that many of their limitations are caused by the
few resources, whether economic, human, time, among others. In addition, no strong
contrasts between national and international development, highlighting international
companies over national ones in more limited times of development, greater use of
tools that support the development of the project, time ﬂexibility and stages of
development more deﬁned. As similarities were identiﬁed the conformation of teams,
number of stages of development, number of projects, structure and organizational
climate, among others. Finally, in future work it is expected to continue contributing to
the national industry, based on this research, through the delivery of a set of proposals
for improvements that help progress and strengthen software development.
Acknowledgements. This work was funded by the Research fund of the University of Atacama,
project DIUDA 22298.
References
1. Achá, V., Bravo, C.: Desarrollo Endógeno con Proyecciones Externas. Desafíos y
Oportunidades de la Industria del Software en América Latina (2013)
2. País digital: Fundación país digital. País digital 2017, Chile (2017)
3. GEMS: Análisis estadístico de la encuesta de caracterización de las Pymes de software en
chile. Departamento de Ciencias de la computación. Universidad de Chile (2015)
4. Standish Group: Standish Group, Chaos report (2013)
5. Jiménez Hernández, E.M., Orantes Jiménez, S.D.: Metodologías híbridas para desarrollo de
software: una opción factible para México (2012)
6. Gechs: Sexto Diagnostico de la Industria Nacional de Software y Servicios (2008)
7. Pereira, J., Cerpa, N., Rivas, M.: Factores de éxito en proyectos de desarrollo de software:
análisis de la industria chilena del software. In: Workshop de Ingeniería de Software, Arica
(2004)
8. Carrizo, D., Alfaro, A.: Methodological proposal for ethnographic research in software
engineering: a case study. In: JIISIC-CEIS. XII Jornadas Iberoamericanas de Ingeniería de
Software e Ingeniería del Conocimiento y Congreso Ecuatoriano en Ingeniería de Software,
Ecuador (2017)
9. Vasconcelos, J.B., Kimble, C., Carreteiro, P., Rocha, Á.: The application of knowledge
management to software evolution. Int. J. Inf. Manag. 37(1), 1499–1506 (2017)
Ethnographic Study on Practices of the Software Development Industry
709

Geolocation Applied to Emergency Care
Systems for Priority Groups
A. José Sánchez1(&), L. Lídice Haz2, B. Datzania Villao3,
and G. Washington Torres1
1 Facultad de Sistemas y Telecomunicaciones, Universidad Estatal Península
de Santa Elena, Avda. principal La Libertad - Santa Elena, La Libertad, Ecuador
{jsanchez,wtorres}@upse.edu.ec
2 Facultad de Ciencias Matemáticas y Física, Universidad de Guayaquil,
Ciudadela Universitaria “Salvador Allende”, Av. Delta y Av. Kennedy,
Casilla Postal 471, Guayaquil, Ecuador
victoria.haz@hotmail.com
3 Facultad de Ciencias Administrativas, Universidad Estatal Península
de Santa Elena, Avda. principal La Libertad - Santa Elena, La Libertad, Ecuador
dvillao@upse.edu.ec
Abstract. The purpose of this research was to design and develop a mobile
location computer system to monitor the location and emergency alerts for
priority groups, in particular, people with disabilities, the elderly and children;
through the use of electronic devices, the mobile network and several web
technologies. For the analysis of the information and functional requirements of
the system, bibliographical and ﬁeld research was applied. It was applied sur-
veys and interviews to the research group and experts in software design and
development, with the aim of validating the functional and non-functional
requirements deﬁned for the project. Its implementation will contribute to
improve the quality of life of people that belong to the priority group, providing
them independence and relaxing because the application can generate alerts to
their caregivers in cases of emergencies.
Keywords: Geo-location  Emergency alerts  Priority groups
Electronic devices
1
Introduction
The lack of information for ﬁnding the physical location of someone is a problem that
causes a huge impact in several sectors of the population, such as people with dis-
abilities and children and elderly. They are called priority groups, because of a com-
bination of physical, social, political and environmental factors that make them the
more vulnerable group in the society. The high degree of dependence of this group on
their caregivers becomes a hard and difﬁcult task, because of their critical health and
the external factors that affect them, which create barriers to their mobility, safety and
performance [2]. In addition, the risks that these people are exposed to accidents inside
or outside their houses, spatial disorientation for long periods of time or any emergency
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_67

that endangers their integrity, which creates a very high level of concern to their
relatives. Therefore, the safety of this group has become in a priority for their relatives,
society and governments. In response to that, it is necessary to develop technological
tools that contribute to the safety of these people, in order to reduce the impact of the
risks mentioned before. Because of that nowadays the world has seen tremendous
advances of information communication technologies that have provided new oppor-
tunities and solutions to emergency events, especially aim to help priority groups.
In Ecuador, priority groups are protected by the Constitution through attention and
inclusion programs that aim to provide specialized care in the public and private
spheres. However, in the last three years it was reported 4.402 missed people among
elderly, children and people with disabilities and only 34% were found. The reasons for
that, according to a report about missing people in Ecuador, were because there are no
databases or information systems for ﬁnding people, there is confusion of competences
between public institutions, there is a delay in the assignment of agents, and lack of
statistics and quantiﬁed information, which clearly shows that there are weaknesses in
the technology needed for ﬁnding people in Ecuador.
According to the Ecuadorian Constitution, Title III Chapter IV of the rights,
guarantees and duties of vulnerable groups, mentions in its article number 47 “… that
priority groups are children, underage, people with disabilities, elderly and people with
terminal illnesses…” [3]. Article 35 of same Constitution states that “elderly, children,
underage, pregnant women, people with disabilities, inmates, and those suffering from
catastrophic illness will receive priority and specialized care in the public and private
spheres” [3]. These articles justify the development and implementation of Techno-
logical tools about geo-location systems for ﬁnding people that belong to the priority
group. For this reason, it is important the assistance of the Government for the pro-
vision of technological equipment and tools required for this type of projects. The use
of web and mobile applications is part of the lifestyle of people. Therefore, there are
different computer applications that use geo-location to ﬁnd places, people, companies
that mostly are used in mobile operating systems.
In the Province of Santa Elena, according to data from the 2010 population census,
there are 308 693 inhabitants between men and women, of whom 5.6% are elderly
(persons over 65), 21.9% are children (between 5 and 14 years old) and 5.35% are
people with some type of disability (intellectual, physical-motor, visual, hearing or
mental) [1], which means that only in this Province the 32.85% are people that belong
to priority groups. Therefore, the use of technological tools to help people that belong
to this priority group, would contributes to their independence through devices that
help them to communicate with others, study, work, and participate in social or
recreational activities [4].
The research describes the development of a mobile application used by people of
priority groups that generates information about their location and emergency alerts.
The location system for emergency care is a support tool in the search and control of
alerts that optimizes the process of emergency care of users who have suffered some
incident. The main beneﬁciaries are the elderly, children, and disabled who can access
to the mobile application through a telephone with internet. The second group of
beneﬁciaries are relatives of these people because they can track location and receive
emergency alerts. The third group of beneﬁciaries are the control and relief entities,
Geolocation Applied to Emergency Care Systems for Priority Groups
711

who can access to statistical information immediately about the people location and
events generated over time, having information of the exact location where events have
happened.
The research was part of a project funded by the Universidad Estatal Península de
Santa Elena that involved the participation of the Cantonal Council for the Protection of
the Rights from La Libertad. The Council has registered 300 people, including 200
elders who were classiﬁed into two groups, the ﬁrst one aged from 65–85 which
represent 85% and the second one aged from 85 and more which represent 15%. There
are also registers 100 people with disabilities such as sensory disabilities in a 30%,
physical disabilities (partial motor or physical absence of a member other than upper
limbs) in a 50%, and psychological disabilities in a 20%. With the application, the
members of the Council for the Protection of Rights can have statistical information on
the location of these people, as well as to register all people that have some emer-
gencies. Moreover, the Council can register the cause and the action taken and keep a
register of actions implemented in case of recidivism.
The paper is structured as follows: the ﬁrst section describes the literature about
mobile location systems for emergency events and its importance for priority groups.
The second part presents the features of the application for emergencies events in La
Libertad, Ecuador. The third section presents the results of the application. The paper
ﬁnishes with a sum up of important insights of the research.
2
Applied Technologies for Finding People
The social interest generated by priority groups such as children, elderly and disabled
in relation to their safety and physical integrity, makes it critical the use of information
and communication technologies to develop new products that help to deliver emer-
gency alerts and accidents. Thus, there are different types of systems focused on the
monitoring of people of priority group using GPS to send the location coordinates and
request help in case of emergencies [3, 7]. In the market, there are some products that
provide this service, which are known as PERS (Personal Emergency Response Sys-
tem), whose architecture is composed of an electronic device that contains an aid
button, a base for the connection and a call center and 24 h monitoring for 365 days a
year to cover assistance.
In Ecuador, there is a product called 24-7 alert that has a necklace in which it is
possible to press an emergency button that sends a signal (of 183 m coverage) through
a base device connected to a telephone exchange. In case of requesting an ambulance,
this is sent, and people registered in the system are contacted to receive inform about
what happened [5].
Other projects use mobile platform-based systems and telecommunications tech-
nologies such as GPS (Global Position System) and SMS (Short Message Service) that
facilitate constant communication with stakeholders about the current situation of
people of priority group. This is called MAE or Mobile Assistance System for the
elderly, where its scheme consists of a client application installed on a smartphone,
which is used by people who receives reminders through text messages. Another
feature of this application is the delivery of the location coordinates through the use of
712
A. José Sánchez et al.

GPS or WI-FI networks, generating periodic follow-ups that are sent to a server and
where coordinates can be reviewed by interested parties (family or caregivers) through
a mobile phone. The tests performed during 14 days in Singapore with volunteers
obtained positive comments demonstrating that the MAE system improves the expe-
rience of the elderly on mobile platforms [6].
In conclusion, there are applications for mobile phones that transform telephones
into a means of security and personal care. Most of these applications have in their
design a panic button that, when is pressed, automatically sends text messages, GPS
location coordinates and/or make calls to previously registered and selected telephone
numbers [6]. Here are some examples, Help Me on Mobile Red Panic Button, Panic
Alert, Emergency Alert, iCare Personal Emergency Alert, and others.
3
Mobile Location Systems to Emergency Attention
3.1
Components of the Technology Platform for Emergency Care
The development of this technological platform aims to integrate location information
systems and current technologies that exploit mobility through new options offered in
the ﬁeld of telecommunications. The system provides a localization service that can be
used from any Android device that has access to internet, to the nearest points of
emergency care.
The development of this project has as objective that a mobile application user, can
generate an alert in the case of an incident, when the alert has been generated, auto-
matically a notiﬁcation will be sent to all persons or entities interested to take the
actions required. In addition, a notiﬁcation will be sent to the emergency centres, which
will be monitoring alarms issued from a web application.
The technological platform uses mobile localization mechanisms to locate people
and take action in case of emergencies. This platform is composed of four components:
(1) the web application; (2) the web service; (3) the mobile application and (4) the
electronic device. The web application allows caregivers to monitor the location and
alerts issued by people that belong to priority groups, as well as the administration of
the site. The web service is used as a communication interface between the electronic
device and the database. Its operation would be the same as the mobile application. The
electronic device and the mobile application allow ﬁnding location and real-time alerts
to the web service, which records the information in the database for further processing
and use by the different components system.
3.2
Web and Mobile Application Features
This section describes the functionalities and results obtained from components 1 (the
web service) and 3 (the mobile application). The web application called Locate-Me has
three different user proﬁles. The ﬁrst proﬁle is related to generic users, that is, people in
the priority group (elderly, children, and the disabled). These people will be the ones
that generate the alert from the mobile application in case of any incident.
Geolocation Applied to Emergency Care Systems for Priority Groups
713

The second user proﬁle are caregivers, these are usually relatives that will have as a
duty monitoring if the priority groups have suffered some incident. These users can also
attend the issued alerts and pay attention to them. Finally, the third user, are the
operators of entities interested in the welfare of these people.
To carry out the pilot tests of this project, it was worked with the members of the
Cantonal Council for the Protection of Rights from La Libertad, Ecuador, who were in
charge of monitoring all the alerts issued by generic users and give the respective
support in case of an emergency (Fig. 1).
3.3
Application Architecture
Communication between the mobile application and the web application is done
through a web service based on reliable communication standards such as the REST
(Representational State Transfer) and the exchange data format JSON (JavaScript
Object Notation), which are effective for the exchange of information among appli-
cations developed in different languages and platforms.
In addition, for security management and persistence control, Spring Data JPA
(Java Persistence API) was implemented, because it simpliﬁes the implementation of
the data access layer, allowing taking advantage of object orientation when interacting
with the database.
In order to send notiﬁcations, the Google cloud messaging service (GCM) was
used. It was also implemented in the web service a class in charge of managing the
device record identiﬁers to which notiﬁcations and communication with the GCM are
sent (Fig. 2).
The Google Maps service is used for the geolocation of people in the web appli-
cation and in the mobile application (Fig. 3).
Fig. 1. General diagram of the system process.
714
A. José Sánchez et al.

3.4
Design and Development Features of the Web and Mobile
Application
For the coding of the web and mobile application we used the Eclipse Mars IDE
development tools for Windows, Android Studio, Apache Tomcat as web server and
MySQL database. The web application called Locate-Me for its architecture works on
any operating system through a web browser and the mobile application is compatible
with the Android operating system. To Access to the web application, it is requires
access credentials.
The following are the main modules implemented in the system:
• User Registration: It registers the form with personal data of generic users,
caregivers and operators.
• Capturing the location of a user of the priority group: It gets the location
coordinates of a user, and his location is visualized through a map. This information
is constantly updated in the application and on the map (Fig. 4).
• Alerts and notiﬁcations delivery: The emergency alert generated by the generic
users (people from the priority group) is captured and the alert is sent through the
mobile application installed on caregiver’s telephones (user relatives) (Fig. 5).
Fig. 2. Push notiﬁcation. Displays the schedule of sending push notiﬁcations to the caregivers in
the event of an alert being generated.
Fig. 3. Communication between the mobile application, the web application and the web
service.
Geolocation Applied to Emergency Care Systems for Priority Groups
715

• Records of recent user locations: It registers the most recent location coordinates
of people of the priority group, which serves for the caregivers to visualize in the
application the last registered places.
• Attention Alert: The caregivers access to the information about the alerts and can
modify their status (unattended and attended), so caregivers can manage the alerts
generated by people of the priority group.
• Queries of locations and alerts: Allows caregivers and operators to query about
events that have suffered the priority group (Fig. 6).
• Locations and alerts reports: Operators generate reports on events that occurred to
users.
• System Settings Module: System administrators make changes to application
settings, such as modifying the census time for registering a user’s location or
selecting the date range for the attention of an alert generated.
Fig. 4. User location. It shows the interface integrated with the Google map in which the user’s
location is marked after the login.
Fig. 5. Mobile app alerts. It allows to display the screen with the options to generate the alerts
according to the event happened, the button called alert is used which must be pressed for more
than 3 s to deliver the alert and send the notiﬁcation to the associated devices (caregivers)
through a message SMS every 3 min after the alert was generated, until to get conﬁrmation of the
caregivers.
716
A. José Sánchez et al.

For the design of the user interfaces, usability criteria were applied with the main
emphasis on the use of minimalist and intuitive schemes that allow improving user
satisfaction, efﬁciency and effectiveness in relation to software manipulation.
4
Results
During the execution of the tests with real users, 30 alerts were generated. These alerts
were delivered with different mobile devices for an approximate time of 60 min. Of the
30 alerts, 27 were delivered under the conditions established in the system, being
considered as successful alerts. The efﬁciency index for these tests in real environments
is 90%. It is important to verify the use of the internet service on the mobile devices in
which the application is installed, which it’s an essential requirement to optimize
response times in the attention of alerts.
Following the alert attention process, notiﬁcations were sent properly to the afﬁl-
iated devices (caregivers), 25 notiﬁcations were received in an approximate of 2 to
5 min, resulting in an efﬁciency rate of 83% for sending and reception of notiﬁcations.
It is necessary to improve the notiﬁcations service, avoiding not sending notiﬁcation
and reducing the time of caregiver’s reception.
The functionality and efﬁciency tests of the prototype demonstrated the operation of
the process of generation, tracking and attention to alerts, guaranteeing the responses to
users’ requests quickly and with the information required. It is convenient to install the
mobile application on smartphone devices with Android operating system in versions
higher than 6.0, and with display sizes higher than 4 inches.
Fig. 6. List of alerts generated by users from the mobile app. The information that is displayed
is related with the user’s ﬁrst and last names, the date when the alert is generated and the location
coordinates. There is also a refresh button, which updates the alerts in real time.
Geolocation Applied to Emergency Care Systems for Priority Groups
717

5
Conclusions
The technological platforms cited in this study show a signiﬁcant advance for
improving the safety and quality of life of the elderly, children and people with dis-
abilities. However, these technologies have limitations due to technical and economic
aspects, such as the lack of precision in GPS of mobile devices and high costs involved
in their development and improvement.
The use of new information and communication technologies provide multiple
beneﬁts, however, in this context they could create barriers especially for the priority
group, in relation to the low level of usage and accessibility of these tools; so, it is
necessary to analyse the requirements of these people avoiding risks in the develop-
ment of future projects. Finally, the researches carried out in the different works cited
here, showed that it is necessary to use GPS for ﬁnding people location, as well as the
use of internet service in mobile devices.
Acknowledgments. To the Cantonal Council for the Protection of Rights of La Libertad,
Province of Santa Elena, for providing the information, make possible the participation of the
priority group to and carry out functional tests of the technology platform for emergency care.
Pedro Quimí Reyes for his collaboration in the software design and development process.
References
1. INEC Ecuador, Instituto Nacional de Estadísticas y Censos. www.ecuadorencifras.gob.ec. [En
línea]. Disponible en: http://www.ecuadorencifras.gob.ec/wp-content/descargas/Manu-lateral/
Resultados-provinciales/santa_elena.pdf. [Último acceso 30 Julio 2015]
2. Dorada, J.R., González, J.R., Adrián, M.E.D.C.: De las ayudas técnicas a la tecnología
asistiva. In: Pérez, F.J.S., Vázquez, J.R. (eds.) Tecnología, Educación y Diversidad: Retos y
Realidades de la Inclusión Digital. Actas del III. Congreso Nacional de Tecnología,
Educación y Diversidad (Tecnoneet), pp. 235–240 (2004)
3. Asamblea Nacional de Ecuador. www.asambleanacional.gob.ec. 2008 [En línea] Disponible
en: http://www.asambleanacional.gob.ec/documentos/constitucion_de_bolsillo.pdf. [Último
acceso 01 Septiembre 2014]
4. Huei, Y.C., Swe, T., Xian, L.Z., Yee, N.S.: Enterprise mobile tracking and reminder system:
MAE. iJIM 6(3), 25–33 (2012)
5. 2.-7 Alert. www.247alert.ec 24-7 Alert 2014 [En línea]. Disponible en: http://www.247alert.
ec/como-funciona.html. [Último acceso 20 Agosto 2014]
6. Mitchell, H., Johnson, J., LaForce, Y.S.: The human side of regulation: emergency alerts. In:
The 8th International Conference on Advances in Mobile Computing and Multimedia, de
MoMM 2010, New York (2010)
7. Ferrás, C., García, Y., Aguilera, A., Rocha, Á.: How can geography and mobile phones
contribute to psychotherapy? J. Med. Syst. 41(6), 92 (2017)
718
A. José Sánchez et al.

MORPHY: A Multiobjective Software Tool
for Phylogenetic Inference of Protein
Coded Sequences
Cristian Zambrano-Vega1(B), Antonio J. Nebro2, Jos´e F. Aldana Montes2,
and Byron Oviedo1
1 Facultad de Ciencias de la Ingenier´ıa, Universidad T´ecnica Estatal de Quevedo,
Quevedo, Los R´ıos, Ecuador
{czambrano,boviedo}@uteq.edu.ec
2 Ediﬁcio de Investigaci´on Ada Byron, Universidad de M´alaga,
Arquitecto Francisco Pe˜nalosa, 18, 29071 M´alaga, Spain
{antonio,jfam}@llc.uma.es
Abstract. Most of software solutions for phylogenetic inference try to
ﬁnd the best phylogenetic tree according to one reconstruction criterion,
maximum parsimony or maximum likelihood, making the exploration of
diﬀerent hypothesis based on these two features a complex process. In
this work, we present a novel software tool for phylogenetic inference
based on a multiobjective approach called MORPHY, which searches
for a set of compromise solutions according to the criteria of maximum
parsimony and maximum likelihood at the same time. This tool not only
works with DNA sequences, but also allows to deal with protein coded
datasets. It is implemented using the multiobjective and phylogenetic
features of the software MO-Phylogenetics, and the program outputs are
a set of optimized trees in Newick format. A consensus tree from all the
obtained solutions can also be produced. MORPHY’s executable, source
code, and sample datasets are publicly available at the web repository:
https://github.com/KhaosResearch/MORPHY.
Keywords: Multiobjective metaheuristics · Phylogenetics
Optimization tools · Computational biology
1
Introduction
Phylogenetic inference is the process of ﬁnding an estimate of the evolutionary
history of a set of organisms. This estimate is typically a tree representing the
evolutionary relationships among species. A method for inferring phylogenies is
to deﬁne some optimality criteria based on biological aspects, so the process
can be formulated as an optimization problem aimed at ﬁnding the best tree
according to those criteria. Two popular phylogenetic functions are maximum
parsimony and maximum likelihood [1].
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_68

720
C. Zambrano-Vega et al.
Parsimony and Likelihood are diﬀerent criteria often used separately, which
are able to induce heterogeneous learning models to the search procedure. An
alternative approach is to optimize these two criteria at the same time, so phy-
logenetic inference can be studied from a multiobjective optimization approach.
The optimization of problems involving two or more conﬂicting objectives
are referred to as Multi-objective Optimization Problems (MOPs). Typically,
there is no a single solution which optimizes all the objective functions at the
same time. The optimum of a MOP is given by a set of solutions known as the
Pareto optimal set. The elements in this set are said to be nondominated since
none of them is better than the others for all the objective function values. The
representation of the Pareto optimal set in the objective space is known as the
Pareto front.
Under certain assumptions and taking into account theoretical issues [2],
parsimony and likelihood are diﬀerent criteria that contradict each other, so
phylogenetic inference can be tackled from a multiobjective optimization app-
roach. This way, the result of the search process will be a set of trade-oﬀsolutions
(phylogenetic trees) with diﬀerent likelihood and parsimony scores, so biologists
will have the chance of choosing a particular tree according to their preferences.
This idea has been approached in the last years by applying non-exact optimiza-
tion techniques known as metaheuristics [3,4], which are high-level strategies
that rule a set of underlying simpler methods (typically, heuristics) designed to
ﬁnd optimal or near-optimal solutions to a given optimization problem.
In this context, a key issue is to provide powerful software tools allowing
the expert in phylogenetics to easily deﬁne the problem and, with a minimum
knowledge of the underlying solving technique, to get accurate results. In this
paper, we present a software application called MORPHY (Multi-Objective soft-
waRe for PHYlogenetic inference) aimed at solving multiobjective phylogenetic
inference problems according to the aforementioned parsimony and likelihood
goals.
To the best of our knowledge, existing works dealing with multiobjective
phylogenetic inference are only focused on DNA sequences and their used imple-
mentations are rarely available. In contrast, MORPHY is unique allowing to
work with protein coded sequences (plus DNA sequences) supporting a number
of common substitution models, and its code is freely available.
The remainder of this paper is organized in the following way. In the Sect. 2,
we introduce concepts about the basis of Phylogenetics. Section 3 details the
MORPHY algorithm description and a summary of its features. The phyloge-
netic and multiobjective performance of our proposal is illustrated in the Sect. 4.
And ﬁnally, Sect. 5 explains some conclusions about our results and the future
works in this topic.
2
Phylogenetic Inference Problem
The main goal of the phylogenetic inference is the determination of a tree that
best explains the evolutionary events of species under analysis.

Software MORPHY
721
We consider as input a multiple sequence alignment (MSA) composed by n
aligned sequences of species, with columns of N characters, belong to an alphabet
Σ and represent molecular characteristics of the species under review. For DNA
sequences, Σ consists of four characters of the nucleotides {A, T, G, C} and for
protein sequences, Σ consists of 20 characters of the amino acids {A, C, D, E, F,
G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y}. The output of the inference process
is a tree-shaped structure (phylogenetic tree) τ = (V, E), where V represents the
set of nodes in the tree τ and E the branches that connect related nodes V in
the tree τ.
The main computational problem of phylogenetic inference is the large num-
ber of possible topologies in the search space, which grows exponentially with
the number of species to be analyzed. Given n species, the number of possible
binary unrooted trees is deﬁned by Eq. 1 [5]:
|SS| =
k

i=1
(2i −5) =
(2n −5)!
2n−3(n −3)!
(1)
Due to the large number of possible combinations, the exhaustive meth-
ods become totally complex from a computational approach, when trying to
infer phylogenies with more of ten species. Because of this “combinatorial explo-
sion”, the phylogenetic inference is considered as NP-Hard problem, formally
demonstrated both under an approach Maximum Parsimony [6] and Maximum
Likelihood [7].
2.1
Maximum Parsimony Method
The maximum parsimony method aims at ﬁnding a tree that minimizes the num-
ber of character state changes (or evolutionary steps) that are needed to explain
the data. The preferred tree is that whose topology implies a smaller amount of
transformations at molecular level [1]. The problem of maximum parsimony is
described as follows. Let D be an input dataset containing n number of aligned
sequences of species. Each aligned sequence has columns of N characters (sites),
where dij is the state character of the sequence i at the site j. Given the τ tree
with the set of nodes V (τ) and the set of branches E(τ), the parsimony value of
the tree τ is deﬁned as Eq. 2 [8].
PS(τ) =
N

j=1

(v,u)∈E(τ)
wjMC(vj, uj)
(2)
where wj refers to the weight of the site j, vj and uj are the character states
of the nodes v and u in the site j for each branch (u, v) in τ, respectively, and
MC is the cost matrix, such that MC(vj, uj) is the cost to change the state vj
to state uj.

722
C. Zambrano-Vega et al.
2.2
Maximum Likelihood Method
Likelihood is a statistical function that, applied to phylogenetics, indicates the
probability that the evolutionary hypothesis involving a phylogenetic tree topol-
ogy and a molecular evolution model Φ would give rise to the set of organisms
observed in the input data D (set of aligned sequences) [8]. The maximum like-
lihood approach aims to ﬁnd that tree representing the more likely evolutionary
history of the organisms of the input data. It can be deﬁned as follows: The
likelihood of a phylogenetic tree, denoted by L = P(D|τ, Φ), is the conditional
probability of the data D given a tree τ and an evolutionary model Φ [1].
Given τ, L = (τ) can be deﬁned as Eq. 3:
L(τ) =
N

j=1
Lj(τ)
(3)
where Lj(τ) = P(Dj|τ, Φ) is the likelihood in the site j, which is denoted as
Eq. 4:
Lj(τ) =

rj
Cj(rj, r).πrj
(4)
where r is the root node of τ, rj refers to any possible state of r in the site j,
πrj is the frequency of the state rj and Cj(rj, r) is the conditional likelihood of
the sub-tree rooted by r. Speciﬁcally, Cj(rj, r) is the probability of everything
that is observed from the root node r to the leaves of the tree τ, in the site j
and given r, has state rj. Let u and v the descendant nodes next to r, Cj(rj, r)
can be formulated as Eq. 5:
Cj(rj, r) =
⎡
⎣
uj
Cj(uj, u).P(rj, uj, tru)
⎤
⎦
⎡
⎣
vj
Cj(vj, v).P(rj, vj, trv)
⎤
⎦
(5)
where uj y vj refers to any character state of the nodes u y v, respectively. tru
and trv are the branch lengths that join the node r with the nodes v and u,
respectively. P(rj, uj, tru) is the probability of change from the state rj to the
state uj while the evolutionary time tru. Similarly, P(rj, vj, trv) is the probability
of change from the state rj to the state vj in the time trv. Both probabilities are
provided by the evolutionary model Φ.
Usually, it is convenient to use logarithmic values of L, so that the Eq. (3)
can be redeﬁned as Eq. 6:
ln L(τ) =
N

j=1
ln Lj(τ)
(6)

Software MORPHY
723
3
MORPHY: MultiObjective softwaRe for PHYlogenetic
Inference
The MultiObjective softwaRe for PHYlogenetic inference (MORPHY) is a opti-
mization tool that allows infer phylogenetic trees under a multiobjective app-
roach using both DNA and protein coded sequences. Its algorithm is based on
the classical Multiobjective algorithm NSGAII [9] a very popular metaheuristic
for solving multiobjective problems. Has been implemented in C++ using the
features of MO-Phylogenetics [10], a tool that combines the jMetalCpp frame-
work for multiobjective optimization [11] with two phylogenetic packages: the
Phylogenetic Likelihood Library (PLL) [12] and the Bio++ framework [13].
3.1
Algorithm Description
Algorithm 1 shows MORPHY’s general workﬂow. The parameters of the
algorithm are: Population Size P, Number maximum of evaluations C, Prob-
abilities of the Selection and Mutation operators, SP and PM respectively, and
the threshold value δ. We have included the input dataset to infer phylogenies:
the multiple sequence alignments with the set of aligned sequences and the evo-
lutionary model parameters for each dataset, which can be computed by using
jModelTest [14]. The internal representation of the phylogenetic trees is based
on a data structure template provided by BIO++ [13].
Unlike to the main phylogenetic heuristics, which typically start the process
from a quickly built initial tree, the algorithm generates a set of parsimony trees
using the Stepwise Addition method [1] (Line 1) and optimizes the substitu-
tion model parameters applying the Brent’s algorithm and the branch lengths
with the Newton-Raphson [15] numerical optimization method (Lines 2 and 3),
obtaining a broad sampling of initial starting trees.
During the life-cycle of the algorithm, with the aim of ﬁnding new and promis-
ing phylogenetic trees, MORPHY generates a population of alternative solutions
from initial population, called oﬀspring, where the most promising solutions are
selected through a binary tournament operator (Line 9). Then, a random per-
turbation is applied through a NNI topological move [1] mutation operator to
avoid local optimal (Line 10). Finally, these selected solutions are improved with
an new hybrid tree-search strategy, based on two eﬃcient and accurate tech-
niques under parsimony and likelihood approach, the Parametric Progressive
Tree Neighborhood (PPN) [16] and the Likelihood rearrange function of PLL [12]
(Line 11). This hybrid technique explores the tree-space applying SPR topo-
logical movements and adjusting the branch-length only in the aﬀected nodes,
calculating only partially the parsimony and likelihood score every movement.
Furthermore, to escape from optimal local, if the number of new solutions
of the oﬀspring is less than a minimal threshold value (δ), a fully hybrid tree-
search technique is applied to the set of dominated-solutions. This means that the
phylogenetic trees with scores of parsimony and/or likelihood worse than other
have to be replaced to generate a new topology that allow access to unexplored
zones of the tree-space (Lines 23 to 26).

724
C. Zambrano-Vega et al.
Algorithm 1. MORPHY: Multi-Objective Phylogenetic algorithm
Input: population size NP, number max. of evaluations C, probability of
selection SP, probability of mutation PM, umbral δ
Data: multiple sequence alignment, initial trees and evolutionary model
Result: An approximation L of the true Pareto set L∗
1 P ←Initialize Population(NP) ;
/* Use the Stepwise Addition method */
2 optimize SubstitutionModel Parameters(P) ;
/* Brent algorithm */
3 optimize Branch Lengths(P) ;
/* Newton Raphson Method */
4 evaluate population(P);
5 c ←1;
6 while number of evaluations c ≤C do
7
i ←1;
8
while i ≤NP do
9
Qi ←binary Tournament selection(Pop, SP);
10
NNImutation(Qi, PM) ;
/* Nearest Neighbor Interchange */
11
Phylogenetic Optimization LS(Qi) ; /* PLL&PPN hybrid technique */
12
evaluateFitness(Qi);
13
i ←i + 1;
14
R ←P ∪Q;
15
R ←nonDominatedSort(R) ;
/* R = (F1, F2, ...) */
16
P ←0;
17
i ←1;
18
while |P + Fi| ≺NP do
19
P ←P ∪Fi;
20
i ←i + 1;
21
Fi ←CrowdingDistanceSort(Fi);
22
P ←P ∪Fi[1 : (NP −|P|)];
23
if getNumberOfNewSolutions(P) ≺δ then
24
NDS ←getDominatedSolutions(P);
25
foreach s ∈NDS do
26
fully Phylogenetic Optimization LS(s);
27
P ←resetState(P);
28
if getUpdateInterval() then
29
optimize Branch Lengths(P);
/* Each interval of evaluations */
30
c ←c + 1;
31 return L
Finally, with the aim of improving the likelihood score, each interval of N
evaluations of the algorithm, MORPHY optimizes all the branch-length of the
trees and best-ﬁt adjust the parameters of the substitution model (Lines 28
and 29). The output of the algorithm will be a set of non-dominated solutions
L (Pareto set approximation) that describes trade-oﬀphylogenetic topologies
(Line 31).

Software MORPHY
725
3.2
Summary of Features
MORPHY has the following features:
– Given an input composed of a set of species, MORPHY searches for the set
of optimal trees according to the two quality criteria, returning as output the
found trees (in Newick format), as well as their corresponding likelihood and
parsimony scores.
– The optimization technique used is an Evolutionary Algorithm.
– Nucleotide (DNA) and amino-acids (Protein) sequences are supported.
– Tree-Space exploration under Parsimony and Likelihood criteria, Parametric
Progressive Tree Neighborhood (PPN) and PLL Rearrange Search, respec-
tively.
– A fully implementation of the GTR (General Time Reversible) model for DNA
datasets and all common substitution models for protein datasets, provided
by PLL Library.
– Partitioned analysis by assigning diﬀerent substitution models to each parti-
tion of the sequence alignment similar to RAxML.
– Rate heterogeneity across sites models: Discrete Γ with default 4 rate cate-
gories and CAT with N categories.
– Set of parsimony trees auto-generated by Step-wise Addition method to get
a representative sample of plausible initial trees.
– Creation of the Consensus Tree of the ﬁnal set of non-dominated solutions
(phylogenetic trees) generated by the algorithm, deﬁned from the number of
occurrences of bipartitions.
– Minimal command-line execution. (The conﬁguration parameters are kept to
a minimum).
– It is an open source project, which is hosted in: https://github.com/
KhaosResearch/MORPHY.
4
Phylogenetic and Multiobjective Performance
In this section, we describe our experimental methodology used to assess per-
formance of our proposal MORPHY and show the multiobjective and biological
results.
4.1
Multiobjective Performance
We have performed a preliminary comparative analysis with other optimiza-
tion metaheuristics implemented in the framework MO-Phylogenetics: NSGA-
II [9] and two other moderm techniques of the state-of-the-art: MOEA/D [17]
and SMS-EMOA [18], which are representative techniques of decomposition and
indicator-based algorithms, respectively. The problems of proteins to be solved
have been obtained from the public repository TreeBASE [19]: Matrix ID M510
with 57 sequences of 430 characters, Matrix ID M3807 with 82 sequences of 591
characters, Matrix ID M3810 with 55 sequences of 271 characters and Matrix ID

726
C. Zambrano-Vega et al.
M9973 with 60 sequences of 327 characters. We have used three multiobjective
quality indicators: the Inverted Generational Distance Plus or IGD+ (IIGD+)
to take into account both the convergence and diversity of the Pareto front
approximations, the Unary Additive Epsilon (Iϵ+) and the Spread or Δ (IΔ)
indicators, that are used as a complement to measure the degree of convergence
and diversity, respectively. As we are dealing with real-world optimization prob-
lems, the Pareto fronts to calculate these two metrics are not known, so we have
generated a reference Pareto front for each protein dataset by combining all the
non-dominated solutions computed in all the executions of all the algorithms.
All the algorithms were conﬁgured with the same values. Each independent
execution of each of the algorithms compute 10000 evaluations, the population
size is 100 individuals, the probability of crossover is set to 80% and mutation
20%. The substitution model parameters were adjusted for each protein dataset
using the software w-IQTRee [20].
Table 1. Median and interquartile range IQR of the values of the Iϵ+ indicator.
1.00e + 007.8e−01 1.00e + 000.0e+00 2.78e + 004.2e+00 1.00e + 001.0e+00
2.58e + 001.1e+01 9.92e + 009.1e+00 1.69e + 011.3e+01 1.08e + 011.1e+01
1.25e −016.3e−02 1.25e −017.8e−02 1.56e −019.4e−02 1.56e −016.3e−02
1.10e + 001.3e+00 2.27e + 000.0e+00 2.28e + 001.6e−01 2.27e + 007.5e−01
Table 2. Median and interquartile range IQR of the values of the IΔ indicator.
1.16e + 003.1e−01 1.36e + 006.2e−02 1.33e + 009.1e−02 1.17e + 002.3e−01
1.32e + 006.0e−02 1.30e + 002.1e−01 1.21e + 001.1e−01 1.25e + 002.2e−01
1.32e + 009.6e−02 1.36e + 006.3e−02 1.29e + 006.5e−02 1.21e + 002.0e−01
1.41e + 007.4e−01 9.50e −012.4e−02 1.17e + 007.7e−02 1.22e + 001.4e−01
Table 3. Median and interquartile range IQR of the values of the IIGD+ indicator.
8.18e −018.4e−01 8.18e −018.0e−02 2.33e + 003.9e+00 7.95e −011.2e+00
2.24e + 001.1e+01 9.49e + 009.1e+00 1.65e + 011.3e+01 1.03e + 011.1e+01
3.35e −022.0e−02 3.86e −022.7e−02 5.53e −025.0e−02 5.12e −022.9e−02
7.84e −011.3e+00 2.22e + 000.0e+00 2.22e + 003.0e−01 2.13e + 009.6e−01
For each combination of algorithm and protein dataset problem we have car-
ried out 20 independent runs, and we report the median, ˜x, and the interquartile
range, IQR, as measures of location (or central tendency) and statistical disper-
sion, respectively, for every considered indicators. The Tables 1, 2 and 3 illustrate
these results. When presenting the obtained values in tables, we emphasize with

Software MORPHY
727
a dark gray background the best result for each problem, and a clear grey back-
ground to indicate the second best result; this way, we can see at a glance the
most salient algorithms. We have to consider the highest values of IΔ and the
lowest of Iϵ+ and IIGD+.
The results illustrated by the Tables 1, 2 and 3 show that MORPHY is
the algorithm that oﬀers the best performance if we take into account the
Iϵ+ + and IGD+ indicators, since it obtains the best values in three of the
four selected problems. NSGA-II lies behind this algorithm and ahead of the
MOEA/D and SMS-EMOA techniques. However, these algorithms are the best
performers according to the quality indicator that measures diversity.
To check diﬀerences in these results are statistically signiﬁcant, we have
applied the unpaired Wilcoxon rank-sum test. A conﬁdence level of 95% (i.e.,
signiﬁcance level of 5% or p-value under 0.05) has been used in all cases. The
results of these tests have been summarized in tables where each cell contains
the results of this test for a pair of algorithms. Three diﬀerent symbols are used:
“–” indicates that there is no statistical signiﬁcance between these algorithms,
“▲” means that the algorithm in the row has yielded better results than the
algorithm in the column with statistical conﬁdence, and “▽” is used when the
algorithm in the column is statistically better than the algorithm in the row.
These results are detailed in Tables 4, 5 and 6 for the indicators ´Epsilon (IE+),
Spread (IΔ) and IGD+ respectively and conﬁrm that MORPHY yielded better
performance at 95% signiﬁcance level on the Iϵ+ and IIGD+ values, the best
performance of NSGAII for the IΔ indicator and that MOEA/D reports worst
performance overall the algorithms.
Table 4. Results of the Wilcoxon rank-sum test for the Iϵ+ values for the datasets
M510 - M3807 - M3810 - M9973.
NSGAII
MOEAD
SMSEMOA
MORPHY – ▲– ▲▲▲▲▲– – ▲▲
NSGAII
▲–
–
–
– – –
–
MOEAD
– – –
–
Table 5. Results of the Wilcoxon rank-sum test for the IΔ values for the datasets
M510 - M3807 - M3810 - M9973.
NSGAII
MOEAD
SMSEMOA
MORPHY ▲– – ▽▲▽–
▽–
– –
▽
NSGAII
▽▽▽▲▽– ▽▲
MOEAD
▽– –
–

728
C. Zambrano-Vega et al.
Table 6. Results of the Wilcoxon rank-sum test for the IGD+ values for the datasets
M510 - M3807 - M3810 - M9973.
NSGAII
MOEAD
SMSEMOA
MORPHY – ▲– ▲▲▲▲▲–
– ▲▲
NSGAII
▲–
▲–
–
– –
–
MOEAD
▽– –
–
4.2
Biological Results
With the aim of assess the biological results of our proposal, we have compared
MORPHY with others well-known phylogeny packages and biological methods
from the state-of-the-art in phylogenetics: the softwares IQ-Tree [20], RAxML
and PhyML. In Table 7 we detail the results analyzing the maximum likelihood
of inferred trees.
Table 7. Results of the scores of maximum likelihood between MORPHY and other
reference softwares.
Dataset IQ-TREE
RAxML
PhyML
MORPHY
M510
−8164.14
−8,165.75
−8,175.26
−8077.26
M3807
−38,706.56
−38,709.34 −38,722.00 −38,467.10
M3810
−11,061.54 −11,061.76 −11,061.55 −11,074.10
M9973
−13,077.17
−13,077.98 −13,077.18 −13,042.50
Table 7 shows that MORPHY ﬁnds maximum likelihood trees that can com-
pete with the ones achieved by the others reference softwares. MORPHY gener-
ates phylogenetic trees that dominate the reference scores for the M510, M3807
and M9973 datasets, but for the M3810 dataset, IQ-TREE software infers more
accuracy phylogenetic trees according to the maximum likelihood criterion.
4.3
Perfomance Example
With the aim of illustrating one example of the performance of our approach, we
have carried out some experiments over the M2926 Protein alignment extracted
from TreeBase and used by IQ-TREE [20], and we have obtained the following
results.
Figure 1 illustrates a Pareto front approximation generated from the set of
solutions (phylogenetic trees) inferred by MORPHY, in which the points of the
extremes represent the best scores of parsimony (left) and likelihood (right),
this score represents an improvement to the average likelihood score published
by IQTREE.

Software MORPHY
729
Maximum Parsimony
×104
1.98
1.981
1.982
1.983
1.984
1.985
1.986
1.987
1.988
1.989
Maximum Likelihood
×104
-8.482
-8.48
-8.478
-8.476
-8.474
-8.472
-8.47
-8.468
Best
Parsimony
Tree
Best
Likelihood
Tree
Fig. 1. Pareto Front Aproximations for M2926 Protein Dataset perfomed by MORPHY.
5
Discussion and Conclusions
MORPHY provides several advantages for those researchers working on phylo-
genetic analyses. It does not give a single phylogenetic tree as a ﬁnal result, but
a set of multiobjective optimized phylogenetic trees supported by parsimony and
likelihood criteria, yielding to alternative hypotheses that can be useful from dif-
ferent biological points of view. Within this set of solutions, biologists can ﬁnd
the best parsimony and likelihood trees represented by the extreme points of
the Pareto front approximation, instead of using diﬀerent phylogenetic software
for each optimization criterion. In fact, our experimental results indicate that
MORPHY provides competitive results against other phylogenetic tools based
on one single-criterion point of view.
Has been proved that applying separately the Parsimony and Likelihood
criterion, the biologist could obtain two phylogenies with relevants diﬀerencies
on evolutionary relationships, [21,22] reported have conﬂict between parsimony
and likelihood phylogenetic trees. So with the aim of deal this problem, our
multiobjective software provides a set of alternative phylogenies inferred that
are supported by both criteria and that can be obtanied from the middle of the
Pareto front.
These initial results indicate that the initial proposal of MORPHY obtains
competitive results, for which it is necessary to continue investigating in this
line. It should also be noted that only four problems have been used in this ﬁrst
study, which does not allow conclusive results either; a future work is to carry
out a more comprehensive study with a wider set of problems.
In conclusion, we provide to biologists a software tool that provides a novel
approach based on multiobjective optimization to generate a set of phylogenetics
trees attending to multiple criteria simultaneously, suggesting a trade-oﬀalter-
native evolutionary hypotheses, furthermore the results include the high-quality
parsimony and likelihood tree. The software supports DNA and Protein data,

730
C. Zambrano-Vega et al.
include common substitution models for DNA and protein, easy to use with a
simple command-line, open-access and multi-platform.
Acknowledgements. This work has been partially supported by the 4th convoca-
tion of Fondo Competitivo de Investigaci´on Cient´ıﬁca y Tecnol´ogica FOCICYT of the
Universidad T´ecnica Estatal de Quevedo from Ecuador, and Spanish Grants TIN2014-
58304-R (Ministerio de Ciencia e Innovaci´on, Spain), P11-TIC-7529 and P12-TIC-1519
(Plan Andaluz I+D+I - Junta de Andaluc´ıa, Spain).
References
1. Felsenstein, J.: Inferring Phylogenies. Palgrave Macmillan (2004). http://books.
google.fr/books?id=GI6PQgAACAAJ
2. Steel, M., Penny, D.: Parsimony, likelihood, and the role of models in molecular
phylogenetics. Mol. Biol. Evol. 17(6), 839–850 (2000)
3. Santander-Jim´enez, S., Vega-Rodr´ıguez, M.A.: A hybrid approach to parallelize a
fast non-dominated sorting genetic algorithm for phylogenetic inference. Concur-
rency and Computation: Practice and Experience (Apr 2014)
4. Santander-Jim´enez, S., Vega-Rodr´ıguez, M.A.: A multiobjective proposal based
on the ﬁreﬂy algorithm for inferring phylogenies. In: Evolutionary Computation,
Machine Learning and Data Mining. LNCS, vol. 7833, pp. 141–152. Springer,
Heidelberg (2013)
5. Edwards, A., Cavalli-Sforza, L., Heywood, V., et al.: Phenetic and phylogenetic
classiﬁcation. Systematic Association Publication No. 6, pp. 67–76 (1964)
6. Day, W.H., Johnson, D.S., Sankoﬀ, D.: The computational complexity of inferring
rooted phylogenies by parsimony. Math. Biosci. 81(1), 33–42 (1986)
7. Chor, B., Tuller, T.: Maximum likelihood of evolutionary trees: hardness and
approximation. Bioinformatics 21(suppl 1), i97–i106 (2005)
8. Swoﬀord, D., Olsen, G., Waddell, P., Hillis, D.: Phylogeny reconstruction. In:
Molecular Systematics, Chap. 11, 3rd edn., pp. 407–514. Sinauer (1996)
9. Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE T. Evol. Comp. 6(2), 182–197 (2002)
10. Zambrano-Vega, C., Nebro, A., Aldana-Montes, J.: Mo-phylogenetics: a phylo-
genetic inference software tool with multi-objective evolutionary metaheuristics.
Methods Ecol. Evol. 7(7), 800–805 (2016). https://doi.org/10.1111/2041-210X.
12529
11. L´opez-Camacho, E., Garc´ıa-Godoy, M.J., Nebro, A.J., Aldana-Montes, J.F.:
jMetalCpp: optimizing molecular docking problems with a C++ metaheuristic
framework. Bioinformatics 30(3), 437–438 (2014)
12. Flouri, T., Izquierdo-Carrasco, F., Darriba, D., Aberer, A., Nguyen, L.T., Minh,
B., Von Haeseler, A., Stamatakis, A.: The phylogenetic likelihood library. Syst.
Biol. 64(2), 356–362 (2015)
13. Dutheil, J., Gaillard, S., Bazin, E., Gl´emin, S., Ranwez, V., Galtier, N.,
Belkhir,
K.:
Bio++:
a
set
of
C++
libraries
for
sequence
analysis,
phy-
logenetics,
molecular
evolution
and
population
genetics.
BMC
Bioinfor-
matics
7,
188
(2006).
http://www.pubmedcentral.nih.gov/articlerender.fcgi?
artid=1501049&tool=pmcentrez&rendertype=abstract
14. Darriba, D., Taboada, G.L., Doallo, R., Posada, D.: jModelTest 2: more models,
new heuristics and parallel computing. Nat. Methods 9(8), 772–772 (2012)

Software MORPHY
731
15. Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes
in C: The Art of Scientiﬁc Computing, 2nd edn. (1992)
16. Go¨eﬀon, A., Richer, J.M., Hao, J.K.: Progressive tree neighborhood applied to the
maximum parsimony problem. IEEE/ACM Trans. Comput. Biol. Bioinform. 5(1),
136–45 (2008). http://www.ncbi.nlm.nih.gov/pubmed/18245882
17. Zhang, Q., Li, H.: MOEA/D: a multiobjective evolutionary algorithm based on
decomposition. IEEE Trans. Evolutionary Computation 11(6), 712–731 (2007)
18. Beume, N., Naujoks, B., Emmerich, M.: SMS-EMOA: Multiobjective selection
based on dominated hypervolume. Eur. J. Oper. Res. 181(3), 1653–1669 (2007)
19. Sanderson, M., Donoghue, M., Piel, W., Eriksson, T.: TreeBASE: a prototype
database of phylogenetic analyses and an interactive tool for browsing the phy-
logeny of life. Am. J. Bot. 81(6), 183 (1994)
20. Nguyen, L.T., Schmidt, H.A., von Haeseler, A., Minh, B.Q.: IQ-TREE: a fast and
eﬀective stochastic algorithm for estimating maximum-likelihood phylogenies. Mol.
Biol. Evol. 32(1), 268–274 (2015)
21. Macey, J.R.: Plethodontid salamander mitochondrial genomics: a parsimony eval-
uation of character conﬂict and implications for historical biogeography. Cladistics
21(2), 194–202 (2005). https://doi.org/10.1111/j.1096-0031.2005.00054.x
22. Burleigh, J.G., Mathews, S.: Assessing systematic error in the inference of seed
plant phylogeny. Int. J. Plant Sci. 168(2), 125–135 (2007)

Adaptive Harris Corner Detector Evaluated
with Cross-Spectral Images
Patricia L. Su´arez1(B), Angel D. Sappa1,2, and Boris X. Vintimilla1
1 Facultad de Ingenier´ıa en Electricidad y Computac´ıon, CIDIS,
Escuela Superior Polit´ecnica del Litoral, ESPOL,
Campus Gustavo Galindo, 09-01-5863 Guayaquil, Ecuador
{plsuarez,asappa,boris.vintimilla}@espol.edu.ec
2 Computer Vision Center, Ediﬁci O, Campus UAB, Bellaterra,
08193 Barcelona, Spain
Abstract. This paper proposes a novel approach to use cross-spectral
images to achieve a better performance with the proposed Adaptive Har-
ris corner detector comparing its obtained results with those achieved
with images of the visible spectra. The images of urban, ﬁeld, old-building
and country category were used for the experiments, given the variety of
the textures present in these images, with which the complexity of the
proposal is much more challenging for its veriﬁcation. It is a new scope,
which means improving the detection of characteristic points using cross-
spectral images (NIR, G, B) and applying pruning techniques, the com-
bination of channels for this fusion is the one that generates the largest
variance based on the intensity of the merged pixels, therefore, it is that
which maximizes the entropy in the resulting Cross-spectral images.
Harris is one of the most widely used corner detection algorithm, so
any improvement in its eﬃciency is an important contribution in the
ﬁeld of computer vision. The experiments conclude that the inclusion of
a (NIR) channel in the image as a result of the combination of the spec-
tra, greatly improves the corner detection due to better entropy of the
resulting image after the fusion, Therefore the fusion process applied to
the images improves the results obtained in subsequent processes such as
identiﬁcation of objects or patterns, classiﬁcation and/or segmentation.
Keywords: Near Infrared · Cross-spectral · Visible spectra · Pixel
Fusion · Pruning
1
Introduction
Computer vision tackles problems related with object detection and recognition,
texture classiﬁcation, action recognition, segmentation, tracking, data retrieval,
image alignment, just to mention a few. In general, computer vision solutions are
based on representing the given image using some global or local image properties
[1], and then comparing them using some similarity measure [2]. Additionally,
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_69

Adaptive Harris Corner Detector Evaluated with Cross-Spectral Images
733
Fig. 1. Spectrum Band
computer Vision plays a central role in human perception and interpretation of
the world. Our eyes and brain can quickly provide detailed information about
any event that is happening around us, leading to an appropriate choice of action
or response. The importance of human visual perception is also evident when
one considers that the vision processing consumes a proportionately large part
of the function of the human brain.
The most signiﬁcant recent advances in remote sensing has been the devel-
opment of Multi-spectral sensors and specialized programs to analyze the data
of the resulting images. Just ﬁfteen years ago remote sensing experts have had
access to the Multi-spectral imaging and specialized tools to take advantage of
the information they provide such images [3]. In the last decade the analysis of
multi- spectral imaging has matured and has become one of the fastest growing
technologies in the ﬁeld of remote sensing. The term “Multi” in Multi-spectral
means “many” and refers to the large number of wavelength bands that consti-
tute them. Multi-spectral images provide a wide spectral information to appre-
ciate characteristics that can not be seen in other spectral band. See spectrum
band in Fig. 1. As the volume of hyper-spectral data for planetary exploration
increases, eﬃcient yet accurate algorithms are decisive for their analysis. The
capability of spectral unmixing for analyzing hyper-spectral images from Mars
is now under investigation [4]. Hyper-spectral monitoring of large areas (more
than 10 km2) can be achieved via the use of a system employing spectrometers
and CMOS cameras. A robust and eﬃcient algorithm for automatically combin-
ing multiple, overlapping images of a scene to form a single composition (i.e., for
the estimation of the point-to-point mapping between views), which uses only
the information contained within the images themselves [5].

734
P. L. Su´arez et al.
Advances in remote sensing technologies are increasingly becoming more use-
ful for resource, ecosystem and agricultural management applications to the
extent that these techniques can now also be applied for monitoring of soil con-
tamination and human health risk assessment. While, extensive previous studies
have shown that Visible and Near Infrared Spectroscopy (VNIRS) in the spectral
range 400–2500 nm can be used to quantify various soil constituents simultane-
ously, the direct determination of metal concentrations by remote sensing and
reﬂectance spectroscopy is not as well examined as other soil parameters [6].
Multi-spectral images provide the potential for more accurate and detailed
information extraction and that is not possible with any other type of remote
sensing data. Remote sensing applications are usually applied in projects that
generally have one of the following objectives: Detection of targets (objects,
tumors, people, others), Mapping Materials, Tracing, Classiﬁcation, Segmenta-
tion, Mapping the surface properties in material identiﬁcation.
Digital image fusion has been used in research ﬁeld since the late nineties
at the leading edge of available technology. It formed a rapidly developing area
of research in remote sensing [7]. Recently, cross-spectral based approaches are
obtaining remarkable results in computer vision applications, as well as in a large
number of ﬁelds. For instance, using this king of images has been recently pre-
sented with success working on images in the mono-spectral or in cross-spectral
domain [8–10].
In order to prepare the samples of the fused images, the bands of the images
of the visible spectrum (R-red, V-green, B-blue) are separated and merged with
the near-infrared image. It is done the fusion of images with the combination
of channels (NIR, G, B) with which the best entropy is obtained. See Fusion
Process in Fig. 2. Thus it is possible to start the experiments and to use the
merged images for the detection of corners by means of the Adaptive Harris
algorithm and in such a way to demonstrate that there is an improvement of
the results of the detection of the characteristics of the images, a similar work
using cross-spectral imagery was performed to improve edge detection using
morphological operations [11].
Fig. 2. Cross-Spectral fusion process

Adaptive Harris Corner Detector Evaluated with Cross-Spectral Images
735
The corners can be deﬁned as the points with the lower similarity in all
directions, this can be measured by taking the sum of squares diﬀerences (SSD).
The Harris algorithm works by calculating a response function across all pixels of
the images. See Corner representation in Fig. 3. After that, those who exceed the
threshold of its local maximum are recognized as corners and they are retained.
In computer vision, it is necessary to establish matching points between dif-
ferent images, this allows us extract information and to take action on them.
When we talk about matching points we refer generally to the characteristics of
the scene that we need to recognize easily and uni. What are the most important
features, to name a few are: borders, regions and corners. But of all, the cor-
ners are the most important feature because being the intersection of two edges,
they represent a point where the direction changes, therefore, the gradient of the
image has a high variation, which is used to detect it.
According to [12] the traditional Harris Algorithm considers a grayscale
image I. We are going to sweep a window w(x,y) (with displacements u in
the x direction and v in the right direction) I and will calculate the variation
of intensity:
E(u, v) =

x,y
w(x, y)[I(x + u, y + v) −I(x, y)(2))],
(1)
where: w(x,y) is the window to the position (x,y), I(x,y) is the intensity at
(x,y) and I(x+u, y+v) is the intensity at the moved window (x+u, y+v).
Since we are looking for windows with corners, we are looking for windows
with a large variation in intensity. Hence, we have to maximize the equation
above, so using Taylor expansion, expanding the equation and canceling properly,
it can be expressed in a matrix form as:
E(u, v) ≈[u, v]
 
x,y
w(x, y)
 I2
x Ixy
Ixy I2
y
  
u
v

(2)
then let’s denote:
M =
 
x,y
w(x, y)
 I2
x Ixy
Ixy I2
y
 
(3)
So, the equation now is:
E(u, v) ≈[u, v]M

u
v

(4)
Harris algorithm works by calculating a response function (RF) through
all pixels of the image, after which, those who exceed the threshold, which is

736
P. L. Su´arez et al.
also known as a local maximum are retained as corners. Being (I) a 2D image
grayscale. Measure response of a corner:
R = det(M)−k(trace(M))2
(5)
where: det(M) = λ1, λ2, trace(M) = λ1 + λ2, λ1 and λ2 are the eigenvalues of
M and k are a empiric constant between 0.04 and 0.06.
R depends only on the eigenvalues of M. Therefore deﬁned as follows: If
R is large corresponds to a corner, if R is negative with large magnitude it
corresponds to an edge and if |R| is small corresponds to a ﬂat region. Harris
detection algorithm ﬁnds the points with the greatest values in the response
function corners (R) and working with a threshold. Points are taken with a local
maximum R. The quality of the detected corners depend on the threshold used
to discern them. A quite high threshold will detect only very strong corners,
while a too low threshold will detect many false corners, which are originated by
noisy points, this can be computationally expensive, which is why new variants
have been developed that allow to reduce the amount of valid information to be
processed in the process of detection of corners.
In this context, the current paper tackles a more robust feature extractor
using fused cross-spectral images and use a variation of Harris, it is a novel
low complexity pruning technique that removes the non-corners using simple
approximations of the complex Harris corner measure to create a small corner
candidate set [13], that allow to obtain more eﬃciently the principal key points
that correspond to a corner, and this make relevant the improvement to facilitate
process like object detection, image classiﬁcation, panoramic scenes creation and
3D generated image reconstruction. The rest of the paper is organized as follows.
Section 2 describes the most recent work on feature extraction based on remote
sensing for several estimations. Section 3 presents the Adaptive Harry Corner
Detection approach with Cross-Spectral images proposed. Section 4 depicts the
experimental results and ﬁnally, conclusion are presented in Sect. 5.
Fig. 3. Corner representation
2
Related Work
Remote sensing has become a major source of land use information to cover a
range of spatial scales and temporal scales. Recently, Some papers have focused

Adaptive Harris Corner Detector Evaluated with Cross-Spectral Images
737
on the improvement of the classiﬁcation process; others on the use of well-known
classiﬁcation methods in particular types of remote sensing application. Classiﬁ-
cation is regarded as a fundamental process in remote sensing, which lies at the
heart of the transformation from satellite image to usable geographic product
[14].
Traditional classiﬁcation techniques assigns each pixel to a single class, using
remote sensing images. In particular areas with high spatial resolutions, they
are commonly dominated by mixed pixels that contain more than one class in
the soil. For the sub-pixel information is needed to use techniques and soft sort-
ing algorithms in Multi-spectral domain order to obtain the fraction of each
class in a pixel. There are a proposed work based in a Linear spectral unmixing
which is a popular tool in remotely sensed hyper-spectral data interpretation. It
aims at estimating the fractional abundances of pure spectral signatures (also
called as endmembers) in each mixed pixel collected by an imaging spectrome-
ter [15]. Another approach, is the evaluation of an image that depend upon the
purpose for which the image was obtained and the manner in which the image
is to be examined. Where the goal is extraction of information and where the
image is to be processed prior to viewing, the information content of the image
is the only true evaluation criterion. Under these conditions, the improvement
achieved by processing can be evaluated by comparing the ability of the human
observer to extract information from the image before and after processing. The
extent to which the processing approaches the optimum can be evaluated by
determining the fraction of the total information content of the image which can
be visually extracted after processing [16]. Another technique it is based on a
set of high-resolution remote sensing images covering multiple spatial features,
they proposed an classiﬁcation based on unsupervised technique including pixel-
wise and sub-pixel-wise methods to detect possible built-up areas from remote
sensing images. The motivation behind is that the frequently recurring appear-
ance patterns or repeated textures corresponding to common objects of interest
in the input image data set can help us distinguish built-up areas from other
features [17]. Another paper proposes a froth image segmentation method com-
bining image classiﬁcation and image segmentation. In the method, an improved
Harris corner detection algorithm is applied to classify froth images ﬁrst. Then,
for each class, the images are segmented by automatically choosing the corre-
sponding parameters for identifying bubble edge points through extracting the
minimal local gray value. Finally, on the basis of the edge points, the bubbles
are delineated by using a number of post-processing functions. Compared with
the widely used Watershed algorithm and others for a number of lead zinc froth
images in a ﬂotation plant, the new method (algorithm) can alleviate the over-
segmentation problem eﬀectively.
Other approach proposed a multi-target tracking algorithm based on a par-
ticle ﬁlter framework that exploits a sparse distributed shape model to handle
partial occlusions. The state vector is composed by a set of points of interest
(i.e. corners) and it enables to jointly describe position and shape of the target.
An eﬃcient importance sampling strategy is developed to limit the number of

738
P. L. Su´arez et al.
Fig. 4. Pairs of images (1024 × 680 pixels) from [18]; country category (the ﬁrst
column), ﬁeld category (the second column); urban category (the third column) and
old-building category (the fourth column); (top) NIR images; (bottom) RBG images.
used particles and it is based on multiple Kanade-Lucas-Tomasi (KLT) feature
trackers used to estimate local motion [19]. The usage of cross-spectral informa-
tion, although interesting and appealing, implies new challenging and diﬃcult
problems that need to be tackled and eﬃciently solved. For instance, diﬀerent
works have been recently proposed for describing and matching feature points
in cross-spectral domains based on classical approaches. In the current work
we propose the use of Cross-Spectral images to improve the image entropy and
use and adaptive pruning variation before applied traditional Harris algorithm
to reduce the computational cost, improve the accuracy, because signiﬁcantly
reduces the selection and evaluation eﬀort for the presence of corners to only
corner like regions.
3
Proposed Approach
This section presents the approach proposed for improve the corners detection
process, based on the usage of a Cross-Spectral images and an enhanced pruning
technique before to apply the conventional Harris algorithm, we propose to use
them to achieve a better performance and accuracy in the extraction of features
and at the same time reduce the computational cost of the corner detection
process. This can be done because this pruning technique uses a new threshold
model where product of vertical and horizontal diﬀerence in pixel intensities is
used and the candidates with low CR (corner response) values are pruned away.
An adaptive corner response (CR) approach is deﬁned as:
CR =
 Ix · Iy

	
(6)
The Harris detector compute the corner measure on every image pixel and the
obvious non-corners are removed by applying a threshold on the corner measure.
The corner response of pixels that are close to a good corner are also typically
high and hence, a minimum distance is enforced between good corners using
non-maximal suppression (NMS). Finally all the corners are sorted and only the
top few corners are selected for further processing. The high computational load

Adaptive Harris Corner Detector Evaluated with Cross-Spectral Images
739
Fig. 5. Cross-Spectral image, resulting from the fusion process
Fig. 6. Cross-Spectral image corner detection
of the feature detection is mainly due to the complex corner measure with this
prune approach, according with [13], and eﬃcient discard non corners occurs,
which signiﬁcantly reduces the selection and evaluation eﬀort for the presence of

740
P. L. Su´arez et al.
Fig. 7. RGB image corner detection
Table 1. Accuracy from the proposed approach comparing with the traditional Har-
ris algorithm using RGB and Cross-Spectral images with all the processed image
categories.
Technique
Average diﬀerence in repeatability rate
country
ﬁeld
urban
old-
building
Traditional Harris with RGB image
4.22
4.47
3.68
3.84
Traditional Harris with Cross-Spectral
image
2.72
2.85
2.43
2.75
Pruning Harris with RGB image
2.12
2.05
2.32
2.07
Pruning Harris with Cross-Spectral
image
1.68
1.75
1.76
1.31
corners to only corner like regions. In Harris, the trace (M) term is introduced so
that edges can also be detected. Ignoring the trace(M) term, the det(M) term
alone is suﬃcient to select corner regions. Based on their observations, they
propose a pruning technique that approximates the det(M), (i.e. determinant of
the auto correlation matrix M) for selecting corner candidate pixel and after that
they apply the conventional Harris corner measure on only the corner candidate
set to extract the ﬁnal corners. With this, there is not a global compute for
the corner response on the entire image, hence potentially resulting in higher
computation time savings.

Adaptive Harris Corner Detector Evaluated with Cross-Spectral Images
741
Table 2. Average % Speed Up of from the proposed approach comparing with the tra-
ditional Harris algorithm using RGB and Cross-Spectral images with all the processed
image categories.
Technique
% Speed-Up
country
ﬁeld
urban
old-
building
Traditional Harris with Cross-Spectral vs
RGB image
28.37
29.51
28.75
23.41
Pruning Harris vs Traditional Harris
with RGB image
35.72
36.68
36.15
34.95
Pruning Harris vs Traditional Harris
with Cross-Spectral image
46.17
48.85
49.71
49.97
Pruning Harris with Cross-Spectral
versus RGB image
62.33
64.65
67.31
63.39
Pixels that maximize det(M) also have a high value for λ2. For a high value
of det(M), the pixel must have a large value for a corner response (CR). We
propose to choose only such pixels as corner candidates. Applying an appropriate
threshold can discard pixels with low (CR) values.
4
Experimental Results
The proposed approach has been evaluated using Cross-Spectral images obtained
from a fusion process (NIR, G, B), Fig. 5 shows an example of the fusion process,
and also applying a pruning technique obtained from the equation presented
above, The cross-spectral data set came from [18]. The country, urban, old-
building and ﬁeld categories have been considered for evaluating the performance
of the proposed approach, examples of this dataset country, field, urbanandold−
buildingcategory are presented in Fig. 4. This dataset consists of 477 registered
images categorized in 9 groups captured in RGB (visible spectrum) and NIR
(Near Infrared spectrum). The country category contains 52 pairs of images of
(1024 × 680 pixels), the urban category contains 58 pairs of images of (1024 ×
680 pixels), The old-building category contains 51 pairs of images of (1024 × 680
pixels) while the ﬁeld contains 51 pairs of images of (1024 × 680 pixels). In order
to make the experiments 650 pairs of cross-spectral images from each of these
categories has been generated from their corresponding (RGB-NIR) images. It
should be noted that images are correctly registered, so that a pixel-to-pixel
correspondence is guaranteed.
The parameters that we use in the Harris algorithms was a Gaussian window
with W = 3 × 3 kernel and σ = 0.3 as the baseline algorithms. For the proposed
technique, the pruning algorithm is ﬁrst applied to the entire image in order
to select the corner candidate set. Next, the corner measure of the correspond-
ing baseline algorithm is applied to extract the ﬁnal corners. We compare the

742
P. L. Su´arez et al.
obtained results from the traditional Harris algorithm, using the Cross-spectral
images, the original RGB images, an example of Harris detector using RGB
image is showed in Fig. 7 and also applying the pruning technique in terms of
the accuracy and timing. We use aﬃne image transformations such changes in
viewpoint, scale, rotation and illumination. For all images, we apply the thresh-
olds on the corner response so that the ﬁnal corner measures (λ2 or R) could
obtained a better accuracy as shows the table results. An example of Harris
detector using Cross-Spectral image is showed in Fig. 6.
For this experiment of Harris corner evaluation we used a 3.2 eight core
processor with 16 GB of memory with a NVIDIA GeForce GTX970 GPU. The
accuracy of the ﬁnal corners extracted is evaluated using the repeatability rate,
which is deﬁned as the number of points repeated between two images with
respect to the total number of detected points. Despite of a large image data
sets evaluated, a notable speedup of approximately a 30% over the traditional
Harris detector is still observed, when the pruning technique is used.
Table 1 presents the results obtained with the four techniques used to eval-
uated the accuracy and the Table 2 shows the results for the speed-up for each
category. It can be appreciated that the pruning technique reaches the best
results in all cases.
5
Conclusion
This paper tackles the challenging problem of improve the feature detector algo-
rithm, in this case, evaluating the Harris corner detector algorithm, using Cross-
Spectral images, in combination with a pruning technique to obtained a better
accuracy and reduce computational cost.
We have evaluated a low cost pruning technique to accelerate the Harris
corner detectors by using an approximate corner indicator derived from the con-
ventional corner measure. Evaluations for repeatability showed that the corner
candidates selected by the proposed pruning technique include most of the cor-
ners found by the baseline detectors. The approximate measure used for pruning
allows high thresholds to be applied to remove non corner regions, while retain-
ing a signiﬁcant amount of corners. This facilitates the selection of a small but
near-complete set of corner candidates, which results in signiﬁcant computation
savings on corner response evaluation. Experimental results demonstrate that
the proposed technique achieves signiﬁcant speedup in all the experiments real-
ized. The pruning technique is well suited for high performance and low cost
embedded systems. Our future work will focus on improve the thresholding pro-
cess to reach more accuracy.
Acknowledgment. This work has been partially supported by the ESPOL under
projects PRAIM and KISHWAR.

Adaptive Harris Corner Detector Evaluated with Cross-Spectral Images
743
References
1. Sappa, A.D., Vintimilla, B.X.: Cost-based closed-contour representations. J. Elec-
tron. Imaging 16, 023009–023009 (2007)
2. Ricaurte, P., Chil´an, C., Aguilera-Carrasco, C.A., Vintimilla, B.X., Sappa, A.D.:
Feature point descriptors: Infrared and visible spectra. Sensors 14, 3690–3701
(2014)
3. Shaw, G.A., Burke, H.K.: Spectral imaging for remote sensing. Lincoln Lab. J. 14,
3–28 (2003)
4. Ceamanos, X., Dout´e, S., Luo, B., Schmidt, F., Jouannic, G., Chanussot, J.: Inter-
comparison and validation of techniques for spectral unmixing of hyperspectral
images: a planetary case study. IEEE Trans. Geosci. Remote Sens. 49, 4341–4358
(2011)
5. Moroni, M., Dacquino, C., Cenedese, A.: Mosaicing of hyperspectral images: the
application of a spectrograph imaging device. Sensors 12, 10228–10247 (2012)
6. Bruce, D.A., Owens, G.P., Maliki, A., Ajeel, A.A., et al.: Capabilities of remote
sensing hyperspectral images for the detection of lead contamination: a review
(2012)
7. Pohl, C., Van Genderen, J.L.: Review article multisensor image fusion in remote
sensing: concepts, methods and applications. Int. J. Remote Sens. 19, 823–854
(1998)
8. Zagoruyko, S., Komodakis, N.: Learning to compare image patches via convo-
lutional neural networks. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4353–4361 (2015)
9. Su´arez, P.L., Sappa, A.D., Vintimilla, B.X.: Cross-spectral image patch similar-
ity using convolutional neural network. In: 2017 IEEE International Workshop of
Electronics, Control, Measurement, Signals and their Application to Mechatronics
(ECMSM), pp. 1–5. IEEE (2017)
10. Aguilera, C.A., Aguilera, F.J., Sappa, A.D., Aguilera, C., Toledo, R.: Learning
cross-spectral similarity measures with deep convolutional neural networks. In: The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Work-
shops, p. 9. IEEE (2016)
11. Su´arez, P.L., Villavicencio, M.: Canny edge detection in cross-spectral fused images.
In: 2017 International Conference on Information Systems and Computer Science,
ENFOQUE UTE (INCISCOS), pp. 16–30. LATINDEX (2017)
12. Harris, C., Stephens, M.: A combined corner and edge detector. In: Alvey Vision
Conference, Manchester, UK, vol. 15, pp. 10–5244 (1988)
13. Wu, M., Ramakrishnan, N., Lam, S.K., Srikanthan, T.: Low-complexity pruning for
accelerating corner detection. In: 2012 IEEE International Symposium on Circuits
and Systems (ISCAS), pp. 1684–1687. IEEE (2012)
14. Wilkinson, G.G.: Results and implications of a study of ﬁfteen years of satellite
image classiﬁcation experiments. IEEE Trans. Geosci. Remote Sens. 43, 433–440
(2005)
15. Iordache, M.D., Bioucas-Dias, J.M., Plaza, A.: Sparse unmixing of hyperspectral
data. IEEE Trans. Geosci. Remote Sens. 49, 2014–2039 (2011)
16. Harris, J.L.: Image evaluation and restoration. JOSA 56, 569–574 (1966)
17. Pavithra,
A.S.:
Agile
segmentation
and
classiﬁcation
for
hyper
spectral
image using Harris Corner Detector. Int. J. Sci. Res. 4, 196–199 (2015)

744
P. L. Su´arez et al.
18. Brown, M., S¨usstrunk, S.: Multi-spectral SIFT for scene category recognition. In:
2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
177–184. IEEE (2011)
19. Dore, A., Beoldo, A., Regazzoni, C.S.: Multitarget tracking with a corner-based
particle ﬁlter. In: 2009 IEEE 12th International Conference on Computer Vision
Workshops (ICCV Workshops), pp. 1251–1258. IEEE (2009)

JavaScript Middleware for Mobile
Agents Support on Desktop
and Mobile Platforms
Carlos Silva1,2(&), Nuno Costa1, Carlos Grilo1, and Jorge Veloz2
1 School of Technology and Management, Polytechnic Institute of Leiria,
Leiria, Portugal
2162317@my.ipleiria.pt,
{nuno.costa,carlos.grilo}@ipleiria.pt
2 Universidad Técnica de Manabí, Portoviejo, Ecuador
jorge.veloz@fci.edu.ec
Abstract. The evolution of technology in interconnection solutions such as
Networks or the Internet, have allowed many communication architectures to be
born and a varied interconnectivity. Here, we present a project that relies on the
mobile agent computing paradigm. A middleware using the JavaScript language
that allows the execution and ability to move mobile agents through the local
network and Internet. This initiative arose as a way of dealing with problems
raised by the considerable amount of existing Java based mobile agents mid-
dleware, which force the installation of the Java Virtual Machine in the devices,
making complicated its execution in operating systems like macOS, iOS and
others non-java friendly O.S. Our middleware works steadily in all operating
systems, requiring only the installation of node.js. For mobile platforms the
middleware is developed using React-native that allows it to run on mobile
operating systems such as Android and iOS.
Keywords: Middleware  Mobile agents  JavaScript  node.js
React-native  Mobile operating systems
1
Introduction
In distributed computing, code mobility is the ability of running programs, where code
or objects migrate (or move) from one machine or application to another [1]. This is the
process of moving mobile code across the nodes of a network as opposed to the
request/response model of distributed computation where the data is moved. For
example, an application can send an object to another machine, and the object can
resume execution inside the application on the remote machine with the same state as it
had in the originating application.
Traditionally, applications in distributed systems have been structured using the
Client/Server paradigm (request/response model), in which client and server processes
communicate either through message passing or Remote Procedure Calls (RPC). This
communication model is usually synchronous, i.e., the client suspends itself after
sending a request to the server, waiting for the results of the call. An alternative
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_70

architecture is called Remote Evaluation (REV). In REV the client, instead of invoking
a remote procedure, sends its own procedure code to a server, and requests the server to
execute it and return the results. The mobile agent paradigm is an evolution of these
antecedents. A mobile agent is a program which represents a user in a network and is
capable of migrating autonomously from node to node, performing computations on
behalf of that user. The main advantages of the mobile agent paradigm lie in its ability
to move client code and computation to remote server resources, and in permitting
increased asynchrony in client server interactions [2].
Software agents can be used for information searching, ﬁltering and retrieval, or for
electronic commerce on the Web, thus acting as personal assistants for their owners.
Software agents can also be used in low-level network maintenance, testing, fault
diagnosis, and for dynamically upgrading the capabilities of existing services.
During the investigation of middleware and mobile agents, we have noticed that
there are many middleware packages developed in Java and some in other program-
ming languages that are no iOS and MacOS friendly. For such reasons we set out to
develop a middleware in JavaScript and create mobile agents to test the functionality of
developed middleware. The tests done in real and virtual machines, with different
operating systems, such as Windows, Linux, macOS and mobile operating system,
such as Android and iOS, took advantage of the fact that the JavaScript language is
able to run on different operating systems, using node.js for desktop operating systems
and react-native for mobile platforms.
The purpose of this research is to develop a Middleware that works on all platforms
regardless the operating systems. The Middleware must be able to execute the func-
tionalities for which it was created, such as the interpretation and sending of a mobile
agent through the local network and Internet.
Throughout this document, the state of the art is presented and discussed in Sect. 2.
Section 3 shows middleware speciﬁcations and architecture. In Sect. 4 the imple-
mentation is presented with real and virtual machines. Section 5 shows the test’s
obtained results. Finally, conclusions and future work are presented in Sect. 6.
2
State of the Art
In the past decade, many mobile agent platforms have been developed. Some of the
well-known mobile agent platforms include Mole [3], Aglets [4], Concordia [5],
D’Agents [6], Ara [7], TACOMA [8], JADE [9, 10], Ajanta [11], Tryllian’s agent
development kit [12], Fipa-os [13], grasshopper [14], jack intelligent agent [15] and
zeus [16]. However, most of them, such as Mole, Aglets, Concordia and JADE, were
developed to support only Java mobile agents. D’Agents supports mobile agents
written in Tcl, Scheme and Java, whereas Ara supports those written in Tcl, C, C++
and Java. TACOMA was originally developed to support Tcl mobile agents, and it was
subsequently extended to support mobile agents written in multiple languages
including C, Tcl, Perl, Python and Scheme.
746
C. Silva et al.

In the research work [17] the authors performed some evaluations of platforms for
mobile agents. The used criteria were the following:
– Communication: support for inter-platform messaging.
– Agent mobility: strong (ability of system to migrate code and execution state of
executing unit), weak (migration of code only).
– Clean, efﬁcient method of migrating, threads needed to be recreated/restarted by an
awaiting daemon.
– Availability.
– Usability and documentations: user and developer level of acceptance.
The authors concluded that, among the evaluated frameworks for mobile agents, the
best are: Grasshopper, JADE and Aglets, according to the evaluation criteria (priority
as in the order).
2.1
Middleware and Frameworks
2.1.1
Java Agent Development Framework (JADE)
JADE (Java Agent Development framework) is a software framework to aid the
development of agent applications in compliance with the FIPA (Foundation for
Intelligent Physical Agents) speciﬁcations for interoperable intelligent multi-agent
systems. JADE is an Open Source project, and the complete system can be downloaded
from the JADE home page [18].
JADE is fully developed in Java and, from a software engineering standpoint,
values highly, among others, the following driving principles [19].
JADE includes both the libraries (i.e. the Java classes) required to develop appli-
cation agents, and the run-time environment that provides the basic services and that
must be active on the device before agents can be executed. Each instance of the JADE
run-time is called container (since it ‘‘contains’’ agents). The set of all containers is
called platform and provides a homogeneous layer that hides to agents (and to appli-
cation developers as well) the complexity and the diversity of the underlying tiers
(hardware, operating system, type of network, JVM). From a functional point of view,
JADE provides the basic services for distributed peer-to-peer applications in the wired
and mobile environment. JADE allows each agent to dynamically discover other agents
and to communicate via the peer-to-peer paradigm. From an application point of view,
each agent is identiﬁed via a unique name and it provides a set of services. It can
register and modify its services and/or search for agents providing a given service; it
can also control its life cycle and, in particular, communicate with all other peers [19].
Agents communicate by asynchronous message exchange, a communication model
almost universally accepted for distributed and loosely coupled interactions, i.e.
between heterogeneous entities [20]. In order to communicate, an agent just sends a
message to a destination. Agents are identiﬁed by a name, therefore the send operation
does not need the destination object reference and, as a direct consequence, there is no
temporal dependency between communicating agents. The sender and the receiver
could be available at different times. The receiver may not even exist (or not yet exist)
or could not be directly known by the sender that can specify a property (e.g. ‘‘all
agents interested in football’’) as a destination. Because agents identify each other only
JavaScript Middleware for Mobile Agents Support
747

by name, any change at run-time of their object reference is fully transparent to
applications [19].
2.1.2
Aglets Software Development Kit
The Aglets Software Development Kit (ASDK) [21] is a Java mobile agent platform
and library that eases the development of agent based applications. An aglet is a Java
agent able to autonomously and spontaneously move from one host to another.
Aglets includes both a complete Java mobile agent platform, with a stand-alone
server called Tahiti, and a library that allows developer to build mobile agents and to
embed the Aglets technology in their applications.
2.1.3
JACKTM Intelligent Agent
JACKTM Intelligent Agent [22] is an agent oriented development environment fully
integrated with the Java programming language. JACK provides agent-oriented
extensions to the Java programming language. JACK has a development environment
and provides a set of classes, which are available to develop JACK applications.
The JACK source code is ﬁrstly compiled into regular Java code before being
executed, which is then complied into Java virtual machine code to run on the target
system. JACK implements the FIPA standards and for communication needs DCI
network for communication; similar to TCP/IP it needs one process running as a
name-server. Jack does not offer mobility.
2.1.4
Ajanta
Ajanta is a system for programming agent based applications over the Internet. The
main focus of the Ajanta design is on mechanisms for secure and robust executions of
mobile agents in open systems. Agents in this system are active mobile objects, which
encapsulate code and execution context along with data [17].
Ajanta is a product of the Department of Computer Science, University of Min-
nesota. The binary is free but available upon request, Ajanta does no implement
standards. For Communication it uses Java method invocation, authenticated RMI,
ATP, agent collaboration XML and offers weak mobility through Java serialization
(byte code) [17].
2.2
Applications
2.2.1
Mobile Virtual Butler to Bridge the Gap Between Users
and Ambient Assisted Living Irtual Butler
In the project [23] was developed by our team and relies on the mobile agent com-
puting paradigm in order to create a Virtual Butler that provides the interface between
the elderly and the smart home infrastructure. The Virtual Butler is receptive to user
questions, also capable of interacting with the user whenever it senses that something
has gone wrong, notifying next of kin and/or medical services, etc.
Virtual Butler resorts to the “follow me” approach of mobile agents. For this
purpose, it is receptive to user voice commands, informing and alerting users by voice
synthesis. It can also collect state and ‘emotions’ in order to enrich the facts database,
even when the users are outside their instrumented smart home.
748
C. Silva et al.

The middleware platform used for this work was JADE and the mobile agents were
developed using the Java language. However, the mobile agent that runs on the mobile
platform has severe limitations and it is not the same one that runs in the in-home
computers. This was the main reason that pushed us to develop mobile agents mid-
dleware based on JavaScript, is a mobile platform friendly language.
3
The JavaScript Middleware
The developed JavaScript Middleware allows the execution of mobile JavaScript
agents and the ability to move it through the network. Figure 1 shows a set of devices
in which the middleware is running. The middleware establishes a connection with the
server (Registry), and middleware runs the mobile (test) agent on each connected
device, both desktop computers and mobile devices. The Registry server is developed
in JavaScript language too. The communication between the server and the clients
(Middleware) is conceived through the TCP/IP protocol.
The developed middleware works as a distributed software abstraction layer, which
sits between the application layers (agents) and the operating system and network
layers. Figure 2 shows the complete software stack including the hardware layer.
The middleware was developed in the JavaScript language and is executed by node.
js. It uses modules such as ‘fs’ to perform operations on the ﬁle system, ‘Web Socket’
for communication and module ‘serialize’ to perform serialization and unserialization
Fig. 1. Mobile agent system architecture.
JavaScript Middleware for Mobile Agents Support
749

operations. All these modules/libraries can be installed through the package manager
“npm”.
The middleware for mobile devices is also developed in JavaScript but supported
by react-native, allowing it to be compiled in native applications for mobile operating
systems, such as Android and iOS operating systems.
The Middleware JS layer abstracts all the required communication among hosts. It
supplies functions such as MoveTo() and GetAvailableHosts() useful for agents to
know (if required) the nodes where the middleware is installed and running. All
communication between agent and middleware and between middleware and agent is
abstracted by the API layer. For now the API supplies only the functions MoveTo() and
GetAvailableHosts(). When the agent is started, the middleware stores in the ﬁle
system the mobile agent for the case of an error occur or the middleware closes
unexpectedly, adding the characteristic of data persistence to the mobile agent. The
middleware has the characteristic of moving the mobile agent through the network.
Once the sending host receives the conﬁrmation message that the mobile agent was
successfully received and executed on the destination host, the origin host delete the
mobile agent from the memory and ﬁle system. As last action, it notiﬁes the server
about the operation performed in relation to the mobile agent and the hosts involved.
Figure 3 shows the sending process of the mobile agent through the network by the
middleware.
4
Implementation
In a ﬁrst stage, the sending of mobile agents was done following a point-to-point
approach and had the characteristic of ﬁnding other computers in the network that were
running the middleware and automatically created a list of available devices, being
Fig. 2. The complete software stack.
750
C. Silva et al.

shared for all hosts of the network. However, there were limitations, such as the
following:
– It was only possible to send the mobile agent to the nodes that were within the same
local network.
– The search of installed Middlewares; in the network was not efﬁcient, because the
devices could be in different network segments.
– It was not possible to send the agent through the Internet. Also, it is possible that
two computers have the same (private) IP address, because they may belong to
different local networks.
As a measure to solve these constraints, it was decided that the middleware should
communicate with a central server (Registry server), which would do the proxy
function, allowing to send mobile agents through available nodes. Besides, this will
also allow, as future work the creation of a framework to be able to communicate and
interact with the existing mobile agents in the network, the creation of new agents, as
well as the remote control execution.
The requirements for the execution of the middleware for desktop operating sys-
tems are as follows:
– To have installed the runtime environment for JavaScript (node.js) version 6.11 or
higher;
– To have installed the package manager for JavaScript (npm) version 3.10 or higher.
For mobile devices it is not necessary to have something installed, because the
middleware is developed with the React-native Facebook Framework [24] that allows
to develop native mobile application in JavaScript language, for Android and iOS
operating systems.
Fig. 3. Middleware functionality.
JavaScript Middleware for Mobile Agents Support
751

To run the middleware on mobile devices, it is only necessary to open the appli-
cation installed on the device. However, to start it on desktop computers, it is necessary
to run the following command on the console line or transform it in a daemon:
The developed middleware has the following options:
For example, to load the mobile agent into memory and execute it in the device, it is
necessary to execute the following:
As the middleware uses the object-oriented paradigm, mobile agents should contain
the following:
– An attribute “name”, which stores the agent’s name;
– An attribute “type”. It is mandatory that it contains the value “Agent”;
– And a function called “runAgent”. This function is mandatory.
The structure for a mobile agent: ‘helloworld.js’ would be the following:
5
Test and Results
The middleware functionality was tested out in real and virtual environments, using
virtual machines running on VMware Fusion v8.5.6. A total of two real machines, one
virtual machines and two mobile devices, were used Windows, Linux, MacOS, iOS
and Android (See Table 1).
752
C. Silva et al.

Tests were carried out to verify the following:
– Is the middleware sending the mobile agent through the network successfully?
– What happens if the device that is running the middleware loses the connection with
the Registry server?
– Can two or more mobile agents move through the network correctly, without
affecting one another?
– What happens to the mobile agent if the middleware closes unexpectedly?
During the tests it was possible to verify the correct functioning of the developed
middleware, as well as loading, running and moving through the mobile agent network.
The middleware’s characteristic of storing the mobile agent in the device ﬁle
system, allowed to recover the mobile agent in its last state, which is possible under
circumstances such as: unexpected closure of the middleware, an operating system
failure or restart, etc.
The middleware provides a quality of delivery service when sending a mobile agent
from one host to another, allowing in this way not to lose the agent due to problems,
such as loss of communication or other events not controlled by the Middleware.
6
Conclusions and Future Work
As stated in the state of the art section, there is a lack of mobile agents platforms that
could run in mobile platforms and this work is the kick-off to fulﬁll that need.
The developed middleware, not only moves agents among various different hard-
ware hosts (including mobile ones), but it is also able to recover the agent in case of
middleware or hardware failure. We also conducted tests with success that included
speech recognition at the agent level, which means that this developed middleware
could be very soon a valid replacement for JADE in the Mobile Butler [17] project. By
using the API layer, we have decoupled agent(s) from the middleware, hence, it is
possible to run n agents per device and move n agents through the network.
As future work, we intend to add new and useful functions to the API layer and
develop a framework to aid programmers in the development, execution and control of
mobile agents. Another future task could be the testing of the middleware in other
operating systems, such as Centos, Debian and on mobile operating systems, such as
Windows Phone, Firefox OS, Symbian, Maemo and others.
Table 1. Description of used devices.
Device
Operative system
Platform version used
Desktop macOS Sierra 10.12.4
Node.js v6.11.1
Desktop Windows 10.0 (64-bit, Build 14393) Node.js v6.11.1
Desktop Ubuntu 16.04.2 LTS
Node.js v6.11.1
Mobile
iOS 10.3.1
React-native 0.46.11
JavaScript Middleware for Mobile Agents Support
753

References
1. Fuggetta, A., Picco, G.P., Vigna, G.: Understanding code mobility. IEEE Trans. Softw. Eng.
24(5), 342–361 (1998)
2. Harrison, C., Chess, D., Kershenbaum, A.: Mobile Agents: Are they a good idea? Technical
report, IBM Research Division, T. J. Watson Research Center (1995)
3. Baumann, J., Hohl, F., Rothermel, K., Strasser, M., Theilmann, W.: MOLE: a mobile agent
system. Softw. Pract. Exp. 32(6), 575–603 (2002)
4. Lange, D.B., Oshima, M.: Programming and Deploying Java Mobile Agents with Aglets.
Addison-Wesley, MA (1998)
5. Wong, D., Paciorek, N., Walsh, T., DiCelie, J., Young, M., Peet, B.: Concordia: an
infrastructure for collaborating mobile agents. In: Rothermel, K., Popescu-Zeletin, R. (eds.)
MA 1997. LNCS, vol. 1219, pp. 86–97. Springer, Heidelberg (1997). https://doi.org/10.
1007/3-540-62803-7_26
6. Gray, R.S., Cybenko, G., Kotz, D., Peterson, R.A., Rus, D.: D’Agents: applications and
performance of a mobile–agent system. Softw. Pract. Exp. 32(6), 543–573 (2002)
7. Peine, H.: Application and programming experience with the ara mobile agent system.
Softw. Pract. Exp. 32(6), 515–541 (2002)
8. Johnansen, D., Lauvset, K.J., van Renesse, R., Schneider, F.B., Sudmann, N.P., Jacobsen,
K.: A TACOMA retrospective. Softw. Pract. Exp. 32(6), 605–619 (2002)
9. Bellifemine, F., Caire, G., Poggi, A., Rimassa, G.: JADE: a software framework for
developing multi-agent applications. Lessons learned. Inform. Softw. Technol. 50(1–2), 10–
21 (2008)
10. JADE - Java Agent Development Framework. http://jade.tilab.com/
11. Tripathi, A.R., Karnik, N.M., Ahmed, T., Singh, R.D., Prakash, A., Kakani, V., Vora, M.K.,
Pathak, M.: Design of the Ajanta system for mobile agent programming. J. Syst. Softw. 62
(2), 123–140 (2002)
12. Tryllian’s. https://www.trillian.im/eula/
13. Emorphia. http://ﬁpa-os.sourceforge.net/index.htm
14. Baumer, C., Breugst, M., Choy, S., Magedanz, T.: Grasshopper—a universal agent platform
based on OMG MASIF and FIPA standards. In: First International Workshop on Mobile
Agents for Telecommunication Applications (MATA 1999), pp. 1–18. Sn, October 1999
15. Howden, N., Rönnquist, R., Hodgson, A., Lucas, A.: JACK intelligent agents- summary of
an agent infrastructure. In: 5th International Conference on Autonomous Agents, May 2001
16. Nwana, H.S., Ndumu, D.T., Lee, L.C., Collis, J.C.: ZEUS: a toolkit and approach for
building distributed multi-agent systems. In: Proceedings of the Third Annual Conference on
Autonomous Agents, pp. 360–361. ACM, April 1999
17. Nguyen, G., Dang, T.T., Hluchy, L., Balogh, Z., Laclavik, M., Budinska, I.: Agent platform
evaluation and comparison. Rapport technique, Institute of Informatics, Bratislava, Slovakia
(2002)
18. The JADE Project home page. http://sharon.cselt.it/projects/jade
19. Bellifemine, F., Caire, G., Poggi, A., Rimassa, G.: JADE: a software framework for
developing multi-agent applications. Lessons learned. Inf. Softw. Technol. 50(1), 10–21
(2008)
20. Bellifemine, F., Caire, G., Poggi, A., Rimassa, G., Jade, A.: A white paper. Telecom Italia
EXP magazine, vol. 3 (2008)
21. Aglet community. http://aglets.sourceforge.net/
754
C. Silva et al.

22. Howden, N., Rönnquist, R., Hodgson, A., Lucas, A.: JACK intelligent agents-summary of
an agent infrastructure. In: 5th International Conference on Autonomous Agents, May 2001
23. Costa, N., Domingues, P., Fdez-Riverola, F., Pereira, A.: A mobile virtual butler to bridge
the gap between users and ambient assisted living: a smart home case study. Sensors 14(8),
14302–14329 (2014)
24. React-native. https://facebook.github.io/react-native/
JavaScript Middleware for Mobile Agents Support
755

Analyzing UAV-Based Remote Sensing
and WSN Support for Data Fusion
Ramón Alcarria1(&), Borja Bordel2, Miguel Ángel Manso1,
Teresa Iturrioz1, and Marina Pérez3
1 Department of Topographic Engineering and Cartography,
Universidad Politécnica de Madrid, Avenida del Mediterráneo km 7,
28031 Madrid, Spain
{ramon.alcarria,m.manso,teresa.iturrioz}@upm.es
2 Department of Telematics Systems Engineering,
Universidad Politécnica de Madrid, Avenida Complutense n. 30,
28040 Madrid, Spain
borja.bordel@upm.es
3 Department of Physical Electronics, Universidad Politécnica de Madrid,
Avenida Complutense n. 30, 28040 Madrid, Spain
marina.perez@isom.upm.es
Abstract. Recent developments in remote sensing are signiﬁcantly contribut-
ing to exploration and data acquisition through improved efﬁciency and risk
reduction. Unmanned Aerial Vehicles (UAV) are involved in a wide range of
remote sensing applications, as they are rapid, efﬁcient and ﬂexible acquisition
systems. They represent a valid alternative or a complementary solution to
satellite or airborne sensors. Wireless Sensor Networks (WSN), on the other
hand, have proliferated signiﬁcantly in recent years thanks to their timely, cheap
and extremely rich data acquisition capacity with respect to other acquisition
systems. This paper analyzes current state of the art in UAV-based remote
sensing and WSN support for the generation of integrated data. An architecture
is proposed for the combination of these technologies and the data acquisition,
communication, fusion, and presentation processes are analyzed along with the
proposal of future challenges.
Keywords: Wireless Sensor Networks (WSN)
Remote sensing  Survey  Data fusion  Unmanned Aerial Vehicles (UAV)
1
Introduction
Currently, spatial information representation uses traditional techniques of data capture
and communication, such as photogrammetric restitution, tiling or meshing of terrain
cartography, and the query of spatial attributes through standardized interfaces [1].
With the advent of new methods for information capture such as Unmanned Aerial
Vehicles (UAV), and new information sources such Wireless Sensor Networks
(WSN) new concepts related to data integration appear, such as data fusion [2]. Data
fusion is deﬁned as the integration of multiple data sources to produce more consistent,
accurate, and useful information than that provided by any individual data source.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_71

On the other hand, UAV are beginning to be used in multiple sectors, since they
solve some complications associated to other traditional remote sensing solutions such
as satellite return times, plane ﬂight logistics, cloud cover, and atmospheric effects on
imagery [3]. Current applications for UAV include public protection [4], natural dis-
aster monitoring [5], environmental monitoring [6], precision agriculture [7], water
resource management [8], etc.
The technical characteristics of the UAV allow them to take the mobile commu-
nications to another level. These vehicles are non-static, deployable, they can carry
ﬂexible payloads, they can be re-programmable in mission and they can measure
multiple variables, without space limitations, depending on the measurement device
they carry (LiDAR, microelectromechanical systems, chemical sensors, etc.).
Wireless sensor networks (WSN) have also been used to capture information in
numerous scenarios such as precision agriculture [9], critical infrastructure surveillance
[10], or environmental monitoring [11].
However, few works proposed a convergence of these two ﬁelds (remote sensing
and WSN). This paper analyzes the state of the art in UAV-Based Remote Sensing and
WSN support from the point of view of data fusion, which emphasizes the increase in
value by combining different sources of information.
The paper is structured as follows. Section 2 describes a general architecture for
UAV-based remote sensing and WSN support. Section 3 analyses some works
regarding data acquisition and communication. Sections 4 and 5 describe data fusion
and presentation, and ﬁnally, Sects. 6 and 7 present some future challenges and
conclusions.
2
UAV-Based Remote Sensing and WSN Support
After the analysis of several works that propose architectures for the use of UAV for
remote sensing [3, 5–7, 12] and for WSN [4, 9, 10, 13, 14] we propose a solution that
integrates both disciplines. Figure 1 shows our architectural proposal. Below we
describe the main elements, whose functions will be analyzed in the following sections.
Deployed UAV receive control information generally containing a ﬂying path, as a
set of waypoints over a region [15]. UAV networks are formed based on the geo-
graphical proximity, designated task, ﬂying altitude, geography of the ﬂying zone,
radio access technology, etc. They need to be self-organized in order to avoid collisions
with other UAV or obstacles [16]. Some papers propose [12] area coverage path
planning strategy to obtain images of the ground considering a multi-UAV scenario.
Communication stations are responsible for driving the ﬂight of the aircraft, con-
trolling the use of payload sensors and receiving and distributing the information col-
lected during the ﬂight. All this functions can be separated in various functional modules,
as in some works it is common to separate UAV control from acquired data [17].
The control and data management system is responsible for the exchange of
information between service providers and the UAV, and is composed of data acqui-
sition and assessment, mission execution and monitoring, data storage and visualiza-
tion, and data fusion modules.
Analyzing UAV-Based Remote Sensing and WSN Support for Data Fusion
757

The data acquisition and assessment module recovers all captured information,
including video, performs some processing and allows performing detailed post mis-
sion analysis and mission report generation. With respect to geospatial information, it is
responsible for managing geo-spatial and temporal referencing of the collected infor-
mation, for later integration with geo-referenced content, such as Volunteered Geo-
graphic Information (VGI) [18].
The mission execution and monitoring module supports the synchronous transfer of
telemetry data, the development of the mission plan, the overall strategic plan of
multiple UAS, it can also be used to simulate mission plans and handle all the nec-
essary data to perform real-time mission monitoring. It also provides computational
capabilities as well, i.e., algorithms used to process the received data, and generate
additional information for the users’ beneﬁt.
The data storage and visualization module can perform some spatial operations
such as data comparison and correlation, data conversion for data storage and repre-
sentation. The data storage function allows the allocation of resulting information, such
as Digital Terrain Models (DTM), annotated data, mission major events, failures and
problems, operational area description, mission plan data, recorded video and images,
reports, etc. Data representation function provide to the operator several functionalities
related to real-time mission monitoring, post-mission data analysis, etc.
The data fusion module allows the integration of acquired data with other infor-
mation sources, either from WSN, external drone networks, or third party databases or
cloud environments.
3
Data Acquisition and Communication
The use of UAV as ﬂying wireless communication platforms has received signiﬁcant
attention recently [5, 13, 15]. Depending of the UAV type data acquisition tech-
nologies may vary. In these sense, and based on their maximum altitude and maximum
Data acquisition 
and assessment
WSN
UAV 
network
Comm 
station
Comm 
station
control / information flow
Control and data 
management system
Mission executing 
and monitoring
Data fusion
Data storage and 
visualization
External data 
sources
information flow
data acquisition
Fig. 1. Architecture for the integration of UAV-based remote sensing and WSN support
758
R. Alcarria et al.

range, UAV can be classiﬁed into three classes: small, medium, and large [19].
Medium UAV are positioned in an altitude range between 300 m and 5500 m, also
limits for small and large ones respectively. Regarding the maximum range, it is less
than 3 km for small drones and between 150–250 km for the medium ones [20], both
representing line-of-sight (LoS) communications. From the operational perspectives,
UAV can be classiﬁed into governmental, nongovernmental, or recreational [21].
All these functions can be separated in various functional modules, as in some
works it is common to separate UAV control from acquired data [13, 14].
Many works focus in considering the UAV as a data collector from the WSN
perspective, transmitting the data to other devices which are out of the communication
ranges of the transmitters [22]. They mainly focus on optimal trajectories [14], mini-
mizing energy consumption, intelligent UAV deployments, base station collocation,
number of updates, etc.
Data acquisition technologies depend on the application scenarios, as Table 1
summarizes.
Regarding the communication capabilities of the UAV, we must differentiate
between the technology for navigation control and positioning, and the communication
enablers available to the drone to establish communications to capture and transmit
information. Related to the positioning of the drone, some works [23] relate the
experiences with UAV incorporating GPSs, GNSSs and INSs for road surveying,
pollution monitoring, city modelling areas. Almost all UAV have failsafe retrieval
mechanisms permitting the recovering of the drone in extensive or difﬁcult access
scenarios such as agriculture. If the aircraft loses its GPS signal, the drone automati-
cally returns to its original launch point.
Table 1. Data acquisition technologies for scenarios
Phenomena to monitor
Remote sensing
devices
WNS-based
devices
Geophysical
Geography,
geomorphology,
earthquake, volcano
Mechanical sensors,
multispectral camera
Vibration, force,
momentum,
positioning
Meteorological
Tropical storms,
hurricanes, heavy
rainfalls, avalanches
Heat and radiation
sensors, radiometer,
optical probe
Temperature,
humidity,
precipitation,
pressure
Environmental
and planning
Vegetation, landscape,
ecology, settlement
dynamics, agriculture,
forestry
Chemical sensors,
multispectral,
pollution,
hyperspectral, LiDAR,
gas
Solution
concentration,
polarography,
substance
composition
Surveying and
GIS industry
Persons, vehicles, trafﬁc
surveillance, road
conditions, emergency
Night vision, real-time
video cameras, contact
less (NFC)
Touch, presence,
mechanical, heat,
radiation, virtual,
web
Analyzing UAV-Based Remote Sensing and WSN Support for Data Fusion
759

Related to the capturing and transmission of information, there are many papers
reporting a wide variety of communication technologies.
In the case of RFID, the equipment of drones with this technology has been used to
improve the construction supply chain management by identifying the location of
construction materials [24]. The capture distance is a few meters, while there are also
other short range communication technologies such as Zigbee and Bluetooth, and,
ﬁnally, wide range communication technologies such as GSM, 3G, LTE, and other
broadband solutions such as WiMAX. The problem with these technologies is that they
generally offer little robustness to the mobility characteristics of UAV [15]. Due to
these mobility features (i.e., relative speed and moving direction), fast changes in the
topology happen, resulting in frequent disconnections between adjacent UAV.
To solve this, the new 5G technology is considered, which, by its characteristics of
minimum latency and high bandwidth, it is very suitable for next generation UAV
communication. There are some tests with this technology in drones, promoted by
Google in its Project Skybender [25].
4
Data Fusion
Data fusion [2] is a form of data processing which enhances, reﬁnes and augments
existing data by using other information sources. This type of processing that we can
deﬁne as intensive can impose some requirements of delay, computing intensity,
bandwidth and scalability, which exceed the capacities of a UAV. Through the ana-
lyzed works we classify the processing modes in UAV environments in local, cloud
and hybrid (combination between local and cloud), with the characteristics that can be
seen in Table 2.
Table 2. Data processing and processing modes.
UAV local execution
Cloud execution
Hybrid execution
Time
limitation
Real-time
applications where
inputs are acquired
by UAV. Low delay
Non-real-time
applications. High
delay
Real-time applications
where inputs are not
necessarily acquired by
UAV. Medium delay
Computing
intensity
Non-intensive
processing. Low
complexity
Highly intensive
processing
Modular processing. Sends
less complex tasks to UAV
Bandwidth
Adequate for low
bandwidth in inputs
and high bandwidth
in outputs
Adequate for high
bandwidth in inputs
and low bandwidth
in outputs
Adequate for medium
bandwidth in inputs and
medium bandwidth
in outputs
Scalability
Depends on number
of UAV/UAV
networks
Depends on server
performance and
parallel computing
Less scalable than Cloud
execution, based on
cloudlets [26]
760
R. Alcarria et al.

For real-time cases, in which the delay is somewhat critical, hybrid execution is
proposed, deﬁned as the augmentation of UAV computational resources with adjacent
servers. We propose the cloudlets [26] solution for this kind of processing, since it is a
novel solution for connecting the high latency remote servers by bridging the cloud to
the mobile [27]. Cloudlets increase the processing capacity, conserve the energy
resource, and ease the deployment.
To illustrate the variety in processing intensity to which Unmanned Aerial Systems
(UAS) may be subjected, various literature situations are shown.
In the case of remote sensing image processing, common data processing tech-
niques [28] are image processing, orthorectiﬁcation, image mosaicking, data compar-
ison and correlation, data representation, data annotation with external sources, etc.
These processes are complex and intensive in nature.
Data fusion often faces the challenge of data interoperability. Semantic models and
geo-ontologies [29] abstract vocabularies and information into a common and share-
able model. Some applications examples are LinkedGeoData initiative [30], which
make the OpenStreetMap information available as semantic database. The processing
requirements are medium, and suitable for a hybrid execution.
Continuing with data fusion, the integration of human observations with observa-
tions from sensors has been analyzed for the design of a noise mapping community in
[31], integrating Smartphone generated sensor observations with human observations
for enhancing noise mapping capabilities. These low processing requirements would
allow hybrid execution as there are information sources not acquired by the UAV that
must be processed on the server side.
When processing sources are acquired by the UAV as in the case of ﬂoating devices
(e.g. buoys, as data collection systems for meteorological/oceanographic parameters
[32]), a UAV local execution can be performed. The same in the case of the incor-
poration of RFID sensors for forest tree marking [33], which allows to merge the
information of identiﬁcation of each tree with the spatial presentation of them, and thus
to combine this information with own characteristics of the tree like diameter at breast
height, stem proﬁle, crown height, etc.
5
Data Presentation
Integrated data representation has special requirements for data from a single source of
information. For example, geo-reference image data (obtained by combination of
Global Positioning System (GPS) and Inertial Measurement Unit (IMU) sensors) is
combined with computer vision (CV) techniques [34] for accuracy purposes and
seamless mosaicking.
In order to produce free of distortion orthomosaics [28], image orthorectiﬁcation
(each image is geometrically corrected according to its associated telemetry data),
alignment (frame geo-referencing and rectiﬁcation with previously aligned ortho-
frames) and stitching (overlapping areas of incoming frames are blended to achieve
seamless smooth transitions) are performed.
Some overlay information is superimposed to orthomosaic base layer, such as
physical sensor information (such as magnetic information as heatmaps [35]), or the so
Analyzing UAV-Based Remote Sensing and WSN Support for Data Fusion
761

called human sensor information (human annotation with geo-position and temporal
validity by following the concept of citizen sensing [36]).
This concept of superimposition of information in maps is known as map mashups
[1]. When data providers do not expose APIs [37], mash-ups are created using a
technique known as web scraping [1], denoting the automatic information extraction
from provider’s Web pages. For spatial data storage and visualization, geo-spatial data
publishing tools are being used. These tools support a variety of data formats and,
through standard protocols and APIs [37], they produce vector and raster images. For
example, some authors [38] use a Geoserver as the implementation of a mash-up
logical component to edit and publish geo-spatial data.
The use of DBMS for data integration in cartography has been widely used in the
literature. In some works [38], the authors propose a PostgreSQL/PostGIS database to
store sensor information coming from UAV employed for territorial surveillance.
6
Future Challenges
We propose some future challenges related to the combination of data from
UAV-based remote sensing techniques and WSN networks.
One of the most mentioned challenges is the possibility of UAV multitasking [15]
for Internet of Things (IoT) services. The challenge is to equip in UAV some IoT
devices like sensors and cameras that can be triggered by a third party. For example,
Transport Safety Agency could request the current trafﬁc status in speciﬁc streets over
which the drones will be ﬂying, in order to, e.g., measure parameters of interest like air
pollution. In this respect, drones can be used for diverse tasks requested by different
stakeholders that do not necessarily have to invest into drones, a fact that would
potentially generate additional sources of revenues for the actual owner of the drones.
Some Works exploit this concept with the term Drone-as-a-Service or DaaS [17].
This challenge would require a greater proliferation and price reduction of UAV, so
that there are several available networks of diverse characteristics and geographically
distributed. This will involve solving the challenges of providing drone networks with
greater coordination [16, 39] and also improving regulations [40–42] and security [15].
Regarding UAV coordination, in scenarios with massive numbers of UAV, the
probability of collisions among these UAV and with other urban and nonurban
obstacles (buildings, mountains, animals) increases; specially in situations of low
visibility or probability of device malfunction due to special conditions such as ﬂight in
fog or smoke. Collision avoidance methods from related work were analyzed, such as
UAV path planning techniques [39] for discovering an optimal and collision free path
and analyzing the problem of uncertainty of the information changes by the relative
position of drone in environments with obstacles [16].
Regarding UAV regulation, regulation has already started in the European Union
(EU) [40], the USA FAA [41], and in many other countries around the world, con-
sidering no-drone zones and exemptions for educational purposes. The spectrum reg-
ulation is also another important issue to be considered [42], where the communication
of UAV should not affect the communication of current air trafﬁc.
762
R. Alcarria et al.

Regarding UAV security, authentication, security and trust in drone communication
such be carefully considered in order to protect the acquired and processed information
and to ease the public acceptance of drone-based services [15].
The last detected future challenge is about applying machine learning technologies
(such as document classiﬁers and document clustering) on the acquired information to
improve the quality of the information categorization, reduce the amount of record
duplications and apply speciﬁc semantic representation languages (like RDF) to
improve data integration [38] according to the new Web of Data paradigm.
7
Conclusions
In this paper we analyze the state of the art in UAV-based remote sensing and WSN
support for the generation of integrated data. After describing the beneﬁts of these two
information capture technologies, we propose a uniﬁed architecture that integrates the
main functions necessary for the generation of combined data, as value-adding
contribution.
The processes of data acquisition and communication, fusion, and presentation are
analyzed by studying related literature. Finally, some future challenges regarding the
combination of data from UAV and WSN networks are proposed. These challenges
include
the
Drone-as-a-Service
paradigm,
novel
collision
avoidance
methods,
improved regulation and security, and ﬁnally, machine learning for the Web of Data.
In conclusion, the remote sensing and WSN ﬁelds are very mature, with many
works focusing on information capture, processing, storage and presentation. The
union of these ﬁelds offers a promising perspective for the hybrid process execution,
the integration of the geospatial data, the ﬁelds of web mashups and citizen sensing,
and the Semantic Web or Web of Data.
Acknowledgments. These results were supported by UPM’s ‘Programa Propio’, and within the
framework of the educational innovation project IE1617.1200, funded by UPM in its 2016–2017
call. Also by the Ministry of Economy and Competitiveness through SEMOLA project
(TEC2015-68284-R) and the Autonomous Region of Madrid through MOSI-AGIL-CM project
(grant P2013/ICE-3019, co-funded by EU Structural Funds FSE and FEDER). Borja Bordel has
received funding from the Ministry of Education through the FPU program (grant number
FPU15/03977).
References
1. Hartmann, B., Doorley, S., Klemmer, S.R.: Hacking, mashing, gluing: understanding
opportunistic design. IEEE Pervasive Comput. 7, 46–54 (2008)
2. Haghighat, M., Abdel-Mottaleb, M., Alhalabi, W.: Discriminant correlation analysis:
real-time feature level fusion for multimodal biometric recognition. IEEE Trans. Inf.
Forensics Secur. 11, 1984–1996 (2016)
3. Sankey, T., Donager, J., McVay, J., Sankey, J.B.: UAV lidar and hyperspectral fusion for
forest monitoring in the southwestern USA. Remote Sens. Environ. 195, 30–43 (2017)
Analyzing UAV-Based Remote Sensing and WSN Support for Data Fusion
763

4. Loke, S.W.: The internet of ﬂying-things: opportunities and challenges with airborne fog
computing and mobile cloud in the clouds (2015)
5. Popescu, D., Ichim, L., Stoican, F.: Unmanned aerial vehicle systems for remote estimation
of ﬂooded areas based on complex image processing. Sensors 17, 446 (2017)
6. Balampanis, F., Maza, I., Ollero, A.: Coastal areas division and coverage with multiple
UAVs for remote sensing. Sensors 17, 808 (2017)
7. Ni, J., Yao, L., Zhang, J., Cao, W., Zhu, Y., Tai, X.: Development of an unmanned aerial
vehicle-borne crop-growth monitoring system. Sensors 17, 502 (2017)
8. DeBell, L., Anderson, K., Brazier, R.E., King, N., Jones, L.: Water resource management at
catchment scales using lightweight UAVs: current capabilities and future perspectives.
J. Unmanned Veh. Syst. 4, 7–30 (2016)
9. Deepika, G., Rajapirian, P.: Wireless sensor network in precision agriculture: a survey. In:
2016 International Conference on Emerging Trends in Engineering, Technology and Science
(ICETETS), pp. 1–4. IEEE (2016)
10. Niedermeier, M., He, X., de Meer, H., Buschmann, C., Hartmann, K., Langmann, B., Koch,
M., Fischer, S., Pﬁsterer, D.: Critical infrastructure surveillance using securewireless sensor
networks. J. Sens. Actuator Netw. 4, 336–370 (2015)
11. Posnicek, T., Kellner, K., Brandl, M.: Wireless sensor network for environmental monitoring
with 3G connectivity. Procedia Eng. 87, 524–527 (2014)
12. Avellar, G., Pereira, G., Pimenta, L., Iscold, P.: Multi-UAV routing for area coverage and
remote sensing with minimum time. Sensors 15, 27783–27803 (2015)
13. Zeng, Y., Zhang, R., Lim, T.J.: Wireless communications with unmanned aerial vehicles:
opportunities and challenges. IEEE Commun. Mag. 54, 36–42 (2016)
14. Mozaffari, M., Saad, W., Bennis, M., Debbah, M.: Unmanned aerial vehicle with underlaid
device-to-device communications: performance and tradeoffs. IEEE Trans. Wirel. Commun.
15, 3949–3963 (2016)
15. Hossein Motlagh, N., Taleb, T., Arouk, O.: Low-altitude unmanned aerial vehicles-based
internet of things services: comprehensive survey and future perspectives. IEEE Internet
Things J. 3, 899–922 (2016)
16. Ueno, S., Higuchi, T., Iwama, K.: Collision avoidance control law of a helicopter using
information amount feedback. In: 2008 SICE Annual Conference, pp. 2118–2121. IEEE
(2008)
17. Choi, S.-C., Sung, N.-M., Park, J.-H., Ahn, I.-Y., Kim, J.: Enabling drone as a service:
OneM2M-based UAV/drone management system. In: 2017 Ninth International Conference
on Ubiquitous and Future Networks (ICUFN), pp. 18–20. IEEE (2017)
18. Gómez-Barrón, J.-P., Manso-Callejo, M.-Á., Alcarria, R., Iturrioz, T.: Volunteered
geographic information system design: project and participation guidelines. ISPRS Int.
J. Geo Inf. 5, 108 (2016)
19. Characteristics of unmanned aircraft systems and spectrum requirements to support their safe
operation in non-segregated airspace, ITU. Mobile, Radio Determination, Amateur Related
Satellite Services. Technical report M.2171. http://www.itu.int/pub/R-REP-M.2171-2009
20. UVS International – Remotely Piloted Systems: Promoting International Cooperation &
Coordination. RPAS Related Documents (2017). http://uvs-international.org/
21. Gavan, J., Tapuchi, S.: The potential of High Altitude Platforms (HAPS) for low interference
and broadband radio services. In: 2009 5th Asia-Paciﬁc Conference on Environmental
Electromagnetics, pp. 17–25. IEEE (2009)
22. Lien, S.-Y., Chen, K.-C., Lin, Y.: Toward ubiquitous massive accesses in 3GPP
machine-to-machine communications. IEEE Commun. Mag. 49, 66–74 (2011)
764
R. Alcarria et al.

23. Caroti, G., Piemonte, A.: Kinematic positioning: from mobile mapping systems to unmanned
aerial vehicles at Pisa University. In: Cefalo, R., Zieliński, J.B., Barbarella, M. (eds.) New
Advanced GNSS and 3D Spatial Techniques. LNGC, pp. 261–270. Springer, Cham (2018).
https://doi.org/10.1007/978-3-319-56218-6_21
24. Hubbard, B., Wang, H., Leasure, M., Ropp, T., Lofton, T., Hubbard, S., Lin, S.: Feasibility
study of UAV use for RFID material tracking on construction sites. In: 51st ASC Annual
International Conference Proceedings (2015)
25. Harris, M.: Project Skybender: Google’s secretive 5G internet drone tests revealed. https://
www.theguardian.com/technology/2016/jan/29/project-skybender-google-drone-tests-
internet-spaceport-virgin-galactic
26. Alakbarov, R.G., Pashayev, F.H., Alakbarov, O.R.: Optimal deployment model of cloudlets
in mobile Cloud Computing. In: 2017 IEEE 2nd International Conference on Cloud
Computing and Big Data Analysis (ICCCBDA), pp. 213–217. IEEE (2017)
27. Shariatmadari, H., Ratasuk, R., Iraji, S., Laya, A., Taleb, T., Jäntti, R., Ghosh, A.:
Machine-type communications: current status and future perspectives toward 5G systems.
IEEE Commun. Mag. 53, 10–17 (2015)
28. Laliberte, A.S., Herrick, J.E., Rango, A., Winters, C.: Acquisition, orthorectiﬁcation, and
object-based classiﬁcation of unmanned aerial vehicle (UAV) imagery for rangeland
monitoring. Photogramm. Eng. Remote Sens. 76, 661–672 (2010)
29. Lu, B., He, Y., Liu, H.H.T.: Mapping vegetation biophysical and biochemical properties
using unmanned aerial vehicles-acquired imagery. Int. J. Remote Sens. 1–23, 10 August
2017. (Online)
30. Auer, S., Lehmann, J., Hellmann, S.: LinkedGeoData: adding a spatial dimension to the web
of data. In: Bernstein, A., Karger, D.R., Heath, T., Feigenbaum, L., Maynard, D., Motta, E.,
Thirunarayan, K. (eds.) ISWC 2009. LNCS, vol. 5823, pp. 731–746. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-04930-9_46
31. Foerster, T., Jirka, S., Stasch, C., Pross, B., Everding, T., Broring, A., Juerrens, E.H.:
Integrating human observations and sensor observations – the example of a noise mapping
community. In: Proceedings of Workshop “Towards Digital Earth: Search, Discover and
Share Geospatial Data 2010” at Future Internet Symposium, Berlin (2010)
32. Al-Zaidi, R., Woods, J., Al-Khalidi, M., Hu, H.: An IOT-enabled system for marine data
aquisition and cartography. Trans. Netw. Commun. 5, 53 (2017)
33. Pichler, G., Poveda Lopez, J.A., Picchi, G., Nolan, E., Kastner, M., Stampfer, K., Kühmaier,
M.: Comparison of remote sensing based RFID and standard tree marking for timber
harvesting. Comput. Electron. Agric. 140, 214–226 (2017)
34. Szeliski, R.: Image alignment and stitching: a tutorial, Redmond (2004)
35. Pei, Y., Liu, B., Hua, Q., Liu, C., Ji, Y.: An aeromagnetic survey system based on an
unmanned autonomous helicopter: development, experiment, and analysis. Int. J. Remote
Sens. 38, 3068–3083 (2017)
36. Boulos, M., Resch, B., Crowley, D.N., Breslin, J.G., Sohn, G., Burtner, R., Pike, W.A.,
Jezierski, E., Chuang, K.-Y.S., Sheth, A.: Crowdsourcing, citizen sensing and sensor web
technologies for public and environmental health surveillance and crisis management:
trends, OGC standards and application examples. Int. J. Health Geogr. 3, 1 (2004)
37. Rouse, L.J., Bergeron, S.J., Harris, T.M.: Participating in the geospatial web: collaborative
mapping, social networks and participatory GIS. In: Scharl, A., Tochtermann, K. (eds.) The
Geospatial Web. Advanced Information and Knowledge Processing, pp. 153–158. Springer,
London (2009). https://doi.org/10.1007/978-1-84628-827-2_14
38. Meo, R., Roglia, E., Bottino, A.: The exploitation of data from remote and human sensors
for environment monitoring in the SMAT project. Sensors 12, 17504–17535 (2012)
Analyzing UAV-Based Remote Sensing and WSN Support for Data Fusion
765

39. Yang, L., Qi, J., Jizhong, X., Yong, X.: A literature review of UAV 3D path planning. In:
Proceeding of the 11th World Congress on Intelligent Control and Automation, pp. 2376–
2381. IEEE (2014)
40. A proposal to create common rules for operating drones in Europe, EASA: European
Aviation Safety Agency. https://www.easa.europa.eu/system/ﬁles/dfu/205933-01-EASA_
Summary of the ANPA.pdf
41. Unmanned Aircraft Systems, Federal Aviation Administration. https://www.faa.gov/uas/
42. Manual for Airspace Planning, European Organisation for the Safety of Air Navigation.
https://www.icao.int/safety/pbn/Documentation/EUROCONTROL
766
R. Alcarria et al.

Continuous Speech Recognition
and Identiﬁcation of the Speaker System
Diego Guffanti1(&), Danilo Martínez2, José Paladines3,
and Andrea Sarmiento4
1 Universidad Tecnológica Equinoccial UTE,
Av. Chone Km 4 ½, Santo Domingo, Ecuador
diegogufy0190@hotmail.com
2 Universidad de las Fuerzas Armadas ESPE,
Av. General Rumiñahui s/n, Sangolquí, Ecuador
mdmartinez@espe.edu.ec
3 Technical Sciences Faculty,
Universidad Estatal del Sur de Manabí, Jipijapa, Ecuador
jose.paladines@unesum.edu.ec
4 Universidad Católica de Cuenca UCACUE, Cuenca, Ecuador
andresar1991@hotmail.com
Abstract. Currently speech recognition and speaker identiﬁcation based on a
biometric parameter such as voice have been treated as two different worlds and
in the market there are no integrated applications of these systems. The design of
a system could mean a great contribution to the development of personalized
commands, in the area of home automation and robotics, thanks to the avail-
ability of the message and the identiﬁcation of the speaker. Therefore, the
development of an integrated biometric voice system is proposed, based on a
single voice sample for the identiﬁcation of the speaker and the message. We
use GOOGLE SPEECH API, as a voice text translation tool, and Mel Frequency
Cepstral Coefﬁcients or MFCCs extracted from voice signal to identify speakers
voice. Functional tests were carried with 50 randomly users, in the end of the
study results show 96.4% efﬁciency in identiﬁcation, demonstrating efﬁciency
using MFCCs in speaker’s automatic recognition and verifying the use of
GOOGLE SPEECH API as a fast, accurate and robust translation tool.
Keywords: Mel Frequency Cepstral Coefﬁcients  GOOGLE SPEECH API
RAH  RAL
1
Introduction
Automatic Speech Recognition (ASR) is a technology capable to recognize the human
voice to interpret it for performing any action. This technology, despite being very
recent, has generated important results due to the procedures used to obtain high level
sound decoding and spoken information, which make part of an acoustic message. We
can mention some applications such as SIRI, Google Now and Cortana.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_72

It is convenient to emphasize, on this technology, the need of identifying the
speaker to obtain a more reliable and innovative product. This identiﬁcation can be
achieved through the Automatic Speaker Recognition, currently in an underdeveloped
stage. Nowadays we can ﬁnd products such as KIVOX360, KIVOX Passive Detection,
BATVOX, BS3, ASIS, and SIFT, which although being expensive, are essential in a
high-quality system.
On this regard, this paper proposes a system which integrates ASR and Automatic
Speaker Recognition, using GOOGLE SPEECH API for speech recognition and Mel
Frequency Cepstral Coefﬁcients (MFCC) for speaker identiﬁcation.
Main contribution of this study is integrating of ASR and Automatic Speaker
Recognition in one system, allowing to identify a user efﬁciently through a voice
message, this can be exploited as a service not only for security ﬁeld but as well as
others such as home automation.
Sections of this article are organized as follows: Sect. 2, are exposed some related
work, Sect. 3 describes the construction of the system, Sect. 4 shows conducted tests
and ﬁnally, in Sect. 5, conclusions and future jobs.
2
Related Work
Automatic Speaker Recognition recent techniques based on parameters extracted from
voice signal are found the paper of Leu and Lin [1], they report the development of a
speaker identiﬁcation system based on MFCC technique, where once more, by
extracting the MFCC coefﬁcients, axn user statistic model is generated applying GMM
(Gaussian Mixture Model), and the distance between the test user and the models
registered on the data based is calculated applying Bhattacharyya’s distance. Basing the
whole study in just one user model, means a big risk if we consider that the techniques
proposed until now are not completely reliable, because noise conditions in recording
backgrounds are different and therefore variations when extracting voice signal
parameters exist, even when they come from the same user. On the other hand, high
computational outlay is generated by the calculation method used to measure the
distance among data, this can be reduced when applying an easier way to calculate
distance.
Regarding Automatic Speech Recognition (ASR) techniques, these have matured
much more than the ones used in Automatic Speaker Recognition Systems. We have
open code developing tools such as Hidden Markov Model Toolkit (HTK) [2], Speech
Recognition Toolkit, Kaldi Speech Recognition Toolkit [3], among others which main
disadvantage is that they are centered in a speciﬁc voice recognition scope. Contrary to
this, tools such as GOOGLE CLOUD SPEECH API [4] have implemented continuous
voice recognition, allowing developers to convert audio into text in more than 80
languages and their variations through applying potent model of neuronal networks.
Besides, it is possible to transcribe user’s text spoken on a microphone or transcribing
audio ﬁles among many other use cases.
768
D. Guffanti et al.

3
System Description
The application has direct interaction with the user. Its interface gets the input infor-
mation through the voice signal, the user’s identiﬁcation process was developed in
Matlab and the translation from voice to text in GOOGLE SPEECH API as shown in
Fig. 1.
3.1
Speaker Identiﬁcation
For the identifying process, the voice signal must be treated and processed before
extracting the MFCC’s characteristics, to assure that the user model represents the
speaker in a reliable way.
Sampling. We used a microphone with an answer in a 20 Hz–16 kHz frequency with
103 dB sensibility and maximum 100 MW power to obtain information. The sampling
frequency was Fs = 44100 Hz, recommended by GOOGLE SPEECH API [5]. At least
5 s recording were done so the user can pronounce at least one complete phrase. Each
recording has 220500 voice signal samples in energy unities as regarding to the sample
frequency chosen. The capture process of the voice signal was done using MAT LAB
software.
Signal Noise and silence periods Elimination. To reduce noise, ﬁrst the maximum
pike of signal was calculated and the whole signal was divided for this maximum pike
to separate the wave shape from the signal intensity. Then the average energy square
and divided by the number of signal samples was calculated, as shown in Eq. 1.
Eavg¼ 1
N
XN
k¼1 x[k]
j
j2
ð1Þ
Later the signal was segmented in 400 sample plots, which belong to approximately
10 ms in a frequency of 44100 Hz, reducing the possibility in the loss of phonetic
Fig. 1. Application development outline
Continuous Speech Recognition and Identiﬁcation of the Speaker System
769

information lasting between 10 ms and 20 ms. Next for this case, a 2% from total
energy threshold value is stablished, the signal is removed below this parameter, if the
parameter is exceeded this value is stored in a new vector as a useful signal for its
subsequent analysis.
Framing and Window function. Due to articulatory inertia it can be supposed that
voice’s characteristics and properties don’t vary signiﬁcantly in a short interval of time,
which oscillates between 20 ms and 40 ms [6], therefore, it is possible to make a
spectral almost-stationary over signal segments of this time length. If the length of the
frames of 1024 data is taken, then the overlapping between windows can be ﬁxed in
512 data. It is necessary to process these voice plots through a Hamming window or
sinusoidal bell to remove problems caused by fast changes in signals in the extremes of
each voice frame. Because of this, segmentation is applied with a starting shift to get
soft transitions between frames.
MEL Frequency Cepstral Coefﬁcients. The most popular set of voice extracted
parameters is MFCC, developed by Davis and Mermelstein [7]. This technique is
designed over a base with the human ear’s characteristics which is acoustic sensible to
sounds in different frequencies. Khalifa et al. [8] has identiﬁed the main steps to extract
MFCC coefﬁcients shown in Fig. 2.
When following the algorithm shown on Fig. 2, we get n rows and m columns
matrix, where n is the number of frames formed and m is the number of cepstral
coefﬁcients extracted from each frame. The number of coefﬁcients is given by the
number of Mel ﬁlters applied in the spectrum and varies according to the implemen-
tation, for example, when recognizing voices ﬁrst 10 to 13 coefﬁcients are used. This
matrix represents a user identiﬁcation model. Figure 3 shows the procedure followed to
obtain this model.
Models Comparison. User’s model should be compared with all stored models for
users registered during the previous training stage. The data base has three models for
each registered user, these belong to each recording done by the user during training
phase. Each model generates a MFCCs matrix. For model’s comparison a Euclidean
total distance has been used. With the used technique each test vector row (each frame)
is compared with each characteristic vector row of the user stored in the data base and
the major similarity between two frames is searched (this is done because of the
Magnitude 
spectrum
mel
spectrum
Log mel
spectrum
Preprocessing
Framing
Windowing
DFT
Mel Filterbank
Log
Inverse DFT
Continuous
speech
Fig. 2. MFCC extraction algorithm (Taken from Khalifa et al. [8])
770
D. Guffanti et al.

possibility of independence during training and test stages). When most similarity
frame is found, the separation distance is stored. Total similarity distance is all dis-
tances of separation average as shown in Eqs. 2, 3 and 4.
dTi ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Xm
i ¼ 1 ci  c;
i

2
q
;
m ¼ number of MFCC:
ð2Þ
dTn ¼ min(dTiÞ
ð3Þ
dsimilarity ¼
Pn
j ¼ 1 dTn
n
;
n ¼ number of frames
ð4Þ
The dsimilarity on Eq. 4, shows the connection between the test user model and the
training user model. Since we have N training users, the shortest dsimilarity should be
used to stablish who the user was.
3.2
Voice to Text Translation
To set up voice to text translation GOOGLE CLOUD SPEECH API was used, because
it allows audio to text through the application of strong models of neuronal networks in
an Application Programming Interface (API) which is easy to use. API has automatic
speech recognition, global vocabulary, streaming recognition, (gives back recognition
results while the user keeps speaking), words suggestion, prerecorded or real time
audio support and inappropriate content ﬁlter [4].
Basically, an audio ﬁle is sent in FLAC (Free Lossless Audio Codec) format
through internet to a Google server, and this gives back a chain of characters with the
translation. To be able to use API, previously, a project should be created, the voice
API set up and a service key created.
Fig. 3. User’s model obtaining
Continuous Speech Recognition and Identiﬁcation of the Speaker System
771

3.3
Interfaces
User interface was developed using Microsoft Visual Studio 2013, the call to the
functions implemented in MATLAB is done from here, these allow processing the
voice signal to identify the user. In Fig. 4 we can observe the user windows developed
for the application.
Through these Windows, a user can register his voice in the system, train the
system or delete the data base, as well as carry out translation and identiﬁcation
processes.
4
Tests
System tests were done in three phases. First stage test was carried out to train the
system only with real registered users. Second stage fake users were included, these
were unregistered users. Finally, third stage a 50 users group was chosen randomly.
4.1
System Training Tests
Applying the algorithm explained before, system training sessions were carried out,
using only real users. Each user made 3 recordings of 5 s with a different text.
Afterwards, the system tries to identify the speaker of the message through the cal-
culation of similarity distances shown in Table 1. On this table we can see 3 distances
because each recording generates a different model for each user
To accept a user, the distances chart was transformed in to a chart based in scores,
where one point is given to the user with a similarity distance up to 7% above the
parameter dsimilarity minimum in the table. This way we identify the user with the
highest score (Table 2).
Fig. 4. User interface.
772
D. Guffanti et al.

The training tests only with authentic users depending and not depending on the
text during training as well as during test allowed to ﬁx in an experimental way the
threshold used on the creation of the similarity score chart explained previously.
4.2
Operation Tests with Authentic and Non-authentic Users
After the training stage, the system should have been tested with a group of authentic
and non-authentic users so the system would function as a binary discriminator,
accepting the authentic user and rejecting fake users. To achieve this, a minimum
similarity distance was stablished for each user in the data base, so the test distance
should be near before accepting the user as authentic. But the question is, how near it
should be? to stablish this threshold we used a statistic tool called the ROC curve, its
graphic is shown in Fig. 5, based on a 60 users test, where 30 are authentic and 30 are
non-authentic users.
In Fig. 5 we can see that the ideal prediction model would be when the sensibility
parameter tends to 1, it means to point D; however, with this value some non-authentic
users can be accepted (false positives), this is not convenient in an identiﬁcation
system, therefore the chosen model was C, with 0.91 accuracy, this way the threshold
was set as follows:
THRESHOLD ¼ 1:2  dsimilarity
ð4Þ
Therefore, after the user’s identiﬁcation is done, the similarity distance should be
below this threshold so he/she can be considered as authentic, otherwise he/she is
rejected as a non-authentic user.
4.3
Final Operation Tests
Once conﬁguration tests for the application have concluded, we carried out some test to
determine the accuracy and effectiveness of the system. A group of 50 random users
Table 1. Similarity distances for user’s test
User N. Similarity
d1
d2
d3
US001
1,95 1,96 2,08
US002
2,41 2,32 2,33
US003
2,70 2,97 2,83
US004
2,72 2,67 2,65
US005
2,24 2,17 2,33
Table 2. Similarity scores for user’s test US001
User N. US001 US002 US003 US004 US005
US001
3
0
0
0
0
Continuous Speech Recognition and Identiﬁcation of the Speaker System
773

was used to interact with the application, from these 25 were registered as authentic and
25 were non-authentic, 500 tests with independence of the text, were done in total.
Accuracy shows how well the system can differentiate an authentic user from a
non-authentic one. Table 3 shows the results.
From 250 positive tests, users were accepted in 199 times, while in 250 negative
tests the system rejected non-authentic users appropriately in 149, with 70% accuracy
to classify between authentic and non-authentic users.
Efﬁciency shows us how well the system can identify a user among those registered
in the data base, therefore calculations are make based on tests carried by authentic
users (Table 4).
There were 9 mistakes in 250 positive tests, then system efﬁciency is 96.4%.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
VPR or SensiƟvity
FPR or (1-speciﬁcity) 
ROC Space
A
B
C
D
E
F
G
H
Fig. 5. ROC curves of the binary classiﬁer
Table 3. System accuracy.
Positive tests (P)
250 Negative tests (N)
250
Authentic positives 199 Authentic negatives 149
Accuracy
70.0%
774
D. Guffanti et al.

5
Conclusions and Future Works
The implemented system presents a signiﬁcant advantage over the existing options in
the market, since based on a single voice sample performs the identiﬁcation of the
speaker and the recognition of the message. To achieve this, the GOOGLE SPEECH
API tool was used, as it is a highly efﬁcient commercial system, on the other hand
because there is no fully developed speech identiﬁcation system on the market, it was
decided to design this system based on MFCCs parameters. Based on this, our proposal
is highly effective because the MFCC parameterization technique models the percep-
tion of the auditory frequency of the human being instead of modeling his vocal tract,
this with a right comparison of the user’s model, are the most important processes in an
Automatic Speaker Recognition System. In our study we opted for a model’s com-
parison based in scores, because when we have more chance of comparisons, the
system accuracy increases. To identify a user, it is convenient to implement a shot
threshold which allows to accept a registered user and to reject a non-authentic one. In
our study we used a ROC curve, and we applied a discrimination criterion based on the
idea that it is more harmful to accept an unregistered user than to reject one registered.
GOOGLE SPEECH API as a message recognition system, demonstrated to be a very
strong, effective, fast robust and with a wide sort of possibilities translation engine
when conﬁguring languages and dialects.
For future works it is recommended to go deep in the experimentation about the
robustness of these techniques, applied in noisy environments with methods to balance
the noise effect over the signal representation, as well as methods to adapt the acoustic
models to a noisy environment. Besides, starting with this work many other applica-
tions can be developed, not only on security ﬁeld but also in-home automation, because
when integrating Automatic Speech Recognition (ASR) and Automatic Speaker
Recognition in the same system, it is possible to set up personalized commands that can
be applied in different areas.
References
1. Leu, F.Y., Lin, G.L.: An MFCC-based speaker identiﬁcation system. In: Proceedings of the
International Conference on Advanced Information Networking and Applications, AINA,
pp. 1055–1062 (2017)
2. HTK Speech Recognition Toolkit. http://htk.eng.cam.ac.uk/
3. Povey, D., Ghoshal, A.: The Kaldi speech recognition toolkit. In: IEEE 2011 Workshop on
Automatic Speech Recognition and Understanding, Hilton Waikoloa Village (2011)
4. API Speech: reconocimiento de voz | Google Cloud Platform. https://cloud.google.com/
speech/
Table 4. System efﬁciency.
Positive tests (P) 250
Mistakes
9
Efﬁciency
96.4%
Continuous Speech Recognition and Identiﬁcation of the Speaker System
775

5. Introduction to Audio Encoding | Google Cloud Speech API | Google Cloud Platform. https://
cloud.google.com/speech/docs/encoding
6. Juang, B.H., Chen, T.: The past, present and future of speech processing. IEEE Signal
Process. Mag. 15, 24–48 (1998). B.H. Juang (ed.)
7. Davis, S.B., Mermelstein, P.: Comparison of parametric representations for monosyllabic
word recognition in continuously spoken sentences. IEEE Trans. Acoust. Speech Sig.
Process. 28(4), 357–366 (1980)
8. Khalifa, O., Islam, R., Khan, S., Faizal, M., Dol, D.: Text independent automatic speaker
recognition. In: 3rd International Conference on Electrical and Computer Engineering, Dhaka,
Bangladesh, pp. 561–564 (2004)
776
D. Guffanti et al.

Competitive Intelligence Using Domain
Ontologies on Facebook
of Telecommunications Companies of Peru
Geraldo Colchado1(&)
and Andrés Melgar2
1 Escuela de Posgrado, Maestría en Informática,
Pontiﬁcia Universidad Católica del Perú, Lima, Peru
geraldo.colchado@pucp.edu.pe
2 Departamento de Ingeniería, Sección de Ingeniería Informática,
Grupo de Reconocimiento de Patrones e Inteligencia Artiﬁcial Aplicada,
Pontiﬁcia Universidad Católica del Perú, Lima, Peru
amelgar@pucp.edu.pe
Abstract. Telecommunications companies (TELCOs) in Peru offers promo-
tions almost daily in social networks, mainly Facebook. There’s a lot of data in
Facebook, written in natural language without meaning for computer, that
TELCOs are not using to have Competitive Intelligence (CI). CI is a process that
identiﬁes decision makers information needs about competitors, collects data
from public sources, gives meaning and analyze data to answer information
needs and communicates results to decision makers.
This paper proposes and implements a CI process for TELCOs that includes
collection of 15,634 posts and 1,411,921 comments from Facebook, the creation
process of TELCO ontology with 119 words and 27 concepts and classiﬁcation
of posts, a Web application to perform semantic search of posts and compare the
results by TELCO including positive and negative comments to have CI for
decision making. The proposed CI process can be used in other contexts with
several competitors with equivalent services.
Keywords: Competitive intelligence  Ontology  Decision maker
Social network  Facebook  Natural language processing  Sentiment analysis
1
Introduction
Telecommunications market in Peru is one of the economic sectors that has had the
most growth in the last three years. With the launch of 4G data technology, in 2014 the
number of mobile phones surpassed the population of Peru and ended 2016 with 37.7
million of mobile phones1 versus 31.5 million of population2.
Those mobile phones are distributed among 5 TELCOs in Peru: Movistar, Claro,
Entel, Bitel and Virgin. TELCOs are constantly offering promotions to attract new
customers and maintain and loyalty their existing ones to gain market share. Since
1 Source: https://www.osiptel.gob.pe/articulo/24-lineas-en-servicio-por-empresa.
2 Source: https://www.inei.gob.pe/media/MenuRecursivo/Cap03020.xls.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_73

2009, TELCOs progressively began to publish their promotions in social networks
mainly Facebook. Movistar and Claro have more than 4 million followers, Entel and
Bitel more than 1 million followers and Virgin more than 100,000 followers on
Facebook.
Posting of promotions by TELCOs and their follower’s comments generates a lot of
data in Facebook; despite this data is public domain and legal, TELCOs are not using it
for comparison, in relevant concepts, with other TELCOs and get important infor-
mation for decision making. This is because TELCOs do not have a formal process for
perform CI using social network data and because there is a large amount of data
written in natural language without meaning for a computer.
CI “is a dynamic, systematic and recursive process that transforms, using speciﬁc
analytical techniques, the relevant and legally obtained information about the com-
petitive environment of past, present and future, with the purpose of facilitating deci-
sion making for the beneﬁt of the company” [1].
To have CI, for a TELCO is relevant to know for example “What promotions of
unlimited voice service to any operator are offered by TELCOs for prepaid in Face-
book?” but this is not possible because text is written in natural language without
meaning for a computer. Domain ontologies can be used to get semantics or meaning
of text data sources because they have a “controlled vocabulary for representing the
types of entities in a given domain” [2]. In Table 1 there is an example of how to ﬁnd a
real post (Fig. 1) that answers a relevant question, ﬁnding manually the words that
identiﬁes concepts that could be part of a domain ontology, the words are underlined
and bold.
For a TELCO is also relevant to know for each post the positive or negative
opinions of the followers, this can be done analyzing the follower’s comments text
using deﬁned words that identify the positivity (+) or negativity (–) of the comment
which can be represented by a domain ontology of polarity or sentiment analysis.
Table 1. Example of how to ﬁnd manually a post that answers a question
Relevant question: “What promotions of unlimited voice service to any operator are
offered by TELCOs for prepaid in Facebook?”
Note: Concepts to search are underlined and italic.
Post found translated to English (See Fig. 1 for original post in Spanish):
Condor Mendoza continues calling all Peruvians … among them, the great Ñol Solano!
You could be the next to receive his call. Send us your number by INBOX … and like
him, talk unlimited to any operator, recharging with S./5! #PeruUnlimited
#PrivateNetworksAlreadyWent
More information here: http://www.entel.pe/yapa
Concepts of TELCOs
Words that identify concept
Service: Voice
call, talk
Service Rate: Zero
unlimited
Operator (TELCO): Movistar, Claro, Entel, Bitel, Virgin any operator
Rate Plan: Prepaid
recharge
778
G. Colchado and A. Melgar

To answer relevant questions for a TELCO such as the example in Table 1, in this
paper we propose and implement a CI process of promotions for TELCOs through the
creation of a telecommunications domain ontology and a polarity or sentiment analysis
domain ontology, the collection of all the post and comments in Facebook of 5
TELCOs and their classiﬁcation using the ontologies, the implementation of a semantic
search engine that allows TELCO answer relevant questions, be able to compare
themselves with the competition and have relevant information to improve their pro-
motions to be more competitive.
2
Literature Review
CI is a cyclic process composed of four phases, as shown in Fig. 2, where Collection,
Analysis and Dissemination phases are mentioned in [3–8]; the ﬁrst phase has several
names like Planning and Direction [3], Deﬁnition of the needs [5], Planning and Focus
[6] and Direction [7, 8], we propose as Identiﬁcation of information needs as Arroyo
[9]; Abdellaoui and Nader [3] and Chouder and Chalal [5] mention a ﬁfth phase called
feedback, we propose not consider it as a phase but as an output of Dissemination
phase to continue the cycle as Gógova [1]. The four phases comprise:
(1) Identiﬁcation of information needs: Identify information needs of key decision
makers about the competitors [9]. It will give the north or direction of what to
look for [1].
(2) Collection: Collect data from public and legal sources like social networks [10],
clean and organize it.
(3) Analysis: Classify and give meaning to the data to generate relevant information
and answer the information needs and as Gógova [1] says, to have a deep
understanding of the competitors.
Fig. 1. Example of original real post in Spanish
Fig. 2. Competitive intelligence cycle process
Competitive Intelligence Using Domain Ontologies
779

(4) Dissemination: Relevant information or intelligence is communicated to decision
makers so that they can use it in their plans [9] and strategies. New information
needs can be generated from this phase.
Several authors propose the use of domain ontologies for CI process, however there
are few completed studies with a real application as in this paper which is its main
contribution and novelty, Abdellaoui and Nader [3] proposed the use of a domain
ontology for each data source used for CI to explicit the semantics of the source,
Wongthongtham and Abu-Salih [11] used existing ontologies to obtain the semantics
of Twitter data to get the voice of the market and the voice of the customer and Liu
et al. [12] built a domain ontology of competitive intelligence including corporate and
product proﬁles and proposed an architecture for CI analysis system.
Regarding the ontology creation process, most authors propose to create it manually
with the support of experts in the domain [3–6, 12, 13], while other authors propose to
create it manually from existing ontologies [8, 11] and others propose to create it
automatically [14] or semi-automatic [7]. Regarding the use of ontologies to give
semantics to the data, most authors use them for Topic detection [3, 7, 8, 12, 14, 15] or
Concept classiﬁcation as in this paper and for Named entities recognition [4, 11, 13];
ontologies are also used for Event detection [6, 13] and for Sentiment analysis [11].
3
Materials and Methods
Based on CI Cycle Process of Fig. 2, we designed the CI Cycle Process for TELCOs
shown in Fig. 3, it starts with the Decision makers giving information needs that are
captured in (1) Identiﬁcation of information needs, then an (2a) Extractor collects the
data from the Facebook of 5 TELCOs (Movistar3, Claro4, Entel5, Bitel6 and Virgin7)
and sends the collected posts and comments to a (2b) Cleaner that generates cleaned
posts and cleaned comments, then the cleaned posts are used by a (3b) Posts classiﬁer
that applies a (3a) TELCO ontology to generate classiﬁed posts, cleaned comments are
used by a (3d) Comments classiﬁer that applies a (3c) Polarity ontology to generate
classiﬁed comments, then the Decision makers uses a (4a) Web application for
semantic search of posts that uses the (3a) TELCO ontology and the classiﬁed posts
and classiﬁed comments to ﬁnd the relevant posts, the found posts are sent to a (4b)
Web application for comparison of posts that gives CI to Decision makers and the
Cycle could continue with new information needs.
For the implementation of each component in Fig. 3 we used the following
Materials and Methods:
3 https://www.facebook.com/movistarperu/.
4 https://www.facebook.com/AmericaMovilPeruSAC/.
5 https://www.facebook.com/EntelPeru/.
6 https://www.facebook.com/bitelperu/.
7 https://www.facebook.com/VirginMobilePe/.
780
G. Colchado and A. Melgar

• (2a) Extractor: Python8 was used to program the extractor using requests9 library
for HTTPS and Graph Api10 of Facebook. To use Graph Api we registered as
developers11 and got a token from Facebook valid for 2 months. The extractor
generates the posts and comments in text ﬁles in CSV format.
• (2b) Cleaner: Python was used to program the cleaner. The cleaner generates
cleaned posts and cleaned comments in text ﬁles in CSV format.
• (3a) TELCO ontology: Protégé12 was used to create the ontology and generates a
ﬁle in RDF format. The reasoner HermiT13, included in Protégé, was used to
generate the inferences in a ﬁle in RDF format.
• (3b) Posts classiﬁer: Python was used to program the posts classiﬁer using
ontospy14 library to read the ontology in RDF format. The classiﬁer generates a
single text ﬁle in CSV format with all the classiﬁed posts.
• (3c) Polarity ontology and (3d) Comments classiﬁer: Are under development,
they will be explained in a future paper.
• (4a) Web application for semantic search of posts and (4b) Web application for
comparison of posts: Python was used to program both web applications using
microframework ﬂask15 and the libraries Google charts16 and bootstrap17.
Fig. 3. Competitive intelligence cycle process of promotions for TELCOs
8 https://www.continuum.io/downloads.
9 http://docs.python-requests.org.
10 https://developers.facebook.com/docs/graph-api.
11 https://developers.facebook.com/.
12 http://protege.stanford.edu.
13 http://www.hermit-reasoner.com/.
14 https://github.com/lambdamusic/OntoSpy/wiki.
15 http://ﬂask.pocoo.org.
16 https://developers.google.com/chart/.
17 http://getbootstrap.com/.
Competitive Intelligence Using Domain Ontologies
781

4
Results
4.1
Collection
Using the Extractor (Fig. 3(2a)), it was collected from Facebook 15,863 posts pub-
lished by 5 TELCOs since their ﬁrst post until May 16, 2017, the execution, in
sequence, took approximately 16 min and generated one CSV ﬁle per TELCO; once
we got the posts we executed the Cleaner (Fig. 3(2b)), that deleted 229 posts with
empty text, joined all CSV ﬁles and sorted the posts descending by date and time
generating one CSV ﬁle with 15,634 cleaned posts.
For each 15,634 cleaned posts, it was executed the Extractor (Fig. 3(2a)) on May,
2017 (days 17, 18, 21, 22, 23, 27, 28) that generated one CSV ﬁle with all comments
per post, it was collected from Facebook 1,430,315 comments, the execution, in
sequence, took approximately 14 and a half hours; once we got the comments we
executed the Cleaner (Fig. 3(2b)) that deleted 18,394 comments with empty text, sorted
the comments descending by date and time generating one CSV ﬁle with cleaned
comments per post, it was generated 1,411,921 cleaned comments. Table 2 shows the
ﬁnal corpus.
4.2
Analysis
For the development of TELCO ontology (Fig. 3(3a)), we started deﬁning these main
relevant concepts for mobile phones, from the point of view of one of the authors of
this paper that works for more than 10 years in telecommunications domain:
• Services: Services that the customer can use with a mobile phone.
– Voice: Allows customer to call mobiles of same TELCO (on-net) or other
TELCOs (off-net). Also, calls to ﬁxed numbers and international numbers.
– SMS: Allows customer to send on-net and off-net short text messages.
– Data: Allows the customer use internet and data apps such as, for example:
• Social networking apps: Facebook, Instagram, WhatsApp, Messenger.
• Music apps: Spotify, Apple music.
• Video apps: Youtube, Netﬂix.
• Rate plans: Offers the services to the customers with some conditions.
– Prepaid: The customer buys a money recharge and then buy packages of voice
minutes, a quantity of SMS or data megabytes to use the services until a due
date.
– Postpaid: The customer pays to TELCO a ﬁxed monthly fee money to have
ﬁxed quantities of voice minutes, SMS and data megabytes valid for one month
until the end of bill cycle. When the ﬁxed quantities ﬁnish:
• Controlled: Behaves like a Prepaid to use the services.
• Not Controlled or Open: The customer can continue using the services, and
excess is charged to customer in monthly bill.
Once deﬁned the main relevant concepts, we took the more recent 110 cleaned
posts (0.7% of total cleaned posts) and for each one we read the text and we extracted
manually the words that identify each concept, we found 119 new words for the
782
G. Colchado and A. Melgar

ontology in 61 cleaned posts and for the rest of 49 cleaned posts the extracted words
were already in the ontology, we also found more relevant concepts that we added to
the ontology. Figure 4 shows TELCO ontology which has 27 concepts and 6 relations
in 5 hierarchy levels. Table 3 shows an example of the extracted words that identify a
concept in TELCO ontology.
For each 15,634 cleaned posts, it was executed the Posts classiﬁer (Fig. 3(3b)) that
read all the words in TELCO ontology RDF ﬁle and searched the words in each post to
classify them in concepts; 10,489 cleaned posts (67% of total cleaned posts) were
classiﬁed at least with one concept.
4.3
Dissemination
The Web application for semantic search of posts (Fig. 3(4a)), has a word tree interface
that is user friendly for decision makers to select, in the word tree, several concepts and
Table 2. Final corpus from Facebook
TELCOs Cleaned posts
Cleaned comments
Quantity CSV ﬁles Size
Quantity
CSV ﬁles Size
Movistar
6,004
1
4.79 MB
446,874
5,781
68.28 MB
Claro
7,625
390,925
6,979
77.73 MB
Entel
975
231,958
974
38.31 MB
Bitel
869
330,444
868
54.15 MB
Virgin
161
11,720
159
1.82 MB
15,634
1
4.79 MB 1,411,921 14,761
240.30 MB
Fig. 4. TELCO ontology concepts and relations
Table 3. TELCO ontology examples of concepts and words
Concepts
Words that identify concept
Voice
call, talk, minutes, telephony
Data
chat, connect, 4g, megabytes, photo, gb, internet, speed, post, upload
Zero
unlimited, free, “without consuming your balance”, “without minimum balance”
Customer dinner, travel, club, concert, rock, coupon, gift, surprise, theater
Competitive Intelligence Using Domain Ontologies
783

words to search. Figure 5 shows the word tree interface, translated to English, that
follows the same structure of concepts and relations of Fig. 4 and words of Table 3.
Once selected several concepts and words to search, it is retrieved the results
highlighting in colors the words that identiﬁes the concepts as shown in Fig. 6, where
the decision maker can also select the button “Comparison” and the Web application
for comparison of posts (Fig. 3(4b)) shows a comparison of total posts per TELCO
(Fig. 7(a)) in pie chart, a comparison of total post per TELCO by year (Fig. 7(b)) in bar
chart, and a similar format (pie and bar chart) was programmed, but not shown due to
space limit in the paper, for total comments, total positive comments, total negative
comments and ﬁnally it shows three Top 10 lists of Posts with: more comments, more
positive comments and more negative comments.
Fig. 5. TELCO ontology word tree web interface for posts search (translated to English)
Fig. 6. Posts search (partially translated to English)
784
G. Colchado and A. Melgar

5
Discussion
It was extracted all the information of posts and comments available in Facebook for 5
TELCOs, since its beginning, because “CI requires information about the competitive
environment of past and present” [1].
A well-deﬁned ontology becomes the backbone of the system [8]. In this research,
the main component of the CI process is TELCO ontology, despite it was created
manually as most authors propose [3–6, 12, 13], it required only less than 1% (0.7% or
110 posts) of total cleaned posts to be reviewed and to extract the words related to
concepts to create the ontology that classiﬁed 67% of total cleaned posts. Due to its
importance, it was necessary an expert in telecommunications domain and a manual
process to create TELCO ontology to ensure success. In this case, does not apply
semi-automatic [7] or automatic [14] population of words in an ontology that not
requires an expert or only partially.
For comparison of posts it was implemented a bar chart of total post per TELCO by
year (Fig. 7(b)) and a similar format for total comments, total positive comments and
total negative comments. This format allows decision makers to perform a timeline
analysis and correlate events that probably explain the values obtained over time, this is
called Event Timeline Analysis (ETA) as proposed by Chouder and Chalal [5] and was
used by Dai et al. [13] in a research for detecting CI from Social Media.
6
Conclusions and Future Work
In this paper, we propose and implement a CI process of promotions for TELCOs since
the Identiﬁcation of information needs, the Extraction and Cleaning of posts and
comments from Facebook, the creation of a TELCO ontology, the classiﬁcation of
posts using TELCO ontology, the implementation of a Web application where decision
Fig. 7. Posts comparison (partially translated to English)
Competitive Intelligence Using Domain Ontologies
785

makers can get answers to relevant questions in a user friendly word tree interface, the
search of posts using several concepts and words of TELCO ontology and the com-
parison of found posts of promotions including competitors in a timeline format to have
CI for decision making.
The CI process and TELCO ontology creation process, proposed in this paper, can
be used by TELCOs from other countries and in other economic or business sectors
where there are several competitors that offers equivalent services or products that can
be compared and classiﬁed by a domain ontology; the competitors must post promo-
tions of their services or products on social networks where they have followers.
For Future Work, we will complete the development of the Polarity or Sentiment
Analysis domain ontology and the Comments classiﬁer; to improve TELCO ontology
we will validate their concepts relevance with TELCO decision makers based on their
information needs; to increase the percentage of classiﬁed cleaned posts from 67% to
90% we will review manually at least 1% of the 33% not classiﬁed cleaned posts. And
ﬁnally, to give more information to decision makers we will split timeline from years to
months in comparison bar charts and will change to line charts format for better event
timeline analysis and reviewing of trends.
References
1. Gógova, S.: Inteligencia Competitiva: ¿Espías? ¿Oráculos? ¿Estrategas?. Ediciones Díaz de
Santos, España (2015)
2. Arp, R., Smith, B., Spear, A.D.: Building Ontologies with Basic Formal Ontology. The MIT
Press, Cambridge, USA (2015)
3. Abdellaoui, S., Nader, F.: Semantic data warehouse at the heart of competitive intelligence
systems: design approach. In: 2015 6th International Conference on Information Systems
and Economic Intelligence (SIIE), pp. 141–145. IEEE, Hammamet, Tunisia (2015). https://
doi.org/10.1109/ISEI.2015.7358736
4. Spruit, M., Cepoi, A.: CIRA: a competitive intelligence reference architecture for dynamic
solutions. In: 2015 7th International Joint Conference on Knowledge Discovery, Knowledge
Engineering and Knowledge Management (IC3K), vol. 1, pp. 249–258. IEEE, Lisbon,
Portugal (2015)
5. Chouder, M.L., Chalal, R.: Models and tools support to the competitive intelligence process.
In: 2014 4th International Symposium on ISKO-Maghreb: Concepts and Tools for
Knowledge Management (ISKO-Maghreb), pp. 1–7. IEEE, Algiers, Algeria (2014). https://
doi.org/10.1109/ISKO-Maghreb.2014.7033466
6. Olszak, C.M.: An overview of information tools and technologies for competitive
intelligence building: theoretical approach. Issues Informing Sci. Inf. Technol. 11, 139–
153 (2014)
7. Chen, Y., Jin, P., Yue, L.: Ontology-driven extraction of enterprise competitive intelligence
in the Internet. In: 2008 Second International Conference on Future Generation Commu-
nication and Networking Symposia, FGCNS 2008, vol. 2, pp. 35–38. IEEE, Sanya, China
(2008). https://doi.org/10.1109/FGCNS.2008.72
8. Li, J., Huang, M., Zhu, X.: An ontology-based mining system for competitive intelligence in
neuroscience. In: Zhong, N., Liu, J., Yao, Y., Wu, J., Lu, S., Li, K. (eds.) WImBI 2006.
LNCS (LNAI), vol. 4845, pp. 291–304. Springer, Heidelberg (2007). https://doi.org/10.
1007/978-3-540-77028-2_17
786
G. Colchado and A. Melgar

9. Arroyo Varela, S.R.: Inteligencia competitiva: Una herramienta clave en la estrategia
empresarial. Ediciones Pirámide, Madrid (2005)
10. Muñoz Cañavate, A.: Recursos de información para la inteligencia competitiva: Una guía
para la toma de decisiones. Ediciones Trea, Gijón, Asturias (2012)
11. Wongthongtham, P., Abu-Salih, B.: Ontology and trust based data warehouse in new
generation of business intelligence: state-of-the-art, challenges, and opportunities. In: 2015
IEEE 13th International Conference on Industrial Informatics (INDIN), pp. 476–483. IEEE,
Cambridge, UK (2015). https://doi.org/10.1109/INDIN.2015.7281780
12. Liu, C., Yang, D., Wang, Y.: Domain ontology and semantic web applications for study of
web competitive intelligence analysis system. Int. J. Web Sci. 1(1–2), 99–113 (2011).
https://doi.org/10.1504/IJWS.2011.044083
13. Dai, Y., Kakkonen, T., Sutinen, E.: SoMEST: a model for detecting competitive intelligence
from social media. In: Proceedings of the 15th International Academic MindTrek
Conference: Envisioning Future Media Environments, pp. 241–248. ACM, Tampere,
Finland (2011). https://doi.org/10.1145/2181037.2181078
14. Hassan, T., Cruz, C., Bertaux, A.: Ontology-based approach for unsupervised and adaptive
focused crawling. In: Proceedings of the International Workshop on Semantic Big Data,
SBD 2017, p. 2. ACM, Chicago, USA (2017). https://doi.org/10.1145/3066911.3066912
15. Kimble, C., de Vasconcelos, J.B., Rocha, Á.: Competence management in knowledge
intensive organizations using consensual knowledge and ontologies. Inf. Syst. Front. 18(6),
1119–1130 (2016)
Competitive Intelligence Using Domain Ontologies
787

Sustainability Performance Evaluation
of Groundwater Remediation Technologies
Santoso Wibowo(&) and Srimannarayana Grandhi
School of Engineering and Technology,
Central Queensland University, Melbourne, Australia
{s.wibowo1,s.grandhi}@cqu.edu.au
Abstract. This paper presents a multicriteria group decision making method
for evaluating the sustainability performance of groundwater remediation
technologies. Interval-valued based intuitionistic fuzzy numbers are used to deal
with the inherent vagueness and imprecision of the performance evaluation
process. An efﬁcient algorithm is developed for producing a closeness coefﬁ-
cient for determining the sustainability performance of the most suitable
groundwater remediation technology alternative across all evaluation criteria.
An example is presented for demonstrating the applicability of the method.
Keywords: Groundwater remediation technologies
Vagueness and imprecision  Sustainability performance
Multicriteria  Decision makers
1
Introduction
Groundwater is one of the most important water sources for drinking in the world [1].
The use of groundwater is also of great important for agriculture and industrial pur-
poses. In fact, about 40% of the drinking water comes from groundwater, about 97% of
the rural population drinks groundwater, and about 30–40% of the water used for
agriculture comes from groundwater [2]. However, it is found the quality of ground-
water continues to deteriorate due to the inadequate protection urbanization processes
and agricultural intensiﬁcation activities [1], and at the same time, many of the
groundwater resources have been polluted by contaminants due to the intensive
industrialization [3]. These contaminants which include heavy metals, chlorides, sul-
phates, nitrogen and iron compounds and petroleum products can be lethal to human
health and at the same time, affect food chain if the groundwater is not treated [2, 3].
In order to protect the precious water resource and the environment for future
generations, various technologies have been developed for groundwater remediation
[4]. Groundwater remediation is the process of detecting and preserving groundwater
by using an appropriate treatment. The remediation process usually involves with either
removing contaminants or converting the hazardous materials into harmless products.
Many different types of groundwater remediation technologies are already available for
groundwater remediation [3, 4]. However, the process of selecting the sustainability
performance of the most suitable groundwater remediation is normally complex. This is
due to (a) the presence of multiple decision makers, (b) the conﬂicting nature of the
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_74

multiple evaluation criteria, and (c) the presence of vagueness and imprecision in the
decision making process.
This paper presents a multicriteria group decision making method for evaluating the
sustainability performance of groundwater remediation technologies. Interval-valued
based intuitionistic fuzzy numbers are used for dealing with the inherent vagueness and
imprecision of the performance evaluation process. An efﬁcient algorithm is developed
for determining the sustainability performance of the most suitable groundwater
remediation technology alternative. An illustrative example is presented for showing
the applicability of the proposed method.
2
Evaluating the Sustainability Performance of Groundwater
Remediation Technologies
The contamination of groundwater by heavy metal, originating either from natural soil
sources or from anthropogenic sources is a matter of utmost concern to the public
health [1]. Remediation of contaminated groundwater is of highest priority since bil-
lions of people all over the world use it for drinking purpose [3]. However, it is usually
difﬁcult for the decision makers to select the most suitable technology among multiple
alternatives. As a result, evaluating and selecting the most suitable groundwater
remediation technology for adoption becomes a critical issue.
There are various factors that affect the sustainability performance of groundwater
remediation technologies. Several researches have been conducted for identifying the
critical factors used in evaluating the sustainability performance of groundwater
remediation technologies. Pandey et al. [3] state that economic, environmental, and
social factors are important factors assessing the sustainability performance of the
groundwater remediation technologies. De Carvalho et al. [4] believe that environment,
economy and politics are the relevant factors for evaluating the suitable groundwater
remediation technology for adoption. Ren et al. [5] point out that technological and
political factors should also be incorporated in sustainability assessment as these two
aspects have signiﬁcant effect on sustainability. Kheliﬁet al. [6] state that technical,
economical, environmental and social factors are important for determining the suitable
groundwater remediation technology.
A comprehensive review of the related literature shows ﬁve criteria are important
for evaluating the sustainability performance of groundwater remediation technologies.
These criteria include Economic (C1), Environmental (C2), Technological (C3), Social
(C4), and Political (C5).
Economic (C1) criteria refer to the cost associated with the adoption and imple-
mentation of a speciﬁc groundwater remediation technology. As not all the remediation
technologies are suitable in every site conditions, cost comparisons are usually made on
the basis of costs associated with adoption and implementation of remediation tech-
nologies, excavation and disposal of waste [7]. Environmental (C2) criteria deal with
the environmental laws which may vary from country to country. The laws and reg-
ulations clearly state the processes and protocols to be followed in the clean-up process.
The regulations also outline transport and storage requirements to ensure the safety of
local environment [8]. Technological (C3) criteria deal with the efﬁciency of chosen
Sustainability Performance Evaluation
789

groundwater remediation technology and its ability to reduce risk. For example, some
of these groundwater remediation technologies may require extra amount of resources,
and therefore, they are not considered to be efﬁcient [9]. Social (C4) factors affect
people’s lifestyle and it is important to consider social factors in the selection of
groundwater remediation technologies. Social criteria refer to the planned use of a
speciﬁc remediation technology on the social fabric of communities [1, 7]. Political
(C5) refers to the use of remediation technologies to treat contaminated groundwater
without risking the needs of future generations. This aspect refers to the adoption of
technology in line with government policies. As groundwater is a valuable resource, it
has political ramiﬁcations and the remediation technology has to be in accordance with
government regulations [1, 10].
To effectively evaluate the performance of the most suitable groundwater reme-
diation technology in a given situation, it is important for the decision makers to
simultaneously consider the multiple evaluation criteria discussed as above.
3
The Multicriteria Group Decision Making Method
The groundwater remediation technologies performance evaluation problem with
respect to the multiple evaluation criteria can be formulated as a multicriteria group
decision
making
problem.
This
problem
includes
a
number
of
alternatives
Ai i ¼ 1; 2; . . .; n
ð
Þ with respect to each criterion Cij i ¼ 1; 2; . . .; n; j ¼ 1; 2; . . .; m
ð
Þ, to
be evaluated by multiple decision makers Dk. It is often that decision makers provide
subjective assessments for determining the performance of the available alternatives
with respect to each criterion and the relative importance of the criteria [11].
To model the vagueness and imprecision of the decision making process,
interval-valued intuitionistic fuzzy numbers [12] are used. For the multicriteria group
decision making problem, let A ¼ A1; A2; . . .; An
f
g be the set of n alternatives, C ¼
C1; C2; . . .; Cm
f
g be the set of m criteria, and D ¼ D1; D2; . . .; Dk
f
g be the set of
decision makers.
Step 1: Obtain the performance ratings with respect to all available criteria from the
decision makers. The interval-valued intuitionistic fuzzy decision matrix for each
decision maker is represented as RðkÞ ¼
rðkÞ
ij


mn, where k ¼ 1; 2; . . .; s and
rðkÞ
ij


¼
fðlLðkÞ
ij
; lUðkÞ
ij
Þ; ðmLðkÞ
ij
; mUðkÞ
ij
Þg is an interval-valued intuitionistic fuzzy number which
represents the performance rating of alternatives with respect to all available criteria.
ðlLðkÞ
ij
; lUðkÞ
ij
Þ indicates the range of degree to which alternative Ai satisﬁes criterion Cj,
while ðmLðkÞ
ij
; mUðkÞ
ij
Þ indicates the range of degree to which alternative Ai dissatisﬁes
criterion Cj. Here, w ¼ ðw1; w2; . . .; wmÞT is the weight of each decision maker and
P
n
j¼1
wj ¼ 1 and wj 2 0; 1
½
:
Step 2: Utilize the interval-valued intuitionistic fuzzy weighted geometric (IIFWG)
operator [13] for aggregating individual fuzzy decision matrices into a collective
interval-valued intuitionistic fuzzy decision matrix rij.
790
S. Wibowo and S. Grandhi

rij ¼
Y
s
k¼1
aij

k
;
Y
s
k¼1
bij

k
"
#
; 1 
Y
s
k¼1
1  cij

k
; 1 
Y
s
k¼1
1  dij

k
"
#
 
!
ð1Þ
Step 3: Determine the interval-valued intuitionistic fuzzy positive ideal solution a
and the interval-valued intuitionistic fuzzy negative ideal solution a as in (3) and (4)
respectively.
a ¼ ðr þ
1 ; r þ
2 ; . . .; r þ
m Þ
¼
maxi; r
ij j 2 B
j


; mini; r
ij j 2 C
j


D
E
i ¼ 1; 2; . . .; n
n
o
ð2Þ
a ¼ ðr
1 ; r
2 ; . . .; r
mÞ
¼
maxi; r
ij j 2 B
j


; mini; r
ij j 2 C
j


D
E
i ¼ 1; 2; . . .; n
n
o
ð3Þ
where B and C are the set of beneﬁt and cost criteria, r þ
j
¼ ½ðlL þ
j
; lU þ
j
Þ; ðmL þ
j
; mU þ
j
Þ
and r
j ¼ ½ðlL
j ; lU
j
Þ; ðmL
j ; mU
j
Þ .
Step 4: Calculate the weighted separation measures.
The distance between alternative Ai and the positive ideal solution S þ
i
and the
negative solution S
i can be calculated respectively as follows
S þ
i
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
4ðt þ 1Þp
X
n
j¼1
wj
tðlL
ij  l þ L
j
Þ  ðmL
ij  m þ L
j
Þ


p
þ tðmL
ij  m þ L
j
Þ  ðlL
ij  l þ L
j
Þ


p
þ tðlU
ij  l þ U
j
Þ  ðmU
ij  m þ U
j
Þ


p
þ tðmU
ij  m þ U
j
Þ  ðlU
ij  l þ U
j
Þ


p
*
+
p
v
u
u
u
u
t
ð4Þ
S
i ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
4ðt þ 1Þp
X
wj
tðlL
ij  lL
j Þ  ðmL
ij  mL
j Þ


p
þ tðmL
ij  mL
j Þ  ðlL
ij  lL
j Þ


p
þ tðlU
ij  lU
j
Þ  ðmU
ij  mU
j
Þ


p
þ tðmU
ij  mU
j
Þ  ðlU
ij  lU
j
Þ


p
*
+
p
v
u
u
u
u
t
ð5Þ
Step 5: Calculate the closeness coefﬁcient CCi. The closeness coefﬁcient determines
the ranking order of each alternative. This is calculated by
CCi ¼
S
i
S þ
i
þ S
i
; i ¼ 1; 2; . . .; n
ð6Þ
Step 6: Determine the ranking order of each alternative Ai based on the closeness
coefﬁcient CCi.
Sustainability Performance Evaluation
791

4
An Example
This section presents an illustrative example to demonstrate the applicability of the
proposed multicriteria group decision making method in evaluating the sustainability
performance of groundwater remediation technologies.
The sustainability performance evaluation process starts with the formation of a
committee involving three managers. Four evaluation criteria are identiﬁed for evalu-
ating the sustainability performance of groundwater remediation technologies which
include Economic criteria (C1), Environmental criteria (C2), Technological criteria
(C3), Social criteria (C4), and Political criteria (C5). Four groundwater remediation
technologies; namely monitored natural attenuation (A1), air sparging (A2), pump-treat
(A3), and permeable reactive barriers (A4) are to be evaluated for the remediation of the
groundwater.
The steps involved in the sustainability performance evaluation process are
described below.
Step 1: The performance ratings with respect to all available criteria are obtained
from the decision makers, shown as in Table 1.
Table 1. Interval-valued intuitionistic fuzzy decision matrix
Criteria
C1
C2
C3
C4
C5
A1
D1
([0.4;0.6],
[0.3;0.5])
([0.2;0.3],
[0.3;0.5])
([0.3;0.5],
[0.1;0.2])
([0.4;0.6],
[0.3;0.4])
([0.4;0.7],
[0.1;0.3])
D2
([0.3,0.4],
[0.1,0.2])
([0.6,0.7],
[0.1,0.2])
[0.2,0.3],
[0.1,0.2])
([0.2,0.3],
[0.1,0.2])
([0.1,0.3],
[0.2,0.4])
D3
([0.4,0.5],
[0.3,0.4])
([0.3,0.6],
[0.2,0.4])
([0.2,0.5],
[0.1,0.2])
([0.2;0.4],
[0.3;0.5])
([0.4,0.5],
[0.3,0.4])
A2
D1
([0.4,0.5],
[0.1,0.4])
([0.3,0.5],
[0.3,0.4])
([0.3,0.5],
[0.1,0.4])
([0.3,0.5],
[0.1,0.2])
([0.2,0.3],
[0.1,0.2])
D2
([0.3,0.6],
[0.2,0.4])
([0.2,0.5],
[0.1,0.2])
([0.3,0.5],
[0.1,0.4])
([0.3,0.6],
[0.2,0.4])
([0.2,0.4],
[0.1,0.2])
D3
([0.2,0.5],
[0.1,0.2])
([0.4,0.5],
[0.2,0.4])
([0.3,0.5],
[0.1,0.4])
([0.3,0.6],
[0.2,0.4])
([0.3,0.5],
[0.2,0.4])
A3
D1
([0.5,0.8],
[0.1,0.2])
([0.5,0.6],
[0.2,0.4])
([0.4,0.7],
[0.2,0.3])
([0.3,0.4],
[0.1,0.2])
([0.3,0.5],
[0.1,0.3])
D2
([0.7,0.8],
[0.1,0.2])
([0.6,0.7],
[0.2,0.3])
([0.5,0.6],
[0.3,0.4])
([0.5,0.9],
[0.2,0.4])
([0.6,0.7],
[0.1,0.2])
D3
([0.5,0.9],
[0.3,0.4])
([0.4,0.8],
[0.2,0.4])
([0.5,0.8],
[0.1,0.2])
([0.5,0.8],
[0.1,0.3])
([0.3,0.5],
[0.1,0.4])
A4
D1
([0.2,0.5],
[0.1,0.2])
([0.2,0.5],
[0.1,0.2])
([0.2,0.5],
[0.1,0.2])
([0.3,0.6],
[0.2,0.4])
([0.3,0.6],
[0.1,0.2])
D2
([0.3,0.6],
[0.1,0.2])
([0.1,0.3],
[0.1,0.2])
([0.2,0.3],
[0.1,0.2])
([0.2,0.5],
[0.1,0.2])
([0.3,0.4],
[0.2,0.4])
D3
[0.2,0.3],
[0.1,0.2])
([0.3,0.4],
[0.1,0.3])
([0.4,0.5],
[0.3,0.4])
([0.4,0.7],
[0.1,0.3])
([0.4,0.5],
[0.2,0.4])
792
S. Wibowo and S. Grandhi

Step 2: By using (1), the aggregated interval-valued intuitionistic fuzzy decision
matrix rij can be determined. The results are shown in Table 2.
Step 3: The interval-valued intuitionistic fuzzy positive ideal solution a and the
interval-valued intuitionistic fuzzy negative ideal solution a are calculated by using
(2) and (3). Table 3 shows the results.
Step 4: The weighted separation measures between alternative Ai and the positive
ideal solution S þ
i
and the negative solution S
i can be calculated based on (4) and (5)
respectively. Here, w is taken as 0:15; 0:3; 0:20; 0:20; 0:15
f
g, p ¼ 1, and t ¼ 1. Table 4
shows the results.
Table 2. The aggregated interval-valued intuitionistic fuzzy decision matrix
Criteria
C1
C2
C3
C4
C5
A1 ([0.117,0.162],
[0.564,0.783])
([0.136,0.248],
[0.425,0.592])
([0.081,0.129],
[0.664,0.825])
([0.102,0.148],
[0.379,0.426])
([0.094,0.116],
[0.273,0.459])
A2 ([0.168,0.233],
[0.341,0.526])
([0.104,0.239],
[0.214,0.416])
([0.142,0.358],
[0.416,0.572])
([0.118,0.318],
[0.573,0.865])
([0.167,0.263],
[0.561,0.721])
A3 ([0.246,0.582],
[0.672,0.874])
([0.439,0.682],
[0.762,0.963])
([0.377,0.646],
[0.741,0.842])
([0.569,0.871],
[0.241,0.472])
([0.173,0.346],
[0.741,0.842])
A4 ([0.135,0.269],
[0.529,0.632])
([0.122,0.178],
[0.529,0.637])
([0.138,0.277],
[0.265,0.481])
([0.127,0.184],
[0.529,0.632])
([0.228,0.369],
[0.324,0.581])
Table 3. Interval-valued intuitionistic fuzzy positive and negative ideal solutions
a
a
([0.148, 0.269], [0.528, 0.682]) ([0.237, 0.419], [0.336, 0.631])
([0.265, 0.381], [0.473, 0.589]) ([0.246, 0.447], [0.536, 0.772])
([0.567, 0.741], [0.335, 0.482]) ([0.158, 0.316], [0.386, 0.528])
([0.248, 0.524], [0.624, 0.783]) ([0.235, 0.478], [0.439, 0.793])
([0.138, 0.341], [0.426, 0.717]) ([0.168, 0.316], [0.143, 0.286])
Table 4. The weighted separation measures between alternatives and ideal solutions
S þ
i
S
i
0.116 0.169
0.127 0.288
0.154 0.427
0.133 0.254
Sustainability Performance Evaluation
793

Step 5: The closeness coefﬁcient CCi values can be calculated by using (6). Table 5
shows the calculated results.
Step 6: Alternative A3 is the most suitable groundwater remediation technology as it
has the highest closeness coefﬁcient value of 0.735, shown as in Table 5.
5
Conclusion
Evaluating the sustainability performance of groundwater remediation technologies is
complex due to the involvement of multiple decision makers, the conﬂicting nature of
the multiple evaluation criteria, and the vagueness and imprecision of the decision
making process. To effectively solve this problem, this paper has presented a multi-
criteria group decision making method for evaluating the sustainability performance of
groundwater remediation technologies. The multicriteria group decision making
method presented in this paper shows that the method is effective for dealing with the
groundwater remediation technology performance evaluation problem.
References
1. An, D., Xi, B., Ren, J., Wang, Y., Jia, X., He, C., Li, Z.: Sustainability assessment of
groundwater remediation technologies based on multi-criteria decision making method.
Resour. Conserv. Recycl. 119, 36–46 (2017)
2. Hashim, M.A., Mukhopadhyay, S., Sahu, J.N., Sengupta, B.: Remediation technologies for
heavy metal contaminated groundwater. J. Environ. Manage. 92, 2355–2388 (2011)
3. Pandey, V.P., Shrestha, S., Chapagain, S.K., Kazama, F.: A framework for measuring
groundwater sustainability. Environ. Sci. Policy 14, 396–407 (2011)
4. De Carvalho, S.C.P., Carden, K.J., Armitage, N.P.: Application of a sustainability index for
integrated urban water management in Southern African cities: case study comparison -
Maputo and Hermanus. Water SA 35, 44–151 (2009)
5. Ren, J., Andreasen, K.P., Sovacool, B.K.: Viability of Hydrogen pathways that enhance
energy security: a comparison of China and Denmark. Int. J. Hydrogen Energy 39, 15320–
15329 (2014)
6. Kheliﬁ, O., Lodolo, A., Vranes, S., Centi, G., Miertus, S.: A Web-based decision support
tool for groundwater remediation technologies selection. J. Hydro. Inf. 8, 91–100 (2006)
7. An, D., Xi, B., Wang, Y., Xu, D., Tang, J., Dong, L., Ren, J., Pang, C.: A sustainability
assessment methodology for prioritizing the technologies of groundwater contamination
remediation. J. Clean. Prod. 112, 4647–4656 (2016)
Table 5. The closeness coefﬁcient values of groundwater remediation technologies and their
rankings
Alternatives Closeness coefﬁcient Ranking
A1
0.593
4
A2
0.694
2
A3
0.735
1
A4
0.658
3
794
S. Wibowo and S. Grandhi

8. Wibowo, S., Deng, H.: Multi-criteria group decision making for evaluating the performance
of e-waste recycling programs under uncertainty. Waste Manage. 40, 127–135 (2015)
9. Musango, J.K., Brent, A.C., Amigun, B., Pretorius, L., Muller, H.: Technology sustainability
assessment of biodiesel development in South Africa: a system dynamics scenario. Energy
36, 6922–6940 (2011)
10. Cheremisinoff, N.P.: Groundwater Remediation and Treatment Technologies. William
Andrew Publishing, USA (1999)
11. Wibowo, S., Deng, H.: Consensus-based decision support for multicriteria group decision
making. Comput. Ind. Eng. 66, 625–633 (2013)
12. Atanassov, K., Gargov, G.: Interval-valued intuitionistic fuzzy sets. Fuzzy Sets Syst. 31,
343–349 (1989)
13. Xu, Z.S.: Methods for aggregating interval-valued intuitionistic fuzzy information and their
application to decision making. Control Decis. 22, 215–219 (2007)
Sustainability Performance Evaluation
795

Mobile Application to Encourage Local
Tourism with Context-Aware Computing
Carlos A. Silva1,2(&), Renato Toasa1,4, Juan Guevara1,5,
H. David Martinez3, and Javier Vargas4
1 School of Technology and Management, Polytechnic Institute of Leiria,
R. Gen. Norton de Matos, 2411-901 Leiria, Portugal
{2162317,2162342,2162315}@my.ipleiria.pt
2 Facultad de Ciencias Informáticas, Universidad Técnica de Manabí,
Av. José María Urbina y Che Guevara, 130104 Portoviejo, Ecuador
3 Universidad de Guayaquil, Guayaquil, Ecuador
hector.martinezvi@ug.edu.ec
4 Facultad de ingeniería en sistemas electrónica e industrial,
Universidad Técnica de Ambato, Ambato, Ecuador
js.vargas@uta.edu.ec
5 Facultad de Ingeniería, Ciencias Físicas y Matemáticas,
Universidad Central del Ecuador, Av. Universitaria y Av. América,
170129 Quito, Ecuador
Abstract. The mobile applications and context-aware or assistive technologies
have the potential to improve living conditions in society. Nowadays the
applications are of great help when the services or features that these offers are
based on the context of the user, in the area of tourism many applications only
offer to the people information of the place but it does not encourage the user to
go to visit it, neither provides help or suggestion of which would be the best
place to visit based on the current situation of the user. This work presents an
application for Android operating system that uses the user context-aware,
obtained through device sensors and the use of public APIs (Application Pro-
gramming Interface), offering to the users suggestions of places to visit based on
their context and encourage the user to visit more places, allowing them to
interact quickly with the environment in which they are through the use of their
mobile device.
Keywords: Context-aware  Tourism  API’s  Sensors  Android
Mobile application
1
Introduction
The smartphone adoption continues to permeate society, majority of people of the
world now use a smartphone. It is an irrefutable fact that, in recent years mobile phones
have revolutionized the entire world, and part of this important global trend are the
services or features that the applications offer. These services are possible through the
use of new technologies as context-aware, this allows people to interact directly with
the environment in which they are.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_75

Context-aware computing is a mobile computing paradigm in which applications
can discover and take advantage of contextual information such as user location, time
of day, neighboring users and devices, and user activity.
The major areas of context-aware computing evolved from desktop applications,
web applications, mobile computing, and ubiquitous computing to the Internet of
Things (IoT) over the last years [1].
Many researchers have studied this phenomenon and built diverse context-aware
applications. Chen and Kotz examined context-aware systems and applications, types
of context used and models of context information, systems that support collecting and
disseminating context and applications that adapt to changing context [2].
Different perspectives on how mobile applications can take advantage of context
have been advanced. These applications can automatically adapt their behavior
according to discovered context (active context), or present the context to the user on
the ﬂy and/or store it for the user to retrieve later (passive context) [3]. This has led to
context-aware computing, deﬁned in two ways: ﬁrstly, active context-aware automat-
ically adapts to discovered context by changing the application’s behavior; and sec-
ondly, passive context-aware presents the new or updated context to an interested user
or makes the context persist for the user to retrieve later.
This paper presents a prototype of an android application focused on the use of
context-aware systems, an analysis is performed of the API’s and sensors available to
work with context-awareness, the application was developed for android operating
system and uses API’s, such as Google Maps, Weather, others and mobile device
sensors, allowing to application to adapt their functionality based on the context in
which the users interact.
The rest of the paper is organized as follows. Section 2 describes the context and
state of the art. Section 3 presents the context-aware computing, while the context-
aware in mobile applications is detailed in Sect. 4. Next, in Sect. 5 show the applicable
areas where the context-aware could be used. Finally, in Sect. 6 the conclusions and
future work are presented.
2
Context and State of the Art
Currently there are few reports of mobile computing and the use of context-aware to
develop applications that discover and take advantage of contextual information, the
search for articles on this topic has proved unsuccessful, but a wider search helped ﬁnd
cases of this important topic, below:
In the work [4] is described the Pokemon Go App, which allows users to interact
with the environment in which it surrounds them, thanks to the sensors and services
available on the mobile device, this application implements context-aware and visu-
alizes it through the AR (Augmented reality). This application is attractive for the
ability to behave in a transparent way to the user when the environment changes.
There are also some health works that use context-aware in their applications. There
is a project called Poseidon and it is focused on empowering citizens with Down’s
Syndrome to support their independence outdoors [5]. The system helps people in their
Mobile Application to Encourage Local Tourism
797

school or work, restaurant or cinema or return home, by means of notiﬁcations of
location or climate.
Another research provides an overview of how the principles of context-aware
design may be applied to the design of future assistive devices to make them suitable
for older adults with dementia [6].
Within the investigation of this paper [7] mention is made to a case study that tries
to predict the preferences of the users and to provide the personalized services or
products based on the preferences, this system is based on the user input data, and then
it will present the preferences according to the context.
The application proposed in this research promotes tourism within the city as it is
based on tasks that users must perform, most of them are taken in some important
places or statues of the city, once the challenge is met the users earn points that are
added to the other challenges, there is also a penalty points when the user selects a help
within the application.
3
Context-Aware Computing
One challenge of mobile distributed computing is to exploit the changing environment
with a new class of applications that are aware of the context in which they are run.
Such context-aware system adapts according to the location of use, the climate, the
collection of nearby people, hosts and accessible devices as, a system with these
capabilities can examine the computing environment and react to changes to the
environment [8].
Context as any information that can be used to characterize the situation of an
entity, where an entity can be a person, place, or physical or computational object [9].
Context-awareness or context-aware computing as the use of context to provide
task-relevant information and/or services to a user.
Although information about the current context may be available to mobile
applications, how to effectively use that information is still a challenging problem for
application programmers. Schilit deﬁnes context-aware computing by categorizing
context-aware applications as follows [10]:
– Proximate selection, a user-interface technique where the objects located nearby are
emphasized or otherwise made easier to choose;
– Automatic contextual reconﬁguration, a process of adding new components,
removing existing components, or altering the connections between components
due to context changes;
– Contextual information and commands, which can produce different results
according to the context in which they are issued;
– Context-triggered actions, simple IF-THEN rules used to specify how context-
aware systems should adapt.
Different researchers have identiﬁed context types differently based of different
perspectives. In [11] the authors introduced one of the leading mechanisms of deﬁning
context types. They identiﬁed location, identity, time and activity as the primary
context type, for example: [11].
798
C. A. Silva et al.

– Computing context: Battery, network connectivity, work connectivity, nearby
resource;
– User context: User proﬁle, location, social situation, emotional state, nearby people;
– Physical context: Weather condition, lighting, noise, trafﬁc condition, temperature;
– Time context: Time of a day, week, month and season of the year;
– Context history: Searches, preferences, user cooperation and make the technology
as ‘calm’ as possible.
4
Context-Awareness in Mobile Application Prototype
The developed prototype consists of different modules developed, in each module
sensors and API’s were used to determine different user contexts. Figure 1 shows the
general architecture of the application and some tools used, it is also necessary mention
that some functions do not require internet connection because it performs a data
storage in a local database.
The functions of the application can be classiﬁed in three main modules that are
detailed below:
4.1
Generation Module
The challenges are generated partially, based on the user’s initial context, according to
the nearby places of interest, time of day and climate.
Fig. 1. General architecture in the mobile application.
Mobile Application to Encourage Local Tourism
799

For this module, the Open Weather map has been used, it is free meteorological
REST API that gives current climate information and makes some predictions, it has
been used for get the weather information and provide the user with the best challenge
to do in relation to the current weather.
4.2
Companion Module
Context-aware computing leads to new applications, especially in mobile computing,
and requires more infrastructure support to help eliminate unnecessary.
The application reacts to the current situation of the user, providing help notiﬁ-
cations according to location, time, GPS (Global Positioning System) signal and
availability of network access, allowing to offer help to user for complete the challenge.
In this module, Google Maps Android API has been used, this API allows to add
maps to Android applications, it is used to show the current location of the user in
relation to the distance and time to reach the location of the challenge, it allows to show
the distance of the place if the user is walking or driving.
In this module also has been used mobile device sensors, among which are GPS,
temperature sensor and states of the mobile device, such as battery charging status, type
of Internet connection WIFI (Wireless Fidelity), GSM (Global System for Mobile),
among others.
4.3
Evaluation Module
The API used in this module allows to analyze an image that the user could choose
from device gallery or take from camera, according to the challenge choose, the
application validate the answer and assigns the score to the user when the answer is
correct.
For this module has been use the Google Cloud Vision API, this REST API
allowed to get the content of an image by encapsulating powerful machine learning
models. It classiﬁes quickly the images into thousands of categories (e.g., “sailboat”,
“lion”, “Eiffel Tower”), detects individual objects and faces within images and ﬁnds
and reads printed words contained within images. It has been used to validate the
challenges answers, some challenges are taking photos in a speciﬁc place or with a
speciﬁc object, the answers of the challenges were already recorded and with these
parameters, it is evaluated if the users completed the challenge.
It has also been used the Firebase Real Time Database, this is a database hosted in
the cloud offered by Google, this API allows users to create new challenges and share
them with other users in the community, the changes are visualized in real time.
Once the development of each module mentioned above is completed, there is a
prototype of the mobile application and you can see the main interfaces below Fig. 2.
800
C. A. Silva et al.

5
Applicable Areas
The applications for the context-aware are inﬁnite; here there is a list of some appli-
cable areas and their respective example:
5.1
Health Field
Health care will evolve as new technologies are adopted. A system that use
context-aware healthcare systems will monitor patients as they maintain their normal
everyday activities, in order to warn the patients or healthcare workers of problems as
well as collecting data for trend analysis and medical research.
Health care systems could integrate context-aware computing, not only to explore
new tools but to propose useful and acceptable systems.
5.2
Home
Context-aware home consists of gathering all the information available in the home and
performing the tasks as discreet as possible, and with the least amount of effort by users
or people. Therefore, the information collected will be by sensors instead of requiring
the users to enter them [12].
Fig. 2. Main application interfaces.
Mobile Application to Encourage Local Tourism
801

Within the scenario of home could have the following: Lights, chairs and tables
automatically adjust as soon as the family gathers in the living room to watch TV.
5.3
Enterprise
The context-aware applications are currently more common in the consumer space, for
this reason the interest is growing in their enterprise value.
Contact and context-aware are a huge part of what is driving technology change at
the moment, and the reason is not just that devices increasingly have sensors in them,
but that innovation is increasingly connecting these devices with business.
5.4
Tourist
The context in the tourist applications provide information to the users, through current
location GPS. This is input information that the application uses to become and show
information to user. For example, where is the museum more near or where is the most
famous location in the city?
6
Conclusions and Future Work
This article examines context-awareness in a mobile application prototype. It explores
and discusses how the context is represented, classiﬁed, and applied, and also discusses
the API’s and the sensors with respect to context-aware models. Allowing you to
understand the beneﬁts it generates by facilitating people’s activities.
Considering all the issues and aspects of context-aware highlighted here, more
understanding of requirements in design and development of context-aware applica-
tions is necessary. Our paper investigates application of context-aware in modelling of
mobile applications.
Determining the user context is not an easy task, it is possible through sensors, but
sometimes some devices do not have these sensors or the quality of sensor data is very
poor, in this circumstance it is feasible to use API to determine the user context, such
as climate, nearby places, weather and others events, currently the API’s are growing,
there are many services offered through them, some are free and others are paid, even
so the subscription value is usually not very high.
Applications of this type allow to increase the growth of tourism within the city,
increasing the development of the city and society in the creation of new fun places or
services to give tourists.
As future work, we have to search and test more API’s that allow better deﬁnition
of the context of the users or that allows to obtain more speciﬁcations of the context,
and to ﬁnd a way to create and use these services without need of internet connection.
802
C. A. Silva et al.

References
1. Perera, C., Zaslavsky, A., Christen, P., Georgakopoulos, D.: Context aware computing for
the internet of things: a survey. IEEE Commun. Surv. Tutorials 16(1), 414–454 (2014)
2. Musumba, G.W., Nyongesa, H.O.: Context awareness in mobile computing: a review. Int.
J. Mach. Learn. Appl. 2(1), 5 (2013)
3. Teemu, H.L.: Technology Integration in Context-Aware Learning Spaces [unpublished Ph.
D. thesis]. University of Eastern Finland (2011)
4. Earl, A.: Pokémon GO – gaming gone mobile, 14 July 2016. [En línea]. https://www.
betterinternetforkids.eu/web/portal/practice/awareness/detail?articleId=1037236
5. Kramer, D., Augusto, J.C., Clark, T.: Context-awareness to increase inclusion of people with
DS in society. In: Workshops at the Twenty-Eighth AAAI Conference on Artiﬁcial
Intelligence (2014)
6. Mihailidis, A., Fernie, G.R.: Context-aware assistive devices for older adults with dementia.
Gerontechnology 2(2), 173–188 (2002)
7. Hong, J., Suh, E.H., Kim, J., Kim, S.: Context-aware system for proactive personalized
service based on context history. Expert Syst. Appl. 36(4), 7448–7457 (2009)
8. Schilit, B., Adams, N., Want, R.: Context-aware computing applications. In: 1994 First
Workshop on Mobile Computing Systems and Applications, WMCSA 1994, pp. 85–90.
IEEE (1994)
9. Dey, A.K.: Understanding and using context. Pers. Ubiquit. Comput. 5(1), 4–7 (2001)
10. Schilit, B., Adams, N., Want, R.: Context-aware computing applications. In: Proceedings of
IEEE Workshop on Mobile Computing Systems an Applications, pp. 85–90. IEEE
Computer Society Press, Santa Cruz, December 1994
11. Abowd, G.D., Dey, A.K., Brown, P.J., Davies, N., Smith, M., Steggles, P.: Towards a better
understanding of context and context-awareness. In: Proceedings of 1st International
Symposium on Handheld and Ubiquitous Computing, HUC 1999, pp. 304–307. Springer,
London (2009). http://dl.acm.org/citation.cfm?id=647985.743843
12. Meyer, S., Rakotonirainy, A.: A survey of research on context-aware homes. In: Proceedings
of the Australasian Information Security Workshop Conference on ACSW Frontiers 2003,
vol. 21, pp. 159–168. Australian Computer Society Inc. (2003)
Mobile Application to Encourage Local Tourism
803

Modelled Testbeds: Visualizing and Augmenting
Physical Testbeds with Virtual Resources
Stephane Kundig1(B), Constantinos Marios Angelopoulos2, and Jose Rolim1
1 University of Geneva, Geneva, Switzerland
{stephane.kundig,jose.rolim}@unige.ch
2 Bournemouth University, Poole, UK
mangelopoulos@bournemouth.ac.uk
Abstract. Testbed facilities play a major role in the study and evo-
lution of emerging technologies, such as those related to the Internet
of Things. In this work we introduce the concept of modelled testbeds,
which are 3D interactive representations of physical testbeds where the
addition of virtual resources mimicking the physical ones is made possi-
ble thanks to back-end infrastructure. We present the architecture of the
Syndesi testbed, deployed at the premises of University of Geneva, which
was used for the prototype modelled testbed. We investigate several
extrapolation techniques towards realistic value assignment for virtual
sensor measurements. K-fold cross validation is performed in a dataset
comprising of nearly 300’000 measurements of temperature, illuminance
and humidity sensors collected from the physical sensors of the Syndesi
testbed, in order to evaluate the accuracy of the methods. We obtain
strong results including Mean Absolute Percentage Error (MAPE) levels
below 7%.
Keywords: Internet of Things · Testbeds · Modelling · Visualization
1
Introduction
In the ﬁelds of computing and networking, experimental facilities and testbeds
play a key role in studying and evaluating new technologies. They provide the
necessary controlled environment and tools that enable researchers to run exper-
iments and evaluate novel protocols and architectures. They support agile archi-
tectures that are easy to re-conﬁgure in the context of experiments and provide
additional services and tools for collecting meta-information on the experiment
execution (e.g. monitoring several performance metrics or providing execution
logs for post-experiment processing).
However, despite the services and the advantages provided, testbeds also
pose some limitations. By nature, each testbed facility focuses on a speciﬁc
area of interest (e.g. in Internet of Things (IoT) applications or Machine-to-
Machine (M2M) communication protocols) and therefore its architecture and
the services provided deﬁne the supported experiments. Another limiting factor
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_76

Modelled Testbeds
805
is the number of available resources and subsequently the number of experiments
that can be run simultaneously. In general, scaling up an experimental facility
either for increasing its size, the number of simultaneous experiments supported
or for improving its availability, infers high costs and requires signiﬁcant eﬀort.
Trying to address the aforementioned restrictions of traditional testbed facil-
ities in IoT research, we introduce a new type of facilities; namely Modelled
Testbeds. A modelled testbed consists of two components. The ﬁrst component
consists of physical IoT resources (e.g. IPv6-enabled sensor motes) that are actu-
ally deployed in the premises of the facility. The resources operate with the aid
of a back-end system that orchestrates and monitors their operation, similarly to
any other regular testbed. The second component consists of virtual resources
that quantitatively and qualitatively augment the physical component of the
testbed. In particular, a digital model of the physical component is extracted
capturing the space of the facility and the operation of the physical resources.
Then, via an intuitive Graphical User Interface (GUI), virtual resources can be
spawned and deployed within the digital model in positions that correspond
to the actual premises of the facility. The operation of the virtual resources is
simulated in a way that captures the characteristics of the physical space (e.g.
light distribution) by drawing information out of the physical sensors. This way
the simulation of the virtual resources is seeded by the physical component and,
therefore, their operation is intertwined with that of the physical resources. The
end result is a mixed set of physical and virtual resources whose operation is
transparently perceived by the end-user as a uniﬁed testbed facility.
Following, we present the architecture of Syndesi; a smart building IoT
testbed deployed at the premises of University of Geneva that has been aug-
mented with virtual resources. Using the physical component of the derived
modelled testbed, a dataset consisting of 300’000 measurements has been built.
Several numerical methods are used in order to extract the corresponding values
that seed the simulation of the virtual resources. Their accuracy in capturing the
actual conditions of the physical space is evaluated using K-fold cross validation.
The results - including Mean Absolute Percentage Error (MAPE) levels below
7% - demonstrate that high accuracy can be achieved.
2
Related Work
Experiment driven research communities in the ﬁelds of computing and net-
working heavily rely on experimenting facilities. Researchers have been trying
to alleviate the limitations that characterise testbeds mainly by federating indi-
vidual facilities into meta-testbeds. This way, individual research teams can join
forces and provide researchers with the ability to run broader and more diverse
experiments by accessing several - potentially heterogeneous - testbeds.
Such federations are feasible with tools such as those introduced in [1]. The
OneLab experimental facility, presented in [2], is a leading prototype for a ﬂexi-
ble federation of testbeds that is open to the current Internet. GENI, the Global
Environment for Networking Innovation [3], is a distributed virtual laboratory

806
S. Kundig et al.
for transformative, at scale experiments in network science, services, and secu-
rity. The Fed4FIRE federation framework [4] is gradually enabling experiments
that combine facilities from the diﬀerent FIRE research communities while the
GEANT World Testbed Facility [5] focuses on regular computer networks. In
[6], the IoT Lab platform is presented where several IoT testbeds across Europe
are federated along with crowdsourced resources (e.g. smartphones) that are
provided by the general public. In [7], authors study the technological issues
related to the provision of a web-based simulation environment for supporting
interactivity between remote scheduling and control systems and a locally resi-
dent simulation system. Finally, in [8] authors present SWiMNet; a framework
for parallel simulation of wireless and mobile Personal Communication Service
(PCS) networks, which allows realistic and detailed modelling of mobility, call
traﬃc, and PCS network deployment.
All the above eﬀorts as well as additional ones in the literature, focus either
on federating diﬀerent and potentially heterogeneous physical testbeds or on
providing simulation frameworks as testbeds. On the contrary, our work focuses
on the fusion of physical testbeds and simulation frameworks and in particular
on how to utilize virtual resources in order to augment physical testbeds.
3
The Syndesi Testbed
The Syndesi testbed is a system and a platform comprised of heterogenous
devices, sensors and resources focusing on the Internet of Things. The ﬁrst ver-
sion [9] was mostly focused on providing personalized services for smart envi-
ronments combining sensor networks, electrical appliances, actuators and gate-
ways via various communication protocols and technologies such as Near-Field
Communication (NFC), Bluetooth, ZigBee, 802.15.4, 6LoWPAN etc. That ﬁrst
version, also referred as Syndesi 1.0, has been extensively updated over the last
years to the Syndesi 2.0.
3.1
Syndesi 2.0
The scope of the Syndesi 2.0 is not only to provide eﬃcient and smart services
to its users but also to serve as system-as-a-service. Through multiple entry
points, a large number of heterogeneous devices are able to interconnect with the
testbed and provide their resources. The architecture is designed with scalability
and interoperability in mind, which allows even mobile resources and data to be
aggregated in it. Since the update, smartphone users equipped with the Syndesi
2.0 Android application, can contribute with data collected from smartphone
sensors directly into the server’s database. In addition, the Syndesi testbed is
following a RESTful architectural approach providing interoperability between
its resources, devices, services and the internet. Beneﬁting from Syndesi’s REST-
enabled services, external requesting systems are able to access and manipulate
textual representations of resources exposed to the Web, using a uniform set of
stateless operations.

Modelled Testbeds
807
The overall architecture of the Syndesi 2.0 testbed is shown in Fig. 1. The
core of the Syndesi 2.0 testbed is behind the gateway. The role of this gateway
is two-fold; ﬁrst it serves as a connection point for all the elements and compo-
nents as the backbone wireless sensor network (WSN), the mobile crowdsensing
smart-phone application, the web etc. Second, the gateway is implemented on
a Linux based machine which acts also as the heart of the system in which all
the information from the various components are collected and stored. Since
September 2016, this server automatically queries the testbed sensors for mea-
surements, storing the sensed data in an SQL database as well as keeping track
of active/inactive sensors. To this day, the above database contains more than
300,000 measurements and it is being utilized for improving virtual sensor value
assignment.
Syndesi 2.0 
Framework
Architecture
Gateway
Rest – CoAP Proxy
WEB
BackBone-WSN
CoAP – IPv6 – 6LoWPAN
USERS with
Bluetooth,
Smartphones,
NFC tags
Electrical Interface
WSN for User 
IdenƟﬁcaƟon
ZigBee
Electrical Devices
DC
AC
Mobile Crowd Sensing
Fig. 1. Syndesi 2.0 framework architecture
3.2
Testbed Visualization
Traditionally, sensor testbeds are visually represented on top of ﬂoor plans, with
sensor position information being 2D. This trend is also evident in sensor network
emulator software, where visualization tools are often 2D only. We believe that
3D models are essential in order to store and represent models of testbeds, since
algorithms that require spatial information (e.g. for distance calculation) will fail
if the sensors are not co-planar, which is seldom the case in real world wireless
sensor network deployments. We propose a simple model for storing the entire
infrastructure, including buildings and interior objects spatial information as
well as the sensors spatial information. For buildings representation, we store
each block (either a wall, ceiling, ﬂoor or piece of furniture) by using 6 values:
its x, y, z position and a value for scaling on the x, y, z axis. Using these 6

808
S. Kundig et al.
Fig. 2. Interactive interface of a modelled testbed
values, we can recreate rooms and interior objects which are in turn used to
place sensors on. Regarding sensor information, only the spatial coordinates
x, y, z are stored as well as the type of sensor.
The same model is used for storing information of physical as well as virtual
sensors, making it easier for researchers to augment physical testbeds and use
physical and virtual sensors interchangeably in algorithms. In Fig. 2 we see a
screenshot of the interface that models Syndesi, which was developed with the
help of the Unity engine [10].
4
Augmentation with Virtual Resources
When navigating in the 3D-visualization interface, a researcher can create virtual
sensors to be added on the modelled testbed, which will have exactly the same
properties and functionalities as the physical ones. The virtual sensors can be
spawned anywhere a physical sensor could exist, i.e. in walls, windows etc.,
but not mid-air, and get their values from the physical ones via extrapolation
techniques described in detail in the next section. Interaction with the virtual
sensors such as drag and drop, sensor type conﬁguration or sensor deletion are

Modelled Testbeds
809
all provided by our interface. Once virtual sensors have been added in the 3D
model of the physical testbed, the new augmented testbed can be saved as a
separate instance, a function enabled by our back-end infrastructure, and then
re-loaded in the future for further inspection/editing. This way, based on a single
parent physical testbed, unlimited user-personalized modelled testbeds can be
created to serve researcher demands.
4.1
Value Assignment
To calculate the values assigned to the virtual sensors, two approaches are fol-
lowed. First, we look at the problem in a no-memory manner and we calculate
the virtual values at any requested time using information only of the present
moment, i.e. the values of all the physical sensors of the testbed. Then, with
the end goal of improving the overall accuracy we make use of the collected
dataset mentioned in Sect. 3, in order to identify relational patterns between
groups of physical sensors. These patterns are later used for associating newly
spawned virtual sensors with speciﬁc subsets of the physical ones, resulting in
more realistic value assignment.
Formally, we denote a physical sensor as:
si(xi, yi, zi),
si ∈S = {s1, s2, ..., sn}
(1)
where xi, yi, zi are the coordinates in space and S is the set of all n physical
sensors that belong to the testbed. Respectively, a virtual sensor is denoted as:
vi(xi, yi, zi),
vi ∈V = {v1, v2, ..., vm}
(2)
where the set V consists of the m virtual sensors created until that time in
the modelled testbed. Sensor measurements for illuminance, temperature and
humidity are denoted respectively as si(ill), si(tmp), si(hum) and likewise for
virtual sensors.
When a virtual sensor vk is spawned, e.g. with type illumance in an oﬃce A
containing a physical sensors, its measurement value is calculated as a weighted
average of the sensors located in the same oﬃce:
vk(ill) = w1
˙
s1(ill) + w2
˙
s2(ill) + ... + wn
˙
sa(ill)
n
i=1 wi
(3)
We examined the use of the inverse and the inverse square of the euclidean
distance between vk and each of si ∈S as weights in the above equation, given
that the spatial coordinates of virtual sensors are determined the moment it is
created. This initial approach provided average results (MAPE levels over 20%)
so we decided to use only a subset of the physical sensors s ⊂SA to be taken
into account in the calculation. In order to extract relevant subsets we eﬀected
the following brute force procedure using the past measurements stored in the
server database:

810
S. Kundig et al.
1. Remove all measurements from a single sensor sk.
2. Generate the powerset of S −{sk}, i.e. all possible sensor subsets excluding
the empty set.
3. Calculate the removed measurements via the weighted average using one sub-
set at a time.
4. Compare calculated and actual measurements and keep the subset that pro-
duced the least absolute error.
5. Repeat for all k ∈{1 . . . n}
With the above method, we associate every physical sensor with a speciﬁc
subset of the remaining sensors. That way, when a virtual sensor is spawned,
depending on the proximity to the physical sensors, it receives its values based
on the corresponding subset.
4.2
Evaluation of Accuracy
The dataset we conducted the evaluation comprises of a total of 295.536 measure-
ments, 94.140 for illuminance, 103.062 for temperature and 98.328 for humidity
collected from Syndesi during the period of 15-09-2016 to 31-12-2016. K-fold
cross-validation was applied to evaluate the accuracy of the method, with the
overall procedure depicted in Fig. 3. For each sensor type, the dataset was divided
in 10, as close to equal, folds and each time the training process was applied in all
of them but one which was kept for the testing phase. The process was repeated
until all folds were selected for testing. It is worth mentioning here that given
that a total of a sensors corresponds to 2a −1 possible subsets the above brute
force method does not scale well; in Syndesi testbed the experiments included 2
oﬃces each containing 6 sensors, a setting which is computationally aﬀordable,
but for larger infrastructures diﬀerent methods are envisioned.
Training module
Associated physical 
sensor subsets
TesƟng module
EvaluaƟon 
Results
Training Phase
TesƟng Phase
k folds
.  .  .
Unused fold
keep one
Dataset
Fig. 3. Evaluation process
The results of the evaluation are shown in Table 1. First, we notice that there
is minimal diﬀerence between inverse distance and inverse distance squared as a

Modelled Testbeds
811
choice for weights in the calculation function, with the latter producing slightly
better results. Overall, we observe very strong results regarding temperature and
humidity measurements with Mean Absolute Percentage Error (MAPE) values
around 1.5%. Such low values of MAPE ensure the credibility and scientiﬁc
correctness of experiments conducted in our modelled testbed. Illuminance is
a tougher value to predict, as it can have highly uneven distribution in space,
nevertheless the overall MAPE for all settings did not exceed 7% which is still
a good result concerning the accuracy of virtual sensors. Finally, the noticeable
diﬀerence between the two separate rooms regarding illuminance prediction (3%
MAPE for oﬃce A and 7% for oﬃce B) can be explained due to diﬀerent exposure
in sunlight which result in discontinuities that are harder to predict.
Table 1. Root mean square error and mean absolute percentage error average scores
after 10-fold cross validation.
Weighted average over inverse distance
Sensor: Illuminance (lux)
Temperature (◦C)
Humidity (%)
Room
Average
RMSE
Average
MAPE
Average
RMSE
Average
MAPE
Average
RMSE
Average
MAPE
Oﬃce A 9.22 ± 0.55
3.02 ± 0.12%
0.33 ± 0.01
1.07 ± 0.02%
0.60 ± 0.02
1.50 ± 0.01%
Oﬃce B 44.4 ± 3.21
6.58 ± 0.20%
0.60 ± 0.01
1.91 ± 0.03%
1.17 ± 0.10
2.61 ± 0.05%
Weighted average over inverse distance squared
Sensor: Illuminance (lux)
Temperature (◦C)
Humidity (%)
Room
Average
RMSE
Average
MAPE
Average
RMSE
Average
MAPE
Average
RMSE
Average
MAPE
Oﬃce A
9.39 ± 0.60 3.20 ± 0.14%
0.30 ± 0.01
0.98 ± 0.01%
0.58 ± 0.01
1.42 ± 0.01%
Oﬃce B 43.69 ± 3.41 6.57 ± 0.19%
0.61 ± 0.02
1.88 ± 0.04%
1.18 ± 0.10
2.62 ± 0.06%
5
Conclusion
In this paper we introduced the notion of modelled testbeds, which is based
on the fusion of physical testbeds and simulation frameworks. The University
of Geneva physical testbed Syndesi was modelled through an interactive GUI,
where the addition of virtual resources is made possible for the users. To assign
sensor measurement values to the virtual resources, several extrapolation meth-
ods were investigated and k-fold cross validation was performed to evaluate their
accuracy. Results from experimentation in a dataset of 300’000 ambient tempera-
ture, illuminance and humidity measurements, collected from Syndesi, show that
extrapolation from a subset of physical sensors based on proximity via weighted
average over inverse distance squared provides optimal accuracy. As future work,
we plan to extend the merging of physical and simulated environments to include
the networking layer, thus enabling a broader range of experiments which can
be conducted using modelled testbeds.

812
S. Kundig et al.
Acknowledgement. We would like to thank Dr. Orestis Evangelatos and Dr. Marios
Karagiannis for their initial work and ideas on the deployment and expansion of Syndesi
Testbed.
References
1. Aug´e, J., Parmentelat, T., Turro, N., Avakian, S., Baron, L., Larabi, M.A.,
Rahman, M.Y., Friedman, T., Fdida, S.: Tools to foster a global federation of
testbeds. Comput. Netw. 63, 205–220 (2014)
2. Baron, L., Klacza, R., Rahman, M.Y., Scognamiglio, C., Kurose, N., Friedman, T.,
Fdida, S.: Onelab tutorial: a single portal to heterogeneous testbeds. EAI Endorsed
Trans. Ubiquitous Environ. 2(6), e4 (2015)
3. Berman, M., Chase, J.S., Landweber, L.H., Nakao, A., Ott, M., Raychaudhuri, D.,
Ricci, R., Seskar, I.: GENI: a federated testbed for innovative network experiments.
Comput. Netw. 61, 5–23 (2014)
4. Fed4ﬁre project. https://www.fed4ﬁre.eu/
5. Farina, F., Szegedi, P., Sobieski, J.: G´eant world testbed facility: federated and
distributed testbeds as a service facility of g´eant. In: 2014 26th International Tele-
traﬃc Congress (ITC), Karlskrona, Sweden, 9–11 September 2014, pp. 1–6 (2014)
6. Alexandrou, P., Angelopoulos, C.M., Evangelatos, O., Fernandes, J., Filios, G.,
Karagiannis, M., Loumis, N., Nikoletseas, S.E., Rankov, A., Raptis, T.P., Rolim,
J.D.P., Souroulagkas, A.: A service based architecture for multidisciplinary IoT
experiments with crowdsourced resources. In: Ad-hoc, Mobile, and Wireless Net-
works, 15th International Conference, ADHOC-NOW 2016, Lille, France, 4–6 July
2016, Proceedings, pp. 187–201 (2016)
7. Cavalieri, S., Macchi, M., Valckenaers, P.: Benchmarking the performance of man-
ufacturing control systems: design principles for a web-based simulated testbed. J.
Intell. Manufact. 14(1), 43–58 (2003)
8. Boukerche, A., Das, S.K., Fabbri, A.: SWiMNet: a scalable parallel simulation
testbed for wireless and mobile networks. Wirel. Netw. 7(5), 467–486 (2001)
9. Evangelatos, O., Samarasinghe, K., Rolim, J.: Syndesi: a framework for creating
personalized smart environments using wireless sensor networks. In: 2013 IEEE
International Conference on Distributed Computing in Sensor Systems, pp. 325–
330, May 2013
10. Unity Technologies: Unity, a cross-platform game engine. https://unity3d.com/

Proposal for an Integrated Framework
for Mobile Applications Development
Danilo Martínez1,2(&), Xavier Ferré2(&), and Diego Marcillo1(&)
1 Universidad de las Fuerzas Armadas ESPE, Av. General Rumiñahui s/n,
Sangolquí, Ecuador
{mdmartinez,dmmarcillo}@espe.edu.ec
2 Escuela Técnica Superior de Ingenieros en Informática,
Universidad Politécnica de Madrid, Campus de Montegancedo,
28660 Boadilla del Monte, Spain
xavier@ﬁ.upm.es
Abstract. The development of mobile apps during the last decade has had a
signiﬁcant increase just like the mobile devices themselves. However, the pro-
cesses for developing those apps are still quite a few and not all of them consider
the particular aspects of the mobile ﬁeld. The present study proposes an inte-
grated framework for mobile applications development based on improve that
integrates the activities of the application development process with the main
aspects of mobile environment, with the goal of improving the quality of the
resulting applications.
Keywords: Framework  Mobile application  Agile methods
1
Introduction
The sharp rise in the development of mobile devices during the last decade have
increased its beneﬁts and have determined in many cases the need of having such
devices in our daily lives. These technological advances have allowed the development
of devices with better characteristics that help us carry out complex applications.
The development of mobile applications (apps) is a relatively new ﬁeld that
appeared from the hand of the ﬁrst smartphone, which for some authors, such as
Wroblewski [1] was the iPhone in 2007. However, development processes for appli-
cations are scarce, we did not ﬁnd any referent. The published literature on the
development process applied to generate apps is very scarce.
In industrial environment, the development of apps are carried out applying tra-
ditional software development processes that do not contemplate the particularities of
the mobile environment [2].
In this paper, we present a proposal framework for mobiles apps development, we
follow an agile approach and harmonize the activities of app development with the
most relevant aspects of app development. Being based on an agile approach allows to
obtain the app in a short period of time considering that the mobile market is changing
and very competitive.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_77

The main contribution of the paper is an agile framework for the development of
mobile apps, which aims to improve the quality and user experience of the app.
The remainder of the paper is structured as follows. Section 2 addresses Related
Work. Section 3 details the solution approach. Section 4 presents the proposal of
Integrated Framework for Mobile Applications Development. Finally, Sect. 5 outlines
the conclusions and future work.
2
Related Work
Since the advent of mobile devices there have been several attempts to generate new
processes for the development of applications. The strategy has been the combination
of existing methodologies or part of them.
Abrahamsson et al. [3] present Mobile-D an agile approach for mobile applications
development, which considers the particularities of the mobile environment such as
limited capabilities, various standards and network technologies, variety of different
platforms and speciﬁc needs of mobile users. Mobile-D is based on Extreme Pro-
gramming (development practices), Crystal methodologies (method scalability), and
Rational Uniﬁed Process (life-cycle coverage). A development project, following the
Mobile-D approach, is divided into ﬁve iterations. Each phase consists of three days
(Planning, Working, and Release). Mobile-D, being one of the ﬁrst reported approa-
ches, is very cited in the literature, however, being developed in 2004, several years
before the launch of the iPhone, it does not consider several aspects of new devices,
such as touch screens or Internet connection, among others. We must highlight the idea
of analyzing several methodologies and extract from them the activities that allow the
development of a mobile app for the devices of that time. Another point to highlight is
the way to manage the development time for each phase. Finally, the contribution of
the industry in the development of Mobile-D demonstrates the interest aroused by
mobile devices since its appearance.
Santos et al. [4], present a proposal that combines Challenge-Based Learning
(CBL) with Scrum for mobile applications development. CBL is a learning framework
but in this case, it was used to identify the functionalities of the application and
generate both the Product Backlog and the Sprint Backlog. Scrum is used to manage
the development of the mobile application through Sprints. The authors argue the ease
of understanding, ﬂexibility, effectiveness, among others. This proposal, in the ﬁrst
part, deals separately with the deﬁnition of the concept of the product, which is carried
out by applying CBL and the development of the application, for which Scrum applies.
Then it tries to link the last phases of CBL with the Sprint that is running. This proposal
does not speciﬁcally detail any analysis of the particularities mobile environments. It is
interesting the two frameworks are linked, the ﬁrst what (CBL) delivers the list of
functionalities of the new app and the second what (Scrum) takes that list as an input,
which highlights the ﬂexibility of the agile models.
Both proposals are based on well-known agile development processes, which
facilitates their application.
814
D. Martínez et al.

3
Solution Approach
The literature review results revealed the existence of a few appropriate processes for
apps development, the high degree of homogeneity of the development activities that
can form the processes, the application of traditional development processes that do not
consider the particularities of the mobile world, and the lack of a reference process for
app’s development. In addition, a series of mobile environment restrictions were
identiﬁed, including communication problems, energy consumption, a variety of
platforms, a variety of devices and information security.
On the other hand, we studied the way industry develops apps [2], it was observed
that there is a preference for agile methods. It was also observed that the particularities
of the mobile ﬁeld are not contemplated during the apps development process.
The proposed framework should aim at the construction of apps considering the
particularities of the mobile environment from the early stages. In addition, it must
handle the development time in such a way that it is as small as possible without
sacriﬁcing the quality of the application. It is essential to seek harmonization between
the activities of the application development lifecycle with the own or relevant aspects
of mobile context and an agile way of managing the process of application development.
The main characteristics that must comply with the new proposal are the following:
Agile Focus and Relevant issues for mobile design. For apps development, devel-
opers have a preference for agile processes. This preference is based on the fact that
agile processes are more ﬂexible and easily adapted to changing requirements and the
environment. At the literature level, Mobile-D [3] is cited as one of the ﬁrst agile
methodologies for apps development. According to the results obtained by Chandi
et al. [2] the industry prefers SCRUM as a development process. However, the
development of the app is performed like a desktop application, without considering
aspects of the mobile context. Additionally, authors like Wasserman [5] state that
mobile development should be done in short periods of time in such a way that the apps
can be competitive in the market.
One of the interesting aspects of agile processes, especially SCRUM, is the way in
which the project is managed. Time management during the Sprint is one of the
strengths, since each Sprint has a determined duration in which the product must be
obtained. The daily communication with team members, as well as periodic follow-up,
guarantees in part the completion of the project.
With these considerations, for our proposal we used the idea of Sprint to manage the
app development.
Relevant issues for mobile design. Mobile devices are not small computers; there-
fore, the development of their app must pay special attention to the characteristics of
such devices and the surrounding contexts.
The main characteristics of the mobile ﬁeld should be considered:
Network Connectivity. Connectivity is one of the key aspects of smartphones so many
applications depend on an Internet connection. Therefore, it is necessary to guarantee
the normal operation of the application even when connectivity is null. Also, it is
necessary to consider the bandwidth, especially in cellular networks, where the coverage
Proposal for an Integrated Framework for Mobile Applications Development
815

of the operator may affect it, this entails paying particular attention to the size of the ﬁles
that are transmitted and the associated computational cost.
Interoperability with other applications. An advantage of mobile applications is the
possibility of interacting with other applications hosted on the same device, which in
turn allows interaction with the hardware elements of the device, or that is presented as
external services. The interaction between applications is achieved through the use of
APIs that allow communication and consumption of resources between applications,
resources such as camera, GPS, communication sensors (Bluetooth, NFC), gyroscope.
Development Platform. The new applications should be executed in a range of plat-
forms as wide as possible, ensuring that the use of the app is consistent across all
platforms, so it is advisable to use those common elements between platforms. How-
ever, there will be cases in which, due to budgetary, technological, restrictions, app
only be implemented for one or two platforms, these cases, a process must be deﬁned to
select the best options.
Flexibility. The ﬂexibility of an app lies in the ability of the app to adapt to the various
contexts that the user determines. User-deﬁned contexts involve custom device set-
tings, brightness conditions, connectivity, power consumption, and more.
Energy. Energy consumption is one of the critical points in the mobile environment,
considering the battery life as a primary attribute of the device. The battery life depends
on several factors such as the amount of resources used by the apps, the exchange ofﬁles
(Bluetooth, NFC, etc.), connectivity through cellular networks, screen’s luminosity, use
of multimedia applications such as music, videos, use of camera ﬂash [6, 7]. This is an
aspect that you have to pay special attention, since if the app consumes many resources,
the device operating system can stop the app, or the user will stop using the app.
Devices heterogeneity. In the market, there are a diversity of mobile devices whose
characteristics vary in function of costs, brands, models, functionalities. These factors
determine the processing capacity, storage capacity, memory and other functionalities of
the device. The characteristics of the device directly affect its performance. Ideally, the
app should work optimally in all ranges of existing devices now and in the future. So,
when designing the app, it is necessary to deﬁne strategies to optimize the processing of
information, make efﬁcient use of memory and optimize the ﬁles that are stored.
Screen size. Has been one of a critical factor since the ﬁrst mobile devices appeared
because it is not being standardized. Currently, we have devices with small screens of
3.5-in such as the IPhone 4s and large screens that reach 7 in like the Samsung Galaxy J
max. This is still a factor that should be given special attention, as it directly affects the
perception of the app. Currently the main development platforms propose their own
design guides [8–10], as well as studies that present user interface design patterns for
mobile applications [11] that can help in the design and improve the user experience.
Data Security. The information stored mobile devices can be considered as an asset that
can be the target of an attack by exploiting the vulnerabilities and threats of the system.
The vulnerabilities that a device faces can be internal such as implementation errors,
incompatibility or user unawareness, and external that relate to network connectivity and
816
D. Martínez et al.

external objects such as APIs or web services [12]. While the threats may be malware,
phishing, platform alteration. When developing the new app, you should consider
strategies to safeguard sensitive device information such as the interaction with antivirus
programs, use of secure APIs, authentication systems, spam ﬁlters.
4
Proposed Solution
The proposal is a framework that integrates application development activities according
to the IEEE 1074-1997 standard [13] and the SWEBOK, Software Engineering Body of
Knowledge [14] with the management of activities presented by SCRUM [15] and the
relevant aspects of the design of mobile applications identiﬁed in the literature review and
the industry [2].
The framework is formed by three layers; the ﬁrst layer is conﬁrmed by the type
activities of developing software life cycle processes. The second is the Scrum Core in
which the Sprint idea is used to manage the development of the application. The third
layer contains the relevant aspects for the design of mobile applications, see Fig. 1.
The development of an app starts with the deﬁnition of the concept of the product,
for which techniques such as Surveys, user stories, surveys, interviews, are applied. As
well as deﬁning a general architecture of the app where the identiﬁed requirements until
they are going to be coupled.
Once the product concept has been deﬁned, the list of functionalities that the new
app (Product Backlog) must have is generated. Next, the functionalities that are going
to be developed are selected and the Sprint list is generated (Sprint backlog). For the
planning of the implementation of the functionalities, the mobile key issues for mobile
application design are used. According to the functionality that is going to be imple-
mented, the development team analyzes and deﬁnes which aspects are to be taken into
account. This part of the proposed framework is important because it is the difference
between the development of a traditional application and a mobile application since all
Fig. 1. Integrated framework for mobile application development
Proposal for an Integrated Framework for Mobile Applications Development
817

the functionalities are going to be implemented paying special attention to the partic-
ularities of the device where the app will be executed.
When the list of functionalities is ready, we proceed to give way to the Sprint in
which we will execute development activities according to the selected type activity.
As it is an agile process, not all the type activities should be executed in the Sprint, but
the developers evaluate and deﬁne which are the activities to be executed for the
implementation of the functionality.
For each activity, a series of techniques and tasks that can be read during the
execution of the Sprint have been selected, as shown in Fig. 2.
When the sprint ﬁnishes, the sprint review meeting is held, where the product or
functionality is deﬁned, otherwise the Sprint list is feedback to the new sprint.
During the app implementation, the team can use the relevant aspects, as well as in
the Sprint review.
After the Sprint review, the Sprint retrospective meeting can be held, where the
work done is inspected and an improvement plan is created that will be addressed in the
new Sprint.
5
Conclusion and Future Works
This paper proposes an integrated framework for mobile application development
based on an approach that integrates the activities of the life cycle of the development
of applications with a set of relevant aspects of the mobile ﬁeld with the aim of
improving the quality of the product and increasing the user experience.
The main future work of this proposal is the validation of the Framework both in
the academic context and in the industry. In the academic context, we can validate with
two groups of developers. The ﬁrst group consists of undergraduate students of the ﬁrst
levels with little experience in application development, and the second group formed
by students from the high levels who already have experience in software development.
Fig. 2. Techniques, Tasks y Activities
818
D. Martínez et al.

In the ﬁeld of industry, we can identify companies or freelance developers that apply
the Framework in the development of an app.
References
1. Wroblewski, L.: Mobile First. Jeffrey Zeldman Designer, New York (2011)
2. Chandi, L., Silva, C., Martínez, D., Tatiana, G.: Mobile application development process. In:
Rocha, Á., Alturas, B., Costa, C., Reis, L.P., Cota, M.P. (eds.) 2017 12th Iberian Conference
on Information Systems and Technologies (CISTI), Lisboa, pp. 2113–2118 (2017)
3. Abrahamsson, P., Hanhineva, A., Hulkko, H., Ihme, T., Jäälinoja, J., Korkala, M., Koskela,
J., Kyllönen, P., Salo, O.: Mobile-D: an agile approach for mobile application development.
In: OOPSLA, pp. 174–175 (2004)
4. Santos, A.R., Fernandes, P., Sales, A., Nichols, M.: Combining challenge-based learning and
scrum framework for mobile application development. In: ACM Conference on Innovation
and Technology in Computer Science Education, pp. 189–194 (2013)
5. Wasserman, A.I.: Software engineering issues for mobile application development. In:
Proceedings of FSE/SDP Workshop on Future of Software Engineering Research, pp. 397–
400. ACM (2010)
6. Carroll, A., Heiser, G.: An analysis of power consumption in a smartphone. In: Proceedings
of 2010 USENIX Conference USENIX Annual Technical Conference, p. 21 (2010)
7. Perrucci, G.P., Fitzek, F.H.P., Widmer, J.: Survey on energy consumption entities on the
smartphone platform. In: IEEE Vehicular Technology Conference, pp. 1–6 (2011)
8. Apple: Themes - Overview - iOS Human Interface Guidelines. https://developer.apple.com/
ios/human-interface-guidelines/overview/design-principles/
9. Android Developers: User Interface Guidelines | Android Developers. https://developer.
android.com/guide/practices/ui_guidelines/index.html
10. Microsoft: Controls design guidelines for Windows Phone. https://msdn.microsoft.com/en-
us/library/windows/apps/hh202879(v=vs.105).aspx
11. Nilsson, E.G.: Design patterns for user interface for mobile applications. Adv. Eng. Softw.
40, 1318–1328 (2009)
12. Jeon, W., Kim, J., Lee, Y., Won, D.: A practical analysis of smartphone security. In: Human
Interface and the Management of Information. Interacting with Information, pp. 311–312
(2011)
13. Schultz, D.: IEEE standard for developing software life cycle processes (1997)
14. IEEE Computer Society: Guide to the software engineering body of knowledge version 3.0
(2014)
15. Schwaber, K., Sutherland, J.: The Scrum Guide, vol. 17 (2013). Scrum.Org, ScrumInc
Proposal for an Integrated Framework for Mobile Applications Development
819

Computer Networks, Mobility and
Pervasive Systems

An Adaptive-Bounds Band-Pass
Moving-Average Filter to Increase Precision
on Distance Estimation from Bluetooth RSSI
Diego Ord´o˜nez-Camacho(B) and Edwin Cabrera-Goyes
Computer Science Department, Universidad Tecnol´ogica Equinoccial,
Quito, Ecuador
{dordonez,cgep85623}@ute.edu.ec
Abstract. Estimating distance from RSSI is not a straightforward task,
especially when using consumer devices. The signal presents large levels
of noise, and it is heavily aﬀected by the conditions of the environment
and by the devices themselves. In this paper we characterize experimen-
tally diﬀerent conditions of this noise, then we propose a ﬁlter to reduce
it and to smooth and stabilize the signal; ﬁnally, we apply the ﬁlter to our
test environment and validate its data. The experimental results showed
that the ﬁlter has notorious beneﬁts on precision when estimating dis-
tance from RSSI signal.
Keywords: RSSI · Bluetooth · Signal ﬁlter
Indoor positioning system
1
Introduction
The Global Positioning System (GPS) is the most widely used technique for
localization [1]. It presents, nevertheless, substantial problems indoors, becoming
almost useless [2].
Indoor Positioning Systems (IPS) are designed for determining location where
no GPS lock is possible [3]. Radio Frequency (RF) emitters can be used to
broadcast a signal. Receivers capture the signal and process it to estimate the
distance from the emitter. Gathering the signal from at least three emitters, it
is possible for a receiver to infer its position [4,5].
Calculating the distance from RF signals involves at least two conditions:
the position of the emitters must be well known by the receivers, and the signal
strength must decay through distance. Signal strength decay is intrinsic on RF
communications systems like WiFi or Bluetooth [6].
Bluetooth seems to be pervasive nowadays; it is present even in household
appliances. Moreover, people have Bluetooth in their smartphones. Because of
the availability and reasonable price of second-hand Android consumer devices,
they are a good candidate for building experimental positioning systems [7].
c
⃝Springer International Publishing AG 2018
´A. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_78

824
D. Ord´o˜nez-Camacho and E. Cabrera-Goyes
When Android Bluetooth devices are set as discoverable, they emit a signal
that can be captured by receivers set into discovery mode. Then, it is possible
to extract information about the emitter, like name, address and the Received
Signal Strength Indicator (RSSI). RSSI is a value in the theoretical range from
0 dBm to −100 dBm, that decreases as the distance between emitter and receiver
increases [8].
Estimating distance from RSSI is not a straightforward task. The signal
is heavily aﬀected by the conditions of the environment and by the devices
themselves. Minor variations produce diﬀerent readings and, even under similar
conditions, the signal itself is noisy and unstable [9]. Previous studies have shown,
nevertheless, that ﬁlters can have a positive eﬀect on RSSI signal quality, and
therefore on distance estimation [10].
1.1
Related Work
The ﬁeld of IPS has been studied for some years already, and there has been a
boom recently, probably with the advent of the Internet of things. We present
some up-to-date related work, considering mainly its relevance regarding signal
ﬁlters.
Using Bluetooth Low Energy beacons, Heo and Kwon [11] use smartphones as
receivers and propose a compensated gyroscope sensor algorithm to clear noise.
Onofre et al. [12] use Fuzzy Logic to deal with the accuracy problem. Kuxdorf
et al. [13] implement a bi-directional mechanism using both, the emitter and the
receiver to calibrate the signal. Jadidi et al. [14] use Gaussian Processes clas-
siﬁcation to learn decision regions, accepting measurements that are consistent
with this model.
On ZigBee networks, Zong-zuo and Gai-zhi [15] study how Kalman ﬁlters can
improve RSSI accuracy. Ayka¸c et al. [16] work with particle ﬁlters. Lin et al. [17]
propose a modiﬁed least squares iterated method to reduce errors and optimize
the relationship between reference and destination nodes, making positioning
results closer to the actual location of the node.
On WiFi networks, Luo et al. [18] propose a data distribution-based ﬁnger-
printing to reduce error on distance estimation. Pyda et al. [19] introduce a
secure localization protocol, and Xue et al. [20] use a variable number of max-
imum RSSI measures, to tackle the multipath interference. Finally, Nagaraju
et al. [21] use a single anchor node with sector antenna, that estimates the
distance of the target node pertaining to a particular sector, with an included
interference avoidance mechanism.
1.2
Contributions
This paper delves into the problem of noise and instability. Based on the pre-
sented background, we provide the following contributions:
– An experimental characterization of RSSI signal noise and stability on
Android consumer devices, and its relationship with distance.

An AbBpMa RSSI Filter for Distance Estimation
825
– An Adaptive-Bounds Band-Pass Moving-Average (AbBpMa) Filter is pro-
posed, justiﬁed and deﬁned.
– The beneﬁts of the approach are shown experimentally, and data to validate
the improvement on distance estimation precision is presented.
In the remainder of the paper we present, in Sect. 2, the main characteristics
of the RSSI noise and instability. In Sect. 3, the proposed ﬁlter is described. In
Sect. 4, the eﬀects of the ﬁlter on distance estimation are justiﬁed. Finally, in
Sect. 5, conclusions are drawn and future work is suggested.
2
Noise on Bluetooth RSSI Measurements
To measure noise and stability on Bluetooth RSSI, we designed several experi-
ments, all of them considering only Android consumer devices. The most rele-
vant experiment, used throughout the paper (unless explicitly noted), consisted
of 1715 signal samples, captured across a distance range of 1 m to 27 m, with
1 m intervals. Data was sent by two emitters, an I8190 (S3) and a p500h(LG),
and captured by two receivers, an F5121 (SX) and a N9000 (N3). There were
no obstacles between emitters and receivers, and the experiment took place on
a corridor, reported as a high noise environment [18].
Strength of RSSI signal is supposed to decrease as distance between emitter
and receiver increases. It can be shown experimentally, as seen in Fig. 1, that
if we gather enough data and calculate a linear regression, this is the case. It
can also be seen, nevertheless, that measurements are not stable. We have a
variance (σ2) of 56.79 in average, with a Relative Absolute Error (RAE) of
72.45 %, as calculated against a linear regression with a 10-fold cross validation.
The diﬀerence in signal strength, for any distance, varies from 18 dBm to 34 dBm.
Moreover, diﬀerent Bluetooth devices present diﬀerent amplitude signatures,
as seen on Fig. 2. For the same distances, with the same emitter, two diﬀerent
receivers have a diﬀerence in amplitude of 12.9 dBm on average; it is also possible
to see that the diﬀerence is not stable across the distance, ranging from 0.3 dBm
to 24.7 dBm.
Finally, it is relevant to understand if noise in the signal depends on distance.
As we saw, diﬀerent devices provide diﬀerent readings. In Fig. 3 we can see
how error behaves with respect to distance. x axis presents distance, and y
axis presents the error with respect to the regression line. The density chart
shows that in the middle of the distance range, at around 15 m, error amplitude
decreases; we can see, nevertheless, that at the same distance error is more dense.
On both extremes of the chart the error can be considered equally high. An small
increment in amplitude on the close range, is compensated with more density on
the far range. This last fact contradicts what we expected when being close to
the emitter, especially considering the already seen relationship between RSSI
and distance. We can only conclude that the noise is present across the whole
distance range in similar proportions, and that we should treat it identically,
ruling out the small diﬀerences.

826
D. Ord´o˜nez-Camacho and E. Cabrera-Goyes
−90
−80
−70
−60
−50
0
5
10
15
20
25
Signal (dBm)
Distance (m)
Fig. 1. Dispersion on RSSI measurements
−60
−40
−20
Distance (m)
Signal (dBm)
0
0.2
0.4
0.6
0.8
1
Fig. 2. Amplitude signature for two diﬀerent receivers, an S5830 and a D6503, across
a range of 0 m to 1 m

An AbBpMa RSSI Filter for Distance Estimation
827
Fig. 3. Errors with respect to a regression line
3
An Adaptive-Bounds Band-Pass Moving-Average
Filter
Raw RSSI signal is not reliable, especially when using smartphones [11]. Even
when several measurements are taken from the same position and with the same
equipment, values diﬀer signiﬁcantly in amplitude. This circumstance leads us to
think that a plausible solution could be to apply a ﬁlter to smooth the signal and
remove extreme values. Considering that the signal strength naturally changes
with distance, the ﬁlter ought to adapt itself, changing its bounds according to
signal conditions over time.
Smoothing the signal can be achieved with a moving-average ﬁlter, easy to
implement and fast to execute. Dealing with extreme values, considering that
signal jumps both, to the high and to the low amplitude, can be possible thanks
to a band-pass ﬁlter. Finally, given that the cut-oﬀvalues of the band-pass ﬁl-
ter cannot remain static, an adaptive-bounds mechanism should be conceived.
Bounds must smoothly shift up and down, responding to signal strength varia-
tions as distance changes.
The ﬁlter is deﬁned in Eqs. 1 to 4. In (1) we calculate the lower bound for
the nth input of the band-pass ﬁlter. f[n] represents the must recent n values
ﬁltered by the band-pass stage. α is a small constant allowing to admit values
slightly smaller than the minimum calculated by min; this allows the bound to
better adapt to sustained decrease in signal input. In (2) we calculate the upper
bound of the ﬁlter, with similar semantics to lb(n), where β is equivalent to α
for the upper bound. Then, in (3), we apply the ﬁlter to the nth element of the
raw RSSI input represented by raw(n), considering the lower and upper bounds.
Finally, in (4), we calculate the ﬁnal average value for the input.

828
D. Ord´o˜nez-Camacho and E. Cabrera-Goyes
lb(n) = min(f[n])
1 + α
(1)
ub(n) = max(f[n]) ∗(1 + β)
(2)
f(n) =
⎧
⎪
⎨
⎪
⎩
lb(n−1),
if raw(n) ≤lb(n−1)
ub(n−1),
if raw(n) ≥ub(n−1)
raw(n),
otherwise
(3)
¯f(n) = 1
n

f[n]
(4)
The presented ﬁlter have three parameters that can be calibrated to adapt
it to the conditions of the signal:
– n, the window size that deﬁnes how many raw values will be used to calculate
the ﬁltered value;
– α, the constant allowing the lower bound to adapt to a decreasing strength
of the signal;
– β, the constant allowing the upper bound to adapt to an increasing strength
of the signal.
4
Improving Precision on Distance Estimation
To calibrate the ﬁlter we used a small dataset. RSSI was captured, as the receiver
was progressively going from 0 m to 5 m. We used a window n = 5, recommended
on a previous study [22]. The lower and upper bounds constants where set to
α = 0.015 and β = 0.013 that preserve the slope of a regression line, thus
preserving the RSSI - distance relationship found on the raw data. Figure 4
shows the diﬀerence between raw (black) and ﬁltered (red) data. The slopes
diﬀerence is 0.001 only, but R2 improves dramatically, going from 0.08 to 0.45.
Then, we applied the ﬁlter to the large dataset presented on Sect. 2. We can
see on Fig. 5 the ﬁltered dataset (red dots), superimposed to the raw dataset
(gray dots). Signal peaks have been reduced, and data is now more dense. Global
variance fell from 56.79 to 43.95. More important, the distance estimation by
linear regression has been improved as well, with a RAE reducing from 72.45 %
to 67.27 %. After a 10 times 10-fold cross validation, we found that the diﬀerence
is signiﬁcant (t = 10.296, p < 2.2e−16).
As shown in Sect. 2, diﬀerent devices provide diﬀerent RSSI signatures. Some
combinations of emitter-receiver can be particularly noisy, as we can see on the
ﬁrst row of Table 1, with an error reduction of only 1.23 %. The other device
combinations present less noise, and RAE can be reduced, thanks to the ﬁlter,
by up to 20.51 %.

An AbBpMa RSSI Filter for Distance Estimation
829
−80
−75
−70
−65
−60
−55
Distance (m)
Signal (dBm)
0
1
2
3
4
5
Line types
raw
lmraw
flmf
Fig. 4. Filtered versus raw RSSI data, on a 0.5 m–5 m range, with n = 5, α = 0.015
and β = 0.013, between a D855 receiver and a S5830 emitter
−100
−90
−80
−70
−60
−50
−40
0
5
10
15
20
25
Signal (dBm)
Distance (m)
Fig. 5. Dispersion data from the large dataset, ﬁltered

830
D. Ord´o˜nez-Camacho and E. Cabrera-Goyes
Table 1. Improvements in Relative Absolute Error between raw and ﬁltered RSSI.
Diﬀerent combinations of devices
RAE
Improvement
devices raw
¯f
raw −¯f
LG-SX
73.57% 72.34%
1.23%
LG-N3
47.43% 31.64% 15.79%
S3-SX
48.49% 33.70% 14.79%
S3-N3
59.86% 39.35% 20.51%
5
Conclusions and Future Work
In this paper we analyzed noise and instability on RSSI signal, when using
Android consumer devices as emitter and receiver nodes on an IPS. We conﬁrmed
that the RSSI signal strength is inversely related to distance, and we found that
noise presence is not dependent on the distance between intervening nodes. It was
also shown that diﬀerent devices present diﬀerent patterns of signal amplitude.
We proposed the Adaptive-Bounds Band-Pass Moving-Average Filter. The
ﬁlter ﬁrst smooths the signal by averaging signal values on a moving window;
then it cuts extreme values thanks to a band-pass ﬁlter. Last, it adapts to natural
variations of the signal with an adaptive-bounds mechanism.
Finally, we calibrated the ﬁlter parameters preserving the relationship
between signal strength and distance, and applied it to the experimental data.
Both, global variance and relative absolute error, experienced a signiﬁcant reduc-
tion, showing that the ﬁlter has notorious beneﬁts on distance estimation.
Future work considers experimenting in diﬀerent environments to conﬁrm
the best ﬁlter parameters, coupling the ﬁlter with positioning techniques and
personalized mapping mechanisms, and conceiving an integrated system with
interfaces for diﬀerent kinds of representations.
References
1. Moser, V., Bariˇsi´c, I., Rajle, D., Dimter, S.: Comparison of diﬀerent survey methods
data accuracy for road design and construction. In: Proceedings of the International
Conference on Road and Rail Infrastructure CETRA (2016)
2. Lu, M., Chen, W., Shen, X., Lam, H.-C., Liu, J.: Positioning and tracking construc-
tion vehicles in highly dense urban areas and building construction sites. Autom.
Constr. 16, 647–656 (2007)
3. Mainetti, L., Patrono, L., Sergi, I.: A survey on indoor positioning systems. In: 2014
22nd International Conference on Software, Telecommunications and Computer
Networks (SoftCOM), pp. 111–120. IEEE (2014)
4. An, K., Xie, S., Ouyang, Y.: Reliable sensor location for object positioning and
surveillance via trilateration. Transp. Res. Procedia. 23, 228–245 (2017)

An AbBpMa RSSI Filter for Distance Estimation
831
5. Rusli, M.E., Ali, M., Jamil, N., Din, M.M.: An Improved Indoor Positioning Algo-
rithm Based on RSSI-Trilateration Technique for Internet of Things (IOT). In: 2016
International Conference on Computer and Communication Engineering (ICCCE),
pp. 72–77. IEEE (2016)
6. Hara, S., Zhao, D., Yanagihara, K., Taketsugu, J., Fukui, K., Fukunaga, S.,
Kitayama, K.: Propagation characteristics of IEEE 802.15. 4 radio signal and their
application for location estimation. In: 2005 IEEE 61st Vehicular Technology Con-
ference, VTC 2005-Spring, pp. 97–101. IEEE (2005)
7. Piyare, R.: Internet of Things: ubiquitous home control and monitoring system
using android based smart phone. Int. J. Internet Things. 2, 5–11 (2013)
8. IEEE Standard for Information technology-Telecommunications and information
exchange between systems Local and metropolitan area networks-Speciﬁc require-
ments Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer
(PHY) Speciﬁcations. IEEE Std 80211–2012 Revis. IEEE Std 80211–2007. 1–2793
(2012)
9. Wu, R.-H., Lee, Y.-H., Tseng, H.-W., Jan, Y.-G., Chuang, M.-H.: Study of char-
acteristics of RSSI signal. In: 2008 IEEE International Conference on Industrial
Technology, ICIT 2008, pp. 1–3. IEEE (2008)
10. Bellavista, P., Corradi, A., Giannelli, C.: Evaluating ﬁltering strategies for decen-
tralized handover prediction in the wireless internet. In: 11th IEEE Symposium
on Computers and Communications, ISCC 2006, Proceedings, pp. 167–174. IEEE
(2006)
11. Heo, J., Kwon, Y.: Improved indoor positioning system using BLE beacons and
a compensated gyroscope sensor. In: Intelligent Robotics and Applications, pp.
69–76. Springer, Cham (2017)
12. Onofre, S., Caseiro, B., Piment˜ao, J.P., Sousa, P.: Using fuzzy logic to improve
BLE indoor positioning system. In: Technological Innovation for Cyber-Physical
Systems, pp. 169–177. Springer, Cham (2016)
13. Kuxdorf-Alkirata, N., Werthwein, T., Heinemann, A., Br¨uckmann, D.: A self-
calibrating bidirectional indoor localization system. In: 2017 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3276–3280
(2017)
14. Jadidi, M.G., Patel, M., Miro, J.V.: Gaussian processes online observation classi-
ﬁcation for RSSI-based low-cost indoor positioning systems. In: 2017 IEEE Inter-
national Conference on Robotics and Automation (ICRA), pp. 6269–6275 (2017)
15. Yu, Z., Guo, G.: Improvement of positioning technology based on RSSI in ZigBee
networks. Wirel. Pers. Commun. 95, 1943–1962 (2017)
16. Ayka¸c, M., Er¸celebi, E., Aldin, N.B.: ZigBee-based indoor localization system with
the personal dynamic positioning method and modiﬁed particle ﬁlter estimation.
Analog Integr. Circuits Signal Process. 92, 263–279 (2017)
17. Lin, K.H., Lu, C.C., Chen, H.M., Li, H.F., Chuang, C.F.: A modiﬁed least squares
iteration for indoor positioning system. In: 2017 IEEE International Conference on
Consumer Electronics - Taiwan (ICCE-TW), pp. 109–110 (2017)
18. Luo, Q., Yan, X., Li, J., Peng, Y., Tang, Y., Wang, J., Wang, D.: DEDF: lightweight
WSN distance estimation using RSSI data distribution-based ﬁngerprinting. Neural
Comput. Appl. 27, 1567–1575 (2016)
19. Pyda, J., Prokop, W., Rusinek, D., Ksiezopolski, B.: Secure and reliable localization
in wireless sensor network based on RSSI mapping. In: Computer Networks, pp.
55–69. Springer, Cham (2017)
20. Xue, W., Qiu, W., Hua, X., Yu, K.: Improved Wi-Fi RSSI measurement for indoor
localization. IEEE Sens. J. 17, 2224–2230 (2017)

832
D. Ord´o˜nez-Camacho and E. Cabrera-Goyes
21. Nagaraju, S., Gudino, L.J., Kadam, B.V., Ookalkar, R., Udeshi, S.: RSSI based
indoor localization with interference avoidance for Wireless Sensor Networks using
anchor node with sector antennas. In: 2016 International Conference on Wireless
Communications, Signal Processing and Networking (WiSPNET), pp. 2233–2237
(2016)
22. Ord´o˜nez-Camacho, D., Cabrera-Goyes, E.: Towards a Bluetooth Indoor Positioning
System with Android Consumer Devices. To be Presented at the International
Conference on Information Systems and Computer Sciences, INCISCOS (2017)

Health Informatics

Towards a Framework to Enable Semantic
Interoperability of Data in Heterogeneous
Health Information Systems in Namibian
Public Hospitals
Nikodemus Angula(&) and Nomusa Dlodlo
Department of Informatics, Namibia University of Science and Technology,
Windhoek, Namibia
chcangula@gmail.com, ndlodlo@nust.na
Abstract. The District Health Information System (DHIS) is an information
system that is hosted in the Khomas regional ofﬁce of the Ministry of Health and
Social Services (MoHSS) in Namibia. Parallel to the DHIS, the MoHSS runs
silo information systems in the 14 regions of Namibia which were donated by
non-governmental organisations in addition to a regional DHIS. The DHIS and
silo systems currently work in isolation from one another, hence this study is on
ﬁnding a framework to enable semantic interoperability of data in these
heterogeneous health information systems (HIS) so that the DHIS and these silo
systems in the Namibian public hospitals can act as an integrated platform to
share and exchange health- related information with each other. Thus, a protocol
called Interlink protocol is developed in this research to enable integration.
The DHIS and silo- interfaced system that will be developed in this research
should be able to link or connect all the public hospitals in Namibia to the
central database at the MoHSS for health information feed. The system would
allow public hospitals to interlink with each other through a technology inte-
grated platform. The study therefore seeks to interface DHIS and silo systems at
a data level. The aim of this research therefore is to design and develop a
framework for data semantic interoperability of DHIS and these other health
information silo systems so that they can exchange health data and information.
Keywords: Health information systems in Namibia
Current health silo systems in Namibia  Health information systems
Sematic interoperability
1
Introduction
The District Health Information System (DHIS), is an information system that is hosted
in the Khomas regional ofﬁce of the Ministry of Health and Social Services (MoHSS)
in Namibia (MOHSS 2014). Parallel to the DHIS, the MoHSS runs silo information
systems which were donated by non-government organisations. The DHIS and silo
systems work in isolation from one another, hence this study is on designing a
framework to enable semantic interoperability of data in these heterogeneous health
information systems (HIS) so that the DHIS and these silo systems in the Namibian
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_79

public hospitals can act as an integrated platform to share and exchange health- related
information with each other (MOHSS 2008). Thus although the DHIS and these silo
systems are housed in the same health sector or environment, they are unable to share
and exchange data (MOHSS, Analytical Summary - Health System Outcomes 2008).
What hinders the exchange of data between any heterogeneous systems could be
anything from middleware problems, the different data formats, the different protocols,
the lack of communication or the negotiation of interoperability, the scope of the data,
data inconsistency, inadequate resources, scalability, and a lack support systems, to
name but a few (Kadadi et al. 2014).
In each of Namibia’s fourteen (14) regions there is a DHIS deployed in each
regional ofﬁce which does not exchange or share health information directly with other
silo systems (MoHSS, Analytic Summary - Health Systems Outcomes 2008). The
standalone silo systems in the various health departments of the MoHSS also do not
directly exchange data with the main central DHIS in the Khomas regional ofﬁce. The
fact that the DHIS and silo systems at the present moment do not exchange
health-related information impacts on the delivery of health services to the public
hospitals. The same problem of accessing and exchanging health data and information
is also experienced within MoHSS headquarters as a unit. There is a lack of DHIS and
silo systems that are interfaced in public hospitals in Namibia.
The DHIS and silo systems framework to enable semantic interoperability of data
that will be developed in this research should be able to link or connect all the data
exchange in public hospitals in Namibia to the central database at the MoHSS for
health information feed. The system will allow public hospitals to interlink with each
other through a technology integrated platform. As things stand now, a user in one
public hospital struggles to access health-related information that is available in another
public hospital timely. This study has identiﬁed a gap in the Namibian public hospitals
of a lack of data interoperability which hinders the Namibian public hospitals from
sharing health information with each other, timeously and efﬁciently. If this gap were to
be closed, this will create a strong relationship between public hospitals in Namibia by
ensuring that health –related information/data available in one entity can be exchanged
with other entities across the health sector anywhere and anytime without a person
being physically present in any institution from which they intend to get data from. The
study therefore seeks to interface DHIS and silo systems at a data level. An interfaced
DHIS and its health information silo systems will serve to achieve the best outcomes in
health for Namibia as a whole.
1.1
Research Problem Statement
While interoperability represents the accurate exchange of information and the use of
information for effective decision-making, for information (data) exchange to occur, it
must be interpretable between multiple information systems (Berryman et al. 2013).
The issue as it stands, is that the silo systems that are hosted by the MoHSS are
stand-alone and they do not communicate or are not inter-linked with each other, which
makes the process ﬂow of health–related information between hospitals difﬁcult,
adoption of data interchange hence reducing service delivery and performance. Despite
the rapid increase in the adoption of integrated HIS worldwide generally, the real
836
N. Angula and N. Dlodlo

challenge facing health data providers is that many of the HIS are unable to interface
with each other, let alone exchange patient information in an efﬁcient way (Challenges
of establishing EHR interoperability). The Namibian health is not spared from this
challenge. The problem therefore that this research will focus on is data integration in
this heterogeneous Namibian health environment consisting of silo systems and the
DHIS. The problem of technical variations in the different technology architectures,
service models and capabilities makes it difﬁcult for one to create one standard format
for the sharing of data (Challenges of establishing EHR interoperability). The problem
also is that the huge number of silo systems hosted in various hospitals in Namibia are
expensive and difﬁcult to maintain due to the dispersed nature of health institutions
operating in different regions in Namibia which makes it impossible for these silo
systems to communicate with each other.
1.2
Preliminary Literature Review
The next section is the literature review.
1.3
Health Information Systems
The wide spread use of information and communication technologies (ICTs) has
permeated almost all aspects of life including the healthcare sector. The intersection
between healthcare business processes and information systems to deliver better ser-
vices is popularly known as health information systems (HIS) (Anshari 2011; Tossy
2014). According to White (2015), HIS is the organisation of people, institutions, and
resources to deliver healthcare services to meet the health needs of target populations
(White 2015). Healthcare systems encompass all organisations, people and factions
whose primary intent is to promote, restore or maintain health to ensure that health
information is made available to the general public. The use of HIS is important
because they help societies to access healthcare information. The World Health
Organisation (WHO 2008) report articulates that health systems are deﬁned as com-
prising of all the organisations, institutions and resources that are devoted to producing
health-related activities. A health activity is deﬁned as any effort, whether in personal
healthcare, public health services or through intersectional initiatives, whose primary
purpose is to improve health. Furthermore, the use of HIS can allow health service
providers to promote, restore or maintain health through healthcare systems
technologies.
1.4
Health Information Systems in Namibia
Namibia’s National Health Information System (NHIS) falls under the Primary Care
Directorate in the Ministry of Health and Social Services (MoHSS 2014). It is charged
with the responsibility of providing a comprehensive source of data on a large number of
health-related indicators. The NHIS was designed to improve service delivery in terms
of quality and effectiveness of strategies and to monitor the trends in disease occurrence.
In addition, it also provides information for national policy makers, socio-economic and
Towards a Framework to Enable Semantic Interoperability of Data
837

health personnel, as well as the public at large (Haoses-Gorases 2015). The organisa-
tional structure of the NHIS system is fragmented across different directorates and
institutions. The challenges to the system are that there is a shortage of human resources
to coordinate, analyse, and report on the information in a comprehensive and timely
fashion. In the Namibian health environment, a large number of systems, databases, and
processes are fully manual, paper-based, or only partially electronic, and to a large
extent formats are either fragmented or nonstandard. This adds signiﬁcantly to work
burdens and seriously undermines efﬁciency. The Namibian HIS as a routine health and
management information system was ﬁrst introduced in Namibia in 1992 (Chotard
1992). The Namibian HIS was tasked with the responsibility of collecting routine data
from all health facilities, that is, 36 hospitals, 281 clinics, 33 health centres and a number
of outreach points The raw data is collated, analysed and processed into information that
can be used for programme planning, implementation, evaluation and can also inﬂuence
changes in policies, strategies, programmes and resource allocation. This system of
collecting information is usually passive.
1.5
Semantic Interoperability
Semantic interoperability means the ability of multi health information systems to work
together within and across organisation boundaries in order to advance the effective
delivery of healthcare for individuals and communities (HIMSS 2005). HIMSS (2005)
further argued that semantic interoperability is deﬁned as the highest level of inter-
operability in which two or more systems can exchange information and at the same
time the exchanged information can be used for decision making purposes. Further-
more, semantic interoperability is beyond the ability of two or more computer systems
to exchange information. Semantic interoperability is the ability to automatically
interpret the information exchanged meaningfully and accurately in order to produce
useful results as deﬁned by the end users both systems (The EN 1306 Association
2011). To achieve semantic interoperability, both sides must defer to a common
information exchange reference. The content of the information exchange requests are
unambiguously deﬁned: what is sent is the same as what is understood. In this case the
JavaScript object notation will act as a data structure format of different heterogonous
health information systems as it is a standardised format for describing objects in a
browser. In addition, JavaScript object notation is light weight because it describes
itself. JavaScript object notation is a data structure made up of a property and value.
Types of
interoperability
Description of each interoperability
Foundational
interoperability
Basic level of interoperability
Data from one information technology system can be received by
another
The receiving system does not need to be able to interpret it
Structural
interoperability
Intermediate level of interoperability
Data exchanges between information technology systems can be
interpreted at the data ﬁeld level
(continued)
838
N. Angula and N. Dlodlo

(continued)
Types of
interoperability
Description of each interoperability
Clinical or operational purpose and meaning of the data is preserved
Semantic
interoperability
Highest level of interoperability
Two or more systems can exchange information
Exchanged information can be used
Electronic exchange of patient summary information among
caregivers and other authorized parties via potentially disparate
electronic health record (EHR) systems
1.6
Current Health Silo Systems in Namibia
The MOHSS is committed to improvements in all these areas and it is currently
working on several reforms such as: restructuring the MoHSS; systems integration;
improved
NHA;
and
implementing
the
health
extension
workers
strategy.
Hence MOHSS (2008) has many stand-alone information systems managed by dif-
ferent divisions in different directorates and running on different software. These
systems include the health information system (HIS) which is in primary health care
services directorate; social welfare information system (SWIS) which is in the devel-
opmental social welfare directorate; management information and research system
which is in the policy planning and human resource development directorate; and
monitoring and evaluation specialisation in the programmes directorate. This frag-
mented structure has created overlaps and duplication between the various systems and
disparities as resource-strong programmes are able to “push” their own information
system agenda. The improvement of the information management system is a high
priority for the MoHSS.
1.7
Integrated Health Care Information System (IHCIMS)
Effective management of information plays a signiﬁcant role in the delivery of health
care services for health care providers. Information Technology (IT) is changing the
face of health care dramatically all over the world and IT is moving merely to
automating current processes to becoming the center piece of health care integration
(Administrator Chief System 2016). According to the Chief System Administrator
(2016), the Ministry of Health and Social Services (MOHSS) of the Republic of
Namibia thus put in place a comprehensive modern Integrated Health Care Information
management System (IHCIMS) to improve the quality and effectiveness of health care
for all Namibians. Integrated healthcare systems would not achieve their goals without
systems designed to support the operation of a continuum of services (Miller 1995).
Miller (1995), further stated that integrated Health Care Information management
System is a patient record management system, with the goal of enhancing health care
by lessening the workload of health workers to a paperless working environment.
Towards a Framework to Enable Semantic Interoperability of Data
839

It ensures that complete and accurate patient records is captured and shared across
health institutions, so that health workers can make informed and responsible decision.
Chief system Administrator (2016), further stated that Integrated Health Care Infor-
mation Management System (IHCIMS) is a web based, web enabled application that is
used by the Ministry of Health and Social Services (MOHSS) for its healthcare
management. Integrated Healthcare Information Management System is a compre-
hensive enterprise-wide software, which covers all aspects of hospital management and
day-to-day operations of a hospital.
2
Methodology
2.1
Research Design
The research was in two phases. The ﬁrst phase was the collection of data on the status
of integration of HIS in the Namibian healthcare sector. The study used a qualitative
approach. The second phase was the design and development of the semantic inter-
operability framework. The study adopted a case study setting looking at 8 hospitals in
Namibia. The case studies selected in this case was the Khomas regional ofﬁce
(KRO) and public hospitals. Therefore, the study used the grounded theory to guide in
data collection and Design Science Research Theory was used to develop the artefact.
Expert review was used to review and validate the artefact to be developed.
The aim of this research was to design and develop a framework for data semantic
interoperability of DHIS and these other health information silo systems so that they
can exchange health data and information.
The main research question was “How can semantic interoperability of data in
heterogeneous health information systems in Namibian hospitals be achieved”?
i. How is healthcare information currently shared between existing DHIS and silo
systems in Namibia?
ii. How can a data interlink protocol that would govern heterogeneous health
information system in Namibian public hospitals be developed?
iii. How can information relating to healthcare be interfaced between existing DHIS
and silo systems in Namibia by adopting technologies in the distribution of health
information?
iv. How can a model to enable semantic interoperability of data in heterogeneous
health information systems in Namibian hospitals be designed and developed?
3
Results/Findings
According to the two IT Technician at the local hospital in Windhoek they clearly
stated that they do not have a management system for advanced staff. In addition, they
further stated that they do not have a system or framework that aggregates data from
remote health systems for management staff to view and analyse information and also
they do not have a system that exchange data automatically. The system or framework
840
N. Angula and N. Dlodlo

developed in this study would aggregates data from remote health systems for man-
agement staff to view and analyse information for hospitals decision making. The two
technicians further, stated that they do not have a system or framework for semantic
interoperability of data in Namibian public hospital. Currently, what system adminis-
trators does at the present moment they can only login remotely and access a ﬁle in
another computer on the same network which do not allow silo systems data to be
accessed, communicate, and exchange health data and information from other silo
systems hosted in other Namibian public hospitals. The systems proposed would
exchange data automatically from heterogeneous health information silo systems in
Namibian public hospitals. The systems exchange data automatically so there should be
no staff entering data manually. Furthermore, the Technician indicated that public
hospitals have standalone silo systems, no communication between silo systems at the
moment, at the present moment users goes physically to the silo systems wherever they
are installed in public hospitals, all users are role based which means every user on a
silo system is responsible for the silo they use, there is no method used to access health
data from another silo systems at the moment, there is no method of extracting health
data from another silo systems operating in the same environment, there is no protocol
or layer used to govern silo systems in public hospitals at the moment, therefore the
study proposed to develop a model to enable semantic interoperability of data in
heterogeneous health information systems in Namibian public hospitals for manage-
ment committees that involves all the head of departments in all public hospitals in
Namibia.
The table below is symbolising the responses from the two technicians interviewed
in the local hospital in Windhoek who clearly stated that at the present moment hos-
pitals in Namibia has standalone silo systems that works in isolation and they do not
communicate and exchange health related information to one another. The two tech-
nicians highlighted that this is a main challenge in the Namibian health sector simply
because if a technician requires data from a remote silo systems in another hospital they
are required to go physically to that speciﬁc hospitals in order to acquire health related
data. As result the study developed a protocol for the exchange of data from remote
systems and aggregation of that data into meaningful information (Table 1).
Table 1. Technicians interviews
Technicians interviews
Technician
1
No exchanging of
data in the current
system
Technician move from
one hospital if they need
data
Technician
1
Stand
alone
systems
No exchanging of
data in the current
systems
No exchanging of data
Type of
job
Number of
technicians
Silo
systems
Exchanging of health
data
The current process of
exchanging data in
hospital
Towards a Framework to Enable Semantic Interoperability of Data
841

In today’s world protocol are known as the most essential components in semantic
interoperability of multiple information systems for the fact that they act as agreement
on how something has to be done. In addition, a protocol in generally is the special set
of rules that end points in a telecommunication connection use when they communi-
cate. Moreover, protocol specify interactions between the communication entities in
this case the protocol was applied to govern heterogeneous systems in Namibian public
hospitals for data semantic interoperability of DHIS and these other health information
silo systems so that they can exchange health data and information. The study has
discovered a new interlink protocol as a layer between health remote systems to govern
the communication, exchange of data and information among heterogeneous health silo
systems in Namibian Public hospitals.
3.1
The Application of Interlink Protocol to Silo Systems in Namibia
• The remote systems will organise their data in a certain format (that is a protocol)
• That format is a JSON object
That JSON ﬁle created by the remote system is then uploaded into a database.
A PHP (Hypertext Preprocessor) ﬁle will get all the data from JSON ﬁle and insert into
tables. Hence, it is JSON structure that holds the data and that data is inserted into
database tables. The protocol function in converting different data structures to enable
aggregation of data is technically deﬁned in the source codes and this is the protocol
agreed upon between DHIS system and remote systems called interlink protocol.
The study has identiﬁed/discovered a new protocol called interlink protocol in the
Namibian health sector that would govern heterogeneous health information systems to
communicate, aggregate any type of data in comparison with the existing protocols
available in the Namibian health sectors. The other protocols that exist do not aggregate
data and they do not enable semantic interoperability of data between silo systems to
take place hence the new protocol invented can allow multiple systems to share health
data with each other. The functionality and deﬁnition of existing protocols are as
follows:
• IP
The protocol that deﬁnes the unreliable, connectionless, delivery mechanism is
called the internet protocol (IP) (Krishnan 2005). IP provides three basic deﬁnitions:
• The IP protocol deﬁnes the basic unit of data transfer for internet software.
• It provides the routing function, i.e., choosing a path over which data is to be sent.
• IP includes a set of rules that embody the rules of unreliable packet delivery. The
rules characterise how hosts and routers should process packets, how and when
error messages should be generated, and the conditions under which packets can be
discarded. These form part of the internet control message (ICMP) protocol suite.
842
N. Angula and N. Dlodlo

3.2
The DHIS Demonstrator
The following section will describe the Interlink protocol and the user interfaces to the
technology demonstrator developed based on a data interface speciﬁcation or system.
3.3
The Demonstrator
The ﬁrst interface to the system is the login interface for the DHIS. To log onto the
system, a user should be added to the system by the administrator so that they can user
their username and password (see Fig. 1)
When a user login to the system they can view a dashboard with all the information
uploaded to the system in a format of charts and presentation of the information in
percentages (see Fig. 2). On the system dashboard, the user can view sources by region
representation of information and various disease can be viewed on the Namibian map
and also disease occurrence by region.
The map is for the user to navigate and check from which geographical location the
patient is from. It checks the type of disease they are suffering from and relates it to
geographic area.
Fig. 1. Log in interface
Fig. 2. Distribution of disease
Towards a Framework to Enable Semantic Interoperability of Data
843

4
Conclusion
Data interoperability has been the main challenges in health sector across the globe.
One of the most challenging problems in the healthcare domain today is providing
interoperability among health care information systems. In order to tackle this problem,
semantic interoperability data model will be designed and developed among public
hospitals and clinics in Namibia. The use of information and communication tech-
nologies (ICT) by healthcare service providers is often driven by the need to achieve
effective and efﬁcient services delivery to other public hospitals in Namibia. Public
hospitals in Namibia have been challenged in the way they carry out their operations,
processes and how health related information is distributed and accessed by other
public hospitals in all the 14 regions in Namibia. The challenge include distribution and
information ﬂow between public health institutions and other public hospitals operating
in the regions. Unfortunately, these challenges and gaps hampers and negatively impact
healthcare service delivery between public hospitals operating within the same health
sector environment. This study will design and develop a data interface for health
information sharing between different public hospitals in Namibia.
References
Administrator Chief System: Integrated health care information systems, Windhoek Central
Hospital, Windhoek, Namibia (2016)
Anshari, M.N.: Health Information Systems (HIS): Concept and Technology (2011)
The EN 1306 Association: Semantic interoperability of health information (2011). http://www.
en13606.org/the-ceniso-en13606-standard/semanticinteroperability
Berryman, R., Yost, N., Dunn, N., Edwards, C.: Data interoperability and information security in
healthcare. In: Transactions of the International Conference on Health information technology
Advancement, pp. 84–93. Western Michigan University, Michigan (2013)
Chotard, S.J.: Nutrition and Health Information and Surveillance Systems (1992). https://www.
scribd.com/doc/969821/Nutrition-Information-inNamibia-Situation-Analysis-the-Way-Forward
Haoses-Gorases, L.: Utilisation of Health Information System (HIS) in Namibia: Challenges and
Opportunities Faced by the Health Care Delivery System/Health Sector. Strategies for
permanent access to scientiﬁc information in South Africa: focus on environmental
information (2015)
HIMSS: Interoperability Deﬁnition and Background (2005). http://www.himss.org/sites/himssorg/
ﬁles/HIMSSorg/Content/ﬁles/AUXILIOHIMSSInteroperabilityDeﬁned
Kadadi, A., Agrawal, R., Nyamful, C., Atiq, R.: Challenges of data integration and interoperability
in big data. In: IEEE International Conference on Big Data 2014, Washington DC, US (2014)
Krishnan, K.: SFWR 4C03: Computer Networks and Computer Security (2005). https://www.
coursera.org/browse/computer-science/computer-security-and-networks?languages=en
Miller, J.: Integrated Healthcare Information Systems (1995). https://www.ncbi.nlm.nih.gov/
pubmed/10154233
MOHSS: Analytical Summary - Health System Outcomes (2008). http://www.aho.afro.who.int/
proﬁles_information/index.php/Ethiopia:Analytical_summary_-_Health_system_outcomes
MoHSS: National Public Health Laboratory Policy (establishing a strong public health laboratory
system), Windhoek, Namibia (2008)
844
N. Angula and N. Dlodlo

MoHSS: Public Private Partnerships Framework: Discussion Paper (2014). http://www.mhss.
gov.na/ﬁles/downloads/c68_PPPF_A5_Booklet_Correction_REPRO2.pdf. Accessed 31 Mar
2014
Tossy, T.: Major Challenges and Constraint of Integrating Health Information Systems (2014).
https://www.researchgate.net/publication/272163842_Major_Challenges_and_Constraint_of_
Integrating_Health_Information_Systems_in_African_Countries_A_Namibian_Experience
White, F.: Primary Health Care and Public Health: Foundations of Universal Health Systems.
Medical Principles and Practices (2015). https://en.wikipedia.org/wiki/Health_system
WHO: Developing Health Management Information Systems: A Practical Guide for Developing
Countries
(2004/2008).
http://www.wpro.who.int/health_services/documents/developing_
health_management_information_systems.pdf
Towards a Framework to Enable Semantic Interoperability of Data
845

Automatic Extraction and Aggregation
of Diseases from Clinical Notes
Ruth Reátegui1,2(&) and Sylvie Ratté1
1 École de technologie supérieure, Montreal, Canada
Sylvie.Ratte@etsmtl.ca
2 Universidad Técnica Particular de Loja, Loja, Ecuador
rmreategui@utpl.edu.ec
Abstract. Clinical notes provide medical information about the patient’s health.
The automatic extraction of this information is relevant in order to analyze pat-
terns for grouping patients with similar characteristics. In this paper, we used
MetaMap to extract diseases present in 412 discharge summaries of obesity
patients. The UMLS intra-source vocabulary relationships were used to make
automatic aggregation of diseases. The results showed an average of 0.81 for
recall, 0.92 for precision, and 0.84 for F-score. Finally, with the diseases extracted
and aggregated three sub-graphs were identiﬁed; they correspond to patients with
sleep apnea, those with heart diseases, and those with communicable diseases.
Keywords: Clinical notes  UMLS  MetaMap  Clustering graphs
1
Introduction
Clinical notes such as discharge summaries provide useful information about the
patient’s health. Typically, these notes contain medical terminology such as disease,
treatment, and drugs, that are relevant for patients, physicians and clinical researchers
[1]. Natural Language Processing (NLP) tools like MetaMap are widely used to extract
medical terminology from these clinical notes [1, 2]. Also, MetaMap works with the
Uniﬁed Medical Language System (UMLS) that standardizes and codiﬁes medical
terminology.
In previous research, the network-based approach has been applied to extract
information from clinical notes in order to analyze and visualize associations among
medical terms [3]. Some researchers have been using this approach for many tasks. For
example, Gangopadhyay et al. created sub-graphs to represent patterns within clinical
notes related with anemia [4]. Lyalina et al. proposed a visualization network to show
associations between concepts related with three neuropsychiatric disorders [5]. Roque
et al. used a graph representation to visualize 26 patients’ cohorts constructed with
ICD10 codes obtained from a dataset of psychiatric patients [2]. Finally, Bauer-Mehren
et al. constructed patients cohorts and made an outcome analysis based on unstructured
clinical notes related to peripheral artery diseases [3].
Considering the importance of extracting information from clinical notes and the
use of network-based approach to analyze and visualize the information, two main
goals were deﬁned in this work: ﬁrst, to use MetaMap to extract diseases from
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_80

discharge summaries and to propose a method to aggregate them, and second, to
identify sub-graphs of patients with similar diseases by applying a modularity function.
In our previous work [6], 15 diseases related to obesity were extracted with
MetaMap. In this paper, we extend our analysis to include all possible diseases present
in the discharge summaries. Our aim is to discover news insights about the dataset
used. For the purpose of this research and to allow comparison with previous works, we
are maintaining the same number (412) of patients.
2
Methodology
2.1
Dataset
The i2b2 (Informatics for Integrating Biology to the Bedside) Obesity dataset was used;
this dataset consists of 1237 discharge summaries of overweight and diabetic patients
[7]. In 2008, the i2b2 organized the Obesity Challenge task, that event consisted in
automatically extracting information about obesity and some obesity co-morbidities:
asthma, atherosclerotic cardiovascular diseases, congestive heart failure, depression,
diabetes mellitus, gallstones/cholecystectomy, gastroesophageal reﬂux disease, gout,
hypercholesterolemia, hypertension, hypertriglyceridemia, obstructive sleep apnea,
osteoarthritis, peripheral vascular disease, and venous insufﬁciency.
This dataset contains expert annotations that classiﬁes obesity and ﬁfteen
co-morbidities as Present, Absent, Questionable, or Unmentioned.
Out of 1237 summaries, 412 documents of patients were selected using the
methodology exposed in our previous work [6].
2.2
UMLS
The National Library of Medicine Uniﬁed Medical Language System (UMLS) pro-
vides terminology, coding standards, and resources for biomedical and electronic
health systems. UMLS contains a Metathesaurus that is organized by concepts or
meanings. A concept has a unique and permanent identiﬁer (CUI) and a preferred
name. UMLS also includes a Semantic Network that provides: (1) a categorization
(semantic type) of all concepts; (2) a set of relationships (semantic relations) between
concepts [8].
UMLS is based on many vocabulary sources; one of them is the Systematized
Nomenclature of Medicine—Clinical Terms (SNOMED CT). In this paper, we are
using SNOMED CT since this health lexicon is typically used for the electronic
exchange of clinical health information [9].
2.3
Automatic Extraction of UMLS Codes
MetaMap (version 2016AB) was used to extract concepts from the 412 summaries.
This tool, used in conjunction with UMLS [10], allowed us to identify both the CUIs
and the preferred name that correspond to two semantic types: (1) Disease or Syn-
drome, and (2) Mental or Behavioral Dysfunction.
Automatic Extraction and Aggregation of Diseases
847

More speciﬁcally, we extracted obesity comorbidities mentioned before, and other
diseases that can appear into the summaries. To ease the exposition, we will refer to
both as “diseases”.
All the diseases were treated as dichotomy variables or features (values 0 or 1,
respectively depicting the non-existence or existence of the disease in each discharge
summary). The NegEx [11] algorithm, implemented in MetaMap, was also used to
exclude diseases mentioned in a negative form (e.g. “the patient is not depressed”).
2.4
Automatic Aggregation of UMLS Codes
Using MetaMap, we extracted the diseases in the summaries, retaining those that are
present in at least 4 patients. To reduce again the number of diseases, we carried out an
automatic aggregation of diseases, taking into account the UMLS relationships between
concepts in SNOMED CT (intra-source relationships) using the information contained
in the MRREL.RR ﬁle.
The MRREL.RR ﬁle in SNOMED CT contains different types of intra-source
relationships [8], two of them are: (1) the hierarchical relationship “is_a” to identify
parents, children, and siblings; (2) the “same_as” relationship to identify similar
concepts.
From MRREL.RR, we retained the “is_a” (only child) and “same_as” relationships
that correspond to the vocabulary of SNOMED CT. After that, three consecutive steps
were followed to aggregate the diseases:
(1) First Step: The “same_as” relationship was applied to identify and aggregate
similar diseases.
(2) Second Step: After the ﬁrst step, the “is_a” relationship was applied to identify
diseases that are treated as a child of other (father) disease.
(3) Third Step: Finally, the “is_a” relationship was reapplied to reduce further the
number of diseases obtained in the second step.
2.5
Graph Representation
We then used the tool Gephi1 to obtain a graph representation of patients. The algo-
rithm of Blonde et al., implemented in Gephi, was used to calculate modularity par-
titions; it is particularly useful for large networks [12]. This algorithm was applied over
an adjacency matrix consisting of 412 nodes, with the aim to identify sub-graphs of
patients that share similar diseases. This matrix is non-directional. Each node represents
a patient, and the edge between two different nodes represents the number of diseases
that both nodes have in common.
1 https://gephi.org/.
848
R. Reátegui and S. Ratté

3
Results and Discussion
3.1
Automatic Aggregation
With these aggregations, we reduced the number of diseases from 249 to 180. In each
step, the following results were obtained:
First step: With the “same_as” relationship, 249 diseases were reduced to 247.
Despite the small amount of reduction, this step permitted us to aggregate two sig-
niﬁcant diseases. The “myocardial ischemia” (CUI: C0151744) was aggregated to
“coronary arteriosclerosis” (CUI: C0010054), and the “chronic renal impairment”
(CUI: C0403447) was aggregated to “chronic kidney disease” (CUI: C1561643).
Second step: With the “is_a” relationship, more speciﬁcally the child relation, the
247 diseases were further reduced to 186. Consequently, 61 children diseases were
aggregated to their father diseases. For example, “diabetes mellitus non-insulin
dependent” (CUI: C0011860) and “diabetes mellitus insulin dependent” (CUI:
C0011854) were aggregated to diabetes mellitus (CUI: C0011849). Some diseases
aggregated in this second step are shown in Table 1.
Table 1. Children diseases aggregated in the second step.
Father diseases
Children diseases
Hypertensive disease (C0020538)
Hypertensive urgency (C0745138)
Diabetes mellitus (C0011849)
Diabetes mellitus insulin dependent (C0011854)
Diabetes mellitus non insulin dependent (C0011860)
Hyperlipidemia (C0020473)
Hypertriglyceridemia (C0020557)
Hypercholesterolemia (C0020443)
Sleep apnea syndrome (C0037315)
Obstructive sleep apnea (C0520679)
Coronary artery disease
(C1956346)
Coronary arteriosclerosis (C0010054)
Morbid obesity (C0028756)
Obesity hypoventilation syndrome (C0031880)
Communicable disease
(C0009450)
Infections, Hospital (C0205721)
Viral illness/Virus diseases (C0042769)
Abscess (C0000833)
Deep vein thrombosis (C0149871)
Deep vein thrombosis of lower limb (C0340708)
Deep thrombophlebitis (C0151950)
Chronic kidney disease
(C1561643)
Chronic renal failure/kidney failure chronic
(C0022661)
Chronic kidney disease stage 5 (C2316810)
Atrial ﬁbrillation (C0004238)
Chronic atrial ﬁbrillation (C0694539)
Paroxysmal atrial ﬁbrillation (C0235480)
Depressive disorder (C0011581)
Depression (C0011570)
Diseases CUI code appears in parentheses.
Automatic Extraction and Aggregation of Diseases
849

Third step: Considering that the second step identiﬁed the immediate children in a
hierarchical relationship, we decided to apply again the “is_a” relationship over the
resulting 186 diseases of step 2. In this step, 6 disease were eliminated to obtain the
ﬁnal number of 180 disease. As an example, in this step, “diabetes mellitus” (CUI:
C0011849) was aggregated to “Hyperglycemia” (CUI: C0020456). See Table 2 for
more details.
With the resulting 180 diseases, we constructed a patient-disease matrix where each
row represents a patient, and each column, a disease (or CUI). The content of each cell
indicates the presence or absence of the disease in the discharge summary for each patient.
3.2
Evaluation of the Automatic Extraction and Aggregation
As mentioned in Sect. 2.1, the dataset contains experts’ annotations covering obesity,
and ﬁfteen diseases that can be used as a gold standard. Following the automatic
extraction and aggregation explained in Sect. 3.1, we obtained 180 diseases, some of
them corresponding to the annotated diseases. To evaluate the results of our extraction
and aggregation mechanisms, we calculated the recall (or sensitivity), the precision,
and the F-score. From the 180 diseases, thirteen diseases were evaluated. It is important
to note that with the automatic aggregation some diseases formed groups. As an
example, the annotated diseases “hypercholesterolemia” and “hypertriglyceridemia”
are presented under “hyperlipidemia” by our automatic aggregation process. The same
mechanism resulted in the grouping of “peripheral vascular” disease and “venous
insufﬁciency” under “vascular disease”. Table 3, columns 1 and 2 show the diseases
and the number of patients suffering each disease according to experts. In the same
table, columns 3 and 4 show the results of the automatic extraction and aggregation
processes.
Gallstones had the lowest evaluations since we chose, in this work, not to extract
medical procedures. Therefore, “cholecystectomy” do not appear in the list of informa-
tion automatically extracted. Despite this, the results showed an average of 0.81 for recall,
0.92 for precision, and 0.84 for F-score. Only the “Present” annotation was considered for
identifying whether the patient has the diseases or not.
Table 2. Children diseases aggregated in the third step.
Father diseases
Children diseases
Vascular disease (C0042373) Peripheral vascular diseases (C0085096)
Heart diseases (C0018799)
Coronary artery disease (C1956346)
Lung diseases (C0024115)
Pulmonary edema (C0034063)
Cardiomyopathy (C0878544) Myocardial infarction (C0027051)
Hyperglycemia (C0020456)
Diabetes mellitus (C0011849)
Fibrillation (C0232197)
Atrial ﬁbrillation (C0004238)
Diseases CUI code appears in parentheses.
850
R. Reátegui and S. Ratté

3.3
Graph Representation
With the modularity function, three sub-graphs were obtained. Figure 1 shows the
distribution of these sub-graphs, which represent groups of patients with similar
diseases.
In order to present more details about the diseases present in each group, the 30
prevalent diseases were selected to be analyzed. Table 4 presents the number and
percentages of patients, in each sub-graph, suffering from these 30 diseases.
Table 3. Evaluation of the automatic extraction and aggregation processes.
Experts’ annotations
Automatic extraction and
aggregation
Evaluation
Diseases
NP
Diseases
NP
R
P
F
Hypertension
325
Hypertensive disease
(C0020538)
323
0.97
0.97
0.97
Diabetes
259
Hyperglycemia
(C0020456)
250
0.91
0.94
0.93
Atherosclerotic
cardiovascular disease
181
Heart diseases
(C0018799)
129
0.63
0.88
0.74
CHF
172
Congestive heart
failure (C0018802)
158
0.87
0.95
0.91
Hypercholesterolemia and
Hypertriglyceridemia
177
Hyperlipidemia
(C0020473)
155
0.86
0.98
0.92
Obstructive sleep apnea
127
Sleep apnea syndrome
(C0037315)
138
0.99
0.91
0.95
Osteoarthritis
87
Degenerative
polyarthritis
(C0029408)
70
0.77
0.96
0.85
Depression
83
Depressive disorder
(C0011581)
74
0.86
0.96
0.90
Asthma
81
Asthma (C0004096)
79
0.91
0.94
0.93
Gastroesophageal reﬂux
disease
76
Gastroesophageal
reﬂux disease
(C0017168)
78
0.93
0.91
0.92
Gallstones
74
Cholelithiasis
(C0008350)
13
0.16
0.92
0.28
Gout
56
Gout (C0018099)
58
0.98
0.95
0.96
Peripheral vascular disease
and Venous insufﬁciency
55
Vascular disease
(C0042373)
57
0.73
0.70
0.71
Evaluation results average:
0.81
0.92
0.84
NP: number of patients, R: recall; P: precision; F: F-score
Some rows in column 1 show some diseases annotated by experts; after the automatic
aggregation process, they formed part of a father disease shown in column 3.
Automatic Extraction and Aggregation of Diseases
851

The description of the diseases in each group are as follow:
– SG1 has 165 patients. 80% of the patients in this group have hypertension, 73%
have sleep apnea syndrome, 46% have morbid obesity, 41% have hyperglycemia,
36% have congestive heart failure, 34% have asthma, 30% have deep vein
thrombosis and gastroesophageal reﬂux. Other diseases are present in less than 30%
of the population in this group.
– SG2 has 154 patients. 81% of the patients in this group have hypertension, 73%
have heart disease, 65% have hyperglycemia, 56% have hyperlipidemia, 45% have
cardiomyopathy, and 44% have congestive heart failure. Other diseases are present
in less than 30% of the population in this group.
– SG3 has 93 patients. 88% of the patients in this group have hyperglycemia, 72%
have hypertension, 52% have communicable disease, 33% have congestive heart
disease and chronic kidney disease, 32% have hyperlipidemia, and 31% have uri-
nary tract infection. Other diseases are present in less than 30% of the population in
this group.
As we can see, hypertension and hyperglycemia are the diseases present in more
than 60% of the population. Also, all comorbidities (except Gallstones) mentioned in
the annotations made by physicians are part of the 30 prevalent diseases.
Fig. 1. Sub-graph of patients. Sub-graph 1 (SG1, in purple) contains 165 nodes and 12466
edges. Sub-graph 2 (SG2, in orange) contains 154 nodes and 11295 edges. Sub-graph 3 (SG3, in
green) contains 93 nodes and 4178 edges.
852
R. Reátegui and S. Ratté

SG1 has the highest percentage of patients with sleep apnea syndrome and morbid
obesity. SG2 has the highest percentage of patients with heart problems (heart disease,
cardiomyopathy, congestive heart failure). Also, this group has a high percentage of
patients with hyperglycemia. SG3 has the highest percentage of patients with
Table 4. Details of the top 30 prevalent diseases in each sub-graph.
Num Diseases extracted automatically
Sub-graphs
Diseases
N P %
SG1 SG2 SG3
1
Hypertensive disease (C0020538)
323 78
80
81
72
2
Hyperglycemia (C0020456)
250 61
41
65
88
3
Congestive heart failure (C0018802)
158 38
36
44
33
4
Hyperlipidemia (C0020473)
155 38
24
56
32
5
Sleep apnea syndrome (C0037315)
138 34
73
5
12
6
Heart diseases (C0018799)
129 31
7
73
5
7
Morbid obesity (C0028756)
102 25
46
8
14
8
Communicable disease (C0009450)
95 23
15
15
52
9
Cardiomyopathy (C0878544)
88 21
6
45
10
10
Deep vein thrombosis (C0149871)
80 19
30
12
13
11
Fibrillation (C0232197)
78 19
13
26
17
12
Asthma (C0004096)
79 19
34
8
12
13
Chronic kidney disease (C1561643)
78 19
8
21
33
14
Gastroesophageal reﬂux disease (C0017168)
78 19
30
12
12
15
Depressive disorder (C0011581)
74 18
27
11
14
16
Left ventricular hypertrophy (C0149721)
73 18
15
22
16
17
Anemia (C0002871)
70 17
16
15
23
18
Degenerative polyarthritis (C0029408)
70 17
25
13
9
19
Erythema (C0041834)
70 17
15
12
28
20
Endometriosis (C0014175)
68 17
18
16
15
21
Chronic obstructive airway disease (C0024117)
68 17
22
11
15
22
Lung diseases (C0024115)
59 14
18
13
10
23
Urinary tract infection (C0042029)
67 16
13
11
31
24
Vascular disease (C0042373)
57 14
7
24
9
25
Gout (C0018099)
58 14
18
6
20
26
Kidney diseases (C0022658)
53 13
5
14
26
27
Renal insufﬁciency/Kidney failure (C0035078)
49 12
10
9
20
28
Anxiety (C0003467)
47 11
13
7
15
29
Pneumonia (C0032285)
45 11
7
12
15
30
Cerebrovascular accident (C0038454)
44 11
5
18
8
Patients per sub-graph
165
154
93
The results are represented in percentages according to the number of patients in each
sub-graph.
Bold numbers are the disease present in more than 30% of the population in each
sub-graph.
Automatic Extraction and Aggregation of Diseases
853

hyperglycemia. Also, in this group, the patients present communicable disease, chronic
kidney disease and urinary tract infection.
4
Conclusion and Future Works
Similar to previous works [6], this research conﬁrm that MetaMap is a good strategy to
replace manual extraction. Using MetaMap, we showed that it is possible to auto-
matically extract diseases present in 412 discharge summaries of obesity patients. The
extraction gives as a result a list of diseases, some of them (15 diseases) mentioned by
Uzuner [7]. The UMLS intra-source vocabulary relationships (“is_a”, “same_as”)
allowed us to automatically aggregate the diseases and reduce them to 180.
Using these 180 disease, we identiﬁed 3 groups of patients that could be charac-
terized such as: SG1, morbid patients with sleep apnea syndrome; SG2, obese patients
with heart disease; and SG3, obese patients with hyperglycemia and communicable
diseases.
For future works, other medical information present in clinical notes could be
considered such as treatments and drugs. Also, the point of view of physician could be
necessary to analyze the relation between the diseases in each sub-graph.
References
1. Pradhan, S., Elhadad, N., South, B.R., Martinez, D., Christensen, L., Vogel, A., Suominen,
H., Chapman, W.W., Savova, G.: Evaluating the state of the art in disorder recognition and
normalization of the clinical narrative. J. Am. Med. Inform. Assoc. JAMIA 22, 143–154
(2015)
2. Roque, F.S., Jensen, P.B., Schmock, H., Dalgaard, M., Andreatta, M., Hansen, T., Brunak,
S.: Using electronic patient records to discover disease correlations and stratify patient
cohorts. PLoS Comput. Biol. 7(8), e1002141 (2011)
3. Bauer-Mehren, A., LePendu, P., Iyer, S.V., Harpaz, R., Leeper, N.J., Shah, N.H.: Network
analysis of unstructured EHR data for clinical research. AMIA Summits on Translational
Science Proceedings 2013, pp. 14–18 (2013)
4. Gangopadhyay, A., Yesha, R., Siegel, E.: Knowledge discovery in clinical data. In:
Holzinger, A. (ed.) Machine Learning for Health Informatics: State-of-the-Art and Future
Challenges, pp. 337–356. Springer, Cham (2016)
5. Lyalina, S., Percha, B., LePendu, P., Iyer, S.V., Altman, R.B., Shah, N.H.: Identifying
phenotypic signatures of neuropsychiatric disorders from electronic medical records. JAMIA
20, e297–e305 (2013)
6. Reátegui, R., Ratté, S.: Comparison of MetaMap and cTAKES for entity extraction in
clinical notes: some remarks about aggregation and semantic types. In: The 7th Annual
Translational Bioinformatics Conference Translational Bioinformatics in Precision Medi-
cine, Los Angeles, California, USA (2017)
7. Uzuner, Ö.: Recognizing obesity and comorbidities in sparse data. JAMIA 16, 561–570
(2009)
8. http://www.ncbi.nlm.nih.gov/books/NBK9676/
9. https://www.nlm.nih.gov/healthit/snomedct/snomed_overview.html
854
R. Reátegui and S. Ratté

10. Aronson, A.R., Lang, F.-M.: An overview of MetaMap: historical perspective and recent
advances. JAMIA 17, 229–236 (2010)
11. Chapman, W.W., Bridewell, W., Hanbury, P., Cooper, G.F., Buchanan, B.G.: A simple
algorithm for identifying negated ﬁndings and diseases in discharge summaries. J. Biomed.
Inform. 34, 301–310 (2001)
12. Blondel, V.D., Guillaume, J.L., Lambiotte, R., Lefebvre, E.: Fast unfolding of communities
in large networks. J. Stat. Mech. Theory E (2008)
Automatic Extraction and Aggregation of Diseases
855

eHealth Applications in Portuguese Hospitals:
A Continuous Benchmarking
with European Hospitals
João Vidal Carvalho1(&), Álvaro Rocha2, and António Abreu1
1 Politécnico do Porto, ISCAP, CEOS.PP, S. Mamede de Infesta, Portugal
{cajvidal,aabreu}@iscap.ipp.pt
2 Departamento de Engenharia Informática, Universidade de Coimbra,
Coimbra, Portugal
amrocha@dei.uc.pt
Abstract. The aim of this study is to describe and compare the use of eHealth
applications in selected European Union (EU) countries. Using data available
from the European Hospital Survey (2012/2013), the study reported in this paper
analyses the adoption of ﬁve of the most commonly used Health Applications in
European Hospitals, namely Electronic Patient Records (EPR), Picture Archiv-
ing and Communication System (PACS), ePrescribing, Integrated system for
eReferral and Tele-monitoring. The analysis of the adoption of these applica-
tions by the Hospitals allows us to understand the position of Portugal in relation
to the European average. It has been found that the Portuguese Hospitals are
well positioned with regard to the adoption of these applications and in spite of
this, remains a permanent concern to raise the levels of adoption of these
applications through new projects in progress.
Keywords: eHealth  Hospital information systems
Health information systems  eHealth applications  Portugal
1
Introduction
The rapid development of the Information and Knowledge Society and consequently,
the rapid advancement of Information and Communication Technologies (ICT) have
revolutionized the way we interact with each other. The convergence between the
acceleration capabilities of computers, the range and expansion of the Internet and the
increase in the ability to capture and leverage the knowledge in digital format are key
drivers for the technological revolution that we live nowadays. The current information
society, has the potential to cause a revolution similar in health care [1].
It could change the relationship between the patient and the professional, providing
valuable opportunities for health professionals rendering healthcare services effectively
through the use of Information Systems and Technologies (IST) to their patients and
providing them ease access to relevant (clinical) information. However, healthcare
systems around the world, currently, are facing considerable pressure to reduce costs,
enhance and improve service efﬁciency, expand access while maintaining or even
improving the quality of health services provided
[2–4]. The side effects, such
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_81

as demographic trends, the lack of qualiﬁed health professionals and the expectations
and demands of patients, local administrators or health insurers come to hinder the
fulﬁlment of this mission [5]. There are strong expectations that a wider adoption of IST
in the health ﬁeld will contribute to improve the health of individuals and the perfor-
mance of providers, yielding improved quality, cost savings, and greater engagement by
patients in their own healthcare [6]. However, there is evidence that the implementation
of the IST without the adaptation of the structures and the strategic and organizational
processes behind it, will not necessarily generate the expected beneﬁts [7].
In this context, appears the concept of eHealthcare deﬁned by Sharma [8] as: “The
way to achieve the best results of the health process through the effective and inno-
vative use of IT”. eHealthcare is described as the application of IT Infrastructure across
the spectrum of functions that affect the health sector [9]. eHealthcare includes tools for
health authorities and their professionals, as well as, customized health systems for
patients and citizens. eHealthcare may therefore be adopted to cover the interaction
between patients and health care providers, may include health information networks,
electronic health records, telemedicine services and personal portable systems, in
support of management of prevention, diagnosis, treatment, follow-up of patients’
health and lifestyle.
eHealthcare also offers signiﬁcant opportunities for health professionals to provide
technologically effective services to their consumers by providing them with ways to
access the information they need. In the same sense, the European Commission
(EC) clariﬁes that eHealth is “the use of ICT in health products, services and processes
combined with organizational change in healthcare systems and new skills, in order to
improve health of citizens, efﬁciency and productivity in healthcare delivery, and the
economic and social value of health” [10]. eHealth represents ICT application in the
entire set of functions affecting health care sector. It contains applications and tools not
only for health professionals and health care providers, but also for patients. From the
standpoint of this study: “eHealth covers the interaction between patients and
health-service providers, institution-to-institution transmission of data, or peer-to-peer
communication between patients and/or health professionals” [10].
In this article, we will initially present the indicators of a study carried out in
Europe in 2012/2013 that will be analyzed and compared with Portugal Healthcare
reality. After describing the study carried out by the EU, the values of ﬁve health
applications will be compared between Portugal and the European average. Finally, we
will present recent projects carried out in Portugal that suggest a positive evolution of
the indicators considered in this study.
2
Scope of eHealth Applications
Based on the eHealth Benchmarking III study from the European Hospital Survey
(2012/2013) [11], country proﬁles have been built using 13 eHealth indicators. These
eHealth indicators consist in speciﬁc answers to the questionnaire and identify the
eHealth best practices in Europe.
eHealth Applications in Portuguese Hospitals
857

Regarding the study presented in this article, a selection of ﬁve medical applications
was made. These include: Single EPR shared by all departments; PACS usage; ePre-
scribing; Integrated system for eReferral; Tele-monitoring.
2.1
Single EPR Shared by All Departments
The deﬁnition of Electronic Health Record (EHR) is variable. According to eHealth
Strategies Report [10], EHR is the longer-term electronic record of a patient which
comprise or virtually interconnected data in numerous Electronic Medical Records
(EMR) and Electronic Patient Records (EPR). EHR is a real-time record that should be
shared and exchanged across healthcare system, between institutions and healthcare
service providers. It should be interoperable, as its purpose is to incorporate the con-
tacting history of patients with healthcare providers from all the organizations which
delivered care to this particular individual [12]. The term EMR is understood as the
electronic record of patient located in an outpatient clinic or doctor’s ofﬁce. The
adoption of an EMR system is a primary goal of modern health organizations, as it is
mainly destined to improve their effectiveness when treating patient information and
making it available in a timely and accurate form at point-of-service [13]. The deﬁ-
nition of EPR indicates electronic record of a patient conducted in a hospital or other
medical facility.
2.2
PACS Usage
There are many deﬁnitions for PACS, from the simple systems used for image scanning
to integrated organization-wide image management systems that streamline operations
throughout the entire process of patient care. Huang [14] deﬁnes PACS as an imaging
system, associated with workﬂow that is designed to streamline operations throughout
the entire healthcare delivery process. Anderson e Flynn [15] have systematically
reviewed the literature on a wide range of topics on PACS, and have deﬁned PACS
from a technical point of view, stating that PACS are high-speed computer network
systems that store, retrieve, and display radiological images. As evidenced by the
various deﬁnitions, PACS is a broad term encompassing a set of components and
systems related to medical imaging for clinical practice [16]. In short, PACS can be
either a very simple or rather complex system [17].
2.3
ePrescribing
Electronic prescription has proved to play an important role in patient safety, quality of
care as well as cost control [18]. Electronic prescribing and dispensing (EPD) of
medication has been developed in recent years and is now broadly available in many
countries. Despite some speciﬁc differences in the process of prescribing, transmitting
medication information from physicians to pharmacies and dispensing medication to
the patient, all of which are improved with additional patient safety quality and control
when using this system.
ePrescribing services are recognized as a major improvement to the traditional
paper based medication prescribing and dispensing ones due to the following reasons:
858
J. V. Carvalho et al.

reduction of mistyping and misreading errors; less wrong patient wrong medication
issues; improved patient conﬁdentiality; better methods to health care worker identi-
ﬁcation conﬁrmation; integration with patients health records; more security in the
dispensing phase; cost monitoring.
2.4
eReferral
The imperatives of timely access and rational triage drove the creation, implementation,
and spread of our home-grown, web-based, integrated specialty referral and consul-
tation system, called eReferral. It uses health information technology to link primary
care providers (PCP) and specialists, with the goals of increasing access to care,
improving dialogue, optimizing the efﬁcient use of specialty resources, and enhancing
primary care capacity. Adoption of an eReferral model will be increasingly feasible,
because this type of system holds the potential for addressing care-coordination
challenges, boosting the effectiveness of in-person specialty visits, and producing cost
savings by reducing the number of specialty visits for conditions that can be managed
by PCP [19]. eReferral, if widely adopted, could help in achieving better care for
individuals, better health for populations, and lower costs.
2.5
Tele-monitoring
Tele-monitoring is a category of telemedicine or telehealth. Telemedicine can be
deﬁned as the use of medical information exchanged between two geographically
distant locations through electronic communications to improve a patient’s clinical
health status [20]. It is possible that new modalities of medical action, where Tele-
medicine is being applied, will emerge on a large scale in the coming years. With the
evolution of the media, it is natural that the contact between the doctor and the patient
can be done at a distance, adopting a growing variety of applications and services that
use bidirectional video, email, smart phones, wireless devices and other Telecommu-
nications. With the use of these ICT for patients outside hospital facilities, health
professionals can more effectively monitor patient health and prevent complications
that may lead to rehospitalization. These services are particularly useful for patients
living with chronic diseases or elderly patients.
3
eHealth Benchmark Study in Europe
The European Union, through its executive body, the European Commission (EC), has
been a very active stakeholder in promoting the digital agenda in health in recent
decades [21, 22]. Since 1989, the EC has invested over €1 billion in over 450 eHealth
projects [22]. The work includes action plans for eHealth (e.g. [10]) directives and
guidelines related to eHealth (e.g. [23]), sponsored eHealth projects (e.g. [24]),
benchmarking activities (e.g. [25]) and commissioned research (e.g. [11]).
The EC has undertaken extensive research into the adoption of eHealth in European
countries in the last decade, and this material was used in the study. The EC commis-
sioned three major surveys into the adoption of eHealth in primary care in 2002 [25],
eHealth Applications in Portuguese Hospitals
859

2007 [26] and 2013 [11] in 15, 29 and 31 countries, respectively. In the last survey
(2013), the total sample are 1753 Hospitals of which 579 are Portuguese. Taken toge-
ther, this qualitative and quantitative research paints a rich and complex picture of the
development and adoption of eHealth in Europe over the past two decades.
The 2013 EC survey studied general practitioners (GP) in the primary care setting,
which the report deﬁned as “physicians working in outpatient establishments in spe-
cialties such as general practice, family doctor, internal medicine and general medi-
cine” [11]. The survey study took 18 months, and 9196 GP (2%) from 31 European
countries were surveyed in detail about the adoption of eHealth in primary care.
3.1
Limitations of the Study
This study was limited by the availability of comparable data for 31 countries and by
material in the English language. Other European countries in the Balkan area, Eastern
Europe and Switzerland were not included in the study. This study focuses mainly on
aspects of eHealth, with a comparatively higher level of detail. However, questions of
general scope such as hospital growth, ﬁnancial position or strategic direction are
missing. As a result of missing information for some explanatory variables, the analysis
is based on smaller samples. It is probable that some countries have made further
progress since their data were published in 2014, and the adoption rates in these
countries may be higher than those reported in this study.
3.2
Questions Used in the Study for eHealth Applications
To build the eHealth indicators, we have relied on the most important questions of the
survey. Table 1 presents these questions and answer options used to deﬁne the eHealth
applications indicators and assess the implementation of eHealth in European acute1
care hospitals.
3.3
Portugal’s Acute Hospitals eHealth Proﬁle
The study identiﬁed 589 hospitals in Portugal. Within this sample, 224 (38%) com-
pleted the screener part of the questionnaire and, of these, 12% qualiﬁed as acute care
hospitals. Of the 73 screened in, 41 acute hospitals (56%) completed the survey, mostly
under 250 beds (Table 2).
Table 3 shows that most hospitals involved in the study are public.
3.4
Position of the Portuguese eHealth Proﬁle in EU27 + 3
The following ﬁgure presents the spider chart showing the proﬁle of the Portuguese
Hospitals in the indicators evaluated in the 2013 study. It is observed in Fig. 1 that the
score presented in the applications “Single EPR shared by all departments”, “PACS
1 The study deﬁned the following criteria to qualify survey participants as acute care hospital: The
hospital has an emergency department, and at least a routine and/or life-saving surgery operating
room and/or an intensive care unit.
860
J. V. Carvalho et al.

usage” and “ePrescribing” show values higher than the European average, especially
the last one.
Table 4 show that Portugal is close to the European average in its eHealth proﬁle.
However, the gains over and above the European average are not evenly distributed,
Table 1. Question items used for the eHealth indicators.
eHealth
indicator
Question used
Answer option used
Single EPR
shared by all
departments
Which type of EMRs/EHRs/EPRs
does your hospital mainly use?
A hospital-wide EMR/EHR/EPR
shared by all the clinical service;
Multiple dept EMR/EHR/EPR
witch share information; Multiple
dept EMR/EHR/EPR not sharing
information; No EMR/EHR/EPR
used; Don’t Know; Question
Skipped
PACS usage
Does the hospital use a
Picture Archiving and
Communication System (PACS)?
Yes; No; Don’t Know; Question
Skipped
ePrescribing
Which of the following
computerised systems has the
hospital integrated?
A computerized system for
ePrescribing
Integrated
system for
eReferral
Which of the following
computerised systems has the
hospital integrated?
An integrated system to send or
receive electronic referral letters
Tele-monitoring
Does the hospital have the
following computer-based system
or applications?
Tele-homecare/tele-monitoring
services to outpatients (at home)?
Table 2. Portuguese breakdown by size of hospital.
Hospitals
<101
beds
 101
and  250
 251
and  750
>750
beds
No
answer
Census
73
21 (29%)
16 (22%)
7 (10%)
3 (4%)
26 (36%)
Survey
2013
41
13 (32%)
11 (27%)
6 (15%)
3 (7%)
8 (20%)
Table 3. Portuguese breakdown by ownership type.
Hospitals Public
Private
Private (not proﬁt) No answer
Census
73
34 (47%) 20 (27%) 8 (11%)
11 (15%)
Survey 2013 41
24 (59%) 13 (32%) 4 (10%)
eHealth Applications in Portuguese Hospitals
861

with “ePrescribing” alone standing 48% above the EU27 + 3 average. Similarly,
“PACS Usage” was 12% and “Single EPR shared by all departments” was 6% above
the average, with other applications with values below the EU27 + 3.
Private acute hospitals in Portugal appear to be the best endowed in terms of
eHealth capabilities, with Private hospitals leading by a wide margin in two areas:
“Single EPR shared by all departments” and “PACS usage”. It should also be noted
that Private not for proﬁt acute hospitals led notably in “Single EPR shared by all
departments” at 100% (Fig. 2).
The distribution of eHealth capabilities appears to be relatively even in terms of
hospital size. Although the largest hospital segments (Between 251 and 750 beds, as
well as More than 750 beds) have leadership positions in “PACS usage”, smaller
hospitals lead in “Single EPR shared by all departments” (Fig. 3).
Fig. 1. Portuguese acute hospital eHealth proﬁle in study
Table 4. Portuguese eHealth indicators.
eHealth indicator
Score difference Portugal vs EU27 + 3
Single EPR shared by all departments 6%
PACS usage
12%
ePrescribing
48%
Integrated system for eReferral
−23%
Tele-monitoring
−5%
Note: Results are based on valid answers only. The scoring scale from 0 to 5 points
corresponds to an implementation rate from 0% to 100%.
862
J. V. Carvalho et al.

4
Portuguese eHealth Applications Projects in Progress
After the publication of these results in 2014, it is very likely that the values associated
with these indicators show progress, and thus, the adoption rates may be higher than
those reported in this study. Indeed, the Portuguese governmental organization SPMS
(Serviços Partilhados do Ministério da Saúde2, EPE), develops and manages several
eHealth projects, which have certainly contributed to this progress over the last two
years. To reinforce this idea, the Strategic Plan 2017–2019 [27] expresses the SPMS
contribution in order to improve the management of hospitals, circulation of clinical
information and articulation with other levels of care and other agents of the sector,
betting on the EHR as an indispensable tool for efﬁcient access management, equity
and quality by creating effective conditions for sharing results from complementary
diagnostic and therapeutic resources, harmonizing data sets, enhancing clinical research
and secondary use of data through interoperability initiatives, as well as creating a
national telehealth network.
Fig. 2. Portuguese acute hospitals eHealth proﬁle by ownership
Fig. 3. Portuguese acute hospitals eHealth proﬁle by size
2 Shared Services of the Ministry of Health.
eHealth Applications in Portuguese Hospitals
863

One of the projects we can highlight is the Medical Electronic Prescription (PEM).
The PEM has the objective of simplifying the act of prescribing support products and
facilitating the access of users. The adoption of PEM allowed immediate gains in time
management of physicians, who stopped using the previous system of prescription,
more time consuming and complex [28].
Regarding the adoption of Telemedicine in Portuguese Hospitals, the Secretary of
State for Health, Manuel Delgado, stated that telemedicine is a government bet, rein-
forcing that “these projects go forward, they are reproductive and they pay for
themselves because they will save money on transport costs, treatments and successive
consultations that are avoided” [29]. With such projects proliferating in hospitals, it is
very likely that the indicators with the lowest values in this study will rapidly approach
the European average.
5
Conclusion
The analysis reported in this paper reveals that Portuguese hospitals are above the
European Hospitals average, for the adoption of the eHealth EPR, PACS and ePre-
cribing applications. However, it is slightly lower in terms of the adoption of integrated
system for eReferral and Tele-monitoring applications. It has been observed that in
recent years there has been an effort by the Portuguese government to promote inno-
vative projects in the health area that suggest a positive evolution of the indicators
considered in this study, improving the levels of adoption of eHealth applications.
Finally, the study reported in this paper is not free of limitations. First, the data
collected in the study were published in 2014 and their analysis may be slightly
outdated. Second, there is a need for further studies to monitor and optimize eHealth
applications in Hospitals for the healthcare system in Portugal. Finally, the scope and
size of this paper does not allow to evaluate the level of maturity of the information
systems of Portuguese hospitals.
References
1. Carvalho, J.V., Rocha, Á., Abreu, A.: HISMM–hospital information system maturity model:
a synthesis. In: International Conference on Software Process Improvement, vol. 537,
p. 189–200. Springer, Cham (2016)
2. Fitterer, R., Rohner, P.: Towards assessing the networkability of health care providers: a
maturity model approach. Inf. Syst. e-Bus Manag. 8, 309–333 (2010)
3. Ludwick, D., Doucette, J.: Adopting electronic medical records in primary care: lessons
learned from health information systems implementation experience in seven countries. Int.
J. Med. Inform. 78(1), 22–31 (2009)
4. Jha, A.K., et al.: Use of electronic health records in US hospitals. N. Engl. J. Med. 360(16),
1628–1638 (2009)
5. Ahtonen, A.: Healthy and active ageing: turning the silver economy into gold. European
Policy Centre, Europe’s Political Economy–Coalition for Health, Ethics and Society (CHES)
(2012)
864
J. V. Carvalho et al.

6. Buntin, M.B., et al.: The beneﬁts of health information technology: a review of the recent
literature shows predominantly positive results. Health Aff. 30(3), 464–471 (2011)
7. Mettler, T.: Transformation of the hospital supply chain: how to measure the maturity of
supplier relationship management systems in hospitals? Int. J. Healthc. Inf. Syst. Inf. 6(2), 1–
13 (2011)
8. Sharma, B.: Electronic Healthcare Maturity Model (eHMM): A White Paper. Quintegra
Solutions Limited (2008)
9. NHS, Information Management and Technology (IM&T) Strategy 2012–2017: To provide
the knowledge, skills, technology and tools that enable information to be collected,
managed, used and shared to deliver excellence in healthcare. Mid-Cheshire Hospitals NHS
Found Trust (2011)
10. Commission To The European Parliament: Communication from the Commission to the
European Parliament, the Council, the European Economic and Social Committee and the
Committee of the Regions, eHealth Action Plan 2012–2020 – Innovative healthcare for the
21st century. COM (2012) 736 ﬁnal (2012)
11. The European Commission: European Hospital Survey: Benchmarking Deployment of
eHealth Services (2012–2013) Final report. JRC Scientiﬁc and Policy Reports (2014)
12. OECD: Strengthening Health Information Infrastructure for Health Care Quality Gover-
nance: Good Practices, New Opportunities and Data Privacy Protection Challenges. OECD
Health Policy Studies, OECD Publishing, Paris (2013)
13. Priestman, W.: ICT Strategy 2007–2011 for The Royal Liverpool and Broadgreen
University Hospitals NHS Trust. Trust Board Meeting 6th November 2007, Doc Number:
V1.4 (2007)
14. Huang, H.K.: PACS is only in the beginning of being used as a clinical research tool. In: The
24th International EuroPACS Conference, Trondheim, Norway, pp. 1–10 (2006)
15. Anderson, D., Flynn, K.: Picture Archiving and Communication Systems: A Systematic
Review of Published Studies of Diagnostic Accuracy. Radiology Work Processes, Outcomes
of Care, and Cost. Technology Assessment Program. Report No. 5 (1997)
16. Huang, H.K.: Some historical remarks on picture archiving and communication systems.
Comput. Med. Imaging Graph. 27, 93–99 (2003)
17. van de Wetering, R., Batenburg, R.: A PACS maturity model: A systematic meta-analytic
review on maturation and evolvability of PACS in the hospital enterprise. Int. J. Med.
Informatics 78, 127–140 (2009)
18. Patrao, L., Deveza, R., Martins, H.: PEM-A new patient centred electronic prescription
platform. Procedia Technology 9, 1313–1319 (2013)
19. Chen, A.H., et al.: eReferral–a new model for integrated care. N. Engl. J. Med. 368(26),
2450–2453 (2013)
20. ATA. What is Telemedicine? American Telemedicine Association 2012, October 2015,
http://www.americantelemed.org/about-telemedicine/what-is-telemedicine
21. Currie, W., Seddon, J.: A cross-national analysis of eHealth in the European Union: some
policy and research directions. Inf. Manag. 51(6), 783–797 (2014)
22. Piha, T.: How Can the EU Drive ehealth Development? Mede-Tel, Luxembourg: European
Commission. DG Health and Consumers (2014)
23. European-Commission.: Directive 2011/24/Eu of the European Parliament and of the
Council of 9 March 2011 on the Application of Patients’ Rights in Cross-Border Healthcare.
Brussels: European Commission (2011)
24. epSOS, Smart Open Services for European Patients–WP 1.1: Analysis and Comparison of
National Plans/Solutions. European Commission (2009)
25. European-Commission: SIBIS Report No. 9: Benchmarking e-Health in the IS in Europe and
the US. European Commission (2003)
eHealth Applications in Portuguese Hospitals
865

26. Empirica: Benchmarking ICT Use among General Practitioners in Europe. Bonn: European
Commission: Information Society and Media Directorate General (2008)
27. SPMS, Plano Estratégico SPMS 2017–2019 (2016)
28. SPMS, PEM: Simpliﬁca prescrição de produtos de apoio para pessoas com deﬁciência e com
incapacidade temporária. Newsletter Cuidados de Saúde Hospitalares, 3 (2017)
29. Delgado, M.: Telemedicina em Hematologia: CHCB e CHUC Iniciam Colaboração.
A Enfermagem e as Leis (2017)
866
J. V. Carvalho et al.

Innovation Process in Cancer Treatment:
The Implementation of Picture Archiving
and Communication System (PACS)
at Brazilian National Cancer Institute
Antônio Augusto Gonçalves1,2, Carlos Henrique Fernandes Martins1,
José Geraldo Pereira Barbosa2,
and Sandro Luís Freire de Castro Silva1(&)
1 COAE Tecnologia da Informação, Instituto Nacional do Câncer,
Rua do Resende 195, Rio de Janeiro 20230-092, Brazil
sandrofreire@gmail.com
2 MADE, Universidade Estácio de Sá, Avenida Presidente Vargas 642,
Rio de Janeiro 200071-001, Brazil
Abstract. Picture Archiving and Communication System (PACS) presents an
innovation process which radically changes radiology services both inside and
outside the hospital setting. Earlier, the usual method for capturing, storing,
retrieving, and viewing radiology images was hard copy ﬁlm. The adoption of a
new system frequently comes with challenges. In this article, the employment of
PACS by the Brazilian National Cancer Institute was analyzed. Medical images
were collected from several imaging equipment, stored and distributed to the
physicians. Distinct of the implementation of other clinical information systems,
PACS implementation was considered a success; its beneﬁts were signiﬁcant.
The results of this research indicate that process innovation involves governance
and group work. Collaboration is one of the major drivers of process innovation
in healthcare. Open communication, social relations and networks, and close
contacts between innovators are frequently mentioned as indispensable princi-
ples for an innovative organization. These practices should prevail across
functional and organizational frontiers.
Keywords: Picture Archiving and Communication System  Innovation
1
Introduction
Picture Archiving and Communication System (PACS) presents an innovation process
which radically changes radiology services both inside and outside the hospital setting.
Earlier, the usual method for capturing, storing, retrieving, and viewing radiology
images was hard copy ﬁlm. The adoption of a new system frequently comes with
challenges. The challenges generally highlighted are training issues, lack of support,
system failures, and difﬁculty accessing images A shift to digital imaging is not a
simple change. The users need to improve new abilities and make modiﬁcations in the
workﬂow process through training [1].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_82

When considering the ﬁnancial impact of PACS, beneﬁts can basically fall into two
categories: cost savings and increased proﬁt. Cost savings are achieved through the
reduction of costs associated with activities such as excluding staff levels required for
maintaining the hard copy ﬁlm library, excluding cost for chemicals and ﬁlm, elimi-
nating transportation activities and releasing space traditionally used to store hard copy
ﬁlm [2].
PACS has the capacity to deliver efﬁcient access to images and related data. It
breaks down the physical and cultural barriers associated with traditional ﬁlm-based
image retrieval, distribution and display. Images can be extracted from several medical
imaging equipment including Positron Emission Tomography (PET), Computed
Tomography (CT), Magnetic Resonance (MR), Endoscopy (ENDO), Mammograms
(MG), Digital Radiography (DR), Computed Radiography (CR) [3].
The organization’s intention to adopt a new technology is inﬂuenced by a number
of other factors, including the cost of the desired innovation, its user-friendliness, the
technology’s compatibility with existing systems, and its alignment with the organi-
zational structure [4]. Some innovations like PACS require a level of investment,
training or reinforcement that healthcare organizations have difﬁculties to achieve.
Improvement in health care clearly depends on change, but change always creates
new challenges. Quality improvement systems must strive to keep under control the
effects of implementing new practices or technologies. Generally, the lack of adequate
performance indicators related to these innovations can make managers uncomfortable
with them [5].
Advances in ICT applications like PACS helps healthcare managers establish the
basis for fundamental changes within the healthcare organization. Besides, allowing the
physicians to communicate easily and quickly with staff increases awareness about
healthcare issues [6]. ICT systems are increasingly demanded by healthcare workers. In
order to make correct implementation decisions, it is essential to know the beneﬁts and
challenges of every single costly technology [7].
In latest years, the implementation of PACS has become widespread in the area of
diagnostic radiology. However, the adoption of this software within the ﬁeld of
radiotherapy is still very limited, although the fact that the majority of radiotherapy
systems use Digital Imaging and Communications in Medicine (DICOM). The
increased availability of PACS within the hospital environment offers the potential and
opportunity to use a single well-managed storage solution for radiotherapy data [8].
The quick advances in the capabilities of information and communication tech-
nology and its recent fast adoption in healthcare delivery have ensured that ICT will
play a vital role in cancer care too. It can support and improve the effectiveness,
efﬁciency and quality of care delivered by the healthcare system [9].
Brazil currently has a complex cancer scenario. General incidence and mortality
rates are elevated, with a particularly high incidence of prostate cancer in men and
breast cancer in women. These cases have been responsible for over 19.0000 deaths per
year. There are approximately 600.000 new diagnoses of cancer each year in Brazil,
and the vast majority of these patients have had some contact with hospital services.
However, research also shows that, in several regions of the country, long waiting lists
for diagnostics and treatments have become commonplace, which has led to a situation
people being diagnosed with cancer at a very advanced stage.
868
A. A. Gonçalves et al.

In this context, Brazilian National Cancer Institute is the federal agency in charge of
setting and implementing assistance, education and cancer prevention policies, with a
speciﬁc coordination team based in the city of Rio de Janeiro. INCA has made huge
changes in its organization, with the aim of leading Brazilian efforts towards the
development of cancer care innovation process. Information and Communication
Technologies (ICTs) have played an important role in these changes.
In this article, the employment of PACS by the Brazilian National Cancer Institute
was analyzed. Medical images were collected from several imaging equipment, stored
and distributed to the physicians. Distinct of the implementation of other clinical
information systems, PACS implementation was considered a success; its beneﬁts were
signiﬁcant. Getting end users to accept and really adopt this technology was one of the
ultimate barriers that INCA had to overcome.
2
Literature Review
Healthcare organizations have generated huge amounts of data spread on hospital
information systems, patient electronic records, administrative systems, etc. [10].
However, most of this effort has proven unproductive, as this information is hardly ever
effectively used in decision-making processes. This situation demands from health
organizations the need to create an environment that facilitates the transformation of all
this information into knowledge [11].
Picture archiving and communication system (PACS) is an innovative imaging
informatics application in health care organizations, speciﬁcally designed for radiology
departments. It is an integrated repository for all imaging data and provides diagnostic
images and radiology reports electronically to clinicians. PACS has revolutionized the
communication between radiologists and clinicians, improving clinical decision mak-
ing and enabling efﬁciently patient care. It can be deﬁned as an electronic information
system (IS) used to acquire, store, transmit, and display medical images [12].
Hospitals face several obstacles that obstruct the implementation of PACS ranging
from deﬁciency of interoperability to human and material resources. Another signiﬁ-
cant PACS implementation barrier has traditionally been the system cost. Unfortu-
nately, the majority of savings are not straightly traceable. For this reason, the
economical and organizational impact of PACS has been underestimated for long time.
This situation is expected to change due to the progressively increased number of
treatments as well as a reduction of healthcare technologists and medical staff brought
by PACS, which will possible speedy additional development and diffusion of inno-
vative applications within the healthcare enterprise [13].
Notwithstanding these barriers, a recent study showed that 94.2% of the physicians
perceived that the PACS improved the efﬁciency of the patient’s follow-up process
because of the availability of images in multi locations, and easy consultations between
different departments [14].
The adoption of PACS is different when compared to other types of Information
Systems due to its inherent complexity. PACS is required to integrate and process data
from hospital information system (HIS) in order to provide adequate patient
Innovation Process in Cancer Treatment
869

information to the entire healthcare staff. Additionally, the implementation of PACS is
expensive requiring justiﬁcation for its high costs [15].
Innovation in healthcare can take many types. Typically, they may range from
product, process, to structure. It can vary from new drug therapies and surgical pro-
cedures, to innovative forms of health professional training, patient education, and
management, ﬁnancing and service delivery models [5].
Innovation in healthcare can take several forms, going from original medication
therapies, robotic surgery through to new treatment protocols. ‘Innovation’ is widely
assumed to be positive in its effects. If a new technology or technique is being con-
sidered for use, health systems should consider the principles of quality improvement
when they are introduced rather than wait for the inevitable problems to occur [16].
Many new devices, particularly diagnostic equipment, need do more than only
demonstrate that they comply with basic safety standards. Diffusion of innovation
without proven efﬁcacy introduces several threats to quality improvement [16].
Cooperative efforts may be weakened by the risk that individuals will substitute
their private goals for those of the group, so that the collaboration is determined by
individual or group interests. Professional boundaries, particularly between different
professional groups, may be detrimental to appropriate collaboration. Social activities
can become sites of struggle and contestation, and may never succeed in fully deliv-
ering their objectives [17].
Research conducted at a cancer treatment hospital has shown that health organi-
zations that promote a constructive relationship between workers, emphasizing their
participation in social activities and sharing best practices with other members of the
team, deal more successfully with innovations. This study also indicated that those
healthcare organizations which are hierarchical in nature have professional conﬂicts
and are less creative [18].
It is signiﬁcant to emphasize that cancer patient services are inherently multidis-
ciplinary, involving the primary care doctor, the radiologist, the pathologist, the
oncologist and the surgeon [19]. The quality of these services has been affected by the
scarce amount of qualiﬁed oncology specialists. The use of information technology can
contribute considerably to the improvement of healthcare [20].
Galligioni have described their experience in developing a web-based oncology
medical records to support patient care [21]. The records were designed to integrate the
range of cancer care management activities, from consultations to chemotherapy
treatment, allowing efﬁcient patient care.
3
Methodology
This study was established through a qualitative research project to show a descriptive
analysis of PAC’s implementation into the Brazilian National Cancer Institute (INCA).
A single-case study intends to make progress to the knowledge of organizational
phenomena, presenting a description of the system implemented, through an empirical
inquiry, answering the questions what, who, where and how [22]. INCA was chosen
as a unit of analysis to take advantage of the professional experience of three of the
authors while working at its Information and Communication Technology Division.
870
A. A. Gonçalves et al.

The researchers conducted seven semi-structured interviews with the technicians in
charge of the implementation of PACS in the area of Information and Communication
Technology and with twelve staffs of the Radiology Department (users of the system).
The interviews had been conducted over the ﬁrst six months of 2017, based on a script
with open questions, and structured from the literature review used in the research.
The data collected was treated by the categorical content analysis method, trian-
gulating the frequency in which terms and concepts present in the recorded answers
occurred, with the observations and data got by the researchers. All the interviews were
conducted on the premises of INCA and recorded with the consent of the interviewees.
The investigation is exploratory in nature and it is believed that through direct
observation and semi structured interviewing, the case study methodology would be
most valuable to show the process innovation. In order to mitigate the lack of rigor
frequently attributed to case study methodology, we have closely followed suggestions
from previous researches to ensure the validity of this study.
4
PAC’s Implementation at INCA
INCA is an agency under the direct administration of the Ministry of Health, associated
with the health care secretariat. INCA has ﬁve specialized hospital units and is a large
organization with over 650 physicians plus an allied health staff of nearly 3.400. It
attends over 50.000 outpatients per year and has approximately 350 inpatient beds with
over 13.000 hospital admissions per year.
In order to be clinically effective, PACS is fully integrated with the main healthcare
information systems at INCA, including radiology information system (RIS), hospital
information system (HIS) and other medical information systems. The average radi-
ology exams interpretation time after PACS implementation has decreased in average
from over 55 min to less than 20 min.
The adoption of PACS has required a paradigm change in the way INCA performs
its activities. The two key points of the implementation strategy were an active
stakeholder’s participation before and during the implementation, and the decision to
initiate the implementation at the radiology unit and then gradually to roll out to other
units when deemed appropriate. In the case of PACS implementation, technologists,
nurses and radiologists could believe that the automation of some tasks possibly would
result in job shortage. Physicians could feel comfortable in the way they have been
handling ﬁlm for years and to resist to adopt this new technology. Therefore, change
management and communication were critical factors of success of this project.
Before PACS implementation each imaging modality such as X- ray, computed
tomography (CT) and magnetic resonance imaging (MRI) had its own printer to print
images. The operation was completely paper and ﬁlm based. Patient’s diagnostic image
was always performed using ﬁlm with light boards on the walls. Films were stored in
warehouses occupying large areas. The intense ﬂow of ﬁlms and papers into hospital
units required by this process were leading to losses and rework.
The PACS’ implementation main beneﬁts were cost reduction and rapidly and
efﬁciently images access anywhere into INCA units. Elimination of the need of ﬁlms
has provided a saving of U$ 650,000.00/year in ﬁlms, chemicals, water and
Innovation Process in Cancer Treatment
871

maintenance. Furthermore, the new procedure obtained a reduction of around 8 million
liters of water, 15 thousand liters of chemicals for revelation, and 52 kg of silver which
has contributed hugely to decrease the environmental impact of the hospital units
activities.
5
Conclusion
Modern cancer care is structured around three vectors: First, multifaceted treatments
directed to patients’ tumor and biological characteristics. Second, an approach to care
that is focused on patients’ requirements (physical, psychosocial, functional, spiritual).
Finally, the use of information systems that support organizations in realizing their
clinical medicine and patient-centered care delivery goals.
This research has concluded that PACS is a vital tool for patient-centered care, but
must be fully integrated with other information systems relevant to the cancer care
environment. Initially, we need the right information and the right procedures, both of
which must be responsive to the requirements of organizations, clinicians, and patients.
The decision to adopt PACS does not guarantee its accomplishment. PACS
deployment should be considered not only as a merely rollout of new technology but as
an innovation process. The strategy should contemplate all technical, economic,
organizational and human issues. The beneﬁts of such an innovation are immediately
perceived in the improvement of the access to useful information and in the increase of
operational efﬁciency patient care management.
If a new technology like PACS is being implemented, health systems must consider
the premises of quality improvement as they are introduced rather than wait for the
inevitable complications to happen. It is essential to recognize the risks of innovation –
including the negative effects of barriers to innovation.
Once the decision is to adopt PACS, there are many barriers to overcome. Man-
agers must be aware of impacts on end users’work process and be prepared to address
issues that will arise. Technicians who understand the technology of PACS must be
identiﬁed to support decision making about vendor selection, network architecture,
workstations functionalities, and archives.
The results of this research indicate that process innovation involves governance
and group work. Collaboration is one of the major drivers of process innovation in
healthcare. Open communication, social relations and networks, and close contacts
between innovators are frequently mentioned as indispensable principles for an inno-
vative organization. These practices should prevail across functional and organizational
frontiers.
References
1. MacDonald, D.M.: Evaluating the implementation of picture archiving and communication
systems in Newfoundland and Labrador. ProQuest (2008)
2. Chan, L., Trambert, M., Kywi, A., Hartzman, S.: PACS in private practice—effect on proﬁts
productivity. J. Digit. Imaging 15, 131–136 (2002)
872
A. A. Gonçalves et al.

3. Reiner, B.I., Siegel, E.L., Flagle, C., Hooper, F.J., Cox, R.E., Scanlon, M.: Effect of ﬁlmless
imaging on the utilization of radiologic services. Radiology 215, 163–167 (2000)
4. Fichman, R.G.: The diffusion and assimilation of information technology innovations. In:
Zmud, R.B. (ed.) Framing the Domains of IT Management: Projecting the Future through
the Past, pp. 105–127. Pinnaﬂex Educational Resources Inc., Cincinnati (2000)
5. Varkey, P., Horne, A., Bennet, K.E.: Innovation in health care: a primer. Am. J. Med. Qual.
23(5), 382–388 (2008)
6. Lester, H., Hobbs, F.D.R.: Of family medicines from the quality and outcomes framework in
the United Kingdom. Fam. Med. 39(2), 96–102 (2007)
7. Alamu, F., Oke, A.: Developing a robust multimedia picture archiving and communication
system (PACS). Int. J. Comput. Appl. 34(4), 12–25 (2011)
8. Shakeshaft, J.: PACS in Radiotherapy. Clin. Oncol. 22(8), 681–687 (2010)
9. Rhandhawa, G.S., Ahern, D., Hesse, B.: Information technology-enabled team-based,
patient-centered care: the example of depression screening and management in cancer care.
Health Policy Technol. 6, 67–71 (2016)
10. Côrtes, P.L., Côrtes, E.G.D.P.: Hospital information systems: a study of electronic patient
records. J. Inf. Syst. Technol. Manag. JISTEM 8(1), 131–154 (2011)
11. Barbosa, J.G.P., Gonçalves, A.A., Simonetti, V., Leitão, A.R.: A proposed architecture for
implementing a knowledge management system in the Brazilian national cancer institute.
Braz. Adm. Rev. BAR 6(3), 246–262 (2009)
12. Tzeng, W., Kuo, K., Lin, H., Chen, T.: A socio-technical assessment of the success of
picture archiving and communication systems: the radiology technologist’s perspective.
BMC Med. Inf. Decis. Making 13, 109 (2013). https://doi.org/10.1186/1472-6947-13-109
13. Faggioni, L., Neri, E., Castellana, C., et al.: The future of PACS in healthcare enterprises.
Eur. J. Radiol. 78(2), 253–258 (2011)
14. Heerden, V.J., Lokhat, Z., Bam, D., Fletcher, L., Sommerville, J.: PACS: do clinical users
beneﬁt from it as a training adjunct? SA J. Radiol. 15, 38–41 (2011)
15. Chang, I.C., Hwang, H.G., Yen, D.C., Lian, J.W.: Critical factors for adopting PACS in
Taiwan: views of radiology department directors. Decis. Support Syst. 42(2), 1042–1053
(2006)
16. Dixon-Woods, M., Amalberti, R., Goodman, S., Bergman, B., Glaszio, P.: Problems and
promises of innovation: why healthcare needs to rethink its love/hate relationship with the
new. Qual. Saf. 20(Supl 1), i47–i51 (2011)
17. Adler, P.S., Heckscher, C., Prusak, L.: “Building a collaborative enterprise.” Harv. Bus. Rev.
89(7-8), 95–101 (2011)
18. Ekedahl, M., Wengstrom, Y.: Coping processes in a multidisciplinary healthcare team — a
comparison of nurses in cancer care and hospital chaplains. Eur. J. Cancer Care 17, 42–48
(2008)
19. O’Brien, D.M., Kaluzny, A.D., Sheps, C.G.: The role of a public-private partnership:
translating science to improve cancer care in the community. J. Healthc. Manag. 59(1), 17–29
(2014).
http://search.ebscohost.com/login.aspx?direct=true&db=c8h&AN=2012448224&
lang=pt-br&site=ehost-live
20. Abraham, C., Nishihara, E., Akiyama, M.: Transforming healthcare with information
technology in Japan: a review of policy, people, and progress. Int. J. Med. Inform. 80(3),
157–170 (2011)
21. Galligioni, E., Berloffa, F., Caffo, O., Tonazzolli, G., Ambrosini, G., Valduga, F., et al.:
Development and daily use of an electronic oncological patient record for the total
management of cancer patients: 7years’ experience. Ann. Oncol. 20, 349–352 (2009)
22. Cooper, D.R., Schindler, P.S.: Métodos de pesquisa em administração, 7th edn. Bookman,
Porto Alegre (2003)
Innovation Process in Cancer Treatment
873

The Higher-Order Spectra as a Tool
for Assessing the Progress in Rehabilitation
of Patients After Ischemic Brain Stroke
Ewaryst Tkacz(&), Zbigniew Budzianowski, and Wojciech Oleksy
Department of Biosensors and Biomedical Signals Processing,
Faculty of Biomedical Engineering, Technical University of Silesia,
Roosevelta 40-40a, 41-800 Zabrze, Poland
ewaryst.tkacz@polsl.pl
Abstract. This article explores the possibility of using the higher-order spectra
as a tool to determine the progress in rehabilitation of patients after ischemic
brain stroke (IBS). In order to evaluate the effectiveness of this tool patients’ heart
rate variability (HRV) recordings obtained at 1, 10 and 60 day after stroke are
listed and compared to those recorded for healthy individuals. Each set of HRV
signals is processed with bispectral and bicoherent analysis. In each case three
statistical parameters are observed. The values of investigated parameters differ
for healthy people and these affected by the IBS and the rehabilitation progress is
visible based on the parameters changing their values towards these characteristic
for healthy individuals. The obtained results show usefulness of higher-order
spectra as a tool for assessment of the rehabilitation progress. Authors believe
that further work would greatly improve potential of the described tool, allowing
to increase its precision and expand the application ﬁeld.
Keywords: Heart rate variability  Higher-order statistics  Signal processing
1
Introduction
Recent decades have brought a number of changes in technology, aimed at facilitating
the activities of daily living. With the changing nature of work in many professions,
this resulted in a signiﬁcant reduction of physical activities. Especially in highly
developed countries. At the same time the speeding up pace of life resulted in increased
levels of stress and frequent use of a diet based on meals prepared and consumed in the
shortest possible period of time.
All these factors should be classiﬁed as raising the risk of stroke. Thus, it becomes
clear why the statistics show that this disease becomes an increasingly important
problem among nowadays societies. Currently, it is estimated that almost 6 million
people die every year as a result of a stroke. Even more suffer from permanent invalidity.
These numbers could be quite different had improved the diagnostic process. At the
moment diagnosis is mainly based on expensive computed tomography scanners. This
work focuses on the possibilities of using records from commonly used ECG Holter
monitors to detect and diagnose the course of recovery after Ischemic Stroke.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_83

2
Materials and Methods
2.1
A Description of the Data Used
The data used in following paragraphs was collected within the project 4T11E02425,
during recovery of patients after ischemic stroke.
The total number of patients under observation was equal to 23. In each case the
results from the ﬁrst, tenth and sixtieth day of convalescence were collected. These
records were used as the main material for further research.
As a point of reference authors used HRV signals stored in an online database
PhysioNet [1].
All data used as comparison was collected from the Normal Sinus Rhythm RR
Interval Database (nsr2db) [1]. This database includes beat annotation for long-term
ECG recordings of subjects in normal sinus rhythm.
2.2
Data Processing
In order to eliminate artifacts simple ﬁltering mechanism was applied. In case of each
long-term recording the average length of RR interval was calculated. Next, the length
of each RR interval in this recording was compared with the previously computed
average. If the length of interval was shorter than 80% or longer than 120% of the
average, such interval was rejected. It was assumed that all the intervals that remained
after ﬁltration were normal-to normal (NN) ones.
In case of each bispectral and bicoherent analysis authors used the ﬁrst 256 reg-
istered NN intervals, dividing them into 8 equal segments (each of 32 intervals) [2].
The analysis was performed using Matlab’s HOSA Toolbox [2]. Each analysis was
performed using the direct method [3, 4].
3
Calculation
Frequency analysis allows the separation of the individual components of the spectrum.
This type of study allows to detect cyclicality in change of the NN intervals length [5, 6].
The aim of the analysis in the frequency domain is to decompose the total vari-
ability of NN intervals into individual frequency components. The result is a plot of the
power spectrum as a function of frequency [7].
To evaluate the total spectra power of NN intervals variability the following
parameters are used [5]:
• ULF - ultra low frequency component (under 0.0033 Hz). The value expressed in
ms2
• VLF - very low frequency component (between 0.0033 to 0.04 Hz). The value
expressed in ms2
• LF - low frequency component (from 0.04 to 0.15 Hz). The value expressed in ms2
• HF - high frequency components (from 0.15 to 0.4 Hz). The value expressed in ms2
The Higher-Order Spectra as a Tool for Assessing the Progress
875

During the work it was decided not to treat ULF band as a separate one. Instead, it
is assumed that VLF band covers a range of 0  0.04 Hz.
The following Table 1 shows ﬁnal division into six regions of analysis based on
combinations of four sub-bands:
Due to the symmetry of the bispectrum [8, 9], regions 1, 3 and 6 are limited by a
diagonal of coordinates 0, 0, and 0.5, 0.5.
The ﬁnal distribution of the analyzed regions is illustrated below (Fig. 1).
In each of the six analyzed regions the maximum and average values of bispectrum
are calculated along with its variance. The results are presented in tabular form.
Table 1. Regions of bispectral analysis
Region Range
Band f1 [Hz] Band f2 [Hz]
1
VLF-VLF 0  0.04
0  0.04
2
LF-VLF
0.04  0.15
0  0.04
3
LF-LF
0.04  0.15
0.04  0.15
4
HF-VLF
0.15  0.4
0  0.04
5
HF-LF
0.15  0.4
0.04  0.15
6
HF-HF
0.15  0.4
0.15  0.4
Fig. 1. Distribution of the analyzed regions
876
E. Tkacz et al.

4
Results
The charts below show the values obtained during bispectral and bicoherent analyses of
HRV records. As mentioned before, there are four groups. Three of them are patients at
different stages of recovery after ischemic stroke. The last one is the reference (healthy
ones).
Objective of the study is to verify whether the rehabilitation progress is visible
based on the observed parameters changing their values towards these characteristic for
healthy people.
4.1
Bispectral Analyses
4.1.1
Comparison of the Maximum Values of the Bispectra
Observations
• There are four bands in which the maximum value of bispectrum recorded for the
reference group is lower than the values obtained from patients after ischemic
stroke. These bands are: VLF-VLF, HF-VLF, HF-LF, HF-HF.
• In each of these bands one can observe the same relationship. The values recorded
for groups after ischemic stroke decrease with the progress in rehabilitation:
– during day one maximum value of bispectrum varies from 200% (VLF-VLF
band) up to 500% (HF-HF band) of the value recorded for healthy individuals.
– during the sixtieth day it reaches values very close to those characteristic for
healthy people.
Fig. 2. Comparison of the maximum values of the bispectra – reference group and patients after
IBS
The Higher-Order Spectra as a Tool for Assessing the Progress
877

• In the LF-VLF band one can observe an opposite phenomenon. Result of the
patients during ﬁrst day of rehabilitation is lower than in case of healthy individuals,
but keeps increasing towards the reference value (Fig. 2).
4.1.2
Comparison of the Average Values of the Bispectra
In order to improve the readability of the chart the logarithmic scale was used (Fig. 3).
Observations
• Observing the mean values of bispectrum, one can notice that the process of shifting
towards reference threshold is present only in the VLF-VLF and HF-HF bands.
However only in case of the VLF-VLF band the value registered in the sixtieth day
is similar to the reference one.
4.1.3
Comparison of the Variances of the Bispectra
In order to improve the readability of the chart the logarithmic scale was used (Fig. 4).
Observations
• In this case one can observe four frequency bands in which the variance of the
patients after ischemic stroke changes towards threshold characteristic for healthy
people. These bands are: VLF-VLF, LF-VLF, HF-LF, HF-HF.
• In the bands VLF-VLF, HF-LF and HF-HF it’s easy to spot that the values recorded
during the ﬁrst day after ischemic stroke are clearly higher than in the case of
healthy individuals, but with the progress in rehabilitation they fall down towards
reference value.
Fig. 3. Comparison of the average values of the bispectra – reference group and patients after
IBS
878
E. Tkacz et al.

• In the case of the LF-VLF band an opposite phenomenon can be seen. Value
recorded in the ﬁrst day after stroke is signiﬁcantly lower than the result of a group
of healthy subjects, but
with
the
successive
convalescence
it’s growing,
approaching the reference value.
4.2
Bicoherent Analyses
4.2.1
Comparison of the Maximum Values of the Bicoherence
Observations
• In this case one can see only single region, in which the progress in rehabilitation
results in maximum value of bicoherence registered for patients after stroke
approaching reference threshold. It is the HF-HF band.
• In the LF-VLF band the maximum value of bicoherence increases towards the
reference value, however minimal change between tenth and sixtieth day suggests
that this level won’t be achieved (Fig. 5).
4.2.2
Comparison of the Average Values of the Bicoherence
Observations
• Neither of the analyzed bands allows for the simultaneous assessment of the pro-
gress in rehabilitation and separation of patients after ischemic stroke from the
reference group.
Fig. 4. Comparison of the variances of the bispectra – reference group and patients after IBS
The Higher-Order Spectra as a Tool for Assessing the Progress
879

• The LF-LF band shows constant change of results with the progress in rehabilitation
However, in this case, the value of the analyzed parameter does not come close to
the reference level. Contradictory, it’s moving away from it (Fig. 6).
Fig. 5. Comparison of the maximum values of the bicoherence – reference group and patients
after IBS
Fig. 6. Comparison of the average values of the bicoherence – reference group and patients after
IBS
880
E. Tkacz et al.

4.2.3
Comparison of the Variances of the Bicoherence
Observations
• Neither of the analyzed bands allows for the simultaneous assessment of the pro-
gress in rehabilitation and separation of patients after ischemic stroke from the
reference group.
• The only band that shows constant change of results with the progress in rehabil-
itation is VLF-VLF one. However, in this case, the value of the analyzed parameter
does not come close to the reference level. Contradictory, it’s moving away from it
(Fig. 7).
5
Discussion
5.1
Bispectral Analysis - Summary
Comparison of the results of the analysis of patients after ischemic stroke and healthy
individuals showed promising results.
It is possible to distinguish the registration performed in each group. Thus, the
assessment of the progress in rehabilitation of patients with ischemic stroke is also
possible.
Putting together the obtained results, it seems that the best method to do this is to
compare the maximum values of bispectrum in the VLF-VLF, HF-VLF, HF-LF and
HF-HF bands. In all these cases each value recorded for patients with ischemic stroke
was higher than for healthy people and (with the progress in rehabilitation) was
approaching the reference level.
Fig. 7. Comparison of the variances of the bicoherence – reference group and patients after IBS
The Higher-Order Spectra as a Tool for Assessing the Progress
881

It is also possible to use the average value of bispectrum and its variance, however,
the analysis based on these two parameters tend to be less obvious.
5.2
Bicoherent Analysis - Summary
Comparison of the results of the groups suffering from the ischemic stroke and healthy
subjects has shown that the bicoherence analysis is also able to distinguish the regis-
tration performed in each group and assess the progress in rehabilitation.
To be able to simultaneously separate patients after ischemic stroke form the ref-
erence group and assess the progress in rehabilitation one should focus on HF-HF band
and compare the maximum values of bicoherence.
Results coming from the comparison of the average or variance of bicoherence
don’t allow for the similar effect.
6
Conclusions
As the result of the work the analysis of the suitability of the higher-order spectra to
assess the progress in rehabilitation of patients after ischemic stroke was presented.
It has been shown that the use of higher-order spectra as a diagnostic tool enables
efﬁcient identiﬁcation of the period of recovery after ischemic stroke. Thus, this
analysis allows to separate the individual registrations on the basis of changes speciﬁc
to the progressive healing process. This is equivalent to the assessment of the progress
of the rehabilitation.
References
1. PhysioNet. http://physionet.org/. Accessed 5 Mar 2015
2. Swami, A., Mendel, J., Chrysostomos, L.: Higher-Order Spectral Analysis Toolbox: For Use
with MATLAB. The MathWorks, Inc. (1993)
3. Goshvarpour, A., et al.: Comparison of higher order spectra in heart rate signals during two
techniques of meditation: Chi and Kundalini meditation. Cogn. Neurodyn. 7(1), 39–46 (2013)
4. Chua, C.: Analysis of cardiac and epileptic signals using higher order spectra, Praca
doktorska. Queensland University of Technology, Queensland (2010)
5. Krauze, T., Guzik, P., Wysocki, H.: Zmienność rytmu serca: aspekty techniczne. Nowiny
Lekarskie 9(70), 973–984 (2001)
6. Mazur, P., Pﬁtzner, R., Matusik, P.: Analiza parametrów częstotliwościowych zmienności
rytmu serca po pomostowaniu aortalno-wieńcowym. Folia Cardiologica 6(1), 76–81 (2011)
7. Kłopocka, M., Budzyński, J., Bujak, R., Świątkowski, M., Sinkiewicz, W., Ziółkowski, M.:
Dobowa zmienność rytmu zatokowego serca jako wskaźnik aktywności autonomicznego
układu nerwowego u mężczyzn z zespołem zależności alkoholowej w okresie abstynencji.
Alkoholizm i Narkomania 13(4), 491–501 (2000)
8. Jouny, I., Moses, R.: The bispectrum of complex signals: deﬁnitions and properties. IEEE
Trans. Signal Process. 40(11), 2833–2836 (1992)
9. Saliu, S., Birand, A., Kudaiberdieva, G.: Bispectral Analysis of Heart Rate Variability Signal
(2002)
882
E. Tkacz et al.

Developing and Testing an Application
to Assess the Impact of Smartphone Usage
on Well-Being and Performance Outcomes
of Student-Athletes
Poppy DesClouds1, Fedwa Laamarti2, Natalie Durand-Bush1,
and Abdulmotaleb El Saddik2(&)
1 SEWPLab, School of Human Kinetics, University of Ottawa, Ottawa, Canada
{poppy.desclouds,ndbush}@uottawa.ca
2 MCRLab, School of Electrical Engineering and Computer Science,
University of Ottawa, Ottawa, Canada
{ﬂaamart,elsaddik}@uottawa.ca
Abstract. This paper outlines the development and testing of a novel mobile
research application, which was speciﬁcally conceptualized and produced for
sport psychology research assessing the prevalence and impact of student-
athletes’ smartphone usage. Research examining athletes’ use of smartphones
within and related to sport is scarce. As well, the research pertaining to
smartphone usage in similar domains (i.e., physical activity and exercise), has
relied heavily on retrospective, self-report data to determine smartphone usage
prevalence. This study takes a step toward leveraging the smartphone as a sport
psychology research tool, and toward recognizing the potential impact of
smartphone usage in the sport domain. Sport psychology and computer science
researchers collaborated to conceptualize, develop, and test a specialized mobile
application that remotely tracks objective, detailed, smartphone usage, in
real-time, via the personal smartphones of ﬁve (n = 5) student-athlete partici-
pants. The application also allows for efﬁcient administration, completion, and
storage of demographic and self-report surveys on key psychosocial, behavioral,
and performance variables in student athletics. The results of this pilot study
highlight preliminary smartphone usage trends of student-athletes and some key
psychosocial variables related to their well-being. As well, the results under-
score the capabilities of the research app to be used as a sport psychology
assessment tool in an upcoming longitudinal prospective study with 500 varsity
athletes.
Keywords: Smartphones  Mobile application  Sport psychology
Student-athletes
1
Introduction
We have witnessed an explosive growth in the use of smartphones in contemporary
society. The ‘iGeneration’ (iGen), or those born in the 1990s and beyond [1], are
signiﬁcant users of these technologies for interpersonal communication, self-expression,
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_84

and media multitasking [2–4]. In every area of modern life, this generation has been
privy to the presence of mobile media and mobile access to the Internet [4–6]. Among
Canadian youth, e-mail, text messaging, and social networking have become dominant
forms of interpersonal communication and motives for using smartphones. A recent
study by Bentley et al. [7] showed that young people interacted with their smartphones
for an average of three hours per day and were drawn to use their phones in ‘any spare
second’, especially during waiting or ‘unscheduled’ time. Many in the 18-to-34-year
demographic report going no more than one hour without checking their cellphones, and
they engage in this sporadic checking in practically any environment, including when
they wake up in the middle of the night, during meals with others, and while going to the
washroom [6]. Moreover, the iGen represents a group of heavy ‘media-multitaskers’,
that is, they use several devices as well as features of those devices to perform multiple
tasks at once [1].
Athletes are important and unique members of the iGen. Despite the overall express
research interest in the use of smartphones and social media, there are only a handful of
studies in which athletes’ use of smartphones have been examined. Athletes’ use of
these devices is of interest because they must consistently perform at a high level under
pressure and rely on well-reﬁned psychosocial skills that can be impacted by smart-
phone usage. However, their capacity to self-manage optimally may be directly
impacted by the presence and prevalent use of smartphones. Of particular interest are
varsity athletes, who must play the dual role of student and athlete while coping with
strenuous physical and mental demands and expectations [8, 9]. The demands put on
student-athletes have the potential to induce stress, burnout, and illness [10, 11, 13, 14].
However, some researchers have found that stress and burnout may be buffered by
sound self-regulation skills [8, 12, 15, 16].
Research has shown that there is potential for smartphones to facilitate a range of
self-regulatory behaviours [17–21]. This may be regarded as a positive implication of
smartphone usage, particularly because self-regulatory behaviors have also been pos-
itively associated with learning, development, and performance in sport [22–27].
However, the literature also exposes the potential for smartphones to promote addictive
and problematic behaviours [28–30], disrupt attention and executive function [1, 2, 31,
32], cause anxiety [1, 33, 34], induce mental health concerns [35–37], and cause
deﬁcits in task performance [33, 38]. The mere presence of a smartphone is distracting
enough to thwart self-regulation capacity for some individuals and cause detriments to
attention and performance [32]. The potential for distractions and setbacks is high if
athletes’ self-control becomes hindered in some way (e.g., cannot resist temptation to
check phone) [8]. Furthermore, there is potential for well-being to be hindered if
athletes cannot maintain an optimal capacity to self-regulate and perceive growth and
development toward their performance goals.
Drawing on the literature that is available, it is clear that there are both beneﬁts and
drawbacks of smartphone usage. Unfortunately, evidence regarding the impact of
mobile technology in the context of sport is scarce. However, based on the various
aforementioned effects of smartphones identiﬁed in other domains, one could
hypothesize that varsity athletes’ use of smartphones could equally help or hinder their
capacity to self-regulate and maintain optimal levels of well-being and performance.
Given the deﬁcit of research related to the use of smartphones in the athlete population,
884
P. DesClouds et al.

and given the important inﬂuence usage may have on their performance and devel-
opment, studies in this ﬁeld are warranted.
Another important topic to address in relation to smartphone usage is social media,
which is a signiﬁcant motive of the iGen’s use of smartphones. In a recent study, 16%
of Ontario students reported spending more than ﬁve hours per day on social media
[39]. It is no wonder then, that there is research interest in the ways social media is
impacting the psychosocial functioning and development of young people [33, 34, 37,
38]. Although empirical studies regarding social media tend to focus speciﬁcally on the
platform itself (i.e., Facebook, Instagram, Tinder), without explicitly referring to where
or how the social media was accessed (i.e., via smartphone), results show that young
people are increasingly using smartphones for social media purposes [30]. Notably, a
recent study [40] showed that Facebook use was signiﬁcantly correlated with con-
centration disruption in athletes. Thus, the literature also points to the importance of
understanding the particularly impact of social media usage via smartphones. It is
pertinent to achieve clearer understanding of which applications have the most sig-
niﬁcant impact in the sport environment, and why.
To better and more accurately understand athletes’ smartphone usage, researchers
from the University of Ottawa’s SEWP (Self-Regulation for the Enhancement of
Well-Being) Laboratory and MCRLab (Multimedia Communications Research) Lab-
oratory collaborated to create a novel mobile application. The use of apps in research
been shown to be valid, reliable, and feasible [41]. Therefore, together, these
researchers are taking the ﬁrst step toward ﬁlling an important knowledge gap and
leveraging the use of smartphones in sport psychology research and assessment.
2
Method
2.1
Purpose
This paper addresses the development and testing of a mobile research application, and
the preliminary results emerging from the data collected during a one-month pilot study
carried out with ﬁve student-athletes. There were two main objectives of the pilot
study: (1) to test the functionality of the mobile application as a remote, efﬁcient, and
objective data collection tool; and (2) to collect and analyze preliminary data related to
student-athlete smartphone usage, and key psychosocial variables related to attention,
awareness, mental health, and well-being. The pilot study also afforded us the
opportunity to subject the data to preliminary algorithms, which were created to extract
detailed statistical information from a large pool of data.
The mobile application was conceptualized for a prospective longitudinal study
with 500 Canadian varsity athletes. Although this longitudinal study has not yet
commenced, the purpose of the large-scale project is to (1) understand athletes’ lived
experiences with smartphones, (2) assess trends in athletes’ usage and determine if
usage impacts sport performance and development, and (3) examine how athletes
employ social media to self-manage and self-present in their sport. Results of this
large-scale study will provide evidence regarding the effectiveness and feasibility of
using a mobile app to carry out sport psychology research. Most importantly, this
Developing and Testing an Application to Assess the Impact of Smartphone Usage
885

project will lay the groundwork for developing more sophisticated research programs
addressing emerging technologies in sport.
Seven participants initially took part in the pilot study. Two of them dropped out,
thus, ﬁve participants (1 = male, 4 = female) are included in the pilot study results. All
ﬁve participants were student-athletes from the University of Ottawa, competing at
various competitive levels in basketball, ultimate Frisbee, track and ﬁeld, and ﬂag
football.
2.2
The Mobile Application
The mobile application, of which the system architecture is depicted in Fig. 1, was
purposefully developed to automatically track varsity athletes’ actual and perceived
smartphone usage, as well as their perceived level of sport performance and devel-
opment. Athletes interested in participating in the study were given a password by the
researchers, allowing them to download and access the app on their smartphone and
complete a consent form. Once they provided their consent, the app automatically
began to covertly collect usage data in the background of their regular smartphone
activity. It periodically uploaded this data to a secure web server, with minimal impact
on battery and data usage, when the participant was securely connected to wiﬁ. The app
also automatically prompted the athletes to complete a self-report survey, with
reminders, as necessary. The protocol for the pilot study mirrored that of the anticipated
8-month longitudinal study, but was condensed to be implemented over a one-month
period.
The researchers aimed to create a minimally invasive app that did not constrain or
inﬂuence athletes’ regular smartphone usage. The app thus runs covertly, in the
‘background’ of participants’ regular smartphone usage, and can be silenced, deleted,
and re-installed, just like any other mobile application. Several steps were followed to
ensure that participants’ self-reported perception of usage would not unduly inﬂuence
their actual regular usage trends. First, participants were asked to self-report on both
Fig. 1. System architecture
886
P. DesClouds et al.

their positive and negative perceptions of smartphone usage, so as to not inadvertently
sway their focus toward one or the other. Second, social desirability items were
included within the self-report survey in order to assess and control for this charac-
teristic in the participant sample. Third, usage data was not provided to participants
during the research period.
The fact that the app captures both self-report and real-time data is indeed a unique
feature and beneﬁt [42]. It was surprising to ﬁnd that most of the studies on the impact
of smartphone usage rely solely on self-report data [43]. Self-report data are retro-
spective and subject to over-reporting. Since youth and young adults’ use of smart-
phones is sporadic and nuanced [7], it is highly unlikely that they can accurately
remember to what extent they use their mobile device throughout the day. Additional
beneﬁts of the app are that it reduces participant burden and optimizes response rates by
providing all surveys remotely, allowing immediate upload of responses to a secure
server, and mirroring the sporadic nature of smartphone usage by allowing participants
to complete surveys at their own pace.
2.3
Measures
Data Collected via the Mobile App. The mobile application captures usage (i.e.,
types, frequency, duration of usage of all features and applications on an individual’s
smartphone, including those related to text messages, emails, phone calls, video games,
social media, photos, videos, music). Of note, the application does not track the content
within applications so as to not violate participant privacy. Once athletes upload the
app to their phone (accessed with a study-speciﬁc password) and provide consent via
the application, they are prompted, at one time-point only, to complete a demographic
questionnaire. Using the mobile app, this questionnaire asks them to provide personal
(e.g., age, gender, ethnicity), academic and sport-speciﬁc information (e.g., institution,
type of sport, year of eligibility), and basic smartphone usage information (e.g.,
imposed time restrictions, contexts in which smartphones are used, perceptions of
personal smartphone usage). Once participants consent, automatic tracking commences
and access to a monthly self-report survey becomes available. The self-report survey
assesses psychosocial, behavioral, and sport performance variables that are important in
competitive sport and possibly impacted by smartphone usage. The following measures
are included in the mobile survey: (1) the Self-Regulation for the Enhancement of
Performance and Well-Being Scale (SEWP-S) [44]; (2) the Cognitive Affective
Mindfulness Scale-Revised (CAMS-R) [45]; (3) the Perfectionistic Self-Presentation
Scale (PSPS) [46]; (4) The Modiﬁed Self-Presentation Tactics Scale (social-media
speciﬁc) [47]; (5) the Perceived Stress Scale [49]; (6) the New General Self-Efﬁcacy
Scale [50]; (7) the Mental Health Continuum – Short Form (MHC-SF) [51]; (8) the
6-Item De Jong Gierveld Loneliness Scale [52]; and (9) the Communications Skills
Assessment (COMSA-R2) [53].
The participants complete the survey within the last two weeks of every month.
Although the survey requires approximately 15 to 20 min to complete, athletes can do
this at their own leisure and respond to the questions in multiple sittings. Although
items are the same across the eight surveys, the order varies from survey to survey to
Developing and Testing an Application to Assess the Impact of Smartphone Usage
887

reduce a potential learning effect over time. Each athlete’s usage and self-report survey
data is automatically uploaded to a secure web server that was designed for this study.
Data Reported for the Pilot Study. The pilot study focused on testing the range of
tracking ability and accuracy of the mobile app, and was used to glean preliminary
results from the small participant pool. From the signiﬁcant amount of usage data
collected during the pilot period, the researchers focused on reporting total smartphone
usage, usage based on time of day, the most frequently used apps, and mean scores
related to athlete well-being.
Automatic Objective Tracking Measures. The total smartphone usage for each par-
ticipant throughout a 15-day period during which all ﬁve participants responded to the
survey was assessed, as well as the percentage of usage that took place during the
following 4 time points: morning (6 am–12 pm), afternoon (12 pm–6 pm), evening
(6 pm–12 am), and overnight (12 am–6 am). The top 3 applications, used for the
greatest amount of time were identiﬁed, as well as the total usage time for each
application.
Mindfulness. The CAMS-R [45] is a 12-item measure used to evaluate four domains of
mindfulness: attention, present-focus, awareness, and acceptance. The CAMS-R shows
convergent and discriminant validity, and has been replicated in student, community,
and clinical samples [45]. For the purposes of this study, one additional item was
added: “I pay attention to sensations in my body”. This item targets participants’
awareness of bodily sensations - an important element of sport performance [15].
Mental Health. The MHC-SF [51] is a 14-item measure of mental health used to assess
three subscales of emotional, psychological, and social well-being. The MHC-SF is a
psychometrically sound measure [54].
Self-regulation. The SEWP-S [44] is a 16-item measure of self-regulation capacity
informed by Zimmerman’s model of self-regulated learning [48]. It aims to evaluate
three subscales of self-regulation: preparation, performance, and evaluation. As the
SEWP-S is a new scale, basic psychometric properties have yet to be assessed.
3
Results
Participants’ total smartphone usage over 15 days ranged from 20.5 h to 119.4 h, with
an average usage of 4.5 h per day and 31.7 h per week. Average smartphone usage
among participants (31.7 h per week) far surpassed their average self-reported time
spent studying (20 h per week) and training for sport (11.4 h per week). The partici-
pants’ perceived smartphone usage varied in accuracy, but 4 out of 5 participants’
self-reports were lower than their actual usage. Notably, the participant with the highest
real-time usage (55.7 h per week), accurately reported his smartphone usage as being
approximately 50 h per week.
The participants’ usage was spread throughout the day, peaking either in the
morning (between 6 am and 12 pm) or overnight (between 12 pm and 6 am). Four of
the participants’ peak usage period accounted for more than 40% of their total usage
888
P. DesClouds et al.

(see Fig. 1). One participant’s peak usage period was overnight (47.8 h; 45% of total
usage). Coincidentally, this athlete was the only one who reported a mental illness
diagnosis.
Four out of the ﬁve participants’ most frequently used application involved social
media (i.e., Snapchat, Instagram, and Facebook). These participants spent at least 7 h
more on their social media application than their next-most-used application. One
participants’ social media usage time was 19 h more than his next-most-used applica-
tion. All participants had at least one social media application in their top three most
used apps. Moreover, each participant’s top three most-used applications accounted for
the majority of their usage per week (see Fig. 2). Table 1 provides a summary of the
usage percentages of each participant’s top three most-used apps versus other appli-
cations, as well as total smartphone usage per week (across all apps) for each participant.
In general, the ﬁve participants reported moderate to high self-regulation capacity,
mindfulness, and mental health. All but one of them was categorized as ‘ﬂourishing’
based on the mental health scale. Although nuances can be observed across ﬁve
Fig. 2. Prevalence of smartphone usage throughout four periods of the day, morning (6 am–
12 pm), afternoon (12 pm–6 pm), evening (6 pm–12 am), and overnight (12 am–6 am).
Table 1. App usage distribution
Student % Top 3 apps % Other apps Total hours/week
1
80.50
19.50
11.79
2
59.04
40.96
9.56
3
60.95
39.05
55.7
4
59.16
40.84
49.32
5
75.44
24.56
32.12
Developing and Testing an Application to Assess the Impact of Smartphone Usage
889

participants, one in particular exhibited lower scores on all three measures (see Fig. 3).
Scores for self-regulation capacity, mindfulness, and mental health followed a similar
pattern for each participant, respectively, showing that if one of the three scores were
low, moderate, or high, the other two scores followed suit. The two participants with
the highest smartphone usage reported elevated scores for mental health, mindfulness,
and self-regulation capacity, while the participant with the lowest scores on these thee
measures also had the lowest smartphone usage.
In terms of results pertaining to the mobile app itself, technical modiﬁcations were
made as a result of the pilot testing process. The researchers were able to ﬂag technical
glitches and optimize usability according to participant feedback (e.g., minimize
keyboard after use, re-install the app when get a new phone, modify registration screen
to easily register and provide consent). Participants did not report any signiﬁcant time
burdens or usability concerns with the mobile survey.
The mobile survey is meant to uncover changes in capacity, perception, and
experience over time. As such, the app was tweaked so that participants cannot
complete two self-report surveys for two different time points back-to-back. The app
now restricts survey availability so that it is only accessible for a period of 15 days at
the end of each month, after which participants have a 2-week break before being
prompted to report again.
Furthermore, results showed that the app generates a signiﬁcant amount of data
requiring many calculations to assess phone usage. As such, the researchers have
developed algorithms to analyze the data and will merge two different Android tracking
usage Application Programming Interfaces (APIs) to extract data.
0
50
100
1
2
3
4
5
Percentage of Usage of Top 3 Apps
Top 3 apps
Other Apps
Fig. 3. Percentage of usage of top 3 apps (per week)
890
P. DesClouds et al.

4
Discussion
The purpose of this study was to test a sport psychology mobile application and report
preliminary results emerging from the one-month pilot phase carried out with ﬁve
university student-athletes. Overall, smartphone usage among the ﬁve student-athletes
was high but individualized, as depicted in Fig. 4. Furthermore, social media apps
accounted for a staggering percentage of the participants’ usage. These ﬁndings are in
line with the literature in other domains demonstrating the prevalent use of smart-
phones in the 18-to-34-year demographic and the overwhelming presence of social
media [3, 4, 7, 30]. When examining differences between participants’ perceived and
actual smartphone usage, results suggest varying levels of awareness. This supports the
notion that actual usage among iGen members may be under-reported in self-report
research [43], and underscores the importance of measuring real-time data.
It is compelling to note that the ﬁve student-athletes’ average usage per week was
equivalent to more than one full day (31.7 h). This is interesting to observe among a
group of individuals who are performing multiple time-consuming demands, including
training for sport, attending university, and holding a part-time job. This weekly
average of 31.7 h far surpassed the average self-reported time studying per week (20 h)
and training for sport (11.4 h). This ﬁnding is in line with literature that proposes the
iGen has a propensity for multitasking [3], as there are not enough hours in the day to
engage in this much activity without relying on multitasking. Research on media- and
social media-multitasking has shown that engagement in these types of behaviours can
cause detriments to self-control, task performance, and goal achievement [2, 3, 38].
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0
1
2
3
4
5
Survey Score (% of total possible)
Participants
Mindfulness
Mental Health
Self-Regulation
Fig. 4. Individual survey scores (reported as % of total possible survey score for comparisons)
for mindfulness (CAMS-R), self-regulation (SEWP-S), and mental health (MHC-SF)
Developing and Testing an Application to Assess the Impact of Smartphone Usage
891

However, four out of the ﬁve participants in the pilot study reported moderate to high
self-regulation capacity, mindfulness, and mental health, which contradicts previous
ﬁndings. Nonetheless, the ﬁfth participant’s excessive overnight smartphone usage,
lower scores on the three psychosocial measures, and self-reported mental illness
diagnosis raise concern. This ﬁnding does ﬁt with previous ﬁndings that suggest
smartphone usage is linked to mental health concerns [33, 35, 36]. However, more
research is necessary to conﬁrm the interrelationships between the aforementioned
variables, which are all important in the context of competitive sport.
There were indeed challenges in developing and testing the sport psychology
research app, which required ﬁxing technical glitches and providing options to respect
ethical research guidelines (i.e., allow participants to turn off notiﬁcations and close the
app completely). Future researchers endeavouring to develop similar mobile applica-
tions stand to learn from the results of this pilot study. There is a ﬁne line between
essential research data and breach of privacy. In spite of the difﬁculties encountered, it
has afforded the researchers many beneﬁts. The study demonstrates that it is possible to
successfully collect objective, nuanced, real-time usage data, which can differ from
self-reported usage. It shows the smartphone’s potential as a research tool that not only
appeals to the iGen, but also reduces participant burden and optimizes response rates.
The pilot study also provided the opportunity to subject the smartphone usage data to
preliminary algorithms, which were created to extract detailed statistical information
from a large pool of data.
Overall, results will enhance the efﬁciency, accuracy, trustworthiness, and focus of
the large-scale longitudinal study that will be carried out in the near future.
Moving forward, researchers in the sport and computer science domains are
encouraged to investigate the prevalence and trends of athletes’ smartphone usage, and
build a database depicting the variety of smartphone features and functions that may be
leveraged to facilitate positive sport and well-being outcomes for athletes. Considering
that smartphone usage and social media appear to go hand-in-hand, it is necessary to
study both together to gain a full and rich understanding of the psychosocial impact of
smartphone usage. If researchers do not consider how athletes access and use social
media, it becomes difﬁcult to draw clear, evidence-based guidelines around optimal
smartphone usage.
4.1
Limitations
This study provided noteworthy ﬁndings to inform further research. Nonetheless, some
limitations must be acknowledged. First, the sample was small and precluded the
researchers from running statistical analyses to determine the signiﬁcance of relation-
ships between measures. As such, the study only provides a descriptive ‘snap shot’ of
athletes’ smartphone usage and level of well-being. Secondly, the research app is only
available for Android phones at the moment, so participants with iOS devices, who
could potentially exhibit very different usage proﬁles, were excluded from participating.
892
P. DesClouds et al.

5
Conclusion
Smartphones, particularly social media apps, were highly used by this sample of
university student-athletes. Usage was most prevalent during the morning and over-
night. Participants generally under-reported usage, when compared to the real-time data
collected. Beneﬁts of the mobile app developed for this research include the automatic,
non-invasive, longitudinal tracking of objective usage data and the administration of a
user-friendly mobile survey. The mobile app shows great promise as it limits partici-
pant burden and mirrors the sporadic usage patterns of the iGen.
This is the ﬁrst known study to track and explore the use of smartphones in a
sample of athletes. It will inform a forthcoming large-scale study with 500 athletes that
will: (a) ﬁll a gap in the literature pertaining to the use of mobile technology in sport,
(b) fulﬁll the growing need for robust remote data collection methods (apps, mobile
surveys, web servers), and (c) propose avenues for new collaborations and further
interdisciplinary research. From a practical perspective, ﬁndings from this research will
help to inform strategies to leverage the use of smartphones to optimize performance in
sport.
References
1. Rosen, L., Samuel, A.: Conquering digital distraction. Harvard Bus. Rev. 93(6), 110–113
(2015)
2. Magen, H.: The relations between executive functions, media multitasking and polychronic-
ity. Comput. Hum. Behav. 67, 1–9 (2017)
3. Rosen, L.D., Carrier, L.M., Cheever, N.A.: Facebook and texting made me do it:
media-induced task-switching while studying. Comput. Hum. Behav. 29, 948–958 (2013)
4. Steeves, V.: Young Canadians in a Wired World, Phase III: Life Online. MediaSmarts,
Ottawa (2014). http://mediasmarts.ca/ycww
5. Collins, L., Ellis, S.R.: Mobile Devices: Tools and Technologies. CRC Press, Boca Raton
(2015)
6. Lookout Mobile Security: Mobile Mindset Study. Lookout, Inc., San Francisco (2012)
7. Bentley, F., Church, K., Harrison, B., Lyons, K., Rafalow, M.: Three hours a day:
understanding current teen practices of smartphone application use. arXiv:1510.05192
(2015)
8. Dubuc-Charbonneau, N., Durand-Bush, N.: Moving to action: the effects of a self-regulation
intervention on the stress, burnout, well-being, and self-regulation capacity levels of
university student-athletes. J. Clin. Sport Psychol. 9, 173–192 (2015)
9. Watt, S., Moore, J., Howard-Hamilton, M.: Who are student-athletes? New Dir. Stud. Serv.
2001(93), 7–18 (2001)
10. Adlaf, E.M., Demers, A., Gliksman, L.: Canadian Campus Survey 2004. Centre for
Addiction and Mental Health, Toronto (2005)
11. Dubuc-Charbonneau, N., Durand-Bush, N., Forneris, T.: Exploring levels of student athlete
burnout at two Canadian Universities. Can. J. High. Educ. 44(2), 135–151 (2014)
12. Hofer, J., Busch, H., Kärtner, J.: Self-regulation and well-being: the inﬂuence of identity and
motives. Eur. J. Pers. 25, 211–224 (2011)
13. Gould, D., Whitley, M.A.: Sources and consequences of athletic burnout among college
athletes. J. Intercoll. Sports 2, 16–30 (2009)
Developing and Testing an Application to Assess the Impact of Smartphone Usage
893

14. Watson, J.C., Kissinger, D.B.: Athletic participation and wellness: implications for
counseling college student-athletes. J. Coll. Couns. 10(2), 153–162 (2007)
15. Durand-Bush, N., McNeil, K., Harding, M., Dobransky, J.: Investigating stress, psycholog-
ical well-being, mental health functioning, and self-regulation capacity among university
undergraduate students: is this population optimally functioning? Can. J. Couns. Psychother.
49(3), 253–274 (2015)
16. Park, C.L., Edmondson, D., Lee, J.: Development of self-regulation abilities as predictors of
psychological adjustment across the ﬁrst year of college. J. Adult Dev. 19, 40–49 (2012)
17. Angster, A., Frank, M., Lester, D.: An exploratory study of students’ use of cell phones,
texting, and social networking sites. Psychol. Rep. 107(2), 402–404 (2010)
18. Green, E., Singleton, C.: Mobile connections: an exploration of the place of mobile phones
in friendship relations. Sociol. Rev. 57(1), 125–144 (2009)
19. Ling, R.: Mobile communications vis-à-vis teen emancipation, peer group integration and
deviance. In: Harper, R., Taylor, A., Palen, L. (eds.) The Inside Text: Social Perspectives on
SMS in the Mobile Age, pp. 175–189. Springer, Norwell (2005). https://doi.org/10.1007/1-
4020-3060-6_10
20. Rau, P.L.P., Gao, Q., Wu, L.M.: Using mobile communication technology in high school
education: motivation, pressure, and learning performance. Comput. Educ. 50, 1–22 (2008)
21. Young, N.: The Virtual Self: How Our Digital Lives are Altering the World Around Us.
McClelland & Stewart, Toronto (2012)
22. Cleary, T.J., Zimmerman, B.J.: Self-regulation differences during athletic practice by
experts, non-experts, and novices. J. Appl. Sport Psychol. 13, 185–206 (2001)
23. Collins, J., Durand-Bush, N.: Enhancing the cohesion and performance of an elite curling
team through a self-regulation intervention. Int. J. Sports Sci. Coach. 5(3), 343–362 (2010)
24. Goudas, M., Kolovelonis, A., Dermitzaki, I.: Implementation of self-regulation interventions
in physical education and sports contexts. In: Bembenutty, H., Cleary, T., Kitsantas, A.
(eds.) Applications of Self-regulated Learning across Diverse Disciplines: A Tribute to
Barry J. Zimmerman, pp. 383–416. Information Age, Greenwich (2013)
25. Jonker, L., Elferink-Gemser, M.T., Visscher, C.: Differences in self-regulatory skills among
talented athletes: the signiﬁcance of competitive level and type of sport. J. Sports Sci. 28(8),
901–908 (2010)
26. Kermarrec, G., Pasco, D.: Self-regulation, training and performance. In: Chang, C. (ed.)
Handbook of Sports Psychology, pp. 297–313. Nova Science Publishers, New York (2009)
27. Toering, T., Elferink-Gemser, M., Jordet, G., Jorna, C., Pepping, G.J., Visscher, C.:
Self-regulation of practice behavior among elite youth soccer players: an exploratory
observation study. J. Appl. Sport Psychol. 23, 110–128 (2011)
28. Elhai, J.D., Levine, J.C., Dvorak, R.D., Hall, B.J.: Non-social features of smartphone use are
most related to depression, anxiety and problematic smartphone use. Comput. Hum. Behav.
69, 75–82 (2017)
29. Roberts, J.A., Yaya, L., Manolis, C.: The invisible addiction: cell-phone activities and
addiction among male and female college students. J. Behav. Addict. 3(4), 254–265 (2014)
30. Tzavela, E.C., Macromati, F.M.: Online social networking in adolescence: associations with
development, well-being and internet addictive behaviours. Int. J. Adolesc. Health 6(4),
411–420 (2013)
31. Barr, N., Pennycook, G., Stolz, J.A., Fugelsang, J.A.: The brain in your pocket: evidence
that smartphones are used to supplant thinking. Comput. Hum. Behav. 48, 473–480 (2015)
32. Thornton, B., Faires, A., Robbins, M., Rollins, E.: The mere presence of a cell phone may be
distracting: implications for attention and task performance. Soc. Psychol. 45(6), 479–488
(2014)
894
P. DesClouds et al.

33. Rosen, L.D., Whaling, K., Rab, S., Carrier, L.M., Cheever, N.A.: Is Facebook creating
“iDisorders”? The link between clinical symptoms of psychiatric disorders and technology
use, attitudes and anxiety. Comput. Hum. Behav. 29, 1243–1254 (2013)
34. Skierkowski, D., Wood, R.M.: To text or not to text? The importance of text messaging
among college-aged youth. Comput. Hum. Behav. 28, 744–756 (2012)
35. Elhai, J.D., Dvorak, R.D., Levine, J.C., Hall, B.J.: Problematic smartphone use: a conceptual
overview and systematic review of relations with anxiety and depression psychopathology.
J. Affect. Disord. 207, 251–259 (2017)
36. Knapton, S.: High-ﬂiers at risk of isolation and depression from internet addiction. The
Telegraph.
http://www.telegraph.co.uk/news/health/news/10557025/High-ﬂiers-at-risk-of-
isolation-and-depression-from-internet-addiction.html#disqus_thread. Accessed 15 Sep 2017
37. Oberst, U., Wegmann, E., Stodt, B., Brand, M., Chamarro, A.: Negative consequences from
heavy social networking in adolescents: the mediating role of fear of missing out. J. Adolesc.
55, 51–60 (2017)
38. Lau, W.F.: Effects of social media usage and social media multitasking on the academic
performance of university students. Comput. Hum. Behav. 68, 286–291 (2017)
39. Boak, A., Hamilton, H.A., Adlaf, E.M., Henderson, J.L., Mann, R.E.: The mental health and
well-being of Ontario students, 1991–2015: detailed OSDUHS ﬁndings (CAMH Research
Document Series No. 43). Centre for Addiction and Mental Health, Toronto (2015)
40. Encel, K., Mesango, C., Brown, H.: Facebook and its relationship with sport anxiety.
J. Sports Sci. 35(8), 756–761 (2017)
41. Heron, K.E., Smyth, J.M.: Ecological momentary interventions: incorporating mobile
technology into psychosocial and health behaviour treatments. Br. J. Health. Psychol. 15, 1–
39 (2011)
42. Holtz, B., Buis, L.: Effectively promoting healthy living and behaviours through mobile
phones. In: Ahmed, R., Bates, B.R. (eds.) Health Communication and Mass Media: An
Integrated Approach to Policy and Practice, pp. 99–114. Routledge, New York (2013)
43. DesClouds, P., Durand-Bush, N.: A scoping review: what is the impact of mobile
communication devices on youth in sport and physical activity settings? (2017, Manuscript
in preparation)
44. Durand-Bush, N., DesClouds, P.: Self-regulation for the Enhancement of Performance and
Well-Being (SEWP) Scale (2016, Unpublished)
45. Feldman, G., Hayes, A., Kumar, S., Greeson, J., Laurenceau, J.P.: Mindfulness and emotion
regulation: The development and initial validation of the cognitive and affective mindfulness
scale - revised (CAMS-R). J. Psychopathol. Behav. Assess. 29(3), 177–190 (2007)
46. Hewitt, P.L., Flett, G.L., Sherry, S.B., Habke, M., Parkin, M., Lam, R.W., et al.: The
interpersonal expression of perfection: perfectionistic self-presentation and psychological
distress. J. Pers. Soc. Psychol. 84(6), 1303–1325 (2003)
47. Lee, S., Quigley, B.M., Nesler, M.S., Corbett, A.B., Tedeschi, J.T.: Development of a
self-presentation tactics scale. Personality Individ. Differ. 26, 701–722 (1999)
48. Zimmerman, B.J.: Attaining self-regulation: A social cognitive perspective. In: Boekaerts,
M., Pintrich, P.R., Zeidner, M. (eds.) Handbook of Self-regulation, pp. 13–39. Academic
Press, San Diego (2000)
49. Cohen, S., Kamarak, T., Mermelstein, R.: A global measure of perceived stress. J. Health
Soc. Behav. 24(4), 385–396 (1983)
50. Chen, G., Gully, S.M., Eden, D.: Validation of a new general self-efﬁcacy scale. Organ. Res.
Methods 4(1), 62–83 (2001)
Developing and Testing an Application to Assess the Impact of Smartphone Usage
895

51. Keyes, C.L.M., Wissing, M., Potgieter, J.P., Temane, M., Kruger, A., van Rooy, S.:
Evaluation of the mental health continuum-short form (MHC-SF) in Setswana-speaking
South Africans. Clin. Psychol. Psychother. 15, 181–192 (2008)
52. Gierveld, J.D., Tilburg, T.V.: A 6-item scale for overall, emotional, and social loneliness:
conﬁrmatory tests on survey data. Res. Aging 28(5), 582–598 (2006)
53. COMSA-R2 (Communication Skills Assessment): Psychometric Report. PsychTests AIM,
Inc. (2014)
54. Lamers, S.M., Westerhof, G.J., Bohlmeijer, E.T., ten Klooster, P.M., Keyes, C.L.:
Evaluating the psychometric properties of the mental health continuum-short form
(MHC-SF). J. Clin. Psychol. 67(1), 99–110 (2011)
896
P. DesClouds et al.

Multisensory Virtual Game with Use
of the Device Leap Motion to Improve the Lack
of Attention in Children of 7–12 Years
with ADHD
David Chilcañán Capelo1(&), Milton Escobar Sánchez1,
Jhonatan Salazar Hurtado1, and Daniela Benalcázar Chicaiza2
1 Universidad de las Fuerzas Armadas ESPE, Av. General Rumiñahui s/n,
171-5-231B, Sangolquí, Ecuador
{dachilcannc,meesacobar1,jssalazar3}@espe.edu.ec
2 Universidad Técnica de Ambato UTA, Av. Los Chasquis y Río Payamino,
Huachi Chico, Ambato, Ecuador
da.benalcazar@uta.edu.ec
Abstract. The ADHD (Attention Deﬁcit and Hyperactivity Disorder) in the
child population affects between 5 and 10% and in adulthood even in the 60% of
the cases. It is characterized by a difﬁculty that has children to maintain the
voluntary attention with respect to proposed activities, both academic and
character to the day-to-day. This is coupled with the lack of impulse control. The
frequency of cases occurs in a proportion of 4:1, which is to say mostly among
children than among girls and may be suffering from both children and ado-
lescents and adults from all walks of social and cultural rights. That is why, by
resorting to the motivation to achieve attention of this speciﬁc group of children,
the proposal of this research is a multi-sensory virtual game using the Leap
Motion for that in a three-dimensional plane children have the opportunity to
manipulate the objects in the virtual environment to achieve concentration,
learning and motivation.
Keywords: Virtual game  Multisensory  Leap Motion device
Attention  Playful learning
ADHD (Attention Deﬁcit and Hyperactivity Disorder)
1
Introduction
Some children and young people at some point in their education show learning
problems, they need to readjust the teaching-learning process. Usually present with a
coefﬁcient of normal intelligence, and sometimes the academic problems are caused
because ADHD (Attention Deﬁcit and Hyperactivity Disorder) [1–4]. This is one of the
most diagnosed psychiatric disorders in childhood and adolescence [5], which starts
before the age of 7 years and can in a 75% of the cases persist into adulthood [6].
Little-studied group, which are often confused with the so-called hyperactive. It is
deﬁned as a behavioral syndrome, which can be caused by neurobiological components,
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_85

genetic, environmental and psychosocial [7]. It occurs in children, adolescents or adults,
and is characterized by the inability or difﬁculty maintaining attention, possessing at the
same time states of hyperactivity, which infers in cognitive development and postural
control [8]. This demonstrated the existence of a close relationship between the motor
clumsiness and ADHD [9]. It is pronounced with motor impairment during their pre-
school and school development that fails to perform complex movements and with the
speed required to start acquiring some academic content [10–12].
In this syndrome there are secondary symptoms associated with conduct disorders,
emotional difﬁculties and of the visuo-spatial skills-perceptual [1–4], which makes it
difﬁcult for the execution of ﬁne movements of the hand [8, 13], having an impact on
the learning and performance as students of the different subjects that students during
their student life.
The inﬂuence of video games on children is an area of research developed so far.
There is a theory that the dimensionality and the virtual reality beneﬁt the “immersion”
perceptual and emotional well-being of those who use it. What would the openness to
the possibility of creation of three-dimensional visually hypermedia materials and with
spatial sound. Technological-creative ideas useful for children or young adolescents
(students) with special educational needs [14, 15]. Video games provide a challenge to
the children or young adolescents who use them, favors the observation and analysis of
your environment, assimilating and retaining information, inductive and deductive
reasoning, constructing and applying cognitive strategies in an organized manner,
fostering the development of certain psychomotor skills (laterality, psychomotor
coordination, etc.). Children or young adolescents to play are forced to make decisions
and execute motor actions continuously, very important aspect for children and young
people with ADHD, since the game becomes an outlet for their stress [16].
This research proposes a multi-sensory virtual game for the development of ﬁne
motor skills of children or young adolescents aged 7–12 years, uses the technology
Leap Motion, which makes the interaction with the 3D space, detects the bones of the
human hand (skeletal tracking), develops, strengthens and improves the sustained
attention of children with ADHD mixed type making the obtaining, centralization and
conservation of the attention, by means of the Multisensory Teaching (touch, sight,
hearing and movement) using the graphic editor of Unity and the interaction of the
Leap Motion Controller.
The game conﬁgures the objects of the proposed game: blocks, cubes, spheres, and
the same containers that are created in advance in the implementation of graphic design
Blender. Are your modeling, lighting and rendering, importing them subsequently to
the virtual environment of Unity 3D for conﬁguration in the virtual environment.
Subsequently develops in the object-oriented language CSharp(C#) the scripts of
animation and connection: Manager of Interactions, Interaction behavior, Hand Con-
troller (Leap Head Controller) in order to achieve a more efﬁcient handling of your
customization and usability. Then generates an application (App) that is uploaded to
the server of applications Google Play Store for downloading and use of children or
childhood education centers that require it.
The article has been organized as follows: Sect. 2 describes the background and
related work that supports the research, in Sect. 3 we present the architecture of the
software used in Sect. 4 shows the execution of the Multisensory virtual game, while
898
D. C. Capelo et al.

Sect. 5 sets out the results and discussion. Finally, in Sect. 6, the end of the conclusions
based on the results gathered and describes future works.
2
Background and Relates Works
2.1
Blender
Cross-platform computer program of free software, dedicated to the modeling, ani-
mation and creation of 3D graphics [17], it also has an internal game engine, com-
patible for several operating systems (OS).
2.2
Microsoft Visual Studio
It is an integrated development environment (IDE), with a complete set of development
tools for building ASP.NET Web applications, XML Web services, desktop applica-
tions, and mobile applications [18].
2.3
Unity3D
3D graphics engine for developing cross-platform games. Compatible operating sys-
tems (OS) such as: Windows, GNU/Linux, MacOS, among other game platforms. This
tool is very well accepted in the community of developers, for his great set of tools that
are integrated in a quick, simple interface that is user-centered [19].
2.4
App Stores
The app stores today are very helpful to developers who want to make their software
products, so that users can download them to their computers or mobile devices,
making a payment or free of charge [20].
2.5
Leap Motion
Sensor for the capture and recognition of gestures that is made in the air, with ﬁngers or
hands, tracing a virtual image with a high resolution on your computer or mobile
device, allowing full control of three-dimensional objects and providing accurate
information [21].
2.6
Related Jobs
Regarding the issue, there are jobs that use the games as aid to learning and psy-
chomotor development with the use of device technology Leap Motion in interaction
with 3D space and with skeletal tracking methods to identify the bones of the human
hand and their movements [22]. There are also jobs related to the design and use of
games to support the treatment of attention of users with ADHD [23] in conjunction
with research methodologies for the development of interactive activities for the
treatment of ADHD. It should be noted that have not been taken into consideration the
Multisensory Virtual Game with Use of the Device Leap Motion
899

virtual learning oriented to the development of multi-sensory virtual games for the
treatment or improvement of patients with ADHD.
3
Architecture of the Software Used
This research starts by determining the architecture of the software required for the
design and development of a virtual game multisensory, which achieves the
improvement on the lack of care for children between 7–12 years old with ADHD of
mixed type. When performing 3D modeling and the use of the Leap Motion controller,
as shown in Fig. 1, there are eight components that are interrelated each with a speciﬁc
objective: (1) Leap Motion Controller, (2) Unity 3D Development platform, (3) IDE
Microsoft Visual Studio, (4) 3D Modeling Blender, (5) Executable, (6) Executable
multiplatform, (7) App Stores and (8) Children.
(1) Leap Motion Controller: The purpose of this component consists in interaction
and the movement that is being generated when using the virtual game, since the
device Leap Motion maps and capture the location of the hands using infrared
light with a wavelength of 850 nm by the three leds that has its hardware [24], the
application is displayed at the time of being taken in the hands the blocks, cubes,
spheres, and deposit them in their corresponding containers.
(2) Unity 3D Development platform: The purpose of this component is to relate the
objects
generated
in
the
IDE’s
(Integrated
Development
Environments).
Improving and correcting errors of both animation and texturing. Integrating
virtual reality technology, through dynamic simulation, real-time interaction.
Through the use of scripts is encoded in the Microsoft Visual Studio IDE.
Fig. 1. Architecture of the software used for the development of the virtual game multisensory
using Leap Motion.
900
D. C. Capelo et al.

Obtaining a cohesion of quality also allows you to perform the connectivity
between the application and the device Leap Motion.
(3) IDE Microsoft Visual Studio: This component facilitates the development of
functionalities (scripts) that are imported into the Unity 3D platform, allowing you
to decrease the margin of error from the sensors of the LeapMotion for obtaining
more accurate data in real time. This tool reduces the development time consid-
erably, thanks to the Autocomplete function and by having a direct connection
with Unity.
(4) 3D Modeling Blender: Through this program, it is possible to model the objects
that make up the interface of the application (3 spheres, 3 squares, 3 rectangles
and 3 containers) in addition to modeling, also makes the lighting, rendering and
animation. The modeling is based on the render engine that Blender has, this
provides a more realistic appearance to objects in any perspective resulting in a
better ﬁnish at the time of export to Unity.
(5) Executable: It is used when the application is in a ﬁnal version for your tests,
facilitating the distribution and installation in different compatible devices. The
purpose of this component is to use the application in different environments or
operating systems (multi-platform environments).
(6) Cross-platform: With this component you can run the application on a variety of
platforms with different operating systems (OS), achieving greater accessibility to
the east.
(7) App stores: With the help of this component will reach a more extensive popu-
lation not only at the national level, also at the international level, facilitating
accessibility to children, from any device.
(8) Children: For the obtaining of data, this component is the most important, so that
the data obtained are obtained thanks to the test carried out to a set of children
with ADHD, displaying the effectiveness of the application.
4
Implementation of the Multisensory Virtual Game Using
Leap Motion
The following is the implementation of the multi-sensory virtual game (cross-platform)
using the Leap Motion to improve the lack of sustained attention of children between
7–12 years of age who suffer from ADHD. As can be seen in the Fig. 2. The envi-
ronment consists of three containers of different colors (red, green, and yellow) and
nine geometric ﬁgures (3 spheres, 3 squares and 3 rectangles) three for each color. Each
of the containers must contain 3 geometric ﬁgures distinct one from the other by
keeping the color, both the container of the selected objects (geometric ﬁgures). The
child must interact with their hands, using the device Leap Motion and concentrating
on place each of the objects inside the containers, without crashing it into the attempt.
To ﬁll with success each of the containers the child must relate each of the colors, as
well as consider the missing ﬁgure corresponding to each of the containers.
Multisensory Virtual Game with Use of the Device Leap Motion
901

Concentration: to move each of the objects and bring success to each container, each
container has a regular size hole to increase a little the difﬁculty at the time of pour
objects, that is, the size of the hole is two times larger than the size of the object, in this
way, the child should focus more. Learning: relate objects in different ways but with
similar color to the container, this prevents the child put the geometric ﬁgures similar in
form, in a single container. Motivation: manipulate virtual objects with the hands
leaving to one side the input devices like the mouse or the keyboard, it is an experience
for each one of the children, to see wh they can get to play each object by means of his
hand virtualized through Leap Motion.
5
Results and Discussion
The multisensory virtual game using the Leap Motion was carried out with 20 children
aged 7 to 12 years, they formed two groups: one group of 10 children with ADHD and
another of 10 children without ADHD. This is intended to contrast the results to
evaluate the effectiveness of the game and device.
As can be seen in Table 1, describes the groups they belong to children who
participated in the virtual game mutisensorial using the Leap Motion.
Table 2 describes the groups that carried out three attempts and the time spent on
each one of them, there is also the time average and baseline as a reference for the
Fig. 2. Implementation of the Multisensory virtual game using Leap Motion
902
D. C. Capelo et al.

percentage of improvement with respect to the same. Both in the group of children with
ADHD as in the group of children without ADHD participated the same number of
girls and boys.
The tables above show us the comparison in the times to attempt with the times and
average times for a week. Monday, wednesday and friday are attempts in which par-
ticipants played. The average time of 16.56 min equivalent to 10.67% in children with
ADHD and a 13.54 min which is equivalent to 10.14% in children without ADHD, this
resulted in progress of 0.53% in the estimated time of one week, to 2.12% improve-
ment in the month and a 25.44% in the year in children with ADHD. These results
improve in accordance with the application time of the multi-sensory virtual game
using the Leap Motion in the participants.
With these data, it is noted that, in the three attempts made in the week was
obtained positive results, giving the opportunity to the participant with ADHD do not
require or tire too with manipulation of the game. This is necessary to avoid negative
results, as well as the abandonment of the game. On the contrary, with participants
without ADHD, the results did not vary signiﬁcantly, demonstrating that their degree of
concentration is in an adequate average, as shown in Fig. 3 below.
Table 1. Participants
Groups
Age range
Years
Participants
Children
Girls
Boys
Method
of assessment
Children with
ADHD
7–12
10
4
6
Playful learning with
Leap Motion
Children without
ADHD
7–12
10
4
6
Total
8
12
Table 2. Execution time of the game for attempts, with improvement
Groups
1°
Tried
Minutes
2°
Tried
Minutes
3°
Tried
Minutes
Average
time
Minutes
Time
Base
Minutes
Improvement
%
Children with
ADHD
17.10
16.50
16.10
16.56
15.52
10.67
Children without
ADHD
14.31
13.41
13.00
13.54
13.35
10.14
Multisensory Virtual Game with Use of the Device Leap Motion
903

6
Conclusions and Future Works
The multisensory virtual game proposed in this research helped that children feel more
relaxed at the time to interact with the game due to the Leap Motion device is easy to
use.
In this way, the children established a pattern matching at the time related objects in
different ways but with similar color to the container, as well as the handling of
three-dimensional models with the Leap Motion Controller in the instant that they
moved each of the objects to the suitable container, that which increase the level of
concentration of children unlike the usual games due to the fact that, each receptacle
has a hole the size necessary to only enter the corresponding object, important fact to
improve concentration in children with ADHD.
The game developed in this research was a big success and caused motivation in
children who suffer from ADHD. It was interesting and fun interaction with the Leap
Motion Controller especially when they observe how your hand is virtualized in a
three-dimensional plane that helped them to manipulate the objects in the virtual
environment.
How future work, we will seek to obtain a greater immersion of children with
ADHD, with the use of the virtual reality haptic device using the HTC VIVE with
which it is projected improvement in the process of sustained concentration. It is also
intended to implement new scenarios with new activities by incorporating a custom
wizard to guide children while the interaction with the application. Additionally,
expand the game developed to a new multiplayer version with manipulation of the
Leap Motion Controller and the Sphero device, bringing the application to a more
interactive environment, and group living among children.
Fig. 3. Results obtained at the end of the tests carried out, with a total time and the degree of
improvement in children with ADHD.
904
D. C. Capelo et al.

References
1. Trujillo-Orrego, N., Ibáñez, A., Pineda, D.A.: Validity of the diagnosis of attention deﬁcit
disorder/hyperactivity disorder: what phenomenological to neurobiological (II). Rev Neurol.
54, 367–379 (2012)
2. Portellano, J.A.: Child Neuropsychology. Síntesis, Madrid (2007)
3. Roca, P., Mulas, F., Presentación–Herrero, M.J., Ortiz–Sánchez, P., Idiazábal–Alecha, M.A.,
Miranda–Casas, A.: Evoked potentials and executive functioning in children with
attention-deﬁcit/hyperactivity disorder. Rev. Neurol. 54(Supll. 1), 95–103 (2012)
4. Colomer–Diago, C., Miranda–Casas, A., Herdoiza–Arroyo, P., Presentación–Herrero, M.J.:
Stressful
executive
functions
and
characteristics
of
children
with
attention-deﬁcit/
hyperactivity disorder: inﬂuence on the results during adolescence. Rev. Neurol. 54(Supll.
1), 117–126 (2012)
5. American Psychological Association (APA): Diagnostic and Statistical Manual of Mental
Disorders. DSM–lV. Revised text. Masson, Barcelona (2003)
6. Martínez-León, N.C.: Psychopathology of the disorder, attention deﬁcit disorder and
hyperactivity. Int. J. Clin. Health Psychol. 6(2), 379–399 (2006)
7. Vásquez, R., Benítez, M., Izquierdo, Á., Gómez, Z.D., Dora, L.G.M., Mera, J.C.C.: What is
the hyperactivity and how they see the problem parents?: Analysis of the reasons for
consultation and the coping strategies of ADHD in a sample of high socio-economic strata of
Bogotá. Colomb. J. Psychiatry 40(3), 488–503 (2011)
8. Amador-Rodero, E.M.: Relationship between the visomotriz integration and academic
performance in children of 5–9 years old diagnosed with ADHD. In order to master work in
Neuropsychology and Education. Universidad Internacional de La Rioja, Logroño, España
(2014)
9. Vidarte, J.A., Ezquerro, M., Giráldez, M.A.: Psychomotor proﬁle of children from 5 to 12
years clinically diagnosed cases of attention deﬁcit/hyperactivity disorder in Colombia.
J. Neurol. 49(2), 69–75 (2009)
10. Cardo, E., Casanovas, S., de la Banda, G., Servera, M.: Soft neurological signs do you have
any utility in the evolution and diagnosis of after-about attention deﬁcit/hyperactivity
disorder? Rev. Neurol. 46(Supll. 1), 51–54 (2013)
11. Rowe, J.B., Eibner, H.R.: The motor system. NeuroImage (2012). https://doi.org/10.1016/j.
neuroimage.2011.12.042
12. Benítez, Y.R., Bringas, M.D.: Neuropsychological battery Luria initial and attentional
processes. Rev. Chil. Neuropsicol. 6(1), 1–6 (2011)
13. González, G.L., Bringas, M.D., Benitez, Y.R., Torres, P.C.: Fine motor skills in attention
deﬁcit disorder with hyperactivity. Cuban J. Neurol. Neurosurg. 3(1), 13–17 (2012)
14. Pascual Sevillano, M.A., Ortega Carrillo, J.A.: Video games and education. In: Ortega
Carrillo, J.A., Chacon Medina, A. (eds.) New Technologies for Education in the Digital Age,
pp. 207–228. Pirámide, Madrid (2007)
15. García, Á.P.: Learning with video games. Experiences and good practices carried out in the
classroom. Open Sch. J. Educ. Res. 17, 135–156 (2014). ISSN: 1138-6908
16. Marqués Graells, P.: Videojuegos. The keys to success. Noteb. Pedag. 291, 55–62 (2014)
17. Blender Foundation: Blender software, May 2017. http://www.blender.org/
18. Microsoft Visual Studio, October 2016. https://msdn.microsoft.com/es-es/library/dd831853.
aspx
19. Unity Technologies: Unity software, Junio 2017. https://unity3d.com/es/
20. Play Store: Abril 2017. https://play.google.com/store/apps?hl=es
21. Nandy, A.: Leap Motion for Developers. Press, Kolkata (2016)
Multisensory Virtual Game with Use of the Device Leap Motion
905

22. Pambudi, R.A., Ramadijanti, N., Basuki, A.: Psychomotor game learning using skeletal
tracking method with leap motion technology. In: 2016 International Electronics Symposium
(IES), pp. 142–147. IEEE, September 2016
23. Arteaga, J.M., Cáceres, J.R.R., Sierra, E.A.: Advances in Interactive Technologies applied to
the disability (2016)
24. Belda, J., Montalvo Martínez, M.: Blog de Show Leap. Leap Motion (II): Principle of operation
(2015). http://blog.showleap.com/2015/05/leap-motion-ii-principio-de-funcionamiento
906
D. C. Capelo et al.

Development and Improvement
of the Visomotriz Coordination: Virtual Game
of Learning and Using the Sphero Haptic
Device for Alpha Generation
David Chilcañán Capelo1(&), Milton Escobar Sánchez1,
Chrystian López Hidalgo1, and Daniela Benalcázar Chicaiza2
1 Universidad de las Fuerzas Armadas ESPE,
Av. General Rumiñahui s/n, 171-5-231B, Sangolquí, Ecuador
{dachilcannc,meesacobar1,calopez16}@espe.edu.ec
2 Universidad Técnica de Ambato UTA,
Av. Los Chasquis y Río Payamino, Huachi Chico, Ambato, Ecuador
da.benalcazar@uta.edu.ec
Abstract. The haptic devices provide three-dimensional navigation and force
feedback and integrates the sense of touch. In the area of education and child
growth (5 to 6 years), the components of the psychomotor skills and particularly
of the visomotriz coordination, allow to children through the virtual simulation
of playful character, the increase and improvement in the handling of objects,
the development of thinking toward the achievement of more complex skills
such as reading and writing. The objective of this research is to explore the
possibility of interaction through the sensory channels of girl children of the
Alpha generation, with a virtual environment through physical contact and
improve the experiences of learning and enrichment of the perception of the real
world based on the virtual simulation, in this case by means of a virtual game
using the Sphero haptic device.
Keywords: Psychomotor skills  Visomotriz  Playful learning
Alpha generation  Sphero haptic device  Virtual game
1
Introduction
The paper assesses the progress of the possibilities drive, expressive and creative that the
human body can manifest [1], 0 to 6 years the main feature of children is the movement
[2] since, with the explores her around the time that communicates and interacts with
those who are in your environment showing emotions and affections from the experi-
ences sensory, motor, intellectual and affective [3]. Learning at this age is from your
own body and the possibilities of action of the same, always recreationally [4].
The coordination subscale is “the role of the integrated agency, by which it
responds to stimuli given as a whole, being the very answer, a constellation, a pattern, a
Gestalt” [5]. Gross motor skills are based on the degree of maturity neurological exam,
which manifests itself in healthy children between 5 and 6 years of age [6]. Which
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_86

leads to the development of the skills and abilities of children, through the use of the
Sphero haptic device, allowing the intervention of a greater number of nerve trans-
mitters associated with the small muscles of the hand and arm of children [7].
The children in this study belong to a digital generation, in this case, the Alpha
generation, children born since the year 2011 up to the present. Children that almost
from the time of their birth have had full access to a cell phone, tablet, or computer and
in addition to the Internet. Accustomed to the bombardment of information and con-
tent, the technology is its advantage over other generations because this is part of their
everyday life [8]. Focus their work, learning, and your way of playing in a different
way, assimilate quickly the media information. Prefer instant answers to their actions,
which provides them to create their own content to improve their learning process [9].
This research proposes a virtual game of learning for children of 5 to 6 years of age
to develop and reﬁne their coordination visomotriz, stimulating eye-hand movements
(eyes-hands-ﬁngers) through the use of a graphic editor of Unity, focused in a virtual
environment (EV) and the interaction of the Sphero haptic device. For this, it were
conﬁgured models (house, dogs, jar and containers), created in the application of
di-breast chart Blender (modeling, lighting and rendering), which is important taran to
the virtual environment of Unity 3D for their positioning within the virtual environment
proposed and by using the scripts of animation and connection are: View of Sphero
Sphero connection (Connection View), control Object of the Game (Game Object), the
scripts are developed in the object-oriented programming language CSharp (C#) which
gives the animations and custom functionality to be more efﬁcient in their adaptation
and usability. Subsequently, it generates an application (App) that is uploaded to the
server of the Google Play Store applications to be downloaded by children or children’s
centers. By facilitating their interaction and intervention involvement of nerve trans-
missions of children.
This article has been organized as follows: Sect. 2, describes the background and
related work that underpin this research, Sect. 3, presents the architecture of the soft-
ware used, Sect. 4, shows the execution of the virtual game of learning, Sect. 5, sets
out the results and discussion, and ﬁnally, in Sect. 6, set out the conclusions on the
basis of the results obtained and describes the future works.
2
Background and Relates Works
2.1
Unity3D
Graphics engine to create cross-platform games developed for Windows, Linux and
OSX (Mac). Its spread in the world of developers of video-games is extensive due to its
set of intuitive tools that are integrated in a simple, fast and with a high quality image
[10, 11]. Makes it easier for robots Sphero haptic device (GUI) use their ability to
position and orient themselves in order to “reﬂect” the state of an object on the screen.
In this scenario, the user can move and rotate the robot to move and rotate the object on
the screen. These movements can also be reﬂected from the application on screen, so
that if the model on the screen rotates, the robot GUI will, too. This capability can be
908
D. C. Capelo et al.

used to monitor the information in an application, such as for example, the tracking of
the movement of an opponent in an online game [12].
2.2
Microsoft Visual Studio
Integrated development environment for desktop devices or applications, whether for
the web or in the cloud, encodes for iOS, Android and Windows on the same IDE [13].
This tool is integrated into Unity 3D (MonoDevelop). Enables the creation of scripts in
JavaScript, C# and Python [14].
2.3
Sphero
Field robotics designed by Orbotix. Its color is white with a plastic polycarbonate wrap
layers of rotate around the environment, change color, run programs and be controlled
from a Smartphone or Tablet [15]. Spherical Robot that contains a platform of
self-balance with two independent wheels, a 3-axis accelerometer, a gyroscope, a
RGB LED contro-label and Bluetooth [12]. Is able to roll around and perform a wide
range of actions, such as the color change, jump, dance and rotation. We have designed
and developed different applications that allow you to have different uses [16–20].
Currently, however, there is a good system that you use all the functionality of this ball
of robot [21, 22]. In comparison to the majority of robots, which are used for thera-
peutic purposes, and extremely portable, and more accessible for both children and
therapists. Tool for the rehabilitation of skills (motor-attention-memory) as a result of
their interaction with those who use it. Through exercises such as: pronation and
supination that are normal movements in therapeutic sessions [21]. Promoting the
coordination of synchronized movements of the hands. In addition to adjusts for both
home and the site of therapy [16, 23].
2.4
Related Job
This section contains a quick overview of the work relating to the subject matter
proposed in this investigation and their approaches. There are some works related with
the management of the haptic devices and your way to perform man-machine inter-
action. Through the management of tactile information and the strength of a human
operator which is interacting with a real or simulated through a computer. Allowing
users to touch the objects to feel their properties (texture, stiffness, shape) or to
manipulate objects directly [24, 25]. Some jobs are:
Development of multidedo haptic interfaces. Using structures type gloves or
exoskeletons. That allow direct interaction with the tactile or cutaneous mechanore-
ceptors in the skin of a person. Being more sensitive to the touch hands, lips and feet
[26–28].
Development of interactive activities to enhance the visuo-motor coordination in
children. Using technological devices mobile phones (Smart-people, tablets) that take
advantage of the interest of the infants by the use of the technology [29].
Development and Improvement of the Visomotriz Coordination
909

Design and implementation of mobile applications for the control and management
of the Sphero haptic device. To enable the learning of basic programming concepts in
children [30].
The research consulted in this section give as a result. There are applications that
control the Sphero haptic device or applications that can handle mobile technological
devices. These applications do not use the devices as a tool to allow the use of the small
muscles of the hand and arm. Facilitating the involvement of a larger number of nerve
transmitters associated with the movements of the children. It will be reﬂected in the
development and improvement of child labor visomotriz coordination.
3
Architecture of the Software Used
To design and build the application proposed in this research, ﬁrst of all, the archi-
tecture of the software that is required, verging on the deployment of a virtual envi-
ronment that facilitates the learning and development of the skills needed in the
visomotriz coordination of children 5 to 6 years belonging to the Alpha generation.
Were implemented graphics, 3D modeling and manipulation of the Sphero haptic
device, as shown in Fig. 1. Used eight components that are integrated, each with its
speciﬁc purpose and functioning: (1) 3D Modeling Blender, (2) IDE Microsoft Visual
Studio, (3) Unity 3D Development platform, (4) Executable, (5) App Stores,
(6) Multi-platform environments, (7) Children, and (8) Sphero (Haptic Device).
(1) 3D Modeling Blender: The purpose of this component is to model the objects that
make up the interface of the application (2 dogs, 1 1 cat, rabbit, pork, 1 1
container for each animal, a mug and a house) This program allows objects to give
Fig. 1. Architecture of the software used for the development of virtual learning game
910
D. C. Capelo et al.

modeling, lighting, rendering and animation, modeling is based on the render
engine that Blender has, this provides a more realistic appearance to objects in any
perspective. Resulting in a better ﬁnish at the time of export to Unity.
(2) IDE Microsoft Visual Studio: This component displays the features that will be
imported to the silver-Unity 3D form, through the creation of scripts (encoding)
that controls the GUI (graphical user interface), which facilitates children to
interact in a friendly manner and entertaining. Of course, reduces development
time by having a direct connection with Unity, which simpliﬁes the copy and
paste the generated scripts or modiﬁed from one place to another, since these are
in a folder of the project.
(3) Unity 3D Development Platform: This component relates the objects generated in
the IDE’s (Integrated Development Environments), improving and correcting
errors of both animation and texturing. Integrates virtual reality technology,
through dynamic simulation, real-time interaction through the use of scripts is
encoded in the Microsoft Visual Studio IDE. Gets a cohesion of quality, also
allows you to perform the connectivity between the application and the haptic
device.
(4) Executable: It is a binary ﬁle that has the possibility to start a program on a
computer or mobile device. In its interior consists of precise instructions that are
interpreted in the electronic device.
(5) App Stores: Site in which to store the application developed to download it from
any compatible device, in this way makes it easier for children accessibility to the
same.
(6) Multi-platform environments: This component is considered one of the most
important in the development of the application as it extends the accessibility in
various devices with different operating systems (OS).
(7) Children: A key component in the ﬂow of evidence of the application, it facilitates
the obtaining of results and veriﬁcation of the effectiveness of the device and the
application.
(8) Sphero (Haptic Device): With the help of this electronic device children can
interact with the application that resides on a computer or mobile device, its
accelerometer and gyroscope allows greater mobility to the object that is located
in the virtual environment by performing the effect of irrigation.
4
Implementation of the Virtual Learning Game
Now the example in the implementation of the virtual learning game (cross-platform)
using the Sphero haptic device. This allows the development and improvement of the
coordination visomotriz, focused on generating Alpha.
As shown in the Fig. 2 at the beginning of the application the child ﬁnds the 5
animals and for each one of them an empty container, with the movement of the Sphero
device will have to move the bottle, on each of the containers to ﬁll them.
Development and Improvement of the Visomotriz Coordination
911

Above each animal is a sad face if the recipient of the animal in question is empty.
Once the vessel has been ﬁlled, the face changes of state, that is to say, from sadness to
happiness.
5
Results and Discussion
The haptic virtual game using the Sphero device was carried out with 10 children of 5
to 6 years, as had been exposed previously chose this age range, correspond both to the
Alpha generation as the characteristics of maturity and psychomotor development.
Each of the children, had three attempts in these, record the time it takes to com-
plete the task on each occasion. Table 1 describes the times employees in each of their
attempts according to the age of the children. As soon as he was ﬁnished each attempt,
Fig. 2. Implementation of the virtual learning game using the sphero haptic device.
Table 1. Execution times of the game for attempt
Age
Years
Children
Number
1° Tried
Minutes
2° Tried
Minutes
3° Tried
Minutes
5
1
11,21
8
7,50
2
11
8,5
7,4
3
11,50
9
7,32
4
10,1
9,1
7
5
12
10
7,25
6
6
8,2
8
6,55
7
9
7,50
6,59
8
8,1
7,3
6,4
9
9,50
7
6,30
10
10
8
6,25
912
D. C. Capelo et al.

it was observed the decrease of the times and the improvement in the handling of the
device with the consequent visomotriz improvement.
As can be seen, in the group of children of 5 years the times in the ﬁrst attempt were
10 to 12 min; in the second attempt were reduced from 8 to 10 min while in the third
attempt ﬁnally reached 7 times to 7.50 min.
In the group of 6 years the times of departure in the ﬁrst attempt were 8 to 10 min,
in the second attempt of 7 to 8 min and, on the third attempt the reduction came to be of
6.59 to 6 min.
This shows the effectiveness of the Sphero device in a virtual environment applied
to a game of learning for the selected population, as can be seen in the Fig. 3 below:
The virtual reality applied to the ﬁeld of learning represent an element to boost the
activities and experiences of growth and maturation in the infant stage. The digital
natives is to say to the children of the Alpha generation, they interact with electronic
devices easily, hence the results showed that, with the help of the Sphero haptic device,
children tend to improve their ﬁne motor skills in a timely fashion, the visomotriz
coordination. The decrease in the time of execution of the tasks present in the game
raised, clearly is a sample of the eye-hand dexterity is enriched with increasing fre-
quency and articulating the bi and three-dimensionality with sensory perception.
It should be noted that the technological devices to be a reality in the everyday life
of the Alpha generation, have become an option to the traditional forms of visomotriz
develop coordination. When using the haptic device Sphero has managed to articulate
the need current technology with the touch and movement skills in a virtual environ-
ment, but with a body exercising that optimizes the psychomotor development and
maturation.
Fig. 3. The variation between the ﬁrst attempt (blue line), the second attempt (red line) and the
third attempt (yellow line) shows a reduction in the time (minutes) by the participants (children).
Development and Improvement of the Visomotriz Coordination
913

6
Conclusions and Future Works
The game virtual learning proposed in this research allowed children interact in an easy
and intuitive way with the Sphero haptic device, so that the objects in the virtual
learning environment proposed are very friendly and attractive. Another achievement
was that children respond instantly to visual stimuli and multimedia information,
enhancing their skills and ﬁne motor skills.
Also, the application stimulated the involvement of a larger number of nerve
transmitters of girls/or since, the haptic device Sphero as a technological tool, helped in
the interaction of the small muscles of the hand and ﬁngers of girls/or by direct
handling with the device, which in turn contributed to the development and
improvement of their visomotriz coordination.
How to future work seeks to incorporate virtual environments (EV) using the Leap
Motion and HTC device live, as well as also, add options of augmented reality. This
will generate a mixed environment that provides girls/or a better perception in the
manipulation of objects with the Sphero 2.0, improving their cognitive and creative
process.
On the other hand, implement progressive levels of difﬁculty in the tasks to be
performed within the game, which will correspond to the age of the children. In this
way you will be able to apply in areas of learning math, language arts, natural sciences,
artistic expression among other.
References
1. Lin, C.-K., Meng, L.-F., Yu, Y.-W., Chen, C.-K., Li, K.-H.: Factor analysis of the contextual
ﬁne motor questionnaire in children. Research in Developmental Disabilities 35(2), 512–519
(2014). ISSN 0891-4222
2. Comellas, M.J.: Psicomotricidad en la educación infantil: recursos pedagógicos. 2ª ed., 1ª
imp. Grupo Editorial CEAC, S. A. Barcelona (2003)
3. Muñoz, L.: Educación psicomotriz, 4ª edn. Editorial Kinesis, Armenia Colombia (2003)
4. El-Sayed, M., El-Sayed, J.: Importance of psychomotor development for innovation and
creativity. Int. J. Process Educ. 4(1), 1–6 (2012)
5. Bender, L.: Test guestáltico visomotor (B.G): usos y aplicaciones clínicas. 1ª ed., 17ª
reimp. Buenos Aires. Paidós (2003)
6. Cameron, C.E., Brock, L.L., Murrah, W.M., Bell, L.H., Worzalla, S.L., Grissmer, D., et al.:
Fine motor skills and executive function both contribute to kindergarten achievement. Child
Dev. 83(4), 1229–1244 (2012)
7. Cameselle, R.P.: Psicomotricidad: Teoría y praxis del desarrollo psicomotor en la infancia.
Ideaspropias Editorial SL (2005)
8. Ospina, K.L.J., Mayorga, F.A.N., Villota, W.A.C.: Niños y adolescentes. Su dependencia de
la tecnología móvil. Revista Pertinencia Académica (2), 57–68 (2017)
9. Thompson, P.: The digital natives as learners: Technology use patterns and approaches to
learning. Comput. Educ. 65, 12–33 (2013). ISSN 0360-1315
10. Unity Technologies: Unity software, Junio 2017. https://unity3d.com/es/
11. Unity Documentation, Junio 2017. https://docs.unity3d.com/es/current/Manual/
914
D. C. Capelo et al.

12. Guinness, D., Szaﬁr, D., Kane, S.K.: GUI Robots: Using off-the-shelf robots as tangible
input and output devices for unmodiﬁed GUI applications. In: Proceedings of the 2017
Conference on Designing Interactive Systems, pp. 767–778. ACM (2017)
13. Microsoft Visual Studio, Mayo 2016. https://msdn.microsoft.com/es-es/library/dd831853.
aspx
14. Mamone, M.: Introducing development tools and MonoDevelop. In: Practical Mono, pp. 21–
40 (2006)
15. Sphero, Junio 2015. http://www.Sphero.com
16. Mendez-Zorrilla, A., Garcia-Zapirain, B., Eskubi-Astobiza, J., Fernandez-Cordero, L.:
Sphero as an interactive tool in computer games for people with ID. In: 2015 Computer
Games: AI, Animation, Mobile, Multimedia, Educational and Serious Games (CGAMES),
pp. 99–102. IEEE, July 2015
17. Carroll, J., Polo, F.: Augmented reality gaming with sphero. In: ACM SIGGRAPH 2013
Mobile, p. 17. ACM, July 2013
18. Kurkovsky, S.: Android+Sphero: teaching mobile computing and robotics in a single course.
In: Proceeding of the 44th ACM Technical Symposium on Computer Science Education,
p. 765. ACM, March 2013
19. Trower, J., Gray, J.: Blockly language creation and applications: Visual programming for
media computation and bluetooth robotics control. In: Proceedings of the 46th ACM
Technical Symposium on Computer Science Education, p. 5. ACM, February 2015
20. Carlson, D., Pagel, M.: Tap to interact: Towards dynamically remixing the internet of things.
In: Proceedings of the 11th International Conference on Mobile and Ubiquitous Systems:
Computing, Networking and Services, pp. 376–378. ICST (Institute for Computer Sciences,
Social-Informatics and Telecommunications Engineering), December 2014
21. Albiol-Pérez, S., Pruna-Panchi, E.P., Escobar-Anchaguano, I.P., Bucheli-Andrade, J.G.,
Pilatasig-Panchi, M.A., Mena-Mena, L.E., Zumbana, P.: Acceptance and suitability of a
novel virtual system in chronic acquired brain injury patients. In: Rocha, Á., Correia, A.,
Adeli, H., Reis, L., Mendonça Teixeira, M. (eds.) New Advances in Information Systems
and Technologies. Advances in Intelligent Systems and Computing, vol. 444, pp. 1065–
1071. Springer, Cham (2016)
22. Matos, N., Santos, A., Vasconcelos, A.: Kinteract: A multi-sensor physical rehabilitation
solution based on interactive games. In: Proceedings of the 8th International Conference on
Pervasive Computing Technologies for Healthcare (PervasiveHealth 2014), pp. 350–353
(2014)
23. Golestan, S., Soleiman, P., Moradi, H.: Feasibility of using sphero in rehabilitation of
children with autism in social and communication skills. In: 2017 International Conference
on Rehabilitation Robotics (ICORR), pp. 989–994. IEEE, July 2017
24. Biggs, S.J., Srinivasan, M.A.: Haptic interfaces. In: Stanney, K.M. (ed.) Handbook of
Virtual Environments: Design, Implementation, and Applications, pp. 93–115. Erlbaum,
Mahwah (2002)
25. Monroy, M., Oyarzabal, M., Ferre, M., Cobos, S., Barrio, J., Ortego, J.: Dispositivos
hápticos: Una forma de realizar la interacción hombremáquina. Domótica, Robótica y
Teleasistencia para Todos, p. 39 (2007)
26. Ariza, V.Z.P., Santís-Chaves, M.: Interfaces Hápticas: Sistemas Cinestésicos Vs. Sistemas
Táctiles. Revista EIA 13(26) (2017)
27. Chen, L.J., Holbein, M., Zelek, J.S.: Intro to haptic communications for high school
students. In: 2006 Proceedings- IEEE International Conference on Robotics and Automation,
pp. 733–738, May 2006
Development and Improvement of the Visomotriz Coordination
915

28. Nakamura, M., Jones, L.A., Sonin, A.A.: A Torso Haptic Display based on Shape Memory
Alloy Actuators. Thesis. Master of Science in Mechanical Engineering. Massachusetts
Institute of Technology (2003)
29. Robalino Ramos, M.E.: Desarrollo de actividades interactivas para tablet como apoyo en la
coordinación visomotora en niños de primer año de educación básica (Bachelor’s thesis,
Pontiﬁcia Universidad Católica del Ecuador) (2017)
30. Rojas Alcón, A.: Desarrollo de una aplicación móvil multiplataforma utilizando un Sphero
para la enseñanza de programación en niños (Doctoral dissertation) (2015)
916
D. C. Capelo et al.

Information Technologies in Education

Experiential Education: Creation of a Business
Game to Enhance Learning of Business
Administration Students
Eduardo de Oliveira Ormond(&), Gustavo Olivares,
and Saulo Barbará de Oliveira
Computer Science Editorial, Springer-Verlag, Tiergartenstr. 17,
69121 Heidelberg, Germany
eduardoormond@hotmail.com, olivares.rural@gmail.com,
saulobarbara@gmail.com
Abstract. Business administration program in this millennium must prepare
decision-makers to deal effectively with the organizational transformation pro-
cess, through effective planning and creation of propitious conditions for
achieving the corporate objectives. This article presents a learning environment
created to foster the development of skills and competencies through a
theoretical-practical experiment. For this purpose, a business game was devel-
oped that incorporates the lean manufacturing philosophy, and simulations were
carried out in an experiential learning environment created for this purpose. The
simulations applied with undergraduate business administration students at a
school located in the city of Rio de Janeiro, Brazil. The analysis of the results
shows that after playing the simulation game, the participants stated it changed
their way of acting and thinking in relation to the relevant theoretical knowledge
and its practical applications, with positive impacts on the learning process.
Keywords: Teaching-learning  Decision-making  Business game
Production management  Simulation
1
Introduction
The phenomenon of globalization is no longer a novelty. Virtually everyone feels its
impacts, both positive and negative. Its socioeconomic impact imposes major changes
in countries and organizations the world over, and demands agile organizational pro-
cesses. This phenomenon has modiﬁed social, and particularly professional, relations in
the contemporary world. Technological advances are increasingly overcoming geo-
graphic distances and making competition sharper for all: individuals, organizations
and nations. Decision-making needs to be ever-more assertive, incorporating change as
a natural factor in the world of business. All this requires great ﬂexibility and adapt-
ability along with expertise and constant updating of knowledge, experiences and
applications in the various social segments, to stay abreast of the rapid changes in all
areas of human endeavor [1].
However, institutions of higher learning in the state of Rio de Janeiro, with rare
exceptions, still use the teaching methods ﬁrst developed during the start of the
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_87

Industrial Revolution, formulated to serve large numbers of students with mainly rural
backgrounds, with teaching focused on preparing people to ﬁll technical and man-
agement positions in urban industries and where assessment of learning was almost
exclusively through application of tests [2].
The purpose of this article is to present Acróbata, a business game that aims to
stimulate individual and group learning by simulating practical situations, to enhance
the development of important academic competencies for working and making deci-
sions in the corporate world, to complement theoretical teaching. The topic to which
the game is applied is management by processes, with focus on the Toyota production
system. The activity based on the theories of games and lean manufacturing, both of
them fully applicable to organizational learning [3].
The study is empirical in nature, for responding to the following question: To what
extent can the application of a business game contribute to the training of business
administration students?
The article organized into seven sections including this introduction. The second
section discusses the idea of production management, with focus on the Toyota pro-
duction system; the third explains the reason for using the business game; the fourth
describes the methodology; the ﬁfth covers application of the game; the sixth presents
the results; and the seventh concludes.
2
Production Management
Production systems involve the entrance of inputs, the application of a transformation
process and the output of products or services [4]. The management of production deals
with the way that organizations produce goods and services [5]. The main purpose of
improving a production system is to streamline/optimize the entire chain of the pro-
ductive process. Figure 1 illustrates the logic of the production system (Transformation
of productive resources/Goods and services).
Although at ﬁrst glance they may seem simple, productive systems in reality are
complex, because they also involve the external environment with which the organi-
zation is constantly interacting, governed by legal rules, public policies, economic facts
and other factors, as well as relying on internal and external resources. Because of the
rapid pace of change in virtually all aspects of modern life, transformation of pro-
duction processes must occur in a continuous cycle, to improve the use of resources
and the process itself, and the general performance of the productive system.
Fig. 1. Representation of a production process
920
E. de Oliveira Ormond et al.

Although this conception originated in the studies of Frederick Winslow Taylor in
the early twentieth century and evolved with the philosophy of William Edwards
Deming and other thinkers, individuals, organizations and society at large [6] are still
developing the idea of the continuous improvement cycle today, in response to the
tireless search for excellence.
2.1
Toyota Production System
The automotive industry played a leading role in the technological, economic and
social development of the twentieth century. Twice in that period the industry altered
the fundamental notions of how to produce goods and services, giving new direction to
way of working, thinking, producing and engaging in all other human activities [7].
The ﬁrst was the perfection by Henry Ford of mass production of cars on assembly
lines. Then, new transformations triggered by the creation of the lean manufacturing
system in Japan after the Second World War. Japan, devastated by the war, simply did
not have the resources to make the investments necessary to reestablish Ford’s mass
production model (push production system) [8]. Lean manufacturing considered a
production system whose main characteristic is ﬂexibility [9]. The basic practice
underpinning Toyota’s production system is total elimination of waste [10]. The
deepening changes caused by globalization and economic developments have forced
alterations in business models.
With the major paradigm shift resulting from the global oil crisis in the 1970s, push
production gave way to pull production, also spurred by automation and the
just-in-time model – and an inventory management strategy whereby stocks maintained
only at the levels strictly necessary to keep the productive system supplied [11]. The
base of lean manufacturing is a combination of management techniques with tools and
machines, with the goal of producing more with less [4]. In this model, to attain
objectives managers form teams of workers with various skills and hierarchical levels,
associating “quality, productivity and variety” to produce goods or services [12].
2.2
Why Use a Business Game?
Business administration programs must be able to prepare students for executive
positions in their future careers [12]. Where urgent needs and the capacity for
decision-making will be put to the test of ﬁre, resulting from the dizzying pace of
change, for which competitive organizations must be prepared [2]. The transformations
in the business world make it increasingly essential to teach, train and recycle students
and executives in the area of organizational management [13]. Many tools and tech-
niques can used for these purposes: expositive classes, presentations, visits to com-
panies, and application of business games, among others. The last of these is of
particular importance, by inserting the student in a simulated world reﬂecting organi-
zational reality [2]. The experiential learning models, represented in this study by
simulation games, entail a considerable effort to integrate business theory and practice.
The game-playing approach increasingly used, given its ability to ﬁll some gaps in the
area of training business executives [1]. A structured and sequential exercise in making
decisions in a model based on rules enables the participants to assume the role of
Experiential Education: Creation of a Business Game
921

manager of simulated operations [8]. In this way, the participants in this type of game
can experiment, test and improve their knowledge through simulation, where the
learning process and consolidation of knowledge gain greater speed and efﬁciency
compared to traditional teaching-learning methods [1].
3
Methodology
To carry out this study, we applied the action research method. This is a type of social
research with empirical basis, conceived and carried out in close association with a set
of actions aiming to resolve a collective problem in which the researcher and partici-
pants are involved, in cooperative or participative fashion [14, 15]. The procedures
composing this study were the following:
(a) Creation of the game, manuals and guidelines.
(b) Preparation of the environment, with all the material necessary: plastic toy trucks,
table, projector, white board and stopwatch.
(c) Beginning of the game, with annotation of behavioral observations, to monitor
each cycle of the game.
(d) Evaluation of the results through interviews with the participants.
(e) Introduction of corrections and adjustments based on the lessons learned.
(f) Repetition of the game cycle, until reaching the objective.
A group interview was conducted to evaluate the usefulness and efﬁciency of the
game, based on the perception and reaction of the participants about their learning
progress. Various aspects and types of individual and team behavior could be observed
and noted as lessons learned, for the purpose of improving the learning process.
4
Application of the Business Game
The game was dubbed “Acróbata”. Its validation cycle occurred in September 2016, at
an institution of higher learning located in the city of Rio de Janeiro, Brazil. Two teams
of 10 students each participated directly in the validation cycles, and 66 more students
participated indirectly as observers of the simulation events, for the preparation and
analysis of the ﬁnal report on the experiments. Participation was voluntary and all were
told they could cease participating at any time, although this never happened in any
rounds of the game.
The functioning of a simulation game and its experiential setting applied to the
production of knowledge can follow different routes, so the researcher must choose
those that contribute most to reaching the proposed objective. In this study, the objective
was to convey knowledge about the lean manufacturing model. Figure 2 shows the
design of the setting, its context and the simulation resources. The model of this game
has a board game format and can be applied in the classroom itself. It simulates a truck
production line where the players manipulate small plastic trucks (tractor-trailer rigs)
that are assembled along the line. After the conclusion of the various production cycles,
the customers evaluate the quality of the product and productivity of the line. Therefore,
922
E. de Oliveira Ormond et al.

when the game ends, the team that has the largest number of trucks with the lowest unit
cost will be the winner.
The game consists of ﬁve production cycles, the ﬁrst having the purpose of training
and adaptation of the participants. Each cycle (or round) lasts around ﬁve minutes and
is played by two teams of 10 students each. During the round, various problems can
arise, such as breakdowns of production machines, breakage of parts or components
and improper positioning of workers on the production line, among others. A teacher or
instructor is present during all cycles to guide the production process.
4.1
The Dynamics of the Game
The development of Acróbata was based on the basic precepts of the Toyota production
system or lean manufacturing. It was created to facilitate students’ understanding of the
productive process, by enhancing the absorption of knowledge, acquired during the
game. By playing the game, the following principles of lean manufacturing were
gradually learned and developed by the students: (a) the need to work in teams (for-
mation of team spirit); (b) reduction of inventory to the minimum level possible, without
appreciable risk of shut-downs due to lack of materials, parts or components (notion of
lean inventory); (c) introduction to the concept of pull production; (d) continuous
improvement of processes by reducing costs; (e) notion of the need for shared
decision-making by the team; (f) holistic vision of the entire process/multi-task
Fig. 2. Representation of the game environment
Experiential Education: Creation of a Business Game
923

professional training; (g) management of people; and (h) organizational learning. The
winner is considered the team (company) that delivers to the customer the largest
number of products (trucks) with the lowest unit cost and consequently the best pro-
duction quality at the end of the four rounds or cycles. The toy trucks are assembled in
the counter-clockwise direction, and as they become ﬁnished products, the customer
evaluates and indicates the results regarding conformity and non-conformity in each
cycle, as shown in Fig. 3. The monetary values shown in the ﬁgure indicate the behavior
of the cost in relation to the quantity. For example, it can be seen that in the training
cycle, 19 tractor-trailer rigs were produced at a unit cost of R$7.00 by the winning team.
In turn, in cycle 1, the number of trucks was 35 at a unit cost of R$4.00, and so on.
At the end of the four production, the score obtained by each team is computed
(since the initial cycle has the purpose of familiarizing the participants with the
dynamics of the game, it is not considered in computing the score). The actions chosen
by the students have costs, indicated in advance, for subsequent calculation and gen-
eration of the ﬁnal report. After the start of each round, machines can suffer break-
downs or be shutdown for maintenance, and at the end of each cycle, “workers” can be
hired or ﬁred (inclusions or exclusions). All of these decisions have associated costs,
stipulated in advance. The report compiling the results will allow the students to make
better future decisions, because they participate in evaluating the results.
5
Conclusion
The application of the business game in all the evaluations was a very important
experience for all the participants. After each production cycle, the students were more
familiar with the game and more skillful in producing trucks, which was directly
reﬂected in the improving performance of the teams.
Fig. 3. Good Production x Unit Cost
924
E. de Oliveira Ormond et al.

Additionally, the ongoing practice of the game was able to introduce the concepts
of lean manufacturing in the way of thinking and acting of the teams, and it was clear in
the perception of the participants that the teaching-learning process became faster and
more consistent as the logic of the game was assimilated by them. This corroborates the
idea that the development of theory along with practice improves the ability for rea-
soning, abstraction and absorption of knowledge by students, and in this case made a
signiﬁcant contribution to the construction and reinforcement of learning and the
continuous development of knowledge.
The environment based on production management with focus on lean manufac-
turing associated with the game, as explained, sought to construct knowledge through
experimentation and theoretical-practical experience, to prepare future business leaders
to resolve problems through creativity and innovation.
The application of the game in line with the method chosen served to demonstrate
that the traditional method of teaching when associated with an experiential tool,
helped to promote moments of reﬂection by the students. That can be extremely useful
to mold their attitudes, according to the demands of the job market and to overcome the
challenges they will face in their careers. In the ﬁnal analysis, the game improved
strategic thinking of the participants, helping them interpret scenarios and situations.
References
1. Silva, P., Pedrosa, D., Trigo. A., Varajão, J.: Simulation, games and challenges: from
schools to enterprises. In: Barjis, J., Eldabi, T., Gupta, A. (eds) Enterprise and Organizational
Modeling and Simulation. EOMAS 2011. Lecture Notes in Business Information
Processing, vol 88. Springer, Heidelberg (2011)
2. Vicente, P.: Jogos de empresa. Makron Books, São Paulo (2001)
3. Alves, C.S, Filho, P.A.L.S., Martins, T.A., Pereira, R.: Contribuição dos jogos empresariais
dentro da política da Universidade Corporativa nas Organizações. Seminário Regional de
Integração e Desenvolvimento Regional. Ponta Porã Mato Grosso (2013)
4. Sauaia, A.C.A.: Laboratório de Gestão: Simulador organizacional, jogo de empresas e
pesquisa aplicada, 3rd edn. Manole, São Paulo (2013)
5. Venanzi, D., Silva, O.R.: Gerenciamento da produção e operações, 1st edn. LTC, Rio de
Janeiro (2013)
6. Liker, J.K., Meier, D.: O modelo Toyota: manual de aplicação. Bookman, Porto Alegre
(2007)
7. Pontes, J.M., Figueiredo, O.C.: Proposta de implantação da ﬁlosoﬁa Lean Manufacturing em
uma confecção de pequeno porte através do mapeamento do Fluxo de valor. XII Congresso
Nacional de excelência em gestão. Rio de janeiro (2016)
8. Bouzada, M.A.C.: Jogando Logística no Brasil. XXIX Encontro Nacional de Engenharia de
Produção. Salvador (2009)
9. Campos, C.A., Rodrigues, M., Oliveira, R.S.: Lean Manufaturing: Produção Enxuta. Revista
Cientíﬁca E-locução (2016)
10. Motta, F., Prestes, C., Vasconcelos, I.F.G.: Teoria Geral da Administração. 3ªed. Rev.
Cengage Learning, São Paulo (2013)
11. Junior, U.O., Biasoli, R.C., Sacomano, J.B., Rocha, W., Feriagatto: A importância da
Manufatura enxuta em um mercado competitivo. Estudo de Caso em uma indústria de
autopeças. XIV Conferência Internacional de Engenharia e Educação Tecnológica (2016)
Experiential Education: Creation of a Business Game
925

12. Pacheco, D.A.J.: Production. UFRS, Porto Alegre (2013)
13. Souza, J.M.: PDCA e Lean Manufacturing: Estudo de caso de aplicação de processos de
qualidade na Gráﬁca Alfa. Unopar Cinet (2016)
14. Thiollent, M.: Metodologia da Pesquisa-ação. Cortez e Autores Associados, São Paulo
(1988)
15. Gonçalves, M.J.A., Rocha, Á., Cota, M.P.: Information management model for competen-
cies and learning outcomes in an educational context. Inf. Syst. Front. 18(6), 1051–1061
(2016)
926
E. de Oliveira Ormond et al.

Educational Computing Resources Applied
to the Teaching of Manufacturing
Leonardo Emiro Contreras Bravo(&),
Jose Ignacio Rodriguez Molano, and Edwin Rivas Trujillo
Universidad Distrital Francisco José de Caldas, Bogotá, Colombia
{lecontrerasb,jirodriguez,erivas}@udistrital.edu.co
Abstract. Due to the incursion of information and communication technologies
in the educational context, teachers have seen the need to change their role,
product because of the integration of different technologies that students use. In
that sense, his pedagogical and didactic work is changing according to dynamics
that are much more collaborative with his students. This paper aims to show a
proposal for incursion of Information and Communication Technologies
(ICT) and other teaching methodologies in the subject mechanical processes,
evaluating their implementation through an instrument of descriptive records
and unstructured interviews to which some results are analyzed.
Keywords: Engineering education  ICT  Learning tools  Teacher
Virtual environments
1
Introduction
Times have changed, and information is presented to students every day through
different channels that make them more attractive to them. At present, thanks to the
web 2.0 new possibilities and methodologies favor the autonomous learning of the
student. To this extent, and according to [1], liquid modernity, in which everything is
ephemeral and changing, leads the teacher, rather than imparting information, to
generate spaces for reﬂection whose imprint is based on the ability to solve problems
and adapt to different situations of complex order that are presented at a professional
and personal level. In this direction, the problem addressed by this work is to identify
how the use of information and communication technologies (ICTs) could be intro-
duced as a pedagogical proposal and/or work proposal for teachers and students in the
teaching process of the subject Mechanical Processes of the industrial engineering
career at the Universidad Distrital Francisco José de Caldas (Colombia).
The application of this technology in the subject of processes seeks to develop in
students competences of the following types: the selection of materials and the han-
dling of variables within a productive process, own and understand the theoretical
foundations to direct and implement the correct application of the installation and
distribution of industrial equipment within a production plant, appropriate the technical
languages of process engineering, with its regulations, representations and symbologies
in traditional and computer media (ICTs), understand the technical, constitutive,
tooling and inputs of the production processes, know, apply and integrate to their
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_88

professional performance the strict monitoring of norms, standards and regulations with
eco-sustainable and ethical approaches and understand the conceptual foundations to
redesign and innovate general products and processes applicable in the industrial ﬁeld
and consumption, applying new technologies.
2
Generalities
2.1
Traditional Teaching
The methodology of traditional teaching has been based on a transfer of information
from the teacher to a group of students who listens attentively, trying to assimilate the
ideas transmitted, which in turn are evaluated by quices and mid-term; That is to say, it
is the use of an expositive teaching methodology, to this must be added the need to
develop classes too long to close to the objective of representing the class exercises in a
way that is fairly clear and understandable to the student and, in most cases, the teacher
has short curricular time [2]. In this scenario, the student is a passive actor, who only
resorts to group work in some cases at the time of studying for written evaluations from
textbooks, and the notes taken in master classes, leaving aside the search for a solution
to simple everyday problems or perhaps engineering. Of course, many leaders, aca-
demics, and practitioners believe that traditional approaches to teaching, such as reli-
ance on textbooks, mass instruction, conferences, and multiple choice tests, are
outdated in the information age [3].
2.2
E-learning and B-learning
E-learning is mainly a form of distance learning or virtual, through which individuals
and groups appropriate new knowledge and skills with the support of computer net-
work technology; students can interact with the teachers (tutors) through the Internet. It
has certain characteristics thanks to the new technologies of information and com-
munication (ICT), as they are: the communication synchronous or asynchronous way,
without space-temporal limitations between classmates and teachers; The support of the
tutors is given by means of electronic mail, text and voice chats, messages, discussion
forums or even videoconferences; In conclusion, literally e-learning is “e-learning”;
That is, learning with electronic means. On the other hand, b-learning is described as a
way of learning that combines teaching through traditional face-to-face and virtual
teaching activities [4]. Therefore, blended learning (Blended or mixed learning), is
more closely related to a hybrid training model that has the possibility of collecting the
best of distance learning and the best of face-to-face teaching; that is to say to use
correctly the available electronic resources and digital infrastructure and to use the
appropriate methods of active participation in class in order to facilitate learning.
Thus, there are several technological possibilities to implement online learning,
each with its advantages and disadvantages, [10, 12] give us an overview of how
e-learning environments have evolved. However here is limited to the resource used in
the experience: The Virtual Classroom Moodle (VCM). This virtual classroom does not
imply a mere repository of documents in web format or of exposing contents but of
928
L. E. C. Bravo et al.

designing pedagogical proposals that favor the social construction of knowledge,
autonomously, through a process of gradual handover of control, as indicated in [11].
2.3
Active Learning
The traditional techniques are focused on the work of the teacher (master presentations,
solution of theoretical problems posed by the teacher, completely deﬁned laboratory
practices in terms of their methodologies and results, etc.), leaving a completely passive
role for students [6]. For several authors, the level of appropriation of knowledge that
students may have according to the teaching methodology used may vary. According to
[7] it clearly divides education into two fundamental types: Passive and active teaching,
referring to the role that the student fulﬁlls during his or her own formation. The former
achieves retention levels of less than 50%, while the latter achieves levels above 70%.
According to [8], in adopting the principle of “learn by doing” or that of “hands-on
work”, the construction of notions is replaced by the action itself and the passive
reception of knowledge is abandoned. Consequently, the learning process is understood
as a direct, experiential and discovery learning; In this way, the appropriate methods for
the self-structuring model are based on experimentation (experimental trial, which is a
natural and universal activity) and on exploration, where the student is the artisan of his
own knowledge [9].
3
Materials and Methods
The type of study is Descriptive, whose foundation is the analysis of characteristics that
look for to specify the important properties of people or groups; A series of questions
are selected and each is measured independently to describe what is being investigated
[5]. The objective under this model is to investigate the situation posed as a research
question, for this it was necessary to collect data in the Universidad Distrital, specif-
ically in the subject of Mechanical processes, to describe how the technological
resource strengthens the teaching process and the students of the Subject mechanical
processes through the formulation of ICT implementation strategies.
3.1
Research Environment
The mechanical process course corresponds to the fourth semester of the industrial
engineering career at the Universidad Distrital Francisco José de Caldas; have an
average of 40 students, hourly intensity of 4 h a week for expository master classes and
2 for Lab practices. Both the theoretical and the practical training are evaluated through
different instruments, being the most used: quices, mid-term, workshops of textbook
exercises and the ﬁnal work of application of the topics seen in the semester. To this
must be added the need to develop classes too long to close to the objective of
representing the exercises in a manner that is fairly clear and understandable to the
student and, in most cases, the teacher has a short curricular time [2] to show the
student practical, daily and real examples of how to perform calculations of variables
present in each of the manufacturing processes of industrial use applied to metals,
polymers and ceramic materials.
Educational Computing Resources Applied to the Teaching
929

According to the work titled: “process of recognition of teaching and learning
methods 2014” carried out by the industrial engineering program of the Universidad
Distrital (Fig. 1a), in almost all areas the teacher’s work tool is the board and marker
(traditional teaching); The eight areas in which the program is subdivided show that
there is a shortage of audio-visual resources or other methodology to help class
development. Although the fact that the course in question has laboratories (belongs to
the area of design and manufacturing) gives students a better understanding of the
processes. Also, it was evidenced that the collaborative work as a study strategy, is
little used by the teachers of Industrial Engineering; most of them are evaluated in a
traditional way in which there is a high percentage of memory work and little inno-
vation of improvement for the students (Fig. 1b). In the area of design and manufac-
turing, the evaluation of written mid-term and quices prevails, and little or nothing is
used ICT tools.
3.2
Proposed Teaching-Learning Process
It is then intended to integrate many ways of reaching the student in order to help him
improve his teaching process. Therefore, it is proposed to integrate the computer,
didactic and other tools available on the internet with the classroom classes of the
subject mechanical processes through the LMS platform called Moodle, presenting a
B-learning or bimodal methodology in which the Fact that the student must distribute
his time between direct working hours (HTD) corresponding to the class hours that
students must attend in person and for use in laboratories (traditional teaching);
Cooperative Work (HTC) which correspond to hours where students attend, according
to needs and methodologies with the purpose that working individually or in groups,
develop topics, advance practices, solve application exercises (traditional and virtual
teaching); And the Autonomous Working Hours (HTA) in which the student autono-
mously (virtual and traditional) perform activities to consolidate their learning.
Fig. 1. (a) Types of Tools used for teaching in Industrial Engineering UDFJC. (b) Evaluation
tools by academic areas in Industrial Engineering UD Source: Process of recognition of methods
of teaching and learning-2014
930
L. E. C. Bravo et al.

The student’s hours are inverted in each of the phases of the teaching-learning process
with the support of a virtual course (Fig. 2), passing from a subject receiving infor-
mation only in the master class to one that has access 24 h Day to different elements for
the development of their learning.
The ﬁrst phase called recognition seeks to create the right environment to involve
the student in the ﬁeld of manufacturing processes. This is done through readings and
documents related to the prior knowledge related to the engineering materials, their
structures, their properties and their calculations and their deformations due to the
different types of loads, as well as the basic knowledge of Reading engineering
drawings. Examples of this learning phase are the pre-know review (virtual course) and
the introduction to the subject (traditional class). The second phase, in depth, refers to
the set of situations and activities previously designed in a didactic way leading to the
appropriation of concepts. Examples of this learning phase for each of the thematic
units are quices, class and partial workshops (traditional teaching) and quices online
and forums (teaching with virtual support). The last stage of transfer refers to the
situations and activities of learning designed to add productivity to the knowledge that
is learned. Examples of this phase are individual or collaborative group work and
laboratory reports.
3.3
Strategies and Technological Resources Used to Improve Teaching
and Learning
Initially, the syllabus of the subject was organized in different thematic units in order to
facilitate the assembly and understanding of the virtual course. The resources used and
didactic material developed allow the student to have a series of interactive multimedia
Fig. 2. Phases and learning resources in the course of mechanical processes
Educational Computing Resources Applied to the Teaching
931

resources for the development of the course, so that he acquires more easily certain
knowledge and important skills in the subject. The resources used to carry out this
project seek to be exploited to the maximum in order to improve the learning process of
the subject. These can be grouped as bibliographical (textbooks, articles), computer
equipment (text editor, spreadsheets, presentations, CAM software), laboratory
equipment (machining, welding, plastics, rapid prototyping, etc.), multimedia tools
(YouTube videos, On Line Course that uses forums, network tutorials, images, photos,
applications).
3.3.1
Moodle
Within the framework of the development of this research project, a virtual course for
the subject was developed. Moodle is an Internet-based, virtual educational environment
designed to support a constructivist social education framework (knowledge is con-
structed in the student’s mind instead of being transmitted without from books or
teachings), therefore is a free course management system that helps the online learning
process and collaborative learning. Online activities take place at different hours to
theoretical-practical classes in which students can connect from any geographic location
that has an internet connection. At the beginning of the course students are shown the
course’s functioning system, in order to familiarize them. This introduction to the course
is integrated by the Forum News of the course, the schedule of activities (traditional and
virtual), and the website called bibliography and other resources. Other distinctive
elements of the academic activities of the virtual course integrated to the platform LMS
of Moodle are: Chat, Forum, Quices, Videos, Blogs, Wikis, among others.
3.3.2
Educational Videos
One of the tools used in this work are the videos about the manufacturing processes
coming from numerous universities that have their own institutional YouTube chan-
nels, where they publish multimedia materials for class, etc. For the use of this work
proposal was counted on 40 videos distributed in each thematic unit, whose access
links are found by the student in virtual course of Moodle. The reason why these videos
are available is the difﬁculty of carrying out industrial visits for a large number of
students and even more, to visit all the manufacturing processes under study. Intro-
ducing this tool to the course goes hand in hand with the studies carried out by the
Instituto de Ciencias del Comportamiento (NTL), Fundación de Salamanca, Spain,
(cited by [12]), indicate that viewing and listening with multimedia helps to learning,
although it is greater if done through digital simulations, who obtained the highest rate
of learning retention in various learning experiences and their impact on the organi-
zation (Table 1).
3.3.3
Software and Laboratories
Computer-aided design (CAD) revolutionized the stage of product creation (scale cars),
and the workgroup instantly visualized the object designed in 3D, being able to per-
form checks of interferences and magnitudes of volumes in virtual environments
immediately. The software was useful for the use of laboratories such as conventional
machining, CNC and rapid prototyping, among others (Fig. 3). As it is clear the next
stage was the use of the computational tools for the design and the planning of the
932
L. E. C. Bravo et al.

manufacture. This is known as CAM (Computer Assisted Manufacturing). With the
CAM it is possible that from a design made in a CAD it is possible to automatically
program the CNC machines to make a new part. In the folder called CONTENT Unit,
there are transparencies, such as slide shows (PowerPoint) for each thematic block, as
well as books and related articles, available through the Internet, that refer to the topic
in question.
In addition, the subject consists of 2 h of weekly laboratories in which they enter
machining, welding, plastics, among and others workshops. Practices for example in
the case of conventional machining consist of the procurement of material, putting on
the machine and perform machining operations by the student. In this sense, it is
difﬁcult to see the relationship between the theoretical part (calculation of machining
variables) and the practical part (machining operations), for example.
3.3.4
Multimedia Environment and Collaborative Works
In this type of subjects (theoretical-practical) it is important the two-dimensional and
three-dimensional graphic visualization; hence it is very helpful to show the systems
that are being studied in a graphical environment, with the possibility of interacting
with them. In this regard, one of the objectives pursued by this work was to create an
interactive database of consultation, aimed at enabling the student to reinforce key
concepts necessary for successfully completing the subject in question; this because
sometimes it happens that many basic concepts, which the teacher gives by known, are
not clear enough when the student begins to develop exercises.
The quices as an on-line evaluation were carried out on the Moodle platform for
each of the units (3) of which the program of the subject is included. While partial, ﬁnal
Table 1. Learning and retention activities. Source: [12]
Learning activities
Average retention rate of learning
Listening
5%
Reading
10%
View and listen with multimedia elements 20%
Practice doing homework
50%
Immediate application training
Up to 80%
Fig. 3. (a) SolidWorks design of a group of students (b) Image of the CAM Software ﬁle
Educational Computing Resources Applied to the Teaching
933

examination and exhibition are activities related to a course of processes in traditional
teaching. As far as the collaborative works, these have guides with the speciﬁcations of
the work, learning strategy, expected products and the rubric of evaluation of the same.
These have at last the quest for learning in the student, which can be achieved in
different ways: reading, observing, doing and/or reﬂecting.
4
Results
In this work we tried to integrate a little more the theoretical part of the practice (active
learning). In this sense, the elaboration of various models of equipment at scale
(Fig. 4a) was proposed, to which the groups had to theoretically calculate the
parameters of the material (taught in class), to establish the machining times of the
whole car and later they could adjust the machines with the conditions established by
them verifying tolerances and times and adjusting them if necessary to obtain optimum
times and dimensions as established in the plans that were elaborated by the working
groups. In this way they manage to properly conceptualize the mathematical models
(their limitations and approximations) and the actual behavior of the product as if it
were in a real work of the engineer. The average grades obtained through the
face-to-face system, which were carried out until 2011, were 3.1, which have pro-
gressively improved by using ICT tools, obtaining in the last semester (2017-1) a value
of (3, 70) for 127 students (all groups of the subject) as shown in Fig. 4b. As for the
standard deviation, the group that received traditional teaching had a value of 5.49
while the teaching of ICT measured had a value of 3.7. For the year 2012, academic
performance using ICT tools improved to reach 3.45 for the ﬁrst half of the year and
3.5 for the second half of the year. In this case it can be concluded that the application
of the methodology is giving good results.
In order to collect information relevant to the implementation of technological
resources and the change in the methodology mediated by ICT, a data collection
instrument was applied to the students which included questions related to pedagogical
aspects, didactic aspects, educational tolos approaches and evaluation methodologies.
Some answers are described below. As for the software resource, it was asked if the
teaching of CAD/CAM software through the virtual course seemed correct. It should be
noted that 48% and 36% of respondents answered “quite” and “totally” about the
question (Fig. 5a). It is understood in this case that some doubts about the handling of
the same were solved in person and online. It is also worth clarifying that students have
some knowledge of CAD tool management.
In relation to if “the program and the organization of the subject are adequate”. 45%
stated that it was “quite” adequate and 42% stated that it was “totally”; this gives rise to
say that the work done previously to identify aspects related to the content of the
subject; in which a parallel was made between the Pensum and contents of Industrial
Engineering of the District University with other public and private Universities of
National and International nature was adequate. In general, the students surveyed say
that the use of the Moodle platform has helped them to follow the course better and that
they have worked something more since using the virtual classroom.
934
L. E. C. Bravo et al.

5
Conclusions
The incursion of B-learning was a methodology that provided quality educational
experiences in innovative formats (internet and computer science), which stimulated
the participation and integration in spaces of communication and collaboration between
the actors of the educational process and which constitutes a ﬁrst step to the reduction
of resistances to change not only of teachers and students, but also of managers.
The different computer resources not only facilitate the task of the teacher com-
plementing the learning process of the student but also induce certain skills and abilities
in the student, and also overcome some of the limitations present in traditional
teaching, such as space and time limits, and that it is possible to give the student
training courses through Internet or Intranet training courses, which to a certain extent
would reduce the costs of teaching staff, costs of the equipped classroom, costs of
maintenance of equipment and software.
To propose a methodological strategy that involves the use of ICT requires an
evolution and/or transformation of teaching and learning models, to one that facilitates,
on the one hand, to think about training more focused on the student and his work, and
on the other hand, that these strategies are assumed and understood as complementary
and valid instruments that contribute to the qualiﬁcation of learning.
Fig. 4. (a) Machining work performed by students (b) Grading statistics 2017-1
Fig. 5. Results of the instrument. (a) CAD/CAM teaching (b) Learning strategies
Educational Computing Resources Applied to the Teaching
935

References
1. Bauman, Z.: Los retos de la educación en la modernidad líquida. Editorial Gedisa, S.A.,
Barcelona (2005)
2. Roger, Y., Scaife, M.: How can interactive multimedia facilitate learning. In: First
International Work Shop on Intelligence and Multimodalities in Multimedia, pp. 123–142
(1997)
3. Cuban, L.: Oversold and Underused. Harvard University Press (2009)
4. Coaten, N.: Blended e-learning. educaweb, 69. http://www.educaweb.com/esp/servicios/
monograﬁco/formacionvirtual/1181076.asp
5. Sandoval, C.: Módulo 4: investigación cualitativa. medellín: iner-universidad de antioquia
(2002)
6. Shuman, L., Besterﬁeld-sacre, M., Mcgourty, J.: The ABET “professional skills” – can they
be taught? Can they be assessed. J. Eng. Educ. 94(1), 41–55 (2005)
7. Chrobak, R.: The globalization and the engineering teaching for the xxi century. Primer
congreso Argentino de Enseñanza en la Ingeniería (1996)
8. Not, L.: Las pedagogías del conocimiento. Fondo de Cultura Económica, México (2002)
9. Jaramillo, J.: Notas de clase del curso profesionalización docente. Facultad de Ingeniería,
Facultad de Educación, Pontiﬁcia Universidad Javeriana, Bogotá (2005)
10. Rogoff, B.: Aprendices del pensamiento. El desarrollo cognitivo en el contexto social,
Paidós, Barcelona (1998)
11. Franco, I., Alvarez, F.: Los simuladores, estrategia formativa en ambientes virtuales de
aprendizaje. Fundación Universitaria católica del Norte. http://revistavirtual.ucn.edu.co/
index.php/revistaucn/article/view/167/321
12. Lino, A., Rocha, A., Sizo, A.: Virtual teaching and learning environments: automatic
evaluation with symbolic regression. J. Intell. Fuzzy Syst. 31(4), 2061–2072 (2016)
936
L. E. C. Bravo et al.

Recommendation Systems in Education:
A Systematic Mapping Study
Abdon Carrera Rivera1(&), Mariela Tapia-Leon1,
and Sergio Lujan-Mora2
1 Universidad de Guayaquil, Guayaquil, Ecuador
{abdon.carrerar,mariela.tapial}@ug.edu.ec
2 Universidad de Alicante, Alicante, Spain
sergio.lujan@ua.es
Abstract. Several researchers study recommendation systems to assist users in
the retrieval of relevant goods and services, mostly used in e-commerce.
However, there is limited information of the impact of recommender systems in
other domains like education. Thus, the objective of this study is to summarize
the current knowledge that is available as regards recommendation systems that
have been employed within the education domain to support educational prac-
tices. By performing a systematic mapping study, a total of 44 research papers
have been selected, reviewed and analyzed from an initial set of 1181 papers.
Our results provide some ﬁndings regarding how recommendation systems can
be used to support main areas in education, what approaches techniques or
algorithms recommender systems use and how they address different issues in
the academic world. Moreover, this work has also been useful to detect some
research gaps and key areas where further investigation should be performed,
like the introduction of data mining and artiﬁcial intelligence in recommender
system algorithms to improve personalization of academic choices.
Keywords: Recommendation systems  Education  E-learning
Mapping study
1
Introduction
Systems that retrieve and ﬁlter the data through content and similar proﬁles are known
as recommendation systems (RS). These systems are usually used within the
e-commerce domain. For example, some websites, such as Amazon, through the
application of RS allow offering the user recommendations for products that users do
not know and could be of their interest. Suggested recommendations help to overcome
the distressing search problem for the user. But this technology is not only used to sell
products, but it is also used to suggest videos (YouTube), movies (Netﬂix), friends
(Facebook), among others.
This demand spans across several domains, among which is the educational
domain. RS, which are applied in education, have the role of supporting teaching and
learning activities through enhanced information retrieval. Nevertheless, there is lim-
ited
information
of
the
application
of
recommender
systems
in
educational
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_89

environments. Consequently, this study aims to summarize the current knowledge that
is available concerning RS that have been employed to support educational practices.
This paper is structured as follows. Section 2 explains the research method and
stages applied for the systematic mapping. Section 3 provides the results of the map-
ping study. And ﬁnally, Sect. 4 present our main conclusions.
2
Research Method
We have performed a systematic mapping study by considering the guidelines that are
provided in the works of Kitchenham [1] and Petersen et al. [2] to obtain an overview
of recommendation systems in education. Our systematic mapping study involved
several stages and activities. In the planning stage, the need for the mapping is iden-
tiﬁed, the research questions are speciﬁed, the search strategy is established and the
mapping protocol is deﬁned. In the conducting stage, the primary studies are selected,
the data extraction is performed, and the obtained data is analyzed and synthesized.
Finally, in the reporting stage, the mapping study ﬁndings are presented. The details
concerning the planning and the conducting of our systematic mapping are presented in
the following subsections. The reporting stage is described in Sect. 3.
2.1
Research Questions
Our research aims to examine an overview of the areas in education that can be
addressed by current use of recommendation systems. Thus, the following research
questions are presented: “What are the educational areas covered by RS?” and “What
are the approaches used to generate recommendations within educational scenarios?”.
For further understanding, three research questions were added to support our primary
goal. The idea behind the following research questions is that each one addresses
different aspects of RS in education to identify gaps or issues with the current research.
Table 1 shows all research questions of this mapping study along with their rationale.
2.2
Search Strategy
The conducted mapping study was based on an online search from the following digital
repositories: Scopus, IEEE Xplore, ACM Digital Library and Web of Science. The
mentioned repositories, which cover most of the speciﬁc conference proceedings and
journals for our topic, are relevant to software engineering and computer science.
The ﬁrst part of the search strategy was to deﬁne the search string used for query. In
this part, the papers’ keywords, abstracts, and titles were searched, using the following
terms with the combination of boolean operator “AND”, “OR”: (“recommender sys-
tems” AND “education”) OR (“recommendation systems” AND “education”). When
using the search terms, synonyms and plurals were taken into account. e.g., “recom-
mender” or “recommendation”; “system” or “systems”.
Table 2 shows the terms used in the different scientiﬁc databases with the results of
the search, the asterisk “*” symbol was used in some databases as a wildcard to indicate
938
A. C. Rivera et al.

Table 1. Research questions of the mapping study.
ID
Research question
Rationale
RQ1.
What are the educational areas covered
by RS?
Overview of main areas in education
where recommendation systems can
play a major role for supporting
educational practices. Thus, possible
results of this question are crucial to
analyze and understand in which
domains of the education the
recommender systems are applied
RQ2.
What are the approaches used to
generate recommendations within the
educational context?
It aims to discover the most important
techniques, approaches or algorithms
used to make a recommendation. This
research question gives us insights of
the most frequently employed
approaches in recommendation systems
and education
RQ3.
Which platform is used for the
recommender system deployment?
It is important to discover whether the
recommender system has been
speciﬁcally crafted for the web domain,
developed as a stand-alone desktop
platform or based on a conceptual model
RQ4.
Which evaluation or validation
strategies are applied to
recommendation systems?
It aims to analyze the research rigor of
RS evaluation and validation
procedures. Results will give insights
about how the RS are commonly
evaluated within the educational context
RQ5.
What are the challenges addressed by
adopting a recommendation system in
the educational context?
In this regard, we refer to the most
common challenges that can be solved
when choosing a recommendation
system. Results of this question will
help researchers and practitioner to
understand the strengths of applying a
RS in education
Table 2. Research questions of the mapping study.
Database
Search string
Num. of
studies
ACM Digital
Library
(+“recommend* system*” +education)
181
IEEE Xplore
((“recommender system” OR “recommendation system”)
AND education)
224
Scopus
TITLE-ABS-KEY (“recommend* system*”) AND TITLE-
ABS-KEY (education)
596
Web of Science
Topic: (“recommend* system*”) AND Topic: (education)
180
Recommendation Systems in Education: A Systematic Mapping Study
939

any word variation on each term e.g. “recommend*” implies terms like “recommen-
dation”, “recommender”.
2.3
Selection Criteria
In this process, we ensure that only relevant studies about RS in the context of education
are included for further analysis. This was achieved by applying an inclusion-exclusion
criterion on the mapping study to determine whether a paper should be included in the
following steps.
Papers that met the following criteria were included: Papers supporting educational
practices by using recommendation systems, full papers, academic journal and con-
ference proceedings studies.
Papers that met at least one of the following criteria were excluded: Papers that are
not focused on education, papers presenting only development and implementation of a
recommender system, papers not written in english, duplicate reports of the same study
in different sources, introductory papers for special issues, books, workshops and
technical reports (grey literature).
2.4
Screening Process and Selection of Primary Studies
Once the search results were available, a pre-selection criterion was applied to these
papers. Researchers read paper title, abstract and keywords to apply the inclusion
criteria and consider that at least papers should mention the keywords presented in the
search strategy. Then, the exclusion criterion was used during the full paper reading,
generating what we call the primary studies list.
2.5
Data Extraction and Data Analysis
During data extraction, selected primary studies were fully read and analyzed by
researchers for further classiﬁcation. Data extraction was carried out by breaking down
each research question into more speciﬁc criteria in which a set of possible options was
established. The idea behind this approach is that each paper will be classiﬁed with the
same extraction criteria, thus making it easier the document categorization and data
extraction. The possible answers to each research question and criteria are detailed in
Table 3, each paper can be classiﬁed based on the different options of each criterion.
Further, our data analysis involved a quantitative synthesis based on the number of
primary studies that are classiﬁed in each answer from our research questions. As stated
in Petersen et al. [2] bubble plots are useful to provide a map and give quick insights of
a research ﬁeld. Therefore, we make use of graphical synthesis methods to report the
frequencies of combining the results from different research questions.
940
A. C. Rivera et al.

3
Results
By performing the search strategy, we identiﬁed 1181 papers extracted from digital
database searches. Researchers read title, abstract and keywords of the papers resulting
from the initial search; the application of selection criteria produced a list of 206
primary study candidates. Finally, as result of the whole screening process and data
extraction, this work identiﬁed 44 primary studies. Each reviewed primary study was
classiﬁed based on the possible answers of the main research questions. The results
from the mapping study are structured according to the research questions, as shown in
Table 4. Note that RQ1 and RQ5 are not exclusive; a study can be classiﬁed in one or
Table 3. Data extraction criteria.
Research question
Criteria
Options
RQ1. What are the educational areas covered
by RS?
C1. Areas in
education
– Academic choices
– Learning activities
– Learning resources
– Academic
performance
– Vocational and
educational training
– e-learning
RQ2. What are the approaches used to
generate recommendations within the
educational context?
C2. RS
approach
– Collaborative ﬁltering
– Content based
– Hybrid approach
– Knowledge based
– Other
RQ3. Which platform is used for the
recommender system deployment?
C3. RS
Development
– Desktop based
– Web based
– Mobile based
– Conceptual model
RQ4. Which evaluation or validation
strategies are applied to recommendation
systems?
C4. RS
empirical
validation
– Survey
– Case Study
– Experiment
– None
RQ5. What are the challenges addressed by
adopting a recommendation system in the
educational context?
C5. Issues
addressed by
the RS
– Availability of
information &
content sharing
– Personalized
recommendations
– Prediction accuracy &
efﬁciency
– Improve educational
practices
Recommendation Systems in Education: A Systematic Mapping Study
941

more of the answers. The summation of the percentages is therefore over 100%. In
addition, Fig. 1 shows a bubble plot that summarizes mapping results obtained from
the combination of research question options.
Table 4. Results of the mapping study based on the possible answers to the research question.
Research question
Options
Number
of
studies
Percentage
(%)
RQ1. What are the educational areas
covered by RS?
– Academic choices
21
47.78
– Learning activities
8
18.18
– Learning resources
13
29.55
– Academic
performance
11
25.00
– Vocational and
educational
training
4
9.09
– e-learning
15
34.09
RQ2. What are the approaches used to
generate recommendations within the
educational context?
– Collaborative
ﬁltering
13
29.55
– Content based
1
2.27
– Hybrid approach
20
45.45
– Knowledge based
1
2.27
– Other
9
20.45
RQ3. Which platform is used for the
recommender system deployment?
– Desktop based
3
6.82
– Web based
23
52.27
– Mobile based
1
2.27
– Conceptual model
17
38.63
RQ4. Which evaluation or validation
strategies are applied to
recommendation systems?
– Survey
8
18.18
– Case Study
1
2.27
– Experiment
21
47.73
– None
14
31.82
RQ5. What are the challenges
addressed by adopting a
recommendation system in the
educational context?
– Availability of
information &
content sharing
17
38.64
– Personalized
recommendations
19
43.18
– Prediction
accuracy &
efﬁciency
18
40.91
– Improve
educational
practices
11
25.00
942
A. C. Rivera et al.

3.1
Areas in Education
Results of RQ1 show that around 47% of the papers reviewed are based on academic
choices. Some studies like [3–7] reveal that the primary use of recommendation sys-
tems is to provide advice to students on their educational choices. This involves rec-
ommending students a place to study which could be a faculty, university, college or
moreover support choices of speciﬁc academic courses or disciplines. Other studies
focus on RS for e-learning courses. The authors from [7] use recommender systems to
suggest online courses from different vendors as an embedded software.
Academic choices are not merely limited to the recommendation of universities and
go beyond advising personalized courses; an example is [8] in which authors propose
personalized curriculums. Other studies also focus on other aspects of education like
recommending candidate students to available scholarships [9].
Another area supported by recommendations systems is the e-learning. From the
reviewed primary studies, 15 papers (34%) were exclusively based on enhancing e-
learning through RS. Studies like [10] proposed e-learning recommender system with
the goal of helping students ﬁnding learning materials they need to study.
Few studies have been focusing on using RS for vocational and educational
training. As shown in Table 4, only four papers, about 9%, considered some ways were
students after graduation or during their studies could be introduced to the labor
environment. Another study [11] proposed a hybrid recommender approach to support
knowledge sharing and transfer among trainees, teachers and trainers for vocational
education and training.
From the analysis and extracted data, 11 papers representing 25% of the reviewed
studies used RS to enhance the academic performance. The study [12] applied a RS to
identify students with learning deﬁciencies in assessments. The idea is through a
Fig. 1. Mapping results obtained from the combination of the ﬁrst three research questions
Recommendation Systems in Education: A Systematic Mapping Study
943

recommender system ﬁnd areas and indicators of achievement where students need to
reinforce their knowledge and therefore, identify students with poor academic
performance.
The remaining reviewed studies refer to resources and activities of the learning
process suggested by RS. Around 18% of the reviewed papers use recommender
systems on learning activities, [13] recommend online learning activities on a course
web site. Besides, a reasonable amount of papers, about 29%, refers to recommending
learning resources. Authors in [14] show a different RS with collective intelligence
which suggests academic resources on the web including, educational videos, scientiﬁc
literature, books, and all of the useful stuff for research.
3.2
Recommender System Approach
The results of RQ2 revealed that the most frequent type of recommender system used
in education is the Hybrid approach, around 46% of the papers reviewed are based on a
combination of different recommender types. The main choice of a hybrid approach is
to improve the performance of the recommender model and overcome the problems of
other types of recommender like collaborative and content based ﬁltering.
Collaborative ﬁltering (CF) accounts for around 30% of the reviewed papers. In CF
the recommendation is assigned based on existing relations between users and items.
Studies that apply the CF approach do not require content or any information about
items, rather CF produce personalized recommendations, because they consider other
people’s similar experience, therefore they can suggest appropriate academic or edu-
cational matters by observing same people’s behavior.
A small amount, 2% refer to using a Content based (CB) approach which aims to
ﬁnd similarities by the items’ properties. Moreover, less than 2% of the reviewed
papers are based on a knowledge approach (see Fig. 1). Studies in this ﬁeld suggest
educational matters based on inferences about students or professors needs and
preferences.
Other approaches of recommendations represent the rest 20%. Studies reviewed in
this section consider other algorithms and classiﬁcation schemes for the recommen-
dation. The study [15] proposes a RS that exploits the knowledge, learns, discovers
new information and infers preferences by using the knowledge from Smart Classroom.
3.3
RS Development Platform
The Web is the most adopted platform for using RS in education; it is reported by the
52% of the reviewed papers. As displayed in, Fig. 1 web-based platforms are present in
many areas of education supporting different educational scenarios. In [16] authors
describe an educational RS developed on a web platform, which recommends learning
programming activities in a personalized way, i.e., according to the user proﬁle. The
web-based platform is named Wise Coach and enables the user to solve problems
according to their programming level.
Other primary studies proposed different platforms for RS: based on desktops
applications and mobile applications. The desktop-based platform was included in
around 7% of the reviewed papers. Another study [17] describes the design of a
944
A. C. Rivera et al.

stand-alone desktop platform that uses local information stored in databases and logs.
Mobile based accounts for the 2% of the papers selected. [18] is an example where
authors have designed and developed an android app which will recommend the
graduate admission seekers to apply for suitable graduate schools.
Furthermore, there are 34% of the reviewed papers which propose some model or
architecture. These studies do not develop or deploy a recommendation system and
rather they explain possible conceptual models where recommendation systems can be
based on. Some examples of this kind of study are presented in [19, 20].
3.4
Recommendation System Evaluation
The results for RQ4 revealed that 31% of the studies did not conduct any type of
validation of the method (see Table 4). Around 18% of the studies presented RS which
had been validated through a survey. For instance, [16] proposed a questionnaire for
evaluating e-learning applications. Students answered a survey that aimed to validate
the recommendations received, results obtained from the empirical evaluation allowed
enhancement on the educational RS to be developed for it to be more reliable.
Around 47% of the papers report some kind of experimentation. For instance, [21]
performed an experimentation to evaluate the effectiveness of a recommender system.
Moreover, just one research representing 2% reported a case study. [22] conducted a
case study to validate the proposed hybrid recommender system that demonstrates the
effectiveness of using hybrid approaches in virtual learning environments.
3.5
Issues Addressed by the RS
One of the main issues addressed by RS is how to provide personalized recommen-
dations. It accounts 43% of the papers reviewed. Some studies aim to give tailored
suggestions to students and academics.
RS that solve the problems of Availability of information are presented in 31% of
the selected papers. Studies like [4, 7, 9, 18] make available the scattered information
of different online courses, universities, faculties, study plans, etc. in a single repository
which is accessed through the RS.
Prediction accuracy and efﬁciency is addressed in about 29% of the reviewed
papers. When implementing and building a RS, some studies test the system to provide
the most accurately prediction using performance metrics like recall and precision with
the idea to enhance the RS model.
Improve educational practices accounts for the 25% of reviewed papers. In those
papers, the recommendation is targeted to a speciﬁc learning context, to engage stu-
dents through the recommendation of educational resources.
4
Conclusions
Overall, there has been signiﬁcant interest in the use of recommendation systems in
educational scenarios. However, due to limited information about how recommenda-
tion systems have been used in education environments, what approaches are used and
Recommendation Systems in Education: A Systematic Mapping Study
945

how they addressed different issues in the academic world, many researchers and
practitioners are experiencing difﬁculties in retrieving relevant and useful information
that summarize the utilities of using RS for educational purposes.
Therefore, in this work, we performed a systematic mapping study to investigate
the use of recommendation systems in education. As the result of an automatic search
on scientiﬁc databases, 44 primary studies were identiﬁed and thoroughly reviewed.
From selected studies, relevant data was extracted and classiﬁed to get valuable insights
about uses, approaches, and challenges addressed by RS. There are several uses of RS
in education, the most reported is helping on academic choices, assisting in suggesting
courses, or simply e-courses, research documents’ management, course complementary
materials, resources and academic activities. Another area of education enhanced by RS
is the e-learning education through web platforms.
Moreover, this work has also been useful to detect some research gaps and key
areas where further investigation should be performed, like the introduction of data
mining and artiﬁcial intelligence in recommender system algorithms to improve per-
sonalization of academic choices.
Furthermore, most of the existing recommendation approaches do not consider
differences in the learner proﬁle and characteristics. This issue can be addressed by
introducing additional information about the student into the recommendation process
and applying hybrid approaches that combine knowledge from learners. Although in
this systematic mapping study we have not detailed possible drawbacks and gaps from
the implementation of RS for education, as future work we intend to extend our study
to improve the available knowledge of implementing and deploying models of RS
within the educational context.
References
1. Kitchenham, B., Pearl Brereton, O., Budgen, D., Turner, M., Bailey, J., Linkman, S.:
Systematic literature reviews in software engineering – a systematic literature review. Inf.
Softw. Technol. 51, 7–15 (2009)
2. Petersen, K., Vakkalanka, S., Kuzniarz, L.: Guidelines for conducting systematic mapping
studies in software engineering: an update. Inf. Softw. Technol. 64, 1–18 (2015)
3. Meryem, G., Douzi, K., Chantit, S.: Toward an e-orientation platform (2016)
4. Iyengar, M., Sarkar, A., Singh, S.: A collaborative ﬁltering based model for recommending
graduate schools. In: 2017 7th International Conference on Modeling, Simulation, and
Applied Optimization, ICMSAO 2017 (2017)
5. Iru, E.V.: HRSPCA: hybrid recommender system for predicting college admission, pp. 107–
113 (2012)
6. Bokde, D.K., Girase, S., Mukhopadhyay, D.: An approach to a university recommendation
by multi-criteria collaborative ﬁltering and dimensionality reduction techniques. In:
Proceedings of the 2015 IEEE International Symposium on Nanoelectronic and Information
Systems, iNIS 2015, pp. 231–236 (2016)
7. Tan, H., Guo, J., Li, Y.: E-learning recommendation system. In: 2008 International
Conference on Computer Science and Software Engineering, pp. 430–433 (2008)
946
A. C. Rivera et al.

8. Cho, J., Kang, E.Y.: Personalized curriculum recommender system based on hybrid ﬁltering.
In: Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics. LNCS, vol.
6483, pp. 62–71 (2010)
9. Pinto, F.M., Estefania, M., Cerón, N., Andrade, R.: iRecomendYou: a design proposal for
the development of a pervasive recommendation system based on students proﬁle for
Ecuador’s students’ candidature to a scholarship. In: New Advances in Information Systems
and Technologies, vol. 445, pp. 537–546 (2016)
10. Ansari, M.H., Moradi, M., Nikrah, O., Kambakhsh, K.M.: CodERS: a hybrid recommender
system for an E-learning system. In: Proceedings - 2016 2nd International Conference of
Signal Processing and Intelligent Systems, ICSPIS 2016, pp. 14–15 (2017)
11. Kong, X., Boll, S., Heuten, W.: Towards recommender systems supporting knowledge
sharing and transfer in vocational education and training. In: 2013 Second International
Conference of E-Learning E-Technologies Education, pp. 25–30 (2013)
12. Ibarra, M.J., Serrano, C., Navarro, Á.F.: Recommender system to identify students with
learning deﬁciencies in assessments. In: 2016 International Symposium on Computers in
Education, SIIE 2016. Learning Analytics Technologies (2016)
13. Zaiane, O.: Building a recommender agent for e learning systems. In: Computers in
Education, Citeulike.Org, pp. 55–59 (2002)
14. Zhou, J., Luo, T., Lin, H.: A novel recommendation system with collective intelligence. In:
Proceedings - 2010 IEEE 2nd Symposium on Web Society, SWS 2010, 151–157 (2010)
15. Valdiviezo-Díaz, P., Aguilar, J., Riofrio, G.: A fuzzy cognitive map like recommender
system of learning resources. In: 2016 IEEE International Conference on Fuzzy Systems,
FUZZ-IEEE 2016, pp. 1539–1546 (2016)
16. Soldatova, E., Bach, U., Vossen, R., Jeschke, S.: Creating an e-learning recommender
system supporting teachers of engineering disciplines. In: 2013 International Conference on
Interactive Collaborative Learning, ICL 2013, pp. 811–815 (2013)
17. Liu, J., Wang, X., Liu, X., Yang, F.: Analysis and design of personalized recommendation
system for university physical education. In: 2010 International Conference on Networking
and Digital Society, ICNDS 2010, vol. 2, pp. 472–475 (2010)
18. Hasan, M., Ahmed, S., Abdullah, D.M., Rahman, M.S.: Graduate school recommender
system: assisting admission seekers to apply for graduate studies in appropriate graduate
schools. In: 2016 5th International Conference on Informatics, Electronics and Vision,
ICIEV 2016, pp. 502–507 (2016)
19. de Vries, P., Oinas-Kukkonen, H., Siemons, L., Beerlage-de Jong, N., van Gemert-Pijnen,
L.: Persuasive Technology: Development and Implementation of Personalized Technologies
to Change Attitudes and Behaviors, vol. 10171, pp. 227–239 (2017)
20. Li, J.Z.: Quality, evaluation and recommendation for learning object. In: ICEIT 2010 - 2010
International Conference on Educational and Information Technology, Proceedings, vol. 2,
pp. 533–537 (2010)
21. Hoic-Bozic, N., Holenko Dlab, M., Mornar, V.: Recommender system and web 2.0 tools to
enhance a blended learning model. IEEE Trans. Educ. 59, 1 (2015)
22. Rodríguez, P.A., Ovalle, D.A., Duque, N.D.: A student-centered hybrid recommender
system to provide relevant learning objects from repositories, pp. 291–300 (2015)
Recommendation Systems in Education: A Systematic Mapping Study
947

Looking for Usability and Functionality Issues:
A Case Study
Karina Jiménes, Jhonny Pincay(&), Mónica Villavicencio,
and Alberto Jiménez
Facultad de Ingeniería en Electricidad y Computación, Escuela Superior
Politécnica del Litoral, ESPOL, Campus Gustavo Galindo Km 30.5 Vía
Perimetral, P.O. Box 09-01-5863, Guayaquil, Ecuador
{kbjimene,jvpincay,mvillavi,albjimen}@espol.edu.ec
Abstract. Looking for quality issues in a system can be a very demanding
activity. In this article, we propose an approach based on text mining techniques
to quickly identify usability and functionality drawbacks in a learning manage-
ment system - LMS. The techniques were performed to 421 comments written by
university students who frequently use a LMS. Results indicate that a dendro-
gram is a suitable tool to have a quick look of the issues faced by LMS’ users as
well as their expectations about new functionalities that the system should pro-
vide. By using these techniques, we identiﬁed more than ten usability issues and
the need for seven new functionalities to be implemented in the system.
Keywords: Software engineering  Text mining  Dendrogram
Usability  Functionality  LMS
1
Introduction
The software engineering ﬁeld concerns the development process as well as the quality
of a software product. Among the commonly used products are the Learning Man-
agement Systems (LMS), also known as e-learning platforms. In this article, we present
how a LMS was analyzed by using both qualitative and quantitative techniques to
discover usability and functionality issues. Usability issues refer to actions that are
misperformed by the system while functionality issues are functions or capabilities that
the system lacks and students expect them to be implemented.
LMS are online platforms available 24 h that provide a communication channel
between teachers and students, allowing: the sharing of contents and assignments, as
well as, the tracking of students’ performance [1, 10, 14]. The LMS under analysis is
used in an Ecuadorian university (Escuela Superior Politécnica del Litoral - ESPOL)
and is a customized version of CANVAS, an open-source platform developed by
Canvas Instructure Company.
Every software product should be functional (i.e. practical and useful) evidencing
quality attributes. According to Kiget et al. [14], every LMS shall contribute positively
to the learning experience of their users and must not cause frustration, confusion or
make students to lose focus. The quality of a LMS can be measured by considering
several dimensions such as usability, accessibility, and reliability [1, 14, 26].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_90

The usability is the dimension that inﬂuences the quality of an e-learning platform the
most [1]; it may also affect the learning experience of the users [18]. In this sense, it is
imperative that all LMS products have an acceptable usability degree.
The standard ISO/IEC 25010 deﬁnes usability as the “degree to which a product or
system can be used by speciﬁed users to achieve speciﬁed goals with effectiveness,
efﬁciency and satisfaction in a speciﬁed context of use.” [11]. The study of usability
issues in e-learning platforms has become a research ﬁeld aiming to ﬁnd methods to
recognize those issues, how to leverage their effects in the users’ experience and to
discover new functionalities. A good usability degree may improve the user engage-
ment and the overall learning experience [13, 24].
The methods used to evaluate usability have evolved from rigorous to less complex
approaches, including experimental psychology and qualitative analysis, respectively
[7]. When it comes to discover usability issues, the qualitative approach is one of the
most commonly used methods. Usually, in this kind of analysis, users’ comments and
observations are evaluated and categorized by experts, in order to get an insight of their
experience using the platform and to estimate its usability degree [7]. One of the
drawbacks of this kind of analysis is that it requires a considerable amount of time and
people with some expertise. To tackle these issues, some authors have explored the use
of natural language processing (NLP) and text mining techniques to evaluate usability
[8, 19, 22], since they allow the gathering of relevant information from the text being
reviewed, based on the frequency of appearance of certain terms. Such techniques can
process a large amount of comments, and in certain cases, more reliable results can be
obtained [12, 20, 23].
In this work, we present a case study (a LMS system) in which qualitative analysis
in conjunction with a text mining approach -using several methods- were applied to
discover usability and functionality issues. The text-mining techniques conﬁrmed the
issues identiﬁed during the qualitative analysis. Therefore, these techniques may be
considered useful for the identifying issues in systems where users are able to write
comments about their experiences with such systems.
The rest of the article is structured as follows: Sect. 2 presents related works;
Sect. 3 describes the methodology of the study; Sect. 4 presents the results and its
analysis respectively; and Sect. 5 concludes the article.
2
Related Works
Regarding the LMS usability evaluation, a variety of methods can be applied,
including: experts’ examination; qualitative and quantitative analysis of data provided
by users; and observational evaluation can be applied [7, 8, 13].
Particularly, analyzing the user experience through qualitative approaches is a ﬁeld
that has been widely explored. Alturki et al. [3] performed a study to evaluate the usability
and accessibility of the LMS Blackboard. They interviewed and applied questionnaires to
professors and staff of a university in Saudi Arabia to collect information about their
perception and overall experience using Blackboard. As a result, the researchers found
that customization of functionalities and language was needed to increase the users’
willingness to use the system although it was perceived as accessible and usable.
Looking for Usability and Functionality Issues: A Case Study
949

The application of qualitative methods to evaluate usability might result costly in
terms of time and human resources [8, 19]. That is why, several authors have opted for
applying quantitative evaluations using automatic techniques to analyze big amounts of
users’ opinions, expressed in natural language. One example is the work conducted by
Al Mazroui [2] who evaluated the feasibility of applying text mining techniques in the
context of e-learning. According to his ﬁndings, it is possible to determine the use-
fulness of an e-learning system through the automatic analysis of students’ opinions.
Another related initiative was conducted by El-Halees [8], in which a usability model
evaluation system -based on opinion mining (a subﬁeld of text mining)- was proposed.
The objective was to automatically classify a set of 565 reviews into positive or
negative. The system achieved an accuracy of 85%.
Regarding the Canvas LMS usability, Thacker et al. [25] conducted an study in
which a group of ﬁve faculty members were observed and tracked while performing
several tasks using Canvas. It was found that Canvas is perceived as an easy-to-use tool
that has a good interface design and makes an adequate use of metaphors. Yet, it
presents some problems when it comes to provide appropriate feedback and help
documentation, as well as to help the user to recover from errors.
The efforts described above have been oriented to evaluate usability of LMS
platforms through either quantitative or qualitative approaches. The present study
builds upon previous works, by performing a qualitative analysis of users’ opinions
regarding their experience using a LMS. In addition, a quantitative analysis, based on
text mining, was performed to conﬁrm the ﬁndings. This way, we propose the use of a
number of text mining techniques to identify usability and functionality issues based on
the analysis of written comments regarding a system. Certainly, using text mining
techniques allow having a quick look of issues in a system.
3
Methodology
We performed qualitative and quantitative analyses to identify the usability issues that
students face when using the university LMS, as well as to discover desirable
functionalities.
Qualitative analysis is effective when obtaining speciﬁc information regarding
values, opinions and behaviors and social context of particular groups is required [26].
Moreover, this type of analysis allows to perform exploratory research through
open-ended questions, giving participants the possibility of naturally expressing their
opinions, instead of forcing them to choose from a narrow set of alternative answers.
Accordingly, this study followed a content analysis approach.
The input used to perform the qualitative analysis was a set of comments written by
ESPOL university students, gathered through an electronic survey administered
through the university LMS. Students were asked to write their opinions about their
experiences using the LMS features, as well as the frequency of accessing it to check
for updates and new contents. Their answers were submitted in Spanish.
To identify the usability problems experienced by students while interacting with the
LMS, their comments were categorized by following the Nielsen’s Heuristics for User
Interfaces [17]: (1) visibility of system status; (2) match between system and the real
950
K. Jiménes et al.

world; (3) user control and freedom; (4) consistency and standards; (5) error prevention;
(6) recognition rather than recall; (7) ﬂexibility and efﬁciency of use; (8) aesthetic and
minimalist design; (9) help users recognize, diagnose and recover from errors; (10) help
and documentation. Nielsen’s Heuristics were chosen as they are rather simple to apply
with rules of thumb that have been proved. In addition, several works have been based
on them, aiming to provide heuristics speciﬁc for LMS [9, 15, 24].
For the quantitative analysis, text mining techniques were applied to the comments
given by students. This approach is a variation of the think aloud methodology, where
the subjects express their thinking while performing well-deﬁned tasks [16]. These
techniques allow to discover relevant topics from text written in natural language [5],
which can be compared with the results obtained from the qualitative analysis as to
reinforce or reject the ﬁndings [8, 9]. The corpus used for quantitative analysis contains
the comments, each comment was considered as a single document. As recommended
by Krzysztof [6], we performed a text pre-processing, with the idea of removing
irrelevant information. The following steps were performed: (1) tokenization, where
each document was represented as a set of words; (2) stop words removal in which,
terms such as interjections, articles, conjunctions and auxiliary verbs, irrelevant for the
analysis are excluded from the documents; (3) spelling correction and synonym
replacement, through which spelling mistakes were automatically corrected and the
synonym replacement was manually done. We identiﬁed the most common term to
replace it for its equivalent meaning (e.g. “task, homework” were replaced for “as-
signment”); and (4) stemming, which involves taking out preﬁxes and sufﬁxes from the
words [20].
Afterwards, the most signiﬁcant terms of the whole corpus were identiﬁed through
the TF*IDF (Term Frequency-Inverse Document Frequency) model, which assigns a
weight to each term based on its relevance among a collection of documents [4, 21].
With the resulting weights per term, a Document Term Matrix (DTM) was obtained, in
which each column denotes the terms, and each element of the matrix represents the
TF*IDF weight of the term within a document. With the DTM matrix, the most
frequent terms mentioned into the documents were identiﬁed [13]. A threshold sparsity
of 93% was set to retain a considerable number of terms, since the sparsity is a measure
of relative document frequency, above which the term will be removed (e.g. if the
sparsity is 93% means that we are keeping the terms that appear at least in 7% of the
documents).
Subsequently, the Ward’s hierarchical cluster analysis, with Euclidean distance,
was performed on the DTM matrix in order to identify the semantic relationships
between relevant terms. With these results, a dendrogram (a tree-like diagram) was
drawn to identify terms closely related among all the comments.
4
Results
From the online survey, we gathered 439 students’ responses with comments. Out of
them, 18 were not considered as valid comments as they included special characters or
words such as none, nothing, or I don’t know. By doing so, 421 valid comments were
considered, 35.4% from female and 64.6% from male students. The sample included
Looking for Usability and Functionality Issues: A Case Study
951

students attending their degree programs as follows: 39.9% (i.e. 168) in their ﬁrst and
second semester of study; 26.8% in the third and fourth semester; 17.1% in the ﬁfth and
sixth semester; and, 16.2% in the remaining semesters. Table 1 shows the frequency of
use of the LMS with respect to the semester at which students were in their study
program.
According to Table 1, 53.2% of students, who participated in this study, use the
LMS many times a day (i.e. 224 out of 421). Interesting to observe is that 23.8% of
students uses the system several times a week, while 20.2% does it once a day. Also, it
should be noticed that students attending the ﬁrst semesters of their undergraduate
programs tend to use the LMS more frequently than the rest.
The results obtained from the qualitative and quantitative analysis are presented
next.
4.1
Qualitative Analysis
For this purpose, the research team performed a content analysis by reading all the 421
comments and categorizing them according to the Nielsen’s heuristics. Table 2 pre-
sents examples of usability issues and the new functionalities expected by students.
This table has three columns: the ﬁrst corresponds to the heuristic under analysis; the
second shows some examples of students’ comments translated to English, reﬂecting
usability issues in the LMS; and the third column presents issues with regard to the
functionalities required by students and the ones that the LMS lacks. These func-
tionalities are expressed as user stories (i.e. functional requirements in the Scrum
argot).
The ﬁndings revealed that the LMS has problems with seven out of ten usability
heuristics. In addition, seven new functional requirements were identiﬁed as well as the
need for a mobile app.
Regarding the usability issues, the main ﬁndings are presented next:
Visibility of the LMS status: Students’ comments indicate that the platform does not
properly inform the user about a given action. In the ESPOL’s LMS, the lack of
Table 1. Frequency of use of the LMS by students
LMS frequency of use Semester attended by
students
Total
1–2 3–4 5–6 7–10
Several times a month
0
1
2
3
6
Once a week
3
1
1
1
6
Several times a week
36
25 15
24
100
Once a day
39
20 14
12
85
Many times a day
90
66 40
28
224
Total
168 113 72
68
421
952
K. Jiménes et al.

Table 2. Examples of usability and functionality issues found in a LMS
Heuristics
Usability issues
Functionality issues
Visibility of the LMS
status
When the system fails, I do not
know if my homework was sent
or not
Match between
system and the real
world
The courses to which the
announcements belong should
appear with the course name
(e.g. Programming
Fundamentals) instead of the
code (e.g. FIEC04341)
As a student, I want to have a
timer to know the time I have
left during an exam or quiz
As a student, I want to be able to
chat with my teacher and
classmates through the LMS
The system should have a timer
to know exactly how much time
we have left in a quiz or
evaluation test
When the deadline of a task has
expired, we can no longer
visualize the instructions of the
task
Error prevention
The system does not allow me
to delete a ﬁle (wrong or old) to
upload it again
As a student, I want to be able
to delete ﬁles that I do not need
anymore
When I remove elements from
the wall, they disappear;
however, when I reload the
page, they are still there
Recognition rather
than recall
The messages and
announcements that we receive
should be organized as a mail
inbox; it is difﬁcult to ﬁnd them
As a student, I want to organize
my announcements and
messages using certain criteria
(e.g. date, alphabetical order)
As a student, I want to search
announcements and messages
using certain criteria (e.g. date,
alphabetical order)
As a student, I want to receive a
notiﬁcation to my mobile when a
new assignment is posted by the
teacher
The system should highlight the
new assignments that a teacher
has posted or should send a
notiﬁcation (like Facebook) to
warn us about the new
assignment
For me, it is not clear how to
upload a homework, the
interface has to be improved
Flexibility and
efﬁciency of use
The ﬁles in the “planning”
section should be downloaded
directly
As a student, I want to
synchronize the LMS calendar
with my Google calendar
The calendar of the LMS should
be synchronized with my
Google calendar
(continued)
Looking for Usability and Functionality Issues: A Case Study
953

feedback in the “Assignments” module causes discomfort to students as they are not
informed whether their homework was successfully uploaded.
Match between the LMS and the real world: Students indicated that the platform
presents the code but not the name of the courses when notiﬁcations are sent. Currently,
the system only allows to receive the notiﬁcations by means of the LMS messages or
by electronic mails. Unfortunately, students rarely use the email system; they prefer to
receive notiﬁcations directly through their mobile devices.
Error prevention: Students mentioned that the LMS does not prevent them from
making errors since conﬁrmation options do not appear before performing an action.
Recognition rather than recall: The students’ comments evidence that several LMS
options are not always visible; hence, some activities turn out to be more complex than
they are, for example, uploading a homework ﬁle or ﬁnding a course announcement.
Flexibility and efﬁciency of use: We found that students experienced navigation
problems because several clicks are needed to perform an activity, for instance,
downloading a ﬁle. Students also indicated the need for synchronize the calendar of the
LMS with that of Google, which could prevent them from using two calendars.
Aesthetic and minimalist design: In this regard, some students expressed their
feeling of nonconformity with the design of the platform. That is, the LMS shows
information that is irrelevant or not useful, for example, the ads appearing on their
walls (these are often repeated, there are many ads and some of them may be too old).
Help users recognize, diagnose, and recover from errors: According to students,
when an error occurs while loading a ﬁle (task), they receive no warning.
Regarding the functionality issues, we identiﬁed seven new functionalities based on
the complaints written by students. A frequent comment by students is related to the
need for a mobile application for the LMS.
Table 2. (continued)
Heuristics
Usability issues
Functionality issues
Aesthetic and
minimalist design
I would like the design of the
system to be more modern and
friendly
Please, do not show so many
past ads
Help users recognize,
diagnose, and recover
from errors
I sent a task, it appeared as
“delivered” but the teacher
could not download it. The error
displayed was “ﬁle not
uploaded correctly”, but I never
received a notiﬁcation
When an error occurred while
sending a document, the
systems has to notify both the
teacher and the student
954
K. Jiménes et al.

4.2
Quantitative Analysis
The corpus for this analysis is composed of 1533 terms contained in the 421 comments.
After the pre-processing stage, the number of terms was reduced to 703 (46.5%), from
which the most frequent terms were obtained, as well as their correlation with key
nouns. In addition, a hierarchical clustering by the similarity of terms was performed.
To determine the most frequent terms, we analyzed the density of terms’ weight
produced by the TD-IDF model. Twenty three terms (3.3%) have a weight greater than
110, whereas 680 terms (96.7%) lower. Figure 1 shows the most frequent terms with a
weight higher than 110. For the sake of space, we only present 15 terms: LMS,
assignment, mobile, allow, announcement, course, improve, send, ﬁle, notiﬁcation,
visualize, application, student, professor and chat.
These terms give us a clear idea of the subjects that students mention most fre-
quently into their comments. However, the terms alone do not provide a complete
picture to get understanding about the usability issues faced by students. Therefore, the
need for looking associated terms arose. Associated terms have a correlation, which is a
quantitative measure, between 0 and 1, of the occurrence of words in several com-
ments. In this respect, whether two terms always appear together, then the correlation is
1.0. Table 3 presents the most frequent terms and a set of them that correlate with a
value greater than 0.20. Normally, terms with correlation higher than 0.60 are chosen,
but we did not get such values. We set a threshold of 0.20 to choose the relevant terms,
because we considered that the terms with that correlation level still provide an idea
about the issues the students ﬁnd in the platform.
This association of terms put in evidence some usability problems with the LMS,
for example: using the LMS is annoying and training is needed; it is not possible to
upload or download a ﬁle with one click; when a new assignment is posted by the
teacher, students would like to receive a notiﬁcation; new announcements should be
highlighted; etc. Moreover, by looking at this table, it becomes apparent the need for
having a mobile application (correlation = 0.61) as well as a chat service to facilitate
the communication with the teacher and classmates.
Fig. 1. The most frequent terms
Looking for Usability and Functionality Issues: A Case Study
955

Table 3. Words associated with key terms
Frequent term
Correlations
LMS
home (0.27), reload (0.26), training (0.25), responsive (0.23), portable
(0.21)
assignment
send (0.33), classify (0.31), group (0.31), announcement (0.25), notiﬁcation
(0.25)
mobile
application (0.61), browse (0.22), device (0.22), create (0.20)
allow
ﬁle (0.30), download (0.28), differentiate (0.26), visualize (0.25), search
(0.20)
announcement
address (0.37), important (0.37), group (0.33), long (0.28), highlight (0.26)
course
code (0.43), sequence (0.37), tutorial (0.37), name (0.35), distinguish (0.35)
improve
label (0.35), outdated (0.34), gadget (0.30), aspect (0.27), efﬁciency (0.27)
send
ﬁnal (0.34), assignment (0.33), message (0.30), read (0.25), necessary (0.20)
ﬁle
upload (0.47), click (0.39), size (0.36), download (0.34), save (0.34),
disappear (0.31), allow (0.30), delete (0.26), timeline (0.24)
notiﬁcation
help (0.31), current (0.28), assignment (0.25), deliver (0.22), highlight
(0.20)
visualize
allow (0.25), professor (0.23), author (0.22), automatically (0.22), course
(0.21)
application
mobile (0.61), create (0.29), versatile (0.22)
student
conﬁdence (0.53), sequence (0.53), tutorial (0.53), professor (0.45), search
(0.43)
professor
conﬁdence (0.46), sequence (0.46), tutorial (0.46), student (0.45), ask (0.45)
chat
online (0.38), write (0.23), whatsapp (0.23)
Fig. 2. Dendrogram of relevant terms created by hierarchical clustering by the Ward method,
using a 93% dispersion
956
K. Jiménes et al.

Another way to see how the terms are grouped is through hierarchical clustering.
We performed a hierarchical agglomerate clustering which results are shown in Fig. 2.
This ﬁgure represents the semantic relation of the most frequent terms in a dendrogram.
From this tree diagram, three clusters are clearly observed in red: (1) mobile appli-
cation, (2) chat, and (3) LMS. Clusters 1 and 2 refer to expectations or needs that
students have regarding the systems (functionalities that are not currently available).
The cluster 3 groups the main usability problems experienced by students with the
LMS. In this group, a number of verbs suggests changes in the LMS (e.g. to improve,
to allow, to visualize, to realize, to send).
5
Conclusions
In this article, we report how we performed a qualitative and a quantitative analysis
based on text mining techniques to a set of comments provided by students. The aim
was to see in what extent text mining techniques are suitable to identify functionality
and usability issues of a system based on written comments.
From the qualitative analysis, we identiﬁed problems faced by students when
interacting with the e-learning platform, as well as functionalities the system lacks.
Applying text mining techniques allowed us to conﬁrm the ﬁndings of the qualitative
analysis. We found relevant terms used by students to express their opinions regarding
the LMS, as well as correlations between terms. These results provide evidence the
usability issues that students would like to be solved. Also, they gave us insights about
the system’s functionalities as expected by students. Furthermore, performing a cluster
analysis to the relevant terms let us visualize how strong the semantic relations among
terms are. Also, it allows to clearly identify the drawbacks of the system.
The results presented in this article provide evidences that the use of text mining
techniques together with cluster analysis are powerful tools. They facilitate the search
of usability issues of a software system and the disclosure of users’ expectations about
new functionalities, without spending large amount of resources. Although the ﬁndings
presented in this work are useful, caution should be paid when using text mining
techniques as the human’s judgement will always be advisable to make a proper
interpretation of the results based on the context the analysis is performed.
References
1. Alla, M., et al.: The impact of system quality in e-learning system. J. Comput. Sci. Inf.
Technol. 1(2), 14–23 (2013)
2. ALMazroui, Y.A.: A survey of Data mining in the context of E-learning. Int. J. Inf. Technol.
Comput. Sci. 7(3), 8–18 (2013)
3. Alturki, U.T., et al.: Evaluating the usability and accessibility of LMS “Blackboard” at King
Saud University. Contemp. Issues Educ. Res. Online 9(1), 33 (2016)
4. Azam, N., Yao, J.: Comparison of term frequency and document frequency based feature
selection metrics in text categorization. Expert Syst. Appl. 39(5), 4760–4768 (2012)
5. Cheng, B., et al.: Research on e-learning in the workplace 2000–2012: a bibliometric
analysis of the literature. Educ. Res. Rev. 11, 56–72 (2014)
Looking for Usability and Functionality Issues: A Case Study
957

6. Cios, K.J., et al.: Data Mining: A Knowledge Discovery Approach. Springer Science &
Business Media, New York (2007)
7. Dumas, J.S., Fox, J.E.: Usability testing: current practice and future directions. Hum.-
Comput. Interact. Dev. Process. 231, 1–7 (2009)
8. El-Halees, A.M.: Software usability evaluation using opinion mining. JSW 9, 2 (2014)
9. Freire, L., et al.: A literature review about usability evaluation methods for e-learning
platforms (2012). https://doi.org/10.3233/WOR-2012-0281-1038
10. ISO/IEC: ISO/IEC 25010:2011 - System and Software Quality Models. https://www.iso.org/
obp/ui/#!iso:std:35733:en
11. Joachims, T.: A probabilistic analysis of the Rocchio Algorithm with TFIDF for text
categorization. DTIC Document (1996)
12. Junus, I.S., et al.: Usability evaluation of the student centered e-learning environment. Int.
Rev. Res. Open Distrib. Learn. 16, 4 (2015)
13. Kiget, N.K., et al.: Evaluating usability of e-learning systems in universities. Int. J. Adv.
Comput. Sci. Appl. 5, 97–102 (2014)
14. Masood, M., Musman, A.: The usability and its inﬂuence of an e-learning system on student
participation. Procedia-Soc. Behav. Sci. 197, 2325–2330 (2015)
15. Murtagh, F., Legendre, P.: Ward’s hierarchical clustering method: clustering criterion and
agglomerative algorithm. arXiv Preprint arXiv:11116285 (2011)
16. Nielsen, J.: Designing Web Usability: The Practice of Simplicity. New Riders Publishing,
Indianapolis (1999)
17. Orfanou, K., et al.: Perceived usability evaluation of learning management systems:
empirical evaluation of the system usability scale. Int. Rev. Res. Open Distrib. Learn. 16, 2
(2015)
18. Oztekin, A., et al.: A machine learning-based usability evaluation method for eLearning
systems. Decis. Support Syst. 56, 63–73 (2013)
19. Pincay, J., Ochoa, X.: Automatic classiﬁcation of answers to discussion forums according to
the cognitive domain of blooms taxonomy using text mining and a Bayesian classiﬁer. In:
Proceedings of EdMedia Conference, pp. 626–634 (2013)
20. Salton, G., Buckley, C.: Term-weighting approaches in automatic text retrieval. Inf. Process.
Manage. 24(5), 513–523 (1988)
21. Shah, F., Pfahl, D.: Evaluating and improving software quality using text analysis
techniques-a mapping study. In: REFSQ Workshops (2016)
22. Shen, C., et al.: Integrating clustering and multi-document summarization by bi-mixture
probabilistic latent semantic analysis (PLSA) with sentence bases. In: AAAI (2011)
23. Thacker, J., et al.: Learning Management System Comparative Usability Study (2014)
24. Tripathy, P., Tripathy, P.K.: Fundamentals of Research: A Dissective View (2015)
25. Zhang, Y., et al.: Learning the semantic correlation: an alternative way to gain from
unlabeled text. In: Advances in Neural Information Processing Systems, pp. 1945–1952
(2009)
26. Rocha, Á.: Framework for a global quality evaluation of a website. Online Inf. Rev. 36(3),
374–382 (2012)
958
K. Jiménes et al.

Determinants of ICT Integration in Teaching
Secondary School Agriculture: Experience
of Southern Africa (Swaziland)
Nomsa M. Mndzebele1, Mzomba Nelson Dludlu2,
and Comfort B. S. Mndebele3(&)
1 Faculty of Commerce, University of Swaziland, Kwaluseni, Swaziland
2 Ngwane Teachers Training College, Nhlangano, Swaziland
3 Faculty of Agriculture, University of Swaziland, Kwaluseni, Swaziland
mndebelecbs@uniswa.sz, mndebelecbs@gmail.com
Abstract. ICT integration into secondary school teachings in developing
countries faces challenges. Swaziland, a small country in Southern Africa is no
exception. The challenges include: teacher attitude and skill, administrative
support and limited resources. This study explored determinants of ICT inte-
gration in teaching of secondary school agriculture in Swaziland, a developing
country in Southern Africa. The study adopted a descriptive survey research
design. Population of study was a census of secondary agriculture teachers and
employed a survey questionnaire. Data analysis employed descriptive statistics,
correlations and regression. Findings established that teachers have a positive
attitude and engage in ICT integration despite the challenges. Factors
inﬂuencing ICT integration: (1) Digital proﬁciency in using ICT tools, (2) Social
inﬂuence, (3) Sharing digital content with learners, (4) Possession of personal
computer, and (5) Internet connectivity. It was recommended that continued
professional development be strengthened. Internet connectivity deserves spe-
cial attention and upliftment for uptake of ICT integration.
Keywords: ICT integration  Technology adoption
Secondary agriculture teachers
1
Introduction
In an endeavour to enhance student learning and achievement, many schools have
jumped on to the bandwagon to engage in information technology (IT). This paradigm
shift in thinking has culminated in many scholars engaged in unpacking what IT
integration in teaching-learning means in the school context. The infusion of ICT in
educational institutions has great implications for teaching and learning. The deﬁnition
of ICT integration adopted in this paper is “the use of computing devices such as
desktop computers, laptops, tablets, software, or Internet for educational purposes”
(Hew and Tan 2016, p. 2).
In modern society ICT is ever-present with over three billion people having access
to the Internet (Williams and Gift 2016); with about 8 out of 10 Internet users owning a
Smartphone, information and data are increasing by leaps and bounds. “This rapid
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_91

growth especially in developing countries, has led ICT to become a keystone of
everyday life, in which life without some facets of technology renders most clerical,
work and routing tasks dysfunctional” (Williams and Gift 2016, p. 161). Of the 4.3
billion people not yet using the Internet, 90% live in developing countries. However, it
is worth noting that Internet use is growing steadily at 6.6% globally in 2014 (3.3%)
with 8.7% in developed countries and 8.7% in developing countries, Internet users in
developing countries doubled in ﬁve years (2009-2014), with two thirds of all people
online now in the developing world (Williams and Gift 2016). In the Sub Saharan
Africa (SSA) where ICT use in teaching and learning is deemed important, the inte-
gration is not fully utilised as expected and experienced in the developed countries.
2
ICT Integration in SSA School Context
Governments, to include Swaziland, in Sub-Saharan Africa (SSA), are placing an
emphasis on teacher development as key to effectively implementing policy and cur-
ricula, using ICT to enhance teaching and learning and thus raise educational standards.
In SSA schools are increasingly being equipped with computers for teaching, learning
and administrative purposes; connectivity is improving and learner enthusiasm is on the
rise. Selected SSA countries are developing digital content in the respective subject
areas. However, the major impediment to integration in the SSA context, to include
Swaziland, is treating ICT as a discrete subject in the form of computer science or
information technology when assessed by the local examination boards. However, ICT
integration into teaching subjects, on the contrary, is more effective for learners.
While developed countries have reported up to 41% integration of ICT into teaching
and learning, the proportion remains substantially low in Africa. Singapore established
four stages for ICT integration in education: Firstly, envision the future, secondly,
develop country master plan, thirdly, implement initiatives and lastly evaluate and adapt
Government policy for educational transformation (Nchunge et al. 2013).
In Swaziland, the use of ICT in many settings is increasing rapidly. As a result, ICT
education has become basically every society’s effort to teach people under various
settings because people need valuable knowledge and skills about computing and
communications devices, software that operates them, applications that run on them
and systems that are built with them (Dlamini and Dube 2014).
3
Theoretical Framework and Literature Review
3.1
Theoretical Foundation
Perceived usefulness (PU) and perceived ease of use (PEU) are theorized to be key
determinants of ICT product use. Perceived usefulness is a primary determinant and
perceived ease of use is a secondary determinant of people’s intention to use computers
(Davis 1989). Technology Acceptance Model (TAM) points out that perceived ease of
use and perceived usefulness affects the intention to use. Perceived ease of use is
deﬁned as the extent to which a person believes that using a particular system would be
960
N. M. Mndzebele et al.

free from effort and perceived usefulness as the degree to which a person believes that a
particular system would enhance his/her job performance (Davis 1989) This paper is
about integration which connotes a sense of acceptance within the user environment.
The TAM has been applied in developing countries Ease of use is a major constraint for
the adoption and use of the Internet innovations in Kenya (Longe et al. 2010).
3.2
Teacher Characteristics
Successful integration of ICT in teaching and learning is largely depending on the
availability of ICT infrastructure and teachers’ adoption and embracing it (Mathipa and
Mukhari 2014, p. 1213). Teacher competency is a key and indispensable factor to
ensure successful use of ICT in the teaching and learning environment (p. 1213).
Teachers are expected to be digital competent to cope with the 21st century skills
enabling them to produce learners to create and share higher order thinking skills,
collaborate with other learners, and manage and control their own learning (Mathipa
and Mukhari 2014, p. 1214). One major challenge identiﬁed in developing countries
pertaining to adoption of ICT in schools is inadequate IT professionals with education
professional qualiﬁcations. To harness ICT for school purposes requires sustained
investments in supporting training of teachers. Successful integration of ICT into the
classroom depends on the ability of teachers to structure learning environments in
non-traditional ways, merging technology with new pedagogies.
3.3
School Leadership
Over and above teacher competency and positive attitude, adoption and use of ICT in
schools require visionary leadership. School leaders as well as teachers need to be
knowledgeable about the potential ICT presents in teaching and learning. If this
knowledge is missing, formulated government policies and ICT investments made in
the school, opportunities are missed to meet desired school reforms and goals
(Mingaine 2013). Information technology will easily and smoothly be implemented in
schools if the Principal actively supports, learns as well, and provides or supports
adequate continuing professional development. In essence, Principals are key deter-
minants in the implementation and integrating of ICT.
3.4
Teacher Work Load, Age, Gender, and Digital Divide
In Gode et al. (2014) teacher workload is a factor inﬂuencing adoption of ICT
infrastructure in teaching and learning. Heavy teacher workload occasioned by high
class enrolment as well as level of education to include computer prior experience
inﬂuence perceived ease and perceived ease of use (Longe et al. 2010).
Age and gender are non-manipulative factors at teacher level. Computer anxiety is
often highlighted as the fundamental problem behind the digital divide (Cooper 2006).
There are contradictions in research about the inﬂuence of gender on the use of ICT
(Abbiss 2008). Age can inﬂuence the uptake of ICT for teaching (Jones 2004). Prensky
(2001) distinguished between ICT natives, born in a digital world, and digital immi-
grants who have to learn the digital language and for whom ICT will always be a
Determinants of ICT Integration in Teaching
961

second language. Other than age and gender, the teaching subject matter inﬂuences ICT
use. School subject culture are built on deep traditions and these need to considered in
ICT usage as embedded in the school curriculum.
3.5
Teacher Adoption and Integration of ICT into Teaching
Andoh (2012) reported increased workload coupled with teaching with technology
critical to the participants. Factors reported contributing to increased workload were
constant upgrades, student emails, the learning of new skills and continuous search of
sustainable strategies. A computer coordinator noted that increased workload of
teachers was alarming; on asking them to take on board yet another task an already
overcrowded curriculum and extremely busy workday is pushing many teachers to the
limits and beyond.
Ang’ondi (2013) on teachers’ attitude and perceptions on the use of ICT in teaching
and learning in Kenya determined ﬁve themes: ICT infrastructure, knowledge and
skills, attitudes and beliefs, and the curriculum. Ang’ondi reported ICT resources were
inadequate and this was the reason most of the teachers did not have time to use the
ICT room. Teachers lacked the skills to manage an ICT integrated class. Other teachers
felt ICT integration in teaching was an additional bother to their already so huge
burden. Lastly teachers felt that school curriculum is loaded with too much work
against the backdrop of little time allocation to the extent that teachers have to ﬁnd
extra time to complete the syllabus. This naturally discouraged teachers to use ICT
which they claim is time consuming.
Muhammed (2010) reported challenges facing implementation of in agricultural
education in Nigeria were: inadequate funding, lack of motivation, poor staff devel-
opment, and inadequately trained ICT compliant teachers. ICT can be a powerful
catalyst in improving teaching and learning for all and that they should play part in
broadening access to ICT and engaging by empowering learners and teachers to see
technology in creative and innovative ways. Educators’ knowledge and willingness to
adopt ICT is often associated with sociological factors such as age and teaching
experience in using ICT. Eickelmann (2011) identiﬁed supportive and hindering factors
to a sustainable implementation of ICT in schools as the way Principals perform as
school leaders and how ICT use meets the pedagogical aims.
4
Statement of the Problem
Africa, in particular Sub Saharan Africa (SSA), does not have the infrastructure or the
skilled manpower to ICTs adoption and integration into teaching and learning.
Learning opportunities provided by the increasing use of technology in classrooms are
not being harnessed in schools (Alabi 2016, p. 138). Integration of ICT depends on the
availability of ICT infrastructure and teacher competence is key variables indispensible.
In spite of the availability of computer laboratories and other ICT tools, teachers
expressed various issues impeding the use and integration of ICT (Mathipa and
Mukhari 2014, p. 1213). In the Sub Saharan region, integration is not fully embraced
as expected and experienced in developed countries. In Swaziland adoption and
962
N. M. Mndzebele et al.

integration of ICT is at the infancy stage. Variables related to ICT integration in
teaching agriculture at secondary schools have been inadequately documented and
determined in Swaziland. The study was to answer the research question: What
variables inﬂuence ICT integration in teaching secondary school agriculture in
Swaziland?
Purpose and Objectives: The purpose of the study was to determine variables
inﬂuencing ICT integration in teaching secondary agriculture in the Swaziland context
as a developing country in Southern Africa. The objectives were to: (1) Describe ICT
integration by agriculture teachers in teaching secondary school agriculture in the
Swaziland context; (2) Identify explanatory and predictor variables of ICT integration
in teaching secondary school agriculture in the Swaziland context.
5
Methodology
Research design: The study employed descriptive survey research design. Descriptive
survey research is intended to produce statistical information about aspects of education
that interest educators and policy makers. It is a method of collecting information by
using a questionnaire. Descriptive surveys are designed to secure information on the
current status of a phenomenon. Target population: The target population for the study
was agricultural teachers (N = 312). A Census study was done, therefore, all agriculture
teachers, both Modern Agriculture and Pre-Vocational Agriculture, were included in the
study. Only 284 of 312 agriculture teachers returned usable questionnaires for analysis.
Research Instrumentation: Questionnaire was used for data collection. Development of
the questionnaire was guided by literature review and semi-structured interview data.
Survey questionnaire was validated for content relevance by a panel of ICT experts with
experience in the teaching and learning environment. Reliability of the Questionnaire
was computed after pilot tested with forty ﬁnal year Bachelor of Science in Agricultural
students at the University of Swaziland and coefﬁcient was r = .85. Data collection and
analysis: The survey questionnaires were delivered to respondents and collected on
agreed dates. All questionnaires were coded to allow follow up on non-respondents. The
Statistical Package for Social Science (SPSS) version 20.0 was used for data analysis.
An a priori probability of p  .05 was established to determine the level of statistical
signiﬁcance. Descriptive statistics were used for analysis. Correlations were computed
to establish relationships between variables. Multiple regression procedures were
employed to identify explanatory and predictor determinants of ICT integration in
teaching secondary school agriculture curriculum.
6
Findings and Discussion
Findings
Objective 1: ICT integration by agriculture teachers in teaching secondary school
agriculture
Means (M) and standard deviation (SD) values were computed for each item (Table 1).
The highest mean values were: (1) Prepare tests using Word processing (3.52),
Determinants of ICT Integration in Teaching
963

(2) Search information through Internet for use in teaching (3.44), (3) Search Internet
for journal articles (2.22), and 4) Prepare teaching timetable (schedule) on the computer
(1.99).
Overall ﬁndings indicated agriculture teachers occasionally use Word Processing to
prepare tests (3.52); Searching information through Internet for use in teaching (3.44)
and Search Internet for journal articles rarely used (2.22). Agriculture teachers were
found rarely utilising the majority of ICT tools in the teaching-learning process.
Objective 2: Explanatory and predictor variables of ICT integration in secondary
school agriculture
In Table 2 R2 values indicate the amount of variance explained by independent
variables, while R2 change values present the amount of variance explained by indi-
vidual independent variables. Beta values indicate the relative importance of the
variables in explaining or predicting variance on the dependent variable (Zwane and
Table 1. Extent of agriculture teachers engaging in ICT integration secondary school
agriculture (N = 284)
Item
Extent of
use
M
SD
1
Prepare tests using Word Processing
3.52 1.73
2
Search information through Internet for use in teaching
3.44 1.49
3
Search Internet for journal articles
2.22 1.76
4
Prepare teaching time table on computer
1.99 2.08
5
Use a digital camera to take pictures for lessons
1.20 1.52
6
Use a scanner to copy pictures or text
1.20 1.49
7
Keep enterprise records on a computer
0.99 1.54
8
Use mobile phone to disseminate information to learners 0.81 1.29
9
Prepare lesson plan using a computer
0.79 1.42
10 Access information on CD for teaching
0.71 1.16
11 Prepare power point slides for teaching
0.70 1.39
12 Use video clips to teach concepts
0.57 1.14
13 Use an overhead projector for teaching
0.50 1.17
14 Distribute information on ﬂash drive to learners
0.50 1.07
15 Give assignment to learners using Internet
0.47 1.01
16 Use e-mail to send electronic materials to learners
0.44 0.64
17 Use a tape recorder for teaching
0.20 0.68
18 Use a radio in class for teaching
0.14 0.54
Overall
1.13 1.28
Rating scale: 0 = Do not use, 1 = Very rarely (once a month), 2 = Rarely
(2–3 times a month), 3 = Occasionally (2–3 times a week), 4 = Frequently
(almost every day), 5 = Very Frequently (every day)
964
N. M. Mndzebele et al.

Dlamini 1999). Five variables were found to explain and predict ICT integration in
teaching agriculture, Key variables were: (i) Digital proﬁciency in using ICT tools in
teaching, (ii) Social inﬂuence, (iii) Sharing digital subject content with learners as
notes, (iv) Owning a personal computer, and (v) Internet connectivity in the school
(Table 2).
The cumulative variance (R2) in ICT integration explained by the independent
variables was 31%. Digital proﬁciency of the teacher explained 14% with a beta value
of .56. Adjusted R2 measures the proportion of the variation in the dependent variable
accounted for by the explanatory variable. Therefore, even though the residual of sum
of squares decreases or remains the same as new explanatory variables are added, the
residual variance does not change. Hence, adjusted R2 is considered a more accurate
goodness-of-ﬁt measure of R2. Adjusted R2 conﬁrms the independent variables account
for 31% of the variance on dependent variable (Fig. 1).
Table 2. Explanatory and predictor variables of ICT integration in secondary school agriculture
Determinants of ICT integration
R
R2
R2
change
B
B
t-value
P
1. Digital proﬁciency in using ICT
tools in teaching
.38
.14
.14
.56
.38
6.81
.00
2. Social inﬂuence
.46
.21
.07
.23
.27
5.04
.00
3. Sharing digital subject content
with learners using ICT tools
.51
.26
.05
.45
.23
4.14
.00
4. Possession of personal
computer/laptop
.53
.29
.03
.30
.17
3.23
.00
5. Internet connectivity at school
.56
.31
.02
.24
.16
3.04
.00
Constant .88
P  .05
Adjusted R2 = .29; Standard error = .15
Digital proficiency in using ICT tools in teaching 14%
Social influence
7%
Sharing digital subject content for learners in the 
computers to be accessed such as notes 5%
Owning a personal computer     3%
ICT integration 
in teaching 
agriculture        
(31%)
Internet availability at the school
2%
Fig. 1. Explanatory variables of ICT integration in teaching secondary school agriculture
Determinants of ICT Integration in Teaching
965

7
Discussion of Findings
Objective 1: ICT integration in Teaching Secondary School Agriculture: Teachers
occasionally engaged in preparing tests using Word Processing, searching information
through Internet for use in teaching, and searching for journal articles from the Internet.
These activities are occasionally (2–3 times a week) carried out. Teachers were not
frequently engaged in ICT integration related tasks in their teaching. Other ICT inte-
gration related activities rarely engage in include: (1) preparing teaching time table
using computers, (2) using digital camera to take pictures for lessons, and (3) using
scanners to copy pictures from books.
Agriculture teachers rarely (once a month) keep enterprise records on computers,
use mobile phones for dissemination of information to learners, preparing lesson plan
using computers, accessing information on CDs and preparing power point presenta-
tions. These activities are tied to the use of computers. The use of radio, audio, tape
recorder, e-mails electronic transmission materials to learners such as giving assign-
ment to learners using Internet was rarely performed. In Du Plessis (2010) ICT inte-
gration at school exist in three levels: (1) learning about computers-for basic computer
literacy, (2) learning from computers- that is, use as transmitters of knowledge, and
(3) learning with computers when learners construct knowledge by designing and
creating their own representations of knowledge through mindful and challenging
learning situated in realistic and meaningful context; this is generative use of com-
puters. Findings were consistent with the Adeyinka et al. (2007) that Word Processing
and e-mail promote communication skills; database and spreadsheet promote organi-
zational skills; and modelling software promotes understanding of Science and
Mathematics concepts; agriculture is an applied science.
Objective 2: Explanatory and Predictor Variables of ICT Integration
Digital proﬁciency in using ICT tools in teaching: This variable accounted for 14% of
the variance on ICT integration in teaching school secondary agriculture. Teachers are
expected to be conversant with ICT integration to reach a stage where learners would
create new knowledge responding to problems in their locality. ICT integration in
teaching starts with the teacher making the teaching-learning activities learner centred.
In Alazam et al. (2012) ICT integration in the classroom teachers were found to be
skilled in Microsoft Excel, digital video and animation, and simulation. The teachers’
level of ICT skill was found to be moderate. The teachers’ proﬁciency in using ICT
tools (hardware, soft-ware & networks) are consistent with Bhattacharjee and Deb
(2016) that knowledge of ICT is required in teacher education training programmes,
because it helps a prospective teacher to know the world of technology for the bet-
terment of learners. In Afshari et al. (2009) similar ﬁndings were found with high levels
of skill and knowledge proﬁciency of ICT producing higher levels of technology
integration reﬂected on positive student achievements. Afshari et al. (2009) postulated
that teachers with higher levels of ICT skill, knowledge, and tools exhibit higher levels
of technology integration.
Social inﬂuence: In this study social inﬂuence explained 7% of the variance on ICT
integration.
Social
inﬂuence
was
measured
using
administrative
support
and
966
N. M. Mndzebele et al.

community of practice. In this study administrative support was important for ICT
integration. The community of practice among members is important in promoting
frequent use of ICT tools. Teachers and learners are expected to be actively involved in
technologies that promote learning in solving problems. Kurga (2014) conﬁrmed on
social inﬂuence as a variable explaining and predicting ICT integration.
In Kurga (2014) one potentially important contextual factor that shapes technology
is perceived and used by teachers is the community of practice associated with their
subject. These ﬁndings are in line with Divaharan and Ping (2010) stating that besides
infrastructure and hardware support, support from administrators and colleagues
seemed an important motivating factor for teachers.
Computer use in the classroom is highly associated with administrative support and
peer support (Afshari et al. 2009). Faculty’s belief in their computer competence was
the greatest predictor of use computer use in the classroom. In this study social
inﬂuence was found to be one of the predictors of ICT integration in teaching sec-
ondary school agriculture. This conﬁrms teachers need support from school adminis-
trators and peers.
Sharing digital subject matter content with learners: This accounted for 5% on ICT
integration in teaching. Teachers use ICT tools to make learning materials accessible to
learners. When learners know the beneﬁts of engaging in technology, they are moti-
vated. This predictor implies that teachers must be competent users of technology to
create platforms where information sharing can be done with learners. This is in line
with Divaharan and Ping et al. (2010) stating that successful learning organizations,
schools need a curriculum focused, over-arching ICT goal to provide clear direction to
key players, namely: teachers and school administrators. This connotes the whole
school must have a plan for the integration of ICT.
Possession of a personal computer: In some schools it was determined that teachers
were making personal investments in purchasing personal computers. Owning a per-
sonal laptop accounted for 3% in ICT integration in teaching. Consistent with this, is by
Ang’ondi (2013) stating that many teachers are said to have bought their own personal
computers to and curb the problem of inadequate ICT resources in their schools.
Possession of a personal computer indicates a positive attitude towards ICT. In the
context of Swaziland, the majority of agriculture teachers are reported to own personal
computers.
Internet connectivity in schools: Internet connectivity in schools accounted for 2% in
ICT integration. Internet is mostly used by teachers and learners to search for new
information. In Nyembezi et al. (2015) there was a positive relationship between the
usefulness of Internet for learning and teachers’ willingness to interact with learners.
Internet connectivity is a major setback in Swaziland in respect of ICT integration. The
majority of agriculture teachers are in schools with no Internet connectivity. In
Swaziland, the urban secondary schools are beneﬁtting from unlimited Internet con-
nectivity. This challenge is largely caused by service provider’s not prioritizing rural
schools and concentrating on urban situated schools.
Determinants of ICT Integration in Teaching
967

8
Conclusion and Recommendations
Overall Conclusion
Teachers demonstrate some understanding of selecting appropriate hardware and
software to make a lesson more meaningful to learners. ICT integration occasionally
occurs in teaching secondary school agriculture. Agriculture teachers are moderately
engaged in ICT integration activities. They are aware that ICT integration in teaching is
not practical unless the subject matter content is digital. Agriculture teachers possess
the knowledge of using various hardware and software for integration in teaching.
As ICT integration is an instructional delivery strategy, teachers are aware of the
potential beneﬁts of using ICT in improving learning outcomes.
Extent of ICT integration in teaching secondary school agriculture
Recommendation: Teacher preparatory programmes must be redesigned to equip
prospective teachers with digitisation competencies in ICT integration. New teachers
need to demonstrate competence in creating digital learning contents to impart inno-
vations to learners. Furthermore, the in-service teacher education sub-sector should be
strengthened revamping aspects of ICT integration in teaching as an instructional
delivery strategy. In-service training should infuse components of ICT integration.
Conclusion: Digital proﬁciency: ICT integration is linked to digital proﬁciency of the
teacher in using ICTs. Agriculture teachers are expected to be proﬁcient users of
technologies. Agriculture teachers possess basic ICT integration competence acquired
from the pre-service teacher preparatory programme.
Recommendation: It is recommended that the National Curriculum Centre, producing
and publishing teaching and learning materials should be revamped to include digiti-
zation of teaching materials.
Social inﬂuence: Social inﬂuence is an explanatory and predictor of ICT integration
among the four major constructs of the Uniﬁed Theory of Acceptance and Use of
Technology (UTAUT), namely: (1) Performance expectancy, (2) Effort expectancy,
(3) Social inﬂuence, and (4) Facilitating conditions; social inﬂuence is a greater
determinant of ICT integration in teaching secondary school agriculture in the
Swaziland context. Social aspects speaking to ICT integration must be strengthened.
Parental support can be engaged to exploit the full advantages of social inﬂuence in
schools motivating teachers towards ICT integration.
Possession of a personal computer: ICT integration in teaching requires teachers to
spend time using computers searching for information thus possession of a personal
computer is of signiﬁcance. Personal computers may either be at the workplace or at
home to allow more time on the computer. Possession of a personal computer is
indicative of high level of commitment on the part of teachers.
Internet connectivity in the school: The Internet is a major element of the infras-
tructure for ICT integration. Internet is used to search for new information to support
teaching and learning. Internet connectivity was a signiﬁcant variable in explaining and
968
N. M. Mndzebele et al.

predicting ICT integration. Many schools do not have Internet connectivity to develop
innovative approaches and adopt latest technologies.
Determinants of ICT integration in teaching secondary school agriculture
Recommendations: Continuing professional development workshop or in-service
training must be redesigned to enhance teacher proﬁciency in the use of ICT tools and
integration. This will keep teachers digitally competent. This is necessitated by the
ﬁndings that digital proﬁciency in using ICTs explains and predict ICT integration in
teaching. Schools should partner with business and industry as well as development
partners to mobilize ICT resources to revamp secondary schools with computers.
Student to computer ratio should be reduced to facilitate learning and effective
acquisition of ICT skills.
School administrators must be equipped with strategies to support ICT integration
and to promote community of practice among. Community of practice among teachers
must be fostered with ICT tools availability in schools and school administrators taking
leadership role.
References
Hew, K.F., Tan, C.Y.: Predictors of Technology Integration in Secondary Schools: Evidence
from a Large Scale Study of More than 30,000 Students (2016). https://doi.org/10.1371/
journal.pone.0168547
Williams, P., Gift, P.G.: Factors affecting ICT implementation in selected secondary schools in
Chipata district. Int. J. Multi. Res. Dev. 3(10), 161–175 (2016)
Nchunge, D.M., Sakwa, M., Mwangi, W.: Assessment of ICT infrastructure on ICT adoption in
educational institutions: a descriptive survey of secondary schools in Kiambu County Kenya.
J. Comput. Sci. Inf. Technol. 1(1), 32–45 (2013)
Dlamini, M., Dube, M.: Students’ use of computers and challenges faced at the University of
Swaziland, Faculty of Agriculture. Department of Agricultural Education and Extension,
University of Swaziland, Mbabane (2014)
Davis, F.: Perceived usefulness, perceived ease of use, and user acceptance of information
technology. MIS Quart. 3(3), 319–340 (1989)
Longe, O., Boateng, R., Longe, F., Olatubosun, K.: Information communication technology
adoption among adults in South Western Nigeria: an assessment of usage-phobia factors.
Univ. Ghana Digit. Collections 10(1), 65–85 (2010)
Mathipa, E.R., Mukhari, S.: Teacher factors inﬂuencing the use of ICT in teaching and learning
in South African Urban schools. Mediterr. J. Soc. Sci. 5(23), 1213–1220 (2014)
Mingaine, L.: Skill challenges in adoption of ICT in Public Secondary Schools, Kenya. Int.
J. Humanit. Soc. Sci. 3(13), 61–72 (2013)
Afshari, M., Bakar, K.A., Luan, W.S., Samah, B.A., Fooi, F.S.: Factors affecting teachers’ use of
information and communication technology. Int. J. Instr. 2(1), 77–104 (2009)
Gode, V.O., Obegi, F.M., Macharia, A.: Factors inﬂuencing integration of information and
communication technologies in Public Primary Teacher Training Colleges in Central Region
of Kenya. J. Emerg. Trends Comput. Inf. Sci. 5(12), 968–974 (2014)
Cooper, J.: The digital divide: the special case of Gender. J. Comput. Assist. Learn. 22(5), 320–
334 (2006)
Abbiss, J.: Rethinking the “problem” of gender and IT schooling: discourses in literature. Gen.
Educ. 20(2), 153–165 (2008)
Determinants of ICT Integration in Teaching
969

Jones, A.: A Review of the research literature on barriers to the uptake of ICT by teachers. British
Educational
Communications
and
Technology
Agency
(Becta)
(2004).
http://www.
becta.org.uk
Prensky, M.: Digital natives, digital immigrants. Horiz. 9(5), 1–2 (2001)
Andoh, C.B.: Factors inﬂuencing teacher’s adoption and integration of information and
communication technology into teaching: a review of the literature. Int. J. Educ. Dev. Inf.
Commun. Technol. 8(1), 137–158 (2012)
Ang’ondi, E.K.: Teachers attitudes and perceptions on the use of ICT in teaching and learning as
observed by ICT champions. In: X World Conference on Computers in Education, pp. 1–8.
Turn, Poland (2013)
Muhammed, D.I.: Inculcating Information and Communication Technology into the Teaching of
Agricultural Education in Nigeria’s Colleges of Education: Challenges of the Moment. Kwara
State College of Education (Technical Laﬁagi) (2010)
Eickelmann, B.: Supportive and hindering factors to a sustainable implementation of ICT in
schools. J. Educ. Res. 3(1), 75–103 (2011)
Alabi, O.O.: Adoption of information and communication technologies (ICT) by agricultural
science and extension teachers in Abuja, Nigeria. J. Agric. Educ. 57(1), 137–149 (2016)
Zwane, L.M., Dlamini, B.M.: The relationship between high school students’ performance in
science and agriculture. Uniswa Res. J. Agric. Sci. Technol. 8, 72–77 (1999)
Plessis, D.: A The Introduction of Cyber Hunts as a Teaching and Learning Strategy to Guide
Towards the Integration of Computer Technology in Schools. Nelson Mandela Metropolitan
University, Port Elizabeth (2010)
Adeyinka, T., Adedeji, T., Majekodunmi, T.O., Lawrence, A., Ayodele, A.A.: An assessment of
secondary school teachers uses of ICT’s: implications for further development of ICT’s use in
Nigerian secondary schools. Turk. Online J. Educ. Technol. (TOJET) 6(3), 1–13 (2007)
Alazam, A.O., Bakar, A.R., Asmiran, H.S.: Teachers’ skills and ICT integration in the classroom:
the case of vocational and technical teachers in Malaysia. Creative Educ. 3, 70–76 (2012).
Published Online Journal
Bhattacharjee, B., Deb, K.: Role of ICT in 21st century’s teachers education. Int. J. Educ. Inf.
Stud. 6(1), 1–6 (2016)
Kurga, S.J.: The inﬂuence of teachers’ age, gender, and level of training on attitudes towards the
use of integrated E-learning approach to the teaching and learning of business studies in
Kenyan secondary schools. J. Emerg. Trends Educ. Res. Policy Stud. (JETERPS) 5(2), 190–
198 (2014)
Divaharan, S., Ping, L.C.: Secondary school-cultural context inﬂuencing ICT integration: a case
study approach. Aust. J. Educ. Technol. 26(6), 741–763 (2010)
Nyembezi, N., Bayaga, A.: Analysis of effect of expectancy on school learners’ adoption and use
of cloud computing. J. Commun. 6(1), 113–119 (2015)
970
N. M. Mndzebele et al.

Media, Applied Technology and
Communication

Design of a Recommender System
for Intelligent Classrooms Based
on Multiagent Systems
Dulce Rivero-Albarrán1,2(&), Francklin Rivas-Echeverria1,2,
Laura Guerra1, Brian Arellano1, and Stalin Arciniegas1
1 Pontiﬁcia Universidad Católica del Ecuador,
Sede Ibarra. Escuela de Ingeniería, Ibarra, Ecuador
{dmrivero,ﬁrivas,lrguerra,bgarellano,
smarciniegas}@pucesi.edu.ec
2 Facultad de Ingeniería, Escuela de Ingeniería de Sistemas,
Universidad de Los Andes, Mérida, Venezuela
Abstract. In this work it’s presented the description of a Recommender Sys-
tems to be used in a learning environment. For this, it’s deﬁned the diverse
recommendation methods that can be used: Content-based Recommendations,
Collaboration recommendations and Hybrid Systems. It is proposed a Teaching
agent design for obtaining the most appropriate learning objects based on
Multiagent Systems framework composed of a Planning Agent, a Recommen-
dation System and a Learning Object Management System; all of them col-
laborating on an intelligent platform. This design can be used in intelligent
classrooms because it can adapt the environment and the teaching contents
according to the needs of each student.
Keywords: Recommender System  Intelligent classrooms
Multiagent systems  Learning objects  Learning environments
1
Introduction
The new technologies have changed the way that people perform their daily processes,
these are already ubiquitous in the daily work. For this reason, organizations have
incorporated these new technologies in their computer systems, in order to guarantee
their permanence in an increasingly competitive market. These systems provide the set
of services that employees and users require, improving the quality of their products
and increasing the efﬁciency of their processes.
New technologies have had a positive impact on the way that systems perform their
functions. This is the case of online shopping systems and some search engines, which
have incorporated in their applications the ability to guide users in a personalized way,
presenting objects of interest, where objects are in a wide space of options possible [1].
These applications are known as Recommender Systems (RSs).
Educational environments have not been out of these changes, technologies have
improved the educational process. In technology-enhanced learning (TEL), its objective
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_92

is to design, develop and test socio-technical innovations that support and improve the
learning practices of both individuals and organizations. It can be found a variety of
applications in this domain, some of these systems have been developed as virtual
platforms for teaching, this is the case of Blackboard [2], Moodle [3], Schoology [4],
Mahara [5]. Other applications were oriented to the development of Intelligent Tutorial
Systems (ITS), among this category are Aspire [6], ITSpoke [7], AGT (Advanced
Geometry Tutor) [8]. ITSs are systems of mass teaching, so do not contemplate modes
of learning that adequately adapt to the previous knowledge and ability of evolution of
each student. In recent years, as a branch of research in the ﬁeld of intelligent envi-
ronments (IE) has been working on the development of Intelligent Classrooms (IC).
Intelligent Classrooms (IC) are spaces enriched with electronic devices that per-
ceive their environment, because they are sensitive environments, that respond and are
adapted to the needs of the present users, they are transparent, ubiquitous and intelli-
gent. In summary, it can be said that the IC are spaces equipped with devices and
sensors that communicate, with the purpose of adapting both the environment and the
teaching contents according to the needs of each person in the classroom. It creates
more comfortable and pleasant spaces, improving productivity and satisfaction, all in a
transparent and non-intrusive way. Among this area is ClassMate, an open framework
for Intelligent Environments in Education proposed by Leonidis et al. [9]; the con-
ceptual models for designing multiagent systems (MAS) architectures for intelligent
rooms can be found in [10–12], and works oriented to the perception of the environ-
ment in [13].
A MAS is a system consisting of a set of intelligent agents, with autonomy and
proactivity that collaborate with each other using a communication protocol to reach
some global objective, these systems are used in various ﬁelds (robotics, simulation,
mathematics, among others). In the educational environments are aimed at the control
of their environment and the management of the teaching-learning process.
The search for learning resources is a fundamental element in TEL, so RSs have
become an important element in teaching systems, allow more personalized systems,
because identifying the characteristics of each student they can suggest courses and
learning activities according to their proﬁle [14, 15]. There are also RSs that suggest
changes to teachers in order to improve the effectiveness of education systems using
web environments [9]. An improvement to RSs in education environments is found in
those, which suggest learning objects (LO) based on the student proﬁle [16].
The literature in this area is broad, different methods and recommendation tech-
niques are proposed. Some authors use multi-attribute collaborative ﬁlters with fuzzy
matching methods for selecting learning objects [17]; other authors focus on the use of
collaborative ﬁlters [18, 19] or use content-based method implemented using
case-based reasoning (CBR) [20, 21] or using the attribute-based technique [22]. Each
work offers a solution to the classic RS problems, which are the “Cold Strat”, and the
scattered LO.
This paper presents a MAS that emulates activities in the teaching process in
intelligent educational environments. The MAS integrates two systems: (1) a course
planning system, whose function is to schedule each course and, if it does not reach an
objective in the estimated approving rate, re-schedule its programming; (2) an RS in
charge of suggesting to the students, based on their proﬁle and the established plan, the
974
D. Rivero-Albarrán et al.

learning objects to be selected that are most suitable to that proﬁle. The RS was designed
as a hybrid architecture, in which teachers and students can rate the quality of LO.
The presented system gives a solution to the “Cold Start” problem of each method:
For the search method based on content, it starts from the tags found in the analysis of
the contents and the LO/teacher matrix that contains the score of each LO suggested by
the teacher in the planning phase. In the collaborative method, student proﬁles are
established and the LO/proﬁle matrix is used, which is debugged with the allocated LO
utility deﬁned value.
The work is organized as follows, section two presents the objective of this work
and the basic concepts. In section three, it’s presented the design of the teaching agent
with its two components: the planning and recommendation system. Finally, section
four presents the conclusions and future work.
2
Objectives of the Work and Basic Concepts
This work aims to develop a more personalized teaching system, therefore, the learning
agent is deﬁned as a system that, knowing the students in classroom, ﬁnd their proﬁle
and based on their level of knowledge, learning characteristics, course syllabus and
level of learning achieved, suggest the most appropriate learning objects. The design of
the agent required, for its design, the concepts and foundations associated with the
recommending systems, planning systems and learning object management systems.
2.1
Recommender Systems (RSs)
The RSs are information systems that provide suggestions of objects that are more
likely to be of interest to a particular user [23]. Suggestions require a decision-making
process such as: what type of learning objects are most appropriate according to the
characteristics of the individual since the individual does not have enough knowledge
to decide or evaluate the objects that could be of interest.
The RSs distinguish three important elements: the items that are the elements to be
recommended, the users who are the individuals who are identiﬁed their needs and
preferences and the transactions that are the interactions between the user and the
system and can be collected implicitly or explicitly. A RS should be able to predict the
expected utility of each item in order to select those that are of interest and classify
them based on some mechanism of comparison.
According the way the recommendations are classiﬁed, there are three methods [24]:
• Content-based Recommendations.
• Collaboration recommendations.
• Hybrid approaches.
2.1.1
Content-Based Method
In the content-based RSs, it’s calculated a utility function u (ci, sj) of item “ci” for the
user “sj” which is calculated on the basis of the utility u(c, sk) assigned by user “sk” to
items c 2 C that are “similar” to the item ci.
Design of a Recommender System for Intelligent Classrooms
975

This approach emerges from information recuperation systems and information
ﬁltering. The main differences from recovery systems is that it uses user proﬁles that
contain information about tastes, preferences and needs. Proﬁle information can be
obtained from users explicitly, through questionnaires, or implicitly learned from their
transactional behavior over time. It focuses mainly on recommending items that contain
textual information, such as documents.
It basically uses two vectors: one that contains the set of attributes that characterize
an item and is used to determine the suitability of the item for recommendation pur-
poses; and a second to store the preferences or tastes of the user - The contents are
usually keywords (k) and indicate by weight the importance of ki in document j (dj).
2.1.2
Collaborative Method
The collaborative method tries to predict the usefulness of the elements for a given user
based on elements previously qualiﬁed by other users. In this method the utility u(c, s)
of the item s for user c is estimated based on the utilities u(cj, s) assigned to the element
s by those users cj 2 C that are “similar” to the user c.
The algorithms for collaborative recommendations can be grouped into two cate-
gories: (1) memory-based or heuristic-based making predictions based on the complete
collection of previously-qualiﬁed elements by users; (2) based on models that use the
collection of scores to learn a model, which is then used to make score predictions.
2.1.3
Hybrid Method
Combine the content-based method and the collaborative method, there are four ways
to combine these methods: (1) implement collaborative and content-based methods
separately and combine their predictions, (2) incorporate some content-based features
into a collaborative approach, (3) incorporate some collaborative features into a
content-based approach and (4) build a general unifying model that incorporates
content-based and collaborative features.
2.2
Planning
Planning is a searching and articulating process for a sequence of actions (tasks,
activities, processes) that achieve an objective [25]. A plan is generally described by
the set of states, the goal to achieve and a set of actions. Each action contains the name
of the action that moves from one state to another, the preconditions that must be met to
reach the state, the effect that describes how the state changes when the action is
executed.
One method to perform task planning is through graphs, this method provides
better heuristic estimates. It consists of a sequence of levels corresponding to time steps
in the plan, and where the level 0 is the initial state. Each level contains a set of literals
that indicates the state that is reached if the actions were executed and a set of actions
for the next level. A plan selects “the best sequence of actions” in order to reach the
ﬁnal state (target or goal).
976
D. Rivero-Albarrán et al.

2.3
Learning Objects (LO)
A LO is any digital or non-digital entity that can be used, reused or referenced for
technology-supported learning [26]. An LO is characterized because they are small
units (low granularity) and reusable. Other features are their customization, the
sequence of how to present the LO can vary, their ﬂexibility adaptable to different
learning styles and because they are modular. There are several standards to deﬁne the
structure of LO, some of them are NET Learnactivity and SCORM [27, 28], the latter is
the most used model in the creation of learning objects, due to its ease of interchange
between platforms or teaching environments and offers the possibility of dynamic
content.
3
Teaching Agent Design
For the design of the teaching agent, it was necessary to perform an analysis. First, the
processes, activities and objects involved in the teaching process were identiﬁed. It was
worked with a group of teachers from different university (Pontiﬁcia Universidad
Católica del Ecuador, Universidad de Los Andes), it was identiﬁed which processes
were executed and how each was performed. From this study we identiﬁed the
activities that can be assumed by an agent. Then, in a second phase, it was analyzed the
collaboration between this teaching agent with the rest of the agents speciﬁed in the
platform conceptual model [12].
As a result of the ﬁrst phase, a process model was obtained, this model allows to
specify the processes that are performed, as well as the objective, the inputs (data or
events), the products that are generated at the output and the resources and information
required in each process. Figure 1 presents the teaching process model using the model
proposed by Eriksson and Penker [29]. In the model three processes are distinguished,
e-p Eriksson-Penker Diagram
New 
course
Course planning
«deliverabl...
Contents 
planning
Time control 
«information»
Attendance list
«informati...
Analytical 
program
«goal»
To program the 
contents o the subject 
to be insert iin the 
course
Select material 
support
«deliverable»
Support material
«resource»
Syllabus
«deliverable»
Support material
«goal»
Select de most 
suitable course 
material
Material 
supportt 
managment
«goal»
Manage support 
material of the course
«deliverable»
Course content 
support material
New class
«output»
«output»
«input»
«goal»
«goal»
«input»
«goal»
«input»
«supply»
«supply»
«input»
«output»
«output»
Fig. 1. Teaching process model using UML business.
Design of a Recommender System for Intelligent Classrooms
977

two of them are fundamental processes (planning of the content of the course and
selection of course support material), the other is a support process (management of
support material). The support process is transversal to the two previous processes, it is
responsible for maintaining the register of books, guides, videos, among others, related
to each course content and to support students.
After knowing that activities that are carried out in each process, the services that
the intelligent platform had to support were identiﬁed. Finally, the services were
grouped into three subsystems [30]: A Planning Agent, a Recommendation System and
a LO Management System. Figure 2 presents the component model of the teaching
agent, which shows the components and functions of each of them as well as their
collaboration on the intelligent platform.
3.1
Planning Agent
The planning agent is responsible for the planning, re-planning and follow-up of the
course. The agent is activated when it’s received the message changePeriodo() (see
Fig. 2), this message notiﬁes the teachers of each subject of the new period that they
must update or create the course planning. The planning is done in three stages: ﬁrst the
teacher, using the analytical program of the subject, establishes the contents of the
course (setContentData()). These data are: the content identiﬁer, the speciﬁc content to
be taught (derived from the analytical plan and/or competences to be developed), time
dedicated to content (indicated in hours), hours taught (0 by default), type of activity
cmp Agente de enseñanza
PlanningAgent
+ changePeriodo()  :void
+ changeClass()  :void
+ setCourseData()  :void
+ setContentData()  :void
+ getContentData()  :void
+ setPrecondition()  :void
+ setEfect()  :void
+ getSequenceContent()  :void
+ setSequenceContent()  :void
+ newEvaluation()  :void
+ replaning()  :void
+ schedulePlan()  :void
+ notifyTeacher()  :void
+ updatePlan()  :void
RecommendedSystem 
+ changeCourse()  :void
+ setStudentProfiel()  :void
+ getStudentProfile()  :void
+ getRecomendation()  :void
# doRecomeded()  :void
# colaborativeMethod()  :void
# ContendMethod()  :void
# joinRecommendation()  :void
# clusterEM()  :void
# clusterKM()  :void
LearningObject
+ upLoadLO()  :void
+ getLOContentProperty()  :void
+ setLOContentProperty()  :void
+ setLOColaborativeProperty()  :void
+ getLOColavorativeProperty()  :void
+ getURL-LO()  :void
+ updateUtilitiLO()  :void
EnviromentAgent
+ detectPeople()  :void
+ activateCamera()  :void
+ desactiveCamera()  :void
+ getListOfAssistants()  :void
+ controlTemperature()  :void
+ controlLigt()  :void
+ NotifyNewAcademicPeriod()  :void
+ NotifyChangeCourse()  :void
LearningAgent
+ doEvaluation()  :void
+ evaluate()  :void
+ sendEvaluation()  :void
+ displayOL()  :void
+ showListOL()  :void
+ sendMsgSudent()  :void
TimeTraking
+ sedNotiyCourse()  :void
+ listCourses()  :void
+ newCourse()  :void
+ getCourseScedule()  :void
Fig. 2. Components diagram of the teaching agent.
978
D. Rivero-Albarrán et al.

(theory or practice, evaluation), descriptor 1 of content, descriptor 2 of content, content
priority over the entire course, content status (not taught by default), minimum
approval percentage.
Subsequently, the teacher enters the data sequence in which the contents will be
taught (setSequenceContent ()), this is the plan itself. The data describing the sequence
of contents are: the state indicated by the content to be taught (content identiﬁer), the
preconditions to be fulﬁlled to teach a certain content, that is, the contents previously
seen and the effects caused at the end of the content, these are the contents that can be
taken later. These data represent an unguided graph where the nodes represent the state
and the arcs represent the preconditions and effects.
Finally, the teacher will be able to upload the LOs, scoring each according to the
LO properties. Each learning object is scored in terms of its relation to the charac-
teristics of the content of the subjects to be imparted, these data (LO Attributes), is
stored in a matrix (LO x ContentPropierty). Table 1 shows the structure of this table.
The process of monitoring the plan is activated with the arrival of the event that
indicates the start of a class session (ChangeClass()), this event activates the schd-
ulePlan() method and causes the agent to send a message to the recommending system
to proceed to search for the appropriate LOs for the content being sent (ChangeCourse
()). In addition, the system updates the course plan (updatePlan()), modifying the total
number of hours given for the content that is taught, which allows to know the progress
and status of a course.
Finally, the planning system, based on the learning results of each content (sent by
teaching agent newEvaluation()), calculates the percentage of students who have
approved the content and based on this value makes the decision to re-plan the course.
If so, modify the time dedicated to that content (replanning()). To do this, the agent
uses the graph storing the plan and for calculating the possible sequences contained, it
applies a variant of algorithm Graphplan [25], these scenarios are presented to the
teacher (getSequenceContent ()) so that the new plan should be selected.
3.2
Recommender System (RSs)
This system is responsible for ﬁnding the LOs that best ﬁt the characteristics of each
student. The RSs designed uses the hybrid method with a combined approach, where
the recommendations generated by the collaborative method and the content-based
method are combined into a single recommendation [23], as shown in Fig. 3.
Table 1. Score of the characteristics of the LO vs the characteristics of a learning content
Id
LO
Complex
level
Type
Time
duration
Quality
Type of
material
Related topics
1
..related topics
n
tag1
. . .::
tag n
1
5
3
30
5
video
math
algebra
principle
. . .:
Invt. opera
2
1
2
15
3
lecture
phisycal
termo
energy c.
. . .:
principle
...
...
...
...
...
...
...
...
...
...
...
n
3
3
120
3
practice
design
material
. . .
.
. . .
Design of a Recommender System for Intelligent Classrooms
979

The recommendation system is made in two phases, in the ﬁrst one the students’
learning proﬁles are constructed and all the LOs that are associated to the set of char-
acteristics of the content to be taught are searched, this search is done using the algorithm
that calculates the similarity of cosines [31] that indicates the percentage of similarity that
exists between the descriptors of the subjects and a LO. It’s selected the LO with value
greater than 50%; in the second, the recommendation process is carried out.
To identify the learning proﬁles, an array of learning characteristics (reﬂective,
intuitive, active, logical, visual, reading, acoustic, prior knowledge, among others) vs
students is deﬁned, which stores the score of those characteristics for each student test
results. This matrix is used to ﬁnd student groups. It’s used the EM clustering algo-
rithm, this algorithm belongs to a family of models known as Finite Mixture Models
[32]. For each proﬁle a selection of LO is done.
The content-based method uses the characterization of Table 1, the characteristics
of the course content and the characteristics of each group of students. First it’s
combined the last two tables and then is used this result to ﬁnd the LO groups that are
close to the proﬁle.
For the collaborative method it’s used the characteristics of each group of students
and the matrix of LO valued by the students in previous sessions. There are selected the
groups of LO most appropriate. For both methods, the cosine similarity algorithm is used.
There are combined the recommendations resulting from each method in order to
obtain the ﬁnal list of LO suggested by proﬁle and students. In this version of the
system only the LOs found in both recommendations are selected.
Finally, an attribute is attached to the LOs scored by the students, this attribute
measures the usefulness of each LO, it is calculated as the ratio between the numbers of
times that has been suggested among the number of suggestions. If the value tends to 1
LO has a high probability of being suggested.
4
Conclusions and Future Work
In this work, it has been presented a mutiagent systems-based design for a Recom-
mender System to be used in an intelligent classroom.
act Agente de enseñanza
Content LO method
Colaborative LO 
method
LO-ContentProperty
LO-ColaborativeProperty
ProfileStudent
LO_ListContent
LO_lisColaborative
joinRecomendation
  
LO_Recomendation
Init
end
 
Fig. 3. Activity diagram of the recommendation hybrid strategy for the Recommender System
980
D. Rivero-Albarrán et al.

This design aims to obtain learning objects according to the environment and to the
knowledge and skills of each student, obtaining customized learning activities that can
be used by the teacher for each of the students.
The designed recommender systems based on multiagent system framework is
composed of three services: Planning Agent, a Recommendation System and a
Learning Object Management System. All this services have to collaborate in order to
guarantee the appropriate selection of the customized learning objects.
Acknowledgments. The authors of this paper thank the support given by PUCESI to the
project: Intelligent platform in educational environments case: PUCESI Student Ofﬁce.
References
1. Lops, P., de Gemmis, M., Semeraro, G., Handbook, R.S.: Content-based recommender
systems: state of the Art and Trends. In: Ricci, F., Rokach, L., Shapira, B. Kantor, P.B. (eds.)
Recommender Systems Handbook, p. 96. Springer, Heidelberg (2011)
2. Virtual Classroom Software: Blackboard Collaborate. http://www.blackboard.com/online-
collaborative-learning/
3. Moodle. https://moodle.org
4. Schoology, Learning Management System: LMS. https://www.schoology.com
5. Mahara. https://mahara.org/
6. Mitrovic, A., Suraweera, P., Martin, B., Zakharov, K., Milik, N., Holland, J.: Authoring
constraint-based tutors in ASPIRE. In: Ikeda, M., Ashley, K., Chan, T.-W. (eds.) ITS 2006,
LNCS, vol. 4053, pp. 41–50 (2006)
7. Litman, D.J., Silliman, S.: Itspoke: an intelligent tutoring spoken dialogue system. In:
Proceedings of the Human HLT/NAACL, Boston, MA, May 2004
8. Matsuda, N., Vanlehn, K.: Advanced geometry tutor: an intelligent tutor that teaches
proof-writing with construction. In: Looi, C.-K., McCalla, G., Bredeweg, B., Breuker,
J. (eds.) Proceedings of the 12th International Conference on Artiﬁcial Intelligence in
Education AIED 2005, pp. 443–450. IOS Press, Amsterdam (2005)
9. Antona, M., Margetis, G., Ntoa, S., Leonidis, A., Korozi, M., Paparoulis, G., Stephanidis,
C.: Ambient Intelligence in the classroom: an augmented school desk. World Acad. Sci. Eng.
Technol. 66, 594–598 (2010)
10. José-Guillermo, H.-C., Benítez-Guerrero, E., Mezura-Godoy, C.: Ambientes inteligentes en
contextos educativos: modelo y arquitectura. Res. Comput. Sci. 77, 55–65 (2014)
11. Arturo, O.D., Alberto, J.J.: Ambiente inteligente distribuido de aprendizaje: integracion de
ITS y CLSC por medio de agente pedagogicos. In: Revista de la Escuela de Ingeniería de
Antoquia (EIA 2006), pp. 89–104. Antoquia (2006)
12. Rivero, D., Arciniega, S., Narváez, L., Puetate, G.C.: Ambientes Inteligente para la
Educación: Un modelo conceptual. In: Avances y aplicaciones de sistemas inteligentes y
nuevas tecnologías, pp. 117–130. Universidad de Los Andes, Ecuador (2016)
13. Rabie, A., Hagras, R.H., Nawito, M., El Faham, A.: The intelligent classroom: towards an
educational ambient intelligence testbed. In: Sixth International Conference on Intelligent
Environments (IE), pp. 344–349. IEEE (2010)
14. Farzan, R., Brusilovsly, P.: Social navigation support in a course recommender system. In:
Proceedings of the International Conference on Adaptive Hypermedia and Adaptive
Web-Based Systems, pp. 91–100. Springer, Berlin (2006)
Design of a Recommender System for Intelligent Classrooms
981

15. Hummel, H., Van den Berg, B., Berlanga, A., Drachsler, H., Jansenn, J., Nadolski, R.,
Koper, R.: Combining social-based and information-based approaches for personalised
recommendation on sequencing learning activities. Int. J. Learn. Technol. 3(2), 152–168
(2007)
16. García-Salcines, E., Romero-Morales, C., Ventura-Soto, S., Castro-Lozano, C.: Sistema
recomendador colaborativo usando minería de datos distribuida para la mejora continua de
cursos e-learning. IEEE-RITA 3(1), 19–30 (2008)
17. Lu, J.: Personalized e-learning material recommender system. In: International Conference
on Information Technology for Application, pp. 374–379 (2004)
18. Koper, R.: Increasing learner retention in a simulated learning network using indirect social
Interaction. J. Artif. Soc. Soc. Simul. 8(2) (2005)
19. Janssen, J., den Berg, B., Tattersall, C., Hummel, H., Koper, R.: Navigational support in
lifelong learning: enhancing effectiveness through indirect social navigation. Interact. Learn.
Environ. 15(2), 127–136 (2007)
20. Heraud, J.-M., France, L., Mille, A.: Pixed: an ITS that guides students with the help of
learners’ interaction log. In: International Conference on Intelligent Tutoring Systems,
Workshop Analyzing Student Tutor Interaction Logs to Improve Educational Outcomes,
Maceio, Brazil, pp. 57–64 (2004)
21. Sørmo, F., Aamodt, A.: Knowledge communication and CBR. In: ECCBR Workshops,
pp. 47–60 (2002)
22. Drachsler, H., Hummel, H.G.K., Koper, R.: Personal recommender systems for learners In
lifelong learning networks: the requirements, techniques and model. Int. J. Learn. Technol. 3
(4), 404–423 (2008)
23. Ricci, F., Rokach, L., Shapira, B.: Recommender Systems Handbook. Springer, Berlin
(2015)
24. Klasna-Milicevic, A., Ivanovic, M., Nanopoulos, A.: Recommender systems in e-learning
environments: a survey of the state-of-the-art and possible extensions. Artif. Intell. Rev. 44
(4), 571–604 (2015)
25. Russell, S., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach. Prentice-Hall,
Egnlewood Cliffs 25 (1995)
26. IEEE: Learning Object Metadata. http://ltsc.ieee.org/wg12/
27. L’Allier, J.: Frame of Reference: NETg’s Map to Its Products, Their Structures and Core
Beliefs. http://web.archive.org/web/20020615192443/www.netg.com/research
28. ADL. http://www.adlnet.org/
29. Eriksson, H.-E., Penker, M.: Business Modeling with UML, New York (2000)
30. Rivero-Albarrán, D., Barrios-Albornóz, J., Rivas-Echeverría, F.: An evolutionary strategy
for transforming the Business Object Model in a Web Services Model. Int. J. Internet Things
Web Serv. 1(1), 8–21 (2016).
31. Cohen, W.W.: Integration of heterogeneous databases without common domains using
queries based on textual similarity. In: Proceedings of the SIGMOD International
Conference Management of Data SIGMOD 1998, pp. 201–212 (1998)
32. Garre, M., Charro, M.: Estimación del esfuerzo de un proyecto software utilizando el criterio
MDLEM y componentes normales NDimensionales. Aplicación a un caso práctico. In:
Revista de Procesos y Métricas de las Tecnologías de la Información (RPM), 13-24 March
2005, vol. 2, no. 1. Asociación Española de Sistemas de Informáticos (AEMES) (2005).
ISSN: 1698 2029
982
D. Rivero-Albarrán et al.

The Visual Speech and Creativity
in Advertising Impressed in Ecuador in Daily
“El Comercio” Between 1908 and 1950
Marco López-Paredes(&)
Pontiﬁcia Universidad Católica del Ecuador, Quito, Ecuador
mvlopez@puce.edu.ec
Abstract. This research work is an extension of the ﬁrst study that was devel-
oped in Ecuador on the subject and that generated the analysis based on a single
decade, seeks to present several moments of the rhetorical concept and its use in
print advertising in Ecuador in a period of time ranging from 1908 to 1950. It will
try to establish the conﬁguration that the use of rhetoric has had in advertising in
the period proposed to provide an instrument of analysis to those who investigate
this subject, it is noteworthy that it is the ﬁrst project of extensive research in this
area in Ecuador, the ﬁrst advances have been discussed and presented in previous
publications with minor lapses of temporality in the analysis.
Keywords: Advertising  Rhetoric  Image  Press
1
Introduction
The construction of rhetoric necessarily refers to the fact of abstraction and con-
struction of a message related to advertising practice, given the characteristic of
advertising with a persuasive purpose and copywriting, the methodical structure is
mixed between classical rhetoric and Your current applications. Citing Jacques Durand
expresses that “Rhetoric is shown as a range of possibilities to be original.” [6]. It can
be noticed that talking about rhetoric and advertising is not a simple coincidence, but
rather they support the formulation of a message whose purpose is closely related to the
activation of the senses and perceptions.
It is pertinent then to analyze the rhetoric not only from the language as most
studies establish it but also from a practical perspective and formulation to new
research scenarios.
In its early stages the advertising analysis was submerged in the structural analysis
of literary semiotics and its translation to the image, so publicity exerted a punitive
supply of the senses to stimulate it so explains it (González Martín, 1996, p. Publicity is
explained as “mechanism of imposition of senses, it is a discourse that has as purpose
to motivate an action”.
It is important, then, to start and approach rhetoric from the Aristotelian perspective
as a starting point to be able to base and structure the course that the term and practice are
having to time and resources being important this last analysis because the publicity in
this sense has a character Practical - understood from a historical point of reference [1].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_93

Then we notice that in this study the use of a rhetorical approach confronts the new
research in the advertising analysis, which face the argument and the fact of the
message as a whole, is the union of techniques that assume the discursive and
essentially Graphic, the message is no longer either the one or the other is the con-
ﬂuence of all the elements that are part of the strategy, creative practice, argumentation
and staging of the advertising message. Rey [9, p. 92] argues that the argumentative
current directly connects with Aristotelian rhetoric, considers rhetoric not as a practice
to disguise a discourse, but as a technique for constructing persuasive discourses with
an advertising character.
It is necessary to emphasize that the publicity exerts a persuasive activity destined
to motivate the ideations of market and society.
In the research that summons us to the theoretical lines that deﬁne and contextualize
the theme are of vital importance as these will raise the discussion around rhetoric,
image and advertising as a whole, as it has been established these three disciplines are
almost always investigated By separate accounts however the empirical and profes-
sional practice warns us of the encounter that they have in the advertising technique,
they converge and contribute to that the advertising resource goes beyond the
creativity.
2
Theoretical Framework
2.1
Aristotle and the Hellenic Period
Aristotle deﬁnes rhetoric as the constructed opposition of dialectics. Aristotle con-
ceptualizes that, although both contribute to the growth of knowledge, the former seeks
to persuade and construct a discourse, this can sometimes serve to refute, not only to
assent to a truth, on the other hand, the second only exposes the “irrefutable”. In the
Aristotelian rhetoric, an emitter, a message and a receiver is already posed.
On the other hand, most coincides with the idea that Rhetoric begins its activity in
Greece, here arises the interest to regulate the verbal and written communication, it was
unconsciously known that the communicative exercise could exert a certain degree of
inﬂuence in those who had Contact with this resource well elaborated. The discursive
premium in this period.
“The Greeks deﬁne and conceptualize the effect of discourse as a means of contact and
inﬂuence in the minds of its inhabitants, taking the ideological (religion, beliefs and political
tendencies) as their central sources. The Greeks began to give value to the oratory even in the
military commands, in the political life a good speaker was essential. It could be said that
Rhetoric appears as an element of high society and intelligence” (López [7, pp. 39–40]).
It is important then to start and approach the rhetoric from the Aristotelian view as a
starting point to base and structure the route that the term and practice are having to
time and resources being important this last analysis because the publicity in this sense
has a practical character - thus understood from a historical point of reference.
Then we notice that in this study the use of a rhetorical approach confronts the new
research in the advertising analysis, which face the argument and the fact of the
message as a whole, is the union of techniques that assume the discursive and
984
M. López-Paredes

essentially Graphic, the message is no longer either the one or the other is the con-
ﬂuence of all the elements that are part of the strategy, creative practice, argumentation
and staging of the advertising message. Rey argues that the argumentative current
directly connects with Aristotelian rhetoric, considers rhetoric not as a practice to
disguise a discourse, but as a technique for constructing persuasive discourses with
advertising character [9].
In this sense, according to Beristáin Rhetoric is deﬁned as “the regulated trans-
formation of compendia of a statement, so that in the perceived degree of an element
where the receiver can dialectically superimpose a conceived degree” [5]. We could
then explain that rhetoric and language are:
“Art of elaborating grammatically correct, reﬁned and, above all, persuasive discourses. The art
of extracting, speculatively, from any everyday matter of opinion, a construction of a character
that is related to the justice of a cause, with the desirable cohesion between the members of a
community and with regard to its future destiny” (Beristáin [5, p. 426]).
2.2
Perception, Image and Rhetoric in Advertising
According to López the senses connect us with the world around us, provide us with
information and allow us to communicate. However, as will be seen, mere sensory
stimulation is useless if it does not become a signiﬁcant perception through the brain
[8]. We must not, therefore, confuse stimulus and perception. Stimulation is energy
captured by sensory receptors (senses), whereas perception is the processing of that
information carried out by the brain to interpret it and give it meaning. Hence we can
afﬁrm, together with “all perception is an act of searching for meaning, and, in this
sense, it is an act of communication.”
Perception can, therefore, be deﬁned as the process by which consciousness con-
verts sensory stimuli into meaningful conﬁgurations.
Expressed in other words, theories of perception study the mechanisms by which
the brain translates visual cues (and other senses) into recognizable objects or facts.
That is why, as points out, perception occurs “when strictly physiological processes
become perceptually organized matter” [6].
It is obvious then, that without communication is not possible. For this reason, no
message will serve anything if it is not perceived. In addition, such perception must be
performed correctly, so that the signiﬁcant conﬁguration that generates the receiver in
its brain is approximately that which the sender of the message intended.
Taking into account these reﬂections, it will be imperative that the issuer - designer
- takes into account the mechanisms of perception if he intends that his messages work
correctly. In this sense, Durand explain “Every form generates a response… Hence the
importance of the control exercised by the designer on the signiﬁcant aspect of the
components that he selects for his designs Only on the aesthetic aspect) and on the
conﬁgurations that it uses to organize those components” [6].
Despite the great importance of perception, as we have just seen, there is no clear
and unique paradigm on the processes that determine it, so we will see the different
theoretical approaches on the subject.
The phenomenon of perception has been studied from various areas of knowledge,
ranging from philosophy and psychology to physiology or neurology. Although each
The Visual Speech and Creativity in Advertising Impressed
985

of these disciplines has been ﬁxed on different issues, it must be noted that they all start
from a common idea: perception is the result of the processing and interpretation of
individual sensations, coming from the sensory organs, by the brain.
However, even on the basis of this basic idea, the discussion about the mechanisms
of perception is complex and it can traditionally identify two main lines of research:
empiricism and nativism. Empiricism - also called associationism - understands per-
ception as the fruit of learning and the accumulation of experiences. Thus, this
approach considers that perception is never completely determined by physical stim-
ulus. One of the main representatives of this position is the German physiologist
Ferdinand von Helmholtz, who, in the mid-nineteenth century, posits that perception is
the result of the brain’s ability to integrate present-day sensory stimuli with past
experiences. This perception approach insisted in particular on the linkage and asso-
ciations, learned by experience, between optical data and non-visual data. In this sense,
the brain groups and organizes any stimulus so that it resembles something already
known.
The nativism understands the perception, as opposed to the previous approach, as
an innate and intuitive reaction in the individual. Nativism, also called Innatism, was
deﬁned, as Rifkin [10] writes, “as opposed to all theories that implied a learning of
vision.” Kant himself afﬁrmed that “we see the world through the eyes and the other
senses directly and instinctively.” Form theorists (Gestalttheorie) can ﬁt into this line of
research, as their work revolves around the innate ability of the brain to organize visual
perception according to universal and eternal laws.
2.3
The Advertising Metaphor
According to Durand with advertising we not only persuade for the commerce of things
and services: we create worlds. I’m watching TV right now. I hear a chorus of voices
imitating the sounds of rain, of a car; I will soon see what I am hearing, and will
continue to hear the sounds of a perfectly imitated car. Only those sounds are repre-
sented by the voices of a choir. The engine is a choir. The voices of the choir, the sound
of the engine. The sound of the tire rubbing on the pavement rubs the spaces of the
stage, the voices make music of special effects.
“The ﬁgures of rhetoric are produced by a kind of perceptual juggling, whether it is words,
graphic representations or the combinatorics of one or the other, in which something is hidden
and something is shown; Is what can be called “signiﬁcant operation”, that is (…)”, (Durand [6,
p. 11]).
Finally, in “this very moment when my eyes read your little poem,” says López [8],
“there is a man who suffers only because he loves freedom.” The poet does not know
his name, nor who he is, nor where he is, but he is very sure that this man exists. And
call the consciousness.
In the four previous texts there is an element that is substituted. In the ﬁrst two,
which correspond to commercials, the products appear at some point in the adver-
tisement, but in any case are characterized by substitutions, by visual or sound meta-
phors, the ﬁgure of the female ﬁgure riding a horse at a trot at Seashores, by the drink;
The voices of the choir, by the sounds of the car.
986
M. López-Paredes

The rhetorical ﬁgure consists in that the dominant concepts are represented by the
images, and a transfer of value is made, of appropriation of the concept of the
advertisement to the product; For Terry’s announcement, beauty, the mixture of
masculinity and femininity, freedom, among others; For car advertisement, perfection,
human imitation, quality.
3
Methodology
The methodology is based on several of the author’s publications, since the instrument
is validated and a type of experimental exploratory research is structured. This research
seeks to establish the relations between the advertising illustration and the rhetoric of
the image, in the cause - effect relationship is established. For the case are used
(Table 1):
Cross Design
This structure will allow us to measure the level or state of one or several variables at a
given time or in what is the relationship between a set of variables exposed to study in
terms of rhetoric and advertising image at a point in time. The essential purpose is to
describe variables and analyze their incidence and interrelation at a given moment
3.1
Longitudinal Design
With this we will analyze changes over time in certain variables or in the relationships
between them. We collect data over time at points or periods speciﬁed by decades since
1908, to make inferences about change, its determinants and consequences. In this case
the universe of study has an empirical method and a research sample that is intentional
non-probabilistic sampling, in which we use participant observation and content
analysis as an instrument. For the indicated, the following interpretation variables
distributed in matrix 1 are proposed (Table 2).
The coding results in:
That is, each variable are images that correspond to advertising of the period in
question, which are grouped with a letter of the alphabet in order to quantify each one
of them, in the investigation these are important as they allow to verify which one is
Table 1. Interpretation variables
Advertisements Interpretation variables
Years
A B C D E F G H I
Source: Prepared by the author
Table 2. Variable encoding
A
B
C
D
E
F
G
H
I
Hyperbole Antithesis Analogy Prosopopey Metaphor Anchorage Appeal Repetition Coding
Source: Prepared by the author
The Visual Speech and Creativity in Advertising Impressed
987

The one of greater acceptance. In this context a decade will be represented by three
images that seek to expose the rhetorical value of the advertisement. Thus the exposed
images are shown in a row with the data to be analyzed:
All the ﬁgures analyzed below belong to “Diario el Comercio of Ecuador” (Figs. 1,
2, 3, 4, 5, 6, 7, 8, 9, and 10).
Fig. 1. Year 1908
Fig. 2. Year 1909
Fig. 3. Year 1911
Fig. 4. Year 1915
Fig. 5. Year 1924
Fig. 6. Year 1927
Fig. 7. Year 1934
Fig. 8. Year 1939
Fig. 9. Year 1948
988
M. López-Paredes

3.2
Data Analysis
Once the interpretation of the application of the analysis matrix has been made we can
explain the following: The objective population of analysis were professionals trained
in advertising with total knowledge in the direction of art and graphic construction of
advertising media. The results obtained from the question determined that of the total of
ads investigated the most used rhetorical resources are F, E and I with a representative
percentage on the 78%, 67% and 57% in the decades between 1908 to 1910 and 1930
to 1940 respectively, Followed by Figures C, A and H with 71%, 62% and 60%
correspondingly. This for each ﬁgure marked in the decades studied (Table 3).
Based on the study carried out, professionals determine in their opinion that the
ﬁgures that impacted in the years 1908 to 1950, where Figures C, G and I are the most
representative with 93%, 92% and 87% in the years 1915, 1924 and 1948 respectively.
It is very interesting because in inferring the answers obtained we can conclude that the
Fig. 10. Year 1950
Table 3. Decades and years with greater representatives
Decades
Figure Percentage Figure Percentage
1908–1910 F
78%
G
87%
1910–1920 A
62%
F
78%
1920–1930 G
81%
I
76%
1930–1940 I
57%
E
67%
1940–1950 I
87%
C
71%
1915
C
93%
G
92%
1924
G
92%
H
60%
1933
G
50%
G
71%
1939
A
62%
G
87%
1944
C
61%
G
81%
1948
I
87%
C
93%
Source: Prepared by the author
The Visual Speech and Creativity in Advertising Impressed
989

greatest representation in years corresponds to ﬁgures with large images or headlines,
the ad quoted in 1948 breaks the current scheme of the physical archetype, for example.
In this ad it is alluded to that you are ill because of this “skinny”, as we can see here the
ﬁgure is opposed to the current thinness, therefore, advertising can inﬂuence what we
think of as good or bad.
4
Discussion
In concluding with this work we can provide a basic instrument elaborated from the
compilation of Ecuadorian historical advertising, not trying to be determinant in the
ﬁndings only because these are the beginning of the advertising research that in
Ecuador is lacking, a scope is presented to a First study trying to build the route of
advertising images in newspaper “El Comercio” of Ecuador.
Being an initial study we can not only conclude but put in discussion those
methodological elements and their results so that they continue to be perfected with the
application and contributions of the colleagues in this area:
5
From the Theoretical Framework
It is deﬁnitely essential to continue with the analysis of the state of the matter, even
though the rhetoric as object has not conceived of greater changes in time its appli-
cation if it has been modiﬁed. We have gone from a literary and sophist discursive
analysis to the analysis composed by the images, to conclude in the construction of the
image or the mark. Without a doubt the change seems to be important however its
theorizing in these last points still is poor.
The Hellenic period is undoubtedly the formal beginning of the understanding of
discursive rhetoric and Barthes still remains the “visual” and advertising icon of this
along with that act from a more advertising and not only phenomenological view of its
application or use in the commercial message [2–4].
6
The Investigation
The research presented, as indicated, provides a methodological basis and it opens the
possibility to the professional and scientiﬁc use of advertising for further studies; when
detaching itself from the communication ways. In Ecuador, the evolution of the
advertising message, more than exposing itself to the advance of technology in each
era, has been change due to the colloquial use of language. The usage of technology in
some cases has modify language in advertising pieces; especially in those ads that
denote greater purchasing power.
It is interesting to illustrate those graphs that tried to explain the medical beneﬁts of
certain products with “crazy promises” such as curing or “removing” the thinness, or
those “instant effect” products.
990
M. López-Paredes

References
1. Aristóteles: Retórica, Madrid, Instituto De Estudios Políticos, Edición De Antonio Tovar
(1971)
2. Barthes, R.: La Antigua Retórica, Buenos Aires, Comunicación (1970)
3. Barthes, R.: “La Leçon”, Editions De Sutil, France. T: Propia (1978)
4. Barthes, R.: “La Aventura Semiológica”, Paidos, Barcelona (1985)
5. Beristáin, H.: Diccionario De Retórica Y Poética, 8ª. edn. Editorial Porrúa, México (1991)
6. Durand, J.: Retórica E Imagen Publicitaria. Buenos Aires: Ed. Tiempo Contemporáneo
(1973)
7. López, E.: Sobre los orígenes de la oratoria. Madrid: Minerva I. Y en Aristóteles (1971):
Retórica, Madrid (1988)
8. López, M.: La retórica en la imagen publicitaria en el Ecuador entre 1930 y 1940.
Revista KEPES Año 12 No. 12 julio-diciembre 2015, págs. 157–175 (2015). https://doi.org/
10.17151/kepes.2015.12.12.8. ISSN: 1794-7111(Impreso) ISSN: 2462-8115 (En línea)
9. Rey, J.: Sobre El Reason Why, La Argumentación. Una Relectura (Comparada) De Los
Clásicos De La Publicidad Y La Retórica. Pensar La Publicidad 3(2), 89–108 (2009)
10. Rifkin, J.: La Era Del Acceso. La Revolución De La Nueva Economía. Barcelona: Paidós
(2000)
The Visual Speech and Creativity in Advertising Impressed
991

Inﬂuence of Social Networks from Cellphones
to Choose Restaurants, Salinas – 2016
Homero Rodríguez1(&), Jeyco Macías2, Néstor Montalván3,
and René Garzozi3,4
1 Universidad de Valencia, Valencia, Spain
horoin@uv.es
2 Facultad de Ciencias Administrativas, Carrera de Marketing,
Universidad Tecnológica Equinoccial, Salinas, Ecuador
3 Universidad de Almería, Almería, Spain
4 Facultad de Ciencias Administrativas, Carrera de Empresas y Negocios,
Universidad Tecnológica Equinoccial, Salinas, Ecuador
Abstract. Inclusion of technology as part of business strategies has made con-
sumers have other tools that help them to ﬁnd information and making consumer
decisions. It is here the importance of knowing inﬂuence of comments have in
social networks in taking decisions of visits to restaurants. For this work, a qual-
itative method (focal group) was used in the ﬁrst part and then a quantitative
method was used (off line and online survey). The sample for focal group was made
up by Equinoctial Technological University’s students in Santa Elena, the same
ones belonged to the careers of Finance and Audit (7 students), Business and
Business (1 student) and 2 graduates of the Finance and Audit. An experimental
survey was conducted for 40 people and ﬁnally 342 people were surveyed in the
Salinas city. The scientiﬁc method used was the hypothetical - deductive one.
Keywords: Consumer marketing  Consumer research  Social networks
1
Introduction
The launching of new tools is related to technology (ICTs) information and commu-
nication, it have generated diverse impacts in different social aspects, encouraging
signiﬁcant economic growth [1]. The ICTs helps to ease the people’s daily life [2], a
useful ally to this new trend has been internet which has guided momentous of change
for both in business world and in the use and habits of society, thus having a direct
impact on different economic, social and cultural aspects [3].
Another ally is the cellphone which has its origins in 1973, when Martin Cooper,
general manager of Motorola created the ﬁrst radio telephone being considered the father
of mobile telephony [4] At the end of 2001 there were a total of 933 million mobile
phone users, [5] in 2003 smartphones became very popular and in 2007 their sales went
beyond superior than laptop computers for ﬁrst time [6], in 2012 the number of smart
phones reached 80% of world population [7], with a great number of characteristics, such
as connectivity, larger screens, internet access and multiple applications, such as tourists
seeking information to reduce uncertainty and improve their decisions [8].
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_94

2
Restaurant Experience
Much of the social information about consumers acquire is conveyed through narrative,
which is used as a form of effective communication of information on abstract,
intangible beneﬁts associated with restaurant experiences [9]. One of the most relevant
aspects of restaurant experience is the quality measure perceived by customers within
these categories: quality of food, quality of service, environment [10]; the same ones
may be positively linked with the popularity of restaurants [11]. Negative comments
(interior, service, etc.) necessarily do not have a negative impact in case the well-
known brand of restaurants is present [12].
The interactions of consumers’ experience in restaurants presents a great welcome
in these innovative electronic media of communication since it allows to interact with
clients among themselves sharing information, experience, images, audio and videos of
restaurants, directly inﬂuencing in decision making of people’s nutrition [13]. A study
in China, indicates that consumer opinion and volume of reviews of these opinions can
increase online restaurants’ popularity [11], promoting eWOM where The quality of
service provided by the restaurant is the antecedent of communication and can directly
inﬂuence the increase of customers’ intention to buy [10], although it must be taken
into account that some negative messages can help in Promote credibility and positive
attitude towards a website [14]. When consumers plan to visit a place (hotel, restaurant,
etc.), they often rely more on messages and comments posted on social networks by
other consumers than on published sales and marketing messages [15].
3
Inﬂuence of Social Networks: Electronic Word of Mouth
(eWOM)
With the increase of users connected to cellphones and web-based platforms have
helped consumers to publish about their thoughts, opinions or feelings about products
or services on the internet, the so-called “eWOM e-mail”, [16] which has helped other
consumers to obtain reliable information widely and rapidly [10]. The eWom can be
described as the generation of content with the objective of fulﬁlling an integrative
function that initiates, stimulates and is triggered by individual reasons, the same that is
done by the use of a platform with technical support [17].
The term web 2.0 originated in 2004 at O’Reilly Media conference and it was
described as a series of innovations both hardware and software helps creation,
interaction and interoperability [18, 19] and where the user increases his leading role
and is called “the King” [20] since they are responsible for creating content through
publications of his opinions, comments and product reviews in weblogs [21], discus-
sion forums [22], Review websites [21, 24], retail websites, bulletin board sites,
newsgroups or social networks [21, 22], podcasting, wikis, music and video services,
internet telephony, and video confer And telepresence [23], content aggregators
(RSS) [22, 23].
One of the most used ways by users within the Web 2.0 are social networks which
are “nothing more than the evolution of traditional ways of communication of human
Inﬂuence of Social Networks from Cellphones
993

being, who have advanced with the use of new channels and tools based on
Co-creation, collective knowledge and widespread trust [24]. Social networks allow
consumers to share their feelings, thoughts and experiences [13] which may encourage
a client to visit a particular place guided by the others’ opinions.
Social networks are popular, especially among young Internet users [25], with
general features such as Facebook, MySpace and Twitter [26]. Currently social net-
works are highly frequented through smart cellphones such as a Smartphone or Tablet,
one reason is there is a strong positive experiential value [27] and increases the
entertainment value in The shopping experience [28]. For both organizations and users,
smartphones or tablets are a key element for electronic commerce, meaning different
advantages for companies such as having greater availability of information for their
customers, expanding the market and reducing the number of intermediaries [29],
which would mean a favorable reduction of costs [30], while consumers have less and
less time to make purchases, increasing the frequency of purchases Virtual as an
alternative tool [31].
4
Inﬂuence of Socio-Demographic Variables
For the purposes of this research, the analysis of socio-demographic variables such as
gender, age, education, marital status and labor sector was carried out since they have
great inﬂuence with respect to the use of the internet and electronic commerce.
In general, older people are last to access the internet, they are called laggards and
have a low income level [32], which is supported by other research indicating that
telephone users cellphones from 20 to 30 years old are often more prominent customers
for mobile entertainment services [33], these results are in agreement with those
obtained in another research that concluded that a high percentage of young people
who are active mobile internet users can visit some restaurants for information read in
these cellphones [34]. On the other hand these results are contrasted since in Spain 40%
of people included in the range of 35 to 49 years make purchases by Internet [35].
A more recent study indicates that young tourists, business travelers, educated and
high income are more predisposed to use smart phones in the search for information
[36]. Other research showed that reading messages or comments has a stronger
inﬂuence on women than men at the time of the intention to visit [37].
In view of the above, the investigation’s purpose is to determine if there is inﬂuence
of social networks from smart cellphone in taking decisions to visit to restaurants, for it
the following hypotheses were formulated:
First of all, a focus group was held in the audiovisual room of the Equinoctial
Technological University in Santa Elena, the sample was taken from students from the
ninth semester belonging to the ﬁnance and audit ﬁeld CPA (7), the career of com-
panies And businesses (1) and graduates from the Technological University of Santa
Elena, Finance and Audit CPA (2) leaving a total of 10 participants, 5 men and 5
women, who should meet three qualities such as proﬁle, The ﬁrst of which were
incorporated into the workforce either in public or private institutions, the second that
they had a smart cellphone (Smartphone or Tablet) and ﬁnally they had at least one
account in social networks.
994
H. Rodríguez et al.

This experimental research had as objective to identify the main ideas or opinions
regarding the researches subject to later analyze the information taken which lead to the
elaboration of the ﬁnal instrument (deﬁnitive survey), the subjects were discussed are
disclosed in the following Phases:
It started with a question regarding the frequency of use they gave to smart cell-
phones and the type of activities they perform most in them.
Next phase had to do with social networks use through smart cellphones, daily
connection time and if they often liked to comment, upload photos or make
publications.
In the middle phase they asked about aspects such as whether they had made
purchases from their smart cellphones and what purchasing decision process was
followed, to this was added a question about the consideration of online shopping risk.
Focusing in our research topic in fourth phase, everything about restaurant theme
was investigated, and if they would comment on any experience they had, and why
they would do so, as well as what factors inﬂuenced the most when they visit a
restaurant.
Finally as closing phase, participants were asked to agree that there would be a
speciﬁc restaurant application.
Following the methodology, an experimental survey was carried out on 40 people,
who were equally divided into 20 men, 10 from the public sector and 10 from the
private sector, and 20 women, 10 from the public sector and 10 from the private sector,
The application of this research instrument was for experimental purposes whose
purpose was to verify that the ﬁnal survey does not present any error in order to be able
to proceed with the investigation.
Once the focal group and the experimental test were carried out, ﬁnal survey was
carried out based on literary review and the results of two previous instruments. The
questionnaire was based on proposed objectives and previous studies, modiﬁed and
adapted for development of the research, the survey was about 7 sections comprise
following topics: (1) demographic aspects, (2) types of cellphones, (3) purchase
decision process, (4) inﬂuence of comments, (5) visits to restaurants, (6) type of social
network used and (7) factors inﬂuencing restaurant visit decision.
In the ﬁrst section of questionnaire demographic aspects such as age, gender,
income level, education level, etc. were met. Age was given according to the study
done by [38], which established the following age ranges: less than 20, 20–29, 30–39,
and 40–49 and 50 years and over. Gender: male and female based [39]. The income
level in reference to [33] who establishes the wages that a worker can naturally earn:
less than a basic salary, between 1 and 2 basic salaries, 3 and 4 basic salaries, 5 and 6
basic salaries and more 6 uniﬁed basic salaries; For this investigation it was simply
adapted according to uniﬁed basic salary a person earns in Ecuador. The other
demographic aspects were of own elaboration.
Among factors inﬂuencing decision to visit restaurants were based on the study of
Hwang and Park [40] who mentioned different factors but only those were considered
for research such as: quality food, staff attention, environment, price and location, the
latter two also recommended by Gavalas et al. [41] in turn these factors were added
infrastructure came out as relevant in focus group.
Inﬂuence of Social Networks from Cellphones
995

In the eighth section, which seeks to identify reasons why a user is motivated to
make a publication or commentary after their experience in a restaurant, the study of
Jeong and Jang [10] was used. Based on previous studies in which following aspects
were investigated: concern for others, positive feelings and help to restaurant,
according to these reasons were used questions from questionnaire proposed by the
authors.
The ninth and last section referring to speciﬁc application for restaurant, question
was based on the study carried out by Payne et al. [39] in the same way question asked
by these authors to research theme was adapted; Mo Kwon et al. [6] proposed the
options: special restaurant offers, exclusive promotions, restaurant menu, make a
reservation; We also considered the study of Wang et al. [8] in which propose the
options: comments and map of how to get there. These elements were added rating
stars and menu prices were considered as relevant elements.
The second, third, ﬁfth, sixth and seventh sections related to: smart cellphones users
have, purchase decision process, restaurant visit, afﬁrmations and social network,
respectively, were made by themselves.
The survey was performed using the 5-point Likert scale method except for the
second section where a two-choice question (Smartphone Tablet) was established, the
ﬁfth section corresponding to a dichotomous yes or no question and the seventh section
where Respondents should write their response. For the third section the points were
equal to: 1 = never and 5 = always; The fourth section was equivalent to 1 = not
important and 5 = very important; The sixth, eighth and ninth sections of the rating
options were: 1 = totally disagree and 5 = totally agree.
In this research he used sampling for convenience or intentional, and the sample is
comprised of the inhabitants of the parish of Salinas who have a Smartphone or Tablet,
an account in their social network of preference and in turn are incorporated into the
workforce either In public or private economic sector.
5
Data Collection and Analysis
The survey was conducted in Salinas parish located on south of Santa Elena province, a
total of 342 surveys were collected, data were obtained through online surveys through
the Google Drive tool, a total of 296 surveys were conducted by This medium which
consisted of sharing following link: https://goo.gl/forms/YIMEalDaWgzgETll2 where
people receiving it had to ﬁll out the survey according to their characteristics and
opinions, once ﬁlled the next step was to send it to automatically The system was
archived, the link was only shared with people who fulﬁlled the previously established
proﬁle and the duration of the survey was approximately 3 min, the remaining 46
surveys were carried out physically through the collaboration of two university
students.
996
H. Rodríguez et al.

6
General Information
Then the internal calculation consistency index or Cronbach’s alpha whose value was
obtained with the statistical program IBM SPSS Statistics 21, resulting in 0.88 see
Table 1, which shows that there is consistency or reliability in the work accomplished.
The corresponding analysis was also carried out to verify the study’s hypothesis.
Give the meaning of the variables in linear wording and it is important to compare
the criteria used.
Total number of people surveyed, 54.1% were men and 45.9% were women, most
of them single, 47.7%. 57% belong to the private sector, while 43% to the public
sector. The majority indicated to have an age between 20 and 29 years equivalent to
57.9%. Regarding the level of education, the university prevailed with 66%, ﬁnally
51.2% earned between $ 366 to $ 732 dollars.
Table 2 shows the results by gender of users who visited a restaurant inﬂuenced by
comments read in a social network, where the number of men surveyed was slightly
higher than that of women, however it can be indicated that the percentage results are
very similar, for 66.5% of the male gender the comments if they have been inﬂuential,
in the same way for 65% of the feminine gender so it is concluded that there is no
signiﬁcant difference between both genders (Table 3).
A total of 6 factors previously studied were used in the survey, below are detailed
the results of the “important” and “very important” options that answered people who
equated where the most inﬂuential factors when visiting a restaurant are the attention of
staff with 87.8% followed by quality of food with 86.8% and environment 71.7%
(Table 4).
The following table shows the gender and percentage of social networks preferred
by users to comment and/or publish their positive experience lived in a restaurant, the
most prevalent social network among respondents was Facebook with 68.4% followed
by Instagram with 14.9% and thirdly Twitter with 13.2%, while 3.5% indicated the
other option (Table 5).
The variables used to test the hypothesis demonstrate users’ decision to visit a
restaurant dependent on the information evidenced in social networks through a
Smartphone or Tablet (Table 6).
From the selection of new study variables, these are:
• Age
• Have you ever visited a restaurant inﬂuenced by comments read on a social
network?
Table 1. Cronbach’s Alpha.
Reliability statistics
Cronbach’s alpha No. of elements
0.88
40
Inﬂuence of Social Networks from Cellphones
997

Table 2. Results general data
Variables
Frequency Percentage
Gender
Male
185
54.09
Female
157
45.91
Total
342
100.00
Age
Less than 20
5
1.46
20–29
198
57.89
30–39
82
23.98
40–49
34
9.94
50 years old and over
23
6.73
Total
342
100.00
Level of scholarship
Primary school
3
0.88
High school
56
16.37
University
227
66.37
Postgraduate
56
16.37
Total
342
100.00
Civil status
Single
163
47.66
Married
106
30.99
Cohabitation
47
13.74
Divorced
18
5.26
Widower
8
2.34
Total
342
100.00
Economic sector
Public
147
42.98
Private
195
57.02
Total
342
100.00
Table 3. Gender * Have you ever visited a restaurant inﬂuenced by comments read on a social
network?
Yes
No
Total
Gender Male
Count 123
62
185
%
66.5% 33.5% 100.0%
Female Count 102
55
157
%
65.0% 35.0% 100.0%
Total
Count 225
117
342
%
65.8% 34.2% 100.0%
998
H. Rodríguez et al.

We proceed to the statistical context, considering the following detail:
Hypothesis Formulation:
H0:
The means of the samples are equal with 95% reliability
Ha:
The means of the variables are not equal with 95% reliability
The statistical software SPSS is used, obtaining the following result (Table 7):
Table 4. Most inﬂuential factors for visiting a restaurant
Factors
Important
Very important Total
Price
26.9% (99)
35.4% (121)
64.3% (211)
Quality of food
21.6% (74)
65.2% (223)
86.8% (297)
Staff`s Attention 32.5% (111) 55.3% (189)
87.8% (300)
Environment
35.4% (121)
36.3% (124)
71.7% (245)
Localization
34.2% (117)
24.6% (84)
58.8% (201)
Infrastructure
34.8% (119)
28.1% (96)
62.9% (215)
Table 5. Gender * Social Networks
If you had to comment or post your positive experience lived in a
restaurant, what social network would you do?
Facebook Twitter Instagram Other Total
Gender Male
Count
134
31
13
7
185
%
72.4%
16.8%
7.0%
3.8%
100.0%
Female Counnt 100
14
38
5
157
%
63.7%
8.9%
24.2%
3.2%
100.0%
Total
Count
234
45
51
12
342
%
68.4%
13.2%
14.9%
3.5%
100.0%
Table 6. Chi-square test
Value
gl Sig. asymptotic (bilateral)
Chi-squared of Pearson
111.403a 4
0.000
Reason of plausibility
117.685
4
0.000
Linear association by linear 105.77
1
0.000
# valid cases
342
a 0 box (0, 0%) have an expected frequency less than 5. Minimum
expected frequency 5, 47
Table 7. Have you ever visited a restaurant inﬂuenced by comments read on a social network?
Sum of squares gl
Quadratic mean F
Sig
Inter-groups
6110
4 1.528
7.265 0.000
Intra-groups 70863
337 0.210
Total
76973
341
Inﬂuence of Social Networks from Cellphones
999

1. Level of signiﬁcance: 5%
2. Choice of statistical test: Analysis of variance (ANOVA)
3. Estimation of p-value: 0.384
4. Decision making: Because: p > 0.00. The null hypothesis is not accepted, so the
mean value of the samples is signiﬁcantly different
However, with the intention of knowing which groups show signiﬁcant differences
between user’s ages and signiﬁcant between the use of intelligent equipment among the
people working in the public and private sector, a post hoc test is carried out, the same
one that Evidence the following results (Table 8):
With these results, it is evident that two groups are formed, from which it can be
seen that there are no signiﬁcant differences for the users whose ages oscillate between
20–29 years; 30–39 years; Less than 20 years; 40 and 49 years of age. Not so with
those users whose age is between 50 and over. It is understood therefore that the
difference between this last categories is remarkably signiﬁcant with respect to the
other 4 groups.
7
Conclusions
According to the objectives that were raised in the investigation the following con-
clusions can be reached:
(1) The most inﬂuential factors in the decision-making process of visiting restaurants
by the users are: the attention of the staff, quality food and atmosphere.
(2) The social network preferred by users to comment and/or post after their positive
experience lived in a restaurant is Facebook, followed by Instagram and Twitter.
(3) It is concluded that there is no greater variability with respect to gender when
visiting a restaurant or making a comment unlike the age where there is a greater
tendency in young people than in the elderly.
Table 8. HSD de Tukeya, b
Age
N
Subset for
alpha = 0.05
1
2
20–29
198
1.26
1.37
30–39
82
1.37
1.40
Less than 20
5
1.40
1.44
40–49
34
1.44
1.78
50 years old and over
23
Sig.
0.784
0.061
The means for the groups in the homogeneous subsets. a. Uses
the sample size of the harmonic mean = 17.233. b. The group
sizes are not equal. The harmonic mean of the group sizes is
used. Level I error levels are not guaranteed.
1000
H. Rodríguez et al.

This research is not free of some limitations, since this study uses intentional
sampling or convenience, so the results may not be compatible for other types of
sampling, such as people who are not incorporated into the labor force, in the same way
the research employed a certain sector of study (parish Salinas) so the results may be
different in other segments or places of the province.
References
1. Sellens, J.T., Requena, J.V.: TIC, conocimietno y crecimiento económcio: un análisis
empírico, agregado e internacional sobre las fuentes de la productividad. Econ. Ind. 360, 41–
60 (2006)
2. Echeberúa, E., De Corral, P.: Adicción a las nuevas tecnologías y a las redes sociales en
jóvenes: un nuevo reto Addiction to new technologies and to online social networking in
young people. Adicciones 22(2), 91–96 (2010)
3. Fernández, M.T., Cuadrado, R.: El impacto de las nuevas tecnologías en el sector turístico.
Int. J. World Tour. 1, 10–18 (2014)
4. Bergamini, T.P., de Bernardo González, C.M.: Marketing móvil: una nueva herramienta de
comunicación: análisis y nuevas perspectivas para el mercado español. (primera ed.).
Netbiblo La Coruña (2007)
5. Martínez, I.J., Aguado, J.M.: El desarrollo de la telefonía móvil como plataforma mediática
Hologramática 3(5), 21–39 (2006)
6. Mo Kwon, J., Bae, J.I., Blum, S.C.: Mobile applications in the hospitality industry.
J. Hosp. Tour. Technol. 4(1), 81–92 (2013)
7. Okumus, B., Bilgihan, A.: Proposing a model to test smartphone users’ intention to use
smart applications when ordering food in restaurants. J. Hosp. Tour. Technol. 5(1), 31–49
(2014)
8. Wang, D., Park, S., Fesenmaier, D.: An examination of information services and smartphone
applications. In: Proceedings of 16th Annual Graduate Student Research Conference in
Hospitality and Tourism (2011)
9. Mattila, A.S.: The use of narrative appeals in promoting restaurant experiences.
J. Hosp. Tour. Res. 26(4), 379–395 (2002). https://doi.org/10.1177/109634802237485
10. Jeong, E., Jang, S.S.: Restaurant experiences triggering positive electronic word-of-mouth
(eWOM) motivations. Int. J. Hosp. Manage. 30(2), 356–366 (2011). https://doi.org/10.1016/
j.ijhm.2010.08.005
11. Zhang, Z., Ye, Q., Law, R., Li, Y.: The impact of e-word-of-mouth on the online popularity
of restaurants: A comparison of consumer reviews and editor reviews. Int. J. Hosp. Manage.
29, 694–700 (2010)
12. Boo, S., Kim, J.: Comparison of negative eWOM intention: an exploratory study. J. Qual.
Assur. Hosp. Tour. 14(1), 24–48 (2013). https://doi.org/10.1080/1528008x.2013.749381
13. Qureshi, I.A., Nasim, I., Whitty, M.: Impact of social media marketing on the consumer
preferences in restaurant industry: an empirical study of Pakistan. IOSR J. Bus. Manage.
(IOSR-JBM) 16(9), 65–74 (2014)
14. Doh, S.J., Hwang, J.S.: How consumers evaluate eWOM (electronic word-of-mouth)
messages. CyberPsychol. Behav. 12(2), 193–197 (2009). https://doi.org/10.1089/cpb.2008.
0109
15. Kwok, L., Yu, B.: Spreading social media messages on Facebook: an analysis of restaurant
business-to-consumer communications. Cornell Hosp. Q. 54(1), 84–94 (2013). https://doi.
org/10.1177/1938965512458360
Inﬂuence of Social Networks from Cellphones
1001

16. Rothenﬂuh, F., Germeni, E., Schulz, P.J.: Consumer decision-making based on review
websites: are there differences between choosing a hotel and choosing a physician? J. Med.
Internet Res. 18(6), 1–13 (2016). https://doi.org/10.2196/jmir.5580
17. Shin, D., Song, J.H., Biswas, A.: Electronic word-of-mouth (eWOM) generation in new
media platforms: the role of regulatory focus and collective dissonance. Mark. Lett. 25(2),
153–165 (2014). https://doi.org/10.1177/1096348013515918
18. Wang, W., Li, Y., Duan, Z., Yan, L., Li, H., Yang, X.: Integration and Innovation Orient to
E-Society Volume 2: Seventh IFIP International Conference on e-Business, e-Services, and
e-Society, vol. 252. Springer, China (2010)
19. Berthon, P.R., Pitt, L.F., Plangger, K., Shapiro, D.: Marketing meets Web 2.0, social media,
and creative consumers: implications for international marketing strategy. Bus. Horiz. 55(3),
261–271 (2012). https://doi.org/10.1016/j.bushor.2012.01.007
20. Nafría, I.: Web 2.0: El usuario, el nuevo rey de Internet (Cuarta ed.). Gestión 2000,
Barcelona (2007)
21. Cheung, C.M., Lee, M.K.: What drives consumers to spread electronic word of mouth in
online consumer-opinion platforms. Decis. Supp. Syst. 53(1), 218–225 (2012). https://doi.
org/10.1016/j.dss.2012.01.015
22. Constantinides, E., Fountain, S.J.: Web 2.0: conceptual foundations and marketing issues.
J. Direct Data Digit. Mark. Pract. 9(3), 231–244 (2008). https://doi.org/10.1057/palgrave.
dddmp.4350098
23. Laudon, K.C., Guercio, T.C.: E-commerce 2013. Negocios, tecnología, sociedad (Novena
ed.). Pearson, México (2014)
24. Merodio, J.: Marketing en Redes Sociales: Mensajes de empresa para gente selectiva. Juan
Merodio (2010)
25. Kaplan, A., Haenlein, M.: Users of the world, unite! The challenges and opportunities of
social media. Bus. Horiz. 53(1), 59–68 (2010)
26. Siemens, G., Weller, M.: La enseñanza superior y las promesas y los peligros de las redes
sociales. Univ. Knowl. Soc. J. 8(1), 157–163 (2011)
27. Maghnati, F., Ling, K.C., Nasermoadeli, A.: Exploring the relationship between experiential
marketing and experiential value in the smartphone industry. Int. Bus. Res. 5(11), 169–177
(2012)
28. Mafé, C.R., De LosRíos, J.T.G.: Inﬂuencia de las motivaciones hedónicas en el valor
percibido de las web 2.0 de alojamientos turísticos. Gran Tour Rev. Investig. Turíst. 7, 4–22
(2014)
29. Ricolfe, J.C., Pérez, C.E.: Inﬂuencia del comercio electrónico en el sistema agroalimentario.
Distrib. Consumo 3(69), 93–99 (2003)
30. Morales, P.M.C., Guzmán, T.J.L.G., Cuadra, S.M., Agüera, F.O.: Análisis de la demanda del
oleoturismo en Andalucía/Analysis of demand of olive tourism in Andalusia. Rev. Estud.
Reg. 104, 133 (2015)
31. Jiang, L., Yang, Z., Jun, M.: Measuring consumer perceptions of online shopping
convenience. J. Serv. Manage. 24(2), 191–214 (2013)
32. Bigné, J.E., Ruiz, C.: Antecedentes de la decisión de compra en los entornos virtuales.
Propuesta de un modelo descriptivo en la compra interactiva. Rev. Eur. Dir. Econ. Empresa
14(4), 141–158 (2005)
33. Barutçu, S.: Attitudes towards mobile marketing tools: a study of Turkish consumers.
J. Target. Meas. Anal. Mark. 16(1), 26–38 (2007). https://doi.org/10.1057/palgrave.jt.
5750061
34. Wei, Y.P., Long, P.H.: Consumer’s perception of mobile social media advertising: the case
of casual-dining restaurants. Adv. Inf. Sci. Serv. Sci. 7(1), 13 (2015)
1002
H. Rodríguez et al.

35. Urueña-López, A., Agudo-Peregrina, Á.F., Hidalgo-Nuchera, A.: Internet como fuente de
información en el proceso de compra: hacia una concepción integral del consumidor. El Prof.
Inf. 20(6), 627–633 (2011)
36. Kim, H.H., Law, R.: Smartphones in tourism and hospitality marketing: a literature review.
J. Travel Tour. Mark. 32(6), 692–711 (2015). https://doi.org/10.1080/10548408.2014.
943458
37. Karjaluoto, H., Lehto, H., Leppäniemi, M., Jayawardhena, C.: Exploring gender inﬂuence on
customer’s intention to engage permission-based mobile marketing. Electron. Mark. 18(3),
242–259 (2008). https://doi.org/10.1080/10196780802265793
38. Xie, F.: The University Of Shefﬁeld. Understanding Mobile Search: A Survey of
Smartphone Users’ Location, Social Context, Search Activity and Search Motivation (2015)
39. Payne, K.F.B., Wharrad, H., Watts, K.: Smartphone and medical related App use among
medical students and junior doctors in the United Kingdom (UK): a regional survey. BMC
Med. Inform. Decis. Mak. 12(1), 1 (2012)
40. Hwang, J., Park, S.: Social media on smartphones for restaurant decision-making process. In:
Information and Communication Technologies in Tourism 2015, pp. 269–281. Springer
International Publishing, Suiza(2015)
41. Gavalas, D., Konstantopoulos, C., Mastakas, K., Pantziou, G.: Mobile recommender systems
in tourism. J. Netw. Comput. Appl. 39, 319–333 (2014)
Inﬂuence of Social Networks from Cellphones
1003

Digital Feedback and Academic Resilience
Laura Guerra(&), Dulce Rivero, Stalin Arciniegas,
and Santiago Quishpe
Pontiﬁcal Catholic University of Ecuador, Headquarter Ibarra,
Jorge Guzman Rueda Avenue, The Victoria, Ibarra, Ecuador
{lrguerra,dmrivero,smarciniegas,
squishpe}@pucesi.edu.ec
Abstract. This descriptive research aims to explore the student perceptions
about technology and their resilience when they develop mathematical problems
through digital technologies. The population was 19 students, who had allowed
to use geogebra software and matrixcalc application to verify problem solving.
The study is supported by the theories of self-determination, connectivism and
social construction of knowledge. The methodology is a combination of pro-
posals of Hattie et al. named by [4, 6] and of the authors. A survey with a
reliability coefﬁcient of 0.81 was distributed to students, obtaining that more
than half of them perceive themselves as capable of achieving their goals,
maintaining their sense of humor, feeling the family support, controlling the
development of activities, considering the authorized programs that are easy to
use, useful and reliable. So, the academic strategy used favors the development
of academic resilience, which is consistent with the postulates of Vaquero [9].
Keywords: Digital feedback  Academic resilience  Geogebra
Theory of self-determination
1
Introduction
The process of teaching and learning mathematics can be a challenging process for both
students and teachers. Some abstract concepts are not as friendly as for the student to
internalize and understand immediately (Prieto et al. named by [1]); for the traditional
teacher it is imperative to emigrate from the use of conventional resources to tech-
nological resources and to expose their knowledge through the resolution of practical
problems solved with the use of technology, emphasizing critical thinking and
reﬂection rather than response itself [2]. The technological tools that approach the study
of mathematics allow the student to solve a problem through dynamic interactions such
as making visual representations of ﬁgures, graphs, checking properties, varying
parameters, among others [3]. These digital activities are part of a transversal learning
process, because mathematical concepts are more appropriately assimilated and tech-
nological skills are reinforced. When the development of a mathematical problem is
combined with its basic principles and theorems and the use of technological tools, the
apprentice is being offered the opportunity that, even if he is mistaken in an exercise
step, he can learn by strengthening his resilient capacity. [4] states that resilience is a
condition that can be learned by humans, and deﬁnes it as the ability to resist
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_95

unpleasant disturbances and events, to reorganize and emerge from the stressful situ-
ation. As an evolutionary process, whenever the student tries to solve a problem, he
makes a mistake, the system indicates where he was wrong and why, the learning will
be greater and will affect his attitude and ability to face challenges.
2
Objective of Research and Previous Studies
This work aims to implement a software in solving problems related to mathematical
presuming a theoretical model linking technology and resilient capacity in order to
describe the perceptions of students about the use of technological tools that provide
learning feedback immediately to foster academic resilience.
2.1
Theoretical Bases
The study of university mathematics involves relating formulas, theorems, properties
with approaches of problems that are generally associated with the exercise of their
profession and which, being a subject of the ﬁrst semesters, prepares the student to a
higher cognitive level for the processing of further knowledge. To carry out this task,
researchers such as [3], afﬁrm that the students reasoning increases when they use
technological programs that feedback or reinforce knowledge during the development
of a task. Nazihatulhasanah and Nurbiha [1] found that students are more interested in
the study of statistics with the use of digital tools if these are properly selected by the
teacher. Studies that relate timely feedback as a stage of instruction were performed by
[5], where their ﬁndings reveal that the student should be given the opportunity to
internalize the given feedback so that they have an effect on learning; hence the
importance of electronic systems, where the learner may have several opportunities to
answer a task. [6] analyzed the formative digital evaluation as a mechanism of moti-
vation and improvement of student performance.
On the other hand, resilience is the ability of the person to adapt to the challenging
events that are presented, through processes of transformation and growth [7]. In the
opinion of [8], the resilience of a system is the ability to absorb the effects of adverse
situations, effectively and timely resetting, including restoration and improvement of its
essential basic functions, the author mentions the project emBRACE—Building
Resilience Amongst Communities in Europe, that frames the resilience as a concept
that includes the domains of actions, learning and resources-capacities. Navarro named
by [9] states that resilience is a dynamic concept related to personal characteristics
(skills, qualities), multisystemic characteristics (family, school, society) and the rela-
tionships between the person and his/her context. Resilience and technology are two
very close concepts in engineering, for example in the ﬁeld of information systems and
telecommunications; the technological resilience is conceived as the capacity of a
person, group, organization to face the changes, challenges and advances of the
technological society of these days. In the educational context, resilience called aca-
demic resilience is deﬁned as the ability of the student to successfully face all the
difﬁculties that arise during the learning of a particular subject [10].
Digital Feedback and Academic Resilience
1005

Theories that support the research are the theory of self-determination, connec-
tivism theory, and the theory of social construction of knowledge. The theory of
self-determination establishes that the human being adapts or updates their capacities
through three processes that are intrinsic motivation, social internalization and inte-
gration with others; being a characteristic that can be sponsored or overshadowed by
conditions of the environment [11]. The environment collaborates with the three pro-
cesses mentioned, when the person has satisﬁed his basic psychological needs. Skinner
and Pitzer [12], deﬁned as autonomy (the behavior assumed is optional is not coercive),
competence (feeling effective and useful in the performance of a task) and relationship
(feeling connected with other important people). According to [6], numerous studies
have revealed that there have been improvements in the student’s interest, behaviors,
and achievements when interventions are made using the theory of self-determination.
The theory of connectivism treats the learning process in the digital age, indicating that
there are two domains that interact in this process, which are the cognitive domain and
the social domain (Siemens named by [13]). The social constructivism theory aims to
explain how students learn when they interact with other people. Vygotsky established
the term of Zone of proximal development (ZDP), to refer to the difference between the
current cognitive development and the potential development of a human being. His
approaches indicate that, if a student relates to another person who is more outstanding
than him on a particular subject, the breach in his ZDP can be shortened, due to the
support, guidance or reinforcements given by his or her peer, to gain knowledge.
2.2
Literature Review
[14] carried out a research to transfer basic concepts of resilience to the educational
ﬁeld; distinguishing between health specialists who treat resilience “from within” and
school teachers as factors to promote “contextual” resilience. They analyzed two types
of surveys related to the topic, such as CMC (Class Map Survey) and PPEcoS (The
Protective Peer Ecology Scale), determining that the ﬁrst poses as important attributes
of resilience the relationship with peers, teachers and parents, expectations of school
success by their own merits, self-determination to achieve their goals and make their
own decisions and capacity of self-control to manage their behavior in order to achieve
their goals; The second survey is focused on how to cope with bullying.
[15] studied the resilience from the orientation to the future of the students. They
based their research on the Design of Life (LD) scheme, which deals with individuals
who build their future from routes or processes that continue in the present. These
people with orientations to the future, tend to face any obstacle in order to achieve at
the proposed goal, that was the reason why the researchers considered them resilient
people. To test their hypothesis, they designed and validated an instrument to measure
personal characteristics and determined a high correlation between resilience and future
orientation. [6] analyzed the contribution of digital feedback to students motivation and
achievements, developing a system that provided feedback to the students, the teacher,
and assigned a task adapted to the level of the learner. Their ﬁndings demonstrated
relationships between achievement and motivation and use of the prototype used.
The acceptance model of the technology (TAM) developed by Davis in 1989
allows to study the degree of acceptance of a technological system by the users,
1006
L. Guerra et al.

reﬂected in their use behavior, from constructs such as the perceived ease of use and
utility perceived [16]. However, several authors have modiﬁed the original model,
adding or eliminating constructs and applying and validating them in different contexts
with different technologies [17, 18]. For example, [16] considered the motivation and
perceived conﬁdence in his TAM model to study the acceptance of mobile banking
systems.
3
Methodology
This research aims to explore how students develop mathematical problems through
digital technologies and environments and their perception of their resilience. It is a
descriptive investigation, of qualitative nature, where the case study method was
applied in a course of systems engineering. The study population was 19 students of
linear algebra, who had at their disposal a virtual course developed with Moodle for the
reinforcement of face-to-face classes, geogebra software (free software) and matrixcalc
application (web page).
In the teacher’s interventions they were shown the use of geogebra to solve the
exercises, stressing the importance of learning to solve problems in the conventional
way because only by using the software was not a guarantee of optimal training, and
specially for them that as students of systems engineering had to know the natural
mechanics of the operation of a process and then to carry out the computer programs
that automated it.
For some mathematical calculations, many consecutive operations must be per-
formed which are interlaced and that a small error in one of them can affect all
subsequent ones. This situation affects the students, causing them frustration and
fatigue. In this experience, students were allowed to use technological tools to verify
the resolution of problems of this nature. This strategy was assumed by the students as
a challenge or competition with the software package and allowed the teacher to
establish different exercises of the same subject, so that the students grouped in pairs
compete| with each other, while being evaluated by the software.
The experience was developed during ﬁfteen sessions of formative evaluation
during a semester, where a pair was cataloged like winner if they obtained the accuracy
of the results in the shortest time. The rest of the couples had to inform in which step of
the procedure they had been wrong and the reasons of it. The last step of the activity
was to vary problem parameters (examples, a coefﬁcient, a sign, a coordinate, a vector)
and perform the corresponding analysis. Pairs were discretionally reformulated by the
teacher, randomly and even by the students themselves.
At the beginning of the course a diagnostic test was applied to know the students
previous knowledge about the subject and their expectations of success in the subject.
The methodology used is a combination of the proposal of Hattie et al. (named by [6])
for the differentiation of feedback levels (task, task processing, self regulation, self
level); of the [4] guidelines for the resilience approach (vulnerability and adaptability)
and contributions of the authors.
At the end of the course, a survey was distributed to students to determine their
perceptions about the use of technological tools for learning and their resilience
Digital Feedback and Academic Resilience
1007

capacity. This instrument was based on items proposed by [15], Alalwan et al.
appointed by [16], Mohammadi nominated by [17], and [19–24]. A pilot test was
carried out with ﬁve students from another course and the ﬁnal instrument was vali-
dated by two experts to determine if the adaptation of the instrument was in accordance
with the context, then a reliability coefﬁcient of 0.81 was obtained. A Likert scale was
used for the evaluation of the answers of thirty-four questions that constitute the
questionnaire, which were distributed in nine dimensions of analysis: perceived ease of
use; perceived utility; technological conﬁdence, behavioral intention, self-regulation;
sense of humor; control under pressure; empathy and support; control and purpose
(self-efﬁcacy).
Fig. 1. Proposed academic resilience model
Fig. 2. Geogebra application in equations system
1008
L. Guerra et al.

Based on the resilience diagram presented by [4], the model proposed in this
research is summarized in Fig. 1:
Evidences of the applied strategy are shown in the following ﬁgures. In Fig. 2, the
system of equations is solved by determinants, where varying the coefﬁcients of the
variables different solutions of the system are obtained. In Fig. 3, the determinant of a
2 * 2 matrix is applied to calculate the area of the parallelogram, sliding points
established in the ﬁgure is detected as they change the elements of the matrix and
therefore its determinant.
4
Results Analysis
In agreement with [7], the analysis of the results is based on the interpretation of the
theory and not verify any of them, for which the triangulation of the information
obtained from the instrument applied with the academic performance at the end of the
semester is done.
First, it is observed that the most of students used the mobile technology to connect
to the software allowed to verify the results of the exercises, as shown in Fig. 4.
Regarding the constructs associated with the personal characteristics that inﬂuence
resilience, it was determined that more than half of respondents perceive themselves as
capable of achieving their goals (88.83%), maintaining their sense of humor despite
they work under adverse situations or of high tension (75%). Only 4.17% of the
respondents reveal their disagreement with seeing the funny side to pressing situations.
91.67% said that they felt the support of family and friends for the achievement of their
goals and 83.34% stated that they always maintain control in the development of
activities that facilitate their purposes in life (Fig. 5).
Fig. 3. Geogebra application in parallelogram area
Digital Feedback and Academic Resilience
1009

The students’ perceptions about the use of the considered technological tools
(geogebra and matrixcalc) are shown in Fig. 6, where it can be observed that 100% of
the students consider these programs easy to use, while the 91.66% of them ﬁnd useful
for their study; all the respondents declare they have conﬁdence in the results generated
by these tools when performing the exercises and therefore they intend to continue to
use them in the future.
By comparing students’ perceptions about the personal characteristics considered
and on the use of technology for learning algebra in conjunction with academic per-
formance, it was noted that all students approved the subject with the exception of a
student who retired from college.
Fig. 4. Technological medium used (%)
Fig. 5. Perceibed personal characteristics (%)
1010
L. Guerra et al.

5
Conclusions and Recommendations
It is very important to study the effects of technology in the teaching-learning process
because the advances in this area are constantly growing, and it is part of the everyday
life of any person.
The objective of this research was to describe the students’ perceptions in two
respects: regarding the acceptance of computer programs that feedback immediately
during the resolution of mathematical exercises; and with respect to their personal
characteristics related to resilience. Most of the students expressed their acceptance in
the use of technology (geogebra and matrixcalc) for the study of linear algebra and
were described with personal characteristics that determine the resilient individuals,
which allows to infer that the teaching - learning strategy used favors the development
of academic resilience, as it was outlined in the proposed model and agrees with the
postulates of [9] who assert that resilience can be learned through interactions with
technological environments.
The immediate evaluation of the exercises solved by the students gives the teacher,
opportunities to detect the mathematical weaknesses of the students and to exert actions
for their strengthening, as refers [6], obtaining thus, a high performance of the course.
The course performance was good because all students were approved with the
exception of a student who withdrew from college for personal reasons. This study did
not contemplate conducting correlational analysis among the variables studied, how-
ever, it is recommended to continue with research in this regard, in order to discern
whether there is a relationship between immediate digital feedback and Academic
performance.
Fig. 6. Acceptance of the use of technology (%)
Digital Feedback and Academic Resilience
1011

References
1. Nazihatulhasanah, A., Nurbiha, A.: The effects of GeoGebra on students achievement.
Procedia Soc. Behav. Sci. 172, 208–214 (2015)
2. Bozkurt, G., Ruthven, K.: Classroom-based professional expertise: a mathematics teacher’s
practice with technology. Educ. Stud. Math. 94, 309–328 (2017)
3. Olsson, J.: The contribution of reasoning to the utilization of feedback from software when
solving mathematical problems. Int. J. Sci. Math. Educ. 1, 1–21 (2017)
4. Matzenberger, J.: A novel approach to exploring the concept of resilience and principal
drivers in a learning environment. Multicult. Educ. Technol. J. 7, 192–206 (2013)
5. Narciss, S.: Designing and evaluating tutoring feedback strategies for digital learning
environments on the basis of the interactive tutoring feedback model. Digit. Educ. Rev. 23,
1–26 (2013)
6. Faber, J.M., Luyten, H., Visscher, A.J.: The effects of a digital formative assessment tool on
mathematics achievement and student motivation: results of a randomized experiment.
Comput. Educ. 106, 83–96 (2017)
7. Savolainen, T., Ikonen, M., Nurmenniemi, H.: Trust and resilience in entrepreneurial
perspective: empirical ﬁndings from the developments in entrepreneurs’ stories. In: 11th
European
Conference
on
Innovation
and
Entrepreneurship,
Aaltio,
liris
and
Tunkkari-Eskelinen, Minna, pp. 726–733 (2016)
8. Jülich, S.: Towards a local-level resilience composite index: introducing different degrees of
indicator quantiﬁcation. Int. J. Disaster Risk Sci. 8, 91–99 (2017)
9. Vaquero, E., Urrea, A., Mundet, A.: Promoting resilience through technology, art and a child
rights-based approach. Rev. Cercet. Interv. Soc. 45, 144–159 (2014)
10. Coronado-Hijón, A.: Academic resilience: a transcultural perspective. Procedia Soc. Behav.
Sci. 237, 594–598 (2017)
11. DeHann, C., Hirai, T., Ryan, R.: Nussbaum’s capabilities and self-determination theory’s
basic psychological needs: relating some fundamentals of human wellness. J. Happiness
Stud. 17, 2037–2040 (2015)
12. Skinner, E., Pitzer, J.: Developmental dynamics of student engagement, coping, and
everyday resilience. In: Christenson, S.L., et al. (eds.) Handbook of Research on Student
Engagement, vol. 1, pp. 21–44. Springer (2012)
13. Durairaj, K., Umar, I.: A proposed conceptual framework in measuring social interaction and
knowledge construction level in asynchronous forum among university students. Procedia
Soc. Behav. Sci. 176, 451–457 (2015)
14. Song, S., Doll, B., Marth, K.: Classroom resilience: practical assessment for intervention. In:
Prince-Embury, S., Saklofske, D.H. (eds.) Resilience in Children, Adolescents, and Adults:
61 Translating Research into Practice. The Springer Series on Human Exceptionality, vol. 1,
pp. 61–72. Springer Science + Business Media, New York (2013)
15. Di Maggio, I., Ginevra, M., Nota, L., Soresi, S.: Development and validation of an
instrument to assess future orientation and resilience in adolescence. J. Adolesc. 51, 114–122
(2016)
16. Kumar, S.: Integrating cognitive antecedents into TAM to explain mobile banking
behavioral intention: a SEM-neural network modeling. Inf. Syst. Front. 1, 1–13 (2017)
17. Almaiah, M., Jalil, M., Man, M.: Extending the TAM to examine the effects of quality
features on mobile learning acceptance. J. Comput. Educ. 3, 453–485 (2016)
18. Bazelais, P., Doleck, T., Lemay, D.J.: Investigating the predictive power of TAM: a case
study. of CEGEP students’ intentions to use online learning technologies. Educ. Inf.
Technol. 1, 1–19 (2017)
1012
L. Guerra et al.

19. Ngai, E.W.T., Poon, J.K.L., Chan, Y.H.C.: Empirical examination of the adoption of
WebCT using TAM. Comput. Educ. 48, 250–267 (2007)
20. Davis, F.D.: Perceived usefulness, perceived ease of use, and user acceptance of information
technology. MIS Q. 13, 319–340 (1989)
21. Liébana-Cabanillas, F., Sánchez-Fernández, J., Muñoz-Leiva, F.: Antecedents of the
adoption of the new mobile payment systems: the moderating effect of age. Comput. Hum.
Behav. 35, 464–478 (2014)
22. Pulgar, L.: Factores de Resiliencia presentes en estudiantes de la Universidad del Bío Bío,
sede Chillán. http://cybertesis.ubiobio.cl/tesis/2010/pulgar_l/doc/pulgar_l.pdf
23. Broche, Y., Rodriguez, B., Pérez, S., Santaella, G., Díaz, A., Hernández, A., Blanco, Y.:
Escala de Resiliencia Connor y Davidson (CD-Risk). In: Feijóo, S. (eds.) Validación de
Instrumentos Psicológicos: Criterios Básicos, vol. 1, pp. 71–75 (2012)
24. Wu, M.-C.Yang, F.: An empirical investigation of habitual usage and past usage on
technology acceptance evaluations and continuance intention. ACM SIGMIS Database 39,
48–73 (2008)
Digital Feedback and Academic Resilience
1013

Mobile Learning: Challenging the Current
Educational Model of Communication Studies
Verónica Yépez-Reyes(&)
Pontiﬁcia Universidad Católica del Ecuador, Quito, Ecuador
vyepezr@puce.edu.ec
Abstract. Mobile learning or M-learning, regarded as a speciﬁc ﬁeld of elec-
tronic learning, is considered to stem from the affordances of computer-mediated
communication. In this way, it fosters connection and interaction “on the move”.
M-learning beneﬁts from the ubiquity of information and communication
technologies (ICTs), consequently it is not bounded in space and time. However,
its study goes beyond a techno-deterministic understanding of its performance.
Studies on M-learning refer less to the functionality of appliances and the
development of mobile applications than address the subjective perception of
how this special type of educational venue leads to the acquisition of meaningful
and effective learning. This study focuses on Communication Studies in higher
education in Ecuador that deal with the paradox of studying the media through
the media, which results in studying theories and practice of communication by
using these same tools. This meta-communication dilemma becomes conﬂicting
when confronted to the still current face-to-face jotting and listening educational
model that keeps being backed by strong numbers of faculty members. By
unfolding the varied affordances of M-learning, the outcomes of this study
suggest that blending formal education with informal learning could lead to
encouraging, accessible and cooperative meaning-making on the move.
Keywords: Mobile learning  M-learning  Communication studies
ICTs
1
Introduction
Communication studies date to the Hellenic period where the expressive sense of
communication was put over senses of transference, connection or transmission. Due to
the advent of the media, at the beginning of the XX century other senses of commu-
nication were spotlighted, i.e. diffusion and subsequently the critical school. New
senses of communication emerged, particularly those of exchange, social interaction,
hybridization and convergence, which uphold the current discussion of communication
as a ﬁeld of knowledge.
Peters (2000) elaborates on four different senses of the meaning of communication:
imparting, transmitting, exchanging and symbolic interaction. Imparting is synchronic;
it refers to a one-way process that puts an issue in common, makes it public, shares,
reveals and broadcasts. Transfer or transmission describes the two-way communication
process of sharing and receiving, from one to another or from one to many, it supposes
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_96

a dual presence of sender and receiver which lies at the core of information and mass
communication theory. Paradoxically, instances of communication between a person
and an artiﬁcial agent, or between two or more artiﬁcial agents, can also refer to
transmitting and, thereafter, communicating.
Exchange supposes “a meeting of minds, psychosemantic sharing, even fusion of
consciousness” (Peters 2000:8). In this sense, communication refers to the possibility
of transferring and acquiring meaning, encouraging discussion and dialogue. Finally,
symbolic interaction involves the “mechanism through which human relations develop
all the symbols of the mind, together with the means of conveying them through space
and preserving them in time” (Cooley cited by Peters 2000:9). It suggests the creation
of meaning through interaction; the making of something shared and conveyed, a
creation in consensus.
In the same vein, Figueroa (2013) divides into four dimensions the conceptual
study of communication along time: (a) expression, referring to the linguistic turn in
social studies; (b) diffusion, involving a technological focus on mass media; (c) inter-
action, with sheer emphasis on the dialogic nature of symbolic construction, and
(d) structuration, with particular stress of systemic and networked approaches com-
bining the three previous turns of communication. While the ﬁrst dimension puts
emphasis in rhetoric and in how messages are constructed but not necessarily mediated,
all three other senses are analyzed with a sheer focus on mediation, where commu-
nication is built sometimes in face-to-face encounters but often through the use of the
media whether it is mass media or web-mediated digital communication.
The different senses of communication have also determined different ways in
which Communication Studies have been approached in higher education. In Ecuador,
Communication Studies started in the 40s through the School of Journalism, launched
as part of the College of Philosophy at the national university (FACSO 2006). At that
time, the philosophical, rhetoric and expressive sense of communication prevailed. On
1963 in Quito, the School of Journalism turned into the Information Science College
established at the same time when the deeper discussions on mass media emerged
particularly in the Latin-American region (Martin-Barbero 2014). By 1985 this college
was renamed to the current College of Social Communication which addressed new
approaches and challenges. Soon after, public and private universities all around the
country opened their doors to Communication students, to the extent that as for today
the national higher education system (SNNA)1 accounts for more than 30 communi-
cation programs around the country, six universities that offer communication degrees
in the country are also members of FELAFACS2, the Latin-American Federation of
Social Communication Colleges.
In spite of this, the Spanish appeal to “social communication” can be considered a
factual pleonasm, since the XXI century imaginary “expels” all previous senses of the
word communication by establishing a hegemonic uncontested sense that involves the
dialogic symbolic exchange of meaning-making in which society is ubiquitous.
Moreover another pervasive component of communication is the media, as stated in
1 Sistema nacional de nivelación y admisiones (SNNA – Senescyt) http://www.snna.gob.ec/.
2 FELAFACS. Ecuador http://felafacs.org/secciones/ecuador/ Retrieved 09.30.2017.
Mobile Learning: Challenging the Current Educational Model
1015

Jesús Martin Barbero’s seminal work “From the Media to Mediations” (2003) the
media, through its different tools and gears, is a fundamental constituent of current
communication studies.
However, when reﬂecting on learning scenarios often the media is not perceived in
this same way. In learning spaces it is common to refer to communication and infor-
mation technologies (ICTs) and not to the media, neglecting somehow the mediation
feature that frames these technologies in their direct relation to education. Media
technologies, especially the so-called “new technologies” (internet-enabled computers
and mobile communication devices), have gradually removed the notion of “mass”
from media communication studies (Jarvis 2011). In communication studies this
determines an endless paradox: to study the media through the media, or which is the
same: to study the media through ICTs.
2
The Field of Mobile Learning
When analyzing communication, Peters (2000) implies that in its “deeper sense of
establishing ways to share one’s hours meaningfully with others, is sooner a matter of
faith and risk than of technique and method” (p. 30). This suggests that technological
appliances are nothing but media to achieve higher objectives of interpersonal
communication.
M-learning, a recent ﬁeld in communication studies could stem from computer-
mediated communication (CMC). This ﬁeld’s enhanced research and writing started
from the 1990s (Thurlow et al. 2004). CMC studies illustrate the social imaginary of
the 90s. At that time, the use of computers was not as extended as today. To this regard,
Shirky (2008) states “it is when a technology becomes normal, then ubiquitous, and
ﬁnally so pervasive as to be invisible, that the really profound changes happen” (:105).
Currently, computers and the media are bound together with the glue of the internet and
the world wide web (WWW), which is the virtual network of websites connected by
hyperlinks.
Nevertheless, there are types of CMC that do not necessarily take place online, i.e.
on the WWW, for example learning activities using speciﬁc computer software,
physical meetings that utilize computers or interactive whiteboards, and photography,
video and music sharing through tablets or other mobile devices. Conversely, mobile
learning is only possible through the internet and the WWW; hence, it is not only a
two-ways communication process involving the action and reaction of two participants.
It also opens the landscape to multiple participants interacting digitally and mediated
by the internet.
M-learning moved from a type of electronic learning (E-learning) to establish as a
ﬁeld of its own. E-learning refers to “ICT-mediated teaching and learning activities”
(Dohn et al. 2015: p. 299) which could take place in face-to-face lessons, in-between
classrooms or in full online courses. Nonetheless, E-learning evokes the technological
features of wired education while M-learning goes beyond.
As suggested by Mohammed Ally and Josef Prieto-Blásquez (2014) “mobile
learning is not about the technology, it is about the learner”. It is possible to suggest
that the technological appeal overwhelms and leads to focus on the way meanings arise
1016
V. Yépez-Reyes

and happen (the technological apparatus). However, technology seems to be of less
importance than staring on the deeper content of what happens (learning acquisition
and performance). As a result, M-learning needs to surpass techno-deterministic
stances to take advantage of the tools and gears for fostering meaningful learning.
M-learning is being considered “the newest technology to achieve optimum learning
advantages” where participants in the learning process welcome the affordances of the
media to make “mobile learning not only possible but practical” (Almaiah et al. 2016:
p. 1314)
M-learning deals with students “on the move”. They establish innovative and
unaccustomed learning spaces and meanings through interaction with other people,
places and spaces. Therefore, understanding M-learning challenges our social imagi-
nary of traditional learning settings in classrooms.
Ally argues that the entrenched problem deals with the current educational model:
The current educational model is outdated because it was developed before the advent of
information and communication technologies. The current model, based on classroom-based
face-to-face delivery, is geared towards educating a certain segment of the population. Also,
teachers are being trained for the current model of education, and will therefore continue using
the model when they become teachers. Teacher training must be re-invented to prepare teachers
for the technology-enhanced educational system (Ally and Prieto-Blásquez 2014: p. 145).
Once again the media paradox arises. It is not random to listen that the use of
mobiles in classrooms is discouraged furthering the ofﬂine social imaginary of learning,
where cell phones are regarded as “bothersome distractions to the learning process”
(Prensky 2012: p. 180).
Furthermore, as suggested by Perales et al. (2015: p. 182) “the classic approach to
teaching has revealed itself a major obstacle for today’s students characterized by being
users of advanced technology in their daily life”. This is why the face-to-face
listening-and-jotting model of Communication Studies in higher education classrooms
needs to be revisited by putting into work precisely the instruments of its studies that
are both handy and useful to encourage technology-enhanced meaningful learning: an
engaging and creative venture.
2.1
Why M-learning?
According to the National Statistics Institute of Ecuador (INEC) at the end of 2016,
73.6% of the total population between 16 and 24 years old owned a smartphone3, data
increases on urban centers. This age range could correspond to students attending
bachelor programs in any of the various universities of the country. Students in
undergraduate programs start in average at the age of 18 and continue their studies over
4 to 5 years. Mobiles turn to be compulsory companions for them.
Mobiles or smartphones are small pocket-sized high-speed computers that are
usually brought into classrooms by the great majority of students. Mobiles beneﬁt from
3 INEC (2016). Tecnologías de Comunicación e Información-TIC. http://www.ecuadorencifras.gob.
ec/tecnologias-de-la-informacion-y-comunicacion-tic/ Retrieved 30.09.2016.
Mobile Learning: Challenging the Current Educational Model
1017

high-speed technology progress which results into enhanced mobile supported learning
environments (Ferdousi and Bari 2015).
M-learning supports the “anytime and anywhere access” maxim, determining a
sense of freedom both of time and geographical location. This leads both to reachability
and mobility. Nevertheless, M-learning is not constrained to mobile telephones but
refers to any type of portable device that can access the internet, i.e. laptops, tablets,
IPads, e-readers or smartphones.
However, from all sort of mobile devices, smartphones proﬁt particularly from the
familiarity of use. They “do not require technological training, do not intimidate users,
and remain unobtrusive in classrooms” (Cavus and Uzunboylu 2009: p. 434). It has
been observed that during face-to-face lessons, students often keep their mobiles out of
sight unless prompted to use them for academic purposes. However, without any
instruction, built-in cameras are frequently used in class to take snapshots instead of
jotting or drawing whiteboard annotations, pictures or presentations; yet the practice of
recording lectures has been less frequent.
2.2
Affordances of M-learning
In technological enhanced learning spaces, mobile devices turn into mediation tools. As
such, M-learning could stand both for “media learning” or “mobile learning” as the two
meanings share a wide range of affordances for effective and meaningful learning.
M-learning could suggest three main affordances: (1) its potential for promoting,
facilitating and enhancing student collaboration and peer interaction (Cavus and
Uzunboylu 2009); (2) information seeking and information sharing (Mills et al. 2014)
and (3) a “space of ﬂows” and “timeless time” that characterize the “network society”
(Castells 2013)
Collaboration and interaction
M-learning promotes cooperation as it provides functions of virtual collaboration
linked to the extended use of social media in informal settings. It also facilitates
teamwork and virtual collaboration by sharing resources and “learning objects” (Dohn
et al. 2015). Learning objects are regarded as individual pieces of digital learning
content that could be used again and again, such as slide-presentations, videos or
asynchronous communication.
Locher (2014) reviews a number of studies dealing with electronic discourses and
suggests that, despite the way the internet affords multi-modal discourses (including
video, images, virtual world, sounds, etc.) ‘written language is still the primary means
in which communication is achieved’(:556). Collaboration and interaction take the
form of different types of content digitally shared in the continuum of posting and
sharing that sustains digital interaction. Posts may trigger comment threads, building
discourses, narratives, conversations and stories that may also split into other dis-
courses, narratives, conversations and stories of the same or other participants some-
how connected to at least one participant in digital interaction.
Information seeking and sharing
M-learning affords bringing into the classroom knowledge or information from the
physical world (Ferdousi and Bari 2015), real-time in the moment information. Through
1018
V. Yépez-Reyes

this, students are not isolated from the world during learning periods but stay tuned and
are able to perform virtual and active presence through posting and sharing content.
As more and more mobile applications are developed, many virtual processes that
turn to be cumbersome on computers may be enhanced in mobiles through small apps
that produce quick results without much effort on a wide variety of topics. For example,
collaborative geo-referencing tools for enhancing situated projects such as the recently
launched Urbamapp4 in Ecuador. This app provides a wide range of signiﬁcant
information both on desktop and on mobile platforms. Nevertheless, the possibility to
introduce georeferenced pictures and information is limited to mobiles that are able to
provide meta-information on precise location coordinates and in that way, the use of
mobiles harness collective intelligence which is one of the main features of the Web 2.0
(O’Reilly 2007).
Information seeking and sharing takes place through mobiles in the internet, these
activities illustrate practices enhanced by the Web 2.0, such as collaboration,
‘bottom-up’ participation, interactive multi-way communication, the continuous pro-
duction, reproduction and transformation of material across contexts, openness of
content, and open-endedness of activity (Dohn 2009).
Anywhere and anytime
What Castells (2013) terms the “space of ﬂows” refers to the way ICTs allow things to
happen simultaneously without contiguity, this is that they take place (therefore they
are not placeless) but can be experienced at the same time in different and multiple
locations. It refers as well to asynchronous interaction, which happens both in time and
at a distance through connected networks and messaging services. It is also considered
a hybrid type of communication, both individualized and connected at once.
In the use of ICTs, time can be “compressed” through multi-tasking possibilities –
an ever presence of digital interaction, regardless of chronological timing– or by
blurring sequences of social practices (past, present and future), showing them in
random order that results in an emerging, alternative time “made of a hybrid between
the now and the long now”(Castells 2012:223).
This blurring of space and time in M-learning addresses for the most collaborative
activities outside the classroom which are unlimited by geographical and timing con-
strains, but are for the most infrequent in traditional classrooms (Cavus and Uzunboylu
2009).
3
Mobility, Accessibility and Reachability:
Moving from Formal to Informal Worlds
M-learning literature highlights the pros of embracing a shift in traditional learning
environments that could include effective in-class designs which allow students to
move seamlessly from formal to informal learning settings. This capability to comprise
both learning worlds is what makes M-learning so appealing for effective and mean-
ingful educational outcomes.
4 UrbaMapp. https://www.urbamapp.com/ Retrieved 01.10.2017.
Mobile Learning: Challenging the Current Educational Model
1019

Griff Foley et al. (2007: p. viii) state that ‘human life has a learning dimension that
is just as important as its economic or political dimensions’. But this learning
dimension is not constrained to formal educational settings but also, and for the most to
informal, self-directed, incidental and tacit forms of learning (Duguid et al. 2013).
While formal education supposes regular learning sessions organised by professional
educators, informal self-directed learning occurs when students consciously and vol-
untarily try to learn from their experience through individual or group reﬂections and
discussions. Here is where M-learning arises focusing on mobility and reachability of
peers to reﬂect and make sense of events, ideas and meanings.
On the other hand, incidental learning occurs while performing other activities, it is
conscious but involuntary and can arise from different types of interaction which for
example could arise from accessibility to information, events or learning situations.
Finally, tacit learning is often not perceived as learning because it happens sponta-
neously. It is a way of learning by experience and is both unconscious and involuntary.
4
Discussion
The affordances of M-learning for effective meaningful education are not constrained to
a speciﬁc discipline, its beneﬁts could reach all ﬁelds of knowledge. In that way the
meta-communication paradox of studying the media through the media for commu-
nication students turns to be just a rhetoric constrict. However, a pragmatic constrain to
M-learning in Communication Studies could raise from the paradox envisaged by
Robin Mansell (2012) in the imaginary of learners’ self-realisation through mastering
the media or ICTs.
The characteristics of the Web 2.0 suggest that through the social capabilities of
ICTs, students “are becoming information creators, information sharers, and infor-
mation consumers as they exploit the vast array of tools to personalize their environ-
ments and to construct online identities” (Mansell 2012: p. 118). Mansell stresses that
there is a paradox in this understanding since this sense of the mastery is only possible
through tools that are continuously created by others, where students remain at the
stage of users and are never actual producers.
On the contrary, the need to move towards an educational model that brings ICTs
into the classroom, take instructional content in-between classrooms and feedback
education environments from outside classrooms require engagement of both educators
and students who need to keep moving from one role to another by sharing and
building relentlessly a participatory communication and learning process where the
peculiar and charming gizmos of M-learning could be stressed in informal as well as
formal edifying spaces.
References
Ally, M., Prieto-Blásquez, J.: What is the future of mobile learning in education? RUSC: Univ.
Knowl. Soc. J. 11, 142–151 (2014). https://doi.org/10.7238/rusc.v11i1.2033
1020
V. Yépez-Reyes

Almaiah, M.A., Jalil, M.M.A., Man, M.: Empirical investigation to explore factors that achieve
high quality of mobile learning system based on students’ perspectives. Eng. Sci. Technol.
Int. J. 19(3), 1314–1320 (2016). https://doi.org/10.1016/j.jestch.2016.03.004
Castells, M.: Networks of Outrage and Hope: Social Movements in the Internet Age. Polity
Press, Cambridge (2012)
Castells, M.: Communication Power. Oxford University Press, Oxford (2013)
Cavus, N., Uzunboylu, H.: Improving critical thinking skills in mobile learning. Procedia Soc.
Behav. Sci. 1(1), 434–438 (2009). https://doi.org/10.1016/j.sbspro.2009.01.078
Dohn, N.B.: Web 2.0: inherent tensions and evident challenges for education. Int. J. Comput.
Support. Collab. Learn. 4(3), 343–363 (2009). https://doi.org/10.1007/s11412-009-9066-8
Dohn, N.B., Thorsen, M., Larsen, S.: E-learning. In: Dolin, J., Ingerslev, G.H., Jørgensen, P.S.,
Rienecker, L. (eds.) University Teaching and Learning, pp. 299–326. Samfundslitteratur,
Copenhaguen (2015)
Duguid, F., Mündel, K., Schugurensky, D.: Volunteer Work, Informal Learning and Social
Action. Sense Publishers, Rotterdam/Boston/Taipei (2013)
FACSO: Plan Director de Carrera. F. d. C. Social, p. 20. Universidad Central del Ecuador, Quito
(2006)
Ferdousi, B., Bari, J.: Infusing mobile technology into undergraduate courses for effective
learning. Procedia Soc. Behav. Sci. 176(Supplement C), 307–311 (2015). https://doi.org/10.
1016/j.sbspro.2015.01.476
Figueroa, R.: Introducción a las Teorías de la Comunicación. Pearson, Mexico (2013)
Foley, G., Gonczi, A., Stromquist, N.P., et al.: Dimensions of Adult Learning: Adult Education
and Training in a Global Era. McGraw-Hill International (UK) Ltd., Maidenhead (2007)
Jarvis, J.: Public parts: how sharing in the digital age improves the way we work and live. Simon
& Schuster, New York (2011)
Locher, M.A.: Electronic discourse. In: Schneider, K.P., Barron, A. (eds.) Pragmatics of
Discourse, pp. 555–582. De Gruyter Mouton, Berlin/Boston (2014)
Mansell, R.: Imagining the Internet: Communication, Innovation, and Governance. Oxford
University Press, Oxford (2012)
Martin-Barbero, J.: Thinking communication in Latin America. In: Christians, C., Nordenstreng, K.
(eds.) Communication Theories in a Multicultural World, pp. 46–59. Peter Lang, New York
(2014)
Martín-Barbero, J.: De los medios a las mediaciones. Convenio Andrés Bello, Bogota (2003)
Mills, L.A., Knezek, G., Khaddage, F.: Information seeking, information sharing, and going
mobile: three bridges to informal learning. Comput. Hum. Behav. 32(Supplement C), 324–
334 (2014). https://doi.org/10.1016/j.chb.2013.08.008
O’Reilly, T.: What Is Web 2.0: design patterns and business models for the next generation of
software. Int. J. Digit. Econ. 65, 17–37 (2007)
Perales, M., Barrero, F., Arahal, M.R., Toral, S.: Problem based learning case in a control
undergraduate subject. IFAC-PapersOnLine 48(29), 182–187 (2015). https://doi.org/10.1016/
j.ifacol.2015.11.234
Peters, J.D.: Speaking into the Air: A History of the Idea of Communication. University of
Chicago Press, Chicago (2000)
Prensky, M.: From digital natives to digital wisdom: hopeful essays for 21st century learning.
SAGE Publications Ltd., Thousand Oaks (2012). https://doi.org/10.4135/9781483387765
Shirky, C.: Here Comes Everybody: The Power of Organizing Without Organizations. Penguin
Books, New York (2008)
Thurlow, C., Lengel, L., Tomic, A.: Computer Mediated Communication: Social Interaction and
the Internet. Sage, London (2004)
Mobile Learning: Challenging the Current Educational Model
1021

Competencies and Indicators for a Productive
Digital Communication
Laura Guerra1(&), Stalin Arciniegas1, Luis David Narváez1,
and Francisca Grimon2
1 Pontiﬁcal Catholic University of Ecuador, Headquarter Ibarra, Ibarra, Ecuador
{lrguerra,smarciniegas,ldnarvaez}@pucesi.edu.ec
2 University of Carabobo, Barbula, Venezuela
grimon.francisca@gmail.com
Abstract. The research aims to describe the digital capacities and the type of
communication of university students, by applying virtual collaborative learning
strategies. It is a descriptive investigation, carried out in two phases, supported
by the theories of connectivism and social constructivism. The case study
method was applied to a population of 41 students. Among the obtained results
it is highlighted that the studied population has the digital capacities necessary to
participate in collaborative environments related to mathematics, but that are not
enough to enrich the social capital of the knowledge of the learning community.
As an attitudinal indicator, the students are considered willing to work in col-
laborative groups using information and communication technologies but
require more training to build knowledge. Finally, it is recommended that the
collaborative activities are guided by the teacher so that they are more fruitful
and taken advantage by the students.
Keywords: Digital abilities  Digital communication  Collaborative learning
Moodle
1
Introduction
Technology is advancing every day more in the ﬁeld of communication and infor-
mation, it is part of daily life; services are paid virtually, work is done from different
places, health diagnoses are performed using virtual images, among others. All these
technological innovations are also manifested in education, so it is necessary to
establish educational actions that allow educators and learners to adapt with these
advances. Salinas [1], says that learning is throughout life and work, it is continuous
learning, ubiquitous and based on social contact, without differences between living,
working and learning, facilitated in large part by advances in the system technology
connection for personal use. Researchers such as [2], prospectively studied the
dynamics of formation and the methodologies to be used and they determined that the
trends in formation strategies are the personalization, collaborative learning, autono-
mous learning and learning throughout life, which require learning methodologies
based on experience and inquiry, compatible with mobile technology and social
networks.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_97

The study of the effectiveness of collaborative learning has been the work of several
authors. Among these works, the researches of [3] can be commented, they analyzed
the activities that the students performed in a virtual collaborative environment and
their relationship with performance. To do this, the authors took each message shared
by the students, segmented it and classiﬁed it according to an established activity
system: Task information discussion, task regulatory activities, regulatory activities
Social and social activities. Meanwhile, [4] presented a scheme for the logical analysis
of medical discussions based on arguments; an argument scheme is a premise, con-
clusion, set of critical questions and all of them involve variables that according to their
values will support the given argument. All the information obtained from these col-
laborative activities served to make decisions about diagnoses, possible treatments,
warnings of risks, additional explorations, among others.
2
Research Problem
The term of digital competences, distinguishes the knowledge, expertise, abilities that a
person has over communication and information technology, that is, how to search,
ﬁlter, share information that can be data ﬁles, images, video and web pages. Several
authors and organizations consider important that the education should encourage the
development of these transversal skills, studying their implications with the social and
affective aspect, [5–7]. In today’s society, it is a primordial necessity to have the
minimum digital competences to deal with the information age, which some authors
deﬁne as a human right [6].
There is no point in continuing to develop learning environments if the students are
not able to use them. This research explored how the students behave in a virtual
learning community to solve the task and how they perceive themselves in regard to the
digital capabilities required for it, providing a learning collaborative, ubiquitous and
social experience.
3
Theoretical Basis
The theories of learning that support the investigation are the theory of connectivism
and the theory of social constructivism. The theory of connectivism: according to its
founder Siemens in 2004 (named by [8]), explains the learning process in the digital
age, reporting that there are two domains that interact in this process, which are the
cognitive and the social domain. The importance of obtaining truthful and timely
information on a topic implies that the individual has to ﬁlter the information available
and in this screening, peers play a very important role, since they are the ones who can
share knowledge and exchange data. The individual develops skills of critical thinking,
analysis, synthesis, evaluation to obtain relevant knowledge on a treated topic. The
theory of social constructivism aims to explain how students learn when they interact
with other people. Vygotsky postulated the term called the Zone of Near Development
(ZPD), to refer to the difference between current cognitive development and the
potential development of a human being. Their approaches indicate that if a student
Competencies and Indicators for a Productive Digital
1023

relates to a person who is more advantaged than he or she in a particular subject, the
gap in his/her ZPD may be shortened due to the support, guidance, or reinforcement
given by his or her peer to gain knowledge.
The Collaborative learning is a didactic strategy linked to the theory of social
constructivism, because it consists of a work plan where a group of learners meet
together, exchange knowledge, discuss points of view, analyze scenarios, select
strategies, in order to perform a task [3, 9]. In this process, the students have the
opportunity to develop a range of skills such as leadership, negotiation, conﬂict
management, decision making, critical thinking, among others that will enrich their
professional and civic formation. The use of collaborative learning is not restricted to
classrooms, but rather as expressed [10], it is a tool that is used in several domains such
as science and language, among others. In that sense, Jeff [11] refers to the application
of collaborative learning in areas of health, which improves the knowledge of clinical
cases by learning from the failures and successes of colleagues and [12] report its use in
a community of teachers who design their virtual teaching environments.
4
Methodology
This research intends to describe the digital competences and the kind of communi-
cation of the students when they carry out collaborative activities through Internet. It is
a descriptive investigation, where the case study method was applied, to study the
problem in a mathematics course of systems engineering. The study population was 41
students, consisting of all the students who attended the subject at that time and a
virtual course built where Moodle was used. The development of the research com-
prises two phases:
First phase: at the beginning of the course an instrument was applied to obtain
information about the digital competences that the students had for that moment. This
instrument was based on items proposed by [13], was validated by two experts and
obtained a reliability coefﬁcient of 0.87. A Likert scale was used for the evaluation of
the answers. Twenty-two questions constitute the questionnaire, which are distributed
in four dimensions of analysis: basic computer skills; use of the web- basic tools;
attitude towards the use of information and communication technologies in education;
attitude towards collaborative work.
Second phase: virtual collaborative learning strategies were developed, integrated
to the selected theoretical bases, using the Moodle platform. The didactic technique of
problem-based learning was used, implementing it in an asynchronous group forum
and in the development of a personal group learning environment (wiki).
The analysis of the results of the forum was carried out by means of the adaptation
of codiﬁcation proposed by [3]. The coding was performed by two researchers who
worked independently, obtaining a Kappa coefﬁcient of 0.8. To classify the student
interventions, each message was separated into smaller units, taking into account
punctuation, exclamation, and question marks. Then these segmented messages were
coded as follows:
Activities related to the accomplishment of the task (TA). They are subdivided into
the following classes:
1024
L. Guerra et al.

ATD: discussions of relevant theoretical topics of the evaluated subject, which
contribute to solve the task.
ATC: when students share their knowledge, related to the subject of the task, in
order to achieve the objective of it.
ATP: questions or queries related to the task.
Regulation and coordination of the activities of accomplishment of the task (RT):
RTM: are the metacognitive activities dedicated to planning and monitoring the
progress of the task. They include discussions about strategies for carrying out the
task and delegating responsibilities.
RTP: positive assessment of task progress.
RTN: negative assessment of task progress.
Social activities (AS): are the social interactions that contribute to achieve a good
group climate, reafﬁrm the conﬁdence among group members, but do not add value to
the resolution of the task itself. They are classiﬁed in:
ASS: greeting of arrival or farewell
ASA: agreement on the contribution of the partner
ASD: disagreement in the contribution of the partner
ASO: other social commentaries
Regulation and control of social activities (RS)
RSA: offering or requesting help from peers
RSD: discuss collaborative strategy
RSP: positive evaluation of the group process
RSN: negative evaluation of the group process
Comments out of the task (CT)
CTR: Valid comments on the task but repeated
CTI: comments about the task that are theoretically incorrect
CTF: comments on other topics outside the subject of the task
During the strategy implementation phase, the concept of wiki in Moodle was used
so that the students formed in groups, established their own learning environments on a
theme of the subject, where it was valid to incorporate everything that they considered
necessary under any computer scheme. This was to determine which are the digital
tools most used by students to communicate an idea.
5
Data Analysis and Results
The results are presented according to the research phase:
Competencies and Indicators for a Productive Digital
1025

5.1
Phase 1
The digital capacities of the studied population were analyzed through the information
provided by the students surveyed, through an instrument that was available to them in
the Virtual Classroom, from which 70% of the answers were validated. The established
dimensions were:
Basic computer knowledge. All students considered themselves to be able to work
with the most popular Microsoft packages (Excel, Word, Power Point), only 54% of the
respondents indicate that they have worked with some mathematical software. On the
other hand, 79% of the students state that they know how to upload, download and
modify images (Fig. 1).
Use of the Web. Basic tools. Most of the respondents said that they know and use web
applications such as search engines (96%), cloud storage (71%) and electronic ﬁle
management (96%). One surprising result was that only half of the students confessed
to using email frequently (Fig. 2).
Formation in the use of ICT. Most students considered that they have been trained in
the use of information and communication technologies, either before entering the
institution (63%) or when entering it (75%). It is noteworthy that 54% of students
reported having worked with forums, chats, wikis, prior to this investigation (Fig. 3).
Attitude towards the use of ICT in education. More than 70% of the students
perceive that the ICT in education favors the creativity and motivation of the pupils
with the subject, since there is the possibility to manage their own learning in the place
and time that they wish, strengthening their professional training. In addition, they
admit that they revised the virtual classroom of the University weekly, because these
are educational strategies used by teachers with the support of the authorities of the
Institution (Fig. 4).
100
79
54
0
20
40
60
80
100
120
Microsoft
packages
Image
processing
Mathematics
Software
Fig. 1. Perception of students regarding their basic knowledge in computer (%)
1026
L. Guerra et al.

Attitude towards virtual collaborative work. Most of the respondents afﬁrm that
they have the capacity to participate in virtual collaborative activities. They also
consider these activities a way to clarify doubts and reinforce what has been learned,
maintaining interest in the subject and with the opportunity of each group, to stand out
among others, strengthening bonds of friendship for the achievement of a common
goal. Likewise, it was determined that 42% of respondents do not feel comfortable
writing the mathematical language digitally (Fig. 5).
96
96
71
50
0
20
40
60
80
100
120
Electronic files
Explorers
Google drive or
Dropbox
email
Fig. 2. Students’ perceptions about the use of web tools (%)
75
63
54
0
10
20
30
40
50
60
70
80
Upon entering the
University
Before entering the
University
Participation in
virtual group
activities
Fig. 3. Students’ perception about the moment of their training in ICT (%).
Competencies and Indicators for a Productive Digital
1027

5.2
Phase 2
The strategies used were based on the use of virtual environments for the development
of group activities. In this paper two activities of this kind are reported.
Virtual Forum Analysis. The messages written by the participants coded by the
researchers, according to the proposed classiﬁcation, are shown in Table 1. Note that
179 messages were detected in total. The students dedicated more messages (50.8%) to
share their knowledge of the task, but without establishing a signiﬁcant amount of
discussions or questions on the topic discussed. A sample of the little cognitive conﬂict
presented in the forum was the number of messages on key aspects to carry out the
activity, which had already been proposed by another partner in the group or were not
71
79
83
88
88
96
0
20
40
60
80
100
120
Motivation
Support from the university
Provide opportunity
Creativity development
Professional formation
Weekly frequency
Fig. 4. Perceptions of students about their attitudes toward ICT (%)
58
67
75
79
88
88
0
20
40
60
80
100
Mastery of mathematical language
Willingness to participate
Motivation of the group
Reinforcing learning
Leadership
Friendship and bonding
Fig. 5. Students’ perceptions about their attitudes towards collaborative work (%)
1028
L. Guerra et al.

Table 1. Encoded Forum Messages
Code
Messages
Nº
%
Activities related to the accomplishment of the task (TA)
ATC
71
39,7
ATP
9
5
ATD
11
6,1
Total (TA)
91
50,8
Regulation and coordination of the activities of
accomplishment of the task (RT)
RTP
2
1,1
RTM
9
5
Total (RT)
11
6,1
Social activities (AS)
ASS
13
7,3
ASA
14
7,8
ASD
4
2,2
ASO
3
1,7
Total (AS)
34
19,0
Regulation and control of social activities (RS)
RSA
3
1,7
Total (RS)
3
1,7
Comments out of the task (CT)
CTR
4
2,2
CTI
35
19,6
CTF
1
0,6
Total (CT)
40
22,4
Total encoded messages
179
100
Fig. 6. Classiﬁcation of students’ messages in the forum (%).
Competencies and Indicators for a Productive Digital
1029

completely correct (22.4%). Also, it can be seen that the social activities or interrela-
tionships (19%) are located in the third place. On the other hand, the messages linked to
the regulation of activities of the task or related to social relations, were not of much
interest to the students (Fig. 6).
Wiki Analysis. It was observed that out of 15 established groups, only fourteen
worked, demonstrating skills for the competencies which can be visualized in Fig. 7.
The participations showed that the work with images and texts was the most used by
the respondents, for the development of their personal learning environment.
6
Conclusions and Recommendations
Most of the students surveyed perceive themselves with digital abilities corresponding
to Microsoft packet handling, web browsers use, text and image ﬁle processing, willing
to work collaboratively with their peers and considering the very important ICT in the
development of their profession. The weaknesses that they admitted to possess, and
that were demonstrated in the activities, are not knowing the operation of some soft-
ware of mathematics, or writing mathematical expressions in digital format. This fact
can affect the ﬂow of communication between collaborators when presenting the
barriers of mathematical language, in an unfavorable cognitive development, due to not
visualizing the studied concepts and simulated in the respective software. The behavior
of the participants in the discussion forum reﬂects that they require more training to
build knowledge in a collaborative way, since the messages were focused on
expressing their knowledge about a topic of the subject, but without discussing them,
for this reason, the study had little depth. Most of the participations were not counted as
valid because they were exposed by students who were not prepared for the activity, a
fact that was demonstrated in incorrect comments on the subject treated.
57
50
50
14
0
10
20
30
40
50
60
Insert images
Upload videos of
other owners
Copy or write
texts
Place links to
web pages
Fig. 7. Digital communication tools used on the wiki by groups (%)
1030
L. Guerra et al.

In accordance with the previous paragraph, only one group used a computer
package to support its approaches in its personalized learning environment (wiki). Most
of the groups were dedicated to upload ﬁles (text or images) and/or videos without
communicating an own analysis.
Finally, it can be concluded that the studied population has the digital abilities
necessary to participate in collaborative environments related to mathematics, but these
are not enough to enrich the social capital of knowledge of the learning community.
From the point of view of attitude, it is considered that the members of this population
are consciously willing to work in collaborative groups using information and com-
munication technologies. However, when the collaborative environment does not
demonstrate aggregated values of knowledge, students may withdraw from participa-
tion, considering that the reason sharing beneﬁt - sharing cost, is vulnerable. From
there, it is recommended to carry out the collaborative activities with teacher inter-
ventions, to guide, promote and act as scaffolding, so that the students’ participations
really build knowledge in depth and, if possible, generate new knowledge.
References
1. Salinas, J.: La investigación ante los desafíos de los escenarios de aprendizaje futuros. Rev.
Educ. Distancia 1, 1–24 (2016)
2. Gros, B., Noguera, I.: Mirando el futuro: evolución de las tendencias tecnopedagógicas en
Educación Superior. Campus Virtuales, Rev. Cient. Tecnol. Educ. II, 130–140 (2013)
3. Janssen, J., Erkens, G., Kirschner, P., Kanselaar, G.: Task-related and social regulation
during online collaborative learning. Metacogn. Learn. 7, 25–43 (2012)
4. Qassas, M., Fogli, D., Giacomin, M., Guida, G.: Analysis of clinical discussions based on
argumentation schemes. Procedia Comput. Sci. 64, 282–289 (2015)
5. Ilomäki, L., Paavola, S., Lakkala, M., Kantosalo, A.: Digital competence – an emergent
boundary concept for policy and educational research. Educ. Inf. Technol. 21, 655–679
(2016)
6. Mengual, A., Roig, R., Blasco, J.: Delphi study for the design and validation of a
questionnaire about digital competences in higher education. Int. J. Educ. Technol. High.
Educ. 13, 1–11 (2016)
7. Schleicher, A.: The case for 21st century learning. Organización Europea para la
Cooperación
y
el
Desarrollo
(O.E.C.D.)
(2011).
http://www.oecdobserver.org/news/
fullstory.php/aid/3403/The_case_for_21st_century_learning.html
8. Durairaj, K., Umar, I.: A proposed conceptual framework in measuring social interaction and
knowledge construction level in asynchronous forum among university students. Procedia
Soc. Behav. Sci. 176, 451–457 (2015)
9. Saab, N.: Team regulation, regulation of social activities or co-regulation: different labels for
effective regulation of learning in CSCL. Metacogn. Learn. 7, 1–6 (2012)
10. Lu, J., Lajoie, S., Wiseman, J.: Scaffolding problem-based learning with CSCL tools.
Comput. Support. Collab. Learn. 5, 283–298 (2010)
11. Jeffs, L., McShane, J., Flintoft, V., White, P., Indar, A., Maione, M., Lopez, A., Bookey, S.,
Scavuzzo, L.: Contextualizing learning to improve care using collaborative communities of
practices. BMC Health Serv. Res. 16, 1–8 (2016)
Competencies and Indicators for a Productive Digital
1031

12. McKenney,
S.,
Boschman,
F.,
Pieters,
J.,
Voogt,
J.:
Collaborative
design
of
technology-enhanced learning: what can we learn from teacher talk? TechTrends 60, 385–
391 (2016)
13. Agreda, M., Hinojo, M., Sola, J.: Diseño y validación de un instrumento para evaluar la
competencia digital de los docentes en la educación superior española. Pixel-Bit. Rev.
Medios Educ. 49, 39–56 (2016)
1032
L. Guerra et al.

Transparency and Participation in Public
Service Television Broadcasters: The American
Southern Cone
Paulo Carlos López-López1(&), Mónica López-Golán1,
and Iván Puentes-Rivera1,2
1 METACOM Research Group Investigación Medios,
Tecnologías Aplicadas y Comunicación, Pontiﬁcal Catholic University
of Ecuador, Ibarra campus, Quito, Ecuador
{pclopez,molopez}@pucesi.edu.ec
2 Grupo de CP2: Comunicación Persuasiva, University of Vigo, Vigo, Spain
ivanpuentes@uvigo.es
Abstract. Public service televisions must face a double challenge in the scope
of accountability. On the one hand, the need of providing the citizens with all
the data related to the institution’s daily activities. On the other hand, the
requirement of making the production stages transparent in order to prove the
degree of compliance and commitment to a quality service. This paper analyzes
the information available on the websites of the public service televisions in the
American Southern Cone, meaning Chile, Argentina, Uruguay and Paraguay.
When it comes to performance, the fulﬁlment of these indicators can be con-
cluded as unsatisfactory, even though transparency in audiovisual corporations
becomes one of the most appreciated intangible values that enable the television
public service to regain credibility.
Keywords: Transparency  Public service television  Accountability
America  Public information
1
Introduction
The public service televisions, immersed in an increasingly competitive communica-
tion system, should act as broadcasting tools that guarantee the effective defense of
civil rights linked to the right of access to public information, the plurality of content or
the promotion of native cultures and languages. For that purpose, these organizations
have increased the use of transparency practices that break with the traditional
impenetrability uphold for decades regarding their management.
1.1
Public Televisions, Transparency and Participation
Transparency, understood as “the public accessibility to information about the entity’s
management, so that the data are understandable, accessible and expressed in the
current language” [1], implies that the internal structures, regarding public service
televisions in this case, become more open and communicative with their audience.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_98

Internet and the new digital platforms like, for example, websites have become the
most suitable tools to meet the transparency legal standards [2]. Public service tele-
visions, considering the different regulations, must make it clear who they are and how
they work, especially towards society, which acts as their main ﬁnancing sources.
Likewise, but taking into account that transparency is a prerequisite, public service
televisions must encourage civic participation by means of speciﬁc mechanisms and
policies. Thus, it is only within a transparency environment that society is in a position
to participate. This participation can be encouraged by fostering a direct dialogue
through interactive media like the social networks, by channels for the reception of
suggestions and complaints, or by consultative bodies.
The “best practices” in management shorten the distance between the ideal prin-
ciples of public media and their actual situation [3]. Therefore, in order to improve that
actual situation and increase the transparency levels, which are far from the desirable, it
is important to create appropriate institutional conditions based on the transparent
management of ﬁnancing, decision-making processes – supported by consultations and
by the participation of relevant actors –, the usage of quality assessment mechanisms,
and the provision of information to the audience on the institution’s functioning and its
management results. The higher the degree of transparency, the better the quality of the
public service that the organization provides to the audience [1].
1.2
Accountability: Answerability and Enforcement
The accountability process is part of the much broader concept of social responsibility.
Both public and private organizations promote a detailed information of their daily
activities with the objective of proving that what they do: (a) complies with the law;
(b) meets the people’s needs; (c) boosts transparency and, in the case of those organi-
zations receiving public funds directly or indirectly, reduces the potential for corruption.
From a conceptual point of view [4], accountability is the sum of two key elements,
such as the so-called answerability, that is, the data publication and the explanation of
the public activity developed by a given organization; and enforcement, which are the
procedures and resources focused on the law enforcement [5]. Along with the devel-
opment of this practice throughout the years, both concepts experienced an enlarge-
ment process without substantially modifying their meaning. In this manner,
answerability, besides referring to the obligation of the public sector to report their
activities, it was also introduced in the private sector and focused on the creation of
effective channels using the new technologies to obtain feedback [2] from the citizens
or the interest groups. Moreover, enforcement, regarding the sanction applicable to
those who do not comply with the law when it comes to fulﬁlling their attributions, has
been reinforced by a signiﬁcant regulatory development in several countries. Fur-
thermore, it has also been enlarged because of the international recommendations that,
even though they are not included in the national legislations, foster the institutional
legitimacy that, in the case of television broadcasters, is one of the most appreciated
intangible values regarding quality of service [6].
When it comes to the operational level, “transparency ultimately constitutes a tool
enabling accountability by companies, institutions and the other social players” [7],
allowing the citizens to oversee the institutions, and allowing the public and private
1034
P. C. López-López et al.

authorities to apply the principle of publicity of proceedings. This element takes part in
the new civic requirements promoted by the development of the information and
communication technologies [8], being accountability in media companies a key ele-
ment for the creation of values and trust.
2
Methodology Organization
The subject of study of this research focuses on the information and participation tools
included in the websites of the public television broadcaster of Chile (TVN), Argentina
(TPA), Uruguay (TNU) and Paraguay (Paraguay TV). In this sense, the starting
hypothesis is that the compliance of these four television broadcasters is uneven, either
through their own or their corporations’ sites (www.tvn.cl/, www.tvpublica.com.ar/,
www.tnu.com.uy/, www.paraguaytv.gov.py/). The Chilean, Argentinian and Uru-
guayan television broadcasters are the ones with the highest level of transparency in
their management, and the ones with the most participation tools available to the
citizens.
2.1
Sample Justiﬁcation
When it comes to choosing the sample, we paid attention mainly to a geographic
criterion, the so-called American Southern Cone, which is the most Southern area of
the American continent, including Chile, Argentina, Uruguay and Paraguay. We also
paid attention to a legal criterion, since these four countries have in recent years
promoted laws encouraging transparency, accountability and participation. Such is the
case of Chile, with the law Nº 20.285 on access to public information in 2008; in
Paraguay the law Nº 5.282/2014, Libre Acceso Ciudadano a la Información Pública y
Transparencia Gubernamental (public access to public information and government
transparency); in Uruguay the law Ley de Acceso a la Información Pública del 2008
(access to public information in 2008), the latter extended and improved by several
decrees, as well as other more recent laws, such as the law Ley de Transparencia
Fiscal, del 2016 (ﬁscal transparency, of 2016) and, more recently the law Ley de
Acceso a la Información Pública (access to public information) in Argentina.
Nevertheless, if we consider the social, economic and political aspects, Paraguay
would be excluded from our subject of study. That way, in the Human Development
Index elaborated by the United Nations in 2016 [9], Chile ranks 38th, Argentina 45th
and Uruguay 77th (very high or high). On the other side, Paraguay ranks 110th (middle),
with per capita income of 13,342 $, 12,590$, 17,000$ and 4,134$ respectively,
according to data from the World Bank [10]. Other studies have been added to these
economic elements, like the Corruption Perception Index of 2016 [11], elaborated by
Transparency International using a methodology that takes into account several data
source related to the public sector. In this respect, Uruguay would rank 21st worldwide
(3rd in the American continent as a whole, only behind Canada and the United States).
Chile would rank 24th (4th in the continent), Argentina would rank 95th worldwide (18th
in the continent) and Paraguay would rank 123rd worldwide and ranking the lowest in
the continent. Nevertheless, it has been decided to work on the basis of previous
Transparency and Participation in Public Service
1035

research (as the carried out in the Community of Andean Nations), showing signiﬁcant
differences among the countries analyzed, as well as on the basis of a practical and
opportunity criterion, since Paraguay could not be included in any other political or
geographical area.
2.2
Objective and Research Structuring
The main objective of this paper is to review transparency and the participation
mechanisms enabled in the websites of TVN, TPA, TNU and Paraguay TV by means
of the application of a series of indicators. This research, which is based on the
analytical and comparative method, is carried out as a second review in the Chilean
television [12], and for the ﬁrst time in the Uruguayan, Argentinian and Paraguayan
televisions, fulﬁlling the basic precepts that justify the pertinence of the research: its
relevance, its practical implications and its theoretical value. The rhythm of the
research is as follows:
• Auditing the websites of the television broadcasters or of their corporations in
accordance with the indicators presented by López-López, López Golán and
Puentes-Rivera [13].
• Presenting the data and the results obtained in three big blocks.
• Quantifying the indicator fulﬁllment in a range from 0 to 100, being 0 the absence
of transparency and 100 the maximum degree. In accordance with this, the value of
1.724 is allotted to the fully met indicators, 0.862 to the partially met indicators and
0 to the non-met indicators.
2.3
Information Collection Tool
This research uses as information collection tools the 58 indicators developed and
applied as trial to the public television broadcasters of Spain and Portugal for a number
of reasons. Firstly, the tool enables a holistic and integral analysis of the economic and
institutional information, in addition to assessing the information production parameters
and the institutionalized participation of the audience or interest group. Secondly, the
tool helps establish fair comparisons among the different entities by means of three
different blocks. Thirdly, the tool shows severity in the ﬁeld of construction, for which
the UNESCO Recommendations have been reviewed [14], as well as the ones belonging
to the European Broadcasting Union for the media development [15] or to the param-
eters set by Laboratori de Periodisme i Comunicació per a la Ciutadania Plural of
Universitat Autònoma de Barcelona, applied in the case of the Infoparticipa map.
3
Results
The audit results regarding the institutional information included on the websites of the
four television broadcasters in the American Southern Cone are of various types,
though with a constant general unfulﬁllment of the applied indicators. For example, the
Chilean TVN provides comprehensive information on the composition, functions and
1036
P. C. López-López et al.

remuneration of the Management Board, but the procedure of appointment of its
members must be clearer and more explicit. It should be noted that there is a crime and
corruption prevention program, as well as a document called memoria (report), that
becomes an accountability tool, but there is also a lack of many other basic elements
(Table 1).
Table 1. Transparency indicators in the institutional, governance and group ﬁeld
Indicator
TVN TPA TNU PTV
Is there a section containing any information about the channel’s
history?
✓
√
√
✓
Is there any information provided about the legislation applicable to
this entity?
✓
√
P
X
Is there any information on the method of appointing members of
the Management Board or similar body, its structure and
remuneration?
✓
X
P
X
Are the minutes of the regular and extraordinary meetings of the
Management Board published?
X
X
X
X
Is there any declaration of assets, interests and activities of the
members of the Management Board published?
X
X
X
X
Is there any information on the cost of representation and
subsistence allowance of the Management Board?
✓
X
X
X
Are there any rules governing incompatibility (if there is any) and
independence of the Management Board?
P
X
X
X
Is there any information given about the leading manager of the
entity?
✓
√
X
X
Is there any institutional agenda established for this leading
manager?
X
X
X
X
Is the validity of the framework mandate and its applicable
legislation clariﬁed?
X
X
X
X
Is there any information on the method of appointing members of
the Advisory Board or similar body, its structure and remuneration?
X
X
X
X
Are the minutes of the regular and extraordinary meetings of the
Advisory Board or similar body published?
X
X
X
X
Is there any information on the cost of representation and
subsistence allowance of the Advisory Board?
X
X
X
X
Is audience participation reﬂected directly or indirectly in the
Advisory Board?
X
X
X
X
Is there any basic information provided about the organization chart
and are the functions clearly deﬁned?
P
√
P
X
Is there a clearly deﬁned and accessible directory?
X
√
X
P
Is there an accountability report published?
✓
X
X
X
(continued)
Transparency and Participation in Public Service
1037

The Argentinian public television broadcaster scores negative in many of the
analysis indicators within the institutional ﬁeld. The broadcaster’s objectives related to
the institutional activity are supposed to be described and explained in formal docu-
ments, but these are not published in its website. The same applies to the balance sheet
and the accountability report, which are two of the variables included in this paper and
whose unfulﬁllment is conﬁrmed.
The Uruguayan national television broadcaster ﬁnds itself in a similar situation,
whose website concentrates most of the information related to the institution in a
document stored in the “Institutional” section. Such document summarizes the activity
carried out by this broadcaster, as well as the programming strategies or the advertising
guidelines developed by the channel. However, it is about a piece of information that
barely meets the indicators listed in the transparency table regarding the institutional
ﬁeld, and when it meets them, a partial fulﬁllment is recognized by the fact of being a
document elaborated in 2013. Besides, the web of the Uruguayan television broad-
caster, aligned with our hypothesis, does not comply with the legal requirements,
having only a small access directory and a summary on the broadcaster’s history.
From an economic perspective, the Chilean public television broadcaster holds
comprehensive information and documentation on the budget and its execution,
allowing for having data at the ﬁrst level (audit, amount of budget, information on
technology). Nevertheless, the institution does not provide any kind of report at the
following level, either when using big events or providing an incomplete report (the
outside production is included in the general budgetary report, and it is not accessible
and not very clear). When it comes to the advertising information, it is valid due to the
fact of being perfectly broken down within the budget and including an own section in
Comercial (Advertising), just like the infrastructure basic ﬁelds, such as the job list or
the trade union representatives.
For its part, the Argentinian broadcaster does not either include any information on
the allotted budgets, subsidies, costs, business model, or reports on budget imple-
mentation. Thus, the unfulﬁllment of these variables is veriﬁed and conﬁrmed, pre-
venting the citizens from the access to relevant information on that public entity. On the
Table 1. (continued)
Indicator
TVN TPA TNU PTV
Is there a section called Corporate Social Responsibility, where
information is provided regarding the actions executed by the
entity?
X
X
X
X
Is the project called Contribution to Society or similar accessible for
the impact assessment of television?
X
X
X
X
Is there a section devoted to feedback and interest group reports?
X
X
X
X
Are the corporate or television bylaws published?
✓
X
X
X
Is there a code of good business practices, such as a corruption
prevention plan or a promotion of transparency and good
governance?
✓
X
X
X
Is there a clearly deﬁned section with the name “Transparency”?
✓
X
X
X
1038
P. C. López-López et al.

contrary, there is a high level of fulﬁllment in the indicators related to the recruitment of
external providers. The existence of a regulation explaining the recruitment procedure
is also conﬁrmed through the website. Likewise, the current processes are published, as
well as the open for bidding and the concluded ones.
The informative advertising associated with the management of the broadcaster’s
economic resources is much higher in number in the case of the Uruguayan public
television broadcaster. However, as with institutional data, it is about obsolete infor-
mation belonging to the 2013 exercise. Some of the most relevant data, contributing to
the protection of transparency regarding the management of the public broadcaster,
include the allocation of the annual budget to the entity, the percentage report of the
business model, or the broadcaster’s contribution to the national cinema (Table 2).
Table 2. Transparency indicators on economic and infrastructural information
Indicator
TVN TPA TNU PTV
Is there any information provided about the assigned amount to the
entity from the State budget?
✓
X
P
X
Is there any information given about subsidies or any other
contribution granted to the entity?
✓
X
P
X
Is a percentage report of the business model elaborated?
P
X
P
X
Are the annual accounts published together with their audit report? ✓
X
X
X
Is there a monthly report offered on budgetary implementation?
P
X
X
X
Is there any information about the outside production costs?
P
X
X
X
Is there any place available on the website for the contractor’s
proﬁle?
X
√
P
X
Is there a list of companies which are beneﬁciary of contracts?
X
√
X
X
Is there a speciﬁc section where the advertising regulation is
explained, as well as their broadcast average minutes?
✓
P
P
X
Is there any document providing information on the use of the sport
rights and their costs?
X
X
X
X
Is there any information available on the production costs and/or
purchase rights regarding big events?
X
X
X
X
Are there investment data offered in cinema and/or other cultural
industry?
X
X
P
X
Is the cost per head and household posed by the operation of this
public television broadcaster published?
X
X
X
X
Is the investment in technology, research, development and
innovation published in detail?
P
X
P
X
Is the inventory regarding resources, personal property or real estate
speciﬁed?
X
X
X
X
Is there any report on work positions including the remuneration
based on categories?
✓
X
X
X
(continued)
Transparency and Participation in Public Service
1039

With regard to the information production and content access, the Chilean televi-
sion broadcaster fulﬁlls the requirements by publishing its editorial line and
self-regulation codes, even though it does not foster the audience or the interest groups
participation. Besides, the entity must improve some of its indicators, as for example
the right of access – where legislation is speciﬁed, but not the manner –, as well as the
control mechanisms or the content archive, which is clearly insufﬁcient (Table 3).
Table 2. (continued)
Indicator
TVN TPA TNU PTV
Is there any information available on the working conditions or the
applicable collective agreement?
X
X
X
X
Is there any clear information on the job offers and selection
processes?
X
√
√
X
Do the names, surnames, photos and trade union groups belonging
to the workers’ representatives?
✓
X
X
X
Table 3. Transparency indicators on information production and content access
Indicator
TVN TPA TNU PTV
Is there any basic information provided about the Drafting
Committee or any other similar body in the ﬁeld of content?
X
X
X
X
Is the editorial approach of the medium expressed, as well as the
existence of a clear and accessible code of ethics?
✓
X
√
X
Is there a data protection policy?
✓
X
X
X
Are there effective channels for the audience
participation/consultation/claims?
✓
√
√
P
Is there any speciﬁc place on the website for the ﬁgure of the viewer
defender or similar role?
X
√
X
X
Can the right of access to public information be exercised?
P
P
P
X
Is there a content archive?
P
√
√
X
Are there self-regulation codes on contents and children?
✓
X
P
X
Are there information treatment codes on vulnerable groups?
✓
X
√
X
Are there mechanisms provided for the program and content
assessment?
X
√
√
X
Are the intellectual property laws published, applicable to the
broadcasted content?
X
X
X
X
Are the program audience data published?
X
X
X
X
Is there any information on the quality control mechanisms?
P
X
√
X
Is the website adapted to mobile devices?
✓
√
√
✓
Are there any applications enabling the content accessibility to
people with some sort of disability?
X
P
√
X
Are there operational social networks?
✓
√
√
✓
1040
P. C. López-López et al.

Moreover, besides the most basic channels aimed at establishing a link with citi-
zens, as is the case of the email, the Argentinian public television broadcaster offers a
section called Defensoría ciudadana (Protection of citizens’ rights), oriented towards
the promotion of dialogue with the audience on the journalistic functioning with the
objective of improving the procedures and better understanding the user needs. Sim-
ilarly, the Uruguayan entity includes documents regulating the informative treatment of
violence against women or sexual diversity within the broadcaster. In this sense, the
Uruguayan broadcaster obtains a high degree of fulﬁllment in the indicators measuring
the existence of a code of ethics, as well as a high degree of information treatment on
vulnerable groups. Finally, the Paraguayan television broadcaster fulﬁlls the purely
formal requirements: adaptation to mobile devices and operational social networks.
4
Discussion and Conclusions
Once the pertinent indicators applied, our ﬁrst conclusion is that all the television
broadcasters from the so-called American Southern Cone fail in transparency, although
some nuances need to be clariﬁed. From a quantitative point of view, the highest
scoring broadcaster is TVN, with a total score of 45.686 points out of 100, rather
equally divided among the three analyzed blocks, highlighting the block related to
institutional information. The second highest scoring broadcaster is the Uruguayan
television (TNU), scoring 27.584 points and obtaining a high score in the scope of
information production and participation (17.24). In third place, the Argentinian entity
scoring 24.998 stands also out when it comes to the block devoted to audience par-
ticipation and to the publication of its information production processes. Lastly, the
PTV scores only 6.896 points and it does not obtain any scoring regarding economy
and infrastructure. As a second conclusion, we must say that, excluding the Paraguayan
public television broadcaster, the other broadcasters do reach the minimum required
parameters in the ﬁeld of information production and participation. This is due to three
elements: the ease of establishing this type of data, that does not clash with any interest
or directed will; the experience gained in journalism and the social consensus present in
the treatment of the priority attention groups; ﬁnally, the development of the infor-
mation technologies and the new use patterns that force them to have, among other
things, operational social networks or adapted web spaces.
Thirdly, public television broadcasters do not comply with their respective national
transparency laws in the institutional and economic ﬁeld. Nevertheless, we must note
that a large number of precepts are too generic and not explicit enough to fulﬁll them.
Thus, the regulatory and instrumental development of such laws is important, together
with international recommendations and common sense in order to improve in the ﬁeld
of accountability. Fourthly, the situation of Paraguay TV is extremely vulnerable. This
broadcaster, dependent on the government and on the air since 2011, does not comply
with the minimum democratic standards. Even though the conditions for the economic
and social development have a strong impact on the organizations’ transparency, these
conditions are not an excuse since in Paraguay there are legislations (no matter how
meager they are), tools and knowledge allowing for improvement. This paper does not
intend to assess the political will of those responsible for making the public information
Transparency and Participation in Public Service
1041

of Paraguay TV available to the audience, but the lack of the political will is certainly
one of the reasons why this entity obtains such a low score in the indicators.
Finally, we must highlight the need for an enlargement and readjustment of those
indicators to study the private entities and public television broadcasters from other
latitudes. Similarly, the afore-mentioned three blocks must be systematically applied in
the scope of other research tasks in order to be fully validated and consolidated in the
scientiﬁc ﬁeld. This is aimed at establishing an international classiﬁcation of public
television broadcasters that would allow for deducing which broadcasters hold the most
transparency, and for concluding whether transparency is linked or not to the country’s
economic, social, regulatory, or cultural development, as well as to the political sys-
tem’s structure and its connection with media companies.
Acknowledgments. The results of this paper are taken from the project “Indicators related to
European broadcasters governance, funding, accountability, innovation, quality and public ser-
vice applicable to Spain in the digital context” (CSO2015-66543-P), from the National Program
for Fostering Excellence in Scientiﬁc and Technical Research, National Subprogram for
Knowledge Generation from the Spanish Ministry of Economy, Industry and Competitiveness,
co-ﬁnanced by ERDF.
References
1. Bucci, E., Chiaretti, M., Fiorini, A. M.: Indicadores de qualidade nas emissoras públicas,
uma avaliação contemporânea In: Serie Debates CI: Comunicação e Informação, vol. 10,
p. 26. UNESCO, Brasilia (2012)
2. Fernández Lombao, T.: La página web como espacio de transparencia de las radiotelevi-
siones públicas. In: Mateos, C., Herrero, J., (coord) La pantalla insonme, pp. 543–554.
Universidad de La Laguna, Tenerife (2015)
3. Becerra, M., Waibord, S.: Principios y “buenas prácticas” para los medios públicos en
América Latina. In: Cuadernos de Discusión de Comunicación e información 3. UNESCO,
Montevideo (2015)
4. López Rubí Calderón, J.R.: Reseña de ¿Qué es la rendición de cuentas? de Andreas
Schedler. In: Foro Internacional, México, XLVI, pp. 164–166 (2006)
5. Schedler, A.: ¿Qué es la rendición de cuentas?. Instituto Federal de Acceso a la Información
Pública, México (2004)
6. Túñez, M.: La gestión de la comunicación en las organizaciones. Comunicación Social
Ediciones y Publicaciones, Zamora (2012)
7. López-López, P.C., Vaca-Tapia, A.C., Molina Rodríguez-Navas, P.M.: La transparencia en
las televisiones del Ecuador: una revision legal de la información pública. In: Comhuman-
itas: revista cientíﬁca de comunicación, vol. 8(1), pp. 137–158. Universidad de los
Hemisferios, Quito, Ecuador (2017)
8. López-López, P.C., Ulloa-Erazo, N., Puentes-Rivera, I.: Transparency and new technologies:
accountability of public television broadcasters in the Andean Countries. In: Rocha, Á.,
Correia, A., Adeli, H., Reis, L., Costanzo, S. (eds.) Recent Advances in Information Systems
and Technologies, WorldCIST 2017. Advances in Intelligent Systems and Computing, vol.
571, pp. 73–82. Springer, Cham (2017)
9. Informe sobre desarrollo humano. http://hdr.undp.org/sites/default/ﬁles/HDR2016_SP_
Overview_Web.pdf
1042
P. C. López-López et al.

10. PIB pér cápita Banco Mundial. https://datos.bancomundial.org/indicador/NY.GDP.PCAP.
CD
11. Índice de Percepción de la Corrupción de Transparencia Internacional. http://transparencia.
org.es/wp-content/uploads/2017/01/tabla_sintetica_ipc-2016.pdf
12. López-López, P.C., Puentes-Rivera, I., Rúas-Araújo, J.: Transparencia en televisiones
públicas: desarrollo de indicadores y análisis de los casos de España y Chile. In: Revista
Latina de Comunicación Social, Tenerife, Spain, vol. 72, pp. 253–272 (2017)
13. Lopez-Lopez, P.C, Lopez-Golan, M., Puentes-Rivera, I.: Hypertransparency and new
technologies: analysis of public information in TVE and RPT. In: Iberian Conference on
Information Systems and Technologies, CISTI (2017)
14. UNESCO: Quality Indicators for Public Broadcasters – Contemporary Evaluation.
UNESCO, Brasilia (2012)
15. European Broadcasting Union: PSM Values Review: The tool. European Broadcasting
Union. EBU, Ginebra (2014)
Transparency and Participation in Public Service
1043

Knowledge Based of an Expert System
Using the Horizontal Analysis for Financial
Statements of National, Private TV
Companies in Ecuador
Ana Cecilia Vaca-Tapia1,2(&), Francisco Campos Freire1,2,
Francklin Iván Rivas-Echeverría1,3,
and Johnny Alejandro Aragón-Puetate1
1 Pontiﬁcia Universidad Católica del Ecuador, Sede Ibarra, Ibarra, Ecuador
{acvaca,jaaragon}@pucesi.edu.ec,
francisco.campos.freire@gmail.com, frivas6@gmail.com
2 Universidade Santiago de Compostela, Santiago de Compostela, Spain
3 Universidad de los Andes, Mérida, Venezuela
Abstract. This paper presents the knowledge base of the Expert System using
the horizontal analysis as a way of supporting ﬁnancial trends research. There
must be handled knowledge of both accounting and management of television
companies, as well as a knowledge engineer who analyzes the situations typi-
cally presented, and from them extracts the rules of application. The applied
rules are increasing, decreasing and stationary trends, from which the results
help to make decisions for the ﬁnancial development of private television
companies in Ecuador.
Keywords: Expert systems  Horizontal/trend analysis  Knowledge base
1
Introduction
The horizontal trend analysis is an important tool to evaluate the performance of a
business over time, making it feasible to distinguish what progress or setbacks have
been made and, based on the preferences of management or the owners, adopt the
necessary measures to achieve short, medium and long-term goals [1].
This study utilizes a horizontal analysis. This process uses various accounting
periods of the same business, such as absolute values and indices, and presents them
graphically to determine the slope of ﬁgures of the different components of the ﬁnancial
statements, in order to make future plans. Changes in the company do not occur in a
speciﬁc moment, but are presented gradually over time. Thus, observing past behaviors
while thinking of the future allows the company to rectify past tendencies so that they
are in agreement with its future goals [1].
This research also uses an experimental method, through an expert system, to
process and analyze the ﬁnancial information of the television companies under study.
In this work, the knowledge base of such an expert system for a ﬁnancial-economic
analysis is presented; an original model designed for this research, which is not known
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_99

to have been used in a similar way in any other research of this kind surrounding the
economic management of television.
The expert system uses artiﬁcial intelligence, which is, “the capacity to emulate the
intelligent functions of the human mind” [2].
Within the areas of artiﬁcial intelligence lie the expert systems, which are computer
processes that incorporate the knowledge of an experienced person within their oper-
ation system, in such a way that they are able to both respond and explain and justify
their responses [3].
For an expert system to be a practical tool, it must be understandable. Based on the
methods used, as well as the important acquired experience of those carrying out the
ﬁnancial analysis, this computational tool offers vital support to the studies developed
in this research.
Thus, to develop the expert system for ﬁnancial analyses, information provided by
the Superintendent of Companies, Insurance and Security and expert criteria were taken
into account.
2
Methodology for the Generation of the Knowledge Base
The methodology used in this study is “the qualitative method, which uses data col-
lection without a numerical measurement to discover or ﬁne tune research questions in
the data interpretation process” [4].
For the development of an expert system, the methodology that emerges from the
integration of approaches, techniques and other methodologies from different areas
related to knowledge-based systems and Software Engineering are taken into account.
The advantage of this method is that it is based on knowledge and seeks the greatest
use of all available resources in the area where it will be developed [5]. The phases of
the system are:
Phase1. Knowledge Acquisition: Where the Knowledge Engineer interacts with the
expert in order to obtain information for the solution to the problems, as well as the
strategies used to obtain each solution.
Phase2. Knowledge Shaping: The Knowledge Engineer must bring the information
provided by the expert to a knowledge base [5].
3
Knowledge-Based Expert System of the Horizontal
Analysis
For the development of the knowledge base, the experts were asked to speak about the
competencies involved and to consolidate the concepts on the indexes of the horizontal
analysis. A ﬁnancial study of several accounting periods of the companies is carried out
through liquidity, solvency, management and proﬁtability factors. The parameterization
of these factors is associated with different values; the knowledge engineer was the one
who discovered them in the discourse of ﬁeld experts. The generic preparation for this
study is Increasing Trend (IT), Decreasing Trend (DT) and Stationary Trend (ST).
Knowledge Based of an Expert System
1045

Following is a detail of the trends, which will be presented at the moment of
entering the information of each of the accounts on the ﬁnancial statements, as reported
to the Superintendent of Companies, Insurance and Securities:
3.1
Liquidity Factor
3.1.1
Current Liquidity
3.1.1.1. IT. Represents an incremental variation in current assets or a decrease in
current liabilities. This may be due to a change in credit policies, inventory overstock,
or a surplus of liquid assets with a minimum operating cash surplus. The deduction of
current liabilities arises from the payment of debts to suppliers and a reduction in credit.
It is recommended to perform a maturity analysis as well as analyze incremental costs
and yields of the ﬂexibility of credit policies.
3.1.1.2. DT. Represents an increase in the risk of achieving the short-term operational
obligations due to the decrease in the minimum operating cash, restoration and/or
restriction of credit policies, a deduction in inventory levels or an increase in short-term
debt. It is recommended to perform a maturity analysis as well as analyze incremental
costs and yields of the ﬂexibility of credit policies and minimum and maximum levels
of inventory to not waste the market demand.
3.1.1.3. ST. Represents that the company is static under conservative management
criteria. An analysis of index components and strategic planning is recommended.
3.1.2
Acid Test
3.1.2.1. IT. It would represent an increase in payments, ﬂexibility on credit policies,
reduction, lack of replenishment or optimization of inventories and reduction of current
liabilities. It is suggested to analyze the impact of the ﬂexibilization results on the credit
policy, inventory rotation and analysis of portfolio growth.
3.1.2.2. DT. Represents a decrease in the immediate liquidity ratio allowing an
increase of obligations without using the inventory. This could be due to overstock
and/or low turnover of inventories, restrictive credit policies or an increase in
short-term liabilities. An analysis of inventory rotations is recommended as well as a
review of purchasing volume, credit policies and average payment period at the end of
the year.
3.1.2.3. ST. Represents that the company is static under conservative management
criteria. An analysis of index components and strategic planning is recommended.
1046
A. C. Vaca-Tapia et al.

3.2
Solvency Factor
3.2.1
Asset Indebtedness
3.2.1.1. IT. Represents over-indebtedness in short and long-term liabilities, weakening
decision-making capacity, increasing compliance risk, pressing cash ﬂow and jeopar-
dizing future resources. An analysis of the composition of total assets and conditions of
indebtedness is recommended.
3.2.1.2. DT. Represents ﬁnancial autonomy due to an increase in assets by own
investment and a decrease in liabilities. It improves decision-making authority, reduces
the pressure on cash ﬂow and the compliance risk with ﬁnancial obligations. It is
recommended to review credit conditions and the asset composition.
3.2.1.3. ST. Represents that the company is static or performing little business activity.
It is suggested to review operational plans and programs in the short-term.
3.2.2
Equity Indebtedness
3.2.2.1. IT. Represents an overindebtness in short-term liabilities, debilitating
decision-making capacity, increasing compliance risk, pressing cash ﬂow and jeopar-
dizing future resources. It is recommended to perform an analysis of the composition of
total assets and debt conditions.
3.2.2.2. DT. Represents that ﬁnancing sources in the long-term is majorly given by
own property. It is produced by the contribution of cash ﬂow and the generation of net
proﬁts, producing an increase in equity, as well as by the cancellation of long-term
liabilities. It is recommended to analyze credit conditions as well as the beneﬁcial cost
of using own investment.
3.2.2.3. ST. Represents that the percentage of participation is the same as the other
sources of ﬁnancing that surround the investment. It is recommended to perform a cost
and beneﬁt analysis of using own investment and that of third parties.
3.2.3
Indebtedness of the Fixed Asset
3.2.3.1. IT. Represents a greater ﬁnancing with third party resources, and as such an
increase in the risk and pressure of cash ﬂow. The expansion of this index can also be
caused by a decrease in total investments, maintaining the same level of debt. It is
suggested to analyze liabilities in the short and long-term, as well as credit conditions.
3.2.3.2. DT. Represents a decrease in debts and thus a greater participation of own
resources in the assets. It is recommended to analyze the liabilities in the short and
long-term, as well as credit conditions.
Knowledge Based of an Expert System
1047

3.2.3.3. ST. Represents the same ﬁnancing relationship across different accounting
years. It is recommended to analyze the liabilities in the short and long-term, as well as
credit conditions.
3.2.4
Leverage
3.2.4.1. IT. Represents the relationship of total investment with own resources and the
support garnered from third parties. An incremental variation signiﬁes the expansion of
risk and vulnerability to a reduction in assets. It is recommended to analyze the
composition of assets and equity income and verify debt volumes and their conditions.
3.2.4.2. DT. Represents a higher degree of participation of own resources in the
ﬁnancing of total assets. It is recommended to analyze the composition of the assets and
equity income and verify the debt volumes and their conditions.
3.2.4.3. ST. Represents that the integral ﬁnancing policies have not been modiﬁed. It is
recommended to analyze the composition of the assets and equity income and verify
the debt volumes and their conditions.
3.2.5
Financial Leverage
3.2.5.1. IT. Represents the relationship of equity income with the surplus of assets,
without taking into consideration the interests generated by indebtedness. It signiﬁes
that the company had greater ﬁnancial leverage, producing a greater proﬁtability of its
own resources. It is recommended to ﬁnd sources of ﬁnancing from third parties with
soft costs and deadlines that do not pressure cash ﬂow.
3.2.5.2. DT. Represents that the company used a lower leverage ratio, producing a
decrease in the proﬁtability of its own resources. It is suggested to ﬁnd sources of
ﬁnancing from third parties with soft costs and deadlines that do not pressure cash ﬂow.
3.2.5.3. ST. Represents that the management of the business is satisﬁed with the ratio
between its own resources and those of third parties. It is suggested to ﬁnd sources of
ﬁnancing with soft costs and deadlines that do not pressure cash ﬂow.
3.3
Management Factor
3.3.1
Portfolio Turnover
3.3.1.1. IT. Represents the number of times that the investment in client portfolios is
recycled in a determined period, reducing the cash cycle. It is recommended to revise
the credit and collection policies.
3.3.1.2. DT. Represents an increase in the investment of clients and as such an increase
in the cash cycle. It is recommended to revise the credit and collection policies.
1048
A. C. Vaca-Tapia et al.

3.3.1.3. ST. Represents that the company is satisﬁed with the credit volumes. It is
recommended to revise the credit and collection policies.
3.3.2
Rotation of Fixed Assets
3.3.2.1. IT. Represents the visualization of the contribution of ﬁxed assets to the
volume of sales. Investment in long-lived assets is more efﬁcient than in the previous
year and overall proﬁts will be higher. An analysis is suggested to determine the
existence or not of unproductive assets.
3.3.2.2. DT. Represents that the ﬁxed assets do not contribute signiﬁcantly to the
volume of sales, generating higher operational costs and a decrease in global proﬁts. It
is recommended to perform an analysis to determine the existence or not of unpro-
ductive assets.
3.3.2.3. ST. Represents that the company has not managed increases in sales of its
long-lived assets efﬁciently. An analysis is recommended to determine if unproductive
assets exist.
3.3.3
Sales Rotation
3.3.3.1. IT. Represents the efﬁciency of the business management in the use of total
investments to generate larger volumes of sales. It is recommended to revise sales
policies, as well as consumer behavior and investment in short and long-term assets.
3.3.3.2. DT. Represents little efﬁciency in investment and as a consequence, increases
in unproductive assets and a decrease in sales. It is recommended to revise sales
policies, consumer behavior and investment in short and long-term assets.
3.3.3.3. ST. Represents that the company has not managed investment to execute
sales. It is recommended to revise sales policies, as well as consumer behavior and
investment in short and long-term assets.
3.3.4
Average Collection Period
3.3.4.1. IT. Represents the time it takes the company to recuperate the credit granted.
An increase in this index signiﬁes that the consumers are taking longer to pay their
debts. It is recommended to revise the allowed time limit and the degree of efﬁciency as
well as the effectiveness of the credit and collection policies.
3.3.4.2. DT. Represents more restrictive credit policies or a better application of
collection statutes. It is recommended to revise the allowed time limit and the degree of
efﬁciency as well as the effectiveness of the credit and collection policies.
Knowledge Based of an Expert System
1049

3.3.4.3. ST. Represents a lack of client analysis in regards to the allowed time limit. It
is recommended to revise the allowed time limit and the degree of efﬁciency as well as
the effectiveness of the credit and collection policies.
3.3.5
Average Payment Period
3.3.5.1. IT. Represents the time it takes the company to meet their obligations with
suppliers. An increase in the index could be read as an increase in the time limit granted
by the suppliers or a lack of liquidity to comply with the payments by the agreed dates.
It is recommended to analyze the time limit granted by suppliers, the average collection
period and inventory turnover.
3.3.5.2. DT. Represents that the company recovers their credits faster, being positive,
granted they receive discounts for prompt payment. It should also be taken into account
whether the decrease in the average payment period means getting close to the deadline
determined by the supplier. It is recommended to analyze the time limit granted by
suppliers, the average collection period and inventory turnover.
3.3.5.3. ST. Represents that the company has not analyzed its behavior in regards to
the time limit set by third parties. It is recommended to analyze the terms granted by
suppliers, the average collection period and inventory turnover.
3.3.6
Impact of Administrative Expenses and Sales
3.3.6.1. IT. Represents a proportional relationship between operational expenses and
sales made. An increase in this ratio signiﬁes that the company has raised its opera-
tional expenses and seen a decrease in proﬁts. It is recommended to perform a detailed
analysis of the operational expenses to optimize the operational disbursements.
3.3.6.2. DT. Represents a better administration of operational expenses, generating
higher proﬁtability levels. It is recommended to perform a detailed analysis of the
operational expenses to optimize the operational disbursements.
3.3.6.3. ST. Represents that the company does not perform analyses to optimize its
resources. It is recommended to examine in detail the operational expenses to optimize
the operational disbursements.
3.3.7
Impact of Financial Burden
3.3.7.1. IT. Represents the incidence of interest paid on leverage in the company’s
sales. This signiﬁes that the sales decreased or interest expenses increased due to the
rates and/or higher amounts of debt. It is recommended to revise the credit conditions
and the market to increase sales.
1050
A. C. Vaca-Tapia et al.

3.3.7.2. DT. Represents an improvement in the volume of sales or a decrease in
interest costs via renegotiation and/or debt payment. It is recommended to revise the
credit conditions and the market to increase sales.
3.3.7.3. ST. Represents that the company considers the interest load and sales volumes
adequate or does not perform analyses to optimize its resources. It is recommended to
revise the credit conditions and the market to increase sales.
3.4
Proﬁtability Factor
3.4.1
Net Proﬁtability of the Asset
3.4.1.1. IT. Represents that the company utilizes the investment in assets to better
generate sales. It is recommended to periodically revise operational expenses, sales
policies and the quality of investment in long-term assets.
3.4.1.2. DT. Represents a decrease in net proﬁts or an increase in the investment in
unproductive assets. It is recommended to revise operational expenses, sales policies
and the quality of investment in long-term assets.
3.4.1.3. ST. Represents a lack of analysis of opportunities for the optimization of
resources. It is recommended to revise operational expenses, sales policies and the
quality of investment in long-term assets.
3.4.2
Gross Margin
3.4.2.1. IT. Represents that the company optimizes its sales costs and generates a
better capacity to cover costs and ﬁxed operational expenses. It is recommended to
revise acquisition cost and use of inventory.
3.4.2.2. DT. Represents an increase in sales costs and a decrease of the margin of
contribution to cover its ﬁxed costs and expenses, decreasing the company´s prof-
itability. It is recommended to revise operational expenses, sales policies and the
quality of investment in long-term assets.
3.4.2.3. ST. Represents that the company has not looked for a way to reduce the costs
of buying and managing inventory. It is suggested to review the amount of acquisition
and use of inventory.
3.4.3
Operational Margin
3.4.3.1. IT. Represents that the company has optimized the use of operational
expenses or has reduced its sales costs. It is recommended to analyze the administrative
and sales expenses, breaking down each component with the goal of optimizing them
and verifying the inventory turnover to try to increase the volume of goods or services.
Knowledge Based of an Expert System
1051

3.4.3.2. DT. Represents that the company is incurring higher costs for the same vol-
ume of sales or that the increase in expenses and costs is superior to the increase in the
volume thereof. It is recommended to analyze the administrative and sales expenses,
breaking down each component with the goal of optimizing them and verifying the
inventory turnover to try to increase the volume of goods or services.
3.4.3.3. ST. Represents that the administrative and sales costs and expenses did not
change or that the change is equal to the alteration thereof. It is recommended to
analyze the administrative and sales expenses, breaking down each component with the
goal of optimizing them and verifying the inventory turnover to try to increase the
volume of goods or services.
3.4.4
Net Proﬁtability of Sales
3.4.4.1. IT. Represents that the company could generate a signiﬁcant increase in
non-operational revenue, which should be analyzed by its seasonal characteristic. It is
recommended to analyze this indicator with the operational margin to determine the
quality of the company’s own earnings.
3.4.4.2. DT. Represents that the company has decreased its volume of non-operational
revenue. It is recommended to analyze this indicator with the operational margin to
determine the quality of the company´s own earnings.
3.4.4.3. ST. Represents that the company is satisﬁed with the activity to generate proﬁt
for each monetary unit. It is recommended to analyze this indicator with the operational
margin to determine the quality of the company´s own earnings.
3.4.5
Operational Return on Equity
3.4.5.1. IT. Represents a better administration of revenues and operational costs by
management, beneﬁtting capital remuneration. It is recommended to analyze the
proﬁtability after interest and taxes, and compare it with this indicator to measure the
impact of ﬁnancing costs and the tax burden on proﬁts for investors.
3.4.5.2. DT. Represents a very inconvenient decrease in capital remuneration for the
owners of the company. It is recommended to analyze the proﬁtability after interest and
taxes, and compare it with this indicator to measure the impact of ﬁnancing costs and
the tax burden on proﬁts for investors.
3.4.5.3. ST. Represents that no strategies have been generated to improve proﬁtability
levels, and as such the owners receive the same remuneration as their invested capital.
It is recommended to analyze the proﬁtability after interest and taxes, and compare it
with this indicator to measure the impact of ﬁnancing costs and the tax burden on
proﬁts for investors.
1052
A. C. Vaca-Tapia et al.

3.4.6
Financial Proﬁt
3.4.6.1. IT. Represents that the establishment produces greater wealth, thus directly
beneﬁtting capital remuneration. It is suggested to analyze this ﬁnancial ratio against
the opportunity cost of the investors, to verify the degree of convenience of maintaining
the resources invested in the business.
3.4.6.2. DT. Represents a decrease in the perceived beneﬁt by the investors. It is
suggested to analyze this ﬁnancial ratio against the opportunity cost of the investors, to
verify the degree of convenience of maintaining the resources invested in the business.
3.4.6.3. ST. Represents that no strategy has been generated to improve proﬁt levels. It
is suggested to analyze this ﬁnancial ratio against the opportunity cost of the investors,
to verify the degree of convenience of maintaining the resources invested in the
business.
4
Conclusions
This model is a proposal with respect to the economic-ﬁnancial research of television in
particular, and of media companies in general. The development of this model is also of
interest for the analysis of the ﬁnancing of public radio broadcasting in Europe, which
serves as the object of study and research, with the participation of several authors of
this chapter.
The expert system is a set of rules which, through artiﬁcial intelligence, helps to
process the economic-ﬁnancial information, for which the needs of the model have
been delimited. Once its design has been outlined, it is granted the support of a
computer programmer for its technical programming.
The proposed knowledge base makes the expert system a practical tool, as it is now
understandable. Based on the techniques used, and the importance of the experience
gained by those conducting the ﬁnancial analysis, this computational instrument pro-
vides important support to the studies to be carried out on private television companies
in Ecuador.
Recognition of research
The results of this article form a part of the activities of the project entitled “Indi-
cadores de gobernanza, ﬁnanciación, rendición de cuentas, innovación, calidad y
servicio público de las RTV europeas aplicables a España en el contexto digital”
(Indicators of governance, ﬁnancing, accountability, innovation, quality and public
service of European radio & television, applicable to Spain in the digital context)
(Reference CSO2015-66543-P) by the Spanish National Plan for Scientiﬁc and
Technical Research and Innovation and The National Sub-program of Knowledge
Generation of the Ministry of Economy, Industry and Competitiveness of Spain,
co-ﬁnanced by the European Regional Development Fund (ERDF) of the European
Union.
Knowledge Based of an Expert System
1053

References
1. Arias Anaya, R.M.: Análisis e interpretación de los estados ﬁnancieros. Editorial Trillas,
México (2012)
2. Badaro, S., Ibañez, L., Agüero, M.: Sistemas expertos: fundamentos, metodologías y
aplicaciones. Rev. Cienc. Tecnol. 13, 349–364 (2013)
3. Munera, L.: Principios de la Inteligencia Artiﬁcial y Sistemas Expertos. Editorial U.I.ESI,
Colombia (2001)
4. Hernández, R., Fernández, C., Baptista, P.: Metodología de la Investigación. Mc Graw Hill,
México (2010)
5. Rodríguez, W., Rivas, F.: Sistemas expertos. In: Aguilar, J., Rivas, F. (eds.) Introducción a las
Técnicas de Computación Inteligente, pp. 13–51. Meritec, Mérida (2001)
1054
A. C. Vaca-Tapia et al.

The Use of Facebook in Community Radio:
A Quantitative Analysis of the Andean
Community of Nations
Viviana Galarza-Ligña(&), Amparo Reascos-Trujillo,
and Stalin Rivera-Imbaquingo
Research Group Media, Applied Technologies and Communication
(METACOM), Pontiﬁcal Catholic University of Ecuador, Ibarra Campus,
Jorge Guzmán Rueda Ave. and Aurelio Espinosa Pólit Ave., Citadel
“La Victoria”, 100112 Ibarra, Ecuador
{vngalarza,areascos,sirivera}@pucesi.edu.ec
Abstract. Facebook, the social network with the greatest number of users at a
global level, is presented as a tool of great interaction and reach for the service
of thousands of corporations, industries and media: of all sizes and character-
istics, which ﬁnd in this platform an ideal method to get closer to its users,
clients and/or audiences. In this way, community radio unites its own charac-
teristics that may be strengthened through the use of this digital platform,
securing social cohesion and the audience participation in all daily tasks. Thus,
this research revises the quantitative means of the principal parameters of
integration and the use of this social network by community radio transmitters in
the Andean Community of Nations (CAN) with the goal of establishing a
comparative analysis between the countries it is made of.
Keywords: Community radio  Andean Community of Nations
Social networks  Alternative communication  Facebook
1
Introduction
Communication as a process of an exchange of meanings and signiﬁers framed in a
cultural context is constructed as a participatory and democratic action. That is how
Pasquali [1] referred to it almost three decades ago, presenting it as a dialogical,
participatory and horizontal process, characterized by inter-learning and empowerment
of the subjects in social construction.
In the same way, Mattelart and Piemme [2] defended the generation of an alter-
native communication in their work, returning the word to the people and imposing
itself as a means of popular mobilization for social transformation.
In this theoretical-critical context of communication, the ﬁst community experi-
ences were ﬁttingly born on the radio. This included the North American initiatives
such as the KPFA in the United States and the Wawatay Radio Network in Canada,
which had already achieved a certain independence from radio media in relation to the
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_100

dominant power of the market and government, permitting a larger ﬁght for equality
and social justice.
The Global Association of Community Radio (AMARC) [3] deﬁnes this type of
transmitter as private actors that have a social goal and are characterized by being
managed through diverse non-proﬁt social organizations. Their fundamental charac-
terization is active community participation in radio programming, operation and
evaluation; as well as their empowerment in its management, ﬁnancing and property.
They are independent, non-governmental media, that do not have a religious connec-
tion and are not the property of or controlled by nor tied to political parties or com-
mercial businesses.
This deﬁnition, like the one of many other Latin American theories, presents the
immersion of the community in radio as a principal characteristic, not only as assiduous
listeners or economic sponsors, but as content generators and, in the best case, as media
owners with management and ideological autonomy. Community radio, then, is media
that evolves with its people, a cultural experience that transforms reality and transforms
with reality.
In this context, community radio cannot remain in archaic processes of communi-
cation in this intent to maintain cultural roots that fears changes and evolution. Rather, as
Soengas [4] explains, radio should be a participant of the evolution of society and of the
media convergence in new information and communication technologies.
1.1
Community Radio and the Network Society
The development and increase of the access to the Internet, now almost a human right,
generates new forms of social interaction, due to the fact that currently, a large part of
human interaction is produced virtually. This, coined by Castells [5] as a Network
Society, is based on “networks of production, power and experience” which allows one
to “acquire, process, store, distribute and access information” [6] in a rapid fashion.
These new platforms of communication disrupt the forms of elaboration and
structuring of messages. The speciﬁcity of media has remained diluted against the
convergence that demands structural changes in the production of content in light of the
new forms of consumption and interaction of the public [7]. However, community
radio, especially in Latin America, has inserted itself in these new platforms without
greater preparation, and has formed a part of the network society, taking on a new
space of fashion, though without understanding its demands and the new communi-
cational paradigms it faces.
Additionally, not only does the Network Society evolve, but also the concept of
community. Although interpersonal links continue to provide sociability, support,
information, a sense of belonging and a social identity [8], they also take on new
characteristics of the virtual spaces, referred to as such by Marta González, mentioning
four basic elements that create a “community”:
• Belonging (Log in): The people in the community share the same codes that make
them feel like they are a part of the group.
• Inﬂuence (Share this): Membership of this group grants its members the possibility
to establish reciprocal inﬂuences.
1056
V. Galarza-Ligña et al.

• Integration (Content is King): Members of the group arrive to a certain dependence,
sharing content, achieving authenticity, operability, and durability. It works better
on a personal level than on an anonymous level.
• Connection (Contacts are Queen): There is regular and satisfactory contact between
its members with actualization, bonding, propagation, afﬁliation, and monitoring [9].
These parameters should be taken into account by the community media in order to
understand the implications that the management of communication 3.0 brings with it,
when more demanding users that search for personalized attention and relationships
that transcend media.
The majority of community radio outlets do not appear to be ready to face the
demands of the new audience, which searches on the Internet what Aparici calls an
integrated language [10] in which the “word, image, and sound form a uniﬁed and
coherent whole” [11], acting as the basis of media convergence. This requires not a
simple adaptation, but a complete transformation of its contents. Another difﬁculty
community media faces is the consistent taking on of new roles in online communi-
cation tools, which are constantly emerging and even overlapping and require their own
language and an individual management to generate the expected results.
Besides that, is the lack of economic resources for training and technological
innovation, without neglecting the digital divide that is still considerable in the Andean
Community countries. This reality often causes the media to become isolated and
stagnant in simple and well-known forms of radio production, without taking advan-
tage of the new possibilities offered by ICT.
1.2
Community Radio, Social Networks and Relationship Management
In the industry ﬁeld relationship management is a term focused on contributing to the
improvement of relationships and the closeness between a company and its clients, in
an organized and personalized way, with social networks serving as one of the most
preferred tools in this ﬁeld. In this way, social networks have become the best window
of opportunities for all types of businesses, institutions, NGOs, and communication
media to be more visible for their clients and generate a valued reputation for their fans
as well as improve interactions that are produced in this space.
Orihuela and Salverría, as cited by Dupin [12], agree that social networks are a
prolongation of personal relationships in real life, that is to say, they are new envi-
ronments that allow one to maintain contact with their communities, family, friends or
colleagues. This is why these spaces make up an important part of people’s lives, since
there is where they share, debate and express themselves with much more freedom,
even more so than in reality.
However, in light of this new system, community communication media has
principally understood social networks as a means of diffusion and distribution [13],
precisely leaving the management of relationships behind and throwing away its efforts
to be present in each social network that arises. It has concentrated on generating the
greatest quantity of information possible, leaving behind user participation. And thus,
the function of a telephone call or a letter to the editor in traditional media is replaced
The Use of Facebook in Community Radio
1057

by a like or by visualizations, in lieu of taking advantage of the characteristics of the
virtual platform to generate an active and mobilized public.
For the journalist and expert in digital culture, Fernández [14], social networks
should be considered as “virtual spaces organized to develop projects, integrate com-
munities, create services and project themselves using all virtual ability”.
Community radio, then, should use the potential of these new forms of creating a
community to strengthen the very characteristic of media that makes the roles of sender
and receiver be easily interchangeable and, especially, generate closeness and famil-
iarity with the listener to empower the community to feel a shared responsibility of the
management of the media.
1.3
Facebook, a Platform for Social Integration
One of the social networks with the greatest impact at the global level is Facebook,
created by Mark Zuckerberg in 2004 as a class project, which sought to create an
exchange of messages and content in a simple manner through the Internet. In June of
2017, Zuckerberg announced that it had reached 2 billion users at a global level. This
network, in only 13 years, has become the website with the most important number of
users of all types and ages. Its acceptance is due in great deal to its easy access,
intuitive interface, and constant innovation.
The greatest advantage this network offers is its high component of interactivity and
the generation of pro-consumers, making all the communication tools available to any
person on a personal, familial, educational, social, and even global level, with a high
sense of belonging and identity, causing to occupy a large space in people’s lives [15].
Sued indicates that the virtual communities on Facebook are made up of four
principle elements that are present to a greater or a smaller extent. The emergence of
political debates, support of members of distinct causes, the formation of an individual
and group identity, and the narrow relationship between what is real and what is virtual
(…). Facebook users, when posting, generate a convergence of points of view. The
repercussion that a post has on other users generates that… this idea must be com-
pleted, in a platform that permits everyone to express themselves horizontally without
hierarchies. Collective intelligence, then, is a consequence, fundamentally based on
communication [16].
In this way, Facebook, ignoring the many critiques it faces for its management of
information and user privacy, constitutes a platform that allows social relationships to
be visualized, offering a scenario for community radio that, in the ﬁrst place,
strengthens community unity, and in second place, promotes development and social
projects that may beneﬁt its audience and the collective community.
2
Methodology
This research aims to quantify the basic paradigms of management and the use of the
social network Facebook by community radio of the countries that are part of the
Andean Community of Nations (CAN). It starts with the bibliographic review of
several authors which serves as the basis for analysis. In this exploratory descriptive
1058
V. Galarza-Ligña et al.

study, the Facebook accounts of the community radios of Colombia, Ecuador, Peru and
Bolivia were observed in order to determine the numerical indices of their
management.
The analysis begins with the determination of samples, for which a census was
distributed to the community radios in the Andean Community countries, studying the
ofﬁcial webpages of governmental entities that government telecommunications in
these countries. In the case of Peru, as we were not able to ﬁnd any detailed information
in the ofﬁcial organism, the list provided by the Global Association of Community
Radio (AMARC) was taken as the basis, from which a digital review of the afore-
mentioned radios was developed to determine if there was indeed a self-designation of
community. To complement this list, journalistic and academic research was accessed
as well as a selection of the quantity established in government data. Once the radios
were identiﬁed, their respective Facebook accounts were located to determine their
presence on this social network.
The second sample was formed by the radios that possessed a fanpage established
on Facebook’s platform, in which the following indicators were examined (Table 1):
For the collection of data, the digital tool Fanpage karma was used, applied to the
fanpages of community radios in a period of 90 days, that is to say, from June 1st to
August 31st, 2017. From there, the data from each country was developed to create
comparative tables that permit a global vision of the situation of community radios on
Facebook.
Table 1. Indicators in the analysis of fanpages of community radio in CAN countries
Variable
Indicator
Detail
Performance
Performance index
Average increase and participation of fans, out
of 100
% increase from the
starting point
Increase in fans and publications from the
opening of the fanpage
Recognition
Number of fans
Quantity of followers
Number of “likes”
Quantity of interested parties
Total growth
Increase in fans within the period
Management
Publications
Total number of publications within the period
“image” publications
Quantity of images posted within the period
“video” publications
Quantity of videos posted within the period
“link” publications
Quantity of links posted within the period
Participation
Commitment
Average number of interactions reactions/fans
Reactions
Number of reactions within the time period
Comments
Number of comments within the time period
Shares
Number of shares within the time period
Fan publications
Number of publications by followers within the
time period
Interactivity
Comments on fan
publications
Number of feedbacks within the time period
The Use of Facebook in Community Radio
1059

3
Analysis of Results
The countries analyzed, while having similar socioeconomic and cultural realities,
show stark differences in terms of the existence of state-recognized community radios
and their access to Facebook. Figure 1 shows the total quantity of community radios in
the countries, separating them into three categories; Fanpage, which refers to the radios
that have an institutional page on Facebook; Does not analyze, in terms of the exis-
tence of personal proﬁles; and Does not have, which corresponds to groups with less
than 100 followers or which have insufﬁcient activity, which are not an object of
analysis for the indicators.
In the graph, one can clearly observe the existent gap between the quantity of
community radios in Colombia (627) with the other CAN countries, which, all together
(120) do not reach even 25% of the radios in Colombia. Even so, in terms of a
percentage difference, Colombian radios do not present a high institutional access to
Facebook, with only 34% of radios having fanpages, in comparison with Ecuador
(62%), Peru (74%) and Bolivia (17%), which has more radio stations than Ecuador and
Peru (56) but does not use Facebook in its communication processes.
In terms of the analysis of management parameters of social networks, the ﬁrst two
variables are shown, indicating the performance and recognition of the Fanpage, taking
into account that, due to the difference in the number of radio stations per country, the
averages of each country are presented.
In Table 2, it can be observed that Peru leads in the majority of indicators, pre-
senting a higher average in Performance index (increase in fans and participation)
with 3 radios obtaining a percentage of 100/100; in Ecuador, the highest percentage
achieved by a radio is 64/100; in Colombia, they reach 59/100 and in Bolivia 53/100 in
terms of the maximum range for individual qualiﬁcations. The same tendency is
maintained in the Percent increase from the start date, and in total growth, but falls
Fig. 1. Relationship with access to Facebook by community radios in CAN countries
1060
V. Galarza-Ligña et al.

in terms of the average number of followers and number of people interested; indicators
in which Ecuador presents the highest index. The averages that Colombian radio
present are important, as this country has the highest Internet access and social media
presence, obtaining ﬁrst place in the 2017 statistics, according to the organization
“Alianza para el Internet Asequible” (Alliance for Accessible Internet) (A4AI). Even
so, it is evident that community radio is relegated in terms of the management of
Facebook.
In terms of the management variable, the average number of publications of the
radios in each country are shown, taking into account multimedia indicators as well,
such as the publication of images, videos and links (Fig. 2).
A big advantage for Peru can be seen in terms of the average number of publi-
cations by its community radios, which shows a high management factor, taking into
account that an analysis of 90 days was carried out, implying an average of 7 publi-
cations daily. The data for Ecuador, however, presents a low index of the publication of
links, indicating a lack of hypertextuality in the fanpages of its radios; a situation that is
also true for Bolivia. In regard to Colombia, its lowest index is for the publication of
videos, which constitute a powerful tool in social media to eliminate the limitations the
radio faces of only appealing to one of the ﬁve senses.
Table 2. Relationship and recognition of fanpages of community radios in CAN countries
Countries
Performance
index
% Increase from the
start date
Fans
Likes
Growth
Colombia
10.13%
0.55%
3,574.50
1,271.49
48.54
Ecuador
14.83%
1.28%
89,714.04
31,744.88
50.54
Peru
38.43%
7.22%
21,579.71
21,265.43
1,916.07
Bolivia
23.75%
3.37%
1,626.67
876.56
35.00
Fig. 2. Average number of publications by community radios in CAN countries
The Use of Facebook in Community Radio
1061

The last two variables analyzed show the situation of the radios in terms of par-
ticipation and interactivity, which are of vital importance. It could even be said that
they represent the reason to use social media (Table 3).
According to the comparative analysis, Peru has the greatest advantage over the
other countries in the majority of indicators, but in terms of reactions to publications, a
higher average is seen in Ecuador. Although in terms of comments and, above all, the
action of sharing publications, Peru exceeds the indexes in Ecuador. On the other hand,
the variable of interactivity analyzed through the comments on fan publications indi-
cates a poor feedback process by the community radios towards their audiences,
resulting that Colombia is the country in which less radios respond to their listeners.
In a general vision of the data, the participation indexes are very low. In the case of
Peru, for example, a relationship of only 44.87 reactions per publication is seen, which
indicates that from an average of more than 21 thousand fans, only 50 participate in a
basic manner. Actually, in the two ﬁnal indicators, the active participation index with
comments (10.35 comments per publication) and fan publications (.0001 publications
per fan), show a reality that is not much better in the other countries, as observed in
Fig. 3.
Table 3. Indicators of the participation and interactivity of community radios in CAN countries.
Countries Commitment Reactions Comments Shares
Fan
publications
Comments on fan
publications
Colombia 0.17%
1,513.07
157.20
1,157.95
4.07
0.40
Ecuador
0.43%
34,451.85 2,037.31
7,656.85
6.85
2.08
Perú
2.21%
30,608.07 7,063.21
20,497.36 28.14
6.43
Bolivia
0.81%
1,124.67
153.33
440.00
1.89
2.11
Fig. 3. Relationship of the indicators of participation, management and recognition in
community radios in CAN countries.
1062
V. Galarza-Ligña et al.

The indicator that sticks out in the graph shows that Ecuador has a good average in
terms of the relationship between reactions and publications, showing a basic level of
participation by the Ecuadorian audience in community radios. While a more active
participation is desired, through comments and publications, its relationship between
the other indicators is very low.
4
Conclusions
• Community radio is a current and necessary form of media in the current context to
promote the democratization of communication, as well as the strengthening of
social relationships for the generation of a cultural identity.
• The situation of the use and management of Facebook in the Andean Community of
Nations presents various weaknesses that begin with the legislative reality of
community radios, which, regardless of many efforts, are still relegated to rural
spaces, experiencing coverage limitations and are minimized by the government.
• The percent analyses of the variables and indicators of community radios constitute
the ﬁrst step of this research. It is necessary to generate social change projects, with
the development of community media, providing the necessary tools for a just
competition against private communication in order to achieve the necessary
sustainability.
• For social networks, so congested and saturated with information, such as in the
case of Facebook, it is very important to be aware of the limitations and threats that
it may be bring. Community radio must be able to mark differences and spaces of
their own, which link the interests of the public with the goal of identifying with the
media.
• It is necessary to understand the virtual aspect of social networks as a tool with
advantages and disadvantages, not as the panacea of communication. Community
radio must not forget that its principal role is making a community not only in the
virtual world, but in reality, face-to-face.
• The research and analysis of these new processes of communication are needed to
know the reality of community radio in order to delineate proposals according to the
need of the media and beneﬁt from the advantages offered by social networks as
means of empowerment and even as a strategy to achieve the desired sustainability.
References
1. Pasquali, A.: Comprender la comunicación, Monte Ávila Editores, p. 50 (1990)
2. Mattelart, A., Piemme J.M.: La televisión alternativa. Anagrama, p. 13 (1981)
3. Asociación Mundial de Radios Comunitarias, Principios para garantizar la diversidad y el
pluralismo en la radiodifusión y los servicios de comunicación audiovisual, p. 4. Fundación
Ford, Buenos Aires (2010)
4. Soengas, X.: Retos de la radio en los escenarios de la convergencia digital. En: adComunica.
Revista Cientíﬁca de Estrategias, Tendencias e Innovación en Comunicación, no. 5, pp. 23–
26. Universidad Complutense de Madrid y Universitat Jaume I, Castellón (2013)
The Use of Facebook in Community Radio
1063

5. Castells, M.: La era de la información. Economía, sociedad y cultura, vol. 2, p. 36. Alianza
Editorial, Madrid (1998)
6. Acevedo, M.: Integración de las tecnologías de la información y comunicación. Asignatura
pendiente de la cooperación, p. 84. CONGDE, Madrid (2006)
7. Ramos, F.: Redes Sociales en el entorno radiofónico: el uso de twitter como fuente
periodística en la Cadena SER. Revista Mediterránea de Comunicación. vol. 4, n° 2,
pp. 173–188. Editorial Universitas S.A., Alicante (2014)
8. Wellman, B.: Physical place and cyberplace: the rise of networked individualism. Int.
J. Urban Reg. Res. 25(2), 227–252 (2001)
9. González, M.: Nuevas Tecnologías y Redes Sociales en la Comunicación para la
Solidaridad: análisis de la campaña de sensibilización #StopAblacion. Actas del I Congreso
Internacional Comunicación y Sociedad, p. 358. UNIR. Logroño (2013)
10. Aparici, R.: El documento integrado, en: La revolución de los medios audiovisuales, p. 26.
Ediciones de la Torre, Madrid (1993)
11. Saez, M.: Nuevas tecnologías de la información, movimientos sociales y cambio social, en:
VV.AA. Solidaridad en red. Nuevas tecnologías, ciudadanía y cambio social, pp. 321–326.
Ed. HEGOA, Bilbao (2005)
12. Dupín, M.E.: [Redes sociales: ¿ciudades de vidrio? BBC. Recuperado el 15 de julio de 2009.
http://www.bbc.co.uk/mundo/cultura_sociedad/2009/07/0905281120especialredessocial
esintro_med.shtml?s
13. Domingo, D., Salaverría, R., Cabrera, M.Á., Aguado, J.M.: Convergencia de medios:
dimensiones del debate sobre la disolución de los límites de los modelos periodísticos
establecidos. IV Congreso Internacional de Comunicación y Realidad, p. 218, Barcelona
(2007)
14. Fernández, L.: Investigar en tiempo de crisis… y de redes. Notiweb, Boletín de Weblogs
MadrI+D. Recuperado el 08 de agosto de 2017. http://www.madrimasd.org/informacionIdi/
analisis/opinion/opinion.asp?id=37289
15. Lopes, P.: Nuevos patrones de integración social. El uso del Facebook y el Twitter en
adolescentes y el impacto en su autoestima. Revista de Investigación en Psicología Social,
vol. 1, n° 2, pp. 51–67. Facultad de Ciencias Sociales de la Universidad Nacional de Lomas
de Zamora, Bueno Aires (2015)
16. Sued, G.: Pensando a Facebook, una aproximación colectiva por dimensiones. En: Piscitelli,
A., Adaime, I., Binder, I. El proyecto Facebook y la posuniversidad: sistemas operativos
sociales y entornos abiertos de aprendizaje, pp. 59–69, Ariel, Barcelona (2010)
1064
V. Galarza-Ligña et al.

The Interaction Gap: From the Bit
to the Resurgence of a New Information
and Communication System
Carmelo Márquez-Domínguez(&), Nancy Ulloa-Erazo,
and Yalitza Therly Ramos-Gil
Research Group Media, Applied Technologies and Communication
(METACOM), Pontiﬁcia Universidad Católica del Ecuador -Sede Ibarra
(Pontiﬁcal Catholic University of Ecuador), Ibarra Campus, Jorge Guzmán
Rueda Ave. and Aurelio Espinosa Pólit Ave., Citadel “La Victoria”,
100112 Ibarra, Ecuador
{camarquez,nulloa,ytramos}@pucesi.edu.ec
Abstract. The necessary revision of the concept of information forces its
constant historical contextualization. The different stages of the international
system of information and communication (ISIC) attest to this, through theories
and theorists, socio-political and economic events and media realities. Through a
methodological analysis of qualitative research (grounded theory), it is proposed
to express new contemporary periods of ISIC, as well as its main characteristics
and a possible point from which to begin an academic reﬂection and discussion.
The result of the interdisciplinary analysis is summed up in a transition of phases
that gives rise to “interdumbre”, which is a phenomenon of information
structuring - characterized by a non-communicative interaction - that resurfaces
as international information changes its course.
Keywords: Interdumbre  Interaction  Information  Communication
International system
1
Introduction
The essential role that communication plays - and, by extension, information and
knowledge- is, besides evident, essential for the comprehension of contemporary
reality in all aspects of life. It is a vital element, in the same way, for the progress of any
type of collective organization (public or private) in today’s society, the unstoppable
“information and knowledge” [1]. Effectively, the information process measures and
forms the base of all the folds, contours and environments that surround human action.
For this reason, communication is considered a common denominator of the diverse
and differing ﬁelds of social and human sciences [2]; as well as what is communicated -
the information - as it crosses the boundary towards the natural and biosocial sciences,
condensing the juncture and synthesis points of the wisdom of nature and society. The
social structure becomes an indirect reﬂection of the communication system [3] and the
information, determined by ideals, groups and technological progress in modernity, of
social and economic forces. Consequently, it adopts a meaning in society as a system of
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_101

communication and modernity, a process of change and social differentiation [4] that
introduces a new cultural behaviour.
The obligation to historically contextualize is made palpable at the moment of
reﬂection on the (today, quasi-sacred) notion of information as an object of analysis.
Situating the global information system in time and space inherently describes in detail
the sociocultural and economic reality, its production models, evolutionary - perhaps
Lampedusan- behaviours of subjects with an industrialized spirit [5], its unusual and
novel ways of interaction, or measures of ﬁction and reality (post-truth).
In light of the facts, it is now possible to afﬁrm that the different forms of com-
munication [6] are given at the same time and as never before: on the one hand, as a
process of coding and decoding between senders and receivers, provisions of channels
and media for the efﬁcient transmission of the information product; and, on the other, as
a process of interaction, social practice and meaning construction. The quantitative
measurement that is extracted from the bit and mathematical theory [7], which deﬁnes
the transmission capacity of communication technologies, is not at odds with the
qualitative nature of information (exchange of information, the social paradigm, power
structures) [8]. The quantity of information processed in a message has to do with
unique symbolic codes, whether it is entropy or not [9]. “Human nature was being
translated rapidly into information systems that would produce an enormous global
sensibility and no secrets. As always, mankind was not aware of the transformation”
[10]. Thus, we arrive to the current moment, in which it is postulated that the “universe
is only quantum information; it is not composed of matter or energy but of informa-
tion” [11], establishing a transdisciplinary bridge between social sciences and nature.
The transdisciplinary combination is—and has always been—the key idea to address
the information system.
The future will be based on the information processing of security data, arranged in
two facets: “the quality of information content and identity as a security code” [12] and
“Big data, a term proposed by NASA researchers Michael Cox and David Ellsworth,
who understood it as an information storage problem,” conveying the contemporary
use of technology that changes behaviour and outlook; a battle in the hands of distinct
actors such as China, NASA, and Google [9].
2
Above All, Information
The Altamira cave paintings, the Inca Quipu record, art in all its forms of expression,
books, news products from newspapers, television and radio, DNA, the Big Bang, a
child’s smile, the grafﬁti in Medellin, refugees vs. borders, or the bourdon of a ﬂa-
menco guitar in Jérez de la Frontera. All of this is information, it is measurable and it is
signiﬁcant.
However, reaching these assertions took many time. Academic and theoretic
reﬂections start from a praxis. Thus, all research, such as the present, has its origin in
the description and analysis of social change. Speciﬁcally, in regard to information,
hypotheses and paradigms began to arise in the beginning of the twentieth century,
when media started to develop, mainly in revolutionary American society. The rela-
tionships between information, production and consumption are vital to understand the
1066
C. Márquez-Domínguez et al.

reality of the collective media, which constitute a strategic problem in development and
social change; problems of modernity that give way to myths of modern communi-
cation: progress, freedom, peace [13], transparency, control and power [14].
In the wake of epistemological advancement, a system of international communi-
cation was simultaneously being developed, more speciﬁcally, during the Space Race
between the two main world powers, after World War II. The planet and its system of
bipolar international organization was based on the surveillance and control of the
“other”. Information and communication was involved in all creation and any design of
the structures and inﬂuences in international relations. Also, in the task of outlining a
chronicle, it is important to not forget some international actors that have changed
abruptly during the last century—institutions like the Organization of the United
Nations Educational, Scientiﬁc and Cultural Organization (UNESCO), the European
Commission, multinational and nongovernmental organizations that arrived to the
international debate lead by the States, their principal characteristics and the study of
the information system itself covering distinct areas such as economics, politics,
sociology, culture and jurisprudence, always coming from different levels of analysis.
2.1
Steps of the International Communication and Information System
Certainly, the world of information and communication is one of the spheres of reality
that has changed and developed most quickly. The scientiﬁc and technological revo-
lution has changed everything, from the moment of production (based on constant
innovation), to the nature of interpersonal relations, sustained now by globalized
information networks, without forgetting daily life of work and play. But, until arriving
to hyper-modernization [15], there was an entire twentieth century in which the
international system of communication and information developed in ﬁve stages [13]:
• Historical Optimism: In 1949, engineer and mathematician, Claude Eldwood
Shannon, formulated a mathematical theory of communication, converted in a
paradigm, followed by the human sciences. It is not surprising, through theoretical
and practical realization, that the receptor of the era was subjugated to the emitter
around a rupture between communication and culture. In North American society,
an important paradigmatic representative of this phase, culture was only a fragile
and slight term [16], and therefore it was internationalized. For the moment, the-
oretical reﬂections on the arrival of the society of information and knowledge were
not perceived. The emergence of a new world order came about after the Second
World War, with its losers and its winners. Thus, a search for peace began, which,
through the international system of communication, seemed easily achievable.
There lay the vision of jubilant optimism of the role of communication. Liberalism
had won and, by the hand of the United States, would establish an international
system of communication based on the principle of free ﬂow of information,
“essential to achieve that longed-for peace and freedom.” However, information
continued to be subject to the State in this international system of communication,
until the 1970s.
The Interaction Gap: From the Bit to the Resurgence
1067

• Public Diplomacy: World order1 was more than a fact and the United States, a
great winner of the barbarism in Europe, aimed to dominate the international
communication system in the Space Race with telecommunication satellites.
Likewise, the 1960s is a period in which the United States wished to participate in
the international panorama, sharing and dialoguing. A stage of diplomacy, order
and peace in the West where mass media was a key instrument used to achieve, in
addition to defending the American way of life, a suitable foreign policy strategy,
closely linked to national security. The media began to be conceived as powerful
tools that, of course, must be controlled, while still allowing the principle of the free
ﬂow of information from the previous period to prevail. In this context, discourse
had already begun on the “information age” and the “information society”, a fun-
damental fact that later acted as the basis for information economics2.
• New world order of information and communication: In the 1970s, world peace
after the Second World War, brought about by the United States and its allies, and
through international institutions such as those aforementioned, would began to be
questioned by the Third World or peripheral countries, which would also achieve an
unprecedented victory in UNESCO3. The main problems reported can be summa-
rized as an international system of asymmetric and unidirectional communication
(from the north to the south or centre to the periphery) and, as such, there existed an
incapacity for the autonomous development of their culture and technology, pro-
voking a deep technological dependence on certain areas of the world. At this point
in history, the ﬁrst intergovernmental conference on international communication
was held (San José, Costa Rica, 1975), with the goal of overcoming the weakness of
underdeveloped countries against the enormously manifested sovereignty of the
United States. This signiﬁed the materialization of the ﬁrst global discussion of
communication, today practically missing from the memory of the dependent
organism of the United Nations.
• Liberal Reaction: Since the 1980s, the international system of communication and
information has returned to the ﬁrst period, characterized by an optimistic vision and
the principle of free ﬂow of information, possibly because of a global privatization of
telecommunication companies and, more than probably, because of the failure of the
McBride report. In summary, this signiﬁes the hegemonic return of the United States,
represented notably by the alliance of the Administrations of Margaret Thatcher and
Ronald Raegan who, undoubtedly, deﬁned the twentieth century. The new global
system of communications came about in part due to this conservative neoliberalism.
1 Permit the notion of the term “order” in substitute of “peace”, as in the majority of studies the latter is
used.
2 The balance signals that, in 1967, information represented 46% of the brute national product of the
United States and 53% of wage bills [17].
3 The McBride Commission (One world, multiple voices: the ﬁght for information), tried to ﬁnd
solutions to information problems of the world, demanding a new international economic order, more
than just a new world order of information and communication against the geopolitical and cultural
dominance of the West. UNESCO also intervened to democratize communication. It is important to
remember that this fact provokes a historical abandonment by the United States and the United
Kingdom of the aforementioned international institution.
1068
C. Márquez-Domínguez et al.

• Program for the Global System of Information: In 1993, the program of the
United States, promoted by Al Gore4, deﬁned what would become the new global
system of information (GSI), that is, an interdependent and integrated system in
which American dominance is overwhelming. There were and are two positions on
the globalization of information: those who see an opportunity for openness and
connection, democratization and diversity; and those more critical, who understand
that behind this exists a process of ideological inﬂuence, with the objectives of
standardizing and homogenizing society.
3
Methods and Methodology
Research processes that generate knowledge imply that the researcher collects, codiﬁes
and analyses data simultaneously, as a methodological, systematic and interpretive
process, typical of the qualitative paradigm [18]. The theoretical triangulation has been
framed in the method of analysis and speciﬁc techniques of the grounded theory to
present the data [19] that aim to identify the basic social processes (BSPs) as a focal
point of the employed method [20].
The possibility to observe social events is a difﬁcult task that requires to spend time
in many dimensions of the social sciences as well as the use of the scientiﬁc method.
Efforts by brilliant minds, who have placed exact theories on the world stage, give
reason for reﬂection5. This research, with no ambitious claim, subscribes to explain the
processes that surround the meaning of the present object of study.
The data collection is based on academic-scientiﬁc experience in university insti-
tutions. Since March 2015, there have been endless discussions and debates within the
Department of Social Communication of the Pontiﬁcal Catholic University of Ecuador,
on the Ibarra city campus. In order to reﬂect on the phenomenon of information as
unﬁnished knowledge, it was necessary to rewrite an important point of the discussion,
identiﬁed within the included phases in the theoretical framework and the argued
relevance. Thereby, the exhaustive revision of the concept of interaction that invisibly
affects the way to approach certain theories is expressed in detail - not just presented.
In order to arrive at this theoretical documentation, intensive seminars on theories
(both previous and present) were given, to identify isolated concepts and ideas. Five
scientiﬁc articles were written, which reviewed different literary ideas on emerging
information theories in the knowledge society; data was taken from the results of a
doctoral thesis on the theories of communication6; many meetings were held to
compare the concepts on the development of the background attributed by theoretical
insight; the systemized information was declassiﬁed in printed and digital books,
multiple web pages, essays, class preparations, and monographs, bringing meaning to
4 The concrete document was titled Agent for Action.
5 The brilliant mind referred to is John Nash, who won the Nobel Prize in Economics in 1994 for his
theories on games and to Vlacko Vedral, for his enormous contribution Decoding Reality.
6 Doctoral thesis titled, Study of an organizational model of communication for public and private
institutions and NGOs in Imbabura province.
The Interaction Gap: From the Bit to the Resurgence
1069

the data; and, lastly, with the creation of a Media, Information Technology and
Communication group, researchers in the communication sciences and social sciences
came together, discerning the concretization of ideas as methodologically viable and,
through interdisciplinary learning, developed the proposed hypothesis (Table 1).
In order to arrive at the theoretical proposal, strategies of collecting, categorizing,
coding and data analysis were used simultaneously and, as such, generated the theory
referred to as the new phases of the international information system. The technique
used through two types of observation - structured and in a team- allowed data to be
collated with the chronological systematization of the historical communication para-
digms, without reaching a determinant framework of the currents of thought of modern
and postmodern social science paradigms.
4
Geomatics of the New Information and Communication
System
Following the above analysis of the ﬁve stages that determine the dimensions of the
evolution of the international information system, which are subjects of study in this
present research, we need to shift focus to the current period, the context of which is
identiﬁed through complexities and diverse social structures that determine the beha-
viour of the subject.
Table 1. Methods used for this research. Prepared by the authors.
Concepts
Terms as identiﬁed objectives of study, in this case:
Interaction: The gap left in the disciplines by mistaking that all orders and
processes of interaction communicate
Information: Understood as an isolated concept, distinct from the uniqueness
of information in the paradigms and scientiﬁc methods
Category
A classiﬁcation created after comparing the concepts and integrating them into
a higher order
Coding
The processing and analysis of the authors’ data, characterized by different
schools, thoughts and tendencies
Hypothesis
Interaction is a concept that alters the current international information and
communication system, based on the performance of the smallest information
exchange of the historic subject
Proposal
Interdumbre: For this research, this concept is understood as a phenomenon of
the structuring of information that behaves in phases and emerges as changes in
the New International Information and Communication System. The NSIIC
analyses the social, technological, computational and sociological connections
for the creation of different institutions in a society as a kind of information
structuring. When society advances from one phase to another, such as the ﬁve
mentioned in this study, (acknowledging the existence of many others), little is
understood by the cognitive confusion of believing and learning in diverse
paradigmatic realities that the interaction “communicates” to us. This is what is
understood as phase transition [21]
1070
C. Márquez-Domínguez et al.

The current reality is, without a doubt, the result of a sociological phenomenon
interfered by globalization, technologies of communication and information, modernity
and the survival demands of an information umbrella, which drives the thought process
in small towns. The so-called parents of communication had to rediscover communi-
cational practices. As such, Bateson, the inspiration of the Palo Alto school, after the
1950s, proposed the new communication and systemic approximation; Ray Bird-
whistell introduced language gestures; Edward T. Hall set the basis for an anthropology
of space; and Erving Goffman constructed a species of grammar of interaction that
regulates social relations. They were authors who created private research; however,
they were connected by the same goal: to analyse social phenomena, or the new
communication [22].
Precisely, to speak of the present means to enter into (post-modern) processes,
cultural practices, use of new media and, in this context, constitutes taking on and
understanding the new information production, where there are no longer consumers,
but interacting parties, who do not conform to being part of data and source symme-
tries. On the contrary, they require an integration into the informational correlations. In
such case, the introduction of a new interdisciplinary research brings with it a
historic-social context, whereby the analysis cannot be isolated from the dimensions of
the paradigms of information and communication. This thus makes a further devel-
opment in the direction of a more comprehensive theory necessary, which sees within
the dimensions of reality the possible social interrelations and interactions. “Mattelart
observes that the fascination with theories of hegemonic social control has prevented
the Marxist analysis of the processes of popular class-based struggles to change
communicative and informative structures” [23].
The urgent need to identify the new international communication and information
system implies the behavioural development of critical thinking that cannot be dis-
placed by the new technologies of information and communication, as societal
advancement depends on the productivity of man. The promotion of the era of
“management of knowledge” should transcend to the “construction of knowledge”,
precisely because it has a social dimension and should regain collective and shared
knowledge. “The Theory of Cognition says that in order to form critical thinking,
human beings must be equipped with intelligent processes; it is, therefore, inevitable
that learning cannot be decontextualized from knowledge and in accordance with social
interaction” [24]. The connotation of a new concept of social interaction gives rise to a
realm of possibilities that signify a nucleus of information, leading man to reinvent
himself out of his own worldview. Meanwhile, communication and information cannot
remain implicit, nor superﬂuous deﬁnitions of being a response or stimulus, but must
be understood as elements of a communication process that lies precisely in cyber-
netics. Therefore, they go beyond the limits of one’s own inﬂuences because they
converge in the new dynamics of systematic and cultural interaction, which signiﬁes a
rediscovery of new ways to behave and understand the world.
The new phase proposed, then, enters into a theoretical current of social interactions
and deepens into the sociology of actions and human behaviour. “From the study of
man begins a series of movement in the problem: it is the ﬁrst step towards the
rehabilitation of the subject, a new approach to questions about identity” [25]. From the
deﬁnition of the term ‘paradigm’, which is always set as a pattern or model within a
The Interaction Gap: From the Bit to the Resurgence
1071

system of rules that establish limits or borders; the traditional societies that, with the
industrial revolution, globalization and new technologies of information and commu-
nication, position themselves in other cultural and media practices.
Paradigms help to understand the perceptions of the world and drive the tools of
social and human development. Although the information society has been introduced
into the thinking of man as the era of great transformations at the level of information
and communication development, the cyber age has exclusively conquered science.
Without a doubt, it crosses through a millennium in which the culture of paradigms has
turned to the thought and desire of freedom of expression. Thus, the information system
has strategized to immerse itself in the new environment of global society. “For
Horkheimer, the need for a critical anthropology that reinstates the human being in its
historical place transverses the emancipation of the almost autistic subjectivism in
which the individual has been conﬁned to consumption and intellectual positions -
including metaphysics, art or theology - have propelled human beings towards their
liberation” [26].
5
Interdumbre: Non-communicative Interaction (Reﬂection)
As of today, we can postulate two new phases: from 1993 until 2001, synthetically
characterized by a continuation of the globalization process, also informative; and,
from the desert of reality7, which produced an unprecedented, authentic psychological,
cultural and communicational shock. In all environments of reality and study, the world
changed forever. It is not a surprise that virtual aspect cognitively reproduces the
imaginary meaning of non-communicative actions.
After 1993, a framework of substantial changes in the information processes was
created, generating a clear difference between modern and post-modern societies. It
was then when Marshall McLuhan’s ideas brought about different facets that are now
studied by various authors. One aspect is the planetary globalization (the winning
ideology) that the world undergoes, analysing the ﬁeld of communication from the
perspective of material evolution of media and new information technologies. Pre-
cisely, McLuhan predicted modiﬁcations of communication and information at the
level of interaction; this being an important, but not integral, element that generated
response from some societies which, facing overlapping power, will over identify- if
the term is permitted- the facts, mixing them into the discourse. From there, the creation
of the “Macluhian” global village is addressed, not only by the implications of man in
society and his cultural development, but also by the accumulation of symbolisms that
begin to unfold from the new tendencies that mark the media and its reality.
McLuhan’s media ecology was the result of awareness and, therefore, constituted a
metaphor in which the message represents the medium [27], as well as the domain itself.
It is precisely this theory that led to new perceptions of the information economy and the
accelerated pace of technological innovation that contrasted with neo-capitalism, where
7 That is the title of the book of the polemic, contemporary philosopher, Slavoj Žižek, founded on a
discursive critique following the fall of the Twin Towers. It is also a phrase by Morpheus, a known
character of The Matrix, who leads the resistance in a global post-war period.
1072
C. Márquez-Domínguez et al.

the latter dominated the modes of consumption and the behaviour of man. The
neo-capitalist system forced the creation of more technobureaucratic realities and the
people’s participation became a utopia. Likewise, a series of movements in the ﬁeld of
communication was exhibited, which was articulated by a character of the media, with
alliances, concentration and absorption by the hands of large conglomerates, main-
taining control of the message, up until today.
Completing this ﬁrst period, it should be noted that the registration of a series of
approaches towards new production models, and in this dimension, the media, are
connected to other economies from the ﬁgure of different information companies
seeking to harmonize their capital with new transmedia narratives, characterized now
by, yes, instantaneity. However, after the terrorist attack in New York, information has
been transferred more than just instantaneously, and what was once understood as
interaction; now is versed in the notion of “real time” and a “world without screens”, in
which the virtual and real world intertwine.
Citing modernity vs. post-modernity means to study new concepts of objectivity,
precisely because postmodernity is constructed in a world “of dark screens”, where
information is a labour of world peace. The technique predominates over science and
opens, in such, a distinct global vision of understanding information processes and
structures. The scientiﬁc expansion becomes a necessity for which a branch of struc-
tural changes in formal education originates, and therefore, the era of information in
which inevitably man should produce signiﬁcant knowledge to integrate himself into
the new tendencies of understanding. These approaches have led to other debates in
which the orientations of the theoretic, scientiﬁc and cultural explanations collect new
emotional values that also give rise to a new populism, based on post-truths - typical
lies -, with the goal of de-globalization. It is important to not bypass the new evolution
towards an artiﬁcial intelligence that will, undoubtedly, be able to connote love.
It is time to explain the new phase in which computer interlinking is identiﬁed. Large
databases have come onto the scene, with an accumulation of information surrounding
social, personal and collective interests. Within this phase, people no longer only seek
responses or colloquial dialogs; it is no longer enough to make interaction possible by
speaking in similar codes, but rather they must become interactive. In this new society,
people want to be informed not only on generalities, but also on speciﬁcs and unique
topics; they want to overcome levels of information that only serve to make the subject
react; and be engaged more than communicate. They want to, though they cannot, because
interference levels do not only lie in understanding the environment, but must reach a
self-realization from critical thought. At the same time, this conﬁguration becomes a
strategy to confront innovation and the incidence of postmodernism. It is about the new
social interaction that is equivalent to a symbolic interactionism of communication and
information, a communication gap, a non-communicative interaction, the phenomenon of
interdumbre as a new paradigm that still must be approached and deﬁned by social science
researches. It is not just a consequence of learning, but an implication of the discursive
action for the construction of messages and concepts from a shared experience. In this
reality, the storage of data becomes obsolete, as it should overcome the archive stage
through the advancement of real-time information. Thus, other languages can be consid-
ered, as referred to by the reality of big data the describes the evolution of structured or
semi-structured data for the conﬁguration of new information.
The Interaction Gap: From the Bit to the Resurgence
1073

Ultimately, people with more informative and communicational arguments can
make decisions, self-educate, demonstrate propositional attitudes towards their own
informational nature, and make use of the rights and liberties of humankind. And yet,
however, dangerous tendencies exist for - more than citizens - users, who are (pas-
sively) interacted, informed, and communicated - more than are interactors, informants
and communicators -, leading to passive and uncritical thinking. The same idea
revolves around the scope of information that booms as available processes, with which
the phrase, “everything is on the Internet”, is made absolute. This meaning is dele-
gitimized because what is not found on the Internet, for the time being, is the power of
discernment and the interpellation of ideas for the rational use of content. This is,
therefore, the new stage, called “interdumbre: non-communicative interaction”, which
leads us from the void of information towards the construction of value judgments and
consensus criteria in dialogical dimensions. This is the “daily bread” of Twitter.
References
1. Sakaiya, T.: Historia del Futuro. La sociedad del conocimiento. Editorial Andrés Bello.
Santiago de Chile (1995). books.google.com/books/about/Historia_del_futuro
2. Bateson, G.: Espíritu y naturaleza: una unidad necesaria (avances en teoría de sistemas,
complejidad y ciencias humanas). Bantam Books (1979)
3. Pascuali, A.: Comprender la comunicación, 4ª edn. Monte Ávila Latinoamericana, Caracas,
Venezuela (1990)
4. Luhmann, N.: Sistemas sociales: lineamientos para una teoría general, vol. 15, p. 157.
Anthropos Editorial (1998)
5. Enzensberger, H.M.: En el laberinto de la inteligencia: guía para idiotas (2006)
6. Fiske, J.: Television Culture (1987). books.google.com/books/about/Television_Culture
7. Shanon, E.: A Mathematical Theory of Communication, vol. 27, pp. 379–423, 623–656,
July, October 1948
8. Díaz, D.S., Zakhi, M.: Innovation in Business Models Based on Data: The ‘Big’ and the So
Much (2015). www.cid.uchile.cl
9. Ramos-G, Y., Domínguez-Márquez, C., Ulloa-Erazo, N.: Big Data y Cronotopos como
modelo de negocio en la prensa (2017) www.academia.edu
10. Macluhan, M.: The Global Village. Transformations in Life and the Global Media in the 21st
Century, p. 13. Oxford University Press, New York (1995)
11. Vedral, V.: Decoding reality, The universe as quantum information Buridá Libraryn,
pp. 15–40 (2011)
12. Sierra, F.C.: La Guerra en la Era de la información (2003). redalyc.org/pdf/297/29700314.
pdf
13. Caballero, F.S.: Elementos de Teoría de la Información, Madrid (1999)
14. Foucault, M.: Vigilar y castigar: nacimiento de la prisión. Siglo XXI (1990)
15. Lipovetsky, G., Charles, S., Moya, A.P.: Los tiempos hipermodernos. Anagrama (2006)
16. Carey, J.W., Kreiling, A.L.: Popular culture and uses and gratiﬁcations: notes toward an
accommodation. Uses Mass Commun. Curr. Pers. Gratiﬁcations Res. 3, 225–248 (1974)
17. Mattelart, A.: Historia de la sociedad de la información. Paidós, Barcelona (2007)
18. Del Valle-Rojas, C., Nitrihual-Valdebenito, L., Mayorga-Rojel, A.J.A.: Estrategias de
investigación cualitativa, p. 156 (2012)
1074
C. Márquez-Domínguez et al.

19. Cuñat, R.G.: Aplicación de la Teoría Fundamentada (Grounded Theory) al Estudio del
proceso de la creación de Empresas (2007). http://dialnet.unirioja.es/servlet/oaiart?codigo=
2499458
20. Strauss, A.L., Corbin, J.: Basics of Qualitative Research: Greounded Theory, Procedures and
Techniques. Sages Publications, Newbury Park (1990)
21. Vedral, V.: Decoding reality, The universe as quantum information. Buridá Libraryn,
pp. 95–108 (2011)
22. Marc, E., Picard, D.: La interacción social, cultura, instituciones y comunicación. Ediciones
Paidos, Barcelona (1992)
23. White, R.: El signiﬁcado de los adelantos recientes en el campo de la comunicación masiva
(1987). redalyc.org/articulo.oa?id=31610204.pdf
24. Resnick, L.: Cognición y aprendizaje (1996). raco.cat/index.php/anuariopsicologia/article/
viewFile/61324/
25. Ramírez, M.: Introducción a los estudios culturales Armand Mattelart y Erick Neveu.
Revista Comunicación (4ª ed.), Barcelona (2004)
26. Muñoz, B.: Actualidad de la Teoría Crítica (2009). dialnet.unirioja.es/servlet/articulo?
codigo=3988686
27. Strate, L.: El medio y el mensaje de McLuhan (2012). infoamerica.org/icr/n07_08/strate.pdf
The Interaction Gap: From the Bit to the Resurgence
1075

The Inﬂuence of New Technologies
on University Radio in Ecuador
Ana Culqui Medina(&) and Elizabeth Granda Sánchez
Research Group Media, Applied Technologies and Communication
(METACOM), Pontiﬁcia Universidad Católica del Ecuador,
Sede Ibarra (Pontiﬁcal Catholic University of Ecuador), Ibarra Campus,
Jorge Guzmán Rueda Ave. and Aurelio Espinosa Pólit Ave.,
Citadel “La Victoria”, 100112 Ibarra, Ecuador
{amculqui1,megranda}@pucesi.edu.ec
Abstract. University radio is a source of knowledge, a space for expression
and healthy coexistence between professors, students and society, searching for
feedback and, as such, active participation. Additionally, the huge advancements
in technology have permitted various Ecuadorian universities to have their own
radio transmitter through the Internet. In this article, 45 indicators have been
taken to analyze the digital platforms of each medium and therefore determine
the inﬂuence of new technologies on University radio.
Keywords: University radio  New technologies  Digital platforms
Digital communication
1
Introduction
When speaking of what radio has meant to society over the years, University radio
emerges, considered a fundamental aspect of broadcasting in many countries. However,
a few years ago in Ecuador, stations emerged that responded more to the needs and
interests of the students, being useful for the entire university community. The new
information technologies and their associated uses have allowed an explosion of the
radio and its contents linked to knowledge and entertainment [1].
This research has used indicators that contribute to the creation of a study model for
the virtual platforms of university radios, applicable in any sound environment, and has
achieved in this sense that every radio consumer be a microniche in himself, deciding
what to listen to and when to do it. In order to analyze this renewed phenomenon of
university radios, the sense of the University must be understood as well as that of the
citizens, so that they can form their knowledge and skills to improve society. In this
sense, radio becomes the way through which any citizen who has enough curiosity can
continue to learn and grow throughout life. Additionally, university radio takes the
essential step to reach students and the entire population, content to learn more [4],
thanks to new technologies.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_102

1.1
Communication and New Technologies
In 1960 Marshall McLuhan already predicted the power that technology would have in
the ﬁeld of communication, considering it an extension of man. From that point of view,
he spoke of the immediacy conceived through new media. The new electrical tech-
nology extends the instantaneous treatment of knowledge through an interrelation [1].
The concepts that have been offered on the new information and communication
technologies are varied, but they all consider them to be technical instruments that
work around information and the innovations that are arising. The accelerated advance
of technology brings with it a change in the archetype of the communication process
through the media; a change in culture that proposes another bidirectional reality,
resulting in equal interaction through social networks. These new communication tools
generate a debate around the web revolution originated with the new semiotic discourse
2.0 that includes access to information, photographs, audio, video and chat in a single
package [2].
Social networks, through personal, institutional or group proﬁles, allow people to
maintain relationships independently of where they are and becomes an ideal space for
people to keep in touch with each other. There are authors who establish categories
according to the mode of operation, purpose, degree of openness and integration in the
networks.
The Era of New Technologies
Not too long ago, information and communication technologies were inaccessible in
order to address and generalize relevant changes in diverse areas of behavior and
human activity. Currently, there is now access to a signiﬁcant set of sustainable and
sensible technologies that enable us to carry out different activities that strengthen our
communicative and expressive, industrial, cultural, and recreational possibilities that
were not known to be possible until recently.
What characterizes the current technological revolution is not the centrality of
knowledge and information, but the application of this knowledge and information to
knowledge generation and information processing/communication devices, in a
cumulative feedback circuit, given between innovation and its uses. The feedback
generated between the introduction of new technology, its use and its development into
new territories occurs much faster under the new technological paradigm [3].
Digital Communication
If in the 1980s the largest change was social, today it seems to be technological [4].
ICT and society go hand in the values they give their products, the consumer processes
they unleash, and the links established with other technologies, called the sociotech-
nical network [4]. This novel form of communication opens many doors to the users. It
gives them the tools to not only to express themselves in writing, but allows them to
use interactivity; to communicate with the world.
The digital age extends man’s opportunities for growth in the ﬁeld of research,
thanks to search engines and information. A site comes to life and is developed so that
users are involved and actively participate. Digital communication becomes a key
aspect in daily life, arising as an effect of new technologies that are introduced rapidly
into the ﬁeld of communication.
The Inﬂuence of New Technologies on University Radio in Ecuador
1077

1.2
The Internet and the Evolution of the Radio
The radio is a traditional means of audiovisual communication; the product of a service
that is integrating all its technological potential to the Internet and to digital technology,
constituting an electronic means of communication with a large technological capacity
and global coverage. By adopting the characteristics of an electronic form of com-
munication, the radio has regenerated its productive system, supporting the dynamic of
the digital economy; giving form and strength to a new system of radio production,
based on the notion of technological innovation.
The Internet demands a new form of communication: new genres and formats [5],
production strategies and different forms of relationships with listeners and users of
radio content. Additionally, the appearance of multiple Internet-based radio channels
has created an unprecedented reach in terms of the quantity of users around the world.
Undoubtedly, we are faced with a transformation of radio media that allows the
multi-diffusion of content, and puts the radio in front of a major challenge caused by
technological advances, based on its adaptation and use of the environment, versus a
broadcast and simultaneous listening time. Time and space disappear so that the
audience has the option of an active and critical consumption, provided accurate
information is available.
1.3
University Radio
The University, as a center of knowledge and innovation production, is the protagonist
in the analysis and application of technological tendencies that enhance our society. It
is in this way that the radio becomes a way to continuously inform students and the
general population, by way of content that permits productive learning, constituting a
paradigm in which knowledge and training go hand in hand [5].
The University radio moves with us through technology, the ﬁnal radio product
being in the mind of the users. As they are public service stations, University radios
may be purely interactive and experimental. The network offers the possibility to break
time and space barriers, increasing interactivity and new communication opportunities.
University Radio in Ecuador
University radios in Ecuador must be placed within a scenario of digital conver-
gence, as the birth of a new radio dimension is evident, introducing new audiences and
using the social media revolution as a tool of sound content interaction and distribution
[5]. This is so true that today, radio can no longer be perceived without social networks,
due to the challenge and urgency to contribute science and knowledge to society, as
engines of progress and social transformation [6].
These stations offer a communication alternative in the media landscape of
Ecuadorian society, as the radio has always been a known media source and is capable
of serving as a loudspeaker for young people, researchers, teachers and even for other
members within or outside of university communities. Besides giving students the
limelight to discuss their concerns and hobbies, the radio also allows them to acquire
communication and professional skills that will serve them in their future development
and goals.
1078
A. C. Medina and E. G. Sánchez

The Ecuadorian radio, due to its accessibility, has been chosen not only by uni-
versities [6] but by various education centers, as a tool to connect the world of com-
munication, as it is a privileged tool to acquire reading habits, critical attitudes,
teamwork, and is even used as a distraction and reinforcement of self-esteem.
2
Methodology
This research aims to identify the inﬂuence of new technologies on the 25 University
radios in Ecuador, starting from the hypothesis that in the virtual platforms of these
media sources, the parameters of effective digital communication do not apply, and
thus inﬂuence the feedback and interaction of the media with the listener.
Through the analytical method and scientiﬁc observation, the amount of Ecuado-
rian University radios with a virtual platform have been identiﬁed; based on this ﬁrst
parameter and a qualitative character frame, an analysis sheet was created in which 45
total indicators were applied, formed by the following blocks: navigation, content
structure, multimedia, hypertextuality, interactivity and connectivity, making up the
principle characteristics of digital communication.
Each one of these indicators has been subdivided and has been assessed in the
following way. If the virtual platform in this sub indicator complies in an optimal way,
it is valued over 2, if it is basic or medium-level it is valued at 1 and if it is devoid, it is
valued over 0. For the ﬁnal result, an evaluation scale is used for the entire virtual
platform, considering 73–90 points as excellent and 0–18 points as insufﬁcient.
The principle objective of this work is to investigate the use of virtual platforms by
these channels to determine their grade of communication efﬁciency and how the new
technologies have disrupted Ecuadorian radio.
2.1
Deﬁnition of the Sample
For the effects of the ﬁrst diagnostic, it is necessary to determine how many Ecuadorian
universities have a radio and how many of them have a webpage. From this, it was
determined that 25 online radios exist, which we will now present.
3
Results
The results obtained in the application of the indicators based on the parameters of
digital communication will hereby be presented, preceding a block analysis and global
study.
3.1
Navigation
In navigation, new parameters exist; the ﬁrst makes reference to the easy access to
content, which is evidenced by the fact that the large majority (21 radios) of digital
radio platforms present their services without complications. In the same way, in the
parameter comprising an adequate URL, it is determined that 75% of the platforms
The Inﬂuence of New Technologies on University Radio in Ecuador
1079

meet this parameter. Similarly, in the aspect of the identiﬁable links, the platforms
present results indicating a high number of radios with content linking information or
services (85%). However, exceptions exist in which this characteristic is not exposed.
In terms of what corresponds to the correct use of icons, the level is average (50%) as
not all the digital platforms employ the icons in a valid manner, adding to these
observations that they do not provide any use. In terms of what causes the percentage to
decrease even more is the criteria referring to the internal search engine, as a small
number (25%) of radio platforms even offer this option.
In terms of an adequate font size used in the platforms, the results indicated in the
Table 1 show that 17 digital radio sites satisfy this parameter. On the other hand, in
regards to the point about a good contrast between the background and text, the number
is average, due to the fact that these characteristics are not employed in their totality.
However, in the aspect denominated ‘text and visually organized graphics’, 18 online
radios fulﬁlled this characteristic, indicating a high percentage, considering there are 25
radio platforms in total. Finally, in the parameter that indicates if the colors correspond
to the University’s image, the number of platforms that fulﬁll this aspect is 12 radios,
which constitutes a small number.
Table 1. List of Ecuadorian university radios
Name of the University Radio
1
Quantica Radio
2
Radio of Cuenca
3
Skpe Radio
4
Uce Radio Interactive Press
5
Utn Radio
6
Upec Radio
7
Puce-Yes Net
8
Utm Radio
9
Ueb Radio
10 Utc Radio
11 Cocoa Radio
12 Flacso Radio
13 Ute Radio
14 Uta Radio
15 Espam Radio
16 Uda Radio
17 Ucsg Radio
18 Secular Radio
19 Espol Tv
20 Public Radio, Tv and Press Company
21 Espoch Radio
22 We Are Uniandes
(continued)
1080
A. C. Medina and E. G. Sánchez

3.2
Content Structure
In terms of content structure, the ﬁrst point evaluated is the use of ordering tools, which
include indexes, shown in 85% of digital radio platforms to not be adequately visible
and in the other cases, nonexistent. On the contrary, the second aspect referenced is the
use of titles, which is present in 20 platforms, with an irrelevant minority not fulﬁlling
this characteristic. On the other hand, the parameter that calls for the presence of photo
captions exists in a low number of platforms (5%).
However, in the area of participation, this space exists, thanks to social networks
that are linked to these platforms. The displacement to other pages is minimal;
according to what has been analyzed not many platforms automatically direct to other
websites, though a small number do (15%). The same ratio of platforms uses com-
mercials, representing only 3 digital radio platforms, and in an average number of
platforms, radio programming is exposed to users.
3.3
Multimediality
In the analysis concerning multimediality, the ﬁrst point tackled was the use of pho-
tographs, identifying that 13 of 25 radio websites adequately use this platform.
Additionally, 7 use illustrations when presenting their services or content, thus making
it clear that the rest do not use these graphic resources. In terms of the use of texts, 12
platforms fulﬁll this aspect and more than half present ﬂaws when publishing infor-
mation, as it is either nonexistent or not current. While speaking of the use of videos,
10 of the internet radios completely fulﬁll this option. On the other hand, the use of gifs
is null, on all websites analyzed. However, online audio is present in the majority of the
websites in a high percentage (90%) and ofﬂine in an average percentage (50%).
Regarding podcasts, 10 platforms offer this option to its users. A view counter can be
identiﬁed on only 4 pages, leaving behind the other 20 that do not present this data.
3.4
Hypertextuality
Within the analysis on hypertextuality, it is apparent that 15 digital platforms have
internal links on their spaces, which indicates that more than half (65%) utilize links
within the content they present to users. Meanwhile, 14 of 25 websites use external
links, which demonstrates that a medium-high percentage uses links (65%); both
internal and external in a similar proportion.
Table 1. (continued)
Name of the University Radio
23 Click Radio
24 Upse Radio
25 University Radio - Loja Academy
The Inﬂuence of New Technologies on University Radio in Ecuador
1081

3.5
Interactivity
Interactivity may be understood as the chat option, which is present in a low number of
digital platforms, as only 5 provide the facility to communicate through chat. The
following point refers to participation in forums, blogs and comments, which Click
Radio, Espol Tv and Public Radio utilize, leaving behind the 23 platforms that do not
have this option. On the other hand, 16 websites make their transmission online and 15
of these radios offer downloadable material. Likewise, users cannot share content on
only four websites; the majority do offer this possibility. Additionally, more than half
of the platforms have a help page that facilitates listeners with the access to information
or the use of radio products.
3.6
Connectivity
Focusing on the aspect of connectivity, a large number of digital platforms (85%)
actively post their content or transmit their programming on social media sites.
However, there are also platforms that have no social media presence.
To conclude in a general matter, a graph is presented in which Click Radio,
followed by PUCE-YES Net, UTM Radio, Cocoa Radio, Flacso Radio, Espol, UTE
Radio, and University Radio - Loja Academy present the highest number of analyzed
aspects, which were: navigation, content structure, multimediality, hyptertextuality,
interactivity and connectivity, achieving a grade between 70 and 90.
4
Discussion and Conclusions
In this study, it can be determined that University radio is intimately connected to the
Universities. Barrios tells us that these means of communication should constitute a
potential tool for young people and, especially, for academia, having a principal goal of
including educational themes in juvenile contents [7]. However, this concept has not
yet been achieved in Ecuador.
The digital media sources in this study fall within radiophonic typology known as
“institutional services”, being specialized media, with a preselection audience by the
very fact that they pertain to educational groups.
When determining the Internet presence of the radio stations, referring to how the
websites are organized, it was found that 15% of them utilize online broadcasts, rel-
egating all the characteristics and options offered by the Internet (connectivity,
hypertextuality, multimedia, interrelation). Thus, the Internet becomes the means of
transmitting sound waves, just like the radio spectrum, without downplaying the added
value that is an emission without borders.
95% have a webpage and use an online broadcast to host the radio production.
Ecuadorian University websites are being underutilized, without easy usability or
interactivity. The reconstruction of web pages is unavoidable due to the demands of the
digital world, considering that millions of pages are created daily on the virtual plat-
form [8]. Therefore, their communication success depends on how well they are
1082
A. C. Medina and E. G. Sánchez

structured, that is to is say the information architecture can bring knowledge and
expansion to the website or lead to its total disappearance.
Through university radios, we look for horizontal communication; a participatory
communication that permits all groups and people to express themselves, without fear
of repression, allowing for interculturality and pluriculturality. However, it is evident
that only 18% offer active participation for students or the public, as opposed to the
ease of access that websites have, at 94%.
In terms of information architecture, frequencies should take rapid actions to
improve their image, as only 50% of them have a navigation bar, only 44% a search
engine and 81% do not have a site map.
Audiovisual resources are underutilized; 45% do not have a photo gallery, 50% do
have an audio gallery, although all radios should have this tool due to the fact that they
are based on sound and auditory concepts.
69% do not implement neither videos nor animated graphics. If we speak about
hyptertexuality, we also see extremely low levels, which evidences the deﬁciency of
online university radios, with the exception of the creation of speciﬁc offers distinct to
traditional radio.
Online Ecuadorian University radios have an encouraging future when taking the
necessary corrective measures that the Internet offers. Technology transcends bound-
aries, allowing people to create unimaginable things and foster true communication
processes.
References
1. McLuhan, M.: Comprender los medios de comunicación, p. 354. Paidós, Buenos Aires (1994)
2. Lemos, A.: Medios sociales ¿herramientas de la revolución? La Crujía, Argentina (2013)
3. Castells, M.: La era de la información. Economía, sociedad y cultura, Siglo XXI (2002)
4. Scolari, C.: Hipermediaciones, elementos para una Teoría de la Comunicación Digital. España
(2008)
5. Narváez, C.: Las radios universitarias, más allá de la radio – Las TIC como recursos de
interacción radiofónica. UOC, España (2012)
6. Aguaded, I.: Contreras Paloma; “La radio universitaria como servicio público para una
ciudadanía democrática”. NETBIBLO S.L, España (2011)
7. Barrios, A.: La radio universitaria ¿Una mezcla de experiencia, juentud y tecnología?
Barcelona (2014)
8. Pérez, M.: Arquitectura de la información en entonrnos web. Observatorio, 333–339 (2010)
The Inﬂuence of New Technologies on University Radio in Ecuador
1083

The Press in the Context of the Andean
Community of Nations (CAN):
Without Sustainable Monetization
in the Digital Economy
Yalitza Ramos-Gil(&), Carmelo Márquez-Domínguez,
and Aldo Romero-Ortega
Research Group Media, Applied Technologies
and Communication (METACOM), Pontiﬁcia Universidad Católica del Ecuador,
Sede Ibarra (Pontiﬁcal Catholic University of Ecuador, Ibarra Campus),
Jorge Guzmán Rueda Ave. and Aurelio Espinosa Pólit Ave.,
Citadel “La Victoria”, 100112 Ibarra, Ecuador
{ytramos,camarquez,abromero1}@pucesi.edu.ec
Abstract. In the current context of the Andean Community of Nations (CAN),
little has been studied on the monetization of digital newspapers. To analyse this
hypothesis, three member countries of this trade bloc, Colombia, Ecuador and
Peru, were taken as samples, through three digital newspapers. Following a
method of mixed analysis, the observation technique was used to record web
data, interview experts, and develop an observation sheet with inference
parameters. This was based on the technique of content analysis of the sales
strategies of newspaper companies from the digital newspapers “El Comercio”
(Peru and Ecuador), and “El Tiempo de Bogotá” (Colombia). Finally, the
conceptual levels of analysis of interview texts as well as digital literature and a
review of the homepages of the aforementioned newspapers have been included
in the theoretical framework.
Keywords: Digital  Monetize  Andean Community of Nations
Economy  Culture
1
Introduction
The regional press may take on a sustainable economy towards the technological
revolution if they change cultural contexts at the level of editorial integration. For some
experts, using techniques that provoke changes in reading habits is a viable indicator of
information quality; for others, it is a blind effort for a culture that continues to depend
on paper. An uncertain future arises from the challenge of monetizing the use of new
technologies in Latin American newspapers. The Technical Association of Latin
American Diaries (ATDL) looks for ways to guarantee different scenarios from those
experienced so far in printed editions. Open and free digital journals are still alive
because of paper sales [1]. Its future is to generate “niches” or products that culturally
yield the value that is sought after in a hyperconnected world. Six ATDL managers
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_103

urgently plot a new route to monetize the economy from relationships in the new
cultural industry: the meta-media [2].
The emerging start-up companies challenge the old status quo in the quest to
reinvent themselves. While it is true that 2.0 technologies are transforming the spaces
made for interaction and participation in cybermedia [3], it is also true that some
avant-garde media still leaves a certain gap in the knowledge society.
The pillars of the companies, which measure the information economy in the new
technological paradigms, respond to three essential elements: knowledge and its source
of value as a primary resource; the Internet and information technologies; and the
technological transfer [2]. However, it has been insufﬁcient to precede stable models in
the environment of payments and ﬁnancing that guarantees the substitution of the old for
the new [2]. The creation of methodologies intended for the transformation of the
economy of the media in order to make it more proﬁtable is included, among other
things, to a canvas of seven that helps to understand the strategic nature of a business
model [4]. Amazingly, the kiosk has not disappeared from the reader's afﬁnity and mind.
But, who reads, buys, sells, and innovates? Are the digital journals of southern
Latin American countries capable of generating opportunities and evolving business
models
beyond
the
knowledge
society?
Or
will
they
simply
be
based
on
customer-reader knowledge [5], without monetizing their own economy? “Only places
which understand how to generate knowledge and protect it, how to ﬁnd young people
who have the capacity to do it, and make sure they stay in the country, will be
successful” [6].
2
Political Economy of the CAN
Forty-eight years ago, an intergovernmental, supranational organization was created
that consisted of a community of countries (Bolivia, Colombia, Ecuador and Peru),
voluntarily united with the goal of achieving an integral geo-territorial development.
This integration institution of Andean countries, considered as the foundation of the
Cartagena Agreement [7], delimits two cultural aspects in the so-called political
economy of communication. The ﬁrst aspect, maybe inadvertent, is the autarchic
unexpectedness of the old basic premise, “the medium is the message.” Today, we
continue to nod under the shadow of the dominant communication paradigm, that is to
say, under contradictions of looking at societies with media conditioning. Critical
studies on the analysis of media in Andean countries have been frozen in a time capsule
as a basis for admitting that journalistic enterprises, and the entire sphere of the cultural
industry that produces money and impacts society with cultural changes, continue to
ignore the macroeconomic theories of politics and communication. Paradoxically, the
information economy in social means of communication is closely tied to the so-called
political economy of communication [8]. The second cultural aspect in the autarchic
vision to be explained is that the centre of the media macroeconomics [9] ignores the
existing potential of the Gross National Product of Latin American societies [10].
The principal defect has been the reproduction of erroneous policies in commu-
nicative materials and in the lack of conﬁdence to give credibility to the new public
agendas, in both the thematic establishment and the laws thereof [2]. An unequivocal
The Press in the Context of the Andean Community of Nations (CAN)
1085

decision is that the business models of the digital newspapers referenced, located in
CAN member countries, are not part of the environment in the framework of Decision
792 on the implementation of the Reengineering of the Andean Integration System and
does not form a part of the Decision 797, which approved the list of Committees and ad
hoc Groups of the CAN on October 14th, 20141. “Economists such as Robert Theobald,
Rostow and John Kenneth Galbaith, have explained for years why the ‘classic econ-
omy’ was incapable of explaining changes or growth” in different countries of the
world [11].
2.1
From the Digital Economy to the Real Economy
The digital or web economy is knowledge based clearly on technology [12]. Its
inﬂuence and acceptance have been so incredible that it has transformed the way in
which we communicate, consume information, offer products and services and tran-
scend from the world of play on Earth to virtual reality. It has revolutionized, along
with other aspects, the current ﬁnancial services. Reasons exist to believe that digi-
talization is a phenomenon of economic information that has permitted business models
to be understood by attributes coming from the knowledge of client-reader [5]: the
treatment and use of informative content; ease; storage and speed of volumes of
information; multimedia expansion; the large reach of multiple products and services;
and, lastly, the immediacy of information with a socio-global impact.
2.2
The Monetization of the Press in CAN Countries
The paradigm of web monetization in CAN countries is far from materialization due to
the fact that there are no studies or research that question the processes used until now
and that ﬁt the traditional economy to new models [13]. In the digital literary review,
hardly any pilot projects were found, and almost all of them were isolated and in early
development stages [14]. Monetization is far from the issues of integrationist economic
policies in Andean countries through CAN, nor is the theoretical application of
intertemporal inconsistency and monetary policy [9] intended for interaction and
participation in cybermedia. The challenge is in ﬁnding how to monetize this digital
circulation. The solution to this question is in the minds of managers and editors in
Latin America, not exclusively in CAN countries. The hassle is looking at the market
with the creation of paywalls, and charging for information quality. To arrive at this
point in an integral manner, the idea of a guild of regional press exists, to unite digital
newspapers and adapt them to technological disruptions. Each newspaper company
would consider how to reinvent itself in the daily labor process through credible
information to minimize fake news circulating on the Internet [1, 2]. After achieving
these strategic standardization processes in the region, competition between large world
newspapers on the Internet would greatly increase. Although it is clear there is still
much to do to arrive to the monetization and positioning of digital press, Latin
1 Text that has been analyzed from the response to the interview question given via Skype to the
General Secretary of CAN in an email dated July 21st, 2017.
1086
Y. Ramos-Gil et al.

American media is convinced that the printed newspaper is going to remain important.
Its only goal is to gradually change business models with digital alternatives [2].
3
Methodology
The methodological study is organized by the method of mixed analysis. The research
design composes the current context [15] of the digital newspapers, “El Comercio”
(Ecuador and Peru) and “El Tiempo” (Colombia), which represent the sample of three
web-based newspapers in CAN countries [16].
The work is organized by interviews, direct observation [17], and conceptual cat-
egories of analysis that register qualitative data with eighteen indicators [18] of the
research through the elaboration and application of a descriptive model sheet for the
collection of data that determines the underlying logic of the applied media economy
for the aforementioned newspapers.
Data is classiﬁed in agreement with the technique of content analysis [19] in four
subcategories of the main theoretical framework that deals with observation as a
strategy or business model. To avoid the error of direct observation by the interference
of the observer, data has been ﬁltered by a rigorous scale of validity, trustworthiness
and measurement.
3.1
Data Collection: The Interview
To determine the coding rules and analysis categories [20] of the descriptive ﬁles, a
textual analysis was developed after the interviews. The in-depth interview was carried
out directly with the General Editor of the newspaper “El Comercio”, Marco Arauz, in
May 2017. The interviews conducted with the Director of the ATDL by the “Expreso”
are newsworthy starting from the press declaration. The veriﬁcation of the commu-
nicative messages in the interview text has been classiﬁed with keywords, referring to
the objectives of monetizing newspapers. This analysis has been developed by the
number of times that the interlocutors repeat the word ‘monetize’ in the conversation.
After quantifying the act of speech at the conceptual level, it is contextualized with the
intention of operating it through pragmatic inference [21]. Part of the analysis has
already been incorporated into the monetization section of the theoretical framework.
The result of this process is intersected with the analysis of the categories and indi-
cators of fact sheets to arrive at an interpretation of the results on the inference of
underlying economic logic.
3.2
Direct Observation
For the systematization of information designed to “formulate, from certain data,
reproducible and valid inferences that may be applied to the context” [18] of the CAN,
a model sheet was designed which categorizes four business strategies, based on
eighteen conceptual indicators aimed at obtaining indicators of systematic procedures
and objectives of the content description of classiﬁed messages in the payment of
products, digital services, advertising and multimedia (aggregators) to the digital
The Press in the Context of the Andean Community of Nations (CAN)
1087

newspapers of “El Comercio” (Ecuador and Peru) and “El Tiempo” (Colombia), per-
mitting the deduction of knowledge relative to the conditions of production/reception
of social context [18]. The model sheet represented in Table 1 was reproduced for each
CAN country, containing data from the three newspapers analyzed in this research. The
information is classiﬁed in the following manner (Table 2):
• Product payment. The indicator is the package that temporarily promotes digital
media.
• Digitalservice payment. Collectseleven indicatorsfocusedontheserviceoffered and
promoted by the three newspapers, and responds to Internet business strategies that, at
Table 1. Descriptive sheet of data collection. Created by the authors.
Context: CAN
Category: Client-Reader
Inference: Underlying Economic Logic
El Comercio Newspaper – Ecuador
Business
Product
payment
DIGITAL service
payment
Advertising
Multimedia payment (Aggregators)
Indicators
Unit or
package
1. Audio-visual:
triple or multiple
pay
1. Sponsorship
Customer loyalty platform (offers free
content to capture customers for paid
services)
2. Download
2. Open access
3. Packages
3. Bartering
4. Subscription
4.
Merchandizing
5. Streaming
5. Localization
of native
product
6. Paywall press
7. Micropayments
8. Sale of loose
items
9. Mediated
payment
10. Membership
11. Club
Date
Hour
Time
13-09-2017
12:00–
13:00
1 h
Monthly
and
annual
packages
offer
1. Multiplay permits
users to review a news
item without reading
it, reducing their
consult time, and
making it easier for the
newspaper to get more
customers
1. The “El
Comercio” group
sponsors cultural and
sports activities.
Through these
events, the
newspaper
consolidates its
image
The service
allows
downloading
aggregators
through Really
technology
1088
Y. Ramos-Gil et al.

the same time, are oriented in the tactics of experimentation and rationalization of
digital business. These indicators are described as multiple play, ﬁle downloads,
product packages, digital newspaper subscriptions, streaming, paywall press, micro-
payments, sale of loose items, mediated payment, membership and club afﬁliation.
• Advertising. There are ﬁve indicators of digital marketing through sponsorship,
open access for the catchment of customers-readers, bartering, merchandizing, and
the localization of native product.
• Multimedia payment (aggregators). Its focus through the platform identiﬁes
strategic alliances at the editorial level as a free service.
3.3
Technique of Content Analysis
The indicators were extracted from the interviews and from a literary review. The
indicators were recorded from an arrangement of linguistic acts at the conceptual level
(Table 3).
Table 2. Systematization of data collection. Created by the authors.
Dates
Observation hours Consumption time Newspaper
Country
13-09-2017 to 17-09-2017 09:00–10:00
30 h
El Comercio Ecuador
13:00–14:00
15:00–16:00
09:00–10:00
30 h
El Tiempo
Colombia
10:00–11:00
14:00–15:00
16:00–17:00
10:00–11:00
30 h
El Comercio Peru
11:00–12:00
15:00–16:00
17:00–18:00
Table 3. Selection of indicators within the analysis categories. Created by the authors.
Analysis categories: indicators
Newspaper Product
payment
Digital service
payment
Advertising
Multimedia payment
El
Comercio
(Ecuador)
Offers
monthly
packages
Indicators
developed: 2,
4, 5, 10, and 11
Indicators
developed: 1
and 2
The service permits to download
aggregators through Really Simple
Syndication (RSS) technology to
computers. Thus, readers are
informed in real time and the
newspaper reaches a certain degree
of loyalty
El Tiempo
(Colombia)
Monthly,
annual and
bi-annual
Indicators
developed: 1,
4, 5, 10, and 11
Indicators
developed: 1,
2, 4, and 5
El
Comercio
(Peru)
Monthly
and annual
Indicators
developed: 1,
4, 5, 10, and 11
Indicators
developed: 1
and 5
The Press in the Context of the Andean Community of Nations (CAN)
1089

3.3.1
Analysis: “El Comercio” Newspaper (Ecuador)
“El Comercio” uses the multiplay mode, which permits users to review a news item
without reading it, reducing their consult time, and making it easier for the newspaper
to get more customers. The Internet allows consumers to access news in an unlimited
and free manner. Additionally, it allows users to evaluate each article in regard to their
mood and the effect that its content has on them.
When subscribing, the reader obtains a virtual card that gives him or her access to
diverse beneﬁts in exchange for a fee. The webpage has a streaming service, with the
goal of offering information immediately and interacting with digital readers. Mem-
bership comes with beneﬁts and discounts in health services, gyms and various others;
15% for newspaper collections and 20% per word in classiﬁed advertisements. The
section of the subscribers’ club allows members to obtain information immediately and
access diverse products of the “El Comercio” group. In this way, the newspaper may
segment its products and offer them according to one’s preferences.
The “El Comercio” group sponsors cultural and sports activities. Through these
events, the newspaper consolidates its image in the various target and age groups
through sponsors with a pronounced social emphasis in an effective way. The Inter-
net allows one to access unlimited news freely. Also, with just a click, the user
evaluates each article. Therefore, in an internal way, the newspaper measures the
reader’s mood and the effect that its content produces.
3.3.2
Analysis: “El Tiempo” (Colombia)
The option to reproduce texts, audio and videos allows an interaction with the reader
and, in turn, obtains a return of immediate information. By subscribing, the reader
accesses beneﬁts such as discounts on books, restaurants, travel, cultural and artistic
events, and health.
The “El Tiempo” Group has a deal with various companies that allows it to cap-
italize on their agreements by way of offers for subscribers. The streaming service
offers an affordable product through infographics, videos and live broadcasts. The user
can also access a radio platform and audible news. When paying for the membership
service, a reader can access the beneﬁts of the newspaper. By subscribing to the
Vivamos Club one automatically becomes a partner, along with his or her family
members. This club promotes the Loyalty Program of Casa Editorial El Tiempo,
composed of the newspaper subscribers, Portfolio and the Magazines: Don Juan, Aló,
ABC of the Baby, Habitar, Hola, and Bocas, and the Journals: Boyacá Seven Days and
Diario Llano.
To access these beneﬁts, each partner is identiﬁed by a personalized card, which
opens the door to discounts, invitations to movie premieres, theater productions, gifts
and certain experiences the newspaper promotes. Each subscriber also receives special,
unique beneﬁts.
The “El Tiempo” Group sponsors cultural and sports activities to guarantee loyalty
with its diverse targets. To access news articles, the webpage asks for registration. This
permits the newspaper to capture and learn about the habits of its new readers and
increase its number of potential subscribers. Products are offered via banners on simple
visuals and are easily accessed. The webpage allows advertisements to be visualized
through static banners and the user can access discounts for a product he or she buys.
1090
Y. Ramos-Gil et al.

3.3.3
Analysis: The Newspaper “El Comercio” (Peru)
The option to reproduce text, images and sound allows an interaction with the reader
via chat. At the same time, it allows for a return of immediate information.
The subscription allows one to access beneﬁts in a user-friendly manner, including
discounts for restaurants, cultural events, and others. Also, once a reader is registered,
he or she may participate in promotions and rafﬂes. When subscribing, the reader
should select the district where he or she lives and may geotag his or her location,
which will allow a categorization of a sociocultural status, delivering products in a
direct way. The streaming service increases new users and facilitates an interaction with
digital subscribers.
With membership, one obtains beneﬁts of the newspaper “El Tiempo” in Colombia,
as well as access to consumption invoices in the “El Comercio” Group. Likewise, one
may access a catalogue where diverse technological products are offered. This permits
the generation of advertisement revenue and strengthens the image of the newspaper
for readers in the area. To have unlimited access to news articles, the webpage asks the
user to register, allowing it to capture and identify the habits of its new readers.
4
Results and Conclusions
The “El Comercio” Group (Ecuador) uses its webpage with the idea of offering
information immediately, using various techniques and digital tools (such as a
streaming service and RSS). The subscription modality ‘advanced payment’ - with its
wide range of beneﬁts -, allows the newspaper to maintain loyalty with its users and
follow user preferences, based on the most quoted products. In terms of accessing news
articles (in an unlimited way), the model differs from other media sources, such as “El
Comercio” in Peru and “El Tiempo” in Colombia. This modality allows the newspaper
to substantially increase its number of potential subscribers.
Additionally, in the speciﬁc case of “El Comercio” in Ecuador, the webpage does
not allow comments on its news articles, but offers an evaluation marker by way of
icons, representing diverse moods. In terms of its advertisements, it was observed that
on September 16th, 2017, the webpage displayed an advertisement that covered almost
all information, making it necessary to either close the banner or read the advertise-
ment. “El Comercio” also has a tab where various websites of institutions that form part
of the Media Network of Latin America are listed, and through these links one may
access the principal channels. “El Tiempo” in Colombia and “El Comercio” in Peru
occupy more web space on their home pages and hyperlinks to promote subscriptions.
With all the aforementioned aspects based on the three newspapers, the actions
carried out in the past year in the CAN countries have deﬁned new strategic guidelines
and prioritized the areas of action left out of the framework of the Decision 792, a
deliberation on business inclusion of the media. The three digital media sources studied
promote subscriptions through literary product strategies, such as magazines, to gain
reader loyalty. They mostly incorporate Internet-based business strategies, through
The Press in the Context of the Andean Community of Nations (CAN)
1091

micropayments, membership and club afﬁliations, scarcely using inserted advertising.
Payments for consumption or streaming are from a free multimedia press. The
experimentation and rationalization they have are gamiﬁed and direct sales are small.
With all this considered, one could say that they do not have business models. The
strategies and value creation of the press in the CAN countries indicate that they are not
prepared for economic monetization. The Latin American region aims only to continue
gaining reader loyalty, something that does not differ from the large digital media
sources of the world.
References
1. Ramos, G.Y.: Entrevista de Profundidad sobre Modelos de Negocio. Gerente-Editor Marco
Arauz. Diario El Comercio. Ecuador-Quito. Realizada el 8 de mayo (2017)
2. Zumba, L.: La Prensa en la región escribe un Nuevo futuro. Diario El Expreso. 30 abril
(2017). www.elexpreso.com
3. Casani, F., Rodríguez J, Sánchez, F.: New business models in the creative economy:
emotions and social networks (2012). www.redalyc.org/articulo.oa?id=43323186003
4. Osterwalder, A., Pigneur, Y.: Generación de modelos de negocio (2011)
5. El Mundo: Fernández-Galiano: ‘El modelo de negocio de la era digital se basará en el
conocimiento del cliente-lector’ (2014). http://www.expansión.com
6. Cortés, D. et al.: Todos los imperios del futuro serán imperios del conocimiento (2017).
http://ingenieriaeconomicauni.blogspot.com
7. Blanco, C.: La Comunidad Andina en el Marco de lo Jurídico y Político. Revista
Prolegómenos - Derechos y Valores, pp. 173–188 (2013)
8. Mosco, V.: La economía política de la comunicación: una actualización diez años después.
Cuadernos de información y comunicación 11, 57–59 (2006)
9. Branson, W.: Teoría y Política Macroeconómica. Fondo de Cultura Económica, México
(1981)
10. De Gregorio, J.: Macroeconomía. Teorías y Políticas. Santiago de Chile (2012). www.
degregorio.cl/pdf/Macroeconomia.pdf
11. Marshall, M.: Comprender los medios de comunicación. Paidós, Barcelona (1996)
12. Cepal.: Economía Digital (2013). https://www.cepal.org
13. Carrión, F.: Procesos de descentralización en la Comunidad Andina. Flacso Ecuador (2013).
www.ﬂacsoandes.edu.ec/libros/digital
14. CAN: Documentos oﬁciales (2016). www.comunidadandina.org
15. Del Valle-Rojas, C., Nitrihual-Valdebenito, L., Mayorga-Rojel, A.J.: Estrategias de
investigación cualitativa, p. 156, Abril 2012
16. Riveros, E.: Ranking: Diarios impresos más inﬂuyentes de América Latina en su versión 2.0
(2014). http://www.hufﬁngtonpost.com/
17. Díaz, L.: La Observación (2011). www.psicologia.unam.mx/…/La_observacion
18. Abela, J.M.: Las técnicas de Análisis de Contenido: Una revisión actualizada (2012). www.
public.centrodeestudiosandaluces.es/pdfs/S200103.pdf
19. Noguero, F.: El análisis de contenido como método de investigación. En-clave pedagógica,
[S.l.], vol. 4, ISSN 2341-0744, Jan. 2011. www.uhu.es
1092
Y. Ramos-Gil et al.

20. Piñuel, J.L.: Epistemología, metodología y técnicas. del análisis de contenido (2002). www.
anthropostudio.com/…/José-Luis-Piñuel-Raigada
21. de Cabeza, L.M.: Lingüística y Discurso. Ediciones de la Facultad Experimental de Ciencias
Sociales, LUZ-Venezuela (1985)
The Press in the Context of the Andean Community of Nations (CAN)
1093

Ecuador, the Non-communication: Postdrama
or Performance?
(The Chaos Order in Ecuador’s Communication
and Culture)
Miguel Ángel Orosa(&) and Aldo Romero-Ortega
Pontiﬁcal Catholic University of Ecuador, Ibarra Campus, Ibarra, Ecuador
{maorosa1,abromero1}@pucesi.edu.ec
Abstract. When it comes to tackling the main difference between “postdrama”
and “performance” based on architectural and esthetic criteria, that is, whether it
is possible for the “order” (logos) of both (posdrama and performance) to be
inside or outside of the artistic work, we (de)construct the two key features of
the term “postdrama”: the “order” and the “chaos” of postdrama, namely where
those patterns lie and what they are based on. These variables allow us to go
more deeply into the identity culture of Ecuador, especially into its (non)com-
munication, using the postdramatic categories that are speciﬁed in an informal
model resulting from these inquiries.
Keywords: Postdrama and performance  Ecuador identity
(Non)communication  Order in chaos
1
Introduction: “Postdrama” and “Performance”
In our view, the main difference between “postdrama” and “performance” [1], (if we
understand the latter as any esthetic action associated with “improvisation” or lacking
any kind of “order” or artistic organization looking to some extent for a “form” or an
internal single movement), even if the ﬁrst of them, that is “postdrama”, is heavily
inﬂuenced by the features of the latter, would be that “performance”, if it made any
“sense”, revolving around a reason that boosted its existence, would be placed outside
the “esthetic and organizational” or architectural scope of the “dramatic” play.
Meanwhile, in the ﬁeld of “postdrama” (despite its development in the “chaos” world),
we will always ﬁnd the reason or its “reason” (logos, “order”), the sense of its
architectonical-dramatic organization, within the esthetic space in which the sense is
developed. In other words, we will ﬁnd it inside the “dramatic” discourse or devel-
opment in which we operate and, besides, it will have an esthetic incentive, even
though this “sense” or “reason” would show in many cases a minimalist approach. Of
course, all this has to be done whether that “postdrama” has or can have non-esthetic
inﬂuence, derivations or impacts, such as the common ones in “performance”, in media
or environments different to the purely artistic ones, as for example in political, ethical,
ideological, religious, social, philosophical or simply psychological spaces.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_104

All the movements that are somehow linked to what we call “performance” would
develop their existence in that “dramatic and presentative” hallmark space that we
would call “chaos”, due to the lack of at least a minimum overview in its constructive
or architectural building. We mean here that the “reasons” explaining the existence of
these movements or of each of their plays would or could be of a potential political
nature, for instance, or generic (not “logical-technical-formal”) esthetic dogmas. These
reasons could also seek to produce certain reactions in the spectator (external to a given
architecture within a play), or they would refuse certain artistic approaches typical from
the dramatic tradition. Nevertheless, there would not be a minimum “constructive
reason” inside each of their plays, that is, any kind of organizational essence of the
whole building that claimed for a synthetic architectural “sense”, that is to say, that
looked for an effect in that speciﬁc play and within the play itself, an effect resulting
from its “fact organization” or from each of its sections inside that play, which we are
tackling in the following paragraph.
In fact, “postdrama” [2, 3], as we conceive it, would show “typical features and
phenomena” that could perfectly be considered as “notes” or “variables” of an “open
model” in which this trend takes place. In other words, its manifestations and
parameters would live within the chaotic world of water, that is, the “nonsense” and the
“parataxis”, regardless of whether we could ﬁnd certain “patterns” in that world, since
in the ﬁeld of art they are not necessarily constituting an “order”. However, there could
also be a higher or lower rate regarding “sense”, reason (logos), or “order” (within
“chaos”), revolving around the concept of “composition” or at least of a “fact orga-
nization”, the latter being based on “techniques” coming from the “dramatic” phase.
Those techniques are variables typical from the organization of the “plot”, the “dra-
matic development”, the “dramatic tension”, the “narration” and its inner “provisions”
to the dramatic progress, or the architectures of a mere “technical-formal” nature. When
talking about a higher or lower rate regarding “order”, we refer to the “higher or lower”
participation or presence of the updated “dramatic compositional techniques”, or to the
use of “tools” arising from the “drama” phase, as well as the stylistic ones within the
“postdrama” general constitutive framework.
2
Method
With regard to the identity culture of Ecuador, with its notes and features within those
belonging to the “postdrama” or seen from a postdramatic perspective, the method to
use in this sense is crucial and essentially “empirical” and “observational”. On the one
hand, the fact of having a day-to-day direct and intense contact with this country,
Ecuador, as well as with its Latin and Andean culture from long ago already and, on the
other hand, the fact of having studied its culture from many perspectives makes it easier
to gain access to its identity in terms of “postdrama” and “performance”.
When notes from “postdrama” are described, these derive also from observations
linked to this artistic phenomenon and that are based on the plays that establish and
revolve around this trend in Europe and Latin America. Regarding the chaos “order”,
the “sense” of “postdrama”, we use both an analytical and a concise model to build a
Ecuador, the Non-communication: Postdrama or Performance?
1095

“dramatic space” that we could call “common” for the whole western world and that
would include many tools of regular use in this phase of dramatic hallmark.
3
The “Postdrama Chaos”
When it comes to the features and patterns of “postdrama” [4, 5], we consider that these
variables belonging to “chaos” arise and start from a chronotope typical from this
artistic trend, that is to say, exactly in that “place” and at that “moment” where this kind
of situations are told: we refer to a point in which there is no time nor space. It is as if
those situations were told from within our mind, and all that takes place there, that is,
meditation, is rather mental. Moreover, in some cases it is as if we reached a meditation
attitude and we were at a place beyond reason: consciousness, that is to say, a divine
gaze or position outside the emotional world. If we take a look at the following
features, they will be, from this perspective, undoubtedly much more logical and
understandable when associating them with the “lens” they arise from.
The “features” or “patterns” [6] of “postdrama”, which we can ﬁnd in the perfor-
mance of the plays of our time (we refer here to drama plays, not to their presence in
television ﬁction – television series in the United States – or in movies, as they also
contain these postdramatic features), could be summarized, without being exhausting,
in some of the following ones, since “postdrama” is not a “model”, less still a closed
model, but rather a question [7]:
– Lack of chronological or linear times, or spatial marks typical from the dramatic era.
– There are no meta-narratives, but small life fragments which are so often
juxtaposed.
– Tendency to live kairos moments and times, that is to say, this suitable and
appropriate time lapse, the “present” time, which is strongly vital and only lasts
until it is replaced by a new time.
– Suspension of textual autarchy, which shares its importance with other show
dimensions.
– The concept of “reality” is questioned, being understood from a univocal point of
view, as if reality was the emotional world within our grasp and the closest one to us.
– Visual and multidisciplinary standpoints regarding the show.
– There is usually a single theme treated during each play, being divided in several
sub-themes.
– The establishment of the characters has nothing to do with the traditional procedure.
They usually have a fragmented personality, as if a mirror was broken in many
pieces and each of them reﬂected different aspects of its personality, not necessarily
fully coherent with each other. They are often made of papier mâché.
– The dialogues lack a logical or causal structure that is appropriate to the situation or
debate the characters are supposed to be experiencing.
– There may be conditional stage settings, that is to say, developments in potential
future worlds.
– The plays are composed of fragments not necessarily fully coordinated nor related
with each other.
1096
M. Á. Orosa and A. Romero-Ortega

– Juxtaposition and tendency towards rhizome, multiplicity, disintegration.
– No rational understanding of the plot, the scenes, or the full play; no immediate
understanding.
– There are no necessarily hierarchical gazes.
– Tendency towards meditative liturgies, towards an “enough is enough; there is a call
to something or someone that might listen to our questions”.
– Tendency to see everything at the same time or a collage.
– Tendency towards
simultaneity of disciplines and events, and towards a
non-hierarchical horizontal approach of the different disciplines on stage.
– Tendency towards multiplicity and not towards unity or synthesis.
– Non-representative scenes between text and scene.
– Search for a sensory communication with the spectator.
– Reﬂection on the world, in an open sense.
– Unplanned process and result in the text. The truth and exclusivity of the word are
questioned.
– Stage directions are introduced as textual material. Landscapes and characters are
closer to a poetic or symbolic image rather than a potential staging.
– There are micro-conﬂicts or micro-scenes, as well as discontinuous series of tension
moments.
– Saying the opposite of what it seemed before.
– Scene repetitions.
4
The Chaos “Order” in “Postdrama”
Watching a play belonging to the theatrical “postdrama”, which is the ﬁeld analyzed in
this article, the spectators will observe the notes and features pointed out in the pre-
vious section. Depending on the culture, country, company and author, the spectators
will also notice sensations, emotions or an “order” release, to a greater or lesser extent,
within the play architecture. This section focuses mainly on researching where the
chaos “order” comes from [8], which is also the “postdrama”, the “organizational
sense” of its nonsense. As we have said before, we do not refer here to potential
“senses” that are external to the “postdrama esthetic organization” itself, such as
political reasons, declarative artistic refusals, manifestos, dogmatic or undogmatic
motivations with a destination outside the work of art. We refer here to the inherent
order in the play architecture, that is, in a potential internal organization to the play
itself [9].
Without any order, there is only a mere juxtaposition of ideas. This may happen to
some authors who are passionate about juxta-positive conglomerates of ideas, sensa-
tions, images… But for many other authors there are “techniques” supporting or
protecting the simple “irrational” discourse typical of the “postdrama features”, so that
the architecture of this tendency can enjoy a certain “order”. Thus, only the most
important ones are mentioned below. When it comes to the “dramatic organization”
[10], we have to say that the three dramatic contexts (exposition, conﬂict and resolu-
tion) can serve to introduce a certain order in the life of postdrama, but always
Ecuador, the Non-communication: Postdrama or Performance?
1097

adjusting to our current philosophy [11]. Regarding the “dramatic tension” techniques,
these are developed around two pillars throughout our Western tradition: the “conﬂict”
on the one hand and the “activation” on the other. The latter is made of several factors
such as the “characters”, the “stylistic discourse”, the “spectacular discourse”, the
“intellectual discourse”, or the “music”, as already highlighted in the “Western canon”
and, speciﬁcally, in Aristotle’s Poetics [12]. In terms of the “technical and formal”
tools, the most abstract tools for the “dramatic organization”, they would be comprised
of a number of elements such as the “situation” of the speciﬁc scene within the
dramatic time-space (that is, from the beginning to the end of the play), the “complexity
of the formal drawing”, the “intensity” with which any dramatic element is empha-
sized, the “duration” of the dramatic elements in scene, the dramatic “coloring” and
“textures” in the framework of the play concerned, the dramatic “rhythm” and tempo,
the dramatic “plot” considered in its strictly formal dimensions, etc.
Well, these elements we refer to (the distinctive elements of the “plot”, the “dra-
matic organization”, the “tension”, the “narrative” techniques within the drama, and the
“formal” techniques) would give meaning and their presence to a greater or lesser
extent to the apparent “organizational nonsense” regarding “postdrama”. These ele-
ments would in some way constitute the “order” of the postdramatic chaos to which we
refer in this article and their emergence would represent the main difference between
the concepts of “postdrama” and “performance” previously stated in the text. The fact
of trying to understand the chaos “order” is both a fascinating task and probably one of
the most complex acts in the postdramatic spectacles already mentioned.
In order to better understand each other and to set out the case quite clearly, we
have to say that there are a great number of “chaotic” elements and variables of
“postdrama”, since “postdrama” is based on the idea of “juxtaposition” and not
“composition”. We described some of these elements above in the “postdrama chaos”
section. Besides, these “chaos” variables generally give birth to a “nonsense” feeling
that is to a large extent in the spectator’s mind and that can ﬁnd its origins in the play
itself that the spectator is watching. Nevertheless, in a large number of the authors and
“postdramatic” spectacles we can also ﬁnd traces or minimum tools for “order” that, in
our opinion, tend to give “sense” to the nonsense in the scope of this artistic trend.
These traces and tools become “order” elements within the “postdrama” chaos. Finally,
these constructive “order” elements are the ones described in this section.
This chaos “order” is not about and is not comprised of a series of “patterns” or
“parameters” which we would ﬁnd in a constant way in the middle of the “postdramatic
nonsense”. They are just, but nonetheless important, dramatic elements that are char-
acteristic of the Western tradition. They act at some speciﬁc moments of the “post-
dramatic” presentation, giving the latter what we could call “nonsense, organization,
order, construction”, and not allowing the play to lose its own feature, that is, its
“postdramatic” nature of vast idiosyncratic hallmark.
If we refer to speciﬁc cases that help us understand what we call here the “chaos
order”, let us imagine a group of colors and voices on stage to which we do not ﬁnd any
sense, in a timeless world and without recognizable spaces: this would be chaos (it
would make sense for the play’s creator but it is not perceptible for the others). In a
given moment of the spectacle, these feelings caused by lights that ﬁght shouting on
stage materialize or crystallize, for example, in an enormous “conﬂict” where all the
1098
M. Á. Orosa and A. Romero-Ortega

symbols or disciplines of the presentation take the “drama” action to opposite sides.
This causes a clear feeling of “confrontation”: this would imply giving “sense” to
“nonsense”, and in this case caused by the “agon”, the “conﬂict”, one of the elements
already mentioned above [13].
We turn to the “conﬂict”, an element characteristic of the dramatic tradition, so that
the dramatized “chaos” (the “incomprehensible disorder”) has a pinch of “sense”
(within the play). It is still “chaos”, but now we ﬁnd a recognizable element within
“chaos” that is the “conﬂict”, and then, only then, the “chaos” becomes recognizable.
This “conﬂict” will make a different sense from the characteristic sense of the tradi-
tional dramatic organization; it will not be an obstacle anymore so that the main
character obtains his or her dramatic objective (as in the drama), but it will still be a
“conﬂict” that, in this case, will give order and recognition to the play. However, this
will not be a simple formal or conventional meaning. Quite on the contrary, the
“conﬂict” has, like many of the tools belonging to the drama era, vital or natural roots
in the human being that is quite recognizable as inherent. It is enough to observe the
individual history of the human being, the history of wars or the theatrical dramatic
tradition, as some examples.
Referring to a different speciﬁc case or example that allows us to understand what
we call “the chaos order”, we can think of a “postdrama” plot where a collage made up
of images and ideas would be the plot of a stage play. All these images are uncon-
nected, incomprehensible by themselves (or at least for the spectator) and as a whole in
the case we are explaining here. Without losing this disconnection or juxtaposition of
images or ideas, we can also use a certain number of dramatic elements from the
Common Western Dramatic Space (CWDS) [14] that help us “recognize the chaos”,
that is, “understand” what is happening, the sense’s “nonsense”. We logically use here
the term “understand” from a postdramatic perspective.
Firstly, we start decoding the “cruxes” or “drama twists” (turning points) outside
the plot that could exist among all those collage stories that we have just mentioned,
that is to say, what links could there be among such diverse stories. Secondly, we could
also turn to the external or extradiegetic “narrative” of the play that could explain that
collage but without leaving the “chaos” behind. Lastly, we could use as well, for
example, the “intellectual discourse” (dianoia), one of the elements for the “dramatic
tension activation” that would make sense or give unity to that collage. Let us imagine
that the “intellectual discourse” is about “death”; this dianoia, as Aristotle would call it,
could give sense to the whole “plot collage” without the plot losing its style and
“organization” so characteristic of “postdrama”.
5
Results: The Identity Culture of Ecuador: Postdrama
and Performance?
Ecuador would apparently be a good example of what we could call a “postdrama”
society. Thus, what we state below is like a compliment to Ecuador due to the fact of
being a symptom of belonging to that group of societies that ﬁnd themselves on the
crest of the vanguard. In our opinion, Ecuador could never be a “performance” society
because, even though its culture lives immersed in the “performing chaos” due to its
Ecuador, the Non-communication: Postdrama or Performance?
1099

tendency towards “silence” and “non-communication”, its approach towards its own
“unity”, patriotism, government, “sense”, “order”, etc. saves this country from the
performative abyss. This is the main cause of its “liturgical, mystic and visual” culture
that “dramatizes” its “chaotic” nature, in other words, that gives “sense” to the “non-
sense”, “order” to “chaos” and, thus, places Ecuador on the altar of “postdrama” at the
expense of the “performing arts”, these latter terribly jealous since they are not able to
take over the whole spectacle to which we are admiringly used in this country.
When we refer for example to the term of non-communication, or to what the
Ecuadorians call facilismo (easiness) or quémeimportismo (“I do not care”), to name
just a few (in fact they are just a consequence of their innermost “spiritual path”), we
can conclude that their most characteristic culture is not essentially the “word”, in other
terms, the Aristotelian predicamentum (substance, copula and categories), which is the
basis for the Western reasoning, but rather a different communication culture, old and
new at the same time, whose features have not been given much attention by the fact of
imposing different models to it, including learning models, that were not its own. In this
sense, Ecuador is a culture-country unsteadily and imperfectly (the perfection only
exists in classicism) combining liberty and order, sense and nonsense, communication
and silence, word and emptiness, body and music, spirit and sex, substance in love and
irregular mystic, hated contemporaneity and Greek-Andean liturgy, hunger and
fanesca, breaches due to centuries-old thirst and a sip in love that makes you fall down,
etc., and all this, as it should be, is said with admiration and the utmost respect.
Its culture is Pre-Christian because, even though it enjoys the inﬂuence of the
Christianity, the foundations are obviously Andean, as well as Ecuadorian and
“indigenous”, in the most appreciative sense. The “liturgical, mystical and visual”
culture that is so typical of the country does not accept as natural the European
“reasoning” that comes from Greece and develops itself until reaching the rationalism,
in order to arrive to the phenomenology in the 20th century, the analytical philosophy
or other trends typical of our world. The essence of Ecuador is rather Presocratic but, of
course, it is an “Andean”-style one, leading to the fact that the “spiritual path” or the
mystic of these Andes create an impassable wall for all those who try to impose their
customs, different to the local ones, even if it is a very welcoming mystic. The nearness
to the earth and to the other three elements, that is, ﬁre, air and water, is much stronger
than what we could have imagined. This “mystical and liturgical” attitude is the basis to
the “visual” (what is beyond, metaphysics), and that is the essence of our current
Ecuador: it is as if the indigenous peoples or the citizens themselves were contem-
plating a god from eternity while the ﬂoor is collapsing, and this is exactly the glory
and the basis of “postdrama”. This is also the essence of drama and this is how it is
born as a dramatic “catharsis”, as liturgy, as puriﬁcation, as an aspiration for something
better, etc. However, this, the ideal situation, is much more distant from what is within
our grasp, from our ordinary and well-known emotional world.
Please note that what has been mentioned so far would be the so-called “dramatic”
share of Ecuador, its appeal to “sense”, of course not the rational one (logos), but
mystical and spiritual, and also its containment or paralysis regarding the “performing
chaos”. Regarding the cultural features that would bring Ecuador closer to “perfor-
mance”, or rather to “postdramatic chaos”, to the “chaos” without order or to the abyss
of “nonsense”, that is to say, all those features deﬁning “postdrama” in an experimental
1100
M. Á. Orosa and A. Romero-Ortega

way and that we have mentioned above, they are all closely related to the following
characteristics, extracted from the daily life and culture of the Ecuadorian society:
– The so-called “magical” or imaginary world, which turns reality into something
“fantastical”. In “postdrama” texts, reality is mainly avoided by means of a post-
modern vision that deﬁnitely has an inﬂuence on them. It is not in vain that post-
modernism is a corollary of rationalism.
– The reality that would only be acceptable as an “exaggeration”, that is, as something
grotesque. For example, and we say it with respect, suicidal or impassioned murder
tendencies.
– The fact that behind what is apparent and what we could call luminous (that is, the
smile, the education, the politeness), we often ﬁnd a world full of ruptures,
breakings, fragmentations, tears, hatred, passion, anticlassical revenges, etc., and all
this is often presented in the form of a “parody”. All this leads also into a kind of
liturgical paralysis that is characteristic of “postdrama”. What is parodic also takes
part in this artistic trend.
– The fact that the night has a strong inﬂuence in the lives of the characters and
introduces them into a process of unveiling, even sometimes transformation, in
order to know what they are or what their passages are. Thus, this is a tendency
towards a metamorphosis or deconstruction of characters and situations, and that is
also characteristic of the “postdramatic theater”.
– The characters suffer from pitilessness; they are imbued with great difﬁculty for
forgetting. Such situation paralyzes their souls and places them in thick fog, as if
they were wandering through an ambiguous world placed between Heaven and
Hades, until the revenge arrives and the dishonor dies, allowing the soul to travel to
its own homeland.
– The sense of time and space as an absence or emptiness. Time and space are real
and tangible obstacles. In this manner, everything contributes to a life that we could
describe as liturgical, fantastical, imaginary and contemplative. All this happens in
light of a vital idyll that could perfectly lead to misgovernment.
– The sense of the eternal now, the present time. There is no future; there is only a
kairos time and its inﬂuence in the “dramatic tension is perceptible”, as well as the
resulting lack of linear time.
– The family is an absolute, exclusive and demanding institution. Related to this, we
witness the tragedy or enshrinement of the culture of the mother, belonging to a
pre-Western and pre-Olympic hallmark.
– Conception of what we call freedom from quite a skeptical attitude (tragedy,
destiny).
– The “word” lives particularly through dance, music, the somatic communication,
etc., and that is the liturgical trance.
– Creation of superstructures that distract us from reality and force us to focus on the
processes. It is about avoiding chaos but we are necessarily coming across it
(“postdrama” and “misfortune”).
– Tendency towards the indiscriminate questioning of others and any of their com-
mands and, thus, doom to chaos and “postdrama”. Related to this, we witness the
establishment of strong governments.
Ecuador, the Non-communication: Postdrama or Performance?
1101

– Close relationship with the world of ambiguity, a world never understood as a “lie”,
but as a typical way of life (denial of a tangible truth, relativism, lack of any kind of
obstinate reality).
– The magical ayahuasca that saves you again from a reality, in order to experience
what the real life is, which is not perceptible to everyone, only to those spiritually
suitable. Thus, there is a link to the Andean spiritual path and to the magical realism.
– Denial of self-criticism, not due to the lack of a self-assessment, but because there is
no reality to observe, except for the magical or fantastical realism.
– Liturgical gratitude to what saves us from the inferior, do ut des. Self-interested
deep-rooted brotherhood.
– Celebrating, laughing, crying, etc. as expressive means of the tragedy itself.
– “Narcissistic” vision of the individual life as well as the dialogical experience in the
midst of the current society (very characteristic of “postdrama” too).
– Tendency towards the non-communication through the word.
– Deterministic and fatalistic vision of the whole existence: “we have to do it”.
6
Conclusions
Firstly, we established a difference between “postdrama” and “performance”. This
latter, if it had any “sense”, “order”, or “reason”, would be placed outside the “esthetic
and organizational” scope, as well as the “architectural” scope of the performing
spectacle. Then, the so-called “postdrama”, though its development in the “chaos”
world, would enjoy, in our view, an internal “reason” to the play itself that would be
placed within the “esthetic and dramatic organization”, that is, within the “(post)dra-
matic” discourse in which this spectacle would take place. These logos sections could
easily hold a minimalistic nature, which would also depend on the culture of the play,
on the authors, on the geographical areas, etc.
Secondly, we deﬁned the so-called “chaotic” nature of the “postdrama” phe-
nomenon and, besides, its potential “patterns” or its most signiﬁcant features. As we
have just mentioned, it was in truth about “patterns” or even simple descriptions of
phenomena that we have observed when contemplating the so-called “chaos” (of this
artistic trend). These features will certainly help us “understand” this “chaos”, or at
least partially. This also includes there where “nonsense” can be understood, which
seems a contradiction, or where it is subject to any kind of understanding. Nevertheless,
this potential and open “model” is a mere desire of making our own complex reality
which by its nature, by itself, is elusive. When it comes to searching that pinch of
“sense”, of logos, on the broad map of what we would call “chaos” or, better said, in
the scope of “postdrama”, we pointed out that we should mainly carry out this search
for “sense” in the Common Western Dramatic Space (CWDS) [15]. For analysis
purposes, we divided this in ﬁve variables: “plot” organization, “(post)dramatic dis-
position or organization”, “dramatic tension”, the area of “narrative elements” inherent
to drama, and, lastly, the whole “technical and formal organization”. This piece of
sense within “postdrama”, if present in the given author or culture, could include any
kind of dimension, though it should always have at least minimum but enough
dimensions to be able to “organize” these postdramatic spectacles [16].
1102
M. Á. Orosa and A. Romero-Ortega

By applying these different concepts or categories to the identity study of the
Ecuadorian society, we encounter different “notes” that could have an impact on the
culture and the openness of this speciﬁc Latin American society. Some of these notes
would be included within the scope of the so-called “order”, of the characteristic “sense”
of “postdramatic theater”. However, other speciﬁcities of the same Ecuadorian society
would rather ﬁt or identify themselves better with the “chaos” of the “postdramatic
phenomenon”. Thus, this article was about creating a series of “postdramatic” and
“performing” categories that would enable a “posttheatrical” comparison or analysis of
the Ecuadorian society. This practice will at least allow us to get to know this Latin
American country in a new way and with a new vision. In this manner, we can have a
more contemporary and international perspective of this country. Moreover, this type of
analysis will favor the establishing of some differences within the current “postdra-
matic” knowledge, and they will probably bring some clarity to the nature of this
movement, as well as to the esthetic and organizational nature of avant-garde theater.
References
1. Camilletti, G.: Teatro y performance en la fotografía en teatralidad expandida, libro
coordinado por Julio Elena Sagaseta. Editorial Nueva Generación, Buenos Aires (2013)
2. Carlson, M.: Postdramatic Theatre and Postdramatic Performance. Revista Brasileira de
Estudos da Presença 5(3), 577–595 (2015)
3. Cornago, Ó.: Teatro postdramático: Las resistencias de la representación. Artea. Investi-
gación y creación escénica [en línea, sin paginación]. Recuperado noviembre 15, de 2014.
www.arte-a.org
4. Lehmann, H.: Teatro postdramático. Cendeac, Murcia (2013)
5. López-Antuñano, J.: Tendencias en el teatro europeo actual, Teatrología, pp. 184–201
(2010)
6. Sarrazac, J.P.: El drama no será representado. Teatrología, pp. 92–103 (2010)
7. López-Antuñano, J.: La Escena del Siglo XXI. Asociación de Directores de Escena de
España, Madrid (2017)
8. Bloom, H.: El Canon Occidental. Anagrama, Barcelona (1995)
9. Orosa, M.A., López-Golán, M., Márquez-Domínguez, C., Ramos-Gil, Y.: El posdrama
teleserial norteamerciano: poética y composición (cómo entender el guion de las mejores
series escritas para la televisión en los Estados Unidos). Revista Latina de Comunicación
Social 72, 500–520 (2017). http://www.revistalatinacs.org/072paper/1176/26es.html
10. Alonso de Santos, J.L.: La estructura dramática, las puertas del drama 10, 4–9 (2002)
11. Cabal, F.: Historia y evolución de la estructura dramática. Primer acto 215, 92–99 (1986)
12. Aristóteles (ed.): Poética, trilingüe de Valentín García Yebra. Gredos, Madrid (1974)
13. Sarrazac, J.P.: Léxico del drama moderno y contemporáneo. Paso de Gato, México (2013)
14. Orosa, M.A., López-Golán, M., Márquez-Domínguez, C., Ramos-Gil, Y.: The American
postdramatic television series: the art of poetry and the composition of chaos (How to
understand the script of the best American television series). Revista Latina de Comuni-
cación Social 72, 500–520 (2017). http://www.revistalatinacs.org/072paper/1176/26en.html
15. Orosa, M.A.: El cambio dramático en el modelo teleserial norteamericano. Publicia,
Saarbrücken (2012)
16. Debord, G.: La sociedad del espectáculo. Archivo Situacionista Hispano (1998). http://
serbal.pntic.mec.es/*cmunoz11/Societe.pdf
Ecuador, the Non-communication: Postdrama or Performance?
1103

Media Processes of Communicational
Management, Information Transparency
and the Incidence of TIC
Nancy Ulloa-Erazo(&) and Álvaro Cevallos Ramírez
Research Group Media, Applied Technology y Communication (METACOM),
Pontiﬁcal Catholic University of Ecuador, Ibarra Campus,
Ave. Ajaorge Guzmán Rueda and Ave. Aurelio Espinosa Pólit.
Citadel “La Victoria”, 100112 Ibarra, Ecuador
{nulloa,amcevallos}@pucesi.edu.ec
Abstract. The following article is based on the study and application of pro-
cess maps to measure the effectiveness of communication and information in the
organizations of Northern Ecuador. Meanwhile, a descriptive methodology
supported mainly by analytical, qualitative and quantitative methods is used.
The importance of the systematization of well-structured tasks in the organi-
zational context is assessed. Hence, communication in Ecuadorian companies
has somehow overcome staleness, achieving an innovation in terms of the
development of strategic objectives. Behind this visible change are the infor-
mation and communication technologies that allow the establishment of new
and novel ways of transmitting and receiving messages. As a result of this
research, a proposal is considered for the communicational management by
processes, based on a global reality that leads to new mechanisms for transparent
messages.
Keywords: Process maps  Organizations  Communication  TIC
1
Introduction
Process management in companies of the 21st century has become the strategy for
measuring institutional objectives and an innovative tendency to predict and project
organizational actions. Currently, the terms “quality and evaluation” mark the indica-
tors for development and impact that are generated in the public. In this sense, it will be
necessary to quote “a systematic method of improvement and reengineering, both
applicable to speciﬁc processes. On the other hand, there are management models,
which have a central role as the basis of the organization and as a guide by which the
system of indicators is articulated” [1].
The importance of managing communication from the vision of processes is con-
sidered an operational and functional basis for the determination of effectiveness and
efﬁciency levels, and in the same way, the compliance of assumed commitments. Every
organization must visualize its tasks, achievements and successes; from a reality that
allows it to be recognized in history and in the market. “The image of an organization is
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_105

built according to the perception of its members, users and consumers in the feedback
process, granted by the role of the economic agent in any society” [2].
Ecuadorian organizations continue to be submerged in traditional realities, where
many of the processes continue to be manual and the bureaucratic systems show a static
image. For the respective study, and according to the methodology, this research will
use the case of organizations in Northern Ecuador. It is necessary to reference the
function of transparency and social control that has promulgated since the Ecuadorian
Constitution of 2008, which mentions: “This function aims to promote and impulse
control on the entities and bodies of the public sector, and on individual people or legal
entities of the private sector, that provide services or develop activities of public
interest, so that they do so with responsibility, transparency and equity…” [3].
This regulatory body prefers a process of management transparency, this is why
one of the tasks assumed by the public and private companies is accountability.
However, it is necessary to strengthen these strategies from the beginning, precisely to
evidence sequentially which is possible with the incorporation of processes.
2
Management and Processes
Process management is formed by a set of tasks that are interconnected with each other.
The elements that conﬁgure this vision are constituted by the start and end parameters
of an activity. This speciﬁes a procedure that determines the incidence of actions that
aim to achieve the strategic objectives of an organization. A process map is also
considered a valued diagram; a graphic inventory of the methods used by an institution.
Ecuadorian organizations are undergoing major structural and functional changes. The
very reality of the country has demanded the redesign of a new organizational culture
in recent years. Although the importance of the communicator and the role they play in
an organization is not yet fully understood in Ecuador, in the meantime, it has become
necessary that a communication professional implement process map. In this way, a
new plane of communication is conﬁgured with the emergent, integral and holistic
needs of society; and for this the communicator should be prepared, this being their
main challenge. The institution must also forge from its ﬁeld of competence the
communication and information policies that transcend the outdated and traditional
schemes.
2.1
The Processes and Their Insertion into the Organizations
In the ﬁeld of communicational management, the processes of media intervention are
clearly identiﬁed; on top of being considered the strategic allies of the companies, they
also contribute to the achievement of the organizational culture. It will be necessary,
then, for the company to have a global thinking in terms of the insertion and adaptation
of the processes in all their work. That is to say, they will have to include the public,
products, services, and the very nature of the company. “The analysis of productive
processes within an organization is the current challenge to understand the customer’s
way of production, which bears itself the relation and the need to act as a function of
processes” [4] (Table 1).
Media Processes of Communicational Management
1105

In these 72 organizations studied in Northern Ecuador, the importance given to
communication planning is evident. For the case study, speaking of this topic means
making plans for communication; in some cases media advertising, and, for others, the
relationship and permanent insertion in the organizational life of the media. That is why
it is pertinent that the processes are related to media intervention, as channels that
facilitate the dissemination and transparency of information.
A communication process includes communication elements that reﬂect its inten-
tions, which will have to adapt to the reality of the company. One of the determinant
factors in the study of the country’s organizations is that they have a strategic plan,
many of which include areas of communication, though the tasks are restricted to
dissemination issues. Likewise, they tend to strengthen their institutional philosophy;
that is, they appropriate the institutional mission, vision and values as a phenomenon of
identity. It is necessary to recognize these aspects, to feed the organizational culture.
When asked what sources of media they used or preferred to disseminate both
internal and external information, the organizations in this study responded with:
intranet, press releases, billboards, newsletters, meetings and social networks. This data
reveals that companies have assumed the need to promote their actions through new
media. “The application of information technologies in organizations helps activities to
be carried out in a more segmented way and the communicative capacities improve in
efﬁciency and efﬁciency due to a specialization of the tools directed at the public… The
emergence of technologies has favored interpersonal and inter-organizational com-
munication ﬂows modifying, strategies, objectives and tools” [5].
To consider the new digital environments and social networks a victory is to admit
that all specialties, including marketing, is part of communication. It could then be said
that communication connects other sciences as a hub that carries out multiple functions.
Even so, the various disciplines and theoretical contexts can generate confusion when
analyzing these new ways of understanding communication development. “The ﬁrst
decade of the 21st century has been characterized by a transformation of civil and media
communication that have modiﬁed the models of mass message transmission and have
built a new map of communication channels… the participation of Internet users in
social networks signiﬁes a new way of creating and disseminating information [6].
2.2
The Reality of DirCom
It is necessary to raise the question of the development of DirCom, which is rightfully
obliged to manage communication from information technologies. In theory, the public
is attentive to social media content, digital advertising and media information. Mean-
while, the communication professional should be able to manage from this new
approach of sending and receiving messages. A communicator cannot be one, if he or
she does not identify with the new digital trends.
Table 1. Organizations studied in Northern Ecuador
Public institutions Private institutions Non-governmental organizations
25
43
4
Total. 72 Analyzed institutions
1106
N. Ulloa-Erazo and Á. C. Ramírez

In Ecuador, the importance of professionalism prevails in order to be able to carry
out the role of Director of Communication, even more so when the very Organic Law
of Communication protects, in its Section 42, the right to free communication. “All
persons shall freely exercise the rights to communication recognized in the Constitution
and this Law through any means of social communication. The journalistic activities of
a permanent nature carried out in the media, at any level or position, must be performed
by professionals in journalism or communication… In public entities, the positions
inherent to communication will be carried out by professional communicators or
journalists” [7].
Additionally, the insertion of technology 3.0 is undeniable, [8] understanding that
the products and services announced by companies and organizations through virtual
networks 3.0, the managerial implications are important as it opens an inﬁnite world of
possibilities for marketing and contextual communication. In this study, therefore, in
addition to recognizing the scenarios and inﬂuence of communication in the organi-
zations of northern Ecuador, the importance of outlining the processes is translated,
considering the new media reality and changes of the era.
The following data shows the means of communication that the organizations of
Northern Ecuador consider important to spread information, as well as the willingness
of companies to incorporate communicational management by processes.
It is a fact that in traditional companies there still exists the culture of arranging
meetings in which prompt information is made ofﬁcial (see Fig. 1). In this same
context, new technologies are affecting the levels of transmission and reception of
messages. Given this, it is validated that the process tends to specify and control each
task, thus facilitating the evaluation and coordination of results. Following the analysis
of the companies studied, Fig. 2 shows that 91.7% of companies are willing to
incorporate a communication system that contributes to the improvement of the indexes
of organizational development, as part of their strategic planning. It must be noted that
the worldview of internal communication cannot be qualiﬁed in so far as only the
insertion of new channels of diffusion, but also in the quality and relevance of the
messages.
Fig. 1. Means of communication to spread
information, incorporate a system of processes
Fig. 2. Willingness of companies to incor-
porate communicational management by
processes
Media Processes of Communicational Management
1107

“It is clear that organizations no longer use social media to only communicate better
with their clients, but also to share knowledge with their main stakeholders and, per-
haps more importantly, to improve internal ﬂows of knowledge transmission. More
than 80% of companies already use some form of technology in their processes and
90% of managers say they have quantiﬁable beneﬁts and are optimistic about
improving productivity. Increasingly, managers believe that to accelerate these chan-
ges, it is necessary to support and favor the acquisition of digital competencies for the
entire organization” [9] (Table 2).
Next, the process maps are presented, divided by different functions, which must be
applied according to the reality and needs of Ecuadorian companies. These schemes
provide a global-local perspective, positioning each item in the chain sequence of
activities. At the same time, the purpose of the organization is related to its manage-
ment techniques, also serving as a tool of consensus and learning.
The analysis of the processes of the interior of an organization has to do with the
great challenge of understanding the structures that compose it, in relation to them-
selves and with the users of the services. These aspects are complicated in the way in
that it is perceived that any organization is not mobilized only by the laws that intend to
govern its operation [4]. The following process maps have an input element and an
output element, which are numbered in the color red, in order to attract the reader’s or
implementer’ attention (Fig. 3).
Table 2. Process map application
Description of the
activity
Deﬁnition of four process maps with their respective activities
Objective
Validate the communicational elements proposed by the process map
by their justiﬁcation and pertinence
Process map
(inputs)
1. Context, ﬁrst input element
2. Intention, second input element
3. Coding, third input element
4. Virtuality, fourth input element
5. The Message, ﬁfth input element
6. The Dialogue, sixth input element
Process map
(outputs)
1. The Experience, ﬁrst output element
2. The Word, second output element
3. The Technology, third output element
4. The Paradigm, fourth output element
5. The Reality, ﬁfth output element
6. The Knowledge, sixth output element
Indicators
The three elements that act as indicators are: the context, message and
paradigm. The ﬁrst case is about understanding the reality of an
organization. The message is the content that serves for the feedback
processes and the paradigm evidences the construction of new
concepts or processes
1108
N. Ulloa-Erazo and Á. C. Ramírez

In an organizational reality in which one wants to manage communication by
processes, it is pertinent to begin by identifying roles. As such, the process starts from
the location of the issuing subject, which for this company would speak of the DirCom,
which in theory identiﬁes the context. This element refers to the general scope of the
organization, then corresponds to the location of the reference, which is the closest
reality, in which ﬂows of services, type of clients, and institutional philosophy can
clearly be identiﬁed. This transmitter assumes the responsibility of encoding, inter-
preting and decoding the detected reference. This addresses the vision of the reality of
the company, thus building the experience of recognition and the value of the
organization.
In this map both the sender and the receiver use common codes so that the message
is highly understandable. The input is the context and the output is the experience
(Fig. 4).
This second process determines the measures that the transmitter can establish
previously in the organization, and also makes it possible to locate intentions in the
communications projects that are feasible to implement. That is to say, the intention-
ality becomes the objective of which the issuing subject is appropriated, which is
translated in the choice of a new situation and improvements in the company. This
innovation could be, for example, to improve internal communication. To do this, the
proposed changes will have to be designed and applied, according to the reality pre-
sented by each entity.
To conﬁgure a new situation or reality within the organization, it is necessary to
consider the sources from which processed data is obtained, with which the objectives
and results to be achieved are described. From this analysis, the proposed process map
uses the word as a necessary element, this being a communicational tool. With the
Fig. 3. First process map, in which the organizational context is identiﬁed
Media Processes of Communicational Management
1109

identiﬁcation of this element the production of discursive messages begins. This map
exposes the elaboration of the message as the focal point; part of the methodology for
the structuring of informative and communicational content, which, from a process,
will allow the organization to establish connections with its internal and external
audiences. The entry of this map is the intention, the output is the word, and the result
is the message (Fig. 5).
The next process contains icons and other resources, such as codes, which are
referred to as signs and signals within a given text. These elements are important for
extracting the meaning of the contents, for various purposes, one being the criticality of
Fig. 4. Second process map. Fuentes, here manages the new challenges that the organization
may face.
Fig. 5. Process map. Perceptions - serves to evaluate the pertinence of the messages
1110
N. Ulloa-Erazo and Á. C. Ramírez

ideas, and another being the reﬂection that inspires the message itself. In any case, the
icons, codes, signs and signals constitute a general and particular meaning according to
each case.
The processes continue with the following map, relating the elements with others as
the symbol, which is immersed in the systematic and constructive process of a com-
municational environment. This becomes information, which will later be transmitted
through different channels. The process then links to the technology, knowing that the
new mechanisms help in the spreading of messages in Northern Ecuadorian companies.
After applying these elements, it is possible to generate realities about the abstractions
that enable the information and/or communicational texts. It will be necessary then to
demonstrate that technological advancements are very necessary to fulﬁll the objectives
of marketing, advertising and communication.
The map shows that information is the result of a systemic process; that is to say,
that messages emerge from the composition of elaborated data. It is also understood
that, by diversifying the mechanisms of diffusion by using digital processes, and with
the diversity of recipients, the responses or reactions can open a number of possibilities.
The input of this map is the code and the output is technology. It is important to feature
the three essential stages in the processes of communication: diagnostics, planning and
management (Fig. 6).
This map represents in theory those virtual realities that indicate the messages, in
which the consequences are qualiﬁed in the construction of new paradigms, produced
as the incorporation of innovations that must be applied in organizational contexts.
From another analysis, it may be necessary to break paradigms in businesses, which
oftentimes are standardized and difﬁcult to placate, either because of the organizational
culture or simply because of their traditions and customs.
In general, a virtual world is generated by the intervention of technological
mechanisms, like the new informational and communicational tendencies are the result
of the incorporation of the TIC. Technology dominates in the current reality, and the
support that it generates within the media signiﬁes a turn in business and the traditional
way of working in companies. Undoubtedly, organizations at the general level and civil
society pay more attention to technological agents such as social networks (Facebook,
Fig. 6. Process map. Information and communication technologies
Media Processes of Communicational Management
1111

Twitter, WhatsApp, Snapchat, among others); which have played a transcendental role
in the way in which news is obtained worldwide.
These tools that at ﬁrst sight seem simple to manipulate require professionals who
manage them to assure proper usability. The input of this map is the virtual aspect, the
output is the paradigm (Fig. 7).
This process map begins by detecting the message, which is the result of a process
of analysis and correspondence with localized needs. Thus, it has a strict meaning and
signiﬁcance that will be comprehended by the public once it reaches its destination.
This, in turn, becomes the space where the message overﬂows with all its possible
beneﬁts. However, this message may be disturbed by interference or noise. If the
message is retained and understood, new paradigms become possible or ruptured,
precisely, because the content must be sufﬁciently proactive so that people can build
new processes. The map focuses on the localized needs of the organization as an entry
point. Additionally, based on these, a strategic action plan will be implemented.
It is important to note that the map shows that both the transmitter and receiver
have the responsibility of driving the message, in addition to ensuring that the content
is accepted, to foster new paradigms. This allows public, private and non-governmental
organizations to engage in various communication schemes. The input is the message
and the output is reality (Fig. 8).
The last proposed process map emphasizes the need to incorporate dialogue as an
element that, since its application, ratiﬁes the existence of feedback. Therefore, from
the comprehension of the message, these spaces are possibly enriched, becoming a
method of learning; through which levels of knowledge acquisition are incorporated.
In this sense, the receiver receives the message and takes on the ability of encoding,
interpreting and decoding, to be located at the same level as the sender, with whom he
or she shares equal opportunities of perception when connecting in a similar sense of
reciprocity. The result is knowledge. Dialogue is the opportunity by which a feedback
process is generated, thus meaning that there is a higher possibility of understanding
the message effectively within the organization. Both transmitters and receivers
Fig. 7. Process map. Localized needs.
1112
N. Ulloa-Erazo and Á. C. Ramírez

genuinely share, so they can experiment with learning methods. In the last map, the
input is dialogue and the output is knowledge. The information and knowledge society
is not limited to technological advances or new communicative scenarios, but requires
renewed forms of media and content management [10].
3
Methodology
In this research descriptive methodology was used, for which ﬁeldwork was carried
out, identifying organizational components and revealing the needs and opportunities
of each sector studied. This was supported by qualitative and quantitative analytical
methods. In the ﬁrst case, it was necessary to draw conclusions from the determined
and described parameters that show the advantages and disadvantages of the integration
of a process map in business management. By representing both the quantitative and
qualitative approaches in a set of processes, its analysis becomes sequential and pro-
bative. Each stage precedes the next, the order is rigorous and is constructed from a
theoretical perspective [11]. Meanwhile, the realities of the public, private and NGO
entities of northern Ecuador have been deepened, situations that overﬂow in traditional
schemes, but at the same time having possibilities for substantial changes.
4
Conclusions
Communication by processes is conﬁgured in a new option to manage new organi-
zational realities; articulating planning and communication. It will be easier to deter-
mine the importance of each task and function with the use of process maps; by which
each area can be evaluated individually, thereby facilitating decision-making or
Fig. 8. Process map that drives the adoption of learning
Media Processes of Communicational Management
1113

showing if corrective actions need to be taken along the way. The organizational
environment of Northern Ecuador is still positioned within a traditional system;
therefore, it will be necessary to move forward with research and set the guidelines for
proposing and developing structural innovations within the organizational reality of
Ecuador.
Management by processes becomes a new trend to strategically introduce com-
munication, from a direct articulation with the reality of organizations, audiences and
the media.
References
1. Zaratiegui, J.R: La gestión por procesos: su papel e importancia en la empresa, p. 81 (1999)
2. Tarazona, M.P., De Miguel Calvo, J.M., Hernández Valz, H., Vicuña Peri, L.A., Marzal, H.S.,
Sánchez, M.L.: El papel de la construcción con respecto a la percepción de la condiciones de
trabajo y el manejo del medio ambiente en cuatro caseríos de una comunidad campesina
andina: Caraz, Ancash. Revista del Instituto de Investigación FIGMMG 2005 8(16), 57–67
(2005). (p. 59)
3. Asamblea, N.: Ley Orgánica de la Función de Transparencia y Control Social (2012). http://
www.oas.org/juridico/PDFs/mesicic4_ecu_proy.pdf
4. Batista, F., Merhy, E.: Mapas analíticos: una mirada sobre organización y sus procesos de
trabajo. Salud Colectiva, Buenos Aires (2009)
5. Castillo, A., Martínez, A.: Relaciones Públicas y Tecnologías de la comunicación. Análisis
de los sitios de prensa virtuales (2005)
6. Túñez, M., García, J.: Las redes sociales en las estrategias de comunicación: del Prestige a
Fukushima (2012)
7. Asamblea, N.: Ley Orgánica de Comunicación (2013). http://www.arcotel.gob.ec/wp-
content/uploads/downloads/2013/07/ley_organica_comunicacion.pdf
8. Küster, I., Hernández, A.: De la web 2.0 a la web 3.0: antecedentes y consecuencias de la
actitud e intención de uso de las redes sociales en la web semántica (2013)
9. Magro, C., Salvatella, J., Álvarez, M., Herrero, O., Paredes, A., Vélez, G.: Cultura digital y
transformación de las organizaciones. RocaSalvatella 2014, Barcelona (2014)
10. Núñez, M.: La gestión de la comunicación en las organizaciones (2012)
11. Hernández, R., Fernández, C., Baptista, M.: Metodología de la Investigación (5ta. Edición)
(2010)
1114
N. Ulloa-Erazo and Á. C. Ramírez

Radio: A Didactic Meaningful Strategy,
for Strengthening the Oral Communicative
Competence in the English Language
María Fernanda Ibadango-Tabango(&),
Armida Mariela Montenegro-Cevallos,
and Luz Marina Rodríguez-Cisneros
Pontiﬁcia Universidad Católica del Ecuador – Sede Ibarra,
(Pontiﬁcal Catholic University of Ecuador - Ibarra Campus),
Jorge Guzmán Rueda Ave. and Aurelio Espinosa Pólit Ave.,
Citadel “La Victoria”, 100112 Ibarra, Ecuador
{mﬁbadango1,ammontenegro,lmrodriguez1}@pucesi.edu.ec
Abstract. In this article entitled “Uso de la radio como estrategia interactiva
para desarrollar la competencia comunicativa oral del idioma inglés, (speaking)”
are outlined some experiences resulting from a research project carried out at
Pontiﬁcia Universidad Católica del Ecuador Sede Ibarra, during the academic
term 2016–2017, with students of eighth and ﬁfth level of the Languages and
Linguistic School. This research talks about the importance of the radio for the
learners’ English speaking skill strengthening. The radio has been used as an
object of study, but has not been used as a didactic strategy so it is very
innovative for the educational ﬁeld to access to new teaching methodologies.
The quantitative method was used; ﬁeldwork was conducted in weekly sessions.
The methodology used was mixed, since data were evaluated and descriptive.
Results were gathered simultaneously to the signiﬁcant learning linked to
innovative and interesting methodological strategies for students like the radio.
Keywords: Radio  Didactic strategy  Innovative  Speaking skills
1
Introduction
The challenges of higher education at the global, regional, and national levels are
vertiginous, and are intimately related to the innovation embedded in their curricula.
Methodologies become the means by which learning passes. Therefore, designing,
accommodating, or reinventing them is pressing.
The radio appears as one of those didactic methodologies, that help to the inno-
vation, since it is implied in issues referring to: the imagination, the right decision
making, to work collaboratively, the creativity, and in this case, it is related to the
signiﬁcant improvement of communicative competences related to the oral skill of the
most recognized language in the world, the English language.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_106

1.1
Theoretical Framework
In previous researches [1], referring to the importance of radio for the signiﬁcant
development of language, states that the development of the language is one of the
most important educative objectives in all levels, because it is the main tool by which
the human being build and understand the world. The language helps learners to
develop reﬂexive and critical thinking.
Some researchers around the world studied this topic in its natural environment and
with educative purposes.
In other researches “The radio in informal education” [2], states that: “the radio
contributes to people take decisions and learn by themselves”, while this author [3],
recalls that: “the educational radio broadcasts are both those that literate and dissem-
inate basic knowledge, such as those that promote the communication of values, the
promotion and integral development of people and communities. Other author [4],
makes a reference to an audio education that is achieved thanks to the radio, because
the media enables oral expression, beneﬁts the listening skills, stimulates the imagi-
nation and allows to understand the qualities and typology of sound.
It is clear, that the radio has some didactic beneﬁts that are worthy of replication, so
it is contextualized in the state responsibilities through higher education in Ecuador:
“To incorporate the information technologies and communication in the educative
process and propitiate the link between the teaching and the social and productive
activities” [5].
1.2
Education and Communication: Some Premises
This author [6], states that education should be practical and enable student’s inter-
active processes for developing their skills, it also facilitates that the dialectical inter-
action between people and their reality can develop their mental abilities and their
social awareness.
Education, for this researcher [7], has a kind of “the common denominator, the
improvement idea, linked ideal vision of the human being and the society. The edu-
cation appears like enabling of human being ideal”.
According to the researcher [8]: “The education is an individual whole, higher
individual, higher organic. It is dynamic and tends to be perpetuated by a strange
inertial force. But it is also exposed to drastic, sometimes traumatic, and times of crisis
and confusion, when very few know what to do; arising from contradictions, inade-
quacies, casuistic and misguided decisions, catastrophes, drastic changes. It is good to
know that education changes because time makes it so, because it becomes. It itself
alters, changes and moves in a continuous and sometimes discontinuous way; grows
and decreases, can become and cease to be”.
In the pedagogical model that Pablo Freire calls “free and transformer education”:
“The education is praxis, reﬂection and action of the human being over the world in
order to transform it” [9], therefore, education must be considered as a permanent
process, in which the learner is discovering, building, reinventing and acquiring
knowledge.
1116
M. F. Ibadango-Tabango et al.

There are as many constructs about education as the subjectivity of those who raise
them, thus: “The education is a necessity of the modern human being. The human
being is believed the owner of the truth” [10], for Monseñor Leonidas Proaño it was the
process whereby the person elevates his skills and abilities to the highest level, but for
the people’s beneﬁt [11], as the Pope Bergoglio points out, quoted by this author [12],
it is necessary to humanize education to make full sense of the integral formation of the
person. The important thing in this scenario is to delineate that education is a funda-
mental process for the development of the human being.
On the other hand, the term Communication has its contexts, functions and qual-
ities. Some authors deﬁne it as:
“Long time ago, there were two ways of understand the term communication: 1.
Act of inform, transmit, send. Verb. To Communicate. 2. Dialogue, Interchange;
relationship of sharing, to be in correspondence, in reciprocity. Verb: Communicate” 3.
The researcher says: [13] “The communication is a process of democratic social
interaction that is based in the interchange of symbols whereby the human being share
in a freeway the experiences under some conditions of free access and egalitarian,
dialogue and participation”. This writer [14] afﬁrms that it is the human being com-
munitarian relationship which consists on the reception and emission of messages
between inter-speakers who are in a state of complete reciprocity.
The investigator [15] states that: “Communication can be viewed as a transaction in
which the meaning of messages is negotiated between people”. The world is changing
all the time and with these changes communication is different and it becomes one of
the most relevant aspects in the development of relationships.
To summarize, communication is the process that permits sending common mes-
sages for senders and receptors offering a feedback in which it could be possible to ﬁnd
improvement and development objectives.
1.3
The Didactic: Principle for English Learning
Recently, the popularity of Task-Based Learning (TBL) has made many researchers,
teachers, and methodologists evaluate the effectiveness of this approach. Although it
has been used for many years, its popularity has increased. One of the main reasons for
this attention is the desire of educators to promote real communication or the exchange
of meanings rather than forms. If task-based instruction takes place, language learning
is more meaningful and natural. The task is currently considered to be the most
effective means of promoting second language acquisition (SLA) in the classroom. The
radio as a didactic strategy should provide our learners with meaningful tasks which
include plenty of opportunities for developing speaking skills, in the same way the
television and other multimedia resources could be important strategies that help
teachers with the development of communicative skills.
1.4
The Radio and the Relationship with the Oral Meaningful Learning
The listening skill is relegated in language teaching. According to (3) “The educative
radios promote the values transmission, the human being promotion, the integral
development of the human being and the community; improve the level of conscious,
Radio: A Didactic Meaningful Strategy
1117

stimulate the reﬂection and change each person in an active agent for transforming the
natural, social and economic environment”. There is a high possibility of success, if the
radio is linked to the curricula, in any subject as a complement. The cooperative learning
is generally deﬁned as a teaching arrangement in which small, heterogeneous groups of
students work together to achieve a common goal. Students encourage and support each
other, assume responsibility for their own and each other’s learning, employ group
related social skills, and evaluate the group’s progress. The basic elements are positive
interdependence, equal opportunities, and individual accountability. Human beings are
social creatures by nature and cooperation has been used throughout history in all
aspects of our lives. The radio as a didactic strategy incorporates cooperative learning
and improves the achievement of students and their interpersonal relationships. Master
Viviana Galarza Professor at Universidad Católica del Ecuador Sede Ibarra mentioned
that the capacity of learners in the radio recording room is for four or ﬁve people and this
didactic strategy help to improve all language skills.
2
Methodology
“Uso de la radio como estrategia interactiva para desarrollar la competencia comu-
nicativa oral del idioma inglés, (speaking)” is a Proyecto Tipo A carried out in the
Languages and Linguistics School at Pontiﬁcia Universidad Católica del Ecuador Sede
Ibarra. The main objective is to determine the effects of the radio as a didactic strategy
in order to strengthen the development of the oral communicative competence.
For developing this project, it was necessary to apply an eleven questions survey to
200 students of the ﬁfth level of general English. According to the main results 91% of
them consider important the oral communicative competence, 63% say that a dynamic
strategy help them to develop ﬂuency and get conﬁdence on speaking competence, and
64% of students would like to participate in a radio program with some interesting
activities related to their main preferences. So these results showed that the radio could
be a positive didactic strategy that improves the level of oral English communication
(Fig. 1).
Fig. 1. Oral communicative competence
1118
M. F. Ibadango-Tabango et al.

So this research is a mix of quantitative and qualitative methods, because the data
and descriptions about the results of the observation developed. This research is
descriptive because the information and characteristics of the object of study. And also
empiric because it is the ﬁrst time these type of projects is developed in this institution.
There were two groups of four and ﬁve learners on each one; they were students of
ﬁfth and eighth levels of Applied Linguistics Major. The radio programs were devel-
oped on the studio of Social Communication School at Universidad Católica del
Ecuador Sede Ibarra. It is important to mention that this school has just one studio
recording room; consequently, each group had one session per week for one academic
term according to the following information (Table 1):
The sessions were planned according to the contents of the purposes of English
communicative competences in the subjects of Communication I and II, Introduction to
teaching and Methodology according to the approved program in the academic term
2016 and 2017. For every radio program the Applied Linguistics students who par-
ticipated in this project were motivated to use target language with some meaningful
tasks and they had the opportunity to interact in a simple way with other learners.
According to Jane Willis with the task based learning approach the students can include
a holistic experience in language in use, also the technology and communicative the-
ories give teachers the opportunity to encourage a positive language learning envi-
ronment with good results.
As part of methodology, some interviews were done to the learners before and after
the radio experience and some audio recordings were part of the data collection that
gave us the ﬁnal results about how the radio like a didactic strategy can help learners to
get conﬁdence on speaking and get better outcomes with creativity, imagination and
main aspects of oral language learning.
Table 1. Schedule of sessions
Academic term: 2016                   Group: 1
M          o         n        t        h        s
Activities 
July
August 
September
October
November 
December 
Session 
Planning
Radio 
Program
Academic term: 2017                   Group: 2
M          o         n        t        h        s
Activities 
March
April
May
June
July
August
Session 
Planning
Radio 
Program
Radio: A Didactic Meaningful Strategy
1119

The rubric designed for the purposes of this project cover main factors of the oral
communicative competence in order to help teachers with this big challenge that
requires an active participation in the educative process and also students need a very
structured motivation that allows them the possibility to express their own ideas,
opinions and feelings in a good way. They can also improvise and participate ﬂuently.
This rubric has components like language use, comprehension, accuracy, pronunciation
and ﬂuency (Table 2).
3
Results and Discussion
In relation to the ﬁve evaluated criteria, the descriptive data of group 1 are shown
below (Table 3):
Table 2. Rubric
Poor 1
Fair 2
Good 4
Excellent 5
Language use
Students use
the language
but with some
errors
The use of
language is
simple
The use of
language is clear
and with
communicational
purposes
The use of language
is so clear and easy
to understand
Comprehension The
participation is
simple and
difﬁcult to
understand
The participation
is unclear
The participation
is clear and with
few errors
The participation is
effective with the
communicational
purposes
Accuracy
There is no
accuracy
There is little
accuracy
There is accuracy
and the
presentations are
clear
There is accuracy
and the
presentations are
easy to understand
Pronunciation
The
pronunciation
is difﬁcult to
understand the
message
The
pronunciation is
with some errors
of structure and
basic forms
The
pronunciation is
clear and easy to
understand
The pronunciation
and intonation is
clear and the
presentation is
appropriate
Fluency
There is no
ﬂuency
There is ﬂuency
with some
interruptions
The ﬂuency is on
presentation and
is clear
The participation is
with total ﬂuency
Table 3. Descriptive statistics group 1
Group 1
Language use Comprehension Accuracy Pronunciation Fluency
Average
3,5
3,6
3,3
3,7
3,3
Max
4,5
4,3
4,3
4,7
4,2
Min
2,7
3,0
2,3
3,0
2,7
Deviation 0,7
0,6
0,9
0,6
1,1
1120
M. F. Ibadango-Tabango et al.

It is evident that the minor value in this group corresponds to accuracy, while in the
maximum assessment there is no signiﬁcant difference; however the pronunciation
criterion is somehow emphasized. In regard to standard deviation, it is noticed that it is
the ﬂuency criterion in which the most improvement was shown in the graph (Fig. 2):
In regard to group 2, the results are shown on the chart (Table 4):
For group 2, it is evident that the minimum value corresponds to the pronunciation
and the highest value is to the use of language, whereas in the standard deviation
reckoning, the criteria of comprehension and ﬂuency appear as representative, as it is
shown in the graph (Fig. 3):
Fig. 2. Deviation group 1
Table 4. Descriptive statistics group 2
Group 2
Use of language Comprehension Accuracy Pronunciation Fluency
Average
3,4
3,4
3,1
3,0
3,2
Max
4,4
3,8
3,6
3,8
3,4
Min
2,8
3,0
2,8
2,0
3,0
Deviation 0,7
1,0
0,7
0,7
1,1
Fig. 3. Deviation group 2
Radio: A Didactic Meaningful Strategy
1121

Notice that in the contrast of evaluated criteria for the two groups, the ﬂuency
stands out, therefore, it can be afﬁrmed that the radio as didactic strategy supports the
development of this fundamental characteristic in the speaking. On the contrary, this
strategy contributed less in the criterion of the use of language. Likewise, there are
differences in the deviation calculated in the criteria of comprehension, accuracy and
pronunciation in both groups (Fig. 4).
Checking the progress of each group during the process, the following information
is available (Fig. 5):
It can be seen in the graph that the ﬂuency criterion started with the lowest
assessment (2.0) and at the end of the project it reached the highest score (4, 5).
Although all the criteria showed high valuations with respect to their initial valuations
in November 2016 there was a slight stagnation in the development of the use and
Fig. 4. Standard deviation in the groups
Fig. 5. Progress group 1
1122
M. F. Ibadango-Tabango et al.

comprehension of the language. Based on the results shown, it can be afﬁrmed that
there was a relatively constant progress in the development of the speaking
characteristics.
With regard to group 2, in the ﬂuency characteristic there is a remarkable devel-
opment, similar to the behavior shown by group 1, also in the competence of com-
prehension, this second group showed a more signiﬁcant advance compared to the one
registered in the ﬁrst group. On the other hand, although in accuracy and pronunciation,
group 2 registered lower scores (3, 6) with respect to the development shown in the
other characteristics, the evaluation of the same criteria increases relatively constant
each month. In conclusion, the recorded data reinforce the statement that the radio as a
didactic strategy allows a signiﬁcant development in the speaking skills (Fig. 6).
With regard to group 2, in the ﬂuency characteristic there is a remarkable devel-
opment, similar to the behavior shown by group 1, also in the competence of com-
prehension, this second group showed a more signiﬁcant advance compared to the one
registered in the ﬁrst group. On the other hand, although in accuracy and pronunciation,
group 2 registered lower scores (3, 6) with respect to the development shown in the
other characteristics, the evaluation of the same criteria increases relatively constant
each month. In conclusion, the recorded data reinforce the statement that the radio as a
didactic strategy allows a signiﬁcant development in the speaking skills.
4
Conclusions
According to the results the radio is a positive didactic strategy and also could be a
great innovative tool for teachers. With the communicative objectives about the English
language it is essential to incorporate new strategies.
Fig. 6. Progress group 2
Radio: A Didactic Meaningful Strategy
1123

Based on this research, it can be concluded that the communication can be effec-
tively measured with the formulation of criteria and descriptors, which can be taken as
a reference to verify the development of oral competence.
It is evident that ﬂuency criteria have a constant progress, however it is important to
mention that the improvement is general in the other criteria as: use of language, accuracy,
pronunciation,comprehension.Ontheotherhand,itisnecessarytohighlightthatstudents’
progress was really signiﬁcant especially of those who got low grades at the beginning.
Depending on the results obtained by each of the criteria, it is established that the
use of radio as a didactic strategy allows the consistent development of oral compe-
tence although at different levels according to the characteristics of the group.
The radio helps students to develop the use of the language, imagination, creativity
and also ﬂuency in speaking skills. So, it is essential to involve students in meaningful
activities to improve their level of speaking.
The results of this research can be used as reference in the development of skills in
subjects with high oral communication component, even in the speaking of other
languages.
References
1. Catalán Abarca, M.: La radio escolar digital y su aporte al aprendizaje en la asignatura de
Lenguaje y Comunicación en el colegio Altazor. Comunicación y medios, 101–117 (2015)
2. Gascón
Baquero,
M.C.:
La
radio
en
educación
no
formal,
p.
8.
CEAC
S.A,
Barcelona-España (1991)
3. Kaplún, M.: El comunicador popular, p. 24. Intiyan, Quito (1985)
4. Rodero, E.: Educar a través de la radio. Signo y Pensamiento, 98–109 (2008)
5. Asamblea Constituyente: Constitución de la República del Ecuador, p. 161. Asamblea
Nacional, Montecristi (2008)
6. Kaplún, M.: Producción de programas de radio. El guión y la realización. CIESPAL,
Quito-Ecuador (1999)
7. Sarramona, J.: Fundamentos de Educación, p. 27. CEAC, España (1989)
8. León, A.: Qué es la educación. Educere, 595–604 (2007)
9. Freire, P.: La Educación como Práctica de la Libertad, p. 7. Siglo XXI, España (2009)
10. López, M.J.: Emergencia de la educación en la sociedad contemporánea, p. 30. Cometa,
España (2012)
11. Secretaría Nacional de Planiﬁcación y Desarrollo. http://www.planiﬁcacion.gob.ec/leonidas-
proano-una-obra-emancipadora-desde-el-territorio/
12. Bernascoli, M.: http://es.radiovaticana.va/news/2017/02/09/papa_humanizar_la_educaci%
C3%B3n_impulsando_el_di%C3%A1logo/1291481
13. Beltrán Salmón, L.R.: https://www.infoamerica.org/teoria_textos/lrb_com_desarrollo.pdf
14. Pasquali, A.: Comprender la comunicación. Monte Avila, Caracas (1979)
15. Fujishin, R.: Creating Comunication. The Rowman and Littleﬁeld Publishing, Maryland
(2009)
1124
M. F. Ibadango-Tabango et al.

The Communication in English
from an Educational Perspective and Its
Relationship with the Competence-Based
Teaching Proﬁle
Brenda Gutierrez-Franco(&), Armida Mariela Montenegro-Cevallos,
and Hazel Machado-Rosales
Pontiﬁcia Universidad Católica del Ecuador–Sede Ibarra,
(Pontiﬁcal Catholic University of Ecuador),
Ibarra Campus, Jorge Guzmán Rueda Av. and Aurelio Espinosa Pólit Av.,
“La Victoria”, 100112 Ibarra, Ecuador
{bkgutierrez,ammontenegro,hsmachado}@pucesi.edu.ec
Abstract. English is regarded as a primary language for communication pur-
poses worldwide. Therefore, using it has become a global need. The present
work aims to support that in the ﬁeld of education - higher level - the mastering
of certain communicative competences in the professional proﬁle of the teachers
of that speciﬁc area should be favored, so that the teaching process they perform
generates signiﬁcant learning outcomes for students and that they, in turn, can
access the advantages of English speaking. At the methodological level, 142
teachers of the English area from northern Ecuador were surveyed. In addition, a
panel of experts was held to endorse the competences mentioned. This work has
a substantive value of usefulness for the educational, didactic and curricular
development of communication in English, since the successful communication
of all the speakers who attend permanently classes in this foreign language will
depend to a great extent on the rigor that teachers possess.
Keywords: Communicative competences  English  Teaching proﬁle
1
Introduction
Ecuadorian education and especially higher education - in recent years - have under-
gone substantial changes. There are several reasons that have caused this transforma-
tion: the increase and complexity of the problems in social structures, among which
globalization has been substantial, the change of the production model, the rapid
increase of knowledge and research, evidence that shows higher education itself has
been forced to propose radical internal changes. Ineludible issues that the university
must assume as its own since it is an entity responsible for making training decisions.
No right to doubt must be added to the above-mentioned conﬂicts that arise from the
globalization of the world, a result of learning that therefore must settle with greater
accuracy and convergence in college is learning English as means of expansion,
conjuncture and global communication.
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_107

According to Article 350 of the Constitution of the Republic of Ecuador [1], Higher
Education is that which has as its purpose academic and professional training with a
scientiﬁc and humanistic vision, “academic and professional training with a scientiﬁc
and humanist vision; scientiﬁc and technological research; innovation, promotion,
development and dissemination of knowledge and cultures; building solutions for the
country’s problems”, which refers to the genuine responsibility that the university has
in the face of the evolution of the world.
In this sense, the improvement of the actors of the teaching and learning process
becomes imperative, according to the research carried out next, it proposes to increase
the knowledge and the mastery of the communicative competences at world level,
through the systematic and effective acquisition of English language, all this in relation
to the development of the professional proﬁle of those who teach it.
Consequently, there is an urgent need to prepare teachers for new trends and other
realities. The training in competences responds to the application of the knowledge
acquired, as well as to the production that the professional and acting sectors request.
1.1
Some Deﬁnitions of Competition
A complex system of action that encompasses intellectual abilities, attitudes, and other
non-cognitive elements such as motivation, values and emotions that are acquired and
developed by individuals throughout their lives and are indispensable to participate
effectively in different social contexts [2].
Motives, character traits, concepts of oneself, attitudes or values, knowledge con-
tent, or cognitive or behavioral capacities: that is, any characteristic that can be reliably
measured, and which can be shown to differ in a way between workers who maintain
an excellent performance of the appropriate or between effective and ineffective
workers [3].
Set of knowledge, skills, etc., that the subjects already have, regardless of where
and how they have acquired them. In the knowledge society and lifelong learning,
training is not only produced in formal systems (regulated training processes) but can
be achieved through a variety of sources and agents. It starts from the principle that
what is important is what a person knows how to do, and it is less how or where he has
learned it. Accreditation systems (including the training institutions themselves) must
be in a position to recognize these competences and certify them as the patrimony of
the subjects [4].
Aptitude that a person has to provide quality products, in different contexts. This
ability is developed based on knowledge, skills and abilities that are expressed in
knowledge, in doing, in knowing and in knowing how to live together. Therefore, the
competent person is related to a successful and complete performance [5].
In this context, training in skills has enabled the student to “demonstrate and apply
knowledge and skills in human situations” [6].
However, if the university teacher has a high level of competence innate to his ﬁeld
of action, in the case that concerns us - the teaching of English - surely the results of his
teaching will be parallel to that level of rigor. Today there is a great difference between
the functions of the teacher and the student, the ﬁrst teaches the second learns. And in
1126
B. Gutierrez-Franco et al.

that signiﬁcant learning that the student must demonstrate appear imperative constructs
for the university curriculum: the learning results.
1.2
What are Learning Outcomes?
If the competences are the set of skills, procedures, attitudes, values that a person has to
become a better person, the learning results refer to the demonstration of those com-
petences, “are statements about what the student is expected to be able to do, to
understand and/or to be able to demonstrate once a learning process is ﬁnished” [6].
These are measured with evidence, so that students see their knowledge reﬂected in
tangible products that transcend pure fulﬁllment to reach a mark, as well as they can
understand the real value of their learning [5].
1.3
Skills-Based Teacher Proﬁle
The education based on a competence approach, not only is centered in the student, but
is also centered in the teachers. Since the curriculum will change to improve and to
achieve this, it is necessary to have several wills, especially of the subjects that are
immersed in the educational fact.
On his article about competence-based teacher proﬁle [7] considers that: It commits
to this in the modiﬁcation of its teaching practice, its way of designing the activities and
strategies, its planning not as a mere administrative requirement, but as a reference of
how to lead the student in the achievement of the objectives, purposes and in the
development of their competences and knowledge, so that they serve to face and
respond to certain problems present throughout their lives.
Therefore, the role of the teacher should be framed in that of a person who “un-
derstands, promotes, guides and gives meaning to the inevitable change that transforms
us all and what is asked of him is a commitment to personal improvement, with
learning, with students” [8].
Likewise, [7] argues that, the university teacher must have new functions, such as:
Accompany and guide the work and the student’s search. Promote the integral
development and the continuous improvement of the student. Support and sustain the
student’s unrenounceable effort. Design scenarios, processes and experiences of
meaningful and relevant learning. Prepare students to adapt to the current culture and,
especially, prepare them for the future.
1.4
Importance of the Development of Communicative Competences
in the English Language
From different perspectives, the importance of the knowledge of foreign languages as a
requirement in different academic and professional environments has been emphasized.
In particular, strong attention is placed on mastery of the English language to establish
relationships of different kinds, whether educational, professional, cultural or tourist,
also in order to gain access to the media and develop new technologies.
Thus, due to the technological advance in the Anglo-Saxon countries, the number
of English Speakers has increased considerably. English has become the language of
The Communication in English from an Educational Perspective
1127

communication and has adopted an important role in the scientiﬁc and business ﬁeld
and has become the indispensable language in the technological and commercial sector,
considering it as a lingua franca [9].
Today, many are the reasons why English has become an international language to
the point that its domain is almost a prerequisite for achieving a certain professional
success [10]. For most workers in the world, knowledge of English has been chal-
lenging at the same time as an opportunity, as it is not simply a labor requirement, but
also as a competitive advantage for individual workers, collectives and whole econo-
mies [11]. In the same way and according to [12]: Other authors […] state that doubling
the proﬁciency score in English in China results in tripling or quadrupling of salary.
And India has become one of the favorite destinations, not only because its people are
technically capable, but because they speak English.
The importance of English in business is irrefutable. The article The many
opportunities of EFL training [11] highlights the relevance of multinational corpora-
tions giving English expertise as a component of success, for instance, the author states
that Toyota has speciﬁc requirements regarding the level of management of this lan-
guage, for which employees must pass a TOEFL English test.
Taking into account the above statements, it can be deduced that the knowledge of
the language in a certain way ensures a better remuneration, in addition to increasing
the possibility of accessing international companies where the predominant language is
English. For this reason, its knowledge has acquired the same importance that the
knowledge of computer systems or other skills have.
These perceptions entail naming the phrase “business culture”, since, to be com-
petitive today, it is not enough to produce a lot, but it is also necessary to be ﬂexible
and to understand the different contexts of the market that someone want to access.
The author [13] emphasizes the importance of the knowledge of the language to
access the world of knowledge and research, through the following quotation extracted
from [14] research: “most of the different specialties, the publications of magazines in
English language are those of greater prestige and international diffusion”. Likewise,
“English-language press, television, ﬁlm and literature are available to almost every
country in the world, and most of the world’s scientiﬁc and technological studies are
written in English” [13]. For this reason, it is not surprising that English is learned as a
foreign language in most of the world.
2
Materials and Methods
For this work, a mixed ﬁeld research (qualitative and quantitative) with a descriptive
design was held. The deductive method was also used, based on educational problems
diagnosed on the English teaching ﬁeld divided into science and reality issues con-
sidering that “every research process makes a proposal of organization of knowledge
for the solution of problems, whether science or reality” [15].
This study was carried out on 2016 by a survey applied to 142 English teachers
from primary and high schools on Imbabura and Carchi provinces belonging to
Ecuadorian Educational Zone 1 [16]. On University level the survey was applied to
teachers of the Center of International Exams (CIE) and the School of Languages and
1128
B. Gutierrez-Franco et al.

Linguistics of the Pontiﬁcal Catholic University of Ecuador Ibarra Headquarters
(PUCESI). The questions of the survey asked for information about real necessity of an
integral Higher formation on English teaching, in the same way, the importance they
grant to the fact of mastering the communicative competences on English language and
using it accurately (Table 1).
Globalization has repercussions on the educational ﬁeld, that is the reason why
perfectioning the proﬁle of English teachers becomes necessary nationwide. As informs
the Education First report published on 2015 “in a globalized world, to trade across
borders you have to rely on effective communication. For that it is necessary a lingua
franca that turns out to be English” [17].
As for the methodology used, it started with a documentary study on the basis of
which a group of communicative competences required for teachers in the area of
English was stated. These competences were analyzed by groups of experts which
guarantees the results obtained. The implementation is presented as it follows:
1. Review and delimitation of the science and reality issues which refer to the status of
English teaching as a foreign language in the North zone of Ecuador.
2. Analysis of the regulations governing the Ecuadorian educational system at all
levels. These regulations were: Constitución de la República del Ecuador, Ley
Orgánica de Educación Superior, Ley Orgánica de Educación Intercultural, Plan
Nacional del Buen Vivir, Agendas Zonales, Reglamento de Régimen Académico,
among others.
3. Moreover, global educational trends were investigated in regard to training pro-
grams for teachers of English as a foreign language from what we can highlight the
Master’s Degree in Applied Linguistics and TESOL in which the language and
communicative competences are mainly valued [18].
4. As a last step, a systematization of competences was elaborated through which it is
intended to solve the problems detected at the beginning of the research. These
competences were written up according to the performance expected on the theo-
retical ﬁeld corresponding to the knowledge, on the professional ﬁeld corresponding
to the know-how, the research ﬁeld corresponding to the knowing and the axio-
logical ﬁeld referring to the being [15]. These competences were ﬁnally endorsed by
PUCESI teachers, experts in the English teaching ﬁeld.
Table 1. Inquired population
Educational level Total
Primary
37
High School
77
University
28
Total
142
The Communication in English from an Educational Perspective
1129

3
Discussion and Results
From the diagnosis performed in Imbabura and Carchi provinces from Educational
Zone 1 in Ecuador with 142 English teachers from private and public institutions, the
results obtained were consentient with the importance of developing communicative
competences in the English language according to the strictness on the professional
proﬁle of teachers imparting this subject. The most relevant are shown:
In Table 2, we can see the positive high endorsement expressed by the respondents
regarding the importance of improving English in this global society and the beneﬁts that
communication in this language offers. The most notorious fact is the evidence of a
strengtheningoftheability tocommunicateinauniversallanguage(76,76%),followedby
the one that exposes the best jobopportunitiesthat subjectswillhave in English (72, 54%).
From years ago, April 2014, Ecuador has implemented the Productive Matrix
through which it is intended to give an innovative approach to education. Through this
strategy, difﬁculties that the country has regarding its development and production will
be solved. The learning of a universal language such English would improve the
tourism ﬁeld in Ecuador, since foreign public will be better served [19]. Likewise,
among the advantages scored highly in the survey there are the improvement in the
intellectual-communicative capacity that is closely related to greater access to sources
of updated information in another language. These two aspects are considered of vital
importance to contribute to the development of an effective culture of scientiﬁc
research in Ecuador, as well as to promote the publication of articles and indexed
journals whose need was proposed in the document Plan Nacional del Buen Vivir [20]
(Table 3).
Table 2. Advantages of mastering English language
Advantages
High
Medium
Low
Total
surveyed
subjects
Better job options
72,54%
21,83%
5,63%
142
Better salary
57,04%
32,39%
10,56%
142
More opportunities to access
international scholarships
69,72%
23,94%
6,34%
142
More access to updated information
sources in another language
68,31%
25,35%
6,34%
142
Ability to communicate in an
international language
76,76%
19,01%
4,23%
142
Better capacity of intercultural tolerance
55,63%
38,73%
5,63%
142
Mejor capacidad
intelectual-comunicativa
69,01%
26,76%
4,23%
142
1130
B. Gutierrez-Franco et al.

On the other hand, the results of the survey indicate that more than half of the
subjects have borderline and poor levels in the area of oral skills, listening and reading
comprehension in English. From one point of view, the oral and written modes can be
considered as autonomous norms of language, since they respond to speciﬁc cultural
needs, are associated with particular social practices and are related to certain
expressions of structural meaning. This structural meaning is developed in a contex-
tualized way in a set of systems that go from orality to writing and vice versa. The
reﬂection above is related to one of the reality issues that is shown in the results of this
work, where it is afﬁrmed that the decontextualized teaching of the grammar of the
English language limits its accurate learning (Fig. 1).
Likewise, the majority of the respondents afﬁrm to have developed their level of
English through a university career. However, the low level of communicative com-
petence in the use of the language could be due to the fact that the institution where the
respondents studied emphasized the precision in the use of the language (Grammar)
above ﬂuency, even though the program may have been planned under a commu-
nicative approach. For this reason, the competences that are proposed to solve this type
Table 3. Estimated level on English language according to communicative competences
Skills
High
Medium Low
Total surveyed subjects
Speaking 35,21% 61,97%
2,82% 142
Listening 24,65% 69,72%
5,63% 142
Reading
39,44% 69,72%
5,63% 142
Writing
36,62% 54,93%
1,41% 142
University 
career
46%
English 
courses
31%
Visit to 
English 
speaker 
countries
7%
Contact with 
native 
English 
speakers
16%
YOU DEVELOPED YOUR ENGLISH LEVEL 
THANKS TO:
Fig. 1. Ways of acquisition of the teachers’ English level
The Communication in English from an Educational Perspective
1131

of problems are based on the use of communication, production and understanding of
the language to promote the accurate and practical acquisition of it.
It is important to emphasize that only 55.6% of the surveyed teachers granted high
importance to improving the capacity of intercultural tolerance through communication
in English language, this can be considered an unexpected fact, as the author [21] says
“learning a language is to approach a new culture and reconstruct meanings during the
exchange of experiences”.
The analysis of educational problems in the area of English teaching was the
guideline to consolidate the systematization of communicative competence that
teachers should have in this language. The relations that the problems have with the
competences that will contribute to their resolution are consolidated in the following
Table 4:
Table 4. Relation between problems and communicative competences
Problems diagnosed
Competences
Crisis in communicative skills in the English
language
To demonstrate the teaching of the English
language focusing on communication, in
order to promote the acquisition of linguistic
knowledge by developing communicative
competence and its production
The disparity of levels of Linguistic
Competence of the foreign language among
the students of the same group hinders the
development of the teaching and learning
processes
To know the teaching practice through the
use of different strategies that allow
meaningful and cooperative work
Teachers prioritize curricular planning to the
detriment of the development of
communicative competences of the English
language
To investigate different strategies that
develop linguistic and psycholinguistic
abilities that promote effective
communication, motivating the learning of
languages through their use for real speciﬁc
aims
Deﬁciency in the English language in
teachers and students that hinders the
communicative process
To develop understanding of articulatory
phonetics; a well-trained ear; knowledge of
the morphological, syntactic, semantic,
pragmatic and phonological contrasts of both
the mother tongue and the target language
The decontextualized teaching of the
Grammar of the English language that limits
the learning of it
To deﬁne the different theories on linguistics
and discourse analysis in order to teach the
main grammatical structures of English and
how to use them in speciﬁc contexts
The difﬁculty of adapting the pedagogical
models of English language teaching in the
classroom to technological changes
To exemplify innovative techniques and
strategies for the communicative teaching of
the four English language skills with the use
of technological tools
1132
B. Gutierrez-Franco et al.

4
Conclusions
The main purpose of this study was to identify the problems experienced by English
teachers in Zone 1 of Ecuador regarding communicative skills in the language. At the
same time, this diagnosis was deepened through the establishment of meeting points
between the description of the current problems and the communicative competence in
English of these subjects, while an analysis of the norms of the academic system in
Ecuador was carried out and a research on global trends in teacher training programs in
teaching English as a foreign language was also made. The analysis of the results leads
us to the following conclusions:
According to the analyzed points, the formation of the teaching proﬁle based on
communicative competences is imperative in order to guarantee meaningful learning in
the classroom.
The assessment of competences through expert groups solved the scope of the
proposed competences more adequately and established the coherence between the
problems and those competences.
From the diagnosis that emerges from studies such as the one described in this
article, it would be possible to think of adjustments that can be implemented in uni-
versity or higher education systems. Considering the observations made in this
description about communicative competences and their relationship with the teaching
proﬁle, below we present some suggestions that could contribute to develop better
levels of the set of competences necessary to be a competent English user/teacher:
To revalue the role of the formal teaching of grammar and vocabulary under the
communicative approach of teaching English, with perspectives to integrate the
development of precision and ﬂuency when using the language.
To emphasize the teaching and use of memory strategies that are sometimes the
weakest and which, given the importance they collect when storing and recovering
vocabulary and grammatical structures, interfere in the achievement of a good level of
communicative competence in written production and an accurate use of the language.
To increase the offer of opportunities that aim at the development of competition in
written production in functional contexts and to promote the use of a set of writing
strategies, such as being very aware of the audience for those who we write to, plan,
revise and adjust the writing, using paraphrase and synonyms, in order to respond to
the requirements of an efﬁcient written communication.
To sum up, this study is important in the ﬁeld of research on the potential rela-
tionships between the communicative competence of English educators and its inﬂu-
ence on the communicative competence acquired by their students.
References
1. Asamblea Constituyente: Constitución del Ecuador, p. 62 (2008)
2. PISA. https://www.oecd.org/pisa/39730818.pdf
3. Pirela, L., Prieto, L.: Perﬁl de competencias del docente en la función de investigador y su
relación con la producción intelectual. Redalyc, pp. 160–177 (2006)
The Communication in English from an Educational Perspective
1133

4. Zabalza, M.: Competencias docentes del profesorado universitario calidad y desarrollo
profesional. Narcea, Madrid (2007)
5. Montenegro Cevallos, A.: Formación en competencias perfecciona el perﬁl profesional de
los comunicadores de la Zona 1 del Ecuador. Xescom, De los Medios y la Comunicación de
las Organizaciones a las Redes de Valor, pp. 52–59 (2016)
6. Kennedy, D.: Redactar y utilizar los resultados de aprendizaje, p. 19 (2007)
7. Álvarez, M.: Perﬁl del docente en el enfoque basado en competencias. Educare, pp. 99–107
(2011)
8. Instituto Nacional de Formación del Magisterio. http://www.inafocam.edu.do/portal/index.
php/noticias-inafocam/item/525-abordan-el-perﬁl-del-educador-en-el-siglo-xxl-en-
seminario-taller-sobre-habilitación-docente.html
9. Graddol, D.: The Future of English? The British Council (1997)
10. Garcia, O., Fishman, J.A.: The Multilingual Apple: Languages in New York City.
Identiﬁers, New York (2002)
11. Eagan, D.: The Many Opportunities of EFL Training, pp. 56–59. T+D Magazine, USA
(2004)
12. León, C.: Importancia del dominio del idioma inglés en el desarrollo profesional en áreas
administrativas, contables y económicas de Bogotá, p. 60. Universidad de San Buenaven-
tura, Bogotá (2006)
13. Saorín, A.M.: Las cartas de queja en el aula de inglés para turismo: Implicaciones
pedagógicas basadas en el uso de recursos de cortesía, p. 12. Universidad Jaume I, Castellón
(2003)
14. Alcaráz Varó E.: El inglés profesional y académico, p. 15. Alianza, Madrid (2000)
15. CES: Guía para la presentación de proyectos de programas de posgrado (2014)
16. Ministerio de Educación: Desglose general de direcciones distritales de educación. Obtenido
de.
https://educacion.gob.ec/wp-content/uploads/downloads/2013/10/Desglose_DirDist_
SEP.pdf
17. Education
First.
http://www.eluniverso.com/noticias/2015/02/11/nota/4547176/ecuador-
tiene-nivel-bajo-ingles-segun-informe-education-ﬁrst
18. Macquarie
University.
https://courses.mq.edu.au/2018/domestic/postgraduate/master-of-
applied-linguistics
19. Senplades: Transformación de la Matriz Productiva (2014)
20. Senplades: Plan Nacional para el Buen Vivir 2013–2017 (2013)
21. Morales, O.C.H.: Textos escolares y prácticas pedagógicas en una propuesta de educación
intercultural bilingüe. Educación y Humanismo, pp. 102–118 (2015)
1134
B. Gutierrez-Franco et al.

The Diffusion of Public Policies on Technical
Training for the Textile and Clothing Industry
in Ecuador
Tania Aguilera1,2(&), Andrea Mila1(&), Daniela Batallas1,
and Giovannina Torres3
1 Docente de la Pontiﬁcia Universidad Católica del Ecuador Sede Ibarra,
Escuela de Negocios y Comercio Internacional, Av. Jorge Guzmán Rueda y Av.
Aurelio Espinosa Pólit. ciudadela “La Victoria”, 100112 Ibarra, Ecuador
{tpaguilera,admila,idbatallas}@pucesi.edu.ec
2 Doctorando del Programa de Desarrollo Regional e Integración Económica,
Universidad Santiago de Compostela, Santiago, Spain
3 Egresada de la Pontiﬁcia Universidad Católica del Ecuador Sede Ibarra,
Escuela de Negocios y Comercio Internacional, Av. Jorge Guzmán Rueda y Av.
Aurelio Espinosa Pólit. ciudadela “La Victoria”, 100112 Ibarra, Ecuador
gdtorres@pucesi.edu.ec
Abstract. This research aims to determine if the technical training policies
proposed by the Ecuadorian government after the adoption of the method-
ological model of productivity and systematic competition (which originated in
the German Institute of Development), were correctly distributed and imple-
mented in one of the prioritized industries: the textile and clothing industry.
Keywords: Diffusion of public policies  Systemic competitiveness
Textile and clothing industry
1
Introduction
1.1
An Approach to the Diffusion of Public Policies
With the North American current proposed by Laswell (1951), it became aware that
“policy process, its elaboration and realization, is the object of study in its own right”.
That is, with Laswell and the “political sciences” there is a need to “know” the policy
process, as well as the “decision process” itself, based on seven stages: intelligence,
invocation, application, termination and evaluation [1]. In his proposal, Laswell sought
to explain “an (interdisciplinary) systematic science and a (democratic) government
decision” and thus respond to the fragmentation of the social sciences and the need for
greater knowledge by the government in its public decisions [2].
For Aguilar, in the “normative sense, public domain has to do with needs, interests
and projects of general scope” [2]. It also refers to a “manifest nature, to the principle of
free access, transparency and openness (…) to public resources, to tax revenues”
among others, thus creating “precisely because of its public nature (…) a range of
strategies of corresponding actions between the government and society” [2]. Public
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_108

policy refers to “governmental decisions that incorporate the opinion, participation,
co-responsibility and money of private entities in their quality of citizens, voters and
taxpayers” [2]. For Lahera, an excellent public policy corresponds to courses of action
and information ﬂows related to a political goal, deﬁned democratically; those which
are developed by the public sector and, frequently, with the participation of the
community and the private sector. A public policy of quality will include orientations
or contents, instruments or mechanisms, deﬁnitions or institutional modiﬁcations and
an anticipation of its results [3].
In terms of the cycle of public policies, the Guía para la Formulación de Políticas
Públicas Sectoriales del Ecuador (2011) deﬁned as the following [4]: 1. Preparatory
and Diagnostic Stage, 2. State of deﬁning policies, programs and projects, 3. Appro-
bation of sectorial policies and inclusion in the system, 4. Diffusion of policies, pro-
grams and projects to the population. For this research, Phase 4 will be taken into
account, which represents the diffusion of public policies. There are some theories
which present the perspective of the diffusion of public policies linked to the theory of
the diffusion of innovation. Rogers believes that this is the “process by which an
innovation is spread through certain channels of communication over time between
members of a society” Rogers [5] cited by Vergara [6]. The diffusion of public policies
can be understood as “the process in which information about new policies or insti-
tutions is communicated through certain channels in time between members of a social
system” [6].
In terms of the mechanisms of diffusion, they present the understanding of the
objective after the adoption of a determined policy. There are then four mechanisms:
coercion, emulation, competition, and learning. These help to distinguish how it should
be carried out. However, the idea of who adopts the policy should not be overlooked,
making it necessary to “clarify at least who is involved in the processes and what their
competencies, capabilities and motivations are”, considering variables such as the type
of political regime, institutions, ideology, and gross domestic product, among others,
that together explain or determine if a public policy is adopted or not [6].
The mechanisms used by Ecuador to dissemination public policies are four:
(a) sector policy document that includes a list of policies and sectorial actions; (b) the
publication of the sector policy document; (c) dissemination to public servants of the
institutions and; (d) dissemination of the document through the media, forums and
other means of communication [7]. In the case of Ecuador, the Secretaría Nacional de
Planiﬁcación y Desarrollo is the public institution responsible for conducting national
planning and for managing and coordinating the Sistema Nacional Descentralizado de
Planiﬁcación Participativa, through the planning cycle, with a national, sectorial and
territorial approach. Thus, it articulates the public investment towards the established
objectives and goals, with the respective processes of: monitoring and evaluating their
compliance [8]. Thus, according to Article 280 of the Constitución del Ecuador, the
Development Plan or Plan Nacional del Buen Vivir, “is the instrument to which public
policies, programs and projects will be subjected; the programming and execution of
the State budget; and investment and allocation of public resources (…) will be
mandatory for the public sector and indicative for other sectors” [9].
1136
T. Aguilera et al.

While it is true that there is a clear institutional framework when planning, for-
mulating, coordinating and evaluating public policies, it is necessary to add to the
debate whether diffusion is a factor to be considered in any of the phases of public
policy, or, how the State’s communication policies intervene when it comes to
socializing the different public policies that it has implemented or will implement.
1.2
The Model of Systemic Competition (MCS), Meso Level
and the Proposal for Technical Formation
Ecuador, in its Constitution, deﬁnes in Article 284 that its economic policy will have,
as “objective 2”: “To stimulate national production, productivity and systemic com-
petition”. Likewise, in its Plan Nacional para el Buen Vivir 2013–2017 (PNBV 2013–
2017), it states in objective 10: “To promote the transformation of the productive
matrix”, and more speciﬁcally in line 10.9, “the conditions of competition and systemic
productivity necessary to make the transformation of the productive matrix feasible and
the consolidation of more equitable structures for the generation and distribution of
wealth” [10].
The model of systemic competition and its application in Ecuador is the result of
the development proposal of the Economic Commission for Latin America and the
Caribbean (ECLAC), which gave rise to Latin American structuralism, founded in the
1950s with Prebish, who proposed the conception of the center - periphery system and
the theory of endogenous development and regional substitution industrialization. In
this context, ECLAC “proposed to modify the outward development, based on the
increase of exports of primary goods, to the inward development, based on the
expansion of industrial production” [11]. To increase the latter, the model required an
increase in investment that would be stimulated through the incorporation of protec-
tionist measures. However, what was proposed by ECLAC and implemented in Latin
American countries did not achieve the expected results, as the impact of the
endogenous development process, according to Ocampo [12] cited by FitzGerald [13],
“was less evident than it was thought to be, because it could not be incorporated into an
effective policy”.
In the late 1980s and early 1990s, a new stream of thought called “neo struc-
turalism” began to be developed at ECLAC. The failure of so-called structural
adjustment policies created a favorable environment for the emergence of alternative
paradigms [14]. As such, the Systemic Competitiveness Model emerged (SCM), pro-
posed in the 1990s by Esser, K., Hillebrand, W., Messner D., & Meyer-Stamer, J. of
the German Development Institute (GDI). For the authors of the model, after the 1980s,
structural adjustment programs were designed to encourage economic modernization
processes through a repositioning of market forces and a reduction in the scope of
government intervention. However, they did not take into account that developing
countries were characterized by fragile companies and markets, governments that were
omnipresent and weak at the same time, and weak social actors. They started from this
premise to ask: “What should be the starting points for developing countries that want
to establish competitive industries at the international level, or provide international
competition to existing industries? What measures should be applied ﬁrst?” [15].
The Diffusion of Public Policies on Technical Training
1137

In order to answer these questions, they deﬁne industrial competition as the product
of the complex and dynamic interaction between four economic and social levels of a
national system, being: the micro level of enterprises, those seeking efﬁciency, quality,
ﬂexibility and speed of reaction simultaneously, many of them being articulated in
networks of mutual collaboration; the meso level, corresponding to the State and social
actors, who develop speciﬁc support policies, encourage the formation of structures and
articulate learning processes at the level of society; the macro level, that analyzes the
market and the macroeconomic conditions; and (…) the target level, which is structured
with solid basic patterns of legal, political and economic organization, to achieve a
sufﬁcient social capacity of organization and integration as well as a capacity of actors
for strategic integration [16].
According to the aforementioned deﬁnition, industrial competition is the product of
a complex and dynamic interaction pattern between the State, companies, intermediary
institutions and the organizational capacity of a society. The parameters of competitive
relevance at all levels of the system and the interaction between them is what generates
competitive advantages, and is also based on a multidimensional concept of conduction
that includes competition, dialogue and joint decision-making by the relevant groups of
actors. Therefore, the competition of each company is based on the organizational
pattern of society as a whole. Thus, competition is “systemic” [17]. According to the
SCM, the State and industry will institutionally structure speciﬁc services for the sector,
being the meso level. These “services” are translated into speciﬁc policies, one of them
being the policy of technical training. The main postulates are [17]:
Postulate 1. Orientation of education towards values: The new mass production
demands a type of ﬂexible, cooperative worker, committed to producing quality and
accustomed to learning, with autonomous capacity for professional action, able to
relate causes to effects and who is a very skilled communicator.
Postulate 2. Increase in social effectiveness: If there is a corresponding demand, the
vocational qualiﬁcation can be adapted to the speciﬁc requirements of the informal
sector, to vocational-technical vocational training or to the training of skilled workers
in economic segments oriented to competition.
Postulate 3. Preparation for new proﬁles for qualiﬁcations: In the phase of reori-
enting the economic policy, the task of intensifying training and improving work in the
company almost always comes to fruition. As the required vocational qualiﬁcations are
high and continue to grow. In addition, specialization will be based on training deﬁned
by “learning by doing”.
Postulate 4. Regulation with a view of system integration: Educational reforms will
be guided by international standards, as well as requirements emanating from the new
concepts of organization and production, though taking into account the structures and
speciﬁc needs of each country.
Postulate 5. Gradual development and priority areas: The educational system must
be gradually developed in close coordination with the needs of the economy. For
example, vocational education in economic sectors with a strong demand for skilled
1138
T. Aguilera et al.

workers is a priority, as is the interest of business associations to participate in the
design and ﬁnancing of institutions.
Postulate 6. Mutual relations between State and productive sector: The new
requirements of the economy can only be satisﬁed through close cooperation between
the State, educational institutions and private industry: the development of an educa-
tional system structured on the basis of cooperation.
Postulate 7. Concrete cooperation between education, research and the productive
sector: concrete cooperation contributes to bringing vocational training closer to
economic needs, rapidly recruiting graduates in production, jointly improving dialogic
techniques, to form networks and improve international competition.
As indicated above, the Ecuadorian State presents in the Constitution of 2008 and
the Plan Nacional para el Buen Vivir (PNBV 2013-2017), “the transformation of the
productive matrix” [10] as a priority, being one of its pillars fostering the productive
development of sectors, industries or productive activities with strong positive exter-
nalities: creators of higher value added and job seekers [18]. Among the industries
prioritized by the Ecuadorian State is the textile and garment industry, which is the
second generator of employment in Ecuador after the food processing industry.
In the Constitution of 2008, consistent with the SCM, it is pointed out that the
“State will develop labor training programs, depending on one’s vocation and aspi-
rations” [9]. In Objective 4 of the Plan Nacional del Buen Vivir 2013–2017, it is
pointed out that the State will guarantee the right to education/technical training,
centered on human beings and the environment. Speciﬁcally, Policy 4.6 stipulates: “To
promote the reciprocal interaction between education, the productive sector and sci-
entiﬁc and technological research, for the transformation of the productive matrix and
the satisfaction of needs”. On the other hand, in Objective 9, policy 9.5, it is indicated
that the State will tend to “Strengthen the schemes of occupational training and training
focused on the needs of the work system and the increase of labor productivity” [10].
According to the Agenda para la Transformación Productiva de Ecuador, in
accordance with Training Policy 8.8, the capacities of the public and private training
agencies and the articulation mechanisms between the institutions that are part of the
National Training and Professional Training System will be strengthened. Additionally,
vocational training with a labor competencies approach, aimed at the economically
active population and priority attention groups, in tune with the needs of the productive
sector [19] will be promoted. This policy requires the prioritization of sectors for
vocational training and identiﬁcation of training needs with a productive chain
approach, especially in the strategic productive sectors that have been prioritized by the
national government. In order to achieve effective national coverage in vocational
training services, a territorial approach to service provision should be used, in accor-
dance with the new system proposed by SENPLADES.
Given that the job segment that receives less training in the development of their
skills and abilities is that of micro and small enterprises, permanent training plans
should be focused on the most vulnerable workers and priority attention groups.
Companies, especially micro and small enterprises, should also be encouraged to draw
up training plans annually for their workers in a complementary way with public
policy.
The Diffusion of Public Policies on Technical Training
1139

In terms of institutionality, the policy establishes that there will be close coordi-
nation between the Ministerio de Coordinación de la Producción, Empleo y Compet-
itividad (MCPEC), the Ministerio Coordinador de Desarrollo Social (MCDS), the
Centro Nacional de Capacitación y Formación Profesional (CNCF) and the Ministerio
de Relaciones Laborales (MRL), to reduce the level of informal work and focus
training on the most vulnerable groups. It should be mentioned that Ecuador has a
Secretaría Técnica del Sistema Nacional de Cualiﬁcaciones Profesionales, which seeks
to “increase access to certiﬁcation and training of workers both employed and those
with their own businesses, popular and solidary economy, priority attention groups,
public servants and citizens in general” [20].
Policies are institutionalized through an important range of tools, for example:
“professional proﬁles will be developed with a tripartite conformation and with the
productive and social sectors in the different territories that contribute to reduce the
gaps between the required proﬁle and the existent proﬁle of the workers”; “to redesign
the vocational training programs towards a system focused on labor competencies”,
“encourage the incorporation of research and development results generated in the
country”, “to establish differentiated accreditation mechanisms for providers of training
and vocational formation services” “to strengthen the permanent system of monitoring
and evaluating the results and impact of vocational training and formation”, “strengthen
the mechanisms of public-private articulation for the detection of training demands
through sectorial and territorial research processes in accordance with the goals of
productive transformation”, “articulate the processes of professional training the pop-
ulation uses and to encourage the creation and integration of local providers of training
and vocational formation services as well as the mobility of those who have the
capacities to meet the needs of the territories”, and “to design and implement a strategy
for the permanent dissemination of training and professional formation at the national
level” [19].
As for the incentives provided by the State to promote technical training, the
Código Orgánico de la Producción, Comercio e Inversiones, indicates that: for the
business sector, while calculating income tax, during the period of 5 years from the ﬁrst
year of creation, the medium-sized companies will be entitled to deduct the additional
100% of the expenses incurred in the following category: Technical training aimed at
research, development and technological innovation, productivity, and ensuring the
beneﬁt does not exceed 1% of the value of expenses incurred by salary and wages in
the year in which the beneﬁt is applied [18].
2
Materials and Methods
In order to determine whether the technical training policies proposed by the
Ecuadorian State since the adoption of the SCM were correctly disseminated and
implemented, a total of 174 surveys were carried out in the county of Antonio Ante and
156 in the county of Ibarra to owners/managers of the different companies of the textile
and clothing sector, from May to July 2017. The patent databases of the textile and
clothing sector, prepared by the Gobierno Autónomo Descentralizado de Antonio Ante
(2016) and the Gobierno Autónomo Descentralizado de Ibarra (2016) were used to
1140
T. Aguilera et al.

classify companies according to the International Standard Industrial Classiﬁcation
(ISIC) in the case of Ibarra; not so much for Antonio Ante, which uses a different
classiﬁcation criterion. The number of surveys was determined using the formula for
obtaining the sample for ﬁnite populations, speciﬁcally the stratiﬁed sample design;
using the criterion of afﬁxation proportional to the size of each stratum according to the
classiﬁcation of the aforementioned patent base. The survey tool consists of three parts;
the ﬁrst one inquired about the proﬁle of the companies; the second was based on its
knowledge and perception in reference to public policies on technical training, adopted
by the Ecuadorian State starting in 2008; in the third part, information was requested on
the training requirements of entrepreneurs. The information requested was disaggre-
gated according to the organizational levels of the company: workers, operational staff
and management staff. The information obtained was complemented by the analysis of
public policies in its diffusion phase.
3
Results
Imbabura is a province in the northern Andes of Ecuador, and its counties of Ibarra and
Antonio Ante are historically based on the textile industry. It is in this context that the
survey was applied and obtained the following results. The type of predominant
business in both countries is microbusiness which, according to the classiﬁcation of
businesses by the Superintendent of Companies of Ecuador, is characterized by having
1 to 9 employees, a brute value of annual sales less than or equal to $100,000 USD and
an amount of assets up to $100,000 USD [21]. In terms of age, the majority of
microenterprises surveyed in Antonio Ante were registered for 10 years or more, while
in Ibarra the majority were registered from 4 to 6. In both counties, the origin of the
company equity is mostly family-based. Regarding the destination of the production,
according to the respondents in Antonio Ante, it is directed towards the local market
(30.9%) and local and national market (30.3%), whereas in Ibarra its production is
directed mainly to the local market (74.1%) and the regional market (13.5%).
The representatives of the companies were asked about the prioritization of train-
ing. The majority of respondents answered that the priority is “moderate” in the two
counties, with the difference being that in Antonio Ante the option that is in second
place is “high”, while in Ibarra the second place option is categorized as “low”, because
microenterprises that are especially dedicated to sales, indicate that there is no need to
train their staff, or do not have staff to train. The factors determining the choice of a
staff training program were discussed. In Antonio Ante before the criterion “content” is
the most important factor, while in Ibarra it is the “price”; both counties agreeing that
the factor “method” occupies second place. In reference to the problems that prevent a
training program from being effective, the respondents from both counties, for the most
part, responded with “lack of time”; “poor training” and “lack of links with the textile
sector”. As for the annual investment made by entrepreneurs to train staff, the results
are as follows:
(a) Workers: for the training of workers in Antonio Ante, according to what is
mentioned by most of the entrepreneurs surveyed, there is an annual investment of
between $1 and $ 200 (39.1%); the group of entrepreneurs seeking free training
The Diffusion of Public Policies on Technical Training
1141

(25.9%) is included in this percentage; in Ibarra, the majority of respondents answered
that they do not invest in training for the workers; (b) Operational staff: most com-
panies surveyed do not have personnel at this level, however, in the county of Antonio
Ante, there are companies that invest from US$ 1 to 600 (14.4%), in Ibarra this
percentage is 3.6%; (c) Management staff: In the “Management Staff”, Antonio Ante’s
entrepreneurs invest annually between $ 1 and $ 200, followed by the group of those
who seek “free training”, while in Ibarra 37.6% of entrepreneurs “do not invest”; but
those who do, invest between $401 and $600, saying that as production companies,
they must have adequate training to face competition.
The survey also asked about the incentives that companies give to retain trained
staff; most of them do not give incentives. Of those entrepreneurs who answered yes,
they mentioned offering the following: public recognition, schedule ﬂexibility, com-
missions or bonuses. Entrepreneurs were asked if they had obtained certiﬁcations from
the Sistema Nacional de Cualiﬁcaciones, only 4% of Antonio Ante’s companies
obtained certiﬁcations in subjects such as: cut and clothing, skilled workers, continuous
improvement and industrial safety; in Ibarra they obtained certiﬁcations in subjects
such as: customer service, neuro-marketing, marketing and fashion design. The rest of
the companies in the two counties have been trained without obtaining certiﬁcations
and did not remember the subjects. It is important to note that, according to the
respondents, neither county has received training from the government on tax beneﬁts
for training staff; though 1% in Ibarra did receive the training. Therefore, most com-
panies have not made use of this beneﬁt. Regarding the proposals of the government
for the implementation of programs of training in corporate social responsibility,
ancestral and traditional knowledge and in association with the popular and solidarity
economy, the majority did not know about these proposals.
Likewise, the majority of entrepreneurs in the two counties did not know that
Universities offer training. It was asked if they knew about other offers, and both in
Antonio Ante and Ibarra the businesspeople surveyed indicated not knowing about the
providers of training services. In general, their common practices to train the staff are:
(a) the owners of the companies are trained and then become multipliers of the
knowledge, thus decreasing the expense of training; (b) in the moments of crisis of the
industry, the costs of training are eliminated; (c) there are no relevant mechanisms to
encourage and retain trained staff. Currently the textile and clothing sector of the
counties under study are not organized and associations, networks, etc. are not formed.
It is important to indicate that this local reality still exists, even though the Ecuadorian
government has invested signiﬁcantly in education since 2008.
4
Conclusions and Recommendations
In terms of the capacity of the State to disseminate policies, according to the different
tools available: Guía para la Formulación de Políticas Públicas Sectoriales y la Prop-
uesta de Lineamientos Generales para la Planiﬁcación Territorial Descentralizada [4] a
strategy of promotion and diffusion of the Plans at the different levels (national,
provincial, cantonal) must exist, leading to the “appropriation by the citizens”, which is
developed through the formulation of a communication strategy that includes the
1142
T. Aguilera et al.

diffusion of material, making it possible to transmit the goals to the public, along with
the results and proposed procedures, conveying messages that consider the different
population groups that inhabit the area [22]. It is evident from the surveys that among
the factors that have made technical training in the textile and clothing sector difﬁcult in
the counties of Antonio Ante and Ibarra is the lack of knowledge on the part of the
owners about the means of production, training and state policies in this area.
Another element to be considered is the lack of knowledge of the owners/managers,
even of the incentives provided by the State to promote technical training, which, as
mentioned, once calculating income tax, during the term of 5 years from the ﬁrst year
of creation, medium-sized companies will be entitled to a deduction of an additional
100% of expenses incurred in the following category: Technical training aimed at
research, development and technological innovation, improving productivity. The
beneﬁt does not exceed 1% of the value of expenses incurred by salary and wages in
the year in which the beneﬁt is applied [17].
The Systemic Competitiveness Model SCM proposes the creation of an environ-
ment capable of boosting, improving and increasing the efforts of companies to train a
worker accustomed to learning and oriented to competitiveness. Therefore, training
human talent to be able to generate new knowledge, and improve their skills and
techniques will determine the competitive position of the industry. In this context, it is
imperative that the Ecuadorian government, the textile and clothing companies in
Antonio Ante and Ibarra counties of the province of Imbabura (duly organized), the
universities and technical institutions and, in general, the actors involved in the
education/training system design a policy focused on generating a learning system
adapted to the needs of the sector; integrating professional training and technical
specialization at all levels of the company.
The system will be duly planned and will ensure that all companies based in the
territory have the same opportunity for access to technical training; one objective being
the mass dissemination of the taught information. Universities and industries will
contribute with resources so that the system operates, for example, the assignation of
teachers who teach subjects according to the training plan and the industry can con-
tribute with scholarships so that their workers are trained on speciﬁc issues.
The government, through the Ministries and Departments of Economic Develop-
ment of the Decentralized Autonomous Governments, will carry out the diffusion
regarding the application of the regulations for training, focused mainly on public
incentives that are given to the companies that train personnel. The entities that gen-
erate the aforementioned learning system will be fully aware of the regulations and will
coordinate in order to apply it in a way that maximizes its beneﬁts. The learning system
will
be
generated
by
the
textile
and
clothing
companies,
organized
in
associations/networks. This ﬁrst level of organization is the basis for stimulating
learning processes and collective action and the following actors will be included:
public institutions, funders, foundations, public or mixed companies, universities and
social organizations that will promote the formation of human talent. Associativity will
allow the organization to have joint training plans, minimizing training costs, but will
also seek speciﬁc training in accordance with the strategic lines of each company.
The Diffusion of Public Policies on Technical Training
1143

It is important to change the way entrepreneurs think in the textile and clothing
sector; training must go from having a low priority to a high priority, especially if
Ecuador’s goal is to become a “knowledge society”. If companies are already making
an effort to partner and create a learning system for the textile and clothing industry, the
second goal is then to think about how to retain trained personnel, not only in com-
panies but also in the area. The strategy will be to create incentive schemes, which will
include monetary and non-monetary incentives; for example, the design of compen-
sation systems based on the skills and techniques applied to work at each level, bonuses
for seniority or through indicators of productivity, support for fellowships, or creating
merit promotion programs. The learning system of the textile and clothing sector of the
territory itself should develop the capacity to participate in the design of policies, to
implement them and especially to disseminate them among the actors.
References
1. De León, P.: Una revisión del proceso de las políticas: de Laswell a Sabatier. In: Gestión y
política pública, I, vol. 6, pp. 5–17. Centro de Investigación y Docencia Económicas,
México D.F., México (1997)
2. Aguilar L.F.: El estudio de las Políticas Públicas. Miguel Ángel Porrua. Grupo Editorial
México, México (1992)
3. Lahera, E.: Política y políticas públicas. CEPAL, Santiago de Chile (2004)
4. SENPLADES: Guía para la formulación de políticas públicas sectoriales del Ecuador,
Ecuador (2011)
5. Rogers, E.M.: A prospective and retrospective look at the diffusion model. J. Health
Commun. 6(9), 13–19 (2004)
6. Vergara C., Osorio J.: La difusión de políticas. Estado del arte y contribuciones para la
disciplina de América Latina. In: Política: Revista de Ciencia Política, 2, vol. 54, pp. 235–
254. Universidad de Chile, Chile (2016)
7. SENPLADES: Guía Metodológica de Planiﬁcación Institucional. SENPLADES, Ecuador
(2011)
8. Secretaría Nacional de Planiﬁcación y Desarrollo (SENPLADES). http://www.planiﬁcacion.
gob.ec/mision-vision-principios-valores/
9. Asamblea Constitucional del Ecuador: Constitución de la República del Ecuador Ecuador
(2008)
10. SENPLADES: Plan Nacional para el Buen Vivir 2013–2017. SENPLADES, Ecuador (2013)
11. Rodríguez, O.: El estructuralismo latinoamericano. CEPAL, México (2006)
12. Ocampo, J.: New economic thinking in Latin America. J. Latin American Stud. 22(1-2),
169–181 (1990). Cambridge University Press
13. FitzGerald, V.: La CEPAL y la teoría de la industrialización. In: Revista de la CEPAL,
Número Extraordinario, pp. 47–61. CEPAL (2017)
14. CEPAL: Curso Neoestructuralismo y corrientes heterodoxas en América Latina. CEPAL
(2015)
15. Messner D., Stamer, J: Competitividad Sistémica. In: Revista Nueva Sociedad, vol. 42,
pp. 72–87. Argentina (1994)
16. Esser, K., Hillebrand W., Messner D., Meyer-Stamer, J.: Competitividad Sistémica, nuevo
desafío para las empresas y la política. In: Revista de la CEPAL, 59, pp. 32–52. Santiago de
Chile (1996)
1144
T. Aguilera et al.

17. Esser, K., Hillebrand, W., Messner, D, Meyer-Stamer, J.: Competitividad sistémica.
Competitividad internacional de las empresas y políticas requeridas. Instituto Alemán de
Desarrollo, Berlín (1994)
18. Asamblea Nacional del Ecuador: Código Orgánico de la Producción, Comercio e
Inversiones, Ecuador (2010)
19. Consejo Sectorial de la Producción: Agenda para la Transformación Productiva de Ecuador
2010–2013, Quito, Ecuador (2010)
20. Secretaría Técnica del Sistema Nacional de Cualiﬁcaciones Profesionales. http://www.
cualiﬁcaciones.gob.ec/objetivos-estrategicos-2/
21. Superintendencia de Compañías del Ecuador: Clasiﬁcación de las PYMES de acuerdo a la
normativa implantada por la Comunidad Andina en su Resolución 1260 y la legislación
interna vigente, Ecuador (2010)
22. SENPLADES: Propuesta de Lineamientos Generales para la Planiﬁcación Territorial
Descentralizada, Ecuador (2011)
The Diffusion of Public Policies on Technical Training
1145

Open Government and Citizen Participation
in the Web Portals of Ecuador GADM
Patricia Henríquez-Coronel(&)
, Jennifer Bravo-Loor,
Enrique Díaz-Barrera, and Yosselin Vélez-Romero
Universidad Laica Eloy Alfaro, 130802 Manta, Manabí, Ecuador
patricia.henriquez@uleam.edu.ec, jenimarb@gmail.com,
lupitavelezromero@gmail.com,
enrique_diazb@hotmail.com
Abstract. This paper presents the results of an investigation that analyzed the
services of citizen participation and open government offered by the municipal
governments of Ecuador in their web pages. It is a study of quantitative and
descriptive approach that has studied a sample 121 web portals using a rubric
designed by researchers and evaluated by experts. Heading evaluated the
presence of services and tools for citizen participation (Nine in total). The most
relevant results indicate that citizen participation services offered are minimal
compared to the ﬁve forms of citizen participation contained in the Act of
Citizen Participation, portals have moderate information about participatory
budget and an incipient use of the empty chair. Open governance indicators such
as perceived quality of public services are not listed on any country’s municipal
website and others like the number of existing virtual courses are just outlined in
two portals. The incipient degree of citizen participation suggests that the
municipal e-government in Ecuador has not reached the transactional level,
characterized by UNESCO for two-way communication between government
and citizens. Web portals of municipalities in the Sierra region have a slightly
higher development compared to that of the other regions.
Keywords: Open government  E-government  Citizen participation
Local government
1
Introduction
1.1
E-Government, Open Government and Citizen Participation
The Electronic Government or e-Government refers to the use of Information and
Communication Technologies (ICT) by public institutions in order to increase the
efﬁciency and effectiveness in the processes of public management and, in turn, to
improve transparency in information, procedures and processes in the public sector, thus
also ensuring citizen participation [1]. Various authors [2–5] include public participation
as a key indicator when evaluating the achievements in e-government. Public admin-
istrations emphasizes citizen participation as an element that strengthens its manage-
ment under the so-called open government. An open government is one that establishes
a constant conversation with citizens in order to hear what they say and ask, who makes
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7_109

decisions based on their needs and takes into account their preferences, which facilitates
collaboration of citizens and public ofﬁcers in the development of its services, and
communicating all the decision taken in an open and transparent manner [6].
According to Gonzalez [7], an open government is a form of government where all
people: (1) Have access to the information that interests them. (2) Where you can
participate in decision making at all levels. (3) Ensures that ofﬁcials responsible for
decisions and actions are held accountable and (4) uses technology to promote all goes
above values.
Open government is not only a strategy that governments can adopt. In order to
endure and to be inserted into a system, you need a profound change in the paradigms
of governments and especially in public administrations. It begins with the necessary
strengthening of democratic factors, avoiding power concentration and; creating spaces
of citizen participation [8].
1.2
Ecuadorian Legal Framework for Open Government
in the Municipality
Citizen participation, the rights to information and to an active democracy within the
constitutional framework are cross-cutting principles that tend ultimately to the Citizen
Power construction [9] reﬂected in the Constitution of 2008 in Ecuador. The municipal
government, known in Ecuador as Decentralized Autonomous Municipal Government
(from now on GADM) is the closest government to the citizen which takes the brunt of
powers relating to open and close government.
The Organic Code of Territorial Organization, Autonomy and Decentralization
(hereinafter COOTAD) [10], in its articles establishes the obligation of the GADM to
implement citizen participation systems (Art. 3) and to provide electronic services in
line with technological development 363).
Another Act establishing guidelines on open government and citizen participation is
the Organic Law of Transparency and access to Public Information (LOTAIP) [11]. This
law provides in Article 7 that for transparency in the administration, all State institutions
must provide through an information portal or website, issues of collective interest.
The Act of Citizen Participation [12] provides at least ﬁve forms of participation:
(a) A public Hearings to address statements or submissions and to support decisions or
actions of government (Article 73), (b) Empty Chair establishes the mandate for
GADM, contemplating the empty chair sessions in order to ensure public participation
in the debate and decision-making on matters of general interest (Art. 77) (c) Audits, as
a tool for controlling public administration at all levels of government, private insti-
tutions that manage public funds, and natural or legal persons from the private sector
who provide services or perform activities of public interest (Article 78) (d) Observa-
tories, with the objective of developing diagnoses, and reports with independence and
technical criteria, in order to promote, evaluate, monitor and monitor compliance with
public policies (Article 79) (e) Advisory councils, which are advisory mechanisms
composed of citizens, or by civil organizations that constitute spaces and consultation
bodies. The authorities or joint bodies may at any time convene such councils. Its
function is merely advisory.
Open Government and Citizen Participation
1147

Finally, the National Secretary of Public Administration formulated the National
Plan for E-Government 2014–2017 (from now on PNGE) [13]. This plan is conceived
as “a guiding principle of all automation efforts of the State and frame of reference,
inspiration and projection for the various parts involved.” The plan aims to achieve an
open, close to the people and efﬁcient government. Among the indicators of close and
open government, this plan includes mechanisms for evaluating citizen perception and
mechanisms for citizen participation, among others.
A review of research on e-government and open government in Latin America
reveals signiﬁcant progress in terms of public information and transparency of
municipal governments and instead limited citizen participation initiatives.
Arias and Laica [14] conducted an investigation which analyzes the state of
e-government in Ecuador by the Electronic Government Development Index of the
United Nations (EGDI). According to United Nations reports cited by the authors,
Ecuador would occupy the 83rd place in the ranking with an index of 0.5053.
Líppez-De Castro and Alonzo [5], by exploring the web portals of the 32 capitals of
various department municipalities in Colombia, assessed the active participation, public
consultation and information provided. The results suggest that the information they
provide is not understandable to citizens; it has accountability, but does not provide
tools for citizen feedback. As for active participation there is a timid inclusion of
participation tools and if any, its use is limited.
Rea [15] makes a comparative study between Colombia, Ecuador, Peru, Venezuela
and Bolivia regarding progress in electronic government which uses as a framework the
e-government indicators proposed by the United Nations Department of Economic and
Social Affairs (UNDESA). Regarding e-Government in Ecuador it notes that “started
its implementation phase of technology projects oriented to its citizenship, many of the
efforts have focused on simplifying procedures and automation to generate concrete
beneﬁts for citizen through online services”.
Robayo [16] conducted a study where he analyzed the situation of Electronic
Government in Ecuador and its state of maturity regarding the framework of the
National Plan of Electronic Government (PNGE) 2014–2017. The results on open
government say “minor achievement goals in open government evidence, which could
indicate that they have not had the necessary attention from those responsible for
implementing these initiatives.”
The Secretary of Public Administration of Ecuador [17] points out that according to
the world ranking of e-government of the UN trough (EGDI), the agency positioned
Ecuador for the 2016 ranking number 74 with an index of 0.56, which meant climbing
28 places from 2012 and be above the world average of 0.49”.
Another study on citizen participation in municipal governments is the Infopar-
ticipa Map (http://www.infoparticipa.com/index/mapa/), which evaluates the quality
and transparency of local public communication since 2012 in Catalonia and from 2016
includes Ecuador. A marker called an Infometer places each municipality on a scale of
1 to 100 according to the evaluation of its services. On the day of its consultation, an
emblematic GADM such as Quito’s one shows 50, 94% compliance with the
indicators.
1148
P. Henríquez-Coronel et al.

This paper shows results of research on the tools, procedures and forms of citizen
participation present in web portals of the municipal governments (GADM) of Ecuador
as part of the concept of open government.
2
Method
The research whose partial results are presented, responds to a quantitative approach
[18]. Its purpose was to analyze a sample of 121 GADM web portals from universe of
221, to describe e-government services in three areas according to Cerezo [2]: infor-
mation, citizen participation and transactions. This paper shows the results for the
citizen participation area.
Stratiﬁed random sampling was in response to the four regions in which Ecuador is
divided: Sierra, Costa, Amazonía and Galapagos. In the Sierra a total of 47 GADM was
chosen, on the Costa a total of 45, in the Amazon 29 and a single GADM in Galapagos.
Information on the total number of municipal government’s per area was obtained from
the INEC [19] and lists were set up with the names of municipalities and the URL of
each municipal website. All web portals for the selected GADM were downloaded and
housed in local storage to ensure that the traits that were analyzed were stable over the
days that the analysis lasted.
The instrument used for the analysis was a rubric that was constructed starting from
the proposals of indicators identiﬁed in Cerezo [2] PNGE [13], Esteves [3], LOTAIP
[11] Law on Citizenship Participation [12], Naser [4], COOTAD [10], Líppez-De
Castro and Alonzo [5], Bersano [20]. The section has a total of 32 indicators, including
9 that refer to open government and citizen participation: Citizen networks, Access to
the Empty Chair, Referendum, Chat, Online survey, Public Services Rating, Com-
plaints and Suggestions Box, Training and Participatory budget.
Once built, the rubric was evaluated by three experts. The ﬁnal version was used by
researchers during four weeks for the analysis of each of the aspects evaluated. A total
of 160 h was used for sample analysis portals. Data was collected with the elaborated
rubric which was processed using software for descriptive statistics.
3
Results
3.1
Citizen Networks
Citizen networks refer to online democracy under the Act for Citizenship participation
in its Article 101, according to which the GADM should have asynchronous services
such as forums to allow discussion of issues affecting citizenship and whose deliber-
ations must be present. 99% of the websites analyzed does not have discussion forums.
1% belonging to the Costa region includes a link to forums but is unavailable or does
not work. In the web portal of Ibarra GADM there is an advertisement where the
citizens are invited to dialogue with the mayor every Tuesday at 8:00 am in the Cultural
Center “El cuartel” (Fig. 1).
Open Government and Citizen Participation
1149

3.2
Access to the Empty Chair
The Act on Citizen Participation in Article 77 refers to the empty chair access, as the
opportunity for one or more representatives of the people of occupying the chair that is
empty at meetings of the GADM. According to article 311 of the COOTAD, each
GADM will establish the mechanisms of participation of the empty chair. Once
accredited by the GADM secretary ofﬁce, the citizen who occupies the empty chair
may participate with voice and vote in the discussions and decision-making. However,
most Web portals (96%) does not publish calls for its citizens to access it, compared to
3% that contains calls and 1% in which this option is not available, when assessing the
portals. The GADM of Sucúa, Morona Santiago province in the east of the country,
presents the calls to the regular sessions of the council in chronological order. When
making a call, the agenda of the day appears. Calls are published four days before the
session to allow citizens the time required to formulate the request of empty chair,
however, the option of online application is not presented nor additional information on
the ﬁgure of participation of the empty chair is given.
In the Sierra, the GADM of Cañar also has access to the empty chair, but does not
contain the calls in chronological order, neither can you verify the date of the call, since
it contains only the date in which the session is to be performed.
In the Costa Region, the webpage of the Decentralized Autonomous Government
of Manta provides the online application for the empty chair, and it links to a down-
loadable PDF ﬁle containing the data for each year the service was offered. This annual
ﬁle includes all requests made by citizens to occupy the empty chair, the resolutions in
which the participation is agreed and the plenary act.
3.3
Popular Consultation
Article 20 of the Citizenship Participation Act, referring to popular consultation,
convened by the decentralized autonomous governments mentions that it is a faculty of
the municipal GAD to conduct popular referendums, as long as the decision is certiﬁed
by three quarter parts of the members of council. These consultations may not relate to
matters relating to taxes, public expenditure of the central government or the political
and administrative organization of the country. However, as a participatory mechanism,
Fig. 1. Citizen networks.
1150
P. Henríquez-Coronel et al.

it empowers citizens to raise proposals, which could be used for municipal GAD to
promote them.
The consultation to the citizenship seems to have little root in the analyzed sample.
98% of the portals do not include this option. Of the ﬁve GADM portals containing a
referendum when reviewing, four are located in the Sierra (Ibarra, Quito, Salcedo,
Guaranda) and one in the Amazon (Pablo Sexto).
Consulted issues relate primarily to public services, as the case of GADM of Pablo
Sexto, where citizens are consulted regarding garbage collection schedules. It is pos-
sible to display the results of the query in the same bar. An interesting fact is that the
revision of September 2nd consultations shows it has a total of 22 votes, on September
28th the vote count hadn’t changed, which means that citizens do not participate
actively in the portal web of the GADM.
3.4
Chats
It is an instant communication between citizens and an ofﬁcial of the institution to
express themselves or to clarify its opinion in one or more topics. In the observed
GADM, as you can see in Fig. 2, only 29 web portals have the service, however, the
answer is not immediate. These portals belong to the Costa (1), to the Sierra (23) and
Amazonia (5).
Moving onto the chat service, they had a different interface on each website. Some
very simple, requiring only a username and email to participate, and others a little more
sophisticated allowed access to communicate with a department of the government.
When trying to contact through the chat of the Quito GADM on the second and
26th of September, the message could not be sent by ﬂaws in the interface. The GADM
of Logroño asks the user to verify that he is human, and when doing it an error pops up
in the web and prompts the user to try again later or consult the administrator.
Fig. 2. Chats.
Open Government and Citizen Participation
1151

3.5
Online Surveys
The survey is a technique by which the GADM can learn about the opinions of citizens
on an issue; however, 96% of municipalities do not contain online surveys. A 2% of the
websites analyzed show surveys that are not available and only 2% offer participation
through online surveys.
The GADM Taisha, in the province of Morona Santiago, carries out a survey in
order to collect data from people who have visited the site. The survey results can be
viewed in the “View” option. In the review of September 28 the survey indicates that
75% do not know the county. It would be desirable for it to conduct the user to a tourist
information section, but the portal does not currently offer it.
The GADM Ibarra in the province of Imbabura, surveys citizens on preference
schedules for the evening walk cycle, providing four different schedules so that citizens
can choose according to their convenience. The results are visible online.
3.6
Rating of Public Services
This tool would allow a citizen to assess the quality of services offered by the GADM
in order to meet the shortcomings, to improve and to provide better service to its
inhabitants. None of the reviewed portals have this option.
The GADM Quito, belonging to the region Sierra gives citizens the opportunity to
assess the service provided by the web portal and describe and qualify through a survey
the page layout, easiness to navigate the site and ease of access to information.
3.7
Complaints and Suggestions Box
According to the data analyzed, 13% of the sample analyzed uses complaints and
suggestions box and requires user authentication, we assume that is to follow up on the
complaint. Instead, 2% receives complaints anonymously and 85% of the sample does
not have this tool.
The GADM San Cristóbal of Galapagos has a mailbox of complaints and sug-
gestions presented in a very detailed structure where the user can choose the service
needed, the place where the service is needed or where the incident has happened and
also the option to describe it.
3.8
Training
Trainings are activities aimed at the development and strengthening of certain skills on
the citizenship, they are under Articles 54 and 55 of COOTAD. This research has
considered important that the web portals had a link with information about this and
even to allow direct registration of interested citizens. 91% of GADM don’t have
citizen training programs, 7% has it, but registration must be made by going to the
headquarters of GADM and only 2% has information and inscriptions through the web.
The GADM of Manta publishes information and allows the user to register from the
website in the training titled “councilman for a day”. This training activity lets people
between 16 and 22 years old to accompany the councilman of the county in its
1152
P. Henríquez-Coronel et al.

activities of legislation and audit to promote amongst youth the empowerment of public
affairs and at the same time to participate in electoral contests.
The GADM of Ibarra offers citizenship participation in painting competitions,
selﬁes, among others, in which they show their skills in those areas. There are
downloadable .pdf ﬁles containing the rules of participation.
3.9
Participatory Budgeting
The participatory budget, according to article 67 of the Act of Citizen Participation, is
the process in which citizens, individually or through social organizations, contribute
voluntarily in the decision-making process on state budgets, in meetings with elected
and designated authorities. Only 11% of the sample of web portals analyzed has calls
for participatory budgeting. The GADM of Lago Agrio, publishes a timeline for citi-
zens’ assemblies of participatory budgeting detailing the time, date and place where the
meetings take place. Subsequently, the GADM published on its website, as news,
perceptions and orders of citizens in the meeting held. The GADM of Quito publishes
on its news website dates for socialization of the participative budget in certain areas.
Both the GADM of Ibarra and the one of Manta have a section where citizens can ﬁnd
ﬁles on participatory budget in a PDF format. For the rest of the municipal portals
analyzed, we found news related to the socialization of the participatory budget with
the communities. Probably, the calls for citizen participation to discuss budgets have
been made by other means of communication other than the web portal.
4
Discussion
The results of this study conﬁrm the ﬁndings of Arias and Laica [14], Rea [15] and
Robayo [16] which show that in the electronic government of the GADM in Ecuador
currently predominates information and some online services, speciﬁcally, automation
of procedures and in much lesser degree citizen participation services. According to the
four levels of e-government established by UNESCO, Ecuador’s GADMs would not
have reached the transactional level of two-way communication between government
and citizens. Out of the ﬁve forms of citizen participation contained in the Act of
Citizen Participation, web portals realize a moderate application of participatory bud-
geting and an incipient use of the empty chair. Tools such as surveys, observatories or
advisory councils do not appear on municipal web portals. Other investigations should
determine whether the GADM use different means rather than the web portal to favor
citizen participation.
Some indicators of close government included in the PNGE such as the index of
perception of quality of public services do not appear in any municipal portal of the
country. Others such as the number of existing virtual courses are hardly reviewed in
two web portals. In the midst of the incipient situation of citizen participation described
above, the GADMs of the Sierra region seem to have a higher degree of development
compared to the Coast, Amazon and Galapagos. Similarly, there are important dif-
ferences between the services of citizen participation offered by GADM in major cities,
such as Quito, and GADM from rural areas or small towns. This reality conﬁrms the
Open Government and Citizen Participation
1153

numerous territorial inequities that have characterized Ecuador and have been widely
reported in the National Plan for Good Living (SENPLADES) [21].
The legal framework of Ecuador on citizen participation is lengthy and vigorous
and is aimed at creating a true citizen power, the investment made by the State in
network infrastructure to facilitate e-government has been notorious, placing Ecuador
among the intermediate countries in the region. However, indicators of both, citizen
participation and open government on the web portals of Ecuadorian’s GADM are low.
Citizen training schools should promote citizen empowerment so that they can prepare
for the use of all the participation tools provided for in the laws, only this way it will be
possible to move from a government where the citizen has information to another in
which he also participates in decision-making at all levels.
References
1. United Nations: United Nations E-government Survey 2012: E-government for the People.
United Nations (2012)
2. Cerezo, J.M.: The challenges of the administration in Spain. Paper presented at the II Jornada
Democracia digital, eAdministración y participación ciudadana, Barcelona, España, Marzo
2005. (in Spanish)
3. Esteves, J.: Analysis of the development of municipal E-government in Spain. IE Working
Paper, Madrid (2005)
4. Naser, A.: Electronic Government, Indicators. Latin American and Caribbean Institute for
Economic and Social Planning (2009). (in Spanish). https://www.cepal.org/ilpes/noticias/
paginas/0/40660/alejandra_naser_INDICADORES.pdf. Accessed 26 June 2017
5. Líppez-De, C.S., García, A.R.: Citizens and E-government: citizen orientation in municipal
websites of Colombia for the promotion of participation. Universitas Humanística 82, 279–
304 (2006)
6. Sánchez, J.: Citizen participation as an instrument of open government. Espacios Públicos
18, 51–73 (2015). (in Spanish)
7. González, E.: Open Government Model. Transparency, citizen participation and collabo-
ration (2016). (in Spanish). http://190.104.117.163/2016/Agosto/gobdi/contenido/ponencias/
Eduardo%20Gonzales/Modelo%20de%20gobierno%20abierto.pdf. Accessed 20 June 2017
8. Figueroa, M.: Citizen participation in the context of open government and a path to public
co-creation. Paper presented at the XXI Congreso Internacional del CLAD sobre la Reforma
del Estado y de la Administración Pública, Santiago, Chile, Noviembre 2016. (in Spanish)
9. Gallegos, F., Espinosa, A.: Occupying the empty chair, representation and participation in
the post-constitutional adaptation of Ecuador. Cuadernos del Cendes 29(81), 109–140
(2012). (in Spanish)
10. Act No. 0: Organic Code of Territorial Organization, Autonomy and Decentralization.
Asamblea Nacional, Quito (2010). (in Spanish)
11. Act No. 24: Organic Law of Transparency and access to Public Information. Asamblea
Nacional, Quito (2004). (in Spanish)
12. Act No. 0: Act of Citizen Participation. Asamblea Nacional, Quito (2011). (in Spanish)
13. National Secretary of Public Administration: Electronic Government Plan 2014–2017. (in
Spanish).
http://www.gobiernoelectronico.gob.ec/wp-content/uploads/downloads/2017/09/
Plan-Gobierno-Electronico-V1.pdf
1154
P. Henríquez-Coronel et al.

14. Arias, J., Laica, S.: Analysis of the implementation of Electronic Government in Ecuador.
Thesis, Escuela Superior Politécnica Del Litoral (2015). (in Spanish)
15. Rea, C.: Analysis of the development of E-government in the Andean states based on the
Electronic Government Development Index (IDGE) for the period 2008–2013. Thesis,
Escuela Politécnica Nacional (2016). (in Spanish)
16. Robayo, M.: Electronic government in Ecuador: Analysis of its implementation in the
framework of the National Plan 2014–2017. Thesis, UPS (2017). (in Spanish)
17. National Secretary of Public Administration. http://www.administracionpublica.gob.ec/plan-
nacional-de-gobierno-electronico-2/
18. Hernández, R., Fernández, C., Baptista, P.: Investigation Methodology. McGraw-Hill,
México D.F. (2014). (in Spanish)
19. National Institute of Statistics and Census. http://www.ecuadorencifras.gob.ec/estadisticas/?
option=com_content&view=article&id=300
20. Bersano, L.: Best Practices for Electronic Government in Latin America. What indicators are
used to qualify them? What is its relevance measuring the level achieved on the road to
E-society? El autor, Argentina (2006). (in Spanish)
21. National Secretary of Public Administration. http://www.buenvivir.gob.ec
Open Government and Citizen Participation
1155

Author Index
A
Abreu, António, 86, 856
Acosta, Tania, 538, 602
Acosta-Vargas, Patricia, 538, 602
Acurio, Andrés, 593
Adewumi, Adewole, 95, 105
Aguilar, Jose, 427
Aguilera, Tania, 1135
Aguirre-Munizaga, Maritza, 663
Al Osman, Hussein, 309
Alcarria, Ramón, 77, 319, 756
Aldana Montes, José F., 719
Alén-González, Elisa, 86
Alfaro, Andrés, 700
Alhassan, J. K., 95, 105
Angelopoulos, Constantinos Marios, 804
Angula, Nikodemus, 835
Aragón-Puetate, Johnny Alejandro, 1044
Arciniegas, Stalin, 973, 1004, 1022
Arellano, Brian, 973
Arias Almeida, Gloria I., 688
Armas, Carlos Montenegro, 264
Augusto, Maria Fernanda, 129, 234, 242
B
Bacilio, Jacqueline, 234
Barba-Guamán, Luis, 614
Barbosa, José Geraldo Pereira, 467, 867
Barragán, Geovani, 178
Barros, Thiago, 387
Batallas, Daniela, 1135
Bayas, M. M., 343
Bello, Alejandro, 397
Benac, Marcos Azevedo, 20
Benavides, Diego Eduardo, 497
Blanco, Monica, 677
Bocchi, Yann, 77
Bohórquez, Emanuel, 160
Bordel, Borja, 319, 756
Bravo, Leonardo Emiro Contreras, 927
Bravo-Loor, Jennifer, 1146
Budzianowski, Zbigniew, 476, 874
Buele, Jorge, 583, 593
Buenaño-Fernandez, Diego, 138
Bustamante, Leslie, 387
Byron Oviedo, B., 719
C
Cabezas, Cristian F., 688
Cabrera-Goyes, Edwin, 823
Caiche, William, 160
Calderón, Frankz Alberto Carrera, 688
Calderón, Manolo Paredes, 508
Capelo, David Chilcañán, 897, 907
Caranqui, Víctor, 355
Carrera, Enrique V., 367
Carrera, Ricardo, 367
Carrión, Pablo Torres, 148
Carrizo, Dante, 700
Carvalho, João Alvaro, 50
Carvalho, João Vidal, 856
Castellanos, Esteban X., 573
Castro, John W., 508
Cerón, M., 284
Chacón, Galo, 573
Chanatasig, Henry, 252
Chandi, Lizeth, 62
Chavarria, Johnny, 242
Chicaiza, Daniela Benalcázar, 897, 907
Chuquitarco, Alexandra, 583, 593
Colchado, Geraldo, 777
Contreras-Bravo, Leonardo Emiro, 275
Costa, Elisangela, 3
Costa, Nuno, 745
Cruz, Luis J., 387
© Springer International Publishing AG 2018
Á. Rocha and T. Guarda (eds.), Proceedings of the International Conference on Information
Technology & Systems (ICITS 2018), Advances in Intelligent Systems and Computing 721,
https://doi.org/10.1007/978-3-319-73450-7
1157

D
da Silva, André Constantino, 631
Damaševičius, Robertas, 95, 105, 115
Datzania Villao, B., 710
de Castro Silva, Sandro Luís Freire, 20, 467,
867
de Oliveira Ormond, Eduardo, 919
de Oliveira, Daniel Ribeiro, 3
de Oliveira, Saulo Barbará, 3, 10, 20, 40, 191,
919
de Paula Junior, Iális C., 387
del Pilar Villamil, Maria, 443
DesClouds, Poppy, 883
Díaz-Barrera, Enrique, 1146
Dlodlo, Nomusa, 835
Dludlu, Mzomba Nelson, 959
do Nascimento, Ricardo Luiz Schiavo, 10
Dominguez, David, 407
Durand-Bush, Natalie, 883
Duvoboi, V. M., 343
E
Egas, Fabricio D., 563
El Saddik, Abdulmotaleb, 309, 883
Entriel, Aparecida Laino, 10
Epifânio, Flávio, 294
Escobar, Ivón, 593
Espinel, Patricio, 688
Espinosa, Edison, 252
Espinosa, Jessy, 573, 583
Espinosa-Gallardo, Edison, 688
Espinoza, John, 573, 583
Estrada, Yovany Salazar, 129
F
Fajardo, Jorge I., 387
Faria, Ada Guagliardi, 40
Fernández, David, 77
Fernández-Carmona, M., 284
Ferré, Xavier, 813
Ferreira, Juan M., 252
Flores, Pamela, 497
Florez, Hector, 333
Fonseca C., Efraín R., 508
Freire, Fernanda Maria Pereira, 631
Freire, Francisco Campos, 1044
Fuertes, Walter, 497
Fujarewicz, Krzysztof, 417, 484
G
Galarza, Eddie D., 563
Galarza, Eddie E., 563
Galarza-Ligña, Viviana, 1055
Garbacik, Małgorzata, 476
García-Santillán, Iván, 355
Garrido, Fernando, 355
Garzon-Goya, Mayra, 663
Garzozi, René, 992
Gómez, Andrea, 77
Gómez, Pablo, 148
Gomez-Torres, Estevan, 30
Gonçalves, Antônio Augusto, 20, 467, 867
Gonzalez, Karen, 677
González, Mario, 407
González-Eras, Alexandra, 148
Granda, Pedro, 355
Grandhi, Srimannarayana, 788
Grilo, Carlos, 745
Grimon, Francisca, 1022
Gualotuña, Tatiana, 62
Guarda, Teresa, 129, 160, 234, 242
Guerra, Laura, 973, 1004, 1022
Guerrero, Lucía, 573, 583
Guevara, Juan, 796
Guffanti, Diego, 767
Gutierrez-Franco, Brenda, 1125
H
Henríquez-Coronel, Patricia, 1146
Herrera, Nelson Herrera, 30
Hidalgo, Chrystian López, 907
Hoda, Mohamad, 309
Hurtado, Jhonatan Salazar, 897
I
Ibadango-Tabango, María Fernanda, 1115
Inthiran, Anushia, 531
Iturrioz, Teresa, 756
Izurieta, Santiago, 367
J
Jácome-Guerrero, Santiago P., 688
Jaksik, Roman, 484
Jakubczak, Michał, 417
Jara, Antonio, 319
Jara, Antonio J., 77
Jaramillo-Alcázar, Angel, 552
Jara-Olmedo, Aníbal, 653
Jiménes, Karina, 948
1158
Author Index

Jiménez, Alberto, 170, 948
José Sánchez, A., 710
K
Kolog, Emmanuel Awuni, 453
Kundig, Stephane, 804
L
Laamarti, Fedwa, 883
Larco, Andrés, 640
Lavín, José M., 653
León, Marcelo, 129, 160, 234, 242
Lídice Haz, L., 710
Liberato, Dália, 86
Liberato, Pedro, 86
Lopes, Jose J., 224
López-Golán, Mónica, 1033
López-López, Paulo Carlos, 1033
López-Paredes, Marco, 983
Loza-Aguirre, Edison, 264
Lozano, Leidy Alexandra, 443
Luján-Mora, Sergio, 30, 138, 518, 538, 552,
602, 640, 937
M
Macêdo, Fábio Carlos, 40
Machado, Jose, 224
Machado-Rosales, Hazel, 1125
Macías, Jeyco, 992
Macredie, Robert D., 531
Manso, Miguel Ángel, 756
Marcillo, Diego, 813
Márquez-Domínguez, Carmelo, 1065, 1084
Martínez, Danilo, 62, 767, 813
Martinez, H. David, 796
Martins, Carlos Henrique Fernandes, 20, 467,
867
Maskeliūnas, Rytis, 95, 105, 115
Mazon, Luis, 129
Medina, Ana Culqui, 1076
Medina-Pazmiño, Wilson, 653
Melgar, Andrés, 397, 777
Mera-Moya, Victoria, 387
Mila, Andrea, 1135
Misra, Sanjay, 95, 105, 115
Mndebele, Comfort B. S., 959
Mndzebele, Nomsa M., 959
Molano, Jose Ignacio Rodriguez, 927
Molina, Germán, 77
Montaghami, Valeh, 309
Montalván, Néstor, 992
Montenegro, Carlos, 178, 640
Montenegro-Cevallos, Armida Mariela, 1115,
1125
Montero, Calkin Suero, 453
Morales, Marco Segura, 264
Moritz, Manuel, 200
Moscoso-Zea, Oswaldo, 30, 518
N
Narváez, Luis David, 1022
Nebro, Antonio J., 719
Niranjan, S. K., 376
O
Oduh, Isaac U., 115
Oguntoye, R. T., 105
Oleksy, Wojciech, 476, 874
Olivares, Gustavo, 919
Ordóñez-Camacho, Diego, 823
Orosa, Miguel Ángel, 1094
Orozco, Jaime, 242
Orozco, Walter, 234, 242
P
Palacios-Pacheco, X., 138
Paladines, José, 624, 767
Pantoja, Odette, 407
Peluffo-Ordoñez, Diego, 355
Peña, Humberto, 160
Pérez, Hugo, 234, 242
Pérez, Marina, 756
Pestana, Gabriel, 294
Pilatásig, Marco, 573, 583, 593
Pilatásig, Pablo, 593
Pincay, Jhonny, 948
Pires, Isaque Miguel, 631
Pizarro, Daniel, 397
Pojda, Katarzyna, 417
Polzonetti, Alberto, 211
Pruna, Edwin, 593
Puentes-Rivera, Iván, 1033
Pusdá, Marco, 355
Q
Quishpe, Santiago, 1004
R
Ramírez, Álvaro Cevallos, 1104
Ramírez, Jaime, 624
Ramos-Gil, Yalitza, 1084
Ramos-Gil, Yalitza Therly, 1065
Ratté, Sylvie, 846
Reascos, Irving, 50
Reascos-Trujillo, Amparo, 1055
Reátegui, Ruth, 846
Redlich, Tobias, 200
Reyes Ch., Rolando P., 508
Author Index
1159

Riofrio, Guido, 427
Rivas-Echeverria, Francklin, 973
Rivas-Echeverría, Francklin Iván, 1044
Rivas-Trujillo, Edwin, 275
Rivera, Abdon Carrera, 937
Rivera-Imbaquingo, Stalin, 1055
Rivero, Dulce, 1004
Rivero-Albarrán, Dulce, 973
Rizzo, Gianluca, 319
Roa, Henry N., 264
Rocha, Álvaro, 86, 856
Rodríguez, Germán Eduardo, 497
Rodríguez, Homero, 992
Rodriguez, Jose Ignacio, 677
Rodríguez-Cisneros, Luz Marina, 1115
Rodríguez-Molano, José Ignacio, 275
Rolim, Jose, 804
Romero-Ortega, Aldo, 1084, 1094
Rosero, Shendry, 170
Rovira, R. H., 343
S
Saa, Pablo, 518
Salvador-Ullauri, Luis, 538, 602
Sanchez, Daniel, 333
Sánchez, Elizabeth Granda, 1076
Sánchez, Milton Escobar, 897, 907
Sandoval, F., 284
Sappa, Angel D., 732
Sarmiento, Andrea, 767
Sarmiento-Guerrero, Christian, 614
Sasgratella, Matteo, 211
Serrano, Martin, 77
Silva, Carlos, 745
Silva, Carlos A., 796
Silva, Catarina, 62
Silva, Franklin, 573, 583
Silva, Franklin M., 563
Silva, Michele Mendes Hiath, 191
Solís-Avilés, Evelyn, 663
Student, Sebastian, 417
Suárez, Patricia L., 732
Świerniak, Andrzej, 417
T
Tapia, Víctor, 573
Tapia-Leon, Mariela, 937
Tasiguano-Pozo, Cristian, 653
Tejada-Castro, Mariuxi, 663
Tigse, Jenny, 583, 593
Tkacz, Ewaryst, 476, 874
Toasa, Renato, 796
Toivonen, Tapani, 453
Torres, Giovannina, 1135
Torres, Jenny, 497
Torres, W. D., 343
Torres, Washington, 234
Torres-Carrión, Pablo, 614
Torres-Diaz, Juan Carlos, 614
Trojanowska, Justyna, 224
Trujillo, Edwin Rivas, 927
U
Ulloa-Erazo, Nancy, 1065, 1104
Umar, A., 95
Urdiales, C., 284
Uyaguari, Alvaro, 688
V
Vaca, Hugo Pérez, 508
Vaca-Tapia, Ana Cecilia, 1044
Valdiviezo-Diaz, Priscila, 427
Varela, Maria L. R., 224
Vargas, Javier, 796
Velasco, Paola M., 563
Vélez-Romero, Yosselin, 1146
Veloz, Jorge, 745
Vergara-Lozano, Vanessa, 663
Villao, Datzania, 129
Villao, José, 160
Villavicencio, Mónica, 948
Villegas-Ch, W., 138
Vinod, H. C., 376
Vintimilla, Boris X., 732
Vitor, José Avelino, 129
W
Washington Torres, G., 710
Wibowo, Santoso, 788
Wulfsberg, Jens, 200
Y
Yanez, Cesar, 640
Yépez-Reyes, Verónica, 1014
Yukhimchuk, M. S., 343
Z
Zambrano-Vega, Cristian, 719
1160
Author Index


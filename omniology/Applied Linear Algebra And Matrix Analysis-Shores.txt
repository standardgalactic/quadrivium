APPLIED LINEAR ALGEBRA
AND
MATRIX ANALYSIS
Thomas S. Shores
Author address:
COPYRIGHT c
MAY 2000 ALL RIGHTS RESERVED


Contents
Preface
i
Chapter 1.
LINEAR SYSTEMS OF EQUATIONS
1
1.1.
Some Examples
1
1.2.
Notations and a Review of Numbers
9
1.3.
Gaussian Elimination: Basic Ideas
18
1.4.
Gaussian Elimination: General Procedure
29
1.5.
*Computational Notes and Projects
39
Review
47
Chapter 2.
MATRIX ALGEBRA
49
2.1.
Matrix Addition and Scalar Multiplication
49
2.2.
Matrix Multiplication
55
2.3.
Applications of Matrix Arithmetic
62
2.4.
Special Matrices and Transposes
72
2.5.
Matrix Inverses
85
2.6.
Basic Properties of Determinants
96
2.7.
*Applications and Proofs for Determinants
106
2.8.
*Tensor Products
114
2.9.
*Computational Notes and Projects
118
Review
123
Chapter 3.
VECTOR SPACES
125
3.1.
Deï¬nitions and Basic Concepts
125
3.2.
Subspaces
135
3.3.
Linear Combinations
142
3.4.
Subspaces Associated with Matrices and Operators
152
3.5.
Bases and Dimension
160
3.6.
Linear Systems Revisited
166
3.7.
*Change of Basis and Linear Operators
174
3

4
CONTENTS
3.8.
*Computational Notes and Projects
178
Review
182
Chapter 4.
GEOMETRICAL ASPECTS OF STANDARD SPACES
185
4.1.
Standard Norm and Inner Product
185
4.2.
Applications of Norms and Inner Products
192
4.3.
Unitary and Orthogonal Matrices
202
4.4.
*Computational Notes and Projects
210
Review
212
Chapter 5.
THE EIGENVALUE PROBLEM
213
5.1.
Deï¬nitions and Basic Properties
213
5.2.
Similarity and Diagonalization
223
5.3.
Applications to Discrete Dynamical Systems
232
5.4.
Orthogonal Diagonalization
240
5.5.
*Schur Form and Applications
244
5.6.
*The Singular Value Decomposition
247
5.7.
*Computational Notes and Projects
250
Review
259
Chapter 6.
GEOMETRICAL ASPECTS OF ABSTRACT SPACES
261
6.1.
Normed Linear Spaces
261
6.2.
Inner Product Spaces
266
6.3.
Gram-Schmidt Algorithm
276
6.4.
Linear Systems Revisited
286
6.5.
*Operator Norms
295
6.6.
*Computational Notes and Projects
299
Review
306
Appendix A.
Table of Symbols
307
Appendix B.
Solutions to Selected Exercises
309
Bibliography
323
Index
325

Preface
This book is about matrix and linear algebra, and their applications. For many students
the tools of matrix and linear algebra will be as fundamental in their professional work
as the tools of calculus; thus it is important to ensure that students appreciate the utility
and beauty of these subjects, as well as understand the mechanics. One way to do so is to
show how concepts of matrix and linear algebra make concrete problems workable. To
this end, applied mathematics and mathematical modeling ought to have an important
role in an introductory treatment of linear algebra.
One of the features of this book is that we weave signiï¬cant motivating examples into
the fabric of the text. Needless to say, I hope that instructors will not omit this ma-
terial; that would be a missed opportunity for linear algebra! The text has a strong
orientation towards numerical computation and applied mathematics, which means that
matrix analysis plays a central role. All three of the basic components of linear algebra
â€“ theory, computation and applications â€“ receive their due. The proper balance of these
components will give a diverse audience of physical science, social science, statistics,
engineering and math students the tools they need as well as the motivation to acquire
these tools. Another feature of this text is an emphasis on linear algebra as an exper-
imental science; this emphasis is to be found in certain examples, computer exercises
and projects. Contemporary mathematical software makes an ideal â€œlabâ€ for mathemat-
ical experimentation. At the same time, this text is independent of speciï¬c hardware
and software platforms. Applications and ideas should play center stage, not software.
This book is designed for an introductory course in matrix and linear algebra. It is
assumed that the student has had some exposure to calculus. Here are some of its main
goals:
 To provide a balanced blend of applications, theory and computation which em-
phasizes their interdependence.
 To assist those who wish to incorporate mathematical experimentation through
computer technology into the class. Each chapter has an optional section on
computational notes and projects and computer exercises sprinkled throughout.
The student should use the locally available tools to carry out the experiments
suggested in the project and use the word processing capabilities of the com-
puter system to create small reports on his/her results. In this way they gain
experience in the use of the computer as a mathematical tool. One can also en-
vision reports on a grander scale as mathematical â€œterm papers.â€ I have made
such assignments in some of my own classes with delightful results. A few
major report topics are included in the text.
i

ii
PREFACE
 To help students to think precisely and express their thoughts clearly. Requir-
ing written reports is one vehicle for teaching good expression of mathematical
ideas. The projects given in this text provide material for such reports.
 To encourage cooperative learning. Mathematics educators are becoming in-
creasingly appreciative of this powerful mode of learning. Team projects and
reports are excellent vehicles for cooperative learning.
 To promote individual learning by providing a complete and readable text. I
hope that students will ï¬nd the text worthy of being a permanent part of their
reference library, particularly for the basic linear algebra needed for the applied
mathematical sciences.
An outline of the book is as follows: Chapter 1 contains a thorough development of
Gaussian elimination and an introduction to matrix notation. It would be nice to assume
that the student is familiar with complex numbers, but experience has shown that this
material is frequently long forgotten by many. Complex numbers and the basic lan-
guage of sets are reviewed early on in Chapter 1. (The advanced part of the complex
number discussion could be deferred until it is needed in Chapter 4.) In Chapter 2, basic
properties of matrix and determinant algebra are developed. Special types of matrices,
such as elementary and symmetric, are also introduced. About determinants: some in-
structors prefer not to spend too much time on them, so I have divided the treatment
into two sections, one of which is marked as optional and not used in the rest of the text.
Chapter 3 begins by introducing the student to the â€œstandardâ€ Euclidean vector spaces,
both real and complex. These are the well springs for the more sophisticated ideas of
linear algebra. At this point the student is introduced to the general ideas of abstract
vector space, subspace and basis, but primarily in the context of the standard spaces.
Chapter 4 introduces goemetrical aspects of standard vectors spaces such as norm, dot
product and angle. Chapter 5 provides an introduction to eigenvalues and eigenvectors.
Subsequently, general norm and inner product concepts are examined in Chapter 5. Two
appendices are devoted to a table of commonly used symbols and solutions to selected
exercises.
Each chapter contains a few more â€œoptionalâ€ topics, which are independent of the non-
optional sections. I say this realizing full well that one instructorâ€™s optional is anotherâ€™s
mandatory. Optional sections cover tensor products, linear operators, operator norms,
the Schur triangularization theorem and the singular value decomposition. In addition,
each chapter has an optional section of computational notes and projects. I have em-
ployed the convention of marking sections and subsections that I consider optional with
an asterisk. Finally, at the end of each chapter is a selection of review exercises.
There is more than enough material in this book for a one semester course. Tastes vary,
so there is ample material in the text to accommodate different interests. One could
increase emphasis on any one of the theoretical, applied or computational aspects of
linear algebra by the appropriate selection of syllabus topics. The text is well suited to
a course with a three hour lecture and lab component, but the computer related material
is not mandatory. Every instructor has her/his own idea about how much time to spend
on proofs, how much on examples, which sections to skip, etc.; so the amount of mate-
rial covered will vary considerably. Instructors may mix and match any of the optional
sections according to their own interests, since these sections are largely independent

PREFACE
iii
of each other. My own opinion is that the ending sections in each chapter on computa-
tional notes and projects are partly optional. While it would be very time consuming to
cover them all, every instructor ought to use some part of this material. The unstarred
sections form the core of the book; most of this material should be covered. There are
27 unstarred sections and 12 optional sections. I hope the optional sections come in
enough ï¬‚avors to please any pure, applied or computational palate.
Of course, no one shoe size ï¬ts all, so I will suggest two examples of how one might use
this text for a three hour one semester course. Such a course will typically meet three
times a week for ï¬fteen weeks, for a total of 45 classes. The material of most of the the
unstarred sections can be covered at a rate of about one and one half class periods per
section. Thus, the core material could be covered in about 40 class periods. This leaves
time for extra sections and and in-class exams. In a two semester course or a semester
course of more than three hours, one could expect to cover most, if not all, of the text.
If the instructor prefers a course that emphasizes the standard Euclidean spaces, and
moves at a more leisurely pace, then the core material of the ï¬rst ï¬ve chapters of the
text are sufï¬cient. This approach reduces the number of unstarred sections to be covered
from 27 to 23.
In addition to the usual complement of pencil and paper exercises (with selected so-
lutions in Appendix B), this text includes a number of computer related activities and
topics. I employ a taxonomy for these activities which is as follows. At the lowest level
are computer exercises. Just as with pencil and paper exercises, this work is intended to
develop basic skills. The difference is that some computing equipment (ranging from
a programmable scientiï¬c calculator to a workstation) is required to complete such ex-
ercises. At the next level are computer projects. These assignments involve ideas that
extend the standard text material, possibly some experimentation and some written ex-
position in the form of brief project papers. These are analogous to lab projects in the
physical sciences. Finally, at the top level are reports. These require a more detailed
exposition of ideas, considerable experimentation â€“ possibly open ended in scope, and a
carefully written report document. Reports are comparable to â€œscientiï¬c term papersâ€.
They approximate the kind of activity that many students will be involved in through
their professional life. I have included some of my favorite examples of all three ac-
tivities in this textbook. Exercises that require computing tools contain a statement to
that effect. Perhaps projects and reports I have included will be paradigms for instruc-
tors who wish to build their own project/report materials. In my own classes I expect
projects to be prepared with text processing software to which my students have access
in a mathematics computer lab.
Projects and reports are well suited for team efforts. Instructors should provide back-
ground materials to help the students through local system dependent issues. For exam-
ple, students in my own course are assigned a computer account in the mathematics lab
and required to attend an orientation that contains speciï¬c information about the avail-
able linear algebra software. When I assign a project, I usually make available a Maple
or Mathematica notebook that amounts to a brief background lecture on the subject of
the project and contains some of the key commands students will need to carry out the
project. This helps students focus more on the mathematics of the project rather than
computer issues.

iv
PREFACE
Most of the computational computer tools that would be helpful in this course fall into
three categories and are available for many operating systems:
 Graphing calculators with built-in matrix algebra capabilities such as the HP
28 and 48, or the TI 85 and 92. These use ï¬‚oating point arithmetic for system
solving and matrix arithmetic. Some do eigenvalues.
 Computer algebra systems (CAS) such as Maple, Mathematica and Macsyma.
These software products are fairly rich in linear algebra capabilities. They pre-
fer symbolic calculations and exact arithmetic, but will do ï¬‚oating point calcu-
lations, though some coercion may be required.
 Matrix algebra systems (MAS) such as MATLAB or Octave. These software
products are speciï¬cally designed to do matrix calculations in ï¬‚oating point
arithmetic, though limited symbolic capabilities are available in the basic pro-
gram. They have the most complete set of matrix commands of all categories.
In a few cases I have included in this text some software speciï¬c information for some
projects, for the purpose of illustration. This is not to be construed as an endorsement
or requirement of any particular software or computer. Projects may be carried out with
different software tools and computer platforms. Each system has its own strengths. In
various semesters I have obtained excellent results with all these platforms. Students
are open to all sorts of technology in mathematics. This openness, together with the
availability of inexpensive high technology tools, is changing how and what we teach
in linear algebra.
I would like to thank my colleagues whose encouragement has helped me complete this
project, particularly Jamie Radcliffe, Jim Lewis, Dale Mesner and John Bakula. Special
thanks also go to Jackie Kohles for her excellent work on solutions to the exercises
and to the students in my linear algebra courses for relentlessly tracking down errors.
I would also like to thank my wife, Muriel, for an outstanding job of proofreading and
editing the text.
Iâ€™m in the process of developing a linear algebra home page of material such as project
notebooks, supplementary exercises, etc, that will be useful for instructors and students
of this course. This site can be reached through my home page at
http://www.math.unl.edu/~tshores/
I welcome suggestions, corrections or comments on the site or book; both are ongoing
projects. These may be sent to me at tshores@math.unl.edu.

CHAPTER 1
LINEAR SYSTEMS OF EQUATIONS
There are two central problems about which much of the theory of linear algebra re-
volves: the problem of ï¬nding all solutions to a linear system and that of ï¬nding an
eigensystem for a square matrix. The latter problem will not be encountered until Chap-
ter 4; it requires some background development and even the motivation for this prob-
lem is fairly sophisticated. By contrast the former problem is easy to understand and
motivate. As a matter of fact, simple cases of this problem are a part of the high school
algebra background of most of us. This chapter is all about these systems. We will
address the problem of when a linear system has a solution and how to solve such a sys-
tem for all of its solutions. Examples of linear systems appear in nearly every scientiï¬c
discipline; we touch on a few in this chapter.
1.1. Some Examples
Here are a few elementary examples of linear systems:
EXAMPLE 1.1.1. For what values of the unknowns
x and
y are the following equations
satisï¬ed?
x
+
2y
=
5
4x
+
y
=
6
SOLUTION. The ï¬rst way that we were taught to solve this problem was the geometrical
approach: every equation of the form
ax
+
by
+
c
=
0 represents the graph of a straight
line, and conversely, every line in the xy-plane is so described. Thus, each equation
above represents a line. We need only graph each of the lines, then look for the point
where these lines intersect, to ï¬nd the unique solution to the graph (see Figure 1.1.1). Of
course, the two equations may represent the same line, in which case there are inï¬nitely
many solutions, or distinct parallel lines, in which case there are no solutions. These
could be viewed as exceptional or â€œdegenerateâ€ cases. Normally, we expect the solution
to be unique, which it is in this example.
We also learned how to solve such an equation algebraically: in the present case we
may use either equation to solve for one variable, say
x, and substitute the result into
the other equation to obtain an equation which is easily solved for
y
: For example,
the ï¬rst equation above yields
x
=
5
 2y and substitution into the second yields
4(5
 2y
)
+
y
=
6 , i.e.,
 7y
=
 14, so that
y
=
2: Now substitute 2 for
y in the ï¬rst
equation and obtain that
x
=
5
 2(2)
=
1:
1

2
1. LINEAR SYSTEMS OF EQUATIONS
y
x
6
5
4
3
2
1
0
1
2
3
4
5
6
4x + y = 6
x + 2y = 5
(1,2)
FIGURE 1.1.1. Graphical solution to Example 1.1.1.
EXAMPLE 1.1.2. For what values of the unknowns
x,
y and
z are the following equa-
tions satisï¬ed?
x
+
y
+
z
=
4
2x
+
2y
+
5z
=
11
4x
+
6y
+
8z
=
24
SOLUTION. The geometrical approach becomes somewhat impractical as a means of
obtaining an explicit solution to our problem: graphing in three dimensions on a ï¬‚at
sheet of paper doesnâ€™t lead to very accurate answers! Nonetheless, the geometrical
point of view is useful, for it gives us an idea of what to expect without actually solving
the system of equations.
With reference to our system of three equations in three unknowns, the ï¬rst fact to
take note of is that each of the three equations is an instance of the general equation
ax
+
by
+
cz
+
d
=
0: Now we know from analytical geometry that the graph of this
equation is a plane in three dimensions, and conversely every such plane is the graph of
some equation of the above form. In general, two planes will intersect in a line, though
there are exceptional cases of the two planes represented being identical or distinct
and parallel. Hence we know the geometrical shape of the solution set to the ï¬rst two
equations of our system: a plane, line or point. Similarly, a line and plane will intersect
in a point or, in the exceptional case that the line and plane are parallel, their intersection
will be the line itself or the empty set. Hence, we know that the above system of three
equations has a solution set that is either empty, a single point, a line or a plane.
Which outcome occurs with our system of equations? We need the algebraic point of
view to help us calculate the solution. The matter of dealing with three equations and
three unknowns is a bit trickier than the problem of two equations and unknowns. Just
as with two equations and unknowns, the key idea is still to use one equation to solve
for one unknown. Since we have used one equation up, what remains is two equations
in the remaining unknowns. In this problem, subtract
2 times the ï¬rst equation from the
second and
4 times the ï¬rst equation from the third to obtain the system
3z
=
3
2y
+
4z
=
8

1.1. SOME EXAMPLES
3
(1,2,1)
-2
2
1
3
4
-1
0
1
2
x + y + z = 4
4x + 6y + 8z = 24
2x + 2y + 5z = 11
3
y
x
3
2
1
FIGURE 1.1.2. Graphical solution to Example 1.1.2.
which are easily solved to obtain
z
=
1 and
y
=
2: Now substitute into the ï¬rst equation
and obtain that
x
=
1: We can see that the graphical method of solution becomes
impractical for systems of more than two variables, though it still tells us about the
qualitative nature of the solution. This solution can be discerned roughly in Figure 1.1.2.
Some Key Notation
Here is a formal statement of the kind of equation that we want to study in this chapter.
This formulation gives us a means of dealing with the general problem later on.
DEFINITION 1.1.3. A linear equation in the variables
x
1
;
x
2
;
:::;
x
n is an equation of
the form
a
1
x
1
+
a
2
x
2
+
:::
+
a
n
x
n
=
b
where the coefï¬cients
a
1
;
a
2
;
:::;
a
n and right hand side constant term
b are given con-
stants.
Of course, there are many interesting and useful nonlinear equations, such as
ax
2
+
bx
+
c
=
0, or
x
2
+
y
2
=
1, etc. But our focus is on systems that consist solely of linear
equations. In fact, our next deï¬nition gives a fancy way of describing the general linear
system.
DEFINITION 1.1.4. A linear system of
m equations in the
n unknowns
x
1
;
x
2
;
:::;
x
n
Linear Systems
is a list of
m equations of the form
a
11
x
1
+
a
12
x
2
+



+
a
1j
x
j
+



a
1n
x
n
=
b
1
a
21
x
1
+
a
22
x
2
+



+
a
2j
x
j
+



a
2n
x
n
=
b
2
...
...
...
a
i1
x
1
+
a
i2
x
2
+



+
a
ij
x
j
+



a
in
x
n
=
b
i
...
...
...
a
m1
x
1
+
a
m2
x
2
+



+
a
mj
x
j
+



a
mn
x
n
=
b
m
(1.1.1)

4
1. LINEAR SYSTEMS OF EQUATIONS
x
y
x
x
x
x
x
x
x
0
1
2
3
4
5
6
y
y
y
y
y
1
2
3
4
5
FIGURE 1.1.3. Discrete approximation to temperature function
(n
=
5).
Notice how the coefï¬cients are indexed: in the
ith row the coefï¬cient of the
jth variable,
x
j, is the number
a
ij, and the right hand side of the
ith equation is
b
i
: This systematic
way of describing the system will come in handy later,when we introduce the matrix
concept.
* Examples of Modeling Problems
It is easy to get the impression that linear algebra is about the simple kinds of problems
of the preceding examples. So why develop a whole subject? Next we consider two
examples whose solutions will not be so apparent as the previous two examples. The
real point of this chapter, as well as that of Chapters 2 and 3, is to develop algebraic and
geometrical methodologies which are powerful enough to handle problems like these.
Diffusion Processes
We consider a diffusion process arising from the ï¬‚ow of heat through a homogeneous
material substance. A basic physical observation to begin with is that heat is directly
proportional to temperature. In a wide range of problems this hypothesis is true, and
we shall always assume that we are modeling such a problem. Thus, we can measure
the amount of heat at a point by measuring temperature since they differ by a known
constant of proportionality. To ï¬x ideas, suppose we have a rod of material of unit
length, say, situated on the x-axis, for
0

x

1: Suppose further that the rod is
laterally insulated, but has a known internal heat source that doesnâ€™t change with time.
When sufï¬cient time passes, the temperature of the rod at each point will settle down
to â€œsteady stateâ€ values, dependent only on position
x: Say the heat source is described
by a function
f
(x);
0

x

1;which gives the additional temperature contribution per
unit length per unit time due to the heat source at the point
x: Also suppose that the left
and right ends of the rod are held at ï¬xed at temperatures
y
0 and
y
1
:
How can we model a steady state? Imagine that the continuous rod of uniform material
is divided up into a ï¬nite number of equally spaced points, called nodes, namely
x
0
=
0;
x
1
;
:::;
x
n+1
=
1 and that all the heat is concentrated at these points. Assume the
nodes are a distance
h apart. Since spacing is equal, the relation between
h and
n is
h
=
1=(n
+
1): Let the temperature function be
y
(x) and let
y
i
=
y
(x
i
): Approximate

1.1. SOME EXAMPLES
5
y
(x) in between nodes by connecting adjacent points
(x
i
;
y
i
) with a line segment. (See
Figure 1.1.3 for a graph of the resulting approximation to
y
(x):) We know that at the
end nodes the temperature is speciï¬ed:
y
(x
0
)
=
y
0 and
y
(x
n+1
)
=
y
1
: By examining
the process at each interior node, we can obtain the following linear equation for each
interior node index
i
=
1;
2;
:
:
:
;
n involving a constant
k called the conductivity of the
material. A derivation of these equations follows this example.
k
 y
i 1
+
2y
i
 y
i+1
h
2
=
f
(x
i
)
or
 y
i 1
+
2y
i
 y
i+1
=
h
2
k
f
(x
i
)
(1.1.2)
EXAMPLE 1.1.5. Suppose we have a rod of material of conductivity
k
=
1 and situated
on the x-axis, for
0

x

1: Suppose further that the rod is laterally insulated, but
has a known internal heat source and that both the left and right ends of the rod are
held at
0 degrees Fahrenheit. What are the steady state equations approximately for this
problem?
SOLUTION. Follow the notation of the discussion preceding this example. Notice that
in this case
x
i
=
ih: Remember that
y
0 and
y
n+1 are known to be
0, so the terms
y
0
and
y
n+1 disappear. Thus we have from Equation 1.1.2 that there are
n equations in the
unknowns
y
i
;
i
=
1;
2;
:
:
:
;
n:
It is reasonable to expect that the smaller
h is, the more accurately
y
i will approximate
y
(x
i
): This is indeed the case. But consider what we are confronted with when we take
n
=
5, i.e.,
h
=
1=(5
+
1)
=
1=6, which is hardly a small value of
h: The system of
ï¬ve equations in ï¬ve unknowns becomes
2y
1
 y
2
=
f
(1=6)=36
 y
1
+2y
2
 y
3
=
f
(2=6)=36
 y
2
+2y
3
 y
4
=
f
(3=6)=36
 y
3
+2y
4
 y
5
=
f
(4=6)=36
 y
4
+2y
5
=
f
(5=6)=36
This problem is already about as large as we would want to work by hand. The basic
ideas of solving systems like this are the same as in Example 1.1.1 and 1.1.2, though
for very small
h, say
h
=
:01, clearly we would like some help from a computer or
calculator.
*Derivation of the diffusion equations. We follow the notation that has already
been developed, except that the values
y
i will refer to quantity of heat rather than tem-
perature (this will yield equations for temperature, since heat is a constant times tem-
perature). What should happen at an interior node? The explanation requires one more
experimentally observed law known as Fourierâ€™s heat law. It says that the ï¬‚ow of heat
per unit length from one point to another is proportional to the rate of change in tem-
perature with respect to distance and moves from higher temperature to lower. The
constant of proportionality
k is known as the conductivity of the material. In addition,
we interpret the heat created at node
x
i to be
hf
(x
i
), since
f measures heat created per
unit length. Count ï¬‚ow towards the right as positive. Thus, at node
x
i the net ï¬‚ow per

6
1. LINEAR SYSTEMS OF EQUATIONS
unit length from the left node and to the right node are given by
Left ï¬‚ow
=
k
y
i
 y
i 1
h
Right ï¬‚ow
=
k
y
i
 y
i+1
h
Thus, in order to balance heat ï¬‚owing through the
ith node with heat created per unit
length at this node, we should have
Leftï¬‚ow
+ Rightï¬‚ow
=
k
y
i
 y
i 1
h
+
k
y
i
 y
i+1
h
=
hf
(x
i
)
In other words,
k
 y
i 1
+
2y
i
 y
i+1
h
2
=
f
(x
i
)
or
 y
i 1
+
2y
i
 y
i+1
=
h
2
k
f
(x
i
)
(1.1.3)
Input-Output models
We are going to set up a simple model of an economy consisting of three sectors that
supply each other and consumers. Suppose the three sectors are (E)nergy, (M)aterials
and (S)ervices and suppose that the demands of a sector are proportional to its output.
This is reasonable; if, for example, the materials sector doubled its output, one would
expect its needs for energy, material and services to likewise double. Now let
x;
y
;
z be
the total outputs of the sectors E,M and S respectively. We require that the economy
be closed in the sense that everything produced in the economy is consumed by the
economy. Thus, the total output of the sector E should equal the amounts consumed by
all the sectors and the consumers.
EXAMPLE 1.1.6. Given the following table of demand constants of proportionality and
consumer (D)emand (a ï¬xed quantity) for the output of each service, express the closed
property of the system as a system of equations.
Consumed by
E
M
S
D
E
0.2
0.3
0.1
2
Produced by
M
0.1
0.3
0.2
1
S
0.4
0.2
0.1
3
SOLUTION. Consider how we balance the total output and demands for energy. The
total output is
x units. The demands from the three sectors E,M and S are, according to
the table data,
0:2x;
0:3y and
0:1z
; respectively. Further, consumers demand
2 units of
energy. In equation form
x
=
0:2x
+
0:3y
+
0:1z
+
2

1.1. SOME EXAMPLES
7
Likewise we can balance the input/output of the sectors M and S to arrive at a system
of three equations in three unknowns.
x
=
0:2x
+
0:3y
+
0:1z
+
2
y
=
0:1x
+
0:3y
+
0:2z
+
1
z
=
0:4x
+
0:2y
+
0:1z
+
3
The questions that interest economists are whether or not this system has solutions, and
if so, what they are.
Note: In some of the text exercises you will ï¬nd references to â€œyour computer system.â€
This may be a calculator that is required for the course or a computer system for which
you are given an account. This textbook does not depend on any particular system, but
certain exercises require a computational device. The abbreviation â€œMASâ€ stands for a
matrix algebra system like MATLAB or Octave. Also, the shorthand â€œCASâ€ stands for
a computer algebra system like Maple, Mathematica or MathCad. A few of the projects
are too large for most calculators and will require a CAS or MAS.
1.1 Exercises
1. Solve the following systems algebraically.
(a)
x
+
2y
=
1
3x
 y
=
 4
(b)
x
 y
+
2z
=
6
2x
 z
=
3
y
+
2z
=
0
(c)
x
 y
=
1
2x
 y
=
3
x
+
y
=
3
2. Determine if the following systems of equations are linear or not. If so, put them in
standard format.
(a)
x
+
2
=
y
+
z
3x
 y
=
4
(b)
xy
+
2
=
1
2x
 6
=
y
(c)
x
+
2
=
1
x
+
3
=
y
2
3. Express the following systems of equations in the notation of the deï¬nition of linear
systems by specifying the numbers
m;
n;
a
ij and
b
i
:
(a)
x
1
 2x
2
+
x
3
=
2
x
2
=
1
 x
1
+
x
3
=
1
(b)
x
1
 3x
2
=
1
x
2
=
5
(c)
x
1
 x
2
=
1
2x
1
 x
2
=
3
x
2
+
x
1
=
3
4. Write out the linear system that results from Example 1.1.5 if we take
n
=
6:
5. Suppose that in the input-output model of Example 1.1.6 we ignore the Materials
sector input and output, so that there results a system of two equations in two unknowns
x and
z. Write out these equations and ï¬nd a solution for them.
6. Here is an example of an economic system where everything produced by the sectors
of the system is consumed by those sectors. An administrative unit has four divisions
serving the internal needs of the unit, labelled (A)ccounting, (M)aintenance, (S)upplies
and (T)raining. Each unit produces the â€œcommodityâ€ its name suggests, and charges the
other divisions for its services. The fraction of commodities consumed by each division

8
1. LINEAR SYSTEMS OF EQUATIONS
is given by the following table , also called an â€œinput-output matrixâ€.
Produced by
A
M
S
T
A
0.2
0.1
0.4
0.4
Consumed by
M
0.3
0.4
0.2
0.1
S
0.3
0.4
0.2
0.3
T
0.2
0.1
0.2
0.2
One wants to know what price should each division charge for its commodity so that
each division earns exactly as much as it spends? Such a pricing scheme is called
an equilibrium price structure; it assures that no division will earn too little to do its
job. Let
x,
y,
z and
w be the price per unit commodity charged by A, M, S and T,
respectively. The requirement of expenditures equaling earnings for each division result
in a system of four equations. Find these equations.
7. A polynomial
y
=
a
+
bx
+
cx
2 is required to interpolate a function
f
(x) at
x
=
1;
2;
3 where
f
(1)
=
1;
f
(2)
=
1 and
f
(3)
=
2: Express these three conditions as a
linear system of three equations in the unknowns
a;
b;
c
8. Use your calculator, CAS or MAS to solve the system of Example 1.1.5 with known
conductivity
k
=
1 and internal heat source
f
(x)
=
x: Then graph the approximate
solution by connecting the nodes
(x
j
;
y
j
) as in Figure 1.1.3.
9. Suppose that in Example 1.1.6 the Services sector consumes all of its output. Modify
the equations of the example accordingly and use your calculator, CAS or MAS to solve
the system. Comment on your solution.
10. Use your calculator, CAS or MAS to solve the system of Example 1.1.6.
11. The topology of a certain network is indicated by the following graph, where ï¬ve
vertices (labelled
v
j) represent locations of hardware units that receive and transmit
data along connection edges (labelled
e
j) to other units in the direction of the arrows.
Suppose the system is in a steady state and that the data ï¬‚ow along each edge
e
j is the
non-negative quantity
x
j. The single law that these ï¬‚ows must obey is this: net ï¬‚ow in
equals net ï¬‚ow out at each of the ï¬ve vertices (like Kirchoffâ€™s law in electrical circuits).
Write out a system of linear equations that the variables
x
1
;
x
2
;
x
3
;
x
4
;
x
5 must satisfy.
e5
v
v
v
e
e
v
e
e
v1
2
3
4
1
e
e
5
2
3
4
6
7

1.2. NOTATIONS AND A REVIEW OF NUMBERS
9
1.2. Notations and a Review of Numbers
The Language of Sets
The language of sets pervades all of mathematics. It provided a convenient shorthand
for expressing mathematical statements. Loosely speaking, a set can be deï¬ned as a
collection of objects, called the members of the set. This deï¬nition will sufï¬ce for
us. We use some shorthand to indicate certain relationships between sets and elements.
Usually, sets will be designated by upper case letters such as
A,
B, etc., and elements
will be designated by lower case letters such as
a,
b, etc. As usual, a set
A is a subset of
the set
B if every element of
A is an element of
B
; and a proper subset if it is a subset
not equal to
B
: Two sets
A and
B are said to be equal if they have exactly the same
elements. Some shorthand:
; denotes the empty set, i.e., the set with no members.
Set Operations
a
2
A means â€œa is a member of the set
A:â€
A
=
B means â€œthe set
A is equal to the set
B
:â€
A

B means â€œA is a subset of
B
:â€
A

B means â€œA is a proper subset of
Bâ€
There are two ways in which we may prescribe a set: we may list its elements, such
as in the deï¬nition
A
=
f0;
1;
2;
3g or specify them by rule such as in the deï¬nition
A
=
fx
j
x is an integer and
0

x

3g: (Read this as â€œA is the set of
x such that
x
is an integer and
0

x

3:â€) With this notation we can give formal deï¬nitions of set
intersections and unions:
DEFINITION 1.2.1. Let
A and
B be sets. Then the intersection of
A and
B is deï¬ned
to be the set
A
\
B
=
fx
j
x
2
A and
x
2
B
g: The union of
A and
B is the set
A
[
B
=
fx
j
x
2
A or
x
2
B
g: The difference of
A and
B is the set
A
 B
=
fx
j
x
2
A and
x
62
B
g:
EXAMPLE 1.2.2. Let
A
=
f0;
1;
3g and
B
=
f0;
1;
2;
4g: Then
A
[
;
=
A
A
\
;
=
;
A
[
B
=
f0;
1;
2;
3;
4g
A
\
B
=
f0;
1g
A
 B
=
f3g
About Numbers
One could spend a full course fully developing the properties of number systems. We
wonâ€™t do that, of course, but we will review some of the basic sets of numbers, and
assume the reader is familiar with properties of numbers we have not mentioned here.

10
1. LINEAR SYSTEMS OF EQUATIONS
At the start of it all are the kind of numbers that every child knows something about â€“
the natural or counting numbers. This is the set
N
=
f1;
2;
3;
:
:
:
g
One could view most subsequent expansions of the concept of number as a matter of
rising to the challenge of solving equations. For example, we cannot solve the equation
x
+
m
=
n;
m;
n
2
N
for the unknown
x without introducing subtraction and extending the notion of natural
number that of integer. The set of integers is denoted by
Z
=
f0;
1;
2;
:
:
:
g:
Next, we cannot solve the equation
ax
=
b;
a;
b
2
Z
for the unknown
x with introducing division and extending the notion of integer to that
of rational number. The set of rationals is denoted by
Q
=
fa=b
j
a;
b
2
Z and
b
6=
0g:
Rational number arithmetic has some characteristics that distinguish it from integer
arithmetic. The main difference is that nonzero rational numbers have multiplicative
inverses (the multiplicative inverse of
a=b is
b=a). Such a number system is called a
ï¬eld of numbers. In a nutshell, a ï¬eld of numbers is a system of objects, called numbers,
together with operations of addition, subtraction, multiplication and division that satisfy
the usual arithmetic laws; in particular, it must be possible to subtract any number from
any other and divide any number by a nonzero number to obtain another such number.
The associative, commutative, identity and inverse laws must hold for each of addition
and multiplication; and the distributive law must hold for multiplication over addition.
The rationals form a ï¬eld of numbers; the integers donâ€™t since division by nonzero
integers is not always possible if we restrict our numbers to integers.
The jump from rational to real numbers cannot be entirely explained by algebra, al-
though algebra offers some insight as to why the number system still needs to be ex-
tended. An equation like
x
2
=
2
does not have a rational solution, since
p
2 is irrational. (Story has it that this is lethal
knowledge, in that followers of a Pythagorean cult claim that the gods threw overboard
a ship one of their followers who was unfortunate enough to discover the fact.) There
is also the problem of numbers like
 and Eulerâ€™s constant
e which do not even satisfy
any polynomial equation. The heart of the problem is that if we only consider rationals
on a number line, there are many â€œholesâ€ which are ï¬lled by numbers like
 or
p
2
:
Filling in these holes leads us to the set
R of real numbers, which are in one-to-one
correspondence with the points on a number line. We wonâ€™t give an exact deï¬nition
of the set of real numbers. Recall that every real number admits a (possibly inï¬nite)
decimal representation, such as
1=3
=
0:333
:
:
: or

=
3:14159
:
:
:
: This provides us
with a loose deï¬nition: real numbers are numbers that can be expressed by a decimal
representation, i.e., limits of ï¬nite decimal expansions.

1.2. NOTATIONS AND A REVIEW OF NUMBERS
11
Î¸
r
z = a + bi =rei Î¸
b
a
FIGURE 1.2.1. Standard and polar coordinates in the complex plane.
There is one more problem to overcome. How do we solve a system like
x
2
+
1
=
0
over the reals? The answer is we canâ€™t: if
x is real, then
x
2

0, so
x
2
+
1
>
0: We
need to extend our number system one more time, and this leads to the set
C of complex
numbers. We deï¬ne
i to be a quantity such that
i
2
=
 1 and
C
=
fa
+
bi
j
a;
b
2
R
g:
If the complex number
z
=
a
+
bi is given, then we say that the form
a
+
bi is the
Standard Form
standard form of
z
: In this case the real part of
z is
<(z
)
=
a and the imaginary part is
deï¬ned as
=(z
)
=
b: (Notice that the imaginary part of
z is a real number: it is the real
coefï¬cient of
i.) Two complex numbers are equal precisely when they have the same
real parts and the same imaginary parts. All of this could be put on a more formal basis
by initially deï¬ning complex numbers to be ordered pairs of real numbers. We will not
do so, but the fact that complex numbers behave like ordered pairs of real numbers leads
to an important geometrical insight: complex numbers can be identiï¬ed with points in
the plane. Instead of an x and y axis, one lays out a real and imaginary axis (which is
still usually labeled with
x and
y) and plots complex numbers
a
+
bi as in Figure 1.2.1.
This results in the so-called complex plane.
Arithmetic in
C is carried out by using the usual laws of arithmetic for
R and the alge-
braic identity
i
2
=
 1 to reduce the result to standard form. Thus we have the following
laws of complex arithmetic.
(a
+
bi)
+
(c
+
di)
=
(a
+
c)
+
(b
+
d)i;
(a
+
bi)

(c
+
di)
=
(ac
 bd)
+
(ad
+
bc)i
In particular, notice that complex addition is exactly like the vector addition of plane
vectors. Complex multiplication does not admit such a simple interpretation.
EXAMPLE 1.2.3. Let
z
1
=
2
+
4i and
z
2
=
1
 3i: Compute
z
1
 3z
2
:

12
1. LINEAR SYSTEMS OF EQUATIONS
SOLUTION. We have that
z
1
 3z
2
=
(2
+
4i)
 3(1
 3i)
=
2
+
4i
 3
+
9i
=
 1
+
13i
There are several more useful ideas about complex numbers that we will need. The
length or absolute value of a complex number
z
=
a
+
bi is deï¬ned as the nonnegative
real number
jz
j
=
p
a
2
+
b
2, which is exactly the length of
z viewed as a plane vector.
The complex conjugate of
z is deï¬ned as
z
=
a
 bi: Some easily checked and very
useful facts about absolute value and complex conjugation:
jz
1
z
2
j
=
jz
1
j
jz
2
j
jz
1
+
z
2
j

jz
1
j
+
jz
2
j
jz
j
2
=
z
z
z
1
+
z
2
=
z
1
+
z
2
z
1
z
2
=
z
1
z
2
EXAMPLE 1.2.4. Let
z
1
=
2
+
4i and
z
2
=
1
 3i: Verify for this
z
1 and
z
2 that
jz
1
z
2
j
=
jz
1
j
jz
2
j:
SOLUTION. First calculate that
z
1
z
2
=
(2
+
4i)(1
 3i)
=
(2
+
12)
+
(4
 6)i so
that
jz
1
z
2
j
=
p
14
2
+
( 2)
2
=
p
200
; while
jz
1
j
=
p
2
2
+
4
2
=
p
20 and
jz
2
j
=
p
1
2
+
( 3)
2
=
p
10
: It follows that
jz
1
z
2
j
=
p
10
p
20
=
jz
1
j
jz
2
j:
EXAMPLE 1.2.5. Verify that the product of conjugates is the conjugate of the product.
SOLUTION. This is just the last fact in the preceding list. Let
z
1
=
x
1
+
iy
1 and
z
2
=
x
2
+
iy
2 be in standard form, so that
z
1
=
x
1
 iy
1 and
z
2
=
x
2
 iy
2
: We
calculate
z
1
z
2
=
(x
1
x
2
 y
1
y
2
)
+
i(x
1
y
2
+
x
2
y
1
)
so that
z
1
z
2
=
(x
1
x
2
 y
1
y
2
)
 i(x
1
y
2
+
x
2
y
1
):
Also,
z
1
z
2
=
(x
1
 iy
1
)(x
2
 iy
2
)
=
(x
1
x
2
 y
1
y
2
)
+
( i(x
1
y
2
 x
2
y
1
)
=
z
1
z
2
:
The complex number
i solves the equation
x
2
+
1
=
0 (no surprise here: it was invented
expressly for that purpose). The big surprise is that once we have the complex numbers
in hand, we have a number system so complete that we can solve any polynomial equa-
tion in it. We wonâ€™t offer a proof of this fact â€“ itâ€™s very nontrivial. Sufï¬ce it to say
that nineteenth century mathematicians considered this fact so fundamental that they
dubbed it the â€œFundamental Theorem of Algebra,â€ a terminology we adopt.
THEOREM 1.2.6. Let
p(z
)
=
a
n
z
n
+
a
n 1
z
n 1
+



+
a
1
z
+
a
0 be a non-constant
Fundamental
Theorem of
Algebra
polynomial in the variable
z with complex coefï¬cients
a
0
;
:
:
:
;
a
n
: Then the polynomial
equation
p(z
)
=
0 has a solution in the ï¬eld
C of complex numbers.

1.2. NOTATIONS AND A REVIEW OF NUMBERS
13
Note that the Fundamental Theorem doesnâ€™t tell us how to ï¬nd a root of a polynomial
â€“ only that it can be done. As a matter of fact, there are no general formulas for the
roots of a polynomial of degree greater than four, which means that we have to resort to
numerical approximations in most cases.
In vector space theory the numbers in use are sometimes called scalars, and we will use
this term. Unless otherwise stated or suggested by the presence of
i, the ï¬eld of scalars
in which we do arithmetic is assumed to be the ï¬eld of real numbers. However, we shall
see later when we study eigensystems, that even if we are only interested in real scalars,
complex numbers have a way of turning up quite naturally.
Letâ€™s do a few more examples of complex number manipulation.
EXAMPLE 1.2.7. Solve the linear equation
(1
 2i)z
=
(2
+
4i) for the complex variable
z
: Also compute the complex conjugate and absolute value of the solution.
SOLUTION. The solution requires that we put the complex number
z
=
(2
+
4i)=(1
 2i)
in standard form. Proceed as follows: multiply both numerator and denominator by
(1
 2i
)
=
1
+
2i to obtain that
z
=
2
+
4i
1
 2i
=
(2
+
4i)(1
+
2i)
(1
 2i)(1
+
2i)
=
2
 8
+
(4
+
4)i
1
+
4
=
 6
5
+
8
5
i:
Next we see that
z
=
 6
5
+
8
5
i
=
 6
5
 8
5
i
and
jz
j
=




1
5
( 6
+
8i)




=
1
5
j( 6
+
8i)j
=
1
5
p
( 6)
2
+
8
2
=
10
5
=
2:
Practical Complex Arithmetic
We conclude this section with a discussion of the more advanced aspects of complex
arithmetic. This material will not be needed until Chapter 4. Recall from basic algebra
the Roots Theorem: the linear polynomial
z
 a is a factor of a polynomial
f
(z
)
=
a
0
+
a
1
x
+



a
n
x
n if and only if
a is a root of the polynomial, i.e.,
f
(a)
=
0: If we
team this fact up with the Fundamental Theorem of Algebra, we see an interesting fact
about factoring polynomials over
C : every polynomial can be completely factored into
a product of linear polynomials of the form
z
 a times a constant. The numbers
a that
occur are exactly the roots of
f
(z
): Of course, these roots could be repeated roots, as in
the case of
f
(z
)
=
3z
2
 6z
+
3
=
3(z
 1)
2
: But how can we use the Fundamental
Theorem of Algebra in a practical way to ï¬nd the roots of a polynomial? Unfortunately,
the usual proofs of Fundamental Theorem of Algebra donâ€™t offer a clue because they
are non-constructive, i.e., they prove that solutions must exist, but do not show how to
explicitly construct such a solution. Usually, we have to resort to numerical methods
to get approximate solutions, such as the Newtonâ€™s method used in calculus. For now,
we will settle on a few ad hoc methods for solving some important special cases. First

14
1. LINEAR SYSTEMS OF EQUATIONS
degree equations offer little difï¬culty: the solution to
ax
=
b is
x
=
b=a, as usual. The
one detail to attend to: what complex number is represented by the expression
b=a ? We
saw how to handle this by the trick of â€œrationalizingâ€ the denominator in Example 1.2.7.
Quadratic equations are also simple enough: use the quadratic formula, which says that
Quadratic
Formula
the solutions to
az
2
+
bz
+
c
=
0
are given by
z
=
 b

p
b
2
 4ac
2a
There is one little catch: what does the square root of a complex number mean? What
we are really asking is this: how do we solve the equation
z
2
=
d for
z, where
d is a
complex number? Letâ€™s try for a little more: how do we solve
z
n
=
d for all possible
solutions
z, where
d is a given complex number? In a few cases, such an equation is
quite easy to solve. We know, for example, that
z
=
i are solutions to
z
2
=
 1, so
these are all the solutions. Similarly, one can check by hand that
1;
i are all solutions
to
z
4
=
1: Consequently,
z
4
 1
=
(z
 1)(z
+
1)(z
 i)(z
+
i): Roots of the equation
z
n
=
1 are sometimes called the
nth roots of unity. Thus the
4th roots of unity are
1
and
i: But what about something like
z
3
=
1
+
i?
The key to answering this question is another form of a complex number
z
=
a
+
bi: In
Polar form
reference to Figure 1.1.3 we can write
z
=
r
(cos

+
i
sin

)
=
r
e
i, where
 is a real
number,
r is a non-negative real and
e
i is deï¬ned by the following expression:
DEFINITION 1.2.8.
e
i

cos

+
i
sin

:
Notice that
je
i
j
=
cos
2

+
sin
2

=
1; so that
jr
e
i
j
=
jr
jje
i
j
=
r
; provided
r is non-
negative. The expression
r
e
i with
r
=
jz
j and the angle
 measured counterclockwise
in radians, is called the polar form of
z
: The number
r is just the absolute value of
z
:
The number
 is sometimes called an argument of
z
: It is important to notice that
 is
not unique. If the angle

0 works for the complex number
z, then so does

=

0
+
2
k,
for any integer
k
; since
sin and
cos are periodic of period
2
: It follows that a complex
number may have more than one polar form. For example,
i
=
e
i
=2
=
e
i5
=2 (here
r
=
1). In fact, the most general polar expression for
i is
i
=
e
i(
=2+2k

)
; where
k is
an arbitrary integer.
EXAMPLE 1.2.9. Find the possible polar forms of
1
+
i:

1.2. NOTATIONS AND A REVIEW OF NUMBERS
15
SOLUTION. Draw a picture of the number
1
+
i
as in the adjacent ï¬gure. We see that the angle

0
=

=4 works ï¬ne as a measure of the angle
from the positive
x-axis to the radial line from
the origin to
z
:Moreover, the absolute value of
z is
p
1
+
1
=
p
2: Hence, a polar form for
z
is
z
=
p
2
e
i
=4. However, we can adjust the
angle

0 by any multiple of
2
; a full rotation,
and get a polar form for
z
: So the most general
polar form for
z is
z
=
p
2
e
i(
=4+2k

), where
k
is any integer.
2
1 + i
Ï€/4
1
1
Figure 1.2.2: Form of
1
+
i
As the notation suggests, polar forms obey the laws of exponents. A simple application
of the laws for the sine and cosine of a sum of angles shows that for angles
 and
 we
have the identity
e
i(
+ 
)
=
e
i
e
i 
:
By using this formula
n times, we obtain that
e
in
=
(e
i
)
n which can also be expressed
as DeMoivreâ€™s Formula:
(cos

+
i
sin

)
n
=
cos
n
+
i
sin
n
Now for solving
z
n
=
d: First, ï¬nd the general polar form of
d, say
d
=
ae
i(
0
+2k

)
;
where

0 is the so-called principal angle for
d, i.e.,
0


0
<
2
; and
a
=
jdj: Next,
write
z
=
r
e
i, so that the equation to be solved becomes
r
n
e
in
=
ae
i(
0
+2k

)
:
Taking absolute values of both sides yields that
r
n
=
a, whence we obtain the unique
value of
r
=
n
p
a
=
n
p
jdj: What about
? The most general form for
n is
n
=

0
+
2k

:
Hence we obtain that

=

0
n
+
2k

n
:
Notice that the values of
e
i2k

=n start repeating themselves as
k passes a multiple of
n,
since
e
i2
=
e
0
=
1: Therefore, one gets exactly
n distinct values for
e
i, namely

=

0
n
+
2k

n
;
k
=
0;



;
n
 1:
These points are equally spaced around the unit circle in the complex plane, starting
with the point
e
i
0
: Thus we have obtained
n distinct solutions to the equation
z
n
=
d;
where
d
=
ae
i
0
; namely
General
Solution to
z
n
=
d
z
=
a
1=n
e
i(
0
=n+2k

=n)
;
k
=
0;



;
n
 1
EXAMPLE 1.2.10. Solve the equation
z
3
=
1
+
i for the unknown
z
:

16
1. LINEAR SYSTEMS OF EQUATIONS
21/6
e
Ï€/12
i
21/6
    

  

 
  

-1
1
-1
1
y
x
1+i = 2
e i 17Ï€/12
1/6
2
Ï€9/12
i
e
1/2
Ï€/4
i
e
1/6
2
FIGURE 1.2.3. Roots of
x
3
=
1
+
i:
SOLUTION. The solution goes as follows: we have seen that
1
+
i has a polar form
1
+
i
=
p
2
e
i
=4
:
Then according to the previous formula, the three solutions to our cubic are
z
=
(
p
2
)
1=3
e
i(
=4+2k

)=3
=
2
1=6
e
i(1+8k
)
=12
k
=
0;
1;
2:
See Figure 1.2.3 for a graph of these complex roots.
We conclude with a little practice with square roots and the quadratic formula. In re-
gards to square roots, notice that the expression
w
=
p
d is ambiguous. With a positive
real number
d this meant the positive root of the equation
w
2
=
d: But when
d is com-
plex (or even negative), it no longer makes sense to talk about â€œpositiveâ€ and â€œnegativeâ€
roots of
w
2
=
d: In this case we simply interpret
p
d to be one of the roots of
w
2
=
d:
EXAMPLE 1.2.11. Compute
p
 4 and
p
i:
SOLUTION. Observe that
 4
=
4

( 1): It is reasonable to expect the laws of exponents
to continue to hold, so we should have
( 4)
1=2
=
4
1=2

( 1)
1=2
: Now we know that
i
2
=
 1; so we can take
i
=
( 1)
1=2 and obtain that
p
 4
=
( 4)
1=2
=
2i: Letâ€™s
check it:
(2i)
2
=
4i
2
=
 4:
We have to be a bit more careful with
p
i: Weâ€™ll just borrow the idea of the formula for
solving
z
n
=
d: First, put
i in polar form as
i
=
1

e
i
=2
: Now raise each side to the
1=2 power to obtain
p
i
=
i
1=2
=
1
1=2

(e
i
=2
)
1=2
=
1

e
i
=4
=
cos
(
=4)
+
i
sin(
=4)
=
1
p
2
(1
+
i):

1.2. NOTATIONS AND A REVIEW OF NUMBERS
17
A quick check conï¬rms that
((1
+
i)=
p
2)
2
=
2i=2
=
i:
EXAMPLE 1.2.12. Solve the equation
z
2
+
z
+
1
=
0:
SOLUTION. According to the quadratic formula, the answer is
z
=
 1

p
1
2
 4
2
=
 1

i
p
3
2
EXAMPLE 1.2.13. Solve
z
2
+
z
+
1
+
i
=
0 and factor this polynomial.
SOLUTION. This time we obtain from the quadratic formula that
z
=
 1

p
1
 4(1
+
i)
2
=
 1

p
 (3
+
4i)
2
What is interesting about this problem is that we donâ€™t know the polar angle
 for
z
=
 (3
+
4i): However, we know that
sin

=
 4=5 and
cos

=
 3=5: We also have the
standard half angle formulas from trigonometry to help us:
cos
2

=2
=
1
+
cos

2
=
1
5
; and
sin
2

=2
=
1
 cos

2
=
4
5
Since
 is in the third quadrant of the complex plane,

=2 is in the second, so
cos

=2
=
 1
p
5
; and
sin

=2
=
2
p
5
Now notice that
j
 (3
+
4i)j
=
5: It follows that a square root of
 (3
+
4i) is given by
s
=
p
5(
 1
p
5
+
2
p
5
i)
=
 1
+
2i
Check that
s
2
=
 (3
+
4i): It follows that the two roots to our quadratic equation are
given by
z
=
 1

( 1
+
2i)
2
=
 1
+
i;
 i
In particular, we see that
z
2
+
z
+
1
+
i
=
(z
+
1
 i)(z
+
i):
1.2 Exercises
1. Given that
A
=
fxjx
2
R and
x
2
<
3g and
B
=
fxjx
2
Z and
x
>
 1g;
enumerate the following sets:
(a)
A
\
B
(b)
B
 A
(c)
Z
 B
(d)
N
[
B (e)
R
\
A
2. Put the following complex numbers into polar form and sketch them in the complex
plane:
(a)
 i (b)
1
+
i
(c)
 1
+
i
p
3 (d)
 1 (e)
2
 2i
(f)
2i
(g)

3. Calculate the following (your answers should be in standard form):
(a)
(4
+
2i)
 (3
 6i) (b)
2
+
i
2
 i
(c)
(2
+
4i)(3
 i) (d)
1
+
2i
1
 2i (e)
i(1
 i)

18
1. LINEAR SYSTEMS OF EQUATIONS
4. Solve the equations for the unknown
z
: Be sure to put your answer in standard form.
(a)
(2
+
i)z
=
1 (b)
 iz
=
2z
+
5 (c)
=(z
)
=
2<(z
)
+
1 (d)
z
=
z
5. Find all solutions to the equations
(a)
z
2
+
z
+
3
=
0 (b)
z
2
 1
=
iz (c)
z
2
 2z
+
i
=
0
(d)
z
2
+
4
=
0
6. Find the solutions to the following equations. Express them in both polar and stan-
dard form and graph them in the complex plane.
(a)
z
3
=
1 (b)
z
3
=
 8
(c)
(z
 1)
3
=
 1
(d)
z
4
+
z
2
+
1
=
0
7. Write out the values of
i
k in standard form for integers
k
=
 1;
0;
1;
2;
3;
4 and
deduce a formula for
i
k consistent with these values.
8. Sketch in the complex plane the set of complex numbers
z such that
(a)
jz
+
1j
=
2
(b)
jz
+
1j
=
jz
 1j
(c)
jz
 2j
<
1
Hint:
Itâ€™s easier to work with absolute value squared.
9. Let
z
1
=
2
+
4i and
z
2
=
1
 3i: Verify for this
z
1 and
z
2 that
z
1
z
2
=
z
1
z
2
:
10. Verify that for any two complex numbers, the sum of the conjugates is the conjugate
of the sum.
11. Use the notation of Example 1.2.5 to show that
jz
1
z
2
j
=
jz
1
j
jz
2
j
: Hint:
Remem-
ber that if
z
=
x
+
iy then
jz
j
2
=
x
2
+
y
2
:
12. Use the deï¬nitions of exponentials along with the sum of angles formulas for
sin(
+
 
) and
cos
(
+
 
) to verify the law of addition of exponents:
e
i(
+ 
)
=
e
i
e
i 
:
13. Use a computer or calculator to ï¬nd all roots to the polynomial equation
z
5
+
z
+
1
=
0: How many roots should this equation have? How many of these roots can you ï¬nd
with your system?
14. Show that if
w is a root of the polynomial
p(z
); that is,
p(w
)
=
0; where
p(z
) has
real coefï¬cients, then
w is also a root of
p(z
):
15. Show that
1
+
i;
1
 i and
2 are roots of the polynomialp(z
)
=
z
3
 4z
2
+
6z
 4
and use this to factor the polynomial.
16. Show that if
w is a root of the polynomial
p(z
); that is,
p(w
)
=
0; where
p(z
) has
real coefï¬cients, then
w is also a root of
p(z
):
1.3. Gaussian Elimination: Basic Ideas
We return now to the main theme of this chapter, which is the systematic solution of
linear systems, as deï¬ned in equation 1.1.1 of Section 1.1. The principal methodology
is the method of Gaussian elimination and its variants, which we introduce by way of
a few simple examples. The idea of this process is to reduce a system of equations by

1.3. GAUSSIAN ELIMINATION: BASIC IDEAS
19
certain legitimate and reversible algebraic operations (called â€œelementary operationsâ€)
to a form where we can easily see what the solutions to the system, if any, are. Specif-
ically, we want to get the system in a form where only the ï¬rst equation involves the
ï¬rst variable, only the ï¬rst and second involve the next variable to be solved for, and so
forth. Then it will be simple to solve for each variable one at a time, starting with the
last equation and variable. In a nutshell, this is Gaussian elimination.
One more matter that will have an effect on our description of solutions to a linear
system is that of the number system in use. As we noted earlier, it is customary in
linear algebra to refer to numbers as â€œscalars.â€ The two basic choices of scalar ï¬elds
are the real number system or the complex number system. Unless complex numbers
occur explicitly in a linear system, we will assume that the scalars to be used in ï¬nding
a solution come from the ï¬eld of real numbers. Such will be the case for most of the
problems in this chapter.
An Example and Some Shorthand
EXAMPLE 1.3.1. Solve the simple system
2x
 y
=
1
4x
+
4y
=
20
(1.3.1)
SOLUTION. First, letâ€™s switch the equations to obtain
4x
+
4y
=
20
2x
 y
=
1
(1.3.2)
Next, multiply the ï¬rst equation by
1=4 to obtain
x
+
y
=
5
2x
 y
=
1
(1.3.3)
Now, multiply a copy of the ï¬rst equation by
 2 and add it to the second. We can do this
easily if we take care to combine like terms as we go. In particular, the resulting
x term
in the new second equation will be
 2x
+
2x
=
0, the
y term will be
 2y
 y
=
 3y,
and the constant term on the right hand side will be
 2

5
+
1
=
 9: Thus we obtain
x
+
y
=
5
0x
 3y
=
 9
(1.3.4)
This completes the ï¬rst phase of Gaussian elimination, which is called â€œforward solv-
ing.â€ Note that we have put the system in a form where only the ï¬rst equation involves
the ï¬rst variable and only the ï¬rst and second involve the second variable. The second
phase of Gaussian elimination is called â€œback solvingâ€, and it works like it sounds. Use
the last equation to solve for the last variable, then work backwards, solving for the
remaining variables in reverse order. In our case, the second equation is used to solve
for
y simply by dividing by
 3 to obtain that
y
=
 9
 3
=
3
Now that we know what
y is, we can use the ï¬rst equation to solve for
x, and we obtain
x
=
5
 y
=
5
 3
=
2

20
1. LINEAR SYSTEMS OF EQUATIONS
The preceding example may seem like too much work for such a simple system. We
could easily scratch out the solution in much less space. But what if the system is
larger, say
4 equations in
4 unknowns, or more? How do we proceed then? It pays
to have a systematic strategy and notation. We also had an ulterior motive in the way
we solved this system. All of the operations we will ever need to solve a linear system
were illustrated in the preceding example: switching equations, multiplying equations
by nonzero scalars, and adding a multiple of one equation to another.
Before proceeding to another example, letâ€™s work on the notation a bit. Take a closer
look at the system of equations (1.3.1). As long as we write numbers down systemat-
ically, there is no need to write out all the equal signs or plus signs. Isnâ€™t every bit of
information that we require contained in the following table of numbers?

2
 1
1
4
4
20

Of course, we have to remember that the ï¬rst two columns of numbers are coefï¬cients
of
x and
y
; respectively, and the third column consists of right hand side terms. So we
could embellish the table with a few reminders in the top row:
2
4
x
y
=
r
:h:s:
2
 1
1
4
4
20
3
5
With a little practice, we will ï¬nd that the reminders are usually unnecessary; so we
dispense with them for the most part. We can see that rectangular tables of numbers
are very useful in representing a system of equations. Such a table is one of the basic
objects studied in this text. As such, it warrants a formal deï¬nition.
DEFINITION 1.3.2. A matrix is a rectangular array of numbers. If a matrix has
m rows
Matrices and
Vectors
and
n columns, then the size of the matrix is said to be
m

n: If the matrix is
1

n
or
m

1, it is called a vector. Finally, the number that occurs in the
ith row and
jth
column is called the
(i;
j
)th entry of the matrix.
The objects we have just deï¬ned are basic â€œquantitiesâ€ of linear algebra and matrix
analysis, along with scalar quantities. Although every vector is itself a matrix, we want
to single vectors out when they are identiï¬ed as such. Therefore, we will follow a
standard typographical convention: matrices are usually designated by capital letters,
while vectors are usually designated by boldface lower case letters. In a few cases these
conventions are not followed, but the meaning of the symbols should be clear from
context.
We shall need to refer to parts of a matrix. As indicated above, the location of each
entry of a matrix is determined by the index of the row and column it occupies.
NOTATION 1.3.3. The statement â€œA
=
[a
ij
]â€ means that
A is a matrix whose
(i;
j
)th
entry is denoted by
a
ij
: Generally, the size of
A will be clear from context. If we want
to indicate that
A is an
m

n matrix, we write
A
=
[a
ij
]
m;n
:
Similarly, the statement â€œb
=
[b
i
]â€ means that
b is a column vector whose
ith entry is
denoted by
b
i, and â€œc
=
[c
j
]â€ means that
c is a row vector whose
j
th entry is denoted

1.3. GAUSSIAN ELIMINATION: BASIC IDEAS
21
by
c
j
: In case the type of the vector (row or column) is not clear from context, the
default is a column vector.
Another term that we will use frequently is the following.
NOTATION 1.3.4. The leading entry of a row vector is the ï¬rst nonzero element of that
vector. If all entries are zero, the vector has no leading entry.
The equations of (1.3.1) have several matrices associated with them. First is the full
matrix that describes the system, which we call the augmented matrix of the system. In
our example, this is the
2

3 matrix

2
 1
1
4
4
20

Next, there is the submatrix consisting of coefï¬cients of the variables. This is called the
coefï¬cient matrix of the system, and in our case it is the
2

2 matrix

2
 1
4
4

Finally, there is the single column matrix of right hand side constants, which we call the
right hand side vector. In our example, it is the
1

2 vector

1
20

How can we describe the matrices of the general linear system of Equation 1.1.1? First,
there is the
m

n coefï¬cient matrix
A
=
2
6
6
6
6
6
6
6
6
4
a
11
a
12



a
1j



a
1n
a
21
a
22



a
2j



a
2n
...
...
...
...
a
i1
a
i2



a
ij



a
in
...
...
...
...
a
m1
a
m2



a
mj



a
mn
3
7
7
7
7
7
7
7
7
5
Notice that the way we subscripted entries of this matrix is really very descriptive: the
ï¬rst index indicates the row position of the entry and the second, the column position
of the entry. Next, there is the
m

1 right hand side vector of constants
b
=
2
6
6
6
6
6
6
6
6
4
b
1
b
2
...
b
i
...
b
m
3
7
7
7
7
7
7
7
7
5

22
1. LINEAR SYSTEMS OF EQUATIONS
Finally, stack this matrix and vector along side each other (we use a vertical bar below
to separate the two symbols) to obtain the
m

(n
+
1) augmented matrix
e
A
=
[A
j
b]
=
2
6
6
6
6
6
6
6
6
4
a
11
a
12



a
1j



a
1n
b
1
a
21
a
22



a
2j



a
2n
b
2
...
...
...
...
...
a
i1
a
i2



a
ij



a
in
b
i
...
...
...
...
...
a
m1
a
m2



a
mj



a
mn
b
m
3
7
7
7
7
7
7
7
7
5
The Elementary Row Operations
There is another matter of notation that we will ï¬nd extremely handy in the sequel. This
is related to the operations that we performed on the preceding example. Now that we
have the matrix notation we could just as well perform these operations on each row of
the augmented matrix, since a row corresponds to an equation in the original system.
There were three types of operations used. We shall catalogue these and give them
names, so that we can document our work in solving a system of equations in a concise
way. Here are the three elementary operations we shall use, described in terms of their
action on rows of a matrix; an entirely equivalent description applies to the equations of
the linear system whose augmented matrix is the matrix below.

E
ij
: This is shorthand for the elementary operation of switching the
ith and
jth rows of the matrix. For instance, in Example 1.3.1 we moved from Equa-
tion 1.3.1 to equation 1.3.2 by using the elementary operation
E
12
:

E
i
(c)
: This is shorthand for the elementary operation of
multiplying the
ith
Notation for
Elementary
Operations
row by the nonzero constant
c: For instance, we moved from Equation 1.3.2 to
(1.3.3) by using the elementary operation
E
1
(1=4):

E
ij
(d)
: This is shorthand for the elementary operation of adding
d times the
jth row to the
ith row. (Read the symbols from right to left to get the right
order.) For instance, we moved from Equation 1.3.3 to Equation 1.3.4 by using
the elementary operation
E
21
( 2):
Now letâ€™s put it all together. The whole forward solving phase of Example 1.3.1 could
be described concisely with the notation we have developed:

2
 1
1
4
4
20

 !
E
12

4
4
20
2
 1
1

     !
E
1
(1=4)

1
1
5
2
 1
1

     !
E
21
( 2)

1
1
5
0
 3
 9

This is a big improvement over our ï¬rst description of the solution. There is still the job
of back solving, which is the second phase of Gaussian elimination. When doing hand
calculations, weâ€™re right back to writing out a bunch of extra symbols again, which is
exactly what we set out to avoid by using matrix notation.

1.3. GAUSSIAN ELIMINATION: BASIC IDEAS
23
Gauss-Jordan Elimination
Hereâ€™s a better way to do the second phase by hand: stick with the augmented matrix.
Starting with the last nonzero row, convert the leading entry (this means the ï¬rst nonzero
entry in the row) to a
1 by an elementary operation, and then use elementary operations
to convert all entries above this
1 entry to
0â€™s. Now work backwards, row by row, up
to the ï¬rst row. At this point we can read off the solution to the system. Letâ€™s see how
it works with Example 1.3.1. Here are the details using our shorthand for elementary
operations:

1
1
5
0
 3
 9

      !
E
2
( 1=3)

1
1
5
0
1
3

     !
E
12
( 1)

1
0
2
0
1
3

All we have to do is remember the function of each column in order to read off the
answer from this last matrix. The underlying system that is represented is
1

x
+
0

y
=
2
0

x
+
1

y
=
3
This is, of course, the answer we found earlier:
x
=
2;
y
=
3:
The method of combining forward and back solving into elementary operations on the
augmented matrix has a name: it is called Gauss-Jordan elimination, and is the method
of choice for solving many linear systems. Letâ€™s see how it works on an example from
Section 1.1.
EXAMPLE 1.3.5. Solve the following system by Gauss-Jordan elimination.
x
+
y
+
z
=
4
2x
+
2y
+
5z
=
11
4x
+
6y
+
8z
=
24
SOLUTION. First form the augmented matrix of the system, the
3

4 matrix
2
4
1
1
1
4
2
2
5
11
4
6
8
24
3
5
Now forward solve:
2
4
1
1
1
4
2
2
5
11
4
6
8
24
3
5
     !
E
21
( 2)
2
4
1
1
1
4
0
0
3
3
4
6
8
24
3
5
     !
E
31
( 4)
2
4
1
1
1
4
0
0
3
3
0
2
4
8
3
5
 !
E
23
2
4
1
1
1
4
0
2
4
8
0
0
3
3
3
5
Notice, by the way, that the row switch of the third step is essential. Otherwise, we
cannot use the second equation to solve for the second variable,
y
: Now back solve:
2
4
1
1
1
4
0
2
4
8
0
0
3
3
3
5
     !
E
3
(1=3)
2
4
1
1
1
4
0
2
4
8
0
0
1
1
3
5
     !
E
23
( 4)
2
4
1
1
1
4
0
2
0
4
0
0
1
1
3
5

24
1. LINEAR SYSTEMS OF EQUATIONS
     !
E
13
( 1)
2
4
1
1
0
3
0
2
0
4
0
0
1
1
3
5
     !
E
2
(1=2)
2
4
1
1
0
3
0
1
0
2
0
0
1
1
3
5
     !
E
12
( 1)
2
4
1
0
0
1
0
1
0
2
0
0
1
1
3
5
At this point we can read off the solution to the system:
x
=
1;
y
=
2;
z
=
1:
Systems with Non-Unique Solutions
Next, we consider an example that will pose a new kind of difï¬culty, namely, that of
inï¬nitely many solutions. Here is some handy terminology.
NOTATION 1.3.6. An entry of a matrix used to zero out entries above or below it by
Pivots
means of elementary row operations is called a pivot.
The entries that we use in Gaussian or Gauss-Jordan elimination for pivots are always
leading entries in the row which they occupy. For the sake of emphasis, in the next few
examples, we will put a circle around the pivot entries as they occur.
EXAMPLE 1.3.7. Solve for the variables
x,
y and
z in the system
x+
y
+
z
=
2
2x+
2y
+
4z
=
8
z
=
2
.
SOLUTION. Here the augmented matrix of the system is
2
4
1
1
1
2
2
2
4
8
0
0
1
2
3
5
Now proceed to use Gaussian elimination on the matrix.
2
4
1
1
1
2
2
2
4
8
0
0
1
2
3
5
     !
E
21
( 2)
2
4
1
1
1
2
0
0
2
4
0
0
1
2
3
5
What do we do next? Neither the second nor the third row correspond to equations that
involve the variable
y
: Switching second and third equations wonâ€™t help, either. Here
is the point of view that we adopt in applying Gaussian elimination to this system: the
ï¬rst equation has already been â€œused upâ€ and is reserved for eventually solving for
x:
We now restrict our attention to the â€œunusedâ€ second and third equations. Perform the
following operations to do Gauss-Jordan elimination on the system.
2
6
4
1
1
1
2
0
0
2
4
0
0
1
2
3
7
5
     !
E
2
(1=2)
2
6
4
1
1
1
2
0
0
1
2
0
0
1
2
3
7
5
     !
E
32
( 1)
2
6
4
1
1
1
2
0
0
1
2
0
0
0
0
3
7
5
     !
E
12
( 1)
2
6
4
1
1
0
0
0
0
1
2
0
0
0
0
3
7
5

1.3. GAUSSIAN ELIMINATION: BASIC IDEAS
25
How do we interpret this result? We take the point of view that the ï¬rst row represents
an equation to be used in solving for
x since the leading entry of the row is in the column
of coefï¬cients of
x: By the same token, the second row represents an equation to be used
in solving for
z, since the leading entry of that row is in the column of coefï¬cients of
z
:
What about
y? Notice that the third equation represented by this matrix is simply
0
=
0,
which carries no information. The point is that there is not enough information in the
system to solve for the variable
y, even though we started with three distinct equations.
Somehow, they contained redundant information. Therefore, we take the point of view
Free and Bound
Variables
that
y is not to be solved for; it is a free variable in the sense that it can take on any
value whatsoever and yield a legitimate solution to the system. On the other hand, the
variables
x and
z are bound in the sense that they will be solved for in terms of constants
and free variables. The equations represented by the last matrix above are
x
+
y
=
0
z
=
2
0
=
0
Use the ï¬rst equation to solve for
x and the second to solve for
z to obtain the general
form of a solution to the system:
x
=
 y
z
=
2
y
is
free
In the preceding example
y can take on any scalar value. For example
x
=
0,
z
=
2,
y
=
0 is a solution to the original system (check this). Likewise,
x
=
 5,
z
=
2,
y
=
5 is a solution to the system. Clearly, we have an inï¬nite number of solutions to the
system, thanks to the appearance of free variables. Up to this point, the linear systems
we have considered had unique solutions, so every variable was solved for, and hence
bound. Another point to note, incidentally, is that the scalar ï¬eld we choose to work on
has an effect on our answer. The default is that
y is allowed to take on any real value
from
R: But if, for some reason, we choose to work with the complex numbers as our
scalars, then
y would be allowed to take on any complex value from
C
: In this case,
another solution to the system would be given by
x
=
 3
 i,
z
=
2,
y
=
3
+
i, for
example.
To summarize, then, once we have completed Gauss-Jordan elimination on an aug-
mented matrix, we can immediately spot the free and bound variables of the system:
the column of a bound variable will have a pivot in it, while the column of a free vari-
able will not. Another example will illustrate the point.
EXAMPLE 1.3.8. Suppose the augmented matrix of a linear system of three equations
involving variables
x;
y
;
z
;
w becomes, after applying suitable elementary row opera-
tions,
2
4
1
2
0
 1
2
0
0
1
3
0
0
0
0
0
0
3
5
Describe the general solution to the system.

26
1. LINEAR SYSTEMS OF EQUATIONS
SOLUTION. We solve this problem by observing that the ï¬rst and third columns have
pivots in them, which the second and fourth do not. The ï¬fth column represents the
right hand side. Put our little reminder labels in the matrix and we obtain
2
6
6
6
4
x
y
z
w
rhs
1
2
0
 1
2
0
0
1
3
0
0
0
0
0
0
3
7
7
7
5
Hence,
x and
z are bound variables, while
y and
w are free. The two nontrivial equations
that are represented by this matrix are
x
+
2y
 w
=
2
z
+
3w
=
0
Use the ï¬rst to solve for x and the second to solve for
z to obtain the general solution
x
=
2
 2y
+
w
z
=
 3w
y
;
w
are
free
We have seen so far that a linear system may have exactly one solution or inï¬nitely
many. Actually, there is only one more possibility which is illustrated by the following
example.
EXAMPLE 1.3.9. Solve the linear system
x
+
y
=
1
2x
+
y
=
2
3x
+
2y
=
5
We extract the augmented matrix and proceed with Gauss-Jordan elimination. This
time weâ€™ll save a little space by writing more than one elementary operation between
matrices. It is understood that they are done in order, starting with the top one. This is
a very efï¬cient way of doing hand calculations and minimizing the amount of rewriting
of matrices as we go.
2
4
1
1
1
2
1
2
3
2
5
3
5
       !
E
21
( 2)
E
31
( 3)
2
4
1
1
1
0
 1
0
0
 1
2
3
5
     !
E
32
( 1)
2
4
1
1
1
0
 1
0
0
0
2
3
5
Stop everything! We arenâ€™t done with Gauss-Jordan elimination yet since weâ€™ve only
done the forward solving portion. But something strange is going on here. Notice that
the third row of the last matrix above stands for the equation
0x
+
0y
=
2, i.e.,
0
=
2:
This is impossible. What this matrix is telling us is that the original system has no
solution, i.e., it is inconsistent. A system can be identiï¬ed as inconsistent as soon as
one encounters a leading entry in the column of constant terms. For this always means
that an equation of the form
0
=nonzero constant has been formed from the system by
legitimate algebraic operations. Thus, one needs proceed no further. The system has no
solutions.
DEFINITION 1.3.10. A system of equations is consistent if it has at least one solution.
Consistent
Systems
Otherwise it is called inconsistent.

1.3. GAUSSIAN ELIMINATION: BASIC IDEAS
27
Our last example is one involving complex numbers explicitly.
EXAMPLE 1.3.11. Solve the following system of equations:
x
+
y
=
4
( 1
+
i)x
+
y
=
 1
SOLUTION. The procedure is the same, no matter what the ï¬eld of scalars is. Of course,
the arithmetic is a bit harder. Gauss-Jordan elimination yields

1
1
4
 1
+
i
1
 1

      !
E
21
(1
 i)

1
1
4
0
2
 i
3
 4i

         !
E
2
(1=(2
 i))

1
1
4
0
1
2
 i

     !
E
12
( 1)

1
0
2
+
i
0
1
2
 i

Here we used the fact that
3
 4i
2
 i
=
(3
 4i)(2
+
i)
(2
 i)(2
+
i)
=
10
 5i
5
=
2
 i
Thus, we see that the system has unique solution
x
=
2
+
i
y
=
2
 i
1.3 Exercises
1. For each of the following matrices identify the size and the
(i;
j
)th entry for all
relevant indices
i and
j:
(a)

1
 1
0
1
 2
2
1
1

(b)
2
4
0
1
0
2
2
1
2
2
0
3
5 (c)
2
4
1
2
 2
4
0
0
3
5
2. Exhibit the augmented matrix of each system and give its size. Then use Gaussian
elimination and backsolving to ï¬nd the general solution to the systems.
(a)
2x
+
3y
=
7
(b)
3x
1
+
6x
2
 x
3
=
 4
(c)
x
1
+
x
2
=
 2
x
+
2y
=
 2
 2x
1
 4x
2
+
x
3
=
3
5x
1
+
2x
2
=
5
x
3
=
1
x
1
+
2x
2
=
 7
3. Exhibit the augmented matrix of each system and give its size. Then use Gauss-
Jordan elimination to ï¬nd the general solution to the systems.
(a)
x
1
+
x
2
+
x
4
=
1
(b)
x
3
+
x
4
=
0
(c)
x
1
+
x
2
+
3x
3
=
2
2x
1
+
2x
2
+
x
3
+
x
4
=
1
 2x
1
 4x
2
+
x
3
=
0
2x
1
+
5x
2
+
9x
3
=
1
2x
1
+
2x
2
+
2x
4
=
2
3x
1
+
6x
2
 x
3
+
x
4
=
0
x
1
+
2x
2
+
4x
3
=
1

28
1. LINEAR SYSTEMS OF EQUATIONS
4. Each of the following matrices results from applying Gauss-Jordan elimination to
the augmented matrix of a linear system. In each case, write out the general solution to
the system or indicate that it is inconsistent.
(a)
2
4
1
0
0
4
0
0
1
2
0
0
0
0
3
5
(b)
2
4
1
0
0
1
0
1
0
2
0
0
1
2
3
5 (c)
2
4
1
0
0
1
0
1
0
2
0
0
0
1
3
5
5. Use any method to ï¬nd the solution to each of the following systems. Here,
b
1
;
b
2
are constants and
x
1
;
x
2 are the unknowns.
(a)
x
1
 x
2
=
b
1
(b)
x
1
 x
2
=
b
1
(c)
ix
1
 x
2
=
b
1
x
1
+
2x
2
=
b
2
2x
1
 2x
2
=
b
2
2x
1
+
2x
2
=
b
2
6. Use Gauss-Jordan elimination to ï¬nd the general solution to these systems. Show
the elementary operations you use.
(a)
2x
1
+
x
2
+
7x
3
=
 1
3x
1
+
2x
2
 2x
4
=
1
2x
1
+
2x
2
+
2x
3
 2x
4
=
4
(b)
x
1
+
x
2
+
x
3
 x
4
=
2
2x
1
+
x
2
 2x
4
=
1
2x
1
+
2x
2
+
2x
3
 2x
4
=
4
7. Exercise 6 of Section 1.1 led to the following system. Solve it and see if there
exists a nontrivial solution consisting of positive numbers. Why is this important for the
problem?
8x
 y
 4z
 4w
=
0
 3x
+
6y
 2z
 1w
=
0
 3x
 4y
+
8z
 3w
=
0
 2x
 1y
 2z
+
8w
=
0
8. Apply the operations found in Exercise 6 in the same order to right hand side vector
b
=
2
4
b
1
b
2
b
3
3
5
:What does this tell you about the systemâ€™s consistency?
9. Suppose that we want to solve the three systems with the same left hand side
(a)
x
1
+
x
2
=
1
(b)
x
1
+
x
2
=
0
(c)
x
1
+
x
2
=
2
x
2
+
2x
3
=
0
x
2
+
2x
3
=
0
x
2
+
2x
3
=
3
2x
2
+
x
3
=
0
2x
2
+
x
3
=
0
2x
2
+
x
3
=
3
Show how to do this efï¬ciently by using only one augmented matrix consisting of the
common coefï¬cient matrix and the three right hand sides stacked along side each other.
10. Show that the following nonlinear systems become linear if we view the unknowns
as
1=x;
1=y and
1=z rather than
x;
y and
z
: Use this to ï¬nd the solution sets of the
nonlinear systems. (You must also account for the possibilities that one of
x;
y
;
z is
zero.)
(a)
2x
 y
+
3xy
=
0
4x
+
2y
 xy
=
0
(b)
y
z
+
3xz
 xy
=
0
y
z
+
2xy
=
0
11. Use a computer program or calculator with linear algebra capabilities (such as De-
rive, Maple, Mathematica, Macsyma, MATLAB, TI-85, HP48, etc.) to solve the system
of Example 1.1.5 with
n
=
8 and
f
(x)
=
x:

1.4. GAUSSIAN ELIMINATION: GENERAL PROCEDURE
29
12. Write out the system derived from the input-output model of page 7 and use your
computer system or calculator to solve it. Is the solution physically meaningful?
13. Solve the linear system that was found in Exercise 11 on page 8. Does this data
network have any steady state solutions?
14. Suppose the function
f
(x) is to be interpolated at three interpolating points
x
0
;
x
1
;
x
2
by a quadratic polynomial
p(x)
=
a
+
bx
+
cx
2, that is,
f
(x
i
)
=
p(x
i
);
i
=
0;
1;
2: As
in Exercise 7 of Section 1.1, this leads to a system of three linear equations in the three
unknowns
a;
b;
c:
(a) Write out these equations.
(b) Apply the equations of part (a) to the speciï¬c
f
(x)
=
sin(x);
0

x

 with
x
j
equal
0;

=2;

; and graph the resulting quadratic against
f
(x):
(c) Plot the error function
f
(x)
 p(x) and estimate the largest value of the error function
by trial and error.
(d) Find three points
x
1
;
x
2
;
x
3 on the interval
0

x

 for which the resulting
interpolating quadratic gives an error function with a smaller largest value than that
found in part (c).
15. Solve the network system of Exercise 11 and exhibit all physically meaningful
solutions.
1.4. Gaussian Elimination: General Procedure
The preceding section introduced Gaussian elimination and Gauss-Jordan elimination
at a practical level. In this section we will see why these methods work and what they
really mean in matrix terms. Then we will ï¬nd conditions of a very general nature
under which a linear system has (none, one or inï¬nitely many) solutions. A key idea
that comes out of this section is the notion of the rank of a matrix.
Equivalent Systems
The ï¬rst question to be considered is this: how is it that Gaussian elimination or Gauss-
Jordan elimination gives us every solution of the system we begin with and only so-
lutions to that system? To see that linear systems are special, consider the following
nonlinear system of equations.
EXAMPLE 1.4.1. Solve for the real roots of the system
x
+
y
=
2
p
x
=
y

30
1. LINEAR SYSTEMS OF EQUATIONS
SOLUTION. Letâ€™s follow the Gauss-Jordan elimination philosophy of using one equa-
tion to solve for one unknown. So the ï¬rst equation enables us to solve for
y to get
y
=
2
 x: Next substitute this into the second equation to obtain
p
x
=
2
 x: Then
square both sides to obtain
x
=
(2
 x)
2, or
0
=
x
2
 5x
 4
=
(x
 1)(x
 4)
Now
x
=
1 leads to
y
=
1, which is a solution to the system. But
x
=
4 gives
y
=
 2,
which is not a solution to the system since
p
x cannot be negative.
What went wrong in this example is that the squaring step introduced extraneous solu-
tions to the system. Why is Gaussian or Gauss-Jordan elimination safe from this kind of
difï¬culty? The answer lies in examining the kinds of operations we perform with these
methods. First, we need some terminology. Up to this point we have always described
a solution to a linear system in terms of a list of equations. For general problems this is
a bit of a nuisance. Since we are using the matrix/vector notation, we may as well go all
the way and use it to concisely describe solutions as well. We will use column vectors
to deï¬ne solutions as follows.
DEFINITION 1.4.2. A solution vector for the general linear system given by Equa-
tion 1.1.1 is a vector
x
=
2
6
6
6
4
s
1
s
2
...
s
n
3
7
7
7
5
such that the resulting equations are satisï¬ed for these choices of the variables. The
set of all such solutions is called the solution set of the linear system, and two linear
systems are said to be equivalent if they have the same solution sets.
We will want to make frequent reference to vectors without having to display them in the
text. Of course, for row vectors (1

n) this is no problem. To save space in referring to
column vectors, we shall adopt the convention that a column vector will also be denoted
by a tuple with the same entries.
NOTATION 1.4.3. The
n-tuple
(x
1
;
x
2
;
:
:
:
;
x
n
) is a shorthand for the
n

1 column
vector
x with entries
x
1
;
x
2
;
:
:
:
;
x
n
:
For example, we can write
(1;
3;
2) in place of
2
4
1
3
2
3
5
EXAMPLE 1.4.4. Describe the solution sets of all the examples worked out in the pre-
vious section.
SOLUTION. Here is the solution set to Example 1.3.1. It is the singleton set
S
=


2
3

=
f(2;
3)g
The solution set for Example 1.3.5 is
S
=
f(1;
2;
1)g: (Remember that we can designate
column vectors by tuples if we wish.)

1.4. GAUSSIAN ELIMINATION: GENERAL PROCEDURE
31
For Example 1.3.7 the solution set requires some fancier set notation, since it is an
inï¬nite set. Here it is :
S
=
8
<
:
2
4
 y
y
2
3
5
j
y
2
R
9
=
;
=
f
( y
;
y
;
2)
j
y
2
Rg
Example 1.3.9 was an inconsistent system, so had no solutions. Hence its solution set
is
S
=
;:
Finally, the solution set for Example 1.3.11 is the singleton set
S
=
f(2
+
i;
2
 i)g:
A key question about Gaussian elimination and equivalent systems: what happens to a
system if we change it by performing one elementary row operation? After all, Gauss-
ian and Gauss-Jordan elimination amount to a sequence of elementary row operations
applied to the augmented matrix of a given linear system. The answer: nothing happens
to the solution set!
THEOREM 1.4.5. Suppose a linear system has augmented matrix
A upon which an ele-
mentary row operation is applied to yield a new augmented matrix
B corresponding to
a new linear system. Then these two linear systems are equivalent, i.e., have the same
solution set.
PROOF. If we replace the variables in the system corresponding to A by the values
of a solution, the resulting equations will be satisï¬ed. Now perform the elementary
operation in question on this system of equations to obtain that the equations for the
system corresponding to the augmented matrix
B are also satisï¬ed. Thus, every solu-
tion to the old system is also a solution to the new system resulting from performing an
elementary operation. It is sufï¬cient for us to show that the old system can be obtained
from the new one by another elementary operation. In other words, we need to show
that the effect of any elementary operation can be undone by another elementary oper-
ation. This will show that every solution to the new system is also a solution to the old
system. If
E represents an elementary operation, then the operation that undoes it could
reasonably be designated as
E
 1
; since the effect of the inverse operation is rather like
cancelling a number by multiplying by its inverse. Let us examine each elementary
operation in turn.

E
ij
: The elementary operation of switching the
ith and
jth rows of the ma-
trix. Notice that the effect of this operation is undone by performing the same
operation,
E
ij, again. This switches the rows back. Symbolically we write
E
 1
ij
=
E
ij
:

E
i
(c)
: The elementary operation of multiplying the
ith row by the nonzero
constant
c: This elementary operation is undone by performing the elementary
Inverse
Elementary
Operations
operation
E
i
(1=c); in other words, by dividing the
ith row by the nonzero con-
stant
c: We write
E
i
(c)
 1
=
E
i
(1=c):

E
ij
(d)
: The elementary operation of adding
d times the
jth row to the
ith row.
This operation is undone by subtracting
d times the
jth row to the
ith row. We
write
E
ij
(d)
 1
=
E
ij
( d):
Thus, in all cases the effects of an elementary operation can be undone by applying
another elementary operation of the same type, which is what we wanted to show.

32
1. LINEAR SYSTEMS OF EQUATIONS
The inverse notation we used here doesnâ€™t do much for us yet. In Chapter 2 this notation
will take on an entirely new and richer meaning.
The Reduced Row Echelon Form
Theorem 1.4.5 tells us that the methods of Gaussian or Gauss-Jordan elimination do
not alter the solution set we are interested in ï¬nding. Our next objective is to describe
the end result of these methods in a precise way. That is, we want to give a careful
deï¬nition of the form of the matrix that these methods lead us to, starting with the
augmented matrix of the original system. Recall that the leading entry of a row is the
ï¬rst nonzero entry of that row. (So a row of zeros has no leading entry.)
DEFINITION 1.4.6. A matrix
R is said to be in reduced row form if:
(1) The nonzero rows of
R precede the zero rows.
(2) The column numbers of the leading entries of the nonzero rows, say rows
1;
2;
:::;
r, form an increasing sequence of numbers
c
1
<
c
2
<



<
c
r
:
The matrix
R said to be in reduced row echelon form if, in addition to the above:
(3) Each leading entry is a
1:
(4) Each leading entry has only zeros above it.
EXAMPLE 1.4.7. Consider the following matrices (whose leading entries are enclosed
in a circle). Which are in reduced row form? reduced row echelon form?
(a)
"
1
2
0
3
#
(b)
"
1
2
0
0
0
3
#
(c)
"
0
0
0
1
0
0
#
(d)
2
6
4
1
2
0
0
0
1
0
0
0
3
7
5
(e)
2
6
6
4
1
0
0
0
0
1
0
1
0
3
7
7
5
SOLUTION. Checking through (1)-(2), we see that (a), (b) and (d) fulï¬ll all the con-
ditions for reduced row matrices. But (c) fails, since a zero row precedes the nonzero
ones; matrix (e) fails to be reduced row form because the column numbers of the lead-
ing entries do not form an increasing sequence. Matrices (a) and (b) donâ€™t satisfy (3),
so matrix (d) is the only one that satisï¬es (3)-(4). Hence, it is the only matrix in the list
in reduced row echelon form.
We can now describe the goal of Gaussian elimination as follows: use elementary row
operations to reduce the augmented matrix of a linear system to reduced row form;
then back solve the resulting system. On the other hand, the goal of Gauss-Jordan
elimination is to use elementary operations to reduce the augmented matrix of a linear
system to reduced row echelon form. From this form one can read off the solution(s) to
the system.
Is it always possible to reduce a matrix to a reduced row form or row echelon form?
If so, how many? These are important questions because, when we take the matrix
in question to be the augmented matrix of a linear system, what we are really asking
becomes: does Gaussian elimination always work on a linear system? If so, do they lead

1.4. GAUSSIAN ELIMINATION: GENERAL PROCEDURE
33
us to answers that have the same form? Notice how the last question was phrased. We
know that the solution set of a linear system is unaffected by elementary row operations.
Therefore, the solution sets we obtain will always be the same with either method, as
sets. But couldnâ€™t the form change? For instance, in Example 1.3.7 we obtained a form
for the general solution that involved one free variable,
y, and two bound variables
x
and
z
: Is it possible that by a different sequence of elementary operations we could have
reduced to a form where there were two free variables and only one bound variable?
This would be a rather different form, even though it might lead to the same solution
set.
Certainly, matrices can be transformed by elementary row operations to different re-
duced row forms, as the following simple example shows:
A
=

1
2
4
0
2
 1

     !
E
12
( 1)

1
0
5
0
2
 1

     !
E
2
(1=2)

1
0
5
0
1
 1=2

Every matrix of this example is already in reduced row form. The last matrix is also in
reduced row echelon form. Yet all three of these matrices can be obtained from each
other by elementary row operations. It is signiï¬cant that only one of the three matrices
is in reduced row echelon form. As a matter of fact, any matrix can be reduced by
elementary row operations to one and only one reduced row echelon form, which we can
call the reduced row echelon form of the given matrix. The example above shows that
the matrix
A has as its reduced row echelon form the matrix
E
=

1
0
5
0
1
 1=2

:
Our assertions are justiï¬ed by the following fundamental theorem about matrices.
THEOREM 1.4.8. Every matrix can be reduced by a sequence of elementary row oper-
Uniqueness of
Reduced Row
Echelon Form
ations to one and only one reduced row echelon form.
PROOF. First we show that every
m

n matrix
A can be reduced to some reduced
row echelon form. Here is the algorithm we have been using: given that the ï¬rst
s
columns of
A are in reduced row echelon form with
r nonzero rows and that
r
<
m and
s
<
n, ï¬nd the smallest column number
j such that
a
ij
6=
0 and
i
>
r,
j
>
s: If none is
found,
A is in reduced row echelon form. Otherwise, interchange rows
i and
r
+
1, then
use elementary row operations to convert
a
r
+1;j to
1, and to zero out the entries above
and below this one. Now set
s
=
j and increment
r by one. Continue this procedure
until
r
=
m or
s
=
n: This must occur at some point since both
r and
s increase with
each step, and when it occurs, the resulting matrix is in reduced row echelon form.
Next, we prove uniqueness. Suppose that some matrix could be reduced to two distinct
reduced row echelon forms. We show this is impossible. If it were possible, we could
ï¬nd an example
m

n matrix
e
A with the fewest possible columns
n; that is, the theorem
is true for every matrix with fewer columns. Then
n
>
1; since a single column matrix
can be reduced to only one reduced row echelon form, namely either the
0 column or a
column with ï¬rst entry
1 and the other entries
0: Now
e
A can be reduced to two reduced
row echelon forms, say
R
1 and
R
2, with
R
1
6=
R
2
: Write
e
A
=
[A
j
b] so that we can
think of
e
A as the augmented matrix of a linear system (1.1.1). Now for
i
=
1;
2 write
each
R
i as
R
i
=
[L
i
j
b
i
], where
b
i is the last column of the
m

n matrix
R
i, and
L
i
is the
m

(n
 1) matrix formed from the ï¬rst
n
 1 columns of
R
i
: Each
L
i satisï¬es
the deï¬nition of reduced row echelon form, since each
R
i is in reduced row echelon
form. Also, each
L
i results from performing elementary row operations on the matrix

34
1. LINEAR SYSTEMS OF EQUATIONS
A, which has only
n
 1 columns. By the minimum columns hypothesis, we have that
L
1
=
L
2
: There are two possibilities to consider.
Case 1: The last column
b
i of either
R
i has a leading entry in it. Then the system
of equations represented by
e
A is inconsistent. It follows that both columns
b
i have a
leading entry in them, which must be a
1 in the ï¬rst row whose portion in
L
i consists
of zeros, and the entries above and below this leading entry must be
0: Since
L
1
=
L
2,
it follows that
b
1
=
b
2, and thus
R
1
=
R
2, a contradiction. So this case canâ€™t occur.
Case 2: Each
b
i has no leading entry in it. Then the system of equations represented by
e
A is consistent. Both augmented matrices have the same basic and free variables since
L
1
=
L
2
: Hence we obtain the same solution with either augmented matrix by setting
the free variables of the system equal to
0: When we do so, the bound variables are
uniquely determined: the ï¬rst equation says that the ï¬rst bound variable equals the ï¬rst
entry in the right hand side vector, the second says that the second bound variable equals
the second entry in the right hand side vector, and so forth. Whether we use
R
1 or
R
2 to
solve the system, we obtain the same result, since we can manipulate one such solution
into the other by elementary row operations. Therefore,
b
1
=
b
2 and thus
R
1
=
R
2,
a contradiction again. Hence, there can be no counterexample to the theorem, which
completes the proof.
The following consequence of the preceding theorem is a fact that we will ï¬nd helpful
in Chapter 2.
COROLLARY 1.4.9. Let the matrix
B be obtained from the matrix
A by performing a
sequence of elementary row operations on
A: Then
B and
A have the same reduced
row echelon form.
PROOF. We can obtain the reduced row echelon form of
B in the following man-
ner: ï¬rst perform the elementary operations on
B that undo the ones originally per-
formed on
A to get
B
: The matrix
A results from these operations. Now perform
whatever elementary row operations are needed to reduce
A to its reduced row eche-
lon form. Since
B can be reduced to one and only one reduced row echelon form, the
reduced row echelon forms of
A and
B coincide, which is what we wanted to show.
Rank and Nullity of a Matrix
Now that we have Theorem 1.4.8 in hand, we can introduce the notion of rank of a
matrix. Since
A can be reduced to one and only one reduced row echelon form by
Theorem 1.4.8, we see that the following deï¬nition is unambiguous.
DEFINITION 1.4.10. The rank of a matrix
A is the number of nonzero rows of the
reduced row echelon form of
A: This number is written as
rank
A:
Rank can also be deï¬ned as the number of nonzero rows in any reduced row form of
a matrix. One has to check that any two reduced row forms have the same number of
nonzero rows. Notice that the rank can also be deï¬ned as the number of columns of
the reduced row echelon form with leading entries in them, since each leading entry
of a reduced row echelon form occupies a unique column. We can count up the other
columns as well.

1.4. GAUSSIAN ELIMINATION: GENERAL PROCEDURE
35
DEFINITION 1.4.11. The nullity of a matrix
A is the number of columns of the reduced
row echelon form of
A that do not contain a leading entry. This number is written as
n
ull
A:
In the case that
A is the coefï¬cient matrix of a linear system, we can interpret the rank
of
A as the number of bound variables of the system and the nullity of
A as the number
of free variables of the system.
Observe that the rank of a matrix is a non-negative number. But it could be
0! This
happens when the matrix is a zero matrix, so that it has no nonzero rows. In this case,
the nullity of the matrix is as large as possible. Here are some simple limits on the
size of
rank
A and
n
ull
A: In one limit we shall use a notation that occurs frequently
throughout the text, so we explain it ï¬rst.
NOTATION 1.4.12. For a list of real numbers
a
1
;
a
2
;
:
:
:
;
a
m
;
minfa
1
;
a
2
;
:
:
:
;
a
m
g
means the smallest number in the list and
maxfa
1
;
a
2
;
:
:
:
;
a
m
g means the largest num-
ber in the list.
THEOREM 1.4.13. Let
A be an
m

n matrix. Then
1.
0

rank
A

minfm;
ng:
2.
rank
A
+
n
ull
A
=
n:
PROOF. By deï¬nition,
rank
A is the number of nonzero rows of the reduced row
echelon form of
A, which is itself an
m

n matrix. There can be no more leading
entries than rows, hence
rank
A

m: Also, each leading entry of a matrix in reduced
row echelon form is the unique nonzero entry in its column. Therefore, there can be no
more leading entries than columns
n: Since
rank
A is less than or equal to both
m and
n, it must be less than or equal to their minimum, which is what the ï¬rst inequality says.
Also notice that every column of
A either has a pivot in it or not. The number of pivot
columns is just
rank
A and the number of non-pivot columns is
n
ull
A: Hence the sum
of these numbers is
n:
In words, part 1 of Theorem 1.4.13 says that the rank of a matrix cannot exceed the
number of rows or columns of the matrix. One situation occurs often enough enough
that it is entitled to its own name: if the rank of a matrix equals its column number we
say that the matrix has full column rank.
One has to be a little careful about this idea
Full Column
Rank
of rank. Consider the following example.
EXAMPLE 1.4.14. Find the rank and nullity of the matrix
A
=
2
4
1
1
2
2
2
5
3
3
7
3
5
SOLUTION. We know that the rank is at most
3 by the preceding theorem. Elementary
row operations give
2
4
1
1
2
2
2
5
3
3
7
3
5
     !
E
21
( 2)
2
4
1
1
2
0
0
1
3
3
2
3
5
     !
E
31
( 3)
2
4
1
1
2
0
0
1
0
0
 4
3
5

36
1. LINEAR SYSTEMS OF EQUATIONS
       !
E
32
(4)
E
12
( 2)
2
4
1
1
0
0
0
1
0
0
0
3
5
From the reduced row echelon form of
A at the far right we see that the rank of
A is
2:
Notice that one canâ€™t simply count the number of nonzero rows of
A, which in this case
is
3, to get the rank of
A:
Caution: Remember that the rank of
A is the number of nonzero rows of its reduced
row form, and not the number of nonzero rows of
A itself.
The notion of rank of a matrix gives us some more leverage on the question of how the
solution set of a linear system behaves.
THEOREM 1.4.15. The general linear system 1.1.1 with
m

n coefï¬cient matrix
A,
Consistency in
Terms of Rank
right hand side vector
b and augmented matrix
e
A
=
[A
j
b] is consistent if and only if
rank
A
=
rank
e
A, in which case either
1.
rank
A
=
n, in which case the system has a unique solution, or
2.
rank
A
<
n, in which case the system has inï¬nitely many solutions.
PROOF. We can reduce
e
A to reduced row echelon form by ï¬rst doing the elemen-
tary operations that reduce the
A part of the matrix to reduced row echelon form, then
attending to the last column. Hence, it is always the case that
rank
A

rank
e
A: The
only way to get strict inequality is to have a leading entry in the last column, which
means that some equation in the equivalent system corresponding to the reduced aug-
mented matrix is
0
=
1, which implies that the system is inconsistent. On the other
hand, we have already seen (in the proof of Theorem 1.4.8, for example) that if the last
column does not contain a leading entry, then the system is consistent. This establishes
the ï¬rst statement of the theorem.
Now suppose that
rank
A
=
rank
e
A, so that the system is consistent. By Theorem
1.4.13,
rank
A

n, so that either
rank
A
<
n or
rank
A
=
n: The number of variables
of the system is
n: Also, the number of leading entries (equivalently, pivots) of the
reduced row form of
e
A, which is
rank
A, is equal to the number of bound variables;
the remaining
n
 rank
A variables are the free variables of the system. Thus, to say
that
rank
A
=
n, is to say that no variables are free; that is, solving the system leads to
a unique solution. And to say that
rank
A
<
n is to say that there is at least one free
variable in which case the system has inï¬nitely many solutions.
Here is an example of how this theorem can be put to work. It conï¬rms our intuition
that if a system does not have â€œenoughâ€ equations, then it canâ€™t have a unique solution.
COROLLARY 1.4.16. If a consistent linear system of equations has more unknowns
than equations, then the system has inï¬nitely many solutions.
PROOF. In the notation of the previous theorem, the hypothesis simply means that
m
<
n: But we know from Theorem 1.4.13 that
rank
A

minfm;
ng: Thus
rank
A
<
n and the last part of Theorem 1.4.15 applies to give the desired result.

1.4. GAUSSIAN ELIMINATION: GENERAL PROCEDURE
37
Of course, there is still the question of when a system is consistent. In general, there
isnâ€™t an easy way to see when this is so. However, in special cases we can answer the
question easily. One such important special case is given by the following deï¬nition.
DEFINITION 1.4.17. The general linear system 1.1.1 with
m

n coefï¬cient matrix
A
Homogeneous
Systems
and right hand side vector
b is said to be homogeneous if the entries of
b are all zero.
Otherwise, the system is said to be non-homogeneous.
The nice feature of homogeneous systems is that they are always consistent! In fact,
it is easy to exhibit a speciï¬c solution to the system, namely, take the value of all the
variables to be zero. For obvious reasons this solution is called the trivial solution to
the system. Thus, the previous corollary implies that a homogeneous linear system with
fewer equations than unknowns must have inï¬nitely many solutions. Of course, if we
want to ï¬nd all the solutions, we will have to do the work of Gauss-Jordan elimination.
However, we acquire a small notational convenience in dealing with homogeneous sys-
tems. Notice that the right hand side of zeros is never changed by an elementary row
operation. So why bother writing out the augmented matrix of such a system? It sufï¬ces
to perform elementary operations on the coefï¬cient matrix alone. In the end, the right
hand side is still a column of zeros.
EXAMPLE 1.4.18. Solve and describe the solution set of the homogeneous system
x
1
+
x
2
+
x
4
=
0
x
1
+
x
2
+
2x
3
=
0
x
1
+
x
2
=
0
SOLUTION. In this case we only perform row operations on the coefï¬cient matrix to
obtain
2
4
1
1
0
1
1
1
2
0
1
1
0
0
3
5
       !
E
21
( 1)
E
31
( 1)
2
4
1
1
0
1
0
0
2
 1
0
0
0
 1
3
5
       !
E
2
(1=2)
E
3
( 1)
2
4
1
1
0
1
0
0
1
 1=2
0
0
0
1
3
5
       !
E
23
(1=2)
E
13
( 1)
2
4
1
1
0
0
0
0
1
0
0
0
0
1
3
5
One has to be a little careful here: the leading entry in the last column does not indicate
that the system is inconsistent. Had we carried the right hand side column along in the
calculations above, we would have obtained
2
4
1
1
0
0
0
0
0
1
0
0
0
0
0
1
0
3
5
which is the matrix of a consistent system. We see from the reduced row echelon form
of the coefï¬cient matrix that
x
2 is free and the other variables are bound. The general
solution is
x
1
=
 x
2
x
3
=
0
x
4
=
0
x
2
is
free.

38
1. LINEAR SYSTEMS OF EQUATIONS
Finally, the solution set S of this system can be described as
S
=
f( x
2
;
0;
0;
x
2
)
j
x
2
2
Rg:
1.4 Exercises
1. Circle leading entries and determine which of the following matrices are in reduced
row echelon form.
(a)
2
4
1
0
0
4
0
0
0
0
0
0
1
2
3
5
(b)
2
4
1
0
0
1
0
1
0
2
0
0
0
1
3
5
(c)

0
1
0
1
1
0
0
2

(d)

1
0
2
0
0
0

(e)
2
4
1
0
0
0
1
0
0
0
1
3
5
(f)
2
4
0
1
0
0
0
0
3
5
2. Show the elementary operations you use to ï¬nd the reduced row echelon form of the
following matrices. Give the rank and nullity of each matrix.
(a)
2
4
1
 1
2
1
3
4
2
2
6
3
5
(b)
2
4
3
1
9
2
 3
0
6
 5
0
0
1
2
3
5
(c)

0
1
0
1
2
0
0
2

(d)
2
4
2
4
2
4
9
3
2
3
3
3
5
(e)

2
2
5
6
1
1
 2
2

(f)
2
4
2
1
1
1
2
1
1
1
2
3
5
3. Find the rank of the augmented and coefï¬cient matrix of the following linear systems
and the solution to to following systems.
(a)
x
1
+
x
2
+
x
3
 x
4
=
2
(b)
x
3
+
x
4
=
0
2x
1
+
x
2
 2x
4
=
1
 2x
1
 4x
2
=
0
2x
1
+
2x
2
+
2x
3
 2x
4
=
4
3x
1
+
6x
2
 x
3
+
x
4
=
0
4. Consider two systems of equations
(A)
x
1
+
x
2
+
x
3
 x
4
=
2
(B)
x
3
+
x
4
=
0
2x
1
+
x
2
 2x
4
=
1
 2x
1
 4x
2
=
0
2x
1
+
2x
2
+
2x
3
 2x
4
=
4
3x
1
+
6x
2
 x
3
+
x
4
=
0
(a) Find a sequence of elementary operations that transforms system (A) into (B).
(b) It follows that these two systems are equivalent. Why?
(c) Conï¬rm part (b) by explicitly solving each of these systems.
5. The rank of the following matrices can be determined by inspection. Inspect these
matrices and give their rank. Give reasons for your answers.
(a)
2
4
1
3
0
0
1
1
0
1
1
3
5 (b)
2
4
0
0
0
0
0
0
0
0
0
3
5 (c)
2
4
0
0
1
0
1
0
1
0
0
3
5 (d)
2
4
3
1
1
3
5

1.5. *COMPUTATIONAL NOTES AND PROJECTS
39
6. Find upper and lower bounds on the rank of the
4

3 matrix
A; given that some
system with coefï¬cient matrix
A has inï¬nitely many solutions.
7. Answer True/False and explain your answers:
(a) Any homogeneous linear system with more unknowns than equations has a nontriv-
ial solution.
(b) Any homogeneous linear system is consistent.
(c) If a linear system is inconsistent, then the rank of the augmented matrix exceeds the
number of unknowns.
(d) Every matrix can be reduced to only one matrix in reduced row form.
(e) A system of 3 linear equations in 4 unknowns must have inï¬nitely many solutions.
8. Suppose that
A
=

a
b
c
d

and further that
a
6=
0 and
ad
 bc
=
0: Find the
reduced row echelon form of
A:
9. Give a rank condition for a homogeneous system that is equivalent to the system
having a unique solution. Justify your answer.
10. Prove or disprove: if two linear systems are equivalent, then they must have the
same size augmented matrix.
11. Use Theorem 1.4.8 to show that any two reduced row forms for a matrix
A must
have the same number of nonzero rows.
12. Suppose that the matrix
C can be written in the augmented form
C
=
[A
j
B
];
where the matrix
B may have more than one column. Prove that
rank
C

rank
A
+
rank
B
:
1.5. *Computational Notes and Projects
Roundoff Errors
In many practical problems, calculations are not exact. There are several reasons for this
unfortunate fact. For one thing, scientiï¬c calculators are by their very nature only ï¬nite
precision machines. That is, only a ï¬xed number of signiï¬cant digits of the numbers we
are calculating may be used in any given calculation. For instance, verify this simple
arithmetic fact on a calculator or computational software like MATLAB (but exclud-
ing computer algebra systems such as Derive, Maple or Mathematica â€“ since symbolic
calculation is the default on these systems, they will give the correct answer):
((
2
3
+
100)
 100)
 2
3
=
0

40
1. LINEAR SYSTEMS OF EQUATIONS
In many cases this calculation will not yield
0: The problem is that if, for example, a
calculator uses
6 digit accuracy, then
2=3 is calculated as
0:666667, which is really
incorrect. Even if arithmetic calculations were exact, the data which form the basis of
our calculations are often derived from scientiï¬c measurement which themselves will
almost certainly be in error. Starting with erroneous data and doing an exact calculation
can be as bad as starting with exact data and doing an inexact calculation. In fact, in a
certain sense they are equivalent to each other. Error resulting from truncating data for
storage or ï¬nite precision arithmetic calculations is called roundoff error.
We will not give an elaborate treatment of roundoff error. A thorough analysis can be
found in the Golub and Van Loan text [5] of the bibliography, a text which is considered
a standard reference work. This subject is a part of an entire ï¬eld of applied mathematics
known as numerical analysis. We will consider this question: could roundoff error be a
signiï¬cant problem in Gaussian elimination? It isnâ€™t at all clear that there is a problem.
After all, even in the above example, the ï¬nal error is relatively small. Is it possible that
with all the arithmetic performed in Gaussian elimination the errors pile up and become
large? The answer is â€œyes.â€ With the advent of computers came a heightened interest in
these questions. In the early 1950â€™s numerical analysts intensiï¬ed efforts to determine
whether or not Gaussian elimination can reliably solve larger linear systems. In fact,
we donâ€™t really have to look at complicated examples to realize that there is a potential
difï¬culty. Consider the following example.
EXAMPLE 1.5.1. Let
 be a number so small that our calculator yields
1
+

=
1: This
equation appears a bit odd, but from the calculatorâ€™s point of view it may be perfectly
correct; if, for example, our calculator performs
6 digit arithmetic, then

=
10
 6will
do nicely. Notice that with such a calculator,
1
+
1=
=
(
+
1)=
=
1=: Now solve
the linear system
x
1
+
x
2
=
1
x
1
 x
2
=
0
SOLUTION. Letâ€™s solve this system by Gauss-Jordan elimination with our calculator to
obtain


1
1
1
 1
0

       !
E
21
( 1=)


1
1
0
1=
 1=

   !
E
2
()


1
1
0
1
1

     !
E
12
( 1)


0
0
0
1
1

     !
E
1
(1=)

1
0
0
0
1
1

Thus we obtain the calculated solution
x
1
=
0,
x
2
=
1: This answer is spectacularly
bad! If

=
10
 6 as above, then the correct answer is
x
1
=
x
2
=
1
1
+

=
0:99999909999990



Our calculated answer is not even good to one digit. So we see that there can be serious
problems with Gaussian or Gauss-Jordan elimination on ï¬nite precision machines.
It turns out that information that would be signiï¬cant for
x
1 in ï¬rst equation is lost in
the truncated arithmetic that says that
1
+
1=
=
1=: There is a ï¬x for problems such
as this, namely a technique called partial pivoting. The idea is fairly simple: do not
choose the next available column entry for a pivot. Rather, search down the column in
Pivoting
Strategies

1.5. *COMPUTATIONAL NOTES AND PROJECTS
41
question for the largest entry (in absolute value). Then switch rows, if necessary, and
use this entry as a pivot. For instance, in the preceding example, we would not pivot off
the
 entry of the ï¬rst column. Since the entry of the second row, ï¬rst column, is larger
in absolute value, we would switch rows and then do the usual Gaussian elimination
step. Here is what we would get (remember that with our calculator
1
+

=
1):


1
1
1
 1
0

 !
E
21

1
 1
0

1
1

     !
E
21
( )

1
 1
0
0
1
1

     !
E
12
( )

1
0
1
0
1
1

Now we get the quite acceptable answer
x
1
=
x
2
=
1:
But partial pivoting is not a panacea for numerical problems. In fact, it can be easily
defeated. Multiply the second equation by

2 and we get a system for which partial piv-
oting still picks the wrong pivot. Here the problem is a matter of scale. It can be cured
by dividing each row by the largest entry of the row before beginning the Gaussian
elimination process. This procedure is known as row scaling. The combination of row
scaling and partial pivoting overcomes many of the numerical problems of Gaussian or
Gauss-Jordan elimination (but not all!). There is a more drastic procedure, known as
complete pivoting. In this procedure one searches all the unused rows (excluding the
right hand sides) for the largest entry, then uses it as a pivot for Gaussian elimination.
The columns used in this procedure do not move in that left-to-right fashion we are
used to seeing in system solving. It can be shown rigorously that the error of roundoff
propagates in a predictable and controlled fashion with complete pivoting; in contrast,
we do not really have a satisfactory explanation as to why row scaling and partial piv-
oting work well. Yet in most cases they do reasonably well. Since this combination
involves much less calculation than complete pivoting, it is the method of choice for
many problems.
There are deeper reasons for numerical problems in solving some systems than the one
the preceding example illustrates. One difï¬culty has to do with the â€œsensitivityâ€ of the
coefï¬cient matrix to small changes. That is, in some systems, small changes in the
coefï¬cient matrix lead to dramatic changes in the exact answer. The practical effect
of roundoff error can be shown to be equivalent to introducing small changes in the
coefï¬cient matrix and obtaining an exact answer to the perturbed (changed) system.
There is no cure for these difï¬culties. A classical example of this type of problem, the
Hilbert matrix, is discussed in one of the projects below. We will attempt to quantify
this â€œsensitivityâ€ in Chapter 6.
Computational Efï¬ciency of Gaussian Elimination
How much work is it to solve a linear system and how does the amount of work grow
with the dimensions of the system? The ï¬rst thing we need is a unit of work. In
computer science one of the principal units of work is a ï¬‚op (ï¬‚oating point operation),
namely a single
+;
 ;
; or
: For example, we say the amount of work in computing
e
+
 or
e

 is one ï¬‚op, while the work in calculating
e
+
3

 is two ï¬‚ops. The
following example is extremely useful.
EXAMPLE 1.5.2. How many ï¬‚ops does it cost to add a multiple of one row to another,
as in Gaussian elimination, given that the rows have
n elements each?

42
1. LINEAR SYSTEMS OF EQUATIONS
SOLUTION. A little experimentation with an example or two shows that that the answer
should be
2n: Here is a justiï¬cation of that count. Say that row
a is to be multiplied by
the scalar
, and added to the row
b: Designate the row
a
=
[a
i
] and the row
b
=
[b
i
]:
We have
n entries to worry about. Consider a typical one, say the
ith one. The
ith entry
of
b, namely
b
i, will be replaced by the quantity
b
i
+
a
i:
The amount of work in this calculation is two ï¬‚ops. Since there are
n entries to compute,
the total work is
2n ï¬‚ops.
Our goal is to determine the expense of solving a system by Gauss-Jordan elimination.
For the sake of simplicity, letâ€™s assume that the system under consideration has
n equa-
tions in
n unknowns and the coefï¬cient matrix has rank
n: This ensures that we will
have a pivot in every row of the matrix. We wonâ€™t count row exchanges either, since
they donâ€™t involve any ï¬‚ops. (We should remark that this may not be realistic on a fast
computer, since memory fetches and stores may not take signiï¬cantly less time than a
ï¬‚oating point operation.) Now consider the expense of clearing out the entries under
the ï¬rst pivot. A picture of the augmented matrix looks something like this, where an
â€˜â€™ is an entry which may not be 0 and an â€˜
 â€™ is a nonzero pivot entry:
2
6
6
6
6
4












...
...
...
...






3
7
7
7
7
5
     !
n
 1
el.ops
2
6
6
6
6
6
4






0





...
...
...
...
0





3
7
7
7
7
7
5
Each elementary operation will involve adding a multiple of the ï¬rst row, starting with
the second entry, since we donâ€™t need to do arithmetic in the ï¬rst column â€“ we know
what goes there, to the
n
 1 subsequent rows. By the preceding example, each of
these elementary operations will cost
2n ï¬‚ops. Add
1 ï¬‚op for the cost of determining
the multiplier to obtain
2n
+
1: So the total cost of zeroing out the ï¬rst column is
(n
 1)(2n
+
1) ï¬‚ops. Now examine the lower unï¬nished block in the above ï¬gure.
Notice that itâ€™s as though we were starting over with the row and column dimensions
reduced by 1. Therefore, the total cost of the next phase is
(n
 2)(2(n
 1)
+
1) ï¬‚ops.
Continue in this fashion and we obtain a count of
0
+
n
X
j
=2
(j
 1)(2j
+
1)
=
n
X
j
=1
(j
 1)(2j
+
1)
=
n
X
j
=1
2j
2
 j
 1
ï¬‚ops. Recall the identities for sums of consecutive integers and their squares:
n
X
j
=1
j
=
n(n
+
1)
2
n
X
j
=1
j
2
=
n(n
+
1)(2n
+
1)
6

1.5. *COMPUTATIONAL NOTES AND PROJECTS
43
Thus we have a total ï¬‚op count of
n
X
j
=1
2j
2
 3j
+
1
=
2
n(n
+
1)(2n
+
1)
6
 n(n
+
1)
2
 n
=
2n
3
3
+
n
2
2
 7n
6
This is the cost of forward solving. Now letâ€™s simplify our answer a bit more. For large
n we have that
n
3 is much larger than
n or
n
2 (e.g., for
n
=
10 compare
1000 to
10
or
100). Hence, we ignore the lower degree terms and arrive at a simple approxima-
tion to the number of ï¬‚ops required to forward solve a linear system of
n equations in
n unknowns using Gauss-Jordan elimination. There remains the matter of back solv-
ing. We leave as an exercise to show that the total work of back solving is quadratic
in
n: Therefore the â€œleading orderâ€ approximation which we found for forward solving
remains unchanged.
THEOREM 1.5.3. The number of ï¬‚ops required to solve a linear system of
n equations
in
n unknowns using Gaussian or Gauss-Jordan elimination without row exchanges is
approximately
2n
3
=3:
Thus, for example, the work of forward solving a system of
21 equations in
21 un-
knowns is approximately
2

21
3
=3
=
6174 ï¬‚ops. Compare this to the exact answer of
6374:
Project Topics
In this section we give a few samples of project material. These projects provide an
opportunity for students to explore a subject in a greater depth than exercises permit.
They also provide an opportunity for students to hone their scientiï¬c computing and
writing skills. They are well suited to team effort, and writing expectations can range
from a summary of the answers to a detailed report on the project. The ï¬rst sample
project is written with the expectation of a fairly elaborate project report. The instructor
has to deï¬ne her/his own expectations for students. Likewise, the computing platform
used for the projects will vary. We cannot discuss every platform in this text, so we
will give examples of implementation notes that an instructor might supply for a few
of them. The instructor will have to modify that portion of the project to match the
local conï¬guration and provide additional background about the use of the computing
platform.
Notes to students about project/reports: The ï¬rst thing you need to know about report
writing is the intended audience. Usually, you may assume that your report will be read
by your supervisors, who are technical people such as yourself. Therefore, you should
write a brief statement of the problem and discussion of methodology. In practice re-
ports, you assume physical laws and assumptions without further justiï¬cation, but in
real life you would be expected to offer some explanation of physical principles you em-
ploy in constructing your model. Another good point to have in mind is a target length
for your paper. Do not clutter your work with long lists of numbers and try to keep
the length at a minimum rather than maximum. Most kinds of discourse should have
three parts: a beginning, a middle and an end. Roughly, a beginning should consist of
introductory material. In the middle you develop the ideas described or theses proposed
in the introduction, and in the end you summarize your work and tie up loose ends.

44
1. LINEAR SYSTEMS OF EQUATIONS
Of course, rules about paper writing are not set in concrete, and authors vary on exactly
how they organize papers of a given kind. Also, a part could be quite short; for example,
an introduction might only be a paragraph or two. Here is a sample skeleton for a report
(perhaps rather more elaborate than you need): 1. Introduction (title page, summary
and conclusions), 2. Main Sections (problem statement, assumptions, methodology,
results, conclusions), 3. Appendices (such as mathematical analysis, graphs, possible
extensions, etc.) and References.
A few additional notes: Pay attention to appearance and neatness, but donâ€™t be overly
concerned about your writing style. A good rule to remember is â€œSimpler is better.â€
Prefer short and straightforward sentences to convoluted ones. Use a vocabulary that
you are comfortable with. Be sure to use a spell-checker if one is available to you.
A given project/report assignment may be supplied with a report template by your in-
structor or carry explicit instructions about format, intended audience, etc. It is impor-
tant to read and follow these instructions carefully. Naturally, such instructions would
take precedence over any of the preceding remarks.
The ï¬rst two of these projects are based on the material of Section 1.1 in relation to
diffusion processes.
Project: Heat Flow I
Description of the problem: You are working for the ï¬rm Universal Dynamics on a
project which has a number of components. You have been assigned the analysis of a
component which is similar to a laterally insulated rod. The problem you are concerned
with is as follows: part of the specs for the rod dictate that no point of the rod should
stay at temperatures above
60 degrees Celsius for a long period of time. You must
decide if any of the materials listed below are acceptable for making the rod and write a
report on your ï¬ndings. You may assume that the rod is of unit length. Suppose further
that internal heat sources come from a position dependent function
f
(x),
0

x

1
and that heat is also generated at each point in amounts proportional to the temperature
at the point. Also suppose that the left and right ends of the rod are held at
0 and
50
degrees Celsius, respectively. When sufï¬cient time passes, the temperature of the rod at
each point will settle down to â€œsteady stateâ€ values, dependent only on position
x: These
are the temperatures you are interested in. Refer to the discussion in 1.1 for the details
of the descriptive equations that result from discretizing the problem into ï¬nitely many
nodes. Here
k is the thermal conductivity of the rod, which is a property associated
with the material used to make the rod. For your problem take the source term to be
f
(x)
=
200
cos
(x
2
): Here are the conductivity constants for the materials with which
your company is considering building the rod. Which of these materials (if any) are
acceptable?
Platinum:
k
=
:17
Zinc:
k
=
:30
Aluminum:
k
=
:50
Gold:
k
=
:75
Silver:
k
=
1:00

1.5. *COMPUTATIONAL NOTES AND PROJECTS
45
Procedure: For the solution of the problem, formulate a discrete approximation to the
BVP just as in Example 1.1.5. Choose an integer
n and divide the interval [0, 1] into
n
+
1 equal subintervals with endpoints
0
=
x
0
;
x
1
;
:::;
x
n+1
=
1: Then the width of each
subinterval is
h
=
1=(n
+
1): Next let
u
i be our approximation to
u(x
i
) and proceed
as in Example 1.1.5 . There results a linear system of
n equations in the
n unknowns
u
1
;
u
2
;
:::;
u
n
: For this problem divide the rod into
4 equally sized subintervals and take
n
=
3: Use the largest
u
i as an estimate of the highest temperature at any point in
the rod. Now double the number of subintervals and see if your values for
u change
appreciably at a given value of x. If they do, you may want to repeat this procedure
until you obtain numbers that you judge to be satisfactory.
Implementation Notes (for users of Mathematica): Set up the coefï¬cient matrix
a and
right hand side
b for the system. Both the coefï¬cient matrix and the right hand side
can be set up using the Table command of Mathematica. For
b, the command 100*
h^2*Table[Cos[(i h)^ 2,{i,n}]/k will generate
b, except for the last coor-
dinate. Use the command b[[14]] = b[[14]] + 50 to add
u(1) to the right hand
side of the system and get the correct
b: For
a: the command Table[Switch[i-
j,1,-1,0,2,-1,-1,_,0],{i,n},{j,n}]will generate a matrix of the desired
form. (Use the Mathematica on line help for all commands you want to know more
about.) For ï¬‚oating point numbers: we want to simulate ordinary ï¬‚oating point calcu-
lations on Mathematica. You will get some symbolic expressions which we donâ€™t want,
e.g., for
b: To turn
b into ï¬‚oating point approximation, use the command b = N[b].
The N[ ] function turns the symbolic values of b into numbers, with a precision of about
16 digits if no precision is speciï¬ed. For solving linear systems use the command u =
LinearSolve[a,b], which will solve the system with coefï¬cient matrix
a and right
hand side
b, and store the result in
u: About vectors: Mathematica does not distinguish
between row vectors or column vectors unless you insist on it. Hardcopy: You can get
hardcopy from Mathematica. Be sure to make a presentable solution for the project.
You should describe the form of the system you solved and at least summarize your
results. This shouldnâ€™t be a tome (donâ€™t simply print out a transcript of your session),
nor should it be a list of numbers.
Project: Heat Flow II
Problem Description: You are given a laterally insulated rod of a homogeneous material
whose conductivity properties are unknown. The rod is laid out on the x-axis, 0 <= x
<= 1. A current is run through the rod, which results in a heat source of 10 units of heat
(per unit length) at each point along the rod. The rod is held at zero temperature at each
end. After a time the temperatures in the rod settle down to a steady state. A single
measurement is taken at x=0.3 which results in a temperature reading of approximately
11 units. Based on this information, determine the best estimate you can for the true
value of the conductivity constant k of the material. Also try to guess a formula for the
shape of the temperature function on the interval [0,1] that results when this value of
the conductivity is used.
Methodology: You should use the model that is presented on pages 4-6 of the text. This
will result in a linear system, which Maple can solve. One way to proceed is simply to
use trial and error until you think youâ€™ve hit on the right value of k, that is, the one that
gives a value of approximately 11 units at x =0.3. Then plot the resulting approximate

46
1. LINEAR SYSTEMS OF EQUATIONS
function doing a dot-to-dot on the node values. You should give some thought to step
size h.
Output: Return your results in the form of an annotated Maple notebook, which should
have the name of the team members at the top of the ï¬le and an explanation of your
solution in text cells interspersed between input cells, that the user can happily click
his/her way through. This explanation should be intelligible to your fellow students.
Comments: This project introduces you to a very interesting area of mathematics called
"inverse theory." The idea is, rather than proceeding from problem (the governing equa-
tions for temperature values) to solution (temperature values), you are given the "so-
lution", namely the measured solution value at a point, and are to determine from this
information the "problem", that is, the conductivity coefï¬cient that is needed to deï¬ne
the governing equations.
Project: The Accuracy of Gaussian Elimination
Description of the problem: This project is concerned with determining the accuracy
of Gaussian elimination as applied to two linear systems, one of which is known to be
difï¬cult to solve numerically. Both of these systems will be square (equal number of
unknowns and equations) and have a unique solution. Also, both of these systems are
to be solved for various sizes, namely
n
=
4;
8;
12;
16: In order to get a handle on the
error, our main interest, we shall start with a known answer. The answer shall consist of
setting all variables equal to
1: So it is the solution vector
(1;
1;
:
:
:
;
1): The coefï¬cient
matrix shall be one of two types:
(1) A Hilbert matrix, i.e., an
n

n matrix given by the formula
H
n
=

1
i
+
j
 1

(2) An
n

n matrix with random entries between
0 and
1:
The right hand side vector
b is uniquely determined by the coefï¬cient matrix and solu-
tion. In fact, the entries of
b are easy to obtain: simply add up all the entries in the
ith
row of the coefï¬cient matrix to obtain the
ith entry of
b:
The problem is to measure the error of Gaussian elimination. This is done by ï¬nding
the largest (in absolute value) difference between the computed value of each variable
and actual value, which in all cases is
1: Discuss your results and draw conclusions from
your experiments.
Implementation Notes (for users of Maple): Maple has a built-in procedure for deï¬n-
ing a Hilbert matrix
A of size
n; as in the command A := hilbert(n);. Before
executing this command (and most other linear algebra commands), you must load the
linear algebra package by the command with(linalg);. A vector of 1â€™s of size
n
can also be constructed by the single command x := vector(n,1);. To multi-
ply this matrix and vector together use the command evalm(A &* x); . There is a
feature that all computer algebra systems have: they do exact arithmetic whenever pos-
sible. Since we are trying to gauge the effects of ï¬nite precision calculations, we donâ€™t
want exact answers (such as 425688/532110), but rather, ï¬nite precision ï¬‚oating point
answers (such as 0.8). Therefore, it would be a good idea at some point to force the
quantities in question to be ï¬nite precision numbers by encapsulating their deï¬nitions

REVIEW
47
in an evaluate as ï¬‚oating point command, e.g., evalf(evalm(A &* x));. This
will force the CAS to do ï¬nite precision arithmetic.
1.5 Exercises
1. Carry out the calculation
((
2
3
+
100)
 100)
 2
3 on a scientiï¬c calculator. Do you
get the correct answer?
2. Enter the matrix
A given below into a computer algebra system and use the available
commands to compute (a) the rank of
A and (b) the reduced row echelon form of A.
(For example, in Maple the relevant commands are rref(A) and rank(A).) Now
convert
A into its ï¬‚oating point form and execute the same commands. Do you get the
same answers? If not, which is correct?
A
=
2
6
6
4
1
3
 2
0
2
0
0
6
18
 15
 6
12
 9
 3
0
0
5
10
0
15
5
2
6
0
8
4
18
6
3
7
7
5
3. Show that the ï¬‚op count for back solving an
n

n system is quadratic in
n: Hint:
At the
jth stage the total work is
j
+
2
[
(n
 1)
+
(n
 2)
+
:
:
:
+
(n
 j
)]
:
Review
Chapter 1 Exercises
1. Calculate the following:
(a)
j2
+
4ij
(b)
 7i
2
+
6i
3
(c)
(3
+
4i)(7
 6i)
(d)
7
 6i
2. Solve the following systems for the (complex) variable
z
: Express your answers in
standard form where possible.
(a)
(2
+
i)z
=
4
 2i (b)
z
4
=
 16
(c)
z
+
1=z
=
1 (d)
(z
+
1)(z
2
+
1)
=
0
3. Find the polar and standard form of the complex numbers
(a)
1=(1
 i) (b)
 2e
i
=3 (c)
i(i
+
p
3
) (d)
 1
+
i (e)
ie

=4
4. The following are augmented matrices of linear systems. In each case, reduce the
matrix to reduced row echelon form and exhibit the solution(s) to the system.
(a)
2
4
1
1
 1
 1
2
2
2
0
3
1
1
1
0
 1
 1
3
5
(b)
2
4
0
0
1
1
0
1
0
2
1
0
0
1
3
5
(c)

0
1
3
1
1
0
0
1


48
1. LINEAR SYSTEMS OF EQUATIONS
5. Consider the linear system:
x
1
+
2x
2
=
2
x
1
+
x
2
+
x
3
 x
4
=
3
2x
3
+
2x
4
=
2
(a) Solve this system by reducing the augmented matrix to reduced row echelon form.
Show all row operations.
(b) Find the rank of the augmented matrix and express its reduced row echelon form as
a product of elementary matrices and the augmented matrix.
(c) This system will have solutions for any right hand side. Justify this fact in terms of
rank.
6. Fill in the blanks or answer True/False with justiï¬cation for your answer:
(a) If
A is a
3

7 matrix then the rank of
A is at most
(b) If
A is a
4

8 matrix, then the nullity of
A could be larger than
4 (T/F):
(c) Any homogeneous (right hand side vector 0) linear system is consistent (T/F):
(d) The rank of a nonzero
3

3 matrix with all entries equal is
.
(e) Some polynomial equations
p(z
)
=
0 have no solutions
z (T/F):
7. What is the locus in the plane of complex numbers
z such that
jz
+
3j
=
jz
 1j?
8. For what values of
c are the following systems inconsistent, with unique solution or
with inï¬nitely many solutions?
(a)
cx
1
+
x
2
+
x
3
=
2
(b)
x
1
+
2x
2
 x
1
=
c
(c)
x
2
+
cx
3
=
0
x
1
+
cx
2
+
x
3
=
2
x
1
+
3x
2
+
x
3
=
1
x
1
 cx
2
=
1
x
1
+
x
2
+
cx
3
=
2
3x
1
+
7x
2
 x
3
=
4
9. Show that a system of linear equations has a unique solution if and only if every
column, except the last one, of the reduced row echelon form of the augmented matrix
has a pivot entry in it.

CHAPTER 2
MATRIX ALGEBRA
In Chapter 1 we used matrices and vectors as simple storage devices. In this chapter
matrices and vectors take on a life of their own. We develop the arithmetic of matrices
and vectors. Much of what we do is motivated by a desire to extend the ideas of ordinary
arithmetic to matrices. Our notational style of writing a matrix in the form
A
=
[a
ij
]
hints that a matrix could be treated like a single number. What if we could manipulate
equations with matrix and vector quantities in the same way that we do scalar equa-
tions? We shall see that this is a useful idea. Matrix arithmetic gives us new powers for
formulating and solving practical problems. In this chapter we will use it to ï¬nd effec-
tive methods for solving linear and nonlinear systems, solve problems of graph theory
and analyze an important modeling tool of applied mathematics called a Markov chain.
2.1. Matrix Addition and Scalar Multiplication
To begin our discussion of arithmetic we consider the matter of equality of matrices.
Suppose that
A and
B represent two matrices. When do we declare them to be equal?
The answer is, of course, if they represent the same matrix! Thus we expect that all the
usual laws of equalities will hold (e.g., equals may be substituted for equals) and in fact,
they do. There are times, however, when we need to prove that two symbolic matrices
must be equal. For this purpose, we need something a little more precise. So we have
the following deï¬nition, which includes vectors as a special case of matrices.
DEFINITION 2.1.1. Two matrices
A
=
[a
ij
] and
B
=
[b
ij
] are said to be equal if
these matrices have the same size, and for each index pair
(i;
j
),
a
ij
=
b
ij, that is,
corresponding entries of
A and
B are equal.
EXAMPLE 2.1.2. Which of the following matrices are equal, if any?
(a)

0
0

(b)

0
0

(c)

0
1
0
2

(d)

0
1
1
 1
1
+
1

SOLUTION. The answer is that only (c) and (d) have any chance of being equal, since
they are the only matrices in the list with the same size (2

2). As a matter of fact, an
entry by entry check veriï¬es that they really are equal.
49

50
2. MATRIX ALGEBRA
Matrix Addition and Subtraction
How should we deï¬ne addition or subtraction of matrices? We take a clue from ele-
mentary two and three dimensional vectors, such as the type we would encounter in
geometry or calculus. There, in order to add two vectors, one condition had to hold: the
vectors had to be the same size. If they were the same size, we simply added the vectors
coordinate by coordinate to obtain a new vector of the same size. That is precisely what
the following deï¬nition says.
DEFINITION 2.1.3. Let
A
=
[a
ij
] and
B
=
[b
ij
] be
m

n matrices. Then the sum of
the matrices, denoted as
A
+
B, is the
m

n matrix deï¬ned by the formula
A
+
B
=
[a
ij
+
b
ij
]
The negative of the matrix
A, denoted as
 A, is deï¬ned by the formula
 A
=
[ a
ij
]
Finally, the difference of
A and
B, denoted as
A
 B, is deï¬ned by the formula
A
 B
=
[a
ij
 b
ij
]
Notice that matrices must be the same size before we attempt to add them. We say that
two such matrices or vectors are conformable for addition.
EXAMPLE 2.1.4. Let
A
=

3
1
0
 2
0
1

and
B
=

 3
2
1
1
4
0

Find
A
+
B,
A
 B, and
 A:
SOLUTION. Here we see that
A
+
B
=

3
1
0
 2
0
1

+

 3
2
1
1
4
0

=

3
 3
1
+
2
0
+
1
 2
+
1
0
+
4
1
+
0

=

0
3
1
 1
4
1

Likewise,
A
 B
=

3
1
0
 2
0
1

 
 3
2
1
1
4
0

=

3
  3
1
 2
0
 1
 2
 1
0
 4
1
 0

=

6
 1
 1
 3
 4
1

:
The negative of
A is even simpler:
 A
=

 3
 1
 0
  2
 0
 1

=

 3
 1
0
2
0
 1


2.1. MATRIX ADDITION AND SCALAR MULTIPLICATION
51
Scalar Multiplication
The next arithmetic concept we want to explore is that of scalar multiplication. Once
again, we take a clue from the elementary vectors, where the idea behind scalar mul-
tiplication is simply to â€œscaleâ€ a vector a certain amount by multiplying each of its
coordinates by that amount. That is what the following deï¬nition says.
DEFINITION 2.1.5. Let
A
=
[a
ij
] be an
m

n matrix and
c a scalar. Then the scalar
product of the scalar
c with the matrix
A, denoted by
cA, is deï¬ned by the formula
cA
=
[ca
ij
]:
Recall that the default scalars are real numbers, but they could also be complex numbers.
EXAMPLE 2.1.6. Let
A
=

3
1
0
 2
0
1

,
and
c
=
3
Find
cA,
0A, and
 1A:
SOLUTION. Here we see that
cA
=
3

3
1
0
 2
0
1

=

3

3
3

1
3

0
3

 2
3

0
3

1

=

9
3
0
 6
0
3

while
0A
=
0

3
1
0
 2
0
1

=

0
0
0
0
0
0

and
( 1)A
=
( 1)

3
1
0
 2
0
1

=

 3
 1
0
2
0
 1

=
 A
Linear Combinations
Now that we have a notion of scalar multiplication and addition, we can blend these two
ideas to yield a very fundamental notion in linear algebra, that of a linear combination.
DEFINITION 2.1.7. A linear combinationof the matrices
A
1
;
A
2
;
:
:
:
;
A
n is an expres-
Linear
Combinations
sion of the form
c
1
A
1
+
c
2
A
2
+
:
:
:
+
c
n
A
n
where
c
1
;
c
2
;
:
:
:
;
c
n are scalars and
A
1
;
A
2
;
:
:
:
;
A
n are matrices all of the same size.
EXAMPLE 2.1.8. Given that
A
1
=
2
4
2
6
4
3
5 ,
A
2
=
2
4
2
4
2
3
5 ,
and
A
3
=
2
4
1
0
 1
3
5 ,
compute the linear combination
 2A
1
+
3A
2
 2A
3
:

52
2. MATRIX ALGEBRA
SOLUTION. The solution is that
 2A
1
+
3A
2
 2A
3
=
 2
2
4
2
6
4
3
5
+
3
2
4
2
4
2
3
5
 2
2
4
1
0
 1
3
5
=
2
4
 2

2
+
3

2
 2

1
 2

6
+
3

4
 2

0
 2

4
+
3

2
 2

( 1)
3
5
=
2
4
0
0
0
3
5
It seems like too much work to write out objects such as the vector
2
4
0
0
0
3
5 that occurred
in the last equation; after all, we know that all the entries are all
0: So we make the
following convention for convenience.
NOTATION 2.1.9. A zero matrix is a matrix whose every entry is
0: We shall denote
such matrices by the symbol
0:
We have to be a bit careful, since this convention makes the symbol
0 ambiguous, but
the meaning of the symbol will be clear from context, and the convenience gained is
worth the potential ambiguity. For example, the equation of the preceding example is
stated very simply as
 2A
1
+
3A
2
 2A
3
=
0, where we understand from context that
0 has to mean the
3

1 column vector of zeros. If we use boldface for vectors, we will
also then use boldface for the vector zero, so some distinction is regained.
EXAMPLE 2.1.10. Suppose that a linear combination of matrices satisï¬es the identity
 2A
1
+
3A
2
 2A
3
=
0, as in the preceding example. Use this fact to express
A
1 in
terms of
A
2 and
A
3
:
SOLUTION. To solve this example, just forget that the quantities
A
1
;
A
2
;
A
3 are any-
thing special and use ordinary algebra. First, add
 3A
2
+
2A
3 to both sides to obtain
 2A
1
+
3A
2
 2A
3
 3A
2
+
2A
3
=
 3A
2
+
2A
3
so that
 2A
1
=
 3A
2
+
2A
3
and then multiplying both sides by the scalar
 1=2 yields the identity
A
1
=
 1
2
( 2A
1
)
=
 3
2
A
2
+
A
3
The linear combination idea has a really interesting application to linear systems, namely,
it gives us another way to express the solution set of a linear system that clearly identi-
ï¬es the role of free variables. The following example illustrates this point.
EXAMPLE 2.1.11. Suppose that a linear system in the unknowns
x
1
;
x
2
;
x
3
;
x
4 has gen-
eral solution
(x
2
+
3x
4
;
x
2
;
2x
1
 x
4
;
x
4
), where the variables
x
2
;
x
4 are free. Describe
the solution set of this linear system in terms of linear combinations with free variables
as coefï¬cients.

2.1. MATRIX ADDITION AND SCALAR MULTIPLICATION
53
SOLUTION.The trick here is to use only the parts of the general solution involving
x
2
for one vector and the parts involving
x
4 as the other vectors in such a way that these
vectors add up to the general solution. In our case we have
2
6
6
4
x
2
+
3x
4
x
2
2x
1
 x
4
x
4
3
7
7
5
=
2
6
6
4
x
2
x
2
2x
1
0
3
7
7
5
+
2
6
6
4
3x
4
0
 x
4
x
4
[r
]
3
7
7
5
=
x
2
2
6
6
4
1
1
2
0
3
7
7
5
+
x
4
2
6
6
4
3
0
 1
1
3
7
7
5
Now simply deï¬ne vectors
A
1
=
(1;
1;
2;
0),
A
2
=
(3;
0;
 1;
1) and we see that, since
x
2 and
x
4 are arbitrary, the solution set is
S
=
fx
2
A
1
+
x
4
A
2
j
x
2
;
x
4
2
Rg
In other words, the solution set to the system is the set of all possible linear combinations
of the vectors
A
1 and
A
2
:
The idea of solution sets as linear combinations is an important one that we will return
to in later chapters. You might notice that once we have the general form of a solution
vector itâ€™s easy to determine the constant vectors
A
1 and
A
2
: Simply set
x
2
=
1 and
the other free variable(s) â€“ in this case just
x
4 â€“ to get the solution vector
A
1, and set
x
4
=
1 and
x
2
=
0 to get the solution vector
A
2
:
Laws of Arithmetic
The last example brings up an important point: to what extent can we rely on the ordi-
nary laws of arithmetic and algebra in our calculations with matrices and vectors? We
shall see later in this section that for matrix multiplication there are some surprises. On
the other hand, the laws for addition and scalar multiplication are pretty much what we
would expect them to be. Taken as a whole, these laws are very useful; so much so that
later in this text they will be elevated to the rank of an axiom system for what are termed
â€œvector spaces.â€ Here are the laws with their customary names. These same names can
apply to more than one operation. For instance, there is a closure law for addition and
one for scalar multiplication as well.
Laws of Matrix Addition and Scalar Multiplication. Let
A;
B
;
C be matri-
ces of the same size
m

n,
0 the
m

n zero matrix, and
c and
d scalars.
Then
1. (Closure Law)
A
+
B is an
m

n matrix.
2. (Associative Law)
(A
+
B
)
+
C
=
A
+
(B
+
C
)
3. (Commutative Law)
A
+
B
=
B
+
A
4. (Identity Law)
A
+
0
=
A
5. (Inverse Law)
A
+
( A)
=
0
6. (Closure Law)
cA is an
m

n matrix.
7. (Associative Law)
c(dA)
=
(cd)A
8. (Distributive Law)
(c
+
d)A
=
cA
+
dA
9. (Distributive Law)
c(A
+
B
)
=
cA
+
cB
10. (Monoidal Law)
1A
=
A

54
2. MATRIX ALGEBRA
It is fairly straightforward to prove from deï¬nitions that these laws are valid. The veri-
ï¬cations all follow a similar pattern, which we illustrate by verifying the commutative
law for addition: let
A
=
[a
ij
] and
B
=
[b
ij
] be given
m

n matrices. Then we have
that
A
+
B
=
[a
ij
+
b
ij
]
=
[b
ij
+
a
ij
]
=
B
+
A
where the ï¬rst and third equalities come from the deï¬nition of matrix addition, and the
second equality follows from the fact that for all indices
i and
j,
a
ij
+
b
ij
=
b
ij
+
a
ij
by the commutative law for addition of scalars.
2.1 Exercises
1. Calculate
(a)

1
2
 1
0
2
2

 
3
1
0
1
1
1

(b)
2

1
3

 5

2
2

+
3

4
1

(c)
2

1
4
0
0

+
3

0
0
2
1

(d)
a

1
1
0
1

+
b

 1
0
 1
1

(e)
2
2
4
1
2
 1
0
0
2
0
2
2
3
5
 3
2
4
3
1
0
5
 2
1
1
1
1
3
5 (f)
x
2
4
1
3
0
3
5
 5
2
4
2
2
1
3
5
+
y
2
4
4
1
0
3
5
2. Let
A
=

1
0
 1
1
1
2

,
B
=

2
2
1
 2

,
C
=

1
0
 1
1
1
2

, and compute the
following, where possible.
(a)
A
+
3B (b)
2A
 3C (c)
A
 C (d)
6B
+
C (e)
2C
 3(A
 2C
)
3. With
A;
C as in Exercise 2, solve the equations (a)
2X
+
3A
=
C and (b)
A
 3X
=
3C for the unknown
X
:
4. Show how to write each of the following vectors as a linear combination of constant
vectors with scalar coefï¬cients
x;
y or
z
:
(a)

x
+
2y
2x
 z

(b)

x
 y
2x
+
3y

(c)
2
4
3x
+
2y
 z
x
+
y
+
5z
3
5 (d)
2
4
x
 3y
4x
+
z
2y
 z
3
5
5. Find scalars
a;
b;
c such that

c
b
0
c

=

a
 b
c
+
2
a
+
b
a
 b

6. Show that any matrix of the form

a
b
c
d

can be expressed as a linear combination
of the four matrices

1
0
0
0

;

0
1
0
0

;

0
0
1
0

; and

0
0
0
1

:

2.2. MATRIX MULTIPLICATION
55
7. Verify that the associative law and both distributive laws for addition hold for
c
=
2;
d
=
 3 and
A
=

 1
0
 1
0
1
2

B
=

1
2
 1
4
1
3

C
=

 1
0
 1
1
 1
0

8. Prove that the associative law for addition of matrices (page 53) holds.
9. Prove that both distributive laws (page 53) hold.
10. Prove the following assertions for
m

n matrices
A and
B by using the laws of
matrix addition and scalar multiplication. Clearly specify each law that you use.
(a) If
cA
=
0 for some scalar
c; then either
c
=
0 or
A
=
0:
(b) If
B
=
cB for some scalar
c
6=
 1; then
B
=
0:
2.2. Matrix Multiplication
Matrix multiplication is somewhat more subtle than matrix addition and scalar multi-
plication. Of course, we could deï¬ne matrix multiplication to be coordinate-wise, just
as addition is. But our motivation is not merely to make deï¬nitions, but rather to make
useful deï¬nitions.
Deï¬nition of Multiplication
To motivate the deï¬nition, let us consider a single linear equation
2x
 3y
+
4z
=
5:
We will ï¬nd it handy to think of the left hand side of the equation as a â€œproductâ€ of
the coefï¬cient matrix
[2;
 3;
4] and the column matrix of unknowns
2
4
x
y
z
3
5
: Thus, we
have that the product of this row and column is
[2;
 3;
4]
2
4
x
y
z
3
5
=
[2x
 3y
+
4z
]:
Notice that we have made the result of the product into a matrix (in this case
1

1).
This introduces us to a permanent abuse of notation that is almost always used in linear
algebra: we donâ€™t distinguish between the scalar
a and the
1

1 matrix
[a], though
technically perhaps we should. In the same spirit, we make the following deï¬nition.

56
2. MATRIX ALGEBRA
DEFINITION 2.2.1. The product of the
1

n row
[a
1
;
a
2;
:::;
a
n
] with the
n

1 column
2
6
6
6
4
b
1
b
2
...
b
n
3
7
7
7
5 is deï¬ned to be the
1

1 matrix
[a
1
b
1
+
a
2
b
2
+
:::
+
a
n
b
n
]:
It is this row-column product strategy that guides us to the general deï¬nition. Notice
how the column number of the ï¬rst matrix had to match the row number of the second,
and that this number disappears in the size of the resulting product. This is exactly what
happens in general.
DEFINITION 2.2.2. Let
A
=
[a
ij
] be an
m

p matrix and
B
=
[b
ij
] be a
p

n matrix.
Then the product of the matrices
A and
B, denoted by
A

B (or simply
AB), is the
m

n matrix whose
(i;
j
)th entry, for
1

i

m and
1

j

n, is the entry of the
product of the
ith row of
A and the
jth column of
B; more speciï¬cally, the
(i;
j
)th entry
of
AB is
a
i1
b
1j
+
a
i2
b
2j
+
:::
+
a
ip
b
pj
Notice that, unlike addition, two matrices may be of different sizes when we can multi-
ply them together. If
A is
m

p and
B is
p

n, we say that
A and
B are conformable
for multiplication. It is also worth noticing that if
A and
B are square and of the same
size, then the products
AB and
B
A are always deï¬ned.
Some Illustrative Examples
Letâ€™s check our understanding with a few examples.
EXAMPLE 2.2.3. Compute, if possible, the products of the following pairs of matrices
A;
B
:
(a)

1
2
1
2
3
 1

;
2
4
4
 2
0
1
2
1
3
5
(b)

1
2
3
2
3
 1

;

2
3

(c)

1
0
0
1

;

1
2
1
2
3
 1

(d)

0
0

;

1
2

(e)

1
2

;

0
0

;
(f
)

1
1
1
1

;

1
1
 1
 1

SOLUTION. In problem (a)
A is
2

3 and
B is
3

2: First check conformability for
multiplication. Stack these dimensions along side each other and see that the
3â€™s match;
now â€œcancelâ€ the matching middle
3â€™s to obtain that the dimension of the product is
2
6
3
6
3

2
=
2

2: To obtain, for example, the
(1;
2)th entry of the product matrix
multiply the ï¬rst row of
A and second column of
B to obtain
[1;
2;
1]
2
4
 2
1
1
3
5
=
[1

( 2)
+
2

1
+
1

1]
=
[1]

2.2. MATRIX MULTIPLICATION
57
The full product calculation looks like this:

1
2
1
2
3
 1

2
4
4
 2
0
1
2
1
3
5
=

1

4
+
2

0
+
1

2
1

( 2)
+
2

1
+
1

1
2

4
+
3

0
+
( 1)

2
2

( 2)
+
3

1
+
( 1)

1

=

6
1
6
 2

A size check of part (b) reveals a mismatch between the column number of the ï¬rst
matrix (3) and the row number (2) of the second matrix. Thus these matrices are not
conformable for multiplication in the speciï¬ed order. Hence

1
2
3
2
3
 1


2
3

is undeï¬ned.
Things work better in (c), where the size check gives
2
6
2
6
2

3
=
2

3 as the size
of the product. As a matter of fact, this is a rather interesting calculation:

1
0
0
1


1
2
1
2
3
 1

=

1

1
+
0

2
1

2
+
0

3
1

1
+
0

( 1)
0

1
+
1

2
0

2
+
1

3
0

1
+
1

( 1)

=

1
2
1
2
3
 1

Notice that we end up with the second matrix in the product. This is similar to the
arithmetic fact that
1

x
=
x for a given real number
x: So the matrix on the left acted
like a multiplicative identity. We will see later that this is no accident.
In problem (d) a size check shows that the product has size
2
6
1
6
1

2
=
2

2: The
calculation gives

0
0


1
2

=

0

1
0

2
0

1
0

2

=

0
0
0
0

For part (e) the size check shows gives
1
6
2
6
2

1
=
1

1: Hence the product exists
and is
1

1: The calculation gives

1
2


0
0

=
[1

0
+
2

0]
=
[0]
Something very interesting comes out of this calculation. Notice that for this choice of
Matrix
Multiplication
Not
Commutative or
Cancellative
A and
B we have that
AB and
B
A are not the same matrices - never mind that their
entries are all
0â€™s - the important point is that these matrices are not even the same size!
Thus a very familiar law of arithmetic, the commutativity of multiplication, has just
fallen by the wayside.
Finally, for the calculation in (f), notice that

1
1
1
1


1
1
 1
 1

=

1

1
+
1

 1
1

1
+
1

 1
1

1
+
1

 1
1

1
+
1

 1

=

0
0
0
0

Thereâ€™s something very curious here, too. Notice that two nonzero matrices of the same
size multiplied together to give a zero matrix. This kind of thing never happens in
ordinary arithmetic, where the cancellation law assures that if
a

b
=
0 then
a
=
0 or
b
=
0:

58
2. MATRIX ALGEBRA
The calculation in (c) inspires some more notation. The left-hand matrix of this product
has a very important property. It acts like a â€œ1â€ for matrix multiplication. So it deserves
its own name.
NOTATION 2.2.4. A matrix of the form
Identity Matrix
I
n
=
2
6
6
6
6
6
4
1
0
:
:
:
0
0
1
0
...
...
1
0
0
0
1
3
7
7
7
7
7
5
=
[Ã†
ij
]
is called an
n

n identity matrix. The
(i;
j
)th entry of
I
n is designated by the Kronecker
symbol
Ã†
ij which is
1 if
i
=
j and
0 otherwise. If
n is clear from context, we simply
write
I in place of
I
n
:
So we see in the previous example that the left hand matrix of part (c) is

1
0
0
1

=
I
2
Linear Systems as a Matrix Product
Letâ€™s have another look at a system we examined in Chapter 1. Weâ€™ll change the names
of the variables from
x;
y
;
z to
x
1
;
x
2
;
x
3 in anticipation of a notation that will work
with any number of variables.
EXAMPLE 2.2.5. Express the following linear system as a matrix product.
x
1
+
x
2
+
x
3
=
4
2x
1
+
2x
2
+
5x
3
=
11
4x
1
+
6x
2
+
8x
3
=
24
SOLUTION. Recall how we deï¬ned multiplication of a row vector and column vector at
the beginning of this section. We use that as our inspiration. Deï¬ne
x
=
2
4
x
1
x
2
x
3
3
5
;
b
=
2
4
4
11
24
3
5
and
A
=
2
4
1
1
1
2
2
5
4
6
8
3
5
Of course,
A is just the coefï¬cient matrix of the system and
b is the right hand side vec-
tor, which we have seen several times before. But now these take on a new signiï¬cance.
Notice that if we take the ï¬rst row of
A and multiply it by
x we get the left hand side of
the ï¬rst equation of our system. Likewise for the second and third rows. Therefore, we
may write in the language of matrices that
Ax
=
2
4
1
1
1
2
2
5
4
6
8
3
5
2
4
x
1
x
2
x
3
3
5
=
2
4
4
11
24
3
5
=
b
Thus the system is represented very succinctly as
Ax
=
b:

2.2. MATRIX MULTIPLICATION
59
Once we understand this example, it is easy to see that the general abstract system that
we examined in the ï¬rst section of Chapter 1 can just as easily be abbreviated. Now we
have a new way of looking at a system of equations: it is just like a simple ï¬rst degree
equation in one variable. Of course, the catch is that the symbols
A;
x;
b now represent
an
m

n matrix,
n

1 and
m

1 vectors, respectively. In spite of this the matrix
multiplication idea is very appealing. For instance, it might inspire us to ask if we could
somehow solve the system
Ax
=
b by multiplying both sides of the equation by some
kind of matrix â€œ1=
Aâ€ so as to cancel the
A and get
(1=
A)Ax
=
I
x
=
x
=
(1=
A)b
Weâ€™ll follow up on this idea in Section 2.5.
Here is another perspective on matrix-vector multiplication that gives a very powerful
way of thinking about such multiplications. We will use this idea frequently in Chap-
ter 3.
EXAMPLE 2.2.6. Interpret the matrix product of Example 2.2.5 as a linear combination
of column vectors.
SOLUTION. Examine the system of this example and we see that the column
(1;
2;
4)
appears to be multiplied by
x
1
: Similarly, the column
(1;
2;
6) is multiplied by
x
2 and
the column
(1;
5;
8) by
x
3
: Hence, if we use the same right hand side column
(4;
11;
24)
as before, we obtain that this column can be expressed as a linear combination of column
vectors, namely
x
1
2
4
1
2
4
3
5
+
x
2
2
4
1
2
6
3
5
+
x
3
2
4
1
5
8
3
5
=
2
4
4
11
24
3
5
We could write the equation of the previous example very succinctly as follows: let
A
have columns
a
1
;
a
2
;
a
3
; so that
A
=
[a
1
;
a
2
;
a
3
] and let
x
=
2
4
x
1
x
2
x
3
3
5
: Then
Ax
=
x
1
a
1
+
x
2
a
2
+
x
3
a
3
Laws of Arithmetic
We have already seen that the laws of matrix arithmetic may not be quite the same as the
ordinary arithmetic laws that we are use to. Nonetheless, as long as we donâ€™t assume a
cancellation law or a commutative law for multiplication, things are pretty much what
one might expect.

60
2. MATRIX ALGEBRA
Laws of Matrix Multiplication. Let
A;
B
;
C be matrices of the appropriate
sizes so that the following multiplications make sense,
I a suitably sized iden-
tity matrix, and
c and
d scalars. Then
1. (Closure Law)
AB is an
m

n matrix.
2. (Associative Law)
(AB
)C
=
A(B
C
)
3. (Identity Law)
AI
=
A and
I
B
=
B
4. (Associative Law for Scalars)
c(AB
)
=
(cA)B
=
A(cB
)
5. (Distributive Law)
(A
+
B
)C
=
AC
+
B
C
6. (Distributive Law)
A(B
+
C
)
=
AB
+
AC
One can formally verify these laws by working through the deï¬nitions. For example,
to verify the ï¬rst half of the identity law, let
A
=
[a
ij
] be an
m

n matrix, so that
I
=
[Ã†
ij
] has to be
I
n in order for the product
AI to make sense. Now we see from the
formal deï¬nition of matrix multiplication that
AI
=
[
n
X
k
=1
a
ik
Ã†
k
j
]
=
[a
ij

1]
=
A
The middle equality follows from the fact that
Ã†
k
j is
0 unless
k
=
j: Thus the sum
collapses to a single term. A similar calculation veriï¬es the other laws.
We end our discussion of matrix multiplication with a familiar looking notation that will
prove to be extremely handy in the sequel.
NOTATION 2.2.7. Let
A be a square
n

n matrix and
k a nonnegative integer. Then
we deï¬ne the
kth power of
A to be
A
k
=
8
<
:
I
n
if
k
=
0
A

A

:
:
:

A
|
{z
}
if
k
>
0
k
times
As a simple consequence of this deï¬nition we have the standard exponent laws.
Laws of Exponents. For nonnegative integers
i;
j and square matrix
A we
have that

A
i+j
=
A
i

A
j

A
ij
=
(A
i
)
j
Notice that the law
(AB
)
i
=
A
i
B
i is missing. Why do you think it wonâ€™t work with
matrices?
2.2 Exercises
1. Express these systems of equations in the notation of matrix multiplication.
(a)
x
1
 2x
2
+
4x
3
=
3
(b)
x
 y
 3z
=
3
(c)
x
 3y
+
1
=
0
x
2
 x
3
=
2
2x
+
2y
+
4z
=
10
2y
=
2
 x
1
+
4x
3
=
1
 x
+
z
=
3
 x
+
3y
=
0

2.2. MATRIX MULTIPLICATION
61
2. Let
A
=

0
2
1
1

and verify the identity
(I
+
A
+
A
2
)(I
 A)
=
I
 A
3
:
3. Express each system of Exercise 1 as an equation whose left hand side is a linear
combination of the columns of the coefï¬cient matrix (see Example 2.2.6).
4. Carry out these calculations or indicate they are impossible, given that
a
=

2
1

,
b
=

3
4

, and
C
=

2
1
+
i
0
1

(a)
bC
a (b)
ab (c)
C
b (d)
(C
b)
a (e)
C
a (f)
C
(ab
) (g)
ba (h)
C
(a
+
b)
5. Compute the product
AB of the following pairs
A;
B, if possible.
(a)

1
0
2
0

;

3
 2
0
 2
5
8

(b)

2
1
0
0
8
2

;

1
1
2
2

(c)
2
4
3
1
2
1
0
0
4
3
2
3
5
;
2
4
 5
4
 2
 2
3
1
1
0
4
3
5
(d)
2
4
3
1
1
0
4
3
3
5
;

 5
4
 2
 2
3
1

(e)

 2
1
 3

;
2
4
 5
4
 2
 2
3
1
1
0
4
3
5
(f)
2
4
1
0
0
0
1
0
0
0
1
3
5
;

 5
4
 2

6. Find examples of
2

2 matrices
A and
B that fulï¬ll each of the following conditions.
(a)
(AB
)
2
6=
A
2
B
2
(b)
AB
6=
B
A
(c)
(A
 B
)
2
=
A
2
 2AB
+
B
2
7. Prove that if two matrices
A and
B of the same size have the property that
Ab
=
B
b
for every column vector
b of the correct size for multiplication, then
A
=
B
: (Try using
vectors with a single nonzero entry of
1: )
8. A square matrix
A is said to be nilpotent if there is a positive integer
m such that
A
m
=
0: Determine which of the following matrices are nilpotent; justify your answer.
(a)
2
4
0
2
0
0
0
2
0
0
0
3
5
(b)

1
1
1
1

(c)

0
0
1
0

(d)
2
4
2
2
 4
 1
0
2
1
1
 2
3
5
9. A square matrix
A is idempotent if
A
2
=
A: Determine which of the following
matrices are idempotent.
(a)

1
2
0
1

(b)

1
0
0
1

(c)

0
0
 1
1

(d)
2
4
0
0
2
1
1
 2
0
0
1
3
5
10. Verify that the product
uv
T
; where
u
=
(1;
0;
2) and
v
=
( 1;
1;
1); is a rank one
matrix.
11. Verify that the associative law for scalars and both distributive laws hold for
c
=
4
and these matrices
A
=

2
0
 1
1

B
=

0
2
0
3

C
=

1
+
i
1
1
2

:

62
2. MATRIX ALGEBRA
12. Prove that the associative law for scalars (page 60) is valid.
13. Prove that both distributive laws for matrices (page 60) are valid.
14. Let
A
=

1
2
 1
1

and
B
=
2
4
0
1
0
0
1
5
2
 3
2
0
3
5 Compute
f
(A) and
f
(B
) where
f
(x)
=
2x
3
+
3x
 5: Here it is understood that when a square matrix is substituted for
the variable
x in a polynomial expression, the constant term has an implied coefï¬cient
of
x
0 in it which becomes the identity matrix of appropriate size.
2.3. Applications of Matrix Arithmetic
We have already seen an important application of matrix multiplication to linear sys-
tems. We next examine a few more applications of the matrix multiplication idea which
should reinforce the importance of this idea, as well as provide us with some interpre-
tations of matrix multiplication.
Matrix Multiplication as Function Composition
The function idea is basic to mathematics. Recall that a function
f is a rule of corre-
spondence that assigns to each argument
x in a set called its domain, a unique value
y
=
f
(x) from a set called its range. Each branch of mathematics has its own spe-
cial functions; for example, in calculus differentiable functions
f
(x) are fundamental.
Linear algebra also has its special functions. Suppose that
T
(u) represents a function
whose arguments
u and values
v
=
T
(u) are vectors. We say the function
T is linear
if, for all vectors
u;
v in the domain of
T
; and scalars
c;
d; we have
Linear
Functions
T
(cu
+
dv
)
=
cT
(u)
+
dT
(v
)
EXAMPLE 2.3.1. Show that the function
T with domain the set of
2

1 vectors and
deï¬nition by the formula
T

x
y

=
x
is a linear function.

2.3. APPLICATIONS OF MATRIX ARITHMETIC
63
SOLUTION. Let

x
y

and

z
w

be two elements in the domain of
T and
c;
d any
two scalars. Now compute
T

c

x
y

+
d

z
w

=
T


cx
cy

+

dz
dw

=
T

cx
+
dz
cy
+
dw

=
cx
+
dz
=
cT

x
y

+
dT

z
w

Thus,
T satisï¬es the deï¬nition of linear function.
One can check that the function
T just deï¬ned can be expressed as a matrix multiplica-
tion, namely,
T

x
y

=
[1;
0]

x
y

: This kind of linear function gives yet another
reason for deï¬ning matrix multiplication in the way that we do. More generally, let
A
be an
m

n matrix and deï¬ne a function
T
A that maps
n

1 vectors to
m

1 vectors
according to the formula
T
A
(u)
=
Au:
First we verify that
T is linear. Use the deï¬nition of
T
A along with the distributive law
of multiplication and associative law for scalars to obtain that
T
A
(cu
+
dv
)
=
A(cu
+
dv
)
=
A(cu)
+
dA(v
)
=
c(Au)
+
d(Av
)
=
cT
A
(u)
+
dT
A
(v
)
This proves that multiplication of vectors by a ï¬xed matrix
A is a linear function.
EXAMPLE 2.3.2. Use the associative law of matrix multiplication to show that the com-
position of matrix multiplication functions corresponds to the matrix product.
SOLUTION. For all vectors
u and for suitably sized matrices
A;
B
; we have by the
associative law that
A(B
u)
=
(AB
)u: In function terms, this means that
T
A
(T
B
(u))
=
T
AB
(u): Since this is true for all arguments
u; it follows that
T
A
Ã†
T
B
=
T
AB
; which
is what we were to show.
We will have more to say about linear functions in Chapters 3 and 6, where they will
go by the name of linear operators. For now weâ€™ll conclude our discussion of linear
functions with an example that gives a hint of why the â€œlinearâ€ in â€œlinear function.â€
EXAMPLE 2.3.3. Let
L be the set of points
(x;
y
) in the plane that satisfy the linear
equation
y
=
x
+
1,
A
=

2
1
4
2

and let
T
A
(L)
=
fT
((x;
y
))
j
(x;
y
)
2
Lg:
Describe and sketch these sets in the plane.
SOLUTION. Of course the set L is just the straight line deï¬ned by the linear equation
y
=
x
+
1: To see what elements of
T
A
(L) look like, we write a typical element of
L in
the form
(x;
x
+
1): Now calculate
T
A
((x;
x
+
1))
=

2
1
4
2


x
x
+
1

=

3x
+
1
6x
+
2

:

64
2. MATRIX ALGEBRA
    

  
  

  
                                             








    



2
3
2
1
3
1
(1/6,7/6)
(0,0)
(-1/3,2/3)
(3/2,3)
y
x
T (L)
A
L
FIGURE 2.3.1. Action of
T
A on line
L given by
y
=
x
+
1 and points
on
L:
Now make the substitution
t
=
3x
+
1 and we see that a typical element of
T
A
(L) has
the form
(t;
2t); where
t is any real number, since
x is arbitrary. We recognize these
points as exactly the points on the line
y
=
2x: Thus, the function
T
A maps the line
y
=
x
+
1 to the line
y
=
2x: Figure 2.3.1 illustrates this mapping as well as the fact
that
T
A
(( 1=3;
2=3))
=
(0;
0) and
T
A
((1=6;
7=6))
=
(3=2;
3):
Markov Chains as Matrix Products
A Markov chain is a certain type of matrix model which we will illustrate with an
example.
EXAMPLE 2.3.4. Suppose two toothpaste companies compete for customers in a ï¬xed
market in which each customer uses either Brand A or Brand B. Suppose also that a
market analysis shows that the buying habits of the customers ï¬t the following pattern
in the quarters that were analyzed: each quarter (three month period) 30% of A users
will switch to B while the rest stay with A. Moreover, 40% of B users will switch to A
in a given quarter, while the remaining B users will stay with B. If we assume that this
pattern does not vary from quarter to quarter, we have an example of what is called a
Markov chain model. Express the data of this model in matrix-vector language.
SOLUTION. Notice that if
a
0 and
b
0 are the fractions of the customers using A and B,
respectively, in a given quarter,
a
1 and
b
1 the fractions of customers using A and B in
the next quarter, then our hypotheses say that
a
1
=
0:7a
0
+
0:4b
0
b
1
=
0:3a
0
+
0:6b
0
We could ï¬gure out what happens in the quarter after this by replacing the indices
1 and
0 by
2 and
1, respectively, in the preceding formula. In general, we replace the indices

2.3. APPLICATIONS OF MATRIX ARITHMETIC
65
1;
0 by
k
;
k
+
1, to obtain
a
k
+1
=
0:7a
k
+
0:4b
k
b
k
+1
=
0:3a
k
+
0:6b
k
We express this system in matrix form as follows: let
x
(k
)
=

a
k
b
k

and
A
=

0:7
0:4
0:3
0:6

: Then the system may be expressed in the matrix form
x
(k
+1)
=
Ax
(k
)
The state vectors
x
(k
) of the preceding example have the following property: each
coordinate is non-negative and all the coordinates sum to
1: Such a vector is called a
probability distribution vector. Also, the matrix
A has the property that each of its
columns is a probability distribution vector. Such a square matrix is called a transition
matrix. In these terms we now give a precise deï¬nition of a Markov chain.
DEFINITION 2.3.5. A Markov chain is a transition matrix
A together with a probability
Markov Chain
distribution vector
x
(0)
: The state vectors of this Markov chain are the vectors given by
x
(k
+1)
=
Ax
(k
)
;
k
=
0;
1;
:
:
:
:
Let us return to Example 2.3.4. The state vectors and transition matrices
x
(k
)
=

a
k
b
k

and
A
=

0:7
0:4
0:3
0:6

should play an important role. And indeed they do, for in light of our interpretation of
a linear system as a matrix product, we see that the two equations of Example 2.3.4 can
be written simply as
x
(1)
=
Ax
(0)
: A little more calculation shows that
x
(2)
=
Ax
(1)
=
A

(Ax
(0)
)
=
A
2
x
(0)
and in general,
x
(k
)
=
Ax
(k
 1)
=
A
2
x
(k
 2)
=
:
:
:
=
A
k
x
(0)
Now we really have a very good handle on the Markov chain problem. Consider the
following instance of our example.
EXAMPLE 2.3.6. In the notation of Example 2.3.4 suppose that initially Brand A has
all the customers (i.e., Brand B is just entering the market). What are the market shares
2 quarters later? 20 quarters? Answer the same questions if initially Brand B has all the
customers.
SOLUTION. To say that initially Brand A has all the customers is to say that the initial
state vector is
x
(0)
=

1
0

: Now do the arithmetic to ï¬nd
x
(2):

a
2
b
2

=
x
(2)
=
A
2
x
(0)
=

0:7
0:4
0:3
0:6


0:7
0:4
0:3
0:6


1
0

=

0:7
0:4
0:3
0:6


0:7
0:3

=

:61
:39


66
2. MATRIX ALGEBRA
Thus, Brand A will have
61% of the market and Brand B will have
39% of the market
in the second quarter. We did not try to do the next calculation by hand, but rather used
a computer to get the approximate answer:
x
(20)
=

0:7
0:4
0:3
0:6

20

1
0

=

:57143
:42857

Thus, after
20 quarters, Brand Aâ€™s share will have fallen to about
57% of the market
and Brand Bâ€™s share will have risen to about
43%: Now consider what happens if the
initial scenario is completely different, i.e.,
x
(0)
=

0
1

: We compute by hand to ï¬nd
that
x
(2)
=

0:7
0:4
0:3
0:6


0:7
0:4
0:3
0:6


0
1

=

0:7
0:4
0:3
0:6


0:4
0:6

=

:52
:48

Then we use a computer to ï¬nd:
x
(20)
=

0:7
0:4
0:3
0:6

20

0
1

=

:57143
:42857

Surprise! We get the same answer as we did with a completely different initial condition.
Is this just a coincidence? We will return to this example again in Chapter 3, where
concepts introduced therein will cast new light on this model.
Calculating Power of Graph Vertices
EXAMPLE 2.3.7. (Dominance Directed Graphs) Suppose you have incomplete data
about four teams who have played each other, and further that the matches only have a
winner and a loser, with no score attached. Given that the teams are identiï¬ed by labels
1;
2;
3; and
4 we could describe a match by a pair of numbers
(i;
j
) where we understand
that this means that team
i played and defeated team
j (no ties allowed). Here is the
given data:
f(1;
2);
(1;
4);
(3;
1);
(2;
3);
(4;
2)g
Give a reasonable graphical representation of this data.
SOLUTION. We can draw a picture of all the data that we are given by representing
each team, as a point called a â€œvertexâ€ and each match by connecting two points with
an arrow, called a â€œdirected edgeâ€, which points from the winner towards the loser in
the match. See Figure 2.3.2 for the picture that we obtain.
Consider the following question relating to Example 2.3.7. Given this incomplete data
about the teams, how would we determine the ranking of each team in some reasonable
way? In order to answer this question, we are going to introduce some concepts from
graph theory which are useful modeling tools for many problems.
The data of Figure 2.3.2 is an example of a directed graph, a modeling tool which can
be deï¬ned as follows.
A directed graph (digraph for short) is a set
V , whose elements
Directed Graph
are called vertices, together with a set or list (to allow for repeated edges)
E of ordered

2.3. APPLICATIONS OF MATRIX ARITHMETIC
67
Edge 4
Edge 2
Edge 5
Edge 3
Vertex 4
Vertex 3
Vertex 2
Vertex 1
Edge 1
FIGURE 2.3.2. Data from Example 2.3.7
pairs with coordinates in
V , whose elements are called (directed) edges. Another useful
idea for us is the following: a walk in the digraph
G is a sequence of digraph edges
(v
0
;
v
1
);
(v
1
;
v
2
);
:::;
(v
m 1
;
v
m
) which goes from vertex
v
0 to vertex
v
m
: The length of
the walk is
m:
Here is an interpretation of â€œpowerâ€ that has proved to be useful in many situations,
including our own. The power of a vertex in a digraph is the number of walks of length
1 or
2 originating at the vertex. In our example, for instance, the power of vertex
1
is
4: Why only walks of length
1 or
2? For one thing, walks of length
3 introduce
the possibility of loops, i.e., walks that â€œloop aroundâ€ to the same point. It isnâ€™t very
informative to ï¬nd out that team
1 beat team
2 beat team
3 beat team
1: For another,
information more than two hops away isnâ€™t very deï¬nitive. So we donâ€™t count it in the
deï¬nition of power.
The type of digraph we are considering has no edges from a vertex to itself (so-called
â€œself-loopsâ€) and for a pair of distinct vertices at most one edge connecting the two
vertices. In other words, a team doesnâ€™t play itself and plays another team at most once.
Such a digraph is called a dominance-directed graph. Although the notion of power
of a point is deï¬ned for any digraph, it makes the most sense for dominance-directed
graphs.
EXAMPLE 2.3.8. Find the power of each vertex in the graph of Example 2.3.7 and use
this information to rank the teams.
SOLUTION. In this example we could ï¬nd the power of all points by inspection of
Figure 2.3.2. Letâ€™s do it: simple counting gives that the power of vertex
1 is
4, the
power of vertex
3 is
3, and the power of vertices
2 and
4 is
2: Consequently, teams
2
and
4 are tied for last place, team
3 is in second place and team
1 is ï¬rst.
One can imagine situations (like describing the structure of the communications net-
work pictured in Figure 2.3.3) where the edges shouldnâ€™t really have a direction, since
connections are bidirectional. For such situations a more natural tool is the concept of
a graph, which can be deï¬ned as follows: a graph is a set
V , whose elements are called
vertices, together with a set or list (to allow for repeated edges)
E of unordered pairs
with coordinates in
V , called edges.

68
2. MATRIX ALGEBRA
e3
v
v
v
v
e
e
v
e
e
e
v1
2
3
5
7
8
4
2
1
5
6
e
e4
6
FIGURE 2.3.3. A communication network graph.
Just as with digraphs, we deï¬ne A walk in the graph
G is a sequence of digraph
edges
(v
0
;
v
1
);
(v
1
;
v
2
);
:::;
(v
m 1
;
v
m
) which goes from vertex
v
0 to vertex
v
m
: The
length of the walk is
m: For example, the graph of Figure 2.3.3 has vertex set
V
=
fv
1
;
v
2
;
v
3
;
v
4
;
v
5
;
v
6
g and edge set
E
=
fe
1
;
e
2
;
e
3
;
e
4
;
e
5
;
e
6
;
e
7
;
e
8
g, with
e
1
=
(v
1
;
v
2
);
etc, as in the ï¬gure. Also, the sequence
e
1
;
e
4
;
e
7 is a walk from vertex
v
1 to
v
5 of length
2: Just as with digraphs, we can deï¬ne the power of a vertex in any graph as the number
of walks of length at most
2 originating at the vertex.
A very sensible question to ask about these examples: how could we write a computer
program to compute powers? More generally, how can we compute the total number of
walks of a certain length? Here is a key to the answer: all the information about our
graph (or digraph) can be stored in its adjacency matrix. In general, this is deï¬ned to
be a square matrix whose rows and columns are indexed by the vertices of the graph
and whose
(i;
j
)th entry is the number of edges going from vertex
i to vertex
j (it is
0
if there are none). Here we understand that a directed edge of a digraph must start at
i
and end at
j, while no such restriction applies to the edges of a graph.
Just for the record, if we designate the adjacency matrix of the digraph of Figure 2.3.2
by
A and the adjacency matrix of the graph of Figure 2.3.3 by
B
; then
A
=
2
6
6
4
0
1
0
1
0
0
1
0
1
0
0
0
0
1
0
0
3
7
7
5
and
B
=
2
6
6
6
6
6
6
4
0
1
0
1
0
1
1
0
1
0
0
1
0
1
0
0
1
0
1
0
0
0
0
1
0
0
1
0
0
1
1
1
0
1
1
0
3
7
7
7
7
7
7
5
Notice that we could reconstruct the entire digraph or graph from this matrix. Also
notice that in the adjacency matrix for a graph, an edge gets accounted for twice, since
it can be thought of as proceeding from one vertex to the other, or from the other to the
one.
For a general graph with
n vertices and adjacency matrix
A
=
[a
ij
], we can use this
matrix to compute powers of vertices without seeing a picture of the graph. To count
up the walks of length
1 emanating from vertex
i: simply add up the elements of the
ith

2.3. APPLICATIONS OF MATRIX ARITHMETIC
69
row of
A: Now what about the paths of length
2? Observe that there is an edge from
i to
k and then from
k to
j precisely when the product
a
ik
a
k
j is equal to
1: Otherwise, one
of the factors will be
0 and therefore the product is
0: So the number of paths of length
2 from vertex
i to vertex
j is the familiar sum
a
i1
a
1j
+
a
i2
a
2j
+



+
a
in
a
nj
This is just the
(i;
j
)th entry of the matrix
A
2
: A similar argument shows the following
fact:
THEOREM 2.3.9. If
A is the adjacency matrix of the graph
G, then the
(i;
j
)th entry of
A
r gives the number of walks of length
r starting at vertex
i and ending at vertex
j:
Since the power of vertex
i is the number of all paths of length
1 or
2 emanating from
vertex
i, we have the following key fact:
THEOREM 2.3.10. If
A is the adjacency matrix of the digraph
G, then the power of the
ith vertex is the sum of all entries in the
ith row of the matrix
A
+
A
2
:
EXAMPLE 2.3.11. Use the preceding facts to calculate the powers of all the vertices in
the digraph of Example 2.3.7.
SOLUTION. Using the matrix
A above we calculate that
A
+
A
2
=
2
6
6
4
0
1
0
1
0
0
1
0
1
0
0
0
0
1
0
0
3
7
7
5
+
2
6
6
4
0
1
0
1
0
0
1
0
1
0
0
0
0
1
0
0
3
7
7
5
2
6
6
4
0
1
0
1
0
0
1
0
1
0
0
0
0
1
0
0
3
7
7
5
=
2
6
6
4
0
2
1
1
1
0
1
0
1
1
0
1
0
1
1
0
3
7
7
5
An easy way to sum each row is to multiply
A
+
A
2 on the right by a column of
1â€™s,
but in this case we see immediately that the power of vertex
1 is
4, the power of vertex
3 is
3, and the power of vertices
2 and
4 is2; which is consistent with what we observed
earlier by inspection of the graph.
Difference Equations
The idea of a difference equation has numerous applications in mathematics and com-
puter science. In the latter ï¬eld, these equations often go by the name of â€œrecurrence
relations.â€ They can be used for a variety of applications ranging from population mod-
eling to analysis of complexity of algorithms. We will introduce them by way of a
simple ï¬nancial model.
EXAMPLE 2.3.12. Suppose that you invest in a contractual fund that has the stipulation
that you must invest in the funds for three years before you can receive any return on
your investment (with a positive ï¬rst year investment). Thereafter, you are vested in
the fund and may remove your money at any time. While you are vested in the fund,
annual returns are calculated as follows: money that was in the fund one year ago earns
nothing, while money that was in the fund two years ago earns
6% of its value and
money that was in the fund three years ago earns
12% of its value. Find an equations
that describes your investmentâ€™s growth.

70
2. MATRIX ALGEBRA
SOLUTION. Let
a
k be the amount of your investment in the
kth year. The numbers
a
0
;
a
1
;
a
2 represent your investments for the ï¬rst three years (weâ€™re counting from
0.)
Consider the
3rd year amount
a
3
: According to your contract, your total funds in the
3rd year will be
a
3
=
a
2
+
0:06a
1
+
0:12a
0
Now itâ€™s easy to write out a general formula for
a
k
+3 in terms of the preceding three
terms, using the same line of thought, namely
a
k
+3
=
a
k
+2
+
0:06a
k
+1
+
0:12a
k
;
k
=
0;
1;
2;
:
:
:
(2.3.1)
This is the desired formula.
In general, a homogeneous linear difference equation (or recurrence relation) of order
m in the variables
a
0
;
a
1
;
:
:
: is an equation of the form
a
k
+m
+
c
m 1
a
k
+m 1
+
:
:
:
+
c
1
a
k
+1
+
c
0
a
k
=
0;
k
=
0;
1;
2;
:
:
:
Notice that such an equation cannot determine the numbers
a
0
;
a
1
;
:
:
:
;
a
k
 1
: These
values have to be initially speciï¬ed, just as in our fund example. Notice that in our fund
example, we have to bring all terms of Equation 2.3.1 to the left hand side to obtain the
difference equation form
a
k
+3
 a
k
+2
 0:06a
k
+1
 0:12a
k
=
0
Now we see that
c
2
=
 1;
c
1
=
 0:06; and
c
0
=
 0:12:
There are many ways to solve difference equations; we are not going to give a complete
solution to this problem at this point â€“ we postpone this issue to Chapter 5, where we
introduce eigenvalues and eigenvectors. However, we can now show how to turn a
difference equation as given above into a matrix equation. Weâ€™ll illustrate the key idea
with our fund example. The secret is to identify the right vector variables. To this end,
deï¬ne an indexed vector
x
k by the formula
x
k
=
2
4
a
k
+2
a
k
+1
a
k
3
5
;
k
=
0;
1;
2;
:
:
:
We see that
x
k
+1
=
2
4
a
k
+3
a
k
+2
a
k
+1
3
5
from which it is easy to check that since
a
k
+3
=
a
k
+2
+
0:06a
k
+1
+
0:12a
k, we have
x
k
+1
=
2
4
1
0:06
0:12
1
0
0
0
1
0
3
5
x
k
=
Ax
k
This is the matrix form we seek. Notice that it seems to have a lot in common with the
Markov chains examined earlier in this section, in that we pass from one â€œstate vectorâ€
to another by multiplication by a ï¬xed â€œtransition matrixâ€
A:

2.3. APPLICATIONS OF MATRIX ARITHMETIC
71
2.3 Exercises
1. Let a function of the vector variable
x
=
(x
1
;
x
2
) be given by the formula
T
(x)
=
(x
1
+
x
2
;
2x
1
;
4x
2
 x
1
)
Show how to express this function as a matrix multiplication and deduce that it is a
linear function.
2. Prove that if
A
=

a
b
c
d

is a real
2

2 matrix, then the matrix multiplication
function maps a line through the origin onto a line through the origin or a point. Hint:
Recall that points on a non-vertical line through the origin have the form
(x;
mx):
3. Determine the effect of the matrix multiplication function
T
A on the
x-axis,
y-axis,
and the points
(1;
1), where
A is one of the following, and sketch your results.
(a)

1
0
0
 1

(b)

0
 1
 1
0

(c)
1
5

 3
 4
 4
3

(d)

1
0
0
0

(e)

1
 1
 1
1

(f)
1
5

2
4
4
8

4. Use the deï¬nition of matrix multiplication function to show that if
T
A
=
T
B
; then
A
=
B
: (See Exercise 7 of Section 2.)
5. Inpection of the graph in Figure 2.3.1 of the matrix multiplication function
T
A from
Example 2.3.3 suggests that this function has a ï¬xed point, that is, a vector
(x;
y
) such
that
T
A
((x;
y
))
=
(x;
y
): Describe this point on the graph and calculate it algebraically.
6. Suppose a Markov chain has transition matrix
2
4
:1
:3
0
0
:4
1
:9
:3
0
3
5 and initial state
2
4
0
1
0
3
5
:
(a) Calculate the second state vector of the system. (b) Use a computer to decide exper-
imentally whether or not the system tends to a limiting steady state vector. If so, what
is it?
7. You are given that a Markov chain has transition matrix

a
b
c
d

;where all entries
are between
0 and
1: Show how this matrix can be described using only two variables.
8. You are given that the digraph
G has vertex set
V
=
f1;
2;
3;
4;
5g and edge set
E
=
f(2;
1);
(1;
5);
(2;
5);
(5;
4);
(4;
2);
(4;
3);
(3;
2)g: Do the following for the graph
G:
(a) Sketch a picture of the graph.
(b) Find the adjacency matrix of the graph.
(c) Find the power of each vertex of the graph. Which vertices are the strongest?
(d) Is this graph a dominance-directed graph?

72
2. MATRIX ALGEBRA
9. A digraph has the following adjacency matrix:
2
6
6
6
6
4
1
0
0
1
0
0
0
0
1
1
1
1
0
0
1
0
1
1
1
0
1
1
0
1
0
3
7
7
7
7
5
(a) Draw a picture of this digraph (this graph has some self-loops).
(b) Compute the power of each node and the total number of walks in the digraph of
length at most 5. Use a computer algebra system or calculator to help you with the
computations.
10. Find the adjacency matrix of the graph of Figure 2.3.3 and use it to determine the
total number of walks of length of length less than or equal to
3 starting at the node
v
6
:
11. Convert the fourth order difference equation
a
k
+4
 2a
k
+3
+
3a
k
+2
 4a
k
+1
+
5a
k
=
0
into vector form.
12. Suppose that in Example 2.3.12 you invest $1,000 initially (the zeroth year) and
no further amounts. Make a table of the value of your investment for years
0 to
12:
Also include a column which calculates the annual interest rate that your investment is
earning each year, based on the current and previous yearsâ€™ value. What conclusions do
you draw? You will need a computer or calculator for this exercise.
2.4. Special Matrices and Transposes
There are certain types of matrices that are so important that they have acquired names
of their own. We are going to introduce some of these in this section, as well as one
more matrix operation which has proved to be a very practical tool in matrix analysis,
namely the operation of transposing a matrix.
Elementary Matrices and Gaussian Elimination
We are going to show a new way to execute the elementary row operations used in
Gaussian elimination. Recall the shorthand we used:

E
ij
: The elementary operation of switching the
ith and
jth rows of the matrix.

E
i
(c)
: The elementary operation of multiplying the
ith row by the nonzero
constant
c:

2.4. SPECIAL MATRICES AND TRANSPOSES
73

E
ij
(d)
: The elementary operation of adding
d times the
jth row to the
ith row.
From now on we will use the very same symbols to represent matrices. The size of the
matrix will depend on the context of our discussion, so the notation is ambiguous, but it
is still very useful.
NOTATION 2.4.1. An elementary matrix of size
n is obtained by performing the corre-
sponding elementary row operation on the identity matrix
I
n
: We denote the resulting
matrix by the same symbol as the corresponding row operation.
EXAMPLE 2.4.2. Describe the following elementary matrices of size
n
=
3:
(a)
E
13
( 4)
;
(b)
E
21
(3);
(c)
E
23
;
(d)
E
1
(1=2)
SOLUTION. We start with
I
3
=
2
4
1
0
0
0
1
0
0
0
1
3
5
For part (a) we add
 4 times the
3rd row of
I
3 to its ï¬rst row to obtain
E
13
( 4)
=
2
4
1
0
 4
0
1
0
0
0
1
3
5
For part (b) add
3 times the ï¬rst row of
I
3 to its second row to obtain
E
21
(3)
=
2
4
1
0
0
3
1
0
0
0
1
3
5
For part (c) interchange the second and third rows of
I
3 to obtain that
E
23
=
2
4
1
0
0
0
0
1
0
1
0
3
5
Finally, for part (d) we multiply the ï¬rst row of
I
3 by
1=2 to obtain
E
1
(1=2)
=
2
4
1=2
0
0
0
1
0
0
0
1
3
5
What good are these matrices? One can see that the following fact is true:
THEOREM 2.4.3. Let
C
=
B
A be a product of two matrices and perform an elemen-
tary row operation on
C
: Then the same result is obtained if one performs the same
elementary operation on the matrix
B and multiplies the result by
A on the right.
We wonâ€™t give a formal proof of this statement, but it isnâ€™t hard to see why it is true. For
example, suppose one interchanges two rows, say the
ith and
jth, of
C
=
B
A to obtain
a new matrix
D
: How do we get the
ith or
jth row of
C? Answer: multiply the corre-
sponding row of
B by the matrix
A: Therefore, we would obtain
D by interchanging

74
2. MATRIX ALGEBRA
the
ith and
jth rows of
B and multiplying the result by the matrix
A, which is exactly
what the Theorem says. Similar arguments apply to the other elementary operations.
Now take
B
=
I, and we see from the deï¬nition of elementary matrix and Theo-
rem 2.4.3 that the following is true.
COROLLARY 2.4.4. If an elementary row operation is performed on a matrix
A to
Elementary
Operations as
Matrix
Multiplication
obtain a matrix
A
0, then
A
0
=
E
A; where
E is the elementary matrix corresponding to
the elementary row operation performed.
The meaning of this corollary is that we accomplish an elementary row operation by
multiplying by the corresponding elementary matrix on the left. Of course, we donâ€™t
need elementary matrices to accomplish row operations; but they give us another per-
spective on row operations.
EXAMPLE 2.4.5. Express these calculations of Example 1.3.1 of Chapter 1 in matrix
product form:

2
 1
1
4
4
20

 !
E
12

4
4
20
2
 1
1

     !
E
1
(1=4)

1
1
5
2
 1
1

     !
E
21
( 2)

1
1
5
0
 3
 9


1
1
5
0
 3
 9

      !
E
2
( 1=3)

1
1
5
0
1
3

     !
E
12
( 1)

1
0
2
0
1
3

SOLUTION. One point to be careful about: the order of elementary operations. We
compose the elementary matrices on the left in that same order that the operations are
done. Thus we may state the above calculations in the concise form

1
0
2
0
1
3

=
E
12
( 1)E
2
( 1=3)E
21
( 2)E
1
(1=4)E
12

2
 1
1
4
4
20

It is important to read this line carefully and understand how it follows from the long
form above. This conversion of row operations to matrix multiplication will prove to be
very practical in the next section.
Some Matrices with Simple Structure
Certain types of matrices have already turned up frequently in our discussions. For
example, the identity matrices are particularly easy to deal with. Another example is
the reduced row echelon form. So let us classify some simple matrices and attach names
to them. The simplest conceivable matrices are zero matrices. We have already seen
that they are important in matrix addition arithmetic. Whatâ€™s next? For square matrices,
we have the following deï¬nitions, in ascending order of complexity.
DEFINITION 2.4.6. Let
A
=
[a
ij
] be a square
n

n matrix. Then
A is
 Scalar if
a
ij
=
0 and
a
ii
=
a
j
j for all
i
6=
j: (Equivalently:
A
=
cI
n for some
scalar
c, which explains the term â€œscalar.â€)

2.4. SPECIAL MATRICES AND TRANSPOSES
75
 Diagonal if
a
ij
=
0 for all
i
6=
j: (Equivalently: the off-diagonal entries of
A
are
0:)
 (Upper) triangular if
a
ij
=
0 for all
i
>
j: (Equivalently: the sub-diagonal
entries of
A are
0:)
 (Lower) triangular if
a
ij
=
0 for all
i
<
j: (Equivalently: the super-diagonal
entries of
A are
0:)
 Triangular if the matrix is upper or lower triangular.
 Strictly triangular if it is triangular and the diagonal entries are also zero.
SOLUTION. The index conditions that we use above have
simple interpretations. For example, the entry
a
ij with
i
>
j is located further down than over, since the row number
is larger than the column number. Hence, it resides in the
â€œlower triangleâ€ of the matrix. Similarly, the entry
a
ij with
i
<
j resides in the â€œupper triangle.â€ Entries
a
ij with
i
=
j reside along the main diagonal of the matrix. See
the adjacent ï¬gure for a picture of these triangular regions
of the matrix.
i = j
i < j
i > j
Figure 2.4.1: Matrix regions
EXAMPLE 2.4.7. Classify the following matrices (elementary matrices are understood
to be
3

3) in the terminology of Deï¬nition 2.4.6.
(a)
2
4
1
0
0
0
1
0
0
0
 1
3
5
(b)
2
4
2
0
0
0
2
0
0
0
2
3
5 (c)
2
4
1
1
2
0
1
4
0
0
2
3
5
(d)
2
4
0
0
0
1
 1
0
3
2
2
3
5 (e)
2
4
0
2
3
0
0
4
0
0
0
3
5 (f)
E
21
(3)
(g)
E
2
( 3)
SOLUTION. Notice that (a) is not scalar, since diagonal entries differ from each other,
but it is a diagonal matrix, since the off-diagonal entries are all
0: On the other hand,
the matrix of (b) is really just
2I
3, so this matrix is a scalar matrix. Matrix (c) has all
terms below the main diagonal equal to
0, so this matrix is triangular and, speciï¬cally,
upper triangular. Similarly, matrix (d) is lower triangular. Matrix (e) is clearly upper
triangular, but it is also strictly upper triangular since the diagonal terms themselves are
0: Finally, we have
E
21
(3)
=
2
4
1
0
0
3
1
0
0
0
1
3
5 and
E
2
( 3)
=
2
4
1
0
0
0
 3
0
0
0
1
3
5
so that
E
21
(3) is (lower) triangular and
E
2
( 3) is a diagonal matrix.
Here is another kind of structured matrix that occurs frequently enough in applications
to warrant a name. In Example 1.1.5 of Chapter 1 we saw that an approximation to a

76
2. MATRIX ALGEBRA
certain diffusion problem led to matrices of the form
A
5
=
2
6
6
6
6
4
2
 1
0
0
0
 1
2
 1
0
0
0
 1
2
 1
0
0
0
 1
2
 1
0
0
0
 1
2
3
7
7
7
7
5
If we want more accurate solutions to the original problem, we would need to solve sys-
tems with coefï¬cient matrix
A
n, where
n is larger than
5: Notice that the only nonzero
entries of such a matrix are those along the main diagonal, the entries along the ï¬rst
subdiagonal and ï¬rst superdiagonal. Such a matrix is called a tridiagonal matrix. For-
mally speaking, these are the square matrices
A
=
[a
ij
] such that
a
ij
=
0 if
i
>
j
+
1
or
j
>
i
+
1:
Block matrices
Another type of matrix that occurs frequently enough to be discussed is a block matrix.
Actually, we already used the idea of blocks when we describe the augmented matrix
of the system
Ax
=
b as the matrix
~
A
=
[A
j
b]: We say that
~
A has the block, or parti-
tioned, form
[A;
b]: What we are really doing is partitioning the matrix
~
A by inserting
a vertical line between elements. There is no reason we couldnâ€™t partition by inserting
more vertical lines or horizontal lines as well, and this partitioning leads to the blocks.
The main point to bear in mind when using the block notation is that the blocks must be
correctly sized so that the resulting matrix makes sense. The main virtue of the block
form that results from partitioning is that for purposes of matrix addition or multiplica-
tion, we can treat the blocks rather like scalars, provided the addition or multiplication
that results makes sense. We will use this idea from time to time without fanfare. One
could go through a formal description of partitioning and proofs; we wonâ€™t. Rather,
weâ€™ll show how this idea can be used by example.
EXAMPLE 2.4.8. Use block multiplication to simplify the following multiplication
SOLUTION. Here is the blocking that we want to use. It makes the column numbers of
the blocks on the left match the row numbers of the blocks on the right:
2
4
1
2
0
0
3
4
0
0
0
0
1
0
3
5
2
6
6
4
0
0
2
1
0
0
1
1
0
0
1
0
0
0
0
1
3
7
7
5
We see that these submatrices are built from zero matrices and these blocks:
A
=

1
2
3
4

;
B
=

1
0

;
C
=

2
1
1
1


2.4. SPECIAL MATRICES AND TRANSPOSES
77
Now we can work this product out by interpreting it as

A
0
0
B


0
C
0
I
2

=

A

0
+
0

0
A

C
+
0

I
2
0

0
+
B

0
0

C
+
B

I
2

=
2
4
0
0
4
3
0
0
10
7
0
0
1
0
3
5
For another example of block arithmetic, examine Example 2.2.6 and the discussion
following it. There we view a matrix as blocked into its respective columns, and a
column vector as blocked into its rows, to obtain
Ax
=
[a
1
;
a
2
;
a
3
]
2
4
x
1
x
2
x
3
3
5
=
a
1
x
1
+
a
2
x
2
+
a
3
x
3
Transpose of a Matrix
Sometimes we would prefer to work with a different form of a given matrix that contains
the same information. Transposes are operations that allow us to do that. The idea of
transposing is simple: interchange rows and columns. It turns out that for complex
matrices, there is an analogue which is not quite the same thing as transposing, though
it yields the same result when applied to real matrices. This analogue is called the
Hermitian transpose. Here are the appropriate deï¬nitions.
DEFINITION 2.4.9. Let
A
=
[a
ij
] be an
m

n matrix with (possibly) complex entries.
Then the transpose of
A is the
n

m matrix
A
T obtained by interchanging the rows and
columns of
A; so that the
(i;
j
)th entry of
A
T is
a
j
i
: The conjugate of
A is the matrix
A
=
[a
ij
]: Finally, the Hermitian transpose of
A is the matrix
A
H
=
A
T
:
Notice that in the case of a real matrix (that is, a matrix with real entries)
A there is
no difference between transpose and Hermitian transpose, since in this case
A
=
A:
Consider these examples.
EXAMPLE 2.4.10. Compute the transpose and Hermitian transpose of the following
matrices:
(a)

1
0
2
0
1
1

;
(b)

2
1
0
3

; (c)

1
1
+
i
0
2i

SOLUTION. For matrix (a) we have

1
0
2
0
1
1

H
=

1
0
2
0
1
1

T
=
2
4
1
0
0
1
2
1
3
5
Notice, by the way how the dimensions of a transpose get switched from the original.
For matrix (b) we have

2
1
0
3

H
=

2
1
0
3

T
=

2
0
1
3


78
2. MATRIX ALGEBRA
and for matrix (c) we have

1
1
+
i
0
2i

H
=

1
0
1
 i
 2i

;

1
1
+
i
0
2i

T
=

1
0
1
+
i
2i

In this case, transpose and Hermitian transpose are not the same.
Even when dealing with vectors alone, the transpose notation is rather handy. For ex-
ample, there is a bit of terminology that comes from tensor analysis (a branch of higher
linear algebra used in many ï¬elds including differential geometry, engineering mechan-
ics and relativity) that can be expressed very concisely with transposes:
DEFINITION 2.4.11. Let
u and
v be column vectors of the same size, say
n

1: Then
Inner and Outer
Products
the inner product of
u and
v is the scalar quantity
u
T
v and the outer product of
u and
v is the
n

n matrix
uv
T
:
EXAMPLE 2.4.12. Compute the inner and outer products of the vectors
u
=
2
4
2
 1
1
3
5
and
v
=
2
4
3
4
1
3
5
:
SOLUTION. Here we have the inner product
u
T
v
=
[2;
 1;
1]
2
4
3
4
1
3
5
=
2

3
+
( 1)4
+
1

1
=
3
while the outer product is
uv
T
=
2
4
2
 1
1
3
5
[3;
4;
1]
=
2
4
2

3
2

4
2

1
 1

3
 1

4
 1

1
1

3
1

4
1

1
3
5
=
2
4
6
8
2
 3
 4
 1
3
4
1
3
5
Here are a few basic laws relating transposes to other matrix arithmetic that we have
learned. These laws remain correct if transpose is replaced by Hermitian transpose,
with one exception:
(cA)
H
=
cA
H
:
Laws of Matrix Transpose. Let
A and
B be matrices of the appropriate sizes
so that the following operations make sense, and
c a scalar. Then
1.
(A
+
B
)
T
=
A
T
+
B
T
2.
(AB
)
T
=
B
T
A
T
3.
(cA)
T
=
cA
T
4.
(A
T
)
T
=
A
These laws are easily veriï¬ed directly from deï¬nition. For example, if
A
=
[a
ij
] and
B
=
[b
ij
] are
m

n matrices, then we have that
(A
+
B
)
T is the
n

m matrix given

2.4. SPECIAL MATRICES AND TRANSPOSES
79
by
(A
+
B
)
T
=
[a
ij
+
b
ij
]
T
=
[a
j
i
+
b
j
i
]
=
[a
j
i
]
+
[b
j
i
]
=
A
T
+
B
T
The other laws are proved similarly.
We will require explicit formulas for transposes of the elementary matrices in some later
calculations. Notice that the matrix
E
ij
(c) is a matrix with
1â€™s on the diagonal and
0â€™s
elsewhere, except that the
(i;
j
)th entry is
c: Therefore, transposing switches the entry
c
to the
(j;
i)th position and leaves all other entries unchanged. Hence
E
ij
(c)
T
=
E
j
i
(c):
With similar calculations we have these facts

E
T
ij
=
E
ij
Transposes of
Elementary
Matrices

E
i
(c)
T
=
E
i
(c)

E
ij
(c)
T
=
E
j
i
(c)
These formulas have an interesting application. Up to this point we have only con-
sidered elementary row operations. However, there are situations in which elementary
column operations on the columns of a matrix are useful. If we want to use such opera-
Elementary
Column
Operations and
Matrices
tions, do we have to start over, reinvent elementary column matrices, and so forth? The
answer is no and the following example gives an indication of why the transpose idea
is useful. This example shows how to do column operations in the language of matrix
arithmetic. In a nutshell, hereâ€™s the basic idea: suppose we want to do an elementary
column operation on a matrix
A corresponding to elementary row operation
E to get a
new matrix
B from
A. To do this, turn the columns of
A into rows, do the row operation
and then transpose the result back to get the matrix
B that we want. In algebraic terms:
B
=
(E
A
T
)
T
=
(A
T
)
T
E
T
=
AE
T
So all we have to do to perform an elementary column operation is multiply by the
transpose of the corresponding elementary row matrix on the right. Thus we see that the
transposes of elementary row matrices could reasonably be called elementary column
matrices.
EXAMPLE 2.4.13. Let
A be a given matrix. Suppose that we wish to express the result
B of swapping the second and third columns of
A, followed by adding
 2 times the
ï¬rst column to the second, as a product of matrices. How can this be done? Illustrate
the procedure with the matrix.
A
=

1
2
 1
1
 1
2

SOLUTION. Apply the preceding remark twice to obtain that
B
=
AE
T
23
E
21
( 2)
T
=
AE
23
E
12
( 2):
Thus we have
B
=

1
2
 1
1
 1
2

2
4
1
0
0
0
0
1
0
1
0
3
5
2
4
1
 2
0
0
1
0
0
0
1
3
5
as a matrix product.

80
2. MATRIX ALGEBRA
A very important type of special matrix is one which is invariant under the operation of
transposing. These matrices turn up naturally in applied mathematics. They have some
very remarkable properties that we will study in Chapters 4, 5 and 6.
DEFINITION 2.4.14. The matrix
A is said to be symmetric if
A
T
=
A and Hermitian
if
A
H
=
A: (Equivalently,
a
ij
=
a
j
i and
a
ij
=
a
j
i, for all
i;
j, respectively.)
From the laws of transposing elementary matrices above we see right away that
E
ij and
E
i
(c) supply us with examples of symmetric matrices. Here are a few more.
EXAMPLE 2.4.15. Are the following matrices symmetric or Hermitian?
(a)

1
1
+
i
1
 i
2

;
(b)

2
1
1
3

; (c)

1
1
+
i
1
+
i
2i

SOLUTION. For matrix (a) we have

1
1
+
i
1
 i
2

H
=

1
1
+
i
1
 i
2

T
=

1
1
+
i
1
 i
2

Hence this matrix is Hermitian. However, it is not symmetric since the
(1;
2)th and
(2;
1)th entries differ. Matrix (b) is easily seen to be symmetric by inspection. Matrix
(c) is symmetric since the
(1;
2)th and
(2;
1)th entries agree, but it is not Hermitian since

1
1
+
i
1
 i
2i

H
=

1
1
+
i
1
 i
2i

T
=

1
1
+
i
1
 i
 2i

and this last matrix is clearly not equal to matrix (c).
EXAMPLE 2.4.16. Consider the quadratic form
Q(x;
y
;
z
)
=
x
2
+
2y
2
+
z
2
+
2xy
+
y
z
+
3xz
:
Express this function in terms of matrix products and transposes.
SOLUTION. Write the quadratic form as
x(x
+
2y
+
3z
)
+
y
(2y
+
z
)
+
z
2
=

x
y
z

2
4
x
+
2y
+
3z
2y
+
z
z
3
5
=

x
y
z

2
4
1
2
3
0
2
1
0
0
1
3
5
2
4
x
y
z
3
5
=
x
T
Ax;
where
x
=
(x;
y
;
z) and
A
=
2
4
1
2
3
0
2
1
0
0
1
3
5
:

2.4. SPECIAL MATRICES AND TRANSPOSES
81
Rank of the Matrix Transpose
An important question to ask is how the rank of a matrix transpose (or Hermitian trans-
pose) is connected to the rank of matrix. We focus on transposes. First we need the
following
THEOREM 2.4.17. Let
A;
B be matrices such that the product
AB is deï¬ned. Then
rank
AB

rank
A
PROOF. Let
E be a product of elementary matrices such that
E
A
=
R, where
R
is the reduced row echelon form of
A: If
rank
A
=
r, then the ï¬rst
r rows of
A have
leading entries of
1, while the remaining rows are zero rows. Also, we saw in Chapter
1 that elementary row operations do not change the rank of a matrix since, according to
Corollary 1.4.9 they do not change the reduced row echelon form of a matrix. Therefore,
rank
AB
=
rank
E
(AB
)
=
rank
(E
A)B
=
rank
R
B
Now the matrix
R
B has the same number of rows as
R and the ï¬rst
r of these rows
may or may not be nonzero, but the remaining rows must be zero rows, since they result
from multiplying columns of
B by the zero rows of
R
: If we perform elementary row
operations to reduce
R
B to its reduced row echelon form we will possibly introduce
more zero rows than
R has. Consequently,
rank
R
B

r
=
rank
A, which completes
the proof.
THEOREM 2.4.18. For any matrix
A,
rank
A
=
rank
A
T
PROOF. As in the previous theorem, let
E be a product of elementary matrices such
that
E
A
=
R, where
R is the reduced row echelon form of
A: If
rank
A
=
r, then the
ï¬rst
r rows of
R have leading entries of
1 whose column numbers form an increasing
sequence, while the remaining rows are zero rows. Therefore,
R
T
=
A
T
E
T is a matrix
whose columns have leading entries of
1 and whose row numbers form an increasing
sequence. Use elementary row operations to clear out the nonzero entries below each
column with a leading
1 to obtain a matrix whose rank is equal to the number of such
leading entries, i.e., equal to
r
: Thus,
rank
R
T
=
r
:
From Theorem 2.4.17 we have that
rank
A
T
E
T

rank
A
T
: It follows that
rank
A
=
rank
R
T
=
rank
A
T
E
T

rank
A
T
If we substitute the matrix
A
T for the matrix
A in this inequality, we obtain that
rank
A
T

rank
(A
T
)
T
=
rank
A
It follows from these two inequalities that
rank
A
=
rank
A
T , which is what we wanted
to show.
It is instructive to see how a speciï¬c example might work out in the preceding proof.
For example,
R might look like this, where an â€œxâ€ designates an arbitrary entry,
R
=
2
6
6
4
1
0
x
0
x
0
1
x
0
x
0
0
0
1
x
0
0
0
0
0
3
7
7
5

82
2. MATRIX ALGEBRA
so that
R
T would look like this
R
T
=
2
6
6
6
6
4
1
0
0
0
0
1
0
0
x
x
0
0
0
0
1
0
x
x
x
0
3
7
7
7
7
5
Thus if we use elementary row operations to zero out the entries below a column pivot,
all entries to the right and below this pivot are unaffected by these operations. Now start
with the leftmost column and proceed to the right, zeroing out all entries under each
column pivot. The result is a matrix that looks like
2
6
6
6
6
4
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
3
7
7
7
7
5
Now swap rows to move the zero rows to the bottom if necessary and we see that the
reduced row echelon form of
R
T has exactly as many nonzero rows as did
R, that is,
r
nonzero rows.
A ï¬rst application of this important fact is to give a fuller picture of the rank of a product
of matrices than Theorem 2.4.17:
COROLLARY 2.4.19. If the product
AB is deï¬ned, then
Rank of Matrix
Product
rank
AB

minfrank
A;
rank
B
g
PROOF. We know from Theorem 2.4.17 that
rank
AB

rank
A and
rank
B
T
A
T

rank
B
T
Since
B
T
A
T
=
(AB
)
T
; Theorem 2.4.18 tells us that
rank
B
T
A
T
=
rank
AB and
rank
B
T
=
rank
B
Put all this together and we have
rank
AB
=
rank
B
T
A
T

rank
B
T
=
rank
B
It follows that
rank
AB is at most the smaller of
rank
A and
rank
B, which is what the
corollary asserts.
2.4 Exercises
1. Write out explicitly what the following
4

4 elementary matrices are:
(a)
E
24
(3)
(b)
E
14
(c)
E
3
(2)
(d)
E
T
23
( 1)
(e)
E
T
24
2. Describe the effects of these multiplications as column operations on the matrix
A:
(a)
AE
12
(b)
AE
13
( 2)
(c)
AE
2
( 1)E
14
(3) (d)
AE
14
E
41

2.4. SPECIAL MATRICES AND TRANSPOSES
83
3. For each of the following matrices, identify all of the simple structure descriptions
that apply to the matrix.
(a)
2
4
0
0
0
0
0
3
0
0
0
3
5 (b)

2
0
3
1

(c)
I
3 (d)
2
6
6
4
2
1
4
2
0
2
1
1
0
0
1
1
0
0
0
1
3
7
7
5 (e)

1
0
0
 1

4. Calculate the matrix product
AB using block multiplication, where
A
=

R
0
S
T

B
=

U
V

R
=

1
1
0

S
=

1
1
1
1
2
1

T
=

1
 1
2
2

U
=
2
4
1
0
1
2
1
1
3
5
V
=

3
1
0
1

5. Let
A and
B be square matrices and suppose that the matrix
M
=

A
C
0
D

in
block form. Show that
M
2
=

A
2
D
0
D
2

for some matrix
D
:
6. Interpret the calculation of Example 2.2.6 as a block multiplication.
7. Express the rank
1 matrix
2
4
1
2
1
0
0
0
2
4
2
3
5 as an outer product of two vectors.
8. Express the following in the elementary matrix notation:
(a)
2
4
1
3
0
0
1
0
0
0
1
3
5
(b)
2
4
1
0
0
0
1
0
2
0
1
3
5
(c)
2
4
1
0
0
0
1
0
2
0
1
3
5
(d)
2
4
1
0
0
0
3
0
0
0
1
3
5
9. Compute the reduced row echelon form of the following matrices and express each
form as a product of elementary matrices and the original matrix.
(a)

1
2
1
3

(b)
2
4
1
1
0
0
1
1
0
2
2
3
5
(c)

1
1
0
1
1
 2

(d)
2
4
2
1
0
1
0
2
3
5
10. Compute the transpose and Hermitian transpose of the following matrices and de-
termine which, if any, are symmetric or Hermitian symmetric.
(a)

1
 3
2

(b)
2
4
2
1
0
3
1
 4
3
5
(c)

1
i
 i
2

(d)
2
4
1
1
3
1
0
0
3
0
2
3
5
11. Verify that the elementary matrix transpose law holds for
3

3 elementary matrix
E
23
(4):

84
2. MATRIX ALGEBRA
12. Answer True/False and give reasons:
(a) For matrix
A and scalar
c,
(cA)
H
=
cA
H:
(b) Every diagonal matrix is symmetric.
(c) The rank of the matrix
A may differ from the rank of
A
T .
(d) Every diagonal matrix is Hermitian.
(e) Every tridiagonal matrix is symmetric.
13. Show that a triangular and symmetric matrix must be a diagonal matrix.
14. Show that if
C has block form
C
=

A
0
0
B

; then
rank
C
=
rank
A
+
rank
B
:
15. Prove from deï¬nition that
(A
T
)
T
=
A:
16. Express the quadratic form
Q(x;
y
;
z
)
=
2x
2
+
y
2
+
z
2
+
2xy
+
4y
z
 6xz in the
matrix form
x
T
Ax as in Example 2.4.16.
17. Let
A
=

 2
1
 2i
0
3

and verify that both
A
H
A and
AA
H are Hermitian.
18. Let
A be an
m

n matrix. Show that both
A
H
A and
AA
H are Hermitian.
19. Use Corollary 2.4.19 to prove that the outer product of any two vectors is at most a
rank
1 matrix.
20. Let
A be a square real matrix. Show the following:
(a) The matrix
B
=
1
2
(A
+
A
T
) is symmetric.
(b) The matrix
C
=
1
2
(A
 A
T
) is skew-symmetric (a matrix
C is skew-symmetric if
C
T
=
 C
:)
(c) The matrix
A can be expressed as the sum of a symmetric matrix and a skew-
symmetric matrix.
(d) With
B and
C as in parts (a) and (b), show that for any vector
x of conformable size,
x
T
Ax
=
x
T
B
x:
21. Use Exercise 20 to express
A
=
2
4
2
2
 6
0
1
4
0
0
1
3
5 as a sum of a symmetric and
a skew-symmetric matrix. What does part (d) of this exercise say about the quadratic
form
Q(
x)
=
x
T
Ax?
22. Find all
2

2 idempotent upper triangular matrices
A (idempotent means
A
2
=
A).
23. Show that an
n

n strictly upper triangular matrix
N is nilpotent. (It might help
to see what happens in a
2

2 and
3

3 case ï¬rst.)
24. Let
D be a diagonal matrix with distinct entries on the diagonal and
B
=
[b
ij
] any
other matrix matrix of the same size. Show that
D
B
=
B
D if and only if
B is diagonal.
Hint: Compare
(i;
j
)th

2.5. MATRIX INVERSES
85
2.5. Matrix Inverses
Deï¬nitions
We have seen that if we could make sense of â€œ1=
Aâ€, then we could write the solution to
the linear system
Ax
=
b as simply
x
=
(1=
A)b: We are going to tackle this problem
now. First, we need a deï¬nition of the object that we are trying to uncover. Notice that
â€œinversesâ€ could only work on one side. For example,

1
2


 1
1

=
[1]
=

2
3


 1
1

which suggests that both

1
2

and

2
3

should qualify as left inverses of the
matrix

 1
1

, since multiplication on the left by them results in a
1

1 identity matrix.
As a matter of fact right and left inverses are studied and do have applications. But they
have some unusual properties such as non-uniqueness. We are going to focus on a type
of inverse that is closer to the familiar inverses in ï¬elds of numbers, namely, two-sided
inverses. These only make sense for square matrices, so the non-square example above
is ruled out.
DEFINITION 2.5.1. Let
A be a square matrix. Then a (two-sided) inverse for
A is a
square matrix
B of the same size as
A such that
AB
=
I
=
B
A: If such a
B exists,
then the matrix
A is said to be invertible.
Of course, any non-square matrix is non-invertible. Square matrices are classiï¬ed as
either â€œ singularâ€, i.e., non-invertible, or â€œ nonsingularâ€, i.e., invertible. Since we will
mostly be concerned with two-sided inverses, the unqualiï¬ed term â€œinverseâ€ will be un-
derstood to mean a â€œtwo-sided inverse.â€ Notice that this deï¬nition is actually symmetric
in
A and
B
: In other words, if
B is an inverse for
A, then
A is an inverse for
B
:
Examples of Inverses
EXAMPLE 2.5.2. Show that
B
=

1
1
1
2

is an inverse for the matrix
A
=

2
 1
 1
1

:
SOLUTION. All we have to do is check the deï¬nition. But remember that there are two
multiplications to conï¬rm. (Weâ€™ll show later that this isnâ€™t necessary, but right now we
are working strictly from the deï¬nition.) We have
AB
=

2
 1
 1
1


1
1
1
2

=

2

1
 1

1
2

1
 1

2
 1

1
+
1

1
 1

1
+
1

2

=

1
0
0
1

=
I

86
2. MATRIX ALGEBRA
and similarly
B
A
=

1
1
1
2


2
 1
 1
1

=

1

2
+
1

( 1)
1

( 1)
+
1

1
1

2
+
2

( 1)
1

( 1)
+
2

1

=

1
0
0
1

=
I
Therefore the deï¬nition for inverse is satisï¬ed, so that
A and
B work as inverses to each
other.
EXAMPLE 2.5.3. Show that the matrix
A
=

1
1
1
1

cannot have an inverse.
SOLUTION. How do we get our hands on a â€œnon-inverseâ€? We try an indirect approach.
If
A had an inverse
B, then we could always ï¬nd a solution to the linear system
Ax
=
b
by multiplying each side on the left by
B to obtain that
B
Ax
=
I
x
=
x
=
B
b, no
matter what right hand side vector
b we used. Yet it is easy to come up with right hand
side vectors for which the system has no solution. For example, try
b
=

1
2

. Since
the resulting system is clearly inconsistent, there cannot be an inverse matrix
B, which
is what we wanted to show.
The moral of this last example is that it is not enough for every entry of a matrix to be
nonzero for the matrix itself to be invertible. Our next example yields a gold mine of
invertible matrices, namely any elementary matrix we construct.
EXAMPLE 2.5.4. Find formulas for inverses of all the elementary matrices.
SOLUTION. Remember from Corollary 2.4.4 that left multiplication by an elementary
matrix is the same as performing the corresponding elementary row operation. Further-
more, from the discussion following Theorem 1.4.5 we see the following

E
ij
: The elementary operation of switching the
ith and
jth rows is undone by
applying
E
ij
: Hence
E
ij
E
ij
=
E
ij
E
ij
I
=
I
so that
E
ij works as its own inverse. (This is rather like
 1, since
( 1)

( 1)
=
1:)
Elementary
Matrix Inverses

E
i
(c)
: The elementary operation of multiplying the
ith row by the nonzero
constant c, is undone by applying
E
i
(1=c): Hence
E
i
(1=c)E
i
(c)
=
E
i
(1=c)E
i
(c)I
=
I
; and
E
i
(c)E
i
(1=c)
=
E
i
(c)E
i
(1=c)I
=
I

E
ij
(d)
: The elementary operation of adding d times the
jth row to the
ith row
is undone by applying
E
ij
( d): Hence
E
ij
( d)E
ij
(d)
=
E
ij
( d)E
ij
(d)I
=
I
; and
E
ij
(d)E
ij
( d)
=
E
ij
( d)E
ij
(d)I
=
I

2.5. MATRIX INVERSES
87
Speciï¬cally, in the case of
2

2 matrices, this means, e.g., that
E
12
=

0
1
1
0

has an inverse

0
1
1
0

, while the matrix
E
21
( 3)
=

1
0
 3
1

has an inverse

1
0
3
1

=
E
21
(3):
Laws of Inverses
Here are some of the basic laws of inverse calculations.
Laws of Matrix Inverses. Let
A;
B
;
C be matrices of the appropriate sizes
so that the following multiplications make sense,
I a suitably sized identity
matrix, and
c a nonzero scalar. Then
1. (Uniqueness) The matrix
A has at most one inverse, henceforth de-
noted as
A
 1, provided
A is invertible.
2. (Double Inverse) If
A is invertible, then
 A
 1

 1
=
A:
3. (2=3 Rule) If any two of the three matrices
A,
B and
AB are invertible,
then so is the third, and moreover
(AB
)
 1
=
B
 1
A
 1
:
4. If
A is invertible, then
(cA)
 1
=
(1=c)A
 1
:
5. (Inverse/Transpose) If
A is invertible, then
(A
T
)
 1
=
(A
 1
)
T and
(A
H
)
 1
=
(A
 1
)
H
:
6. (Cancellation) Suppose
A is invertible. If
AB
=
AC or
B
A
=
C
A,
then
B
=
C
:
Notes: Observe that the
2=3 Rule reverses order when taking the inverse of a product.
This should remind you of the operation of transposing a product. A common mistake
is to forget to reverse the order. Secondly, notice that the cancellation law restores
something that appeared to be lost when we ï¬rst discussed matrices. Yes, we can cancel
a common factor from both sides of an equation, but (1) the factor must be on the same
side and (2) the factor must be an invertible matrix.
Veriï¬cation of Laws: Suppose that both
B and
C work as inverses to the matrix
A:
We will show that these matrices must be identical. For associativity of matrices and
identity laws give that
B
=
B
I
=
B
(AC
)
=
(B
A)C
=
I
C
=
C
Henceforth, we shall write
A
 1 for the unique (two-sided) inverse of the square matrix
A, provided of course that there is an inverse at all (remember that existence of inverses
is not a sure thing).
The double inverse law is a matter of examining the deï¬nition of inverse:
AA
 1
=
I
=
A
 1
A
shows that
A is an inverse matrix for
A
 1
: Hence,
(A
 1
)
 1
=
A:
Now suppose that
A and
B are both invertible and of the same size. Use the laws of
matrix arithmetic and we see that
AB
(B
 1
A
 1
)
=
A(B
B
 1
)A
 1
=
AI
A
 1
=
AA
 1
=
I

88
2. MATRIX ALGEBRA
and that
(B
 1
A
 1
)AB
=
B
 1
(A
 1
A)B
=
B
 1
I
B
=
B
 1
B
=
I
In other words, the matrix
B
 1
A
 1 works as an inverse for the matrix
AB, which is
what we wanted to show. We leave the remaining cases of the
2=3 Rule as an exercise.
Suppose that
c is nonzero and perform the calculation
(cA)(1=c)A
 1
=
(c=c)AA
 1
=
1

I
=
I
A similar calculation on the other side shows that
(cA)
 1
=
(1=c)A
 1
:
Next, apply the transpose operator to the deï¬nition of inverse (Equation 2.5.1) and use
the law of transpose products to obtain that
(A
 1
)
T
A
T
=
I
T
=
I
=
A
T
(A
 1
)
T
This shows that the deï¬nition of inverse is satisï¬ed for
(A
 1
)
T relative to
A
T , that is,
that
(A
T
)
 1
=
(A
 1
)
T , which is the inverse/transpose law. The same argument works
with Hermitian transpose in place of transpose.
Finally, if
A is invertible and
AB
=
AC, then multiply both sides of this equation on
the left by
A
 1 to obtain that
A
 1
(AB
)
=
(A
 1
A)B
=
B
=
A
 1
(AC
)
=
(A
 1
A)C
=
C
which is the cancellation that we want.
We can now extend the power notation to negative exponents.
NOTATION 2.5.5. Let
A be an invertible matrix and
k a positive integer. Then we write
A
 k
=
A
 1
A
 1

:
:
:

A
 1
where the product is taken over
k terms.
The laws of exponents that we saw earlier can now be expressed for arbitrary integers,
provided that
A is invertible. Here is an example of how we can use the various laws of
arithmetic and inverses to carry out an inverse calculation.
EXAMPLE 2.5.6. Let
A
=
2
4
1
2
0
0
1
1
0
0
1
3
5
Show that
(I
 A)
3
=
0 and use this to ï¬nd
A
 1
:
SOLUTION. First we check that
(I
 A)
=
2
4
1
0
0
0
1
0
0
0
1
3
5
 2
4
1
2
0
0
1
1
0
0
1
3
5
=
2
4
0
 2
0
0
0
 1
0
0
0
3
5

2.5. MATRIX INVERSES
89
so that
(I
 A)
3
=
2
4
0
 2
0
0
0
 1
0
0
0
3
5
2
4
0
 2
0
0
0
 1
0
0
0
3
5
2
4
0
 2
0
0
0
 1
0
0
0
3
5
=
2
4
0
0
2
0
0
0
0
0
0
3
5
2
4
0
 2
0
0
0
 1
0
0
0
3
5
=
2
4
0
0
0
0
0
0
0
0
0
3
5
Now we do some symbolic algebra, using the laws of matrix arithmetic:
0
=
(I
 A)
3
=
(I
 A)(I
2
 2A
+
A
2
)
=
I
 3A
+
3A
2
 A
3
Subtract all terms involving
A from both sides to obtain that
3A
 3A
2
+
A
3
=
A

3I
 3A
2
+
A
3
=
A(3I
 3A
+
A
2
)
=
I
Since
A(3I
 3A
+
A
2
)
=
(3I
 3A
+
A
2
)A, we see from deï¬nition of inverse that
A
 1
=
3I
 3A
+
A
2
=
2
4
1
 2
2
0
1
 1
0
0
1
3
5
Notice, by the way, that in the preceding example we were careful not to leave a â€œ3â€
behind when we factored out
A from
3A: The reason is that
3
+
3A
+
A
2 makes no
sense as a sum, since one term is a scalar and the other two are matrices.
Rank and Inverse Calculation
Although we can calculate a few examples of inverses such as the last example, we
really need a general procedure. So letâ€™s get right to the heart of the matter. How can
we ï¬nd the inverse of a matrix, or decide that none exists? Actually, we already have
done all the hard work necessary to understand computing inverses. The secret is in the
notion of reduced row echelon form and the attendant idea of rank. (Remember, we use
elementary row operations to reduce a matrix to its reduced row echelon form. Once
we have done so, the rank of the matrix is simply the number of nonzero rows in the
reduced row echelon form.) Letâ€™s recall the results of Example 2.3.12:

1
0
2
0
1
3

=
E
12
( 1)E
2
( 1=3)E
21
( 2)E
1
(1=4)E
12

2
 1
1
4
4
20

Now remove the last column from each of the matrices at the right of each side and we
have this result:

1
0
0
1

=
E
12
( 1)E
2
( 1=3)E
21
( 2)E
1
(1=4)E
12

2
 1
4
4

This suggests that if
A
=

2
 1
4
4

, then
A
 1
=
E
12
( 1)E
2
( 1=3)E
21
( 2)E
1
(1=4)E
12
To prove this, we argue in the general case as follows: let
A be an
n

n matrix and
suppose that by a succession of elementary row operations
E
1
;
E
2;
:
:
:
;
E
k we reduce

90
2. MATRIX ALGEBRA
A to its reduced row echelon form
R, which happens to be
I
: In the language of matrix
multiplication, what we have obtained is
I
=
E
k
E
k
 1

:
:
:

E
1
A:
Now let
B
=
E
k
E
k
 1

:
:
:

E
1
: By repeated application of the 2/3 theorem, we see
that a product of any number of invertible matrices is invertible. Since each elementary
matrix is invertible, it follows that
B is. Multiply both sides of the equation
I
=
B
A by
B
 1 to obtain that
B
 1
I
=
B
 1
=
B
 1
B
A
=
A: Therefore, A is the inverse of the
matrix
B, hence is itself invertible.
Hereâ€™s a practical trick for computing this product of elementary matrices on the ï¬‚y:
Super-
Augmented
Matrix
form what we term the super-augmented matrix
[A
j
I
]: Now, if we perform the ele-
mentary operation
E on the super-augmented matrix we have the same result as
E
[A
j
I
]
=
[E
A
j
E
I
]
=
[E
A
j
E
]
So the matrix occupied by the
I part of the super-augmented matrix is just the product
of the elementary matrices that we have used so far. Now continue applying elemen-
tary row operations until the part of the matrix originally occupied by
A is reduced
to the reduced row echelon form of
A: We end up with this schematic picture of our
calculations:

A
j
I

          !
E
1
;
E
2
;
:
:
:
;
E
k

R
j
B

where
R is the reduced row echelon form of
A and
B
=
E
k
E
k
 1

:
:
:

E
1 is the product
of the various elementary matrices we used, composed in the correct order of usage. We
can summarize this discussion with the following
Inverses Algorithm : Given an
n

n matrix
A, to compute
A
 1:
1. Form the super-augmented matrix
e
A
=
[A
j
I
n
]:
2. Reduce the ï¬rst
n columns of
e
A to reduced row echelon form by per-
forming elementary operations on the matrix
e
A resulting in the matrix
[R
j
B
]:
3. If
R
=
I
n then set
A
 1
=
B, else
A is singular and
A
 1 does not exist.
EXAMPLE 2.5.7. Use the Inverses Algorithm to compute the inverse of Example 2.2.5,
A
=
2
4
1
2
0
0
1
1
0
0
1
3
5
SOLUTION. Notice that this matrix is already upper triangular. Therefore, as in Gauss-
ian elimination, it is a bit more efï¬cient to start with the bottom pivot and clear out
entries above in reverse order. So we compute
[A
j
I
3
]
=
2
4
1
2
0
1
0
0
0
1
1
0
1
0
0
0
1
0
0
1
3
5
     !
E
23
( 1)
2
4
1
2
0
1
0
0
0
1
0
0
1
 1
0
0
1
0
0
1
3
5
     !
E
1;2
( 2)
2
4
1
0
0
1
 2
2
0
1
0
0
1
 1
0
0
1
0
0
1
3
5

2.5. MATRIX INVERSES
91
So we conclude that
A is indeed invertible and
A
 1
=
2
4
1
 2
2
0
1
 1
0
0
1
3
5
There is a simple formula for the inverse of a general
2

2 matrix
A
=

a
b
c
d

: Set
D
=
ad
 bc: It is easy to verify that if
D
6=
0; then
A
 1
=
1
D

d
 b
 c
a

EXAMPLE 2.5.8. Use the two by two inverse formula to ï¬nd the inverse of the ma-
trix
A
=

1
 1
1
2

; and verify that the same answer results if we use the inverses
algorithm.
SOLUTION. First we apply the inverses algorithm.

1
 1
1
0
1
2
0
1

      !
E
21
( 1)

1
 1
1
0
0
3
 1
1

     !
E
3(1=3)

1
 1
1
0
0
1
 1=3
1=3

    !
E
12
(1)

1
0
2=3
1=3
0
1
 1=3
1=3

Thus we have found that

1
 1
1
2

 1
=
1
3

2
1
 1
1

:
To apply the inverse formula, calculate
D
=
1

2
 1

( 1)
=
3: Swap diagonal entries
of
A, negate the off-diagonal entries and divide by
D to get the same result as we have
just obtained in the preceding equation for the inverse.
The formula of the preceding example is well worth memorizing, since we will fre-
quently need to ï¬nd the inverse of a
2

2 matrix. Notice that in order for it to make
sense, we have to have
D nonzero. The number
D is called the determinant of the ma-
trix
A: We will have more to say about this number in the next section. In our current
example it is fairly easy to see why
A must have
D
6=
0 in order for its inverse to exist
if we look ahead to the next theorem. Notice in the above elementary operation calcu-
lations that if
D
=
0 then elementary operations on
A lead to a matrix with a row of
zeros. Therefore, the rank of
A will be smaller than
2: Here is a summary of our current
knowledge of the invertibility of a square matrix.
THEOREM 2.5.9. The following are equivalent conditions on the square
n

n matrix
Conditions for
Invertibility
A:
1. The matrix
A is invertible.
2. There is a square matrix
B such that
B
A
=
I
:
3. The linear system
Ax
=
b has a unique solution for every right hand side vector
b:
4. The linear system
Ax
=
b has a unique solution for some right hand side vector
b:
5. The linear system
Ax
=
0 has only the trivial solution.

92
2. MATRIX ALGEBRA
6.
rank
A
=
n:
7. The reduced row echelon form of
A is
I
n
:
8. The matrix
A is a product of elementary matrices.
PROOF. The method of proof here is to show that each condition implies the next,
and that the last condition implies the ï¬rst. This connects all the conditions in a circle,
so that any one condition will imply any other and therefore all are equivalent to each
other. Here is our chain of reasoning:
(1) implies (2): Assume
A is invertible. Then the choice
B
=
A
 1 certainly satisï¬es
condition (2).
(2) implies (3): Assume (2) is true. Given a system
Ax
=
b, we can multiply both sides
on the left by
B to get that
x
=
I
x
=
B
Ax
=
B
b: So there is only one solution, if any.
On the other hand, if the system were inconsistent then we would have
rank
A
<
n: By
Corollary 2.4.19
rank
B
A
<
n, contradicting the fact that
rank
I
n
=
n: Hence, there
is a solution, which proves (3).
(3) implies (4): This statement is obvious.
(4) implies (5): Assume (4) is true. Say the unique solution to
Ax
=
b is
x
0
: If the
system
Ax
=
0 had a nontrivial solution, say
z, then we could add
z to
x
0 to obtain a
different solution
x
0
+
z of the system
Ax
=
b (check:
A(z
+
x
0
)
=
Az
+
Ax
0
=
0
+
b
=
b:) This is impossible since (4) is true, so (5) follows.
(5) implies (6): Assume (5) is true. We know from Theorem 1.4.15 of Chapter 1 that
the consistent system
Ax
=
0 has a unique solution precisely when the rank of
A is
n:
Hence (6) must be true.
(6) implies (7): Assume (6) is true. The reduced row echelon form of
A is the same
size as
A, that is
n

n, and must have a row pivot entry
1 in every row which must be
the only nonzero entry in its column. This exactly describes the matrix
I
n, so that (7) is
true.
(7) implies (8): Assume (7) is true. We know that the matrix
A is reduced to its re-
duced row echelon form by applying a sequence of elementary operations, or what
amounts to the same thing, by multiplying the matrix
A on the left by elementary
matrices
E
1
;
E
2
;
:
:
:
;
E
k, say. Then
E
1
E
2
:
:
:
E
k
A
=
I
: But we know from Exam-
ple 2.5.4 that each elementary matrix is invertible and that their inverses are them-
selves elementary matrices. By successive multiplications on the left we obtain that
A
=
E
 1
k
E
 1
k
 1
:
:
:
E
 1
1
I, showing that
A is a product of elementary matrices which is
condition (8).
(8) implies (1): Assume (8) is true. Repeated application of the
2=3 Inverses Rule
shows that the product of any number of invertible matrices is itself invertible. Since
elementary matrices are invertible, condition (1) must be true.
There is an interesting consequence to Theorem 2.5.9 that has been found to be useful
in some contexts. Itâ€™s an either/or statement, so it will always have something to say
about any square linear system. This type of statement is sometimes called a Fredholm
alternative. Many theorems go by this name, and weâ€™ll state another one in Chapter 5.
Fredholm
Alternative
Notice that a matrix is not invertible if and only one of the conditions of the Theorem
fail. Certainly it is true that either a square matrix is invertible or not invertible. Thatâ€™s

2.5. MATRIX INVERSES
93
all this Fredholm alternative really says, but it uses the equivalent conditions (3) and (5)
of Theorem 2.5.9 to say it in a different way:
COROLLARY 2.5.10. Given a square linear system
Ax
=
b, either the system has a
unique solution for every right hand side vector
b or there is a nonzero solution
x
=
x
0
to the homogeneous system
Ax
=
0:
We conclude this section with an application of the matrix algebra developed so far to
the problem of solving nonlinear equations. Although we focus on two equations in
two unknowns, the same ideas can be extended to any number of equations in as many
unknowns.
Recall that we could solve the one variable (usually nonlinear) equation
f
(x)
=
0 for
a solution point
x
1 at which
f
(x
1
)
=
0 from a given â€œnearbyâ€ point
x
0 by setting
dx
=
x
1
 x
0
; and assuming that the change in
f is
f
=
f
(x
1
)
 f
(x
0
)
=
 f
(x
0
)

d
f
=
f
0
(x
0
)
dx
=
f
0
(x
0
)(x
1
 x
0
):
Now solve for
x
1 in the equation
 f
(x
0
)
=
f
0
(x
0
)(x
1
 x
0
) and get the equation
x
1
=
x
0
 f
(x
0
)
f
0
(x
0
)
Replace
1 by
n
+
1 and
0 by
n to obtain the famous Newton formula:
x
n+1
=
x
n
 f
(x
n
)
f
0
(x
n
)
(2.5.1)
The idea is to start with
x
0
; use the formula to get
x
1 and if
f
(x
1
) is not close enough
to
0; then repeat the calculation with
x
1 in place of
x
0
; and so forth until a satisfactory
value of
x
=
x
n is reached. How does this relate to a two variable problem? We
illustrate the basic idea in two variables.
EXAMPLE 2.5.11. Describe concisely an algorithm analogous to Newtonâ€™s method in
one variable to solve the two variable problem
Newtonâ€™s
Method for
Systems
x
2
+
y
2
+
sin(xy
)
=
1
xe
x+y
 y
sin(x
+
y
)
=
0
SOLUTION. Our problem can be written as a system of two (nonlinear) equations in
two unknowns, namely
f
(x;
y
)
=
x
2
+
y
2
+
sin(xy
)
 1
=
0
g
(x;
y
)
=
xe
x+y
 y
sin(x
+
y
)
=
0
Now we can pull the same trick with differentials as in the one variable problem by
setting
dx
=
x
1
 x
0
;
dy
=
y
1
 y
0
; where
f
(x
1
;
y
1
)
=
0, approximating the change in
both
f and
g by differentials, and recalling the deï¬nition of these differentials in terms
of partial derivatives. This leads to a system
f
x
(x
0
;
y
0
)
dx
+
f
y
(x
0
;
y
0
)
dy
=
 f
((x
0
;
y
0
)
g
x
(x
0
;
y
0
)
dx
+
g
y
(x
0
;
y
0
)
dy
=
 g
((x
0
;
y
0
)

94
2. MATRIX ALGEBRA
Next, write everything in vector style, say
F(x)
=

f
(
x)
g
(
x)

;
x
(0)
=

x
0
y
0

;
x
(1)
=

x
1
y
1

Now we can write the vector differentials in the forms
dF
=

d
f
dg

;
and
dx
=

dx
dy

=

x
1
 x
0
y
1
 x
0

=
x
(1)
 x
(0)
The original Newton equations now look like a matrix multiplication involving
d
x;
F
and a matrix of derivatives of
F, namely the so-called Jacobian matrix
J
F
(x
0
;
y
0
)
=

f
x
(x
0
;
y
0
)
f
y
(x
0
;
y
0
)
g
x
(x
0
;
y
0
)
g
y
(x
0
;
y
0
)

Speciï¬cally, we see from the deï¬nition of matrix multiplication that the Newton equa-
tions are equivalent to the vector equations
dF
=
J
F
(x
0
)
d
x
=
 F(x
(0)
)
Thus we obtain that if the Jacobian matrix is invertible then
x
(1)
 x
(0)
=
d
x
=
 J
F
(x
(0)
)
 1
F(x
(0)
)
whence by adding
x
0 to both sides we see that
x
(1)
=
x
(0)
 J
F
(x
(0)
)
 1
F(x
(0)
)
Now replace
1 by
n
+
1 and
0 by
n to obtain the ever famous Newton formula in vector
form:
Newtonâ€™s
Formula in
Vector Form
x
(n+1)
=
x
(n)
 J
F
(
x
(n)
)
 1
F(
x
(n)
)
This is a beautiful analogy to the Newton formula of (2.5.1) which would not have been
possible without the language of vectors and matrices.
2.5 Exercises
1. Find the inverse of the following matrices, or show that it does not exist:
(a)
2
4
1
 2
1
0
2
0
 1
0
1
3
5 (b)
2
4
1
1
1
0
1
1
 1
0
1
3
5 (c)
2
4
2
 2
1
0
2
0
2
0
1
3
5
(d)
2
6
6
4
2
1
0
0
0
1
 2
1
0
0
2
0
0
0
0
1
3
7
7
5 (e)

1
2
+
i
i
2

(f)

1
a
a
1

2. Given the matrix
A and vector
b, ï¬nd the inverse of the matrix
A and use this to
solve the system
Ax
=
b; where
(a)A
=
2
4
1
 2
1
0
2
0
 1
0
1
3
5
;
b
=
2
4
3
0
1
3
5 (b)A
=
2
6
6
4
1
0
1
0
0
1
0
1
0
0
1
0
0
0
0
1
3
7
7
5
;
b
=
2
6
6
4
 1
0
1
0
3
7
7
5

2.5. MATRIX INVERSES
95
3. Solve the following systems by inverting the coefï¬cient matrix of the system.
(a)
2x
+
3y
=
7
(b)
3x
1
+
6x
2
 x
3
=
 4
(c)
x
1
+
x
2
=
 2
x
+
2y
=
 2
 2x
1
+
x
2
+
x
3
=
3
5x
1
+
2x
2
=
5
x
3
=
1
4. Find
2

2 matrices
A;
B and
C such that
AB
=
C
A but
B
6=
C
:
5. Find
A
 1
B if
A
=
2
4
1
2
 3
0
 1
1
2
5
 6
3
5 and
C
=
2
4
1
0
0
2
0
 1
1
1
2
0
 6
0
3
5
:
6. Determine the inverses for the following matrices in terms of the parameter
c and
conditions on
c for which the matrix has an inverse.
(a)

1
2
c
 1

(b)
2
4
1
2
c
0
1
1
0
0
1
3
5
(c)
2
6
6
4
1
0
0
1
0
 1
0
0
0
0
 6
0
0
0
0
c
3
7
7
5
7. Prove from the deï¬nition that if a square matrix
A satisï¬es the equation
A
3
 2A
+
3I
=
0, then the matrix
A must be invertible.
8. Express the following matrices and their inverses in the notation of elementary ma-
trices.
(a)
2
4
1
0
0
3
1
0
0
0
1
3
5 (b)

1
0
0
 2

(c)
2
4
0
0
1
0
1
0
1
0
0
3
5 (d)
2
4
1
 2
0
0
1
0
0
0
1
3
5
9. Show directly from the deï¬nition of inverse that the two by two inverse formula
gives the inverse of a
2

2 matrix.
10. Assume that the product of invertible matrices is invertible and deduce that if
A
and
B are invertible matrices of the same size and both
B and
AB are invertible, then
so is
A:
11. Let
A be an invertible matrix.
(a) Show that if the product of matrices
AB is deï¬ned, then
rank
(AB
)
=
rank
(B
):
(b) Show that if
B
A is deï¬ned, then
rank
(B
A)
=
rank
(B
):
12. Suppose the matrix
M
=

A
B
0
C

; where the blocks
A and
C are invertible
matrices. Find a formula for
M
 1 in terms of
A;
B
;
C
: Hint:
Assume
M
 1 has the
same form as
M and solve for the blocks in
M using
M
M
 1
=
I
:
13. Verify that for any square matrix
N and positive integer
k that
(I
+
N
+
N
2
+
:
:
:
+
N
k
 1
)(I
 N
)
=
I
 N
k
:
14. Use the Exercise 13 to ï¬nd a formula for the inverse of the matrix
I
 N
; where
N
is nilpotent, i.e.,
N
k
=
0 for some positive
k
: Test this formula on the matrices
(a)
2
4
1
 1
2
0
1
1
0
0
1
3
5
(b)

1
0
 2
1

(c)
2
4
1
0
0
0
1
0
1
0
1
3
5

96
2. MATRIX ALGEBRA
15. Use a calculator to apply the Newton formula (Equation (2.5.1)) to the one variable
problem
x
2
+
sin
2
(x)
=
1
starting with
x
0
=
1=2 and performing three iterations. Remember to ï¬rst rewrite the
equation in the form
f
(x)
=
0: What is the value of
f
(x
3
)?
16. Solve the nonlinear system of equations of Example 2.5.11 by using four iterations
of the vector Newton formula (2.5), starting with the initial guess
x
0
=
(1=2;
1=2):
How small is
F
(x
4
)? You will need a reasonably competent calculator or a computer
algebra program to do this exercise.
17. Find the minimum value of the function
F
(x;
y
)
=
(x
2
+
y
+
1)
2
+
x
4
+
y
4 by using
the Newton method to ï¬nd critical points of the function
F
(x;
y
); i.e., points where the
system
f
(x;
y
)
=
F
x
(x;
y
)
=
0 and
g
(x;
y
)
=
F
y
(x;
y
)
=
0:
18. Show that if the product of matrices
B
A is deï¬ned and
A is invertible, then
rank
(B
A)
=
rank
(B
)
19. Show that if
D is an
nn diagonal matrix with nonzero diagonal entries

1
;

2
;
:
:
:
;

n
;
then the inverse of
D is the
nn diagonal matrix with diagonal entries
1=
1
;
1=
2
;
:
:
:
;
1=
n
:
2.6. Basic Properties of Determinants
What are they?
Many students have already had some experience with determinants and may have used
them in high school to solve square systems of equations. Why have we waited until
now to introduce them? In point of fact, they are not really the best tool for solving
systems. That distinction goes to Gaussian elimination. Were it not for the theoretical
usefulness of determinants they might be consigned to a footnote in introductory linear
algebra texts as an historical artifact of linear algebra.
To motivate determinants, consider Example 2.5.8. Something remarkable happened in
that example. Not only were we able to ï¬nd a formula for the inverse of a
2

2 matrix
A
=

a
b
c
d

, but we were able to compute a single number
D
=
ad
 bc that told us
whether
A was invertible or not. The condition of non-invertibility, namely that
D
=
0,
has a very simple interpretation: this happens exactly when one row of
A is a multiple
of the other, since the example showed that this is when elementary operations use the
ï¬rst row to zero out the second row. Can we extend this idea? Is there a single number
that will tell us whether or not there are dependencies among the rows of the square

2.6. BASIC PROPERTIES OF DETERMINANTS
97
matrix
A that cause its rank to be smaller than its row size? The answer is yes. This
is exactly what determinants were invented for. The concept of determinant is subtle
and not intuitive, and researchers had to accumulate a large body of experience before
they were able to formulate a â€œcorrectâ€ deï¬nition for this number. There are alternate
deï¬nitions of determinants, but the following will suit our purposes. It is sometimes
referred to as â€œexpansion down the ï¬rst column.â€
DEFINITION 2.6.1. The determinant of a square matrix
n

n matrix
A
=
[a
ij
] is
the scalar quantity
det
A deï¬ned recursively as follows: if
n
=
1 then
det
A
=
a
11;
otherwise, we suppose that determinants are deï¬ned for all square matrices of size less
than
n and specify that
det
A
=
n
X
k
=1
a
k
1
( 1)
k
+1
M
k
1
(A)
=
a
11
M
11
(A)
 a
21
M
21
(A)
+
:
:
:
+
( 1)
n+1
a
n1
M
n1
(A)
where
M
ij
(A) is the determinant of the
(n
 1)

(n
 1) matrix obtained from
A by
deleting the
ith row and
jth column of
A:
Caution: The determinant of a matrix
A is a scalar number. It is not a matrix quantity.
EXAMPLE 2.6.2. Describe the quantities
M
21
(A) and
M
22
(A) where
A
=
2
4
2
1
0
1
1
 1
0
1
2
3
5
:
SOLUTION. If we erase the second row and ï¬rst column of
A we obtain something like
2
4
1
0
1
2
3
5
:
Now collapse the remaining entries together to obtain the matrix

1
0
1
2

:
Therefore
M
21
(A)
=
det

1
0
1
2

:
Similarly, erase the second row and column of
A to obtain
2
4
2
0
0
2
3
5
:
Now collapse the remaining entries together to obtain
M
22
(A)
=
det

2
0
0
2

:
Now how do we calculate these determinants? Part (b) of the next example answers the
question.

98
2. MATRIX ALGEBRA
EXAMPLE 2.6.3. Use the deï¬nition to compute the determinants of the following ma-
trices:
(a)
[ 4]
(b)

a
b
c
d

(c )
2
4
2
1
0
1
1
 1
0
1
2
3
5
SOLUTION. (a) From the ï¬rst part of the deï¬nition we see that
det
[ 4]
=
 4
For (b) we set
A
=

a
b
c
d

=

a
11
a
12
a
21
a
22

and use the formula of the deï¬nition to
obtain that
det

a
b
c
d

=
a
11
M
11
(A)
 a
21
M
21
(A)
=
a
det[d]
 c
det[b]
=
ad
 cb
This calculation gives a handy formula for the determinant of a
2

2 matrix. For (c)
use the deï¬nition to obtain that
det
2
4
2
1
0
1
1
 1
0
1
2
3
5
=
2
det

1
 1
1
2

 1
det

1
0
1
2

+
0
det

1
0
1
 1

=
2(1

2
 1

( 1))
 1(1

2
 1

0)
+
0(1

( 1)
 1

0)
=
2

3
 1

2
+
0

( 1)
=
4
A point worth observing here is that we didnâ€™t really have to calculate the determinant
of any matrix if it is multiplied by a zero. Hence, the more zeros our matrix has, the
easier we expect the determinant calculation to be!
NOTATION 2.6.4. Another common symbol for
det
A is
jAj, which is also written with
respect to the elements of
A by suppressing matrix brackets:
det
A
=
jAj
=









a
11
a
12



a
1n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









This notation invites a certain oddity, if not abuse, of language: we sometimes refer
to things like the â€œsecond rowâ€ or â€œ(2;
3)th elementâ€ or the â€œsizeâ€ of the determinant.
Yet the determinant is only a number and doesnâ€™t really have rows or entries or a size.
Rather, it is the underlying matrix whose determinant is being calculated that has these
properties. So be careful of this notation; we plan to use it frequently because itâ€™s
handy, but you should bear in mind that determinants and matrices are not the same
thing! Another reason that this notation can be tricky is the case of a one dimensional
matrix, say
A
=
[a
11
]: Here it is deï¬nitely not a good idea to forget the brackets, since
we already understand
ja
11
j to be the absolute value of the scalar
a
11
; a nonnegative

2.6. BASIC PROPERTIES OF DETERMINANTS
99
number. In the
1

1 case use
j[a
11
]j for the determinant, which is just the number
a
11
and may be positive or negative.
NOTATION 2.6.5. The number
M
ij
(A) is called the
(i;
j
)th minor of the matrix
A:
Minors and
Cofactors
If we collect the sign term in the deï¬nition of determinant together with the minor we
obtain the
(i;
j
)th cofactor
A
ij
=
( 1)
i+j
M
ij
(A) of the matrix
A: In the terminology
of cofactors,
det
A
=
n
X
k
=1
a
k
1
A
k
1
Laws of Determinants
Our primary goal here is to show that determinants have the magical property we
promised: a matrix is singular exactly when its determinant is
0: Along the way we
will examine some useful properties of determinants. There is a lot of clever algebra
that can be done here; we will try to keep matters straightforward (if thatâ€™s possible with
determinants). In order to focus on the main ideas, we will place most of the proofs of
key facts at the end of the next section for optional reading. Also, a concise summary of
the basic determinantal laws is given at the end of this section. Unless otherwise stated,
we assume throughout this section that matrices are square, and that
A
=
[a
ij
] is an
n

n matrix.
For starters, letâ€™s observe that itâ€™s very easy to calculate the determinant of upper trian-
gular matrices. Let
A be such a matrix. Then
a
k
1
=
0 if
k
>
1, so
det
A
=









a
11
a
12



a
1n
0
a
22



a
2n
...
...
...
0
0



a
nn









=
a
11









a
22
a
23



a
2n
0
a
33



a
3n
...
...
...
0
0



a
nn









=



=
a
11

a
22

:
:
:

a
nn
Hence we have established our ï¬rst determinantal law:
D1: If
A is an upper triangular matrix, then the determinant of
A is the product of all
the diagonal elements of
A:
EXAMPLE 2.6.6. Compute
D
=








4
4
1
1
0
 1
2
3
0
0
2
3
0
0
0
2








and
jI
n
j
=
det
I
n
:
SOLUTION. By D1 we can do this at a glance:
D
=
4

( 1)

2

2
=
 16: Since
I
n
is diagonal, it is certainly upper triangular. Moreover, the entries down the diagonal of
this matrix are
1â€™s, so D1 implies that
jI
n
j
=
1:
Next, suppose that we notice a common factor of the scalar
c in a row, say for conve-
nience, the ï¬rst one. How does this affect the determinantal calculation? In the case
of a
1

1 determinant, we could simply factor it out of the original determinant. The
general situation is covered by this law:

100
2. MATRIX ALGEBRA
D2: If
B is obtained from
A by multiplying one row of
A by the scalar
c, then
det
B
=
c

det
A:
Here is a simple illustration:
EXAMPLE 2.6.7. Compute
D
=






5
0
10
5
5
5
0
0
2






:
SOLUTION. Put another way, D2 says that scalars may be factored out of individual
rows of a determinant. So use D2 on the ï¬rst and second rows and then use deï¬nition
of determinant to obtain






5
0
10
5
5
5
0
0
2






=
5







1
0
2
5
5
5
0
0
2






=
5

5







1
0
2
1
1
1
0
0
2






=
25


1





1
1
0
2




 1





0
2
0
2




+
0





0
2
1
1





=
50
One can easily check that this is the same answer we get by working the determinant
directly from deï¬nition.
Next, suppose we interchange two rows of a determinant. Then we have the following:
D3: If
B is obtained from
A by interchanging two rows of
A , then
det
B
=
 det
A:
EXAMPLE 2.6.8. Use D3 to show the following handy fact: if a determinant has a
repeated row, then it must be
0:
SOLUTION. Suppose that the
ith and
jth rows of the matrix
A are identical, and
B
is obtained by switching these two rows of
A: Clearly
B
=
A: Yet, according to D3,
det
B
=
 det
A: It follows that
det
A
=
 det
A, i.e., if we add
det
A to both sides,
2

det
A
=
0, so that
det
A
=
0, which is what we wanted to show.
Now we ask what happens to a determinant if we add a multiple of one row to another.
The answer is as follows.
D4: If
B is obtained from
A by adding a multiple of one row of
A to another row of
A,
then
det
B
=
det
A:
EXAMPLE 2.6.9. Compute
D
=








1
4
1
1
1
 1
2
3
0
0
2
3
0
0
1
2








:
SOLUTION. What D4 really says is that any elementary row operation
E
ij
(c) can be
applied to the matrix behind a determinant and the determinant will be unchanged. So
in this case, add
 1 times the ï¬rst row to the second and
 1=2 times the third row to
the fourth, then apply D1 to obtain








1
4
1
1
1
 1
2
3
0
0
2
3
0
0
1
2








=








1
4
1
1
0
 5
1
2
0
0
2
3
0
0
0
1=2








=
1

( 5)

2

1
2
=
 5

2.6. BASIC PROPERTIES OF DETERMINANTS
101
EXAMPLE 2.6.10. Use D3 to show that a matrix with a row of zeros has zero determi-
nant.
SOLUTION. Suppose
A has a row of zeros. Add any other row of the matrix
A to this
zero row to obtain a matrix
B with repeated rows.
We now have enough machinery to establish the most important property of determi-
nants. First of all, we can restate laws D2-D4 in the language of elementary matrices as
follows:
 D2:
det(E
i
(c)A)
=
c

det
A (remember that for
E
i
(c) to be an elementary
Determinant of
Elementary
Matrices
matrix,
c
6=
0:)
 D3:
det(E
ij
A)
=
 det
A
 D4:
det(E
ij
(s)A)
=
det
A
Apply a sequence of elementary row operations on the
n

n matrix
A to reduce it to
its reduced row echelon form
R, or equivalently, multiply
A on the left by elementary
matrices
E
1
;
E
2
;
:
:
:
;
E
k and obtain
R
=
E
1
E
2
:
:
:
E
k
A
Take determinant of both sides to obtain
det
R
=
det
(E
1
E
2
:
:
:
E
k
A)
=
(nonzeroconstant)

det
A
Therefore,
det
A
=
0 precisely when
det
R
=
0: Now the reduced row echelon form
of
A is certainly upper triangular. In fact, it is guaranteed to have zeros on the diagonal,
and therefore have zero determinant by D1, unless
rank
A
=
n, in which case
R
=
I
n
:
According to Theorem 2.5.9 this happens precisely when
A is invertible. Thus we have
shown:
D5: The matrix
A is invertible if and only if
det
A
6=
0:
EXAMPLE 2.6.11. Determine if the following matrices are invertible or not without
actually ï¬nding the inverse:
(a )
2
4
2
1
0
1
1
 1
0
1
2
3
5
(b )
2
4
2
1
0
1
1
 1
0
 1
2
3
5
SOLUTION. Compute determinants:






2
1
0
1
1
 1
0
1
2






=
2




1
 1
1
2




 1




1
0
1
2




=
2

3
 2
=
4
and






2
1
0
1
1
 1
0
 1
2






=
2




1
 1
 1
2




 1




1
0
 1
2




=
2

1
 1

2
=
0
Hence by D5 matrix (a) is invertible and matrix (b) is not invertible.

102
2. MATRIX ALGEBRA
There are two more surprising properties of determinants that we now discuss. Their
proofs involve using determinantal properties of elementary matrices (see the next sec-
tion for details).
D6: Given matrices
A;
B of the same size,
det
AB
=
det
A
det
B
EXAMPLE 2.6.12. Verify D6 in the case that
A
=

1
0
1
1

and
B
=

2
1
0
1

.
How do
det(A
+
B
) and
det
A
+
det
B compare in this case?
SOLUTION. We have easily that
det
A
=
1 and
det
B
=
2: Therefore,
det
A
+
det
B
=
1
+
2
=
3, while
det
A

det
B
=
1

2
=
2: On the other hand
AB
=

1
0
2
1


2
1
0
1

=

2
1
4
3

A
+
B
=

1
0
1
1

+

2
1
0
1

=

3
1
1
2

so that
det
AB
=
2

3
 4

1
=
2
=
det
A

det
B, as expected. On the other hand we
have that
det(A
+
B
)
=
3

2
 1

1
=
5
6=
det
A
+
det
B
:
This example raises a very important point.
Caution: In general,
det
A
+
det
B
6=
det(A
+
B
), though there are occasional excep-
tions.
In other words, determinants do not distribute over sums. (It is true, however, that the
determinant is additive in one row at a time. See the proof of D4 for details.)
Finally, we ask how
det
A
T compares to
det
A: Try a simple case like the
2

2 and
we discover that there seems to be no difference in determinant. This is exactly what
happens in general.
D7: For all square matrices
A,
det
A
T
=
det
A:
EXAMPLE 2.6.13. Compute
D
=








4
0
0
0
4
1
0
0
1
2
 2
0
1
0
1
2








:
SOLUTION. By D7 and D1 we can do this at a glance:
D
=
4

1

( 2)

2
=
 16:
D7 is a very useful fact. Letâ€™s look at it from this point of view: transposing a matrix in-
terchanges the rows and columns of the matrix. Therefore, everything that we have said
about rows of determinants applies equally well to the columns, including the deï¬ni-
tion of determinant itself! Therefore, we could have given the deï¬nition of determinant
in terms of expanding across the ï¬rst row instead of down the ï¬rst column and gotten
the same answers. Likewise, we could perform elementary column operations instead
of row operations and get the same results as D2-D4. Furthermore, the determinant
of a lower triangular matrix is the product of its diagonal elements thanks to D7+D1.
By interchanging rows or columns then expanding by ï¬rst row or column, we see that
the same effect is obtained by simply expanding the determinant down any column or

2.6. BASIC PROPERTIES OF DETERMINANTS
103
across any row. We have to alternate signs starting with the sign
( 1)
i+j of the ï¬rst
term we use.
Now we can really put it all together and compute determinants to our heartâ€™s content
with a good deal less effort than the original deï¬nition speciï¬ed. We can use D1-D4
in particular to make a determinant calculation no worse than Gaussian elimination in
the amount of work we have to do. We simply reduce a matrix to triangular form by
elementary operations, then take the product of the diagonal terms.
EXAMPLE 2.6.14. Calculate
D
=








3
0
6
6
1
0
2
1
2
0
0
1
 1
2
0
0








:
SOLUTION. We are going to do this calculation two ways. We may as well use the
same elementary operation notation that we have employed in Gaussian elimination.
The only difference is that we have equality instead of arrows, provided that we modify
the value of the new determinant in accordance with the laws D1-D3. So here is the
straightforward method:
D
=
3








1
0
2
2
1
0
2
1
2
0
0
1
 1
2
0
0








=
E
21
( 1)
E
31
( 2)
E
41
(1)
3








1
0
2
2
0
0
0
 1
0
0
 4
 3
0
2
2
2








=
E
24
 3








1
0
2
2
0
2
2
2
0
0
 4
 3
0
0
0
 1








=
 24
Here is another approach: letâ€™s expand the determinant down the second column, since it
is mostly
0â€™s. Remember that the sign in front of the ï¬rst minor must be
( 1)
1+2
=
 1:
Also, the coefï¬cients of the ï¬rst three minors are
0, so we need only write down the last
one in the second column:
D
=
+2






3
6
6
1
2
1
2
0
1






Expand down the second column again:
D
=
2

 6




1
1
2
1




+
2




3
6
2
1





:
=
2( 6

( 1)
+
2

( 9))
=
 24
Summary of Determinantal Laws
Now that our list of the basic laws of determinants is complete, we record them in a
concise format which includes two laws (D7 and D8) to be discussed in the next section.

104
2. MATRIX ALGEBRA
Laws of Determinants. Let
A;
B be
n

n matrices. Then
D1: If
A is an upper triangular matrix, then
det
A is the product of all the
diagonal elements of
A:
D2:
det
(E
i
(c)A)
=
c

det
A (here
c
6=
0:)
D3:
det
(E
ij
A)
=
 det
A:
D4:
det
(E
ij
(s)A)
=
det
A:
D5: The matrix
A is invertible if and only if
det
A
6=
0:
D6:
det
AB
=
det
A
det
B
:
D7:
det
A
T
=
det
A:
D8:
A
adj
A
=
(adj
A)A
=
(det
A)I
:
D9: If
det
A
6=
0, then
A
 1
=
1
det
A
adj
A:
2.6 Exercises
1. Compute all minors and cofactors for these matrices:
(a)
2
4
1
1
 1
1
0
1
3
0
1
3
5
(b)

2
1
0
1

(c)

1
1
 i
i
0

2. Compute these determinants and determine which of these matrices whose are in-
vertible.
(a)




2
 1
1
1



 (b)






1
 1
0
0
1
1
0
0
1
+
i






(c)






1
1
0
1
0
1
2
0
1






(d)








1
 1
4
2
0
1
0
3
0
0
2
7
 2
3
4
6








(e)








1
 1
4
2
0
1
0
3
0
0
2
7
 2
3
4
6








(f)








1
1
0
1
1
2
1
1
0
0
1
3
1
1
2
1








(g)








1
1
0
1
1
2
1
1
0
0
1
3
0
0
2
0








(h)




cos

sin

 sin

cos





3. Verify the determinants laws D6 and D7 for the following matrices:
A
=
2
4
 2
1
0
1
2
1
0
0
1
3
5
B
=
2
4
1
0
1
1
2
0
 1
0
1
3
5
4. Verify that








a
b
0
0
c
d
0
0
0
0
e
f
0
0
g
h








=




a
b
c
d








e
f
g
h




:
5. Use determinants to ï¬nd conditions on the parameters in these matrices under which
the matrices are invertible.
(a)

a
1
ab
1

(b)
2
4
1
1
 1
1
c
1
0
0
1
3
5
(c)
2
4

 1
0
0
1

 2
1
3
1

 1
3
5

2.6. BASIC PROPERTIES OF DETERMINANTS
105
(d)
I
2
 
0
1
 c
0
 c
1

6. Let
V
=
2
4
1
x
0
x
2
0
1
x
1
x
2
1
1
x
2
x
2
2
3
5
(Such a matrix is called a Vandermonde matrix.) ) Express
det
V as a product of factors
(x
j
 x
k
): Hint: Use elementary operations to clear the ï¬rst column and factor out as
many
(x
j
 x
k
) as possible in the resulting determinant.
7. Use the determinantal law D6 to show that
det
A
det
A
 1
=
1 if
A is invertible.
8. Show by example that
det
A
H
6=
det
A and prove that in general
det
A
H
=
det
A:
9. Use the determinantal laws to show that any matrix with a row of zeros has zero
determinant.
10. If
A is a
5

5 matrix, then in terms of
det(A), what can we say about
det
( 2A)?
det
(A
 1
)? Explain.
11. Show that if
M
=

A
B
0
C

then
det
M
=
det
A

det
C
: Hint: Use row operations to make the diagonal submatri-
ces triangular.
12. Prove that if
A is
n

n, then
det( A)
=
( 1)
n
det
A:
13. Let
A be a skew-symmetric matrix, that is,
A
T
=
 A: Show that
A must be
singular.
14. Let
J
n be the
n

n counteridentity , that is,
J
n is a square matrix with ones along
the counterdiagonal (the diagonal that starts in the lower left corner and ends in the
upper right corner), and zeros elsewhere.
(a) Prove that
J
2
n
=
I
n.
(b) Prove that
J
T
n
=
J
n
:
(c) Find a formula for
det
J
n
:
15. Show that the companion matrix of the polynomial
f
(x)
=
c
0
+c
1
x+


c
n 1
x
n 1
+
x
n, that is,
2
6
6
6
6
6
4
0
1
0



0
0
0
1



0
...
...
...
...
...
0
0



0
1
 c
0
 c
1



 c
n 2
 c
n 1
3
7
7
7
7
7
5
is invertible if and only if
c
0
6=
0:

106
2. MATRIX ALGEBRA
2.7. *Applications and Proofs for Determinants
There are two fundamental applications of determinants that we develop in this section.
The ï¬rst is the derivation of an explicit formula for the inverse of a matrix in terms of
its coefï¬cients and determinant, which extends the
2

2 Example 2.5.8 to matrices of
all sizes. From this example, one might wonder if a similar calculation could be done
for any matrix. We will see that it can. The second application is something that many
have already seen in high school algebra, at least for
2

2 and
3

3 systems: Cramerâ€™s
Rule gives a way of solving square systems, provided that they have a unique solution.
An Inverse Formula
Let
A
=
[a
ij
] be
n

n: We have already seen that we can expand the determinant of
A down any column of
A (see the discussion following Example 2.6.13). These lead to
cofactor formulas for each column number
j:
det
A
=
n
X
k
=1
a
k
j
A
k
j
=
n
X
k
=1
A
k
j
a
k
j
This formula resembles a matrix multiplication formula. Consider the slightly altered
sum
n
X
k
=1
A
k
i
a
k
j
=
A
1i
a
1j
+
A
2i
a
2j
+
:
:
:
+
A
ni
a
nj
The key to understanding this expression is to realize that it is exactly what we would
get if we replaced the
ith column of the matrix
A by its
jth column and then computed
the determinant of the resulting matrix by expansion down the
ith column. But such a
matrix has two equal columns and therefore has a zero determinant, which we can see
by applying Example 2.6.8 to the transpose of the matrix and using D7. So this sum
must be
0 if
i
6=
j: We can combine these two sums by means of the Kronecker delta in
the formula
n
X
k
=1
A
k
i
a
k
j
=
Ã†
ij
det
A
In order to exploit this formula we make the following deï¬nitions:
DEFINITION 2.7.1. The matrix of minors of the
n

n matrix
A
=
[a
ij
] is the matrix
Minor and
Cofactor
Matrices
M
(A)
=
[M
ij
(A)] of the same size. The matrix of cofactors of
A is the matrix
A
cof
=
[A
ij
] of the same size. Finally, the adjoint matrix of
A is the matrix
adj
A
=
A
T
cof
:
EXAMPLE 2.7.2. Compute the determinant, minors, cofactors and adjoint matrices for
A
=
2
4
1
2
0
0
0
 1
0
2
1
3
5 and compute
A
adj
A:

2.7. *APPLICATIONS AND PROOFS FOR DETERMINANTS
107
SOLUTION. The determinant is easily seen to be
2: Now for the matrix of minors:
M
(A)
=
2
6
6
6
6
6
6
4




0
 1
2
1








0
 1
0
1








0
0
0
2








2
0
2
1








1
0
0
1








1
2
0
2








2
0
0
 1








1
0
0
 1








1
2
0
0




3
7
7
7
7
7
7
5
=
2
4
2
0
0
2
1
2
 2
 1
0
3
5
To get the matrix of cofactors, simply overlay
M
(A) with the following â€œcheckerboardâ€
of
+= â€™s
2
4
+
 +
 +
 +
 +
3
5
to obtain
A
cof
=
2
4
2
0
0
 2
1
 2
 2
1
0
3
5
Now transpose to get
adj
A
=
2
4
2
 2
 2
0
1
1
0
 2
0
3
5
We check that
(adj
A)A
=
2
4
2
 2
 2
0
1
1
0
 2
0
3
5
2
4
1
2
0
0
0
 1
0
2
1
3
5
=
2
4
2
0
0
0
2
0
0
0
2
3
5
=
(det
A)I
3
Of course, the example simply conï¬rms the formula that preceded it since this formula
gives the
(i;
j
)th entry of the product
(adj
A)A: If we were to do determinants by row
expansions, we would get a similar formula for the
(i;
j
)th entry of
A
adj
A: We sum-
marize this information in matrix notation as the determinantal property
D8: For a square matrix
A;
Adjoint Formula
A
adj
A
=
(adj
A)A
=
(det
A)I
What does this have to do with inverses? We already know that
A is invertible exactly
when
det
A
6=
0, so the answer is staring at us! Just divide the terms in D8 by
det
A to
obtain an explicit formula for
A
 1:
D9:
For a square matrix
A such that
det
A
6=
0,
Inverse Formula
A
 1
=
1
det
A
adj
A

108
2. MATRIX ALGEBRA
EXAMPLE 2.7.3. Compute the inverse of the matrix
A of Example 2.7.2 by the Inverse
Formula.
SOLUTION. We already computed the adjoint matrix of
A; and the determinant of
A is
just
2, so we have that
A
 1
=
1
det
A
adj
A
=
1
2
2
4
2
 2
 2
0
1
1
0
 2
0
3
5
EXAMPLE 2.7.4. Interpret the Inverse Formula in the case of the
2

2 matrix
A
=

a
b
c
d

:
SOLUTION. In this case we have
M
(A)
=

d
c
b
a

A
cof
=

d
 c
 b
a

adj
A
=

d
 b
 c
a

so that the Inverse Formula becomes
A
 1
=
1
det
A

d
 b
 c
a

As you might expect, this is exactly the same as the formula we obtained in Exam-
ple 2.5.8.
Cramerâ€™s Rule
Thanks to the Inverse formula, we can now ï¬nd an explicit formula for solving linear
systems with a nonsingular coefï¬cient matrix. Hereâ€™s how we proceed. To solve
Ax
=
b we multiply both sides on the left by
A
 1 to obtain that
x
=
A
 1
b: Now use the
Inverse formula to obtain
x
=
A
 1
b
=
1
det
A
adj(A)b
The explicit formula for the
ith coordinate of
x that comes from this fact is
x
i
=
1
det
A
n
X
j
=1
A
j
i
b
j
The summation term is exactly what we would obtain if we started with the determinant
of the matrix
B
i obtained from
A by replacing the
ith column of
A by
b and then
expanding the determinant down the
ith column. Therefore, we have arrived at the
following rule:

2.7. *APPLICATIONS AND PROOFS FOR DETERMINANTS
109
THEOREM 2.7.5. Let
A be an invertible
n

n matrix and
b an
n

1 column vector.
Denote by
B
i the matrix obtained from
A by replacing the
ith column of
A by
b: Then
the linear system
Ax
=
b has unique solution
x
=
(x
1
;
x
2
;
:
:
:
;
x
n
) where
x
i
=
det
B
i
det
A
;
i
=
1;
2;
:
:
:
;
n
EXAMPLE 2.7.6. Use Cramerâ€™s rule to solve the system
Cramerâ€™s Rule
2x
1
 x
2
=
1
4x
1
+
4x
2
=
20
SOLUTION. The coefï¬cient matrix and right hand side vectors are
A
=

2
 1
4
4

and
b
=

1
20

so that
det
A
=
8
 ( 4)
=
12
and
x
1
=




1
 1
20
4








2
 1
4
4




=
24
12
=
2
x
2
=




2
1
4
20








2
 1
4
4




=
36
12
=
3
The truth of the matter is that Cramerâ€™s Rule and adjoints are only good for small ma-
Computational
Efï¬ciency of
Determinants
trices and theoretical arguments. For if you evaluate determinants in a straightforward
way from the deï¬nition, the work in doing so is about
2n! ï¬‚ops for an
n

n system.
(Recall that a â€œï¬‚opâ€ in numerical linear algebra is a single addition or subtraction, or
multiplication or division. For example, it is not hard to show that the operation of
adding a multiple of one row vector of length
n to another requires
2n ï¬‚ops. This
number
2n! is vast when compared to the number
2n
3
=3 ï¬‚ops required for Gaussian
elimination, even with â€œsmallâ€
n, say
n
=
10: In this case we have
2

10
3
=3

667,
while
2

10!
=
7;
527;
600:
On the other hand, there is a clever way to evaluate determinants that is much less
work than the deï¬nition: use elementary row operations together with D2, D6 and the
elementary operations that correspond to these rules to reduce the determinant to that
of a triangular matrix. This will only require about
2n
3
=3 ï¬‚ops. As a matter of fact,
it is tantamount to Gaussian elimination. But to use Cramerâ€™s Rule, you will have to
calculate
n
+
1 determinants. So why bother with Cramerâ€™s Rule on larger problems
when it still will take about
n times as much work as Gaussian elimination? A similar
remark applies to computing adjoints instead of using Gauss-Jordan elimination on the
super-augmented matrix of
A:

110
2. MATRIX ALGEBRA
*Proofs of Some of the Laws of Determinants
D2: If
B is obtained from
A by multiplying one row of
A by the scalar
c, then
det
B
=
c

det
A:
To keep the notation simple, assume the ï¬rst row is multiplied by
c; the proof being
similar for other rows. Suppose we have established this for all determinants of size less
than
n (this is really another â€œproof by inductionâ€, which is how most of the following
determinantal properties are established). For an
n

n determinant we have
det
B
=









c

a
11
c

a
12



c

a
1n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









=
c

a
11









a
22
a
23



a
2n
a
32
a
33



a
3n
...
...
...
a
n2
a
n3



a
nn









+
n
X
k
=2
a
k
1
( 1)
k
+1
M
k
1
(B
)
But the minors
M
k
1
(B
) all are smaller and have a common factor of
c in the ï¬rst row.
Pull this factor out of every remaining term and we get that









c

a
11
c

a
12



c

a
1n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









=
c










a
11
a
12



a
1n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









Thus we have shown that property D2 holds for all matrices.
D3: If
B is obtained from
A by interchanging two rows of
A , then
det
B
=
 det
A:
To keep the notation simple, assume we switch the ï¬rst and second rows. In the case
of a
2

2 determinant, we get the negative of the original determinant (check this for
yourself). Suppose we have established the same is true for all matrices of size less than
n: For an
n

n determinant we have
det
B
=









a
21
a
22



a
2n
a
11
a
12



a
1n
...
...
...
a
n1
a
n2



a
nn









=
a
21
M
11
(B
)
 a
12
M
21
(B
)
+
n
X
k
=3
a
k
1
( 1)
k
+1
M
k
1
(B
)
=
a
21
M
21
(A)
 a
12
M
11
(A)
+
n
X
k
=3
a
k
1
( 1)
k
+1
M
k
1
(B
)
But all the determinants in the summation sign come from a submatrix of
A with the ï¬rst
and second row interchanged. Since they are smaller than
n, they are just the negative
of the corresponding minor of
A: Notice that the ï¬rst two terms are just the ï¬rst two
terms in the determinantal expansion of
A, except that they are out of order and have

2.7. *APPLICATIONS AND PROOFS FOR DETERMINANTS
111
an extra minus sign. Factor this minus sign out of every term and we have obtained
D3.
D4: If
B is obtained from
A by adding a multiple of one row of
A to another row of
A,
then
det
B
=
det
A:
Actually, itâ€™s a little easier to answer a slightly more general question: what happens
if we replace a row of a determinant by that row plus some other row vector
r (not
necessarily a row of the determinant)? Again, simply for convenience of notation, we
assume the row in question is the ï¬rst. The same argument works for any other row.
Some notation: let
B be the matrix which we obtain from the
n

n matrix
A by adding
the row vector
r
=
[r
1
;
r
2
;
:
:
:
;
r
n
] to the ï¬rst row and
C the matrix obtained from
A
by replacing the ï¬rst row by
r:The answer turns out to be that the
jB
j
=
jAj
+
jC
j: One
way of saying this is to say that the determinant function is â€œadditive in each row.â€ Letâ€™s
see what happens in the one dimensional case:
jB
j
=
j[a
11
+
r
1
]j
=
a
11
+
r
1
=
j[a
11
]j
+
j[r
1
]j
=
jAj
+
jC
j
Suppose we have established the same is true for all matrices of size less than
n and let
A be
n

n: Then the minors
M
k
1
(B
), with
k
>
1, are smaller than
n so the property
holds for them. Hence we have
det
B
=









a
11
+
r
1
a
12
+
r
2



a
1n
+
r
n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









=
(a
11
+
r
1
)M
11
(A)
+
n
X
k
=2
a
k
1
( 1)
k
+1
M
k
1
(B
)
=
(a
11
+
r
1
)M
11
(A)
+
n
X
k
=2
a
k
1
( 1)
k
+1
(M
k
1
(A)
+
M
k
1
(C
))
=
n
X
k
=1
a
k
1
( 1)
k
+1
M
k
1
(A)
+
r
1
M
11
(C
)
+
n
X
k
=2
a
k
1
( 1)
k
+1
M
k
1
(C
)
=
det
A
+
det
C
Now what about adding a multiple of one row to another in a determinant? For nota-
tional convenience, suppose we add
s times the second row to the ï¬rst. In the notation
of the previous paragraph,
det
B
=









a
11
+
s

a
21
a
12
+
s

a
22



a
1n
+
s

a
2n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









and
det
C
=









s

a
21
s

a
22



s

a
2n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









=
s










a
21
a
22



a
2n
a
21
a
22



a
2n
...
...
...
a
n1
a
n2



a
nn









=
0

112
2. MATRIX ALGEBRA
where we applied D2 to pull the common factor
s from the ï¬rst row and the result of
Example 2.6.8 to get the determinant with repeated rows to be
0: But
jB
j
=
jAj
+
jC
j:
Hence we have shown D4.
D6: Given matrices
A;
B of the same size,
det
AB
=
det
A
det
B
The key is that we now know that determinant calculation is intimately connected with
elementary matrices, rank and the reduced row echelon form. First letâ€™s reinterpret
D2-D4 still one more time. First of all take
A
=
I in the discussion of the previous
paragraph and we see that

det
E
i
(c)
=
c

det
E
ij
=
 1

det
E
ij
(s)
=
1
Therefore, D2-D4 can be restated (yet again) as
 D2:
det
(E
i
(c)A)
=
det
E
i
(c)

det
A (here
c
6=
0:)
 D3:
det
(E
ij
A)
=
det
E
ij

det
A
 D4:
det
(E
ij
(s)
=
det
E
ij
(s)

det
A
In summary: For any elementary matrix
E and arbitrary matrix
A of the same size,
det(E
A)
=
det(E
)
det
(A):
Now letâ€™s consider this question: how does
det
(AB
) relate to
det(A) and
det
(B
)? If
A is not invertible,
rank
A
<
n by Theorem 2.5.9 and so
rank
AB
<
n by Corol-
lary 2.4.19. Therefore,
det(AB
)
=
0
=
det
A

det
B in this case. Next suppose that
A
is invertible. Express it as a product of elementary matrices, say
A
=
E
1
E
2
:
:
:
E
k and
use our summary of D1-D3 to disassemble and reassemble the elementary factors:
det(AB
)
=
det(E
1
E
2
:
:
:
E
k
B
)
=
(det
E
1
det
E
2
:
:
:
det
E
k
)
det
B
=
det(E
1
E
2
:
:
:
E
k
)
det
B
=
det
A

det
B
Thus we have shown that D6 holds.
D7: For all square matrices
A,
det
A
T
=
det
A:
Recall these facts about elementary matrices:

det
E
T
ij
=
det
E
ij

det
E
i
(c)
T
=
det
E
i
(c)

det
E
ij
(c)
T
=
det
E
j
i
(c)
=
1
=
det
E
ij
(c)
Therefore, transposing does not affect determinants of elementary matrices. Now for the
general case observe that, since
A and
A
T are transposes of each other, one is invertible
if and only if the other is by the Transpose/Inverse law. In particular, if both are singular,
then
det
A
T
=
0
=
det
A: On the other hand, if both are nonsingular, then write
A as

2.7. *APPLICATIONS AND PROOFS FOR DETERMINANTS
113
a product of elementary matrices, say
A
=
E
1
E
2
:
:
:
E
k, and obtain from the product
law for transposes that
A
T
=
E
T
k
E
T
k
 1
:
:
:
E
T
1 , so by D6
det
A
T
=
det
E
T
k
det
E
T
k
 1
:
:
:
det
E
T
k
=
det
E
k
det
E
k
 1
:
:
:
det
E
1
=
det
E
1
det
E
2
:
:
:
det
E
k
=
det
A
2.7 Exercises
1. For each of the following matrices ï¬nd (1) the matrix of minors, (2) the matrix of
cofactors, (3) the adjoint matrix for each matrix, and (4) the product of matrix and its
adjoint.
(a)
2
4
2
1
0
 1
1
2
1
2
2
3
5 (b)
2
4
1
0
1
0
1
0
1
0
 1
3
5 (c)

1
3
 1
2
 i

(d)
2
4
 1
1
1
0
0
2
0
0
2
3
5
2. For each of the following matrices, ï¬nd the inverses in two ways: ï¬rst by superaug-
mented matrices, then by adjoints.
(a)
2
4
1
0
0
2
2
1
1
0
1
3
5
(b)

1
2
2
2

(c)
2
4
c
 s
0
s
c
0
0
0
1
3
5
3. Use Cramerâ€™s Rule to solve the following systems.
(a)
x
 3y
=
2
2x
+
y
=
11
(b)
2x
1
+
x
2
=
b
1
2x
1
 x
2
=
b
2
(c)
3x
1
+
x
3
=
2
2x
1
+
2x
2
=
1
x
1
+
x
2
+
x
3
=
6
4. Suppose we want to interpolate three points
(x
k
;
y
k
);
k
=
0;
1;
2: Write out the
system of equations that results from plugging these points into the equation of a qua-
dratic
y
=
c
0
+
c
1
x
+
c
2
x
2 and calculate the determinant of the coefï¬cient matrix.
When is this determinant
0? (This coefï¬cient matrix is an example of what is called a
Vandermonde matrix. )
5. Conï¬rm that the determinant of the matrix
A
=
2
4
1
0
2
2
1
1
1
0
1
3
5 is
 1: We can now
assert without any further calculation that the inverse matrix of
A has integer coefï¬-
cients, thanks to the adjoint formula. Explain.
6. Prove that if the matrix
A is invertible, then
adj(A
T
A
_
)
>
0:
7. Let
A and
B be invertible matrices of the same size. Prove the following.
(a)
adj
A
 1
=
(adj
A)
 1
(b)
adj(AB
)
=
adj
A
adj
B
Hint: Determinantal law D9 can be very helpful here.

114
2. MATRIX ALGEBRA
8. Suppose that the square matrix
A is singular. Prove that if the system
Ax
=
b is
consistent, then
(
adj
A)
b
=
0:
2.8. *Tensor Products
How do we solve a system of equations in which the unknowns can be organized into a
matrix
X and the linear system in question is of the form
AX
 X
B
=
C
(2.8.1)
where
A;
B
;
C are given matrices? We call this equation the Sylvester equation. Such
systems occur in a number of physical applications; for example, discretizing certain
partial differential equations in order to solve them numerically can lead to such a sys-
tem. We are going to examine a matrix method for systematically reorganizing the data
into a single column so that the resulting system looks like an ordinary linear system.
The basic idea needed here is that of the tensor product of two matrices, which is deï¬ned
as follows:
DEFINITION 2.8.1. Let
A
=
[a
ij
] be an
m

p matrix and
B
=
[b
ij
] an
n

q matrix.
Then the tensor product of
A and
B is the
mn

pq matrix which can be expressed in
block form as
A

B
=
2
6
6
6
6
6
6
6
6
4
a
11
B
a
12
B



a
1j
B



a
1n
B
a
21
B
a
22
B



a
2j
B



a
2n
B
...
...
...
...
a
i1
B
a
i2
B



a
ij
B



a
in
B
...
...
...
...
a
m1
B
a
m2
B



a
mj
B



a
mn
B
3
7
7
7
7
7
7
7
7
5
EXAMPLE 2.8.2. Let
A
=

1
3
2
1

;
B
=

4
 1

and describe the matrices
A

B,
B

A and
I
2

A explicitly.
SOLUTION. First we have from the deï¬nition that
A

B
=

1B
3B
2B
1B

=
2
6
6
4
4
12
 1
 3
8
4
 2
 1
3
7
7
5
and
B

A
=

4A
 1A

=
2
6
6
4
4
12
 8
 2
 1
 3
 2
 1
3
7
7
5

2.8. *TENSOR PRODUCTS
115
Similarly
I
2

A
=

1A
0A
0A
1A

=
2
6
6
4
1
3
0
0
2
1
0
0
0
0
1
3
0
0
2
1
3
7
7
5
We can think of the tensor product as a kind of matrix multiplication. One point that
comes out of Example 2.8.2 is that, even though
A

B and
B

A have the same size,
A

B
6=
B

A in general.
The other ingredient that we need to solve Equation 2.8.1 is an operator that turns ma-
trices into vectors which is deï¬ned as follows.
DEFINITION 2.8.3. Let
A be an
m

n matrix. Then the
mn

1 vector
v
ec
A is obtained
from
A by stacking the
n columns of
A vertically, with the ï¬rst column at the top and
the last column of
A at the bottom.
EXAMPLE 2.8.4. Let
A
=

1
3
2
2
1
4

: Compute
v
ec
A:
SOLUTION. There are three columns to stack, yielding
v
ec
A
=
2
6
6
6
6
6
6
4
1
2
3
1
2
4
3
7
7
7
7
7
7
5
Here are a few simple facts about tensor products that are more or less immediate from
the deï¬nition.
THEOREM 2.8.5. Let
A;
B
;
C
;
D be suitably sized matrices. Then
1.
(A
+
B
)

C
=
A

C
+
B

C
2.
A

(B
+
C
)
=
A

B
+
A

C
3.
(A

B
)

C
=
A

(B

C
)
4.
(A

B
)
T
=
A
T

B
T
5.
(A

B
)(C

D
)
=
(AC
)

(B
D
)
6.
(A

B
)
 1
=
A
 1

B
 1
The next theorem lays out the connection between tensor products and the
v
ec operator.
THEOREM 2.8.6. Let
A;
X
;
B be matrices conformable for multiplication. Then
v
ec
AX
B
=
 B
T

A

v
ec
X
The proof of this statement amounts to checking corresponding entries of each side of
the equation above; we leave this to the reader. It is easy to check that the
v
ec operator
is linear, that is,
v
ec
(A
+
B
)
=
v
ec
A
+
v
ec
B
: As a consequence, we have this very
useful fact, which we state for just two summands.

116
2. MATRIX ALGEBRA
y1
x1
y
y
y
x
x
x
4
3
2
4
3
2
y
x
0
0
FIGURE 2.8.1. Molecules for
(1;
1)th and
(3;
2)th grid points.
COROLLARY 2.8.7. Any solution matrix
X to the linear system
A
1
X
B
1
+
A
2
X
B
2
=
C
satisï¬es the linear system
(
 B
T
1

A
1

+
 B
T
2

A
2

)
v
ec
X
=
v
ec
C
The following is a very basic application of the tensor product. Suppose we wish to
model a two dimensional heat diffusion process on a ï¬‚at plate that occupies the unit
square in the
xy-plane. We proceed as we did in the one dimensional process described
in the introductionof Chapter 1. To ï¬x ideas, we assume that the heat source is described
by a function
f
(x;
y
);
0

x

1;
0

y

1, and that the temperature is held at
0 at
the boundary of the unit square. Also, the conductivity coefï¬cient is assumed to be the
constant
k
: Cover the square with a uniformly spaced set of grid points
(x
i
;
y
j
);
0

i;
j

n
+
1, called nodes, and assume that the spacing in each direction is a width
h
=
1=(n
+
1): Also assume that the temperature function at the
(i;
j
)th node is
u
ij
=
u(x
i
;
y
j
) and that the source is
f
ij
=
f
(x
i
;
y
j
): Notice that the values of
u on boundary
grid points is set at 0. For example,
u
01
=
u
20
=
0: By balancing the heat ï¬‚ow in the
horizontal and vertical directions, one arrives at a system of linear equations, one for
each node, of the form
 u
i 1;j
 u
i+1;j
+
4u
ij
 u
i;j
 1
 u
i;j
+1
=
h
2
k
f
ij
;
i;
j
=
1;
:
:
:
;
n
(2.8.2)
Observe that values of boundary nodes are zero, so these are not unknowns, which is
why the indexing of the equations starts at
1 instead of
0: There are exactly as many
equations as unknown grid point values. Each equation has a â€œmoleculeâ€ associated
with it which is obtained by circling the nodes that occur in the equation and connecting
these circles. A picture of a few nodes is given in Figure 2.8.1.
EXAMPLE 2.8.8. Set up and solve a system of equations for the two dimensional heat
diffusion problem described above.

2.8. *TENSOR PRODUCTS
117
SOLUTION.
Equation 2.8.2 gives us
n
2 equations in the
n
2 unknowns
u
ij
;
i;
j
=
1;
2;
:
:
:
;
n: Rewrite Equation 2.8.2 in the form
( u
i 1;j
+
2u
ij
 u
i+1;j
)
+
(
 u
i;j
 1
+
2u
ij
 u
i;j
+1
)
=
h
2
k
f
ij
Now form the
n

n matrices
T
n
=
2
6
6
6
6
4
2
 1
0
0
 1
2
...
0
0
...
...
 1
0
0
2
2
3
7
7
7
7
5
U
=
[u
ij
] and
F
=
[f
ij
] and we see that the general equations can be written in matrix
form as
T
n
U
+
U
T
n
=
T
n
U
I
n
+
I
n
U
T
n
=
h
2
k
F
:
However, we canâ€™t as yet identify a coefï¬cient matrix, which is where Corollary 2.8.7
comes in handy. Note that both
I
n and
T
n are symmetric and apply the Corollary to
obtain that the system has the form
(I
n

T
n
+
T
n

I
n
)
v
ec
U
=
v
ec
h
2
k
F
:
Now we have a coefï¬cient matrix and, whatâ€™s more, we have an automatic ordering of
the doubly indexed variables
u
ij
; namely
u
1;1
;
u
2;1
;
:
:
:
;
u
n;1
;
u
1;2
;
u
2;2
;
:
:
:
;
u
n;2
;
:
:
:
;
u
1;n
;
u
2;n
;
:
:
:
;
u
n;n
:
This is sometimes called the â€œrow ordering,â€ which refers to the rows of the nodes in
Figure 2.8.1, and not the rows of the matrix
U:
2.8 Exercises
1. Let
A
=
2
4
1
0
0
2
2
1
1
0
1
3
5 and
B
=

2
 1
1
0

: Write the matrices
A

B and
B

A
explicitly.
2. With
A;
B as above,
C
=
2
4
2
 1
1
0
1
3
3
5
; and
X
=
[x
ij
] a
3

2 matrix of unknowns,
use tensor products to determine the coefï¬cient matrix of the linear system
AX
+
X
B
=
C
:
3. Verify parts 1 and 4 of Theorem 2.8.5.
4. Verify parts 5 and 6 of Theorem 2.8.5.
5. If heat is transported with a horizontal velocity
v as well as diffused in Example 2.8.8
a new equation results at each node in the form
 u
i 1;j
 u
i+1;j
+
4u
ij
 u
i;j
 1
 u
i;j
+1
 v
h
2k
(u
i+1;j
 u
i 1;j
)
=
h
2
k
f
ij

118
2. MATRIX ALGEBRA
for
i;
j
=
1;
:
:
:
;
n: Vectorize the system and use tensor products to identify the coefï¬-
cient matrix of this linear system.
2.9. *Computational Notes and Projects
LU Factorization
Here is a problem: suppose we want to solve a nonsingular linear system
Ax
=
b repeat-
edly, with different choices of
b: A perfect example of this kind of situation is the heat
ï¬‚ow problem Example 1.1.5 where the right hand side is determined by the heat source
term
f
(x): Suppose that we need to experiment with different source terms. What hap-
pens if we do straight Gaussian elimination or Gauss-Jordan elimination? Each time
we carry out a complete calculation on the augmented matrix
e
A
=
[A
j
b] we have to
resolve the whole system. Yet, the main part of our work is the same: putting the part of
e
A corresponding to the coefï¬cient matrix
A into reduced row echelon form. Changing
the right hand side has no effect on this work. What we want here is a way to somehow
record our work on
A, so that solving a new system involves very little additional work.
This is exactly what the LU factorization is all about.
DEFINITION 2.9.1. Let
A be an
n

n matrix. An LU factorization of
A is a pair of
n

n matrices
L;
U such that
1.
L is lower triangular.
2.
U is upper triangular.
3.
A
=
LU:
Even if we could ï¬nd such beasts, what is so wonderful about them? The answer is that
triangular systems
Ax
=
b are easy to solve. For example, if
A is upper triangular,
we learned that the smart thing to do was to use the last equation to solve for the last
variable, then the next to the last equation for the next to the last variable, etc. This is the
secret of Gaussian elimination! But lower triangular systems are just as simple: use the
ï¬rst equation to solve for the ï¬rst variable, the second equation for the second variable,
and so forth. Now suppose we want to solve
Ax
=
b and we know that
A
=
LU: The
original system becomes
LU
x
=
b: Introduce an intermediate variable
y
=
U
x: Now
perform these steps:
1. (Forward solve) Solve lower triangular system
Ly
=
b for the variable
y
:
2. (Back solve) Solve upper triangular system
U
x
=
y for the variable
x:
This does it! Once we have the matrices
L;
U, we donâ€™t have to worry about right hand
sides, except for the small amount of work involved in solving two triangular systems.
Notice, by the way, that since
A is assumed nonsingular, we have that if
A
=
LU, then

2.9. *COMPUTATIONAL NOTES AND PROJECTS
119
det
A
=
det
L
det
U
6=
0: Therefore, neither triangular matrix
L or
U can have zeros
on its diagonal. Thus, the forward and back solve steps can always be carried out to
give a unique solution.
EXAMPLE 2.9.2. You are given that
A
=
2
4
2
1
0
 2
0
 1
2
3
 3
3
5
=
2
4
1
0
0
 1
1
0
1
2
1
3
5
2
4
2
1
0
0
1
 1
0
0
 1
3
5
Use this fact to solve
Ax
=
b, where (a)
b
=
[1;
0;
1]
T and (b)
b
=
[ 1;
2;
1]
T
:
SOLUTION. Set
x
=
[x
1
;
x
2
;
x
3
]
T and
y
=
[y
1
;
y
2;
y
3
]: For (a) forward solve
2
4
1
0
0
 1
1
0
1
2
1
3
5
2
4
y
1
y
2
y
3
3
5
=
2
4
1
0
1
3
5
to get
y
1
=
1, then
y
2
=
0
+
1y
1
=
1, then
y
3
=
1
 1y
1
 2y
2
=
 2: Then back solve
2
4
2
1
0
0
1
 1
0
0
 1
3
5
2
4
x
1
x
2
x
3
3
5
=
2
4
1
1
 2
3
5
to get
x
3
=
 2=( 1)
=
2, then
x
2
=
1
+
x
3
=
3, then
x
1
=
(1
 1x
2
)=2
=
 1:
For (b) forward solve
2
4
1
0
0
 1
1
0
1
2
1
3
5
2
4
y
1
y
2
y
3
3
5
=
2
4
 1
2
1
3
5
to get
y
1
=
 1, then
y
2
=
0
+
1y
1
=
 1, then
y
3
=
1
 1y
1
 2y
2
=
4: Then back
solve
2
4
2
1
0
0
1
 1
0
0
 1
3
5
2
4
x
1
x
2
x
3
3
5
=
2
4
 1
 1
4
3
5
to get
x
3
=
4=( 1)
=
 4, then
x
2
=
1
+
x
3
=
 3, then
x
1
=
(1
 1x
2
)=2
=
2:
Notice how simple the previous example was, given the LU factorization. Now how do
we ï¬nd such a factorization? In general, a nonsingular matrix may not have such a fac-
torization. A good example is the matrix

0
1
1
0

: However, if Gaussian elimination
can be performed on the matrix
A without row exchanges, then such a factorization is
really a by-product of GE. In this case let
[a
(k
)
ij
] be the matrix obtained from
A after
using the
kth pivot to clear out entries below it (thus
A
=
[a
(0)
ij
]). Remember that in GE
we only need two types of elementary operations, namely row exchanges and adding
a multiple of one row to another. Furthermore, the only elementary operations of the
latter type that we use are of this form:
E
ij
( a
(k
)
j
j
=a
(k
)
ij
), where
[a
(k
)
ij
] is the matrix
obtained from
A from the various elementary operations up to this point. The numbers
m
ij
=
 a
(k
)
j
j
=a
(k
)
ij , where
i
>
j, are sometimes called multipliers. In the way of nota-
tion, let us call a triangular matrix a unit triangular matrix if its diagonal entries are all
1â€™s.

120
2. MATRIX ALGEBRA
THEOREM 2.9.3. If Gaussian elimination is used without row exchanges on the non-
singular matrix
A, resulting in the upper triangular matrix
U, and if
L is the unit lower
triangular matrix whose entries below the diagonal are the negatives of the multipliers
m
ij, then
A
=
LU:
PROOF. The proof of this theorem amounts to noticing that the product of all the
elementary operations that reduces
A to
U is a unit lower triangular matrix
e
L with the
multipliers
m
ij in the appropriate positions. Thus
e
L
A
=
U: To undo these operations,
multiply by a matrix
L with the negatives of the multipliers in the appropriate positions.
This results in
L
e
L
A
=
A
=
LU
as desired.
The following example shows how one can write an efï¬cient program to implement LU
factorization. The idea is this: as we do Gaussian elimination the U part of the factor-
ization gradually appears in the upper parts of the transformed matrices
A
(k
)
: Below the
diagonal we replace nonzero entries with zeros, column by column. Instead of wasting
this space, use it to store the negative of the multipliers in place of the element it zeros
out. Of course, this storage part of the matrix should not be changed by subsequent
elementary row operations. When we are ï¬nished with elimination, the diagonal and
upper part of the resulting matrix is just
U and the strictly lower triangular part on the
unit lower triangular matrix
L is stored in the lower part of the matrix.
EXAMPLE 2.9.4. Use the shorthand of the preceding discussion to compute an LU fac-
torization for
A
=
2
4
2
1
0
 2
0
 1
2
3
 3
3
5
SOLUTION. Proceed as in Gaussian elimination, but store negative multipliers:
2
4
2
1
0
 2
0
 1
2
3
 3
3
5
       !
E
21
(1)
E
31
( 1)
2
4
2
1
0
 1
1
 1
1
2
 3
3
5
       !
E
32
( 2)
2
4
2
1
0
1
1
 1
 1
2
 1
3
5
Now we read off the results from the last matrix:
L
=
2
4
1
0
0
1
1
0
 1
2
1
3
5
and
U
=
2
4
2
1
0
0
1
 1
0
0
 1
3
5
What can be said if pivoting is required (for example, we might want to use a partial piv-
oting strategy)? Take the point of view that we could see our way to the end of Gaussian
elimination and store the product
P of all row exchanging elementary operations that
we use along the way. A product of such matrices is called a permutation matrix; such
a matrix is invertible, since it is a product of invertible matrices. Thus if we apply the
correct permutation matrix
P to
A we obtain a matrix for which Gaussian elimination
will succeed without further row exchanges. Consequently, we have a theorem that
applies to all nonsingular matrices. Notice that it does not limit the usefulness of LU

2.9. *COMPUTATIONAL NOTES AND PROJECTS
121
factorization since the linear system
Ax
=
b is equivalent to the system
P
Ax
=
P
b:
The following theorem could be called the â€œPLU factorization theorem.â€
THEOREM 2.9.5. If
A is a nonsingular matrix, then there exists a permutation matrix
P, upper triangular matrix
U, and unit lower triangular matrix
L such that
P
A
=
LU:
There are many other useful factorizations of matrices that numerical analysts have
studied, e.g., LDU and Cholesky. We will stop at LU, but there is one last point we
want to make. The amount of work in ï¬nding the LU factorization is the same as
Gaussian elimination itself, which we saw in Section 1.5 of Chapter 1 is approximately
2n
3
=3 ï¬‚ops. The addition work of back and forward solving is about
2n
2 ï¬‚ops. So the
dominant amount of work is done by computing the factorization rather than the back
and forward solving stages.
Project Topics
Project: LU Factorization
Write a program module that implements Theorem 2.9.5 using partial pivoting and im-
plicit row exchanges. This means that space is allocated for the
n

n matrix
A
=
[a[i;
j
]] and an array of row indices, say
indx[i]: Initially,
indx should consist of the
integers
1;
2;
:
:
:
;
n: Whenever two rows need to be exchanged, say e.g., the ï¬rst and
third, then the indices
indx[1] and
indx[3] are exchanged. References to array elements
throughout the Gaussian elimination process should be indirect: refer to the
(1;
4)th en-
try of
A as the element
a[indx[1];
4]: This method of reference has the same effect as
physically exchanging rows, but without the work. It also has the appealing feature that
we can design the algorithm as though no row exchanges have taken place provided
we replace the direct reference
a[i;
j
] by the indirect reference
a[indx[i];
j
]: The mod-
ule should return the lower/upper matrix in the format of Example 2.9.4 as well as the
permuted array
indx[i]: Effectively, this index array tells the user what the permutation
matrix
P is.
Next write an LU system solver module that uses the LU factorization to solve a general
linear system.
Finally, write a module that ï¬nds the inverse of an
n

n matrix
A by ï¬rst using the
LU factorization module, then making repeated use of the LU system solver to solve
Ax
(i)
=
e
i, where
e
i is the
ith column of the identity. Then we will have
A
 1
=
[x
(1)
;
x
(2)
;
:
:
:
;
x
(n)
]
Be sure to document and test your code. Report on the results of its application.
Project: Markov Chains
Refer to Example 2.3.4 and Section 2.3 for background. Three automobile insurance
ï¬rms compete for a ï¬xed market of customers. Annual premiums are sold to these
customers. We will label the companies A, B and C. You work for Company A, and
your team of market analysts has done a survey which draws the following conclusions:
in each of the past three years, the number of A customers switching to B is 20%, and
to C is 30%. The number of B customers switching to A is 20%, and to C is 20%.
The number of C customers switching to A is 30%, and to B is 10%. Those who do not

122
2. MATRIX ALGEBRA
switch continue to use their current companyâ€™s insurance for the next year. The ï¬rst part
of your problem is to model this market as a Markov chain. Display the transition matrix
for the model. To illustrate the workings of the model, show what it would predict as
the market shares three years from now if currently A, B and C owned equal shares of
the market.
The next part of your problem is as follows: your team has tested two advertising cam-
paigns in some smaller test markets and are conï¬dent that the ï¬rst campaign will con-
vince 20% of the B customers who would otherwise stay with B in a given year to
switch to A. The second advertising campaign would convince 20% of the C customers
who would otherwise stay with C in a given year to switch to A. Both campaigns have
about equal costs and would not change other customers habits. You have to make a
recommendation, based on your experiments with various possible initial state vectors
for the market. Will these campaigns actually improve your companyâ€™s market share?
If so, which one are you going to recommend to your superiors? Write up your recom-
mendation in the form of a report, with supporting evidence. Itâ€™s a good idea to hedge
on your bets a little by pointing out limitations to your model and claims, so devote a
few sentences to those points.
It would be a plus to carry the analysis further (your manager might appreciate that).
For instance, you could turn the additional market share from, say B customers, into a
variable and plot the long term gain for your company against this variable. A manager
could use this data to decide if it were worthwhile to attempt gaining more customers
from B. This is a bit open ended and optional.
Project: Modeling with Directed Graphs I
Refer to Example 2.3.7 and Section 2.3 for background. As a social scientist you have
studied the inï¬‚uence factors that relate seven coalition groups which, for simplicity, we
will simply label
1;
2;
3;
4;
5;
6;
7: Based on empirical studies, you conclude that the
inï¬‚uence factors can be well modeled by a dominance-directed graph with each group
as a vertex. The meaning of the presence of an edge
(i;
j
) in the graph is that coalition
group
i can dominate, i.e., swing coalition group
j its way on a given political issue.
The data you have gathered suggests that the appropriate edge set is the following:
E
=
f(1;
2);
(1;
3);
(1;
4);
(1;
7);
(2;
4);
(2;
6);
(3;
2);
(3;
5);
(3;
6);
(4;
5);
(4;
7);
(5;
1);
(5;
6);
(5;
7);
(6;
1);
(6;
4);
(7;
2);
(7;
6)g
Do an analysis of this power structure. This should include a graph. (It might be a good
idea to arrange the vertices in a circle and go from there.) It should also include a power
rating of each coalition group. Now suppose you were an advisor to one of these coali-
tion groups and, by currying certain favors, this group could gain inï¬‚uence over another
coalition group (thereby adding an edge to the graph or reversing an existing edge of
the graph). In each case, if you could pick the best group for your client to inï¬‚uence,
which would that be? Explain your results in the context of matrix multiplication if you
can.

REVIEW
123
2.9 Exercises
1. Find the LU factorization of
A
=
2
4
2
 1
1
2
3
 2
4
2
 2
3
5 and use it to solve the system
Ax
=
b where
b is
(a)
[6;
 8;
 4]
T (b)
(2;
 1;
2) (c)
(1;
2;
4)
2. Show that if
A is a nonsingular matrix with a zero
(1;
1)th entry, then
A does not
have an LU factorization.
3. Find a PLU factorization of
A
=
2
4
0
 1
1
2
3
 2
4
2
 2
3
5, and use it to solve the system
Ax
=
b where
b is
(a)
(3;
1;
4) (b)
(2;
 1;
3) (c)
(1;
2;
0)
Review
Chapter 2 Exercises
1. Let
A
=
2
4
2
 1
1
2
3
 2
4
2
 2
3
5 and
x
=
[x;
y
;
z
]: Then the equation
(xA)
T
+
Ax
T
=
[1;
4;
2]
T represents a linear system in the variables
x;
y
;
z
: Find the coefï¬cient matrix
of this system.
2. Determine for what values of
k the matrix
A
=

2
1
k
3

is invertible and ï¬nd the
inverse in that case.
3. Find the determinant of
A
=
2
6
6
4
2
1
0
3
0
2
 1
0
0
 1
2
0
 1
0
 1
2
3
7
7
5
:
4. Show by example that the sum of invertible matrices need not be invertible.
5. Show that if
A is any square matrix, then
A
+
A
T is symmetric. Use this to show
that every quadratic form
Q(x)
=
x
T
Ax can be deï¬ned by a symmetric matrix
B
=
(A
+
A
T
)=2 as well. Apply this result to the matrix of Example 2.4.16.

124
2. MATRIX ALGEBRA
6. A square matrix
A is called normal if
A
H
A
=
AA
H
: Determine which of the
following matrices are normal:
(a)

2
i
1
2

, (b)
2
4
1
0
0
0
1
 1
0
 1
1
3
5 , (c)

1
i
1
2
+
i

7. Express the matrix
D
=

3
3
1
 3

as a linear combination of the matrices
A
=

1
1
1
0

;
B
=

0
1
1
1

and
C
=

0
2
0
 1

:
8. Find all possible products of two matrices from among the following:
A
=

1
2
1
3

;
B
=

2
4

;
C
=

1
5

;
D
=

1
3
0
 1
2
1

9. Prove that if
D
=
AB
C, where
A;
C and
D are invertible matrices, then
B is
invertible.
10. Use a block multiplication to ï¬nd the square of
2
4
3
1
 1
2
0
0
1
0
0
3
5
:
11. Given that
C
=

A
0
0
B

in block form with
A and
B square, show that
C is
invertible if and only if
A and
B are, in which case
C
 1
=

A
 1
0
0
B
 1

12. Show by example that a sum or product of nilpotent matrices need not be nilpotent.
13. Suppose that
A
=
B
+
C, where
B is a symmetric matrix and
C is a skew-
symmetric matrix. Show that
B
=
1
2
(A
+
A
T
) and
B
=
1
2
(A
 A
T
).
14. Let
T be an upper triangular matrix.
(a) Show that
T
=
D
+
M
; where
D is diagonal and
M is strictly upper triangular.
(b) If
D is invertible, show that
T
=
D
(I
 N
); where
N is strictly upper triangular.
(c) If
D is invertible, use (b) and Exercise 14 to obtain a formula for
T
 1 involving
D
and
N
:

CHAPTER 3
VECTOR SPACES
It is hard to overstate the importance of the idea of a vector space, a concept which has
found application in the areas of mathematics, engineering, physics, chemistry, biology,
the social sciences and others. What we encounter is an abstraction of the idea of vector
space that we studied in calculus. In this Chapter, abstraction will come in two waves.
The ï¬rst wave, which could properly be called generalization, consists of generalizing
the familiar ideas of geometrical vectors of calculus to vectors of size greater than three.
These vector spaces could still be regarded as â€œconcrete.â€ The second wave consists of
abstracting the vector idea to entirely different kinds of objects. Abstraction can some-
times be difï¬cult. For some, the study of abstract ideas is its own reward. For others, the
natural reaction is to expect some payoff for the extra effort required to master abstrac-
tion. In the case of vector spaces we are happy to report that both kinds of students will
be satisï¬ed: vector space theory really is a thing of beauty in itself and there is indeed
a payoff for its study. It is a practical tool that enables us to understand phenomena
that would otherwise escape our comprehension. For example, in this chapter we will
use the theory in network analysis and in ï¬nding the â€œbestâ€ solution to an inconsistent
system (least squares), as well as new perspectives on our old friend
Ax
=
b:
3.1. Deï¬nitions and Basic Concepts
Generalization
We begin with the most concrete form of vector spaces, one that is closely in tune
with what we learned in calculus, when we were ï¬rst introduced to two and three di-
mensional vectors. Bear in mind that in calculus we were only concerned with real
numbers as scalars. However, we have seen that the complex numbers are a perfectly
legitimate (and sometimes more useful than the reals) ï¬eld of numbers to work with.
Therefore, our concept of a vector space must include the selection of a ï¬eld of scalars.
The requirements for such a ï¬eld are that it have binary operations of addition and
multiplication which satisfy the usual arithmetic laws: both operations are closed, com-
mutative, associative, have identities, satisfy distributive laws, and that there exist addi-
tive inverses and multiplicative inverses for nonzero elements. Although other ï¬elds are
possible, for our purposes the only ï¬elds of scalars are
F
=
R or
F
=
C
: As has been
the case previously in this text, unless there is some indication to the contrary, the ï¬eld
of scalars will be assumed to be the default, the real numbers
R:
125

126
3. VECTOR SPACES
A formal deï¬nition of vector space will come later. For now we describe a â€œvector
Concrete Vector
Spaces
spaceâ€ over a ï¬eld of scalars
F as a nonempty set
V of vectors of the same size, together
with the binary operations of scalar multiplication and vector addition, subject to the
following laws: for all vectors
u;
v
2
V and scalars
a
2
F
; (a) (Closure of vector
addition)
u
+
v
2
V
: (b) (Closure of scalar multiplication)
av
2
V
:
Very simple examples are
R
2 and
R
3 which we discuss below. Another is any line
through the origin in
R
2
; which takes the form
V
=
fc(x
0
;
y
0
)
j
c
2
Rg:
NOTATION 3.1.1. For vectors
u;
v, we deï¬ne
 u
=
( 1)u and
u
 v
=
u
+
( v
):
Geometrical vector spaces. We have already seen the vector idea in geometry or cal-
culus. In those contexts, a vector was supposed to represent a direction and a magnitude
in two or three dimensional space. At ï¬rst, one had to deal with these intuitive deï¬ni-
tions until they could be turned into something more explicitly computational, namely
the following two vector spaces over the ï¬eld of real numbers:
R
2
=
f(x;
y
)
j
x;
y
2
Rg
R
3
=
f(x;
y
;
z
)
j
x;
y
;
z
2
Rg
The distinction between vector spaces and ordinary geometrical spaces becomes a little
hazy here. Once we have set up a coordinate system we can identify each point in two or
three dimensional space with its coordinates, which we write in the form of a tuple, i.e.,
a vector. The arithmetic of these two vector spaces are just the usual coordinate-wise
vector addition and scalar multiplication. One can visualize the direction represented
by a vector
(x;
y
) by drawing an arrow, i.e., directed line segment, from the origin (the
point with coordinates
(0;
0) in the plane) to the point with coordinates
(x;
y
): The
magnitude of this vector is the length of the arrow, which is just
p
x
2
+
y
2
: The arrows
that we draw only represent the vector we are thinking of. More than one arrow could
represent the same vector as in Figure 3.3.1. The deï¬nitions of vector arithmetic could
be represented geometrically too. For example, to get the sum of vectors
u and
v, one
places a representative of vector
u in the plane, then places a representative of
v whose
tail is at the head of
v, and the vector
u
+
v is then represented by the third leg of this
triangle, with base at the base of
u: To get a scalar multiple of a vector
w one scales
w
in accordance with the coefï¬cient. See Figure 3.1.1.Though instructive, this version of
vector addition is not practical for calculations.
As a practical matter, it is also convenient to draw directed line segments connecting
points; such a vector is called a displacement vector. For example, see Figure 3.1.1 for
representatives of a displacement vector
w
=
 !
P
Q from the point
P with coordinates
(1;
2) to the point
Q with coordinates
(3;
3): One of the ï¬rst nice outcomes of vector
arithmetic is that this displacement vector can be deduced from a simple calculation
Displacement
Vector
w
=
(3;
3)
 (1;
2)
=
(3
 1;
3
 2)
=
(2;
1)
As a matter of fact, this example has familiar objects in it. We already agreed in Chapter
2 to use the tuple notation as a shorthand for column vectors. The arithmetic of
R
2
and
R
3 is the same as the usual arithmetic for column vectors. Now even though we
canâ€™t draw real geometrical pictures of vectors with four or more coordinates, we have
seen that larger vectors are useful in our search for solutions of linear systems. So the
question presents itself: why stop at three? The answer is that we wonâ€™t! We will

3.1. DEFINITIONS AND BASIC CONCEPTS
127
3
2
1
1
3
2
w
w
w
2
u
v
v
u
u
u
+
v
v
-
FIGURE 3.1.1. Displacement vectors and graphical vector operations.
use the familiar pictures of
R
2 and
R
3 to guide our intuition about vectors in higher
dimensional spaces, which we now present.
DEFINITION 3.1.2. Given a positive integer
n, we deï¬ne the standard vector space of
dimension
n over the reals to be the set of vectors
Standard Vector
Spaces
R
n
=
f(x
1
;
x
2
;
:
:
:
;
x
n
)
j
x
1
;
x
2
;
:
:
:
;
x
n
2
Rg
together with the usual vector addition and scalar multiplication.
(Remember that
(x
1
;
x
2
;
:
:
:
;
x
n
) is shorthand for the column vector
[x
1
;
x
2
;
:
:
:
;
x
n
]
T
:)
We see immediately from deï¬nition that the required closure properties of vector addi-
tion and scalar multiplication hold, so these really are vector spaces in the sense deï¬ned
above. The standard real vector spaces are often called the real Euclidean vector spaces
once the notion of a norm (a notion of length covered in the next section) is attached to
them. As in Chapter 2, we donâ€™t have to stop at the reals. For those situations in which
we want to use complex numbers, we have the following vector spaces:
DEFINITION 3.1.3. Given a positive integer
n, we deï¬ne the standard vector space of
dimension
n over the complex numbers to be the set of vectors
C
n
=
f(x
1
;
x
2
;
:
:
:
;
x
n
)
j
x
1
;
x
2
;
:
:
:
;
x
n
2
C
g
together with the usual vector addition and scalar multiplication.
The standard complex vector spaces are also sometimes called Euclidean spaces. Itâ€™s
rather difï¬cult to draw honest spatial pictures of complex vectors. The space
C
1 isnâ€™t
too bad: complex numbers can be identiï¬ed by points in the complex plane. What about
C
2? Where can we put
(1
+
2i;
3
 i)? It seems like we need four real coordinates,
namely the real and imaginary parts of two independent complex numbers, to keep
track of. This is too big to ï¬t in real three dimensional space, where we have only three
independent coordinates. We donâ€™t let this technicality deter us. We can still draw fake
vector pictures of elements of
C
2 to help our intuition, but do the algebra of vectors
exactly from deï¬nition.
EXAMPLE 3.1.4. Find the displacement vector from the point
P with coordinates
(1
+
2i;
1
 2i) to the point
Q with coordinates
(3
+
i;
2i):

128
3. VECTOR SPACES
SOLUTION. We compute
 !
P
Q
=
(3
+
i;
2i)
 (1
+
2i;
1
 2i)
=
(3
+
i
 (1
+
2i);
2i
 (1
 2i))
=
(2
 i;
 1
+
4i)
Abstraction
Now we examine the abstraction of our concept of vector space. First we have to iden-
tify the essential vector spaces properties, enough to make the resulting structure rich,
but not so much that it is tied down to an overly speciï¬c form. We saw in Chapter 2 that
many laws hold for the standard vector spaces. The essential laws were summarized in
Section 2.1 of Chapter 2. These laws become the basis for our deï¬nition of an abstract
vector space.
DEFINITION 3.1.5. An (abstract) vector space is a nonempty set
V of elements called
Abstract Vector
Space
vectors, together with operations of vector addition (
+ ) and scalar multiplication (
 ),
such that the following laws hold: for all vectors
u;
v
;
w
2
V and scalars
a;
b
2
F
;
1. (Closure of vector addition)
u
+
v
2
V
:
2. (Commutativity of addition)
u
+
v
=
v
+
u:
3. (Associativity of addition)
u
+
(v
+
w
)
=
(u
+
v
)
+
w
:
4. (Additive identity) There exists an element
0
2
V such that
u
+
0
=
u
=
0
+
u:
5. (Additive inverse) There exists an element
 u
2
V such that
u
+
( u)
=
0
=
( u)
+
u:
6. (Closure of scalar multiplication)
au
2
V
:
7. (Distributive law)
a(u
+
v
)
=
au
+
av
:
8. (Distributive law)
(a
+
b)u
=
au
+
bu:
9. (Associative law)
(ab)u
=
a(bu):
10. (Monoidal law)
1u
=
u:
Examples of these abstract vector spaces are the standard spaces just introduced, and
these will be our main focus in this section. Yet, if we squint a bit, we can see vector
spaces everywhere. There are other, entirely non-standard examples, which make the
abstraction worthwhile. Here are just a few such of examples. Our ï¬rst example is
closely related to the standard spaces, though strictly speaking it is not one of them. It
blurs the distinction between matrices and vectors in Chapter 2, since it makes matrices
into â€œvectorsâ€ in the abstract sense of the preceding deï¬nition.
EXAMPLE 3.1.6. Let
R
m;n denote the set of all
m

n matrices with real entries. Show
this set, with the usual matrix addition and scalar multiplication, forms a vector space.
SOLUTION. We know that any two matrices of the same size can be added to yield a
matrix of that size. Likewise, a scalar times a matrix yields a matrix of the same size.
Thus the operations of matrix addition and scalar multiplication are closed. Indeed,
these laws and all the other vector space laws are summarized in the laws of matrix
addition and scalar multiplication of page 53.

3.1. DEFINITIONS AND BASIC CONCEPTS
129
The next example is important in many areas of higher mathematics and is quite differ-
ent from the standard vector spaces, yet all the same a perfectly legitimate vector space.
All the same, at ï¬rst it feels odd to think of functions as â€œvectorsâ€ even though this is
meant in the abstract sense.
EXAMPLE 3.1.7. Let
C
[0;
1] denote the set of all real valued functions that are continu-
ous on the interval
[0;
1] and use the standard function addition and scalar multiplication
for these functions. That is, for
f
(x);
g
(x)
2
C
[0;
1] and real number
c, we deï¬ne the
functions
f
+
g and
cf by
(f
+
g
)(x)
=
f
(x)
+
g
(x)
(cf
)(x)
=
c(f
(x)):
Show that
C
[0;
1] with the given operations is a vector space.
SOLUTION. We set
V
=
C
[0;
1] and check the vector space axioms for this
V
: For the
rest of this example, we let
f
;
g
;
h be arbitrary elements of
V
: We know from calculus
that the sum of any two continuous functions is continuous and that any constant times
a continuous function is also continuous. Therefore the closure of addition and scalar
multiplication hold. Now for all
x such that
0

x

1, we have from deï¬nition and
the commutative law of real number addition that
(f
+
g
)(x)
=
f
(x)
+
g
(x)
=
g
(x)
+
f
(x)
=
(g
+
f
)(x)
Since this holds for all
x; we conclude that
f
+
g
=
g
+
f
; which is the commutative
law of vector addition. Similarly,
((f
+
g
)
+
h)(x)
=
(f
+
g
)(x)
+
h(x)
=
(f
(x)
+
g
(x))
+
h(x)
=
f
(x)
+
(g
(x)
+
h(x))
=
(f
+
(g
+
h))(x)
Since this holds for all
x; we conclude that
(f
+
g
)
+
h
=
f
+
(g
+
h); which is the
associative law for addition of vectors.
Next, if
0 denotes the constant function with value
0, then for any
f
2
V we have that
for all
0

x

1,
(f
+
0)(x)
=
f
(x)
+
0
=
f
(x)
Since this is true for all
x we have that
f
+
0
=
f, which establishes the additive identity
law. Also, we deï¬ne
( f
)(x)
=
 (f
(x)) so that for all
0

x

1,
(f
+
( f
))(x)
=
f
(x)
 f
(x)
=
0
from which we see that
f
+
( f
)
=
0: The additive inverse law follows. For the
distributive laws note that for real numbers
a;
b and continuous functions
f
;
g
2
V , we
have that for all
0

x

1,
a(f
+
g
)(x)
=
a(f
(x)
+
g
(x))
=
af
(x)
+
ag
(x)
=
(af
+
ag
)(x)
which proves the ï¬rst distributive law. For the second distributive law, note that for all
0

x

1,
((a
+
b)g
)(x)
=
(a
+
b)g
(x)
=
ag
(x)
+
bg
(x)
=
(ag
+
bg
)(x)
and the second distributive law follows. For the scalar associative law, observe that for
all
0

x

1,
((ab)f
)(x)
=
(ab)f
(x)
=
a(bf
(x))
=
(a(bf
))(x)

130
3. VECTOR SPACES
so that
(ab)f
=
a(bf
), as required. Finally, we see that
(1f
)(x)
=
1f
(x)
=
f
(x)
from which we have the monoidal law
1f
=
f
: Thus,
C
[0;
1] with the prescribed oper-
ations is a vector space.
The preceding example could have just as well been
C
[a;
b]; the set of all continuous
functions on the interval
a

x

b: Indeed, most of what we say about
C
[0;
1] is
equally applicable to the more general space
C
[a;
b]: We stick to the interval
0

x

1
for simplicity. The next example is also based on the â€œfunctions as vectorsâ€ idea.
EXAMPLE 3.1.8. Let
V
=
ff
(x)
2
C
[0;
1]
j
f
(1=2)
=
0g and
W
=
ff
(x)
2
C
[0;
1]
j
f
(1=2)
=
1g; where each set has the operations of function addition and scalar multiplication as
in Example 3.1.7. One of these sets forms a vector space over the reals, while the other
does not. Determine which.
SOLUTION. Notice that we donâ€™t have to check the commutativity of addition, associa-
tivity of addition, distributive laws, associative law or monoidal law. The reason is that
we already know from the previous example that these laws hold when the operations
of the space
C
[0;
1] are applied to any elements of
C
[0;
1]; whether they belong to
V or
W or not. So the only laws to be checked are the closure laws and the identity laws.
First let
f
(x);
g
(x)
2
V and let
c be a scalar. By deï¬nition of the set
V we have that
f
(1=2)
=
0 and
g
(1=2)
=
0: Add these equations together and we obtain
(f
+
g
)(1=2)
=
f
(1=2)
+
g
(1=2)
=
0
+
0
=
0
It follows that
V is closed under addition with these operations. Furthermore, if we
multiply the identity
f
(1=2)
=
0 by the real number
c we obtain that
(cf
)(1=2)
=
c

f
(1=2)
=
c

0
=
0
It follows that
V is closed under scalar multiplication. Now certainly the zero function
belongs to
V , since this function has value
0 at any argument. Therefore,
V contains an
additive identity element. Finally, we observe that the negative of a function
f
(x)
2
V
is also an element of
V , since
( f
)(1=2)
=
 1

f
(1=2)
=
 1

0
=
0
Therefore, the set
V with the given operations satisï¬es all the vector space laws and is
an (abstract) vector space in its own right.
When we examine the set
W in a similar fashion, we run into a roadblock at the closure
of addition law. If
f
(x);
g
(x)
2
W, then by deï¬nition of the set
W we have that
f
(1=2)
=
1 and
g
(1=2)
=
1: Add these equations together and we obtain
(f
+
g
)(1=2)
=
f
(1=2)
+
g
(1=2)
=
1
+
1
=
2
This means that
f
+
g is not in
W
; so the closure of addition fails. We need go no further.
If only one of the vector space axioms fails, then we do not have a vector space. Hence,
W with the given operations is not a vector space.
Notice that there is a certain economy in this situation. A number of laws did not need
to be checked by virtue of the fact that the sets in question were subsets of existing
vector spaces with the same vector operations. We shall have more to say about this
situation in the next section. Here is another example that is useful and instructive.

3.1. DEFINITIONS AND BASIC CONCEPTS
131
EXAMPLE 3.1.9. Show that the set
S
n of all
n

n real symmetric matrices with the
usual matrix addition and scalar multiplication form a vector space.
SOLUTION. Just as in the preceding example, we donâ€™t have to check the commutativity
of addition, associativity of addition, distributive laws, associative law or monoidal law
since we know that these laws hold for any matrices, symmetric or not. Now let
A;
B
2
S
n
: This means by deï¬nition that
A
=
A
T and
B
=
B
T
: Let
c be any scalar. Then we
have both
(
A
+
B
)
T
=
A
T
+
B
T
=
A
+
B
and
(
cA)
T
=
cA
T
=
cA
It follows that the set
S
n is closed under the operations of matrix addition and scalar
multiplication. Furthermore, the zero
n

n matrix is clearly symmetric, so the set
S
n
has an additive identity element. Finally,
( A)
T
=
 A
T
=
 A; so each element of
S
n has an additive inverse as well. Therefore, all of the laws for a vector space are
satisï¬ed, so
S
n together with these operations is an (abstract) vector space.
One of the virtues of abstraction is that it allows us to cover many cases with one state-
ment. For example, there are many simple facts that are deducible from the vector space
laws alone. With the standard vector spaces, these facts seem transparently clear. For
abstract spaces, the situation is not quite so obvious. Here are a few examples of what
can be deduced from deï¬nition.
EXAMPLE 3.1.10. Let
v
2
V
; a vector space and
0 the vector zero. Deduce from the
vector space properties alone that
0v
=
0:
SOLUTION. Certainly we have the scalar identity
0
+
0
=
0: Multiply both sides on the
right by the vector
v to obtain that
(0
+
0)v
=
0v
Now use the distributive law to obtain
0v
+
0v
=
0v
Next add
 (0v
) to both sides (remember, we donâ€™t know itâ€™s
0 yet), use the associative
law of addition to regroup and obtain that
0v
+
(0v
+
( 0v
))
=
0v
+
( 0v
)
Now use the additive inverse law to obtain that
0v
+
0
=
0
Finally, use the identity law to obtain
0v
=
0
which is what we wanted to show.
EXAMPLE 3.1.11. Show that the vector space
V has only one zero element.

132
3. VECTOR SPACES
SOLUTION. Suppose that both
0 and
0
 act as zero elements in the vector space. Use
the additive identity property of
0 to obtain that
0

+
0
=
0

; while the additive identity
property of
0
 implies that
0
+
0

=
0: By the commutative law of addition,
0

+
0
=
0
+
0

: It follows that
0

=
0; whence there can be only one zero element.
There are several other such arithmetic facts that we want to identify, along with the one
of this example. In the case of standard vectors, these facts are obvious, but for abstract
vector spaces, they require a proof similar to the one we have just given. We leave these
as exercises.
Laws of Vector Arithmetic. Let
v be a vector in some vector space
V and let
c be any scalar. Then
1.
0v
=
0
2.
c0
=
0
3.
( c)v
=
c( v
)
=
 (cv
)
4. If
cv
=
0; then
v
=
0 or
c
=
0:
5. A vector space has only one zero element.
6. Every vector has only one additive inverse.
A reminder about notation: just as in matrix arithmetic, for vectors
u;
v
2
V
; we under-
stand that
u
 v
=
u
+
( v
):
Linear Operators
We were introduced in Section 2.3 of Chapter 2 to the idea of a linear function in the
context of standard vectors. Now that we have a notion of an abstract vector space, we
can examine linearity in this larger setting. We have seen that some of our â€œvectorsâ€
can themselves be functions, as in the case of the vector space
C
[0;
1] of continuous
functions on the interval
[0;
1]: In order to avoid confusion in cases like this, we prefer
to designate linear functions by the term linear operator. Other common terms for this
object are linear mapping or linear transformation.
Before giving the deï¬nition of linear operator, let us recall some notation that is associ-
ated with functions in general. We identify a function
f with the notation
f
:
D
!
T,
where
D and
T are the domain and target of the function, respectively. This means that
for each
x in the domain
D
; the value
f
(x) is a uniquely determined element in the
target
T
: We want to emphasize at the outset, that there is a difference here between the
target of a function and its range. The range of the function
f is deï¬ned as the subset
of the target
range
(f
)
=
fy
j
y
=
f
(x) for some
x
2
D
g
which is just the set of all possible values of
f
(x): For example, we can deï¬ne a function
f
:
R
!
R by the formula
f
(x)
=
x
2
: It follows from our speciï¬cation of
f that the
target of
f is understood to be
R, while the range of
f is the set of nonnegative real
numbers.
A function that maps elements of one vector space into another, say
f
:
V
!
W is
sometimes called an operator or transformation. For example, the operator
f
:
R
2
!

3.1. DEFINITIONS AND BASIC CONCEPTS
133
R
3 might be given by the formula
f

x
y

=
2
4
x
2
xy
y
2
3
5
Notice in this example that the target of
f is
R
3, which is not the same as the range of
f
;
since elements in the range have nonnegative ï¬rst and third coordinates. From the point
of view of linear algebra, this function lacks the essential feature that makes it really
interesting, namely linearity.
DEFINITION 3.1.12. A function
T
:
V
!
W from the vector space
V into the space
W over the same ï¬eld of scalars is called a linear operator (mapping, transformation)
if, for all vectors
u;
v
2
V and scalars
c;
d, we have
T
(cu
+
dv
)
=
cT
(u)
+
dT
(v
)
By taking
c
=
d
=
1 in the deï¬nition, we see that a linear function
T is additive, that is,
T
(u
+
v
)
=
T
(u)
+
T
(v
): Also, by taking
d
=
0 in the deï¬nition, we see that a linear
function is outative, that is,
T
(cu)
=
cT
(u): As a matter of fact, these two conditions
imply the linearity property, and so are equivalent to it. We leave this fact as an exercise.
By repeated application of the linearity deï¬nition, we can extend the linearity property
to any linear combination of vectors, not just two terms. This means that for any scalars
c
1
;
c
2
;
:
:
:
;
c
n and vectors
v
1
;
v
2
;
:
:
:
;
v
n
; we have
T
(c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
)
=
c
1
T
(v
1
)
+
c
2
T
(v
2
)
+



+
c
n
T
(v
n
)
EXAMPLE 3.1.13. Determine if
T
:
R
2
!
R
3 is a linear operator, where
T is given by
the formula (a)
T
((x;
y
))
=
(x
2
;
xy
;
y
2
) or (b)
T
((x;
y
))
=

1
0
1
 1


x
y

:
SOLUTION. If
T is deï¬ned by (a) then we show by a simple example that
T fails to be
linear. Let us calculate
T
((1;
0)
+
(0;
1))
=
T
((1;
1))
=
(1;
1;
1)
while
T
((1;
0))
+
T
((0;
1))
=
(1;
0;
0)
+
(0;
0;
1)
=
(1;
0;
1):
These two are not equal, so
T fails to satisfy the linearity property.
Next consider the operator
T deï¬ned as in (b). Write
A
=

1
0
1
 1

and
v
=

x
y

and we see that the action of
T can be given as
T
(v
)
=
Av
: Now we have already seen
in Section 2.3 of Chapter 2 that the operation of multiplication by a ï¬xed matrix is a
linear operator.
In Chapter 2 the following useful fact was shown, which we now restate for real vectors,
though it is equally valid for standard complex vectors.
THEOREM 3.1.14. Let
A be an
m

n matrix and deï¬ne an operator
T
A
:
R
n
!
R
m
by the formula
T
(v
)
=
Av, for all
v
2
R
n
: Then
T
A is a linear operator.

134
3. VECTOR SPACES
Abstraction gives us a nice framework for certain key properties of mathematical ob-
jects, some of which we have seen before. For example, in calculus we were taught that
differentiation has the â€œlinearity property.â€ Now we can view this assertion in a fuller
context: let
V be the space of differentiable functions and deï¬ne an operator
T on
V by
the rule
T
(f
(x))
=
f
0
(x): Then
T is a linear operator on the space
V
:
3.1 Exercises
In Exercises 1-6 you are to determine if the given set and operations deï¬ne a vector
space. If not, indicate which laws fail.
1.
V
=
f

a
b
0
a
+
b

ja;
b
2
Rg with the usual matrix addition and scalar multipli-
cation.
2.
V
=
f

a
0
0
1

ja
2
Rg with the usual matrix addition and scalar multiplication.
3.
V
=
f

a
b
a

ja;
b
2
C
g with the usual matrix addition and scalar multiplica-
tion. In this example the scalar ï¬eld is the complex numbers.
4.
V consists of all continuous functions
f
(x) on the interval
[0;
1] such that
f
(0)
=
0:
5.
V consists of all quadratic polynomial functions
f
(x)
=
ax
2
+
bx
+
c;
a
6=
0:
6.
V consists of all continuous functions
f
(x) on the interval
[0;
1] such that
f
(0)
=
f
(1):
7. Use the deï¬nition of vector space to prove Vector Law of Arithmetic 2:
c0
=
0:
8. Use the deï¬nition of vector space to prove Vector Law of Arithmetic 3:
( c)v
=
c( v
)
=
 (cv
):
9. Use the deï¬nition of vector space to prove Vector Law of Arithmetic 4: If
cv
=
0;
then
v
=
0 or
c
=
0:
10. Let
u;
v
2
V
; where
V is a vector space. Use the vector space laws to prove that
the equation
x
+
u
=
v has one and only one solution vector
x
2
V
; namely
x
=
u
 v
:
11. Let
V be a vector space and form the set
V
2 consisting of all ordered pairs
(u;
v
)
where
u;
v
2
V
: We can deï¬ne an addition and scalar multiplication on these ordered
pairs as follows
(u
1
;
v
1
)
+
(u
1
;
v
1
)

(u
1
+
u
2
;
v
1
+
v
2
)
c

(u
1
;
v
1
)
=
(cu
1
;
cv
1
)
Verify that with these operations
V
2 becomes a vector space over the same ï¬eld of
scalars as
V .
12. Determine which of the following functions
T
:
R
3
!
R
2 is a linear operator and
if so, write the operator as a matrix multiplication. Here
x
=
(x;
y
;
z
)
(a)
T
(x)
=
(x
 y
;
x
+
2y
 4z
)
(b)
T
(x)
=
(x
+
y
;
xy
) (c)
T
(x)
=
(y
;
z
;
x)

3.2. SUBSPACES
135
13. Let
V
=
C
[0;
1] and deï¬ne an operator
T
:
V
!
V by the following formulas for
T
(f
):
(a)
T
(f
)(x)
=
f
(1)x
2
(b)
T
(f
)(x)
=
f
2
(x)
(c)
T
(f
)(x)
=
2f
(x) (d)
T
(f
)(x)
=
R
x
0
f
(s)
ds
Which, if any of these operators is linear? If so, is the target
V of the operator equal to
its range?
14. Determine if the operator
T
:
R
2
!
R
2 is a linear transformation or not (give
reasons) where
(a)
T
(x;
y
)
=
x(0;
y
) (b)
T
(x;
y
)
=
(x
+
2y
;
0) (c)
T
(x;
y
)
=
(sin
x;
cos
y
)
15. Let
T
:
R
3
!
P
2 be deï¬ned by
T
((a;
b;
c))
=
a
+
bx
+
cx
2
: Show that
T is a
linear operator whose range is
P
2
:
16. Prove the remark following Deï¬nition 3.1.12: if a function
T
:
V
!
W between
vector spaces
V and
W is additive and outative, then it is linear.
3.2. Subspaces
We now turn our attention to the concept of a subspace, which is a rich source of useful
examples of vector spaces. It frequently happens that a certain vector space of interest
is a subset of a larger, and possibly better understood vector space, and that the vector
operations are the same for both spaces. A good example of this situation is given by the
vector space
V of Example 3.1.8 which is a subset of the larger vector space
C
[0;
1] with
both spaces sharing the same deï¬nitions of vector addition and scalar multiplication.
Here is a precise deï¬nition for the subspace idea.
DEFINITION 3.2.1. A subspace of the vector space
V is a subset
W of
V such that
W,
together with the binary operations it inherits from
V , forms a vector space (over the
same ï¬eld of scalars as
V ) in its own right.
Given a subset
W of the vector space
V , we can apply the deï¬nition of vector space
directly to the subset
W to obtain the following very useful test.
THEOREM 3.2.2. Let
W be a subset of the vector space
V
: Then
W is a subspace of
V
if and only if
Subspace Test
1.
W contains the zero element of
V
:
2. (Closure of addition) For all
u;
v
2
W
;
u
+
v
2
W
:
3. (Closure of scalar multiplication) For all
u
2
W and scalars
c,
cu
2
W
:

136
3. VECTOR SPACES
PROOF. Let
W be a subspace of the vector space
V
: Then the closure of addition
and scalar multiplication are automatically satisï¬ed by the deï¬nition of vector space.
For condition 1, we note that
W must contain a zero element by deï¬nition of vector
space. Let
0
 be this element and
0 the zero element of
V , so that
0

+
0

=
0

: Add
the negative of
0
 (as an element of
V ) to both sides, cancel terms and we see that
0

=
0: This shows that
W satisï¬es condition 1.
Conversely, suppose that
W is a subset of
V satisfying the three conditions. Since the
operations of
W are those of the vector space
V , and
V is a vector space, most of
the laws for
W are automatic. Speciï¬cally, the laws of commutativity, associativity,
distributivity and the monoidal law hold for elements of
W
: The additive identity law
follows from condition 1.
The only law that needs any work is the additive inverse law. Let
w
2
W
: By closure of
scalar multiplication,
( 1)w is in
W
: By the laws of vector arithmetic in the preceding
section, this vector is simply
 w. This proves that every element of
W has an additive
inverse in
W
; which shows that
W is a subspace of
V
:
One notable point that comes out of the subspace test is that every subspace of
V con-
tains the zero vector. This is certainly not true of arbitrary subsets of
V and serves to
remind us that, although every subspace is a subset of
V
; not every subset is a subspace.
Confusing the two is a common mistake, so much so that we issue the following
Caution: Every subspace of a vector space is a subset, but not conversely.
EXAMPLE 3.2.3. Which of the following subsets of the standard vector space
V
=
R
3
are subspaces of
V ?
(a)
W
1
=
f(x;
y
;
z
)
j
x
 2y
+
z
=
0g
(b)
W
2
=
f(x;
y
;
z
)
j
x;
y
; and
z are positive
g
(c)
W
3
=
f(0;
0;
0)g
(d)
W
4
=
f(x;
y
;
z
)
j
x
2
 y
=
0g
SOLUTION. (a) Take
w
=
(0;
0;
0) and obtain that
0
 2

0
+
0
=
0;
so that
w
2
W
1
: Next, check closure of
W
1 under addition. Letâ€™s name two general
elements from
W
1, say
u
=
(x
1
;
y
1
;
z
1
) and
v
=
(x
2
;
y
2
;
z
2
): Then we know from
deï¬nition of
W
1 that
x
1
 2y
1
+
z
1
=
0
x
2
 2y
2
+
z
2
=
0
We want to show that
u
+
v
=
(x
1
+
x
2
;
y
1
+
y
2
;
z
1
+
z
2
)
2
W
1
: So add the two
equations above and group terms to obtain
(x
1
+
x
2
)
 2(y
1
+
y
2
)
+
(z
1
+
z
2
)
=
0:
This equation shows that the coordinates of
u
+
v ï¬t the requirement for being an
element of
W
1, i.e.,
u
+
v
2
W
1
: Similarly, if
c is a scalar then we can multiply the
equation that says
u
2
W
1
; i.e.,
x
1
 2y
1
+
z
1
=
0; by
c to obtain
(cx
1
)
 2(cy
1
)
+
(cz
1
)
=
c0
=
0:

3.2. SUBSPACES
137
This shows that the coordinates of
cv ï¬t the requirement for being in
W
1, i.e.,
cu
2
W
1
:
It follows that
W
1 is closed under both addition and scalar multiplication, so it is a
subspace of
R
3
:
(b) This one is easy. Any subspace must contain the zero vector
(0;
0;
0): Clearly
W
2
does not. Hence it cannot be a subspace. Another way to see it is to notice that closure
under scalar multiplication fails (try multiplying
(1;
1;
1) by
 1).
(c) The only possible choice for arbitrary elements
u;
v, in this space are
u
=
v
=
(0;
0;
0): But then we see that
W
3 obviously contains the zero vector and for any scalar
c
(0;
0;
0)
+
(0;
0;
0)
=
(0;
0;
0)
c(0;
0;
0)
=
(0;
0;
0)
Therefore
W
3 is a subspace of
V by the subspace test.
(d) First of all,
0
2
 0
=
0, which means that
(0;
0;
0)
2
W
4
: Likewise we see that
(1;
1;
0)
2
W
4 as well. But
(1;
1;
0)
+
(1;
1;
0)
=
(2;
2;
0) which is not an element of
W
4 since
2
2
 2
6=
0: Therefore, closure of addition fails and
W
4 is not a subspace of
V by the subspace test.
Part (c) of this example highlights part of a simple fact about vector spaces. Every vector
space
V must have at least two subspaces, namely,
f0g (where
0 is the zero vector in
V ) and
V itself. These are not terribly exciting subspaces, so they are commonly called
the trivial subspaces.
EXAMPLE 3.2.4. Show that the subset
P of
C
[0;
1] consisting of all polynomial func-
tions is a subspace of
C
[0;
1] and that the subset
P
n consisting of all polynomials of
degree at most
n is a subspace of
P
:
SOLUTION. Certainly
P is a subset of
C
[0;
1], since any polynomial is uniquely de-
termined by its values on the interval
[0;
1] and
P contains the zero constant function
which is a polynomial function. Let
f and
g be two polynomial functions on the interval
[0;
1]; say
f
(x)
=
a
0
+
a
1
x
+



+
a
n
x
n
g
(x)
=
b
0
+
b
1
x
+



+
b
n
x
n
where
n is an integer equal to the maximum of the degrees of
f
(x) and
g
(x): Let
c be
any real number and we see that
(f
+
g
)(x)
=
(a
0
+
b
0
)
+
(a
1
+
b
1
)x
+



+
(a
n
+
b
n
)x
n
(cf
)(x)
=
ca
0
+
ca
1
x
+



+
ca
n
x
n
which shows that
P is closed under function addition and scalar multiplication. By the
subspace test
P is a subspace of
C
[0;
1]: The very same equations above also show that
the subset
P
n passes the subspace test, so it is a subspace of
C
[0;
1]:
EXAMPLE 3.2.5. Show that the set of all upper triangular matrices (see page 74) in the
vector space
V
=
R
n;n of
n

n real matrices is a subspace of
V
:

138
3. VECTOR SPACES
SOLUTION. Since the zero matrix is upper triangular, the subset
W of all upper trian-
gular matrices contains the zero element of
V
: Let
A
=
[a
i;j
] and
B
=
[b
i;j
] be any two
matrices in
W and let
c be any scalar. By deï¬nition of upper triangular, we must have
a
i;j
=
0 and
b
i;j
=
0 if
i
>
j: However,
A
+
B
=
[a
i;j
+
b
i;j
]
cA
=
[ca
i;j
]:
and for
i
>
j we have
a
i;j
+
b
i;j
=
0
+
0
=
0 and
ca
i;j
=
c0
=
0, so that
A
+
B and
cA
are also upper triangular. It follows that
W is a subspace of
V by the subspace test.
There is an extremely useful type of subspace which requires the notion of a linear
combination of the vectors
v
1
;
v
2
;
:
:
:
;
v
n in the vector space
V : an expression of the
form
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
where
c
1
;
c
2
;
:
:
:
;
c
n are scalars. We can consider the set of all possible linear combi-
nations of a given list of vectors, which is what our next deï¬nition is about.
DEFINITION 3.2.6. Let
v
1
;
v
2
;
:
:
:
;
v
n be vectors in the vector space
V
: The span of
these vectors, denoted by
spanfv
1
;
v
2
;
:
:
:
;
v
n
g, is the subset of
V consisting of all
possible linear combinations of these vectors, i.e.,
Linear
Combinations
and Span
spanfv
1
;
v
2
;
:
:
:
;
v
n
g
=
fc
1
v
1
+
c
2
v
2
+
:
:
:
+
c
n
v
n
j
c
1
;
c
2
;
:
:
:
;
c
nare scalarsg
Caution: The scalars we are using really make a difference. For example, if
v
1
=
(1;
0)
and
v
2
=
(0;
1) are viewed as elements of the real vector space
R
2, then
span
fv
1
;
v
2
g
=
fc
1
(1;
0)
+
c
2
(0;
1)
j
c
1
;
c
2
2
Rg
=
f(c
1
;
c
2
)
j
c
1
;
c
2
2
Rg
=
R
2
Similarly, if we view
v
1 and
v
2 as elements of the complex vector space
C
2, then we see
that
span
fv
1
;
v
2
g
=
C
2
: Now
R
2 consists of those elements of
C
2 whose coordinates
have zero imaginary parts, so
R
2 is a subset of
C
2; but these are certainly not equal
sets. By the way,
R
2 is deï¬nitely not a subspace of
C
2 either, since the subset
R
2 is not
closed under multiplication by complex scalars.
We should take note here that the deï¬nition of span would work perfectly well with
inï¬nite sets, as long as we understand that linear combinations in the deï¬nition would
be ï¬nite and therefore not involve all the vectors in the span. A situation in which this
extension is needed is as follows: consider the space
P of all polynomials with the usual
addition and scalar multiplication. It makes perfectly good sense to write
P
=
span
f1;
x;
x
2
;
x
3
;
:
:
:
;
x
n
;
:
:
:
g
since every polynomial is a ï¬nite linear combination of various monomials
x
k.
EXAMPLE 3.2.7. Interpret the following linear spans in
R
3 geometrically:
(a)
W
1
=
spanf
2
4
1
1
2
3
5
g;
(b)
W
2
=
spanf
2
4
1
1
2
3
5
;
2
4
2
0
0
3
5
g

3.2. SUBSPACES
139
1
2
3
1
2
3
1
2
3
(2,0,0)
(1,1,2)
z
x
y
span{(2,0,0), (1,1,2)}
span{(1,1,2)}
FIGURE 3.2.1. Cross-hatched portion of span{(2,0,0), (1,1,2)} and span{(1,1,2)}.
SOLUTION. Elements of
W
1 are simply scalar multiples of the single vector
(1;
1;
2):
The set of all such multiples gives us a line through the origin
(
0;
0;
0)
: On the other
hand, elements of
W
2 give all possible linear combinations of two vectors
(1;
1;
2) and
(2;
0;
0): The locus of points generated by these combinations is a plane in
R
3 contain-
ing the origin, so it is determined by the points with coordinates
(0;
0;
0);
(1;
1;
2); and
(2;
0;
0): See Figure 3.2.1 for a picture of these spans.
Spans are the premier examples of subspaces. In a certain sense, it can be said that every
subspace is the span of some of its vectors. The following important fact is a very nice
application of the subspace test.
THEOREM 3.2.8. Let
v
1
;
v
2
;
:
:
:
;
v
n be vectors in the vector space
V
: Then
W
=
spanfv
1
;
v
2
;
:
:
:
;
v
n
g is a subspace of
V
:
PROOF. First, we observe that
W the zero vector can be expressed as the linear
combination
0v
1
+
0v
2
+



+
0v
n, which is an element of
W
: Next, let
c be any scalar
and form general elements
u;
v
2
W, say
u
=
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
v
=
d
1
v
1
+
d
2
v
2
+



+
d
n
v
n
Add these vectors and collect like terms to obtain
u
+
v
=
(c
1
+
d
1
)v
1
+
(c
2
+
d
2
)v
2
+



+
(c
n
+
d
n
)v
n
Thus
u
+
v is also a linear combination of
v
1
;
v
2
;
:
:
:
;
v
n, so
W is closed under vector
addition. Finally, form the product
cu to obtain
cu
=
(cc
1
)v
1
+
(cc
2
)v
2
+



+
(cc
n
)v
n
which is again a linear combination of
v
1
;
v
2
;
:
:
:
;
v
n, so
W is closed under scalar
multiplication. By the subspace test,
W is a subspace of
V
:

140
3. VECTOR SPACES
There are a number of simple properties of spans that we will need from time to time.
One of the most useful is this basic fact.
THEOREM 3.2.9. Let
v
1
;
v
2
;
:
:
:
;
v
n be vectors in the vector space
V and let
w
1
;
w
2
;
:
:
:
;
w
k
be vectors in
spanfv
1
;
v
2
;
:
:
:
;
v
n
g: Then
span
fw
1
;
w
2
;
:
:
:
;
w
k
g

span
fv
1
;
v
2
;
:
:
:
;
v
n
g
PROOF. Suppose that for each index
j
=
1;
2;
:
:
:
k
;
w
j
=
c
1j
v
1
+
c
2j
v
2
+



+
c
nj
v
n
:
Then we may write a linear combination of the
w
jâ€™s by regrouping the coefï¬cients of
each
v
k as
d
1
w
1
+
d
2
w
2
+



+
d
k
w
k
=
d
1
(c
11
v
1
+
c
21
v
2
+



+
c
n1
v
n
)
+
d
2
(c
12
v
1
+
c
22
v
2
+



+
c
n2
v
n
)
+
:
:
:
+
d
k
(c
1k
v
1
+
c
2k
v
2
+



+
c
nk
v
n
)
=
 
k
X
i=1
d
i
c
i1
!
v
1
+
 
k
X
i=1
d
i
c
i1
!
v
2
+
:
:
:
 
k
X
i=1
d
i
c
i1
!
v
n
:
It follows that each element of
spanfw
1
;
w
2
;
:
:
:
;
w
k
g belongs to the vector space
spanfv
1
;
v
2
;
:
:
:
;
v
n
g, as desired.
Here is a simple application of this theorem: if
v
i
1
;
v
i
2
;
:
:
:
;
v
i
k is a subset of
v
1
;
v
2
;
:
:
:
;
v
n,
then
spanfv
i
1
;
v
i
2
;
:
:
:
;
v
i
k
g

spanfv
1
;
v
2
;
:
:
:
;
v
n
g
The reason is that each
w
j
=
v
i
j
=
0v
1
+
0v
2
+



+
1v
i
j
+



+
0v
n
so that the theorem applies to these vectors. Put another way, if we enlarge the list of
spanning vectors, we enlarge the spanning set. However, we may not obtain a strictly
larger spanning set, as the following example shows.
EXAMPLE 3.2.10. Show that
spanf

1
0

;

1
1

g
=
spanf

1
0

;

1
1

;

1
2

g
Why might one prefer the ï¬rst spanning set?
SOLUTION. Label vectors
v
1
=

1
0

,
v
2
=

1
1

, and
v
3
=

1
2

: Every element
of
spanfv
1
;
v
2
g belongs to
span
fv
1
;
v
2
;
v
3
g, since we can write
c
1
v
1
+
c
2
v
2
=
c
1
v
1
+
c
2
v
2
+
0v
3
: So we certainly have that
span
fv
1
;
v
2
g

span
fv
1
;
v
2
;
v
3
g: However, a
little ï¬ddling with numbers reveals this fact:

1
2

=
( 1)

1
0

+
2

1
1


3.2. SUBSPACES
141
In other words
v
3
=
 v
1
+
2v
2
: Therefore any linear combination of
v
1
;
v
2
;
v
3 can
be written as
c
1
v
1
+
c
2
v
2
+
c
3
v
3
=
c
1
v
1
+
c
2
v
2
+
c
3
( v
1
+
2v
2
)
=
(c
1
 c
3
)v
1
+
(c
2
+
2c
3
)v
2
Thus any element of
spanfv
1
;
v
2
;
v
3
g belongs to
spanfv
1
;
v
2
g, so the two spans are
equal. This is an algebraic representation of the geometric fact that the three vectors
v
1
;
v
2
;
v
3 belong to the same plane in
R
2 that is spanned by the two vectors
v
1
;
v
2
:
It seems reasonable that we should prefer the spanning set
v
1
;
v
2 to the set
v
1
;
v
2
;
v
3,
since the former is smaller yet carries just as much information as the latter. As a matter
of fact, we would get the same span if we used
v
1
;
v
3 or
v
2
;
v
3
: The spanning set
v
1
;
v
2
;
v
3 has â€œredundantâ€ vectors in it.
3.2 Exercises
In Exercises 1-7, determine if the subset
W is a subspace of the given vector space
V
:
1.
V
=
R
3 and
W
=
f(a;
b;
a
 b
+
1)
j
a;
b
2
Rg
:
2.
V
=
R
3 and
W
=
f(a;
0;
a
 b)
j
a;
b
2
Rg
:
3.
V
=
R
3 and
W
=
f(a;
b;
c)
j
2a
 b
+
c
=
0g
:
4.
V
=
R
2;3 and
W
=
f

a
b
0
b
a
0

j
a;
b
2
Rg
:
5.
V
=
C
[0;
1] and
W
=
ff
(x)
2
C
[0;
1]
j
f
(1)
+
f
(1=2)
=
0g:
6.
V
=
C
[0;
1] and
W
=
ff
(x)
2
C
[0;
1]
j
f
(1)

0g:
7.
V
=
R
n;n and
W is the set of all invertible matrices in
V
:
8. Recall that a matrix
A is skew-symmetric if
A
T
=
 A.
(a) Show that every skew-symmetric matrix has the form
A
=

a
b
 b
c

; for some
scalars
a;
b;
c:
(b) Show that the set
V of all
2

2 skew-symmetric real matrices is a subspace of
R
2;2
:
9. Show that
spanf

1
0

;

0
1

g
=
R
2
:
10. Show that
span
f

1
0

;

1
1

g
=
R
2
:
11. Which of the following spans equal the space
P
2 of polynomials of degree at most
2? Justify your answers.
(a)
spanf1;
1
+
x;
x
2
g
(b)
span
fx;
4x
 2x
2
;
x
2
g (c)
spanf1
+
x
+
x
2
;
1
+
x;
3g
12. Find two vectors
v
;
w
2
R
3 such that if
u
=
(1;
 1;
1), then
R
3
=
spanfu;
v
;
w
g
13. Let
u
=
(2;
 1;
1);
v
=
(0;
1;
1) and
w
=
(
2;
1;
3)
: Show that
span
fu
+
w
;
v
 w
g

span
fu;
v
;
w
g and determine whether or not these spans are actually equal.

142
3. VECTOR SPACES
14. Prove that if
V
=
R
n;n, then the set of all diagonal matrices is a subspace of
V
:
15. Let
U and
V be subspaces of
W. Use the subspace test to prove the following.
(a) The set intersection
U
\
V is a subspace of
W
:
(b) The sum of the spaces,
U
+
V
=
fu
+
v
j
u
2
U and
v
2
V
g is a subspace of
W
:
(c) The set union
U
[
V is not a subspace of
W unless one of
U or
V is contained in
the other.
16. Let
V and
W be subspaces of
R
3 given by
V
=
f(x;
y
;
z
)
j
x
=
y
=
z
2
Rgand
W
=
f(x;
y
;
0)
j
x;
y
2
Rg
Show that
V
+
W
=
R
3 and
V
\
W
=
f0g:
17. Let
V be the space of
2

2 matrices and associate with each
A
2
V the vector
v
ec
(A)
2
R
4 obtained from
A by stacking the columns of
A underneath each other.
(For example,
v
ec
(

1
2
 1
1

)
=
(1;
 1;
2;
1):) Prove the following
(a) The vec operation establishes a one-to-one correspondence between matrices in
V
and vectors in
R
4
:
(b) The vec operation preserves operations in the sense that if
A;
B are matrices and
c;
d
scalars, then
v
ec
(cA
+
dB
)
=
c
v
ec
(A)
+
d
v
ec
(B
)
18. You will need a computer algebra system (CAS) such as Mathematica or Maple for
this exercise. Use the matrix
A
=
2
4
1
0
2
1
 1
0
1
0
1
3
5
and the translation method of the preceding exercise to turn powers of
A into vectors.
Then use your CAS to ï¬nd a spanning set (or basis, which is a special spanning set) for
subspaces
V
k
=
spanfA
0
;
A
1
;
:
:
:
;
A
k
g;
k
=
1;
2;
3;
4;
5;
6: Based on this evidence,
how many matrices will be required for a span of
V
k
? (Remember that
A
0
=
I
:)
3.3. Linear Combinations
We have seen in Section 3.2 that linear combinations give us a rich source of subspaces
for a vector space. In this section we will take a closer look at properties of linear
combinations.

3.3. LINEAR COMBINATIONS
143
Linear Dependence
First we need to make precise the idea of redundant vectors that we encountered in
Example 3.2.10. About lists and sets: Lists involve an ordering of elements (they can
just as well be called ï¬nite sequences), while sets donâ€™t really imply any ordering of
elements. Thus, every list of vectors, e.g.,
v
1
;
v
2
;
v
3, gives rise to a unique set of
vectors
fv
1
;
v
2
;
v
3
g: A different list
v
1
;
v
3
;
v
2 may deï¬ne the same set
fv
1
;
v
3
;
v
2
g
=
fv
1
;
v
2
;
v
3
g: Lists can have repeats in them, while sets donâ€™t. For instance, the list
v
1
;
v
2
;
v
1 deï¬nes the set
fv
1
;
v
2
g: The terminology â€œthe vectors
v
1
;
v
2
;
v
3â€ really
means â€œthe set or a list of vectors consisting of
v
1
;
v
2
;
v
3â€; the deï¬nitions below work
perfectly well for either sets or lists.
DEFINITION 3.3.1. The vector
v
i is redundant in the vectors
v
1
;
v
2;
:
:
:
;
v
n if the lin-
ear span
V
=
spanfv
1
;
v
2;
:
:
:
;
v
n
g does not change when
v
i is removed.
EXAMPLE 3.3.2. Which vectors are redundant in the set consisting of
v
1
=

1
0

;
v
2
=

1
1

;
v
3
=

1
2

?
SOLUTION. As in Example 3.2.10, we notice that
v
3
=
( 1)v
1
+
2v
2
Thus any linear combination involving
v
3 can be expressed in terms of
v
1 and
v
2
:
Therefore,
v
3 is redundant in the list
v
1
;
v
2
;
v
3
: But there is more going on here. Letâ€™s
write the equation above in a form that doesnâ€™t single out any one vector:
0
=
( 1)v
1
+
2v
2
+
( 1)v
3
Now we see that we could solve for any of
v
1
;
v
2
;
v
3 in terms of the remaining two
vectors. Therefore, each of these vectors is redundant in the set. However, this doesnâ€™t
mean that we can discard all three and get the same linear span. This is obviously false.
What we can do is discard any one of them, then start over and examine the remaining
set for redundant vectors.
This example shows that what really counts for redundancy is that the vector in question
occur with a nonzero coefï¬cient in a linear combination that equals
0: This situation
warrants its own name:
DEFINITION 3.3.3. The vectors
v
1
;v
2
;
:
:
:
;v
n are said to be linearly dependent if
Linearly
Dependant or
Independent
Vectors
there exist scalars
c
1
;
c
2;
:
:
:
;
c
n, not all zero, such that
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
=
0
(3.3.1)
If these vectors are not linearly dependent, i.e., no nontrivial linear combination of them
is equal to zero, then they are called linearly independent.
For convenience, we will call a linear combination trivial if it equals
0: Just as with re-
dundancy, linear dependence or independence is a property of the list or set in question,
not of the individual vectors. We are going to connect the ideas of linear dependence
and redundancy. Here is the key fact.

144
3. VECTOR SPACES
THEOREM 3.3.4. The list of vectors
v
1
;
v
2;
:
:
:
;
v
n of a vector space has redundant
Redundancy
Test
vectors if and only if it is linearly dependent, in which case the redundant vectors are
those that occur with nonzero coefï¬cient in some linear combination that sums to zero.
PROOF. Observe that if (3.3.1) holds and some scalar, say
c
1, is nonzero, then we
can use the equation to solve for
v
1 as a linear combination of the remaining vectors:
v
1
=
 1
c
1
(c
2
v
2
+
c
3
v
3
+



+
c
n
v
n
)
Thus we see that any linear combination involving
v
1
;
v
2;
:
:
:
;
v
n can be expressed
using only
v
2
;
v
3;
:
:
:
;
v
n
: It follows that
span
fv
2
;
v
3;
:
:
:
;
v
n
g
=
span
fv
1
;
v
2;
:
:
:
;
v
n
g
Conversely, if these spans are equal then
v
1 belongs to the left hand side, so there are
scalars
d
2
;
d
3;
:
:
:
;
d
n such that
v
1
=
d
2
v
2
+
d
3
v
3
+



+
d
n
v
n
Now bring all terms to the right hand side and obtain the nontrivial linear combination
 v
1
+
d
2
v
2
+
d
3
v
3
+



+
d
n
v
n
=
0
All of this works equally well for any index other than
1 so the theorem is proved.
It is instructive to examine the simple case of two vectors
v
1
;
v
2
: What does it mean
to say that these vectors are linearly dependent? Simply that one of the vectors can be
expressed in terms of the other. In other words, that each vector is a scalar multiple
of the other. However, matters are more complex when we proceed to three or more
vectors, a point that is often overlooked. So we issue a warning here.
Caution: If we know that
v
1
;
v
2;
:
:
:
;
v
n is linearly dependent, it does not necessarily
imply that one of these vectors is a multiple of one of the others unless
n
=
2: In
general, all we can say is that one of these vectors is a linear combination of the others.
EXAMPLE 3.3.5. Which of the following lists of vectors have redundant vectors, i.e.,
are linearly dependent?
(a)
2
4
1
1
0
3
5
;
2
4
0
1
1
3
5
;
2
4
1
 1
 2
3
5
(b)
2
4
0
1
0
3
5
;
2
4
1
1
0
3
5
;
2
4
0
1
1
3
5
(c)
2
4
1
1
0
3
5
;
2
4
2
1
0
3
5
;
2
4
1
1
0
3
5
SOLUTION. Consider the vectors in each list to be designated as
v
1
;
v
2
;
v
3
: Letâ€™s try to
see the big picture. The general linear combination can be written as
c
1
v
1
+
c
2
v
2
+
c
3
v
3
=
[v
1
;
v
2
;
v
3
]
2
4
c
1
c
2
c
3
3
5
=
Ac
where
A
=
[v
1
;
v
2
;
v
3
] and
c
=
(c
1
;
c
2
;
c
3
): Now we see that a nontrivial linear com-
bination that adds up to
0 amounts to a nontrivial solution to the homogeneous equation
Ac
=
0: We know how to ï¬nd these! In case (a) we have that
2
4
1
0
1
1
1
 1
0
1
 2
3
5
       !
E
21
( 1)
2
4
1
0
1
0
1
 2
0
1
 2
3
5
       !
E
32
( 1)
2
4
1
0
1
0
1
 2
0
0
0
3
5

3.3. LINEAR COMBINATIONS
145
so that the solutions to the homogeneous system are
c
=
( c
3
;
2c
3
;
c
3
)
=
c
3
( 1;
2;
1):
Take
c
3
=
1 and we have that
 1v
1
+
2v
2
+
1v
3
=
0
which shows that
v
1
;
v
2
;
v
3 is a linearly dependent list of vectors.
Weâ€™ll solve (b) by a different method. Notice that
det
2
4
0
1
0
1
1
1
0
0
1
3
5
=
 1
det

1
0
0
1

=
 1:
It follows that
A is nonsingular, so the only solution to the system
Ac
=
0 is
c
=
0:
Since every linear combination of the columns of
A takes the form
Ac; the vectors
v
1
;
v
2
;
v
3 must be linearly independent.
Finally, we see by inspection in (c) that since
v
3 is a repeat of
v
1, the linear combination
v
1
+
0v
2
 v
3
=
0:
Thus, this list of vectors is linearly dependent. Notice, by the way, that not every coef-
ï¬cient
c
i has to be nonzero.
EXAMPLE 3.3.6. Show that any list of vectors that contains the zero vector is linearly
dependent.
SOLUTION. Let
v
1
;
v
2
;
:
:
:
;
v
n be such a list and suppose that for some index
j,
v
j
=
0: Examine the following linear combination which clearly sums to
0:
0v
1
+
0v
2
+



+
1v
j
+



0v
n
=
0:
This linear combination is nontrivial because the coefï¬cient of the vector
v
j is
1: There-
fore this list is linearly dependent by deï¬nition of dependence.
The Basis Idea
We are now ready for one of the big ideas of vector space theory, the notion of a basis.
We already know what a spanning set for a vector space
V is. This is a set of vec-
tors
v
1
;
v
2
;
:
:
:
;
v
n such that
V
=
span
fv
1
;
v
2
;
:
:
:
;
v
n
g: However, we saw back in
Example 3.2.10 that some spanning sets are better than others because they are more
economical. We know that a set of vectors has no redundant vectors in it if and only if it
is linearly independent. This observation is the inspiration for the following deï¬nition.
DEFINITION 3.3.7. A basis for the vector space
V is a spanning set of vectors
v
1
;
v
2
;
:
:
:
;
v
nwhich is a linearly independent set.
We should take note here that we could have just as well deï¬ned a basis as a mini-
mal spanning set, by which we mean a spanning set such that any proper subset is not
spanning set. The proof that this is equivalent to our deï¬nition of basis is left as an
exercise.
Usually we think of a basis as a set of vectors and the order in which we list them
is convenient but not important. Occasionally, ordering is important. In such a situa-
tion we speak of an ordered basis of
v by which we mean a spanning list of vectors
v
1
;
v
2
;
:
:
:
;
v
n which is a linearly independent list.

146
3. VECTOR SPACES
EXAMPLE 3.3.8. Which subsets of
fv
1
;
v
2
;
v
3
g
=
f

1
0

;

1
1

;

1
2

g yield bases
of the vector space
R
2?
SOLUTION. These are just the vectors of Example 3.2.10 and Example 3.3.2. Referring
back to that Example, we saw that
 v
1
+
2v
2
 v
3
=
0
which told us that we could remove any one of these vectors and get the same span.
Moreover, we saw that these three vectors span
R
2, so the same is true of any two of
them. Clearly, a single vector cannot span
R
2 since the span of a single vector is a line
through the origin. Therefore, the subsets
fv
1
;
v
2
g,
fv
2
;
v
3
g and
fv
1
;
v
3
g are all bases
of
R
2
:
An extremely important generic type of basis is provided by the columns of the identity
matrix. For future reference, we establish this notation.
NOTATION 3.3.9. The standard basis of
R
n or
C
n is the set
fe
1
;
e
2
;
:
:
:
;
e
n
g where
e
j
is the column vector of size
n whose
jth entry is
1 and all other entries
0:
EXAMPLE 3.3.10. Let
V be the standard vector space
R
n or
C
n
: Verify that the stan-
dard basis really is a basis of this vector space.
SOLUTION. Let
v
=
(c
1
;
c
2
;
:
:
:
;
c
n
) be a vector from
V so that
c
1
;
c
2
;
:
:
:
;
c
n are
scalars of the appropriate type. Now we have
v
=
2
6
6
6
4
c
1
c
2
...
c
n
3
7
7
7
5
=
c
1
2
6
6
6
4
1
0
...
0
3
7
7
7
5
+
c
2
2
6
6
6
4
0
1
...
0
3
7
7
7
5
+



+
c
n
2
6
6
6
4
0
...
0
1
3
7
7
7
5
=
s
1
e
1
+
s
2
e
2
+



+
s
n
e
n
This equation tells us two things: ï¬rst, every vector in
V is a linear combination of the
e
jâ€™s, so
V
=
span
fe
1
;
e
2
;
:
:
:
;
e
n
g: Secondly, if some linear combination of vectors
sums to the zero vector, then each scalar coefï¬cient of the combination is
0: Therefore,
these vectors are linearly independent. Therefore the set
fe
1
;
e
2
;
:
:
:
;
e
n
g is a basis of
V
:
Coordinates
In the case of the standard basis
e
1
;
e
2
;
;
e
3 of
R
3 we know that it is very easy to write
out any other vector
v
=
(c
1
;
c
2
;
c
3
) in terms of the standard basis:
v
=
2
4
c
1
c
2
c
3
3
5
=
c
1
e
1
+
c
2
e
2
+
c
3
e
3
We call the scalars
c
1
;
c
2
;
c
3 the coordinates of the vector
v
: Up to this point, this is
the only sense in which we have used the term â€œcoordinates.â€ We can see that these
coordinates are strongly tied to the standard basis. Yet
R
3 has many bases. Is there a

3.3. LINEAR COMBINATIONS
147
corresponding notion of â€œcoordinatesâ€ relative to other bases? The answer is a deï¬nite
â€œyes,â€ thanks to the following fact.
THEOREM 3.3.11. Let
v
1
;
v
2
;
:
:
:
;
v
n be a basis of the vector space
V
: Then every
v
2
V can be expressed uniquely as a linear combination of
v
1
;
v
2
;
:
:
:
;
v
n up to
order of terms.
PROOF. To see this, note ï¬rst that since
V
=
span
fv
1
;
v
2
;
:
:
:
;
v
n
g
there exist scalars
c
1
;
c
2
;
:
:
:
;
c
n such that
v
=
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
Suppose also that we could write
v
=
d
1
v
1
+
d
2
v
2
+



+
d
n
v
n
Subtract these two equations and obtain
0
=
(c
1
 d
1
)v
1
+
(c
2
 d
2
)v
2
+



+
(c
n
 d
n
)v
n
However, a basis is a linearly independent set, so it follows that each coefï¬cient of this
equation is zero, whence
c
j
=
d
j
; for
j
=
1;
2;
:
:
:
;
n:
In view of this fact, we may speak of coordinates of a vector relative to a basis. Here is
the notation that we employ:
DEFINITION 3.3.12. If
v
1
;
v
2
;
:
:
:
;
v
n is a basis of the vector space
V and
v
2
V with
v
=
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n, then the scalars
c
1
;
c
2
;
:
:
:
;
c
n are called the coordinates
of
v with respect to the basis
v
1
;
v
2
;
:
:
:
;
v
n.
As we have noted, coordinates of a vector with respect to the standard basis are what
we have referred to as â€œcoordinatesâ€ so far in this text. Perhaps we should call these the
standard coordinates of a vector, but we will usually stick with the convention that an
unqualiï¬ed reference to a vectors coordinates assumes we mean standard coordinates.
Normally vectors in
R
n are given explicitly in terms of their standard coordinates, so
these are trivial to identify. Coordinates with respect to other bases are fairly easy to
calculate if we have enough information about the structure of the vector space.
EXAMPLE 3.3.13. The following vectors form a basis of
R
3:
v
1
=
(1;
1;
0),
v
2
=
(0;
2;
2) and
v
3
=
(1;
0;
1): Find the coordinates of
v
=
(2;
1;
5) with respect to this
basis.
SOLUTION. Notice that the basis
v
1
;
v
2
;
v
3 was given in terms of standard coordinates.
Begin by writing
v
=
2
4
2
1
5
3
5
=
c
1
v
1
+
c
2
v
2
+
c
3
v
3
=
[v
1
;
v
2
;
v
3
]
2
4
c
1
c
2
c
3
3
5
=
2
4
1
0
1
1
2
0
0
2
1
3
5
2
4
c
1
c
2
c
3
3
5

148
3. VECTOR SPACES
where the coordinates
c
1
;
c
2
;
c
3 of
v relative to the basis
v
1
;
v
2
;
v
3 are to be determined.
This is a straightforward system of equations with coefï¬cient matrix
A
=
[v
1
;
v
2
;
v
3
]
and right hand side
v
: It follows that the solution we want is given by
2
4
c
1
c
2
c
3
3
5
=
2
4
1
0
1
1
2
0
0
2
1
3
5
 1
2
4
2
1
5
3
5
=
2
4
1
2
1
2
 1
2
 1
4
1
4
1
4
1
2
 1
2
1
2
3
5
2
4
2
1
5
3
5
=
2
4
 1
1
3
3
5
This shows us that
v
=
 1
2
4
1
1
0
3
5
+
1
2
4
0
2
2
3
5
+
3
2
4
1
0
1
3
5
It does not prove that
v
=
( 1;
1;
3), which is plainly false. Only in the case of the
standard basis can we expect that a vector actually equals its vector of coordinates with
respect to the basis.
In general, vectors
v
1
;
v
2
;
:
:
:
;
v
n
2
R
n are linearly independent if and only if the
system
Ac
=
0 has only the trivial solution, where
A
=
[v
1
;
v
2
;
:
:
:
;
v
n
]: This in turn is
equivalent to the matrix
A being of full column rank
n (see Theorem 2.5.9 of Chapter 2
where we see that these are equivalent conditions for a matrix to be invertible). We can
see how this idea can be extended, and doing so tells us something remarkable. Let
v
1
;
v
2
;
:
:
:
;
v
k be a basis of
V
=
R
n and form the
n

k matrix
A
=
[v
1
;
v
2
;
:
:
:
;
v
k
]:
By the same reasoning as in the example, for any
b
2
V there is a unique solution to
the system
Ax
=
b: In view of the Rank Theorem of Chapter 1 we see that
A has full
column rank. Therefore,
k

n: On the other hand, we can take
b to be any one of
the standard basis vectors
e
j,
j
=
1;
2;
:
:
:
;
n, solve the resulting systems and stack the
solutions vectors together to obtain a solution to the system
AX
=
I
n
: From our rank
inequalities, we see that
n
=
rank
I
n
=
rank
AX

rank
A
=
n
What this shows is that
k
=
n; that is, every basis of
R
n has exactly
n elements in it.
Does this idea extend to abstract vector spaces? Indeed it does, and we shall return to
this issue in Section 3.5.
We are going to visit a problem which comes to us straight from calculus and analytical
geometry (classiï¬cation of conics) and show how the matrix and coordinate tools we
have developed can shed light on this problem.
EXAMPLE 3.3.14. Suppose we want to understand the character of the graph of the
curve
x
2
 xy
+
y
2
 6
=
0: It is suggested to us that if we that we execute a change of
variables by rotating the
xy-axis by

=4 to get a new
x
0
y
0-axis, the graph will become
more intelligible. OK, we do it. The algebraic connection between the variables
x;
y
and
x
0
;
y
0, resulting from a rotation of
 can be worked out using a bit of trigonometry
(which we omit) to yield
x
0
=
x
cos

+
y
sin

y
0
=
 x
sin

+
y
cos


3.3. LINEAR COMBINATIONS
149
yâ€™
x
y
e1
u1
xâ€™
u2
e2
FIGURE 3.3.1. Change of variables and the curve
x
2
 xy
+
y
2
 6
=
0:
Use matrix methods to formulate these equations and execute the change of variables.
SOLUTION. First, we write the change of variable equations in matrix form as
x
0
=

x
0
y
0

=

cos

sin

 sin

cos



x
y

=
G(
)x
We might recognize
G(
) as the Givens matrix introduced in Exericse x. This matrix
isnâ€™t exactly what we need for substitution into our curve equation. Rather, we need
x;
y
explicitly. Thatâ€™s easy enough. Simply invert
G(
) to obtain the rotation matrix
R
(
)
as
G(
)
 1
=
R
(
)
=

cos

 sin

sin

cos


so that
x
=
R
(
)x
0
: Now observe that the original equation can be put in the form
x
2
 xy
+
y
2
 6
=
x
T

1
 1
2
 1
2
1

x
 6
=
x
T
R(
)
T

1
 1
2
 1
2
1

R(
)x
 6
We leave it as an exercise to check that with

=

=4 the equation reduces to
1
2
(x
0
2
+
3y
0
2
)
 6
=
0 or equivalently
x
0
2
12
+
y
0
2
4
=
1
This curve is simply an ellipse with axes
2
p
3 and
2. With respect to the
x
0
y
0-axes, this
ellipse is in so-called â€œstandard form.â€ For a graph of the ellipse, see Figure 3.3.1.
The change of variables we have just seen can be interpreted as a change of coordinates
in the following sense. Notice that the variables
x and
y are just the standard coordinates

150
3. VECTOR SPACES
(with respect to the standard basis
e
1
;
e
2 ) of a general vector
x
=

x
y

=
x

1
0

+
y

0
1

=
xe
1
+
y
e
2
The meaning of the variables
x
0 and
y
0 becomes clear when we set
x
0
=
(x
0
;
y
0
) and
write the matrix equation
x
=
R
(
)x
0 out in detail as a linear combination of the
columns of
R
(
):
x
=
R
(
)x
0
=
x
0

cos

sin


+
y
0

 sin

cos


=
x
0
u
1
+
y
0
u
2
Thus the numbers
x
0 and
y
0 are just the coordinates of the vector
x with respect to a new
basis
u
1
;
u
2 of
R
2. This basis consists of unit vectors in the direction of the
x
0 and
y
0
axes. See Figure 3.3.1 for a picture of the two bases. Thus we see that the matrix
R
(
)
could reasonably be called a change of coordinates matrix or, as it is more commonly
called, a change of basis matrix. Indeed, we can see from this example that the change
of variables we encountered is nothing more than a change from one basis (the standard
one,
e
1
;
e
2) to another (u
1
;
u
2). The reason for a change of basis is that sometimes a
problem looks a lot easier if we look at it using a basis other than the standard one, such
as in our example. The concept of change of basis is explored in more generality in
Section 3.7.
3.3 Exercises
1. Which of the following sets are linearly independent in
V
=
R
3? If not linearly
independent, which vectors are redundant in the lists?
(a)
(1;
0;
1);
(1;
 1;
1)
(b)
(1;
2;
1);
(2;
1;
1);
(3;
4;
1);
(2;
0;
1)
(c)
(1;
0;
 1);
(1;
1;
0);
(1;
 1;
 2)
(d)
(0;
1;
 1);
(1;
0;
0);
( 1;
1;
3)
2. Which of the following sets are linearly independent in
V
=
P
2
? If not linearly
independent, which vectors are redundant in the lists?
(a)
x;
5x
(b)
2;
2
 x;
x
2
;
1
+
x
2 (c)
1
+
x;
1
+
x
2
;
1
+
x
+
x
2 (d)
x
 1;
x
2
 1;
x
+
1
3. Which of the following sets are linearly independent in
V
=
P
3? If not linearly
independent, which vectors are redundant in the lists
(a)
1;
x;
x
2
;
x
3
(b)
1
+
x;
1
+
x
2
;
1
+
x
3
(c)
1
 x
2
;
1
+
x;
1
 x
 2x
2
(d)
x
2
 x
3
;
x;
 x
+
x
2
+
3x
3
4. Find the coordinates of
v with respect to the following bases:
(a)
v
=
(0;
1;
2) with respect to basis
(2;
0;
1);
( 1;
1;
0);
(0;
1;
1) of
R
3
:
(b)
v
=2
+
x
2 with respect to basis
1
+
x;
x
+
x
2
;
1
 x of
P
2
:
(c)
v
=

a
b
b
c

with respect to basis

0
1
1
0

;

1
0
0
0

;

0
0
0
1

of the space
of real symmetric
2

2 matrices.
(d)
v
=
(1;
2) with respect to basis
(2
+
i;
1);
( 1;
i) of
C
2
:

3.3. LINEAR COMBINATIONS
151
5. In the following,
u
1
=
(1;
0;
1) and
u
2
=
(1;
 1;
1):
(a) Determine if
v
=
(2;
1;
2) belongs to the space
span
fu
1
;
u
2
g:
(b) Find a basis of
R
3 which contains
u
1 and
u
2
:
6. In the following,
u
1
=
1
 x
+
x
2 and
u
2
=
x
+
2x
2
:
(a) Determine if
v
=
4
 7x
 2x
2 belongs to the space
spanfu
1
;
u
2
g:
(b) Find a basis of
R
3 which contains
u
1 and
u
2
:
7. If
2v
1
+
v
3
+
v
4
=
0 and
v
2
+
v
3
=
0 then what is the smallest spanning set
span
fv
1
;
v
2
;
v
3
;
v
4
g may be reduced to?
8. Let
V
=
R
n;n be the vector space of real
n

n matrices and let
A;
B
2
R
n;n
such that both are nonzero matrices,
A is nilpotent (some power of
A is zero) and
B is
idempotent (B
2
=
B). Show that the subspace
W
=
spanfA;
B
g cannot be spanned
by a single element of
W
:
9. Let
V be a vector space whose only subspaces are
f0g and
V
: Show that
V is the
span of a single vector.
10. Suppose that
v
1
;
v
2
;
:
:
:
;
v
k are linearly independent elements of
R
n and
A
=
[v
1
;
v
2
;
:
:
:
;
v
k
]: Show that
rank
A
=
k
:
11. Determine a largest subset of the following set of matrices which is linearly inde-
pendent. Then expand this subset to a basis of
R
2;2
:
A
=

1
1
1
1

;
B
=

0
1
1
0

;
C
=

0
0
1
1

12. The Wronskian of smooth functions
f
(x);
g
(x);
h(x) is deï¬ned as follows (a simi-
lar deï¬nition can be made for any number of functions):
W
(f
;
g
;
h)(x)
=
det
2
4
f
(x)
g
(x)
h(x)
f
0
(x)
g
0
(x)
h
0
(x)
f
00
(x)
g
00
(x)
h
00
(x)
3
5
Calculate the Wronskians of the three polynomial functions in parts (b) and (c) of Ex-
ercise 3.
13. Show that if
f
(x);
g
(x);
h(x) are linearly dependent smooth functions, then for all
x;
W
(f
;
g
;
h)(x)
=
0
14. Show that the functions
e
x
;
x
3 and
sin(x) are linearly independent in
C
[0;
1] in
two ways:
(a) Use Exercise 13.
(b) Assume a linear combination sums to zero and evaluate it at various points to obtain
conditions on the coefï¬cients.
15. Prove that a list of vectors
v
1
;
v
2
;
:
:
:
;
v
n with repeated vectors in it is linearly
dependent.
16. Show that a linear operator
T
:
V
!
W maps a linearly dependent set
v
1
;
v
2
;
:
:
:
;
v
n
to linearly dependent set
T
(v
1
);
T
(v
2
);
:
:
:
;
T
(v
n
); but if
v
1
;
v
2
;
:
:
:
;
v
n is linearly in-
dependent,
T
(v
1
);
T
(v
2
);
:
:
:
;
T
(v
n
) need not be linearly independent (give a speciï¬c
counterexample).

152
3. VECTOR SPACES
17. Let
R
(
)
=

cos

 sin

sin

cos


and
A
=

1
 1
2
 1
2
1

: Calculate
R
(
)
T
AR
(
)
in the case that

=

=4:
18. Use matrix methods as in Example 3.3.14 to express the equation of the curve
11x
2
+
10
p
3xy
+
y
2
 16
=
0 in new variables
x
0
;
y
0 obtained by rotating the
xy-axis
by an angle of

=4:
19. Suppose that a linear change of variables from old coordinates
x
1
;
x
2 to new coor-
dinates
x
0
1
;
x
0
2 is deï¬ned by the equations
x
1
=
p
11
x
0
1
+
p
12
x
0
2
x
2
=
p
21
x
0
1
+
p
22
x
0
2
where the
2

2 change of basis matrix
P
=
[p
ij
] is invertible. Show that if a linear
matrix multiplication function
T
A
:
R
2
!
R
2 is given in old coordinates by
y
=

y
1
y
2

=
T
A

x
1
x
2


=
T
A
(x)
=
Ax
where
A
=
[a
ij
] is any
2

2 matrix, then it is given by
y
0
=P
 1
AP
x
0
=
T
P
 1
AP
(x
0
)
in new coordinates. Hint: Both domain and range elements
x and
y are given in terms
of old coordinates. Express them in terms of new coordinates.
3.4. Subspaces Associated with Matrices and Operators
Certain subspaces are a rich source of information about the behavior of a matrix or
a linear operator. We will deï¬ne and explore the properties of these subspaces in this
section.
Subspaces Deï¬ned by Matrices
Suppose we are given a matrix A. There are three very useful subspaces that can be
associated with the matrix
A: Understanding these subspaces is a great aid in vector
space calculations that might have nothing to do with matrices per se, such as the deter-
mination of a minimal spanning set for a vector space. We shall follow each deï¬nition
below by an illustration using the following example matrix.
A
=

1
1
1
 1
0
1
2
1

(3.4.1)
We make the default assumption that the scalars are the real numbers, but the deï¬nitions
we will give can be stated just as easily for the complex numbers.

3.4. SUBSPACES ASSOCIATED WITH MATRICES AND OPERATORS
153
DEFINITION 3.4.1. The column space of the
m

n matrix
A is the subspace
C
(A) of
Column Space
R
m spanned by the columns of
A.
EXAMPLE 3.4.2. Describe the column space of the matrix
A in Equation 3.4.1
SOLUTION. Here we have that
C
(A)

R
2
: Also
C
(A)
=
span
f

1
0

;

1
1

;

1
2

;

 1
1

g
Technically, this describes the column space in question, but we can do much better. We
saw in Example 3.2.10 that the vector

1
2

was really redundant since it is a linear
combination of the ï¬rst two vectors. We also see that

 1
1

=
 2

1
0

+
1

1
1

so that Theorem 3.2.9 shows us that
C
(A)
=
span
f

1
0

;

1
1

g
This description is much better, in that it exhibits a basis of
C
(A):
This example shows that not all the columns of the matrix
A are really needed to span
the entire subspace
C
(A): Clearly, this last expression for
C
(A) is much more econom-
ical than the ï¬rst. How do you think it compares to the containing space
R
2?
DEFINITION 3.4.3. The row space of the
m

n matrix
A is the subspace
R(A) of
R
n
spanned by the transposes of the rows of
A:
The â€œtransposeâ€ part of the preceding deï¬nition seems a bit odd. Why would we want
rows to look like columns? Itâ€™s a technicality, but later it will be convenient for us
to have the row spaces live inside a
R
n instead of an
(R
n
)
T
: Remember, we had to
make a choice about
R
n consisting of rows or columns. Just to make the elements of a
row space look like rows, we can always adhere to the tuple notation instead of matrix
notation.
EXAMPLE 3.4.4. Describe the row space of
A in Equation 3.4.1
SOLUTION. We have from deï¬nition that
R(A)
=
spanf(1;
1;
1;
 1);
(0;
1;
2;
1)g

R
4
Now itâ€™s easy to see that neither one of these vectors can be expressed as a multiple
of the other (if we had
c(1;
1;
1;
 1)
=
(0;
1;
2;
1), then read the ï¬rst coordinates and
obtain
c
=
0), so that span is given as economically as we can do, that is, the two vectors
listed constitute a basis of
R(A):
DEFINITION 3.4.5. The null space of the
m

n matrix
A is the subset
N
(A) of
R
n
deï¬ned by
N
(A)
=
fx
2
R
n
j
Ax
=
0g

154
3. VECTOR SPACES
Observe that
N
(A) is the solution set to the homogeneous linear system
Ax
=
0: This
means that null spaces are really very familiar. We were computing these solution sets
back in Chapter 1. We didnâ€™t call them subspaces at the time. Here is an application
of this concept. Let
A be a square matrix. We know that
A is invertible exactly when
the system
Ax
=
0 has only the trivial solution (see Theorem 2.5.9). Now we can
add one more equivalent condition to the long list of equivalences for invertibility:
A is
invertible exactly when
N
(A)
=
f0g: Let us next justify the subspace property implied
by the term â€œnull space.â€
EXAMPLE 3.4.6. Use the subspace test to verify that
N
(A) really is a subspace of
R
n
:
SOLUTION. Since
A0
=
0, the zero vector is in
N
(A). Now let
c be a scalar and
u;
v
2
R
n arbitrary elements of
N
(A): By deï¬nition,
Au
=
0 and
Av
=
0: Add these
two equations to obtain that
0
=
0
+
0
=
Au
+
Av
=
A(u
+
v
)
Therefore
u
+
v
2
N
(A): Next multiply the equation
Au
=
0 by the scalar
c to obtain
0
=
c0
=
c(Au)
=
A(cu)
Thus we see from that deï¬nition that
cu
2
N
(A): The subspace test implies that
N
(A)
is a subspace of
R
n
:
EXAMPLE 3.4.7. Describe the null space of the matrix
A of Equation 3.4.1.
SOLUTION. Proceed as in Chapter 1. We ï¬nd the reduced row echelon form of
A,
identify the free variables and solve for the bound variables using the implied zero right
hand side and solution vector
x
=
[x
1
;
x
2
;
x
3
;
x
4
]
T :

1
1
1
 1
0
1
2
1

       !
E
12
( 1)

1
0
 1
 2
0
1
2
1

Pivots are in the ï¬rst and second columns, so it follows that
x
3 and
x
4 are free,
x
1 and
x
2 are bound and
x
1
=
x
3
+
2x
4
x
2
=
 2x
3
 x
4
Letâ€™s write out the form of a general solution, which will be strictly in terms of the free
variables, and write the result as a combination of
x
3 times some vector plus
x
4 times
another vector:
2
6
6
4
x
1
x
2
x
3
x
4
3
7
7
5
=
2
6
6
4
x
3
+
2x
4
 2x
3
 x
4
x
3
x
4
3
7
7
5
=
x
3
2
6
6
4
1
 2
1
0
3
7
7
5
+
x
4
2
6
6
4
2
 1
0
1
3
7
7
5
This is really a wonderful trick! Remember that free variables can take on arbitrary
values, so we see that the general solution to the homogeneous system has the form of
an arbitrary linear combination of the two vectors on the right. In other words,
N
(A)
=
span
f
2
6
6
4
1
 2
1
0
3
7
7
5
;
2
6
6
4
2
 1
0
1
3
7
7
5
g

R
4

3.4. SUBSPACES ASSOCIATED WITH MATRICES AND OPERATORS
155
Neither of these vectors is a multiple of the other, so this is as economical an expression
for
N
(A) as we can hope for. In other words, we have exhibited a minimal spanning
set, that is, a basis of
N
(A).
EXAMPLE 3.4.8. Suppose that we have a sequence of vectors
x
(k
)
;
k
=
0;
1;
:
:
:
:
which are related by the formula
x
(k
+1)
=
Ax
(k
)
where
x
(k
)
=

a
k
b
k

and
A
=

0:7
0:4
0:3
0:6

Further, suppose these vectors converge to some limiting vector
x: What could the lim-
iting vector be? (This example relates null spaces to the idea of a limiting state for a
Markov chain as discussed in Example 2.3.4 of Chapter 2.)
SOLUTION. We reason as follows: since the limit of the sequence is
x, we can take the
limits of both sides of the matrix equation above and obtain that
x
=
Ax
=
I
x
so that
0
=
x
 Ax
=
I
x
 Ax
=
(I
 A)x
It follows that
x
2
N
(I
 A) . Now
I
 A
=

1
0
0
1

 
0:7
0:4
0:3
0:6

=

0:3
 0:4
 0:3
0:4

Calculate the null space by Gauss-Jordan elimination:

0:3
 0:4
 0:3
0:4

        !
E
21
(1)
E
1
(1=0:3)

1
 4=3
0
0

Therefore the null space of
I
 A is spanned by the single vector
4=3;
1): In particular,
any multiple of this vector qualiï¬es as a possible limiting vector. If we want a limiting
vector whose entries are nonnegative and sum to
1, then the only choice is the vector
resulting from dividing
(4=3;
1) by the sum of its coordinates to obtain
(3=7)(4=3;
1)
=
(4=7;
3=7)

(0:57143;
0:42857)
:
Interestingly enough, this is the vector that resulted from the calculation on page 66.
An important point that comes out of the previous examples is that we can express a null
space as a simple linear span by using the methods for system solving we developed in
Chapter 1, together with a little algebra.
We conclude this excursion on subspaces with an extremely powerful way of thinking
about the product
Ax; which was ï¬rst introduced in Example 2.2.6 of Chapter 2. We
shall use this idea often.
THEOREM 3.4.9. Let the matrix A have columns
a
1
;
:::;
a
n, i.e.,
A
=
[a
1
;
a
2
;
:::;
a
n
]:
Let
x
=
[x
1
;
x
2
;
:::;
x
n
]
T
: Then
Ax
=
x
1
a
1
+
x
2
a
2
+



+
x
n
a
n

156
3. VECTOR SPACES
In other words,
Ax is simply a linear combination of the columns of
A with vector of
coefï¬cients
x:
PROOF. As usual, let
A
=
[a
ij
]: The
ith entry of the vector
v
=
Ax is, by deï¬ni-
tion of matrix multiplication
v
i
=
a
i1
x
1
+
a
i2
x
2
+



+
a
in
x
n
However,
a
j
=
[a
1j
;
a
2j
;
:
:
:
;
a
nj
]
T so that the
ith entry of the linear combination
x
1
a
1
+
x
2
a
2
+



+
x
n
a
n is
x
1
a
i1
+
x
2
a
i2
+



+
x
n
a
in
=
v
i
which is what we wanted to show.
This theorem shows that the column space of the matrix
A can be thought of as the set
of all possible matrix products
Ax, i.e.,
C
(A)
=
fAx
j
x
2
R
n
g
Furthermore, the coefï¬cients of a linear combination
Ax are precisely the entries of the
vector
x: Two important insights follow from these observations: ï¬rstly, we see that the
linear system
Ax
=
b is consistent exactly when
b
2
C
(A): Secondly, we see that the
linear combination of columns of
A with coefï¬cients from the vector
x is trivial, i.e.,
the combination sums to the zero vector, exactly when
x
2
N
(A):
EXAMPLE 3.4.10. Interpret the linear combination
v
=
3

1
0

 
1
1

+
5

1
2

as a matrix multiplication.
SOLUTION. By using Theorem 3.4.9 we can obtain several interpretations of this linear
combination. The most obvious is
v
=

1
1
1
0
1
2

2
4
3
 1
5
3
5
Notice that the linear combination of the previous example is not trivial, i.e., does not
sum to the zero vector. Also, if we were working with the matrix of Equation 3.4.1, we
could obtain a linear combination with the same value as the previous example in the
form

1
1
1
 1
0
1
2
1

2
6
6
4
3
 1
5
0
3
7
7
5
This gives the same result.

3.4. SUBSPACES ASSOCIATED WITH MATRICES AND OPERATORS
157
Subspaces Deï¬ned by a Linear Operator
Suppose we are given a linear operator
T
:
V
!
W
: We immediately have three
spaces we can associate with the operator, namely, the domain
V
; target
W and range
range
(T
)
=
fy
j
y
=
T
(x) for some
x
2
V
g of the operator. The domain and range
are vector spaces by deï¬nition of linear operator. That that range is a vector space is a
nice example of using the subspace test.
EXAMPLE 3.4.11. Show that if
T
:
V
!
W is a linear operator, then
range
(T
) is a
subspace of
W
:
SOLUTION. Apply the subspace test. First, we observe that
range
(T
) contains
T
(0):
We leave it as an exercise for the reader to check that
T
(0) is the zero element of
W
:
Next let
y and
z be in
range
(T
); say
y
=
T
(u) and
z
=
T
(v
): We show closure of
range
(T
) under addition: by the linearity property of
T
y
+
z
=
T
(u)
+
T
(v
)
=
T
(u
+
v
)
2
range
(T
)
where the latter term belongs to
range
(T
) by deï¬nition of image. Finally, we show
closure under scalar multiplication: let
c be a scalar and we obtain from the linearity
property of
T that
cy
=
cT
(u)
=
T
(cu)
2
range
(T
)
where the latter term belongs to
range
(T
) by deï¬nition of range. Thus, the subspace
test shows that
range
(T
) is a subspace of
W
:
Here is another space that has proven to be very useful in understanding the nature of a
linear operator.
DEFINITION 3.4.12. The kernel of the linear operator
T
:
V
!
W is the subspace of
V given by
k
er
(T
)
=
fx
2
V
j
T
(x)
=
0g
The deï¬nition claims that the kernel is a subspace and not merely a subset of the domain.
The proof of this is left to the exercises. The fact is that we have been computing kernels
since the beginning of the text. To see this, suppose that we have a linear transformation
T
:
R
n
!
R
m given by matrix multiplication, that is,
T
A
(x)
=
Ax; for all
x
2
R
n
:
Then
k
er
(T
)
=
fx
2
R
n
j
T
A
(x)
=
0g
=
fx
2
R
n
j
Ax
=
0g
=
N
(A)
In other words, for matrix multiplications kernels are the same thing as null spaces.
Here is one really nice application of kernels. Suppose we are interested in knowing
whether or not a given operator
T
:
V
!
W is one-to-one, i.e., if the equation
T
(u)
=
T
(v
) necessarily implies that
u
=
v
: In general, this is a nontrivial question. If, for
example,
V
=
W
=
R, then we could graph the function
T and try to determine if a
horizontal line cut the graph twice. But for linear operators, the answer is much simpler:

158
3. VECTOR SPACES
THEOREM 3.4.13. If
T
:
V
!
W is a linear operator, then
T is one-to-one if and only
if
k
er
(T
)
=
f0g:
PROOF. If
T is one-to-one, then only one element can map to
0 under
T
: Thus,
k
er
(T
) can consist of only one element. However, we know that
k
er
(T
) contains the
zero vector since it is a subspace of the domain of
T
: Therefore,
k
er
(T
)
=
f0g:
Conversely, suppose that
k
er
(T
)
=
f0g: If
u and
v are such that
T
(u)
=
T
(v
); then
subtract terms and use the linearity of
T to obtain that
0
=
T
(u)
 T
(v
)
=
T
(u)
+
( 1)T
(v
)
=
T
(u
 v
):
It follows that
u
 v
2
k
er
(T
)
=
f0g: Therefore,
u
 v
=
0 and hence
u
=
v
:
Before we leave the topic of one-to-one linear mappings, letâ€™s digest its signiï¬cance
in a very concrete case. The space
P
2
=
spanf1;
x;
x
2
gof polynomials of degree at
most
2 has a basis of three elements, like
R
3 and it seems very reasonable to think that
P
2 is â€œjust likeâ€
R
3 in that a polynomial
p(x)
=
a
+
bx
+
cx
2 is uniquely described
by its vector of coefï¬cients
(a;
b;
c)
2
R
3 and corresponding polynomials and vectors
add and scalar multiply in a corresponding way. Here is the precise version of these
musings: deï¬ne an operator
T
:
P
2
!
R
3 by the formula
T
(a
+
bx
+
cx
2
)
=
(a;
b;
c):
One can check that
T is linear, the range of
T is its target,
R
3, and
k
er
(T
)
=
0: By
Theorem 3.4.13 the function
T is one-to-one and maps its domain onto its target. Hence,
it describes a one-to-one correspondence between elements of
P
2 and elements of
R
3
such that sums and scalar products in one space correspond to the corresponding sums
and scalar products in the other. In plain words, this means we can get one of the vector
spaces from the other simply by relabelling elements of one of the spaces. So, in a
very real sense, they are â€œthe same thing.â€ More generally, whenever there is a one-to-
one linear mapping of one vector space onto another, we say the two vector spaces are
isomorphic, which is a fancy way of saying that they are the same, up to a relabelling of
elements. A one-to-one and onto linear mapping, like our
T, is called an isomorphism.
In summary, there are four important subspaces associated with a linear operator, the
domain, target, kernel and range. In symbols
domain(T
)
=
V
target
(T
)
=
W
k
er
(T
)
=
fv
2
V
j
T
(v
)
=
0g
range
(T
)
=
fT
(v
)
j
v
2
V
g
There are important connections between these subspaces and those associated with a
matrix. Let
A be an
m

n matrix and
T
A
:
R
n
!
R
m the corresponding operator
deï¬ned by matrix multiplication by
A: Then
domain(T
A
)
=
R
n
target
(T
A
)
=
R
m
k
er
(T
A
)
=
N
(A)
range
(T
A
)
=
C
(A)

3.4. SUBSPACES ASSOCIATED WITH MATRICES AND OPERATORS
159
The proofs of these are left to the exercises. One last example of subspaces associated
with a linear operator
T
:
V
!
W is really a whole family of subspaces. Suppose that
U is a subspace of the domain
V
: Then we deï¬ne the image of
U under
T to be the set
T
(U
)
=
fT
(u)
j
u
2
U
g
Interestingly enough,
T
(U
) is always a subspace of
range
(T
). We leave the proof of
this fact as an exercise. In words, what it says is that a linear operator maps subspaces
of its domain into subspaces of its range.
3.4 Exercises
1. Let
A
=

2
 1
0
4
 2
1

(a) Find the reduced row echelon form of the matrix
A
:
(b) Find a spanning set for the null space of
A: Hint: See Example 3.4.7.
(c) Find a spanning set for the column space of
A:
(d) Find a spanning set for the row space of
A:
(e) Find all possible linear combinations of the columns of
A that add up to
0: Hint: See
the remarks following Theorem 3.4.9.
2. Let
A
=
2
4
1
2
0
0
1
1
2
1
1
1
3
6
2
2
3
3
5 and
b
=
2
4
7


3
5 deï¬ne the system
Ax
=
b: First
ï¬nd the reduced row echelon form for the augmented matrix
[Ajb]. Then answer ques-
tions (a)-(e) of Exercise 1 and also
(f) For what values of
 and
 is the vector
b in the column space of
A? Hint:
The
remarks following Theorem 3.4.9 are helpful.
3. Repeat Exercise 1 with
A
=

1
i
0
1
2
1

and
b
=




:
4. Let
u
=
[1;
1;
0]
T,
v
=
[0;
1;
1]
T, and
w
=
[1;
3
 2i;
1]
T
: Express the expression
2u
 4v
 3w as a single matrix product.
5. The matrix
2
4
0:5
0
0:5
0:5
0:5
0
0
0:5
0:5
3
5 is a transition matrix for a Markov chain. Find the
null space of
I
 A and determine all state vectors (nonnegative entries that sum to 1)
in the null space.
6. Show the range of the linear transformation
T
:
R
3
!
R
3 given by
T
0
@
2
4
x
1
x
2
x
3
3
5
1
A
=
2
4
x
1
 2x
2
+
x
3
x
1
+
x
2
+
x
3
2x
1
 x
2
+
2x
3
3
5
is not
R
3, and ï¬nd a vector not in the range. Is
T one-to-one? Give reasons.
7. Show that if
T
:
V
!
W is a linear operator, then
T
(0)
=
0:

160
3. VECTOR SPACES
8. Show that if
T
:
V
!
W is a linear operator, then the kernel of
T is a subspace of
V
:
9. Prove that if
T is a linear operator, then for all
u;
v in the domain of
T and scalars
c
and
d; we have
T
(cu
 dv
)
=
cT
(u)
 dT
(v
):
10. Prove that if
A is a nilpotent
n

n matrix then
N
(A)
6=
f0g and
N
(I
 A)
=
f0g:
11. Let the function
T
:
R
3
!
P
2 be deï¬ned by
T
(
2
4
a
b
c
3
5
)
=
ax
+
b(x
 1)
+
cx
2
(a) Show that
T is a linear operator.
(b) Show that
k
er
T
=
f0g:
(c) Show that
range
T
=
P
2
:
(d) Conclude that
R
3 and
P
2 are isomorphic vector spaces.
12. Let
T
:
V
!
W be a linear operator and
U a subspace of
V
: Show that the image
of
U;
T
(U
)
=
fT
(v
)
j
v
2
U
g; is a subspace of
W
:
13. Let
T
A
:
R
n
!
R
m be the matrix multiplication operator deï¬ned by the
m

n
matrix
A: Show that
k
er
T
A
=
N
(A) and
range
T
=
C
(A).
3.5. Bases and Dimension
We have used the word â€œdimensionâ€ many times already, without really making the
word precise. Intuitively, it makes sense when we say that
R
2 is â€œtwo dimensionalâ€
or that
R
3 is â€œthree dimensionalâ€: we reason that it takes two coordinate numbers to
determine a vector in
R
2 and three for a vector in
R
3
: What can we say about general
vector spaces? Is there some number that is a measure of the size of the vector space?
These are questions we propose to answer in this section. In the familiar cases of geo-
metrical vector spaces, the answers will merely conï¬rm our intuition. The answers will
also enable us to solve this kind of problem, which is a bit more subtle: suppose we
solve a linear system of
5 equations in
10 unknowns and obtain through Gauss-Jordan
elimination a solution that involves
4 free variables, which means that we can express
all
10 unknowns in terms of these
4 free variables. Is it somehow possible, perhaps by
using a totally different method of system solving, to arrive at a similar kind of solution
that involves fewer free variables, say
3? This would, in effect, reduce the â€œdegrees of
freedomâ€ of the system.
We arenâ€™t going to answer this question just yet. This example is rather vague; nonethe-
less, our intuition might suggest that if the free variables really are independent, we

3.5. BASES AND DIMENSION
161
shouldnâ€™t be able to reduce their number. Therefore the answer to the question of this
example should be â€œno.â€ What we are really asking is a question about the nature of the
solution set of the system. As a matter of fact, our intuition is correct. Reasons for this
answer will be developed in this section.
The Basis Theorem
We now know that the standard vector spaces always have a basis. Given an abstract
vector space
V , can we be sure that a basis for
V exists? For the type of vector spaces
that we introduced in Section 1, that is, subspaces of the standard vector spaces, we
will see that the answer is an unconditional â€œyes.â€ For the more general concept of an
abstract vector space the answer is â€œsometimes.â€ The following concept turns out to be
helpful.
DEFINITION 3.5.1. The vector space
V is called ï¬nite dimensional if there is a ï¬nite
Finite
Dimensional
Vector Space
set of vectors
fv
1
;v
2
;
:
:
:
;v
n
g which is a spanning set for
V
:
Examples of ï¬nite dimensional vector spaces are the standard spaces
R
n or
C
n
: As a
matter of fact, we will see shortly that every subspace of a ï¬nite dimensional vector
space is ï¬nite dimensional, which includes most of the vector spaces we have studied
so far. However, some very important vector spaces are not ï¬nite dimensional, and
accordingly, we call them inï¬nite dimensional spaces. Here is an example.
EXAMPLE 3.5.2. Show that the space of all polynomial functions
P is not a ï¬nite di-
mensional space, while the subspaces
P
n are ï¬nite dimensional.
SOLUTION. If
P were a ï¬nite dimensional space, then there would be a ï¬nite spanning
set of polynomials
p
1
(x);
p
2
(x);
:
:
:
;
p
m
(x) for
P
: This means that any other polyno-
mial could be expressed as a linear combination of these polynomials. Let
m be the
maximum of all the degrees of the polynomials
p
j
(x): Notice that any linear combina-
tion of polynomials of degree at most
m must itself be a polynomial of degree at most
m:
(Remember that polynomial multiplication plays no part here, only addition and scalar
multiplication.) Therefore, it is not possible to express the polynomial
q
(x)
=
x
m+1
as a linear combination of these polynomials, which means that they cannot be a basis.
Hence, the space
P has no ï¬nite spanning set.
On the other hand, it is obvious that the polynomial
p(x)
=
a
0
+
a
1
x
+



+
a
n
x
n
is a linear combination of the monomials
1;
x;
:
:
:
;
x
n from which it follows that
P
n is
a ï¬nite dimensional space.
Here is the ï¬rst basic result about these spaces. It is simply a formalization of what we
have already done with preceding examples.
THEOREM 3.5.3. Every ï¬nite dimensional vector space has a basis.
Basis Theorem
PROOF. To see this, suppose that
V is a ï¬nite dimensional vector space with
V
=
span
fv
1
;
v
2
;
:
:
:
;
v
n
g

162
3. VECTOR SPACES
Now if the set
fv
1
;
v
2
;
:
:
:
;
v
n
g has a redundant vector in it, discard it and obtain a
smaller spanning set of
V
: Continue discarding vectors until you reach a spanning set
for
V that has no redundant vectors in it. (Since you start with a ï¬nite set, this canâ€™t go
on indeï¬nitely.) By the redundancy test, this spanning set must be linearly independent.
Hence it is a basis of
V
:
The Dimension Theorem
No doubt you have already noticed that every basis of the vector space
R
2 must have
exactly two elements in it. Similarly, one can reason geometrically that any basis of
R
3
must consist of exactly three elements. These numbers somehow measure the â€œsizeâ€
of the space in terms of the degrees of freedom (number of coordinates) one needs to
describe a general vector in the space. The content of the dimension theorem is that
this number can be unambiguously deï¬ned. First, we need a very handy theorem which
is sometimes called the Steinitz substitution principle. This principle is a mouthful to
swallow, so we will precede its statement with an example that illustrates the basic idea.
EXAMPLE 3.5.4. Let
w
1
=
(1;
 1;
0);
w
2
=
(0;
 1;
1);
v
1
=
(0;
1;
0);
v
2
=
(1;
1;
0);
v
3
=
(0;
1;
1): Then
w
1
;
w
2 form a linearly independent set and
v
1
;
v
2
;
v
3 form a
basis of
V
=
R
3 (assume this). Show how to substitute both
w
1 and
w
2 into the set
v
1
;
v
2
;
v
3 while substituting out some of the
v
j and at the same time retaining the basis
property of the set.
SOLUTION. Since
R
3
=
span
fv
1
;
v
2
;
v
3
g, we can express
w
1 as a linear combination
of these vectors. We have a formal procedure for ï¬nding such combinations, but in this
case we donâ€™t have to work too hard. A little trial and error shows
w
1
=
2
4
1
 1
0
3
5
=
 1
2
4
0
1
0
3
5
+
2
2
4
1
1
0
3
5
=
 1v
1
+
2v
2
+
0v
3
so that
1w
1
+
1v
1
 2v
2
 0v
3
=
0: It follows that
v
1 or
v
2 is redundant in the set
w
1
;
v
1
;
v
2
;
v
3
: So discard, say,
v
2, and obtain a spanning set
w
1
;
v
1
;
v
3
: In fact, it is
actually a basis of
V since two vectors can only span a plane. Now start over: express
w
2 as a linear combination of this new basis. Again, a little trial and error shows
w
2
=
2
4
0
 1
1
3
5
=
 2
2
4
0
1
0
3
5
+
2
4
0
1
1
3
5
=
0w
1
 2v
1
+
1v
3
Therefore
v
1 or
v
3 is redundant in the set
w
1
;
w
2
;
v
1
;
v
3
: So discard, say,
v
3, and
obtain a spanning set
w
1
;
w
2
;
v
1
: Again, this set is actually a basis of
V since two
vectors can only span a plane; and this is the kind of set we were looking for.
THEOREM 3.5.5.
Let
w
1
;
w
2
;
:
:
:
;
w
r be a linearly independent set in the space
V
Steinitz
Substitution
Principle
and let
v
1
;
v
2
;
:
:
:
;
v
n be a basis of
V
: Then
r

n and we may substitute all of the
w
iâ€™s for some of the
v
jâ€™s in such a way that the resulting set of vectors is still a basis of
V
:

3.5. BASES AND DIMENSION
163
PROOF. Letâ€™s do the substituting one step at a time. Suppose that
k
<
r and that
we have relabelled the remaining
v
iâ€™s so that
V
=
spanfw
1
;
w
2
;
:
:
:
w
k
;
v
1
;
v
2
;
:
:
:
;
v
s
g
with
k
+
s

n and
w
1
;
w
2
;
:
:
:
w
k
;
v
1
;
v
2
;
:
:
:
;
v
s a basis of
V
: (Notice that
k
=
0 and
s
=
n when we start, so
k
+
s
=
n:)
We show how to substitute the next vector
w
k
+1 into the basis. Certainly
V
=
span
fw
1
;
w
2
;
:
:
:
w
k
;
w
k
+1
;
v
1
;
v
2
;
:
:
:
;
v
s
g
as well, but this spanning set is linearly dependent since
w
k
+1 is linearly dependent
on
w
1
;
w
2
;
:
:
:
w
k
;
v
1
;
v
2
;
:
:
:
;
v
s
: Also, there have to be some
v
is left if
k
<
r
; for
otherwise a proper subset of the
w
js would be a basis of
V
: Now use the redundancy
test to discard, one at a time, as many of the
v
jâ€™s from this spanning set as possible,
all the while preserving the span. Again relabel the
v
jâ€™s that are left so as to obtain for
some
t

s a spanning set
w
1
;
w
2
;
:
:
:
w
k
+1
;
v
1
;
v
2
;
:
:
:
;
v
t
of
V from which no
v
j can be discarded without shrinking the span.
Could this
set be linearly dependent? If so, there must be some equation of linear dependence
among the vectors such that none of the vectors
v
j occurs with a nonzero coefï¬-
cient; otherwise, according to the redundancy test, such a
v
j could be discarded and
the span preserved. Therefore, there is an equation of dependency involving only the
w
jâ€™s.
This means that the vectors
w
1
;
w
2
;
:
:
:
;
w
r form a linearly dependent set,
contrary to hypothesis. Hence, there is no such linear combination and the vectors
w
1
;
w
2
;
:
:
:
w
k
+1
;
v
1
;
v
2
;
:
:
:
;
v
t are linearly independent, as well as a spanning set of
V
: Now we have to have discarded at least one of the
v
iâ€™s since
w
1
;
w
2
;
:
:
:
w
k
;
w
k
+1
;
v
1
;
v
2
;
:
:
:
;
v
s is a linearly dependent set. Therefore,
t

s
 1: It follows that
(k
+
1)
+
t

k
+
1
+
s
 1

k
+
s

n
Now continue this process until
k
=
r
:
THEOREM 3.5.6.
Let
V be a ï¬nite dimensional vector space. Then any two bases of
Dimension
Theorem
V have the same number of elements which is called the dimension of the vector space
and denoted as
dim
V
:
PROOF. Let
w
1
;
w
2
;
:
:
:
;
w
r and
v
1
;
v
2
;
:
:
:
;
v
n be two given bases of
V
: Apply
the Steinitz substitution principle to the linearly independent set
w
1
;
w
2
;
:
:
:
;
w
r and
the basis
v
1
;
v
2
;
:
:
:
;
v
n to obtain that
r

n: Now reverse the roles of these two sets
in the substitution principle to obtain the reverse inequality
n

r
: We conclude that
r
=
n, as desired.
Remember that a vector space always carries a ï¬eld of scalars with it. If we are con-
cerned about that ï¬eld we could specify it explicitly as part of the dimension notation.
For instance, we could write
dim
R
n
=
dim
R
R
n or
dim
C
n
=
dim
C
C
n
:
Usually, the ï¬eld of scalars is clear and we donâ€™t need the subscript notation.

164
3. VECTOR SPACES
As a ï¬rst application, letâ€™s dispose of the standard spaces. We already know from Ex-
ample 3.3.10 that these vector spaces have a basis consisting of
n elements, namely the
standard basis
e
1
;
e
2
;
:
:
:
;
e
n
: According to the dimension theorem, this is all we need
to specify dimension of these spaces.
COROLLARY 3.5.7. For the standard spaces we have
dim
R
n
=
n
dim
C
n
=
n
There is one more question we want to answer right away. How do dimensions of a ï¬nite
dimensional vector space
V and a subspace
W of
V relate to each other? Actually, we
donâ€™t even know if
W is ï¬nite dimensional. Our intuition tells us that subspaces should
have smaller dimension. Sure enough, our intuition is right this time! The tool that we
use to conï¬rm this fact is useful in its own right.
COROLLARY 3.5.8. If
W is a subspace of the ï¬nite dimensional vector space
V , then
W is also ï¬nite dimensional and
dim
W

dim
V
with equality if and only if
V
=
W
:
PROOF. Let
w
1
;
w
2
;
:
:
:
;
w
r be a linearly independent set in
W and suppose that
dim
V
=
n: According to the Steinitz substitution principle,
r

n: So there is an
upper bound on the number of elements of a linearly independent set in
W
: Now if
we had that the span of
w
1
;
w
2
;
:
:
:
;
w
r is smaller than
W, then we could ï¬nd a vec-
tor
w
r
+1 in
W but not in
spanfw
1
;
w
2
;
:
:
:
w
r
g: The new set
w
1
;
w
2
;
:
:
:
;
w
r
;
w
r
+1
would also be linearly independent (we leave this as an exercise). Since we cannot
continue adding vectors indeï¬nitely, we have to conclude that at some point we obtain
a basis
w
1
;
w
2
;
:
:
:
;
w
sfor
W
: Furthermore,
s

n, so we conclude that
W is ï¬nite
dimensional and
dim
W

dim
V
: Finally, if we had equality, then a basis of
W would
be the same size as a basis of
V
: However, Steinitz substitution ensures that any linearly
independent set can be expanded to a basis of
V
: It follows that a basis for
W is also a
basis for
V , whence
W
=
V
:
3.5 Exercises
1. Find all possible subsets of the following sets of vectors that form a basis of
R
3
:
(a)
(1;
0;
1);
(1;
 1;
1)
(b)
(1;
2;
1);
(2;
1;
1);
(3;
4;
1);
(2;
0;
1)
(c)
(2;
 3;
1);
(4;
 2;
 3);
(1;
1;
1)
2. The sets of vectors listed below form bases and linearly independent sets in their
respective spaces. According to Steinitz substitution , the
w
1â€™s may be substituted in
for some
v
jâ€™s and retain the basis property. Which
v
jâ€™s could be replaced by
w
1and
w
2
:
(a) In
R
3
;
v
1
=
(1;
3;
1);
v
2
=
(2;
 1;
1);
v
3
=
(1;
0;
1) and
w
1
=
(0;
1;
0);
w
2
=
(1;
1;
1)
(b) In
P
2
;
v
1
=
1
 x;
v
2
=
2
+
x;
v
3
=
1
+
x
2 and
w
1
=
x;
w
2
=
x
2

3.5. BASES AND DIMENSION
165
3. If
U and
V are subspaces of the ï¬nite dimensional vector space
W and
U
\
V
=
f0g,
prove that
dim(U
+
V
)
=
dim
U
+
dim
V
:
4. Let
w
1
;
w
2
;
:
:
:
;
w
r be linearly independent vectors in the vector space
W
: Show
that if
w is a vector in
W and
w
62
spanfw
1
;
w
2
;
:
:
:
;
w
r
g; then
w
1
;
w
2
;
:
:
:
;
w
r
;
w
is a linearly independent set.
5. Answer True/False to each part. In what follows, assume that
V is a vector space of
dimension
n and
S
=
fv
1
;
v
2
;
:
:
:
;
v
k
g

V
:
(a) If
S is a basis of
V then
k
=
n.
(b) If
S spans
V then
k

n.
(c) If
S is linearly independent then
k

n.
(d) If
S is linearly independent and
k
=
n then
S spans
V .
(e) If
S spans
V and
k
=
n then
S is a basis for
V .
(f) If
A is a 5 by 5 matrix and
det
A
=
2, then the ï¬rst 4 columns of
A span a 4
dimensional subspace of
R
5.
(g) A linearly independent set contains redundant vectors.
(h) If
V
=
span
fv
2
;
v
3
g and
dim
V
=
2, then
fv
1
;
v
2
;
v
3
g is a linearly dependent set.
(i) A set of vectors containing the zero vector is a linearly independent set.
(j) Every vector space is ï¬nite dimensional.
(k) The set of vectors
[i;
0]
T
;
[0;
i]
T
;
[1;
i]
T in
C
2 contains redundant vectors.
6. Show that the functions
1;
x;
x
2
;
:
:
:
;
x
n form a basis for the space
P
n of polynomi-
als of degree at most
n:
7. Prove that
C
[0;
1] is an inï¬nite dimensional space (Hint:
P is a subspace of
C
[0;
1]).
8. Let
E
i;j be the
m

n matrix with a unit in the
(i;
j
)th entry and zeros elsewhere.
Prove that
fE
i;j
j
i
=
1;
:
:
:
;
m;
j
=
1;
:
:
:
;
ng is a basis of the vector space
R
m;n
:
9. Let
T
:
V
!
W be a linear operator such that
range
T
=
W and
k
er
T
=
f0g: Let
v
1
;
v
2
;
:
:
:
;
v
n be a basis of
V
: Show that
T
(v
1
);
T
(v
2
);
:
:
:
;
T
(v
n
) is a basis of
W
:
10. Let
V
=
f0g; a vector space with a single element. Explain why the element
0 is
not a basis of
V and the dimension of
V must be
0:
11. Show that a set of vectors
v
1
;
v
2
;
:
:
:
;
v
n in the vector space
V is a basis if and only
if it is a minimal spanning set, that is, no proper subset of these vectors is a spanning
set.
12. Let
T
:
V
!
W be a linear operator where
V is a ï¬nite dimensional space and
U
is a subspace of
V
: Prove that
dim
T
(U
)

dim
U:
13. Determine the dimension of the subspace of
R
n;n consisting of all symmetric ma-
trices by exhibiting a basis.

166
3. VECTOR SPACES
14. Let
U be the subspace of
W
=
R
n;n consisting of all symmetric matrices and
V
the subspace of all skew-symmetric matrices.
(a) Show that
U
\
V
=
f0g.
(b) Show that
U
+
V
=
W
:
(c) Use Exercises 8, 3 and 13 to calculate the dimension of
V
:
3.6. Linear Systems Revisited
We now have some very powerful tools for understanding the nature of solution sets of
the standard linear system
Ax
=
b: This understanding will help us design practical
computational methods for ï¬nding dimension and bases for vector spaces and other
problems as well.
The ï¬rst business at hand is to describe solution sets of non-homogeneous systems.
Recall that every homogeneous system is consistent since it has the trivial solution.
Nonhomogeneous systems are another matter. We already have one criterion, namely
that rank of augmented matrix and coefï¬cient matrix of the system must agree. Here is
one more way to view the consistency of such a system in the language of vector spaces.
THEOREM 3.6.1. The linear system
Ax
=
b of
m equations in
n unknowns is consis-
Consistency in
Terms of
Column Space
tent if and only if
b
2
C
(A):
PROOF. The key to this fact is Theorem 3.4.9, which says that the vector
Ax is
a linear combination of the columns of
A with the entries of
x as scalar coefï¬cients.
Therefore, to say that
Ax
=
b has a solution is simply to say that some linear combi-
nation of columns of
A adds up to
b, i.e.,
b
2
C
(A):
EXAMPLE 3.6.2. One of the following vectors belongs to the space
V spanned by
v
1
=
(1;
1;
3;
3),
v
2
=
(0;
2;
2;
4) and
v
3
=
(1;
0;
2;
1): The vectors in question are
u
=
(2;
1;
5;
4) and
w
=
(1;
0;
0;
0): Which and why?
SOLUTION. Theorem 3.6.1 tells us that if
A
=
[v
1
;
v
2
;
v
3
], then we need only deter-
mine whether or not the systems
Ax
=
u and
Ax
=
w are consistent. In the interests
of efï¬ciency, we may as well do both at once by forming the augmented matrix for both
right hand sides at once as
[A
j
u
j
v
]
=
2
6
6
4
1
0
1
2
1
1
2
0
1
0
3
2
2
5
0
3
4
1
4
0
3
7
7
5

3.6. LINEAR SYSTEMS REVISITED
167
The reduced row echelon form of this matrix (whose calculation we leave as an exercise)
is
2
6
6
4
1
0
0
0
0
0
1
 1
2
 1
2
0
0
0
0
0
1
0
0
0
0
0
3
7
7
5
Observe that there is a pivot in the ï¬fth column but not in the fourth column. This
tells us that the system with augmented matrix
[A
j
u] is consistent, but the system
with augmented matrix
[A
j
v
] is not consistent. Therefore
u
2
spanfv
1
;
v
2
;
v
3
g, but
v
=
2
span
fv
1
;
v
2
;
v
3
g: As a matter of fact, the reduced row echelon form of
[
A
j
u] tells
us what linear combinations will work, namely
u
=
(2
 c
3
)v
1
 1
2
(1
 c
3
)v
2
+
c
3
v
3
:
where
c
3 is an arbitrary scalar. The reason for the non-uniqueness of the coordinates of
u is that the vectors
v
1
;
v
2
;
v
3 are not linearly independent.
The next matter of business is a description of the solution space itself, given that it
is not empty. We already have a pretty good conceptual model for the solution of a
homogeneous system
Ax
=
0: Remember that this is just the null space,
N
(A), of the
matrix
A: In fact, the deï¬nition of
N
(A) is the set of vectors
x such that
Ax
=
0: The
important point here is that we proved that
N
(A) really is a subspace of the appropriate
n dimensional standard space
R
n or
C
n
: As such we can really picture it when
n is
2 or
3:
N
(A) is either the origin, a line through the origin, a plane through the origin, or in
the case
A
=
0; all of
R
3
: What can we say about a non-homogeneous system? Here is
a handy way of understanding these solution sets.
THEOREM 3.6.3.
Suppose the system
Ax
=
b is consistent with a particular solution
Form of
General
Solution
x
0
: Then the general solution
x to this system can be described by the equation
x
=
x
0
+
z
where
z runs over all elements of
N
(A):
PROOF. On the one hand, suppose we are given a vector of the form
x
=
x
0
+
z,
where
Ax
0
=
b and
z
2
N
(A): Then
Ax
=
A(x
0
+
z)
=
Ax
0
+
Az
=
b
+
0
=
b:
Thus
x is a solution to the system. Conversely, suppose we are given any solution
x to
the system and that
x
0 is a particular solution to the system. Then
A(x
 x
0
)
=
Ax
 Ax
0
=
b
 b
=
0
It follows that
x
 x
0
=
z
2
N
(A) so that
x has the required form
x
0
+
z:
This is really a pretty fact, so letâ€™s be clear about what it is telling us. It says that the
solution space to a consistent system, as a set, can be described as the set of all translates
of elements in the null space of
A by some ï¬xed vector. Such a set is sometimes called
an afï¬ne set or a ï¬‚at. When
n is
2 or
3 this says that the solution set is either a single
point, a line or a plane â€“ not necessarily through the origin!

168
3. VECTOR SPACES
EXAMPLE 3.6.4. Describe geometrically the solution sets to the system
x
+
2y
=
3
x
+
y
+
z
=
3
SOLUTION. First solve the system, which has augmented matrix

1
2
0
3
1
1
1
3

     !
E
21
( 1)

1
2
0
3
0
 1
1
0

      !
E
12
(2)
E
2
( 1)

1
0
2
3
0
1
 1
0

The general solution to the system is given in terms of the free variable
z, which we will
relabel as
z
=
t to obtain
x
=
3
 2t
y
=
t
z
=
t
We recognize this from calculus as a parametric representation of a line in three dimen-
sional space
R
3
: Notice that this line does not pass through the origin since
z
=
0 forces
x
=
3: So the solution set is deï¬nitely not a subspace of
R
3
:
Now we turn to another computational matter. How do we ï¬nd bases of vector spaces
prescribed by a spanning set? How do we ï¬nd the linear dependencies in a spanning
set or implement the Steinitz substitution principle in a practical way? We have all the
tools we need now to solve these problems. Letâ€™s begin with the question of ï¬nding a
basis. We are going to solve this problem in two ways. Each has its own merits.
First we examine the row space approach. We require two simple facts.
THEOREM 3.6.5. Let
A be any matrix and
E an elementary matrix. Then
R(A)
=
R(E
A):
PROOF. Suppose the rows of
A are the vectors
r
1
;
r
2
;
:
:
:
;
r
n, so that we have
R(A)
=
span
fr
1
;
r
2
;
:
:
:
;
r
n
g: If
E
=
E
ij, then the effect of multiplication by
E is to
switch the
ith and
jth rows, so the rows of
E
A are simply the rows of
A in a different
order. Hence,
R(A)
=
R(E
A) in this case. If
E
=
E
i
(a), with
a a nonzero scalar,
then the effect of multiplication by
E is to replace the
ith row by a nonzero multiple
of itself. Clearly, this doesnâ€™t change the span of the rows either. To simplify notation,
consider the case
E
=
E
12
(a): Then the ï¬rst row
r
1 is replaced by
r
1
+
ar
2, so that any
combination of the rows of
E
A is expressible as a linear combination of the rows of
A:
Conversely, since
r
1
=
r
1
+
ar
2
 ar
2, we see that any combination of
r
1
;
r
2
;
:
:
:
;
r
n
can be expressed in terms of the rows of
E
A: This proves the theorem.
THEOREM 3.6.6. If the matrix
R is in reduced row echelon form, then the nonzero rows
of
R form a basis of
R(R
):
PROOF. Suppose the rows of
R are given as
r
1
;
r
2
;
:
:
:
;
r
n, so that we have
R(R
)
=
spanfr
1
;
r
2
;
:
:
:
;
r
k
g, where the ï¬rst
k rows of
R are nonzero and the remaining rows
are zero rows. So certainly the nonzero rows span
R(R
): In order for these vectors to
form a basis, they must also be a linearly independent set. If some linear combination
of these vectors is zero, say
0
=
c
1
r
1
+



+
c
k
r
k

3.6. LINEAR SYSTEMS REVISITED
169
we examine the
ith coordinate of this linear combination, corresponding to the column
in which the
ith pivot appears. In that column
r
i has a coordinate value of
1 and all other
r
j have a value of zero. Therefore, the linear combination above yields that
c
i
=
0:
Since this holds for each
i

k, we obtain that all
c
i
=
0 and the nonzero rows must be
linearly independent. It follows that these vectors form a basis of
R(R
):
These theorems are the foundations for the following algorithm for ï¬nding a basis for a
vector space.
Row Space Algorithm: Given
V
=
spanfv
1
;
v
2
;
:
:
:
;
v
m
g

R
n or
C
n
:
1. Form the
m

n matrix
A whose rows are
v
T
1
;
v
T
2
;
:
:
:
;
v
T
m
:
2. Find the reduced row echelon form
R of
A:
3. List the nonzero rows of
R
: Their transposes form a basis of
V
:
EXAMPLE 3.6.7. Given that the vector space
V is spanned by
v
1
=
(1;
1;
3;
3)
v
2
=
(0;
2;
2;
4)
v
3
=
(1;
0;
2;
1)
v
4
=
(2;
1;
5;
4)
Find a basis of
V by the row space algorithm.
SOLUTION. Form the matrix
A
=
2
6
6
4
1
1
3
3
0
2
2
4
1
0
2
1
2
1
5
4
3
7
7
5
Now ï¬nd the reduced row echelon form of
A:
2
6
6
4
1
1
3
3
0
2
2
4
1
0
2
1
2
1
5
4
3
7
7
5
       !
E
31
( 1)
E
41
( 2)
E
2
(1=2)
2
6
6
4
1
1
3
3
0
1
1
2
0
 1
 1
 2
0
 1
 1
 2
3
7
7
5
       !
E
32
(1)
E
42
(1)
E
12
( 1)
2
6
6
4
1
0
2
1
0
1
1
2
0
0
0
0
0
0
0
0
3
7
7
5
From this we see that the vectors
(1;
0;
2;
1) and
(0;
1;
1;
2) form a basis for the row
space of
A:
The second algorithm for computing a basis does a little more than ï¬nd a basis: it gives
us a way to tackle the question of what linear combinations sum to zero.
THEOREM 3.6.8. Let
A
=
[a
1
;
a
2
;
:
:
:
;
a
n
] be a matrix with columns
a
1
;
a
2
;
:
:
:
;
a
n
:
Suppose the indices of the non-pivot columns in the reduced row echelon form of
A are
i
1
;
i
2
;
:
:
:
;
i
k
: Then every trivial linear combination
0
=
c
1
a
1
+
c
2
a
2
+



+
c
n
a
n
of the columns of
A is uniquely determined by the values of
c
i
1
;
c
i
2
;
:
:
:
;
c
i
k
: In partic-
ular, if these coefï¬cients are
0, then all the other coefï¬cients must be
0:

170
3. VECTOR SPACES
PROOF. Express the linear combination in the form
0
=
c
1
a
1
+
c
2
a
2
+



+
c
n
a
n
=
Ac
where
c
=
(c
1
;
c
2
;
:
:
:
;
c
n
) and
A
=
[a
1
;
a
2
;
:
:
:
;
a
n
]: In other words, the column
c
of coefï¬cients is in the null space of
A: Every solution
c to this system is uniquely
speciï¬ed as follows: assign arbitrary values to the free variables, then use the rows of
the reduced row echelon form of
A to solve for each bound variable. This is exactly
what we wanted to show.
In view of this theorem, we see that the columns of
A corresponding to pivot columns
(equivalently, bound variables) in the reduced row echelon form of
A must be them-
selves a linearly independent set. We also see from the proof of this theorem that we can
express any column corresponding to a non-pivot column (equivalently, free variable )
in terms of columns corresponding to bound variables by setting the free variable cor-
responding to this column to
1, and all other free variables to
0: Therefore, the columns
of
A corresponding to pivot columns form a basis of
C
(A): This justiï¬es the following
algorithm for ï¬nding a basis for a vector space.
Column Space Algorithm:
Given
V
=
span
fv
1
;
v
2
;
:
:
:
;
v
n
g

R
m or
C
m
:
1. Form the
m

n matrix
A whose columns are
v
1
;
v
2
;
:
:
:
;
v
n
:
2. Find the reduced row echelon form
R of
A:
3. List the columns of
A that correspond to pivot columns of
R
: These
form a basis of
V
:
Caution: It is not the columns (nor the rows) of the reduced row echelon form matrix
R that yield the basis vectors for
V
: In fact, if
E is an elementary matrix, in general we
have
C
(A)
6=
C
(E
A):
EXAMPLE 3.6.9. Given that the vector space
V is spanned by
v
1
=
(1;
1;
3;
3)
v
2
=
(0;
2;
2;
4)
v
3
=
(1;
0;
2;
1)
v
4
=
(2;
1;
5;
4)
Find a basis of
V by the column space algorithm.
SOLUTION. Form the matrix
A whose columns are these vectors:
2
6
6
4
1
0
1
2
1
2
0
1
3
2
2
5
3
4
1
4
3
7
7
5
       !
E
21
( 1)
E
31
( 3)
E
41
( 3)
2
6
6
4
1
0
1
2
0
2
 1
 1
0
2
 1
 1
0
4
 2
 2
3
7
7
5
       !
E
32
( 1)
E
42
( 2)
E
2
(1=2)
2
6
6
4
1
0
1
2
0
1
 1=2
 1=2
0
0
0
0
0
0
0
0
3
7
7
5
We can see from this calculation that the ï¬rst and second columns will be pivot columns,
while the third and fourth will not be. According to the column space algorithm,
C
(A)
is a two dimensional space with the ï¬rst two columns for a basis.

3.6. LINEAR SYSTEMS REVISITED
171
Just for the record, letâ€™s notice here that Theorem 3.6.8 shows us exactly how to express
the last two columns in terms of the ï¬rst two. From the ï¬rst two rows of the reduced
row echelon form of
A we see that if
c
=
(c
1
;
c
2
;
c
3
;
c
4
) and
Ac
=
0; then
c
1
+
c
3
+
2c
4
=
0
c
2
 1
2
c
3
 1
2
c
4
=
0
So for
v
3 we choose
c
3
=
1 and
c
4
=
0 to obtain that
c
1
=
 1 and
c
2
=
1=2: Therefore
we have
 1v
1
+
1
2
v
2
+
1v
3
+
0v
4
=
0
so that
v
3
=
v
1
 1
2
v
2
:
A similar calculation with
c
3
=
0 and
c
4
=
1 yields that
c
1
=
 2 and
c
2
=
1=2:
Therefore we obtain
 2v
1
+
1
2
v
2
+
0v
3
+
1v
4
=
0
so that
v
4
=
2v
1
 1
2
v
2
Finally, we consider the problem of ï¬nding a basis for a null space. Actually, we have
already dealt with this problem in an earlier example (Example 3.4.7), but now we will
justify what we did there.
THEOREM 3.6.10. Let
A be an
m

n matrix such that
rank
A
=
r
: Suppose the general
solution to the homogeneous equation
Ax
=
0 with
x
=
(x
1
;
x
2
;
:
:
:
;
x
n
) is written in
the form
x
=
x
i
1
v
1
+
x
i
2
v
2
+



+
x
i
n r
v
n r
where
x
i
1
;
x
i
2
;
:
:
:
;
x
i
n r are the free variables. Then
v
1
;
v
2
;
:
:
:
;
v
n r form a basis
of
N
(A):
PROOF. The vector
x
=
0 occurs precisely when all the free variables are set equal
to
0, for the bound variables are linear combinations of the free variables. This means
that the only linear combination of the vectors
v
1
;
v
2
;
:
:
:
;
v
n r that sum to
0 are those
for which all the coefï¬cients
x
i
1
;
x
i
2
;
:
:
:
;
x
i
n r are
0: Hence these vectors are linearly
independent. They span
N
(A) since every element
x
2
N
(A) is a linear combination
of them. Therefore,
v
1
;
v
2
;
:
:
:
;
v
n r form a basis of
N
(A):
We see from the statement of this theorem that the nullity of a matrix is simply the
dimension of the null space of the matrix. It is also the basis of this algorithm.

172
3. VECTOR SPACES
Null Space Algorithm: Given an
m

n matrix
A:
1. Compute the reduced row echelon form
R of
A:
2. Use
R to ï¬nd the general solution to the homogeneous system
Ax
=
0:
3. Write the general solution
x
=
(x
1
;
x
2
;
:
:
:
;
x
n
) to the homogeneous
system in the form
x
=
x
i
1
v
1
+
x
i
2
v
2
+



+
x
i
n r
v
n r
where
x
i
1
;
x
i
2
;
:
:
:
;
x
i
n r are the free variables.
4. List the vectors
v
1
;
v
2
;
:
:
:
;
v
n r
: These form a basis of
N
(A):
EXAMPLE 3.6.11. Find a basis for the null space of the matrix
A in the preceding
example by the null space algorithm.
SOLUTION. We already found the reduced row echelon form of
A as
R
=
2
6
6
4
1
0
1
2
0
1
 1=2
 1=2
0
0
0
0
0
0
0
0
3
7
7
5
The variables
x
3 and
x
4 are free, while
x
1 and
x
2 are bound. Hence the general solution
of
Ax
=
0 can be written as
x
1
=
x
3
+
2x
4
x
2
=
 1
2
x
3
+
 1
2
x
4
x
3
=
x
3
x
4
=
x
4
which becomes, in vector notation,
2
6
6
4
x
1
x
2
x
3
x
4
3
7
7
5
=
x
3
2
6
6
4
1
 1=2
1
0
3
7
7
5
+
x
4
2
6
6
4
2
 1=2
0
1
3
7
7
5
:
Therefore,
v
1
=
(1;
 1=2;
1;
0) and
v
1
=
(2;
 1=2;
0;
1) form a basis of
N
(A):
Here is a summary of the key dimensional facts that we have learned in this section:
THEOREM 3.6.12. Let
A be an
m

n matrix such that
rank
A
=
r. Then
Rank Theorem
1.
dim
C
(A)
=
r
2.
dim
R(A)
=
r
3.
dim
N
(A)
=
n
 r

3.6. LINEAR SYSTEMS REVISITED
173
3.6 Exercises
1. Find bases for the row space, column space and null space of the following matrices
and the dimension of each of these subspaces.
(a)
A
=
2
4
0
2
 1
1
1
1
3
5 (b)
B
=
2
4
2
2
 4
 1
0
2
1
1
 2
3
5 (c)
C
=

1
2
 1
2
 1
0
2
2

(d)

1
1
 1
1

(e)

2
2
 4
 4
 4
8

(f)

1
+
i
2
2
 i
 1
0
i

2. Find all possible linear combinations of the following sets of vectors that sum to
0
and the dimension of the subspaces spanned by these vectors.
(a)
(0;
1;
1);
(2;
0;
1);
(2;
2;
3);
(0;
2;
2) in
R
3
:
(b)
x;
x
2
+
x;
x
2
 x in
P
2
:
3. Let
A
=
2
4
4
3
5
5
4
3
2
1
9
3
5
;
B
=
2
4
1
1
3
 2
 1
 4
7
5
17
3
5
Compute a basis for (a)
R(A)
+
R(B
) and for (b)
N
(A)
+
N
(B
):
4. In this exercise you should use the fact that
B is the reduced row-echelon form of
A
where
A
=
2
6
6
6
6
4
3
1
 2
0
1
2
1
1
1
0
 1
1
2
2
3
2
 1
1
1
8
9
0
2
2
 1
1
6
8
0
3
3
3
 3
0
3
3
7
7
7
7
5
;
B
=
2
6
6
6
6
4
1
0
 1
0
0
 2
 3
0
1
1
0
0
2
3
0
0
0
1
0
4
5
0
0
0
0
1
6
7
0
0
0
0
0
0
0
3
7
7
7
7
5
(a) Find a basis for the row space of
A and give the rank of
A:
(b) Find a basis for the column space of
A:
(c) Find a basis for the set of solutions to
Ax
=
0 and give the nullity of
A:
5. Describe geometrically the solution sets to the systems
(a)
3x
+
6y
+
3z
=
9
(b)
x
+
2y
+
z
=
3
(c)
6x
+
4y
 4z
=
0
x
 z
=
1
5x
+
3y
+
3z
=
6
6x
+
2y
 2z
=
0
6. Let
A be an
m

n matrix of rank
r
: Suppose that there exists a vector
b
2
R
m such
that the system
Ax
=
b is inconsistent. Use the consistency and rank theorems of this
section to deduce that the system
A
T
y
=
0 must have nontrivial solutions. Hint: What
does
b
=
2
C
(A) tell you about
r
?
7. Find bases for the subspace
V
=
span
fv
1
;
v
2
;
v
3
;
v
4
g of
R
3 by the row space
algorithm and the column space algorithm, where
v
1
=
(1;
1;
3;
3);
v
2
=
(0;
2;
2;
4);
v
3
=
(1;
0;
2;
1); and
v
4
=
(2;
1;
5;
4):

174
3. VECTOR SPACES
8. Find bases for the subspace
V
=
spanfv
1
;
v
2
;
v
3
;
v
4
g of
R
3 by the row space
algorithm and the column space algorithm, where
v
1
=
(1;
1;
2;
2);
v
2
=
(0;
2;
0;
2);
v
3
=
(1;
0;
2;
1) and
v
4
=
(2;
1;
4;
3):
9. Find two bases for the subspace
V
=
spanfv
1
;
v
2
;
v
3
;
v
4
;
v
5
g of
P
2 where
v
1
=
1
+
x;
v
2
=
1
+
x
 x
2
;
v
3
=
1
+
x
+
x
2
;
v
4
=
x
 x
2
; and
v
5
=
1
+
2x: Hint:
You can tackle this directly or use standard vectors instead, which can be done by the
isomorphism of Page 158.
10. Suppose that the linear system
Ax
=
b is a consistent system of equations, where
A is an
m

n matrix and
x
=
[x
1
;
:
:
:
;
x
n
]
T
: Prove that if the set of columns of
A has
redundant vectors in it, then the system has more than one solution.
11. Let
p(x)
=
c
0
+
c
1
x
+



+
c
m
x
m be a polynomial and
A an
n

n matrix. The
value of
p at
A is deï¬ned to be the
n

n matrix
p(A)
=
c
0
I
+
c
1
A
+



+
c
m
A
m
Use the result of Exercise 8 of Section 6 to show that there exists a polynomial
p(x) of
degree at most
n
2 for which
p(A)
=
0: (Aside: this estimate is actually much too pes-
simistic. There is a theorem, the Cayley-Hamilton theorem, that shows that
n works.)
12. Use Theorem 3.6.3 and the Dimension theorem to answer the question posed in
Example 3.5 of Section 3.6.
13. Use the rank theorem to prove that any rank
1 matrix can be expressed in the form
uv
T for suitable standard vectors
u and
v
:
14. Let
u
1
;
u
2
;
:
:
:
;
u
m and
v
1
;
v
2
;
:
:
:
;
v
n be bases of
U and
V , respectively, where
U and
V are subspaces of the vector space
W
:
(a) Show that the set of
mn vectors
u
j
+
v
k
;
j
=
1;
2;
:
:
:
;
m;
k
=
1;
2;
:
:
:
;
n spans
the subspace
U
+
V
:
(b) Show that if
U
\
V
=
f0g; then the vectors in (a) form a basis of
U
+
V
:
(c) Show by example that part (b) fails if
U
\
V
6=
f0g:
3.7. *Change of Basis and Linear Operators
How much information do we need to uniquely identify an operator? For a general
operator the answer is: a lot! Speciï¬cally, we donâ€™t really know everything about it
until we know how to ï¬nd its value at every possible argument. This is an inï¬nite
amount of information. Yet we know that in some circumstances we can do better.
For example, to know a polynomial function completely, we only need a ï¬nite amount
of data, namely the coefï¬cients of the polynomial. We have already seen that linear

3.7. *CHANGE OF BASIS AND LINEAR OPERATORS
175
operators are special. Are they described by a ï¬nite amount of data? The answer is a
resounding â€œyesâ€ in the situation where the domain and target are ï¬nite dimensional.
Let
T
:
V
!
W be such an operator. Suppose that
B
=
fv
1
;
v
2
;
:
:
:
;
v
n
g is a basis
of
V and
C
=
fw
1
;
w
2
;
:
:
:
;
w
m
g is a basis of
W
: Now let
v
2
V be given. We know
that there exists a unique set of scalars (the coordinates of
v with respect to this basis)
c
1
;
c
2
;
:
:
:
;
c
n such that
v
=
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
Thus by linearity of
T we see that
T
(v
)
=
T
(c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
)
=
c
1
T
(v
1
)
+
c
2
T
(v
2
)
+



+
c
n
T
(v
n
)
It follows that we know everything about the linear operator
T if we know the vectors
T
(v
1
);
T
(v
2
);
:
:
:
;
T
(v
n
):
Now go a step further. Each vector
T
(v
j
) can be expressed uniquely as a linear combi-
nation of
w
1
;
w
2;
:
:
:
;
w
m, namely
T
(v
j
)
=
a
1;j
w
1
+
a
2;j
w
2
+



+
a
m;j
w
m
:
(3.7.1)
In other words, the scalars
a
1;j
;
a
2;j
;
:
:
:
a
m;j are the coordinates of
T
(v
j
) with respect
to the basis
w
1
;
w
2;
:
:
:
;
w
m
: Stack these in columns and we now have the
m

n matrix
A
=
[a
i;j
] which contains everything we need to know in order to compute
T
(v
): In
fact, with the above terminology, we have
T
(v
)
=
c
1
T
(v
1
)
+
c
2
T
(v
2
)
+



+
c
n
T
(v
n
)
=
c
1
(
a
1;1
w
1
+
a
2;1
w
2
+



+
a
m;1
w
m
)
+



+
c
n
(a
1;n
w
1
+
a
2;n
w
2
+



+
a
m;n
w
m
)
=
(a
1;1
c
1
+
a
1;2
c
2
+



+
a
1;n
c
n
)w
1
+



+
(a
m;1
c
1
+
a
m;2
c
2
+



+
a
m;n
c
n
)
w
m
Look closely and we see that the coefï¬cients of these vectors are themselves coordinates
of a matrix product, namely the matrix
A times the column vector of coordinates of
v with respect to the chosen basis of
V
: The result of this matrix multiplication is a
column vector whose entries are the coordinates of
T
(v
) relative to the chosen basis of
W
: So in a certain sense, computing the value of a linear operator amounts to no more
than multiplying a (coordinate) vector by the matrix
A: Now we make the following
deï¬nition.
DEFINITION 3.7.1. The matrix of the linear operator
T
:
V
!
W relative to the
bases
B
=
fv
1
;
v
2
;
:
:
:
;
v
n
g of
V and
C
=
fw
1
;
w
2
;
:
:
:
;
w
m
g of
W is the matrix
Matrix of Linear
Operator
[T
]
B
;C
=
[a
i;j
] whose entries are speciï¬ed by Equation 3.7.1. In the case that
B
=
C
;
we simply write
[T
]
B
:
Denote the coordinate vector of a vector
v with respect to a basis
B by
[v
]
B
: Then the
above calculation of
T
(v
) can be stated succinctly in matrix/vector terms as
[T
(v
)]
C
=
[T
]
B
;C
[v
]
B
(3.7.2)

176
3. VECTOR SPACES
Even in the case of an operator as simple as the identity map
I
(v
)
=
v, the matrix of a
linear operator can be useful and interesting.
DEFINITION 3.7.2. Let
B
=
fv
1
;
v
2
;
:
:
:
;
v
n
g and
C
=
fw
1
;
w
2
;
:
:
:
;
w
n
g both be
bases of
V
: Then the matrix
[I
]
B
;C of the identity map
I
:
V
!
V relative to these
bases is called the change of basis matrix from the basis
C to the basis
B
:
A very fundamental fact about these change of bases matrices is the answer to the fol-
lowing question. Suppose that
T
:
V
!
W and
S
:
U
!
V are linear operators. Can
we relate the matrices of
T
;
S and
T
Ã†
S? The answer is as follows.
THEOREM 3.7.3. Suppose that
B
=
fv
1
;
v
2
;
:
:
:
;
v
n
g,
C
=
fw
1
;
w
2
;
:
:
:
;
w
m
g, and
Change of
Basis
D
=
fu
1
;
u
2
;
:
:
:
;
u
n
g are bases of
V
;
W and
U, respectively, and that
T
:
V
!
W
and
S
:
U
!
V are linear operators. Then
[T
Ã†
S
]
D
;C
=
[T
]
B
;C
[S
]
D
;B
An immediate corollary is that if
T is invertible, then so is the matrix of
T with respect
to any basis. In particular, all change of bases matrices are invertible.
We can now also see exactly what happens when we make a change of basis in the do-
main and target of a linear operator and recalculate the matrix of the operator. Specif-
ically, suppose that
T
:
V
!
W and that
B
;
B
0 are bases of
V and
C
;
C
0 are bases of
W
: Let
P and
Q be the change of basis matrices from
B
0 to
B and
C
0 to
C, respec-
tively. Identify a matrix with its operator action by multiplication and we have a chain
of operator maps
B
0
P
!
B
T
!
C
Q
 1
!
C
0
so that application of the theorem shows that
[T
]
B
0
;C
0
=
Q
 1
[T
]
B
;C
P
NOTATION 3.7.4. If
T
:
R
n
!
R
m is a linear operator, then the matrix of
T with
respect to the standard bases of the domain and target of
T, which we simply denote as
[T
] is called the standard matrix of
T
:
We have just obtained a very important insight into the matrix of a linear transformation.
Here is the form it takes for the standard spaces.
COROLLARY 3.7.5. Let that
T
:
R
n
!
R
m be a linear operator,
B a basis of
R
n and
C a basis of
R
m
: Let
P and
Q be the change of basis matrices from the standard bases
to
B and
C
; respectively. If
A is the matrix of
T with respect to the standard bases and
M the matrix of
T with respect to the bases
B and
C
; then
M
=
Q
 1
AP
EXAMPLE 3.7.6. Given the linear operator
T
:
R
4
!
R
2 by the rule
T
(x
1
;
x
2
;
x
3
;
x
4
)
=

x
1
+
3x
2
 x
3
2x
1
+
x
2
 x
4

ï¬nd the standard matrix of
T
:

3.7. *CHANGE OF BASIS AND LINEAR OPERATORS
177
SOLUTION. We see that
T
(e
1
)
=

1
2

;
T
(e
2
)
=

3
1

;
T
(e
3
)
=

 1
0

;
T
(e
4
)
=

0
 1

Thus, since the standard coordinate vector of a standard vector is simply itself, we have
[T
]
=

1
3
 1
0
2
1
0
 1

EXAMPLE 3.7.7. With
T as above, ï¬nd the matrix of
T with respect to the domain basis
B
=
f(1;
0;
0;
0);
(1;
1;
0;
0);
(1;
0;
1;
0);
(1;
0;
0;
1
)gand range basis
C
=
f(1;
1);
(1;
 1)g:
SOLUTION. Let
A be the matrix of the previous example, so it represents the stan-
dard matrix of
T
: Let
B
0
=
f(1;
0;
0;
0);
(1;
0;
0;
0);
(0;
0;
1;
0);
(0;
0
;
0;
1
)g and
C
0
=
f(1;
0);
(0;
1)g be the standard bases for the domain and target of
T
: Then we have
A
=
[T
]
=
[T
]
B
0
;C
0
The change of basis matrix from any basis
B to the standard basis is easy to calculate:
simply form the matrix that has the vectors of
B listed as its columns. In our case, this
means that
P
=
[I
]
B
;B
0
=
2
6
6
4
1
1
1
1
0
1
0
0
0
0
1
0
0
0
0
1
3
7
7
5
and
Q
=
[I
]
C;C
0
=

1
1
1
 1

Now the chain of operators
B
P
!
B
0
A
!
C
0
Q
 1
!
C
Therefore
[T
]
B
;C
=
Q
 1
AP
=
 1
2

 1
 1
 1
1


1
3
 1
0
2
1
0
 1

2
6
6
4
1
1
1
1
0
1
0
0
0
0
1
0
0
0
0
1
3
7
7
5
=

3
2
7
2
1
1
 1
2
1
2
 1
0


178
3. VECTOR SPACES
3.7 Exercises
1. Let the operator
T
:
R
3
!
R
3 be given by
f
((x;
y
;
z
))
=
2
4
x
+
2y
x
 y
y
+
z
3
5
Show that
T is linear and ï¬nd the standard matrix for
T
: Determine bases for each of
the spaces associated with the operator (domain, range, image and kernel).
2. Let
B
=
fu
1
;
u
2
g and
B
0
=
fu
0
1
;
u
0
2
g be bases of
R
2, where
u
1
=
[2;
2]
T,
u
2
=
[4;
 1]
T,
u
0
1
=
[1;
3]
T and
u
0
2
=
[ 1;
 1]
T
:
(a) Find the change of basis (transition) matrix
T from
B to
B
0
:
(b) Given that
w
=
3u
1
+
4u
2, use (a) to express
w as a linear combination of
u
0
1 and
u
0
2
(c) What is the transition matrix from
B to the standard basis
B
00
=
fe
1
;
e
2
g?
3. Let the linear transformation
T
:
R
2
!
R
2 be given by the formula
T
(

x
1
x
2

)
=

x
1
 4x
2
2x
1
+
x
2

, and
B
0
=
f
1
5
[3;
4]
T
;
1
5
[ 4;
3]g: Find the matrix of
T with respect to the
standard basis
B
=
fe
1
;
e
2
g, the transition matrix from
B
0 to
B and use this informa-
tion to ï¬nd the matrix of
T with respect to the basis
B
:
4. Deï¬ne the determinant of a linear operator
T
:
V
!
V to be the determinant of
[T
]
B, where
B is any basis of the ï¬nite dimensional vector space
V
: Show that this
deï¬nition is independent of the basis
B
:
3.8. *Computational Notes and Projects
Project Topics
Project: Modeling with Directed Graphs II
We develop the background for this project by way of an example. You might also
review the material of page 66.
EXAMPLE 3.8.1. Suppose we have a communications network that connects ï¬ve nodes
which we label
1;
2;
3;
4;
5: Communications between points are not necessarily two-
way. We specify the network by listing ordered pairs
(i;
j
);the meaning of which is that

3.8. *COMPUTATIONAL NOTES AND PROJECTS
179
Vertex 3
Vertex 2
Vertex 1
Edge 1
Edge 2
Edge 4
Edge 6
Edge 5
Vertex 4
Vertex 5
Edge 7
Edge 8
Edge 3
FIGURE 3.8.1. Data from Example 3.8.1
information can be sent from point
i to point
j: For our problem the connection data is
the set
E
=
f(1;
2);
(3;
1);
(1;
4);
(2;
3);
(3;
4);
(3;
5);
(4;
2);
(5;
3)g
By a loop we mean a walk that starts and ends at some node, i.e., a sequence of directed
edges connecting a node to itself. For example, the sequence
(3;
5);
(5;
3) is a loop in
our example. It is important to be able to account for loops in such a network. For
one thing, we know that we have two-way communication between any two points in a
loop (start at one point and follow the arrows around the loop till you reach the other).
Find all the loops of this example and formulate an algorithm that one could program to
compute all the loops of the network.
SOLUTION. We recognize this as data that can be modeled by a directed graph (see Ex-
ample 2.3.4). Thus, â€œnodesâ€ are just vertices in the graphs and connections are edges.
We can draw a picture that contains all the data that we are given by representing each
team, or â€œvertexâ€, as a point and then connecting two points with an arrow, or â€œdi-
rected edgeâ€, which points from the winner towards a loser in one of the matches. See
Figure 3.8.1 for a picture of this graph.
It isnâ€™t so simple to eyeball this graph and count all loops. In fact, if you count go-
ing around and around in the same loop as different from the original loop, there are
inï¬nitely many. Perhaps we should be a bit more systematic about it. Letâ€™s count the
smallest loops only, that is, the loops that are not themselves a sum of other loops. It
appears that there are only three of these, namely,
L
1
:
(3;
5);
(5;
3)
L
2
:
(2;
3);
(3;
4);
(4;
2)
L
3
:
(1;
2);
(2;
3);
(3;
1)
There are other loops, e.g.,
L
4
:
(2;
3);
(3;
5);
(5;
3);
(3;
4);
(4;
2): But this is built up
out of
L
1and
L
2
: In a certain sense,
L
4
=
L
1
+
L
2
: There seems to be a â€œcalcu-
lus of loops.â€ Lurking in the background is another matrix, different from the adja-
cency matrix that we encountered in Chapter 2, that describes all the data necessary
to construct the graph. It is called the incidence matrix of the graph and is given as
follows: the incidence matrix has its rows index by the vertices of the graph and its

180
3. VECTOR SPACES
columns by the edges. If the edge
(i;
j
) is in the graph, then the column correspond-
ing to this edge has a
 1 in its
ith row and a
+1 in its
jth row. All other entries
are
0: In our example we see that the vertex set is
V
=
f1;
2;
3;
4;
5g, the edge set is
E
=
f(1;
2);
(2;
3);
(3;
4);
(4;
2);
(1;
4);
(1;
3);
(3;
5);
(5;
3)g and so the incidence ma-
trix is
A
=
2
6
6
6
6
4
 1
0
0
0
 1
1
0
0
1
 1
0
1
0
0
0
0
0
1
 1
0
0
 1
 1
1
0
0
1
 1
1
0
0
0
0
0
0
0
0
0
1
 1
3
7
7
7
7
5
=

v
1
v
2
v
3
v
4
v
5
v
6
v
7
v
8

We can now describe all loops. Each column of
A deï¬nes an edge. Thus, linear combi-
nations of these columns with integer coefï¬cients represent a listing of edges, possibly
with repeats. Consider such a linear combination with deï¬ning vector of coefï¬cients
c
=
(c
1
;
:
:
:
;
c
8
),
v
=
c
1
v
1
+



c
8
v
8
=
Ac
When will such a combination represent a loop? For one thing the coefï¬cients should all
be nonnegative integers. But this isnâ€™t enough. Hereâ€™s the key idea: we should examine
this combination locally, that is, at each vertex. There we expect the total number of
â€œin-arrowsâ€ ( 1â€™s) to be exactly cancelled by the total number of â€œout-arrowsâ€ (+1â€™s).
In other words, each coordinate of
v should be
0 and therefore
c
2
N
(A): Now letâ€™s
ï¬nd a basis of
N
(A) by using the null space algorithm. To make our work a little easier,
compute the null space of
 A instead of
A:
 A
=
2
6
6
6
6
4
1
0
0
0
1
 1
0
0
 1
1
0
 1
0
0
0
0
0
 1
1
0
0
1
1
 1
0
0
 1
1
 1
0
0
0
0
0
0
0
0
0
 1
1
3
7
7
7
7
5
     !
E
21
(1)
E
32
(1)
E
43
(1)
2
6
6
6
6
4
1
0
0
0
1
 1
0
0
0
1
0
 1
1
 1
0
0
0
0
1
 1
1
0
1
 1
0
0
0
0
0
0
1
 1
0
0
0
0
0
0
 1
1
3
7
7
7
7
5
       !
E
45
(1)
E
34
( 1)
2
6
6
6
6
4
1
0
0
0
1
 1
0
0
0
1
0
 1
1
 1
0
0
0
0
1
 1
1
0
0
0
0
0
0
0
0
0
1
 1
0
0
0
0
0
0
0
0
3
7
7
7
7
5
From this we see that the free variables are
c
4
;
c
5
;
c
6
;
c
8
: The general form of an element
of the null space takes the form
c
=
c
4
v
4
+
c
5
v
5
+
c
6
v
6
+
c
8
v
8

3.8. *COMPUTATIONAL NOTES AND PROJECTS
181
where the columns are given by setting the corresponding free variable to
1 and the
others to
0:
v
4
=
(0;
1;
1;
1;
0;
0;
0;
0)
v
5
=
( 1;
 1;
 1;
0;
1;
0;
0;
0)
v
6
=
(1;
1;
1;
0;
0;
1;
0;
0)
v
8
=
(0;
0;
0;
0;
0;
0;
1;
1)
Now we see that
v
5 and
v
6 donâ€™t represent loops in the sense that we originally deï¬ned
them since we only allow loops to move in the direction of the edge arrow. However,
the basis vectors of coefï¬cients that represent loops as we deï¬ned them are
v
4 and
v
8
:
The loop can be expressed algebraically as
1
2
(v
6
 v
5
): Therefore, all possible loops
can be represented by the basis vectors for the null space.
This null space calculation is trying to tell us something. What the null space says is
that if we allowed for paths that moved against the direction of the edge arrows when
the coefï¬cient of the edge is negative, we would have four independent loops. These
â€œalgebraicâ€ loops include our original loops. They are much easier to calculate since
we donâ€™t have to worry about all the coefï¬cients
c
i being of the same sign. They may
not be very useful in the context of communication networks, since they donâ€™t specify
a ï¬‚ow of information; but in the context of electrical circuits they are very important.
In fact, the correct deï¬nition of a â€œloopâ€ in electrical circuit theory is an element
N
(A)
with integer coefï¬cients.
Project Description: This assignment is intended to introduce you to another application
of the concept of a graph as used as a tool in mathematical modeling. You are given that
the (directed) graph
G has vertex set
V
=
f1;
2;
3;
4;
5;
6g
and edge set
E
=
f(2;
1);
(1;
5);
(2;
5);
(5;
4);
(3;
6);
(4;
2);
(4;
3);
(3;
2);
(6;
4);
(6;
1)g
Answer the following questions about the graph
G: It involves one more idea about
graphs. If we thought of the graph as representing an electrical circuit where the pres-
ence of an edge indicates some electrical object like a resistor or capacitor we could
attach a potential
1. What does the graph look like? You may leave space in your report and draw this by
hand or, if you prefer, you may use the computer drawing applications available to you
on your system.
2. Next view the graph as representing an electrical circuit with potentials
x
1
;
:::;
x
5 to
be assigned to the vertices. Find
N
(A) and
N
(A
T
) using a computer algebra system
available to you. What does the former tell you about the loop structure of the circuit?
Distinguish between graphical and â€œalgebraicâ€ loops. Finally, use that fact that
Ax
=
b
implies that for all
y
2
N
(A
T
),
y
T
b
=
0 to ï¬nd conditions that a vector
b must satisfy
in order for it to be a vector of potential differences for some potential distribution on
the vertices.

182
3. VECTOR SPACES
Review
Chapter 3 Exercises
1. Use the subspace test to determine which of the following subsets
W is a subspace
of the vector space
V :
(a)
V is the space of all
2

2 matrices and
W is the subset of matrices of the form
A
=

a
b
0
c

, where
a;
c are arbitrary scalars and
b is an arbitrary nonzero scalar.
(b)
V is the space of all polynomials and
W is the subset of polynomials that have
nonzero constant term.
2. Let
W
=
R
2;2 and consider the subsets
U
=
f

0
0
a
b

j
a;
b
2
Rg and
V
=
f

0
c
0
d

j
c;
d
2
Rg
(a) Show that
U and
V are subspaces of
W
:
(b) Find a basis for the subspace
U
+
V
:
(c) Find a basis for the subspace
U
\
V
:
3. Show that
u
1
=
(1;
0;
1) and
u
2
=
(1;
 1;
1) form a linearly independent set. Then
ï¬ll this set out to a basis of
R
3
:
4. Show that
1;
1
+
x;
1
+
x
+
x
2 is a basis of
P
3 and compute the coordinates of the
polynomial
2
 x
+
4x
2 with respect to this basis.
5. Let
T
:
V
!
W be a linear operator and suppose that
dim
V
=
4 and
dim
W
=
8:
Determine all possible values for
k
er
T and
range
T
:
6. You are given that
T
:
V
!
R
3 is a linear operator, where
v
1
;
v
2
;
v
3 is a basis of
V
;
and moreover
T
(v
1
)
=
(0;
1;
1);
T
(v
2
)
=
(1;
1;
0) and
T
(v
3
)
=
( 1;
0;
1):
(a) Compute
k
er
T
: Hint: Find conditions on coefï¬cients such that
T
(c
1
v
1
+
c
2
v
2
+
c
3
v
3
)
=
0:
(b) Find
range
T
:
(c) Is the vector
( 3;
2;
5) in
range
T
? Explain.
7. Let
V be a real vector space with basis
v
1
;
v
2
;
v
3 and deï¬ne the coordinate map as
the operator
T that assigns to each
v
2
V the vector of coordinates of
v relative to the
basis
v
1
;
v
2
;
v
3
: Prove the following:
(a)
T
:
V
!
R
3 is a linear operator.
(b)
k
er
T
=
f0g (c)
range
T
=
R
3
:
8. A square matrix
H
=
[h
ij
] is called upper Hessenberg if all entries below the ï¬rst
subdiagonal are zero, that is,
h
ij
=
0 when
i
>
j
+
1: Prove that the set
V of all
n

n
real Hessenberg matrices is a subspace of
R
n;n
:

REVIEW
183
9. Answer True/False:
(a) Every spanning set of a vector space contains a basis of the space.
(b) The set consisting of the zero vector is a linearly independent set.
(c) The dimension of the real vector space
C
n as a vector space over
R is
n.
(d) The vectors
[1;
0]
T,
[0;
1]
T and
[1;
1]
T are linearly dependent.
(e) If
A is a
6

4 matrix and the system
Ax
=
b has inï¬nitely many solutions, then
A
has rank at most 3.
(f) If
A is a
4

4 matrix and the system
Ax
=
b has no solutions, then the columns of
A are linearly independent.
(g) In a vector space the set consisting of the zero vector is a linearly independent set.
(h) Every subspace of an inï¬nite dimensional space is inï¬nite dimensional.
(i) A square matrix is invertible if and only if its rows form a linearly independent set.

184
3. VECTOR SPACES

CHAPTER 4
GEOMETRICAL ASPECTS OF STANDARD SPACES
The standard vector spaces have many important extra features that we have ignored
up to this point. These extra features made it possible to do sophisticated calculations
in the spaces and enhanced our insight into vector spaces by appealing to geometry.
For example, in the geometrical spaces
R
2 and
R
3 that were studied in calculus, it was
possible to compute the length of a vector and angles between vectors. These are visual
concepts that feel very comfortable to us. In this chapter we are going to generalize
these ideas to the standard spaces and their subspaces. We will abstract these ideas to
general vector spaces in Chapter 6.
4.1. Standard Norm and Inner Product
Throughout this chapter vector spaces will be assumed to be subspaces of the standard
vector spaces
R
n or
C
n
:
The Norm Idea
Consider this problem. Can we make sense out of the idea of a sequence of vectors
u
i
converging to a limit vector
u, i.e.,
lim
n!1
u
n
=
u
in standard spaces? What we need is some idea about the length, or norm, of a vector,
so we can say that the length of the difference
u
 u
n should tend to
0 as
n
!
1:
We have seen such an idea in the geometrical spaces
R
2 or
R
3
: There are different
ways to measure length. We shall begin with the most standard method, one which you
have already encountered in calculus. It is one of the outcomes of geometry and the
Pythagorean theorem. There is no compelling reason to stop at geometrical dimensions
of two or three, so here is the general deï¬nition.
DEFINITION 4.1.1. Let
u
=
(x
1
;
x
2
;
:
:
:
;
x
n
)
2
R
n
: The (standard) norm of
u is the
nonnegative real number
jj
u
jj
=
q
x
2
1
+
x
2
2
+



+
x
2
n
EXAMPLE 4.1.2. Compute the norms of the vectors
u
=
(1;
 1;
3) and
v
=
[2;
 1;
0;
4;
2]
T
:
185

186
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
SOLUTION. From deï¬nition
jj
u
jj
=
p
1
2
+
( 1)
2
+
3
2
=
p
11

3:3166
and
jj
v
jj
=
p
2
2
+
( 1)
2
+
0
2
+
4
2
+
2
2
=
p
25
=
5
Even though we canâ€™t really â€œseeâ€ the ï¬ve dimensional vector
y of this example, it is
interesting to note that calculating its length is just as routine as calculating the length
of the three dimensional vector
x: What about complex vectors? Shouldnâ€™t there be an
analogous deï¬nition of norm for such objects? The answer is â€œyes,â€ but we have to be
a little careful. We canâ€™t use the same deï¬nition that we did for real vectors. Consider
the vector
x
=
(1;
1
+
i): The sum of the squares of the coordinates is just
1
2
+
(1
+
i)
2
=
1
+
1
+
2i
 1
=
1
+
2i
This isnâ€™t good. We donâ€™t want â€œlengthâ€ to be measured in complex numbers. The ï¬x
is very simple. We already have a way of measuring the length of a complex number
z,
namely the absolute value
j
z
j: So length squared should be
j
z
j
2
: That is the inspiration
for the following deï¬nition which is entirely consistent with our ï¬rst deï¬nition when
applied to real vectors:
DEFINITION 4.1.3. Let
u
=
(z
1
;
z
2
;
:
:
:
;
z
n
)
2
C
n
: The norm of
x is the nonnegative
real number
jj
u
jj
=
p
jz
1
j
2
+
jz
2
j
2
+



+
jz
n
j
2
Notice that
jz
j
2
=
z
z
: (Remember that if
z
=
a
+
bi, then
z
=
a
 bi and
z
z
=
a
2
+
b
2
=
j
z
j
2
:) Therefore,
jj
u
jj
=
p
z
1
z
1
+
z
2
z
2
+



+
z
n
z
n
EXAMPLE 4.1.4. Compute the norms of the vectors
u
=
(1;
1+i) and
v
=
[2;
 1;
i;
3 2i]
T
SOLUTION. From deï¬nition
jj
u
jj
=
p
1
2
+
(1
 i)(1
+
i)
=
p
1
+
1
+
1

1:7321
and
jj
v
jj
=
p
2
2
+
( 1)
2
+
( i)i
+
(3
+
2i)(3
 2i)
=
p
4
+
1
+
1
+
9
+
4
=
p
19

4:3589
Just as we have asked for other key ideas, we now ask the question â€œWhat are the
essential properties of a norm concept?â€ The answer:
Basic Norm Laws. Let
c be a scalar and
u;
v
2
V where the vector space
V
has the standard norm
jj
jj: Then the following hold.
1.
jj
u
jj

0 with equality if and only if
u
=
0:
2.
jj
cu
jj
=
j
c
j
jj
u
jj
3. (Triangle Inequality)
jj
u
+
v
jj

jj
u
jj
+
jj
v
jj

4.1. STANDARD NORM AND INNER PRODUCT
187
That (1) is true is immediate from the deï¬nition of
jj
u
jj as a sum of the lengths squared
of the coordinates of
u: This sum is zero exactly when each term is zero. Condition (2)
is fairly straightforward too. Suppose
u
=
(z
1
;
z
2
;
:
:
:
;
z
n
), so that
jj
cu
jj
=
p
(cz
1
)cz
1
+
(cz
2
)cz
2
+



+
(cz
n
)cz
n
=
p
(cc)(z
1
z
1
+
z
2
z
2
+



+
z
n
z
n
)
=
p
jcj
2
p
z
1
z
1
+
z
2
z
2
+



+
z
n
z
n
=
jcj
jj
u
jj
The triangle inequality (which gets its name from the triangle with representatives of
the vectors
u;
v
;
u
+
v as its legs) can be proved easily in two or three dimensional
geometrical space by appealing to the fact that the sum of lengths of any two legs of a
triangle is greater that the length of the third leg. A justiï¬cation for higher dimensions
is a nontrivial piece of algebra that we will postpone until after the introduction of inner
products below.
First we consider a few applications of the norm concept. The ï¬rst of these is the idea
Unit Vectors
of â€œnormalizingâ€ a vector. This means ï¬nding a unit vector, which means a vector of
length
1, that has the same direction as the given vector. This process is sometimes
called â€œnormalization.â€ How do we do it? The following simple fact helps.
THEOREM 4.1.5. Let
u be a nonzero vector. Then the vector
w
=
1
jj
u
jj
u
is a unit vector in the same direction as
u:
PROOF. Any two vectors determine the same direction if one is a positive multiple
of the other (actually, this is a deï¬nition of â€œdetermining the same directionâ€). Therefore
we see immediately that
w and
u determine the same direction. Now check the length
of
w and use basic norm law 2 to obtain that
jj
w
jj
=




1
jj
u
jj
u




=




1
jj
u
jj




jj
u
jj
=
jj
u
jj
jj
u
jj
=
1
Hence
w is a unit vector, as desired.
EXAMPLE 4.1.6. Use the normalization procedure to ï¬nd unit vectors in the directions
of vectors
u
=
(2;
 1;
0;
4) and
v
=
( 4;
2;
0;
 8): Conclude that these vectors deter-
mine the same direction.
SOLUTION. Let us ï¬nd a unit vector in the same direction of each vector. We have
parallel
jj
u
jj
=
p
2
2
+
( 1)
2
+
0
2
+
4
2
=
p
21
and
jj
v
jj
=
p
 4
2
+
(2)
2
+
+0
2
+
( 8)
2
=
p
84
=
2
p
21
It follows that unit vectors in the directions of
u and
v, respectively, are
w
1
=
(2;
 1;
0;
4)=
p
21
w
2
=
( 4;
2;
0;
 8)=(2
p
21
)
=
(2;
 1;
0;
4)=
p
21
=
w
1

188
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
It follows that
u and
v determine the same direction.
EXAMPLE 4.1.7. Find a unit vector in the direction of the vector
v
=
(2
+
i;
3):
SOLUTION. We have
jj
v
jj
=
p
2
2
+
(1)
2
+
3
2
=
p
14
It follows that a unit vector in the direction of
v is
w
=
1
p
14
(2
+
i;
3)
In order to work the next example we must express the idea of vector convergence
of a sequence
u
1
;
u
2
;
:
:
: to the vector
u in a sensible way. The norm idea makes this
straightforward: to say that the
u
nâ€™s approach the vector
u should mean that the distance
between
u and
u
n goes to
0 as
n
!
1: But norm measures distance. Therefore the
correct deï¬nition is as follows:
DEFINITION 4.1.8. Let
u
1
;
u
2
;
:
:
: be a sequence of vectors in the vector space
V and
u also a vector in
V
: We say that the sequence converges to
u and write
Convergence of
Vectors
lim
n!1
u
n
=
u
if the sequence of real numbers
jj
u
n
 u
jj converges to
0, i.e.,
lim
n!1
jj
u
n
 u
jj
=
0
EXAMPLE 4.1.9. Use the norm concept to justify the statement that sequence of vectors
u
n converges to a limit vector
u, i.e.,
lim
n!1
u
n
=
u
where
u
n
=
[1
+
1=n
2
;
1=(n
2
+
1);
sin
n=n]
T and
u
=
[1;
0;
0]
T
:
SOLUTION. In our case we have
u
n
 u
=
2
4
1
+
1=n
2
1=(n
2
+
1)
sin
n=n
3
5
 2
4
1
0
0
3
5
=
2
4
1=n
2
1=(n
2
+
1)
sin
n=n
3
5
so
jj
u
n
 u
jj
=
s

1
n

2
+

1
(n
2
+
1)

2
+

sin
n
n

2
!
n!1
p
0
+
0
+
0
=
0
which is what we wanted to show.

4.1. STANDARD NORM AND INNER PRODUCT
189
Î¸
u
v
FIGURE 4.1.1. Angle
 between vectors
u and
v.
The Inner Product Idea
In addition to norm concept we had another fundamental tool in our arsenal when we
tackled two and three dimensional geometrical vectors. This tool was the so-called â€œdot
productâ€ of two vectors. It had many handy applications, but the most powerful of these
was the ability to determine the angle between two vectors. In fact, some authors use
this idea as the basis for deï¬ning dot products as follows: let
 be the angle between
representatives of the vectors
u and
v
: (See Figure 4.1.1.) The dot product of
u and
v
is deï¬ned to be the quantity
kuk
kv
k
cos

: It turned out that with some trigonometry
and algebra, one could come up with a very convenient form for inner products; for
example, in the two dimensional case, if
u
=
(u
1
;
u
2
) and
v
=
(v
1
;
v
2
), then
u

v
=
u
1
v
1
+
u
2
v
2
(4.1.1)
This made the calculation of dot products vastly easier since we didnâ€™t have to use any
trigonometry to compute it. A particularly nice application was that we could determine
cos
 quite easily from the dot product, namely
cos

=
u

v
jj
u
jj
jj
v
jj
(4.1.2)
We have seen that it is useful to try to extend these geometrical ideas to higher dimen-
sions even if we canâ€™t literally use trigonometry and the like. So what we do is reverse
the sequence of ideas weâ€™ve discussed and take Equation 4.1.1 as the prototype for our
next deï¬nition. As with norms, we are going to have to distinguish carefully between
the cases of real or complex scalars. First we focus on the more common case of real
coefï¬cients.
DEFINITION 4.1.10. Let
u
=
(x
1
;
x
2
;
:
:
:
;
x
n
) and
v
=
(y
1
;
y
2
;
:
:
:
;
y
n
) be vectors in
R
n
: The (standard) inner product, also called the dot product of
u and
v, is the real
number
u

v
=
u
T
v
=
x
1
y
1
+
x
2
y
2
+
:
:
:
+
x
n
y
n
We can see from the ï¬rst form of this deï¬nition where the term â€œinner productâ€ came
from. Recall from Section 2.4 of Chapter 2 that the matrix product
u
T
v is called the
inner product of these two vectors.
EXAMPLE 4.1.11. Compute the dot product of the vectors
u
=
(1;
 1;
3;
2) and
v
=
(2;
 1;
0;
4) in
R
4
:

190
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
SOLUTION. From deï¬nition
u

v
=
1

2
+
( 1)

( 1)
+
3

0
+
2

4
=
11:
There is a wonderful connection between the standard inner product and the standard
norm for vectors which is immediately evident from the deï¬nitions. Here it is:
jj
u
jj
=
p
u

u
(4.1.3)
Thus computing norms amounts to an inner product calculation followed by a square
root. Actually, we can even avoid the square root and put the equation in the form
jj
u
jj
2
=
u

u
We say that the standard norm is induced by the standard inner product. We would
like this property to carry over to complex vectors. Now we have to be a bit careful.
In general, the quantity
u
T
u may not even be a real number, or may be negative. This
means that
p
u
T
u could be complex, which doesnâ€™t seem like a good idea for measuring
â€œlength.â€ So how can we avoid this problem? Recall that when we introduced trans-
poses, we also introduced Hermitian transposes and remarked that for complex vectors,
this is a more natural tool than the transpose. Now we can back up that remark! Recall
the deï¬nition for complex norm: for
u
=
(z
1
;
z
2
;
:
:
:
;
z
n
)
2
C
n, the norm of
x is the
nonnegative real number
jj
u
jj
=
p
z
1
z
1
+
z
2
z
2
+



+
z
n
z
n
=
p
u
H
u:
Therefore, in our deï¬nition of complex â€œdot productsâ€ we had better replace transposes
by Hermitian transposes. This inspires the deï¬nition
DEFINITION 4.1.12. Let
u
=
(w
1
;
w
2
;
:
:
:
;
w
n
) and
v
=
(z
1
;
z
2
;
:
:
:
;
z
n
) be vectors
in
C
n
: The (standard) inner product, which is also called the dot product of
u and
v, is
the complex number
u

v
=
w
1
z
1
+
w
2
z
2
+
:
:
:
+
w
n
z
n
=
u
H
v
With this deï¬nition we still have the close connection given above in (4.1.3) between
norm and standard inner product of complex vectors.
EXAMPLE 4.1.13. Compute the dot product of the vectors
u
=
(1
+
2i;
i;
1) and
v
=
(i;
 1
 i;
0) in
C
3
:
SOLUTION. Just apply the deï¬nition:
u

v
=
(1
+
2i)
i
+
i( 1
 i)
+
1

0
=
(1
 2i)i
 i( 1
 i)
=
1
+
2i

4.1. STANDARD NORM AND INNER PRODUCT
191
What are the really essential deï¬ning properties of these standard inner products? It
turns out that we can answer the question for both real and complex inner products at
once. However, we should bear in mind that most of the time we will be dealing with
real dot products, and in this case all the dot products in questions are real numbers, so
that any reference to a complex conjugate can be omitted.
Basic Inner Product Laws. Let
c be a scalar and
u;
v
;
w
2
V where
V is a
vector space with the standard inner product. Then the following hold.
1.
u

u

0 with equality if and only if
u
=
0
2.
u

v
=
v

u
3.
u

(v
+
w
)
=
u

v
+
u

w
4.
u

(cv
)
=
c(u

v
)
That (1) is true is immediate from the fact that
u

u
=
u
H
u is a sum of the length
squared of the coordinates of
u: This sum is zero exactly when each term is zero. Con-
dition (2) follows from this line of calculation:
v

u
=
v
H
u
=

v
H
u

T
=
(v
H
u)
H
=
u
H
v
=
u

v
One point that stands out in this calculation is the following
Caution: A key difference between real and complex inner products is in the commu-
tative law
u

v
=
v

u, which holds for real vectors but not for complex vectors, where
u

v
=
v

u
:
Conditions (3) and (4) are similarly veriï¬ed and left to the exercises. We can also use
(4) to prove this fact for real vectors:
(cu)

v
=
v

(cu)
=
c(v

u)
=
c(u

v
)
If we are dealing with complex dot products, matters are a bit trickier. One can show
then that
(cu)

v
=
c(u

v
)
so we donâ€™t quite have the symmetry that we have for real dots.
4.1 Exercises
1. For the following pairs of vectors, calculate
jjujj;
u

v
; and a unit vector in the
direction of
u
:
(a)
u
=
[1;
 1]
T and
v
=
[ 2;
3]
T
(b)u
=
(2;
0;
1) and
v
=
( 3;
4;
1)
(c)u
=
[1;
2;
2
 i;
0]
T and
v
=
[ 2;
1;
1;
1]
T
(d)
u
=
(1
+
2i;
2
+
i) and
v
=
(4
+
3i;
1):
2. Verify that
u
n converges to a limit vector
u, where
u
n
=
[2=n;
(1
+
n
2
)=(2n
2
+
n
+
1)]
T by using the norm deï¬nition of vector limit.
3. Compute an angle
 between the following pairs of real vectors.
(a)
(3;
 5) and
(2;
4) (b)
(3;
4) and
(4;
 3)
(c)
(1;
1;
2) and
(2;
 1;
3)

192
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
4. Let
c
=
3,
u
=
(4;
 1;
2;
3) and
v
=
( 2;
2;
 2;
2):
(a) Verify that the three basic norm laws hold for these vectors and scalars.
(b) Verify the four basic inner product laws for these vectors and scalars.
5. Verify basic norm law 1:
jj
u
jj

0 with equality if and only if
u
=
0:
6. Prove that if
v is a vector and
c is a positive real, then normalizing
v and normalizing
cv yield the same unit vector. How are the normalized vectors related if
c is negative?
7. Show that for real vectors
u,
v and real number
c one has
(cu)

v
=
v

(cu)
=
c(v

u)
=
c(u

v
)
8. Show that if
u;
v
;
w
2
R
n (or
C
n) and
c is a scalar, then
(a)
u

(v
+
w
)
=
u

v
+
u

w
(b)
u(cv
)
=
c(u

v
)
9. Show from deï¬nition that if
lim
n!1
u
n
=
u; where
u
n
=
(x
n
;
y
n
)
2
R
2 and
u
=
(x;
y
); then
lim
n!1
x
n
=
x and
lim
n!1
y
n
=
y
:
10. Show that for any two vectors
u;
v in the same space,
jk
uk
 kv
kj

ku
 w
k
:
Hint: Apply the triangle inequality to
u
+
(v
 u) and
v
+
(u
 v
):
4.2. Applications of Norms and Inner Products
Projections and Angles
Now that we have dot products under our belts we can tackle geometrical issues like
angles between vectors in higher dimensions. For the matter of angles, we will stick to
real vector spaces, though we could do it for complex vector spaces with a little extra
work. What we would like to do is take Equation 4.1.2 as the deï¬nition of the angle
between two vectors. Thereâ€™s one slight problem: how do we know that it will give a
quantity that could be a cosine? After all, cosines only take on values between
 1 and
1: We could use some help and the Cauchy-Bunyakovsky-Schwarz inequality (CBS for
short) is just what we need:
THEOREM 4.2.1. For vectors
u;
v
2
R
n,
CBS Inequality
j
u

v
j

jj
u
jj
jj
v
jj

4.2. APPLICATIONS OF NORMS AND INNER PRODUCTS
193
PROOF. Let
c be an arbitrary real number and compute the nonnegative quantity
f
(c)
=
jj
u
+
cv
jj
2
=
(u
+
cv
)

(u
+
cv
)
=
u

u
+
u

(cv
)
+
(cv
)

u
+
(cv
)

(cv
)
jj
u
jj
2
+
2c(u

v
)
+
c
2
jj
v
jj
2
:
The function
f
(c) is therefore a quadratic in the variable
c with nonnegative values. The
low point of this quadratic occurs where
f
0
(c)
=
0, that is, where
0
=
2(u

v
)
+
2cjj
v
jj
2
i.e., where
c
=
 (u

v
)
jj
v
jj
2
Evaluate
f at this point to get that
0

jj
u
jj
2
 2
(u

v
)
2
jj
v
jj
2
+
(u

v
)
2
jj
v
jj
4
jj
v
jj
2
=
jj
u
jj
2
 (u

v
)
2
jj
v
jj
2
Now add
(u

v
)
2
=jj
v
jj
2 to both sides and multiply by
jj
v
jj
2 to obtain that
(u

v
)
2

jj
u
jj
2
jj
v
jj
2
Take square roots and we have the desired inequality.
This inequality has a number of useful applications. For instance, because of it we
can articulate the following deï¬nition. There is a certain ambiguity in discussing angle
between vectors, since more than one angle works. Actually itâ€™s the cosine of these
angles that is really unique.
DEFINITION 4.2.2. For vectors
u;
v
2
R
n we deï¬ne the angle between
u and
v to be
any angle
 satisfying
Angle Between
Vectors
cos

=
u

v
jj
u
jj
jj
v
jj
Thanks to the CBS Inequality, we know that
j
u

v
j=(jj
u
jj
jj
v
jj)

1 so that this
formula for
cos
 makes sense.
EXAMPLE 4.2.3. Find the angle between the vectors
u
=
(1;
1;
0;
1) and
v
=
(1;
1;
1;
1)
in
R
4
:
SOLUTION. We have that
cos

=
(1;
1;
0;
1)

(1;
1;
1;
1)
jj
(1;
1;
0;
1)
jj
jj
(1;
1;
1;
1)
jj
=
3
2
p
3
=
p
3
2
:
Hence we can take

=

=6:
EXAMPLE 4.2.4. Use the laws of inner products and the CBS Inequality to verify the
triangle inequality for vectors
u and
v
: What happens to this inequality if we also know
that
u

v
=
0?

194
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
SOLUTION. Here the trick is to avoid square roots. So square both sides of Equa-
tion 4.1.3 to obtain that
jju
+
v
j
j
2
=
(u
+
v
)

(u
+
v
)
=
u

u
+
u

v
+
v

u
+
v

v
=
jjujj
2
+
2(u

v
)
+
jj
v
jj
2

jjujj
2
+
2
ju

v
j
+
jjv
j
j
2

jjujj
2
+
2
jjujj
j
jv
jj
+
jjv
j
j
2
=
(j
jujj
+
jjv
j
j)
2
where the last inequality follows from the CBS Inequality. If
u

v
=
0; then the single
inequality can be replaced by an equality.
We have just seen a very important case of angles between vectors that warrants its own
name. Recall from geometry that two vectors are perpendicular or orthogonal if the
angle between them is

=2: Since
cos

=2
=
0, we see that this amounts to the equation
u

v
=
0: Now we can extend the perpendicularity idea to arbitrary vectors, including
complex vectors.
DEFINITION 4.2.5.
Two vectors
u and
v in the same vector space are orthogonal if
Orthogonal
Vectors
u

v
=
0: In this case we write
u
?
v
:
In the case that one of the vectors is the zero vector, we have the little oddity that the
zero vector is orthogonal to every other vector, since the dot product is always
0 in this
case. Some authors require that
u and
v be nonzero as part of the deï¬nition. Itâ€™s a minor
point and we wonâ€™t worry about it. When
u and
v are orthogonal, i.e.,
u

v
=
0, we
see from the third equality in the derivation of CBS above that
jju
+
v
j
j
2
=
jjujj
2
+
jjv
j
j
2
which is really the Pythagorean theorem for vectors in
R
n
:
EXAMPLE 4.2.6. Determine if the following pairs of vectors are orthogonal.
Pythagorean
Theorem
(a)
u
=
(2;
 1;
3;
1) and
v
=
(1;
2;
1;
 2):
(b)
u
=
(1
+
i;
2) and
v
=
( 2i;
1
+
i):
SOLUTION. For (a) we calculate
u

v
=
2

1
+
( 1)2
+
3

1
+
1( 2)
=
1
so that
u is not orthogonal to
v
: For (b) we calculate
u

v
=
(1
 i)( 2i)
+
2(1
+
i)
=
 2i
 2
+
2
+
2i
=
0:
so that
u is orthogonal to
v in this case.
The next example illustrates a really handy little trick that is well worth remembering.
EXAMPLE 4.2.7. Given a vector
(a;
b) in
R
n or
C
n
; ï¬nd a vector orthogonal to
(a;
b):

4.2. APPLICATIONS OF NORMS AND INNER PRODUCTS
195
v
u
u
p
u - p
Î¸
Î¸
FIGURE 4.2.1. Angle between vectors
u and
v, and projection
p of
u along
v .
Solution. Simply interchange coordinates, conjugate them (this does nothing if entries
are real) and insert a minus sign in front of one of the coordinates, say the ï¬rst. We
obtain
( b
;
a): Now check that
(a;
b)

( b
;
a):
=
 a
( b)
+
b
a
=
0
By parallel vectors we mean two vectors that are nonzero scalar multiples of each other.
Notice that parallel vectors may determine the same or opposite directions. Our next
application of the dot product relates back to a fact that we learned in geometry: given
two nonzero vectors in the plane, it is always possible to resolve one of them into a sum
of a vector parallel to the other and a vector orthogonal to the other (see Figure 4.2.1).
The parallel component was called the projection of one vector along the other. As a
matter of fact, we can develop this same idea in arbitrary standard vector spaces. That is
the content of the following useful fact. Remember, by the way, that â€œparallelâ€ vectors
simply means that the vectors in question are scalar multiples of each other (any scalar).
THEOREM 4.2.8. Let
u and
v be vectors in a vector space with
v
6=
0: Let
Projection
Formula for
Vectors
p
=
v

u
v

v
v
and
q
=
u
 p
Then
p is parallel to
v,
q is orthogonal to
v and
u
=
p
+
q:
PROOF. Let
p
=
cv
; an arbitrary multiple of
v
: Then
p is automatically parallel to
v
: Impose the constraint that
q
=
u
 p be orthogonal to
v
: This means, by deï¬nition,
that
0
=
v

q
=
v

(u
 p)
=
v

u
 v

(cv
):
Add
v

(cv
) to both sides and pull the scalar
c outside the dot product to obtain that
c(v

v
)
=
v

u
and therefore
c
=
v

u
v

v
So for this choice of
c;
q is orthogonal to
p: Clearly,
u
=
p
+
u
 p; so the proof is
complete.

196
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
It is customary to call the vector
p of this theorem the projection of
u along
v. We
write
Components
and Projections
pro
j
v
u
=
v

u
v

v
v
The projection of one vector along another is itself a vector quantity. A scalar quantity
that is frequently associated with these calculations is the so-called component of
u
along
v
: It is deï¬ned as
comp
v
u
=
v

u
jj
v
jj
The connection between these two quantities is that
pro
j
v
u
=
comp
v
u
v
jj
v
jj
Notice that
v
=jj
v
jj is a unit vector in the same direction as
v
: Therefore,
comp
v
u is
the signed magnitude of the projection of
u along
v
:
EXAMPLE 4.2.9. Calculate the projection and component of
u
=
(1;
 1;
1;
1) along
the
v
=
(0;
1;
 2;
 1) and verify that
u
 p
?
v
:
SOLUTION. We have that
v

u
=
0

1
+
1( 1)
+
( 2)1
+
( 1)1
=
 4
v

v
=
0
2
+
1
2
+
( 2)
2
+
( 1)
2
=
6
so that
p
=
pro
j
v
u
=
 4
6
(0;
1;
 2;
 1)
=
1
3
(0;
 2;
4;
2)
It follows that
u
 p
=
1
3
(3;
 1;
 1;
1)
and
(u
 p)

v
=
1
3
(3

0
+
1( 1)
+
( 1)( 2)
+
1( 1))
=
0:
Also, the component of
u along
v is
comp
v
u
=
v

u
jj
v
jj
=
 4
p
6
:
Least Squares
EXAMPLE 4.2.10. You are using a pound scale to measure weights for produce sales
when you notice that your scale is about to break. The vendor at the next stall is leaving
and loans you another scale as she departs. Soon afterwards your scale breaks. You
then realize that the new scale is in units you donâ€™t recognize. You happen to have a
some known weights that are approximately
2,
5 and
7 pounds respectively. When you
weigh these items on the new scale you get the numbers
0:9,
2:4 and
3:2: You get your
calculator out and hypothesize that the unit of weight should be some constant multiple

4.2. APPLICATIONS OF NORMS AND INNER PRODUCTS
197
of pounds. Model this information as a system of equations. Is it clear from this system
what the units of the scale are?
SOLUTION. Express the relationship between the weight
p in pounds and the weight
w
in unknown units as
w

c
=
p, where
c is an unknown. Your data show that we have
0:7c
=
2
2:4c
=
5
3:4c
=
7
As a system of three equations in one unknown you see immediately that this overdeter-
mined system (too many equations) is inconsistent. After all, the pound weights were
only approximate and in addition there is always some error in measurement. Conse-
quently, it is not at all clear what the units of the scale are, and we will have to investigate
this problem further. You could just average the three inconsistent values of
c, thereby
obtaining
c
=
(2=0:7
+
5=2:4
+
7=3:4)=3
=
2:3331
It isnâ€™t at all clear that this should be a good strategy.
There really is a better way and it will lead to a slightly different estimate of the number
c: This method, called the method of least squares, was invented by C. F. Gauss to
handle uncertainties in orbital calculations in astronomy.
Here is the basic problem: suppose we have data that leads to a system of equations for
unknowns that we want to solve for, but the data has errors in it and consequently leads
to an inconsistent linear system
Ax
=
b
How do we ï¬nd the â€œbestâ€ approximate solution? One could answer this in many ways.
One of the most commonly accepted ideas is one that goes back to C. F. Gauss: the
quantity so-called residual
r
=
b
 Ax should be
0 so its departure from
0 is a measure
of our error. Thus we should try to ï¬nd a value of the unknown
x that minimizes the
norm of the residual squared, i.e., a â€œsolutionâ€
x so that
jjb
 Axjj
2
is minimized. Such a solution is called a â€œleast squaresâ€ solution to the system. This
is used extensively by statisticians, in situations where one has many estimates for un-
known parameters which, taken together, are not perfectly consistent. Letâ€™s try to get a
ï¬x on this problem. Even the 1 variable case is instructive, so letâ€™s use the preceding
example.
In this case the coefï¬cient matrix
A is the column vector
a
=
[0:7;
2:4;
3:4]
T and the
right hand side vector is
b
=
[2;
5;
7]
T
: What we are really trying to ï¬nd is a value
of the scalar
x
=
c such that
b
 Ax
=
b
 xa is a minimum. Here is a geometrical
interpretation: we want to ï¬nd the multiple of the vector
a that is closest to
b: Geometry
suggests that this minimum occurs when
b
 xa is orthogonal to
a, in other words, when
xa is the projection of
b along
a: Inspection of the projection formula shows us that we
must have
x
=
a

b
a

a
=
0:7

2
+
2:4

5
+
3:4

7
0:7

0:7
+
2:4

2:4
+
3:4

3:4
=
2:
088
7:

198
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
a1
a2
a2
x2
a1
x1
b
x
-A
b
x
  A
=
+
V
FIGURE 4.2.2. The vector in subspace
C
(A) nearest to
b.
Notice that this doesnâ€™t solve any of the original equations exactly, but it is, in a certain
sense, the best approximate solution to all three equations taken together. Also, this
solution is not the same as the average of the solutions to the three equations, which we
computed to be
2:3331:
Now how do we tackle the more general system
Ax
=
b? Since
Ax is just a linear
combination of the columns, what we should ï¬nd is the vector of this form which is
closest to the vector
b: See Figure 6 for a picture of the situation with
n
=
2: Our
experience with the 1-dimensional case suggests that we should require that the residual
be orthogonal to each column of
A; that is,
a
i

(b
 Ax)
=
a
T
i
(b
 Ax)
=
0, for
all columns
a
i of
A: Each column gives rise to one equation. We can write all these
equations at once in the form of the so-called normal equations:
Normal
Equations
A
T
Ax
=
A
T
b
In fact, this is the same set of equations we get if we were to apply calculus to the scalar
function of variables
x
1
;
x
2
;
:::;
x
n given as
f
(x)
=
jjb
 Axjj
2 and search for a local
minimum by ï¬nding all partials and setting them equal to
0: Any solution to this system
will minimize the norm of the difference
b
 Ax as
x ranges over all elements of
R
n
:
The coefï¬cient matrix
B
=
A
T
A of the normal system has some pleasant properties.
For one, it is a symmetric matrix. For another, it is a positive semideï¬nite matrix, by
which we mean that
B is a square (say
n

n) matrix such that
x
T
B
x

0 for all vectors
x
2
R
n
: In fact, in some cases
B is even better behaved because it is a positive deï¬nite
matrix , by which we mean that
B is a square (say
n

n) matrix such that
x
T
B
x

0
Positive Deï¬nite
Matrix
for all vectors
x
2
R
n
:
Does there exist a solution to the normal equations? The answer is â€œyes.â€ In general,
any solution to the normal equations minimizes the residual norm and is called a least
squares solution to the problem
Ax
=
b: Since we now have two versions of â€œsolutionâ€
for the system
Ax
=
b; we should distinguish between them in situations which may
refer to either. If the vector
x actually satisï¬es the equation
Ax
=
b; we call
x a
genuine solution to the system to contrast it with a least squares solution. Certainly,
every genuine solution is a least squares solution, but the coverse will not be true if the
original system is inconsistent. We leave the veriï¬cations as exercises.

4.2. APPLICATIONS OF NORMS AND INNER PRODUCTS
199
The normal equations are guaranteed to be consistent â€“ a nontrivial fact â€“ and will have
inï¬nitely many solutions if
A
T
A is a singular matrix. However, we will focus on the
most common case, namely that in which
A is a rank
n matrix. Recall that in this case
we say that
A has full column rank. We can show that the
n

n matrix
A
T
A is also
rank
n: This means that it is an invertible matrix and therefore the solution to the normal
equations is unique. Here is the necessary fact.
THEOREM 4.2.11. Suppose that the
m

n matrix
A has full column rank
n: Then the
n

n matrix
A
T
A also has rank
n and is invertible.
PROOF. Assume
A has rank
n: Now suppose that for some vector
x we have
0
=
A
T
Ax
Multiply on the left by
x
T to obtain that
0
=
x
T
0
=
x
T
A
T
Ax
=
(Ax)
T
(Ax)
=
jj
Ax
jj
2
so that
Ax
=
0: However, we know by Theorem 1.4.15 that the homogeneous system
with
A as its coefï¬cient matrix must have a unique solution. Of course, this solution is
the zero vector. Therefore,
x
=
0: It follows that the square matrix
A
T
A has rank
n
(and is invertible as well) by Theorem 2.5.9
EXAMPLE 4.2.12. Two parameters,
x
1 and
x
2, are linearly related. Three samples are
taken that lead to the system of equations
2x
1
+
x
2
=
0
x
1
+
x
2
=
0
2x
1
+
x
2
=
2
Show this system is inconsistent, and ï¬nd the least squares solution for
x
=
(x
1
;
x
2
):
What is the minimum norm of the residual
b
 Ax in this case?
SOLUTION. In this case it is obvious that the system is inconsistent: the ï¬rst and third
equations have the same quantity,
2x
1
+
x
2, equal to different values
0 and
2: Of course,
we could have set up the augmented matrix of the system and found a pivot in the right
hand side column as well. We see that the (rank
2) coefï¬cient matrix
A and right hand
side
b are
A
=
2
4
2
1
1
1
2
1
3
5
;
b
=
2
4
0
0
2
3
5
Thus
A
T
A
=

2
1
2
1
1
1

2
4
2
1
1
1
2
1
3
5
=

9
5
5
3

and
A
T
b
=

2
1
2
1
1
1

2
4
0
0
2
3
5
=

4
2


200
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
As predicted by the preceding theorem,
A
T
A is invertible and we recall the
22 formula
for the inverse:
(A
T
A)
 1
=

9
5
5
3

 1
=
1
2

3
 5
 5
9

so that the unique least squares solution is
x
=
(A
T
A)
 1
A
T
b
=
1
2

3
 5
 5
9


4
2

=

1
 1

The minimum value for the residual
b
 Ax occurs when
x is a least squares solution,
so we get
b
 Ax
=
2
4
0
0
2
3
5
 2
4
2
1
1
1
2
1
3
5

1
 1

=
2
4
0
0
2
3
5
 2
4
1
0
1
3
5
=
2
4
 1
0
1
3
5
and therefore
jj
b
 Ax
jj
=
p
2
=
1:414
This isnâ€™t terribly small, but itâ€™s the best we can do with this system. This number tells
us the system is badly inconsistent.
4.2 Exercises
1. Determine if the following pairs of vectors are orthogonal, and if so, verify that the
Pythagorean theorem holds for the pair. If not, use the projection formula (which is
valid even in the complex case â€“ assume this) to ï¬nd the projection of
u along the
vector
v and express
u as the sum of a vector parallel to
v and a vector orthogonal to
v
:
(a)
u
=
( 2;
1;
3) and
v
=
(1;
2;
0)
(b)
u
=
(i;
2) and
v
=
(2;
i)
(c)
u
=
(1;
1;
0;
 1) and
v
=
(1;
 1;
3;
0)
(d)
u
=
(i;
1) and
v
=
(1;
 i)
2. Let
v
1
=
[1;
0;
1]
T and
v
2
=
[1;
1;
 1]
T
:
(a) Find the cosine of the angle between the vectors
v
1 and
v
2
:
(b) Find unit vectors in the directions of
v
1 and
v
2
:
(c) Find the projection and component of the vector
v
1 along
v
2
:
(d) Verify the CBS Inequality for the vectors
v
1 and
v
2
:
3. Repeat Exercise 2 with
v
1
=
( 1;
0;
2) and
v
2
=
(1;
1;
 1):
4. For the following, ï¬nd the normal equations and solve them for the system
Ax
=
b:
Also ï¬nd the residual vector and its norm in each case. (Note: these systems need not
have a unique least squares solution.)
(a)
A
=
2
4
2
 2
1
1
3
1
3
5 ,
b
=
2
4
2
 1
1
3
5
(b)
A
=
2
4
1
 1
0
1
1
2
1
2
3
3
5,
b
=
2
4
1
1
3
3
5

4.2. APPLICATIONS OF NORMS AND INNER PRODUCTS
201
(c)
A
=
2
6
6
4
0
2
1
1
1
0
 1
1
0
1
 2
1
3
7
7
5 ,
b
=
2
6
6
4
3
1
0
0
3
7
7
5
(d)
A
=
2
4
1
2
0
1
0
2
1
2
3
3
5,
b
=
2
4
1
1
3
3
5
5. Show that if two vectors
u and
v satisfy the equation
jju
+
v
j
j
2
=
jjujj
2
+
j
jv
jj
2
;
then
u and
v must be orthogonal. Hint: Express each norm in terms of dot products.
6. Suppose that you have collected data points
(x
k
;
y
k
) that are theoretically linearly
related by a line of the form
y
=
ax
+
b: Each data point gives an equation for
a and
b: Suppose the collected data points are
(0;
:3);
(1;
1:1);
(2;
2);
(3;
3:5); and
(3:5;
3:6):
Write out the system of 5 equations that result, compute the normal equations and solve
them to ï¬nd the line that best ï¬ts this data. A calculator or computer might be helpful.
7. Let
A be an
m

n real matrix and
B
=
A
T
A: Show the following
(a) The matrix
B is nonnegative deï¬nite.
(b) If
A has full column rank, then
B is positive deï¬nite.
8. Show that the CBS inequality is valid for complex vectors
u and
v by evaluating the
nonnegative expression
jju
+
cv
jj
2 with the complex dot product and evaluating it at
c
=
jjujj
2
=
(u

v
) in the case
u

v
6=
0:
9. In Example 4.2.10 two values of
c are calculated: The average value and the best
squares value. Calculate the resulting residual and its norm in each case.
10. Show that if
A is a rank
1 real matrix, then the normal equations with coefï¬cient
matrix
A are consistent. Hint: Use Exercise 13.
11. Show that if
u and
v are vectors of the same length, then
u
+
v is orthogonal to
u
+
v
: Sketch a picture in the plane and interpret this result geometrically.
12. Verify that the projection formula (Theorem 4.2.8) is valid for complex vectors.
13. If
A is a real matrix, then
A
T
A is symmetric nonnegative deï¬nite.
14. If
A is a real matrix, then
A
T
A is positive deï¬nite if and only if
A has full column
rank.

202
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
4.3. Unitary and Orthogonal Matrices
Orthogonal Sets of Vectors
In our discussion of bases in Chapter 3, we saw that linear independence of a set of
set of vectors was a key idea for understanding the nature of vector spaces. One of
our examples of a linearly independent set (a basis, actually) was the standard basis
e
1
;
e
2
;
:
:
:
;
e
n of
R
n
: Here
e
i is the vector with a
1 in the
ith coordinate and zeros
elsewhere. In the case of geometrical vectors and
n
=
3, these are just the familiar
i;
j;
k: These vectors have some particularly nice properties that go beyond linear inde-
pendence. For one, each is a unit vector with respect to the standard norm. Furthermore,
these vectors are pairwise orthogonal to each other. These properties are so desirable
that we elevate them to the status of a deï¬nition.
DEFINITION 4.3.1. The set of vectors
v
1
;
v
2
;
:
:
:
;
v
n in a standard vector space are
said to be an orthogonal set if
v
i

v
j
=
0 whenever
i
6=
j: If, in addition, each vector
has unit length, i.e.,
v
i

v
i
=
1 then the set of vectors is said to be an orthonormal set
of vectors.
EXAMPLE 4.3.2. Which of the following sets of vectors are orthogonal? Orthonormal?
Use the standard inner product in each case.
(a)
f(3=5;
4=5);
( 4=5;
3=5)g
(b)
f(1;
 1;
0);
(1;
1;
0);
(0;
0;
1)g
SOLUTION. In the case of (a) we let
v
1
=
(3=5;
4=5);
v
2
=
( 4=5;
3=5) to obtain that
v
1

v
2
=
 12
25
+
12
25
=
0 and
v
1

v
1
=
9
25
+
16
25
=
1
=
v
2

v
2
:
It follows that the ï¬rst set of vectors is an orthonormal set.
In the case of (ii) we let
v
1
=
(1;
 1;
0);
v
2
=
(1;
1;
0);
v
3
=
(0;
0;
1) and see that
v
1

v
2
=
1

1
 1

1
+
0

0
=
0 and
v
1

v
3
=
1

0
 1

0
+
0

1
=
0
=
v
2

v
3
Hence this set of vectors is orthogonal, but
v
1

v
1
=
1

1
+
( 1)

( 1)
+
0
=
2; which
is sufï¬cient to show that the vectors do not form an orthonormal set.
One of the principal reasons that orthogonal sets are so desirable is the following key
fact, which we call the orthogonal coordinates theorem.
THEOREM 4.3.3.
Let
v
1
;
v
2
;
:
:
:
;
v
n be an orthogonal set of nonzero vectors and
Orthogonal
Coordinates
Theorem
suppose that
v
2
spanfv
1
;
v
2
;
:
:
:
;
v
n
g: Then
v can be expressed uniquely (up to
order) as a linear combination of
v
1
;
v
2
;
:
:
:
;
v
n, namely
v
=
v
1

v
v
1

v
1
v
1
+
v
2

v
v
2

v
2
v
2
+
:
:
:
+
v
n

v
v
n

v
n
v
n

4.3. UNITARY AND ORTHOGONAL MATRICES
203
PROOF. Since
v
2
spanf
v
1
;
v
2
;
:
:
:
;
v
n
g, we know that
v is expressible as some
linear combination of the
v
iâ€™s, say
v
=
c
1
v
1
+
c
2
v
2
+
:
:
:
+
c
n
v
n
Now we carry out a simple but wonderful trick that one sees used frequently with or-
thogonal sets, namely, take the inner product of both sides with the vector
v
k
: Also, we
have that
h
v
k
;
v
j
i
=
0 if
j
6=
k, so we obtain
v
k

v
=
v
k

(c
1
v
1
+
c
2
v
2
+
:
:
:
+
c
n
v
n
)
=
c
1
v
k

v
1
+
c
2
v
k

v
2
+
:
:
:
+
c
n
v
k

v
n
=
c
k
v
k

v
k
Since
v
k
6=
0, it follows that
v
k

v
k
6=
0, so that we may solve for
c
k to obtain that
c
k
=
v
k

v
v
k

v
k
This proves that the coefï¬cients
c
k are unique and establishes the formula of the theo-
rem.
COROLLARY 4.3.4. Every orthogonal set of nonzero vectors is linearly independent.
PROOF. Consider a linear combination of the vectors
v
1
;
v
2
;
:
:
:
;
v
n
: If some lin-
ear combination were to sum to zero, say
0
=
c
1
v
1
+
c
2
v
2
+
:
:
:
+
c
n
v
n
it would follow from the preceding theorem that
c
k
=
v
k

0
v
k

v
k
=
0
It follows from the deï¬nition of linear independence that the vectors
v
1
;
v
2
;
:
:
:
;
v
n are
linearly independent.
Several observations are worth noting:
 The converse of the corollary is false, that is, not every linearly independent
set of vectors is orthogonal. For an example, consider the linearly independent
vectors
v
1
=
(1;
0);
v
2
=
(1;
1) in
V
=
R
2
:
 The vector
v
k
v
v
k
v
k
v
k looks familiar. In fact, it is the projection of the vector
v along the vector
v
k
: Thus, we can say Theorem 6.2.18 in words as follows:
any linear combination of an orthogonal set of nonzero vectors is the sum of its
projections in the direction of each vector in the set.
 The coefï¬cients
c
k of Theorem 6.2.18 are also familiar: they are the coordinates
of
v relative to the basis
v
1
;
v
2
;
:
:
:
;
v
n
: This terminology was introduced in
Section 3.3 of Chapter 3. Thus Theorem 6.2.18 shows us that coordinates are
rather easy to calculate with respect to an orthogonal basis. Contrast this with
Example 3.3.13 of Chapter 3.
 The formula of Theorem 6.2.18 simpliï¬es very nicely if vectors
v
1
;
v
2
;
:
:
:
;
v
n
form an orthonormal set (which automatically consists of nonzero vectors!),
namely
v
=
v
1

v
v
1
+
v
2

v
v
2
+
:
:
:
+
v
n

v
v
n

204
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
 Given an orthogonal set of nonzero vectors, it is easy to manufacture an or-
thonormal set of vectors. Simply replace every vector in the original set by the
vector divided by its length.
Orthogonal and Unitary Matrices
In general, if we want to determine the coordinates of a vector
b with respect to a certain
basis of vectors in
R
n or
C
n, we stack the basis vectors together to form a matrix
A,
then solve the system
Ax
=
b for the vector of coordinates
x of
b with respect to this
basis. In fact,
x
=
A
 1
b: Now we have seen that if the basis vectors happen to form
an orthonormal set, the situation is much simpler and we certainly donâ€™t have to ï¬nd
A
 1
: Is this simplicity reï¬‚ected in properties of the matrix
A? The answer is â€œyesâ€ and
we can see this as follows: suppose that
u
1
;
u
2
;
:
:
:
;
u
n is an orthonormal basis of
R
n
and let
A
=
[
u
1
;
u
2
;
:
:
:
;
u
n
]: Orthogonality says that
u
T
i
u
j
=
Ã†
ij
: This means that
the matrix
A
T
A, whose
(i;
j
)th entry is
u
T
n
u
n, is simply
[Ã†
ij
]
=
I, that is,
A
T
A
=
I
:
Now recall that Theorem 2.5.9 of Chapter 2 shows that if a matrix acts as an inverse
on one side and the matrices in question are square, then the matrix really is the two-
sided inverse. Hence,
A
 1
=
A
T
: A similar argument works if
u
1
;
u
2
;
:
:
:
;
u
n is an
orthonormal basis of
C
n and we use Hermitian transpose instead of transpose. Matrices
bearing these properties are important enough to have their own names.
DEFINITION 4.3.5. A square matrix
U is called unitary if
U
H
=
U
 1 and
Q is called
orthogonal if
Q is real and
Q
T
=
Q
 1
:
One could allow orthogonal matrices to be complex as well, but these are not particu-
larly useful for us, so in this text we will always assume that orthogonal matrices have
real entries. Since for real matrices
Q, we have
Q
H
=
Q
T , we see from the deï¬nition
that orthogonal matrices are exactly the real unitary matrices.
The naming is traditional in matrix theory, but a bit unfortunate because it sometimes
causes confusion between the terms â€œorthogonal vectorsâ€ and â€œorthogonal matrix.â€ By
orthogonal vectors we mean a set of vectors with a certain relationship to each other,
while an orthogonal matrix is a real matrix whose inverse is its transpose. And to make
matters more confusing, there is a close connection between the two terms, since a
square matrix is orthogonal exactly when its columns form an orthonormal set.
EXAMPLE 4.3.6. Show that the matrix
U
=
1
p
2

1
i
i
1

is unitary and that for any
angle
, the matrix
R
(
)
=

cos

 sin

sin

cos


is orthogonal.

4.3. UNITARY AND ORTHOGONAL MATRICES
205
Î¸
R(  )
R(  )
v
w
v
w
Î¸
Î¸
Î¸
FIGURE 4.3.1. Action of rotation matrix
R
(
).
SOLUTION. It is sufï¬cient to check that
U
H
U
=
I and
R
(
)
T
R
(
)
=
I
: So we
calculate
U
H
U
=

1
p
2

1
i
i
1

H
1
p
2

1
i
i
1

=
1
p
2

1
 i
 i
1

1
p
2

1
i
i
1

=
1
2

1
 i
2
i
 i
 i
+
i
1
 i
2

=

1
0
0
1

which shows that
U is unitary. For the real matrix
R
(
) we have
R
(
)
T
R
(
)
=

cos

 sin

sin

cos


T

cos

 sin

sin

cos


=

cos

sin

 sin

cos



cos

 sin

sin

cos


=

cos
2

+
sin
2

cos

sin

 sin

cos

 cos

sin

+
sin

cos

cos
2

+
sin
2


=

1
0
0
1

;
which shows that
R
(
) is orthogonal.
Orthogonal and unitary matrices have a certain â€œrigidityâ€ quality about them which is
nicely illustrated by the rotation matrix
R
(
) that turns up in calculus as coefï¬cients for
a rotational change of basis. The effect of multiplying a vector
x
2
R
2 by
R
(
) is to
rotate the vector counterclockwise through an angle of
 as illustrated in Figure 4.3.1. In
particular, angles between vectors and lengths of vectors are preserved by such a mul-
tiplication. This is no accident of
R
(
), but rather a property of orthogonal and unitary
matrices in general. Here is a statement of these properties for orthogonal matrices. An
analogous fact holds for complex unitary matrices with vectors in
C
n
:
THEOREM 4.3.7. Let
Q be an orthogonal
n

n matrix and
x;
y
2
R
n with the standard
inner (dot) product. Then
jjQxjj
=
jjxjj
and
Qx

Qy
=
x

y

206
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
PROOF. Let us calculate the norm of
Qx:
jjQxjj
2
=
Qx

Qx
=
(Qx)
T
Qx
=
x
T
Q
T
Qx
=
x
T
x
=
jj
xjj
2
which proves the ï¬rst assertion, while similarly
Qx

Qy
=
(
Qx)
T
Qy
=
x
T
Q
T
Qy
=
x
T
y
=
x

y
There is one more kind of orthogonal matrix that has turned out to be very useful in
numerical calculations and has a very nice geometrical interpretation as well. It gives
us a very simple way of forming orthogonal matrices directly.
DEFINITION 4.3.8. A matrix of the form
H
v
=
I
 2(vv
T
)=(v
T
v
), where
v
2
R
n, is
called a Householder matrix.
EXAMPLE 4.3.9. Let
v
=
(3;
0;
4) and compute the Householder matrix
H
v
: What is
the effect of multiplying it by the vector
v
?
SOLUTION. We calculate
H
v to be
I
 2
v
T
v
vv
T
=
2
4
1
0
0
0
1
0
0
0
1
3
5
 2
3
2
+
4
2
2
4
3
0
4
3
5

3
0
4

=
2
4
1
0
0
0
1
0
0
0
1
3
5
 2
25
2
4
9
0
12
0
0
0
12
0
16
3
5
=
1
25
2
4
7
0
 24
0
25
0
 24
0
 7
3
5
Thus we have that multiplying
H
v by
v gives
H
v
v
=
1
25
2
4
7
0
 24
0
25
0
 24
0
 7
3
5
2
4
3
0
4
3
5
=
1
25
2
4
 75
0
 100
3
5
=
 2
4
3
0
4
3
5
The behavior of this example is no accident. Multiplication by a Householder matrix
can be thought of as a geometrical reï¬‚ection that reï¬‚ects the vector
v to
 v and leaves
any vector orthogonal to
v unchanged. This is implied by the following theorem. For a
picture of this geometrical interpretation, see Figure 4.3.2. Notice that in this ï¬gure
V
is the plane perpendicular to
v and the reï¬‚ections are across this plane.
THEOREM 4.3.10. Let
H
v be the Householder matrix deï¬ned by
v
2
R
n and let
w
2
R
n be written as
w
=
p
+
u, where
p is the projection of
w along
v and
u
=
w
 p:
Then
H
v
w
=
 p
+
u

4.3. UNITARY AND ORTHOGONAL MATRICES
207
v
w
p
u
-p
Hvw
V
FIGURE 4.3.2. Action of
H
v on
w as a reï¬‚ection across the plane
V perpendicular to
v.
PROOF. With notation as in the statement of the theorem, we have
p
=
v
T
w
v
T
v
v and
w
=
p
+
u: Let us calculate
H
v
w
=
(I
 2
v
T
v
vv
T
)(p
+
u)
=
p
+
u
 2
v
T
w
(v
T
v
)
2
vv
T
v
 2
v
T
w
v
T
v
vv
T
u
=
p
+
u
 2
v
T
w
v
T
v
v
 0
=
p
+
u
 2p
=
u
 p
EXAMPLE 4.3.11. Let
v
=
(3;
0;
4) and
H
v the corresponding Householder matrix (as
in Example 4.3.9). The columns of this matrix form an orthonormal basis for the space
R
3
: Find the coordinates of the vector
w
=
(2;
1;
 4) relative to this basis.
SOLUTION. We have already calculated
H
v
=
[u
1
;
u
2
;
u
3
] in Example 4.3.9. The
vector
c
=
(c
1
;
c
2
;
c
3
) of coordinates of
w must satisfy the equations
w
=
c
1
u
1
+
c
2
u
2
+
c
3
u
3
=
H
v
c:
Since
H
v is orthogonal, it follows that
c
=
H
 1
v
w
=
H
T
v
=
1
25
2
4
7
0
 24
0
25
0
 24
0
 7
3
5
2
4
2
1
 4
3
5
=
2
4
4:56
1
 0:8
3
5
which gives us the required coordinates.

208
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
For the most part we work with real Householder matrices. However, occasionally
complex numbers are a necessary part of the scenery. In such situations we can deï¬ne
the complex Householder matrix by the formula
H
v
=
I
 2(vv
H
)=(v
H
v
)
The projection formula (Theorem 4.2.8) remains valid for complex vectors which is
all we need to see that the proof of Theorem 4.3.10 carries over to complex vectors
provided that we replace all transposes by Hermitian transposes.
One might ask if there is any other way to generate orthogonal matrices. In particular, if
we start with a single unit vector, can we embed it as a column in an orthogonal matrix?
The answer is â€œyes,â€ and truth of this answer follows from an even stronger statement,
which is reminiscent of the Steinitz substitution principle.
THEOREM 4.3.12. Every orthogonal set of nonzero vectors in a standard vector space
can be expanded to an orthogonal basis of the space.
PROOF. Suppose that the space in question is
R
n and we have expanded our orig-
inal orthogonal set to
v
1
;
v
2
;
:
:
:
;
v
k
; where
k
<
n: We show how to add one more
element. This is sufï¬cient, because by repeating this step we eventually ï¬ll up
R
n
: Let
A
=
[v
1
;
v
2
;
:
:
:
;
v
k
]
T and let
v
k
+1 be any nonzero solution to
Ax
=
0; which exists
since
k
<
n: This vector is orthogonal to the
v
1
;
v
2
;
:
:
:
;
v
k
:
Weâ€™ll see a more efï¬cient way to perform this calculation when we study the Gram-
Schmidt algorithm in Chapter 6.
EXAMPLE 4.3.13. The vectors
u
1
=
1
3
(1;
2;
2) and
u
2
=
1
p
5
( 2;
1;
0) form an or-
thonormal set. Find an orthogonal matrix with these vectors as the ï¬rst two columns.
SOLUTION. To keep the arithmetic simple, let
v
1
=
(1;
2;
2) and
v
2
=
( 2;
1;
0).
Form the matrix
A with these vectors as rows and solve the system
Ax
=
0 to get a
general solution (the reader should check this)
x
=
( 2
5
x
3
;
 4
5
x
3
;
x
3
): So take
x
3
=
5
and get a particular solution
v
3
=
( 2;
 4;
5): Now normalize all three vectors
v
j to
recover the original
u
1
;
u
2 and the new
u
3
=
1
3
p
5
( 2;
 4;
5): Stack these columns
together and factor out
1
3
p
5 to obtain the orthogonal matrix
P
=
[u
1
;
u
2
;
u
3
]
=
1
3
p
5
2
4
p
5
 6
 2
2
p
5
3
 4
2
p
5
0
5
3
5
which is the matrix we want.
4.3 Exercises
1. Which of the following sets of vectors are linearly independent? Orthogonal? Or-
thonormal?
(a)
(1;
 1;
2);
(2;
2;
0)
(b)
(3;
 1;
1);
(1;
2;
 1);
(2;
 1;
0)
(c)
1
5
(3;
4);
1
5
(4;
 3)
(d)
(1
+
i;
1);
(1;
1
 i)

4.3. UNITARY AND ORTHOGONAL MATRICES
209
2. Let
v
1
=
2
4
1
1
0
3
5
;
v
2
=
2
4
 1
1
1
3
5
;
v
3
=
2
4
1=2
 1=2
1
3
5
: Show this set is an orthogonal
basis of
R
3 and ï¬nd the coordinates of
v
=
2
4
1
2
 2
3
5 with respect to this basis.
3. For what values of the angle
 is the orthogonal matrix
A
=

cos

 sin

sin

cos


symmetric? Skew-symmetric?
4. Determine which of the following matrices are orthogonal or unitary. For such ma-
trices, ï¬nd their inverses.
(a)
1
5

3
4
4
 3

(b)
1
p
2
2
4
1
0
 1
0
1
0
 1
0
1
3
5 (c)
1
p
3

1
+
i
i
i
1
 i

(d)
1
p
2
2
4
1
0
1
0
p
2
i
0
i
0
 i
3
5 (e)
2
4
1
0
0
0
1
 1
0
1
1
3
5 (f)
1
2
2
6
6
4
1
1
 1
1
1
 1
1
1
 1
1
 1
1
 1
 1
1
1
3
7
7
5
5. Let
u
=
(1;
2;
 1) and
w
=
(
p
6;
0;
0): Let
v
=
u
 w and construct the House-
holder matrix
H
v
: Now apply it to the vectors
u and
w
: Conclusions?
6. Find orthogonal or unitary matrices that include the following orthonormal vectors
in their columns
(a)
u
1
=
1
p
6
(1;
2;
 1);
u
2
=
1
p
3
( 1;
1;
1)
(b)
u
1
=
1
5
(3;
 4)
(c)
u
1
=
1
2
(1
+
i;
1
 i)
(d)
u
1
=
1
p
3
(1;
1;
1);
(e)
u
1
=
1
2
(1;
1;
 1;
 1);
u
2
=
1
2
(1;
 1;
1;
 1);
u
3
=
1
p
2
(0;
1;
1;
0)
7. Show that if
P is an orthogonal matrix, then
e
i
P is a unitary matrix for any real

:
8. Let
P
=
1
2
2
4
1
0
 1
0
0
0
 1
0
1
3
5
: Verify that
P is a projection matrix , that is,
P
T
=
P
and
P
2
=
P
; and that if
R
=
I
 2P
; then
R is a reï¬‚ection matrix, that is,
R is a
symmetric orthogonal matrix.
9. Let
P be a real projection matrix and
R
=
I
 2P
: Prove that
R is a reï¬‚ection
matrix. (See Exercise 8 for deï¬nitions.)
10. Let
R
=
2
4
0
0
1
0
 1
0
1
0
0
3
5 and
P
=
1
2
(I
 R
): Verify that
R is a reï¬‚ection matrix
and
P is a projection matrix. (See Exercise 8 for deï¬nitions.)
11. Let
R be a reï¬‚ection matrix. Prove that
P
=
1
2
(I
 R
) is a projection matrix.
12. Prove that every Householder matrix is a reï¬‚ection matrix.

210
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
13. Let
U and
V be orthogonal matrices.
(a) Show that the product
U
V is also orthogonal.
(b) Find examples of orthogonal matrices
U and
V whose sum is not an orthogonal
matrix.
14. Let the quadratic function
f
:
R
n
!
R be deï¬ned by the formula
y
=
f
(x)
=
x
T
Ax; where
A is a real matrix. Suppose that an orthogonalchange of variables is made
from in both domain and range, say
x
=
Qx
0 and
y
=
Qy
0
; where
Q is orthogonal.
Show that in the new coordinates
x
0 and
y
0
;
y
0
=
x
0T
(Q
T
AQ)x
0
4.4. *Computational Notes and Projects
Project: Least Squares
The Big Eight needs your help! Below is a table of scores from the games played thus
far: The
(i;
j
)th entry is team
iâ€™s score in the game with team
j: Your assignment is
two-fold. First, write a notebook that contains instructions for the illiterate on how to
plug in known data and obtain team ratings and predicted point spreads based on the
least squares and graph theory ideas you have seen. Secondly, you are to write a brief
report (one to three pages) on your project which describes the problem, your solution
to it, its limitations and the ideas behind it.
CU
IS
KS
KU
MU
NU
OS
OU
CU
24
21
45
21
14
IS
12
42
21
16
7
KS
12
21
3
27
24
KU
9
14
30
10
14
MU
8
3
52
18
21
NU
51
48
63
26
63
OS
41
45
49
42
28
OU
17
35
70
63
31
Implementation Notes: You will need to set up suitable system of equations, form the
normal equations, and have a computer algebra system solve the problem. For purposes
of illustration, we assume in this project that the tool in use is Mathematica. If not, you
will need to replace these commands with the appropriate ones that your computational
tools provide. The equations in question are formed by letting the variables be a vector
x of â€œpotentialsâ€
x(i); one for each team
i; so that the â€œpotential differencesâ€ best
approximate the actual score differences (i.e., point spreads) of the games. To ï¬nd the

4.4. *COMPUTATIONAL NOTES AND PROJECTS
211
vector
x of potentials you solve the system
Ax
=
b, where
b is the vector of observed
potential differences. N.B: the matrix
A is not the table given above. You will get
one equation for each game played. For example, by checking the
(1;
2)th and
(2;
1)th
entries, we see that CU beat IS by a score of 24 to 12. So the resulting equation for this
game is
x(1)
 x(2)
=
24
 12
=
12: Ideally, the resulting potentials would give you
numbers that would enable you to predict the point spread of an as yet unplayed game:
all you would have to do to determine the spread for team
i versus team
j is calculate
the difference
x(j
)
 x(i): Of course, it doesnâ€™t really work out this way, but this is a
good use of the known data. When you set up this system, you obtain an inconsistent
system. This is where least squares enter the picture. You will need to set up and solve
the normal equations, one way or another. You might notice that the null space of the
resulting coefï¬cient matrix is nontrivial, so this matrix is not full column rank. This
makes sense: potentials are unique up to a constant. To ï¬x this, you could arbitrarily ï¬x
the value of one teamâ€™s potential. E.g., set the weakest teamâ€™s potential value to zero by
adding one additional equation to the system of the form
x(i)
=
0:
Notes to the Instructor: the data above came from the now defunct Big Eight Confer-
ence. This project works better when adapted to your local environment. Pick a sport in
season at your institution or locale. Have students collect the data themselves, make out
a data table as above, and predict the spread for some (as yet) unplayed games of local
interest. It can be very interesting to make it an ongoing project, where for a number
of weeks the students are required to collect last weekâ€™s data and make predictions for
next week based on all data collected to date.
4.4 Exercises
1. It is hypothesized that sale of a certain product is linearly dependent on three factors.
The sales output is quantiï¬ed as
z and the three factors as
x
1
;
x
2 and
x
3
: Six samples
are taken of the sales and the factor data. Results are contained in the following table.
Does the linearity hypothesis seem reasonable? Explain your answer.
z
x
1
x
2
x
3
527
13
5
6
711
6
17
7
1291
12
16
23
625
11
13
4
1301
12
27
14
1350
5
14
31

212
4. GEOMETRICAL ASPECTS OF STANDARD SPACES
Review
Chapter 4 Exercises
1. Find a unit vector orthogonal and a unit vector parallel to
[1;
3;
2]
T in
R
3
:
2. Let
u
=
[1;
2;
 1;
1]
T and
v
=
[ 2;
1;
0;
0]
T and compute
jjujj,
u

v, and the angle
between these vectors.
3. Find the projection of
u
=
(1;
2;
0;
1) along
v
=
(1;
1;
1;
1) and the projection of
v
along
u and express
v as the sum of a vector parallel to
u and a vector orthogonal to
u:
4. Let
u;
v be linearly independent vectors in a standard vector space and let
W
=
spanfu;
v
g: Show that
u;
v
 ((u

v
)=(v

v
))u is an orthogonal basis of
W
:
5. Let
W
=
spanf(1;
2;
1);
(2;
 1;
0)g
(a) Show this spanning set is an orthogonal set.
(b) The vector
v
=
( 4;
7;
2) belongs to
W
: Calculate its coordinates with respect to
this basis.
6. Determine if
W
=
fv
2R
3
j
k
v
k
=
1g is a subspace of
R
3
:
7. Find an orthogonal matrix which has as its ï¬rst column the vector
1
3
(1;
0;
2;
 2):
8. Show that if
A is a real symmetric
n

n matrix and
u;
v are vectors in
R
n
; then
(Au)

v
=
uAv

CHAPTER 5
THE EIGENVALUE PROBLEM
The ï¬rst major problem of linear algebra is to understand how to solve the basis linear
system
Ax
=
b and what the solution means. We have explored this system from three
points of view: in Chapter 1 we approached the problem from an operational point
of view and learned the mechanics of computing solutions. In Chapter 2, we took a
more sophisticated look at the system from the perspective of matrix theory. Finally, in
Chapter 3, we viewed the problem from the vantage of vector space theory.
Now it time for us to begin study of the second major problem of linear algebra, namely
the eigenvalue problem. It was necessary for us to tackle the linear systems problem ï¬rst
because the eigenvalue problem is more sophisticated and will require most of the tools
that we have thus far developed. This subject has many useful applications; indeed, it
arose out of these applications. One of the more interesting applications of eigenvalue
theory that we study in this chapter is the analysis of discrete dynamical systems. Such
systems include the Markov chains we have seen in earlier chapters as a special case.
5.1. Deï¬nitions and Basic Properties
What are They?
Good question. Letâ€™s get right to the point.
DEFINITION 5.1.1. Let
A be a square
n

n matrix. An eigenvector of
A is a nonzero
vector
x in
R
n (or
C
n, if we are working over complex numbers), such that for some
scalar
 , we have
Eigenvalues
and
Eigenvectors
Ax
=
x
The scalar
 is called an eigenvalue of the matrix
A, and we say that the vector
x is an
eigenvector belonging to the eigenvalue
. The pair
(;
x) is called an eigenpair for the
matrix
A:
The only kinds of matrices for which these objects are deï¬ned are square matrices, so
weâ€™ll assume throughout this Chapter that we are dealing with such matrices.
Caution: Be aware that the eigenvalue
 is allowed to be the
0 scalar, but eigenvectors
x are, by deï¬nition, never the
0 vector.
213

214
5. THE EIGENVALUE PROBLEM
As a matter of fact, it is quite informative to have an eigenvalue
0: This says that the
system
Ax
=
0x
=
0 has a nontrivial solution
x, in other words,
A is not invertible by
Theorem 2.5.9.
Here are a few simple examples of eigenvalues and vectors. Let
A
=

7
4
3
6

;
x
=
( 1;
1) and
y
=
(4;
3): One checks that
Ax
=
( 3;
3)
=
3x and
Ay
=
(40;
30)
=
10y
: It follows that
x and
y are eigenvectors corresponding to eigenvalues
3 and
10;
respectively.
Why should we have any interest in these quantities? A general answer goes something
like this: knowledge of eigenvectors and eigenvalues gives us deep insights into the
structure of the matrix
A: Here is just one example: suppose that we would like to have
a better understanding of the effect of multiplication of a vector
x by powers of the
matrix
A, that is, of
A
k
x: Letâ€™s start with the ï¬rst power,
Ax: If we knew that
x were
an eigenvector of
A, then we would have that for some scalar
,
Ax
=
x
A
2
x
=
A(Ax)
=
Ax
=
Ax
=

2
x
...
A
k
x
=
A(A
k
 1
x)
=



=

k
x
This is very nice, because it reduces something complicated, namely matrix-vector mul-
tiplication, to something simple, namely scalar-vector multiplication.
There are other reasons for the usefulness of the eigenvector/value concept which we
will develop later, but here is one that is fairly immediate: is there any signiï¬cance in
knowing that one of the eigenvalues of
A is
0? Check the deï¬nition of eigenvalue and
we see that this means that
Ax
=
0 for some nonzero vector
x: By Theorem 2.5.9 of
Chapter 2 (page 91) it follows that
A is not invertible. So eigenvalues can tell us about
invertibility.
We need some handles on these quantities. Letâ€™s ask how we could ï¬gure out what
they are for speciï¬c matrices. Here are some of the basic points about eigenvalues and
eigenvectors.
THEOREM 5.1.2. Let
A be a square
n

n matrix. Then
1. The eigenvalues of
A consist of all scalars
 that are solutions to the
nth degree
polynomial equation
det(I
 A)
=
0
2. For a given eigenvalue
, the eigenvectors of the matrix
A belonging to that
eigenvalue consist of all nonzero elements of
N
(I
 A):
PROOF. Note that
x
=
I
x: Thus we have the following chain of thought:
A has
eigenvalue
 if and only if
Ax
=
x, for some nonzero vector
x, which is true if and
only if
0
=
x
 Ax
=
I
x
 Ax
=
(I
 A)x
for some nonzero vector
x: This last statement is equivalent to the assertion that
0
6=
x
2
N
(I
 A): The matrix
I
 A is square, so it has a nontrivial null space precisely

5.1. DEFINITIONS AND BASIC PROPERTIES
215
when it is singular (recall the characterizations of nonsingular matrices in Theorem 2.5.9
of Chapter 2). This occurs only when
det(I
 A)
=
0: If we expand this determinant
down the ï¬rst column, we see that the highest order term involving
 that occurs is the
product of the diagonal terms
(
 a
ii
), so that the degree of the expression
det(I
 A)
as a polynomial in
 is
n: This proves (1).
We saw from this chain of thought that if
 is an eigenvalue of
A, then the eigenvectors
belonging to that eigenvalue are precisely the nonzero vectors
x such that
(I
 A)x
=
0, that is, the nonzero elements of
N
(A), which is what (2) asserts.
Here is some terminology that we will use throughout this chapter.
NOTATION 5.1.3. We call a polynomial monic if the leading coefï¬cient is
1:
For instance,

2
+
2
+
3 is a monic polynomial in
 while
2
2
+

+
1 is not.
DEFINITION 5.1.4. Given a square
n

n matrix
A, the equation
det(I
 A)
=
0 is
called the characteristic equation of
A and the
nth degree monic polynomial
p()
=
det
(I
 A) is called the characteristic polynomial of
A:
Suppose we already know the eigenvalues of
A and want to ï¬nd the eigenvalues of
something like
3A
+
4I
: Do we have to start over to ï¬nd them? The next calculation is
really a useful tool for answering such questions.
THEOREM 5.1.5. If
B
=
cA
+
dI for scalars
d and
c
6=
0, then the eigenvalues of
B
are of the form

=
c
+
d, where
 runs over the eigenvalues of
A; and the eigenvectors
of
A and
B are identical.
PROOF. Let
x be an eigenvector of
A corresponding to the eigenvalue
: Then by
deï¬nition
x
6=0 and
Ax
=
x
Also, we have that
dI
x
=
dx
Now multiply the ï¬rst equation by the scalar
c and add these two equations to obtain
(cA
+
dI
)x
=
B
x
=
(c
+
d)x
It follows that every eigenvector of
A belonging to
 is also an eigenvector of
B be-
longing to the eigenvalue
c
+
d: Conversely, if
y is an eigenvalue of
B belonging to
,
then
B
y
=
y
=
(cA
+
dI
)y
Now solve for
Ay to obtain that
Ay
=
1
c
(
 d)y
so that

=
(
 d)=c is an eigenvalue of
A with corresponding eigenvector
y
: It
follows that
A and
B have the same eigenvectors and their eigenvalues are related by
the formula

=
c
+
d:

216
5. THE EIGENVALUE PROBLEM
EXAMPLE 5.1.6. Let
A
=

7
4
3
6

;
x
=
( 1;
1) and
y
=
(4;
3); so that
Ax
=
( 3;
3)
=
3x and
Ay
=
(40;
30)
=
10y
: Find the eigenvalues and corresponding
eigenvectors for the matrix
B
=
3A
+
4I
:
SOLUTION. From the calculations given to us, we observe that
x and
y are eigenvec-
tors corresponding to the eigenvalues
3 and
10, respectively, for
A: These are all the
eigenvalues of
A, since the characteristic polynomial of
A is of degree
2; so has only
two roots. According to Theorem 5.1.5, the eigenvalues of
3A
+
4I must be

1
=
3

3
+
4
=
13 with corresponding eigenvector
x
=
( 1;
1), and

2
=
3

10
+
4
=
34
with corresponding eigenvalue
y
=
(4;
3):
DEFINITION 5.1.7. Given an eigenvalue
 of the matrix
A, the eigenspace correspond-
ing to
 is the subspace
N
(I
 A) of
R
n (or
C
n).
Notation: We write
E

(A)
=
N
(I
 A):
DEFINITION 5.1.8. By an eigensystem of the matrix
A, we mean a list of all the eigen-
values of
A and, for each eigenvalue
, a complete description of the eigenspace corre-
sponding to
:
The usual way to give a complete description of an eigenspace is to list a basis of the
space. Remember that there is one element of the eigenspace
N
(I
 A) that is not an
eigenvector, namely
0: In any case, the computational route is now clear. To call it an
algorithm is really an abuse of language, since we donâ€™t have a complete computational
description of the root ï¬nding phase, but here it is:
Eigensystem Algorithm. Let
A be an
n

n matrix. To ï¬nd an eigensystem
of
A:
1. Find the scalars that are roots to the characteristic equation
det(I
 A)
=
0:
2. For each scalar
 in (1), use the null space algorithm to ï¬nd a basis of
the eigenspace
N
(I
 A):
As a matter of convenience, it is sometimes a little easier to work with
A
 I when
calculating eigenspaces (because there are fewer extra minus signs to worry about).
This is perfectly OK, since
N
(A
 I
)
=
N
(I
 A): It doesnâ€™t affect the eigenvalues
either, since
det(I
 A)
=

det(A
 I
): Here is our ï¬rst eigensystem calculation.
EXAMPLE 5.1.9. Find an eigensystem for the matrix
A
=

7
4
3
6

:
SOLUTION. First solve the characteristic equation
0
=
det(I
 A)
=
det


 7
 4
 3

 6

=
(
 7)(
 6)
 ( 3)( 4)
=

2
 13
+
42
 12
=

2
 13
+
30
=
(
 3)(
 10)

5.1. DEFINITIONS AND BASIC PROPERTIES
217
Hence the eigenvalues are

=
3;
10: Next, for each eigenvector calculate the corre-
sponding eigenspace.

=
3: Then
A
 3I
=

7
 3
4
3
6
 3

=

4
4
3
3

and row reduction gives

4
4
3
3

         !
E
21
( 3=4)
E
1
(1=4)

1
1
0
0

so the general solution is

x
1
x
2

=

 x
2
x
2

=
x
2

 1
1

Therefore a basis of
E
3
(A) is
f( 1;
1)g:

=
10: Then
A
 10I
=

7
 10
4
3
6
 10

=

 3
4
3
 4

and row reduction
gives

 3
4
3
 4

        !
E
21
(1)
E
1
( 1=3)

1
 4=3
0
0

so the general solution is

x
1
x
2

=

(4=3)x
2
x
2

=
x
2

4=3
1

Therefore a basis of
E
10
(A) is
f(4=3;
1)g:
Concerning this example, there are several interesting points worth noting:
1. Since the
2

2 matrix
A
 I is singular for eigenvalue
, one row should
always be a multiple of the other. Knowing this, we didnâ€™t have to do even the
little row reduction we did above. However, its a good idea to check this; it helps
you avoid mistakes. Remember: any time that row reduction of
A
 I leads to
full rank (only trivial solutions) you have either made an arithmetic error or you
do not have an eigenvalue.
2. This matrix is familiar. In fact,
B
=
(0:1)A is the Markov chain transition
matrix from Example 2.3.4 of Chapter 2. Therefore the eigenvalues of
B are
0:3 and
1, by Example 5.1.9 with
c
=
0:1 and
d
=
0: The eigenvector belonging
to

=
1 is just a solution to the equation
B
x
=
x, which was discussed in
Example 3.4.8 of Chapter 2.
3. The vector

4=7
3=7

=
3
7

4=3
1

is also an eigenvector of
A (or
B) belonging to

=
1 since it too belongs to
E
2
(A):
EXAMPLE 5.1.10. How do we ï¬nd eigenvalues of a triangular matrix? Illustrate the
method with
A
=
2
4
2
1
1
0
1
1
0
0
 1
3
5
:

218
5. THE EIGENVALUE PROBLEM
SOLUTION. Eigenvalues are just the roots of the characteristic equation
det
(I
 A)
=
0: Notice that
 A is triangular if
A is. Also, the only entries in
I
 A that are any
different from the entries of
 A are the diagonal entries, which change from
 a
ii to

 a
ii
: Therefore,
I
 A is triangular if
A is. We already know that the determinant
of a triangular matrix is easy to compute: just form the product of the diagonal entries.
Therefore, the roots of the characteristic equation are the solutions to
0
=
det(I
 A)
=
(
 a
11
)(
 a
22
)



(
 a
nn
)
that is,

=
a
11
;
a
22
;
:
:
:
;
a
nn
: In other words, for a triangular matrix the eigenvalues
are simply the diagonal elements! In particular, for the example
A given above, we see
with no calculations that the eigenvalues are

=
2;
1;
 1:
Notice, by the way, that we donâ€™t quite get off the hook in the preceding example if we
are required to ï¬nd the eigenvectors. It will still be some work to compute each of the
relevant null spaces, but much less than it would take for a general matrix.
Example 5.1.10 can be used to illustrate another very important point. The reduced
row echelon form of the matrix of that example is clearly the identity matrix
I
3
: This
matrix has eigenvalues
1;
1;
1, which are not the same as the eigenvalues of
A (would
that eigenvalue calculations were so easy!). In fact, a single elementary row operation
on a matrix can change the eigenvalues. For example, simply multiply the ï¬rst row of
A above by
1
2
: This point warrants a warning, since it is the source of a fairly common
mistake.
Caution: The eigenvalues of a matrix
A and the matrix
E
A, where
E is an elementary
matrix, need not be the same.
EXAMPLE 5.1.11. Find an eigensystem for the matrix
A
=

1
 1
1
1

:
SOLUTION. For eigenvalues, compute the roots of the equation
0
=
det(A
 I
)
=
det

1
 
 1
1
1
 

=
(1
 )
2
 ( 1)
=

2
 2
+
2:
Now we have a little problem. Do we allow complex numbers? If not, we are stuck
because the roots of this equation are

=
 ( 2)

p
( 2)
2
 4

2
2
=
1

i
In other words, if we did not enlarge our ï¬eld of scalars to the complex numbers, we
would have to conclude that there are no eigenvalues or eigenvectors! Somehow, this
doesnâ€™t seem like a good idea. It is throwing information away. Perhaps it comes as
no surprise that complex numbers would eventually ï¬gure into the eigenvalue story.
After all, ï¬nding eigenvalues is all about solving polynomial equations, and complex
numbers were invented to overcome the inability of real numbers to provide solutions
to all polynomial equations. Letâ€™s allow complex numbers as the scalars. Now our
eigenspace calculations are really going on in the complex space
C
2 instead of
R
2
:

5.1. DEFINITIONS AND BASIC PROPERTIES
219

=
1
+
i: Then
A
 (1
+
i)I
=

1
 (1
+
i)
 1
1
1
 (1
+
i)

=

 i
 1
1
 i

and
row reduction gives (recall that
1=i
=
 i)

 i
 1
1
 i

         !
E
21
( i)
E
1
(1=( i))

1
 i
0
0

so the general solution is

z
1
z
2

=

iz
2
z
2

=
z
2

i
1

Therefore a basis of
E
1+i
(A) is
f(i;
1)g:

=
1
 i: Then
A
 (1
 i)I
=

1
 (1
 i)
 1
1
1
 (1
 i)

=

i
 1
1
i

and row
reduction gives (remember that
1=i
=
 i)

i
 1
1
i

      !
E
21
(i)
E
1
(1=i)

1
i
0
0

so the general solution is

z
1
z
2

=

 iz
2
z
2

=
z
2

 i
1

Therefore a basis of
E
1+i
(A) is
f( i;
1)g:
In view of the previous example, we are going to adopt the following practice: unless
otherwise stated, if the eigenvalue calculation leads us to complex numbers, we take the
point of view that the ï¬eld of scalars should be enlarged to include the complex numbers
and the eigenvalues in question.
Multiplicity of Eigenvalues
The following example presents yet another curiosity about eigenvalues and vectors.
EXAMPLE 5.1.12. Find an eigensystem for the matrix
A
=

2
1
0
2

:
SOLUTION. Here the eigenvalues are easy. This matrix is triangular, so they are

=
2;
2: Now for eigenvectors.

=
2: Then
A
 2I
=

2
 2
1
0
2
 2

=

0
1
0
0

and row reduction is not
necessary here. Notice that the variable
x
1 is free here while
x
2 is bound. The general
solution is

x
1
x
2

=

x
1
0

=
x
1

1
0

Therefore a basis of
E
2
(A) is
f(1;
0)g:

220
5. THE EIGENVALUE PROBLEM
The manner in which we list the eigenvalues in this example is intentional. The number
2 occurs twice on the diagonal, suggesting that it should be counted twice. As a matter
of fact,

=
2 is a root of the characteristic equation
(
 2)
2
=
0 of multiplicity
2: Yet there is a curious mismatch here. In all of our examples to this point, we have
been able to come up with as many eigenvectors as eigenvalues, namely the size of the
matrix if we allow complex numbers. In this case there is a deï¬ciency in the number
of eigenvectors, since there is only one eigenspace and it is one dimensional. Is this a
failing entirely due to the occurrence of multiple eigenvalues? The answer is no. The
situation is a bit more complicated, as the following example shows.
EXAMPLE 5.1.13. Discuss the eigenspace corresponding to the eigenvalue

=
2 for
these two matrices for these two matrices
(a)
2
4
2
1
2
0
1
 2
0
0
2
3
5
(b)
2
4
2
1
1
0
1
1
0
0
2
3
5
SOLUTION. Notice that each of these matrices has eigenvalues

=
1;
2;
2: Now for the
eigenspace
E
2
(A):
(a) For this eigenspace calculation we have
A
 2I
=
2
4
2
 2
1
2
0
1
 2
 2
0
0
2
 2
3
5
=
2
4
0
1
2
0
 1
 2
0
0
0
3
5
and row reduction gives
2
4
0
1
2
0
 1
 2
0
0
0
3
5
     !
E
21
(1)
2
4
0
1
2
0
0
0
0
0
0
3
5
so that free variables are
x
1
;
x
3 and the general solution is
2
4
x
1
x
2
x
3
3
5
=
2
4
x
1
 2x
3
x
3
3
5
=
x
1
2
4
1
0
0
3
5
+
x
3
2
4
0
 2
1
3
5
Thus a basis for
E
2
(A) is
f(1;
0;
0);
(0;
 2;
1)g: Notice that in this case we get as many
independent eigenvectors as the eigenvalue

=
2 occurs.
(b) For this eigenspace calculation we have
A
 2I
=
2
4
2
 2
1
1
0
1
 2
1
0
0
2
 2
3
5
=
2
4
0
1
1
0
 1
1
0
0
0
3
5
and row reduction gives
2
4
0
1
2
0
 1
1
0
0
0
3
5
     !
E
21
(1)
2
4
0
1
2
0
0
 1
0
0
0
3
5
      !
E
12
(2)
E
2
( 1)
2
4
0
1
0
0
0
1
0
0
0
3
5
so that the only free variable is
x
1 and the general solution is
2
4
x
1
x
2
x
3
3
5
=
2
4
x
1
0
0
3
5
=
x
1
2
4
1
0
0
3
5

5.1. DEFINITIONS AND BASIC PROPERTIES
221
Thus a basis for
E
2
(A) is
f(1;
0;
0)g: Notice that in this case we donâ€™t get as many
independent eigenvectors as the eigenvalue

=
2 occurs.
This example shows that there are two kinds of â€œmultiplicitiesâ€ of an eigenvector. On
the one hand there is the number of times that the eigenvalue occurs as a root of the
characteristic equation. On the other hand there is the dimension of the corresponding
eigenspace. One of these is algebraic in nature, the other is geometric. Here are the
appropriate deï¬nitions.
DEFINITION 5.1.14. Let
 be a root of the characteristic equation
det(I
 A)
=
0:
Algebraic and
Geometric
Multiplicity
The algebraic multiplicity of
 is the multiplicity of
 as a root of the characteris-
tic equation. The geometric multiplicity of
 is the dimension of the space
E

(A)
=
N
(I
 A):
We categorize eigenvalues as simple or repeated, according to the following deï¬nition.
DEFINITION 5.1.15. The eigenvalue
 of
A is said to be simple if its algebraic multi-
plicity is
1; that is, the number of times it occurs as a root of the characteristic equation
is
1: Otherwise, the eigenvalue is said to be repeated.
In Example 5.1.13 we saw that the repeated eigenvalue

=
2 has algebraic multiplicity
2 in both (a) and (b), but geometric multiplicity
2 in (a) and
1 in (b). What can be said
in general? The following theorem summarizes the facts. In particular, part 2 says that
algebraic multiplicity is always greater than or equal to geometric multiplicity. Part 1
is immediate since a polynomial of degree
n has
n roots, counting complex roots and
multiplicities. We defer the proof of part 2 to the next section.
THEOREM 5.1.16. Let
A be an
n

n matrix with characteristic polynomial
p()
=
det
(I
 A): Then:
1. The number of eigenvalues of
A, counting algebraic multiplicities and complex
numbers, is
n:
2. For each eigenvalue
 of
A, if
m() is the algebraic multiplicity of
, then
1

dim
E

(A)

m()
Now when we wrote that each of the matrices of Example 5.1.13 has eigenvalues

=
1;
2;
2, what we intended to indicate was a complete listing of the eigenvalues of the
matrix, counting algebraic multiplicities. In particular,

=
1 is a simple eigenvalue of
the matrices, while

=
2 is not. The geometric multiplicities of (a) are identical to the
algebraic in (a) but not in (b). The latter kind of matrix is harder to deal with than the
former. Following a time honored custom of mathematicians, we call the more difï¬cult
matrix by a less than ï¬‚attering name, namely, â€œdefective.â€
DEFINITION 5.1.17. A defective matrix is one for which the sum of the geometric mul-
tiplicities is strictly less than the sum of the algebraic multiplicities.
Notice that the sum of the algebraic multiplicities of an
n

n matrix is the size
n of
the matrix. This is due to the fact that the characteristic polynomial of the matrix has
degree
n; hence exactly
n roots, counting multiplicities.

222
5. THE EIGENVALUE PROBLEM
5.1 Exercises
1. Find eigenvalues for these matrices:
(a)

2
 12
1
 5

(b)
2
4
2
0
0
0
3
1
0
6
2
3
5 (c)
2
4
2
1
1
0
3
1
0
0
2
3
5 (d)
2
4
2
0
1
0
0
0
1
0
2
3
5
(e)
2
4
 1
0
0
1
 1
0
0
1
 1
3
5 (f)

0
 2
2
0

(g)

0
2
2
1

(h)

1
+
i
3
0
i

2. Find eigensystems for the matrices of Exercise 1 and specify the algebraic and geo-
metric multiplicity of each eigenvalue.
3. You are given that the matrix

0
1
1
0

has eigenvalues
1;
 1 and respective eigen-
vectors
(1;
1);
(1;
 1): Use Theorem 5.1.5 to determine an eigensystem for

3
 5
 5
3

without further eigensystem calculations.
4. The trace of a matrix
A is the sum of all the diagonal entries of the matrix and
denoted
tr
A: Find the trace of each matrix in Exercise 1 and verify that it is the sum of
the eigenvalues of the matrix.
5. Let

a
b
c
d

be a general
2

2 matrix.
(a) Compute the characteristic polynomial of
A and ï¬nd its roots, i.e., the eigenvalues
of
A:
(b) Show that the sum of the eigenvalues is the trace of
A:
(c) Show that the product of the eigenvalues is the determinant of
A:
6. Let
A
=

1
1
0
2

and show that
A and
A
T have different eigenvectors but the
same eigenvalues.
7. Show that the matrix
A
=

1
i
i
2

is symmetric and that its eigenvalues are com-
plex.
8. Show that for any square matrix
A; the matrices
A and
A
T have the same eigenval-
ues.
9. Show that if
x is an eigenvector for the matrix
A belonging to the eigenvalue
, then
so is
cx for any scalar
c
6=
0:
10. Let
A be a matrix whose eigenvalues are all less than
1 in absolute value. Show
that every eigenvalue of
I
 A is nonzero and deduce that
I
 A is invertible.
11. Prove that if
A is invertible and
 is an eigenvalue of
A; then
1= is an eigenvalue
of
A
 1
:

5.2. SIMILARITY AND DIAGONALIZATION
223
12. Let
A be any square real matrix and show that the eigenvalues of
A
T
A are all
nonnegative.
13. Show that if
 is an eigenvalue of an orthogonal matrix
P
; then
jj
=
1:
14. Let
T
k be the
k

k tridiagonal matrix whose diagonal entries are
2 and off-diagonal
nonzero entries are
 1: Use a MAS or CAS (MAS would probably be better) to build
an array
y of length
30 whose
kth entry is the minimum of the absolute value of the
eigenvalues of
T
k
+1
: Plot this array. Use the graph as a guide and try to approximate
y
(k
) as a simple function of
k
:
15. Show that if
B is a real symmetric positive deï¬nite matrix, then the eigenvalues of
B are positive.
16. Let
A be a real matrix and
(;
x) an eigenpair for
A.
(a) Show that
(
;
x) is also an eigenpair for
A:
(b) Given that
A
=
2
4
2
 2
0
1
0
1
0
0
2
3
5 and that
(2;
( 1;
0;
1)) and
(1
+
i;
(1
+
i;
1;
0)) are
eigenpairs, with no further calculations exhibit an eigensystem for the matrix
A and a
matrix
P for which
P
 1
AP is diagonal.
(c) Deduce from part (a) that the real quadratic

2
 2<()
+
jj
2 is a factor of the
characteristic polynomial of
A:
17. Let
A and
B be matrices of the same size with eigenvalues
 and

; respectively.
Show by example that it is false to conclude that

+
 is an eigenvalue of
A
+
B or
that
 is an eigenvalue of
AB
:
18. Show that
A and
A
T have the same eigenvalues.
19. Show that if
A and
B are the same size, then
AB and
B
A have the same eigenval-
ues. Hint: Deal with the
0 eigenvalue separately. If
 is an eigenvalue of
AB
;multiply
the equation
AB
x
=
x on the left by
B
:
5.2. Similarity and Diagonalization
Diagonalization and Matrix Powers
Eigenvalues: why are they? This is a good question and the justiï¬cation for their exis-
tence and study could go on and on. We will try to indicate their importance by focusing
on one special class of problems, namely, discrete linear dynamical systems. Here is
the deï¬nition of such a system.

224
5. THE EIGENVALUE PROBLEM
DEFINITION 5.2.1. A discrete linear dynamical system is a sequence of vectors
x
(k
)
;
k
=
0;
1;
:
:
:, called states, which is deï¬ned by an initial vector
x
(0) and by the rule
x
(k
+1)
=
Ax
(k
)
;
k
=
0;
1;
:
:
:
where
A is a given ï¬xed square matrix, called the transition matrix of the system.
We have seen examples of this kind of system before, namely in Markov chains and
Discrete
Dynamical
System
difference equations. Evidently, the entire sequence of state vectors is determined by
the matrix
A and the initial state
x
(0)
: Here is the sort of question that we would like
to answer: when is it the case that there is a limiting vector
x for this sequence of
vectors and, if so, how does one compute this vector? The answers to these questions
will explain the behavior of the Markov chain that was introduced in Example 2.3.4 of
Chapter 2.
If there is such a limiting vector
x for a Markov chain, we saw in Example 3.4.8 of
Chapter 3 how to proceed: ï¬nd the null space of the matrix
I
 A, that is, the set of
all solutions to the system
(I
 A)x
=0. However, the question of whether or not all
initial states
x
(0) lead to this limiting vector is a more subtle issue which requires the
insights of the next section. Weâ€™ve already done some work on this problem. We saw
in Section 2.3 that the entire sequence of vectors is uniquely determined by the initial
vector and the transition matrix
A in the explicit formula
x
(k
)
=
A
k
x
(0)
:
Before proceeding further, letâ€™s consider another example that will indicate why we
would be interested in limiting vectors.
EXAMPLE 5.2.2. By some unfortunate accident a new species of frog has been intro-
duced into an area where it has too few natural predators. In an attempt to restore
the ecological balance, a team of scientists is considering introducing a species of bird
which feeds on this frog. Experimental data suggests that the population of frogs and
birds from one year to the next can be modeled by linear relationships. Speciï¬cally, it
has been found that if the quantities
F
k and
B
k represent the populations of the frogs
and birds in the
kth year, then
B
k
+1
=
0:6B
k
+
0:4F
k
F
k
+1
=
 r
B
k
+
1:4F
k
Here the positive number
r is a kill rate which measures the consumption of frogs by
birds. It varies with the environment, depending on factors such as the availability of
other food for the birds, etc. Experimental data suggests in the environment where the
birds are to be introduced,
r
=
0:35: The question is this: in the long run, will the
introduction of the birds reduce or eliminate the frog population growth?
SOLUTION. The discrete dynamical system concept introduced in the preceding dis-
cussion ï¬ts this situation very nicely. Let the population vector in the
kth year be
x
(k
)
=
(B
k
;
F
k
): Then the linear relationship above becomes

B
k
+1
F
k
+1

=

0:6
0:4
 0:35
1:4


B
k
F
k

which is a discrete linear dynamical system. Notice that this is different from the
Markov chains we studied earlier, since one of the entries of the coefï¬cient matrix

5.2. SIMILARITY AND DIAGONALIZATION
225
is negative. Before we can ï¬nish solving this example we need to have a better under-
standing of discrete dynamical systems and the relevance of eigenvalues.
Letâ€™s try to understand how state vectors change in the general discrete dynamical sys-
tem. We have
x
(k
)
=
A
k
x
(0)
: So, to understand how a dynamical system works, what
we really need to know is how the powers of the transition matrix A behave. But in
general, this is very hard!
Here is an easy case we can handle: what if
A
=
[a
ij
] is diagonal? Since weâ€™ll make
extensive use of diagonal matrices, letâ€™s use the following notation.
Notation: The matrix
diagf
1
;

2
;
:
:
:
;

n
g is the
n

n diagonal matrix with entries

1
;

2
;
:
:
:
;

n down the diagonal.
For example,
diagf
1
;

2
;

3
g
=
2
4

1
0
0
0

2
0
0
0

3
3
5
By matching up the
ith row and
jth column of
A we see that the only time we could
have a nonzero entry in
A
2 is when
i
=
j, and in that case the entry is
a
2
ii
: A similar
argument applies to any power of
A: In summary, we have this handy
THEOREM 5.2.3. If
D
=
diag
f
1
;

2
;
:
:
:
;

n
g, then
D
k
=
diagf
k
1
;

k
2
;
:
:
:
;

k
n
g;
for all positive integers
k
:
Just as an aside, this theorem has a very interesting consequence. We have seen in some
exercises that if
f
(x)
=
a
0
+
a
1
x
+
:
:
:
+
a
n
x
n is a polynomial, we can evaluate
f
(x)
at the square matrix
A as long as we understand that the constant term
a
0 is evaluated
as
a
0
I
: This notion of
f
(A) has some important applications. In the case of a diagonal
A; the following fact reduces evaluation of
f
(A) to a sequence of scalar calculations.
COROLLARY 5.2.4. If
D
=
diagf
1
;

2
;
:
:
:
;

n
gand
f
(x) is a polynomial, then
f
(D
)
=
diag
ff
(
1
);
f
(
2
);
:
:
:
;
f
(
n
)g
PROOF. Simply observe that
f
(D
)
=
a
0
I
+
a
1
D
+
:
:
:
+
a
n
D
n
; apply the preceding
theorem to each monomial and add diagonal terms up.
Now for the powers of a more general
A: For ease of notation, letâ€™s consider a
3

3
matrix
A: What if we could ï¬nd three linearly independent eigenvectors
v
1
;
v
2
;
v
3? We
would have
Av
1
=

1
v
1
;
Av
2
=

2
v
2
;
Av
3
=

3
v
3
or
A[v
1
;
v
2
;
v
3
]
=
[v
1
;
v
2
;
v
3
]
2
4

1
0
0
0

2
0
0
0

3
3
5
=
[v
1
;
v
2
;
v
3
]
diagf
1
;

2
;

3
g:
Now set
P
=
[v
1
;
v
2
;
v
3
]

226
5. THE EIGENVALUE PROBLEM
and
D
=
diagf
1
;

2
;

3
g
Then
P is invertible since the columns of
P are linearly independent. (Remember that
any nonzero solution to
Ax
=
0 would give rise to a nontrivial linear combination of
the column of
A that sums to
0:) Moreover, the equation
AP
=
P
D
; if multiplied on
the left by
P
 1, gives the equation
P
 1
AP
=
D
This is a beautiful equation, because it makes the powers of
A simple to understand.
The procedure we just went through is reversible as well. In other words, if
P is a given
invertible matrix such that
P
 1
AP
=
D, then we can obtain that
AP
=
P
D, identify
the columns of
P by the equation
P
=
[v
1
;
v
2
;
v
3
] and conclude that the columns of
P are linearly independent eigenvectors of
A: We make the following deï¬nition and
follow it with a simple but key theorem relating similar matrices.
DEFINITION 5.2.5.
A matrix
A is said to be similar to a matrix
B if there exists an
Similar Matrices
invertible matrix
P such that
P
 1
AP
=
B
A simple size check shows that similar matrices have to be square and of the same size.
Furthermore, if
A is similar to
B, then
B is similar to
A: To see this, suppose that
P
 1
AP
=
B and multiply by
P on the left and
P
 1 on the right to obtain that
A
=
P
P
 1
AP
P
 1
=
P
B
P
 1
=
(P
 1
)
 1
B
P
 1
Similar matrices have much in common, as the following theorem shows.
THEOREM 5.2.6. Suppose that
A is similar to
B, say
P
 1
AP
=
B
: Then:
1. For every positive integer
k,
B
k
=
P
 1
A
k
P
2. The matrices
A and
B have the same characteristic polynomial, hence the same
eigenvalues.
PROOF. We see that successive terms
P
 1
P cancel out in the
k-fold product
B
k
=
(P
 1
AP
)(P
 1
AP
)



(P
 1
AP
)
to give that
B
k
=
P
 1
A
k
P
This proves (1). For (2), remember that the determinant distributes over products, so
that we can pull this clever little trick:
det(I
 B
)
=
det
(P
 1
I
P
 P
 1
AP
)
=
det
(P
 1
(I
 A)P
)
=
det
(P
 1
)
det(I
 A)
det
(P
)
=
det
(I
 A)
det(P
 1
P
)
=
det
(I
 A):
This proves (2).

5.2. SIMILARITY AND DIAGONALIZATION
227
Now we can see the signiï¬cance of the equation
P
 1
AP
=
D, where
D is diagonal. It
follows from this equation that for any positive integer
k, we have
P
 1
A
k
P
=
D
k, so
multiplying on the left by
P and on the right by
P
 1 yields
A
k
=
P
D
k
P
 1
(5.2.1)
As we have seen, the term
P
D
k
P
 1 is easily computed.
EXAMPLE 5.2.7. Apply the results of the preceding discussion to the matrix in part (a)
of Example 5.1.13.
SOLUTION. The eigenvalues of this problem are

=
1;
2;
2: We already found the
eigenspace for

=
2: Denote the two basis vectors by
v
1
=
(1;
0;
0) and
v
2
=
(0;
 2;
1): For

=
1, apply Gauss-Jordan elimination to the matrix
A
 1I
=
2
4
2
 1
1
2
0
1
 1
 2
0
0
2
 1
3
5
=
2
4
1
1
2
0
0
 2
0
0
1
3
5
and we can obviously reduce this matrix by using the pivot in the third row to
2
4
1
1
0
0
0
1
0
0
0
3
5
which gives a general eigenvector of the form
2
4
x
1
x
2
x
3
3
5
=
2
4
 x
2
x
2
0
3
5
=
x
2
2
4
 1
1
0
3
5
Hence the eigenspace
E
1
(A) has basis
f( 1;
1;
0)g: Now set
v
3
=
( 1;
1;
0): Form
the matrix
P
=
[v
1
;
v
2
;
v
3
]
=
2
4
1
0
 1
0
 2
1
0
1
0
3
5
This matrix is nonsingular since
det
P
=
 1, and a calculation which we leave to the
reader shows that
P
 1
=
2
4
1
1
2
0
0
1
0
1
2
3
5
The discussion of the ï¬rst part of this section shows us that
P
 1
AP
=
2
4
2
0
0
0
2
0
0
0
1
3
5
=
D

228
5. THE EIGENVALUE PROBLEM
As we have seen, this means that for any positive integer
k, we have
A
k
=
P
D
k
P
 1
=
2
4
1
0
 1
0
 2
1
0
1
0
3
5
2
4
2
k
0
0
0
2
k
0
0
0
1
k
3
5
2
4
1
1
2
0
0
1
0
1
2
3
5
=
2
4
2
k
2
k
 1
2
k
+1
 2
0
1
 2
k
+1
+
2
0
0
2
k
3
5
This is the formula we were looking for. Itâ€™s much easier than calculating
A
k directly!
This example showcases a very nice calculation. Given a general matrix
A, when can
we pull off the same calculation? First, letâ€™s give the favorable case a name.
DEFINITION 5.2.8. The matrix
A is diagonalizable if it is similar to a diagonal matrix,
that is, there is an invertible matrix
P and diagonal matrix
D such that
P
 1
AP
=
D
:
In this case we say that
P is a diagonalizing matrix for
A or that
P diagonalizes
A:
The question is, can we be more speciï¬c about when a matrix is diagonalizable? We
can. As a ï¬rst step, notice that the calculations that we began the section with can easily
be written in terms of an
n

n matrix instead of
3

3: What these calculations prove
is the following basic fact.
THEOREM 5.2.9.
The
n

n matrix
A is diagonalizable if and only if there exists
Diagonalization
Theorem
a linearly independent set of eigenvectors
v
1
;
v
2
;
:
:
:
;
v
n of
A, in which case
P
=
[v
1
;
v
2
;
:
:
:
;
v
n
] is a diagonalizing matrix for
A:
Can we be more speciï¬c about when a linearly independent set of eigenvectors ex-
ists? Actually, we can. Clues about what is really going on can be gleaned from a
re-examination of Example 5.1.13.
EXAMPLE 5.2.10. Apply the results of the preceding discussion to the matrix in part
(b) of Example 5.1.13 or explain why they fail to apply.
SOLUTION. The eigenvalues of this problem are

=
1;
2;
2: We already found the
eigenspace for

=
2: Denote the single basis vector of
E
2
(A) by
v
1
=
(1;
0;
0) . For

=
1, apply Gauss-Jordan elimination to the matrix
A
 1I
=
2
4
2
 1
1
1
0
1
 1
1
0
0
2
 1
3
5
=
2
4
1
1
1
0
0
1
0
0
1
3
5
and we can obviously reduce this matrix by using the pivot in the second row to
2
4
1
1
0
0
0
1
0
0
0
3
5
which gives a general eigenvector of the form
2
4
x
1
x
2
x
3
3
5
=
2
4
 x
2
x
2
0
3
5
=
x
2
2
4
 1
1
0
3
5
:

5.2. SIMILARITY AND DIAGONALIZATION
229
Hence the eigenspace
E
1
(A) has basis
f( 1;
1;
0)g: All we could come up with here
is two eigenvectors. As a matter of fact, they are linearly independent since one is
not a multiple of the other. But they arenâ€™t enough and there is no way to ï¬nd a third
eigenvector, since we have found them all! Therefore we have no hope of diagonalizing
this matrix according to the diagonalization theorem. The real problem here is that
A is
defective, since the algebraic multiplicity of

=
2 exceeds the geometric multiplicity
of this eigenvalue.
It would be very handy to have some working criterion for when we can manufacture
linearly independent sets of eigenvectors. The next theorem gives us such a criterion.
THEOREM 5.2.11. Let
v
1
;
v
2
;
:
:
:
;
v
k be a set of eigenvectors of the matrix
A such
that corresponding eigenvalues are all distinct. Then the set of vectors
v
1
;
v
2
;
:
:
:
;
v
k
is linearly independent.
PROOF. Suppose the set is linearly dependent. Discard redundant vectors until
we have a smallest linearly dependent subset, say
v
1
;
v
2
;
:
:
:
;
v
m is such a set with
v
i belonging to

i
: All the vectors have nonzero coefï¬cients in a linear combination
that sums to zero, for we could discard the ones that have zero coefï¬cient in the linear
combination and still have a linearly dependent set. So there is some linear combination
of the form
c
1
v
1
+
c
2
v
2
+
:
:
:
+
c
m
v
m
=
0
(5.2.2)
with each
c
j
6=
0 and
v
j belonging to eigenvalue

j. Multiply (5.2.2) by

1 to obtain
the equation
c
1

1
v
1
+
c
2

1
v
2
+
:
:
:
+
c
m

1
v
m
=
0:
(5.2.3)
Next multiply (5.2.2) on the left by
A to obtain
0
=
A(c
1

1
v
1
+
c
2

1
v
2
+
:
:
:
+
c
m

1
v
m
)
=
c
1
Av
1
+
c
2
Av
2
+
:
:
:
+
c
k
Av
k
;
that is,
c
1

1
v
1
+
c
2

2
v
2
+
:
:
:
+
c
k

m
v
m
=
0:
(5.2.4)
Now subtract (5.2.4) from (5.2.3) to obtain
0v
1
+
c
2
(
1
 
2
)v
2
+
:
:
:
+
c
k
(
1
 
m
)v
m
=
0
This is a new nontrivial linear combination ( since
c
2
(
1
 
2
)
6=
0) of fewer terms
which contradicts our choice of
v
1
;
v
2
;
:
:
:
;
v
k
: It follows that the original set of vectors
must be linearly independent.
Actually, a little bit more is true: if
v
1
;
v
2
;
:
:
:
;
v
k is such that for any eigenvalue

of
A, the subset of all these vectors belonging to
 is linearly independent, then the
conclusion of the theorem is valid. We leave this as an exercise. Hereâ€™s an application
of the theorem which is useful for many problems.
COROLLARY 5.2.12. If the
n

n matrix
A has
n distinct eigenvalues, then
A is diag-
onalizable.
PROOF. We can always ï¬nd one nonzero eigenvector
v
i for each eigenvalue

i of
A: By the preceding theorem, the set
v
1
;
v
2
;
:
:
:
;
v
n is linearly independent. Thus
A is
diagonalizable by the Diagonalization theorem.

230
5. THE EIGENVALUE PROBLEM
Caution: Just because the
n

n matrix
A has fewer than
n distinct eigenvalues, you
may not conclude that it is not diagonalizable.
An example that illustrates this caution is part (a) of Example 5.1.13.
5.2 Exercises
1. Given each matrix
A below, ï¬nd a matrix
P such that
P
 1
AP is diagonal. Use this
to deduce a formula for
A
k
:
(a)

1
2
3
2

(b)
2
4
1
0
0
0
2
1
0
0
1
3
5
(c)

0
2
2
0

2. Determine if the following matrices are diagonalizable with a minimum of calcula-
tion.
(a)

0
0
1
0

(b)
2
4
1
3
0
0
2
1
0
1
1
3
5
(c)

2
1
0
3

3. For each of the following matrices
A ï¬nd the characteristic polynomial
p(x) and
evaluate
p(A): (This means that the matrix
A replaces every occurrence of
x and the
constant term
c
0 is replaced by
c
0
I
:)
(a)

2
0
1
1

(b)
2
4
1
3
0
 2
2
1
4
0
 2
3
5
(c)

a
0
0
b

4. Suppose that
A is an invertible matrix which is diagonalized by the matrix
P
; that is,
P
 1
AP
=
D is a diagonal matrix. Use this information to ï¬nd a diagonalization for
A
 1
:
5. Adapt the proof of Theorem 5.2.11 to prove that if eigenvectors
v
1
;
v
2
;
:
:
:
;
v
k are
such that for any eigenvalue
 of
A, the subset of all these vectors belonging to
 is
linearly independent, then the vectors
v
1
;
v
2
;
:
:
:
;
v
k are linearly independent.
6. Suppose that the kill rate
r of Example 5.2.2 is viewed as a variable positive param-
eter. There is a value of the number
r for which the eigenvalues of the corresponding
matrix are equal.
(a) Find this value of
r and the corresponding eigenvalues by examining the character-
istic polynomial of the matrix.
(b) Use the available MAS (or CAS) to determine experimentally the long term behavior
of populations for the value of
r found in (a). Your choices of initial states should
include
[100;
1000]:
7. The thirteenth century mathematician Leonardo Fibonacci discovered the sequence
of integers
1;
1;
2;
3;
5;
8;
:
:
: called the Fibonacci sequence. These numbers have a way
of turning up in many applications. They can be speciï¬ed by the formulas
f
0
=
1
f
1
=
1
f
n+2
=
f
n+1
+
f
n
;
n
=
0;
1;
:
:
:
:

5.2. SIMILARITY AND DIAGONALIZATION
231
(a) Let
x
n
=
(f
n+1
;
f
n
) and show that these equations are equivalent to the matrix
equations
x
0
=
(1;
1) and
x
n+1
=
Ax
n
;
n
=
0;
1;
:
:
:
; where
A
=

1
1
1
0

(b) Use part (a) and the diagonalization theorem to ï¬nd an explicit formula for the
nth
Fibonacci number.
8.
Calculate second, third and fourth powers of the matrix
J

(3)
=
2
4

1
0
0

1
0
0

3
5
:
Based on these calculations, make a conjecture about the form of
J

(3)
k
; where
k is
any positive integer.
9. Show that any upper triangular matrix with constant diagonal is diagonalizable if
and only if it is already diagonal. Hint: What diagonal matrix would such a matrix be
similar to?
10. Let
A be a
2

2 transition matrix of a Markov chain where
A is not the identity
matrix.
(a) Show that
A can be written in the form
A
=

1
 a
b
a
1
 b

for suitable real
numbers
0

a;
b

1:
(b) Show that
(b;
a) and
(1;
 1) are eigenvectors for
A:
(c) Use (b) to diagonalize the matrix
A and obtain a formula for the powers of
A:
11. Show that if
A is diagonalizable, then so is
A
H
:
12. Let
A;
B
;
P square matrices of the same size with
P invertible.
(a) Show that
P
 1
(A
+
B
)P
=
P
 1
AP
+
P
 1
B
P
(b) Show that
P
 1
AB
P
=
(P
 1
AP
)(P
 1
B
P
)
(c) Use (a) and (b) to show that if
f
(x)
=
a
0
+
a
1
x
+



+
a
n
x
n is a polynomial
function, then
P
 1
f
(A)P
=
f
(P
 1
AP
):
13. Prove the Cayley-Hamilton theorem for diagonalizable matrices; that is, show that
if
p(x) is the characteristic polynomial of the diagonalizable matrix
A; then
A satisï¬es
its characteristic equation, that is,
p(A)
=
0: Hint: You may ï¬nd Exercise 12 and
Corollary 5.2.4 very helpful.
14. Let
A and
B be matrices of the same size and suppose that
A has no repeated eigen-
values. Show that
AB
=
B
A if and only if
A and
B are simultaneously diagonalizable,
that is, a single matrix
P diagonalizes
A and
B
: Hint: The diagonalization theorem and
Exercise 24 are helpful.

232
5. THE EIGENVALUE PROBLEM
5.3. Applications to Discrete Dynamical Systems
Now we have enough machinery to come to a fairly complete understanding of the
discrete dynamical system
x
(k
+1)
=
Ax
(k
)
:
Diagonalizable Transition Matrix
Let us ï¬rst examine the case that
A is diagonalizable. So we assume that the
n

n
matrix
A is diagonalizable and that
v
1
;
v
2
;
:
:
:
;
v
n is a complete linearly independent
set of eigenvectors of
A belonging to the distinct eigenvalues

1
;

2
;
:
:
:
;

n of
A: Let
us further suppose that these eigenvalues are ordered so that
j
1
j

j

2
j

:
:
:

j
n
j
:
The eigenvectors
v
1
;
v
2
;
:
:
:
;
v
n form a basis of
R
n or
C
n, whichever is appropriate.
In particular, we may write
x
(0) as a linear combination of these vectors, say
x
(0)
=
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
:
(5.3.1)
Now we can see what the effect of multiplication by
A is:
Ax
(0)
=
A(c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
)
=
c
1
(Av
1
)
+
c
2
(Av
2
)
+



+
c
n
(Av
n
)
=
c
1

1
v
1
+
c
2

2
v
2
+



+
c
n

n
v
n
:
Now apply
A on the left repeatedly and we see that
x
(k
)
=
A
k
x
(0)
=
c
1

k
1
v
1
+
c
2

k
2
v
2
+



+
c
n

k
n
v
n
:
(5.3.2)
Equation 5.3.2 is the key to understanding how the state vector changes in a discrete
dynamical system. Now we can see clearly that it is the size of the eigenvalues that
governs the growth of successive states. Because of this fact, a handy quantity that
can be associated with a matrix
A (whether it is diagonalizable or not) is the so-called
spectral radius of
A, which we denote by
(A): This number is deï¬ned by the formula
(A)
=
maxfj

1
j
;
j
2
j
;
:
:
:
;
j
n
jg:
That is,
(A) is the largest absolute value of the eigenvalues of
A: We summarize a
few of the conclusions that can be drawn in terms of the spectral radius and dominant
eigenvalues.
THEOREM 5.3.1. Let the transition matrix for a discrete dynamical system be the
n

n
diagonalizable matrix
A as described above. Let
x
(0) be an initial state vector given as
in Equation 5.3.1. Then the following are true:
1. If
(A)
<
1, then
lim
k
!1
x
(k
)
=
0:
2. If
(A)
=
1; then the sequence of norms
f




x
(k
)




g is bounded.

5.3. APPLICATIONS TO DISCRETE DYNAMICAL SYSTEMS
233
3. If
(A)
=
1 and the only eigenvalues
 of
A with
jj
=
1 are

=
1; then
lim
k
!1
x
(k
) is an element of
E
1
(A); hence either an eigenvector or
0:
4. If
(A)
>
1, then for some choices of
x
(0) we have
lim
k
!1




x
(k
)




=
1:
PROOF. Suppose that
(A)
<
1: Then for all
i,

k
i
!
0 as
k
!
1, so we see
from Equation 5.3.2 that
x
(k
)
!
0 as
k
!
1, which is what (1) says. Next suppose
that
(A)
=
1: Then take norms of Equation 5.3.2 to obtain that, since each
j
i
j

1;



x
(k
)



=



A
k
x
(0)



=


c
1

k
1
v
1
+
c
2

k
2
v
2
+



+
c
n

k
n
v
n



j
1
j
k
k
c
1
v
1
k
+
j
2
j
k
kc
2
v
2
k
+



+
j
n
j
k
kc
n
v
n
k

kc
1
v
1
k
+
kc
2
v
2
k
+



+
kc
n
v
n
k
:
Therefore the sequence of norms


x
(k
)

 is bounded by a constant that only depends on


x
(0)

, which proves (2). The proof of (3) follows from inspection of (5.3.2): Observe
that the eigenvalue powers

k
j
=
1 if

=
1 and otherwise the powers tend to zero since
all other eigenvalues are less than
1 in absolute value. Hence if any coefï¬cient
c
j of
an eigenvector
v
j corresponding to
1 is not zero, the limiting vector is an eigenvector
corresponding to

=
1: Otherwise, the coefï¬cients all tend to
0 and the limiting vector
is
0: Finally, if
(A)
>
1, then for
x
(0)
=
c
n
v
n, we have that
x
(k
)
=
c
n

k
n
v
n
: However,
j
n
j
>
1, so that



k
n


!
1, as
k
!
1, from which the desired conclusion for (4)
follows.
We should note that the cases of the preceding theorem are not quite exhaustive. One
possibility that is not covered is the case that
(A)
=
1 and
A has other eigenvalues of
absolute value
1: In this case the sequence of vectors
x
(k
)
;
k
=
0;
1;
:
:
:
; is bounded in
norm but need not converge to anything. An example of this phenomenon is given in
Example 5.3.4
EXAMPLE 5.3.2. Apply the preceding theory to the population of Example 5.2.2.
SOLUTION. We saw in this example that the transition matrix is
A
=

0:6
0:4
 0:35
1:4

:
The characteristic equation of this matrix is
det

0:6
 
0:4
 0:35
1:4
 

=
(0:6
 )(1:4
 )
+
0:35

0:4
=

2
 2
+
0:84
+
0:14
=

2
 2
+
0:98
whence we see that the eigenvalues of
A are

=
1:0

p
4
 3:92
=2

0:85858;
1:1414
A calculation which we leave to the reader also shows that the eigenvectors of
A cor-
responding to these eigenvalues are approximately
v
1
=
(:8398;
:54289) and
v
2
=
(1:684;
2:2794), respectively. Since
(A)
t
1:1414
>
1, it follows from (1) of Theo-
rem 5.3.1 that for every initial state except a multiple of
v
1, the limiting state will grow

234
5. THE EIGENVALUE PROBLEM
without bound. Now if we imagine an initial state to be a random choice of values for
the coefï¬cients
c
1 and
c
2, we see that the probability of selecting
c
2
=
0, is for all prac-
tical purposes,
0: Therefore, with probability
1, we will make a selection with
c
2
6=
0,
from which it follows that the subsequent states will tend to arbitrarily large multiples
of the vector
v
2
=
(1:684;
2:2794):
Finally, we can offer some advice to the scientists who are thinking of introducing a
predator bird to control the frog population of this example: donâ€™t do it! Almost any
initial distribution of birds and frogs will result in a population of birds and frogs that
grows without bound. Therefore, we will be stuck with both non-indigenous frogs and
birds. To drive the point home, start with a population of
10;
000 frogs and
100 birds.
In
20 years we will have a population state of

0:6
0:4
 0:35
1:4

20

100
10;
000



197;
320
267;
550

In view of our eigensystem analysis, we know that these numbers are no ï¬‚uke. Almost
any initial population will grow similarly. The conclusion is that we should try another
strategy or perhaps leave well enough alone in this ecology.
EXAMPLE 5.3.3. Apply the preceding theory to the Markov chain Example 2.3.4 of
Chapter 2.
SOLUTION. Recall that this example led to a Markov chain whose transition matrix is
given by
A
=

0:7
0:4
0:3
0:6

:
Conveniently, we have already computed the eigenvalues and vectors of
10A in Ex-
ample 5.1.9. There we found eigenvalues

=
3;
10, with corresponding eigenvectors
v
1
=
(1;
 1) and
v
2
=
(4=3;
1), respectively. It follows from Example 5.1.9 that the
eigenvalues of
A are

=
0:3;
1, with the same eigenvectors. Therefore
1 is the dom-
inant eigenvalue. Any initial state will necessarily involve
v
2 nontrivially, since mul-
tiples of
v
1 are not probability distribution vectors (the entries are of opposite signs).
Thus we may apply part 3 of Theorem 5.3.1 to conclude that for any initial state, the
only possible nonzero limiting state vector is some multiple of
v
2
: Which multiple?
Since the sum of the entries of each state vector sum to
1, the same must be true of the
initial vector. Since
x
(0)
=
c
1
v
1
+
c
2
v
2
=
c
1

1
 1

+
c
2

4=3
1

=

c
1
1
+
c
2
(4=3)
c
1
( 1)
+
c
2
1

we see that
1
=
c
1
1
+
c
2
(4=3)
+
c
1
( 1)
+
c
2
1
=
c
2
(7=3)
so that
c
2
=
3=7: Now use the facts that

1
=
0:3;

2
=
1 and Equation 5.3.2 with
n
=
2 to see that the limiting state vector is
lim
k
!1
c
1
(0:3)
k
v
1
+
c
2
1
k
v
2
=
c
2
v
2
=

4=7
3=7



:57143
:42857

:

5.3. APPLICATIONS TO DISCRETE DYNAMICAL SYSTEMS
235
Compare this vector with the result obtained by direct calculation in Example 2.2.5.
When do complex eigenvalues occur and what do they mean? In general, all we can say
is that the characteristic polynomial of a matrix, even if it is real, may have complex
roots. This is an unavoidable fact, but it can be instructive. To see how this is so,
consider the following example.
EXAMPLE 5.3.4. Suppose that a discrete dynamical system has transition matrix
A
=

0
a
 a
0

; where
a is a positive real number. What can be said about the states
x
(k
)
;
k
=
1;
2;
:
:
: if the initial state
x
(0) is an arbitrary nonzero vector?
SOLUTION. The eigenvalues of
A are
ai: Now if
a
<
1 then according to part 1 of
Theorem 5.3.1 the limiting state is
0: Part 3 of that theorem cannot occur for our matrix
A since
1 cannot be an eigenvalue. So suppose
a

1: Since the eigenvalues of
A are
distinct, there is an invertible matrix
P such that
P
 1
AP
=
D
=

ai
0
0
 ai

:
So we see from Equation 5.2.1 that
A
k
=
P
D
k
P
 1
=
P

(ai)
k
0
0
( ai)
k

P
 1
The columns of
P are eigenvectors of
A, hence complex. We may take real parts of the
matrix
D
k to get a better idea of what the powers of
A do. Now
i
=
e
i

2
; so we may
use DeMoivreâ€™s formula to get
<((ai)
k
)
=
a
k
cos
(k

2
)
=
( 1)
k
=2
a
k
if
k is even
We know that
x
k
=
A
k
x
0
: In view of the above equation, we see that the states
x
k
will oscillate around the origin. In the case that
a
=
1 we expect the states to remain
bounded, but if
a
>
1; we expect the values to become unbounded and oscillate in sign.
This oscillation is fairly typical of what happens when complex eigenvalues are present,
though it need not be as rapid as in this example.
Non-Diagonalizable Transition Matrix
How can a matrix be non-diagonalizable? All the examples we have considered so
far suggest that non-diagonalizability is the same as being defective. Put another way,
diagonalizable equals non-defective. This is exactly right, as the following shows.
THEOREM 5.3.5. The matrix
A is diagonalizable if and only if it is non-defective.
PROOF. Suppose that the
n

n matrix
A is diagonalizable. According to the di-
agonalization theorem, there exists a complete linearly independent set of eigenvectors
v
1
;
v
2
;
:
:
:
v
n of the matrix
A: The number of these vectors belonging to a given eigen-
value
 of
A is a number
d() at most the geometric multiplicity of
, since they form a
basis of the eigenspace
E

(A): Hence their number is at most the algebraic multiplicity
m() of
 by Theorem 5.1.16. Since sum of all the numbers
d
() is
n, as is the sum of

236
5. THE EIGENVALUE PROBLEM
all the algebraic multiplicities
m(), it follows that the sum of the geometric multiplic-
ities must also be
n: The only way for this to happen is that for each eigenvalue
, we
have that geometric multiplicity equals algebraic multiplicity. Thus,
A is non-defective.
Conversely, if
A is non-defective, we can produce
m() linearly independent eigen-
vectors belonging to each eigenvalue
: Assemble all of these vectors and we have
n
eigenvectors such that for any eigenvalue
 of
A, the subset of all these vectors belong-
ing to
 is linearly independent. Therefore, the entire set of eigenvectors is linearly
independent by the remark following Theorem 5.2.11. Now apply the diagonalization
theorem to obtain that
A is diagonalizable.
The last item of business in our examination of diagonalization is to prove part 2 of
Theorem 5.1.16, which asserts:
For each eigenvalue
 of
A, if
m() is the algebraic multiplicity of
, then
1

dim
E

(A)

m():
To see why this is true, suppose the eigenvalue
 has geometric multiplicity
k and
that
v
1
;
v
2
;
:
:
:
;
v
k is a basis for the eigenspace
E

(A): We know from the Steinitz
substitution theorem that this set can be expanded to a basis of the vector space
R
n (or
C
n), say
v
1
;
v
2
;
:
:
:
;
v
k
;
v
k
+1
;
:
:
:
;
v
n
Form the nonsingular matrix
S
=
[v
1
;
v
2
;
:
:
:
;
v
n
]
Let
B
=
[S
 1
Av
k
+1
;
S
 1
Av
k
+2
;
:
:
:
;
S
 1
Av
n
]
=

F
G

where
F consists of the ï¬rst
k rows of
B and
G the remaining rows. Thus we obtain
that
AS
=
[Av
1
;
Av
2
;
:
:
:
;
Av
n
]
=
[v
1
;
v
2
;
:
:
:
;
v
k
;
Av
k
+1
;
:
:
:
;
Av
n
]
=
S

I
k
F
0
G

Now multiply both sides on the left by
S
 1 and we have
C
=
S
 1
AS
=

I
k
F
0
G

We see that the block upper triangular matrix
C is similar to
A: By part 2 of Theo-
rem 5.2.6 we see that
A and
C have the same characteristic polynomial. However, the

5.3. APPLICATIONS TO DISCRETE DYNAMICAL SYSTEMS
237
characteristic polynomial of
C is
p()
=
det

I
n
 
I
k
F
0
G

=
det

(
 )I
k
F
0
G
 I
n k

=
det(
 )I
k

det
(G
 I
n k
)
=
(
 )
k
det
(G
 I
n k
)
The product term above results from Exercise 11 of Section 2.6 of Chapter 2. It follows
that the algebraic multiplicity of
 as a root of
p() is at least as large as
k, which is
what we wanted to prove.
Our newfound insight into non-diagonalizablematrices is somewhat of a negative nature
â€“ they are defective. Unfortunately, this isnâ€™t much help in determining the behavior of
discrete dynamical systems with a non-diagonalizable transition matrix. If matrices are
not diagonalizable, what simple kind of matrix are they reducible to? There is a very
nice answer to this question; this answer requires the notion of a Jordan block, which
can be deï¬ned as a
d

d matrix of the form
J
d
()
=
2
6
6
6
6
4

1

...
...
1

3
7
7
7
7
5
where the entries off the main diagonal and ï¬rst super-diagonal are understood to be
zeros. This matrix is very close to being a diagonal matrix. Its true value comes from
the following classical theorem, the proof of which is somewhat beyond the scope of
this text. We refer the reader to the textbooks [7] or [6] of the bibliography for a proof.
These texts are an excellent references for higher level linear algebra and matrix theory.
THEOREM 5.3.6.
Every matrix
A is similar to a block diagonal matrix which consists
Jordan
Canonical Form
Theorem
of Jordan blocks down the diagonal. Moreover, these blocks are uniquely determined
by
A up to order.
In particular, if
J
=
S
 1
AS, where
J consists of Jordan blocks down the diagonal, we
call
J â€œtheâ€ Jordan canonical form of the matrix
A: This is a slight abuse of language,
since the order of occurrence of the Jordan blocks of
J could vary. To ï¬x ideas, letâ€™s
consider an example.
EXAMPLE 5.3.7. Find all possible Jordan canonical forms for a
3

3 matrix
A whose
eigenvalues are
 2;
3;
3:
SOLUTION. Notice that each Jordan block
J
d
() contributes
d eigenvalues
 to the
matrix. Therefore, there can be only one
1

1 Jordan block for the eigenvalue
 2 and
either two
1

1 Jordan blocks for the eigenvalue
3 or one
2

2 block for the eigenvalue
3: Thus, the possible Jordan canonical forms for
A (up to order of blocks) are
2
4
 2
0
0
0
3
0
0
0
3
3
5 or
2
4
 2
0
0
0
3
1
0
0
3
3
5

238
5. THE EIGENVALUE PROBLEM
Notice that if all Jordan blocks are
1

1, then the Jordan canonical form of a matrix is
simply a diagonal matrix. Thus, another way to say that a matrix is diagonalizable is to
say that its Jordan blocks are
1

1: In reference to the previous example, we see that if
the matrix has the ï¬rst Jordan canonical form, then it is diagonalizable, while if it has
the second, it is non-diagonalizable.
Now suppose that the matrix
A is a transition matrix for a discrete dynamical system
and
A is not diagonalizable. What can one say? For one thing, the Jordan canonical
form can be used to recover part 1 of Theorem 5.3.1. Part 4 remains valid as well; the
proof we gave does not depend on
A being diagonalizable. Unfortunately, things are
a bit more complicated as regards parts (2) and (3). In fact, they fail to be true, as the
following example shows.
EXAMPLE 5.3.8. Let
A
=
J
2
(1): Show how parts (2) and (3) of Theorem 5.3.1 fail to
be true for this matrix.
SOLUTION. We check that
A
2
=

1
1
0
1


1
1
0
1

=

1
2
0
1

;
A
3
=

1
2
0
1


1
1
0
1

=

1
3
0
1

and in general
A
k
=

1
k
0
1

Now take
x
(0)
=
(0;
1) and we see that
x
(k
)
=
A
k
x
(0)
=

1
k
0
1


0
1

=

k
1

It follows that the norms




x
(k
)




=
p
k
2
+
1 are not a bounded sequence, so that part
2 of the theorem fails to be true. Also, the sequence of vectors
x
(k
) does not converge
to any vector in spite of the fact that
1 is the largest eigenvalue of
A: Thus (3) fails as
well.
In spite of this example, the news is not all negative. It can be shown by way of the
Jordan canonical form that a weaker version of (3) holds: if
(A)
=
1;
1 is the only
eigenvalue of
A of absolute value
1 and the eigenvalue
1 has multiplicity
1; then the
conclusion of (3) in Theorem 5.3.1 holds.
5.3 Exercises
1. Which of the following matrices are diagonalizable? Do not carry out the diagonal-
ization, but give reasons for your answers.
(a)

2
0
1
1

; (b)

2
1
 1
2

; (c)
2
4
1
0
0
0
3
1
0
1
1
3
5
;

5.3. APPLICATIONS TO DISCRETE DYNAMICAL SYSTEMS
239
(d)
2
4
1
0
1
0
1
0
0
0
1
3
5
; (e)
2
4
2
0
0
0
1
1
0
0
2
3
5
; (f)
2
4
2
1
0
0
2
1
0
0
1
3
5
2. For each matrix
A below ï¬nd an eigensystem of
A and use this to produce an invert-
ible matrix
P and diagonal matrix
D such that
P
 1
AP
=
D, where
(a)
A
=
2
4
2
0
0
0
3
1
0
0
2
3
5
; (b)
A
=
1
2

3
2
 4
 3

; (c)
A
=
1
2

3
0
8
 1

3. Do the powers
A
k tend to 0 as
k tends to inï¬nity for any of the matrices of Exer-
cise 2?
4. You are given that a
5

5 matrix has eigenvalues
2;
2;
3;
3;
3: What are the possible
Jordan canonical forms for this matrix?
5. Compute by hand or calculator various power of the matrix
J
3
(1)
=
2
4
1
1
0
0
1
1
0
0
1
3
5
:
Make a conjecture about a general formula for the
kth power of this matrix, based on
these calculations.
In the next two exercises, we use the idea of a dominant eigenvalue, that is, an eigenvalue
 of the matrix
A such that
 is a simple eigenvalue and
jj
>
jj for every other
eigenvalue
 of
A:
6. If the eigenvalues of a
3

3 matrix
A are one of the following lists, determine which
is the dominant eigenvalue of
A, if any.
(a)

=
1;
2;
2
(b)

=
2;
2;
 4
(c)

=
5=4;
1
+
i;
1
 i
7. Suppose the transition matrix
A of a discrete dynamical system has a dominant
eigenvalue of
1: What conclusion can you extract from Theorem 5.3.1? Illustrate this
conclusion with an example.
8. Part (3) of Theorem 5.3.1 suggests that two possible limiting values are possible. Use
your CAS or MAS to carry out this experiment: Compute a random
2

1 vector and
normalize it by dividing by its length. Let the resulting initial vector be
x
(0)
=
(x
1
;
x
2
)
and compute the state vector
x
(20) using the transition matrix
A of Example 5.3.3. Do
this for a large number of times (say
500) and keep count of the number of times
x
(20)
is close to
0; say
kx
(20)
k
<
0:1. Conclusions?
9. Use a CAS or MAS to construct a
3

10 table whose
jth column is
A
j
x; where
x
=
(1;
1;
1) and
A
=
2
4
10
17
8
 8
 13
 6
4
7
4
3
5
: What can you deduce about the eigenvalues
of
A based on inspection of this table? Give reasons. Check your claims by ï¬nding the
eigenvalues of
A:
10. A species of bird can be divided into three age groups, say birds of age less than
2 years for group 1, birds of age between 2 and 4 years for group 2, and birds of age
between 4 and 6 years for the third group. Assume that for all practical purposes, these
birds have at most a 6 year life span. It is determined that the survival rates for group 1
and 2 birds are
50% and
25%; respectively. On the other hand, group 1 birds produce no

240
5. THE EIGENVALUE PROBLEM
offspring, while group 2 birds average 4 offspring in any biennium (period of 2 years),
and birds in group 3 average 3 offspring in a biennium. Model this bird population as a
discrete dynamical system, where a state vector
(p
1
;
p
2
;
p
3
) for a given biennium means
that there are
p
1 birds in group 1,
p
2 birds in group 2 and
p
3 birds in group 3.
11. Compute the largest eigenvalue of the transition matrix for the model of Exer-
cise 10. What does this suggest about the biennial growth rate of the bird population?
12. Let
A
=
J
3
(); the Jordan block. Show that the Cayley-Hamilton theorem is valid
for
A; that is,
p(A)
=
0; where
p(x) is the characteristic polynomial of
A:
13. The ï¬nancial model of Example 2.3.12 gave rise to difference equation which was
converted to the dynamical system
x
(k
+1)
=
Ax
(k
)
; where the transition matrix is given
by
A
=
2
4
1
0:06
0:12
1
0
0
0
1
0
3
5
Use a CAS or MAS to calculate the eigenvalues of this matrix. Deduce that
A is diago-
nalizable and determine the approximate growth rate from one state to the next, given a
random initial vector. Compare the growth rate with a ï¬‚at interest rate.
5.4. Orthogonal Diagonalization
Orthogonal and unitary matrices are particularly attractive when we have to deal with
inverses. Recall that one situation which calls for an inverse matrix is that of diago-
nalization. For
A is diagonalizable when there exists an invertible matrix
P such that
P
 1
AP is a diagonal matrix. We are going to explore some very remarkable facts
about Hermitian and real symmetric matrices. These matrices are diagonalizable, and
moreover, diagonalization can be accomplished by a unitary (orthogonal if
A is real)
matrix. This means that
P
 1
AP
=
P
H
AP is diagonal. In this situation we say that
the matrix
A is unitarily (orthogonally) diagonalizable.
Eigenvalue of Hermitian Matrices
As a ï¬rst step, we need to observe a curious property of Hermitian matrices. It turns
out that their eigenvalues are guaranteed to be real, even if the matrix itself is complex.
This is one reason that these matrices are so nice to work with.
THEOREM 5.4.1. If
A is a Hermitian matrix, then the eigenvalues of
A are real.

5.4. ORTHOGONAL DIAGONALIZATION
241
PROOF. Let
 be an eigenvalue of
A with corresponding nonzero eigenvector
x,
so that
Ax
=
x: Form the scalar
c
=
x
H
Ax: We have that
c
=
c
H
=
 x
H
Ax

H
=
x
H
A
H
(x
H
)
H
=
x
H
Ax
=
c
It follows that
c is a real number. However, we also have that
c
=
x
H
x
=
x
H
x
=

jjxj
j
2
so that

=
c=
jjxjj
2 is also real.
EXAMPLE 5.4.2. Show that Theorem 5.4.1 is applicable if
A
=

1
1
 i
1
+
i
0

and
verify the conclusion of the Theorem.
SOLUTION. First notice that
A
H
=

1
1
 i
1
+
i
0

H
=

1
1
+
i
1
 i
0

T
=

1
1
 i
1
+
i
0

=
A
It follows that
A is Hermitian and the theorem is applicable. Now we compute the
eigenvalues of
A by solving the characteristic equation
0
=
det
(A
 I
)
=
det

1
 
1
 i
1
+
i
 

=
(1
 )( )
 (1
+
i)
(1
 i)
=

2
 
 2
=
(
+
1)(
 2)
Hence the eigenvalues of
A are

=
 1;
2 which are real.
Caution: Although the eigenvalues of a Hermitian matrix are guaranteed to be real, the
eigenvectors may not be real unless the matrix in question is real.
The Principal Axes Theorem
A key fact about Hermitian matrices is the so-called principal axes theorem; its proof
is a simple consequence of the Schur triangularization theorem which is proved in Sec-
tion 5.5. We will content ourselves here with stating the theorem and supplying a proof
for the case that the eigenvalues of
A are distinct. This proof also shows us one way to
carry out the diagonalization process.
THEOREM 5.4.3.
Every Hermitian matrix is unitarily diagonalizable, and every real
Principal Axes
Theorem
Hermitian matrix is orthogonally diagonalizable.
PROOF. Let us assume that the eigenvalues of the
n

n matrix
A are distinct.
We saw in Theorem 5.4.1 of Chapter 4 that the eigenvalues of
A are real. Let these
eigenvalues be

1
;

2
;
:
:
:
;

n
: Now ï¬nd an eigenvector
v
k for each eigenvalue

k
: We
can assume that each
v
k is unit length by replacing it by the vector divided by its length
if necessary. We now have a diagonalizing matrix, as prescribed by the Diagonalization
Theorem, namely the matrix
P
=
[v
1
;
v
2
;
:
:
:
v
n
]:

242
5. THE EIGENVALUE PROBLEM
Remembering that
Av
j
=

j
v
j,
Av
k
=

k
v
k and that
A
H
=
A, we see that

k
v
H
j
v
k
=
v
H
j

k
v
k
=
v
H
j
Av
k
=
(Av
j
)
H
v
k
=
(
j
v
j
)
H
v
k
=

j
v
H
j
v
k
Now bring both terms to one side of the equation and factor out the term
v
H
j
v
k to obtain
(
k
 
j
)v
H
j
v
k
=
0
Thus if

k
6=

j, it follows that
v
j

v
k
=
v
H
j
v
k
=
0: In other words the eigenvectors
v
1
;
v
2
;
:
:
:
v
n form an orthonormal set. Therefore, the matrix
P is unitary. If
A is real,
then so are the vectors
v
1
;
v
2
;
:
:
:
v
n and
P is orthogonal in this case.
The proof we have just given suggests a practical procedure for diagonalizing a Her-
mitian or real symmetric matrix. The only additional information that we need for the
complete procedure is advice on what to do if the eigenvalue
 is repeated. This is a
sticky point.What we need to do in this case is ï¬nd an orthogonal basis of the eigenspace
E

(A)
=
N
(A
 I
): It is always possible to ï¬nd such a basis using the so-called Gram-
Schmidt algorithm, which is discussed in Chapter 6 or the modiï¬ed Gram-Schmidt al-
gorithm discussed on Page 208. For the hand calculations that we do in this chapter, the
worst situation that we will encounter is that the eigenspace
E
 is two-dimensional, say
with a basis
v
1
;
v
2
: In this case replace
v
2 by
v

2
=
v
2
 pro
j
v
1
v
2
: We know that
v

2 is
orthogonal to
v
1 (see Theorem 6.2.16) so that
v
1
;
v

2is an orthogonal basis of
E

(A):.
We illustrate the procedure with a few examples.
EXAMPLE 5.4.4. Find an eigensystem for the matrix
A
=
2
4
1
2
0
2
4
0
0
0
5
3
5 and use this
to orthogonally diagonalize
A:
SOLUTION. Notice that
A is real symmetric, so diagonalizable by the principal axes
theorem. First calculate the characteristic polynomial of
A as
jA
 I
j
=






1
 
2
0
2
4
 
0
0
0
5
 






=
((1
 )(4
 )
 2

2)
(5
 )
=
 (
 5)
2
so that the eigenvalues of
A are

=
0;
5;
5:
Next ï¬nd eigenspaces for each eigenvalue. For

=
0, we ï¬nd the null space by row
reduction
A
 0I
=
2
4
1
2
0
2
4
0
0
0
5
3
5
       !
E
21
( 2)
2
4
1
2
0
0
0
0
0
0
5
3
5
     !
E
23
E
2
(
1
5
)
2
4
1
2
0
0
0
1
0
0
0
3
5
so that the null space is spanned by the vector
( 2;
1;
0): Normalize this vector to obtain
v
1
=
( 2;
1;
0)=
p
5
: Next compute the eigenspace for

=
5 via row reductions
A
 5I
=
2
4
 4
2
0
2
 1
0
0
0
0
3
5
       !
E
21
(1=2)
2
4
 4
2
0
0
0
0
0
0
0
3
5
        !
E
1
( 1=4)
2
4
1
 1=2
0
0
0
0
0
0
0
3
5

5.4. ORTHOGONAL DIAGONALIZATION
243
which gives two eigenvectors,
(1=2;
1;
0) and
(0;
0;
1): Normalize these to get
v
2
=
(1;
2;
0)=
p
5 and
v
3
=
(0;
0;
1): In this case
v
2 and
v
3 are already orthogonal, so the
diagonalizing matrix can be written as
P
=
[v
1
;
v
2
;
v
3
]
=
1
p
5
2
4
 2
1
0
1
2
0
0
0
1
3
5
In fact, we can check that
P
T
AP
=
2
4
0
0
0
0
5
0
0
0
5
3
5
We leave this calculation to the reader.
PROOF. EXAMPLE 5.4.5. Let
A
=

1
1
 i
1
+
i
0

as in Example 5.4.2. Uni-
tarily diagonalize this matrix.
SOLUTION. In Example 5.4.2 we computed the eigenvalues to be

=
 1;
2: Next ï¬nd
eigenspaces for each eigenvalue. For

=
 1, we ï¬nd the null space by row reduction
A
+
I
=

2
1
 i
1
+
i
1

             !
E
21
( (1
+
i)=2)

2
1
 i
0
0

     !
E
1
(1=2)

1
(1
 i)=2
0
0

so that the null space is spanned by the vector
(( 1
+
i)=2;
1): A similar calculation
shows that a basis of eigenvectors for

=
2 consists of the vector
( 1;
( 1
 i)=2):
Normalize these vectors to obtain
u
1
=
(( 1
+
i)=2;
1)=
p
3=2 and
u
2
=
( 1;
( 1
 i)=2)=
p
3=2: So set
U
=
r
2
3

 1+i
2
 1
1
 1 i
2

and obtain that
U
 1
AU
=
U
H
AU
=

 1
0
0
2

The last calculation is left to the reader.
5.4 Exercises
1. Show the following matrices are Hermitian and ï¬nd their eigenvalues:
(a)

3
 i
i
1

(b)

 2
2
2
1

(c)
2
4
1
1
+
i
0
1
 i
2
0
0
0
2
3
5
(d)

1
1
+
i
1
 i
2

(e)

3
i
 i
0

(f)
2
4
1
2
0
2
1
 2
0
 2
1
3
5
2. Find eigensystems for the matrices of Exercise 1 and orthogonal (unitary) matrices
that diagonalize these matrices.

244
5. THE EIGENVALUE PROBLEM
3. Show that these matrices are orthogonal and compute their eigenvalues. Determine
if it is possible to orthogonally diagonalize these matrices
(a)
2
4
0
1
0
 1
0
0
0
0
 1
3
5
(b)
1
p
2

1
1
 1
1

(c)
1
p
2

1
i
i
1

4. Let
A
=

3
i
 i
3

:
(a) Show that
A is a Hermitian matrix.
(b) Find the eigenvalues and eigenvectors of
A:
(c) Find a unitary matrix
P and diagonal
D such that
P
 1
AP
=
D
:
(d) Use (c) to ï¬nd a formula for the
kth power of
A:
5. Suppose that
A is symmetric and orthogonal. Prove that the only possible eigenval-
ues of
A are
1:
6. Let
A be real symmetric positive deï¬nite matrix. Show that
A has a real symmetric
positive deï¬nite square root, that is, there is a symmetric positive deï¬nite matrix
S such
that
S
2
=
A: Hint: First show it for a diagonal matrix with positive diagonal entries.
Then use Exercise 15 and the principal axes theorem.
7. Let
A
=
2
4
2
1
0
1
3
 1
0
 1
2
3
5 so that the eigenvalues of
A are
1;
2 and
4 (assume this).
Use the method of Exercise 6 to ï¬nd a square root of
A:
5.5. *Schur Form and Applications
Recall that matrices
A and
B are similar if there is an invertible matrix
S such that
B
=
S
 1
AS; if the transformation matrix
S is unitary, then
S
 1
=
S
H
: The main
object of this section is to prove a famous theorem in linear algebra which provides a
nice answer to the following question: if we only wish to use orthogonal (or unitary)
matrices as similarity transformation matrices, what is the simplest form to which a
given matrix
A can be transformed. It would be nice if we could say something like
â€œdiagonalâ€ or â€œJordan canonical form.â€ Unfortunately, neither is possible. However,
upper triangular matrices are very nice special forms of matrices. In particular, we can
see the eigenvalues of an upper triangular matrix at a glance. That makes the follow-
ing theorem extremely attractive. Its proof is also very interesting, in that it actually
suggests an algorithm for computing the so-called Schur triangular form.

5.5. *SCHUR FORM AND APPLICATIONS
245
THEOREM 5.5.1.
Let A be an arbitrary square matrix. Then there exists a unitary
Schur Triangu-
larization
Theorem
matrix
U such that
U
T
AU is an upper triangular matrix. If
A and its eigenvalues are
real, then
U can be chosen to be orthogonal.
PROOF. We will show how to triangularize
A one column at time. First we show
how to start the process. Compute an eigenvalue

1 of
A and a corresponding eigenvec-
tor
w of unit length in the standard norm. We may assume that the ï¬rst coordinate of
w
is real. If not, replace
w by
e
 i
w where
 is a polar argument of the ï¬rst coordinate
of
w
: This does not affect the length of
w and any multiple of
w is still an eigenvector
of
A. Now let
v
=
w
 e
1, where
e
1
=
(1;
0;
:
:
:
;
0). We make the convention that
H
0
=
I
: Form the (possibly complex) Householder matrix
H
v
: Since
w

e
1 is real,
it follows from Exercise 3 that
H
v
w
=
e
1
: Now recall that Householder matrices are
unitary and symmetric, so that
H
H
v
=
H
v
=
H
 1
v
: Hence
H
H
v
AH
v
e
1
=
H
v
AH
 1
v
e
1
=
H
v
Aw
=
H
v

1
w
=

1
e
1
Therefore, the entries under the ï¬rst row and in the ï¬rst column of
H
H
v
AH
v are zero.
Suppose we have reached the
kth stage (k
=
0 is start) where we have a unitary matrix
V
k such that
V
H
k
AV
k
=
2
6
6
6
4

1





...
...

...
0




k

0



0
B
3
7
7
7
5
=

R
k
C
0
B

with the submatrix
R
k upper triangular. Compute an eigenvalue

k
+1 of the submatrix
B and a corresponding eigenvector
w of unit length in the standard norm. Now repeat
the argument of the ï¬rst paragraph with
B in place of
A to obtain a Householder matrix
H
v of the same size as
B such that the entries under the ï¬rst row and in the ï¬rst column
of
H
H
v
B
H
v are zero. Form the unitary matrix
V
k
+1
=

I
k
0
0
H
v

V
k
and obtain that
V
H
k
+1
AV
k
+1
=

I
k
0
0
H
v

V
H
k
AV
k

I
k
0
0
H
v

=

I
k
0
0
H
v


R
k
C
0
B


I
k
0
0
H
v

=

R
k
C
H
v
0
H
H
v
B
H
v

This new matrix is upper triangular in the ï¬rst
k
+
1 columns, so we can continue in
this fashion until we reach the last column, at which point we set
U
=
V
n to obtain that
U
H
AU is upper triangular. Finally, notice that if the eigenvalues and eigenvectors that
we calculate are real, which would certainly be the case if
A and the eigenvalues of
A
were real, then the Householder matrices used in the proof are all real, so that the matrix
U is orthogonal.

246
5. THE EIGENVALUE PROBLEM
Of course, the upper triangular matrix
T and triangularizing matrix
U are not unique.
Nonetheless, this is a very powerful theorem. Consider what it says in the case that
A
is Hermitian: the Principal Axes Theorem is a simple special case of it.
COROLLARY 5.5.2. Every Hermitian matrix is unitarily (orthogonally, if the matrix is
Principal Axes
Theorem
real) diagonalizable.
PROOF. Let
A be Hermitian. According to the Schur triangularization theorem
there is a unitary matrix
U such that
U
H
AU
=
R is upper triangular. We check that
R
H
=
 U
H
AU

H
=
U
H
A
H
 U
H

H
=
U
H
AU
=
R
:
Therefore
R is both upper and lower triangular. This makes
R a diagonal matrix and
proves the theorem, except for the fact that
U may be chosen orthogonal if
A is real. To
see this last fact, notice that if
A is real symmetric, then
A and its eigenvalues are real,
so according to the triangularization theorem,
U can be chosen orthogonal.
As a last application of the Schur triangularization theorem, we show the real signiï¬-
cance of normal matrices. This term has appeared in several exercises. Recall that a
(square) matrix
A is normal if
A
H
A
=
AA
H
:
COROLLARY 5.5.3. A matrix is unitarily diagonalizable if and only if it is normal.
PROOF. It is easy to see that every unitarily diagonalizable matrix is normal. We
leave this as an exercise.
Let
A be normal. According to the Schur triangularization theorem there is a unitary
matrix
U such that
U
H
AU
=
R is upper triangular. But then we have that
R
H
=
U
H
A
H
U; so that
R
H
R
=
U
H
A
H
U
U
H
AU
=
U
H
A
H
AU
=
U
H
AA
H
U
=
U
H
AU
U
H
A
H
U
=
R
R
H
Therefore
R commutes with
R
H
; which means that
R is diagonal by Exercise . This
completes the proof.
Our last application of Schurâ€™s theorem is a far reaching generalization of Theorem 5.1.5.
COROLLARY 5.5.4. Let
f
(x) and
g
(x) be polynomials and
A a square matrix such that
g
(A) is invertible (e.g., g(x)=1). Then the eigenvalues of the matrix
f
(A)g
(A)
 1 are of
the form
f
()=g
(); where
 runs over the eigenvalues of
A:
PROOF. We sketch the proof. As a ï¬rst step, we make two observations about up-
per triangular matrices
S and
T with diagonal terms

1
;

2
;
:
:
:
;

n
; and

1
;

2
;
:
:
:
;

n
;
respectively.
1.
S
T is upper triangular with diagonal terms

1

1
;

2

2
;
:
:
:
;

n

n
:
2. If
S is invertible, then
S is upper triangular with diagonal terms
1=
1
;
1=
2
;
:
:
:
;
1=
n
:
Now we have seen in Exercise 12 of Section 5.2 that for any invertible
P of the right
size,
P
 1
f
(A)P
=
f
(P
 1
AP
): Similarly, if we multiply the identity
g
(A)g
(A)
 1
=
I by
P
 1 and
P
; we see that
P
 1
g
(A)
 1
P
=
g
(P
 1
AP
)
 1
: Thus, if
P is a matrix
that unitarily diagonalizes
A; then
P
 1
f
(A)g
(A)
 1
P
=
f
(P
 1
AP
)g
(P
 1
AP
)
 1

5.6. *THE SINGULAR VALUE DECOMPOSITION
247
so that by our ï¬rst observations, this matrix is upper triangular with diagonal entries of
the required form. Since similar matrices have the same eigenvalues, it follows that the
eigenvalues of
f
(A)g
(A)
 1 are of the required form.
5.5 Exercises
1. The matrix
A
=
2
4
 1
2
2
2
 1
2
2
2
 1
3
5
has
3 as an eigenvalue. Carry out the ï¬rst step of Schur triangulation on
A:
2. Prove that every unitarily diagonalizable matrix is normal.
3. Use Corollary 5.5.2 to show that the eigenvalues of a Hermitian matrix must be real.
4. Prove that if an upper triangular matrix commutes with its Hermitian transpose, then
the matrix must be diagonal. Hint: Equate
(1;
1)th coefï¬cients of the equation
R
H
R
=
R
R
H and see what can be gained from it. Proceed to the
(2;
2)th coefï¬cient, etc.
5. Show that if
x;
y
2
C
n,
x and
y have the same length and
x

y is real, then
x
+
y
is orthogonal to
v
=
x
 y
:
6. With
x;
y
;
v as in Exercise 5, show that
x
=
1
2
(x
 y
)
+
(x
+
y
)
=
p
+
u
where
p is parallel to
v and
u is orthogonal to
v
:
7. With
x;
y
;
v as in Exercise 5, show that
H
v
x
=
y
: Hint: A text theorem about
Householder matrices applies to this setup.
5.6. *The Singular Value Decomposition
The object of this section is to develop yet one more factorization of a matrix that tells
us a lot about the matrix. For simplicity, we stick with the case of a real matrix
A
and orthogonal matrices. However, the factorization we are going to discuss can be
done with complex
A and unitary matrices. This factorization is called the singular
value decomposition (SVD for short). It has a long history in matrix theory, but was
popularized in the sixties as a powerful computational tool. Here is the basic question
that it answers: if multiplication on one side can produce an upper triangular matrix, as
in the QR factorization, how simple a matrix can be produced by multiplying on each
side by a (possibly different) orthogonal matrix? The answer, as you might guess, is a

248
5. THE EIGENVALUE PROBLEM
matrix that is both upper and lower triangular, that is, diagonal. However, veriï¬cation
of this fact is much more subtle than that of one sided factorizations such as the QR
factorization. Here is the key result:
THEOREM 5.6.1. Let
A be an
m

n real matrix. Then there exist
m

m orthogonal
Singular Value
Decomposition
matrix
U,
n

n orthogonal matrix
V and
m

n diagonal matrix
 with diagonal
entries

1


2

:
:
:


p
; with
p
=
minfm;
ng, such that
U
T
AV
=
. Moreover,
the numbers

1
;

2
;
:
:
:
;

p are uniquely determined by
A.
PROOF. There is no loss of generality in assuming that
n
=
min
fm;
ng: For if this
is not the case, we can prove the theorem for
A
T and by transposing the resulting SVD
for
A
T , obtain a factorization for
A. Form the
n

n matrix
B
=
A
T
A. This matrix
is symmetric and its eigenvalues are nonnegative (we leave these facts as exercises).
Because they are nonnegative, we can write the eigenvalues of
B in decreasing order
of magnitude as the squares of positive real numbers, say as

2
1


2
2

:
:
:


2
n.
Now we know from the Principal Axes Theorem that we can ï¬nd an orthonormal set of
eigenvectors corresponding to these eigenvalues, say
B
v
k
=

2
k
v
k
;
k
=
1;
2;
:
:
:
;
n.
Let
V
=
[v
1
;
v
2
;
:
:
:
;
v
n
]. Then
V is an orthogonal
n

n matrix. Next, suppose that

r
+1
;

r
+2
;
:
:
:
;

n are zero, while

r
6=
0.
Next set
u
j
=
1

j
Av
j
;
j
=
1;
2;
:
:
:
;
r. These are orthonormal vectors in
R
m since
u
T
j
u
k
=
1

j

k
v
T
j
A
T
Av
k
=
1

j

k
v
T
j
B
v
k
=

2
k

j

k
v
T
j
v
k
=

0; if
j
6=
k
1; if
j
=
k
Now expand this set to an orthonormal basis
u
1
;
u
2
;
:
:
:
;
u
m of
R
m. This is possible
by Theorem 4.3.12 in Section 4.3. Now set
U
=
[u
1
;
u
2
;
:
:
:
;
u
m
]. This matrix is
orthogonal and we calculate that if
k
>
r, then
u
T
j
Av
k
=
0 since
Av
k
=
0, and if
k
<
r, then
u
T
j
Av
k
=
u
T
j
Av
k
=

k
u
T
j
u
k
=

0; if
j
6=
k

k
; if
j
=
k
It follows that
U
T
AV
=
[u
T
j
Av
k
]
=
.
Finally, if
U;
V are orthogonal matrices such that
U
T
AV
=
, then
A
=
U
V
T and
therefore
B
=
A
T
A
=
V
U
T
U
V
T
=
V

2
V
T
so that the squares of the diagonal entries of
 are the eigenvalues of
B. It follows that
the numbers

1
;

2
;
:
:
:
;

n are uniquely determined by
A.
NOTATION 5.6.2. The numbers

1
;

2
;
:
:
:
;

p are called the singular values of the
matrix
A, the columns of
U are the left singular vectors of
A, and the columns of
V
are the right singular values of
A.
There is an interesting geometrical interpretation of this theorem from the perspective
of linear transformations and change of basis as developed in Section 3.7. It can can be
stated as follows.

5.6. *THE SINGULAR VALUE DECOMPOSITION
249
COROLLARY 5.6.3. Let
T
:
R
n
!
R
m be a linear transformation with matrix
A with
respect to the standard bases. Then there exist orthonormal bases
u
1
;
u
2
;
:
:
:
;
u
m and
v
1
;
v
2
;
:
:
:
;
v
n of
R
m and
R
n
; respectively, such that the matrix of
T with these bases
is diagonal with nonnegative entries down the diagonal.
PROOF. First observe that if
U
=
[u
1
;
u
2
;
:
:
:
;
u
m
] and
V
=
[v
1
;
v
2
;
:
:
:
;
v
n
];
then
U and
V are the change of basis matrices from the standard bases to the bases
u
1
;
u
2
;
:
:
:
;
u
m and
v
1
;
v
2
;
:
:
:
;
v
n of
R
m and
R
n
; respectively. Also,
U
 1
=
U
T
:
Now apply Corollary 3.7.5 of Section 3.7 and the result follows.
We leave the following fact as an exercise.
COROLLARY 5.6.4. Let
U
T
AV
=
 be the SVD of
A and suppose that

r
6=
0 and

r
+1
=
0: Then
1.
rank
A
=
r
2.
k
er
A
=
spanfv
r
+1
;
v
r
+2
;
:
:
:
;
v
n
g
3.
range
A
=
span
fu
1
;
u
2
;
:
:
:
;
u
r
g
We have only scratched the surface of the many facets of the SVD. Like most good
ideas, it is rich in applications. We mention one more. It is based on the following fact,
which can be proved by examining the entries of
A
=
U
V
T
: The matrix
A of rank
r
can be written as a sum of
r rank one matrices, namely
A
=

1
u
1
v
T
1
+

2
u
2
v
T
2
+



+

r
u
r
v
T
r
where the

k
;
u
k
;
v
k are the singular values, left and right singular vectors, respectively.
In fact, it can be shown that this representation is the most economical in the sense that
the partial sums

1
u
1
v
T
1
+

2
u
2
v
T
2
+



+

k
u
k
v
T
k
;
k
=
1;
2;
:
:
:
;
r
give the rank
k approximation to
A that is closest among all rank
k approximations to
A: This gives us an intriguing way to compress data in a lossy way (i.e., with some loss
of data). For example, suppose
A is a matrix of ï¬‚oating point numbers representing a
picture. We might get a reasonable good approximation to the picture by using only the

k larger than a certain threshhold. Thus, with a
1;
000

1;
000; matrix
A that has a very
small

21
; we could get by with the data

k
;
u
k
;
v
k
;
k
=
1;
2;
:
:
:
;
20: Consequently, we
would only store these quantities, which add up to
1000

40
+
20
=
40;
020 numbers.
Contrast this with storing the full matrix of
1;
000

1;
000
=
1;
000;
000 entries, and
you can see the gain in economy.
5.6 Exercises
1. Exhibit a singular value decomposition for the following matrices.
(a)

3
0
0
0
 1
0

(b)
2
4
 2
0
0
1
0
 1
3
5 (c)
2
4
1
0
1
0
0
0
0
0
2
3
5
2. Express the singular values and vectors of
A
T in terms of those of
A:

250
5. THE EIGENVALUE PROBLEM
3. Trace through the proof of the SVD with the matrix and construct the SVD of the
matrix
2
4
1
0
1
1
0
 1
3
5
4. Find the least squares solution to the problem
Ax
=
b when
A only has nonzero
entries along the main diagonal. Then use this solution to design an algorithm for
solving the general least squares problem by means of the SVD of
A:
5. Let
A be a real matrix and
U;
V orthogonal matrices.
(a) Show from deï¬nition that
kU
T
AV
k
2
=
kAk
2
(b) Determine
kk
2 if
 is a diagonal matrix with non-negative entries.
(c) Use parts (a),(b) and the SVD to express
kAk
2 in terms of the singular values of
A:
6. Prove Corollary 5.6.4.
7. Digitize a picture into a
640

400 (standard VGA) matrix of greyscale pixels, where
the value of each pixel is a number
x;
0

x

1; with black corresponding to
x
=
0 and white to
x
=
1: Compute the SVD of this image matrix and display various
approximations using
10;
20 and
40 of the singular values and vector pairs. Do any of
these give a good visual approximation to the picture? If not, ï¬nd a minimal number
that works. You will need computer support for this exercise.
5.7. *Computational Notes and Projects
Computation of Eigensystems
Nowadays, one can use a MAS like MATLAB or Octave on a home PC to ï¬nd a com-
plete eigensystem for, say a
100

100 matrix, in less than a second. Thatâ€™s pretty
remarkable and, to some extent, a tribute to the fast cheap hardware commonly avail-
able to the public. But hardware is only part of the story. Bad computational algorithms
can bring the fastest computer to its knees. The rest of the story concerns the remark-
able developments in numerical linear algebra over the past 50 years which have given
us fast reliable algorithms for eigensystem calculation. We can only scratch the surface
of these developments in this brief discussion. At the outset, we rule out the methods
developed in this chapter as embodied in the eigensystem algorithm (Page 216). These
are for simple hand calculations and theoretical purposes. See the polynomial equations
project that follows this discussion for some more comments about root ï¬nding.

5.7. *COMPUTATIONAL NOTES AND PROJECTS
251
We are going to examine some iterative methods for selectively ï¬nding eigenpairs of a
real matrix whose eigenvalues are real and distinct. Hence the matrix
A is diagonaliz-
able. The hypothesis of diagonalizability may seem too constraining, but there is this
curious aphorism that â€œnumerically every matrix is diagonalizable.â€ The reason is as
follows: once you perform store and numerical calculations on the entries of
A, you
perturb them a small essentially random amount. This has the effect of perturbing the
eigenvalues of the calculated
A a small random amount. Thus, the probability that any
two eigenvalues of
A are numerically equal is quite small. To focus matters, consider
the test matrix
A
=
2
4
 8
 5
8
6
3
 8
 3
1
9
3
5
Just for the record, the actual eigenvalues of
A are
 2;
1 and
5 (see Exercise 1 for an
explanation). Now we ask three questions about
A
:
(1) How can we get a ballpark estimate of the location of the eigenvalues of
A?
(2) How can we estimate the dominant eigenpair (;
x) of
A? (Dominant means that

is larger in absolute value than any other eigenvalue of
A.
(3) Given a good estimate of any eigenvalue
 of
A; how can we improve the estimate
and compute a corresponding eigenvector?
One answer to question (1) is the following theorem, which predates modern numerical
analysis, but has proved to be quite useful. Because it helps locate eigenvalues, it is
called a â€œlocalization theorem.â€
THEOREM 5.7.1.
Let
A
=
[a
ij
] be an
n

n matrix and deï¬ne disks in the complex
Gershgorin
Circle Theorem
plane by
r
j
=
n
X
k
=
1
k
6=
j
ja
j
k
j
C
j
=
f
z
j
jz
 r
j
j

ja
j
j
j
Then
1. Every eigenvalue of
A is contained in some disk
C
j
:
2. If
k of the disks are disjoint from the others, then exactly
k eigenvalues are
contained in the union of these disks.
PROOF. To prove 1, let
 be an eigenvalue of
A and
x
=
(x
1
;
x
2
;
:
:
:
;
x
n
) be
an eigenvector corresponding to
: Suppose that
x
j is the largest coordinate of
x in
absolute value. Divide
x by this entry to obtain an eigenvector whose largest coordinate
is
x
j
=
1. Without loss of generality, this vector is
x. Consider the
jth entry of the zero
vector
x
 Ax which is
(
 a
j
)1
+
n
X
k
=
1
k
6=
j
a
j
k
x
k
=
0:

252
5. THE EIGENVALUE PROBLEM
x
y
-8
9
3
13
14
4
FIGURE 5.7.1. Gershgorin circles for
A:
Bring the sum to the right hand side and take absolute values to obtain
j
 a
j
j
=
j
n
X
k
=
1
k
6=
j
a
j
k
x
k
j

n
X
k
=
1
k
6=
j
j
a
j
k
j
j
x
k
j

r
j
since each
j
x
k
j
1: This shows that

2
C
j which proves 1. We will not prove 2, as it
requires some complex analysis.
EXAMPLE 5.7.2. Apply the Gershgorin circle theorem to
A and sketch the resulting
Gershgorin disks.
SOLUTION. The circles are easily seen to be
C
1
=
f
z
j
jz
+
8j

13g
C
2
=
f
z
j
jz
 3j

14g
C
3
=
f
z
j
jz
 9j

4g
A sketch of them is provided in Figure 5.7.1.
Now we turn to question (2). One answer to it is contained in the following algorithm,
known as the power method.
Power Method: To compute an approximate eigenpair
(;x) of
A with
kxk
=
1 and

the dominant eigenvalue.
1. Input an initial guess
x
0 for
x
2. For
k
=
0;
1;
:
:
: until convergence of

(k
)â€™s:
(a)
y
=
Ax
k
(b)
x
k
+1
=
y
k
y
k

5.7. *COMPUTATIONAL NOTES AND PROJECTS
253
(c)

(k
+1)
=
x
T
k
+1
Ax
k
+1
Thatâ€™s all there is to it! Why should this algorithm converge? The secret to this al-
gorithm lies in a formula we saw earlier in our study of discrete dynamical systems,
namely Equation 5.3.2 which we reproduce here
x
(k
)
=
A
k
x
(0)
=
c
1

k
1
v
1
+
c
2

k
2
v
2
+



+
c
n

k
n
v
n
:
Here it is understood that
v
1
;
v
2
;
:
:
:
;
v
n is a basis of eigenvectors corresponding to
eigenvalues

1

2
;
:
:
:
;

n
; which, with no loss of generality, we can assume to be unit
length vectors. Notice that at each stage of the power method we divided the com-
puted iterate
y by its length to get the next
x
k+1
; and this division causes no direc-
tional change. Thus we would get exactly the same vector if we simply set
x
k
+1
=
x
(k
+1)
=


x
(k
+1)

. Now for large
k the ratios
(
j
=
1
)
k can be made as small as we
please, so we can rewrite the above equation as
x
(k
)
=
A
k
x
(0)
=

k
1
(
c
1
v
1
+
c
2


2

1

k
v
2
+



+
c
n


n

1

k
v
n
)


k
1
c
1
v
1
:
Assuming that
c
1
6=
0; which is likely if
x
0 is randomly chosen, we see that
x
k
+1
=
Ax
(k
)


Ax
(k
)




k
1
c
1

1
v
1



k
1
c
1

1


=
v
1

(k
+1)
=
x
T
k
+1
Ax
k
+1

(v
1
)
T
A(v
1
)
=

1
Thus we see that the sequence of

(k
)â€™s converges to

1 and the sequence of
x
kâ€™s con-
verges to
v
1
: The argument (it isnâ€™t rigorous enough to be called a proof) we have just
given shows that the oscillation in sign in
x
koccurs in the case

<
0: You might notice
also that the argument doesnâ€™t require the initial guess to be a real vector. Complex
vectors are permissible.
If we apply the power method to our test problem with an initial guess of
x
0
=
(1;
1;
1),
we get every third value as follows:
k

(k
)
x
k
0
(1;
1;
1)
3
5:7311
(0:54707;
 0:57451;
0:6
08
81
)
6
4:9625
(0:57890;
 0:57733;
0:5
75
81
)
9
5:0025
(0:57725;
 0:57735;
0:5
77
45
)
12
4:9998
(0:57736;
 0:57735;
0:5
77
34
)
Notice, that the eigenvector looks a lot like a multiple of
(1;
 1;
1) and the eigenvalue
looks a lot like
5: This is an exact eigenpair, as one can check.
Finally, we turn to question (3). One answer to it is contained in the following algorithm,
known as the inverse iteration method.
Inverse Iteration Method: To compute an approximate eigenpair
(;x) of
A with
kxk
=
1:
1. Input an initial guess
x
0 for
x and a close approximation

=

0 to
:
2. For
k
=
0;
1;
:
:
: until convergence of

(k
)â€™s:
(a)
y
=
(A
 I
)
 1
x
k

254
5. THE EIGENVALUE PROBLEM
(b)
x
k
+1
=
y
k
y
k
(c)

(k
+1)
=
x
T
k
+1
Ax
k
+1
Notice that the inverse iteration method is simply the power method applied to the ma-
trix
(A
 I
)
 1
: In fact, it is sometimes called the inverse power method. The scalar
 is called a shift. Here is the secret of success for this method: we assume that
 is
closer to a deï¬nite eigenvalue
 of
A than to any other eigenvalue. But we donâ€™t want
too much accuracy! We need

6=
. Theorem 5.1.5 in Section 1 of this chapter shows
that the eigenvalues of the matrix
A
 I are of the form

  where
 runs over the
eigenvalues of
A: Thus the matrix
A
 I is nonsingular since no eigenvalue is zero,
and Exercise 11 shows us that the eigenvalues of
(A
 I
)
 1 are of the form
1=(
 )
where
 runs over the eigenvalues of
A: Since
 is closer to
 than to any other eigen-
value of
A; the eigenvalue
1=(
 ) is the dominant eigenvalue of
(A
 I
)
 1
; which
is exactly what we need to make the power method work on
(A
 I
)
 1
: Indeed, if

is very close (but not equal!) to
 convergence should be very rapid.
In a general situation, we could now have the Gershgorin circle theorem team up with
inverse iteration. Gershgorin would put us in the right ballpark for values of
 and
inverse iteration would ï¬nish the job. Letâ€™s try this with our test matrix and choices of

in the interval
[ 21;
17] suggested by Gershgorin. Letâ€™s try

=
0: Here are the results
in tabular form.
k

(k
)
x
k with

=
0:0
0
0:0
(1;
1;
1)
3
0:77344
( 0:67759;
0:65817;
 0
:32
81
5)
6
1:0288
( 0:66521;
0:66784;
 0
:33
39
1)
9
0:99642
( 0:66685;
0:66652;
 0
:33
32
6)
12
1:0004
( 0:66664;
0:66668;
 0
:33
33
4)
It appears that inverse iteration is converging to

=
1 and the eigenvector looks suspi-
ciously like a multiple of
( 2;
2;
 1): This is in fact an exact eigenpair.
There is much more to modern eigenvalue algorithms than we have indicated here. Cen-
tral topics include deï¬‚ation, the QR algorithm, numerical stability analysis and many
other issues. The interested reader might consult more advanced text such as references
such as [5], [4], [8] or [3], to name a few.
Project Topics
Project: Solving Polynomial Equations
In homework problems we solve for the roots of the characteristic polynomial in order
to get eigenvalues. To this end we can use algebra methods or even Newtonâ€™s method
for numerical approximations to the roots. This is the conventional wisdom usually
proposed in introductory linear algebra. But for larger problems than the simple
2

2
or
3

3 matrices we encounter, this method can be too slow and inaccurate. In fact,
numerical methods hiding under the hood in a MAS (and some CASs) for ï¬nding eigen-
values are so efï¬cient that it is better to turn this whole procedure on its head. Rather

5.7. *COMPUTATIONAL NOTES AND PROJECTS
255
than ï¬nd roots to solve linear algebra (eigenvalue) problems, we can use (numerical)
linear algebra to ï¬nd roots of polynomials. In this project we discuss this methodology
and document it in a fairly nontrivial example.
Given a polynomial
f
(x)
=
c
0
+
c
1
x
+



+
c
n 1
x
n 1
+
x
n, form the companion
matrix of
f
(x)
C
(f
)
=
2
6
6
6
6
6
4
0
1
0
:
:
:
0
0
0
1



0
...
...
...
...
...
0
0



0
1
 c
0
 c
1



 c
n 2
 c
n 1
3
7
7
7
7
7
5
It is a key fact that the eigenvalues of
C
(f
) are precisely the roots of the equation
f
(x)
=
0: Experiment with
n
=
2;
3;
4 and try to ï¬nd a proof by expansion across the
bottom row of
det(A
 I
) that this result is true for all
n:
Then use a CAS (or MAS) to illustrate this method by ï¬nding approximate roots of
three polynomials: a cubic and quartic of your choice and then the polynomial
f
(x)
=
5
+
11x
+
4x
2
+
6x
3
+
x
4
 15x
5
+
5x
6
 3x
7
 2x
8
+
8x
9
 5x
10
+
x
11
In each case use Newtonâ€™s method to improve the values of some of the roots (it works
with complex numbers as well as reals, provided one starts close enough to a root.)
Check your answers to this problem by evaluating the polynomial. Use your results to
write the polynomial as a product of the linear factors
x
 ; where
 is a root and check
the correctness of this factorization.
Project: Finding a Jordan Canonical Form A challenge: Find the Jordan canon-
ical form of this matrix, which is given exactly as follows. The solution will require
some careful work with a CAS or (preferably) MAS.
A
=
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
1
1
1
 2
1
 1
2
 2
4
 3
 1
2
3
 4
2
 2
4
 4
8
 6
 1
0
5
 5
3
 3
6
 6
12
 9
 1
0
3
 4
4
 4
8
 8
16
 12
 1
0
3
 6
5
 4
10
 10
20
 15
 1
0
3
 6
2
 2
12
 12
24
 18
 1
0
3
 6
2
 5
15
 13
28
 21
 1
0
3
 6
2
 5
15
 11
32
 24
 1
0
3
 6
2
 5
15
 14
37
 26
 1
0
3
 6
2
 5
15
 14
36
 25
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Your main task is to devise a strategy for identifying the Jordan canonical form matrix
J
: Do not expect to ï¬nd the invertible matrix
S for which
J
=
S
 1
AS: However, a
key fact to keep in mind is that if
A and
B are similar matrices, i.e.,
A
=
S
 1
B
S for
some invertible
S; then
rank
A
=
rank
B
: In particular, if
S is a matrix that puts
A into
Jordan canonical form, then
J
=
S
 1
AS:
First prove this rank fact for
A and
B
: Show it applies to
A
 cI and
B
 cI as well,
for any scalar
c: Then extend it to powers of
A and
B
:

256
5. THE EIGENVALUE PROBLEM
Now you have the necessary machinery for determining numerically the Jordan canon-
ical form. As a ï¬rst step, one can use a CAS or MAS to ï¬nd the eigenvalues of
A: Of
course, these will only be approximate, so one has to decide how many eigenvalues are
really repeated.
Next, one has to determine the number of Jordan blocks of a given type. Suppose
 is
an eigenvalue and ï¬nd the rank of various powers of
A
 I
: It would help greatly in
understanding how all this counts blocks if you ï¬rst experiment with a matrix already
in Jordan canonical form, say, for example,
J
=
2
4
J
1
(2)
0
0
0
J
2
(2)
0
0
0
J
1
(3)
3
5
:
Project: Classiï¬cation of Quadratic Forms
Recall from calculus that in order to classify all quadratic equations in x and y one went
through roughly three steps. First, do a rotation transformation of coordinates to get rid
of mixed terms like, say,
2xy in the quadratic equation
x
2
+
2xy
 y
2
+
x
 3y
=
4:
Second, do a translation of coordinates to put the equation in a â€œstandard form.â€ Third,
identify the curve by your knowledge of the shape of a curve in the given standard
form. Standard forms were equations like
x
2
=4
+
y
2
=2
=
1: Also recall from your
calculus study of the conics that it was the second degree terms alone that determined
the nature of a quadratic. For example, the second degree terms of the equation above
are
x
2
+
2xy
 y
2
: The discriminant of the equation was determined by these terms. In
this case the discriminant is 8, which tells us that the curve represented by this equation
is a hyperbola. Finally, recall that when it came to quadric equations, i.e., quadratic
equations in 3 unknowns, your text simply provided some examples in â€œstandard formâ€
(six of them to be exact) and maybe mumbled something about this list being essentially
all surfaces represented by quadric equations.
Now you are ready for the rest of the story. Just as with curves in x and y, the basic
shape of the surface of a quadric equation in x, y and z is determined by the second
degree terms. Since this is so, we will focus on an example with no ï¬rst degree terms,
namely,
Q(x;
y
;
z
)
=
2x
2
+
4y
2
+
6z
2
 4xy
 2xz
+
2y
z
=
1:
The problem is simply this: ï¬nd a change of coordinates that will make it clear which
of the six standard forms is represented by this surface.
Here is how to proceed:
ï¬rst you must express the so-called â€œquadratic formâ€
Q(x;
y
;
z
) in matrix form as
Q(x;
y
;
z
)
=
[x;
y
;
z
]A[x;
y
;
z
]
T
: It is easy to ï¬nd such matrices
A: But any such
A
wonâ€™t do. Next, you must replace
A by the equivalent matrix
(A
+
A
T
)=2: (Check
that if
A speciï¬es the quadratic form
Q, then so will
(A
+
A
T
)=2:) The advantage of
this latter matrix is that it is symmetric. Now our theory of symmetric matrices can be
brought to bear. In particular, we know that there is an orthogonal matrix
P such that
P
T
AP is diagonal, provided
A is symmetric. So make the linear change of variables
[x;
y
;
z
]
T
=
P
[x
0
;
y
0
;
z
0
]
T and get that
Q(x;
y
;
z
)
=
[x
0
;
y
0
;
z
0
]P
T
AP
[x
0
;
y
0
;
z
0
]
T
: But
when the matrix in the middle is diagonal, we end up with squares of
x
0,
y
0 and
z
0, and
no mixed terms.
Use the computer algebra system available to you to calculate a symmetric
A and to
ï¬nd the eigenvalues of this
A: From this data alone you will be able to classify the

5.7. *COMPUTATIONAL NOTES AND PROJECTS
257
surface represented by the above equation. Also ï¬nd unit length eigenvectors for each
eigenvalue. Put these together to form the desired orthogonal matrix
P which eliminates
mixed terms.
An outstanding reference on this topic and many others relating to matrix analysis is the
recently republished textbook [1] by Richard Bellman which is widely considered to be
a classic in the ï¬eld.
A Report Topic: Management of Sheep Populations
Description of the problem: You are working for the New Zealand Department of Agri-
culture on a project for sheep farmers. The species of sheep that these shepherds raise
have a life-span of 12 years. Of course, some live longer but they are sufï¬ciently few in
number and their reproductive rate is so low that they may be ignored in your popula-
tion study. Accordingly, you divide sheep into 12 age classes, namely those in the ï¬rst
year of life, etc. You have conducted an extensive survey of the demographics of this
species of sheep and obtained the following information about the demographic param-
eters
a
i and
b
i, where
a
i is the reproductive rate for sheep in the
ith age class and
b
i is
the survival rate for sheep in that age class, i.e., the fraction of sheep in that age class
that survive to the
i
+
1th class. (As a matter of fact, this table is related to real data.
The interested reader might consult the article [2] in the bibliography.)
i
1
2
3
4
5
6
7
8
9
10
11
12
a
i
.000
.023
.145
.236
.242
.273
.271
.251
.234
.229
.216
.210
b
i
.845
.975
.965
.950
.926
.895
.850
.786
.691
.561
.370
-
The problem is as follows: in order to maintain a constant population of sheep, shep-
herds will harvest a certain number of sheep each year. Harvesting need not mean
slaughter; it can be accomplished by selling animals to other shepherds, for example. It
simply means removing sheep from the population. Denote the fraction of sheep which
are removed from the
ith age group at the end of each growth period (a year in our case)
by
h
i
: If these numbers are constant from year to year, they constitute a harvesting
policy. If, moreover, the yield of each harvest, i.e., total number of animals harvested
each year, is a constant and the age distribution of the remaining populace is essentially
constant after each harvest, then the harvesting policy is called sustainable. If all the
h
iâ€™s are the same, say
h, then the harvesting policy is called uniform. An advantage
of uniform policies is that they are simple to implement: One selects the sheep to be
harvested at random.
Your problem: Find a uniform sustainable harvesting policy to recommend to shep-
herds, and ï¬nd the resulting distribution of sheep that they can expect with this policy.
Shepherds who raise sheep for sale to markets are also interested in a sustainable pol-
icy that gives a maximum yield. If you can ï¬nd such a policy that has a larger annual
yield than the uniform policy, then recommend it. On the other hand, shepherds who
raise sheep for their wool may prefer to minimize the annual yield. If you can ï¬nd a
sustainable policy whose yield is smaller than that of the uniform policy, make a recom-
mendation accordingly. In each case ï¬nd the expected distribution of your harvesting
policies. Do you think there are optimum harvesting policies of this type? Do you

258
5. THE EIGENVALUE PROBLEM
think that there might be other economic factors that should be taken into account in
this model? Organize your results for a report to be read by your supervisor and an
informed public.
Procedure: Express this problem as a discrete linear dynamical system
x
k
+1
=
Lx
k,
where
L is a so-called Leslie matrix of the form
L
=
2
6
6
6
6
6
4
a
1
a
2
a
3

a
n 1
a
n
b
1
0
0



0
0
0
b
2
0



0
0
...
...
...
...
...
...
0
0
0



b
n 1
0
3
7
7
7
7
7
5
:
It is understood that the
0
<
b
i

1,
0

a
i and at least one
a
i
6=
0: The facts you
need to know (and may assume as standard facts about Leslie matrices) are as follows:
such a matrix will have exactly one positive eigenvalue which turns out to be a simple
eigenvalue (not repeated). Moreover, if at least two adjacent entries of the ï¬rst row
are positive, this eigenvalue will be a dominant eigenvalue, i.e., it is strictly larger than
any other eigenvalue in absolute value. In particular, if the positive eigenvalue is 1,
then starting from any nonzero initial state with nonnegative entries, successive states
converge to an eigenvector belonging to the eigenvalue 1 which has all nonnegative
entries. Scale this vector by dividing it by the sum of its components and one obtains
an eigenvector which is a probability distribution vector, i.e., its entries are nonnegative
and sum to 1. The entries of this vector give the long term distribution of the population
in the various age classes.
In regards to harvesting, let
H be a diagonal matrix with the harvest fractions
h
i down
the diagonal. (Here
0

h
i

1:) Then the population that results from this harvesting
at the end of each period is given by
x
k
+1
=
Lx
k
 H
Lx
k
=
(I
 H
)Lx
k
: But the
matrix
(I
 H
)L is itself a Leslie matrix, so the theory applies to it as well. There are
other theoretical tools, but all you need to do is to ï¬nd a matrix
H
=
hI so that 1 is
the positive eigenvalue of
(I
 H
)L: You can do this by trial and error, a method which
is applicable to any harvesting policy, uniform or not. However, in the case of uniform
policies itâ€™s simpler to note that
(I
 H
)L
=
(1
 h)L, where
h is the diagonal entry
of
H
:
Implementation Notes: (To the instructor: Add local notes here and discuss available
aids. For example, when I give this assignment under Maple or Mathematica, I create
a notebook that has the correct vector of
a
iâ€™s and
b
iâ€™s in it to avoid a very common
problem: data entry error.)
5.7 Exercises
1. Let
D
=
2
4
 2
0
0
0
1
0
0
0
5
3
5 and
M
=
2
4
1
1
0
0
1
1
0
0
1
3
5
: Compute
P
=
M
T
M and
A
=
P
 1
D
P
: Explain why the eigenvalues of
A are
 2;
1;
5. Also explain why, given
the form of
M, we know that
P
 1 is sure to have integer entries before we even calcu-
late
P
 1
:

REVIEW
259
2. A square matrix is said to be strictly diagonally dominant if in each row the sum of
the absolute values of the off-diagonal entries is strictly less than the absolute value of
the diagonal entry. Show that a strictly diagonally dominant matrix is invertible. Hint:
Use Gershgorin to show that
0 is not an eigenvalue of the matrix.
3. The matrix
A below may have complex eigenvalues.
A
=
2
6
6
4
1
 2
 2
0
6
 7
21
 18
4
 8
22
 18
2
 4
13
 13
3
7
7
5
Use the Gerschgorin circle theorem to locate eigenvalues and the iteration methods of
this section to compute an approximate eigensystem.
Review
Chapter 5 Exercises
1. Find the characteristic polynomial and eigenvalues of matrix
A
=
2
4
2
1
0
0
1
 1
0
2
4
3
5
:
2. For the matrix
A
=
2
4
1
3
3
0
5
4
0
0
1
3
5 ï¬nd a matrix
P and a diagonal matrix
D such
that
P
 1
AP
=
D
: (You do not have to ï¬nd
P
 1).
3. Given that a
5

5 has only one eigenvalue
; what are the possible Jordan canonical
forms of the matrix?
4. Answer True/False:
(a) The matrix

2
1
0
2

is diagonalizable.
(b) The matrix

2
1
0
1

is diagonalizable.
(c) Every eigenvalue of the matrix
A is nonzero.
(d) Every real matrix is similar to a diagonal matrix.
(e) If
A is diagonalizable, then
A
T is diagonalizable.
5. Verify directly from deï¬nition that if
 is an eigenvalue of
A, then

+
1 is an
eigenvalue of
A
+
I
:

260
5. THE EIGENVALUE PROBLEM
6. Find two matrices
A;
B such that the only eigenvalues of
A and
B are
0 but
AB has
nonzero eigenvalues.
7. Let
A
=
ab
T where
a and
b are
n

1 column vectors. Find all eigenvalues and
eigenvectors for
A:
8. Show that if
0 is an eigenvalue of
A, then
A is not invertible
9. A matrix
A is called normal if
A
H
A
=
AA
H
: Prove that every Hermitian symmetric
matrix is normal.
10. Let
A
=

1
1
+
i
1
 i
2

: One of the eigenvalues of
A is
0:
(a) Use the trace (Exercise 4) to ï¬nd the other eigenvalue.
(b) Find eigenvectors for each eigenvalue.
(c) Unitarily orthogonalize the matrix
A:
11. Show that if the matrix
A is diagonalizable and has only one eigenvalue (repeated,
of course), then
A is a scalar matrix.
12. Show that if the matrix
A is unitarily diagonalizable, then so is
A
H
: Prove or dis-
prove that the same is true for
A
T

CHAPTER 6
GEOMETRICAL ASPECTS OF ABSTRACT SPACES
Two basic ideas that we learn in geometry are that of length of a line segment and angle
between lines. We have already seen how to extend the ideas to the standard vector
spaces. The objective of this chapter is to extend these powerful ideas to general linear
spaces. A surprising number of concepts and techniques that we learned in a standard
setting can be carried over, almost word for word, to more general vector spaces. Once
this is accomplished, we will be able to use our geometrical intuition in entirely new
ways. For example, we will be able to have notions of length and perpendicularity for
nonstandard vectors such as functions in a function space. Another application is that
we will be able to give a sensible meaning to the size of the error incurred in solving
a linear system with ï¬nite precision arithmetic. There are many more uses for this
abstraction, as we shall see.
6.1. Normed Linear Spaces
Deï¬nitions and Examples
The basic function of a norm is to measure length and distance, independent of any other
considerations, such as angles or orthogonality. There are different ways to accomplish
such a measurement. One method of measuring length might be more natural for a
given problem, or easier to calculate than another. For these reasons, we would like to
have the option of using different methods of length measurement. You may recognize
the properties listed below from earlier in the text; they are the basic norm laws given in
Section 4.1 of Chapter 4 for the standard norm. We are going to abstract the norm idea
to arbitrary vector spaces.
DEFINITION 6.1.1.
A norm on the vector space
V is a function
jj
jj which assigns
Norm
Properties
to each vector
v
2
V a real number
jjv
jj such that for
c a scalar and
u;
v
2
V the
following hold:
1.
jj
u
jj

0 with equality if and only if
u
=
0:
2.
jj
cu
jj
=
j
c
j
jj
u
jj
3. (Triangle Inequality)
jj
u
+
v
jj

jj
u
jj
+
jj
v
jj
DEFINITION 6.1.2. A vector space
V , together with a norm
jjjj on the space
V , is
called a normed linear space.
261

262
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
Notice that if
V is a normed linear space and
W is any subspace of
V , then
W automat-
ically becomes a normed linear space if we simply use the norm of
V on elements of
W
: For obviously all the norm laws still hold, since they hold for elements of the bigger
space
V
:
Of course, we have already studied some very important examples of normed linear
spaces, namely the standard vector spaces
R
n and
C
n, or any subspace thereof, together
with the standard norms given by
jj(z
1
;
z
2
;
:
:
:
;
z
n
)j
j
=
p
z
1
z
1
+
z
2
z
2
+



+
z
n
z
n
=

jz
1
j
2
+
jz
2
j
2
+



+
jz
n
j
2

1=2
If the vectors are real then we can drop the conjugate bars. This norm is actually one of
a family of norms which are commonly used.
DEFINITION 6.1.3. Let
V be one of the standard spaces
R
n or
C
n and
p

1 a real
number. The
p-norm of a vector in
V is deï¬ned by the formula
p-Norms
jj(z
1
;
z
2
;
:
:
:
;
z
n
)j
j
p
=
(jz
1
j
p
+
jz
2
j
p
+



+
jz
n
j
p
)
1=p
Notice that when
p
=
2 we have the familiar example of the standard norm. Another
important case is that in which
p
=
1 which gives (not surprisingly) the so-called
1-
norm. The last important instance of a
p-norm is one that isnâ€™t so obvious:
p
=
1: It
turns out that the value of this norm is the limit of
p-norms as
p
!
1: To keep matters
simple, weâ€™ll supply a separate deï¬nition for this norm.
DEFINITION 6.1.4. Let
V be one of the standard spaces
R
n or
C
n
: The
1-norm of a
vector in
V is deï¬ned by the formula
jj(z
1
;
z
2
;
:
:
:
;
z
n
)j
j
1
=
max
fjz
1
j
;
jz
2
j
;
:
:
:
;
jz
n
jg
EXAMPLE 6.1.5. Calculate
jjv
j
j
p where
p
=
1;
2 or
1 and
v
=
(1;
 3;
2;
 1)
2R
4
:
SOLUTION. We calculate:
jj(1;
 3;
2;
 1)j
j
1
=
j1j
+
j 3j
+
j2j
+
j 1j
=
7
jj(1;
 3;
2;
 1)j
j
2
=
q
j1j
2
+
j 3j
2
+
j2j
2
+
j 1j
2
=
p
15
jj(1;
 3;
2;
 1)j
j
1
=
max
fj1j
;
j 3j
;
j2j
;
j 1j
g
=
3
It may seem a bit odd at ï¬rst to speak of the same vector as having different lengths. You
should take the point of view that choosing a norm is a bit like choosing a measuring
stick. If you choose a yard stick, you wonâ€™t measure the same number as you would by
using a meter stick on the same object.
EXAMPLE 6.1.6. Verify that the norm properties are satisï¬ed for the
p-norm in the case
that
p
=
1:
SOLUTION. Let
c be a scalar,
u
=
(z
1
;
z
2
;
:
:
:
;
z
n
) and
v
=
(w
1
;
w
2
;
:
:
:
;
w
n
) two
vectors. Any absolute value is nonnegative and any vector whose largest component in

6.1. NORMED LINEAR SPACES
263
absolute value is zero must have all components equal to zero. Property (1) follows.
Next, we have that
jjcujj
1
=
jj(cz
1
;
cz
2
;
:
:
:
;
cz
n
)jj
1
=
max
fjcz
1
j
;
jcz
2
j
;
:
:
:
;
jcz
n
jg
=
jcj
max
fjz
1
j
;
jz
2
j
;
:
:
:
;
jz
n
jg
=
jcj
jjujj
1
which proves (2). For (3) we observe that
jju
+
v
j
j
1
=
max
fjz
1
j
+
jw
1
j
;
jz
2
j
+
jw
2
j
;
:
:
:
;
jz
n
j
+
jw
n
jg

max
fjz
1
j
;
jz
2
j
;
:
:
:
;
jz
n
jg
+
max
f
jw
1
j
;
jw
2
j
;
:
:
:
;
jw
n
jg

jjujj
1
+
jjv
j
j
1
Unit Vectors
Sometimes it is convenient to deal with vectors whose length is one. Such a vector is
called a unit vector. We saw in Chapter 3 that it is easy to concoct a unit vector
u in the
same direction as a given nonzero vector
v when using the standard norm, namely take
u
=
v
jjv
j
j
(6.1.1)
The same formula holds for any norm whatsoever because of norm property (2).
EXAMPLE 6.1.7. Construct a unit vector in the direction of
v
=
(1;
 3;
2;
 1), where
the
1-norm,
2-norm, and
1-norms are used to measure length.
SOLUTION. We already calculated each of the norms of
v in Example 6.1.5. Use these
numbers in Equation 6.1.1 to obtain unit length vectors
u
1
=
1
7
(1;
 3;
2;
 1)
u
2
=
1
p
15
(1;
 3;
2;
 1)
u
1
=
1
3
(1;
 3;
2;
 1)
From a geometric point of view there are certain sets of vectors in the vector space
V
that tell us a lot about distances. These are the so-called balls about a vector (or point)
v
0 of radius
r whose deï¬nition is as follows:
B
r
(v
0
)
=
f
v
2
V
j
jjv
 v
0
jj

r
g
Sometimes these are called closed balls, as opposed to open balls which are deï¬ned by
using strict inequality. Here is a situation in which these balls are very helpful: imagine
trying to ï¬nd the distance from a given vector
v
0 to a closed (this means it contains
all points on its boundary) set
S of vectors which need not be a subspace. One way to
accomplish this is to start with a ball centered at
v
0 such that the ball avoids
S: Then
expand this ball by increasing its radius until you have found a least radius
r such that
the ball
B
r
(v
0
) intersects
S nontrivially. Then the distance from
v
0 to this set is this

264
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
number
r
: Actually, this is a reasonable deï¬nition of the distance from
v
0 to the set
S:
One expects these balls, for a given norm, to have the same shape, so it is sufï¬cient to
look at the unit balls, that is, the case where
r
=
1:
EXAMPLE 6.1.8. Sketch the unit balls centered at the origin for the
1-norm,
2-norm,
and
1-norms in the space
V
=R
2
:
SOLUTION. In each case itâ€™s easiest to determine the boundary of the ball
B
1
(0), i.e.,
the set of vectors
v
=
(x;
y
) such that
jjv
j
j
=
1: These boundaries are sketched in
Figure 6.1.1 and the ball consists of the boundaries plus the interior of each boundary.
Letâ€™s start with the familiar norm
2-norm. Here the boundary consists of points
(x;
y
)
such that
1
=
jj(x;
y
)j
j
2
=
x
2
+
y
2
which is the familiar circle of radius
1 centered at the origin. Next, consider the
1-norm
in which case
1
=
jj(x;
y
)j
j
1
=
jxj
+
jy
j
Itâ€™s easier to examine this formula in each quadrant, where it becomes one of the four
possibilities
x

y
=
1
For example, in the ï¬rst quadrant we get
x
+
y
=
1: These equations give lines which
connect to from a square whose sides are diagonal lines. Finally, for the
1-norm we
have
1
=
jj(x;
y
)j
j
1
=
max
fjxj
;
jy
jg
which gives four horizontal and vertical lines
x
=
1 and
y
=
1 which intersect to
form another square. Thus we see that the unit â€œballsâ€ for the
1- and
1-norms have
corners, unlike the
2-norm. see Figure 6.1.1 for a picture of these balls.
One more comment about norms.
Recall from Section 4.1 that one of the impor-
tant applications of the norm concept is that it enables us to make sense out of the
idea of limits of vectors.
In a nutshell
lim
n!1
v
n
=
v was taken to mean that
lim
n!1
jjv
n
 v
j
j
=
0: Will we have to have a different notion of limits for dif-
Limit of Vectors
ferent norms? The somewhat surprising answer is â€œno.â€ The reason is that given any
two norms
jjjj
a and
jjjj
b on a ï¬nite dimensional vector space, it is always possible to
ï¬nd positive real constants
c and
d such that for any vector
v
jjv
jj
a

c

jjv
j
j
b and
jjv
j
j
b

d

jjv
jj
a
Hence, if
jjv
n
 v
j
j tends to
0 in one norm, it will tend to
0 in the other norm. In this
sense, it can be shown that all norms on a ï¬nite dimensional vector space are equivalent.
Indeed, it can be shown that the condition that
jjv
n
 v
j
j tends to
0 in any one norm
is equivalent to the condition that each coordinate of
v
n converge to the corresponding
coordinate of
v
: We will verify the limit fact in the following example.
EXAMPLE 6.1.9. Verify that
lim
n!1
v
n exists and is the same with respect to both the
1-norm and
2-norm, where
v
n
=

(1
 n)=n
e
 n
+
1


6.1. NORMED LINEAR SPACES
265
|| v|| = 1
1
|| v|| = 1
3
|| v|| = 1
2
x
y
FIGURE 6.1.1. Boundaries of unit balls in various norms.
Which norm is easier to work with?
SOLUTION. First we have to know what the limit will be. Letâ€™s examine the limit in
each coordinate. We have
lim
n!1
1
 n
n
=
lim
n!1
1
n
 1
=
0
 1
=
 1
and
lim
n!1
e
 n
+
1
=
0
+
1
=
1:
So we will try to use
v
=
( 1;
1) as the limiting vector. Now calculate
v
 v
n
=

 1
1

 
1 n
n
e
 n
+
1

=

1
n
e
 n

so that
jjv
 v
n
jj
1
=
j
1
n
j
+
je
 n
j
 !
n!1
0;
and
jjv
 v
n
jj
=
s

1
n

2
+
(
e
 n
)
2
 !
n!1
0
which shows that the limits are the same in either norm. In this case the
1-norm appears
to be easier to work with since no squaring and square roots are involved.
6.1 Exercises
1. Let
x
=
[1
+
i;
 1;
0;
1]
T and
y
=
[1;
1;
2;
 4]
T be vectors in
C
4
: Find the 1- 2-
and
1- norms of the vectors
x and
y
:
2. Find unit vectors in the direction of
v
=
(1;
 3;
 1) with respect to the
1-,
2-, and
1-norms.

266
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
3. Verify the triangle inequality for
u
=
(0;
2;
3;
1),
v
=
(1;
 3;
2;
 1) and the
1-norm.
4. Verify that all three of the norm laws hold in the case that
c
=
 3;
u
=
(2;
 4;
1);
v
=
(1;
2;
 1) and the norm is the inï¬nity norm.
5. Find the distance from the origin to the line
x
+
y
=
2 using the
1-norm by sketching
a picture of the ball centered at the origin that touches the line.
6. Given the matrix

a
b
c
d

; ï¬nd the largest possible value of
jjAxj
j
1, where
x
ranges over the vectors whose
1-norm is
1:
7. Verify that the
1-norm satisï¬es the deï¬nition of a norm.
8. Verify that
lim
n!1
v
n exists and is the same with respect to both the
1- and
2-norm,
where
v
n
=

(1
 n)=n
e
 n
+
1

9. Calculate
lim
n!1
v
n using the
1-norm, where
v
n
=

n
2
=(n
3
+
1)
sin(n)=(n
3
+
1)

10. An example of a norm on
R
m;n (or
C
m;n
) is the Frobenius norm of an
m

n
matrix
A
=
[a
ij
] is deï¬ned by the formula
kAk
F
=
0
@
m
X
i=1
n
X
j
=1
j
a
ij
j
2
1
A
:
1=2
Compute the Frobenius norm of the following matrices.
(a)
2
4
1
1
0
0
1
1
0
2
2
3
5 (b)

1
1
0
1
1
 2

(c)

1
+
2i
2
1
 3i

11. Show that the Frobenius norm satisï¬es norm properties 1 and 2.
6.2. Inner Product Spaces
Deï¬nitions and Examples
We saw in Section 4.2 that the notion of a dot product of two vectors had many handy
applications, including the determination of the angle between two vectors. This dot
product amounted to the â€œconcreteâ€ inner product of the two standard vectors. We now
extend this idea in a setting that allows for real or complex abstract vector spaces.

6.2. INNER PRODUCT SPACES
267
DEFINITION 6.2.1. An (abstract) inner product on the vector space
V is a function
h;
i
which assigns to each pair of vectors
u;
v
2
V a scalar
h
u;
v
i such that for
c a scalar
and
u;
v
;
w
2
V the following hold:
1.
h
u;
ui

0 with equality if and only if
u
=
0
2.
hu;
v
i
=
hv
;
ui
3.
hu;
v
+
w
i
=
h
u;
v
i
+
hu;
w
i
4.
hu;
cv
i
=
c
h
u;
v
i
DEFINITION 6.2.2. A vector space
V , together with an inner product on the space
V ,
is called an inner product space.
Notice that in the case of the more common vector spaces over real scalars, property 2
becomes a commutativity law:
hu;
v
i
=
hv
;
ui
:
Also observe that if
V is an inner product space and
W is any subspace of
V , then
W
automatically becomes an inner product space if we simply use the inner product of
V
on elements of
W
: For obviously all the inner product laws still hold, since they hold
for elements of the bigger space
V
:
Of course, we have the standard examples of inner products, namely the dot products on
R
n and
C
n
: Here is an example of a nonstandard inner product that is useful in certain
engineering problems.
EXAMPLE 6.2.3. For vectors
u
=
(u
1
;
u
2
) and
v
=
(v
1
;
v
2
) in
V
=
R
2, deï¬ne an
inner product by the formula
hu;
v
i
=
2u
1
v
1
+
3u
2
v
2
Show that this formula satisï¬es the deï¬nition of inner product.
SOLUTION. First we see that
hu;
ui
=
2u
2
1
+
3u
2
2
so the only way for this sum to be
0 is for
u
1
=
u
2
=
0: Hence (1) holds. For (2)
calculate
h
u;
v
i
=
2u
1
v
1
+
3u
2
v
2
=
2v
1
u
1
+
3v
2
u
2
=
hv
;
ui
=
h
v
;
ui
since all scalars in question are real. For (3) let
w
=
(w
1
;
w
2
) and calculate
hu;
v
+
w
i
=
2u
1
(v
1
+
w
1
)
+
3u
2
(v
2
+
w
2
)
=
2u
1
v
1
+
3u
2
v
2
+
2u
1
w
1
+
3u
2
=
hu;
v
i
+
h
u;
w
i
For the last property, check that for a scalar
c
hu;
cv
i
=
2u
1
cv
1
+
3u
2
cv
2
=
c(2u
1
v
1
+
3u
2
v
2
)
=
c
hu;
v
i
It follows that this â€œweightedâ€ inner product is indeed an inner product according to
our deï¬nition. In fact, we can do a whole lot more with even less effort. Consider this
example, of which the preceding is a special case.

268
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
EXAMPLE 6.2.4. Let
V
=R
n or
C
n and let
u;
v
2
V
: Let
A be a ï¬xed
n

n nonsin-
gular matrix. Show that the matrix
A deï¬nes an inner product by the formula
hu;
v
i
=
(Au)
H
(Av
)
=
u
H
A
H
Av
:
SOLUTION. As usual, let
u;
v
;
w
2
V and let
c be a scalar. Use the norm sign for the
ordinary
2-norm and we have
hu;
ui
=
jjAuj
j
2
so that if
hu;
ui
=
0, then
Au
=
0: We are assuming
A is nonsingular, so that this
implies that
u
=
0, which establishes property (1). For (2), remember that for a
1

1
scalar quantity
q,
q
H
=
q so we calculate:
h
v
;
ui
=
v
H
A
H
Au
=
(u
H
A
H
Av
)
H
=
hu;
v
i
H
=
hu;
v
i
Next, we have
hu;
v
+
w
i
=
u
H
A
H
A(v
+
w
)
=
u
H
A
H
Av
+
u
H
A
H
Aw
=
hu;
v
i
+
h
u;
w
i
Finally, we have that
hu;
cv
i
=
u
H
A
H
Acv
=
cu
H
A
H
Av
=
c
hu;
v
i
This shows that the inner product properties are satisï¬ed.
We leave it to the reader to check that if we take
A
=

p
2
0
0
p
3

then the inner product deï¬ned by this matrix is exactly the inner product of Example
6.2.3.
There is an important point to be gleaned from the previous example, namely, that a
given vector space may have more than one inner product on it. In particular,
V
=
R
2
could have the standard inner product â€“ dot products â€“ or something like the previous
example. The space
V , together with each one of these inner products, provide us with
two separate inner product spaces.
Here is a rather more exotic example of an inner product, in that it does not involve one
of standard spaces for its underlying vector space.
EXAMPLE 6.2.5. Let
V
=
C
[0;
1], the space of continuous functions on the interval
[0;
1] with the usual function addition and scalar multiplication. Show that the formula
h
f
;
g
i
=
Z
1
0
f
(x)g
(x)dx
deï¬nes an inner product on the space
V
:

6.2. INNER PRODUCT SPACES
269
SOLUTION. Certainly
h
f
;
g
i is a real number. Now if
f
(x) is a continuous function then
f
(x)
2 is nonnegative on
[0;
1] and therefore
R
1
0
f
(x)
2
dx
=
hf
;
f
i

0: Furthermore, if
f
(x) is nonzero, then the area under the curve
y
=
f
(
x)
2 must also be positive since
f
(x) will be positive and bounded away from
0 on some subinterval of
[0;
1]: This
establishes property (1) of inner products.
Now let
f
(x);
g
(x);
h(x)
2
V
: For property (2), notice that
hf
;
g
i
=
Z
1
0
f
(x)g
(x)dx
=
Z
1
0
g
(x)f
(x)dx
=
h
g
;
f
i
:
Also,
hf
;
g
+
hi
=
Z
1
0
f
(x)(g
(x)
+
h(x))dx
=
Z
1
0
f
(x)g
(x)dx
+
Z
1
0
f
(x)h(x)dx
=
hf
;
g
i
+
hf
;
hi
which establishes property (3). Finally, we see that for a scalar
c
h
f
;
cg
i
=
Z
1
0
f
(x)cg
(x)
dx
=
c
Z
1
0
f
(x)g
(x)
dx
=
c
hf
;
g
i
which shows that property (4) holds.
Clearly we could similarly deï¬ne an inner product on the space
C
[a;
b] of continuous
functions on any ï¬nite interval
[a;
b] just as in the preceding example by changing the
limits of the integrals from
[0;
1] to
[a;
b]: We shall refer to this inner product on a
function space as the standard inner product on a function space.
Following are a few simple facts about inner products that we will use frequently. The
proofs are left to the exercises.
THEOREM 6.2.6. Let
V be an inner product space with inner product
h;
i
: Then we
have that for all
u;
v
;
w
2
V and scalars
a
1.
h0;
ui
=
0
2.
hu
+
v
;
w
i
=
hu;
w
i
+
hv
;
w
i
3.
hau;
v
i
=
ahu;
v
i
Induced Norms and the CBS Inequality
It is a striking fact that we can accomplish all the goals we set for the standard inner
product with general inner products as well: we can introduce the ideas of angles, or-
thogonality, projections and so forth. We have already seen much of the work that has
to be done, though it was stated in the context of the standard inner products. As a ï¬rst
step, we want to point out that every inner product has a â€œnaturalâ€ norm associated with
it.

270
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
DEFINITION 6.2.7. Let
V be an inner product space. For vectors
u
2
V , the norm
deï¬ned by the equation
jjuj
j
=
p
h
u;
ui
is called the norm induced by the inner product
h;
i on
V
:
As a matter of fact, this idea is not really new. Recall that we introduced the standard
Induced Norm
inner product on
V
=
V
=
R
n or
C
n with an eye towards the standard norm. At the
time it seemed like a nice convenience that the norm could be expressed in terms of the
inner product. It is, and so much so that we have turned this cozy relationship into a
deï¬nition. Is the induced norm really a norm? We have some work to do. The ï¬rst norm
property is easy to verify for the induced norm: from property (1) of inner products we
see that
hu;
ui

0, with equality if and only if
u
=
0: This conï¬rms norm property
(1). Norm property (2) isnâ€™t too hard either: let
c be a scalar and check that
jjcujj
=
p
hcu;
cui
=
p
cc
hu;
ui
=
q
jcj
2
p
hu;
ui
=
jcj
jjujj
Norm property (3), the triangle inequality, remains. This one isnâ€™t easy to verify from
ï¬rst principles. We need a tool that we have seen before, the Cauchy-Bunyakovsky-
Schwarz (CBS) inequality. We restate it below as the next theorem. Indeed, the very
same proof that is given in Theorem 4.2.1 of Chapter 3 carries over word for word
to general inner products over real vector spaces. We need only replace dot products
u

v by abstract inner products
hu;
v
i
: Similarly, the proof of the triangle inequality as
given in Example 4.2.4 of Chapter 3, carries over to establish the triangle inequality for
abstract inner products. Hence property (3) of norms holds for any induced norm.
THEOREM 6.2.8. Let
V be an inner product space. For
u;
v
2
V , if we use the inner
CBS Inequality
product of
V and its induced norm, then
j
h
u;
v
i
j

jj
u
jj
jj
v
jj
Just as with the standard dot products, thanks to this inequality, we can formulate the
following deï¬nition.
DEFINITION 6.2.9. For vectors
u;
v
2
V
; an inner product space, we deï¬ne the angle
Angle Between
Vectors
between
u and
v to be any angle
 satisfying
cos

=
hu;
v
i
jj
u
jj
jj
v
jj
We know that
j
hu;
v
i
j=(jj
u
jj
jj
v
jj)

1 so that this formula for
cos
 makes sense.
EXAMPLE 6.2.10. Let
u
=
(1;
 1) and
v
=
(1;
1) be vectors in
R
2
: Compute an angle
between these two vectors using the inner product of Example 6.2.3. Compare this to
angle found when one uses the standard inner product in
R
2
:
SOLUTION. According to 6.2.3 and the deï¬nition of angle, we have
cos

=
hu;
v
i
jjujj

j
jv
jj
=
2

1

1
+
3

( 1)

1
p
2

1
2
+
3

( 1)
2
p
2

1
2
+
3

1
2
=
 1
5
Hence the angle in radians is

=
arccos
(
 1
5
)

1:7722

6.2. INNER PRODUCT SPACES
271
On the other hand, if we use the standard norm then
hu;
v
i
=
1

1
+
( 1)

1
=
0
from which it follows that
u and
v are orthogonal and

=

=2

1:
5708:
In the previous example, it shouldnâ€™t be too surprising that we can arrive at two different
values for the â€œangleâ€ between two vectors. Using different inner products to measure
angle is somewhat like measuring length with different norms. Next, we extend the
perpendicularity idea to arbitrary inner product spaces.
DEFINITION 6.2.11. Two vectors
u and
v in the same inner product space are orthog-
onal if
hu;
v
i
=
0:
Note that if
hu;
v
i
=
0; then
hv
;
ui
=
hu;
v
i
=
0: Also, this deï¬nition makes the
zero vector orthogonal to every other vector. It also allows us to speak of things like
â€œorthogonal functions.â€ One has to be careful with new ideas like this. Orthogonality
in a function space is not something that can be as easily visualized as orthogonality of
geometrical vectors. Inspecting the graphs of two functions may not be quite enough.
If, however, graphical data is tempered with a little understanding of the particular inner
product in use, orthogonality can be detected.
EXAMPLE 6.2.12. Show that
f
(x)
=
x and
g
(x)
=
x
 2=3 are orthogonal elements
of
C
[0;
1] with the inner product of Example 6.2.5 and provide graphical evidence of
this fact.
SOLUTION. According to the deï¬nition of inner product in this space,
hf
;
g
i
=
Z
1
0
f
(x)g
(x)dx
=
Z
1
0
x(x
 2
3
)dx
=
(
x
3
3
 x
2
3
)




1
0
=
0:
It follows that
f and
g are orthogonal to each other. For graphical evidence, sketch
f
(x),
g
(x) and
f
(x)g
(x) on the interval
[0;
1] as in Figure 6.2.1. The graphs of
f and
g are not especially enlightening; but we can see in the graph that the area below
f

g
and above the
x-axis to the right of
(2=3;
0) seems to be about equal to the area to the
left of
(2=3;
0) above
f

g and below the
x-axis. Therefore the integral of the product
on the interval
[0;
1] might be expected to be zero, which is indeed the case.
Some of the basic ideas from geometry that fuel our visual intuition extend very ele-
gantly to the inner product space setting. One such example is the famous Pythagorean
Theorem, which takes the following form in an inner product space.
THEOREM 6.2.13.
Let
u;
v be orthogonal vectors in an inner product space
V
: Then
Pythagorean
Theorem
kuk
2
+
kv
k
2
=
k
u
+
v
k
2
:
PROOF. Compute
k
u
+
v
k
2
=
hu
+
v
;
u
+
v
i
=
hu;
ui
+
hu;
v
i
+
hv
;
ui
+
h
v
;
v
i
=
hu;
ui
+
hv
;
v
i
=
k
uk
2
+
k
v
k
2

272
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
f(x) g(x)
.
1
-1
f(x)=x
x
y
g(x)=x-2/3
1
2/3
FIGURE 6.2.1. Graph of
f,g and
f

g on the interval
[0;
1]:
Here is an example of another standard geometrical fact that ï¬ts well in the abstract
setting. This is equivalent to the law of parallelograms, which says that the sum of the
squares of the diagonals of a parallelogram is equal to the sum of the squares of all four
sides.
EXAMPLE 6.2.14. Use properties of inner products to show that if we use the induced
norm, then
jju
+
v
jj
2
+
jju
 v
j
j
2
=
2

j
jujj
2
+
j
jv
jj
2

SOLUTION. The key to proving this fact is to relate induced norm to inner product.
Speciï¬cally,
ku
+
v
k
2
=
h
u
+
v
;
u
+
v
i
=
hu;
ui
+
hu;
v
i
+
hv
;
ui
+
h
v
;
v
i
while
ku
 v
k
2
=
h
u
 v
;
u
 v
i
=
hu;
ui
 hu;
v
i
 hv
;
ui
+
h
v
;
v
i
Now add these two equations and obtain by using the deï¬nition of induced norm again
that
k
u
+
v
k
2
+
k
u
 v
k
2
=
2
hu;
ui
+
2
hv
;
v
i
=
2(kuk
2
+
k
v
k
2
)
which is what was to be shown.
It would be nice to think that every norm on a vector space is induced from some inner
product. Unfortunately, this is not true, as the following example shows.
EXAMPLE 6.2.15. Use the result of Example 6.2.14 to show that the inï¬nity norm on
V
=
R
2 is not induced by any inner product on
V
:
SOLUTION. Suppose the inï¬nity norm were induced by some inner product on
V
: Let
u
=
(1;
0) and
v
=
(0;
1=2): Then we have
j
ju
+
v
j
j
2
1
+
jju
 v
j
j
2
1
=
jj(1;
1=2)j
j
2
1
+
jj(1;
 1=2)j
j
2
1
=
2

6.2. INNER PRODUCT SPACES
273
while
2

jjuj
j
2
+
jjv
j
j
2

=
2(1
+
1=4)
=
5=2
This contradicts Example 6.2.14, so that the inï¬nity norm cannot be induced from an
inner product.
One last example of a geometrical idea that generalizes to inner product spaces is the
notion of projections of one vector along another. The projection formula for vectors of
Chapter 4 works perfectly well for general inner products. Since the proof of this fact
amounts to replacing dot products by inner products in the original formulation of the
theorem (see page 273), we omit it and simply state the result.
THEOREM 6.2.16.
Let
u and
v be vectors in an inner product space with
v
6=
0:
Projection
Formula for
Vectors
Deï¬ne the projection of
u along
v as
p
=
pro
j
v
u

hv
;
ui
hv
;
v
i
v
and let
q
=
u
 p: Then
p is parallel to
v,
q is orthogonal to
v and
u
=
p
+
q:
Orthogonal Sets of Vectors
We have already seen the development of the ideas of orthogonal sets of vectors and
bases in Chapter 4. Much of this development can be abstracted easily to general inner
product spaces, simply by replacing dot products by inner products. Accordingly, we
can make the following deï¬nition.
DEFINITION 6.2.17. The set of vectors
v
1
;
v
2
;
:
:
:
;
v
n in an inner product space are
said to be an orthogonal set if
h
v
i
;
v
j
i
=
0 whenever
i
6=
j: If, in addition, each vector
has unit length, i.e.,
hv
i
;
v
i
i
=
1 then the set of vectors is said to be an orthonormal set
of vectors.
The proof of the following key fact and its corollary are the same as that of Theo-
rem 6.2.18 in Section 4.3 of Chapter 4. All we have to do is replace dot products by
inner products. The observations that followed the proof of this theorem are valid for
general inner products as well. We omit the proofs and refer the reader to Chapter 4.
THEOREM 6.2.18.
Let
v
1
;
v
2
;
:
:
:
;
v
n be an orthogonal set of nonzero vectors and
Orthogonal
Coordinates
Theorem
suppose that
v
2
spanf
v
1
;
v
2
;
:
:
:
;
v
n
g: Then
v can be expressed uniquely (up to
order) as a linear combination of
v
1
;
v
2
;
:
:
:
;
v
n, namely
v
=
hv
1
;
v
i
hv
1
;
v
1
i
v
1
+
hv
2
;
v
i
hv
2
;
v
2
i
v
2
+
:
:
:
+
hv
n
;
v
i
hv
n
;
v
n
i
v
n
COROLLARY 6.2.19. Every orthogonal set of nonzero vectors is linearly independent.
Another useful corollary is the following fact about length of a vector whose proof is
left as an exercise.

274
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
COROLLARY 6.2.20. If
v
1
;
v
2
;
:
:
:
;
v
n is an orthonormal set of vectors and
v
=
c
1
v
1
+
c
2
v
2
+
:
:
:
+
c
n
v
n
then
jjv
j
j
2
=
c
2
1
+
c
2
2
+
:
:
:
+
c
2
n
EXAMPLE 6.2.21. Turn the orthogonal set
f(1;
 1;
0);
(1;
1;
0);
(0;
0;
1)g into an or-
thonormal set, calculate the coordinates of the vector
v
=
(2;
 1;
 1) with respect to
this orthonormal set and verify the formula just given for the length of
v
:
SOLUTION. From the example we have
v
1
=
(1;
 1;
0);
v
2
=
(1;
1;
0); and
v
3
=
(0;
0;
1): We see that
h
v
1
;
v
1
i
=
2
=
hv
2
;
v
2
i and
hv
3
;
v
3
i
=
1: So set
u
1
=
(1=
p
2
)v
1
;
u
2
=
(1=
p
2
)v
2
; and
u
3
=
v
3 to obtain an orthonormal set of vectors
u
1
;
u
2
;
u
3
: Now the coordinates of
v are easily calculated:
c
1
=
hu
1
;
v
i
=
1
p
2
(
1

2
+
( 1)

( 1)
+
0

( 1))
=
3
p
2
c
2
=
hu
2
;
v
i
=
1
p
2
(
1

2
+
1

( 1)
+
0

( 1))
=
1
p
2
c
3
=
hu
3
;
v
i
=
0

2
+
0

( 1)
+
1

( 1)
=
 1
from which we conclude that
v
=
3
p
2
u
1
+
1
p
2
u
2
 u
3
=
3
2
v
1
+
1
2
v
2
 v
3
Now from deï¬nition we have thatk
v
k
2
=
2
2
+
( 1)
2
+
( 1)
2
=
6 while
c
2
1
+
c
2
2
+
c
2
3
=
9=2
+
1=2
+
1
=
6 as well. This conï¬rms that the length squared of
v is the sum of
squares of the coordinates of
v with respect to an orthonormal basis.
6.2 Exercises
1. Verify the Cauchy-Bunyakovsky-Schwarzinequality for
u
=
(1;
2) and
v
=
(1;
 1)
using the weighted inner product on
R
2 given by
<
(x;
y
);
(w
;
z
)
>=
2xw
+
3y
z
:
2. Find the angle between the vectors
u and
v in the following:
(a)
u and
v in Exercise 1 with the inner product given therein.
(b)
u
=
x and
v
=
x
3 in
V
=
C
[0;
1] with the standard inner product as in Exam-
ple 6.2.5.
3. Which of the following sets of vectors are linearly independent? Orthogonal? Or-
thonormal?
(a)
(1;
 1;
2);
(2;
2;
0) in
R
2 with the standard inner product.
(b)
1;
x;
x
2 as vectors in
C
[ 1;
1] with the standard inner product on the interval
[ 1;
1]:
(c)
1
5
(3;
4);
1
5
(4;
 3) in
R
2 with the standard inner product.
(d)
1,cos
(x);
sin(x) in
C
[ 
;

] with the standard inner product.
(e)
(2;
4);
(1;
0) in
R
2 with inner product (assume it is)
h
x;
y
i
=
x
T

2
 1
 1
2

y
:

6.2. INNER PRODUCT SPACES
275
4. The law
<
[x
1
;
x
2
]
T
;
[y
1
;
y
2
]
T
>=
3x
1
y
1
 2x
2
y
2 fails to deï¬ne an inner product
on
R
2
: Why?
5. If the square real matrix
A has a nonpositive real eigenvalue, then the formula
hu;
v
i
=
u
T
Av does not deï¬ne an inner product. Why? Hint:
Start with the deï¬-
nition of eigenvalue.
6. Show that the law
<
(x
1
;
x
2
);
(y
1
;
y
2
)
>=
x
1
y
1
 x
1
y
2
 x
2
y
1
+
2x
2
y
2 deï¬nes an
inner product on
R
2
: (It helps to know that

1
0
 1
1


1
 1
0
1

=

1
 1
 1
2

:)
7. Verify that the inner product of Example 6.2.3 can be deï¬ned by using the matrix
A
=

p
2
0
0
p
3

together with the deï¬nition of matrix deï¬ned inner products from Example 6.2.4.
8. If
A
=

3
1
 1
2

, ï¬nd the cosine of the angle
 between the two vectors
u
=
[1;
0]
T and
v
=
[0;
1]
T in the vector space
R
2 with respect to the inner product deï¬ned
by
hu;
v
i

(Au)
T
(Av
)
=
u
T
(A
T
A)v
:
9. Explain how one could use Theorem 4.3.3 to test for whether or not a given vector
w
in the inner product space
W belongs to a subspace
V and illustrate it by determining
if
w
=
(2;
 4;
3) belongs to the subspace of
R
3 spanned by
v
1
=
(1;
1;
0) and
v
2
=
( 1;
1;
1):
10. Prove that
jjjj
1 is not an induced norm on
R
n
: Hint: See Example 6.2.15.
11. Let
A be an
n

n real matrix and deï¬ne the product
hu;
v
i

u
T
Av for all
u;
v
2R
n
:
(a) Show this product satisï¬es inner product laws 2,3 and 4 (page 267).
(b) Show that if
A is a diagonal matrix with positive entries, then the product satisï¬es
inner product law 1.
(c) Show that if
A is a real symmetric positive deï¬nite matrix, then the product satisï¬es
inner product law 1. Hint: Let
P be an orthogonal matrix that diagonalizes
A, write
x
=
P
y and calculate
hx;
xi: Now use Exercise 15 and part (b).
12. Let
v
1
=
(1;
0;
0)
;
v
2
=
( 1;
2;
0)
;
v
3
=
(1;
 2;
3)
: Let
V
=
R
3 be an inner
product space with inner product deï¬ned by the formula
hx;
y
i
=
x
T
Ay
; where
A
=
2
4
2
1
0
1
2
1
0
1
2
3
5
(a) Use Exercise 11 to show that the formula really does deï¬ne an inner product.
(b) Verify that
v
1
;
v
2
;
v
3 form an orthogonal basis of
V
:
(c) Find the coordinates of
(1;
2;
 2) with respect to this basis by using the orthogonal
coordinates theorem.

276
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
13. Let
V be an inner product space. Use the deï¬nition of inner product to prove that
for
u;
v
2
V
; and scalar
a; the following are true.
(a)
h0;
v
i
=
0
(b)
hu
+
v
;
w
i

hu;
w
i+
hv
;
w
i
(c)
hau;
v
i
=
ahu;
v
i
14. Prove the following generalization of the Pythagorean Theorem: If
v
1
;
v
2
;
:
:
:
;
v
n
are pairwise orthogonal vectors in the inner product space
V and
v
=
v
1
+
v
2
+
:
:
:
+
v
n
;
then
jjv
j
j
2
=
jjv
1
jj
2
+
jjv
2
jj
2
+
:
:
:
+
jjv
n
j
j
2
15. Show that
k
k
1 is not an induced norm on
R
2
: Hint: See Example 6.2.15.
16. Let
V be an inner product space with inner product
h;
i and induced norm
kk
:
Express
k
u
+
v
k
2 and
k
u
 v
k
2 in terms of inner products and use this to prove the
polarization identity
hu;
v
i
=
1
4
n
ku
+
v
k
2
 ku
+
v
k
2
o
(This identity shows that any inner product can be recovered from its induced norm.)
6.3. Gram-Schmidt Algorithm
We have seen that orthogonal bases have some very pleasant properties, such as the ease
with which we can compute coordinates. Our goal in this section is very simple: given
a subspace
V of some inner product space and a basis
w
1
;
w
2
;
:
:
:
w
n of
V , to turn
this basis into an orthogonal basis. This is exactly what the Gram-Schmidt algorithm is
designed to do.
Description of the Algorithm
THEOREM 6.3.1. Let
w
1
;
w
2
;
:
:
:
w
n be a basis of the inner product space
V
: Deï¬ne
Gram-Schmidt
Algorithm
vectors
v
1
;
v
2
;
:
:
:
v
n recursively by the formula
v
k
=
w
k
 h
v
1
;
w
k
i
h
v
1
;
v
1
i
v
1
 h
v
2
;
w
k
i
h
v
2
;
v
2
i
v
2
 :
:
:
 hv
k
 1
;
w
k
i
hv
k
 1
;
v
k
 1
i
v
k
 1
;
k
=
1;
:
:
:
;
n:
Then we have
1. The vectors
v
1
;
v
2
;
:
:
:
v
k form an orthogonal set.
2. For each index
k
=
1;
:
:
:
n,
spanfw
1
;
w
2
;
:
:
:
w
k
g
=
spanfv
1
;
v
2
;
:
:
:
v
k
g:

6.3. GRAM-SCHMIDT ALGORITHM
277
PROOF. In the case
k
=
1, we have that the single vector
v
1
=
w
1 is an orthogonal
set and certainly
spanfw
1
g
=
span
fv
1
g: Now suppose that for some index
k
>
1 we
have shown that
v
1
;
v
2
;
:
:
:
v
k
 1 is an orthogonal set such that
spanfw
1
;
w
2
;
:
:
:
w
k
 1
g
=
spanfv
1
;
v
2
;
:
:
:
v
k
 1
g
Then it is true that
hv
r
;
v
s
i
=
0 for any indices
r
;
s both less than
k
: Take the inner
product of
v
k
; as given by the formula above, with the vector
v
j, where
j
<
k and we
obtain
h
v
j
;
v
k
i
=

v
j
;
w
k
 hv
1
;
w
k
i
hv
1
;
v
1
i
v
1
 hv
2
;
w
k
i
hv
2
;
v
2
i
v
2
 


 h
v
k
 1
;
w
k
i
h
v
k
 1
;
v
k
 1
i
v
k
 1

=
h
v
j
;
w
k
i
 h
v
1
;
w
k
i
h
v
j
;
v
1
i
hv
1
;
v
1
i
 


 hv
k
 1
;
w
k
i
h
v
j
;
v
k
 1
i
hv
k
 1
;
v
k
 1
i
=
h
v
j
;
w
k
i
 h
v
j
;
w
k
i
hv
j
;
v
j
i
hv
j
;
v
j
i
=
0
It follows that
v
1
;
v
2
;
:
:
:
;
v
k is an orthogonal set. The Gram-Schmidt formula show
us that one of
v
k or
w
k can be expressed as a linear combination of the other and
v
1
;
v
2
;
:
:
:
;
v
k
 1
: Therefore
span
fw
1
;
w
2
;
:
:
:
w
k
 1
;
w
k
g
=
spanfv
1
;
v
2
;
:
:
:
v
k
 1
;
w
k
g
=
spanfv
1
;
v
2
;
:
:
:
v
k
 1
;
v
k
g
which is the second part of the theorem. We can repeat this argument for each index
k
=
2;
:
:
:
;
n to complete the proof of the theorem.
The Gram-Schmidt formula is easy to remember: One simply subtracts from the vec-
tor
w
k all of the projections of
w
k along the directions
v
1
;
v
2
;
:
:
:
v
k
 1 to obtain the
vector
v
k
: The Gram-Schmidt algorithm applies to any inner product space, not just the
standard ones. Consider the following example.
EXAMPLE 6.3.2. Let
C
[0;
1] be the space of continuous functions on the interval
[0;
1]
with the usual function addition and scalar multiplication, and (standard) inner product
given by
hf
;
g
i
=
Z
1
0
f
(x)g
(x)dx
as in Example 6.2.5. Let
V
=
P
2
=
span
f1;
x;
x
2
g and apply the Gram-Schmidt
algorithm to the basis
1;
x;
x
2 to obtain an orthogonal basis for the space of quadratic
polynomials.

278
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
SOLUTION. It helps to recall the calculus formula
R
1
0
x
m
x
n
dx
=
1=(m
+
n
+
1): Now
set
w
1
=
1;
w
2
=
x;
w
3
=
x
2 and calculate the Gram-Schmidt formulas:
v
1
=
w
1
=
1;
v
2
=
w
2
 hv
1
;
w
2
i
h
v
1
;
v
1
i
v
1
=
x
 1=2
1
1
=
x
 1
2
v
3
=
w
3
 hv
1
;
w
3
i
h
v
1
;
v
1
i
v
1
 h
v
2
;
w
3
i
h
v
2
;
v
2
i
v
2
=
x
2
 1=3
1
1
 1=12
1=12
(x
 1
2
)
=
x
2
 x
+
1
6
:
Had we used
C
[ 1;
1] and required that each polynomial have value
1 at
x
=
1, the
same calculations would have given us the ï¬rst three so-called Legendre polynomials.
These polynomials are used extensively in approximation theory and applied mathemat-
ics.
If we prefer to have an orthonormal basis rather than an orthogonal basis, then, as a ï¬nal
step in the orthogonalizing process, simply replace each vector
v
k by the normalized
vector
u
k
=
v
k
=
j
jv
k
jj
: Here is an example to illustrate the whole scheme.
EXAMPLE 6.3.3. Let
V
=
C
(A), where
A
=
2
6
6
4
1
2
0
 1
1
 1
3
2
1
 1
3
2
 1
1
 3
1
3
7
7
5
and
V has the standard inner product. Find an orthonormal basis of
V
:
SOLUTION. We know that
V is spanned by the four columns of
A: However, the Gram-
Schmidt algorithm requests a basis of
V and we donâ€™t know that the columns are linearly
independent. We leave it to the reader to check that the reduced row echelon form of
A
is the matrix
R
=
2
6
6
4
1
0
2
0
0
1
 1
0
0
0
0
1
0
0
0
0
3
7
7
5
It follows from the column space algorithm that columns
1;
2 and
4 of the matrix
A
yield a basis of
V
: So let
w
1
=
(1;
1;
1;
 1);
w
2
=
(2;
 1;
 1;
1);
w
3
=
( 1;
2;
2;
1)

6.3. GRAM-SCHMIDT ALGORITHM
279
and apply the Gram-Schmidt algorithm to obtain that
v
1
=
w
1
=
(1;
1;
1;
 1);
v
2
=
w
2
 hv
1
;
w
2
i
hv
1
;
v
1
i
v
1
=
(2;
 1;
 1;
1)
  1
4
(1;
1;
1;
 1)
=
1
4
(9;
 3;
 3;
3);
v
3
=
w
3
 hv
1
;
w
3
i
hv
1
;
v
1
i
v
1
 hv
2
;
w
3
i
h
v
2
;
v
2
i
v
2
=
( 1;
2;
2;
1)
 2
4
(1;
1;
1;
 1)
  18
108
(9;
 3;
 3;
3)
=
1
4
( 4;
8;
8;
4)
 1
4
(2;
2;
2;
 2)
+
1
4
(6;
 2;
 2;
2)
=
(0;
1;
1;
2)
:
Finally, to turn this set into an orthonormal basis, we normalize each vector to obtain
the basis
u
1
=
v
1
k
v
1
k
=
1
2
(1;
1;
1;
 1);
u
2
=
v
2
k
v
2
k
=
1
p
108
(9;
 3;
 3;
3)
=
1
2
p
3
(3;
 1;
 1;
1);
u
3
=
v
3
k
v
3
k
=
1
p
6
(0;
1;
1;
2):
There are several useful observations about the preceding example which are particu-
larly helpful for hand calculations.
 If one encounters an inconvenientfraction, such as the
1
4 in
v
2, one could replace
the calculated
v
2 by
4v
2, thereby eliminating the fraction, and yet achieving the
same results in subsequent calculations. The idea here is that for any nonzero
scalar
c
hv
2
;
w
i
hv
2
;
v
2
i
v
2
=
h
cv
2
;
w
i
hcv
2
;
cv
2
i
cv
2
:
So we could have replaced
1
4
(9;
 3;
 3;
3) by
(3;
 1;
 1;
1) and achieved the
same results.
 The same remark applies to the normalizing process, since in general,
v
2
kv
2
k
=
cv
2
kcv
2
k
The Gram-Schmidt algorithm is robust enough to handle linearly dependent spanning
sets gracefully. We illustrate this fact with the following example:
EXAMPLE 6.3.4. Suppose we had used all the columns of
A in Example 6.3.3 instead
of linearly independent ones, labelling them
w
1
;
w
2
;
w
3
;
w
4
: How would the Gram-
Schmidt calculation work out?

280
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
SOLUTION. Everything would have proceeded as above until we reached the calculation
of
v
3, which would then yield
v
3
=
w
3
 hv
1
;
w
3
i
hv
1
;
v
1
i
v
1
 hv
2
;
w
3
i
h
v
2
;
v
2
i
v
2
=
(0;
3;
3;
 3)
 9
4
(1;
1;
1;
 1)
+
1
4
(9;
 3;
 3;
3)
=
1
4
(0;
12;
12;
 12)
+
9
4
( 1;
 1;
 1;
1)
  27
108
(9;
 3;
 3;
3)
=
(0;
0;
0;
0)
This tells us that
v
3 is a linear combination of
v
1 and
v
2, which mirrors the fact that
w
3
is a linear combination of
w
1 and
w
2
: Now discard
v
3 and continue the calculations to
get that
v
4
=
w
4
 h
v
1
;
w
4
i
h
v
1
;
v
1
i
v
1
 hv
2
;
w
4
i
hv
2
;
v
2
i
v
2
=
( 1;
2;
2;
1)
 2
4
(1;
1;
1;
 1)
  18
108
(9;
 3;
 3;
3)
=
(0;
1;
1;
2)
Interestingly enough, this is the same third vector that we obtained in the example cal-
culation. The upshot of this calculation is that the Gram-Schmidt algorithm can be
applied to any spanning set, provided that one discards any zero vectors that result from
the formula. The net result is still an orthogonal basis.
Application to Projections
We can use the machinery of orthogonal vectors to give a nice solution to a very practical
and important question which can be phrased as follows (see Figure 6.3.1 for a graphical
interpretation of it):
The Projection Problem: Given a ï¬nite dimensional subspace
V of a real inner product
space
W, together with a vector
b
2
W, to ï¬nd the vector
v
2
V which is closest to
b
in the sense that
jj
b
 v
j
j
2 is minimized.
Observe that the quantity
jjb
 v
jj
2 will be minimized exactly when
jjb
 v
j
j is mini-
mized, since the latter is always nonnegative. The squared term has the virtue of avoid-
ing square roots that computing
jjb
 v
j
j requires.
The projection problem looks vaguely familiar. It reminds us of the least squares prob-
lem of Chapter 4, which was to minimize the quantity
jjb
 Axj
j
2where
A is an
m

n
real matrix and
b;
x are standard vectors. Recall that
v
=
Ax is a typical element in
the column space of
A: Therefore, the quantity to be minimized is
jjb
 Axjj
2
=
jj
b
 v
j
j
2
where on the left hand side
x runs over all standard
n-vectors and on the right hand side
v runs over all vectors in the space
V
=
C
(A). The difference between least squares
and projection problem is this: in the least squares problem we want to know the vector
x of coefï¬cients of
v as a linear combination of columns of
A; whereas in the projection

6.3. GRAM-SCHMIDT ALGORITHM
281
c2v2
c1v1
b
v
=
projV
V
b
-
b  v
v
FIGURE 6.3.1. Projection
v of
b into subspace
V spanned by or-
thogonal
v
1
;
v
2.
problem we are only interested in
v. Knowing
v doesnâ€™t tell us what
x is, but knowing
x easily gives
v since
v
=
Ax:
To ï¬nd a solution to the projection problem we need the following key concept.
DEFINITION 6.3.5. Let
v
1
;
v
2
;
:
:
:
v
n be an orthogonal basis for the subspace
V of the
inner product space
W
: For any
b
2
W, the projection of
b into the subspace
V is the
Projection
Formula for
Subspaces
vector
pro
j
V
b
=
hv
1
;
bi
hv
1
;
v
1
i
v
1
+
hv
2
;
bi
hv
2
;
v
2
i
v
2
+



+
hv
n
;
bi
hv
n
;
v
n
i
v
n
Notice that in the case of
n
=
1 the deï¬nition amounts to a familiar friend, the projection
of
b along the vector
v
1
: Now we call this the projection of
b into the subspace
V
spanned by
v
1
: This projection has the same nice property that we observed in the case
of standard inner products, namely,
p
=
pro
j
V
b is a multiple of
v
1 which is orthogonal
to
b
 p: Simply check that
hv
1
;
b
 pi
=
hv
1
;
bi
 
v
1
;
hv
1
;
bi
h
v
1
;
v
1
i
v
1

=
hv
1
;
bi
 h
v
1
;
v
1
i
h
v
1
;
v
1
i
hv
1
;
bi
=
0:
It would appear that the deï¬nition depends on the basis vectors
v
1
;
v
2
;
:
:
:
v
n, but we
see from the next theorem that this is not the case.
THEOREM 6.3.6.
Let
v
1
;
v
2
;
:
:
:
v
n be an orthogonal basis for the subspace
V of the
Projection
Theorem
inner product space
W
: For any
b
2
W, the vector
v
=
pro
j
V
b is the unique vector
in
V that minimizes
jjb
 v
j
j
2
:
PROOF. Let
v be a solution to the projection problem and
p the projection of
b
 v
along any vector in
V
: Use the remark preceding this theorem with
b
 v in place of
b to
write
b
 v as the sum of orthogonal vectors
b
 v
 p and
p: Now use the Pythagorean
Theorem to see that
kb
 v
k
2
=
k
b
 v
 pk
2
+
kpk
2

282
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
However,
v
+
p
2
V so that
kb
 v
k cannot be the minimum distance
b to a vector
in
V unless
kpk
=
0: It follows that
b
 v is orthogonal to any vector in
V
: Now let
v
1
;
v
2
;
:
:
:
v
n be an orthogonal basis of
V and express the vector
v in the form
v
=
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
Then for each
v
k we must have
0
=
h
v
k
;
b
 v
i
=
hv
k
;
b
 c
1
v
1
 c
2
v
2
 


 c
n
v
n
i
=
h
v
k
;
bi
 c
1
hv
k
;
v
1
i
 c
2
h
v
k
;
v
2
i
 


c
n
hv
k
;
v
n
i
=
h
v
k
;
bi
 c
k
hv
k
;
v
k
i
from which we deduce that
c
k
=
hv
k
;
bi
=
h
v
k
;
v
k
i
: It follows that
v
=
hv
1
;
bi
hv
1
;
v
1
i
v
1
+
hv
2
;
bi
hv
2
;
v
2
i
v
2
+



+
hv
n
;
bi
hv
n
;
v
n
i
v
n
=
pro
j
V
b
This proves that there can be only one solution to the projection problem, namely the
one given by the projection formula above.
There is one more point to be established, namely that
pro
j
V
b actually solves the
projection problem. This is left to the exercises.
It is worth noting that in proving the preceding theorem, we showed that
pro
j
V
b is
orthogonal to every element of a basis of
V and therefore to every element of
V , since
such elements are linear combinations of the basis elements.
Let us specialize to standard real vectors and inner products and take a closer look at
the formula for the projection operator in the case that
v
1
;
v
2
;
:
:
:
v
n is an orthonormal
set. We then have
h
v
j
;
v
j
i
=
1, so
pro
j
V
b
=
h
v
1
;
bi
v
1
+
h
v
2
;
bi
v
2
+



+
hv
n
;
bi
v
n
=
 v
T
1
b

v
1
+
 v
T
2
b

v
2
+



+
 v
T
n
b

v
n
=
v
1
v
T
1
b
+
v
2
v
T
2
b
+



+
v
n
v
T
n
b
=
 v
1
v
T
1
+
v
2
v
T
2
+



+
v
n
v
T
n

b
=
P
b:
Thus we have the following expression for the matrix
P
:
Orthogonal
Projection
Formula
P
=
v
1
v
T
1
+
v
2
v
T
2
+



+
v
n
v
T
n
The signiï¬cance of this expression for projections in standard spaces over the reals with
the standard inner product is as follows: computing the projection of a vector into a
subspace amounts to no more than multiplying the vector by a matrix
P which can be
computed from
V
: Even in the case
n
=
1 this fact gives us a new slant on projections:
pro
j
v
u
=
(vv
T
)u
So here we have
P
=
vv
T
:
Projection
Matrices
The general projection matrix
P has some interesting properties. It is symmetric, i.e.,
P
T
=
P
; and idempotent, i.e.,
P
2
=
P
: Therefore, this notation is compatible with the

6.3. GRAM-SCHMIDT ALGORITHM
283
deï¬nition of projection matrix introduced in earlier exercises (see Exercise 8). Symme-
try follows from the fact that
 v
k
v
T
k

T
=
v
k
v
T
k
: For idempotence, notice that
(v
j
v
T
j
)(v
k
v
T
k
)
=
(v
T
j
v
k
)(v
k
v
T
j
)
=
Ã†
i;j
:
It follows that
P
2
=
P
: In general, symmetric idempotent matrices are called projection
matrices. The name is justiï¬ed because multiplication by them projects vectors into the
column space of
P
:
EXAMPLE 6.3.7. Find the projection matrix for the subspace of
R
3 spanned by the
orthonormal vectors
v
1
=
(1=
p
2
)[1;
 1;
0]
T and
v
2
=
(1=
p
3)[1;
1;
1]
T and use it to
solve the projection problem with
V
=
spanfv
1
;
v
2
g and
b
=
[2;
1;
 3]
T
:
SOLUTION. Use the formula developed above for the projection matrix
P
=
v
1
v
T
1
+
v
2
v
T
2
=
1
2
2
4
1
 1
0
3
5
[
1
 1
0
]
+
1
3
2
4
1
1
1
3
5
[
1
1
1
]
2
4
5
6
 1
6
1
3
 1
6
5
6
1
3
1
3
1
3
1
3
3
5
The solution to the projection problem is now given by
v
=
P
b
=
2
4
5
6
 1
6
1
3
 1
6
5
6
1
3
1
3
1
3
1
3
3
5
2
4
2
1
 3
3
5
=
2
4
1
2
 1
2
0
3
5
The projection problem is closely related to another problem that we have seen before,
namely the least squares problem of Section 4.2 in Chapter 4. Recall that the least
squares problem amounted to minimizing the function
f
(x)
=
jjb
 Axjj
2, which in
turn led to the normal equations. Here
A is an
m

n real matrix. Now consider the
projection problem for the subspace
V
=
C
(A) of
R
m, where
b
2
R
m
: We know
that elements of
C
(A) can be written in the form
v
=
Ax, where
x
2
R
n
: Therefore,
jjb
 Axjj
2
=
jjb
 v
jj
2, where
v ranges over elements of
V
: It follows that when we
solve a least squares problem, we are solving a projection problem as well in the sense
that the vector
Ax is the element of
C
(A) closest to the right hand side vector
b: One
could also develop normal equations for general spanning sets of
V
: An example of this
is given in the exercises.
The normal equations also give us another way to generate projection matrices in the
case of standard vectors and inner products. As above, let the subspace
V
=
C
(A)
of
R
m, and
b
2
R
m
: Assume that
V
=
C
(A) and that the columns of
A are linearly
independent, i.e., that
A has full column rank. Then, as we have seen in Theorem 4.2.11
of Chapter 3, the matrix
A
T
A is invertible and the normal equations
A
T
Ax
=
A
T
b
have the unique solution
x
=
(A
T
A)
 1
A
T
b:

284
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
Consequently, the solution to the projection problem is
v
=
Ax
=
A(A
T
A)
 1
A
T
b:
It is also true that
v
=
P
b; since this holds for all
b; it follows that the projection matrix
for this subspace is given by the formula
Column Space
Projection
Formula
P
=
A(A
T
A)
 1
A
T
EXAMPLE 6.3.8. Find the projection matrix for the subspace
V
=
span
fw
1
;
w
2
g of
R
3 with
w
1
=
(1;
 1;
0) and
w
2
=
(2;
0;
1):
SOLUTION. Let
A
=
[w
1
;
w
2
] so that
A
T
A
=

1
 1
0
2
0
1

2
4
1
2
 1
0
0
1
3
5
=

2
2
2
5

Thus
P
=
A(A
T
A)
 1
A
T
=
2
4
1
2
 1
0
0
1
3
5
1
6

5
 2
 2
2


1
 1
0
2
0
1

=
2
4
5
6
 1
6
1
3
 1
6
5
6
1
3
1
3
1
3
1
3
3
5
Curiously, this is exactly the same matrix as the projection matrix found in the preceding
example. What is the explanation? Notice that
w
1
=
p
6
v
1 and
w
2
=
p
6v
1
+
p
3v
2,
so that
V
=
span
fw
1
;
w
2
g
=
spanfv
1
;
v
2
g: So the subspace of both examples is
the same, but speciï¬ed by different bases. Therefore we should expect the projection
operator to be the same.
6.3 Exercises
1. Find the projection of the vector
w
=
(2;
1;
2) into the subspace
V
=
span
f(1;
 1;
1);
(1;
1;
0)g
where the inner products used are the standard inner product on
R
3 and the weighted
inner product
<
(x;
y
;
z
);
(u;
v
;
w
)
>=
2xu
+
3y
v
+
z
w
2. Let
[w
1
;
w
2
;
w
3
]
=
2
4
1
1
1
0
 1
 1
0
 1
1
3
5 and
w
=
(2;
1;
4):
(a) Use the Gram-Schmidt algorithm on
w
1
;
w
2
;
w
3 to obtain an orthonormalset
v
1
;
v
2
;
v
3
:
(b) Find the projection of
w into the subspace
V
=
spanfv
1
;
v
2
g:
(c) Use (b) to express
w as a sum of orthogonal vectors, one of which is in
V
:

6.3. GRAM-SCHMIDT ALGORITHM
285
(d) Find the projection matrix
P for the subspace
V
:
(e) Verify that multiplication of
w by
P gives the same result as in (b).
3. Let
w
1
=
( 1;
 1;
1;
1);
w
2
=
(1;
1;
1;
1);
w
3
=
(0;
0;
0;
1);
w
=
(1;
0;
0;
0)
and
V
=
span
fw
1
;
w
2
;
w
3
g: Repeat parts (a)-(e) of Exercise 2 for these vectors, except
in part (b) use
V
=
spanfv
1
;
v
2
;
v
3
g:
4. Find an orthonormal basis of
C
(A), where
A is one of the following
(a)
2
4
1
 1
1
1
2
4
 1
2
0
3
5 (b)
2
6
6
4
1
0
2
1
1
2
 1
1
1
 1
0
0
3
7
7
5 (c)
2
4
1
2
1
0
0
4
1
2
0
3
5
5. Find the projection matrix for the column space of each of the following matrices by
using the orthogonal projection formula.
(a)

1
 2
 1
2

(b)
2
4
2
1
1
0
2
4
 1
2
0
3
5
(c)

1
 1
 1
0

6. Redo Exercise 5 by using the column space projection formula (remember to use a
matrix of full column rank for this formula).
7. Show that the matrices
A
=
2
4
1
3
4
1
4
2
1
1
8
3
5 and
B
=
2
4
1
2
2
 2
 3
 2
7
12
10
3
5 have the
same column space by computing the projection matrices into these column spaces.
8. Let
W
=
C
[ 1;
1] with the standard inner product as in Example 6.3.2. Suppose
V
is the subspace of linear polynomials and
b
=
e
x
:
(a) Find an orthogonal basis for
V
:
(b) Find the projection
p of
b into
V
:
(c) Compute the â€œmean error of approximationâ€
kb
 pk: How does it compare to the
mean error of approximation when one approximates
b by
q; its Taylor series centered
at
0:
(d) Use a CAS to plot
b
 p and
b
 q: Find the points at which this error is largest and
compare the two.
9. Write out a proof of the Gram-Schmidt algorithm (Theorem 6.3.1) in the case that
n
=
3:
10. Complete the proof of the Projection Theorem (Theorem 6.3.6) by showing that
pro
j
V
b solves the projection problem.
11. Verify directly that if
P
=
A(A
T
A)
 1
A
T
; (assume
A has full column rank) then
P is symmetric and idempotent.
12. How does the orthogonal projection formula on page 282 have to be changed if the
vectors in question are complex? Illustrate your answer with the orthonormal vectors
v
1
=
((1
+
i)=2;
0;
(1
+
i)=2);
v
2
=
(0;
1;
0) in
C
2
:

286
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
13. Show that if
P is a square
n

n real matrix such that
P
T
=
P and
P
2
=
P, that is,
A is a projection matrix, then for every
v
2
R
n
;
P
v
2
C
(A) and
v
 P
v is orthogonal
to every element of
C
(A):
6.4. Linear Systems Revisited
Once again, we revisit our old friend,
Ax
=
b, where
A is an
m

n matrix. The notions
of orthogonality can be made to shed still more light on the nature of this system of
equations, especially in the case of a homogeneous system
Ax
=
0: The
kth entry of
the column vector
Ax is simply the
kth row of
A multiplied by the column vector
x:
Designate this row by
r
k and we see that
r
k

x
=
0;
k
=
1;
:
:
:
;
n:
In other words,
Ax
=
0, that is,
x
2
N
(A), precisely when
x is orthogonal (with the
standard inner product) to every row of
A: We will see in Theorem 6.4.4 below that this
means that
x will be orthogonal to any linear combination of the rows of
A: Thus, we
could say
N
(A)
=
fx
2
R
n
j
r

x
=
0 for every
r
2
R(A)g
(6.4.1)
This is an instance of a very important idea. We are going to digress to put this idea
in a more general context, after which we will return to linear systems with a new
perspective on their meaning.
Orthogonal Complements and Homogeneous Systems
DEFINITION 6.4.1. Let
V be a subspace of an inner product space
W
: Then the orthog-
onal complement of
V in
W is the set
V
?
=
fw
2
W
j
hv
;
w
i
=
0
for all
v
2
V
g
We can see from the subspace test that
V
? is a subspace of
W
: Before stating the basic
facts, we mention that if
U and
V are two subspaces of the vector space
W, then two
other subspaces that we can construct are the intersection and sum of these subspaces.
The former is just the set intersection of the two subspaces and the latter is the set of
elements of the form
u
+
v, where
u
2
U, and
v
2
V
: One can use the subspace test
to verify that these are indeed subspaces of
W (see Exercise 15 of Section 2, Chapter 3.
In fact, it isnâ€™t too hard to see that
U
+
V is the smallest space containing all elements
of both
U and
V
: We can summarize the basic facts about the orthogonal complement
of
V as follows.
THEOREM 6.4.2. Let
V be a subspace of the ï¬nite dimensional inner product space
W
:
Then the following are true:

6.4. LINEAR SYSTEMS REVISITED
287
1.
V
? is a subspace of
W
:
2.
V
\
V
?
=
f0g
3.
V
+
V
?
=
W
4.
dim
V
+
dim
V
?
=
dim
W
5.
 V
?

?
=
V
PROOF. We leave 1 and 2 as exercises. To prove 3, we notice that
V
+
V
?

W
since
W is closed under sums. Now suppose that
w
2
W
: Let
v
=
pro
j
V
w
: We know
that
v
2
V and
w
 v is orthogonal to every element of
V
: It follows that
w
 v
2
V
?
:
Therefore every element of
W can be expressed as a sum of an element in
V and an
element in
V
?
: This shows that
W

V
+
V
?, from which it follows that
V
+
V
?
=
W
:
To prove 4, let
v
1
;
v
2
;
:
:
:
;
v
r be a basis of
V and
w
1
;
w
2
;
:
:
:
;
w
s be a basis of
V
?
:
Certainly the union of the two sets spans
V because of 3. Now if there were an equation
of linear dependence, we could gather all terms involving
v
1
;
v
2
;
:
:
:
;
v
r on one side
of the equation, those involving
w
1
;
w
2
;
:
:
:
;
w
s on the other side and deduce that each
is equal to zero separately, in view of 2. It follows that the union of these two bases
must be an independent set. Therefore it forms a basis of
W
: It follows that
dim
W
=
r
+
s
=
dim
V
+
dim
V
?
:
Finally, apply 4 to
V
? in place of
V and obtain that
dim
 V
?

?
=
dim
W
 dim
V
?
:
But 4 implies directly that
dim
V
=
dim
W
 dim
V
?, so that
dim
 V
?

?
=
dim
V
:
Now if
v
2
V , then certainly
h
w
;
v
i
=
0 for all
w
2
V
?
: Hence
V

 V
?

?
: Since
these two spaces have the same dimension, they must be equal, which proves 5.
Orthogonal complements of the sum and intersections of two different subspaces have
an interesting relationship to each other. We will leave the proofs of these facts as
exercises.
THEOREM 6.4.3. Let
U and
V be subspaces of the inner product space
W
: Then the
following are true:
1.
(U
\
V
)
?
=
U
?
+
V
?
2.
(U
+
V
)
?
=
U
?
\
V
?
There is a very useful fact about the orthogonal complement of a ï¬nite dimensional
space that greatly simpliï¬es the calculation of an orthogonal complement. What it says
in words is that a vector is orthogonal to every element of a vector space if and only if
it is orthogonal to every element of a spanning set of the space.
THEOREM 6.4.4. Let
V
=
spanfv
1
;
v
2
;
:
:
:
;
v
n
g be a subspace of the inner product
space
W
: Then
V
?
=
fw
2
W
j
hw
;
v
j
i
=
0;
j
=
1;
2;
:
:
:
;
ng
PROOF. Let
v
2
V
; so that for some scalars
c
1
;
c
2
;
:
:
:
;
c
n
v
=
c
1
v
1
+
c
2
v
2
+



+
c
n
v
n
Take the inner product of both sides with a vector
w
: We see by the linearity of inner
products that
hw
;
v
i
=
c
1
hw
;
v
1
i
+
c
2
hw
;
v
2
i
+



+
c
n
hw
;
v
n
i

288
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
so that if
hw
;
v
j
i
=
0 for each
j then certainly
hw
;
v
i
=
0: Conversely, if
hw
;
v
j
i
=
0;
j
=
1;
2;
:
:
:
;
n; clearly
hw
;
v
j
i
=
0: This proves the theorem.
EXAMPLE 6.4.5. Compute
V
?, where
V
=
span
f(1;
1;
1;
1);
(1;
2;
1;
0)g

R
4
with the standard inner product on
R
4
:
SOLUTION. Form the matrix
A with the two spanning vectors of
V as rows. According
to Theorem 6.4.4,
V
? is simply the null space of this matrix. We have
A
=

1
1
1
1
1
2
1
0

       !
E
21
( 1)

1
1
1
1
0
1
0
 1

       !
E
12
( 1)

1
0
1
2
0
1
0
 1

from which it follows that the null space of
A consists of vectors of the form
2
6
6
4
 x
3
 2x
4
x
4
x
3
x
4
3
7
7
5
=
x
3
2
6
6
4
 1
0
1
0
3
7
7
5
+
x
4
2
6
6
4
 2
1
0
1
3
7
7
5
Therefore the null space is spanned by
( 1;
0;
1;
0) and
( 2;
1;
0;
1):
Nothing prevents us from considering more exotic inner products as well. The arith-
metic may be a bit more complicated, but the underlying principles are the same. Here
is such an example.
EXAMPLE 6.4.6. Let
V
=
spanf1;
xg

W
=
P
2
; where the space
P
2 of polynomials
of degree at most
2 has the same standard inner product as
C
[0;
1]: Compute
V
? and
use this to verify that
dim
V
+
dim
V
?
=
dim
W
:
SOLUTION. According to Theorem 6.4.4,
V
? consists of those polynomials
p(x)
=
c
0
+
c
1
x
+
c
2
x
2 for which
0
=
hp;
1i
=
Z
1
0
 c
0
+
c
1
x
+
c
2
x
2

1
dx
=
c
0
Z
1
0
1
dx
+
c
1
Z
1
0
x
dx
+
c
2
Z
1
0
x
2
dx
and
0
=
h
p;
xi
=
Z
1
0
 c
0
+
c
1
x
+
c
2
x
2

x
dx
=
c
0
Z
1
0
x
dx
+
c
1
Z
1
0
x
2
dx
+
c
2
Z
1
0
x
3
dx
Use the fact that
R
1
0
x
m
dx
=
1
m+1 for nonnegative
m and we obtain the system of
equations
c
0
+
1
2
c
1
+
1
3
c
2
=
0
1
2
c
0
+
1
3
c
1
+
1
4
c
2
=
0:
Solve this system to obtain
c
0
=
1
6
c
2,
c
1
=
 c
2 and
c
2 is free. Therefore,
V
? consists
of polynomials of the form
p(x)
=
1
6
c
2
 c
2
x
+
c
2
x
2
=
c
2

1
6
 x
+
x
2


6.4. LINEAR SYSTEMS REVISITED
289
It follows that
V
?
=
spanf
1
6
 x
+
x
2
g. In particular,
dim
V
?
=
1, and since
f1;
xg
is a linearly independent set,
dim
V
=
2. Therefore,
dim
V
+
dim
V
?
=
dim
P
2
=
dim
W
:
Finally, we return to the subject of solutions to the homogeneous system
Ax
=
b: We
saw at the beginning of this section that the null space of
A consisted of elements that
are orthogonal to the rows of
A: One could turn things around and ask what we can say
about a vector that is orthogonal to every element of the null space of
A: How does it
relate to the rows of
A? This question has a surprisingly simple answer. In fact, there is
a fascinating interplay between row spaces, column spaces and null spaces which can
be summarized in the following theorem:
THEOREM 6.4.7. Let
A be a matrix. Then
Orthogonal
Complements
Theorem
1.
R(A)
?
=
N
(A)
2.
N
(A)
?
=
R(A)
3.
N
(A
T
)
?
=
C
(A)
PROOF. We have already seen item 1 in the discussion at the beginning of this sec-
tion, where it was stated in Equation 6.4.1. For item 2 we take orthogonal complements
of both sides of 1 and use part 5 of Theorem 6.4.2 to obtain that
N
(A)
?
=
 R(A)
?

?
=
R(A)
which proves 2. Finally, for 3 we observe that
R(A
T
)
=
C
(A): Apply 2 with
A
T in
place of
A and the result follows.
The connections spelled out by this theorem are powerful ideas. Here is one example of
how they can be used. Consider the following problem: suppose we are given subspaces
U and
V of the standard space
R
n with the standard inner product (the dot product) in
some concrete form, and we want to compute a basis for the subspace
U
\
V . How do we
proceed? One answer is to use part 1 of Theorem 6.4.3 to see that
(U
\
V
)
?
=
U
?
+
V
?
:
Now use part 5 of Theorem 6.4.2 to obtain that
U
\
V
=
(U
\
V
)
?
?
=
(U
?
+
V
?
)
?
The strategy that this equation suggests is as follows: express
U and
V as row spaces
of matrices and compute bases for the null spaces of each. Put these bases together to
obtain a spanning set for
U
?
+
V
?
: Use this spanning set as the rows of a matrix
B
:
Then the complement of this space is, on the one hand,
U
\
V
; but by the ï¬rst part of
the orthogonal complements theorem, it is also
N
(B
): Therefore
U
\
V
=
N
(B
); so
all we have to do is calculate a basis for
N
(B
); which we know how to do.
EXAMPLE 6.4.8. Find a basis for
U
\
V
; where these subspaces of
R
4 are given as
follows:
U
=
span
f(1;
2;
1;
2);
(0;
1;
0;
1)g
V
=
span
f(1;
1;
1;
1);
(1;
2;
1;
0)g:
SOLUTION. We have already determined in Exercise 6.4.5 that the null space of
V has a
basis
( 1;
0;
1;
0) and
( 2;
1;
0;
1): Similarly, form the matrix
A with the two spanning

290
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
vectors of
U as rows. By Theorem 6.4.4,
V
?
=
N
(A): We have
A
=

1
2
1
2
0
1
0
1

       !
E
21
( 2)

1
0
1
0
0
1
0
1

from which it follows that the null space of
A consists of vectors of the form
2
6
6
4
 x
3
 x
4
x
3
x
4
3
7
7
5
=
x
3
2
6
6
4
 1
0
1
0
3
7
7
5
+
x
4
2
6
6
4
0
1
0
1
3
7
7
5
:
Therefore the null space has basis
( 1;
0;
1;
0) and
(0;
1;
0;
1): One of the vectors in
this set is repeated in the basis of
V so we only to need list it once. Form the matrix
B whose rows are
( 1;
0;
1;
0);
( 2;
1;
0;
1) and
(0;
1;
0;
1); and calculate the reduced
row echelon form of
B
:
B
=
2
4
 1
0
1
0
 2
1
0
1
0
1
0
1
3
5
         !
E
12
( 2)
E
1
( 1)
2
4
1
0
 1
0
0
1
 2
1
0
1
0
1
3
5
       !
E
23
( 1)
2
4
1
0
 1
0
0
1
 2
1
0
0
2
0
3
5
        !
E
3
(1=2)
E
32
(2)
E
31
(1)
2
4
1
0
0
0
0
1
0
1
0
0
1
0
3
5
It follows that the null space of
B consists of vectors of the form
2
6
6
4
0
 x
4
0
x
4
3
7
7
5
=
x
4
2
6
6
4
0
 1
0
1
3
7
7
5
:
Therefore,
U
\
V is a one-dimensional space spanned by the vector
(0;
 1;
0;
1):
Our last application of the orthogonal complements theorem is another Fredholm alter-
native theorem (compare this to Corollary 2.5.10 of Chapter 2).
COROLLARY 6.4.9. Given a square real linear system
Ax
=
b; where
b
6=
0; either
Fredholm
Alternative
the system is consistent or there is a solution
y to the homogeneous system
A
T
y
=
0
such that
y
T
b
6=
0:
PROOF. Let
V
=
C
(A): By part 3 of Theorem 6.4.2,
R
n
=
V
+
V
?
; where
R
n
has the standard inner product. From part 3 of the orthogonal complements theorem,
C
(A)
=
N
(A
T
)
?
: Take complements again and use part 5 of Theorem 6.4.2 to get
that
V
?
=
N
(A
T
): Now the system either has a solution or not. If the system has no
solution, then by Theorem 3.6.1 of Chapter 3,
b does not belong to
V
=
C
(A): Since
b
62
V
; we can write
b
=
v
+
y
; where
y
6=
0;
y
2
V
? and
v
2
V
: It follows that
hy
;
bi
=
y

b
=
y

(v
+
y
)
=
0
+
y

y
6=
0
On the other hand, if the system has a solution
x, then for any vector
y
2
N
(A) we
have
y
T
Ax
=
y
T
b: It follows that if
y
T
A
=
0; then
y
T
b
=
0: This completes the
proof.

6.4. LINEAR SYSTEMS REVISITED
291
The QR Factorization
We are going to use orthogonality ideas to develop one more way of solving the linear
system
Ax
=
b; where the
m

n real matrix
A is full column rank. In fact, if the
system is inconsistent, then this method will ï¬nd the unique least squares solution to
the system. Here is the basic idea: express the matrix
A in the form
A
=
QR
; where
the columns of the
m

n matrix
Q are orthonormal vectors and the
n

n matrix
R is
upper triangular with nonzero diagonal entries. Such a factorization of
A is called a QR
factorization of
A: It follows that the product
Q
T
Q
=
I
n
: Now multiply both sides of
the linear system on the left by
Q
T to obtain that
Q
T
Ax
=
Q
T
QR
x
=
I
R
x
=
Q
T
b
The net result is a simple square system with a triangular matrix which we can solve by
backsolving. That is, we use the last equation to solve for
x
n
; then the next to the last to
solve for
x
n 1
; and so forth. This is the backsolving phase of Gaussian elimination as
we ï¬rst learned it in Chapter 1, before we were introduced to Gauss-Jordan elimination.
One has to wonder why we have any interest in such a factorization, since we already
have Gauss-Jordan elimination for system solving. Furthermore, it can be shown that
ï¬nding a QR factorization is harder by a factor of about 2, that is, requires about twice
as many ï¬‚oating point operations to accomplish. So why bother? There are many
answers. For one, it can be shown that using the QR factorization has an advantage of
higher accuracy than Gauss-Jordan elimination in certain situations. For another, QR
factorization gives us another method for solving least squares problems. Weâ€™ll see an
example of this method at the end of this section.
Where can we ï¬nd such a factorization? As a matter of fact, we already have the
necessary tools, compliments of the Gram-Schmidt algorithm. To explain matters, letâ€™s
suppose that we have a matrix
A
=
[w
1
;
w
2
;
w
3
] with linearly independent columns.
Application of the Gram-Schmidt algorithm leads to orthogonal vectors
v
1
;
v
2
;
v
3 by
the following formulas:
v
1
=
w
1
v
2
=
w
2
 hv
1
;
w
2
i
hv
1
;
v
1
i
v
1
v
3
=
w
3
 hv
1
;
w
3
i
hv
1
;
v
1
i
v
1
  hv
2
;
w
3
i
hv
2
;
v
2
i
v
2
Next, solve for
w
1
;
w
2
;
w
3 in the above equations to obtain
w
1
=
v
1
w
2
=
hv
1
;
w
2
i
hv
1
;
v
1
i
v
1
+
v
2
w
3
=
hv
1
;
w
3
i
hv
1
;
v
1
i
v
1
+
hv
2
;
w
3
i
hv
2
;
v
2
i
v
2
+
v
3
In matrix form, these equations become
A
=
[w
1
;
w
2
;
w
3
]
=
[v
1
;
v
2
;
v
3
]
2
6
4
1
hv
1
;w
2
i
hv
1
;v
1
i
hv
1
;w
3
i
hv
1
;v
1
i
0
1
hv
2
;w
3
i
hv
2
;v
2
i
0
0
1
3
7
5

292
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
Now normalize the
v
jâ€™s by setting
q
j
=
v
j
=
k
v
j
k and observe that
A
=
[q
1
;
q
2
;
q
3
]
2
4
k
v
1
k
0
0
0
k
v
2
k
0
0
0
kv
3
k
3
5
2
6
4
1
hv
1
;w
2
i
hv
1
;v
1
i
hv
1
;w
3
i
hv
1
;v
1
i
0
1
hv
2
;w
3
i
hv
2
;v
2
i
0
0
1
3
7
5
=
[q
1
;
q
2
;
q
3
]
2
6
4
k
v
1
k
hv
1
;w
2
i
kv
1
k
hv
1
;w
3
i
kv
1
k
0
kv
2
k
hv
2
;w
3
i
kv
2
k
0
0
kv
3
k
3
7
5
This gives our QR factorization, which can be alternately written as
A
=
[w
1
;
w
2
;
w
3
]
=
[q
1
;
q
2
;
q
3
]
2
4
kv
1
k
hq
1
;
w
2
i
h
q
1
;
w
3
i
0
kv
2
k
h
q
2
;
w
3
i
0
0
kv
3
k
3
5
=
QR
In general, the columns of
A are linearly independent exactly when
A is full column
rank. It is easy to see that the argument we have given extends to any such matrix, so
we have the following theorem.
THEOREM 6.4.10. If
A is an
m

n matrix full column rank matrix, then
A
=
QR
;
QR
Factorization
where the columns of the
m

n matrix
Q are orthonormal vectors and the
n

n matrix
R is upper triangular with nonzero diagonal entries.
EXAMPLE 6.4.11. Let the full column rank matrix
A be given as
A
=
2
6
6
4
1
2
 1
1
 1
2
1
 1
2
 1
1
1
3
7
7
5
Find a QR factorization of
A and use this to ï¬nd the least squares solution to the problem
Ax
=
b; where
b
=
(1;
1;
1;
1): What is the norm of the residual
r
=
b
 Ax in this
problem?
SOLUTION. Notice that the columns of
A are just the vectors
w
1
;
w
2
;
w
3 of Exam-
ple 6.3.3. Furthermore, the vectors
u
1
;
u
2
;
u
3 calculated in that example are just the
q
1
;
q
2
;
q
3 that we require. Thus we have from those calculations that
kv
1
k
=
k
(1;
1;
1;
 1)k
=
2
kv
2
k
=




1
4
(9;
 3;
 3;
3)




=
3
2
p
3
kv
3
k
=
k
(0;
1;
1;
2)k
=
p
6
q
1
=
1
2
(1;
1;
1;
 1)
q
2
=
1
2
p
3
(3;
 1;
 1;
1)
q
3
=
1
p
6
(0;
1;
1;
2)

6.4. LINEAR SYSTEMS REVISITED
293
Now we calculate
hq
1
;
w
2
i
=
1
2
(1;
1;
1;
 1)

(2;
 1;
 1;
1)
=
 1
2
hq
1
;
w
3
i
=
1
2
(1;
1;
1;
 1)

( 1;
2;
2;
1)
=
1
hq
2
;
w
3
i
=
1
2
p
3
(3;
 1;
 1;
1)

( 1;
2;
2;
1)
=
 p
3
It follows that
A
=
2
6
6
4
1=2
3=(2
p
3
)
0
1=2
 1=(2
p
3
)
1=
p
6
1=2
 1=(2
p
3
)
1=
p
6
 1=2
1=(2
p
3
)
2=
p
6
3
7
7
5
2
4
2
 1=2
1
0
3
2
p
3
 p
3
0
0
p
6
3
5
=
QR
Solving the system
R
x
=
Q
T
b; where
b
=
(1;
1;
1;
1); by hand is rather tedious even
though the system is a simple triangular one. We leave the detailed calculations to the
reader. Better yet, use a CAS or MAS to obtain the solution
x
=
 1
3
;
2
3
;
2
3

: Another
calculation shows that
r
=
b
 Ax
=
2
6
6
4
1
1
1
1
3
7
7
5
 2
6
6
4
1
2
 1
1
 1
2
1
 1
2
 1
1
1
3
7
7
5
2
4
1=3
2=3
2=3
3
5
=
2
6
6
4
0
0
0
0
3
7
7
5
It follows that the system
Ax
=
b is actually consistent, since the least squares solution
turns out to be a genuine solution to the problem.
There remains the question of why we really solve the least squares problem by this
QR Least
Square Solver
method. To see why this is so, notice that with the above notation we have
A
T
=
(QR
)
T
=
R
T
Q
T
; so that the normal equations for the system
Ax
=
b (which are
given by
A
T
Ax
=
A
T
b) become
A
T
Ax
=
R
T
Q
T
QR
x
=
R
T
I
R
x
=
R
T
R
x
=
A
T
b
=
R
T
Q
T
b:
But the triangular matrix
R is invertible because its diagonal entries are nonzero; cancel
it and obtain that the normal equations are equivalent to
R
x
=
Q
T
b; which is exactly
what the method we have described solves.
6.4 Exercises
1. Let
V
=
spanf(1;
 1;
2)g

R
3
=
W with the standard inner product.
(a) Compute
V
?
:
(b) Verify that
V
+
V
?
=
R
3 and
V
\
V
?
=
f0g:
2. Let
V
=
spanf1
+
x;
x
2
g

W
=
P
2
; where the space
P
2 of polynomials of degree
at most
2 has the same standard inner product as
C
[0;
1]: Compute
V
?
:

294
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
3. Let
V be as in Exercise 1 but endow
W with the weighted inner product
<
(x;
y
;
z
);
(u;
v
;
w
)
>=
2xu
+
3y
v
+
z
w
:
(a) Compute
V
?
:
(b) Verify that
 V
?

?
=
V
:
4. Conï¬rm that the Fredholm alternative of this section holds for the system
x
1
+
x
2
+
2x
3
=
5
2x
1
+
3x
2
 x
3
=
2
4x
1
+
5x
2
+
3x
3
=
1
5. Use the subspace test to prove that if
V is a subspace of the inner product space
W,
then so is
V
?
:
6. Show that if
V is a subspace of the inner product space
W, then
V
\
V
?
=
f0g:
7. Let
U and
V be subspaces of the inner product space
W
:
(a) Prove that
(U
\
V
)
?
=
U
?
+
V
?
:
(b) Prove that
(U
+
V
)
?
=
U
?
\
V
?
:
8. Find a QR factorization for the matrix
A
=

1
2
 1
1

:
9. Let
A
=
2
4
3
2
0
1
4
1
3
5
(a) Use the Gram-Schmidt algorithm to ï¬nd a QR factorization of
A:
(b) Use the result of (a) to ï¬nd the least squares solution to the system
Ax
=
b; where
b
=
(1;
2;
3):
10. Carry out the method of computing
U
\
V discussed on page 288 using these two
subspaces of
W
=
R
3
:
U
=
span
f(1;
2;
1);
(2;
1;
0)g
V
=
span
f(1;
1;
1);
(1;
1;
3)g
11. The following is a simpliï¬ed description of the QR algorithm (which is separate
from the QR factorization, but involves it) for a real
n

n matrix
A
:
T
0
=
A;
Q
0
=
I
n
for
k
=
0;
1;
:
:
:
T
k
=
Q
k
+1
R
k
+1
(QR factorization of
T
k)
T
k
+1
=
R
k
+1
Q
k
+1
end
Apply this algorithm to the following two matrices and, based on your results, speculate
about what it is supposed to compute. You will need a CAS or MAS for this exercise

6.5. *OPERATOR NORMS
295
and, of course, you will stop in a ï¬nite number of steps, but expect to take more than a
few.
A
=
2
4
1
2
0
2
1
 2
0
 2
1
3
5
A
=
2
4
 8
 5
8
6
3
 8
 3
1
9
3
5
6.5. *Operator Norms
The object of this section is to develop a useful notion of the norm of a matrix. For
simplicity, we stick with the case of a real matrix
A, but all of the results in this section
carry over easily to complex matrices. In Chapters 3 and 5 we studied the concept
of a vector norm, which gave us a way of thinking about the â€œsizeâ€ of a vector. We
could easily extend this to matrices, just by thinking of a matrix as a vector which had
been chopped into segments of equal length and restacked as a matrix. Thus, every
vector norm on the space
R
mn of vectors of length
mn gives rise to a vector norm on
the space
R
m;n of
m

n matrices. Experience has shown that, with one exception â€“
the standard norm, this is not the best way to look for norms of matrices.
After all,
matrices are deeply tied up with the operation of matrix multiplication. It would be
too much to expect norms to distribute over products. The following deï¬nition takes a
middle ground that has proved to be useful for many applications.
DEFINITION 6.5.1. A vector norm
k
k which is deï¬ned on the vector space
R
m;n of
Matrix Norm
m

n matrices, for any pair
m;
n, is said to be a matrix norm if, for all pairs of matrices
A;
B which are conformable for multiplication,
k
AB
k

k
Ak
kB
k
Our ï¬rst example of such a norm is called the Frobenius norm; it is the one exception
that we alluded to above.
DEFINITION 6.5.2. The Frobenius norm of a matrix
A
=
[a
ij
]
m;n is deï¬ned by
jjAj
j
F
=
0
@
m
X
i=1
n
X
j
=1
ja
ij
j
2
1
A
1=2
THEOREM 6.5.3. The Frobenius norm is a matrix norm.
PROOF. Let
A and
B be matrices conformable for multiplication and suppose that
the rows of
A are
a
T
1
;
a
T
2
;
:
:
:
;
a
T
m, while the columns of
B are
b
1
;
b
2
;
:
:
:
;
b
n. Then

296
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
we have that
AB
=
[a
T
i
b
j
]; so that by applying the deï¬nition and the CBS inequality,
we obtain that
j
jAB
jj
F
=
0
@
m
X
i=1
n
X
j
=1


a
T
i
b
j


2
1
A
1=2

0
@
m
X
i=1
n
X
j
=1
ka
i
k
2
kb
j
k
2
1
A
1=2


k
Ak
2
F
kB
k
2
F

1=2
=
kAk
F
kB
k
F
The most important multiplicative norm comes from a rather general notion. Just as
every inner product â€œinducesâ€ a norm in a natural way, every norm on the standard
spaces induces a norm on matrices in a natural way. This type of norm is deï¬ned as
follows.
DEFINITION 6.5.4. The operator norm induced on matrices by a norm on the standard
spaces is deï¬ned by the formula
jjAj
j
=
sup
x6=0
jjAxj
j
jjxj
j
A useful fact about these norms is the following equivalent form.
jj
Ajj
=
sup
x6=0
jj
Axjj
jjxjj
=
sup
x6=0








A
x
jj
xjj








=
sup
jj
v
jj=1
jjAv
j
j
THEOREM 6.5.5. Every operator norm is a matrix norm.
PROOF. For a given matrix
A clearly
jjAj
j

0 with equality if and only if
Ax
=
0
for all vectors
x, which is equivalent to
A
=
0: The remaining two norm properties are
left as exercises. Finally, if
A and
B are conformable for multiplication, then
jjAB
j
j
=
sup
x6=0
jjAB
xj
j
jjxjj

jjAjj
sup
x6=0
jjB
xj
j
jjxjj
=
jjAj
j

jjB
jj
Incidentally, one difference between the Frobenius norm and operator norms is how the
identity
I
n is handled. Notice that
kI
n
k
F
=
n; while with any operator norm
k

k; we
have from the deï¬nition that
kI
n
k
=
1:
How do we compute these norms? The next result covers the most common cases.
THEOREM 6.5.6. Let
A
=
[a
ij
]
m;n
: Then
1.
jjAj
j
1
=
max
1im
f
P
n
j
=1
ja
ij
jg
2.
jjAj
j
1
=
max
1j
n
f
P
m
i=1
ja
ij
jg
3.
jjAj
j
2
=
(A
T
A)
1=2
PROOF. Items (1) and (3) are left as an exercises. For the proof of (2), use the
fact that
jjAjj
1
=
sup
jjv
j
j
1
=1
jjAv
j
j
1
: Now a vector has inï¬nity norm
1 if each of its
coordinates is
1 in absolute value. Notice that we can make the
ith entry of
Av as large
as possible simply by choosing
v so that the
jth coordinate of
v is
1 and agrees with

6.5. *OPERATOR NORMS
297
the sign of
a
ij
: Hence the inï¬nity norm of
Av is the maximum of the row sums of the
absolute values of the entries of
A, as stated in (2).
One of the more important applications of the idea of a matrix norm is the famous
Banach Lemma. Essentially, it amounts to a matrix version of the familiar geometric
series encountered in calculus.
THEOREM 6.5.7. Let
M be a square matrix such that
k
M
k
<
1 for some operator
Banach Lemma
norm
kk. Then the matrix
I
 M is invertible. Moreover,
(
I
 M
)
 1
=
I
+
M
+
M
2
+



+
M
k
+



and



(I
 M
)
 1




1=(1
 k
M
k
):
PROOF. Form the familiar telescoping series
(I
 M
)
 I
+
M
+
M
2
+



+
M
k

=
I
 M
k
+1
so that
I
 (I
 M
)
 I
+
M
+
M
2
+



+
M
k

=
M
k
+1
Now by the multiplicative property of matrix norms and fact that
kM
k
<
1


M
k
+1



k
M
k
k
+1
!
k
!1
0:
It follows that the matrix
lim
k
!1
 I
+
M
+
M
2
+



+
M
k

=
N exists and that
I
 (I
 M
)
B
=
0; from which it follows that
B
=
(I
 M
)
 1. Finally, note that


I
+
M
+
M
2
+



+
M
k



kI
k
+
k
M
k
+
kM
k
2
+



+
k
M
k
k

1
+
kM
k
+
k
M
k
2
+



+
kM
k
k

1
1
 kM
k
:
Now take the limit as
k
!
1 to obtain the desired result.
A fundamental idea in numerical linear algebra is the notion of the condition number
of a matrix
A: Roughly speaking, the condition number measures the degree to which
changes in
A lead to changes in solutions of systems
Ax
=
b: A large condition number
means that small changes in
A may lead to large changes in
x: In the case of an invertible
matrix
A, the condition number of
A is deï¬ned to be
cond
(A)
=
jjAjj




A
 1




Of course this quantity is norm dependent. In the case of an operator norm, the Banach
lemma has a nice application.
COROLLARY 6.5.8. If
A
=
I
+
N
; where
kN
k
<
1; then
cond(A)

1
+
kN
k
1
 kN
k

298
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
We leave the proof as an exercise.
We conclude with a very fundamental result for numerical linear algebra. The context
is a more general formulation of the problem which is discussed in Section 6.6. Here
is the scenario: suppose that we desire to solve the linear system
Ax
=
b; where
A is
invertible. Due to arithmetic error or possibly input data error, we end up with a value
x
+
Ã†
x which solves exactly a â€œnearbyâ€ system
(A
+
Ã†
A)(x
+
Ã†
x)
=
b
+
Ã†
b: (It
can be shown by using an idea called â€œbackward error analysisâ€ that this is really what
happens when many algorithms are used to solve a linear system.) The question is, what
is the size of the relative error
kÃ†
xk=kxk? As long as the perturbation matrix
kÃ†
Ak is
reasonably small, there is a very elegant answer.
THEOREM 6.5.9. Suppose that
A is invertible,
Ax
=
b;
(A
+
Ã†
A)(x
+
Ã†
x)
=
b
+
Ã†
b
Perturbation
Theorem
and
kA
 1
Ã†
Ak
=
c
<
1 with respect to some operator norm. Then
A
+
Ã†
A is invertible
and
j
Ã†
xk
kxk

cond(A)
1
 c

kÃ†
Ak
kAk
+
kÃ†
bk
kbk

PROOF. That the matrix
I
+
A
 1
Ã†
A follows from hypothesis and the Banach
lemma. Expand the perturbed equation to obtain
(A
+
Ã†
A)(x
+
Ã†
x)
=
Ax
+
Ã†
Ax
+
AÃ†
x
+
Ã†
A
Ã†
x
=
b
+
Ã†
b
Now subtract the terms
Ax
=
b from each side solve for
Ã†
x to obtain
(A
+
Ã†
A)Ã†
x
=
A
 1
(I
+
Ã†
A
 1
A)Ã†
x
=
 Ã†
Ax
+
Ã†
b
so that
Ã†
x
=
(I
+
Ã†
A
 1
A)
 1
A
 1
[ Ã†
Ax
+
Ã†
b]
Now take norms and use the additive and multiplicative properties and the Banach
lemma to obtain
kÃ†
xk

kA
 1
k
1
 c
[kÃ†
Axk
+
kÃ†
bk]
:
Next divide both sides by
kxk to obtain
kÃ†
xk
kxk

kA
 1
k
1
 c

kÃ†
Ak
+
kÃ†
bk
kxk

Finally, notice that
kbk

kAkkxk: Therefore,
1=kxk

kAk=kbk: Replace
1=kxk in
the right hand side by
kAk=kbk and factor out
kAk to obtain
kÃ†
xk
kxk

kA
 1
kkAk
1
 c

kÃ†
Ak
kAk
+
kÃ†
bk
kbk

which completes the proof, since by deï¬nition,
cond
A
=
kA
 1
kkAk:
If we believe that the inequality in the perturbation theorem can be sharp (it can!),
then it becomes clear how the condition number of the matrix
A is a direct factor in
how relative error in the solution vector is ampliï¬ed by perturbations in the coefï¬cient
matrix.

6.6. *COMPUTATIONAL NOTES AND PROJECTS
299
6.5 Exercises
1. Let
A
=
2
4
 1
2
2
2
 1
2
2
2
 1
3
5
Compute the Frobenius,
1 ;
2 ; and
1-norms of
A:
2. With
A as in Exercise 1, compute the condition number of
A using the inï¬nity norm.
3. Prove Corollary 6.5.8 by making use of the Triangle inequality and the Banach
lemma.
4. Use the Banach lemma to show that if
A is invertible, then so is
A
+
Ã†
A provided
that
kA
 1
Ã†
Ak
<
1:
5. Prove that for a square matrix
A;
jjAjj
1
=
max
1im
f
P
n
j
=1
ja
ij
jg:
6. Prove that for a square matrix
A;
jjAjj
2
=
(A
T
A)
1=2
7. Example 6.6.4 gives an upper bound on the error propagated to the solution of a
system due to right hand side error. How pessimistic is it? Experiment with several
different erroneous right hand sides of your own choosing and compare the actual error
with estimated error.
8. Let
A
=
2
4
3
2
0
1
4
1
3
5
(a) Use Householder matrices to ï¬nd a full QR factorization of
A:
(b) Use the result of (a) to ï¬nd the least squares solution to the system
Ax
=
b;
where
b
=
(1;
2;
3):
6.6. *Computational Notes and Projects
Error and Limit Measurements
We are going to consider a situation where inï¬nity norms are both more natural to a
problem and easier to use than the standard norm. This material is a simpliï¬ed treatment
of some of the concepts introduced in Section 6.5 and is independent of that section. The
theorem below provides a solution to this question: how large an error in the solution

300
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
to a linear system can there be, given that we have introduced an error in the right hand
side whose size we can estimate? (Such an error might be due to experimental error or
input error.) The theorem requires an extension of the idea of vector inï¬nity norm to
matrices for its statement.
DEFINITION 6.6.1. Let
A be an
n

n matrix whose rows are
r
1
;
r
2
;
:
:
:
;
r
n
: The in-
ï¬nity norm of the matrix
A is deï¬ned as
jjAjj
1
=
max
f
jjr
1
jj
1
;
jjr
2
jj
1
;
:
:
:
;
jjr
n
jj
1
g
If, moreover,
A is invertible, then the condition number of
A is deï¬ned to be
cond(A)
=
jjAj
j
1




A
 1




1
EXAMPLE 6.6.2. Let
A
=

1
2
1
4

: Find
j
jAjj
1,




A
 1




1 and
cond
(A):
SOLUTION. Here we see that
A
 1
=

2
 1
 1=2
1=2

: From the preceding deï¬nition
we obtain that
jjAjj
1
=
max
f
j1j
+
j2j
;
j1j
+
j4jg
=
5
and




A
 1




1
=
max

j2j
+
j 1j
;




 1
2




+




1
2





=
3
so it follows that
cond
(A)
=
5

3
=
15:
THEOREM 6.6.3. Suppose that the
n

n matrix
A is nonsingular,
Ax
=
b and
A(x
+
Ã†
x)
=
b
+
Ã†
b: Then
jjÃ†
xj
j
1
jjxj
j
1

cond(A)
jjÃ†
bj
j
1
jjbj
j
1
PROOF. Subtract the ï¬rst equation of the statement of the theorem from the second
one to obtain that
A
~
x
 Ax
=
A(
~
x
 x)
=
~
b
 b
=
Ã†
b
from which it follows that
Ã†
x
=
~
x
 x
=
A
 1
Ã†
b
Now write
A
 1
=
[c
ij
],
Ã†
b
=
[d
i
] and compute the
ith coordinate of
Ã†
x:
(Ã†
x)
i
=
n
X
j
=1
c
ij
d
j

6.6. *COMPUTATIONAL NOTES AND PROJECTS
301
so that if
r
i
=
(c
i1
;
c
i2
;
:
:
:
;
c
in
) is the
ith row of
A
 1, then
j(Ã†
x)
i
j

n
X
j
=1
jc
ij
j
jd
j
j

max
f
jd
1
j
;
:
:
:
;
jd
n
j
g
n
X
j
=1
jc
ij
j

jj
Ã†
bjj
1
jjr
i
jj
1
Therefore,
jjÃ†
xj
j
1

jjÃ†
bjj
1




A
 1




1
(6.6.1)
A similar calculation shows us that since
b
=
Ax,
jjbjj
1

jjxj
j
1
jjAj
j
1
Divide both sides by
jjbjj
1
j
jxjj
1 and obtain that
1
jjxj
j
1

jjAjj
1
1
jjbj
j
1
(6.6.2)
Now multiply the inequalities 6.6.1 and 6.6.2 together to obtain the asserted inequality
of the theorem.
EXAMPLE 6.6.4. Suppose we wish to solve the nonsingular system
Ax
=
b exactly,
where the coefï¬cient matrix
A is as in Example 6.6.2 but the right hand side vector
b
is determined from measured data. Suppose also that the error of measurement is such
that the ratio of the largest error in any coordinate of
b to the largest coordinate of
b
(this ratio is called the relative error) is no more than
0:01 in absolute value. Estimate
the size of the relative error in the solution.
SOLUTION. In matrix notation, we can phrase the problem in this manner: let the
correct value of the right hand side be
b and the measured value of the right hand side
be
~
b, so that the error of measurement is the vector
Ã†
b
=
~
b
 b: Rather than solving the
system
Ax
=
b, we end up solving the system
A
~
x
=
~
b
=
b
+
Ã†
b, where
~
x
=
x
+
Ã†
x:
The relative error in data is the quantity
jjÃ†
bj
j
1
=
j
jbjj
1, while the relative error in the
computed solution is
jjÃ†
xj
j
1
=
j
jxj
j
1
: This sets up very nicely for an application of
Theorem 6.6.3. Furthermore, we already calculated
cond
(A)
=
15 in Example 6.6.2.
It follows that the relative error in the solution satisï¬es the inequality
jjÃ†
xj
j
1
jjxj
j
1

15

0:01
=
0:15
In other words, the relative error in our computed solution could be as large as
15%:
A Practical QR algorithm
In the preceding section we saw that the QR factorization can be used to solve systems
including least squares. We also saw the factorization as a consequence of the Gram-
Schmidt algorithm. As a matter of fact, the classical Gram-Schmidt algorithm which we
have presented has certain numerical stability problems when used in practice. There
is a so-called modiï¬ed Gram-Schmidt algorithm that performs better. However, there

302
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
is another approach to QR factorization that avoids Gram-Schmidt altogether. This ap-
proach uses the Householder matrices we introduced in Section 4.3. It is more efï¬cient
and stable than Gram-Schmidt. If you use a MAS to ï¬nd the QR factorization of a
matrix, it is likely that this is the method used by the system.
The basic idea behind this Householder QR is to use a succession of Householder ma-
trices to zero out the lower triangle of a matrix, one column at a time. The key fact
about Householder matrices is the following application of these matrices:
THEOREM 6.6.5. Let
x;
y be nonzero vectors in
R
n of the same length. Then there is a
Householder matrix
H
v such that
H
v
x
=
y.
PROOF. Let
v
=
x
 y. Then we see that
(x
+
y
)
T
(x
 y
)
=
x
T
x
 x
T
y
 y
T
x
+
y
T
y
=
x
T
x
 y
T
y
=
0
since
x and
y have the same length. Now write
x
=
1
2
f
(
x
 y
)
+
(x
+
y
)g
=
p
+
u
and obtain from Theorem 4.3.10 that
H
v
x
=
 p
+
u
=
1
2
f (x
 y
)
+
(x
+
y
)g
=
2y
2
=
y
which is what we wanted to show.
Now we have a tool for massively zeroing out entries in a vector of the form
x
=
(x
1
;
x
2
;
:
:
:
;
x
n
): Set
y
=
(
k
xk
;
0;
:
:
:
;
0) and apply the preceding theorem to con-
struct Householder
H such that
H
v
x
=
y
: It is standard to choose the
 to be the
negative of the sign of
x
1
: In this way, the ï¬rst term will not cause any loss of accuracy
to subtractive cancellation. However, any choice of
 works ï¬ne in theory. We can
picture this situation schematically very nicely by representing possibly nonzero entries
by an â€˜â€™ in the following simple version:
x
=
2
6
6
4




3
7
7
5
     !
H
v
2
6
6
4

k
xk
0
0
0
3
7
7
5
=
H
v
x
We can extend this idea to zeroing out lower parts of
x only, say
x
=

z
w

=
2
6
6
4
z



3
7
7
5 byusing
y
=
2
6
6
4
z

k
w
k
0
0
3
7
7
5 sov
=
2
6
6
4
0



3
7
7
5 and
H
v
x
=
2
6
6
4
0

0
0
3
7
7
5
:
We can apply this idea to systematically zero out subdiagonal entries by successive
multiplication by Householder (hence orthogonal) matrices; schematically we have this
representation of a full rank
m

n matrix
A
A
=
2
6
6
4












3
7
7
5
     !
H
1
2
6
6
4



0


0


0


3
7
7
5
     !
H
2
2
6
6
4



0


0
0

0
0

3
7
7
5

6.6. *COMPUTATIONAL NOTES AND PROJECTS
303
     !
H
3
2
6
6
4



0


0
0

0
0
0
3
7
7
5
=
R
so that
H
3
H
2
H
1
A
=
R. Now we can check easily from the deï¬nition of a Householder
matrix
H that
H
T
=
H
=
H
 1
: Thus, if we set
Q
=
H
 1
1
H
 1
2
H
 1
3
=
H
1
H
2
H
3
;
it follows that
A
=
QR. Notice that we donâ€™t actually have to carry out the multipli-
cations to compute
Q unless they are needed, and the vectors needed to deï¬ne these
Householder matrices are themselves easily stored in a single matrix. What we have
here is just a bit different from the QR factorization discussed in the last section. Here
the matrix
Q is a full
m

m matrix and
R is the same size as
A: Even if
A is not full
column rank, this procedure will work, provided we simply skip construction of
H in
the case that there are no nonzero elements to zero out in some column. Consequently,
we have essentially proved the following theorem, which is sometimes called a full QR
factorization, in contrast to the reduced QR factorization of Theorem 6.4.10.
THEOREM 6.6.6. Let
A be a real
m

n matrix. Then there exists an
m

m orthogonal
Full QR
Factorization
matrix
Q and
m

n upper triangular matrix
R such that
A
=
QR
:
Actually, all of the results we have discussed regarding QR factorization carry over to
complex matrices, provided we replace orthogonal matrices by unitary matrices and
transposes by Hermitian transposes.
Project Topics
Project: Testing Least Squares Solvers
The object of this project is to test the quality of the solutions of three different methods
for solving least squares problems
Ax
=
b:
(a) Solution by solving the associated normal equations by Gaussian elimination.
(b) Solution by reduced QR factorization obtained by Gram-Schmidt.
(c) Solution by full QR factorization by Householder matrices.
Here is the test problem: suppose we want to approximatethe curve
f
(x)
=
e
sin
(6x)
;
0

x

1 by a tenth degree polynomial. The input data will be the sampled values of
f
(x)
at equally spaced nodes
x
=
k
h;
k
=
0;
1;
:
:
:
;
20;
h
=
0:05: The fact that
f
(x)
=
c
0
+
c
1
x
+



c
10
x
10
gives
21 equations for the
11 unknown coefï¬cients
c
k
;
k
=
0;
1;
:
:
:
;
20: The coefï¬cient
matrix that results from this problem is called a Vandermondematrix. Your MAS should
have a have a built-in command for construction of such a matrix.
Procedure: First set up the system matrix
A and right hand side matrix
b. Method (a)
is easily implemented on any CAS or MAS. The built-in procedure for computing a
QR factorization will very likely be Householder matrices which will take care of (c).
You will need to check the documentation to verify this. The Gram-Schmidt method of
ï¬nding QR factorization will have to be programmed by you.

304
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
Once you have solved the system by these three methods, make out a table that has the
computed coefï¬cients for each of the three methods. Then make plots of the difference
between the function
f
(x) and the computed polynomial for each method. Discuss your
results.
There are a number of good texts which discuss numerical methods for least squares;
see, e.g., [3]
Project: Approximation Theory
Suppose you work for a manufacturer of calculators, and are involved in the design of
a new calculator. The problem is this : as one of the â€œfeaturesâ€ of this calculator, the
designers decided that it would be nice to have a key which calculated a transcendental
function, namely,
f
(x)
=
sin(
x);
 1

x

1 Your job is to come up with an
adequate way of calculating
f
(x), say with an error no worse than .001
Polynomials are a natural idea for approximating functions. From a designerâ€™s point of
view they are particularly attractive because they are so easy to implement. Given the
coefï¬cients of a polynomial, it is easy to design a very efï¬cient and compact algorithm
for calculating values of the polynomial. Such an algorithm, together with the coefï¬-
cients of the polynomial, would ï¬t nicely into a small ROM for the calculator, or could
even be microcoded into the chip.
Your task is to ï¬nd a low degree polynomial that approximates
sin(
x) to within the
speciï¬ed accuracy. For comparison, ï¬nd a Taylor polynomial of lowest degree for
sin
x
that gives sufï¬cient accuracy. Next, use the projection problem idea to project the func-
tion
sin
x
2
C
[ 1;
1] with the standard inner product, into the subspace
P
n of polyno-
mials of degree at most
n: You will need to ï¬nd the smallest
n that gives a projection
whose difference from
sin
x is at most
0:001 on the interval
[ 1;
1]: Is it lower degree
than the best Taylor polynomial approximation?
Use a CAS to do the computations and graphics. Then report on your ï¬ndings. Include
graphs that will be helpful in interpreting your conclusions. Also, give suggestions on
how to compute this polynomial efï¬ciently.
A Report Topic: Fourier Analysis
This project will introduce you to a very fascinating and important topic known as
Fourier analysis. The setting is as follows: we are interested in ï¬nding approximations
to functions in the vector space
C
2 of continuous periodic functions on the closed inter-
val
[ 
;

]: This vector space becomes an inner product space with the usual deï¬nition
h
f
;
g
i
=
Z

 
f
(x)g
(x)
dx:
In this space the sequence of trigonometric functions
1
p
2
;
cos
x
p

;
sin
x
p

;
cos
2x
p

;
sin
2x
p

;
:
:
:
;
cos
k
x
p

;
sin
k
x
p

;
:
:
:
forms an orthonormal set. Therefore, we can form the ï¬nite dimensional subspaces
V
n
spanned by the ï¬rst
2n
+
1 of these elements and immediately obtain an orthonormal
basis of
V
n
: We can also use the machinery of projections to approximate any function
f
(x)
2
C
2 by its projection into the various subspaces
V
n
: The coefï¬cients of the
orthonormal basis functions in the projection formula of Deï¬nition 6.3.5 as applied

6.6. *COMPUTATIONAL NOTES AND PROJECTS
305
to a function
f
(x) are called the Fourier coefï¬cients of
f
(x): They are traditionally
designated by the symbols
a
0
2
;
a
1
;
b
1
;
a
2
;
b
2
;
:
:
:
;
a
k
;
b
k
;
:
:
:
In the ï¬rst part of this project you will write a brief introduction to Fourier analysis in
which you exhibit formulas for the Fourier coefï¬cients of a function
f
(x) and explain
the form and meaning of the projection formula in this setting. Try to prove that the
trigonometric functions given above are an orthonormal set. At minimum provide a
proof for the ï¬rst three functions.
In the second part you will explore the quality of these approximations for various test
functions. The test functions are speciï¬ed on the interval
[ 
;

] and then this graph is
replicated on adjacent intervals of length
2
; so they are periodic.
1.
f
(x)
=
sin
x
2

2.
g
(x)
=
x(x
 
)(x
+

)
3.
h(x)
=
x
Notice that the last function violates the continuity condition.
For each test function you should prepare a graph that includes the test function and
at least two projections of it into the
V
n
;
n
=
0;
1;
:
:
:
: Discuss the quality of the
approximations and report on any conclusions that you can draw from this data. You
will need a MAS or CAS to carry out the calculations and graphs, as the calculations
are very detailed. If you are allowed to do so, you could write your report up in the form
of a notebook.
6.6 Exercises
1. Example 6.6.4 gives an upper bound on the error propagated to the solution of a
system due to right hand side error. How pessimistic is it? Experiment with several
different erroneous right hand sides of your own choosing and compare the actual error
with estimated error.
2. Let
A
=
2
4
3
2
0
1
4
1
3
5
(a) Use Householder matrices to ï¬nd a full QR factorization of
A:
(b) Use the result of (a) to ï¬nd the least squares solution to the system
Ax
=
b;
where
b
=
(1;
2;
3):

306
6. GEOMETRICAL ASPECTS OF ABSTRACT SPACES
Review
Chapter 6 Exercises
1. Let the vector space
V
=
R
4, equipped with the standard inner product.
(a) Apply the Gram-Schmidt algorithm to vectors
(1;
1;
1;
1);
(4;
2;
4;
2);
(0;
0;
0;
2) to
obtain an orthogonal set
fv
1
;
v
2
;
v
3
g:
(b) Normalize the orthogonal list obtained to obtain an orthonormal set
fw
1
;
w
2
;
w
3
g:
(c) Use this to ï¬nd the projection matrix for the subspace spanned by these vectors.
2. Let
A
=
2
4
3
2
0
1
4
1
3
5
(a) Use Householder matrices to ï¬nd a full QR factorization of
A:
(b) Use the result of (a) to ï¬nd the least squares solution to the system
Ax
=
b; where
b
=
(1;
2;
3):
3. Determine if the formula
h(u;
v
);
(x;
y
)i
=
ux
 v
y deï¬nes an inner product on
R
2
:
4. Find the projection matrix into the row space of the matrix
2
4
3
1
0
0
1
 1
0
0
1
3
5
:
5. Use the Gram-Schmidt algorithm to expand the orthogonal set
(1;
1;
1);
(1;
 2;
1)
into an orthogonal basis of
R
3 and then normalize this basis to obtain an orthonormal
basis.
6. Suppose that
A is an
n

n matrix of rank
n
 1: Show that all rows of
adj
A are
multiples of each other. Hint: Use the adjoint formula and orthogonal complements.
7. Find the orthogonal complement of
V
=
spanf1
+
x
+
x
2
g in
W
=
P
2 where
W
has the usual function space inner product
f
;
g
=
R
1
0
f
(x)g
(x)
dx:
8. Given an orthonormal basis
u
1
;
u
2
;
u
3 of
R
3
; show that
u
1
u
T
1
+
u
2
u
T
2
+
u
3
u
T
3
=
I
where each term is a projection matrix in the sense of Exercise 8 and the product of any
two distinct terms is
0: (Such an expression is called a resolution of the identity .)

APPENDIX A
Table of Symbols
Symbol
Meaning
Reference
;
Empty set
Page 9
2
Member symbol
Page 9

Subset symbol
Page 9

Proper subset symbol
Page 9
\
Intersection symbol
Page 9
[
Union symbol
Page 9
 !
P
Q
Displacement vector
Page 127
j
z
j
Absolute value of complex
z
Page 12
j
A
j
determinant of matrix
A
Page 98
jj
u
jj
Norm of vector
u
Page 185
jj
u
jj
p
p-norm of vector
u
Page 262
u

v
Standard inner product
Page 189
hu;
v
i
Inner product
Page 267
adj
A
Adjoint of matrix
A
Page 106
A
H
Hermitian transpose of matrix
A
Page 77
A
T
Transpose of matrix
A
Page 77
C
(A)
Column space of matrix
A
Page 153
cond
(A)
Condition number of matrix
A
Page 297
C
[a;
b]
Function space
Page 129
C
Complex numbers
a
+
bi
Page 11
C
n
Standard complex vector space
Page 127
comp
v
u
Component
Page 196
A
cof
Cofactor matrix of
A
Page 11
z
Complex conjugate of
z
Page 12
Ã†
ij
Kronecker delta
Page 106
dim
V
Dimension of space
V
Page 163
det
A
Determinant of
A
Page 97
domain(T
)
Domain of operator
T
Page 158
diagf
1
;

2
;
:
:
:
;

n
g
Diagonal matrix with entries

1
;

2
;
:
:
:
;

n on diagonal
Page 225
E
ij
Elementary row operation switching
ith and
jth rows
Page 22
E
i
(c)
Elementary row operation multiplying
ith row by
c
Page 22
E
ij
(d)
Elementary operation adding
d times
jth row to
ith row
Page 22
307

308
A. TABLE OF SYMBOLS
Symbol
Meaning
Reference
E

(A)
Eigenspace
Page 216
H
v
Householder matrix
Page 206
I
;
I
n
Identity matrix,
n

n identity
Page 73
=(z
)
Imaginary part of
z
Page 11
k
er
(T
)
Kernel of operator
T
Page 158
M
ij
(A)
Minor of
A
Page 99
M
(A)
Matrix of minors of
A
Page 106
max
fa
1
;
a
2
;
:
:
:
;
a
m
g
Maximum value
Page 35
minfa
1
;
a
2
;
:
:
:
;
a
m
g
Minimum value
Page 35
N
(A)
Null space of matrix
A
Page 153
N
Natural numbers
1;
2;
:
:
:
Page 10
n
ull
A
Nullity of matrix
A
Page 35
P
Space of polynomials of any degree
Page 137
P
n
Space of polynomials of degree

n
Page 137
pro
j
v
u
Projection vector along a vector
Page 196
pro
j
V
u
Projection vector into subspace
Page 281
Q
Rational numbers
a=b
Page 10
<(z
)
Real part of
z
Page 11
R(A)
Row space of matrix
A
Page 153
R
(
)
Rotation matrix
Page 149
R
Real numbers
Page 10
R
n
Standard real vector space
Page 127
R
m;n
Space of
m

n real matrices
Page 128
range
(T
)
Range of operator
T
Page 158
rank
A
Rank of matrix
A
Page 34
(A)
Spectral radius of
A
Page 232
spanfS
g
Span of vectors in
S
Page 138
target
(T
)
Target of operator
T
Page 158
V
?
Orthogonal complement of
V
Page 286
Z
Integers
0;
1;
2;
:
:
:
Page 10
[T
]
B
;C
Matrix of operator
T
Page 175

tensor symbol
Page 114

APPENDIX B
Solutions to Selected Exercises
Section 1.1, Page 7
1 (a)
x
=
 1,
y
=
1: (b)
x
=
2,
y
=
 2,
z
=
1: (c)
x
=
2,
y
=
1
2. (a) is linear, and (b) is not linear. (a) in
standard format is
x
 y
 z
=
 2
3x
 y
=
4
:
3. (a)
m
=
3,
n
=
3,
a
11
=
1,
a
12
=
 2,
a
13
=
1,
b
1
=
3,
a
21
=
0,
a
22
=
1,
a
23
=
0,
b
2
=
2,
a
31
=
 1,
a
32
=
0,
a
33
=
1,
b
3
=
1.
4.
2y
1
 y
2
=
1
49
f
(1=7)
 y
1
+
2y
2
 y
3
=
1
49
f
(2=7)
 y
2
+
2y
3
 y
4
=
1
49
f
(3=7)
 y
3
+
2y
4
 y
5
=
1
49
f
(4=7)
 y
4
+
2y
5
 y
6
=
1
49
f
(5=7)
 y
5
+
2y
6
=
1
49
f
(6=7)
5. The equations are
:8x
 :1z
=
2
 :4x
+
:9z
=
3
so
x
=
105
34
;
z
=
80
17
:
6.
 :8x
+
:1y
+
:4z
+
:4w
=
0
:3x
 :6y
+
:2z
+
:1w
=
0
:3x
+
:4y
 :8z
+
:3w
=
0
:2x
+
:1y
+
:2z
 :8w
=
0
7.
a
+
b
+
c
=
1
a
+
2b
+
4c
=
1
a
+
3b
+
9c
=
2
The equation that comes from vertex
v
1 is
 x
1
+
x
4
 x
5
=
0, and vertex
v
3 is
x
2
 x
3
=
0:
Section 1.2, Page 17
1. (a)
f0;
1g (b)
fxjx
2
Zand
x
>
1g (c)
fxjx
2
Zand
x

 1g (d)
B
2. (a)
e
3
i=2, (b)
p
2e

i=4, (c)
2e
2
i=3, (d)
e

i, (e)
2
p
2e
7
i=4
3. (a)
1
+
8i, (b)
3
5
+
4
5
i, (c)
10
+
10i, (d)
 3
5
+
4
5
i
4. (a)
2
5
 i
5, (b)
 2
+
i, (c)
fa
+
bija;
b
2
R
and
b
=
2a
+
1g
309

310
B. SOLUTIONS TO SELECTED EXERCISES
5. (a)
z
=
 1
2

p
11
2
i, (b)
z
=

p
3
2
+
i
2,
(c)
z
=
1

(
 p
2
p
2
+2
2
+
p
2
p
2 2
2
i)
6.
(a)
z
=
e
0
;
e
2
i=3
;
e
4
i=3,
z
=
1;
 1
2
+
p
3
2
i;
 1
2
 p
3
2
i,
(b)
z
=
2e

i
;
2e

i=3
;
2e
5
i=3,
z
=
 2;
1
+
p
3i;
1
 p
3
i
11.
Let
z
1
=
x
1
+
iy
1
and
z
2
=
x
2
+
iy
2
:
Then
z
1
z
2
=
(x
1
x
2
 y
1
y
2
)
+
i(x
1
y
2
+
x
2
y
1
): Thus
jz
1
z
2
j
2
=
(x
1
x
2
 y
1
y
2
)
2
+
(x
1
y
2
+
x
2
y
1
)
2
=
x
2
1
x
2
2
+
y
2
1
y
2
2
+
x
2
1
y
2
2
+
x
2
2
y
2
1
=
(x
2
1
+
y
2
1
)(x
2
2
+
y
2
2
)
=
jz
1
j
2
jz
2
j
2.
Since
jz
1
j,
jz
2
j, and
jz
1
z
2
j all have positive
values, it follows then that
jz
1
z
2
j
=
jz
1
jjz
2
j:
12.
e
i(
+ 
)
=
cos
(
+
 
)
 i
sin(
+
 
)
=
cos

cos
 
 sin

sin
 
 i
sin

cos
 
 i
cos

=
sin
 
=
(cos

 i
sin

)(cos
 
 i
sin
 
)
=
e
i
e
i 
13. The equation should have
5 roots.
Section 1.3, Page 27
2 (a)
x
=
20;
y
=
 11, (b)
x
1
=
3;
x
2
=
 2;
x
3
=
1; (c)
x
1
=
3;
x
2
=
 5:
3. Each augmented matrix is
3

5(a)
A
=
2
4
1
1
0
1
1
2
2
1
1
1
2
2
0
2
2
3
5, size of
A is
3

5,
the general solution is
x
1
=
1
 x
2
 x
4
x
3
=
x
4
 1
x
2
;
x
4 are free.
(b)
A
=
2
4
0
0
1
1
0
 2
 4
1
0
0
3
6
 1
1
0
3
5,
size of
A is
3

5, the general solution is
x
1
=
 2x
2
x
2
=
0
x
3
=
0
x
2 is free.
4. (a)
x
1
=
4
x
3
=
2
x
2 is free
(b)
x
1
=
1
x
2
=
2
x
3
=
2
(c) is in-
consistent.
5. (a)
x
1
=
2
3
b
1
+
1
3
b
2
x
2
=
 1
3
b
1
+
1
3
b
2
(b) If
b
2
 2b
1
6=
0; then the system is inconsistent.
Otherwise, the solution is
x
1
=
b
1
+
x
2 and
x
2 arbitrary.
(a)
2x
1
+
x
2
+
7x
3
=
 1
3x
1
+
2x
2
 2x
4
=
1
2x
1
+
2x
2
+
2x
3
 2x
4
=
4
(b)
x
1
+
x
2
+
x
3
 x
4
=
2
2x
1
+
x
2
 2x
4
=
1
2x
1
+
2x
2
+
2x
3
 2x
4
=
4
6. (b)
x
1
=
 1
+
x
3
+
x
4
x
2
=
3
 2x
3
x
3
;
x
4 are free.
7.
x
=
131
85
w
y
=
128
85
w
z
=
29
17
w
is the systemâ€™s solution
which has a nontrivial solution consisting of
positive numbers if
w
>
0:
8. Applying the same operations to
b, you
ï¬nd
b
0
=
2
4
b
2
 b
1
2b
1
 b
2
b
3
 2b
1
3
5
: Since the bottom
row in the coefï¬cient matrix is all zeros after
Gauss-Jordan elimination, the system is only
consistent when
b
3
 2b
1
=
0:
9. (a)
x
1
=
1
x
2
=
0
x
3
=
0
(b)
x
1
=
0
x
2
=
0
x
3
=
0
(c)
x
1
=
1
x
2
=
1
x
3
=
1
(a)
2x
 y
+
3xy
=
0
4x
+
2y
 xy
=
0
(b)
y
z
+
3xz
 xy
=
0
y
z
+
2xy
=
0
10 (a) If
x
=
0 then the system reduces to
y
=
0: Similarly for
y
=
0: So
(0;
0) is a so-
lution. If neither
x nor
y is zero, then divide
both equations by
xy to get a linear system
in the variables
1=x;
1=y
; then solve this sys-
tem.
11.
y
1
=
40
2187,
y
2
=
77
2187,
y
3
=
4
81,
y
4
=
130
2187 ,
y
5
=
140
2187 ,
y
6
=
5
81,
y
7
=
112
2187 ,
y
8
=
68
2187

B. SOLUTIONS TO SELECTED EXERCISES
311
15.
Physically
meaningful
solu-
tions
are
those
that
are
nonnegative.
Coefï¬cient
matrix
of
the
system
is
2
6
6
6
6
4
1
0
0
 1
 1
0
0
 1
1
0
0
0
 1
0
0
 1
1
0
0
0
0
0
0
 1
1
0
0
1
0
0
0
0
1
1
 1
3
7
7
7
7
5
:
Section 1.4, Page 38
1. (d) and (e) are in reduced row echelon
form.
2. (a)
2
4
1
0
5
2
0
1
1
2
0
0
0
3
5,
rank
=
2,
nul
l
ity
=
1:
(b)
2
4
1
0
0
17
3
0
1
0
 33
0
0
1
2
3
5,
rank
=
3,
nul
l
ity
=
1:
(c)

1
0
0
1
0
1
0
1

,
rank
=
2,
nul
l
ity
=
2:
(d)
2
4
1
0
3
0
1
 1
0
0
0
3
5,
rank
=
2,
nul
l
ity
=
1:
(e)

1
1
0
22
9
0
0
1
2
9

,
rank
=
2,
nul
l
ity
=
2:
3. Let
A be the augmented matrix and
C
be the coefï¬cient matrix. (a)
rank(A)
=
2,
rank
(C
)
=
2, (b)
rank(A)
=
3,
rank(C
)
=
3
5. (a)
2; (b)
0; (c)
3; (d)
1
6.
0

rank(A)

2
7. (a) true, (b) true, (c) false
8.

1
b
a
0
0

9 The condition is that the
m

n coefï¬cient
matrix of the system have rank
n:
Section 1.5, Page 47
2.
(a)
rank(A)
=
3,
(b)
2
6
6
4
1
3
0
4
2
0
0
0
0
1
2
0
0
0
0
0
0
0
0
1
1
3
0
0
0
0
0
0
0
3
7
7
5
:
3. The total work done at the
nth stage is
n
+
2
[1
+
2
+
:
:
:
+
(n
 1)]
=
n
2
+n
2
:
Section 1.6, Page 47
1. (a)
2
p
5, (b)
7
+
6i
2. (a)
z
=
6
5
 8
5
i, (b)
z
=

p
2

p
2i
3. (a)
p
2
2
e

i=4,
1
2
+
i
2
5. Let
A be the augmented matrix of the
system.
(a)
rref
(A)
=
2
4
1
0
0
 4
2
0
1
0
2
0
0
0
1
1
1
3
5
:
The systemâ€™s solution is
x
1
=
2
+
4x
4
x
2
=
 2x
4
x
3
=
1
 x
4
x
4 is free.
(b)
rank
(A)
=
3,
rref
(A)
=
E
12
( 2)E
23
(1)E
2
( 1)E
3
(
1
2
)E
21
( 1)A:
(c) Since the rank of the coefï¬cient matrix is
3, the coefï¬cient matrix augmented with any
right hand side will always be consistent and
therefore have a solution.
6. (a)
3, (b) true, (c) true, (d)
1
Section 2.1, Page 54
2. (a) not possible, (b)

 1
0
1
 1
 1
 2

,
(c)

0
0
0
0
0
0


312
B. SOLUTIONS TO SELECTED EXERCISES
3.
X
=

 1
0
1
 1
 1
 2

4 (a)
x

1
2

+
y

2
0

+
z

0
 1

8. Let
A
=
[a
ij
],
B
=
[b
ij
],
C
=
[
c
ij
]
: So
(A
+
B
)
+
C
=
[a
ij
+
b
ij
]
+
[c
ij
]
=
[a
ij
+
b
ij
+
c
ij
]
=
[a
ij
]
+
[b
ij
+
c
ij
]
=
A
+
(B
+
C
).
9. Let
A
=
[a
ij
],
B
=
[
b
ij
], and
c;
d be
scalars. So
(c
+
d)A
=
[(c
+
d)a
ij
]
=
[ca
ij
+
da
ij
]
=
[ca
ij
]
+
[da
ij
]
=
cA
+
dA.
Similarly, you can show
c(A
+
B
)
=
cA
+
cB
:
Section 2.2, Page 60
1.
(a)
2
4
1
 2
4
0
1
 1
 1
0
4
3
5
2
4
x
1
x
2
x
3
3
5
=
2
4
3
2
1
3
5
(b)
2
4
1
 1
 3
2
2
4
 1
0
1
3
5
2
4
x
y
z
3
5
=
2
4
3
10
3
3
5
2.
(I
+
A
+
A
2
)(I
 A)
=

 1
 6
 3
 4

=
I
 A
3
3.
(a)
x
1
2
4
1
0
 1
3
5
+
x
2
2
4
 2
1
0
3
5
+
x
3
2
4
4
 1
4
3
5
=
2
4
3
2
1
3
5,
(b)
x
2
4
1
2
1
3
5
+
y
2
4
 1
2
0
3
5
+
z
2
4
 3
4
 1
3
5
=
2
4
3
10
 3
3
5
4. (a)
[19
+
3i], (b)

6
8
3
4

, (c) impossi-
ble
5. (a)

3
 2
0
6
 4
0

, (b) not possible, (c)
2
4
 15
15
3
 5
4
 2
 24
25
3
3
5
6. (a)
A
=

0
1
2
3

and
B
=

1
2
1
2

work.
7. Let
A
mn
=
[a
ij
] and
B
mn
=
[b
ij
]
:
If
b
=
[1;
0;
:
:
:
;
0]
T ,
2
6
4
a
11
0



0
...
...
a
m1
0



0
3
7
5
=
2
6
4
b
11
0



0
...
...
b
m1
0



0
3
7
5 so
a
11
=
b
11, etc.
By
similar computations, you can show that for
each
ij,
a
ij
=
b
ij so
A
=
B
:
8. (b) is not nilpotent, the others are. For (d)
A
3
=
0 so
A is nilpotent.
12. Let
A
=
[a
ij
],
B
=
[
b
ij
], and
c be
a scalar.
Then the
ij
th entry of
c(AB
)
=
c(
P
k
a
ik
b
k
j
): The
ij
th entry of
(cA)B
=
P
k
(ca
ik
)b
k
j
=
c
P
k
a
ik
b
k
j
= the
ij
th en-
try of
c(AB
): So
14.
f
(A)
=

 12
10
 5
 12

:
Section 2.3, Page 71
6. (a)
[
0:3;
0:4;
0:3]
T , (b) the system does
tend toward

5
29
;
15
29
;
9
29

T

B. SOLUTIONS TO SELECTED EXERCISES
313
8. (2)
A
=
2
6
6
6
6
4
0
0
0
0
1
1
0
0
0
1
0
1
0
0
0
0
1
1
0
0
0
0
0
1
0
3
7
7
7
7
5
(3)
vertex
power
1
2
2
4
3
3
4
5
5
3
Vertices
4 is the
strongest.
9. (b)
node
power
1
7
2
8
3
10
4
11
5
10
, (b)
939
Section 2.4, Page 82
2. (a) Interchange columns
1 and
2 of
A:
3. (a) upper triangular, triangular, strictly tri-
angular and tridiagonal.
8. (a)
E
12
(3), (b)
E
ij
( a), (c)
E
31
(2), (d)
E
2
(3)
9. (a)
I
2
=
E
12
( 2)E
21
( 1)

1
2
1
3

10. (a)

1
 3
2

T
=
2
4
1
 3
2
3
5
; so
this matrix is not symmetric or Hermitian.
11.
(E
23
(4))
T
=
2
4
1
0
0
0
1
0
0
4
1
3
5
=
E
32
(4)
12. (a) false, (b) true, (c) false, (d) false, (e)
false
16. Let
Q(x;
y
;
z
)
=
x
T
Ax where
x
=
[x;
y
;
z
]
T and
A
=
2
4
2
2
 6
0
1
4
0
0
1
3
5
:
18.
(A
H
A)
H
=
A
H
(A
H
)
H
=
A
H
A so
A
H
A is Hermitian. Similarly, you can show
AA
H is Hermitian.
19.
Since two vectors,
a and
b, will
each have
rank

1,
rank(ab
T
)

min
frank(a);
rank
(b
T
)g

1:
23.
If
N
=
2
6
4
0

...
...
0



0
3
7
5,
N
2
=
2
6
6
6
6
4
0
0

...
...
...
...
...
0
0






0
3
7
7
7
7
5
so
N
2 has zeros
along the ï¬rst superdiagonal (in bold).
By
similar computation,
N
3 has zeros along the
ï¬rst and second superdiagonals, etc. There-
fore
N
n 1 has zeros along all of its super-
diagonals so
N
n 1
=
0:
Section 2.5, Page 94
1.
(a)
2
4
1
2
1
2
 1
2
0
1
2
0
1
2
1
2
1
2
3
5
(b)
2
4
1
 1
0
 1
2
 1
1
 1
1
3
5 (c) inverse does not
exist,
(d)
2
6
6
4
1
2
 1
2
 1
2
1
2
0
1
1
 1
0
0
1
2
0
0
0
0
1
3
7
7
5
(e)

6
13
+
4
13
i
 4
13
 7
13
i
2
13
 3
13
i
3
13
+
2
13
i


314
B. SOLUTIONS TO SELECTED EXERCISES
2.
(a)
A
 1
=
2
4
1
2
1
2
 1
2
0
1
2
0
1
2
1
2
1
2
3
5,
x
=
2
4
1
0
2
3
5
:
3. (a)
x
=

20
 11

, (c)
x
=

3
 5

:
7.
If
A
3
 2A
+
3I
=
0, then
(
2
3
I
 1
3
A
2
)A
=
I
: So
2
3
I
 1
3
A
2 is the inverse
of
A:
8.
(a)
E
21
(3),
E
21
( 3), (b)
E
2
( 2),
E
2
( 1=2)
13.
(I
+
N
+
N
2
+
:
:
:
+
N
k
 1
)(I
 N
)
=
(I
+
N
+
N
2
+
:
:
:
+
N
k
 1
)I
 (I
+
N
+
N
2
+
:
:
:
+
N
k
 1
)N
=
(I
+
N
+
N
2
+
:
:
:
+
N
k
 1
)
 (N
+
N
2
+
N
3
+
:
:
:
+
N
k
)
=
I
 N
k.
14.
If
N is nilpotent,
(I
 N
)
 1
=
I
+
N
+
N
2
+
:
:
:
+
N
k
 1
:
(a)
2
4
1
 1
2
0
1
1
0
0
1
3
5
 1
=
2
4
1
1
 3
0
1
 1
0
0
1
3
5
:
15.
f
(x
3
)

0:7391
Section 2.6, Page 104
2. (a)
3, (b)
1
+
i, (c)
1, (d)
 70, (e)
 6, (f)
 6; (g)
1: All of the matrices are invertible.
3.
det
A
=
 5,
det
B
=
4, and
det
AB
=
 20 so
det
AB
=
det
A
det
B
: Check that
det
A
T
=
 5 and
det
B
T
=
4:
6
det
V
=
(x
1
 x
0
)(x
2
 x
0
)(x
2
 x
1
):
7.
det
A
det
A
 1
=
det
AA
 1
=
det
I
=
1
8.
Let
A
=

1
1
 i
0
1
+
i

:
Then
det
A
H
=
1
 i and
det
A
=
1
+
i so
det
A
H
6=
det
A:
In general,
det
A
H
=
det
A: By D7 and
the deï¬nition of
A
H,
det
A
H
=
det
A:
Two facts about conjugates in section 1.2 are
z
1
+
z
2
=
z
1
+
z
2 and
z
1
z
2
=
z
1
:
z
2
: Since
the computation of
det
A is a combination of
taking the products and sums of conjugates
of the coefï¬cients of
A,
det
A
H
=
det
A
:
9. Let
A be a square matrix with a non-zero
determinant.
By determinantal law D2, if
you multiply one row of
A by the scalar mul-
tiple zero, the determinant of the new matrix
is
0
:
det
A
=
0 so any matrix with a row of
zeros has zero determinant.
10.
 32
det(A),
1
det(A)
11.
After
preforming
row
oper-
ations
on
M,
it
can
be
reduced
to
M
0
=

rref
(A)

0
rref
(C
)

:
Af-
ter reduction,
M
0,
rref
(A), and
rref
(C
)
are all upper triangular so
det
M
0
=
det(rref
(A))
det(rref
(C
)):
Let
A
0 be the product of the row oper-
ations to reduce
A and
C
0 be the prod-
uct of the row operations to reduce
C
:
So
the
product
of
the
row
operations
to reduce
M is
A
0
C
0
:
So
det
M
0
=
det
M
det(A
0
C
0
)
=
det
M
det
A
0
det
C
0
and
det
(rref
(A))
det(rref
(C
))
=
det
A
det
A
0
det
C
det
C
0
: Since
det
M
0
=
det(rref
(A))
det(rref
(C
)),
det
M
=
det
A
det
C
:
Section 2.7, Page 113
2.
(a)
2
4
1
0
0
 1
2
1
2
 1
2
 1
0
1
3
5,
(b)

 1
1
1
 1
2

3. (b)
x
1
=
1
4
b
1
+
1
4
b
2
x
2
=
1
2
b
1
 1
2
b
2 , (c)
x
1
=
 7
6
x
2
=
5
3
x
3
=
11
2

B. SOLUTIONS TO SELECTED EXERCISES
315
4. The system is
y
0
=
c
0
+
c
1
x
0
+
c
2
x
2
0
y
1
=
c
0
+
c
1
x
1
+
c
2
x
2
1
y
2
=
c
0
+
c
1
x
2
+
c
2
x
2
2.
The determinant of the coefï¬cient matrix is
(x
1
 x
0
)(x
2
 x
0
)(x
2
 x
1
) which equals
0 when
x
1
=
x
0,
x
1
=
x
2, or
x
2
=
x
0
:
5.
Since the matrix of minors of
A has
integer coefï¬cients,
adj
A must be a ma-
trix of integer coefï¬cients.
Since
A
 1
=
1
det
A
adj
A
=
 1
:
adj
A,
A
 1 is the prod-
uct of an integer scalar and a matrix with in-
teger coefï¬cients so
A
 1 must have integer
coefï¬cients.
Section 2.8, Page 117
1.
2
6
6
6
6
6
6
4
2
 1
0
0
0
0
1
0
0
0
0
0
4
 2
4
 2
2
 1
2
0
2
0
1
0
2
 1
0
0
2
 1
1
0
0
0
1
0
3
7
7
7
7
7
7
5
,
2
6
6
6
6
6
6
4
2
0
0
 1
0
0
4
4
2
 2
 2
 1
2
0
2
 1
0
 1
1
0
0
0
0
0
2
2
1
0
0
0
1
0
1
0
0
0
3
7
7
7
7
7
7
5
Section 2.9, Page 123
1.
L
=
2
4
1
0
0
1
1
0
2
1
1
3
5,
U
=
2
4
2
 1
1
0
4
 3
0
0
 1
3
5,
x
=
[1;
 2;
2]
T
2.
Let
A
=
[a
ij
],
L
=
2
6
4
1
0
0
...
0

1
3
7
5,
and
U
=
2
6
4
u
11

...
0
u
nn
3
7
5 where
u
ii
6=
0: So
a
11
=
u
11, but
a
11
=
0 so
a
11
6=
u
11
:
Since there is no
U, there is not an LU factor-
ization of
A:
Section 2.10, Page 123
1.
2
4
4
1
5
1
6
0
5
0
 4
3
5
2.
A is invertible if
k
6=
6, and then
A
 1
=

 3
k
 6
1
k
 6
k
k
 6
 2
k
 6

:
3.
21
4.
Let
A
=

1
0
0
1

and
B
=

1
0
0
 1

: Then
A and
B are invertible
matrices since
det
A
6=
0 and
det
B
6=
0, but
A
+
B is not invertible since
det(A
+
B
)
=
0:
5. Let
A
=
[
a
ij
] so the
ij
th entry of
A
+
A
T
is
a
ij
+
a
j
i and the
j
i
th entry is
a
j
i
+
a
ij
:
So
A
+
A
T is symmetric.

316
B. SOLUTIONS TO SELECTED EXERCISES
Section 3.1, Page 134
1.
V is a vector space.
2.
V is not a vector space because it is not
closed under vector addition and scalar mul-
tiplication.
3.
V is not a vector space because it is not
closed under scalar multiplication.
4.
V is a vector space.
5.
V is not a vector space because it is not
closed under vector addition.
6.
V is a vector space.
7.
c0
=
c(0
+
0)
c0
=
c0
+
c0
(c0
+
( c0))
=
c0
+
(c0
+
( c0))
0
=
c0
+
0
0
=
c0
8.
( c)v
=
( 1c)v
=
 1(cv
)
=
 (cv
)
Similarly, you can show
( c)v
=
c( v
)
=
 (cv
):
9. Suppose
cv
=
0: If
c
=
0, weâ€™re done by
Law 1. Else if
c
6=
0, then there is
1
c so that
(
1
c
)cv
=
(
1
c
)0
(
1
c
c)v
=
0
1v
=
0
v
=
0
So
c
=
0 or
v
=
0:
13. (a) Is linear, but range is not
V
; (b) Is not
linear, (c) Is linear, range is
V
:
14.
(a)
T is not a linear transformation
because
T
((1;
0)
+
(0;
1))
=
T
(1;
1)
=
1(0;
1)
=
(0;
1) but
T
(1;
0)
+
T
(0;
1)
=
1(0;
0)
+
0(0;
1)
=
(0;
0):
Section 3.2, Page 141
1.
W is not a subspace of
V because
W is
not closed under addition and scalar multipli-
cation.
2.
W is a subspace of
V
:
3.
W is a subspace of
V
:
4.
W is a subspace of
V
:
5.
W is a subspace of
V
:
6.
W is not a subspace of
V because
W is
not closed under scalar multiplication.
7.Not a subspace, since
W doesnâ€™t contain
the zero element.
11. (a) Spans
P
2
; (b) does not span
P
2
12. For example,
v
=
(
1;
1;
0) and
w
=
(1;
 1;
 2)
:
13. The spans are equal.
14. Let
A and
B be
n

n diagonal matrices.
Then
cA is a diagonal matrix and
A
+
B is a
diagonal matrix so the set of diagonal matri-
ces is closed under matrix addition and scalar
multiplication. Therefore the set of diagonal
matrices is a subspace of
R
n;n
:
15.
(a) If
x;
y
2
U and
x;
y
2
V ,
x;
y
2
U
\
V
: Then
cx
2
U and
cx
2
V so
cx
2
U
\
V , and
x
+
y
2
U and
x
+
y
2
V
so
x
+
y
2
U
\
V
: Therefore
U
\
V is closed
under addition and scalar multiplication so it
is a subspace of
W
:
(b) If
u
1
;
u
2
2
U and
v
1
;
v
2
2
V ,
u
1
+
v
1
;
u
2
+
v
2
2
U
+
V
: Then
cu
1
2
U and
cv
1
2
V so
c(u
1
+
v
1
)
=
cu
1
+
cv
1
2
U
+
V ,
and
u
1
+
u
2
2
U and
v
1
+
v
2
2
V so
(u
1
+
v
1
)
+
(u
2
+
v
2
)
=
(u
1
+
u
2
)
+
(v
1
+
v
2
)
2
U
+
V
: Therefore
U
+
V is closed
under addition and scalar multiplication so it
is a subspace of
W
:
17.
(a) If
A
=
[a
ij
],
v
ec
(A)
=
(a
11
;
a
21
;
a
12
;
a
22
)
so
for
A
there
ex-
ists only one
v
ec
(A):
If
v
ec(A)
=
(a
11
;
a
21
;
a
12
;
a
22
),
A
=
[a
ij
] so for
v
ec(A)
there exists only one
A: Therefore the vec

B. SOLUTIONS TO SELECTED EXERCISES
317
operation establishes a one-to-one correspon-
dence between matrices in
V and vectors in
R
4
:
(b)
Let
A
=
[
a
ij
]
and
B
=
[b
ij
]
:
So
c
v
ec(A)
+
d
v
ec(B
)
=
c(a
11
;
a
21
;
a
12
;
a
22
)
+d(b
11
;
b
21
;
b
12
;
b
22
)
=
(ca
11
+
db
11
;
ca
21
+
db
21
;
ca
12
+
db
12
;
ca
22
+
db
22
)
=
v
ec
(cA
+
dB
).
18. If
k

1,
V
k
=
spanfA
0
;
A
1
g:
Section 3.3, Page 150
1. (a) linearly independent, (b) each vector
is redundant, (c) each vector is redundant
3. (a) linearly independent, (b) linearly inde-
pendent, (c) each vector is redundant
4. (a)
( 1;
 2;
3), (b)
(
1
2
;
1;
3
2
)
5.
(a)
v
2
spanfu
1
;
u
2
g,
(b)
spanfu
1
;
u
2
;
(1;
0;
 1)g
6.
(a)
v
=
2
spanfu
1
;
u
2
g,
(b)
spanfu
1
;
u
2
;
v
g
7.
span
f
v
1
;
v
2
g
13. Start with a nontrivial linear combination
of the functions that sums to
0 and differenti-
ate it.
15. Assume
v
i
=
v
j
: Then there exists
c
i
=
 c
j
6=
0 such that
c
1
v
1
+
c
2
v
2
+
:
:
:
+
c
i
v
i
+
:
:
:
+
c
j
v
j
+
:
:
:
+
c
n
v
n
=
0:
Therefore
fv
1
;
v
2
;
:
:
:
;
v
n
g is linearly de-
pendent.
Section 3.4, Page 159
1
(a)

1
 1=2
0
0
0
1

,
(b)
N
(A)
=
spanfg,
(c)
C
(A)
=
spanf(1;
1;
3);
(0;
1;
2)g,
(d)
R(A)
=
spanf(1;
2;
0;
0;
1);
(1;
2;
1;
1;
1)g,
(e)
vector
of
coefï¬cients
belong
to
spanf( 2;
1;
0;
0;
0);
(0;
0;
 1;
1;
0);
( 1;
0;
0;
0;
1)g
2 (a)
2
4
1
2
0
0
1
7
0
0
1
1
0

 7
0
0
0
0
0

 2
 7
3
5
; (b)
N
(A)
=
spanf( 2;
1;
0;
0;
0);
(0;
0;
 1;
1;
0);
( 1;
0;
0;
0;
1)g, (c)
C
(A)
=
spanf(1;
1;
3);
(0;
1;
2)g,
(d)
R(A)
=
spanf(1;
2;
0;
0;
1);
(1;
2;
1;
1;
1)g,
(e)
vector
of
coefï¬cients
belong
to
spanf( 2;
1;
0;
0;
0);
(0;
0;
 1;
1;
0);
( 1;
0;
0;
0;
1)g,
(f)
f(;

)j
 2
 7
=
0g
4.
2
4
1
0
1
1
1
3
 2i
0
1
1
3
5
2
4
2
 4
 3
3
5
6.
T is not one-to-one since
k
er
T
6=
f0g:
10. Since
A is nilpotent, there exists
m such
that
A
m
=
0 so
det
(A
m
)
=
(det
A)
m
=
0
and
det
A
=
0: Therefore
A is not invertible
so
N
(A)
6=
f0g: Also since
A is nilpotent,
by Exercise 14 in Section 2.4,
(I
 A)
 1
=
I
+
A
+
A
2
+
:
:
:
+
A
m 1
: Since
I
 A is
invertible,
N
(I
 A)
=
f0g:
Section 3.5, Page 164
1 (a) none
5. (a) true, (b) false, (c) true, (d) true, (e)
true, (f) true, (g) false, (h) false, (i) false, (j)
false, (k) true
7.
Since
P is a subspace of
C
[0;
1] and
dim
P is inï¬nite,
dim
(C
[0;
1]) is inï¬nite.
8.
dim
(fE
i;j
g)
=
mn
=
dim
(R
m;n
): If
c
1;1
E
1;1
+
:
:
:
+
c
i;j
E
i;j
=
0,
c
a;b
=
0
for each
a;
b because
E
a;b is the only ma-
trix with a nonzero entry in the
(a;
b)th po-
sition. Therefore
fE
i;j
j
i
=
1;
:
:
:
;
m;
j
=
1;
:
:
:
;
ng is a basis of the vector space
R
m;n
:
13. The dimension of the space is
n(n
+
1)=2:
Section 3.6, Page 173
1.
R(A)
=
spanf(1;
0);
(0;
1)g,
C
(A)
=
spanf(0;
 1;
1);
(2;
1;
1)g,
N
(A)
=

318
B. SOLUTIONS TO SELECTED EXERCISES
spanf0g,
R(B
)
=
spanf(1;
0;
 2);
(0;
1;
0)g,
C
(B
)
=
spanf(2;
 1;
1);
(2;
0;
1)g,
N
(B
)
=
spanf(2;
0;
1)g,
R(C
)
=
spanf(1;
0;
 2;
 2);
(0;
1;
1
2
;
2)g,
C
(C
)
=
spanf(1;
 1);
(2;
0)g,
N
(C
)
=
spanf(2;
 1
2
;
1;
0);
(2;
 2;
0;
1)g
2. (a)
(c
1
;
c
2
;
c
3
;
c
4
) such that
c
1
v
1
+
c
2
v
2
+
c
3
v
3
+
c
4
v
4
=
0 where
c
1
=
 2c
3
 2c
4,
c
2
=
 c
3, and
c
3
;
c
4 are free. Dimension of
spanfv
1
;
v
2
;
v
3
;
v
4
g is
2:
(b)
(c
1
;
c
2
;
c
3
) such that
c
1
v
1
+
c
2
v
2
+
c
3
v
3
=
0 where
c
1
=
2c
3,
c
2
=
 c
3, and
c
3 is free.
4. (a)
R(A)
=
spanf(1;
0;
 1;
0;
0;
 2;
 3),
(0;
1;
1;
0;
0;
2;
3),
(0;
0;
0;
1;
0;
4;
5),
(0;
0;
0;
0;
1;
6;
7)g,
rank
A
=
4:
C
(A)
=
spanf(3;
1;
3;
0;
0),
(1;
1;
2;
2;
3),
(0;
 1;
1;
 1;
3),
(1;
1;
1;
1;
 3)g:
N
(A)
=
spanf(1;
 1;
1;
0;
0;
0;
0),
(2;
 2;
0;
 4;
 6;
1;
0),
(3;
 3;
0;
 5;
 7;
0;
1)g,
nul
l
ity
(A)
=
3:
10. Since
Ax
=
b is a consistent,
b
2
C
(A):
If
fc
i
g, the set of columns of
A,
has redundant vectors in it,
a
1
c
1
+
a
2
c
2
+
:
:
:
+
a
n
c
n
=
0 for some
a
i
6=
0:
If
d
1
c
1
+
d
2
c
2
+
:
:
:
+
d
n
c
n
=
b is a solu-
tion for
Ax
=
b, then
(a
1
+
d
1
)c
1
+
(a
2
+
d
2
)c
2
+
:
:
:
+
(a
n
+
d
n
)c
n
=
b is also a so-
lution.
Therefore the system has more than
one solution.
11.
dim
(M
nn
(R))
=
n
2
so
fI
;
A;
A
2
;
:
:
:
;
A
n
2
g must be linearly de-
pendent since
dim
fI
;
A;
A
2
;
:
:
:
;
A
n
2
g
=
n
2
+
1
>
dim
(M
nn
(R)):
So there ex-
ists
c
0
I
+
c
1
A
+
c
2
A
2
+



+
c
n
2
A
n
2
=
0
2
M
nn
(R) with some
c
i
6=
0:
Pick
p(x)
=
c
0
+
c
1
x
+



+
c
n
2
A
n
2 so
p(x)
6=
0
and
p(A)
=
0:
Section 3.7, Page 178
1.
2
4
1
2
0
1
 1
0
0
1
1
3
5,
dom
(T
)
=
spanf(1;
0;
0);
(0;
1;
0);
(0;
0;
1)g,
range(T
)
=
spanf(1;
1;
0);
(2;
 1;
1);
(0;
0;
1)g,
k
er(T
)
=
f0g
Section 3.9, Page 182
1. (a)
W is not a subspace of
V because
W
is not closed under matrix addition. (b)
W is
not a subspace of
V because
W is not closed
under matrix addition.
9. (a) true, (b) false, (c) true, (d) true, (e) true,
(f) false, (g) false, (h) false, (i) false (consider

1
1
1
1

).
Section 4.1, Page 191
1. (c)
p
10,
2
+
i
6.
k
cv
k
=
jcj
k
v
k by Basic Norm Law #2.
Since
c
2
R and
c
>
0,
kcv
k
=
c
k
v
k
:
So a unit vector in direction of
cv is
u
cv
=
cv
=c
kv
k
=
v
=
k
v
k
=
u
v
: If
c
<
0, then
u
cv
=
 u
v
:
7.
Let
u
=
(u
1
;
:
:
:
;
u
n
)
2
R
n,
v
=
(v
1
;
:
:
:
;
v
n
)
2
R
n, and
c
2
R:
Then
(cu)

v
=
(cu
1
)v
1
+
:
:
:
+
(cu
n
)v
n and
v

(cu)
=
v
1
(cu
1
)
+
:
:
:
+
v
n
(cu
n
) so
(cu)

v
=
v

(cu): Similarly, you can show
(cu)

v
=
v

(cu)
=
c(v

u)
=
c(u

v
):
Section 4.2, Page 200
2. (a)
90
Ã†, (b)
u
v
1
=
[
p
2
2
;
0;
p
2
2
]
T ,
u
v
2
=
[
p
3
3
;
p
3
3
;
 p
3
3
]
T , (c)
0, (d)
jv
1

v
2
j
=
0 and
kv
1
k
k
v
2
k
=
p
6 so
jv
1

v
2
j

k
v
1
k
k
v
2
k

B. SOLUTIONS TO SELECTED EXERCISES
319
4.
The normal equations are
A
T
Ax
=
A
T
b:
(1)
x
=

3
7
;
 2
3

T ,
b
 Ax
=

 4
21
;
 16
21
;
8
21

T ,
kb
 Axk
=
4
p
21
21
(2)
x
=

9
7
 x
3
;
4
7
 x
3
;
x
3

T where
x
3 is
free,
b
 Ax
=

2
7
;
 6
7
;
4
7

,
k
b
 Axk
=
2
p
14
7
Section 4.3, Page 208
1. (a),(b), (c), (d) are linearly independent;
(a), (c) and (d) are orthogonal; (c) is an or-
thonormal set.
2.
v
1

v
2
=
0,
v
1

v
3
=
0, and
v
2

v
3
=
0
so
fv
1
;
v
2
;
v
3
g is an orthogonal basis of
R
3
:
The coordinates of
v,
(c
1
;
c
2
;
c
3
), such that
c
1
v
1
+
c
2
v
2
+
c
3
v
3
=
v are
(
3
2
;
 1
3
;
 5
3
):
3.
f
=

k
jk
2
Z
g makes
A symmetric.
Section 4.5, Page 212
1.
[
p
6
6
;
p
6
6
;
 p
6
3
]
T
2.
jjujj
=
p
7,
u

v
=
0
Section 5.1, Page 222
1.
(a)
 2;
 1, (b)
2;
0;
5, (c)
2;
2;
3, (e)
 1;
 1;
 1.
2.
(a) The basis of
E
 2 is
f(3;
1)g, and
the basis of
E
 1 is
f(4;
1)g: Both alge-
braic and geometric multiplicity of each is
1: (b) The basis of
E
2 is
f(1;
0;
0)g, the
basis of
E
0 is
f(0;
1;
 3)g, and the basis
of
E
5 is
f(0;
1;
2)g: (c) The basis of
E
2 is
f(0;
 1;
1)
;
(1;
0;
0)g, and the basis of
E
3 is
f(1;
1;
0)g: (e) A basis of
E
 1 is
f(0;
0;
1)g:
The algebraic multiplicity of the eigenvalue
 1 is
3; while the geometric multiplicity is
1:
4. (a)
tr
A
=
 3
=
 2
+
 1, (b)
tr
A
=
7
=
2
+
0
+
5, (c)
tr
A
=
7
=
2
+
2
+
3
5.
(a)
p()
=
(a
 )(d
 )
 bc,

1
=
1
2
a
+
1
2
d
+
1
2
p
(a
2
 2ad
+
d
2
+
4bc),

2
=
1
2
a
+
1
2
d
 1
2
p
(a
2
 2ad
+
d
2
+
4bc),
(b)
tr
A
=
a
+
d
=

1
+

2, (c)
det
A
=
ad
 bc
=

1

2
6. For
A, the basis of
E
1 is
f(1;
0)g, and
the basis of
E
2 is
f(1;
1)g: For
A
T , the ba-
sis of
E
1 is
f( 1;
1)g, and the basis of
E
2 is
f(0;
1)g:
7. The eigenvalues of
A are
3
2
+
p
3
2
i and
3
2
 p
3
2
i:
8.
Since
(I
 A)
T
=
I
 A
T and
det
(I
 A)
T
=
det
(I
 A),
det(I
 A)
=
det
(I
 A
T
) so
A and
A
T have the
same eigenvalues.
9. Since
x is an eigenvector of
A with eigen-
value
,
Ax
=
x: So
A(cx)
=
c(Ax)
=
c(x)
=
(cx): Therefore
cx is an eigen-
vector of
A with eigenvalue
:
10. Let
 be an eigenvalue of
A with eigen-
vector
v such that
Av
=
v
: So
(I
 A)v
=
I
v
 Av
=
v
 v
=
(1
 )v
: Thus if

is an eigenvalue of
A,
1
  is an eigenvalue
of
I
 A: Since
jj
<
1,
1
 
>
0 so every
eigenvalue of
I
 A is nonzero. Therefore
I
 A is invertible.
11. Let
 be an eigenvalue of
A with eigen-
vector
v such that
Av
=
v
: If
A is invert-
ible,

6=
0 so
A
 1
Av
=
A
 1
v and thus
A
 1
v
=
1

v
: Therefore
1= is an eigen-
value of
A
 1
:
Section 5.2, Page 230
7.
(a)
Ax
n
=

1
1
1
0


f
n+1
f
n

=

f
n+1
+
f
n
f
n+1

=

f
n+2
f
n+1

=
x
n+1

320
B. SOLUTIONS TO SELECTED EXERCISES
(b)
f
n
=
(
1+
p
5
2
)
n
(
5+
p
5
10
)
+
(
1 p
5
2
)
n
(
5 p
5
10
)
Section 5.3, Page 238
1. (a), (b), (c), and (e) are diagonalizable
because they are non-defective. (d) and (f)
are not diagonalizable because they are de-
fective.
2.
(a)
P
=
2
4
1
0
0
0
1
 1
0
0
1
3
5,
D
=
2
4
2
0
0
0
3
0
0
0
2
3
5, (b)
P
=

1
1
 1
 2

,
D
=

1
2
0
0
 1
2

, (c)
P
=

1
0
2
1

,
D
=

3
2
0
0
 1
2

3. (b) has
lim
A
k
=
0:
6. (a) there is no dominant eigenvalue, (b)
 4, (c) there is no dominant eigenvalue
Section 5.4, Page 243
1. (a)
2
+
p
2
;
2
 p
2, (b)
 3;
2, (c)
0;
2;
3;(f)
1;
1

2
p
2
2.
(a) The basis of
E
2+
p
2 is
f( i
 i
p
2
;
1)g, and the basis of
E
2 p
2 is
f(1;
 i
+
i
p
2
)g: (b) The basis of
E
 3 is
f( 2;
1)g,
and the basis of
E
2 is
f(1;
2)g: Also
P
=
p
5
5

 2
1
1
2

,
D
=

 3
0
0
2

(c) The
basis of
E
0 is
f( 1
 i;
1;
0)g, the basis of
E
2
is
f(0;
0;
1)g, and the basis of
E
3 is
f(1;
1
 i;
0)g: (d) The basis of
E
0 is
f(1;
 1
2
+
i
2
)g,
and the basis of
E
3 is
f(
1
2
+
i
2
;
1)g: Also
P
=
p
6
6

2
1
+
i
 1
+
i
2

,
D
=

0
0
0
3

:
4.
(a)
A
=
A
H, (b) the basis of
E
2 is
f( i;
1)g, the basis of
E
4 is
f(i;
1)g, (c)
P
=
p
2
2

 i
i
1
1

,
D
=

2
0
0
4

, (d)
A
k
=
P
D
k
P
T
Section 5.7, Page 258
3.
Eigenvalues are
5;
 4;
1
+
2i
p
5;
1
 2i
p
5:
Section 5.8, Page 259
2.
P
=
2
4
0
1
3
1
0
4
 1
0
0
3
5,
D
=
2
4
1
0
0
0
1
0
0
0
5
3
5
4. (a) false, (b) true, (c) false, (d) false, (e)
true
5. Let
 be an eigenvalue of
A and
v be
an eigenvector of
A so that
Av
=
v
: So
(A
+
I
)v
=
Av
+
I
v
=
v
+
v
=
(
+
1)v
:
Therefore,

+
1 is an eigenvalue of
A
+
I
:
7.
The eigenvalues of
A are
a

b and
0 (with multiplicity
n
 1).
The ba-
sis of
E
ab is
fag, and the basis of
E
0 is
f( b
2
;
b
1
;
0;
:
:
:
;
0);
( b
3
;
0;
b
1
;
0;
:
:
:
;
0);
:
:
:
;
( b
n
;
0;
:
:
:
;
0;
b
1
)g:
9. Let
A be Hermitian symmetric so
A
=
A
H
: So
A(A
H
)
=
A
H
(A
H
)
=
A
H
(A):
Therefore
A is normal.
Section 6.1, Page 265
1.
kxk
1
=
2
+
p
2,
k
xk
2
=
2,
k
xk
1
=
p
2,
k
y
k
1
=
8,
ky
k
2
=
p
22,
k
y
k
1
=
4

B. SOLUTIONS TO SELECTED EXERCISES
321
2.
u
1
=
1
5
(1;
 3;
 1),
u
2
=
p
11
11
(1;
 3;
 1),
u
1
=
1
3
(1;
 3;
 1)
3.
k
u
+
v
k
1
=
7

6
+
7
=
kuk
1
+
kv
k
1
5.
p
2
6.
max
fj
jAxjj
j
jjxjj
=
1g
=
max
fjaj
+
jbj;
jcj
+
jdjg
7.
Let
u
=
(u
1
;
:
:
:
u
n
) and
v
=
(v
1
;
:
:
:
v
n
) so
k
uk
1
=
ju
1
j
+
:
:
:
+
j
u
n
j

0: Also
kcuk
1
=
jcu
1
j
+
:
:
:
+
jcu
n
j
=
jcj
j
u
1
j
+
:
:
:
+
jcj
j
u
n
j
=
jcj
kuk
1, and
ku
+
v
k
1
=
j
u
1
+
v
1
j
+
:
:
:
+
ju
n
+
v
n
j

ju
1
j
+
:
:
:
+
j
u
n
j
+
jv
1
j
+
:
:
:
+
jv
n
j
=
kuk
1
+
kv
k
1
:
8.
v
=
lim
n!1
v
n
=
[ 1;
1]
T
:
v
 v
n
=

 1
n
;
 e
 n

T so
k
v
 v
n
k
1
=
(
1
n
+
e
 n
)
    !
n !1
0 and
kv
 v
n
k
2
=
q
(
1
n
)
2
+
(e
 n
)
2
     !
n
 !
10:
Therefore
lim
n!1
v
n is the same with respect to both
that
1- and
2-norm.
9.
[0;
0]
T
Section 6.2, Page 274
1.
j
<
u;
v
>j
=
4

p
70
=
kuk
k
v
k
2. (a)


118:56
Ã†, (b)


23:58
Ã†
3. All are linearly independent; (a), (c), (d)
and (e) are orthogonal; (c) is an orthonormal
set.
4.
<
[x
1
;
x
2
]
T
;
[x
1
;
x
2
]
T
>=
3x
2
1
 2x
2
2
:
Since
3x
2
1
 2x
2
2 is not necessarily greater
than or equal to
0, the given law fails the ï¬rst
condition of the deï¬nition of an inner prod-
uct.
7.
<
u;
v
>
=
(Au)
H
Av
=
[
2u
1
;
3u
2
]

v
1
v
2

=
2u
1
v
1
+
3u
2
v
2
8.
p
2
10
9. If
w
2
V
; then Theorem 4.3.3 supplies a
formula for
w
; which we can check. Doing
so shows
w
=
2
spanf(1;
1;
0);
( 1;
1;
1)g:
Section 6.3, Page 284
1. For standard inner product
(
5
2
;
1
2
;
1); the
other gives
(
19
10
;
9
10
;
1
2
)
2.
(a)
f(1;
0;
0);
p
2
2
(0;
 1;
 1);
p
2
2
(0;
 1;
1)g, (b)
(2;
5
2
;
5
2
); (c)
(2;
5
2
;
5
2
)
+
(0;
 3
2
;
3
2
), (d)
2
4
1
0
0
0
1
2
1
2
0
1
2
1
2
3
5, (e)
P
w
=

2;
5
2
;
5
2

T
3.
(a)
V
=
spanf
1
2
( 1;
 1;
1;
1);
1
2
(1;
1;
1;
1);
p
2
2
(0;
0;
 1;
1)g,
(b)
(1=2;
1=2;
0;
0),
(c)
(1=2;
1=2;
0;
0)
+
(
1
=
2;
 1=2;
0;
0), (d)
2
6
6
4
1
2
1
2
0
0
1
2
1
2
0
0
0
0
1
0
0
0
0
1
3
7
7
5,
(e)
P
w
=
(
1
2
;
1
2
;
0;
0)
4. (a)
f
p
3
3
(1;
1;
 1);
p
78
78
( 2;
7;
5)g
Section 6.4, Page 293
1. (a)
V
?
=
spanf(1;
1;
0);
( 2;
0;
1)g
3.
V
?
=
spanf(
3
2
;
1;
0);
( 1;
0;
1)g
10.
U
\
V
=
spanf(3;
3;
1)g:
Section 6.7, Page 306
1.
(a)
f(1;
1;
1;
1);
(1;
 1;
1;
 1);
(0;
 1;
0;
1)g, (b)
f
1
2
(1;
1;
1;
1);
1
2
(1;
 1;
1;
 1);
p
2
2
(0;
 1;
0;
1)g, (c)
2
6
6
4
1
2
0
1
2
0
0
1
0
0
1
2
0
1
2
0
0
0
0
1
3
7
7
5

322
B. SOLUTIONS TO SELECTED EXERCISES
2.
p
2
10
3.
The formula does not deï¬ne an inner
product on
R
2
:

Bibliography
[1] Richarc Bellman. Introduction to Matrix Analysis, 2nd ed. SIAM, Philadelphia, PA, 1997.
[2] G. Caughley. Parameters for seasonally breeding populations. Ecology, 48:834â€“839, 1967.
[3] Biswa Nath Datta. Numerical Linear Algebra and Applications. Brooks/Cole, New York, 1995.
[4] James W. Demmel. Applied Numerical Linear Algebra. SIAM, Philadelphia, PA, 1997.
[5] G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins University Press, Baltimore,
Maryland, 1983.
[6] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, UK, 1985.
[7] P. Lancaster and M. Tismenetsky. The Theory of Matrices. Academic Press, Orlando, Florida, 1985.
[8] Lloyd Trefethen and David Bau. Numerical Linear Algebra. SIAM, Philadelphia, PA, 1997.
323

324
BIBLIOGRAPHY

Index
Abstract vector space, 128
Adjacency matrix, 68
Adjoint
formula, 107
matrix, 106
Afï¬ne set, 167
an, 101
Angle, 193, 270
Argument, 14
Augmented matrix, 22
Ball, 263
closed, 263
open, 263
Banach lemma, 297
Basis, 145
coordinates relative to, 147
ordered, 145
Basis theorem, 161
Block matrix, 76, 83, 124
Bound variable, 25
Cayley-Hamilton theorem, 231, 239
CBS inequality, 192, 270
Change of basis, 176
Change of basis matrix, 150
Change of variables, 152, 210
Characteristic
equation, 215
polynomial, 215
Coefï¬cient matrix, 21
Cofactors, 99
Column space, 153
Companion matrix, 105, 254
Complex
number, 11
plane, 11
Complex number
argument, 14
Polar form, 14
Complex numbers, 11
Component, 196
Condition number, 297, 300
Conditions for matrix inverse, 91
Conductivity, 5
Conformable matrices, 56
Consistency, 26
in terms of column space, 166
in terms of rank, 36
Coordinate map, 182
Coordinates, 146
orthogonal, 202, 273
standard, 147
vector, 147
Counteridentity, 105
Cramerâ€™s rule, 109
DeMoivreâ€™s Formula, 15
Determinant, 97
Determinants
computational efï¬ciency, 109
Diagonal, 75
Diagonalizable
matrix, 228
orthogonally, 240
unitarily, 240, 246
Diagonalization theorem, 228
Diagonalizing matrix, 228
Diffusion process, 4, 116
Digraph, 66
adjacency matrix, 68
walk, 67
Dimension
deï¬nition, 163
theorem, 163
Discrete dynamical system, 224
Displacement vector, 126
Domain, 132, 158
Dominant eigenvalue, 239, 250
Dot product, 190
Eigenpair, 213
Eigenspace, 216
Eigensystem, 216
algorithm, 216
Eigenvalue, 213
dominant, 239, 250
repeated, 221
simple, 221
Eigenvector, 213
Elementary
inverse operations, 31
325

326
INDEX
matrices, 72
row operations, 22
transposes of matrix, 79
Equation
linear, 3
Sylvester, 114
Equivalent linear system, 31
Factorization
full QR, 303
LU, 118
QR, 291, 292
Fibonacci numbers, 230
Field of scalars, 125
Finite dimensional, 161
Flat, 167
Flop, 41
Fourier analysis, 304
Fourier coefï¬cients, 305
Fourier heat law, 5
Fredholm alternative, 92, 290
Free variable, 25
Frobenius norm, 266, 295
Full column rank, 35
Function
continuous, 129, 132, 134
linear, 62
Fundamental Theorem of Algebra, 12
Gauss-Jordan elimination, 23, 29
Gaussian elimination, 29
Gerschgorin circle theorem, 251
Gram-Schmidt algorithm, 276
Graph, 66, 67, 122
adjacency matrix, 68
dominance-directed, 66, 67
loop, 179
walk, 68
Hermitian matrix, 80
Hessenberg matrix, 182
Householder matrix, 206, 209, 244, 302
Idempotent matrix, 61, 84, 283
Identity matrix, 58
Image, 159
Imaginary part, 11
Induced norm, 270
Inner product, 78, 190
abstract, 267
space, 267
standard, 189
weighted, 267
Input-output
matrix, 8
model, 29
Input-output model, 6, 7
Integers, 10
Interpolation, 8, 113
Intersection, 142, 286
set, 9
Inverse, 85, 107
Inverse iteration method, 253
Inverse power method, 253
Isomorphic vector spaces, 158
isomorphism, 158
Jordan block, 237
Jordan canonical form, 237, 255, 259
Kernel, 157
Kronecker delta, 106
Kronecker symbol, 58
Leading entry, 21
Least squares, 197, 283
solver, 293, 303
Least squares solution, 198
Legendre polynomial, 277
Leslie matrix, 257
Limit vector, 71, 155, 188, 191
Linear
mapping, 132, 133
operator, 132, 133
transformation, 132, 133
Linear combination, 51, 138
trivial, 143, 156
Linear dependence, 143
Linear function, 62
Linear independence, 143
Linear system
coefï¬cient matrix, 21
equivalent, 31
form of general solution, 167
right hand side vector, 21
List, 143
Loop, 67, 179
LU factorization, 118
Markov chain, 64, 65
Matrix
adjoint, 106
block, 76
change of basis, 150, 176
cofactors, 106
companion, 254
complex Householder, 208
condition number, 297
defective, 221
deï¬nition, 20
diagonal, 75
diagonalizable, 228
difference, 50
elementary, 72
entry, 20
equality, 49

INDEX
327
exponent, 60
full column rank, 35
Hermitian, 80
Householder, 206, 209
idempotent, 61, 283
identity, 58
inverse, 107
invertible, 85
leading entry, 21
minors, 106
multiplication, 56
multiplication not commutative, 57
negative, 50
nilpotent, 61, 84, 314
nonsingular, 85
normal, 124, 245, 259
of a linear operator, 175
orthogonal, 204
permutation, 120
pivot, 24
positive deï¬nite, 198
positive semideï¬nite, 198
projection, 209, 282
reï¬‚ection, 209
scalar, 74
scalar multiplication, 51
similar, 226
singular, 85
size, 20
skew-symmetric, 84, 141
standard, 176
strictly diagonally dominant, 258
sum, 50
super-augmented, 90
symmetric, 80
transition, 224
triangular, 75
tridiagonal, 76
unitary, 204
upper Hessenberg, 182
Vandermonde, 105, 113
vectorizing, 115
Matrix norm, 295
inï¬nity, 300
Matrix, strictly triangular, 75
Max, 35
Min, 35
Minors, 99
Multiplicity
algebraic, 221
geometric, 221
Multipliers, 119
Natural number, 10
Network, 8, 178
Newton
method, 93
formula, 94
Nilpotent matrix, 61, 84, 95, 160, 314
Non-negative deï¬nite matrix, 201
Nonsingular matrix, 85
Norm
complex, 186
Frobenius, 266, 295
general, 261
induced, 270
inï¬nity, 262
matrix, 295
operator, 296
p-norm, 262
standard, 185
Normal equations, 198
Normal matrix, 124, 245, 259
Normalization, 187, 192
Normed linear space, 261
Notation
for elementary matrices, 22
Null space, 153
Nullity, 35
Number
complex, 11
integer, 10
natural, 10
rational, 10
real, 10
One-to-one, 157
Operator, 132
additive, 133
domain, 158
image, 159
kernel, 157, 158
linear, 133
one-to-one, 157
outative, 133
range, 158
target, 158
Orthogonal
complement, 286
complements theorem, 289
coordinates theorem, 202, 273
matrix, 204
projection formula, 282
set, 202, 273
vectors, 194, 271
Orthogonal coordinates theorem, 202
Orthonormal set, 202, 273
Outer product, 78
Parallel vectors, 195
Parallelogram law, 272
Partial pivoting, 40
Perturbation theorem, 298
Pivot, 24
strategy, 40

328
INDEX
Polar form, 14
Polarization identity, 276
Polynomial, 13
characteristic, 215
companion matrix, 105
Legendre, 277
monic, 215
Positive deï¬nite matrix, 198, 201, 223, 244, 275
Positive semideï¬nite matrix, 198
Power
matrix, 60
vertex, 67
Power method, 252
Principal axes theorem, 241, 245
Probability distribution vector, 65
Product
inner, 78
outer, 78
Projection, 196, 281
column space formula, 284
formula, 195, 273
formula for subspaces, 281
matrix, 282
problem, 280
theorem, 281
Projection formula, 195, 273
Projection matrix, 209
Pythagorean theorem, 200, 271, 273, 276
QR algorithm, 294
QR factorization, 291, 292
full, 303
Quadratic form, 80, 84, 123, 255
Quadratic formula, 14
Range, 158
Rank, 34
full column, 199
of matrix product, 82
theorem, 172
Rational number, 10
Real numbers, 10
Real part, 11
Reduced row echelon form, 32
Reduced row form, 32
Redundancy test, 144
Redundant vector, 143
Reï¬‚ection matrix, 209
Residual, 197
Resolution of identity, 306
Roots of unity, 14
Rotation matrix, 149, 205
Row operations, 22
Row space, 153
Scalar, 19, 74, 125
Schur triangularization theorem, 244
Set, 9, 143
closed, 263
empty, 9
equal, 9
intersection, 9, 142
prescribe, 9
proper, 9
subset, 9
union, 9
Similar matrices, 226
Singular
values, 248
vectors, 248
Singular matrix, 85
Singular Value Decomposition, 247
Skew-symmetric, 141
Skew-symmetric matrix, 84, 105
Solution
general form, 25
genuine, 198
least squares, 198
non-unique, 24
set, 30
to
z
n
=
d, 15
vector, 30
Solutions
to linear system, 19
Soluton
to linear system, 2
Space
inner product, 267
normed linear, 261
Span, 138
Spectral radius, 232
Standard
basis, 146
coordinates, 146
inner product, 189
norm, 185
vector space, 127
Standard form, 11
States, 224
Steinitz substitution, 162, 164
Strictly diagonally dominant, 258
Subspace
deï¬nition, 135
intersection, 142
projection, 281
sum, 142
test, 135
trivial, 137
Sum of subspaces, 142, 286
Super-augmented matrix, 90
SVD, 247
Symmetric matrix, 80
System
consistent, 26
equivalent, 29

INDEX
329
homogeneous, 37
inconsistent, 26
linear, 3
non-homogeneous, 37
overdetermined, 197
Target, 132, 158
Tensor product, 114
Trace, 222
Transformation, 132
Transition matrix, 224
Transpose, 77
rank, 81
Triangular, 75
lower, 75
strictly, 75, 84
unit, 119
upper, 75, 99, 231, 246
Tridiagonal matrix, 76, 223
Tuple notation, 30
Unique reduced row echelon form, 33
Unit vector, 187
Unitary matrix, 204
Vandermonde matrix, 105, 113, 303
Variable
bound, 25
free, 25
Vector
angle between, 193
convergence, 188
coordinates, 147
deï¬nition, 20
direction, 187
displacement, 127
limit, 71, 155, 188, 191
linearly dependent, 143
linearly independent, 143
orthogonal, 194, 271
parallel, 195
product, 56
redundant, 143
unit, 187
Vector space
abstract, 128
concrete, 125
ï¬nite dimensional, 161
geometrical, 126
inï¬nite dimensional, 161
laws, 128
of functions, 129
polynomials, 137
standard, 127
Walk, 67, 68
Wronskian, 151


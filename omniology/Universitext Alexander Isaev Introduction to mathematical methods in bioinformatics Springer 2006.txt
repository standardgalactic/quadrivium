
Alexander Isaev
Introduction
to Mathematical
Methods in
Bioinformatics
With 76 Figures and 3 Tables

Alexander Isaev
Australian National University
Department of Mathematics
Canberra, ACT 0200
Australia
e-mail: alexander.isaev@maths.anu.edu.au
Corrected Second Printing 2006
Mathematics Subject Classiﬁcation (2000): 91-01 (Primary)
91D20 (Secondary)
Library of Congress Control Number: 2006930998
ISBN: 3-540-21973-0
ISBN: 9783540219736
This work is subject to copyright. All rights are reserved, whether the whole or part of the mater-
ial is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlm or in any other way, and storage in data banks. Dupli-
cation of this publication or parts thereof is permitted only under the provisions of the German
Copyright Law of September 9, 1965, in its current version, and permission for use must always
be obtained from Springer. Violations are liable for prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
Springer-Verlag Berlin Heidelberg 2006
The use of general descriptive names, registered names, trademarks, etc. in this publication does
not imply, even in the absence of a speciﬁc statement, that such names are exempt from the rele-
vant protective laws and regulations and therefore free for general use.
Cover design: Erich Kirchner, Heidelberg
Typesetting by the author and SPi using a Springer LATEX macro package
Printed on acid-free paper: SPIN:11809142
41/2141/SPi - 5 4 3 2 1 0

To Esya and Masha

Preface
Broadly speaking, Bioinformatics can be deﬁned as a collection of mathemati-
cal, statistical and computational methods for analyzing biological sequences,
that is, DNA, RNA and amino acid (protein) sequences. Numerous projects for
sequencing the DNA of particular organisms constantly supply new amounts
of data on an astronomical scale, and it would not be realistic to expect that
biologists will ever be able to make sense of it without resorting to help from
more quantitative disciplines.
Many studies in molecular biology require performing speciﬁc computa-
tional procedures on given sequence data, for example, simply organizing
the data conveniently, and therefore analysis of biological sequences is of-
ten viewed as part of computational science. As a result, bioinformatics is
frequently confused with computational sequence analysis, which is somewhat
narrower. However, understanding biological sequences now increasingly re-
quires profound ideas beyond computational science, speciﬁcally, mathemat-
ical and statistical ideas. For example, the protein folding problem incorpo-
rates serious diﬀerential geometry and topology; understanding the evolution
of sequences is dependent upon developing better probabilistic evolutionary
models; analysis of microarray data requires new statistical approaches. Gen-
erally, when one goes beyond algorithms and starts either looking for ﬁrst
principles behind certain biological processes (which is an extremely diﬃcult
task) or paying more attention to modeling (which is a more standard ap-
proach), one crosses the boundary between computational science and math-
ematics/statistics. These days many mathematicians are becoming interested
in bioinformatics and beginning to contribute to research in biology. This is
also my personal story: I am a pure mathematician specializing in several
complex variables, but now I work in bioinformatics as well.
Despite this mathematical trend, bioinformatics is still largely taught by
computational groups and computer science departments, and partly by engi-
neering and biology (in particular, genetics) departments. Naturally, in courses
developed by these departments emphasis is placed on algorithms and their
implementation in software. Although it is useful to know how a particular

VIII
Preface
piece of software works, this software-oriented education does not always re-
veal the mathematical principles on which the algorithms are based. Such
incompleteness may lead to certain problems for the graduates. Suppose, for
example, that a commonly used model is implemented in software and the stu-
dents are taught how to use it. Of course, the model makes some simplifying
assumptions about the biological processes it attempts to describe, and these
assumptions are buried in the mathematical core of the model. If the students
are taught only how to use the software and are not taught the mathematical
foundations of the model, they will know nothing about the assumptions and
therefore limitations of the model; this in turn means that they will not be
able to interpret correctly the results of applying the software to biological
data.
This situation with education in bioinformatics is now beginning to change
as mathematics departments around the world are starting to teach this sub-
ject. I have been teaching two bioinformatics courses at the Department of
Mathematics of the Australian National University (ANU) in Canberra, for
two years now. When I started teaching them I quickly realized that all the
textbooks that I found on the subject were skewed towards computational
issues, reﬂecting, of course, the dominant teaching culture at the time. Those
textbooks were not very satisfying from a mathematician’s point of view and
were unacceptable for my purposes. What I needed was a clear and mathe-
matically rigorous exposition of procedures, algorithms and models commonly
used in bioinformatics. As a result, I began writing my own lecture notes, and
eventually they formed the basis for this book.
The book has two parts corresponding to the two courses. The ﬁrst course
is for second-year students and requires two medium-level ﬁrst year mathemat-
ics courses as prerequisites. It concerns four important topics in bioinformatics
(sequence alignment, proﬁle hidden Markov models, protein folding and phy-
logenetic reconstruction) and covers them in considerable detail. Many math-
ematical issues related to these topics are discussed, but their probabilistic
and statistical aspects are not covered in much depth there, as the students
are not required to have a background in these areas. The second course (in-
tended for third-year students) includes elements of probability and statistics;
this allows one both to explore additional topics in sequence alignment, and
to go back to some of the issues left unexplained in the ﬁrst course, treating
them from the general probabilistic and statistical point of view.
The second course is much more demanding mathematically because of
its probabilistic and statistical component. At the same time, the chapters on
probability and statistics (Chaps. 6 and 8) contain very few proofs. The path
taken in these chapters is to give the reader all the main constructions (for
instance, the construction of probability measure) and to illustrate them by
many examples. Such a style is more gentle on students who only have taken a
couple of mathematical courses and do not possess the mathematical maturity
of a student majoring in mathematics. In fact, this is the general approach
taken in the book: I give very few proofs, but a lot of discussions and examples.

Preface
IX
Nevertheless, the book is quite mathematical in its logical approach, rigor and
paying attention to subtle details.
Thus, for someone who wants to get a mathematical overview of some of
the important topics in bioinformatics but does not want to go too deeply
into the associated probabilistic and statistical issues, Part I of the book is
quite suﬃcient. But it should be stressed that without reading Part II, one’s
understanding of various procedures from Part I will be incomplete.
Although Parts I and II together cover a substantial amount of mater-
ial, none of the topics discussed in the book is treated comprehensively. For
example, the chapter on protein folding (Chap. 4) and the one on phyloge-
netic reconstruction (Chap. 5) could each easily be expanded into a separate
book. The amount of material included in the book is what realistically can be
taught as two one-semester courses. Certainly, if the probability and statistics
components of the book are taught separately in a diﬀerent course, one can ﬁt
in more genuine bioinformatics topics, for example, the analysis of microarray
data, currently not represented in the book at all.
The book concentrates on the mathematical basics of bioinformatics rather
than on recent progress in the area. Even the material included in the book is
found quite demanding by many students, and this is why I decided to select
for it only a few topics in bioinformatics. Thus, this book is by no means a
comprehensive guide to bioinformatics.
This is primarily a textbook for students with some mathematical back-
ground. At the same time, it is suitable for any mathematician, or, indeed,
anyone who appreciates quantitative thinking and mathematical rigor, and
who wants to learn about bioinformatics. It took me a substantial eﬀort to
explain various bioinformatics procedures in a way suitable for a general math-
ematical audience, and hence this book can be thought of, at least to some
extent, as a translation and adaptation of some topics in bioinformatics for
mathematicians. On top of this, the book contains a mathematical introduc-
tion to statistics that I have tried to keep as rigorous as possible.
I would like to thank my colleagues Prof. Sue Wilson and Prof. Simon East-
eal of the Mathematical Sciences Institute (MSI) and the Centre for Bioinfor-
mation Science (CBiS) at the ANU who ﬁrst suggested that I should turn my
lecture notes into a book and encouraged me during the course of writing. I
would like to thank Prof. Peter Hall of the MSI for patiently answering my
many questions on the theory of statistics. Finally, I am grateful to Prof. John
Hutchinson of the Department of Mathematics for encouragement and general
discussions.
Canberra,
March 2004
Alexander Isaev

Contents
Part I Sequence Analysis
1
Introduction: Biological Sequences . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Sequence Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1
Sequence Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
Dynamic Programming: Global Alignment . . . . . . . . . . . . . . . . . .
9
2.3
Dynamic Programming: Local Alignment . . . . . . . . . . . . . . . . . . . 10
2.4
Alignment with Aﬃne Gap Model . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.5
Heuristic Alignment Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5.1
FASTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5.2
BLAST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.6
Signiﬁcance of Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.7
Multiple Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.7.1
MSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.7.2
Progressive Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3
Markov Chains and Hidden Markov Models . . . . . . . . . . . . . . . 25
3.1
Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.2
Hidden Markov Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.3
The Viterbi Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.4
The Forward Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.5
The Backward Algorithm and Posterior Decoding . . . . . . . . . . . 41
3.6
Parameter Estimation for HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.6.1
Estimation when Paths are Known . . . . . . . . . . . . . . . . . . 46
3.6.2
Estimation when Paths are Unknown . . . . . . . . . . . . . . . . 47
3.7
HMMs with Silent States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
3.8
Proﬁle HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.9
Multiple Sequence Alignment by Proﬁle HMMs . . . . . . . . . . . . . 66
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

XII
Contents
4
Protein Folding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.1
Levels of Protein Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.2
Prediction by Proﬁle HMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.3
Threading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
4.4
Molecular Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.5
Lattice HP-Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
5
Phylogenetic Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.1
Phylogenetic Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.2
Parsimony Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
5.3
Distance Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
5.4
Evolutionary Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
5.4.1
The Jukes-Cantor Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.4.2
The Kimura Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.4.3
The Felsenstein Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.4.4
The Hasegawa-Kishino-Yano (HKY) Model . . . . . . . . . . . 130
5.5
Maximum Likelihood Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.6
Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
Part II Mathematical Background for Sequence Analysis
6
Elements of Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
6.1
Sample Spaces and Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
6.2
Probability Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.3
Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
6.4
Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6.5
Integration of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 163
6.6
Monotone Functions on the Real Line . . . . . . . . . . . . . . . . . . . . . . 172
6.7
Distribution Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
6.8
Common Types of Random Variables . . . . . . . . . . . . . . . . . . . . . . 181
6.8.1
The Discrete Type. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
6.8.2
The Continuous Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6.9
Common Discrete and Continuous Distributions . . . . . . . . . . . . . 184
6.9.1
The Discrete Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
6.9.2
The Continuous Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
6.10 Vector-Valued Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.11 Sequences of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205

Contents
XIII
7
Signiﬁcance of Sequence Alignment Scores . . . . . . . . . . . . . . . . . 209
7.1
The Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
7.2
Random Walks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
7.3
Signiﬁcance of Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
8
Elements of Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
8.1
Statistical Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
8.2
Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
8.3
Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
8.4
Signiﬁcance of Scores for Global Alignments . . . . . . . . . . . . . . . . 266
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
9
Substitution Matrices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
9.1
The General Form of a Substitution Matrix . . . . . . . . . . . . . . . . . 271
9.2
PAM Substitution Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
9.3
BLOSUM Substitution Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289

Part I
Sequence Analysis

1
Introduction: Biological Sequences
This book is about analyzing sequences of letters from a ﬁnite alphabet Q.
Although most of what follows can be applied to sequences derived from
arbitrary alphabets, our primary interest will be in biological sequences, that
is, DNA, RNA and protein sequences.
DNA (deoxyribonucleic acid) sequences are associated with the four-letter
DNA alphabet {A, C, G, T}, where A, C, G and T stand for the nucleic acids
or nucleotides adenine, cytosine, guanine and thymine respectively. Most DNA
sequences currently being studied come from DNA molecules found in chro-
mosomes that are located in the nuclei of the cells of living organisms. In fact,
a DNA molecule consists of two strands of nucleotides (attached to a sugar-
phosphate backbone) twisted into the well-known double-helical arrangement.
The two-strand structure is important for the replication of DNA molecules.
There is a pairing (called hybridization) of nucleotides across the two strands:
A is bonded to T, and C is bonded to G. Therefore, if one knows the sequence
of one strand of a DNA molecule, that of the other strand can easily be re-
constructed, and DNA sequences are always given as sequences of single, not
paired nucleotides. The chemistry of the backbone of each strand of a DNA
molecule determines a particular orientation of the strand, the so-called 5′ to
3′ orientation. This is the orientation in which DNA sequences are written.
It should be noted that the orientations of the two strands in a DNA mole-
cule are opposite (for this reason the strands are said to be antiparallel), and
therefore, although the sequences of the strands determine one another, they
are read in opposite directions.
Traditionally, DNA research has been focused on special stretches of the
strands of DNA molecules called protein-coding genes; they are found on both
strands, and rarely overlap across the strands. Protein-coding genes are used
to produce proteins which are linear polymers of 20 diﬀerent amino acids
linked by peptide bonds. The single-letter amino acid notation is given in
Table 1.1.

4
1 Introduction
Table 1.1.
Single letter code Amino acid
A
Alanine
R
Arginine
N
Asparagine
D
Aspartic acid
C
Cysteine
Q
Glutamine
E
Glutamic acid
G
Glycine
H
Histidine
I
Isoleucine
L
Leucine
K
Lysine
M
Methionine
F
Phenylalanine
P
Proline
S
Serine
T
Threonine
W
Tryptophan
Y
Tyrosine
V
Valine
Thus, protein sequences are associated with the 20-letter amino-acid al-
phabet {A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y }. The three-
dimensional structure of a protein molecule results from the folding of the
polypeptide chain and is much more complicated than that of a DNA mole-
cule. A protein can only function properly if it is correctly folded. Deriving
the correct three-dimensional structure from a given protein sequence is the
famous protein folding problem that is still largely unsolved (see Chap. 4).
The main components of a protein-coding gene are codons. Each codon is
a triplet of nucleotides coding for a single amino acid. The process of produc-
ing proteins from genes is quite complex. Every gene begins with one of the
standard start codons indicating the beginning of the process and ends with
one of the standard stop codons indicating the end of it. A particular way
codons code for amino acids is called a genetic code. Several genetic codes
are known, and diﬀerent ones apply to diﬀerent DNA molecules depending on
their origin (see, e.g., [Kan]). When the sequence of a gene is read from the
start to the stop codon, a growing chain of amino acids is made which, once
the stop codon has been reached, becomes a complete protein molecule. It has
a natural orientation inherited from that of the gene used to produce it, and
this is the orientation in which protein sequences are written. The start of a
protein chain is called the amino end and the end of it the carboxy end.

1 Introduction
5
In fact, proteins are derived from genes in two steps. Firstly, RNA (ribonu-
cleic acid) is made (this step is called transcription) and, secondly, the RNA
is used to produce a protein (this step is called translation). RNA is another
linear macromolecule, it is closely related to DNA. RNA is single-stranded,
its backbone is slightly diﬀerent from that of DNA, and instead of the nu-
cleic acid thymine the nucleic acid uracil denoted by U is used. Thus, RNA
sequences are associated with the four-letter RNA alphabet {A, C, G, U}. An
RNA molecule inherits its orientation from that of the DNA strand used to
produce it, and this is the orientation in which RNA sequences are written.
Since RNA is single-stranded, parts of it can hybridize with its other parts
which gives rise to non-trivial three-dimensional structures essential for the
normal functioning of the RNA. There are in fact many types of RNA pro-
duced from not necessarily protein-coding genes, but from RNA-coding genes.
The RNA derived from a protein-coding gene is called messenger RNA or
mRNA. As an example of RNA of another type we mention transfer RNA or
tRNA that takes part in translating mRNA into protein.
In this book we concentrate on DNA and protein sequences although every-
thing that follows can be applied, at least in principle, to RNA sequences as
well (subject to the availability of RNA sequence data). In fact, most proce-
dures are so general that they work for sequences of letters from any ﬁnite
alphabet, and for illustration purposes we often use the artiﬁcial two- and
three-letter alphabets {A, B} and {A, B, C}.
Table 1.2.
Database Principal functionOrganization
Address
MEDLINEBibliographic
National Library of
www.nlm.nih.gov
Medicine
GenBank Nucleotide
National Center for
www.ncbi.nlm.nih.gov
sequences
Biotechnology Information
EMBL
Nucleotide
European Bioinformatics
www.ebi.ac.uk
sequences
Institute
DDBJ
Nucleotide
National Institute of
www.ddbj.nig.ac.jp
sequences
Genetics, Japan
SWISS-
Amino acid
Swiss Institute of
www.expasy.ch
PROT
sequences
Bioinformatics
PIR
Amino acid
National Biomedical
www-nbrf.georgetown.edu
sequences
Research Foundation
PRF
Amino acid
Protein Research
www.prf.or.jp
sequences
Foundation, Japan
PDB
Protein
Research Collaboratory for www.rcsb.org
structures
Structural Bioinformatics
CSD
Protein
Cambridge Crystallographic www.ccdc.cam.ac.uk
structures
Data Centre

6
1 Introduction
Biological sequences are organized in databases, many of which are public.
In Table 1.2 we list all major public molecular biology databases. Detailed
information on them can be found in [Kan].

2
Sequence Alignment
2.1 Sequence Similarity
New DNA, RNA and protein sequences develop from pre-existing sequences
rather than get invented by nature from scratch. This fact is the foundation
of any sequence analysis. If we manage to relate a newly discovered sequence
to a sequence about which something (e.g., structure or function) is already
known, then chances are that the known information applies, at least to some
extent, to the new sequence as well. We will think of any two related sequences
as sequences that arose from a common ancestral sequence during the course
of evolution and say that they are homologous. It is sequence homology that
will be of interest to us during much of the book. Of course, if we believe
that all life forms on earth came from the same origin and apply the above
deﬁnition directly, then all sequences are ultimately homologous. In practice,
two sequences are called homologous, if one can establish their relatedness by
currently available methods, and it is the sensitivity of the methods that pro-
duces a borderline between sequences called homologous and ones that are not
called homologous. For example, two protein sequences can be called homol-
ogous, if one can show experimentally that their functions in the respective
organisms are related. Thus, sequence homology is a dynamic concept, and
families of homologous sequences known at the moment may change as the
sensitivity of the methods improves.
The ﬁrst step towards inferring homology is to look for sequence similarity.
If two given sequences are very long, it is not easy to decide whether or not
they are similar. To see if they are similar, one has to properly align them.
When sequences evolve starting from a common ancestor, their residues can
undergo substitutions (when residues are replaced by some other residues).
Apart from substitutions, during the course of evolution sequences can accu-
mulate a number of events of two more types: insertions (when new residues
appear in a sequence in addition to the existing ones) and deletions (when
some residues disappear). Therefore, when one is trying to produce the best
possible alignment between two sequences, residues must be allowed to be

8
2 Sequence Alignment
aligned not only to other residues but also to gaps. The presence of a gap in
an alignment represents either an insertion or deletion event. Consider, for
example, the following two very short nucleotide sequences, each consisting of
only seven residues
x : TACCAGT
y : CCCGTAA.
The sequences are of the same length, and there is only one way to align them,
if one does not allow gaps in alignments
x : T A C C A G T
y : C C C G T A A.
However, if we allow gaps, there are many possible alignments. In particular,
the following alignment seems to be much more informative than the preceding
one
x : T A C C A G T −−
y : C −C C −G T A A.
(2.1)
Alignment (2.1) indicates that the subsequence CCGT may be an evolutionar-
ily conserved region, which means that both x and y may have evolved from a
common ancestral sequence containing the subsequence CCGT in appropriate
positions. Another possible alignment that also looks reasonable is
x : T A C C A G T −−
y : −−C C C G T A A.
(2.2)
How can one choose between alignments (2.1) and (2.2)? Are there any
better alignments? To answer these questions we need to be able to score
any possible alignment. Then the alignments that have the highest score are
by deﬁnition the best or optimal ones (there may be more than one such
alignments).
The most popular scoring schemes assume independence among the
columns in an alignment and set the total score of the alignment to be
equal to the sum of the scores of each column. Therefore, for such schemes
one only needs to specify the scores s(a, b) = s(b, a) and the gap penalty
s(−, a) = s(a, −), with a, b ∈Q, where Q is either the 4-letter DNA
or RNA alphabet, or the 20-letter amino acid alphabet, depending on the
kind of sequences that we are interested in aligning. The resulting best
alignments between two sequences depend, of course, on the scoring
scheme. It is possible that for two diﬀerent scoring schemes the best
alignments will be entirely diﬀerent. As an example of a scoring scheme
one can set s(a, a) = 1 (the score of a match), s(a, b) = −1, if a ̸= b
(the score of a mismatch), and s(−, a) = s(a, −) = −2 (the gap penalty)
– see Exercise 2.1. However, it is important to keep in mind that a scor-
ing scheme must be biologically relevant in order to produce a sensible
alignment. Such a scheme must take into account factors that constrain se-
quence evolution. For example, many popular scoring schemes introduce some

2.2 Dynamic Programming: Global Alignment
9
degree of dependence among the columns in an alignment by making the score
of a continuous gap region an aﬃne function of its length (note that in the
example above the score of a gap region is linear in its length).
The numbers s(a, b) form a scoring or substitution matrix. Substitution
matrices are always symmetric and must possess some speciﬁc properties in
order to be successfully used for sequence comparison. These properties will be
discussed in detail in Chaps. 7 and 9. The most popular substitution matrices
are so-called PAM and BLOSUM matrices used to align amino acid sequences.
These 20×20-matrices are derived by statistically analyzing known amino acid
sequences. The derivation of PAM and BLOSUM matrices will be explained
in Chap. 9. For the purposes of the ﬁrst part of the book we assume that
the substitution matrix is given. For simplicity, we will initially restrict our
considerations only to DNA sequences, thus assuming that Q = {A, C, G, T}.
This setup will be suﬃcient to demonstrate the main principles of alignment
procedures.
2.2 Dynamic Programming: Global Alignment
In this section we assume a linear gap model (that is, s(−, a) = s(a, −) = −d
for a ∈Q, with d > 0, so that the score of a gap region of length L is equal
to −dL) and present an algorithm, the Needleman-Wunsch algorithm [NW],
[G], that always ﬁnds all optimal global alignments (there are frequently more
than one such alignments).
The idea is to produce an optimal alignment from optimal alignments
of subsequences. Algorithms that achieve optimization by means of perform-
ing optimization for smaller amounts of data (in this case subsequences) are
generally called dynamic programming algorithms. Suppose we are given two
sequences x = x1x2 . . . xi . . . xn and y = y1y2 . . . yj . . . ym. We construct an
(n + 1) × (m + 1)-matrix F. Its (i, j)th element F(i, j) for i = 1, . . . , n,
j = 1, . . . , m is equal to the score of an optimal alignment between x1 . . . xi and
y1 . . . yj. The element F(i, 0) for i = 1, . . . , n is the score of aligning x1 . . . xi to
a gap region of length i. Similarly, the element F(0, j) for j = 1, . . . , m is the
score of aligning y1 . . . yj to a gap region of length j. We build F recursively
initializing it by the condition F(0, 0) = 0 and then proceeding to ﬁll the
matrix from the top left corner to the bottom right corner. If F(i −1, j −1),
F(i −1, j) and F(i, j −1) are known, F(i, j) is clearly calculated as follows
F(i, j) = max
⎧
⎨
⎩
F(i −1, j −1) + s(xi, yj),
F(i −1, j) −d,
F(i, j −1) −d.
Indeed, there are three possible ways to obtain the best score F(i, j): xi can be
aligned to yj (see the ﬁrst option in the formula above), or xi is aligned to a gap
(the second option), or yj is aligned to a gap (the third option). Calculating

10
2 Sequence Alignment
F(i, j) we keep a pointer to the option from which F(i, j) was produced. When
we reach F(n, m) we trace back the pointers to recover optimal alignments.
The value F(n, m) is exactly their score. Note that more than one pointers
may come out of a particular cell of the matrix which results in several optimal
alignments.
Example 2.1. Let x = CTTAGA, y = GTAA, and suppose that we are using
the scoring scheme: s(a, a) = 1, s(a, b) = −1, if a ̸= b, and s(−, a) = s(a, −) =
−2. The corresponding matrix F with pointers is shown in Fig. 2.1. Tracing
F
    0
    _
    1
   G
     2
    T
    3
    A
    4
    A
0     _
    0
    -2
     -4
    -6
   -8
1    C
   -2
    -1
     -3
     -5
   -7
2    T
   -4
    -3
      0
     -2
   -4
3    T
   -6
    -5
     -2
     -1
   -3
4    A
   -8
    -7
     -4
     -1
    0
5    G
  -10
    -7
     -6
     -3
   -2
6    A
  -12
    -9
     -8
     -5
   -2
Fig. 2.1.
back the pointers gives the following three optimal alignments
x : C T T A G A
y : G −T A −A,
x : C T T A G A
y : G T −A −A,
x : C T T A G A
y : −G T A −A
with score −2. The corresponding paths through the matrix F are shown with
thick arrows.
2.3 Dynamic Programming: Local Alignment
A more biologically interesting alignment problem is to ﬁnd all pairs of subse-
quences of two given sequences that have the highest-scoring alignments. We

2.3 Dynamic Programming: Local Alignment
11
will only be interested in subsequences of consecutive elements or segments.
Any such subsequence of a sequence x1x2 . . . xn has the form xixi+1 . . . xi+k
for some 1 ≤i ≤n and k ≤n −i. This alignment problem is called the lo-
cal alignment problem. Here we present the Smith-Waterman algorithm [SW]
that solves the problem for a linear gap model.
We construct an (n + 1) × (m + 1)-matrix as in the previous section, but
the formula for its entries is slightly diﬀerent
F(i, j) = max
⎧
⎪
⎨
⎪
⎩
0,
F(i −1, j −1) + s(xi, yj),
F(i −1, j) −d,
F(i, j −1) −d.
(2.3)
Taking the ﬁrst option in the above formula is equivalent to starting a new
alignment: if an optimal alignment up to some point has a negative score, it
is better to start a new alignment, rather than to extend the current one.
Another diﬀerence is that now an alignment can end anywhere in the
matrix, so instead of taking the value F(n, m) in the bottom right corner of
the matrix for the best score, we look for a maximal elements in the matrix
F and start traceback from there. The traceback ends when we get to a cell
with value 0, which corresponds to the start of the alignment.
Example 2.2. For the sequences from Example 2.1 the only best local align-
ment is
x : T A
y : T A,
and its score is equal to 2. The corresponding dynamic programming matrix
F is shown in Fig. 2.2, where the thick arrows represent traceback. Note that
if an element of F is equal to 0 and no arrows come out of the cell containing
this element, then the element is obtained as the ﬁrst option in formula (2.3).
F
    0
    _
     1
    G
     2
     T
     3
     A
     4
    A
0     _
    0
     0
      0
     0
    0
1   C
    0
     0
      0
     0
    0
2   T
    0
     0
      1
      0
    0
3   T
    0
     0
      1
      0
    0
4   A
    0
     0
      0
      2
    1
5   G
    0
     1
      0
      0
    1
6   A
    0
     0
      0
      1
    1
Fig. 2.2.

12
2 Sequence Alignment
2.4 Alignment with Aﬃne Gap Model
In this section we will assume an aﬃne gap model, that is, we set the score
of any gap region of length L to be equal to −d −e(L −1) for some d > 0
and e > 0. In this case −d is called the gap opening penalty and −e the gap
extension penalty. Usually, e is set to be smaller than d, which reﬂects the fact
known from biology that starting a gap region is harder than extending it.
We will only discuss a global alignment algorithm here, its local version can
be readily obtained as in the preceding section (see also [G]). The algorithm
requires three matrices: one (n+1)×(m+1)-matrix and two n×m-matrices.
Let M(i, j) for i = 1, . . . , n and j = 1, . . . , m be the score of an optimal
alignment between x1 . . . xi and y1 . . . yj, given that the alignment ends with
xi aligned to yj. The element M(i, 0) for i = 1, . . . , n is the score of aligning
x1 . . . xi to a gap region of length i. Similarly, the element M(0, j) for j =
1, . . . , m is the score of aligning y1 . . . yj to a gap region of length j. Further, let
Ix(i, j) for i = 1, . . . , n and j = 1, . . . , m be the score of an optimal alignment
between x1 . . . xi and y1 . . . yj given that the alignment ends with xi aligned
to a gap. Finally, let Iy(i, j) for i = 1, . . . , n and j = 1, . . . , m be the score of
an optimal alignment between x1 . . . xi and y1 . . . yj given that the alignment
ends with yj aligned to a gap. Then, if we assume that a deletion is never
followed directly by an insertion (unless the deletion starts at the beginning
of an alignment), we have
M(i, j) = max
⎧
⎨
⎩
M(i −1, j −1) + s(xi, yj),
Ix(i −1, j −1) + s(xi, yj),
Iy(i −1, j −1) + s(xi, yj),
Ix(i, j) = max

M(i −1, j) −d,
Ix(i −1, j) −e,
Iy(i, j) = max
 M(i, j −1) −d,
Iy(i, j −1) −e.
These recurrence relations allow us to ﬁll in the matrices M, Ix and Iy,
once we initialize the process by the condition M(0, 0) = 0. We note that if,
for some i and j, one of the options in the right-hand sides of the recurrence
relations is not deﬁned (for example, in the formula for M(1, 2) the right-
hand side contains Ix(0, 1) and Iy(0, 1)), then this option is not taken into
account in calculations. The score of an optimal alignment is then given by
max{M(n, m), Ix(n, m), Iy(n, m)}, and traceback starts at the element (or
elements) that realize this maximum.
Example 2.3. Let x = ACGGTAC, y = GAGGT, the score of any match be
equal to 1, the score of any mismatch be equal to -1, d = 3 and e = 2. Then
we obtain the dynamic programming matrices shown in Fig. 2.3.

2.4 Alignment with Aﬃne Gap Model
13
Ix
1
G
2
A
3
G
4
G
5
T
1      A
-6
      M(0,1)
-8
M(0,2)
-10
M(0,3)
-12
M(0,4)
-14
M(0,5)
2      C
-4
M(1,1)
-5
M(1,2)
-9
M(1,3)
-11
M(1,4)
-13
M(1,5)
3      G
-6
-5
M(2,2)
-6
M(2,3)
-9
M(2,4)
-11
M(2,5)
4      G
-7
M(3,1)
-7
-4
M(3,3)
-5
M(3,4)
-10
M(3,5)
5      T
-9
M(4,1)
-8
M(4,2)
-6
-3
M(4,4)
-6
M(4,5)
6      A
-11
-10
M(5,2)
-8
-5
-2
M(5,5)
7      C
-13
-11
M(6,2)
-10
-7
-4
Iy
1
G
2
A
3
G
4
G
5
T
1      A
-6
      M(1,0)
-4
M(1,1)
-5
M(1,2)
-7
-9
2      C
-8
M(2,0)
-7
M(2,1)
-5
M(2,2)
-6
M(2,3)
-8
3      G
-10
M(3,0)
-7
M(3,1)
-8
M(3,2)
-4
M(3,3)
-5
M(3,4)
4      G
-12
M(4,0)
-9
M(4,1)
-8
M(4,2)
-7
M(4,3)
-3
M(4,4)
5      T
-14
M(5,0)
-13
M(5,1)
-10
M(5,2)
-9
M(5,3)
-8
M(5,4)
6      A
-16
M(6,0)
-15
M(6,1)
-11
M(6,2)
-11
M(6,3)
-10
M(6,4)
7      C
-18
M(7,0)
-17
M(7,1)
-15
M(7,2)
-12
M(7,3)
-12
M(7,4)
M
0
_
1
G
2
A
3
G
4
G
5
T
0      _
0
-3
-5
-7
-9
-11
1      A
-3
-1
-2
-6
-8
-10
2      C
-5
-4
-2
-3
-6
Iy(1,3)
-8
Iy(1,4)
3      G
-7
-4
-5
Ix(2,1)
-1
-2
-7
Iy(2,4)
4      G
-9
-6
-5
-4
Ix(3,2)
0
-3
5      T
-11
-10
-7
-6
-5
Ix(4,3)
1
6      A
-13
-12
-8
Ix(5,1)
-8
-7
Ix(5,3)
-4
Ix(5,4)
7      C
-15
-14
-12
Ix(6,1)
-9
-9
Ix(6,3)
-6
Ix(6,4)
Fig. 2.3.
The arrows and labels indicate from which elements of the three matrices
each number was produced (in addition, we draw the vertical and horizontal
arrows in the 0th column and 0th row of the matrix M). The thick arrows show
traceback; it starts at Ix(7, 5) = −4. The corresponding optimal alignment
with score −4 is
x : A C G G T A C
y : G A G G T −−.
There are many other variants of the basic dynamic programming algo-
rithm: for overlap matches, repeated matches, more complex gap models, etc.

14
2 Sequence Alignment
We do not discuss all of them here since the main ideas behind them are
very similar. The interested reader is referred to [DEKM]. We also mention
that dynamic programming algorithms can be easily modiﬁed in order to be
applicable to aligning sequences from diﬀerent alphabets as, for example, is
required in Sect. 4.3. All the algorithms yield the exact highest score according
to a given scoring scheme and generate the corresponding optimal alignments
by a traceback procedure. However, when one has to deal with very long se-
quences, a time complexity O(nm) might not be good enough for performing
the required search on a computer in an acceptable amount of time. There
is a similar problem with memory complexity. Indeed, in practice homology
search takes the form of comparing a new query sequence with all sequences
in a large database (see Table 1.2 in the Introduction). Various heuristic al-
gorithms have been developed to overcome this diﬃculty. These algorithms
are faster, but the trade-oﬀis that one might not necessarily ﬁnd the best
possible alignments.
2.5 Heuristic Alignment Algorithms
In this section we will brieﬂy discuss two alignment algorithms that are not
guaranteed to ﬁnd an optimal alignment, but are substantially faster than the
dynamic programming algorithms introduced above.
2.5.1 FASTA
Calculation of the dynamic programming matrices for a pair of sequences takes
a substantial amount of time and memory. However, as we have seen, the ar-
eas of interest in these matrices (that is, the areas formed by the cells that
take part in a traceback procedure) are minuscule in comparison to the entire
areas of the matrices. The FASTA algorithm [PL] is designed to limit the
areas of the matrices that the dynamic programming examines. First, FASTA
determines candidate areas (that is, the areas that are likely to produce op-
timal alignments) and then the relevant dynamic programming algorithm is
applied with the additional condition that all the elements of the matrices
that lie outside the candidate areas are equal to −∞. With these condition
satisﬁed, the corresponding traceback procedure never leaves the candidate
areas. Since the sizes of the candidate areas are usually very small compared
to the sizes of the full matrices, this reduction gives a signiﬁcant increase in
computational speed.
Candidate areas can be identiﬁed, for example, by considering the dot
matrix for two given sequences. It is formed by dots that are entered for
all positions corresponding to matching pairs of letters in the two sequences.
Local similarities can then be detected as diagonal stretches of consecutive
dots. An example of a dot matrix is shown in Fig. 2.4, where diagonal stretches
of length at least 3 are shaded.

2.5 Heuristic Alignment Algorithms
15
Dot
Matrix
G
T
C
A
G
A
C
G
C
T
C
A
C
*
*
*
   *
A
*
*
*
G
*
*
*
A
*
*
*
G
*
*
*
T
*
*
T
*
*
A
*
*
*
C
*
*
*
*
G
*
*
*
T
*
*
C
*
*
*
*
A
*
*
*
Fig. 2.4.
Once diagonal stretches of length at least k have been identiﬁed, we extend
them to full diagonals, and candidate areas are constructed as bands of width
b around the extensions, for some preset values of k and b. Figure 2.5 shows
the candidate areas for the sequences from Fig. 2.4 with k = 3 and b = 1 in
the case of alignments with linear gap model (we omitted the 0th row and
column that remain unchanged). Such areas can also be deﬁned for each of
the three matrices required in the case of alignments with aﬃne gap model.
F
G
T
C
A
G
A
C
G
C
T
C
A
C
-∞
-∞
-∞
-∞
-∞
-∞
-∞
-∞
A
-∞
-∞
-∞
-∞
-∞
-∞
-∞
G
-∞
-∞
-∞
-∞
-∞
-∞
A
-∞
-∞
-∞
-∞
-∞
G
-∞
-∞
-∞
-∞
-∞
T
-∞
-∞
-∞
-∞
-∞
T
-∞
-∞
-∞
-∞
-∞
A
-∞
-∞
-∞
-∞
-∞
C
-∞
-∞
-∞
-∞
G
-∞
-∞
-∞
-∞
T
-∞
-∞
-∞
-∞
C
-∞
-∞
-∞
-∞
-∞
A
-∞
-∞
-∞
-∞
-∞
-∞
Fig. 2.5.

16
2 Sequence Alignment
Of course, FASTA searches can miss optimal alignments. The values of
k and b are trade-oﬀs between time and optimality. An implementation of
FASTA with many user-adjustable parameters (including k and b) can be
found on http://bioweb.pasteur.fr/seqanal/interfaces/fasta.html.
2.5.2 BLAST
Unlike FASTA, the BLAST (Basic Local Alignment Search Tool) [AL] al-
gorithms is not based on dynamic programming. Initially, BLAST looks
for short stretches of identities (just as FASTA does) and then attempts
to extend them in either direction in search of a good longer alignment.
This strategy is reasonable biologically, since two related sequences tend
to share well-conserved segments. Although the search algorithm imple-
mented in BLAST is entirely heuristic, it is the most successful search
tool up to date. One of many implementations of BLAST can be found on
http://bioweb.pasteur.fr/seqanal/interfaces/blast2.html.
2.6 Signiﬁcance of Scores
A major concern when interpreting database search results is whether a simi-
larity found between two sequences is biologically signiﬁcant, especially when
the similarity is only marginal. Since good alignments can occur by chance
alone, one needs to perform some further statistical analysis to assess their
signiﬁcance. This issue will be discussed in considerable detail in Chap. 7 (see
also Sect. 8.4). Assessment of the signiﬁcance of sequence alignment scores is
part of both FASTA and BLAST algorithms.
2.7 Multiple Alignment
In sequence analysis one is often interested in determining common features
among a collection of sequences. To identify such a feature, one needs to
determine an optimal multiple alignment for the whole collection. As in the
case of two sequences, in order to produce an optimal multiple alignment,
one needs to be able to score any multiple alignment by using some scoring
scheme. Analogously to the case of two sequences, most alignment methods
assume that the individual columns of an alignment not containing gaps are
independent and so use a scoring function of the form
S(M) = G(M) +

i
s(Mi),
where M denotes a multiple alignment, Mi is the ith column not containing
gaps, s(Mi) is the score of Mi, and G is a function for scoring columns with
gaps.

2.7 Multiple Alignment
17
In the standard (but not very satisfactory) methods for scoring multiple
alignments, columns not containing gaps are scored by the “sum of pairs”
(SP) function. The SP-score for a column Mi not containing gaps is deﬁned
as
s(Mi) =

k<l
s(M k
i , M l
i),
where the sum is taken over all pairs (M k
i , M l
i), k < l, of elements of Mi,
and the scores s(a, b), for a, b ∈Q, come from a substitution matrix used for
scoring pairwise sequence alignments (such as PAM and BLOSUM matrices
discussed in Chap. 9). Often gaps are scored by deﬁning s(−, a) = s(a, −),
setting s(−, −) = 0 and introducing the corresponding SP-score for columns
containing gaps. We call any such way of scoring gap regions a linear gap
model for multiple alignments. Summing all pairwise substitution scores may
seem to be a natural thing to do, but in fact there is no statistical justiﬁcation
for the SP-score.
Once a scheme for scoring multiple alignments has been ﬁxed, it is possible
to generalize pairwise dynamic programming algorithms to aligning n ≥3
sequences. In problems involving multiple alignments one is usually interested
in global alignments, and what follows is a generalization of the Needleman-
Winsch algorithm. Here we assume a scoring scheme for which
S(M) =

i
s(Mi),
(2.4)
with the sum taken over all columns of the alignment (including the ones
containing gaps). We note that a multidimensional dynamic programming
algorithm with aﬃne gap model exists as well.
Suppose we have n sequences x1 = x1
1 . . . x1
m1, x2 = x2
1 . . . x2
m2, . . ., xn =
xn
1 . . . xn
mn. Let i1, . . . , in be integers with 0 ≤ij ≤mj, j = 1, . . . , n, where
at least one number is non-zero. Deﬁne F(i1, . . . , in) as the maximal score
of an alignment of the subsequences ending with x1
i1 . . . xn
in (if for some j we
have ij = 0, then the other subsequences are aligned to a gap region). The
recursion step of the dynamic programming algorithm is given by
F(i1, . . . , in) = max
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
F(i1 −1, . . . , in −1) + s(x1
i1, . . . , xn
in),
F(i1, i2 −1 . . . , in −1) + s(−, x2
i2, . . . , xn
in),
F(i1 −1, i2, i3 −1 . . . , in −1) + s(x1
i1, −, x3
i3, . . . , xn
in),
...
F(i1 −1, . . . , in−1 −1, in) + s(x1
i1, . . . , xn−1
in−1, −),
F(i1, i2, i3 −1 . . . , in −1) + s(−, −, x3
i3, . . . , xn
in),
...
where all combinations of gaps occur except the one where all residues are
replaced by gaps. The algorithm is initialized by setting F(0, . . . , 0) = 0.
Traceback starts at F(m1, . . . , mn) and is analogous to that for pairwise

18
2 Sequence Alignment
alignments. The matrix

F(i1, . . . , in)

with 0 ≤ij ≤mj, j = 1, . . . , n, is
an (m1 + 1) × . . . × (mn + 1)-matrix, and it is convenient to visualize it by
considering its two-dimensional sections.
Example 2.4. We will ﬁnd all optimal alignments of the three sequences x =
AATC, y = GTC, z = AAG using the following scoring scheme: the score of
an alignment is calculated from the scores of its columns Mi’s from formula
(2.4); if Mi contains three identical symbols, set s(Mi) = 2; if it contains
exactly two identical symbols, but no gaps, set s(Mi) = 1; if it contains three
distinct symbols, but no gaps, set s(Mi) = −1; if it contains exactly one gap,
set s(Mi) = −2; if it contains two gaps, set s(Mi) = −4. In this case the
indices i1, i2 and i3 correspond to sequences x, y and z respectively. Figure
2.6 shows the four sections of the matrix F in the direction of i3.
F(*,*,0)
0
_
1
G
2
T
3
C
0       _
0
-4
-8
-12
1       A
-4
-2
-6
-10
2      A
-8
-6
-4
-8
3      T
-12
-10
-8
-6
4     C
-16
-14
-12
-10
F(*,*,1)
0
_
1
G
2
T
3
C
0       _
-4
F(0,0,0)
-2
F(0,0,0)
-6
F(0,1,0)
-10
F(0,2,0)
1       A
-2
F(0,0,0)
1
F(0,0,0)
-3
F(0,1,0)
-7
F(0,2,0)
2      A
-6
F(1,0,0)
-3
F(1,0,0)
-1
F(1,1,0)
-5
F(1,2,0)
3      T
-10
F(2,0,0)
-7
-5
F(2,1,0)
-3
4     C
-14
F(3,0,0)
-11
-9
-7
F(3,2,0)
F(*,*,2)
0
_
1
G
2
T
3
C
0       _
-8
F(0,0,1)
-6
F(0,1,1)
F(0,0,1)
-4
F(0,1,1)
-8
F(0,2,1)
1       A
-6
F(1,0,1)
F(0,0,1)
-3
F(1,1,1)
F(0,0,1)
-1
F(1,1,0)
F(0,1,1)
-5
F(1,2,1)
F(0,2,1)
2      A
-4
F(1,0,1)
-1
F(1,1,1)
F(1,0,1)
2
F(1,1,1)
-2
F(1,2,1)
3      T
-8
F(2,0,1)
-5
F(2,1,1)
-2
F(2,1,1)
0
4     C
-12
F(3,0,1)
-9
F(3,1,1)
-6
-4
F(3,2,1)
F(*,*,3)
0
_
1
G
2
T
3
C
0       _
-12
F(0,0,2)
-10
F(0,1,2)
F(0,0,2)
-8
F(0,2,2)
F(0,1,2)
-6
F(0,2,2)
1       A
-10
F(1,0,2)
F(0,0,2)
-7
F(1,1,2)
F(0,0,2)
-5
F(1,2,2)
F(1,1,2)
-3
F(1,2,2)
2      A
-8
F(1,0,2)
F(2,0,2)
-5
F(2,1,2)
F(1,1,2)
F(1,0,2)
-2
F(2,2,2)
0
F(2,2,2)
3      T
-6
F(2,0,2)
-3
F(2,1,2)
F(2,0,2)
0
F(2,2,2)
F(2,1,2)
1
F(2,2,2)
4     C
-10
F(3,0,2)
-7
F(3,1,2)
F(3,0,2)
-4
F(3,2,2)
-1
F(3,2,2)
Fig. 2.6.

2.7 Multiple Alignment
19
As before, the arrows and labels indicate from which elements each number
was derived. The shaded cells and thick arrows correspond to traceback; it
starts at F(4, 3, 3) = −1 and goes through the shaded cells until we reach
F(0, 0, 0). The traceback produces the following three paths
F(4, 3, 3) →F(3, 2, 2) →F(2, 1, 1) →F(1, 0, 0) →F(0, 0, 0),
F(4, 3, 3) →F(3, 2, 2) →F(2, 1, 1) →F(1, 1, 1) →F(0, 0, 0),
F(4, 3, 3) →F(3, 2, 2) →F(2, 2, 2) →F(1, 1, 1) →F(0, 0, 0).
They respectively give rise to the following three optimal alignments with
score -1
x : A A T C
y : −G T C
z : −A A G
,
x : A A T C
y : G −T C
z : A −A G
,
x : A A T C
y : G T −C
z : A A −G
.
Because of the memory and time complexity, the above algorithm in prac-
tice cannot be applied to align a large number of sequences. Therefore alter-
native algorithms (mainly heuristic) have been developed. Below we brieﬂy
mention some of them.
2.7.1 MSA
MSA always ﬁnds all optimal alignments and is based on the multi-dimensional
dynamic programming algorithm. The idea is to reduce the number of ele-
ments of the dynamic programming matrix that need to be examined to ﬁnd
an optimal multiple alignment [CL], [LAK]. MSA can successfully align up to
seven protein sequences of length up to 300 residues.
Here we assume an SP-scoring system for both residues and gaps (that is,
an SP-scoring scheme with linear gap model), hence the score of a multiple
alignment is the sum of the scores of all pairwise alignments induced by the
multiple alignment. Let M denote a multiple alignment, and M kl the induced
pairwise alignment between the kth and lth sequences. Then we have
S(M) =

k<l
S(M kl),
where S(M kl) is the score of M kl. Clearly, if skl is the score of an optimal
global alignment between the kth and lth sequences, then S(M kl) ≤skl.
Assume that we have a lower bound τ on the score of an optimal multiple
alignment. Such a bound can be found by any fast (and not necessarily very
precise) heuristic multiple alignment algorithm such as the Star Alignment
algorithm discussed below. Then, for an optimal multiple alignment M0 we
have
τ ≤S(M0) =

k′<l′
S(M k′l′
0
) ≤S(M kl
0 ) −skl +

k′<l′
sk′l′,
for all k and l. Hence

20
2 Sequence Alignment
S(M kl
0 ) ≥tkl,
where
tkl = τ + skl −

k′<l′
sk′l′.
(2.5)
The scores of the pairwise optimal alignments in the right-hand side of the
above formula can be calculated as discussed in the preceding sections and
hence tkl can be determined (at least approximately).
Thus, we only need to look for such multiple alignments that induce pair-
wise alignments with scores no less than the numbers tkl. This observation
substantially reduces the number of elements in the multi-dimensional dy-
namic programming matrix that need to be examined and hence increases
computational speed.
2.7.2 Progressive Alignment
Progressive alignment methods are heuristic in nature. They produce multiple
alignments from a number of pairwise alignments. A common scheme for such
methods is as follows: ﬁrst, two sequences are chosen and aligned, then a third
sequence is chosen and aligned (in some speciﬁc way) to the alignment of the
ﬁrst two sequences, and the process continues until all the sequences have been
used. Perhaps the most widely used algorithms of this type is CLUSTALW
[THG]. Despite being heuristic, this method uses evolutionary relationships
among the sequences of interest. Speciﬁcally, it produces a phylogenetic tree
(see Sect. 5.1) by the neighbor-joining method (see Sect. 5.3) with pairwise
“distances” calculated from the Kimura model (see Sects. 5.4 and 5.5).
Multiple alignments obtained by methods of this kind are almost always
adjusted by the eye. Below we describe one simple progressive alignment al-
gorithm. For more information on progressive alignment methods the reader
is referred to [DEKM].
Star Alignment
The Star Alignment algorithm is a fast heuristic method for producing multi-
ple alignments. Of course, as any other heuristic algorithm, it does not guaran-
tee to ﬁnd an optimal alignment. The general idea is to ﬁrst select a sequence
which is most similar to all the other sequences, and then to use it as the
center of a “star” aligning all the other sequences to it. We will describe this
algorithm by means of considering the following example.
Example 2.5. Suppose we are given the following ﬁve DNA sequences
x1 : ATTGCCATT
x2 : ATGGCCATT
x3 : ATCCAATTTT
x4 : ATCTTCTT
x5 : ACTGACC.

2.7 Multiple Alignment
21
We assume the same scoring scheme for pairwise alignments as in Example
2.1 and consider the corresponding SP-scoring scheme with linear gap model,
calculate all pairwise optimal scores (that is, the scores found by the global
pairwise alignment algorithm described in Sect. 2.2), write them in the matrix
below and ﬁnd the sum in each row
x1 x2 x3 x4 x5
Total Score
x1
7 −2
0 −3
2
x2
7
−2
0 −4
1
x3 −2 −2
0 −7
−11
x4
0
0
0
−3
−3
x5 −3 −4 −7 −3
−17
Of all the sequences, x1 has the best total score (equal to 2), and is selected to
be at the center of the future star. Optimal alignments between x1 and each
of the other sequences found by the global alignment algorithm from Sect. 2.2
are as follows
x1 : A T T G C C A T T
x2 : A T G G C C A T T,
x1 : A T T G C C A T T −−
x3 : A T C −C A A T T T T,
x1 : A T T G C C A T T
x4 : A T C T T C −T T,
x1 : A T T G C C A T T
x5 : A C T G A C C −−.
We now merge the above alignments using the “once a gap – always a
gap” principle. We start with x1 and x2
x1 : A T T G C C A T T
x2 : A T G G C C A T T
and add x3, but, since x3 is longer than x1 and x2, we add gaps at the ends
of x1 and x2
x1 : A T T G C C A T T −−
x2 : A T G G C C A T T −−
x3 : A T C −C A A T T T T.
These gaps are never removed. If we introduce a gap somewhere, it will be
carried through.
Finally, we add x4 and x5. They too must have gaps added to their ends.
The resulting multiple alignment is as follows

22
2 Sequence Alignment
x1 : A T T G C C A T T −−
x2 : A T G G C C A T T −−
x3 : A T C −C A A T T T T
x4 : A T C T T C −T T −−
x5 : A C T G A C C −−−−.
The Star Alignment algorithm may not ﬁnd an optimal multiple alignment
as shown in the example below.
Example 2.6. Consider the following three sequences
x1 : GGCAA
x2 : GCACA
x3 : GGACA,
and assume the same scoring scheme as in Example 2.5. Here x1 is again the
center of the star, and the optimal alignments between x1 and each of the
other two sequences are
x1 : G G C A −A
x2 : G −C A C A,
x1 : G G −C A A
x3 : G G A C A −.
Merging these alignments, we obtain
x1 : G G −C A −A
x2 : G −−C A C A
x3 : G G A C A −−.
The above alignment is not optimal, its score is equal to -5. The best score is
in fact equal to 0. One optimal alignment is shown below.
x1 : G G C A −A
x2 : G −C A C A
x3 : G G −A C A.
Machine Learning Approach
Another way to produce pairwise and multiple alignments is by utilizing the
machine learning approach. This means that one builds a kind of a “learning
machine” that can be trained from initially unaligned sequences. After that
the trained machine can attempt to align the sequences from the training
data.
There are many types of learning machines, and, apart from sequence
alignment, they have other numerous applications in bioinformatics and other
areas. In the following chapter we will deal with one type of learning machines,
Hidden Markov Models or HMMs, and discuss some of their applications in
bioinformatics, including multiple sequence alignment (see Sect. 3.9).

Exercises
23
Exercises
2.1. Consider the following scoring scheme for DNA sequences: s(a, a) = 1
(the score of any match), s(a, b) = 1 if a ̸= b (the score of any mismatch),
s(−, a) = s(a, −) = −2 (the gap penalty), for all a, b ∈Q = {A, C, G, T}. Let
x : ACTGTCCA
y : CTGAATCAGA.
Find the score of the following alignment
x : A C T G −−−T −C C A
y : C −T G A A T −C A G A.
Is there a higher-scoring alignment between x and y?
2.2. Consider the following scoring scheme with linear gap model for DNA
sequences: the score of any match is equal to 5, the score of any mismatch is
equal to -4, the gap penalty is equal to -2. Using the relevant dynamic pro-
gramming algorithms with this scoring scheme ﬁnd
(i) all optimal global alignments of the sequences
x : AAGTTCGT
y : CAGTAAT,
(ii) all optimal local alignments of the sequences
x : AGTGGCATT
y : TGTCGCAT.
2.3. Consider the following scoring scheme with aﬃne gap model for DNA
sequences: the score of any match is equal to 2, the score of any mismatch
is equal to -3, the gap opening penalty is -4, the gap extension penalty is -1.
Using the relevant dynamic programming algorithm with this scoring scheme
ﬁnd all optimal global alignments of the following sequences
x : TGGCAAC
y : CTGGA.
2.4. Consider the following scoring scheme with linear gap model for DNA
sequences: the score of any match is equal to 1, the score of any mismatch
is equal to -2, the gap penalty is equal to -1. Suppose that we are using the
FASTA algorithm to look for optimal local alignment between the following
sequences
x : CGACTCCGAT
y : CGACTAAAACCGAT.
Set k = 3 and b = 1. Will the algorithm ﬁnd an optimal alignment? What
will change if we set k = 4 and b = 2?

24
2 Sequence Alignment
2.5. For an alignment M of three sequences deﬁne the score S(M) by formula
(2.4). The score s(Mi) of the column Mi is deﬁned as follows: if the column
contains three identical symbols, set s(Mi) = 3; if it contains two identical
symbols, but no gaps, set s(Mi) = 2; if it contains three distinct symbols,
but no gaps, set s(Mi) = 1; if it contains exactly one gap, set s(Mi) =
−1; if it contains two gaps, set s(Mi) = −2. Applying the relevant dynamic
programming algorithm ﬁnd all optimal alignments of the three sequences
x = CAGC
y = CTG
z = TAC.
2.6. Using the Star Alignment algorithm with the scoring scheme for pairwise
alignments from Exercise 2.1, construct a multiple alignment for the following
sequences
x1 : AGTCCT
x2 : ACTGTTC
x3 : CCGCGTT
x4 : GGGTCCT.
Using the SP-score of this alignment ﬁnd the lower bounds tkl for MSA for
the sequences x1, x2, x3, x4, as deﬁned in (2.5).

3
Markov Chains and Hidden Markov Models
3.1 Markov Chains
We introduce discrete-time ﬁnite Markov chains or discrete-time ﬁnite Markov
models (or simply Markov chains or Markov models) as follows. Consider
some ﬁnite set X of all possible states G1, . . . , GN. At each of the time points
t = 1, 2, 3, . . . a Markov chain occupies one of these states. In each time step
t to t + 1 the process either stays in the same state or moves to some other
state in X. It does so in a probabilistic way, rather than in a deterministic
way, that is, if at time t the process is in state Gi, then at time t + 1 the
process moves to any possible state Gj with a certain probability. This prob-
ability is assumed to depend only on i and j, not on t, or the states that
the process occupied before state Gi. The probabilities are then denoted by
pGi Gj, or simply pij, to indicate their dependence on i and j only. The term
“probability” here is understood as follows: if we let the Markov chain run
freely, then, for every pair i, j, the proportion of observed transitions from Gi
to Gj among all observed transitions from Gi tends to pij, as the number of
model runs tends to inﬁnity (this will be explained in more detail below). A
rigorous introduction to the probability theory and statistics (in particular, a
rigorous treatment of Markov chains) will be given in Chaps. 6 and 8.
The probabilities pij, i, j = 1, . . . N, are called the transition probabilities
of the Markov chain and are arranged in the matrix
P =
⎛
⎜
⎜
⎜
⎝
p11 p12 . . . p1N
p21 p22 . . . p2N
...
...
...
...
pN1 pN2 . . . pNN
⎞
⎟
⎟
⎟
⎠,
called the matrix of transition probabilities. Any row of the matrix corresponds
to the state from which transitions are made, and any column to the state to
which transitions are made. Thus the probabilities in any particular row in
the matrix P sum up to 1.

26
3 Markov Chains and Hidden Markov Models
A Markov chain is often represented graphically as a collection of circles
(states), from which arrows representing transitions come out, as in the ex-
ample shown in Fig. 3.1. To each arrow a label (the corresponding transition
probability) is often assigned. If for some i and j we have pij = 0, then in-
stead of drawing an arrow going from state Gi to state Gj with label 0, we do
not draw an arrow at all. Hence the notion of the connectivity of a Markov
chain arises: it is the graph made of the states connected with arrows, where
the corresponding labels are ignored. Markov chains for which all the transi-
tion probabilities are non-zero are called fully connected. A ﬁxed connectivity
deﬁnes a family of Markov chains parametrized by the transition probabili-
ties corresponding to the arrows present in the graph. In some applications
we will be required to choose particular values of the transition probabili-
ties in accordance with given biological data. In this case we say that the
transition probabilities are estimated from the data. In such situations the
connectivity of the chain will be ﬁxed in advance, and all transition probabil-
ities corresponding to the arrows present in the graph will be allowed to take
all possible values (modulo an additional constraint that will be introduced
later). As a result of the estimation process, some of these probabilities may
be set to 0, and thus the connectivity of the resulting chain may be more
constrained than the originally assumed connectivity (that we will call the
a priori connectivity). In such cases we will sometimes draw all the a priori
allowed arrows, even if some of the corresponding transition probabilities are
eventually set to 0.
One can think of a Markov chain as a process of generating all possible
sequences of any ﬁxed ﬁnite length L ≥2, that is, sequences of the form
x1x2 . . . xL, where xj ∈X (in fact, the chain generates all possible inﬁnite
sequences, but we will restrict our attention only to their ﬁrst L elements).
In order to initiate the process, we need to ﬁx in advance the probabilities
P(Gj) for all j = 1, . . . , N (called the initialization probabilities), that is, N
non-negative numbers that sum up to 1. We will arrange the initialization
probabilities in a vector of length N and call it the vector of initialization
probabilities. For each j, P(Gj) is the probability of the sequence generation
process starting at state Gj. Choosing these values means that we impose
the following condition on the process of sequence generation: as the number
of runs of the process goes to inﬁnity, the proportion of sequences that start
with Gj among all generated sequences tends to P(Gj) for all j = 1, . . . , N.
Next, if we generate a collection of sequences of length L and for every pair
i, j determine the ratio of the number of times for which Gi in the generated
sequences is immediately followed by Gj and the number of times for which
Gi is immediately followed by any element from the set X, the resulting ratio
is required to tend to pij, as the number of model runs tends to inﬁnity. This
statement is the exact meaning of the phrase “the process moves from state
Gi to state Gj with probability pij”. Since we are given that the generation of
every element (apart from the ﬁrst one) of any sequence depends only on the
preceding element, it can be shown that the frequency of a particular sequence

3.1 Markov Chains
27
x = x1 . . . xL tends to
P(x) = P(x1)px1x2px2x3 × . . . × pxL−1xL,
(3.1)
as the number of runs increases. We will call the number in the right-hand
side of formula (3.1) the probability of the sequence x. It is easy to prove that
the sum of probabilities of all sequences of length L is equal to 1 (see Exercise
3.1). Thus, in accordance with the above discussion, we can deﬁne a Markov
chain as a pair consisting of a vector of initialization probabilities and a matrix
of transition probabilities.
The term “time” used above is appropriate if, for example, we consider
the evolution through time of a given nucleotide site and try to model it
by means of a Markov chain. For such purposes, however, continuous-time
Markov chains are more appropriate; they will be discussed in Sect. 5.4. The
role of time can be also played by other characteristics of the data in question.
As an example, consider a DNA sequence read from left to right. We can use
a Markov chain to model dependence of a nucleotide site on its neighbor on
the left, which can be useful if we want to model a speciﬁc type of sequence
composition.
Consider for instance protein-coding genes in prokaryotic organisms (for
example, in bacteria). Typically, such genes occupy as much as 90% of bac-
terial DNA. A prokaryotic gene consists of a coding region ﬂanked by a start
codon (usually ATG, but sometimes GTG or TTG) at the beginning (on the
left) and a stop codon (one of TAA, TAG, TGA) at the end (on the right).
The length of the region between the start and stop codons is divisible by
3, and each triplet of nucleotides (codon) codes for an amino acid. Stretches
of DNA that possess this property are called open reading frames or ORFs,
hence every gene is an ORF to which a start codon at the beginning and a
stop codon at the end are added. We note in passing that the structure of a
protein-coding gene in eukaryotes is more complicated, in particular, coding
regions (called exons) are interrupted by non-coding ones (called introns).
What sort of probabilistic model might we use for modeling prokaryotic
genes? As one possibility, we can consider a Markov chain for which the a
priori connectivity is shown in Fig. 3.1. Residues in prokaryotic genes are
not independent, but interact with each other and modeling such genes by
means of a Markov chain takes into account at least the dependence of each
residue on the residue that immediately precedes it. For a particular choice of
transition and initialization probabilities, if we let the chain run freely, it will
generate all possible sequences of letters from the DNA alphabet {A, C, G, T}.
If we want the composition of sequences with high probabilities to resemble
that of prokaryotic genes, the transition and initialization probabilities must
be selected in a certain way. In order to achieve this goal we can use real DNA
data.
Suppose that in a set of prokaryotic DNA sequences n genes were exper-
imentally identiﬁed. Then, it is natural to set the transition probabilities as
follows

28
3 Markov Chains and Hidden Markov Models
A
C
G
 T
Fig. 3.1.
pab =
Hab

c∈Q
Hac
,
(3.2)
where Hab is the number of times nucleotide b follows nucleotide a in the data
(if a particular nucleotide a0 does not occur in the data at all, then the data
is not suﬃcient for estimating pa0 b, and these transition probabilities can be
chosen arbitrarily). The initialization probabilities P(a) for each a ∈Q are
also calculated in the spirit of formula (3.2), that is, P(a) is the number of
times nucleotide a appears at the beginning of the sequences, divided by n. For
this Markov chain we hope that sequences with high probabilities resemble
those of real prokaryotic genes. We say that the resulting Markov chain is a
model for the n sequences that were used to set the parameters, or that the n
sequences are modeled by it. Often we will also say that these sequences are
the training data for the resulting Markov chain.
Note that for the Markov chain constructed in this way we did not con-
strain its connectivity in advance, that is, we did not require a priori that
any particular transition probabilities must be set to 0. The resulting Markov
chain may or may not be fully connected depending on the transition proba-
bilities obtained from the data.
Suppose now that we want to do a search with the Markov model obtained
above. This means that we want to use the model to determine whether or
not a given unannotated DNA sequence is the sequence of a prokaryotic gene.
Search for prokaryotic genes may seem easy at ﬁrst sight: indeed, since every
gene is an ORF, one can look for a start codon and the ﬁrst stop codon that
follows it, and hope that the stretch of DNA so obtained is a gene. However,
it may happen (and it does in fact happen in 50% of cases) that two start
codons are encountered without an intervening stop codon (see Exercise 3.4).
Which of the corresponding ORFs is then a gene? To answer this question one

3.1 Markov Chains
29
has to somehow assess the composition of the ORFs and to compare it with
that of known gene sequences. This can be attempted by using the simple
DNA model constructed above.
Let the unannotated DNA sequence be x = x1 . . . xL. We then treat x
as if it was generated by the Markov chain, that is, assess the probability
P(x) of x found from formula (3.1), which is often called the probability with
which x arises from the model. If P(x) is in some sense large (say, greater
than some preset threshold), then we say that there is a good chance that
x came from a prokaryotic gene. If, on the other hand, the probability P(x)
is small, we say that x probably has no relation to the training data, and
hence no relation to prokaryotic genes (if we believe that the training data
is suﬃciently representative). Of course, one should bear in mind that the
above model is very simple, and its search power is therefore limited. It can
produce low probabilities for real prokaryotic genes (false negatives) and high
probabilities for sequences unrelated to the training data (false positives).
The procedures described above can be applied to any Markov chain. In-
deed, suppose we wish to estimate the transition probabilities of a Markov
chain with state set X = {G1, . . . , GN} from some training data. The train-
ing data consists of a ﬁnite number of ﬁnite sequences of elements from X. We
assume that some a priori connectivity is ﬁxed, that is, some of the transition
probabilities are set to 0 from the start and that the training data agrees with
the connectivity, that is, if, for some i and j, the connectivity requires that
pij = 0, then the training sequences do not contain Gi followed immediately
by Gj anywhere. The same restriction applies to the initialization probabili-
ties: if it is a priori required that the chain starts at state Gj with probability
0, then none of the training sequences begins with Gj. If these conditions
are satisﬁed, we can estimate the transition probabilities of the Markov chain
from the training data by using formula (3.2), and the initialization proba-
bilities as the frequencies of the occurrences of Gj at the beginning of the
sequences for all j. The connectivity of the resulting Markov chain may be
more constrained than the a priori assumed connectivity (see Exercise 3.3).
One should also remember that if a certain state Gi does not occur in any
of the sequences, then the data is not suﬃcient for estimating pij, and these
transition probabilities can be set arbitrarily (we will encounter such a sit-
uation in a more general setting in Example 3.13). Once the transition and
initialization probabilities have been ﬁxed, search can be done in exactly the
same way as described for the DNA model above. The purpose of searches
of this type is to detect potential membership of an unannotated sequence in
the sequence family used as training data.
To use formula (3.1) we must separately specify the initialization proba-
bility P(x1). To avoid this inhomogeneity, it is possible to add an extra begin
state to the Markov chain; we denote it by B. Then for the initialization prob-
abilities we have P(Gj) = pB Gj (we will sometimes also use the notation pB j
for them), j = 1, . . . , N. In the future we will always consider Markov chains
with begin states. Similarly, unless explicitly stated otherwise, we will always

30
3 Markov Chains and Hidden Markov Models
add an end state E to ensure that the end of the sequence is modeled (see
Part 4 of Example 6.11 for an explanation), and we also assume that there
is no transition from B to E. The end state is an absorbing state, that is, a
state, for which the transition probability into itself is equal to 1. We, how-
ever, will ignore this transition and think of a Markov chain with an end state
as a process that generates either inﬁnite sequences B x1x2 . . ., or sequences
of ﬁnite length of the form B x1x2 . . . xL E, where xj ∈X (we will often omit
B at the beginning and E at the end of generated sequences). We will only
be interested in ﬁnite sequences, but we no longer ﬁx their length and con-
sider sequences of all possible lengths. For such Markov chains formula (3.1)
changes to
P(x) = pB x1px1x2px2x3 × . . . × pxL−1xLpxL E,
(3.3)
where pxL E denotes the probability of transition from xL to E. The proba-
bilities pGj E, j = 1, . . . , N, are called the termination probabilities (we will
sometimes also use the notation pj E for them). In the future we will apply
the term “transition probabilities” to all of pB j, pij and pj E. We note that a
Markov chain with an end state may have other absorbing states, hence the
end state is just a distinguished absorbing state for which the transition into
itself is ignored.
Note that no arrows go into B and out of E. Therefore, to simplify notation
we will often denote B and E by 0 and, instead of writing pB x1 and pxL E we
will frequently write p0 x1 and pxL 0 respectively (to simplify notation even
further we will also write p0 Gj = p0j and pGj 0 = pj0 for j = 1, . . . , N). Then
formula (3.3) takes the form
P(x) = p0 x1px1x2px2x3 × . . . × pxL−1xLpxL 0.
(3.4)
In the future we will only consider Markov models whose connectivity
satisﬁes the following assumption: for every state Gj for which there exists
a path from B to Gj, there also exists a path from Gj to E. We will call
such models non-trivially connected. Not every Markov chain is non-trivially
connected. Note that if the transition probabilities are obtained from training
data by means of formula (3.2) naturally extended to transitions from B and
into E, the resulting model is non-trivially connected (whereas the a priori
assumed connectivity may not be that of a non-trivially connected model, as
in Exercise 3.3). Of course, for a Markov chain with an end state none of
the training sequences is allowed to end with Gj, if the a priori connectivity
requires that the termination probability pj0 is equal to 0; this condition is part
of the requirement that the training data agrees with the a priori connectivity
for such a chain. It can be shown that for non-trivially connected models the
sum of probabilities P(x) over all sequences x of all ﬁnite lengths is equal to
1 (see Part 5 of Example 6.11 and Exercise 3.2). For more discussion on the
justiﬁcation of the deﬁnition of probabilities for a Markov chain with an end
state also see Part 5 of Example 6.11.
Figure 3.2 shows the connectivity from Fig. 3.1 modiﬁed by adding the
begin and end states.

3.1 Markov Chains
31
Fig. 3.2.
We will now give an example of parameter estimation for this connectivity
taken as an a priori connectivity.
Example 3.1. Let n = 4 and suppose that we are given the following sequences
that are known to be the sequences of some prokaryotic genes
x1 : ATGCTATTGATTTAA
x2 : GTGAAAGACTTCTAA
x3 : ATGCCCGATGAACGCTAG
x4 : ATGAAGCATGATTAA.
(3.5)
Then, ignoring the start and stop codons (shown in bold), that is, taking into
account only the corresponding ORFs, from formula (3.2) we obtain
p0A = 1
2,
p0C = 1
2,
p0G = 0,
p0T = 0,
pAA = 4
13, pAC = 2
13, pAG = 2
13, pAT = 5
13, pA0 = 0,
pCA = 1
9, pCC = 2
9, pCG = 2
9, pCT = 2
9, pC0 = 2
9,
pGA = 5
7, pGC = 2
7, pGG = 0,
pGT = 0,
pG0 = 0,
pT A = 1
10, pT C = 1
10, pT G = 3
10, pT T = 3
10, pT 0 = 1
5.
Hence, the resulting model is not fully connected and in fact has the con-
nectivity shown in Fig. 3.3. Note, in particular, that this model is indeed
non-trivially connected.

32
3 Markov Chains and Hidden Markov Models
Fig. 3.3.
For an example of searching with this model see Exercise 3.4.
Software package called Glimmer is a primary microbial gene ﬁnder. It
is based on the ideas described above. However, instead of Markov chains
discussed in this section it uses higher order Markov chains that model de-
pendence of every nucleotide site on a number of preceding sites, and their
generalizations (see [DS]).
In Example 3.1 the a priori connectivity did not have any constraints. We
will now give an example of parameter estimation with a priori constraints.
Example 3.2. Consider the a priori connectivity shown in Fig. 3.4. Suppose
Fig. 3.4.
that the training data consists of the following three sequences

3.2 Hidden Markov Models
33
x1 : G1G1G1G1
x2 : G1G1G2
x3 : G1G2.
We see that the training data agrees with the connectivity, and from formula
(3.2) obtain
p01 = 1, p02 = 0,
p11 = 4
7, p12 = 2
7, p10 = 1
7.
It is also obvious that p22 = 0, p20 = 1 (this is clear from the given connectiv-
ity, but also follows from the estimation procedure).
Observe that the connectivity of the resulting Markov chain is more con-
strained than the original connectivity, namely, it has no arrow from B to
G2.
It will be explained in Chap. 8 (see Part 5 of Example 8.12 and Exercise
8.4) that formula (3.2) does not just seem intuitively right, but gives values of
the transition probabilities that guarantee that the likelihood of the training
data P(x1) × . . . × P(xn) is maximal possible. As we will be see in Chap. 8,
the likelihood can be thought of as the joint probability of the occurrence of
the sequences x1, . . . , xn in n “independent runs” of the model. Therefore, it
is often said that with this estimation procedure the sequences in the training
data are assumed to be independent.
3.2 Hidden Markov Models
The approach to look for new prokaryotic genes described in the previous
section required extracting all ORFs from the DNA sequence in question
and analyzing each ORF separately. It would be useful, however, if we could
design an algorithm that could analyze unannotated DNA sequences directly,
without making the preprocessing step of extracting all possible ORFs. The
need for such an algorithm becomes even more clear, if instead of the problem
of searching for prokaryotic genes we consider the problem of searching for
some other DNA features that do not have such well-deﬁned boundaries as
genes (start and stop codons). One example of a search of this kind is the
determination of CpG islands discussed in [DEKM].
To construct such an algorithm, we have to model both the sequence com-
position of genes and that of intergenic regions. One possibility that can be
considered is to use a model with connectivity shown in Fig. 3.1 for genes,
another model with the same connectivity for intergenic regions, allow all
possible transitions between the states of the two models, and add the begin
and end states. Suppose we have somehow set the transition probabilities in
a reasonable way. We will call the resulting model the two-block model. Let

34
3 Markov Chains and Hidden Markov Models
us denote the states of the ﬁrst model by Ag, Cg, Gg, Tg and the states of the
second model by Aig, Cig, Gig, Tig. The complication with this new model is
that there is no longer a one-to-one correspondence between the letters of the
DNA alphabet {A, C, G, T} and the non-zero states of the model. Therefore,
if we are given an unannotated DNA sequence, say x0 = ACCTG, and we
wish to treat x0 as a sequence generated by the model, it is not clear how one
should calculate the probability with which x0 arises from it. Indeed, x0 could
be produced, for example, by the sequence of states AgCigCgTgGig, in which
case according to formula (3.4) the probability of x0 should be computed as
p0AgpAgCigpCigCgpCgTgpTgGigpGig0.
(3.6)
Alternatively, x0 could be produced in several other ways, for example, by the
sequence of states AigCigCgTgGg, in which case the probability of x should be
computed as
p0AigpAigCigpCigCgpCgTgpTgGgpGg0.
(3.7)
This indeterminacy arises because we do not a priori know the way x0 was
generated by the model, that is, the corresponding sequence of states. In this
situation it is common to say that the state sequence is hidden. The two-block
DNA model described above is an example of a hidden Markov model or an
HMM.
We will now give a general deﬁnition of HMM. An HMM is an ordinary
discrete-time ﬁnite Markov chain (that we always assume to be non-trivially
connected) with states G1, . . . , GN, transition probabilities p0j, pij, pj0, i, j =
1, . . . , N, that, in addition, at each state emits symbols from an alphabet Q
(the DNA alphabet in our examples). For each state Gk and each symbol a ∈
Q an emission probability qGk(a) = qk(a) is speciﬁed. For every k = 1, . . . , N,
the probabilities qk(a) sum up to 1 over all a ∈Q. The Markov chain will be
referred to as the underlying Markov chain of the HMM. The two-block DNA
model can be thought of as an HMM where at each state one letter (the letter
used in the name of the state) is emitted with probability 1 and the rest of
the letters are emitted with probability 0.
One can think of an HMM as a process of sequence generation. There are
two possible interpretations of this process, each interpretation being suitable
for particular types of problems. One way is to think of an HMM as a process
that generates pairs of sequences (x, π), where x is a sequence of letters from
Q and π is a sequence of non-zero states of the Markov chain (we will generally
call sequences of states paths through the underlying Markov chain or simply
paths). The lengths of x and π are equal and can be either ﬁnite or inﬁnite.
We will only be interested in ﬁnite sequences and write x and π respectively
as x1x2 . . . xL and π = B π1π2 . . . πL E = 0 π1π2 . . . πL 0 = π1π2 . . . πL, where
xj is the element of Q emitted at the state πj, for j = 1, . . . , L. As the number
of runs of such a model increases, the frequency of an element a ∈Q emitted
at state Gk among all pairs of sequences whose length does not exceed any
given number, tends to qk(a). This is the exact meaning of the phrase “the

3.2 Hidden Markov Models
35
state Gk emits the symbol a with probability qk(a)”. This interpretation of
an HMM is useful, when one applies it to analyzing biological sequences for
which paths through the underlying Markov chain can be assumed known (for
example, when the two-block DNA model is used to model DNA sequences
where prokaryotic genes have been determined experimentally). Another way
is to think of an HMM as a process that generates sequences of letters from
Q, that is, in this interpretation we ignore paths along which sequences are
generated. This interpretation of an HMM is useful, when one applies it to
analyzing biological sequences for which paths through the underlying Markov
chain are unknown, as in the example of the sequence x0 above. A rigorous
treatment of HMMs incorporating both approaches will be given in Chaps. 6
and 8.
As in the case of Markov chains, any a priori connectivity deﬁnes a fam-
ily of HMMs parametrized by the transition probabilities corresponding to
the arrows present in the connectivity graph (subject to constraints dictated
by the condition of non-trivial connectedness) and the emission probabilities.
Analogously to Markov chains, HMMs are often derived as models for partic-
ular training data, from which the transition and emission probabilities can
be estimated. This process of parameter estimation is more complicated than
that for Markov chains (based on formula (3.2)) and will be discussed in Sect.
3.6. We emphasize that the underlying Markov chain obtained as a result of
the estimation procedures described in Sect. 3.6 is always non-trivially con-
nected, even if the a priori assumed connectivity is not that of a non-trivially
connected Markov chain.
Let x = x1 . . . xL be a sequence of letters from Q and π = π1 . . . πL be
a path of the same length. We will now deﬁne the probability P(x, π) of the
pair (x, π) as follows
P(x, π) = p0 π1qπ1(x1)pπ1π2qπ2(x2) × . . . × pπL−1πLqπL(xL)pπL 0.
(3.8)
It is natural to think of this probability as the probability of the sequence x
being generated along the path π. The deﬁnition is reasonable since it can be
shown that the frequency of x being generated by the model along π among
all truncations of pairs of sequences at length greater than L tends to the
number in the right-hand side of formula (3.8) – see Parts 5 and 6 of Example
6.11 for a precise statement. Formula (3.8) is the HMM analogue of formula
(3.4), and (3.6), (3.7) are in fact calculated for the two-block DNA model
in accordance with (3.8) and equal respectively to P(x0, π1) and P(x0, π2)
with π1 = AgCigCgTgGig and π2 = AigCigCgTgGg. It can be shown that for
a general HMM we have 
(x,π) P(x, π) = 1, where the summation is taken
over all pairs of sequences of all ﬁnite lengths (see Exercise 3.5).
However, formula (3.8) is not so useful in practice because in general for
a biological sequence we do not know the path. In the next section we will
describe an algorithm that for a given sequence x ﬁnds all the most probable
paths, that is, paths that maximize P(x, π) over all paths π of length equal to

36
3 Markov Chains and Hidden Markov Models
the length of x. One of the most probable paths is often regarded as the path
along which x is generated by the model.
Further, for the purposes of analyzing sequences for which paths are not
known, it is natural to deﬁne “the total probability” P(x) of x as
P(x) =

all π of length L
P(x, π).
(3.9)
It follows from Exercise 3.5 that 
x P(x) = 1, where the summation is taken
over all sequences of all ﬁnite lengths. In Sects. 3.4 and 3.5 we will describe
algorithms that for a given sequence x calculate P(x).
HMMs can be used for searching in a way similar to that for Markov mod-
els, with the purpose of detecting potential membership of an unannotated
sequence in a sequence family used as training data (see Exercise 3.6). For
example, suppose we are given an unannotated DNA sequence x = x1 . . . xL,
and we wish to determine whether or not the sequence x has something in
common with the training DNA data used to estimate the parameters of the
two-block DNA model. One way to do it is by assessing the probability P(x)
of x found from formula (3.9), which is often called the probability with which
x arises from the model. Alternatively, one can assess P(x, π∗), the probability
with which x arises from the model along the path π∗, calculated from formula
(3.8) for one of the most probable paths π∗. If either of these quantities is in
some sense large (say, greater than some preset threshold), then we say that
there is a good chance that x is related to the training data. If, on the other
hand, the probability P(x) is small, we say that x probably has no relation
to the training data. Often instead of P(x) and P(x, π∗) diﬀerent (but re-
lated) probabilistic quantities are used for assessing unannotated sequences.
Of course, one must bear in mind that searches of this kind may lead to false
negatives and false positives.
Another kind of search that one can perform using HMMs is, for a query
sequence x, determine a likely path through the Markov model. For example,
if we search with the two-block DNA model, we are interested in knowing
which elements of x are likely to have come from the states labeled using
the subscript “g” since continuous path stretches consisting of such states
may correspond to prokaryotic genes. Recovering likely paths for sequences is
called decoding and will be discussed in Sects. 3.3 and 3.5. Of course, searches
of this kind can lead to errors as well.
The two-block DNA model has a very large number of parameters, and one
needs a very large training dataset to reasonably estimate all of them. We will
now give a more appropriate example of an HMM for modeling prokaryotic
DNA. Consider the a priori connectivity shown in Fig. 3.5.

3.3 The Viterbi Algorithm
37
Fig. 3.5.
The state “g” is associated with genes and the state “ig” with intergenic
regions. Each of the states is allowed to emit letters from the DNA alphabet
{A, C, G, T} with certain probabilities (note, however, that a more realistic
model would single out the start codons on the way from “ig” to “g” and stop
codons on the way from “g” to “ig” ). In Sect. 3.6 we will give an example of
parameter estimation for this model.
While search for prokaryotic genes can in fact be done avoiding HMMs (see
the previous section), generalizations of the HMM arising from the connectiv-
ity shown in Fig. 3.5 are useful for ﬁnding genes in eukaryotes. The structure
of eukaryotic genes is much more complicated than that of prokaryotic ones,
and a separate state is required to model every particular gene feature, as well
as the intergenic region. Also, at each state not single letters, but sequences
of letters are emitted. Such a model is implemented in GENSCAN which is
a popular successful gene ﬁnder for human DNA. Details on GENSCAN can
be found in [EG] and the original paper [BK].
3.3 The Viterbi Algorithm
In the previous section we mentioned a particular kind of search performed
with HMMs called decoding. With a search of this type, if x is a sequence of
letters from the alphabet Q, one wishes to know a likely path for x through
the underlying Markov chain. Of course, we ﬁrst have to specify what “likely”
means. One reasonable approach is to attempt to ﬁnd a path π∗that maxi-
mizes the probability P(x, π) over all paths π with length equal to the length
of x (another approach will be discussed in Sect. 3.5). Such a most probable
path may not be unique. One can try to ﬁnd all the most probable paths
directly by listing all possible paths and calculating P(x, π) for each of them.
However, the number of all possible paths increases exponentially with the
length of the sequence, and therefore such a strategy is not practical. There is

38
3 Markov Chains and Hidden Markov Models
a much faster dynamic programming algorithm for determining all the most
probable paths, called the Viterbi algorithm. Accordingly, the most probable
paths are often called the Viterbi paths.
Suppose we are given an HMM whose underlying Markov chain has a
state set X = {G1, . . . , GN}, end state and transition probabilities p0j, pij,
pj0, i, j = 1, . . . , N. Fix a sequence x = x1 . . . xL of letters from the alphabet
Q and deﬁne
vk(1) = p0kqk(x1)
(3.10)
for k = 1, . . . , N, and
vk(i) =
max
π1,...,πi−1∈X p0 π1qπ1(x1)pπ1 π2qπ2(x2) × . . .
×pπi−2 πi−1qπi−1(xi−1)pπi−1 Gkqk(xi),
for i = 2, . . . , L and k = 1, . . . , N. Clearly, we have
vk(i + 1) = qk(xi+1)
max
l=1,...,N(vl(i)plk)
(3.11)
for all i = 1, . . . , L −1 and k = 1, . . . , N. Therefore, vk(i) for i = 2, . . . , L and
k = 1, . . . , N can be calculated from initial conditions (3.10) and recursion
relations (3.11). At each recursion step we form a set Vk(i) that consists of
all integers m for which vm(i)pmk = maxl=1,...,N(vl(i)plk), i = 1, . . . , L −1,
k = 1, . . . , N.
Further, we have
max
all π of length L P(x, π) =
max
l=1,...,N(vl(L)pl0),
and deﬁne V(L) to be the set that consists of all integers m for which
vm(L)pm0 = maxl=1,...,N(vl(L)pl0). Every Viterbi path can be recovered
by the following traceback procedure: choose mL ∈V(L), then choose
mL−1 ∈VmL(L −1), then choose mL−2 ∈VmL−1(L −2), and proceed until
we choose m1 ∈Vm2(1). For the resulting path π∗= 0 Gm1Gm2 . . . GmL 0 =
Gm1Gm2 . . . GmL we have
P(x, π∗) =
max
all π of length L P(x, π).
The Viterbi algorithm is implemented in GENSCAN, where decoding cor-
responds to human gene ﬁnding.
We will now give an example of applying the Viterbi algorithm.
Example 3.3. Consider the HMM for which Q is the two-letter alphabet
{A, B}, whose underlying Markov chain is shown in Fig. 3.6 and whose emis-
sion probabilities are as follows
q1(A) = 0.5, q1(B) = 0.5,
q2(A) = 0.1, q2(B) = 0.9,
q3(A) = 0.9, q3(B) = 0.1.

3.3 The Viterbi Algorithm
39
Fig. 3.6.
We let x = BAB and ﬁnd the (unique) Viterbi path π∗for it. We have
v1(1) = 0.5 × 0.2 = 0.1,
v2(1) = 0.9 × 0.3 = 0.27,
v3(1) = 0.1 × 0.5 = 0.05,
v1(2) = 0.5 × 0.1 × 0.3 = 0.015,
V1(1) = {1},
v2(2) = 0.1 × 0.27 × 0.4 = 0.0108,
V2(1) = {2},
v3(2) = 0.9 × 0.27 × 0.4 = 0.0972,
V3(1) = {2},
v1(3) = 0.5 × 0.015 × 0.3 = 0.00225,
V1(2) = {1},
v2(3) = 0.9 × 0.0972 × 0.3 = 0.026244,
V2(2) = {3},
v3(3) = 0.1 × 0.0972 × 0.3 = 0.002916,
V3(2) = {3}.
This gives
max
all π of length 3 P(x, π) = 0.026244 × 0.2 = 0.0052488,
and
V(3) = {2}.

40
3 Markov Chains and Hidden Markov Models
Tracing back we obtain m3 = 2, m2 = 3, m1 = 2, that is, the only Viterbi
path for x is
π∗= G2G3G2.
3.4 The Forward Algorithm
In this section we will describe an algorithm for ﬁnding the total probability
P(x) (deﬁned in (3.9)) of a sequence x = x1 . . . xL of letters from the alphabet
Q. The straightforward approach to calculate P(x) by enumerating all possible
paths π of length L is not practical, as was explained in the preceding section.
In fact P(x) can be calculated by a faster algorithm similar to the Viterbi
algorithm. It is called the forward algorithm.
Deﬁne
fk(1) = p0kqk(x1)
(3.12)
for k = 1, . . . , N, and
fk(i) =

π1,...,πi−1∈X
p0 π1qπ1(x1)pπ1 π2qπ2(x2) × . . .
×pπi−2 πi−1qπi−1(xi−1)pπi−1 Gkqk(xi),
for i = 2, . . . , L and k = 1, . . . , N. Clearly, we have
fk(i + 1) = qk(xi+1)
N

l=1
fl(i)plk
(3.13)
for all i = 1, . . . , L −1 and k = 1, . . . , N. Therefore, fk(i) for i = 2, . . . , L and
k = 1, . . . , N can be calculated from initial conditions (3.12) and recursion
relations (3.13). Further, we have
P(x) =
N

k=1
fk(L)pk0.
The name of this algorithm is related to the fact that it reads the sequence
x forward. In the next section we will see another algorithm for calculating
P(x) that reads the sequence backward.
We will now give an example of applying the forward algorithm.
Example 3.4. Consider the HMM from Example 3.3 and ﬁnd P(x), where
x = AAB. We have

3.5 The Backward Algorithm and Posterior Decoding
41
f1(1) = 0.5 × 0.2 = 0.1,
f2(1) = 0.1 × 0.3 = 0.03,
f3(1) = 0.9 × 0.5 = 0.45,
f1(2) = 0.5 × 0.1 × 0.3 = 0.015,
f2(2) = 0.1 × (0.1 × 0.3 + 0.03 × 0.4 + 0.45 × 0.3) = 0.0177,
f3(2) = 0.9 × (0.1 × 0.3 + 0.03 × 0.4 + 0.45 × 0.3) = 0.1593,
f1(3) = 0.5 × 0.015 × 0.3 = 0.00225,
f2(3) = 0.9 × (0.015 × 0.3 + 0.0177 × 0.4 + 0.1593 × 0.3) = 0.053433,
f3(3) = 0.1 × (0.015 × 0.3 + 0.0177 × 0.4 + 0.1593 × 0.3) = 0.005937,
which gives
P(x) = 0.00225 × 0.1 + 0.053433 × 0.2 + 0.005937 × 0.4 = 0.0132864.
3.5 The Backward Algorithm and Posterior Decoding
In this section we will describe another algorithm for calculating P(x), the
backward algorithm. The name of this algorithm is related to the fact that,
unlike the forward algorithm, it reads the sequence x backward. However, as
we will explain later in the section, this algorithm also has other uses. In
particular, there is a decoding procedure (alternative to the one based on the
Viterbi algorithm) that requires quantities calculated by both the forward and
backward algorithms.
For a ﬁxed sequence x = x1 . . . xL of letters from the alphabet Q deﬁne
bk(i) =

πi+1,...,πL∈X
pGk πi+1qπi+1(xi+1)pπi+1 πi+2qπi+2(xi+2) × . . .
×pπL−1 πLqπL(xL)pπL 0,
for i = 1, . . . , L −1, k = 1, . . . , N and
bk(L) = pk0,
(3.14)
for k = 1, . . . , N. Clearly, we have
bk(i) =
N

l=1
pklql(xi+1)bl(i + 1)
(3.15)
for i = 1, . . . , L −1, k = 1, . . . , N. Therefore bk(i) for i = 1, . . . , L −1, k =
1, . . . , N can be found from initial conditions (3.14) and recursion relations
(3.15). Further, we have
P(x) =
N

k=1
p0kqk(x1)bk(1).
We will now give an example of applying the backward algorithm.

42
3 Markov Chains and Hidden Markov Models
Example 3.5. Consider the HMM and the sequence x = AAB from Example
3.4 and ﬁnd P(x) using the backward algorithm. We have
b1(3) = 0.1,
b2(3) = 0.2,
b3(3) = 0.4,
b1(2) = 0.3 × 0.5 × 0.1 + 0.3 × 0.9 × 0.2 + 0.3 × 0.1 × 0.4 = 0.081,
b2(2) = 0.4 × 0.9 × 0.2 + 0.4 × 0.1 × 0.4 = 0.088,
b3(2) = 0.3 × 0.9 × 0.2 + 0.3 × 0.1 × 0.4 = 0.066,
b1(1) = 0.3 × 0.5 × 0.081 + 0.3 × 0.1 × 0.088 + 0.3 × 0.9 × 0.066 = 0.03261,
b2(1) = 0.4 × 0.1 × 0.088 + 0.4 × 0.9 × 0.066 = 0.02728,
b3(1) = 0.3 × 0.1 × 0.088 + 0.3 × 0.9 × 0.066 = 0.02046.
This gives
P(x) = 0.2×0.5×0.03261+0.3×0.1×0.02728+0.5×0.9×0.02046 = 0.0132864,
which agrees with the result in Example 3.4.
We will now introduce some concepts from the probability theory. We will
only do it for the special case associated with HMMs. A general treatment of
these concepts is postponed until Chap. 6.
Suppose we are given an HMM for which we assume, as usual, that the
underlying Markov chain is non-trivially connected. Deﬁne the sample space
as follows
S =

(y, π) : y is a sequence of letters from Q of ﬁnite length and
π is a path of the same length through the underlying Markov chain

.
The sample space is the collection of all outcomes of the HMM, if we think
of it as a process that generates pairs of ﬁnite sequences as was explained in
Sect. 3.2. Subsets of S are called events. We say that an event E occurs at a
particular time, if the element of the sample space generated by the model at
that time belongs to E. For an arbitrary event E we deﬁne the probability of
the occurrence of E or simply the probability of E as follows
P(E) =

(y,π)∈E
P(y, π).
Next, for two events E1 and E2 with P(E2) > 0 deﬁne the conditional proba-
bility P(E1|E2) as follows
P(E1|E2) = P(E1 ∩E2)
P(E2)
.
The conditional probability P(E1|E2) is often called the probability of the
event E1 given the event E2 and has the meaning of the probability of the
event E1 calculated with the prior knowledge that the event E2 occurs.

3.5 The Backward Algorithm and Posterior Decoding
43
For a ﬁxed sequence x = x1 . . . xL consider the event
E(x) = {(y, π) ∈S : y = x}.
Clearly, P(E(x)) = P(x), where P(x) was deﬁned in (3.9). Next, ﬁx two
integers 1 ≤i ≤L and 1 ≤k ≤N, and consider the event
Ei,k = {(y, π) ∈S : the length of y and π is ≥i, and πi = Gk}.
We suppose that P(x) > 0 (otherwise x is not an interesting sequence) and
calculate the conditional probability P(Ei,k|E(x)). Assuming that 2 ≤i ≤
L −1 we obtain
P(Ei,k|E(x)) = P(Ei,k ∩E(x))
P(x)
=
1
P(x)

π1,...,πi−1,πi+1,...,πL∈X
p0 π1qπ1(x1)
×pπ1 π2qπ2(x2) . . . pπi−2 πi−1qπi−1(xi−1)pπi−1 Gkqk(xi)pGk πi+1qπi+1(xi+1)
×pπi+1 πi+2qπi+2(xi+2) . . . pπL−1 πLqπL(xL)pπL 0 =
1
P(x)

π1,...,πi−1∈X
p0 π1
×qπ1(x1)pπ1 π2qπ2(x2) . . . pπi−2 πi−1qπi−1(xi−1)pπi−1 Gkqk(xi)
×

πi+1,...,πL∈X
pGk πi+1qπi+1(xi+1)pπi+1 πi+2qπi+2(xi+2) . . . pπL−1 πL
×qπL(xL)pπL 0 = fk(i)bk(i)
P(x)
,
where fk(i) are the quantities deﬁned in the previous section and found from
the forward algorithm. Analogous calculations for i = 1 and i = L show that
the same result holds in these cases as well and thus we have
P(Ei,k|E(x)) = fk(i)bk(i)
P(x)
(3.16)
for i = 1, . . . , L and k = 1, . . . , N. Hence the probabilities P(Ei,k|E(x)) can be
eﬃciently calculated by applying both the forward and backward algorithms
at the same time (the probability P(x) can be found from either of them).
For every i = 1, . . . , L and k = 1, . . . , N the number P(Ei,k|E(x)) is the
probability with which the ith element xi of x is emitted at state Gk and
is also called the posterior probability of state Gk at observation i given x.
Posterior probabilities can be used for decoding in the following way. For
every i = 1, . . . , L consider the set B(i) that consists of all states Gm ∈X for

44
3 Markov Chains and Hidden Markov Models
which P(Ei,m|E(x)) = maxk=1,...,N P(Ei,k|E(x)). In other words, elements of
B(i) are the most probable states for xi, i = 1, . . . , L. The determination of
all such states for every xi is called posterior decoding.
For decoding it is also desirable to know complete paths for x rather than
the most probable states for every xi, and we can consider all paths ˆπ of
length L such that ˆπi ∈B(i) for i = 1, . . . , L. Note, however, that for such a
path ˆπ the probability P(x, ˆπ) may be very low (and, in fact, is often equal
to 0).
There are also other uses of posterior probabilities. Consider, for example,
the two-block model from Sect. 3.2. For a DNA sequence x we are interested in
determining which of the elements of x are likely to have been generated at one
of the states labeled using the subscript “g”. The posterior probability of these
states at observation i given x is equal to the sum of the posterior probabilities
for each of the states. If this probability is large (say, greater than some preset
threshold) for every element in a long continuous subsequence of x, then the
subsequence is accepted as part of a prokaryotic gene. This procedure of gene
ﬁnding is alternative to the one used in GENSCAN.
Another application of posterior probabilities is parameter estimation that
will be discussed in the next section.
We will now give an example of posterior decoding.
Example 3.6. Consider the HMM and the sequence x = AAB from Examples
3.4 and 3.5 and ﬁnd the posterior probabilities P(Ei,k|E(x)) for i, k = 1, 2, 3.
From the calculations in Examples 3.4 and 3.5 and formula (3.16) we obtain
P(E1,1|E(x)) = 0.2454389,
P(E1,2|E(x)) = 0.06159682,
P(E1,3|E(x)) = 0.6929642,
P(E2,1|E(x)) = 0.0914469,
P(E2,2|E(x)) = 0.1172327,
P(E2,3|E(x)) = 0.7913204,
P(E3,1|E(x)) = 0.01693461, P(E3,2|E(x)) = 0.8043262,
P(E3,3|E(x)) = 0.1787392,
and therefore posterior decoding is
B(1) = {G3},
B(2) = {G3},
B(3) = {G2}.
This decoding produces a single path
ˆπ = G3G3G2,
for which
P(x, ˆπ) = 0.5 × 0.9 × 0.3 × 0.9 × 0.3 × 0.9 × 0.2 = 0.006561.
Next, for each xi, i = 1, 2, 3, we will calculate the probability with which
it is generated by either G1 or G2. The result for x1 = A is P(E1,1|E(x)) +
P(E1,2|E(x)) = 0.3070357, for x2 = A is P(E2,1|E(x)) + P(E2,2|E(x)) =
0.2086796, and for x3 = B is P(E3,1|E(x)) + P(E3,2|E(x)) = 0.8212608.

3.6 Parameter Estimation for HMMs
45
For comparison, we will now ﬁnd the (unique) Viterbi path for x. We have
v1(1) = 0.5 × 0.2 = 0.1,
v2(1) = 0.1 × 0.3 = 0.03,
v3(1) = 0.9 × 0.5 = 0.45,
v1(2) = 0.5 × 0.1 × 0.3 = 0.015,
V1(1) = {1},
v2(2) = 0.1 × 0.45 × 0.3 = 0.0135,
V2(1) = {3},
v3(2) = 0.9 × 0.45 × 0.3 = 0.1215,
V3(1) = {3},
v1(3) = 0.5 × 0.015 × 0.3 = 0.00225,
V1(2) = {1},
v2(3) = 0.9 × 0.1215 × 0.3 = 0.032805,
V2(2) = {3},
v3(3) = 0.1 × 0.1215 × 0.3 = 0.003645,
V3(2) = {3}.
This gives
max
all π of length 3 P(x, π) = 0.032805 × 0.2 = 0.006561 = P(x, ˆπ),
and
V(3) = {2}.
Tracing back we obtain m3 = 2, m2 = 3, m1 = 3, and the only Viterbi path
for x is
π∗= G3G3G2 = ˆπ.
Thus in this example the posterior decoding procedure gives the same result
as the one based on the Viterbi algorithm.
3.6 Parameter Estimation for HMMs
As we have already mentioned in Sect. 3.2, a ﬁxed a priori Markov chain
connectivity deﬁnes a family of HMMs parametrized by the corresponding
transition and emission probabilities, where the transition probabilities are
required to satisfy the conditions arising from non-trivial connectedness. In
this section we will explain how for a particular training dataset one can choose
values of the parameters in a reasonable way. A training dataset consists
either of a number of pairs of sequences (x1, π1), . . . , (xn, πn), or a number of
sequences x1, . . . , xn, where xj is a ﬁnite sequence of letters from the alphabet
Q and πj is a path of the same length through the graph representing the a
priori connectivity, that starts at B and ends at E, j = 1, . . . , n. We will
attempt to select parameter values in such a way that, for datasets of the ﬁrst
type, P(x1, π1) × . . . × P(xn, πn) is maximal possible and, for datasets of the
second type, P(x1) × . . . × P(xn) is maximal possible. The resulting HMM is
said to model the training data; we also say that the training data is modeled
by the HMM.
The quantities P(x1, π1) × . . . × P(xn, πn) and P(x1) × . . . × P(xn) are
called the likelihoods of the training datasets. As we will explain in detail

46
3 Markov Chains and Hidden Markov Models
in Chap. 8, for a ﬁxed set of parameters the ﬁrst one can be thought of as
the joint probability of the occurrence of the pairs (x1, π1), . . . , (xn, πn) in n
“independent runs” of the corresponding HMM, if we treat it as a process
that generates pairs of sequences; the second one can be thought of as the
probability of the occurrence of x1, . . . , xn in n “independent runs” of the
HMM if we treat it as a process that generates single sequences. Therefore, it
is often said that parameter estimation procedures that attempt to maximize
the likelihoods assume that the components of the datasets are independent.
3.6.1 Estimation when Paths are Known
We will now explain the estimation procedure for datasets of the ﬁrst type.
Suppose we are given n pairs of sequences (x1, π1), . . . , (xn, πn), where for
each j, xj is a ﬁnite sequence of letters from the alphabet Q and πj is a path
of the same length through the graph representing the a priori connectivity,
that starts at B and ends at E (in particular, the path component of the
dataset agrees with the a priori connectivity). Then we can count the number
of times each particular transition or emission is used in the set of training
sequences. Let these be Hαβ and Jl(a) with α = B, 1, . . . , N, β = 1, . . . , N, E,
l = 1, . . . , N, a ∈Q. Then we set
pαβ =
Hαβ

γ=1,...,N,E
Hαγ
,
ql(a) =
Jl(a)

b∈Q
Jl(b)
,
(3.17)
which is the HMM analogue of formula (3.2). Note that with this estimation
process the Markov chain underlying the resulting HMM is automatically non-
trivially connected. As in the case of Markov chains, one should bear in mind
that if in the training data a particular state Gi does not occur at all, then
the transition and emission probabilities pij, pi 0, qi(a) cannot be estimated
from the data and can be set arbitrarily (see Example 3.13).
It will be explained in Chap. 8 (see Part 6.a of Example 8.12 and Exercise
8.5) that this estimation procedure does not simply look reasonable, but in
fact gives values of transition and emission probabilities that guarantee that
the likelihood P(x1, π1) × . . . × P(xn, πn) is maximal possible.
Example 3.7. Consider the a priori connectivity shown in Fig. 3.5. We wish
to use it for modeling prokaryotic genes, and suppose that we are given four
DNA sequences that are extensions of sequences (3.5)
x1 : ATGACTATGCTATTGATTTAACGC
x2 : CCCATGGTGAAAGACTTCTAAGAT
x3 : AAAGTGACTATGCCCGATGAACGCTAGGAA
x4 : ATGGATATGAAGCATGATTAACAT.
(3.18)

3.6 Parameter Estimation for HMMs
47
Assume that in this data all gene sequences have been determined experimen-
tally (shown in bold). Thus, the experimental information provides us with
the following four paths through the graph in Fig. 3.5, corresponding to the
four sequences
π1 : 0 ig ig ig ig ig ig g g g g g g g g g g g g g g g ig ig ig 0
π2 : 0 ig ig ig ig ig ig g g g g g g g g g g g g g g g ig ig ig 0
π3 : 0 ig ig ig ig ig ig ig ig ig g g g g g g g g g g g g g g g g g g ig ig ig 0
π4 : 0 ig ig ig ig ig ig g g g g g g g g g g g g g g g ig ig ig 0.
Then from formulas (3.17) we obtain
p0 g = 0,
p0 ig = 1,
pg g = 59
63,
pg ig = 4
63,
pg 0 = 0,
pig g = 4
39,
pig ig = 31
39,
pig 0 = 4
39,
qg(A) = 23
63, qg(C) = 1
7,
qg(G) = 13
63, qg(T) = 2
7,
qig(A) = 1
3, qig(C) = 8
39, qig(G) = 3
13, qig(T) = 3
13.
3.6.2 Estimation when Paths are Unknown
We will now discuss estimation procedures for datasets of the second type.
Suppose we are given n sequences x1, . . . , xn of letters from the alphabet
Q. In this case we would like to choose parameter values that maximize the
likelihood P(x1)×. . .×P(xn) (assuming that such optimal values exist). In this
situation there is no explicit formula that would produce optimal values from
the training data x1, . . . , xn. One approach to determining optimal parameter
values in practical applications is to apply standard optimization algorithms
to P(x1) × . . . × P(xn) treated as a function of the parameters. All such
general algorithms are iterative; they start with some initial parameter values
(of course, the initial values of the transition probabilities must agree with the
condition of non-trivial connectedness) and have certain stopping criteria (for
example, the algorithm stops if the change in values of P(x1)×. . .×P(xn) from
iteration to iteration becomes smaller than some predetermined threshold). A
common problem with optimization algorithms is that they may converge to
a point close to a point of local, not global maximum of P(x1)×. . .×P(xn) in
the parameter space. Since the result depends on the initial parameter values,
one way to improve search for a global maximum is to let the optimizer have
several runs starting with diﬀerent sets of initial parameter values.

48
3 Markov Chains and Hidden Markov Models
For the special case of maximizing the function P(x1)×. . . ×P(xn), there
exists a particular iteration method that is commonly used, known as the
Baum-Welch training algorithm [B]. It is possible to show that when the al-
gorithm is applied, the likelihood P(x1) × . . . × P(xn) does not decrease from
iteration to iteration (in fact, it increases if the parameter values change) –
see, e.g., [DEKM]. As all general optimization algorithms, the Baum-Welch
algorithm may converge to a point close to a point of local, not global max-
imum of P(x1) × . . . × P(xn). The algorithm ﬁrst estimates Hαβ and Jl(a)
using the current values of pαβ and ql(a); then (3.17) are used to produce new
values of pαβ and ql(a), α = B, 1, . . . , N, β = 1, . . . , N, E, l = 1, . . . , N, a ∈Q.
We will now explain the algorithm in detail. As in Sect. 3.5, for a ﬁxed
sequence x = x1 . . . xL consider the event E(x) and recall that P(E(x)) =
P(x). Next, ﬁx three integers 1 ≤i ≤L −1, 1 ≤k, l ≤N, and consider the
event
Ei,(k,l) = {(y, π) ∈S : the length of y and π is ≥i + 1,
and πi = Gk, πi+1 = Gl}.
We suppose that P(x) > 0 (otherwise x is not an interesting sequence) and
calculate the conditional probability P(Ei,(k,l)|E(x)). Assuming that 2 ≤i ≤
L −2 we obtain
P(Ei,(k,l)|E(x)) = P(Ei,(k,l) ∩E(x))
P(x)
=
1
P(x)

π1,...,πi−1,πi+2,...,πL∈X
p0 π1
×qπ1(x1)pπ1 π2qπ2(x2) . . . pπi−2 πi−1qπi−1(xi−1)pπi−1 Gkqk(xi)pklql(xi+1)pGl πi+2
×qπi+2(xi+2) . . . pπL−1 πLqπL(xL)pπL 0 =
1
P(x)

π1,...,πi−1∈X
p0 π1qπ1(x1)pπ1 π2
×qπ2(x2) . . . pπi−2 πi−1qπi−1(xi−1)pπi−1 Gkqk(xi)pklql(xi+1)

πi+2,...,πL∈X
pGl πi+2
×qπi+2(xi+2) . . . pπL−1 pLqπL(xL)pπL 0 = fk(i)pklql(xi+1)bl(i + 1)
P(x)
,
where fk(i) and bl(i) are the quantities found from the forward and backward
algorithms respectively. Analogous calculations for i = 1 and i = L −1 show
that the same result holds in these cases as well and thus we have
P(Ei,(k,l)|E(x)) = fk(i)pklql(xi+1)bl(i + 1)
P(x)
(3.19)
for i = 1, . . . , L −1 and k, l = 1, . . . , N. The probability P(Ei,(k,l)|E(x))
is the probability with which xi and xi+1 are emitted at states Gk and Gl

3.6 Parameter Estimation for HMMs
49
respectively and is also called the posterior probability of states Gk and Gl at
observations i and i + 1 respectively given x.
From formula (3.16) we also have
P(E1,l|E(x)) = fl(1)bl(1)
P(x)
= p0lql(x1)bl(1)
P(x)
,
P(EL,k|E(x)) = fk(L)bk(L)
P(x)
= fk(L)pk0
P(x)
.
(3.20)
If we assume for convenience that states B and E emit the symbols B and
E respectively with probability 1 and deﬁne E0,(B,l) = E1,l, EL,(k,E) = EL,k,
fB(0) = 1, bE(L + 1) = 1, for k, l = 1, . . . , N, it then follows from (3.20) that
formulas (3.19) also hold for E0,(B,l) and EL,(k,E), k, l = 1, . . . , N.
We are now ready to describe the Baum-Welch algorithm. The algo-
rithm starts with some initial parameter values p(0)
αβ, q(0)
k (a), α = B, 1, . . . , N,
β = 1, . . . , N, E, a ∈Q, and these values are chosen in such a way that the
corresponding underlying Markov chain is non-trivially connected, its connec-
tivity does not contradict the a priori connectivity, and the initial probabil-
ities P (0)(xr) of the training sequences are positive for all r. One can also
incorporate prior knowledge about the training data into the initial values.
We will now describe the recursion step. Let xr = xr
1 . . . xr
mr, r = 1, . . . , n,
and let p(s)
αβ, q(s)
k (a), α = B, 1, . . . , N, β = 1, . . . , N, E, a ∈Q, be the parameter
values after s steps of the algorithm. Denote by P (s)(xr), f r (s)
k
(i) and br (s)
l
(i)
the probabilities and the quantities found from the forward and backward al-
gorithm for the sequence xr, i = 1, . . . , mr, r = 1, . . . , n, k, l = 1, . . . , N, with
the above parameter values. The recursion step for the transition probabilities
is based on formulas (3.19) and (3.20). For k, l = 1, . . . , N set
H(s)
kl =
n

r=1
1
P (s)(xr)
mr−1

i=1
f r (s)
k
(i)p(s)
kl q(s)
l
(xr
i+1)br (s)
l
(i + 1),
H(s)
Bl =
n

r=1
p(s)
0l q(s)
l
(xr
1)br (s)
l
(1)
P (s)(xr)
,
H(s)
kE =
n

r=1
f r (s)
k
(mr)p(s)
k0
P (s)(xr)
,
(3.21)
and the transition probabilities p(s+1)
αβ
are produced from H(s)
αβ using the ﬁrst
formula in (3.17), α = B, 1, . . . , N, β = 1, . . . , N, E. Note that if for some α, β
we had p(0)
αβ = 0, then the above estimation procedure ensures that p(s)
αβ = 0
for all s. Therefore, the connectivity of the underlying Markov chain obtained

50
3 Markov Chains and Hidden Markov Models
on every iteration step does not contradict the a priori connectivity. It is also
possible to show that the underlying Markov chain obtained on every step
is non-trivially connected (see Exercise 3.11). Note that the ﬁrst formula in
(3.17) can only be used if the corresponding denominator is non-zero; other-
wise the new values of the relevant transition probabilities are set arbitrarily.
Next, we will estimate the emission probabilities. For this purpose we
will use the posterior probabilities P(Ei,k|E(x)) calculated for any sequence
x = x1 . . . xL in the previous section, i = 1, . . . , L, k = 1, . . . , N. The recursion
step for the emission probabilities is based on formula (3.16). For l = 1, . . . , N
and a ∈Q we set
J(s)
l
(a) =
n

r=1
1
P (s)(xr)

{i=1,...,mr: xr
i =a}
f r (s)
l
(i)br (s)
l
(i),
(3.22)
and the emission probabilities q(s+1)
l
(a) are produced from J(s)
l
using the
second formula in (3.17). Note that the second formula in (3.17) can only be
used if the corresponding denominator is non-zero; otherwise the new values
of the relevant emission probabilities are set arbitrarily.
Since the likelihood of the training data does not decrease with every step
of the algorithm, P (s)(xr) > 0 for all r and s.
We will now give an example of applying the Baum-Welch algorithm.
Example 3.8. Consider the connectivity shown in Fig. 3.5. For the purposes
of this example re-denote the state “g” by G1 and the state “ig” by G2. For
simplicity assume that Q is the two-letter alphabet {A, B} and suppose that
we are given the following training data
x1 : ABA
x2 : ABB
x3 : AB.
Set the initial parameter values as follows
p(0)
01 = 1,
p(0)
02 = 0,
p(0)
11 = 1
2,
p(0)
12 = 1
2,
p(0)
10 = 0,
p(0)
21 = 0,
p(0)
22 = 0,
p(0)
20 = 1,
q(0)
1 (A) = 1
4, q(0)
1 (B) = 3
4,
q(0)
2 (A) = 1
2, q(0)
2 (B) = 1
2.

3.6 Parameter Estimation for HMMs
51
Applying the forward and backward algorithms to each of the three training
sequences we obtain
f 1(0)
1
(1) = 1
4,
f 1(0)
2
(1) = 0,
f 1(0)
1
(2) = 3
32, f 1(0)
2
(2) = 1
16,
f 1(0)
1
(3) =
3
256, f 1(0)
2
(3) =
3
128, f 2(0)
1
(1) = 1
4,
f 2(0)
2
(1) = 0,
f 2(0)
1
(2) = 3
32,
f 2(0)
2
(2) = 1
16,
f 2(0)
1
(3) =
9
256, f 2(0)
2
(3) =
3
128,
f 3(0)
1
(1) = 1
4,
f 3(0)
2
(1) = 0,
f 3(0)
1
(2) = 3
32, f 3(0)
2
(2) = 1
16,
b1(0)
1
(1) = 3
32,
b1(0)
2
(1) = 0,
b1(0)
1
(2) = 1
4,
b1(0)
2
(2) = 0,
b1(0)
1
(3) = 0,
b1(0)
2
(3) = 1,
b2(0)
1
(1) = 3
32,
b2(0)
2
(1) = 0,
b2(0)
1
(2) = 1
4,
b2(0)
2
(2) = 0,
b2(0)
1
(3) = 0,
b2(0)
2
(3) = 1,
b3(0)
1
(1) = 1
4,
b3(0)
2
(1) = 0,
b3(0)
1
(2) = 0,
b3(0)
2
(2) = 1,
P (0)(x1) =
3
128, P (0)(x2) =
3
128, P (0)(x3) = 1
16.
The initial value of the likelihood therefore is
P (0)(x1)P (0)(x2)P (0)(x3) =
9
262144.
Next, formulas (3.21) and (3.22) give
H(0)
11 = 2,
H(0)
12 = 3,
H(0)
21 = 0,
H(0)
22 = 0,
H(0)
B1 = 3,
H(0)
B2 = 0,
H(0)
1E = 0,
H(0)
2E = 3,
J(0)
1 (A) = 3, J(0)
1 (B) = 2, J(0)
2 (A) = 1, J(0)
2 (B) = 2,
and therefore from formulas (3.17) we obtain

52
3 Markov Chains and Hidden Markov Models
p(1)
01 = 1,
p(1)
02 = 0,
p(1)
11 = 2
5,
p(1)
12 = 3
5,
p(1)
10 = 0,
p(1)
21 = 0,
p(1)
22 = 0,
p(1)
20 = 1,
q(1)
1 (A) = 3
5, q(1)
1 (B) = 2
5,
q(1)
2 (A) = 1
3, q(1)
2 (B) = 2
3.
Further, applying the forward algorithm to the sequences x1, x2, x3 with the
new parameter values we get
f 1(1)
1
(1) = 3
5,
f 1(1)
2
(1) = 0,
f 1(1)
1
(2) = 12
125, f 1(1)
2
(2) = 6
25,
f 1(1)
1
(3) =
72
3125, f 1(1)
2
(3) = 12
625, f 2(1)
1
(1) = 3
5,
f 2(1)
2
(1) = 0,
f 2(1)
1
(2) = 12
125, f 2(1)
2
(2) = 6
25,
f 2(1)
1
(3) =
48
3125, f 2(1)
2
(3) = 24
625,
f 3(1)
1
(1) = 3
5,
f 3(1)
2
(1) = 0,
f 3(1)
1
(2) = 12
125, f 3(1)
2
(2) = 6
25,
P (1)(x1) = 12
625, P (1)(x2) = 24
625, P (1)(x3) = 6
25.
Hence the value of the likelihood after one step of the Baum-Welch algorithm
is
P (1)(x1)P (1)(x2)P (1)(x3) =
1728
9765625,
which
is
indeed
larger
than
the
previously
determined
initial
value
P (0)(x1)P (0)(x2)P (0)(x3).
There is a frequently used alternative to the Baum-Welch training called
the Viterbi training algorithm. It is also an iterative algorithm that starts with
some initial parameter values satisfying the same conditions as in the case of
the Baum-Welch training. On each step all Viterbi paths π∗(x1), . . . π∗(xn)
for the training sequences are found and then the parameters are re-estimated
as in the case when paths were known. We will show below that on each step
of the Viterbi training P(x1, π∗(x1)) × . . . × P(xn, π∗(xn)) does not decrease,

3.6 Parameter Estimation for HMMs
53
that is, this algorithm is designed to attempt to maximize P(x1, π∗(x1)) ×
. . . × P(xn, π∗(xn)), not the likelihood P(x1) × . . . × P(xn) which may in
fact decrease. Perhaps for this reason, the Viterbi training does not generally
perform as well as the Baum-Welch training.
We will now describe the recursion step in detail. As before, let xr =
xr
1 . . . xr
mr, r = 1, . . . , n, be the training sequences, and let p(s)
αβ, q(s)
k (a), α =
B, 1, . . . , N, β = 1, . . . , N, E, a ∈Q, be the parameter values after s steps of the
algorithm. All quantities calculated using these values will bear the superscript
(s). With these parameter values we will now determine all the Viterbi paths
for each training sequence. For simplicity we assume that there is in fact only
one Viterbi path for each sequence on each step of the algorithm (the general
case is completely analogous to this special one, but is harder notationally).
Let π∗(s)(xr) be the Viterbi path for xr, r = 1, . . . , n. We further assume
that every state of the underlying Markov chain is encountered in the paths
π∗(s)(x1), . . . , π∗(s)(xn) for every s. We will now re-estimate the parameters
from formulas (3.17) for the training dataset that consists of the pairs of
sequences (x1, π∗(s)(x1)), . . . , (xn, π∗(s)(xn)), as in the case when paths are
known. This can be done since the Viterbi paths clearly agree with the a priori
connectivity of the model. The HMM arising on each step has an underlying
Markov chain that is non-trivially connected and whose connectivity does not
contradict the a priori connectivity.
As we remarked above, the estimation procedure in the case when paths
are known maximizes the corresponding likelihood of the training dataset,
and therefore
n

r=1
P (s)(xr, π∗(s)(xr)) ≤
n

r=1
P (s+1)(xr, π∗(s)(xr)).
(3.23)
Further, we clearly have
P (s+1)(xr, π∗(s)(xr)) ≤P (s+1)(xr, π∗(s+1)(xr)),
(3.24)
for all s and r. Together with (3.23) this gives
n

r=1
P (s)(xr, π∗(s)(xr)) ≤
n

r=1
P (s+1)(xr, π∗(s+1)(xr)).
that is, n
r=1 P (s)(xr, π∗(s)(xr)) does not decrease from step to step. In par-
ticular, P (s)(xr, π∗(s)(xr)) > 0 for all s and r.
Suppose that for some s0 ≥0 we have
n

r=1
P (s0+1)(xr, π∗(s0+1)(xr)) =
n

r=1
P (s0)(xr, π∗(s0)(xr)).
(3.25)
Then formulas (3.23) and (3.24) imply that

54
3 Markov Chains and Hidden Markov Models
P (s0+1)(xr, π∗(s0+1)(xr)) = P (s0+1)(xr, π∗(s0)(xr)),
for all r, and hence π∗(s0+1)(xr) = π∗(s0)(xr) for all r. Therefore the parameter
values after s0 + 2 steps coincide with those after s0 + 1 steps. This clearly
implies that the parameter values after s0 + k steps for any k ≥2 coincide
with those after s0 + 1 steps. Therefore the natural stopping criterion for the
Viterbi training is based on condition (3.25) and regards step s0+1 as the last
one. Condition (3.25) is satisﬁed, for example, if π∗(s0)(xr) = π∗(s0−1)(xr) for
all r (here s0 ≥1), in which case one may even stop after s0 steps, since the
parameter values will not change afterwards.
The parameter values on each step are obtained from paths through the
graph that represents the a priori connectivity, such that every state of the
underlying Markov chain is encountered in at least one of the paths. Clearly,
there are only ﬁnitely many sets of parameter values that can be obtained in
this way. Therefore n
r=1 P (s)(xr, π∗(s)(xr)) cannot increase inﬁnitely many
times, and thus the Viterbi training always stops after ﬁnitely many steps.
Note, however, that the algorithm may stop before reaching the maximal value
of P(x1, π∗(x1)) × . . . × P(xn, π∗(xn)).
We will now give an example of applying the Viterbi training. Below we
denote by vr(s)
k
(i), Vr(s)
k
(i), Vr(s)(mr) the quantities and sets calculated from
the Viterbi algorithm for the sequence xr, i = 1, . . . , mr, r = 1, . . . , n, k =
1, . . . , N, using the parameter values after s steps of the training algorithm.
Example 3.9. Consider the situation of Example 3.8, but set the initial para-
meter values in a slightly diﬀerent way
p(0)
01 = 1,
p(0)
02 = 0,
p(0)
11 = 1
2,
p(0)
12 = 1
2,
p(0)
10 = 0,
p(0)
21 = 0,
p(0)
22 = 1
2,
p(0)
20 = 1
2,
q(0)
1 (A) = 1
4, q(0)
1 (B) = 3
4,
q(0)
2 (A) = 1
2, q(0)
2 (B) = 1
2.
We will now re-estimate the parameters using the Viterbi training. We
have

3.6 Parameter Estimation for HMMs
55
v1(0)
1
(1) = 1
4,
v1(0)
2
(1) = 0,
v1(0)
1
(2) = 3
32, V1(0)
1
(1) = {1},
v1(0)
2
(2) = 1
16, V1(0)
2
(1) = {1}, v1(0)
1
(3) =
3
256, V1(0)
1
(2) = {1},
v1(0)
2
(3) =
3
128, V1(0)
2
(2) = {1}, V1(0)(3) = {2},
v2(0)
1
(1) = 1
4,
v2(0)
2
(1) = 0,
v2(0)
1
(2) = 3
32, V2(0)
1
(1) = {1},
v2(0)
2
(2) = 1
16, V2(0)
2
(1) = {1}, v2(0)
1
(3) =
9
256, V2(0)
1
(2) = {1},
v2(0)
2
(3) =
3
128, V2(0)
2
(2) = {1}, V2(0)(3) = {2},
v3(0)
1
(1) = 1
4,
v3(0)
2
(1) = 0,
v3(0)
1
(2) = 3
32, V3(0)
1
(1) = {1},
v3(0)
2
(2) = 1
16, V3(0)
2
(1) = {1}, V3(0)(2) = {2}.
For each sequence there is a single Viterbi path. These paths π∗(0)(x1),
π∗(0)(x2), π∗(0)(x3) and the corresponding probabilities are shown below
π∗(0)(x1) : G1G1G2, P (0)(x1, π∗(0)(x1)) =
3
256,
π∗(0)(x2) : G1G1G2, P (0)(x2, π∗(0)(x2)) =
3
256,
π∗(0)(x3) : G1G2,
P (0)(x3, π∗(0)(x3)) = 1
32.
which, in particular, gives
P (0)(x1, π∗(0)(x1))P (0)(x2, π∗(0)(x2))P (0)(x3, π∗(0)(x3)) =
9
2097152.
We will now consider the dataset (x1, π∗(0)(x1)), (x2, π∗(0)(x2)), (x3, π∗(0)(x3))
and determine H(0)
αβ and J(0)
l
(a) as in the case when paths are known. We
obtain

56
3 Markov Chains and Hidden Markov Models
H(0)
11 = 2,
H(0)
12 = 3,
H(0)
21 = 0,
H(0)
22 = 0,
H(0)
B1 = 3,
H(0)
B2 = 0,
H(0)
1E = 0,
H(0)
2E = 3,
J(0)
1 (A) = 3, J(0)
1 (B) = 2, J(0)
2 (A) = 1, J(0)
2 (B) = 2,
and from formulas (3.17) we get
p(1)
01 = 1,
p(1)
02 = 0,
p(1)
11 = 2
5,
p(1)
12 = 3
5,
p(1)
10 = 0,
p(1)
21 = 0,
p(1)
22 = 0,
p(1)
20 = 1,
q(1)
1 (A) = 3
5, q(1)
1 (B) = 2
5,
q(1)
2 (A) = 1
3, q(1)
2 (B) = 2
3.
It is easy to see that π∗(1)(xr) = π∗(0)(xr) for r = 1, 2, 3. Hence in this
example the Viterbi training stops after one iteration. It is easy to calculate the
probabilities P (1)(x1, π∗(1)(x1)), P (1)(x2, π∗(1)(x2)), P (1)(x3, π∗(1)(x3)), and
we obtain
P (1)(x1, π∗(1)(x1)) P (1)(x2, π∗(1)(x2))P (1)(x3, π∗(1)(x3)) =
12/625 × 24/625 × 6/25 = 1728/9765625,
which
is
indeed
larger
than
the
initial
value
P (0)(x1, π∗(0)(x1))×
P (0)(x2, π∗(0)(x2))P (0)(x3, π∗(0)(x3)).
3.7 HMMs with Silent States
So far we have assumed that an a priori connectivity was ﬁxed in advance.
Specifying it, however, is an important part of modeling by means of HMMs.
It may look reasonable at ﬁrst sight to allow all possible transitions and hope
that a parameter estimation process will eventually lead to a good model.
Unfortunately, this approach almost never works in practice. It usually pro-
duces very bad models, even if training datasets are large. The main problem
here is local maxima. The less constrained the model is, the more local max-
ima the corresponding likelihood function tends to have and hence the harder

3.7 HMMs with Silent States
57
it becomes for an optimization algorithm to distinguish them from global
ones. Well-performing HMMs are built by carefully deciding which transi-
tions should be allowed in the model, and these decisions are based on prior
knowledge about the problem of interest. Choosing a model structure is not
an exact science, and it is diﬃcult to give general recommendations on how to
select the best possible design. In this section we will describe one trick that
is often used in model building. The trick is to introduce states of a special
kind, so-called silent states. These states do not emit any symbols, just like
the begin and end states.
An HMM with silent states is a discrete-time ﬁnite Markov chain (that
we always assume to be non-trivially connected) for which all the states (not
including B and E) are divided into two groups X = {G1, . . . , GN}, Y =
{D1, . . . , DM}, Y ̸= ∅, and for each state Gk and each symbol a in a ﬁnite
alphabet Q an emission probability is speciﬁed. The corresponding transition
and emission probabilities will be denoted as pB Gi = p0 Gi, pB Dk = p0 Dk,
pGi Gj, pGi Dk, pDk Gi, pDk Dl, pGi E = pGi 0, pDk E = pDk 0, qGi(a), for i, j =
1, . . . , N, k, l = 1, . . . , M, a ∈Q. As before, the Markov chain will be called
the underlying Markov chain of the HMM. The states in Y do not emit any
symbols and are called silent. The states in X are called non-silent.
HMMs with silent states are useful, for example, in the following situation.
Suppose we have a very long chain of states in which each state is required to
be connected to any state further in the chain, as in the connectivity shown
in Fig. 3.7. The number of parameters required to be estimated quickly be-
Fig. 3.7.
comes very large as the length of the chain increases, and all the transition
probabilities cannot be well estimated from realistic datasets (note that for
the connectivity in Fig. 3.7 the number of free parameters corresponding to
the transition probabilities is 5). In this situation one can use silent states
to reduce the number of parameters. Consider, for example, the connectivity
shown in Fig. 3.8, where D1, D2, D3 are silent states.

58
3 Markov Chains and Hidden Markov Models
Fig. 3.8.
Since the silent states do not emit any letters, it is possible to go from any
Gi to any Gj with j > i without emitting any letters. Hence the collections
of sequences that are generated with non-zero probabilities by an HMM with
connectivity in Fig. 3.7 and an HMM with connectivity in Fig. 3.8 coincide.
Although for the connectivity shown in Fig. 3.8 the number of free parameters
corresponding to the transition probabilities is also 5, it is easy to see that
connectivities analogous to that in Fig. 3.8 will require a much smaller number
of parameters than those analogous to the connectivity in Fig. 3.7, as the
length of the chains grows.
One can think of an HMM with silent states as a process of sequence
generation. As for ordinary HMMs, there are two possible interpretations
of this process. The ﬁrst interpretation is a procedure that generates pairs
of sequences (x, π), where x is a ﬁnite sequence of letters from Q and π is
a ﬁnite path through the underlying Markov chain such that the non-silent
length Ln/s(π) of π, that is, the number of non-silent states in π, is equal to the
length of x. Another way is to think of an HMM with silent states as a process
that generates sequences of letters from Q, that is, in this interpretation we
ignore paths along which sequences are generated.
All concepts introduced in the preceding sections for ordinary HMMs have
analogues for HMMs with silent states. Let x = x1 . . . xL be a sequence of
letters from Q and π = π1 . . . πT , for T ≥L, be a path with Ln/s(π) = L. Let
πi1, . . . , πiL ∈X for some 1 ≤i1 < . . . < iL ≤T. We will then deﬁne the
probability P(x, π) of the pair (x, π) as follows
P(x, π) = p0 π1pπ1π2 × . . . × pπT −1πT pπT 0 × qπi1(x1)qπi2 (x2) × . . . × qπiL(xL).
It is also natural to deﬁne “the total probability” P(x) of x as
P(x) =

all π: Ln/s(π) = L
P(x, π).
(3.26)

3.7 HMMs with Silent States
59
It follows from Exercise 3.14 that 
x P(x) = 1, where the summation is taken
over all sequences of all ﬁnite lengths, including length 0 (sequences of length
0 correspond to paths that consist only of silent states).
We will be interested in generalizing the algorithms discussed in the pre-
ceding sections for HMMs to the case of HMMs with silent states. It can be
done under the following additional condition: we say that an HMM does not
have silent loops if the silent states can be enumerated in such a way that
transitions with non-zero probabilities between any two silent states are only
possible from the lower numbered state to the higher numbered one, and the
probability of any transition from a silent state into itself is equal to 0. From
now on we will assume that all HMMs do not have silent loops and that
the enumeration of silent states D1, . . . , DM agrees with this condition. We
will also apply this terminology to connectivities and speak about a priori
connectivities that do not have silent loops.
We will ﬁrst generalize the Viterbi algorithm. Fix a sequence x = x1 . . . xL
of letters from the alphabet Q and deﬁne for k = 1, . . . , N
vGk(1) = qGk(x1) max

p0 Gk,
max
1 ≤s ≤M,
1 ≤i1 < . . . < is ≤M
p0 Di1pDi1 Di2 . . . pDis−1 Dis pDis Gk

.
For every k let VGk(0) contain all paths among 0 Gk and 0 Di1Di2 . . . DisGk
that realize the value vGk(1). Next, for i = 1, . . . , L and m = 1, . . . , M set
vDm(i) =
max
k=1,...,N

vGk(i) max

pGk Dm,
max
1 ≤s ≤m −1,
1 ≤i1 < . . . < is ≤m −1
pGk Di1 pDi1 Di2 . . . pDis−1 Dis pDis Dm

,
and let VDm(i −1) be the collection of all paths among GkDm and
GkDi1Di2 . . . DisDm that realize the value vDm(i).
In order to perform a recursion step, for i = 1, . . . , L −1 and k = 1, . . . , N
set
vGk(i+1) = qGk(xi+1) max

max
l = 1, . . . , N vGl(i)pGl Gk,
max
m = 1, . . . , M vDm(i)pDm Gk

,
and deﬁne VGk(i) to be the collection of all paths among GlGk and DmGk
that realize the value vGk(i + 1). It is not diﬃcult to show that
max
all π: Ln/s(π) = L P(x, π) = max

max
k = 1, . . . , N vGk(L)pGk 0,
max
m = 1, . . . , M vDm(L)pDm 0

(3.27)

60
3 Markov Chains and Hidden Markov Models
(see Exercise 3.15).
We also deﬁne V(L) to be the collection of all states that realize the
value maxall π: Ln/s(π) = L P(x, π). Traceback starts at any element of V(L), goes
through the relevant paths in some VGk(i) and VDm(i) with 0 ≤i ≤L −1,
ending with a path in a certain VGk(0).
We will now give an example of applying the Viterbi algorithm in the case
of HMMs with silent states.
Example 3.10. Consider the HMM for which Q is the two-letter alphabet
{A, B}, whose underlying Markov chain is shown in Fig. 3.9 and whose emis-
Fig. 3.9.
sion probabilities are as follows
qG1(A) = 0.2,
qG1(B) = 0.8.
We let x = ABB and ﬁnd the (unique) Viterbi path π∗for it. We have
vG1(1) = 0.2 × 0.8 × 0.7 × 0.9 = 0.1008,
VG1(0) = {0 D1D2G1},
vD1(1) = 0.1008 × 0.8 = 0.08064,
VD1(0) = {G1D1},
vD2(1) = 0.1008 × 0.8 × 0.7 = 0.056448,
VD2(0) = {G1D1D2},
vG1(2) = 0.8 × 0.056448 × 0.9 = 0.04064256,
VG1(1) = {D2G1},
vD1(2) = 0.04064256 × 0.8 = 0.03251405,
VD1(1) = {G1D1},
vD2(2) = 0.04064256 × 0.8 × 0.7 = 0.02275983,
VD2(1) = {G1D1D2},
vG1(3) = 0.8 × 0.02275983 × 0.9 = 0.01638708,
VG1(2) = {D2G1},
vD1(3) = 0.01638708 × 0.8 = 0.01310966,
VD1(2) = {G1D1},
vD2(3) = 0.01638708 × 0.8 × 0.7 = 0.009176765, VD2(2) = {G1D1D2}.
This gives

3.7 HMMs with Silent States
61
max
all π: Ln/s(π) = 3 P(x, π) = 0.009176765 × 0.1 = 0.0009176765,
and
V(3) = {D2}.
Tracing back we obtain the Viterbi path
π∗= 0 D1D2G1D1D2G1D1D2G1D1D2 0.
We will now generalize the forward algorithm. Fix a sequence x = x1 . . . xL
and deﬁne for k = 1, . . . , N
fGk(1) = qGk(x1)

p0 Gk+

1 ≤s ≤M,
1 ≤i1 < . . . < is ≤M
p0 Di1 pDi1 Di2 . . . pDis−1 Dis pDis Gk

.
Next, for i = 1, . . . , L and m = 1, . . . , M set
fDm(i) =

k=1,...,N

fGk(i)

pGk Dm
+

1 ≤s ≤m −1,
1 ≤i1 < . . . < is ≤m −1
pGk Di1 pDi1 Di2 . . . pDis−1 Dis pDis Dm

.
In order to perform a recursion step, for i = 1, . . . , L −1 and k = 1, . . . , N set
fGk(i + 1) = qGk(xi+1)


l = 1, . . . , N
fGl(i)pGl Gk +

m = 1, . . . , M
fDm(i)pDm Gk

.
It is not diﬃcult to show that
P(x) =


k = 1, . . . , N
fGk(L)pGk 0 +

m = 1, . . . , M
fDm(L)pDm 0

(3.28)
(see Exercise 3.16).
We will now give an example of applying the forward algorithm in the case
of HMMs with silent states.
Example 3.11. We will ﬁnd P(x) for the sequence x = ABB in Example 3.10.
We have

62
3 Markov Chains and Hidden Markov Models
fG1(1) = 0.2 × (0.2 + 0.8 × 0.3 + 0.8 × 0.7 × 0.9) = 0.1888,
fD1(1) = 0.1888 × 0.8 = 0.15104,
fD2(1) = 0.1888 × (0.1 + 0.8 × 0.7) = 0.124608,
fG1(2) = 0.8 × (0.1888 × 0.1 + 0.15104 × 0.3 + 0.124608 × 0.9) = 0.1410714,
fD1(2) = 0.1410714 × 0.8 = 0.1128571,
fD2(2) = 0.1410714 × (0.1 + 0.8 × 0.7) = 0.09310712,
fG1(3) = 0.8 × (0.1410714 × 0.1 + 0.1128571 × 0.3 + 0.09310712 × 0.9)
= 0.1054085,
fD1(3) = 0.1054085 × 0.8 = 0.0843268,
fD2(3) = 0.1054085 × (0.1 + 0.8 × 0.7) = 0.06956961,
which gives
P(x) = 0.06956961 × 0.1 = 0.006956961.
It is also possible to generalize the backward algorithm as well as the
Baum-Welch and Viterbi training algorithms to the case of HMMs with silent
states (see Exercises 3.17–3.19). These generalizations follow the principles
that we have utilized generalizing the Viterbi and forward algorithms: one
must include in consideration chains of silent states in an appropriate way.
To illustrate this point once again, below we give an example of parameter
estimation for HMMs with silent states in the case when paths are known.
Example 3.12. Let the a priori connectivity be as shown in Fig. 3.10, and the
Fig. 3.10.
training dataset be (x1, π1), (x2, π2), (x3, π3) with
x1 : ABBA, π1 : 0 G1G1G1D1D2G1D1D2 0
x2 : AB,
π2 : 0 D1D2G1G1D1D2 0
x3 : BAB,
π3 : 0 D1D2G1D1D2G1D1D2G1D1D2 0.

3.8 Proﬁle HMMs
63
Clearly, the path component of the dataset agrees with the a priori connec-
tivity.
From formulas (3.17) we obtain
p0 G1 = 1
3,
p0 D1 = 2
3,
pG1 G1 = 1
3, pG1 D1 = 2
3,
pD2 G1 = 5
8, pD2 0 = 3
8,
qG1(A) = 4
9, qG1(B) = 5
9.
It is also clear that pD1 D2 = 1.
3.8 Proﬁle HMMs
Recall that Markov chains and HMMs can be used for searching with the
purpose of detecting potential membership of a query sequence in a sequence
family. Suppose, for example, that we are given a family F of related se-
quences, and we wish to know whether or not a query sequence x shares some
of the common features of the sequences in F. A straightforward approach is
to ﬁrst choose a connectivity that looks suitable (for example, the simple one
shown in Fig. 3.1, if we are dealing with DNA sequences and Markov mod-
els), to use the sequences in F as the training data to estimate the parameters
and, ﬁnally, assess a probabilistic quantity arising from the resulting model
(for instance, P(x)) to decide whether or not x should be accepted as a new
member of the family F. The problem with this approach, however, is the
choice of connectivity. An arbitrarily picked connectivity will almost certainly
lead to a very poorly performing model, and there is no comprehensive theory
that could help to choose a “good” connectivity.
In this section we will present a particular connectivity for the underlying
Markov models of HMMs that performs very well in various searches. To use
it for searching, however, a multiple alignment of sequences in F is required.
As we explained in Chap. 2, producing a “good” multiple alignment is a
diﬃcult problem in itself, but for the purposes of this section we will assume
that a multiple alignment is given. Certainly, in this situation one may try to
avoid constructing a probabilistic model altogether and to attempt to detect
directly the presence of common features identiﬁable from the alignment in
the sequence x. However, having a probabilistic way of doing so has numerous

64
3 Markov Chains and Hidden Markov Models
advantages; in particular, it allows to compute, at least in some sense, the
probability of x belonging to F. There are special HMMs called proﬁle HMMs
that are commonly used for modeling multiple alignments. They are one of
the most popular applications of HMMs in molecular biology.
We will now explain how a proﬁle HMM is built from a multiple alignment.
Consider the a priori connectivity of the type shown in Fig. 3.11.
Fig. 3.11.
Here Mj and Ij are non-silent, and Dj are silent states; Mj are called
match states, Ij insert states, and Dj delete states. The connectivity shown in
Fig. 3.11 has length (the number of match states) equal to 3. In general, the
length is derived from the given multiple alignment. One simple rule is to set
the length to be equal to the number of columns in the alignment where gaps
occupy no more than half the positions. Such columns will be called match
columns, and all the other ones will be called insert columns. There are also
other methods for selecting the length, but everywhere below we will assume
that this rule is applied.
We wish to use the family F as a training dataset for this a priori connec-
tivity. Training will be done as in the case when paths are known, so for each
sequence in F we will ﬁrst produce a path through the connectivity graph.
Paths come from the multiple alignment. For each sequence in F consider
the corresponding row in the alignment. The row contains letters from Q and
may contain gaps. If a letter is found in a match column, we assign a match
state to its position; if a letter is found in an insert column, we assign an
insert state to its position; if a gap is found in a match column, we assign a
delete state to its position; if a gap is found in an insert column, we skip this

3.8 Proﬁle HMMs
65
position and do not assign any state to it. Reading the row from left to right
then produces a path through the connectivity graph for the sequence.
We will illustrate the above procedure by the following example.
Example 3.13. Let
Q
be
the
two-letter
alphabet
{A, B},
F = {BAAB, AABA, BBB, AA}, and suppose we are given the multiple
alignment
1 2 3 4 5 6 7
x1 : −B A −A B −
x2 : A −A B −A −
x3 : −−B −B −B
x4 : −−A −−A −.
Here columns 3,5,6 are match columns, and the other ones are insert
columns. Hence, the length of the connectivity is chosen to be equal to 3,
as in the example in Fig. 3.11. The corresponding paths for the sequences x1,
x2, x3, x4 are as follows
π1 : 0 I0M1M2M3 0
π2 : 0 I0M1I1D2M3 0
π3 : 0 M1M2D3I3 0
π4 : 0 M1D2M3 0.
We will now consider the dataset (x1, π1), (x2, π2), (x3, π3), (x4, π4) and es-
timate the parameters using formulas (3.17), as in Example 3.12. We obtain
p0 I0 = 1
2,
p0 D1 = 0,
p0 M1 = 1
2,
pM1 I1 = 1
4, pM1 D2 = 1
4, pM1 M2 = 1
2,
pM2 I2 = 0, pM2 D3 = 1
2, pM2 M3 = 1
2,
pM3 I3 = 0, pM3 0 = 1,
pI0 I0 = 0,
pI0 D1 = 0,
pI0 M1 = 1,
pI1 I1 = 0,
pI1 D2 = 1,
pI1 M2 = 0,
pI3 I3 = 0,
pI3 0 = 1,

66
3 Markov Chains and Hidden Markov Models
pD2 I2 = 0,
pD2 M3 = 1,
pD2 D3 = 0,
pD3 I3 = 1,
pD3 0 = 0,
qM1(A) = 3
4, qM1(B) = 1
4,
qM2(A) = 1
2, qM2(B) = 1
2,
qM3(A) = 2
3, qM3(B) = 1
3,
qI0(A) = 1
2, qI0(B) = 1
2,
qI1(A) = 0,
qI1(B) = 1,
qI3(A) = 0,
qI3(B) = 1.
Note that the given data does not allow us to estimate the transition and
emission probabilities pI2 I2, pI2 D3, pI2 M3, qI2(A), qI2(B), since I2 does not
occur anywhere in the sequences π1, π2, π3, π4. This is a common problem
with parameter estimation for proﬁle HMMs. To set the remaining transition
and emission probabilities, heuristic procedures (such as adding pseudocounts)
are frequently used. These procedures may incorporate prior knowledge about
the dataset in question.
For an example of parameter estimation from a family of globin sequences
and subsequent search of a protein sequence database by means of the result-
ing proﬁle HMM see [DEKM].
3.9 Multiple Sequence Alignment by Proﬁle HMMs
In the previous section we showed how one could construct a proﬁle HMM from
a given multiple alignment. In this section we will assume that the sequences
in a family F are unaligned and explain how a proﬁle HMM can be used to
produce a multiple alignment for these sequences. This procedure is heuristic,
but has shown good results.
Consider the a priori connectivity of a proﬁle HMM as introduced in the
previous section. First of all, we must choose its length. The commonly used
rule is to set it to the average length of the sequences in F. Next, we use
the Baum-Welch training or the Viterbi training to estimate the transition

3.9 Multiple Sequence Alignment by Proﬁle HMMs
67
and emission probabilities from the training sequences. Note that since the
sequences are unaligned, we do not have paths for the training data and
hence cannot estimate the parameters directly as it was done in the preceding
section. Next, by the Viterbi algorithm, for each training sequence we ﬁnd the
most probable paths through the resulting model. Finally, we build a multiple
alignment from these paths. Letters are aligned if they are emitted by the same
match states in their paths. Gaps are then inserted appropriately. Since there
may be several ways of positioning the letters emitted at the insert states with
respect to the letters emitted at the match states, we may obtain more than
one multiple alignment, as in Example 3.14 below. We may obtain more than
one alignment also because some sequences can have more than one Viterbi
path. The HMM does not attempt to align subsequences that came from the
insert states in the paths, hence we may not obtain a complete alignment.
The following example illustrates the last step of the multiple alignment
process.
Example 3.14. Let Q be the DNA alphabet {A, C, G, T} and F = {x1 =
ACCG, x2 = TGCG, x3 = AT, x4 = CG, x5 = ATC, x6 = TTG}. The
average length of the sequences is 3, so we will consider the proﬁle HMM
connectivity of length 3, as shown in Fig. 3.11. Suppose we have estimated
the parameters by either the Baum-Welch or Viterbi training procedure, and
suppose that we have found the Viterbi paths for each sequence from the es-
timated parameter values. Assume further that there is only one Viterbi path
π∗(xj) for each xj, and that the paths are
π∗(x1) : 0 M1M2M3I3 0
π∗(x2) : 0 I0D1I1M2D3I3 0
π∗(x3) : 0 D1M2M3 0
π∗(x4) : 0 M1M2D3 0
π∗(x5) : 0 D1M2D3I3I3 0
π∗(x6) : 0 D1D2M3I3I3 0.
These paths lead to the following three multiple alignments
x1 : −−A C C g
x2 : T G −C −g
x3 : −−−A T −−
x4 : −−C G −−−
x5 : −−−A −t c
x6 : −−−−T t g,
x1 : −A −C C g
x2 : T −G C −g
x3 : −−−A T −−
x4 : −C −G −−−
x5 : −−−A −t c
x6 : −−−−T t g,

68
3 Markov Chains and Hidden Markov Models
x1 : A −−C C g
x2 : −T G C −g
x3 : −−−A T −−
x4 : C −−G −−−
x5 : −−−A −t c
x6 : −−−−T t g,
where we do not attempt to align the letters emitted at I3 (shown in lower
case).
The multiple alignment method described in this section was successfully
tested in [KH] on a family of globin sequences. It is implemented in the soft-
ware package HMMER [E] (see http://hmmer.wustl.edu). More details on
using proﬁle HMMs for sequence alignment can be found in [DEKM].
HMMs in general and proﬁle HMMs in particular can be applied to mod-
eling datasets of many types, both in biology and other areas. In the next
chapter we will mention an application of proﬁle HMMs to the problem of
protein folding.
Exercises
3.1. Consider a Markov chain with state set X = {G1, . . . , GN}, transi-
tion probabilities pij = pGi Gj and initialization probabilities P(Gj), i, j =
1, . . . , N. Fix L ∈N and for a sequence x = x1 . . . xL with xj ∈X for all j,
deﬁne P(x) by formula (3.1). Show that 
x P(x) = 1, where the summation
is taken over all sequences of length L.
3.2. Consider a Markov chain with state set X = {G1, . . . , GN}, end state
and transition probabilities p0 Gj, pGi Gj, pGj 0, i, j = 1, . . . , N. Suppose that
the chain is non-trivially connected. For a sequence x = x1 . . . xL with xj ∈X
for all j, deﬁne P(x) by formula (3.4). Show that 
x P(x) = 1, where the
summation is taken over all sequences of ﬁnite positive length (the length is
not ﬁxed). [If you ﬁnd it hard to prove this statement in general, do it in the
case when X contains only two elements.]
3.3. Estimate the transition probabilities for the a priori connectivity shown
in Fig. 3.12 from the following training sequences
x1 : 0 G1G2G3G3G2 0
x2 : 0 G2G3G3G3G2G3 0
x3 : 0 G1G2G3 0
x4 : 0 G2G3G2G3G2 0
(note that the training data agrees with the above connectivity). What is the
connectivity of the resulting Markov model?

Exercises
69
Fig. 3.12.
3.4. Consider
the
DNA
model
obtained
in
Example
3.1.
Let y = AAAATGCTATTGATGGATGAAATGAAAATTTAAAAG be
a stretch of prokaryotic DNA, where the start and stop codons present in y
are shown in bold. This sequence contains three candidate gene sequences
y1 : ATGCTATTGATGGATGAAATGAAAATTTAA
y2 : ATGGATGAAATGAAAATTTAA
y3 : ATGAAAATTTAA.
The ORFs corresponding to these sequences are
z1 : CTATTGATGGATGAAATGAAAATT
z2 : GATGAAATGAAAATT
z3 : AAAATT.
respectively. Suppose that an ORF x is accepted as the ORF of a prokaryotic
gene, if P(x) ≥0.0003, with P(x) calculated from formula (3.4). Assess on
this basis z1, z2 and z3. Will any of y1, y2, y3 be accepted as prokaryotic
genes?
3.5. Consider an HMM whose underlying Markov chain has a state set X =
{G1, . . . , GN}, end state and transition probabilities p0 Gj, pGi Gj, pGj 0, i, j =
1, . . . , N. Suppose that the chain is non-trivially connected, and let qGj(a) be
the emission probabilities of the HMM, j = 1, . . . , N, a ∈Q. For a pair

70
3 Markov Chains and Hidden Markov Models
of sequences (x, π) = (x1 . . . xL, π1 . . . πL) deﬁne P(x, π) by formula (3.8).
Show that 
(x,π) P(x, π) = 1, where the summation is taken over all pairs of
sequences of ﬁnite positive length (the length is not ﬁxed).
3.6. Let the underlying Markov chain of an HMM be as shown in Fig. 3.13.
Fig. 3.13.
Assume that Q is the two-letter alphabet {A, B} and let the emission
probabilities be as follows
q1(A) = 0.5, q1(B) = 0.5,
q2(A) = 0.9, q2(B) = 0.1.
Suppose that a sequence x is accepted as a member of the family F of training
sequences, if P(x) ≥0.02, with P(x) calculated from formula (3.9). Assess on
this basis the sequences y1 = ABB and y2 = BAA. Will either of y1, y2 be
accepted as a member of F? What are the most probable paths π∗(y1) and
π∗(y2) for these sequences? [In this case there is only one most probable path
for each sequence.]
3.7. Consider the HMM for which Q is the two-letter alphabet {A, B}, whose
underlying Markov chain is shown in Fig. 3.14 and whose emission probabili-
ties are as follows

Exercises
71
Fig. 3.14.
q1(A) = 0.7, q1(B) = 0.3,
q2(A) = 0.2, q2(B) = 0.8,
q3(A) = 0.4, q3(B) = 0.6.
For the sequence x = ABA ﬁnd all the Viterbi paths and maxall π of length 3
P(x, π) by applying the Viterbi algorithm.
3.8. Consider the HMM from Exercise 3.7 and ﬁnd P(x), where x = BAB,
by applying the forward algorithm.
3.9. In Exercise 3.8 ﬁnd P(x) by the backward algorithm and perform pos-
terior decoding for the sequence x. Also ﬁnd the (unique) Viterbi path for x
and compare it with the one provided by the posterior decoding.
3.10. For the a priori connectivity shown in Fig. 3.4 in Example 3.2 and the
two-letter alphabet {A, B} estimate the transition and emission probabilities
from the following training data
x1 : AAAB, π1 : G1G1G1G1,
x2 : BBA,
π2 : G1G1G2,
x3 : BB,
π3 : G1G2.

72
3 Markov Chains and Hidden Markov Models
3.11. Show that the underlying Markov chain obtained on every step of the
Baum-Welch algorithm is non-trivially connected.
3.12. In the setup of Example 3.8 calculate the parameter values and the like-
lihood of the training data on the second step of the Baum-Welch algorithm.
3.13. Consider the situation of Example 3.8, but set the initial parameter
values diﬀerently as follows
p(0)
01 = 1
4,
p(0)
02 = 3
4,
p(0)
11 = 1
4,
p(0)
12 = 1
4,
p(0)
10 = 1
2,
p(0)
21 = 0,
p(0)
22 = 1
3,
p(0)
20 = 2
3,
q(0)
1 (A) = 1
5, q(0)
1 (B) = 4
5,
q(0)
2 (A) = 1
3, q(0)
2 (B) = 2
3.
Re-estimate the parameters once using the Viterbi training algorithm.
3.14. Consider an HMM with silent states, and for every ﬁnite sequence x
of letters from the corresponding alphabet Q deﬁne P(x) by formula (3.26).
Show that 
x P(x) = 1, where the summation is taken over all sequences of
all ﬁnite lengths, including length 0 (the length is not ﬁxed).
3.15. Prove identity (3.27).
3.16. Prove identity (3.28).
3.17. Generalize the backward algorithm to the case of HMMs with silent
states which do not have silent loops.
3.18. Generalize the Baum-Welch training algorithm to the case of HMMs
with silent states, where the a priori connectivities do not have silent loops.
For the a priori connectivity from Example 3.12 shown in Fig. 3.10 make one
step of the generalized Baum-Welch algorithm for the training data
x1 : AB
x2 : BB
x3 : BA
starting with the initial values

Exercises
73
p(0)
0 G1 = 1
4,
p(0)
0 D1 = 3
4,
p(0)
G1 G1 = 3
5, p(0)
G1 D1 = 2
5,
p(0)
D2 G1 = 5
7, p(0)
D2 0 = 2
7,
q(0)
G1(A) = 1
8, q(0)
G1(B) = 7
8.
3.19. Generalize the Viterbi training algorithm to the case of HMMs with
silent states, where the a priori connectivities do not have silent loops. In the
situation of Exercise 3.18 make one step of the generalized Viterbi training
algorithm.
3.20. As in Example 3.13, estimate the transition and emission probabilities
for
the
corresponding
connectivity
from
the
family
F = {ABAAA, BAAABB, BABA, AAB, ABABBA} of sequences of letters
from the two letter alphabet {A, B}, given the following multiple alignment
x1 : A B A −−A A −
x2 : B A A A −B −B
x3 : −B −A B A −−
x4 : −−−A A B −−
x5 : −A −B A B B A.
Have you been able to estimate all the parameters?
3.21. In the situation of Example 3.14 let the only Viterbi paths for xj, j =
1, 2, 3, 4, 5, 6 be
π∗(x1) : 0 D1I1I1M2D3I3 0
π∗(x2) : 0 M1D2I2I2M3 0
π∗(x3) : 0 M1D2M3 0
π∗(x4) : 0 I0D1D2D3I3 0
π∗(x5) : 0 M1M2M3 0
π∗(x6) : 0 M1M2D3I3 0.
Find all multiple alignments of the sequences arising from these paths.

4
Protein Folding
One of the most important problems in molecular biology is the protein fold-
ing problem: given the amino acid sequence of a protein, what is the protein’s
structure in three dimensions? This problem is important since the struc-
ture of a protein provides a key to understanding its biological function. In
this chapter we will only slightly touch on the problem. For a more in-depth
discussion see, e.g., the surveys [N], [DC].
4.1 Levels of Protein Structure
It is widely assumed that the amino acid sequence contains all information
about the native three-dimensional structure of a protein molecule under
given physiological conditions. One reason to believe this is the following
thermodynamic principle established in the 1950’s by Christian Anﬁnsen’s
denaturation-renaturation experiments on ribonuclease. If one changes the
values of environmental parameters such as temperature or solvent conditions,
the protein will undergo a transition from the native state to an unfolded state
and become inactive. When the parameter values are reset to the values cor-
responding to the physiological conditions, the protein refolds and becomes
active again. This thermodynamic principle was later conﬁrmed by similar
experiments on other small globular proteins.
Determining the tree-dimensional structure of a protein is a very hard
task. The most reliable results are produced by experimental approaches such
as nuclear magnetic resonance (NMR) and X-ray crystallography. However,
such approaches are expensive and can require years to produce the structure
of a single protein. Therefore the number of known protein sequences is much
larger than the number of known three-dimensional protein structures, and
this gap constantly grows as a result of various sequencing projects. Thus,
mathematical, statistical and computational methods that may give some
indication of structure (and as a result, function) are becoming increasingly
important.

76
4 Protein Folding
There are diﬀerent levels of protein structure.
(i) Primary Structure: the amino acid sequence of a protein.
(ii) Secondary Structure: local regular structures commonly found within pro-
teins. There are two main types of such structures: α-helices and β-sheets.
In an α-helix amino acids are arranged into a right-handed spiral with 3.6
residues per turn. An α-helix is traditionally pictured either as a ribbon (see
Fig. 4.1) or as a cylinder.
Fig. 4.1.
A β-sheet consists of two or more β-strands connected by hydrogen bonds.
Atoms in a β-strand are arranged in a particular way as shown in Fig. 4.2.
Fig. 4.2.
If the orientations of all β-strands in a β-sheet coincide, it is called parallel,
if the orientations alternate, it is called antiparallel, and otherwise it is called
of mixed type. A β-sheet is pictured as a collection of oriented strips, where
each of the strips represents a single β-strand. Figure 4.3 shows a simple
two-strand antiparallel β-sheet.

4.1 Levels of Protein Structure
77
Fig. 4.3.
Other secondary structure elements are less pronounced. There are, for
example, turns and loops.
(iii) Super-Secondary Structure or Motif: local folding patterns built up from
particular secondary structures. Examples are the EF-hand motif that con-
sists of an α-helix, followed by a turn, followed by another α-helix, the β-helix
motif that consists of three β-sheets, coiled coil motifs that consist of two or
more α-helices wrapped around each other. Figure 4.4 shows a protein con-
taining two β-barrels, which are “closed” β-sheets (PDB ID: 1CBI, [TBB]).
Fig. 4.4.
(iv) Fold: the three-dimensional structure of an independently folding frag-
ment of the polypeptide chain that often has an associated function; such
fragments are called domains. A protein can have several folds connected by
loops. Figure 4.5 shows a two-domain protein (PDB ID: 1QRJ, [KS]).
(v) Tertiary Structure: the full three-dimensional structure of a protein. Fig-
ure 4.6 shows the tertiary structure of sperm whale myoglobin (PDB ID:
108M, [S]).

78
4 Protein Folding
Fig. 4.5.
Fig. 4.6.
(vi) Quaternary Structure: an arrangement of several protein chains in space.
Figure 4.7 shows the bovine mitochondrial Cytochrome Bc1 complex that
consists of 11 chains (PDB ID: 1BE3, [IJ]).
Fig. 4.7.

4.3 Threading
79
In these terms the protein folding problem can be stated as follows: pre-
dict the secondary, super-secondary, tertiary, quaternary structures and folds
from the primary structure. In practice, one can only hope to predict sec-
ondary structure, motifs and in some cases folds and tertiary structures. In
the following sections we will discuss several methods used for prediction.
4.2 Prediction by Proﬁle HMMs
To predict secondary structure elements, motifs or folds from the primary
structure one can directly use proﬁle HMMs as deﬁned in Sect. 3.8. To con-
struct a proﬁle HMM, one has to start with a family of aligned sequences. If
one aims at predicting a particular structural feature, then as the sequence
family a collection of protein sequences that contain the feature must be taken.
The sequences are aligned and a proﬁle HMM is derived from the alignments
as in Sect. 3.8. For a query protein sequence x search is performed with the
resulting HMM as was described in Sect. 3.2. If P(x, π∗) or P(x) is greater
than a certain cutoﬀ, the feature is reported in the sequence x. We also men-
tion that other learning machines such as neural networks can be used in a
similar way instead of HMMs.
This
principle
is
implemented
in
the
web-based
resource
Pfam
(see http://www.sanger.ac.uk/Pfam/) that concentrates on fold prediction.
Pfam contains over 2000 folds and for each of them a proﬁle HMM has been
estimated from a family of proteins containing the fold. A query sequence is
run past each of these HMMs. If a part of the sequence has probability greater
than a certain cutoﬀfor a particular HMM, the corresponding domain is re-
ported in the sequence. For more information the reader is referred to the
Pfam web site.
4.3 Threading
Threading is an approach to the protein folding problem based on the obser-
vation that many protein folds in existing databases are very similar. As a
result of this, it has been conjectured that there are only a limited number of
folds in nature. Some predict that there are fewer than 1000 diﬀerent folds.
Hence it is natural to attempt to determine the structure of a query sequence
by ﬁnding the “optimal ﬁt” of fragments of the sequence to some folds in a
library of folds, that is, to “thread” it through known structures.
There are many ways to do threading, and we will only describe one of
them. Threading can be done by aligning sequences to structures. Suppose
we have an amino acid sequence x and a structure S. We have to deﬁne
what aligning x to S means and how one can score such an alignment. A
straightforward approach would be to choose an amino acid sequence y that
is known to give rise to the structure S and align x to y. Such an alignment can

80
4 Protein Folding
be scored by using standard PAM or BLOSUM matrices described in Chap. 9.
The major problem with this method is that similar structures have signiﬁcant
sequence variability, so it is not clear which sequence y corresponding to the
structure S one should choose. Selecting diﬀerent sequences y may produce
signiﬁcantly diﬀerent alignments of x to S.
We will now describe a more realistic approach to threading that is some-
times called the environmental template method. The idea behind it is that
instead of aligning a sequence to a sequence, one aligns a sequence to a string
of descriptors that describe the environment of each residue in the structure
S. Speciﬁcally, for each residue in S one determines how buried it is, how
polar/hydrophobic its environment is, and what secondary structure it is in.
Figure 4.8 shows six burial/polarity environments introduced in [BLE]: B1
(buried, hydrophobic), B2 (buried, moderately polar), B3 (buried, polar), P1
(partially buried, moderately polar), P2 (partially buried, polar), E (exposed).
Fig. 4.8.
In addition to the six environments each residue in S is categorized into
three classes corresponding to the secondary structure it is in: α (for α-helices),
β (for β-sheets) and γ (for all other secondary structures). In total there are
18 environmental descriptors. We denote the collection of these descriptors by
C.
One can now convert the known structure S into a string: S = e1, . . . , en,
where ej are elements of C. To align x to this string a scoring scheme is
required. A scoring matrix is obtained in the following way. For a from the
amino acid alphabet and e ∈C set
s(a, e) = log
pa e
pa

,

4.3 Threading
81
where pa e and pa are derived from a database of known protein structures:
pa e is the frequency of the occurrence of the amino acid a in the environment
e in the database and pa is the frequency of the occurrence of a anywhere in
the database. We remark that this scoring matrix is produced in the spirit
of the general approach to constructing scoring matrices discussed in Sect.
9.1 (note that pa e can be treated as the conditional probability of a given
e). Figure 4.9 shows a scoring matrix which is a slight improvement of that
derived in [BLE].
Fig. 4.9.
We note that assigning a residue to a particular burial/polarity class is a
non-trivial task and further improvements in the scoring matrix can be made

82
4 Protein Folding
by obtaining more precise estimates on the buried area and polarity of residues
in known structures.
When x is aligned to S, gaps in x are allowed. Therefore one also has
to select a model for aligning gap regions to sequences of the elements of
C. Usually smaller gap penalties are selected for the α- and β-environments.
The rationale for this strategy is that insertions and deletions are less likely
to occur in regions of regular secondary structure. The gap model is usually
either linear or aﬃne. Similarly, gaps in S may be allowed, in which case one of
the standard gap models is selected to score gap regions aligned to a sequence
of amino acid.
Once a scoring system has been selected, one can ﬁnd all optimal local
alignments between x and S. This can be done by applying either the Smith-
Waterman algorithm (see Sect. 2.3) or a faster heuristic algorithm. Aligning
x to all structures in a database, one obtains overall optimal local alignments
that hopefully characterize the three-dimensional structures of the correspond-
ing subsequences in x. Of course, as with any sequence alignment procedure,
one also must assess the statistical signiﬁcance of the score.
4.4 Molecular Modeling
We will now turn to a biophysical method that attempts to predict the struc-
ture of a protein ab initio, that is, by trying to apply laws of physics to describe
the protein molecule, rather than by using databases of known structures. Due
to a large number of atoms involved, it is very hard to give a complete mathe-
matical description of a protein molecule, including both quantum mechanical
and relativistic eﬀects. Therefore, in applications the classical approach is uti-
lized. Speciﬁcally, we will attempt to model the potential energy of a protein
molecule under normal physiological conditions, as well as the energy of the
surrounding media. As we will see, the energy function depends on a num-
ber of parameters whose values determine the three-dimensional structure of
the molecule. Under this approach, it is assumed that the native state of the
molecule is given by those parameter values that minimize the energy function
(assuming that a point of minimum exists). Note that the energy function may
have multiple minima; in this case the molecule is assumed to have multiple
native states, which indeed occurs in reality.
To convey the general ideas of the approach, we will only discuss one rather
simplistic way to model the potential energy function of a protein molecule.
For more sophisticated energy functions and more detailed discussion of mole-
cular modeling of proteins see the survey paper [N]. Energy functions take into
account all atoms in a protein molecule. Since the number of amino acids in
proteins ranges from 25 to 3000 and the number of atoms ranges from around
500 to more than 10000, dealing even with the simplest energy functions may
be a diﬃcult computational task.

4.4 Molecular Modeling
83
The potential energy function that we present here is broken up into two
parts: the bonded interaction component and non-bonded interaction compo-
nent. The bonded interaction component itself consists of three parts corre-
sponding to bonds, bond angles and dihedral angles as shown in Fig. 4.10,
where circles represent atoms and segments represent bonds.
Fig. 4.10.
All bonds are modeled as harmonic oscillators (springs), and therefore the
bond energy is given by
EB =

i
1
2KBi(ri −r0
i )2,
where for the ith bond, KBi is the spring constant, ri is the length of the
bond, r0
i is its equilibrium length, and the summation is taken over all bonds.
Bond angles are also modeled as harmonic oscillators, and therefore the
bond angle energy is given by
EBA =

j
1
2KAj(θj −θ0
j)2,
where for the jth angle, KAj is the spring constant, θj is the size of the angle,
θ0
j is its equilibrium size, and the summation is taken over all bond angles.
It is natural to model the dihedral angle energy by using a periodic func-
tion. We allow this energy for each dihedral angle to have multiple minima
which corresponds to potential multiple barriers encountered when rotating
the molecule by 2π around the relevant bond. Hence one way to model the
dihedral angle energy is as follows
EDA =

k
1
2KDk(1 + cos(nkχk −χ0
k)),
where for the kth dihedral angle, KDk is the dihedral energy barrier, χk is
the size of the angle, nk is introduced to allow for multiple minima, χ0
k is the
phase angle, and the summation is taken over all dihedral angles.

84
4 Protein Folding
The non-bonded interaction component consists of two parts: Van der
Waals energy and electrostatic energy.
The Van der Waals energy between two atoms is given in terms of an
attractive interaction and repulsive interaction as follows
EVDW =

i<j

Aij
r12
ij
−Bij
r6
ij

,
where the summation is taken over all pairs of atoms, and for the (i, j)th
pair rij is the distance between the atoms, Aij is a constant associated with
the repulsion between the atoms, and Bij is a constant associated with the
attraction between the atoms.
The electrostatic energy arises from the Coulomb law
Eelec =

i<j
qiqj
εr2
ij
,
where, as before, the summation is taken over all pairs of atoms i, j, rij is
the distance between the atoms, qi, qj are the electrostatic charges on the
atoms and ε is the dielectric constant of the surrounding medium in which
the protein is being modeled.
Combining all the above terms we can calculate the total potential energy
of the molecule as
Etotal = EB + EBA + EDA + EVDW + Eelec.
The constants appearing in the terms of Etotal are determined based on experi-
ments with smaller molecules from which the molecules under study are built.
However, there is usually not enough experimental information to completely
determine the constants, and therefore certain ab initio quantum mechanical
calculations are done as well.
So far we have modeled the energy function in the absence of any ex-
ternal factors like solvents. In practice, however, it is important to take into
account the medium surrounding the protein molecule, usually water (in the
case of globular proteins). One approach is to randomly distribute thousands
of water molecules around the protein and to explicitly take into account the
contributions made by the atoms that make up these molecules to the po-
tential energy function. This way we obtain the ﬁnal formula for the energy.
This approach, however, is computationally expensive because of a large num-
ber of additional atoms. Therefore, simpliﬁed ways to model the surrounding
medium are usually utilized.
4.5 Lattice HP -Model
The complexity of the protein folding problem has led to various simpliﬁed
approaches that concentrate only on some known major factors that aﬀect

4.5 Lattice HP-Model
85
protein structure. One such major factor (in the case of globular proteins)
is the presence of hydrophobic and hydrophilic (also called polar) residues at
particular sites in the molecule. Hydrophobic amino acids tend to stay away
from water and therefore are buried in the core of a protein molecule, whereas
polar ones tend to be exposed to water and therefore stay close to the surface
of the protein. The structure of a protein molecule is very strongly aﬀected
by the proportions of hydrophobic and polar residues and their positions in
the sequence. The lattice HP-model (see, e.g., [Di], [DC], [LD], [CD], [LTW])
singles out this particular factor and attempts to deduce the structure of a
protein molecule solely from its hydrophobic/polar composition.
For the purposes of this model the amino acid sequence of a protein is
ﬁrst converted into a binary string, called the HP-sequence of the amino
acid sequence. Namely, each amino acid is classiﬁed as either hydrophobic
and replaced with H, or polar and replaced with P. Table 4.1 shows the
HP-classiﬁcation of amino acids.
Table 4.1.
Single letter code HP-code
A
H
R
P
N
P
D
P
C
H
Q
P
E
P
G
H
H
P
I
H
L
H
K
P
M
H
F
H
P
H
S
P
T
P
W
H
Y
H
V
H
Further, space is discretized into a three-dimensional unit lattice, and the
residues of a molecule are only allowed to occupy positions at the vertices of
the lattice in such a way that there is no more than one residue per vertex
and adjacent residues in the sequence occupy adjacent lattice points. Every
conﬁguration obtained in this way corresponds to a possible tertiary structure

86
4 Protein Folding
of the protein. There are, of course, many structures that can be obtained from
“threading” the amino acid sequence through the lattice. In order to choose
the ones that we believe may have something in common with the native
structure of the protein, an “energy function” E is introduced. Then, as in
the preceding section, optimal structures, that is, structures for which E takes
its minimal value, are taken as the native ones.
Many energy functions are considered. One popular choice is as follows:
E is equal to minus the number of adjacent HH-pairs in the lattice. Hence,
for this choice of E optimal structures are those for which the H residues are
packed most densely.
Example 4.1. Consider the following sequence of amino acids: IPTGEC. Its
HP-sequence is HHPHPH, and one possible conﬁguration for this sequence
in the lattice is shown in Fig. 4.11.
I
P
T
G
E
C
Fig. 4.11.
The value of E for this conﬁguration is -3. Figure 4.12 shows another
possible conﬁguration. The value of E for it is equal to -1. Therefore, from
the point of view of the lattice HP-model with the energy function deﬁned
above, the second conﬁguration is less optimal than the ﬁrst one.
Certainly, optimal structures found by the model depend on the choice of
energy function. Another frequently used function is as follows: E is equal to
minus the number of adjacent HH-pairs in the lattice that are not adjacent in
the sequence. Using this deﬁnition for the conﬁgurations shown in Figs. 4.11
and 4.12 we obtain E = −2 and E = 0 respectively; thus the conﬁguration in
Fig. 4.11 is more optimal for this choice of energy function as well.
Although many biophysicists work with the lattice HP-model, it is in
fact NP-complete which indicates that an algorithm for ﬁnding optimal con-
ﬁgurations in polynomial time may not exist [BL]. However, there are fast
algorithms that obtain structures with values of the energy function not ex-
ceeding particular fractions of the optimal value, for example, not exceeding

4.5 Lattice HP-Model
87
T
G
E
I
P
T
G
E
C
Fig. 4.12.
3/8 of the optimal energy value [HI]. There have also been many computer
simulations with this model.
I
P
T
G
E
C
Fig. 4.13.
Other lattice models are used as well; for instance, one popular simpliﬁca-
tion of the model considered above is the two-dimensional lattice HP-model,
where residues are allowed to occupy the vertices of a planar lattice. The two-
dimensional model is simpler, but, unfortunately, is also NP-complete [CY].
One possible two-dimensional conﬁguration for the amino acid sequence from
Example 4.1 is shown in Fig. 4.13. The value of the energy function used in
Example 4.1 for this conﬁguration is -1.

88
4 Protein Folding
Exercises
4.1. Let a protein structure S be converted into the following string of en-
vironmental descriptors: S = B1α P1γ B2β Eα P2β. Find all optimal local
alignments between the amino acid sequence x = WY V ARK and this string.
For this purpose use a suitably modiﬁed Smith-Waterman algorithm from
Section 2.3 with scoring matrix shown in Fig. 4.9 and the following linear
gap model: if e is an α- or β-environment, set s(−, e) = −50, otherwise set
s(−, e) = −10; for any amino acid a set s(a, −) = −5.
4.2. Write the HP-sequence for the amino acid sequence LASV EGAS and
ﬁnd all its optimal conﬁgurations for the three- and two-dimensional lattice
HP-models with energy function used in Example 4.1.

5
Phylogenetic Reconstruction
5.1 Phylogenetic Trees
Trees have been used in biology for a long time to graphically represent evo-
lutionary relationships among species and genes. A rooted tree by deﬁnition
descends in two directions from a single node called the root, bifurcates at
lower nodes and ends at terminal nodes called tips or leaves. The tips are
labeled by the names of the species or sequences being considered; the latter
are called operational taxonomic units (OTUs) or simply taxa.
A rooted tree represents evolution directed from the common ancestor of
all the OTUs (the root) towards the OTUs. The other internal nodes of the
tree represent the ancestors of particular groups of the OTUs. By removing
the root from a rooted tree and joining the two branches descending from
the root into a single branch, one obtains an unrooted tree. Such trees do
not contain information about the direction of evolution and specify only
evolutionary relationships among the OTUs. Figure 5.1 shows a rooted tree
for four mammalian species on the left and the corresponding unrooted tree
on the right.
Fig. 5.1.

90
5 Phylogenetic Reconstruction
Two representations are given for each tree. Note that the two internal
nodes of the rooted tree represent respectively the common ancestor of the
group {dolphin, whale, pig} and the common ancestor of the smaller group
{dolphin, whale}.
The length of each branch of a tree is (ideally) a positive number that
represents the degree of relatedness between the species or sequences corre-
sponding to the nodes at the endpoints of the branch and is often computed
as the product of the length of the time interval that historically separates
the species or sequences and a particular value of the evolutionary rate, which
attempts to take into account the fact that some species or genes evolve faster
than others (evolutionary rates will be formally introduced in Sect. 5.4). Note
that Fig. 5.1 gives only the correct branching pattern, not branch lengths.
Branch lengths are often shown as labels next to the corresponding branches
(see, e.g., Fig. 5.10). The branching pattern of a tree (without any reference to
the branch lengths, but with a ﬁxed assignment of the OTUs to the leaves) is
called the (labeled) tree topology. Trees as described above are called phyloge-
netic trees. Any phylogenetic tree whose leaves are labeled by particular OTUs
is said to relate the OTUs. In this chapter we will consider methods for iden-
tifying trees that relate species or sequences in an “optimal” or “likely” way.
These methods produce trees that we hope reﬂect, at least to some extent,
the real evolutionary relationships among the OTUs.
One should bear in mind that representing species or gene divergence by
means of a rooted phylogenetic tree contains the hidden assumption that the
divergence occurred “instantaneously” or, rather, that the product of the
period of time that divergence had taken and a “divergence rate” is small
compared to the branch lengths. In reality it may take a substantial amount
of time for divergence to manifest itself on the population level.
For N OTUs the number of topologies of all possible rooted trees that
relate the OTUs is (2N −3)!/(2N−2(N −2)!) and the number of topologies
of all possible unrooted ones is (2N −5)!/(2N−3(N −3)!) (see Exercise 5.1).
For example, there is only one unrooted topology for N = 3 (see Fig. 5.5) and
only three unrooted tree topologies for N = 4 (see Example 5.10). However,
as N grows, the number of topologies quickly becomes very large. Each rooted
tree has 2N −2 branches and each unrooted one 2N −3 branches. Hence from
every unrooted topology one can produce 2N −3 rooted ones by placing a
root on any of the topology branches.
Traditionally, trees were built from morphological similarities among the
OTUs in question. Figure 5.2 shows an early tree published in [H] in 1866
(note that this tree has three branches at the root, which does not agree
with our deﬁnition of a phylogenetic tree). Over the last few decades tree
building has been based on gene and protein sequences (which explains the
term phylogenetic). Sequence based methods are more sensitive than those
based on morphological similarities, since sequence divergence precedes species
divergence. One should also bear in mind that gene divergence can occur
not just because of speciation (in which case the diverged genes are called

5.1 Phylogenetic Trees
91
Fig. 5.2.
orthologues), but also because of gene duplication (in which case the diverged
genes are called paralogues). Therefore, in principle phylogenetic trees can be
built for families of gene sequences some of which come from the same organ-
ism. Hence it is natural to think of phylogenetic trees as gene trees, not species
trees. Figure 5.3 shows a rooted phylogenetic tree for 52 mammalian species
determined by considering the DNA sequences of the breast and ovarian can-
cer susceptibility gene 1 (BRCA1). The ﬁgure is reprinted with permission
from Nature (see [MS]). Copyright 2001 Macmillan Magazines Limited.

92
5 Phylogenetic Reconstruction
Fig. 5.3.
It frequently happens that choosing diﬀerent sets of sequences for the same
family of species results in diﬀerent gene trees, and even from a single dataset
multiple trees can be produced. However, historically there is only one se-
quence of events that had led to the formation of the species being studied,
and hence only one tree represents the true evolutionary relationships. The
problem of combining diﬀerent gene trees into a single “consensus tree” re-
lating the actual species is still largely open (see, e.g., [BHV]).
The process of building phylogenetic trees that are in some sense optimal
for a given set of OTUs is called phylogenetic reconstruction and involves the
following basic steps.

5.1 Phylogenetic Trees
93
(i) Choosing a family of homologous sequences as OTUs (see Sect. 2.1 for
a brief discussion of homology). One must attempt to select the sequences
in such a way that there is a suﬃciently strong “phylogenetic signal” in the
family. If the signal is weak (that is, if the sequences are extremely diverged),
the existing phylogenetic methods will still produce some tree, but it probably
will not be very informative (note that assessing the reliability of phylogenetic
trees is an important problem in its own right, but we do not discuss it in this
book – see, e.g., [F2] and a brief survey in [EG]). Choosing suitable OTUs is
a kind of art and is often constrained by the availability of sequence data for
particular organisms.
(ii) Putting the sequences into a multiple alignment and obtaining a reduced
multiple alignment by discarding the columns that contain gaps (discarding
the gaps, in fact, is not necessary and there are ways to incorporate them in
phylogenetic reconstruction, but here we will only discuss simpliﬁed methods
that produce trees from reduced multiple alignments). For example, suppose
that for the four species from Fig. 5.1 we are given the DNA sequences (of
course, in reality sequences are much longer)
pig:
GCTGCA
horse:
GCTGA
whale:
GTCC
dolphin: GCTCCC,
and suppose that by some method we have produced the following multiple
alignment for these sequences
pig:
G C T G C A
horse:
G C T G −A
whale:
G −T C C −
dolphin: G C T C C C.
Then, after discarding the columns containing gaps, we obtain the reduced
multiple alignment
pig:
G T G
horse:
G T G
whale:
G T C
dolphin: G T C.
(iii) Inferring a phylogenetic tree from the reduced multiple alignment. Usu-
ally, the hardest part of any such inference is the determination of tree topol-
ogy. In the example considered above it is clear that pig should be clustered
with horse and whale with dolphin, as in the unrooted topology shown in Fig.
5.1. The determination of branch lengths and a root position requires further
analysis (and in some cases additional information).

94
5 Phylogenetic Reconstruction
In reality, when the sequences involved are much longer and the number
of OTUs is much larger, the resulting reduced multiple alignment is analyzed
by a particular method of phylogenetic reconstruction. Such methods can be
classiﬁed into three groups:
(a) Parsimony methods,
(b) Distance methods,
(c) Probabilistic methods arising from the maximum likelihood approach.
Methods of each type make speciﬁc assumptions about evolution, and we
discuss each type separately in the forthcoming sections. We will see that
some of the methods concentrate on determining only tree topologies, whereas
others also produce branch lengths. Further, some of the methods give only
unrooted trees, whereas others also indicate where the roots could be. We
will also see methods that attempt to reconstruct ancestral sequences at the
root and internal nodes. We remark that methods of each type may produce
several trees from a single reduced multiple alignment. If there are more than
one optimal multiple alignments for the sequences of interest (and hence more
than one reduced multiple alignments), one should deal with each alignment
separately, and thus the number of phylogenetic trees for a single sequence
family can be quite large.
In reality methods of phylogenetic reconstruction are applied to either
DNA or protein sequences. However, all the procedures make sense for se-
quences of letters from any ﬁnite alphabet Q, and, as before, we will sometimes
use the artiﬁcial alphabets for illustration purposes.
5.2 Parsimony Methods
Parsimony methods ﬁnd rooted tree topologies, not branch lengths. They also
attempt to reconstruct ancestral sequences at the root and internal nodes.
Under this approach a total cost is calculated for each rooted tree topology,
and optimal topologies are deﬁned as the ones that have the smallest total
cost. They are called the most parsimonious topologies. Thus, the parsimony
approach is based on the assumption that sequences always evolve in the
“most economic way” in the sense that the total cost is minimized.
There are many speciﬁc cost functions. As an example, we will consider
the simplest one where unit cost is made for each substitution. For a ﬁxed
rooted topology, we assign sequences to the root and all internal nodes; the
length of these sequences is equal to that of the reduced multiple alignment.
For any such sequence assignment the cost is the sum of costs of all branches,
where the cost of the branch joining two nodes is the minimal number of
substitutions required to move from the sequence at one node to the sequence

5.2 Parsimony Methods
95
at the other one. The cost of the topology then is the minimal cost over all
such sequence assignments.
We will illustrate this method by the following example.
Example 5.1. Let Q be the three-letter alphabet {A, B, C}, and suppose that
we are given the following reduced multiple alignment of three sequences
x1 : A A B
x2 : A C B
x3 : C C B.
There are only three possible rooted topologies for three OTUs, and it is easy
to observe that each topology is most parsimonious for x1, x2, x3. Figure 5.4
shows the topologies together with all optimal sequence assignments. The cost
of each topology is equal to 2.
Fig. 5.4.
The cost calculation for a ﬁxed topology can be done by simple Fitch’s al-
gorithm. This algorithm is quick, and if the number N of OTUs is moderate, it
is realistic to compute the cost of each topology and then select the most par-
simonious ones. If, however, N is very large, doing so is a huge computational
task. In such cases a non-comprehensive search of the topology space may
be conducted, but with this approach some or all of the most parsimonious
topologies may be missed. As we will see later, the problem of searching the
topology space for large values of N is common for many phylogenetic meth-
ods. However, there is a speciﬁc algorithm for enumerating tree topologies
called the branch and bound algorithm that, when coupled with Fitch’s algo-
rithm, is guaranteed to ﬁnd all the most parsimonious trees in an acceptable
amount of time, even if N is quite large.

96
5 Phylogenetic Reconstruction
The branch and bound algorithm is only useful for the parsimony methods,
which makes them the fastest available methods of phylogenetic reconstruc-
tion. Despite the fact that they are perhaps the least accurate ones, they are
still sometimes used because of their computational speed. We do not concen-
trate on them here. The interested reader can ﬁnd the descriptions of Fitch’s
algorithm and the branch and bound algorithm in [DEKM].
5.3 Distance Methods
Distance methods reconstruct trees (rooted or unrooted, depending on the
method) from a set of pairwise distances between the sequences in a ﬁxed
reduced multiple alignment. In this section we do not explain how such dis-
tances are obtained and assume that they are given. One popular procedure
for producing distances, or, rather, certain substitutes for distances that we
call “pseudodistances”, will be discussed in Sect. 5.5.
We start with a formal deﬁnition.
Deﬁnition 5.2. Let M be a set and let d : M × M →R be a function. We
say that d is a distance function on M if
(i)
d(u, v) > 0 for all u, v ∈M, u ̸= v,
(ii) d(u, u) = 0 for all u ∈M,
(iii) d(u, v) = d(v, u) for all u, v ∈M,
(iv) the triangle inequality holds: d(u, v) ≤d(u, w) + d(w, v) for all
u, v, w ∈M.
A set with a distance function on it is called a metric space.
If d is a distance function on M, then, for u, v ∈M, the number d(u, v) is
called the distance between u and v. Any set M can be turned into a metric
space if we introduce a distance function on M by setting d(u, v) = 1 for all
u, v ∈M, u ̸= v, and d(u, u) = 0 for all u ∈M, but, of course, this distance
function is not very informative.
We will be interested in the special case of distance functions on a ﬁnite
set M = {x1, . . . , xN} of sequences (OTUs) for which we would like to build
a phylogenetic tree. Assume that a distance function d is deﬁned on M and
that d is biologically relevant, that is, it incorporates some information about
the degree of divergence among the sequences in M: for example, d(xi, xj) >
d(xk, xl), if xi and xj have diverged from their common ancestor more than
xk and xl have diverged from theirs. For simplicity we will write dij instead of
d(xi, xj). It will be convenient for us to represent d by the symmetric distance
matrix Md = (dij).
If we ﬁx an unrooted tree T relating the OTUs, we obtain a tree-generated
distance function dT on M by declaring dT (xi, xj) = dT
ij to be the length
of the shortest path from xi to xj in T . It is not hard to check that under

5.3 Distance Methods
97
very general assumptions dT is indeed a distance function on M (see Exercise
5.3). The broad aim of distance methods for phylogenetic reconstruction is
to determine all trees T for which dT is in some sense as close as possible
to d. Any such tree is considered to be optimal from the point of view of
distance methods. Thus, the emphasis of distance methods is on determining
unrooted trees together with branch lengths (although we will also describe
one distance method that produces rooted trees).
The following natural question arises: is any distance function d on M
additive, that is, does there exist a tree T that generates d, which means
that dT = d (dT
ij = dij for all i, j)? The answer to this question is obviously
positive for N = 2. Suppose now that N = 3. In this case we are looking for
three positive numbers x, y, z such that
x + y = d12,
x + z = d13,
y + z = d23
(5.1)
(see Fig. 5.5).
Fig. 5.5.
The solution to equations (5.1) is
x = 1
2(d12 + d13 −d23),
y = 1
2(d12 + d23 −d13),
z = 1
2(d13 + d23 −d12).
(5.2)
We note that the numbers in the right-hand side of identities (5.2) are non-
negative due to the triangle inequality. However, since this inequality is not

98
5 Phylogenetic Reconstruction
strict, they do not have to be positive, some of them may be equal to 0. Hence,
it will be convenient for us to allow zero branch lengths, and we will assume
that all branch lengths are non-negative, rather than positive numbers from
now on. In biological applications branches of zero length are interpreted as
“very short” branches. The deﬁnition of additivity in this case is identical to
the previously given one, and thus (5.2) shows that, if N = 3, any distance
function is additive on M in this generalized sense. If some of the branch
lengths of a tree T are zero, dT may not satisfy condition (i) of Deﬁnition 5.2,
and we will sometimes impose this condition separately (see, e.g., Exercise
5.3). Note also that if zero branch lengths are allowed, phylogenetic trees are
no longer required to have the bifurcating pattern discussed in the preceding
section, but may have any branching pattern at any of its internal nodes.
As we have seen, for N = 2, 3 there is a unique tree that generates a given
distance function. The uniqueness of such a tree is a general fact for additive
distance functions (see Exercise 5.4).
Later we will observe (see, e.g., Example 5.6), that for N ≥4 not every
distance function on M is additive. In fact, additive distance functions can
be characterized in the following way.
Theorem 5.3. Let d be a distance function on a M and N ≥4. Then d
is additive if and only if the following condition holds: for every set of four
distinct numbers 1 ≤i, j, k, l ≤N two of the sums dij +dkl, dik +djl, dil +djk
coincide and are greater than or equal to the third one.
The condition from Theorem 5.3 is called the four-point condition. This
condition is clearly necessary for additivity (see Exercise 5.5). The proof of
suﬃciency was given in [SN] (see also [SK]). The proof in [SN] is constructive
in the sense that it gives an explicit procedure for determining the (unique)
tree that generates d. This procedure is called the neighbor-joining algorithm.
It is an iterative algorithm that on every step replaces a pair of OTUs with
a single new OTU, and iterates until there are only three OTUs left. For
N = 3 there is just one unrooted tree topology, and the corresponding branch
lengths are found from formulas (5.2). The tree is then built by a traceback
procedure that works by recalling which single OTU on a particular step arose
from which pair of OTUs available on the preceding step.
We will now describe the algorithm in detail. For every i = 1, . . . , N deﬁne
ri =
1
N −2
N

k=1
dik.
Further, for all i, j = 1, . . . , N, i < j, set
Dij = dij −(ri + rj).
It will be convenient for us to write Dij into an upper-triangular matrix
D = (Dij). Now, pick a pair 1 ≤i, j ≤N for which Dij is minimal (such a

5.3 Distance Methods
99
pair may not be unique). We will now group together the OTUs xi, xj, that
is, replace them with a single element that we call the OTU xN+1. The new
OTU xN+1 represents an internal node of the future tree connected to xi and
xj, and is placed at the following distances from them respectively
dN+1 i = 1
2(dij + ri −rj),
dN+1 j = 1
2(dij + rj −ri).
We will now deﬁne the distances between xN+1 and any xm with m ̸= i, j as
follows
dN+1 m = 1
2(dim + djm −dij).
Now we have the new collection of N−1 OTUs M′ = {xm, xN+1, m ̸= i, j}
and can repeat the above procedure once again. The algorithm is iterated
until only three OTUs are left, in which case there is just one unrooted tree
topology, and the corresponding branch lengths are found from formulas (5.2).
The tree is then built by a traceback procedure.
We will now give an example of applying the neighbor-joining algorithm.
Example 5.4. Let N = 6 and suppose that we are given the following distance
matrix
Md x1 x2 x3 x4 x5 x6
x1
0
8
3 14 10 12
x2
8
0
9 10 6
8
x3
3
9
0 15 11 13
x4 14 10 15 0 10 8
x5 10 6 11 10 0
8
x6 12 8 13 8
8
0
It is not hard to verify that d is indeed a distance function. Formally, before
applying the neighbor-joining algorithm, we need to check that d satisﬁes the
four-point condition. However, we will apply the neighbor-joining algorithm
to d without doing such veriﬁcation. We will obtain a tree T and compare
the corresponding function dT with d. We will see that dT = d, and hence,
indeed, d satisﬁes the four-point condition.
We have
r1 = 47
4 , r2 = 41
4 , r3 = 51
4 ,
r4 = 57
4 , r5 = 45
4 , r6 = 49
4 .
This gives the following matrix

100
5 Phylogenetic Reconstruction
D x1
x2
x3
x4
x5
x6
x1
−14 −43
2
−12 −13 −12
x2
−14 −29
2 −31
2 −29
2
x3
−12 −13 −12
x4
−31
2 −37
2
x5
−31
2
In the above matrix the minimal value is D13 = −43/2. We now introduce a
new OTU x7 that will replace the pair x1, x3. We place x7 at the distance
d71 = 1
2(d31 + r1 −r3) = 1
from x1 and at the distance
d73 = 1
2(d31 + r3 −r1) = 2
from x3, as shown in Fig. 5.6.
Fig. 5.6.
We will now compute distances between x7 and each of x2, x4, x5, x6. We
have

5.3 Distance Methods
101
d72 = 1
2(d12 + d32 −d13) = 7,
d74 = 1
2(d14 + d34 −d13) = 13,
d75 = 1
2(d15 + d35 −d13) = 9,
d76 = 1
2(d16 + d36 −d13) = 11,
which gives the following distance matrix for the OTUs x2, x4, x5, x6, x7
Md x2 x4 x5 x6 x7
x2
0 10 6
8
7
x4 10 0 10 8 13
x5
6 10 0
8
9
x6
8
8
8
0 11
x7
7 13 9 11 0
For this new distance matrix we will repeat the process again and obtain
r2 = 31
3 , r4 = 41
3 , r5 = 11,
r6 = 35
3 , r7 = 40
3 ,
which gives
D x2
x4
x5
x6
x7
x2
−14 −46
3
−14 −50
3
x4
−44
3 −52
3
−14
x5
−44
3 −46
3
x6
−14
We now introduce a new OTU x8 that will replace the pair x4, x6 (note that
D46 is minimal in the above matrix). We place x8 at the distance 5 from x4
and at the distance 3 from x6 as shown in Fig. 5.7.

102
5 Phylogenetic Reconstruction
Fig. 5.7.
The distance matrix for the OTUs x2, x5, x7, x8 is
Md x2 x5 x7 x8
x2
0
6
7
5
x5
6
0
9
5
x7
7
9
0
8
x8
5
5
8
0
On the next step of the algorithm we obtain
r2 = 9, r5 = 10, r7 = 12, r8 = 9,
and
D x2
x5
x7
x8
x2
−13 −14 −13
x5
−13 −14
x7
−13
At this point we can group together either x2 and x7, or x5 and x8, since
both D27 and D58 are minimal in the above matrix (the resulting tree will
not depend on our choice). We group together x5 and x8, that is, we introduce
a new OTU x9, place it at the distance 3 from x5 and at the distance 2 from
x8 as shown in Fig. 5.8, and calculate distances from x9 to x2 and x7 which
gives the following distance matrix for the three OTUs
Md x2 x7 x9
x2
0
7
3
x7
7
0
6
x9
3
6
0
It follows from formulas (5.2) that the above distance function is generated
by the tree shown in Fig. 5.9.

5.3 Distance Methods
103
Fig. 5.8.
Fig. 5.9.
Next, merging the trees from Figs. 5.6–5.9, we obtain the tree T shown in
Fig. 5.10.
Fig. 5.10.
It is easy to verify that T generates d and therefore the distance function
d indeed satisﬁes the four-point condition.

104
5 Phylogenetic Reconstruction
In Example 5.4 all the branch lengths that we computed were positive
numbers. In fact, some branch lengths can be equal to zero for a distance
function satisfying the four-point condition, as the following example shows.
Example 5.5. Let N = 5 and suppose that we are given the following distance
matrix
Md x1 x2 x3 x4 x5
x1
0 11 8
9
8
x2 11 0 13 14 13
x3
8 13 0
9
8
x4
9 14 9
0
9
x5
8 13 8
9
0
It is not hard to verify that d is indeed a distance function. From the
matrix Md we obtain
r1 = 12, r2 = 17, r3 = 38
3 ,
r4 = 41
3 , r5 = 38
3 ,
and
D x1
x2
x3
x4
x5
x1
−18 −50
3 −50
3 −50
3
x2
−50
3 −50
3 −50
3
x3
−52
3 −52
3
x4
−52
3
Since D12 is minimal in the above matrix, we replace the pair x1, x2 with a
single new OTU x6 that we place at the distance 3 from x1 and at the distance
8 from x2.
The distance matrix for the OTUs x3, x4, x5, x6 is as follows
Md x3 x4 x5 x6
x3
0
9
8
5
x4
9
0
9
6
x5
8
9
0
5
x6
5
6
5
0

5.3 Distance Methods
105
and therefore we obtain
r3 = 11, r4 = 12, r5 = 11, r6 = 8,
and
D x3
x4
x5
x6
x3
−14 −14 −14
x4
−14 −14
x5
−14
At this point we can group together any pair of OTUs, say, x4, x5. A new
OTU x7 is placed at the distance 5 from x4 and at the distance 4 from x5.
The distance matrix for the OTUs x3, x6, x7 is
Md x3 x6 x7
x3
0
5
4
x6
5
0
1
x7
4
1
0
and formulas (5.2) give for these OTUs the tree shown in Fig. 5.11 (note that
Fig. 5.11.
the length of the branch leading to x7 is equal to 0).
Then the tree T produced by the neighbor-joining algorithm is as shown
in Fig. 5.12.
Fig. 5.12.

106
5 Phylogenetic Reconstruction
It is easy to verify that T generates d and therefore the distance function
d indeed satisﬁes the four-point condition.
Often the neighbor-joining algorithm is applied in the situation when the
distance function in question does not satisfy the four-point condition. This
is done because distance functions obtained in reality almost never satisfy it.
Moreover, in practice the triangle inequality (see (iv) in Deﬁnition 5.2) is hard
to satisfy either, and what one usually deals with in reality is a pseudodistance
function, rather than a distance function. The deﬁnition of pseudodistance
function is obtained from Deﬁnition 5.2 by removing requirement (iv), and,
as before, for a set of OTUs we will usually write a pseudodistance function
d as the corresponding pseudodistance matrix Md = (dij).
If the neighbor-joining algorithm is formally applied to a pseudodistance
function that does not satisfy the four-point condition, various anomalies may
occur: the algorithm may produce more than one tree, these trees may have
branches of negative lengths (note that biologists are sometimes able to in-
terpret negative branch lengths), the functions dT obtained from these trees
may not coincide with the original pseudodistance function d.
We illustrate these anomalies by the following example (see also Exercise
5.7). Before proceeding, we note that pseudodistance functions may lead to
negative branch lengths even for N = 3, which can be seen from formulas
(5.2).
Example 5.6. Let N
= 4 and suppose that we are given the following
pseudodistance matrix
Md x1 x2 x3 x4
x1
0
5
2
7
x2
5
0
1
a
x3
2
1
0
3
x4
7
a
3
0
where a > 6. The function d does not satisfy the triangle inequality since
d14 > d13 + d34. It does not satisfy the four-point condition either because
d13 + d24 = 2 + a > 8, d12 + d34 = 8, d14 + d23 = 8.
Let us apply the neighbor-joining algorithm to the pseudodistance function
d and see what it will produce. We have
r1 = 7, r2 = 3 + a
2, r3 = 3, r4 = 5 + a
2,
and

5.3 Distance Methods
107
D x1
x2
x3
x4
x1
−5 −a
2
−8 −5 −a
2
x2
−5 −a
2
−8
x3
−5 −a
2
Since a > 6, each of D12, D14, D23, D34 is minimal. As we will see, grouping
together diﬀerent pairs of OTUs will lead in some cases to diﬀerent trees.
First, let us group together x1, x2, and introduce a new OTU x5 that we
place at the distance 9/2 −a/4 from x1 and at the distance 1/2 + a/4 from
x2 (note that the ﬁrst of these branch lengths is negative if a > 18). The new
“pseudodistance matrix” for x3, x4, x5 is as follows
Md x3
x4
x5
x3
0
3
−1
x4
3
0
1 + a
2
x5 −1 1 + a
2
0
The above matrix is not in fact that of a pseudodistance function since some
of its entries are negative. However, we can formally apply formulas (5.2) to
it to obtain the tree shown in Fig. 5.13 (note that one of the branch lengths
is negative).
x
x
x
3
4
5
a/4-3/2
5/2+a/4
1/2-a/4
Fig. 5.13.
This tree gives rise to the tree T1 shown in Fig. 5.14.

108
5 Phylogenetic Reconstruction
Fig. 5.14.
Although some of the branches of T1 have negative lengths, we can still
attempt to sum up the relevant branch lengths and calculate dT1. The resulting
matrix is
MdT1
x1
x2
x3
x4
x1
0
5
7
2 −a
4
11
2 + a
4
x2
5
0
a
4 −1
2
3
2 + 3a
4
x3
7
2 −a
4
a
4 −1
2
0
3
x4
11
2 + a
4
3
2 + 3a
4
3
0
which is clearly diﬀerent from Md.
If we group together x3, x4, it is easy to see that we will again obtain the
tree T1 in Fig. 5.14.
Let us now group together x1, x4. We introduce a new OTU x5 and place
it at the distance 9/2 −a/4 from x1 and at the distance 5/2 + a/4 from x4.
The new “pseudodistance matrix” for x2, x3, x5 is as follows
Md
x2
x3
x5
x2
0
1
a
2 −1
x3
1
0
−1
x5 a
2 −1 −1
0

5.3 Distance Methods
109
Again, the above matrix is not in fact that of a pseudodistance function since
some of its entries are negative. However, as we have already done earlier, we
can formally apply formulas (5.2) to it to obtain the tree shown in Fig. 5.15.
Fig. 5.15.
The above tree gives rise to the tree T2 shown in Fig. 5.16.
Fig. 5.16.
Although some of the branches of T2 have negative lengths, we, as before,
can still attempt to sum up the relevant branch lengths and calculate dT2.
The resulting matrix is

110
5 Phylogenetic Reconstruction
MdT2
x1
x2
x3
x4
x1
0
7
2 + a
4
7
2 −a
4
7
x2
7
2 + a
4
0
1
3
2 + 3a
4
x3
7
2 −a
4
1
0
3
2 + a
4
x4
7
3
2 + 3a
4
3
2 + a
4
0
which is diﬀerent from Md as well.
Finally, if we group together x2, x3, we again obtain the tree T2 in Fig.
5.16. Thus, the neighbor-joining algorithm in this example produces two trees,
and some of their branch lengths are negative. Neither of these trees generates
the original pseudodistance function.
We will now introduce a special class of distance functions. A distance
function d on a set M of OTUs is called ultrameric, if for any three distinct
elements xi, xj, xk ∈M, two of the distances dij, dik, djk coincide and are
greater than or equal to the third one. It can be checked directly that an
ultrameric distance function satisﬁes the four point condition (see Exercise
5.8). Therefore, any ultrameric distance function is additive and the tree that
generates it can be recovered by the neighbor-joining algorithm.
We will now give an example of applying the neighbor-joining algorithm
to an ultrameric distance function.
Example 5.7. Let N = 5 and suppose that we are given the following distance
matrix
Md x1 x2 x3 x4 x5
x1
0 16 6 16 6
x2 16 0 16 8 16
x3
6 16 0 16 2
x4 16 8 16 0 16
x5
6 16 2 16 0
It is easy to verify directly that d is indeed a distance function and is
ultrameric (see Exercise 5.9). From the matrix Md we obtain
r1 = 44
3 , r2 = 56
3 , r3 = 40
3 ,
r4 = 56
3 , r5 = 40
3 ,

5.3 Distance Methods
111
and
D x1
x2
x3
x4
x5
x1
−52
3 −22 −52
3
−22
x2
−16 −88
3
−16
x3
−16 −74
3
x4
−16
Since D24 is minimal in the above matrix, we replace the pair x2, x4 with a
single new OTU x6 that we place at the distance 4 from each of x2 and x4.
The distance matrix for the OTUs x1, x3, x5, x6 is as follows
Md x1 x3 x5 x6
x1
0
6
6 12
x3
6
0
2 12
x5
6
2
0 12
x6 12 12 12 0
and therefore we obtain
r1 = 12, r3 = 10, r5 = 10, r6 = 18,
and
D x1
x3
x5
x6
x1
−16 −16 −18
x3
−18 −16
x5
−16
We now group together x1, x6. A new OTU x7 is placed at the distance 3
from x1 and at the distance 9 from x6.
The distance matrix for the OTUs x3, x5, x7 is
Md x3 x5 x7
x3
0
2
3
x5
2
0
3
x7
3
3
0
and formulas (5.2) give for these OTUs the tree shown in Fig. 5.17.

112
5 Phylogenetic Reconstruction
Fig. 5.17.
Then the tree T produced by the neighbor-joining algorithm is as shown
in Fig. 5.18.
Fig. 5.18.
The tree T found in Example 5.7 possesses an interesting property: there
is a way to place a root on this tree so that for any node the lengths of all
paths descending from this node to the tips lying below the node are equal.
This root position in shown in Fig. 5.19.
Fig. 5.19.

5.3 Distance Methods
113
Rooted trees satisfying the above property describe the evolution of species
or sequences whose evolutionary rates do not change through time, and hence
the branch lengths are proportional to the real periods of time with the pro-
portionality coeﬃcient being independent of the branch. This condition is
called the molecular clock condition and trees satisfying this condition are
called molecular clock trees. It should be noted that the evolution of any fam-
ily of species or sequences can be represented by a molecular clock tree, if one
sets the branch lengths to be equal to the corresponding time intervals elapsed
between various separation events. In practice, however, branch lengths are
always computed by scaling the time intervals, which reﬂects the observa-
tion that evolution along diﬀerent branches often occurs with diﬀerent rates.
Therefore, in biology molecular clock trees are usually reserved to represent
the evolution of species or sequences with constant evolutionary rates.
If T is a molecular clock tree, then dT is an ultrameric distance function
under very general assumptions (see Exercise 5.10). Conversely, any ultra-
meric distance function is generated by a molecular clock tree. The proof of
this statement can be derived from the arguments in [SN], [SK] that show
that if one applies the neighbor-joining algorithm to an ultrameric distance
function, then the resulting tree can be turned into a molecular clock tree
by appropriately placing a root, just as we did in Example 5.7. However, for
an ultrameric distance function there is a simpler algorithm that recovers the
corresponding molecular clock tree. It is called the Unweighted Pair Group
Method Using Arithmetic Averages or UPGMA [SM].
We will now describe the UPGMA algorithm. Suppose we are given an
ultrameric distance function d on a set M = {x1, . . . , xN} of OTUs. We place
the OTUs at height 0 and will build the tree from bottom to top by introducing
new OTUs representing the interior nodes and the root of the future tree, and
placing them at particular heights. UPGMA combines the OTUs in clusters.
If Ci and Cj are two clusters of OTUs from M, deﬁne the distance between
them as
d(Ci, Cj) =
1
N(Ci)N(Cj)

a∈Ci, b∈Cj
dab,
(5.3)
where N(Ci) and N(Cj) denote the numbers of OTUs in the clusters Ci and
Cj respectively.
Assign initially each OTU xi to its own single-element cluster Ci; we say
that xi is associated with Ci. Next, choose two clusters Ci and Cj for which
d(Ci, Cj) is minimal. Deﬁne a new cluster CN+1 = Ci ∪Cj and set the
distances from CN+1 to the remaining clusters by formula (5.3). Introduce
now a new OTU xN+1 and place it at the total height d(Ci, Cj)/2 above xi
and xj. The OTU xN+1 is associated with the cluster CN+1 and represents an
interior node of the future tree connected to xi and xj. We replace the pair xi,
xj with xN+1 and set the distance between xN+1 and any other OTU as the
distance between the associated clusters. Thus we now have N −1 clusters,
for which we repeat the above procedure. We iterate the algorithm until only

114
5 Phylogenetic Reconstruction
two clusters remain, say, Cm associated with an OTU xm and Cl associated
with an OTU xl. We then place the root of the tree above xm and xl at the
total height d(Cm, Cl)/2.
We will now apply the UPGMA algorithm to the distance function from
Example 5.7.
Example 5.8. Since d35 is minimal, we form a new cluster C6 as C6 = {x3, x5}.
From formula (5.3) we obtain
d(C1, C6) = 1
2(d13 + d15) = 6,
d(C2, C6) = 1
2(d23 + d25) = 16,
d(C4, C6) = 1
2(d43 + d45) = 16.
We now introduce a new OTU x6 and place it at the height d35/2 = 1 above
x3 and x5, as shown in Fig. 5.20.
Fig. 5.20.
For the OTUs x1, x2, x4 and x6 we have the following distance matrix
Md x1 x2 x4 x6
x1
0 16 16 6
x2 16 0
8 16
x4 16 8
0 16
x6
6 16 16 0
Here d16 is minimal, and we form a new cluster C7 as C7 = {x1, x3, x5} (recall
that x6 is associated with the cluster C6 = {x3, x5}). From formula (5.3) we
obtain

5.3 Distance Methods
115
d(C2, C7) = 1
3(d21 + d23 + d25) = 16,
d(C4, C7) = 1
3(d41 + d43 + d45) = 16.
We now introduce a new OTU x7 and place it at the total height d16/2 = 3
above x1 and x6, as shown in Fig. 5.21.
Fig. 5.21.
For the OTUs x2, x4 and x7 we have the following distance matrix
Md x2 x4 x7
x2
0
8 16
x4
8
0 16
x7 16 16 0
Here d24 is minimal, and we form a new cluster C8 as C8 = {x2, x4}. From
formula (5.3) we obtain
d(C7, C8) =
1
3 × 2(d12 + d32 + d52 + d14 + d34 + d54) = 16.
We now introduce a new OTU x8 and place it at the height d24/2 = 4 above
x2 and x4, as shown in Fig. 5.22.

116
5 Phylogenetic Reconstruction
Fig. 5.22.
Finally, we place the root at the total height d(C7, C8)/2 = 8 above x7
and x8 which gives the tree shown Fig. 5.23.
Fig. 5.23.
This tree is identical to the tree in Fig. 5.19, except that it has a “squared”
shape which is the preferred shape for UPGMA.
We also remark that we did not need to check that d is ultrameric at the
beginning. We could simply apply the UPGMA algorithm to it and notice
that the resulting tree generates d.
If a distance function satisﬁes the four-point condition, but is not ultra-
meric, the UPGMA algorithm will produce a wrong tree (that is, a tree diﬀer-
ent from the one produced by the neighbor-joining algorithm), since it always
ﬁnds a molecular clock tree. Even the topology of the tree derived by UPGMA
may be wrong. We will illustrate this eﬀect by the following example (see also
Exercise 5.13).
Example 5.9. Let N = 4 and suppose that we are given the following distance
matrix

5.3 Distance Methods
117
Md x1 x2 x3 x4
x1
0
3
9
9
x2
3
0 10 8
x3
9 10 0 16
x4
9
8 16 0
It is easy to check that d is indeed a distance function, that it satisﬁes the
four-point condition, and that it is not ultrameric (see Exercise 5.12).
We will ﬁrst apply the UPGMA algorithm to d. Since d12 is minimal, we
form a new cluster C5 as C5 = {x1, x2}. The new OTU x5 associated with
C5 is placed at the height 3/2 above x1, x2, and the distance matrix for the
OTUs x3, x4, x5 is as follows
Md x3 x4 x5
x3
0 16 19
2
x4 16 0 17
2
x5 19
2
17
2
0
Here d45 is minimal, and we form a new cluster C6 = {x1, x2, x4} and associate
with it a new OTU x6 that we place at the total height 17/4 above x4 and x6.
The distance from C3 to C7 is 35/3, and we therefore place the root above x3
and x7 at the total height 35/6. The resulting tree T1 is shown in Fig. 5.24.
It is easy to observe that dT1 ̸= d.
Fig. 5.24.
Let us now apply the neighbor-joining algorithm to d. We obtain
r1 = r2
21
2 , r3 = 35
2 , r4 = 33
2 ,

118
5 Phylogenetic Reconstruction
and
D x1
x2
x3
x4
x1
−18 −19 −18
x2
−18 −19
x3
−18
Since D13 is minimal, we group together the OTUs x1, x3, and introduce a
new OTU x5 that replaces them and that is placed at the distance 1 from x1
and at the distance 8 from x3. The distance matrix for the OTUs x2, x4, x5
is as follows
Md x2 x4 x5
x2
0
8
2
x4
8
0
8
x5
2
8
0
and from formulas (5.2) we obtain for them the tree shown in Fig. 5.25.
Fig. 5.25.
Hence, the tree T2 produced by the neighbor-joining algorithm is as shown
Fig. 5.26.
Fig. 5.26.
Observe that the tree T2 and the unrooted variant of the tree T1 are very
diﬀerent. Not only the branch lengths do not match, but the topologies do
not coincide either.
UPGMA, as the neighbor-joining algorithm, is often applied in the situ-
ation when d does not satisfy either the triangle inequality or the four point

5.3 Distance Methods
119
condition, that is, when d is just a pseudodistance function. We have seen
above that using the neighbor-joining method with a pseudodistance function
may lead to various anomalies (see Example 5.6). In contrast, if we use UP-
GMA with a pseudodistance function, we will always obtain a (not necessarily
unique) molecular clock tree with positive branch lengths, but the reliability
of such a tree may be equally low.
Certainly, if one applies either the neighbor-joining algorithm or UPGMA
to a pseudodistance function d and obtains a tree T0, one cannot in general
hope for the identity dT0 = d, but one may ask the natural questions: how
close is dT0 to d and is dT0 the closest function to d among all functions dT
calculated from all possible trees T relating the OTUs from M (possibly, even
allowing negative branch lengths)? To attempt to answer these questions one
ﬁrst has to deﬁne what “close” means, that is, to introduce a way of comparing
two pseudodistance functions. We will compare two pseudodistance functions
in the spirit of the sum of squares [C-SE] and its variants (see, e.g., [FM]).
For two pseudodistance functions d and d′ on the same set of OTUs M =
{x1, . . . , xN} deﬁne the sum of squares as
ϱ(d, d′) =

1≤i<j≤N
(dij −d′
ij)2.
We will be interested in the special case when d′ = dT , where an unrooted
tree T relates the OTUs from M. We set
ssd(T ) = ϱ(d, dT ).
There is a method of phylogenetic reconstruction associated with ssd called
the least squares method. With this method, for a given pseudodistance func-
tion d one looks for all unrooted trees T with the property that ssd(T ) is
minimal (assuming that a point of minimum of ssd exists in the space of all
trees). Every such a tree is optimal from the point of view of the least squares
method. Hence, the second question from the previous paragraph can be re-
formulated as follows: if T is a tree produced from a pseudodistance function
d by either the neighbor-joining or UPGMA algorithm, is it optimal from the
point of view of the least squares method? Certainly, if d is a distance func-
tion that respectively either satisﬁes the four-point condition or is ultrameric,
then the answer is positive. For general pseudodistance functions, however, it
is not always positive, as we will see in Example 5.10 below for the case of the
neighbor-joining algorithm (one can give an analogous example for the case
of the UPGMA algorithm as well – see Exercise 5.14).
The least squares method ideally requires minimizing ssd(T ) over all pos-
sible unrooted trees, but, of course, in real software packages only some un-
rooted topologies with some branch length assignments can be examined,
which reduces the sensitivity of the method. Branch lengths can be either
constrained to be non-negative numbers, or be allowed to be any real num-
bers. In Example 5.10 below we determine the (unique) optimal tree for the

120
5 Phylogenetic Reconstruction
pseudodistance function from Example 5.6. Since for N = 4 there are only
three unrooted topologies, we will be able to give an analytic solution to the
least square method in this case. Recall that the neighbor-joining method in
Example 5.6 produced trees where some of the branch lengths were negative;
therefore in order to compare the two methods, we do not constrain branch
lengths for the least square method in Example 5.10.
Example 5.10. We will consider three types of trees corresponding to the three
possible unrooted topologies.
Type I. A general tree TI(α, β, γ, δ, ε) of this type is shown in Fig. 5.27.
Fig. 5.27.
We have
ϕ(α, β, γ, δ, ε) = ssd

(TI(α, β, γ, δ, ε)

= (α + β −5)2 + (β + γ + δ −2)2
+(β + γ + ε −7)2 + (α + γ + δ −1)2 + (α + γ + ε −a)2 + (δ + ε −3)2.
We will attempt to ﬁnd the points of minimum of ϕ by determining its critical
points. Diﬀerentiating ϕ with respect to each of the ﬁve variables and setting
all partial derivatives to zero gives the following linear system of equations
3α + β + 2γ + δ + ε = 6 + a,
α + 3β + 2γ + δ + ε = 14,
2α + 2β + 4γ + 2δ + 2ε = 10 + a,
α + β + 2γ + 3δ + ε = 6,
α + β + 2γ + δ + 3ε = 10 + a.
The solution to this system is not hard to ﬁnd and we obtain
α = 1
2 + a
4, β = 9
2 −a
4, γ = a
4 −3
2,
δ = 1
2 −a
4, ε = 5
2 + a
4.

5.3 Distance Methods
121
It is easy to see that these values of branch lengths give the unique point of
minimum for ϕ, and the value of ϕ at this point is 4(3/2 −a/4)2. Observe
now that the resulting tree TI(α, β, γ, δ, ε) is precisely the tree T1 from Fig.
5.14 found in Example 5.6.
Type II. A general tree TII(α, β, γ, δ, ε) of this type is shown in Fig. 5.28.
Fig. 5.28.
It
can
be
proved
(see
Exercise
5.15)
that
ψ(α, β, γ, δ, ε)
=
ssd

(TII(α, β, γ, δ, ε)

is minimized by the tree T2 from Fig. 5.16 found in
Example 5.6, that the point of minimum is unique and that the minimal
value of ψ is also 4(3/2 −a/4)2.
Type III. A general tree TIII(α, β, γ, δ, ε) of this type is shown in Fig.
5.29.
Fig. 5.29.
We have
η(α, β, γ, δ, ε) = ssd

(TIII(α, β, γ, δ, ε)

= (β + γ + δ −5)2 + (α + β −2)2
+(β + γ + ε −7)2 + (α + γ + δ −1)2 + (δ + ε −a)2 + (α + γ + ε −3)2.

122
5 Phylogenetic Reconstruction
As above, we will attempt to ﬁnd the points of minimum of η by determining
its critical points. Diﬀerentiating η with respect to each of the ﬁve variables
and setting all partial derivatives to zero gives the following linear system of
equations
3α + β + 2γ + δ + ε = 6,
α + 3β + 2γ + δ + ε = 14,
2α + 2β + 4γ + 2δ + 2ε = 16,
α + β + 2γ + 3δ + ε = 6 + a,
α + β + 2γ + δ + 3ε = 10 + a.
The solution to this system is
α = −1,
β = 3,
γ = 3 −a
2,
δ = a
2 −1, ε = 1 + a
2.
It is easy to see that these values of branch lengths give the unique point of
minimum for η, and that the value of η at this point is equal to 0.
Hence, if negative branch lengths are allowed, the least squares method in
this example performs better than the neighbor-joining algorithm. The least
squares method ﬁnds a tree T (where some branch lengths are negative) for
which dT = d. In general, however, even if negative branch lengths are allowed,
one cannot hope to obtain a tree T such that ssd(T ) = 0, as in this example.
Often variants of the sum of squares are used. For example, the weighted
sum of squares introduced in [FM] is quite popular, and the corresponding
method of phylogenetic reconstruction is implemented in a program called
Fitch which is part of the software package PHYLIP [F3].
We will now discuss the probabilistic maximum likelihood approach to
phylogenetic reconstruction. It requires selecting an evolutionary model, and
therefore we will consider such models ﬁrst.
5.4 Evolutionary Models
Evolutionary models describe the substitution process in DNA, RNA and
amino acid sequences through time. In the next section they will be used
to model this process along the branches of a phylogenetic tree, and model
parameters will be allowed to change from branch to branch. We will concen-
trate on DNA sequences, since RNA and amino acid sequences are treated
similarly. Most currently available evolutionary models assume independence
among nucleotide sites, and it is therefore suﬃcient to describe evolution at a
single site. This is done by specifying a continuous-time ﬁnite Markov chain
or continuous-time ﬁnite Markov model depending on the time parameter
t ≥0. For the purposes of this exposition a continuous-time Markov model

5.4 Evolutionary Models
123
will be understood as a family of ordinary discrete-time Markov models, one
for each value of t ≥0, where the states are labeled by the letters of the
DNA alphabet Q = {A, C, G, T} and no constraints on the connectivity are
imposed in advance. The corresponding matrices of transition probabilities
will be written as
P(t) =
⎛
⎜
⎜
⎝
pAA(t) pAC(t) pAG(t) pAT (t)
pCA(t) pCC(t) pCG(t) pCT (t)
pGA(t) pGC(t) pGG(t) pGT (t)
pT A(t) pT C(t) pT G(t) pT T (t)
⎞
⎟
⎟
⎠.
In particular, for every value of t ≥0 each element in P(t) is non-negative
and the entries in each row sum up to 1. As we will explain below, there is
also a relation among the matrices P(t) for diﬀerent values of t. We further
assume that each of the discrete-time Markov models has the same vector of
initialization probabilities.
For comparison, we note that a discrete-time Markov chain with matrix of
transition probabilities P can also be regarded as a family of Markov chains
given by P n, n ∈N. For every n the matrix P n can be thought of as the
matrix of transition probabilities after n + 1 steps of the Markov chain (see
Sect. 6.11).
Modeling the substitution process at a nucleotide site is done by assuming
that P(t) gives the probabilities of all possible state changes in time t. Namely,
at any moment the site can be in one of the four possible states: A, C, G and
T, and the assumption is that the above matrix gives the probabilities of state
change in time t. For example, pAC(t) is the probability of the site changing
its state from A to C in time t.
If we suppose that the substitution process at the nucleotide site goes in
accordance with such a Markov chain, we in fact make the following
Assumption. If at some time t0 the site was in state i ∈{A, C, G, T},
then the probability of the event that at time t0 + t the site will be in state
j ∈{A, C, G, T} depends only on i, j and t (and is exactly the element pij(t)
of the matrix P(t)) .
This assumption leads to the following important observation. Consider
the probability pij(t + τ) for some t, τ ≥0. This is the probability of the site
going from state i to state j in time t+τ. Any such transition can be realized
by ﬁrst going from state i to any state k in time t and then going from state
k to state j in time τ. In accordance with the above assumption it is natural
to require that
pij(t + τ) =

k∈Q
pik(t)pkj(τ)
for all i, j. In the matrix form this identity can be written as follows

124
5 Phylogenetic Reconstruction
P(t + τ) = P(t)P(τ).
(5.4)
This identity is part of the deﬁnition of continuous-time Markov chain, and
we will always assume that it is satisﬁed.
We remark that there is an analogue of identity (5.4) for ordinary discrete-
time Markov chains as well. In this case (5.4) becomes the tautological identity
P n+m = P nP m,
n, m = 0, 1, 2 . . . .
We will only consider regular continuous-time Markov chains, which means
that P(0) is the identity matrix E and that P(t) is diﬀerentiable at every
t ≥0, that is, each element in the matrix P(t) is diﬀerentiable at every t ≥0
as a function of t (for t = 0 diﬀerentiability is understood as the existence of
one-sided derivatives).
We will need the following theorem.
Theorem 5.11. Under the above assumptions P(t) has the form
P(t) = exp(tQ),
(5.5)
where Q is some 4 × 4-matrix.
Of course, we have to deﬁne what the exponential of a matrix is.
Deﬁnition 5.12. Let A be a square m × m-matrix. Then exp(A) is deﬁned
to be the m × m-matrix given by the sum of the following series
exp(A) = E + A + A2
2! + A3
3! + ... =
∞

n=0
An
n! .
(5.6)
Identity (5.6) must be understood as a collection of m2 scalar identities, one
for each matrix element. Thus, the right-hand side of (5.6) consists of m2
series. It is possible to prove (see Exercise 5.16) that each of these series is
convergent for any matrix A.
Sketch of Proof of Theorem 5.11:
It follows from (5.4) that for t ≥0, h > 0 the following holds
P(t + h) −P(t)
h
= P(t)(P(h) −E)
h
= P(t)(P(h) −P(0))
h
.
When h →0 the above identity implies
P ′(t) = P(t)P ′(0),
which gives, as in the case of scalar functions (see Exercise 5.17), that
P(t) = exp(tQ),

5.4 Evolutionary Models
125
where Q = P ′(0).
Since in identity (5.5) we have Q = P ′(0), the matrix Q is sometimes called
the matrix of instantaneous change. One important property of Q is that the
elements in each row of Q sum up to 0 (see Exercise 5.18). By varying Q one
obtains all popular models. Four such models will be described below.
Before considering the speciﬁc models we will brieﬂy discuss two condi-
tions that continuous-time Markov chains describing the substitution process
in DNA are often required to satisfy.
(1) The uniqueness of a stationary probability distribution
Deﬁnition 5.13. A vector ϕ = (ϕA, ϕC, ϕG, ϕT ) with ϕi ≥0 and 
i∈Q ϕi =
1 is called a stationary probability distribution of a Markov chain if ϕ Q = 0.
Due to (5.5) this is equivalent to requiring that ϕ P(t) ≡ϕ.
The concept of stationary probability distribution also makes sense for
ordinary discrete-time Markov chains. In this case a vector ϕ with ϕi ≥0 and
N
i=1 ϕi = 1 is called a stationary probability distribution of a Markov chain
whose matrix of transition probabilities is P, if ϕ P = ϕ (see Exercise 5.19).
Any Markov chain (either continuous-time or discrete-time) possesses a
stationary probability distribution (see [Do]), but it may not be unique. For
example, for the trivial continuous-time Markov chain with Q = 0, any vector
ϕ with ϕi ≥0 and N
i=1 ϕi = 1 is a stationary probability distribution.
We will only consider continuous-time Markov chains for which a stationary
probability distribution is unique, which is the case under certain assumptions.
This holds, for example, if for some t0 > 0 we have pij(t0) > 0 for all i, j ∈Q
(see [Do]).
We will now discuss two important properties of stationary probability
distributions.
(a) Deﬁne the probability pi(t) of the event that the nucleotide site is in state
i at time t as follows
pi(t) =

k∈Q
ϱkpki(t),
where ϱ = (ϱA, ϱC, ϱG, ϱT ) is the vector of initialization probabilities for the
Markov chain. Setting ϱ = ϕ we obtain
pi(t) =

k∈Q
ϕkpki(t) = ϕi.
Hence, if the nucleotide site is assumed to evolve in accordance with a
Markov chain with stationary probability distribution ϕ, and if ϕ is taken as
the vector of initialization probabilities (which is commonly done), then pi(t)

126
5 Phylogenetic Reconstruction
does not depend on t and is equal to ϕi for all i. This is of course a very strong
(and hardly realistic) assumption on the way the nucleotide site evolves. It is
used mainly because of its computational convenience.
(b) We have
P(t) →
⎛
⎜
⎜
⎝
ϕA ϕC ϕG ϕT
ϕA ϕC ϕG ϕT
ϕA ϕC ϕG ϕT
ϕA ϕC ϕG ϕT
⎞
⎟
⎟
⎠,
as t →∞, that is, for large t, P(t) “stabilizes” in accordance with the station-
ary probability distribution. A similar property holds for ordinary discrete-
time Markov chains as well. Namely, if a discrete-time Markov chain with
matrix of transition probabilities P possesses a unique stationary probability
distribution ϕ, then under certain additional assumptions we have
P n →
⎛
⎜
⎝
ϕ1 . . . ϕN
...
...
...
ϕ1 . . . ϕN
⎞
⎟
⎠,
(5.7)
as n →∞(see Exercise 5.20). For example, if for some n0 ∈N all elements of
P n0 are positive (and hence all elements of P n for n ≥n0 are positive as well),
then a stationary probability distribution is unique and (5.7) holds (see [Do]).
For the general theory of stationary probability distributions the inter-
ested reader is referred to [Kar], [Do] (see also [EG]).
(2) Time reversibility
Deﬁnition 5.14. Let ϱ be the vector of initialization probabilities of a
continuous-time Markov chain, and suppose that pi(t) ̸= 0 for all i ∈Q and
t ≥0. We deﬁne the reversed Markov chain as the continuous-time Markov
chain given by the transition probability matrices P ∗(t) with
p∗
ij(t) = ϱjpji(t)
pi(t) ,
for all i, j.
Loosely speaking, replacing P(t) with P ∗(t) corresponds to reversing the time
parameter t. Assuming that each component of the (unique) stationary prob-
ability distribution ϕ is non-zero and setting ϱ = ϕ, we obtain
p∗
ij(t) = ϕjpji(t)
ϕi
,
for all i, j.

5.4 Evolutionary Models
127
Deﬁnition 5.15. A Markov chain satisfying the assumptions of Deﬁnition
5.14 is called time-reversible or simply reversible if P ∗(t) = P(t) for all t ≥0.
If each component of ϕ is non-zero, then for ϱ = ϕ reversibility is equivalent
to
ϕipij(t) = ϕjpji(t),
(5.8)
for all i, j. Note that identity (5.8) makes sense even if some of the compo-
nents of ϕ are equal to zero, and in what follows we will always understand
reversibility in the sense of this identity. Note that due to (5.5), identity (5.8)
is equivalent to the condition that the matrix
⎛
⎜
⎜
⎝
ϕA 0
0
0
0 ϕC
0
0
0
0 ϕG 0
0
0
0 ϕT
⎞
⎟
⎟
⎠Q
is symmetric.
We will now describe four evolutionary models commonly used for phylo-
genetic reconstruction.
5.4.1 The Jukes-Cantor Model
The Jukes-Cantor model was introduced in [JC] and is given by setting
Q =
⎛
⎜
⎜
⎝
−3α/4
α/4
α/4
α/4
α/4
−3α/4
α/4
α/4
α/4
α/4
−3α/4
α/4
α/4
α/4
α/4
−3α/4
⎞
⎟
⎟
⎠,
where α is a positive constant called the evolutionary rate (the role of evo-
lutionary rates in phylogenetic reconstruction will be discussed in the next
section).
We will now calculate the corresponding matrix P(t) = exp(tQ). Since we
need to ﬁnd the sum of the series
∞

n=0
tnQn
n!
,
we will ﬁrst determine the powers of Q. We will show by induction that
Qn = (−α)n−1Q,
(5.9)
for all n ∈N.
Clearly, (5.9) holds for n = 1. Assume that n0 ≥2 and that (5.9) holds
for all n < n0. Then

128
5 Phylogenetic Reconstruction
Qn0 = Qn0−1 Q = (−α)n0−2Q2.
Calculating Q2 we obtain
Q2 =
⎛
⎜
⎜
⎝
3α2/4 −α2/4 −α2/4 −α2/4
−α2/4 3α2/4 −α2/4 −α2/4
−α2/4 −α2/4 3α2/4 −α2/4
−α2/4 −α2/4 −α2/4 3α2/4
⎞
⎟
⎟
⎠= −αQ.
Hence
Qn0 = (−α)n0−2(−α)Q = (−α)n0−1Q,
and (5.9) is proved.
We can now ﬁnd P(t) = exp(tQ). Indeed,
∞

n=0
tnQn
n!
= E +
∞

n=1
tnQn
n!
= E +
 ∞

n=1
tn(−α)n−1
n!

Q = E −1
α
 ∞

n=1
(−tα)n
n!

Q
= E −1
α(exp(−tα) −1)Q.
This implies
pii(t) = 1
4 + 3
4 exp(−tα)
for all i,
pij(t) = 1
4 −1
4 exp(−tα)
for all i ̸= j.
(5.10)
Let ϕ = (1/4, 1/4, 1/4, 1/4). A simple calculation shows that ϕ Q = 0, and
hence ϕ is a stationary probability distribution of the Jukes-Cantor model. It
is not hard to show that ϕ is the only stationary probability distribution of
the model and that the model is time-reversible (see Exercise 5.21).
The Jukes-Cantor model is one of the earliest models and is not very
realistic. In particular, it assumes that the probabilities to ﬁnd a nucleotide
site in any of the four possible states are all equal to 1/4 for all t.
5.4.2 The Kimura Model
The Kimura model [Ki] is a generalization of the Jukes-Cantor model and is
given by setting
Q =
⎛
⎜
⎜
⎝
−(2β + 1)α/4
βα/4
α/4
βα/4
βα/4
−(2β + 1)α/4
βα/4
α/4
α/4
βα/4
−(2β + 1)α/4
βα/4
βα/4
α/4
βα/4
−(2β + 1)α/4
⎞
⎟
⎟
⎠,

5.4 Evolutionary Models
129
where, as before, α > 0 is referred to as the evolutionary rate, and β > 0 is an
additional parameter. The Kimura model turns into the Jukes-Cantor model
for β = 1.
As before, we can ﬁnd the corresponding matrix P(t) = exp(tQ) for the
Kimura model (see Exercise 5.22). We have
pii(t) = 1
4 + 1
4 exp(−tβα) + 1
2 exp

−t(β + 1)α
2

,
pAC(t) = pCA(t) = pAT (t) = pT A(t) = pCG(t) = pGC(t)
= pGT (t) = pT G(t) = 1
4 −1
4 exp(−tβα),
pAG(t) = pGA(t) = pCT (t) = pT C(t) = 1
4 + 1
4 exp(−tβα)
−1
2 exp

−t(β + 1)α
2

.
(5.11)
The Kimura model thus incorporates a certain diﬀerence between two types
of nucleotide substitutions: transversions (A →C, C →A, A →T, T →A,
C →G, G →C, G →T, T →G) and transitions (A →G, G →A, C →T,
T →C). Its stationary probability distribution is unique (and is identical to
that of the Jukes-Cantor model), and the model is reversible (see Exercise
5.23).
5.4.3 The Felsenstein Model
The Felsenstein model [F1] is also a generalization of the Jukes-Cantor model
and is given by the matrix
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−α(πC + πG
απC
απG
απT
+πT )
απA
−α(πA + πG
απG
απT
+πT )
απA
απC
−α(πA + πC
απT
+πT )
απA
απC
απG
−α(πA + πC
+πG)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where, as before, α > 0 is the evolutionary rate, and πi for i ∈Q are non-
negative parameters satisfying πA+πC+πG+πT = 1. The Jukes-Cantor model
is a special case of the Felsenstein model for πA = πC = πG = πT = 1/4. The
corresponding matrix P(t) = exp(tQ) is given by

130
5 Phylogenetic Reconstruction
pii(t) = πi + (1 −πi) exp(−tα), for all i,
pij(t) = πj −exp(−tα)πj,
for all i ̸= j.
The stationary probability distribution of the Felsenstein model is unique
and coincides with the vector (πA, πC, πG, πT ); this model is also reversible
(see Exercise 5.24). The Felsenstein model is much more ﬂexible than the
Jukes-Cantor model since it allows to construct a continuous-time Markov
model with a given stationary probability distribution.
5.4.4 The Hasegawa-Kishino-Yano (HKY) Model
The HKY model [HKY] generalizes the Felsenstein model in the same way
as the Kimura model generalizes the Jukes-Cantor model. The corresponding
matrix Q is as follows
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−α(βπC + πG
βαπC
απG
βαπT
+βπT )
βαπA
−α(βπA + βπG
βαπG
απT
+πT )
απA
βαπC
−α(πA + βπC
βαπT
+βπT )
βαπA
απC
βαπG
−α(βπA + πC
+βπG)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where α > 0 is the evolutionary rate, β > 0 is a parameter responsible for
distinguishing between transitions and transversions, πi for i ∈Q are non-
negative parameters satisfying πA + πC + πG + πT = 1. The only stationary
probability distribution of the HKY model is the vector (πA, πC, πG, πT ), and
the model is reversible (see Exercise 5.25). This model generalizes the three
models described above. It diﬀerentiates between transitions and transver-
sions, and allows to construct a continuous-time Markov model with a given
stationary probability distribution. The matrix P(t) = exp(tQ) has a rather
complicated form and we omit it.
5.5 Maximum Likelihood Method
In this section we will show how evolutionary models can be used to infer
a phylogenetic tree (or trees) from a reduced multiple alignment of DNA
sequences. Let M = {x1, . . . , xN} be the original set of OTUs and let
D = {ˆx1, . . . , ˆxN} be the collection of the shorter sequences that form the
reduced multiple alignment. First of all, we select an evolutionary model that
we assume to be one of the four models described in the previous section
(although everything that follows can be also applied to an arbitrary regular

5.5 Maximum Likelihood Method
131
reversible evolutionary model). Here we do not explain how one can choose
the “best” model for a particular dataset; this question will be addressed to
some extent in the next section.
Suppose we have selected a model. We will now make the following
Evolutionary Assumption.
(i) The sequences in the dataset D evolved from their common ancestor along
a molecular clock tree whose branches are measured as time intervals (we will
refer to this tree as the true tree),
(ii) the evolution along the true tree involved only substitutions, not deletions
or insertions,
(iii) each site evolved along the true tree independently of the others and
identically to the others,
(iv) the evolution of each site along each branch of the true tree did not depend
on its evolution along any other branch,
(v) the substitution process for each site along each branch of the true tree
went in accordance with the selected Markov model,
(vi) the evolutionary rate parameter α is branch-speciﬁc, that is, allowed to
change from branch to branch,
(vii) the parameter β (in the case of the Kimura and HKY models) is also
branch-speciﬁc subject to the constraint that it has the same value on the two
branches descending from the root.
Our ultimate goal is to determine the true tree. We will attempt to ﬁnd
it by calculating the likelihood L(D|T ) of the dataset D given T (a formula
for L(D|T ) will be given later) for every molecular clock tree T relating the
sequences in D, and by selecting the tree (or trees) for which L(D|T ) is
maximal (assuming that a point of maximum exists). This procedure is the
core of the maximum likelihood approach. The likelihood L(D|T ) is in fact
the probability (calculated in accordance with the evolutionary assumption)
of the event that the sequences in the dataset D are related by the tree T ,
and the maximal likelihood method selects the trees for which this probability
has the largest value as optimal trees.
The maximum likelihood approach is probabilistic rather than determin-
istic which has many advantages. For instance, unlike most approaches to
phylogenetic reconstruction discussed in the preceding sections, it comes with
its own measure of error. If, for example, T is optimal for the maximal like-
lihood method, but L(D|T ) is small, one probably should not accept T as a
reliable tree.
Observe that it is unreasonable to expect that the branch lengths of the
true tree can be determined by any method whatsoever. Indeed, the true

132
5 Phylogenetic Reconstruction
tree was assumed to be a molecular clock tree whose branch lengths are mea-
sured as time intervals. However, evolution might have been faster along some
branches and slower along others. Since no information on the speed of evo-
lution is usually available for the dataset D, no analysis can extract the ac-
tual time intervals from it. Instead, one can hope to determine time intervals
scaled by particular values of the evolutionary rate. Biologically, the evolu-
tionary rate is thought of as the speed of evolution along a particular branch
of a tree. Mathematically, it is a model parameter (that we always denote by
α) which is simply a scaling factor for the matrix Q = P ′(0). In the previ-
ous section we have seen evolutionary rates incorporated into the matrices of
instantaneous change for the four speciﬁc models. In accordance with (vi) of
the evolutionary assumption, the evolutionary rate is branch-speciﬁc. Later
we will see from the explicit formula for the likelihood that, for a ﬁxed tree
topology, if tj is the parameter for the length of the jth branch and αj is
value of the evolutionary rate along this branch, then L(D|T ) depends on αj
and tj by way of the product αj × tj. Hence, when L(D|T ) is maximized for
every topology and subsequently over all topologies, one only obtains optimal
values of the product of time and the evolutionary rate, and these values are
taken as the branch lengths of the corresponding optimal trees. Therefore,
from now on we will assume that the tree space we are working on consists of
all rooted (not necessarily molecular clock) trees relating the sequences from
D, and that the evolutionary rate is set to 1 on each branch of every tree.
The likelihood L(D|T ) is a function of T (which includes the topology and
branch lengths – constrained to be non-negative) as well as the branch-speciﬁc
parameter β (in the case of the Kimura and HKY models), and therefore even
for a moderate number N of OTUs L(D|T ) typically depends on a large
number of variables. Ideally, one would like to maximize L(D|T ) over all
these variables, but in practice doing so is a slow process even for a ﬁxed
topology. Therefore, in reality a very limited number of topologies can be
examined, which decreases the sensitivity of the method. Luckily, as will be
explained below, the reversibility of the evolutionary model helps to slightly
reduce the topology space that needs to be searched, but one should keep in
mind that, ﬁrstly, reversibility imposes biologically unjustiﬁable constraints
on the evolution of each nucleotide site, and, secondly, the reduced topology
space is still very large in cases of interest.
The maximum likelihood method is implemented, for example, as part of
the PHYLIP software package [F3], and the interested reader can learn about
various topology space search algorithms utilized there as well as other fea-
tures
of
this
package
from
the
PHYLIP
website
http://evolution.genetics.washington.edu/phylip.html.
We will now explain how the likelihood of a dataset given a tree is cal-
culated. Rather than writing a general formula, we will consider a speciﬁc
example that will convey the idea. Suppose that the reduced multiple align-
ment is as follows

5.5 Maximum Likelihood Method
133
ˆx1 : A A G
ˆx2 : C T C
ˆx3 : C G G
ˆx4 : G G A,
and let T be the rooted tree shown in Fig. 5.30.
Fig. 5.30.
First of all, we deﬁne site-speciﬁc likelihoods as
L1(D|T ) =

ijk∈Q
ϕipij(u5)pik(u6)pjA(u1)pjC(u2)pkC(u3)pkG(u4),
L2(D|T ) =

ijk∈Q
ϕipij(u5)pik(u6)pjA(u1)pjT (u2)pkG(u3)pkG(u4),
L3(D|T ) =

ijk∈Q
ϕipij(u5)pik(u6)pjG(u1)pjC(u2)pkG(u3)pkA(u4),
where ϕ is the stationary probability distribution of the model. In the above
formulas the parameter β (in the case of the Kimura and HKY models) is
branch-speciﬁc. More precisely, the lth branch carries its own parameter βl
for l = 1, . . . , 6, with the constraint β5 = β6, which agrees with (vii) of the
evolutionary assumption. For example, pjA(u1) is calculated using β1 and
pij(u5), pik(u6) using β5 = β6. The likelihood of the dataset is then deﬁned as
the product of the site-speciﬁc likelihoods
L(D|T ) = L1(D|T )L2(D|T )L3(D|T ).

134
5 Phylogenetic Reconstruction
An analogous deﬁnition can be given for the case of molecular clock trees
and arbitrary values of the evolutionary rate, which was our original setup. It
is clear that in this case, for a ﬁxed tree topology, L(D|T ) indeed depends on
αj and tj by way of the product αj × tj, as we stated above.
Further, one can show that the reversibility of the model implies that
L(D|T ) does not in fact depend on the root position, that is, L(D|T1) =
L(D|T2), if the topologies and branch lengths of the corresponding unrooted
trees for T1 and T2 coincide, and the values of the parameter β (in the case
of the Kimura and HKY models) are the same on respective branches. This
statement is called the pulley principle and was proved in [F1] (see a discus-
sion at the end of this section and Exercise 5.27). Due to the pulley principle,
it only makes sense to apply the maximum likelihood method to the space
of all unrooted trees. Certainly, to calculate the likelihood a root is required,
but it can be placed on an unrooted tree arbitrarily, for example, any of the
internal nodes can be taken as a root. The resulting optimal trees are of course
unrooted, and in order to place roots on them one needs additional informa-
tion about the OTUs in question. Since the number of unrooted topologies is
smaller than the number of rooted ones (see Sect. 5.1), passing to unrooted
trees somewhat reduces the computational complexity of the problem. How-
ever, for many datasets of interest the number of OTUs exceeds 20; in such
cases even the space of unrooted topologies is far too large to be examined
comprehensively, and only a minuscule part of it can be explored by various
existing heuristic search algorithms.
We will now show how the maximum likelihood method can be utilized
to produce a pseudodistance function on a set M = {x1, . . . , xN} of OTUs.
As we have seen in the preceding section, pseudodistance functions can be
used by any distance method for inferring a phylogenetic tree. Let, as before,
D = {ˆx1, . . . , ˆxN} be the collection of the sequences that form the reduced
multiple alignment. A pseudodistance function on M comes from the pairwise
alignments induced by the reduced multiple alignment, namely, dij is obtained
by considering the alignment between ˆxi and ˆxj. Speciﬁcally, the maximal
likelihood method is applied to the dataset Dij = {ˆxi, ˆxj} of two aligned
sequences. Any unrooted tree relating ˆxi and ˆxj is a segment as shown in Fig.
5.31.
x
x
u
^
^
i
j
Fig. 5.31.
Maximizing L(Dij|T ) over u and (in the case of the Kimura and HKY
models) β, we obtain some optimal values of the parameters. Then dij is set
to be equal to the optimal value of u (assuming that it is unique). Of course,
the result depends on the evolutionary model chosen in advance.

5.5 Maximum Likelihood Method
135
As we will see in Example 5.16 below, it may happen that dij = ∞for some
i, j, and therefore the resulting “function” d may not be a pseudodistance
function. In such cases further adjustments are required. For example, dij can
be set to a very large number instead.
The above procedure is based on maximizing the likelihood of a dataset
consisting of two DNA sequences of equal length put in a (unique) ungapped
alignment. We will now give an example of such likelihood maximization (and
hence pseudodistance calculation) for the case of the Jukes-Cantor model (see
also Exercise 5.26).
Example 5.16. Consider two sequences x = x1, . . . , xn and y = y1, . . . , yn,
x ̸= y, with xi, yi ∈Q = {A, C, G, T} for i = 1, . . . , n, aligned as follows
x : x1 . . . xn
y : y1 . . . yn.
We will ﬁnd the distance d(x, y) between x and y. Consider the two-sequence
dataset D = {x, y} and for any tree T relating x and y as shown in Fig. 5.32,
calculate the likelihood L(D|T ).
Fig. 5.32.
First of all, we need to ﬁnd the site-speciﬁc likelihoods Li(D|T ), i =
1, . . . , n. We place a root in the middle of the branch and use formulas (5.10)
with α = 1. For each 1 ≤i ≤n we will consider two cases.
Let ﬁrst xi = yi = z. Then we have
Li(D|T ) = 1
4

j∈Q
pjz
u
2
2
= 1
4

pzz
u
2
2
+

j∈Q, j̸=z
pjz
u
2
2

= 1
4
1
4 + 3
4 exp

−u
2
2
+3
1
4 −1
4 exp

−u
2
2
= 1
16 (1 + 3 exp(−u)) .
(5.12)

136
5 Phylogenetic Reconstruction
Suppose now that xi ̸= yi. Then we obtain
Li(D|T ) = 1
4

j∈Q
pjxi
u
2

pjyi
u
2

= 1
4

pxixi
u
2

pxiyi
u
2

+pyixi
u
2

pyiyi
u
2

+

j∈Q, j̸=xi, j̸=yi
pjxi
u
2

pjyi
u
2

= 1
4

2
1
4 + 3
4 exp

−u
2
 1
4 −1
4 exp

−u
2

+2
1
4 −1
4 exp

−u
2
2
= 1
16 (1 −exp(−u)) .
(5.13)
Let 0 ≤m < n be the number of indices 1 ≤i ≤n for which xi = yi. It
then follows from the above formulas that the likelihood of D is given by
L(D|T ) =
1
16n (1 + 3p)m(1 −p)n−m,
where p = exp(−u). Clearly, 0 ≤p ≤1, and in order to maximize the likeli-
hood, we need to ﬁnd the maximum of the function
ψ(p) = (1 + 3p)m(1 −p)n−m
on the segment [0, 1].
Let ﬁrst m = 0. Then ψ(p) = (1−p)n attains its maximum at p = 0 which
gives u = ∞and consequently d(x, y) = ∞.
Suppose now that m > 0. We will initially consider ψ on the whole real
line and ﬁnd its points of maximum. Diﬀerentiating we obtain
ψ′(p) = (1 + 3p)m−1(1 −p)n−m−1(4m −n −3pn).
Clearly, p = −1/3 and p = 1 are not points of maximum since there the value
of ψ is 0. The only remaining critical point is
p = p0 = 4m −n
3n
.
Clearly, −1/3 < p0 < 1 and
ψ
′′(p0) = −3n(1 + 3p0)m−1(1 −p0)n−m−1
is negative. Therefore, p0 is the unique point of maximum of ψ on R.

5.6 Model Comparison
137
If 4m > n, then p0 > 0 and ψ attains its maximum on [0, 1] at p0 which
yields
d(x, y) = u = −ln p0 = −ln 4m −n
3n
> 0.
If 4m ≤n, then p0 ≤0, and ψ attains its maximum on [0, 1] at 0, which,
as for the case m = 0, gives u = ∞and therefore d(x, y) = ∞.
We will now show a simpler way to compute the site-speciﬁc likelihoods
from Example 5.16. We will do it for an arbitrary regular time-reversible
model. As before, denote by ϕ the stationary probability distribution. Place a
root on the tree in Fig. 5.32, and let the lengths of the branches leading from
the root to x and y be u1 and u2 respectively, with u1 +u2 = u. In accordance
with (vii) of the evolutionary assumption, the parameters of the model (such
as β in the case of the Kimura and HKY models) are assumed to be constant
on T . Fix 1 ≤i ≤n. Using (5.4) and (5.8) we obtain
Li(D|T ) =

j∈Q
ϕjpjxi(u1)pjyi(u2) = ϕxi

j∈Q
pxij(u1)pjyi(u2)
= ϕxipxiyi(u) = ϕyipyixi(u),
which in the case of the Jukes-Cantor model gives formulas (5.12) and (5.13).
This last calculation in fact proves the pulley principle for an arbitrary
regular time-reversible model and N = 2. It is easy to generalize this proof to
any number of OTUs, but one has to impose a condition analogous to (vii) of
the evolutionary assumption: each model parameter (apart from α that is set
to 1 everywhere) is branch-speciﬁc subject to the constraint that it has equal
values on the two branches descending from the root (see Exercise 5.27).
5.6 Model Comparison
It is important to choose a correct evolutionary model to assess a particular
dataset. Indeed, as we have seen in the preceding section, the analysis of a
dataset by the maximum likelihood method depends on the model selected in
advance. Choosing a model from a range of available ones is a diﬃcult task. In
this section we will address a somewhat easier question: given a pair of models,
how can one select a model “most suitable” for the dataset in question?
Speciﬁcally, we will describe a statistical hypothesis testing procedure that
allows to compare the two models.
Suppose we are given a dataset D as in the preceding section and two
regular reversible evolutionary models M1 and M2. Assume further that M1
is a special case of M2. For example, M1 can be the Jukes-Cantor model and
M2 the Kimura model. We wish to know whether or not passing to the more
general model M2 gives us a signiﬁcantly better understanding of the dataset
compared to the understanding that we gained by applying model M1 to it.

138
5 Phylogenetic Reconstruction
Since M1 is a special case of M2, using the latter model explains the data at
least as well as using the former one. But if the more general model does not
lead to a signiﬁcantly better understanding of the dataset, we may as well
be content with using model M1 for it, which has its advantages, since M1 is
simpler.
Statistically, this setup is formalized as follows: consider the null hypothesis
H0 : the evolutionary assumption holds for D with model M1,
and the alternative hypothesis
HA : the evolutionary assumption holds for D with model M2.
The null hypothesis is a special case of the alternative one, that is, the hy-
potheses are nested. We wish to know whether the null hypothesis should be
accepted or rejected in favor of the alternative hypothesis. Below we will brieﬂy
outline one procedure used in statistics for making a decision to either accept
or reject the null hypothesis; more detail on this procedure will be given in
Sect. 8.3, where a general approach to statistical hypothesis testing will be
discussed.
The procedure is a special case of the likelihood ratio test. First of all, we
ﬁnd the maxima of the likelihoods of D under H0 and HA (assuming that
points of maximum exist). We denote the maximal likelihoods by Lmax
0
(D)
and Lmax
A (D) respectively. Assume that Lmax
A (D) ̸= 0 and form the likelihood
ratio
∆(D) = Lmax
0
(D)
Lmax
A (D).
Clearly, ∆(D) ≤1. Intuitively, if ∆(D) is very close to 1, then H0 probably
explains the data almost as well as HA, and should be accepted; on the other
hand, if ∆(D) is much less than 1, then H0 probably should be rejected in
favor of HA. However, we do not know what “∆(D) is very close to 1” and
“∆(D) is much less than 1” exactly mean. It is in fact impossible to make a
decision to either accept or reject the null hypothesis on the basis of ∆(D)
alone. What we need to know is how close the value ∆(D) is to the “typical”
values of the likelihood ratio calculated for datasets that genuinely obey H0.
To determine such typical values we need to produce a large number of
artiﬁcial datasets Dj that obey H0. This can be done by data simulation
under the null hypothesis. To simulate data at a single nucleotide site, we
choose a rooted (not necessarily molecular clock) tree T , assign one of the four
nucleotide states to the root in accordance with the stationary probability
distribution of model M1 and let the site evolve down the tree to the tips
according to M1. Doing so for many nucleotide sites produces a simulated
dataset that obeys H0. This data simulation process is implemented in the
software package Seq-Gen [RG].
Of course, before we can attempt such data simulation, we need to select
the tree T as well as values of the parameters of model M1 for each branch

Exercises
139
of T (note that the evolutionary rates are always set to 1). We obtain T by
arbitrarily placing a root on a maximum likelihood tree found for D under
the null hypothesis, and set the parameters of model M1 to be equal to their
optimal values found under the null hypothesis. With T and model parameters
so selected we can simulate a number of datasets Dj, j = 1, . . . , L, that obey
H0. These datasets can be assessed under each of H0, HA, and the values
∆(Dj) can be calculated for them (here we assume that Lmax
A (Dj) ̸= 0 for all
j). If L is large, we hope that ∆(Dj), j = 1, . . . , L, give us a good idea about
what a “typical” value of the likelihood ratio calculated for a dataset that
obeys H0 is.
On the basis of the values ∆(Dj), j = 1, . . . , L, we will now make a decision
whether or not H0 should be accepted. We ﬁnd the signiﬁcance point of the
test, which is the largest number K satisfying the condition that the ratio of
the number of elements in the set {j : ∆(Dj) ≤K} and L does not exceed
0.05. Then H0 is rejected if ∆(D) ≤K. Thus, loosely speaking, H0 is accepted
if ∆(D) lies within the highest 95% of the values ∆(Dj), j = 1, . . . , L, and
rejected otherwise.
The hypothesis testing procedure outlined above is called the parametric
bootstrap method and is widely used not just in phylogenetic reconstruction,
but in statistics in general.
Exercises
5.1. Prove the formulas from Sect. 5.1 for the numbers of topologies of rooted
and unrooted trees relating N ≥2 OTUs.
5.2. Let Q be the three letter-alphabet {A, B, C} and suppose that we are
given the following reduced multiple alignment of three sequences
x1 : A B C
x2 : A A C
x3 : B C C.
Find all the most parsimonious rooted topologies for x1, x2, x3 together with
all optimal sequence assignments.
5.3. Let T be a phylogenetic tree relating OTUs from a family M, where
branch lengths are allowed to be any non-negative numbers. Show that the
tree-generated function dT is a distance function on M, if condition (i) from
Deﬁnition 5.2 is satisﬁed.
5.4. Suppose that d is an additive distance function on a set of OTUs. Show
by induction that there exists precisely one tree T relating the OTUs and
having non-negative branch lengths, that generates d.

140
5 Phylogenetic Reconstruction
5.5. Show that any additive distance function on a set M of N OTUs with
N ≥4 satisﬁes the four-point condition.
5.6. Consider the following distance matrix
Md x1 x2 x3 x4 x5 x6 x7
x1
0 13 9
6 13 13 17
x2 13 0
4 15 16 16 20
x3
9
4
0 11 12 12 16
x4
6 15 11 0 15 15 19
x5 13 16 12 15 0 16 6
x6 13 16 12 15 16 0 20
x7 17 20 16 19 6 20 0
Show that d is indeed a distance function and that it satisﬁes the four-point
condition. Applying the neighbor-joining algorithm, ﬁnd the tree that gener-
ates d.
5.7. Consider the following pseudodistance matrix
Md x1 x2 x3 x4
x1
0
3
2
7
x2
3
0
3
4
x3
2
3
0
3
x4
7
4
3 0.
Show that d does not satisfy either the triangle inequality or the four-point
condition, apply the neighbor-joining algorithm to d and ﬁnd all trees that it
can produce.
5.8. Let d be an ultrameric distance function on a set M of OTUs. Prove that
d satisﬁes the four-point condition.
5.9. Prove that the distance function from Example 5.7 is ultrameric.
5.10. Show that if T is a molecular clock tree, then dT is an ultrameric
distance function, provided dT satisﬁes condition (i) of Deﬁnition 5.2.
5.11. Consider the distance matrix
Md x1 x2 x3 x4 x5 x6 x7
x1
0 24 24 8 24 8
8
x2 24 0
6 24 2 24 24
x3 24 6
0 24 6 24 24
x4
8 24 24 0 24 6
4
x5 24 2
6 24 0 24 24
x6
8 24 24 6 24 0
6
x7
8 24 24 4 24 6
0
Prove that d is indeed a distance function and that it is ultrameric. Find the
corresponding molecular clock tree by applying the UPGMA algorithm.

Exercises
141
5.12. Show that for the distance matrix from Example 5.9, d is indeed a
distance function, that it satisﬁes the four-point condition, and that it is not
ultrameric.
5.13. Consider the following distance matrix
Md x1 x2 x3 x4 x5 x6
x1
0
7
3
8
8 11
x2
7
0
8 13 13 16
x3
3
8
0
7
7 10
x4
8 13 7
0 10 13
x5
8 13 7 10 0 11
x6 11 16 10 13 11 0
Prove that d is indeed a distance function, that it satisﬁes the four-point
condition, and that it is not ultrameric. Apply the neighbor-joining algorithm
and the UPGMA algorithm to d. How diﬀerent are the two resulting trees?
5.14. Give an example of a pseudodistance function d on a set of four OTUs
with the property that none of the trees produced from d by UPGMA is
optimal in the sense of the least squares method.
5.15. Prove that in Example 5.10, the function ψ is minimized by the tree T2
from Fig. 5.16 found in Example 5.6, that a point of minimum is unique and
that the minimal value of ψ is 4(3/2 −a/4)2.
5.16. Show that each of the m2 scalar series in the right-hand side of formula
(5.6) converges for any matrix A.
5.17. Let f(t) be a function deﬁned on [0, ∞), diﬀerentiable at any t > 0 and
having a one-sided derivative at 0. Suppose that for all t ≥0 f satisﬁes the
diﬀerential equation
f ′(t) = αf(t),
where α ∈R, with the initial condition f(0) = 1. Prove directly (without
resorting to the general theory of diﬀerential equations) that f(t) = exp(αt).
5.18. Prove that the elements in each row of the matrix of instantaneous
change of a regular Markov chain sum up to 0.
5.19. Consider a discrete-time Markov chain with two states and the following
matrix of transition probabilities
P =

1 −a
a
b
1 −b

,
where 0 ≤a, b ≤1 and a + b > 0. Show that this Markov chain has a unique
stationary probability distribution and ﬁnd it.

142
5 Phylogenetic Reconstruction
5.20. For the Markov chain from Exercise 5.19 ﬁnd the limit of P n, as n →∞,
directly.
5.21. Show that (1/4, 1/4, 1/4, 1/4) is the only stationary probability distri-
bution of the Jukes-Cantor model and that the model is time-reversible.
5.22. Prove formulas (5.11).
5.23. Show that (1/4, 1/4, 1/4, 1/4) is the only stationary probability distri-
bution of the Kimura model and that the model is reversible.
5.24. Show that (πA, πC, πG, πT ) is the only stationary probability distribu-
tion of the Felsenstein model and that the model is reversible.
5.25. Show that (πA, πC, πG, πT ) is the only stationary probability distribu-
tion of the HKY model and that the model is reversible.
5.26. Generalize the result of Example 5.16 to the case of the Kimura model.
5.27. Prove the pulley principle for an arbitrary regular time-reversible evo-
lutionary model, assuming that each model parameter (apart from α that is
set to 1 everywhere) is branch-speciﬁc subject to the constraint that it has
equal values on the two branches descending from the root.
5.28. Let M1 and M2 be the Jukes-Cantor and Kimura models respectively,
and for a dataset D consider the corresponding null and alternative hypothe-
ses. Let T be a maximum likelihood tree for D found using model M1, and
suppose that we have generated 60 artiﬁcial datasets Dj obeying H0 by simu-
lating data along the tree T . Let the likelihood ratios ∆(Dj) for j = 1, . . . , 60
be as follows

0.2, 0.007, 0.18, 0.15, 0.014, 0.35, 0.19, 0.17, 0.3, 0.013, 0.75, 0.2, 0.132, 0.08, 0.6,
0.6, 0.84, 0.013, 0.15, 0.6, 0.58, 0.63, 0.19, 0.2, 0.082, 0.12, 0.1, 0.11, 0.5, 0.9, 0.2,
0.4, 0.16, 0.165, 0.188, 0.154, 0.121, 0.132, 0.3, 0.14, 0.87, 0.9, 0.5, 0.15, 0.19, 0.19,
0.35, 0.014, 0.14, 0.178, 0.189, 0.19, 0.12, 0.14, 0.17, 0.567, 0.145, 0.17, 0.32, 0.2

,
and suppose that ∆(D) = 0.0131. Will the null hypothesis be accepted or
rejected in favor of the alternative one by the parametric bootstrap method
used with the above values ∆(Dj)?

Part II
Mathematical Background for Sequence
Analysis

6
Elements of Probability Theory
In this chapter we will give a brief introduction to the theory of probability.
Our exposition gives main constructions and illustrates them by many ex-
amples, but largely avoids detailed proofs. The interested reader can ﬁnd all
proofs in [KF], [W].
6.1 Sample Spaces and Events
We denote by S the set of all possible outcomes of a trial, experiment or
operation that we are interested in and call S the sample space.
Example 6.1.
1. Tossing a coin once produces two possible outcomes: a head H or a tail
T. Hence the sample space for this experiment is S = {H, T}.
2. Analogously, tossing a coin twice gives the following sample space
S = {HH, HT, TH, TT}.
3. Suppose we are performing the experiment of shooting a bullet into the
segment [0, 1]. In this case the outcome is the point of [0, 1] hit by the bullet.
Hence S = [0, 1] and thus is an inﬁnite set.
4. Suppose we are given a Markov chain without an end state. If we let
the model run freely, it will generate all possible sequences of inﬁnite length.
Any sequence has the form 0 x1x2 . . ., where 0 denotes the begin state and
all xj belong to the set X of all non-zero states of the chain. The sample
space S consists of all such sequences. Similarly, the sample space associated
with a general continuous-time Markov chain is a collection of all sequences of
the form 0 xt1, xt2, . . ., where {tj} is a strictly increasing sequence of positive
numbers and xtj ∈X for all j.

146
6 Elements of Probability Theory
5. Suppose we are given a Markov chain with an end state (both the
begin and end states are denoted by 0 below). Suppose that the model is
non-trivially connected (see Sect. 3.1). If we let the model run freely, it will
generate all possible sequences of either ﬁnite or inﬁnite length. Any ﬁnite
sequence has the form 0 x1x2 . . . xL 0 and any inﬁnite sequence has the form
0 x1x2 . . ., where the xjs belong to the set X of all non-zero states of the chain
(we will often omit the zeroes at the beginning and at the end). The sample
space S consists of all such sequences.
6. Suppose we are given an HMM whose underlying Markov chain has an
end state and is non-trivially connected as in Part 5 above. Suppose that the
states of the HMM (apart from the begin and end states) emit symbols from
an alphabet Q. There are two natural sample spaces that one can associate
with the HMM.
a. If we let the HMM run freely, it will generate all possible pairs of
sequences (x, π), where x is a sequence of letters from Q and π is a sequence
of elements from X called the path for x (as before, we will call sequences
of elements from X paths through the Markov chain or simply paths). The
lengths of x and π are equal and can be either ﬁnite or inﬁnite. We will write
x and π respectively as x1x2 . . . xL and π = π1π2 . . . πL in the ﬁnite case and
x1x2 . . . and π1π2 . . . in the inﬁnite case, where xj is the element of Q emitted
at the state πj, for j = 1, . . . , L. We denote the sample space that consists of
all such pairs (x, π) by Sa.
b. Sometimes it will be useful for us to ignore paths and consider an
HMM as a process that generates only sequences x of letters from Q. As
above, such sequences can be either ﬁnite or inﬁnite. We denote the sample
space that consists of such sequences by Sb.
7. Suppose that we are given a rooted phylogenetic tree with N labeled
leaves. Fix a regular evolutionary model with particular branch-speciﬁc para-
meter values, where the values corresponding to the two branches descending
from the root are equal, as stated in condition (vii) from Sect. 5.5, and the
evolutionary rate is set to 1 on each branch. If we let the process of data simu-
lation run freely starting at the root with the stationary probability distribu-
tion of the model, it will generate all possible nucleotide values at each OTU.
Hence in this case the sample space S is the collection of all such N-tuples of
nucleotides (corresponding to columns in ungapped multiple alignments of N
sequences).
Subsets of a sample space S are called events and points from S are
sometimes called elementary events. For example, in Part 2 of Example 6.1,

6.1 Sample Spaces and Events
147
E = {HH, TH} is an event that consists of two elementary events. We say
that an event E occurs at a particular time, if the outcome of the experiment
in question performed at that time is contained in E. We therefore say that
the event S always occurs and the empty event ∅never occurs.
We will use the ordinary set theory for dealing with events. If e is an
element of an event E, we will write e ∈E. Further, if for every e ∈E1 we
have e ∈E2, we write E1 ⊂E2 and say that the occurrence of E1 implies
the occurrence of E2 or that E1 is a subset of E2. Two events E1 and E2 are
equal if E1 ⊂E2 and E2 ⊂E1. The empty event ∅is a subset of any other
event.
The intersection or product of two events E1 and E2 is the collection of all
elementary events e such that e ∈E1 and e ∈E2, and is denoted by E1 ∩E2.
If the event E1 ∩E2 occurs, we say that that we have an instance of joint
occurrence of E1 and E2. In a similar way one can deﬁne the intersection of
any ﬁnite number of events ∩n
j=1Ej, of a countable inﬁnite number of events
∩∞
j=1Ej and in fact of any number of events ∩α∈AEα, where A is an index
set.
The union or sum of two events E1 and E2 is the collection of all elemen-
tary events e such that e ∈E1 or e ∈E2, and is denoted by E1 ∪E2. If the
event E1 ∪E2 occurs, we say that that we have an instance of occurrence of
at least one of E1 and E2. As above, one can deﬁne the union of any ﬁnite
number of events ∪n
j=1Ej, of a countable inﬁnite number of events ∪∞
j=1Ej
and of any number of events ∪α∈AEα.
The diﬀerence between events E1 and E2 is the collection of all elemen-
tary events e such that e ∈E1, but e ̸∈E2 and is denoted by E1 \ E2. It
translates into the occurrence of E1, but not E2. Note that the diﬀerence is
not a symmetric operation: E1 \ E2 is generally diﬀerent from E2 \ E1. The
special case when E1 = S corresponds to taking the complement of an event
and is denoted by Ec = S \ E.
Here are some useful rules that involve the operations with events deﬁned
above
(E1 ∪E2) ∪E3 = E1 ∪(E2 ∪E3),
(E1 ∩E2) ∩E3 = E1 ∩(E2 ∩E3),
E1 ∩(E2 ∪E3) = (E1 ∩E2) ∪(E1 ∩E3),
E1 ∪E2 = E1 ∪(E2 \ (E1 ∩E2)),
E1 ∩E2 = (Ec
1 ∪Ec
2)c,
E1 ∪E2 = (Ec
1 ∩Ec
2)c.
The more general forms of the last two identities are

α∈A
Eα =
 
α∈A
Ec
α
c
,
(6.1)

α∈A
Eα =
 
α∈A
Ec
α
c
.
(6.2)

148
6 Elements of Probability Theory
One more operation is the symmetric diﬀerence: E1∆E2 = (E1\E2)∪(E2\E1).
It translates into saying that the event E1 occurs, but E2 does not, or the event
E2 occurs, but E1 does not.
Apart from individual events we will be also interested in speciﬁc families
of events.
Deﬁnition 6.2. A family F of events is called an algebra of events if the
following holds
(i) S ∈F,
(ii) for every E ∈F we have Ec ∈F,
(iii) for all E1, E2 ∈F we have E1 ∪E2 ∈F.
We note that (i) and (ii) imply that ∅∈F and that it follows from (ii),
(iii) and identity (6.1) that for all E1, E2 ∈F we have E1∩E2 ∈F. Of course,
the sum and product of any ﬁnite number of events from F is contained in F.
We will be interested in the following special class of algebras that are closed
with respect to taking any general countable sums and products.
Deﬁnition 6.3. A family F of events is called a σ-algebra of events if the
following holds
(i) S ∈F,
(ii) for every E ∈F we have Ec ∈F,
(iii) for any sequence of events {Ej} with Ej ∈F we have ∪∞
j=1Ej ∈F.
As before, (i) and (ii) imply that ∅∈F and it follows from (ii), (iii)
and (6.1) that for or any sequence of events {Ej} with Ej ∈F we have
∩∞
j=1Ej ∈F. We also note that every σ-algebra is an algebra; this is proved
by setting in (iii) of Deﬁnition 6.3 all but ﬁnitely many events to be equal to ∅.
If an algebra contains only ﬁnitely many events, then it is clearly a σ-algebra
as well. However, in general an algebra may not be a σ-algebra as shown in
Part 3 of Example 6.4 below.
We will now give examples of algebras and σ-algebras of events.
Example 6.4.
1. For any S the family F = {S, ∅} is an algebra. Since it contains only
ﬁnitely many events, it is also a σ-algebra.
2. For any S let F be the family of all events in S. Clearly, F is a σ-algebra.

6.1 Sample Spaces and Events
149
3. Let S = [0, 1] and F be the family that consists of ﬁnite unions of
intervals in S, where an interval is either [a, b], or [a, b), or (a, b], or (a, b) for
some 0 ≤a ≤b ≤1. First, we will show that F is an algebra by verifying that
it satisﬁes the conditions of Deﬁnition 6.2. Indeed, (i) holds since S itself is
an interval. Next, (ii) holds since the complement of a ﬁnite union of intervals
is again a ﬁnite union of intervals. Finally, the sum of two ﬁnite unions of
intervals is a ﬁnite union of intervals, and thus (iii) holds as well. Hence F is
an algebra. It is also clear that F is not a σ-algebra. For example, the union
of inﬁnitely many disjoint intervals in S does not belong to F.
For future considerations we will need the following theorem.
Theorem 6.5.
(i) The intersection of any number of algebras is an algebra.
(ii) The intersection of any number of σ-algebras is a σ-algebra.
(iii) If F is a non-empty family of events in S, then there exists a unique alge-
bra A(F) that contains F and that is contained in every algebra containing F.
(iv) If F is a non-empty family of events in S, then there exists a unique
σ-algebra B(F) that contains F and that is contained in every σ-algebra con-
taining F.
(v) A(F) ⊂B(F).
Proof: Parts (i) and (ii) are obvious and follow directly from Deﬁnitions 6.2
and 6.3.
We will now prove (iii). Let M be the algebra of all events in S (see Part 2
of Example 6.4) and let Σ be the collection of all algebras that contain F. Σ is
non-empty since M ∈Σ. Denote by A(F) the intersection of all the algebras
in Σ. A(F) is an algebra by (i) and clearly contains F. Suppose now that R is
another algebra containing F. Hence R ∈Σ and therefore A(F) ⊂R. Thus
A(F) satisﬁes the requirements of (iii). It is also clear that such an algebra is
unique, and (iii) is proved. The proof of Part (iv) is analogous to the above.
Finally, (v) holds since B(F) is an algebra that contains F.
Deﬁnition 6.6. The algebra A(F) and σ-algebra B(F) are called the algebra
generated by F and σ-algebra generated by F respectively.
We will now introduce another useful type of families of events.

150
6 Elements of Probability Theory
Deﬁnition 6.7. A family F of events is called a semi-algebra of events if the
following holds
(i) S ∈F,
(ii) ∅∈F,
(iii) for all E1, E2 ∈F we have E1 ∩E2 ∈F,
(iv) for all E, E′ ∈F such that E′ ⊂E, there exist ﬁnitely many pairwise
disjoint events E1, . . . , En ∈F such that E = ∪n
j=1Ej and E1 = E′.
One can show by induction that (iv) of Deﬁnition 6.7 can be replaced
by a stronger requirement: let events E′
1, . . . , E′
k ∈F be pairwise disjoint
and all contained in E ∈F, then there exist ﬁnitely many pairwise disjoint
events E1, . . . , En ∈F with n ≥k such that E = ∪n
j=1Ej and Ej = E′
j for
j = 1, . . . , k.
Any algebra of events is an example of a semi-algebra. Indeed, we only need
to prove property (iv) in Deﬁnition 6.7. If F is an algebra we set E1 = E′ and
E2 = E \ E′. The event E2 belongs to F since E2 = E ∩E′c, and algebras are
closed with respect to taking complements and ﬁnite intersections. We will
now give a less trivial example of a semi-algebra.
Example 6.8. Let S = [0, 1] and F be the collection of all intervals in S, that
is, F = {[a, b], [a, b), (a, b], (a, b), 0 ≤a ≤b ≤1}. To show that F is a semi-
algebra we have to verify (i)-(iv) of Deﬁnition 6.7. Clearly, S ∈F since S is an
interval, and ∅∈F since ∅is the interval (a, a), so (i) and (ii) hold. Further,
the intersection of two intervals is an interval, so (iii) holds as well. To prove
(iv) we notice that the diﬀerence between an interval and a subinterval is
either one or two intervals. Thus F is a semi-algebra. It is also clear that F
is not an algebra since the union of two disjoint intervals is not an interval.
If F is a semi-algebra, then the algebra A(F) introduced in Theorem 6.5
is easy to ﬁnd, as stated in the following theorem. This fact will be of some
importance for us later on.
Theorem 6.9. If F is a semi-algebra of events then A(F) coincides with the
collection Z(F) of all ﬁnite unions of pairwise disjoint events from F.
Proof: First, we will show that Z(F) is an algebra of events. Since F contains
S, so does Z(F), and (i) of Deﬁnition 6.2 holds.
To prove (ii) of Deﬁnition 6.2, consider E ∈Z(F), E = ∪n
j=1Ej, where
Ej ∈F are pairwise disjoint. Then there exist events D1, . . . , Dk ∈F such
that
S =
⎛
⎝
n

j=1
Ej
⎞
⎠∪
 k
i=1
Di

,

6.2 Probability Measure
151
and all the events E1, . . . , En, D1, . . . , Dk are pairwise disjoint. Hence Ec =
∪k
i=1Di ∈Z(F), and (ii) of Deﬁnition 6.2 holds.
To show (iii) consider two events E, D ∈Z(F), E = ∪n
j=1Ej, D = ∪k
i=1Di,
where E1, . . . , En are pairwise disjoint and D1, . . . , Dk are pairwise disjoint.
Let Cji = Ej∩Di for all j, i. Then for every j there exist events Q1j, . . . , Qpjj ∈
F such that
Ej =
 k
i=1
Cji

∪
 pj

q=1
Qqj

,
and the events Cj1, . . . , Cjk, Q1j, . . . , Qpjj are pairwise disjoint. Similarly, for
every i there exist events R1i, . . . , Rsii ∈F such that
Di =
⎛
⎝
k
j=1
Cji
⎞
⎠∪
 si

r=1
Rri

,
and the events C1i, . . . , Cni, R1i, . . . , Rsii are pairwise disjoint. We note that
each Qqj is disjoint from Qq′j′ for j′ ̸= j, each Rri is disjoint from Rr′i′ for
i′ ̸= i and each Qqj is disjoint from each Rri. Hence
E ∪D =
⎛
⎝
n

j=1
k
i=1
Cji
⎞
⎠∪
⎛
⎝
n

j=1
pj

q=1
Qqj
⎞
⎠∪
 k
i=1
si

r=1
Rri

,
and all the sets in the right-hand side are pairwise disjoint. Thus E ∪D ∈
Z(F), and we have shown that Z(F) is an algebra of events. Clearly, Z(F)
is contained in any algebra that contains F and therefore A(F) = Z(F).
For a semi-algebra F the minimal σ-algebra B(F) is harder to describe
than the minimal algebra A(F), and we do not do it here. We note that for
F from Example 6.8, B(F) is the σ-algebra of Borel sets in [0, 1] that play an
important role in analysis. Borel sets can be much more complicated than sets
from A(F). For example, all open sets and all closed sets in [0, 1] are Borel.
One famous example of a Borel set is the so-called Cantor set which is the
countable intersection of sets En ⊂[0, 1], n = N, where En is obtained from
[0, 1] by removing 3n−1 intervals of the form
3k −2
3n
, 3k −1
3n

,
k = 1, . . . , 3n−1.
6.2 Probability Measure
We will now start assigning probabilities to events. The common notion of the
probability of an event E is an abstraction of the idea of the relative frequency
with which the event occurs in a sequence of trials of an experiment, that is

152
6 Elements of Probability Theory
mE/m, where m is the total number of trials and mE is the number of trials
for which the outcomes belong to E. In dealing with any events of interest,
numbers between 0 and 1 can be assigned as the probabilities of events in some
initial class of relatively simple events, and those will produce the probabilities
of more complex events. In the actual assignment of probabilities to events
in the initial class, one is usually guided by a hypothesis based on what one
expects the relative frequencies of events to be in a large series of trials. The
mathematical theory begins once an assignment of probabilities to events in
the initial class has been made. In order to build a good theory, the initial
class of events is always assumed to be a semi-algebra, and the probabilities of
events in the initial class are assumed to satisfy certain conditions as indicated
in the following deﬁnition.
Deﬁnition 6.10. Let F be a semi-algebra of events in a sample space S. A
probability measure or simply probability on F is a set function P deﬁned for
all events in F and having the following three properties
(i) for every E ∈F we have P(E) ≥0,
(ii) P(S) = 1,
(iii) for any sequence {Ej} of pairwise disjoint events in F such that ∪∞
j=1Ej ∈
F we have
P
⎛
⎝
∞

j=1
Ej
⎞
⎠=
∞

j=1
P(Ej).
Note that if F is in fact a σ-algebra, then the condition ∪∞
j=1Ej ∈F
holds automatically. Property (iii) in Deﬁnition 6.10 is called the σ-additivity
property of the probability measure. It implies that P(∅) = 0. Indeed, let
P(∅) = c ≥0 and set Ej = ∅for all j. Then ∪∞
j=1Ej = ∅∈F and the
σ-additivity property gives
∞

j=1
c = c,
which is only possible if c = 0.
The σ-additivity property is stronger than the ﬁnite additivity property
which is obtained from (iii) by replacing the sequence of events by a ﬁnite
collection of pairwise disjoint events. Indeed, setting in the sequence {Ej} all
but ﬁnitely many events to be ∅and taking into account that P(∅) = 0, we
obtain the ﬁnite additivity property.
Example 6.11. Consider the experiments from Example 6.1.
1. It is natural to choose F to be the collection of all events in S:
F = {S, ∅, H, T} and, if the coin is fair, set P(S) = 1, P(∅) = 0, P(H) = 1/2,

6.2 Probability Measure
153
P(T) = 1/2.
2. Let F = {S, ∅, HH, HT, TH, TT}. Clearly, F is a semi-algebra. Assum-
ing again that the coin is fair, we set P(S) = 1, P(∅) = 0, P({HH}) = 1/4,
P({HT}) = 1/4, P({TH}) = 1/4, P({TT}) = 1/4.
3. Let F be as in Example 6.8. It is not hard to check that setting
P([a, b]) = P([a, b)) = P((a, b]) = P((a, b)) = b −a,
we obtain a probability measure on F (see Exercise 6.3).
4. We consider the following family F of events in S: F includes S, ∅and
all cylinder events, that is, events of the form
Ex0
i1,...,x0
im =

e = 0 x1x2 . . . ∈S : xi1 = x0
i1, . . . , xim = x0
im

,
for all ﬁnite subsets of indices i1 < . . . < im and all possible x0
i1, . . . , x0
im ∈X.
It is easy to check that F is a semi-algebra. We deﬁne a probability measure
on F as follows. We set P(S) = 1, P(∅) = 0 and
P

Ex0
i1,...,x0
im

=

x1, . . . , xim ∈X :
xi1 = x0
i1
, . . . , xim = x0
im
im−1

j=0
pxj xj+1,
(6.3)
where x0 = 0. It is not hard to check that P is a probability measure on F
(see, in particular, Exercise 3.1).
The assignment of probabilities by formula (6.3) is natural in the following
sense. Fix L ∈N, and for every sequence in S consider its ﬁrst L elements (not
counting the initial 0); we denote the collection of such truncated sequences
by SL. It can be shown that an arbitrary ﬁnite sequence x = 0 x1 . . . xL is
generated by the Markov chain among other elements of SL with frequency
that tends to L−1
j=0 pxj xj+1, as the number of model runs increases (see (3.1)).
Setting L = im and Eim
x0
i1,...,x0
im to be the collection of all sequences 0 x1 . . . xim
for which xi1 = x0
i1, . . . , xim = x0
im, it is then natural to deﬁne
P

Eim
x0
i1,...,x0
im

=

x1, . . . , xim ∈X :
xi1 = x0
i1
, . . . , xim = x0
im
im−1

j=0
pxj xj+1.
This formula gives a probability measure on the σ-algebra of all events in SL
and is analogous to formula (6.3).
Formula (6.3) clariﬁes the need for introducing an end state for modeling
ﬁnite sequences (see Sect. 3.1). Indeed, formula (3.1) gives the probability of
a whole cylinder event, not a single ﬁnite sequence as required.

154
6 Elements of Probability Theory
We note that it is possible to give a similar construction of a probability
measure in the case of continuous-time Markov chains where the right-hand
side in (6.3) is replaced with p0 xt1 pxt1 xt2 (t2−t1)×. . .×pxtm−1 xtm(tm−tm−1).
5. In this case we choose F to be the collection of all events in S, and deﬁne
a probability measure P on F as follows. If e is a ﬁnite sequence 0 x1x2 . . . xL 0,
set
P({e}) = p0 x1 ×
L−1

j=1
pxj xj+1 × pxL 0,
(6.4)
a formula familiar from (3.4). If E ∈F is an arbitrary event, we deﬁne P(E)
as the sum of the probabilities of all ﬁnite sequences contained in E (note
that the number of ﬁnite sequences is countable). In particular, if E contains
only inﬁnite sequences, P(E) = 0. To check that P is indeed a probability
measure on F, we only need to check that P(S) = 1, that is
∞

L=1

x1,...,xL∈X
p0 x1 ×
L−1

j=1
pxj xj+1 × pxL 0 = 1.
(6.5)
One can show that identity (6.5) holds for all non-trivially connected
Markov chains (see Exercise 3.2). Here we will only prove identity (6.5) for a
Markov chain with pa 0 = τ ̸= 0 for all a ∈X (note that, in general, if pa 0 ̸= 0
for all a ∈X, then the model is non-trivially connected). Indeed, in this case
the left-hand side in formula (6.5) becomes ∞
L=1 τ(1−τ)L−1 which is clearly
equal to 1.
Hence when in Sect. 3.1 we calculated the probability of a sequence y =
y1 . . . yL derived from real data by using formula (6.4), we in fact calculated
the probability of y in the sense of the sample space S, that is, as if y was the
outcome of a model run. In Sect. 3.1 we called this probability “the probability
with which y arises from the model”.
The above deﬁnition of probability measure can be justiﬁed as follows. As
in Part 4 above, for any L ≥2 consider the collection SL of truncated se-
quences. Clearly, SL consists of all ﬁnite sequences of length not exceeding L
generated by the model (note, however, that one cannot distinguish between
a ﬁnite sequence of length L, a truncation of a ﬁnite sequence of length > L
and a truncation of an inﬁnite sequence). As we noted in Part 4, it is possible
to prove that any ﬁnite sequence x1 . . . xm with m ≤L −1 is generated by
the Markov chain among other elements of SL with frequency that tends to
p0 x1 × m−1
j=1 pxj xj+1 × pxm 0, as the number of model runs increases.
6. We will consider two cases.
a. We choose F to be the collection of all events in Sa and for a pair
of ﬁnite sequences e = (x1 . . . xL, π1 . . . πL) deﬁne

6.2 Probability Measure
155
P a({e}) = p0 π1 ×
L−1

j=1
qπj(xj)pπj πj+1 × qπL(xL) × pπL 0.
(6.6)
Formula (6.6) is familiar from (3.8). The probability of any event E ∈F is
then deﬁned to be the sum of the probabilities of all pairs of ﬁnite sequences
contained in E. Veriﬁcation that P a(S) = 1 is done as in Part 5 above,
assuming that the underlying Markov chain is non-trivially connected (see
Exercise 3.5). This assignment of probabilities can be justiﬁed as in Part 5.
Hence when in Sect. 3.2 we calculated the probability of a pair of ﬁnite
sequences (y, ν) derived from real data, by using formula (6.6), we in fact cal-
culated the probability of (y, ν) in the sense of the sample space Sa, that is,
as if y was emitted along the path ν during a model run. In Sect. 3.2 we also
called this probability “the probability with which y arises from the model
along the path ν”.
b. We choose F to be the collection of all events in Sb and for a ﬁnite
sequence e = x1 . . . xL, deﬁne
P b({e}) =

π1,...,πL∈X
p0 π1 ×
L−1

j=1
qπj(xj)pπj πj+1 × qπL(xL) × pπL 0.
(6.7)
Formula (6.7) is familiar from (3.9). The probability of any event E ∈F is
then deﬁned to be the sum of the probabilities of all ﬁnite sequences contained
in E. This assignment of probabilities can be justiﬁed as in Part 5 as well.
Hence when in Sect. 3.2 we calculated the probability of a ﬁnite sequence
y derived from real data, by using formula (6.7), we in fact calculated the
probability of y in the sense of the sample space Sb, that is, as if y was the
outcome of a model run. In Sect. 3.2 we also called this probability “the prob-
ability with which y arises from the model”.
7. We again choose F to be the collection of all events in S. We treat
every N-tuple of nucleotides e ∈S as a nucleotide site and deﬁne P({e}) to
be the likelihood of the site, as introduced in Sect. 5.5. This assignment of
probabilities is reasonable since it can be shown that the frequency of every
particular N-tuple of nucleotides tends to the value speciﬁed above as the
number of simulation runs increases. The probability of any event E ⊂S is
then the sum of the probabilities of the elementary events in E (note that
S is ﬁnite). It is straightforward to prove that P(S) = 1, and hence P is a
probability measure on F.
We will be usually interested in extending a probability measure P from
the initial class of events (which is some semi-algebra F) to more complex
events. It is easy to extend P from F to A(F). Indeed, by Theorem 6.9, every
element E ∈A(F) can be represented as a ﬁnite union of pairwise disjoint

156
6 Elements of Probability Theory
events from F: E = ∪n
j=1Ej, and we set P(E) = n
j=1 P(Ej). It is easy to
show that this deﬁnition does not depend on the representation of E as a union
of events in F and that the extended function P is a probability measure on
A(F), that is, satisﬁes the conditions in Deﬁnition 6.10 on A(F).
However, the σ-additivity property of P allows it to be extended past
A(F), at least as far as B(F). An extension of P to B(F) can be constructed
as follows.
Deﬁnition 6.12. For any event E ⊂S deﬁne its outer measure P ∗(E) as
follows
P ∗(E) = inf
∞

j=1
P(Ej),
where the inﬁmum is taken over all countable coverings of E by elements from
F: E ⊂∪j=1Ej, Ej ∈F.
Clearly, P ∗(E) = P(E) for all E ∈F. Further, the following important
statement holds.
Theorem 6.13. P ∗is a probability measure on B(F).
Theorem 6.13 shows that the probability measure P deﬁned on F can be
extended to a probability measure deﬁned on B(F). In the future we will drop
the superscript ∗when considering P ∗on B(F) and call the triple (S, B(F), P)
the probability space arising from S, F and P.
Example 6.14. Consider the probability measures deﬁned in Example 6.11.
1,5,6,7. In these cases A(F) = B(F) = F, hence the extension procedure
for P is trivial.
2. In this case A(F) = B(F) is the collection of all events in S, hence
the extension of P to A(F) is the only non-trivial one. For example, for
E = {HT, TH} we have P(E) = 1/4 + 1/4 = 1/2.
3. In this case we get an extension of the probability measure P to all
Borel sets in [0, 1]. In particular, the probability measure of the Cantor set
is deﬁned as well (and in fact is equal to zero). We note here that one can
construct certain Borel sets similar to the Cantor set for which the probabil-
ity measure is non-zero. Not every subset of [0, 1] is Borel, and we will show
in Example 6.16 that P cannot be reasonably extended to all subsets of [0, 1].
4. In this case we obtain an extension of the probability measure P to the
σ-algebra B(F) that, as in Part 3 above, one can show to be strictly smaller
than the σ-algebra of all events in S.

6.2 Probability Measure
157
We remark here that the probability space from Part 4 of Example 6.14 can be
taken as a deﬁnition of a Markov chain. Similarly, the probability spaces from
Parts 5 and 6 can be taken respectively as deﬁnitions of a non-trivially con-
nected Markov chain with an end state and an HMM for which the underlying
Markov chain has an end state and is non-trivially connected.
While for the purposes of the probability theory it will be generally suﬃ-
cient to consider the extension of the probability measure only to B(F), it is
interesting to remark that it can in fact be extended beyond events in B(F),
to include so-called Lebesgue measurable events.
Deﬁnition 6.15. An event E ⊂S is called Lebesgue measurable if for any
ε > 0 there exists E0 ∈A(F) such that P ∗(E∆E0) < ε. The collection of all
Lebesgue measurable events is denoted by L(F).
When considering P ∗on L(F) we will drop the superscript ∗. One can
show that B(F) ⊂L(F), L(F) is a σ-algebra, and P is a probability measure
on L(F) which is called the Lebesgue extension of the original probability
measure. Of course, for the probability measures deﬁned in Parts 1 and 2 of
Example 6.11, B(F) and L(F) coincide, but often L(F) is much larger than
B(F). For instance, in Part 3 of Example 6.11, L(F) is substantially larger
than B(F). In particular, one can show that not every subset of the Cantor
set belongs to B(F), but every such subset belongs to L(F).
Despite the fact that one can extend every probability measure to the large
σ-algebra L(F), one cannot hope that this extension deﬁnes the probability
of every event E ⊂S, as the following example shows.
Example 6.16. Consider the probability measure deﬁned in Part 3 of Example
6.11 and extend it to the corresponding σ-algebra L(F). Deﬁne on S = [0, 1]
the equivalence relation: x ∼y, if x −y ∈Q. Let E ⊂(0, 1] be the event that
contains one element from each equivalence class. For r ∈(0, 1] deﬁne
Er =

(r + E) ∪(r −1 + E)

∩(0, 1].
(6.8)
We have
(0, 1] =

r∈Q∩(0,1]
Er,
and the events Er are pairwise disjoint for r ∈Q ∩(0, 1].
If we suppose that E ∈L(F), then Er ∈L(F) for all r ∈Q ∩(0, 1], and
P(Er) = P(E). But then the σ-additivity of P on L(F) gives
1 =

r∈Q∩(0,1]
P(E),
which is impossible. Hence E ̸∈L(F), that is, E is a non-measurable event;
one cannot set the value P(E) in a reasonable way.

158
6 Elements of Probability Theory
Example 6.16 shows that it is impossible to deﬁne a non-negative set func-
tion P for all events in S = [0, 1] in such a way that: (i) for the translates Er
of E ⊂[0, 1] deﬁned in (6.8) P(Er) = P(E); (ii) P is σ-additive.
We remark that the Lebesgue extension of the probability measure deﬁned
in Part 4 of Example 6.11 is also deﬁned on a σ-algebra that generally does not
include all possible events in S. In fact, if p0j ̸= 0 and pij ̸= 0 for all i, j ∈X,
one can establish a natural almost one-to-one correspondence between the
elements of the σ-algebras L(F) arising from Parts 3 and 4 of Example 6.11.
Thus, in this situation Example 6.16 can be used to produce a Lebesgue non-
measurable event for the probability measure deﬁned in Part 4 of Example
6.11 as well.
Originally we assumed that the probability measure P is deﬁned only on
a semi-algebra F. As we have seen, P can always be extended to the minimal
σ-algebra B(F). For the purposes of the probability theory it is usually not
necessary to extend P beyond B(F), and therefore we will assume from now
on that any probability measure that we will consider is deﬁned on a σ-algebra
and ignore any possible extensions of the probability measure beyond the σ-
algebra. Thus, we will be dealing with general probability spaces, that is, triples
of the form (S, B, P), where S is a sample space, B is a σ-algebra of events
in S and P is a probability measure deﬁned for all events in B. Sometimes
for simplicity we say that P is a probability measure on the sample space S,
meaning that P is in fact deﬁned on a certain σ-algebra B of events in S.
Any event outside of B will be considered non-measurable (even if a value of
probability can be assigned to it by performing the Lebesgue extension), and
events in B will be called measurable.
We will now list some useful properties of probability measures:
(i) for any E ∈B we have P(E) + P(Ec) = 1,
(ii) for any E1, E2 ∈B such that E1 ⊂E2 we have P(E1) ≤P(E2),
(iii) for any E1, E2 ∈B we have P(E1 ∪E2) = P(E1) + P(E2) −P(E1 ∩E2),
(iv) if {Ej} is a sequence of non-decreasing events in B, that is, Ej ⊂Ej+1
for all j, then we have limj→∞P(Ej) = P

∪∞
j=1Ej

,
(v) if {Ej} is a sequence of non-increasing events in B, that is, Ej+1 ⊂Ej for
all j, then we have limj→∞P(Ej) = P

∩∞
j=1Ej

,
(vi) if {Ej} is a sequence of events in B and E ∈B is such that E ⊂∪∞
j=1Ej,
then we have P(E) ≤∞
j=1 P(Ej).

6.3 Conditional Probability
159
We will now mention one useful way of constructing new probability
spaces from existing ones. Let (Sj, Bj, Pj), for j = 1, . . . , n, be a collection
of probability spaces. Let the new sample space be S = S1 × . . . × Sn. For
every event E ⊂S of the form E = E1 × . . . × En, with Ej ∈Bj, deﬁne
P(E) = P1(E1) × . . . × Pn(En). All such events E form a semi-algebra F of
events in S, and it can be checked directly that P is a probability measure
on F. It can be extended to the σ-algebra B = B(F), and we obtain the
probability space (S, B, P). It is called the Cartesian product of the probabil-
ity spaces (Sj, Bj, Pj), j = 1, . . . , n and is denoted by n
j=1(Sj, Bj, Pj). If the
probability spaces (Sj, Bj, Pj) are all the same for j = 1, . . . , n, and coincide
with say a space (S′, B′, P ′), then we will call the Cartesian product (S, B, P)
the nth Cartesian power of (S′, B′, P ′) and denote it by (S′, B′, P ′)n. In this
case we will also call S the nth Cartesian power of S′ and denote it by S′n.
Suppose now that we have a sequence {(Sj, Bj, Pj)} of probability spaces.
We will now construct the Cartesian product in this situation. Set S =
∞
j=1 Sj, that is, let S consist of sequences e = {ej} with ej ∈Sj. For
any m and any set of indices i1 < i2 < . . . < im deﬁne the projection
hi1,...,im : S →m
j=1 Sij as follows
hi1,...,im(e) = (ei1, . . . , eim).
If E′ ⊂m
j=1 Sij is a measurable event in the sense of the Cartesian product
m
j=1(Sij, Bij, Pij), then we call the event E = h−1
i1,...,im(E′) a cylinder event.
We will now set P(E) = P(E′), where P(E′) is computed in the sense of the
Cartesian product m
j=1(Sij, Bij, Pij). We also set P(S) = 1. The collection
F of events in S that consists of all cylinder events and S itself, can be shown
to be an algebra. One can prove that P is a probability measure on F, and
therefore it can be extended to the σ-algebra B = B(F). Thus we obtain a
probability space (S, B, P). It is called the Cartesian product of the sequence
of probability spaces {(Sj, Bj, Pj)} and is denoted by ∞
j=1(Sj, Bj, Pj). If the
probability spaces (Sj, Bj, Pj) are all the same for all j and coincide with a
space (S′, B′, P ′), then we will call the Cartesian product (S, B, P) the inﬁnite
Cartesian power of (S′, B′, P ′) and denote it by (S′, B′, P ′)∞. In this case we
will also call S the inﬁnite Cartesian power of S′ and denote it by S′∞.
6.3 Conditional Probability
Let (S, B, P) be a probability space and E an event in B such that P(E) > 0.
Consider the triple (E, BE, PE), where BE consists of those elements of B
that are contained in E, and for E0 ∈BE we deﬁne PE(E0) = P(E0)/P(E).
One can check directly that BE is a σ-algebra and that PE is a probability
measure on BE, hence the triple (E, BE, PE) is a probability space. In this
reduced probability space E represents the event that always occurs.

160
6 Elements of Probability Theory
By using the reduced probability space (E, BE, PE), for any event E′ ∈B
one can deﬁne the probability of E′ under the condition that the event E
occurs, as the value of PE calculated for the portion of E′ contained in E.
Deﬁnition 6.17. The conditional probability of the event E′ ∈B given that
the event E ∈B occurs is the following number
P(E′|E) = PE(E′ ∩E) = P(E′ ∩E)
P(E)
.
Conditional probability leads to the important concept of independence
of two events which takes place when the conditional probability P(E′|E)
coincides with P(E′), that is, when the value of the probability of E′ does not
depend on whether or not E occurs.
Deﬁnition 6.18. Two events E, E′ ∈B with P(E) > 0 and P(E′) > 0 are
called independent, if P(E′|E) = P(E′), or, equivalently P(E|E′) = P(E).
To incorporate events with zero probabilities a broader deﬁnition of indepen-
dence is usually used.
Deﬁnition 6.19. Two events E, E′ ∈B are called independent, if P(E′∩E) =
P(E′)P(E).
Clearly, if either P(E) = 0 or P(E′) = 0, then E and E′ are independent. Note
that independence of events depends on a particular probability measure and
that two events that are independent for one choice of probability measure
may not be independent for another.
Example 6.20.
1. Consider the probability space from Part 2 of Example 6.14 and choose
E1 = {HH, HT}, E2 = {HT, TH}. We have P(E1 ∩E2) = P({HT}) = 1/4
and P(E1)P(E2) = (1/4 + 1/4) × (1/4 + 1/4) = 1/2 × 1/2 = 1/4, hence E1
and E2 are independent.
Suppose now that the coin used to conduct the experiments is not fair
and that it is reasonable to deﬁne a probability measure on F as follows:
P(S) = 1, P(∅) = 0, P({HH}) = 9/16, P({HT}) = 3/16, P({TH}) = 3/16,
P({TT}) = 1/16. In this case P(E1 ∩E2) = P({HT}) = 3/16 whereas
P(E1)P(E2) = (9/16 + 3/16) × (3/16 + 3/16) = 3/4 × 3/8 = 9/32, hence E1
and E2 are not independent for this probability measure.
2. Let the sample space S to be the closed rectangle in the (x, y)-plane
with vertices (0, 0), (0, 1), (3/4, 0) and (3/4, 1). Let F be the semi-algebra
that consists of all rectangles in S with sides parallel to the x- and y-axes

6.3 Conditional Probability
161
F =

(a, b) × (c, d); [a, b) × (c, d); (a, b] × (c, d); [a, b] × (c, d);
(a, b) × [c, d); [a, b) × [c, d); (a, b] × [c, d); [a, b] × [c, d);
(a, b) × (c, d]; [a, b) × (c, d]; (a, b] × (c, d]; [a, b] × (c, d];
(a, b) × [c, d]; [a, b) × [c, d]; (a, b] × [c, d]; [a, b] × [c, d],
0 ≤a ≤b ≤3
4, 0 ≤c ≤d ≤1
 
.
For a rectangle E in F deﬁne P(E) to be equal to the sum of the area of
the intersection E ∩{[0, 1/2] × [0, 1]} and twice the area of the intersection
E∩{[1/2, 3/4]×[0, 1]}. One can check directly that P is a probability measure
on F, and therefore one can extend it to the minimal σ-algebra B(F).
Consider two events: E1 = {(x, y) ∈S : x + y ≤1/2} and E2 =
{(x, y) ∈S : 1/8 ≤x ≤19/32, 0 ≤y ≤1}. It is not hard to see that E1, E2 ∈
B(F), and that P(E1 ∩E2) = 9/128, P(E1)P(E2) = 1/8 × 9/16 = 9/128,
which shows that E1 and E2 are independent.
Consider another pair of events: E3 = {(x, y) ∈S : x + 2y > 1} and E4 =
{(x, y) ∈S : y ≥x}. Again, E3, E4 ∈B(F), and we have P(E3∩E4) = 69/144
and P(E3)P(E4) = 23/32×9/16 = 207/512, which shows that E3 and E4 are
not independent.
The concept of independence in Deﬁnition 6.19 can be extended to any
ﬁnite number of events.
Deﬁnition 6.21. Events E1, . . . , En ∈B are called independent if for any
collection of indices 1 ≤i1 < i2 < . . . < ik ≤n we have
P
⎛
⎝
k
j=1
Eij
⎞
⎠=
k

j=1
P(Eij).
As the following example shows, it is not suﬃcient for independence of
E1, . . . , En that all the events are pairwise independent (that is, for every
1 ≤i < j ≤n the events Ei and Ej are independent).
Example 6.22. Suppose a fair die is thrown twice. In this case the sample space
is
S = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5),
(2, 6), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (4, 1), (4, 2), (4, 3), (4, 4),
(4, 5), (4, 6), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (6, 1), (6, 2), (6, 3),
(6, 4), (6, 5), (6, 6)}.
Since the die is fair we deﬁne the probability of each elementary event to be
1/36 and extend this probability measure to the algebra of all events in S.
Let

162
6 Elements of Probability Theory
E1 = {(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5),
(3, 6), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6)},
E2 = {(1, 1), (1, 3), (1, 5), (2, 1), (2, 3), (2, 5), (3, 1), (3, 3), (3, 5), (4, 1), (4, 3),
(4, 5), (5, 1), (5, 3), (5, 5), (6, 1), (6, 3), (6, 5)},
E3 = {(1, 2), (1, 4), (1, 6), (2, 1), (2, 3), (2, 5), (3, 2), (3, 4), (3, 6), (4, 1), (4, 3),
(4, 5), (5, 2), (5, 4), (5, 6), (6, 1), (6, 3), (6, 5)}.
Clearly, P(E1) = P(E2) = P(E3) = 1/2, hence P(Ei)P(Ej) = 1/4 for all
i ̸= j. Also, P(E1 ∩E2) = P(E1 ∩E3) = P(E2 ∩E3) = 1/4 and hence the
events E1, E2, E3 are pairwise independent. However, P(E1 ∩E2 ∩E3) = 0
since E1 ∩E2 ∩E3 = ∅whereas P(E1)P(E2)P(E3) = 1/8. Hence E1, E2, E3
are not independent.
6.4 Random Variables
For a given probability space (S, B, P) we will often consider real-valued func-
tions on S. We will be interested in a particular class of functions as deﬁned
below.
Deﬁnition 6.23. A function f : S →R is called a random variable on the
probability space (S, B, P), if for every b ∈R, the event Eb(f) = {e ∈S :
f(e) ≤b} is measurable (that is, belongs to B).
It is not hard to show that f is a random variable if and only if f −1(E) ∈B for
every Borel set E in R (Borel sets in R are deﬁned analogously to Borel sets
in [0, 1] – see Sect. 6.7). The property for a function to be a random variable
depends on the choice of B. The same function can be a random variable for
one choice of B and fail to be a random variable for another. We will now
give examples of random variables as well as functions that are not random
variables.
Example 6.24.
1. Let S consist of all possible hands of 7 cards (it contains
 25
7

elemen-
tary events). Let B be the σ-algebra of all events in S and, assuming that all
hands are equiprobable, for an event E ∈B deﬁne P(E) to be the number of
elementary events in E divided by

25
7

. For each hand e ∈S deﬁne f(e)
to be the number of aces contained in e. Clearly, f is a random variable on
(S, B, P) with integer values between 0 and 4.
2. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14
and let f be any real-valued continuous function on S = [0, 1]. Then for every
b ∈R the event Eb(f) is a closed subset of [0, 1] and hence a Borel set (that
is, a set in B(F)). Therefore f is a random variable on (S, B(F), P).

6.5 Integration of Random Variables
163
Consider now the event E constructed in Example 6.16. Let f : [0, 1] →R
be the function that takes value 0 on E and value 1 on [0, 1] \ E. Clearly, in
this case E1/2(f) = E and hence does not belong to B(F). Therefore f is not
a random variable on (S, B(F), P). In fact, it is not a random variable even
on (S, L(F), P).
3. Let the sample space S be as in Part 2 of Example 6.1 and let B be the
following σ-algebra
B = {S, ∅, {HH, HT}, {TH, TT}}.
Deﬁne a probability measure P on B as follows
P(S) = 1,
P(∅) = 0,
P({HH, HT}) = P({TH, TT}) = 1
2.
For each e ∈S deﬁne f(e) to be the number of H′s in e. Then we have
E1(f) = {HT, TH, TT}.
Since E1(f) ̸∈B, f is not a random variable on (S, B, P). However, f becomes
a random variable if we choose B = B(F), where F is as in Part 2 of Example
6.11.
6.5 Integration of Random Variables
We will now discuss integration of random variables over an event. We will
start with random variables that take only countably many values.
Deﬁnition 6.25. A function f : S →R is called simple if the number of
values that it takes is countable.
The functions from Parts 1 and 3 of Example 6.24 take only ﬁnitely many
values and hence are simple.
Let f be a simple function and let r1, r2, . . . be its values, ri ̸= rj for i ̸= j.
It is easy to show that f is a random variable if and only if each of the events
E′
rj(f) = {e ∈S : f(e) = rj}
is measurable.
Deﬁnition 6.26. A random variable that is at the same time a simple func-
tion is called discrete.
The function from Part 1 of Example 6.24 and the function from Part 3 of
Example 6.24 with B = B(F) are discrete random variables.

164
6 Elements of Probability Theory
Deﬁnition 6.27. Suppose that a discrete random variable f on a probability
space (S, B, P) takes values r1, r2, . . ., with ri ̸= rj for i ̸= j, and let E be an
event in B. Then f is called integrable on E with respect to P, if the series
∞

j=1
rjP(E ∩E′
rj(f))
(6.9)
converges absolutely, that is, if the series
∞

j=1
!!!rjP(E ∩E′
rj(f))
!!!
converges. If f is integrable over E, the sum of series (6.9) is called the integral
of f over E with respect to P and is denoted by
"
E f(e) dP(e), or simply
"
E f dP.
Note that if a discrete random variable takes only ﬁnitely many values on S,
then it is integrable on any E ∈B.
We will now deﬁne integrability for not necessarily discrete random vari-
ables. This will be done by means of approximating a random variable by
discrete random variables. The sort of approximation we require is described
in the following deﬁnition. Note that for the purposes of the deﬁnition the
functions involved do not need to be random variables and do not need to be
deﬁned on all of S.
Deﬁnition 6.28. Let f and {fj} be real-valued functions on E ⊂S. We say
that the sequence {fj} converges to f uniformly on E, if for every ε > 0 there
exists a number J ∈N such that |fj(e) −f(e)| ≤ε for all j ≥J and all e ∈E.
We will now give a general deﬁnition of integrability.
Deﬁnition 6.29. If f is a random variable on a probability space (S, B, P)
and E ∈B, then f is called integrable on E with respect to P if there exists
a sequence of discrete random variables {fj} integrable on E with respect to
P that converges to f uniformly on E.
We remark that if a sequence of random variables converges to a function f
uniformly on S, then f is itself a random variable. Further, for any random
variable f one can ﬁnd a sequence of discrete random variables that uniformly
converges to f on all of S (and hence on any E ⊂S). Indeed, for j ∈N and
k ∈Z deﬁne
Ekj(f) =

e ∈S : k
j ≤f(e) < k + 1
j
 
(6.10)
and set
fj(e) = k
j ,
if
e ∈Ekj(f).
(6.11)

6.5 Integration of Random Variables
165
Clearly, fj are discrete random variables. Further, |fj(e) −f(e)| ≤1/j for all
e ∈S and hence {fj} converges to f uniformly on S. However, the random
variables fj may not be integrable on E, and it is the existence of a sequence
of integrable random variables that is required in Deﬁnition 6.29.
Suppose that {fj} is a sequence of discrete random variables integrable on
E that converges uniformly on E to a random variable f as in Deﬁnition 6.29.
We will show that the sequence of integrals xj =
"
E fj dP also has a limit. If
P(E) = 0, then xj = 0 for all j and {xj} obviously converges. Assume that
P(E) > 0. It is suﬃcient to prove that {xj} is a fundamental sequence, that
is, for every δ > 0 there exists J ∈N such that |xk −xl| ≤δ for all k, l ≥J.
We have
|xk −xl| =
!!!!
#
E
fk dP −
#
E
fl dP
!!!! =
!!!!
#
E
(fk −fl) dP
!!!!
≤supe∈E |fk(e) −fl(e)| P(E)
= supe∈E |(fk(e) −f(e)) −(fl(e) −f(e))| P(E)
≤(supe∈E |fk(e) −f(e)| + supe∈E |fl(e) −f(e)|) P(E).
(6.12)
If in Deﬁnition 6.28 we choose J corresponding to ε = δ/(2P(E)), then from
(6.12) we obtain that for k, l ≥J the following holds
|xk −xl| ≤2εP(E) = δ,
which shows that {xj} is indeed a fundamental sequence.
Let x = limj→∞xj. Suppose now that {f ′
j} is another sequence of discrete
random variables integrable on E that converges uniformly on E to f. Set
x′
j =
"
E f ′
j dP and let x′ = limj→∞x′
j. We will show that x′ = x. Indeed,
suppose that x′ ̸= x. Consider the sequence {f ′′
j } obtained by taking the
union of the sequences {fj} and {f ′
j}. Clearly, {f ′′
j } converges uniformly on E
to f and therefore the sequence of integrals x′′
j =
"
E f ′′
j dP also has a limit. On
the other hand, it cannot have a limit since the subsequences {xj} and {x′
j}
converge to diﬀerent numbers. This contradiction shows that in fact x′ = x.
Hence, we have just shown that for any sequence of discrete random vari-
ables integrable on E that converges uniformly on E to a random variable f,
the sequence of integrals over E also converges, and the limit of the sequence of
integrals does not depend on the choice of approximating sequence of discrete
random variables. We therefore will call the limit of the sequence of integrals
the integral of f over E with respect to P and denote it by
"
E f(e) dP(e), or
simply
"
E f dP.
One very useful suﬃcient condition for the integrability of a random vari-
able f on E ∈B is the boundedness of f on E, that is, the existence of a
number M such that |f(e)| ≤M for all e ∈E. Indeed, suppose that f is

166
6 Elements of Probability Theory
bounded on E. Consider the sequence {fj} of discrete random variables con-
structed in (6.10), (6.11). Since f is bounded, each fj takes only ﬁnitely many
values on E and thus is integrable on E. Thus we have found a sequence of
discrete random variables integrable on E that converges uniformly on E to
f. Therefore f is integrable on E. As we will see below, boundedness is not a
necessary condition for integrability, there are integrable and non-integrable
unbounded random variables.
We will often say that a condition holds almost certainly, or a.c. on S, if it
holds for all elementary events in S except possibly for elementary events in an
event of probability 0. For example, we say that two random variables f and g
coincide almost certainly if P ({e ∈S : f(e) ̸= g(e)}) = 0. Clearly, if two such
random variables are integrable, then the corresponding integrals coincide. In
fact, in order to speak about the integrability of a random variable on an event
E it is suﬃcient to have the random variable deﬁned almost certainly on E.
One can deﬁne the random variable on the rest of E arbitrarily, and doing
so does not aﬀect either the integrability of the random variable or the value
of the integral. Note, however, that we always assume for simplicity that all
random variables are deﬁned everywhere on S.
Clearly, if a random variable is integrable on S, it is integrable on any
E ∈B. The integral of a random variable over the whole sample space S has
a special name and will be of great importance later on.
Deﬁnition 6.30. Let f be a random variable on a probability space (S, B, P).
Suppose that f is integrable on S. The expected value of f or the mean value
of f or simply the mean of f is the integral
E(f) =
#
S
f dP.
We will also introduce two more important characteristics of random vari-
ables.
Deﬁnition 6.31. Let f be a random variable on a probability space (S, B, P).
Suppose that both f and f 2 are integrable on S. The variance of f is deﬁned
as
V ar(f) = E

(f −E(f))2
= E

f 2
−E(f)2,
and the standard deviation of f is deﬁned as
σ(f) =
$
V ar(f).
We will now list without proof some important properties of integrals. In
the formulas below E is a measurable event.
(i) If c ∈R, then
"
E c dP = cP(E).

6.5 Integration of Random Variables
167
(ii) If f is integrable on E and c ∈R, then cf is integrable on E and
"
E cf dP = c
"
E f dP.
(iii) If f and g are integrable on E, then f + g are integrable on E and
"
E(f + g) dP =
"
E f dP +
"
E g dP.
(iv) If f and g are integrable on E and f(e) ≤g(e) for all e ∈E, then
"
E f dP ≤
"
E g dP. In particular, if m ≤f(e) ≤M for all e ∈E, then
mP(E) ≤
"
E f dP ≤MP(E).
(v) A random variable f is integrable on E if and only if |f| is integrable on
E. In this case
!!"
E f dP
!! ≤
"
E |f| dP.
(vi) If |f(e)| ≤ϕ(e) for all e ∈E and ϕ is integrable on E, then f is integrable
on E.
(vii) The Cauchy-Bunyakowski Inequality. If f 2 and g2 are integrable on
E, then fg is also integrable on E and
#
E
fg dP
2
≤
#
E
f 2 dP
#
E
g2 dP.
The above inequality becomes an equality if and only if f and g are almost
certainly proportional on E. Note that the Cauchy-Bunyakowski inequality
with g ≡1 implies that f is integrable on E if f 2 is integrable on E. This
observation shows that if the variance of a random variable exists, then the
mean exists as well. In particular, in Deﬁnition 6.31 above it is suﬃcient to
assume only that f 2 is integrable on S.
(viii) The Triangle Inequality. If f 2 and g2 are integrable on E, then
(f + g)2 is also integrable on E and
%#
E
(f + g)2 dP ≤
%#
E
f 2 dP +
%#
E
g2 dP.
We will now give examples of integrable and non-integrable random vari-
ables and calculate some integrals.
Example 6.32.
1. Consider the probability space from Part 2 of Example 6.14, and let f
be the random variable deﬁned in Part 3 of Example 6.24. Since f takes only
ﬁnitely many values, f 2 is integrable on any E ∈B. We will ﬁnd E(f), V ar(f)
and σ(f).
The values of f are 0, 1, 2 and we have

168
6 Elements of Probability Theory
E′
0(f) = {TT},
E′
1(f) = {HT, TH},
E′
2(f) = {HH}.
Therefore
E(f) =
#
S
f dP = 0 × P(E′
0) + 1 × P(E′
1) + 2 × P(E′
2) = 1 × 1
2 + 2 × 1
4 = 1.
Further
V ar(f) = E(f 2) −1 = 0 × P(E′
0) + 1 × P(E′
1) + 4 × P(E′
2) −1
= 1 × 1
2 + 4 × 1
4 −1 = 1
2,
and therefore σ(f) = 1/
√
2.
We will now compute the integral of f over a subset E ⊂S. Let E =
{HT, TH, TT}. Then E ∩E′
0 = E′
0, E ∩E′
1 = E′
1, E ∩E′
2 = ∅. Therefore
#
E
f dP = 0 × P(E′
0) + 1 × P(E′
1) + 2 × P(∅) = 1 × P(E′
1) = 1 × 1
2 = 1
2.
2. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Consider the following function
f(e) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
if e = 0,
n2 + n
2n
,
if
1
n + 1 < e ≤1
n, for n ∈N.
Clearly, f is a simple function that takes inﬁnitely many values: 0 and (n2 +
n)/2n for n ∈N. We have
E′
0(f) = {0},
E′
(n2+n)/2n(f) =

1
n + 1, 1
n
&
.
All these sets clearly belong to B(F) and therefore f is a discrete random
variable. We will now show that f is integrable on S. Indeed, consider the
series from Deﬁnition 6.27
0 × P({0}) +
∞

n=1
n2 + n
2n
× P

1
n + 1, 1
n
&
=
∞

n=1
n2 + n
2n
×
1
n2 + n =
∞

n=1
1
2n .
The series ∞
n=1 1/2n clearly converges absolutely and its sum is equal to 1.
Therefore f is integrable on S, and E(f) = 1.

6.5 Integration of Random Variables
169
We will now ﬁnd the integral of f over E = [0, 3/4]. We have
E ∩E′
0(f) = {0},
E ∩E′
1(f) =
1
2, 3
4
&
,
E ∩E′
(n2+n)/2n(f) =

1
n + 1, 1
n
&
,
for n ≥2.
Therefore by Deﬁnition 6.27 we have
#
E
f dP = 0 × P({0}) + 1 × P
1
2, 3
4
&
+
∞

n=2
n2 + n
2n
× P

1
n + 1, 1
n
&
= 1
4 +
∞

n=2
1
2n = 1
4 + 1
2 = 3
4.
3. Let (S, B(F), P) be the probability space as in Part 2 above and let f
be a continuous function on S = [0, 1]. Since f is continuous, it is a random
variable. Further, since f is continuous, it is bounded on S and hence inte-
grable on S. Next, it is not hard to prove that
"
S f dP is equal to
" 1
0 f(e) de,
the usual Riemann integral of f. In fact, for any E = [a, b] with 0 ≤a < b ≤1
we have
"
E f dP =
" b
a f(e) de.
Let, for example, f(e) = e2. Then we have
E(f) =
# 1
0
e2 de = e3
3
!!!!!
1
0
= 1
3,
V ar(f) = E

e2 −1
3
2
= E

e4 −2
3e2 + 1
9

=
# 1
0

e4 −2
3e2 + 1
9

de
=
e5
5 −2e3
9
+ e
9
 !!!!!
1
0
= 1
5 −2
9 + 1
9 = 4
45,
σ(f) =
2
3
√
5,
and for E = [1/4, 1/2] we have
#
E
f dP =
# 1/2
1/4
e2 de = e3
3
!!!!!
1/2
1/4
=
7
192.

170
6 Elements of Probability Theory
4. Let (S, B(F), P) be the probability space as in Part 2 above and let
f(e) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
if e = 0,
1
√e,
if 0 < e ≤1.
The function f is not continuous on S, but is nevertheless a random variable.
Indeed, we have
Eb(f) = {0} ∪
' 1
b2 , 1
&
, if b > 0,
E0(f) = {0},
Eb(f) = ∅,
if b < 0,
and hence Eb(f) ∈B(F) for all b ∈R, that is, f is a random variable. We
approximate f by the functions constructed in (6.10), (6.11). For the function
fj we have
E′
k/j(fj) =

j2
(k + 1)2 , j2
k2
(
, if k ≥j,
E′
0(fj) = {0},
E′
k/j(fj) = ∅,
if 0 < k < j or k < 0.
Calculating series (6.9) for fj we obtain the following series
∞

k=j
k
j × P

j2
(k + 1)2 , j2
k2
(
= j
∞

k=j
2k + 1
k(k + 1)2 ,
which converges absolutely. Hence fj is integrable on S, and we have found
a sequence of discrete random variables integrable on S that uniformly on S
converges to f. Therefore f is integrable on S.
We also have
#
S
fj dP = j
∞

k=j
2k + 1
k(k + 1)2 .
(6.13)
By deﬁnition, E(f) = limj→∞
"
S fj dP and hence to calculate E(f) we need
to ﬁnd the limit of the expressions in the right-hand side of formula (6.13) as
j →∞. We have

6.5 Integration of Random Variables
171
j
∞

k=j
2k + 1
k(k + 1)2 = j
⎛
⎝2
∞

k=j
1
(k + 1)2 +
∞

k=j
1
k(k + 1)2
⎞
⎠.
Since
0 ≤j
∞

k=j
1
k(k + 1)2 ≤
∞

k=j
1
(k + 1)2
and the series ∞
k=1 1/(k + 1)2 converges, we have
lim
j→∞j
∞

k=j
1
k(k + 1)2 = 0,
and hence we only need to determine the limit of 2j ∞
k=j 1/(k+1)2 as j →∞.
The following estimates hold
∞

k=j
1
(k + 1)2 ≥
# ∞
j+1
dx
x2 = −1
x
!!!!!
∞
j+1
=
1
j + 1,
∞

k=j
1
(k + 1)2 ≤
# ∞
j
dx
x2 = −1
x
!!!!!
∞
j
= 1
j ,
where the integrals involved are understood as Riemann improper integrals.
Hence
2j
j + 1 ≤2j
∞

k=j
1
(k + 1)2 ≤2,
and therefore limj→∞2j ∞
k=j 1/(k + 1)2 = 2. Thus E(f) = 2.
5. Let (S, B(F), P) be the probability space as in Part 2 above and let
f(e) =
⎧
⎪
⎨
⎪
⎩
0,
if e = 0,
1
e,
if 0 < e ≤1.
The function f is not continuous on S, but is nevertheless a random variable.
Indeed, we have
Eb(f) = {0} ∪
'1
b , 1
&
, if b > 0,
E0(f) = {0},
Eb(f) = ∅,
if b < 0,

172
6 Elements of Probability Theory
and hence Eb(f) ∈B(F) for all b ∈R, that is, f is a random variable.
We will show that f is not integrable on S. Consider the following discrete
random variable
g(e) =
⎧
⎪
⎨
⎪
⎩
0,
if e = 0,
n,
if
1
n + 1 < e ≤1
n, n ∈N.
Clearly, g is a discrete random variable and |g(e)| ≤f(e) for all e ∈S. There-
fore, by property (vi) of integrals, if f were integrable, g would be integrable
as well. For g we ﬁnd
E′
n(g) =

1
n + 1, 1
n
&
,
n ∈N,
E′
0(g) = {0}.
Hence series (6.9) for g is
∞

n=1
n × P

1
n + 1, 1
n
&
=
∞

n=2
1
n,
which is a divergent series. Thus f is not integrable on S. In particular, for
the random variable from Part 4 above, the variance does not exist.
6.6 Monotone Functions on the Real Line
In our study of random variables in the forthcoming sections we will rely on a
number of properties of certain functions on R. In this section we will list all
the required facts, mostly without proofs. At the same time we will present
several important constructions some of which resemble those already given
for probability measures and random variables.
We will start with constructing the Lebesgue measure on R. This measure
serves to indicate the size of a subset of R in the same way as the probability
measure is used for events. First, we will construct the Lebesgue measure
on [0, 1]. In fact, this has already been done in Sect. 6.2. Recall Part 3 of
Example 6.11 and extend the probability measure P to the σ-algebra L(F)
of all Lebesgue measurable events in [0, 1] (see Deﬁnition 6.15). The extended
probability measure on L(F) is called the Lebesgue measure on [0, 1]. In the
same way we can construct the Lebesgue measure on every segment [n, n+1],
with n ∈Z. Denote the Lebesgue measure on [n, n + 1] by µn.
Deﬁnition 6.33. A set A ⊂R is called Lebesgue measurable if for every
n ∈Z the intersection A ∩[n, n + 1] is Lebesgue measurable in [n, n + 1]. In
this case the Lebesgue measure of A is

6.6 Monotone Functions on the Real Line
173
µ(A) =
∞

n=−∞
µn(A ∩[n, n + 1]).
The collection of all Lebesgue measurable sets in R is denoted by L(R).
One can verify that L(R) is a σ-algebra where R plays the role of S and that
µ is σ-additive on L(R). Note that µ can take inﬁnite values: for example
µ(R) = µ([1, ∞)) = ∞.
Next, we will consider certain real-valued functions on R.
Deﬁnition 6.34. A function f : R →R is called Lebesgue measurable or
simply measurable if for every b ∈R the set Ab(f) = {x ∈R : f(x) ≤b} is
Lebesgue measurable (that is, belongs to L(R)).
It is not hard to show that f is Lebesgue measurable if and only if f −1(E) is
Lebesgue measurable for every Borel set E in R (see Sect. 6.7). Clearly, mea-
surable functions are analogues of random variables for the Lebesgue measure.
We will now discuss the integrability of measurable functions on Lebesgue
measurable sets in R. Let ﬁrst A ∈L(R) be a set of ﬁnite measure, µ(A) < ∞.
Then the deﬁnition of integrability is identical to that for random variables.
We ﬁrst deﬁne the integrability of a simple measurable function as in Def-
inition 6.27 and then treat arbitrary measurable functions as in Deﬁnition
6.29. The Lebesgue integral of a function f over the set A is denoted by
"
A f(x) dµ(x), or simply
"
A f dµ. All properties of integrals with respect to
probability measures stated in Sect. 6.5 remain true for Lebesgue integrals
over sets of ﬁnite measure.
We will now brieﬂy compare the classes of Lebesgue and Riemann inte-
grable functions on a segment. If f is Riemann integrable on [a, b], then f
is also Lebesgue integrable on [a, b], and
"
[a,b] f dµ =
" b
a f(x) dx. However,
there are many functions on [a, b] that are Lebesgue integrable, but are not
Riemann integrable. Suppose now that f is not Riemann integrable on [a, b],
but the improper Riemann integral of f exists over [a, b]. Then f may not
be Lebesgue integrable on [a, b], but if the improper Riemann integral of |f|
over [a, b] exists, then f is Lebesgue integrable on [a, b]. For example, let f be
unbounded near a, Riemann integrable on every segment [a + ε, b], and sup-
pose that the improper Riemann integral
" b
a |f(x)| dx = limε→0
" b
a+ε |f(x)| dx
exists. Then the improper Riemann integral
" b
a f(x) dx = limε→0
" b
a+ε f(x) dx
also exists, f is Lebesgue integrable on [a, b], and
"
[a,b] f dµ =
" b
a f(x) dx.
Suppose now that A ∈L(R) has inﬁnite measure, µ(A) = ∞. Let An =
A ∩[−n, n] for n ∈N. Then An ∈L(R), µ(An) < ∞, An ⊂An+1 and
A = ∪∞
n=1An.
Deﬁnition 6.35. A sequence {Xn} of subsets of A ∈L(R) is called exhaust-
ing if Xn ∈L(R), µ(Xn) < ∞, Xn ⊂Xn+1 for all n ∈N, and A = ∪∞
n=1Xn.

174
6 Elements of Probability Theory
As we have seen, any A ∈L(R) has an exhausting sequence of subsets.
Now we can give a deﬁnition of integrability over subsets of inﬁnite mea-
sure.
Deﬁnition 6.36. Let A ∈L(R), µ(A) = ∞, and suppose that f : R →R
is measurable. Then f is called Lebesgue integrable on A if f is Lebesgue
integrable over any B ⊂A with B ∈L(R), µ(B) < ∞, and for any exhausting
sequence of subsets {Xn} of A the limit
lim
n→∞
#
Xn
f dµ
exists and is independent of the sequence. This limit is called the Lebesgue
integral of f over A and is denoted by
"
A f(x) dµ(x), or simply
"
A f dµ.
Any function f can be represented as a diﬀerence of two non-negative
functions f = f+ −f−, where
f+(x) =
⎧
⎨
⎩
f(x),
if f(x) ≥0,
0,
if f(x) < 0,
f−(x) =
⎧
⎨
⎩
−f(x),
if f(x) ≤0,
0,
if f(x) > 0.
One can show that f is Lebesgue integrable on A if and only if f+ and f−
are Lebesgue integrable on A in which case
"
A f dµ =
"
A f+ dµ −
"
A f−dµ.
Hence it is in fact suﬃcient to understand Lebesgue integrability only for
non-negative functions. For such functions a more constructive deﬁnition of
integrability on a set of inﬁnite measure is possible.
Theorem 6.37. Let A ∈L(R), µ(A) = ∞, and suppose that f : R →R is
measurable and f(x) ≥0 for all x ∈A. Then f is Lebesgue integrable on A if
and only if the following two conditions hold
(i) for every segment [a, b] ⊂R, f is Lebesgue integrable on A ∩[a, b],
(ii) there exists a sequence of segments [an, bn] such that [an, bn] ⊂[an+1, bn+1]
for all n, an →−∞, bn →∞as n →∞, for which the limit
lim
n→∞
#
A∩[an,bn]
f dµ
(6.14)
exists.
In this case
"
A f dµ coincides with limit (6.14).
Most properties of integrals with respect to probability measures stated in
Sect. 6.5 remain true for Lebesgue integrals over sets of inﬁnite measure. One
should note, however, that a simple measurable function that takes ﬁnitely

6.6 Monotone Functions on the Real Line
175
many values or, more generally, a bounded measurable function may not be
integrable on a set of inﬁnite measure. In particular, constant non-zero func-
tions are not integrable on such sets, hence one has to make corresponding
adjustments in properties (i) and (iv).
We will now brieﬂy compare Lebesgue integrals over the half-lines (−∞, b],
[c, ∞) and over the whole real line with the corresponding improper Rie-
mann integrals. This comparison is similar to that for the case of improper
Riemann integrals over a segment discussed above. Suppose, for example,
that a function f is Riemann integrable on every segment [a, b] and the im-
proper Riemann integral
" b
−∞f(x) dx = lima→−∞
" b
a f(x) dx exists. Then
f may not be Lebesgue integrable on (−∞, b]. However, if the above im-
proper integral converges absolutely, that is, if the improper Riemann inte-
gral
" b
−∞|f(x)| dx = lima→−∞
" b
a |f(x)| dx exists, then f is Lebesgue inte-
grable on (−∞, b] and
"
(−∞,b] f dµ =
" b
−∞f(x) dx. An analogous statement
holds for the half-line [c, ∞). Further, the improper Riemann integral over
the real line R is just the sum of two improper integrals of the type discussed
above
" ∞
−∞f(x) dx =
" a
−∞f(x) dx +
" ∞
a f(x) dx for any a, and hence f is
Lebesgue integrable if each of these integrals converges absolutely, in which
case
"
R f dµ =
" ∞
−∞f(x) dx.
We will often say that a condition holds almost everywhere, or a.e. on R,
if it holds for all points in R except possibly for points in a set of Lebesgue
measure 0. For example, we say that two measurable functions f and g coin-
cide almost everywhere if µ

{x ∈R : f(x) ̸= g(x)}

= 0. Clearly, if two such
functions are Lebesgue integrable, then the corresponding integrals coincide.
In fact, in order to speak about the integrability of a function on a set A it is
suﬃcient to have the function deﬁned almost everywhere on A and measur-
able there. One can deﬁne the function on the rest of A arbitrarily, and doing
so does not aﬀect either the integrability of the function or the value of the
integral.
We will now discuss functions of the following type.
Deﬁnition 6.38. A function f : R →R is called monotone non-decreasing,
if for all x ≤y we have f(x) ≤f(y).
It is not hard to show that a monotone non-decreasing function can only
have discontinuities of the ﬁrst kind (that is, such that the limits f(x + 0) =
limh>0,h→0 f(x + h) and f(x −0) = limh>0,h→0 f(x −h) exist for all x ∈R),
and the set of points of discontinuity is countable. In particular, monotone
non-decreasing functions are continuous almost everywhere. It turns out that
such functions also have good diﬀerentiability properties.
Theorem 6.39. Let f : R →R be a monotone non-decreasing function.
Then f is diﬀerentiable almost everywhere on R. The derivative f ′ is a non-
negative function that is Lebesgue integrable on any segment [a, b] ⊂R.

176
6 Elements of Probability Theory
We will be interested in monotone non-decreasing functions of a special
kind.
Deﬁnition 6.40. A monotone non-decreasing function f is called absolutely
continuous on R if for all a ≤b we have
f(b) −f(a) =
#
[a,b]
f ′ dµ.
Not every monotone non-decreasing function is absolutely continuous.
Example 6.41. Consider the function
f(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
if x < 0,
1,
if 0 ≤x < 2,
2,
if x ≥2.
Clearly, f ′(x) exists and is equal to 0 for all x except x = 0, 2. Therefore for
all a ≤b we have
"
[a,b] f ′ dµ = 0. However, the function f is not constant.
Hence f is not absolutely continuous.
In the next section for any random variable we will introduce a special
function called the distribution function. Distribution functions are real-valued
non-decreasing functions on R. The special case when distribution functions
are absolutely continuous will be of particular importance.
6.7 Distribution Functions
Many properties of a random variable are captured by a special function
associated with the random variable as deﬁned below.
Deﬁnition 6.42. Let f be a random variable on a probability space (S, B, P).
The distribution function of f is the function Ff : R →R deﬁned as follows
Ff(x) = P(Ex(f)).
In the future it will be also convenient to consider the “complementary” func-
tion F ∗
f (x) = 1 −Ff(x) = P(Ex(f)c). The distribution function Ff has the
following properties
(i) Ff is monotone non-decreasing, that is, F(x) ≤F(y), if x ≤y,
(ii) F(−∞) = limx→−∞Ff(x) = 0,

6.7 Distribution Functions
177
(iii) F(∞) = limx→∞Ff(x) = 1,
(iv) Ff is upper semi-continuous on R. Since Ff is monotone non-decreasing,
this condition is equivalent to the condition F(x + 0) = F(x) for all x ∈R,
which follows from property (v) of probability measures in Sect. 6.2.
The function Ff can be used to deﬁne a probability measure on a certain
σ-algebra of subsets of R in such a way that the identity function x on the re-
sulting probability space will have the same distribution function Ff. This can
be done as follows. Let F0 = {[a, b], [a, b), (a, b], (a, b), (a, ∞), [a, ∞), (−∞, b),
(−∞, b], (−∞, ∞), −∞< a ≤b < ∞} be the collection of all intervals in R.
Clearly, F0 is a semi-algebra. Deﬁne
PFf ([a, b]) = Ff(b) −Ff(a −0),
PFf ([a, b)) = Ff(b −0) −Ff(a −0),
PFf ((a, b]) = Ff(b) −Ff(a),
PFf ((a, b)) = Ff(b −0) −Ff(a),
for a < b,
PFf ((a, ∞)) = 1 −Ff(a),
PFf ([a, ∞)) = 1 −Ff(a −0),
PFf ((−∞, b)) = Ff(b −0),
PFf ((−∞, b]) = Ff(b),
PFf ((−∞, ∞)) = 1.
(6.15)
One can check directly that PFf is a probability measure on F0. It can be
extended to a probability measure on the minimal σ-algebra B(F0), which is
the σ-algebra of all Borel sets in R. This way we obtain the probability space
(R, B(F0), PFf ) such that the identity function x is a random variable on this
space and Fx = Ff. Indeed, for every t ∈R the set Et(x) = {y ∈R : y ≤
t} = (−∞, t] is a Borel set and Fx(t) = PFf (Et(x)) = PFf ((−∞, t]) = Ff(t).
This construction works for any function satisfying conditions (i)-(iv) above.
In problems involving random variables distribution functions are suﬃcient
for many purposes, and in the future we will sometimes specify only distribu-
tion functions without specifying random variables and probability spaces. We
wish to stress, however, that, as we have seen above, for a given distribution
function, one can always construct a probability space and a random variable
on it whose distribution function coincides with the given one.
To some extent the notion of integrability of random variables can be
reformulated in terms of the sample space (R, B(F0), PFf ). Let E ∈B(F0)
and let E′ = {e ∈S : f(e) ∈E}. Since E is a Borel set and the sets Eb(f)
belong to B for all b, we have E′ ∈B. It can be shown that f is integrable on
E′ with respect to P if and only if x is integrable on E with respect to PFf ,
and that in this case
#
E′ f dP =
#
E
x dPFf .

178
6 Elements of Probability Theory
In particular, if x is integrable on R, we have
E(f) =
#
R
x dPFf .
(6.16)
Let g be a random variable on (R, B(F0), PFf ). Then g(f) is a random
variable on (S, B, P). One can show that g(f) is integrable on E′ if and only
if g(x) is integrable on E, and that in this case
#
E′ g(f) dP =
#
E
g(x) dPFf .
(6.17)
In particular, for g ≡1 we have
P

{e ∈S : f(e) ∈E}

= P(E′) =
#
E′ dP =
#
E
dPFf = PFf (E).
(6.18)
We will now derive another useful consequence of formula (6.17). If x2 is
integrable on R with respect to PFf , then x is integrable on R with respect
to PFf as well, and setting g(x) = (x −E(f))2, we obtain
V ar(f) =
#
R
(x −E(f))2 dPFf =
#
R
x2 dPFf −E(f)2.
(6.19)
We will now give some examples of distribution functions and applications
of formulas (6.16) and (6.19).
Example 6.43.
1. Consider the probability space from Part 2 of Example 6.14, and let f
be the random variable deﬁned in Part 3 of Example 6.24. Then we have
Ff(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
if x < 0,
1
4,
if 0 ≤x < 1,
3
4,
if 1 ≤x < 2,
1,
if x ≥2.
Let us calculate E(f) from formula (6.16). We approximate the function
x by the discrete random variables deﬁned in (6.10) and (6.11). For the cor-
responding function fj with j ≥2 we have
#
R
fj dPFf = 1 ×
3
4 −1
4

+ 2 ×

1 −3
4

= 1.

6.7 Distribution Functions
179
Hence E(f) = 1 which agrees with the result of the calculation of E(f) in Part
1 of Example 6.32.
To calculate V ar(f) from formula (6.19), we approximate the function
(x −E(f))2 = (x −1)2 by the discrete random variables deﬁned in (6.10) and
(6.11). For the corresponding function fj with j ≥2 we have
#
R
fj dPFf = (−1)2 × 1
4 + 12 ×

1 −3
4

= 1
2.
Therefore V ar(f) = 1/2 and σ(f) = 1/
√
2 which agrees with the correspond-
ing result in Part 1 of Example 6.32.
2. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Consider the random variable f(e) = e2 as in Part 3 of Example 6.32. Then
we have
Ff(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
if x < 0,
√x,
if 0 ≤x < 1,
1,
if x ≥1.
(6.20)
Let us calculate E(f) from formula (6.16). We approximate the function x
by the discrete random variables deﬁned in (6.10) and (6.11). For the corre-
sponding function fj we have
#
R
fj dPFf = 1
j ×
√
2
√j −1
√j

+ 2
j ×
√
3
√j −2
√j

+ . . . + j −1
j
×

1 −j −1
√j

=
j−1

k=1
k
j
√
k + 1
√j
−
√
k
√j

=
1
j3/2
j−1

k=1
k
√
k + 1 −
√
k

= 1 −
1
j3/2
j

k=2
√
k −
1
j3/2 .
It is easy to see that
j

k=2
√
k ≤
# j+1
2
√x dx = 2
3

(j + 1)3/2 −23/2
,
and
j

k=2
√
k ≥
# j
1
√x dx = 2
3

j3/2 −1

.

180
6 Elements of Probability Theory
Hence
1
j3/2
j

k=2
√
k →2
3,
as j →∞,
(6.21)
and thus E(f) = 1/3 which agrees with the result of the calculation of E(f)
in Part 3 of Example 6.32.
To calculate V ar(f) from formula (6.19), we approximate the function
(x −E(f))2 = (x −1/3)2 by the discrete random variables deﬁned in (6.10)
and (6.11). For the corresponding function fj we have
#
R
fj dPFf =
1
j −1
3
2
×
√
2
√j −1
√j

+
2
j −1
3
2
×
√
3
√j −2
√j

+ . . .
+
j −1
j
−1
3
2
×

1 −j −1
√j

=
j−1

k=1
k
j −1
3
2 √
k + 1
√j
−
√
k
√j

= 1
√j
j−1

k=1
k2
j2 −2k
3j + 1
9
 √
k + 1 −
√
k

=
1
j5/2
j−1

k=1
k2(
√
k + 1 −
√
k)
−
2
3j3/2
j−1

k=1
k(
√
k + 1 −
√
k) +
1
9√j
j−1

k=1
(
√
k + 1 −
√
k).
From the calculation of E(f) we know that
1
j3/2
j−1

k=1
k(
√
k + 1 −
√
k) →1
3,
as j →∞,
and therefore
2
3j3/2
j−1

k=1
k(
√
k + 1 −
√
k) →2
9,
as j →∞.
Further,
1
9√j
j−1

k=1
(
√
k + 1 −
√
k) =
1
9√j (−1 +
$
j) →1
9,
as j →∞.
We also have
1
j5/2
j−1

k=1
k2(
√
k + 1 −
√
k) = 1 −
2
j5/2
j

k=2
k3/2 +
1
j5/2
j

k=2
√
k −
1
j5/2 .

6.8 Common Types of Random Variables
181
It follows from (6.21) that
1
j5/2
j

k=2
√
k →0,
as j →∞.
Also, it is easy to see that
j

k=2
k3/2 ≤
# j+1
2
x3/2 dx = 2
5

(j + 1)5/2 −25/2
,
and
j

k=2
k3/2 ≥
# j
1
x3/2 dx = 2
5

j5/2 −1

.
Hence
1
j5/2
j

k=2
k3/2 →2
5,
as j →∞.
Thus V ar(f) = 1 −4/5 −2/9 + 1/9 = 4/45 and σ(f) = 2/(3
√
5) which agrees
with the corresponding result in Part 3 of Example 6.32.
6.8 Common Types of Random Variables
Most random variables that arise in statistical analysis and that will be of
interest to us belong to one of two types: the discrete type and the continuous
type.
6.8.1 The Discrete Type
We have encountered discrete random variables before. Let f be a discrete
random variable on a probability space (S, B, P) that takes values r1, r2, . . .,
with ri ̸= rj for i ̸= j and recall that
E′
rj = {e ∈S : f(e) = rj}.
Deﬁnition 6.44. The collection of probabilities {pf(rj) = P(E′
rj(f))} is
called the probability distribution of f.
The distribution function Ff of f is fully determined by the probability
distribution of f. Namely, Ff is a step function whose value changes only at
the points rj where jumps of the sizes pf(rj) occur
F(x) =

rj≤x
pf(rj).

182
6 Elements of Probability Theory
Hence
F(rj) −F(rj −0) = pf(rj).
Recall that if f is a discrete random variable, E(f) and V ar(f) are easy
to ﬁnd either directly by using Deﬁnition 6.27 or from formulas (6.16) and
(6.19)
E(f) =
∞

j=1
rjpf(rj),
(6.22)
V ar(f) =
∞

j=1
(rj −E(f))2pf(rj) =
∞

j=1
r2
jpf(rj) −E(f)2.
(6.23)
6.8.2 The Continuous Type
Deﬁnition 6.45. A random variable f on a probability space is called con-
tinuous, if its distribution function is absolutely continuous. The derivative F ′
f
is called the density function of f, or simply the density of f, and is denoted
by ϱf.
The concept of continuity deﬁned above should not be confused with the
usual continuity of functions. Indeed, consider the random variable x on a
probability space (R, B(F0), PFf ). Regarded as a function from R into R, x
is continuous at every point. However, the distribution function of x coincides
with Ff and may not be absolutely continuous; in this case x is not continuous
as a random variable on the given probability space.
Recall that strictly speaking the density function ϱf is only deﬁned almost
everywhere on R (see Theorem 6.39). However, since ϱf will be almost ex-
clusively used for integration, we can set it to be equal to any non-negative
number on the set of Lebesgue measure 0, where it was not originally deﬁned.
Hence in the future we will often assume that ϱf is a non-negative function
deﬁned everywhere on R. For a continuous random variable f we have
Ff(x) = Ff(t) +
#
[t,x]
ϱf dµ,
(6.24)
for all t ≤x. Since limt→−∞F(t) = 0 and limx→∞F(x) = 1, the limit
limn→∞
"
[an,bn] ϱf dµ exists and is equal to 1 for any expanding sequence
of segments as in Theorem 6.37. Therefore this theorem implies that ϱf is
integrable on R and hence on any Lebesgue measurable subset of R.
Letting in formula (6.24) t →−∞we obtain a formula for Ff(x) in terms
of ϱf
Ff(x) =
#
(−∞,x]
ϱf dµ.
By the deﬁnition of Ff this means

6.8 Common Types of Random Variables
183
P ({e ∈S : f(e) ≤x}) =
#
(−∞,x]
ϱf dµ.
It turns out that this last fact can be generalized to an arbitrary Borel set in
R as follows.
Theorem 6.46. Let f be a continuous random variable on a probability space
(S, B, P). Then for every Borel set E ⊂R we have
P ({e ∈S : f(e) ∈E}) =
#
E
ϱf dµ.
(6.25)
Note that formula (6.25) is a version of formula (6.18) for the special case of
continuous random variables.
If ϱf
is continuous on R then for any a
≤
b the probability
P ({e ∈S : a ≤f(e) ≤b}) is equal to the area under the graph of ϱf on the
segment [a, b]. If b = a we obtain P ({e ∈S : f(e) = a}) = 0 for all a ∈R
which is not the case for any discrete random variable. More generally, if
µ(E) = 0, then the probability in the left-hand side of formula (6.25) is equal
to zero. This shows, in particular, that the classes of discrete and continuous
random variables do not intersect (one can also make this observation by ar-
guing as in Example 6.41). In fact, discrete and continuous random variables
represent two extremes among all random variables.
We will now show that the random variable from Part 3 of Example 6.32
is continuous.
Example 6.47. Let (S, B(F), P) be the probability space from Part 3 of Ex-
ample 6.14. Consider the random variable f(e) = e2 as in Part 3 of Example
6.32. In Part 2 of Example 6.43 we found the distribution function Ff of f
(see formula (6.20)). Clearly, F ′
f(x) exists for all x ∈R except x = 0, 1, and
F ′
f(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
if x < 0 or x > 1,
1
2√x,
if 0 < x < 1.
Let [a, b] be a segment. If b ≤0 or a ≥1 we clearly have
#
[a,b]
F ′
f dµ = 0 = F(b) −F(a).
Suppose that 0 < a < 1 and let c = min{1, b}. Then
#
[a,b]
F ′
f dµ = √x
!!!!!
c
a
= √c −√a = F(b) −F(a).
Let now a = 0 and set again c = min{1, b}. The improper Riemann integral
" b
0 F ′
f(x) dx exists and is equal to √c. Hence
"
[0,c] F ′
f dµ exists and

184
6 Elements of Probability Theory
#
[0,b]
F ′
f dµ = √c = F(b) −F(a).
Finally, for a < 0, b > 0 we argue as in the case a = 0 above and again obtain
the required identity.
Thus we have proved that f is absolutely continuous.
For a continuous random variable f it is not hard to show that if E is a
Borel set in R, then x is integrable on E with respect to PFf if and only if
xϱf(x) is Lebesgue integrable on E, and in this case
#
E
x dPFf =
#
E
xϱf(x) dµ.
More generally, a random variable g on (R, B(F0), PFf ) is integrable on E
with respect to PFf , if and only if g(x)ϱf(x) is Lebesgue integrable on E, and
in this case
#
E
g(x) dPFf =
#
E
g(x)ϱf(x) dµ.
Therefore formulas (6.16) and (6.19) for continuous random variables take the
forms
E(f) =
#
R
xϱf(x) dµ,
(6.26)
and
V ar(f) =
#
R
(x −E(f))2ϱf(x) dµ =
#
R
x2ϱf dµ −E(f)2
(6.27)
respectively.
In the future we will often speak about the distribution of a random vari-
able. In the discrete case this will mean the corresponding probability distri-
bution or the distribution function, in the continuous case the corresponding
density function or the distribution function. In the general case this will mean
the distribution function.
6.9 Common Discrete and Continuous Distributions
In this section we will list some standard discrete and continuous distributions
that are of interest in bioinformatics and in statistics in general.
6.9.1 The Discrete Case
We will start with the simplest distribution.

6.9 Common Discrete and Continuous Distributions
185
The Uniform Distribution
A uniformly distributed discrete random variable takes ﬁnitely many (say N)
values, and for each value rj, pf(rj) = 1/N. If, for example, a random variable
f takes values a, a + 1, a + 2, . . . , a + N −1, then formulas (6.22) and (6.23)
give
E(f) = a + N −1
2
,
V ar(f) = N 2 −1
12
.
In the above calculations we used the identities
n

k=1
k = n(n + 1)
2
,
n

k=1
k2 = n(n + 1)(2n + 1)
6
.
A Bernoulli Trial
A Bernoulli trial is a single experiment with two possible outcomes: success
and failure. The probability of success is denoted by p and hence the proba-
bility of failure is 1 −p. We have already seen a Bernoulli trial with p = 1/2
in Part 1 of Example 6.11. The Bernoulli random variable f is the number of
successes obtained in the trial. Clearly, f takes only two values 0 and 1, and
the probability distribution of f is {pf(0) = 1 −p, pf(1) = p}. Further, by
applying formulas (6.22) and (6.23) we obtain
E(f) = p,
V ar(f) = p(1 −p).
The Binomial Distribution
The binomial random variable f is the number of successes in a ﬁxed number n
of independent Bernoulli trials. It takes values 0, 1, . . . , n, and the probability
distribution of f is

pf(k) =
 n
k

pk(1 −p)n−k,
k = 0, . . . , n
 
,
where we set 00 = 1.
We will now ﬁnd E(f) and V ar(f) from formulas (6.22) and (6.23). Con-
sider the function of two variables

186
6 Elements of Probability Theory
ϕ(x, y) =
n

k=0

n
k

xkyn−k = (x + y)n.
Diﬀerentiating ϕ with respect to x for x ̸= 0 we obtain
∂ϕ
∂x = 1
x
n

k=1
k
 n
k

xkyn−k = n(x + y)n−1.
Setting in the above formula x = p, y = 1 −p we obtain for p ̸= 0
E(f) =
n

k=1
k

n
k

pk(1 −p)n−k = np.
For p = 0 it is obvious that E(f) = 0, hence we always have E(f) = np. In
order to ﬁnd V ar(f) we diﬀerentiate ϕ twice with respect to x for x ̸= 0
∂2ϕ
∂x2 = 1
x2
n

k=1
k(k −1)

n
k

xkyn−k = n(n −1)(x + y)n−2.
Therefore for p ̸= 0
V ar(f) =
n

k=1
k2
 n
k

pk(1−p)n−k−n2p2 = p2n(n−1)+np−n2p2 = np(1−p).
For p = 0 it is obvious that V ar(f) = 0, hence we always have V ar(f) =
np(1 −p).
The Geometric Distribution
In this case an inﬁnite sequence of independent Bernoulli trials with p ̸= 1 is
conducted and a random f variable is deﬁned as the number of trials before
but not including the ﬁrst failure. This random variable takes values 0, 1, 2 . . .
and its probability distribution is the geometric distribution

pf(k) = (1 −p)pk,
k = 0, 1, 2, . . .

,
where we set 00 = 1.
From formulas (6.22) and (6.23) we have
E(f) =
∞

k=1
k(1 −p)pk = p(1 −p)
 ∞

k=0
pk
′
= p(1 −p)

1
1 −p
′
=
p
1 −p,
and

6.9 Common Discrete and Continuous Distributions
187
V ar(f) =
∞

k=1
k2(1 −p)pk −
p2
(1 −p)2 = p(1 −p)
 ∞

k=1
kpk
′
−
p2
(1 −p)2
= p(1 −p)

p
 ∞

k=0
pk
′′
−
p2
(1 −p)2 = p(1 −p)

p

1
1 −p
′′
−
p2
(1 −p)2
= p(1 + p)
(1 −p)2 −
p2
(1 −p)2 =
p
(1 −p)2 .
Let us also ﬁnd the distribution function Ff(x). Clearly, Ff(x) = 0, if
x < 0. Suppose that k ≤x < k + 1 for some non-negative integer k. Then we
have
Ff(x) =
k

j=0
pf(j) = (1 −p)
k

j=1
pj = (1 −p)1 −pk+1
1 −p
= 1 −pk+1.
Hence F ∗
f (x) = pk+1. In particular, F ∗
f (k) = pk+1 for all k = 0, 1, 2, . . .. In the
future we will be interested in random variables whose distribution functions
have a similar property.
Deﬁnition 6.48. Suppose a discrete random variable f takes values 0, 1, 2, . . ..
Then f is called geometric-like if
F ∗
f (k) ∼Cpk+1,
as k →∞,
for some constants C > 0, 0 < p ≤1, where k ∈N.
The above condition means that there exists a number 0 < p ≤1 such that
the limit
lim
k→∞, k∈N
F ∗
f (k)
pk+1
exists and is positive. In this case F ∗
f (k) becomes arbitrarily close to Cpk+1
as k →∞, and for large k we can write
F ∗
f (k) ≈Cpk+1.
The Poisson Distribution
A Poisson-distributed random variable f also takes inﬁnitely many values
0, 1, 2, . . ., and the corresponding probability distribution is

pf(k) = exp(−λ)λk
k!
,
k = 0, 1, 2, . . .
 
,

188
6 Elements of Probability Theory
where λ ≥0, and we set 00 = 1.
For E(f) from formula (6.22) we obtain
E(f) =
∞

k=1
k exp(−λ)λk
k!
= exp(−λ)λ
∞

k=0
λk
k! = λ.
For V ar(f) from formula (6.23) we obtain
V ar(f) =
∞

k=1
k2 exp(−λ)λk
k!
−λ2 = exp(−λ)

λ2
∞

k=0
λk
k! + λ
∞

k=0
λk
k!

−λ2 = λ.
6.9.2 The Continuous Case
Again, we will start with the simplest distribution.
The Uniform Distribution
A continuous random variable f is said to have a uniform distribution if for
some a < b its density function is
ϱf(x) =
⎧
⎪
⎨
⎪
⎩
0,
if x ≤a or x > b,
1
b −a,
if a < x ≤b.
The distribution function of f is easily found
Ff(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
if x < a,
x −a
b −a ,
if a ≤x < b,
1,
if x ≥b.
From formulas (6.26) and (6.27) it is straightforward to calculate E(f) and
V ar(f).
E(f) =
# b
a
x
b −a dx =
x2
2(b −a)
!!!!!
b
a
= a + b
2
,
V ar(f) =
# b
a
x2
b −a dx −(a + b)2
4
=
x3
3(b −a)
!!!!!
b
a
−(a + b)2
4
= (b −a)2
12
.

6.9 Common Discrete and Continuous Distributions
189
The Normal Distribution
The density function for the normal distribution is
ϱf(x) =
1
√
2πσ2 exp

−(x −µ)2
2σ2

,
where µ ∈R and σ2 > 0. We calculate E(f) from formula (6.26).
E(f) =
1
√
2πσ2
# ∞
−∞
x exp

−(x −µ)2
2σ2

dx
= µ +
1
√
2πσ2
# ∞
−∞
(x −µ) exp

−(x −µ)2
2σ2

dx = µ.
(6.28)
In the above calculation we used the well-known identity
# ∞
−∞
exp

−(x −µ)2
2σ2

dx =
√
2πσ2
(6.29)
and the fact that the graph of ϱf(x) is symmetric with respect to the line
x = µ which gives that the last integral in formula (6.28) is equal to 0. Next,
we calculate V ar(f) from formula (6.27) using integration by parts.
V ar(f) =
1
√
2πσ2
# ∞
−∞
(x −µ)2 exp

−(x −µ)2
2σ2

dx
=
1
√
2πσ2
# ∞
−∞
t2 exp

−t2
2σ2

dt = −
√
σ2
√
2π
# ∞
−∞
t d

exp

−t2
2σ2

= −
√
σ2
√
2π

t exp

−t2
2σ2
 !!!!!
∞
−∞
−
# ∞
−∞
exp

−t2
2σ2

dt

= σ2,
where we again used identity (6.29).
The standard normal distribution is the normal distribution with µ = 0
and σ2 = 1. If a random variable f has a normal distribution with parameters
µ and σ2, then it can be “standardized” by introducing the new random
variable g = (f −µ)/
√
σ2 that has the standard normal distribution.
The Exponential Distribution
For an exponentially distributed random variable the density function is
ϱf(x) =
⎧
⎨
⎩
0,
if x ≤0,
λ exp(−λx),
if x > 0,

190
6 Elements of Probability Theory
where λ > 0. We will now ﬁnd E(f) from formula (6.26) using integration by
parts.
E(f) =
# ∞
0
xλ exp(−λx) dx = −
# ∞
0
x d(exp(−λx))
= −

x exp(−λx)
!!!!!
∞
0
−
# ∞
0
exp(−λx) dx

= −1
λ exp(−λx)
!!!!!
∞
0
= 1
λ.
Similarly, for V ar(f) from formula (6.27) we obtain
V ar(f) =
# ∞
0
x2λ exp(−λx) dx −1
λ2 = −
# ∞
0
x2 d(exp(−λx)) −1
λ2
= −

x2 exp(−λx)
!!!!!
∞
0
−2
# ∞
0
x exp(−λx) dx

−1
λ2 = 2
λE(f) −1
λ2 = 1
λ2 .
The Gamma Distribution
For the gamma distribution the density function is
ϱf(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
if x ≤0,
λpxp−1 exp(−λx)
Γ(p)
,
if x > 0,
where λ, p > 0 and Γ(p) is the gamma function
Γ(p) =
# ∞
0
tp−1 exp(−t) dt.
The gamma function extends the factorial function to all positive numbers
since one can show that Γ(n) = (n −1)! for all n ∈N.
We will now ﬁnd E(f) from formula (6.26) using integration by parts as
for the exponential distribution.
E(f) =
λp
Γ(p)
# ∞
0
xp exp(−λx) dx = −λp−1
Γ(p)
# ∞
0
xp d(exp(−λx))
= −λp−1
Γ(p)

xp exp(−λx)
!!!!!
∞
0
−p
# ∞
0
xp−1 exp(−λx) dx

= p
λ.
Similarly, for V ar(f) from formula (6.27) we obtain

6.10 Vector-Valued Random Variables
191
V ar(f) =
λp
Γ(p)
# ∞
0
xp+1 exp(−λx) dx −p2
λ2 = −λp−1
Γ(p)
# ∞
0
xp+1 d(exp(−λx))
−p2
λ2 = −λp−1
Γ(p)

xp+1 exp(−λx)
!!!!!
∞
0
−(p + 1)
# ∞
0
xp exp(−λx) dx

−p2
λ2 = p + 1
λ
E(f) −p2
λ2 = p
λ2 .
The exponential distribution is a special case of the gamma distribution
for p = 1. Another special case of the gamma distribution is the chi-square
distribution with ν degrees of freedom obtained for λ = 1/2 and p = ν/2, where
ν ∈N. The chi-square distribution is important for statistical hypothesis
testing (see Sect. secthyptes).
There is an interesting connection between the normal and chi-square dis-
tributions, as shown in the following proposition.
Proposition 6.49. If a random variable f has the standard normal distrib-
ution, then f 2 has the chi-square distribution with 1 degree of freedom.
Proof: Let f be deﬁned on a probability space (S, B, P). Clearly, Ff 2(x) = 0,
if x < 0. For x ≥0 using formula (6.25) we obtain
Ff 2(x) = P

{e ∈S : f 2(e) ≤x}

= P

{e ∈S : −√x ≤f(e) ≤√x}

=
1
√
2π
# √x
−√x
exp

−x2
2

dx.
Diﬀerentiation with respect to x for x > 0 yields
ϱf 2(x) =
1
√
2π
1
√x exp

−x
2

,
which is the density function of the chi-square distribution with 1 degree of
freedom (it follows from identity (6.29) that Γ(1/2) = √π).
6.10 Vector-Valued Random Variables
A vector-valued random variable is a ﬁnite collection f = (f1, . . . , fn) of ordi-
nary random variables all of which are deﬁned on the same probability space
(S, B, P). If b1, . . . , bn ∈R, then the event Eb1,...,bn(f) = {e ∈S : f1(e) ≤
b1, . . . , fn(e) ≤bn)} is measurable since Eb1,...,bn(f) = ∩n
k=1Ebk(fk). Hence

192
6 Elements of Probability Theory
P(Eb1,...,bn(f)) is deﬁned and we can consider the distribution function of the
vector-valued random variable f
Ff(x1, . . . , xn) = P(Ex1,...,xn(f)).
Clearly, Ff is deﬁned on all of Rn.
One can also consider the distribution functions Ffk(x) of the components
fk. They are called the marginal distribution functions of f and obtained
from Ff as follows
Ffk(x) = Ff(∞, . . . , ∞, x, ∞, . . . , ∞),
where in the right-hand side x occupies the kth position. Marginal distribution
functions are used to introduce the following important concept.
Deﬁnition 6.50. Let f1, . . . , fn be random variables deﬁned on the same
probability space. They are called independent if for the vector-valued random
variable f = (f1, . . . , fn) we have
Ff(x1, . . . , xn) = Ff1(x1) × . . . × Ffn(xn),
(6.30)
for all (x1, . . . , xn) ∈Rn.
We note that if f1, . . . , fn are independent, then any smaller number of ran-
dom variables are independent as well. Indeed, consider random variables
fk1, . . . , fkm with 1 ≤k1 < . . . < km ≤n and the corresponding vector-
valued random variable f ′ = (fk1, . . . , fkm). Then Ff ′(y1, . . . , ym) is obtained
from Ff(x1, . . . , xn) by setting all values of the variables to be inﬁnite except
xk1, . . . , xkm and replacing xkj with yj for j = 1, . . . , m. Making these changes
in formula (6.30) turns it into
Ff ′(y1, . . . , ym) = Ffk1(y1) × . . . × Ffkm(ym)
which means that fk1, . . . , fkm are independent.
If f = (f1, . . . , fn) is a vector-valued random variable, then it is possible
to generalize formula (6.15) to obtain a probability measure PFf on Borel
sets in Rn from the distribution function Ff (the σ-algebra B(Fn
0 ) of Borel
sets in Rn is generated by the semi-algebra Fn
0 of all – possibly unbounded –
parallelepipeds in Rn with faces parallel to the coordinate hyperplanes). One
can show that the distribution function of the vector-valued random variable
(x1, . . . , xn) on the probability space (Rn, B(Fn
0 ), PFf ) coincides with Ff. The
condition of independence of f1, . . . , fn can be formulated in terms of PFf .
Namely, f1, . . . , fn are independent if and only if for every Borel set E ⊂Rn
of the form E = E1 × . . . × En, with Ej ∈B(F0) (that is, each Ej is a Borel
set in R), we have PFf (E) = PFf1 (E1) × . . . × PFfn(En).
We will now describe an important construction that leads to independent
random variables. Let f1, . . . , fn be random variables deﬁned on probability
spaces (Sj, Bj, Pj), for j = 1, . . . , n, respectively. Consider the Cartesian pro-
duct
(S, B, P)
of
these
probability
spaces.
For
an
elementary
event

6.10 Vector-Valued Random Variables
193
e = (e1, . . . , en) ∈S set ˜fj(e) = fj(ej), j = 1, . . . , n. Then the random
variables ˜f1, . . . , ˜fn are independent. If (Sj, Bj, Pj) = (S′, B′, P ′) and fj = f
for j = 1, . . . , n, we denote the vector-valued random variable ( ˜f1, . . . , ˜fn)
by f ×n and call it the nth Cartesian power of f. Clearly, f ×n is deﬁned on
(S′, B′, P ′)n, and its components are independent.
One is often interested in discrete and continuous vector-valued random
variables.
Deﬁnition 6.51. A vector-valued random variable f = (f1, . . . , fn) is called
discrete, if fk is discrete for k = 1, . . . , n. Let r1,k, r2,k, . . ., with ri,k ̸= rj,k for
i ̸= j, be the values of fk. Then the collection of numbers

pf(rj1,1, . . . rjn,n) = P{e ∈S : f1(e) = rj1,1, . . . , fn(e) = rjn,n}
= P
 n

k=1
E′
rjk,k(fk)

is called the probability distribution of the vector-valued random variable f.
The probability distributions {pfk(rj,k)} are called the marginal probability
distributions of f.
The distribution function of a discrete vector-valued random variable is en-
tirely determined by its probability distribution, namely
Ff(x1, . . . , xn) =

rj1,1≤x1,...rjn,n≤xn
pf(rj1,1, . . . rjn,n).
It is not hard to show that discrete random variables f1, . . . , fn deﬁned on the
same probability space are independent if and only if the probability distri-
bution of f = (f1, . . . , fn) is related to its marginal probability distributions
as follows
pf(rj1,1, . . . rjn,n) = pf1(rj1,1) × . . . × pfn(rjn,n),
for all possible values rjk,k.
One can give a deﬁnition of continuous vector-valued random variable
along the lines of Deﬁnition 6.45, but this approach requires some prelimi-
nary work on analogues of “monotone non-decreasing functions” on Rn (cf.
Sect. 6.6). However, since in the future we will be dealing mainly with vector-
valued random variables with independent components, we will only give a
deﬁnition of continuous vector-valued random variable in this situation and
quickly explain how the deﬁnition can be generalized to the case of not nec-
essarily independent components.
Deﬁnition 6.52. Let f = (f1, . . . , fn) be a vector-valued random variable
whose components f1, . . . , fn are independent. Then f is called continuous if
each fk is continuous. In this case the function ϱf(x1, . . . , xn) = ϱf1(x1) ×
. . . × ϱfn(xn) is called the density function of f or simply the density of f.

194
6 Elements of Probability Theory
It follows from formula (6.30) that if the components of f = (f1, . . . , fn) are
independent and f is continuous in the sense of Deﬁnition 6.52, then
F(x1, . . . , xn) =
#
(−∞,x1]
ϱf1 dµ × . . . ×
#
(−∞,xn]
ϱfn dµ.
(6.31)
Identity (6.31) can be written in a more compact form if we introduce
a Lebesgue measure µn on Rn. It is produced in the same way as the
Lebesgue measure µ on R. Namely, µn is ﬁrst constructed on the unit cube
{(x1, . . . , xn) ∈Rn : 0 ≤x1 ≤1, . . . , 0 ≤xn ≤1} starting with the semi-
algebra of parallelepipeds lying inside the cube with faces parallel to the co-
ordinate hyperplanes, and afterwards extended to Rn. Lebesgue integrability
with respect to µn is deﬁned analogously. Since ϱfk is Lebesgue integrable on
R for all k, the function ϱf is Lebesgue integrable on Rn, and identity (6.31)
can be written as
F(x1, . . . , xn) =
#
(−∞,x1]×...×(−∞,xn]
ϱf dµn.
(6.32)
More generally, if E ⊂Rn is a Borel set, we have
P({e ∈S : f(e) ∈E}) =
#
E
ϱf dµn,
which is a version of formula (6.25) for the case of vector-valued random
variables.
In fact, the existence of a non-negative Lebesgue integrable on Rn function
ϱf such that (6.32) holds can be taken as a general deﬁnition of continuous
Rn-valued random variable in which case the corresponding function ϱf is
called the density function or density of f.
Moreover, in this case the nth
order partial derivative ∂nFf/∂x1 . . . ∂xn exists almost everywhere on Rn and
coincides almost everywhere with ϱf.
As we noted above, for a vector-valued random variable f = (f1, . . . , fn) on
a probability space (S, B, P) one can construct a probability measure PFf on
the σ-algebra B(Fn
0 ) of all Borel sets in Rn in such a way that the distribution
function of the vector-valued random variable (x1, . . . , xn) on the probability
space (Rn, B(Fn
0 ), PFf ) coincides with Ff. Let E ∈B(Fn
0 ) and let E′ = {e ∈
S : f(e) ∈E}. Since E is Borel, the event E′ belongs to B. Let g be a random
variable on (Rn, B(Fn
0 ), PFf ). Then g(f) is a random variable on (S, B, P).
One can show that g(f) is integrable on E′ with respect to P if and only if
g(x1, . . . , xn) is integrable on E with respect to PFf , and that in this case
#
E′ g(f) dP =
#
E
g(x1, . . . , xn) dPFf .
(6.33)
Formula (6.33) is a generalization of formula (6.17). If we set g ≡1 in (6.33),
we obtain a useful generalization of formula (6.18)

6.10 Vector-Valued Random Variables
195
P

{e ∈S : f(e) ∈E}

= P(E′) =
#
E′ dP =
#
E
dPFf = PFf (E).
(6.34)
If f1, . . . , fn are independent and E = E1 × . . . × En with Ej being Borel
sets in R, formula (6.33) becomes
#
E′ g(f) dP =
#
E1
. . .
#
En
g(x1, . . . , xn) dPFfn . . . dPFf1 .
In particular, if g(x1, . . . , xn) = g1(x1) × . . . × gn(xn), we have
#
E′ g(f) dP =
#
E1
g1(x1) dPFf1 × . . . ×
#
En
gn(xn) dPFfn .
The last formula has an important application. Setting E = Rn = R×. . .×R
and g(x1, . . . xn) = x1 × . . . × xn we notice that E′ = S and therefore obtain
E(f1 × . . . × fn) =
#
S
f1 × . . . × fn dP =
#
R
x1 dPFf1 × . . .
×
#
R
xn dPFfn = E(f1) × . . . × E(fn).
(6.35)
Identity (6.35) is very useful when independent random variables are con-
sidered.
Further, for a pair of random variables the following quantity is frequently
used.
Deﬁnition 6.53. Let f1, f2 be two random variables deﬁned on the same
probability space (S, B, P). The covariance of f1 and f2 is the following num-
ber
Cov(f1, f2) = E

(f1 −E(f1))(f2 −E(f2))

.
Of course, for the covariance to exist the random variables f1, f2 and
f1f2 must be integrable on S with respect to P. Expanding the formula in
Deﬁnition 6.53, we obtain
Cov(f1, f2) = E(f1f2) −E(f1)E(f2).
For a vector-valued random variable f = (f1, . . . , fn) deﬁne the variance-
covariance matrix of f as
(Σ(f))ii = V ar(fi),
(Σ(f))ij = Cov(fi, fj),
i ̸= j,
for i, j = 1, . . . , n. Clearly, Σ(f) is a symmetric n × n matrix. Variance-
covariance matrices are widely used in statistical analysis.
It follows from formula (6.35) that for a vector-valued random variable
with independent components the corresponding variance-covariance matrix
is diagonal.

196
6 Elements of Probability Theory
6.11 Sequences of Random Variables
In this section we will consider sequences {fk} of random variables all of
which are deﬁned on the same probability space (S, B, P). One important
example of such sequences is associated with Markov chains. Indeed, consider
a discrete-time Markov chain without an end state and denote its begin state
by 0. Let P = (pij) be the N × N-matrix of transition probabilities and
p0 = (p01, . . . , p0N) the vector of initialization probabilities. As before, let X
denote the set of all non-zero states of the chain, and we assume that X is
realized as the set {1, . . . , N}, so that Gj corresponds to j for j = 1, . . . , N.
Let (S, B, P) be the probability space arising from the Markov chain (see Part
4 of Example 6.14). For every k ∈N deﬁne a function on S as follows: for
e = 0 x1x2 . . . set fk(e) = xk. Loosely speaking, fk represents the state reached
by the Markov chain after k steps (counting the initialization step). Clearly,
fk is a discrete random variable on (S, B, P). Its probability distribution is
easy to ﬁnd. We have
P

{e ∈S : fk(e) = i}

= (p0Pk−1)i,
where (p0Pk−1)i is the ith component of the vector p0Pk−1. Further, for the
joint distribution of fk and fk+1 we have
P

{e ∈S : fk+1(e) = j, fk(e) = i}

= (p0Pk−1)ipij.
Therefore, for all i, j ∈X we have
pij = P

{e ∈S : fk+1(e) = j|fk(e) = i}

,
for every k ∈N such that P

{e ∈S : fk(e) = i}

̸= 0. Hence in some cases the
transition probabilities of a Markov chain can be treated as certain conditional
probabilities. More generally, for all i, j ∈X and n ∈N the following holds
P

{e ∈S : fk+n(e) = j, fk(e) = i}

= (p0Pk−1)ip(n)ij
(6.36)
and hence
p(n)ij = P

{e ∈S : fk+n(e) = j|fk(e) = i}

,
for every k ∈N such that P

{e ∈S : fk(e) = i}

̸= 0, where p(n)ij are the
entries of the matrix P(n) = Pn. The matrix P(n) can be thought of as the
matrix of transition probabilities after n + 1 steps of the Markov chain.
We will now turn to the general theory of sequences of random variables.
We will be mainly interested in the convergence properties of such sequences.
There are many types of convergence for random variables, and in this section
we will explore some of them.
We will start with perhaps the most popular type of convergence.

6.11 Sequences of Random Variables
197
Deﬁnition 6.54. A sequence {fk} of random variables on a probability space
(S, B, P) is said to converge in probability to a random variable f deﬁned on
the same probability space, if for every ε > 0 we have
lim
k→∞P

{e ∈S : |fk(e) −f(e)| ≥ε}

= 0.
(6.37)
In this case we write fk
P→f.
An important tool for verifying convergence in probability is the following
inequality called Chebyshev’s inequality.
Theorem 6.55. (Chebyshev’s Inequality)Let f ≥0 be an integrable ran-
dom variable on a probability space (S, B, P), and c > 0. Then
P

{e ∈S : f(e) ≥c}

≤E(f)
c
.
(6.38)
Proof: Let Ec = {e ∈S : f(e) ≥c}. Since f is a random variable, Ec is
measurable. We have
E(f) ≥
#
Ec
f dP ≥cP(Ec),
which gives inequality (6.38).
The theorem is proved.
If ϕ is a random variable with mean µ and variance σ2, then setting in in-
equality (6.38) f = (ϕ−µ)2 and c = λ2σ2, with λ > 0, we obtain the classical
form of Chebyshev’s inequality
P

{e ∈S : |ϕ(e) −µ| ≥λ
√
σ2}

≤1
λ2 .
We will now prove the following important theorem.
Theorem 6.56. (Weak Law of Large Numbers)Let {fk} be a sequence
of pairwise independent random variables on a probability space (S, B, P).
Suppose that f 2
k is integrable on S for each k. Let E(fk) = µ for all k. Also
let σ2
k = V ar(fk) and assume that
lim
n→∞
1
n2
n

k=1
σ2
k = 0.
Then fn
P→µ, where
fn = 1
n
n

k=1
fk.

198
6 Elements of Probability Theory
Proof: Clearly, fn is integrable on S and E(fn) = µ. To show that V ar(fn)
exists we need to prove that fn
2 is also integrable on S. We have
fn
2 = 1
n2
⎛
⎝
n

k=1
f 2
k + 2

1≤i<j≤n
fifj
⎞
⎠.
The random variables f 2
k are integrable on S by assumption. The products
fifj with i ̸= j are integrable on S by the Cauchy-Bunyakowski inequality (see
property (vii) of integrals in Sect. 6.5). One can also deduce the integrability
of fn
2 from the triangle inequality (see property (viii) of integrals in Sect.
6.5).
We further have
V ar(fn) = E(fn
2) −µ2 = 1
n2
⎛
⎝
n

k=1
E(f 2
k) + 2

1≤i<j≤n
E(fifj)
⎞
⎠−µ2
= 1
n2
 n

k=1
σ2
k + nµ2 + (n2 −n)µ2

−µ2 = 1
n2
n

k=1
σ2
k,
where in the above calculation we used E(fifj) = E(fi)E(fj) for i ̸= j which
follows from pairwise independence of the random variables fk (see formula
(6.35)).
Fix ε > 0 and apply Chebyshev’s inequality (6.38) to |fn −µ|.
P

{e ∈S : |fn(e) −µ| ≥ε}

= P

{e ∈S : (fn(e) −µ)2 ≥ε2}

≤V ar(fn)
ε2
= 1
ε2
1
n2
n

k=1
σ2
k,
and hence P

{e ∈S : |fn(e) −µ| ≥ε}

→0 as n →∞. Thus we have shown
that fn
P→µ.
The theorem is proved.
We will often consider sequences of random variables satisfying the follow-
ing condition.
Deﬁnition 6.57. Let {fk} be either a sequence or a ﬁnite collection of ran-
dom variables. The random variables fk are said to be independent identically
distributed or simply iid if:
(i) for every n ≥2 the random variables f1, . . . , fn are independent,

6.11 Sequences of Random Variables
199
(ii) the distributions of all fk are identical.
If {fk} is a sequence of random variables for which only (i) is known to hold,
we say that fk are independent.
If in Deﬁnition 6.57, {fk} = {f1, . . . , fm} is a ﬁnite collection of random
variables, then it is of course suﬃcient to require that (i) holds only for n = m.
Examples of ﬁnite collections of iid random variables are the components of
f ×m, the mth Cartesian power of any random variable f, in which case each
component has the same distribution as f.
It is also possible to give examples of sequences of independent and iid
random variables. Let fj be random variables deﬁned on probability spaces
(Sj, Bj, Pj), for j = 1, 2, . . ., respectively. Consider the Cartesian product
(S, B, P) of these probability spaces. For an elementary event e = {ej} =
(e1, e2, . . .) ∈S set ˜fj(e) = fj(ej), j = 1, 2 . . .. Then the random variables
˜fj are independent. If (Sj, Bj, Pj) = (S′, B′, P ′) and fj = f for j = 1, 2 . . .,
then the random variables ˜fj are iid. In this case we denote the sequence { ˜fj}
by f ×∞and call it the inﬁnite Cartesian power of f. We also denote the
vector-valued random variable ( ˜f1, . . . , ˜fn) by f ×∞,n and call it the sample
of size n from the inﬁnite Cartesian power of f. Clearly, f ×∞,n is deﬁned on
(S′, B′, P ′)∞for every n ∈N.
Remark 6.58. Obviously, Theorem 6.56 holds for a sequence of iid random
variables for which the variance exists. In fact, for a sequence of iid random
variables Theorem 6.56 can be proved only with the assumption of the exis-
tence of the mean, by an argument that does not use Chebyshev’s inequality.
We will now introduce another type of convergence.
Deﬁnition 6.59. Let {fk} be a sequence of random variables on a sample
space (S, B, P) such that f 2
k is integrable on S for every k. Let f be a random
variable on the same sample space. Then the sequence {fk} is said to converge
to f in the square mean or simply in the mean if (fk −f)2 is integrable on S
for all k and
lim
k→∞E((fk −f)2) = 0.
In this case we write fk
M
→f.
We note that in Deﬁnition 6.59, f 2 is automatically integrable on S which
follows from the triangle inequality (see property (viii) of integrals in Sect.
6.5).
It is not hard to show that if fk
M
→f, then fk
P→f. Indeed, it follows from
Chebyshev’s inequality (6.38) that

200
6 Elements of Probability Theory
P ({e ∈S : |fk(e) −f(e)| ≥ε}) = P

{e ∈S : (fk(e) −f(e))2 ≥ε2}

≤E

(fk −f)2
ε2
,
and hence convergence of {fk} to f in the mean implies convergence of {fk}
to the same random variable f in probability. The converse is not true as the
following example shows.
Example 6.60. Consider the probability space from Part 3 of Example 6.14
and the following sequence of random variables
fk(e) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
√
k,
if 0 ≤e ≤1
k ,
0,
if 1
k < e ≤1.
We have fk
P→0 since for any ε < 1 the following holds
P ({e ∈[0, 1] : |fk(e)| ≥ε}) = 1
k →0,
as k →∞.
It is also clear that f 2
k is integrable on S = [0, 1] for all k. However, E(f 2
k) = 1
for all k, and thus {fk} does not converge to 0 in the mean. In fact, {fk}
does not converge in the mean to any random variable. Indeed, suppose that
there exists a random variable f such that fk
M
→f. Then from the triangle
inequality (see property (viii) of integrals in Sect. 6.5) we have
E((fk −fn)2) = E

(fk −f)+(f −fn)
2

≤

E((fk −f)2)+E((fn −f)2)
2
,
and hence E((fk −fn)2) →0 as k, n →∞. However, we have
E((fk −f2k)2) = 2 −
√
2,
for all k,
which shows that {fk} does not converge in the mean to any random variable.
We will now introduce one more type of convergence.
Deﬁnition 6.61. A sequence {fk} is said to converge in distribution to a
random variable f if for every point x ∈R at which Ff is continuous, we
have
lim
k→∞Ffk(x) = Ff(x).
In this case we write fk
D
→f.

6.11 Sequences of Random Variables
201
Note that the limit of a sequence that converges in distribution is not unique.
Indeed, if fk
D
→f and if ˜f is another random variable such that F ˜
f = Ff, then
we also have fk
D
→˜f. Also note that in Deﬁnition 6.61 one can in fact allow
each of the random variables fk and f to be deﬁned on its own probability
space. Convergence in distribution is often understood in this broader sense.
It is not hard to show that if fk
D
→f and Ff is continuous on R, then Ffk
converges to Ff uniformly on R (see Exercise 6.21).
We will now show that convergence in distribution is a weaker condition
than convergence in probability.
Theorem 6.62. If fk
P→f, then fk
D
→f.
Proof: Suppose Ff is continuous at x0 ∈R and let x′ < x0. Consider the
following three events
E1 = Ex′(f),
Ek
2 = Ex0(fk),
Ek
3 =

Ex0−x′(|fk −f|)
c
= {e ∈S : |fk(e) −f(e)| > x0 −x′} .
Clearly, E1 ⊂E2 ∪E3, and therefore we have
Ff(x′) = P(E1) ≤P(Ek
2 ) + P(Ek
3 ) = Ffk(x0) + P(Ek
3 ).
Since fk
P→f, we have P(Ek
3 ) →0 as k →∞, and therefore we obtain
Ff(x′) ≤lim infk→∞Ffk(x0).
Similarly, by taking x′′ > x0 we obtain Ff(x′′) ≥lim supk→∞Ffk(x0). We
therefore have
Ff(x′) ≤lim inf
k→∞Ffk(x0) ≤lim sup
k→∞
Ffk(x0) ≤Ff(x′′).
Now let in the above inequality x′, x′′ →x0. Since Ff is continuous at x0,
Ff(x′) →Ff(x0) as x′ →x0 and Ff(x′′) →Ff(x0) as x′′ →x0. Hence
limk→∞Ffk(x0) exists and is equal to Ff(x0). Thus fk
D
→f.
The theorem is proved.
Theorem 6.62 implies that if fk
M
→f, then fk
D
→f. The converse is not
true as shown in the following example.
Example 6.63. Consider again the probability space from Part 3 of Example
6.14 and the following sequence of random variables
f2m−1(e) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1,
if 0 ≤e ≤1
2,
0,
if 1
2 < e ≤1,
f2m(e) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
if 0 ≤e ≤1
2,
1,
if 1
2 < e ≤1,

202
6 Elements of Probability Theory
for m ∈N. All fk are discrete random variables with the same probability
distribution {pfk(0) = 1/2, pfk(1) = 1/2}, and hence their distribution func-
tions coincide. Therefore, fk
D
→f1 (and, in fact, fk
D
→f2). On the other hand,
for ε ≤1, we have
P({e ∈[0, 1] : |f2m −f1| ≥ε}) = 1,
for all m ∈N, and hence {fk} does not converge to f1 in probability. In fact,
{fk} does not converge in probability to any random variable. Indeed, suppose
that fk
P→f. Then we have
P({e ∈[0, 1] : |f2m+1 −f| ≥ε}) = P({e ∈[0, 1] : |f1 −f| ≥ε}),
P({e ∈[0, 1] : |f2m −f| ≥ε})
= P({e ∈[0, 1] : |f2 −f| ≥ε}).
Therefore P({e ∈[0, 1] : |f1 −f| ≥ε}) = P({e ∈[0, 1] : |f2 −f| ≥ε}) = 0 for
every ε > 0, which implies that f almost certainly coincides with each of f1
and f2. However, f1 and f2 do not coincide almost certainly. Therefore, {fk}
does not converge in probability to any random variable.
The sequence of random variables from Example 6.63 is also an example of
a sequence that converges in distribution but does not converge in the mean.
We will now state without proof a fundamental theorem of the probability
theory and statistics. The theorem is formulated in terms of convergence in
distribution.
Theorem 6.64. (Central Limit Theorem)Let {fk} be a sequence of iid
random variables for which the variance exists. Let µ and σ2 be the mean
and variance of the fk’s respectively. Assume that σ2 > 0 and consider the
random variables
gn =
n

k=1
fk −nµ
√
nσ2
.
Then gn
D
→f, where f is a continuous random variable whose distribution
is the standard normal distribution, and, moreover, Fgn converges to Ff uni-
formly on R.
Theorem 6.64 shows that for a sequence of iid random variables, the distri-
bution function of the sum n
k=1 fk is uniformly on R close to the normal
distribution with mean nµ and variance nσ2, thus providing one indication of
the importance of the normal distribution in probability and statistics.
Next, we will consider another frequently used type of convergence.
Deﬁnition 6.65. A sequence {fk} of random variables is said to converge
almost certainly or a.c. to a random variable f deﬁned on the same probability
space, if

6.11 Sequences of Random Variables
203
lim
k→∞fk(e) = f(e),
for all e ∈E, where E is an event with P(E) = 1. In this case we write
fk
a.c.
→f and call E the set of convergence of {fk}.
We note that in the above deﬁnition it is not necessary to require that f is a
random variable. One can show that the almost certain limit of a sequence of
random variables is a random variable as well (see Exercise 6.22).
We will now show that almost certain convergence is stronger than con-
vergence in probability.
Theorem 6.66. If fk
a.c.
→f, then fk
P→f.
Proof: Let (S, B, P) be the probability space on which the random variables
fk are deﬁned. Fix ε > 0 and consider the following sequence of events
Ek = {e ∈S : |fn(e) −f(e)| < ε for all n ≥k} .
Clearly, Ek ∈B and Ek ⊂Ek+1 for all k ∈N. Let E be the set of conver-
gence of {fk}. We obviously have E ⊂∪∞
k=1Ek. Since P(E) = 1, we obtain
P (∪∞
k=1Ek) = 1 and hence by property (iv) of probability measures (see Sect.
6.2), P(Ek) →1 as k →∞.
We now have
P({e ∈S : |fk(e) −f(e)| < ε}) ≥P(Ek) →1,
as k →∞,
and therefore
P({e ∈S : |fk(e) −f(e)| ≥ε}) →0,
as k →∞,
which shows that fk
P→f.
The theorem is proved.
We will now formulate a version of Theorem 6.56 in which convergence in
probability is replaced with almost certain convergence.
Theorem 6.67. (Strong Law of Large Numbers)Let {fk} be a sequence
of independent random variables on a probability space (S, B, P). Suppose
that f 2
k is integrable on S for each k. Let E(fk) = µ for all k. Also let σ2
k =
V ar(fk) and assume that the series
∞

k=1
σ2
k
k2
converges. Then fn
a.c.
→µ, where
fn = 1
n
n

k=1
fk.

204
6 Elements of Probability Theory
Note that Theorem 6.67 holds for sequences of iid random variables.
Theorem 6.66 implies that if fk
a.c.
→f, then fk
D
→f. The following example
shows that the converse to Theorem 6.66 does not hold.
Example 6.68. As before, consider the probability space from Part 3 of Ex-
ample 6.14. We also consider a sequence of random variables {fk,n} indexed
for convenience by two indices, k ∈N, n = 1, . . . , k, deﬁned as follows
fk,n(e) =
⎧
⎪
⎨
⎪
⎩
1,
if n −1
k
< e ≤n
k ,
0,
otherwise.
The sequence {fk,n} converges in probability to 0. Indeed, for ε < 1 we have
P({e ∈[0, 1] : |fk,n(e)| ≥ε} = 1
k →0,
as k →∞.
However, {fk,n} does not converge to 0 almost certainly since the only point
e ∈[0, 1] for which limk→∞fk,n(e) = 0 is e = 0.
Nevertheless, one can show that if fk
P→f, then one can always ﬁnd a
subsequence {fkm} of {fk} such that fkm
a.c.
→f. We do not prove this fact
here but notice that in Example 6.68 for the subsequence {fk,1} we have
fk,1
a.c.
→0.
Note that the sequence from Example 6.68 in fact converges to 0 in the
mean since
E(f 2
k,n) = 1
k →0,
as k →∞.
Hence this example also shows that convergence in the mean does not imply
almost certain convergence. Nevertheless, if fk
M
→f, then one can always ﬁnd
a subsequence {fkm} of {fk} such that fkm
a.c.
→f. Also note that the sequence
from Example 6.60 shows that almost certain convergence does not imply
convergence in the mean. Finally, Example 6.63 also shows that convergence
in distribution does not imply almost certain convergence.
Recall that in Sect. 6.5 we deﬁned yet another kind of convergence, namely
uniform convergence (see Deﬁnition 6.28). Clearly, uniform convergence im-
plies almost certain convergence (the set of convergence in this case is all of
the sample space) and hence convergence in probability and convergence in
distribution. A proof analogous to that in Sect. 6.5 for convergence of inte-
grals of discrete random variables shows that uniform convergence also implies
convergence in the mean. However, none of the above types of convergence im-
plies uniform convergence. Indeed, the sequence from Example 6.60 converges
in probability, but does not converge uniformly, the sequence from Example
6.63 converges in distribution, but does not converge uniformly, the sequence
from Example 6.68 converges in the mean, but does not converge uniformly,
its subsequence {fk,1} converges almost certainly, but not uniformly.

Exercises
205
We summarize the relationships among the various types of convergence in
Fig. 6.1. Note that uniform convergence is the strongest type of convergence
and convergence in distribution is the weakest type of convergence among all
convergence types considered in this section.
Fig. 6.1.
Exercises
6.1. Prove identities (6.1) and (6.2).
6.2. Let the sample space S be the closed square in the (x, y)-plane with
vertices (0, 0), (0, 1), (1, 0) and (1, 1). Deﬁne F to be the collection of all
rectangles in S with sides parallel to the x- and y-axes
F =

(a, b) × (c, d), [a, b) × (c, d), (a, b] × (c, d), [a, b] × (c, d),
(a, b) × [c, d), [a, b) × [c, d), (a, b] × [c, d), [a, b] × [c, d),
(a, b) × (c, d], [a, b) × (c, d], (a, b] × (c, d], [a, b] × (c, d],
(a, b) × [c, d], [a, b) × [c, d], (a, b] × [c, d], [a, b] × [c, d],
0 ≤a, b, c, d ≤1
 
.
Prove that F is a semi-algebra of events in S.
6.3. Prove that P deﬁned in Part 3 of Example 6.11 is a probability measure
on the semi-algebra F from Example 6.8.
6.4. Let S and F be as in Exercise 6.2. For a rectangle E in F deﬁne P(E)
to be equal to the area of E. Prove that P is a probability measure on F.

206
6 Elements of Probability Theory
6.5. Let S and F be as in Exercise 6.2 and P be the probability measure on F
deﬁned in Exercise 6.4. Consider the minimal σ-algebra B(F) generated by F
(which is the σ-algebra of Borel sets in S) and extend the probability measure
to events in B(F). Prove that every open or closed triangle in S with two sides
parallel to the x- and y-axes is measurable with respect to this probability
measure. Give an example of an non-measurable event in S.
6.6. Consider the probability space (S, B(F), P) from Exercise 6.5 and two
events E1 = {(x, y) ∈S : y ≤1 −x} and E2 = {(x, y) ∈S : y < x} in S. Are
E1 and E2 measurable? If yes, are they independent? Prove your conclusions.
6.7. For the probability space (S, B(F), P) from Exercise 6.5 give an example
of three independent events E1, E2, E3 in S whose pairwise intersections have
non-zero probabilities.
6.8. For the probability space (S, B(F), P) from Exercise 6.5 give an example
of a function on S that is not a random variable on (S, B(F), P).
6.9. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Consider the following function on S
f(e) =
⎧
⎪
⎨
⎪
⎩
0,
if e = 0,
(−1)n(n + 1),
if
1
n + 1 < e ≤1
n, for n ∈N.
Is f a random variable on (S, B(F), P)? If yes, is it integrable on either of the
events E1 = [0, 1/2], E2 = [1/2, 1]? Prove your conclusions.
6.10. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Consider the following function on S
f(e) =
⎧
⎨
⎩
0,
if e = 0,
e−1/3,
if 0 < e ≤1.
Prove that f is a random variable on (S, B(F), P), that f 2 is integrable on S
and ﬁnd the expected value E(f) and variance V ar(f) of f.
6.11. Give an example of an everywhere continuous, but not absolutely con-
tinuous monotone non-decreasing function on the real line. [Hint: use the
Cantor set deﬁned at the end of Sect. 6.1.]
6.12. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Find the distribution function of the random variable f deﬁned as f(e) = e3.
Is f a continuous random variable? If yes, what is its density function? Prove
your conclusions.

Exercises
207
6.13. Consider the probability space (S, B(F), P) from Exercise 6.5 and let
f : S →R be deﬁned as follows
f(e) = x + 2y,
where e = (x, y). Prove that f is a random variable on (S, B(F), P), that f 2
is integrable on S and ﬁnd the expected value E(f) and variance V ar(f) of f.
In addition, ﬁnd the distribution function Ff of f. Is f a continuous random
variable? If yes, what is its density function? Prove your conclusions.
6.14. Let f be a random variable on a probability space (S, B, P) having a
binomial distribution. Find
"
S f 3dP.
6.15. Let f be a random variable on a probability space (S, B, P) having a
geometric distribution. Find
"
S f 4dP.
6.16. Let f be a random variable on a probability space (S, B, P) having a
normal distribution. Find
"
S f 3(f −1)dP.
6.17. Let f be a random variable on a probability space (S, B, P) having an
exponential distribution. Find
"
S f 5dP.
6.18. Consider the probability space (S, B(F), P) from Exercise 6.5 and let
f : S →R2 be the following vector-valued function
f(e) = (x2, sin y),
where e = (x, y). Prove that f is a vector-valued random variable on
(S, B(F), P), ﬁnd the distribution function Ff of f and show that the com-
ponents of f are independent random variables.
6.19. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14
and let f : S →R3 be the following vector-valued random variable
f(e) = (e2, e3, exp(e)).
Are the components of f independent random variables? Prove your conclu-
sion.
6.20. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Consider the following sequence {fk} of random variables on it: fk(e) = ek+e.
Prove that fk
P→f, where f(e) = e.
6.21. Let fk
D
→f, and assume that Ff is continuous everywhere on R. Prove
that Ffk converges to Ff uniformly on R.
6.22. Let {fk} be a sequence of random variables on a probability space
(S, B, P) that converges almost certainly to a function f : S →R. Prove
that f is a random variable on (S, B, P).

208
6 Elements of Probability Theory
6.23. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Deﬁne a sequence of functions fk : S →R as follows
fk(e) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−k2e + k,
if 0 ≤e ≤1
k ,
0,
if 1
k ≤e ≤1.
Show that {fk} converges almost certainly, ﬁnd its limit and the set of con-
vergence.
6.24. Let (S, B(F), P) be the probability space from Part 3 of Example 6.14.
Deﬁne a sequence of random variables fk : S →R as follows
fk(e) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
2k√e,
if 0 ≤e ≤1
2k ,
√
−4k2e + 4k,
if 1
2k ≤e ≤1
k ,
0,
if 1
k ≤e ≤1.
Prove that {fk} converges to f ≡0 almost certainly, but does not converge
to f in the mean.

7
Signiﬁcance of Sequence Alignment Scores
In this chapter we generally follow the line of argument in [EG], but our
exposition is more focused and mathematically rigorous. In particular, we
carefully describe the sample spaces on which each of the random variables
involved is deﬁned.
7.1 The Problem
In this section we return to the problem of aligning two biological sequences
considered in Chap. 2. Suppose that for two given sequences we have been
able to ﬁnd optimal alignments (local or global, gapped or ungapped) for
some scoring scheme, and assume that these alignments have a high score
that we denote by s0. Now we want to know if the alignments found are bio-
logically meaningful and give evidence for homology, or they are just some of
the best alignments between two unrelated sequences. There are two possible
approaches to this problem: one is classical, the other is Bayesian. In this
book we only deal with the more commonly used classical approach.
The classical approach will require applying the probability theory pre-
sented in Chap. 6 to the sequence alignment problem. We wish to calculate
the probability of the event that the best alignments between two “randomly
generated sequences” (as explained below) have score greater than or equal
to s0. If this probability is small (for example, does not exceed 0.05), then we
say that the similarity observed between the original sequences is signiﬁcant
which indicates that the sequences are possibly homologous. Otherwise, we
say that the similarity is not signiﬁcant which indicates that the sequences
are probably unrelated.
The problem is most interesting for local alignments. We will consider the
problem only for the case of ungapped local alignments. It is complex enough
even in this case. The corresponding problem for global ungapped alignments
is much easier and will be dealt with in Sect. 8.4 within the general framework
of statistical hypothesis testing.

210
7 Signiﬁcance of Sequence Alignment Scores
Let x0 be a sequence of length N1, y0 be a sequence of length N2, and sup-
pose that we have found some of the optimal ungapped alignments between
segments in x0 and segments in y0. Denote by s0 the score of these alignments.
Let Q be the alphabet corresponding to x0 and y0 (recall that in our appli-
cations Q is either the DNA alphabet, or the RNA alphabet, or the amino
acid alphabet). For ungapped alignments scores are calculated using a substi-
tution matrix alone, and such a matrix whose elements we, as before, denote
by s(a, b), with a, b ∈Q, will be ﬁxed from now on. We assume that s(a, b)
are integers for all a, b ∈Q and hence s0 is assumed to be a large integer. Let
pa be the frequency of a ∈Q determined from x0 and p′
b be the frequency of
b ∈Q determined from y0, calculated for all a, b ∈Q (in applications when
a query sequence x0 is compared to every sequence y0 in a database, {p′
b}
are either calculated from the whole database, or taken from some published
sources). Let S be the collection of all pairs (x, y) of sequences of letters from
Q, where x has length N1 and y has length N2. We will deﬁne a probability
measure on S as follows: for (x, y) ∈S with x = x1 . . . xN1, y = y1 . . . yN2 set
P

{(x, y)}

=
N1

i=1
pxi ×
N2

j=1
p′
yj.
Since S is a ﬁnite set and since the probability measure P is deﬁned for every
elementary event, it can be extended to the σ-algebra B of all events in S.
One can think of the probability space (S, B, P) as the collection of pairs of
sequences (x, y), where x has length N1, y has length N2, and x and y are
generated by independent random processes from the frequencies {pa} and
{p′
b} respectively, with each site in x and y being generated independently of
the others.
On the probability space (S, B, P) we deﬁne a random variable s as follows:
for (x, y) ∈S we set s((x, y)) to be the score of any optimal local alignment
between x and y. Under the classical approach we are interested in determining
the probability P

{(x, y) ∈S : s((x, y)) ≥s0}

= P

{(x, y) ∈S : s((x, y)) >
s0 −1}

= 1−Fs(s0 −1) = F ∗
s (s0 −1). If F ∗
s (s0 −1) < 0.05, then we say that
the similarity observed between x0 and y0 is signiﬁcant; otherwise we say that
it is not signiﬁcant. The problem of calculating an approximation to Fs(α)
for large values of α will be addressed in considerable detail in subsequent
sections. In this section we will only present an argument that gives some
indication of what the answer may look like.
Let (x, y) ∈S with x = x1 . . . xN1, y = y1 . . . yN2. A general local ungapped
alignment between x and y is given as follows
xi . . . xi+N
yj . . . yj+N,
for some 1 ≤i ≤N1, 1 ≤j ≤N2 and N ≤min{N1 −i, N2 −j}. If we denote
this alignment by Ai,j,N(x, y), then for its score S

Ai,j,N(x, y)

we have

7.2 Random Walks
211
S

Ai,j,N(x, y)

= s(xi, yj) + s(xi+i, yj+1) + . . . + s(xi+N, yj+N).
The random variables s(xi+k, yi+k) on the probability space (S, B, P) are
clearly iid. Hence by the Central Limit Theorem (Theorem 6.64), if N is large,
the distribution function of S

Ai,j,N(x, y)

is approximated by the distribu-
tion function of a normally distributed random variable. Hence, the scores of
long local alignments can be assumed to be normally distributed. Clearly, we
have
s((x, y)) = max
i,j,N S

Ai,j,N(x, y)

.
The random variables S

Ai,j,N(x, y)

are not independent, but for the pur-
poses of our (not entirely rigorous) argument we assume that they are inde-
pendent. We also assume that they are identically distributed. Hence, we are
considering the following situation: we are given a collection {f1, . . . , fM} of iid
normally distributed random variables on some probability space (S′, B′, P ′),
and we need to ﬁnd the distribution function of the random variable fmax =
max{f1, . . . , fM}. The distribution of fmax is called the extreme value distrib-
ution for {f1, . . . , fM}. Let F be the distribution function of the fk’s. Then
we have
Ffmax(α) = P

{e ∈S′ : fmax(e) ≤α}

= P

{e ∈S′ : f1(e) ≤α, . . . ,
fM(e) ≤α}

=
M

k=1
P

{e ∈S′ : fk(e) ≤α}

= F(α)M.
One can now show that as M and α become large, we have
Ffmax(α) ≈exp(−KM exp(−λα)),
or
F ∗
fmax(α) ≈1 −exp(−KM exp(−λα)),
(7.1)
for some constants K > 0 and λ > 0. In the following sections we will derive
in a much more rigorous way a distribution of a form similar to that in (7.1)
that gives an approximation to F ∗
s (α), if N1, N2 and α are suﬃciently large.
7.2 Random Walks
In this section we will introduce and study an important class of random
processes.
Deﬁnition 7.1. A random walk is a discrete-time process that starts at 0 and
can move up or down by one of ﬁnitely many prescribed values (step sizes)
with prescribed probabilities independently of previously made moves.

212
7 Signiﬁcance of Sequence Alignment Scores
We will always assume that the set of all possible step sizes in a random walk
has the form T = {−c, −c + 1, . . . , 0, . . . , d −1, d} for some ﬁxed c, d ∈N and
denote the respective probabilities by p−c, p−c+1, . . . , pd; these probabilities
are required to sum up to 1. We will now introduce a random variable called
the step size whose probability distribution is {pj, j = −c, . . . , d}. Deﬁne a
probability measure on T by setting P({j}) = pj for all j ∈T and let the step
size on the resulting probability space (T, BT , P), where BT is the σ-algebra
of all events in T, be the identity mapping from T into itself. The random
walk can be identiﬁed with this random variable.
We assume three conditions throughout
(i) p−c > 0 and pd > 0,
(ii) the step size has a negative mean, that is, d
j=−c jpj < 0,
(iii) the greatest common divisor of all positive elements j ∈T for which
pj > 0 is equal to 1.
We will now associate a random walk with the substitution matrix (s(a, b))
and frequencies {pa}, {p′
b}. Let T be the collection of all integers between the
minimal element ˜s and the maximal element ˆs of the substitution matrix. If j
is an element of the substitution matrix, we deﬁne the corresponding proba-
bility as pj = 
(a,b):s(a,b)=j pap′
b. If ˜s < j < ˆs and s(a, b) ̸= j for all a, b ∈Q,
we set pj = 0. In accordance with the above requirements, in the future we
will always assume that the elements of the substitution matrix satisfy the
following conditions
(iv) ˆs > 0, pˆs > 0, and p˜s > 0,
(v) 
a,b∈Q s(a, b)pap′
b < 0,
(vi) the greatest common divisor of the positive elements of the substitution
matrix is equal to 1.
Note that condition (v) implies that the minimal element ˜s in the substitution
matrix is negative, as required.
In the remainder of this section we will consider random walks without
reference to sequence comparison, to which we will return in the following
section. The results presented in this section were ﬁrst obtained in [KD].
Let Sw be the collection of all possible trajectories of a random walk, that
is, the collection of all inﬁnite sequences t = t1, t2, . . ., where tj ∈T. Figure
7.1 shows a trajectory for a random walk with step sizes −1, 0, 1.

7.2 Random Walks
213
Fig. 7.1.
The crosses in this ﬁgure relate to ladder points, that is, to points in the
trajectory lower than any previously reached point. The part of the trajectory
from a ladder point until the highest point visited before the next ladder point,
is called an excursion.
One can think of Sw as the sample space that consists of all outcomes of
the random walk if we think of it as a sequence-generation process. We will
now introduce a probability measure on Sw. Consider the following family Fw
of events in Sw: Fw includes Sw, ∅and all cylinder events, that is, events of
the form
Et0
i1,...,t0
im =

t ∈Sw : ti1 = t0
i1, . . . , tim = t0
im

,
(7.2)
for all ﬁnite subsets of indices i1 < . . . < im and all possible t0
i1, . . . , t0
im ∈T.
It is easy to check that Fw is a semi-algebra (see Exercise 7.1), hence we can
consider the minimal σ-algebra Bw = B(Fw) generated by Fw. We deﬁne a
probability measure on Fw as follows. Set P(Sw) = 1, P(∅) = 0 and
P

Et0
i1,...,t0
im

=
m

k=1
pt0
ij .
(7.3)
It is not hard to check that P is a probability measure on Fw (see Exercise
7.2), and therefore it can be extended to Bw. The resulting probability space
(Sw, Bw, P) is the space we will work on while studying random walks. We
remark that (Sw, Bw, P) is simply the inﬁnite Cartesian power of the proba-
bility space (T, BT , P) as deﬁned in Sect. 6.2 (see Exercise 7.3). We also note
that if pj ̸= 0 for all j ∈T, then the σ-algebra to which P extends from Fw
by the Lebesgue extension procedure can be essentially identiﬁed with the
σ-algebra of Lebesgue measurable sets in [0, 1].

214
7 Signiﬁcance of Sequence Alignment Scores
For every k ∈N deﬁne a random variable fk on (Sw, Bw, P) by setting
fk(t) = tk. Clearly, {fk} is a sequence of iid random variables, and each of
them has the probability distribution of the step size {p−c, p−c+1, . . . , pd}.
Property (ii) above states that µ = E(fk) is negative. By the Strong Law of
Large Numbers (see Theorem 6.67), 1/n n
k=1 fk
a.c.
→µ, and hence for large n,
n
k=1 fk(t) = n
k=1 tk is a negative number with large absolute value almost
certainly on Sw. This means that all trajectories t ∈Sw apart from those in
an event E0 of probability 0, eventually “drift” to −∞.
For the purposes of sequence alignment, we will be interested in the follow-
ing function on (Sw, Bw, P). For t ∈Sw, deﬁne Y (t) to be the maximal value
visited by the trajectory t until t reaches -1, that is, Y (t) is the height of the
excursion between the ﬁrst ladder point 0 and the second ladder point (the
ﬁrst negative value visited by t). Clearly, Y is deﬁned everywhere on Ec
0. Since
we will only be interested in the distribution of Y , we deﬁne Y arbitrarily on
E0. It can be shown that Y is a random variable on (Sw, Bw, P) (see Exercise
7.4). Our goal is to determine the behavior of FY (α) or F ∗
Y (α) , where α is a
positive integer, as α →∞. Speciﬁcally, we will show that Y is geometric-like
(see Deﬁnition 6.48). For convenience we introduce another function ˜Y deﬁned
as follows: for a trajectory t ∈Ec
0 deﬁne ˜Y (t) as the maximal value visited
by t and deﬁne ˜Y arbitrarily on E0. It can be shown that ˜Y is also a random
variable on (Sw, Bw, P) (see Exercise 7.4). We will study the behavior of F ∗
Y
and F ∗
˜Y .
Let α be a positive integer. For j = 1, . . . , c denote by R−j the probability
of the event that the ﬁrst negative value that a trajectory visits is −j. More
precisely, R−j is the probability of the following event
E−j =

t ∈Sw : for some n ∈N we have
m

i=1
ti ≥0, for all m < n
and
n

i=1
ti = −j

(7.4)
(see Exercise 7.5). Then we have
F ∗
˜Y (α) = F ∗
Y (α) +
c

j=1
R−jF ∗
˜Y (α + j),
hence
F ∗
Y (α) = F ∗
˜Y (α) −
c

j=1
R−jF ∗
˜Y (α + j).
(7.5)
Therefore, to study the behavior of F ∗
Y (α) (or, equivalently, FY (α)) we can
study that of F ∗
˜Y (α) (or, equivalently, F ˜Y (α)).
We now need the following deﬁnition.

7.2 Random Walks
215
Deﬁnition 7.2. Let f be a discrete random variable that takes ﬁnitely many
values r1, r2, . . . , rn, with ri ̸= rj for i ̸= j, and let {pf(rj), j = 1, . . . , n} be
the probability distribution of f. Then the following function of a real variable
Mf(θ) = E(exp(θf)) =
n

j=1
exp(θrj)pf(rj),
θ ∈R,
is called the moment-generating function of f.
Of course, a moment-generating function can be introduced for any random
variable, but it may not be deﬁned for all θ ∈R (see Exercise 7.6).
We will now prove the following theorem.
Theorem 7.3. Let f be a discrete random variable that takes ﬁnitely many
values and such that E(f) ̸= 0. Suppose that f takes a positive value a and a
negative value b with positive probabilities pf(a) and pf(b) respectively. Then
there exists a unique non-zero θ∗∈R such that Mf(θ∗) = 1.
Proof: We have
Mf(θ) > exp(θa)pf(a),
Mf(θ) > exp(θb)pf(b),
for all θ ∈R. Thus Mf(θ) →∞as θ →±∞. Further,
M′′
f(θ) =
n

j=1
r2
j exp(θrj)pf(rj) > 0,
for all θ ∈R, that is, Mf is a convex function. We also have Mf(0) = 1 and
M′
f(0) = E(f) ̸= 0. Together with the convexity of f this implies that there
exists a unique non-zero θ∗∈R such that Mf(θ∗) = 1.
It is clear from the proof of Theorem 7.3 that θ∗> 0, if E(f) < 0 and θ∗< 0,
if E(f) > 0.
We will now apply Theorem 7.3 to the random walk we are considering.
Let M be the moment-generating function of the step size
M(θ) =
d

j=−c
exp(θj)pj.
Clearly, the step size satisﬁes the conditions of Theorem 7.3, and therefore
there exists a unique non-zero θ∗such that M(θ∗) = 1, that is
d

j=−c
exp(θ∗j)pj = 1.
(7.6)
Since the step size has a negative mean, we have θ∗> 0.

216
7 Signiﬁcance of Sequence Alignment Scores
For k = 1, 2 . . . denote by Qk the probability of the event that a trajectory
visits k before visiting any other positive value. More precisely, Qk is the
probability of the following event
Ek =

t ∈Sw : for some n ∈N we have
m

i=1
ti ≤0, for all m < n
and
n

i=1
ti = k

(7.7)
(see Exercise 7.5). Clearly, Qk = 0 for all k > d, and we also set for convenience
Q0 = 0. Since the probability of the event that a trajectory never visits any
positive values is non-zero, we have d
k=1 Qk < 1. To ﬁnd the behavior of
F ˜Y (α), where α is a positive integer, we note that
F ˜Y (α) = Q +
α

k=0
QkF ˜Y (α −k),
(7.8)
where Q = 1 −d
k=1 Qk. Deﬁne
V (α) = F ∗
˜Y (α) exp(θ∗α).
(7.9)
We will now show that the limit limα→∞V (α) exists and determine its value.
Equation (7.8) can be rewritten as
1 −V (α) exp(−θ∗α) = Q +
α

k=0
Qk

1 −V (α −k) exp(−θ∗(α −k))

,
which gives
V (α) = exp(θ∗α)
d

k=α+1
Qk +
α

k=0
(Qk exp(θ∗k))V (α −k),
if α < d, (7.10)
and
V (α) =
d

k=0
(Qk exp(θ∗k))V (α −k),
if α ≥d.
(7.11)
Next, we will use the following fact that we state without proof.
Theorem 7.4. (The
Renewal
Theorem)Suppose
three
sequences
{a0, a1, . . .}, {b0, b1, . . .} and {c0, c1, . . .} of non-negative numbers satisfy the
equation
cj = aj + (cjb0 + cj−1b1 + . . . + c1bj−1 + c0bj),
(7.12)
for all j ≥0. Suppose further that the sequence {cj} is bounded, ∞
j=0 bj = 1
and that the series ∞
j=0 aj and ∞
j=0 jbj converge. Denote respectively by A

7.2 Random Walks
217
and µ the sums of these series. Assume in addition that the greatest common
divisor of the integers j for which bj > 0 is equal to 1. Then the limit limj→∞cj
exists and
lim
j→∞cj = A
µ .
A proof of Theorem 7.4 can be found in [Kar].
Theorem 7.4 can be used to ﬁnd the limit limα→∞V (α). Indeed, set
aj = exp(θ∗j) d
k=j+1 Qk for j < d and aj = 0 for j ≥d. Also set
bj = Qj exp(θ∗j), cj = V (j) for all j ≥0. Clearly, the series ∞
j=0 aj and
∞
j=0 jbj converge. Identities (7.10) and (7.11) show that condition (7.12) is
satisﬁed. Also, requirement (iii) from the beginning of this section guarantees
that the greatest common divisor of the integers j for which bj > 0 is equal
to 1. In order to use Theorem 7.4 we further need to show that ∞
j=0 bj = 1,
that is,
d

k=1
Qk exp(θ∗k) = 1.
(7.13)
To obtain (7.13), choose L ∈N, and denote by Qk(L), for k = 1, . . . , d,
the probability of the event that a trajectory visits k before visiting any other
positive value and before reaching −L; also denote by Qk(L), for k = −L −
c + 1, . . . , −L, the probability of the event that a trajectory visits k before
visiting any positive values and does not reach −L before visiting k. Clearly,
we have
lim
L→∞Qk(L) = Qk,
for k = 1, . . . , d.
We now need the following theorem that we also state without proof. A
proof can be found in [KT].
Theorem 7.5. (Wald’s Identity) Let N denote a random variable on
(Sw, Bw, P) whose value N(t), for every t ∈Ec
0, is equal to the number of
steps that t takes to reach either a positive value or −L for the ﬁrst time. Let
further TN denote the random variable on (Sw, Bw, P) whose value, for every
t ∈Ec
0, is equal to the value that t visits after N(t) steps. Then for all θ ∈R
we have
E

M(θ)−N exp(θTN)

= 1.
(7.14)
Note that in particular Theorem 7.5 states that M(θ)−N exp(θTN) is inte-
grable on Sw with respect to P for every θ ∈R.
Wald’s identity (7.14) for θ = θ∗becomes
E

exp(θ∗TN)

= 1,
that is,

218
7 Signiﬁcance of Sequence Alignment Scores
−L

k=−L−c+1
Qk(L) exp(θ∗k) +
d

k=1
Qk(L) exp(θ∗k) = 1.
Letting L →∞in this identity gives identity (7.13), as required.
Identity (7.13) means that ∞
j=0 bj = 1. It also shows that the sequence
{cj} is bounded (see Exercise 7.7). Hence the Renewal Theorem can be applied
to the three sequences {aj}, {bj}, {cj} introduced above. We then obtain that
the limit limα→∞V (α) exists and is equal to V = A/µ, where A = ∞
j=0 aj
and µ = ∞
j=0 jbj. We will now ﬁnd A. We have
A =
∞

j=0
aj =
d

j=0
exp(θ∗j)
d

k=j+1
Qk.
Multiplying both parts of this identity by exp(θ∗) −1 we obtain
A(exp(θ∗)−1) =
d

k=1
Qk exp(θ∗k)−(Q1 +. . .+Qd) = 1−(Q1 +. . .+Qd) = Q,
where we again used identity (7.13). Hence
A =
Q
exp(θ∗) −1,
and therefore
V =
Q
(exp(θ∗) −1)
d

k=1
kQk exp(θ∗k)
.
(7.15)
Now from (7.9) we obtain
F ∗
˜Y (α) ∼V exp(−θ∗α),
with V given by formula (7.15). Therefore formula (7.5) yields
F ∗
Y (α) ∼C exp(−θ∗(α + 1)),
where
C =
Q
⎛
⎝1 −
c

j=1
R−j exp(−θ∗j)
⎞
⎠
(1 −exp(−θ∗))
d

k=1
kQk exp(θ∗k)
.
(7.16)
Thus, Y is geometric-like (note that C > 0).
Next, let N ′ be the following random variable on (Sw, Bw, P): for a tra-
jectory t ∈Ec
0 deﬁne N ′(t) to be the number of steps that t takes to reach

7.2 Random Walks
219
the value -1. We will ﬁnd E(N ′). Let TN′(t) be the value that t visits after
N ′ steps. One can show that Theorem 7.5 holds for N ′ and TN′ in place of N
and TN as well, and therefore for all θ ∈R we have
E

M(θ)−N′ exp(θTN′)

= 1.
(7.17)
Since N ′ is a random variable that takes inﬁnitely many values, the expression
in the left-hand side of identity (7.17) is an inﬁnite series. It is possible to prove
that this series can be diﬀerentiated term by term, and hence we obtain
d
dθE

M(θ)−N′ exp(θTN′)

= E
 d
dθ

M(θ)−N′ exp(θTN′)

= E

−N ′M(θ)−N′−1 exp(θTN′) d
dθM(θ) + M(θ)−N′TN′ exp(θTN′)

.
Therefore (7.17) implies
E

−N ′M(θ)−N′−1 exp(θTN′) d
dθM(θ) + M(θ)−N′TN′ exp(θTN′)

= 0,
which for θ = 0 gives
−E(N ′)
d

j=−c
jpj + E(TN′) = 0,
that is,
E(N ′) = E(TN′)
d

j=−c
jpj
.
Clearly, the probability distribution of TN′ is {R−j, j = 1, . . . , c}. Therefore,
E(TN′) = −
c

j=1
jR−j,
and hence
E(N ′) = −
c

j=1
jR−j
d

j=−c
jpj
.
(7.18)

220
7 Signiﬁcance of Sequence Alignment Scores
7.3 Signiﬁcance of Scores
In this section we will apply the theory of random walks from Sect. 7.2 to
determine the behavior of Fs, the distribution function of the random variable
s on the probability space (S, B, P) introduced in Sect. 7.1. Let (x, y) ∈S.
Every local ungapped alignment between x and y can be extended as far as
possible in either direction and thus deﬁnes a particular overlap between x
and y. There are a total of N1 + N2 −1 such overlaps. Figure 7.2 shows some
overlaps between two DNA sequences of lengths 8 and 10.
Fig. 7.2.
We now ﬁx a particular overlap O of some length N ≤min{N1, N2}, the
same for all (x, y) ∈S, and consider a random variable sO on (S, B, P) with
sO((x, y)) deﬁned as the score of an ungapped local alignment that has the
highest score among all ungapped local alignments between x and y that ﬁt
in the overlap O. We will now determine the behavior of FsO. Let SN be the
collection of all pairs (x, y) of sequences of length N of letters from Q. We will
deﬁne a probability measure on SN analogously to that on S: for (x, y) ∈S
with x = x1 . . . xN, y = y1 . . . yN set
P

{(x, y)}

=
N

i=1
pxip′
yi.
Since SN is a ﬁnite set and since the probability measure P is deﬁned for every
elementary event, it can be extended to the collection BN of all events in SN.
Thus, we obtain a probability space (SN, BN, P). The random variable sO
was originally introduced for the probability space (S, B, P), but can also be
naturally deﬁned on (SN, BN, P). It is easy to observe that the distributions
of sO on these two probability spaces coincide (see Exercise 7.8). Therefore,

7.3 Signiﬁcance of Scores
221
for the purposes of studying FsO we can consider sO on (SN, BN, P), which
will be a more convenient setup for us.
Consider the random walk associated with the substitution matrix (s(a, b))
and the frequencies {pa}, {p′
b}, as at the beginning of Sect. 7.2. Let (Sw, Bw, P)
be the corresponding probability space. We will now consider a “truncated”
probability space deﬁned as follows. Let Sw,N be the collection of all possible
trajectories t of length N. We deﬁne a probability measure on Sw,N as follows.
For a trajectory t = t1, . . . , tN, with tj ∈T, set
P

{t}

=
N

j=1
ptj.
Since P is deﬁned for all elementary events in Sw,N and since Sw,N is ﬁ-
nite, P can be extended to the collection Bw,N of all events in Sw,N. In
our considerations N is ﬁxed, but if for the moment we allow N →∞,
then the resulting sequence of probability spaces {(Sw,N, Bw,N, P)} can be
thought of as an approximation of the probability space (Sw, Bw, P). Indeed,
if we consider only the initial N elements t1, . . . , tN in every t from a cylin-
der event Et0
i1,...,t0
im deﬁned in (7.2), the probability of the resulting event
Et0
i1,...,t0
im,N ⊂Sw,N tends to the probability of Et0
i1,...,t0
im , as N →∞. In
fact, P(Et0
i1,...,t0
im,N) = P(Et0
i1,...,t0
im ), if N ≥im.
Let YN be a random variable on (Sw,N, Bw,N, P) analogous to the random
variable Y on (Sw, Bw, P) considered in Sect. 7.2. Namely, if a trajectory
t ∈Sw,N visits negative values, deﬁne YN(t) to be the maximal value achieved
by the trajectory t until it reaches -1, in other words, YN(t) in this case is
the height of the excursion between the ﬁrst ladder point 0 and the second
ladder point (the ﬁrst negative value visited by t); if a trajectory t never visits
negative values, deﬁne YN(t) to be the maximal value achieved by t. One can
show that YN
D
→Y (see Exercise 7.9). In particular, if both N and α are large,
we have
F ∗
YN (α) ≈C exp(−θ∗(α + 1)),
where θ∗and C are found from formulas (7.6) and (7.16) respectively.
We will now relate the distribution of YN on (Sw,N, Bw,N, P) to that of
sO on (SN, BN, P). Each element (x, y) ∈SN obviously generates a trajec-
tory t((x, y)) ∈Sw,N, and every t ∈Sw,N is generated by some element of
SN. Note, however, that the mapping (x, y) →t((x, y)) is not one-to-one;
it is possible that for (x′, y′) ̸= (x, y), we have t((x′, y′)) = t((x, y)), for ex-
ample, t(y, x) = t(x, y) for all (x, y) ∈SN. Consider a random variable Y ′
N
on (SN, BN, P) deﬁned as Y ′
N((x, y)) = YN(t((x, y))). It is straightforward to
verify that the distribution of Y ′
N on (SN, BN, P) coincides with that of YN
on (Sw,N, Bw,N, P) (see Exercise 7.10). Hence for large N and α we have
F ∗
Y ′
N (α) ≈C exp(−θ∗(α + 1)).
(7.19)

222
7 Signiﬁcance of Sequence Alignment Scores
Further, let t ∈Sw,N be a trajectory that visits negative values. We will
modify it into a path t in the following way. Suppose that t visits its ﬁrst
negative value after m steps. Then we leave the portion of t up to and including
the (m −1)th step unchanged, set the value of t at the mth step to be equal
to 0, and continue along the steps taken by t starting from 0 until t reaches
its next negative value, after which we will repeat the above procedure, etc.
If t never visits any negative values, we set t = t. The path t never visits any
negative values. Note that it is possible that t1 = t2, if t1 ̸= t2. Figure 7.3
shows the path for a portion of the trajectory in Fig. 7.1.
Fig. 7.3.
As for random walk trajectories, one can introduce the notion of an ex-
cursion of a path t as a part of t starting at 0 until the highest point visited
by t before the next 0. In this terms, for (x, y) ∈SN, the value sO((x, y))
is the maximum of the heights of the excursions of the path t((x, y)). The
maximum of the heights of excursions of t((x, y)) between the ﬁrst 0 and the
second ladder point of t((x, y)) is Y ′
N((x, y)). If for every element (x, y) ∈SN
the trajectory t((x, y)) had the same number of ladder points n and the last
step of t((x, y)) was a ladder point as well, then we would have
sO((x, y)) = max{Y1((x, y)), . . . , Yn((x, y))} = Ymax((x, y)),
(7.20)
where Yj are iid random variables on (SN, BN, P), each having the distribution
of Y ′
N (in fact, Y ′
N itself would have been one of Yj). In reality, of course, the
number of ladder points of t((x, y)) depends on (x, y), and the last step of
t((x, y)) may not be a ladder point (the latter eﬀect is sometimes called the
edge eﬀect). In our considerations we will ignore these complications, and as
an approximation to the distribution of sO we will study the distribution

7.3 Signiﬁcance of Scores
223
of the random variable Ymax in the right-hand side of identity (7.20) with
n = N/E(N ′), where E(N ′) is found from formula (7.18). In fact, one can
show that, as N →∞, the distribution functions of sO and Ymax become
arbitrarily close.
We will calculate an approximation to F ∗
Ymax(α), where α is a large integer.
We will also assume that N is large and therefore use the approximations
F ∗
Yj(α) ≈C exp(−θ∗(α + 1)),
j = 1, . . . , n,
(7.21)
that follow from (7.19). From formulas (7.21) we have
F ∗
Ymax(α) = 1 −FYmax(α) = 1 −
n

j=1
FYj(α) = 1 −
n

j=1
(1 −F ∗
Yj(α))
≈1 −

1 −C exp(−θ∗(α + 1))
n
.
Let β = C exp(−θ∗(α+1)). If α is large, we can approximate 1−β by exp(−β)
and hence we have
F ∗
Ymax(α) ≈1 −exp

−nC exp(−θ∗(α + 1))

.
Therefore for F ∗
sO(α) with large N and α we obtain
F ∗
sO(α) ≈1 −exp

−nC exp(−θ∗(α + 1))

,
or
FsO(α) ≈exp

−nC exp(−θ∗(α + 1))

.
(7.22)
Recall that in Sect. 7.1 we formulated the problem of the signiﬁcance of
sequence alignment scores as the problem to calculate F ∗
s (s0 −1), where s
is the random variable on the probability space (S, B, P) introduced in Sect.
7.1, and s0 is a large integer equal to the score of an optimal local alignment
between two ﬁxed sequences x0 and y0. Clearly, for all (x, y) ∈S we have
s((x, y)) = max
O sO((x, y)),
where the maximum is taken over all possible overlaps O. Suppose for the
moment that the random variables sO are independent and that for each sO
approximation (7.22) held for large α with n = NO/E(N ′), where NO is the
length of the overlap O. Then we have

224
7 Signiﬁcance of Sequence Alignment Scores
Fs(α) =

O
FsO(α) ≈exp

−
C
E(N ′) exp(−θ∗(α + 1))

O
NO

.
Suppose for convenience that N1 ≥N2. Then we obtain

O
NO = 2
N2−1

j=1
N2+N2(N1−N2+1) = N2(N2−1)+N2(N1−N2+1) = N1N2,
which gives
Fs(α) ≈exp

−
C
E(N ′)N2N2 exp(−θ∗(α + 1))

,
or
F ∗
s (α) ≈1 −exp

−
C
E(N ′)N1N2 exp(−θ∗(α + 1))

.
(7.23)
To derive formula (7.23) we assumed that the random variables sO were
independent and that for each sO approximation (7.22) holds for large α.
However, sO are in fact not independent. Further, approximation (7.22) de-
pends on the assumption that the length of the overlap is large, whereas, of
course, there are very short overlaps. There exists a general theory that deals
with these and other complications [DKZ1], [DKZ2]. This theory is beyond
the scope of this book; we only state here that under some assumptions it
can be shown that the right-hand side of formula (7.23) indeed serves as a
reasonable approximation to F ∗
s (α) (see a discussion in [EG]). We also note in
passing that a better approximation can be achieved by taking into account
the edge eﬀect.
Since s0 is a large integer, we will use the approximation for F ∗
s (s0 −1)
found from (7.23) with α = s0 −1 to test the signiﬁcance of the score of
any optimal local alignment between x0 and y0. Note that for very large
s0 (such that

C/E(N ′)

N1N2 exp(−θ∗s0) is close to 0) we can rewrite the
approximation to F ∗
s (s0 −1) in the form
F ∗
s (s0 −1) ≈
C
E(N ′)N1N2 exp(−θ∗s0).
(7.24)
We will now introduce some notation standard for sequence similarity
searches. First of all, θ∗is usually denoted by λ. Further, let
K =
C
E(N ′) exp(−λ).
(7.25)
In this notation formulas (7.23) and (7.24) take the forms
F ∗
s (s0 −1) ≈1 −exp

−KN1N2 exp(−λ(s0 −1))

,
(7.26)

7.3 Signiﬁcance of Scores
225
and
F ∗
s (s0 −1) ≈KN1N2 exp(−λ(s0 −1)),
(7.27)
respectively, where in formula (7.27) we assume that KN1N2 exp(−λ(s0 −1))
close to 0. Note that the right-hand side in formula (7.1) is analogous to that
in formula (7.26) with M = N1N2, which is the total number of the starts
of all possible local alignments between two sequences of lengths N1 and N2
respectively, but not the total number of local alignments, as M was taken to
be in (7.1). This inconsistency arises because of our assumption in Sect. 7.1
that the random variables S

Ai,j,N(x, y)

are independent.
The value F ∗
s (s0 −1) is called the P-value and is sometimes denoted
by P(s0 −1). The expression KN1N2 exp(−λ(s0 −1)) is called the E-value
and is sometimes denoted by E(s0 −1). One can show that the E-value is
approximately the mean number of local alignments between (x, y) ∈S with
score ≥s0. Using this notation we can rewrite formulas (7.26) and (7.27) as
P(s0 −1) ≈1 −exp(−E(s0 −1)),
(7.28)
and, if E(s0 −1) is close to 0,
P(s0 −1) ≈E(s0 −1).
To use formula (7.26) or formula (7.27) we must compute the constants λ
and K. Recall that λ is found from equation (7.6) which for the case of the
random walk associated with the matrix (s(a, b)) and frequencies {pa}, {p′
b}
becomes

a,b∈Q
pap′
b exp(λs(a, b)) = 1.
(7.29)
The above equation is usually dealt with numerically, and an approximate
value of λ is generated. Once λ has been determined, K is found from formula
(7.25) using (7.16) and (7.18). The latter two formulas contain the probabili-
ties Qk and R−j that are in fact not easy to compute analytically. However,
there exist rapidly converging methods that allow to calculate K with high
degree of accuracy. Methods for computing approximate values of λ and K
are beyond the scope of this book, but below we give an example where we
ﬁnd these values analytically for a simple case.
Example 7.6. Let Q be the DNA alphabet, pa = 1/4, p′
b = 1/4 for all a, b ∈Q,
and the substitution matrix be as in Example 2.1, that is,
(s(a, b)) =
⎛
⎜
⎜
⎝
1 −1 −1 −1
−1
1 −1 −1
−1 −1
1 −1
−1 −1 −1
1
⎞
⎟
⎟
⎠.
(7.30)
Clearly, the above substitution matrix and frequencies {pa}, {p′
b} satisfy con-
ditions (iv) and (vi) from Sect. 7.2. Condition (v) is satisﬁed as well, since

226
7 Signiﬁcance of Sequence Alignment Scores

a,b∈Q
s(a, b)pap′
b = −1
2 < 0.
(7.31)
The constant λ is easy to ﬁnd from equation (7.29). Indeed, for this ex-
ample the equation becomes
exp(λ)1
4 + exp(−λ)3
4 = 1,
and its only positive solution is λ = ln 3.
To ﬁnd K, we need to determine Q1 and R−1 (see formulas (7.16), (7.18)
and (7.25)). Clearly, R−1 = 1. In order to ﬁnd Q1 consider the associated
random walk. We have T = {−1, 0, 1}, p−1 = 3/4, p0 = 0, p1 = 1/4. For this
random walk c = d = 1, that is, it is an example of a simple random walk. It
is not hard to show (see Exercise 7.11) that
Q1 =
∞

n=1
B2n−1
1
4
n 3
4
n−1
,
(7.32)
where B2n−1 is the number of trajectories t = t1, . . . , t2n−1 in the sample
space Sw,2n−1 for which 2n−1
j=1 tj = 1 and m
j=1 tj ≤0 for m = 1, . . . , 2n−2.
Similarly, we have (see Exercise 7.11)
R−1 =
∞

n=1
B2n−1
1
4
n−1 3
4
n
,
(7.33)
and therefore
Q1 = 1
3R−1 = 1
3.
Now formulas (7.16), (7.18), (7.31), (7.25) yield that K = 1/9. For more
information on simple random walks see, for example, [EG].
We will now give an example of using the above values of λ and K for
assessing the signiﬁcance of the score of a local alignment. Suppose we are
given two sequences x0 = ACATGCTG, y0 = CATTGCGA. It is easy to see
that the frequency of each letter from the DNA alphabet in either sequence
is equal to 1/4, and therefore the above calculations for λ and K are valid for
these sequences, if we align them by means of substitution matrix (7.30). It is
easy to ﬁnd all optimal local ungapped alignments between them by applying
the Smith-Waterman algorithm described in Sect. 2.3. Looking for ungapped
alignments means that we utilize the linear gap model with d = ∞. There are
two optimal alignments
x0 : C A T
y0 : C A T,
x0 : T G C
y0 : T G C,

7.3 Signiﬁcance of Scores
227
each having score equal to 3.
Thus, we have s0 = 3, N1 = N2 = 8, λ = ln 3, K = 1/9. From (7.28) we
then obtain that the P-value is approximately equal to 0.54 which is larger
than 0.05. Therefore, on the basis of this analysis neither of the above optimal
alignments is ragarded as signiﬁcant. Of course, one should bear in mind that
we gave this example for illustration purposes only, and that in order for
approximation (7.28) to work, the values of s0, N1, N2 must be much larger.
In practice a query sequence x0 is compared not just to a single sequence
y0, but to all sequences z in a particular database D of sequences. Both
BLAST and FASTA searches involve such comparisons. In this case we are
looking for an optimal alignment between segments in x0 and segments in all
possible sequences z ∈D. Suppose that a highest-scoring alignment with a
large score s0 occurs between a segment in x0 and a segment contained in
some sequence y0 ∈D. How do we decide in this case whether the similarity
found is signiﬁcant or not? One approach is to just assess the local alignment
between x0 and y0 in the way it was done above. Such assessment, however,
would ignore the fact that the alignment was found by comparing x0 to all
sequences in D, not just the single sequence y0. In reality assessment is done
as follows. Let |D| be the sum of the lengths of all sequences in D. We then
treat the optimal local alignment as an optimal local alignment between x0
and the sequence of length |D| obtained by joining together all the sequences
contained in D (this treatment ignores some edge eﬀects that can be assumed
to be small if all sequences in D are suﬃciently long). Then the corresponding
E-value (that we denote by ED(s0 −1)) is
ED(s0 −1) = KN1|D| exp(−λs0),
and the corresponding P-value (that we denote by PD(s0 −1)) found from
(7.28) is
PD(s0 −1) ≈1 −exp

−KN1|D| exp(−λ(s0 −1))

,
that is PD(s0−1) is bigger than the P-value P(s0−1) found by from comparing
x0 to the sequence y0 alone. If PD(s0 −1) is bigger than say 0.05, we conclude
that the match that we have found is not signiﬁcant. However, it may happen
that in this case the P-value P(s0 −1) is less than 0.05. Therefore, using
P(s0 −1) instead of PD(s0 −1) leads to overestimating the signiﬁcance of the
scores of optimal local alignments.
In BLAST computations, signiﬁcance is assessed as follows. First, P(s0−1)
is computed, next, ED(s0 −1) is approximated as follows
ED(s0 −1) ≈E′
D(s0 −1) = P(s0 −1)|D|
N2
,
and, ﬁnally, an approximation to PD(s0 −1) is found as

228
7 Signiﬁcance of Sequence Alignment Scores
PD(s0 −1) ≈1 −exp(−E′
D(s0 −1)).
We note that BLAST attempts to assess not only the highest-scoring lo-
cal alignments, but other high scoring local alignments as well, for example,
second best, third best, etc. Assessing the signiﬁcance of the scores of all such
alignments requires certain modiﬁcations of the procedure discussed above
which are beyond the scope of this book. The interested reader is referred to
[KA] and a discussion in [EG].
Exercises
7.1. Prove that the collection of events Fw deﬁned in Sect. 7.2 is a semi-
algebra.
7.2. Show that formula (7.3) deﬁnes a probability measure on the semi-algebra
of events Fw from Exercise 7.1.
7.3. Consider the probability spaces (Sw, Bw, P) and (T, BT , P) deﬁned in
Sect. 7.2. Show that (Sw, Bw, P) is the inﬁnite Cartesian power of (T, BT , P).
7.4. Show that the functions Y and ˜Y deﬁned in Sect. 7.2 are random variables
on the probability space (Sw, Bw, P).
7.5. Show that the events E−j and Ek, j = 1, . . . , c, k ∈N, deﬁned in (7.4)
and (7.7) respectively are measurable, that is, belong to Bw.
7.6. For a discrete random variable f that takes inﬁnitely many values
r1, r2, . . ., with ri
̸= rj for i ̸= j, and has a probability distribution
{pf(rj), j = 1, 2 . . .} deﬁne the moment-generating function as follows
Mf(θ) = E(exp(θf)) =
∞

j=1
exp(θrj)pf(rj),
θ ∈R.
This function is only deﬁned on a subset of R for which the series in the right-
hand side of the above formula converges. Let (S, B(F), P) be the probability
space from Part 3 of Example 6.14. Deﬁne the following random variable on
it
f(e) =
⎧
⎪
⎨
⎪
⎩
0,
if e = 0,
n,
if
1
n + 1 < e ≤1
n.
Find the domain of deﬁnition of Mf.
7.7. Show that the sequence {cj} introduced in Sect. 7.2 is bounded.

Exercises
229
7.8. Show that the distribution of the random variable sO on the probability
space (SN, BN, P) coincides with its distribution on the probability space
(S, B, P) (see Sect. 7.3 for details).
7.9. Show that YN
D
→Y (see Sect. 7.3 for details).
7.10. Prove that the distribution of Y ′
N on (SN, BN, P) coincides with that of
YN on (Sw,N, Bw,N, P) (see Sect. 7.3 for details).
7.11. Prove formulas (7.32) and (7.33).
7.12. Find all optimal ungapped local alignments between the sequences x0 =
ACATGCTG, y0 = GCATGCTA using substitution matrix (7.30) and assess
the signiﬁcance of the score of each alignment, assuming that approximation
(7.28) holds for the P-value.
7.13. Let x0 = AGCTGC, y0 = GATTGACTA. For these sequences and
substitution matrix (7.30) verify that conditions (iv)-(vi) from Sect. 7.2 hold.
Further, ﬁnd all optimal ungapped local alignments between x0, y0 and as-
sess the signiﬁcance of the score of each optimal alignment assuming that
approximation (7.28) holds for the P-value.

8
Elements of Statistics
In this chapter we attempt to give a mathematically rigorous exposition of
some aspects of statistics. As in Chap. 6, we do not concentrate on proofs
here. Instead, the emphasis is on the main constructions that we illustrate by
many examples. All proofs can be found, for instance, in [W].
8.1 Statistical Modeling
We start with the following deﬁnition.
Deﬁnition 8.1. A
statistical model
is
a
family
of
probability
spaces
{(Sθ, Bθ, Pθ)} and a family of random variables {fθ} with common range
W ⊂R, each deﬁned on the respective space for θ ∈P, where P is
an index set. The variable θ denotes the parameters of the model, the set
W is called the range of the model, and the index set P the parameter
space of the model. Hence a statistical model can be thought of as a fam-
ily {(Sθ, Bθ, Pθ, fθ, W, P)}.
When studying and applying statistical models, one is primarily interested in
the distributions of fθ. Therefore, often statistical models are not speciﬁed
in full as in Deﬁnition 8.1, but only the family {Fθ = Ffθ}, θ ∈P, of the
distribution functions of fθ is given. Once the family {Fθ}, θ ∈P, is speciﬁed,
one can construct a statistical model in the sense of Deﬁnition 8.1, for example,
by setting Sθ = R, Bθ = B(F0) (the σ-algebra of Borel sets in R), Pθ = PFθ,
fθ(x) = x, W = R, for θ ∈P (see Sect. 6.7). This model, however, is not
always useful.
We also remark that one can consider more general statistical models
by allowing fθ to be vector-valued. For example, one can deﬁne a natural
product of two or more models. We will not consider such generalizations in
this chapter, but most of what follows can be easily adjusted to accommodate
them.

232
8 Elements of Statistics
We will now introduce an important special class of statistical models.
Suppose that Sθ and Bθ do not depend on θ, that is, for all θ ∈P, we have
Sθ = S and Bθ = B, with some S and B. Suppose, in addition, that S is a
countable set (ﬁnite or inﬁnite) and B is the σ-algebra of all events in S. Let
further Pθ, for θ ∈P, be a family of probability measures on B. In this case
one is often interested in Pθ, not in the distribution of any particular random
variable on (S, B, Pθ). To formally satisfy the deﬁnition of statistical model
above, we will construct a discrete random variable fθ on (S, B, Pθ) for each
value of θ ∈P, such that the probability distribution of fθ is {Pθ(e), e ∈S}.
This can be done as follows. Enumerate all elementary events in S, that is,
ﬁnd a (non-unique) one-to-one correspondence ϕ : S →N between points in
S and points in a set D ⊂N, where D is either a ﬁnite set {1, . . . , N} for
some N, or all of N. We then set
fθ = ϕ,
for all θ ∈P.
(8.1)
Clearly, fθ is a discrete random variable on (S, B, Pθ) with probability distrib-
ution {Pθ(e), e ∈S}, as required. Many statistical models we will be interested
in are of this type. For convenience we will call such models reduced statistical
models. The range of a reduced statistical model is the corresponding set D.
We will now give some examples of statistical models.
Example 8.2.
1. Suppose we are given a family of Markov chains with an end state
arising from a ﬁxed a priori connectivity. The transition probabilities are
allowed to vary subject to the a priori connectivity and to the condition
of non-trivial connectedness. Consider the associated probability spaces as
constructed in Part 5 of Example 6.11. Since sequences of inﬁnite length do
not contribute to the probabilities of events, we consider the smaller sample
space that consists only of ﬁnite sequences. This sample space is countable,
and we consider the probability spaces arising from it. Note that the prob-
ability measures deﬁned in formula (6.4) depend on the transition probabil-
ities of the models. We arrange all the transition probabilities in a vector
of real numbers and call this vector θ. Clearly, θ varies over a subset P of
RK
+ = {(x1, . . . , xK) ∈RK : xj ≥0, j = 1, . . . K}, for some K. The parame-
ter space P is given by the conditions that the transition probabilities from
each state to any possible state sum up to 1, the a priori connectivity assump-
tions and by the condition of non-trivial connectedness (see, for example, the
model from Exercise 8.4). If we introduce fθ as in (8.1), we obtain a reduced
statistical model with range N.
2. Suppose we are given a family of HMMs, arising from a ﬁxed a priori
Markov chain connectivity. Consider the associated probability spaces as con-
structed in Part 6 of Example 6.11, and, as in Part 1 above, concentrate only
on sequences of ﬁnite length. We will denote the subset of Sa that consists
of pairs of ﬁnite sequences by ˆSa and the subset of Sb that consists of ﬁnite

8.1 Statistical Modeling
233
sequences by ˆSb. Then we obtain two reduced statistical model. The two dif-
ferent models are suitable for diﬀerent problems (see Part 2 of Example 8.5).
Note that the parameters of the models are the transition and emission prob-
abilities. Hence the parameter space Pa = Pb = P in this case is the product
of the parameter space from Part 1 above and a subset of R|Q|N
+
, where N is
the number of non-zero states of the model and |Q| is the number of elements
in the alphabet Q, given by the condition that the emission probabilities at
each state sum up to 1 (see, for example, the model from Exercise 8.5). In
particular, in this case the parameter space is again a subset of a Euclidean
space. The range of each of the models is N.
3. Fix an evolutionary model and consider all rooted phylogenetic trees
with N labeled leaves. The associated probability spaces as constructed in
Part 7 of Example 6.11 deﬁne a reduced statistical model. Note that in this
case the sample space is ﬁnite (it contains 4N elementary events) and the
parameters of the model are the tree and the parameters arising from the
evolutionary model (for instance, in the case of the Kimura and HKY models
such parameters arise from β – see Sect. 5.4). In particular, the parameter
space in this case is not a subset of any Euclidean space. The range of the
model is the ﬁnite set {1, . . . , 4N}.
In statistical modeling one is interested in a large number of “independent
runs of a model”. This concept is formalized in the following deﬁnition.
Deﬁnition 8.3. Let {(Sθ, Bθ, Pθ, fθ, W, P)} be a statistical model. The ran-
dom sample of size n from the model is the family {f ×∞,n
θ
} of the samples
of size n from the inﬁnite Cartesian powers of the random variables fθ, with
θ ∈P.
Recall that for every θ ∈P, f ×∞,n
θ
is deﬁned on the inﬁnite Cartesian power
of (Sθ, Bθ, Pθ), its range is Wn, and its components are iid random variables
whose distributions coincide with that of fθ.
We are now ready to deﬁne the concept of statistical modeling.
Deﬁnition 8.4. Suppose that we are given a vector w = (w1, . . . , wn) ∈Wn
(usually
in
some
way
derived
from
real
data).
We
say
that
{(Sθ, Bθ, Pθ, fθ, W, P)} is a statistical model for w or w is statistically modeled
by {(Sθ, Bθ, Pθ, fθ, W, P)}, if one tries to “ﬁt” the model to w by choosing a
value θ0 ∈P that is in some sense “optimal” for w.
The vagueness of Deﬁnition 8.4 is due to the fact that the optimality of model
ﬁt is not well-deﬁned and is often understood intuitively. As a result, there
are numerous ﬁtting procedures used for diﬀerent models. Some of them are
model-speciﬁc, but some of them are quite general. In the next section we will
discuss a universal way of model ﬁtting by means of parameter estimation. The
vector w in Deﬁnition 8.4 usually arises from real data, and hence statistical
modeling can be thought of as a process of ﬁtting a model to a dataset.

234
8 Elements of Statistics
We will now give some examples of statistical modeling arising from the
models in Example 8.2.
Example 8.5.
1. Suppose we have DNA data for n prokaryotic genes. Consider the a
priori connectivity shown in Fig. 3.2 and associate with it a statistical model
as explained in Part 1 of Example 8.2. Each of the given sequences x1, . . . , xn
can be regarded as an element of the sample space S (we can add 0 at the
beginning and end of each sequence). Consider the mapping ϕ : S →N (see
the discussion preceding formula (8.1)) and set wj = ϕ(xj), j = 1, . . . n. Let
w = (w1, . . . , wn). This DNA statistical model can be ﬁtted to the vector w,
as it was done in Sect. 3.1, namely, by using formula (3.2). Not only does this
intuitively seem to be the right way to do ﬁtting, but it in fact agrees with
the general theory of parameter estimation discussed in the next section. Of
course, this procedure can be used for arbitrary Markov chains.
2. Suppose we are given a family of HMMs as speciﬁed in Part 2 of Ex-
ample 8.2. Consider the two statistical models that arise from it.
a. Let (x1, π1), . . . , (xn, πn) be pairs of sequences, where for each
j = 1, . . . , n, xj is a sequence of elements from Q, πj is a path, and the
lengths of xj and πj are equal. Each pair can be regarded as an element of
the sample space ˆSa. Consider the corresponding mapping ϕ : ˆSa →N and
set wj = ϕ((xj, πj)), j = 1, . . . n. Let w = (w1, . . . , wn). This statistical model
can be ﬁtted to the vector w as it was done in Sect. 3.6 in the case when paths
are known. This way of model ﬁtting also agrees with the general theory of
parameter estimation discussed in the next section.
b. Let x1, . . . , xn be sequences with elements from Q. Each of the
sequences can be regarded as an element of the sample space ˆSb. Consider
the corresponding mapping ϕ : ˆSb →N and set wj = ϕ(xj), j = 1, . . . n. Let
w = (w1, . . . , wn). This statistical model can be ﬁtted to the vector w as it
was done in Sect. 3.6, by either the Baum-Welch or Viterbi training. Note that
the Baum-Welch training attempts to maximize the product of probabilities
P(x1)×. . .×P(xn) (we called it the “likelihood of the training data” in Sect.
3.6). For every ﬁxed θ ∈P, in terms of the probability distribution of the
sample of size n from the model, this product can be expressed as
P(x1) × . . . × P(xn) =
n

j=1
pfθ(wj) = pf ×∞,n
θ
(w).
Hence, the Baum-Welch training attempts to determine a value of θ for which
the probability pf ×∞,n
θ
(w) is maximal. Note, however, that the Baum-Welch
training may lead only to a local maximum of pf ×∞,n
θ
(w).

8.2 Parameter Estimation
235
3. Suppose that we are given a reduced alignment of N DNA sequences
of length n (see Sect. 5.1). Let c1, . . . , cn be the columns of the alignment.
As before, each of the columns can be regarded as an element of the sample
space S. Consider the corresponding mapping ϕ : S →{1, . . . , 4N} and set
wj = ϕ(cj), j = 1, . . . n. Let w = (w1, . . . , wn). In this case the statistical
model can be ﬁtted to w as it was done in Sect. 5.5, that is, by maximizing
the likelihood of the dataset given the tree L(D|T ) deﬁned there. Note that
we have
L(D|T ) =
n

j=1
pfθ(wj) = pf ×∞,n
θ
(w),
that is, by maximizing L(D|T ) we maximized pf ×∞,n
θ
(w).
In the next section we will see that maximizing pf ×∞,n
θ
(w) as in Parts 2.b
and 3 of Example 8.5, is a special case of a standard model ﬁtting proce-
dure called the maximum likelihood parameter estimation. We will see that
the model ﬁtting procedures discussed in Parts 1 and 2.a are examples of
maximum likelihood parameter estimation as well.
8.2 Parameter Estimation
Let {(Sθ, Bθ, Pθ, fθ, W, P)} be a statistical model. In this section we will dis-
cuss a universal approach to statistical modeling called parameter estimation.
There are two important types of parameter estimation: point estimation and
set estimation. In this book we will be only concerned with point estimation.
For simplicity we will assume ﬁrst that P ⊂Rq, that is, θ is a vector in Rq:
θ = (θ1, . . . , θq). We will use the term parameter when referring to a particular
component of θ.
Point estimation is based on producing a collection of vector-valued func-
tions {gk(x1, . . . , xk)}, with gk : Wk →Rq, k = 1, 2, . . ., such that, for every
θ ∈P and k = 1, 2, . . ., the vector-valued function gθ
k = gk(f ×∞,k
θ
) is a vector-
valued random variable on (Sθ, Bθ, Pθ)∞and its distribution is in some sense
concentrated around θ for large k (this last requirement will be made precise
in Deﬁnition 8.6). Such a collection {gk(x1, . . . , xk)} is called a point estima-
tor for θ. Of course, to construct a point estimator for θ, it is suﬃcient to
construct, for each j = 1, . . . , q, a point estimator for the parameter θj, that
is, a collection of functions {gj
k(x1, . . . , xk)}, with gj
k : Wk →R, k = 1, 2, . . .,
such that, for every θ ∈P and k = 1, 2, . . ., the function gj,θ
k
= gj
k(f ×∞,k
θ
)
is a random variable on (Sθ, Bθ, Pθ)∞and its distribution is in some sense
concentrated around θj for large k.
We will mention brieﬂy that for set estimation with q = 1 (in which
case the procedure is called interval estimation), two collections of functions
{gk(x1, . . . , xk)} and {gk(x1, . . . , xk)} with gk(x1, . . . , xk) < gk(x1, . . . , xk)

236
8 Elements of Statistics
for each k are produced, such that for all θ ∈P, we have with speciﬁed
probability
θ ∈

gθ
k, gθ
k

,
and the above interval is in some sense as short as possible, if k is large. The
collections {gk(x1, . . . , xk)} and {gk(x1, . . . , xk)} are called interval estima-
tors.
From now on we will only consider point estimators and will be interested
in estimators of the following type.
Deﬁnition 8.6. For
a
ﬁxed
j
=
1, . . . , q,
a
collection
of
functions
{gj
k(x1, . . . , xk)}, with gj
k : Wk →R, k = 1, 2, . . ., is called a consis-
tent estimator for θj
if, for all θ ∈P and k = 1, 2, . . ., the function
gj,θ
k
= gj
k(f ×∞,k
θ
) is a random variable on (Sθ, Bθ, Pθ)∞and gj,θ
k
P→θj for
all θ ∈P. If the components of vector-valued functions gk(x1, . . . , xk), with
gk : Wk →Rq, k = 1, 2, . . . form consistent estimators for θ1, . . . , θq, the
collection {gk(x1, . . . , xk)} is called a consistent estimator for θ.
Everywhere below we will be attempting to construct consistent estimators,
but sometimes certain “almost consistent” estimators will be also acceptable
(see, e.g., Exercises 8.4, 8.5).
If, for some n ∈N, we are given a vector w = (w1, . . . , wn) ∈Wn
as in Deﬁnition 8.4, then the numbers gj
n(w1, . . . , wn) are called estimates
for the parameters θj, j = 1, . . . q, respectively. In this case the vector
gn(w1, . . . , wn) is called an estimate for θ. Thus, one way to model the vector
w by {(Sθ, Bθ, Pθ, fθ, W, P)}, is to choose θ0 from Deﬁnition 8.4 to be equal
to gn(w1, . . . , wn). This is a very popular way to perform model ﬁtting. Of
course, it depends on choosing a “good” estimator in the ﬁrst place. Further
in this section we will describe one fairly general way of producing “good”
estimators.
What would one consider to be a “good” estimator? Above we have al-
ready described the consistency property. Now we will introduce another de-
sirable property of estimators.
Deﬁnition 8.7. Let {gj
k(x1, . . . , xk)} be an estimator for θj. Assume that
for every θ ∈P the corresponding random variables gj,θ
k
are integrable on
(Sθ, Bθ, Pθ)∞. Then, for every θ ∈P, the bias of the estimator is the sequence
of numbers {E(gj,θ
k ) −θj}. If, for all θ ∈P and all k = 1, 2, . . ., we have
E(gj,θ
k ) = θj, the estimator is called unbiased; otherwise it is called biased. Let
the components of vector-valued functions gk(x1, . . . , xk), with gk : Wk →Rq,
k = 1, 2, . . . be estimators for θ1, . . . , θq. Then the collection {gk(x1, . . . , xk)}
is called an unbiased estimator for θ, if the components of gk(x1, . . . , xk) are
unbiased estimators for θ1, . . . , θq respectively; otherwise it is called a biased
estimator for θ, and its bias for every ﬁxed θ ∈P is the sequence of vectors
{E(gθ
k) −θ}.

8.2 Parameter Estimation
237
It is generally desirable to have a consistent unbiased estimator. A further
good property of an unbiased estimator is that it has a low variance. It is not
always possible to ﬁnd unbiased estimators, and even if it is possible, such
estimators are not always preferred. For a biased estimator it is natural to
require that it has a low mean square error
MSE

gθ
k

= E

gθ
k −θ
2
= V ar

gθ
k

+

E

gθ
k

−θ
2
,
(8.2)
for all θ ∈P, as k →∞, where the above formulas are understood component-
wise. In some cases the MSE of a biased estimator is less than the MSE, that
is, the variance, of an unbiased estimator. In such cases the biased estimator
might be preferred to the unbiased one.
Finally, it is also desirable that an estimator has, at least approximately,
a well-known distribution, for example, a normal distribution. In this case an
existing theory for the known distribution can be applied when using such an
estimator.
We will now give examples of estimators.
Example 8.8. Let P = R × R+ × P′, where R+ = {x ∈R : x ≥0}. We will
write θ ∈P as θ = (θ1, θ2, θ′) = (µ, σ2, θ′) with θ′ ∈P′. Assume that for
every θ ∈P, fθ has a mean equal to µ and a variance equal to σ2, and also
assume that f 3
θ and f 4
θ are integrable on Sθ with respect to Pθ (in fact, one
can show that if f 4
θ is integrable on Sθ, then so are fθ, f 2
θ and f 3
θ ). We will
now construct consistent estimators for θ1 = µ and θ2 = σ2.
Set
g1
k(x1, . . . , xk) = 1
k
k

j=1
xk.
Then g1,θ
k
= g1
k(f ×∞,k
θ
) = 1/k k
j=1 ˜fj, where ˜f1, . . . , ˜fk are the ﬁrst k ele-
ments in the sequence f ×∞. Clearly, g1,θ
k
are random variables on (Sθ, Bθ, Pθ)∞
for all θ ∈P and k = 1, 2, . . .. Since ˜fj are iid and their distributions coin-
cide with that of fθ, from the Weak Law of Large Numbers (see Theorem
6.56 and Remark 6.58) we obtain for every θ ∈P that g1,θ
k
P→µ, that is,
{g1
k(x1, . . . , xk)} is a consistent estimator for µ. Next, E(g1,θ
k ) = µ, hence
{g1
k(x1, . . . , xk)} is an unbiased estimator for µ.
We will now ﬁnd MSE(g1,θ
k ) from formula (8.2). We have
MSE

g1,θ
k

= V ar

g1,θ
k

= 1
k2 E
⎛
⎜
⎝
⎛
⎝
k

j=1
˜fj
⎞
⎠
2⎞
⎟
⎠−µ2 = 1
k2
 k

j=1
E
 ˜f 2
j

+2

i<j
E
 ˜fi ˜fj


−µ2 = 1
k2

k(σ2 + µ2) + k(k −1)µ2
−µ2 = σ2
k ,

238
8 Elements of Statistics
where we used identity (6.35). Clearly, MSE(g1,θ
k ) →0 as k →∞, for every
θ ∈P.
Further, set
g2
k(x1, . . . , xk) = 1
k
k

j=1

xj −g1
k(x1, . . . , xk)
2
.
Then we have
g2,θ
k
= 1
k
k

j=1

˜fj −g1,θ
k
2
.
Clearly, g2,θ
k
are random variables on (Sθ, Bθ, Pθ)∞for all θ ∈P and k =
1, 2, . . .. We have
E

g2,θ
k

= 1
k
k

j=1
E
 ˜fj −g1,θ
k
2
= 1
k
k

j=1
E
 ˜fj −µ

−

g1,θ
k
−µ
2

= 1
k
k

j=1
E
 ˜fj −µ

−1
k
k

m=1
 ˜fm −µ
2

= 1
k
k

j=1

E
 ˜fj −µ
2
−2
k E
 ˜fj −µ
2
+ 1
k2
k

m=1
E
 ˜fm −µ
2
= σ2 −2
k σ2 + 1
k σ2 = k −1
k
σ2.
(8.3)
Further, by Chebyshev’s inequality (6.38) we have
P

e ∈S∞
θ
:
!!!!g2,θ
k (e) −k −1
k
σ2
!!!! ≥ε
 
≤
V ar

g2,θ
k

ε2
.
(8.4)
For V ar

g2,θ
k

we obtain using (8.3)
V ar

g2,θ
k

= E

g2,θ
k
2
−

E

g2,θ
k
2
= E

g2,θ
k
2
−(k −1)2
k2
σ4.
(8.5)
To ﬁnd the ﬁrst term in (8.5) we calculate

8.2 Parameter Estimation
239
E

g2,θ
k
2
= 1
k2
 k

j=1
E
 ˜fj −g1,θ
k
4
+2

j<m
E
 ˜fj −g1,θ
k
2 ˜fm −g1,θ
k
2
.
(8.6)
We have
E
 ˜fj −g1,θ
k
4
= E
 ˜fj −µ

−

g1,θ
k
−µ
4

= E

 ˜fj −µ
4 −4
 ˜fj −µ
3
g1,θ
k
−µ

+ 6
 ˜fj −µ
2
g1,θ
k
−µ
2
−4
 ˜fj −µ

g1,θ
k
−µ
3 +

g1,θ
k
−µ
4

.
Let E

fθ −µ
4
= ν. Then for the components in the right-hand side of the
above formula we have
E
 ˜fj −µ
4
= ν,
E
 ˜fj −µ
3
g1,θ
k
−µ

= 1
k E

 ˜fj −µ
3
k

m=1
 ˜fm −µ


= 1
k E
 ˜fj −µ
4
= ν
k ,
E
 ˜fj −µ
2
g1,θ
k
−µ
2
= 1
k2 E
⎛
⎝ ˜fj −µ
2

k

m=1
 ˜fm −µ

2⎞
⎠
= 1
k2

E
 ˜fj −µ
4
+

m̸=j
E
 ˜fj −µ
2 ˜fm −µ
2
= 1
k2

ν + (k −1)σ4
,
E
 ˜fj −µ

g1,θ
k
−µ
3
= 1
k3 E
⎛
⎝ ˜fj −µ


k

m=1
 ˜fm −µ

3⎞
⎠

240
8 Elements of Statistics
= 1
k3

E
 ˜fj −µ
4
+ 3

m̸=j
E
 ˜fj −µ
2 ˜fm −µ
2
= 1
k3

ν + 3(k −1)σ4
,
E

g1,θ
k
−µ
4
= 1
k4 E
⎛
⎝

k

m=1
 ˜fm −µ

4⎞
⎠
= 1
k4

k

m=1
E
 ˜fm −µ
4
+ 6

l<m
E
 ˜fm −µ
2 ˜fl −µ
2
= 1
k4

kν + 3k(k −1)σ4
.
Using the above calculations we also have for j ̸= m
E
 ˜fj −g1,θ
k
2 ˜fm −g1,θ
k
2
= E
 ˜fj −µ

−

g1,θ
k
−µ
2 ˜fm −µ

−

g1,θ
k
−µ
2

= E
 ˜fj −µ
2 ˜fm −µ
2
−2E
 ˜fj −µ
 ˜fm −µ
2
×

g1,θ
k
−µ

−2E
 ˜fj −µ
2 ˜fm −µ

g1,θ
k
−µ

+ 4E
 ˜fj −µ
 ˜fm −µ

×

g1,θ
k
−µ
2
+ E
 ˜fj −µ
2
g1,θ
k
−µ
2
+ E
 ˜fm −µ
2
g1,θ
k
−µ
2
−2E
 ˜fj −µ

g1,θ
k
−µ
3
−2E
 ˜fm −µ

g1,θ
k
−µ
3
+ E

g1,θ
k
−µ
4
= σ4 + 2
k2

ν + (k −1)σ4
−4
k3

ν + 3(k −1)σ4
+ 1
k4

kν + 3k(k −1)σ4
−2E
 ˜fj −µ
 ˜fm −µ
2
g1,θ
k
−µ

−2E
 ˜fj −µ
2 ˜fm −µ

g1,θ
k
−µ

+4E
 ˜fj −µ
 ˜fm −µ

g1,θ
k
−µ
2
.
To complete the above calculation for E
 ˜fj −g1,θ
k
2 ˜fm−g1,θ
k
2
we compute
for j ̸= m
E
 ˜fj −µ
 ˜fm −µ
2
g1,θ
k
−µ

= 1
k E

 ˜fj −µ
 ˜fm −µ
2
k

l=1
 ˜fl −µ



8.2 Parameter Estimation
241
= 1
k E
 ˜fj −µ
2 ˜fm −µ
2
= σ4
k ,
E
 ˜fj −µ
2 ˜fm −µ

g1,θ
k
−µ

= 1
k E

 ˜fj −µ
2 ˜fm −µ

k

l=1
 ˜fl −µ


= 1
k E
 ˜fj −µ
2 ˜fm −µ
2
= σ4
k ,
E
 ˜fj −µ
 ˜fm −µ

g1,θ
k
−µ
2
= 1
k2 E

 ˜fj −µ
 ˜fm −µ

 k

l=1
 ˜fl −µ

2
= 2
k2 E
 ˜fj −µ
2 ˜fm −µ
2
= 2σ4
k2 .
Hence we have
E
 ˜fj −g1,θ
k
2 ˜fm −g1,θ
k
2
= σ4 + 2
k2

ν + (k −1)σ4
−4
k3

ν + 3(k −1)σ4
+ 1
k4

kν + 3k(k −1)σ4
−4σ4
k
+ 8σ4
k2 .
Thus, we have calculated E
 ˜fj −g1,θ
k
4
and E
 ˜fj −g1,θ
k
2 ˜fm −g1,θ
k
2
.
Plugging the resulting expressions into formula (8.6) we obtain from (8.5)
V ar

g2,θ
k

= (k −1)2ν
k3
−(k2 −4k + 3)σ4
k3
→0,
as k →∞,
(8.7)
for every θ ∈P.
From (8.4) and (8.7) for every θ ∈P and suﬃciently large k we have
P

e ∈S∞
θ
:
!!!g2,θ
k (e) −σ2!!! ≥ε

≤P

e ∈S∞
θ
:
!!!!g2,θ
k (e) −k −1
k
σ2
!!!!
≥ε −σ2
k

≤P

e ∈S∞
θ
:
!!!!g2,θ
k (e) −k −1
k
σ2
!!!! ≥ε
2
 
→0,
as k →∞.
which tends to 0 as k →∞. Hence g2,θ
k
P→σ2 for every θ ∈P, which
means that {g2
k(x1, . . . , xk)} is a consistent estimator for σ2. Formula (8.3)
shows that the bias of {g2
k(x1, . . . , xk)} is {−σ2/k}. In particular, the bias of
{g2
k(x1, . . . , xk)} tends to 0, as k →∞, for every θ ∈P.
We will now ﬁnd MSE

g2,θ
k

from formula (8.2). Using formulas (8.3)
and (8.7) we obtain

242
8 Elements of Statistics
MSE

g2,θ
k

= V ar

g2,θ
k

+

E

g2,θ
k

−θ
2
= (k −1)2ν
k3
−(k2 −5k + 3)σ4
k3
.
Clearly, MSE

g2,θ
k

→0 as k →∞, for every θ ∈P.
One can also produce an unbiased estimator for θ2 = σ2 as follows
ˆg2
k(x1, . . . , xk) =
1
k −1
k

j=1

xj −g1
k(x1, . . . , xk)
2
.
It can be proved analogously that {ˆg2
k(x1, . . . , xk)} is a consistent estimator
for σ2, and a calculation similar to that in (8.3) shows that it is unbiased. For
this estimator we have
MSE

ˆg2,θ
k

= V ar

ˆg2,θ
k

= ν
k −(k −3)σ4
k(k −1) .
Thus, MSE

ˆg2,θ
k

→0 as k →∞, for every θ ∈P.
Interestingly, there is an estimation procedure that in many cases satisﬁes
most of, and in some cases all of the criteria for “good” estimators as stated
in the discussion following Deﬁnition 8.7. This is the procedure of maximum
likelihood estimation. First, we will deﬁne the likelihood given a statistical
model. It will be deﬁned in the situation when, for every θ ∈P, either fθ is
continuous, or Ffθ is the distribution function of a discrete random variable
(note that in the latter case fθ itself may not be discrete). We will call such
models regular statistical models.
For a regular model, denote by Pc the set of all θ ∈P for which fθ is
continuous and by Pd the complement to Pc in P (that is, the set of all θ ∈P
for which Ffθ is the distribution function of a discrete random variable).
Deﬁnition 8.9. Let {(Sθ, Bθ, Pθ, fθ, W, P)} be a regular statistical model
and θ ∈Pd. The likelihood of order k at the point θ given the model is a
function Lθ
k,d : Rk →R deﬁned in terms of the random sample of size k from
the model as follows
Lθ
k,d(x) = P

e ∈S∞
θ
: f ×∞,k
θ
(e) = x

,
(8.8)
with x ∈Rk.
Since for each θ ∈Pd the components of the random sample of size k are
iid and their distributions are identical to that of fθ, formula (8.8) for x =
(x1, . . . , xk) ∈Rk can be rewritten as
Lθ
k,d(x) = P

{e ∈S∞
θ
: fθ(e) = x1}

× . . .
×P

{e ∈S∞
θ
: fθ(e) = xk}

= (Ffθ(x1) −Ffθ(x1 −0)) × . . .
× (Ffθ(xk) −Ffθ(xk −0)) .
(8.9)

8.2 Parameter Estimation
243
We also remark here that if Pd = P and the random variables fθ are
themselves discrete and have a common range W, then, for r = (r1, . . . , rk) ∈
Wk formulas (8.8) and (8.9) can be rewritten in terms of the probability
distributions of f ×∞,k
θ
and fθ as follows
Lθ
k,d(r) = pf ×∞,k
θ
(r) = pfθ(r1) × . . . × pfθ(rk).
(8.10)
Next we will consider the continuous case.
Deﬁnition 8.10. Let {(Sθ, Bθ, Pθ, fθ, W, P)} be a regular statistical model
and θ ∈Pc. The likelihood of order k at the point θ given the model is a
function Lθ
k,c : Rk →R deﬁned in terms of the density function of the random
sample of size k from the model as follows
Lθ
k,c(x) = ϱf ×∞,k
θ
(x),
(8.11)
with x ∈Rk.
Using Deﬁnition 6.52, we can rewrite formula (8.11) as
Lθ
k,c(x) = ϱfθ(x1) × . . . × ϱfθ(xk),
(8.12)
where x = (x1, . . . , xk). Recall also that ϱfθ is not uniquely deﬁned as a
function on R and that for every θ ∈Pc there is in fact a family of density
functions of fθ such that any two functions from the family coincide a.e. on
R (see the discussion following Deﬁnition 6.45). Therefore, for the purposes
of Deﬁnition 8.10 we assume that a particular “natural” choice of density
function ϱfθ has been made for each θ ∈Pc. Clearly, the likelihoods Lθ
k,c
constructed from two diﬀerent density functions for ﬁxed k ∈N and θ ∈Pc,
coincide a.e. on Rk.
In the remainder of this section we will only consider regular models. For
such models maximum likelihood estimators are constructed as follows.
Deﬁnition 8.11. For any k ∈N and x = (x1, . . . , xk) ∈Wk set gk(x1, . . . , xk)
to be a value of θ selected as follows: if Lθ
k,d(x) ̸= 0 for some θ ∈Pd, then
choose θ ∈Pd for which Lθ
k,d(x) is maximal; if Lθ
k,d(x) = 0 for all θ ∈Pd,
then choose θ ∈Pc for which Lθ
k,c(x) is maximal. Any collection of functions
{gk(x1, . . . , xk)} so constructed is called a maximum likelihood estimator for
θ.
Deﬁnition 8.11 requires some clariﬁcation. First of all, the likelihoods Lθ
k,d(x)
and Lθ
k,c(x) must attain their maxima over Pd and Pc respectively for every
k ∈N and x ∈Wk. This condition is satisﬁed if, for example, Pd and Pc are
compact subsets of Rq and Lθ
k,d(x) and Lθ
k,c(x) are continuous functions of θ
for every k and x. Secondly, it is possible that for some k and x, a value of θ
for which the corresponding likelihood is maximal is not unique. In Deﬁnition

244
8 Elements of Statistics
8.11 the value of the function gk at the point x was taken to be any such
value of θ. Hence a priori there may exist more than one maximum likelihood
estimators. Thirdly, in order for {gk(x1, . . . , xk)} to be an estimator in the
usual sense, it is necessary for the vector-valued functions gθ
k = gk(f ×∞,k
θ
)
deﬁned on (Sθ, Bθ, Pθ)∞to be random variables for all k ∈N and θ ∈P. This
is known to be true under broad assumptions. Fourthly, maximum likelihood
estimators are known to be consistent under certain conditions. However, as
we will see below, such estimators may be biased. Fifthly, under certain con-
ditions, maximum likelihood estimators are known to be asymptotically nor-
mally distributed. More precisely, the sequence of Rq-valued random variables
{
√
k(gθ
k −θ)} converges in distribution to an Rq-valued continuous random
variable with density function of the form
ϱ(y) =
√
det A
(2π)q/2 exp
⎛
⎝−1
2
q

i,j=1
aijyiyj
⎞
⎠,
where A = (aij) is a q × q invertible positive-deﬁnite symmetric matrix, and
y = (y1, . . . , yq) ∈Rq. Finally, it must be stressed that the maximum likeli-
hood estimation procedure produces estimators for all of the parameters θj,
j = 1, . . . , q at the same time. For a precise formulation of conditions un-
der which maximum likelihood estimators are consistent and asymptotically
normally distributed, see Remark 8.17.
Maximum likelihood estimators are often used in the situation when either
Pd = ∅, or Pc = ∅, and the parameter space P is either a domain or the closure
of a domain in Rq, that is, we have either P = Ω, or P = Ω, where Ω⊂Rq
is an open connected set. Let Lθ
k(x) denote the corresponding likelihoods. If
in this case, for every k and x ∈Wk, Lθ
k(x) is diﬀerentiable in Ωas a function
of θ and its maximum over P occurs at a point θ0 ∈Ω(note that θ0 depends
on k and x), then θ0 is a critical point of Lθ
k(x) and we have
∂Lθ
k(x)
∂θj
(θ0) = 0,
j = 1, . . . , q.
(8.13)
Equations (8.13) are called the maximum likelihood equations and are an im-
portant tool for constructing maximum likelihood estimators. Of course, these
equations may have solutions other than θ0. For example, further analysis is
required to distinguish θ0 from those points in Ω, at which Lθ
k(x) has either
local extrema or saddle points. For instance, to distinguish local maxima from
local minima one can use the Hessian matrix of Lθ
k(x) (if Lθ
k(x) ∈C2(Ω)).
If, as above, P = Ω, or P = Ωand, for all k and x, Lθ
k(x) is diﬀerentiable in
Ωas a function of θ, it is customary to start looking for maximum likelihood
estimators by solving the maximum likelihood equations (8.13). One must
bear in mind, however, that, for some k and x, the function Lθ
k(x) may not
attain its maximum in P at all (for example, if Ωis unbounded, Lθ
k(x) may
grow as x approaches ∞) and, even if it does, it may attain its maximum at

8.2 Parameter Estimation
245
a boundary point of Ω(in the case P = Ω). Therefore, for some k and x,
none of the solutions to the maximum likelihood equations may be related to
maximum likelihood estimators. We also remark that the maximum likelihood
equations often make sense for regular models where neither of Pd and Pc is
empty and where P is obtained from a domain Ωby adding some (but not
all) boundary points to it.
Suppose {gk(x1, . . . , xk)} is a maximum likelihood estimator. If, for some
n ∈N, we are given a vector w ∈Wn as in Deﬁnition 8.4, then the vector
gn(w1, . . . , wn) ∈Rq is the corresponding maximum likelihood estimate for θ.
We will now give examples of maximum likelihood estimators and esti-
mates.
Example 8.12.
1. Suppose that q = 1, and denote the parameter θ = θ1 by λ. Let P =
Ω= {λ ∈R : λ ≥0}, with Ω= {λ ∈R : λ > 0}, and assume that fλ for
each λ is discrete, takes values 0, 1, 2, . . . (hence W = Z+ = {0, 1, 2, . . .}) and
has the Poisson distribution with mean λ

pfλ(j) = exp(−λ)λj
j!
,
j = 0, 1, 2, . . .
 
,
(see Sect. 6.9). Clearly, in this example we are dealing with a regular model,
where Pc = ∅. Fix k ∈N and choose x = (x1, . . . , xk) ∈Zk
+. Then from
formula (8.10) we have
Lλ
k(x) = exp(−λ)λx1
x1!
× . . . × exp(−λ)λxk
xk!
= exp(−kλ)λ
k
j=1 xj
k

j=1
xj!
.
We will now consider two cases. Suppose ﬁrst that x1 = ... = xk = 0.
Then Lλ
k(x) = exp(−kλ). In this case Lλ
k attains it maximum at the boundary
point λ = 0 of Ω. Suppose now that k
j=1 xj > 0. Clearly, Lλ
k(x) is inﬁnitely
many times diﬀerentiable in Ωfor all k and x, and we can attempt to ﬁnd
the solutions to the maximum likelihood equations (8.13), that become the
following single equation in this case
exp(−kλ)λ
k
j=1 xj−1
k

j=1
xj!
⎛
⎝−kλ +
k

j=1
xj
⎞
⎠= 0.
Clearly, the only solution to this equation for ﬁxed k and x is
λ = λ0 = 1
k
k

j=1
xj.
(8.14)

246
8 Elements of Statistics
Calculating the second derivative of Lλ
k(x) with respect to λ at the point λ0
we obtain the negative number
−k exp(−kλ0)λkλ0−1
0
k

j=1
xj!
.
Therefore, λ0 is a point of local maximum for Lλ
k(x). Further, since λ0 is the
only critical point of Lλ
k(x) in Ωand since Lλ
k(x) is continuous on Ω, λ0 is in
fact a point of global maximum of Lλ
k(x) on Ω.
We now observe that formula (8.14) gives a value that maximizes Lλ
k(x)
for all x ∈Zk
+, including the case x1 = ... = xk = 0. This formula leads
to precisely the collection of functions {g1
k(x1, . . . , xk)} that we considered in
Example 8.8 in the general case. Hence the maximum likelihood procedure in
this case gives a consistent unbiased estimator for λ.
2. Suppose again that q = 1, and denote the parameter θ = θ1 by p. Let
P = Ω= {p ∈R : 0 ≤p ≤1}, with Ω= {p ∈R : 0 < p < 1}, and assume
that fp for each p is discrete, takes values 0, 1 (hence W = {0, 1}) and its
distribution is that of the Bernoulli trial with mean p
{pfp(0) = 1 −p, pfp(1) = p}.
(see Sect. 6.9). As above, here we are dealing with a regular model with Pc = ∅.
Fix k ∈N and choose x = (x1, . . . , xk) ∈{0, 1}k. Then from formula (8.10)
we have
Lp
k(x) = px1(1 −p)1−x1 × . . . × pxk(1 −p)1−xk = p
k
j=1 xj(1 −p)
k−k
j=1 xj.
We will now consider three cases. Assume ﬁrst that x1 = . . . = xk = 0.
Then Lp
k(x) = (1 −p)k, and hence Lp
k(x) takes its maximum at the boundary
point p = 0 of Ω. Suppose now that x1 = . . . = xk = 1. In this case Lp
k(x) =
pk, and hence Lp
k(x) takes its maximum at the boundary point p = 1 of Ω.
Assume ﬁnally that 0 < k
j=1 xj < k. Clearly, Lp
k(x) is inﬁnitely many times
diﬀerentiable in Ωfor all k and x, and we can attempt to ﬁnd the solutions
to the maximum likelihood equations (8.13) that become the following single
equation in this case
p
k
j=1 xj−1(1 −p)
k−1−k
j=1 xj
⎛
⎝
k

j=1
xj −kp
⎞
⎠= 0.
Clearly, the only solution to this equation for ﬁxed k and x is
p = p0 = 1
k
k

j=1
xj.
(8.15)

8.2 Parameter Estimation
247
Calculating the second derivative of Lp
k(x) with respect to p at the point p0
we obtain the negative number
−kpkp0−1
0
(1 −p0)k−kp0−1.
Therefore, p0 is a point of local maximum for Lp
k(x). Further, since p0 is the
only critical point of Lp
k(x) in Ωand since Lp
k(x) is continuous on Ω, p0 is in
fact a point of global maximum of Lp
k(x) on Ω.
We now observe that formula (8.15) gives a value that maximizes Lp
k(x)
for all x ∈{0, 1}k, including the cases x1 = ... = xk = 0 and x1 = ... = xk = 1.
As in Part 1 above, this formula leads to precisely the collection of functions
{g1
k(x1, . . . , xk)} that we considered in Example 8.8 in the general case. Hence
the maximum likelihood procedure in this case gives a consistent unbiased es-
timator for p.
3. Suppose again that q = 1, and denote the parameter θ = θ1 by V . Let
P = Ω= {V ∈R : V > 0}, and assume that fV for each V is continuous,
has range (0, ∞) (that is, W = (0, ∞)) and is uniformly distributed with the
density function
ϱfV (z) =
⎧
⎪
⎨
⎪
⎩
0,
if z ≤0 or z > V ,
1
V ,
if 0 < z ≤V
(see Sect. 6.9). In this example we are dealing with a regular model with
Pd = ∅. Fix k ∈N and choose x = (x1, . . . , xk) ∈(0, ∞)k. Then from formula
(8.12) we have
LV
k (x) = ϱfV (x1) × . . . × ϱfV (xk).
Deﬁning xmax = max{x1, . . . , xk} we have
LV
k (x) =
⎧
⎪
⎨
⎪
⎩
0,
if V < xmax,
1
V k ,
if V ≥xmax.
Note that LV
k (x) is not diﬀerentiable with respect to V at xmax (but it is
inﬁnitely many times diﬀerentiable at any other point in Ω). It is easy to see
that LV
k (x) takes its maximal value at V = xmax.
Set gk(x1, . . . , xk) = xmax and show that {gk(x1, . . . , xk)} is a consistent
estimator for V . Consider gV
k = gk(f ×∞,k
V
). Since the maximum of a ﬁnite
number of independent random variables is a random variable, gV
k is a random
variable for all k ∈N and V ∈Ω. We will now ﬁnd its distribution.
From the independence of the components of f ×∞,k
V
= ( ˜f1, . . . , ˜fk), for
y ∈R we have

248
8 Elements of Statistics
FgV
k (z)=P

e ∈S∞
V : gV
k (e) ≤z

= P

e ∈S∞
V : ˜f1(e) ≤z, . . . ˜fk(e) ≤z

=
k

j=1
P

e ∈S∞
V : ˜fj(e) ≤z

= (FfV (z))k.
Diﬀerentiating the above identity with respect to z at z ̸= 0, V , we ﬁnd for
the density function of gV
k
ϱgV
k (z) = kϱfV (z)(FfV (z))k−1,
which gives
ϱgV
k (z) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
if z ≤0 or z > V ,
kzk−1
V k
,
if 0 < z ≤V .
It is therefore easy to ﬁnd the mean and the variance of gV
k from formulas
(6.26) and (6.27) respectively. Indeed, we have
E

gV
k

= k
V k
# V
0
zk dz =
kV
k + 1,
V ar

gV
k

= k
V k
# V
0
zk+1 dz −
k2V 2
(k + 1)2 =
kV 2
(k + 1)2(k + 2).
As in Example 8.8 we obtain from Chebyshev’s inequality (6.38) for ε > 0
P

e ∈S∞
V :
!!!!gV
k (e) −
kV
k + 1
!!!! ≥ε
 
≤
V ar

gV
k

ε2
→0,
as k →∞,
for every V . Therefore, for every V and suﬃciently large k we have
P

e ∈S∞
V :
!!gV
k (e) −V
!! ≥ε

≤P

e ∈S∞
V :
!!!!gV
k (e) −
kV
k + 1
!!!!
≥ε −
V
k + 1

≤P

e ∈S∞
V :
!!!!gV
k (e) −
kV
k + 1
!!!! ≥ε
2
 
,
which tends to 0 as k →∞. Thus, we have proved that gV
k
P→V for every V ,
which means that {gk(x1, . . . , xk)} is a consistent estimator for V . The bias
of this estimator is {−V/(k + 1)}; it tends to zero as k →∞, for every V .
Further, for MSE

gV
k

from formula (8.2) we have
MSE

gV
k

=
2V 2
(k + 1)(k + 2) →0,
as k →∞,
for every V .

8.2 Parameter Estimation
249
4. Suppose now that q = 2, and denote the parameters θ1 and θ2 by µ and
σ2 respectively, as we did in Example 8.8. Let P = Ω= {(µ, σ2) ∈R2 : σ2 ≥
0}, with Ω= {(µ, σ2) ∈R2 : σ2 > 0}. Suppose that W = R and assume that
f(µ,σ2) with σ2 > 0 is continuous and has the normal distribution with mean
µ and variance σ2
ϱf(µ,σ2)(z) =
1
√
2πσ2 exp

−(z −µ)2
2σ2

.
Assume further that f(µ,0) is a random variable with range R and the following
distribution function
Ff(µ,0)(z) =
⎧
⎨
⎩
0,
if z < µ,
1,
if z ≥µ.
Note that Ff(µ,0) is the distribution function of a discrete random variable,
but f(µ,0) itself is not discrete. Thus, in this example we are dealing with a
regular model where Pc = Ωand Pd = ∂Ω.
Fix k ∈N and choose x = (x1, . . . , xk) ∈Rk. Let ﬁrst σ2 > 0. Then from
formula (8.12) we have
L(µ,σ2)
k,c
(x) =
k

j=1
1
√
2πσ2 exp

−(xj −µ)2
2σ2

=
1
(
√
2πσ2)k exp
⎛
⎝−1
2σ2
k

j=1
(xj −µ)2
⎞
⎠.
(8.16)
Let now σ2 = 0. In this case formula (8.9) gives
L(µ,0)
k,d (x) =
⎧
⎨
⎩
1,
if x1 = . . . = xk = µ,
0,
if xi ̸= µ for some 1 ≤i ≤k.
Suppose ﬁrst that x1 = . . . = xk = a. In this case the only value of µ for
which L(µ,0)
k,d (x) ̸= 0 is µ = a, that is, we have
gk(x1, . . . , xk) = (a, 0).
(8.17)
Let now xi ̸= xj for some i, j = 1, . . . , k. Then L(µ,0)
k,d (x) = 0 for all µ,
and we will concentrate on formula (8.16). Since L(µ,σ2)
k,c
(x) is inﬁnitely many
times diﬀerentiable in Ω, we can ﬁnd its critical points there. Diﬀerentiating
the right-hand side of (8.16) with respect to µ and σ2 we obtain

250
8 Elements of Statistics
∂L(µ,σ2)
k,c
(x)
∂µ
=
1
(
√
2πσ2)kσ2 exp
⎛
⎝−1
2σ2
k

j=1
(xj −µ)2
⎞
⎠
k

j=1
(xj −µ),
∂L(µ,σ2)
k,c
(x)
∂σ2
=
1
2(
√
2πσ2)k(σ2)2 exp
⎛
⎝−1
2σ2
k

j=1
(xj −µ)2
⎞
⎠
×
⎛
⎝−kσ2 +
k

j=1
(xj −µ)2
⎞
⎠.
From the maximum likelihood equations
∂L(µ,σ2)
k,c
(x)
∂µ
= 0,
∂L(µ,σ2)
k,c
(x)
∂σ2
= 0
we obtain
µ = µ0 = 1
k
k

j=1
xj,
σ2 = σ2
0 = 1
k
k

j=1

xj −1
k
k

m=1
xm
2
.
(8.18)
We will now show that (µ0, σ2
0) is a point of maximum for L(µ,σ2)
k,c
(x).
Calculating its Hessian matrix at (µ0, σ2
0) we obtain
∂2L(µ,σ2)
k,c
(x)
∂µ2
(µ0, σ2
0) = −
k
(
$
2πσ2
0)kσ2
0
exp
⎛
⎝−1
2σ2
0
k

j=1
(xj −µ0)2
⎞
⎠
= −
k
(
$
2πσ2
0)kσ2
0
exp

−n
2

< 0,
∂2L(µ,σ2)
k,c
(x)
∂µ∂σ2
(µ0, σ2
0) = 0,

8.2 Parameter Estimation
251
∂2L(µ,σ2)
k,c
(x)
∂(σ2)2
(µ0, σ2
0) = −
k
2(
$
2πσ2
0)k(σ2
0)2 exp
⎛
⎝−1
2σ2
0
k

j=1
(xj −µ0)2
⎞
⎠
= −
k
2(
$
2πσ2
0)k(σ2
0)2 exp

−n
2

< 0.
Hence the Hessian matrix of L(µ,σ2)
k,c
(x) is negative-deﬁnite at (µ0, σ2
0) and
therefore (µ0, σ2
0) is a point of local maximum of L(µ,σ2)
k,c
(x). Since this is the
only critical point in Ω, it is in fact a point of global maximum of L(µ,σ2)
k,c
(x)
on Ω.
Now we note that (8.17) is a special case of (8.18) and hence formu-
las (8.18) give a maximum likelihood estimator {gk(x1, . . . , xk)}. Note that
gk(x1, . . . , xk) =

g1
k(x1, . . . , xk), g2
k(x1, . . . , xk)

, where {g1
k(x1, . . . , xk)} and
{g2
k(x1, . . . , xk)} are the estimators for µ and σ2 found in Example 8.8.
Hence {gk(x1, . . . , xk)} is a consistent estimator for θ = (µ, σ2) with bias
{(0, −σ2/k)} and MSE {(σ2/k, (k −1)2ν/k3 −(k2 −5k + 3)σ4/k3)}.
5. Consider the statistical model from Part 1 of Example 8.2 and ﬁt it
to a dataset represented by a vector w in the way described in Part 1 of
Example 8.5. It is possible to show that the resulting parameter values are
certain maximum likelihood estimates for the model (note that there are many
maximum likelihood estimators in this case – see, e.g., Exercise 8.4). Below
we will prove this statement for the simplest possible case when X (the set of
all non-zero states of the models) consists of a single state. For the case when
X consists of two states, see Exercise 8.4.
Consider the a priori Markov chain connectivity shown in Fig. 8.1, where
the arrows are labeled with the corresponding transition probabilities. For the
Fig. 8.1.

252
8 Elements of Statistics
corresponding statistical model we have
S = {0
m times
) *+ ,
G . . . G 0, m ∈N},
the range is N, θ = θ1 = α, and the parameter space is P = [0, 1) = {0} ∪Ω,
with Ω= (0, 1) (the point 1 is not included to ensure that the Markov chains
are non-trivially connected).
Fix x = (x1, . . . , xk) ∈Nk and let xj = ϕ−1(xj) and xj = 0
mj times
) *+ ,
G . . . G 0,
j = 1, . . . , k. It then follows from formulas (6.4) and (8.10) that
Lα
k(x) = α
k
j=1 mj−k(1 −α)k.
We will now consider two cases. Suppose ﬁrst that mj = 1 for j = 1, . . . , k.
Clearly, in this case Lα
k(x) = (1 −α)k, and thus Lα
k(x) attains its maximum
at the boundary point α = 0 of Ω. Suppose now that k
j=1 mj > k. The
function Lα
k(x) is inﬁnitely many times diﬀerentiable in Ωfor all k and x,
and we can attempt to ﬁnd the solution to the maximum likelihood equations
(8.13) that become the following single equation in this case
α
k
j=1 mj−k−1(1 −α)k−1
⎛
⎝α
k

j=1
mj −
k

j=1
mj + k
⎞
⎠= 0.
The solution to this equation for ﬁxed k and x is
α = α0 =
k

j=1
mj −k
k

j=1
mj
.
(8.19)
Calculating the second derivative of Lα
k(x) with respect to α at the point α0
we obtain the negative number
−
k
1 −α0
α((k+1)α0−1)/(1−α0)
0
(1 −α0)k−1.
Therefore, α0 is a point of local maximum for Lα
k(x). Further, since α0 is the
only critical point of Lα
k(x) in Ωand since Lα
k(x) is continuous on P, α0 is in
fact a point of global maximum of Lα
k(x) on P.
Observe that formula (8.19) gives a value that maximizes Lα
k(x) for all
x ∈Nk, including the case mj = 1 for j = 1, . . . , k. Thus the maximum likeli-
hood estimator for α is {gk(x1, . . . , xk)}, where gk(x1, . . . , xk) is given by the
right-hand side of formula (8.19). If we ﬁt the model to a dataset represented

8.2 Parameter Estimation
253
by a vector w ∈Nn by setting α = gn(w1, . . . , wn), we obtain exactly the
value given by formula (3.2).
6. Consider the two statistical models from Part 2 of Example 8.2.
a. Suppose that model ﬁtting is performed as in Part 2 of Example 8.5.
It is possible to prove that the resulting parameter values are certain maximum
likelihood estimates for the model (note that there are many maximum likeli-
hood estimators in this case – see, e.g., Exercise 8.5). Below we will prove this
statement for the simplest possible case when the underlying Markov chains of
the HMMs have only one non-zero state (that is, when X consists of a single
point) and when Q is the two-letter alphabet {A, B}. For the case when X
consists of two states, see Exercise 8.5.
The a priori connectivity of the underlying Markov chains of the HMMs
is shown in Fig. 8.1, where the arrows are labeled with the corresponding
transition probabilities. Let state G emit A with probability ν and B with
probability 1 −ν.
For the corresponding statistical model we have
ˆSa = {(x,
m times
) *+ ,
G . . . G) : x is a sequence of length m of letters A and B, m ∈N},
the range is N, θ = (θ1, θ2) = (α, ν), and the parameter space is P = [0, 1) ×
[0, 1] (the points in {1} × [0, 1] are not included to ensure that the underlying
Markov chains are non-trivially connected).
Fix x = (x1, . . . , xk) ∈Nk and let (xj, πj) = ϕ−1(xj); we denote by mj
the length of xj and by lj the number of times xj contains the letter A,
j = 1, . . . , k. It then follows from formulas (6.6) and (8.10) that
L(α,ν)
k
(x) = α
k
j=1 mj−k(1 −α)kν
k
j=1 lj(1 −ν)
k
j=1 mj−k
j=1 lj.
Clearly, to determine the points where L(α,ν)
k
(x) attains its maximum, it is
suﬃcient to determine α ∈[0, 1) at which the value of
ϕα
k(x) = α
k
j=1 mj−k(1 −α)k
is maximal and ν ∈[0, 1] at which the value of
ψν
k(x) = ν
k
j=1 lj(1 −ν)
k
j=1 mj−k
j=1 lj
is maximal.
The function ϕα
k(x) can be dealt with as in Part 5 above. It attains its
maximum at the point α0 given by formula (8.19). Hence we only need to ﬁnd
the point of maximum for the function ψν
k(x), which is done as in Part 2. We
will consider three cases.

254
8 Elements of Statistics
Suppose ﬁrst that lj = 0 for j = 1, . . . , k. Then ψν
k(x) = (1 −ν)
k
j=1 mj,
and the maximum is attained at the boundary point ν = 0. Next, if lj = mj
for j = 1, . . . , k, we have ψν
k(x) = ν
k
j=1 mj, and the maximum is attained at
the boundary point ν = 1. Assume ﬁnally that 0 < k
j=1 lj < k
j=1 mj. The
function ψν
k(x) is inﬁnitely many times diﬀerentiable in (0, 1) for all k and x,
and we can attempt to ﬁnd the points where it attains its maximum on (0, 1)
by determining its critical points in (0, 1). Diﬀerentiating with respect to ν
we obtain
dψν
k(x)
dν
= ν
k
j=1 lj−1(1 −ν)
k
j=1 mj−k
j=1 lj−1
⎛
⎝
k

j=1
lj −ν
k

j=1
mj
⎞
⎠.
Clearly, the only critical point in (0, 1) is
ν = ν0 =
k

j=1
lj
k

j=1
mj
.
(8.20)
Calculating the second derivative of ψν
k(x) with respect to ν at the point ν0
we obtain the negative number
−
k

j=1
mjν
ν0 k
j=1 mj−1
0
(1 −ν0)
k
j=1 mj−ν0 k
j=1 mj−1.
Therefore, ν0 is a point of local maximum for ψν
k(x). Since ν0 is the only
critical point of ψν
k(x) in (0, 1) and since ψν
k(x) is continuous on [0, 1], ν0 is
in fact a point of global maximum of ψν
k(x) on [0, 1]. We also observe that
formula (8.20) gives a value that maximizes ψν
k(x) for all x ∈Nk, including
the cases k
j=1 lj = 0 and k
j=1 lj = k
j=1 mj.
Thus
the
maximum
likelihood
estimator
for
(α, ν)
is
{(g1
k(x1, . . . , xk), g2
k(x1, . . . , xk))}, where g1
k(x1, . . . , xk) is given by the right-
hand side of formula (8.19) and g2
k(x1, . . . , xk) by the right-hand side of for-
mula (8.20). If we ﬁt the model to a dataset represented by a vector w ∈Nn
by setting α = g1
n(w1, . . . , wn), ν = g2
n(w1, . . . , wn), we obtain exactly the
value given by formula (3.17).
b. One way to ﬁt this model to a dataset represented by a vector w is by
means of the Baum-Welch training. As we pointed out in Part 2 of Example
8.5, the Baum-Welch training attempts to maximize Lθ
n,d(w) = pf ×∞,n
θ
(w)
over θ ∈P, but may not necessarily achieve this goal and produce only a

8.2 Parameter Estimation
255
point of local maximum for Lθ
n,d(w). Nevertheless, the idea behind the Baum-
Welch training is based on maximum likelihood estimators.
Another way to do ﬁtting is by means of the Viterbi training (see
Sect. 3.6). Unlike the Baum-Welch training, it only attempts to maximize
P a(x1, π∗(x1)) × . . . × P a(xn, π∗(xn)), where π∗(xj) is a Viterbi path for xj,
j = 1, . . . , n. Probably for this reason, the Viterbi training does not perform
as well as the Baum-Welch training.
At the beginning of this section we assumed that the parameter space P
is a subset of a Euclidean space. In fact, parameter estimation (maximum
likelihood estimation in particular) is used in more general situations as well.
For example, we may want to estimate the parameters of the model described
in Part 3 of Example 8.2. Of course, there are additional complications in such
cases. Suppose that P is a subset of a topological space T. Then for every
k ∈N, the function gk(x1, . . . , xk) is deﬁned on Wk and takes values in T.
The very ﬁrst condition that the collection of functions {gk(x1, . . . , xk)} must
satisfy in order to be a candidate for estimation is that, for every k ∈N, the
mapping gθ
k = gk(f ×∞,k
θ
) is a random variable on the inﬁnite power of the
probability space (Sθ, Bθ, Pθ). Thus, we need a suitable deﬁnition of random
variable with values in a topological space. It must be of the following form: a
mapping f deﬁned on a probability space (S, B, P) with values in a topological
space T is called a random variable if, for certain subsets A of T (analogues of
Borel sets in Rq), we have f −1(A) ∈B. For example, if B is the σ-algebra of
all events in S, then any mapping f : S →T satisﬁes any reasonable deﬁnition
of this kind and hence can be called a random variable. For instance, for the
model in Part 3 of Example 8.2, Sθ is ﬁnite and Bθ is the collection of all
events in Sθ for every θ ∈P. It is then clear from the deﬁnition of inﬁnite
power of a probability space at the end of Sect. 6.2 that every event in S∞
θ
is
measurable.
Next, in order to deﬁne a consistent estimator, we need to be able to
generalize Deﬁnition 6.54 to the case when the corresponding random variables
take values in a topological space. Deﬁnition 6.54 requires a distance function
deﬁned on T, so we assume that T is a metric space (see Deﬁnition 5.2).
For metric spaces Deﬁnition 6.54 can be generalized in the obvious way: the
absolute value in formula (6.37) must be replaced by the distance between
fk(e), f(e) ∈T. For example, for the case of the model from Part 3 of Example
8.2, the parameter space is a metric space obtained by gluing together pieces
of the Euclidean space of dimension 2N −3 (each piece is responsible for
a particular topology and accommodates the corresponding branch length
parameters) multiplied by an appropriate portion of another Euclidean space
to accommodate the parameters arising from the evolutionary model. For
this statistical model, ﬁtting is done as described in Part 3 of Example 8.5,
that is, by the maximum likelihood procedure. It can be proved (the proof is
nontrivial) that in this case any maximum likelihood estimator is consistent,

256
8 Elements of Statistics
if a reversible evolutionary model is used [R]. In particular, it is consistent for
each of the four evolutionary models described in Sect. 5.4.
If T is a general metric space, it is not easy to deﬁne the bias and the mean
square error of an estimator, since such deﬁnitions would require to introduce
the integral of a random variable with values in T. Generalities of this kind
are beyond the scope of the book.
8.3 Hypothesis Testing
In the preceding section we described a way to ﬁt a model to data. In our
considerations we assumed that a model was ﬁxed from the start. It is an
interesting and non-trivial question, however, how one can construct a model
that is “suitable” for a particular dataset or how one can choose such a model
from a range of available ones. This question is very hard to answer in general,
and in this section we will deal with a somewhat easier one: suppose that we
are given a dataset D represented by a vector w of length n and a pair of
statistical models M1 and M2 with ranges W1 and W2 respectively, such that
w ∈Wn
j , j = 1, 2; which of the two models is then “most suitable” for
modeling the dataset D? Recall that we have already addressed this question
to some extent in Sect. 5.6, where we discussed hypothesis testing for the case
of phylogenetic reconstruction. Here we will give a general framework for such
testing.
We will assume that M1 is a special case of M2. In this case we have
W1 = W2 = W, and we will write M2 as M2 = {(Sθ, Bθ, Pθ, fθ, W, P)} with
M1 corresponding to a subset P′ ⊂P. Clearly, M2 models D at least “as well
as M1 does”, and we wish to know whether or not M2 models D “signiﬁcantly
better” than M1. This setup is usually formalized as follows. We consider the
null hypothesis
H0 : the dataset D obeys model M1,
and the alternative hypothesis
HA : the dataset D obeys model M2,
where “D obeys Mj” means that we have chosen to model w by means of
Mj. The null hypothesis is a special case of the alternative one, that is, the
hypotheses are nested. We will now deﬁne what it means to statistically test
the null hypothesis against the alternative one.
Deﬁnition 8.13. A statistical test of the null hypothesis H0 against the alter-
native hypothesis HA or a statistical test of the signiﬁcance of the alternative
hypothesis HA is a choice of a Borel set Wn ⊂Wn ⊂Rn such that, if w ∈Wn,
H0 is rejected in favor of HA, and if w ∈Wn \ Wn, H0 is accepted.
If H0 is rejected in favor of HA, we say that M2 models D signiﬁcantly better
than M1. If H0 is accepted, we say that M2 does not model D signiﬁcantly

8.3 Hypothesis Testing
257
better than M1 or that M1 models D as well as M2 does. Note that a statistical
test of this type does not provide information on “how well” D is modeled by
either M1 or M2, it only concerns their relative modeling performance.
One usually associates two types of error with hypothesis testing: a type
I error is committed if H0 is rejected when it is true and a type II error is
committed if H0 is accepted when it is false and HA is true. More precisely,
the following probabilities
EI(θ) = P


e ∈S∞
θ
: f ×∞,n
θ
(e) ∈Wn


= PFf×∞,n
θ
(Wn),
for θ ∈P′, and
EII(θ) = P


e ∈S∞
θ
: f ×∞,n
θ
(e) ∈Wn \ Wn


= 1 −PFf×∞,n
θ
(Wn),
for θ ∈P \ P′, are called the probabilities of committing a type I error and
a type II error respectively (in the above formulas we used (6.34)). Further,
the function
Qn(θ) = 1 −EII(θ) = PFf×∞,n
θ
(Wn)
deﬁned on P \ P′ is called the power of the test. One can think of Qn(θ) as
the probability of rejecting H0 when it is false and HA is true.
Fix 0 < α < 1 and assume that Wn is chosen to satisfy
EI(θ) ≤α,
(8.21)
for all θ ∈P′. The number α is called the signiﬁcance level of the test. One
always wants to minimize the probability of a type I error and hence α is
chosen to be small (usually 0.01 or 0.05). At the same time, one wants to
maximize the power of the test Qn(θ) for all θ ∈P \ P′. In particular, a
statistical test is called consistent, if Qn(θ) →1, as n →∞, for every θ ∈
P \ P′.
We will now consider a particular testing procedure called the likeli-
hood ratio test. Suppose that M1 and M2 are regular statistical models
for which respective maximum likelihood estimators {g1,k(x1, . . . , xk)} and
{g2,k(x1, . . . , xk)} exist, and for k ∈N and x = (x1, . . . , xk) ∈Wk denote by
Lmax
0,k (x) and Lmax
A,k(x) the values of the likelihoods of order k given models M1,
M2, calculated at x for parameter values g1,k(x1, . . . , xk) and g2,k(x1, . . . , xk)
respectively. For every y ∈Wn we now consider Lmax
0,n (y) and Lmax
A,n(y) and
deﬁne the likelihood ratio of order n
∆n(y) = Lmax
0,n (y)
Lmax
A,n(y).
In the above formula we assume that Lmax
A,n(y) ̸= 0 for every y ∈Wn. How-
ever, everything that follows applies (after making minor adjustments) in the
situation when

258
8 Elements of Statistics
L
max
A,n(w) ̸= 0,
and
PFf×∞,n
θ
(E∞
n ) = 0
for all θ ∈P′,
(8.22)
where E∞
n
=

y ∈Wn : Lmax
A,n(y) = 0

(in particular, E∞
n
is assumed to be
measurable).
Clearly, ∆n(y) ≥0 for all y (note that if Pd = ∅or Pc = ∅, then, in
addition, ∆n(y) ≤1 for all y). For K ≥0 deﬁne
Wn(K) = {y ∈Wn : ∆n(y) ≤K} .
(8.23)
Suppose that Wn(K) is a Borel set in Rn for all K ≥0. Then ∆n(f ×∞,n
θ
) is a
random variable on (Sθ, Bθ, Pθ)∞for every θ ∈P′. Then by property (iv) of
distribution functions in Sect. 6.7 and using formula (6.34) we have for every
θ ∈P′
PFf×∞,n
θ
(Wn(K)) = F∆n(f ×∞,n
θ
)(K) →F∆n(f ×∞,n
θ
)(0) = PFf×∞,n
θ
(E0),
as K →0, where
E0
n = {y ∈Wn : ∆n(y) = 0} .
(8.24)
Suppose now that, for all θ ∈P′, F∆n(f ×∞,n
θ
) is continuous on [0, ∞) and
PFf×∞,n
θ
(E0
n) = 0. Then we can choose the largest number Kα(θ) such that
F∆n(f ×∞,n
θ
)(Kα(θ)) = PFf×∞,n
θ
(Wn(Kα(θ))) = α.
(8.25)
Assume further that infθ Kα(θ) = Kα > 0 and set
Wn = Wn(Kα).
(8.26)
This choice guarantees that condition (8.21) holds. Thus, under the likelihood
ratio test, the null hypothesis is rejected, if ∆n(w) ≤Kα. The number Kα
is called the signiﬁcance point of the test. To determine the signiﬁcance point
Kα, one needs to know the distribution of ∆n(f ×∞,n
θ
) for every θ ∈P′. This
family of distributions is called the null hypothesis distribution of the likelihood
ratio ∆n.
In the above considerations we assumed, in particular, that F∆n(f ×∞,n
θ
) is
continuous on [0, ∞) for all θ ∈P′. In general, F∆n(f ×∞,n
θ
) is only upper semi-
continuous on R, and therefore a number satisfying identity (8.25) may not
exist. If this is the case for some θ0 ∈P′, then we construct Kα(θ0) as follows.
First, we ﬁnd the (unique) number K′
α(θ0) such that F∆n(f ×∞,n
θ0
)(K′
α(θ0)) > α
and F∆n(f ×∞,n
θ0
)(K′
α(θ0) −0) < α. We now set Kα(θ0) to be a number less
than K′
α(θ0) and close to it. Of course, there is a great deal of arbitrariness
in this choice of Kα(θ0). Note that instead of satisfying identity (8.25) with
θ = θ0, it satisﬁes the inequality

8.3 Hypothesis Testing
259
F∆n(f ×∞,n
θ0
)(Kα(θ0)) = PFf×∞,n
θ0
(Wn(Kα(θ0))) < α.
This way we can choose Kα(θ) for all θ ∈P′. If infθ Kα(θ) = Kα > 0, we
deﬁne the set Wn as in (8.26). As before, this choice of Wn guarantees that
condition (8.21) holds.
Instead of using the likelihood ratio ∆n for hypothesis testing, we can
use some other function λn : Wn →R that satisﬁes all the conditions that
we needed above to produce a signiﬁcance point. Such a function λn used
for hypothesis testing is called a test statistic. A variety of test statistics are
utilized, but the likelihood ratio is perhaps the most frequently used one.
One of its advantages is that it is known in some cases to produce a test for
which the power Qn(θ) is maximal possible (this holds, for example, when P
is a two-point set and P′ is a one-point subset of P). Secondly, under certain
conditions, the likelihood ratio test is consistent (for a precise statement see
Remark 8.17). Thirdly, in many cases one can determine the asymptotic null
hypothesis distribution of ∆n, that is, the distribution of ∆n(f ×∞,n
θ
), for
θ ∈P′, as n →∞. Speciﬁcally, one can show that under certain assumptions
−2 ln(∆n(f ×∞,n
θ
)) converges in distribution to a random variable that has
the chi-square distribution with ν degrees of freedom (see Sect. 6.9), where
ν = dim P −dim P′. Because of the importance of this last fact, we will give
a precise formulation below. We will only consider the case of regular models
with either Pc = ∅and fθ discrete for all θ ∈P, or Pd = ∅. More general
cases can be also dealt with, but we do not go in such details here.
First, we need the following deﬁnitions.
Deﬁnition 8.14. Let {(Sθ, Bθ, Pθ, fθ, W, P)} be a regular model with Pc = ∅.
Assume that P = Pd = Ω, where Ω⊂Rq is a domain, and we will write θ =
(θ1, . . . , θq). Let fθ be discrete for all θ ∈Ω, and consider the corresponding
probability distributions

pfθ(r) = P

{e ∈Sθ : fθ(e) = r}

, r ∈W

. Then
the model is called smooth, if the following conditions hold
(i) pfθ(r) is twice diﬀerentiable with respect to θ in Ωfor every r ∈W,
(ii) the series 
r∈W ∂pfθ(r)/∂θi and 
r∈W ∂2pfθ(r)/∂θi∂θj absolutely con-
verge to 0, for every i, j = 1, . . . , q and θ ∈Ω,
(iii) a maximum likelihood estimator {gk(x1, . . . , xk)} exists for θ,
(iv) there is a unique solution to the maximum likelihood equations (8.13) for
every k ∈N and x = (x1, . . . , xk) ∈Wk (this solution is necessarily equal to
gk(x1, . . . , xk)).
Deﬁnition 8.15. Let {(Sθ, Bθ, Pθ, fθ, W, P)} be a regular model with Pd = ∅.
Assume that P = Pc = Ω, where Ω⊂Rq is a domain, and we will write

260
8 Elements of Statistics
θ = (θ1, . . . , θq). Then the model is called smooth, if the following conditions
hold
(i) ϱfθ(z) is twice diﬀerentiable with respect to θ in Ωfor every z ∈R,
(ii) ∂ϱfθ(z)/∂θi and ∂2ϱfθ(z)/∂θi∂θj are Lebesgue integrable on R, for every
i, j = 1, . . . , q and θ ∈Ω,
(iii)
" ∞
−∞∂ϱfθ(z)/∂θi dµ = 0, for every i = 1, . . . , q and θ ∈Ω,
(iv)
" ∞
−∞∂2ϱfθ(z)/∂θi∂θj dµ = 0, for every i, j = 1, . . . , q and θ ∈Ω,
(v) a maximum likelihood estimator {gk(x1, . . . , xk)} exists for θ,
(vi) there is a unique solution to the maximum likelihood equations (8.13) for
every k ∈N and x = (x1, . . . , xk) ∈Wk (this solution is necessarily equal to
gk(x1, . . . , xk)).
We are now ready to formulate the following theorem.
Theorem 8.16. Let M2 be a smooth statistical model and let model M1 be
given by the conditions θ1 = θ0
1, . . . , θr = θ0
r for some θ0
1, . . . , θ0
r ∈R with
0 < r ≤q. Suppose that M1 is smooth as well. Let Lmax
A,n(y) ̸= 0 for all y ∈Wn
and n ∈N. Assume further that for all K ≥0 and n ∈N the set Wn(K)
deﬁned in (8.23) is Borel and that for the set E0
n deﬁned in (8.24) we have
PFf×∞,n
θ
(E0
n) = 0 for every θ ∈P′ and n ∈N.
Then the sequence of random variables

−2 ln

∆n

f ×∞,n
θ

converges
in distribution to a random variable that has the chi-square distribution with
r degrees of freedom, for every θ ∈P′.
Strictly speaking, −2 ln

∆n

f ×∞,n
θ

is not deﬁned at the points of S∞
θ where
the value of f ×∞,n
θ
belongs to E0
n. However, the probability of this event is
assumed to be zero, and this complication does not aﬀect the assertion of
the theorem. We can set the values of −2 ln

∆n

f ×∞,n
θ

at the points of
E0
n arbitrarily, say equal to 0. Also note that, for a similar reason, instead of
requiring that Lmax
A,n(y) ̸= 0 for all y ∈Wn and n ∈N, it is suﬃcient to require
that condition (8.22) holds for all n ∈N.
Remark 8.17. It can be shown that for a smooth statistical model the cor-
responding maximum likelihood estimator is consistent and asymptotically
normally distributed. Further, under the assumptions of Theorem 8.16, the
likelihood ratio test is consistent. Thus, the assumption of smoothness is suf-
ﬁcient for maximum likelihood estimators and the corresponding likelihood
ratios to have all the good properties. They also can be shown to hold for
more general models.

8.3 Hypothesis Testing
261
We will now give examples of applying the likelihood ratio test.
Example 8.18.
1. Let M2 be the statistical model from Part 3 of Example 8.12, and let
M1 be given by setting V = V ∗for some V ∗> 0, that is, by setting P′ to be
a single point P′ = {V ∗} ⊂P. Fix n ∈N and for every y ∈Wn we will now
calculate Lmax
A,n(y) and Lmax
0,n (y).
The value of Lmax
A,n(y) can be found from our calculations in Part 3 of
Example 8.12. We have
L
max
A,n(y) =
1
yn
max
,
where ymax = max{y1, . . . , yn}. For Lmax
0,n (y) we clearly have
L
max
0,n (y) =
⎧
⎪
⎨
⎪
⎩
0,
if ymax > V ∗,
1
V ∗n ,
if ymax ≤V ∗.
Therefore we obtain
∆n(y) =
⎧
⎪
⎨
⎪
⎩
0,
if ymax > V ∗,
ymax
V ∗
n
,
if ymax ≤V ∗.
Clearly, for every K ≥0 the set Wn(K) deﬁned in (8.23) is a Borel set in Rn.
Hence ∆n

f ×∞,n
V ∗

is a random variable on (SV ∗, BV ∗, PV ∗)∞for every n.
Further, we have E0
n = {y ∈Wn : ∆n(y) = 0} = {y ∈Rn : y1 > 0, . . . , yn >
0, and ymax > V ∗} for every n (see (8.24)). It is not hard to observe that
PFf×∞,n
V ∗
(E0
n) = 0.
Next, for n ∈N we need to ﬁnd the null hypothesis distribution of the
likelihood ratio, that is, the distribution of the random variable ∆n

f ×∞,n
V ∗

.
For z ≥0 we have
F∆n(f ×∞,n
V ∗
)(z) = P


e ∈S∞
V ∗: ∆n

f ×∞,n
V ∗
(e)

≤z


= P


e ∈S∞
V ∗: gV ∗
n (e) > V ∗

+ P

e ∈S∞
V ∗: gV ∗
n (e)
≤min{V ∗, z1/nV ∗}

= F ∗
gV ∗
n (V ∗) + FgV ∗
n

min{V ∗, z1/nV ∗}

= 1 −FgV ∗
n (V ∗) + FgV ∗
n

min{V ∗, z1/nV ∗}

.
(8.27)
The random variables gV
k were determined and studied in Part 3 of Example
8.12. In particular, for the distribution function of gV ∗
n
we have

262
8 Elements of Statistics
FgV ∗
n (z) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
0,
if z ≤0,
zn
V ∗n ,
if 0 < z ≤V ∗,
1,
if z > V ∗.
Therefore formula (8.27) gives for z ≥0
F∆n(f ×∞,n
V ∗
)(z) =
⎧
⎨
⎩
z,
if 0 ≤z ≤1,
1,
if z > 1.
(8.28)
Let 0 < α < 1 be the signiﬁcance level of the test. Since F∆n(f ×∞,n
V ∗
) is
continuous on [0, ∞) and strictly increasing on [0, 1], the signiﬁcance point
Kα(V ∗) = Kα is found from the identity
F∆n(f ×∞,n
V ∗
)(Kα) = α,
(see (8.25)), which gives Kα = α. If a vector w ∈Wn represents a real dataset
to which we wish to apply the likelihood ratio test, then the null hypothesis
is rejected, if ∆n(w) ≤α.
The distribution of −2 ln

∆n

f ×∞,n
V ∗

is easily obtained from formula
(8.28). Namely, we have
F−2 ln(∆n(f ×∞,n
V ∗
))(z) =
⎧
⎪
⎨
⎪
⎩
1 −exp

−z
2

,
if z ≥0,
0,
if z < 0.
Diﬀerentiating F−2 ln(∆n(f ×∞,n
V ∗
))(z) for z ̸= 0 we obtain the density function
of −2 ln

∆n

f ×∞,n
V ∗

ϱ−2 ln(∆n(f ×∞,n
V ∗
))(z) =
⎧
⎪
⎨
⎪
⎩
1
2 exp

−z
2

,
if z > 0,
0,
if z ≤0,
where we set ϱ−2 ln(∆n(f ×∞,n
V ∗
))(0) = 0. Thus, −2 ln

∆n

f ×∞,n
θ

has the
exponential distribution with λ = 1/2 (see Sect. 6.9), that is, the chi-square
distribution with 2 degrees of freedom. Note that, according to Theorem 8.16,
we could expect to obtain asymptotically the chi-square distribution with
1 degree of freedom, so the assertion of the theorem does not hold for this
example. The reason for this is that model M2 is not smooth. Indeed, consider
ϱfV from Part 3 of Example 8.12. It is diﬀerentiable at every V0 > 0 for z ̸= V0,
and we have

8.3 Hypothesis Testing
263
∂ϱfV (z)
∂V
(V0) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
if z ≤0 or z > V0,
−1
V 2
0
,
if 0 < z < V0.
The above function is not Lebesgue integrable on R, that is, condition (ii) of
Deﬁnition 8.15 does not hold.
2. Let M2 be the statistical model from Part 4 of Example 8.12 with
σ2 = σ∗2 > 0. More precisely, M2 is described as follows. Set q = 1 and
denote the parameter θ = θ1 by µ. Let P = W = R, and assume that fµ is
continuous and has the normal distribution with mean µ and variance σ∗2
ϱfµ(z) =
1
√
2πσ∗2 exp

−(z −µ)2
2σ∗2

.
Suppose now that M1 is given by setting µ = µ∗for some µ∗∈R, that is, by
setting P′ = {µ∗} ⊂P.
Arguing in the same way as in Part 4 of Example 8.12 we obtain a maxi-
mum likelihood estimator {g2,k(x1, . . . , xk)} for M2, where
g2,k(x1, . . . , xk) = 1
k
k

j=1
xj
(see (8.18)). Therefore for n ∈N and y ∈Rn we obtain
L
max
A,n(y) =
1
√
2πσ∗2
n exp
⎛
⎝−
1
2σ∗2
n

j=1

yj −1
n
n

m=1
ym
2⎞
⎠.
Also, it is clear that
L
max
0,n (y) =
1
√
2πσ∗2
n exp
⎛
⎝−
1
2σ∗2
n

j=1
(yj −µ∗)2
⎞
⎠.
It then follows that
∆n(y) = exp
⎛
⎜
⎝−
1
2nσ∗2
⎛
⎝
n

j=1
yj −nµ∗
⎞
⎠
2⎞
⎟
⎠.
Clearly, for every K ≥0 and n ∈N the set Wn(K) deﬁned in (8.23) is a
Borel set in Rn. Hence ∆n

f ×∞,n
µ∗

is a random variable on (Sµ∗, Bµ∗, Pµ∗)∞
for every n. Further, we have E0
n = ∅.

264
8 Elements of Statistics
We will now attempt to determine the asymptotic null hypothesis distribu-
tion of the likelihood ratio, that is, the distribution of ∆n

f ×∞,n
µ∗

, as n →∞.
Let f ×∞,n
µ∗
= ( ˜f1, . . . , ˜fn). Then we have
−2 ln

∆n

f ×∞,n
µ∗

=
⎛
⎜
⎜
⎜
⎜
⎝
n

j=1
˜fj −nµ∗
√
nσ2∗
⎞
⎟
⎟
⎟
⎟
⎠
2
.
By the Central Limit Theorem (Theorem 6.64) we have
n

j=1
˜fj −nµ∗
√
nσ∗2
D
→f,
where f is a random variable whose distribution is the standard normal dis-
tribution. Clearly,
⎛
⎜
⎜
⎜
⎜
⎝
n

j=1
˜fj −nµ∗
√
nσ∗2
⎞
⎟
⎟
⎟
⎟
⎠
2
D
→f 2.
By Proposition 6.49, f 2 has the chi-square distribution with 1 degree of free-
dom. Hence, −2 ln

∆n

f ×∞,n
µ∗

converges in distribution to a random vari-
able that has the chi-square distribution with 1 degree of freedom, which
agrees with Theorem 8.16. Hence for every 0 < z ≤1 there exists Nz ∈N
such that for n ≥Nz we have
F∆n

f ×∞,n
µ∗
(z) ≈Φ(z) =
1
√
2π
# ∞
−2 ln z
1
√
t exp

−t
2

dt.
(8.29)
Formula (8.29) can be used to ﬁnd an approximate signiﬁcance point of
the test. Let 0 < α < 1 be the signiﬁcance level. Since Φ(z) is continuous and
strictly increasing on [0, 1], the approximate signiﬁcance point Kapprox
α
(V ∗) =
Kapprox
α
is found from the identity
Φ(K
approx
α
) = α,
(see (8.25)). Let a vector w ∈Wn represent a real dataset to which we wish
to apply the likelihood ratio test; then, if we use the approximate signiﬁcance
point Kapprox
α
, the null hypothesis is rejected, if ∆n(w) ≤Kapprox
α
.
Example 8.18 illustrates several important points that one should keep in
mind when using the likelihood ratio test. First of all, it may not be possi-
ble to determine the null hypothesis distribution of the likelihood ratio for a

8.3 Hypothesis Testing
265
given n, in which case one can attempt to use an asymptotic distribution. The
chi-square distribution with an appropriate number of degrees of freedom (as
stated in Theorem 8.16) is often used, but one has to be aware that if the
conditions of Theorem 8.16 are not satisﬁed, an asymptotic distribution may
not exist at all, or, if it does, may not be of the chi-square type, or, if it is,
may have a wrong number of degrees of freedom, as in Part 1 of Example 8.18.
Even if an asymptotic distribution exists and is determined correctly, using
it for hypothesis testing can lead to poor results, since the asymptotic distri-
bution only produces an approximate signiﬁcance point. Under very general
assumptions one can show that the approximate signiﬁcance point approaches
the true signiﬁcance point as n →∞, hence the more data we have (that is,
the longer the vector w is), the more accurate results the asymptotic distribu-
tion produces. From this point of view it is desirable to have some information
on the speed of convergence in Theorem 8.16. This is, however, a non-trivial
issue and we do not discuss it here. We also note that in both parts of Exam-
ple 8.18 the null hypothesis is simple, that is, P′ consists of a single point. In
general, one has to determine an asymptotic distribution of ∆n

f ×∞,n
θ

for
every θ ∈P′, which is an additional complication.
With the increase of computing power it has become possible to obtain
approximations to the null hypothesis distribution of the likelihood ratio by
data simulation, rather than by using asymptotic distributions supplied by
Theorem 8.16. This is done as follows. Suppose, as before, that we are given a
vector w ∈Wn. Then for every θ ∈P′ we randomly generate a large number
of values of f ×∞,n
θ
, say vectors w1(θ), . . . , wN(θ) ∈Wn, with N independent
of θ. We then calculate ∆n(wj(θ)) for j = 1, . . . , N and choose Kα(θ) as
the largest value for which the ratio of the number of elements in the set

j : ∆n(wj(θ)) ≤Kα(θ)

and N does not exceed α. This procedure leads to
an approximate signiﬁcance point of the test Kapprox
α
, if the set P′ is ﬁnite.
Then the null hypothesis is rejected if ∆(w) ≤Kapprox
α
. If, however, P′ is
inﬁnite, the following simpliﬁed testing procedure is used.
Let {g1,k(x1, . . . , xk)} be a maximum likelihood estimator for θ under
model M1, and let θ0 = g1,n(w1, . . . , wn) be the corresponding maximum
likelihood estimate. Then it seems reasonable to base a decision to either ac-
cept or reject the null hypothesis on the number Kα(θ0) alone, rather than
on Kα = infθ Kα(θ), as we did earlier. With this approach we set Wn =
Wn(Kα(θ0)) and hence the null hypothesis is rejected, if ∆(w) ≤Kα(θ0). Of
course, with this approach we can only guarantee that inequality (8.21) holds
for θ = θ0. To determine Kα(θ0) we need to know the distribution of the
random variable ∆n

f ×∞,n
θ0

. An approximation to this distribution can be
obtained either from Theorem 8.16, or from data simulation by generating a
large number of values of f ×∞,n
θ0
. This method of hypothesis testing is called
the parametric bootstrap method and was described in the special case of the
comparison of evolutionary models in Sect. 5.6. The parametric bootstrap
method has been investigated and found satisfactory by many authors. It is
widely used in statistics.

266
8 Elements of Statistics
8.4 Signiﬁcance of Scores for Global Alignments
One can also apply the likelihood ratio test in the case when the null and
alternative hypotheses are not nested. Theorem 8.16 does not generally hold
in this case, so one has to determine the null hypothesis distribution of the
likelihood ratio statistic, or an approximation to this distribution, in a diﬀerent
way. Certainly, one can proceed by simulation, as was described in the previous
section. However, in some cases it is still possible to obtain a theoretical
asymptotic distribution. In this section we give an example of such a situation.
We consider the problem of testing the signiﬁcance of the score of a global
ungapped pairwise alignment of sequences of letters from an alphabet Q.
We deﬁne models M1 and M2 as follows. Set S = {(a, b) : a, b ∈Q} and
let B be the σ-algebra of all events in S. For elementary events in S deﬁne
P

{(a, b)}

= pap′
b,
where pa > 0 and p′
b > 0, a, b ∈Q, are such that 
a∈Q pa = 
b∈Q p′
b = 1.
Let M1 be the reduced statistical model associated with the probability space
(S, B, P). This model is simple, that is, the parameter space of M1 is a single
point. Model M2 is the reduced model associated with the probability space
(S, B, ˆP), where
ˆP

{(a, b)}

= pab.
Here pab > 0, for a, b ∈Q, are such that

a,b∈Q
pab = 1.
(8.30)
Clearly, M2 is also a simple model. The range of the models is the set W =
{1, . . . , |Q|2}, where |Q| is the number of elements in the alphabet Q. The
corresponding random variables for the models are f = ˆf = ϕ (see formula
(8.1)).
Let w = (w1, . . . , wn) ∈Wn. Then, unless pab = pap′
b for all a, b ∈Q, the
corresponding null and alternative hypotheses are not nested. Set (aj, bj) =
ϕ−1(wj) for j = 1, . . . , n and x0 = a1 . . . an and y0 = b1 . . . bn. One can think
of the null hypothesis as the statement that x0 and y0 are unrelated randomly
generated sequences and of the alternative hypothesis as the statement that
the residues occupying the same positions in x0 and y0 are in some way related.
In these terms the rejection of the null hypothesis means that we should treat
x0 and y0 as related sequences, and the acceptance of the null hypothesis
means that we may treat them as unrelated. For this reason model M1 is
sometimes called a random model, and model M2 with pab ̸= pap′
b for some
a, b ∈Q, a match model.
We have
∆n(w) =
n

j=1
pajp′
bj
pajbj
.

8.4 Signiﬁcance of Scores for Global Alignments
267
We will now make a speciﬁc choice of {pab}, provided {pa} and {p′
b} have
been chosen. Suppose there exists a substitution matrix whose elements s(a, b),
a, b ∈Q, are integers satisfying conditions (iv)-(vi) from Sect. 7.2 for {pa} and
{p′
b} and set
pab = pap′
b exp(λs(a, b)),
(8.31)
where λ > 0 is found from equation (7.29). Equation (7.29) guarantees that
condition (8.30) is satisﬁed. Note that a solution to (7.29) exists due to our
assumptions on the substitution matrix.
It follows from (8.31) that
∆n(w) = exp

−λS(x0, y0)

,
where S(x0, y0) denotes the score of the (only) global alignment between the
sequences x0 and y0. Since the likelihood ratio statistic is expressed in terms
of the score of the corresponding global alignment, the likelihood ratio test in
this case is the test of the signiﬁcance of the score S(x0, y0).
Clearly, in this case the set Wn(K) deﬁned in (8.23) is a Borel set in Rn
and E0
n = ∅(see (8.24)). We now ﬁx a signiﬁcance level 0 < α < 1 and attempt
to determine the signiﬁcance point Kα from the equation
F∆n(f ×∞,n)(Kα) = α.
For z ≥0 we have
F∆n(f ×∞,n)(z) = P

(x, y) ∈Sn : S(x, y) ≥−ln z
λ
 
,
where the probability in the right-hand side is calculated in the sense of
the probability space (Sn, Bn, P) introduced in Sect. 7.3. Thus we need to
determine the distribution of S(x, y) considered as a random variable on
(Sn, Bn, P).
We have
S(x, y) =
n

i=1
s(xi, yi),
for all (x, y) ∈Sn, that is, S is the sum of iid random variables, each of which
is distributed as the step size corresponding to the random walk associated
with the substitution matrix and random model (see Sect. 7.2). The mean
value µ and variance σ2 of the step size are
µ =

a,b∈Q
s(a, b)pap′
b,
σ2 =

a,b∈Q
(s(a, b) −µ)2pap′
b,

268
8 Elements of Statistics
where µ < 0 and σ2 > 0 due to our assumptions on the substitution ma-
trix. Hence, by the Central Limit Theorem (Theorem 6.64), the distribution
function of S becomes arbitrarily close to that of a random variable which
is normally distributed with mean nµ and variance nσ2, uniformly on R, as
n →∞. Therefore, if n is large, one can ﬁnd an approximate signiﬁcance
point of the test Kapprox
α
from the equation
1
√
2πσ2
# ∞
−ln Kapprox
α
/λ
exp

−(z −nµ)2
2nσ2

dz = α.
We then set Wn = Wn(Kapprox
α
). Therefore, with this test, H0 is rejected in
favor of HA, if S(x0, y0) ≥−ln Kapprox
α
/λ. Note that since µ < 0, we have
Kapprox
α
→∞as n →∞.
Above we assumed that model M1 is simple. One could, however, allow
{pa} and {p′
b} to be parameters subject to the conditions pa ≥0, p′
b ≥0
for all a, b ∈Q, and 
a∈Q pa = 
b∈Q p′
b = 1. These parameters can be
estimated from the data w (equivalently, the sequences x0 and y0). A natural
way to estimate the parameters for M1 is, for every a ∈Q, to set pa to be the
frequency of the occurrence of a in the sequence x0 and, for every b ∈Q, to set
p′
b to be the frequency of the occurrence of b in the sequence y0, as we did in
Sect. 7.1 for the case of local alignments (here we assume that the composition
of the sequences x0 and y0 shows enough variation to ensure that pa > 0 and
p′
b > 0 for all a, b ∈Q). One can prove that the estimates obtained in this way
are in fact maximum likelihood estimates. Of course, {pa} and {p′
b} are the
parameters of model M2 as well (we assume that the substitution matrix used
to deﬁne M2 in (8.31) satisﬁes all the necessary requirements for the values
of {pa} and {p′
b} estimated for M1), so any choice of parameter values for M1
is automatically a choice of parameter values for M2.
If the parameters of M1 and M2 are estimated from the data as described
above and a signiﬁcance point is produced as explained, then we in fact follow
the procedure of the parametric bootstrap method introduced at the end of
Sect. 8.3, that is, the distribution of the likelihood ratio is studied not for all
possible values of the parameters of M1, but for the single value derived from
the data. Note that a similar approach was taken in Chap. 7 for the case of
local alignments. However, the problem of the signiﬁcance of scores for local
alignments is much harder, as it does not ﬁt into the general framework of
hypothesis testing described in this chapter.
The above discussion applies only to testing the signiﬁcance of the scores
of ungapped alignments. If gaps are allowed in alignments, then this approach
does not work. In this case an approximation to the null hypothesis distribu-
tion of the likelihood ratio can be produced by data simulation. Similarly, for
the problem of testing the signiﬁcance of the scores of local gapped alignments,
an approximation to the distribution of the random variable s introduced in
Sect. 7.1 can be found from data simulation. It turns out that for local gapped
alignments the general form of formulas (7.26), (7.27) remains true, but the
constants K and λ are no longer found from formulas (7.25) and (7.29).

Exercises
269
Exercises
8.1. Consider a statistical model with q = 1, and denote the parameter θ = θ1
by p. Let P = {p ∈R : 0 ≤p < 1}, and assume that fp for each p is discrete,
takes values 0, 1, 2, . . . (hence W = Z+) and has the geometric distribution
with mean p/(1 −p) (see Sect. 6.9). Find a maximum likelihood estimator for
p.
8.2. Consider a statistical model with q = 1, and denote the parameter θ = θ1
by p. Let P = {p ∈R : 0 ≤p ≤1}, and assume that fp for each p is discrete,
takes values 0, . . . , n (hence W = {0, . . . , n}) and has the binomial distribution
with mean np (see Sect. 6.9). Find a maximum likelihood estimator for p.
8.3. Consider a statistical model with q = 1, and denote the parameter θ = θ1
by λ. Let P = {λ ∈R : λ > 0}, and assume that fλ for each λ is continuous,
takes values in (0, ∞) (that is, W = (0, ∞)) and has the exponential distribu-
tion with mean 1/λ (see Sect. 6.9). Find a maximum likelihood estimator for
λ.
8.4. Consider the family of Markov models arising from the a priori con-
nectivity in Fig. 8.2. Denote the parameters associated with the transi-
tion probabilities as shown. For the corresponding statistical model we have
Fig. 8.2.
θ = (α, β, γ, δ, ε), and P = [0, 1]5 \ R, where

270
8 Elements of Statistics
R = {(α, β, γ, δ, ε) ∈[0, 1]5 : β + γ = 1, δ + ε = 1}∪
{(α, β, γ, δ, ε) ∈[0, 1]5 : α ̸= 0, β = 1, γ = 0}∪
{(α, β, γ, δ, ε) ∈[0, 1]5 : α = 0, β = 1, γ = 0, δ ̸= 0}∪
{(α, β, γ, δ, ε) ∈[0, 1]5 : α ̸= 1, δ = 0, ε = 1}∪
{(α, β, γ, δ, ε) ∈[0, 1]5 : α = 1, γ ̸= 0, δ = 0, ε = 1}
(by removing the set R from [0, 1]5 we ensure that the Markov chains are
non-trivially connected).
Show that for this model formula (3.2) gives a maximum likelihood esti-
mator for θ. [If you ﬁnd it hard to prove the general statement, prove it in
a more restricted setting by ﬁxing some of the components of θ, but keep in
mind that the resulting a priori connectivity must be that of a non-trivially
connected Markov chain.] How many maximum likelihood estimators did you
obtain? Are they consistent?
8.5. Let the family of underlying Markov chains of a family of HMMs be as
in Exercise 8.4, and Q be the two-letter alphabet {A, B}. Let
q1(A) = ν, q1(B) = 1 −ν,
q2(A) = η, q2(B) = 1 −η.
For the corresponding statistical model we have θ = (α, β, γ, δ, ε, ν, η), and
the parameter space is the product of that from Exercise 8.1 and [0, 1]2.
Show that for this model formula (3.17) gives a maximum likelihood es-
timator for θ. [If you ﬁnd it hard to prove the general statement, prove it
in a more restricted setting by constraining the connectivity of the family of
underlying Markov chains, but keep in mind that the resulting a priori con-
nectivity must be that of a non-trivially connected Markov chain.] How many
maximum likelihood estimators did you obtain? Are they consistent?
8.6. Consider a statistical model with q = 2, and denote the parameters θ1 and
θ2 by µ and σ2 respectively. Let P = {(µ, σ2) ∈R2 : σ2 > 0}. Suppose that
W = R and assume that f(µ,σ2) is continuous and has the normal distribution
with mean µ and variance σ2. Prove that the model is smooth.

9
Substitution Matrices
9.1 The General Form of a Substitution Matrix
Up to this moment, in problems related to sequence alignment, we have as-
sumed that substitution matrices were given. In this chapter we will show
how such matrices are actually constructed. As we have seen in Chap. 7 and
Sect. 8.4, in order to assess the signiﬁcance of the score of an optimal local or
global ungapped alignment between sequences x0 and y0, one needs to have
a substitution matrix that has integer entries and satisﬁes conditions (iv)-
(vi) from Sect. 7.2 with {pa} and {p′
b} derived from x0 and y0 respectively
as the frequencies with which letters from the relevant alphabet Q occur in
the sequences. Of course, it is impossible to construct a matrix that satis-
ﬁes the above conditions for all choices of {pa} and {p′
b}. Therefore, we will
attempt to construct substitution matrices that satisfy these conditions for
some “generic” choice of {pa} and {p′
b}, that is, for some “generic” choice
of random model. Such substitution matrices will be called admissible. We
will always assume that pa = p′
a > 0 for all a ∈Q and call the numbers
{pa} the background frequencies. In the following sections we will explain how
background frequencies can be constructed, and for the moment we assume
that they have already been chosen.
As we have seen in Sect. 8.4, the elements of every admissible substitution
matrix can be represented in the form
s(a, b) = C × ln
 pab
papb

,
(9.1)
for some choice of {pab} (all of which are positive), that is, for some choice of
match model, and some C > 0. From the uniqueness of a solution to equation
(7.6) (see also (7.29)), it follows that representation (9.1) is unique.
Conversely, if the elements of a substitution matrix are integers represented
in the form (9.1) with pab ̸= papb for some a, b ∈Q, that satisfy condition
(vi) from Sect. 7.2, then the matrix is admissible. Indeed, since pab ̸= papb

272
9 Substitution Matrices
for some a, b ∈Q, there exist ˆa,ˆb ∈Q such that pˆaˆb > pˆapˆb and hence the
maximal element ˆs of the substitution matrix is positive. Further, since pa > 0
for all a ∈Q, condition (iv) follows. Next, condition (v) is a consequence of the
well-known Kullback-Leibler inequality, as stated in the following theorem.
Theorem 9.1. (The Kullback-Leibler Inequality)Let 0 < cj < 1, 0 <
dj < 1 for j = 1, . . . , n be such that n
j=1 cj = n
j=1 dj = 1. Then we have
n

j=1
ln
 cj
dj

dj ≤0,
(9.2)
and the equality holds if and only if cj = dj for j = 1, . . . , n.
Theorem 9.1 easily follows from Jensen’s inequality and the concavity of ln x
(see Exercise 9.1). Applying inequality (9.2) to {pab} and {papb} yields con-
dition (v).
The general approach to constructing admissible substitution matrices
that we will take in the forthcoming sections is as follows. First, we make
particular choices of a random model {pa} and a match model {pab}. Then
we consider the matrix given by (9.1) making a particular choice of C. To
ensure that the resulting matrix is symmetric, {pab} will be chosen to satisfy
pab = pba for all a, b ∈Q. Not all entries of the matrix may be integers, and,
even if they are, the matrix may not satisfy condition (vi) from Sect. 7.2.
Therefore, we may need to ﬁrst round up the entries to the nearest integers
and then alter the integers slightly to ﬁt condition (vi). In most cases this
procedure leads to an admissible substitution matrix with the same random
model and a slightly altered match model {˜pab} and constant ˜C.
The above approach for obtaining substitution matrices in principle can
be applied to sequences of letters from any alphabet Q. For DNA and RNA
sequences very simple substitution matrices are usually eﬀective. For example,
a popular substitution matrix used for DNA sequences has 5 on the main
diagonal and -4 elsewhere. However, when the sequences under consideration
code for protein, in most cases it turns out to be more eﬃcient to compare
the protein translations than to compare the DNA or RNA sequences directly.
Sometimes one may wish to compare non-coding DNA regions, and for such
purposes more sensitive substitution matrices than the one mentioned above
are required. For example, in this case one can set for a random model pA =
pC = pG = pT = 1/4, and for a match model one can choose {pab} in such
a way that the pairs representing transitions are more likely than the pairs
representing transversions.
In this chapter we concentrate on protein coding sequences and for this
reason discuss only amino acid substitution matrices. Thus in the next two
sections we assume that Q is either the 20-letter amino acid alphabet, or the
artiﬁcial 3-letter alphabet {A, B, C} used for illustration purposes.

9.2 PAM Substitution Matrices
273
9.2 PAM Substitution Matrices
In this section we will describe the PAM family of substitution matrices in-
troduced in [DSO]. A particular matrix in this family is denoted by PAMn for
n ∈N. The construction of PAM matrices starts with blocks which are local
ungapped multiple alignments of proteins whose sequences are available from
existing databases. The proteins are required to be closely related, namely,
each sequence in any block is required to be no more than 15% diﬀerent from
any other sequence in the block. The blocks are constructed on the basis of
analyzing matches and mismatches in the sequences, that is, essentially by us-
ing the multidimensional identity matrix as a substitution matrix for multiple
alignments. Before giving the full details of constructing PAM matrices, we
sketch the procedure for choosing a match model provided a random model
has been ﬁxed.
First, a matrix S(1) = (S(1)a b) is produced from the blocks. Its elements
are non-negative and sum up to 1 in each row. Hence S(1) is the matrix of
transition probabilities of a Markov chain whose non-zero states are labeled
by the letters of Q. It turns out that one of the stationary probability dis-
tributions of this Markov chain is ϕ = (p1, . . . , p20), and in most cases this
is the only stationary probability distribution, since usually all entries of the
matrix S(n) = S(1)n are non-zero if n ≥n0 for some n0 (see Sect. 5.4).
We take ϕ as the vector of initialization probabilities. The Markov chain is
meant to model the substitution process in amino acid sequences, and thus
S(1) describes amino acid substitutions after two steps of the chain (counting
the initialization step), that is, short-term evolution. It is assumed that the
short-term evolution leads to very few amino acid mutations. Speciﬁcally, it
is required that the probability of the event that an amino acid produced on
the ﬁrst step of the chain changes on the second step is equal to 0.01. From
the explicit construction of the probability space associated with a Markov
chain given in Part 4 of Example 6.14, it is clear that this condition can be
written as

a,b∈Q:a̸=b
paS(1)a b = 0.01.
(9.3)
If we associate with the Markov chain a sequence of random variables {fk}
as described at the beginning of Sect. 6.11, then from formula (6.36) we have
for all a, b ∈Q and k ∈N
P

{e ∈S : fk+n(e) = b, fk(e) = a}

= (ϕS(k −1))aS(n)a,b
= paS(n)a b,
(9.4)
where we set S(0) to be the identity matrix. Hence, the matrix S(n) describes
amino acid substitutions after n + 1 steps of the chain. If n is large, S(n)
models long-term evolution of amino acid sequences, and thus one is usually
interested in large values of n.
We deﬁne a match model by setting

274
9 Substitution Matrices
pab = P

{e ∈S : fn+k(e) = b, fk(e) = a}

,
a, b ∈Q, k ∈N,
which due to (9.4) is equivalent to setting
pab = paS(n)a b,
a, b ∈Q.
(9.5)
This choice of match model leads to the PAMn matrix. Note that the related-
ness of any two amino acids imposed by the match model is by way of one of
them being separated from the other by n steps of the evolutionary process
modeled by the Markov chain.
As we will see later, the construction leading to the matrix S(1) does not
guarantee that pab > 0 for all a, b ∈Q. Therefore pab may need to be further
adjusted to ensure that all of them are positive. In most cases of interest,
however, all entries of S(n) are non-zero for the relevant values of n, and thus
no such adjustments are required. We will also see that S(1) is constructed in
such a way that pab = pba for all a, b ∈Q.
Since in most cases all entries of S(n) are non-zero if n ≥n0 for some n0,
the sequence of matrices {S(n)} usually converges to the matrix
S(∞) =
⎛
⎜
⎜
⎜
⎝
p1 . . . p20
p1 . . . p20
...
...
...
p1 . . . p20
⎞
⎟
⎟
⎟
⎠
(see Sect. 5.4). Formula (9.1) then implies that the sequence {PAMn} con-
verges to the zero matrix as n →∞, if the value of C is ﬁxed independently
of n. As we mentioned above, one is usually interested in large values of
n. However, n must not be “too large” for very little information is left in
the PAMn matrix as n →∞. Some of the commonly used values of n are
60, 120, 200, 250. The performance of similarity searches with PAMn matrices
for diﬀerent values of n has been studied (see, e.g., [EG]).
It remains to explain how a random model (that is, a set of background
frequencies) is chosen and S(1) is constructed. Suppose we are given a collec-
tion of blocks. For each block we determine all the most parsimonious trees,
and both a random model and S(1) are derived from these trees. We will illus-
trate the process by the following example. Suppose that Q is the three-letter
alphabet {A, B, C}, and that we are given the following two blocks
A A B
A C B
C C B
,
A B C
A B B.
(9.6)
All the most parsimonious trees for the ﬁrst block are shown in Fig. 5.4 in
Sect. 5.2. The most parsimonious trees for the second block are shown in Fig.
9.1.

9.2 PAM Substitution Matrices
275
Fig. 9.1.
From these trees we will ﬁrst produce a matrix H = (Hab) of counts as
follows. The matrix H is symmetric, and its columns and rows are indexed by
the elements of Q. Fix a, b ∈Q and let ﬁrst a ̸= b. For each block we calculate
the number of times a and b occur at the same position in two nodes joined by
a branch of a most parsimonious tree for the block and divide by the number
of trees for the block. Summing up the resulting numbers over all blocks gives
the entries Ha b = Hb a. For the diagonal elements the procedure is exactly the
same, but the resulting value is multiplied by 2. For blocks (9.6) the matrix
H is shown in Fig. 9.2.
Fig. 9.2.
We remark that any method of phylogenetic reconstruction that can pre-
dict sequences at all ancestral nodes could have been used above in place of

276
9 Substitution Matrices
the parsimony method. The parsimony method was chosen because of its com-
putational speed. Note, however, that this method is not the best approach
to phylogenetic reconstruction.
Set
D =

a,b∈Q
Ha b
(9.7)
and deﬁne background frequencies as follows
pa =

b∈Q
Ha b
D
,
a ∈Q.
(9.8)
We assume that the block data used to produce the matrix H is diverse enough
to ensure that pa > 0 for all a ∈Q. In fact, we assume that pa ≥0.01 for all
a ∈Q. For blocks (9.6) we obtain
pA = 1
3,
pB = 7
18,
pC = 5
18.
(9.9)
For real block databases it is often the case that the background frequencies
deﬁned in (9.8) are well approximated by the frequencies of the occurrence of
the elements of Q in the original block data. Therefore these frequencies that
we denote by papprox
a
, a ∈Q, are often used in place of those deﬁned in (9.8).
For blocks (9.6) we have
p
approx
A
= 1
3,
p
approx
B
= 6
15,
p
approx
C
= 4
15.
We will now describe the construction of S(1). First, we introduce a matrix
R = (Ra b)
Ra b =
Ha b

c∈Q
Ha c
= Ha b
Dpa
,
a, b ∈Q.
Clearly, all elements of R are non-negative and sum up to 1 in each row. Hence
R is the matrix of transition probabilities of a Markov chain. The matrix S(1)
is obtained from R by a small modiﬁcation that ensures that condition (9.3)
holds. For λ > 0 we introduce a matrix S[λ] =

S[λ]
ab

as follows
S[λ]
ab = λRa b,
a, b ∈Q, a ̸= b,
S[λ]
aa = 1 −λ

b∈Q:b̸=a
Ra b,
a ∈Q.
The matrix S[λ] satisﬁes condition (9.3) for

9.2 PAM Substitution Matrices
277
λ = λ0 =
0.01

a,b∈Q:a̸=b
paRa b
.
In the above formula we assume that the block data is diverse enough to
ascertain that 
a,b∈Q:a̸=b paRa b ̸= 0 (note that in the critical case when all
the sequences in each block are identical, the above expression is equal to
0). Since pa ≥0.01 for all a ∈Q, the elements of the matrix S[λ0] are non-
negative. It then follows that S[λ0] is the matrix of transition probabilities of
a Markov chain. We now deﬁne S(1) = S[λ0].
Next, we will show that
paS(n)a b = pbS(n)b a,
a, b ∈Q, a ̸= b
(9.10)
for all n ∈N (see (9.5)). We will prove this by induction. This statement holds
for n = 1 since for all a ̸= b we have paS(1)a b = λ0Ha b/D and the matrix H
is symmetric. We now assume that (9.10) holds for all n < N and prove it for
n = N. For all a, b ∈Q we have
paS(N)a b =

c∈Q
paS(N −1)a cS(1)c b =

c∈Q
pcS(N −1)c aS(1)c b
= pb

c∈Q
S(1)b cS(N −1)c a = pbS(N)b a,
which proves (9.10).
It remains to be shown that the vector ϕ = (p1, . . . , p20) is a stationary
probability distribution for the Markov chain deﬁned by S(1), and hence (9.4)
holds. Since H is symmetric, for every a ∈Q we have
(ϕS(1))a =

b∈Q
pbS(1)b a = paS(1)a a +

b∈Q:b̸=a
pbS(1)b a
= pa
⎛
⎝1 −λ0
Dpa

c∈Q:c̸=a
Ha c
⎞
⎠+ λ0
D

b∈Q:b̸=a
Hb a = pa,
as required.
For blocks (9.6) we have
R =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
5
6
0
1
6
0
13
14
1
14
1
5
1
10
7
10
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,

278
9 Substitution Matrices
which gives λ0 = 0.06 and
S(1) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
99
100
0
1
100
0
697
700
3
700
3
250
3
500
491
500
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We see that the matrix S(1) has zero elements. One, however, is always inter-
ested in its powers S(n) for large n. It is not hard to see that for blocks (9.6)
all the elements of the matrix S(n) are non-zero, if n ≥2. For example, for
n = 2 we have
S(2) =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0.98022 0.00006 0.01972
0.00005 0.99147 0.00847
0.02366 0.01186 0.96446
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
where the values were rounded to the ﬁfth decimal place. Then formula (9.5)
gives for n = 2
pAA = 0.32674,
pAB = pBA = 0.00002,
pAC = pCA = 0.00657,
pBB = 0.38557,
pBC = pCB = 0.00329,
pCC = 0.26790,
(9.11)
where again the values were rounded to the ﬁfth decimal place. Finally, from
(9.1), (9.9) and (9.11) we obtain the following PAM2 matrix for blocks (9.6)
PAM2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1 −9 −3
−9
1 −3
−3 −3
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
where we set C = 1 and rounded the values to the nearest integers. Note that
condition (vi) from Sect. 7.2 is satisﬁed for the above matrix, and therefore
no further adjustments are required.
Generally one is interested in constructing the PAMn matrices for much
larger values of n. Figure 9.3 shows the PAM250 matrix for the 20-letter amino
acid alphabet derived from 71 blocks that were produced from databases of
amino acid sequences. The name of PAM matrices originates from accepted
point mutations. An accepted point mutation in a protein is a replacement
of one amino acid by another accepted by natural selection. The amino acid
diﬀerences observed in the sequences in each block from the database of blocks

9.3 BLOSUM Substitution Matrices
279
Fig. 9.3.
used to construct PAM matrices are all accepted point mutations coming from
the predicted ancestral sequences on phylogenetic trees, since all the protein
sequences in the blocks have been taken from currently existing species that
successfully incorporated these mutations.
9.3 BLOSUM Substitution Matrices
PAM substitution matrices appeared at the time when available data consisted
mainly of families of sequences of closely related proteins. From such data one
could only hope to estimate the short-term evolutionary matrix S(1) and
then, in order to extrapolate to long-term evolutionary events, one had to
raise S(1) to a high power, as we described in the preceding section. When
data for more distantly related proteins became available, it was shown that
the matrix S(n) for large values of n does not in fact reﬂect the true patterns
of long-term amino acid substitutions [GCB]. As databases of more diverged
sequences grew, it became possible to estimate such patterns directly from

280
9 Substitution Matrices
the sequences and avoid Markov chain extrapolation. Below we will describe
a very natural procedure that leads to the BLOSUM (BLOcks SUbstitution
Matrices) family of substitution matrices introduced in [HH]. A particular
matrix in this family is denoted by BLOSUMr, where 0 < r < 100.
As the name of BLOSUM matrices suggests, they are also derived from a
database of blocks. As in the case of PAM matrices, the blocks are initially
constructed on the basis of analyzing only matches and mismatches in the
sequences. In fact, there is an automated way for constructing blocks given a
substitution matrix, and initially the substitution matrix is taken to be the
identity matrix. Sequences within each block are allowed to be more diverged
than in the case of PAM matrices. The process of constructing a BLOSUM
matrix starts with choosing a number r and then producing clusters of se-
quences in each block. We group the sequences in each block into clusters in
such a way that each sequence in any cluster has r% or higher sequence iden-
tity with at least one other sequence in the cluster. We will illustrate this by
the following example. Suppose that Q is the three-letter alphabet {A, B, C},
and we are given the three blocks shown below
x1 A B C A B
x2 A B C A C
x3 B B C A B
x4 C B C A C
x5 A A A C B
,
x6 A B C
x7 A B C
x8 A A C
x9 C B C
x10 A A B
x11 B A B
,
x12 A A A C B A B C
x13 B A A C B A B C
x14 A A A C B A C B
x15 A A A C B A C C.
(9.12)
If we choose r = 80, we obtain two clusters in the ﬁrst block: {x1, x2, x3, x4},
{x5}, ﬁve clusters in the second block: {x6, x7}, {x8}, {x9}, {x10}, {x11}, and
one cluster in the third block: {x12, x13, x14, x15}. Clustering is done to reduce
overrepresentation of closely related sequences, and in the future each cluster
will be treated essentially as a single sequence.
Next, we will produce a matrix H = (Hab) of counts as follows. The matrix
H is symmetric, and its columns and rows are indexed by the elements of Q.
Fix a, b ∈Q and let ﬁrst a ̸= b. Next, ﬁx a block and two particular clusters
in the block. Let n and m be the numbers of sequences in the clusters. We
now calculate the number of times a and b occur at the same position in the
clusters, counting only those occurrences when a and b do not belong to the
same cluster, and divide the result by nm. Finally, we sum up the resulting
values over all pairs of clusters in each block and over all blocks afterwards.
This gives the entries Ha b = Hb a. For the diagonal elements the procedure is

9.3 BLOSUM Substitution Matrices
281
exactly the same, but the resulting value is multiplied by 2. For blocks (9.12)
the matrix H is shown in Fig. 9.4.
Fig. 9.4.
We now deﬁne background frequencies by formula (9.8) with D given by
(9.7). As in the preceding section, we assume that the data is diverse enough
to guarantee that pa > 0 for all a ∈Q. For blocks (9.12) we obtain
pA = 57
140,
pB = 91
280,
pC = 75
280.
(9.13)
For real block databases it is often the case that the background frequencies
deﬁned in (9.8) are well approximated by the frequencies of the occurrence of
the elements of Q in the original block data, where each occurrence has the
weight 1/n with n equal to the number of sequences in the cluster containing
the occurrence. Therefore these frequencies that we denote by papprox
a
, a ∈Q,
are often used in place of those deﬁned in (9.8). For blocks (9.12) we have
p
approx
A
= 19
44,
p
approx
B
= 13
44,
p
approx
C
= 3
11.
Further, for a match model we set
pab = Ha b
D .
Here we assume that the data is diverse enough and guarantees that pab > 0
for all a, b ∈Q. For blocks (9.12) this gives

282
9 Substitution Matrices
pAA = 13
70,
pAB = pBA = 41
280,
pAC = pCA = 21
280,
pBB = 5
70,
pBC = pCB = 15
140,
pCC = 6
70.
(9.14)
Next, we construct the Level 1 BLOSUMr matrix from formula (9.1) with
C = 2/ ln 2 (sometimes other choices of C are made). For blocks (9.12), from
formulas (9.13) and (9.14) we obtain
Level 1 BLOSUM80 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
0 −1
0 −1
1
−1
1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
Further, the Level 1 BLOSUMr matrix is used to obtain a new set of blocks,
and the procedure is repeated. It leads to the Level 2 BLOSUMr matrix.
Finally, the Level 2 BLOSUMr matrix is used to obtain a third database of
blocks, and the ﬁnal BLOSUMr matrix is derived from it in the same way.
In fact, on the last step the clustering percentage r is allowed to be diﬀerent
from those on the ﬁrst two steps. In the original construction r was set to be
equal to 60 on the ﬁrst two steps and was allowed to vary arbitrarily on the
last one. The number of blocks on the ﬁrst step was 2205, on the second 1961
and on the third 2106.
One often uses prior knowledge about evolutionary distances between the
sequences of interest to choose which BLOSUM matrix to use. If there is
no prior information, the BLOSUM62 and BLOSUM50 matrices are often
used. The BLOSUM62 matrix (shown in Fig. 9.5) is standard for ungapped
matching, and the BLOSUM50 matrix is often used for ﬁnding alignments
containing gaps.
The relative performance of PAM and BLOSUM matrices has been studied
for a variety of searches, and the broad conclusion is that using BLOSUM
matrices generally leads to better results [HH].

Exercises
283
 C
 S
 T
 P
 A
 G
 N
 D
 E
 Q
 H
 R
 K
 M
 I
 L
 V
 F
 Y
 W
C
9
-1
-1
-3
0
-3
-3
-3
-4
-3
-3
-3
-3
-1
-1
-1
-1
-2
-2
-2
S
-1
4
1
-1
1
0
1
0
0
0
-1
-1
0
-1
-2
-2
-2
-2
-2
-3
T
-1
1
4
1
-1
1
0
1
0
0
0
-1
0
-1
-2
-2
-2
-2
-2
-3
P
-3
-1
1
7
-1
-2
-1
-1
-1
-1
-2
-2
-1
-2
-3
-3
-2
-4
-3
-4
A
0
1
-1
-1
4
0
-1
-2
-1
-1
-2
-1
-1
-1
-1
-1
-2
-2
-2
-3
G
-3
0
1
-2
0
6
-2
-1
-2
-2
-2
-2
-2
-3
-4
-4
0
-3
-3
-2
N
-3
1
0
-2
-2
0
6
1
0
0
-1
0
0
-2
-3
-3
-3
-3
-2
-4
D
-3
0
1
-1
-2
-1
1
6
2
0
-1
-2
-1
-3
-3
-4
-3
-3
-3
-4
E
-4
0
0
-1
-1
-2
0
2
5
2
0
0
1
-2
-3
-3
-3
-3
-2
-3
Q
-3
0
0
-1
-1
-2
0
0
2
5
0
1
1
0
-3
-2
-2
-3
-1
-2
H
-3
-1
0
-2
-2
-2
1
1
0
0
8
0
-1
-2
-3
-3
-2
-1
2
-2
R
-3
-1
-1
-2
-1
-2
0
-2
0
1
0
5
2
-1
-3
-2
-3
-3
-2
-3
K
-3
0
0
-1
-1
-2
0
-1
1
1
-1
2
5
-1
-3
-2
-3
-3
-2
-3
M
-1
-1
-1
-2
-1
-3
-2
-3
-2
0
-2
-1
-1
5
1
2
-2
0
-1
-1
I
-1
-2
-2
-3
-1
-4
-3
-3
-3
-3
-3
-3
-3
1
4
2
1
0
-1
-3
L
-1
-2
-2
-3
-1
-4
-3
-4
-3
-2
-3
-2
-2
2
2
4
3
0
-1
-2
V
-1
-2
-2
-2
0
-3
-3
-3
-2
-2
-3
-3
-2
1
3
1
4
-1
-1
-3
F
-2
-2
-2
-4
-2
-3
-3
-3
-3
-3
-1
-3
-3
0
0
0
-1
6
3
1
Y
-2
-2
-2
-3
-2
-3
-2
-3
-2
-1
2
-2
-2
-1
-1
-1
-1
3
7
2
W
-2
-3
-3
-4
-3
-2
-4
-4
-3
-2
-2
-3
-3
-1
-3
-2
-3
1
2
11
Fig. 9.5.
Exercises
9.1. Prove the Kullback-Leibler inequality.
9.2. To block data (9.6) add the block from Exercise 5.2 and ﬁnd the cor-
responding PAM3 matrix from the three blocks for the three-letter alphabet
{A, B, C}.
9.3. From the following block data construct the Level 1 BLOSUM60 matrix

284
9 Substitution Matrices
x1 A A A A B B B
x2 C C C A C A B
x3 B B C A B A C
x4 C C C A C B C
x5 A A B A B A B
,
x6 A C C A
x7 A C B A
x8 A A A C
x9 C C B A
x10 A A B B
x11 B A A C
,
x12 A A A C C C B B A A
x13 B A A C C A A A A A
x14 A B A B C A C C A C
x15 C A C C B A A C A A
for the three-letter alphabet {A, B, C}.

References
[AL]
Altschul, S. F., Gish, W., Miller, W., Myers, E. W., Lipman, D. J.: Basic
local alignment search tool. J. Mol. Biol., 215, 403–410 (1990)
[B]
Baum, L. E.: An equality and associated maximization technique in statis-
tical estimation for probabilistic functions of Markov processes. Inequali-
ties, 3, 1–8 (1972)
[BL]
Berger, B., Leighton, T.: Protein folding in the hydrophobic-hydrophilic
(HP) model is NP-complete. J. Comput. Biol., 5, 27-40 (1998)
[BHV]
Billera, L., Holmes, S., Vogtman, K.: Geometry of the space of phyloge-
netic trees. Advances in Appl. Math., 27, 733–767 (2001)
[BLE]
Bowie, J. U. L¨uthy, R., Eisenberg, D.: A method to identify protein se-
quences that fold into a known three-dimensional structure. Science 253,
164–170 (1991)
[BK]
Burge, C., Karlin, S.: Prediction of complete gene structures in human
genomic DNA. J. Mol. Biol., 268, 78–94 (1997)
[CL]
Carrillo, H., Lipman, D.: The multiple sequence alignment problem in
biology. SIAM J. Appl. Math., 48, 1073–1082 (1988)
[C-SE]
Cavalli-Sforza, L., Edwards, A.: Phylogenetic analysis: models and esti-
mation procedures. Am. J. Hum. Genetics, 19, 233–257 (1967)
[CD]
Chan, H. S., Dill, K. A.: Compact Polymers. Macromolecules, 22, 4559-
4573 (1989)
[CY]
Crescenzi, P., Goldman, D., Papadimitriou, C., Piccolboni, A., Yan-
nakakis, M.: On the complexity of protein folding. J. Comput. Biol., 5,
423–465 (1998)
[DSO]
Dayhoﬀ, M. O., Schwartz, R. M., Orcutt, B. C.: A model of evolu-
tionary change in proteins. Atlas of Protein Sequence and Structure,
5(Supplement 3), 345–352 (1978)
[DS]
Delcher, A. L., Harmon, D., Kasif, S., White, O., Salzberg, S. L.: Improved
microbial gene identiﬁcation with Glimmer. Nucleic Acids Res., 27, 4636–
4641 (1999)
[DKZ1]
Dembo, A., Karlin, S., Zeitouni, O.: Critical phenomena for sequence
matching with scoring. Ann. Prob., 22, 1993-2021 (1994)
[DKZ2]
Dembo, A., Karlin, S., Zeitouni, O.: Limit distribution of maximal non-
aligned two-sequence segmental score. Ann. Prob., 22, 2022-2039 (1994)

286
References
[Di]
Dill, K. A.: Theory for the folding and stability of globular proteins. Bio-
chemistry, 24, 1501–1809 (1985)
[DC]
Dill, K. A., Bromberg, S., Yue, K., Fiebig, K. M., Yee, D. P., Thomas, P.
D., Chan, H. S.: Principles of protein folding - a perspective from simple
exact models. Protein Sci., 4, 561-602 (1995)
[Do]
Doob, J. L.: Stochastic Processes. John Wiley & Sons (1953)
[DEKM]
Durbin, R., Eddy, S., Krogh, A., Mitchison, G.: Biological Sequence
Analysis. Cambridge University Press (1998)
[E]
Eddy, S. (with contributions by Birney, E.): HMMER, Version 2.3.1.
Washington University, St. Louis (2003)
[EG]
Ewens, W. J., Grant, G. R.: Statistical Methods in Bioinformatics.
Springer-Verlag (2001)
[F1]
Felsenstein, J.: Evolutionary trees from DNA sequences: a maximum like-
lihood approach. J. Mol. Evol., 17, 368–376 (1981)
[F2]
Felsenstein, J.: Conﬁdence limits on phylogenies: an approach using the
bootstrap. Evolution, 39, 783–791 (1985)
[F3]
Felsenstein, J.: PHYLIP, Phylogeny Inference Package, Version 3.57. Uni-
versity of Washington, Seattle (1995)
[FM]
Fitch, W. M., Margoliash, E.: Construction of phylogenetic trees. Science,
155, 279–284 (1967)
[GCB]
Gonnet, G. H., Cohen, M. A., Benner, S. A.: Exhaustive matching of the
entire protein sequence database. Science, 256, 1443–1445 (1992)
[G]
Gotoh, O.: An improved algorithm for matching biological sequences. J.
Mol. Biol., 162, 705–708 (1982)
[H]
H¨ackel,
E.:
Generelle
Morphologie
der
Organismen:
Allgemeine
Grundzuge der Organischen Formen-Wissenschaft, Mechanisch Begrun-
det Durch die von Charles Darwin, Reformite Descendenz-Theorie. Georg
Riemer, Berlin (1866)
[HI]
Hart, W. E., Istrail, S.: Fast protein folding in the hydrophobic-
hydrophilic model within three-eighths of optimal. J. Comp. Biol., 3,
53–96 (1996)
[HKY]
Hasegawa, M., Kishino, H., Yano, T.: Dating of the human-ape splitting
by a molecular clock of mitochondrial DNA. J. Mol. Evol., 22, 160–174
(1985)
[HH]
Henikoﬀ, S., Henikoﬀ, J. G.: Amino acid substitution matrices from pro-
tein blocks. Proc. Nat. Acad. Sci. USA, 89, 10915–1019 (1992)
[IJ]
Iwata, S., Lee, J. W., Okada, K., Lee, J. K., Iwata, M., Rasmussen, B.,
Link, T. A.,Ramaswamy, S., Jap, B. K.: Complete structure of the 11-
subunit bovine mitochondrial cytochrome bc1 complex. Science. 281, 64–
71 (1998)
[JC]
Jukes, T., Cantor, C.: Evolution of protein molecules. In: Munro, H. N.
(ed) Mammalian Protein Metabolism. Academic Press, New York (1969)
[Kan]
Kanehisa, M.: Post-Genome Bioinformatics. Oxford University Press
(2000)
[Kar]
Karlin, S.: A First Course in Stochastic Processes. Academic Press, N.Y.,
London (1968)
[KA]
Karlin, S., Altschul, S. F.: Applications and statistics for multiple high-
scoring segments in molecular sequences. Proc. Nat. Acad. Sci., 90, 5873–
5877 (1993)

References
287
[KD]
Karlin, S., Dembo, A.: Limit distributions of maximal segmental score
among Markov-dependent partial sums. Adv. Appl. Prob., 24, 113–140
(1992)
[KT]
Karlin, S., Taylor, H.: A First Course in Stochastic Processes. Academic
Press, N.Y., London (1975)
[KS]
Khorasanizadeh, S., Campos-Olivas, R., Clark, C. A., Summers, M. F.:
Sequence-speciﬁc 1H, 13C and 15N chemical shift assignment and sec-
ondary structure of the Htlv-I capsid protein. J. Biomol. NMR, 14, 199–
200 (1999)
[Ki]
Kimura, M.: A simple method for estimating evolutionary rate in a ﬁnite
population due to mutational production of neutral and nearly neutral
base substitution through comparative studies of nucleotide sequences. J.
Mol. Biol., 16, 111–120 (1980)
[KF]
Kolmogorov, A. N., Fomin, S. V.: Elements of the Theory of Functions
and Functional Analysis. Graylock Press, Rochester, N.Y. (1957)
[KH]
Krogh, A. M., Brown, I. S., Mian, K., S¨olander, K., Hausler, K.: Hidden
Markov models in computational biology: applications to protein mod-
elling. J. Mol. Biol., 235, 1501–1531 (1994)
[LD]
Lau, K. F., Dill, K. A.: A lattice statistical mechanics model of the confor-
mational and sequence spaces of proteins. Macromolecules, 22, 3986–3997
(1989)
[LTW]
Li, H., Tang, C., Wingreen, N. S.: A protein folds atypical? Proc. Nat.
Acad. Sci. USA, 95, 4987–4990 (1998)
[LAK]
Lipman, D. J., Altschul, S. F., Kececioglu, J. D.: A tool for multiple
sequence alignment. Proc. Nat. Acad. Sci. USA, 86, 4412–4415 (1989)
[MS]
Madsen, O., Scally, M., Douady, C. J., Kao, D. J., DeBry, R. W., Adkins,
R., Amrine, H. M., Stanhope, M. J., de Jong, W. W., Springer, M. S.:
Parallel adaptive radiations in two major clades of placental mammals.
Nature, 409, 610–614 (2001)
[NW]
Needleman, S. B., Wunsch, C. D.: A general method applicable to the
search for similarities in the amino acid sequence of two proteins. J. Mol.
Biol., 48, 443–453 (1970)
[N]
Neumaier, A.: Molecular modeling of proteins and mathematical predic-
tion of protein structure. SIAM Rev., 39, 407–460 (1997)
[PL]
Pearson, W. R., Lipman, D. J.: Improved tools for biological sequence
comparison. Proc. Nat. Acad. Sci. USA, 4, 2444–2448 (1988)
[RG]
Rambaut, A., Grassly, N.: Sequence-Gen: an application for the Monte
Carlo simulation of DNA sequence evolution along phylogenetic trees.
Comput. Appl. Biosci., 13, 235–238 (1997)
[R]
Rogers, J. S.: Maximum likelihood estimation of phylogenetic trees is
consistent when substitution rates vary according to the invariable sites
plus gamma distribution. Syst. Biol., 50, 713–722 (2001)
[SN]
Saitou, N., Nei, M.: The neighbor-joining method: a new method for re-
constructing phylogenetic trees. Mol. Biol. Evol., 4, 406–425 (1987)
[S]
Smith, R. D.: Correlations between bound N-Alkyl isocyanide orientations
and pathways for ligand binding in recombinant myoglobins. Thesis, Rice,
US ISSN 1047-8477 0806 (1999)
[SW]
Smith, T. F., Waterman, M. S.: Identiﬁcation of common molecular sub-
sequences. J. Mol. Biol., 147, 195–197 (1981)

288
References
[SM]
Sokal, R. R., Michener, C. D.: A statistical method for evaluating system-
atic relationships. Univ. of Kansas Scientiﬁc Bull., 28, 1409–1438 (1958)
[SK]
Studier, J. A., Keppler, K. J.: A note on the neighbor-joining algorithm
of Saitou and Nei. Mol. Biol. Evol., 5, 729–731 (1988)
[THG]
Thompson, J. D., Higgins, D. G., Gibson, T. J.: CLUSTALW: improv-
ing the sensitivity of progressive multiple sequence alignment through se-
quence weighting, position speciﬁc gap penalties and weight matrix choice.
Nucleic Acid Res., 22, 4673–4680 (1994)
[TBB]
Thompson, J. R., Bratt, J. M., Banaszak, L. J.: Crystal structure of cel-
lular retinoic acid binding protein I shows increased access to the binding
cavity due to formation of an intermolecular beta-sheet. J. Mol. Biol.,
252, 433–446 (1995)
[W]
Wilks, S. S.: Mathematical Statistics. John Wiley & Sons, N.Y., London
(1962)

Index
HP-classiﬁcation of amino acids, 85
HP-sequence of a protein, 85
SP-score of a multiple alignment, 17
α-helix, 76
β-sheet, 76
β-strand, 76
σ-additivity property of a measure, 152
σ-algebra generated by a family of
events, 149
σ-algebra of events, 148
a priori connectivity, 26
a.c., 166
a.c. convergence, 202
a.e., 175
absolutely continuous function, 176
absorbing state of a Markov chain, 30
accepted point mutation, 278
additive distance function, 97
admissible substitution matrix, 271
aﬃne gap model, 12
algebra generated by a family of events,
149
algebra of events, 148
alignment of sequences, 7
alignment score, 8
almost certain, 166
almost certain convergence, 202
almost everywhere, 175
alternative hypothesis, 138, 256
amino acid, 3
amino acid alphabet, 4
amino end of a protein, 4
background frequencies, 271
backward algorithm, 41
Baum-Welch training algorithm, 48
begin state of a Markov chain, 29
Bernoulli trial, 185
bias of an estimator, 236
biased estimator for parameter(s), 236
binomial distribution, 185
biological sequence, 3
BLAST, 16
block, 273
BLOSUMr matrix, 280
bond angle energy of a protein, 83
bond energy of a protein, 83
Borel sets in [0, 1], 151
Borel sets in R, 177
bounded random variable, 166
branch and bound algorithm, 95
burial/polarity classes, 80
burial/polarity environments, 80
Cantor set, 151
carboxy end of a protein, 4
Cartesian power of a probability space,
159
Cartesian power of a random variable,
193
Cartesian power of a sample space, 159
Cartesian product of a sequence of
probability spaces, 159
Cartesian product of probability spaces,
159
Cauchy-Bunyakowski inequality, 167
Central Limit theorem, 202

290
Index
Chebyshev’s inequality, 197
chi-square distribution, 191
CLUSTALW, 20
cluster of sequences, 280
codon, 4
complement of an event, 147
conditional probability, 42, 160
connectivity of a Markov chain, 26
consistent estimator, 236
consistent statistical test, 257
continuous random variable, 182
continuous vector-valued random
variable, 193
continuous-time ﬁnite Markov chain,
122
continuous-time ﬁnite Markov model,
122
convergence in distribution, 200
convergence in probability, 197
convergence in the mean, 199
convergence in the square mean, 199
cost function, 94
covariance, 195
cylinder event, 153, 159, 213
data simulation, 138
decoding, 36
delete state of a proﬁle HMM, 64
deletion, 7
density function of a random variable,
182
density function of a vector-valued
random variable, 193, 194
density of a random variable, 182
density of a vector-valued random
variable, 193, 194
diﬀerence between events, 147
dihedral angle energy of a protein, 83
discrete random variable, 163
discrete vector-valued random variable,
193
discrete-time ﬁnite Markov chain, 25
discrete-time ﬁnite Markov model, 25
distance between two points, 96
distance function, 96
distance matrix, 96
distribution function of a random
variable, 176
distribution function of a vector-valued
random variable, 192
distribution of a random variable, 184
DNA alphabet, 3
DNA orientation, 3
dot matrix, 14
dynamic programming algorithm, 9
edge eﬀect, 222
electrostatic energy of a protein, 84
elementary event, 146
emission probability of an HMM, 34
empty event, 147
end state of a Markov chain, 30
environmental descriptors, 80
environmental template method, 80
estimate for parameter(s), 236
event, 42, 146
evolutionary assumption, 131
evolutionary rate, 90, 127, 132
excursion of a path, 222
excursion of a trajectory, 213
exhausting sequence of subsets, 173
exon, 27
expected value of a random variable,
166
exponential distribution, 189
exponential of a matrix, 124
extreme value distribution, 211
FASTA, 14
Felsenstein model, 129
ﬁnite additivity property of a measure,
152
Fitch’s algorithm, 95
forward algorithm, 40
four-point condition, 98
fully connected Markov chain, 26
gamma distribution, 190
gamma function, 190
gap, 8
gap extension penalty, 12
gap opening penalty, 12
gap penalty, 8
gene tree, 91
genetic code, 4
GENSCAN, 37
geometric distribution, 186
geometric-like random variable, 187

Index
291
Glimmer, 32
global alignment, 9
hidden Markov model, 34
HKY model, 130
HMM, 34
HMM having no silent loops, 59
HMM with silent states, 57
HMMER, 68
homology of sequences, 7
hydrophilic amino acid, 85
hydrophobic amino acid, 85
iid random variables, 198
independent events, 161
independent identically distributed
random variables, 198
independent random variables, 192, 199
inﬁnite Cartesian power of a probability
space, 159
inﬁnite Cartesian power of a random
variable, 199
inﬁnite Cartesian power of a sample
space, 159
initialization probabilities of a Markov
chain, 26
insert column in a multiple alignment,
64
insert state of a proﬁle HMM, 64
insertion, 7
integrable random variable, 164
integral of a random variable, 164, 165
intersection of events, 147
interval estimation, 235
interval estimator, 236
intron, 27
Jukes-Cantor model, 127
Kimura model, 128
Kullback-Leibler inequality, 272
ladder point of a trajectory, 213
lattice HP-model, 85
leaf of a tree, 89
least squares method, 119
Lebesgue extension of a probability
measure, 157
Lebesgue integrable function, 173, 174
Lebesgue integral, 173, 174
Lebesgue measurable event, 157
Lebesgue measurable function, 173
Lebesgue measurable set, 173
Lebesgue measure on [0, 1], 172
Lebesgue measure on R, 172
level 1 BLOSUMr matrix, 282
level 2 BLOSUM r matrix, 282
likelihood given a statistical model, 242
likelihood of a dataset given a tree, 133
likelihood of training data, 33, 45
likelihood ratio, 138, 257
likelihood ratio test, 138, 257
linear gap model, 9
linear gap model for multiple align-
ments, 17
local alignment, 11
marginal distribution function, 192
marginal probability distribution, 193
Markov chain, 25, 122
Markov model, 25, 122
match column in a multiple alignment,
64
match model, 266
match state of a proﬁle HMM, 64
matrix of instantaneous change of a
Markov chain, 125
matrix of transition probabilities of a
Markov chain, 25
maximum likelihood equations, 244
maximum likelihood estimate, 245
maximum likelihood estimation, 242
maximum likelihood estimator, 243
mean of a random variable, 166
mean square error of an estimator, 237
mean value of a random variable, 166
measurable event, 158
measurable function, 173
messenger RNA, 5
metric space, 96
model for training data, 28, 35, 45
molecular clock condition, 113
molecular clock tree, 113
moment-generating function, 228
monotone non-decreasing function, 175
most parsimonious topology, 94
most probable path, 35
most probable state, 44

292
Index
mRNA, 5
MSA, 19
multiple alignment of sequences, 16
Needleman-Wunsch algorithm, 9
neighbor-joining algorithm, 98
nested hypotheses, 138, 256
non-measurable event, 158
non-silent length of a sequence, 58
non-silent state of an HMM, 57
non-trivially connected Markov chain,
30
normal distribution, 189
nucleic acid, 3
nucleotide, 3
null hypothesis, 138, 256
null hypothesis distribution of the
likelihood ratio, 258
occurrence of an event, 42, 147
open reading frame, 27
operational taxonomic unit, 89
optimal alignment of sequences, 8
optimal multiple alignment of sequences,
16
ORF, 27
orthologues, 91
OTU, 89
outer measure, 156
overlap between sequences, 220
PAMn matrix, 273
paralogues, 91
parameter estimation, 235
parameter space of a statistical model,
231
parameter(s) of a statistical model, 231,
235
parametric bootstrap method, 139, 265
path of a trajectory, 222
path through a Markov chain, 30, 34,
146
Pfam, 79
PHYLIP, 122
phylogenetic reconstruction, 92
phylogenetic tree, 90
point estimation, 235
point estimator for parameter(s), 235
Poisson distribution, 187
polar aminio acid, 85
posterior decoding, 44
posterior probability, 43, 44, 49
potential energy of a protein, 82
power of a statistical test, 257
probability, 42, 152
probability distribution of a random
variable, 181
probability distribution of a vector-
valued random variable, 193
probability measure, 42, 152
probability of a sequence, 27, 36
probability of a sequence along a path,
35
probability of committing a type I error,
257
probability of committing a type II
error, 257
probability space, 156, 158
probability with which a sequence arises
from a model, 29, 36
probability with which a sequence arises
from a model along a path, 36
product of events, 147
proﬁle HMM, 64
progressive alignment of sequences, 20
protein domain, 77
protein fold, 77
protein motif, 77
protein orientation, 4
protein primary structure, 76
protein quaternary structure, 78
protein secondary structure, 76
protein super-secondary structure, 77
protein tertiary structure, 77
protein-coding gene, 3
pseudodistance function, 106
pseudodistance matrix, 106
pulley principle, 134
random model, 266
random sample from a statistical model,
233
random variable, 162
random walk, 212
range of a statistical model, 231
reduced multiple alignment of
sequences, 93
regular Markov chain, 124

Index
293
regular statistical model, 242
Renewal theorem, 217
reversed Markov chain, 126
reversible Markov chain, 127
RNA alphabet, 5
RNA orientation, 5
RNA-coding gene, 5
root of a tree, 89
rooted tree, 89
sample from the inﬁnite Cartesian
power of a random variable, 199
sample space, 42, 145
scoring matrix, 9
search with a model, 28, 36
segment in a sequence, 11
semi-algebra of events, 150
Seq-Gen, 138
sequence similarity, 7
set estimation, 235
set of convergence, 203
signiﬁcance level of a statistical test,
257
signiﬁcance point of a statistical test,
258
signiﬁcant sequence similarity, 210
silent state of an HMM, 57
simple function, 163
simple random walk, 226
Smith-Waterman algorithm, 11
smooth statistical model, 260
standard deviation of a random
variable, 166
Star Alignment algorithm, 20
start codon, 4
state of a Markov chain, 25
stationary probability distribution of a
Markov chain, 125
statistical hypothesis test, 137, 256
statistical model, 231
statistical modeling, 233
step size of a random walk, 212
stop codon, 4
Strong Law of Large Numbers, 204
subset, 147
substitution, 7
substitution matrix, 9
sum of events, 147
sum of squares, 119
symmetric diﬀerence of events, 148
taxon, 89
termination probabilities of a Markov
chain, 30
test of signiﬁcance, 137, 256
test statistic, 259
threading, 79
time-reversible Markov chain, 127
tip of a tree, 89
training data, 28, 35, 45
training data agreeing with a connectiv-
ity, 29
trajectory of a random walk, 212
transcription, 5
transfer RNA, 5
transition, 129
transition probability of a Markov
chain, 25
translation, 5
transversion, 129
tree generating a distance function, 97
tree relating OTUs, 90
tree topology, 90
tree-generated distance function, 96
triangle inequality, 96, 167
tRNA, 5
two-dimensional lattice HP-model, 87
type I error, 257
type II error, 257
ultrameric distance function, 110
unbiased estimator for parameter(s),
236
underlying Markov chain of an HMM,
34
uniform convergence, 164
uniform distribution, 185, 188
union of events, 147
unrooted tree, 89
UPGMA algorithm, 113
Van der Waals energy of a protein, 84
variance of a random variable, 166
variance-covariance matrix of a random
variable, 195
vector of initialization probabilities, 26
vector-valued random variable, 191
Viterbi algorithm, 38

294
Index
Viterbi path, 38
Viterbi training algorithm, 52
Wald’s identity, 217
Weak Law of Large Numbers, 198
weighted sum of squares, 122

Universitext
Aguilar, M.; Gitler, S.; Prieto, C.: Algebraic
Topology from a Homotopical Viewpoint
Aksoy, A.; Khamsi, M. A.: Methods in Fixed
Point Theory
Alevras, D.; Padberg M. W.: Linear Opti-
mization and Extensions
Andersson, M.: Topics in Complex Analysis
Aoki, M.: State Space Modeling of Time
Series
Arnold, V. I.: Lectures on Partial Differen-
tial Equations
Arnold, V. I.; Cooke, R.: Ordinary Differen-
tial Equations
Audin, M.: Geometry
Aupetit, B.: A Primer on Spectral Theory
Bachem, A.; Kern, W.: Linear Programming
Duality
Bachmann, G.; Narici, L.; Beckenstein, E.:
Fourier and Wavelet Analysis
Badescu, L.: Algebraic Surfaces
Balakrishnan, R.; Ranganathan, K.: A Text-
book of Graph Theory
Balser, W.: Formal Power Series and Linear
Systems of Meromorphic Ordinary Differ-
ential Equations
Bapat, R.B.:
Linear Algebra and Linear
Models
Benedetti, R.; Petronio, C.: Lectures on Hy-
perbolic Geometry
Benth, F. E.: Option Theory with Stochastic
Analysis
Berberian, S. K.:
Fundamentals of Real
Analysis
Berger, M.: Geometry I, and II
Bliedtner, J.; Hansen, W.: Potential Theory
Blowey, J. F.; Coleman, J. P.; Craig, A. W.
(Eds.): Theory and Numerics of Differential
Equations
Blyth, T. S.: Lattices and Ordered Algebraic
Structures
B¨orger, E.; Gr¨adel, E.; Gurevich, Y.: The
Classical Decision Problem
B¨ottcher, A; Silbermann, B.: Introduction
to Large Truncated Toeplitz Matrices
Boltyanski, V.; Martini, H.; Soltan, P. S.:
Excursions into Combinatorial Geometry
Boltyanskii, V. G.; Efremovich, V. A.: Intu-
itive Combinatorial Topology
Bonnans, J. F.; Gilbert, J. C.; Lemarchal, C.;
Sagastizbal, C. A.: Numerical Optimization
Booss, B.; Bleecker, D. D.: Topology and
Analysis
Borkar, V. S.: Probability Theory
Brunt B. van: The Calculus of Variations
Carleson, L.; Gamelin, T. W.:
Complex
Dynamics
Cecil, T. E.:
Lie Sphere Geometry: With
Applications of Submanifolds
Chae, S. B.: Lebesgue Integration
Chandrasekharan, K.:
Classical Fourier
Transform
Charlap, L. S.: Bieberbach Groups and Flat
Manifolds
Chern, S.:
Complex Manifolds without
Potential Theory
Chorin, A. J.; Marsden, J. E.: Mathematical
Introduction to Fluid Mechanics
Cohn, H.: A Classical Invitation to Alge-
braic Numbers and Class Fields
Curtis, M. L.: Abstract Linear Algebra
Curtis, M. L.: Matrix Groups
Cyganowski, S.; Kloeden, P.; Ombach, J.:
From Elementary Probability to Stochastic
Differential Equations with MAPLE
Da Prato, G.: An Introduction to Inﬁnite
Dimensional Analysis
Dalen, D. van: Logic and Structure
Das, A.: The Special Theory of Relativity:
A Mathematical Exposition
Debarre, O.: Higher-Dimensional Algebraic
Geometry
Deitmar, A.: A First Course in Harmonic
Analysis

Demazure, M.:
Bifurcations and Cata-
strophes
Devlin, K. J.: Fundamentals of Contempo-
rary Set Theory
DiBenedetto,
E.:
Degenerate Parabolic
Equations
Diener, F.; Diener, M.(Eds.): Nonstandard
Analysis in Practice
Dimca, A.: Sheaves in Topology
Dimca, A.: Singularities and Topology of
Hypersurfaces
DoCarmo, M. P.:
Differential Forms and
Applications
Duistermaat, J. J.; Kolk, J. A. C.: Lie Groups
Dumortier.: Qualitative Theory of Planar
Differential Systems
Edwards, R. E.: A Formal Background to
Higher Mathematics Ia, and Ib
Edwards, R. E.: A Formal Background to
Higher Mathematics IIa, and IIb
Emery, M.: Stochastic Calculus in Mani-
folds
Endler, O.: Valuation Theory
Engel, K.-J.; Nagel, R.: A Short Course on
Operator Semigroups
Erez, B.: Galois Modules in Arithmetic
Everest, G.; Ward, T.: Heights of Polynomi-
als and Entropy in Algebraic Dynamics
Farenick, D. R.: Algebras of Linear Trans-
formations
Foulds, L. R.: Graph Theory Applications
Franke, J.; Hrdle, W.; Hafner, C. M.: Statis-
tics of Financial Markets: An Introduction
Frauenthal, J. C.: Mathematical Modeling in
Epidemiology
Friedman, R.: Algebraic Surfaces and Holo-
morphic Vector Bundles
Fuks, D. B.;
Rokhlin, V. A.:
Beginner’s
Course in Topology
Fuhrmann, P. A.: A Polynomial Approach
to Linear Algebra
Gallot, S.; Hulin, D.; Lafontaine, J.: Rie-
mannian Geometry
Gardiner, C. F.: A First Course in Group
Theory
G˚arding, L.; Tambour, T.: Algebra for Com-
puter Science
Godbillon,
C.:
Dynamical Systems on
Surfaces
Godement, R.: Analysis I, and II
Goldblatt, R.: Orthogonality and Spacetime
Geometry
Gouvˆea, F. Q.: p-Adic Numbers
Gross, M. et al.: Calabi-Yau Manifolds and
Related Geometries
Gustafson, K. E.; Rao, D. K. M.: Numerical
Range. The Field of Values of Linear Oper-
ators and Matrices
Gustafson, S. J.; Sigal, I. M.: Mathematical
Concepts of Quantum Mechanics
Hahn, A. J.:
Quadratic Algebras, Clifford
Algebras, and Arithmetic Witt Groups
H´ajek, P.; Havr´anek, T.: Mechanizing Hy-
pothesis Formation
Heinonen, J.: Lectures on Analysis on Met-
ric Spaces
Hlawka, E.; Schoißengeier, J.; Taschner, R.:
Geometric and Analytic Number Theory
Holmgren, R. A.: A First Course in Discrete
Dynamical Systems
Howe, R., Tan, E. Ch.: Non-Abelian Har-
monic Analysis
Howes, N. R.: Modern Analysis and Topol-
ogy
Hsieh, P.-F.; Sibuya, Y. (Eds.): Basic Theory
of Ordinary Differential Equations
Humi, M., Miller, W.: Second Course in Or-
dinary Differential Equations for Scientists
and Engineers
Hurwitz, A.; Kritikos, N.: Lectures on Num-
ber Theory
Huybrechts, D.: Complex Geometry: An In-
troduction
Isaev, A.:
Introduction to Mathematical
Methods in Bioinformatics
Istas, J.: Mathematical Modeling for the Life
Sciences
Iversen, B.: Cohomology of Sheaves
Jacod, J.; Protter, P.: Probability Essentials

Jennings, G. A.:
Modern Geometry with
Applications
Jones, A.; Morris, S. A.; Pearson, K. R.: Ab-
stract Algebra and Famous Inpossibilities
Jost, J.: Compact Riemann Surfaces
Jost, J.: Dynamical Systems. Examples of
Complex Behaviour
Jost, J.: Postmodern Analysis
Jost, J.: Riemannian Geometry and Geomet-
ric Analysis
Kac, V.; Cheung, P.: Quantum Calculus
Kannan, R.;
Krueger, C. K.:
Advanced
Analysis on the Real Line
Kelly, P.; Matthews, G.: The Non-Euclidean
Hyperbolic Plane
Kempf, G.: Complex Abelian Varieties and
Theta Functions
Kitchens, B. P.: Symbolic Dynamics
Kloeden, P.; Ombach, J.; Cyganowski, S.:
From Elementary Probability to Stochastic
Differential Equations with MAPLE
Kloeden, P. E.; Platen; E.; Schurz, H.: Nu-
merical Solution of SDE Through Computer
Experiments
Kostrikin, A. I.: Introduction to Algebra
Krasnoselskii, M. A.; Pokrovskii, A. V.: Sys-
tems with Hysteresis
Kurzweil, H.; Stellmacher, B.: The Theory of
Finite Groups. An Introduction
Lang, S.:
Introduction to Differentiable
Manifolds
Luecking, D. H., Rubel, L. A.:
Complex
Analysis. A Functional Analysis Approach
Ma, Zhi-Ming; Roeckner, M.: Introduction
to the Theory of (non-symmetric) Dirichlet
Forms
Mac Lane, S.; Moerdijk, I.:
Sheaves in
Geometry and Logic
Marcus, D. A.: Number Fields
Martinez, A.: An Introduction to Semiclas-
sical and Microlocal Analysis
Matouˇsek, J.: Using the Borsuk-Ulam The-
orem
Matsuki, K.: Introduction to the Mori Pro-
gram
Mazzola, G.; Milmeister G.; Weissman J.:
Comprehensive Mathematics for Computer
Scientists 1
Mazzola, G.; Milmeister G.; Weissman J.:
Comprehensive Mathematics for Computer
Scientists 2
Mc Carthy, P. J.: Introduction to Arithmeti-
cal Functions
McCrimmon, K.: A Taste of Jordan Alge-
bras
Meyer, R. M.:
Essential Mathematics for
Applied Field
Meyer-Nieberg, P.: Banach Lattices
Mikosch, T.:
Non-Life Insurance Mathe-
matics
Mines, R.; Richman, F.; Ruitenburg, W.: A
Course in Constructive Algebra
Moise, E. E.: Introductory Problem Courses
in Analysis and Topology
Montesinos-Amilibia, J. M.: Classical Tes-
sellations and Three Manifolds
Morris, P.: Introduction to Game Theory
Nikulin, V. V.; Shafarevich, I. R.: Geome-
tries and Groups
Oden, J. J.; Reddy, J. N.: Variational Meth-
ods in Theoretical Mechanics
Øksendal, B.: Stochastic Differential Equa-
tions
Øksendal, B.; Sulem, A.: Applied Stochastic
Control of Jump Diffusions
Poizat, B.: A Course in Model Theory
Polster, B.: A Geometrical Picture Book
Porter, J. R.; Woods, R. G.: Extensions and
Absolutes of Hausdorff Spaces
Radjavi, H.; Rosenthal, P.: Simultaneous
Triangularization
Ramsay, A.; Richtmeyer, R. D.: Introduc-
tion to Hyperbolic Geometry
Rautenberg, W.: A concise Introduction to
Mathematical Logic
Rees, E. G.: Notes on Geometry
Reisel, R. B.: Elementary Theory of Metric
Spaces

Rey, W. J. J.: Introduction to Robust and
Quasi-Robust Statistical Methods
Ribenboim, P.: Classical Theory of Alge-
braic Numbers
Rickart, C. E.: Natural Function Algebras
Rotman, J. J.: Galois Theory
Rubel, L. A.:
Entire and Meromorphic
Functions
Ruiz-Tolosa, J. R.; Castillo E.: From Vectors
to Tensors
Runde, V.: A Taste of Topology
Rybakowski, K. P.: The Homotopy Index
and Partial Differential Equations
Sagan, H.: Space-Filling Curves
Samelson, H.: Notes on Lie Algebras
Sauvigny, F.:
Partial Differential Equa-
tions I
Sauvigny, F.:
Partial Differential Equa-
tions II
Schiff, J. L.: Normal Families
Sengupta, J. K.: Optimal Decisions under
Uncertainty
S´eroul, R.: Programming for Mathemati-
cians
Seydel, R.:
Tools for Computational Fi-
nance
Shafarevich, I. R.: Discourses on Algebra
Shapiro, J. H.: Composition Operators and
Classical Function Theory
Simonnet, M.: Measures and Probabilities
Smith, K. E.; Kahanp¨a¨a, L.; Kek¨al¨ainen, P.;
Traves, W.:
An Invitation to Algebraic
Geometry
Smith, K. T.: Power Series from a Computa-
tional Point of View
Smory´nski, C.: Logical Number Theory I.
An Introduction
Stichtenoth, H.: Algebraic Function Fields
and Codes
Stillwell, J.: Geometry of Surfaces
Stroock, D. W.: An Introduction to the The-
ory of Large Deviations
Sunder, V. S.: An Invitation to von Neu-
mann Algebras
Tamme, G.: Introduction to ´Etale Coho-
mology
Tondeur, P.:
Foliations on Riemannian
Manifolds
Toth, G.: Finite Mbius Groups, Minimal Im-
mersions of Spheres, and Moduli
Verhulst, F.: Nonlinear Differential Equa-
tions and Dynamical Systems
Wong, M. W.: Weyl Transforms
Xamb´o-Descamps, S.:
Block Error-Cor-
recting Codes
Zaanen, A.C.: Continuity, Integration and
Fourier Theory
Zhang, F.: Matrix Theory
Zong, C.: Sphere Packings
Zong, C.: Strange Phenomena in Convex
and Discrete Geometry
Zorich, V. A.: Mathematical Analysis I
Zorich, V. A.: Mathematical Analysis II


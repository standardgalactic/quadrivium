
The Dynamic Programming Manual
Mastering Efficient Solutions
Gábor László Hajba
This book is for sale at http://leanpub.com/thedynamicprogrammingmanual
This version was published on 2023-06-30
This is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing
process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and
many iterations to get reader feedback, pivot until you have the right book and build traction once
you do.
© 2023 Gábor László Hajba


Contents
Preface: Unleash the Power of Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . .
i
Part I: Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
What is Dynamic Programming? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
Fundamental Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
Advanced Dynamic Programming Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
Dynamic Programming in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
Advanced Topics in Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
Tips and Tricks for Efficient Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . .
42
Conclusion and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
Part II: Dive into Dynamic Programming Chal-
lenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
Fibonacci Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
Fibonacci with Memoization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Last Digit of Fibonacci Number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
Longest Subsequence with Equal Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
Longest Subarray with Equal 0s and 1s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
Longest Common Subsequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
Minimum Subset Sum Difference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
Valid Parentheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75

CONTENTS
Minimum Number of Parentheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
Find the Element that Appears Once . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
Find the Missing Number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
Count Set Bits (Brian Kernighan’s Algorithm) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
Generate All Subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
Power of Two . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
Scalar Multiplication Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
Topological Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
Breadth-First Search (BFS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
Depth-First Search (DFS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
Symmetric Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
Lowest Common Ancestor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
Trim a Binary Search Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
Find Closest Value in BST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
Binary Tree Level Order Traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Climbing Stairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Best Time to Buy and Sell Stock
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
House Robber . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
Coin Change . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
Coin Change 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
Knapsack Problem (Bottom-Up)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
0/1 Knapsack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
Knapsack with Duplicate Items . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
Floyd-Warshall Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Hamiltonian Path and Circuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

N-Queens Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
Sudoku Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
Recover Binary Search Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
Edit Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
Dungeon Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
Cherry Pickup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
Travelling Salesman Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
Part III: Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .199
Proof of Optimal Substructure Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
Derivation of Memoization and Tabulation Techniques . . . . . . . . . . . . . . . . . . . . . . 202
Problem Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
Selecting the Right Data Structures for Dynamic Programming Efficiency . . . . . . . . . . 208

Preface: Unleash the Power of
Dynamic Programming
Why This Book?
Welcome to the world of Dynamic Programming! As an avid problem solver and programmer, I have
encountered numerous challenges where dynamic programming proved to be the most effective
solution. These problems appeared during my journey of expanding my knowledge and testing
myself with algorithmic puzzles. Surprisingly, they even crept into my day-to-day work.
Who Should Care?
Are you passionate about algorithms? Do you want to conquer challenges that become sluggish
with increasing input using brute force techniques? Whether you code in Java or Python, this book
will spark your creativity and provide valuable insights to enhance your code. Even if you work
with a different programming language, fear not! The principles of programming are universal, and
the examples in this book will help you grasp the essence of dynamic programming easily.
Bridging the Gap
I believe that there are like-minded individuals out there who yearn to tackle dynamic programming
problems but lack the proper guidance. This book aims to bridge that gap, providing a comprehensive
understanding of the topic and empowering you to approach dynamic programming problems with
confidence.
A Manual for Hands-On Learning
Prepare yourself for a hands-on experience! The emphasis of this book is on practical problem-
solving. While the background knowledge is essential, I want you to dive into the world of dynamic
programming by reading and analyzing example code. Throughout the book, you will discover
different solutions to the same problem, empowering you to adapt and find elegant solutions to
various programming challenges.
i

Preface: Unleash the Power of Dynamic Programming
ii
Your Input Matters
This book is a labor of love and a result of self-publishing. With no hard deadlines, I am continually
adding problems and their solutions, crafting a valuable resource for you. If you encounter an
interesting problem and wish to explore my solution, I encourage you to reach out through the
book’s LeanPub web page. Let me know which problem you would like to see, and I will do my
best to incorporate it into the book promptly. As a purchaser, you will receive all updates and new
versions—your support matters!
However, I must mention that I am not a mathematician or a theoretical problem-solving expert.
There may be instances where I cannot solve your problem or it may require significant time to find
a solution.
Prerequisites: Let’s Get Started!
To fully grasp the concepts presented in this book, it is essential to have a basic understanding of
programming. I assume you are familiar with compiling and running code, and I do not impose
the use of a specific Integrated Development Environment (IDE). Feel free to work within the
environment of your choice.
Ask, and You Shall Receive
Throughout your journey with this book, if anything seems unclear or you encounter any confusion,
I encourage you to ask questions. Do not hesitate to reach out through LeanPub’s book discussion,
email, or social media channels. Your questions are essential, and seeking clarification ensures that
no part of your understanding is left incomplete.
Remember the wise words of one of my professors:
“Asking is not a shame. Not asking is a shame.”
A Compact and Informative Book
This book is intentionally designed to be concise yet packed with valuable information. Its length is
optimized to provide a price-value correspondence that exceeds your expectations. I hope you will
find immense value in the knowledge shared within these pages.

Preface: Unleash the Power of Dynamic Programming
iii
Book Structure: A Path to Mastery
“The Dynamic Programming Manual” is your ultimate guide to mastering the art of dynamic
programming. This comprehensive book is divided into two parts, designed to provide you with
a complete understanding of the theory and practical implementation of dynamic programming.
In the first part, you will embark on a journey through the theoretical foundation of dynamic
programming. Starting with an introduction to Dynamic Programming, you will explore the history,
evolution, and applications of this powerful problem-solving technique. Gain insights into optimal
substructure, overlapping subproblems, and the memoization and tabulation techniques that form
the backbone of dynamic programming. Dive deep into time and space complexity analysis to
understand the efficiency of your solutions. Discover advanced techniques such as state space
reduction, bitmasking, bitwise operations, divide and conquer, and multidimensional dynamic
programming. With each chapter, you will expand your knowledge and build a solid foundation for
solving complex problems.
In the second part, you will put your theoretical knowledge into practice.
Explore real-world
scenarios where dynamic programming shines, such as computer vision, natural language process-
ing, bioinformatics, financial modeling, and game theory. Learn how dynamic programming is
leveraged in these domains and gain insights into the unique challenges and approaches for each
application.
To further enhance your learning experience, the book provides a wealth of practical examples,
problem statements, and step-by-step solutions. You will find clear explanations, pseudocode, and
code examples in Java and Python, making it easier for you to grasp the concepts and implement your
own solutions. Whether you are a beginner seeking a comprehensive introduction or an experienced
programmer looking to sharpen your skills, “The Dynamic Programming Manual” has something
to offer.
With its detailed explanations, comprehensive coverage, and practical examples, this book serves as
your go-to resource for mastering dynamic programming. Whether you are preparing for coding
interviews, tackling challenging algorithmic problems, or seeking to optimize your code, this book
will equip you with the tools and techniques to tackle any dynamic programming challenge.
Get ready to unlock the full potential of dynamic programming and embark on a journey of problem-
solving mastery. Let “The Dynamic Programming Manual” be your companion in your quest for
efficient, elegant, and optimized solutions.
Why LeanPub?
I chose to publish this book on LeanPub for several reasons:
• Rapid Distribution: LeanPub enables me to distribute new parts and updates swiftly, ensuring
you have the latest content at your fingertips.

Preface: Unleash the Power of Dynamic Programming
iv
• Eco-Friendly Approach: By offering only digital versions, we contribute to the preservation
of trees and reduce our environmental impact.
• Affordability: LeanPub’s digital format allows for cost-effective distribution, providing you
with a valuable resource at an affordable price. Plus, all updates and corrections are included!
Dynamic programming encompasses a vast array of problems, and I am committed to continually
adding new chapters to address different scenarios. My goal is to keep your brain engaged and
motivated, fostering your growth as a problem solver.
Distinguishing Features
You may wonder why I chose to write this book when numerous publications cover the same topic.
Allow me to highlight the unique aspects of “The Dynamic Programming Manual.” Traditional
publishers often limit your access to new versions, leaving you with outdated print books or
requiring you to purchase updated editions. In contrast, LeanPub ensures that you receive every
update and correction seamlessly, providing you with an exceptional learning experience.
In addition, while writing another book with a traditional publisher, Apress, I gained valuable
insights into the technical book production process. It can be rigorous, with strict deadlines and
numerous review phases. Yet, even with these efforts, updates and new editions remain a challenge.
I aim to deliver a different experience—one that surpasses the limitations of traditional publishing
and grants you a deep introduction to Dynamic Programming, covering both beginner and advanced
topics with practical examples.
Conclusion
Thank you for joining me on this exciting journey into the world of dynamic programming. As you
explore the book’s chapters, filled with engaging examples and insightful explanations, I hope you
will find inspiration and gain the knowledge and skills needed to excel in problem solving.
Remember, your questions and feedback are invaluable.
Together, let’s unleash the power of
dynamic programming and embark on a transformational learning adventure!
Happy coding,
Gábor László Hajba

Part I: Theory
Welcome to the gateway of knowledge, where we unlock the theoretical foundations of dynamic
programming. In this captivating section, we delve into the core concepts and principles that lay
the groundwork for solving complex problems.
Unveiling the Hidden Patterns Prepare yourself for an exhilarating exploration into the hidden
patterns and structures that lie beneath dynamic programming problems. We unravel the enigmatic
nature of optimal substructure and overlapping subproblems, shedding light on their pivotal role in
the dynamic programming paradigm. By understanding these fundamental concepts, you will gain
a fresh perspective on problem-solving and embark on a transformative journey through the world
of algorithms.
Harnessing the Power of Memory Witness the alchemical fusion of technique and efficiency as
we uncover the secrets of memoization and tabulation. These remarkable techniques unlock the
true potential of dynamic programming, empowering you to store and retrieve valuable solutions
to subproblems.
Marvel at the elegance of memoization, where the past becomes a treasure
trove of knowledge, and embrace the structured allure of tabulation, where tables hold the key to
optimization. Armed with these techniques, you will conquer complexity and elevate your problem-
solving prowess to new heights.
Navigating Time and Space Embark on a voyage through the intricacies of time and space
complexity analysis. Dive deep into the intricately woven fabric of computational efficiency as we
analyze the performance characteristics of dynamic programming algorithms. Explore the trade-
offs between time and space, unravel the mysteries of polynomial and exponential time complexity,
and chart a course towards optimal solutions. Armed with this knowledge, you will navigate the
vast sea of algorithms with confidence and precision.
Embracing the Code Experience the thrill of discovery as we demonstrate the practical application
of dynamic programming theory through captivating code examples. Immerse yourself in the world
of Java and Python as we bring the theoretical concepts to life, showcasing their potency and
versatility. Witness the intricate dance of logic and syntax, and unlock the door to elegant and
efficient solutions.
By delving into the theoretical underpinnings of dynamic programming, you will acquire the
essential tools and insights necessary to tackle the challenges that lie ahead. Prepare to be captivated
by the intricate tapestry of theory, as we equip you with the knowledge and understanding to
conquer the dynamic programming landscape. Let the journey begin!

What is Dynamic Programming?
Dynamic Programming is a powerful technique used in computer science and mathematics to solve
complex problems by breaking them down into simpler, overlapping subproblems. It provides an
efficient approach to find optimal solutions by storing and reusing solutions to subproblems, rather
than recomputing them repeatedly.
In essence, dynamic programming enables us to solve problems more efficiently by breaking them
into smaller, manageable parts and building up solutions incrementally. By leveraging the principle
of optimal substructure, where an optimal solution to a problem contains optimal solutions to its
subproblems, dynamic programming offers a systematic way to solve a wide range of problems.
History and Evolution of Dynamic Programming
The origins of dynamic programming can be traced back to the mid-20th century.
The term
“dynamic programming” was coined by Richard Bellman in the 1950s while working on mathe-
matical optimization problems. However, the concept itself predates the term, with earlier works
by mathematicians like Euler and Hamilton addressing similar ideas.
Initially, dynamic programming found its application in the fields of operations research and
optimization, where it provided solutions to problems involving resource allocation, scheduling,
and network flow. Over time, its applicability expanded to various domains, including computer
science, algorithms, artificial intelligence, bioinformatics, and economics.
Applications of Dynamic Programming
Dynamic programming has proven to be a versatile and powerful technique, finding applications in
a wide range of problem domains. Some notable applications include:
• Algorithm Design: Dynamic programming is frequently used to solve algorithmic problems
efficiently. It provides solutions to challenges such as shortest paths, sequence alignment, graph
traversal, knapsack problems, and many more.
• Optimization: Dynamic programming plays a crucial role in optimizing resource allocation,
scheduling tasks, inventory management, and production planning. It allows for efficient
utilization of resources, minimizing costs, and maximizing performance.
• Bioinformatics: In the field of genetics and genomics, dynamic programming is used for
sequence alignment, DNA and protein folding, and analyzing genomic data.
It aids in
understanding evolutionary relationships and identifying functional elements within genetic
sequences.
2

What is Dynamic Programming?
3
• Game Theory: Dynamic programming finds applications in game theory for solving games
with optimal strategies, such as chess, checkers, and other board games. It enables intelligent
decision-making and evaluating the best course of action at each step.
These are just a few examples of the extensive range of domains where dynamic programming has
made a significant impact. Its versatility and effectiveness in solving complex problems make it a
valuable tool for problem solvers across various disciplines.
Advantages and Limitations of Dynamic Programming
Dynamic programming offers several advantages that make it a preferred approach for solving
problems:
• Efficiency: By breaking down problems into smaller subproblems and reusing computed
solutions, dynamic programming significantly reduces computation time. It allows for solving
problems that would otherwise be computationally infeasible.
• Optimality: Dynamic programming guarantees finding optimal solutions by leveraging the
principle of optimal substructure. It ensures that the solution to a problem incorporates optimal
solutions to its subproblems, leading to an optimal overall solution.
• Simplicity: Although dynamic programming may initially seem complex, it provides a struc-
tured and systematic approach to problem-solving. Breaking down problems into subproblems
simplifies the solution process and enhances code modularity.
However, dynamic programming also has its limitations:
• Overlapping Subproblems: Not all problems exhibit overlapping subproblems, rendering
the dynamic programming approach less useful in those cases. Identifying the presence of
overlapping subproblems is crucial to determine the applicability of dynamic programming.
• Memory Usage: Dynamic programming algorithms often require additional memory to store
computed solutions for reuse. For problems with large input sizes, this can become a significant
constraint.
• Dependency on Problem Structure: The effectiveness of dynamic programming heavily relies
on the problem’s inherent structure and the ability to break it down into overlapping subproblems.
Some problems may not lend themselves well to dynamic programming techniques.
Overview of the Book
“The Dynamic Programming Manual” serves as a comprehensive guide to understanding and
applying dynamic programming techniques.
Throughout this book, we will explore various

What is Dynamic Programming?
4
dynamic programming problems, providing detailed explanations and implementations in Java and
Python.
The chapters in this book are structured as follows:
1. Introduction to Dynamic Programming: This chapter introduces the concept of dynamic
programming, its history, applications, advantages, and limitations.
2. Problem 1: Each subsequent chapter focuses on a specific problem. We will present the
problem statement, explain the approach and solution, and provide implementations in both
Java and Python.
3. Problem 2
4. Problem 3
…
By working through these chapters, you will gain a solid understanding of dynamic programming
principles and enhance your problem-solving skills. Whether you are a seasoned programmer or
new to the world of dynamic programming, this book will provide valuable insights and practical
examples to strengthen your abilities.
Now, let’s dive into the world of dynamic programming and embark on an exciting journey of
problem solving and optimization!

Fundamental Concepts
In this chapter, we will explore the fundamental concepts that form the basis of dynamic pro-
gramming. Understanding these concepts is crucial to effectively solve problems using dynamic
programming techniques.
Optimal Substructure
Optimal substructure is a fundamental concept in dynamic programming that allows us to break
down complex problems into smaller, more manageable subproblems. It states that an optimal
solution to a larger problem can be constructed from optimal solutions to its smaller subproblems.
To illustrate the concept of optimal substructure, let’s explore a few examples implemented in both
Python and Java.
Example 1: Fibonacci Sequence
The Fibonacci sequence is a classic example that demonstrates optimal substructure. Each number
in the sequence is the sum of the two preceding numbers: 0, 1, 1, 2, 3, 5, 8, 13, and so on. We can
define the Fibonacci sequence recursively as follows:
def fibonacci(n):
if n <= 1:
return n
else:
return fibonacci(n-1) + fibonacci(n-2)
public class Fibonacci {
public static int fibonacci(int n) {
if (n <= 1) {
return n;
} else {
return fibonacci(n-1) + fibonacci(n-2);
}
}
}
5

Fundamental Concepts
6
In this example, the optimal substructure is evident. The Fibonacci number at index n is computed by
adding the Fibonacci numbers at indices n-1 and n-2. By breaking down the problem into smaller
subproblems (calculating the Fibonacci numbers at lower indices), we can obtain the solution for
larger indices.
However, this naive recursive implementation has exponential time complexity, as it recomputes
the same Fibonacci numbers multiple times. To optimize it using dynamic programming, we can
apply memoization or tabulation techniques to store the previously computed Fibonacci numbers
and avoid redundant calculations.
Example 2: Shortest Path in a Graph
Consider the problem of finding the shortest path in a graph from a source vertex to a destination
vertex. This is a well-known problem in graph theory and can be efficiently solved using dynamic
programming.
In Python, we can represent the graph as an adjacency matrix and use the Floyd-Warshall algorithm
to find the shortest path between all pairs of vertices:
def floyd_warshall(graph):
n = len(graph)
dist = [[float('inf') for _ in range(n)] for _ in range(n)]
for i in range(n):
for j in range(n):
dist[i][j] = graph[i][j]
for k in range(n):
for i in range(n):
for j in range(n):
dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])
return dist
In Java, we can implement the Floyd-Warshall algorithm similarly:

Fundamental Concepts
7
public class FloydWarshall {
public static int[][] floydWarshall(int[][] graph) {
int n = graph.length;
int[][] dist = new int[n][n];
for (int i = 0; i < n; i++) {
System.arraycopy(graph[i], 0, dist[i], 0, n);
}
for (int k = 0; k < n; k++) {
for (int i = 0; i < n; i++) {
for (int j = 0; j < n; j++) {
dist[i][j] = Math.min(dist[i][j], dist[i][k] + dist[k][j]);
}
}
}
return dist;
}
}
In this example, the optimal substructure is evident in the way the shortest path between pairs
of vertices is calculated. The algorithm iteratively considers intermediate vertices and determines
the shortest path based on the optimal solutions to subproblems. By building on these optimal
substructures, we can efficiently find the shortest path in a graph.
Summary
By examining these examples, we can observe how optimal substructure is utilized in dynamic
programming. It allows us to decompose complex problems into smaller, solvable subproblems and
construct the optimal solution iteratively. By avoiding redundant computations through techniques
like memoization and tabulation, we can improve the efficiency of our dynamic programming
solutions.
Optimal substructure forms the foundation for developing effective and optimized
algorithms in both Python and Java.
Overlapping Subproblems
Overlapping subproblems is another key concept in dynamic programming.
It refers to the
property of a problem where the set of subproblems required to solve the problem overlap or are
reused multiple times. This overlap allows us to optimize the computation by avoiding redundant
calculations and storing the results of subproblems for future use.

Fundamental Concepts
8
Let’s delve into a couple of examples implemented in Python and Java to better understand the
concept of overlapping subproblems.
Example 1: Fibonacci Sequence (Optimized)
In the previous example of the Fibonacci sequence, we saw how the naive recursive implementation
suffers from redundant calculations. To overcome this issue and take advantage of overlapping
subproblems, we can employ memoization, which involves caching the results of already computed
subproblems.
def fibonacci(n, memo={}):
if n <= 1:
return n
if n not in memo:
memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)
return memo[n]
import java.util.HashMap;
import java.util.Map;
public class Fibonacci {
public static int fibonacci(int n) {
Map<Integer, Integer> memo = new HashMap<>();
return fibonacciHelper(n, memo);
}
private static int fibonacciHelper(int n, Map<Integer, Integer> memo) {
if (n <= 1) {
return n;
}
if (!memo.containsKey(n)) {
memo.put(n, fibonacciHelper(n-1, memo) + fibonacciHelper(n-2, memo));
}
return memo.get(n);
}
}

Fundamental Concepts
9
In these optimized implementations, we introduce a memoization table (memo) to store the computed
Fibonacci numbers. Before calculating the Fibonacci number for a particular index, we check if it
exists in the memoization table. If it does, we directly retrieve the result from the table, avoiding
redundant recursive calls and significantly improving the efficiency of the algorithm.
Example 2: Binomial Coefficient
The binomial coefficient, often represented as “n choose k” (nCk), represents the number of ways to
choose k items from a set of n items. It can be computed using the formula:
C(n, k) = C(n-1, k-1) + C(n-1, k)
We can implement this computation using dynamic programming and exploiting the overlapping
subproblems property.
def binomialCoefficient(n, k):
table = [[0] * (k+1) for _ in range(n+1)]
for i in range(n+1):
for j in range(min(i, k)+1):
if j == 0 or j == i:
table[i][j] = 1
else:
table[i][j] = table[i-1][j-1] + table[i-1][j]
return table[n][k]
public class BinomialCoefficient {
public static int binomialCoefficient(int n, int k) {
int[][] table = new int[n+1][k+1];
for (int i = 0; i <= n; i++) {
for (int j = 0; j <= Math.min(i, k); j++) {
if (j == 0 || j == i) {
table[i][j] = 1;
} else {
table[i][j] = table[i-1][j-1] + table[i-1][j];
}
}
}

Fundamental Concepts
10
return table[n][k];
}
}
In these implementations, we use a 2D table (table) to store the intermediate results of binomial
coefficients.
By iteratively filling the table from smaller subproblems to larger ones, we can efficiently compute
the binomial coefficient for any given n and k. The table ensures that overlapping subproblems are
solved only once, and their results are reused when needed.
These examples demonstrate how overlapping subproblems are identified and leveraged in dynamic
programming. By avoiding redundant computations through memoization or tabulation techniques,
we can significantly improve the efficiency of our algorithms and solve complex problems efficiently.
Summary
In this section, we explored the concept of overlapping subproblems in dynamic programming. We
learned that overlapping subproblems occur when the same subproblems are encountered multiple
times in the computation of a problem’s solution. By leveraging this property, we can optimize our
algorithms by storing and reusing the results of already solved subproblems. Through examples
implemented in both Python and Java, we witnessed how memoization and tabulation techniques
can be used to address overlapping subproblems, resulting in more efficient and optimized solutions.
Understanding and identifying overlapping subproblems is crucial for designing effective dynamic
programming algorithms that can handle complex computational tasks with improved efficiency.
Memoization and Tabulation Techniques
Memoization and tabulation are two common techniques used in dynamic programming to optimize
the computation of solutions by storing and reusing the results of subproblems. These techniques
help avoid redundant calculations and improve the overall efficiency of dynamic programming
algorithms. Let’s explore each technique in detail and provide examples in both Python and Java.
Memoization Technique
Memoization involves caching the results of already computed subproblems in a memoization table
or cache. When a subproblem needs to be solved, its result is first checked in the memoization table.
If the result is present, it is directly returned from the table without recomputing. If the result is not
present, the subproblem is solved and its result is stored in the table for future use.
Here’s an example that demonstrates the memoization technique using the Fibonacci sequence
implemented in Python and Java:

Fundamental Concepts
11
def fibonacci(n, memo={}):
if n <= 1:
return n
if n not in memo:
memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)
return memo[n]
import java.util.HashMap;
import java.util.Map;
public class Fibonacci {
public static int fibonacci(int n) {
Map<Integer, Integer> memo = new HashMap<>();
return fibonacciHelper(n, memo);
}
private static int fibonacciHelper(int n, Map<Integer, Integer> memo) {
if (n <= 1) {
return n;
}
if (!memo.containsKey(n)) {
memo.put(n, fibonacciHelper(n-1, memo) + fibonacciHelper(n-2, memo));
}
return memo.get(n);
}
}
In these implementations, we introduce a memoization table (memo in Python and Map<Integer,
Integer> memo in Java) to store the computed Fibonacci numbers. Before calculating the Fibonacci
number for a particular index, we check if it exists in the memoization table. If it does, we directly
retrieve the result from the table, avoiding redundant recursive calls and significantly improving the
efficiency of the algorithm.
Tabulation Technique
Tabulation, also known as bottom-up dynamic programming, involves solving the subproblems in
a specific order and using a table or array to store the results of each subproblem. The table is filled
iteratively, starting from the base cases and progressing towards the final solution. By computing

Fundamental Concepts
12
the subproblems in a specific order, we can ensure that the results of the dependent subproblems are
already available when needed.
Let’s consider an example of calculating the binomial coefficient using the tabulation technique in
Python and Java:
def binomialCoefficient(n, k):
table = [[0] * (k+1) for _ in range(n+1)]
for i in range(n+1):
for j in range(min(i, k)+1):
if j == 0 or j == i:
table[i][j] = 1
else:
table[i][j] = table[i-1][j-1] + table[i-1][j]
return table[n][k]
public class BinomialCoefficient {
public static int binomialCoefficient(int n, int k) {
int[][] table = new int[n+1][k+1];
for (int i = 0; i <= n; i++) {
for (int j = 0; j <= Math.min(i, k); j++) {
if (j == 0 || j == i) {
table[i][j] = 1;
} else {
table[i][j] = table[i-1][j-1]
+ table[i-1][j];
}
}
}
return table[n][k];
}
}
In these implementations, we create a 2D table (table) to store the binomial coefficients for all
possible values of n and k. We fill the table iteratively using nested loops, computing each coefficient
based on the previously computed coefficients. By leveraging the tabulation technique, we avoid
redundant calculations and improve the efficiency of the algorithm.

Fundamental Concepts
13
Comparison and Usage
Both memoization and tabulation techniques have their advantages and are suitable for different
scenarios. Memoization is typically used when the problem involves recursive subproblems and
requires a top-down approach. It is well-suited for problems with a large number of overlapping
subproblems.
Tabulation, on the other hand, is useful for problems with a clear ordering of
subproblems and can be solved using an iterative, bottom-up approach.
Memoization offers flexibility in solving only the required subproblems, as the results are computed
on-demand. However, it may incur additional overhead due to function call overhead and the need
for maintaining the memoization table. Tabulation, on the other hand, ensures that all subproblems
are solved and can provide a better understanding of the overall problem structure. It is often more
efficient in terms of memory usage and can be easily parallelized.
The choice between memoization and tabulation depends on the problem at hand and the specific
requirements. It is important to analyze the problem’s characteristics, such as the presence of
overlapping subproblems and the order in which they can be solved, to determine the most suitable
technique.
Summary
We explored two important techniques in dynamic programming: memoization and tabulation.
We learned that memoization involves caching the results of already computed subproblems, while
tabulation uses a table to store the results of subproblems in a specific order. By avoiding redundant
calculations, these techniques improve the efficiency of dynamic programming algorithms.
We implemented examples in both Python and Java to demonstrate the application of memoization
and tabulation.
The Fibonacci sequence showcased how memoization can optimize recursive
algorithms by storing and reusing computed results. The calculation of the binomial coefficient
demonstrated how tabulation can efficiently solve problems by filling a table with computed
subproblem results.
Understanding and utilizing memoization and tabulation techniques are crucial for designing
efficient dynamic programming algorithms. The choice between the two techniques depends on
the problem’s characteristics and requirements. By leveraging these techniques appropriately, we
can tackle complex problems with improved time and space efficiency.
Time and Space Complexity Analysis
When working with dynamic programming algorithms, it is essential to analyze their time and
space complexities to understand their efficiency and resource requirements. In this section, we
will explore how to analyze the time and space complexities of dynamic programming solutions.
We will provide examples in both Python and Java to illustrate the concept.

Fundamental Concepts
14
Time Complexity Analysis
Time complexity refers to the amount of time required by an algorithm to run as a function of the
input size. It helps us understand how the algorithm’s performance scales with increasing input
sizes. Analyzing the time complexity of a dynamic programming algorithm involves considering
the number of operations performed and how they relate to the size of the problem.
Let’s consider an example of calculating the factorial of a number using dynamic programming in
Python and Java:
def factorial(n):
if n <= 1:
return 1
dp = [0] * (n+1)
dp[0] = 1
dp[1] = 1
for i in range(2, n+1):
dp[i] = i * dp[i-1]
return dp[n]
public class Factorial {
public static int factorial(int n) {
if (n <= 1) {
return 1;
}
int[] dp = new int[n+1];
dp[0] = 1;
dp[1] = 1;
for (int i = 2; i <= n; i++) {
dp[i] = i * dp[i-1];
}
return dp[n];
}
}
In these implementations, we use dynamic programming to calculate the factorial of a number. We
create an array (dp) to store the factorial values for each index. We iterate through the array, filling

Fundamental Concepts
15
it iteratively by multiplying the current index with the previously computed factorial value. The
time complexity of this algorithm is O(n) because it performs a single loop through the array of size
n.
Space Complexity Analysis
Space complexity refers to the amount of memory required by an algorithm to solve a problem as
a function of the input size. It helps us understand how much memory the algorithm consumes,
which is crucial when dealing with large inputs or limited memory resources. Analyzing the space
complexity of a dynamic programming algorithm involves considering the amount of memory used
and how it scales with the size of the problem.
Let’s consider an example of calculating the Fibonacci sequence using dynamic programming in
Python and Java:
def fibonacci(n):
if n <= 1:
return n
dp = [0] * (n+1)
dp[0] = 0
dp[1] = 1
for i in range(2, n+1):
dp[i] = dp[i-1] + dp[i-2]
return dp[n]
public class Fibonacci {
public static int fibonacci(int n) {
if (n <= 1) {
return n;
}
int[] dp = new int[n+1];
dp[0] = 0;
dp[1] = 1;
for (int i = 2; i <= n; i++) {
dp[i] = dp[i-1] + dp[i-2];
}

Fundamental Concepts
16
return dp[n];
}
}
In these implementations, we use dynamic programming to calculate the Fibonacci sequence. We
create an array (dp) to store the Fibonacci values for each index. We iterate through the array, filling
it iteratively by summing the previous two Fibonacci values. The space complexity of this algorithm
is O(n) because it requires an array of size n to store the Fibonacci values.
Summary
Analyzing the time and space complexities of dynamic programming algorithms is crucial for
understanding their efficiency and resource requirements. Time complexity helps us understand
how the algorithm’s performance scales with increasing input sizes, while space complexity provides
insights into the memory consumption.
In this section, we explored time and space complexity analysis with examples in Python and Java.
We analyzed the factorial calculation algorithm’s time complexity, which was linear with O(n). We
also examined the Fibonacci sequence algorithm’s space complexity, which was linear with O(n).
By understanding the time and space complexities, we can make informed decisions about the
suitability of dynamic programming algorithms for solving specific problems. It enables us to assess
their efficiency and determine if optimizations or alternative approaches are necessary for handling
larger inputs or constrained resources.
Conclusion
In the next chapters, we will apply these fundamental concepts to solve a variety of problems
using dynamic programming techniques.
By understanding optimal substructure, overlapping
subproblems, and choosing appropriate memoization or tabulation techniques, we can develop
efficient and elegant solutions.
Now that we have covered the fundamental concepts, let’s dive into practical examples and explore
the power of dynamic programming in solving complex problems.

Advanced Dynamic Programming
Techniques
Prepare to transcend the boundaries of conventional problem-solving as we embark on a thrilling
expedition into the realm of advanced dynamic programming techniques. In this chapter, we will
explore a diverse array of strategies that push the boundaries of what is possible, enabling you to
solve even the most intricate and complex problems with elegance and efficiency.
Unleashing the Power of the Bottom-Up Approach:
Mastering Iteration
In the realm of dynamic programming, problem-solving is not just a matter of intuition or clever
algorithms; it’s about employing a systematic and structured approach to conquer even the most
challenging problems. One such approach that holds immense power and versatility is the bottom-
up approach. By mastering iteration and building solutions from the ground up, you can unlock the
full potential of dynamic programming and tackle complex problems with elegance and efficiency.
Understanding the Essence of the Bottom-Up Approach
The bottom-up approach, also known as the iterative approach, focuses on solving problems by
breaking them down into smaller subproblems and progressively building solutions based on the
solutions to these subproblems. Unlike the top-down approach, which relies on recursion and
memoization, the bottom-up approach tackles problems in a sequential and systematic manner. It
starts with solving the simplest subproblems and gradually combines their solutions to solve larger
and more complex subproblems until the main problem is resolved.
Harnessing the Power of Iteration
At the heart of the bottom-up approach lies the power of iteration. Instead of relying on recursive
function calls, iteration allows us to employ loops to iteratively solve subproblems and build
solutions incrementally. This approach eliminates the overhead of function calls and reduces the
risk of stack overflow for problems with large input sizes. By leveraging iteration, we can optimize
the runtime and space complexity of our solutions, making them more efficient and scalable.
17

Advanced Dynamic Programming Techniques
18
The Key Steps of the Bottom-Up Approach
To effectively apply the bottom-up approach, it is essential to follow a systematic set of steps:
1. Identify the subproblems: Analyze the main problem and break it down into smaller subprob-
lems that can be solved independently. These subproblems should represent a meaningful
division of the main problem and capture its essential characteristics.
2. Define the base cases: Determine the simplest subproblems that can be solved directly without
further decomposition. These base cases serve as the foundation for building solutions to larger
subproblems.
3. Formulate the recurrence relation: Establish the relationship between the solutions of the
subproblems and the solution to the main problem. This recurrence relation defines how the
solutions of smaller subproblems can be combined to solve larger subproblems and eventually
solve the main problem.
4. Design the iterative algorithm: Based on the recurrence relation, design an iterative algorithm
that starts with the base cases and gradually builds solutions to larger subproblems. This
algorithm typically involves looping through the subproblems in a specific order, ensuring
that the solutions to the smaller subproblems are available before solving the larger ones.
5. Determine the solution to the main problem: Once the iterative algorithm has computed
solutions to all the subproblems, the solution to the main problem can be extracted or derived
from the solutions of the relevant subproblems.
This final step completes the bottom-up
approach, providing a comprehensive solution to the original problem.
Advantages of the Bottom-Up Approach
The bottom-up approach offers several advantages that make it a powerful technique in dynamic
programming:
1. Elimination of redundant computations: Unlike the top-down approach, which relies on
memoization to avoid redundant computations, the bottom-up approach naturally avoids
redundancy. By solving subproblems in a bottom-up fashion, the solutions are computed only
once and stored for future use. This leads to improved efficiency and eliminates unnecessary
recalculations.
2. Enhanced space efficiency: The bottom-up approach often requires less memory compared to
the top-down approach. It avoids the overhead of recursive function calls and memoization
tables, allowing for more efficient utilization of memory resources. This is particularly advan-
tageous when dealing with problems with large input sizes or limited memory constraints.
3. Better scalability: The iterative nature of the bottom-up approach makes it inherently scalable.
It can handle problems with large input sizes more efficiently, as the algorithm progresses
incrementally without

Advanced Dynamic Programming Techniques
19
risking stack overflow. This scalability is crucial in scenarios where efficiency and performance are
paramount.
4. Improved code readability: The bottom-up approach typically results in code that is more
concise, modular, and easier to understand. It breaks down the problem into smaller, self-
contained subproblems, making the code more structured and intuitive. This enhances code
maintainability and facilitates collaboration among team members.
Real-World Examples
To truly grasp the power of the bottom-up approach, let’s explore a couple of real-world examples
where it shines:
Example 1: Fibonacci Sequence
The Fibonacci sequence is a classic problem that can be efficiently solved using the bottom-up
approach. In this scenario, each number in the sequence is the sum of the two preceding numbers. By
starting with the base cases (Fibonacci numbers 0 and 1) and iteratively computing the subsequent
numbers, we can efficiently generate the Fibonacci sequence without redundant computations.
The bottom-up approach allows us to avoid the recursive function calls and repeated calculations
involved in the top-down approach, resulting in a more efficient and scalable solution.
Example 2: Minimum Coin Change
The minimum coin change problem involves finding the minimum number of coins required to make
a certain amount of change. By employing the bottom-up approach, we can iteratively compute
the minimum number of coins needed for each subproblem, starting from the smallest possible
change and progressively building solutions for larger amounts. The bottom-up approach eliminates
redundant computations and ensures that the solutions to smaller subproblems are available when
solving larger ones. This leads to an efficient and optimal solution to the minimum coin change
problem.
Summary
The bottom-up approach is a valuable tool in the arsenal of dynamic programming techniques.
By embracing iteration, breaking down problems into subproblems, and progressively building
solutions, you can unleash the full power of dynamic programming. Its advantages, such as the
elimination of redundant computations, enhanced space efficiency, improved scalability, and code
readability, make it a go-to approach for tackling complex problems.
By mastering the bottom-up approach, you equip yourself with a powerful problem-solving
framework that can be applied to a wide range of dynamic programming challenges. So dive into
the world of iteration, harness the potential of subproblem decomposition, and unlock your ability
to solve intricate problems with elegance and efficiency. The bottom-up approach awaits your
exploration, opening doors to new levels of problem-solving prowess.

Advanced Dynamic Programming Techniques
20
Ascending to Success: Unveiling the Top-Down
Approach in Dynamic Programming
Dynamic programming encompasses a plethora of problem-solving techniques that empower pro-
grammers to tackle complex challenges with finesse. One such technique that deserves admiration
is the top-down approach. By embracing recursion, memoization, and the power of breaking down
problems into smaller subproblems, the top-down approach offers a powerful and elegant solution
framework. In this section, we will delve into the intricacies of the top-down approach and explore
its strengths, applications, and the steps involved in its implementation.
Understanding the Essence of the Top-Down Approach
The top-down approach, also known as the recursive approach, revolves around solving a problem
by recursively breaking it down into smaller subproblems. This approach embodies the concept
of “divide and conquer,” where the main problem is divided into smaller, more manageable
subproblems until the base cases, which represent the simplest form of the problem, are reached.
The solutions to these subproblems are then combined to solve larger subproblems until the main
problem is solved.
Embracing the Power of Recursion
At the core of the top-down approach lies the power of recursion. Recursion allows us to define a
problem in terms of subproblems of the same nature, creating a recursive call stack that elegantly
handles the breakdown and combination of solutions. By leveraging recursion, we can express
complex problems in a concise and intuitive manner, focusing on the essential logic and leaving the
iterative details to the recursive calls.
The Key Steps of the Top-Down Approach
To effectively apply the top-down approach, it is essential to follow a systematic set of steps:
1. Identify the base cases: Determine the simplest form of the problem that can be solved directly
without further decomposition. These base cases serve as the termination condition for the
recursive calls and provide a starting point for building solutions to larger subproblems.
2. Define the recursive function: Create a recursive function that encapsulates the problem-
solving logic. This function should take the necessary parameters representing the state of
the problem and recursively call itself on smaller subproblems until the base cases are reached.
3. Implement memoization: Memoization is a technique used to store the solutions to subprob-
lems for future use, avoiding redundant computations. By caching the results of each subprob-
lem, we can significantly improve the efficiency of the top-down approach. Memoization can
be implemented using data structures such as arrays, maps, or caches to store and retrieve the
computed solutions.

Advanced Dynamic Programming Techniques
21
4. Invoke the recursive function: Invoke the recursive function with the initial problem state,
initiating the top-down computation process. The recursive calls will systematically break
down the problem into smaller subproblems until the base cases are encountered.
5. Return the solution: As the recursive calls unwind, the solutions to the subproblems are
combined to obtain the solution to the main problem.
Return the computed solution,
completing the top-down approach.
Advantages of the Top-Down Approach
The top-down approach offers several advantages that make it a powerful technique in dynamic
programming:
1. Simplicity and readability: The recursive nature of the top-down approach allows for a more
intuitive and straightforward implementation. The code closely reflects the problem statement,
making it easier to understand and maintain. The breakdown of the problem into smaller
subproblems mirrors the problem’s natural structure, enhancing code clarity.
2. Memoization for efficiency: The top-down approach, when combined with memoization, can
significantly reduce redundant computations. By caching the solutions to subproblems, we can
avoid recalculating them and instead retrieve them from the cache when needed. This leads to
improved efficiency, especially when dealing with problems with overlapping subproblems.
3. Flexible problem-solving: The top-down approach embraces the concept of breaking down a
problem into smaller subproblems. This allows for a flexible problem-solving strategy where
different subproblems can be solved independently and combined to solve the main problem. It
provides modularity and reusability, allowing for the application of different strategies to different
subproblems.
Real-World Applications
The top-down approach finds application in various domains and problem scenarios. Here are a few
examples:
Example 1: Longest Common Subsequence
The problem of finding the longest common subsequence between two sequences can be efficiently
solved using the top-down approach. By recursively comparing the elements of the two sequences
and breaking down the problem into smaller subproblems, we can obtain the longest common
subsequence. The use of memoization ensures that the solutions to the subproblems are stored
and reused, improving performance.

Advanced Dynamic Programming Techniques
22
Example 2: Minimum Edit Distance
The minimum edit distance problem, which involves determining the minimum number of opera-
tions required to transform one string into another, lends itself well to the top-down approach. By
recursively comparing the characters of the strings and considering different operations (insertion,
deletion, substitution), we can break down the problem into smaller subproblems and compute the
minimum edit distance. Memoization helps avoid redundant computations, making the solution
efficient.
Summary
The top-down approach stands as a testament to the elegance and power of dynamic programming.
By leveraging recursion, breaking down problems into smaller subproblems, and employing memo-
ization, this approach offers a systematic and intuitive framework for solving complex problems. Its
advantages, such as simplicity, readability, flexibility, and efficiency through memoization, make it
a valuable tool in a programmer’s arsenal.
As you embark on your dynamic programming journey, embrace the top-down approach and
witness how it enables you to conquer intricate problems with grace. The recursive calls will
guide you through the problem landscape, unraveling solutions piece by piece. So, let the top-
down approach elevate your problem-solving prowess and unlock new horizons of programming
excellence.
Unleashing Efficiency: Exploring State Space
Reduction in Dynamic Programming
Dynamic programming offers a powerful arsenal of techniques to tackle complex problems effi-
ciently. Among these techniques, state space reduction stands out as a potent approach to optimize
memory usage and enhance computational performance. By cleverly reducing the state space, we
can trim down the storage requirements and expedite the solution process. In this section, we will
dive deep into the concept of state space reduction, understand its significance, explore various
strategies, and witness its impact on dynamic programming solutions.
Understanding the Essence of State Space
Before delving into state space reduction, let’s grasp the concept of state space.
In dynamic
programming, a state represents the set of variables or parameters that define the problem’s current
state or subproblem. It encapsulates the information necessary to compute the solution for that
particular state. The state space encompasses the entire set of possible states that a problem can
have. For complex problems, the state space can be vast, leading to exponential growth in memory
requirements and computation time.

Advanced Dynamic Programming Techniques
23
The Need for State Space Reduction
In certain scenarios, the full state space is not required to compute the optimal solution. Reducing
the state space can bring significant benefits, such as:
1. Memory Optimization: By reducing the state space, we can limit the storage requirements,
enabling efficient utilization of memory resources. This becomes particularly crucial when
dealing with problems involving large input sizes or limited memory constraints.
2. Improved Efficiency: A smaller state space often results in faster computations. With fewer
states to evaluate, the solution process becomes more streamlined, leading to improved runtime
performance. This reduction in computational complexity can make a substantial difference,
especially for problems with exponential time complexity.
Strategies for State Space Reduction
State space reduction techniques aim to identify and eliminate redundant or unnecessary states,
focusing only on the essential states for computing the optimal solution. Here are some commonly
employed strategies:
1. Partial State Representation
In some cases, not all variables or parameters in a state are relevant for computing the optimal
solution. By considering a partial representation of the state, excluding irrelevant or redundant
variables, we can significantly reduce the state space. This strategy requires careful analysis of the
problem and identifying the key variables that directly impact the solution.
2. Problem Symmetry Exploitation
Symmetry exists in many problems, where different states may lead to the same solution. By
exploiting this symmetry, we can collapse equivalent states into a single representative state.
This consolidation eliminates duplicate computations and reduces the state space. Identifying and
leveraging symmetry requires a deep understanding of the problem structure and properties.
3. State Pruning
State pruning involves selectively discarding certain states based on predefined conditions or
heuristics. Through analysis of the problem and observation of the optimal substructure, we can
determine criteria for pruning states that are unlikely to contribute to the optimal solution. Pruning
non-promising states early on can lead to substantial efficiency gains.

Advanced Dynamic Programming Techniques
24
4. Subset Sum Techniques
For problems involving subsets or combinations, techniques such as bitmasks or bitwise operations
can be employed to represent subsets compactly. By using bitwise representations, we can reduce
the state space from exponential to polynomial, resulting in more efficient computations. Subset
sum techniques are particularly useful when solving problems like the Knapsack problem or subset
sum problems.
Impact on Dynamic Programming Solutions
The application of state space reduction techniques can have a profound impact on dynamic
programming solutions. By intelligently reducing the state space, we can achieve:
1. Memory Efficiency: Reducing the state space minimizes memory usage, making it feasible
to solve problems with large input sizes or limited memory availability. It allows dynamic
programming solutions to scale effectively without exhausting system resources.
2. Improved Performance: With a reduced state space, the solution
process becomes more efficient. The computation time decreases as redundant or irrelevant states
are eliminated, enabling faster generation of optimal solutions. This performance improvement is
particularly significant for problems with exponential time complexity.
3. Simplified Implementation: State space reduction often simplifies the implementation of
dynamic programming algorithms. By focusing only on essential states, the complexity of
the solution logic decreases, leading to cleaner and more concise code. It also facilitates easier
debugging and maintenance of the codebase.
Real-World Examples
State space reduction finds applications in various problem domains. Here are a few examples
showcasing its effectiveness:
Example 1: Traveling Salesman Problem (TSP)
The TSP involves finding the shortest possible route that visits all given cities and returns to the
starting city. By exploiting problem symmetry, we can reduce the state space by considering only
one starting city and eliminating duplicate routes resulting from different starting cities. This state
space reduction drastically improves the efficiency of TSP solutions.

Advanced Dynamic Programming Techniques
25
Example 2: Chessboard Problems
Various chessboard-based problems, such as N-Queens or Knight’s Tour, can benefit from state space
reduction techniques. By identifying symmetry and eliminating equivalent states, the search space
can be significantly reduced. This reduction allows for faster exploration of the solution space and
quicker identification of valid configurations.
Summary
State space reduction is a powerful technique in dynamic programming that allows for efficient
memory utilization and improved computational performance.
By strategically reducing the
state space through partial state representation, symmetry exploitation, state pruning, and subset
sum techniques, we can optimize the solution process and achieve faster, more memory-efficient
solutions.
As you encounter complex problems in your programming journey, remember the significance of
state space reduction. Analyze the problem structure, identify redundant states, and apply the appro-
priate reduction strategies. Embracing state space reduction empowers you to conquer challenging
problems with elegance and efficiency, unlocking the full potential of dynamic programming. So,
leverage the power of state space reduction and embark on a journey of optimized problem-solving.
Harnessing the Power of Bitmasking and Bitwise
Operations in Dynamic Programming
Bitmasking and bitwise operations are formidable tools in the arsenal of dynamic programming.
These techniques provide efficient ways to manipulate and extract information from binary
representations of numbers, offering substantial advantages in terms of performance and memory
utilization. In this section, we will delve into the world of bitmasking and bitwise operations,
exploring their significance, understanding their mechanics, and witnessing their impact on
dynamic programming solutions.
Unveiling the Magic of Bitwise Operations
Bitwise operations operate on individual bits of binary representations of numbers. They allow us
to perform logical operations, such as AND, OR, XOR, shifting, and complementing, on these bits.
By leveraging bitwise operations, we can exploit the inherent properties of binary representations
and efficiently extract and manipulate information.
The Power of Bitmasking
Bitmasking involves using a binary mask to perform operations on specific bits of a number. The
mask acts as a filter, enabling us to select or modify particular bits while preserving the rest.

Advanced Dynamic Programming Techniques
26
Bitmasking is particularly useful in dynamic programming, where problems often involve sets or
subsets, combinations, or binary states.
Applications of Bitmasking in Dynamic Programming
Let’s explore some common scenarios where bitmasking and bitwise operations shine in dynamic
programming:
1. Subset Generation and Manipulation
Bitmasking provides an elegant solution for generating and manipulating subsets of a given set. By
representing subsets as binary numbers, where each bit represents the inclusion or exclusion of an
element, we can efficiently iterate over all possible subsets using bitwise operations. This approach
simplifies the generation of power sets, facilitates subset-based problem-solving, and reduces the
time complexity from exponential to polynomial.
2. State Space Reduction
As discussed in the previous article, state space reduction plays a crucial role in optimizing dynamic
programming solutions. Bitmasking offers a compact representation of states involving binary
variables or flags. By using bitwise operations to manipulate and combine these binary states, we
can significantly reduce the state space, leading to more memory-efficient and faster solutions.
3. Dynamic Programming Optimization Techniques
Bitmasking can enhance the efficiency of dynamic programming algorithms through various
optimization techniques. For example, the bitmask-based memoization technique reduces redun-
dant computations by storing intermediate results using a bitmask as the key. This technique
enables dynamic programming algorithms to skip unnecessary recursive calls and directly retrieve
precomputed values, reducing time complexity and improving performance.
Understanding Bitwise Operations in Practice
To gain a better understanding, let’s delve into the mechanics of some commonly used bitwise
operations:
1. AND (&) Operation
The AND operation compares the corresponding bits of two numbers and returns a new number
with bits set to 1 only if both input bits are 1. In the context of bitmasking, the AND operation helps
us extract specific bits of interest by masking out the rest.

Advanced Dynamic Programming Techniques
27
2. OR (|) Operation
The OR operation compares the corresponding bits of two numbers and returns a new number with
bits set to 1 if either of the input bits is 1. OR operations are useful in bitmasking to combine multiple
masks or set specific bits to 1.
3. XOR (^) Operation
The XOR operation compares the corresponding bits of two numbers and returns a new number
with bits set to 1 if the input bits are different. XOR operations are valuable in bitmasking to toggle
or flip specific bits.
4. Shifting (<< and >>) Operations
Shifting operations move the bits of a number left or right by a specified number of positions. Left
shifts (<<) multiply a number by 2 for each shift, effectively moving the bits to the left. Right shifts
(>>) divide a number by 2 for each shift, effectively moving the bits to the right
. Shifting operations are frequently used in bitmasking to manipulate binary representations and
extract information from different bit positions.
Bitmasking and Bitwise Operations in Action: Sample Problems
Let’s explore a few examples to witness the power of bitmasking and bitwise operations in action:
1. Subset Sum Problem
Given a set of integers and a target sum, the subset sum problem asks whether there exists a subset
whose elements sum up to the target value. By using bitmasking to represent subsets and bitwise
operations to iterate over all possible subsets, we can efficiently solve this problem in polynomial
time.
2. Traveling Salesman Problem with Bitmasking
The Traveling Salesman Problem (TSP) involves finding the shortest Hamiltonian cycle that visits
all given cities. By using bitmasking to represent the visited cities and bitwise operations to generate
all possible permutations, we can solve the TSP more efficiently than traditional approaches.
Summary
Bitmasking and bitwise operations provide a powerful set of tools for tackling dynamic pro-
gramming problems.
With their ability to manipulate binary representations efficiently, these

Advanced Dynamic Programming Techniques
28
techniques enable us to optimize memory usage, reduce time complexity, and simplify problem-
solving strategies. By harnessing the power of bitmasking and bitwise operations, we unlock new
avenues for designing elegant and efficient dynamic programming algorithms. So, embrace the
magic of bit manipulation and let these techniques empower you to conquer complex problems
with grace and speed.
Uniting Forces: Divide and Conquer with Dynamic
Programming
Divide and Conquer and Dynamic Programming are two powerful problem-solving paradigms
that, when combined, can lead to even more efficient and elegant solutions. In this section, we
will explore the synergy between Divide and Conquer and Dynamic Programming, understanding
their individual strengths and witnessing how their fusion can overcome complex problems with
remarkable efficiency.
Understanding Divide and Conquer
Divide and Conquer is a problem-solving technique that breaks down a complex problem into
smaller, more manageable subproblems, solves these subproblems independently, and then com-
bines their solutions to obtain the final result. The fundamental steps of the Divide and Conquer
approach are:
1. Divide: Break the problem into smaller, similar subproblems.
2. Conquer: Solve the subproblems recursively or iteratively, usually using a base case for
termination.
3. Combine: Merge the solutions of the subproblems to obtain the solution to the original
problem.
The power of Divide and Conquer lies in its ability to reduce the complexity of a problem by breaking
it down into smaller, independent subproblems.
The Role of Dynamic Programming
Dynamic Programming, on the other hand, focuses on efficiently solving optimization problems
by breaking them into overlapping subproblems and reusing their solutions.
By solving each
subproblem only once and storing the result in a table or memoization array, Dynamic Programming
avoids redundant computations and significantly improves performance.

Advanced Dynamic Programming Techniques
29
Combining Forces: Divide and Conquer with Dynamic
Programming
When Divide and Conquer and Dynamic Programming converge, we can leverage the strengths of
both paradigms to devise highly efficient solutions. The key idea is to apply Divide and Conquer
recursively to break down a problem into subproblems and then employ Dynamic Programming to
solve these subproblems optimally, storing their solutions for reuse.
This fusion allows us to tackle problems that exhibit overlapping subproblems efficiently while
maintaining the structure and benefits of Divide and Conquer.
By solving subproblems using
Dynamic Programming and storing their solutions, we avoid repetitive computations and achieve
significant performance improvements.
Examples of Divide and Conquer with Dynamic Programming
Let’s explore a couple of examples to better grasp the power of combining Divide and Conquer with
Dynamic Programming:
1. Matrix Chain Multiplication
The Matrix Chain Multiplication problem involves determining the most efficient way to multiply
a chain of matrices. By applying Divide and Conquer to break down the problem into smaller
subproblems and then using Dynamic Programming to store and reuse the optimal solutions, we
can drastically reduce the overall computational complexity.
2. Closest Pair of Points
The Closest Pair of Points problem seeks to find the minimum distance between any two points in
a given set. By using Divide and Conquer to split the problem into smaller subproblems and then
employing Dynamic Programming to compute the closest pair of points efficiently, we can achieve
an optimal solution.
Summary
The combination of Divide and Conquer with Dynamic Programming introduces a powerful
approach to problem-solving. By leveraging the strengths of both paradigms, we can break down
complex problems, solve their subproblems optimally using Dynamic Programming techniques,
and combine the solutions to obtain an efficient and elegant solution to the original problem.
The fusion of these problem-solving strategies opens doors to tackling challenging problems with
improved efficiency and scalability. So, embrace the unity of Divide and Conquer with Dynamic
Programming, and let it empower you to conquer complex problems with confidence.

Advanced Dynamic Programming Techniques
30
Mastering Complexity: Multidimensional Dynamic
Programming
Dynamic Programming is a problem-solving paradigm renowned for its ability to efficiently solve
complex optimization problems by breaking them down into overlapping subproblems. While the
traditional approach focuses on solving problems with one-dimensional states, the world of problem-
solving often demands more dimensions to capture intricate relationships and dependencies. Enter
Multidimensional Dynamic Programming, a powerful extension of Dynamic Programming that
enables us to tackle problems with multiple dimensions. In this article, we will explore the intricacies
of Multidimensional Dynamic Programming and witness its prowess in handling complex real-
world scenarios.
Understanding Multidimensional Dynamic Programming
Multidimensional Dynamic Programming expands the traditional Dynamic Programming frame-
work by introducing additional dimensions to represent the problem’s state space.
Instead of
working with a single state variable, we now have multiple variables that capture different aspects
of the problem. This multidimensional representation allows us to model and solve problems that
involve complex relationships among multiple entities or factors.
The core principles of Dynamic Programming still apply in the multidimensional context. We
break down the problem into smaller, overlapping subproblems, solve them optimally, and store
the solutions for reuse. However, now we must carefully design the state space to account for the
additional dimensions and establish relationships among them.
Benefits of Multidimensional Dynamic Programming
Multidimensional Dynamic Programming offers several advantages when confronted with problems
that require the consideration of multiple dimensions:
1. Enhanced Modeling Capability
By introducing additional dimensions, we can more accurately model and represent real-world
scenarios that involve multiple interacting factors. This enhanced modeling capability allows us
to capture complex relationships and dependencies, leading to more accurate and precise solutions.
2. Improved Problem Decomposition
Multidimensional problems are often decomposable into smaller, independent subproblems along
each dimension. This decomposition enables us to apply the Divide and Conquer strategy within
each dimension, breaking the problem down into manageable subproblems and solving them
independently. Consequently, the overall problem complexity is reduced, leading to improved
efficiency.

Advanced Dynamic Programming Techniques
31
3. Dynamic Programming Optimization
The multidimensional nature of the problem often implies that multiple subproblems share common
substructures, resulting in overlapping subproblems. By exploiting these overlapping subproblems,
we can apply Dynamic Programming techniques to store and reuse the solutions efficiently. This
optimization significantly reduces redundant computations and improves overall performance.
Applying Multidimensional Dynamic Programming
To illustrate the power of Multidimensional Dynamic Programming, let’s consider a few examples
where the multidimensional approach shines:
1. Image Processing and Computer Vision
Problems in image processing and computer vision often involve multidimensional data, such as
images represented as grids of pixels with color channels. By leveraging Multidimensional Dynamic
Programming, we can efficiently process and analyze images, extracting valuable information,
detecting patterns, and performing complex tasks like image segmentation and object recognition.
2. Resource Allocation and Planning
In resource allocation and planning problems, decisions often depend on multiple dimensions, such
as time, budget, and resource availability. Multidimensional Dynamic Programming allows us to
optimize resource allocation, schedule tasks, and make decisions based on the interplay of various
factors, maximizing efficiency and achieving optimal outcomes.
3. Genome Sequencing and Bioinformatics
Genome sequencing and bioinformatics involve analyzing genetic data, which inherently possesses
multiple dimensions, such as nucleotide sequences and structural characteristics. By employing
Multidimensional Dynamic Programming, we can efficiently compare, align, and analyze genetic
sequences, unraveling valuable insights about biological structures and functions.
Summary
Multidimensional Dynamic Programming is a valuable extension of the traditional Dynamic
Programming paradigm that equips problem solvers with the means to handle complex problems
involving multiple dimensions. By expanding the state space and considering intricate relationships
among various factors, we gain the ability to model real-world scenarios accurately. With improved
problem decomposition, Dynamic Programming optimization, and enhanced modeling capability,
Multidimensional Dynamic Programming empowers us to conquer intricate problems across diverse
domains, ranging from image processing to resource allocation and bioinformatics. Embrace the
multidimensional world of Dynamic Programming, and unlock the potential to master complexity
with elegance and efficiency.

Advanced Dynamic Programming Techniques
32
Conclusion
In this chapter, we delved into advanced techniques in Dynamic Programming that extend its
capabilities and enable us to solve even more complex problems. We explored various techniques,
including the Bottom-Up Approach, Top-Down Approach, State Space Reduction, Bitmasking
and Bitwise Operations, Divide and Conquer with Dynamic Programming, and Multidimensional
Dynamic Programming.
By leveraging the Bottom-Up Approach, we can iteratively solve subproblems and build up to
the final solution, eliminating redundant computations and achieving optimal results. The Top-
Down Approach, on the other hand, allows us to break down the problem recursively and memoize
intermediate results, providing efficient solutions by avoiding redundant computations.
State Space Reduction provides a way to optimize memory usage by identifying and eliminating
unnecessary dimensions in the problem’s state space. This technique enables us to solve problems
more efficiently while still capturing the essential information needed for the solution.
Bitmasking and Bitwise Operations are powerful tools when dealing with problems that involve
binary representations or set operations. By leveraging the binary nature of numbers, we can
perform operations efficiently and solve problems with improved time and space complexity.
Divide and Conquer with Dynamic Programming combines the principles of Divide and Conquer
and Dynamic Programming to solve complex problems. By breaking the problem into smaller
subproblems, solving them independently, and combining the solutions, we can efficiently tackle
problems with high complexity.
Lastly, Multidimensional Dynamic Programming extends the traditional Dynamic Programming
framework to handle problems with multiple dimensions.
This technique allows us to model
complex relationships and dependencies accurately, leading to precise solutions in various domains
such as image processing, resource allocation, and bioinformatics.
Each of these advanced techniques provides valuable tools and strategies to tackle a wide range of
problems efficiently. By understanding and mastering these techniques, we enhance our problem-
solving capabilities and become equipped to solve complex optimization problems with elegance
and effectiveness.
In the next part of the book, we will put these techniques into action by applying them to a collection
of dynamic programming problems. So let’s dive in and explore the practical application of these
advanced techniques to solve real-world challenges!

Dynamic Programming in Practice
Dynamic Programming is a powerful algorithmic technique that finds applications in various fields
of computer science and beyond. In this chapter, we will explore the practical implementations
of Dynamic Programming in several domains, including Computer Vision, Natural Language
Processing, Bioinformatics, Financial Modeling, and Game Theory. By understanding how Dynamic
Programming is employed in these domains, we can appreciate its versatility and its ability to solve
a wide range of complex problems efficiently.
Dynamic Programming in Computer Vision
Computer Vision is a field that deals with the analysis and interpretation of visual data, enabling
computers to understand and extract meaningful information from images or videos. Dynamic
Programming plays a crucial role in various computer vision tasks, such as image segmentation,
object recognition, and optical flow estimation.
In image segmentation, Dynamic Programming can be used to partition an image into meaningful
regions by optimizing certain criteria, such as color similarity or edge continuity. By formulating
the segmentation problem as an energy minimization task, Dynamic Programming algorithms can
efficiently find the optimal partitioning of the image.
Object recognition involves identifying and classifying objects in images or videos.
Dynamic
Programming can aid in this task by matching local image features to a database of known object
features. By formulating the matching problem as an optimization task, Dynamic Programming
algorithms can efficiently find the best matching between features, enabling accurate object
recognition.
Optical flow estimation aims to determine the motion vectors of objects in a sequence of images.
Dynamic Programming can be employed to compute the dense optical flow by minimizing an
energy function that captures the consistency of pixel intensities and motion smoothness. Dynamic
Programming algorithms provide an efficient way to solve this optimization problem and estimate
accurate optical flow.
Dynamic Programming in Natural Language
Processing
Natural Language Processing (NLP) focuses on enabling computers to understand and process
human language. Dynamic Programming techniques are widely used in various NLP applications,
such as speech recognition, machine translation, and sentiment analysis.
33

Dynamic Programming in Practice
34
In speech recognition, Dynamic Programming algorithms are utilized to perform phonetic align-
ment, which involves aligning acoustic features with corresponding phonemes. By modeling the
alignment task as an optimization problem, Dynamic Programming algorithms can efficiently find
the best alignment sequence and enable accurate speech recognition.
Machine translation involves translating text from one language to another. Dynamic Programming
techniques, such as the IBM Models, can be employed to align words and phrases between the source
and target languages. By formulating the alignment task as an optimization problem, Dynamic
Programming algorithms can effectively learn the translation probabilities and improve the quality
of machine translation.
Sentiment analysis aims to determine the sentiment expressed in a piece of text, such as positive,
negative, or neutral. Dynamic Programming algorithms can be used to compute sentiment scores by
assigning weights to different words or phrases based on their sentiment polarity. By optimizing the
sentiment scoring task, Dynamic Programming enables efficient sentiment analysis of large volumes
of text data.
Dynamic Programming in Bioinformatics
Bioinformatics is a multidisciplinary field that combines biology, computer science, and statistics
to analyze and interpret biological data. Dynamic Programming is widely employed in various
bioinformatics tasks, such as sequence alignment, RNA folding, and protein structure prediction.
Sequence alignment involves comparing two or more biological sequences, such as DNA or protein
sequences, to identify similarities or evolutionary relationships. Dynamic Programming algorithms,
such as the Needleman-Wunsch and Smith-Waterman algorithms, are utilized to find the optimal
alignment by minimizing a scoring function.
These algorithms provide efficient solutions to
sequence alignment problems, enabling researchers to analyze and compare biological sequences
effectively.
RNA folding refers to predicting the secondary structure of RNA molecules, which is crucial for
understanding their biological functions. Dynamic Programming algorithms, such as the Nussinov
algorithm, can be used to determine the optimal folding by maximizing the stability of base
pair interactions. By applying Dynamic Programming, researchers can accurately predict RNA
secondary structures and gain insights into RNA functionality.
Protein structure prediction aims to determine the three-dimensional structure of proteins based on
their amino acid sequences. Dynamic Programming techniques, such as the Dynamic Programming
Algorithm for Protein Folding (DynaPP), are employed to search for the optimal protein conforma-
tion by considering various energetic constraints. By leveraging Dynamic Programming, scientists
can make significant progress in predicting protein structures and understanding their functions.

Dynamic Programming in Practice
35
Dynamic Programming in Financial Modeling
Financial modeling involves constructing mathematical models to analyze and predict financial
markets, investment strategies, and risk management. Dynamic Programming plays a vital role in
various financial modeling tasks, such as portfolio optimization, option pricing, and risk assessment.
Portfolio optimization aims to construct an optimal investment portfolio by allocating resources
among different assets to maximize return or minimize risk. Dynamic Programming algorithms
can be utilized to determine the optimal asset allocation by considering various factors, such as
historical returns, volatility, and correlation. By applying Dynamic Programming, investors can
make informed decisions and construct efficient portfolios.
Option pricing refers to valuing financial derivative instruments, such as options, based on
underlying assets’ prices and market conditions.
Dynamic Programming techniques, such as
the Black-Scholes model, are employed to compute the fair value of options by optimizing the
expected payoff under different scenarios. By utilizing Dynamic Programming, financial analysts
can accurately price options and assess their risk profiles.
Risk assessment involves evaluating the potential risks associated with financial investments or
decisions. Dynamic Programming algorithms can be used to simulate different scenarios and assess
the likelihood and impact of various risk factors. By leveraging Dynamic Programming, risk analysts
can quantify risks and develop effective risk mitigation strategies.
Dynamic Programming in Game Theory
Game Theory deals with mathematical models of strategic interactions between multiple partici-
pants, such as players or agents. Dynamic Programming is extensively applied in various game
theory problems, such as optimal strategies, equilibrium computation, and game tree traversal.
Optimal strategies refer to determining the best actions or decisions for players in a game to
maximize their utility or payoff. Dynamic Programming techniques, such as backward induction,
can be employed to solve sequential games by computing optimal strategies for each player at each
stage of the game. By applying Dynamic Programming, players can make informed decisions and
maximize their expected outcomes.
Equilibrium computation involves identifying the Nash equilibrium, which represents a stable state
where no player can unilaterally improve their payoff. Dynamic Programming algorithms, such as
the Lemke-Howson algorithm, can be utilized to compute Nash equilibria in various game settings.
By leveraging Dynamic Programming, researchers can analyze strategic interactions and identify
equilibrium outcomes.
Game tree traversal refers to exploring the possible moves and outcomes in a game tree to determine
the optimal path or sequence of actions. Dynamic Programming techniques, such as the Minimax
algorithm, are employed to search the game tree efficiently and compute the optimal strategy. By

Dynamic Programming in Practice
36
utilizing Dynamic Programming, players can devise effective strategies and make optimal decisions
in complex game scenarios.
In conclusion, Dynamic Programming finds extensive practical applications in various domains,
including Computer Vision, Natural Language Processing, Bioinformatics, Financial Modeling, and
Game Theory.
By harnessing the power of Dynamic Programming, researchers, practitioners,
and analysts can solve complex problems efficiently, optimize resource allocation, make informed
decisions, and gain valuable insights. The versatility and effectiveness of Dynamic Programming
make it an invaluable tool in the arsenal of problem solvers across diverse fields of study and industry
domains.
Conclusion
In this chapter, we explored the practical applications of Dynamic Programming in various domains,
including Computer Vision, Natural Language Processing, Bioinformatics, Financial Modeling, and
Game Theory. By delving into these domains, we witnessed the versatility and effectiveness of
Dynamic Programming in solving complex problems and optimizing resource allocation.
Dynamic Programming has proven to be a valuable technique in Computer Vision, where it aids
in image segmentation, object recognition, and optical flow estimation. Through the formulation
of optimization problems and energy minimization, Dynamic Programming algorithms enable
accurate and efficient analysis of visual data.
In Natural Language Processing, Dynamic Programming plays a crucial role in tasks such as
speech recognition, machine translation, and sentiment analysis. By optimizing alignment tasks,
Dynamic Programming algorithms improve the accuracy of speech recognition, enhance the quality
of machine translation, and enable efficient sentiment analysis of large volumes of text data.
Bioinformatics relies heavily on Dynamic Programming for tasks like sequence alignment, RNA
folding, and protein structure prediction.
By formulating alignment and folding problems as
optimization tasks, Dynamic Programming algorithms facilitate the comparison of biological
sequences, prediction of RNA structures, and estimation of protein conformations.
Financial Modeling benefits greatly from the application of Dynamic Programming techniques
in portfolio optimization, option pricing, and risk assessment.
By considering various factors
and formulating optimization problems, Dynamic Programming algorithms assist in constructing
optimal investment portfolios, valuing financial derivatives, and evaluating risk profiles.
In Game Theory, Dynamic Programming proves invaluable in determining optimal strategies,
computing equilibria, and traversing game trees. By leveraging Dynamic Programming, players can
make informed decisions, identify equilibrium outcomes, and devise effective strategies in complex
game scenarios.
The practical implementations of Dynamic Programming showcased in this chapter emphasize its
versatility and broad applicability across diverse domains. The ability to solve complex problems

Dynamic Programming in Practice
37
efficiently, optimize resource allocation, make informed decisions, and gain valuable insights make
Dynamic Programming an invaluable tool for researchers, practitioners, and analysts.
By understanding the principles and techniques of Dynamic Programming, one can unlock a
powerful problem-solving approach applicable in various fields of study and industry domains. The
knowledge gained from this chapter will empower readers to tackle challenging problems, design
efficient algorithms, and unleash the potential of Dynamic Programming in their own endeavors.
As we move forward, armed with the knowledge and understanding of Dynamic Programming’s
practical applications, we can confidently apply its principles to solve real-world problems, drive
innovation, and push the boundaries of what is possible in our respective fields.
Dynamic
Programming continues to be an ever-relevant and impactful technique, driving progress and
advancing our capabilities across diverse domains.

Advanced Topics in Dynamic
Programming
Dynamic Programming, with its rich theoretical foundation and practical applications, continues
to evolve and extend its reach into various advanced topics. In this chapter, we will explore some
of these cutting-edge areas where Dynamic Programming plays a significant role. We will delve
into Approximation Algorithms, Online Dynamic Programming, Parallelization and Distributed
Dynamic Programming, Reinforcement Learning and Dynamic Programming, and even touch upon
the emerging field of Quantum Dynamic Programming.
Approximation Algorithms
Approximation Algorithms are a powerful tool in situations where finding an exact solution to
a problem is computationally infeasible. These algorithms aim to provide near-optimal solutions
with provable performance guarantees. In the context of Dynamic Programming, approximation
algorithms can be used to solve large-scale optimization problems efficiently.
They trade off
optimality for computational efficiency, making them suitable for solving NP-hard problems. We
will explore various approximation techniques, such as greedy algorithms, randomized rounding,
and local search, and discuss their application in Dynamic Programming.
Example: The Traveling Salesperson Problem
The Traveling Salesperson Problem (TSP) is a classic optimization problem where the goal is to find
the shortest route that visits a set of cities and returns to the starting city. Solving TSP optimally for
large problem instances is challenging due to its combinatorial nature. Approximation algorithms,
such as the Christofides algorithm or the 2-approximation algorithm, provide near-optimal solutions
that are efficient to compute. We will discuss how Dynamic Programming can be combined with
approximation techniques to solve TSP efficiently in practice.
Online Dynamic Programming
Online Dynamic Programming deals with scenarios where decisions must be made sequentially
and adaptively in the presence of uncertainty. Unlike traditional offline settings, where all input is
known in advance, online environments pose unique challenges. In Online Dynamic Programming,
decisions are made based on partial information and must be revised as new information becomes
available. We will explore algorithms that dynamically adjust their solutions based on the observed
data, making effective use of past decisions while adapting to changing circumstances.
38

Advanced Topics in Dynamic Programming
39
Example: Stock Trading with Online Dynamic Programming
Consider the problem of stock trading, where an investor makes decisions on buying or selling stocks
based on real-time market data. In an online setting, the investor must make decisions without
knowledge of future stock prices. Online Dynamic Programming algorithms can be employed to
adaptively allocate investment capital and dynamically adjust trading strategies based on observed
market trends.
We will discuss how Online Dynamic Programming techniques can optimize
investment decisions and maximize returns in dynamic market conditions.
Parallelization and Distributed Dynamic Programming
Parallelization and Distributed Dynamic Programming techniques aim to leverage the power of
parallel computing and distributed systems to solve large-scale problems more efficiently.
By
breaking down a problem into smaller subproblems and distributing the computation across multiple
processors or machines, these techniques can significantly reduce the overall computational time.
We will explore parallelization strategies, such as task parallelism and data parallelism, and discuss
how distributed systems can be utilized to accelerate Dynamic Programming algorithms.
Example: Parallelized Matrix Chain Multiplication
Matrix Chain Multiplication is a classic problem in Dynamic Programming that aims to find the
most efficient way to multiply a sequence of matrices. By parallelizing the matrix multiplication
operations across multiple processors or machines, the overall computation time can be significantly
reduced. We will examine how parallelization techniques, such as parallel matrix multiplication
algorithms and parallel task scheduling, can be applied to solve the Matrix Chain Multiplication
problem efficiently in a distributed computing environment.
Reinforcement Learning and Dynamic Programming
Reinforcement Learning (RL) is a field of study that deals with learning optimal actions in an
environment to maximize a cumulative reward. Dynamic Programming serves as a foundational
framework for RL algorithms by providing methods to solve Markov Decision Processes (MDPs)
and compute optimal policies. We will explore how Dynamic Programming concepts, such as value
iteration and policy iteration, form the basis of RL algorithms and enable autonomous agents to
learn optimal behaviors through trial and error.
Example: Dynamic Programming in Q-Learning
Q-Learning is a popular RL algorithm that utilizes Dynamic Programming techniques to learn
optimal action values in an MDP. By iteratively updating the action values based on observed

Advanced Topics in Dynamic Programming
40
rewards and future state transitions, Q-Learning converges to an optimal policy. We will delve
into the details of how Dynamic Programming is applied in Q-Learning, discussing concepts such
as the Bellman equation, Q-value iteration, and exploration-exploitation trade-offs.
Quantum Dynamic Programming
Quantum Dynamic Programming is an emerging field that explores the intersection of Dynamic
Programming and Quantum Computing. Quantum Computing offers the potential for exponential
speedup in certain computations, including optimization problems.
By leveraging quantum
algorithms and principles, Quantum Dynamic Programming aims to design efficient algorithms for
solving complex optimization problems that are intractable for classical computers. We will discuss
the basic principles of quantum computing and explore how Dynamic Programming techniques can
be adapted to the quantum realm.
Example: Quantum Traveling Salesperson Problem
The Quantum Traveling Salesperson Problem (QTSP) is a variation of the classical TSP that leverages
quantum computing principles to explore all possible routes simultaneously. Quantum Dynamic
Programming algorithms can exploit quantum superposition and interference to efficiently find
optimal or near-optimal solutions for large-scale instances of QTSP. We will discuss the potential
advantages and challenges of applying Quantum Dynamic Programming techniques to solve
combinatorial optimization problems.
Conclusion
In this chapter, we delved into advanced topics in Dynamic Programming, including Approximation
Algorithms, Online Dynamic Programming, Parallelization and Distributed Dynamic Programming,
Reinforcement Learning and Dynamic Programming, and Quantum Dynamic Programming. These
topics showcase the versatility and adaptability of Dynamic Programming techniques in solving
complex problems in various domains.
Approximation Algorithms provide efficient solutions for NP-hard problems, allowing us to trade off
optimality for computational efficiency. Online Dynamic Programming enables adaptive decision-
making in dynamic and uncertain environments, optimizing decisions based on partial information.
Parallelization and Distributed Dynamic Programming leverage the power of parallel computing
and distributed systems to accelerate computations and solve large-scale problems efficiently.
Reinforcement Learning with Dynamic Programming forms the basis of autonomous learning
agents, enabling them to learn optimal behaviors through interactions with an environment.
Lastly, Quantum Dynamic Programming explores the potential of quantum computing to achieve
exponential speedup in solving optimization problems.

Advanced Topics in Dynamic Programming
41
By understanding and applying these advanced topics, we can tackle a broader range of problems,
optimize resource allocation, adapt to changing environments, and explore the possibilities of
emerging technologies.
Dynamic Programming continues to evolve and expand its horizons,
remaining a valuable tool for researchers, practitioners, and innovators across various disciplines.
As we embrace the advancements in Dynamic Programming, we open doors to new frontiers and
uncover novel solutions to complex problems. By combining the fundamental principles of Dynamic
Programming with these advanced topics, we pave the way for future breakthroughs, shaping the
landscape of problem-solving and optimization in the years to come.

Tips and Tricks for Efficient Dynamic
Programming
Dynamic Programming is a powerful technique for solving complex optimization problems, but
achieving efficient solutions requires careful consideration of problem analysis, data structures,
space optimization, memoization strategies, and effective debugging techniques. In this chapter,
we will explore a range of tips and tricks that can enhance the efficiency and effectiveness of your
Dynamic Programming solutions.
Problem Analysis and Modeling
Before diving into the implementation of a Dynamic Programming solution, it is crucial to thor-
oughly analyze the problem and identify its underlying structure. Breaking down the problem into
smaller subproblems and understanding their relationships will guide you in designing an efficient
Dynamic Programming algorithm. We will discuss techniques such as problem decomposition,
identifying optimal substructure, and characterizing overlapping subproblems. By grasping the
problem’s essence, you can make informed decisions throughout the solution development process.
Example: The Knapsack Problem
The Knapsack Problem is a classic optimization problem where items with different weights and
values need to be selected to maximize the total value within a given weight constraint. Analyzing
the Knapsack Problem involves decomposing it into smaller subproblems, such as considering
whether to include or exclude each item. By understanding the optimal substructure of the problem,
you can develop a Dynamic Programming algorithm that efficiently computes the maximum value
for various weight constraints.
Choosing the Right Data Structures
Efficient data structures play a crucial role in Dynamic Programming solutions. The choice of
data structures can significantly impact the runtime and memory consumption of your algorithm.
Depending on the problem’s characteristics, you may consider using arrays, matrices, hash maps,
priority queues, or other specialized data structures. We will explore the trade-offs between different
data structures and discuss their suitability for different types of problems. By selecting the right data
structures, you can optimize memory usage and access times, leading to faster and more efficient
solutions.
42

Tips and Tricks for Efficient Dynamic Programming
43
Example: Longest Common Subsequence
The Longest Common Subsequence problem involves finding the longest subsequence shared
by two sequences.
To solve this problem efficiently, you can use a two-dimensional array to
store intermediate results.
By carefully choosing the data structure, you can avoid redundant
computations and enable easy retrieval of previously computed values. This allows the Dynamic
Programming algorithm to efficiently compute the length of the longest common subsequence.
Space Optimization Techniques
Dynamic Programming solutions often involve maintaining a table or array to store intermediate
results.
However, in some cases, the space required to store all intermediate results can be
excessive. Space optimization techniques can help reduce the memory footprint of your Dynamic
Programming algorithm without compromising its correctness. We will explore techniques such
as state space reduction, matrix compression, and rolling arrays.
These techniques allow you
to optimize space usage and solve problems that would otherwise be impractical due to memory
constraints.
Example: Fibonacci Numbers
The Fibonacci Numbers problem involves computing the nth Fibonacci number. A naive Dynamic
Programming solution would store all intermediate results in an array, leading to a space complexity
of O(n). However, by using space optimization techniques, such as storing only the necessary
previous two Fibonacci numbers, the space complexity can be reduced to O(1). This optimization
significantly improves the space efficiency of the algorithm while still producing correct results.
Memoization and Caching Strategies
Memoization, or caching, is a technique that stores the results of expensive function calls and reuses
them when needed. In Dynamic Programming, memoization can be applied to avoid redundant
computations by storing intermediate results in a cache. We will discuss different memoization
strategies, such as top-down memoization and bottom-up tabulation, and explore their trade-offs.
By leveraging memoization, you can optimize the runtime of your Dynamic Programming algorithm
and avoid unnecessary recomputations.
Example: Fibonacci Numbers with Memoization
In the Fibonacci Numbers problem, memoization can be applied to store previously computed
Fibonacci numbers. By caching
the results, subsequent function calls can directly retrieve the cached values instead of recomputing
them. This technique reduces the time complexity of the algorithm from exponential to linear,
greatly improving its efficiency.

Tips and Tricks for Efficient Dynamic Programming
44
Debugging and Troubleshooting Dynamic
Programming Solutions
Developing Dynamic Programming solutions can be challenging, and it’s essential to have effective
debugging and troubleshooting strategies in place. We will explore common pitfalls and challenges
that arise when implementing Dynamic Programming algorithms and discuss techniques for
identifying and resolving issues. Understanding how to debug and troubleshoot your code will
help you refine your solutions, improve their efficiency, and ensure their correctness.
Example: Incorrect State Transition
One common issue in Dynamic Programming solutions is an incorrect state transition, where the
recurrence relation or the update rule for the DP table is not properly defined. This can lead to
incorrect results or infinite loops. By carefully examining the state transition logic and verifying it
against the problem requirements, you can identify and fix such issues, ensuring the correctness of
your algorithm.
Conclusion
In this chapter, we explored a range of tips and tricks for efficient Dynamic Programming solutions.
We discussed the importance of problem analysis and modeling, the selection of appropriate
data structures, space optimization techniques, memoization strategies, and effective debugging
and troubleshooting approaches. By applying these techniques, you can enhance the efficiency,
accuracy, and scalability of your Dynamic Programming solutions.
Dynamic Programming is a powerful tool that can unlock solutions to complex optimization
problems, and by mastering these tips and tricks, you can become a proficient problem solver.
Remember to analyze the problem thoroughly, choose the right data structures, optimize space
usage, leverage memoization, and employ effective debugging strategies.
With practice and
experience, you will develop a strong intuition for applying these techniques, enabling you to tackle
a wide range of problems efficiently and effectively.

Conclusion and Future Directions
Dynamic programming is a versatile problem-solving technique that has found applications in
diverse fields, ranging from computer vision and natural language processing to bioinformatics and
financial modeling. In this final chapter, we summarize the key concepts covered throughout this
book and provide a comprehensive overview of the current trends and emerging research areas
in dynamic programming. We also discuss the challenges and open problems that researchers
and practitioners are actively addressing. Finally, we provide a list of recommended resources
and references for further exploration.
Join us as we conclude this journey through dynamic
programming and explore the exciting possibilities that lie ahead.
Summary of Key Concepts
Throughout this book, we have explored the fascinating world of dynamic programming and its
wide-ranging applications. We began by understanding the fundamental concepts of dynamic pro-
gramming, including optimal substructure, overlapping subproblems, memoization, and tabulation
techniques. We then delved into advanced techniques such as state space reduction, bitmasking,
divide and conquer, multidimensional dynamic programming, approximation algorithms, online
dynamic programming, parallelization and distributed dynamic programming, reinforcement learn-
ing, and even quantum dynamic programming.
We learned how to analyze problems, choose appropriate data structures, optimize space usage,
leverage memoization, and effectively debug dynamic programming solutions.
Each chapter
provided in-depth explanations, examples, and code implementations in Java and Python to
reinforce the concepts and enhance your understanding.
Current Trends and Emerging Research Areas
The field of dynamic programming continues to evolve, with ongoing research uncovering new
applications and pushing the boundaries of what is possible. In this section, we highlight some of
the current trends and emerging research areas in dynamic programming. From the integration
of dynamic programming with machine learning to graph algorithms, computational biology, and
operations research, researchers are exploring new frontiers and finding innovative ways to apply
dynamic programming techniques. These developments pave the way for exciting advancements
and interdisciplinary collaborations.
45

Conclusion and Future Directions
46
1. Dynamic Programming in Machine Learning
The intersection of dynamic programming and machine learning has led to innovative algorithms for
reinforcement learning, sequence generation, and optimization problems. Researchers are exploring
how dynamic programming can be integrated with deep learning models to address complex tasks.
2. Dynamic Programming in Graph Algorithms
Graph algorithms pose unique challenges, and dynamic programming offers powerful tools for
solving graph-related problems efficiently. Recent research focuses on developing dynamic pro-
gramming techniques for graph traversal, shortest path problems, graph coloring, and network
analysis.
3. Dynamic Programming in Computational Biology
Bioinformatics and computational biology heavily rely on dynamic programming techniques to
analyze DNA sequences, protein structures, and evolutionary relationships. Current research in
this field involves developing advanced dynamic programming algorithms for sequence alignment,
genome assembly, and protein folding.
4. Dynamic Programming in Operations Research
Dynamic programming plays a crucial role in optimizing operations research problems, such
as resource allocation, scheduling, and inventory management. Researchers are exploring new
formulations and algorithms to handle large-scale problems and incorporate dynamic programming
into real-time decision-making processes.
Challenges and Open Problems
While dynamic programming has proven to be a powerful problem-solving paradigm, there are
still challenges and open problems that require further investigation. Scalability and efficiency
remain important concerns, particularly as the size and complexity of real-world problems continue
to increase. Handling uncertainty and stochastic environments poses another set of challenges,
requiring the development of dynamic programming techniques that can effectively deal with
probabilistic models. Additionally, multi-objective optimization within the dynamic programming
framework presents intriguing research opportunities, where multiple conflicting objectives need to
be optimized simultaneously.
Despite the extensive research and progress in dynamic programming, several challenges and open
problems remain. Some of these include:

Conclusion and Future Directions
47
1. Scalability and Efficiency
As problems become more complex and datasets grow larger, there is a constant need for more
scalable and efficient dynamic programming algorithms. Improving the time and space complexity
of existing techniques and designing novel algorithms to handle massive data sets are ongoing
challenges.
2. Handling Uncertainty and Stochastic Environments
Many real-world problems involve uncertainty and stochasticity. Incorporating probabilistic models
and developing dynamic programming techniques to handle uncertain environments is an active
research area. This includes problems in finance, resource management, and decision-making under
uncertainty.
3. Multi-objective Optimization
Dynamic programming traditionally focuses on single-objective optimization problems. Extending
dynamic programming techniques to handle multi-objective optimization, where multiple conflict-
ing objectives need to be optimized simultaneously, poses interesting challenges.
Resources and References
To continue exploring dynamic programming and its applications, here are some recommended
resources and references:
• Introduction to the Design and Analysis of Algorithms1 by Anany Levitin
• Dynamic Programming and Optimal Control2 by Dimitri P. Bertsekas
• Algorithms3 by Robert Sedgewick and Kevin Wayne
• The Algorithm Design Manual4 by Steven S. Skiena
• Research papers from prominent conferences and journals such as ACM Transactions on
Algorithms, Journal of the ACM, and IEEE Transactions on Pattern Analysis and Machine
Intelligence.
These materials provide in-depth knowledge, algorithms, and problem-solving strategies that can
further enhance your understanding of dynamic programming.
Whether you are a student,
researcher, or practitioner, these resources serve as valuable references to deepen your expertise
in this field.
1https://www.amazon.com/Introduction-Design-Analysis-Algorithms-Levitin/dp/027376411X
2https://www.athenasc.com/dpbook.html
3https://algs4.cs.princeton.edu/home/
4https://www.algorist.com/

Conclusion and Future Directions
48
In conclusion, dynamic programming is a powerful problem-solving technique that has revolu-
tionized various domains. By mastering the key concepts, exploring advanced techniques, and
staying updated with current trends, you can leverage dynamic programming to tackle complex
optimization problems efficiently.
The future of dynamic programming looks promising, with
ongoing research addressing scalability, uncertainty, and multi-objective optimization. Embrace the
challenges, continue learning, and unlock the potential of dynamic programming in your problem-
solving endeavors.

Part II: Dive into Dynamic
Programming Challenges
Welcome to the exciting realm of dynamic programming problem-solving! In this section, we will
explore a collection of dynamic programming problems and discover the step-by-step approach to
solving them. Each chapter follows a consistent structure, ensuring a smooth learning experience:
Unraveling the Problem We begin by presenting a clear and concise problem statement. This serves
as our compass, guiding us through the intricacies of the challenge. Understanding the problem is
crucial, as it allows us to define our goals and constraints effectively.
Illuminating the Solution Next, we embark on an enlightening journey of solution exploration. In
this section, we delve deep into the thought process behind approaching the problem. By unraveling
the underlying concepts and techniques, we unveil the secrets to solving similar problems that may
cross your path in job interviews or real-world coding scenarios.
Pseudo Code: A Roadmap to Success For certain problems, a well-defined pseudo code can be
immensely helpful in understanding the logical flow of the solution. We will provide pseudo code
when applicable, giving you a valuable roadmap to follow on your way to success. This concise
representation of the algorithmic steps acts as a guiding light, illuminating the path towards a robust
solution.
Implementations in Java and Python To solidify our understanding and demonstrate the practical
implementation of the solutions, we provide fully-fledged code examples in both Java and Python.
These examples showcase the power and versatility of dynamic programming techniques, providing
you with hands-on experience that can be applied to your own projects.
No Particular Order, Gradual Complexity In this challenges section, there is no particular order
in which the examples are presented. However, to ease your learning journey, we have categorized
the challenges based on their difficulty level. We will start with easy challenges, allowing you
to grasp the fundamental concepts of dynamic programming. As you progress, we will gradually
introduce medium-level challenges, providing you with an opportunity to enhance your problem-
solving skills. To truly test your mettle, we have also included some hard challenges at the end,
offering a real challenge to your abilities.
By following this structured approach, you not only gain access to ready-made solutions, but
you also acquire the mental agility required to tackle similar challenges independently. Embrace

50
the intricacies of dynamic programming, sharpen your problem-solving skills, and embark on a
rewarding journey through the world of algorithms and coding prowess. Let’s dive in and conquer
the dynamic programming landscape together!

Fibonacci Sequence
The Fibonacci sequence is a classic mathematical sequence defined as follows: each number in the
sequence is the sum of the two preceding ones. The sequence starts with 0 and 1. Formally, we can
define the Fibonacci sequence as:
F(0) = 0
F(1) = 1
F(n) = F(n-1) + F(n-2) for n > 1
Your task is to write a function that calculates the nth Fibonacci number.
Solution Approach
To efficiently calculate the nth Fibonacci number, we can utilize dynamic programming and avoid
redundant calculations. We will employ an iterative bottom-up approach to build the Fibonacci
sequence iteratively.
1. Initialize an array fib to store the Fibonacci numbers. We start with the base cases: fib[0] =
0 and fib[1] = 1.
2. Iterate from 2 to n to compute the Fibonacci numbers up to the desired number.
3. For each iteration, calculate the next Fibonacci number by summing the two preceding
Fibonacci numbers:
fib[i]
=
fib[i-1]
+
fib[i-2].
This step is the key to dynamic
programming, as we avoid recalculating Fibonacci numbers that have already been computed.
4. Finally, return fib[n] as the nth Fibonacci number.
The bottom-up approach allows us to build the Fibonacci sequence iteratively and store the
previously calculated Fibonacci numbers in the fib array.
By doing so, we avoid redundant
calculations and achieve an efficient solution.
The time complexity of this solution is O(n) since we iterate from 2 to n, calculating each Fibonacci
number only once. The space complexity is also O(n) since we need to store the Fibonacci numbers
in the fib array.
Using the provided pseudo code, you can implement the Fibonacci sequence solution in both Python
and Java.
Pseudo Code
51

Fibonacci Sequence
52
def fibonacci(n):
fib = [0, 1]
# Initialize the array with the base cases
for i in range(2, n + 1):
fib.append(fib[i - 1] + fib[i - 2])
# Compute the next Fibonacci number
return fib[n]
# Return the nth Fibonacci number
The time complexity of this solution is O(n) since we iterate from 2 to n, calculating each Fibonacci
number only once.
Solution in Java
public class Fibonacci {
public static int fibonacci(int n) {
int[] fib = new int[n + 1];
// Initialize the array with the base cases
fib[0] = 0;
fib[1] = 1;
for (int i = 2; i <= n; i++) {
fib[i] = fib[i - 1] + fib[i - 2];
// Compute the next Fibonacci number
}
return fib[n];
// Return the nth Fibonacci number
}
}
Solution in Python
def fibonacci(n):
fib = [0, 1]
# Initialize the array with the base cases
for i in range(2, n + 1):
fib.append(fib[i - 1] + fib[i - 2])
# Compute the next Fibonacci number
return fib[n]
# Return the nth Fibonacci number
By using dynamic programming, we can efficiently calculate the Fibonacci sequence and avoid
redundant calculations.
The bottom-up approach allows us to build the sequence iteratively,
resulting in a time complexity of O(n) for this solution.

Fibonacci with Memoization
Calculate the nth Fibonacci number using the memoization technique.
Solution/Approach Explanation
The Fibonacci sequence is a well-known mathematical sequence where each number is the sum
of the two preceding ones. In the context of dynamic programming, we can solve this problem
efficiently using the memoization technique.
The memoization technique involves storing the results of previously calculated Fibonacci numbers
in a data structure (often an array or a hashmap) to avoid redundant calculations. By storing the
computed values, we can retrieve them later without recalculating, significantly improving the
efficiency of our solution.
To solve the Fibonacci sequence with memoization, we follow these steps:
1. Initialize a memoization table of size n+1.
This table will store the computed Fibonacci
numbers. Initially, all entries in the table are set to a special value, typically -1 or None, to
indicate that the value has not been calculated yet.
2. Define a recursive helper function, often named fibonacciMemoHelper or a similar name, that
takes two parameters: the index n and the memoization table.
3. In the helper function, check if the Fibonacci number for the given index n is already present
in the memoization table. If it is, return the stored value. This serves as the base case of our
recursion, preventing redundant calculations.
4. If the value is not present in the memoization table, calculate the Fibonacci number using
the recursive formula F(n) = F(n-1) + F(n-2). To do this, make recursive calls to the helper
function for the indices n-1 and n-2, passing the memoization table along. These recursive calls
will ensure that all necessary Fibonacci numbers are computed and stored in the memoization
table.
5. Once the Fibonacci number is calculated, store it in the memoization table at the corresponding
index n.
6. Finally, return the calculated Fibonacci number.
By utilizing the memoization technique, we eliminate redundant calculations and ensure that each
Fibonacci number is computed only once. This leads to a significant improvement in the time
complexity of our solution, making it more efficient compared to the naive recursive approach.
With this approach, we can solve the Fibonacci sequence with a time complexity of O(n), where n is
the desired index of the Fibonacci number to be calculated. The memoization technique ensures that
each Fibonacci number is computed once and stored for future use, leading to optimal performance.
53

Fibonacci with Memoization
54
Pseudo Code
function fibonacciMemo(n):
memo = [None] * (n + 1)
return fibonacciMemoHelper(n, memo)
function fibonacciMemoHelper(n, memo):
if memo[n] is not None:
return memo[n]
if n <= 1:
memo[n] = n
else:
memo[n] = fibonacciMemoHelper(n - 1, memo) + fibonacciMemoHelper(n - 2, memo)
return memo[n]
Time Complexity: The time complexity of this solution is O(n), where n is the desired Fibonacci
number to be calculated.
The memoization technique ensures that each Fibonacci number is
calculated only once, resulting in a significant reduction in redundant calculations.
Solution in Java
import java.util.Arrays;
public class FibonacciMemoization {
public static int fibonacciMemo(int n) {
int[] memo = new int[n + 1];
Arrays.fill(memo, -1);
return fibonacciMemoHelper(n, memo);
}
private static int fibonacciMemoHelper(int n, int[] memo) {
if (memo[n] != -1) {
return memo[n];
}
if (n <= 1) {
memo[n] = n;
} else {
memo[n] = fibonacciMemoHelper(n - 1, memo) + fibonacciMemoHelper(n - 2, \

Fibonacci with Memoization
55
memo);
}
return memo[n];
}
}
Solution in Python
def fibonacci_memo(n):
memo = [-1] * (n + 1)
return fibonacci_memo_helper(n, memo)
def fibonacci_memo_helper(n, memo):
if memo[n] != -1:
return memo[n]
if n <= 1:
memo[n] = n
else:
memo[n] = fibonacci_memo_helper(n - 1, memo) + fibonacci_memo_helper(n - 2, \
memo)
return memo[n]
In both the Java and Python solutions, we initialize a memoization table (memo) of size n+1 to store the
computed Fibonacci numbers. The initial values are set to -1, indicating that the Fibonacci numbers
have not been calculated yet.
The fibonacciMemo (Java) or fibonacci_memo (Python) function serves as a wrapper function that
calls the fibonacciMemoHelper (Java) or fibonacci_memo_helper (Python) recursive helper function.
This helper function performs the actual computation, checking if the Fibonacci number for the
given index n is already present in the memoization table. If it is, the stored value is returned.
Otherwise, the function calculates the Fibonacci number using the recursive formula F(n) = F(n-1)
+ F(n-2), stores it in the memoization table, and returns the result.
Both the Java and Python solutions have a time complexity of O(n), thanks to the memoization
technique. The memoization table ensures that each Fibonacci number is computed only once,
leading to a significant improvement in performance compared to the naive recursive approach.
Using these solutions, you can easily calculate the nth Fibonacci number efficiently by calling the
fibonacciMemo (Java)
or fibonacci_memo (Python) function with the desired value of n.

Last Digit of Fibonacci Number
Given a non-negative integer n, find the last digit of the nth Fibonacci number.
Solution/Approach Explanation
To solve the “Last Digit of Fibonacci Number” challenge, we can take advantage of a property of
the Fibonacci sequence: the last digit of each Fibonacci number depends only on the last digits of
its two preceding numbers. This property allows us to calculate the last digit of Fibonacci numbers
efficiently without the need to compute the entire sequence.
The approach to solving this challenge involves iteratively calculating the Fibonacci numbers while
keeping track of only the last digits. We can use a loop to iterate from 2 to n and update the last
digit of Fibonacci numbers in each iteration.
Let’s break down the solution approach:
1. Initialize two variables, a and b, to represent the last digits of the first two Fibonacci numbers.
Typically, a is set to 0 and b to 1.
2. Iterate from 2 to n, updating a and b in each iteration. At each step, calculate the next Fibonacci
number by summing the last digits of a and b, and store it in a temporary variable.
3. Update a and b with the last digits of the previous two Fibonacci numbers. a becomes the last
digit of b, and b becomes the last digit of the temporary variable.
4. Repeat the above steps until the loop reaches n.
5. After the loop completes, the last digit of the nth Fibonacci number is stored in b.
Pseudo Code
56

Last Digit of Fibonacci Number
57
function lastDigitFibonacci(n):
if n <= 1:
return n
a = 0
b = 1
for i from 2 to n:
temp = (a + b) modulo 10
a = b
b = temp
return b
In this pseudo-code, the lastDigitFibonacci function takes an integer n as input and returns the
last digit of the nth Fibonacci number. It uses two variables, a and b, to keep track of the last digits
of the Fibonacci numbers. The loop iterates from 2 to n, calculating the next Fibonacci number
by summing the last digits of a and b. The temporary variable temp stores the new last digit, and
the values of a and b are updated accordingly. Finally, the function returns the value of b, which
represents the last digit of the nth Fibonacci number.
Solution in Java
int lastDigitFibonacci(int n) {
if (n <= 1) {
return n;
}
int a = 0;
int b = 1;
for (int i = 2; i <= n; i++) {
int temp = (a + b) % 10;
a = b;
b = temp;
}
return b;
}
Solution in Python

Last Digit of Fibonacci Number
58
function lastDigitFibonacci(n):
if n <= 1:
return n
a = 0
b = 1
for i in range(2, n + 1):
temp = (a + b) % 10
a = b
b = temp
return b
By iteratively calculating the last digits of the Fibonacci numbers, we avoid the need to compute the
entire sequence. This approach significantly reduces the time complexity of the solution.
The time complexity of this solution is O(n), where n is the input number representing the index of
the Fibonacci number to be calculated. The loop iterates n times to calculate the last digit of the nth
Fibonacci number.
Both the provided Java and Python solutions follow this approach to efficiently determine the last
digit of the Fibonacci number. By only tracking the last digits and performing modular arithmetic,
we achieve an optimized solution with linear time complexity.

Longest Subsequence with Equal
Elements
Given an array of integers, find the length of the longest subsequence in which all the elements are
the same.
Example
Input:
[2, 4, 4, 4, 6, 6, 7, 7, 7, 7, 8]
Output:
4
Explanation: The longest subsequence with equal elements is [7, 7, 7, 7], which has a length of
4.
Approach
To solve this problem, we can iterate through the array and keep track of the longest subse-
quence with equal elements encountered so far. We will use two variables, currentElement and
currentCount, to keep track of the current element being processed and the count of consecutive
elements that are equal to currentElement. Additionally, we will maintain two more variables,
maxElement and maxCount, to store the element and count of the longest subsequence found.
1. Initialize maxElement and maxCount to the first element in the array.
2. Initialize currentElement and currentCount to the first element in the array.
3. Iterate through the array starting from the second element:
• If the current element is equal to currentElement, increment currentCount by 1.
• If the current element is different from currentElement, update currentElement and
currentCount to the current element and 1 respectively.
• If currentCount is greater than maxCount, update maxElement and maxCount to currentElement
and currentCount respectively.
4. At the end of the iteration, maxCount will contain the length of the longest subsequence with
equal elements, and maxElement will represent the element in that subsequence.
59

Longest Subsequence with Equal Elements
60
Pseudo Code
function longestSubsequenceWithEqualElements(arr):
n = length of arr
if n == 0:
return 0
maxElement = arr[0]
maxCount = 1
currentElement = arr[0]
currentCount = 1
for i from 1 to n-1:
if arr[i] == currentElement:
currentCount = currentCount + 1
else:
currentElement = arr[i]
currentCount = 1
if currentCount > maxCount:
maxElement = currentElement
maxCount = currentCount
return maxCount
The time complexity of this solution is O(n), where n is the length of the input array. This is because
we iterate through the array once to find the longest subsequence with equal elements. The space
complexity is O(1) as we only use a constant amount of additional space to store the variables
representing the current and maximum counts and elements.
Java Solution

Longest Subsequence with Equal Elements
61
public class LongestSubsequenceWithEqualElements {
public static int longestSubsequenceWithEqualElements(int[] arr) {
int n = arr.length;
if (n == 0) {
return 0;
}
int maxElement = arr[0];
int maxCount = 1;
int currentElement = arr[0];
int currentCount = 1;
for (int i = 1; i < n; i++) {
if (arr[i] == currentElement) {
currentCount++;
} else {
currentElement = arr[i];
currentCount = 1;
}
if (currentCount > maxCount) {
maxElement = currentElement;
max
Count = currentCount;
}
}
return maxCount;
}
public static void main(String[] args) {
int[] arr = {2, 4, 4, 4, 6, 6, 7, 7, 7, 7, 8};
int longestSubsequenceLength = longestSubsequenceWithEqualElements(arr);
System.out.println("Length of the longest subsequence with equal elements: "\
+ longestSubsequenceLength);
}
}
Python Solution

Longest Subsequence with Equal Elements
62
def longest_subsequence_with_equal_elements(arr):
n = len(arr)
if n == 0:
return 0
max_element = arr[0]
max_count = 1
current_element = arr[0]
current_count = 1
for i in range(1, n):
if arr[i] == current_element:
current_count += 1
else:
current_element = arr[i]
current_count = 1
if current_count > max_count:
max_element = current_element
max_count = current_count
return max_count
arr = [2, 4, 4, 4, 6, 6, 7, 7, 7, 7, 8]
longest_subsequence_length = longest_subsequence_with_equal_elements(arr)
print("Length of the longest subsequence with equal elements:", longest_subsequence_\
length)
In this chapter, we explored the problem of finding the length of the longest subsequence with equal
elements in an array. We discussed a step-by-step approach, provided the corresponding pseudo
code, and implemented the solution in both Java and Python. By understanding the problem and
following the outlined approach, you can effectively solve similar challenges that require identifying
and analyzing subsequences in arrays.

Longest Subarray with Equal 0s and 1s
Given an array of integers containing only 0s and 1s, find the length of the longest subarray with
an equal number of 0s and 1s.
Input: An array nums of size n (1 ≤n ≤10^5), where each element nums[i] is either 0 or 1.
Output: The length of the longest subarray with an equal number of 0s and 1s.
Approach
To solve this problem efficiently, we can utilize the concept of prefix sum and hash maps.
1. Create a variable maxLen and set it to 0. This variable will store the maximum length of the
subarray with equal 0s and 1s.
2. Initialize a variable prefixSum to 0. This variable will keep track of the cumulative sum of the
elements in the array.
3. Create an empty hash map, sumMap, to store the cumulative sum as keys and their corresponding
indices as values.
4. Iterate through the array nums from left to right and do the following for each element:
• If nums[i] is 0, decrement prefixSum by 1.
• If nums[i] is 1, increment prefixSum by 1.
• If prefixSum is 0, it means we have an equal number of 0s and 1s from index 0 to the
current index i. Update maxLen to i + 1 since the subarray from index 0 to i satisfies the
condition.
• If prefixSum is already present in sumMap, it means we have seen this sum before. Calculate
the length of the current subarray as i - sumMap[prefixSum] and update maxLen if it is
greater.
• If prefixSum is not present in sumMap, add it to the map with the current index i as the
value.
5. Return maxLen as the result.
Pseudo Code
63

Longest Subarray with Equal 0s and 1s
64
function findLongestSubarray(nums):
maxLen = 0
prefixSum = 0
sumMap = empty map
for i = 0 to length(nums) - 1:
if nums[i] == 0:
prefixSum -= 1
else:
prefixSum += 1
if prefixSum == 0:
maxLen = i + 1
else if prefixSum in sumMap:
maxLen = max(maxLen, i - sumMap[prefixSum])
if prefixSum not in sumMap:
sumMap[prefixSum] = i
return maxLen
The time complexity of this solution is O(n), where n is the size of the input array nums. This
is because we traverse the array once and perform constant-time operations for each element.
Additionally, the space complexity is O(n) since the maximum number of elements stored in the
hash map sumMap is equal to the size of the array.
By applying this approach, we can efficiently find the length of the longest subarray with an equal
number of 0s and 1s in linear time complexity.
Keep in mind that the provided pseudo code is a language-independent representation of the
solution. You can implement the algorithm in the programming language of your choice based
on this pseudo code.
Java Solution

Longest Subarray with Equal 0s and 1s
65
import java.util.HashMap;
import java.util.Map;
public class LongestSubarrayWithEqualZerosAndOnes {
public static int findLongestSubarray(int[] nums) {
int maxLen = 0;
int prefixSum = 0;
Map<Integer, Integer> sumMap = new HashMap<>();
for (int i = 0; i < nums.length; i++) {
if (nums[i] == 0) {
prefixSum -= 1;
} else {
prefixSum += 1;
}
if (prefixSum == 0) {
maxLen = i + 1;
} else if (sumMap.containsKey(prefixSum)) {
maxLen = Math.max(maxLen, i - sumMap.get(prefixSum));
}
if (!sumMap.containsKey(prefixSum)) {
sumMap.put(prefixSum, i);
}
}
return maxLen;
}
public static void main(String[] args) {
int[] nums = {0, 1, 0, 1, 1, 0, 0};
int longestSubarrayLength = findLongestSubarray(nums);
System.out.println("Longest Subarray Length: " + longestSubarrayLength);
}
}
Python Solution

Longest Subarray with Equal 0s and 1s
66
def find_longest_subarray(nums):
max_len = 0
prefix_sum = 0
sum_map = {}
for i in range(len(nums)):
if nums[i] == 0:
prefix_sum -= 1
else:
prefix_sum += 1
if prefix_sum == 0:
max_len = i + 1
elif prefix_sum in sum_map:
max_len = max(max_len, i - sum_map[prefix_sum])
if prefix_sum not in sum_map:
sum_map[prefix_sum] = i
return max_len
nums = [0, 1, 0, 1, 1, 0, 0]
longest_subarray_length = find_longest_subarray(nums)
print("Longest Subarray Length:", longest_subarray_length)
Both solutions follow the same approach explained earlier. You can run these code snippets in Java
and Python, respectively, to find the length of the longest subarray with an equal number of 0s and
1s in the given input array.

Longest Common Subsequence
Given two strings, str1 and str2, find the length of their longest common subsequence (LCS).
A subsequence is a sequence that can be derived from another sequence by deleting some or
no elements, without changing the order of the remaining elements. For example, “ACE” is a
subsequence of “ABCDE”, but “AEC” is not.
Approach
To find the longest common subsequence of two strings, we can use dynamic programming. We’ll
create a 2D table, dp, of size (len(str1) + 1) x (len(str2) + 1), where dp[i][j] represents the
length of the LCS of the substrings str1[0:i] and str2[0:j].
We’ll iterate through the characters of both strings and update the table accordingly. If the characters
at str1[i-1] and str2[j-1] are the same, we can extend the LCS by 1 from the previous length, so
dp[i][j] = dp[i-1][j-1] + 1. Otherwise, we’ll take the maximum length from the previous row
(dp[i-1][j]) and the previous column (dp[i][j-1]), so dp[i][j] = max(dp[i-1][j], dp[i][j-1]).
Finally, the value at dp[len(str1)][len(str2)] will represent the length of the LCS.
Pseudo Code
function findLongestCommonSubsequence(str1, str2):
n1 = length of str1
n2 = length of str2
// Create a 2D table
dp = new Array(n1+1, n2+1)
// Initialize the table with 0s
for i = 0 to n1:
dp[i][0] = 0
for j = 0 to n2:
dp[0][j] = 0
// Fill the table using dynamic programming
for i = 1 to n1:
for j = 1 to n2:
67

Longest Common Subsequence
68
if str1[i-1] == str2[j-1]:
dp[i][j] = dp[i-1][j-1] + 1
else:
dp[i][j] = max(dp[i-1][j], dp[i][j-1])
// Return the length of the LCS
return dp[n1][n2]
The time complexity of the above solution is O(n1 * n2), where n1 and n2 are the lengths of str1
and str2, respectively.
Java Solution
public class LongestCommonSubsequence {
public static int findLongestCommonSubsequence(String str1, String str2) {
int n1 = str1.length();
int n2 = str2.length();
int[][] dp = new int[n1 + 1][n2 + 1];
for (int i = 0; i <= n1; i++) {
dp[i][0] = 0;
}
for (int j = 0; j <= n2; j++) {
dp[0][j] = 0;
}
for (int i = 1; i <= n1; i++) {
for (int j
= 1; j <= n2; j++) {
if (str1.charAt(i - 1) == str2.charAt(j - 1)) {
dp[i][j] = dp[i - 1][j - 1] + 1;
} else {
dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);
}
}
}
return dp[n1][n2];
}

Longest Common Subsequence
69
public static void main(String[] args) {
String str1 = "ABCDGH";
String str2 = "AEDFHR";
int longestCommonSubsequence = findLongestCommonSubsequence(str1, str2);
System.out.println("Length of Longest Common Subsequence: " + longestCommonS\
ubsequence);
}
}
Python Solution
def find_longest_common_subsequence(str1, str2):
n1 = len(str1)
n2 = len(str2)
dp = [[0] * (n2 + 1) for _ in range(n1 + 1)]
for i in range(n1 + 1):
dp[i][0] = 0
for j in range(n2 + 1):
dp[0][j] = 0
for i in range(1, n1 + 1):
for j in range(1, n2 + 1):
if str1[i - 1] == str2[j - 1]:
dp[i][j] = dp[i - 1][j - 1] + 1
else:
dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
return dp[n1][n2]
str1 = "ABCDGH"
str2 = "AEDFHR"
longest_common_subsequence = find_longest_common_subsequence(str1, str2)
print("Length of Longest Common Subsequence:", longest_common_subsequence)
Both the Java and Python solutions follow the same dynamic programming approach explained

Longest Common Subsequence
70
earlier. You can use these solutions to find the length of the longest common subsequence between
two given strings.

Minimum Subset Sum Difference
Given a set of positive integers, we need to partition the set into two subsets such that the difference
between the sums of the subsets is minimized. We want to find the minimum possible difference.
Input: An array of positive integers.
Output: The minimum difference between the sums of two subsets.
Example:
Input: [1, 6, 11, 5]
Output: 1
Explanation: The minimum difference can be achieved by partitioning the set into [1,\
5] and [6, 11].
Approach
To solve this problem, we can utilize the dynamic programming approach.
Let’s consider the
following steps:
1. Calculating the Total Sum: Compute the total sum of all elements in the given array. Denote
it as totalSum.
2. Creating the Subset Sum Matrix: Construct a 2D boolean matrix, dp, with dimensions n+1 x
(totalSum/2)+1, where n is the size of the input array. This matrix will help us track which
subset sums can be achieved.
3. Initializing the Matrix: Initialize the first column of the matrix as True, indicating that it is
possible to achieve a sum of 0 for any subset by not selecting any element. The first row, except
for the first cell, will be initialized as False.
4. Filling the Subset Sum Matrix: Iterate through the elements of the array and for each element
arr[i], iterate through the possible subset sums from 1 to totalSum/2. For each subset sum j,
check if it is possible to achieve the sum j by either including or excluding the current element
arr[i]. If it is possible, mark dp[i][j] as True. Otherwise, mark it as False.
5. Finding the Minimum Difference: Start from the last column of the last row (dp[n][totalSum/2])
and move upwards. For each cell dp[i][j], if it is True, it means we can achieve the sum j using
some elements from the first i elements of the array. We can update the minimum difference
by taking the difference between totalSum - 2 * j and the current minimum difference.
6. Return the Minimum Difference: After traversing through all cells, return the minimum
difference.
The time complexity of this solution is O(n * totalSum/2), where n is the size of the input array and
totalSum is the sum of all elements in the array.
71

Minimum Subset Sum Difference
72
Pseudo Code
function minimumSubsetSumDifference(arr):
totalSum = calculateTotalSum(arr)
n = arr.length
dp = new 2D boolean array with dimensions (n+1) x (totalSum/2+1)
for i from 0 to n:
dp[i][0] = true
for i from 1 to n:
for j from 1 to totalSum/2:
dp[i][j] = dp[i-1][j]
if arr[i-1] <= j:
dp[i][j] = dp[i][j] || dp[i-1][j-arr[i-1]]
minDiff = infinity
for j from totalSum/2 to 0:
if dp[n][j]:
minDiff = min(minDiff, totalSum - 2*j)
return minDiff
Java Solution
public class MinimumSubsetSumDifference {
public static int minimumSubsetSumDifference
(int[] arr) {
int totalSum = calculateTotalSum(arr);
int n = arr.length;
boolean[][] dp = new boolean[n + 1][(totalSum / 2) + 1];
for (int i = 0; i <= n; i++) {
dp[i][0] = true;
}
for (int i = 1; i <= n; i++) {
for (int j = 1; j <= totalSum / 2; j++) {
dp[i][j] = dp[i - 1][j];
if (arr[i - 1] <= j) {

Minimum Subset Sum Difference
73
dp[i][j] = dp[i][j] || dp[i - 1][j - arr[i - 1]];
}
}
}
int minDiff = Integer.MAX_VALUE;
for (int j = totalSum / 2; j >= 0; j--) {
if (dp[n][j]) {
minDiff = Math.min(minDiff, totalSum - 2 * j);
}
}
return minDiff;
}
private static int calculateTotalSum(int[] arr) {
int sum = 0;
for (int num : arr) {
sum += num;
}
return sum;
}
public static void main(String[] args) {
int[] arr = {1, 6, 11, 5};
int result = minimumSubsetSumDifference(arr);
System.out.println("Minimum Subset Sum Difference: " + result);
}
}
Python Solution

Minimum Subset Sum Difference
74
def minimum_subset_sum_difference(arr):
total_sum = sum(arr)
n = len(arr)
dp = [[False] * ((total_sum // 2) + 1) for _ in range(n + 1)]
for i in range(n + 1):
dp[i][0] = True
for i in range(1, n + 1):
for j in range(1, (total_sum // 2) + 1):
dp[i][j] = dp[i - 1][j]
if arr[i - 1] <= j:
dp[i][j] = dp[i][j] or dp[i - 1][j - arr[i - 1]]
min_diff = float('inf')
for j in range(total_sum // 2, -1, -1):
if dp[n][j]:
min_diff = min(min_diff, total_sum - 2 * j)
return min_diff
arr = [1, 6, 11, 5]
result = minimum_subset_sum_difference(arr)
print("Minimum Subset Sum Difference:", result)
In this chapter, we have explored the “Minimum Subset Sum Difference” challenge. We discussed
the problem statement, presented a detailed approach using dynamic programming, provided the
pseudo code with the time complexity analysis, and presented the Java and Python solutions. By
applying the dynamic programming approach, we can efficiently solve this problem and find the
minimum difference between two subsets.

Valid Parentheses
Given a string containing only parentheses ( ‘(’ and ‘)’), we need to determine if the string is valid.
A valid string has each opening parenthesis ‘(’ followed by a corresponding closing parenthesis ‘)’.
Input: A string containing only parentheses.
Output: Return true if the string is valid, and false otherwise.
Example:
Input: "()((()()))"
Output: true
Explanation: The given string contains valid parentheses pairs and is therefore vali\
d.
Approach
To solve this problem, we can utilize a stack-based approach. Let’s consider the following steps:
1. Initialize a Stack: Create an empty stack to store the opening parentheses.
2. Traverse the String: Iterate through each character in the given string.
• If the current character is an opening parenthesis ‘(’, push it onto the stack.
• If the current character is a closing parenthesis ‘)’ and the stack is not empty, pop the
topmost element from the stack.
This indicates that a pair of parentheses has been
balanced.
• If the current character is a closing parenthesis ‘)’ and the stack is empty, or the top of the
stack does not correspond to the current closing parenthesis, return false as the string is
invalid.
3. Return the Result: After traversing the entire string, if the stack is empty, return true,
indicating that the string contains valid parentheses pairs. Otherwise, return false, indicating
an invalid string.
The time complexity of this solution is O(n), where n is the length of the input string.
Pseudo Code
75

Valid Parentheses
76
function isValidParentheses(s):
stack = empty stack
for i from 0 to length(s)-1:
if s[i] == '(':
stack.push(s[i])
else if s[i] == ')' and stack is not empty:
stack.pop()
else:
return false
return stack.isEmpty()
Java Solution
import java.util.Stack;
public class ValidParentheses {
public static boolean isValidParentheses(String s) {
Stack<Character> stack = new Stack<>();
for (int i = 0; i < s.length(); i++) {
char c = s.charAt(i);
if (c == '(') {
stack.push(c);
} else if (c == ')' && !stack.isEmpty()) {
stack.pop();
} else {
return false;
}
}
return stack.isEmpty();
}
public static void main(String[] args) {
String s = "()((()()))";
boolean isValid = isValidParentheses(s);
System.out.println("Is Valid Parentheses: " + isValid);
}
}

Valid Parentheses
77
Python Solution
def is_valid_parentheses(s):
stack = []
for char in s:
if char == '(':
stack.append(char)
elif char == ')' and stack:
stack.pop()
else:
return False
return len(stack) == 0
s = "()((()()))"
isValid = is_valid_parentheses(s)
print("Is Valid Parentheses:", isValid)
In this chapter, we have explored the “Valid Parentheses” challenge. We discussed the problem
statement, presented a stack-based approach, provided the pseudo code with the time complexity
analysis, and presented the Java and Python solutions. By using a stack to keep track of the balanced
parentheses, we can determine if a given string contains valid parentheses pairs.

Minimum Number of Parentheses
Given a string containing only parentheses ( ‘(’ and ‘)’), we need to determine the minimum number
of parentheses to be removed in order to make the string balanced. A balanced string has each
opening parenthesis ‘(’ followed by a corresponding closing parenthesis ‘)’.
Input: A string containing only parentheses.
Output: The minimum number of parentheses to be removed.
Example:
Input: ")(())("
Output: 2
Explanation: By removing the parentheses at indices 0 and 4, the string can be balan\
ced as "(())".
Approach
To solve this problem, we can utilize a stack-based approach. Let’s consider the following steps:
1. Initialize a Stack: Create an empty stack to store the opening parentheses.
2. Traverse the String: Iterate through each character in the given string.
• If the current character is an opening parenthesis ‘(’, push it onto the stack.
• If the current character is a closing parenthesis ‘)’ and the stack is not empty, pop the
topmost element from the stack.
This indicates that a pair of parentheses has been
balanced.
• If the current character is a closing parenthesis ‘)’ and the stack is empty, increment a
counter to track the number of unbalanced parentheses.
3. Return the Result: After traversing the entire string, the number of unbalanced parentheses
is equal to the size of the stack. Return this value as the minimum number of parentheses to
be removed.
The time complexity of this solution is O(n), where n is the length of the input string.
Pseudo Code
78

Minimum Number of Parentheses
79
function minimumParenthesesRemoval(s):
stack = empty stack
unbalancedCount = 0
for i from 0 to length(s)-1:
if s[i] == '(':
stack.push(s[i])
else if s[i] == ')' and stack is not empty:
stack.pop()
else:
unbalancedCount += 1
return stack.size() + unbalancedCount
Java Solution
import java.util.Stack;
public class MinimumParenthesesRemoval {
public static int minimumParenthesesRemoval(String s) {
Stack<Character> stack = new Stack<>();
int unbalancedCount = 0;
for (int i = 0; i < s.length(); i++) {
char c = s.charAt(i);
if (c == '(') {
stack.push(c);
} else if (c == ')' && !stack.isEmpty()) {
stack.pop();
} else {
unbalancedCount++;
}
}
return stack.size() + unbalancedCount;
}
public static void main(String[] args) {
String s = ")(())(";
int result = minimumParenthesesRemoval(s);
System.out.println("Minimum Parentheses Removal: " + result);

Minimum Number of Parentheses
80
}
}
Python Solution
def minimum_parentheses_removal(s):
stack = []
unbalanced_count = 0
for char in s:
if char == '(':
stack.append(char)
elif char == ')' and stack:
stack.pop()
else:
unbalanced_count += 1
return len(stack) + unbalanced_count
s = ")(())("
result = minimum_parentheses_removal(s)
print("Minimum Parentheses Removal:", result)
In this chapter, we have explored the “Minimum Number of Parentheses” challenge. We discussed
the problem statement, presented a stack-based approach, provided the pseudo code with the time
complexity analysis, and presented the Java and Python solutions. By using a stack to keep track
of the balanced parentheses, we can determine the minimum number of parentheses to be removed
and ensure the string is balanced.

Find the Element that Appears Once
You are given an array of integers, where each element appears twice except for one element that
appears only once. Your task is to find and return the element that appears only once.
For example, given the array [4, 3, 2, 4, 1, 3, 2], the element 1 appears only once, so the output
would be 1.
Write a function findSingle to solve this problem. The function should take in an array of integers
and return the element that appears only once.
Approach
To solve this problem efficiently, we can utilize the properties of bitwise XOR (exclusive OR)
operation. XOR of two identical numbers is 0, and XOR of 0 with any number x gives x.
1. Initialize a variable result to 0.
2. Iterate through each element num in the given array.
• Update result by performing bitwise XOR between result and num.
3. After the iteration, the value of result will be the element that appears only once.
The XOR operation cancels out the duplicate occurrences of the same element, leaving behind the
element that appears only once.
Pseudo Code
function findSingle(arr):
result = 0
for num in arr:
result = result XOR num
return result
Time Complexity Analysis
The time complexity of this approach is O(n), where n is the size of the input array. This is because
we iterate through each element in the array once.
81

Find the Element that Appears Once
82
Solution in Java
public class FindSingleElement {
public static int findSingle(int[] arr) {
int result = 0;
for (int num : arr) {
result ^= num;
}
return result;
}
public static void main(String[] args) {
int[] arr = {4, 3, 2, 4, 1, 3, 2};
int singleElement = findSingle(arr);
System.out.println("Element that appears once: " + singleElement);
}
}
Solution in Python
def findSingle(arr):
result = 0
for num in arr:
result ^= num
return result
arr = [4, 3, 2, 4, 1, 3, 2]
single_element = findSingle(arr)
print("Element that appears once:", single_element)
By utilizing the XOR operation, we can efficiently identify the element that appears only once in
the given array. This approach offers a time complexity of O(n) and guarantees a correct solution.

Find the Missing Number
You are given an array of integers from 1 to n, but one number is missing. Your task is to find and
return the missing number.
For example, given the array [1, 3, 4, 5, 2, 7, 6], the missing number is 8, so the output would
be 8.
Write a function findMissingNumber to solve this problem. The function should take in an array of
integers and return the missing number.
Approach
To solve this problem, we can utilize the property of the XOR (exclusive OR) operation. XOR of two
identical numbers is 0, and XOR of 0 with any number x gives x.
1. Initialize a variable result to 0.
2. Iterate through each element num in the given array.
• Update result by performing bitwise XOR between result and num.
3. After the iteration, perform bitwise XOR between result and all the numbers from 1 to n
(inclusive).
4. The resulting value will be the missing number.
The XOR operation cancels out the duplicate numbers, leaving behind the missing number in the
array.
Pseudo Code
83

Find the Missing Number
84
function findMissingNumber(arr):
n = length(arr) + 1
result = 0
for num in arr:
result = result XOR num
for i in range(1, n+1):
result = result XOR i
return result
Time Complexity Analysis
The time complexity of this approach is O(n), where n is the size of the input array. This is because
we iterate through each element in the array once and perform XOR operations.
Solution in Java
public class FindMissingNumber {
public static int findMissingNumber(int[] arr) {
int n = arr.length + 1;
int result = 0;
for (int num : arr) {
result ^= num;
}
for (int i = 1; i <= n; i++) {
result ^= i;
}
return result;
}
public static void main(String[] args) {
int[] arr = {1, 3, 4, 5, 2, 7, 6};
int missingNumber = findMissingNumber(arr);
System.out.println("Missing number: " + missingNumber);
}
}
Solution in Python

Find the Missing Number
85
def findMissingNumber(arr):
n = len(arr) + 1
result = 0
for num in arr:
result ^= num
for i in range(1, n+1):
result ^= i
return result
arr = [1, 3, 4, 5, 2, 7, 6]
missing_number = findMissingNumber(arr)
print("Missing number:", missing_number)
By utilizing the XOR operation and considering the range of numbers from 1 to n, we can efficiently
identify the missing number in the given array. This approach offers a time complexity of O(n) and
guarantees a correct solution.

Count Set Bits (Brian Kernighan’s
Algorithm)
You are given a non-negative integer n. Your task is to count the number of set bits (bits with a value
of 1) in the binary representation of n using Brian Kernighan’s algorithm.
For example, given n = 13, the binary representation is 1101, and there are 3 set bits. So the output
would be 3.
Write a function countSetBits to solve this problem. The function should take in an integer n and
return the count of set bits.
Approach
Brian Kernighan’s algorithm is an efficient method to count the number of set bits in an integer.
The algorithm works as follows:
1. Initialize a variable count to 0.
2. While n is greater than 0:
• Increment count by 1.
• Update n by performing bitwise AND between n and n-1.
3. Return count.
In each iteration, n & (n-1) turns off the rightmost set bit of n. By repeatedly applying this operation,
we can count the total number of set bits in n.
Pseudo Code
86

Count Set Bits (Brian Kernighan’s Algorithm)
87
function countSetBits(n):
count = 0
while n > 0:
count += 1
n &= (n - 1)
return count
Time Complexity Analysis
The time complexity of this algorithm is O(k), where k is the number of set bits in the binary
representation of n. Since n can have at most log₂(n) + 1 bits, the time complexity can be considered
O(log n).
Solution in Java
public class CountSetBits {
public static int countSetBits(int n) {
int count = 0;
while (n > 0) {
count++;
n &= (n - 1);
}
return count;
}
public static void main(String[] args) {
int n = 13;
int setBitsCount = countSetBits(n);
System.out.println("Number of set bits: " + setBitsCount);
}
}
Solution in Python

Count Set Bits (Brian Kernighan’s Algorithm)
88
def countSetBits(n):
count = 0
while n > 0:
count += 1
n &= (n - 1)
return count
n = 13
set_bits_count = countSetBits(n)
print("Number of set bits:", set_bits_count)
Brian Kernighan’s algorithm provides an efficient way to count the number of set bits in an integer
using bitwise operations. With a time complexity of O(log n), it offers a fast and reliable solution to
the problem.

Generate All Subsets
Given a set of distinct integers, write a function to generate all possible subsets. The subsets should
not contain duplicate elements.
For example, given the set {1, 2, 3}, the possible subsets are:
[], [1], [2], [3], [1, 2], [1, 3], [2, 3], [1, 2, 3]
Write a function generateSubsets to solve this problem. The function should take in a list of integers
and return a list of lists containing all possible subsets.
Approach
To generate all possible subsets, we can use a backtracking approach. We start with an empty subset
and gradually add elements to it. At each step, we have two choices: either include the current
element or exclude it. We continue this process for all elements in the given set to generate all
possible combinations.
The steps for generating subsets are as follows:
1. Initialize an empty list to store the subsets.
2. Define a helper function that takes the current index, the current subset, and the set of integers
as parameters.
3. If the current index is equal to the length of the set, add the current subset to the list of subsets.
4. Otherwise, recursively call the helper function with the next index and the current subset
including the current element.
5. Recursively call the helper function with the next index and the current subset excluding the
current element.
6. Return the list of subsets.
Pseudo Code
89

Generate All Subsets
90
function generateSubsets(nums):
subsets = []
helper(0, [], nums, subsets)
return subsets
function helper(index, currentSubset, nums, subsets):
if index == len(nums):
subsets.add(currentSubset)
return
helper(index + 1, currentSubset + [nums[index]], nums, subsets)
helper(index + 1, currentSubset, nums, subsets)
Time Complexity Analysis
The time complexity of this algorithm is O(2N⁾, ʷʰeʳe N ⁱˢ tʰe ⁿumbeʳ of eˡemeⁿtˢ ⁱⁿ tʰe gⁱveⁿ ˢet. Sⁱⁿce ʷe aʳe
geⁿeʳatⁱⁿg aˡˡ poˢˢⁱbˡe ˢubˢetˢ, tʰe totaˡ ⁿumbeʳ of ˢubˢetˢ ⁱˢ ²N. Therefore, the time complexity can be considered
exponential.
Solution in Java
import java.util.ArrayList;
import java.util.List;
public class GenerateSubsets {
public static List<List<Integer>> generateSubsets(int[] nums) {
List<List<Integer>> subsets = new ArrayList<>();
helper(0, new ArrayList<>(), nums, subsets);
return subsets;
}
public static void helper(int index, List<Integer> currentSubset, int[] nums, Li\
st<List<Integer>> subsets) {
if (index == nums.length) {
subsets.add(new ArrayList<>(currentSubset));
return;
}
helper(index + 1, new ArrayList<>(currentSubset){{add(nums[index]);}}, nums,\
subsets);
helper(index + 1, new ArrayList<>(currentSubset), nums, subsets);
}

Generate All Subsets
91
public static void main(String[] args) {
int[] nums = {1, 2, 3};
List<List<Integer>> subsets = generateSubsets(nums);
System.out.println("Subsets:");
for (List<Integer> subset : subsets) {
System.out.println(subset);
}
}
}
Solution in Python
def generateSubsets(nums):
subsets = []
helper(0, [], nums, subsets)
return subsets
def helper(index, currentSubset, nums, subsets):
if index == len(nums):
subsets.append(currentSubset.copy())
return
helper(index + 1, currentSubset + [nums[index]], nums, subsets)
helper(index + 1, currentSubset, nums, subsets)
nums = [1, 2, 3]
subsets = generateSubsets(nums)
print("Subsets:")
for subset in subsets:
print(subset)
By using a backtracking approach, we can efficiently generate all possible subsets of a given set.
With a time complexity of O(2^N), this algorithm provides a comprehensive solution for finding all
subsets without duplicate elements.

Power of Two
Given an integer n, determine if it is a power of two. Return true if n is a power of two, and false
otherwise.
For example:
• isPowerOfTwo(1) should return true because 1 is a power of two (2^0 = 1).
• isPowerOfTwo(16) should return true because 16 is a power of two (2^4 = 16).
• isPowerOfTwo(3) should return false because 3 is not a power of two.
Write a function isPowerOfTwo to solve this problem. The function should take in an integer n and
return a boolean value indicating whether n is a power of two or not.
Approach
To determine if an integer is a power of two, we can use the following approach:
1. If n is less than or equal to 0, return false since negative numbers and zero cannot be powers
of two.
2. Use bitwise operations to check if n has only one bit set to 1. If this condition is true, then n is
a power of two; otherwise, it is not.
• Use the expression n & (n - 1) to unset the rightmost set bit of n.
• If the result is 0, it means n had only one bit set to 1, indicating it is a power of two.
• If the result is not 0, it means n had more than one bit set to 1, indicating it is not a power
of two.
Pseudo Code
function isPowerOfTwo(n):
if n <= 0:
return false
return (n & (n - 1)) == 0
92

Power of Two
93
Time Complexity Analysis
The time complexity of this algorithm is O(1) because it performs a constant number of operations
irrespective of the input size. The bitwise operations and comparison take a constant amount of
time, making the algorithm highly efficient.
Solution in Java
public class PowerOfTwo {
public static boolean isPowerOfTwo(int n) {
if (n <= 0)
return false;
return (n & (n - 1)) == 0;
}
public static void main(String[] args) {
int n1 = 1;
int n2 = 16;
int n3 = 3;
System.out.println(n1 + " is a power of two: " + isPowerOfTwo(n1));
System.out.println(n2 + " is a power of two: " + isPowerOfTwo(n2));
System.out.println(n3 + " is a power of two: " + isPowerOfTwo(n3));
}
}
Solution in Python
def isPowerOfTwo(n):
if n <= 0:
return False
return (n & (n - 1)) == 0
n1 = 1
n2 = 16
n3 = 3
print(n1, "is a power of two:", isPowerOfTwo(n1))
print(n2, "is a power of two:", isPowerOfTwo(n2))
print(n3, "is a power of two:", isPowerOfTwo(n3))

Power of Two
94
The isPowerOfTwo function efficiently determines whether an integer is a power of two or not using
bitwise operations. With a time complexity of O(1), the algorithm provides an optimal solution for
checking the power of two property.

Scalar Multiplication Order
Given a list of matrices, each represented as a tuple (r, c), where r represents the number of rows
and c represents the number of columns, determine the minimum number of scalar multiplications
required to multiply these matrices together. The order of multiplication is determined by the
parentheses placement.
For example, given the list of matrices [(2, 3), (3, 4), (4, 2)], we can multiply them in the
following order: ((A * B) * C), where A represents the first matrix (2, 3), B represents the second
matrix (3, 4), and C represents the third matrix (4, 2). The total number of scalar multiplications
required in this order is 2 * 3 * 4 + 2 * 4 * 2 = 40.
Write a function minimumScalarMultiplications to solve this problem. The function should take
in a list of matrices and return the minimum number of scalar multiplications required to multiply
them together.
Approach
To determine the minimum number of scalar multiplications, we can use dynamic programming
and the matrix chain multiplication algorithm. The steps involved are as follows:
1. Create a dynamic programming table dp of size n x n, where n is the number of matrices.
2. Initialize the diagonal elements of dp with 0 since multiplying a single matrix requires no scalar
multiplications.
3. Traverse the upper triangle of the dp table in a bottom-up manner. For each cell dp[i][j],
calculate the minimum number of scalar multiplications required to multiply matrices from
index i to index j.
• Iterate k from i to j-1 and calculate the number of scalar multiplications for each possible
split point.
• Update dp[i][j] with the minimum value obtained.
4. The value at dp[0][n-1] represents the minimum number of scalar multiplications required to
multiply all matrices together.
Pseudo Code
95

Scalar Multiplication Order
96
function minimumScalarMultiplications(matrices):
n = length(matrices)
dp = createTable(n, n)
for i = 0 to n-1:
dp[i][i] = 0
for length = 2 to n:
for i = 0 to n - length:
j = i + length - 1
dp[i][j] = infinity
for k = i to j-1:
cost = dp[i][k] + dp[k+1][j] + matrices[i].rows * matrices[k].column\
s * matrices[j].columns
dp[i][j] = min(dp[i][j], cost)
return dp[0][n-1]
Time Complexity Analysis
The time complexity of the matrix chain multiplication algorithm is O(n^3), where n is the number
of matrices. This is because we iterate over a nested loop of size n, and for each cell, we perform a
constant amount of work. Therefore, the algorithm provides an efficient solution for calculating the
minimum number of scalar multiplications required.
Solution in Java
import java.util.Arrays;
public class ScalarMultiplicationOrder {
public static int minimumScalarMultiplications(Matrix[] matrices) {
int n = matrices.length;
int[][] dp = new int[n][n];
for (int i = 0; i < n; i++) {
dp[i][i] = 0;
}
for (int length = 2; length <= n; length++) {

Scalar Multiplication Order
97
for (int i = 0; i <= n - length; i++) {
int j = i + length - 1;
dp[i][j] = Integer.MAX_VALUE;
for (int k = i; k < j; k++) {
int cost = dp[i][k] + dp[k + 1][j] + matrices[i].rows * matrices\
[k].columns * matrices[j].columns;
dp[i][j] = Math.min(dp[i][j], cost);
}
}
}
return dp[0][n - 1];
}
public static void main(String[] args) {
Matrix[] matrices = {
new Matrix(2, 3),
new Matrix(3, 4),
new Matrix(4, 2)
};
int minimumScalarMultiplications = minimumScalarMultiplications(matrices);
System.out.println("Minimum Scalar Multiplications: " + minimumScalarMultipl\
ications);
}
}
class Matrix {
int rows;
int columns;
public Matrix(int rows, int columns) {
this.rows = rows;
this.columns = columns;
}
}
Solution in Python

Scalar Multiplication Order
98
class Matrix:
def __init__(self, rows, columns):
self.rows = rows
self.columns = columns
def minimum_scalar_multiplications(matrices):
n = len(matrices)
dp = [[0] * n for _ in range(n)]
for i in range(n):
dp[i][i] = 0
for length in range(2, n + 1):
for i in range(n - length + 1):
j = i + length - 1
dp[i][j] = float('inf')
for k in range(i, j):
cost = dp[i][k] + dp[k + 1][j] + matrices[i].rows * matrices[k].colu\
mns * matrices[j].columns
dp[i][j] = min(dp[i][j], cost)
return dp[0][n - 1]
matrices = [
Matrix(2, 3),
Matrix(3, 4),
Matrix(4, 2)
]
minimum_scalar_multiplications = minimum_scalar_multiplications(matrices)
print("Minimum Scalar Multiplications:", minimum_scalar_multiplications)
In the above solutions, we first define a Matrix class to represent the matrices. Then, we implement
the minimumScalarMultiplications function in both Java and Python. Finally, we create an example
scenario with three matrices and calculate the minimum number of scalar multiplications required
using the implemented function. The output is displayed to verify the correctness of the solution.
The time complexity of both the Java and Python solutions is O(n^3), where n is the number of
matrices.

Topological Sorting
Given a directed acyclic graph (DAG), perform a topological sorting of its vertices. Topological
sorting is a linear ordering of the vertices such that for every directed edge (u, v), vertex u comes
before vertex v in the ordering.
Write a function topologicalSort that takes in a DAG represented as an adjacency list and returns
a list of vertices in a topological order. If the graph contains a cycle, return an empty list.
Approach
To perform a topological sort on a DAG, we can use a depth-first search (DFS) algorithm. The steps
involved are as follows:
1. Create a stack to store the visited vertices in the order of their completion of DFS.
2. Initialize a visited array to keep track of visited vertices.
3. For each unvisited vertex in the graph, call a recursive helper function dfs to perform DFS.
• In the dfs function, mark the current vertex as visited.
• Recursively visit all the adjacent vertices of the current vertex that are not yet visited.
• After visiting all the adjacent vertices, push the current vertex onto the stack.
4. Once the DFS is complete for all vertices, pop the vertices from the stack one by one to get the
topological order.
If during the DFS, we encounter a visited vertex again, it means there is a cycle in the graph, and
we cannot perform a topological sort. In such cases, we return an empty list.
Pseudo Code
99

Topological Sorting
100
function topologicalSort(graph):
n = graph.length
visited = new boolean[n]
stack = new Stack()
for v = 0 to n-1:
if not visited[v]:
if not dfs(graph, v, visited, stack):
return empty list
result = new list()
while stack is not empty:
result.add(stack.pop())
return result
function dfs(graph, v, visited, stack):
visited[v] = true
for u in graph[v]:
if not visited[u]:
if not dfs(graph, u, visited, stack):
return false
stack.push(v)
return true
Time Complexity Analysis
The time complexity of the topological sort algorithm is O(V + E), where V is the number of vertices
and E is the number of edges in the graph. This is because we perform a DFS on all vertices and
traverse all edges once.
Solution in Java

Topological Sorting
101
import java.util.*;
public class TopologicalSorting {
public static List<Integer> topologicalSort(List<List<Integer>> graph) {
int n = graph.size();
boolean[] visited = new boolean[n];
Stack<Integer> stack = new Stack<>();
for (int v = 0; v < n; v++) {
if (!visited[v]) {
if (!dfs(graph, v, visited, stack)) {
return Collections.emptyList();
}
}
}
List<Integer> result = new ArrayList<>();
while (!stack.isEmpty()) {
result.add(stack.pop());
}
return result;
}
private static boolean dfs(List<List<Integer>> graph, int v, boolean[] visited, \
Stack<Integer> stack) {
visited[v] = true;
for (int u : graph.get(v)) {
if (!visited[u]) {
if (!dfs(graph, u, visited, stack)) {
return false;
}
}
}
stack.push(v);
return true;
}
public static void main(String[] args) {
List<List<Integer>> graph = new ArrayList<>();

Topological Sorting
102
graph.add(Arrays.asList(1, 2));
graph.add(Arrays.asList(3, 4));
graph.add(Arrays.asList(5));
graph.add(new ArrayList<>());
graph.add(Arrays.asList(5, 6));
graph.add(Arrays.asList(6));
graph.add(new ArrayList<>());
List<Integer> topologicalOrder = topologicalSort(graph);
System.out.println("Topological Order: " + topologicalOrder);
}
}
Solution in Python
def topological_sort(graph):
n = len(graph)
visited = [False] * n
stack = []
def dfs(v):
visited[v] = True
for u in graph[v]:
if not visited[u]:
if not dfs(u):
return False
stack.append(v)
return True
for v in range(n):
if not visited[v]:
if not dfs(v):
return []
return stack[::-1]
graph = [
[1, 2],
[3, 4],

Topological Sorting
103
[5],
[],
[5, 6],
[6],
[]
]
topological_order = topological_sort(graph)
print("Topological Order:", topological_order)
In the above solutions, we first define a function topologicalSort that takes in a graph represented
as an adjacency list and performs a topological sort using DFS. We also define a helper function
dfs to perform the recursive DFS traversal. Finally, we create an example graph and call the
topologicalSort function to obtain the topological order. The output is displayed to verify the
correctness of the solution.
The time complexity of both the Java and Python solutions is O(V + E), where V is the number of
vertices and E is the number of edges in the graph.

Breadth-First Search (BFS)
Given a graph represented as an adjacency list and a starting vertex, perform a breadth-first search
(BFS) to traverse all the vertices in the graph in a breadth-wise manner. Return the order in which
the vertices are visited during the BFS.
Write a function bfs that takes in the graph and the starting vertex and returns a list of vertices in
the order they are visited during the BFS.
Approach
To perform a BFS, we use a queue data structure to keep track of the vertices to be visited. The steps
involved are as follows:
1. Create a queue and enqueue the starting vertex.
2. Create a visited array to keep track of visited vertices.
3. Create an empty list to store the order of visited vertices.
4. While the queue is not empty:
• Dequeue a vertex from the queue.
• Mark the dequeued vertex as visited.
• Add the vertex to the list of visited vertices.
• Enqueue all the adjacent vertices of the dequeued vertex that are not yet visited.
5. Return the list of visited vertices.
Pseudo Code
104

Breadth-First Search (BFS)
105
function bfs(graph, start):
n = graph.length
visited = new boolean[n]
queue = new Queue()
order = new list()
queue.enqueue(start)
visited[start] = true
while queue is not empty:
vertex = queue.dequeue()
order.add(vertex)
for neighbor in graph[vertex]:
if not visited[neighbor]:
visited[neighbor] = true
queue.enqueue(neighbor)
return order
Time Complexity Analysis
The time complexity of the BFS algorithm is O(V + E), where V is the number of vertices and E is the
number of edges in the graph. This is because we visit each vertex and traverse each edge exactly
once.
Solution in Java
import java.util.*;
public class BreadthFirstSearch {
public static List<Integer> bfs(List<List<Integer>> graph, int start) {
int n = graph.size();
boolean[] visited = new boolean[n];
Queue<Integer> queue = new LinkedList<>();
List<Integer> result = new ArrayList<>();
queue.offer(start);
visited[start] = true;

Breadth-First Search (BFS)
106
while (!queue.isEmpty()) {
int vertex = queue.poll();
result.add(vertex);
for (int neighbor : graph.get(vertex)) {
if (!visited[neighbor]) {
queue.offer(neighbor);
visited[neighbor] = true;
}
}
}
return result;
}
public static void main(String[] args) {
List<List<Integer>> graph = new ArrayList<>();
graph.add(Arrays.asList(1, 2));
graph.add(Arrays.asList(3));
graph.add(Arrays.asList(4, 5));
graph.add(Arrays.asList(1));
graph.add(Arrays.asList(2));
graph.add(new ArrayList<>());
int startVertex = 0;
List<Integer> bfsOrder = bfs(graph, startVertex);
System.out.println("BFS Order: " + bfsOrder);
}
}
Solution in Python

Breadth-First Search (BFS)
107
from collections import deque
def bfs(graph, start):
n = len(graph)
visited = [False] * n
queue = deque()
result = []
queue.append(start)
visited[start] = True
while queue:
vertex = queue.popleft()
result.append(vertex)
for neighbor in graph[vertex]:
if not visited[neighbor]:
queue.append(neighbor)
visited[neighbor] = True
return result
graph = [
[1, 2],
[3],
[4, 5],
[1],
[2],
[]
]
start_vertex = 0
bfs_order = bfs(graph, start_vertex)
print("BFS Order:", bfs_order)
In the above solutions, we first define a function bfs that takes in a graph represented as an adjacency
list and a starting vertex. We then perform a BFS using a queue and mark the visited vertices along
the way. Finally, we return the order in which the vertices are visited during the BFS. The Java and
Python implementations are provided, and the output is displayed to verify the correctness of the
solution.
The time complexity of both the Java and Python solutions is O(V + E), where V is the number of
vertices and E is the number of edges in the graph.

Depth-First Search (DFS)
Given a graph represented as an adjacency list and a starting vertex, perform a depth-first search
(DFS) to traverse all the vertices in the graph in a depth-wise manner. Return the order in which
the vertices are visited during the DFS.
Write a function dfs that takes in the graph and the starting vertex and returns a list of vertices in
the order they are visited during the DFS.
Approach
To perform a DFS, we use a recursive approach or a stack data structure to keep track of the vertices
to be visited. The steps involved are as follows:
1. Create a visited array to keep track of visited vertices.
2. Create an empty list to store the order of visited vertices.
3. Call the DFS helper function with the starting vertex.
4. In the DFS helper function:
• Mark the current vertex as visited.
• Add the current vertex to the list of visited vertices.
• For each unvisited neighbor of the current vertex, recursively call the DFS helper function.
5. Return the list of visited vertices.
Pseudo Code
108

Depth-First Search (DFS)
109
function dfs(graph, start):
visited = new boolean[graph.length]
order = new list()
dfsHelper(start, visited, order)
return order
function dfsHelper(vertex, visited, order):
visited[vertex] = true
order.add(vertex)
for neighbor in graph[vertex]:
if not visited[neighbor]:
dfsHelper(neighbor, visited, order)
Time Complexity Analysis
The time complexity of the DFS algorithm is O(V + E), where V is the number of vertices and E is the
number of edges in the graph. This is because we visit each vertex and traverse each edge exactly
once.
Solution in Java
import java.util.*;
public class DepthFirstSearch {
public static List<Integer> dfs(List<List<Integer>> graph, int start) {
int n = graph.size();
boolean[] visited = new boolean[n];
List<Integer> order = new ArrayList<>();
dfsHelper(start, graph, visited, order);
return order;
}
public static void dfsHelper(int vertex, List<List<Integer>> graph, boolean[] vi\
sited, List<Integer> order) {
visited[vertex] = true;

Depth-First Search (DFS)
110
order.add(vertex);
for (int neighbor : graph.get(vertex)) {
if (!visited[neighbor]) {
dfsHelper(neighbor, graph, visited, order);
}
}
}
public static void main(String[] args) {
List<List<Integer>> graph = new ArrayList<>();
graph.add(Arrays.asList(1, 2));
graph.add(Arrays.asList(0, 3));
graph.add(Arrays.asList(0, 4));
graph.add(Arrays.asList(1, 5));
graph.add(Arrays.asList(2, 5));
graph.add(Arrays.asList(3, 4, 6));
graph.add(Arrays.asList(5));
int startVertex = 0;
List<Integer> dfsOrder = dfs(graph, startVertex);
System.out.println("DFS Order: " + dfsOrder);
}
}
Solution in Python
def dfs(graph, start):
n = len(graph)
visited = [False] * n
order = []
def dfs_helper(vertex):
visited[vertex] = True
order.append(vertex)
for neighbor in graph[vertex]:
if not visited[neighbor]:
dfs_helper(neighbor)
dfs_helper(start)

Depth-First Search (DFS)
111
return order
graph = [
[1, 2],
[0, 3],
[0, 4],
[1, 5],
[2, 5],
[3, 4, 6],
[5]
]
start_vertex = 0
dfs_order = dfs(graph, start_vertex)
print("DFS Order:", dfs_order)
In the above solutions, we first define a function dfs that takes in a graph represented as an adjacency
list and a starting vertex. We then perform a DFS using recursion and mark the visited vertices along
the way. Finally, we return the order in which the vertices are visited during the DFS. The Java and
Python implementations are provided, and the output is displayed to verify the correctness of the
solution.
The time complexity of both the Java and Python solutions is O(V + E), where V is the number of
vertices and E is the number of edges in the graph.

Symmetric Tree
Given a binary tree, check whether it is a mirror of itself (i.e., symmetric around its center).
Approach
To determine if a binary tree is symmetric, we can perform a recursive comparison of its left and
right subtrees. The base case occurs when both subtrees are null, which is considered symmetric.
If only one subtree is null or the values at corresponding nodes differ, the tree is not symmetric.
Otherwise, we recursively check the left subtree’s left child against the right subtree’s right child,
and the left subtree’s right child against the right subtree’s left child.
Pseudo Code
function isSymmetric(root):
if root is null:
return true
return isMirror(root.left, root.right)
function isMirror(left, right):
if left is null and right is null:
return true
if left is null or right is null or left.value != right.value:
return false
return isMirror(left.left, right.right) and isMirror(left.right, right.left)
Time Complexity
The time complexity of this approach is O(n), where n is the number of nodes in the binary tree. In
the worst case, we need to visit all nodes to determine if the tree is symmetric.
Solution in Java
112

Symmetric Tree
113
class TreeNode {
int value;
TreeNode left;
TreeNode right;
TreeNode(int value) {
this.value = value;
this.left = null;
this.right = null;
}
}
public class SymmetricTree {
public static boolean isSymmetric(TreeNode root) {
if (root == null) {
return true;
}
return isMirror(root.left, root.right);
}
public static boolean isMirror(TreeNode left, TreeNode right) {
if (left == null && right == null) {
return true;
}
if (left == null || right == null || left.value != right.value) {
return false;
}
return isMirror(left.left, right.right) && isMirror(left.right, right.left);
}
public static void main(String[] args) {
// Example tree: symmetric
TreeNode root = new TreeNode(1);
root.left = new TreeNode(2);
root.right = new TreeNode(2);
root.left.left = new TreeNode(3);
root.left.right = new TreeNode(4);
root.right.left = new TreeNode(4);
root.right.right = new TreeNode(3);
boolean isSymmetric = isSymmetric(root);
System.out.println("Is the tree symmetric? " + isSymmetric);
}

Symmetric Tree
114
}
Solution in Python
class TreeNode:
def __init__(self, value):
self.value = value
self.left = None
self.right = None
def is_symmetric(root):
if root is None:
return True
return is_mirror(root.left, root.right)
def is_mirror(left, right):
if left is None and right is None:
return True
if left is None or right is None or left.value != right.value:
return False
return is_mirror(left.left, right.right) and is_mirror(left.right, right.left)
# Example tree: symmetric
root = TreeNode(1)
root.left = TreeNode(2)
root.right = TreeNode(2)
root.left.left = TreeNode(3)
root.left.right = TreeNode(4)
root.right.left = TreeNode(4)
root.right.right = TreeNode(3)
is_symmetric = is_symmetric(root)
print("Is the tree symmetric?", is_symmetric)
In this chapter, we tackled the “Symmetric Tree” challenge, where we determined if a binary tree
is symmetric around its center. We presented a recursive approach to compare the left and right
subtrees, checking if they mirror each other. The time complexity of our solution was determined
to be O(n), where n is the number of nodes in the binary tree. We provided implementations in both
Java and Python programming languages to solve the problem.

Lowest Common Ancestor
Given a binary tree and two nodes, find the lowest common ancestor (LCA) of the two nodes. The
lowest common ancestor is the deepest node that has both nodes as descendants.
Approach
To find the lowest common ancestor, we can use a recursive approach. Starting from the root node,
we check if the current node is one of the given nodes. If it is, we return the current node. Otherwise,
we recursively search for the nodes in the left and right subtrees. If both nodes are found in different
subtrees, then the current node is the lowest common ancestor. If only one node is found, we return
that node as the lowest common ancestor. If neither node is found, we return null.
Pseudo Code
function lowestCommonAncestor(root, p, q):
if root is null or root equals p or root equals q:
return root
left = lowestCommonAncestor(root.left, p, q)
right = lowestCommonAncestor(root.right, p, q)
if left is not null and right is not null:
return root
if left is not null:
return left
if right is not null:
return right
return null
Time Complexity
The time complexity of this approach is O(n), where n is the number of nodes in the binary tree. In
the worst case, we may need to visit all nodes to find the lowest common ancestor.
Solution in Java
115

Lowest Common Ancestor
116
class TreeNode {
int val;
TreeNode left;
TreeNode right;
TreeNode(int val) {
this.val = val;
this.left = null;
this.right = null;
}
}
public class LowestCommonAncestor {
public static TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode \
q) {
if (root == null || root == p || root == q) {
return root;
}
TreeNode left = lowestCommonAncestor(root.left, p, q);
TreeNode right = lowestCommonAncestor(root.right, p, q);
if (left != null && right != null) {
return root;
}
if (left != null) {
return left;
}
if (right != null) {
return right;
}
return null;
}
public static void main(String[] args) {
// Example tree
TreeNode root = new TreeNode(3);
root.left = new TreeNode(5);
root.right = new TreeNode(1);
root.left.left = new TreeNode(6);
root.left.right = new TreeNode(2);
root.right.left = new TreeNode(0);
root.right.right = new TreeNode(8);
root.left.right.left = new TreeNode(7);
root.left.right.right = new TreeNode(4);

Lowest Common Ancestor
117
TreeNode p = root.left;
TreeNode q = root.right;
TreeNode lca = lowestCommonAncestor(root, p, q);
System.out.println("Lowest Common Ancestor: " + lca.val);
}
}
Solution in Python
class TreeNode:
def __init__(self, val):
self.val = val
self.left = None
self.right = None
def lowest_common_ancestor(root, p, q):
if root is None or root == p or root == q:
return root
left = lowest_common_ancestor(root.left, p, q)
right = lowest_common_ancestor(root.right, p, q)
if left is not None and right is not None:
return root
if left is not None:
return left
if right is not None:
return right
return None
# Example tree
root = TreeNode(3)
root.left = TreeNode(5)
root.right = TreeNode(1)
root.left.left = TreeNode(6)
root.left.right = TreeNode(2)
root.right.left = TreeNode(0)
root.right.right = TreeNode(8)
root.left.right.left = TreeNode(7)
root.left.right.right = TreeNode(4)

Lowest Common Ancestor
118
p = root.left
q = root.right
lca = lowest_common_ancestor(root, p, q)
print("Lowest Common Ancestor:", lca.val)
In this chapter, we discussed the “Lowest Common Ancestor” challenge, where we aimed to find the
lowest common ancestor of two given nodes in a binary tree. We presented a recursive approach to
solve the problem and provided implementations in both Java and Python programming languages.
The time complexity of our solution is O(n), where n is the number of nodes in the binary tree.

Trim a Binary Search Tree
Given the root of a binary search tree and two values, low and high, trim the tree such that all
elements in the resulting tree are between low and high (inclusive). The resulting tree should still
be a valid binary search tree.
Approach
To trim a binary search tree, we need to remove any nodes whose values are outside the range
defined by low and high. There are three possible cases to consider:
1. If the current node’s value is less than low, then the entire left subtree of the current node is
outside the desired range. We can safely discard the left subtree and return the trimmed right
subtree.
2. If the current node’s value is greater than high, then the entire right subtree of the current node
is outside the desired range. We can safely discard the right subtree and return the trimmed
left subtree.
3. If the current node’s value is within the desired range, we recursively trim both the left and
right subtrees and update the current node’s left and right pointers accordingly.
Pseudo Code
function trimBST(root, low, high):
if root is null:
return null
if root.val < low:
return trimBST(root.right, low, high)
if root.val > high:
return trimBST(root.left, low, high)
root.left = trimBST(root.left, low, high)
root.right = trimBST(root.right, low, high)
return root
Time Complexity
The time complexity of this approach is O(n), where n is the number of nodes in the binary search
tree. We visit each node once during the trimming process.
119

Trim a Binary Search Tree
120
Solution in Java
class TreeNode {
int val;
TreeNode left;
TreeNode right;
TreeNode(int val) {
this.val = val;
this.left = null;
this.right = null;
}
}
public class TrimBST {
public static TreeNode trimBST(TreeNode root, int low, int high) {
if (root == null) {
return null;
}
if (root.val < low) {
return trimBST(root.right, low, high);
}
if (root.val > high) {
return trimBST(root.left, low, high);
}
root.left = trimBST(root.left, low, high);
root.right = trimBST(root.right, low, high);
return root;
}
public static void main(String[] args) {
// Example tree
TreeNode root = new TreeNode(8);
root.left = new TreeNode(3);
root.right = new TreeNode(10);
root.left.left = new TreeNode(1);
root.left.right = new TreeNode(6);
root.right.right = new TreeNode(14);
root.left.right.left = new TreeNode(4);
root.left.right.right = new TreeNode(7);
root.right.right.left = new TreeNode(13);

Trim a Binary Search Tree
121
int low = 5;
int high = 13;
TreeNode trimmedTree = trimBST(root, low, high);
System.out.println("Trimmed Tree:");
printTree(trimmedTree);
}
public static void printTree(TreeNode root) {
if (root == null) {
return;
}
printTree(root.left);
System.out.print(root.val + " ");
printTree(root.right);
}
}
Solution in Python
class TreeNode:
def __init__(self, val):
self.val = val
self.left = None
self.right = None
def trim_bst(root, low, high):
if root is None:
return None
if root.val < low:
return trim_bst(root.right, low, high)
if root.val > high:
return trim_bst(root.left, low, high)
root.left = trim_bst(root.left, low, high)
root.right = trim_bst(root.right, low, high)
return root
def print_tree(root):
if root is None:
return
print_tree(root.left)
print(root.val, end=" ")

Trim a Binary Search Tree
122
print_tree(root.right)
# Example tree
root = TreeNode(8)
root.left = TreeNode(3)
root.right = TreeNode(10)
root.left.left = TreeNode(1)
root.left.right = TreeNode(6)
root.right.right = TreeNode(14)
root.left.right.left = TreeNode(4)
root.left.right.right = TreeNode(7)
root.right.right.left = TreeNode(13)
low = 5
high = 13
trimmed_tree = trim_bst(root, low, high)
print("Trimmed Tree:")
print_tree(trimmed_tree)
In this chapter, we discussed the “Trim a Binary Search Tree” challenge. We provided a detailed
problem statement and presented an approach to trim the tree based on the given range. We
implemented the solution in both Java and Python programming languages, considering the time
complexity of O(n), where n is the number of nodes in the tree.

Find Closest Value in BST
Given a binary search tree (BST) and a target value, find the value in the BST that is closest to the
target.
Approach
To find the closest value in a BST, we can traverse the tree in a modified depth-first search (DFS)
manner. Starting from the root, we compare the current node’s value with the target value and
update the closest value accordingly.
We follow these steps:
1. Initialize a variable closest to store the closest value found so far. Set it to the root’s value.
2. Traverse the tree recursively, comparing the absolute difference between the current node’s
value and the target value with the absolute difference between the closest value and the
target value.
3. If the absolute difference is smaller, update the closest value to the current node’s value.
4. Recursively search either the left or right subtree based on the target value’s relationship with
the current node’s value.
5. Return the closest value found.
Pseudo Code
function findClosestValue(root, target):
closest = root.val
return findClosestValueHelper(root, target, closest)
function findClosestValueHelper(node, target, closest):
if node is null:
return closest
if abs(node.val - target) < abs(closest - target):
closest = node.val
if target < node.val:
return findClosestValueHelper(node.left, target, closest)
else if target > node.val:
return findClosestValueHelper(node.right, target, closest)
else:
return closest
123

Find Closest Value in BST
124
Time Complexity
The time complexity of this approach is O(log N) on average, where N is the number of nodes in the
binary search tree. In the worst case, when the tree is skewed, the time complexity becomes O(N).
Solution in Java
class TreeNode {
int val;
TreeNode left;
TreeNode right;
TreeNode(int val) {
this.val = val;
this.left = null;
this.right = null;
}
}
public class FindClosestValueInBST {
public static int findClosestValue(TreeNode root, int target) {
int closest = root.val;
return findClosestValueHelper(root, target, closest);
}
private static int findClosestValueHelper(TreeNode node, int target, int closest\
) {
if (node == null) {
return closest;
}
if (Math.abs(node.val - target) < Math.abs(closest - target)) {
closest = node.val;
}
if (target < node.val) {
return findClosestValueHelper(node.left, target, closest);
} else if (target > node.val) {
return findClosestValueHelper(node.right, target, closest);
} else {
return closest;
}
}

Find Closest Value in BST
125
public static void main(String[] args) {
// Example tree
TreeNode root = new TreeNode(8);
root.left = new TreeNode(3);
root.right = new TreeNode(10);
root.left.left = new TreeNode(1);
root.left.right = new TreeNode(6);
root.right.right = new TreeNode(14);
root.left.right.left = new TreeNode(4);
root.left.right.right = new TreeNode(7);
root.right.right.left = new TreeNode(13);
int target = 9;
int closestValue = findClosestValue(root, target);
System.out.println("Closest Value to " + target + ": " + closestValue);
}
}
Solution in Python
class TreeNode:
def __init__(self, val):
self.val = val
self.left = None
self.right = None
def find_closest_value(root, target):
closest = root.val
return find_closest_value_helper(root, target, closest)
def find_closest_value_helper(node, target, closest):
if node is None:
return closest
if abs(node.val - target) < abs(closest - target):
closest = node.val
if target < node.val:
return find_closest_value_helper(node.left, target, closest)
elif target > node.val:
return find_closest_value_helper(node.right, target, closest)
else:

Find Closest Value in BST
126
return closest
# Example tree
root = TreeNode(8)
root.left = TreeNode(3)
root.right = TreeNode(10)
root.left.left = TreeNode(1)
root.left.right = TreeNode(6)
root.right.right = TreeNode(14)
root.left.right.left = TreeNode(4)
root.left.right.right = TreeNode(7)
root.right.right.left = TreeNode(13)
target = 9
closest_value = find_closest_value(root, target)
print("Closest Value to", target, ":", closest_value)
In this chapter, we discussed the “Find Closest Value in BST” challenge. We provided a detailed
problem statement and presented an approach to find the value in a binary search tree (BST) that
is closest to the target value. We implemented the solution in both Java and Python programming
languages, considering the time complexity of O(log N) on average.

Binary Tree Level Order Traversal
Given a binary tree, return the level order traversal of its nodes’ values. The level order traversal is
a breadth-first search (BFS) traversal where we visit nodes level by level, from left to right.
Approach
To perform a level order traversal of a binary tree, we can use a queue data structure. We start by
enqueueing the root node into the queue. Then, while the queue is not empty, we dequeue a node
from the front of the queue, visit its value, and enqueue its left and right child nodes (if they exist)
into the queue. By following this process, we visit the nodes level by level and retrieve their values.
We follow these steps:
1. Create an empty queue and initialize it with the root node.
2. Create an empty list to store the level order traversal result.
3. While the queue is not empty, do the following:
• Dequeue a node from the front of the queue.
• Visit the value of the dequeued node and append it to the result list.
• Enqueue the left and right child nodes of the dequeued node (if they exist).
4. Return the result list containing the level order traversal.
Pseudo Code
function levelOrderTraversal(root):
result = []
if root is null:
return result
queue = createQueue()
enqueue(queue, root)
while not isEmpty(queue):
node = dequeue(queue)
result.append(node.value)
if node.left is not null:
enqueue(queue, node.left)
if node.right is not null:
enqueue(queue, node.right)
return result
127

Binary Tree Level Order Traversal
128
Time Complexity
The time complexity of the level order traversal algorithm is O(N), where N is the number of nodes
in the binary tree. This is because we need to visit each node once. The space complexity is O(M),
where M is the maximum number of nodes at any level in the binary tree. In the worst case, the
maximum number of nodes at any level can be N/2 in a completely balanced binary tree.
Solution in Java
class TreeNode {
int val;
TreeNode left;
TreeNode right;
TreeNode(int val) {
this.val = val;
this.left = null;
this.right = null;
}
}
public class BinaryTreeLevelOrderTraversal {
public static List<Integer> levelOrderTraversal(TreeNode root) {
List<Integer> result = new ArrayList<>();
if (root == null) {
return result;
}
Queue<TreeNode> queue = new LinkedList<>();
queue.offer(root);
while (!queue.isEmpty()) {
TreeNode node = queue.poll();
result.add(node.val);
if (node.left != null) {
queue.offer(node.left);
}
if (node.right != null) {
queue.offer(node.right);
}
}
return result;
}

Binary Tree Level Order Traversal
129
public static void main(String[] args) {
// Example tree
TreeNode root = new TreeNode(3);
root.left = new TreeNode(9);
root.right = new TreeNode(20);
root.right.left = new TreeNode(15);
root.right.right = new TreeNode(7);
List<Integer> traversal = levelOrderTraversal(root);
System.out.println("Level Order Traversal: " + traversal);
}
}
Solution in Python
class TreeNode:
def __init__(self, val):
self.val = val
self.left = None
self.right = None
def level_order_traversal(root):
result = []
if root is None:
return result
queue = []
queue.append(root)
while queue:
node = queue.pop(0)
result.append(node.val)
if node.left:
queue.append(node.left)
if node.right:
queue.append(node.right)
return result
# Example tree
root = TreeNode(3)
root.left = TreeNode(9)

Binary Tree Level Order Traversal
130
root.right = TreeNode(20)
root.right.left = TreeNode(15)
root.right.right = TreeNode(7)
traversal = level_order_traversal(root)
print("Level Order Traversal:", traversal)
In this chapter, we explored the “Binary Tree Level Order Traversal” challenge. We discussed the
problem statement, provided an approach to perform a level order traversal using a queue, and
implemented the solution in both Java and Python programming languages. The time complexity
of the algorithm is O(N), where N is the number of nodes in the binary tree.

Climbing Stairs
You are climbing a staircase that has n steps. You can either take one step or two steps at a time.
Write a function to count the number of distinct ways to climb to the top of the staircase.
Approach
To solve this problem, we can use dynamic programming. Let’s denote dp[i] as the number of
distinct ways to reach the i-th step. We can calculate dp[i] by considering the two possible options:
taking one step from dp[i-1] or taking two steps from dp[i-2]. Therefore, the recurrence relation
for dp[i] is dp[i] = dp[i-1] + dp[i-2]. We initialize dp[0] = 1 and dp[1] = 1 since there is only
one way to reach the first and second steps. Finally, the value of dp[n] will represent the number
of distinct ways to climb to the top of the staircase.
Pseudo Code
function climbStairs(n):
if n <= 2:
return n
dp = [0] * (n + 1)
dp[0] = 1
dp[1] = 1
for i in range(2, n + 1):
dp[i] = dp[i-1] + dp[i-2]
return dp[n]
Time Complexity
The time complexity of the climbStairs algorithm is O(n) since we perform a loop from 2 to n. The
space complexity is O(n) as well since we need an array of size n+1 to store the values of dp.
Solution in Java
131

Climbing Stairs
132
public class ClimbingStairs {
public static int climbStairs(int n) {
if (n <= 2) {
return n;
}
int[] dp = new int[n + 1];
dp[0] = 1;
dp[1] = 1;
for (int i = 2; i <= n; i++) {
dp[i] = dp[i - 1] + dp[i - 2];
}
return dp[n];
}
public static void main(String[] args) {
int n = 4;
int distinctWays = climbStairs(n);
System.out.println("Number of distinct ways to climb the stairs: " + distinc\
tWays);
}
}
Solution in Python
def climb_stairs(n):
if n <= 2:
return n
dp = [0] * (n + 1)
dp[0] = 1
dp[1] = 1
for i in range(2, n + 1):
dp[i] = dp[i - 1] + dp[i - 2]
return dp[n]
n = 4
distinct_ways = climb_stairs(n)
print("Number of distinct ways to climb the stairs:", distinct_ways)
In this chapter, we discussed the “Climbing Stairs” challenge. We presented a dynamic programming
approach to count the number of distinct ways to climb a staircase. We provided the pseudo code,
explained the time complexity, and implemented the solution in both Java and Python programming

Climbing Stairs
133
languages. The time complexity of the algorithm is O(n), where n is the number of steps in the
staircase.

Best Time to Buy and Sell Stock
You are given an array prices where prices[i] represents the price of a stock on the i-th day. You
want to maximize your profit by choosing a single day to buy the stock and a different day in the
future to sell it. Write a function to find the maximum profit you can achieve.
Approach
To solve this problem, we can use a simple approach based on keeping track of the minimum price
encountered so far and updating the maximum profit as we iterate through the prices array. We
start by initializing the minimum price as the maximum integer value and the maximum profit as
zero. Then, for each price in the array, we update the minimum price if the current price is smaller,
and update the maximum profit if the difference between the current price and the minimum price
is larger than the current maximum profit. Finally, we return the maximum profit as the result.
Pseudo Code
function maxProfit(prices):
minPrice = Integer.MAX_VALUE
maxProfit = 0
for price in prices:
if price < minPrice:
minPrice = price
else if price - minPrice > maxProfit:
maxProfit = price - minPrice
return maxProfit
Time Complexity
The time complexity of the maxProfit algorithm is O(n), where n is the number of prices in the array.
We iterate through the array once to find the minimum price and update the maximum profit. The
space complexity is O(1) since we only use a constant amount of additional space.
Solution in Java
134

Best Time to Buy and Sell Stock
135
public class BestTimeToBuyAndSellStock {
public static int maxProfit(int[] prices) {
int minPrice = Integer.MAX_VALUE;
int maxProfit = 0;
for (int price : prices) {
if (price < minPrice) {
minPrice = price;
} else if (price - minPrice > maxProfit) {
maxProfit = price - minPrice;
}
}
return maxProfit;
}
public static void main(String[] args) {
int[] prices = {7, 1, 5, 3, 6, 4};
int maxProfit = maxProfit(prices);
System.out.println("Maximum profit: " + maxProfit);
}
}
Solution in Python
def max_profit(prices):
min_price = float('inf')
max_profit = 0
for price in prices:
if price < min_price:
min_price = price
elif price - min_price > max_profit:
max_profit = price - min_price
return max_profit
prices = [7, 1, 5, 3, 6, 4]
max_profit = max_profit(prices)
print("Maximum profit:", max_profit)
In this chapter, we discussed the “Best Time to Buy and Sell Stock” challenge. We presented a simple
approach to find the maximum profit by keeping track of the minimum price and updating the
maximum profit as we iterate through the prices array. We provided the pseudo code, explained the
time complexity, and implemented the solution in both Java and Python programming languages.
The time complexity of the algorithm is O(n), where n is the number of prices in the array.

House Robber
You are a professional robber planning to rob houses along a street. Each house has a certain amount
of money stashed, represented by an array of non-negative integers nums, where nums[i] represents
the amount of money in the i-th house. The only constraint you have is that adjacent houses have
security systems connected, and it will automatically contact the police if two adjacent houses are
robbed on the same night. Given the array nums, determine the maximum amount of money you
can rob tonight without alerting the police.
Approach
To solve this problem, we can use a dynamic programming approach.
We define a dynamic
programming array dp, where dp[i] represents the maximum amount of money that can be robbed
up to the i-th house.
The dynamic programming array can be calculated using the following
recurrence relation:
dp[i] = max(dp[i-2] + nums[i], dp[i-1])
The intuition behind this recurrence relation is that at each house, we have two options: either rob
the current house and add its value to the maximum amount robbed up to the previous non-adjacent
house, or skip the current house and take the maximum amount robbed up to the previous house.
By selecting the maximum of these two options, we ensure that we maximize the total amount of
money robbed.
Pseudo Code
function rob(nums):
if nums.length == 0:
return 0
if nums.length == 1:
return nums[0]
dp = new Array(nums.length)
dp[0] = nums[0]
dp[1] = max(nums[0], nums[1])
for i from 2 to nums.length-1:
136

House Robber
137
dp[i] = max(dp[i-2] + nums[i], dp[i-1])
return dp[nums.length-1]
Time Complexity
The time complexity of the rob algorithm is O(n), where n is the number of houses. We iterate
through the houses once to calculate the maximum amount of money that can be robbed. The space
complexity is O(n) as well, since we use an array of size n to store the dynamic programming values.
Solution in Java
public class HouseRobber {
public static int rob(int[] nums) {
if (nums.length == 0) {
return 0;
}
if (nums.length == 1) {
return nums[0];
}
int[] dp = new int[nums.length];
dp[0] = nums[0];
dp[1] = Math.max(nums[0], nums[1]);
for (int i = 2; i < nums.length; i++) {
dp[i] = Math.max(dp[i-2] + nums[i], dp[i-1]);
}
return dp[nums.length-1];
}
public static void main(String[] args) {
int[] nums = {1, 2, 3, 1};
int maxAmount = rob(nums);
System.out.println("Maximum amount robbed: " + maxAmount);
}
}
Solution in Python

House Robber
138
def rob(nums):
if len(nums) == 0:
return 0
if len(nums) == 1:
return nums[0]
dp = [0] * len(nums)
dp[0] = nums[0]
dp[1] = max(nums[0], nums[1])
for i in range(2, len(nums)):
dp[i] = max(dp[i-2] + nums[i], dp[i-1])
return dp[-1]
nums = [1, 2, 3, 1]
max_amount = rob(nums)
print("Maximum amount robbed:", max_amount)
In this chapter, we discussed the “House Robber” challenge. We presented a dynamic programming
approach to determine the maximum amount of money that can be robbed without alerting the
police. We provided the pseudo code, explained the time complexity, and implemented the solution
in both Java and Python programming languages. The time complexity of the algorithm is O(n),
where n is the number of houses.

Coin Change
You are given an array of coin denominations and a target amount. Your task is to determine the
minimum number of coins required to make up the target amount. If it is not possible to make up
the amount using the given denominations, return -1.
For example, given the array [1, 2, 5] and the target amount 11, the minimum number of coins
required is 3 (11 = 5 + 5 + 1).
Approach
To solve this problem, we can use a dynamic programming approach. We will create an array dp
of size target + 1 to store the minimum number of coins required to make up each target amount
from 0 to the given target. We will initialize all elements of dp to a value greater than the target
amount itself.
We will iterate through each target amount from 1 to the given target. For each amount, we will
iterate through each coin denomination. If the current coin denomination is less than or equal to
the target amount, we will calculate the minimum number of coins required to make up the current
amount using the formula:
dp[target] = min(dp[target], dp[target - coin] + 1)
Finally, if dp[target] is still greater than the target amount, it means it was not possible to make
up the target amount using the given coin denominations, so we return -1. Otherwise, we return
dp[target], which represents the minimum number of coins required to make up the target amount.
Pseudo Code
139

Coin Change
140
function coinChange(coins, target):
dp = new array of size target + 1
for i = 0 to target:
dp[i] = target + 1
dp[0] = 0
for amount = 1 to target:
for coin in coins:
if coin <= amount:
dp[amount] = min(dp[amount], dp[amount - coin] + 1)
if dp[target] > target:
return -1
else:
return dp[target]
Time Complexity
The time complexity of this solution is O(target * n), where n is the number of coin denominations
and target is the given target amount. This is because we iterate through each target amount from
1 to the given target, and for each amount, we iterate through each coin denomination.
Java Implementation
Here is the Java implementation of the coin change algorithm:
public class CoinChange {
public int coinChange(int[] coins, int target) {
int[] dp = new int[target + 1];
Arrays.fill(dp, target + 1);
dp[0] = 0;
for (int amount = 1; amount <= target; amount++) {
for (int coin : coins) {
if (coin <= amount) {
dp[amount] = Math.min(dp[amount], dp[amount - coin] + 1);
}
}
}

Coin Change
141
return dp[target] > target ? -1 : dp[target];
}
}
Python Implementation
And here is the Python implementation of the coin change algorithm:
class Solution:
def coinChange(self, coins, target):
dp = [float('inf')] * (target + 1)
dp[0] = 0
for amount in range(1, target + 1):
for coin in coins:
if coin <= amount:
dp[amount] = min(dp[amount], dp[amount - coin] + 1)
return -1 if dp [target] == float('inf') else dp[target]
Now you have a comprehensive understanding of the coin change problem, its approach, and the
corresponding Java and Python implementations. You can utilize this knowledge to solve similar
problems and enhance your dynamic programming skills.

Coin Change 2
You are given an array of coin denominations and a target amount. Your task is to determine the
number of distinct ways you can make up the target amount using the given coin denominations.
You have an unlimited supply of each coin denomination.
For example, given the array [1, 2, 5] and the target amount 5, there are four distinct ways to
make up the target amount: [1, 1, 1, 1, 1], [1, 1, 1, 2], [1, 2, 2], and [5].
Approach
To solve this problem, we can use a dynamic programming approach. We will create an array dp of
size target + 1 to store the number of distinct ways to make up each target amount from 0 to the
given target. We will initialize dp[0] to 1, as there is one way to make up the target amount of 0 (by
not selecting any coins).
We will iterate through each coin denomination. For each denomination, we will iterate through
each target amount from the current denomination value to the given target. For each target amount,
we will calculate the number of distinct ways to make up the current amount using the formula:
dp[target] += dp[target - coin]
This formula means that the number of distinct ways to make up the current amount is the sum
of the number of distinct ways to make up the remaining amount after selecting the current coin
denomination.
Finally, we return dp[target], which represents the number of distinct ways to make up the target
amount using the given coin denominations.
Pseudo Code
142

Coin Change 2
143
function change(coins, target):
dp = new array of size target + 1
dp[0] = 1
for coin in coins:
for amount = coin to target:
dp[amount] += dp[amount - coin]
return dp[target]
Time Complexity
The time complexity of this solution is O(n * target), where n is the number of coin denominations
and target is the given target amount. This is because we iterate through each coin denomination,
and for each denomination, we iterate through each target amount from the current denomination
value to the given target.
Java Implementation
Here is the Java implementation of the coin change 2 algorithm:
public class CoinChange2 {
public int change(int[] coins, int target) {
int[] dp = new int[target + 1];
dp[0] = 1;
for (int coin : coins) {
for (int amount = coin; amount <= target; amount++) {
dp[amount] += dp[amount - coin];
}
}
return dp[target];
}
}
Python Implementation
And here is the Python implementation of the coin change 2 algorithm:

Coin Change 2
144
class Solution:
def change(self, amount: int, coins: List[int]) -> int:
dp = [0] * (amount + 1)
dp[0] = 1
for coin in coins:
for i in range(coin, amount + 1):
dp[i] += dp[i - coin]
return dp[amount]
Now you have a thorough understanding of the coin change 2 problem, its approach, and the
corresponding Java and Python implementations. Use this knowledge to solve similar problems
and enhance your dynamic programming skills.

Knapsack Problem (Bottom-Up)
You are given a set of items, each with a weight and a value, and a knapsack with a maximum weight
capacity. Your goal is to maximize the total value of the items you can include in the knapsack
without exceeding its weight capacity.
Write a function knapsackBottomUp that takes the maximum weight capacity W and two arrays
weights and values representing the weights and values of the items, respectively. The function
should return the maximum value that can be obtained by filling the knapsack.
Approach
To solve the Knapsack problem using the Bottom-Up approach, we will use a dynamic programming
table to store the maximum values for different subproblems. We will start by initializing a table dp
of size (n+1) x (W+1), where n is the number of items and W is the maximum weight capacity.
The idea behind the Bottom-Up approach is to build the solution incrementally, starting with the
smallest subproblems and gradually solving larger subproblems until we reach the desired solution.
1. Initialize the table dp with all values set to 0.
2. Iterate through the items from 1 to n:
• For each item i, iterate through the weights from 1 to W:
– If the weight of the current item is less than or equal to the current weight w, compute
the maximum value between including the item and excluding the item:
* dp[i][w] = max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-1][w])
– Otherwise, the item cannot be included, so the value remains the same:
* dp[i][w] = dp[i-1][w]
3. Return the maximum value stored in the bottom-right cell of the table:
• dp[n][W]
Pseudo Code
145

Knapsack Problem (Bottom-Up)
146
knapsackBottomUp(W, weights, values):
n = length(weights)
dp = new 2D array with size (n+1) x (W+1)
for i = 0 to n:
for w = 0 to W:
if i = 0 or w = 0:
dp[i][w] = 0
else if weights[i-1] <= w:
dp[i][w] = max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-1][w])
else:
dp[i][w] = dp[i-1][w]
return dp[n][W]
Time Complexity
The time complexity of the Bottom-Up approach for the Knapsack problem is O(nW), where n is the
number of items and W is the maximum weight capacity. This is because we fill the entire dynamic
programming table of size (n+1) x (W+1) by iterating through all items and weights once.
Java Implementation
public class Knapsack {
public static int knapsackBottomUp(int W, int[] weights, int[] values) {
int n = weights.length;
int[][] dp = new int[n+1][W+1];
for (int i = 0; i <= n; i++) {
for (int w = 0; w <= W; w++) {
if (i == 0 || w == 0) {
dp[i][w] = 0;
} else if (weights[i-1] <= w) {
dp[i][w] = Math.max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-\
1][w]);
} else {
dp[i][w] = dp[i-1][w];
}
}
}

Knapsack Problem (Bottom-Up)
147
return dp[n][W];
}
public static void main(String[] args) {
int W = 10;
int[] weights = {2, 3, 4, 5};
int[] values = {3, 4, 5, 6};
int max = knapsackBottomUp(W, weights, values);
System.out.println("Maximum value: " + max);
}
}
Python Implementation
def knapsack_bottom_up(W, weights, values):
n = len(weights)
dp = [[0] * (W+1) for _ in range(n+1)]
for i in range(1, n+1):
for w in range(1, W+1):
if weights[i-1] <= w:
dp[i][w] = max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-1][w])
else:
dp[i][w] = dp[i-1][w]
return dp[n][W]
W = 10
weights = [2, 3, 4, 5]
values = [3, 4, 5, 6]
max_value = knapsack_bottom_up(W, weights, values)
print("Maximum value:", max_value)
In the Java and Python solutions above, we have implemented the knapsackBottomUp function to
solve the Knapsack problem using the Bottom-Up approach. We initialize a dynamic programming
table and populate it according to the defined algorithm. Finally, we return the maximum value
obtained.

0/1 Knapsack
You are given a set of items, each with a weight and a value, and a knapsack with a maximum weight
capacity. Your goal is to maximize the total value of the items you can include in the knapsack
without exceeding its weight capacity. However, unlike the unbounded knapsack problem, you can
only include each item once (either 0 or 1 times) in the knapsack.
Write a function knapsack01 that takes the maximum weight capacity W and two arrays weights and
values representing the weights and values of the items, respectively. The function should return
the maximum value that can be obtained by filling the knapsack.
Approach
To solve the 0/1 Knapsack problem, we can use the dynamic programming approach. We will use a
dynamic programming table dp of size (n+1) x (W+1), where n is the number of items and W is the
maximum weight capacity.
The idea behind the dynamic programming approach is to build the solution incrementally, starting
with the smallest subproblems and gradually solving larger subproblems until we reach the desired
solution.
1. Initialize the table dp with all values set to 0.
2. Iterate through the items from 1 to n:
• For each item i, iterate through the weights from 1 to W:
– If the weight of the current item is less than or equal to the current weight w, compute
the maximum value between including the item and excluding the item:
* dp[i][w] = max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-1][w])
– Otherwise, the item cannot be included, so the value remains the same:
* dp[i][w] = dp[i-1][w]
3. Return the maximum value stored in the bottom-right cell of the table:
• dp[n][W]
Pseudo Code
148

0/1 Knapsack
149
knapsack01(W, weights, values):
n = length(weights)
dp = new 2D array with size (n+1) x (W+1)
for i = 0 to n:
for w = 0 to W:
if i = 0 or w = 0:
dp[i][w] = 0
else if weights[i-1] <= w:
dp[i][w] = max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-1][w])
else:
dp[i][w] = dp[i-1][w]
return dp[n][W]
Time Complexity
The time complexity of the 0/1 Knapsack problem using the dynamic programming approach is
O(nW), where n is the number of items and W is the maximum weight capacity. This is because we
fill the entire dynamic programming table of size (n+1) x (W+1) by iterating through all items and
weights once.
Java Implementation
public class Knapsack {
public static int knapsack01(int W, int[] weights, int[] values) {
int n = weights.length;
int[][] dp = new int[n+1][W+1];
for (int i = 0; i <= n; i++) {
for (int w = 0; w <= W; w++) {
if (i == 0 || w == 0) {
dp[i][w] = 0;
} else if (weights[i-1] <= w) {
dp[i][w] = Math.max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-\
1][w]);
} else {
dp[i][w] = dp[i-1][w];
}
}

0/1 Knapsack
150
}
return dp[n][W];
}
public static void main(String[] args) {
int W = 10;
int[] weights = {2, 3, 4, 5};
int[] values = {3, 4, 5, 6};
int max = knapsack01(W, weights, values);
System.out.println("Maximum value: " + max);
}
}
Python Implementation
def knapsack01(W, weights, values):
n = len(weights)
dp = [[0] * (W+1) for _ in range(n+1)]
for i in range(n+1):
for w in range(W+1):
if i == 0 or w == 0:
dp[i][w] = 0
elif weights[i-1] <= w:
dp[i][w] = max(values[i-1] + dp[i-1][w-weights[i-1]], dp[i-1][w])
else:
dp[i][w] = dp[i-1][w]
return dp[n][W]
W = 10
weights = [2, 3, 4, 5]
values = [3, 4, 5, 6]
max_value = knapsack01(W, weights, values)
print("Maximum value:", max_value)
In the Java and Python solutions above, we have implemented the knapsack01 function to solve
the 0/1 Knapsack problem using the dynamic programming approach. We initialize a dynamic

0/1 Knapsack
151
programming table and populate it according to the defined algorithm. Finally, we return the
maximum value obtained.

Knapsack with Duplicate Items
In the Knapsack problem with duplicate items, we are given a knapsack with a maximum weight
capacity and a set of items, each with a weight and a value. Our goal is to determine the maximum
value we can obtain by selecting items from the set, allowing for duplicate items to be chosen
multiple times.
Formally, let’s define the problem:
Input: A set of items with weights and values, the maximum weight capacity of the knapsack.
Output: The maximum value that can be obtained by selecting items, considering duplicate items,
such that the total weight does not exceed the maximum capacity.
Approach
We can solve the Knapsack problem with duplicate items using a dynamic programming approach.
We’ll create an array dp of size W + 1, where W is the maximum weight capacity of the knapsack.
dp[i] will represent the maximum value that can be obtained using a knapsack of weight i.
The algorithm follows these steps:
1. Initialize the dp array with all elements set to 0.
2. Iterate through each item in the set.
• For each item, iterate through the dp array from the item’s weight to the maximum weight
capacity.
• For each weight j, update dp[j] as the maximum value between the current dp[j] and
dp[j - weight] + value, where weight is the weight of the current item and value is the
value of the current item.
3. After iterating through all items, the maximum value that can be obtained will be stored in
dp[W].
At the end of the algorithm, dp[W] will contain the maximum value that can be obtained using the
given set of items and the maximum weight capacity of the knapsack.
Pseudocode
Here is the pseudocode for the Knapsack with Duplicate Items algorithm:
152

Knapsack with Duplicate Items
153
function knapsackWithDuplicateItems(items, W):
dp = new array of size W + 1, initialized with 0
for each item in items:
for weight from item.weight to W:
dp[weight] = max(dp[weight], dp[weight - item.weight] + item.value)
return dp[W]
Time Complexity
The time complexity of the Knapsack with Duplicate Items algorithm is O(nW), where n is the
number of items and W is the maximum weight capacity of the knapsack. This is because we iterate
through each item and, for each item, iterate through the dp array from the item’s weight to the
maximum weight capacity.
Java Implementation
Here’s the Java implementation of the Knapsack with Duplicate Items algorithm:
public class KnapsackWithDuplicateItems {
public int knapsackWithDuplicates(Item[] items, int W) {
int[] dp = new int[W + 1];
for (Item item : items) {
for (int weight = item.getWeight(); weight <= W; weight++) {
dp[weight] = Math.max(dp[weight], dp[weight - item.getWeight()] + it\
em.getValue());
}
}
return dp[W];
}
}
Python Implementation
And here’s the Python implementation of the Knapsack with Duplicate Items algorithm:

Knapsack with Duplicate Items
154
class KnapsackWithDuplicateItems:
def knapsackWithDuplicates(self, items, W):
dp = [0] * (W + 1)
for item in items:
for weight in range(item.getWeight(), W + 1):
dp[weight] = max(dp[weight], dp[weight - item.getWeight()] + item.ge\
tValue())
return dp[W]
With these Java and Python implementations, you can solve the Knapsack problem with duplicate
items and find the maximum value that can be obtained by selecting items from the set while
considering duplicate items.

Floyd-Warshall Algorithm
The Floyd-Warshall algorithm is an efficient algorithm for finding the shortest paths between all
pairs of vertices in a weighted directed graph. Given a graph with n vertices and m edges, the
algorithm computes the shortest distance between every pair of vertices.
Formally, let’s define the problem:
Input: A weighted directed graph G with n vertices and m edges.
Output: A matrix D of size n x n, where D[i][j] represents the shortest distance from vertex i to
vertex j.
Note: If there is no path from vertex i to vertex j, then D[i][j] should be infinity.
Approach
The Floyd-Warshall algorithm solves the problem by iteratively considering intermediate vertices
and updating the shortest distances between pairs of vertices. The algorithm builds the solution
incrementally by considering all possible intermediate vertices k (1 ≤k ≤n) and updating the shortest
distances between each pair of vertices i and j.
The algorithm follows these steps:
1. Initialize a 2D matrix D of size n x n, where D[i][j] represents the shortest distance from vertex
i to vertex j.
2. Set all elements of D to infinity.
3. For each edge (u, v) in the graph, update D[u][v] with the weight of the edge.
4. For each vertex i, set D[i][i] to 0.
5. For each vertex k from 1 to n:
• For each vertex i from 1 to n:
– For each vertex j from 1 to n:
* Update D[i][j] as min(D[i][j], D[i][k] + D[k][j]).
After executing these steps, the matrix D will contain the shortest distances between all pairs of
vertices in the graph.
Pseudocode
Here is the pseudocode for the Floyd-Warshall algorithm:
155

Floyd-Warshall Algorithm
156
function floydWarshall(graph):
n = graph.numVertices
D = new matrix of size n x n
Initialize D with infinity values
for each edge (u, v) in graph:
D[u][v] = graph.weight(u, v)
for i from 1 to n:
D[i][i] = 0
for k from 1 to n:
for i from 1 to n:
for j from 1 to n:
D[i][j] = min(D[i][j], D[i][k] + D[k][j])
return D
Time Complexity
The time complexity of the Floyd-Warshall algorithm is O(n^3), where n is the number of vertices
in the graph. This is because the algorithm uses three nested loops to iterate over all vertices and
update the shortest distances.
Java Implementation
Here’s the Java implementation of the Floyd-Warshall algorithm:
public class FloydWarshall {
public int[][] floydWarshall(Graph graph) {
int n = graph.getNumVertices();
int[][] D = new int[n][n];
for (int i = 0; i < n; i++) {
Arrays.fill(D[i], Integer.MAX_VALUE);
}
for (int u = 0; u < n; u++) {
D[u][u] = 0;
for (Edge edge : graph.getEdges()) {

Floyd-Warshall Algorithm
157
int v = edge.getDestination();
int weight = edge.getWeight();
D[u][v] = Math.min(D[u][v], weight);
}
}
for (int k = 0; k < n; k++) {
for (int i = 0; i < n; i++) {
for (int j = 0; j < n; j++) {
if (D[i][k] != Integer.MAX_VALUE && D[k][j] != Integer.MAX_VALUE\
) {
D[i][j] = Math.min(D[i][j], D[i][k] + D[k][j]);
}
}
}
}
return D;
}
}
Python Implementation
And here’s the Python implementation of the Floyd-Warshall algorithm:
class FloydWarshall:
def floydWarshall(self, graph):
n = graph.getNumVertices()
D = [[float('inf')] * n for _ in range(n)]
for u in range(n):
D[u][u] = 0
for edge in graph.getEdges():
v = edge.getDestination()
weight = edge.getWeight()
D[u][v] = min(D[u][v], weight)
for k in range(n):
for i in range(n):
for j in range(n):
if D[i][k] != float('inf') and D[k][j] != float('inf'):

Floyd-Warshall Algorithm
158
D[i][j] = min(D[i][j], D[i][k] + D[k][j])
return D
With these Java and Python implementations of the Floyd-Warshall algorithm, you can find the
shortest distances between all pairs of vertices in a weighted directed graph.

Hamiltonian Path and Circuit
In graph theory, a Hamiltonian path is a path in a graph that visits every vertex exactly once. A
Hamiltonian circuit is a Hamiltonian path that starts and ends at the same vertex, forming a cycle.
Your task is to write a program that finds a Hamiltonian path or circuit in a given directed or
undirected graph, or determines that no such path or circuit exists.
Approach
Finding a Hamiltonian path or circuit in a graph is an NP-complete problem, which means there
is no known efficient algorithm that solves it for all types of graphs. However, for certain types of
graphs, we can use backtracking to find a solution.
To find a Hamiltonian path or circuit using backtracking, we can follow these steps:
1. Start with an empty path or circuit.
2. Choose a starting vertex.
3. Explore all possible paths or circuits by recursively visiting adjacent vertices.
4. Mark the visited vertices to avoid revisiting them.
5. If all vertices are visited and the current path or circuit forms a Hamiltonian path or circuit,
return the solution.
6. Backtrack by removing the last visited vertex and continue exploring other possibilities.
By systematically exploring all possible paths or circuits, we can find a Hamiltonian path or circuit
if one exists.
Pseudo Code
159

Hamiltonian Path and Circuit
160
function findHamiltonianPath(graph):
path = []
visited = set()
startVertex = chooseStartingVertex(graph)
explore(graph, startVertex, path, visited)
return path
function explore(graph, currentVertex, path, visited):
addVertexToPath(currentVertex, path)
visited.add(currentVertex)
if len(path) == len(graph.vertices):
return True
for nextVertex in graph.getAdjacentVertices(currentVertex):
if nextVertex not in visited:
if explore(graph, nextVertex, path, visited):
return True
removeLastVertexFromPath(path)
visited.remove(currentVertex)
return False
function chooseStartingVertex(graph):
return anyVertexInGraph(graph)
function addVertexToPath(vertex, path):
path.append(vertex)
function removeLastVertexFromPath(path):
path.pop()
Time Complexity Analysis
The time complexity of finding a Hamiltonian path or circuit using backtracking is O(N!), where N
is the number of vertices in the graph. This is because we need to explore all possible arrangements
of the vertices. The worst-case scenario occurs when the graph is complete, and there are N! possible
paths or circuits to explore.

Hamiltonian Path and Circuit
161
Java Implementation
Here’s the Java implementation of the findHamiltonianPath function:
import java.util.*;
public class HamiltonianPath {
public List<Integer> findHamiltonianPath(Graph graph) {
List<Integer> path = new ArrayList<>();
Set<Integer> visited = new HashSet<>();
int startVertex = chooseStartingVertex(graph);
explore(graph, startVertex, path, visited);
return path;
}
private boolean explore(Graph graph, int currentVertex, List<Integer> path, Set<\
Integer> visited) {
addVertexToPath(currentVertex, path);
visited.add(currentVertex);
if (path.size() == graph.getNumVertices()) {
return true;
}
for (int nextVertex : graph.getAdjacentVertices(currentVertex)) {
if (!visited.contains(nextVertex)) {
if (explore(graph, nextVertex, path, visited)) {
return true;
}
}
}
removeLastVertexFromPath(path);
visited.remove(currentVertex);
return false;
}
private int chooseStartingVertex(Graph graph) {
return graph.getAnyVertex();
}
private void addVertexToPath(int vertex, List<Integer> path) {

Hamiltonian Path and Circuit
162
path.add(vertex);
}
private void removeLastVertexFromPath(List<Integer> path) {
path.remove(path.size() - 1);
}
}
Python Implementation
And here’s the Python implementation of the findHamiltonianPath function:
class HamiltonianPath:
def findHamiltonianPath(self, graph):
path = []
visited = set()
startVertex = self.chooseStartingVertex(graph)
self.explore(graph, startVertex, path, visited)
return path
def explore(self, graph, currentVertex, path, visited):
self.addVertexToPath(currentVertex, path)
visited.add(currentVertex)
if len(path) == graph.getNumVertices():
return True
for nextVertex in graph.getAdjacentVertices(currentVertex):
if nextVertex not in visited:
if self.explore(graph, nextVertex, path, visited):
return True
self.removeLastVertexFromPath(path)
visited.remove(currentVertex)
return False
def chooseStartingVertex(self, graph):
return graph.getAnyVertex()
def addVertexToPath(self, vertex, path):
path.append(vertex)

Hamiltonian Path and Circuit
163
def removeLastVertexFromPath(self, path):
path.pop()
The Java and Python implementations provide a way to find a Hamiltonian path using the
backtracking algorithm. The findHamiltonianPath function takes a graph as input and returns a
list of vertices representing a Hamiltonian path. The explore function is used to recursively explore
all possible paths, and the chooseStartingVertex function determines the starting vertex for the
path.

N-Queens Problem
The N-Queens problem is a classic puzzle that asks for the placement of N queens on an N×N
chessboard such that no two queens threaten each other. In other words, no two queens should be
in the same row, column, or diagonal.
Your task is to write a program that finds all possible solutions to the N-Queens problem for a given
value of N.
Approach
To solve the N-Queens problem, we can use a backtracking algorithm. The backtracking algorithm
works as follows:
1. Start with an empty board of size N×N.
2. Place a queen in the first row.
3. Move to the next row and place a queen in an empty column, making sure it is not attacked
by any other queen on the board.
4. Repeat step 3 for each row until all N queens are placed on the board.
5. If all N queens are successfully placed, add the current arrangement to the list of solutions.
6. Backtrack to the previous row and try a different column for the queen placement.
7. Repeat steps 3 to 6 until all possible solutions are found.
By systematically exploring all possible arrangements, the backtracking algorithm ensures that each
solution is unique and satisfies the constraints of the N-Queens problem.
Pseudo Code
164

N-Queens Problem
165
function solveNQueens(n):
solutions = []
board = createEmptyBoard(n)
solve(board, 0, solutions)
return solutions
function solve(board, row, solutions):
if row == board.size():
addSolution(board, solutions)
return
for col in range(board.size()):
if isValidPlacement(board, row, col):
board.placeQueen(row, col)
solve(board, row + 1, solutions)
board.removeQueen(row, col)
function isValidPlacement(board, row, col):
for i in range(row):
if board.hasQueen(i, col) or board.hasQueen(i, col - (row - i)) or board.has\
Queen(i, col + (row - i)):
return False
return True
function addSolution(board, solutions):
solutions.add(copy(board))
function createEmptyBoard(n):
board = new Board(n)
return board
class Board:
constructor(n):
initialize an n×n empty board
function placeQueen(row, col):
place a queen at the specified position
function removeQueen(row, col):
remove the queen from the specified position
function hasQueen(row, col):

N-Queens Problem
166
check if a queen is present at the specified position
function size():
return the size of the board
Time Complexity Analysis
The time complexity of the N-Queens problem using the backtracking algorithm is O(N!), as we
need to explore all possible arrangements of the N queens on the board. The number of possible
arrangements grows rapidly with the size of N, making the problem computationally expensive for
large values of N.
Java Implementation
Here’s the Java implementation of the solveNQueens function:
import java.util.ArrayList;
import java.util.List;
public class NQueens {
public List<List<String>> solveNQueens(int n) {
List<List<String>> solutions = new ArrayList<>();
char[][] board = new char[n][n];
for (int i = 0; i < n; i++) {
for (int j = 0; j < n; j++) {
board[i][j] = '.';
}
}
solve(board, 0, solutions);
return solutions;
}
private void solve(char[][] board, int row, List<List<String>> solutions) {
if (row == board.length) {
addSolution(board, solutions);
return;
}
for (int col = 0; col < board.length; col++) {
if (isValidPlacement(board, row, col)) {

N-Queens Problem
167
board[row][col] = 'Q';
solve(board, row + 1, solutions);
board[row][col] = '.';
}
}
}
private boolean isValidPlacement(char[][] board, int row, int col) {
int n = board.length;
for (int i = 0; i < row; i++) {
if (board[i][col] == 'Q' || (col - (row - i) >= 0 && board[i][col - (row\
- i)] == 'Q')
|| (col + (row - i) < n && board[i][col + (row - i)] == 'Q')) {
return false;
}
}
return true;
}
private void addSolution(char[][] board, List<List<String>> solutions) {
List<String> solution = new ArrayList<>();
for (char[] row : board) {
solution.add(new String(row));
}
solutions.add(solution);
}
}
Python Implementation
And here’s the Python implementation of the solveNQueens function:

N-Queens Problem
168
class Solution:
def solveNQueens(self, n):
solutions = []
board = [['.' for _ in range(n)] for _ in range(n)]
self.solve(board, 0, solutions)
return solutions
def solve(self, board, row, solutions):
if row == len(board):
self.add_solution(board, solutions)
return
for col in range(len(board)):
if self.is_valid_placement(board, row, col):
board[row][col] = 'Q'
self.solve(board, row + 1, solutions)
board[row][col] = '.'
def is_valid_placement(self, board, row, col):
n = len(board)
for i in range(row):
if board[i][col] == 'Q' or (col - (row - i) >= 0 and board[i][col - (row\
- i)] == 'Q') \
or (col + (row - i) < n and board[i][col + (row - i)] == 'Q'):
return False
return True
def add_solution(self, board, solutions):
solution = [''.join(row) for row in board]
solutions.append(solution)
The Java and Python implementations provide a way to solve the N-Queens problem by using the
backtracking algorithm. The solveNQueens function takes the value of N as input and returns a list
of all possible solutions. Each solution is represented as a list of strings, where each string represents
a row of the chessboard. The solve function is used to recursively explore all possible arrangements,
and the isValidPlacement function checks if a queen can be placed in a given position.

Sudoku Solver
Write a program to solve a Sudoku puzzle. The input consists of an incomplete Sudoku board, where
some cells are filled with digits from 1 to 9, and other cells are empty (represented by 0). The goal
is to fill in the empty cells with digits from 1 to 9, following the Sudoku rules:
1. Each row must contain all digits from 1 to 9.
2. Each column must contain all digits from 1 to 9.
3. Each of the nine 3x3 sub-grids (also known as boxes) must contain all digits from 1 to 9.
Approach
To solve the Sudoku puzzle, we can use a backtracking algorithm. The backtracking algorithm works
as follows:
1. Find an empty cell in the Sudoku grid.
2. Try all possible digits from 1 to 9 for the empty cell.
3. If a digit is valid for the current cell, move to the next empty cell and repeat steps 2 and 3.
4. If no digit is valid for the current cell, backtrack to the previous cell and try a different digit.
5. Repeat steps 1 to 4 until the Sudoku puzzle is solved.
The backtracking algorithm guarantees that we explore all possible combinations of digits until a
valid solution is found. It works by making a guess and then undoing the guess if it leads to an
invalid solution.
Pseudo Code
169

Sudoku Solver
170
function solveSudoku(board):
if not findEmptyCell(board):
return True
row, col = findEmptyCell(board)
for digit in range(1, 10):
if isValid(board, row, col, digit):
board[row][col] = digit
if solveSudoku(board):
return True
board[row][col] = 0
return False
function findEmptyCell(board):
for row in range(9):
for col in range(9):
if board[row][col] == 0:
return row, col
return None
function isValid(board, row, col, digit):
for i in range(9):
if board[row][i] == digit or board[i][col] == digit:
return False
box_row = 3 * (row // 3)
box_col = 3 * (col // 3)
for i in range(box_row, box_row + 3):
for j in range(box_col, box_col + 3):
if board[i][j] == digit:
return False
return True

Sudoku Solver
171
Time Complexity Analysis
The time complexity of the Sudoku solver algorithm is exponential as the number of possible
solutions can be very large for some Sudoku puzzles. In the worst-case scenario, the time complexity
is O(9^(n*n)), where n is the size of the Sudoku board (typically n = 9 for a 9x9 Sudoku).
Java Implementation
Here’s the Java implementation of the solveSudoku function:
public class SudokuSolver {
public void solveSudoku(char[][] board) {
solve(board);
}
private boolean solve(char[][] board) {
int[] emptyCell = findEmptyCell(board);
if (emptyCell == null) {
return true;
}
int row = emptyCell[0];
int col = emptyCell[1];
for (char digit = '1'; digit <= '9'; digit++) {
if (isValid(board, row, col, digit)) {
board[row][col] = digit;
if (solve(board)) {
return true;
}
board[row][col] = '.';
}
}
return false;
}
private int[] findEmptyCell(char[][] board) {

Sudoku Solver
172
for (int row = 0; row < 9; row++) {
for (int col = 0; col < 9; col++) {
if (board[row][col] == '.') {
return new int[]{row, col};
}
}
}
return null;
}
private boolean isValid(char[][] board, int row, int col, char digit) {
for (int i = 0; i < 9; i++) {
if (board[row][i] == digit || board[i][col] == digit) {
return false;
}
}
int boxRow = 3 * (row / 3);
int boxCol = 3 * (col / 3);
for (int i = boxRow; i < boxRow + 3; i++) {
for (int j = boxCol; j < boxCol + 3; j++) {
if (board[i][j] == digit) {
return false;
}
}
}
return true;
}
}
Python Implementation
And here’s the Python implementation of the solveSudoku function:

Sudoku Solver
173
class Solution:
def solveSudoku(self, board):
self.solve(board)
def solve(self, board):
empty_cell = self.find_empty_cell(board)
if empty_cell is None:
return True
row, col = empty_cell
for digit in range(1, 10):
if self.is_valid(board, row, col, str(digit)):
board[row][col] = str(digit)
if self.solve(board):
return True
board[row][col] = '.'
return False
def find_empty_cell(self, board):
for row in range(9):
for col in range(9):
if board[row][col] == '.':
return row, col
return None
def is_valid(self, board, row, col, digit):
for i in range(9):
if board[row][i] == digit or board[i][col] == digit:
return False
box_row = 3 * (row // 3)
box_col = 3 * (col // 3)
for i in range(box_row, box_row + 3):
for j in range(box_col, box_col + 3):
if board[i][j] == digit:
return False

Sudoku Solver
174
return True
The Java and Python implementations provide a way to solve the Sudoku puzzle by using the
backtracking algorithm. The solveSudoku function takes the Sudoku board as input and modifies it
in-place to fill in the empty cells with the correct digits. The findEmptyCell function is used to find
the next empty cell in the Sudoku grid, and the isValid function checks if a digit is valid for a given
cell.

Recover Binary Search Tree
You are given a binary search tree (BST), where the values of two nodes have been swapped by
mistake. Your task is to recover the BST by swapping the nodes back to their correct positions.
Approach
One possible approach to solving this problem is to use the properties of a BST to identify the
swapped nodes and then swap them back to their correct positions.
In a valid BST, the inorder traversal (left-root-right) produces a sorted list of values. In our case, due
to the two nodes being swapped, the inorder traversal will yield an unsorted list with two pairs of
nodes in the wrong order.
To identify the swapped nodes, we can perform an inorder traversal of the BST while keeping track
of the previous node. Whenever we encounter a pair of nodes where the current node’s value is
smaller than the previous node’s value, we have found the swapped nodes. Let’s call these nodes
first and second.
After identifying the swapped nodes, we need to swap their values back to their correct positions.
This can be done by swapping the values of first and second.
Pseudo Code
function recoverTree(root):
first = null
second = null
prev = null
function inorderTraversal(node):
nonlocal first, second, prev
if node is null:
return
inorderTraversal(node.left)
if prev is not null and prev.val > node.val:
if first is null:
175

Recover Binary Search Tree
176
first = prev
second = node
prev = node
inorderTraversal(node.right)
inorderTraversal(root)
swapValues(first, second)
function swapValues(node1, node2):
temp = node1.val
node1.val = node2.val
node2.val = temp
Time Complexity Analysis
The time complexity of this approach is O(n), where n is the number of nodes in the BST. This is
because we perform an inorder traversal of the BST, which visits each node once.
Java Implementation
Here’s the Java implementation of the recoverTree function:
class TreeNode {
int val;
TreeNode left;
TreeNode right;
TreeNode(int val) {
this.val = val;
}
}
public class RecoverBST {
public void recoverTree(TreeNode root) {
TreeNode first = null;
TreeNode second = null;
TreeNode prev = null;

Recover Binary Search Tree
177
inorderTraversal(root, first, second, prev);
swapValues(first, second);
}
private void inorderTraversal(TreeNode node, TreeNode first, TreeNode second, Tr\
eeNode prev) {
if (node == null) {
return;
}
inorderTraversal(node.left, first, second, prev);
if (prev != null && prev.val > node.val) {
if (first == null) {
first = prev;
}
second = node;
}
prev = node;
inorderTraversal(node.right, first, second, prev);
}
private void swapValues(TreeNode node1, TreeNode node2) {
int temp = node1.val;
node1.val = node2.val;
node2.val = temp;
}
public static void main(String[] args) {
TreeNode root = new TreeNode(3);
root.left = new TreeNode(1);
root.right = new TreeNode(4);
root.right.left = new TreeNode(2);
RecoverBST recoverBST = new RecoverBST();
recoverBST.recoverTree(root);
}
}

Recover Binary Search Tree
178
Python Implementation
Here’s the Python implementation of the recoverTree function:
class TreeNode:
def __init__(self, val=0, left=None, right=None):
self.val = val
self.left = left
self.right = right
class RecoverBST:
def recoverTree(self, root):
first = None
second = None
prev = None
self.inorderTraversal(root, first, second, prev)
self.swapValues(first, second)
def inorderTraversal(self, node, first, second, prev):
if node is None:
return
self.inorderTraversal(node.left, first, second, prev)
if prev is not None and prev.val > node.val:
if first is None:
first = prev
second = node
prev = node
self.inorderTraversal(node.right, first, second, prev)
def swapValues(self, node1, node2):
node1.val, node2.val = node2.val, node1.val
root = TreeNode(3)
root.left = TreeNode(1)
root.right = TreeNode(4)
root.right.left = TreeNode(2)

Recover Binary Search Tree
179
recoverBST = RecoverBST()
recoverBST.recoverTree(root)
In the above code, we define a TreeNode class to represent the nodes of the BST. The recoverTree
function performs the recovery of the BST by calling the inorderTraversal function to identify the
swapped nodes and the swapValues function to swap their values back to their correct positions.
The main code demonstrates an example usage of the recoverTree function with a sample BST.
By following this approach, we can efficiently recover a binary search tree by identifying the
swapped nodes and swapping their values back to their correct positions.

Edit Distance
Given two strings word1 and word2, we want to convert word1 to word2 with the minimum number
of operations. The possible operations are:
1. Insert a character
2. Delete a character
3. Replace a character
Each operation has a cost associated with it. Your task is to find the minimum cost required to
transform word1 into word2.
Approach
We can solve this problem using dynamic programming. Let’s define a 2D array dp with dimensions
(m+1) x (n+1), where m is the length of word1 and n is the length of word2. The idea is to use dp[i][j]
to store the minimum cost required to convert the substring word1[0...i-1] to word2[0...j-1].
The base cases are as follows:
• If i == 0, it means we have an empty string word1. In this case, the cost of converting word1
to word2 is equal to the length of word2, so dp[0][j] = j for all j.
• If j == 0, it means we have an empty string word2. In this case, the cost of converting word1
to word2 is equal to the length of word1, so dp[i][0] = i for all i.
For the general case, we have two options:
• If the characters at positions i-1 and j-1 are equal, we don’t need to perform any operation,
so the cost remains the same. Hence, dp[i][j] = dp[i-1][j-1].
• If the characters at positions i-1 and j-1 are different, we need to choose the operation with
the minimum cost among the three possible operations: insertion, deletion, or replacement.
The cost of insertion is dp[i][j-1] + 1, the cost of deletion is dp[i-1][j] + 1, and the cost of
replacement is dp[i-1][j-1] + 1. We take the minimum of these three costs and assign it to
dp[i][j].
Finally, the minimum cost required to convert word1 to word2 is given by dp[m][n].
Pseudo Code
180

Edit Distance
181
function minDistance(word1, word2):
m = length of word1
n = length of word2
// Create a 2D array to store the minimum costs
dp = new 2D array with dimensions (m+1) x (n+1)
// Initialize the base cases
for i from 0 to m:
dp[i][0] = i
for j from 0 to n:
dp[0][j] = j
// Fill in the remaining cells of the dp array
for i from 1 to m:
for j from 1 to n:
if word1[i-1] == word2[j-1]:
dp[i][j] = dp[i-1][j-1]
else:
dp[i][j] = minimum of (dp[i][j-1] + 1, dp[i-1][j] + 1, dp[i-1][j-1] \
+ 1)
return dp[m][n]
Time Complexity Analysis
The time complexity of the above approach is O(m * n), where m and n are the lengths of the input
strings word1 and word2, respectively. This is because we fill in a 2D array of size (m+1) x (n+1)
using nested loops.
Java Implementation
Here’s the Java implementation of the minDistance function:

Edit Distance
182
public class EditDistance {
public static int minDistance(String word1, String word2) {
int m = word1.length();
int n = word2.length();
int[][] dp = new int[m+1][n+1];
for (int i = 0; i <= m; i++) {
dp[i][0] = i;
}
for (int j = 0; j <= n; j++) {
dp[0][j] = j;
}
for (int i = 1; i <= m; i++) {
for (int j = 1; j <= n; j++) {
if (word1.charAt(i-1) == word2.charAt(j-1)) {
dp[i][j] = dp[i-1][j-1];
} else {
dp[i][j] = Math.min(dp[i][j-1] + 1, Math.min(dp[i-1][j] + 1, dp[\
i-1][j-1] + 1));
}
}
}
return dp[m][n];
}
public static void main(String[] args) {
String word1 = "horse";
String word2 = "ros";
int minDistance = minDistance(word1, word2);
System.out.println("Minimum edit distance: " + minDistance);
}
}
Python Implementation
Here’s the Python implementation of the minDistance function:

Edit Distance
183
def minDistance(word1, word2):
m = len(word1)
n = len(word2)
dp = [[0] * (n+1) for _ in range(m+1)]
for i in range(m+1):
dp[i][0] = i
for j in range(n+1):
dp[0][j] = j
for i in range(1, m+1):
for j in range(1, n+1):
if word1[i-1] == word2[j-1]:
dp[i][j] = dp[i-1][j-1]
else:
dp[i][j] = min(dp[i][j-1] + 1, dp[i-1][j] + 1, dp[i-1][j-1] + 1)
return dp[m][n]
word1 = "horse"
word2 = "ros"
min_distance = minDistance(word1, word2)
print("Minimum edit distance:", min_distance)
In the above code, we initialize a 2D dp array to store the minimum costs. We then iterate through
the strings word1 and word2 to fill in the dp array using the bottom-up approach. Finally, we return
the value at dp[m][n], which represents the minimum edit distance between word1 and word2. The
main code demonstrates an example usage of the minDistance function with the strings “horse” and
“ros”.

Dungeon Game
You are trapped in a dungeon and need to find the minimum health points required to escape. The
dungeon consists of a 2D grid of size m x n, where each cell represents a room. Each room can have
a positive or negative number representing the health points that will be gained or lost in that room.
The dungeon is filled with monsters, and your goal is to reach the bottom-right room (the princess’s
room) with at least 1 health point.
You start at the top-left room (0, 0) and can only move down or right. The health points cannot drop
below 1. If at any point your health points reach 0 or below, you will die.
Write a function calculateMinimumHP to calculate the minimum health points required to reach the
princess’s room.
Approach
We can solve this problem using dynamic programming. Let’s define a 2D dp array of the same
size as the dungeon grid, where dp[i][j] represents the minimum health points required to reach
the princess’s room starting from room (i, j). We will calculate the dp values from bottom-right to
top-left.
The base case is dp[m-1][n-1] = max(1, 1 - dungeon[m-1][n-1]), which represents the minimum
health points required to reach the princess’s room from the room itself.
For the last row and last column, we can only move right or down, so the minimum health points
required are determined by the next room in that direction. For example, dp[i][n-1] = max(1,
dp[i+1][n-1] - dungeon[i][n-1]).
For the other rooms, we have two options: move down or move right. We choose the direction
that requires fewer health points. The minimum health points required to reach the princess’s room
starting from room (i, j) is given by:
dp[i][j] = max(1, min(dp[i+1][j], dp[i][j+1]) - dungeon[i][j])
Finally, the minimum health points required to escape the dungeon is given by dp[0][0].
Pseudocode
184

Dungeon Game
185
calculateMinimumHP(dungeon):
m = dungeon.length
n = dungeon[0].length
dp = new int[m][n]
dp[m-1][n-1] = max(1, 1 - dungeon[m-1][n-1])
for i from m-2 to 0:
dp[i][n-1] = max(1, dp[i+1][n-1] - dungeon[i][n-1])
for j from n-2 to 0:
dp[m-1][j] = max(1, dp[m-1][j+1] - dungeon[m-1][j])
for i from m-2 to 0:
for j from n-2 to 0:
dp[i][j] = max(1, min(dp[i+1][j], dp[i][j+1]) - dungeon[i][j])
return dp[0][0]
Complexity Analysis
The time complexity for the Dungeon Game algorithm is O(m * n), where m is the number of rows
and n is the number of columns in the dungeon. This is because we fill the dp array by iterating
through each room in the dungeon grid exactly once.
The space complexity is O(m * n) as well, since we use an additional dp array of the same size as
the dungeon grid.
Java Implementation
Here’s the Java implementation for the Dungeon Game problem:

Dungeon Game
186
public class DungeonGame {
public int calculateMinimumHP(int[][] dungeon) {
int m = dungeon.length;
int n = dungeon[0].length;
int[][] dp = new int[m][n];
dp[m - 1][n - 1] = Math.max(1, 1 - dungeon[m - 1][n - 1]);
for (int i = m - 2; i >= 0; i--) {
dp[i][n - 1] = Math.max(1, dp[i + 1][n - 1] - dungeon[i][n - 1]);
}
for (int j = n - 2; j >= 0; j--) {
dp[m - 1][j] = Math.max(1, dp[m - 1][j + 1] - dungeon[m - 1][j]);
}
for (int i = m - 2; i >= 0; i--) {
for (int j = n - 2; j >= 0; j--) {
dp[i][j] = Math.max(1, Math.min(dp[i + 1][j], dp[i][j + 1]) - dungeo\
n[i][j]);
}
}
return dp[0][0];
}
public static void main(String[] args) {
DungeonGame dungeonGame = new DungeonGame();
int[][] dungeon = {
{-2, -3, 3},
{-5, -10, 1},
{10, 30, -5}
};
System.out.println(dungeonGame.calculateMinimumHP(dungeon)); // Output: 7
}
}
In the above Java code, the calculateMinimumHP method takes a 2D dungeon array as input and
returns the minimum health points required to escape the dungeon. The algorithm fills the dp array
by iterating through each room in the dungeon grid from bottom-right to top-left, considering the
minimum health points required to reach the princess’s room.

Dungeon Game
187
The main function demonstrates an example usage of the calculateMinimumHP method.
Python Implementation
Here’s the Python implementation for the Dungeon Game problem:
class DungeonGame:
def calculateMinimumHP(self, dungeon: List[List[int]]) -> int:
m = len(dungeon)
n = len(dungeon[0])
dp = [[0] * n for _ in range(m)]
dp[m - 1][n - 1] = max(1, 1 - dungeon[m - 1][n - 1])
for i in range(m - 2, -1, -1):
dp[i][n - 1] = max(1, dp[i + 1][n - 1] - dungeon[i][n - 1])
for j in range(n - 2, -1, -1):
dp[m - 1][j] = max(1, dp[m - 1][j + 1] - dungeon[m - 1][j])
for i in range(m - 2, -1, -1):
for j in range(n - 2, -1, -1):
dp[i][j] = max(1, min(dp[i + 1][j], dp[i][j + 1]) - dungeon[i][j])
return dp[0][0]
dungeon_game = DungeonGame()
dungeon = [
[-2, -3, 3],
[-5, -10, 1],
[10, 30, -5]
]
print(dungeon_game.calculateMinimumHP(dungeon))
# Output: 7
In the above Python code, the calculateMinimumHP method takes a 2D dungeon list as input and
returns the minimum health points required to escape the dungeon. The algorithm fills the dp list
by iterating through each room in the dungeon grid from bottom-right to top-left, considering the
minimum health points required to reach the princess’s room.
The main code demonstrates an example usage of the calculateMinimumHP method.

Cherry Pickup
In a grid, each cell can contain either an empty cell, a cherry, or an obstacle denoted by 0, 1, or
-1, respectively. Two players start from the top-left cell and want to reach the bottom-right cell.
They can move one cell down or right at a time. The goal is to pick up cherries from the grid while
moving, subject to the following rules:
1. You and your friend start at the top-left cell (0, 0) and must reach the bottom-right cell (N-1,
N-1) by moving only right or down.
2. From each cell, you can move to the cell directly below it (i+1, j) or to the cell directly to the
right of it (i, j+1).
3. If a cell contains a thorn, you cannot pass through it.
4. If both you and your friend are on the same cell, only one of you can pick the cherry. You must
coordinate your movements to maximize the total number of cherries collected.
The task is to determine the maximum number of cherries that both players can pick up following
the rules.
Write a function cherryPickup that takes a 2D array grid representing the grid with cells, cherries,
and obstacles. The function should return the maximum number of cherries that both players can
pick up while moving from the top-left cell to the bottom-right cell.
Approach
To solve the Cherry Pickup problem, we can use dynamic programming with memoization. Since
both players start at the same cell and reach the bottom-right cell, we can use two dynamic
programming tables to keep track of the cherries picked up by both players simultaneously.
1. Define a recursive function dp(r1, c1, r2, c2) that calculates the maximum cherries that
both players can pick up while starting from cell (r1, c1) and (r2, c2) and reaching the
bottom-right cell. The players move simultaneously, so the row and column indices of both
players will be (r1, c1) and (r2, c2) at each step.
2. In the recursive function dp, check the following conditions:
• If either player reaches the bottom-right cell, return the number of cherries in that cell.
• If either player encounters an obstacle, return a negative value to indicate that this path
is not valid.
3. To avoid double-counting cherries when both players land on the same cell, subtract the value
of the cell from the answer if both players are on the same cell.
4. Use memoization to store the results of subproblems to avoid redundant calculations.
5. Finally, return the maximum cherries obtained by both players.
188

Cherry Pickup
189
Pseudo Code
cherryPickup(grid):
n = number of rows in grid
m = number of columns in grid
memo = new 4D array with size n x m x n x m
function dp(r1, c1, r2, c2):
if r1, c1, r2, c2 are out of grid bounds or grid[r1][c1] = -1 or grid[r2][c2\
] = -1:
return -infinity
if r1 = n-1 and c1 = m-1:
return grid[r1][c1]
if r2 = n-1 and c2 = m-1:
return grid[r2][c2]
if memo[r1][c1][r2][c2] is not empty:
return memo[r1][c1][r2][c2]
cherries = 0
if r1 = r2 and c1 = c2:
cherries = grid[r1][c1]
else:
cherries = grid[r1][c1] + grid[r2][c2]
ans = cherries + max(
dp(r1+1, c1, r2+1, c2),
dp(r1+1, c1, r2, c2+1),
dp(r1, c1+1, r2+1, c2),
dp(r1, c1+1, r2, c2+1)
)
memo[r1][c1][r2][c2] = ans
return ans
return max(0, dp(0, 0, 0, 0))
Complexity Analysis
The time complexity for the Cherry Pickup algorithm is O(n³⁾, ʷʰeʳe ⁿ ⁱˢ tʰe ˢⁱze of tʰe gʳⁱd. Tʰⁱˢ ⁱˢ becauˢe
ʷe ʰave a ʳecuʳˢⁱve fuⁿctⁱoⁿ dp tʰat makeˢ ⁴ ʳecuʳˢⁱve caˡˡˢ at eacʰ ˢtep, ʳeˢuˡtⁱⁿg ⁱⁿ a maˣⁱmum of O⁽ⁿ2) subproblems. Each
subproblem takes O(1) time to solve. Therefore, the overall time complexity is O(n^3).

Cherry Pickup
190
The space complexity is also O(n^3) due to the memoization array, which stores the results of
subproblems.
Java Implementation
Here’s the Java implementation for the Cherry Pickup problem:
public class CherryPickup {
public int cherryPickup(int[][] grid) {
int n = grid.length;
int m = grid[0].length;
int[][][][] memo = new int[n][m][n][m];
return Math.max(0, dp(0, 0, 0, 0, grid, memo));
}
private int dp(int r1, int c1, int r2, int c2, int[][] grid, int[][][][] memo) {
int n = grid.length;
int m = grid[0].length;
if (r1 < 0 || c1 < 0 || r2 < 0 || c2 < 0 || r1 >= n || c1 >= m || r2 >= n ||\
c2 >= m || grid[r1][c1] == -1 || grid[r2][c2] == -1) {
return Integer.MIN_VALUE;
}
if (r1 == n - 1 && c1 == m - 1) {
return grid[r1][c1];
}
if (r2 == n - 1 && c2 == m - 1) {
return grid[r2][c2];
}
if (memo[r1][c1][r2][c2] != 0) {
return memo[r1][c1][r2][c2];
}
int cherries = 0;
if (r1 == r2 && c1 == c2) {
cherries = grid[r1][c1];
} else {

Cherry Pickup
191
cherries = grid[r1][c1] + grid[r2][c2];
}
int maxCherries = cherries + Math.max(
Math.max(dp(r1 + 1, c1, r2 + 1, c2, grid, memo), dp(r1 + 1, c1, r2, \
c2 + 1, grid, memo)),
Math.max(dp(r1, c1 + 1, r2 + 1, c2, grid, memo), dp(r1, c1 + 1, r2, \
c2 + 1, grid, memo))
);
memo[r1][c1][r2][c2] = maxCherries;
return maxCherries;
}
public static void main(String[] args) {
CherryPickup cherryPickup = new CherryPickup();
int[][] grid = {
{0, 1, -1},
{1, 0, -1},
{1, 1,
1}
};
System.out.println(cherryPickup.cherryPickup(grid)); // Output: 5
}
}
Python Implementation
Here’s the Python implementation for the Cherry Pickup problem:
class CherryPickup:
def cherryPickup(self, grid: List[List[int]]) -> int:
n = len(grid)
m = len(grid[0])
memo = [[[[0] * m for _ in range(n)] for _ in range(m)] for _ in range(n)]
return max(0, self.dp(0, 0, 0, 0, grid, memo))
def dp(self, r1, c1, r2, c2, grid, memo):
n = len(grid)

Cherry Pickup
192
m = len(grid[0])
if r1 < 0 or c1 < 0 or r2 < 0 or c2 < 0 or r1 >= n or c1 >= m or r2 >= n or \
c2 >= m or grid[r1][c1] == -1 or grid[r2][c2] == -1:
return float('-inf')
if r1 == n - 1 and c1 == m - 1:
return grid[r1][c1]
if r2 == n - 1 and c2 == m - 1:
return grid[r2][c2]
if memo[r1][c1][r2][c2] != 0:
return memo[r1][c1][r2][c2]
cherries = 0
if r1 == r2 and c1 == c2:
cherries = grid[r1][c1]
else:
cherries = grid[r1][c1] + grid[r2][c2]
maxCherries = cherries + max(
self.dp(r1 + 1, c1, r2 + 1, c2, grid, memo),
self.dp(r1 + 1, c1, r2, c2 + 1, grid, memo),
self.dp(r1, c1 + 1, r2 + 1, c2, grid, memo),
self.dp(r1, c1 + 1, r2, c2 + 1, grid, memo)
)
memo[r1][c1][r2][c2] = maxCherries
return maxCherries
cherryPickup = CherryPickup()
grid = [
[0, 1, -1],
[1, 0, -1],
[1, 1,
1]
]
print(cherryPickup.cherryPickup(grid)) # Output: 5
In the above Python code, the CherryPickup class encapsulates the solution. The cherryPickup
function takes a 2D grid as input and returns the maximum number of cherries that both players

Cherry Pickup
193
can pick up. The dp function is the recursive function that calculates the maximum cherries by
considering various possible moves. The memo array is used for memoization to avoid redundant
calculations.
The main function demonstrates an example usage of the CherryPickup class.
Feel free to run the code and modify it to test other test cases.

Travelling Salesman Problem
The Travelling Salesman Problem (TSP) is a classic optimization problem in computer science and
operations research. The problem is defined as follows: given a list of cities and the distances
between each pair of cities, find the shortest possible route that visits each city exactly once and
returns to the starting city.
In other words, the goal is to find the optimal Hamiltonian cycle in a complete weighted graph,
where the vertices represent cities and the edge weights represent the distances between cities.
Your task is to write a program that solves the Travelling Salesman Problem and finds the shortest
route that visits all cities exactly once.
Approach
The Travelling Salesman Problem is an NP-hard problem, meaning there is no known polynomial-
time algorithm that solves it for all inputs.
However, there are several algorithms that can
approximate the solution and provide good results for practical purposes.
One popular approach to solving the Travelling Salesman Problem is using the Held-Karp algorithm,
also known as dynamic programming with bitmasks. The algorithm explores all possible subsets of
cities and computes the shortest path for each subset. It uses memoization to store the computed
results and avoids redundant calculations.
The steps for solving the Travelling Salesman Problem using the Held-Karp algorithm are as follows:
1. Create a memoization table to store the computed results for subsets of cities.
2. Initialize the table with the base cases: the shortest path from the starting city to each individual
city.
3. For each subset of cities of size k (starting from k = 2 up to the total number of cities):
• For each destination city in the subset:
– Compute the shortest path from the starting city to the destination city through each
possible intermediate city.
– Update the memoization table with the shortest path for the current subset and
destination city.
4. Finally, find the shortest path that visits all cities exactly once and returns to the starting city
by iterating through the last row of the memoization table.
Pseudo Code
194

Travelling Salesman Problem
195
function tsp(graph):
numCities = graph.getNumCities()
memoization = new 2D array[numCities][2^numCities]
for each city in graph:
memoization[city][{city}] = graph.getDistance(startCity, city)
for k = 2 to numCities:
for each subset of cities of size k:
for each destinationCity in subset:
if destinationCity != startCity:
shortestPath = INFINITY
for each intermediateCity in subset:
if intermediateCity != destinationCity:
subsetWithoutDestination = subset - {destinationCity}
currentPath = memoization[intermediateCity][subsetWithou\
tDestination] + graph.getDistance(intermediateCity, destinationCity)
shortestPath = min(shortestPath, currentPath)
memoization[destinationCity][subset] = shortestPath
shortestPath = INFINITY
for each destinationCity in graph:
if destinationCity != startCity:
currentPath = memoization[destinationCity][allCitiesExceptStart]
shortestPath = min(shortestPath, currentPath + graph.getDistance(destina\
tionCity, startCity))
return shortestPath
Time Complexity Analysis
The time complexity of the Held-Karp algorithm for solving the Travelling Salesman Problem is
O(n^2 * 2^n), where n is the number of cities. The algorithm iterates through all subsets of cities,
and for each subset, it computes the shortest path through each intermediate city. Since there are
2^n subsets and each computation takes O(n) time, the overall time complexity is O(n^2 * 2^n).
Java Implementation
Here’s the Java implementation of the tsp function using the Held-Karp algorithm:

Travelling Salesman Problem
196
public class TSP {
public int tsp(Graph graph) {
int numCities = graph.getNumCities();
int[][] memoization = new int[numCities][1 << numCities];
for (int city = 0; city < numCities; city++) {
memoization[city][1 << city] = graph.getDistance(graph.getStartCity(), c\
ity);
}
for (int k = 2; k <= numCities; k++) {
for (int subset = 0; subset < (1 << numCities); subset++) {
if (Integer.bitCount(subset) == k) {
for (int destinationCity = 0; destinationCity < numCities; desti\
nationCity++) {
if (destinationCity != graph.getStartCity() && isBitSet(subs\
et, destinationCity)) {
int shortestPath = Integer.MAX_VALUE;
for (int intermediateCity = 0; intermediateCity < numCit\
ies; intermediateCity++) {
if (intermediateCity != destinationCity && isBitSet(\
subset, intermediateCity)) {
int subsetWithoutDestination = subset ^ (1 << de\
stinationCity);
int currentPath = memoization[intermediateCity][\
subsetWithoutDestination] + graph.getDistance(intermediateCity, destinationCity);
shortestPath = Math.min(shortestPath, currentPat\
h);
}
}
memoization[destinationCity][subset] = shortestPath;
}
}
}
}
}
int shortestPath = Integer.MAX_VALUE;
int allCitiesExceptStart = (1 << numCities) - 1 - (1 << graph.getStartCity()\
);
for (int destinationCity = 0; destinationCity < numCities; destinationCity++\
) {
if (destinationCity != graph.getStartCity()) {

Travelling Salesman Problem
197
int currentPath = memoization[destinationCity][allCitiesExceptStart]\
+ graph.getDistance(destinationCity, graph.getStartCity());
shortestPath = Math.min(shortestPath, currentPath);
}
}
return shortestPath;
}
private boolean isBitSet(int num, int bitIndex) {
return ((num >> bitIndex) & 1) == 1;
}
}
The tsp function takes a graph as input and returns the length of the shortest Hamiltonian cycle
that visits all cities exactly once. It uses the Held-Karp algorithm with memoization to solve the
Travelling Salesman Problem efficiently.
Python Implementation
And here’s the Python implementation of the tsp function using the Held-Karp algorithm:
class TSP:
def tsp(self, graph):
num_cities = graph.get_num_cities()
memoization = [[0] * (1 << num_cities) for _ in range(num_cities)]
for city in range(num_cities):
memoization[city][1 << city] = graph.get_distance(graph.get_start_city()\
, city)
for k in range(2, num_cities + 1):
for subset in range(1 << num_cities):
if bin(subset).count('1') == k:
for destination_city in range(num_cities):
if destination_city != graph.get_start_city() and (subset >>\
destination_city) & 1:
shortest_path = float('inf')
for intermediate_city in range(num_cities):
if intermediate_city != destination_city and (subset\
>> intermediate_city) & 1:
subset_without_destination = subset ^ (1 << dest\

Travelling Salesman Problem
198
ination_city)
current_path = memoization[intermediate_city][su\
bset_without_destination] + graph.get_distance(intermediate_city, destination_city)
shortest_path = min(shortest_path, current_path)
memoization[destination_city][subset] = shortest_path
shortest_path = float('inf')
all_cities_except_start = (1 << num_cities) - 1 - (1 << graph.get_start_city\
())
for destination_city in range(num_cities):
if destination_city != graph.get_start_city():
current_path = memoization[destination_city][all_cities_except_start\
] + graph.get_distance(destination_city, graph.get_start_city())
shortest_path = min(shortest_path, current_path)
return shortest_path
The tsp method takes a graph as input and returns the length of the shortest Hamiltonian cycle
that visits all cities exactly once. It uses the Held-Karp algorithm with memoization to solve the
Travelling Salesman Problem efficiently.
Now that we have the Java and Python implementations of the tsp function using the Held-Karp
algorithm, we can solve the Travelling Salesman Problem and find the shortest route that visits all
cities exactly once.

Part III: Appendix

Proof of Optimal Substructure
Property
The optimal substructure property is a fundamental concept in dynamic programming. It states
that if an optimal solution to a problem can be obtained by combining optimal solutions to its
subproblems, then the problem exhibits optimal substructure.
In this chapter, we will explore the proof of the optimal substructure property and its significance
in dynamic programming. Understanding this property is crucial as it forms the foundation for
designing and solving problems using dynamic programming techniques.
Understanding Optimal Substructure
Optimal substructure can be thought of as a recursive property, where an optimal solution to the
problem can be constructed from optimal solutions of its subproblems. This property enables us
to break down a complex problem into smaller, more manageable subproblems and solve them
independently.
To demonstrate the optimal substructure property, we need to establish two key aspects:
1. Overlapping Subproblems: The problem should exhibit overlapping subproblems, meaning
that the same subproblems are encountered multiple times during the solution process. This
repetition allows us to store and reuse the results of solved subproblems, avoiding redundant
computation.
2. Optimal Solution Reconstruction: Given the optimal solutions to the subproblems, we should
be able to construct an optimal solution to the original problem. This step typically involves
making decisions based on the solutions of the subproblems and determining the best course
of action to achieve the optimal solution.
Proof of Optimal Substructure
To prove the optimal substructure property, we use a technique called proof by contradiction. We
assume that a problem does not possess optimal substructure and then show that this assumption
leads to a contradiction.
Let’s consider a problem P and assume it does not exhibit optimal substructure. This means that
the optimal solution to problem P cannot be derived from the optimal solutions of its subproblems.
200

Proof of Optimal Substructure Property
201
However, if we can show that the optimal solution to problem P can indeed be constructed using
optimal solutions of its subproblems, we have a contradiction, proving that our initial assumption
was incorrect.
The proof typically involves breaking down the problem into subproblems, solving them optimally,
and then combining their solutions to obtain the optimal solution for the original problem. By
demonstrating this step-by-step construction, we establish that the problem possesses optimal
substructure.
Significance of Optimal Substructure
The optimal substructure property is a fundamental property in dynamic programming that allows
us to solve complex problems efficiently. By decomposing a problem into smaller subproblems and
reusing their solutions, we can avoid redundant computation and greatly improve the efficiency of
our algorithm.
Dynamic programming algorithms rely heavily on the optimal substructure property to design
efficient solutions. By leveraging the knowledge of optimal solutions to subproblems, dynamic
programming algorithms can build up the solution to the original problem incrementally, ensuring
that the final solution is optimal.
Conclusion
The proof of the optimal substructure property is a critical aspect of dynamic programming. By
establishing the presence of overlapping subproblems and demonstrating the construction of an
optimal solution from the optimal solutions of subproblems, we validate the optimal substructure
property. This property forms the basis for the design and efficiency of dynamic programming
algorithms.
Understanding the optimal substructure property allows us to identify and solve problems using dy-
namic programming techniques effectively. By breaking down a problem into smaller subproblems
and reusing their solutions, we can solve complex problems efficiently and obtain optimal solutions.

Derivation of Memoization and
Tabulation Techniques
In this appendix, we will delve into the derivation and explanation of two key techniques used in
dynamic programming: memoization and tabulation. These techniques serve as powerful tools for
optimizing the computation of solutions to dynamic programming problems. We will explore their
underlying principles, benefits, and considerations for implementation.
Memoization Technique
Memoization is a top-down approach that involves caching the results of expensive function
calls and reusing them when the same inputs occur again. This technique eliminates redundant
calculations, significantly improving the efficiency of dynamic programming algorithms.
Principle of Memoization
The principle of memoization can be summarized as follows:
1. When a function is called with a set of input parameters, check if the function has already
computed and stored the result for those inputs.
2. If the result is present in the cache (also known as a memoization table), return it instead of
recomputing the function.
3. If the result is not available, compute the result and store it in the cache before returning it.
Benefits of Memoization
The memoization technique offers several benefits:
1. Reduced Recomputation: By storing previously computed results, we can avoid redundant
function calls, leading to significant time savings, especially when solving problems with
overlapping subproblems.
2. Improved Time Complexity: Memoization allows us to trade off space complexity for
improved time complexity. By caching results, we can achieve linear or near-linear time
complexity for problems that would otherwise have exponential time complexity.
3. Simplified Code: Memoization simplifies code implementation by separating the logic for
solving subproblems from the logic for reusing previously computed results. This separation
leads to cleaner and more modular code.
202

Derivation of Memoization and Tabulation Techniques
203
Implementation Considerations
When implementing memoization, there are a few considerations to keep in mind:
1. Data Structure for Caching: Choose an appropriate data structure to store the cached results.
Common choices include arrays, hash maps, or memoization tables.
2. Handling Boundary Cases: Ensure proper handling of boundary cases or edge scenarios in
your memoization implementation. These cases might include handling negative inputs, zero
inputs, or other special conditions.
3. Clearing the Cache: In some scenarios, it might be necessary to clear the cache between
different function calls or problem instances to prevent interference or excessive memory usage.
Tabulation Technique
Tabulation is a bottom-up approach that involves building a table (also known as a DP table) and
populating it with intermediate results of subproblems. By systematically filling in the table, we can
compute the solution to the original problem efficiently.
Principle of Tabulation
The principle of tabulation can be summarized as follows:
1. Create a table with dimensions corresponding to the size of the input problem.
2. Initialize the table with base cases or initial values that can be computed directly.
3. Iterate over the table in a systematic manner, filling in each cell based on the values of
previously filled cells and the problem’s recurrence relation.
4. The final value in the table represents the solution to the original problem.
Benefits of Tabulation
The tabulation technique offers several benefits:
1. Elimination of Recursion: Tabulation eliminates the need for recursive function calls and their
associated overhead, resulting in improved performance for problems that have overlapping
subproblems.
2. Optimal Space Complexity: Tabulation often requires less memory compared to memoization
because it avoids the need to store recursive call stacks or maintain a cache of results.
3. Better Comprehension of Bottom-Up Approach: Tabulation provides a clear and intuitive
understanding of the bottom-up approach, as it involves systematically filling in the table and
computing results in a structured manner.

Derivation of Memoization and Tabulation Techniques
204
Implementation Considerations
When implementing tabulation, consider the following points
1. Table Size and Initialization: Determine the dimensions of the table based on the problem’s
input size and initialize the table with base cases or initial values.
2. Iteration Order: Decide on the order of iteration over the table cells to ensure that all required
dependencies are already computed when filling in a particular cell.
3. Optimizing Space Complexity: If the problem’s solution depends only on a fixed number of
previous states or a small window of previous states, it may be possible to optimize the space
complexity by using rolling arrays or variables instead of a complete table.
Conclusion
Memoization and tabulation are two powerful techniques in dynamic programming that enable us
to solve complex problems efficiently. Memoization optimizes computation by caching results and
reusing them when needed, while tabulation builds a table and fills it in a bottom-up manner. Both
techniques help us avoid redundant calculations and achieve optimal solutions.
Understanding the derivation and implementation of memoization and tabulation is crucial for
effectively applying dynamic programming to solve real-world problems.
By mastering these
techniques, you can design efficient algorithms, optimize time and space complexities, and tackle a
wide range of challenging programming problems.

Problem Analysis
In this appendix, we will dive deep into the process of problem analysis for dynamic programming.
Problem analysis is a critical step that allows us to understand the problem requirements, constraints,
and potential solutions.
By thoroughly analyzing the problem, we can formulate an effective
dynamic programming approach. We will discuss the key components of problem analysis and
provide examples to illustrate their application.
Understanding the Problem
The first step in problem analysis is gaining a clear understanding of the problem statement. Read
the problem statement carefully and identify the following:
1. Input: Determine the input parameters required by the problem. These can include integers,
arrays, strings, graphs, or any other data structures.
2. Output: Identify the desired output of the problem. It could be a single value, an array, a
matrix, or any other data structure.
3. Constraints: Note any constraints or limitations specified in the problem statement. This
could include the size of the input, range of values, or any specific conditions that must be
satisfied.
4. Example: Analyze the given example(s) to understand the problem requirements and expected
output. Use the examples to gain insights into the problem’s behavior and patterns.
Identifying Subproblems
Once we have a clear understanding of the problem, we can start identifying subproblems.
Subproblems are smaller versions of the main problem that we can solve independently. Look
for patterns or recurring structures within the problem that can be broken down into smaller
components.
1. Recursive Nature: Determine if the problem exhibits a recursive nature, where solving a larger
problem relies on solving smaller subproblems. This is a common characteristic of problems
suitable for dynamic programming.
2. Breaking Down the Problem: Identify how the problem can be divided into smaller parts.
This can involve splitting the problem into subarrays, subproblems related to different
elements, or any other logical breakdown.
205

Problem Analysis
206
Recognizing Overlapping Subproblems
One crucial aspect of dynamic programming is recognizing and exploiting overlapping subproblems.
Overlapping subproblems occur when the same subproblems are solved multiple times within the
solution process. By avoiding redundant computations, we can optimize the solution.
1. Memoization Potential: Determine if there are subproblems that can be memoized, meaning
their solutions can be stored for future use. Memoization helps avoid recomputing the same
subproblem multiple times, reducing time complexity.
2. Identifying Repetition: Analyze the problem structure and patterns to identify if the same
subproblems are encountered multiple times during the solution process. This indicates the
presence of overlapping subproblems.
Formulating the Dynamic Programming Approach
Based on the problem analysis, we can now formulate the dynamic programming approach. This
involves defining the recurrence relation and designing the solution strategy.
1. Recurrence Relation: Express the problem as a recurrence relation, which defines the
relationship between a larger problem and its subproblems. This relation allows us to break
down the problem and express it in terms of smaller subproblems.
2. Base Cases: Identify the base cases or initial conditions that serve as the starting point for
solving the subproblems. Base cases define the termination conditions for the recursive process.
3. Building the Solution: Determine how the solutions to smaller subproblems can be combined
to solve larger subproblems and ultimately the main problem. This involves designing the
approach for bottom-up or top-down dynamic programming.
Examples of Problem Analysis
To illustrate the problem analysis process, let’s consider a few examples:
1. Fibonacci Sequence: In the Fibonacci sequence problem, we identify that the main problem
can be broken down into smaller subproblems of calculating Fibonacci numbers for smaller
indices. By recognizing the recursive nature of the problem and the presence of overlapping
subproblems, we can formulate a dynamic programming approach using memoization or
tabulation.

Problem Analysis
207
2. Longest Increasing Subsequence: For the longest increasing subsequence problem, we
observe that the length of the longest increasing subsequence for a given array depends on
the lengths of the subproblems involving smaller subsequences. By identifying the repetitive
nature of the subproblems, we can design a dynamic programming solution that builds upon
the solutions of smaller subproblems.
3. Knapsack Problem: In the knapsack problem, we recognize that the problem can be divided
into subproblems involving different items and varying capacities of the knapsack.
By
formulating the recurrence relation and designing a dynamic programming approach, we can
efficiently solve the problem by considering the optimal solutions to smaller subproblems.
By applying problem analysis techniques, we can gain a deeper understanding of the problem,
identify the key components, and formulate an effective dynamic programming approach. This
analysis lays the foundation for developing efficient and optimized solutions.

Selecting the Right Data Structures
for Dynamic Programming Efficiency
In this appendix, we will explore the importance of selecting appropriate data structures when
implementing dynamic programming algorithms. Choosing the right data structures can signifi-
cantly enhance the efficiency and performance of dynamic programming solutions. We will discuss
various factors to consider when selecting data structures and provide examples of commonly used
structures in dynamic programming.
Understanding the Data Requirements
Before selecting data structures, it is crucial to have a clear understanding of the data requirements
of the problem. Consider the following aspects:
1. Data Size: Evaluate the size of the input data and the potential growth rate. This information
will help determine the space complexity of the solution and guide the selection of suitable
data structures.
2. Data Types: Identify the types of data elements involved in the problem. Determine if the
data is numeric, boolean, characters, or custom objects. This knowledge will aid in selecting
appropriate data structures that can efficiently handle the data types.
3. Data Operations: Analyze the operations that need to be performed on the data. Consider
if you need to perform lookups, insertions, deletions, or modifications.
Different data
structures excel at different operations, so understanding the required operations will guide
your selection.
Commonly Used Data Structures in Dynamic
Programming
Here are some commonly used data structures in dynamic programming and their characteristics:
1. Arrays: Arrays provide a contiguous block of memory to store elements. They offer fast
random access to elements and are suitable for problems involving sequences or matrices.
Arrays are efficient for read and write operations but have a fixed size, making them less
flexible for dynamic data.
208

Selecting the Right Data Structures for Dynamic Programming Efficiency
209
2. Matrices: Matrices are two-dimensional arrays often used to represent grid-like structures.
They are suitable for problems involving grid-based calculations, such as pathfinding or image
processing. Matrices provide efficient access to elements based on row and column indices.
3. Lists: Lists, such as linked lists or dynamic arrays, offer flexibility in handling dynamically
changing data. They allow for efficient insertions and deletions at any position, making them
suitable for problems with dynamic data sizes or frequent modifications.
4. Maps: Maps, also known as dictionaries or hash tables, provide key-value pair storage. They
enable fast lookup operations based on keys and are useful for problems that require associating
values with specific keys or performing efficient lookups.
5. Sets: Sets store unique elements and offer efficient operations such as element insertion,
deletion, and membership testing. They are suitable for problems involving uniqueness, such
as finding distinct elements or checking for existence.
6. Priority Queues: Priority queues store elements based on their priority and offer efficient
operations for inserting and retrieving the highest-priority element.
They are useful for
problems that involve prioritization or ordering of elements, such as scheduling or task
optimization.
Data Structure Selection Considerations
When selecting data structures for dynamic programming algorithms, consider the following factors:
1. Time Complexity: Analyze the time complexity of data structure operations. Choose data
structures that offer efficient operations for the problem’s requirements. Consider the trade-
offs between read, write, insert, delete, and search operations.
2. Space Complexity: Evaluate the space complexity of the data structures. Ensure that the
selected structures can handle the problem’s data size within the available memory constraints.
3. Compatibility with Problem Operations: Assess how well the data structure aligns with the
required operations for the problem. Ensure that the chosen structure supports the necessary
operations with optimal efficiency.
4. Data Structure Dependencies: Consider if the chosen data structures have dependencies or
interactions with other parts of the dynamic programming solution. Ensure that the structures
work well together and complement each other’s functionalities.
Examples of Data Structure Selection
To illustrate the importance of data structure selection, let’s consider a few examples:
1. Fibonacci Sequence: For calculating the Fibonacci sequence, an array can be used to store
intermediate results. By utilizing memoization, the array provides efficient access to previously
computed Fibonacci numbers, reducing the time complexity from exponential to linear.

Selecting the Right Data Structures for Dynamic Programming Efficiency
210
2. Knapsack Problem: The knapsack problem can be solved using a 2D matrix, where the rows
represent items and the columns represent the capacity of the knapsack. The matrix allows for
efficient storage and retrieval of computed subproblem results.
3. String Edit Distance: The string edit distance problem can be solved using a 2D matrix to
store the minimum edit distances between substrings. The matrix facilitates efficient retrieval
and updating of subproblem solutions.
Conclusion
Selecting the right data structures is crucial for enhancing the efficiency and performance of dynamic
programming algorithms. By understanding the data requirements, considering the commonly used
data structures, and evaluating their time and space complexities, you can make informed decisions
and design optimal solutions. Choosing appropriate data structures significantly contributes to the
overall efficiency and effectiveness of dynamic programming solutions.


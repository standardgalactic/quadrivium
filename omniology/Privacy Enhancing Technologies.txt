Lecture Notes in Computer Science
7384
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Alfred Kobsa
University of California, Irvine, CA, USA
Friedemann Mattern
ETH Zurich, Switzerland
John C. Mitchell
Stanford University, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz
University of Bern, Switzerland
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Germany
Madhu Sudan
Microsoft Research, Cambridge, MA, USA
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbruecken, Germany

Simone Fischer-Hübner Matthew Wright (Eds.)
Privacy Enhancing
Technologies
12th International Symposium, PETS 2012
Vigo, Spain, July 11-13, 2012
Proceedings
1 3

Volume Editors
Simone Fischer-Hübner
Karlstad University
Department of Computer Science
Universitetsgatan 2, 65188, Karlstad, Sweden
E-mail: simone.ﬁscher-huebner@kau.se
Matthew Wright
University of Texas at Arlington
Department of Computer Science and Engineering
500 UTA Blvd., Arlington, TX 76019, USA
E-mail: mwright@uta.edu
ISSN 0302-9743
e-ISSN 1611-3349
ISBN 978-3-642-31679-1
e-ISBN 978-3-642-31680-7
DOI 10.1007/978-3-642-31680-7
Springer Heidelberg Dordrecht London New York
Library of Congress Control Number: 2012940986
CR Subject Classiﬁcation (1998): K.6.5, D.4.6, C.2, E.3, H.3-4, J.1
LNCS Sublibrary: SL 4 – Security and Cryptology
© Springer-Verlag Berlin Heidelberg 2012
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws
and regulations and therefore free for general use.
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
Privacy and anonymity are increasingly important in the online world. Corpo-
rations, governments, and other organizations are realizing and exploiting their
power to track users and their behavior. Approaches to protecting individu-
als, groups, but also companies and governments from proﬁling and censorship
include decentralization, encryption, distributed trust, and automated policy
disclosure.
The 2012 Privacy Enhancing Technologies Symposium (PETS 2012) ad-
dressed the design and realization of such privacy services for the Internet and
other data systems and communication networks by bringing together privacy
and anonymity experts from around the world to discuss recent advances and
new perspectives.
PETS 2012 was held in Vigo, Spain, during July 11–13, 2012. It was the 12th
in the series of events, and the ﬁfth after the transition from workshop to sympo-
sium. The PETS symposium remains a premier scientiﬁc international event for
publishing on both the theory and practice of privacy-enhancing technologies,
and it has a broad scope that includes all facets of the ﬁeld.
PETS 2012 received 72 submissions, which were all reviewed by at least three
members of the international Program Committee (PC). Based on an intensive
discussion among the reviewers and other PC members, 16 papers were ﬁnally
accepted for presentation at the PETS symposium. Topics addressed by the
accepted papers published in the proceedings include anonymization of statistics,
content, and traﬃc, network traﬃc analysis, censorship-resistant systems, user
proﬁling, training users in privacy risk management, and privacy for Internet
and cloud-based services.
A further highlight of PETS 2012 was the popular HotPETs session, designed
as a venue to present exciting but still preliminary and evolving ideas, rather than
formal and rigorous completed research results. HotPETs included an invited
keynote talk by Moez Chakchouk, the head of the Tunisian Internet Agency. As
with the previous four HotPETs in the past, there were no published proceedings
for HotPETs. PETS also included a panel on “The impact of upcoming privacy
legislation for PETs” organized and moderated by Marit Hansen and a rump
session with brief presentations on a variety of topics. Additionally, a workshop
on Provable Privacy was held in conjunction with PETS 2012.
We would like to thank all PETS and HotPETs authors, especially those
who presented their work selected for the program, as well as all rump session
presenters. Moreover, we are very grateful to all PC members and additional re-
viewers, who contributed with thorough reviews and actively participated in the
PC discussions, ensuring a high quality of all accepted papers. We owe special
thanks to the following PC members and reviewers, who volunteered to shepherd

VI
Preface
some of the accepted papers: Emiliano De Cristofaro, Erman Ayday, Roger
Dingledine, Thomas S. Benjamin, Nicholas Hopper, Aaron Johnson, Damon
McCoy, and Arvind Narayanan.
We gratefully acknowledge the outstanding contributions of the PETS 2012
General Chair, Carmela Troncoso, the Local Arrangements Chair, Fernando
P´erez-Gonz´alez, and of our webmaster since 2007, Jeremy Clark. Moreover, our
gratitude goes to the HotPETs 2012 Chairs, Emiliano De Cristofaro and Julien
Freudiger, who reviewed all HotPETs submissions and put together an excel-
lent program. Last but not least, we would like to thank all sponsors of PETS
2012, including Ministerio de Econom´ıa y Cooperaci´on (Spanish Ministry of
Economy), Gradiant (Galician Research and Development Center in Advanced
Telecommunications), and Ayuntamiento de Vigo (Vigo City Hall) for their gen-
erous support as well as Microsoft for its continued sponsorship.
May 2012
Simone Fischer-H¨ubner
Matthew Wright

Organization
Program Committee
Kevin Bauer
University of Waterloo, Canada
Thomas S. Benjamin
IBM Research Zurich, Switzerland
Jean Camp
Indiana University, USA
George Danezis
Microsoft Research, UK
Sabrina De Capitani Di
Vimercati
DTI - Universit`a degli Studi di Milano, Italy
Emiliano De Cristofaro
Palo Alto Research Center, USA
Roger Dingledine
The Tor Project, USA
Hannes Federrath
University of Hamburg, Germany
Julien Freudiger
EPFL, Switzerland
Simson Garﬁnkel
Naval Postgraduate School, USA
Rachel Greenstadt
Drexel University, USA
Nicholas Hopper
University of Minnesota, USA
Jean-Pierre Hubaux
EPFL, Switzerland
Renato Iannella
Semantic Identity, Australia
Aaron Johnson
Naval Research Laboratory, USA
Damon McCoy
George Mason University, USA
Aleecia Mcdonald
Carnegie Mellon, USA
Steven Murdoch
University of Cambridge, UK
Shishir Nagaraja
University of Birmingham, UK
Arvind Narayanan
Stanford University, USA
Gregory Neven
IBM Research Zurich, Switzerland
Siani Pearson
HP Labs, UK
Kazue Sako
NEC, Japan
Pierangela Samarati
Universit`a degli Studi di Milano, Italy
Michael Waidner
Fraunhofer SIT, Germany
Additional Reviewers
Acs, Gergely
Afroz, Sadia
AlSabah, Mashael
Anirban, Basu
Appelbaum, Jacob
Ayday, Erman
Basu, Anirban
Bilogrevic, Igor
Chothia, Tom
Clauß, Sebastian
Elahi, Tariq
Foresti, Sara
Fuchs, Karl-Peter
Furukawa, Jun
Garg, Vaibhav
Gerber, Christoph
Ghiglieri, Marco
Henry, Ryan
Herrmann, Dominik
Huguenin, Kevin
Humbert, Mathias
Kadianakis, George
Kalabis, Lukas
Kohlweiss, Markulf
Kreitz, Gunnar
Kuzu, Mehmet
Lindqvist, Janne

VIII
Organization
Papanikolaou, Nick
Patil, Sameer
Pham, Vinh
Shokri, Reza
Simo, Hervais
Soriente, Claudio
Stopczynski, Martin
Tancock, David
Teranishi, Isamu
Uzun, Ersin
Wang, Qiyan
Zhang, Nan

Table of Contents
Session 1: User Proﬁling
Betrayed by Your Ads! Reconstructing User Proﬁles from Targeted
Ads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Claude Castelluccia, Mohamed-Ali Kaafar, and Minh-Dung Tran
Private Client-Side Proﬁling with Random Forests and Hidden Markov
Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
George Danezis, Markulf Kohlweiss, Benjamin Livshits, and
Alfredo Rial
Session 2: Traﬃc Analysis
Understanding Statistical Disclosure: A Least Squares Approach . . . . . . .
38
Fernando P´erez-Gonz´alez and Carmela Troncoso
Website Detection Using Remote Traﬃc Analysis . . . . . . . . . . . . . . . . . . . .
58
Xun Gong, Nikita Borisov, Negar Kiyavash, and Nabil Schear
k-Indistinguishable Traﬃc Padding in Web Applications . . . . . . . . . . . . . .
79
Wen Ming Liu, Lingyu Wang, Kui Ren, Pengsu Cheng, and
Mourad Debbabi
Spying in the Dark: TCP and Tor Traﬃc Analysis . . . . . . . . . . . . . . . . . . .
100
Yossi Gilad and Amir Herzberg
Session 3: Applied Diﬀerential Privacy
Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy . . . . .
120
Dima Alhadidi, Noman Mohammed, Benjamin C.M. Fung, and
Mourad Debbabi
Diﬀerentially Private Continual Monitoring of Heavy Hitters from
Distributed Streams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
T.-H. Hubert Chan, Mingfei Li, Elaine Shi, and Wenchang Xu
Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data . . . .
160
Chengfang Fang and Ee-Chien Chang
Session 4: PETs for Cloud Services and Smart Grids
PRISM – Privacy-Preserving Search in MapReduce . . . . . . . . . . . . . . . . . .
180
Erik-Oliver Blass, Roberto Di Pietro, Reﬁk Molva, and Melek ¨Onen

X
Table of Contents
Practical Privacy Preserving Cloud Resource-Payment for Constrained
Clients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Martin Pirker, Daniel Slamanig, and Johannes Winter
Fault-Tolerant Privacy-Preserving Statistics . . . . . . . . . . . . . . . . . . . . . . . . .
221
Marek Jawurek and Florian Kerschbaum
Session 5: Privacy Services
Evading Censorship with Browser-Based Proxies . . . . . . . . . . . . . . . . . . . . .
239
David Fiﬁeld, Nate Hardison, Jonathan Ellithorpe, Emily Stark,
Dan Boneh, Roger Dingledine, and Phil Porras
Exploring the Ecosystem of Referrer-Anonymizing Services . . . . . . . . . . . .
259
Nick Nikiforakis, Steven Van Acker, Frank Piessens, and
Wouter Joosen
Session 6: User-Related Privacy Perspectives
Risk Communication Design: Video vs. Text. . . . . . . . . . . . . . . . . . . . . . . . .
279
Vaibhav Garg, L. Jean Camp, Katherine Connelly, and
Lesa Lorenzen-Huber
Use Fewer Instances of the Letter “i”: Toward Writing Style
Anonymization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
299
Andrew W.E. McDonald, Sadia Afroz, Aylin Caliskan,
Ariel Stolerman, and Rachel Greenstadt
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319

Betrayed by Your Ads!
Reconstructing User Proﬁles from Targeted Ads
Claude Castelluccia, Mohamed Ali Kaafar, and Minh-Dung Tran
Inria, France
{claude.castelluccia,mohamed-ali.kaafar,minh-dung.tran}@inria.fr
Abstract. In targeted (or behavioral) advertising, users’ behaviors are
tracked over time in order to customize served ads to their interests. This
creates serious privacy concerns since for the purpose of proﬁling, private
information is collected and centralized by a limited number of compa-
nies. Despite claims that this information is secure, there is a potential
for this information to be leaked through the customized services these
companies are oﬀering. In this paper, we show that targeted ads expose
users’ private data not only to ad providers but also to any entity that
has access to users’ ads. We propose a methodology to ﬁlter targeted ads
and infer users’ interests from them. We show that an adversary that
has access to only a small number of websites containing Google ads can
infer users’ interests with an accuracy of more than 79% (Precision) and
reconstruct as much as 58% of a Google Ads proﬁle in general (Recall).
This paper is, to our knowledge, the ﬁrst work that identiﬁes and quan-
tiﬁes information leakage through ads served in targeted advertising.
Keywords: Targeted Advertising, Privacy, Information leakage.
1
Introduction
Context. Internet users are being increasingly tracked and proﬁled. Companies
utilize proﬁling to provide customized, i.e. personalized services to their cus-
tomers, and hence increase revenues. In particular, behavioral advertising takes
advantage from proﬁles of users’ interests, characteristics (such as gender, age
and ethnicity) and purchasing activities. For example, advertising or publish-
ing companies use behavioral targeting to display advertisements that closely
reﬂect users’ interests (e.g. ‘sports enthusiasts’). Typically, these interests are
inferred from users’ web browsing activities, which in turn allows building of
users’ proﬁles.
It can be argued that customization resulting from proﬁling is also beneﬁcial
to users who receive useful information and relevant online ads in line with their
interests. However, behavioral targeting is often perceived as a threat to privacy
mainly because it heavily relies on users’ personal information, collected by only
a few companies. In this paper, we show that behavioral advertising poses an
additional privacy threat because targeted ads expose users’ private data to any
entity that has access to a small portion of these ads. More speciﬁcally, we show
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 1–17, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

2
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
that an adversary who has access to a user’s targeted ads can retrieve a large part
of his interest proﬁle. This constitutes a privacy breach because, as illustrated
in Section 2, interest proﬁles often contain private and sensitive information.
Motivation. This work was largely motivated by the Cory Doctorow’s
”Scroogled” short story that starts as follows [12]:
Greg landed at San Francisco International Airport at 8 p.m... The oﬃcer
stared at his screen, tapping...
-“Tell me about your hobbies. Are you into model rocketry?”
-“What?”
-“Model rocketry.”
-“No,” Greg said, “No, I’m not.”
-“You see, I ask because I see a heavy spike in ads for rocketry supplies showing
up alongside your search results and Google mail.”
-“You’re looking at my searches and e-mail?”
-“Sir, calm down, please. No, I am not looking at your searches,... That would
be unconstitutional. We see only the ads that show up when you read your mail
and do your searching. I have a brochure explaining it ...”
The main goal of this paper is to study whether such scenario would be possible
today, and if one can infer a user’s interests from his targeted ads. More specif-
ically, we aim at quantifying how much of a user’s interest proﬁle is exposed by
his targeted ads. However, as opposed to the above story, we do not consider ads
that show up when a user reads his email or uses a search engine. These ads are
often contextual, i.e. targeted to email contents or search queries. Instead, we
consider targeted ads that are served on websites when a user is browsing the
web.
Contributions of This Paper. We describe an attack that allows any entity
that has access to users’ targeted ads to infer these users’ interests recovering a
signiﬁcant part of their interest proﬁles. More speciﬁcally, our experiments with
the Google Display Network[4] demonstrate that by analyzing a small number of
targeted ads, an adversary can correctly infer users’ Google interest categories
with a high probability of 79% and retrieve as much as 58% of Google Ads
proﬁles.
The attack described in this paper is practical and easy to perform, since it
only requires the adversary to eavesdrop on a network for a short period of time
and collect a limited number of served ads.
The crux of the problem is that even if some websites use secure connections
such as SSL (Secure Socket Layer), ads are almost always served in clear. For
example, Google currently does not provide any option to serve ads with SSL1
[2]. We acknowledge that in some scenarios the adversary can recover a user’s
proﬁle directly from the websites he visits, i.e. without considering targeted
ads. However, we show in this paper that targeted ads can often improve the
accuracy of recovered proﬁles and reduce the recovery time. Furthermore, in
1 We
veriﬁed
this
feature
by
browsing
through
several
https
websites
(e.g.
https://www.nytimes.com/).

Betrayed by Your Ads!
3
some circumstances, the victim has diﬀerent browsing behaviors according to
his environment. For example, a user at work mostly visits websites related to
his professional activity, while he visits websites related to his personal interests
at home. We show in this paper that an adversary, such as an employer, that
can eavesdrop on the victim’s computer or network while at work can infer
information about his “private” and personal interest proﬁle. In other words,
targeted ads constitute a covert channel that can leak private information.
Although there are various targeted advertising networks today, this work
focuses on Google advertising system, which is “the most prevalent tracker”
according to a survey of The Wall Street Journal [17]. However, our methodology
is general enough to be extended to other ad networks. The problem of generality
will be discussed in Section 3.1.
Structure of the Paper. Section 2 describes the Google targeted advertising
system. In section 3, we present our approach to ﬁlter targeted ads and describe
how we infer Google Ads proﬁles from them. We then present in Section 4 the
performance of our method through some experiments in the Google Display
Network. In section 5, we discuss the related work. Section 6 presents possible
countermeasures and discusses some relevant problems. Section 7 concludes the
paper.
2
Targeted Advertising: The Case of Google
Google Display Network is a network of websites (also called publishers) that
serve Google ads. Google receives ads from advertisers and then selects the
appropriate publishers using various criteria such as relevant content, bid price
and revenue.
In the Google targeted advertising model, Google Display Network sites are
also used to track users as they browse the Internet. Each time a user visits
a website that contains Google ads, i.e. a website that belongs to the Google
Display Network, he sends his DoubleClick2 cookie to Google, along with infor-
mation about the visited website. As a result, Google collects all the sites within
the Google Display Network that have been visited by a user, and builds an
interest proﬁle from them. A Google proﬁle is deﬁned as a set of categories and
sub-categories (see ﬁgure 1). For example, if a user visits a football site several
times, Google may assign him the category Sport, or more speciﬁcally the sub-
category Sport →Football. In addition, a Google proﬁle may include location
information and some demographic data such as the gender and age of the user.
These proﬁles are then used to target ads to users.
A user can access and modify his Google Ads Preferences by accessing the
webpage http://www.google.com/ads/preferences [8]. Furthermore, a user
can choose to opt out of the Google targeted advertising if he no longer wants
to receive targeted ads. Figure 1 displays an example of a Google user proﬁle
2 In order to keep track of users visiting the Google Display Network, Google uses the
DoubleClick cookie issued from the doubleclick.net domain which belongs to Google.

4
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
Fig. 1. An Example of a Google Ads Preferences Page
that contains potentially private and sensitive information. For example, the
“Job listing” category indicates that the user is probably looking for a job. A
user might probably want to keep this information secret, in particular from
his current employer. Furthermore, the category “Dating & Personals” indicates
that the user is currently actively looking for a relationship, and the subcat-
egories “Baby names” and “Adoption” that he has been recently visiting web
sites related to baby adoption.
In its privacy policy, Google states that “We take appropriate security mea-
sures to protect against unauthorized access to or unauthorized alteration, dis-
closure or destruction of data.”, and “We restrict access to personal information
to Google employees, contractors and agents who need to know that informa-
tion in order to process it on our behalf” [5]. Nevertheless, in this paper we show
that a portion of personal users’ proﬁles could be leaked through targeted ads.
Even if Google does not consider users’ interests as “personal information”, this
data which is related to users online activities, can be very private from a user’s
perspective.
3
Inferring Users’ Proﬁles from Targeted Ads
As targeted ads are personalized to each user based on his proﬁle, they can
reveal a lot of information about users’ interests. This section describes how an
adversary who has access to an user’s ads can derive part of his interests from
them.
As shown in Figure 2, our approach is composed of two main phases. The
ﬁrst phase collects all ads served to a target user and ﬁlters them to only retain
targeted ones. In the second phase, targeted ads are classiﬁed into categories

Betrayed by Your Ads!
5
Ad Filtering
Proﬁle Reconstruction
Targeted Ads
Ads
User Interests
Web Page
Categorization
Use
Use
Fig. 2. Filtering targeted ads and inferring user interests
and a proﬁle is re-constructed. These two phases are detailed in the rest of this
section. However, we ﬁrst start by presenting two building blocks used by these
both phases. The ﬁrst one is an algorithm that is used to categorize webpages.
The second one is a set of algorithms to compare categories.
3.1
Building Blocks
Web Page Categorization. The web page categorization tool is an algorithm
that derives interest categories from a web page. This algorithm relies on the
tool used by Google to generate Google Ads Preferences pages. As described in
the previous section, each time a user visits a web page, Google derives some
interests in the form of categories and updates the user’s Google Ads Preferences
page accordingly.
We use this tool to build our page categorization algorithm. Given a webpage
W, our algorithm operates as follows:
1. W is visited and and the resulting DoubleClick cookie is saved.
2. A request is made to the Google Ads Preferences page with the previously
saved cookie. Note that Google does not always update the Google Ads
Preferences page upon a single web page visit. Usually, a webpage has to
be visited multiple times to update the Google Ads Preferences page. Fur-
thermore, we noticed that users’ Ads preferences are updated after a period
of time ranging between 1 and 2 minutes. Therefore, this step is repeated 5
times (heuristic value) every 2 minutes.
3. The Google Ads Preferences page is parsed and the corresponding categories
are retrieved.
To evaluate the performance of our approach, we scraped 5000 ads from Google
search page and 2000 sites from Google Display Network, classiﬁed them by
the tool, and reviewed the results. We detected that almost all of these pages
can be categorized by Google (more than 90%). We also manually reviewed
the categorization results and observed that, although there are some irrelevant

6
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
Table 1. Ad page categorization example
Ad Url
Categories
http://www.elasticsteel.net
ID=156
Beauty & Fitness →Fitness
http://www.livecarhire.com
Travel →Car Rental & Taxi Services
http://www.terracebeachresort.ca
Travel →Hotels & Accommodations
Travel →Tourist Destinations →Beaches
& Islands
http://www.sanibelbayfronthouse.com
Arts & Entertainment →Entertainment
Industry →Film & TV Industry →Film
& TV Production
Real Estate →Timeshares & Vacation
Properties
Travel →Tourist Destinations →Zoos-
Aquariums-Preserves
http://www.siestakeyaccommodation.com Real Estate →Timeshares & Vacation
Properties
Travel
categories, the categorization generally reﬂects the content of each page. Table
1 presents several examples of ad page categorization.
It should be noted that relying on Google does not reduce the generality of our
method. There exist many eﬃcient techniques to categorize the content of web
pages. For example [16] uses cosine similarity. This method is very eﬃcient since
it relies on social/crowd data (folksonomy) which is continuously updated, and
is appropriate for ﬁne-grained categorization. We implemented this method and
compared its performance with the Google-based categorization approach we
described above. The obtained results were quite similar, with more than 60% of
the categories overlapping. We therefore believe that our work can be extended
to other ad networks, such as Yahoo! or Microsoft, either by applying their own
categorization, or by simply using an existing webpages categorization technique.
Note that Yahoo! and Microsoft also build users’ behavior-based interest proﬁles
and similarly to Google personalize ads to users according to their interests
[9] [10].
Category Comparison Methods. Many of the ﬁltering and evaluation algo-
rithms presented in this paper need to compare categories. We use three methods
for this purpose: “Same category”, “Same parent” and “Same root”:
1. Same category: Two categories are considered equivalent in the “Same cat-
egory” method if they match exactly.
2. Same parent: Two categories are considered equivalent in the “Same par-
ent” method if they have the same parent category. For example, the two
categories “Arts & Entertainment →Entertainment Industry →Film & TV
Industry →Film & TV Awards” and “Arts & Entertainment →Entertain-
ment Industry →Film & TV Industry →Film & TV Production” have

Betrayed by Your Ads!
7
the same parent category “Film & TV Industry”, so they are considered
equivalent to each other in the “Same parent” method.
3. Same root: Two categories with same root category are considered equiva-
lent in the “Same root” method. For example, the two categories “Arts &
Entertainment →Entertainment Industry →Recording Industry →Mu-
sic Awards” and “Arts & Entertainment →Movies →Action & Adventure
Films →Superhero Films” have the same root category “Arts & Enter-
tainment” and therefore are equivalent to each other in the “Same root”
method. Obviously, if two categories are equivalent in the “Same parent”
method, they are also equivalent in the “Same root” method.
3.2
Extracting Targeted Ads
Ads provided by Google are either location-based, content-based (we call here-
after contextual, i.e. related to the visited page’s content), generic, or proﬁle-
based (we call hereafter targeted, i.e. customized to users’ proﬁles). In this paper,
we only consider targeted ads. We therefore need to ﬁlter out location-based,
content-based and generic ads (see ﬁgure 3).
Ads-
Filtering out
Location-based
Ads
-
Filtering out
Content-based
Ads
-
Filtering out
Generic Ads
Targeted
Ads
-
Fig. 3. Filtering targeted ads
We conducted all experiments with users from the same location. As a result,
the location-based ﬁlter is not used (and therefore not presented here). Further-
more, we consider that an ad is contextual if it shares at least one category with
its displaying page (the page on which the ad is delivered). To ﬁlter out contex-
tual ads, we therefore categorize, using the categorization technique described
in Section 3.1, each ad and their displaying page. If at least one category is in
common, the ad is classiﬁed as contextual. To ﬁlter generic (i.e. not customized)
ads, we create a number of non-overlapping user proﬁles (i.e. proﬁles without
any categories in common), and perform 10 requests to the tested pages3. Ads
that are served independently of the requesting proﬁle are then deemed generic
and ﬁltered out.
3.3
User-Proﬁle Reconstruction
Given the targeted ads from the previous step, there are possibly many ap-
proaches to infer user information. In our work, we aim at reconstructing the
3 The number of 10 requests is considered to be enough to get a suﬃcient ad collection
while resisting well to the ad churn [13].

8
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
Google-assigned interest categories which are presented as user proﬁles. In or-
der to reconstruct a user proﬁle, we categorize all of his targeted ads using our
Google-based web page categorization tool. The reconstructed proﬁle is then the
set of resulting Google categories.
For example, considering the ads provided in table 1, the reconstructed proﬁle
will look as follows:
Table 2. Proﬁle reconstruction example
Reconstructed proﬁle
Beauty & Fitness →Fitness
Travel
Travel →Car Rental & Taxi Services
Travel →Hotels & Accommodations
Travel →Tourist Destinations →Beaches & Islandsl
Arts & Entertainment →Entertainment Industry →Film & TV In-
dustry →Film & TV Production
Real Estate →Timeshares & Vacation Properties
4
Experimental Results
In this section, we evaluate the performance of our proﬁle reconstructing tech-
nique.
4.1
Experiment Setup
Figure 4 illustrates the setup of our experiments. Our experiments are composed
of two main phases:
Proﬁle Creation. In this phase, we create a set of proﬁles corresponding to
diﬀerent web users. Each of these proﬁles, that we call targeted proﬁles, Pt, is
obtained by visiting several websites from a user’s real web-history (i.e. list of
websites that the user has visited). We refer to these websites as training sites.
Each of them is visited 15 times to make sure it really aﬀects proﬁles. We then
retrieve the generated Google proﬁle from the Google Ads Preferences page (this
phase corresponds to the lower part of ﬁgure 4).
Proﬁle Re-construction. In this phase, we visit for each targeted proﬁle (Pt)
created as described above another set of websites, that we refer to hereafter
as visited websites. As opposed to the training sites, each visited site is only
visited once. The ads are then collected, ﬁltered and the proﬁle reconstructed as
described in Section 3. We refer to the set of proﬁles we obtain as reconstructed
proﬁles, Pr (this phase corresponds to the upper part of ﬁgure 4).

Betrayed by Your Ads!
9
(s1,s2,...,sY)
Pt
-
Ad
Collecting
Ads-
Ad
Filtering
Targeted
Ads
-
Proﬁle
Reconstruction
Reconstructed proﬁle
(Pr)
?
Comparison
%-
(w1,w2,...,wX)
-
Google
Internal
System
Google proﬁle (Pt)
-
-
w1,w2,...,wY: training websites;
s1,s2,...,sY: visited websites
Fig. 4. Filtering targeted ads and inferring user interests
4.2
Evaluation Methodology
Dataset. Our target web-history data comes from a set of 40 volunteers who
provided their list of websites they visited during two days. The ﬁrst X websites
in each proﬁle were used as the set of training sites to create Pt. The Y following
websites were used to build the reconstructed proﬁles, Pr, as shown in Figure 5.
In the presented experiments, X was set to 30 and diﬀerent values of Y were
used. The average number of root categories and categories in a targeted proﬁle
from X websites is displayed in Table 3.
Table 3. Proﬁle size statistics
# of root categories # of categories
X = 30
6.64
18.06
Pt
Pr
Browsing
Proﬁle Creation
(X training sites)
Proﬁle Reconstruction
(Y visited sites)
Fig. 5. Proﬁle creation and reconstruction
Performance Evaluation Metrics. To evaluate the results, we compare each
reconstructed proﬁle with the corresponding original one. We compare proﬁles
using the “same-category”, “same-parent” and “same-root” methodologies de-
scribed in Section 3.1. We evaluate the performance of our technique by computing
the average Precision, Recall and F-Measure values of all reconstructed proﬁles.
Precision is the fraction of rebuilt categories that are correct, while Recall is the

10
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
fraction of original categories that are correctly rebuilt. F-Measure is the harmonic
mean between Precision and Recall, deﬁned as: F = 2.precision.recall
precision+recall .
In other words, if we denote by Pr,c the categories of the reconstructed proﬁle
Pr that are correct, and Pr,i the categories of Pr that are incorrect, Precision =
|Pr,c|
|Pr| =
|Pr,c|
|Pr,c+Pr,i| and Recall = |Pr,c|
|Pt| .
Pt (Original proﬁle)
Pr (Reconstructed proﬁle)
Pr,c
Pr,i
Fig. 6. An illustration of Precision and Recall
Adversary Strategies. In order to evaluate the performance gain obtained by
using targeted ads as opposed to only using visited websites, we consider the
following three strategies:
- the adversary only uses visited websites (“Sites only”).
- the adversary only uses targeted ads (“Ads only”).
- the adversary uses visited websites and targeted ads (“Ads & Sites”)
Tested Scenarios. Finally, we consider two diﬀerent scenarios, corresponding
to two diﬀerent browsing behaviors:
1. HotSpot Scenario: This scenario corresponds to the case where the victim
is connecting to an open network and browses the Internet according to his
interests. In this scenario, the X training sites and the Y visited sites are
related to each others, i.e. generated from the same interest proﬁles. The
goal of this scenario is to show that targeted ads can be used to boost the
accuracy of proﬁle reconstruction.
2. Workplace Scenario: This scenario corresponds to the case where the victim
changes his browsing behavior during the reconstruction phase. In other
words, proﬁles used to generate the training sites and the visited sites are
signiﬁcantly diﬀerent. This situation happens, for example, when the victim
is moving from his home to his work environment. The goal of this scenario
is to study how much of the home proﬁle leaks from the targeted ads shown
at work.
In the following, we present, for the workplace scenario, how we select the vis-
ited websites so that they are largely separated from a user’s interests. We ﬁrst
randomly select a set of Google root categories, namely “Autos & Vehicles”,
“Law & Government”, “Books & Literature”, “Beauty & Fitness”, “Jobs & Ed-
ucation” and “Business & Industrial”. We then get for each of these categories

Betrayed by Your Ads!
11
500 websites using the Google Adwords Placement Tool [3]. This tool aims at
helping advertisers to select websites to publish their ads. We then get for each
user all of his root categories, and select a root category C that does not be-
long to them. The user’s visited sites are then randomly selected from the 500
websites corresponding to category C. For example, if a proﬁle contains 4 root
categories: “Law & Government”, “Books & Literature”, “Beauty & Fitness”,
“Jobs & Education”, then one of the remaining categories, “Autos & Vehicles”
or “Business & Industrial”, will be chosen for visited websites. We veriﬁed that
none of our test proﬁles contains all the six visited categories.
Note that a website classiﬁed in a Google category according to Google Ad-
words Placement Tool may result in another category in Google Ads Preferences.
For instance, Google may assign a website W to category “Arts & Entertain-
ment”. However, when categorizing this website using Google Ads Preferences,
the result may include, in addition to “Arts & Entertainment”, another root
category, say “Books & Literature”. Therefore, we cannot completely guarantee
that the visited websites are totally separated from the training ones.
4.3
Result Analysis
Tables 4, 5, 6 and 7 represent the achieved Precision, Recall and F-Measure
values in percentage with (X = 30, Y = 10) and (X = 30, Y = 15) for the
hotspot and workplace scenarios respectively. The rows in these tables specify the
category comparison methods used to ﬁlter out contextual ads4. This comparison
method is also used to evaluate the results (i.e. to compare the reconstructed
proﬁles with the original ones)5. We remind the reader that these comparison
methods are described in Section 3.1. The columns of the table specify the three
diﬀerent cases of proﬁle reconstruction, using “Sites only”, “Ads only” and “Ads
& Sites”, respectively. The tables show that the Ads-based information leakage is
signiﬁcantly high, with precision values ranging from 73 to 82% for reconstructed
proﬁles evaluation based on recovering the root of categories solely from Ads. For
example, in case (X = 30, Y = 15) in the workplace scenario, with “Ads only”
and the “Same root” comparison method (used for both ﬁltering and evaluation
processes), we achieve Precision, Recall and F-Measure of more than 79%, 58%
and 67% respectively (Table 7). The average number of targeted ads we observed
accounts for approximately 30% of all collected ads in each case. We note that the
results of the row “Same Category” show in general a relatively lower precision
and recall values than the results of the “Same Parent” and “Same Root” rows.
Figures 7 and 8 display the variation of Precision, Recall and F-Measure
when varying the number Y of visited web sites for each targeted proﬁle, for
diﬀerent comparison methods. We observe that, for a given proﬁle (i.e. when X
and therefore |Pt| are ﬁxed), the recall increases noticeably with Y , the number
4 For example, the row ”same parent” displays results when ads are considered con-
textual if they share the same parent categories with the pages that display them.
5 For example, the column “same parent” means that two categories are deemed iden-
tical if they share the same parent.

12
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
Table 4. Reconstructing Google proﬁles performance in Hotspot scenario (X = 30
and Y = 10)
Av.# of
Sites only
Ads only
Ads & Sites
targ. ads Prec./Recall Prec./Recall Prec./Recall
/F
/F
/F
Same
14.29
19.66/7.6
18.04/7.06
18.3/14
cat.
/10.96
/10.15
/15.86
Same
10.94
58.25/29
53.67/19.38
55.98/42.29
parent
/38.72
/28.48
/48.18
Same
9.24
79.26/51.44
73.08/30.06
79.6/68.33
root
/62.39
/42.6
/73.54
Table 5. Reconstructing Google proﬁles performance in Hotspot scenario (X = 30
and Y = 15)
Av.# of
Sites only
Ads only
Ads & Sites
targ. ads Prec./Recall Prec./Recall Prec./Recall
/F
/F
/F
Same
21.53
19.67/10.28
15.71/8.47
17.07/17.66
cat.
/13.50
/11.01
/17.36
Same
16.67
54.46/34.44
51.26/23.54
52.73/50.16
parent
/42.2
/32.26
/51.41
Same
14.4
75.57/61.13
82.24/40.3
78.5/80.52
root
/67.59
/54.09
/79.5
Table 6. Reconstructing Google proﬁles performance in Workplace scenario (X = 30
and Y = 10)
Av.# of
Sites only
Ads only
Ads & Sites
targ. ads Prec./Recall Prec./Recall Prec./Recall
/F
/F
/F
Same
19.23
2.92/1.05
14.73/10.28
11.84/10.94
cat.
/1.54
/12.11
/11.37
Same
13.6
9.09/3.99
46.31/30.31
34.39/31.56
parent
/5.55
/36.64
/32.91
Same
11.2
12.65/6.43
78.07/53.96
56.49/55.94
root
/8.53
/63.81
/56.21
Table 7. Reconstructing Google proﬁles performance in Workplace scenario (X = 30
and Y = 15)
Av.# of
Sites only
Ads only
Ads & Sites
targ. ads Prec./Recall Prec./Recall Prec./Recall
/F
/F
/F
Same
28.11
2.99/1.31
13.44/11.95
10.89/12.62
cat.
/1.82
/12.65
/11.69
Same
20.3
9.13/5.06
44.95/33.8
32.75/35.45
parent
/6.51
/38.59
/34.05
Same
17.13
14/8.61
79.37/58.12
55.85/60.1
root
/10.66
/67.10
/57.9
of visited web sites, while the precision is steady. This shows that the number
of correctly reconstructed categories, i.e. |Pr,c|, increases with Y . This result is
expected since when Y increases the number of collected ads also increases and
as such the amount of available information is higher. However for a given X,
the precision is not notably aﬀected by Y , which means that the number of
incorrectly reconstructed categories, i.e. |Pr,i|, also increases with Y .

Betrayed by Your Ads!
13










	


















	






 !
" !
"#









	




















	

	











	



	

	












	

	














		
	

	


	









 



	




$
$%









	



 !
" !
"#
Fig. 7. Precision, Recall and F-Measure with the “Same category”, “Same parent”
and “Same Root” comparison methods (from left to right respectively) used in both
ﬁltering and evaluation processes (In hotspot scenario with X = 30)










	




















	





















	






















	

	













	



	














	



	
















		
	

	


	











	



 !
" !
"#









	



 !
" !
"#
Fig. 8. Precision, Recall and F-Measure with the “Same category”, “Same parent”
and “Same Root” comparison methods (from left to right respectively) used in both
ﬁltering and evaluation processes (In workplace scenario with X = 30)

14
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
In the hotspot scenario, the visited websites are largely relevant to the training
websites, therefore reconstructing proﬁles from “Sites only” achieves the results
as good as, if not better than, the results obtained from “Ads only” (see ﬁgure
7). However, when we combine both sites and ads in the reconstruction process,
we get nearly the same Precision, while increasing Recalls remarkably (almost
the sum of the two cases “Sites only” and “Ads only”). In this scenario, Ads are
very useful to boost the performance since they allow the recovery of a larger
part of the targeted proﬁles.
In the workplace scenario, the visited websites are considerably separated from
training websites. Therefore, reconstructing proﬁles from “Sites only” leads to
very poor results, whereas the “Ads only” technique achieves signiﬁcantly better
results (see ﬁgure 8). By combining sites and ads, we slightly increase the Recall
while reducing the Precision. In this scenario, we observe that ads do indeed
constitute a “hidden” channel that leaks private information about users.
While the performance of our method when evaluating the recovery of “root”
and “parent” categories is notably high, we acknowledge that it can only recover
a small proportion of a user’s actual categories (Precision varies between 10 and
18% when using the same category method for evaluation, and Recall ranges
from 10 to 17%). We believe there are several explanations for this result. First,
Google might not be using the user’s actual categories to deliver ads, and instead
might use the root or parent categories for a broader coverage of users’ interests.
Furthermore, our ads classiﬁcation method is probably not optimal and could
be improved in many ways. In particular, we took a simple and conservative
approach in the ﬁltering step by ignoring location-based ads. In addition, we
did not consider remarketing ads that, as discussed in Section 6, may contain
additional information about the users recent online activities.
However, even only 10 to 15% of an user’s interest categories can constitute a
severe privacy breach. To illustrate this statement, we ran our technique on the
proﬁle shown in Figure 1, and using targeted ads only, we recovered the proﬁle
shown in Figure 9. Among the recovered categories, the category “Online Com-
munities →Dating & Personals” may constitute a private piece of information
which a user might not be willing to share.
Fig. 9. Reconstructed Proﬁle

Betrayed by Your Ads!
15
5
Related Work
To the best of our knowledge, this work is the ﬁrst work to quantify the private
information leakage from targeted ads content. In the following, we present the
most relevant work to our paper:
Privacy-Preserving Advertising Systems. Some initial eﬀorts have been
put in designing targeted advertising models yet preventing users from being
tracked by ad networks. Among them are Privad [14] and Adnostic [16]. Their
main idea is to keep behavioral information at the client side and then to perform
the ad selection process locally. The proposed models provide a stronger protec-
tion for user privacy than current systems do, but their feasibility is in turn still
open to debate. Our work considers a diﬀerent adversary model. While these
schemes try to prevent ad networks from tracking users, we assume that the ad
network is trusted and aim at protecting users privacy from eavesdroppers.
Privacy Violations Using Microtargeted Ads. Korolova has recently pre-
sented attacks that could be used by advertisers to obtain private user infor-
mation on Facebook [15]. The author showed that an advertiser can manipulate
its served ads in order to learn information about users’ proﬁles. This work is
complementary to ours, since it considers a diﬀerent adversary model.
Retrieving User’s Proﬁle. [11] presented an attack to infer user search history
from Google Web search suggestions. While the webhistory webpage is protected
by SSL and Google account authentication, the authors showed that a large
part of a user’s search history can be reconstructed by a simple session hijacking
attack.
6
Discussion
Countermeasures. In order to protect against this information leakage, the
easiest solution today is to simply opt out of targeted advertising, frequently
delete cookies or use ad-blocking software. Initiatives such as NAI (Network Ad-
vertising Initiative) [6], DNT (Do Not Track) [1] or TPLs (Tracking Protection
Lists) [7] that aim to provide users with tools to restrict tracking and/or be-
havioral advertising could also mitigate the identiﬁed privacy threat. However,
these solutions often prevent to target ads or even to serve ads to users.
There exist several possible countermeasures that could be used to target ads
to users and mitigate the information leakage identiﬁed in this paper. In particu-
lar, there are ongoing eﬀorts to design and deploy privacy-preserving ad systems
(e.g. Privad [14] and Adnostic [16]) whose main principle is to select ads locally.
These solutions make the eavesdropping and ﬁltering of targeted ads, and there-
fore our inferring attack, much more diﬃcult. Another possible solution would
be to send all ad requests and responses (containing DoubleClick cookies and
ads content) over secure channels (SSL). However, we believe that this solution

16
C. Castelluccia, M.A. Kaafar, and M.-D. Tran
needs deeper analysis from the research community and the advertising industry
since it might be too costly, not practical or altogether hard to deploy.
Stealing Ads Preferences via an Active Attack. The attack presented in
this paper is passive, i.e. completely transparent to the victim and to the ads
providers. We note that a user’s preferences can also be stolen by a simple active
attack. If fact, if an adversary is able to steal the victim’s DoubleClick cookie, it
can connect to his Google Ads preference page and retrieve his preferences. We
examined the top 100 commercial websites from Alexa and found that at least
71% of them exchange DoubleClick cookie in clear with remote servers. Stealing
a Double Click cookie is then quite easy. We implemented and tested this cookie
hijacking attack, and were always able to retrieve the victim’s Ads preferences
page with a simple request to Google Ads servers. This attack is simple, however
as opposed to our scheme, it is active and intrusive.
Remarketing Ads. This paper did not consider “remarketing ads”, which ad-
vertise the services or products of a site that a user has visited. Consider a user
who is looking for a hotel in Vigo, Spain and performs some searches on the
site www.hotels.com. It is very likely that he will consequently receive frequent
ads advertising hotels in Vigo while browsing the Internet. Remarketing ads are
not only targeting a particular user’s interests, but speciﬁcally aim to match an
exact intention or previous online action. Remarketing ads actually leak much
more information about the user. In fact, in our example, they will not only
leak that the user is interested in travelling, but also his actual destination i.e.
Vigo, Spain. Note that remarketing ads are served independently of Google Ads
Preferences proﬁles. A user will receive remarketing ads even if he empties his
ads preferences proﬁle. The only way to stop receiving remarketing ads is to
clear his cookies or to completely opt out of targeted ads.
7
Conclusion
In this paper, we showed that targeted ads contain valuable information that
allows accurate reconstruction of users’ interest proﬁles. We presented a method-
ology to categorize and ﬁlter targeted ads, which are in turn used to infer users’
proﬁles. Based on both real users’ web histories and synthetic users’ proﬁles,
we showed that our technique achieves a high accuracy in predicting general
topics of users’ interests. Additionally, using only a limited number of collected
targeted ads we demonstrated that an adversary can capture on average more
than half of targeted proﬁles. The algorithms described in this paper are simple
and probably not optimal. We believe they could be improved in many ways.
Many people claim that the main issue in online behavioral advertising is not
related to ads personalization itself, which allows users to receive useful ads, but
rather to the fact that it requires users’ activities tracking. In this paper, we
show that ads personalization can also be harmful to users’ privacy and does
actually leak sensitive information such as users’ proﬁles. We also note that this
information leakage is not speciﬁc to online behavioral advertising, but in fact

Betrayed by Your Ads!
17
exists in any personalized content (news, searches, recommendations, etc.). As
the web is moving toward services personalization almost everywhere, special
attention should be paid to these privacy threats. This paper contributes in the
understanding of possible privacy risks of content personalization.
Acknowledgments.
The authors are grateful to numerous colleagues for
thought-provoking discussions on an earlier version of this paper, and to the
anonymous reviewers for their valuable comments.
References
1. Do Not Track (2011), http://donottrack.us/
2. Google AdSense Help (2011),
https://www.google.com/adsense/support/bin/
answer.py?hl=en&answer=10528
3. Google Adwords Placement Tool (2011),
http://adwords.google.com/support/aw/bin/
answer.py?hl=en&answer=179238/
4. Google Display Network (2011),
http://www.google.com/ads/displaynetwork/
5. Google Privacy Policy (2011),
http://www.google.com/intl/en/policies/privacy/
6. Network Advertising Initiative (2011),
http://www.networkadvertising.org/
7. Tracking Protection Lists (2011),
http://www.privacyonline.org.uk/
8. Google Ads Preferences (2012),
http://www.google.com/ads/preferences/
9. Personalized Advertising from Microsoft (2012),
http://choice.live.com/AdvertisementChoice/Default.aspx
10. Yahoo! Ad Interest Manager (2012),
http://info.yahoo.com/privacy/us/yahoo/opt_out/targeting/
11. Castelluccia, C., De Cristofaro, E., Perito, D.: Private Information Disclosure from
Web Searches. In: Atallah, M.J., Hopper, N.J. (eds.) PETS 2010. LNCS, vol. 6205,
pp. 38–55. Springer, Heidelberg (2010)
12. Doctorow, C.: Scroogled (2007),
http://blogoscoped.com/archive/2007-09-17-n72.html
13. Guha, S., Cheng, B., Francis, P.: Challenges in measuring online advertising sys-
tems. In: Internet Measurement (2010)
14. Guha, S., Cheng, B., Francis, P.: Privad: Practical privacy in online advertising.
In: NSDI (2011)
15. Korolova, A.: Privacy violations using microtargeted ads: A case study. In: ICDM
Workshops (2010)
16. Toubiana, V., Narayanan, A., Boneh, D., Nissenbaum, H., Barocas, S.: Adnostic:
Privacy preserving targeted advertising. In: NDSS (2010)
17. Valentino-Devries, J.: What they know about you. The Wall Street Journal (July
31, 2010)

Private Client-Side Proﬁling with Random
Forests and Hidden Markov Models
George Danezis1, Markulf Kohlweiss1, Benjamin Livshits1, and Alfredo Rial2
1 Microsoft Research
{gdane,markulf,livshits}@microsoft.com
2 IBBT and KU Leuven, ESAT-COSIC, Belgium
alfredo.rial@esat.kuleuven.be
Abstract. Nowadays, service providers gather ﬁne-grained data about
users to deliver personalized services, for example, through the use of
third-party cookies or social network proﬁles. This poses a threat both
to privacy, since the amount of information obtained is excessive for
the purpose of customization, and authenticity, because those methods
employed to gather data can be blocked and fooled.
In this paper we propose privacy-preserving proﬁling techniques, in
which users perform the proﬁling task locally, reveal to service providers
the result and prove its correctness. We address how our approach applies
to tasks of both classiﬁcation and pattern recognition. For the former, we
describe client-side proﬁling based on random forests, where users, based
on certiﬁed input data representing their activity, resolve a random forest
and reveal the classiﬁcation result to service providers. For the latter, we
show how to match a stream of user activity to a regular expression,
or how to assign it a probability using a hidden Markov model. Our
techniques, based on the use of zero-knowledge proofs, can be composed
with other protocols as part of the certiﬁcation of a larger computation.
1
Introduction
Many popular business models rely on proﬁling users to deliver customised ser-
vices. Currently, privacy-sensitive ﬁne-grained data about each user’s activities
is gathered on the server side to perform this personalization. As an alterna-
tive strategy, we propose a set of techniques that allow for user proﬁling to be
performed in a privacy-preserving manner.
By way of motivation, consider a retailer of home heating equipment that
wishes to tailor prices or rebates according to a household’s occupancy patterns.
Similarly, a soft drink manufacturer may wish to provide free drink samples to
loyal customers of its competitors. More generally, a marketing company may
wish to classify customers according to their lifestyle to better target reduc-
tion coupons. A news site may adjust its content according to user preferences
on a social network. These are all examples of personalization, a strategy that
necessitates knowing more about the user, or user proﬁling.
In recent years, several mechanisms have emerged for discerning user prefer-
ences on a large scale. The most notable of them is third-party advertising, which
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 18–37, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Private Client-Side Proﬁling
19
involves partially observing user browsing history through third-party cookies
to understand user preferences. Similarly, social network platforms allow third-
party apps to query user proﬁles and “likes” for personalization purposes. If this
information were only to be used for the purposes of personalization, in both
settings the amount of detailed information relating to the user would seem to
be excessive. To summarize, the current approach has the following drawbacks:
– Privacy: service providers not only learn the proﬁles of customers, which
might in themselves be privacy-sensitive, but also acquire a mass of ﬁne-
grained data about users, including click streams, previous purchases, social
contacts, etc.
– Authenticity: the correctness of proﬁling is doubtful as the data has been
gathered in an ad-hoc manner, often through third party networks or web
“bugs” or beacons, which can be blocked and fooled. Moreover, a determined
user may interfere with cookie-based proﬁle gathering by geting involved in
so-called cookie swaps — rings of people exchanging tracking cookies to
confuse the trackers.
In this work, we propose techniques that perform proﬁling on the client side using
state-of-the-art machine learning procedures, namely random forests [11]. Clients
can conclusively prove that they have performed the classiﬁcation task correctly,
with reference to certiﬁed data representing user activity. These certiﬁcates serve
as a root of trust: a service provider can unreservedly rely on the computed proﬁle
for business decisions, such as giving free samples, coupons or rebates, without
learning anything more than the decision or fearing that the user is cheating.
In addition to resolving random forests, we show how to match a stream
of user activity against a regular expression. If desired, we can also assign a
probability to a stream of user activity using a hidden Markov model [4]. This
has applications in matching against streams of user activity, such as ﬁnding
patterns in bank transactions, for example. The same guarantees relating to
privacy and authenticity hold in both of these cases.
Previous private data mining techniques are too speciﬁc to the problem they
solve and can only be used in isolation [44,36]. In contrast, the techniques pre-
sented in this paper are fully composable with other zero-knowledge protocols.
As a result, they may be used as part of certifying a larger computation.
2
System Overview
At a high level, we present cryptographic mechanisms that allow a user to prove
some certiﬁed features match a class or a sequence, deﬁned by a random forest,
regular expression, or a hidden Markov model, respectively.
Our protocol takes place between a prover, a veriﬁer, and two authorities: A
and A′. The basic interactions between those entities are illustrated in Figure 1
and described in detail below. We assume that there is an implicit setup phase,
during which all principals in the system generate their public/private key pairs
and distribute them securely to all other entities. The prover and the veriﬁer
also generate signature and veriﬁcation keys for purposes of authentication.

20
G. Danezis et al.
Prover
Verifier
Authority or 
device A
Authority A’



Authority 
certifies 
features
Authority A’ provides 
encoded model
Prover convinces verifier that features 
match a profile
Fig. 1. An overview of the key interactions between all parties in the system
For the purposes of the protocol, a prover must be provided with a set of
certiﬁed features (Figure 1, step 1). Abstractly, these features are signed by an
authority A that is trusted by the veriﬁer to authoritatively associate features
with the prover. In practice, this authority could be a third party service, such
as a trusted social network platform that observes user actions and provides
a signed account of them for third parties to use. Or it could be a trusted
hardware device, such as a smart meter that is installed on user premises to
provide certiﬁed readings. Similarly an automotive on-board unit could provide
trusted readings, such as time and vehicle speed, that need to be matched or
classiﬁed for the purposes of billing for pay-per-mile insurance. Finally, the user
could self-certify some features, and even keep them secret, and simply attest
them with a third party using a blind signing protocol.. The features need to be
encoded in a speciﬁc manner as described in Section 5 and Section 6.
A separate entity A′, that could be the veriﬁer, cryptographically encodes a
decision forest or a hidden Markov model and provides it to the prover (Figure 1,
step 2). In Section 5 and Section 6 we describe how to encode random forest and
hidden Markov model classiﬁers to support eﬃcient proof of their operation. In
theory, the classiﬁers themselves could be kept secret from the veriﬁer, since
the veriﬁcation does not require them. This may enable some business models,
but is fragile due to the fact that the prover has to know the full details of the
classiﬁers to construct the necessary proofs.
Finally, the prover and veriﬁer interact, in such a way that the veriﬁer is
provided with the outcome of the random forest classiﬁcation or hidden Markov
model match probability, along with a proof that it was performed correctly
(Figure 1, step 3). No other information about the features is revealed. The
subsequent sections of this paper present the details of these proofs. Standard
techniques can be used to turn the interactive protocol into a non-interactive
one. In that setting, the prover simply constructs the proof and sends it to the
veriﬁer that can either accept or reject it.

Private Client-Side Proﬁling
21
Our protocols protect privacy in the sense that they leak no more information
about the detailed features than what becomes known through the decision of a
classiﬁer or a match with a hidden Markov chain. We note that such classiﬁers
could be build to leak speciﬁc features, and therefore a user needs to know what
being subjected to such proﬁling entails. Since the classiﬁer is public it can be
executed, or not, according to the privacy preference of the prover.
The interactions between the authority certifying the features, the prover and
the veriﬁer are reminiscent of anonymous credential protocols. The key diﬀerence
of our approach from such protocols is that we do not attempt to hide the identity
of the prover, but rely on hiding the detailed user features to preserve privacy.
The protocols can be extended with relative ease to protect the identity of the
user but this goes beyond the scope of this work.
3
Applications
This section outlines some of the applications of the theory presented in this
paper. The application focus for this paper is on end-user settings, many of
which pertain to the context of everyday life, rather than enterprise or business-
to-business scenarios.
Behavioural Advertising. Perhaps, the most common application setting on
people’s minds today is that of behavioural proﬁling – web users are classiﬁed
based on their pattern of visiting web pages, according to either a taxonomy or by
associating keywords with their proﬁle, typically, for more accurate ad targeting.
Our techniques allow the user to run the classiﬁcation algorithm locally using
ﬁne-grained behavioural information — usually, on their URL stream — and
only reveal their aggregate proﬁle to an advertiser or web site for the purposes
of customization or ad targeting. Our schemes could make use of features certiﬁed
by social networking sites, or even certiﬁed cookies gathered by browsing on the
internet. As has been proposed in the literature [40,30,43,26], in this case, a set
of ads can be matched against the user proﬁle without revealing precise user web
browsing history.
P2P Dating and Matchmaking. A common functionality of on-line social
sites is the ability to ﬁlter users based on one or more criteria. More speciﬁcally,
imagine a P2P dating site. The site is decentralized, i.e. P2P in order to avoid
sharing rather personal details with the site owner. The process of matchmaking,
i.e. ﬁltering of the users with respect to criteria such as music tastes or hair color,
would be done locally, on each of the P2P nodes. Of course, we need to make
sure that this local ﬁlter is not compromised to always cheerfully, but deceitfully
answer “yes, I am a match.” We can prevent this by requiring users to commit to
their preferences and attributes at most once, and then use the same, committed,
proﬁle when answering matching queries from diﬀerent parties.
In this scenario, trust is rooted in the fact that users cannot have their at-
tributes certiﬁed at will, and can only do so in a restricted manner: either through

22
G. Danezis et al.
paying some money to get their subscription to the dating site to establish an
identity, or any other Sybil defence mechanism [35].
Financial Logs. In banking applications, it is common to look for patterns over
user transaction data. One common application is fraud detection. For example,
two or more bank charges in a stream of account transactions per month or
too many charges by a particular merchant may be indicative of fraud. Banks
themselves are often interested in ﬂagging certain kinds of transactions, primarily
for fraud prevention reasons. Governments are keen to oversee transactions to
avoid tax-evasion or money laundering.
A more speciﬁc application using our techniques is a lender who is interested in
querying your transaction history for determining if you are a reliable borrower,
which might include checking for more than two late fees within a month. Our
techniques could use transaction ﬁnancial logs provided by ﬁnancial institutions
or shops to prove certain properties to third parties, such as a lending institution.
Insurance. Metered automotive insurance is an example of an application
where revealing too much about the user’s driving habits can be privacy-
compromising, as it will leak excessive data about their whereabouts. However,
certain behavioural patterns on the part of the user can be utilized to ﬂag danger-
ous or reckless behaviour, which are legitimate to inquire about for the purpose
of providing and charging for automotive insurance.
For instance, a driver that alternates prolonged day and night driving within
the same 24- or 48-hour period could be at risk for an accident. Someone who
starts the car more than, say, ten times a day does a fair bit of city driving. Such
patterns may be mined to determine the right insurance premium. To support
such settings, a trusted on-board unit in the car can provide certiﬁed readings of
speed and time that are mined to detect the presence of certain driving patterns,
without revealing the rest.
Bio-medical and Genetic. A number of services have been proposed on the
basis of individual genetic or biological proﬁles. These can include claiming a
state beneﬁt due to some debilitating genetic condition or even testing a genetic
ﬁngerprint as part of a criminal investigation. Our techniques could be used
to support privacy in such settings: an authority could provide a user with a
certiﬁed generic proﬁle as detailed as their DNA proﬁle. Third parties can then
provide certiﬁed models that match speciﬁc genes or other biological proﬁles to
tune their services. The results computed by these third parties can be shared,
yet the detailed biological proﬁle will never be revealed.
3.1
Related Work
Random forests are a very popular classiﬁcation algorithm ﬁrst proposed by
Breiman [11]. Their performance, eﬃciency and potential for parallelization

Private Client-Side Proﬁling
23
has made random forests popular for image classiﬁcation [8], and they have
been widely used in bio-informatics [24]. Hidden Markov models [31] have found
applications in speech recognition [39] as well as bio-informatics [32]. The fact
that both techniques have serious medical applications motivates us further to
perform classiﬁcation and matching tasks without revealing raw data.
Previous works have considered privacy in the context of classiﬁcation tasks.
Pinkas [38] argues that arbitrary multi-party computation can be used to jointly
perform privacy-preserving data mining when diﬀerent parties have secrets. The
key building block of the presented protocol is oblivious transfer, which requires
interactions among parties. Our techniques diﬀer in that the only party with se-
crets is the user, and all other parties only care about the integrity and authentic-
ity of the classiﬁcation procedure. As a result, our protocols are not interactive,
and are more eﬃcient than generic secure multi-party computations.
Vaideep et al. [44] consider privacy friendly training of decision trees and
forests. These techniques are orthogonal and complementary to our algorithms.
We provide a zero-knowledge proof that a trained random forest classiﬁer was
applied on a secret data set correctly, and do not discuss the privacy of the train-
ing phase. Similarly, Magkos et al. [36] use protocols based on secure electronic
election to implement a privacy friendly-frequency counter. They discuss how
multiple rounds of their protocol can be used to privately train a random forest
classiﬁer.
Our cryptographic constructions build upon a number of previous crypto-
graphic techniques, mostly involving zero-knowledge proofs, that we describe in
detail in the next section.
4
Cryptographic Foundations
The novel techniques for resolving random forests and matching regular lan-
guages make use of modern cryptographic primitives, and in particular eﬃcient
zero-knowledge proofs of knowledge. We present here the basic building blocks,
namely commitments, zero-knowledge proofs and signatures with eﬃcient proof
protocols (P-signatures). We also dwell deeper into how to use P-signatures to
perform lookups into a public indexed table without revealing the lookup key or
row retrieved.
4.1
Zero-Knowledge, Commitments and P-Signatures
Zero-Knowledge proofs. A zero-knowledge proof of knowledge [6] is a two-party
protocol between a prover and a veriﬁer. The prover demonstrates to the veriﬁer
her knowledge of some secret input (witness) that fulﬁlls some statement without
disclosing this input to the veriﬁer. The protocol should fulﬁll two properties.
First, it should be a proof of knowledge, i.e., a prover without knowledge of the
secret input convinces the veriﬁer with negligible probability. Second, it should
be zero-knowledge, i.e., the veriﬁer learns nothing but the truth of the statement.

24
G. Danezis et al.
We make use of classical results for eﬃciently proving knowledge of discrete
logarithm relations [42,21,19,12,10,22]. To avoid common pitfalls, and to be com-
patible with the newest results in the literature, we use the language proposed
by Camenisch et al. [15]. Their language is inspired by the CKY-language [14],
which formalized and reﬁned the PK notation for proofs of knowledge by Ca-
menisch and Stadler [20]. While proofs of knowledge for the standard model are
addressed in [14], Camenisch et al. [15] show how to realize proofs for their lan-
guage in the UC-framework. As a third option, these proofs can be compiled [25]
into non-interactive proofs in the random oracle model [7].
In the notation of [15], a protocol proving knowledge of integers w1, . . . , wn
satisfying the predicate φ(w1, . . . , wn) is described as
K
w1 ∈I1, . . . , wn ∈In : φ(w1, . . . , wn)
(1)
Here, we use the symbol “
K
” instead of “∃” to indicate that we are proving
“knowledge” of a witness, rather than just its existence.
The predicate φ(w1, . . . , wn) is built up from “atoms” using arbitrary combi-
nations of ANDs and ORs. An atom expresses group relations, such as k
j=1 gFj
j
=
1, where the gj are elements of an abelian group and the Fj’s are integer poly-
nomials in the variables w1, . . . , wn.
Instead of using group relations directly, we rely on classical results that show
how to reduce the following proof components to group relations: (1) linear
relations and equality, (2) inequalities, and (3) proofs about commitments and
signatures.
For the prime-order and the hidden-order setting there are diﬀerent techniques
for dealing with inequalities and P-signature possession. We refer to the literature
for more details. We consciously keep our presentation independent of the lower-
level cryptography by using an abstract notation for zero-knowledge statements.
Inequalities. A couple of techniques are available to prove that a value is positive.
Groth [29], who builds on [9], relies on the fact that a number can be expressed
using the sum of 3 squares. Alternatively, the value can be shown to be within the
set of positive integers as provided by a trusted authority, using set membership
techniques by Camenisch et al. [13].
Commitment schemes. A non-interactive commitment scheme consists of the
algorithms ComSetup, Commit and Open. ComSetup(1k) generates the param-
eters of the commitment scheme parc. Commit(parc, x) outputs a commitment
Cx to x and auxiliary information openx. A commitment is opened by revealing
(x, openx) and checking whether Open(parc, Cx, x, openx) outputs accept. The
hiding property ensures that a commitment Cx to x does not reveal any in-
formation about x, whereas the binding property ensures that Cx cannot be
opened to another value x′. Our scheme requires commitment schemes that sup-
port an eﬃcient proof predicates POpenCx(x, openx) for the opening algorithm
of commitment Cx [37,27].

Private Client-Side Proﬁling
25
P-signatures. A signature scheme consists of algorithms (Keygen, Sign, Verify).
Keygen(1k) outputs a key pair (sk, pk). Sign(sk, m) outputs a signature s on
message m. Verify(pk, s, m) outputs accept if s is a valid signature on m and
reject otherwise. This deﬁnition can be extended to support multi-block messages
m = {m1, . . . , mn}. Existential unforgeability [28] requires that no probabilistic
polynomial time (p.p.t.) adversary should be able to output a message-signature
pair (s, m) unless he has previously obtained a signature on m.
One important proof component that we use in many of our constructions is
a predicate PVerifypk(s, m1, . . . mn) for the veriﬁcation algorithm of a signature
scheme. This allows to prove possession of a signature s, while keeping the sig-
nature itself and parts of the signed message secret. Signature schemes for which
veriﬁcation is eﬃciently provable are thus very attractive for protocol design,
and are variously referred to as CL-signatures or P-signatures, i.e., signatures
with eﬃcient protocols [17,18,5].
Extended notation for zero-knowledge statements. We extend the notation above
to abstract the details of proofs of knowledge that are used to verify private
computations. In particular we allow the deﬁnition of named proof components
that can be reused as sub-components of larger proofs.
We introduce the special notation F(b) →(a) to abstract details of proof com-
ponents. We call this a proof component declaration. For example we may name
a proof F on some secret inputs and outputs as being equivalent to a statement
in zero-knowledge in the following manner: F(a) →(b) ≡
K
a, b : b = a + 1. Se-
mantically, the function F represents the proof that a counter was incremented.
Secret a is the initial value and secret b the new value. Whether a variable ap-
pears on the left or the right is somewhat arbitrary and primarily meant to give
useful intuition to users of the component. In terms of cryptography, it is simply
syntactic sugar for the equivalent statement above.
Named proof components can be used in further higher-level proofs without
their details being made explicit. For example, the proof
K
c, d : d = F(c) is
equivalent to the two statements above. All variables within the component dec-
laration (e.g. variables a, b in F(a) →(b)) can be re-used in the high level proof.
Any variables whose knowledge is proved, but that are not in the declaration,
are considered inaccessible to the higher-level proof.
4.2
Direct Lookups
A P-signature on a sequence of messages, representing a number of keys and
values, can be used to prove the equivalent of a table look-up [41].
Consider a public table T of keys ki each mapping to the corresponding
values vi. Keys and values themselves can have multiple components ki =
(k(i,0), . . . , k(i,n−1)) and vi = (v(i,0), . . . , v(i,m−1)).
A trusted authority A can encode this table to facilitate zero-knowledge look-
ups by providing a set of P-signatures ti:
∀i.
ti = Sign(skA, ⟨Tid, n, m, k(i,0), . . . , k(i,n−1), v(i,0), . . . , v(i,m−1)⟩)
(2)

26
G. Danezis et al.
To prove that a set of secrets (k′, v′) corresponds to a lookup in the table it is
suﬃcient to show that:
LOOKUPT (k′
0, . . . , k′
n−1) →(v′
0, . . . , v′
m−1) ≡
(3)
K
t, k′
0, . . . , k′
n−1, v′
0, . . . , v′
m−1 :
(4)
PVerifypk A(t, Tid, n, m, k′
0, . . . , k′
n−1, v′
0, . . . , v′
m−1)
(5)
The predicate PVerify corresponds to the veriﬁcation equations of the signature
scheme. The proof hides the keys k′ and corresponding values v′, but not the
identity of the table used to perform the lookup. To avoid any confusion, we
always specify the table as part of the name of the lookup.
A variant of the proof allows for range lookups. In this case, a table T ′ contains
as key a pair of values (k(i,min), k(i,max)) deﬁning a range, corresponding to a set
of values (v(i,0), . . . , v(i,m−1)). We assume that the ranges deﬁned by the keys do
not overlap.
We denote the creation of the table with columns a, b and c, signed by author-
ity A, by an algorithm called ZKTABLEA([ai, bi, ci]). The cost of performing a
lookup in terms of computation and communication is O(1) and constructing a
table requires O(n) signatures in the size of the table.
5
Random Forests
Random forests are a state-of-the-art classiﬁcation technique. A random forest
is a collection of decision trees. Each decision tree is trained on a subset of a
training dataset. Once the set of trees is trained, it can be used to classify unseen
data items. Each decision tree is used separately to classify the item using its
features and the majority decision is accepted as valid.
The aim of our protocol is to apply the random forest classiﬁcation algorithm
to a feature set that remains secret. At the same time, we wish to provide a
proof that the classiﬁcation task was performed correctly. We assume that the
random forest classiﬁer is already trained and public.
5.1
Vanilla Training and Resolution of Random Forests
This section provides a short overview of how a random forest is grown from
a training set, as well as how it is used to classify data items, without any
security considerations. Full details are available in Breiman [11]. The notation
and resolution algorithms will be used as part of the privacy friendly protocols.
Consider a labelled training data set comprising items di. Each item has a set
of M features, denoted as Fm(di) for m in [0, M −1]. Each item is labelled with
li into one of two classes c0 or c1.
The training algorithm takes two parameters: the depth D and the number of
items N to be used to train each tree. Training each tree proceeds by sampling
N items, with replacement, from the available data items di. Then, at each
branch, starting with the root, a random feature is selected. The value of the

Private Client-Side Proﬁling
27
feature that best splits the data items corresponding to this branch into the two
separate classes is used to branch left or right. The algorithm proceeds to build
further branches until the tree has reached the maximum depth D. Leaves store
the decision or the relative number of items in each class.
A data item d that has not been previously seen is assigned a class by resolving
each tree t in the forest. Starting at the root, the branching variable b and its
threshold τ is used to decide whether to follow the right or left branch according
to the corresponding feature Fb(d′). The process is repeated until a leaf is reached
and the relative weight of belonging to two classes is stored as (ct0, ct1).
The sum of the weights corresponding to the two classes can be computed as
(
t ct0, 
t ct1). These sums represent the relative likelihood of the item belong-
ing to the corresponding classes.
5.2
Encoding of Features and Trees
The key objective of our algorithm is to correctly execute the random forest
classiﬁcation algorithm on certiﬁed features without revealing the features. To
enable this approach, both the user data and the random forest need to be
cryptographically encoded in a speciﬁc manner.
First, the proﬁle features are encoded as a table supporting zero knowledge
lookups (as described in Section 4.2). A user u, with a proﬁle with features
ki taking values vi, has to be provided by an authority A with a table: Fu ≡
ZKTABLEA([ki, vi, u]).
Second, we need to cryptographically encode each tree of the random forest.
Each decision tree is individually encoded by building two tables supporting
zero-knowledge lookups: one table for branches and one table for leaves.
Each non-leaf node of the decision tree is encoded as two separate left and
right branches. Branches are encoded by an entry containing a node id ni, a
feature id ki, and a range for the value [vmin, vmax] as well as target node id n′
i.
The node id ni represents the node of the tree, while ki is the feature used at
this node to make a decision. If the branch feature is indeed in [vmin,i, vmax,i],
then the algorithm should proceed to n′
i. Representing an interval that has to
be matched represents a uniform way to encode both left and right branches:
left branches are encoded with an interval [vmin, vmax] ≡[MINi, τi −1] and right
branches with an interval [vmin, vmax] ≡[τi, MAXi], where τi is the threshold of
feature ki that has a domain [MINi, MAXi]. An authority A′ encodes branches
as: Bt ≡ZKTABLEA′([ni, ki, vmin,i, vmax,i, n′
i]).
Leafs are encoded separately to store the decision for each decision tree. Each
leaf has a node id ni and two values c0,i, c1,i representing the relative likelihood
of the item being in two classes respectively. The table representing leaves signed
by A′ is Lt ≡ZKTABLEA′([ni, c0,i, c1,i]).
Branches and leaves of decision trees are encoded in order to certify its cor-
rectness when verifying proofs of signature possession. It is worth noting that
they are public vis-a-vis the prover. In particular, the node id of the root node
of each tree is known to all as is the ﬁxed depth D of trees.

28
G. Danezis et al.
5.3
Private Resolution for Random Forests
Given a table of user features Fu, and a set of |t| trees given by the sets of branch
and leaf tables Bt, Lt, we present the proofs of correct classiﬁcation.
In a nutshell, we can decompose the zero-knowledge proof required into per-
forming the classiﬁcation using the separate trees, and then aggregating and re-
vealing the forest decision (without revealing the intermediate decisions). Each
decision tree is matched by proving sequentially that the feature considered at
each branch satisﬁes the prescribed condition.
The zero knowledge proof of correct resolution of a single tree is:
RESOLVETREEt(Fu, Bt, Lt) →(c0,t, c1,t) ≡
(6)
K
∀j.nj, kj, vmin,j, vmax,j, vj, c0,t, c1,t :
(7)
n0 = start∧
(8)
for j = 0 . . . D −2 : {
(9)
nj, kj, vmin,j, vmax,j, nj+1 = LOOKUPBt(nj)∧
(10)
vj, u = LOOKUPFu(kj)∧
(11)
vmin,j ≤vj ∧vj ≤vmax,j}∧
(12)
(c0,t, c1,t) = LOOKUPLt(nD−1)
(13)
The RESOLVETREE proof works by starting with a record of the known root
node, and following D −2 branches to a leaf. For each branch, the proof demon-
strates that it follows the previous branch, and that the feature variable con-
cerned falls within the valid branch interval. The appropriate feature is looked-up
in the feature table without revealing its identity or value. Note that n0, u, D and
j are not secret, which leads to some eﬃciency improvement when implementing
the proof for the root branch.
To resolve a forest of |t| trees, the following proof is required:
RESOLVEFORESTt(Fu, ∀t.Bt, Lt) →(c0, c1) ≡
(14)
K
∀t.c0,t, c1,t, c0, c1 :
(15)
for t = 0 . . . |t| −1 : {
(16)
(c0,t, c1,t) = RESOLVETREEt(Fu, Bt, Lt)}∧
(17)
(c0, c1) = (

t
c0,t,

t
c1,t) ∧
(18)
POpenCc0(c0, openc0) ∧POpenCc1 (c1, openc1)
(19)
The RESOLVEFOREST proof uses the tree resolution as a sub-component. The
secret values returned from each tree are then aggregated into committed values
(c0, c1). These values can be revealed, or kept secret and used as part of a further
protocol (for example to prove that the probability of the user belonging to a
certain proﬁle is higher than a set threshold).

Private Client-Side Proﬁling
29
5.4
Extensions
Decision forests can be used to classify items into more than two classes.
To achieve this, the leaf table can encode likelihoods of multiple classes, and
the aggregation algorithm can sum the evidence for more than two classes in
parallel.
Similarly, more than one feature can be used to branch at each node. In such
cases, each branch needs to encode the identity and interval for all necessary
features. To maintain indistinguishability among all branches, the same number
of features must be tested for all of them.
6
HMMs, Probabilistic Automata and Regular Languages
Regular languages can be used to match streams of symbols to a set of rules
based on an initial state and possible transitions between states. Probabilistic
automata extend regular languages to consider speciﬁc probabilities associated
with initial states and transitions. Finally, hidden Markov models dissociate
states from emitted symbols. We show how a client can match a sequence of
certiﬁed actions or logs to a given model without revealing the speciﬁc actions
or log entries. We illustrate our techniques using hidden Markov models (HMM),
as both probabilistic automata and regular languages can be considered special
cases of such models. We also discuss how matching simpler models can be
implemented more eﬃciently.
6.1
Vanilla Matching for Hidden Markov Models
We ﬁrst review the HMM matching algorithms in the absence of any privacy
protection. A complete tutorial on hidden Markov model based techniques can
be found in [39].
A hidden Markov model is fully deﬁned by three tables: an initial state table,
a transition table, and a symbol emission table. All tables refer to a ﬁnite set of
hidden states and observed symbols. The initial state table maps possible initial
states of the model to a probability. The state transition table maps state pairs to
a probability. Finally, the symbol emission table maps state and symbol tuples
to a probability. We assume that any entry missing from a table is implicitly
assigned a probability of zero. By convention, a speciﬁc state is considered to be
the ﬁnal state of a model, and matching ends there.
Matching a sequence of symbols starts with assigning the ﬁrst symbol both a
state and a probability from the initial state table, and, given the initial state,
a probability of emitting the observed ﬁrst symbol from the symbol emission
table. Matching then proceeds through the sequence of observed symbols by
selecting a new hidden state for each symbol. The probability of state transi-
tion and the probability of emitting the subsequent symbol from the new state
can be retrieved from the state transition table and the symbol emission table
respectively.

30
G. Danezis et al.
The total probability of a match is the product of all the probabilities from
the initial state to the ﬁnal state. This product can also be expressed as the sum
of the logarithms of all the probabilities.
6.2
Encoding of Hidden Markov Models
To enable a user to prove that a sequence of their actions or log is matched by
a hidden Markov model, both their actions and the HMM need to be encoded
in a speciﬁc manner.
First, we consider how an authority A encodes a sequence of symbols re-
lating to a user. This sequence can represent speciﬁc user actions or generic
log entries relating to a user u. Each symbol in the sequence is represented
by an individual entry within a table supporting zero-knowledge lookups Su ≡
ZKTABLEA([ti, ei, u]), where ti is a sequence number and ei represents the sym-
bol observed.
Each table of the hidden Markov model is also encoded using tables that allow
for zero-knowledge lookups signed by authority A′. The initial symbol table It
contains: It ≡ZKTABLEA′([si, pi]), where si is a possible initial state and pi
the logarithm of its probability. The state transition table is represented as:
Tt ≡ZKTABLEA′([si, sj, pi]), where si and sj are states between which there is
a transition with log-probability pi. Finally, the emission table for each state si
and potential emitted symbol ej is encoded as: Et ≡ZKTABLEA′([si, ej, pi]).
We implicitly assign a zero probability to any entries that are not encoded
in the above tables. No proof of a match can be produced for zero-probability
matches. Furthermore, we store all probabilities as a positive integer representing
the quantised version of their negative logarithm. Therefore, the expression pi
used above is a shorthand for −⌈10ψ log πi⌉in case ψ decimal digits of the
logarithm are to be used where πi is the raw probability.
6.3
Private Matching
A user with a certiﬁed sequence of actions Su can prove that it matches a hidden
Markov model described by a triplet of tables (It, Tt, Et). This can be done
without revealing the symbols in the sequence of actions, their position within
the sequence and without revealing the matched sequence or states or matching
probability. In practice, it might be necessary to reveal the position of the match,
the probability of the match or some hidden states for subsequent processing. We
present the algorithms where only commitments to these are produced, which
can be selectively opened to reveal interesting values.
The MATCH function proves that a sequence of hidden states s0 . . . sl−1
match the sequence of symbols e0 . . . el−1 present, at the oﬀset t0 of the user
sequence, with probability p.

Private Client-Side Proﬁling
31
MATCH(Su, It, Tt, Et) →(s0 . . . sl−1, e0 . . . el−1, t0, p) ≡
(20)
K
∀i ∈[0, l −1].ti, si, ei, pi, p′
i, p :
(21)
e0, u = LOOKUPSu(t0) ∧
(22)
p0 = LOOKUPIt(s0) ∧
(23)
p′
0 = LOOKUPEt(s0, e0) ∧
(24)
for i = 1 . . . |i| −1 : {
(25)
ti = t0 + i ∧
(26)
ei, u = LOOKUPSu(ti) ∧
(27)
pi = LOOKUPTt(si−1, si) ∧
(28)
p′
i = LOOKUPEt(si, ei)} ∧
(29)
p =
l−1

0
(pi + p′
i) ∧
(30)
POpenCp(p, openp) ∧POpenCsl−1(sl−1, opensl−1)
(31)
The main result of the protocol is a commitment Csl−1 to the ﬁnal state sl−1
and a commitment Cp to the probability p of reaching this state. The prover can
open Csl−1 and Cp. The raw probability of a match can be computed as π =
e10−ψ·−p. Instead of opening the commitment corresponding to the probability
of the match p the user could prove in zero knowledge that it is within a certain
range.
We note that we do not hide the length of the match denoted by l. The user
is also free to commit to and open commitments to any of the values s0 . . . sl−1,
e0 . . . el−1, t0 to reveal more information to the veriﬁer.
We note that the above procedure proves there is a match and returns its
probability, but does not guarantee in itself that this is the best match, i.e. the
match with the highest probability. Such a procedure is likely to be computation-
ally more expensive, and thus not really practical for real-world applications. In
comparison, the above match algorithm requires only a linear number of proofs
in the length of the match.
6.4
Simpliﬁcations for Finite Automata and Regular Languages
Using the match algorithm for HMMs, we can trivially match regular languages
and probabilistic automata. For regular languages, we assume that each state
only emits one symbol with probability one, and that all probability values are
equal. For ﬁnite automata, we extend the encoding for regular languages to allow
for diﬀerent probabilities for the initial state and transitions between states. In
both cases, the matching algorithm can be simpliﬁed to avoid proving the link
between symbols and states, as they are in fact the same entities.
Similarly, for regular languages there is no need to store or process probabil-
ities as the length of the match contains all the necessary information.

32
G. Danezis et al.
Prover
Verifier
Authority or 
device A
Authority A’


Authority A 
confirms features
Authority A’ provides 
model
Prover asks trusted party to convince 
verifier that features match a profile

Fig. 2. An ideal protocol with a trusted party
7
Discussion
7.1
Security
Security and privacy properties. The security and privacy properties of our
client-side proﬁling schemes can be described via an ideal protocol depicted in
Figure 2 that carries out the same task. In the ideal protocol, a trusted party
T receives all the inputs of the parties involved in the protocol, performs the
required computations, and provides each party with its output.
As mentioned in Section 2, we have a setting with four parties: provers P,
veriﬁers V , an authority A that certiﬁes prover’s proﬁles, and an authority A′
that certiﬁes the model. In the ideal protocol, ﬁrst, A′ sends T the model, which
is either the random forest or the hidden Markov model. P sends T its features,
which forwards them to A. A replies T whether the features are correct, and, if it
is the case, T stores them. When P tells T to send the result of the proﬁling task
to V , T employs P’s features and the received model to perform the proﬁling
task and sends V the result.
As can be seen, in the ideal protocol V only learns the result, which protects
P’s privacy. The resolution of the proﬁling task is performed by T , which is
trusted, and thus the correctness of the result is guaranteed.
Our scheme mirrors the security and privacy properties of the ideal protocol
while limiting the involvement of trusted parties. Namely, our protocols require
A and A′ to be trusted when certifying the correctness of provers’ proﬁles and
of the model respectively. Once this is done, A and A′ do not participate in the
protocol anymore. P and V can be adversarial, and yet our scheme guarantees
that, as in the ideal protocol, the result computed by P is correct and V does
not learn more information on P’s features. We note that our protocols reveal to
V a small amount of information on the model, such as the depth of the decision
trees. Such information is also revealed to V by the ideal functionality in the
ideal protocol.

Private Client-Side Proﬁling
33
Secure implementation of our protocols. It may seem that the provable guar-
antees aﬀorded by zero-knowledge proofs of knowledge, particularly when using
the compilation techniques of [15] to generate a universally composable protocol,
are all that is needed to implement such an ideal functionality, and thus to auto-
matically guarantee the authenticity and privacy of the classiﬁcation result. The
soundness of proofs guarantees the correctness and authenticity of the classiﬁ-
cation task towards veriﬁers, and the fact that the protocols are zero-knowledge
guarantees to users that nothing besides the result of the classiﬁcation is re-
vealed.
Our system is, however, not only a zero-knowledge proof of knowledge. It also
consists of input generators that provide auxiliary information for the proofs,
e.g., signed lookup tables. Unfortunately, as noted by [14] many zero-knowledge
proof protocols designed with only honestly generated input in mind, are not
portable, i.e., they do not necessarily retain their properties when executed on
malicious input. In our system the input generation stage involves the prover
P, veriﬁers V , an authority A that certiﬁes users proﬁles, and an authority A′
that certiﬁes the tables and algorithms used for classiﬁcation. When considering
authenticity A and A′ are considered as being fully trusted by the veriﬁer. For
authenticity, portability is thus naturally maintained. For privacy we consider
A and A′, however, as potentially colluding with the veriﬁer. To prevent the
attacks of [33] the prover needs to be able to check that the cryptographic group
parameters provided by A and A′ are well formed as required in [14]. In our case
these parameters primarily consist of P-signature public keys, and commitment
parameters. To meet their security properties these schemes should already be
carefully designed to be portable. Note that provers need to verify the correct
generation of these public keys and commitment parameters only once in the
setup phase.
Portability is signiﬁcantly easier for prime order groups. Also in light of [34],
using a single pairing-based elliptic curve could thus be a conscious design deci-
sion, if one is willing to live without Strong RSA based CL-signatures (there are
pairing-based alternatives [18]) and sum-of-squares based range proofs [9]. This
may, however, exclude credential issuing authorities [16] that only support the
more mature Strong RSA based CL-signatures.
We do not provide a concrete implementation and leave many low-level design
decisions underspeciﬁed. We advice against adhoc implementations, and recom-
mend the use of zero-knowledge compilation frameworks such as [3,1] that are
approaching maturity.
Systems security issues. A malicious service provider could utilize our client-side
proﬁling schemes and yet employ covertly web-bugs, cookies or other techniques
to obtain further information about users. We point out that existing counter-
measures, e.g., [2] for web-bugs, can be employed to prevent that. When used in
combination with out scheme, service providers still get the result of the proﬁling
task.

34
G. Danezis et al.
7.2
Eﬃciency of Proposed Schemes
Encoding a table for zero-knowledge lookups using ZKTABLEA(·) requires one
re-ranomizable signature per encoded row. The LOOKUP operation requires the
proof of possession of a single P-signature, and RLOOKUP additionally requires
a range proof to ensure the key is within a secret range.
To prove that a value x lies within an interval [a, b], it is necessary to show
that x−a ≥0 and b−x ≥0. The techniques of Groth [29] requiring 3 additional
commitments and 3 zero-knowledge proofs of multiplication. Using set member-
ship techniques by Camenisch et al. [13] requires two proof of possession of a
signature per range proof.
The RESOLVETREE procedure relies heavily on zero-knowledge lookups and
range proofs: two LOOKUP and a range proof are needed per branch followed in
the tree. A single lookup is needed to retrieve the leaf with the decision. In total
4·(D−1)+1 proofs of possession of a signature are needed to resolve each decision
tree, where D is the depth of the tree. Aggregating the decisions of multiple
trees in RESOLVEFOREST can be done very eﬃciently using homomorphic
properties of commitments.
The MATCH procedure to resolve hidden Markov models requires 3 lookups
per symbol matched in the general case. This can be reduced to two lookups if
only regular automata are used (and the emitted symbols are equivalent to the
matched states). In case the location of the sequence into the user stream is not
secret, the stream can be encoded as a sequence of commitments, and a single
lookup is necessary per matched symbol.
8
Conclusion
A rule for building secure system states that all user input must be positively
validated to be correct. Traditionally, any computation performed on the client-
side cannot be trusted. As a result, personal information must be available to
be processed on the server side, which leads to a number of privacy problems.
In this paper, we extend the paradigm of secure client-side computation [23],
and show that complex classiﬁcation and pattern recognition tasks on user se-
crets can be proved correct. We heavily rely on the eﬃciency of proving posses-
sion of P-signatures to eﬃciently implement random forest and hidden Markov
model matching.
Our techniques allow for classiﬁcation and matching without any unnecessary
leakage of other information. Matching and classiﬁcation are often necessary to
transform unstructured data into information that can be acted upon: to perform
authentication, to tune a business decision, to detect intrusion, to reason about
a stream of activity.
Yet, in many contexts the mere act of proﬁling in itself can violate privacy.
Recent news stories, for example, describe how data mining techniques can be
used to detect whether one is pregnant for the purposes of marketing from shifts

Private Client-Side Proﬁling
35
in their buying habits1. Our techniques could deliver comparative functionality
without the need to reveal the detailed shopping list of a customer. The extent
to which one should be subjected to such proﬁling at all is highly debatable.
Providing a deﬁnite answer as to the necessity or oppression inherent in pro-
ﬁling is beyond the scope of this work. Those applying it to speciﬁc settings
would be advised to reﬂect on the remarks Roland Barthes made at his inaugu-
ral lecture at the College de France: “We do not see the power which is in speech
because we forget that all speech is a classiﬁcation, and that all classiﬁcations
are oppressive.”2
Acknowledgements. The authors would like to thanks the anonymous review-
ers and the paper shepherd, Ayday Erman, for suggestions that improved this
work.
References
1. Almeida, J.B., Bangerter, E., Barbosa, M., Krenn, S., Sadeghi, A.-R., Schneider,
T.: A Certifying Compiler for Zero-Knowledge Proofs of Knowledge Based on Σ-
Protocols. In: Gritzalis, D., Preneel, B., Theoharidou, M. (eds.) ESORICS 2010.
LNCS, vol. 6345, pp. 151–167. Springer, Heidelberg (2010)
2. Alsaid, A., Martin, D.: Detecting Web Bugs with Bugnosis: Privacy Advocacy
through Education. In: Dingledine, R., Syverson, P.F. (eds.) PET 2002. LNCS,
vol. 2482, pp. 13–26. Springer, Heidelberg (2003)
3. Bangerter, E., Briner, T., Henecka, W., Krenn, S., Sadeghi, A.-R., Schneider, T.:
Automatic Generation of Sigma-Protocols. In: Martinelli, F., Preneel, B. (eds.)
EuroPKI 2009. LNCS, vol. 6391, pp. 67–82. Springer, Heidelberg (2010)
4. Baum, L.E., Petrie, T.: Statistical Inference for Probabilistic Functions of Finite
State Markov Chains. The Annals of Mathematical Statistics 37(6), 1554–1563
(1966)
5. Belenkiy, M., Chase, M., Kohlweiss, M., Lysyanskaya, A.: P-signatures and Nonin-
teractive Anonymous Credentials. In: Canetti, R. (ed.) TCC 2008. LNCS, vol. 4948,
pp. 356–374. Springer, Heidelberg (2008)
6. Bellare, M., Goldreich, O.: On Deﬁning Proofs of Knowledge. In: Brickell, E.F.
(ed.) CRYPTO 1992. LNCS, vol. 740, pp. 390–420. Springer, Heidelberg (1993)
7. Bellare, M., Rogaway, P.: Random oracles are practical: A paradigm for designing
eﬃcient protocols. In: First ACM Conference on Computer and Communication
Security, pp. 62–73. Association for Computing Machinery (1993)
8. Bosch, A., Zisserman, A., Muoz, X.: Image classiﬁcation using random forests and
ferns. In: IEEE 11th International Conference on Computer Vision, ICCV 2007,
pp. 1–8. IEEE (2007)
9. Boudot, F.: Eﬃcient Proofs that a Committed Number Lies in an Interval. In:
Preneel, B. (ed.) EUROCRYPT 2000. LNCS, vol. 1807, pp. 431–444. Springer,
Heidelberg (2000)
1 Charles Duhigg. How Companies Learn Your Secrets. The New York Times. Febru-
ary 16, 2012.
2 In Barthes: Selected Writings (1982). Lecon (1978).

36
G. Danezis et al.
10. Brands, S.: Rapid Demonstration of Linear Relations Connected by Boolean Op-
erators. In: Fumy, W. (ed.) EUROCRYPT 1997. LNCS, vol. 1233, pp. 318–333.
Springer, Heidelberg (1997)
11. Breiman, L.: Random forests. Machine Learning 45(1), 5–32 (2001)
12. Camenisch, J.: Group Signature Schemes and Payment Systems Based on the Dis-
crete Logarithm Problem. PhD thesis, ETH Z¨urich (1998)
13. Camenisch, J.L., Chaabouni, R., Shelat, A.: Eﬃcient Protocols for Set Membership
and Range Proofs. In: Pieprzyk, J. (ed.) ASIACRYPT 2008. LNCS, vol. 5350, pp.
234–252. Springer, Heidelberg (2008)
14. Camenisch, J., Kiayias, A., Yung, M.: On the Portability of Generalized Schnorr
Proofs. In: Joux, A. (ed.) EUROCRYPT 2009. LNCS, vol. 5479, pp. 425–442.
Springer, Heidelberg (2009)
15. Camenisch, J., Krenn, S., Shoup, V.: A Framework for Practical Universally Com-
posable Zero-Knowledge Protocols. In: Lee, D.H., Wang, X. (eds.) ASIACRYPT
2011. LNCS, vol. 7073, pp. 449–467. Springer, Heidelberg (2011)
16. Camenisch, J.L., Lysyanskaya, A.: An Eﬃcient System for Non-transferable Anony-
mous Credentials with Optional Anonymity Revocation. In: Pﬁtzmann, B. (ed.)
EUROCRYPT 2001. LNCS, vol. 2045, pp. 93–118. Springer, Heidelberg (2001)
17. Camenisch, J.L., Lysyanskaya, A.: A Signature Scheme with Eﬃcient Protocols.
In: Cimato, S., Galdi, C., Persiano, G. (eds.) SCN 2002. LNCS, vol. 2576, pp.
268–289. Springer, Heidelberg (2003)
18. Camenisch, J.L., Lysyanskaya, A.: Signature Schemes and Anonymous Credentials
from Bilinear Maps. In: Franklin, M. (ed.) CRYPTO 2004. LNCS, vol. 3152, pp.
56–72. Springer, Heidelberg (2004)
19. Camenisch, J., Michels, M.: Proving in Zero-Knowledge that a Number Is the Prod-
uct of Two Safe Primes. In: Stern, J. (ed.) EUROCRYPT 1999. LNCS, vol. 1592,
pp. 107–122. Springer, Heidelberg (1999)
20. Camenisch, J.L., Stadler, M.A.: Eﬃcient Group Signature Schemes for Large
Groups. In: Kaliski Jr., B.S. (ed.) CRYPTO 1997. LNCS, vol. 1294, pp. 410–424.
Springer, Heidelberg (1997)
21. Chaum, D., Pedersen, T.P.: Wallet Databases with Observers. In: Brickell, E.F.
(ed.) CRYPTO 1992. LNCS, vol. 740, pp. 89–105. Springer, Heidelberg (1993)
22. Cramer, R., Damg˚ard, I.B., Schoenmakers, B.: Proof of Partial Knowledge and
Simpliﬁed Design of Witness Hiding Protocols. In: Desmedt, Y.G. (ed.) CRYPTO
1994. LNCS, vol. 839, pp. 174–187. Springer, Heidelberg (1994)
23. Danezis, G., Livshits, B.: Towards ensuring client-side computational integrity. In:
Cachin, C., Ristenpart, T. (eds.) CCSW, pp. 125–130. ACM (2011)
24. D´ıaz-Uriarte, R., De Andres, S.: Gene selection and classiﬁcation of microarray
data using random forest. BMC Bioinformatics 7(1), 3 (2006)
25. Fiat, A., Shamir, A.: How to Prove Yourself: Practical Solutions to Identiﬁcation
and Signature Problems. In: Odlyzko, A.M. (ed.) CRYPTO 1986. LNCS, vol. 263,
pp. 186–194. Springer, Heidelberg (1987)
26. Fredrikson, M., Livshits, B.: Repriv: Re-imagining content personalization and in-
browser privacy. In: IEEE Symposium on Security and Privacy, pp. 131–146. IEEE
Computer Society (2011)
27. Fujisaki, E., Okamoto, T.: Statistical Zero Knowledge Protocols to Prove Modular
Polynomial Relations. In: Kaliski Jr., B.S. (ed.) CRYPTO 1997. LNCS, vol. 1294,
pp. 16–30. Springer, Heidelberg (1997)
28. Goldwasser, S., Micali, S., Rivest, R.: A digital signature scheme secure against
adaptive chosen-message attacks. SIAM J. Comput. 17(2), 281–308 (1988)

Private Client-Side Proﬁling
37
29. Groth, J.: Non-interactive Zero-Knowledge Arguments for Voting. In: Ioannidis,
J., Keromytis, A.D., Yung, M. (eds.) ACNS 2005. LNCS, vol. 3531, pp. 467–482.
Springer, Heidelberg (2005)
30. Guha, S., Cheng, B., Francis, P.: Privad: Practical Privacy in Online Advertising.
In: Proceedings of the 8th Symposium on Networked Systems Design and Imple-
mentation (NSDI), Boston, MA (March 2011)
31. Juang, B.: Hidden markov models. Encyclopedia of Telecommunications (1985)
32. Karplus, K., Barrett, C., Hughey, R.: Hidden markov models for detecting remote
protein homologies. Bioinformatics 14(10), 846–856 (1998)
33. Kunz-Jacques, S., Martinet, G., Poupard, G., Stern, J.: Cryptanalysis of an Eﬃ-
cient Proof of Knowledge of Discrete Logarithm. In: Yung, M., Dodis, Y., Kiayias,
A., Malkin, T. (eds.) PKC 2006. LNCS, vol. 3958, pp. 27–43. Springer, Heidelberg
(2006)
34. Lenstra, A.K., Hughes, J.P., Augier, M., Bos, J.W., Kleinjung, T., Wachter, C.:
Ron was wrong, whit is right. Cryptology ePrint Archive, Report 2012/064 (2012),
http://eprint.iacr.org/
35. Levine, B.N., Shields, C., Margolin, N.B.: A survey of solutions to the sybil attack
(2006)
36. Magkos, E., Maragoudakis, M., Chrissikopoulos, V., Gritzalis, S.: Accurate and
large-scale privacy-preserving data mining using the election paradigm. Data &
Knowledge Engineering 68(11), 1224–1236 (2009)
37. Pedersen, T.P.: Non-Interactive and Information-Theoretic Secure Veriﬁable Secret
Sharing. In: Feigenbaum, J. (ed.) CRYPTO 1991. LNCS, vol. 576, pp. 129–140.
Springer, Heidelberg (1992)
38. Pinkas, B.: Cryptographic techniques for privacy-preserving data mining. SIGKDD
Explorations 4(2), 12–19 (2002)
39. Rabiner, L.: A tutorial on hidden markov models and selected applications in
speech recognition. Proceedings of the IEEE 77(2), 257–286 (1989)
40. Reznichenko, A., Guha, S., Francis, P.: Auctions in Do-Not-Track Compliant In-
ternet Advertising. In: Proceedings of the 18th ACM Conference on Computer and
Communications Security (CCS), Chicago, IL (October 2011)
41. Rial, A., Danezis, G.: Privacy-Preserving Smart Metering. In: Proceedings of the
11th ACM Workshop on Privacy in the Electronic Society (WPES 2011). ACM,
Chicago (2011)
42. Schnorr, C.: Eﬃcient signature generation for smart cards. Journal of Cryptol-
ogy 4(3), 239–252 (1991)
43. Toubiana, V., Narayanan, A., Boneh, D., Nissenbaum, H., Barocas, S.: Adnostic:
Privacy preserving targeted advertising. In: NDSS (2010)
44. Vaidya, J., Clifton, C., Kantarcioglu, M., Patterson, A.S.: Privacy-preserving de-
cision trees over vertically partitioned data. TKDD 2(3) (2008)

Understanding Statistical Disclosure:
A Least Squares Approach
Fernando P´erez-Gonz´alez1,2,3 and Carmela Troncoso4
1 Signal Theory and Communications Dept., University of Vigo
2 Gradiant (Galician R&D Center in Advanced Telecommunications)
3 Electrical and Computer Engineering Dept., University of New Mexico
fperez@gts.uvigo.es
4 K.U. Leuven/IBBT, ESAT/SCD-COSIC
carmela.troncoso@esat.kuleuven.be
Abstract. It is widely accepted that Disclosure Attacks are eﬀective
against high-latency anonymous communication systems. A number of
Disclosure Attack variants can be found in the literature that eﬀectively
de-anonymize traﬃc sent through a threshold mix. Nevertheless, these
attacks’ performance has been mostly evaluated through simulation and
how their eﬀectiveness varies with the parameters of the system is not
well-understood. We present the LSDA, a novel disclosure attack based
on the Maximum Likelihood (ML) approach, in which user proﬁles are
estimated solving a Least Squares problem. Further, contrary to previ-
ous heuristic-based attacks, our approach allows to analytically derive
formulae that characterize the proﬁling error of the LSDA with respect
to the system’s parameters. We verify through simulation that our pre-
dictors for the error closely model reality, and that the LSDA recovers
users’ proﬁles with greater accuracy than its predecessors.
1
Introduction
Mixes, relaying routers that hide the relation between incoming and outgoing
messages [2], are one of the main building blocks for high-latency anonymous
communications [4, 6, 9, 16]. A variety of Disclosure or Intersection Attacks [1,
3, 7, 8, 11, 12, 18, 19, 21] have been proposed to uncover persistent and repeated
patterns of communication taking place through a mix. In a nutshell, these
attacks ﬁnd a target user’s likely set of friends, also known as user proﬁle, by
intersecting the recipient anonymity sets of the messages this user sends.
Even though all attacks operate on the same principle they diﬀer on how they
exploit the observations in order to obtain user proﬁles. Statistical variants [3,7,
21] rely on heuristics to operate, while the Bayesian inference-based method by
Danezis and Troncoso [8] use Bayesian sampling techniques to recover accurately
users’ proﬁles in more complex systems than its predecessors. For any of these
attacks it is diﬃcult to obtain analytic results that characterize the dependence
of their eﬀectiveness on the parameters of the system, and hence they (as well
as their sequels [5,14,15,18]) have been mostly evaluated through simulation.
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 38–57, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Understanding Statistical Disclosure: A Least Squares Approach
39
In this paper we propose a novel proﬁling attack based on the Maximum
Likelihood (ML) approach. The attack estimates user proﬁles by solving a Least
Squares problem, ensuring that the mean squared error between the real and
estimated proﬁles is minimized. We empirically show that our attack indeed
minimizes the mean squared error with respect to heuristic disclosure attack
variants [3, 7, 21], although it performs slightly worse than the Bayesian ap-
proach [8] in the scenarios considered in this paper.
Nevertheless, we note that the most outstanding feature of the Least Squares
approach is that, contrary to its predecessors, it allows us to derive analytical
expressions that describe the evolution of the proﬁling error with the parameters
of the system. This is a key property, as it allows designers to choose system
parameters that provide a certain level of protection without the need to run
simulations. We thoroughly validate our results through simulation, proving that
our formulae reliably predict the evolution of our attack’s error as the parameters
of the system change.
The rest of the paper is organized as follows: in the next section we revisit
previous work on Disclosure Attacks and we describe our system and adversarial
models in Sect. 3. We introduce the Least Squares approach to disclosure in
Sect. 4; where we derive equations that characterize its error with respect to the
system parameters which we validate in Sect. 5. Finally, we discuss future lines
of work in Sect. 6, and we conclude in Sect. 7.
2
Related Work
We can ﬁnd diﬀerent ﬂavors of disclosure attacks in the literature [1,3,7,8,10–13,
18,19,21] which we now proceed to revisit. A ﬁrst family of intersection attacks
are the so-called Disclosure Attack [1,10] and its sequels [11–13,18]. These attacks
rely on Graph Theory in order to uncover the recipient set of a target user Alice.
They seek to identify mutually disjoint sets of receivers amongst the recipient
anonymity sets of the messages sent by Alice, which are intersected with the
anonymity sets of Alice’s sent messages to ﬁnd her communication partners.
The main drawback of the Disclosure attack is that it is equivalent to solving a
Constraint Satisfaction Problem which is well-known to be NP-complete.
The subfamily of Hitting Set Attacks [11,12,18] speeds up the search for Alice’s
messages recipients by looking for unique minimal hitting sets. An evaluation
of this attack is provided in [18], where the relationship between the number of
rounds the adversary needs to observe to uniquely identify the set of receivers is
analyzed. The study by Pham et al. is similar to our work in spirit, but diﬀerent
in that they focus on attacks that unambiguously identifying recipient sets while
our focus is on statistical attacks that only provide an estimation of such sets as
the ones discussed below.
The series of statistical attacks was started by Danezis in [3] where he intro-
duced the Statistical Disclosure Attack (SDA). Danezis observed that for a large
enough number of observed mixing rounds the average of the probability distri-
butions describing the recipient anonymity set [20] of Alice’s messages oﬀers a

40
F. P´erez-Gonz´alez and C. Troncoso
very good estimation of her sending proﬁle. Danezis considers that in each round
where Alice sends a message, the recipient anonymity set of this message is uni-
form over the receivers present in the round (and zero for the rest of users). The
SDA was subsequently extended to more complex mixing algorithms [7], to traﬃc
containing replies [5], to consider other users in order to improve the identiﬁcation
of Alice’s contacts [14], and to evaluate more complex user models [15].
Troncoso et al. proposed in [21] two attacks that outperform the SDA, the
Perfect Matching Disclosure Attack (PMDA) and the Normalized Statistical
Disclosure Attack (NSDA). Under the observation that in a round of mixing
the relationships between sent and received messages must be one-to-one, the
attacks consider interdependencies between senders and receivers in order to as-
sign most likely receivers to each of the senders: the PMDA searches for perfect
matchings in the underlying bipartite graph representing a mix round, while the
NSDA normalizes the adjacency matrix representing this graph. The recipient
anonymity set of each sender’s message in a round is built taking into consid-
eration the result of this assignment, instead of assigning uniform probability
amongst all recipients.
Last, Danezis and Troncoso propose to approach the estimation of user proﬁles
as a Bayesian inference problem [8]. They introduce the use of Bayesian sampling
techniques to co-infer user communication proﬁles and de-anonymize messages.
The Bayesian approach can be adapted to analyze arbitrarily complex systems
and outputs reliable error estimates, but it requires the adversary to repeatedly
seek for perfect matchings increasing the computational requirements of the
attack.
We note that previous authors evaluated the attacks either from mostly a
de-anonymization of individual messages perspective (e.g., [8, 21]), or from the
point of view of the number of rounds necessary to identify a percentage of
Alice’s recipients (e.g., [14, 15]). In this work we are interested in the accuracy
with which the adversary can infer the sender (respectively, receiver) proﬁle of
Alice, i.e., we not only seek to identify Alice’s messages receivers, but also to
estimate the probability that Alice sends (or receives) a message to (from) them.
3
System Model
In this section we describe our model of an anonymous communication system
an introduce the notation we use throughout the paper, summarized in Table 3.
System Model. We consider a system in which a population of Nusers users,
designated by an index i ∈{1, . . ., Nusers}, communicate through a threshold
mix. This mix operates as follows. In each round of mixing it gathers t messages,
transforms them cryptographically, and outputs them in a random order; hence
hiding the correspondence between incoming and outgoing messages.
We model the number of messages that the ith user sends in round r as the
random variable Xr
i ; and denote as xr
i the actual number of messages i sends
in that round. Similarly, Y r
j is the random variable that models the number of
messages that the jth user receives in round r; and yr
j the actual number of

Understanding Statistical Disclosure: A Least Squares Approach
41
messages j receives in that round. Let xr and yr denote the column vectors
that contain as elements the number of messages sent or received by all users in
round r: xr = [xr
1, · · · , xr
Nusers]T , and yr = [yr
1, · · · , yr
Nusers]T , respectively. When
it is clear from the context, the superindex r is dropped.
Users in our population choose their recipients according to their sending
proﬁle qi
.= [p1,i, p2,i, · · · , pNusers,i]T ; where pj,i models the probability that user
i sends a message to user j. We consider that a user i has f friends to whom
she sends with probability pj,i, and assign pj,i = 0 for each user j that is not
a friend of i. Conversely, pj is the column vector containing the probabilities
of those incoming messages to the jth user, i.e., pj
.= [pj,1, pj,2, · · · , pj,Nusers]T .
(This vector can be related to the receiving proﬁle of user j through a simple
normalization, i.e., by dividing its components by Nusers
i=1
pj,i.) We denote as
fj the number of senders that send messages to receiver j, i.e., the cardinality
of the set Fj = {i|pj,i > 0, pj,i ∈pj}; and deﬁne τf .= Nusers
i=1
f 2
i /(f 2Nusers),
which shall come handy in the performance evaluation performed in Sect. 5.
Adversary Model. We consider a global passive adversary that observes the
system during ρ rounds. She can observe the identity of the senders and receivers
that communicate through the mix. As our objective is to illustrate the impact
of disclosure attacks on the anonymity provided by the mix we assume that the
cryptographic transformation performed during the mixing is perfect and thus
the adversary cannot gain any information from studying the content of the
messages.
The adversary’s goal is to uncover communication patterns from the observed
ﬂow of messages. Formally, given the observation xr = {xr
i } and yr = {yr
j}, for
i, j = 1, . . . , Nusers, and r = 1, . . . , ρ , the adversary’s goal is to obtain estimates
ˆpj,i as close as possible to the probabilities pj,i, which in turn can be used to
recover the users’ sender and receiver proﬁles.
4
A Least Squares Approach to Disclosure Attacks
We aim here at deriving a proﬁling algorithm based on the Maximum Likelihood
(ML) approach to recover the communication patterns of users anonymously
communicating through a threshold mix. The general idea is to be able to esti-
mate the probabilities pj,i that user i sends a message to user j, which allow to
simultaneously determine the sender and receiver proﬁles of all users.
We make no assumptions on the user’s proﬁles (i.e., we impose no restrictions
on the number of friends a user may have, nor on how messages are distributed
amongst them). Nevertheless, we follow the standard assumptions regarding users’
behavior and consider that they are memoryless (i.e., for a user the probability of
sending a message to a speciﬁc receiver does not depend on previously sent mes-
sages), independent (i.e., the behavior of a certain user is independent from the
others), with uniform priors (i.e., any incoming message to the mix is a priori sent
by any user with the same probability), and stationary (i.e., the parameters mod-
eling their statistical behavior do not change with time).

42
F. P´erez-Gonz´alez and C. Troncoso
Table 1. Summary of notation
Symbol
Meaning
Nusers
Number of users in the population, denoted by i = {1, · · · , Nusers}
f
Number of friends of each sender i
t
Threshold mix
fj
Number of senders sending messages to receiver j
τf
Nusers
j=1
f 2
j /(f 2Nusers)
pj,i
Probability that user i sends a message to user j
qi
Sender proﬁle of user i, qi = [p1,i, p2,i, · · · , pNusers,i]T
pj
Unnormalized receiver proﬁle of user j, pj = [pj,1, pj,2, · · · , pj,Nusers]T
ρ
Number of rounds observed by the adversary
xr
i (yr
j )
Number of messages that the ith (jth) user sends (receives) in round r
xr (yr)
Column vector containing elements xr
i (yr
j ), i = 1, · · · , Nusers
ˆpj,i
Adversary’s estimation of pj,i
ˆqi
Adversary’s estimation of user i’s sender proﬁle qi
ˆpj
Adversary’s estimation of user j’s unnormalized receiver proﬁle pj
4.1
Analysing One Round of Mixing
For simplicity of notation we will consider ﬁrst a single round of observations,
and later explain how to extend the derivation to an arbitrary number of rounds.
Hence, for the moment, we will drop the superindex r. Let Yj,i be the random
variable that models the number of messages received by user j that were sent
by user i in the round under consideration. Then the number of messages that
the jth user receives in this round can be computed as:
Yj =
Nusers

i=1
Yj,i .
Recall that pj,i represents the probability that user i sends a message to user
j. Then, the probability of user j receiving yj,i messages when the number of
messages sent by user i is Xi = xi is given by a binomial distribution:
Pr(Yj,i = yj,i|Xi = xi) =
 xi
yj,i

pyj,i
j,i (1 −pj,i)xi−yj,i ,
(1)
whose mean is xi · pj,i and variance xi · pj,i(1 −pj,i). This probability can be
approximated by a Gaussian with the same mean and variance.
It is important to notice that the variables Yj,i, j = 1, · · · , Nusers are not
independent, and rather they are jointly modeled by a multinomial distribu-
tion. However, the covariance cov(Yj,i, Yk,i) = −xi · pj,i · pk,i, k ̸= j, is small
(in comparison with diagonal terms of the covariance matrix) if the transition
probabilities are also small. Moreover, in such case the variance of the binomial
can be approximated by xi · pj,i. Therefore, when the transition probabilities
are small, and recalling that the sum of independent Gaussian random variables

Understanding Statistical Disclosure: A Least Squares Approach
43
is itself Gaussian, we can approximate the conditional distribution of Yj by a
normal:
Pr(Yj|X = x) ∼N
Nusers

i=1
xipj,i,
Nusers

i=1
xipj,i

,
and consider that cov(Yj, Yk) ≈0, whenever k ̸= j.
Under the hypothesis above, since the random variables Yj are approximately
independent, we can write the joint probability of Y as
Pr(Y|X = x) ∼N (Hp, Σy) ,
where pT .= [pT
1 , pT
2 , · · · , pT
Nusers], Σy .= diag(Hp), and HT .= x ⊗INusers. Here,
In denotes the identity matrix of size n×n, and ⊗denotes the Kronecker product.
For a ML solution to the proﬁling problem, after observing Y = y, we seek
that vector ˆp of probabilities that maximizes Pr(Y = y|X = x).1 This can be
explicitly written as follows:
ˆp = arg max
p∈P
1

det(Σy)
· exp

−1
2(y −Hp)T Σ−1
y (y −Hp)

,
(2)
where P denotes the set of valid probability vectors.2
For the unconstrained problem in (2) it is possible to uncouple the diﬀerent
terms and show that the solution must satisfy
xT ˆpj = 1
2
	
1 + 4y2
j −1

,
j = 1, · · · , Nusers ,
(3)
where ˆpj is the estimated unnormalized receiver proﬁle of user j.
The right hand side of (3) is smaller than yj; however, it can be well approxi-
mated by yj when the latter is large. Notice that (3) becomes an underdetermined
linear system of equations.
4.2
Analysing ρ Rounds
A diﬀerent situation arises when the number of observed rounds is larger than
the number of users. In this case, we form the following vectors/matrices:
YT .= [Y 1
1 , Y 2
1 , · · · , Y ρ
1 , Y 1
2 , Y 2
2 , · · · , Y ρ
2 , · · · , Y 1
Nusers, Y 2
Nusers, · · · , Y ρ
Nusers]
UT .= [x1, x2, · · · , xρ]
H .= U ⊗INusers
1 Notice that since the random variable X does not depend on the probabilities p, the
maximization of Pr(Y = y|X = x) is equivalent to that of Pr(Y = y; X = x).
2 Without further constraints, that may be furnished when there is partial knowledge
about the transition probabilities, P is simply given by the constraints 0 ≤pj,i ≤1
for all j, i, and Nusers
j=1
pj,i = 1, for all i.

44
F. P´erez-Gonz´alez and C. Troncoso
The ML solution must satisfy (2). However, notice that in the case of ρ rounds,
the involved matrices and vectors are larger than those found in the case of a
single observation.
Unlike (3), a closed-form solution seems not exist (even for the unconstrained
case, i.e., when no constraints are imposed upon P). We examine next some
approximate solutions to the unconstrained problem that satisfy that ˆp →p as
ρ →∞. To make this possible, we disregard the dependence of the covariance
matrix Σy with p making the following approximation Σy ≈diag(y).
In such case, the approximate ML estimator is given by
ˆp = arg min
p∈P ||Σ−1/2
y
(y −Hp)||2 ,
(4)
which is nothing but a constrained weighted least squares (WLS) problem.
For simplicity, we consider here the unweighted least squares (LS) case, i.e.,
ˆp = arg min
p∈P ||y −Hp||2 ,
(5)
which, for the unconstrained case, has the well-known Moore-Penrose pseudoin-
verse solution:
ˆp = (HT H)−1HT y .
(6)
At ﬁrst sight, it might look that the matrix inversion needed in (6) is formidable:
the matrix HT H has size N 2
users × N 2
users. However, its block-diagonal structure
allows for a much more eﬃcient solution; indeed,
HT H = (U ⊗INusers)T · U ⊗INusers = (UT U) ⊗INusers
and, hence,
(HT H)−1 = (UT U)−1 ⊗INusers
where now UT U has size Nusers × Nusers.
The decoupling above allows us to write a more eﬃcient solution as follows.
Let yj · [y1
j , y2
j , · · · yρ
j ]T . Then, the LS estimate ˆpj for the jth probability vector
can be written as
ˆpj = (UT U)−1UT yj,
j = 1, · · · , Nusers .
The decoupling above is possible only in the unconstrained case; this consider-
ation, together with the simplicity of the performance analysis, make us focus
on the unconstrained LS approach. Notice, however, that, as a consequence, the
obtained solution is not guaranteed to meet the constraints on the transition
probabilities. This can be overcome by projecting the solution onto the set P. In
any case, the fact that the error p −ˆp tends to zero as ρ →∞, ensures that ˆp
can be made arbitrarily close to P by increasing the number of observed rounds.
Finally, note that when ˆpj is computed for all users, it is also possible to recover
the sender proﬁles qi by taking the rows of the matrix ˆp.
In any case, it is worth remarking that there are many iterative algorithms
for solving (constrained) least squares problems, which do not require matrix

Understanding Statistical Disclosure: A Least Squares Approach
45
inversion. We leave the discussion on how they can be adapted to the problem
as subject for future work. It is also worth stressing that we could have arrived
at the LS estimate from the perspective of minimizing the mean square error
between the observed y and a predictor based on a linear combination of the
given inputs x1, x2, · · · , xρ.
Finally, we note that the original Statistical Disclosure Attack (SDA) cor-
responds to a particular case of the proposed LS estimator. The SDA model
assumes that the ﬁrst user (Alice) sends only one message to an unknown re-
cipient chosen uniformly from a set f friends. The other users send messages
to recipients chosen uniformly from the set of all users pj,i = 1/Nusers, ∀i ̸= 1.
From this considerations, for a given round r where Alice does send a message,
we have that xr
1 = 1 and Nusers
j=2
xr
i = (t−1), and all the transition probabilities
of the form pj,i, for i ≥2 are known to be equal to 1/Nusers. If we suppose
that in all rounds Alice transmits a message, we will have a vector y which con-
tains the ρ · Nusers observations, p1 is unknown and all pi, i = 2, · · · , Nusers are
known. From here, it is possible to ﬁnd that the LS estimate of the unknown
probabilities is
ˆpj,1 = 1
ρ
ρ

r=1
yr
j −(t −1)
Nusers
,
j = 1, · · · , Nusers
which coincides with the SDA estimate. (We leave a more detailed derivation of
this equation for an extended version of this paper [17].)
4.3
Performance Analysis with Respect to the System Parameters
The Least Squares estimate in (6) is unbiased: it is straightforward to show that
E[ˆp] = p. On the other hand, the covariance matrix of ˆp, for a ﬁxed matrix H,
is given by
E[(p −ˆp)(p −ˆp)T ] = (HT H)−1HT ΣyH(HT H)−1 .
(7)
Notice that the performance will depend on the actual input matrix H; however,
when the input process is wide-sense stationary, and ρ →∞then UT U will
converge to the input autocorrelation matrix Rx. Then, when the number of
observations is large, approximating UT U ≈Rx will allow us to extract some
quantitative conclusions that are independent of U. To this end, notice that if
Cov(Yi, Yj) ≈0 for all i ̸= j, then
Σy ≈diag(ξy) ⊗INusers ,
with ξy = [Var{Y1}, · · · , Var{YNusers}].
In this case, (7) becomes
E[(p −ˆp)(p −ˆp)T ] = diag(ξy) ⊗(UT U)−1 .
(8)

46
F. P´erez-Gonz´alez and C. Troncoso
Still we would need to quantify how large (UT U)−1 is. Since UT U is symmetric,
we can write the following eigendecomposition
UT U = QΛQ−1 ,
(9)
where Q is orthonormal and Λ is diagonal. In this case, (UT U)−1 = Q−1Λ−1Q.
Then, if we deﬁne the transformed probability space where p′
j
.= Qpj and ˆp′
j
.=
Qˆpj we have
E[(p′ −ˆp′)(p′ −ˆp′)T ] = diag(ξy) ⊗Λ−1
(10)
A measure of the total error variance made with the proposed estimator is given
by the trace. Notice that
E

tr

(p′ −ˆp′)(p′ −ˆp′)T 
= E

tr

(p −ˆp)(p −ˆp)T 
=
Nusers

i=1
σ2
yi ·
Nusers

j=1
λ−1
u,j
(11)
where λu,j, j = 1, · · · , Nusers denote the eigenvalues of UT U.
Equation (11) can be interpreted as having two terms that depend on the
output covariance and input autocorrelation, respectively. In fact, for some cases
of interest, it is possible to derive explicit expressions, as we discuss next.
Consider the case where each user has exactly the same probability 1/Nusers
of sending a message to one of her friends and that each message is sent inde-
pendently. Then, if t messages are sent per round, the observed input vector at
the jth round xj will follow a multinomial distribution for which
E{X2
i } = t2p2
x + tpx(1 −px)), and E{XiXk} = t2p2
x −tp2
x,
i ̸= k
where px = 1/Nusers. Then, the autocorrelation matrix Rx can be shown to have
(Nusers −1) identical eigenvalues which are equal to ρ · t · px and the remaining
eigenvalue equal to ρ · t · px + ρ · t · p2
x(t −1)Nusers. Therefore,
Nusers

j=1
λ−1
u,j = Nusers
ρt

Nusers −1 + 1
t

(12)
Next we focus on the output variance. We consider the case where each user has
f friends in her sending proﬁle to whom she sends messages with probability
1/f each. Let Fj be the set of users that send messages to the jth user with
non-zero probability, and let fj be its cardinality. Then, for the input conditions
discussed in the previous paragraph (i.e., i.i.d. uniform users), the probability
that one given message is sent by one user in Fj is fj/Nusers. In turn, the
probability that one message originating from a user in Fj is sent to the jth
user is 1/f. Therefore, we can see Y k
j as the output of a binomial process with
probability
pyj =
fj
fNusers
,

Understanding Statistical Disclosure: A Least Squares Approach
47
and with t messages at its input. Hence, the variance of Yj is
σ2
yj = t · pyj(1 −pyj) =
t · fj
f · Nusers
·

1 −
fj
f · Nusers

,
so the sum of variances becomes
Nusers

j=1
σ2
yj = t

1 −
Nusers
j=1
f 2
j
f 2N 2users

= t

1 −
τf
Nusers

,
(13)
where we have used the fact that Nusers
i=1
fj = f · Nusers.
Combining (12) and (13) we can write the MSE as
E

tr

(p −ˆp)(p −ˆp)T 
= 1
ρ

Nusers −1 + 1
t

· (Nusers −τf) .
(14)
It is useful to interpret (14) in terms of the number of friends of each receiver.
We will consider two particular cases of interest: 1) If each receiver has exactly
f friends, then τf = τf,1 = 1; 2) If only f receivers have Nusers friends, and
the remaining Nusers −f receivers have no friends, then τf = τf,2 = Nusers/f.
The second case models a situation where f receivers act as hubs (i.e., f users
concentrate the traﬃc of all the population), while in the ﬁrst there is absolutely
no skew in the distribution. In fact, using the Lagrange multipliers technique, it
can be shown that for all other cases, including random connections (but always
keeping the constraint that each sender has exactly f friends), the parameter
τf satisﬁes that τf,1 ≤τf ≤τf,2. Since (14) monotonically decreases with τf,
we can conclude that for the symmetric case (i.e., τf = 1) the MSE is larger,
revealing that it will be harder to learn the transition matrix.
When Nusers is large, we can approximate (14) as follows
E

tr

(p −ˆp)(p −ˆp)T 
≈N 2
users
ρ
.
(15)
If we recall that there are N 2
users probabilities to estimate from the transition
matrix, we can conclude that the variance per transition element pj,i is approx-
imately 1/ρ. The total MSE decreases as 1/ρ with the number of rounds ρ; this
implies that the unconstrained, unweighted LS estimator is asymptotically eﬃ-
cient as ρ →∞. Even though this is somewhat to be expected, notice that other
simpler estimators might not share this desirable property, as we will experi-
mentally conﬁrm in Sect. 5.
5
Evaluation
5.1
Experimental Setup
We evaluate the eﬀectiveness of the Least Squares approach to Disclosure Attacks
(LSDA) against synthetic anonymized traces created by a simulator written in

48
F. P´erez-Gonz´alez and C. Troncoso
the Python language.3 We simulate a population of Nusers users with f contacts
each, to whom they send messages with equal probability (i.e., pj,i = 1/f if i
is friends with j, zero otherwise). In order to easily study the inﬂuence of the
system parameters on the success of the attack, in our simulations we further
ﬁx the senders that send messages to each receiver to be fj = f. In other words,
every sender (receiver) proﬁle has the same number of non-zero elements, and
hence τf = 1. Messages are anonymized using a threshold mix with threshold
t, and we consider that the adversary observes ρ rounds of mixing. Table 2
summarizes the values of the parameters used in our experiments, where bold
numbers indicate the parameters of the baseline experiment.
Table 2. System parameters used in the experiments
Parameter
Value
Nusers
{50, 100, 150, 200, 250, 300, 350, 400, 450, 500}
f
{5, 10, 15, 20, 25, 30, 35, 40, 45, 50}
t
{2, 5, 10, 20, 30, 40}
ρ
{10 000, 20 000, . . . , 100 000}
τf
{1.0, 1.76, 2.44, 3.04, 3.56, 4.0}
The parameters’ values used in our experiments, though rather unrealistic,
have been chosen in order to cover a wide variety of scenarios in which to study
the performance of the attack while ensuring that experiments could be carried
out in reasonable time. We note, however, that the results regarding the LSDA
can be easily extrapolated to any set of parameters as long as the proportion
amongst them is preserved. Unfortunately, we cannot make a similar claim for
the other attacks. Their heuristic nature makes it diﬃcult to obtain analytical
results that describe the dependence of their success on the system parameters,
and the evolution of their error diﬃcult to predict as we will see throughout this
section.
Besides testing the eﬀectiveness of the LSDA when proﬁling users, we also
compare its results to those obtained performing the Statistical Disclosure Attack
(SDA) [3,7], the Perfect Matching Disclosure Attack (PMDA) [21], the Normal-
ized Statistical Disclosure Attack (NSDA) [21], and the Bayesian inference-based
attack Vida [8].
5.2
Success Metrics
We recall that the goal of the adversary is to estimate the values pj,i with as
much accuracy as possible. The LSDA, as described in Sect. 4, is optimized to
minimize the Mean Squared Error (MSE) between the actual transition probabil-
ities pj,i and the adversary’s estimated ˆpj,i. We deﬁne two metrics to illustrate
the proﬁling accuracy of the attacks. The Mean Squared Error per transition
probability (MSEp) measures the average squared error between the elmements
3 The code will be made available upon request.

Understanding Statistical Disclosure: A Least Squares Approach
49
of the estimated matrix ˆp and the elements of the matrix p describing the actual
behaviour of the users (see (6)):
MSEp =

i,j(ˆpj,i −pj,i)2
N 2users
.
Secondly, we deﬁne the Mean Squared Error per sender proﬁle (MSEqi):
MSEqi =

j(ˆpj,i −pj,i)2
Nusers
, i = 1, . . . , Nusers
which measures the average squared error between the probability of the esti-
mated ˆqi and actual qi user i’s sender proﬁles. Both MSEs measure the amount
by which the values output by the attack diﬀer from the actual value to be es-
timated. The smaller the MSE, the better is the adversary’s estimation of the
users’ actual proﬁles.
For each of the studied set of parameters (Nusers, f, t, ρ, τf) we record
the sets of senders and receivers during ρ rounds and compute the MSEp (or
the MSEqi) for each of the attacks. We repeat this process 20 times and plot the
average of the result in our ﬁgures.
5.3
Results
Estimating Sender and Receiver Proﬁles with the LSDA. We ﬁrst illus-
trate how the LSDA can simultaneously estimate sender and receiver proﬁles.
Traditionally, Disclosure Attacks focus in estimating the sender proﬁles; and re-
ceiver proﬁles can be inferred by resolving the inverse problem (i.e., performing
the same attack inverting the role of senders and receivers). Even the Reverse
Statistical Disclosure Attack [14], that explicitly requires receiver proﬁles to im-
prove the estimation of the sender proﬁles, includes a step in which the SDA is
applied in the reverse direction before results can be obtained.
The LSDA estimates the full matrix p in one go. By either considering
the rows or columns of this matrix the adversary can recover the unnormal-
ized receiver proﬁle pj = [pj,1, pj,2, · · · , pj,Nusers]T , or the sender proﬁle qi =
[p1,i, p2,i, · · · , pNusers,i]T without any additional operation. Fig. 1, left, shows box
plots4 describing the distribution of the MSEp over 20 experiments for senders
and receiver proﬁles, respectively. The right-hand side of the ﬁgure shows the
sender and the receiver proﬁles computed performing the LSDA in the reverse
direction, considering the receivers as senders, and vice versa.
We can see that the results obtained in the “forward” and reverse direction are
not the same. In fact, we have observed that in each instance there is a direction
4 The line in the middle of the box represents the median of the distribution. The
lower and upper limits of the box correspond, respectively, to the distribution’s ﬁrst
(Q1) and third quartiles (Q3). We also show the outliers, represented with +: values
x which are “far” from the rest of the distribution (x > Q3 + 1.5(Q3 −Q1) or
x < Q1 −1 : 5(Q3 −Q1)).

50
F. P´erez-Gonz´alez and C. Troncoso
0
1
2
3
4
5
6
1e
4
MSEp
Sender
Receiver
0
1
2
3
4
5
6
1e
4
Sender inverse
Receiver inverse
Fig. 1. LSDA’s MSEp per transition probability when inferring sender and receiver
proﬁles in the forward (left) and reverse (right) directions (Nusers = 100, f = 25,
t = 10, ρ = 10 000, τf = 1)
that is better than the other in terms of MSEp. While it is not possible to decide
which side is going to provide better results, because a priori all proﬁles are equally
likely, it is easy to see that the average of the estimations ˆp in both directions will
yield a MSE per transition probability smaller than the worst case.
Performance with Respect to the Number of Rounds ρ. As we discuss in
Sect. 4.3, the number of observed rounds ρ has a dominant role in the estimation
error incurred by the LSDA. We plot in Fig. 2, left, the MSE per transition
probability MSEp for the SDA, NSDA, PMDA and LSDA.
The LSDA, optimized to minimize the MSEp, obtains the best results. Fur-
ther, we can see how the approximation in Eq. (15), represented by • in the
ﬁgure, reliably describes the decrease in the proﬁle estimation error as more
information is made available to the adversary.
It is also interesting to notice how the diﬀerent attacks take advantage of the
information procured by additional rounds. The naive approach followed by the
SDA soon maxes out in terms of information extracted from the observation and
its MSEp does not decrease signiﬁcantly as more rounds are observed, conﬁrming
the results in [21]. The NSDA and PMDA perform slightly better in this sense,
although their MSEp also decreases slowly. The LSDA, on the other hand, is
able to obtain information from each new observed round reducing signiﬁcantly
the MSEp, that tends to zero as ρ →∞. This is because, as opposed to its
predecessors which process the rounds one at a time, the LSDA considers all
rounds simultaneously (by means of the matrices Y and U).
Performance with Respect to the Mix Threshold t. By observing Eq. (14)
one can see that the threshold t of the mix has little inﬂuence on the MSEp of
the LSDA, becoming negligible as t increases and t >> 1. This is reﬂected by our
experiments, shown in Fig. 2, left, where the error of the LSDA soon becomes
stable as the threshold of the mix grows.

Understanding Statistical Disclosure: A Least Squares Approach
51
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
1e
3
MSEp
10
20
30
40
50
60
70
80
90
100
Number of rounds (ρ)
1e3
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
1e
3
2
5
10
20
30
40
Mix threshold (t)
SDA
PMDA
NSDA
LSDA
Theory
Fig. 2. MSEp evolution with the number of rounds in the system ρ (N = 100, f = 25,
t = 10, τf=1), and with the threshold mix t (N = 100, f = 25, ρ = 10 000, τf = 1)
(left and right, respectively).
This desirable property does not hold for the other approaches. As expected,
increasing the threshold has a negative eﬀect on the three attacks. Nevertheless
this eﬀect diﬀers depending on the approach used. The SDA’s, surprisingly, seems
to grow proportionally to (1 −1/t) and thus the increase of the error with the
threshold is greatly reduced as t increases. This is not the case for the NSDA and
PMDA, based on solving an optimization problem on the underlying bipartite
graph representing a mix round. This problem becomes harder as the threshold
grows, thus their MSEp signiﬁcantly increases with the number of messages
processed in each mix round.
Performance with Respect to the Number of Users Nusers. Next, we
study the inﬂuence of the number of users in the system on the estimation error.
The results are shown in Fig. 3 for ρ = 10 000 (left) and ρ = 100 000 (right). As
expected (see 15), the LSDA’s MSEp grows slowly with the number of users. The
other three attacks, on the other hand, improve their results when the number
of users increase. When the number of users increases, and the mix threshold
does not vary, the intersection between the senders of diﬀerent mixing rounds
becomes smaller, and thus the SDA can better identify their sender proﬁles.
The PMDA and the NSDA use the result of the SDA as attack seed. Hence, the
better estimations output by the SDA, the better results obtained by the PMDA
and the NSDA.
Even though Nusers has some eﬀect on the MSEp of the LSDA the results in
Fig. 3 reinforce the idea that the number of rounds ρ is the main component
of the error. When ρ = 10 000 rounds are observed the LSDA does not provide
better results than the other attacks. Nevertheless, as the number of rounds
increases, the LSDA outperforms the other attacks regardless of the growth of
the MSE with Nusers.

52
F. P´erez-Gonz´alez and C. Troncoso
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
1e
3
MSEp
50
100
150
200
250
300
350
400
450
500
Number of users (Nusers)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
1e
3
50
100
150
200
250
300
350
400
450
500
Number of users (Nusers)
SDA
PMDA
NSDA
LSDA
Theory
Fig. 3. MSEp evolution with the number of users in the system Nusers (f = 25, t = 10,
ρ = 10 000, 100 000, τf = 1) (left and right, respectively)
Performance with Respect to the Output Variance σ2
yj. The inﬂuence
on the LSDA’s MSE of the output variance σ2
yj can be studied by varying the
value of the parameters f and τf, while maintaining Nusers and t constant (see
Eq. (13)). We ﬁrst vary the number of friends of the senders f while keeping
fj = f for all receivers j, ensuring that τf = 1. We observe in Fig. 4, left, that
the LSDA’s MSEp closely follows the prediction in formula (14).
In a second experiment, we ﬁx the parameter f vary τf to represent diﬀerent
degrees of “hubness” in the population. We construct populations such in which
there are α = 0, · · · , f hub receivers that have Nusers friends, while the remaining
Nusers −α receivers are assigned small amounts of friends in order to obtain
diﬀerent τf arbitrarily chosen between τf,1 = 1 and τf,2 = Nusers/f. The result
is shown in Fig. 4, right. It is worthy to note that the SDA signiﬁcantly beneﬁts
from the hubness of the population. As some users concentrate the traﬃc, and
the sending proﬁles become more uniform all users tend to send their messages
to the same set of receivers. In this scenario the strategy of the SDA, that assigns
equal probability to every receiver in a mix batch, closely models reality and the
error tends to zero. While the error of the SDA is very small, the estimated
proﬁles still have small biases toward some users. This eﬀect is ampliﬁed by the
NSDA and PMDA, signiﬁcantly increasing their estimation error.
Performance with Respect to the User Behaviour. Our experiments so
far considered a very simplistic population in which users choose amongst their
friends uniformly at random (which we denote as SDA). As it has been discussed
in the past [8, 21] this population is unlikely to represent real users. We now
evaluate the four attacks against two more realistic populations in which users
choose the recipients according to an arbitrary multinomial distribution, more
(SKW) or less (ARB) skewed depending on the experiment.
We show in Fig. 5 (left) box plots representing the distribution of the MSE per
sender proﬁle MSEqi for all users in the population. We also plot the MSEp for

Understanding Statistical Disclosure: A Least Squares Approach
53
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1e
3
MSEp
5
10
15
20
25
30
35
40
45
50
Number of friends (f)
SDA
PMDA
NSDA
LSDA
Theory
0.00
0.05
0.10
0.15
0.20
0.25
0.30
1e
3
1.0
1.52
2.06
2.61
3.26
4.0
τf
Fig. 4. MSEp evolution with the number of friends f (N = 100, f = 25, ρ = 10 000,
τf=1), and with τf (N = 100, f = 25, t = 10, ρ = 10 000) (left and right, respectively)
each attack in the ﬁgure, representing it with ⋆(note that the MSEp is also the
mean of MSEqi for all i). We recall that, as the PMDA and NSDA, the LSDA
makes no assumptions on the users’ proﬁles, while the SDA assumes uniform
behavior. Hence, as expected when the proﬁles become increasingly skewed the
SDA performs the worst, obtaining the LSDA the smallest MSEp. Furthermore,
it is worthy to notice that the user behaviour has a strong inﬂuence on the
variance of the MSEqi. The fact that users have favorite friends who receive a
large fraction of their messages makes the probability of these receivers easy to
estimate, while for receivers that are not often chosen the attacks’ estimations
are poor. This explains the large variance in the SKW population with respect
to the other population types.
Comparison between Attack Principles. Throughout the evaluation sec-
tion we have considered four disclosure attacks that estimate users proﬁles using
statistics and optimization techniques. We now compare these attacks to Vida,
the Bayesian inference-based machine learning algorithm proposed by Danezis
and Troncoso in [8]. We can see in Fig. 5 (right), which shows box plots repre-
senting the distribution of the MSEqi for all users under observation, that Vida
outperforms the statistical variants. In order to simplify the ﬁgure, we have not
plotted the the MSEp, that lies extremely close to the median in all cases.
We have already discussed that the LSDA obtains an advantage over the SDA,
PMDA, and NSDA by considering all observed rounds simultaneously, but does
not account for the one-to-one relationship between send and received messages
in the individual rounds of mixing. Vida, on the other hand, not only considers
all rounds, but searches for perfect matchings in each round improving the pro-
ﬁle estimation considerably. These results seemingly contradict the performance
evaluation in [8]. This is because the comparison performed by Danezis and
Troncoso was with respect to the message de-anonymization success rate, while
we focus on the estimation of proﬁles. In fact, the results reported by Danezis

54
F. P´erez-Gonz´alez and C. Troncoso
0.0
0.2
0.4
0.6
0.8
1.0
1e
2
MSEqi
S
N
P
L
S
N
P
L
S
N
P
L
SDA population
ARB population
SKW population
0.000
0.005
0.010
0.015
0.020
0.025
0.030
1e
2
SDA
NSDA
PMDA
LSDA
Vida
Fig. 5. MSEqi evolution with respect to the population type for all attacks (left) and
only comparison between attack principles (right) (N = 100, f = 25, t = 10, ρ = 10 000,
τf = 1). (We represent MSEp with a ⋆.)
and Troncoso show that when 512 rounds of mixing are observed the proﬁling
accuracy of the algorithm is excellent.
While the eﬀectiveness of Vida is desirable, it comes at a high computational
cost because each iteration of the algorithm requires ﬁnding a perfect matching
in all the ρ rounds observed. We note however that, as in [8], we have used
the SDA’s result as seed for the machine learning engine. Intrestingly, using the
LSDA, which yields better estimation of the real proﬁles than the the SDA,
instead may signiﬁcantly speed up the learning time.
6
Discussion
We have shown that the LSDA is more eﬀective than its statistical predeces-
sors. Further, the matrix operations performed by the LSDA have much smaller
computational requirements than the round-by-round processing carried out by
the PMDA or the NSDA. This decrease in computation comes at the cost of
memory: the LSDA operates with big matrices that have to be loaded to the
RAM. The parameters we have used in this paper generated matrices that ﬁt-
ted comfortably in a commodity computer, but larger mix networks may need
extra memory. When memory is an issue a gradient-based approach can be used
to iteratively process the rounds obtaining the same result while reducing the
computational requirements of the attack, that would deal with smaller ma-
trices. This iterative approach can be further adapted to account for temporal
changes in the proﬁles. Extending the LSDA to accommodate such evolution is
a promising line of future work.
The fact that we have considered user proﬁling as an unconstrained problem
(see Eq. (2)) resulted in some of the probabilities ˆpj,i estimated by the LSDA
being negative, corresponding to receivers j that are not friends of user i. When
pj,i = 0 the algorithm returns ˆpj,i that lie near zero, but as the solution is

Understanding Statistical Disclosure: A Least Squares Approach
55
unconstrained it is not guaranteed that ˆpj,i ≥0. One could reduce the error
by just setting those probabilities to zero, disregarding that 
j pj,i = 1 for
all i. Alternatively, it is possible to establish constraints on Eq. (2) to ensure
that the proﬁles recovered by the LSDA are well-deﬁned. However, enforcing
such constraints will no longer guarantee the decoupling of the unnormalized
receiver proﬁles, and hence the solution is likely to be quite cumbersome. The
development and analysis of such solution is left as subject for future research.
Threshold mixes are well ﬁtted to analyse in theory, however deployed systems
use pool mixes, which oﬀer better anonymity. Up to know only the SDA has been
adapted to traﬃc analysis of anonymous communications carried out through
a pool mix [16]. This is because the internal mechanism of this mix, that may
delay messages for more than one round, hinders the construction of a bipartite
graph between senders and receivers. Hence, adapting the PMDA, the NSDA,
or Vida to such scenario is non-trivial. The independence of the LSDA from the
mix threshold makes it an ideal candidate for the analysis of pool mixes. In order
to adapt the attack to this mix it is necessary to estimate the matrices E{UT U}
and E{UT yj}, for all j.
Finally, in some cases it might be possible that some of the transition prob-
abilities are known. It is possible to modify the machine learning approach [8]
to account for this extra knowledge, but this is non-trivial for the SDA, PMDA
or NSDA. The Least Squares formulation can be easily adapted to consider this
additional information. Without loss of generality let us assume that p1,1 is
known. As this corresponds to the ﬁrst element of p, one can work instead with
an equivalent problem in which we remove the ﬁrst column of H and p1,1 from p;
consequently, the observation vector y is replaced by y −p1,1h1. This procedure
can be repeated for every known transition probability. Similar considerations
can be made for the case where the transition probabilities pj,i depend on a
smaller set of parameters (e.g., when some of the probabilities are known to be
identical).
7
Conclusion
Since Kesdogan and Agrawal [1,12] introduced the Disclosure Attack to proﬁle
users sending messages through an anonymous network, a stream of eﬃcient
statistical variants have been proposed [3, 5, 7, 8, 14, 15, 21]. Nevertheless, their
heuristic nature hinders the search for analytical formulae describing the depen-
dence of their success on the system parameters, which is diﬃcult to characterize
and predict as we have shown in our results.
We have introduced the LSDA, a new approach to Disclosure based on solving
a Least Square problem, that minimizes the mean squared error between the
estimated and real proﬁles. Further, the LSDA is the ﬁrst disclosure attack able
to simultaneously estimate sender and receiver proﬁles. The main advantage of
our approach is that it allows the analyst to predict the proﬁling error given the
system parameters. This capability is essential at the time of designing high-
latency anonymous communication systems, as it permits the designer to choose

56
F. P´erez-Gonz´alez and C. Troncoso
the system parameters that provide a desired level of protection depending on the
population characteristics without the need to perform simulations, which may
require a large computational eﬀort as in the case of Vida. We have empirically
evaluated the LSDA and we have proved that our formulae closely model its
error.
Acknowledgements. Research supported by the European Regional Devel-
opment Fund (ERDF); by the Galician Regional Government under projects
Consolidation of Research Units 2010/85 and SCALLOPS (10PXIB322231PR);
by the Spanish Government under project COMONSENS (CONSOLIDER-
INGENIO 2010 CSD2008-00010); by the Iberdrola Foundation through the
Prince of Asturias Endowed Chair in Information Science and Related Tech-
nologies; by the Concerted Research Action (GOA) Ambiorics 2005/11 of the
Flemish Government; and by the IAP Programme P6/26 BCRYPT. C. Troncoso
is a research assistant of the Flemish Fund for Scientiﬁc Research (FWO). The
authors thank G. Danezis and C. Diaz for their comments on earlier versions of
the manuscript.
References
1. Agrawal, D., Kesdogan, D.: Measuring anonymity: The disclosure attack. IEEE
Security & Privacy 1(6), 27–34 (2003)
2. Chaum, D.: Untraceable electronic mail, return addresses, and digital pseudonyms.
Communications of the ACM 24(2), 84–90 (1981)
3. Danezis, G.: Statistical disclosure attacks: Traﬃc conﬁrmation in open environ-
ments. In: Gritzalis, Vimercati, Samarati, Katsikas (eds.) Proceedings of Security
and Privacy in the Age of Uncertainty (SEC 2003), Athens. IFIP TC11, pp. 421–
426. Kluwer (May 2003)
4. Danezis, G., Diaz, C., Syverson, P.: Systems for anonymous communication. In:
Rosenberg, B. (ed.) Handbook of Financial Cryptography and Security. Cryptog-
raphy and Network Security Series, pp. 341–389. Chapman & Hall/CRC (2009)
5. Danezis, G., Diaz, C., Troncoso, C.: Two-Sided Statistical Disclosure Attack. In:
Borisov, N., Golle, P. (eds.) PET 2007. LNCS, vol. 4776, pp. 30–44. Springer,
Heidelberg (2007)
6. Danezis, G., Dingledine, R., Mathewson, N.: Mixminion: Design of a Type III
Anonymous Remailer Protocol. In: IEEE Symposium on Security and Privacy
(S&P 2003), pp. 2–15. IEEE Computer Society (2003)
7. Danezis, G., Serjantov, A.: Statistical Disclosure or Intersection Attacks on
Anonymity Systems. In: Fridrich, J. (ed.) IH 2004. LNCS, vol. 3200, pp. 293–308.
Springer, Heidelberg (2004)
8. Danezis, G., Troncoso, C.: Vida: How to Use Bayesian Inference to De-anonymize
Persistent Communications. In: Goldberg, I., Atallah, M.J. (eds.) PETS 2009.
LNCS, vol. 5672, pp. 56–72. Springer, Heidelberg (2009)
9. Edman, M., Yener, B.: On anonymity in an electronic society: A survey of anony-
mous communication systems. ACM Computing Surveys 42(1) (2010)
10. Kesdogan, D., Agrawal, D., Penz, S.: Limits of Anonymity in Open Environments.
In: Petitcolas, F.A.P. (ed.) IH 2002. LNCS, vol. 2578, pp. 53–69. Springer, Heidel-
berg (2003)

Understanding Statistical Disclosure: A Least Squares Approach
57
11. Kesdogan, D., M¨olle, D., Richter, S., Rossmanith, P.: Breaking Anonymity by
Learning a Unique Minimum Hitting Set. In: Frid, A., Morozov, A., Rybalchenko,
A., Wagner, K.W. (eds.) CSR 2009. LNCS, vol. 5675, pp. 299–309. Springer, Hei-
delberg (2009)
12. Kesdogan, D., Pimenidis, L.: The Hitting Set Attack on Anonymity Protocols.
In: Fridrich, J. (ed.) IH 2004. LNCS, vol. 3200, pp. 326–339. Springer, Heidelberg
(2004)
13. Liu, J., Xu, H., Xie, C.: A new statistical hitting set attack on anonymity protocols.
In: Computational Intelligence and Security, International Conference (CIS 2007),
pp. 922–925. IEEE Computer Society (2007)
14. Mallesh, N., Wright, M.: The Reverse Statistical Disclosure Attack. In: B¨ohme,
R., Fong, P.W.L., Safavi-Naini, R. (eds.) IH 2010. LNCS, vol. 6387, pp. 221–234.
Springer, Heidelberg (2010)
15. Mathewson, N., Dingledine, R.: Practical Traﬃc Analysis: Extending and Resist-
ing Statistical Disclosure. In: Martin, D., Serjantov, A. (eds.) PET 2004. LNCS,
vol. 3424, pp. 17–34. Springer, Heidelberg (2005)
16. M¨oller, U., Cottrell, L., Palfrader, P., Sassaman, L.: Mixmaster Protocol — Version
2. IETF Internet Draft (July 2003)
17. P´erez-Gonz´alez, F., Troncoso, C.: Understanding statistical disclosure: A least
squares approach. IEEE Transactions on Information Forensics and Security (under
submission, 2012)
18. Pham, D.V., Wright, J., Kesdogan, D.: A Practical Complexity-Theoretic Analysis
of Mix Systems. In: Atluri, V., Diaz, C. (eds.) ESORICS 2011. LNCS, vol. 6879,
pp. 508–527. Springer, Heidelberg (2011)
19. Raymond, J.-F.: Traﬃc Analysis: Protocols, Attacks, Design Issues, and Open
Problems. In: Federrath, H. (ed.) Anonymity 2000. LNCS, vol. 2009, pp. 10–29.
Springer, Heidelberg (2001)
20. Serjantov, A., Danezis, G.: Towards
an Information
Theoretic
Metric
for
Anonymity. In: Dingledine, R., Syverson, P.F. (eds.) PET 2002. LNCS, vol. 2482,
pp. 41–53. Springer, Heidelberg (2003)
21. Troncoso, C., Gierlichs, B., Preneel, B., Verbauwhede, I.: Perfect Matching Dis-
closure Attacks. In: Borisov, N., Goldberg, I. (eds.) PETS 2008. LNCS, vol. 5134,
pp. 2–23. Springer, Heidelberg (2008)

Website Detection Using Remote Trafﬁc Analysis
Xun Gong1, Nikita Borisov1, Negar Kiyavash2, and Nabil Schear3
1 Department of Electrical and Computer Engineering, UIUC
2 Department of Industrial and Enterprise Systems Engineering, UIUC
3 Department of Computer Science, UIUC
{xungong1,kiyavash,nikita,nschear2g}@illinois.edu
Abstract. Recent work in trafﬁc analysis has shown that trafﬁc patterns leaked
through side channels can be used to recover important semantic information.
For instance, attackers can ﬁnd out which website, or which page on a website,
a user is accessing simply by monitoring the packet size distribution. We show
that trafﬁc analysis is even a greater threat to privacy than previously thought by
introducing a new attack that can be carried out remotely. In particular, we show
that, to perform trafﬁc analysis, adversaries do not need to directly observe the
trafﬁc patterns. Instead, they can gain sufﬁcient information by sending probes
from a far-off vantage point that exploits a queuing side channel in routers.
To demonstrate the threat of such remote trafﬁc analysis, we study a remote
website detection attack that works against home broadband users. Because the
remotely observed trafﬁc patterns are more noisy than those obtained using pre-
vious schemes based on direct local trafﬁc monitoring, we take a dynamic time
warping (DTW) based approach to detecting ﬁngerprints from the same website.
As a new twist on website ﬁngerprinting, we consider a website detection at-
tack, where the attacker aims to ﬁnd out whether a user browses a particular web
site, and its privacy implications. We show experimentally that, although the suc-
cess of the attack is highly variable, depending on the target site, for some sites
very low error rates. We also show how such website detection can be used to
deanonymize message board users.
1
Introduction
Trafﬁc analysis is the practice of inferring sensitive information from patterns of com-
munication. Recent research has shown that trafﬁc analysis applied to network commu-
nications can be used to compromise users’ secrecy and privacy. By using packet sizes,
timings, and counts, it is possible to ﬁngerprint websites visited over an encrypted tun-
nel [2,4,11,17], infer keystrokes sent over a secure interactive connection [27,34] and
even detect phrases in VoIP sessions [31–33]. These attacks have been explored in the
context of a local adversary who can observe the target trafﬁc directly on a shared
network link or can monitor a wireless network from a nearby vantage point [25].
We consider an alternate trafﬁc analysis approach that is available to remote adver-
saries. We notice that it is possible to infer the state of a router’s queue through the
observed queueing delay of a probe packet. By sending frequent probes, the attacker
can measure the dynamics of the queue and thus learn an approximation of the sizes,
timings, and counts of packets arriving at the router. In the case of home broadband
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 58–78, 2012.
c⃝Springer-Verlag Berlin Heidelberg 2012

Website Detection Using Remote Trafﬁc Analysis
59
networks, in particular, DSL lines, the attacker can send probe packets from a geo-
graphically distant vantage point, located as far away as another country; the large gap
between the bandwidth of the DSL line and the rest of the Internet path makes it possible
to isolate the queueing delay of the “last-mile” hop from that experienced elsewhere.
To demonstrate the feasibility of using remote trafﬁc analysis for real malicious at-
tack to learn sensitive information, we adapt the website ﬁngerprinting attack [2,11,17],
previously targeted at local victims, to a new scenario and introduce a remote website
detection attack. Our attack can ﬁnd out when a victim user under observation visits
a particular target site without directly monitoring the user’s trafﬁc. This would allow,
for example, a company to ﬁnd out when its employees visit its competitors’ sites from
their home computers or deanonymize users of web boards.
In our adaptation, we encountered two challenges: the information obtained through
remote trafﬁc analysis is more noisy than in the local case, and there is no easily avail-
able training set from which to create a ﬁngerprint. To address the former problem, we
improved on the previous inference methodology, which used the distribution of packet
sizes and inter-arrival times, and developed a ﬁngerprint detection technique that makes
use of ordered packet size sequences and the dynamic time warping (DTW) distance
metric. To create a training set, we designed a testbed that uses an emulated DSL link
and a virtual execution environment to replicate the victim’s home environment.
To evaluate our work, we sent probes to a home DSL line in the United States from a
rented server in a data center near Montreal, Canada; we chose this set up to demonstrate
the low cost and barrier to entry to conduct the attack. We then compared the probe
results with proﬁles of website fetches generated in a virtual testbed at our university.
We tested our attack on detecting each of a list of 1 000 popular websites. We found that
detection performance was highly variable; however, for a signiﬁcant fraction of sites, it
was possible to obtain very low false-positive rates without incurring signiﬁcant false-
negative rates. We also found that there is some accuracy loss due to the discrepancies
in the test and training environments (distant from each other) that we were not (yet)
able to eliminate. If the training and test data are both collected from the same location,
a much larger fraction of sites can be accurately detected with low error rates. We
ﬁnd that despite working with a much noisier information source than previous web
ﬁngerprinting work [2, 11, 17], our website detection attack nevertheless shows that
remote trafﬁc analysis is a serious threat to Internet privacy.
The rest of the paper is organized as follows. We describe our approach to remote
trafﬁc analysis in §2. In §3 we describe our adaptation of previous website ﬁngerprint-
ing attack to remotely conﬁrming user’s browsing activities. We evaluate our website
detection attack in §4. We then discuss further extensions and the limitations of our
technique in §5 and present related work in §6, concluding in §7.
2
Remote Trafﬁc Analysis
Trafﬁc analysis attacks have been known to be effective for quite some time. And yet,
for most Internet users, they represent a minor concern at best. Although a dedicated
attacker could always intercept trafﬁc by, say, bribing a rogue ISP employee, or tapping
a switch box, he would run the risk of being caught and potentially incurring criminal

60
X. Gong et al.
Sometown, USA
Montreal, Canada
Internet
ISP B
Website
Host
Web  Trafﬁc
Bob
queue
DSLAM
Alice's House
Alice
Broadband
router
Alice
Ping Probes
DSL Line
ISP A
Fig. 1. Queueing side channel. Bob remotely sends probes to Alice’s router to infer her activities.
charges. In any case, this level of effort seems justiﬁed only for highly sensitive mate-
rial, rather than casual snooping; therefore, as long as sensitive data are protected by
encryption or other techniques, a user may feel relatively safe.
We show, however, that trafﬁc analysis can be carried out at a signiﬁcantly lower
cost, and by attackers who never come into physical proximity with the user. In fact,
the attackers can launch their attacks from another state or country, as long as they have
access to a well-provisioned Internet connection. This, in turn, is very easy to obtain due
to the highly-competitive Internet hosting business sector: a virtual private server in a
data center can cost as little as a few dollars a month.1 We show that the attacker’s trafﬁc
has very low rate, thus attackers do not need to incur high bandwidth costs. Furthermore,
users who are being spied upon are unlikely to notice the small amount of performance
overhead. Thus, anyone with a credit card2 can carry out the attack and leave little trace.
In this section, we describe our approach to remote trafﬁc analysis. We ﬁrst introduce
the queueing side channel, which is the basis of the attack. Then we design an algorithm
to recover users’ trafﬁc patterns from the information leaked through this side channel.
2.1
Queuing Side Channel
We consider the following scenario as depicted in Figure 1. Alice is a home user at
Sometown USA, browsing a website via her DSL Internet connection. Her computer
is connected to a broadband router, using a wireless or wired LAN connection.3 The
router is connected via a DSL line to a DSLAM4 or similar device operated by her ISP,
which is then (eventually) connected to the Internet. Unbeknownst to Alice, Bob, who
is located in another state, or another country wishes to attack Alice’s privacy. If Bob
1 See, for example, www.vpslink.com (retrieved February 2011).
2 Working stolen credit cards are an easily acquired commodity on the black market [10].
3 In some cases, Alice’s computer might be connected to the DSL line directly.
4 DSL access multiplexer.

Website Detection Using Remote Trafﬁc Analysis
61
0
1
2
3
4
5
6
7
0
2000
4000
6000
8000
10000
12000
Traffic (byte)
Time (s)
(a) Alice’s trafﬁc pattern
3
4
5
6
7
8
9
40
50
60
70
80
90
100
Time (s)
RTT(ms)
(b) RTTs measured by Bob
3
4
5
6
7
8
9
5
10
15
20
25
30
35
Time (s)
Recovered Traffic Pattern 
(ms)
 
 
(c) Recovered trafﬁc pattern
Fig. 2. Real trafﬁc on a DSL vs. probe RTTs. Alice resides in Champaign, IL, while Bob is located
in Montereal, Canada.
knows Alice’s IP address (for example, if Alice visited a site hosted by Bob), he can use
his computer to send a series of ICMP echo requests (pings) to the router in Alice’s house
and monitor the responses to compute the round-trip times (RTTs). One component of
the RTTs is the queueing delay that the packets experience at the DSLAM prior to being
transmitted over the DSL line; thus the RTTs leak information about the DSLAM queue
sizes. This leakage in turn reveals trafﬁc patterns pertaining to Alice’s activities.
Since the probe packets traverse many Internet links, and the queuing delays on Al-
ice’s DSL link are but one component of the RTT, the question is, how much informa-
tion is leaked by this side channel? Furthermore, can it be used to infer any information
about Alice’s activities? To evaluate the potential of this attack, we carried out a test
on a home DSL link located in Champaign, IL, USA. In the test, Alice opens a Web
page www.yahoo.com on her computer. Simultaneously, Bob in Montreal, QC, Canada
sends a ping request every 10 ms to Alice’s home router. Figure 2(a) depicts the trafﬁc
pattern of Alice’s download trafﬁc. The height of each peak in the ﬁgure represents the
total size of packets that are downloaded during each 10 ms interval. Figure 2(b) plots
the RTTs of Bob’s ping requests. We can see a visual correlation between the trafﬁc pat-
tern and observed RTTs; whenever there is a large peak in the user’s trafﬁc, the attacker
observes a correspondingly large RTT.
The correlation between Alice’s trafﬁc and Bob’s observed probe RTTs can be ex-
plained as follows. The RTTs include both the queuing delay incurred on the DSL
link and delays on intermediate routers, which sit between Bob’s computer and Alice’s
router. The intermediate routers are typically well provisioned and are unlikely to ex-
perience congestions [1, 16]; furthermore, the intermediate links have high bandwidth
and thus queueing delays will be small in all cases. We validate this using our own
measurements in §4.1.
On the other hand, Alice’s DSL link is, by far, the slowest link that both her trafﬁc
and Bob’s probes are likely to traverse. The queue at Alice’s router can grow to be quite

62
X. Gong et al.
Arrival 
Process
a1
a2
a3
a4
d1
d2
d3
d4
Departure 
Process
Time
P1
P2
P3
P4
P1
P2
P3
P4
Fig. 3. FIFO queuing in the DSL router
long (in relative terms), due to TCP behaviors, which cause the www.yahoo.com
server to send a batch of TCP packets at a fast rate. As most routers schedule packets in
a First In First Out (FIFO) manner, this congestion will lead to large queuing delays of
Bob’s ping packets. We saw that the additional delay caused by Alice’s incoming trafﬁc
could be as high as over 100 ms. Thus, Alice’s trafﬁc patterns are clearly visible in the
RTTs seen by Bob.
2.2
Trafﬁc Pattern Recovery Algorithm
We now show how the attacker can analyze the information leaked through this queue-
ing side channel. We model the incoming DSL link as a FIFO queue. As most trafﬁc
volume in an HTTP session occurs on the download side, we will ignore the queuing
behavior on the outgoing DSL link, though it could be modeled in a similar fashion.
Figure 3 depicts the arrival and departure process in this queuing system. The arrows
are Bob’s ping packets, denoted by Pi’s, and the blocks represent HTTP packets down-
loaded by Alice. The DSLAM serves packets in FIFO manner and at a constant service
rate; i.e., the service time is proportional to the packet size. As most HTTP packets are
more than an order of magnitude larger than ping packets, we ignore the service time
for pings.
Assume that ping packet Pi arrives in the queue at time ai, waits for the router to
serve all the packets currently in the router, and then departs at time di. Let us consider
the observed RTT of the ping packet Pi; we can represent it as:
RT Ti =

l∈links on path
ql
i + pl
i + tl
i
(1)
where ql
i, pl
i, and tl
i are the queueing, propagation, and transmission delays incurred
by packet Pi on link l. Note that the propagation and transmission delays are mostly
constant, and in fact we can approximate:

l∈links on path
pl
i + tl
i ≈min
j
RT Tj
(2)
since Bob is likely to experience near-zero queueing delays for some of the pings. Fur-
thermore, as argued in §2.1, the queueing delay on links other than the DSL line are
going to be minimal, thus we can further approximate:
RT Ti ≈min
j
RT Tj + (di −ai)
(3)

Website Detection Using Remote Trafﬁc Analysis
63
Algorithm 1. Trafﬁc pattern recovery algorithm
1: let d0 = 0
2: for i = 1 to the probe sequence length do
3:
# reconstruct the arrival and departure times using the ping interval
4:
ai = tping · i
5:
di = RT Ti −RT Tmin + ai
6:
# estimate the total size of packets arriving in [ai−1, ai]
7:
si = di −max(di−1, ai)
8:
# discard noise
9:
if si < η then
10:
si = 0
11:
end if
12: end for
Making use of the queuing delay di −ai from (3), the attacker Bob can further infer
the total size of HTTP packets arriving during the interval [ai−1, ai]’s, which produces
a similar pattern as Alice’s trafﬁc in Figure 2(a). For this purpose, two cases need to be
considered.
1. ai ≥di−1. In this case, when Pi enters the queue, the DSLAM is either idle or
serving packets destined for Alice. The delay di −ai reﬂects the time required to
ﬁnish serving the HTTP packets currently in the buffer, and is thus approximately
proportional to the total size of Alice’s arrivals during the interval [ai−1, ai]. P2 in
Figure 3 is one example of this case.
2. ai < di−1. In this case, Pi−1 is still in the queue when Pi arrives. Only after Pi−1
departs at di−1, the router can start to serve packets that arrived in the interval
[ai−1, ai]. Thus the delay di −di−1 is the service time for those packets and can be
used to recover the total size. P4 in Figure 3 is one example of this case.
Algorithm 1 summarizes the trafﬁc pattern recovery procedure based on these obser-
vations. To account for minor queueing delays experienced on other links, we deﬁne
a threshold η such that RTT variations smaller than η are considered noise and do not
correspond to any packet arrival at the DSLAM. Figure 2(c) plots the pattern extracted
from RTTs in Figure 2(b). After processing, the resulting time series proportionally
approximate the packet size sequence of the original trafﬁc in Figure 2(a). As will be
shown in the next section, it can be applied to infer more information about Alice’s
activities, e.g., website ﬁngerprinting.
Note that in case 1, the attacker may underestimate the size of the HTTP packets
arriving in the period [ai−1, ai] because a portion of them will have already been served
by time ai. The error depends both on the frequency of the probes and the bandwidth of
the DSL link. Since most HTTP packets are of maximal size (MTU), we can ensure that
all such packets are observed by setting the ping period to be less than:
MTU
DSL bandwidth.Thus
the adversary must tune the probe rate based on the DSL bandwidth and faster links will
require a higher bandwidth overhead (but the pings will form a constant, small fraction
of the overall DSL bandwidth.)

64
X. Gong et al.
3
Website Fingerprinting
Previous work on trafﬁc analysis has shown that it is often possible to identify the
website that someone is visiting based on trafﬁc timings and packet sizes [2, 11, 17],
namely, website ﬁngerprinting. We consider whether it is possible to carry out a similar
attack using our remote trafﬁc analysis. We ﬁrst review the three basic steps in previous
work when conducting a website ﬁngerprinting attack.
1. First, the attacker decides some feature of web trafﬁc used to distinguish websites.
The feature needs to stay relatively stable for accesses to the same single website,
but has signiﬁcant diversity across different sites. For example, Herrmann et al. use
the size distribution of HTTP packets [11].
2. The next step is the training procedure. The attacker needs a training data set of
ﬁngerprint samples labeled with corresponding destination websites. Usually, these
feature proﬁles are obtained by the attacker browsing websites himself/herself from
the same (or similar) network connection as the user.
3. In the ﬁnal step, the attacker tests his/her knowledge from training on the victim
user. He/She monitors trafﬁc going to the user and matches extracted features with
the proﬁles in his/her database. The one with most similarity is chosen as the web-
site browsed by the user.
As compared with previous work, using our remote trafﬁc analysis technique for iden-
tifying websites introduces two additional challenges. First, previous work used ﬁne-
grained information like exact packet size distributions to create features, whereas in our
setting this information is not available directly, since the queueing side channel pro-
duces only approximate sums of packet sizes. Second, previous work created a training
set from the same vantage point that was then used for ﬁngerprinting tests. An attacker
performing remote trafﬁc analysis must, of course, use a different environment for col-
lecting the training set, potentially affecting the measured features. We describe our
approaches to solving these two challenges next.
3.1
Time Series–Based Feature
Since it is hard to infer information about each single packet from our recovered pat-
tern time series, we use the entire time series, which contains the estimated size of all
HTTP packets downloaded during each probe period, to create one ﬁngerprint trace.
Identiﬁcation of websites is based on the similarity between the observed ﬁngerprints
and samples in the training set.
The challenge is to ﬁnd a meaningful distance metric between ﬁngerprint traces.
Note that pointwise comparisons will produce poor results. This is because parts of
the ﬁngerprint may be impacted by the noise from a small queueing delay on a core
Internet link. Additionally, the ﬁngerprint could miss some packets contained in the
original trafﬁc due to pattern recovery errors. Finally, even ﬁngerprints of the same
website are not strictly synchronized in time due to the inter-packet delay variations.
To deal with these issues, we turn to the Dynamic Time Warping (DTW) distance [24].
DTW was developed for use in speech processing to account for the fact that when

Website Detection Using Remote Trafﬁc Analysis
65
A
xI
x1
B
y1
yJ
z(3) = (2, 3)
z(K) = (I, J)
x2
...
y2
. . .
Fig. 4. Warping function in DTW
people speak, they pronounce various features of the phonemes at different speeds, and
do not always enunciate all of the features. DTW attempts to ﬁnd the best alignment
of two time series by creating a non-linear time warp between the sequences. Figure 4
visualizes the DTW-based distance between two time series: X = {x1, x2 . . . , xI} and
Y = {y1, y2 . . . , yJ}. Let function F(z) = {z(1), . . . , z(K)} be a mapping from se-
ries X to series Y where z(k) = (x(i), y(j)). For every pair of matched points based
on the mapping, we deﬁne the distance as d(z(k)) = d(i, j) = |xi −yj|. The ﬁnal dis-
tance between the X and Y can then be deﬁned as a weighted and normalized sum over
all matched point pairs as D(X, Y ) = minF
 K
k=1 d(z(k))w(k)
K
k=1 w(k)

. The weights w(k)’s
are ﬂexible parameters picked based on the speciﬁc application scenario. Applying dy-
namic programming, one can ﬁnd the warping function with minimum distance, which
captures the similarity between the two time series under best matched alignment.
In our attack, we apply DTW-based distance to account for the estimation errors and
time desynchronizations in ﬁngerprints. Based on the distances with the training data
set, the attacker will know if a test sample indicates the activity that the user browsed
the website of interest.
3.2
Training Environment
To obtain an accurate training ﬁngerprint for a particular user’s trafﬁc, the attacker must
be able to replicate the network conditions on that user’s home network. The approach
we use is to set up a virtual machine running a browser that is connected to the Internet
via a virtual Dummynet link [23]. The virtual machine is then scripted to fetch a set of
web pages of interest; at the same time, an outside probe is sent across the Dummynet
link, simulating the attack conditions on a real DSL link.
A number of parameters of the link need to be carefully decided. We found that the
most important parameter for the attacker to replicate was the link bandwidth. First, as
discussed in §2.2, the probe frequency should be adjusted based on the link bandwidth.
Bandwidth also affects the magnitude of observed queuing delays. Additionally, it can
signiﬁcantly alter the trafﬁc pattern itself, as TCP congestion control mechanisms are
affected by the available bandwidth. Fortunately, estimating the bandwidth on a link
is a well-studied problem [20, 22, 28]. In our tests, we use a packet-train technique by
sending a burst of probe packets and measuring the rate at which responses are returned.

66
X. Gong et al.
Since most DSL lines have asymmetric bandwidth, we used TCP ACK packets with
1000 data bytes to measure the download bandwidth on the link. The target would send
a short TCP reset packet for each ACK that it received, with the spacing between resets
indicating the downstream bandwidth; we found this method to be fairly accurate.
The round-trip time between the home router and the website hosts also affects the
ﬁngerprint. When opening a webpage, the browser can download objects from several
host servers. The trafﬁc pattern is the sum of all download connections, hence the shape
of observed ﬁngerprint does depend on the RTTs to these servers. However, we did not
explicitly model this parameter considering the difﬁculty to accurately tune up the link
delays to multiple destinations. The effects to the attack will be further discussed in §4.
The ﬁngerprint may be affected by the choice of browsers and operating systems as
well; for best results, the training environment should model the target as closely as
possible. Information about browser and operating system versions can be easily ob-
tained if the target can be convinced to visit a website run by the attacker; additionally,
ﬁngerprinting techniques in [18] may be used to recover some of this information.
3.3
Attack Scenarios
We consider several attack scenarios that make use of website ﬁngerprinting. We can
ﬁrst consider the classic website ﬁngerprinting scenario: Bob obtains traces from Al-
ice’s computer by sending probes to her DSL router and compares them to ﬁngerprints
of websites that he has generated, in order to learn about her browsing habits. Note that
this can be seen as a classiﬁcation task: each web request in Alice’s trace is classiﬁed
as belonging to a set of sites. This scenario has been used in most of the previous work
on website ﬁngerprinting, but it introduces the requirement that Bob must know the set
of potential sites that Alice may visit. Without some prior information about Alice’s
browsing habits, this potential set includes every site on the Internet, making it infea-
sible to generate a comprehensive set of ﬁngerprints. One could create ﬁngerprints for
popular sites only, but this reduces the accuracy of the classiﬁcation task [6,11,29]. For
example, the top 1 000 US sites, as tracked by Alexa, are responsible for only 56% of
all page views, therefore, even a perfect classiﬁer trained on the 1 000 sites would give
the wrong result nearly half the time.5
We therefore consider a different scenario, where Bob wants to detect whether Alice
visits a particular site. For example, if Bob is Alice’s employer, he may wish to check
to see if she is considering going to work for Bob’s competitor, Carol. To carry out this
attack, Bob would create a ﬁngerprint for Carol’s jobs site; he would then perform a
binary classiﬁcation task on Alice’s trafﬁc, trying to decide whether a trace represents a
visit to the target site or some other site on the Internet. As we will see, such binary clas-
siﬁcation can be performed with relatively high accuracy for some choices of sites. Note
that, as Alice’s employer, Bob has plenty of opportunities to learn information about Al-
ice’s home network, such as her IP address, browser and operating system versions, and
download bandwidth, by observing Alice when she connects to a password-protected
Intranet site, and can therefore use this information to create accurate training data for
building ﬁngerprints.
5 In fact, the situation is even worse, since Alexa counts all page views within a certain top-level
domain, whereas ﬁngerprints must be created on each individual URL.

Website Detection Using Remote Trafﬁc Analysis
67
0
5
10
15
20
25
30
35
40
RTT (ms)
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Fraction
Target
Previous Hop
(a) Empirical CDF of ping RTTs from a typ-
ical host.
0
10
20
30
40
50
60
RTT span between 95th-percentile and minimum (ms)
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Fraction
Target
Previous Hop
(b) Empirical CDF of 95th percentile minus
minimum RTTs.
Fig. 5. Measurement of DSL probe variances
As another example, Bob may be trying to identify an employee who makes posts
to a web message board critical of Bob.6 Bob can similarly build proﬁles, tailored for
each employee’s home computer, of the web board and perform remote trafﬁc analysis.
He can then correlate any detected matches to the times of the posts by the offending
pseudonym; note that this deanonymization attack is able to tolerate a signiﬁcant num-
ber of false-positive and false-negative errors by combining observations over many
days to improve conﬁdence [7].
4
Evaluation
We next present our results of website detection attack. First, through measurements of
DSL probe variances, we demonstrate the feasibility of the RTT based remote trafﬁc
analysis, which is the basis of our attack.
4.1
Measurement of DSL Probe Variance
To further conﬁrm that the ﬂuctuation of user’s RTTs are primarily determined by the
congestion at the DSL link, we conducted a small Internet measurement study. We
harvested IP addresses from access logs of servers run by the authors. We noted that
many DSL providers assign a DNS name containing “dsl” to customers. Using reverse
DNS lookups, we were able to locate 918 potential DSL hosts. To determine each host’s
suitability for measurement, we ﬁrst determine if it responds to ping requests. We then
use traceroute to locate the hop before the target DSL host (e.g., the DSLAM). Next,
we ensure this previous hop also responds to ping requests. Lastly, we measure the
minimum RTT of several hundred ping probes. We exclude any host with a minimum
RTT of greater than 100ms to bound the study to hosts in a wide geographical area
around our Montreal, Canada probe server. Using this method, we identiﬁed 189 DSL
6 This example is motivated by several actual cases of companies seeking to do this;
see
https://www.eff.org/cases/usa-technologies-v-stokklerk
and
https://www.eff.org/cases/first-cash-v-john-doe.

68
X. Gong et al.




	

	

	
	

	
	



(a) The DSL-Setup for test

	





	

	





	

	


	
(b) The VM-Setup for training
Fig. 6. Experimental setups for website detection
hosts to measure.7 The measurement consists of sending ping probes every 2 ms for 30
seconds to the target DSL host and then to its previous hop. We collected these traces
in a loop over a period of several hours.
We found that, on average, the target host RTT was ∼10 ms greater than the previous
hop. We frequently observed the pattern in Figure 5(a) where the previous hop RTT was
very stable and target RTT variations greater than 10 ms. We then measured the span
between the 95th percentile and the minimum observation in each sample. Figure 5(b)
shows the CDFs of this data for the each target DSL host and its previous hop from the
measurement set. We see that the previous hop RTT span shows more stability than the
end host, conﬁrming the one of the primary assumptions of our work.
4.2
Attack Setups and Data Collection
We built a DSL-Setup consisting of a target system and a ping server, as shown in
Figure 6(a). The target system captured the real environment of a home user. It ran on
a laptop, located in Champaign, IL, connected to DSL line with 3 Mbps download and
512 Kbps upload speeds. On the laptop, we used a shell script to automatically load
websites using Firefox 4.08. The ping server was a commercial hosting system, located
in Montreal, QC, Canada, acting as the remote attacker. It was scripted to send pings
at precise time intervals with hping9 and record ping traces with tcpdump10. We set the
ping interval to 2 ms.
To emulate the attacker’s training procedure, we also built a VM-Setup, a VMware
ESX host testbed located in our lab, as shown in Figure 6(b). On this machine, we ran
several VMware guest operating systems: a Ubuntu VM Client, a virtual router and a
host implementing a transparent Dummynet link. The Ubuntu VM Client acted as a
virtual target, and was scripted to browse websites using Firefox, similar to the real
home user. The virtual router provided NAT service for the client, and was connected
7 This number is underestimated as many of the IPs on our server log already became out-of-
date (the hosts hooked went ofﬂine) by the time of ping tests. More accurate number can be
obtained if a list of valid DSL IPs is provided in real time.
8 http://www.mozilla.com/firefox/
9 http://www.hping.org
10 http://www.tcpdump.org

Website Detection Using Remote Trafﬁc Analysis
69
to the Internet through the Dummynet link. The Dummynet bridge was conﬁgured to
replicate the network conditions of the target DSL link (i.e., the bandwidths). As in the
DSL-Setup, we sent probes from another host outside the constrained Dummynet link
to the virtual NAT router periodically. The attacker then collected training ﬁngerprints
while the virtual client was browsing websites through this virtual ‘DSL’ link. Note
the virtual router and ping host were connected to the same dedicated high-speed LAN
minimizing the impact of additional noise added by intermediate routers or network
congestion caused by other hosts.
We collected ﬁngerprints of the front pages for 1000 websites on the top list on
Alexa11. For websites which have multiple mirrors in different countries like
google.com, we only considered the site with the highest rank. We excluded websites
with extremely large loading time (greater than 60 s). For each website, we collected 12
ﬁngerprint samples from both the DSL and VM setups. The delay between collecting
two samples is half an hour. Following the same assumptions in previous papers [11,17],
the browsers were conﬁgured appropriately (no caching, no automatic update checks
and no unnecessary plugins). This makes our results comparable with previous work.
4.3
Website Detection
We ﬁrst analyze the ability of an attacker to detect whether a user visits a particular site.
To do so, the attacker checks whether the distance between the user trace and the target
web site is smaller than some threshold, and if so, the web site is considered detected.
This is a binary classiﬁcation task and its performance can be characterized by the
rates of false positives—a different website incorrectly identiﬁed as the target–and false
negatives–the target website not being identiﬁed. The choice of threshold ν creates a
tradeoff between the two rates: a smaller threshold will decrease false positives at the
expense of false negatives.
To estimate false-positive rate given a particular threshold ν, we ﬁx a target site w
and use the 12 samples T = {sw,1, . . . , sw,12} as the training set. We use the samples
from the other sites as a test set; i.e., U = {si,j} for i ̸= w, j ∈{1, . . . , 12}. Given a
sample si,j ∈U, we calculate the minimum distance from it to the training samples:
dw(si,j) = min
k D(si,j, sw,k)
(4)
where D(·, ·) is the DTW-based distance function deﬁned in §3.1. We then consider
every sample si,j ∈U such that dw(si,j) < ν to be a false positive and therefore
estimate the false-positive rate:
ˆpw = |{si,j ∈U|dw(si,j) < ν}|
|U|
(5)
To estimate the false-negative rate, we pick one of the sample sw,i and calculate its
minimum distance to the other 11 samples sw,j, j ̸= i, and count it as a false negative
if the distance is at least ν. We then repeat this process for each i = 1, . . . , 12:
ˆqw = |{sw,i|i ∈{1, . . . , 12}, minj̸=i d(sw,i, sw,j) ≥ν}|
12
(6)
11 http://www.alexa.com

70
X. Gong et al.
0
0.2
0.4
0.6
0.8
1
0
200
400
600
800
1000
Cumulative Bar Graph for FN with bounded FP
False Negative Rate
 
 
FP<0.5%
FP<1%
FP<5%
(a) VM–DSL case
0
0.2
0.4
0.6
0.8
1
0
200
400
600
800
1000
False Negative Rate
Cumulative Bar Graph for FN with bounded FP
 
 
FP<0.5%
FP<1%
FP<5%
(b) DSL–DSL case
Fig. 7. Number of sites with a given false negative rate or smaller
Given a target false positive rate of p∗
w, we can calculate the threshold ν∗
w that would
ensure pw < p∗
w. Note that because ˆ
pw is only an estimate of pw, we calculate a 95%
conﬁdence interval for pw and chose ν such that the upper limit of the CI is below
p∗
w
12. Note that this threshold will be different for each site. We can then estimate the
corresponding false negative rate ˆq∗
w that corresponds to ν∗
w.
The target false positive rate will largely depend on the prior knowledge the attacker
has. Typically, we will want to aim for a small false-positive rate, since even if Bob
considers it likely that Alice does in fact visit the target site w at some point, most of
the web browsing in any trace will still be to other sites; thus a low false-positive rate
is needed for the test to have high positive predictive value. On the other hand, Bob
can easily tolerate a moderate false-negative rate, since even if he only ﬁnds out about
employees searching for other jobs 90%, or even 50% of the time, this information is
useful nevertheless. Likewise, perfect detection is not needed for the potential attack to
have a chilling effect on Alice’s behavior.
Figure 7 shows the false negative rates that can be achieved given a target false-
positive rate below 0.5%, 1%, and 5%. Each bar represents a cumulative number of
websites, i.e., websites for which ˆq∗
w is at or below the x-axis value. We show two sets
of results; one using the VM setup for training and DSL for testing (Figure 7(a)) and
one using the DSL samples for both training and test data sets (Figure 7(b)). Note that
there is a signiﬁcant difference between the two graphs, resulting from the discrepan-
cies between the simulated (VM) and the test environment. We expect that, with some
work, an attacker may be able to reduce such discrepancies by more carefully tuning the
parameters of the virtual machine and the simulated link, or by using actual hardware
and a real DSL line that mimics Alice’s setup. The DSL–DSL case therefore shows the
limits of what can be achieved by improving the training environment.
An important observation is that, in both cases, the success of the web detection is
highly dependent on the target site. For a small number of sites—75 in the VM–DSL
case and 320 in the DSL–DSL case—the web detection attack works very well: we are
12 We use a binomial proportion conﬁdence interval here. This is slightly imprecise, as the 12·999
samples are not independent; we leave computation of conﬁdence intervals that take this into
account for future work.

Website Detection Using Remote Trafﬁc Analysis
71
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Upper Bound For False Positive Rate
False Negative Rate
 
 
Training with other pages
Training with the test page
Fig. 8. Detection performance for Warrior Forum when training with the correct post page and
when using other pages
able to maintain a very low false-positive rate of 0.5% while experiencing few false
negatives (17% or below). On the other hand, some sites are virtually invulnerable to
our attack: for 65 of the sites we tested, we were unable to observe any true positives
with a target false-positive rate of 5% (i.e., ˆq∗
w = 100%), even in the best-case DSL–
DSL scenario. We found these sites to have either very short traces, making it difﬁcult
to distinguish them from other such sites, or highly variable trafﬁc patterns due, for
example, to dynamic content, making it difﬁcult to create a useful ﬁngerprint. Note
the attacker can predict false-positive rates for different websites ahead of time using
experiments carried on VM setups. Making use of this knowledge would make the
attack even stronger.
4.4
Deanonymization
We next consider the deanonymization attack described in §3.3. As a case study, we
considered the site www.warriorforum.com, a popular Internet marketing forum. It uses
the vBulletin software, which was, as of August 2011, the most popular bulletin board
software13, and thus should be representative of a number of other sites. In our attack
scenario, Bob wishes to ﬁnd out if Alice is using a particular pseudonym (say, “di-
ane123”) to post on the site. To accomplish this, he ﬁrst collects traces from Alice’s
home computer for a period of time. He then waits for posts to the forum from diane123
and performs a detection attack to see if Alice was visiting the site at the time of post.
Repeated successful matches can then be used to obtain increasing conﬁdence in tying
Alice to diane123.Note that Bob will need to build a proﬁle that targets internal pages of
www.warriorforum.com, rather than the front page. Alice’s post requests will be
too small to create an easily-observable feature; however, vBulletin displays the forum
thread after a post has been made. Therefore, Bob can collect samples visiting threads
where diane123 has posted to create a ﬁngerprint. Note that, in this attack, ﬁngerprint
creation happens after the trace collection. A problem facing Bob is that different post
pages on Warrior Forum will have similar features in their RTT proﬁle. A match, there-
fore, can show that Alice visited some Warrior Forum page with high conﬁdence, but
it may not have been the correct thread. Even this information, however, is likely to
13 http://www.big-boards.com/statistics/

72
X. Gong et al.
be enough for deanonymization. For example, Alexa shows that fewer than 1% of US
Internet users actually visit the Warrior Forum, and those that do tend to stay on the
site less than 10 minutes on average. If we make the simplifying assumptions that these
visits are distributed randomly across a three-hour evening period, the additional false-
positive rate due to random visits to other Warrior Forum pages is no more than 6%,
even if Alice is known to be a Warrior Forum user. Combining observations across sev-
eral posts allows Bob to improve his conﬁdence.
Over a long term, even simpler attacks may sufﬁce: since most people’s Internet
usage is bursty, simply observing that Alice always actively used the Internet in some
way whenever diane123 made posts can be used for deanonymization [7]. Likewise,
Bob may be able to rule out Alice as a suspect if she was known to be at home (due to
recent DSL activity) but her connection was idle at the times of the target posts.
Finally, Bob may be able to use the similarity between internal forum pages to his
advantage. In particular, suppose that Alice publicly participates in the forum under
her real identity, in addition to potentially posting under a pseudonym. Bob can use
the times of Alice’s posts under her real name to label traces collected from Alice’s
computer and create a training set. In this case, Bob does not need to simulate Alice’s
computing environment as the training and test environments are exactly the same—the
ideal conditions we used in the DSL–DSL case. To study this attack, we collected sam-
ples from 100 different posts on the Warrior Forum site. For each sample, we attempt
to match it to a ﬁngerprint created from the other 99 posts; our process is similar to
(6), except using a different threshold for each sample. From this, we estimate the false
negative rate for a given target false-positive rate, calculated using (5), using the traces
from 999 other websites. Figure 8 shows the results. The use of different pages to test
degrades the matching performance, but it still provides sufﬁcient detection power for
deanonymization after a few posts.
5
Discussion
In this section, we discuss more on the feasibility and limitations of our work.
1. ICMP support. The attack scenario of our scheme relies on ICMP probe packets,
hence we care about whether ICMP is enabled in real routers. In testing over 918
probable DSL hosts on the Internet in Section §4.1, we found over 25% responded
to ping requests. Since we harvested these probable DSL hosts from the Internet
over a period of several months, it is not clear how many that failed to respond
were simply down rather than blocking our probes. Thus, we can assume that the
fraction of hosts that respond to ping is even larger. Additionally, in a brief survey
of consumer-grade router hardware, we found that many of them do not perform
ICMP ﬁltering, at least not in the default conﬁguration. Moreover, even though the
ping packets are blocked by ﬁrewalls on some home routers, other forms of probes
may be exploited as well; for example, if the home router exposes TCP ports for
ﬁle sharing or other applications, SYN packets can be used as probes with the same
effectiveness.
2. FIFO scheduling policy. The high correlation between the user’s trafﬁc pattern and
the attacker’s ping RTTs comes from the fact that the router serves packets in FIFO

Website Detection Using Remote Trafﬁc Analysis
73
order. Note that most home routers today do not use QoS extensions and schedule
packets on a given link in FIFO order. Thus, information leaked by these routers
can be exploited with remote trafﬁc analysis. Certainly, a fair queuing implemen-
tation [26] would reduce the impact that cross-trafﬁc would have on the probe se-
quence and hence reduce the effectiveness of the side channel, but not entirely
eliminate it [15].
3. Limited Last-hop Bandwidth. The information leaked through our side channel are
the states of the queue length in the router’s buffer. Hence, to have nontrivial queues
built up in the buffer, the broadband link must have limited bandwidth compared
to the rest of the links in the path. In our experiments, we have used speeds typical
of current home broadband speeds—several Mbps, and our scheme worked well
in those environments. The deployment of faster links, such as Fiber-to-the-Home
(FTTH), may reduce the effectiveness of the queueing side channel, but notice that
if the core network is similarly upgraded in speed, the bandwidth disparity neces-
sary for our attack will remain.
4. Victim’s IP address. In our attack, the attacker needs to know the user’s IP address
to send the probes. Although this mapping is typically only explicitly known to
ISPs, many protocols, such as ﬁle sharing, instant messaging, VoIP, and email, will
reveal the IP address of a user. Other forms of IP address reconnaissance may also
be possible but are outside the scope of this work.
5. Multiple users. In the scenario of our remote trafﬁc analysis, the attacker’s probes
cannot distinguish between the trafﬁc of multiple users on the same link, so shared
broadband connections present an obstacle to our attack. However, even in multi-
user installations, it is still common for only one user to be using the Internet at
any given point during the day. Some previous work on trafﬁc analysis has used
blind source separation to separate trafﬁc from multiple users [35]; similar tech-
niques may be applicable here. For example, in Figure 2, trafﬁc follows a periodic
pattern based on the RTT between Alice and the website; such periodicity might
help separate the sources.
6. Dynamic nature of websites. Our attack relies on web sites having relatively sta-
ble ﬁngerprints. Although the overall pattern captured by our RTT probes remains
static enough within days, the website content may incur signiﬁcant changes (e.g.,
site redesigns) over time; which in turn will result in a change of its ﬁngerprint.
Thus, for best results, the training set should be updated continuously. This limita-
tion applies to any website ﬁngerprinting approach even local website ﬁngerprint-
ing techniques which beneﬁt from better vantage points [11,17].
7. Content distribution networks. Websites that use content delivery networks (CDNs)
will use different servers to deliver content based on the user’s location. They may
present localized versions of the site to users in different countries or regions. As
shown in our experimental results, this can cause ﬁngerprints to differ signiﬁcantly.
If identifying these sites is a high priority for the attacker, additional work would
be needed to obtain ﬁngerprints of the right version by, for example, using proxies
and other techniques to fool IP-based localization.
8. Cache issues. In our tests, we followed the assumption in previous work [11, 17]
and disabled the cache in the browser. This implies that our results demonstrates

74
X. Gong et al.
the attacker’s ability to verify that a user visits a web page for the ﬁrst time. To
investigate cases with cache enabled, one possible solution would be build separate
ﬁngerprints based on the time since the site was ﬁrst downloaded, e.g., after 1 hour,
6 hours, 1 day, 1 week, to minimize the effect that caching would have on the attack.
Note that with continuous observation of a computer, the attacker may be able to
guess how long ago the last visit was.
6
Related Work
The use of network probes to infer information about trafﬁc at a remote location has
been explored in previous work in the context of anonymous communication networks.
Murdoch and Danezis used a remote trafﬁc analysis approach to expose the identity of
relays participating in a circuit in the Tor [8] and MorphMix [21] anonymous commu-
nication networks [19]. Their approach was to send an on–off pattern of high-volume
trafﬁc through the anonymous tunnel and a low-volume probe to a router under test.
If the waiting times of the probe showed a corresponding increase during the “on”
periods, the router was assumed to be routing the ﬂow. However, when Murdoch and
Danezis evaluated their attack, the Tor network was lightly loaded and consisted only of
13 relays; to repeat their attack on today’s network, with around 2 000 relays and high
trafﬁc load14, an attacker would needs extremely large amounts of bandwidth to mea-
sure enough relays during the attack window. Evans et al. [9] strengthened Murdoch and
Danezis’s attack of by a bandwidth ampliﬁcation attack which make their attack feasible
in modern-day deployment of Tor. Hopper et al. [13,14] use a combination of Murdoch
and Danezis’s approach and pairwise round trip times (RTTs) between Internet nodes to
correlate Tor nodes to likely clients. Chakravarty et al. [3] propose an attack for expos-
ing Tor relays participating in a circuit of interest by modulating the bandwidth of an
anonymous connection and then using available bandwidth estimation to observe this
pattern as it propagates through the Tor network. Note that these techniques relied on
detecting a specially-crafted coarse-grained communication pattern, whereas our attack
makes use of ﬁne-grained information obtained through remote trafﬁc analysis.
We also survey previous work on recovering information about encrypted HTTP
trafﬁc.The fact that object sizes could be used to infer sensitive information, even after
encryption, was ﬁrst mentioned by Yee (as related by Wagner and Schneier [30]). A
speciﬁc concern listed by Yee is that the particular page within a site accessed by the
user could be revealed by considering URL and object lengths. Chen et al. [4] applied
this observation to AJAX applications to recover detailed information about the internal
state of the application and users’ data. Cheng et al. [5] present the earliest implemen-
tation of website ﬁngerprinting. The classiﬁcation features used in their scheme are
the object sizes and the HTML ﬁle sizes. Hintz [12] and Sun et al. [29] both consider
website ﬁngerprinting attacks in SSL-encrypted HTTP connections. Their classiﬁcation
features are object sizes and counts. While Hintz did not present implementation details
and experimental results, Sun et al. use a Jaccard’s coefﬁcient based classiﬁer and show
that their attack can achieve a correct identiﬁcation rate of 75%. Instead of looking at
web objects, Bissias et al. [2], Liberatore et al. [17], and Herrmann et al. [11] study
14 See http://torstatus.blutmagie.de (retrieved November 2010)

Website Detection Using Remote Trafﬁc Analysis
75
the statistical characteristics of individual packets in the trafﬁc ﬂows. Bissias et al. use
packet sizes and inter-arrival timings as classiﬁcation features. Their method is fragile
to the changes in the network environment, as the inter-arrival timing is highly depen-
dent on the speciﬁc routing path and varies from time to time. To address this problem,
Liberatore et al. only use packet sizes and counts in classiﬁcation. They implement
both Jaccard coefﬁcient and Na¨ıve Bayes classiﬁer, and show the efﬁcacy of the attack
in practice. Using similar scheme, Herrmann et al. further improve the classiﬁcation
accuracy using Multinomial Na¨ıve Bayes classiﬁer.
7
Conclusion
We show that trafﬁc analysis attacks can be carried out remotely, without access to the
analyzed trafﬁc, thus greatly increasing the attack surface and lowering the barrier to
entry for conducting the attack. We identify a queuing side channel that can be used to
infer the queue size of a given link with good accuracy and thus monitor trafﬁc patterns.
We show how this channel can be used to carry out a remote attack to detect a remote
user’s browsing patterns. This highlights the importance of trafﬁc analysis attacks in
today’s connected Internet.
References
1. Akella, A., Seshan, S., Shaikh, A.: An empirical evaluation of wide-area Internet bottlenecks.
In: Crovella, M. (ed.) 3rd ACM SIGCOMM Conference on Internet Measurement, pp. 101–
114. ACM, New York (2003),
http://dl.acm.org/citation.cfm?id=948205.948219
2. Bissias, G.D., Liberatore, M., Jensen, D., Levine, B.N.: Privacy Vulnerabilities in Encrypted
HTTP Streams. In: Danezis, G., Martin, D. (eds.) PET 2005. LNCS, vol. 3856, pp. 1–11.
Springer, Heidelberg (2006)
3. Chakravarty, S., Stavrou, A., Keromytis, A.D.: Identifying proxy nodes in a Tor anonymiza-
tion circuit. In: Dipanda, A., Chbeir, R., Yetongnon, K. (eds.) IEEE International Conference
on Signal Image Technology and Internet Based Systems, pp. 633–639. IEEE Computer So-
ciety, Los Alamitos (2008)
4. Chen, S., Wang, R., Wang, X., Zhang, K.: Side-Channel Leaks in Web Applications:
A Reality Today, a Challenge Tomorrow. In: Evans, D., Vigna, G. (eds.) IEEE Sym-
posium on Security and Privacy, pp. 191–206. IEEE Computer Society (May 2010),
http://ieeexplore.ieee.org/lpdocs/epic03/
wrapper.htm?arnumber=5504714
5. Cheng, H., Avnur, R.: Trafﬁc Analysis of SSL Encrypted Web Browsing (1998),
http://www.cs.berkeley.edu/ daw/teaching/cs261-f98/projects/
final-reports/ronathan-heyning.ps
6. Coull, S.E., Collins, M.P., Wright, C.V., Monrose, F., Reiter, M.K.: On web browsing privacy
in anonymized netﬂows. In: Provos, N. (ed.) 16th USENIX Security Symposium. USENIX
Association, Berkeley (2007),
http://www.usenix.org/events/sec07/tech/coull.html
7. Danezis, G., Serjantov, A.: Statistical Disclosure or Intersection Attacks on Anonymity Sys-
tems. In: Fridrich, J. (ed.) IH 2004. LNCS, vol. 3200, pp. 293–308. Springer, Heidelberg
(2004), http://www.springerlink.com/index/TQLJB3HYBK4RUBLA.pdf

76
X. Gong et al.
8. Dingledine, R., Mathewson, N., Syverson, P.: Tor: The Second-Generation Onion Router.
In: Blaze, M. (ed.) USENIX Security Symposium, pp. 303–320. USENIX Association, San
Diego (2004), http://portal.acm.org/citation.cfm?id=1251396
9. Evans, N.S., Dingledine, R., Grothoff, C.: A practical congestion attack on Tor using long
paths. In: Monrose, F. (ed.) 18th USENIX Security Symposium, pp. 33–50. USENIX Asso-
ciation (August 2009),
http://www.usenix.org/events/sec09/tech/full_papers/evans.pdf
10. Franklin, J., Paxson, V., Perrig, A., Savage, S.: An inquiry into the nature and causes of
the wealth of Internet miscreants. In: De Capitani di Vemarcati, S., Syverson, P. (eds.) 14th
ACM Conference on Computer and Communications Security, pp. 375–388. ACM, New
York (2007), http://dl.acm.org/citation.cfm?id=1315245.1315292
11. Herrmann, D., Wendolsky, R., Federrath, H.: Website ﬁngerprinting: attacking pop-
ular privacy enhancing technologies with the multinomial na¨ıve-Bayes classiﬁer. In:
ACM Workshop on Cloud Computing Security, pp. 31–42. ACM, Chicago (2009),
http://portal.acm.org/citation.cfm?id=1655013
12. Hintz, A.: Fingerprinting Websites Using Trafﬁc Analysis. In: Dingledine, R., Syver-
son, P.F. (eds.) PET 2002. LNCS, vol. 2482, pp. 171–178. Springer, Heidelberg (2003),
http://www.springerlink.com/index/C4QWE6D608P2CJYV.pdf
13. Hopper, N., Vasserman, E.Y., Chan-Tin, E.: How much anonymity does network la-
tency leak? In: De Capitani di Vimercati, S., Syverson, P. (eds.) 14th ACM Confer-
ence on Computer and Communications Security, pp. 82–91. ACM, New York (2007),
http://dl.acm.org/citation.cfm?id=1315245.1315257
14. Hopper, N., Vasserman, E., Chan-Tin, E.: How much anonymity does network la-
tency leak? ACM Transactions on Information and System Security 13(2) (2010),
http://portal.acm.org/citation.cfm?id=1698753
15. Kadloor, S., Gong, X., Kiyavash, N., Tezcan, T., Borisov, N.: Low-Cost Side Chan-
nel Remote Trafﬁc Analysis Attack in Packet Networks. In: Xiao, C., Olivier, J.C.
(eds.) 2010 IEEE International Conference on Communications. IEEE (May 2010),
http://ieeexplore.ieee.org/xpl/
articleDetails.jsp?arnumber=5501972
16. Lakshminarayanan, K., Padmanabhan, V.N.: Some ﬁndings on the network performance of
broadband hosts. In: Crovella, M. (ed.) Proceedings of the 2003 ACM SIGCOMM Con-
ference on Internet Measurement, IMC 2003, pp. 101–114. ACM Press, New York (2003),
http://portal.acm.org/citation.cfm?doid=948205.948212
17. Liberatore, M.,
Levine,
B.N.: Inferring
the
source
of
encrypted
HTTP connec-
tions. In: Wright, R., De Capitani di Vemarcati, S. (eds.) 13th ACM Conference
on Computer and Communications Security, pp. 255–263. ACM, New York (2006),
http://portal.acm.org/citation.cfm?id=1180437
18. Lyon, G.F.: Nmap Network Scanning. Nmap Project (1999)
19. Murdoch, S., Danezis, G.: Low-Cost Trafﬁc Analysis of Tor. In: Paxson, V., Waidner, M.
(eds.) 2005 IEEE Symposium on Security and Privacy, pp. 183–195. IEEE Computer Society,
Berkeley (2005),
http://ieeexplore.ieee.org/lpdocs/epic03/
wrapper.htm?arnumber=1425067
20. Prasad, R., Davrolis, C., Murray, M., Claffy, K.: Bandwidth estimation: metrics, measure-
ment techniques, and tools. IEEE Network 17(6), 27–35 (2003),
http://ieeexplore.ieee.org/xpl/
articleDetails.jsp?arnumber=1248658

Website Detection Using Remote Trafﬁc Analysis
77
21. Rennhard, M., Plattner, B.: Introducing MorphMix: peer-to-peer based anonymous Internet
usage with collusion detection. In: Samarati, P. (ed.) ACM Workshop on Privacy in Electronic
Society, pp. 91–102. ACM Press, New York (2002),
http://portal.acm.org/citation.cfm?id=644537
22. Ribeiro, V., Riedi, R., Baraniuk, R., Navratil, J., Cottrell, L.: pathchirp: Efﬁcient available
bandwidth estimation for network paths. In: Passive and Active Measurement Workshop,
vol. 4. Citeseer (March 2003)
23. Rizzo,
L.:
Dummynet:
a
simple
approach
to
the
evaluation
of
network
proto-
cols.
ACM
SIGCOMM
Computer
Communication
Review
27(1),
31–41
(1997),
http://portal.acm.org/citation.cfm?doid=251007.251012
24. Sakoe, H., Chiba, S.: Dynamic programming algorithm optimization for spoken word recog-
nition. IEEE Transactions on Acoustics, Speech, and Signal Processing 26(1), 43–49 (1978),
http://ieeexplore.ieee.org/lpdocs/epic03/
wrapper.htm?arnumber=1163055
25. Saponas,
T.S.,
Lester,
J.,
Hartung,
C.,
Agarwal,
S.,
Kohno,
T.:
Devices
that
tell on
you:
Privacy trends
in consumer
ubiquitous
computing.
In:
Provos,
N.
(ed.) 16th USENIX Security Symposium, pp. 55–70. USENIX Association (2007),
http://portal.acm.org/citation.cfm?id=1362908
26. Shreedhar, M., Varghese, G.: Efﬁcient fair queuing using deﬁcit round-robin. IEEE/ACM
Transactions on Networking 4(3), 375–385 (1996),
http://ieeexplore.ieee.org/lpdocs/epic03/
wrapper.htm?arnumber=502236
27. Song, D.X., Wagner, D., Tian, X.: Timing analysis of keystrokes and SSH timing attacks.
In: Wallach, D.S. (ed.) 10th USENIX Security Symposium. USENIX Association (August
2001), http://www.usenix.org/events/sec01/song.html
28. Strauss, J., Katabi, D., Kaashoek,
F.: A measurement
study of available band-
width
estimation
tools.
In:
Crovella,
M.
(ed.)
3rd
ACM
SIGCOMM
Con-
ference
on
Internet
Measurement,
pp.
39–44.
ACM,
New
York
(2003),
http://portal.acm.org/citation.cfm?id=948211
29. Sun, Q., Simon, D.R., Wang, Y.M., Russell, W., Padmanabhan, V.N., Qiu, L.: Statistical
identiﬁcation of encrypted Web browsing trafﬁc. In: Abadi, M., Bellovin, S.M. (eds.) IEEE
Symposium on Security and Privacy, pp. 19–30. IEEE Computer Society (May 2002),
http://ieeexplore.ieee.org/xpl/
articleDetails.jsp?arnumber=1004359
30. Wagner, D., Schneier, B.: Analysis of the SSL 3.0 Protocol. In: Tygar, D. (ed.) USENIX
Workshop on Electronic Commerce. USENIX Association (November 1996),
http://www.usenix.org/publications/
library/proceedings/ec96/wagner.html
31. White, A.M., Matthews, A.R., Snow, K.Z., Monrose, F.: Phonotactic reconstruction of en-
crypted VoIP conversations: Hookt on Foniks. In: Vigna, G., Jha, S. (eds.) IEEE Symposium
on Security and Privacy, pp. 3–18. IEEE Computer Society (May 2011),
http://ieeexplore.ieee.org/xpl/
articleDetails.jsp?arnumber=5958018
32. Wright, C.V., Ballard, L., Coull, S.E., Monrose, F., Masson, G.M.: Spot me if you can: Un-
covering spoken phrases in encrypted VoIP conversations. In: IEEE Symposium on Security
and Privacy, pp. 35–49. IEEE Computer Society, Washington, DC (2008),
http://ieeexplore.ieee.org/xpl/
articleDetails.jsp?arnumber=4531143

78
X. Gong et al.
33. Wright,
C.V.,
Ballard,
L.,
Coull,
S.E.,
Monrose,
F.,
Masson,
G.M.:
Un-
covering
Spoken
Phrases
in
Encrypted
Voice
over
IP
Conversations.
ACM
Transactions
on
Information
and
System
Security
13(4),
1–30
(2010),
http://doi.acm.org/10.1145/1880022.1880029
34. Zhang,
K.,
Wang,
X.:
Peeping
Tom
in
the
neighborhood:
Keystroke
eaves-
dropping
on
multi-user
systems.
In:
Monrose,
F.
(ed.)
18th
USENIX
Se-
curity
Symposium
USENIX
Security.
USENIX
Association
(August
2009),
http://www.usenix.org/events/sec09/tech/full_papers/zhang.pdf
35. Zhu, Y., Bettati, R.: Unmixing Mix Trafﬁc. In: Danezis, G., Martin, D. (eds.) PET 2005.
LNCS, vol. 3856, pp. 110–127. Springer, Heidelberg (2006)

k-Indistinguishable Trafﬁc Padding in Web Applications
Wen Ming Liu1, Lingyu Wang1, Kui Ren2, Pengsu Cheng1, and Mourad Debbabi1
1 Concordia Institute for Information Systems Engineering, Concordia University
2 Department of Electrical and Computer Engineering, Illinois Institute of Technology
Abstract. While web-based applications are becoming increasingly ubiquitous,
they also present new security and privacy challenges. In particular, recent re-
search revealed that many high proﬁle Web applications might cause private user
information to leak from encrypted trafﬁc due to side-channel attacks exploiting
packet sizes and timing. Moreover, existing solutions, such as random padding
and packet-size rounding, are shown to incur prohibitive cost while still not ensur-
ing
sufﬁcient
privacy
protection.
In
this
paper,
we
propose
a
novel
k-indistinguishable trafﬁc padding technique to achieve the optimal tradeoff be-
tween privacy protection and communication and computational cost. Speciﬁ-
cally, we ﬁrst present a formal model of the privacy-preserving trafﬁc padding
(PPTP). We then formulate PPTP problems under different application scenarios,
analyze their complexity, and design efﬁcient heuristic algorithms. Finally, we
conﬁrm the effectiveness and efﬁciency of our algorithms by comparing them to
existing solutions through experiments using real-world Web applications.
1
Introduction
Web-based applications are gaining popularity. By providing software services through
Web browsers, such applications demand less client-side resources and are easier to
deliver and maintain than their desktop counterparts. However, they also present new
security and privacy challenges partly because the untrusted Internet now becomes an
integral part of the application for carrying the continuous interaction between users
and service providers. Recent study showed that the encrypted trafﬁc of many popular
Web applications may actually disclose highly sensitive data, and consequently lead to
serious breaches of user privacy [9]. By analyzing packets’ sizes and timing, an eaves-
dropper can potentially identify an application’s internal state transitions as well as
users’ inputs. Moreover, such side-channel attacks are shown to be pervasive and fun-
damental to Web applications due to their intrinsic characteristics , such as low entropy
inputs, rich and diverse resource objects, and stateful communications.
For example, Table 1 shows the size and direction of packets observed between users
and a popular real-world search engine. Observe that due to the auto-suggestion feature,
with each keystroke, the browser sends a b-byte packet to the server; the server then
replies with two packets of 54 bytes and s bytes, respectively; ﬁnally, the browser sends
a 60-byte packet to the server. In addition, in the same input string, each subsequent
keystroke increases the b value by one byte, and the s value depends not only on the
current keystroke but also on all the previous ones. Clearly, an eavesdropper can pin-
point packets corresponding to an input string from observed trafﬁc by the packets with
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 79–99, 2012.
c⃝Springer-Verlag Berlin Heidelberg 2012

80
W.M. Liu et al.
Table 1. User Inputs and Corresponding Packet Sizes
User Input Observed Directional Packet Sizes
a
b1 →,
←54, ←509,
60 →
00
b2 →,
←54, ←505,
60 →,
b2 + 1 →, ←54, ←507,
60 →
(b bytes)
(s bytes)
ﬁxed pattern in size(ﬁrst, second, and last), even though the trafﬁc has been encrypted.
In this paper, we assume such a worst case scenario in which an eavesdropper can iden-
tify trafﬁc related to a Web application (such as using de-anonymizing techniques [27])
and locate packets for user inputs using the above technique.
Moreover, the size of the third packet(s) will provide a good indicator of the input it-
self. Speciﬁcally, the left tabular of Table 2 shows the s value for each character entered
as the ﬁrst keystroke of an input string. We can see that six characters can be uniquely
identiﬁed with this s value. The right tabular shows the s value for a character entered
as the second keystroke. In this case, the s value for each character in the right tabular
is different from that in the left, since the packet size now depends on both the current
keystroke and the preceding one. Clearly, every input string can be uniquely identiﬁed
by combining observations about the two consecutive keystrokes shown in both tables
(for simplicity, we are only considering four characters here, whereas in reality it may
take more than two keystrokes to uniquely identify an input string).
Table 2. s Value for Each Char Entered as the First or Second Keystroke (Left or Right Tabular)
a
b
c
d
e
f
g
h
i
509 504 502 516 499 504 502 509 492
j
k
l
m
n
o
p
q
r
517 499 501 503 488 509 525 494 498
s
t
u
v
w
x
y
z
488 494 503 522 516 491 502 501
Second keystroke
First keystroke
a
b
c
d
a
487 493 501 497
b
516 488 482 481
c
501 488 473 477
d
543 478 509 499
A natural solution for preventing such a side channel attack is to pad packets such
that each packet size will no longer map to a unique input. However, such a solution
does not come free, since padding packets will result in additional overhead. In fact, it
has been shown that a straightforward solution, such as random padding and rounding,
may incur a prohibitive overhead (e.g. 21074% for a well-known online tax system [9]).
Moreover, such an application-agnostic approach typically aim to maximize, but cannot
guarantee, the amount of privacy protection.
In Table 3, we consider a different way for padding the packets. The ﬁrst and last
columns respectively show the s value and corresponding input (the second keystroke).
The middle two columns give two options for padding packets (although not shown
here, there certainly exist many other options). Speciﬁcally, each option ﬁrst divides
the six characters into three (or two) padding groups, as illustrated by the (absence
of) horizontal lines. Packets within the same padding group are then padded in such a
way that their corresponding s values are all identical to the maximum value. Thus the

k-Indistinguishable Trafﬁc Padding in Web Applications
81
Table 3. Mapping PPTP to PPDP
s Value
Padding
(1st Keystroke) 2nd Keystroke
Option 1 Option 2
473
477
478
(c)c
477
477
478
(c)d
478
499
478
(d)b
499
499
509
(d)d
501
509
509
(c)a
509
509
509
(d)c
Quasi-ID
Generalization
Sensitive Value
characters inside each padding group will no longer be distinguishable from each other
based on their s values. The objective now is to ﬁnd a padding option that can provide
sufﬁcient privacy protection and meanwhile minimize the padding cost.
Interestingly, this privacy-preserving trafﬁc padding (PPTP) problem can be natu-
rally interpreted as another well studied problem, privacy-preserving data publishing
(PPDP) [13]. To revisit Table 3, if we regard the s value as a quasi-identiﬁer (such
as DoB), the input as a sensitive value (such as medical condition), and the padding
options as different ways for generalizing the DoB into anonymized groups (for ex-
ample, by removing the day from a DoB), then we immediately have a classic PPDP
problem, that is, publishing DoBs and medical conditions while preventing adversaries
from linking any published medical condition to a person through his/her DoB.
The similarity between the two problems implies we may borrow many existing
efforts in the PPDP domain to address the PPTP issue. On the other hand, there also
exist signiﬁcant differences between them. For example, in Table 3, the second option
will typically be considered as worse (than the ﬁrst) in PPDP since it results in larger
anonymized groups, whereas it is actually better in terms of padding cost (totally 24
bytes, in contrast to 33 by the ﬁrst option). As another example, we will show later
that the effect of combining two keystrokes will be equivalent to releasing multiple
inter-dependent tables, which actually leads to a novel PPDP problem.
In this paper, we ﬁrst brieﬂy review the formal model of the PPTP issue based on
the mapping to PPDP which introduced in our short version [18]. We then formulate
several PPTP problems under different assumptions, and discuss the complexity. We
show that minimizing padding cost under a given privacy requirement is generally in-
tractable. Next, we design several heuristic algorithms for solving the PPTP problems
in polynomial time with acceptable padding cost. Finally, we evaluate the effective-
ness and efﬁciency of our algorithms by comparing them to existing solutions through
experiments with real-world Web applications.
The contribution of this paper is threefold. First, the identiﬁed similarity between
PPTP and PPDP establishes a bridge between the two research communities, which will
not only allow for reusing many existing models and methods in the well investigated
PPDP domain, but serve to attract more interest to the important PPTP issue. Second, to
the best of our knowledge, our PPTP model is among the ﬁrst efforts on formally address-
ing this issue (in contrast to our work, the formal model given by Chen et al. [9] lacks
a clear deﬁnition of privacy requirements and only considers two application-agnostic

82
W.M. Liu et al.
padding methods). Third, the proposed padding algorithms can lead to practical solu-
tions for real world Web applications, as evidenced by our experiments.
The rest of the paper is organized as follows. Section 2 deﬁnes our PPTP model.
Section 3 formulates PPTP problems and analyzes the complexity. Section 4 devises
heuristic algorithms for the formulated problems. Section 5 experimentally evaluates
the performance of our algorithms. Section 6 discusses the extensions and implementa-
tion of our solution. Section 7 reviews related work and Section 8 concludes the paper.
2
The Model
To be self contained, we brieﬂy repeat here the PPTP model introduced in our short
version [18], and shall delay the discussion about extending it to encompass l-diversity
in Section 6. Table 4 lists main notations that will be used throughout the paper.
Table 4. The Notation Table
a, ⃗a, Ai or A
Action, action-sequence, action-set
s, v, ⃗v, Vi or V Flow, ﬂow-vector, vector-sequence, vector-set
⃗a[i],⃗v[i]
The ith element in ⃗a and ⃗v
VAi or VA
Vector-action set
pre(a,i)
i-Preﬁx
dom(P)
Dominant-vector
vdis(v1, v2)
Vector-distance
2.1
The Basic Model
We model the PPTP issue from two perspectives, the interaction between users and
servers, and the observation made by eavesdroppers. First, Deﬁnition 1 formalizes the
interaction. Our discussions about Table 2 demonstrated how one keystroke may affect
another in terms of observations (packet sizes), and how an eavesdropper may com-
bine such multiple observations for a reﬁned inference. Such related user actions are
modeled as an action-sequence in Deﬁnition 1. The concept of action-set models a col-
lection of actions whose corresponding observations may be padded together. Actions
inside an action-sequence are separated into different action-sets since their relationship
is known from trafﬁc patterns and thus padding them together will not work (preventing
such inferences about the application’s state transitions comprises a future direction).
Deﬁnition 1 (Interaction). Given a Web application, we deﬁne
- an action a as an atomic user input that triggers trafﬁc, such as a keystroke or a
mouse click.
- an action-sequence ⃗a as a sequence of actions with known relationships, such as
consecutive keystrokes entered into real-time search engine or a series of mouse
clicks on hierarchical menu items. We use ⃗a[i] to denote the ith action in ⃗a.
- an action-set Ai as the collection of all the ith actions in a set of action-sequences.
We will simply use A if all action-sequences are of length one.

k-Indistinguishable Trafﬁc Padding in Web Applications
83
Example 1. Assume “a” and “00” in Table 1 to be the only possible inputs, there are
two action-sequences a and 00, and two action-sets A1 = {a, 0} and A2 = {0}.
⊡
Deﬁnition 2 models concepts related to the observation made by an eavesdropper. Note
that a ﬂow-vector is intended to only model those packets that may contribute to identify
an action (such as the s value in Table 1). Further, a vector-set is deﬁned as a multiset,
since it may contain duplicates (that is, packets may share the same size).
Deﬁnition 2 (Observations). Given a web application, we deﬁne
- a ﬂow-vector v as a sequence of ﬂows where each ﬂow s is an integer (a directional
packet size). An action corresponds to a ﬂow-vector based on packets it triggers.
- a vector-sequence ⃗v as a sequence of ﬂow-vectors corresponding to an equal-length
action-sequence⃗a, with each ⃗v[i] corresponding to ⃗a[i] (1 ≤i ≤| ⃗v |).
- a vector-set Vi (or simply V ) as the collection of all the ith ﬂow-vectors in a set of
vector-sequences, which corresponds to an action-set in the straightforward way.
Example 2. Following Example 1, we have three ﬂow-vectors, v1 = 509, v2 = 505,
and v3 = 507 (note that we only model those packets whose sizes can help to iden-
tify an action), corresponding to actions a, 0 (as ﬁrst keystroke), and 0 (as second
keystroke), respectively. We have two vector-sequences, v1 and v2v3, corresponding to
action-sequences a and 00, respectively. We also have two vector-sets V1 = {509, 505}
and V2 = {507} corresponding to the two action-sets A1 and A2 in Example 1.
⊡
Finally, Deﬁnition 3 models the joint information about interaction and observation,
which is the collection of the pairs of the action and its corresponding ﬂow-vector.
Deﬁnition 3 (Vector-Action Set).
Given an action-set Ai and its corresponding
vector-set Vi, a vector-action set VAi is the set {(v, a) : v ∈Vi ∧a ∈Ai}.
Example 3. Following above examples, given the action-set A1 and vector-set V1, then
the vector-action set is VA1 = {(509, a), (505, 0)}. Similarly, VA2 = {(507, 0)}.
⊡
2.2
Privacy and Cost Model
For simplicity, we ﬁrst consider a simpliﬁed case where every action-sequence and ﬂow-
vector are of length one, namely, the Single-Vector Single-Dimension (SVSD) case. In
this case, we can map a given vector-action set VA = {(v, a) : v ∈V ∧a ∈A}
to a table T (v, a) with two attributes, the ﬂow-vector v (equivalent to a ﬂow s here) as
quasi-identiﬁer and the action a as sensitive attribute. Note that we will interchangeably
refer to a vector-action set and its tabular representation from now on.
Inspired by k-anonymity [24] in PPDP domain, Deﬁnition 4 quantiﬁes the amount
of privacy protection under a given vector-action set. This model follows the widely
adopted approach of assuming a ﬁxed privacy requirement while minimizing the cost.
Deﬁnition 4 (k-Indistinguishability). Given a vector-action set VA, we deﬁne
- a padding group as any S ⊆VA satisfying that all the pairs in S have identical
ﬂow-vectors and no S′ ⊃S can satisfy this property, and
- we
say VA satisﬁes
k-indistinguishability (k
is
an
integer) or VA is
k-indistinguishable if the cardinality of every padding group is no less than k.

84
W.M. Liu et al.
Discussion. One may argue that, in contrast to encryption, k-indistinguishability may
not provide strong enough protection. However, as mentioned before, we are consid-
ering cases where encryption is already broken by side-channel attacks, so the strong
conﬁdentiality provided by encryption is already not an option. Second, in theory k
could always be set to be sufﬁcient large to provide enough conﬁdentiality, although
we believe a reasonably large k would usually satisfy users’ privacy requirements for
most practical applications. Finally, since most web applications are publicly accessi-
ble and consequently an eavesdropper can unavoidably learn about possible inputs, we
believe focusing on protecting sensitive user input (by hiding it among other possible
inputs) yields higher practical feasibility and signiﬁcance than on perfect conﬁdentiality
(attempting to hide everything).
Furthermore, such mapped PPDP problems actually possess a unique characteris-
tic. That is, the sensitive values (actions) are always unique. Thus, by satisfying k-
indistinguishability, the vector-action set also satisﬁes l-diversity (l = k) in its simplest
form [20]. We will also apply more general forms of l-diversity to address cases where
not all actions should be treated equally in padding, as sketched in Section 6. Further-
more, a probabilistic approach based on differential privacy [12] is another possible
extension to enhance our model such that the padding result will be immune to eaves-
droppers’ prior knowledge. Nonetheless, this simple model is sufﬁcient to demonstrate
the usefulness of mapping PPTP to PPDP.
In addition to privacy requirement, we also need a quantitative measure for the cost
of padding and processing. Across the whole vector-set, Deﬁnition 5 counts the number
of additional bytes after padded, while Deﬁnition 6 counts the number of ﬂows that
are involved in padding. We focus on these simple models in this paper while there
certainly exist other ways for modeling such costs.
Deﬁnition 5 (Distance and Padding Cost). Given a vector-set V , we deﬁne
- the vector-distance between two equal-length ﬂow-vectors v1 and v2 as:
vdis(v1, v2) = |v1|
i=1(|s1i −s2i|) where s1i and s2i are the ith ﬂow in v1 and
v2, respectively.
- the padding cost of V as: cost = |V |
i=1(vdis(vi, v′
i)) where vi and v′
i denote a
ﬂow-vector in V and its counterpart after padding, respectively.
Deﬁnition 6 (Processing Cost). Given a vector-set V , we deﬁne the processing cost of
V as the number of ﬂows in V which corresponding packets should be padded.
2.3
The SVMD and MVMD Cases
In the previous section, we focused on the simpliﬁed SVSD case to facilitate a focused
discussion on the privacy and cost model. We now look at the more realistic cases.
First, we consider the Single-Vector Multi-Dimension (SVMD) case where each
ﬂow-vector may include more than one ﬂows whereas each action-sequence is still
composed of a single action. In this case, the vector-action set needs to be mapped to a
table T (s1, . . . , s|v|, a) with multiple quasi-identiﬁer attributes (each ﬂow corresponds
to an attribute). Thus, based on Deﬁnition 4, ﬂow-vectors can form a padding group

k-Indistinguishable Trafﬁc Padding in Web Applications
85
only if they are identical with respect to every ﬂow inside the vectors. Another subtlety
is that the model of vector-action set requires all the ﬂow-vectors to have the same
number of ﬂows, which is not always possible in practice. One solution is to insert
dummy packets of size zero which will then be handled as usual in the process of
padding.
Next, we consider the Multi-Vector Multi-Dimension (MVMD) case in which each
action-sequence consists of more than one actions and each ﬂow-vector includes multi-
ple ﬂows. Deﬁnition 7 expresses the relationship between actions in an action-sequence.
Deﬁnition 7 (i-preﬁx, adjacent-preﬁx). We deﬁne
- the i-preﬁx of an action-sequence ⃗a = (a1, a2, . . . , at) (i ∈[1, t]), denoted as
pre(⃗a, i), as the sequence (a1, a2, . . . , ai), and we say ai−1 is the adjacent-preﬁx
(or simply preﬁx) of ai.
- similarly, we deﬁne the i-preﬁx of vector-sequence ⃗v, and the adjacent-preﬁx of vi.
In the MVMD case, due to the preﬁx relationship, the ﬂow-vector for an action may
provide additional information about ﬂow-vectors that correspond to the previous ac-
tions in the same action-sequence. Such knowledge may enable the eavesdropper to
reﬁne his guesses about an action. Such a scenario is illustrated in Figure 1. Also, we
slightly change the deﬁnition of a vector-action set to accommodate the added preﬁx
action information, as shown in Deﬁnition 8. We will delay the discussion about how a
padding algorithm may satisfy k-indistinguishability in this case to the next section.
 
Prefix
Flow-Vector v Action a 
a22 
v31 
a31 
 
v12 
a32 
 
v11 
… 
 
v11 
… 
 
v11 
… 
 
v11 
… 
 
v11 
… 
 
 
… 
 
v1n 
… 
Prefix
Flow-Vector v Action a
a11 
v21 
a21 
 
v12 
a22 
 
v11 
… 
 
v11 
… 
 
v11 
… 
 
v11 
… 
 
v11 
… 
 
 
… 
 
v1n 
Prefix Flow-Vector v
Action a 
 
v11 
a11
 
v12 
a12
 
… 
… 
 
… 
… 
 
… 
… 
 
… 
… 
 
… 
… 
Fig. 1. The Vector-Action Set in MVMD Case
Deﬁnition 8 (Vector-Action Set (MVMD Case)). Given n action-sets {Ai : 1 ≤i ≤
n} and its corresponding vector-sets {Vi : 1 ≤i ≤n}, the vector-action set VA is the
collection of sets {{(v, a) : v ∈Vi ∧a ∈Ai} : 1 ≤i ≤n}.
3
PPTP Problems
The formal model introduced in the previous section enables us to formulate a series of
PPTP problems and study their complexity. We ﬁrst discuss the choice of our ceiling
padding approach among other possibilities in Section 3.1, and then address the SVSD
and SVMD cases in Section 3.2 and the MVMD case in Section 3.3.

86
W.M. Liu et al.
3.1
Padding Method
In choosing a padding method, we need to address two aspects, privacy protection by
satisfying the k-indistinguishability property, and minimizing padding cost. As previ-
ously mentioned, an application-agnostic approach, such as packet-size rounding and
random padding, will usually incur high padding cost while not necessarily guarantee-
ing sufﬁcient privacy protection [9]. We now revisit this argument by showing that a
larger rounding size does not necessarily lead to more privacy. With our model, more
privacy can now be clearly deﬁned as satisfying k-indistinguishability for a larger k.
Consider rounding the ﬂows shown in the left tabular of Table 2 to a multiple of 128
(for example, 509 to 4 × 128 = 512). It can be shown that such rounding can achieve
5-indistinguishability (detailed calculations will be omitted due to space limitations).
However, increasing the rounding size to 512 can still only satisfy 5-indistinguishability,
whereas further increasing it to 520 will actually only satisfy 2-indistinguishability.
On the other hand, as demonstrated in Section 1, we can now apply the PPDP tech-
nique of generalization to addressing the PPTP problem. A generalization technique
will partition the vector-action set into padding groups, and then break the linkage
among actions in the same group. One unique aspect in applying generalization to PPTP
is that padding can only increase each packet size but cannot decrease it, or replace it
with a range of values like in normal generalization. The above considerations lead to a
new padding method given in Deﬁnition 9. Basically, after partitioning a vector-action
set into padding groups, we pad each ﬂow in a padding group to be identical to the
maximum size of that ﬂow in the group.
Deﬁnition 9 (Dominance and Ceiling Padding). Given a vector-set V , we deﬁne
- the dominant-vector dom(V ) as the ﬂow-vector in which each ﬂow is equal to the
maximum of the corresponding ﬂow among all the ﬂow-vectors in V .
- a ceiling-padded group in V as a padding group in which every ﬂow-vector is
padded to the dominant-vector. We also say V is ceiling-padded if all the groups
are ceiling-padded.
We will focus on the ceiling padding method in the rest of the paper. When no ambiguity
is possible, we will not distinguish between vector-set, vector-action set, ﬂow-vector,
and vector-sequence.
3.2
The SVSD and SVMD Cases
In the SVSD case, there is only a single ﬂow in each ﬂow-vector of the vector-set.
Therefore, we only need to modify the vector-set by increasing the value of some ﬂows
to form padding groups. The padding problem can be formally deﬁned as follows.
Problem 1 (SVSD Problem). Given a vector-action set VA and the corresponding vector-
set V and action set A, the privacy property k ≤|V |, ﬁnd a partition P VA on VA such
that the corresponding partition on V , denoted as P V = {P1, P2, . . . , Pm}, satisﬁes
- ∀(i ∈[1, m]), |Pi| ≥k;
- The padding cost m
i=1(dom(Pi) × |Pi|) is minimal.
⊡

k-Indistinguishable Trafﬁc Padding in Web Applications
87
In the SVMD case, there are more than one ﬂows in each ﬂow-vector of the vector-set.
The padding problem can be deﬁned as follows:
Problem 2 (SVMD problem).
Given a vector-action set VA and the corresponding
vector-set V (in which each ﬂow-vector includes np ﬂows) and action set A, the privacy
property k ≤| V |, ﬁnd a partition P VA on VA such that the corresponding partition on
V , denoted as P V = {P1, P2, . . . , Pm}, satisﬁes
- ∀(i ∈[1, m]), |Pi| ≥k;
- The cost m
i=1(np
j=1((dom(Pi))[j]) × |Pi|) is minimal.
⊡
Theorem 1 shows that the above PPTP problem is intractable. The proof is omitted due
to space limitations (basically, we prove the result through a reduction to the problem
of Edge Partition Into Triangles (EPIT) [14]). Theorem 1 indicates that Problem 2 is
NP-hard even when there are only two different ﬂow values (that is, the sizes of packets
in the trafﬁc) in the vector-set.
Theorem 1. Problem 2 is NP-complete when k = 3 and the ﬂow-vectors are from any
binary alphabet .
Note that, at ﬁrst glance, the SVMD problem may resemble the problem of k-means
clustering [15]. However, algorithms for k-means clustering cannot be directly applied
to our problem due to following differences between these two problems. First, k-means
clustering needs to partition a multiset into k groups, whereas in our problem, the min-
imal size of each group must be at least k. Second, k-means clustering is to minimize
the within-cluster sum of squares, while our problem is to minimize the total distance
between each of the ﬂow-vectors and the dominant-vector.
3.3
MVMD Problem
As mentioned in Section 2.3, by correlating ﬂow-vectors in the vector-sequence, an
eavesdropper may reﬁne his guesses of the actual action-sequence. We ﬁrst discuss the
challenges of trafﬁc padding in such cases by a toy example of auto-suggestion feature.
Example 4. In Table 5, the second column shows each ﬂow corresponding to c1, c2, c3,
c4 when entered as the ﬁrst keystroke, respectively. Similarly, the 16 cells cij in row:
(i ∈[2 −5]) and column: (j ∈[3 −6]) show the ﬂow corresponding to the second
keystroke when ci−1 is the ﬁrst keystroke and cj−2 the second keystroke.
Suppose an eavesdropper has observed the ﬂow for the second keystroke. In order
to preserve 2-indistinguishability with minimal padding overhead, we will partition the
16 cells into eight groups Pi = {cjk : cjk
10 = i}, such that the size of each group is not
less than 2. For example, the queried strings c1c1 and c3c2 are in one group and c1c1
should be padded to 15. When the eavesdropper observes that the ﬂow for the second
keystroke is 15, she cannot determine whether the queried string is c1c1 or c3c2.
However, suppose that the eavesdropper also observes the ﬂow corresponding to
the ﬁrst keystroke, they can determine that the ﬁrst keystroke is either c1 or c3 when
the ﬂow is 5 or 15, respectively. Consequentially, the eavesdropper can eventually infer
the queried string by combining these observations.
⊡

88
W.M. Liu et al.
Table 5. Padding in the MVMD Case
c1
c2
c3
c4
c1
5
10
20
80
50
c2
10
40
70
30
60
c3
15
65
15
45
75
c4
20
35
55
85
25
One seemingly valid solution is padding the ﬂow-vector for each keystroke so that
2-indistinguishability is satisﬁed separately for each keystroke. Unfortunately, this will
fail to satisfy 2-indistinguishability. To pad trafﬁc for the ﬁrst keystroke, the optimal
solution is to partition {5, 10, 15, 20} into two padding groups, {5, 10} and {15, 20}.
However, when the eavesdropper observes the ﬂow corresponding to the ﬁrst keystroke,
he/she can still determine it must be either c1 or c3 when the size is 10 or 20, respec-
tively, because only when the ﬁrst keystroke starts with c1 or c3 can the ﬂow for second
keystroke be padded to 15. Therefore, the eavesdropper will eliminate c2 and c4 from
possible guesses, which violates 2-indistinguishability.
Another seemingly viable solution is to ﬁrst collect all vector-sequences for the se-
quence of keystrokes and then pad them such that the current input string as a whole
cannot be distinguished from at least k−1 others. Unfortunately, such an approach can-
not guarantee the privacy property, either. First, the auto-suggestion feature requires the
server to immediately respond to the client upon each single keystroke. Second, when
receiving a single keystroke, the server cannot predict what would be the next input
and hence cannot decide which padding option is suitable. For example, suppose the
ﬂow corresponding to c1 in c1c2 should be padded to 10, while in c1c3 to 15. When the
server receives c1, it cannot determine whether to pad c1 to 10 or to 15.
The above challenges mainly arise due to the approach of padding each vector-set
independently. We now propose a different approach. Intuitively, the partitioning of a
vector-set corresponding to each action will respect the partitioning results of all the
previous actions in the same action-sequence. More precisely, the padding of different
vector-sets is correlated based on following two conditions.
- Given two t-sized vector-sequences ⃗v1 and ⃗v2, any preﬁx pre(⃗v1, i) and pre(⃗v2, i)
(i ∈[2, t]), can be padded together only if ∀(j < i), pre(⃗v1, j) and pre(⃗v2, j) are
padded together.
- For any two t-sized action-sequences⃗a1 and⃗a2 and correspondingvector-sequences
⃗v1 and ⃗v2, if pre(⃗a1, i) = pre(⃗a2, i)(i ∈[1, t]), then pre(⃗v1, i) and pre(⃗v2, i) must
be padded together.
Once a partition satisﬁes aforementioned conditions, no matter how an eavesdropper
analyzes trafﬁc information, either for an action alone or combining multiple observa-
tions of previous actions, the mental image about the actual action-sequence (or any of
its subsets) remains the same (detailed proof is omitted due to space limitations). Due
to the similarity between the conditions and a related concept in graph theory, we call a
partition satisfying such conditions the oriented-forest partition.
Problem 3 (MVMD problem). Given a vector-action set VA = (VA1, VA2, . . . , VAt)
where VAi = (Vi, Ai) (i ∈[1, t]), the privacy property k ≤|Vt|, ﬁnd the partition

k-Indistinguishable Trafﬁc Padding in Web Applications
89
P VAi on VAi such that the corresponding partition P Vi = {P i
1, P i
2, . . . , P i
mi} on Vi
satisﬁes
- ∀((i ∈[1, t −1]) ∧(j ∈[1, mi]))

|P i
j| ≥k,
if (|Vi| ≥k),
|P i
1| = |Vi|,
if (|Vi| < k);
- ∀(j ∈[1, mt]), |P t
j | ≥k;
- The sequence of P Vi is an oriented-forest partition;
- The total padding cost of P Vi (i ∈[1, t]) is minimal.
⊡
Obviously, Problem 3 is also NP-complete when k ≥3 since Problem 2 is special case
of Problem 3.
4
The Algorithms
In this section, we design three algorithms for partitioning the vector-action set into
padding groups to satisfy a given privacy requirement. Our intention is not to design an
exhaustive list of solutions but rather to demonstrate the existence of abundant possibil-
ities in approaching this PPTP issue.
4.1
The svsdSimple Algorithm
The main intention of presenting the svsdSimple algorithm is to show that, when apply-
ing k-indistinguishability to PPTP problems, an algorithm may sometimes be devised
in a very straightforward way, and yet achieve a dramatic reduction in costs when com-
pared to existing approaches (as shown in the next section). The svsdSimple algorithm
shown in Table 6 basically attempts to minimize the cardinality of padding groups in the
SVSD case. Note that when the cardinality of vector-action set is less than the privacy
property k, there is no solution to satisfy the privacy property. In such cases, our algo-
rithms will simply exit, which will not be explicitly shown in each algorithm hereafter.
Table 6. The svsdSimple Algorithm for SVSD-Problem
Algorithm svsdSimple
Input: a vector-action set VA, the privacy property k;
Output: the partition P VA of VA;
Method:
1.
Let P VA = φ;
2.
Let SVA be the sequence of VA in a non-decreasing order of V ;
3.
Let N = |SVA|
k
;
4.
For i = 0 to N −2
5.
Let Pi = (i+1)×k−1
j=i×k
(SVA[j]);
6.
Create partition Pi on P VA;
7.
Create PN−1 = |SVA|−1
j=(N−1)×k(SVA[j]) on P VA;
8.
Return P VA;

90
W.M. Liu et al.
More speciﬁcally, svsdSimple ﬁrst sorts each single ﬂow in the ﬂow-vector into a
non-decreasing order of the ﬂows, and then selects k pairs of (ﬂow-vector, action) each
time in that order to form a padding group. This is repeated until the number of pairs is
less than k. The remainder of pairs is simply appended to the last padding group.
The computational complexity is O(nlogn) where n = |VA|, since step 2 costs
O(nlogn) time and each (ﬂow-vector, action) pair is considered once for the remaining
steps.
4.2
The svmdGreedy Algorithm
The svmdGreedy algorithm, which aims at both SVSD and SVMD problems, is shown
in Table 7. Roughly speaking, the svmdGreedy recursively divides the padding group
Pi in P VA, where |Pi| ≥2×k, into two padding groups Pi1 and Pi2 until the cardinality
of any padding group in P VA is less than 2 × k. When svmdGreedy splits a padding
group Pi(VAi) into two, these resultant padding groups, Pi1 and Pi2, must satisfy that
(Pi1 ∪Pi2 = Pi) ∧(Pi1 ∩Pi2 = φ) ∧(|Pi1| ≥k) ∧(|Pi2| ≥k). Obviously, there
must exist many solutions of Pi1 and Pi2. svmdGreedy limits the optimizing process
insides a subset of possible solutions as follows. For each ﬂow, svmdGreedy ﬁrst sorts
the ﬂow-vectors in non-decreasing order of that ﬂow, then splits Pi into Pi1 and Pi2
at position pos in the sorted sequence of ﬂow-vectors where (pos ∈[k, |Pi| −k]).
There are totally (np × (|Pi| −2 × k)) possible solutions for all ﬂows in the ﬂow-
vector, where np is the number of ﬂows in ﬂow-vector. SvmdGreedy ﬁnally selects the
one with minimal padding cost among this set of solutions. Clearly, this algorithm can
solve SVSD-problem when np is set to be 1.
Table 7. The svmdGreedy Algorithm For SVMD-Problem
Algorithm svmdGreedy
Input: a vector-action set VA, the privacy property k;
Output: the partition P VA of VA;
Method:
1. If(|VA| < 2 × k)
2.
Create in P VA the VA;
3.
Return;
4. Let np be the number of ﬂows in ﬂow-vector;
5. For p = 1 to np
6.
Let SVA
p
be the sequence of VA in the non-decreasing order of the pth ﬂow in the vector;
7.
For i = k to |SVA
p
| −k
8.
Let costp,i as the cost when SV
p is split at position i;
9.
Let costp be a pair (c, i) where c is the minimal in (costp,i) and i is the position;
10.Let cost be a triple (c, p, i) where c is the minimal in c of costp(p ∈[1, np]), and
p and i are the corresponding p and i;
11.Split SVA
cost.p into VA1 and VA2 at position cost.i;
12.Return svmdGreedy(VA1);
13.Return svmdGreedy(VA2);
The svmdGreedy algorithm has an O(np × n2) time complexity in the worst case
(each time, the algorithm splits Pi into k-size Pi1 and (|Pi| −k)-size Pi2), and O(np ×
n × logn) in average cases (each time, the algorithm halves Pi), where n = |VA|.

k-Indistinguishable Trafﬁc Padding in Web Applications
91
4.3
The mvmdGreedy Algorithm
Both svsdSimple and svmdGreedy algorithms tackle cases where each action-sequence
consists of a single action (correspondingly, each vector-sequence consists of a single
ﬂow-vector). Our intention now in devising the mvmdGreedy is to demonstrate how
the two conditions mentioned in Section 3.3 facilitate the algorithm design. In this al-
gorithm, we extend PPDP solutions to a sequence of inter-dependent vector-action sets.
The only constraint in partitioning vector-action set VAi is to ensure all ﬂow-vectors in
a padding group should have their preﬁx in an identical padding group of VAi−1.
The mvmdGreedy algorithm for MVMD-Problem is shown in Table 8. Roughly
speaking, mvmdGreedy partitions each vector-action set in the sequence in the given
order, each for the ﬂow-vector corresponding to an action in an action-sequence. More
speciﬁcally, mvmdGreedy applies svmdGreedy to partition the ﬁrst vector-action set in
the sequence. For each remaining vector-action set VAi, mvmdGreedy ﬁrst partitions
it into |P VAi−1| number of padding groups based on the adjacent-preﬁx of the ﬂow-
vectors, and then applies svmdGreedy to further partition these padding groups.
Similarly, the mvmdGreedy algorithm also has an O(np×n2) time complexity in the
worst case (each time, the algorithm splits VAi into k-size VAi1 and (|VAi| −k)-size
VAi2), and O(np × n × logn) in average cases (each time, the algorithm halves VA),
where n is the total number of ﬂow-vectors in those vector-sets.
Table 8. The mvmdGreedy Algorithm For MVMD-Problem
Algorithm mvmdGreedy
Input: a t-size sequence D of vector-action sets, the privacy property k;
Output: the partition P D of D;
Method:
1. Let D = (VA1, VA2, . . . , VAt);
2. Let P 1 = svmdGreedy(VA1, k);
3. For each (w ∈[1, |P 1|]), assign group G1
w ∈P 1 a unique gid = w;
4. For i = 2 to t
5.
Create in P i |P i−1| number of empty groups Gi
w(w ∈[1, |P i−1|]);
6.
For each via in VAi
7.
Let w be the gid of the group Gi−1
w
in P i−1 that the preﬁx of via in VAi−1 belongs to;
8.
Insert via into Gi
w;
9.
For each (w ∈[1, |P i−1|])
10.
P i = (P i \ Gi
w) ∪svmdGreedy(Gi
w, k);
11. For each (w ∈[1, |P i|]), assign group Gi
w ∈P i a unique gid = w;
12.Return P D = {P i : 1 ≤i ≤t};
5
Evaluation
In this section, we evaluate the effectiveness of our solutions and efﬁciency through
experiments with real world Web applications. Section 5.1 ﬁrst elaborates on the ex-
perimental settings. Then, Section 5.2, 5.3, and 5.4 present experimental results of the
communication, computation, and processing overhead, respectively.

92
W.M. Liu et al.
5.1
Experimental Setting
We collect testing vector-action sets from two real-world web applications, a popular
search engine engineB (where users’ searching keyword needs to be protected) and
an authoritative drug information system drugB from a national institute (where users’
possible health information need to be protected). Specially, for engineB, we collect
ﬂow-vectors with respect to query suggestion widget for all possible combinations of
four letters by crafting requests to simulate the normal AJAX connection request. For
drugB, we collect the vector-action set for all the drug information by mouse-selecting
following the application’s three-level tree-hierarchical navigation. Such data can be
collected by acting as a normal user of the applications without having to know internal
details of the applications. For our experiment, these data are collected using separate
programs whose efﬁciency is not our main concern in this paper.
Information about data cardinality and action levels is shown in Table 9(a), and infor-
mation about the distribution and distinct number of data sizes is shown in Table 9(b).
We observe that the ﬂows of drugB are more diverse than those of engineB evidenced
by the standard deviations (σ) of the ﬂows. The ﬂows of drugB are also larger than
those of engineB based on their means (μ). Besides, the ﬂows of drugB are much
more disparate in values than those of engineB. For example, there are only 889 differ-
ent ﬂow sizes among 456976 ﬂow-vectors in engineB, while there are 1015 different
among 4883 vectors in drugB. Later we will show the effect of these different charac-
teristics of ﬂows on the costs.
Table 9. Flow Data Outline of engineB and drugB
engineB drugB
Level Number
4
3
1
26,
1,
2
676,
27,
3
17.6K,
4883
4
457K
-
Vector Number in Total 475254
5091
(a).The Number of Vectors
Level
engineB
drugB
μ
σ
#
μ
σ
#
1
936 71
23
-
0
1
2
896 70 229 37833 22102
27
3
768 216 741 23411 9796 1015
4
429 173 889
-
-
-
(b). Distribution of Data Sizes
Note that the size information collected through our programs may have integrally
shifted from the original one. However, we argue that such information is still sufﬁ-
cient and reasonable for our experimental evaluation due to following facts. First, the
collected data preserve adequate characteristics of the original data with respect to the
trafﬁc-size distinction. Second, although the length of HTTP request and response may
vary due to different browsers and platforms, the variance is constant for the same
setting and can be determined in advance. Also, the size information may vary when
adopting compression in the web objects or HTTP body. Our solutions regard such
variances as different inputs.
All experiments are conducted on a PC with 2.20GHz Duo CPU and 4GB memory.
We evaluate the overhead of computation, communication, and processing using exe-
cution time, padding cost ratio, and processing cost ratio, respectively. Speciﬁcally, for

k-Indistinguishable Trafﬁc Padding in Web Applications
93
each application, we ﬁrst obtain the total size of all ﬂows ttl for all possible actions
before padding, and then compute the padding cost cost as shown in Deﬁnition 5 after
padding. The padding cost ratio of trafﬁc padding is formulated as cost
ttl . We also count
the number of ﬂows which need to be padded, and then formulate the processing cost
ratio as the percent of ﬂows to be padded among all ﬂows.
5.2
Communication Overhead
We ﬁrst compare the communication overhead of our algorithms against an existing
padding method, namely, packet-size rounding (simply rounding) [9]. We set the round-
ing parameter Δ = 512 and Δ = 5120 for engineB and drugB, respectively. Note that
these Δ values just lead to results satisfying 5-indistinguishability in the padded data,
and are adapted only for the comparison purpose. The ﬁrst set of experiments evaluate
svsdSimple, svmdGreedy, and mvmdGreedy algorithms. To apply the svsdSimple algo-
rithm, we generate two vector-action sets by synthesizing the ﬂow-vectors for the last
action of engineB, drugB, respectively. Note that the svmdGreedy and mvmdGreedy
algorithms are equivalent with length-one action sequences.
Figure 2(a) shows padding cost of each algorithm against k. Compared to round-
ing [9], our algorithms have less padding cost, while svmdGreedy incurs signiﬁcantly
less cost than that of rounding. Table 10(a) shows the details of padding cost overhead
ratio in percentage for k = 192. We observe that our algorithms are superior specially
when the number of ﬂow-vectors in a vector-action set is larger since our algorithms
have high possibility to partition the ﬂow-vectors with close value into padding group.
64
128
192
256
320
20%
40%
EngineB
k
(a). One−Level Action
 
 
64
128
192
256
320
5%
10%
15%
DrugB
64
128
192
256
320
20%
40%
EngineB
k
(b). Many−Level Action
64
128
192
256
320
5%
10%
15%
20%
DrugB
svsdSimple
(Engine)
mvmdGreedy
(Engine)
Rounding
(Engine)
svsdSimple
(Drug)
mvmdGreedy
(Drug)
Rounding
(Drug)
Fig. 2. Padding Cost Overhead Ratio
We then compare the mvmdGreedy with rounding algorithm in the case of action-
sequences of lengths larger than one. Figure 2(b) shows padding cost of our mvmd-
Greedy algorithm and rounding algorithm against k. Packet-size rounding incurs larger
padding cost than mvmdGreedy in all cases. Note that, rounding will get worse when
Δ is set to be larger to minimize an eavesdropper’s capability of inference [9]. For ex-
ample, the padding ratio is 118% for drugB when applying rounding to make all drug

94
W.M. Liu et al.
Table 10. Padding Cost Overhead Ratio When k = 192
Application svsdSimple mvmdGreedy Rounding
EngineB
0.0748
0.0604
39.4043
DrugB
3.0743
1.8097
10.5922
(a). One-level Action
Application mvmdGreedy Rounding
EngineB
3.3297
38.2659
DrugB
2.8864
10.5672
(b). Many-level Action
information indistinguishable. Table 10(b) shows the detail of overhead ratio in per-
centage for k = 192. The reason for mvmdGreedy algorithm has more padding cost
in the case of many-level action than in one-level is as follows. In many-level action,
mvmdGreedy ﬁrst partition each vector-action set (except VA1) into padding groups
based on the preﬁx of actions and regardless of the values of ﬂow-vectors.
5.3
Computational Overhead
Figure 3(a) illustrates the computation time of mvmdGreedy algorithm and rounding
algorithm against the ﬂow data cardinality n. We generate n-sized ﬂow data by synthe-
sizing
n

i(|VAi|) copies of engineB, drugB respectively. We set k = 160 for this set of
experiments, and conduct each experiment 1000 times and then take the average.
1
2
3
4
5
6 X |Engine|
0.5
1
1.5
Cardinality
Computation Overhead (s)
EngineB
 
 
8
16
32
64 X |Drug|
0.10
0.20
DrugB
 
 
mvmdGreedy:engine
Rounding:engine
mvmdGreedy:Drug
Rounding:Drug
(a) Flow Data Cardinality
64
128
192
256
320
1.15
1.2
1.3
1.4
1.5
EngineB
k
Computation Overhead (s)
 
 
64
128
192
256
320
0.135
0.14
0.15
DrugB
Engine
Drug
(b) Privacy Property
Fig. 3. Execution Time
As the results show, the mvmdGreedy algorithm is practically efﬁcient (1.2s for 2.7m
ﬂow-vectors) and the computation time increases slowly with n, although our algorithm
requires slightly more overhead than rounding when it is applied to a single Δ value.
However, this is partly due to the application-agnostic nature of the rounding method,
which results in worse performance in terms of padding cost. Also, as shown in the
previous section, such a method may require to test many Δ values for an optimal
choice since larger values do not guarantee better privacy protection.
We then study computation time against privacy property k on the two synthesized
vector-action sets (6 × engineB and 64 × drugB). As expected, rounding is insensitive
to k since it does not have the concept of k. On the other hand, a tighter upper bound
on the time required for mvmdGreedy is O(np × n × 2k × λ) in the worse case and
O(np × n × log(2k × λ)) in the average case, where λ is the maximal number of

k-Indistinguishable Trafﬁc Padding in Web Applications
95
actions which has the same preﬁx in all action-sequences. Clearly, when λ is O(n),
the computational complexity here is equivalent to that in Section 4.3. The reason for
this tighter upper bound is that mvmdGreedy always feeds a vector-action set with
maximal 2k × λ cardinality to svmdGreedy (except VA1 whose size is 26, a constant,
for searchB), since:
- For each vector-action set VAi, mvmdGreedy ﬁrst partitions it into padding groups
based on the preﬁx of each ﬂow-vector (which has O(|VAi|) solution).
- There are at most 2k adjacent-preﬁxes in same padding group of VAi−1.
Therefore, when 2k × λ ≪n, the execution time of mvmdGreedy algorithms for con-
crete vector-action sets is also a function of k. These two datasets in our experimen-
tal environment satisfy above condition, for example, 26(λ) × 320(k) ≪2.7m for
searchB. In other words, in such case the execution time should be in the range of
[log(2k ×λ), 2k ×λ] times of O(np ×n) which is the execution time of rounding algo-
rithm. Figure 3(b) illustrates the computation time of mvmdGreedy algorithm against
the privacy property k. Interestingly, the computation time increases slowly (from 1.19s
to 1.42s) with k for engineB, and decreases slowly (from 0.147s to 0.136s) for drugB.
Stress that the results are reasonable since both results fall within the expected range.
5.4
Processing Overhead
Our previous discussions have focused on reducing the communication overhead of
padding while ensuring each ﬂow-vector to satisfy the desired privacy property. To
implement trafﬁc padding in an existing Web application, if the HTTPS header or data
is compressed, we can pad after compression, and pad to the header; if header and data
are not compressed, we can pad to the data itself (e.g., spaces of required padding bytes
can be appended to textual data). Clearly, the browser’s TCP/IP stack is responsible
for the header padding, while the original web applications regard the data padding as
64
128
192
256
320
0
0.2
0.4
0.6
0.8
1
k
(a). One−Level Action
 
 
64
128
192
256
320
64
128
192
256
320
0
0.2
0.4
0.6
0.8
1
k
(b). Many−Level Action
64
128
192
256
320
svsdSimple
(Engine)
mvmdGreedy
(Engine)
Rounding
(Engine)
svsdSimple
(Drug)
mvmdGreedy
(Drug)
Rounding
(Drug)
Fig. 4. Processing Cost Overhead Ratio

96
W.M. Liu et al.
normal data. An application can choose to incorporate the padding at different stage of
processing a request. First, an application can consult the outputs of our algorithms for
each request and then pad the ﬂow-vectors on the ﬂy. Second, an application can modify
the original data beforehand based on the outputs of our algorithms such that the privacy
property is satisﬁed under the modiﬁcations. However, padding may incur a processing
cost regardless of which approach to be taken. Therefore, we must aim to minimize the
number of packets to be padded. For this purpose, we evaluate the processing cost ratio,
which captures the proportion of ﬂow-vectors to be padded among all such vectors.
Figure 4 shows the processing cost of each algorithm against k. Rounding algorithm
must pad each ﬂow-vector regardless of the k’s and the applications, while our algo-
rithms have much less cost for engineB and slightly less for drugB. Note that in this
paper our model and algorithm design has focused on minimizing the padding cost
only. We consider reﬁning them for reducing both the padding and processing cost as
our future work.
6
Extension and Discussion
In this section, we ﬁrst present an extension of our model and then discuss the imple-
mentation of our solutions.
6.1
Extension to l-Diversity
We now outline an extension of our model to further show that many existing PPDP
concepts may be adapted to address PPTP issues. Speciﬁcally, we adapt l-diversity [20]
to address cases that no all actions should be treated equally in padding (e.g., some
statistical results regarding the likelihood of different inputs may be publicly known).
We ﬁrst assign an integer weight to each action to represent the possibility it occurs.
A larger weight indicates that the corresponding action is more likely to be performed.
We also slightly change the deﬁnition of vector-action set to include the weight infor-
mation. Then we apply l-diversity to quantify the privacy by ensuring the constraint as
follows. For each padding group, the summation of weights corresponding to the ac-
tions in the group should be at least l times of the maximal weight value in that group.
With the aforementioned revisions, we reformulate the PPTP problem (MVMD prob-
lem in Section 3.3) to satisfy l-diversity instead. Observe that, the reformulated prob-
lem, called diversity problem, is simpliﬁed to Problem 3 if the weights of actions in a
vector-action set are set to be identical and l = k. Thus, the diversity problem is at least
as hard as Problem 3.
Although l-diversity in PPTP shares the same spirit with that in PPDP, algorithms for
PPDP cannot be directly applied here, because in PPDP, many tuples may have the same
sensitive values, whereas any action in an action-set is always unique, and we assign a
weight for each action to distinguish its possibility to be performed from others. The
detail of diversity problem and its algorithms are omitted due to space limitations.
6.2
Implementation Issues
In previous sections, we have presented algorithms for determining the amount of
padding for each ﬂow given the vector-action set. To incorporate our techniques into an

k-Indistinguishable Trafﬁc Padding in Web Applications
97
existing Web application requires following three steps. First, gather information about
possible action-sequences and corresponding vector-sequences in the application. Sec-
ond, feed the vector-action sets into our algorithms to calculate the desired amount of
padding. Third, implement the padding according to the calculated sizes.
The main difference between implementing an existing method, such as rounding,
and the ceiling padding method lies in the second stage. Thus, we have focused on
this stage in this paper. Nonetheless, we have also brieﬂy described how to collect the
vector-action sets in Section 5.1 and how to facilitate the third stage in Section 5.4.
One may question the practicality of gathering information about possible action-
sequences since the number of such sequences can be very large. However, we believe
it is practical for most Web applications due to following facts. First, the aforemen-
tioned side-channel attack on web applications typically arises due to highly interactive
features, such as auto-suggestion. The very existence of such features implies that the
application designer has already proﬁled the domain of possible inputs (that is, action-
sequences) for implementing the feature. Therefore, such information must already ex-
ist in certain forms and can be easily extracted at a low cost. Second, even though a
Web application may take inﬁnite number of inputs, this does not necessarily mean
there would be inﬁnite action-sequences. For example, a search engine like Google
will no longer provide auto-suggestion feature once the query string exceeds a certain
length. Third, all the three steps mentioned above are part of the off-line processing,
and would only need to be repeated when the Web application undergoes a redesign.
We also note that implementing an existing padding method, such as packet-size
rounding, will also need to go through the above three steps if only the padding cost is
to be optimized. For example, without collecting and analyzing the vector-action sets,
a rounding method cannot effectively select the optimal rounding parameter.
7
Related Work
The privacy preserving issue has received signiﬁcant attentions in various domains,
such as, data publishing and data mining [10,24], mobile and wireless network [4,5],
social network [11,22], multiparty computation [21], web applications [6,9,26], and so
on. In the context of privacy-preserving data publishing, since the introduction of the k-
anonymity concept [24,28], much effort has been made on developing efﬁcient privacy-
preserving algorithms [1,16]. Many other models are also proposed to enhance the
k-anonymity, such as l-diversity [20], t-closeness [17]. Recently, differential privacy
[12] has been widely accepted as a strong privacy model for answering statistic queries.
Various side-channel leakages have been extensively studied in the literature. By
measuring the amount of time taken to respond to the queries, an attacker may extract
OpenSSL RSA privacy keys [7]. By differentiating the sounds produced by keys, an
attacker may recognize the key pressed [3]. Ristenpart et al. discover cross-VM infor-
mation leakage on Amazon EC2 [23]. Search histories may be reconstructed by session
hijacking attack [8]. Saponas et al. show the transmission characteristics of encrypted
video streaming may allow attackers to recognize the title of movies [25]. HTTPOS are
proposed to prevent information leakages of encrypted HTTP trafﬁc in [19]. Timing
mitigator is introduced to achieve any given bound on timing channel leakage in [2].

98
W.M. Liu et al.
Closest to our work, Chen et al. in [9] demonstrate through case studies that side-
channel attacks are fundamental to web applications. Our model and solutions provide
ﬁner control over the tradeoff between privacy protection and cost. The model section
of this paper has previously appeared as a short paper in [18] and now we signiﬁcantly
extend it with problem formulation, algorithm design, and experimental evaluations.
8
Conclusion
As Web-based applications become more popular, their security issues will also attract
more attention. In this paper, we have demonstrated an interesting connection between
the trafﬁc padding issue of Web applications and the privacy-preservingdata publishing.
Based on this connection, we have proposed a formal model for quantifying the amount
of privacy protection provided by trafﬁc padding solutions. We have also designed three
algorithms by following the proposed model. Our experiments with both real-world
applications have conﬁrmed the performance of our solutions to be superior to existing
ones in terms of communication and computation overhead. For future research, we
intend to investigate padding approaches for frequently updated vector-action sets, and
the possibility of extrapolating the proposed model and approach to mitigate threats of
other side-channel leaks.
Acknowledgment. The authors thank the anonymous reviewers for their valuable com-
ments. Authors with Concordia University are partially supported by Natural Sciences
and Engineering Research Council of Canada under Discovery Grant N01035, Strate-
gic Project Grant STPGP/396651-2010, and Canada Graduate Scholarship, while Kui’s
research is supported in part by US National Science Foundation through grant CNS-
1117811.
References
1. Aggarwal, G., Feder, T., Kenthapadi, K., Motwani, R., Panigrahy, R., Thomas, D., Zhu, A.:
Anonymizing Tables. In: Eiter, T., Libkin, L. (eds.) ICDT 2005. LNCS, vol. 3363, pp. 246–
258. Springer, Heidelberg (2005)
2. Askarov, A., Zhang, D., Myers, A.C.: Predictive black-box mitigation of timing channels. In:
CCS 2010, pp. 297–307 (2010)
3. Asonov, D., Agrawal, R.: Keyboard acoustic emanations. In: IEEE Symposium on Security
and Privacy, p. 3 (2004)
4. Backes, M., Doychev, G., D¨urmuth, M., K¨opf, B.: Speaker Recognition in Encrypted Voice
Streams. In: Gritzalis, D., Preneel, B., Theoharidou, M. (eds.) ESORICS 2010. LNCS,
vol. 6345, pp. 508–523. Springer, Heidelberg (2010)
5. Bauer, K., McCoy, D., Greenstein, B., Grunwald, D., Sicker, D.: Physical Layer Attacks on
Unlinkability in Wireless LANs. In: Goldberg, I., Atallah, M.J. (eds.) PETS 2009. LNCS,
vol. 5672, pp. 108–127. Springer, Heidelberg (2009)
6. Bilogrevic, I., Jadliwala, M., Kalkan, K., Hubaux, J.-P., Aad, I.: Privacy in Mobile Computing
for Location-Sharing-Based Services. In: Fischer-H¨ubner, S., Hopper, N. (eds.) PETS 2011.
LNCS, vol. 6794, pp. 77–96. Springer, Heidelberg (2011)
7. Brumley, D., Boneh, D.: Remote timing attacks are practical. In: USENIX (2003)

k-Indistinguishable Trafﬁc Padding in Web Applications
99
8. Castelluccia, C., De Cristofaro, E., Perito, D.: Private Information Disclosure from Web
Searches. In: Atallah, M.J., Hopper, N.J. (eds.) PETS 2010. LNCS, vol. 6205, pp. 38–55.
Springer, Heidelberg (2010)
9. Chen, S., Wang, R., Wang, X., Zhang, K.: Side-channel leaks in web applications: A reality
today, a challenge tomorrow. In: IEEE Symposium on Security and Privacy 2010, pp. 191–
206 (2010)
10. Ciriani, V., De Capitani di Vimercati, S., Foresti, S., Samarati, P.: k-anonymous data mining:
A survey. In: Privacy-Preserving Data Mining: Models and Algorithms (2008)
11. Danezis, G., Aura, T., Chen, S., Kıcıman, E.: How to Share Your Favourite Search Results
while Preserving Privacy and Quality. In: Atallah, M.J., Hopper, N.J. (eds.) PETS 2010.
LNCS, vol. 6205, pp. 273–290. Springer, Heidelberg (2010)
12. Dwork, C.: Differential Privacy. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.)
ICALP 2006, Part II. LNCS, vol. 4052, pp. 1–12. Springer, Heidelberg (2006)
13. Fung, B.C.M., Wang, K., Chen, R., Yu, P.S.: Privacy-preserving data publishing: A survey of
recent developments. ACM Comput. Surv. 42, 14:1–14:53 (2010)
14. Kann, V.: Maximum bounded h-matching is max snp-complete. Inf. Process. Lett. 49, 309–
318 (1994)
15. Kanungo, T., Mount, D.M., Netanyahu, N.S., Piatko, C., Silverman, R., Wu, A.Y.: An efﬁ-
cient k-means clustering algorithm: Analysis and implementation. IEEE Trans. Pattern Anal.
Mach. Intell. 24, 881–892 (2002)
16. LeFevre, K., DeWitt, D.J., Ramakrishnan, R.: Incognito: Efﬁcient fulldomain k-anonymity.
In: SIGMOD, pp. 49–60 (2005)
17. Li, N., Li, T., Venkatasubramanian, S.: t-closeness: Privacy beyond k-anonymity and l-
diversity. In: ICDE 2007, pp. 106–115 (2007)
18. Liu, W.M., Wang, L., Cheng, P., Debbabi, M.: Privacy-preserving trafﬁc padding in web-
based applications. In: WPES 2011, pp. 131–136 (2011)
19. Luo, X., Zhou, P., Chan, E.W.W., Lee, W., Chang, R.K.C., Perdisci, R.: Httpos: Sealing
information leaks with browser-side obfuscation of encrypted ﬂows. In: NDSS 2011 (2011)
20. Machanavajjhala, A., Kifer, D., Gehrke, J., Venkitasubramaniam, M.: L-diversity: Privacy
beyond k-anonymity. ACM Trans. Knowl. Discov. Data 1(1), 3 (2007)
21. Nagaraja, S., Jalaparti, V., Caesar, M., Borisov, N.: P3CA: Private Anomaly Detection Across
ISP Networks. In: Fischer-H¨ubner, S., Hopper, N. (eds.) PETS 2011. LNCS, vol. 6794, pp.
38–56. Springer, Heidelberg (2011)
22. Narayanan, A., Shmatikov, V.: De-anonymizing social networks. In: IEEE Symposium on
Security and Privacy 2009, pp. 173–187 (2009)
23. Ristenpart, T., Tromer, E., Shacham, H., Savage, S.: Hey, you, get off of my cloud: exploring
information leakage in third-party compute clouds. In: CCS, pp. 199–212 (2009)
24. Samarati, P.: Protecting respondents’ identities in microdata release. IEEE Trans. on Knowl.
and Data Eng. 13(6), 1010–1027 (2001)
25. Saponas, T.S., Agarwal, S.: Devices that tell on you: Privacy trends in consumer ubiquitous
computing. In: USENIX 2007, pp. 5:1–1:16 (2007)
26. Sun, J., Zhu, X., Zhang, C., Fang, Y.: Hcpp: Cryptography based secure ehr system for patient
privacy and emergency healthcare. In: ICDCS 2011, pp. 373–382 (2011)
27. Sun, Q., Simon, D.R., Wang, Y.M., Russell, W., Padmanabhan, V.N., Qiu, L.: Statistical iden-
tiﬁcation of encrypted web browsing trafﬁc. In: IEEE Symposium on Security and Privacy
(2002)
28. Sweeney, L.: k-anonymity: a model for protecting privacy. International Journal on Uncer-
tainty, Fuzziness and Knowledge-based Systems 10(5), 557–570 (2002)

Spying in the Dark:
TCP and Tor Traﬃc Analysis
Yossi Gilad and Amir Herzberg
Department of Computer Science, Bar Ilan University, Israel
mail@yossigilad.com, amir.herzberg@gmail.com
Abstract. We show how to exploit side-channels to identify clients with-
out eavesdropping on the communication to the server, and without re-
lying on known, distinguishable traﬃc patterns. We present diﬀerent
attacks, utilizing diﬀerent side-channels, for two scenarios: a fully oﬀ-
path attack detecting TCP connections, and an attack detecting Tor
connections by eavesdropping only on the clients.
Our attacks exploit three types of side channels: globally-incrementing
IP identiﬁers, used by some operating systems, e.g., in Windows; packet
processing delays, which depend on TCP state; and bogus-congestion
events, causing impact on TCP’s throughput (via TCP’s congestion con-
trol mechanism). Our attacks can (optionally) also beneﬁt from sequen-
tial port allocation, e.g., deployed in Windows and Linux. The attacks
are practical - we present results of experiments for all attacks in diﬀerent
network environments and scenarios. We also present countermeasures
for these attacks.
1
Introduction
Internet communication is often sensitive, and security measures must be taken
to protect privacy against attackers. The exact measures depend on the exact
threat; in particular, encryption protocols such as TLS [9] are necessary to pro-
tect content from an eavesdropping attacker.
However, encryption is insuﬃcient to prevent traﬃc analysis, and in partic-
ular, to prevent exposure of the identities of the communicating peers. Users
concerned against traﬃc analysis by eavesdropping attackers, use anonymizing
services such as Tor. Other users may simply assume that the adversary is oﬀ-
path (non-eavesdropping), and expect privacy against such (weaker) attackers.
We present three traﬃc-analysis attacks against these scenarios. Two attacks
identify clients that communicate to a speciﬁc server directly over TCP (without
anonymizing intermediaries such as Tor). Such attacks do not require eavesdrop-
ping at all, and may be launched by weak, oﬀ-path attackers, even for commercial
motivations. In fact, since the attacks do not involve eavesdropping, they may
even be deemed to be legal (not wiretapping). We believe technical measures
should (and can) prevent such oﬀ-path traﬃc analysis.
Our third traﬃc analysis attack is against Tor users. It requires eavesdropping
abilities only on the client side, and only spooﬁng abilities on the server side. We
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 100–119, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Spying in the Dark: TCP and Tor Traﬃc Analysis
101
believe that this is an important scenario, since Tor clients are often concerned
about attackers which can eavesdrop on their connection to the Tor relay, since
the client-relay connection may be insecure (e.g., Internet cafe) or controlled by a
potential snoopy organization (employer, government, etc). Our evaluation of this
attack is yet incomplete; however, the preliminary results which we present in this
paper provide a warning about a new attack vector on the Tor anonymity network.
Our attacks exploit diﬀerent side-channels, providing useful information on a
TCP/Tor connection to an oﬀ-path attacker (for Tor, attacker can eavesdrop,
but only on the client). Side channels have been extensively used in attacks on
cryptographic systems, e.g., [22], but also in attacks on privacy, e.g., [13], and
more recently also applied to traﬃc analysis [6,26,35].
Our attacks on direct (i.e., non-anonymous) TCP connections can be viewed
as instances of an attack pattern which we call Query-Probe-Query, illustrated
in Figure 1; this is a generalization of the well-known idle (stealth) scan attack
[25,34], and of the measurement method used in [5]. In the Query-Probe-Query
attack pattern illustrated in Figure 1, the ﬁrst query measures some ‘pre-probe
state’; the probe may cause some change on the state, where the change depends
on the property measured, e.g., whether a speciﬁc client port is open; and the
ﬁnal query measures the ‘post-probe state’. This attack pattern can be applied
with diﬀerent queries and probes, to measure diﬀerent values.
	

	

		







	









 
Fig. 1. Query-Probe-Query oﬀ-path attack
pattern. Attacker uses spoofed source ad-
dress for probe.
In our implementations, each probe
is a packet, or few packets, sent to
the client C. The probes test for some
event e, such that if e holds then C will
send some packet(s), and otherwise he
will not send a packet (or send less
packets). We use the pre/post probe
response to infer on e: we measure the
increase in the IP-ID counter, or mea-
sure time between receipt of the two
response packets (1b. and 3b in Fig-
ure 1). Table 1 summarizes our results
for traﬃc analysis on direct TCP con-
nections.
Table 1. Results for probing direct (TCP) connections. Attacker location is relative
to the client (victim). Success rates can be improved by repeating the attack.
Side
Adversary Location
Channel
Local
Remote
Success Rate
IP-ID (Section 3)
1
0.92
Timing (Section 4)
0.74
0.58
Attack Duration (seconds)/
Data Sent (MB)
IP-ID (Section 3) 14/0.6
38/2
Timing (Section 4) 70/5
50/4

102
Y. Gilad and A. Herzberg
In the recent years, there is growth in the use of anonymity mechanisms such
as Tor [10]. Tor is a low latency, circuit based, anonymity network that is widely
used and increasing in popularity (according to [1], increase of about 70% in
recent year). Tor is designed to ensure traﬃc privacy, even when the adversary
is able to eavesdrop on either C or S. However, due to its low latency, Tor cannot
ensure traﬃc privacy against attackers eavesdropping at both ends (C and S),
see discussion on related works below.
We show how an adversary capable of eavesdropping on the client, C, but not on
the server, S, is able to detect that C is communicating with S. This attack uses a
side channel as well, but does not follow the query-probe-query pattern; here the
attacker causes a reduction in the rate of packets that would reach C in case that
he is communicating with S, and then tests whether reduction had occurred.
Our attacks on Tor are active, i.e., involve sending packets to Tor exit relay.
When there is a known, distinct traﬃc pattern to the communication of speciﬁc
server (website ﬁngerprint), then alternative passive attacks may be applicable
as well, e.g., [18,19,28]. It may be possible to extend our techniques to also take
advantage of such site ﬁngerprint, when available.
1.1
Related Works
IP-ID Side-Channel and Oﬀ-Path Traﬃc Analysis. We show how the use
of globally-incrementing IP-ID ﬁeld in IP headers, provides side-channel allowing
eﬀective oﬀ-path traﬃc analysis. The use of globally-incrementing IP-ID is recog-
nized, in [17], as a common practice with known security implications; e.g., both
globally-incrementing and per-destination incrementing IP-ID allow intercep-
tion, injection and discarding of fragmented traﬃc [15]. Globally-incrementing
IP-ID can allow estimation of the number of packets sent [32], stealth-scan for
open ports (idle scan) [31] and counting hosts behind NAT [5].
The technique that we present for the case of a client that uses a globally-
incrementing IP ID and is not connected via a ﬁrewall/NAT (see Figure 1) is a
variant of idle-scan [25,34]. The diﬀerence is that while idle-scan probes a server
for an open (i.e., ‘listening’) port, our attack probes a client for a connection
with a server.
The only other previous work we found that performs traﬃc analysis by oﬀ-path
attacker, using a side-channel, is [19]. Their attack is based on detecting changes in
the round trip delays from the attacker to the DSL router; this is a rather crude
side channel, much less eﬃcient than both the IP-ID and the timing side chan-
nels we use. Indeed, they only present detection of whether a client is browsing or
playing a video, based on the signiﬁcant diﬀerence in bandwidth, and assuming no
other traﬃc. Our results dramatically improve the impact of detection compared
to theirs, provided that the attacker can communicate with the clients.
Tor Traﬃc Analysis. Low-latency anonymity networks are known to be vul-
nerable to traﬃc correlation attacks by an attacker that eavesdrops on both
ends; this problem, and possible countermeasures, was studied in several works,

Spying in the Dark: TCP and Tor Traﬃc Analysis
103
e.g., [7,24,36]; eﬃciency and accuracy can signiﬁcantly improve if attacker can
also manipulate traﬃc at the exit relay, see [30]. Indeed, Tor designers are well
aware of its inability to properly protect against an attacker (eavesdropping) at
both ends of the circuit.
Other attacks manipulate the traﬃc at the server or the last (exit) relay in the
circuit, and use diﬀerent techniques to detect the relay along the path based on
delays [6,12,27]. These works assume that the attacker controls the server or the
exit relay, but do not require client-side eavesdropping. In contrast, our attack
on Tor requires client-side eavesdropping, but does not require control over the
server or exit relay. An obvious challenge is to combine the results, and identify
clients without controlling server or exit relay, and without eavesdropping at all.
Our traﬃc detection attacks on TCP may be applicable.
1.2
Our Contributions
The main contribution of this paper is identiﬁcation and analysis of side channels
in the TCP/IP suite and their practical implications on privacy, as we verify in
experiments. We provide practical countermeasures to the problems that we
identify, these allow quick patching at the ﬁrewall level and require no changes
to hosts or core operating system services.
This work motivates use of cryptography in lower network layers and in partic-
ular IPsec [20] as we show that higher network layer solutions such as SSL/TLS
do not prevent blind traﬃc analysis.
1.3
Paper Organization
In Section 2 we present our attacker models and the scenarios that we consider,
we also present the criteria we use to measure the eﬀectiveness of the attacks. Sec-
tions 3, 4 present the global-ID and timing side channels; both sections provide
results of empirical experiments. Section 5 presents our attack on Tor and cor-
responding experiments. Section 6 presents practical defenses. Finally, Section 7
presents our conclusions from this work, as well as future research directions.
2
Model
Let C and S be communicating TCP client and server (respectively). We consider
two types of adversaries, depending on how C and S are connected. In Sections
3 and 4, we consider the case that C and S have a direct TCP connection. In
Section 5, C connects to S through the onion routing anonymity network, Tor
[10]; i.e., C communicates with S via a circuit of relays (proxies). The goal of
the attacker is to identify clients who connect to a server S. We identify S using
its IP address and port.
We consider two types of attackers: Mallory, an oﬀ-path adversary, and Eve,
an eavesdropping adversary. The attackers can send spoofed packets, i.e., packets
with fake (spoofed) sender IP address. Due to ingress ﬁltering [4,14,21] and other

104
Y. Gilad and A. Herzberg
anti-spooﬁng measures, IP spooﬁng is less commonly available than before, but
still feasible, see [2,11]. Apparently, there is still a signiﬁcant number of ISPs
that do not perform ingress ﬁltering for their clients (especially to multihomed
customers). Furthermore, with the growing concern of cyberwarfare and cyber-
crime, some ISPs may intentionally support spooﬁng. Hence, it is still reasonable
to assume spooﬁng ability.
We describe both adversary models in Sections 2.1 and 2.2 below. Section 2.3
presents the criteria we use to evaluate the attacks we present.
2.1
Mallory - Oﬀ-path Adversary


	




Fig. 2. C is surﬁng in both Mallory and S’s
sites, Mallory tries to detect whether there
is a connection between C and S
We assume that C
visits a web-
site that Mallory controls, denoted
www.mallory.com. Mallory uses this
(legitimate)
connection,
to
probe
whether C has any connections S, see
Figure 2.
We
consider
three
variants
of
Mallory, as illustrated in Figure 3:
with-C, near-C and remote. These dif-
fer with respect to Mallory’s abilities
to communicate with C; the greater
the distance, the more likely it is
that packet loss or reordering oc-
curs, decreasing the quality of the side
channels.


		
	






	
		

		
	
		
Fig. 3. Three variants of the Mallory adver-
sary
The with-C and near-C attackers
are located near the client (C); the
diﬀerence between them is that the
with-C adversary directly communi-
cates with the client, allowing Mallory
to take advantage of Windows glob-
ally incrementing port allocation (if C
runs Windows). When the adversary
and C communicate via a NAT (near-
C or remote), we assume that the NAT
uses per destination incremental as-
signment of external ports (e.g., as in the widely-used IP-tables NAT/Firewall
provided in Linux). See in Section 3 how we exploit diﬀerent client port alloca-
tion techniques. Finally, the remote Mallory attacker simulates an adversary that
communicates with the clients from a remote location, i.e., via a high latency,
jittery and lossy channel.
2.2
Eve - Adversary for Anonymized Connections
In the attacks on Tor, we consider the adversary Eve who is able to eavesdrop
on many clients that use Tor, however, Eve cannot eavesdrop on the servers

Spying in the Dark: TCP and Tor Traﬃc Analysis
105
(see Figure 4). Such an adversary may include a government or an employer,
spying on citizens or employees. Eve’s goal is to detect which of the clients is
communicating (using Tor) with a particular watched/restricted site, S.
2.3
Attack Evaluation Criteria
	






	
	








Fig. 4. Eve identiﬁes that some of the
clients she eavesdrops on are using Tor and
wants to detect which of them is commu-
nicating with www.restricted.com. C (Cm)
connects to www.restricted.com via a circuit
of 3 relays.
In addition to measuring the success,
false positive and false negative rates,
we consider two additional measures.
The ﬁrst measure is the time that
an adversary (with some reasonable
constant bandwidth) needs to run the
attack in order to reach a particu-
lar success probability for detecting
a connection. This value also pro-
vides the minimal detectable connec-
tion time. The second measure is the
average amount of data per victim
that the attacker is required to send
to reach a particular success rate.
3
Globally-Incrementing Identiﬁer Based Traﬃc Analysis
This section presents a probing technique that allows an oﬀ-path (blind) adver-
sary, Mallory, to identify a connection between a client C and a server S when C
uses a globally incrementing IP identiﬁer (IP-ID)1. This side channel is only ap-
plicable when the TCP connection is over IPv4, since in IPv6 [8] the IP-ID ﬁeld
is only speciﬁed in fragmented traﬃc and TCP packets are rarely fragmented.
In the following section we introduce a general technique that does not rely on
IP-ID and also applies to IPv6.
A globally-incrementing identiﬁer is not really hidden from Mallory, who can
usually learn its value simply by receiving some packet from the victim. A glob-
ally incrementing IP identiﬁer is used in all Windows versions we tested (includ-
ing XP, Vista and 7) and is also the default conﬁguration in FreeBSD; clients
running these systems are vulnerable to the attack below. The vast deployment
of Windows on client machines (more than 70% according to browser user-agent
based surveys, see [33]) makes IP-ID attack vector very practical.
Section 3.1 deﬁnes a port test that uses the leakage in the IP-ID ﬁeld to detect
whether C is communicating with S through a tested port. The test depends on
whether C is connected to the network through a NAT or a stateful ﬁrewall that
keeps track of existing connections; the test used when C is connected through a
NAT/ﬁrewall device the attack is a bit simpler. We believe that this is the more
common scenario, since recent versions of Windows (XP SP2 and later) ship with a
1 S’s IP-ID implementation does not inﬂuence the probing technique.

106
Y. Gilad and A. Herzberg
built in (stateful) ﬁrewall that is enabled by default, and furthermore, use of NAT
devices in small local area networks connecting clients to the Internet is common.
Due to space limitations we describe only this test and include the test for the
complementary scenario (no ﬁrewall/NAT) in an online technical report [16].
In Section 3.2 we describe how Mallory can identify a relatively small set of
client ports to test for a connection with S; Mallory performs the port-test for
all of them. Section 3.3 presents our experimental setup and empirical results.
3.1
Port-test for a Client Behind a Firewall/NAT
According to the TCP speciﬁcation [29] (Section 3.9, bottom of page 69), the
ﬁrst check that a recipient conducts on an incoming packet, in case it belongs to
an established connection, validates that the sequence number is within the con-
gestion window. If this check ﬁnds the packet invalid, then the recipient discards
the packet and sends a duplicate Ack feedback. A stateful ﬁrewall or NAT device
connecting C to the network keeps track of existing connections and processes all
incoming packets before they reach C. We use the following observation: incom-
ing packets that do not belong to an established connection will be discarded
before reaching C (by ﬁrewall/NAT), whereas packets that belong to an existing
connection, but specify arbitrary (probably invalid) sequence numbers will reach
C who replies with a duplicate Ack.



	
	


		






!"
#	




$
#%
%
%
%
#		


##	

#
##


#


#$#
#	
	


&!'(
#$
	'

	'

	'

	'

	'

")!"*
Fig. 5. Two iterations of port test
The port test for the case of ﬁre-
wall/NAT deploying client is accord-
ing to the general query-probe-query
pattern. The probe speciﬁes S’s ad-
dress and port as source (i.e., probe
is spoofed) and C’s address as des-
tination, Mallory speciﬁes a diﬀerent
destination port in each test. Fig-
ure 5 illustrates two iterations of the
port test: in the ﬁrst iteration, the
ﬁrewall/NAT blocks the probe packet
(i.e., no connection through the tested
port). In the second iteration, the
probe speciﬁes existing connection pa-
rameters (IP addresses and ports) and therefore reaches C who processes the
probe and sends a duplicate Ack to S.
Notice that since the probe packet appears to be from S (in case it speciﬁes a
valid 4-tuple), it is diﬃcult to block the probe in ﬁrewalls without blocking the
legitimate connection that C has with S.
When C uses a global identiﬁer, the diﬀerence in the IP-ID ﬁeld in C’s re-
sponses to Mallory indicates whether C had sent a packet in response to the
probe (duplicate Ack). If Mallory identiﬁes that C had sent a packet, then it is
likely that C is communicating with S via the tested port; however, the identiﬁer
may have increased since C had sent an independent packet to some other peer.

Spying in the Dark: TCP and Tor Traﬃc Analysis
107
Repeating this test several times allows Mallory to eﬃciently detect whether C
is connected to S and reveal C’s, see empirical evaluation below.
We keep a ‘score’ for each possible port, and increment a speciﬁc port’s score
by 1 point for every test that seems to indicate that there is a connection through
that port. We conduct r > 1 rounds of the attack, where each port is tested.
Finally, we decide that there is a connection if there is a port with a score higher
than a threshold, TH.
Some ﬁrewalls have an option to randomize the IP-ID; our tests would, of
course, fail if the packets pass through such randomizing ﬁrewall. The attack
we describe in the following section applies even in this scenario (but is less
accurate).
Implementing Test Queries/Responses. Our attacks use packets that
Mallory receives from C to learn the eﬀect of the (spoofed) probe packet. Mallory
can cause C to send her such packets by using the legitimate TCP connection
that she has with C: a query is some short data packet that Mallory sends to C,
the response is the C’s acknowledgment sent back to Mallory. This allows Mallory
to bypass typical ﬁrewall defenses (e.g., Windows), since all packets in the test
appear to belong to legitimate connections (requests to C-Mallory connection,
probe to C-S connection). See further details in the technical report [16].
3.2
Improving the Search: Client Port Allocation Algorithms
The port test that we presented allows Mallory to test whether the client has a
connection to some server via a speciﬁc port. There are 216 possible ports that
C might use to communicate with S. However, common client port allocation
paradigms allow more eﬃcient attacks.
Below we present two common paradigms and methods to reduce the number
of tests for each of them.
Globally Incrementing. the client port is incremented for every new con-
nection (initialized to a random value) Algorithm 1 in [23] describes the imple-
mentation. This approach is used in Windows and FreeBSD. If C uses this port
allocation paradigm, then recent connections that the client forms are likely to
use ‘close’ ports to that C uses in the connection with Mallory. Hence, Mallory
can test only these ports.
Per-server Incrementing. the client port is incremented for every new con-
nection with the server. Connections to diﬀerent servers use diﬀerent counters.
This approach is used in Linux; Algorithm 3 in [23] describes the implementa-
tion. The previous ‘trick’ we presented does not work in this case since the port
that C uses for the connection with Mallory does not correlate to that C uses
to communicate with S. However, we can still use the counter property of this
paradigm: Mallory causes C to create x ‘dummy’ connections to S (we explain
how below); since these connections all share the same counter, they are sequen-
tial. Hence, Mallory can test every port y = 0 (mod x) and identify p, a port C
that uses to communicate with S. Next, Mallory checks all ports in the interval

108
Y. Gilad and A. Herzberg
[p −x, p + x] and checks whether there are at least x + 1 connection ports. If
yes, then C has an ‘independent’ connection with S. In this method, the attacker
would test roughly 216
x diﬀerent ports.
In is left to describe how Mallory causes C to establish multiple connections
with S. Since C is in Mallory’s site, she can run a script (in the browser sand-
box) on C. This script, while very limited, can open connections with other
servers to dynamically embed remote objects. We use it to open connections to
www1.mallory.com,. . . ,wwwx.mallory.com which are domains that Mallory con-
trols. Since Mallory controls the DNS records for these domains, she sets each of
these records to point to the same IP, that of S. Browsers open a new connec-
tion for each domain (regardless of its IP address); hence, this technique, which
we veriﬁed on Internet Explorer, Firefox and Chrome, opens x new connections
to S.
The typical limitation of x is the number of connections that a browser can
have simultaneously; this limitation is typically one or few dozens; e.g., 16 in
Firefox. In our experiments below and in the following section, we use x = 10.
3.3
Empirical Evaluation
Setup. In our empirical evaluation, the client network is a class C subnet that
has 5 clients running Windows 7, each of them sends on average 64 packets
per second to other peers in the subnet (these packets are short, to simulate
clients that usually send Ack packets or short requests). Mallory probes one of
the clients in the network, C, who connects to her (malicious) website. Mallory’s
bandwidth is limited to 10 mbps. We used the network topology illustrated in
Figure 3, network nodes are connected through switch devices. The NAT device
in the network topology is a Linux machine (kernel version 2.6.35) running IP-
tables (version 1.4.4). The server machine runs Linux (kernel version 2.6.35) and
uses an Apache web-server (version 2.2.14). When we evaluate the attack for the
‘Remote Attacker’ scenario, the adversary communicates with the clients via
a traﬃc shaper that induces high latency (200ms), signiﬁcant loss probability
(0.5%) and jitter (1-10 milliseconds).
Evaluation. We ﬁrst evaluated the attack in case that C is communicating with
S. We compared between the score of the ‘connection port’ (i.e., port that C uses
for the connection) to that of the best appearing non-connection port (i.e., port
with the highest score that is not the connection port) in each round (repetition
of the attack, see discussion above); note that the highest scoring non-connection
port may change between rounds.
Figure 6 shows results for near-C and remote attackers. In both environments,
the score of the connection port was well above 50% of the maximal score,
certainly after ﬁve or more reounds; hence, for eﬃciency, we can continue testing
only ‘high scoring’ ports in advanced rounds. Namely, a port is tested in the next
round only if its current score is above 50% of the maximal possible score.

Spying in the Dark: TCP and Tor Traﬃc Analysis
109
0
2
4
6
8
10
0
2
4
6
8
10
Score
Number of Rounds
Connection Port (near-C)
Best Non-Connection Port (near-C)
Connection Port (remote)
Best Non-Connection Port (remote)
Fig. 6. Global-ID attack. Comparison of a
connection port to that of the highest scor-
ing non connection port as a function of
round number. Each measurement is an av-
erage of 10 runs, error-bars mark the stan-
dard deviation values.
We
implemented
an
adversarial
website that presents its clients a re-
quest to arbitrarily decide whether to
connect (‘surf’) to a third-party web-
site, S; our website attempted to de-
tect the clients’ choice. We used an
automatic client, C, that chooses to
connect to S with probability 1
2 and
implemented the port-test above.
The choice of whether there is a
connection between C and S is accord-
ing to a threshold over the ﬁnal score
of the ports. Namely, if there exists a
port with a score over this threshold,
then we identify that there is a con-
nection. Figure 6 shows that a choice of 70% of the maximal possible score as
a threshold provides a good seperation between the connection port (in case it
exists) and other ports. Figures 7 - 9 show the success rate in detecting whether
C communicates with S for diﬀerent adversary locations as a function of the du-
ration of the attack. Figure 10 compares the average amount of data that Mallory
sends (per victim) to reach diﬀerent success rates and for diﬀerent locations in
the network.
In Figures 7 - 10, the measurements are the average of 50 runs; error-bars mark
the standard deviation values (for readability, not all measurements specify the
error bars). Note that the thresholds that we have used in our evaluation may
not work as well in other scenarios, e.g., when the client sends much more than
64 packets per second the thresholds should be higher.
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
12
14
16
Rate
Attack Duration (seconds)
Success
False Positives
False Negatives
Fig. 7. Global-ID attack, with-C attacker
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
Rate
Attack Duration (seconds)
Success
False Positives
False Negatives
Fig. 8. Global-ID attack, near-C attacker
4
Time-Based Traﬃc Analysis
The globally incrementing IP-ID side channel, presented in Section 3, exploits
an operating system ﬂaw. In this section we explore a more generic, timing
based, side channel that is applicable when C is behind a ﬁrewall or a NAT. We

110
Y. Gilad and A. Herzberg
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
Rate
Attack Duration (seconds)
Success
False Positives
False Negatives
Fig. 9. Global-ID attack, remote attacker
0
0.5
1
1.5
2
2.5
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Data Sent (MB)
Success Rate
with-C attacker
near-C attacker
remote attacker
Fig. 10. Amount of data Mallory sends as
a function of her success rate
deﬁne below a new port test which resembles the IP-ID based port test and is
illustrated in Figure 5 as well.
The timing attack is based on the following observation: if C is protected by
a ﬁrewall or connects through a NAT device, then in case that Mallory tests the
correct port, C sends an additional packet to S (response to the probe); this
delays processing of following packets, and in particular the post-probe query;
see illustration in Figure 5. We use this delay to identify the connection.
4.1
Timing-Based Port Test
A signiﬁcant challenge is the jitter in the network, i.e., latencies may vary while
testing diﬀerent ports. Thus, identifying the longest time diﬀerence between
two responses and testing whether it is over a threshold is likely to produce an
incorrect result. We cope with this challenge by relatively comparing ports: we
assign each port to a small group of s arbitrary ports.
Ports in each group are tested one after the other; we assume that jitter does
not vary much during the short time interval of testing a speciﬁc (small) group.
After testing a group, each port is assigned with a relative rank according to
the time diﬀerence between responses in the corresponding port-test; the lower
the (group-relative) rank, the greater the time diﬀerence and the more likely is
a connection through that port. We conduct several rounds of this attack (to
reduce the probability of errors).
Similarly to the attack presented in the previous section, we keep a score for
each port and after each round increase a port’s score according to its rank:
denote by σi the number of points that a port gains if it has rank i within the
group, these weights are normalized; i.e., s
i=1 σi = 1, and for every i < j,
σi ≥σj. The values of s and the vector σ = (σ1, · · · , σs) depend on the channel
between Mallory and C. We employ a machine learning approach (genetic algo-
rithm) to learn appropriate value for the vector σ; see details of the algorithm in
[16]. Let μ, μ′ denote the expected scores of connection and non connection ports
respectively. The target function of the learning algorithm is to maximize μ−μ′.
In our empirical evaluation below we explain how Mallory obtains measurements
of μ, μ′ for diﬀerent values of s, σ.

Spying in the Dark: TCP and Tor Traﬃc Analysis
111
4.2
Empirical Evaluation
The environment we used to evaluate the timing attack is as described in Section
3.3, except that the client machines run Linux (kernel version 2.6.35) instead of
Windows; hence, the attacker cannot employ the global IP-ID based attack. All
Linux distributions ship with IP-tables ﬁrewall, its rule-set is empty by default;
we therefore evaluated only the scenarios where Mallory is near-C remote (see
Figure 3), i.e., Mallory communicates with C and S via a NAT device.
0
2
4
6
8
10
12
14
0
5
10
15
20
Score
Number of Rounds
Connection Port (near-C)
Best Non-Connection Port (near-C)
Connection Port (remote)
Best Non-Connection Port (remote)
Fig. 11. Timing Attack. Comparison of a
connection port to that of the highest scor-
ing non connection port as a function of
round number. Average of 10 runs, error-
bars mark the standard deviation values.
The ﬁrst task is to obtain a good
estimation of the optimal values of
s, σ for the channel between C and
Mallory (this depends on Mallory rel-
ative location to C). The machine
learning algorithm we employ uses
the connection that Mallory has with
C (see Figure 2): since for this con-
nection Mallory knows the client’s
port, he is able to obtain measure-
ments for diﬀerent group sizes (s)
and weights (the vector σ), see more
details in [16]. We found that these
values signiﬁcantly diﬀer between the
two attacker locations; e.g., in our
setup we found s = 31 to be suitable
for a near-C attacker while s = 4 ap-
peared optimal for the remote attacker. Figure 11 compares the connection-port
score to that of the highest scoring non-connection port as a function of the
number of rounds.
Next, we derive two thresholds for promoting ports to following rounds ac-
cording to their current score, this is similar to the experiments in Section 3.3.
According to the training set results, a choice of 60% of the maximal score for
the near-C attacker scenario and 40% in for the Interent attacker scenario appear
to be reasonable. As in Section 3.3, these thresholds require further research for
other scenarios, e.g., thresholds are eﬀected by the victim’s transmission rate.
We implemented the timing attack and conducted an experiment similar to
that presented in Section 3.3. We set the threshold for deciding whether a con-
nection exists between C and S according to the diﬀerence between the expected
scores of a connection port (μ) and a non connection port (μ′) as derived from
our training measurements. See analysis in [16]; in this experiment we set the
threshold to 0.2μ′ + 0.8μ. We measured our success rate in probing whether C is
communicating with a (third-party) website, S. Results are in Figures 12 - 14.
Comparing these results to those of the ID based attack, more time is required
to obtain similar success rates, and the maximal success rates reached are lower.
However, the results show that the timing attack does provide information on
the connection (since success ratio is greater than 0.5); but its hint is often

112
Y. Gilad and A. Herzberg
misleading (since success ratio is signiﬁcantly less than 1). Attacker can repeat
the attack several times and select by the majority.
Figure 15 illustrates the average amount of data that Mallory needs to send
in order to reach a particular success rate for diﬀerent locations in the network
and number of probes in each test.
In Figures 12 - 15, the measurements are the average of 50 runs; error-bars
mark the standard deviation values.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
10
20
30
40
50
60
70
80
Rate
Attack Duration (seconds)
Success
False Positives
False Negatives
Fig. 12. Timing attack, near-C attacker.
Mallory sends 2 probes in each test.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
10
20
30
40
50
60
70
80
Rate
Attack Duration (seconds)
Success
False Positives
False Negatives
Fig. 13. Timing attack, near-C attacker.
Mallory sends 5 probes in each test.
0
0.2
0.4
0.6
0.8
1
0
10
20
30
40
50
60
70
80
Rate
Attack Duration (seconds)
Success
False Positives
False Negatives
Fig. 14. Timing attack, remote attacker.
Mallory sends 5 probes in each test.
0
1
2
3
4
5
6
0.45
0.5
0.55
0.6
0.65
0.7
0.75
Data Sent (MB)
Success Rate
near-C attacker, 2 probes
near-C attacker, 5 probes
remote attacker, 5 probes
Fig. 15. Amount of data Mallory sends as
a function of her success rate.
5
Traﬃc Analysis for Tor Clients
In this section we consider the second scenario presented in Section 2, where C
uses an onion routing infrastructure to connect to S. We focus on the popular
Tor network, but similar attacks may apply to other low latency anonymity
networks. In this section we assume that the attacker, Eve, is able to eavesdrop
on C (but not on S).
Here, Eve actively interferes in the possible connection between C and S and
then tests whether a change in the rate of packets that C receives occurred. If
the result is positive, then it is an indication that C communicates with S. As of
writing this version of the paper, we only did preliminary testing of this attack;
more work is required to evaluate the practicality of this attack.

Spying in the Dark: TCP and Tor Traﬃc Analysis
113
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
Portion of Connections
Number of Different Exit Nodes
Fig. 16. The portion of 2000 circuits we
created using the Tor client as a function
of the number of diﬀerent exit relays used
A Tor client connects to a remote
server via a chain of relays (proxies).
The last relay in the chain, i.e., the
exit relay, has a direct TCP connec-
tion with the server. The number of
possible Tor exit relays is important
for our attacks (since a direct con-
nection exists between the exit relay
and the server); the Tor network com-
prises of few thousand relays, about
one thousand of which can perform as
exit relays (see [1]). However the num-
ber of diﬀerent exit relays that a client is likely to use is signiﬁcantly lower: ﬁrst,
a client can only use online relays; second, Tor clients typically choose the exit
relays according to various parameters such as stability and bandwidth. We have
formed Tor circuits from two clients in diﬀerent geographic locations and kept
track on the exit relay that was used. The measurements show that 20% of the
2000 circuits which we created (using Tor client version 0.2.2.35) had one of 7
speciﬁc exit relays. Figure 16 illustrates our measurements.
5.1
The Indirect Rate Reduction Attack
In this section we present an attack that uses the following observation: if Eve
inﬂuences the rate of communication between S and the exit relay, then this, in
turn, will change the rate of the connection between C and the ﬁrst (entrance)
relay. Eve sees the latter connection and is able to detect the change.
Since Eve can only observe the aggregated rate of data that C receives from
the entrance relay (since communication is encapsulated), this attack vector
weakens when C communicates with several other servers via one Tor circuit
and C’s connection rate with S is relatively small to that of the other servers.
The following attack uses TCP congestion control mechanisms to fake conges-
tion events; hence, reducing the communication rate. This attack is based on the
insight previously noted in Section 3.1: by sending a (spoofed) packet to an exit
relay, Eve would cause that relay to immediately send a duplicate acknowledg-
ment (Ack) in response to S, as long as Eve’s packet appears from an existing
connection between the exit relay and S. The duplicate Ack that the exit relay
sends to S in response, has a valid sequence number and S will accept it. A
sequence of three duplicate Acks is interpreted by TCP as a congestion event,
see [3]; when it occurs, S’ congestion window shrinks. The exact eﬀect depends
on the TCP implementation that the server runs. Until recently, TCP Reno
variant was default in Linux (the common operating system of server machines);
for this variant each congestion event halves the size of the congestion window.
Recent Linux kernels use the TCP Cubic variant, where the TCP window size
is multiplied by a constant of 0.8 for each congestion event.

114
Y. Gilad and A. Herzberg

	







 
!
"
"
"
#!
 
!
"
#$
Fig. 17. Eve causes the exit relay to send 3
duplicate Acks to S. S’ congestion window
is halved as a result.
The congestion window size di-
rectly eﬀects the sender’s (S) trans-
mission rate: S only sends as much as
the congestion window allows. Thus,
by causing the exit relay to send a
sequence of 3 duplicate Acks to the
server, Eve causes the latter to sig-
niﬁcantly reduce its ‘sending’ rate.
This attack is illustrated in Figure 17,
which shows the eﬀect when Eve sends
the spoofed packets to an exit relay
and port through which there is a con-
nection with S.
Attack Process. We use the asymmetry in the distribution of client choice
for exit relays to reduce the number of packets that the attacker needs to send
to perform indirect rate reduction. Namely, while there are many exit relays
available, there are only few ‘likely’ exit relays that a client might use (see
discussion above and Figure 16). For every server IP address s and likely exit
relay x, Eve can optionally employ one of the attacks in the Sections 3 and 4
to identify those exit relays that communicate with the server. This optional
step will reduce the eﬀort in the following steps of the attack. The techniques in
Sections 3 and 4 do not only identify the existence of a connection between two
peers, but also identify the client port – if a connection exists, then this is the
port with the highest score; see details on how Eve employs these techniques in
[16]. Next, for each of the ‘suspected’ connections, she performs the indirect rate
reduction attack described above and checks which of the clients had experienced
‘rate reduction’. This process repeats several times for statistical coherency; after
each iteration the attack is suspended to allow S’s congestion window to recover.
An important property of this attack is that the spoofed packets that Eve
sends to the exit relay in order to reduce the server’s rate, are not client speciﬁc.
Hence, in case that Eve eavesdrops on multiple clients (e.g., a government spying
on its citizens) this attack would simultaneously check which of these possible
clients has a connection with S.
Characteristics of Vulnerable Connections. Since the attack repeats for
several iterations with intermediate suspensions, this attack requires connections
lasting several minutes (see evaluation below). Furthermore, the connection must
be ‘active’, i.e., the server should send data to the client while the attack takes
place; this allows Eve to detect rate reductions and allows the congestion window
to recover when the attack is suspended. These type of connections include, for
example, ﬁle transfers (over FTP or HTTP).
5.2
Analysis
Our analysis in this subsection assumes that Eve does not try to detect a direct
connection between the exit relays and the server S (the optional step). Instead,

Spying in the Dark: TCP and Tor Traﬃc Analysis
115
she performs the indirect rate reduction attack on every likely exit relay and all
possible ports.
When using Tor, clients connect to S via proxies; therefore, clients’ geographic
location does not hint Eve on the server IP address that they will connect to (in
case S has multiple physical servers, e.g., for load balancing). As a result, Eve
must enumerate all the IP addresses of S during the attack.
For each of the ns server addresses and for every exit relay that Eve tries, she
performs 216 iterations, trying a diﬀerent port in each iteration; for each port
she sends three packets that would cause the exit relay to send three duplicate
Acks to the server, if a connection exists through that port. These packets can
be short, with only one byte of data, i.e., 41 bytes long. Hence, the overall data
that Eve sends to a particular exit relay, using a particular source IP of S in
a single attack is 216 · 3 · 41 < 7.7MB. As shown in Figure 16, a small set of
exit relays allows a good ‘hit’ rate. If Eve enumerates on all ns possible server
addresses and the most likely seven exit relays, then by our measurements the
attack results in a ‘hit’ rate of about 1
5 (see Figure 16); in this attack, she sends
53 · ns MB in each round. As noted at the end of the previous subsection, Eve’s
eﬀort is divided on the number of clients (victims) that she probes.
5.3
Empirical Evaluation
Setup. We used the Tor network to evaluate the indirect rate reduction attack.
To simplify the experiment and limit the eﬀect on other Tor users, we performed
the following measures: the restricted web-site server, a Linux machine (kernel
version 2.6.35) which runs an Apache web-server (version 2.2.14), had only one
IP address; furthermore, when running the attack, Eve was aware of the exit relay
that is used and its port used for the connection. Given these three parameters,
Eve only sends 3 packets of 41 bytes, i.e., 123 bytes, to carry out a single rate
reduction iteration. Below, we describe the frequency of iterations and show that
we send about 0.5 KB per second; we believe that this did not load the exit relay
or caused damage for other Tor users. The client machine in our experiments
runs Windows 7 and uses Tor (version 0.2.1.30) to connect to web-servers. While
running our evaluation, we created Tor circuits using 12 diﬀerent exit relays.
Evaluation. First, we observed the eﬀect of the rate reduction attack (three
duplicate Ack technique). To measure this eﬀect, C connects to one of our servers
through Tor; our server reports to Eve the IP address and port of the exit relay.
Eve sends her packets only to the reported exit relay and only to the speciﬁc
port used in the connection with our server. Eve performs three iterations of
rate reduction every second, aiming to fake three congestion events and decrease
the congestion window to about half of its size (in case of cubic variant). This
implies that in every second, Eve sends 369 bytes to the exit relay. In Figure
18 we compare between the rate of packets that the client receives (as observed
by Eve) on normal conditions and when Eve attacks the exit relay; our attack
reduces the average rate.

116
Y. Gilad and A. Herzberg
0
5
10
15
20
25
0
20
40
60
80
100
Incoming Packets In Last 100ms
Time (seconds)
(a) Normal Conditions
0
5
10
15
20
25
0
20
40
60
80
100
Incoming Packets In Last 100ms
Time (seconds)
(b) Under Attack
Fig. 18. Comparison between a rate of a TCP connection (via Tor) in normal conditions
and when under rate reduction attack
We next tested the scenario considered in Section 2, i.e., of a client that con-
nects through Tor to one of two sites. Eve uses rate reduction to test whether the
client is communicating with the restricted site. We conducted the experiment
as follows: the victim C connects to one of two servers in each time, each server
is chosen with probability 1
2. Regardless of the choice that the client makes, the
‘restricted’ server sends Eve an IP address and port, allegedly describing the exit
relay connected to it. In case that the client does not connect to the restricted
server, these values specify an arbitrary exit relay and port. Eve then employs
the attack above, performing three rate reductions per second and sending a
total of 369 Bps to the speciﬁed exit relay.
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
300
350
400
450
Portion
Attack Duration (seconds)
Success Rate
False Positives
False Negatives
Fig. 19. Eve’s success rate in detecting
client access to a restricted site via Tor.
Each measurement is the average of 20 runs.
Server runs TCP cubic variant.
If client rate decreased by at least
20% during the last 30 seconds, then
the client’s score is incremented. The
20% threshold is motivated by the re-
sults in Figure 18, but may change
in other scenarios, e.g., for a diﬀerent
server. This process is repeated; be-
tween iterations there is a 30 seconds
suspension that allows the TCP con-
nection between the server and exit
relay to recover to its normal rate (in
case the connection exists) and allows
Eve to obtain a recent measurement
of the average rate in C’s connection.
Eventually, Eve decides that C is com-
municating with the restricted site if C has more than half of the possible points.
Figure 19 shows Eve’s success rate as a function of the duration of the attack.
In these experiments, the servers run TCP Cubic variant; an improvement in
success rate is observed when server runs TCP Reno.

Spying in the Dark: TCP and Tor Traﬃc Analysis
117
6
Defense Mechanisms
The countermeasures that we propose in this section do not completely eliminate
the related side channel threat, however, they make it more diﬃcult to exploit.
These defenses are suitable for deployment on ﬁrewalls to ease deployment.
The globally incrementing IP identiﬁer side channel, as mentioned in Sec-
tion 3, is only relevant while still using IPv4.
One way to avoid it is to use
random IP-ID values; however, this can result in collisions and loss for frag-
mented traﬃc. The attack in Section 3 can be prevented by simply moving from
globally-incrementing IP-ID to per-destination IP-ID; this would preferably be
done by hosts, but until hosts do so2, a ﬁrewall can implement this by adding
(pseudo)random per-destination oﬀset to the IP-ID. See analysis and better ways
to ﬁx the IP-ID in [15,17].
It is more challenging to block or reduce the timing side channels and cope
with the rate reduction attack presented in Section 5. The ﬂaw that we iden-
tify is that a blind adversary is able to cause a TCP recipient an involuntary
reaction by sending arbitrary (spoofed) packets. We propose keeping a small
window of acceptable sequence numbers that may be processed. This window
resembles the receiver’s congestion window, but is more aggressive: while packets
outside the congestion window cause a duplicate acknowledgment (which we use
in the attacks described in Sections 3-5), packets that specify sequence numbers
outside the acceptable-window are silently discarded. The acceptable-window is
larger than the host’s congestion window and includes it. A congestion window
is usually up to 216 bytes, an acceptable-window that is twice as large, i.e., 217
bytes, will signiﬁcantly degrade the attacker’s ability to conduct all the attacks
in this paper. Since the sequence number is 32 bits long, the attacker is required
to send 232
217 = 215 times the number of packets to conduct similar attacks. How-
ever, this technique requires that the ﬁrewall will inspect the sequence numbers
in incoming TCP packets, which increases the packet processing overhead.
7
Conclusions and Future Work
Our primary conclusion is that TCP implementations leak information that
allows attackers to study the existence of connections via side channels as we
demonstrated in three attacks.
We leave several research directions for future work. Speciﬁcally, a more ex-
tensive empirical study is required to complete the evaluation of the Indirect
Rate Reduction attack on the Tor network. Furthermore, it would be desirable
to provide an analytic analysis for the attacks presented in this paper.
An important question is, can we perform a more eﬃcient and more accurate
attack on Tor anonymity by combining the indirect rate reduction attack pre-
sented in this paper with other existing attacks on Tor anonymity which exploit
other attack vectors, e.g., [12,18,19,28].
2 We informed Microsoft to the IP-ID issues, but we are not aware of intention to ﬁx
the IP-ID in Windows.

118
Y. Gilad and A. Herzberg
Acknowledgements. Thanks to Moti Geva, Amit Klein, Roger Dingledine and
the anonymous referees for their comments and suggestions.
References
1. Tor
Metrics
Portal.
Network
and
Usage
Graphs
(November
2011),
http://metrics.torproject.org/graphs.html
2. Advanced
Network
Architecture
Group.
ANA
Spoofer
Project
(2012),
http://spoofer.csail.mit.edu/summary.php
3. Allman, M., Paxson, V., Blanton, E.: TCP Congestion Control. RFC 5681 (Draft
Standard) (September 2009)
4. Baker, F., Savola, P.: Ingress Filtering for Multihomed Networks. RFC 3704 (Best
Current Practice) (March 2004)
5. Bellovin, S.M.: A Technique for Counting Natted Hosts. In: Internet Measurement
Workshop, pp. 267–272. ACM (2002)
6. Chakravarty,
S.,
Stavrou,
A.,
Keromytis,
A.D.:
Traﬃc
Analysis
against
Low-Latency
Anonymity
Networks
Using
Available
Bandwidth
Esti-
mation.
In:
Gritzalis,
D.,
Preneel,
B.,
Theoharidou,
M.
(eds.)
ES-
ORICS 2010. LNCS, vol. 6345, pp. 249–267.
Springer, Heidelberg (2010),
http://dx.doi.org/10.1007/978-3-642-15497-3
7. Danezis, G.: The Traﬃc Analysis of Continuous-Time Mixes. In: Martin, D., Ser-
jantov, A. (eds.) PET 2004. LNCS, vol. 3424, pp. 35–50. Springer, Heidelberg
(2005)
8. Deering, S., Hinden, R.: Internet Protocol, Version 6 (IPv6) Speciﬁcation. RFC
2460 (Draft Standard), Updated by RFCs 5095, 5722, 5871, 6437 (December 1998)
9. Dierks, T., Rescorla, E.: The Transport Layer Security (TLS) Protocol Version 1.2.
RFC 5246 (Proposed Standard), Updated by RFCs 5746, 5878, 6176 (2008)
10. Dingledine, R., Mathewson, N., Syverson, P.F.: Tor: The Second-Generation Onion
Router. In: USENIX Security Symposium, pp. 303–320. USENIX (2004)
11. Ehrenkranz, T., Li, J.: On the State of IP Spooﬁng Defense. ACM Transactions
on Internet Technology (TOIT) 9(2) (2009)
12. Evans, N.S., Dingledine, R., Grothoﬀ, C.: A Practical Congestion Attack on Tor
Using Long Paths. In: USENIX Security Symposium, pp. 33–50. USENIX Associ-
ation (2009)
13. Felten, E.W., Schneider, M.A.: Timing Attacks on Web Privacy. In: Jajodia, S.
(ed.) Proceedings of the 7th ACM Conference on Computer and Communications
Security, Greece, pp. 25–32. ACM Press (November 2000)
14. Ferguson, P., Senie, D.: Network Ingress Filtering: Defeating Denial of Service
Attacks which employ IP Source Address Spooﬁng. RFC 2827 (Best Current Prac-
tice), Updated by RFC 3704 (May 2000)
15. Gilad, Y., Herzberg, A.: Fragmentation Considered Vulnerable: Blindly Intercept-
ing and Discarding Fragments. In: Proceedings of USENIX Workshop on Oﬀensive
Technologies (August 2011)
16. Gilad, Y., Herzberg, A.: Spying in the Dark: TCP and Tor Traﬃc Analysis - Tech-
nical Report (April 2012),
http://u.cs.biu.ac.il/~herzbea/security/TR/TR12_02
17. Gont, F.: Security Assessment of the Internet Protocol Version 4. RFC 6274 (In-
formational) (July 2011)

Spying in the Dark: TCP and Tor Traﬃc Analysis
119
18. Hintz, A.: Fingerprinting Websites Using Traﬃc Analysis. In: Dingledine, R., Syver-
son, P.F. (eds.) PET 2002. LNCS, vol. 2482, pp. 171–178. Springer, Heidelberg
(2003)
19. Kadloor, S., Gong, X., Kiyavash, N., Tezcan, T., Borisov, N.: Low-Cost Side Chan-
nel Remote Traﬃc Analysis Attack in Packet Networks. In: ICC, pp. 1–5. IEEE
(2010)
20. Kent, S., Seo, K.: Security Architecture for the Internet Protocol. RFC 4301 (Pro-
posed Standard) (December 2005)
21. Killalea, T.: Recommended Internet Service Provider Security Services and Proce-
dures. RFC 3013 (Best Current Practice) (November 2000)
22. Kocher, P.C.: Timing Attacks on Implementations of Diﬃe-Hellman, RSA, DSS,
and Other Systems. In: Koblitz, N. (ed.) CRYPTO 1996. LNCS, vol. 1109, pp.
104–113. Springer, Heidelberg (1996)
23. Larsen, M., Gont, F.: Recommendations for Transport-Protocol Port Randomiza-
tion. RFC 6056 (Best Current Practice) (January 2011)
24. Levine, B.N., Reiter, M.K., Wang, C.-X., Wright, M.: Timing Attacks in Low-
Latency Mix Systems. In: Juels, A. (ed.) FC 2004. LNCS, vol. 3110, pp. 251–265.
Springer, Heidelberg (2004)
25. Lyon, G.: Nmap Network Scanning: The Oﬃcial Nmap Project Guide to Network
Discovery and Security Scanning (2009), http://nmap.org/book/
26. Mittal, P., Khurshid, A., Juen, J., Caesar, M., Borisov, N.: Stealthy Traﬃc Analysis
of Low-Latency Anonymous Communication Using Throughput Fingerprinting. In:
Chen, Y., Danezis, G., Shmatikov, V. (eds.) ACM Conference on Computer and
Communications Security, pp. 215–226. ACM (2011)
27. Murdoch, S.J., Danezis, G.: Low-Cost Traﬃc Analysis of Tor. In: IEEE Symposium
on Security and Privacy, pp. 183–195. IEEE Computer Society (2005)
28. Panchenko, A., Niessen, L., Zinnen, A., Engel, T.: Website Fingerprinting in Onion
Routing Based Anonymization Networks. In: Proceedings of the 10th Annual ACM
Workshop on Privacy in the Electronic Society, WPES 2011, pp. 103–114. ACM,
New York (2011)
29. Postel, J.: Transmission Control Protocol. RFC 793 (Standard), Updated by RFCs
1122, 3168, 6093, 6528 (September 1981)
30. Pries, R., Yu, W., Fu, X., Zhao, W.: A New Replay Attack Against Anonymous
Communication Networks. In: IEEE International Conference on Communications
(ICC), pp. 1578–1582 (2008)
31. Sanﬁlippo, S.: A New TCP Scan Method (1998),
http://seclists.org/bugtraq/1998/Dec/79
32. Sanﬁlippo, S.: About the IP Header ID (December 1998),
http://www.kyuzz.org/antirez/papers/ipid.html
33. Wikipedia. Usage Share of Operating Systems (2011),
http://en.wikipedia.org/wiki/Usage_share_of_operating_systems
34. Zalewski, M.: Silence on the wire: a ﬁeld guide to passive reconnaissance and indi-
rect attacks. No Starch Press (2005)
35. Zander, S., Murdoch, S.J.: An Improved Clock-Skew Measurement Technique for
Revealing Hidden Services. In: van Oorschot, P.C. (ed.) USENIX Security Sympo-
sium, pp. 211–226. USENIX Association (2008)
36. Zhu, Y., Fu, X., Graham, B., Bettati, R., Zhao, W.: On Flow Correlation Attacks
and Countermeasures in Mix Networks. In: Martin, D., Serjantov, A. (eds.) PET
2004. LNCS, vol. 3424, pp. 207–225. Springer, Heidelberg (2005)

Secure Distributed Framework for Achieving
ϵ-Diﬀerential Privacy
Dima Alhadidi, Noman Mohammed, Benjamin C.M. Fung,
and Mourad Debbabi
Concordia Institute for Information Systems Engineering,
Concordia University, Montreal, Quebec, Canada
{dm alhad,no moham,fung,debbabi}@encs.concordia.ca
Abstract. Privacy-preserving data publishing addresses the problem of
disclosing sensitive data when mining for useful information. Among
the existing privacy models, ϵ-diﬀerential privacy provides one of the
strongest privacy guarantees. In this paper, we address the problem
of private data publishing where data is horizontally divided among
two parties over the same set of attributes. In particular, we present
the ﬁrst generalization-based algorithm for diﬀerentially private data re-
lease for horizontally-partitioned data between two parties in the semi-
honest adversary model. The generalization algorithm correctly releases
diﬀerentially-private data and protects the privacy of each party accord-
ing to the deﬁnition of secure multi-party computation. To achieve this,
we ﬁrst present a two-party protocol for the exponential mechanism.
This protocol can be used as a subprotocol by any other algorithm that
requires exponential mechanism in a distributed setting. Experimental
results on real-life data suggest that the proposed algorithm can eﬀec-
tively preserve information for a data mining task.
1
Introduction
Data can be horizontally-partitioned among diﬀerent parties over the same set
of attributes.These distributed data can be integrated for making better deci-
sions and providing high-quality services. However, data integration should be
conducted in a way that no more information than necessary should be revealed
between the participating entities. At the same time, new knowledge that results
from the integration process should not be misused by adversaries to reveal sen-
sitive information that has not been available before the data integration. In this
paper, we propose an algorithm to securely integrate sensitive data , which is
horizontally divided among two parties over the same set of attributes, whereby
the integrated data still retains the essential information for supporting data
mining tasks. The following scenario further motivates the problem.
Consider a blood bank collects and examines the blood provided from donors
and then distributes the blood to diﬀerent hospitals. Periodically, hospitals are
required to submit the blood transfusion information, together with the patient
surgery data, to the blood bank for classiﬁcation analysis [1]. Due to privacy con-
cerns and privacy regulations, hospitals cannot provide any information about
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 120–139, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
121
Table 1. Data Set D1
ID Class
Job
Sex Age
Surgery
1
N
Janitor M
34
Transgender
2
Y
Lawyer
F
58
Plastic
3
Y
Mover
M
58
Urology
4
N
Lawyer M
24
Vascular
5
Y
Mover
M
34
Transgender
6
Y
Janitor M
44
Plastic
7
Y
Doctor
F
44
Vascular
Table 2. Data Set D2
ID Class
Job
Sex Age Surgery
8
N
Doctor
M
58
Plastic
9
Y
Doctor
M
24
Urology
10
Y
Janitor
F
63
Vascular
11
Y
Mover
F
63
Plastic
individual medical records to the blood bank. Accordingly, there is a desider-
atum for an approach that allows anonymizing horizontally-partitioned data
from diﬀerent providers for data release. The resulted anonymizing data should
not contain individually identiﬁable information and at the same time the data
providers should not reveal their private data or the ownership of the data to
each other.
Example 1. Suppose the ﬁrst hospital P1 and the second hospital P2 own the
data sets D1 and D2 as shown in Table 1 and Table 2, respectively. Each hospital
has records for diﬀerent individuals. The attribute Class contains the label Y
or N, representing whether or not the patient has received blood transfusion.
Both parties want to integrate their data and use the integrated data to build a
classiﬁer on the Class attribute. After the integration, the sensitive data of the
patient #5 can be uniquely identiﬁed since he is the only 34-year mover in the
data set. Moreover, we can infer that a 34-year male has performed a transgender
surgery since both patients in the integrated data set has performed it.
In this context, Jurczyk and Xiong [2] have proposed an algorithm to securely in-
tegrate horizontally-partitioned data from multiple data owners. Mohammed et
al. [1] have proposed a distributed algorithm to integrate horizontally-partitioned
high-dimensional health care data. Their methods [1,2] adopt k-anonymity [3,4]
or its extensions [5,6] as the underlying privacy principle. Recently, Wong et al. [7]
and Zhang et al. [8] have shown that algorithms, which satisfy k-anonymity [3,4]
or its extensions [5,6], are vulnerable to minimality attack and do not provide the
claimed privacy guarantee. Although several ﬁxes against minimality attack have
been proposed [9], new attacks such as composition attack [10] and deFinetti attack
[11] have emerged against algorithms that adopt k-anonymity or its extensions.
In this respect, diﬀerential privacy [12], which is a recently proposed privacy
model, provides provable privacy guarantee and it is, by deﬁnition, immune against
all these attacks. A diﬀerentially-private mechanism ensures that the probability
of any output (released data) is equally likely from all nearly identical input data
sets and thus guarantees that all outputs are insensitive to any individual’s data.
In other words, an individual’s privacy is not at risk because of the participation in
the data set. In this paper, we present the ﬁrst generalization-based algorithm for
diﬀerentially-private data release for horizontally-partitioned data between two
parties in the semi-honest adversary model. We take the single-party algorithm

122
D. Alhadidi et al.
Table 3. Related Work - Summary
Algorithms
Data Owner
Privacy Model
Single
Multi
Diﬀerential Pri-
vacy
Partition-based
Privacy
Horizontally Vertically
LeFevre et al. [15], Fung et al. [16], etc
✓
✓
Xiao et al. [17], Mohammed et al. [13], etc.
✓
✓
Jurczyk and Xiong [2], Mohammed et al. [1]
✓
✓
Jiang and Clifton [18], Mohammed et al. [19]
✓
✓
Our proposal
✓
✓
for diﬀerential privacy that has been recently proposed by Mohammed et al. [13]
as a basis and extend it to the two-party setting. The main contribution of our
paper can be summarized as follows:
– Wepresentatwo-partyprotocolfortheexponentialmechanismforhorizontally-
partitioned data. We use this protocol as a subprotocol of our main algorithm.
– We present the ﬁrst non-interactive two-party data publishing algorithm for
horizontally-partitioned data which achieves diﬀerential privacy and satisﬁes
the security deﬁnition of secure multiparty computation (SMC). In a non-
interactive framework, a database owner ﬁrst anonymizes the raw data and
then releases the anonymized version for data analysis. This approach is also
known as privacy-preserving data publishing (PPDP) [14].
– We experimentally show that the proposed algorithm can preserve informa-
tion for classiﬁcation analysis..
The rest of the paper is organized as follows. Section 2 presents related work.
Section 3 overviews privacy and security models adopted in this paper.The two-
party data publishing algorithm for horizontally-partitioned data is presented
in Section 4. In Section 5, we describe the two-party protocol for the exponen-
tial mechanism. We discuss in Section 6 the correctness, the security and the
eﬃciency of the two-party data publishing algorithm. Section 7 presents the
experimental results, and estimates the computation and communication cost
of the algorithm for a real data set. Section 8 answers some frequently raised
questions. Finally, concluding remarks as well as a discussion of future work are
presented in Section 9.
2
Related Work
The primary goal of our study in this paper is to share data. In contrast, pri-
vacy preserving distributed data mining (PPDDM) [20] allows sharing of the
computed result (e.g., a classiﬁer), but completely prohibits sharing data. In
PPDDM, multiple data owners want to compute a function based on their in-
puts without sharing their data with others. This function can be as simple as a
count query or as complex as a data mining task such as classiﬁcation, cluster-
ing, etc. However, compared to data mining result sharing, data sharing gives
greater ﬂexibility because recipients can perform their required analysis and data
exploration, and apply diﬀerent modeling methods and parameters.

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
123
Our approach allows anonymizing data from diﬀerent sources for data re-
lease without exposing the sensitive information. Jiang and Clifton [18] have
proposed Distributed k-Anonymity (DkA) framework to securely integrate two
data tables while satisfying k-anonymity requirement. Mohammed et al. [19] have
proposed an eﬃcient anonymization algorithm to integrate data from multiple
data owners. Unlike the distributed anonymization problem for horizontally-
partitioned data studied in this paper, these methods [18,19] propose algorithms
for vertically-partitioned data. Jurczyk and Xiong [2] have proposed an algo-
rithm to securely integrate horizontally-partitioned data from multiple data
owners. Mohammed et al. [1] have proposed a distributed algorithm to inte-
grate horizontally-partitioned high-dimensional health care data. To the best of
our knowledge, these are the only two methods [1,2] that generate an anony-
mous table for horizontally-partitioned data. However, both the methods adopt
k-anonymity [3,4] or its extensions [5,6] as the underlying privacy principle;
therefore, are vulnerable to the recently discovered privacy attacks [7,10,11].
Diﬀerential privacy [12] has received considerable attention recently as a sub-
stitute for partition-based privacy models for PPDP . However, most of the
research on diﬀerential privacy so far concentrates on the interactive [12,21]
and non-interactive [13,17] setting for the single-party scenario. Therefore, these
techniques do not address the problem of privacy-preserving data sharing for
classiﬁcation analysis; the primary theme of this paper. Finally, Dwork et al.
[22] have proposed a distributed interactive protocol for computing a function
while guaranteeing diﬀerential privacy. Given a function, each party ﬁrst com-
putes the function on its own data and then perturbs the result appropriately
such that the summation of all the perturbed results from all the parties gener-
ates a diﬀerentially private output. As mentioned already, interactive approach
does not allow data sharing and therefore does not address the problem studied
in this paper.
3
Background
In this section, we ﬁrst present an overview of diﬀerential privacy. Then, we
brieﬂy discuss the security deﬁnition in the semi-honest adversary model. Ad-
ditionally, we overview the required cryptographic primitives for the proposed
algorithm.
3.1
Privacy Model
Diﬀerential privacy is a recent privacy deﬁnition that provides a strong privacy
guarantee. It guarantees that an adversary learns nothing more about an indi-
vidual, regardless of whether her record is present or absent in the data.
Deﬁnition 1. (ϵ-Diﬀerential Privacy) [12] A randomized algorithm Ag is dif-
ferentially private if for all data sets D and D′ where their symmetric diﬀerence
contains at most one record (i.e., |D△D′| ≤1), and for all possible anonymized
data sets ˆD,
Pr[Ag(D) = ˆD] ≤eϵ × Pr[Ag(D′) = ˆD],
(1)

124
D. Alhadidi et al.
where the probabilities are over the randomness of the Ag.
A standard mechanism to achieve diﬀerential privacy is to add random noise to
the true output of a function. The noise is calibrated according to the sensitivity
of the function. The sensitivity of a function is the maximum diﬀerence of its
outputs from two data sets that diﬀer only in one record.
Deﬁnition 2. (Sensitivity) [12] For any function f : D →Rd, the sensitivity
of f is
Δf = max
D,D′ ||f(D) −f(D′)||1
(2)
for all D, D′ diﬀering in at most one record.
For example, let f be the count function. The Δf is 1 because f(D) can diﬀer
at most by 1 due to the addition or to the removal of a single record.
Dwork et al. [12] have proposed the Laplace mechanism. The mechanism takes
a data set D, a function f, and the parameter λ that determines the magnitude
of noise as inputs. It ﬁrst computes the true output f(D), and then perturbs the
output by adding noise. The noise is generated according to a Laplace distri-
bution with probability density function Pr(x|λ) =
1
2λexp(−|x|/λ); its variance
is 2λ2 and its mean is 0. Laplace mechanism guarantees that perturbed output
f( ˆD) = f(D) + Lap(Δf/ϵ) satisﬁes ϵ-diﬀerential privacy, where Lap(Δf/ϵ) is a
random variable sampled from the Laplace distribution.
McSherry and Talwar [23] have proposed the exponential mechanism to achieve
diﬀerential privacy whenever it makes no sense to add noise to outputs. The ex-
ponential mechanism can choose an output t ∈T that is close to the optimum
with respect to a utility function while preserving diﬀerential privacy. It takes
as inputs a data set D, an output range T , a privacy parameter ϵ, and a util-
ity function u : (D × T ) →R that assigns a real valued score to every output
t ∈T , where a higher score means better utility. The mechanism induces a
probability distribution over the range T and then samples an output t. Let
Δu = max∀t,D,D′ |u(D, t) −u(D′, t)| be the sensitivity of the utility function.
The probability associated with each output is proportional to exp( ϵu(D,t)
2Δu ); that
is, the output with a higher score is exponentially more likely to be chosen.
3.2
Security Model
In this subsection, we brieﬂy present the security deﬁnition in the semi-honest
adversary model. Moreover, we overview the required cryptographic primitives.
Secure Multiparty Computation. In the semi-honest model, adversaries fol-
low the protocol but may try to deduce additional information from the received
messages. A protocol is private in a semi-honest environment if the view of each
party during the execution of the protocol can be eﬀectively simulated by a
probabilistic polynomial-time algorithm knowing only the input and the output
of that party [24]. Many of the protocols, as it is the case with the proposed
algorithm in this paper, involve the composition of privacy-preserving subpro-
tocols in which all intermediate outputs from one subprotocol are inputs to the

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
125
next subprotocol. These intermediate outputs are either simulated given the ﬁ-
nal output and the local input for each party or computed as random shares.
Using the composition theorem [24], it can be shown that if each subprotocol is
privacy-preserving, then the resulting composition is also privacy-preserving.
Cryptographic Primitives. The required cryptographic primitives utilized in
this paper are:
– Yao’s Protocol [26]. It is a constant-round protocol for secure computa-
tion of any probabilistic polynomial-time function in the semi-honest model.
More speciﬁcally, assume that we have two parties P1 and P2 with their
inputs x and y, respectively. Both parties want to compute the value of the
function f(x, y). Then, P1 needs to send P2 an encrypted circuit computing
f(x, .). The received circuit is encrypted and accordingly P2 learns nothing
from this step. Afterwards, P2 computes the output f(x, y) by decrypting
the circuit. This can be achieved by having P2 obtaining a series of keys
corresponding to its input y from P1 such that the function f(x, y) can be
computed given these keys and the encrypted circuit. However, P2 must ob-
tain these keys from P1 without revealing any information about y. This is
done by using oblivious transfer protocol [24].
– Random Value Protocol (RVP) [27]. It describes how two parties can
share a value R ∈ZQ where R has been chosen uniformly at random
and Q ∈ZN is not known by either party, but is shared between them.
More speciﬁcally, P1 has R1 ∈ZN and P2 has R2 ∈ZN such that R =
R1 + R2 mod N ∈[0, Q −1] where N is the public key for the an additive
homomorphic scheme.
– Oblivious Polynomial Evaluation (OPE) [28]. It is a protocol involv-
ing two parties, a sender whose input is a polynomial P, and a receiver whose
input is a value α. At the end of the protocol, the receiver learns P(α) and
the sender learns nothing.
4
Two-Party Diﬀerentially Private Data Release
In this section, we present our two-party algorithm for diﬀerentially-private data
release for horizontally-partitioned data. To facilitate understanding the algo-
rithm, we ﬁrst present the notation that is used along this paper.
4.1
Notation and Preliminaries
Suppose two parties P1 and P2 own data table D1 and D2, respectively. Both
the parties want to release an integrated anonymous data table ˆD(Apr
1 , . . . ,
Apr
d , Acls) to the public for classiﬁcation analysis. The attributes in D1 and
D2 are classiﬁed into three categories: (1) An explicit identiﬁer attribute Ai
that explicitly identiﬁes an individual, such as SSN and Name. These attributes
are removed before releasing the data. (2) A class attribute Acls that contains
the class value, and the goal of the data miner is to build a classiﬁer to ac-
curately predict the value of this attribute. (3) A set of predictor attributes

126
D. Alhadidi et al.
Algorithm 1. Two-Party Algorithm
Input: Raw data set D1, privacy budget ϵ, and number of specializations h
Output: Anonymized data set ˆD
1: Initialize Dg with one record containing top most values;
2: Initialize Cuti to include the topmost value;
3: ϵ′ ←
ϵ
2(|Apr
n |+2h);
4: for l = 1 to h do
5:
Determine winner candidate w by DEM(D1,D2,∪Cuti,ϵ′);
6:
Specialize w on Dg;
7:
Replace w with child(w) in ∪Cuti;
8: end for
9: for each leaf node of Dg do
10:
Compute the share C1 of the true count C;
11:
Compute X1 = C1 + Lap(2/ϵ);
12:
Exchange X1 with P2 to compute (C + 2 × Lap(2/ϵ));
13: end for
14: return Each leaf node with count (C + 2 × Lap(2/ϵ))
Apr = {Apr
1 , . . . , Apr
d }, whose values are used to predict the class attribute.
Given a table D1 owned by P1, a table D2 owned by P2 and a privacy parameter
ϵ, our objective is to generate an integrated anonymized data table ˆD such that
(1) ˆD satisﬁes ϵ-diﬀerential privacy and (2) the algorithm to generate ˆD satisﬁes
the security deﬁnition of the semi-honest adversary model.
We require the class attribute to be categorical. However, the values of the
predictor attribute can be either numerical vn or categorical vc. Further, we
require that for each predictor attribute Apr, which is either numerical or cat-
egorial, a taxonomy tree is provided. We assume that there is no trusted third
party who computes the output table ˆD and the parties are semi-honest. More-
over, we assume that the two data sets include disjoint tuples and are deﬁned
on exactly the same schema.
4.2
Anonymization Algorithm
In this section, we present our distributed diﬀerentially-private anonymization
algorithm based on generalization for two parties as shown in Algorithm 1. Algo-
rithm 1 is executed by the party P1 (same for the party P2). The algorithm ﬁrst
generalizes the raw data and then adds noise to achieve ϵ-diﬀerential privacy.
Generalizing the Raw Data. The general idea is to anonymize the raw data
by a sequence of specializations starting from the topmost general state. A spe-
cialization, written v →child(v) replaces the parent value v with its set of child
values child(v). The specialization process can be viewed as pushing the ”cut”
of each taxonomy tree downwards. A cut of the taxonomy tree for an attribute
Apr
i , denoted by Cuti, contains exactly one value on each root-to-leaf path. Each
party keeps a copy of the current ∪Cuti and a generalized table Dg, in addition
to the private table D1 or D2. Initially, all values in Apr are generalized to the

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
127
Job
Sex
Age
Any_Job
Any_Sex
[1-99)
Blue-collar
Any_Sex
[1-60)
[1-99]
{[1-60), [60-99)}
White-collar
Any_Sex
[60-90)
White-collar
Any_Sex
[1-99)
Blue-collar
Any_Sex
[1-99)
Blue-collar
Any_Sex
[60-99)
∪Cuti = {Any_Job, Any_Sex, [1-99)}
Any_Job
{White-collar, Blue-collar}
∪Cuti = {White-collar,Blue-collar, Any_Sex, [1,99)}
White-collar
Any_Sex
[1- 60)
Doctor
Any_Job
White-collar
Blue-collar
Lawyer
Janitor
Mover
Any_Sex
Male
Female
Sex
Job
[1-99)
[1-60)
[60-99)
Age
Fig. 1. Generalized Data Table (Dg)
topmost value in their taxonomy trees, and Cuti contains the topmost value for
each attribute Apr
i . At each iteration, Algorithm 1 uses the distributed exponen-
tial mechanism to select a candidate for specialization (Line 5) depending on its
score. This can be achieved by calling Algorithm 2 detailed in Section 5. Once
a candidate is determined, both the parties specialize the winner candidate w
on Dg (Line 6) by splitting their records into child partitions according to the
provided taxonomy trees. Then, the parties update their local copy of ∪Cuti
(Line 7). This process is repeated according to the number of specializations h.
Example 2. Consider Table 1 and Table 2 and the taxonomy trees presented at
the bottom of Fig. 1. We do not show the class and the surgery attributes in
Fig. 1 due to space limitation. Initially, Dg contains one root node represent-
ing all the records that are generalized to ⟨Any Job, Any Sex, [1-99)⟩. ∪Cuti
is represented as {Any Job, Any Sex, [1-99)} and includes the initial candi-
dates. To ﬁnd the winner candidate, both parties run DEM. Suppose that w
is Any Job →{White-collar, Blue-collar}. Both parties create two child nodes
under the root node as shown in Fig. 1 and updates ∪Cuti to {White-collar,
Blue-collar, Any Sex, [1-99)}. Suppose
that the next winning candidate is
[1-99) →{[1-60), [60-99)}. Similarly, the two parties create further specialized
partitions resulting the generalized table in Fig. 1.
Adding Noisy Count. Each party computes the number of its records un-
der each leaf node (Line 10). To have an exchange between the parties that is
diﬀerentially-private, each party adds a Laplace noise to its count (Line 11) and
sends the result to the other party (Line 12). The protocol ends up with two
Laplace noises added to the count of each leaf (Line 14).
5
Two-Party Protocol for Exponential Mechanism
Exponential mechanism chooses a candidate that is close to optimum with
respect to a utility function while preserving diﬀerential privacy. In the

128
D. Alhadidi et al.
Table 4. MAX score calculation for the candidate Any Job
Max
Class
Job
Data Set
Y
N
5
3
1
Blue-collar
D1
2
1
White-collar
3
2
0
Blue-collar
D2
1
1
White-collar
8
5
1
Blue-collar
Integrated D1 and D2
3
2
White-collar
distributed setting, the same candidates are owned by two parties while records
are horizontally-partitioned among them. Consequently, we need a private mech-
anism to compute the same output while ensuring that no extra information is
leaked to any party. In this section, we present a two-party protocol for expo-
nential mechanism in a distributed setting. We adopt the max utility function
to compute the scores. For this reason, we illustrate ﬁrst how this function is
computed. Other utility functions can be adopted as discussed in Section 8.
5.1
Max Utility Function
To compute the score of each candidate, we adopt the max utility function [13].
Max(D, v) =

a∈child(v)
max
c (|TD(a, c)|)
(3)
where the notation T denotes the set of transactions (records) and |TD(a, c)|
denotes the number of records in D having the generalized value a and the class
value c. Thus, Max(D, v) is the summation of the highest class frequencies over
all child values. The sensitivity Δu of the Max function is 1 because the value
Max(D, v) can vary at most by 1 due to a record change. The following example
clariﬁes how to evaluate the max utility function.
Example 3. The maximum utility function of the candidate Any Job of Table 1
is 5. Table 4 demonstrates how the value 5 is computed. For each possible child
value of the candidate Any Job, we compute the number of records having the
class value Y and the class value N. Afterwards, we pick the maximum class
frequency for each child value and sum them. In the same vein, the maximum
utility function of the the candidate Any Job of Table 2 is 3. If we integrate the
two tables, the maximum utility function of the candidate Any Job is 8. Note
that, the maximum utility function of an integrated table is not the sum of the
values of maximum utility function of each source data set.
5.2
Distributed Exponential Mechanism
The distributed exponential mechanism (DEM) presented in Algorithm 2 takes
the followings as inputs:(1) Two raw data sets D1 and D2 owned by P1 and P2,
respectively, (2) set of candidates {v1, . . . , vk}, and (3) privacy budget ϵ. The

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
129
Algorithm 2. Distributed Exponential Mechanism (DEM)
Input: Raw data set D1 owned by P1, raw data set D2 owned by P2, a set of candidates
{v1, . . . , vk} and privacy budget ϵ
Output: Winner w
1: for each candidate vx where x = 1 to k do
2:
for (each possible value of aj of vx where j = 1 to m) do
3:
for (each class value ci where i = 1 to l) do
4:
P1 computes |TD1(aj, ci)|;
5:
P1 computes |TD2(aj, ci)|;
6:
end for
7:
end for
8:
P2 generates a random share α2;
9:
(P1
←
α1, P2
←
⊥)
←
MAX(|TD1(aj, ci)|i=1 to l,j=1 to m, |TD2(aj, ci)|
i=1 to l,j=1 to m, α2);
10:
P1 chooses a random share βx and deﬁnes the following polynomial Q(z) =
lcm(2!, . . . , w!) .10sw. w
i=0
((
ϵ
2Δu )s.10s.(α1+z))i
10s(i−1).i!
−βx;
11:
P1 and P2 execute a private polynomial with P1 inputting Q(.) and P2 inputting
α2, in which P2 obtains β′
x = Q(α2).
12: end for
13: (P1 ←γ1, P2 ←⊥) ←SUM(βx,x=1 to k, β′
x,x=1 to k, γ2);
14: P1 and P2 execute RVP to compute random shares R1 and R2, where (R1 + R2) ∈
Z(γ1+γ2);
15: P1 and P2 evaluates x ←COMPARISON(R1, R2, βx,x=1 to k, β′
x,x=1 to k);
16: return vx;
protocol outputs a winner candidate depending on its score using the exponen-
tial mechanism. The scores of the candidates can be calculated using diﬀerent
utility functions [13]. In this paper, we adopt the max utility function described
previously to calculate the scores. Given the scores of all the candidates, ex-
ponential mechanism selects the candidate v having score u with the following
probability where Δu is the sensitivity of the chosen utility function.
exp( ϵu
2Δu)
k
n=1 exp( ϵun
2Δu)
(4)
Next, we detail the steps of the distributed exponential mechanism (DEM).
Computing Max Utility Function. To compute the max utility function for
each candidate vx, the parties P1 and P2 compute |TD1(aj, ci)| and |TD1(aj, ci)|,
respectively for every possible value aj of vx and for every possible value ci
of the class attribute (Lines 2 to 7). After that, the two parties engage in
a secure circuit evaluation process using Yao’s Protocol (Line 9). The values
|TD1(aj, ci)|i=1 to l,j=1 to m, |TD2(aj, ci)|i=1 to l,j=1 to m and α2 are passed to
the MAX circuit where α2 is randomly generated by P2. For each child value
aj of the candidate vx, the circuit MAX, as shown in Algorithm 3, adds the corre-
sponding values |TD1(aj, ci)| and |TD2(aj, ci)| for every possible value ci of the

130
D. Alhadidi et al.
Algorithm 3. MAX Circuit
Input: |TD1(aj, ci)|i=1 to l,j=1 to m, |TD2(aj, ci)|i=1 to l,j=1 to m and α2
Output: α1 to P1,⊥to P2
1: sum = 0;
2: for j = 1 to m do
3:
max = 0;
4:
for i = 1 to l do
5:
ss= |TD1(aj, ci)| + |TD2(aj, ci)|;
6:
if (ss > max) then
7:
max = ss;
8:
end if
9:
end for
10:
sum = sum + max;
11: end for
12: α1 = sum - α2;
13: return α1,⊥;
Algorithm 4. COMPARISON Circuit
Input: Random shares R1 and R2, βx,x=1 to k, and β′
x,x=1 to k
Output: Index x to P1 and P2
1: L = 0;
2: R = R1 + R2;
3: for x = 1 to k do
4:
β = βx + β′
x;
5:
L = L + β;
6:
if (R ≤L) then
7:
return x;
8:
end if
9: end for
class attribute. It then computes the maximum value of the results. After that,
the maximum values associated with each child value aj should be summed to
get the max utility function for the candidate vx. To produce random shares of
the max utility function, the circuit subtracts α2, which is randomly generated
by P2, from the resulted score and outputs the result α1 to P1.
Computing Equation 4. The exponential function, exp(x) can be deﬁned
using the following Taylor series:
1 + x
1 + x2
2! + · · · + xi
i! + . . .
(5)
To evaluate the nominator of Equation 4 for each vx, we need to evaluate the
expression exp( ϵu
2Δu) which is equal to exp( ϵ(α1+α2)
2Δu
). Given the aforementioned

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
131
Taylor series:
exp(ϵ(α1 + α2)
2Δu
) =
w

i=0
( ϵ(α1+α2)
2Δu
)i
i!
(6)
Hence, the next step involves computing shares of the Taylor series approxima-
tion. In fact, it computes shares of:
lcm(2!, . . . , w!) .10s(w+1).
w

i=0
((
ϵ
2Δu)s.(α1 + α2))i
i!
where:
– lcm(2!, . . . , w!) is the lowest common multiple of {2!,. . . ,w!} and we multiply
by it to ensure that there are no fractions.
– (
ϵ
2Δu)s refers to approximating the value of
ϵ
2Δu up to a predetermined num-
ber s after the decimal point. For example, if we assume s = 4 and ϵ = ln2
then ( ln2
2×1)4 = (0.3465). Note that, this approximation does not eﬀect pri-
vacy guarantee since we are using less privacy budget. Also, the impact on
the utility is insigniﬁcant. In Section 7, we experimentally show the accuracy
for diﬀerent privacy budgets.
– 10sw.10s is multiplied by the series to ensure that we end up with an integer
result such that:
lcm(2!, . . . , w!) .10sw.10s.
w

i=0
((
ϵ
2Δu)s.(α1 + α2))i
i!
= lcm(2!, . . . , w!) .10sw.
w

i=0
10s.((
ϵ
2Δu)s.(α1 + α2))i
i!
= lcm(2!, . . . , w!) .10sw.
w

i=0
((
ϵ
2Δu)s.10s.(α1 + α2))i
10s(i−1).i!
Since s and w are known to both parties, the additional multiplicative factor
lcm(2!, . . . , w!).10sw.10s is public and can be removed at the end (if desired).
This equation is accurate up to an approximation error which depends on the
value of w. Therefore, scaling is needed and consequently the accuracy of the
exponential mechanism could be aﬀected. However, if the scaling factor is very
large, the total cost in terms of bits will increase. We experimentally measure the
impact of scaling in Section 7 and show that the scaling has very little impact
for the max utility function. The parties should agree on the number of the
considered digits s after the decimal point. The higher accuracy (in terms of the
number of the considered digits after the decimal point) we demand, the higher
cost we pay (in terms of bits). That is for s decimal points, we need log210s
extra bits. These extra bits result additional computation and communication
cost. More details are provided in Section 7. Note that restricting the values of

132
D. Alhadidi et al.
exp( ϵu
2Δu) to a ﬁnite range is completely natural as calculations performed on
computers are handled in this manner due to memory constraints.
To evaluate the nominator of Equation
4 for each vx in Algorithm 2, P1
chooses a random share βx and deﬁnes the following polynomial where s is a
constant number (Line 10):
Q(z) = lcm(2!, . . . , w!) .10sw.
w

i=0
((
ϵ
2Δu)s.10s.(α1 + z))i
10s(i−1).i!
−βx
Afterwards, P1 and P2 execute a private polynomial with P1 inputting Q(.) and
P2 inputting α2, in which P2 obtains β′
x = Q(α2) (Line 11). To evaluate the
denominator of Equation 4, the two parties execute the circuit SUM which takes
as input the random shares βx and β′
x for each candidate vx and a random
number γ2 generated by P2 (Line 13). The circuit computes the total sum of the
results that come out because of adding the random shares βx and β′
x for each
candidate vx. It then subtracts γ2, which is randomly generated by P2, from the
value of the total sum and outputs the share γ1 to P1.
Once we compute the denominator and numerator of Equation 4, we can
implement the exponential mechanism by ﬁrst partitioning the interval [0,1]
into segments according to the corresponding probability mass of each candidate.
Next, we sample a random number uniformly in the range [0,1] and the partition
in which the random number falls determines the winner candidate. However,
this method involves computing a secure division (Equation 4). Unfortunately,
we are not aware of any secure division scheme that ﬁts our scenario where
the nominator value is less than the denominator value. Alternatively, we solve
this problem without a secure division protocol. We ﬁrst partition the interval
[0, k
x=1 exp( ϵux
2Δu)] into k segments where k
x=1 exp( ϵux
2Δu) ≈γ1 + γ2 and each
segment corresponds to a candidate vx has a subinterval of length equal to
βx+β′
x. We then sample a random number uniformly in the range [0, γ1+γ2] and
the segment in which the random number falls determines the winner candidate.
Picking a Random Number. The parties P1 and P2 need to pick a random
number uniformly in the range [0, γ1 + γ2], where γ1 + γ2 ≈k
x=1 exp( ϵux
2Δu).
This can be achieved by using the Random Value Protocol (RVP) [27] (Line 14).
RVP takes γ1 and γ2 from the parties as input and outputs the random value
shares R1 and R2 to the respective parties, where R = R1 + R2.
Example 4. Suppose the values of the expression exp( ϵu
2Δu) is approximated to
60, 150 and 90 for three candidates as shares. Both parties then pick a random
number in the range [0, 300] using the RVP where 300 = 50 + 150 + 90.
Picking a Winner. The two parties engage again in a simple secure circuit
evaluation process using Yao’s Protocol [26] (Line 15). The circuit COMPARISON
compares their random number R with the sum L. The winner vx is the ﬁrst
candidate such that R ≤L where L = x
r=1(βx + β′
x).

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
133
6
Analysis
We discuss in this section the correctness, security and eﬃciency of Algorithm 1.
Proposition 1. (Correctness) Assuming both parties are semi-honest, Algo-
rithm 1 releases ϵ-diﬀerentially private data when data records are divided hori-
zontally among two parties over the same set of attributes.
Proof. Algorithm 1 performs the same function as the single-party algorithm
DiﬀGen
[13] but in a distributed setting. DiﬀGen is ϵ-diﬀerentially private.
Therefore, we prove the correctness of Algorithm 1 by just proving the steps
that are diﬀerent from DiﬀGen:
– Candidate selection. Algorithm 1 selects a candidate for specialization (Line
5) using Algorithm 2. Algorithm 2 selects a candidate vw with probability
∝exp( ϵuw
2Δu). The two parties compute cooperatively exp( ϵu
2Δu) for the can-
didates. Then the parties build an interval in the range [0,k
x=1 exp( ϵux
2Δu)]
and partition it among the candidates where each subinterval has a length
equal to exp( ϵu
2Δu). Since, the random value lies uniformly between
[0,k
x=1 exp( ϵux
2Δu)] and a candidate is chosen according to this value, the
probability of choosing any candidate is
exp( ϵu
2Δu )
k
x=1 exp( ϵux
2Δu ). Therefore, Algorithm 2
correctly implements exponential mechanism.
– Updating the tree Dg and ∪Cuti. Each party has its own copy of Dg and
∪Cuti. Each party updates these items exactly like DiﬀGen (Lines 6-7).
– Computing the noisy count. Algorithm 1 also outputs the noisy count of
each leaf node (Line 14), where the noise is equal to 2 × Lap(2/ϵ). Thus, it
guarantees ϵ
2-diﬀerential privacy.
Since Algorithm 1 performs exactly the same sequence of operations as the single-
party algorithm in a distributed setting where D1 and D2 are kept locally, it is
also ϵ-diﬀerentially private.
⊓⊔
Proposition 2. (Security) Algorithm 1 is secure under the semi-honest adver-
sary model.
Proof. The security of Algorithm 1 depends on the following steps where the
parties exchange information:
– Algorithm DEM (Line 5): The privacy proof of DEM is as follows:
• Circuit MAX: It can be evaluated securely [24]. Parties input their local
counts |T (aj, ci)| and receive the random share of the MAX value.
• Oblivious Polynomial Evaluation: It has been proven to be secure [28].
• Random Value Protocol (RVP): It has proven to be secure [27].
• Circuits SUM and COMPARISON: Similarly, these circuits can be evaluated
securely [24].
Since, all the above protocols produce random shares and proved to be se-
cure, DEM is also secure due to the composition theorem [24].

134
D. Alhadidi et al.
– Exchanging noisy counts (Line 12): Each party initially adds Laplace noise
to its local count and then exchange the noisy count with the other party.
This does not violate diﬀerential privacy because the noisy count is already
private according to Laplace mechanism [12].
Therefore, due to Composition Theorem [24], Algorithm 1 is secure.
⊓⊔
Proposition 3. (Complexity) The encryption and the communication costs of
Algorithm 1 are bounded by O(hk log R) and O(hk log RK), respectively.
Proof. Distributed exponential mechanism (Algorithm 2) dominates the overall
complexity of Algorithm 1. The complexity of DEM is computed as follows:
– Circuit MAX: This circuit is composed of simple add and compare operations
and thus can be implemented by the number of gates linear to the input size
of the circuit. The input includes m×l local counts |T (aj, ci)| and these val-
ues are of size at most log |D|. Hence, the encryption and the communication
complexity of MAX are bounded by O(ml log |D|) and O(ml log |D|K), respec-
tively, where K is the length of the key for a pseudorandom function [29].
The MAX protocol is called at most k times. Therefore, the encryption and the
communication costs are O(kml log |D|) and O(kml log |D|K), respectively.
– Oblivious Polynomial Evaluation: This protocol involves the private evalua-
tion of a polynomial of degree w. Thus, the encryption and the communica-
tion complexity are bounded by O(w) and O(we), where e is the length of an
encrypted element [30]. This protocol is also called k times. Therefore, the
encryption and the communication cost are O(kw) and O(kwe), respectively.
– Random Value Protocol (RVP): The costs of RVP are negligible and there-
fore they are ignored.
– Circuit SUM and COMPARISON: The analysis is similar to MAX circuit. The en-
cryption and the communication complexity of both the circuits are bounded
by O(k log R) and O(k log RK), where R =

exp( ϵ′ux
2Δu ) × 10s
.
Both the parties execute DEM (Algorithm 2) h times to select the winner can-
didates. Note that Lines 1-12 of Algorithm 2 are not executed in every iteration.
Rather, these lines are only invoked once for each candidate. Hence, the over-
all encryption and communication costs are O(max{kml log |D|, kw, hk log R})
and O(max{kml log |D|K, kwe, hk log RK}), respectively. Since the value of R
is usually very large, the encryption and communication costs can be deﬁned as
O(hk log R) and O(hk log RK), respectively.
⊓⊔
7
Performance Analysis
In this section, we evaluate the scaling impact on the data quality in terms of
classiﬁcation accuracy. Moreover, we estimate the computation and the commu-
nication costs of Algorithm 1. We employ the publicly available data set Adult;
a real-life census data set that has been used for testing many anonymization

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
135
74
76
78
80
82
84
86
2
4
6
8
10
Average Accuracy (%)
Number of decimal points (s)
ε = 0.1
ε = 0.25
ε = 0.5
ε = 1
BA = 85.3%
LA = 75.5%
Fig. 2. Classiﬁcation Accuracy for Diﬀerent Scaling
algorithms [18,5,6]. It has 45, 222 census records with 6 numerical attributes, 8
categorical attributes, and a binary class column representing two income levels,
≤50K or >50K. All experiments are conducted on an Intel Core i7 2.7GHz PC
with 12GB RAM.
7.1
Experiments
To evaluate the impact on classiﬁcation quality, we divide the data into training
and testing sets. First, we apply our algorithm to anonymize the training set and
to determine the ∪Cuti. Then, the same ∪Cuti is applied to the testing set to
produce a generalized testing set. Next, we build a classiﬁer on the anonymized
training set and measure the Classiﬁcation Accuracy (CA) on the generalized
records of the testing set. For classiﬁcation models, we use the well-known C4.5
classiﬁer [31]. To better visualize the cost and beneﬁt of our approach, we provide
additional measures: (1) Baseline Accuracy (BA) is the classiﬁcation accuracy
measured on the raw data without anonymization; (2) BA - CA represents the
cost in terms of classiﬁcation quality for achieving a given ϵ-diﬀerential privacy
requirement; (3) Lower bound Accuracy (LA) is the accuracy on the raw data
with all attributes (except for the class attribute) removed and (4) CA - LA
represents the beneﬁt of our method over the naive non-disclosure approach.
Fig. 2 depicts the classiﬁcation accuracy CA for the utility function Max where
the privacy budget ϵ ∈{0.1, 0.25, 0.5, 1} and the number of considered digits
after the decimal point 2 ≤s ≤10 (i.e., scaling as described in Section 5).
The BA and LA are 85.3% and 75.5%, respectively, as shown in the ﬁgure by
the dotted lines. We use 2/3 of the records (i.e., 30,162) to build the classiﬁer
and measure the accuracy on the remaining 1/3 of the records (i.e., 15060). For
each experiment, we execute 10 runs and average the results over the runs. The
number of specializations h is 10 for all the experiments. For ϵ = 1 and s = 10,
BA - CA is around 2.6% whereas CA - LA is 7.1%. For ϵ = 0.5, BA - CA
spans from 3.6% to 4%, whereas CA - LA spans from 5.7% to 6.2%. However,
as ϵ decreases to 0.1, CA quickly decreases to about 79% (highest point), the

136
D. Alhadidi et al.
cost increases to about 6.5%, and the beneﬁt decreases to about 3.3%. The
experimental result demonstrates that the classiﬁcation accuracy is insensitive
to the scaling (the number of considered digits after the decimal points) for the
Max function. This is because the value of exp( ϵ′
2Δuu) is large due to the score of
the Max function which is usually a large integer. Therefore, scaling has hardly
any impact on the data utility.
7.2
Cost Estimates
Most of the computation and the communication of Algorithm 1 take place dur-
ing the execution of the DEM (Line 5). The runtime of the other steps is less
than 30 seconds for Adult dataset. Hence, we only elaborate the runtime of the
DEM. As discussed in Section 6, the computation and the communication com-
plexity of the distributed exponential mechanism are dominated by the cost of
the SUM (Line 13) and COMPARISON (Line 15) circuits. In the following, we provide
an estimate for the computation and the communication costs of evaluating the
SUM and COMPARISON circuit. Here, we assume that P1 encodes and P2 evaluates
the encrypted circuit. The roles of P1 and P2 can be swapped.
Computation. The cost of an encryption is denoted by Cm which is 0.02 second
for 1024-bit numbers on a Pentium III processor [28]. For both the circuits, P2
needs to execute a 1-out-of-2 oblivious transfer protocol to get the corresponding
encryption key for its input bits. This is the major computational overhead of
the distributed exponential mechanism. The computation cost of an oblivious
transfer protocol is roughly equal to the cost of a modular exponentiation, which
is Cm. Therefore, the computation overhead is equal to the number of input bits
of P2 times Cm. Each input of the circuit is bounded by ⌈log2 R⌉bits, where
R =

exp( ϵ′
2Δuu(D, vi)) × 10s
.
⌈log2 R⌉=

log2

exp( ϵ′
2Δu) × 10s
	

=

ϵ′
2Δu
ln 2 + log2 10s

=

ϵ′
2Δu
ln 2 + (3.3219 × s)

Here, Δu = 1, ϵ′ =
1
2(6+2×10) = 0.02, u(D, vi) is bounded by the number of the
records |D| = 30162 for Max function, and s = 10 suﬃces the desired accuracy.
Hence, we have ⌈log2 R⌉= 469 bits. The input size of P2 is O(k log R) bits,
where the constant is fairly small. Here, k is the total number of candidates
which is 24 at most for Adult data set. Thus, the cost is k × ⌈log2 R⌉× Cm

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
137
= 24 × 469 × 0.02s ≈225 seconds. As mentioned in Section 6, there are at most
h invocations of these circuits. Here, h is the number of specializations which is
set to 10. Hence, the total computational cost is h × 225 ≈37.5 mins .
Communication. P1 needs to send a table of size 4K for each gate of the SUM
and COMPARISON circuit, where we assume the key size K is 128 bits. This is the
major communication overhead of the distributed exponential mechanism. Since
these circuits only use addition and comparison operations, the total number of
gates needed to implement these circuits are O(k log R). Thus, the number of
gates, Tg ≈24 × 469 = 11256. Therefore, the communication cost of sending the
tables is h × 4K × Tg ≈5.76 × 107 bits, which takes approximately 37.3 seconds
using a T1 line with 1.544 Mbits/second bandwidth.
Remark. Our estimation ignores the computational cost of evaluating the
circuit and the communication cost of the oblivious transfer protocol. The eval-
uation of the circuit involves decrypting a constant number of ciphertexts (sym-
metric encryption) for every gate which is very eﬃcient compared to oblivious
transfer (modular exponentiations) since the number of gates of the circuit is
linear to the number of input bits. Also, the communication cost of the obvious
transfer protocol is negligible compared to the cost of sending the tables.
8
Discussion
Is diﬀerential privacy good enough? What changes are required if there are more
than two parties? Can the algorithm be easily adapted to accommodate a dif-
ferent utility function? In this section, we provide answers to these questions.
Differential privacy. Diﬀerential privacy is a strong privacy deﬁnition.
However, Kifer and Machanavajjhala [32] have shown that if the records are not
independent or an adversary has access to aggregate level background knowledge
about the data, then privacy attack is possible. In our application scenario,
each record is independent of each other and we assume that no deterministic
statistics of the raw database have ever been released. Hence, diﬀerential privacy
is appropriate for our problem.
More than two parties. The proposed algorithm is only applicable for the
two-party scenario because the distributed exponential algorithm, and the other
primitives (e.g., random value protocol) are limited to two-party scenario. The
proposed algorithm can be extended for more than two parties by modifying all
the subprotocols while keeping the general top-down structure of the algorithm
as it is.
Other Utility functions. For each new utility function, we only need to
devise an algorithm to calculate the utility function. Hence, we only have to
change Algorithm 3 to adapt our approach for other utility function.

138
D. Alhadidi et al.
9
Conclusion
In this paper, we have presented a two-party diﬀerentially-private data release
algorithm for horizontally-partitioned data for the non-interactive setting. We
have shown that the proposed algorithm is diﬀerentially private and secure un-
der the security deﬁnition of semi-honest adversary model. Moreover, we have
experimentally evaluated the data utility of the algorithm. An intersecting re-
search direction, as a future work, is devising diﬀerent heuristics for diﬀerent
data mining tasks.
Acknowledgments. We sincerely thank the reviewers for their valuable com-
ments. The research described in this paper is part of the project in cloud com-
puting security and privacy with Ericsson Canada and Alcatel Lucent, funded
by an NSERC Strategic Grant and NSERC Canada Graduate Scholarships.
References
1. Mohammed, N., Fung, B.C.M., Hung, P.C.K., Lee, C.: Centralized and distributed
anonymization for high-dimensional healthcare data. ACM Transactions on Knowl-
edge Discovery from Data (TKDD) 4(4), 18:1–18:33 (2010)
2. Jurczyk, P., Xiong, L.: Distributed anonymization: Achieving privacy for both data
subjects and data providers. In: Proceedings of the Annual IFIP WG 11.3 Working
Conference on Data and Applications Security, DBSec (2009)
3. Samarati, P.: Protecting respondents’ identities in microdata release. IEEE Trans-
action on Knowledge and Data Engineering (TKDE) (2001)
4. Sweeney, L.: k-anonymity: A model for protecting privacy. International Journal
on Uncertainty, Fuzziness and Knowledge-based Systems (2002)
5. Machanavajjhala, A., Kifer, D., Gehrke, J., Venkitasubramaniam, M.: ℓ-diversity:
Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery from
Data (TKDD) (2007)
6. Wang, K., Fung, B.C.M., Yu, P.S.: Handicapping attacker’s conﬁdence: An alter-
native to k-anonymization. Knowledge and Information Systems (KAIS) 11(3),
345–368 (2007)
7. Wong, R.C.W., Fu, A.W.C., Wang, K., Pei, J.: Minimality attack in privacy pre-
serving data publishing. In: Proceedings of the International Conference on Very
Large Data Bases (VLDB) (2007)
8. Zhang, L., Jajodia, S., Brodsky, A.: Information disclosure under realistic assump-
tions: Privacy versus optimality. In: Proceedings of the ACM Conference on Com-
puter and Communications Security (CCS) (2007)
9. Cormode, G., Srivastava, D., Li, N., Li, T.: Minimizing minimality and maximizing
utility: Analyzing methodbased attacks on anonymized data. In: Proceedings of the
International Conference on Very Large Data Bases (VLDB) (2010)
10. Ganta, S.R., Kasiviswanathan, S., Smith, A.: Composition attacks and auxiliary
information in data privacy. In: Proceedings of the ACM International Conference
on Knowledge Discovery and Data Mining (SIGKDD) (2008)
11. Kifer, D.: Attacks on privacy and de ﬁnetti’s theorem. In: Proceedings of the ACM
Conference on Management of Data (SIGMOD) (2009)

Secure Distributed Framework for Achieving ϵ-Diﬀerential Privacy
139
12. Dwork, C., McSherry, F., Nissim, K., Smith, A.: Calibrating Noise to Sensitivity in
Private Data Analysis. In: Halevi, S., Rabin, T. (eds.) TCC 2006. LNCS, vol. 3876,
pp. 265–284. Springer, Heidelberg (2006)
13. Mohammed, N., Chen, R., Fung, B.C.M., Yu, P.S.: Diﬀerentially private data re-
lease for data mining. In: Proceedings of the ACM International Conference on
Knowledge Discovery and Data Mining (SIGKDD) (2011)
14. Fung, B.C.M., Wang, K., Chen, R., Yu, P.S.: Privacy-preserving data publishing:
A survey of recent developments. ACM Computing Surveys 42(4), 1–53 (2010)
15. LeFevre, K., DeWitt, D.J., Ramakrishnan, R.: Mondrian multidimensional k-
anonymity. In: Proceedings of the IEEE International Conference on Data En-
gineering (ICDE) (2006)
16. Fung, B.C.M., Wang, K., Yu, P.S.: Anonymizing classiﬁcation data for pri-
vacy preservation. IEEE Transactions on Knowledge and Data Engineering
(TKDE) 19(5), 711–725 (2007)
17. Xiao, X., Wang, G., Gehrke, J.: Diﬀerential privacy via wavelet transforms. In:
Proceedings of the International Conference on Data Engineering (ICDE) (March
2010)
18. Jiang, W., Clifton, C.: A secure distributed framework for achieving k-anonymity.
Very Large Data Bases Journal (VLDBJ) 15(4), 316–333 (2006)
19. Mohammed, N., Fung, B.C.M., Debbabi, M.: Anonymity meets game theory: se-
cure data integration with malicious participants. Very Large Data Bases Journal
(VLDBJ) 20(4), 567–588 (2011)
20. Clifton, C., Kantarcioglu, M., Vaidya, J., Lin, X., Zhu, M.Y.: Tools for privacy
preserving distributed data mining. ACM International Conference on Knowledge
Discovery and Data Mining (SIGKDD) Explorations Newsletter 4(2), 28–34 (2002)
21. Roth, A., Roughgarden, T.: Interactive privacy via the median mechanism. In:
Proceedings of the ACM Symposium on Theory of Computing (STOC) (2010)
22. Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M.: Our Data, Our-
selves: Privacy Via Distributed Noise Generation. In: Vaudenay, S. (ed.) EURO-
CRYPT 2006. LNCS, vol. 4004, pp. 486–503. Springer, Heidelberg (2006)
23. McSherry, F., Talwar, K.: Mechanism design via diﬀerential privacy. In: Proceed-
ings of the IEEE Symposium on Foundations of Computer Science (2007)
24. Goldreich, O.: Foundations of Cryptography, vol. 2. Cambridge University Press
(2001)
25. Paillier, P.: Public-Key Cryptosystems Based on Composite Degree Residuosity
Classes. In: Stern, J. (ed.) EUROCRYPT 1999. LNCS, vol. 1592, pp. 223–238.
Springer, Heidelberg (1999)
26. Yao, A.C.: Protocols for secure computations. In: Proc. of the IEEE Proceedings
of the IEEE Symposium on Foundations of Computer Science (FOCS) (1982)
27. Bunn, P., Ostrovsky, R.: Secure two-party k-means clustering. In: Proceedings of
the ACM Conference on Computer and Communications Security (CCS), 486–497
(2007)
28. Naor, M., Pinkas, B.: Eﬃcient oblivious transfer protocol. In: Proceedings of the
12th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) (2001)
29. Goldreich, O., Micali, S., Wigderson, A.: How to play any mental game - a com-
pleteness theorem for protocols with honest majority. In: Proceedings of the ACM
Symposium on the Theory of Computing (STOC) (1987)
30. Lindell, Y., Pinkas, B.: Privacy preserving data mining. Journal of Cryptol-
ogy 15(3), 177–206 (2002)
31. Quinlan, J.R.: C4.5: Programs for Machine Learning. Morgan Kaufmann (1993)
32. Kifer, D., Machanavajjhala, A.: No free lunch in data privacy. In: Proceedings of
the ACM Conference on Management of Data (SIGMOD) (2011)

Differentially Private Continual Monitoring of Heavy
Hitters from Distributed Streams
T.-H. Hubert Chan1, Mingfei Li1, Elaine Shi2,⋆, and Wenchang Xu3
1 The University of Hong Kong
2 UC Berkeley
3 Tsinghua University
Abstract. We consider applications scenarios where an untrusted aggregator
wishes to continually monitor the heavy-hitters across a set of distributed streams.
Since each stream can contain sensitive data, such as the purchase history of cus-
tomers, we wish to guarantee the privacy of each stream, while allowing the un-
trusted aggregator to accurately detect the heavy hitters and their approximate
frequencies. Our protocols are scalable in settings where the volume of stream-
ing data is large, since we guarantee low memory usage and processing overhead
by each data source, and low communication overhead between the data sources
and the aggregator.
1
Introduction
Consider k data streams at k data sources, where items from some set U arrive at each
stream. An untrusted aggregator wishes to continually monitor the most recent heavy
hitters (i.e. the frequent items) over a sliding window – however, the data sources do
not trust the aggregator, and wish to guarantee the privacy of their data streams. For
example, a public health provider would like to monitor the potential outbursts of new
epidemics (where the heavy hitters are the most common symptoms or diseases) by
studying hospital visit records from k hospitals. Since medical records contain highly
sensitive information, the hospitals may be legally obliged to protect their patients’ pri-
vacy from the third-party public health provider. In Figure 1, we show another example
where each stream represents a store, and the aggregator wishes to track the most pop-
ular items in the past week.
1.1
Results and Contributions
In this paper, we propose novel protocols that allow an untrusted aggregator to contin-
ually monitor the heavy hitters over a sliding window of duration W, while protecting
the privacy of each data source. Since the aggregator is untrusted and there is no sin-
gle trusted entity, standard privacy frameworks like PINQ [17] cannot be used directly
⋆This material is based upon work partially supported by the Air Force Ofﬁce of Scientiﬁc
Research under MURI Grant No. 22178970-4170 and No. FA9550-08-1-0352. Any opinions,
ﬁndings, and conclusions or recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reﬂect the views of the funding agencies.
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 140–159, 2012.
c⃝Springer-Verlag Berlin Heidelberg 2012

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
141
Fig. 1. Problem setup. In this example, k stores wishes to continually monitor the popular items
over the past week. The aggregator is assumed to be untrusted; and the stores may be concerned
about protecting their secret business information such as sales revenue, and protecting the pri-
vacy of their customers.
in our distributed setting. In our protocols, each data source periodically sends sani-
tized (and potentially also encrypted) updates to the aggregator in order to notify the
aggregator of latest trends as observed by the data source. The aggregator is then able
to reconstruct the most recent popular items and their respective frequencies through
these sanitized updates.
We conduct experiments using the Netﬂix Contest Dataset, and demonstrate that
our algorithms can achieve low communication bandwidth and good utility in realistic
application scenarios. We next explain the privacy guarantees and desirable features
that our constructions achieve.
Two Levels of Privacy Protection. We propose protocols that achieve the following two
different aspects of privacy.
Event-Level Differential Privacy. Our ﬁrst construction, referred to as the PDCH-LU
algorithm (which stands for Private Distributed Continual Heavy-hitter - Lazy Update),
achieves event-level differential privacy. Roughly speaking, the sanitized updates re-
leased should be insensitive to the occurrence or non-occurrence of a single event.
Intuitively, event-level differential privacy allows a store to guard the privacy of its
customers, by concealing whether a certain purchase has taken place.
In our constructions, we achieve event-level differential privacy through the addition
of appropriate noises before the release of any statistics. Note also that ϵ event-level
differential privacy immediately implies mϵ user-level differential privacy, where m is
the maximum number of items for each user.
Aggregator Obliviousness. Through the use of bloom ﬁlters and special encryption
schemes, our second protocol, referred to as PDCH-BF (which stands for Private Dis-
tributed Continual Heavy-hitter - Bloom Filter), achieves even stronger privacy guar-
antees: speciﬁcally, it achieves aggregator obliviousness in addition to event-level dif-
ferential privacy.
On a high level, aggregator obliviousness advocates the need-to-know principle, i.e.,
the aggregatorshould ideally learn the least amount of information necessary to perform
the heavy-hitter monitoring task. Speciﬁcally, in our second construction PDCH-BF,

142
T.-H. Hubert Chan et al.
apart from the approximate frequencies of a subset of the relatively more popular items
across all streams, the aggregator learns nothing else.
To achieve aggregator obliviousness, our second protocol PDCH-BF employs bloom
ﬁlters, as well as special encryption schemes [15; 20; 21] which support secure aggre-
gation and controlled decryption of selective statistics.
Notice that aggregator obliviousness immediately implies the following: 1) in the
example in Figure 1 the aggregator can learn the approximate frequencies of (a subset
of) the items, but it cannot learn the transaction volume of each individual store which
may be considered secret business information; and 2) although the aggregator can
learn which the heavy hitters are and their approximate frequencies across all streams,
the aggregator fails to learn which streams are contributing to these heavy-hitters, and
how much each stream is contributing to these heavy hitters.
A more detailed discussion of our privacy notions and their nuances can be found in
Section 2.3.
Low Computational and Communication Overhead. Our protocols require only a small
amount of computation and memory from each data source. To process an item from
the stream, a data source needs to update only a small number of counters; and in each
time step, it needs to sample only a small number of random variables. This is desirable
in numerous application settings – for example, in sensor network applications, where
each node has low computational resources; or network intrusion detection scenarios,
where routers cannot afford expensive real-time computation due to the large bandwidth
throughput.
Our protocols also requires low communication costs between the data sources and
the aggregator. Moreover, all communications are uni-directional from the data sources
to the aggregator, and the data sources need not have interactions among themselves. In
contrast, generic secure multi-party computation construction [13] requires expensive
interactive communication between the data sources.
1.2
Related Work
Our work builds on several well-known lines of research. We describe some of the
works that are the most related to ours and refer the readers to the cited references for
more extensive review on the relevant research areas.
Differential Privacy in Continual Setting. Ever since Dwork [7] has introduced
differential privacy, this notion has gained popularity in both the theory and security
communities (see [8] for a quick review of the latest development). The idea of intro-
ducing randomness to perturb the outputs of algorithms allows a clean and formal way
to analyze the tradeoff between preserving input privacy and achieving output accu-
racy. Recently, privacy has been studied in the continual setting [5; 9; 10]. Speciﬁcally,
a change in the input in the current time step would not only affect the output in the cur-
rent time step, but also might have a long lasting effect in the future. Useful continual
differentially private algorithms would need to mask this long term effect without sac-
riﬁcing too much on accuracy. Chan et al. [5] gave a differentially private mechanism
to continually report the number of 1’s seen so far in a bit stream with additive error

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
143
that is polylogarithmic in the number of time steps. In the streaming model, Dwork et
al. [10] also distinguish between event-level privacy and user-level privacy: event-level
privacy hides the occurrence of a particular event, while user-level privacy prevents
adversaries from determining whether the stream contains any of a particular user’s ac-
tivities at all. In this paper, we use event-level differential privacy as our privacy notion.
Mir et al. [18] also considered the problem of private streaming algorithms to return
the counts of heavy hitters, not the heavy hitters themselves. However, they consider a
more general setting in which in each update, the counter of an item can be increased
or decreased arbitrarily, as long as the counter remains non-negative. Moreover, their
notion of privacy hides the following change in the stream: any subset of occurrences
of an item can be replaced with another item, and remaining updates can be arbitrarily
reordered; on the other hand, the total count of all items is public knowledge in their
setting.
Untrusted Aggregator. There have been works on studying the case when the aggre-
gator is untrusted [15; 20; 21], where cryptographic techniques are employed. We ﬁrst
consider protocols in which each node will desensitize its data ﬁrst so that cryptography
will not be necessary; in order to achieve a stronger notion of privacy and security, we
augment our protocols by employing cryptographic techniques.
Streaming Algorithms for Heavy Hitters. The ﬁrst algorithm to output frequent items
was given by Misra and Gries [19] (MG algorithm). They designed a deterministic
algorithm that reads a stream of W items and at the end gives the count of every item in
the stream with relative error λ (i.e., additive error at most λW); the algorithm only uses
O( 1
λ) words of memory. The MG algorithm was rediscovered several times [6; 14].
Using the MG algorithm concurrently on overlapping blocks of different sizes, Arasu
and Manku [1] gave a deterministic algorithm that continually estimate the count of
every item with relative error λ with respect to the current window; the query and the
update time is O( 1
λ log 1
λ), while O( 1
λ log2 1
λ) words of memory is required. They also
gave a randomized version, where both the time and the memory is O( 1
λ log 1
δλ), where
δ is the failure probability.
Lee and Ting [16] augmented the counters in the MG algorithm to include ap-
proximate positions of where items occur, and consequently they improved Arasu and
Manku’s algorithm performance to O( 1
λ), for both running time and memory require-
ment.
Distributed Streaming Protocols with Low Communication Cost. In the distributed
streaming model, each of k nodes receives its own stream, and the nodes communicate
with the aggregator,who wishes to estimate the number of times each item appears in all
the streams in the current window with relative error λ. Yi and Zhang [22] considered
the special case with inﬁnite window size and gave a 2-way communication (between
nodes and aggregator) protocol with total communication cost of O( k
λ log N) words,
where N is the total number of items arriving at all streams. Chan et al. [3] considered
the case of a sliding window, and gave a one-way communication (from nodes to ag-
gregator) protocol. Under the special case of exactly one item arriving in each time step

144
T.-H. Hubert Chan et al.
for each stream, the communication cost for their protocol in L consecutive time steps
is O( k
λ ·
 L
W

log W) words.
Related Notion of Privacy. In Gantal et al. [11], it is mentioned that Dwork and Mc-
Sherry proposed semantic privacy which measures how the posterior distribution on
the database changes after the transcript is observed. In particular, it is shown that ϵ-
differential privacy implies (eϵ −1)-semantic privacy.
2
Preliminaries
2.1
Notations and Conventions
Given a positive integer m, we let [m] := {1, 2, . . ., m}, and use N := {1, 2, 3, . . .}
to index time steps. Let U be a set of n items. We use the standard notation ˜O(·) to
suppress poly-logarithmic factors.
We assume that an integer can be represented by O(1) words. Although later on
we use random distribution on unbounded integers, we show that with high probability
the magnitudes of the sampled integers are small. Hence, we can use modulo arithmetic
over some large enough integer. We do not explicit explain how each data source obtains
its source of randomness, but we assume that it takes O(1) operations to sample a
random variable that is independent of the input data stream.
2.2
Problem Setup
We assume a set of k ∈N streams, originating at k data sources (also referred to as
nodes) respectively. We assume that each data source only has limited memory and
computational power. Each stream σ(i) ∈UN where i ∈[k] is a sequence of items from
U, where σ(i)(t) ∈U is the item appearing at time step t in the i-th stream.
We consider an untrusted aggregator who wishes to continually monitor the heavy
hitters over a sliding window of size W ∈Z. Formally, the window at time step t over
stream σ is the multiset Wt(σ) := σ([t −W + 1, t]) containing all items coming to
stream σ between time t −W + 1 and t. Given k streams {σ(i) : i ∈[k]}, we write
W(i)
t
:= Wt(σ(i)) and denote W[k]
t
:= ⊎i∈[k]Wt(σ(i)) as the multiset containing all
items from the k streams in the window at time t.
The notion of heavy hitters is formally deﬁned as below. Given a multiset W, we use
|W| to denote the number of items it contains and for x ∈U, countx(W) is the number
of times item x appears in W. Given 0 < θ < 1, we say an item x ∈U is a θ-heavy
hitter in the multiset W if countx(W) ≥θ · |W|.
Deﬁnition 1 (Approximate Heavy Hitters). Given 0 < λ, θ < 1 and a multiset W, a
set S ⊆U is a λ-approximation for θ-heavy hitters in W if
1. the set S contains all θ-heavy hitters in W; and
2. if x ∈S, then x is a (θ −λ)-heavy hitter in W.
Deﬁnition 2 (λ-approximate Count). Given a multi-set W on items in U, and an item
x ∈U, an estimate c(x) is called a λ-approximate count for x with respect to W, if
|c(x) −countx(W)| ≤λ|W|.

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
145
Observe that if we have a λ-approximate count for every item x ∈U with respect to
W, then we can compute a 2λ-approximation for heavy hitters in W.
Communication Protocol. Consider a node receiving some stream σ. At every time
step t, upon receiving the item σ(t), the node might send messages to the aggregator to
update some counters. In the protocols that we consider, each message contains items
of the form c ∈Z or ⟨x, c⟩∈U × Z, each of which we assume can be expressed in
O(1) words. Given a (randomized) protocol Π, we denote by Π(σ) the (randomized)
transcript which consists of the messages sent at every time step by the node that applies
the protocol on the stream σ. We wish to reduce the amount of communication, say the
average number of words sent per time step.
Remark 1. In order to show that each item count has small relative error, we need a
lower bound on the total number items in the ﬁnite stream to absorb the noise error.
Moreover, the way in which we use PMG in combination with Arasu and Manku’s
algorithm [1] for ﬁx-sized windows requires the assumption that exactly one item comes
in the stream at every time step.
2.3
Deﬁning Privacy
As mentioned earlier, we deﬁne our privacy notions based on the following principles:
1) We advocate a need-to-know principle, the aggregator should ideally learn the least
amount of information necessary to perform the heavy hitter monitoring task. 2) While
the amount of information revealed to the aggregator is kept at a minimum, the in-
formation eventually revealed to the aggregator should satisfy event-level differential
privacy. In other words, any statistics revealed should be insentive to the occurrence or
non-occurrence of a single event. Intuitively, this helps to conceal whether some event
of interest has happened, e.g., whether a customer Alice has purchased a speciﬁc item.
Below, we formally deﬁne differential privacy and aggregator obliviousness.
Differential Privacy. In our setting, each node regards the contents on its stream as
private data. In particular, from the transcript of a node, the aggregator should not be
able to distinguish between input streams that are close to each other. Formally, two
different streams σ1, σ2 ∈UN are adjacent or neighbors (denoted as σ1 ∼σ2) if they
differ at exactly one time step. We use the notion of event-level differential privacy for
protocols.
Deﬁnition 3 (Differential Privacy for Protocols). Given ϵ > 0, a (randomized) pro-
tocol Π is ϵ-differentially private if for any adjacent streams σ1 and σ2, any subset O
of possible output transcripts, Pr[Π(σ1) ∈O] ≤exp(ϵ) · Pr[Π(σ2) ∈O], where the
randomness comes from the protocol.
Aggregator Obliviousness. As we shall see, as an intermediate step in our protocols,
each node has some private number and the goal is for the aggregator to learn the sum of
all the nodes’ numbers, but nothing more. Formally, each node has some data in D and
we use x ∈Dn to denote a conﬁguration of all the nodes’ data. Intuitively, a protocol
Π is aggregator oblivious with respect to some function f : Dn →O if for all x and
y such that f(x) = f(y), no polynomial-time adversary can distinguish between the
transcripts Π(x) and Π(x) with non-negligible probability.

146
T.-H. Hubert Chan et al.
Deﬁnition 4 (Aggregator Obliviousness). Let κ ∈N be a security parameter. A pro-
tocol ensemble {Πκ}κ∈N is aggregator obliviousness with respect to the function f :
Dn →O if there exists a negligible function η : N →R+ such that for all x and y such
that f(x) = f(y), for all decisional probabilistic polynomial-time Turing machines A,
| Pr[A(Πκ(x)) = 1] −Pr[A(Πκ(y)) = 1]| ≤η(κ),
where the probability is over the randomness of the protocol Πκ and the Turing machine
A.
2.4
Deﬁning Utility
Recall that for each i ∈[k], each node i receives some stream σ(i) and follows some
(randomized) protocol to send messages to the aggregator in every time step. Based
on the messages received up to time t, the aggregator computes for each x ∈U some
number A(t, x), which is an estimate for countx(W[k]
t ). Observe that A(t, x) is a ran-
dom variable, whose randomness comes from the randomized protocols. We use the
following notion to measure the usefulness of A(t, ·) with respect to W[k]
t
for each t.
Deﬁnition 5 ((ξ, δ)-Usefulness). Suppose W is a multiset containing items in U, and
A ∈RU is a collection of random variables indexed by U. Then, A is (ξ, δ)-useful with
respect to W, if with probability at least 1 −δ, for every item x ∈U,
|A(x) −countx(W)| ≤ξ; in particular, if ξ = λ|W|, then A(x) is a λ-approximate
count for x with respect to W.
Deﬁnition 6 ((ξ, δ)-Simultaneous Usefulness.). Let T be an index set. Suppose for
any t ∈T , Wt is a multiset containing items in U, and At is a collection of random
variables indexed by U. Then, {At}t∈T is (simultaneously) (ξ, δ)-useful with respect
to {Wt}t∈T , if with probability at least 1 −δ, for every t ∈T and every item x ∈U,
|At(x) −countx(Wt)| ≤ξ.
3
Achieving Differential Privacy
3.1
Roadmap
This section describes a protocol between the data sources and an aggregator, allowing
the aggregator to continually monitor the heavy hitters over a sliding window. We will
show how to achieve event-level differential privacy in this section. Later in Section 4,
we show how to achieve aggregator obliviousness.
We proceed with the following three-step recipe:
1. The PMG algorithm outputs the heavy hitters in a single stream. In Section 3.2,
we describe a private streaming algorithm, which allows a single data source to
output the approximate heavy hitters in a stream, after a one-pass scan of the en-
tire stream. Since this algorithm builds on top of the Misra-Gries streaming al-
gorithm [19], we refer to it as the Private Misra-Gries (PMG) algorithm. The MG
Algorithm maintains an approximate vector of item counts by storing only non-zero
counts explicitly for only a small number of items. The main technical challenge
to privatize the MG Algorithm is to show that this approximate vector has small
sensitivity.

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
147
2. The PCC algorithm continually monitors the recent heavy hitters in a single
stream. In Section 3.3, we extend the aforementioned PMG algorithm to derive
a Private Continual Heavy-hitter (PCC) algorithm, which supports the continual
monitoring of heavy hitters over a sliding window in a single stream. The main
technique in this step is the use of a binary interval tree which allows us to achieve
small memory when the window size is large.
3. The PDCH-LU protocol continually monitors the recent heavy hitters across
multiple streams. Finally, in Section 3.4, we extend the above PCC algorithm,
which works for a single stream, to the distributed setting. In the resulting protocol
PDCH-LU (Private Distributed Continual Heavy-hitter - Lazy Update), in order to
save communication cost, each data source sends sanitized updates to the aggre-
gator whenever necessary (hence lazy updates), to inform the aggregator of latest
trends in its observed stream. The aggregator can in turn continually output the
approximate heavy hitters across all streams over a sliding window.
3.2
Private Misra-Gries Algorithm
In this section, we consider a sub-problem, which will be a useful building block to
construct the private streaming protocol at a node. Given a stream of length T and an
error parameter 0 < λ < 1, the goal is to estimate the number of times each item in U
appears in the stream with additive error λT .
Our approach is based on the (non-private) MG Algorithm [19], which keeps explicit
counters for only O( 1
λ) items. Observe that since we accept λT additive error, if an
item x ∈U appears for less than λT times in the stream, then we do not need to keep
a counter explicitly for x ∈U and can give an estimate count of zero. Because at most
O( 1
λ) items can appear for at least λT times in a stream of length T , intuitively it is
sufﬁcient to keep O( 1
λ) explicit counters. The MG Algorithm makes sure that at any
point in time at most O( 1
λ) items have explicit non-zero counts. If an item arrives and
we need to create an extra counter, all existing non-zero counters are decremented by
1; this step keeps the number of non-zero counters small. On the other hand, whenever
a non-zero counter of some item x decreases by 1, there are Θ( 1
λ) other items that also
have their counts decreased by 1. Since this can happen for at most λT times, the ﬁnal
count for each item has additive error at most λT .
At the end of the MG Algorithm, the output corresponds to a count vector f ∈ZU,
which has at most O( 1
λ) non-zero coordinates. Lemma 2 states that this vector has
sensitivity at most O( 1
λ), and hence we can apply the techniques of geometric noise to
achieve differential privacy. The properties of the private version of the MG Algorithm
are given in the following lemma, whose proof is given in the full version [4].
Lemma 1 (Private MG Algorithm). Given a privacy parameter ϵ > 0 and an ap-
proximation parameter 0 < λ < 1, there is a (randomized) mechanism M, denoted
as PMG(ϵ, λ) (Private Misra-Gries), that takes any ﬁnite stream σ and after one pass
outputs a vector f ∈ZU such that the following properties hold.
1. ϵ-Differential Privacy: for any adjacent streams σ1 and σ2, any subset O ⊆ZU,
Pr[M(σ1) ∈O] ≤exp(ϵ) · Pr[M(σ2) ∈O], where the probability is over the
randomness from the mechanism.

148
T.-H. Hubert Chan et al.
2. Utility: Suppose 0 < δ < 1, and the length T of the stream σ satisﬁes T ≥
32
λ2ϵ log n
δ . Then, the vector f is (λT, δ)-useful with respect to the multiset σ([T ])
of items in the stream.
3. The mechanism uses only O( 1
λ) words of memory. In particular, at most O( 1
λ)
coordinates of f are non-zero. Moreover, it takes O( 1
λ) operations to process each
item, and samples O( 1
λ) random variables in total.
We modify the Misra-Gries Algorithm [16; 19] to get a counting mechanism, which we
call the Private Misra-Gries Algorithm (PMG) and is given in Algorithm 1. We outline
the main ideas of the algorithm. In the literature, it is common to achieve differential
privacy by adding geometric noise. However, since we want to be careful about memory
usage, we want the output of the mechanisms to be integral. Therefore, we add noises
sampled from symmetric geometric distributions [12].
Deﬁnition 7 (Geometric Distribution). Let α > 1. We denote by Geom(α) the sym-
metric geometric distribution that takes integer values such that the probability mass
function at l is α−1
α+1 · α−|l|.
The following property of symmetric geometric distribution is useful for designing dif-
ferentially private counting mechanisms.
Fact 1. Let u, v ∈Zn be two vectors such that ||u −v||1 ≤Δ, where ||u −v||1 =
n
i=1 |ui −vi| is the ℓ1-norm of u −v. Let r ∈Zn be a random vector whose coordi-
nates are independent random variables sampled from symmetric geometry distribution
Geom(exp( ϵ
Δ)). Then, for any vector p ∈Zn, Pr[u + r = p] ≤exp(ϵ) · Pr[v + r = p].
Deﬁnition 8 (Sensitivity). Let f : UT →Zn be a function that takes a stream of length
T as input. The sensitivity of f, denoted by Δ(f), is maxσ1∼σ2 ||f(σ1) −f(σ2)||1.
By Fact 1, for any function f : UT →Zn such that Δ(f) ≤Δ, we can make
its output ϵ-differentially private by adding independent random noise sampled from
Geom(exp( ϵ
Δ)) to coordinates of f. The following lemma explains what parameter one
should use for the geometric distribution in the PMG Algorithm to achieve differential
privacy. We give its proof in the full version [4].
Lemma 2. In Algorithm 1 after Line 10, the function f : UT →ZU has sensitivity at
most β + 1, where β := ⌈2
λ⌉.
3.3
Private Continual Heavy-Hitter Monitoring over a Sliding Window
In this section, we use PMG to construct a differentially private mechanism named
Private Continual Heavy-hitter (PCC), that uses small memory and maintains some
efﬁcient data structure at every time step.
As intuitively illustrated as in Figure 2, we build a binary interval tree, where each
leaf node represents
 λW
4

(= 1 in the ﬁgure) time steps, and each non-leaf node rep-
resents a range of time steps. (Note that Figure 2 only depicts the bottom few levels of
this binary interval tree due to reasons stated below). We refer to each node as a block,

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
149
Input: A privacy parameter ϵ, an approximation parameter λ, and a ﬁnite stream σ ∈UT
of length T .
Output: A vector f ∈ZU, where f(x) is a λ-approximate count of x with respect to
σ([T ]) with high probability.
1 For each x ∈U, f(x) and f(x) are (implicitly) initialized to 0;
2 β ←⌈2
λ⌉;
3 for t ←1 to T do
4
f(σ(t)) ←f(σ(t)) + 1;
5
if the number of items x such that f(x) > 0 exceeds the threshold β then
// the decreasing step
6
for x ∈U such that f(x) > 0 do
7
f(x) ←f(x) −1;
8
end
9
end
10 end
// The sensitivity Δ(f) ≤β + 1
11 for each x ∈U do
// In the full version, we give a faster procedure that
samples only O(β) random variables and achieves the
same output distribution.
12
Sample a fresh independent noise rx ∼Geom(exp(
ϵ
β+1));
13
f(x) ←max{f(x) + rx, 0};
// we only keep the top β non-zero f(x)’s
14
if there are more than β items x with f(x) ̸= 0 then
15
Let y be the item with smallest non-zero f(y) (resolving ties arbitrarily);
16
Set f(y) ←0;
17
end
18 end
19 Output f;
Algorithm 1. Private Misra-Gries Algorithm PMG(ϵ, λ)
and run the subroutine PMG algorithm for each block, with appropriate privacy and
approximation parameters. To output the heavy hitters for any time range, it sufﬁces to
“sum up” a logarithmic number of blocks in the binary interval tree – since any range
can be represented by the union of a logarithmic number of disjoint blocks. In Figure 2,
since we consider a window size of W = 7, we only need the bottom 3 levels of the
binary tree. The main purpose of the binary interval tree technique is to save memory
and allow faster computation.
Small Memory. The amount of memory necessary is ˜O( 1
λ) independent of W. To
achieve this, we use a garbage collection technique, where a data source saves only
the blocks which will later be needed, and discard all “expired” blocks which will no
longer be needed. As shown in Figure 2, at any point of time, a block can be one of
the following four types: 1) expired, i.e., will no longer be needed in the future; 2)
active, i.e., the block has completed construction, and will be needed now or in the
future; 3) under-construction, i.e., the heavy hitters for this block are currently being

150
T.-H. Hubert Chan et al.
Fig. 2. Continual counting over a sliding window of the past week
constructed; or 4) future, i.e., construction for this block will start at some point in the
future. Speciﬁcally, in the PCC algorithm, a data source saves only the blocks that are
either active or under construction – and the number of such nodes is O( 1
λ) at any point
in time. Observe that the number of counters kept for binary nodes at different levels
are different, and we later give a calculation to show that the total number of counters
kept at any time is O( 1
λ log2 1
λ).
Faster Computation. Observe that we could achieve even smaller memory if we only
store the leaf nodes of the binary construction. However, in order to produce an estimate
count over a window, we would need to look at Ω( 1
λ) leaf nodes. Using the binary
construction, we only need to look at O(log 1
λ) binary tree nodes for count estimation
at each step.
Low Communication Bandwidth. Notice we could achieve even smaller memory if
only one leaf node (corresponding to W0 =
 λW
4

time steps) in the binary tree con-
struction is stored at any time; however, updates need to be sent to the aggregator every
W0 time steps. As we shall see in Section 3.4, if information about each stream in the
current window is kept at each node, then the communication bandwidth to the aggre-
gator can be greatly reduced.
Note that a similar binary tree technique was also used in [1; 5].
We now give a formal description of the PCC algorithm, as well as its theoretic
guarantees. At every time step t, PCC maintains a dictionary Pt, which is a collection
of at most O( 1
λ log 1
λ) pairs (x, cx) ∈U ×Z where for every item x ∈U, item x appears
in at most one pair in Pt. Hence, Pt can also be interpreted as a vector ZU or a function
from U to Z in the natural way: Pt(x) := cx if (x, cx) ∈Pt, and Pt(x) := 0 otherwise.
Observe that only non-zero counts need to be stored in the dictionary , and we denote
by |Pt| the number of items x having non-zero counts Pt(x). The following lemma is
the main result of the section.
Lemma 3 (Private Continual Heavy Hitter Monitoring). Given a privacy parameter
ϵ > 0, and an approximation parameter 0 < λ < 1, there exists a continual counting
mechanism M, denoted as PCC(ϵ, λ) (Private Continual Heavy-hitter), that takes an
inﬁnite data stream σ ∈UN, and maintains at every time step t a dictionary Pt ∈ZU.
We write M(σ) := (Pt)t∈N ∈ZN×U. The following properties hold.

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
151
1. ϵ-Differential Privacy: for any adjacent streams σ1 and σ2, any subset O ⊆ZN×U,
Pr[M(σ1) ∈O] ≤exp(ϵ) · Pr[M(σ2) ∈O], where the probability is over the
randomness from the mechanism.
2. Utility: Suppose 0 < δ < 1 and L > 0. If W ≥Θ( 1
λ2ϵ(log2 1
λ) · log( n
δ log 1
λ)).
Then, at every time step t ∈N, Pt is (λW, δ)-useful with respect to Wt. If W ≥
Θ( 1
λ2ϵ(log2 1
λ) · log( L+W
λW · n
δ )), then for any T ≥L + 1, {Pt}t∈[T −L,T ] is simul-
taneously (λW, δ)-useful with respect to {Wt}t∈[T −L,T ].
3. The mechanism uses only O( 1
λ log2 1
λ) words of memory. Moreover, it takes
O( 1
λ log 1
λ) operations to process each item in the stream, and samples at most
O( 1
λ log 1
λ) random variables in each time step.
4. For every time step t, |Pt| = O( 1
λ log 1
λ).
Scheme Description PCC(ϵ, λ). We assume 1
λ is a power of 2; otherwise, let λ′ :=
max{ 1
2k :
1
2k < λ} and run PCC(ϵ, λ′).
Let W0 :=
 λW
4

> 0, and let ℓ:= log 4
λ. Note that we have W
2 ≤W0·2ℓ≤W. We
divide the time steps into binary hierarchical blocks with ℓ+ 1 levels, where all blocks
at the same level have the same size, are disjoint and cover all time steps. In particular,
for 0 ≤i ≤ℓand j ≥1, the jth block at level i is the interval [(j −1)Wi + 1, jWi] of
Wi := 2i · W0 time steps; we use block Bi
j to denote the multiset of items from stream
σ contained in this interval, i.e., Bi
j = σ([(j −1)Wi + 1, jWi]). At any time t, each
block Bi
j is in one of the following four states.
1. future: None of Bi
j’s items has come into Wt(σ), i.e., (j −1)Wi + 1 > t;
2. under-construction: Some of Bi
j’s items are in Wt(σ), and the remaining items
have not come into Wt, i.e., (j −1)Wi + 1 ≤t and jWi > t;
3. active: Bi
j is totally within Wt(σ), i.e., t −W + 1 ≤(j −1)Wi + 1 ≤jWi ≤t;
4. expired: At least one of Bi
j’s items has expired, i.e., (j −1)Wi + 1 < t −W + 1.
For
each
block
Bi
j,
right
before
the
time
step
when
its
state
becomes
under-construction, i.e., t
=
(j −1)Wi + 1, PCC(ϵ, λ) initiates an instance of
PMG(ϵi, λi) on Bi
j, where ϵi :=
ϵ
2ℓ−i+1 and λi :=
1
2i(ℓ+1). When Bi
j becomes ac-
tive, PMG(ϵi, λi) produces a vector fBi
j ∈ZU, which has O( 1
λi ) non-zero coordinates.
Then, PCC(ϵ, λ) uses O( 1
λi ) words of memory to maintain this vector until Bi
j expires.
Cover by Disjoint Active Blocks. Observe that at any time t, there exists a collection
Ct of disjoint active blocks, such that Ct contains at most two blocks from each level,
and that the union of blocks in Ct is the union of all active level-0 blocks. At any
time, PCC(ϵ, λ) maintains the dictionary Pt, where Pt = {(x, 
B∈Ct fB(x)) : x ∈
U and 
B∈Ct fB(x) > 0}.
We prove that PCC maintains differential privacy. The analysis of the remaining
properties is similar to that in [1, Section 5], and we include the proofs in the full
version [4].
Privacy Guarantee. Observe that the output of PCC is a deterministic function of
PMG’s outputs on all blocks. Hence, we need only to show ϵ-differential privacy is
maintained with respect to ( fB)∀B. Consider an item arriving at time step t. We analyze
which of the blocks would be affected if σ(t) is replaced with a different item. It is not

152
T.-H. Hubert Chan et al.
hard to see that the item σ(t) can be in at most one block at each level. Observe that fB
for a level-i block B maintains ϵi-differential privacy, where ϵi =
ϵ
2ℓ−i+1 and observe
that l
i=0 ϵi ≤ϵ. Hence, we conclude that PCC(ϵ, λ) preserves ϵ-differential privacy.
3.4
Privately and Continually Monitoring Heavy Hitters Across Distributed
Streams
In this section, we use PCC to design a protocol between k data sources and an ag-
gregator, allowing the aggregator to continually monitor the global heavy hitters. The
resulting protocol, called PDCH-LU (which stands for Private Distributed Continual
Heavy-hitter - Lazy Update) has low communication cost; moreover, the messages sent
by each data source is differentially private against the aggregator.
Observe that each node could send the privatized updates to the aggregator at every
time step in order for the aggregator to compute the approximate heavy hitters. How-
ever, to save communication bandwidth, we use a lazy update approach: updates are
only required when the count of an item changes by a huge amount. Chan et al. [3]
gave a distributed algorithm (called Approximate Counting (AC)) based on this idea
and proved that it achieves small error. Since the AC Algorithm in [3] only needs an
approximate count in the current window for each item in each stream at any time, our
PCC algorithm is sufﬁcient for this purpose. We give the main result and the construc-
tion of the protocol; the detailed analysis is given in the full version [4].
Theorem 1. Suppose ϵ > 0 is a privacy parameter, 0 < λ < 1 is an approximation
parameter, W is the window size and L is some positive integer. Given k streams each
received by a node, every node can run an ϵ-differentially private communication pro-
tocol with the same time and space performance as PCC(ϵ, λ
11) in Lemma 3 to send
messages to the aggregator such that if W ≥Θ( 1
λ2ϵ(log2 1
λ) log( L+W
λW
· kn
δ )), then for
every time interval T = [T + 1, T + L] (where T ≥W), with probability at least 1 −δ,
at every time t ∈T , the aggregator can maintain a λ-approximate count for every item
with respect to the current window in all streams, and the total communication cost by
all nodes in the period T is O( k
λ ·
 L
W

log W) words.
Algorithm for the Aggregator. The aggregator maintains a counter ci(x) (initially 0)
for each stream i ∈[k] and each item x ∈U. Upon receiving a message ⟨x, c⟩from
node i, the aggregator updates the counter ci(x) := c. In each time step t, the aggregator
calculates a count c(x) = 
i∈[k] ci(x); to produce a 2λ-approximate set of θ-heavy
hitters, the aggregator releases the set of items x such that c(x) ≥(θ −λ)kW.
Protocol for Each Data Source. We use Algorithm AC (Approximate Counting) in
[3, Section 2.2] to get a protocol (shown in Algorithm 2), denoted as PDCH-LU(ϵ, λ)
(Private Distributed Heavy-hitter - Lazy Update). Each node i runs an instance of
PDCH-LU(ϵ, λ) on the stream σ(i) it receives.
4
Achieving Aggregator Obliviousness
The main construction described earlier in Section 3 is a protocol for k nodes to com-
municate with an aggregator, whose task is to keep track of heavy hitters over a sliding

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
153
Input: A privacy parameter ϵ, an approximation parameter λ, and a stream σ ∈UN
Run an instance of PCC(ϵ, λ
11) and (implicitly) initialize Last(x) := 0 for each x ∈U;
// We only store non zero Last(x)’s.
for t ←1 to ∞do
for each x such that Pt(x) > 0 or Last(x) > 0 do
Up: if Pt(x) > Last(x) +
9
11 · λW , send ⟨x, Pt(x)⟩and set Last(x) ←Pt(x);
Off: if Last(x) > 0 and Pt(x) <
3
11 · λW , send ⟨x, 0⟩and set Last(x) ←0;
Down: if Pt(x) < Last(x) −
9
11 · λW , send ⟨x, Pt(x)⟩and set
Last(x) ←Pt(x);
end
end
Algorithm 2. PDCH-LU(ϵ, λ)
window. This protocol guarantees differential privacy at the event level, i.e., the statis-
tics released to the aggregator is not affected by the change of one event.
In this section, we describe a protocol which achieves a stronger level of privacy
protection, i.e., we additionally achieve aggregator obliviousness on top of event-level
differential privacy. Speciﬁcally, we wish to reveal the minimum amount of information
possible to the aggregator,for it to successfully perform the heavy hitter monitoring task.
The main techniques we use to achieve aggregator obliviousness include Bloom ﬁl-
ters [2] as well as special encryption schemes [15; 20; 21] that support the controlled
decryption of selected statistics. Using these techniques to augment the PMG protocol
described earlier, we achieve aggregator obliviousness in the sense that the aggregator
learns only the approximate counts of each item, but nothing else. In particular, the ag-
gregator does not learn which data sources are contributing to the heavy hitters and how
much their contributions are.
In our protocol to be described later in this section, each node communicates the
update with the aggregator every W0 =
 λW
4

time steps. We then employ Bloom
ﬁlters to effectively reduce the bandwidth overhead. Observe that without the Bloom
ﬁlters, we would need to perform n secure additions, one for each item, for each update.
We shall see that using Bloom ﬁlters, we can reduce the dependence of the number of
additions per update on n to O(log n).
4.1
Background on Special Encryption Scheme
As a building block for achieving aggregator obliviousness, we employ a special en-
cryption scheme which supports the conditional decryption of selected statistics. In
particular, we can use either the encryption scheme proposed by Shi et al. [21], Ras-
togi et al. [20], or Kursawe et al. [15] In comparison, the scheme by Shi et al. [21]
requires uni-directional communication from the data sources to the aggregator, but the
decryption algorithm is more expensive; whereas the scheme by Rastogi et al. [20] re-
quires bi-directional communication between the data sources and the aggregator, but
has smaller decryption overhead. The scheme by Kursawe et al. [15] only needs uni-
directional communication and has low overhead, but each node needs to store Θ(k)
keys corresponding to all other nodes. We now give a high-level overview of these

154
T.-H. Hubert Chan et al.
special encryption schemes. The special encryption schemes we employ typically in-
volve the following algorithms or phases:
Setup. In a one-time setup phase, cryptographic keying materials are distributed to all
data sources and the aggregator. In particular, each data source receives an encryption
key, and the aggregator receives a cryptographic capability which will allow it to later
decrypt the sum of all data sources in each aggregation time step. The setup phase can
either be performed by an ofﬂine authority (which will no longer be needed after the
setup phase); or through an interactive multi-party protocol amongst the data sources
and the aggregator.
Periodic Encryption and Aggregation. In each time step, each data source i ∈[k] en-
crypts a value xi using the encryption key established in the setup phase, and sends
the ciphertext to the aggregator. After receiving ciphertexts from all data sources, the
aggregator can use its cryptographic capability to decrypt the value k
i=1 xi, but learn
nothing else. In the construction by Shi et al. [21] and Kursawe et al. [15], this decryp-
tion is done solely by the aggregator, whereas in the scheme by Rastogi et al. [20], the
aggregator needs to communicate with the data sources to perform decryption.
In the remainder of this section, we will use this special encryption scheme as a
blackbox – for more algebraic details on how these schemes are constructed, we refer
the readers to [15; 20; 21].
4.2
Augmenting PMG Algorithm with Secure Bloom Filters
Straightforward Solution using Cryptography. We ﬁrst describe a straightforward
construction using one of the special encryption schemes [15; 20; 21]. A brief back-
ground on these encryption schemes was given in Section 4.1. We show that one draw-
back of this straightforward construction is its high bandwidth overhead. Later, we shall
employ Bloom ﬁlters to reduce the bandwidth consumption.
The basic idea is to apply the special encryption scheme all items in the universe.
Suppose each of the k nodes is running PMG(ϵ, λ) on its ﬁnite stream as described
in Section 3.2. Each node v ∈[k] produces some fv : U →Z, which is represented
by at most β = O( 1
λ) non-zero counters. In every time step, for each x ∈U, each
data source encrypts its observed frequency fv(x), and sends the ciphertext to the ag-
gregator. The aggregator may then use its cryptographic capability to decrypt the total
frequency 
v∈[k] fv(x) for each x ∈U – and meanwhile, the security of these en-
cryption schemes guarantee that the aggregator learns nothing else beyond the total
frequency of each item.
It is not hard to see that each node needs to send Ω(n) words (proportional to the size
of U) to the aggregator. Even though each node only has β non-zero counters, it still
has to participate in every addition such that the aggregator does not know where the
non-zero values come from. We would like to decrease the communication cost through
the use of Bloom ﬁlters.
Construction with Bloom Filters. Let 0 < δ < 1 be the desired failure probability, i.e.,
with probability at least 1 −δ, the aggregator can retrieve 
v∈[k] fv(x) for all x ∈U.
Let P :=

ln n
δ

and Q := ⌈ekβ⌉, where e is the natural number and β is the maximum

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
155
number of non-zero counters for each node. We assume there is a public family {Hp :
U →[Q]}p∈[P ] of random hash functions that satisfy the following properties.
1. The functions Hp are totally independent over different p ∈[P].
2. For each p ∈[P], Hp is pairwise independent over U, i.e., for x ̸= y, Hp(x) and
Hp(y) are independent; moreover, for each x ∈U and each q ∈[Q], Pr[Hp(x) =
q] = 1
Q.
Bloom Filters for Each Node. Each node v constructs a table Av of size P × Q that is
constructed in the following way.
1. Initially, every entry Av[p][q] := 0.
2. For each x ∈U such that fv(x) ̸= 0 (note that there are at most β such x’s), for
each p ∈[Q], increment A[p][Hp(x)] by fv(x).
Secure Addition of Bloom Filters. Using one of the secure periodic schemes [15; 20;
21] described above, the aggregator learns for each p ∈[P] and q ∈[Q], A[p][q] :=

v∈[k] Av[p][q]. Observe that each node sends O(PQ) = O( k
λ log n
δ ) words to the
aggregator.
Retrieving Sum of Counts for Each Item. For an item x, the aggregator computes
minp∈[P ] A[p][Hp(x)].
Theorem 2. With probability at least 1 −δ, the aggregator retrieves 
v∈[k] fv(x) ac-
curately for all x ∈U.
Proof. Fix some x ∈U. Observe that the aggregator makes a mistake for item x iff for
all p ∈[P], there exists some y ̸= x such that Hp(x) = Hp(y) and there exists v ∈[k]
such that fv(y) ̸= 0.
Observe that each node v can only have at most β non-zero counts. For ﬁxed p ∈
P, by pairwise independence of Hp over U and uniformity of Hp(y) over [Q], the
probability that there exists some y ̸= x such that some node has non-zero count for y
and Hp(x) = Hp(y) is at most kβ
Q ≤1
e.
By the total independence of Hp over p’s, it follows that the probability that the
aggregator makes a mistake for item x is at most
1
eP ≤δ
n. Using the union bound over
all x ∈U, it follows that the probability that the aggregator makes a mistake for some
item is at most δ, as required.
4.3
Distributed Protocol Achieving Aggregator Obliviousness
We next describe how the PMG Algorithm augmented with Bloom Filters can be used to
design a distributed protocol that achieves aggregator obliviousness – the resulting pro-
tocol is referred to as PDCH-BF (Private Distributed Continual Heavy-hitter - Bloom
Filter).
Each node performs the binary construction as in the PCC algorithm described in
Section 3.3. In particular, for a block at level i with size Wi = 2i · W0 = 2i · λW
4 ,
PMG(ϵi, λi) is run with ϵi :=
ϵ
2ℓ−i+1 and λi :=
1
2i(ℓ+1). As soon as PMG is completed
for a block, a Bloom ﬁlter is constructed for that block as described in Section 4.2. The
Bloom ﬁlter for that block is encrypted and sent to the aggregator.

156
T.-H. Hubert Chan et al.
For a particular block, after the aggregator has received the ciphertexts of the Bloom
ﬁlters from all the k nodes, it can decrypt the sum of the Bloom ﬁlters and reconstruct
the counts for all items in that block.
Observe that to estimate the counts in a window, the aggregator just needs the counts
from at most 2 log 1
λ blocks. Hence, in order to achieve failure probability of δ due to
the Bloom ﬁlters for each window, it sufﬁces to set the Bloom ﬁlter failure probability
for each block to be
δ
2 log 1
λ . There is also failure probability δ due to the randomness
introduced to achieve differential privacy. Hence, Lemma 3 implies the aggregator can
compute λ-approximate heavy hitters within a window with probability 1 −2δ.
Communication Cost. For each node, the communication cost for a block at level
i is O( k
λi log n
δ0 ) words, where λi =
1
2i·(l+1), l = log 4
λ and δ0 =
δ
2(l+1). More-
over, a block at level i is constructed every Wi := 2i · W0 = 2i · λW
4
time steps.
Hence, it follows that the average number of words of communication per time step is
O(
k
λW log2 1
λ log n log 1
λ
δ
).
5
Experiments
5.1
Experimental Setup
The data used in our experiment was constructed from the Netﬂix Contest Dataset,
which contains n = 17770 movies with 480189 users’ ratings from 1999-11-11 to
2005-12-31. We divided the users randomly into 100 groups to construct 100 streams.
We selected roughly 200 days’ data of each stream from 2002-09-23 to 2003-04-30
to conduct our experiments, where we continually monitored the moving average over
a window size of 90 days. We plot the result for the last 10 days by when the sys-
tem should have become stable. We use the following parameters in our experiments:
heavy-hitter fraction θ = 0.004 and error λ = 0.001. In our experiments, we consider
differential privacy parameter ϵ = 1, 2, 5, 10. Note that there is no consensus on what
privacy parameters are acceptable in practice, and even for ϵ = 1, the scheme still offers
some guarantee on privacy.
For the Bloom ﬁlters, we choose the number of hash functions to be P = 8, and
the array size for each hash function to be Q = ⌈e · 3600⌉(we explain the choice of Q
below).
Practical Optimization. Several aspects of our protocols can be further optimized in
this application.
1. Empirical Sensitivity of PMG Count Vector and Bloom Filter Size. Since the value
of λ we choose is small, in practice, the number of distinct movies observed by
each node in each day never exceeds O( 1
λ), the number of counters in PMG. If
we assume that the daily number of distinct movies never exceeds the number of
counters in PMG, the real sensitivity is O(1), and so we can use Geom(eϵ) noise for
PMG, instead of Geom(eΘ(ϵλ)). Moreover, we observe that the number of distinct
movies observed by all nodes in each day never exceeds 3600 and hence we can
choose Q = ⌈e · 3600⌉to be the size of the Bloom ﬁlter array for each hash.

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
157
2. Unnecessary Internal Blocks in Binary Construction. Since the width W of our win-
dow is small, W0 =
 λW
4

= 1. Hence, each leaf node in the binary tree corresponds
to one time step, and therefore,the binaryconstruction does not help to save memory
in this case. Moreover, since W is small, the cost of count reconstruction from W
leaf nodes is still better than the overheadof keeping track of the binary construction.
3. Relaxing Assumption on Item Arrival. Since we no longer need the binary con-
struction, we can further relax the assumption that exactly one item arrives at each
stream per time step. In our dataset, at least one item arrives at each stream at each
time step, and we do not need any upper bound on the number of arriving items.
Within each time step, PMG for each stream serializes the arriving items and pro-
cess them in any arbitrary order.
4. Estimating the Total Number of Arriving Items. With the assumption that only one
item arrives at each time step for each stream, it is trivial to compute the total num-
ber items within a window; this quantity is required for estimating the fraction an
item appears and deciding when to do lazy updates (this is the W in the conditions
for Up, Off and Down in PDCH-LU). However, now each stream also needs to
report to the aggregator the number of items that arrive each day, which has sen-
sitivity 1 for neighboring streams. Hence, in order to achieve ϵ-differential privacy
for the whole protocol, we can assign ϵ1 := 9ϵ
10 and ϵ2 :=
ϵ
10 such that each stream
runs PMG with ϵ1-differential privacy (using Geom(eϵ1) noise) and estimates the
number of items each day with ϵ2-differential privacy (using Geom(eϵ2) noise).1
Performance Metric. We consider the following performance measures. For each per-
formance measure, we plot its mean with an error bar of 2 standard deviations (hence
each plot could go negative).
1. Error in estimated fraction. For each day, suppose H is the set of items whose true
fraction in the current window is at least θ and H is the set of items whose estimated
fraction in the current window is at least θ −λ. For each item x ∈H ∪H, we
calculate the error E(x) which is the absolute difference between the true and the
estimated fractions of item x in the current window. We compute the mean and the
standard deviation of E(x) over x in H ∪H. We evaluate these statistics over time
for different protocols with various parameters.
2. Communication Cost. We measure the number of words each node sends to the
aggregator each day. For each day, we compute the mean and the standard deviation
of the communication cost over different nodes.
5.2
Results
Utility. In Figures 3, 4, and 5, we observe that the error in each case is well below the
theoretic guarantee λ = 0.001, and we interpret each ﬁgure as follows.
1 We give more privacy budget to PMG as it is more complicated, and less privacy budget to
the estimation of number of items each day as it is relatively simpler. It does not really affect
the asymptotic error as long as each part get a constant fraction of the privacy budget, but
experiment suggests that these parameters work well.

158
T.-H. Hubert Chan et al.
Fig. 3. Error under different protocols
Fig. 4. Error under different privacy parame-
ters for the PDCH-LU algorithm
Fig. 5. Error under different privacy parame-
ters for the PDCH-BF algorithm
Fig. 6. The number of words sent per data
source in the PDCH-LU protocol
Figure 3 compares the errors of the PDCH-LU and PDCH-BF protocols, and also
demonstrates a breakdown of the error, i.e., how much error is introduced by com-
pressing the bandwidth, and how much due to the noise necessary for differential pri-
vacy. With our choice of parameters, the Bloom ﬁlter should introduce almost no error
and hence DCH-BF(no noise) forms the baseline for comparison. The plot for protocol
PDCH-BF essentially reﬂects the error introduced when we wish to preserve differen-
tial privacy. The plot for DCH-LU(no noise) reﬂects the error introduced by lazy update
in order to save communication bandwidth. The interesting unexpected result is that for
PDCH-LU when we use lazy update together with noise to ensure differential privacy,
the extra noise does not seem to increase the error by much. In fact, the effect of lazy
updates seem to smooth out some of the error introduced by the added random noise.
Figures 4 and 5 plot the utility of PDCH-BF and PDCH-LU under different differen-
tial privacy parameter ϵ. As we decreases ϵ, the magnitude of noise increase and we can
see in Figure 5 that as expected the error for PDCH-BF is increased as well. However,
we can see that in Figure 4, that reducing ϵ has only a small effect on the performance
of PDCH-LU.
Communication cost. Figure 6 shows that in the PDCH-LU protocol, the number of
item updates sent by each node per day is around 5 and almost never above 20. Typi-
cally, each item update is under 10 bytes of data.

Differentially Private Continual Monitoring of Heavy Hitters from Distributed Streams
159
In comparison, we need to pay higher (but still reasonable) communication cost if we
wish to ensure aggregator obliviousness. In our PDCH-BF experiment, given 8 hashes
each having a ﬁlter of size Q = ⌈e · 3600⌉, and assuming that each Difﬁe-Hellman
ciphertext is 1024 bits, then the each sends about 10MB data per day to the aggregator.
References
[1] Arasu, A., Manku, G.S.: Approximate counts and quantiles over sliding windows. In: PODS
(2004)
[2] Bloom, B.H.: Space/time trade-offs in hash coding with allowable errors. Commun.
ACM 13(7), 422–426 (1970)
[3] Chan, H.-L., Lam, T.-W., Lee, L.-K., Ting, H.-F.: Continuous monitoring of distributed data
streams over a time-based sliding window. In: STACS (2010)
[4] Chan, T.-H.H., Li, M., Shi, E., Xu, W.: Differentially private continual monitoring of heavy
hitters from distributed streams. In: Cryptology ePrint Archive (2012)
[5] Hubert Chan, T.-H., Shi, E., Song, D.: Private and Continual Release of Statistics. In:
Abramsky, S., Gavoille, C., Kirchner, C., Meyer auf der Heide, F., Spirakis, P.G. (eds.)
ICALP 2010, Part II. LNCS, vol. 6199, pp. 405–417. Springer, Heidelberg (2010)
[6] Demaine, E.D., L´opez-Ortiz, A., Munro, J.I.J.: Frequency Estimation of Internet Packet
Streams with Limited Space. In: M¨ohring, R.H., Raman, R. (eds.) ESA 2002. LNCS,
vol. 2461, pp. 348–360. Springer, Heidelberg (2002)
[7] Dwork, C.: Differential Privacy. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds.)
ICALP 2006, Part II. LNCS, vol. 4052, pp. 1–12. Springer, Heidelberg (2006)
[8] Dwork, C.: A ﬁrm foundation for private data analysis. Commun. ACM 54(1), 86–95 (2011)
[9] Dwork, C., Naor, M., Pitassi, T., Rothblum, G.N.: Differential privacy under continual ob-
servation. In: STOC, pp. 715–724 (2010)
[10] Dwork, C., Naor, M., Pitassi, T., Rothblum, G.N., Yekhanin, S.: Pan-private streaming al-
gorithms. In: ICS, pp. 66–80 (2010)
[11] Ganta, S.R., Kasiviswanathan, S.P., Smith, A.: Composition attacks and auxiliary informa-
tion in data privacy. In: KDD, pp. 265–273 (2008)
[12] Ghosh, A., Roughgarden, T., Sundararajan, M.: Universally utility-maximizing privacy
mechanisms. In: STOC (2009)
[13] Goldreich, O.: The Foundations of Cryptography - Volume 2, Basic Applications. Cam-
bridge University Press (2004)
[14] Karp, R.M., Shenker, S., Papadimitriou, C.H.: A simple algorithm for ﬁnding frequent ele-
ments in streams and bags. ACM Trans. Database Syst. 28, 51–55 (2003)
[15] Kursawe, K., Danezis, G., Kohlweiss, M.: Privacy-Friendly Aggregation for the Smart-
Grid. In: Fischer-H¨ubner, S., Hopper, N. (eds.) PETS 2011. LNCS, vol. 6794, pp. 175–191.
Springer, Heidelberg (2011)
[16] Lee, L.K., Ting, H.F.: A simpler and more efﬁcient deterministic scheme for ﬁnding fre-
quent items over sliding windows. In: PODS (2006)
[17] McSherry, F.: Privacy integrated queries: an extensible platform for privacy-preserving data
analysis. In: SIGMOD Conference, pp. 19–30 (2009)
[18] Mir, D.J., Muthukrishnan, S., Nikolov, A., Wright, R.N.: Pan-private algorithms via statis-
tics on sketches. In: PODS, pp. 37–48 (2011)
[19] Misra, J., Gries, D.: Finding repeated elements. Sci. Comput. Program. 2(2), 143–152 (1982)
[20] Rastogi, V., Nath, S.: Differentially private aggregation of distributed time-series with trans-
formation and encryption. In: SIGMOD 2010, pp. 735–746 (2010)
[21] Shi, E., Chan, H., Rieffel, E., Chow, R., Song, D.: Privacy-preserving aggregation of time-
series data. In: NDSS (2011)
[22] Yi,K.,Zhang,Q.:Optimaltrackingofdistributedheavyhittersandquantiles.In:PODS(2009)

Adaptive Diﬀerentially Private Histogram
of Low-Dimensional Data
Chengfang Fang and Ee-Chien Chang⋆
School of Computing
National University of Singapore
{c.fang,changec}@comp.nus.edu.sg
Abstract. We want to publish low-dimensional points, for example 2D
spatial points, in a diﬀerentially private manner. Most existing mech-
anisms publish noisy frequency counts of points in a ﬁxed predeﬁned
partition. Arguably, histograms with adaptive partition, for example V-
optimal and equi-depth histograms, which have smaller bin-widths in
denser regions, would provide more statistical information. However, as
the adaptive partitions leak signiﬁcant information about the dataset, it
is not clear how diﬀerentially private partitions can be published accu-
rately. In this paper, we propose a simple method based on the obser-
vation that the sensitivity of publishing the sorted sequence of a dataset
is independent of the size of dataset. Together with isotonic regression,
the dataset can be reconstructed with high accuracy. One advantage of
the proposed method is its simplicity, in the sense that there are only a
few parameters to be determined. Furthermore, the parameters can be
estimated solely from the privacy requirement ϵ and the total number
of points, and hence do not leak information about the data. Although
the parameters are chosen to minimize the earth mover’s distance be-
tween the published data and original data, empirical studies show that
the proposed method also achieves high accuracy w.r.t. to some other
measurements, for example range query and order statistics.
1
Introduction
The popularity of personal devices equipped with location sensors leads to a large
amount of location data being gathered. Such data contain rich information and
would be valuable if they can be shared and published. As the data may reveal
location of an identiﬁed individual, it is important to anonymize the data before
publishing. The recently developed notion of diﬀerential privacy [5] provides a
strong form of privacy assurance regardless of the background information held
by the adversaries. Such assurance is important, as many case studies and past
events have shown that a seemingly anonymized dataset together with additional
knowledge held by the adversary could reveal information on individuals.
Most studies on diﬀerential privacy focus on publishing statistical values, for
instance, k-means [3], private coreset [7], and median of the database [20]. Pub-
lishing speciﬁc statistics or data-mining results is meaningful if the publisher
⋆Chang is partially supported by grant TDSI/09-003/1A.
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 160–179, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
161
knows what the public speciﬁcally wants. However, there are situations where
the publishers want to give the public greater ﬂexibility in analyzing and ex-
ploring the data, for example, using diﬀerent visualization techniques. In such
scenarios, it is desired to “publish data, not the data mining result” [8].
The histogram of a dataset contains rich information that can be harvested
by subsequent analysis. In the context of diﬀerent privacy, parallel composition
can be exploited to treat non-overlapping bins independently and thus achiev-
ing high accuracy. There are a number of research eﬀorts [14,2] investigating
the dependencies of frequencies counts of ﬁxed overlapping bins, where parallel
composition can not be directly applied. Such overlapping bins are interesting
as diﬀerent domain partition could lead to diﬀerent accuracy and utility. For in-
stance, Xiao et al. [28] proposes publishing wavelet coeﬃcients of an equi-width
histogram, which can be viewed as publishing a series of equi-width histograms
with diﬀerent bin-widths, and is able to provide higher accuracy in answering
range queries compare to a single equi-width histogram.
It is generally well accepted that equi-depth histogram and V-optimal histogram
provide more useful statistical information compare to equi-width histogram
[21,22], especially for multidimensional data. These histograms are adaptive in the
sense that the domain partitions are derived from the data such that denser regions
will have smaller bin-widths and the sparser regions will have larger bin-widths,
as illustrated in Fig. 7(b). Since the bin-widths are derived from the dataset, they
leak information about the original dataset. There are relatively few works that
consider adaptive histogram in the context of diﬀerential privacy. One exception
is the work by Xiao et al. [29]. Their method consists of two steps where ﬁrstly
synthetic data are generated from the diﬀerentially private equi-width histogram.
After that, a k-d tree (which can be viewed as an adaptive histogram) is generated
from the synthetic data, and the noisy counts are then released with the partition.
Machanavajjhala et al. [16] proposed a mechanism that publishes 2D histograms
with varying bin-widths, where the bin-widths are determined from a previously
released similar data. The histograms generated are not adaptive in the sense that
the partitions do not depend on the data to be published.
In this paper, instead of publishing the noisy frequency counts in equi-width
bins, we propose a method that directly publishes the noisy data, which in turn
leads to an adaptive histogram. To illustrate, let us ﬁrst consider a dataset
consisting of a set of real numbers from the unit interval, for example, the nor-
malized distance of Twitter users’ locations [1] to New York City (Fig. 1(a)).
We observe that sorting, as a function that takes in a set of real numbers from
the unit interval and outputs the sorted sequence, interestingly has sensitivity
one (Theorem 1). Hence, the mechanism that ﬁrst sorts, and then adds indepen-
dent Laplace noise of LAP(1/ϵ) to each element achieves ϵ-diﬀerential privacy.
Fig. 1(b) shows the noisy output data after the Laplace noise has been added
to the sorted sequence. Although seemingly noisy, there are dependencies to be
exploited because the original sequence is sorted. By using isotonic regression,
the noise can be signiﬁcantly reduced (Fig. 1(c)). To further reduce noise, be-
fore adding the Laplace noise, consecutive elements in the sorted data can be

162
C. Fang and E.-C. Chang
0
0.5
1
1.5
2
x 10
5
0
0.2
0.4
0.6
0.8
1
Points
Normalized Distance
(a) Sorted 1D points.
0
0.5
1
1.5
2
x 10
5
−5
0
5
Points
Distance
(b) The sorted points with Laplace noise
added. To avoid clogging, only 10% of the
points (randomly chosen) are plotted.
0
0.5
1
1.5
2
x 10
5
0
0.2
0.4
0.6
0.8
1
Points
Normalized distance
Original data
Reconstructed data
(c) Reconstructed with isotonic regres-
sion.
0
0.5
1
1.5
2
x 10
5
−0.04
−0.02
0
0.02
0.04
0.06
Points
Displacement
Reconstructed data
Reconstructed data with grouping
(d) The diﬀerences of the reconstructed
points from the original.
Fig. 1. Overview of proposed method
grouped and each point is replaced by the average of its group. Fig. 1(d) shows
the diﬀerence of the original and the reconstructed points with and without
grouping.
To extend the proposed method to higher dimension data, for example, lo-
cation data of 183,072 Twitter users in North America as shown in Fig. 2(a),
we employ locality-preserving mapping to map the multidimensional data to
one-dimension (Fig. 2(b)), such that any two close points in the one-dimension
domain are mapped from two close multidimensional points. After that, the pub-
lisher can apply the proposed method on the 1D points, and publish the reverse
mapped multidimensional points.
One desired feature of our scheme is its simplicity: there is only one param-
eter, the group size, to be determined. The group size aﬀects the accuracy in
three ways: (1) its eﬀect on the generalization error, which is introduced due to
averaging; (2) its eﬀect on the level of Laplace noise to be added by the diﬀer-
entially private mechanism; and (3) its eﬀect on the number of constraints in
the isotonic regression. Based on our error model, the optimal parameter can
be estimated without knowledge of the dataset distribution. In contrast, many

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
163
−130
−120
−110
−100
−90
−80
−70
−60
30
35
40
45
50
Longitude
Latitude
(a) Locations of Twitter users. To avoid
clogging, only 10% of the points (ran-
domly chosen) are plotted.
0
0.5
1
1.5
2
x 10
5
0
0.2
0.4
0.6
0.8
1
Points
Locations
(b) Sorted 1D images of the data.
Fig. 2. Twitter location data and their 1D images of a locality-preserving mapping
existing methods have many parameters whose optimal values are diﬃcult to
be determined diﬀerentially privately. For instance, although the equi-width his-
togram has only one parameter, i.e. the bin-width, its value signiﬁcantly aﬀects
the accuracy, and it is not clear how to diﬀerentially privately obtain a good
choice of the bin width.
Our error model utilizes the earth mover’s distance (EMD) to measure the
accuracy of the published data. Some existing works measure the accuracy of
a histogram by its distance, such as L2 distance or KL divergence, to a ref-
erence equi-width histogram. One limitation of this measurement is that the
reference histogram can be arbitrary and thus arguably ill-deﬁned. If the refer-
ence bin-width is too small, each bin will contain either one or no point, which
leads to signiﬁcantly large distance from a seemingly accurate histogram. On
the other hand, if its bin-width is too large, the reference histogram would be
over generalized. In contrast, EMD measures the distance of the published data
and original points, where the “reference” is the original points and thus well-
deﬁned. We conduct empirical studies to compare against a few related known
methods: equi-width histogram, wavelet-based method [28] and smooth sensitiv-
ity based median-ﬁnding [20]. Although our method is designed to minimize the
EMD, it also attains high accuracy w.r.t. other measurements. Empirical studies
shows that our method outperforms the wavelet-based method w.r.t. accuracy of
range-query, even for ranges with large sizes. It is also comparable to the smooth
sensitivity based method in publishing median.
Organization:
We ﬁrst describe some background materials in the next sec-
tion. In Section 3 we present our main ideas and mechanism, and show that the
proposed mechanism achieves diﬀerential privacy in Section 4. Next, in Section
5, we formulate and analyze how the group size aﬀects the accuracy and derive
a strategy to choose the group size based on this model. In Section 6, we com-
pare our mechanism with three known mechanisms: (1) equi-width histogram,
(2) wavelet-based method, and (3) smooth sensitivity based median-ﬁnding. In

164
C. Fang and E.-C. Chang
Section 7, we discuss the extensions and limitations of our method. Lastly, we
describe related works in Section 8 and conclude in Section 9.
2
Background
2.1
Diﬀerential Privacy and Laplace Noise
We treat a database as a multi-set (i.e. a set with possibly repeating elements),
and consider two datasets D1 and D2 of size n to be neighbors when D2 can be
obtained from D1 by replacing one element, i.e. D1 = {x} ∪D2 \ {y} for some x
and y. Diﬀerential privacy with this deﬁnition of neighborhood is known as the
bounded diﬀerential privacy [6,13].
A randomized algorithm (also known as mechanism) A achieves ϵ diﬀerential
privacy if,
Pr[A(D1) ∈S] ≤exp(ϵ) × Pr[A(D2) ∈S]
for all S ⊆Range(A), where Range(A) denotes the output range of the algorithm
A, and for any pair of neighboring datasets D1 and D2.
For a function f : D →Rk, the sensitivity [5] of f is deﬁned as
Δ(f) := max∥f(D1) −f(D2)∥1,
where the maximum is taken over all pairs of neighboring D1 and D2. It is
shown [6] that the mechanism A
A(D) = f(D) + (Lap(Δ(f)/ϵ))k
achieves ϵ-diﬀerential privacy, where (Lap(Δ(f)/ϵ))k is a vector of k indepen-
dently and randomly chosen values from the Laplace distribution with standard
deviation 2Δ(f)/ϵ.
2.2
Isotonic Regression
Given a sequence of n real numbers a1, . . . , an, the problem of ﬁnding the least-
square ﬁt x1, . . . , xn subjected to the constraints xi ≤xj for all i < j ≤n is
known as the isotonic regression. Formally, we want to ﬁnd the x1, . . . , xn that
minimizes
n

i=1
(xi −ai)2,
subjected to xi ≤xj for all 1 ≤i < j ≤n.
The unique solution can be eﬃciently found using pool-adjacent-violators al-
gorithms in O(n) time [10]. When minimizing w.r.t. ℓ-1 norm, there is also
an eﬃcient O(n log n) algorithm [25]. There are many variants of isotonic re-
gression, for example, variants with a smoothness component in the objective
function [27,17].

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
165
2.3
Locality-Preserving Mapping
A locality-preserving mapping T : [0, 1]d →[0, 1] maps d-dimensional points to
the unit interval, while preserving locality. In this paper, we seek a mapping
that, if the mapped points T (x), T (y) are “close”, then x and y are “close” in
the d-dimensional space. More speciﬁcally, there is some constant c s.t. for any
x, y in the domain of the mapping T ,
∥x −y∥2 ≤c · (∥T (x) −T (y)∥)1/d.
(1)
The well-known Hilbert curve [9] is a locality-preserving mapping. It is shown
that for any 2D points x, y in the domain of T , ∥x −y∥2 ≤3

|T (x) −T (y)|.
Niedermeier et al. [19] showed that with careful construction, the bound can
be improved to 2

|T (x) −T (y)| for 2D points and 3.25 3
∥T (x) −T (y)∥for 3D
points. In our construction, for simplicity, we use Hilbert curve in our experi-
ments.
Note that it is challenging in preserving locality “in the other direction”,
that is, any two “close” points in the d-dimensional domain are mapped to
“close” points in the one-dimensional range [18]. Fortunately, in our problem,
such property is not required.
2.4
Datasets
We conduct experiments on two datasets: locations of Twitter users [1] (herein
called the Twitter location dataset)
and the dataset collected by Kaluˇza et
al. [12] (herein called Kaluˇza’s dataset). The Twitter location dataset contains
over 1 million Twitter users’ data from the period of March 2006 to March 2010,
among which around 200,000 tuples are labeled with location (represented in
latitude and longitude) and most of the tuples are in the North American con-
tinent, concentrating in regions around the state of New York and California.
Fig. 2(a) shows the cropped region covering most of the North American conti-
nent. The cropped region contains 183,072 tuples. The Kaluˇza’s dataset contains
164,860 tuples collected from tags that continuously record the location infor-
mation of 5 individuals. While some of the tuples consist of many attributes, in
our experiments, only the 2D location data are being used.
3
Proposed Approach
Before receiving the data, the publisher has to make a few design choices. The
publisher need to decide on a locality-preserving mapping T , and the strategy
(which is represented as a lookup table) of determining the group size from the
privacy requirement ϵ and the size of dataset n. Now, given the dataset D of size
n, and the privacy requirement ϵ, the publisher carries out the following:
A1. The publisher maps each point in D to a real number in the unit interval
[0, 1] using T , and lookups the group size based on n and ϵ. Let T (D) be
the set of transformed points. For clarity in exposition, let us assume that
k divides n.

166
C. Fang and E.-C. Chang
A2. The publisher sorts the mapped points, divides the sorted sequence into
groups of k consecutive elements, and then for each group, determines its
average over the k elements. Let the averages be S = ⟨s1, . . . , sn/k⟩.
A3. The publisher releases S = S + (Lap(ϵ−1)/k)(n/k) and the group size k.
A public user may extract information from the published data as follow:
B1. The user performs isotonic regression on S and obtains IR(S), and then
replaces each element si in IR(S) with k points of value si. Let P be the
set of resulting points.
B2. The user maps the data point back to the original domain, that is, computes
D = T −1(P). Let us call D the reconstructed data.
Note that the public user is not conﬁned to performing step B1 and B2. The
user may, for example, incorporates some background knowledge to enhance
accuracy. To relieve the public from computing step B1 and B2, the regression
and the inverse mapping can be carried out by the publisher on behalf of the
users. Nevertheless, the raw data S should be (although it is not necessary)
published alongside the reconstructed data for further statistical analysis.
4
Security Analysis
In this section, we show that the proposed mechanism (Step A1 to A3) achieves
diﬀerential privacy. The following theorem shows that sorting, as a function,
interestingly has sensitivity 1. Note that a straightforward analysis that treats
each element independently could lead to a bound of n, which is too large to be
useful.
Theorem 1. Let Sn(D) be a function that on input D, which is a multi-set con-
taining n real numbers from the unit interval [0, 1], outputs the sorted sequence
of elements in D. The sensitivity of Sn w.r.t. the bounded diﬀerential privacy
is 1.
Proof. Let D1 and D2 be any two neighboring datasets. Let ⟨x1, x2 . . . xi . . . xn⟩
be Sn(D1), i.e. the sorted sequence of D1. WLOG, let us assume that an element
xi is replaced by a larger value A to give D2, for some 1 ≤i ≤n−1 and xi < A.
Let j to be largest index s.t. xj < A ≤1. Hence, the sorted sequence of D2 is:
x1, x2, . . . , xi−1, xi+1, . . . , xj, A, xj+1, . . . , xn.
The L1 diﬀerence due to the replacement is,
∥Sn(D1) −Sn(D2)∥1
= |xi+1 −xi| + |xi+2 −xi+1| + |xj −xj−1| + |A −xj|
= (xi+1 −xi) + (xi+2 −xi+1) + (xj −xj−1) + (A −xj)
= A −xi ≤1.
We can easily ﬁnd an instance of D1 and D2 where the diﬀerence A −xi = 1.
Hence, the sensitivity is 1.
⊓⊔

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
167
Since the sensitivity is 1, the mechanism Sn(D) + Lap(1/ϵ)n enjoys ϵ-diﬀerential
privacy. Also note that the value of n is ﬁxed. Hence, the size of D is not a secret
and is made known to the public.
The following corollary shows (proof omitted) that grouping (in Step A2) has
no eﬀect on the sensitivity.
Corollary 1. Consider a partition H = {h1, h2 . . . hm} of the indices {1, 2, . . .,
n}. Let SH(D) be the function that, on input D, which is a multi-set containing
n real numbers from the unit interval [0, 1], outputs a sequence of m numbers:
yi =

j∈hi
xj,
for 1 ≤i ≤m where ⟨x1, x2, . . . , xn⟩is the sorted sequence of D. The sensitivity
of SH is 1.
Note that the grouping in step A2 is a special partition with equal-sized hi’s,
whereas Corollary 1 gives a more general result where H can be any partition.
From Corollary 1, the proposed mechanism achieves ϵ-diﬀerential privacy.
5
Analysis and Parameter Determination
The main goal of this section is to analyze the eﬀect of the privacy requirement
ϵ, dataset size n and the group size k on the error in the reconstructed data,
which in turn provides a strategy in choosing the parameter k given n and ϵ.
Intuitively, when n and ϵ are ﬁxed, the choice of parameter k aﬀects the accu-
racy in following three ways: (1) a larger k decreases the number of constraints
in isotonic regression, which leads to lower noise reduction; (2) a larger k reduces
the eﬀect of the Laplace noise; and (3) a larger k introduces higher generalization
error due to averaging.
Our analysis consists of the following parts. We ﬁrst describe our utility func-
tion in Section 5.1. In Section 5.2, we consider the case where k = 1 and empiri-
cally show that the expected error of a typical dataset can be well approximated
by the expected error on a synthetic equally-spaced dataset. Let us call this
error Errn,ϵ. Next in Section 5.3, we investigate and estimate the generalization
error due to the averaging and show that with a reasonable assumption on the
dataset distribution, the expected error can be approximated by
k
4n. Let us call
this error Genn,k. Finally, in Section 5.4, we consider the general case of k ≥1
and give an approximation of the expected error in terms of Errn,ϵ and Genn,k.
5.1
Error Function
We use an error function based on the earth mover’s distance(EMD) [24] to
quantify the utility of the published data. The EMD between two pointsets of
equal size is deﬁned to be the minimum cost of bipartite matching between the

168
C. Fang and E.-C. Chang
two sets, where the cost of an edge linking two points is the cost of moving
one point to the other. Hence, EMD can be viewed as the minimum cost of
transforming one pointset to the other. Diﬀerent variants of EMD diﬀer on how
the cost is deﬁned. In this paper, we adopt the typical deﬁnition that deﬁnes the
cost as the Euclidean distant between the two points.
In one-dimensional space, the EMD between two sets D and D is simply
the L1 norm of the diﬀerences between the two respective sorted sequences, i.e.
∥Sn(D)−Sn( D)∥1, which can be eﬃciently computed. Recall that Sn(D) outputs
the sorted sequence of elements in D. In other words,
EMD(D, D) =
n

i=1
|pi −pi|,
(2)
where pi’s and pi’s are the sorted sequence of D and D respectively. Note that
this deﬁnition assumes D and D have the same number of points, which is
ensured by step B1 of our scheme.
Given a dataset D and the published dataset D of a mechanism M where
|D| = | D| = n, let us deﬁne the normalized error as 1
nEMD(D, D) and denote
ErrM,D the expected normalized error,
ErrM,D = Exp
 1
n EMD(D, D)

,
(3)
where the expectation is taken over the randomness in the mechanism.
Our mechanism publishes D based on two parameters: the privacy require-
ment ϵ and the group size k. Therefore, let us write Errϵ,k,D for the expected
normalized error of the dataset published in Step B2.
5.2
Eﬀects on Isotonic Regression
Let us consider the expected normalized error when k = 1, in other words, we
ﬁrst consider the mechanism without grouping. In such case, the reconstructed
dataset is IR(Sn(D) + Lap(ϵ−1)n). Thus, the expected normalized error is
Errϵ,1,D = Exp
 1
n EMD(D, IR(Sn(D) + Lap(ϵ−1))n)

.
To estimate the above expected error, we compute the expected normalized
error on a few datasets of varying size n: (1) Multi-sets containing elements
with the same value 0.5 (herein called repeating single-value dataset), (2) sets
containing equally-spaced numbers (i/(n −1)) for i = 0, . . . , n −1 (herein call
equally-spaced dataset), (3) sets containing n randomly chosen elements from
the Twitter location data [1], and (4) sets containing n randomly chosen elements
from the Kaluˇza’s data [12].
Fig. 4(a) shows the expected error Err1,1,D for the four datasets with diﬀerent
n. Each sample in the graph is the average over 500 runs. Observe that the error

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
169
on equally-spaced data well approximates the errors on the two real-life dataset
(Twitter location dataset and Kaluˇza’s dataset). Hence, we take the error on
the equally-spaced dataset as an approximation of the errors on other datasets.
For abbreviation, let Errϵ,n denote the expected error Errϵ,1,D where D is the
equally-spaced dataset with n points. Based on experiences on other datasets,
we suspect that the expected error depends on the diﬀerence of the minimum
and the maximum element in D, and the repeating single-value dataset is the
extreme case whose error could be served as a lower bound as shown in Fig. 4(a).
Fig. 3(a) shows the expected error Errϵ,1,D for dataset on equally-spaced
points for diﬀerent ϵ and n, and Fig. 3(b) shows the ratios of error for diﬀerent
ϵ to Err1,n. The results agree with the intuition that when ϵ is increased by a
factor of c, the error would approximately decrease by factor of c, that is,
Errϵ,1,D ≈1
cErrcϵ,1,D.
(4)
0
2000
4000
6000
8000
10000
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Number of points
Error
ε = 2
ε = 1
ε = 1/2
ε = 1/3
(a) The normalized error Errϵ,n.
0
2000
4000
6000
8000
10000
0
0.5
1
1.5
2
2.5
3
Number of points
Error
Ratio of ε = 2
Ratio of ε = 1/2
Ratio of ε = 1/3
(b) The ratio of Errϵ,n to Err1,n.
Fig. 3. The normalized error for diﬀerent security parameter ϵ on equally-spaced
dataset, each sample is the average of 500 runs
5.3
Eﬀect on Generalization Noise
When k > 1, the grouping introduces a generalization error, which is incurred
when all elements in a group are represented by their mean. Before giving formal
description of generalization error, let us introduce some notations.
Given a sequence D = ⟨x1, . . . , xn⟩of n numbers, and a parameter k, where
k divides n, let us call the following function downsampling:
↓k (D) = ⟨s1, . . . , s(n/k)⟩,
where each si is the average of xk(i−1)+1, . . . , xik. Given a sequence D′ = ⟨s′
1, . . . ,
s′
m⟩and k, let us call the following function upsampling,
↑k (D′) = ⟨x′
1, . . . , x′
mk⟩,

170
C. Fang and E.-C. Chang
0
2000
4000
6000
8000
10000
0
0.05
0.1
0.15
0.2
Number of points
Error
Repeated single−value data
Equally spaced data
kaluza’s data
Twitter location data
(a) Expected normalized error Err1,1,D.
0
100
200
300
0
0.005
0.01
0.015
Groupsize
Error
k/(2n)
Equally spaced data
kaluza’s data
Twitter location data
(b) Normalized generalization error GenD,k.
Fig. 4. The expected normalized error and normalized generalization error with ϵ = 1
on diﬀerent dataset D
where x′
i = s′
⌊(i−1)/k⌋+1 for each i.
The normalized generalization error is deﬁned as,
GenD,k = 1
n∥D−↑k (↓k (D))∥1.
It is easy to see that, for any k and D of size n, the normalized generalization
error is at most k/(2n). However, this bound is often an overestimate. Fig.
4(b) shows the generalization error of diﬀerent group size a dataset containing
10, 000 equally-spaced values, a dataset containing 10, 000 numbers randomly
drawn from the transformed Kaluˇza’s dataset, and a dataset of 10, 000 numbers
randomly drawn from the transformed Twitter location data.
Observe that, empirically, the generalization error can be well approximated
by
k
4n. To see that such approximation holds for a typical dataset, consider
the following partition of the unit interval: 0 = p0 < p1 < p2, . . . , p(n/k)−1 <
pn/k = 1. Let us consider a sorted sequence S of elements in dataset D, where
the jk + 1, jk + 2, . . . (j + 1)k-th elements in S are uniformly independent and
identically distributed over [pj, pj+1) for j = 0, 1, . . . , (n/k) −1. We can verify
that the expected generalization error GenD,k ≈
k
4n with simulations. Hence, we
approximate the generalization error by
k
4n and denote it as Genn,k.
5.4
Determining the Group Size k
Now, let us combine the components and build an error model of how k aﬀects
the accuracy. First, grouping reduces the number of constraints by a factor of k.
As suggested by Fig. 4(a), when the number of constraints decreases, the error
reduction from isotonic regression decreases. On the other hand, recall that the
regression is performed on the published values divided by k (see the role of k
in Step A3). This essentially reduces the level of Laplace noise by a factor of
k. Hence, the accuracy attained by grouping k elements is “equivalent” to the

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
171
accuracy attained without grouping but with the privacy parameter ϵ increased
by a factor of k. These two components can be estimated in terms of Errϵ,n as
follow:
Errϵ,k,D ≈1
k Errϵ,n/k.
For general k, the reconstructed dataset is
D =↑k (IR(S)),
where S is an instance of ↓k (Sn(D)) + Lap(1)n/k. Now, we have,
EMD (D, D) = ∥Sn(D)−↑k (IR(S))∥1
= ∥Sn(D)−↑k (↓k (Sn(D))+ ↑k (↓(Sn(D)))−↑k (IR(S))∥1
≤n · GenD,k + ∥↑k (↓k (Sn(D)))−↑k (IR(S))∥1
= n · GenD,k + k · ∥↓k (Sn(D)) −IR(S)∥1
= n · GenD,k + k · EMD(↓k (Sn(D)), IR(S)).
(5)
Note that the ﬁrst term n · GenD,k is a constant independent of the random
choices made by the mechanism. Also note that the second term is the EMD
between the down-sampled dataset and its reconstructed copy obtained using
group size 1. Thus, by taking expectation over randomness of the mechanism,
we have
Errϵ,k,D ≤GenD,k + 1
k Errϵ,1,↓k(D).
(6)
In other words, the expected normalized error is bounded by the sum of normal-
ized generalization error, and the normalized error incurred by the Laplace noise.
Fig. 5(a) shows the three values versus diﬀerent group size k for equally-spaced
data of size 10,000. The minimum of the expected normalized error suggests the
optimal group size k.
Fig. 5(b) illustrates the expected errors for diﬀerent k on the Twitter location
data with 10,000 points. The red dotted line is Errϵ,k,D whereas the blue solid line
is the sum in the right-hand-side of the inequality (6). Note that the diﬀerences
between the two graphs are small. We have conducted experiments on other
datasets and observed similar small diﬀerences. Hence, we take the sum as an
approximation to the expected normalized error,
Errϵ,k,D ≈Genn,k + 1
k Errϵ,n/k.
(7)
Now, we are ready to ﬁnd the optimal k given ϵ and n. From Fig. 4(a) and Fig.
4(b) and the approximation given in equation (7), we can determine the best
group size k when given the size of the database n and the security requirement
ϵ. From the parameter ϵ, we can obtain the value
1
kErrn/k,e for diﬀerent k.
From the database’s size n, we can determine Genn,k which is
k
4n. Thus, we can
approximate the normalized error Errk,D with equation (7) as illustrated in Fig.
5(a). Using the same approach, the best group size given diﬀerent n and ϵ can
be calculated and is presented in table 1.

172
C. Fang and E.-C. Chang
0
50
100
150
200
250
300
0
0.002
0.004
0.006
0.008
0.01
Group size
Error
Normalized error by Laplace noise
Generalization error
Expected error
(a) The expected error.
0
50
100
150
200
250
300
3
4
5
6
7
8
9
10
11x 10
−3
Group size
Error
Error on Kaluza’s data
Error on Twitter data
Expected error
(b) Comparison with the actual error.
Fig. 5. The expected error derived based on the equally-spaced dataset and the com-
parison with actual error on the Kaluˇza’s dataset with ϵ = 1
Table 1. The best group size k given n and ϵ
ϵ = 0.5 ϵ = 1 ϵ = 2 ϵ = 3
n= 2,000
44
29
20
12
n= 5,000
59
37
27
18
n= 10,000
79
51
36
27
n= 20,000
121
83
61
41
n= 100,000
234
150
98
73
n= 180,000
300
177
110
94
6
Comparisons
In this section, we compare the performance of the proposed mechanism with
three known mechanisms w.r.t. diﬀerent utility functions. We ﬁrst compare
the mechanism that outputs equi-width histograms. Next, we investigate the
wavelet-based mechanism proposed by Xiao et al. [28] and measure accuracy
of range queries. Lastly, we consider the problem of estimating median, and
compare with a mechanism based on smooth sensitivity proposed by Nissim et
al. [20]. We do not conduct experiments to compare with the k-d tree method [29]
because it is designed for high dimensional data and it is not clear how to apply
it to low dimension eﬀectively. For comparison purposes, we empirically choose
the best parameters for the known mechanisms, although this apriori informa-
tion is not available to the publisher. We remark that the parameter k of our
proposed mechanism is chosen from Table 1.
6.1
Equi-width Histogram
We want to compare the performance of our method with the equi-width his-
togram method. Fig. 6(a) shows a diﬀerentially private equi-width histogram. To

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
173
100
200
300
400
500
600
700
800
900
1000
100
200
300
400
500
600
700
800
900
1000
(a) Equi-width method.
100
200
300
400
500
600
700
800
900
1000
100
200
300
400
500
600
700
800
900
1000
(b) Proposed method.
Fig. 6. Visualization of the density functions, where the darker region corresponds to
higher value. The superposing red dots are randomly selected from original data points
for comparison purposes.
visualize the reconstructed points of our method as a histogram, we construct
the bins in the following way: let B be the set of distinct-points in D, and we
construct the Voronoi diagram of B. The cells in the Voronoi diagram are taken
to be the bins of a histogram as depicted in Fig. 6(b).
To facilitate comparison, we treat the histograms as estimations of the un-
derlying probability density function f, and use the statistical distance between
density functions as a measure of utility. The value of f(x) can be estimated by
the ratio of the number of samples, over the width of the bin where x belongs
to, with some normalizing constant factor.
In this section, we qualify the mechanism’s utility by the distance between
the two density functions: one that is derived from the original dataset, and the
other that is derived from the mechanism’s output.
Fig. 6(a) and 6(b) show the estimated density function from the Twitter’s
location dataset, by equi-width histogram mechanism and by our mechanism.
For comparisons, 1% of the original points are plotted on top of the two recon-
structed density functions. Fig. 7(a) and 7(b) show the zoom-in view of the dense
region around New York City. Observe that the density function produced by
our mechanism has “variable-sized” cells and thus is able to adaptively capture
the ﬁne details.
The statistical diﬀerence, measured with ℓ1-norm and ℓ2-norm, between the
two estimated density functions derived from the original and the mechanism’s
output are shown in Table 2. We remark that it is not easy to determine the
optimal bin-width for the equi-width histogram prior to publishing. Fig. 8 shows
that the optimal bin-width diﬀers signiﬁcantly for three diﬀerent datasets. For
comparison purposes, we empirically choose the best parameters to the advan-
tage of the compared algorithms, although such parameters could be dependent
on the dataset.

174
C. Fang and E.-C. Chang
720
740
760
780
800
820
840
400
420
440
460
480
500
520
540
560
(a) Zoom in view of Fig. 6(a).
720
740
760
780
800
820
840
420
440
460
480
500
520
540
560
580
(b) Zoom in view of Fig. 6(b).
Fig. 7. A more detailed view of the density functions
6.2
Range Query
We consider the scenario where a dataset is to be published, and subsequently
used to answer a series of range queries, where each range query asks for the
total number of points within a query range. Publishing an equi-width histogram
would not attain high accuracy if the size of the query ranges varies drastically.
Intuitively, wavelet-based techniques [28] are natural solutions to address such
multi-scales queries. However, there are many parameters, including the bin-
widths at various scales and the amounts of privacy budget they consume, to be
determined prior to publishing.
To apply the proposed method in this scenario, given a query, we obtain
the number of points within the range from the estimated density function (as
described in Section 6.1) by accumulating the probability over the query region
and then multiplying by the total number of points.
We compare the range query results of the wavelet-based mechanism, the equi-
width histogram mechanism and our mechanism on the 1D Twitter data, and on
the 2D Twitter location dataset. To incorporate the knowledge of the database’s
size n, the total number of points is adjusted to n for the histogram mechanism
and the DC component of the wavelet transform is set to be exactly n for the
wavelet mechanism. For each range query, the absolute diﬀerence between the
the true answer and the answer derived from the mechanism’s output is taken
as the error. We compare the results over diﬀerent query range sizes and for
each query range. For each range size s, 1,000 randomly chosen queries of size s
are asked, and the corresponding errors are recorded. More precisely, the center
of a 1D query range of size s is chosen uniformly at random in the continuous
interval [ s
2, 1 −s
2], whereas the center of a 2D query range of size s is chosen
uniformly at random in the region [ s
2, 1 −s
2] × [ s
2, 1 −s
2].
To determine the parameters for the two compared mechanisms, we conduct
experiments on a few selected values and choose the values to the advantage of

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
175
0
0.5
1
1.5
2
Number of bins
ℓ1-statistical distance
Equally spaced data
kaluza’s data
Twitter location data
1002
502
1502
2002
2502 3002
Fig. 8. The statistical diﬀerences versus
bin-widths for diﬀerent dataset D with
ϵ = 1 and n = 60, 000
Table
2.
The
statistical
dif-
ferences
of
the
two
methods
equi-width proposed method
ℓ1-norm
1.23
1.13
ℓ2-norm
0.25
0.20
0
0.2
0.4
0.6
0.8
0
500
1000
1500
Range size
Error
Equi−width
Wavelet
Proposed
(a) 1D range query.
0
0.2
0.4
0.6
0.8
0
500
1000
1500
2000
Range area
Error
Equi−width
Wavelet
Proposed
(b) 2D range query.
Fig. 9. Comparison of range query performance
the compared mechanisms. For the equi-width histogram, the only parameter is
the number of bins (n1). For the wavelet-based mechanism, the parameter we
considered is the number of bins (n2) of the histogram whereby wavelet trans-
formation is performed on, that is, the number of bins in the “ﬁnest” histogram.
From our experiments, we choose n1 = 1000 and n2 = 1024 for the 1D data,
and n1 = 40 × 40 and n2 = 512 × 512 for the 2D data. The parameter k for our
mechanism is looked up from Table 1. The choice of group size k according to
Table 1 is 177 (n = 180, 000, ϵ = 1). The average errors of the range query is
shown in Fig. 9(a) and 9(b).
Observe that our proposed method is less sensitive to the query range in the
1D case as expected because the accuracy of our range query results depend only
on the boundary points, as opposed to the equi-width histogram method where
errors are induced by each bins within the range. The wavelet-based mechanism
outperforms the equi-width histogram mechanism in larger size range queries,
but performs badly for small range due to the accumulation of noise.

176
C. Fang and E.-C. Chang
6.3
Median
Finding the median accurately in a diﬀerentially private manner is challenging
due to the high “global sensitivity”: there are two datasets that diﬀer by one ele-
ment but having a completely diﬀerent median. Nevertheless, for many instances,
their “local sensitivity” are small. Nissim et al. [20] showed that in general, by
adding noise proportional to the “smooth sensitivity” of the database instance,
instead of the global sensitivity, can also ensure diﬀerential privacy. They also
gave an Θ(n2) algorithm that ﬁnd the smooth sensitivity w.r.t. median.
Our mechanism outputs the sorted sequence diﬀerentially privately, and thus
naturally gives the median. Compare to the smooth sensitivity-based mechanism,
our mechanism provides more information in the sense that it outputs the whole
sorted sequence. Furthermore, our mechanism can be eﬃciently carried out in
O(n log n) time.
We conduct experiments on synthetic datasets of size 129 to compare the
accuracy of both mechanisms. The experiments are conducted for diﬀerent local
sensitivity and diﬀerent ϵ values. To construct a dataset with a particular local
sensitivity, 66 random numbers are generated with the exponential distribution
and then scaled to the unit interval. The dataset contains the 66 random numbers
and 63 ones. Fig. 10(a) and 10(b) shows the noise level with diﬀerent ϵ on datasets
that has a local sensitivity of 0.1 and 0.3.
When the local sensitivity of the median is high, our mechanism tends to
provide a better result. In addition, our mechanism performs well under higher
requirement of security: when the ϵ is smaller, the accuracy of our mechanism
decreases slower than the smooth sensitivity-based method.
0
0.5
1
1.5
2
2.5
3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Value of ε
Error
Our method
Smooth sensitivity based method
(a) Local sensitivity of 0.1.
0
0.5
1
1.5
2
2.5
3
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Value of ε
Error
Our method
Smooth sensitivity based method
(b) Local sensitivity of 0.3.
Fig. 10. The error of median versus diﬀerent ϵ from two datasets
7
Discussion and Future Work
7.1
Hybrid Method
The proposed mechanism can be viewed as the publishing of a “equi-depth” his-
togram, where the “depth” is the group size. Potentially, our proposed method

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
177
and equi-width histogram could complement each other, by alternatively publish-
ing one after another. For example, we can ﬁrst apply an equi-width histogram
to get a coarse distribution of the data, followed by our method for a “zoom-
in” view. Alternatively, we can apply equi-width histogram after our method to
“break” the stepping eﬀect of isotonic regression.
7.2
Eﬀect of Dimension
We rely on a locality-preserving mapping T to extend the mechanism to higher
dimension. Although it is shown that the distant between two d dimensional
points x, y is preserved and bounded by c(∥T (x) −T (y)∥)1/d (see Section 2.3),
the curse-of-dimensionality is still in play in higher dimension. Firstly, to our
best knowledge, there is no known eﬃcient constructions for dimensions higher
than 3. Secondly, the exponential factor 1/d ampliﬁes the error: for example, Fig.
5 shows that our scheme can reduce the error of ∥T (x)−T (y)∥to 0.005, where y
is the reconstructed point for x. When d is 2, we can have ∥x −y∥2 bounded by
0.07c; when d is 3, the bound is increased to 0.17c. We are unable to verify the
performance in higher dimension due to the lack of eﬃcient construction, and
leave the accurate extension to higher dimensional data as future work.
8
Related Work
There are extensive works on privacy-preserving data publishing. The recent
survey by Fung et al. [8] gives a comprehensive overview on various notions, for
example, k-anonymity [26], ℓ-diversity [15], and the recently proposed concept
of diﬀerential privacy [5].
Xiao et al. [28] proposed a mechanism of adding Laplace noise to the co-
eﬃcients of a wavelet transformation of an equi-width histogram. The noisy
wavelet coeﬃcients are then published, from which range queries can be an-
swered. Essentially, what being published is a series of equi-width histograms
for diﬀerent bin-widths where the noise added to the histograms of larger bin-
width are smaller. A range query can then be decomposed and answered from
the histograms series diﬀerent scales.
Isotonic regression has been used to improve a diﬀerentially private query re-
sult. Hay et al. [11] proposed a method that employs isotonic regression to boost
accuracy, but in a way diﬀerent from our mechanism. They consider publishing
unattributed histogram, which is the (unordered) multi-set of the frequencies of
a histogram. As the frequencies are unattributed (i.e. order of appearance is ir-
relevant), they proposed publishing the sorted frequencies and later employing
isotonic regression to improve accuracy.
Machanavajjhala et al. [16] proposed a 2D dataset publishing method that
can handle the sparse data in 2D equi-width histogram. To mitigate the sparse
data, their method shrinks the sparse blocks by examining publicly available
data such as a previously release of similar data. They demonstrate this idea
on the commuting patterns of the population of the United States, which is a

178
C. Fang and E.-C. Chang
real-life sparse 2D map in large domain. As their method partitions the space
based on a previously released data, we consider the partition as pre-determined
partition and is not adaptive to the publishing dataset.
The median is an important statistic, and a diﬀerentially private median ﬁnd-
ing process can be useful in many constructions, such as in pointset spatial de-
composition [4,23]. However, ﬁnding the median diﬀerentially privately is not
easy due to the large global sensitivity. Nissim et al. [20] introduced the notion
of smooth sensitivity and proposed an accurate mechanism with Θ(n2) running
time.
9
Conclusion
Our mechanism is very simple from the publisher’s point of view. The publisher
just has to sort the points, group consecutive values, add Laplace noise and
publish the noisy data. There is also minimal tuning to be carried out by the
publisher. The main design decision is the choice of the group size k, which can
be determined using our proposed noise models, and the locality-preserving map-
ping for which the classic Hilbert curve suﬃces to attain high accuracy. Through
empirical studies, we have shown that the published raw data contain rich infor-
mation for the public to harvest, and provide high accuracy even for usages like
median-ﬁnding and range-searching that our mechanism is not initially designed
for.
References
1. Twitter census: Twitter users by location, http://www.infochimps.com/datasets/
twitter-census-twitter-users-by-location
2. Barak, B., Chaudhuri, K., Dwork, C., Kale, S., McSherry, F., Talwar, K.: Privacy,
accuracy, and consistency too: a holistic solution to contingency table release. In:
Symposium on Principles of Database Systems, pp. 273–282 (2007)
3. Blum, A., Dwork, C., McSherry, F., Nissim, K.: Practical privacy: the sulq frame-
work, pp. 128–138 (2005)
4. Cormode, G., Procopiuc, M., Shen, E., Srivastava, D., Yu, T.: Diﬀerentially private
spatial decompositions. To be appeared in ICDE (2012)
5. Dwork, C.: Diﬀerential privacy. Automata, languages and programming, p. 1 (2006)
6. Dwork, C., McSherry, F., Nissim, K., Smith, A.: Calibrating Noise to Sensitivity in
Private Data Analysis. In: Halevi, S., Rabin, T. (eds.) TCC 2006. LNCS, vol. 3876,
pp. 265–284. Springer, Heidelberg (2006)
7. Feldman, D., Fiat, A., Kaplan, H., Nissim, K.: Private coresets, p. 361 (2009)
8. Fung, B., Wang, K., Chen, R., Yu, P.: Privacy-preserving data publishing: A survey
of recent developments. ACM Computing Surveys, 14 (2010)
9. Gotsman, C., Lindenbaum, M.: On the metric properties of discrete space-ﬁlling
curves. IEEE Transactions on Image Processing, 794–797 (1996)
10. Grotzinger, S., Witzgall, C.: Projections onto order simplexes. Applied Mathemat-
ics and Optimization, 247–270 (1984)
11. Hay, M., Rastogi, V., Miklau, G., Suciu, D.: Boosting the accuracy of diﬀerentially
private histograms through consistency. VLDB Endowment, 1021 (2010)

Adaptive Diﬀerentially Private Histogram of Low-Dimensional Data
179
12. Kaluˇza, B., Mirchevska, V., Dovgan, E., Luˇstrek, M., Gams, M.: An Agent-Based
Approach to Care in Independent Living. In: de Ruyter, B., Wichert, R., Keyson,
D.V., Markopoulos, P., Streitz, N., Divitini, M., Georgantas, N., Mana Gomez, A.
(eds.) AmI 2010. LNCS, vol. 6439, pp. 177–186. Springer, Heidelberg (2010)
13. Kifer, D., Machanavajjhala, A.: No free lunch in data privacy. In: Management of
Data, pp. 193–204 (2011)
14. Li, C., Hay, M., Rastogi, V., Miklau, G., McGregor, A.: Optimizing linear counting
queries under diﬀerential privacy, pp. 123–134 (2010)
15. Machanavajjhala, A., Gehrke, J., Kifer, D., Venkitasubramaniam, M.: ℓ-diversity:
Privacy beyond k-anonymity. In: International Conference on Data Engineering,
pp. 24–24 (2006)
16. Machanavajjhala, A., Kifer, D., Abowd, J., Gehrke, J., Vilhuber, L.: Privacy: The-
ory meets practice on the map. In: International Conference on Data Engineering,
pp. 277–286 (2008)
17. Meyer, M.C.: Inference using shape-restricted regression splines. Annals of Applied
Statistics, 1013–1033 (2008)
18. Mitchison, G., Durbin, R.: Optimal numberings of an n x n array. Algebraic Dis-
crete Methods, 571–582 (1986)
19. Niedermeier, R., Reinhardt, K., Sanders, P.: Towards optimal locality in mesh-
indexings, pp. 364–375 (1997)
20. Nissim, K., Raskhodnikova, S., Smith, A.: Smooth sensitivity and sampling in
private data analysis, pp. 75–84 (2007)
21. Piatetsky-Shapiro, G., Connell, C.: Accurate estimation of the number of tuples
satisfying a condition, pp. 256–276 (1984)
22. Poosala, V., Haas, P., Ioannidis, Y., Shekita, E.: Improved histograms for selectivity
estimation of range predicates. In: ACM SIGMOD Record, p. 294 (1996)
23. Qardaji, W., Li, N.: Recursive partitioning and summarization: a practical frame-
work for diﬀerentially private data publishing. To be appeared in ASIACCS (2012)
24. Rubner, Y., Guibas, L., Tomasi, C.: The earth movers distance, multi-dimensional
scaling, and color-based image retrieval, pp. 661–668 (1997)
25. Stout, Q.F.: Optimal algorithms for unimodal regression. Computer Science and
Statistics, 109–122 (2000)
26. Sweeney, L.: k-anonymity: a model for protecting privacy. International Journal of
Uncertainty, Fuzziness and Knowledge-Based System, 557–570 (2002)
27. Wang, X., Li, F.: Isotonic smoothing spline regression. Journal Computational and
Graphical Statistics, 21–37 (2008)
28. Xiao, X., Wang, G., Gehrke, J.: Diﬀerential privacy via wavelet transforms. IEEE
Transactions on Knowledge and Data Engineering, 1200–1214 (2010)
29. Xiao, Y., Xiong, L., Yuan, C.: Diﬀerentially Private Data Release through Multi-
dimensional Partitioning. In: Jonker, W., Petkovi´c, M. (eds.) SDM 2010. LNCS,
vol. 6358, pp. 150–168. Springer, Heidelberg (2010)

PRISM – Privacy-Preserving Search in MapReduce
Erik-Oliver Blass1, Roberto Di Pietro2, Reﬁk Molva3, and Melek ¨Onen3
1 Northeastern University, Boston, MA
2 Universit`a di Roma Tre, Rome, Italy
3 EURECOM, Sophia Antipolis, France
Abstract. We present PRISM, a privacy-preserving scheme for word search in
cloud computing. In the face of a curious cloud provider, the main challenge is
to design a scheme that achieves privacy while preserving the efﬁciency of cloud
computing. Solutions from related research, like encrypted keyword search or
Private Information Retrieval (PIR), fall short of meeting real-world cloud re-
quirements and are impractical. PRISM’s idea is to transform the problem of
word search into a set of parallel instances of PIR on small datasets. Each PIR
instance on a small dataset is efﬁciently solved by a node in the cloud during the
“Map” phase of MapReduce. Outcomes of map computations are then aggregated
during the “Reduce” phase. Due to the linearity of PRISM, the simple aggrega-
tion of map results yields the ﬁnal output of the word search operation. We have
implemented PRISM on Hadoop MapReduce and evaluated its efﬁciency using
real-world DNS logs. PRISM’s overhead over non-private search is only 11%.
Thus, PRISM offers privacy-preserving search that meets cloud computing efﬁ-
ciency requirements. Moreover, PRISM is compatible with standard MapReduce,
not requiring any change to the interface or infrastructure.
1
Introduction
Today, users take advantage of public clouds operated by large companies like Google
or Amazon. Instead of setting up and maintaining their own data centers, users reduce
their costs by outsourcing both storage and processing to a cloud. One prominent exam-
ple allowing cloud-based storage and processing is Hadoop MapReduce [3], a variant
of Google’s MapReduce system [17]. Hadoop MapReduce is widely used, and public
MapReduce clouds are offered by companies such as Amazon [2, 25].
The advantages of cloud computing unfortunately come with a high cost in terms of
new security and privacy exposures. Apart from classical security challenges of shared
services, outsourcing of data storage and processing raises new challenges in the face of
potentially malicious cloud providers. Privacy of outsourced data appears to be a major
requirement in this context. Some regulations are already provisioned as to the privacy
protection of outsourced governmental documents [11, 12, 18]: these regulations usu-
ally aim at assuring privacy against curious clouds or against clouds with data centers
located in “rogue” countries or with insufﬁcient security guarantees; they also are de-
ﬁned to avoid data leakage in case of operational failures in the cloud. Along these lines,
there is also a raising corporate concern about the privacy of sensitive business data
stored in the cloud [14]. Although cloud providers thrive to meet the increased privacy
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 180–200, 2012.
c⃝Springer-Verlag Berlin Heidelberg 2012

PRISM – Privacy-Preserving Search in MapReduce
181
demand by certifying their services [24], malicious insiders have still been identiﬁed as
one of the top threats in cloud computing [15].
While encryption of outsourced data by the users seems to be a viable protection
against most privacy problems, traditional data encryption does not suit the require-
ments of cloud computing: the cloud not only serves as high capacity memory, but is
also involved in data processing such as statistical data analysis, log analysis, index-
ing, data mining, and searching [25]. However, data processing performed by the cloud
would not be feasible or would be inefﬁcient with encrypted data.
Among data processing primitives, word search, i.e., verifying, whether a certain
word is part of a dataset, is not only one of the most fundamental operations, but sur-
prisingly also one of the most demanded applications in, e.g., MapReduce cloud com-
puting [25]. Related work on search in encrypted data, e.g., Boneh et al. [6], Ogata
and Kurosawa [31], falls short of meeting cloud computing privacy and performance
requirements. These techniques are impractical as they are designed for centralized ex-
ecution models that are incompatible with today’s highly parallel cloud architectures.
In this paper, we present PRISM, a new scheme for privacy-preserving and efﬁcient
word search for MapReduce clouds. PRISM pursues two speciﬁc objectives: 1.) pri-
vacy against potentially malicious cloud providers and 2.) high efﬁciency through the
integration of security mechanisms with the operations performed in the cloud.
In order to achieve efﬁciency, PRISM takes advantage of the inherent parallelization
akin to cloud computing: the word search problem on a very large encrypted dataset is
partitioned into several instances of word search in small datasets that are executed in
parallel (“Map” phase). The individual word search operations performed in the cloud
yield a result amenable to straightforward aggregation in the ultimate phase (“Reduce”
phase) of the word search operation. The word search operation builds on a Private
Information Retrieval (PIR) [29] technique which is extended in order to generate in-
termediate search results that are still encrypted and that can be combined through linear
operations to yield the global result of the word search over the entire dataset.
Summarizing our contributions, PRISM:
• is suited to cloud computing: PRISM is the ﬁrst privacy-preservingsearch scheme
suited to cloud computing; it brings together storage and search privacy with high per-
formance by leveraging the efﬁciency of the MapReduce paradigm. PRISM is paral-
lelizable and also allows efﬁcient combination of individual results. Its efﬁciency has
been evaluated through searching in DNS logs provided by an Internet Service Provider.
Although PRISM’s overhead within the core Map function is large compared to non-
privacy-preserving search (factor of 9), the total system overhead is only 11%.
• preserves privacy in the face of potentially malicious cloud providers: PRISM
allows carrying out these critical operations in the cloud without trusting the cloud.
• is compatible to standard MapReduce: PRISM only requires a standard MapRe-
duce interface without modiﬁcations in the underlying system. PRISM can thus be in-
tegrated on any cloud that provides a standard MapReduce interface such as Amazon.
• provides ﬂexible search: In contrast to traditional encrypted keyword search tech-
niques, PRISM is not limited to searching for a ﬁxed set of predetermined keywords to
be known in advance, but offers ﬂexible search for any words.

182
E.-O. Blass et al.
2
Problem Statement and Adversary Model
Throughout this paper, we will use an application example to motivate our work. In-
spired by recent events [30], we envision a data retention scenario. Due to regulatory
matters, a small, residential Internet Service Provider (ISP) must retain logs of client
accesses. Due to the sheer amount of data to retain, the ISP outsources logﬁles to the
cloud. Files are encrypted as they contain sensitive data. Still, e.g., law enforcement
authorities will contact the ISP to search for words (strings, text, ...) in outsourced ﬁles.
More concretely, assume service provider U (the cloud “user”) providing DNS ser-
vices to clients. U logs each client’s access, i.e., U logs the tuple (timestamp, client ID,
hostname queried). Due to the large amount of log data and cost reasons, U outsources
its logﬁles into a cloud. Regularly, say each day i, U creates a new logﬁle Li. At the end
of a longer period, U wants to (or is forced to) ﬁnd out, whether there was an interest
in a suspicious host w. So, U checks, at which day, i.e., in which logﬁles Li, word w
occurs. U queries the cloud for w, and the cloud responses with an answer R telling U
which of the Li contain(s) w.
Note that U does not know in advance which word w it has to search for. This auto-
matically disqualiﬁes protocols for predeﬁned keyword search, such as PEKS [6] and
derivatives. Also, data retention regulations require outsourced data to be fully recover-
able; storing only digests of data in the cloud, e.g., hash values, is insufﬁcient.
The cloud is assumed to be untrusted, more precisely semi-honest (“honest-but-
curious”). Regulatory matters imply that the cloud must not learn any information about
the content it hosts and search queries performed. This implies both, the encryption of
data by U before outsourcing it to the cloud and “obliviously” processing queries on
encrypted data by the cloud.
Before we formalize our privacy requirements, we ﬁrst deﬁne the main components
for a cloud word search scheme.
Deﬁnition 1 (Cloud Word Search). Let L denote a sequence of ﬁles L := {L1, . . . ,
Lm} and Σ the set of possible words. Each ﬁle Li consists of a sequence of words
Li := {w(i,1), w(i,2), . . . , w(i,|Li|)}, w(i,j) ∈Σ.
A cloud word search scheme comprises the following algorithms:
1. KeyGen(s): using a security parameter s, this algorithm outputs a secret S.
2. Encrypt(S, L): uses the secret S to encrypt the content of ﬁles Li ∈L and outputs
the set of resulting encryptions E := {EL1, . . . , ELm}. Here, ELi denotes the
encryption of ﬁle Li.
3. Upload(E): uploads E to the cloud.
4. PrepareQuery(S, w): takes S, the word w to search for, and produces a query Q.
5. Process(E, Q): with encryptions E and query Q, this algorithm produces result R.
6. Decode(S, R, w): taking result R, secret S, and word w, this algorithm outputs
the set of indices I := {i1, . . . , ir} such that ∀i ∈I : Li ∈L ∧w ∈Li, if
R = Process(E, Q) with Q = PrepareQuery(S, w), and E = Encrypt(S, L).
The basic interaction between user U and the cloud can be summarized as follows:
ﬁrst, user U encrypts and uploads ﬁles. Then, U prepares a search query Q for word
w and sends Q to the cloud. The cloud processes this query using algorithm Process

PRISM – Privacy-Preserving Search in MapReduce
183
and produces output R. This output is sent back to U. Using output R and another
algorithm Decode, U can compute the list of ﬁles containing w. While describing
PRISM’s details later in Section 4, we will show how they map to these algorithms.
Note that, in
a
cloud setting, U
executes KeyGen, Encrypt,
Upload,
PrepareQuery, and Decode, while the cloud executes Process. The idea is that algo-
rithms KeyGen, Encrypt, Upload, PrepareQuery, and Decode are computationally
very “lightweight” for U compared to Process. The main computational burden lies on
the cloud side.
Privacy Requirements
Intuitively, our application demands for two main types of privacy. The cloud (now
called “adversary A”) must neither be able to infer any details about stored ﬁles nor
learn details about U’s queries and results delivered back to U. This implies not only
the secrecy or conﬁdentiality of the content, but also the inability to compute statistics
on the content. Informally, in our setting:
– given E, A must not learn the content of L and must not discover whether ﬁles Li
contain a word w, e.g., multiple times;
– given a set of queries {Qi}, A must not learn the words {wi} U is looking for and
must not discover whether the same word is queried multiple times;
– given the result Ri of a query Qi, U must not learn which ﬁle(s) contain the word
corresponding to this speciﬁc query Wi.
Instead, the adversary should only learn “trivial” properties, such as the total number of
ﬁles, the ﬁle size, and the total number of queries. Along the same lines as traditional
indistinguishability [23], A should not be able to infer any additional information from
encrypted ﬁles, queries, and results. We formally deﬁne privacy for a cloud word search
scheme using a game between adversary A (the cloud) and a challenger (user U).
Deﬁnition 2 (Privacy). Let W denote a sequence of words W := {w1, . . . , wn}. The
game GAME is played as follows.
1. The challenger executes KeyGen(s) to derive secret S.
2. A selects a distinct pair of sequences of ﬁles and words (L0, W0) and (L1, W1),
where |L0| = |L1|, ∀(L0
i ∈L0, L1
i ∈L1) : |L0
i | = |L1
i |, and |W0| = |W1|.
A sends (L0, W0) and (L1, W1) to the challenger.
3. The challenger randomly selects b ∈{0, 1} and
– executes Encrypt(S, Lb), i.e., the challenger computes encrypted ﬁles Eb =
{E(S, Li)|Li ∈Lb}.
– executes Upload(Eb) to send encrypted ﬁles back to A.
– executes PrepareQuery(S, w) for each w in Wb. This results in the sequence
of queries Qb := {Q1, . . . , Q|Wb|} that the challenger also sends to A.
4. A outputs b′ ∈{0, 1}. The outcome of GAME is “1” iff b′ = b.

184
E.-O. Blass et al.
A cloud word search scheme is called privacy-preserving iff
Pr(GAME(A) = 1) ≤1
2 + ϵ(s)
for all probabilistic polynomial-time adversaries A. Here, ϵ(s) is a negligible function,
ϵ(s) <
1
P (s) for every polynomial P with sufﬁciently large security parameter s.
The speciﬁcation of the (Li, Wi) in step 2. of Deﬁnition 2 reﬂects the fact that A can
learn the total number of ﬁles, the size of each ﬁle, and the number of queries.
Limitations: We consider semi-honest clouds. Fully malicious clouds might perform
DoS-attacks or deviate from protocol execution. Similar to “reaction attacks” [26], the
cloud might return garbage to U, to observe U’s reaction (e.g., sending the same query).
Although realistic, we leave such attacks for future work. Also, our privacy deﬁnition
does not capture trivial privacy properties, e.g., the size of outsourced ﬁles. Mitigation
strategies (e.g., padding ﬁles) might be contradictory to cloud efﬁciency. We conjecture
that, for many applications, losing “trivial” privacy properties is acceptable.
3
Background
3.1
MapReduce
We target a system suited for the MapReduce [3] paradigm. We will now give a con-
densed overview of MapReduce, focusing on aspects necessary to understand PRISM.
Upload. A MapReduce cloud comprises a set of “slave” node computers and a “master”
computer. While U uploads ﬁles into the MapReduce cloud, each ﬁle is automatically
split into blocks called InputSplits. InputSplits have a ﬁxed size SInputSplit which is
a pre-conﬁgured system parameter. If SFile denotes the size of an uploaded ﬁle, the
number of InputSplits c computes to c =
SFile
SInputSplit . For each InputSplit, a workload
sharing algorithm selects a slave node and places the InputSplit on it.
In addition to data, the MapReduce also allows U to upload “operations”, i.e., com-
piled Java classes. These classes represent the implementation of three functions.
1.) Scan(INPUTSPLIT) →[(k, v)]), a functions that takes an InputSplit as an input,
parses it, i.e., scans it and generates a set of key-value pairs [(k, v)] out of it.
2.) Map(k, v) →[(k′, v′)], a function that takes as an input a single key-value pair
(k, v) and outputs a set of “intermediate” key-value pairs [(k′, v′)].
3.) Reduce([(k′, v′)]) →FILE, a function that takes as an input a set of intermediate
key-value pairs [(k′, v′)] and writes arbitrary output into a ﬁle.
Uploaded Java classes are sent to all slave nodes storing an InputSplit.
Map Phase. After data and implementations have been uploaded, U speciﬁes one up-
loaded ﬁle and triggers MapReduce operations on that ﬁle. The ﬁrst phase of operation
is the “Map” phase. Each slave node becomes a “mapper” node. Each mapper executes
U’s Scan function on the InputSplit it stores locally. This generates a set of key-value
pairs on each mapper. Furthermore, the mapper node executes U’s Map function on
this generated key-value pairs to produce a set of intermediate key-value pairs.

PRISM – Privacy-Preserving Search in MapReduce
185
Reduce Phase. MapReduce starts the “Reduce” phase. Slave nodes are scheduled to
become “reducers”. For each of the intermediate pairs (k′, v′), MapReduce selects a
reducer and sends (k′, v′) to this reducer. MapReduce selects the same reducer for all
pairs (k′, v′) having the same key. Each reducer executes U’s reduce function on its set
of intermediate key-value pairs and writes the output to a ﬁle. This ﬁle is sent to U.
3.2
Trapdoor Group Private Information Retrieval
PIR allows a user to retrieve data from a server without revealing which data is re-
trieved. For PRISM, we make use of a simple and efﬁcient PIR mechanism as previ-
ously suggested by Trostle and Parrish [37]. As this mechanism is just a building block
for PRISM, we will only give a summary of its mode of operation and rationale.
Overview: Matrix M is a t×t matrix of elements in ZN stored at a server. For example,
N = 2 for a binary matrix. User U is only interested in receiving elements of the kth
row in M, but the server must not learn k. The idea is now that U sends two “types”
of values to the server. For each row that U is not interested in, he sends a value of
the “ﬁrst” type. For the one row that U is interested in, he sends a single value of
the “second” type. To prevent the server from distinguishing between the two types
of values, U blinds each value with a blinding factor b. This blinding factor can later
be removed by U. The server now performs simple additions with received values and
elements stored in M. The result is sent back to U who removes the blinding factor and
determines the values of the row of his interest.
Preparation: Assume U is interested in row k. U chooses a group Zp, with a prime p
of m bits. U also chooses a random b ∈Zp and t random values ai ∈Zp. Therewith, U
computes t values ei <
p
t·(N−1) such that: ek := 1 + ak · N and ∀i ̸= k : ei := ai · N.
Finally, U computes αi := b · ei mod p and sends the αi to the server. Other values
(p, m, b, {ei}, {ai}) remain secret. The server treats αi as large integers and performs
the following integer operations, i.e., without any modulo.
Server computation: Let u be the vector u := (α1, . . . , αt). The server computes the
matrix product v and sends it back to U,
v := (β1, . . . , βt) = u · M = (
t

i:=1
αi · Mi,1, . . . ,
t

i:=1
αi · Mi,t).
Result analysis: Upon receipt, in order to “un-blind” values, U computes the t inverse
values zi := βi · b−1 mod p. Now, U can conclude that zi mod N equals the ith
element of the kth row in M. Therewith, U has retrieved the t elements of the kth row
of M in a privacy-preserving fashion. Note the linearity for two βi and β′
i received
during different PIR runs: βi · b−1 + β′
i · b−1 = (βi + β′
i) · b−1 mod p. That is, the
sum of two individually un-blinded vectors equals un-blinding the sum of two received
vectors v, v′. We will later use this linearity during PRISM’s reduce phase.
Security Rationale: Security and privacy of this protocol are based on the trapdoor
group assumption. With only knowledge of αi, but not secret trapdoor p, it is com-
putationally hard for the server to infer any information about low order bits, i.e., the
modulo of z or ei, cf., Trostle and Parrish [37].

186
E.-O. Blass et al.
Discussion: Again, we stress that this particular PIR scheme is an exchangeable build-
ing block. In general, any of the “traditional” PIR techniques based on group homomor-
phic encryption [29, 33] is suited for use within PRISM. We have chosen Trostle and
Parrish [37] only due to the straightforward way to implement it (Section 6). Other PIR
schemes might reduce the (already small) overhead, but this is out of scope here.
4
PRISM Protocol
PRISM comprises three parts: upload of data into the cloud, the MapReduce search,
and the result analysis where the user decides whether the word has been found. We
will brieﬂy give an overview about each part.
1.) Upload. During upload, U encrypts each word (of a logﬁle) using symmetric encryp-
tion. Ciphertexts are stored in a ﬁle, and this ﬁle is sent to the MapReduce cloud. The
cloud automatically splits large ﬁles and distributes splits (InputSplits) among mapper
nodes. We use a standard blockcipher (AES) to perform ciphering of words. However,
to ensure privacy as of Deﬁnition 2, plaintext is modiﬁed before encryption using a
“stateful cipher” construction. Therewith, U can still search for some word w, but the
cloud cannot compute statistics about ciphertexts.
2.) Search. Eventually, U wants to search his encrypted ﬁles for some word w. There-
fore, U sends implementations of “algorithms” for the map and reduce phases to the
MapReduce cloud, and the cloud executes these on uploaded data. For example, U sends
Java “.class” ﬁles for the mappers and Java “.class” ﬁles for reducer nodes. MapReduce
distributes these implementations to each mapper and reducer, respectively. PRISM’s
rationale is to transform the word search problem into a set of small PIR instances.
To do so, each mapper, scanning through its locally stored InputSplit, creates a binary
matrix. Ciphertexts in the InputSplit are assigned to individual elements in that matrix.
If a ciphertext is present in an InputSplit, its corresponding element in the matrix is
set to either “0” or “1”. Using private information retrieval techniques, PRISM can ex-
tract the value of a single element in the matrix with the mapper being totally oblivious
to which element is extracted. Consequently, U can specify which element to extract
in a privacy-preserving way. All mappers send their obliviously extracted elements as
key-value pairs to reducers. Reducers simply sum up received values and return sums
to U. Therewith, neither mappers nor reducers can learn any information about which
ciphertext U was interested in.
3.) Result analysis. Finally, U receives an encrypted sum for each of the originally up-
loaded ﬁles from reducers. U can decrypt them and decide which of the ﬁles contain w.
However, due to the probability of “collisions” in matrices, i.e., two different cipher-
texts can be assigned to the same element, and due to ambiguities of received sums, U’s
decision whether w is inside some ﬁle might be wrong. Therefore, PRISM repeats the
above process in a total of q so called “rounds”. In each of the rounds, a new matrix is
generated, elements are set to “1” or “0” depending on the round number, and results
are returned as described. This reduces the probability of U making incorrect decisions.
Initialization: Before the actual uploading, initially, and only once, U has to execute
KeyGen. In PRISM, KeyGen outputs secret S := {K, N, p}, where |K|, |N|, |p| are

PRISM – Privacy-Preserving Search in MapReduce
187
Input: words wi
Output: ciphertexts Ci uploaded to cloud
1 Initialize all γ to 0;
2 foreach word wi do
3
γwi := get (wi);
//from hash table
4
γwi := γwi + 1;
5
insert (wi, γwi);
//into hash table
6
Ci := EKd(wi, γwi);
7
upload Ci ;
8 end
Algorithm 1. “Stateful Cipher” example and upload to MapReduce
speciﬁed by security parameter s. K is a symmetric key, and N and p are Trapdoor
Group PIR parameters as presented in Section 3.2.
4.1
Upload
Overview. In our scenario, cloud user U continuously logs customer access and sends
logﬁles to the cloud. Each day, U starts using a new logﬁle. For simplicity, we assume
that entries logged by U are simple words. Each logﬁle is encrypted word by word using
a “stateful cipher” EK, and resulting ciphertexts are written to a ﬁle, respectively. The
encrypted ﬁles are sent to the cloud.
Deﬁnition 3 (Stateful Cipher). Given standard symmetric encryption EK with key K,
e.g., AES, we extend E to a stateful cipher by adding “counters” γwi that count the
history of inputs wi. Each time E encrypts wi, counter γwi is increased by one.
In conclusion, a stateful cipher is a cipher that knows how often it has encrypted a
speciﬁc plaintext. The following presents one trivial stateful cipher construction used
in PRISM to encrypt before uploading.
Stateful Cipher Example (see Algorithm 1): For simplicity, user U uses a secret key
K to derive a different key for each day d, e.g., Kd := HMACK(d).
For each day, U maintains a hash table containing the list of counters γwi in U’s local
storage. At the beginning of each day, U initializes all counters to 0, i.e., γwi = 0. Now,
for each logentry wi that should be stored in the cloud, U computes γwi and increases
γwi by 1. Then, U computes ciphertext Ci := EKd(wi, γwi). User U sends ciphertext
Ci to the cloud that stores it in this day’s ﬁle. For the (AES) encryption EKd(wi, γwi),
“,” denotes an unambiguouspairing of inputs. We discuss the reason for using a “stateful
cipher” over using, e.g., a CBC mode of encryption in Section 5.1.
Summarizing, with respect to Deﬁnition 1, Encrypt in PRISM takes K to derive
a separate key Kd for each ﬁle to be encrypted. Actual encryption of each ﬁle is per-
formed word by word using the stateful cipher and key Kd, so E := {EL1, . . . , ELm}
where ELi := {EKi(w1, γw1), . . . , EKi(w|Li|, γw|Li|)}. Upload in PRISM can be re-
garded as simply sending the encrypted ﬁles E to MapReduce.
4.2
Search
User U wants to search a set of ﬁles for word w within a period of time. For ease of
understanding, we will restrict our description below to PRISM working on a single ﬁle

188
E.-O. Blass et al.
speciﬁed by the user, i.e., the ﬁle of day d. With multiple ﬁles, all ﬁles will be separately
(but in parallel) processed with PRISM exactly like with a single ﬁle.
U sends map and reduce implementations of PRISM to MapReduce, and the map
phase starts. In the following, we describe the PRISM algorithms for, ﬁrst, the mappers
and in Section 14 the reducers. We would like to stress that the PRISM algorithms,
e.g., Java “.class” ﬁles, are not encrypted and not specially protected against a curious
cloud. Even though mappers and reducers know what operations they perform, they
cannot deduce any private information about stored data or details about the search.
Overview. Before scanning through its local InputSplit, a mapper node creates a matrix
with all elements initialized to “0”. PRISM’s main idea is that while the mapper scans
the ciphertexts in its InputSplit, each ciphertext is assigned to one position, a certain
element in the matrix by computing a hash of the ciphertext. Additionally, for each
ciphertext, the mapper computes a single bit hash, and if the hash output bit is “1”, the
mapper puts a “1” in the matrix at the assigned position. The idea is that user U can
also compute the position in the matrix and the one bit hash output for a word w he is
looking for. Roughly speaking, U now queries the mapper for the value of that bit in the
matrix using private information retrieval. If the bit retrieved from the mapper differs
from the bit computed by U, then U can decide, e.g., that w is not in this InputSplit.
Problem is that due to the limited size of the matrix and the properties of the hash
function, there might be collisions in the assignment process. That is, by chance there
can be two different ciphertexts being assigned with the same position in the matrix.
By chance, the bit retrieved by U can therefore be unrelated to w. This problem is
ampliﬁed by the fact that U does not only receive a single bit for a single InputSplit,
but a combination (the sum) of all bits from all mappers working on InputSplits. To
mitigate this problem, PRISM repeats generation and ﬁlling of matrices a total of q
rounds. Also, setting an element in a matrix to “1” depends on the round number. After
q rounds, the probability that the information U retrieved from this mapper is unrelated
to w therefore decreases, and U can ﬁnally decide whether w is inside this ﬁle.
Deﬁnition 4 (PIR Matrix). A binary t×t matrix M with t = 2i, i ∈N is called a PIR
matrix. The mapper uses M to implicitly perform the privacy-preserving word search.
Deﬁnition 5 (Candidate Position). For each ciphertext Ci in an InputSplit, the can-
didate position (Xi, Yi) of Ci in M is computed by (Xi||Yi) := ⌊Ci⌋2·log2 (t). Here,
⌊. . .⌋2·log2 (t) denotes truncation after 2 · log2 (t) bits. So, the ﬁrst log2 t bits of Ci de-
termine Xi, and the second log2 t bits determine Yi.
Deﬁnition 6 (PIR Input). If U is interested in a speciﬁc element (X, Y) in M, he
computes PIR input {α1, α2, . . . , αt}, where αX := b · (1 + aX · N) mod p, and
∀i ̸= X, αi := b · (ai · N) mod p. Random values b and ai are chosen as for the
Trapdoor Group PIR scheme presented in Section 3.2.
Deﬁnition 7 (Column Sum). The column sum σi of the ith column of PIR matrix M
is deﬁned as
σi :=

M1≤j≤t,i=1
αj,
where M1≤j≤t,i = 1 denotes the entries in the ith column of M that are set to 1.

PRISM – Privacy-Preserving Search in MapReduce
189
Note that additions in this deﬁnition are integer additions.
The above computation of column sums is simply a digest of the PIR technique
by Trostle and Parrish [37]. In short, if a mapper computes such a column sum on a
given PIR matrix M and given PIR inputs αi, it is impossible for the mapper to derive
(X, Y). U, however, can compute whether MX ,Y = 1, because MX ,Y = 1 iff (σY ·b−1
mod p) mod 2 = 1 holds.
It is important to point out that not only a mapper can compute a candidate posi-
tion for some ciphertext in its InputSplit, but also U can compute candidate positions.
More precisely, as U is looking for w, he can compute E(w, 1) and candidate position
(X||Y) := ⌊E(w, 1)⌋2·log2 (t). If w has been uploaded into a particular InputSplit at least
once, then this InputSplit contains at least E(w, 1) (maybe also E(w, 2), E(w, 3), . . . ).
Therefore, it is sufﬁcient for U to search for E(w, 1). We will now give detailed de-
scriptions of PRISM’s Map and Reduce algorithms.
Query preparation – User. To start, U chooses parameters t, q ∈N, where t de-
termines the size of the PIR matrix and q the number of rounds. For day d that U
wants to search for w, he determines key Kd := HMACK(d) and the target candidate
position ( ˆ
X|| ˆY) := ⌊EKd(w, 1)⌋2· log2 (2). To prepare PIR, U computes t PIR Inputs
{α1, α2, . . . , αt} as described above. U sends all α as part of the following map algo-
rithm implementation to the cloud.
The above preparation of PIR Input depending on w represents PrepareQuery of
Deﬁnition 1 in PRISM. The algorithm’s output Q is the PIR Input. PRISM’s imple-
mentation of Process, i.e., the cloud’s operation on the encrypted ﬁle using a query Q
comprises the following cloud-side Map as well as the whole cloud-side reduce below.
Map Details – Cloud. On the cloud side, all mappers process PRISM in parallel, each
of them on its own, locally stored InputSplit of the current ﬁle. More precisely, a mapper
executes Algorithm 2. Initially, the mapper generates q PIR matrices Ml, where each
element is initially set to 0. We will now write Ml,X ,Y to denote an element (X, Y) in
matrix Ml.
The mapper node scans its local InputSplit consisting of ciphertexts {C1, . . . , Cn}.
For each ciphertext Ci, the mapper creates a key-value pair (i, Ci). Then, the mapper
ﬁlls matrices Ml, 1 ≤l ≤q. For pair (i, Ci),
– the mapper computes candidate position (Xi||Yi) := ⌊Ci⌋2·log2 (t).
– the mapper puts in PIR matrix Mj, in element Mj,Xi,Yi, a “1”, if the bit bitj :=
⌊h(Ci, j)⌋1 = 1. Here, h denotes a cryptographic hash function and “,” again an
unambiguous pairing of inputs. If bitj = 0, element Mj,Xi,Yi remains untouched.
This means that entries in Mj can ﬂip from 0 to 1, but never from 1 back to 0.
After all q PIR matrices are ﬁlled, the mapper computes for each matrix the t column
sums σ1≤j≤t,1≤l≤q based on U’s input {α1, . . . , αt}: values αk with corresponding
element Ml,k,j set to “1” are simply added. Finally, the mapper outputs intermediate
key-value pairs (k, v). The key comprises the name of the ﬁle of the InputSplit this
mapper was working on, e.g., the ﬁle name could be day d, and the number of the col-
umn sum of Ml. The value consists of a list of the q column sums. These intermediate
key-value pairs will now be input for the reducers during the Reduce phase.

190
E.-O. Blass et al.
Input: pairs (i, Ci), values {α1, . . . , αt}
Output: intermediate key-value pairs (k, v)
1 for l := 1 to q do
2
INITIALIZE Ml;
3 end
4 SCANTHROUGHINPUTSPLIT;
5 foreach pair (i, Ci) do
//Fill
matrices
6
(Xi||Yi) := ⌊Ci⌋2·log2 (t);
7
for j := 1 to q do
8
bitj := ⌊h(Ci, j)⌋1;
9
if bitj = 1 then
10
Mj,Xi,Yi := 1;
11
end
12
end
13 end
14 for l := 1 to q do
//q rounds
15
for j := 1 to t do
//Compute
column sums
16
σj,l := 
Ml,1≤k≤t,j=1 αk;
17
end
18 end
19 for j := 1 to t do
//Intermediate
(k,v) pairs
20
(k, v) :=({FILE, j},{σj,1, . . . , σj,q});
21
OUTPUT (k, v);
22 end
Algorithm 2. Computation of matrices M
Input: reducers’ ﬁles FILE
Output: decision whether w ∈FILE
1 foreach ﬁle FILE do
2
for i := 1 to q do
3
if ⌊h(C, i)⌋1 = 1 then
4
U reads sFILE,Y,i;
5
si := (sFILE,Y,i · b−1
mod p) mod N
6
//si = bitj, see
Alg. 2
7
if si = 0 then
8
OUTPUT w ̸∈FILE;
//Contradiction
9
break;
10
end
11
end
12
end
13
OUTPUT w ∈FILE;
14 end
Algorithm 3. U decides w ∈FILE
Reduce Phase – Overview. Recall that there are c InputSplits and therefore c mappers.
A single reducer receives from all the c mappers working on the same ﬁle all their q
column sums for the same column. The reducer simply adds these received sums and
writes the result into a ﬁle which is sent back to U.
Reduce Phase – Details. For all key-value pairs [({FILE, i},{σi,1, . . . , σi,q})] using
the same {FILE, i} as key, the MapReduce framework designates the same reducer. This
reducer receives from all c different mappers working on the same ﬁle all intermediate
key-value pairs with the same key. That is, a reducer receives c pairs which we rewrite
as ({FILE, i}, {σi,1,1, . . . , σi,q,1}), . . . , ({FILE, i}, {σi,1,c, . . . , σi,q,c}).
Here, for a given σi,j,k, i, 1 ≤i ≤t, denotes the column, j, 1 ≤j ≤q, denotes the
round, and k, 1 ≤k ≤c, the InputSplit.
Using integer addition, reducer computes q “ﬁnal PIR sums” sFILE,i,j := c
k=1
σi,j,k, 1 ≤j ≤q, and stores values {sFILE,i,1, . . . , sFILE,i,q} into an output ﬁle R.
To summarize, sFILE,i,j represents the sum of column sums of all the mappers of one
particular column i in PIR matrix j. This concludes the cloud’s Process algorithm in
PRISM. The output ﬁle R is downloaded by U.
4.3
Result Analysis
The only piece left is the Decode algorithm of Deﬁnition 1 which we will describe in
the following. For each outsourced ﬁle (day d), user U retrieves an output ﬁle generated

PRISM – Privacy-Preserving Search in MapReduce
191
by reducers. Now, U analyzes retrieved ﬁles’ content to ﬁnally conclude which of the
outsourced ﬁles contain w (using S). Again for ease of understanding, we restrict our
description to the analysis of the result generated from PRISM on a single outsourced
ﬁle called FILE. U repeats this process with all other results from the other ﬁles accord-
ingly.
Deﬁnition 8 (Collision). Assume U is looking for w, so C := EKd(w, 1). Similar
to hash functions, a collision in PIR matrix M denotes the case of an event where
the candidate position (X ′, Y′) of another ciphertext C′ ̸= C matches the candidate
position ( ˆ
X || ˆY) = ⌊E(w, 1)⌋2·log2 (t) of w in M. That is, ⌊C⌋2·log2 (t) = ⌊C′⌋2·log2 (t).
Deﬁnition 9 (One-Collision). A one-collision is the event where in an InputSplit a ci-
phertext C′ ̸= EKd(w, 1) puts a 1 into the same candidate position in M as EKd(w, 1).
Overview: The rationale for the result analysis protocol of PRISM is to observe the can-
didate position of C over q rounds to mitigate the effect of one-collisions. Of particular
interest will be rounds where ⌊h(C, i)⌋1 = 1.
First, U un-blinds all values received from reducers. Based on the result, U distin-
guishes two cases.
Case 1.) If a reducer, reducing for a speciﬁc ﬁle FILE, has returned the value 0 for
C’s candidate position, then U knows for sure that all mappers have output 0 for this
candidate position. Consequently, the candidate position in matrix M of each mapper
is 0. Therefore, C has not been in any of the InputSplits of FILE, and U reasons w ̸∈
FILE. If C would have been in one InputSplit, then at least the mapper working on this
InputSplit would have returned a 1 in this round.
Deﬁnition 10 (Contradiction). Let w be the word U is looking for, and C its cipher-
text. If in some round i, ⌊h(C, i)⌋1 = 1 holds, and the reducer for ﬁle FILE sends U a
value of 0 then this is called a contradiction.
In case of such a contradiction, U for sure knows that w is not in ﬁle FILE.
Case 2.) If, however, this reducer returns a value > 0, then w was in at least one
InputSplit or a one-collision has occurred in at least one InputSplit. User U can neither
decide w ̸∈FILE nor w ∈FILE with absolute certainty.
U’s strategy is to keep the probability for one-collisions low and run multiple rounds
q, such that eventually a contradiction occurs (⇒U decides w ̸∈FILE), or, if no con-
tradiction occurs, U decides w ∈FILE with only a small error probability Perr.
Details: U executes Algorithm 3. For each ﬁle, U is only interested in row ˆY of matri-
ces M, as they can refer to candidate position ( ˆ
X , ˆY), only. Therefore, U keeps values
{sFILE, ˆ
Y,1, . . . , sFILE, ˆY,q} only and discards the rest. In each round where ⌊h(C, i)⌋1 =
1, un-blinds sFILE, ˆ
X,i to get value si := c
j=1 bitj. If si = 0, then we have a contra-
diction, and U can infer w ̸∈FILE. If none of the si values has been 0 after all the q
rounds, then U will decide w ∈FILE. U will be wrong with Perr.
Note that, although PIR matrices are binary matrices, U sets N > c to cope with the
larger possible values that sums might take due to collisions.

192
E.-O. Blass et al.
In conclusion, U’s strategy can be summarized by: output w ̸∈FILE, if ∃i, si = 0
or output w ∈FILE, if ∀i, si ̸= 0. We will compute U’s error probability Perr for the
latter case and dependencies between Perr and values t and q in Section 5.2.
Saving Computation: To save some computation in PRISM, we can modify the hash-
based mechanism that determines whether to put a “1” or a “0” in a certain element
in M. Recall that the ﬁrst 2 · log2(t) bit of a ciphertext C are used to determine its
position (element) in M. However, instead of computing an expensive hash function
⌊h(Ci, j)⌋1 to get a single bit in round j, we can simply replace the hash and take C’s
bit on position (2 · log2(t) + j). Assuming that cipher E has good security properties
(each bit of C is “1” with probability 1
2), this results in the same property as using the
hash: eventually two different ciphertexts that collide in M will differ and lead to a
contradiction. We use this computation reduction in our evaluation in Section 6.
5
PRISM Analysis
5.1
Privacy
We will now show why PRISM is privacy-preserving. The main rationale behind our
proof is to show that pairs of output generated by both our stateful cipher construc-
tion EK(wi, γwi) (Section 4.1) and the PIR-based search mechanism (Section 4.2) are
computationally indistinguishable for A. Below, we assume a sufﬁciently large security
parameter s and probabilistic polynomial time adversaries A.
Theorem 1. PRISM is a privacy-preserving cloud word search scheme assuming pseu-
dorandom properties for E and the trapdoor group property of the PIR scheme.
Proof (Sketch). Assume there would be an adversary A with Pr(GAME(A) = 1) >
1
2 + ϵ(s), i.e., A has non-negligible advantage over guessing. As PRISM generates Eb
and Qb independently from each other, this would indicate that A has non-negligible
advantage over guessing in determining b from either Eb or Qb (or both).
We will now show with the following two lemmas that this is impossible.
Lemma 1. In PRISM, any pair of sequences of ciphertexts (ﬁles EL and EL′) gen-
erated by a pair of sequences of words (ﬁles L and L′) is computationally indistin-
guishable for A, assuming E is a pseudorandom permutation and “,” an unambiguous
pairing of inputs.
Proof (Sketch). First, note that our stateful-cipher uses a different random key for each
ﬁle. In a learning phase, A makes a number of queries to two stateful-cipher oracles
encrypting with two different keys K0, K1. Then, A prepares word w, submits to a
challenge oracle and gets back EKb(w, γb,w), b ∈{0, 1}. A has to output b correctly
with only negligible advantage over guessing.
However, we now show by using the hybrid argument [28] that the distributions
generated by EKi(w, γi,w) are computationally indistinguishable for A. That is, pairs
EK0(w, γ0,w), EK1(w, γ1,w) are distinguishable with only negligible advantage over
guessing). As “,” is an unambiguous pairing, we now write w′
i instead of (w, γi,w).

PRISM – Privacy-Preserving Search in MapReduce
193
Our hybrid distributions are: (1.) PRPK0(w′
0), (2.) RPK0(w′
0), (3.) RFK0(w′
0), (4.)
RFK1(w′
1), (5.) RPK1(w′
1), and (6.) PRPK1(w′
1). “PRP” means pseudorandom per-
mutation, “RP” random permutation, and “RF” random function.
(1.) - (2.) and (5.) - (6.): by deﬁnition of pseudorandom permutation, the probability
to distinguish PRPK(w′
i) from RPK(w′
i) is negligible.
(2.) - (3.) and (4.) - (5.): the probability to distinguish a random permutation from a
random function is negligible, cf., Section 3.6.3 in Katz and Lindell [28].
(3.) - (4.): If A observes RFK0(w′
0) = RFK1(w′
1) for a pair w′
0, w′
1, then this only
indicates a collision in RFK0 and RFK1. Even if A queries the same w multiple times,
output RFK0(w, γ0,w) or RFK1(w, γ1,w) will always be different as counters increase.
If RFK0 (or RFK1) outputs the same value twice (unlikely), this only indicates a col-
lision in RFK0 (or RFK1). The advantage over guessing in distinguishing RFK0(w′
0)
from RFK1(w′
1) is zero.
A’s advantage over guessing in distinguishing pairs EL, EL′ is negligible.
⊓⊔
Lemma 2. Based on the trapdoor group assumption (“TGA”), PRISM’s PIR-search
produces computationally indistinguishable pairs of queries Qi.
Proof (Sketch). Assume A submits two words w1, w2 to an oracle. The oracle picks
ˆb ∈{0, 1} and returns Qˆb := {αˆb,1 := b · eˆb,1 mod p, . . . , αˆb,t := b · eˆb,t mod p},
with eˆb,Xˆb := 1+aXˆb·N, ∀i ̸= Xˆb : eˆb,i := ai·N. Here, Xˆb := ⌊EK(wˆb, 1)⌋log2 (t), and
ai are chosen randomly. A has to output ˆb with non-negligible advantage over guessing.
However, we will now show that any pair of sequences of α values is computa-
tionally indistinguishable for A. The proof is a direct implication of the security of
the PIR protocol, based on TGA: for all adversaries A, Pr[A(b · e1, . . . , b · et) =
LSB(e1, . . . , et)] = ϵ(s). That is, given b · ei, the probability that A computes low
order bits of ei mod N (“LSB”) is negligible [37].
Assume that A can distinguish sequences Q0 = {α0,i} and Q1 = {α1,i} with
non-negligible advantage. This would violate TGA as follows. First, note that besides
α0,X0, α1,X1 all elements in both sequences {α0,i} and {α1,i} are created in the same
way (multiplication of b with a random number). Therefore, besides α0,X0, α1,X1, any
pair (α, α′) ∈{α0,i} ∪{α1,i} is computationally indistinguishable for A. If A can still
distinguish between sequences {α0,i} and {α1,i}, then A can determine with non-ne-
gligible probability X0 or X1 and thus value i with ei = 1 mod N, violating TGA.
⊓⊔
5.2
Statistical Analysis
We now discuss how U chooses parameters t and q to get a certain error probability
Perr. This probability describes the chance that, despite w ̸∈FILE, U wrongly outputs
w ∈FILE after q rounds without a contradiction, cf., Algorithm 3. Let n be the number
of ciphertexts in one InputSplit, n :=
SInputSplit
CipherBlockSize. The total number of ciphertexts
stored in the cloud is (c·n). We consider for simplicity only rounds where ⌊h(C, i)⌋1 =
1, cf., Algorithm 3. With h a cryptographic hash, ⌊h(C, i)⌋1 = 1 in q′ ≈q
2 rounds.
While inserting any ciphertext, the collision probability is Pcollision := 1
t2 . The prob-
ability for a one-collision is Pone−collision := Pcollision
2
. If w is not inside an InputSplit,

194
E.-O. Blass et al.
the probability that, after inserting the n ciphertexts of that InputSplit into M, the can-
didate position is not set to 1 is PInputSplit,no−one−collision := (1 −Pone−collision)n.
If w ̸∈FILE, i.e., in none of the InputSplits, the probability that the candidate posi-
tion is not set to 1 in any InputSplit is Pcontradiction := (PInputSplit,no−one−collision)c.
This is the probability that a contradiction occurs in a single round. If w ̸∈FILE, the
probability that a contradiction occurs in at least one round is Pcontradiction,q−rounds :=
1 −(1 −Pcontradiction)q.
After q rounds without a contradiction, U automatically decides that w is in FILE. In
case that w ̸∈FILE, and no contradiction occurs in q rounds, U is therefore wrong with
Perr := 1 −Pcontradiction,q−rounds = (1 −(1 −
1
2·t2 )cn)q.
Given a certain ﬁle size, the size of InputSplits, and the blocksize of the symmetric
cipher, U computes c and n. Therewith, U can target a false-positive probability by ap-
propriately selecting t and q. We evaluate this using a real-world scenario in Section 6.
6
Evaluation
To show its real-world feasibility, we have implemented and evaluated PRISM with the
scenario described in the introduction. The source code is available for public down-
load [1]. We received 16 days of log data from May 2010 from a small local Internet
provider. This provider logs and retains all customers’ DNS resolve requests for possi-
ble forensic analysis and intrusion detection. Log data is split into ﬁles on a daily basis.
Each ﬁle contains one day of logged 3-tuples: timestamp, customer IP (anonymized by
provider for regulatory matters), hostname. The scenario for our evaluation is to use
PRISM to upload this data encrypted to MapReduce and perform a search for speciﬁc
hostnames in a privacy-preserving manner. This is useful for, e.g., “passive DNS anal-
ysis” to determine at which day certain command-and-control centers of botnets have
been accessed by customer machines, cf., Bilge et al. [5]. The goal of our experiments
was to analyze the computational overhead induced by PRISM’s privacy mechanism,
i.e., the additional time consumed by PRISM over non-privacy-preserving MapReduce.
6.1
Setup
For the 16 days, the log data contains ≈3·108 log entries, i.e., ≈2·107 per ﬁle/day. The
total space required by all ﬁles uploaded into MapReduce using PRISM is 27 GByte,
on average 1.7 GByte per ﬁle.
Our experiments have been performed on a small “cloud” comprising 1 master com-
puter and 9 slaves. Computers featured a 2.5 GHz Pentium Dual Core and 4 GByte of
RAM, running a standard desktop installation of Fedora 11. With this hardware conﬁg-
uration, a total of 18 CPUs were available for maps and reduces. We installed Hadoop
version 0.20.2 on our cloud. Being aware that tailoring MapReduce’s conﬁguration pa-
rameters can have a huge impact on performance, we use the standard, out-of-the-box
conﬁguration of Hadoop 0.20.2 without any conﬁguration tweaks. Performance tuning
is out of scope of this paper. Similarly, as the InputSplit size is recommended to be be-
tween 64 MByte and 128 MByte, we chose SInputSplit = 96 MByte (InputSplits must
be dividable by 3 · 32 Byte, since log entries are 3-tuples).

PRISM – Privacy-Preserving Search in MapReduce
195
Table 1. Parameters t, q to achieve Perr < 0.01
File size (GByte)
0.45 1.21 1.32 1.36 1.38 1.45 1.52 1.67 1.78 1.93 2.00 2.08 2.09 2.14 2.21 2.25
t 210
211
212
q 100
60
80
20
In addition to the evaluation with 96 MByte InputSplits, we also performed a second
measurement with larger InputSplits of 120 MByte. We expected a slightly improved
performance of PRISM due to the fact that for the larger ﬁles the total number of Input-
Splits c reduced to less than our 18 available CPUs. Therewith, no costly (re-)scheduling
takes places, and mappers do not have to process 2 InputSplits sequentially.
Finally, to put timing results into perspective, we implemented and measured a triv-
ial, non-privacy-preserving MapReduce search called Baseline. Baseline search con-
sists of an empty map phase, where mappers simply scan over InputSplits and compare
each word of the InputSplit with a predetermined one, but do not generate any key-value
pairs. Only at the end of the map phase, a single intermediate key-value pair per map-
per (e.g., “found”) is sent to reducers. Reducers discard this key-value pair and write
empty ﬁles to disk. This trivial baseline only serves in deducing the overhead implied
by PRISM, not taking MapReduce speciﬁc delays due to rescheduling, speculative exe-
cution of backup tasks etc. [17, 34] into account. Note that linear scanning through the
entire InputSplit is mandatory, as we assume our data to be unordered and unsorted.
For the private information retrieval algorithm, we set m = 400 as suggested by
Trostle and Parrish [37] for good security. Our Java implementation is a naive, straight-
forward implementation using Java’s BigInteger without any performance optimiza-
tions. As symmetric encryption cipher, we used AES with 256 Bit blocksize from the
GNU Crypto Library V2.0.1 [20]. As individual DNS entries occurred way less than
216 times per day, we reserved |γ| = 2 Bytes and truncated entries longer than 30 Byte
down to the last 30 Byte. Because the size of input |wi|+|γwi| is less than E’s blocksize
(using standard padding for wi), concatenation provides an unambiguous pairing.
Simulating U, we computed n and c using blocksize, InputSplit size SInputSplit,,
and individual ﬁle size SFile. Assuming that U targets an error probability of Perr <
0.01, we derived t and q. Table 1 summarizes parameters (t, q) computed for each ﬁle
individually. Compared to q, we observed that parameter t has a much higher impact
on Perr, but a comparatively lower impact on computations. Therefore, we increased
preferably t than q. Higher values for (t, q) will achieve even smaller values for Perr,
but Table 1 shows the computationally “cheapest” combination of (t, q).
6.2
Results
Computational overhead at the cloud is low as indicated by Figure 1 (PRISM’s timing
results). We have sorted the 16 ﬁles based on their size in an increasing order, i.e, the
size of the smallest log ﬁle we received from the Internet provider was 0.45 GByte, the
largest one was 2.25 GByte. PRISM’s execution time was clocked on each ﬁle 6 times,
respectively, and Fig. 1 shows the average. For each ﬁle, Fig. 1 shows two stacked
boxes, respectively: the ﬁrst one for 96 MByte and the second one for 120 MByte

196
E.-O. Blass et al.
 100
 200
 300
 400
 500
0.45
1.21
1.32
1.36
1.38
1.45
1.52
1.67
1.78
1.93
2.00
2.08
2.09
2.14
2.21
2.25
Time (s)
File Size (GByte)
0
PRISM-96
Baseline-96
PRISM-120
Baseline-120
Fig. 1. Wall clock timings for PRISM and Baseline, with SInputSplit = 96 and 120 MByte
InputSplit size. Each of the stacked boxes comprises, ﬁrst, the baseline timing and,
second, the additional time required to run PRISM. To give trust into the evaluation,
Fig. 1 also shows 95% conﬁdence intervals drawn right next to each box.
Timings shown in Fig. 1 are “wall clock” timings. This captures the complete time
elapsed from submitting the PRISM map and reduce classes and starting the job until
the end of the reduce phase. In the real-world, wall clock time reﬂects the time a cloud,
e.g., Amazon [2], would charge a user U. In conclusion, the additional overhead over
the trivial Baseline MapReduce jobs was on average 11% with a 95% conﬁdence inter-
val of ±3. The largest overhead seen was 24% over Baseline. This overhead is mostly
computational overhead, as there is no difference in disk access between Baseline and
PRISM and network volume increases only little by sending slightly larger values dur-
ing “Reduce”. These results do not only show the feasibility of PRISM in practice, but
also demonstrate the low overhead implied by PRISM over the non-privacy preserving
MapReduce job. We claim that a performance optimized (not based on Java BigInteger)
implementation improves performance signiﬁcantly and furthermore reduces overhead.
The simple increase from 96 MByte InputSplit size to 120 MByte InputSplit size
has reduced wall clock times for MapReduce jobs by 9% on average (95% conﬁdence
interval of ±4). Files of size smaller than 2 GByte are split into ≤18 InputSplits, and
both jobs, PRISM and Baseline, are processed completely in parallel. This indicates
that a careful conﬁguration of Hadoop MapReduce’s many system parameters, hand-
crafted and speciﬁc to the scenario and jobs to be executed, will lead to substantial
performance improvements. This also indicates that in a cloud with more CPUs than
in our small setup, the increased number of CPUs will enable to conﬁgure way smaller
InputSplits being processed in parallel. Substantially smaller InputSplits will be beneﬁ-
cial for the overall performance of PRISM or any MapReduce job. However, increasing
the number of InputSplits also implies a performance penalty due to (re-)scheduling
and coordination activities of the central job tracker, cf., Pavlo et al. [34], so a trade-off
has to be found. MapReduce conﬁguration optimizations are, however, out of scope.

PRISM – Privacy-Preserving Search in MapReduce
197
To better understand the cloud’s computational overhead, we also measured the com-
putation time for a PRISM mapper. On a single CPU, execution of an isolated
PRISM map function on a single InputSplit is ca. 9 times slower than Baseline (9.3
for 96 MByte and 9.1 for 120 MByte, 95% conﬁdence interval of ±0.1). While this
seems to be a lot, we remark that 1.) this map overhead is constant for an InputSplit and
does not depend on or scale with the total size of the data, 2.) there is a lot of potential
to improve our map implementation, 3.) this overhead is obviously amortized by other
MapReduce aspects such as the Reduce phase and also disk latency, network overhead
etc., and 4.) a user is charged for the total system time, i.e., the wall clock time.
Computational overhead at the User is also low in PRISM: per ﬁle, the preparation
of, e.g., 212 α values for the underlying PIR scheme is barely measurable (≈200 ms) on
a PC with 2.5 GHz CPU. During result analysis, U automatically discards all received
values that he is not interested in, i.e., all besides sFILE,Y,1≤i≤q. For these q values, a
total of q Java BigInteger multiplications with modulo have to be performed. For our
examples with q ≤100, this was not measurable at less than 1 ms.
Memory consumption for U is, on the one hand, constant; U only stores the 256 bit
AES key K. On the other hand however, the cloud user U’s memory consumption scales
linearly with O(Σ), i.e., the number of different words. This is due to the construction
of our stateful cipher that stores counters γ in a hashtable. In our straightforward im-
plementation with Java’s standard Hashtable, memory consumption of this hashtable
was 548 MByte for the largest log ﬁle. While this is certainly a lot of RAM, we con-
jecture this to be available on PC hardware – moreover, as there is a large potential for
performance tuning with such data structures.
Communication overhead for PRISM is dominated by the underlying PIR scheme.
U sends, besides .class ﬁles once, only the t α values per ﬁle to the cloud. For example,
with t = 212 and m = 400 Bit, this computes to 200 KByte per ﬁle. The response
from the cloud is, for each round, t values of size m. The most expensive conﬁguration
in terms of communication in our experiments has been t = 210, q = 100; this results
in ≈5 MByte communication overhead. Note that communication complexity in the
underlying PIR scheme by Trostle and Parrish [37] is linear in the square root of the
total table size, i.e., O(t). This can be further reduced by using recursive PIR queries
to O(tϵ), for any ϵ > 0 [29]. Those optimizations as well as amortization techniques
discussed by Ostrovsky and Skeith [33] are out of scope.
In conclusion, PRISM is very lightweight for a user using standard PC hardware.
Discussion: On a larger cluster in a more professional environment (hundreds or
even thousands of CPUs [25]), all ﬁles will be processed in parallel. As shown in Fig. 1,
total time for the 2 GByte ﬁle is ≈350 s. However, already ≈340 s are required by
MapReduce just to “scan” through the various InputSplits, see Baseline. Such inefﬁ-
ciency with non-optimal conﬁgurations has been observed before, and our results are
along the lines of Pavlo et al. [34]. Here, a “grep”-like MapReduce job on 1 TByte of
data took ≈1, 500 s on 50 CPUs which would be ≈20 times faster than our Base-
line. However, Pavlo et al. [34] use a slightly tuned conﬁguration and moreover a more
efﬁcient scanning through InputSplits (100 Byte text values instead of 32 Byte binary
values in our case) which is known to lead to signiﬁcant performance increases [27].

198
E.-O. Blass et al.
7
Related Work
Private Information Retrieval: Private Information Retrieval (and similarly oblivious
transfer and oblivious RAM) has received a lot of attention [9, 13, 19, 22, 29, 32, 33,
35]. In PIR, a user retrieves a speciﬁc data from a database. The only “privacy” goal
in PIR is access privacy whereby the server should not discover which data a user is
interested in. Note that PIR does not ensure privacy of data in the database. PRISM,
however, focuses on searching for a word and uses PIR only as a tool.
Searchable Encryption: With searching on encrypted data techniques [6], user pri-
vacy is guaranteed thanks to the encryption of the queries and the stored data. However,
PRISM offers higher privacy guarantees since in existing searchable encryption solu-
tions [4, 6–8, 10, 16, 21, 31, 36], the result (“found” or “not found”) originating from a
query is known to the adversary; therefore as opposed to PRISM, standard searchable
encryption techniques do not ensure query privacy. Moreover, existing mechanisms
cannot be easily extended to leverage from a parallelized cloud setup: while in theory
the search on encrypted data itself could be run in parallel on subsets of data, today’s
solution do not support the combination (aggregation) of results (as in a reduce phase).
To conclude, PRISM not only ensures both storage privacy and query privacy, but also
enables the aggregation of results originating from intermediate parallelized operations.
8
Conclusion
PRISM is the ﬁrst privacy-preserving search scheme suited for cloud computing. That
is, PRISM provides storage and query privacy while introducing only limited overhead.
PRISM is speciﬁcally designed to leverage parallelism and efﬁciency of the MapRe-
duce paradigm. Moreover, PRISM is compatible with any standard MapReduce-based
cloud infrastructure (such as Amazon’s), and does not require modiﬁcations to the un-
derlying system. Thanks to this compatibility, PRISM has been efﬁciently implemented
on an experiemental cloud computing environment using Hadoop MapReduce. Besides
a throughout analysis, performance of PRISM has been evaluated on that environment
through search operations in DNS logs provided by an ISP. PRISM’s overhead over
non-privacy-preserving search is only 11% on average, acertaining its efﬁciency.
References
[1] PRISM source code (2012), http://www.ccs.neu.edu/˜blass/prism.tgz
[2] Amazon. Elastic mapreduce (2010),
http://aws.amazon.com/elasticmapreduce/
[3] Apache. Hadoop (2010), http://hadoop.apache.org/
[4] Bellovin, S.M., Cheswick, W.R.: Privacy-enhanced searches using encrypted Bloom ﬁlters
(2007), http://mice.cs.columbia.edu/
getTechreport.php?techreportID=483
[5] Bilge, L., Kirda, E., Kr¨ugel, C., Balduzzi, M.: Exposure: Finding malicious domains us-
ing passive dns analysis. In: Proceedings of 18th Annual Network and Distributed System
Security Symposium, San Diego, USA, pp. 195–211 (2011) ISBN 1891562320

PRISM – Privacy-Preserving Search in MapReduce
199
[6] Boneh, D., Di Crescenzo, G., Ostrovsky, R., Persiano, G.: Public Key Encryption with Key-
word Search. In: Cachin, C., Camenisch, J.L. (eds.) EUROCRYPT 2004. LNCS, vol. 3027,
pp. 506–522. Springer, Heidelberg (2004)
[7] Boneh, D., Kushilevitz, E., Ostrovsky, R., Skeith III, W.E.: Public Key Encryption That
Allows PIR Queries. In: Menezes, A. (ed.) CRYPTO 2007. LNCS, vol. 4622, pp. 50–67.
Springer, Heidelberg (2007)
[8] Brassard, G., Cr´epeau, C., Robert, J.M.: All-or-Nothing Disclosure of Secrets. In: Odlyzko,
A.M. (ed.) CRYPTO 1986. LNCS, vol. 263, pp. 234–238. Springer, Heidelberg (1987)
[9] Cachin, C., Micali, S., Stadler, M.A.: Computationally Private Information Retrieval with
Polylogarithmic Communication. In: Stern, J. (ed.) EUROCRYPT 1999. LNCS, vol. 1592,
pp. 402–412. Springer, Heidelberg (1999)
[10] Chang, Y.-C., Mitzenmacher, M.: Privacy Preserving Keyword Searches on Remote En-
crypted Data. In: Ioannidis, J., Keromytis, A.D., Yung, M. (eds.) ACNS 2005. LNCS,
vol. 3531, pp. 442–455. Springer, Heidelberg (2005)
[11] Chief Information Ofﬁcer’s Council. Proposed security assessment & authorization for
U.S. government cloud computing (2010),
http://www.digitalgovernment.com/media/
Knowledge-Centers/asset upload file652 2491.pdf
[12] Chief Information Ofﬁcer’s Council. Privacy recommendations for the use of cloud com-
puting by federal departments and agencies (2010), http://www.cio.gov/
[13] Chor, B., Goldreich, O., Kushilevitz, E., Sudan, M.: Private information retrieval. In: Pro-
ceedings of Symposium on Foundations of Computer Science, Milwaukee, USA, pp. 41–51
(1995)
[14] Cloud Security Alliance. Security guidance for critical areas of focus in cloud computing
(2009), https://cloudsecurityalliance.org/
guidance/csaguide.v2.1.pdf
[15] Cloud
Security
Alliance.
Top
cloud
computing
threats
(2010),
https://cloudsecurityalliance.org/
topthreats/csathreats.v1.0.pdf
[16] Curtmola, R., Garay, J., Kamara, S., Ostrovsky, R.: Searchable symmetric encryption: im-
proved deﬁnitions and efﬁcient constructions. In: Proceedings of Conference on Computer
and Communications Security, CCS, Alexandria, USA, pp. 79–88 (2006)
[17] Dean, J., Ghemawat, S.: Mapreduce: Simpliﬁed data processing on large clusters. In: Pro-
ceedings of OSDI, San Francisco, USA, pp. 137–150 (2004)
[18] EU, Eu information management instruments (2010), http://europa.eu/
[19] Gertner, Y., Ishai, Y., Kushilevitz, E.: Protecting data privacy in private information re-
trieval. In: Proceedings of Symposium on Theory of Computing, Dallas, USA, pp. 151–160
(1998) ISBN 0-89791-962-9
[20] GNU, The gnu crypto project (2011), http://www.gnu.org/software/
[21] Goh, E.-J.: Secure indexes. Cryptology ePrint Archive Report 2003/216 (2003),
http://eprint.iacr.org/2003/216
[22] Goldreich, O., Ostrovsky, R.: Software protection and simulation on oblivious ram. Journal
of the ACM 45, 431–473 (1996) ISSN 0004-5411
[23] Goldwasser, S., Micali, S.: Probabilistic encryption. Journal of Computer and System Sci-
ences 28(2), 270–299 (1984) ISSN 0022-0000
[24] Google. Google apps for government (2010),
http://googleenterprise.blogspot.com/2010/07/
google-apps-for-government.html
[25] Hadoop. Powered by hadoop, list of applications using hadoop mapreduce (2011),
http://wiki.apache.org/hadoop/PoweredBy

200
E.-O. Blass et al.
[26] Hall, C., Goldberg, I., Schneier, B.: Reaction Attacks against Several Public-Key Cryptosys-
tem. In: Varadharajan, V., Mu, Y. (eds.) ICICS 1999. LNCS, vol. 1726, pp. 2–12. Springer,
Heidelberg (1999)
[27] Jian, D., Ooi, B.C., Shi, L., Wu, S.: The performance of mapreduce: An in-depth study.
Proceedings of the VLDB Endowment 3(1), 472–483 (2010)
[28] Katz, J., Lindell, Y.: Introduction to modern cryptography. Chapman & Hall/CRC (2008)
ISBN 978-1-58488-551-1
[29] Kushilevitz, E., Ostrovsky, R.: Replication is not needed: single database, computationally-
private information retrieval. In: Proceedings of Symposium on Foundations of Computer
Science, Miami Beach, USA, pp. 364–373 (1997)
[30] McCullagh, D.: Fbi wants records kept of web sites visited (2010),
http://news.cnet.com/8301-13578_3-10448060-38.html
[31] Ogata, W., Kurosawa, K.: Oblivious keyword search. Journal of Complexity – Special Issue
on Coding and Cryptography 20, 356–371 (2004) ISSN 0885-064X
[32] Ostrovsky, R., Skeith III, W.E.: Private Searching on Streaming Data. In: Shoup, V. (ed.)
CRYPTO 2005. LNCS, vol. 3621, pp. 223–240. Springer, Heidelberg (2005)
[33] Ostrovsky, R., Skeith III, W.E.: A Survey of Single-Database Private Information Re-
trieval: Techniques and Applications. In: Okamoto, T., Wang, X. (eds.) PKC 2007. LNCS,
vol. 4450, pp. 393–411. Springer, Heidelberg (2007)
[34] Pavlo, A., Paulson, E., Rasin, A., Abadi, D.J., DeWitt, D.J., Madden, S., Stonebraker, M.:
A comparison of approaches to large-scale data analysis. In: Proceedings of International
Conference on Management of Data, Rhode Island, USA, pp. 165–178 (2009)
[35] Sion, R., Carbunar, B.: On the computational practicality of private information retrieval. In:
Proceedings of Network and Distributed Systems Security Symposium, San Diego, USA,
pp. 1–10 (2007)
[36] Song, D.X., Wagner, D., Perrig, A.: Practical techniques for searches on encrypted data. In:
Proceedings of Symposium on Security and Privacy, Berkeley, USA, pp. 44–55 (2000)
[37] Trostle, J., Parrish, A.: Efﬁcient Computationally Private Information Retrieval from
Anonymity or Trapdoor Groups. In: Burmester, M., Tsudik, G., Magliveras, S., Ili´c, I. (eds.)
ISC 2010. LNCS, vol. 6531, pp. 114–128. Springer, Heidelberg (2011)

Practical Privacy Preserving Cloud
Resource-Payment for Constrained Clients
Martin Pirker1, Daniel Slamanig2, and Johannes Winter1
1 Institute for Applied Information Processing and Communications (IAIK),
Graz University of Technology (TUG), Inﬀeldgasse 16a, 8010 Graz, Austria
{martin.pirker,johannes.winter}@iaik.tugraz.at
2 Department of Engineering and IT, Carinthia University of Applied Sciences,
Primoschgasse 10, 9020 Klagenfurt, Austria
d.slamanig@cuas.at
Abstract. The continuing advancements in microprocessor technologies
are putting more and more computing power into small devices. Today
smartphones are especially popular. Nevertheless, for resource intensive
tasks such devices are still too constrained. However, the simultaneous
trend of providing computing resources as a commodity on a pay-as-you-
go basis (cloud computing) combined with such mobile devices facilitates
interesting applications: Mobile clients can simply outsource resource in-
tensive tasks to the cloud. Since clients have to pay a cloud provider
(CP) for consumed resources, e.g. instance hours of virtual machines,
clients may consider it as privacy intrusive that the CP is able to record
the activity pattern of users, i.e. how often and how much resources are
consumed by a speciﬁc client. In this paper we present a solution to this
dilemma which allows clients to anonymously consume resources of a CP
such that the CP is not able to track users’ activity patterns. We present
a scenario which integrates up-to-date security enhanced platforms as
processing nodes and a recent cloud payment scheme together with a
concrete implementation supporting the practicality of the proposed
approach.
1
Introduction
The sustained advancements in microprocessor technologies put more and more
computing power into smaller and smaller devices. Today’s smartphones essen-
tially contain the computing “brainpower” of a desktop PC of not many years
ago. This development, along with improved wireless connectivity, naturally fu-
els a shift in device usage patterns. It is no longer necessary to use a full-size
desktop PC or laptop for certain tasks. Today, a smartphone is very well capable
to perform many of the tasks desired by common end-users, such as browsing
the web, or participation in social network services. Due to the small form fac-
tor, however, a smartphone’s resources are limited in terms of battery power,
storage, and consequently on available processing capacity.
A parallel trend is the rapid advancement of virtualisation technologies in con-
juction with cheap storage and fast (wireless) Internet connections. This has cre-
ated a market for providing computing resources as a commodity – so called cloud
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 201–220, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

202
M. Pirker, D. Slamanig, and J. Winter
computing. Large data centers can take advantage of the economics of scale and
dynamically lease computing power or storage capacities to clients on demand.
This trend promises to use IT resources more eﬃciently and to reduce costs.
Consequently, in the future the desktop PC may continue to loose importance
and the split processing model may rise to dominance. In the split processing
model small tasks are executed directly on end-user devices such as smartphones,
tablets or other gadgets, while more complex and demanding jobs are delegated
to remote cloud computing services where resources are leased on demand.
Contribution. In this paper, we address the privacy concerns of the split pro-
cessing model. We introduce a scenario where a low-resource client obtains cloud
processing credits from a reseller and then uses them to pay for high-powered ser-
vices at a remote cloud provider. Our main motivation for a “privacy-preserving
by design” approach is that such cloud providers will be able to link data and
information about resource consumption behaviour of their consumers (clients),
allowing them to build dossiers. For many customers such transparency can be
too intrusive. We want clients to be able to hide this information from cloud
providers. As, for instance, argued in [9], activity patterns may constitute con-
ﬁdential business information and if divulged could lead to reverse-engineering
of customer base, revenue size, and the like.
More precisely, we consider a setting where clients should be able to purchase
a set of prepaid resources in form of cloud credits (CCs) from a reseller, e.g.
represented as a scratch-oﬀcard. This card contains information which allows
the client to obtain a single compact software token from a cloud provider that
includes how many CCs a client is allowed to consume from this cloud provider.
Then, clients should be able to consume their CCs from the cloud provider in an
anonymous and unlinkable yet authorized fashion. For instance, if a client wants
to consume n credits, he has to convince the CP that he possesses a valid token
and he is still allowed to consume n credits. If this holds, the anonymous client is
allowed to consume the resources and obtains an updated token corresponding
to the remaining CCs in a privacy-preserving manner. Although the CP should
be unable to track clients and determine how much credits a client has already
consumed, it must be guaranteed that the client only consumes as much resources
as CCs available in his token.
The novelty of our approach presented in this paper is that we use a recent
anonymous yet authorized and bounded cloud resource scheme from [20] in a
scenario which integrates up-to-date security enhanced platforms as processing
nodes. We assume ARM TrustZone architecture enabled smartphone clients and
Trusted Execution Technology attestable servers in the cloud. This scenario re-
sembles a realistic use-case for a cloud computing environment and could be
realised with today’s mass-market hardware in the near future. We also pro-
vide experimental results from a prototypical implementation of our scheme on
various current platforms.
Outline. The remainder of the paper is structured into the following ma-
jor sections. In Section 2 we present our scenario, in which we introduce all

Practical Privacy Preserving Cloud Resource-Payment
203
entities, their motivations as well as their privacy requirements. We then con-
tinue in Section 3 with preliminaries, including a brief background summary of
the underlying anonymous yet authorized and bounded cloud resource scheme
and the capabilities of trusted platform security technologies – in particular the
TrustZone and TXT architectures. Section 4 introduces our concrete scheme and
presents a description of all operations between the entities. We report practical
implementation results in Section 5. We consider supplemental security, privacy
implications, and trade-oﬀs in Section 6. In Section 7 we present a brief overview
of related work. Finally, Section 8 concludes the paper.
2
Scenario and High Level Description
In this section we present a practical scenario for deployment of our privacy
preserving resource payment scheme. First we identify the core participants in-
teracting and then describe the scenario. Then, we provide a high level descrip-
tion of the operations between the entities. We also discuss privacy issues for all
involved entities.
2.1
Entities
In the scenario of this paper a small, resource limited client (C) wants to out-
source computations to a powerful cloud datacenter.
The cloud provider (CP) professionally runs this large datacenter which takes
advantage of the economics of scale. He rents computing power to anyone who
can pay for the resources consumed. We make no assumption about what kind
of resources are leased, we just deﬁne them to be accounted in discrete units of
cloud credits (CC). Naturally, the client and the cloud provider must consent to
a protocol so that the client can prove to the CP that he is eligible to consume a
certain amount of resources. Obviously, client and cloud provider must interact
directly when the client uses a CP service. However, before this step the client
ﬁrst has to obtain a certain amount of cloud credits.
As signiﬁcant feature we thus introduce a third entity, the credit reseller (CR).
The reseller obtains cloud credit units from the cloud provider in bulk and sub-
sequently distributes and resells them through (small) distribution branches.
This enables a wide range of use cases, such as cloud credits “gift-cards”, and
explicitly time-delayed, asynchronous interaction between the entities.
2.2
Scenario
We assume the client to be a state-of-the-art smartphone, based on an ARM
platform with TrustZone capabilities. The smartphone is connected to the In-
ternet, and thereby the cloud provider’s servers, through a wireless connection
to a mobile network. Figure 1 shows the major building blocks of our scenario
and subsequently we reﬂect on the role of each:

204
M. Pirker, D. Slamanig, and J. Winter
Fig. 1. Scenario in which a smartphone client connects to a cloud datacenter
Cloud provider: Computing resources provided by the cloud provider are
servers which were booted with a Trusted Execution Technology measured
process (cf. Section 3.2). This enables the client to verify that he actually
receives what he paid for.
Network: In our scenario the client is a smartphone in a mobile network, and
thus the client is uniquely identiﬁable and his activities can be easily mon-
itored by the mobile network provider. The network connection continues
via the global Internet to the cloud provider. As privacy feature we assume
the use of an anonymizing network like Tor [13] to hide the eﬀective network
address of the client from the cloud provider. Consequently, also the mobile
provider is unable to log where the smartphone is connecting to1.
Smartphone: The client device is essentially split into two processing worlds:
The normal-world which hosts the mobile’s OS, e.g. Android as is common
today, and end user applications. There is also a small isolated execution area
– the secure-world – for sensitive data and operations. The link between the
two worlds is strictly monitored and only well-deﬁned calls are allowed2.
The client’s cloud credits accounting and private data storage is done in
the secure-world. Once the initial credit data structures (representing the
token) are imported, they never leave again (only in the case when they are
transferred to another client).
2.3
High Level Description of the Operations
We model the ﬂows of cloud credits in our scenario as essentially a circle between
our three entities. The cloud provider is the central entity. He is responsible for
providing resources and requires clients to pay for the consumed resources by
providing the necessary number of CCs to him. Clients who want to consume
resources from a cloud provider need to acquire a certain number of CCs from a
credit reseller beforehand. This transaction is carried out without involving the
1 We ignore sophisticated global network surveillance scenarios and defer discussion
of these security edge cases to the Tor community.
2 See e.g. [24] for a practical realization of this approach.

Practical Privacy Preserving Cloud Resource-Payment
205
cloud provider and thus can be carried out oﬄine. After acquiring the credits
from the reseller, clients can activate the credits at the cloud provider, maybe
at some later point in time. Clients may not only consume acquired credits by
themselves, but may also give or sell them to other clients. Furthermore, if clients
pay too many credits at the cloud provider for a certain task, e.g. because they
do not know how many resources the task actually requires beforehand, then a
cloud provider can issue vouchers for unused credits. A graphical sketch of our
high-level operations for the client, cloud provider, and credit reseller setting
is given in Figure 2. Subsequently we present a high level description of the
operations.
Fig. 2. Core Payment Scheme Operations
Issue: This transaction is an initial bulk transfer of cloud credits (CC) from
the cloud provider (CP) to the credit reseller (CR). It is a personal business
relationship operation and assumed to happen in a secure and reliable way.
Acquire: This transaction is carried out between a client (C) and a CR. At
the end of this transaction C holds an amount of CCs, which cannot be used
before they are activated.
Activate: This transaction is carried out between C and the CP. Essentially, C
ends up holding an amount of activated CCs (in form of one compact token),
which may have a limited validity.
Spend: This transaction is carried out between a client C and CP, where C pays
a certain number of CCs and consumes an equivalent amount of resources
from CP. The client ends up with the result of the task performed at the
CP, and the remaining number of activated CCs.
SplitCredits: This is a transaction between C and CP in which C ends up
with (at most) two valid tokens. Let us assume that C wants to split oﬀn
CCs from his collection of m CCs (if m > n)3. Then the ﬁrst token contains
m −n CCs and the second token n CCs.
3 Otherwise, if m = n holds the ﬁrst token will not be valid any more and if m < n
holds, the second token will only contain m CCs. Nevertheless, how such an operation
ends is application dependent and one may also wish to terminate the operation if
m < n.

206
M. Pirker, D. Slamanig, and J. Winter
Refresh: This transaction is carried out between C and CP. It is required when
CCs have a limited validity. When the validity period ends, C carries out this
transaction and gets issued the same amount of activated credits he already
holds, but valid within the new period.
Transfer (not shown in Figure): This transaction is carried out between two
clients C1 and C2, whereas C1 transfers a number of activated CCs to C2
and C1’s CCs are deleted, i.e. we explicitly want to support transferability.
2.4
Privacy Issues
With the entities known we now discuss the privacy requirements which are
eﬀective in our scenario. We assume that two entities can always connect to
each other using a private, i.e., conﬁdential and authenticated, channel4. Note
that this does not mean that the entities need to mutually authenticate to each
other using their identities. Essentially, for our scenario we will realize privacy
by anonymity and unlinkability of conducted transactions. This presents a good
compromise in which clients have privacy guarantees while at the same time the
other entities can be sure that only authorized actions can be conducted.
Cloud Provider. From a privacy perspective the cloud provider is in the posi-
tion to be honest, but curios. Thus, he has the means to sniﬀ, log and track the
cloud’s activities and is subsequently able to ﬁgure out who his clients are and
what they are doing. He must be able to demonstrate a certain level of secu-
rity of his services-for-rent. Without such a proof, customers will eschew doing
business with him as they cannot be sure what is happening with their data.
Credit Reseller. The relationship between the cloud provider and the credit
reseller is a business one. By deﬁnition, the cloud provider wants a reliable and
clearly identiﬁable reseller, and the reseller wants to ensure that he is dealing
with the correct provider contact point. From this follows the requirement for
secure communications and transactions between the two when they trade credits
for money – and privacy is not important in this context.
The reseller is the entity who comes into real-life contact with the end-user.
Consequently, the privacy of the client very much depends on the method of
payment for the prepaid cloud credits oﬀered and how the actual handover is
organized. Similar to the cloud provider, the reseller has no motivation to invest
extra eﬀort to identify his clients without additional external pressure.
Client. The client is the entity motivated to ensure that privacy is enforced at
all steps in the transactions. First he has to obtain credits from the reseller. A
straightforward solution is to pay with money in cash, an intuitive way to ensure
privacy of payment. By deﬁnition the client already knows which cloud provider
he wants to use, thus he must be able to verify that the oﬀered credits from the
reseller are genuine. Second, for establishment of an untraceable, i.e. anonymous
and unlinkable, connection to the cloud provider the client uses an anonymous
communication network, e.g. Tor.
4 E.g., using Transport Layer Security (TLS) [11] over a Tor [13] connection.

Practical Privacy Preserving Cloud Resource-Payment
207
The process of exchanging cloud credits for resources represents the most
challenging issue. How to consume them at the cloud provider without the cloud
provider being able to identify and track clients? The most straightforward non-
privacy friendly solution for client payment is to require clients to register with
the cloud provider and to have the cloud provider maintain an account for ev-
ery client. The provider then bills all consumed resources to this account. In
this scenario clients could pay for their consumed resources on a regular basis.
However, this does not satisfy our privacy demands and we want to achieve the
following privacy requirements for the payment process:
Anonymity. Clients do not want the cloud provider to be able to identify them.
Assuming that we have an anonymous channel between the client and the cloud
provider, this means that none of the operations Activate, Spend, SplitCredits and
Refresh must involve any information that can be used by the cloud provider to
identify a client. When we present our approach in Section 4 it will be clear that
anonymity solely depends on the underlying scheme for which it is shown in [20]
that anonymity is given.
Unlinkability. Clients do not want the cloud provider to be able to link diﬀerent
consumptions of resources (spending of credits), nor to discover how many credits
a client still possesses. This gives the client the guarantee that he is unobservable
from the cloud provider’s perspective. As in case of anonymity, the operations
Activate, Spend, SplitCredits and Refresh must be unlinkable and this follows from
the underlying scheme as shown in [20].
3
Background and Preliminaries
This section is devoted to building blocks and technologies that are employed
by our concrete scheme and deployment scenario. First we provide a compact
description of the underlying cryptographic scheme and present some abstrac-
tions used throughout the paper. We then provide a brief overview of trusted
platform security technologies, how they allow attestation of a platform’s state,
and how they support isolated, secured processing areas.
3.1
Anonymous Cloud Resource Scheme
In the following we represent all cloud credits (CCs) of a client as a single token,
i.e. one may think of a wallet of CCs, and a consumption of CCs from this token
can be thought of as spending some CCs out of the wallet.
We start by reviewing the anonymous yet authorized and bounded cloud re-
source scheme (AABCRS) introduced in [20], which is the most important build-
ing block for the work presented in this paper. The main idea behind such
schemes is that clients are able to purchase a contingent of credits for resources,
such as virtual machine instance hours, from a cloud provider (CP). While clients

208
M. Pirker, D. Slamanig, and J. Winter
spend their credits for resources at the CP, the CP does not learn anything about
the resource consumption behaviour of users. In particular, users can consume an
arbitrary number of their credits as long as there are still enough credits from
their purchased amount. Thereby, in any interaction with a client, the CP is
convinced whether a client is allowed to consume resources, but cannot identify
the client nor link any of the client’s actions and thus cannot ﬁgure out their
consumption patterns.
Below we present the deﬁnition of [20] and slightly modify it by discarding
one of the protocols (the Reclaim protocol). This modiﬁcation, however, has no
impact on the concrete scheme. Hence, an AABCRS is a tuple (ProviderSetup,
ObtainCredits, Consume) of polynomial time algorithms or protocols between
clients C and a cloud provider CP and works as follows:
– ProviderSetup. On input a security parameter k, this algorithms outputs
a private key sk and a public key pk of a suitable signature scheme and an
empty blacklist BL which is required for double-spending detection.
– ObtainCredits. In this protocol a client c wants to obtain a token t for L
credits (the credit limit) from the CP. The client’s output is a token t with
corresponding signature σt issued by CP. The token contains the credits L
and the up to now consumed credits s. These values may be represented by
a single value L′ := L −s within the token. Clearly, initially no credits have
been spent. The output of CP is a transcript TOL of the protocol.
– Consume. In this protocol a client c wants to consume n credits from the
credits still available in his token. The client shows a value id (a unique token
identiﬁer) of a token t and convinces the CP that he holds a valid signature
σt for token t and that t contains at least n credits. If the token was not
already shown in a previous run of a Consume protocol, i.e. t.id /∈BL, the
signature is valid and there are still enough credits available in the token,
i.e. s + n ≤L (or L′ −n ≥0 if both values are represented as a single one),
then c’s output is accept and an updated token t′ (with new id) for credit
limit L and up to now consumed credits s + n (or L′ −n if both values are
represented as a single one) with an updated signature σt′ from CP. Finally,
CP includes id into BL. Otherwise the user’s output is reject. The output
of CP is a transcript TC.
In [20] the author presents two variants of an AABCRS scheme based on the
pairing based Camenisch-Lysyanskaya (CL) signature scheme [7]. In both vari-
ants, a token t is represented as an ordered sequence of values, whereas the
number of elements depends on the variant. In the ﬁrst Variant (V1), a token
is of the form t = (C(id), C(s), L), whereas C(x) denotes an unconditionally
hiding commitment to value x. The value id represents a unique identiﬁer of
the token, s represents the number of CC’s that have been consumed up to now
and L represents the credit limit. The drawback of V1 is that L is included in
plain in the token and thus is always visible to the CP. Hence, in the worst case,
i.e. L is issued to exactly one client, the cloud provider can link actions of this
client. However, if the set of clients associated to the same value L is reasonable

Practical Privacy Preserving Cloud Resource-Payment
209
large, unlinkability is no longer a problem in a practical setting5. In the second
version (V2) even the value L is hidden from the cloud provider and a token is
of the form t = (C(id), C(s)). Here, s represents the number of CCs that are
still available from this token.
Intuition Behind V1. [20] We brieﬂy sketch the idea of V1 for simplicity,
whereas the modiﬁcations for V2 are straightforward (see [20]): The main intu-
ition of the construction is to let a client solely prove in each Consume protocol
that enough cloud credits are still available in a token. Therefore, the cloud
provider generates a key-pair (sk, pk) for the CL signature scheme, publishes pk
and initializes an empty blacklist BL. Then, a client obtains a CL signature σt
for a token t = (C(id), C(s), L), whereas initially no CCs s are consumed. Let
us assume that the client holds a token t = (C(id), C(s), L) and corresponding
signature σt. It is important to note, that id (a random token identiﬁer) and s
were signed as commitments and thus the CP is not aware of these values. If a
user wants to consume n CCs from his token, he computes a commitment C(id′)
representing the updated token identiﬁer for the updated token, randomizes the
signature σt to σ′
t (σt and σ′
t are then unlinkable) and proves in zero-knowledge
that σ′
t is a valid signature for id and L. This includes showing the values id
and L to the CP. Additionally, the client proves that the token includes an un-
known value s, which satisﬁes (s + n) ∈[0, L] or equivalently s ∈[0, L −n].
This proves convinces CP that at least n CCs are available from this token. If
id is not contained in BL and all proofs succeed, then the client is eligible to
consume n CCs. Consequently, the signature will be updated to a signature for
C(id + id′), C(s + n) and L in an interactive manner between the client and
CP. Subsequently, the CP adds id to BL and the client obtains an updated sig-
nature for an updated token t′ = (C(id + id′), C(s + n), L). This signature can
be randomized by the client (which makes t and t′ unlinkable) and used for the
next consumption of resources. Otherwise, CP will reject the client’s request to
consume n resources.
In the following we abstract from the details of the scheme, and use the
following high level parameters and state information respectively:
– The public parameters cpparamspub represent the public key pk of the CL
signature scheme and are public knowledge.
– The private parameters cpparamspriv represent the private key sk of the CL
signature scheme and the blacklist BL and are solely known to the cloud
provider.
– The state cstatepub represents the actual token-signature pair of the client.
– The state cstatepriv represents the values id, s, (L in V1) as well as the
randomizers for the commitments and the randomization factors of the CL
signature of the client’s token. The state information is updated during every
Consume operation, since only the current values are required.
5 Note that cloud resellers may sell cloud credits only in speciﬁc well-known amounts,
similar to cards for prepaid mobile phones, e.g. 5$, 10$, etc.

210
M. Pirker, D. Slamanig, and J. Winter
We note that it is not necessary to keep the token-signature pair cpparamspub
secret, since without knowing cstatepriv no one will be able to convince CP that
the signature σt is a valid signature for token t.
3.2
Trusted Platforms
In the last few years, mass-market computer platforms and devices have been
enhanced with functions dedicated to support advanced security. In the following
we give a short introduction to the features available in industry standard PCs
as well as mobile platforms.
Trusted Platform Modules. The concept of Trusted Computing as promoted
by the Trusted Computing Group (TCG) extends the industry standard PC ar-
chitecture with a specialised hardware component, the Trusted Platform Module
(TPM) [22]. A TPM features cryptographic primitives similar to a smartcard,
but is physically bound to its host platform.
An important concept of Trusted Computing is the measurement logging
and reporting of the platform state. Upon platform hardware reset a special
set of platform conﬁguration registers (PCRs) in the TPM are reset to a well
deﬁned start value. PCRs cannot be directly written to, rather, a PCR with
index i, i ≥0, in state n is extended with input x by setting PCRn+1
i
=
SHA-1(PCRn
i ||x). This enables the construction of a chain-of-trust. From the
BIOS onwards, every block of code is measured into a PCR before execution
control is passed to it. Thus, the current values in the set of PCRs represent a
log of what happend since system reboot, up to the current state of the system.
The current state may then be TPM signed with the TPM Quote operation and
reported in a so-called remote attestation protocol.
Intel Trusted Execution Technology. Recent platforms from Intel6 extend the
basic TCG model of a static chain-of-trust from hardware reboot and trust rooted
in early BIOS. They provide the option of a dynamic switch to a well-deﬁned, mea-
sured system state [16], meaning at any point of execution after platform reboot.
Consequently, this capability signiﬁcantly cuts down the complexity of the chain-
of-trust measurements to assess the platform state by excluding the early, messy
bootup operations, leading to a simpler and practical implementation.
ARM TrustZone. Many computing devices, especially in the embedded and
mobile domain, and more recently also in high-density data centers, do not use
x86 microprocessors, but are rather powered by a processor of the ARM fam-
ily. The ARM architecture follows a building-blocks approach, where the main
processor design is developed and controlled by one company. Individual ven-
dors select and license the intellectual property of the core and desired support
components as needed, and then enhance them with their own functional units
according to the needs of their customers.
6 We restrict our discussion to Intel’s Trusted Execution Technology (TXT) as this is
currently the dominant technology provider – comparable features are also available
on e.g. AMD platforms.

Practical Privacy Preserving Cloud Resource-Payment
211
The TrustZone architecture extension for ARM CPUs is an instruction set
extension for security critical scenarios. Basically, it provides a separation of
the memory resources and the CPU of a device, thereby creating two virtual
domains which are the so-called secure-world (SW) and normal-world (NW) [3].
This approach is an improvement to the basic concept of privileged/unprivileged
mode-split which can be found on many conventional architectures, including
earlier ARM cores.
The normal-world is the containment for user programs or any kind of un-
trusted applications. Security critical code is executed in the secure-world. The
isolation mechanisms of the TrustZone prohibit normal-world applications from
accessing the secure-world. Consequently, the data ﬂow between both worlds is
controlled by a secure monitor entity, which is under control of the secure-world.
The total memory available for software inside the TrustZone is vendor de-
pendent and ranges from 64 kBytes up to 256 kBytes on typical systems. This
size enables the running of a small – hopefully evaluated and certiﬁed – core in
the secure-world along with trusted executables.
Due to the ARM building-block approach there is no standardized way to
report the genuinity of the TrustZone implementation of a certain vendor. How-
ever, typically TrustZone implementors provide a symmetric device-key for de-
vice identiﬁcation and a asymmetric key for the purpose of secure boot. This
allows construction of hardware authentication similar to that implemented by
a TPM device and enforcement of a measured boot chain like with Intel TXT.
4
Practical Anonymous Payment
Building on the scenario, deﬁnitions and restrictions outlined in Section 2 we are
now ready to present our resource payment scheme in more detail. When we write
A(C(a1, . . . , an), CP(b1, . . . , bm)) we mean that operation A is run between entity
C with private inputs a1, . . . , an and entity CP with private inputs b1, . . . , bm.
All operations are conducted by the entities client (C), cloud provider (CP) and
credit reseller (CR). Furthermore, we will use the algorithms of an AABCRS as
deﬁned in section 3.1 as subroutines.
We note, that the Acquire operation can be conducted by several means. One
suitable and also privacy friendly scenario is to require the cloud provider to hand
over scratch-oﬀcards to the credit reseller. These scratch-oﬀcards are of diﬀerent
monetary denominations representing some equivalent of cloud credits (CCs). If
a user buys, e.g. with cash, such a card at a CR, he can scratch oﬀthe opaque
covering and a QR-Code is revealed. This QR-Code contains information about
the denomination, a serial number and potentially a validity period along with a
digital signature for those values. A client can scan this QR-Code with the built-
in camera of his smartphone7 and then holds all information necessary to conduct
an Activate operation with the CP. We denote the information which is necessary
7 The use of a mobile phone camera to provide a trusted import path for cryptographic
data was demonstrated viable in the Seeing-is-Believing eﬀort [17].

212
M. Pirker, D. Slamanig, and J. Winter
for the activation as paramsact subsequently. Note, that the aforementioned ap-
proach is advantageous from a privacy perspective, since CR does not learn the
serial number of the card and thus cannot link (in cooperation with the cloud
provider) the Acquire operation to the respective Activate operation.
Now, we present the remaining operations in a more formal manner, whereas
we assume for simplicity that the CP provides one type of resource and has
already conducted the ProviderSetup procedure. Furthermore, we denote by
NW the normal-world and by SW the secure-world of the client’s platform.
Activate(C(paramsact, cpparamspub), CP(cpparamspub,priv)): The NW sends
paramsact to the CP, who veriﬁes them for validity and returns true or false
to C. In case of true, C imports cpparamspub into SW and C’s SW runs an
ObtainLimit protocol for CC limit L (contained in paramsact) with CP. The
client ends up with storing cstatepub in the NW and cstatepriv in the SW. If
the CP returns false in the ﬁrst interaction, the operation terminates.
Spend(C(cstatepub,priv, n), CP(cpparamspub,priv)): The NW sends the number
n of desired CCs along with cstatepub to SW. If enough CCs are still available
(”in” the token) then SW runs a Consume protocol to consume n CCs with
CP, otherwise it returns false and the operation terminates. Thereby, CP
obtains a proof that cstatepub represents a valid token-signature pair, there
are still enough CCs available and the token id not already contained in
the blacklist BL. If any check fails, the operation terminates. If all checks
succeed, the SW updates cstatepriv and obtains an updated token-signature
pair cstatepub, which is stored in NW.
SplitCredits(C(cstatepub,priv, m), CP(cpparamspub,priv)): The
NW
sends m
along with cstatepub to SW. Let us assume that n > m whereas n repre-
sents the number of remaining CCs in cstatepriv for simplicity. Essentially,
the operation works identical to the Spend operation, but the m resources are
not consumed. Instead, an additional cstate′
pub is returned to C. At the end
of this operation C holds cstatepriv, cstate′
priv (in SW) as well as cstatepub
and cstate′
pub (in NW), whereas the former token represents n −m and the
latter m CCs.
Transfer(C1(cstatepub,priv), C2(·)): The SW of C1 exports cstatepriv and the
public and private state information cstatepub,priv are transferred to C2 who
imports cstatepriv into his SW and cstatepub into his NW. The SW of C1
deletes cstatepriv. Note that C1 transfers all n CCs represented by cstatepriv
to C2.
Refresh(C(cstatepub,priv), CP(cpparamspub,priv, cpparams′
pub,priv)): C’s NW
sends cstatepub to the SW and SW sends cstatepub along with cstatepriv (rep-
resenting n CCs) to CP. Now, CP can check whether cstatepub represents
a valid token-signature pair for n CCs. If this is true, CP engages in an
ObtainLimit protocol with respect to new parameters cpparams′
pub,priv with
C and issues a token for n CCs.

Practical Privacy Preserving Cloud Resource-Payment
213
We additionally observe the following:
SplitCredits: A client may have several motives to invoke a SplitCredits op-
eration. For instance, a client may want to ”split” oﬀsome CC’s from his
token to obtain a new token, in order to give one of the tokens to someone
else, e.g. as a gift. Another scenario is that a client pays n CCs for some
computation, but the computation actually only requires m < n CCs. Then,
after having conducted the computation, CP issues some kind of voucher in
form of a new token to C for n −m CCs.
Refresh: The version of the Refresh operation presented here is the simplest
one. Essentially, the client is issued fresh CCs with respect to new CL signa-
ture parameters, i.e. every validity period is represented by distinct signature
parameters. Note, that providing unlimited validity of tokens would not scale
well, since CP would have to store the entire blacklist for double-spending
detection. More ﬂexibility can be achieved if validity periods are encoded
directly into the tokens as it is proposed as an extension in [20].
Spend: For every Spend operation at least one CC is removed from circulation
and at least one new entry in the blacklist BL is required to prevent double-
spending. Naturally, the amount of CCs in circulation must be known to the
CP as he must be able to manage a blacklist. Consequently, the maximum
amount of credits issued is bound by the maximum size of the blacklist. The
policy of the CP must enforce a periodical Refresh operation by the clients
– in eﬀect accounting periods – to allow periodical clearing of the blacklist
(of the expired period).
5
Implementation
For practical evaluation we prototyped the core anonymous payment operations
of our approach on multiple software and hardware platforms. Some are good
approximations of the current generation of smartphones.
5.1
Speciﬁcations
On the software side, our scheme was implemented in the high-level language
Java and uses the jPBC 1.2.0 library8, a library for Pairing-Based Cryptography
(PBC) in Java. As an alternative to the pure Java implementation there is also
a C implementation of PBC9, which can be called from Java via a Java-to-C
wrapper. In the following we denote these two setups as Java “-J“ and Native C
accelerated ”-C“ variants. Our platforms ran either Linux (Li) or Android (An)
as operating system. The platforms used to measure execution speed were as
follows:
Pc* Laptop HP 8440p Elitebook, Intel i7M620 @2,67 Ghz, running Android for
x86 2.3.5 (RC1 20110828) of the Android x86 porting eﬀort [1], or Ubuntu
11.10 with IcedTea6 1.11pre (OpenJDK 64-Bit Server VM (build 20.0-b11)
8 http://gas.dia.unisa.it/projects/jpbc/
9 http://crypto.stanford.edu/pbc/

214
M. Pirker, D. Slamanig, and J. Winter
Ek* Freescale i.MX51 evaluation kit [15], Freescale MX515D @800Mhz10, run-
ning Android 2.3.4 (build R10.3.2 3), or Ubuntu 10.04 LTS with IcedTea6
1.8.10 (OpenJDK Zero VM (build 14.0-b16))
SpGs Smartphone Google Nexus S, Samsung Exynos 1 GHz (ARM Cortex-A8),
running Android 2.3.6.
SpS2 Smartphone Samsung Galaxy S2, Samsung Exynos 1.2 GHz dual-core
(ARM Cortex-A9), running Android 2.3.3.
5.2
Results
For performance evaluation we focused on the Activate and Spend operations
conducted by the client. This is due to the fact that the remaining protocols
only require negligible computational resources or are based on one of the two
aforementioned protocols, i.e. perform identically. The ﬁrst one being run only
once for initialisation of the credits token, the latter being run everytime credits
are spend at the CP. Table 1 shows the results from our implementation on the
platforms presented in Section 5.1. We measured from 4 to 16 bits for a practical
cloud credits limit of 24 = 16 to 216 = 65536 credits. The credits token is only
valid at one speciﬁc provider and these limits enable many basic use cases. A
larger limit is always possible, if the resulting additional computation time is
acceptable for the client.
Table 1. Execution time of Activate and Spend 4 bits to 16 bits [s]
PcLi-C PcAn-C PcLi-J SpS2-C SpGs-C EkAn-C
Activate
0.06
0.15
0.35
0.64
0.86
1.12
Spend 4 bits
0.16
0.35
0.82
1.43
1.94
2.54
Spend 16 bits
0.34
0.77
1.72
2.99
4.11
5.32
EkLi-C PcAn-J SpS2-J SpGs-J EkAn-J
EkLi-J
Activate
1.09
1.80
6.76
11.1
15.7
19.8
Spend 4 bits
2.53
3.60
13.3
20.9
29.6
39.7
Spend 16 bits
5.42
6.87
24.4
41.7
54.7
77.3
Figure 3 provides a more detailed analysis of the Spend protocol for tokens
containing 2x CCs. As can be seen from the ﬁgure, the time required for the
run of a Spend protocol grows linearly11 in the number of bits x of CCs in the
activated token, which is due to required zero-knowledge range proofs. We do
not provide explicit timings for computations of the cloud provider, since he
10 The processor on this board is based on ARM’s Cortex-A8 core and supports ad-
vanced security features such as ARM’s TrustZone and secure boot facilities. Un-
fortunately, most parts of the documentation is only available under NDA from
Freescale. For this prototype eﬀort this is suﬃcient.
11 With the non-linearities in the measurements caused by OS system services running
in the background.

Practical Privacy Preserving Cloud Resource-Payment
215
uses state-of-the-art servers and the computations are very eﬃcient (the most
expensive operation of the CP, i.e. Consume for a limit of 230 CCs, reported in
[20] takes about 1 second).






	

	




	



 

!"
!
!






	



	




	
	

	
	
	

	
Fig. 3. Spend execution speed, x-axis shows credits token limit L = 2x [bits]
Our results clearly show that the PC* versions dominate, the C versions take
full advantage of the raw processor power. We interpret the diﬀerence between
PcLi-C and PcAn-C due to the ﬁrst being 64bit and the second a 32bit platform.
They are closely followed by PcLi-J, a pure Java version executed by a server
JVM optimized over many years. The next 4 places are claimed by the remaining
C builds, as expected by their platform processor powers: 1.2GHz SpS2-C before
1.0GHz SpGs-C and 800MHz Ek*-C. The EkLi-C build runs almost identical to
EkAn-C. The Java versions trail, again as expected by their processor speed. For
the last platform, EkLi-J, the ARM JVM port appears to be quite unoptimized.
We must note that all Androids except PcAn-J and SpS2-J exhibited slight
process memory leakage during execution. Despite continuous garbage collector
runs memory usage grew. We assume this issue to add GC runtime overhead,
therefore their execution time should actually be a little lower. So far we have
been unable to determine the cause of this problem.
The main Java classes of our basic test code only consume 25 kB in size.
The supporting Java libraries (jpbc-api.jar, jpbc-plaf.jar, jpbc-pbc-jni.jar) require
371 kB. The native C support libs (libgmp.so, libjpbc-pbc.so, libpbc.so) sizes vary
according to the speciﬁc platform: Linux x86 64 772 kB, Android x86 32 688 kB,
Linux Arm 588 kB and Android Arm 800 kB.

216
M. Pirker, D. Slamanig, and J. Winter
5.3
Discussion
The results of our practical evaluation support the feasibility of our scheme.
While the Java numbers may appear to be very high at ﬁrst glance, the C core
accelerated versions are multiple times faster. To achieve this, ﬁrst we replaced
the default JNA wrapper for the Java-to-C bridge with our own custom JNI
coded wrapper to allow the C code to be accessible on Android. Second, the
ARM code was compiled to take advantage of ARM processor Neon SIMD and
ARMv7 instructions features12. Consequently, an optimized, C core enhanced,
Android build runs fast enough on current generation smartphones to allow
Spend(ing) operations without long delays for end users.
Our prototype uses oﬀ-the-shelf libraries for crytography which were not op-
timised at all as initial evaluation of execution speed was our ﬁrst objective.
Consequently, our test code plus support libraries and JavaVM are way too
large to ﬁt into a TrustZone environment (Section 3.2). However, we expect a
standalone implementation of the algorithm along with a small JavaVM to ﬁt
into a small TrustZone environment. This remains future work.
6
Discussion
In the following we reﬂect on supplemental aspects which were not discussed
during the presentation of our scheme or our scenario.
Trusted Computing Attestation. In Section 2.2 we use Trusted Execution
Technology to attest the cloud servers provided to the client. This technology
is mass-market available and TXT integration has already been demonstrated
for Linux based servers [21]. Thus, if the software image to be rented to the
client is agreed upon, a remote client can ask for a remote attestation proof
(TPM Quote) from the server to conﬁrm what speciﬁc software image was actu-
ally booted. Attacks of the TXT components require physical intervention [25]
and consequently raise the bar for manipulation13.
The execution of multiple parallel running client VMs on a cloud server and
subsequent attestation of the security of the system is an active topic of Trusted
Computing research. The current TPM chip generation was not designed for this
use case, an updated revision of the TPM hardware “V2” is expected to make
this use case practical.
Identiﬁcation at Credits Purchase. An obvious privacy problem is a poten-
tial camera at the reseller’s place, which may collect a photo of the client’s face.
This threat is mitigated with the possibility of scratch-oﬀcards and the ability
to Transfer credits between trustworthy friends.
Isolated Secure World. By deﬁnition computation in the SW is isolated from
the rest of the platform and only reachable via a well-deﬁned, narrow interface.
12 GCC CFLAGS=”-march=armv7-a -mﬂoat-abi=softfp -mfpu=neon“
13 Or good old runtime software bugs which allow priviledge escalation.

Practical Privacy Preserving Cloud Resource-Payment
217
Consequently, an obvious problem is to decide from inside the SW whether a
request from the NW is authorized – or not. This problem is non-trivial, but one
solution would be to require a trusted input and display path which provides
user-interaction (e.g. PIN entry) if explicit authorization for sensitive operations
in the SW is required. We assume that users are acting in their own best interests
when using their own smartphones, which is reasonable. Nevertheless, this still
leaves the problem of potential malware on the client’s platform which would be
able to circumvent this feature. Note that this isolation also impacts the privacy
provided by our network connection, as the Tor connection is currently anchored
in the NW. Thus, how could the limited SW verify whether an anonymizing
network connection to the CP is properly used?
Honesty of Clients. If two clients exchange cloud credits by means of a Transfer
operation, say C1 gives n CCs to C2, then C2 has no means to verify whether
C1 has properly deleted the transferred credits. Essentially, if C1 is dishonest
the ﬁrst-come-ﬁrst-served principle applies and whoever is the ﬁrst one to spend
credits from the token will be able to continue spending. We may, however,
assume that only clients who trust each other exchange CCs, in which case this
is not a problem. Furthermore, from the perspective of CP, even if C1 does not
delete his CCs, then this does not mean any harm to the CP, since only n CCs
can be consumed in total and clients cannot create ”extra” credits.
Forward Secrecy. What happens when a smartphone is stolen, lost or seized?
If Spend is not protected by additional secret information, e.g., a PIN, then
someone in possession of the smartphone is able to spend all credits left within
the currently activated token. Nevertheless, we note that even if the adversary
is able to extract information from the secure world, he will not be able to link
previous actions of the smartphone’s holder – since no ”history data” of the
randomization processes of the underlying scheme is stored.
7
Related Work
To the best of our knowledge an approach related to the one presented in this
paper has not been considered before. Nevertheless, there are three lines of work
whose combination leads to the kind of work presented here. We will brieﬂy
elaborate on this below.
Privacy in Trustworthy Mobile Platforms. With the growing popularity of
mobile computing, their ubiquitous application and the resulting privacy issues,
there arises the necessity to provide functionalities of traditional privacy en-
hancing cryptographic protocols. However, due to limited storage capacities and
processing power this task is non trivial and requires clever design. Recent works
include the design of anonymous authentication for mobile devices by modifying
direct anonymous attestation (DAA) and using hardware security features to
prevent copying and sharing of private credentials [23]. Another implementation
of DAA on mobile platforms with TPMs is presented in [12].

218
M. Pirker, D. Slamanig, and J. Winter
Privacy in Cloud Computing. Privacy is considered as one of the main is-
sues in cloud computing. Besides known problems regarding user’s privacy in
traditional web applications, additional aspects imposed by the heavy use of
virtualization seem to be novelties. For instance, sharing of resources among dif-
ferent users may potentially lead to the construction of covert or side channels
which allows to infer activity patterns of other users (cf. [9]). User’s access pat-
terns represent privacy sensitive information that should also be hidden from the
cloud. Recent works are mainly focusing on storing and sharing data in the cloud.
In [14] an oblivious RAM (ORAM) based approach is presented, which allows
users to outsource a set of data items to the cloud and provide read and write
permission to users as follows. Users can only access (and read in plain) data
items when accurate permissions were obtained and can learn nothing about
other items. Additionally, users and even the cloud provider observing all ac-
cesses cannot infer which user is accessing which data items how often. Another
approach based on dynamic accumulators was recently proposed in [19]. Here,
no ORAM is employed and thus the cloud (and other users) may learn which
data items are accessed, but each access is anonymous and unlinkable to each
other. Hence, usage patterns of users can also not be inferred. Independent of
the latter approach [18] also proposed a discretionary access control model for
data outsourced in the cloud which hides access patterns.
Anonymous Payments. Concepts for anonymous and untraceable electronic
payments are around for quite a long time [8]. While these ﬁrst schemes were
based on blind signatures and the cut-and-choose paradigm, over the years sev-
eral improvements, especially for oﬀ-line e-cash, e.g. compact e-cash [5] or divis-
ible e-cash [4], allowing to spend 2n coins from a ”single coin” accumulating all
coins, have been proposed. Recently, the use of anonymous payments for overlay
networks like Tor [2,10], which use lightweight payment protocols for micropay-
ments, have been proposed. Unlike all aforementioned schemes, which assume a
bank, a set of payees and a set of payers, in our scenario we have one bank and
one payee represented as the same entity. Thus, we do not need to employ oﬄine
schemes, but use a kind of online payment scheme. Since we do not need proper-
ties of e-cash schemes such as double-spender identiﬁcation as well as spending
with arbitrary payees (this usually adds a non trivial computational overhead),
we employ a scheme tailored to payment for cloud resources in our work.
8
Conclusion and Outlook
In this paper we present a privacy preserving cloud resource payment scheme
for resource constrained mobile devices such as smartphones. We discuss a con-
crete scenario for a setting which includes clients, credit resellers and a cloud
provider. The client and cloud provider take advantage of the state-of-the-art
security enhanced TrustZone and TXT hardware platforms. Besides theoretical
considerations, we also prototype the core operations on state-of-the-art plat-
forms. Our results suggest that an optimized C implementation of our scheme is
already fast enough for deployment on the current generation of smartphones.

Practical Privacy Preserving Cloud Resource-Payment
219
Future work includes the use of an instantiation of an AABCRS based on
the strong RSA version of the CL signature scheme [6], which should provide a
signiﬁcant performance boost. Other interesting aspects are the extension of the
scenario to multiple cloud providers such that credits can be spend at diﬀerent
CPs (potentially including a ”bank” as within traditional payment systems)
and the consideration of alternative (more eﬃcient) payment mechanisms as for
instance proposed in [10]. The problem of practically realizing such scenarios
with smartphone secure processing and secure data storage technologies remain
an active area of research.
Acknowledgements. We thank the anonymous reviewers for their helpful feed-
back on the paper. In particular we thank Thomas S. Benjamin for his many
suggestions for improving this paper. This work has been supported by the
European Commission through project FP7-SEPIA, grant agreement number
257433. The second author has been supported by an internal grant (zentrale
Forschungsf¨orderung – ZFF) of the Carinthia University of Applied Sciences.
References
1. Android
x86
Team:
Android-x86
-
porting
android
to
x86
(2011),
http://www.android-x86.org/
2. Androulaki, E., Raykova, M., Srivatsan, S., Stavrou, A., Bellovin, S.M.: PAR: Pay-
ment for Anonymous Routing. In: Borisov, N., Goldberg, I. (eds.) PETS 2008.
LNCS, vol. 5134, pp. 219–236. Springer, Heidelberg (2008)
3. ARM Ltd.: TrustZone Technology Overview (2011),
http://www.arm.com/products/esd/trustzone_home.html
4. Au, M.H., Susilo, W., Mu, Y.: Practical Anonymous Divisible E-Cash from
Bounded Accumulators. In: Tsudik, G. (ed.) FC 2008. LNCS, vol. 5143, pp. 287–
301. Springer, Heidelberg (2008)
5. Camenisch, J.L., Hohenberger, S., Lysyanskaya, A.: Compact E-Cash. In: Cramer,
R. (ed.) EUROCRYPT 2005. LNCS, vol. 3494, pp. 302–321. Springer, Heidelberg
(2005)
6. Camenisch, J.L., Lysyanskaya, A.: A Signature Scheme with Eﬃcient Protocols.
In: Cimato, S., Galdi, C., Persiano, G. (eds.) SCN 2002. LNCS, vol. 2576, pp.
268–289. Springer, Heidelberg (2003)
7. Camenisch, J.L., Lysyanskaya, A.: Signature Schemes and Anonymous Credentials
from Bilinear Maps. In: Franklin, M. (ed.) CRYPTO 2004. LNCS, vol. 3152, pp.
56–72. Springer, Heidelberg (2004)
8. Chaum, D.: Blind signatures for untraceable payments. In: CRYPTO, pp. 199–203.
Plenum Press (1982)
9. Chen, Y., Paxson, V., Katz, R.H.: What’s New About Cloud Computing Security?
Tech. Rep. UCB/EECS-2010-5, University of California, Berkeley (2010)
10. Chen, Y., Sion, R., Carbunar, B.: XPay: Practical Anonymous Payments for Tor
Routing and other Networked Services. In: WPES, pp. 41–50. ACM (2009)
11. Dierks, T., Rescorla, E.: The Transport Layer Security (TLS) Protocol Version 1.2.
RFC 5246, IETF (2008), http://tools.ietf.org/html/rfc5246

220
M. Pirker, D. Slamanig, and J. Winter
12. Dietrich, K., Winter, J., Luzhnica, G., Podesser, S.: Implementation Aspects of
Anonymous Credential Systems for Mobile Trusted Platforms. In: De Decker, B.,
Lapon, J., Naessens, V., Uhl, A. (eds.) CMS 2011. LNCS, vol. 7025, pp. 45–58.
Springer, Heidelberg (2011)
13. Dingledine, R., Mathewson, N., Syverson, P.F.: Tor: The Second-Generation Onion
Router. In: USENIX Security Symposium, pp. 303–320 (2004)
14. Franz, M., Williams, P., Carbunar, B., Katzenbeisser, S., Peter, A., Sion, R., So-
takova, M.: Oblivious Outsourced Storage with Delegation. In: Danezis, G. (ed.)
FC 2011. LNCS, vol. 7035, pp. 127–140. Springer, Heidelberg (2012)
15. Freescale Semiconductor Inc.: i.MX51 evaluation kit (2010),
http://www.freescale.com/webapp/sps/site/
prod summary.jsp?code=MCIMX51EVKJ
16. Grawrock, D.: Dynamics of a Trusted Platform: A Building Block Approach. Intel
Press (2009)
17. McCune, J.M., Perrig, A., Reiter, M.K.: Seeing-Is-Believing: Using Camera Phones
for Human-Veriﬁable Authentication. In: IEEE Symposium on Security and Pri-
vacy (2005)
18. Raykova, M., Zhao, H., Bellovin, S.: Privacy Enhanced Access Control for Out-
sourced Data Sharing. In: Financial Cryptography and Data Security. LNCS.
Springer (2012)
19. Slamanig, D.: Dynamic Accumulator based Discretionary Access Control for Out-
sourced Storage with Unlinkable Access. In: Financial Cryptography and Data
Security. Springer (2012)
20. Slamanig, D.: Eﬃcient Schemes for Anonymous Yet Authorized and Bounded Use
of Cloud Resources. In: Miri, A., Vaudenay, S. (eds.) SAC 2011. LNCS, vol. 7118,
pp. 73–91. Springer, Heidelberg (2012)
21. Toegl, R., Pirker, M., Gissing, M.: acTvSM: A Dynamic Virtualization Platform
for Enforcement of Application Integrity. In: Chen, L., Yung, M. (eds.) INTRUST
2010. LNCS, vol. 6802, pp. 326–345. Springer, Heidelberg (2011)
22. Trusted Computing Group: TCG TPM Speciﬁcation Version 1.2 (2007),
https://www.trustedcomputinggroup.org/developers/
23. Wachsmann, C., Chen, L., Dietrich, K., L¨ohr, H., Sadeghi, A.-R., Winter, J.:
Lightweight Anonymous Authentication with TLS and DAA for Embedded Mo-
bile Devices. In: Burmester, M., Tsudik, G., Magliveras, S., Ili´c, I. (eds.) ISC 2010.
LNCS, vol. 6531, pp. 84–98. Springer, Heidelberg (2011)
24. Wiegele, P., Winter, J., Pirker, M., Toegl, R.: A ﬂexible software development and
emulation framework for ARM TrustZone. In: Proceedings of The Third Interna-
tional Conference on Trusted Systems (INTRUST 2011). Springer (2012)
25. Winter, J., Dietrich, K.: A Hijacker’s Guide to the LPC Bus. In: Petkova-Nikova,
S., Pashalidis, A., Pernul, G. (eds.) EuroPKI 2011. LNCS, vol. 7163, pp. 176–193.
Springer, Heidelberg (2012)

Fault-Tolerant Privacy-Preserving Statistics
Marek Jawurek and Florian Kerschbaum
SAP Research
Karlsruhe, Germany
{marek.jawurek,florian.kerschbaum}@sap.com
Abstract. Real-time statistics on smart meter consumption data must
preserve consumer privacy and tolerate smart meter failures. Existing
protocols for this private distributed aggregation model suﬀer from vari-
ous drawbacks that disqualify them for application in the smart energy
grid. Either they are not fault-tolerant or if they are, then they require bi-
directional communication or their accuracy decreases with an increasing
number of failures. In this paper, we provide a protocol that ﬁxes these
problems and furthermore, supports a wider range of exchangeable statis-
tical functions and requires no group key management. A key-managing
authority ensures the secure evaluation of authorized functions on fresh
data items using logical time and a custom zero-knowledge proof pro-
viding diﬀerential privacy for an unbounded number of statistics calcu-
lations. Our privacy-preserving protocol provides all the properties that
make it suitable for use in the smart energy grid.
Keywords: Privacy, Smart Grid, Statistics, Aggregation, Stream, Fault-
Tolerance.
1
Introduction
In the smart energy grid there is a conﬂict between privacy of consumers and
utility for service providers. Utility providers can use real-time household elec-
tricity consumption data for forecasting future consumption. This consumption
forecasting allows them more eﬃcient and more stable operation of the electric-
ity grid. However, real-time consumption data also closely reﬂects any activity
in the household involving electrical appliances. Thus, for the consumer, it rep-
resents a privacy invasion [10,11]. Previous studies [8,9,13,15,16,17,19,22] have
shown that and how information about a household and its inhabitants can be
inferred from its high-resolution energy consumption data. Furthermore, any
viable solution for forecasting consumption must also anticipate failing smart
meters or communication links. A single failure must not prevent the real-time
calculation of statistics.
The ability to calculate statistics in real-time, i.e., in the presence of failures,
can also beneﬁt many other real-world applications like public health and clinical
research on patient information or any collection and monitoring where privacy-
sensitive data is processed.
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 221–238, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

222
M. Jawurek and F. Kerschbaum
In this paper we provide a protocol in the fault-tolerant, private distributed
aggregation model: Many data producers (e.g. smart meters) constantly produce
sensitive data items (e.g. hourly smart meter consumption measurements). An
untrusted data consumer (e.g. service provider) calculates statistics, e.g., for
forecasting future consumption, over subsets of these data items in the presence
of failing data producers. Not even a collusion of malicious data producers and
consumer may lead to the disclosure of sensitive data items.
Our protocol roughly works as follows: We use homomorphic encryption for
aggregation and employ an (w.r.t. privacy) untrusted, possibly distributed key-
managing authority that provides diﬀerentially private decryption services to
the data consumer while neither learning data items nor statistics results.
Recently, [2] also presented a protocol for this model. Without fault-tolerance
it has been considered in [23,25]. In comparison to these existing protocols our
contributions can be summarized as follows:
– The accuracy of the calculated statistics is higher. In our protocol the ac-
curacy is independent of the number of data producers, the number of data
items and the number of failures.
– We do not require synchronized clocks, but only rely on logical time.
– Our protocol enables the calculation of a wider range of statistical functions
(weighted sums). The statistical function can be chosen and exchanged inter-
mittently by the data consumer without notiﬁcation to the data producers.
– We do not require any group key management. Data producers may join or
leave without interaction with other participants.
– We only require uni-directional communication channels between data pro-
ducers and data consumers. This implies a reduced attack surface of the
smart meter.
The remainder of this paper is structured as follows: Section 2 describes the
contributions of our protocol in comparison to [2,23,25]. Section 3 introduces the
prerequisites used in our protocol. In Section 4 we introduce a naive version of
our protocol to achieve diﬀerentially private, fault-tolerant statistics in the semi-
honest model. Then, in Section 5 we present the ﬁnal protocol and our custom
zero-knowledge proof for the malicious model. Finally, we present related work
(Section 6) and conclude with a summary in Section 7.
2
Contributions
We compare our protocol to the protocols in [2,23,25]. We favorably compare
in existing criteria, but also extend their set of criteria. Our extended Table 1
illustrates the diﬀerences.
2.1
Communication
We assume that many (unsynchronized) data producers constantly produce sen-
sitive data items. They send these items to a data consumer over a uni-directional

Fault-Tolerant Privacy-Preserving Statistics
223
Table 1. DP: diﬀerential privacy CDP: Computational diﬀerential privacy AO: Aggre-
gator Obliviousness C →S: client-to-server uni-directional C ⇔S: interactive between
client and server
Scheme
Avg
comm.
per user
Comm.
model
Error
Fault-
tolerant
Group key
management
required
Syn-
chr.
clocks
Secu-
rity
model
Naive DP
O(1)
C →S
O(√n)
Yes
No
No
DP
[23]
O(1)
C ⇔S
O(1)
No
Yes
Yes
CDP
AO
[25]
O(1)
C →S
O(1)
No
Yes
Yes
CDP
AO
[2] Sampling O(
1
φ2∗n)
C ⇔S
O(φ ∗n)
Yes
Yes
Yes
DP
[2] Binary
O(log n)
C →S ˜O((log n)
3
2 )
Yes
Yes
Yes
CDP
this paper
O(1)
C →S
O(1)
Yes
No
No
DP AO
communication channel. Every data producer has constant communication cost
per data item. The data consumer queries the key-managing authority for de-
cryption over a bi-directional channel. Thus, the communication cost for the
key-managing authority is linear in the number of calculated statistics.
[23] requires bi-directional communication between data producers (users) and
the data consumer (aggregator) so that the users can cooperate in decrypting the
threshold decryption system. [25] only requires uni-directional communication
links between data producers and the consumer and has the same communication
cost as our scheme. In [2] data producers also communicate uni-directionally with
data consumers but have higher communication cost. This is due to redundant
information provided by data producers for fault-tolerance.
2.2
Accuracy
In our protocol the accuracy of the data consumer’s calculated statistics is in-
dependent of the number of data producers or data producer failures. The only
error in accuracy is introduced deliberately to ensure diﬀerential privacy and
that is O(1) with respect to the number of data producers or failures.
[25] and [23] also introduce O(1) error for diﬀerential privacy while [2] intro-
duces polylogarithmic error dependent on the number of data producers.
2.3
Fault-Tolerance
Fault-tolerance in our protocol is introduced by a selection process of the data
consumer. The data consumer can arbitrarily select a subset of available data
items as input to the statistics calculation. Consequently, our scheme tolerates an
arbitrary and unbounded number of failing data producers. The key-managing
authority can also be distributed (as we describe in Section 4.3) and we only
require a majority of key-managing authority instances to be available during a
run of the protocol.

224
M. Jawurek and F. Kerschbaum
[25] and [23] do not tolerate any failures of data producers (users). The former
relies on blinding shares of zero which requires all shares to be present. The latter
requires the data producers to participate in a threshold decryption of the ﬁnal
result. This oﬀers some fault-tolerance, but the accuracy of the result degrades
with failures. [2] oﬀers fault-tolerance, but the error of the results grows sub-
linearly in the number of absent users. A bound on the maximum number of
failing users needs to be pre-arranged.
2.4
Group Key Management
In our protocol, there is no group or elaborate key management. Data producers
may join or leave independently at any time without any new key distribution
or setup phase.
[25] requires a new distribution of the blinding shares for all data producers
whenever a new producer joins. [23] requires a new distribution of key shares for
the threshold decryption. [2] introduce a dynamic join & leave protocol which
tolerates joining of data producers up to a certain limit. Beyond, they need to
increase the tree height to accommodate more data producers. In such a case,
the trusted dealer needs to distribute one additional secret key to existing users
and O(log n) secret keys to new users.
2.5
Synchronization
Our protocol does not require synchronized clocks. We use logical time in order
to chronologically order and thus prevent re-use of data items. All data items
are encrypted under the same key.
All protocols [2,23,25] depend on communication rounds and require synchro-
nization, although they do not explicitly mention this.
2.6
Security
Our scheme oﬀers aggregator obliviousness (AO), i.e., the aggregator will not
learn anything else but the ﬁnal result. Input data and intermediate results are
not available to him. Furthermore, we ensure diﬀerential privacy (DP) [4] for
statistics.
Although [23,25] provide aggregator obliviousness they only guarantee the
weaker notion of computationally diﬀerential privacy (CDP). In these proto-
cols the diﬀerential privacy has to be ensured by the data producers while in
our scheme the key-managing authority ensures this. [2] oﬀers protocols with
diﬀerential privacy and computational diﬀerential privacy, but does not ensure
aggregator obliviousness. The data consumer (aggregator) learns some interme-
diate values (even if there is no fault) in order to calculate the result in the
presence of failures.

Fault-Tolerant Privacy-Preserving Statistics
225
3
Prerequisites
3.1
Diﬀerential Privacy
The deﬁnition of diﬀerential privacy according to [4] is the following:
Deﬁnition 1. A randomized function K gives ϵ-differential privacy if, for all
data sets D1 and D2 diﬀering on at most one element and all S ⊂Range(K),
Pr[K(D1) ∈S] ≤exp(ϵ) · Pr[K(D2) ∈S]
It means, that the probability for any result of K changes only slightly (less
than exp(ϵ) if single elements are included/excluded in the set of inputs to K.
In consequence, even with knowledge of the data set, the function result and
arbitrary auxiliary information, it is hard for an attacker to identify whether an
element is present or not. Thus, also the actual value of the element is protected.
Applied to smart metering that means, that if every statistics function on
smart meter consumption data is diﬀerentially private, the individual readings
can not be recovered by the receiver of the statistics’s results, e.g. the service
provider. Thus, diﬀerentially private statistics functions protect consumer pri-
vacy.
We will transform any function f into an ϵ-diﬀerential private version of
itself by adding random noise according to a symmetric geometric distribu-
tion [6]. Speciﬁcally, if δ is the f’s sensitivity, we add a sample from distribution
Geom(exp( ϵ
δ )) to each function result f(x) to make it diﬀerentially private. The
parameter ϵ must be chosen according to the use case at hand. It represents the
desired trade oﬀbetween accuracy of the function K and how well it preserves
privacy.
3.2
Paillier Cryptosystem
We only give a basic introduction to the Paillier cryptosystem, further informa-
tion – including security proofs – can be found in [20]. This cryptosystem deﬁnes
two functions:
– E(m, r) →c, encrypts a message m ∈Zn with random value r ∈Z∗
n to the
ciphertext c ∈Z∗
n2. All encryptions in our protocol are performed by data
producers and use the key-managing authority’s public key.
– D(c) →(Dv(c), Dr(c)) →(m, r), decrypts ciphertext c to a tuple (m,r). In
our protocol, all decryptions are performed by the key-managing authority
using its private key.
We use this cryptosystem’s ability to also recover the random parameter r during
decryption.
The Paillier cryptosystem also has the following homomorphic properties:
D(E(m1, r1)E(m2, r2)
mod n2) = (m1 + m2, r1 · r2)
mod n
(1)
D(E(m, r)k
mod n2) = (km, rk)
mod n
(2)

226
M. Jawurek and F. Kerschbaum
In the following, whenever we refer to the encryption E or the decryption
function D applied to either singular values or vectors of ciphertexts it yields
what makes most sense in the respective context: Singular values are encrypted
(decrypted) to singular values and a vector of values is encrypted (decrypted)
to a vector of values. The function X ∪v appends the scalar value v to the
vector X. Also, for improved readability, we write f ◦h(X) for the sequential
composition of functions f and h.
4
Protocol Description
In this Section we ﬁrst give a description (Section 4.1) of a naive version of our
fault-tolerant, diﬀerentially private statistics protocol. It provides diﬀerential
privacy for a semi-honest data consumer, but may fail in case of a malicious
data consumer. Then, in Section 4.2 we analyze this deﬁciency and explain the
necessary restrictions that we must employ, namely the freshness of used data
items and correctness of computation.
Finally, in the next Section 5, we present the full protocol with a focus on
our custom zero-knowledge proof that provides fault-tolerant diﬀerential privacy
in the presence of malicious data consumers with a distributed key-managing
authority.
4.1
Naive Protocol
We restrict ourselves, without loss of generality, to one round of communica-
tion from data producers to the data consumer and one subsequent function
evaluation and decryption. In the general case, the protocol step Preparation
i.e., the creation of data items, is executed repeatedly in parallel and unsyn-
chronized to the protocol steps that implement calculation and the decryption
(Calculation and Decryption). Therefore, in the general case, in protocol step
Calculation the data consumer could choose among all input values that he has
received during the entire system run time, i.e. multiple rounds of data item
creation. Furthermore, also without loss of generality, we assume, that data con-
sumer and key-managing authority have pre-arranged a function f that the data
consumer wishes to evaluate. We also assume, that in a setup phase all data pro-
ducers obtain the public key that corresponds to the private key only held by
the key-managing authority.
Preparation: Every data producer j encrypts its value vj with a random num-
ber rj. Every data producer sends E(vj, rj) to the data consumer.
Calculation: The data consumer now chooses a vector
V = (E(v1, r1), . . . , E(vm, rm))
out of all encrypted data items that it received and has never used before.
The data consumer calculates any statistics function of the form
f((x1, . . . , xm), c) = (
m

i=1
ai · xi) + c

Fault-Tolerant Privacy-Preserving Statistics
227
by evaluating f’s homomorphic counterpart
fh(V , E(c, rc = 1)) = (

i=1
E(vi, ri)ai) · E(c, rc)
fh is derived from f using the homomorphic relationship (see Equations 1, 2),
the ai are constants and c is a data consumer-chosen variable.
Then, the data consumer sends fh(V , E(c, 1)) to the key-managing au-
thority for decryption to receive the plaintext of the statistics result.
Decryption: The key-managing authority decrypts fh(V , E(c, 1)) which yields
(according to Equations 1,2):
Dv ◦fh(V , E(c, 1)) = f ◦Dv(V , E(c, 1))
Note that the data consumer can use its addend c to prevent the disclosure
of the statistics result to the key-managing authority.
Then, it applies the function mf(s, x) which adds a random sample ac-
cording to the sensitivity of f (see Section 3.1) and a seed s to the decrypted
function result in order to make the statistics function evaluation diﬀeren-
tially private. Finally, it returns the diﬀerentially private statistics result’s
plaintext: mf(s) ◦Dv ◦fh(V , E(c, 1))
4.2
Malicious Behavior
The naive protocol version described in Section 4.1 will provide diﬀerential pri-
vacy in the semi-honest model [7]. We assume that the data consumer abides
by the protocol and computes the function as agreed with the key-managing
authority. This includes that he only chooses data items that he has never used
before. However, this freshness property of the data items is not ensured by the
protocol. Thus, in the presence of malicious data consumers, the naive protocol
cannot protect data items from re-use by the data consumer.
The re-use of data items has severe implications for diﬀerential privacy of
continuous statistics calculation: It is easy (see Section 3.1) to determine the
necessary parameter for the distribution of the random noise that makes a sin-
gle function evaluation ϵ-diﬀerentially private. In [18] McSherry has analyzed
how the diﬀerential privacy of combined function evaluations (queries) over in-
tersecting data sets relates to the diﬀerential privacy of the individual functions:
Let a function fi provide ϵi-diﬀerential privacy. If all functions fi cover disjoint
subsets, the diﬀerential privacy for the combined function is maxiϵi. However,
if several functions span the same set of values, the diﬀerential privacy for the
combination of those functions is 
i ϵi.
Thus, if we do not prevent the re-use of data items, the diﬀerential privacy
will add up. Eventually, it will be insuﬃcient for the protection of data items.
In the PINQ framework [18] the aggregate knowledge gain of the attacker, i.e.
the sum of diﬀerential privacy over all combined functions, is limited. This is
achieved by giving every user a speciﬁc, upfront budget of diﬀerential privacy.

228
M. Jawurek and F. Kerschbaum
Once this budget is used up, no more statistics calculations (and decryptions)
are allowed.
This approach is incompatible with continuous calculation of statistics. In
the smart meter statistics context but also for other applications the number of
statistics calculations must be unbounded. Thus, in order to limit the aggregate
knowledge gain of an attacker we have to prohibit the re-use of data items and
thus make it equivalent to calculations over disjoint subsets.
In order to turn the naive protocol into a secure protocol in the malicious
model, the data consumer must prove to the key-managing authority that it
used fresh data items and that it correctly evaluated a known statistics function.
The evaluated statistics function f must be part of the data consumer’s proof,
because f’s sensitivity determines the variance of the random noise in diﬀerential
privacy (see Section 3.1).
However, Goldreich’s compiler approach [7] to turn the naive protocol se-
cure in the malicious model would not be feasible: The resulting generic zero-
knowledge proof (ZKP) would be prohibitively costly spanning the entire history
of data items.
Therefore, we subdivide the proof in two parts: The ﬁrst part covers the fault-
tolerance of the calculated statistics, i.e., the choice of k fresh data items out of
n total data items. We model this choice with the function freshk(n). Section 4.3
describes how a key-managing authority can guarantee this freshness by keeping
state. Furthermore, in Section 4.4, we implement a distributed key-managing
authority to make it resilient to failures.
The second part of the proof covers the the evaluated statistics function f. In
Section 5 we present the ﬁnal protocol that employs a custom zero-knowledge
proof that covers the evaluated statistics function and completes the freshness
guarantee.
Thus, in total, the data consumer proves to the key-managing authority with
both parts that it honestly evaluated f ◦freshk(n).
4.3
Freshness of Data Items
As identiﬁed in Section 4.2 the key-managing authority must only allow decryp-
tion of function results that incorporate fresh data items.
This can only be accomplished by keeping state with the key-managing au-
thority. The main challenges with keeping state are:
1. Keep the state as small as possible and not dependent on the number of
decryption requests or the frequency of data item creation.
2. Allow for a distributed key-managing authority which makes the protocol
resilient to failure of the key-managing authority.
In order to keep the state at the key-managing authority manageable we further
restrict the freshness property: Data items originating from the same data pro-
ducer can only be used in chronological order. Items may be skipped however.
The function Pj(v) at data producer j returns the position (logical time) of data
item v within the total order of data items of data producer j.

Fault-Tolerant Privacy-Preserving Statistics
229
The key-managing authority remembers for every data producer which data
item was used last in any statistics calculation. At the key-managing authority,
the function C(j) returns for data producer j the position (logical time) of the
latest data item that was used in a statistics calculation in j’s total order of data
items.
Preparation: In addition to the naive protocol, every data producer j also
sends Pj(vj) to the data consumer.
Decryption Request: Let pi denote the data producer, then the data con-
sumer compiles data item information I about the input values V (see the
naive protocol) he used for the current statistics calculation:
I = ((p0, P0(v0), . . . , (pm, Pm(vm)))
We provide integrity protection for I using signatures in Section 5.
The data consumer sends a decryption request with the encrypted statis-
tics result fh(V , E(c, 1)) and I to the key-managing authority.
Validation of Freshness: The key-managing authority veriﬁes the freshness of
every used data item: For every (pj, Pj(vj)) ∈I it checks whether C(pj) <
Pj(vj). If successful, C() is updated so that: C(pj) ←Pj(vj) and the key-
managing authority proceeds with the decryption and the addition of random
noise as in the naive protocol.
4.4
Distributed Key-Managing Authorities
It may be desirable to distribute the key-managing authority to make it resilient
against failure.
However, a key-managing authority like described in the previous Section 4.3
cannot be simply split into several instances because of its state C(). A malicious
data consumer could send decryption requests over intersecting subsets of data
items to diﬀerent instances of the key-managing authority. These instances would
be unable to guarantee freshness of data items which would break our measures
for ensuring diﬀerential privacy (as explained in Section 4.2).
Communication between diﬀerent instances, in order to synchronize state,
would also be prohibitively costly: Upon every decryption request an instance
would have to query all other instances for the current state of their function C.
We also would have to cover merging of diﬀerent functions C and how to cope
with failed instances.
Therefore, we propose the following augmented protocol for a distributed key-
managing authority:
The total number of key-managing authority instances is n = 2t + 1 and we
assume that at least a majority of t + 1 instances are alive at any time. Every
instance holds its own version of function C().
Decryption Request: The data consumer prepares the same decryption re-
quest for all key-managing authority instances. As in the non-distributed

230
M. Jawurek and F. Kerschbaum
protocol, it sends the encrypted statistics result fh(V , E(c, 1)) and the in-
formation about the used input values I to, unlike in the non-distributed
protocol, all key-managing authority instances.
Validation of Freshness: As in the non-distributed case, every instance ver-
iﬁes the freshness of used data items: For every (pj, Pj(vj)) ∈I it checks
whether C(pj) < Pj(vj). If successful, C() is updated: C(pj) ←Pj(vj).
The k-th instance decrypts fh(V , E(c, 1)) and applies the function mf(s, x)
to make it diﬀerentially private. We describe at the end of this Section how
all instances can ensure they apply the same random noise. This yields
x = mf(s) ◦Dv ◦fh(V , E(c, 1))
The k-th instance now creates a share sk of secret x using a random polyno-
mial in Shamir’s secret sharing scheme. In order to ensure that all instances
choose the same random polynomial, we apply the same mechanism as for
the noise (see below). This resulting secret share sk is ﬁnally returned to the
data consumer.
Assembly: The data consumer collects the secret shares from at least t + 1
key-managing authority instances and reassembles the diﬀerentially private
statistics result’s plaintext.
The idea of this protocol is that the data consumer has to contact at least
t + 1 instances of the key-managing authority to obtain enough shares for its
statistics. Every instance in this majority will then also update their state C.
This means, that subsequently, a malicious data consumer will fail to ﬁnd a
disjoint majority set of instances and thus cannot cannot create partitions of
key-managing authority instances with diﬀerent states.
Note that all key-managing instances operate completely independent of each
other. On the one hand, this eliminates any synchronization problems. On the
other hand, every instance must create valid shares of the same x in order to
allow successful assembly of x at the data consumer. We propose a mechanism
that chooses the randomness for the random noise and for the secret sharing
polynomial based on a pseudo-random function with identical seed across all
instances. We then use the commonly available information I as seed.
5
Zero Knowledge Proof
5.1
Properties
Already in the naive protocol version we shown how a key-managing authority
can decrypt a homomorphically encrypted function result and add random noise
in order to guarantee ϵ-diﬀerential privacy in the presence of a semi-honest data
consumer. However, this relies on the key-managing authority’s knowledge about
the sensitivity of the evaluated function (see Section 3.1) and on the freshness
of data items (Section 4.2). In Section 4.3 we describe how a key-managing
authority can guarantee freshness with manageable state and how such a key-
managing authority can be distributed (Section 4.4).

Fault-Tolerant Privacy-Preserving Statistics
231
In order to complete the ﬁnal protocol that ensures diﬀerential privacy over an
unbounded number of statistics calculations in case of a malicious data consumer
we require guarantees for the following properties:
1. The key-managing authority can verify the correctness of the provided data
item information I in order to guarantee freshness.
2. The key-managing authority can verify the correct evaluation of a known
statistical function on data items provided by data producers. This allows the
key-managing authority an appropriate choice of random noise for making
the statistics result diﬀerentially private.
In
the
following
(Section
5.2)
we
describe
the
ﬁnal
fault-tolerant,
privacy-preserving statistics protocol between the data consumer and the dis-
tributed key-managing authority that provides guarantees for these properties.
5.2
The Final Protocol
Like in the naive protocol version we assume, without loss of generality, a pre-
arranged statistics function f and a pre-distributed key-managing authority’s
public key. The function Pj(v) at data producer j returns the position (logical
time) of data item v within the total order of data items of data producer j. At
the key-managing authority (at every instance), the function C(j) returns for
data producer j the position (logical time) of the latest data item that was used
in a statistics calculation in j’s total order of data items.
Preparation: Every data producer j ∈1, . . . , n encrypts its data item vj with
a random number rj. Furthermore, the data producers encrypt the rj with
another chosen random value r′
j and create signatures over the random’s
ciphertext and the data item identiﬁcation tuple (j, Pj(vj)): Sj(E(rj, r′
j),
(j, Pj(vj)))j. Every data producer sends (j, Pj(vj)), E(vj, rj), E(rj, r′
j) and
Sj(E(rj, r′
j), (j, Pj(vj)))j) to the data consumer.
Calculation: The data consumer now chooses a vector
V = (E(v1, r1), . . . , E(vm, rm))
out of all encrypted data items that it received and corresponding vectors of
encrypted randoms R, of data item information I and signatures S:
V =(V1, . . . , Vm)|Vi ∈{E(v1, r1), . . . , E(vn, rn)}
R =(R1, . . . , Rm)|Ri ∈{E(r1, r′
1), . . . , E(rn, r′
n)}
I =(I1, . . . , Im)|Ii ∈{(1, P1(v1)), . . . , (n, Pn(vn))}
S =(S1, . . . , Sm)|
Si ∈{S(E(r1, r′
1), (1, P1(v1))1, . . . , S(E(rn, r′
n), (n, Pn(vn)))n}
Then, exactly like in the naive protocol version, it calculates the pre-arranged
function f of the form
f((x1, . . . , xm), c) = (
m

i=1
ai · xi) + c

232
M. Jawurek and F. Kerschbaum
by evaluating f’s homomorphic counterpart
fh(V , E(c, rc = 1)) = (
m

i=1
E(vi, ri)ai) · E(c, rc)
fh is derived from f using Equations 1 and 2, the ai are constants and c is
a data consumer-chosen input variable.
If V = (E(v1, r1), . . . , E(vm, rm)) and fh and f ′ have been derived from
the same function f the following holds:
Dr ◦fh(V , E(c, 1)) = f ′ ◦Dr(V ∪E(c, 1))
(3)
Then, the data consumer sends fh(V , E(c, 1)), R, I and S to each available
key-managing authority instance for decryption.
Decryption: Every key-managing authority instance performs two checks:
First, for the freshness guarantee: For every Ij = (j, Pj(vj)) ∈I and Rj ∈R
it checks the signature Sj and the freshness of every used data item: C(j) <
Pj(vj). If successful, C() is updated so that: C(j) ←Pj(vj).
Second, for correct function evaluation with data producers’ data items:
It derives function f ′ from f. f ′ represents the homomorphic operations on
the random part of the ciphertext of f (see Equations 1,2):
f ′(V ) =
m

i=1
Dr(E(vi, ri))ai
It checks if (according to Equations 3):
Dr ◦fh(V , E(c, 1)) = f ′ ◦Dv(R)
The key-managing authority instance accepts the proof if both checks pass.
Then, like in the naive protocol it applies the function mf(s, x) which adds
a random sample according to the sensitivity of f (see Section 3.1) and seed
s to the decrypted function result: x = mf(s) ◦Dv ◦fh(V , E(c, 1)). We
use a pseudo-random function on the data item information I as seed s in
order to create the randomness, so that every instance of the distributed
key-managing authority will add the same random sample and thus obtains
the same result x. This allows the k-th key-managing authority instance to
ﬁnally create a share sk of x (using the same seed s, but a diﬀerent pseudo-
random function) and return that to the data consumer.
Remark 1. Note that, if the data consumer supplies diﬀerent input I′ to one
key managing authority, then its resulting share will be uniformly randomly
distributed. Our ZKP only covers malicious behavior in the computation of
the statistical function f by the data consumer and not its decryption. The
data consumer can always prevent the correct decryption of the statistics,
but this only prevents him from obtaining the result.

Fault-Tolerant Privacy-Preserving Statistics
233
Assembly: The data consumer assembles the decryption shares from at least
(t + 1) key-managing authority instances and reassembles the decrypted,
diﬀerentially-private, statistics result:
Dv(mf(I) ◦h fh(V , E(c, 1)))
Theorem 1. Our zero knowledge proof is complete, sound and honest-veriﬁer
zero-knowledge.
Proof. Following Remark 1, we can assume that the vector I is static and omit
it from the proof. For completeness: Assume that the data consumer, according
to protocol step Calculation, chooses V ,R and S, computes the pre-arranged,
linear function fh with V and supplies the function result fh(V , E(c, 1)), R and
S to the key-managing authority. Then the key-managing authority will verify
that R only contains fresh entries and that those entries have valid signatures
in S and according to the last check of the protocol’s step Decryption it will
accept the proof if: Dr ◦fh(V , E(c, 1)) = f ′ ◦Dv(R)
Which is true if R contains the corresponding entries to V (using Equation 3):
Dr ◦fh(V , E(c, 1)) = f ′ ◦Dr(V ∪E(c, 1))
=f ′((Dv(R1), . . . , Dv(Rm)) ∪1)
=f ′ ◦Dv(R)
□
For soundness, we show by contradiction: If our ZKP was not sound, IND-CPA
game for the Paillier cryptosystem would be solvable. By providing a simulator
that reduces the IND-CPA challenge to the problem of forging the ZKP we will
show that forging the ZKP must be hard as well.
For soundness of the prover we need to diﬀerentiate between data producers
(system environment) and consumer (prover). We use – in this and the subse-
quent part of the proof – a signature oracle for the data consumers that returns
any valid signature. In this part of the proof, the data consumer has no access
to this oracle.
Our simulator ﬁrst interacts with the IND-CPA challenger. It provides two
values as input: r0 and r1. The challenger returns the encryption of a randomly
chosen value cx = E(r0|r1, r′
x). Without loss of generality, assume that the func-
tion f takes m data items from data producers as inputs. Now, the simula-
tor takes (m −1) triples {vi, ri, r′
i} and encrypts and signs them, resulting in
(m −1) × {E(vi, ri), E(ri, r′
i), S(E(ri, r′
i), (j, Pj(vi)))}. The simulator also signs
cx and some data item information tuple y (using the oracle) and supplies this
incomplete ZKP Z = {(m −1) × {E(vi, ri), E(ri, r′
i), S(E(ri, r′
i), (j, Pj(vi)))} ∪
{cx, S(cx, y)} to the data consumer. Note that in this setup the data consumer
needs to complete the ZKP, e.g. by supplying the missing (input value) cipher-
text: E(vx, r0|r1). Then the data consumer engages in communication with the
key-managing authority, with our simulator listening in their communication.
The data consumer passes the full ZKP Z ∪E(vx, r0|r1) to the key-managing
authority and obtains the decrypted function result f(V ). The simulator still

234
M. Jawurek and F. Kerschbaum
knows all vi (except vx) and can then infer vx from the function result. Conse-
quently, the simulator can also infer a plausible E(vx, r0|r1). Now, the simulator
performs the last check: It creates E(vx, r0) and E(vx, r1) and compares them to
E(vx, r0|r1). It thus solves the IND-CPA problem.
□
For honest-veriﬁer zero-knowledge we give a simulator of the authority’s (ver-
iﬁer’s) view only from its input and output. We stress, that for zero-knowledge
of the veriﬁer we can view the data producers and consumer as one entity. The
veriﬁer – and not the prover – holds the private keys for the encryption of the
values submitted during the proof. Consequently we need to simulate the plain-
texts of the encryption. Furthermore, we need to take care of the randomization
of the encryption, since it is not always uniformly distributed.
In order to simulate fh(V , E(c, 1)) the simulator uniformly chooses two ran-
dom values x ∈Zn and r ∈Z∗
n. The simulated value is E(x, r). Note that, since
c can be drawn uniformly from Zn, so can x. Then, in order to simulate R the
simulator uniformly chooses m −1 random values ri ∈Z∗
n (1 ≤i ≤m −1).
It sets rm = r(m−1
i=1 ri)−1. The simulated values are the ciphertexts E(ri, r′
i)
where the r′
i are drawn uniformly from Z∗
n. All the random values in the message
of the ZKP are identically distributed – including their arithmetic relationship.
Last, in order to simulate the signatures S the simulator invokes the signature
oracle on the simulated vector R. If this oracle works properly, all signatures will
be valid. This completes our simulator.
6
Related Work
Attacks on Smart Metering Privacy: Several works [8,9,15,16,17,19,22] have
covered the area of behavior analysis from energy consumption traces. The au-
thors of [17] developed a system which inferred behavior events from the elec-
trical consumption data and evaluated the performance of their approach with
control data from video surveillance. This enables them to construct a sam-
ple disclosure metric that “...associates data quality (accuracy of readings, time
resolution, types of readings, and so on) from a particular source with the infor-
mation that the data could reveal.” [15] focuses on detecting and characterizing
diﬀerent appliances according to load signatures.
Privacy-Preserving Smart Meter Billing: A cryptographic approach to pri-
vacy in smart meter billing has been presented in [12]. A privacy component ho-
momorphically calculates the price locally in the household and only reports the
ﬁnal price and a cryptographic proof over commitments on the meter readings to
the supplier. The proof allows the supplier to verify the correct calculation and
tariﬀwithout ever receiving plaintext values. Another cryptographic approach
very similar to [12] is described in [24]. It focuses on realizing a variety of dif-
ferent tariﬀtypes with a cryptographic solution and reducing the complexity of
the calculations in the smart meter.

Fault-Tolerant Privacy-Preserving Statistics
235
Private Distributed Aggregation Model: In the following we describe two
general protocols [25,23] in the private distributed aggregation model and one [2]
in the fault-tolerant, private distributed aggregation model.
[25] describes a system where users provide homomorphically encrypted data
items with individually added random noise. The aggregator homomorphically
aggregates these data items and decrypts them. During aggregation individual
amounts of random noise cancel each other out except for a speciﬁc amount that
guarantees computational diﬀerential privacy.
In [23] bi-directional communication between users and the aggregator is re-
quired. First, users supply their homomorphically encrypted and randomized
value to the aggregator for aggregation. Then, the aggregator sends the aggre-
gated perturbed value back to all users for decryption. Every user removes its
individually added random noise (except for diﬀerential privacy) and replies
with a decryption share of the the ﬁnal result which are ﬁnally combined by the
aggregator.
In [2] a system of intersecting user groups allows the aggregator to compensate
for user failures with the help of redundant information. As every user’s data
item is represented in the aggregate of several groups it can be recovered even
if some group aggregates cannot be recovered due to user failures.
Privacy-Preserving Smart Metering Statistics: Furthermore, we provide
an overview on diﬀerent (non-fault-tolerant) approaches that ensure privacy for
diﬀerent applications of smart metering data: leakage detection [5] by homo-
morphic encryption, general aggregation [14] and comparison by secret-sharing,
aggregation by third party [1,21] or aggregation [1] by randomization.
In [5] a privacy-preserving detection algorithm for leakages in electricity dis-
tribution has been proposed. Using homomorphic encryption and secret sharing
several smart meters engage in a protocol with a (potentially malicious) substa-
tion that searches for diﬀerences between its own measurement and the sum of
all smart meter readings. While both compute the sum in a private manner, our
approach (obviously) does not require a measurement of aggregate consumption,
requires only uni-directional communication and tolerates failing smart meters.
In [14] the authors propose several protocols for privacy-preserving aggrega-
tion or comparison on smart metering data based on secret-sharing. They also
provide an extensive analysis w.r.t. cryptographic veriﬁability, their computa-
tional and communicational complexities and their applicability for settlement
and proﬁling applications. Their work diﬀers from ours mainly in their require-
ment of smart meters to form groups over which aggregates are computed which
also implicates meter to meter communication and implicitly assumes no failures.
Furthermore, in [1] a model for measuring privacy in smart metering is de-
veloped and subsequently two diﬀerent solutions to privacy are presented: A
trusted third party approach, where aggregation takes place at the third party
and alternatively the approach of masking individual values with added noise
directly at the smart meters which is canceled out in the sum over all meters
at the supplier. The trusted third party of their ﬁrst approach is able to calcu-
late arbitrary statistics. However, as it has to perform all calculations instead

236
M. Jawurek and F. Kerschbaum
of one decryption, as in our case, it requires more computational power and
storage than in our protocol. Their second approach has reduced variance in the
function result with increasing number of meters in contrast to our approach.
Ours, however, is independent of the total amount of smart meters contributing
readings and allows the calculation of arbitrary linear functions.
Finally, in [21] the authors suggest to use the electrical grid infrastructure as
a trusted third party to anonymize up-to-date consumption values constantly
sent out by smart meters. This third party veriﬁes the authenticity of the data,
removes the identifying information and forwards it to the consumer of this
data. This solution provides privacy by anonymizing information at a trusted
third party before forwarding them to the data consumer which requires a similar
computational and storage eﬀort as in [1].
Application of Diﬀerential Privacy in Smart Metering: In [3] the authors
build upon a previous work [24] and additionally apply diﬀerential privacy to
hide any information leakage that is implied by the billing amount itself. Using
diﬀerential privacy the consumer presents a randomized bill to the supplier that
is higher then the actual one. Their approach especially targets the remaining
information leakage implied by the bill calculation function itself, in which it is
similar to ours. However, we target forecasting where smart meter readings are
required in real-time and thus aggregation over time is not an option.
7
Summary and Conclusion
We provide a protocol in the fault-tolerant, private distributed aggregation model
that allows a data consumer to calculate unbounded statistics (weighted sums)
over homomorphically encrypted sensitive data items from data producers. Our
protocol is fault-tolerant, as the data consumer can choose to calculate over an
arbitrary subset of all available data items, i.e., failing data producers do not
prevent the statistics calculation. It is also privacy-preserving, because a (pos-
sibly distributed) key-managing authority ensures diﬀerential privacy before re-
sponding to the data consumer’s decryption request for the homomorphically
encrypted statistics result. Our protocol is secure against malicious data con-
sumers (aggregators) and features aggregator obliviousness, diﬀerential privacy
and a uni-directional communication model between data producers and data
consumers. In comparison to the other existing protocol [2] in this model the ac-
curacy of our statistics calculation is higher, particularly in the presence of fail-
ures. We are also more ﬂexible, since we allow the non-interactive, intermittent
change of the statistics function and we do not require group key management.
In summary, our protocol provides the best suitable foundation for privacy-
preserving, real-time energy consumption forecasting using smart meters. Future
work is to extend the availability of consumption data to other data consumers,
such as regulators that need to verify single data items, and to incorporate other
data sources, such as weather forecast and television programs.
Acknowledgments. Marek Jawurek’s work in this paper was partly funded by
the German Federal Ministry of Economics and Technology (BMWi) as part of

Fault-Tolerant Privacy-Preserving Statistics
237
the MEREGIOmobil project with reference number 01ME09007 and the MERE-
GIO project with reference number 01ME08006.
References
1. Bohli, J.-M., Ugus, O., Sorge, C.: A privacy model for smart metering. In: Proceed-
ings of the First IEEE International Workshop on Smart Grid Communications (in
Conjunction with IEEE ICC 2010) (2010)
2. Chan, T.-H.H., Shi, E., Song, D.: Privacy-preserving stream aggregation with fault
tolerance. In: Proceedings of the 16th International Conference on Financial Cryp-
tography and Data Security, FC 2012 (2012)
3. Danezis, G., Kohlweiss, M., Rial, A.: Diﬀerentially private billing with rebates.
Cryptology ePrint Archive, Report 2011/134 (2011), http://eprint.iacr.org/
4. Dwork, C.: Diﬀerential Privacy. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener,
I. (eds.) ICALP 2006, Part II. LNCS, vol. 4052, pp. 1–12. Springer, Heidelberg
(2006)
5. Garcia, F.D., Jacobs, B.: Privacy-Friendly Energy-Metering via Homomorphic En-
cryption. In: Cuellar, J., Lopez, J., Barthe, G., Pretschner, A. (eds.) STM 2010.
LNCS, vol. 6710, pp. 226–238. Springer, Heidelberg (2011)
6. Ghosh, A., Roughgarden, T., Sundararajan, M.: Universally utility-maximizing pri-
vacy mechanisms. In: Proceedings of the 41st Annual ACM Symposium on Theory
of Computing, STOC 2009, pp. 351–360. ACM, New York (2009)
7. Goldreich, O., Warning, A.: Secure multi-party computation (1998)
8. Hart, G.: Nonintrusive appliance load monitoring. Proceedings of the IEEE 80(12),
1870–1891 (1992)
9. Hart, G.W.: Residential energy monitoring and computerized surveillance via util-
ity power ﬂows. IEEE Technology and Society Magazine (June 1989)
10. Heck, W.: Smart energy meter will not be compulsory. NRC Handelsblad
(online) (April 2009), http://www.nrc.nl/international/article2207260.ece/
Smart energy meter will not be compulsory
11. Jamieson, A.: Smart meters could be ’spy in the home’. Telegraph (UK) (online)
(October 2009),
http://www.telegraph.co.uk/finance/newsbysector/energy/6292809/
Smart-meters-could-be-spy-in-the-home.html
12. Jawurek, M., Johns, M., Kerschbaum, F.: Plug-in privacy for smart metering
billing. CoRR, abs/1012.2248 (2010)
13. Jawurek, M., Johns, M., Rieck, K.: Smart metering de-pseudonymization. In: AC-
SAC, pp. 227–236 (2011)
14. Kursawe, K., Danezis, G., Kohlweiss, M.: Privacy-Friendly Aggregation for the
Smart-Grid. In: Fischer-H¨ubner, S., Hopper, N. (eds.) PETS 2011. LNCS, vol. 6794,
pp. 175–191. Springer, Heidelberg (2011)
15. Lam, H., Fung, G., Lee, W.: A novel method to construct taxonomy electrical
appliances based on load signaturesof. IEEE Transactions on Consumer Electron-
ics 53(2), 653–660 (2007)
16. Laughman, C., Lee, K., Cox, R., Shaw, S., Leeb, S., Norford, L., Armstrong, P.:
Power signature analysis. IEEE on Power and Energy Magazine 1(2), 56–63 (2003)
17. Lisovich, M.A., Mulligan, D.K., Wicker, S.B.: Inferring personal information from
demand-response systems. IEEE Security and Privacy 8(1), 11–20 (2010)

238
M. Jawurek and F. Kerschbaum
18. McSherry, F.: Privacy integrated queries: an extensible platform for privacy-
preserving data analysis. Commun. ACM 53(9), 89–97 (2010)
19. Molina-Markham, A., Shenoy, P., Fu, K., Cecchet, E., Irwin, D.: Private memoirs
of a smart meter. In: Proceedings of the 2nd ACM Workshop on Embedded Sensing
Systems for Energy-Eﬃciency in Building, BuildSys 2010, pp. 61–66. ACM, New
York (2010)
20. Paillier, P.: Public-Key Cryptosystems Based on Composite Degree Residuosity
Classes. In: Stern, J. (ed.) EUROCRYPT 1999. LNCS, vol. 1592, pp. 223–238.
Springer, Heidelberg (1999)
21. Petrlic, R.: A privacy-preserving concept for smart grids. In: Sicherheit in vernet-
zten Systemen: 18. DFN Workshop, pp. B1–B14. Books on Demand GmbH (2010)
22. Prudenzi, A.: A neuron nets based procedure for identifying domestic appliances
pattern-of-use from energy recordings at meter panel. IEEE Power Engineering
Society Winter Meeting 2, 941–946 (2002)
23. Rastogi, V., Nath, S.: Diﬀerentially private aggregation of distributed time-series
with transformation and encryption. In: Proceedings of the 2010 International Con-
ference on Management of Data, SIGMOD 2010, pp. 735–746. ACM, New York
(2010)
24. Rial, A., Danezis, G.: Privacy-preserving smart metering. Technical report, Mi-
crosoft Research (November 2010)
25. Shi, E., Chan, T.-H.H., Rieﬀel, E.G., Chow, R., Song, D.: Privacy-preserving ag-
gregation of time-series data. In: NDSS (2011)

Evading Censorship with Browser-Based Proxies
David Fiﬁeld1, Nate Hardison1, Jonathan Ellithorpe1, Emily Stark2,
Dan Boneh1, Roger Dingledine3, and Phil Porras4
1 Stanford University
2 Massachusetts Institute of Technology
3 The Tor Project
4 SRI International
Abstract. While Internet access to certain sites is blocked in some parts
of the world, these restrictions are often circumvented using proxies out-
side the censored region. Often these proxies are blocked as soon as they
are discovered. In this paper we propose a browser-based proxy creation
system that generates a large number of short-lived proxies. Clients using
the system seamlessly hop from one proxy to the next as these browser-
based proxies appear and disappear. We discuss a number of technical
challenges that had to be overcome for this system to work and report
on its performance and security. We show that browser-based short-lived
proxies provide adequate bandwidth for video delivery and argue that
blocking them can be challenging.
1
Introduction
While the Internet began as a research network open to all types of data, many
nations now ﬁlter Internet traﬃc. The OpenNet Initiative, which tracks public
reports of Internet ﬁltering, lists a large number of countries that ﬁlter Internet
traﬃc. Some countries block sites like YouTube and Facebook while others block
access to web content containing materials they consider objectionable. The list
of countries includes well-publicized examples in Asia and the Middle East as
well as Australia and several European countries. Over half of the 74 countries
tested in 2011 imposed some degree of ﬁltering on the Internet [1].
Censored users try to bypass the censor by connecting to sites through a
proxy. Several proxy systems have emerged to help users circumvent censorship.
Most notable among these is Tor [2], which, while originally designed to provide
anonymity, has also seen wide use in circumvention. Other proposals include
Telex [3] and Ultrasurf [4]. The existence of circumvention systems makes the
censor’s job harder: The censor must block access to all circumvention tools, in
addition to any resources it would ordinarily block, if it is to remain eﬀective.
Our goal is to enable access to circumvention even in the face of such blocking.
Proxy-based circumvention systems generally need to solve three problems:
1. Rendezvous. A rendezvous protocol lets a user in the censored region send
and receive a small amount of information (a few bytes) from the circumven-
tion system to outside the censored region, for the purpose of introducing a
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 239–258, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

240
D. Fiﬁeld et al.
user to a proxy. Rendezvous protocols are designed for low-rate traﬃc and
are intended to be diﬃcult to block. Tor, for example, developed rendezvous
protocols to distribute the IP addresses of Tor bridges, which are relays
whose addresses are not universally known so they are harder to block [5].
2. Proxy creation. The circumvention system relies on proxies outside the ﬁl-
tered region to relay traﬃc from the client to the desired site. In response the
censor can masquerade as legitimate users to discover proxy addresses and
promptly block them. One way to combat this Sybil attack is to constantly
create new proxies outside the ﬁltered region. As proxies get blocked, new
ones take their place. Rapid proxy creation is the main topic of this paper.
3. Camouﬂage. Once the client has the address of a non-blocked proxy, it
needs to camouﬂage its conversation with the proxy so that the session can-
not be blocked by traﬃc analysis. The goal of camouﬂage is to make the
conversation look like acceptable traﬃc such as an e-commerce transaction,
a voice conversation, or part of a multiplayer game. Concrete proposals for
camouﬂage include obfsproxy [6] and StegoTorus [7]. We treat camouﬂage
as an independent layer and do not discuss it in this paper.
A complete circumvention system must also address secure client software dis-
tribution, an install system, and secure integration with a web browser. The Tor
Project already handles these issues quite well and we do not discuss them here.
In this paper, we focus on Tor because it is widely deployed, with hundreds
of thousands of daily users [8]. Tor consists of a network of several thousand
volunteer nodes, known as relays. Clients build a three-node encrypted circuit
through the network by selecting relays from a public relay directory. The client
sends data to an entry node, which forwards it through an intermediate and
an exit node, after which the exit node sends the data to the destination host
on the public Internet. More details about the Tor design are given in [2]. The
fact that relays are listed in a public directory makes them easy to block [9] –
a countermeasure to this blocking is Tor bridges, relays whose addresses are not
made universally known. Bridges, too, have been found susceptible to partial
enumeration and blocking [10–12].
Our Contributions. In this paper we propose a new approach to rapid proxy
creation. The core idea is to use the power of the web to create millions of short-
lived proxies, each proxy being active for only a few minutes. To do this, we enlist
the help of volunteer web sites (e.g., personal home pages) outside the ﬁltered
region that want to support an open Internet. These volunteer web sites are
unrelated to the destination web site that the censored user is trying to reach.
A volunteer web site simply embeds a small “Internet Freedom” badge on
its web pages (see Fig. 1). The badge is a tiny user interface on top of some
JavaScript code. When a web browser outside the ﬁltered region visits the vol-
unteer site, it runs the JavaScript program, which relays traﬃc to and from the
ﬁltered region through the visitor’s web browser. In eﬀect, the browser visiting
the volunteer’s site becomes a short-lived proxy. As soon as the visitor navigates
away from the page, the browser unloads the badge and the proxy disappears
leaving no trace on the visitor’s machine. Surprisingly, browsing the web through

Evading Censorship with Browser-Based Proxies
241
these ephemeral browser-based proxies, even when hopping from one proxy to
another, works quite well. Our experiments in Sect. 4 show that the ﬁltered client
can sustain more than enough bandwidth to carry a Tor tunnel.
Fig. 1. Flash proxy badge on a web page. It runs in a visitor’s web browser, and the
(optional) counter increments for each client served. Clicking on the badge disables it.
Our ﬂash proxies1 are at the opposite extreme of Tor bridges. Tor bridges are
intended to serve for a long time; they are created at a relatively low rate and
there are only a few thousand of them. Our proxies are only active while the
visitor’s browser is viewing the volunteer web page – often just a few minutes –
but a new proxy is created every time someone visits a volunteer page, potentially
creating a pool of millions of active proxies at any given time. Building a reliable
transport using these ephemeral proxies presents interesting challenges discussed
in Sect. 3. For completeness we discuss new strategies for rendezvous in Sect. 6.
2
Threat Model and Assumptions
Our setting includes four key players. The client owns a computer in the ﬁltered
region and is trying to access a web site outside the ﬁltered region. We assume
the client has complete control of his computer and, in particular, can install
arbitrary software on the computer. The target web site (e.g., Facebook) is
located outside the ﬁltered region and is generally oblivious to the circumvention
eﬀort. That is, it does not cooperate with nor try to prevent circumvention. The
circumvention tool attempts to relay traﬃc back and forth from the client
to the target host. The tool may include client software as well as network in-
frastructure inside and outside the ﬁltered region. The adversary (censor), who
tries to ﬁlter traﬃc, must allow acceptable Internet traﬃc such as banking and
e-commerce, but tries to block all objectionable traﬃc, including possibly block-
ing systems used by the circumvention tool. The deﬁnition of what is acceptable
and objectionable is up to the adversary.
The adversary achieves its goal by installing hardware and software at Internet
Service Providers (ISPs) in the ﬁltered region. It can inspect all network traﬃc
1 The word “ﬂash” is meant to evoke an idea of quickness and ephemerality. Our ﬁrst
implementation of the system used Adobe Flash, which partly inspired the name.
The current implementation uses JavaScript rather than Flash.

242
D. Fiﬁeld et al.
to and from the ﬁltered region and block any packet it wishes. However, the
adversary operates under the following three constraints:
Line Rate. It must operate at line rate and cannot noticeably slow down legit-
imate traﬃc to and from the ﬁltered region. In other words, it has little time to
make its decision whether to block or allow packets and ﬂows.
No Control of Clients. We assume the adversary does not have software
installed on the client computer. This assumption matches current reality where
ﬁltering happens at the network and not at the end host. If the adversary can
force users to run ﬁltering software – either by law or by using technologies
such as Trusted Computing (TCG) – then the circumvention problem becomes
much harder, though not impossible. Some circumvention tools are designed for
a public-kiosk environment where the client is browsing the web over a public
terminal that may have ﬁltering software installed. These tools, however, are
much harder to use and therefore in this paper we assume the user is in full
control of his computer.
Minimal Collateral Damage. To the extent possible, the adversary tries to
avoid collateral damage. It tries to minimize the impact to acceptable ﬂows, such
as those used for banking and e-commerce. The adversary cannot simply shut
down all Internet traﬃc to the ﬁltered region, as this would halt all banking and
e-commerce in the region. While some governments, including Egypt, Libya and
Syria, have recently implemented ﬁltering by literally shutting down all Internet
traﬃc, most deployed Internet ﬁlters try to minimize collateral damage so as not
to hurt commerce.
3
Rapid Proxy Creation Using Flash Proxies
Next we present an architecture for creating a large number of short-lived prox-
ies. The ﬂash proxy system uses browsers all over the Internet as ephemeral
proxies. The design is in large part dictated by a limitation of web socket tech-
nologies, namely that they can only make outgoing TCP connections, and cannot
receive connections as a normal proxy would. In the ﬂash proxy model, the proxy
connects to the client, not the other way around.
Our implementation uses the Tor pluggable transports support introduced in
Tor version 0.2.2.32 [13]. Pluggable transports are designed to enable diﬀerent
circumvention and tunneling schemes without having to modify core Tor code.
Our system requires a program called a client transport plugin running on the
client and a server transport plugin running on the relay. The functioning of
these pieces is described further below. The integration of pluggable transports
in Tor means that using these auxiliary programs is fairly painless.
Two of the ﬁve system components (the Tor client and relay) are the same
as are used for any Tor connection. The remaining three (the proxy, facilitator,
and transport plugins) are speciﬁc to the ﬂash proxy system (see Fig. 2).

Evading Censorship with Browser-Based Proxies
243
Fig. 2. Flash proxy architecture. When a client wants service, it registers with the
facilitator and waits. A ﬂash proxy appears and polls the facilitator for a client address.
Once the proxy obtains a client address it connects to both the client and to the Tor
relay and proxies traﬃc back and forth for the client.
Tor Client. The censored user runs a Tor client, conﬁgured to use a client
transport plugin.
Flash Proxy. The proxy itself is a small application that runs in a web browser
and is hosted on a volunteer web site. We have made two implementations of the
ﬂash proxy, one using Adobe Flash and one using JavaScript with WebSocket. We
have presented the proxy as a small “Internet Freedom” badge as shown in Fig. 1.
Whenever someone visits the page, the badge begins running in their browser.
When they navigate away the badge is unloaded leaving no trace. The badge
communicates with the facilitator to ﬁnd the addresses of clients that need a
connection. Once it has a client address, it connects to the client transport
plugin, then to the server transport plugin running on the Tor relay, and
begins proxying data between them. The badge itself runs in the background and
has no impact on the visitor’s interaction with the volunteer site. It is important
to understand that censored users do not need to see the badge – in fact web
sites including the badge can be blocked by the censor – because it is visitors to
the web site, not the web site itself, who provide the proxy access.
Facilitator. The facilitator keeps track of client registrations and hands them
out to proxies when requested. When the client transport plugin starts it
registers with the facilitator using a robust rendezvous protocol such as one of
those described in Sect. 6. The facilitator may be blocked by the censor; the
purpose of the rendezvous protocol is to allow the client to send its IP address
– just a few bytes – to the facilitator despite blocking. The ﬂash proxies that
communicate with the facilitator do not go through the censor and are therefore
not impacted by blocking the facilitator. After registration, when a browser-
based ﬂash proxy is available for service, the facilitator assigns it to serve a
censored client.

244
D. Fiﬁeld et al.
Transport Plugins. The Tor client and relay both run their own transport plu-
gin to read from and write to the ﬂash proxy tunnel. In the case of WebSocket,
the transport plugins respond to the HTTP handshake, encode and decode bi-
nary data, and put data into WebSocket frames. The client transport plugin has
the additional important duty of bridging Tor’s “connect outward to a proxy”
expectation and the “receive a connection from a proxy” reality of the ﬂash proxy
architecture: it receives connections from the Tor client (using ordinary TCP),
and on another port receives connections from ﬂash proxies (using WebSocket),
maintaining a pool of connections of each type. Whenever there is at least one
connection in both pools, the transport plugin links them up and begins to relay
data traﬃc back and forth. When a ﬂash proxy disappears, the transport plugin
begins to use another proxy connection from the pool. The Tor client is mostly
unaware that a new ﬂash proxy is put in place.
Tor Relay. Any Tor relay can be used as the entrance to the Tor network, as
long as it runs the server transport plugin. The relay may be blocked from the
point of view of the client.
3.1
Establishing Connections
Programs running in a web browser, whether they use WebSocket, Flash, or
other socket-like technologies, share a limitation: they cannot open a listening
socket and wait passively for connections; they can only initiate new outgoing
connections. This forces an inversion of the usual proxy model: It is the client
(with a full complement of socket operations) that listens for a connection, and
the ﬂash proxy (limited by running in a web browser) that connects to it. This
inversion is the source of most of the complexity of the model.
A less important restriction is that web browser security policies generally pre-
vent programs from making connections to arbitrary destinations. The browser
requires some positive cooperation from the destination that indicates that the
connection should be allowed. For us this poses no problem as the three destina-
tions the proxy connects to – the facilitator, the client transport plugin, and the
server transport plugin – are cooperative. What this means in concrete terms
for the WebSocket implementation is that the facilitator must allow cross-origin
resource sharing (CORS) [14] by sending an Access-Control-Allow-Origin header
ﬁeld, and that the client and server must be able to answer the WebSocket hand-
shake and proxy the tunneled data to Tor. For Flash it means that the endpoints
must serve a crossdomain policy [15], a small chunk of XML specifying which
connections should be allowed.
The fact that the ﬂash proxy may not receive connections has one important
advantage. The ﬂash proxy operator (i.e., web browser) may be behind network
address translation (NAT) or a ﬁrewall that doesn’t allow incoming connections,
and it doesn’t aﬀect the architecture. The unfortunate corollary is that clients
must be able to receive connections, which generally means not being behind
NAT or else being able to conﬁgure port forwarding. We feel that this is at least
the right way to allocate the burden, if it must fall somewhere. We expect ﬂash

Evading Censorship with Browser-Based Proxies
245
proxies to greatly outnumber censored clients; running a proxy should be as
simple as possible while clients are already motivated to take technical steps for
secure communication. Ideally ﬂash proxies would be usable even behind NAT
without any special conﬁguration using NAT punching techniques; Sect. 5.2
discusses diﬃculties and solutions.
Figure 2 illustrates the components operating in sequence in a sample session:
1. The client starts Tor and the client transport plugin, and sends a registration
to the facilitator using a secure rendezvous mechanism. The transport plugin
begins listening for a remote connection.
2. A ﬂash proxy comes online and polls the facilitator.
3. The facilitator returns a client registration, informing the ﬂash proxy where
to connect.
4. The proxy makes an outgoing connection to the client, and this connection
is received by the client’s transport plugin.
5. The proxy makes an outgoing connection to the transport plugin on the Tor
relay, and begins sending and receiving data between the client and relay.
6. Sooner or later, the ﬂash proxy disappears and breaks the connection be-
tween the the client and the relay. The client’s transport plugin then switches
immediately to another available proxy. In the unlikely event that none are
available, the transport plugin waits until one becomes so. In this process,
existing tunneled TCP streams are broken, but see Sect. 5.2 for ideas on
transparently keeping TCP connections intact.
The client transport plugin maintains a pool of up to ﬁve live proxy connections,
in order to make switching between them faster. Only one of the proxies will be
used at a time, but when the one being used disappears, the socket handshake
is already ﬁnished with one of the reserves, for a lower delay in establishing a
new connection. The transport plugin discards reserve proxies as they go oﬄine,
and the facilitator replenishes the reserve as long as there is extra capacity, so
there is no danger of uselessly switching to a defunct proxy.
The ﬂash proxy has features for quality of service and good network behavior.
A single proxy limits itself to 10 simultaneous client–relay pairs. Its built-in
adjustable rate limit avoids using too much of the operator’s bandwidth. The
proxy checks the platform it’s running on, and disables itself if it is on a mobile
phone or similar device where bandwidth that may be limited or expensive.
The goal of the facilitator is to fairly allocate proxies to clients and attempt
to provide good service for all. The facilitator also seeks to even the load carried
by each proxy whenever there is spare proxy capacity. The facilitator keeps track
of the number of clients given to each proxy. When a new client arrives, it is
given to the proxy with the least load, with ties being broken randomly. (In
the usual case we expect most proxies to be idle most of the time, so this will
be a random selection from among all the idle proxies.) While there is unused
proxy capacity, the facilitator seeks to provide each client with a small number
of redundant proxies for faster switching. When one of the proxies goes oﬄine
(which can be detected because the proxy ceases polling), the facilitator will

246
D. Fiﬁeld et al.
attempt to assign a new proxy to the client that it had been serving. As we
discuss in Sect. 6, client communication with the facilitator is low-bandwidth
and infrequent; therefore there is no explicit message for a client to “unregister”
itself. Instead, the facilitator estimates that a client no longer needs service when
a proxy reports that it has been unable to connect to a client, and a reasonable
timeout elapses. (The timeout is to allow other proxies an attempt to serve the
same client, in case the failed proxy is experiencing network problems.)
One unusual feature of this architecture is that the client using the proxy
does not know in advance where it is connecting to. In fact, the client transport
plugin accepts a dummy destination address only to comply with the pluggable
transports protocol, then ignores it and tunnels to the proxy’s other endpoint,
namely the Tor relay. In eﬀect, the ﬁrst hop of a Tor circuit is made blindly,
and then the client may choose the second and third hop from the directory of
relays. Section 5.1 explains why this does not aﬀect security (brieﬂy, the use of
Tor allows authenticating the destination after the connection is made). We also
note that this is the same situation a client ﬁnds itself in when using a bridge
address it has not seen before.
As a practical matter, there may be more than one facilitator, to diﬀuse the
risk of one’s being breached, and to distribute load. However we do not rely on
there being multiple facilitators for the purpose of evading blocking; we assume
that all facilitators will be permanently blocked by the censor. The censored
clients send data only over a secure rendezvous channel whose characteristics
– being very low-bandwidth, write-only, and infrequently used – make it much
harder to block. Section 6 discusses potential rendezvous protocols in more detail.
4
Experimenting with Ephemeral Flash Proxies
4.1
Throughput
We measured the maximum throughput of a single proxy, independent of Tor and
any other network bottleneck, by running transport plugins, facilitator, proxy,
and web server all on the local host. We then started between 1 and 50 simulta-
neous HTTP downloads of a 10 MB ﬁle directly over TCP, through a WebSocket
proxy, and through a Flash-based proxy. The test was run on the Linux kernel
version 3.2 in a QEMU instance with a 2.27 GHz CPU. The proxies ran in Fire-
fox 8.0.1 with Flash Player 11.1 r102. The proxies’ bandwidth and connection
limits were disabled. Figure 3 shows the time taken for each of the simultaneous
clients to download the ﬁle. Each column contains multiple dots, but in the cases
of Flash sockets and direct download the dots approximately coincide.
Both ﬂavors of proxy are slower than a direct download. The WebSocket
measurements show download times being roughly proportional to the number
of clients. There is much variance, perhaps due to unfairness in the way that the
browser schedules WebSocket message events.
A surprising eﬀect is seen with Flash sockets: the time taken to download
per client is almost constant, up to about 16 clients. We suspect some kind
of internal limit within Flash Player that restricts individual connections to

Evading Censorship with Browser-Based Proxies
247
0
10
20
30
40
50
Number of simultaneous clients
0
5
10
15
20
Time to download (s)
WebSocket
Flash sockets
Direct download
Fig. 3. Time taken to download a 10 MB ﬁle through many simultaneous connections
no more than about 6 MB/s. Because of this restriction, WebSocket is faster
than Flash sockets for small numbers of clients. Eventually natural limits on
bandwidth become more restrictive than this artiﬁcial limit, and the download
time becomes approximately linear.
The WebSocket API does not expose a “read” procedure to read from the
socket; rather, one registers a “message” callback function that is called whenever
data have already been read. Received packets not immediately handled by the
application are buﬀered. At high data rates this buﬀer can grow without bound,
possibly eventually crashing the proxy. This issue is not speciﬁc to ﬂash proxies,
and has not been a problem at Tor rates, but it leaves proxies somewhat exposed
to attacks by malicious clients or relays. WebSocket provides a way to control
the size of the write buﬀer; a complementary mechanism for the read buﬀer
would be suﬃcient to solve this. Flash sockets don’t have this problem as they
buﬀer data at the operating system kernel level, so the TCP window prevents
too much data from being received at once.
These results show that once connected, a ﬂash proxy can provide more band-
width than is commonly used by a Tor circuit (which is in the range of hundreds
of kilobytes per second [16]), and even enough for online video sites. With few
clients, browser-based proxies are stable and have predictable performance.
4.2
Switching between Proxies
It is expected that a Tor client will have to switch between ﬂash proxies fre-
quently as they go oﬄine. To measure the eﬀect this has on performance, we
set up two browsers running the ﬂash proxy program, driven by a script that
turned each proxy on for 10 seconds and then oﬀfor 6, so that the two proxies’
periods of operation would overlap by 2 seconds in a cycle (see Fig. 4). There was

248
D. Fiﬁeld et al.
Fig. 4. Alternation of proxies. Solid bars indicate when a proxy is operating.
always at least one proxy available, so any delay was caused purely by the need
to switch between them. We did a test over the public Tor network, retrieving
the same 5 MB ﬁle used by the torperf measurement tool [17]. (It is not possible
to use torperf directly because it does not retry downloads.) This table shows
the diﬀerence between normal Tor, an uninterrupted ﬂash proxy, and alternating
ﬂash proxies. The direct Tor connection was conﬁgured to use the same entry
bridge that the ﬂash proxies use. Because the performance of the Tor network
changes over time, all the tests were run in direct succession.
The switching experiment was run 20 times. Figure 5 shows the measurements
for each test. Switching between proxies causes a visible increase in the mean
time to download, but most of the variance is caused by Tor itself.
0
20
40
60
80 100 120 140
Alternating ﬂash proxies
Uninterrupted ﬂash proxy
Direct through Tor
mean 69.5 KB/s
mean 79.7 KB/s
mean 56.6 KB/s
Fig. 5. Time taken for each trial in the switching experiment. Each data set contains
20 measurements.
In these results we see that a ﬂash proxy that stays online gives performance
roughly the same as connecting directly to Tor. However, overall speed is 20 to
40 percent lower when using proxies that are unreliable, because of the overhead
of building new Tor circuits. The results of the throughput experiments show
that a ﬂash proxy can provide more bandwidth than Tor can use, so it is not
surprising that downloading though a ﬂash proxy is as fast as downloading over
Tor. On the other hand, reestablishing a broken connection is more expensive.
In this test, the time taken to restart the download was variable, but commonly
left only 3–6 seconds of useful downloading time out of each 8-second cycle.
4.3
Capacity
The ﬂash proxy badge has several properties that, we believe, make it easy to
adopt. It can be installed just by pasting an HTML snippet. It does not require
any special access or conﬁguration on the web server, nor cooperation from ISPs
or the servers being proxied to. Web pages displaying the badge do not have to
be in any particular network position. The sites displaying the badge may even
be blocked by the censor, because it is the viewers of the site, not the site itself,
that provide a circumvention bridge. In this section we seek to predict how many
censored users can be helped, given a certain number of ﬂash proxies.

Evading Censorship with Browser-Based Proxies
249
Here we make some simplifying assumptions. First, we approximate the pat-
tern of visits to a web page as a Poisson process, with diﬀerent arrivals being
independent, and the times between arrivals being exponentially distributed.
This is not a very strong assumption; there is evidence that the request arrival
process for a single web page is Poisson [18], even though the process for indi-
vidual packets is not [19]. Second, we assume that traﬃc has reached a steady
state, without edge eﬀects from web pages appearing or disappearing, and with-
out variability due to time of day or other factors.
Under the Poisson arrival approximation, the traﬃc to a single web page is
governed by two parameters: λ, the mean number of arrivals per unit time; and μ,
the mean visit duration. The times between arrivals are exponentially distributed
with density function λe−λt. The mean inter-arrival time is 1/λ. We do not
assume anything about the distribution of visit durations (which correspond to
proxy lifetimes), other than that a mean exists and that the process generating
them is strictly stationary (unchanging over time).
Diﬀerent web pages have diﬀerent traﬃc characteristics, but we may treat
a group of pages uniformly using the same two parameters, for the following
reasons. The exponential distribution has the property that the minimum of
several exponentials with rates λ1, . . . , λk is also exponentially distributed with
rate λ1 + · · · + λk. The minimum time to the next arrival is exactly what inter-
arrival time is, so the inter-arrival times from several web pages come from their
own exponential distribution. The aggregate mean visit duration across several
web pages is just the mean of all of their individual visit durations. The ﬂash
proxy is conﬁgured to serve up to 10 clients at a time; this may be handled
by multiplying the arrival rate by 10. In this section we think of each proxy as
handling only one client at a time.
If proxy badges collectively provide service with parameters λ and μ, then
the expected number of operating proxies (and hence the number of clients that
can be served) at any time is λμ (which is Little’s law [20]). For example, if 3
proxies come online every second (λ = 3) and each lasts 60 seconds on average
(μ = 60), then we expect 180 proxies to be operating at once on average.
Let us substitute some measured numbers into this formula. We wrote a pro-
gram to record the visits of viewers capable of running Flash, and installed it
on a personal home page for about two weeks. For each visitor, the program
recorded (start-time, end-time, bandwidth, latency). The program recorded 784
visits. we discounted the two longest-lived connections of approximately 1.9 and
July 13
July 14
July 15
July 16
July 17
July 18
July 19
July 20
July 21
July 22
July 23
July 24
July 25
0
1
2
3
Connections
Fig. 6. Measured simultaneous connections on a personal home page

250
D. Fiﬁeld et al.
3.1 days (the next longest connection after those was 15 hours). We also ignored
28 connections that had a measured bandwidth of 0, and an apparent outlier
that came much later than any other connection. This left 753 entries.
Figure 6 shows the number of simultaneous visitors to the web page for each
day of the experiment. There were at times as many as three visitors viewing the
page at once, but only about 17% of the time was there even one visitor. This
web page acting alone would not be able to provide good proxy service, because
of the long periods during which there are no visitors and hence no ﬂash proxies.
It would take several such pages working together to ﬁll in the gaps and provide
continuous service with high probability.
We estimate the quantities λ and μ by taking the sample means of inter-arrival
time and visit duration, respectively:
Mean inter-arrival time 1/λ 1407.6 s
Mean arrival rate
λ 0.00071/s
Mean duration
μ
285.8 s
If 100 web pages like the one we tested were to install the ﬂash proxy badge,
then we estimate an overall arrival rate of λ′ = 100 · 10 · λ ≈0.71 per second
(recall that one proxy can handle 10 simultaneous clients). The combined mean
visit duration is unchanged: μ′ = μ. The expected number of clients that these
100 web pages can support is λ′μ′ ≈203.
The number of clients scales linearly with the number of proxies, so 1,000 web
pages with similar traﬃc characteristics would be able to support 2,030 clients
on average, with the same expected duration. Pages with more visitors (lower
inter-arrival time) or longer visit durations will provide better service.
4.4
Field Testing
We tested connecting to the Tor network from within China over ﬂash proxies
in December, 2011. We were running one proxy, and a few others were run by
unknown web users. The proxies worked as expected, and we could use Tor
despite its well-known blocking by China. This test used only a simple HTTP-
based rendezvous, and not any of the advanced rendezvous methods from Sect. 6,
so it could have been blocked by IP address. Nevertheless, the test shows that
the proxies work as they are supposed to, once the rendezvous step is completed.
5
Discussion
5.1
Security and Privacy
Flash proxies can in principle be used to reach any kind of tunnel, such as an
HTTP proxy or SSH tunnel. Tunneling through Tor, however, brings attractive
security features. The most obvious is enhanced anonymity: It is not easy for a
network observer, including the ﬂash proxy itself, to know the ﬁnal destination

Evading Censorship with Browser-Based Proxies
251
of traﬃc. Users who would like to use Tor for circumvention, but cannot because
Tor relays and bridges are blocked, therefore do not have give to up anonymity
when using a ﬂash proxy transport.
A second feature provided by Tor is encryption. Once a proxy has connected
to its two endpoints, it sends and receives only ciphertext. This is important not
only for the client, but for the temporary operator of the proxy. It would not
be friendly toward proxy operators to allow them to send plaintext that could
potentially run afoul of a corporate ﬁrewall, for example. Note, however, that
encryption can also be obtained by directing ﬂash proxies to forward traﬃc to
an SSH server in the open Internet, in which case the client runs an SSH client
instead of a Tor client.
The third feature is authentication. Even though a malicious ﬂash proxy can
connect to any endpoint it wants, it cannot in this way trick a client into con-
necting somewhere unexpected. The most it can do is deny service. If the proxy
connects to something that is not a Tor relay, the Tor client will fail to make
a connection and show error messages. Once a connection is made to the ﬁrst
relay, the proxy cannot interfere with the client’s circuit construction because it
is already within a layer of encryption. A ﬂash proxy operated by a malicious ad-
versary gets to choose the ﬁrst relay, so it could always choose a relay it controls,
and thereby always see the ﬁrst hop of a circuit. However, such an adversary can
already do something similar simply by running its own bridges and waiting for
connections from ordinary bridge users.
How might the ﬂash proxy system be attacked? In our threat model, the
adversary “wins” if it is able to prevent access to sites that it would block
normally. The adversary can also control some fraction of all ﬂash proxies and
relays. Here we list a number of attacks and possible mitigations.
Client Enumeration. The censor can query the facilitator and get a list of
IP addresses of presumed circumventors. This is a consequence of the fact that
ﬂash proxies connect to censored clients, instead of the other way around. Note
however that the adversary is already in a position to learn the addresses of users
of a circumvention tool, just by sniﬃng at the ﬁrewall, but having a centralized
facilitator makes it easier. One possible mitigation of this attack is sheer num-
bers. If there are many more proxies than there are clients, then most proxies
will not have any client to serve at all. (The facilitator will just return “no client
address” when queried.) The adversary will be competing against all legitimate
ﬂash proxies in querying the facilitator to learn client addresses, and most at-
tempts will be fruitless. With successful deployment we expect to have millions
of available proxies at any given time meaning that most, including many of the
adversary’s, will be idle.
Flooding Client Registrations. The facilitator accepts client registrations
from anywhere as a consequence of the indirect rendezvous mechanism. An ad-
versary can ﬂood the facilitator with fake client addresses. When a legitimate
proxy retrieves one of these fake client addresses, it will waste time trying to give
service to that presumed client. This temporarily removes an otherwise useful
proxy from the system. It should be noted, though, that legitimate registrations

252
D. Fiﬁeld et al.
will still get through and will eventually be picked up by a proxy. Mitigating this
attack requires enough proxies to absorb the busywork created by the adversary.
It is also possible to limit the number of registrations accepted over a period of
time from a given source address.
Exhausting Client Registrations. The adversary can pose as multiple ﬂash
proxies to the facilitator, and ask for addresses of clients, which it then ignores.
The aim is to cause the facilitator to think that the client address has been
given to enough proxies that it need not be given out any more, and prevent
legitimate proxies from seeing the address. This too is mitigated by numbers. If
there are enough legitimate proxies, the adversary will have diﬃculty claiming
all the registration “slots” for a particular client. As long as one of the proxies
is legitimate, the client will be able to get service.
Protocol Fingerprinting. The fact that they run through a browser means
that ﬂash proxy tunnels look diﬀerent from ordinary TCP connections at the
network level. With WebSocket, there is the HTTP handshake to begin the
connection, followed by data in a structured framing format. Flash sockets are
distinguishable because they begin with a crossdomain policy request. Even if
obfuscation is used to hide the Tor protocol, it must all happen within the
framework provided by browser sockets. The unblockability of the system rests
in part on the type of connections used by the proxies (e.g., WebSocket) also
carrying enough ordinary traﬃc that the censor will be reluctant to block that
type of connection wholesale. It remains to be seen whether WebSocket will
have this level of popularity, but support for WebSocket in major browsers is a
promising trend.
5.2
Usability
Usability is listed as a security requirement in the design of Tor [2]. In this
section we examine how much additional eﬀort is required to use ﬂash proxies
on the part of users and relay operators, and how this aﬀects usability.
Relay Operators. On the part of Tor relay operators, the only additional
requirement is to run the ﬂash proxy server transport plugin. This is a matter
of installing the plugin and adding a line to the relay’s conﬁguration ﬁle.
Clients. Our programs are designed to work with the Tor pluggable transports
design, so conﬁguration is fairly easy. A user must run the transport plugin, add
a few lines to the Tor conﬁguration ﬁle, and then be able to register with the
facilitator. Our transport plugin is written in Python, which is an additional
requirement beyond plain Tor. If the user is not able to receive direct TCP
connections, the more technical step of enabling port forwarding must be taken.
A Windows installer can automate this process and we plan to build one as
interest increases.
Limitations. The fact that proxies are expected to disappear causes a qualita-
tive diﬀerence in network behavior when using ﬂash proxies. These diﬀerences

Evading Censorship with Browser-Based Proxies
253
vary in importance depending on the application being used. Basic web brows-
ing, with relatively short-lived connections, works quite well. When connections
are short, there is a smaller chance that they will be interrupted. When they
are interrupted, ﬁxing a partially downloaded page only requires refreshing the
page. Some browsers, such as Firefox and Chrome, automatically restart failed
downloads making this seamless. Large web downloads and video work less well,
because it is more likely that a download will be interrupted in the middle.
Again, browsers that automatically restart failed downloads from the point of
failure make this less of a problem.
The fact that clients must not be behind NAT is an impediment to usability. A
NAT traversal mechanism that works within our threat model would be a great
beneﬁt. Typical NAT traversal technologies, such as STUN (Session Traversal
Utilities for NAT) [21] and RTMFP (Real Time Media Flow Protocol) [22], rely
on a stable third-party server to facilitate the connection, which is trivially de-
feated by the censor blocking the third party by IP address. (Also we believe
it is better to avoid informing a third party of each ﬂash proxy connection if it
can be avoided.) Tricks involving low-level packet manipulation, for example pw-
nat [23], are not available to browser sockets. Ideally, any NAT traversal scheme
will not require both the client and the proxy to know each other’s IP address,
so that facilitator registration can remain unidirectional. New technologies like
WebRTC [24] may ﬁll this need in the future, if they become suﬃciently popular
that ﬂash proxies’ use of them does not stand out as unusual.
Applications that inherently rely on long-lived connections, e.g. SSH, have a
poor user experience. The session ends completely whenever a proxy goes oﬄine,
losing state and requiring a new login. For SSH, the most important property of
a proxy is long lifetime, which ﬂash proxies in the wild usually do not provide. On
the other hand, protocols that use long connections but do not maintain much
server-side state can still work tolerably well with enough application support.
For example, an Internet Relay Chat (IRC) client that automatically reconnects
after losing its proxy server will be usable with only brief interruptions. The ﬂash
proxy system can be extended with buﬀers in the transport plugins, to enable
connection continuity across diﬀerent ﬂash proxy sessions.
5.3
Deployment Scenarios
Our proposal hinges on volunteer web pages hosting the ﬂash badge. Recall
that while the badge is running in the visitor’s browser it has no impact on the
visitor’s experience at the site. Nevertheless, web site admins will likely want to
conﬁgure the badge to best suit their objectives. For this reason the badge is
highly conﬁgurable for diﬀerent scenarios:
– Opt-in vs. Opt-out: Sites that maintain user accounts (e.g., Facebook) can
add a check-box to user proﬁles allowing users to specify whether they want
to participate in this system or not. The Flash object will only be served to
users who check the box. The default settings for the checkbox is a matter of
policy. Moreover, the badge itself can be conﬁgured to only begin proxying
after the visitor clicks on the badge.

254
D. Fiﬁeld et al.
– Geographic Limitations: The badge can be conﬁgured to only serve clients
in certain geographies.
– Proxy Targets: If Tor is not used, the badge can be conﬁgured to only
proxy to speciﬁc domains such as YouTube and Facebook.
– Connectivity: The badge is already conﬁgured to shut down when running
on a mobile device so as not to use up the host’s data plan. It can similarly
be conﬁgured to shut down when it detects a low-bandwidth connection so
as not to interfere with the host’s browsing experience.
We envision two types of deployments. Commercial sites that are already blocked,
such as YouTube, can deploy the badge on their home page so that their uncen-
sored users can help their censored users reach the site. In these deployments,
the badge will only forward traﬃc to YouTube, and possibly only from regions
where YouTube is known to be censored.
Another type of deployment can come from people who are concerned about
Internet ﬁltering and choose to deploy the badge on their home page and blogs.
Visitors to those pages will help censored users connect to Tor. People who choose
to deploy the badge on their blogs can customize it as they wish, possibly serving
clients only in certain geographies or forwarding traﬃc only to certain domains.
6
Rendezvous Protocols
The ﬂash proxy system relies on a robust rendezvous mechanism that lets clients
in the censored region register their IP address with the facilitator. If the censor
could simply block the facilitator then the ﬂash proxy system would break down.
The ﬂash proxy rendezvous problem is related to, but somewhat diﬀerent
from the rendezvous problem in Tor. In the Tor system, rendezvous is used to
communicate the address of an unblocked Tor bridge into the censored region.
In the ﬂash proxy system rendezvous is used to communicate the address of a
client out of the censored region.
Our facilitator design is reasonably modular, so that the facilitator itself does
not need to understand all of the potential rendezvous methods, or know in ad-
vance which will be used. The censored client runs a program implementing a
certain rendezvous method; an uncensored recipient that understands the proto-
col then forwards the registration to the facilitator on the client’s behalf. In this
way, new rendezvous methods can be tested without redeploying the facilitator,
and without requiring the cooperation of those who run the facilitator.
We experimented with two methods for communicating a small amount of
information out of the censored region. The ﬁrst uses cloud-based storage servers
outside the blocked region. The second uses cooperating web sites. We note that
many other systems, including Skype and Telex [3] can be used for rendezvous.
Storage Servers. Cloud storage systems like S3 and Box.net provide a good
opportunity for rendezvous. These services are diﬃcult to block due to large
collateral damage caused by blocking them. We experimented with a system
that uses a variety of such cloud storage systems. As long as one system among
many is unblocked, the information will get out.

Evading Censorship with Browser-Based Proxies
255
The idea is that the facilitator signs up for an account on all cloud storage
servers that the system will use. It sets permissions so that anyone may write
to the storage server, but only the facilitator may read from it. Clients in the
censored region who want service write their IP address to all the storage servers
over HTTPS and the facilitator retrieves them by periodically polling the servers.
Our experiments showed that this method has high latency. For example, it
can take several seconds for data written to S3 in one geographic location to
become available for reading in another location. Hence, while this channel is
insuﬃcient for general network access, it is fairly well suited for rendezvous.
Web Sites. Our second approach, which is more speculative, is to embed ren-
dezvous messages in standard HTTP requests.
A client wishing to register with the facilitator would send an HTTP request
to a participating web server where one of the HTTP headers (e.g., a cookie
header) contains a crafted random string. The web server would be conﬁgured
with a secret key that lets it recognize the pattern in the HTTP request and
forward the request to the facilitator. The censor, who does not have the secret
key, cannot recognize that the request encodes a ﬂash proxy registration request.
If many web servers participate in this scheme then the censor’s only hope for
blocking these messages (besides blocking all web sites) is to attempt to compile
a list of cooperating sites to block, by crawling the web and trying to use each site
to rendezvous with the facilitator. If many web sites participate in this scheme,
then blocking all of them will cause considerable collateral damage.
To experiment with this approach, we modiﬁed the Apache web server to
accept rendezvous requests for the ﬂash proxy system. This modiﬁcation could
be packaged as an Apache module for ease of deployment. Users might discover
participating web sites through word of mouth or social media; or if there are
enough of them, a browser plugin could discover them automatically during the
course of normal web browsing. When the client wishes to register with the
facilitator it chooses a random web site that is participating in this rendezvous
mechanism. Next, it encrypts the message (032∥IPaddr) with that server’s public
key and embeds the resulting ciphertext in an HTTP request sent to the web
server. The ciphertext is embedded in a session cookie header that is normally
used when interacting with this web site. The censor cannot tell the diﬀerence
between a real session cookie and a rendezvous request, since both appear to be
random strings. The client uses this header in a request for a page that does not
exist (which can be generated by choosing a random string or a random English
word). When the web server receives an HTTP request with a session cookie
that results in a 404 response, the server tries to decrypt the contents of that
session cookie. If it detects the 032 pattern, it forwards the encoded IP address
to the facilitator. The server sends a 200 response so that the client knows that
the rendezvous request was successful. Note that the public-key decryption step
is only used on HTTP requests that result in a 404 response.
In our prototype, we used identity-based encryption (IBE) [25], in which a
server’s public key is its domain name. With IBE, clients do not need to use some
other mechanism to learn a server’s public key before using it for rendezvous.

256
D. Fiﬁeld et al.
7
Related Work
Blocking resistance is an ongoing area of research. It appears that there is no sin-
gle, simple solution, but rather many diﬀerent problems must be solved together
for eﬀective blocking resistance.
Simple proxy systems suﬀer from the problem that they typically reside on
static IP addresses and are expected to be relatively long-lived, making them
easy to block. It will always be a race between users striving to ﬁnd new usable
proxy addresses and the censor to block them. Additionally, single-hop proxies
cannot provide the same security properties that Tor can; for example they may
be susceptible to subversion of the proxy server itself.
Infranet [26] is a design to conceal traﬃc that would otherwise be blocked
within seemingly normal HTTP traﬃc. A user’s ordinary innocuous browsing
provides cover for messages sent and received to some other blocked server. In-
franet requires the cooperation of unblocked web servers. We note that Infranet’s
covert channel could be used as a method of rendezvous.
In some cases, the mere presence of statistical anomalies in traﬃc, such as an
encrypted stream, could be enough to cause a censor to block access to a resource,
merely on suspicion that the opaque stream may be used for circumvention.
This illustrates the need for protocol camouﬂage, making circumvention traﬃc
look like “normal” traﬃc, at least from the point of a censoring ﬁrewall. The
Tor Project made a proposal for pluggable transports [13], which allow using a
variety of diﬀerent camouﬂage or circumvention techniques, depending on what
is mutually supported by the client and entry node.
Recently Wustrow et al. introduced Telex [3], a system that allows cryp-
tographically “tagging” normal TLS streams so that an ISP-level router may
redirect it to a blocked destination. Telex is diﬀerent from other proposals in
that it involves action by entities in the middle of the network path, not only at
the edges. Unlike with Infranet, unblocked web sites do not need to know about
or participate in the circumvention.
The Tor Project has enhanced blocking resistance by introducing non-public
bridges in addition to public relays [5]. Bridge addresses go not into the main
relay database, but into a special bridge database. The set of bridges is par-
titioned and each partition is distributed over a diﬀerent channel, for example
email, HTTPS, or non-electronic means. The bridge database hands out only a
few bridge addresses at a time, with some additional restrictions making it hard
for one user to learn many or all of them. The goal is to prevent a scarce and
precious resource (bridge addresses) from being completely enumerated, while
still allowing anyone to learn the few addresses they need to get connected.
8
Conclusions
We have introduced ﬂash proxies, a new method of producing many short-lived
proxies that for the purpose of censorship circumvention. Rather than attempting
to hide the addresses of proxies, we aim to create so many that it is not feasible to

Evading Censorship with Browser-Based Proxies
257
block them all. Performance experiments are promising and the system is ready
to be deployed. The ﬂash proxy badge is installed on the project home page,
and visitors are already acting as ﬂash proxies. There are already suﬃciently
many active ﬂash proxies to provide intermittent service. The project code is
open source and available from https://crypto.stanford.edu/flashproxy/.
Acknowledgments. We are grateful for many helpful conversations on this
topic with Drew Dean, Pat Lincoln, Ian Schuler, and Vinod Yegneswaran; and
to Steve Beaty for help in testing.
The work is supported by the Defense Advanced Research Project Agency
(DARPA) and Space and Naval Warfare Systems Center Paciﬁc under Contract
No. N66001-11-C-4022. Any opinions, ﬁndings and conclusions or recommenda-
tions expressed in this material are those of the author(s) and do not necessarily
reﬂect the views of the Defense Advanced Research Project Agency and Space
and Naval Warfare Systems Center Paciﬁc. Distribution Statement “A:” Ap-
proved for Public Release, Distribution Unlimited.
References
1. The OpenNet Initiative: OpenNet Initiative Internet censorship data (2011),
http://opennet.net/research/data
2. Dingledine, R., Mathewson, N., Syverson, P.: Tor: The second-generation onion
router. In: Proceedings of the 13th USENIX Security Symposium (August 2004)
3. Wustrow, E., Wolchok, S., Goldberg, I., Halderman, J.A.: Telex: Anticensorship in
the network infrastructure. In: Proc. 20th USENIX Security Symposium (2011)
4. Ultrareach Internet Corp.: Ultrasurf proxy, http://www.ultrasurf.us/
5. Dingledine, R., Mathewson, N.: Design of a blocking-resistant anonymity system.
Technical Report 2006-1, The Tor Project (November 2006)
6. Kadianakis,
G.,
Mathewson,
N.:
Obfsproxy
architecture
(2011),
https://www.torproject.org/projects/obfsproxy
7. Weinberg, Z., Wang, J., Yegneswaran, V., Briesemeister, L., Boneh, D., Wang, F.
(StegoTorus: A camouﬂage proxy for the Tor anonymity system)
8. Tor Metrics Portal: Users (2011), https://metrics.torproject.org/users.html
9. Lewman,
A.:
Tor
partially
blocked
in
China
(September
2009),
https://blog.torproject.org/blog/tor-partially-blocked-china
10. McLachlan, J., Hopper, N.: On the risks of serving whenever you surf: Vulnera-
bilities in Tor’s blocking resistance design. In: Proceedings of the Workshop on
Privacy in the Electronic Society (WPES 2009). ACM (November 2009)
11. Wilde,
T.:
Knock
knock
knockin’
on
bridges’
doors
(January
2012),
https://blog.torproject.org/blog/knock-knock-knockin-bridges-doors
12. Winter, P., Lindskog, S.: How China is blocking Tor. Technical report, Karlstad
University (April 2012)
13. Appelbaum, J., Mathewson, N.: Pluggable transports for circumvention (October
2010), https://gitweb.torproject.org/torspec.git/blob/HEAD:/proposals/
180-pluggable-transport.txt
14. W3C: Cross-origin resource sharing (April 2012), http://www.w3.org/TR/cors/
15. Adobe Systems Incorporated: Adobe Flash Player 9 security (July 2008),
http://wwwimages.adobe.com/www.adobe.com/content/dam/Adobe/en/devnet/
flashplayer/pdfs/flash player 9 security.pdf

258
D. Fiﬁeld et al.
16. Tor Metrics Portal: Time in seconds to complete 5 MiB request (2012),
https://metrics.torproject.org/performance.html
17. Loesing, K.: torperf measurements-HOWTO (2011),
https://gitweb.torproject.org/torperf.git/
blob plain/HEAD:/measurements-HOWTO
18. Arlitt, M.F., Williamson, C.L.: Internet web servers: workload characterization
and performance implications. IEEE/ACM Transactions on Networking 5 (October
1997)
19. Paxson, V., Floyd, S.: Wide area traﬃc: the failure of Poisson modeling.
IEEE/ACM Trans. Netw. 3, 226–244 (1995)
20. Little, J.D.C.: A proof of the queuing formula L = λW (1960)
21. Rosenberg, J., Mahy, R., Matthews, P., Wing, D.: Session Traversal Utilities for
NAT (STUN). RFC 5389 (Proposed Standard) (October 2008)
22. Adobe Systems Incorporated: Real Time Media Flow Protocol (October 2008),
http://labs.adobe.com/technologies/cirrus/
23. M¨uller, A., Evans, N., Grothoﬀ, C., Kamkar, S.: Autonomous NAT traversal. In:
10th IEEE International Conference on Peer-to-Peer Computing (P2P) (2010)
24. W3C: WebRTC 1.0: Real-time communication between browsers (January 2012),
http://dev.w3.org/2011/webrtc/editor/webrtc.html
25. Lynn, B.: PBC library, http://crypto.stanford.edu/pbc/
26. Feamster, N., Balazinska, M., Harfst, G., Balakrishnan, H., Karger, D.: Infranet:
Circumventing web censorship and surveillance. In: Proceedings of the 11th
USENIX Security Symposium (2002)

Exploring the Ecosystem
of Referrer-Anonymizing Services
Nick Nikiforakis, Steven Van Acker,
Frank Piessens, and Wouter Joosen
IBBT-DistriNet, KU Leuven, 3001 Leuven, Belgium
firstname.lastname@cs.kuleuven.be
Abstract. The constant expansion of the World Wide Web allows users
to enjoy a wide range of products and services delivered directly to their
browsers. At the same time however, this expansion of functionality is
usually coupled with more ways of attacking a user’s security and privacy.
In this arms race, certain web-services present themselves as privacy-
preserving or privacy-enhancing. One type of such services is a Referrer-
Anonymizing Service (RAS), a service which relays users from a source
site to a destination site while scrubbing the contents of the referrer
header from user requests.
In this paper, we investigate the ecosystem of RASs and how they
interact with web-site administrators and visiting users. We discuss their
workings, what happens behind the scenes and how top Internet sites
react to traﬃc relayed through such services. In addition, we present
user statistics from our own Referrer-Anonymizing Service and show the
leakage of private information by others towards advertising agencies as
well as towards ‘curious’ RAS owners.
Keywords: referrer, anonymization, online ecosystem.
1
Introduction
In the infant stages of the Internet, privacy and anonymity were mostly unneces-
sary due to the small size of the online community and the public nature of the
available data. Today however, this has changed. People have online identities,
are connected to the Internet almost permanently and they increasingly store
their sensitive documents, photos and other data online in the cloud. Our new
online way of life provides interesting opportunities to those who seek to exploit
it. In an extreme case, corrupt regimes trying to ﬁnd out what their citizens are
doing and thinking, want to violate both online privacy and anonymity [9]. The
threat however, need not be a far-fetched scenario or exclusive to the paranoid:
large companies and organizations are also interested in the online habits of the
masses for various reasons, e.g. targeted advertising.
Projects like The Onion Router (TOR) [11,25] and the Invisible Internet
Project (I2P) [16] provide online anonymity to their users by routing Internet
traﬃc through a number of relays, thus making it harder for the endpoint to trace
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 259–278, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

260
N. Nikiforakis et al.
the source of the traﬃc. The application-layer however, on top of the network-
layer where TOR or I2P reside, could still carry information that can compromise
a user’s anonymity or privacy. This is especially so when a web-browser is used,
because browsers leak a wealth of information about their users. A study [13] by
the EFF’s Panopticlick project [1] shows that, based on data typically provided
by a browser, a web-site visitor can be uniquely identiﬁed in the majority of
cases. Private details can be extracted even in the cases where users utilize their
browsers’ private modes [2] or spoof their user-agent information [15].
One particularly sensitive piece of data, transmitted with almost every HTTP
request but commonly overlooked, is the referrer information in the ‘Referer’ 1
header, which can be used to trace the page where a visitor came from. Online
services known as Referrer-Anonymizing Services (RASs) scrub this referrer in-
formation from HTTP requests, providing both anonymity to web-sites hosting
links as well as privacy to users following those links. In this paper, we take a
closer look at RASs. We ﬁrst perform a manual analysis of popular RASs and
record their workings and architectural choices. Through a series of experiments
we approach RASs from three diﬀerent perspectives: the perspective of sites
utilizing RASs, the RASs themselves, and the destination sites receiving traﬃc
relayed through a RAS. In the ﬁrst experiment, we determine what type of sites
make use of a RAS and for what reason. The second experiment analyzes the
data that RASs have access to and whether they actually protect the privacy of
visitors and the anonymity of linking sites. In the last experiment, we observe
the reactions of popular web-sites when they are exposed to incoming links re-
layed through a RAS. From these experiments, we can conclude that in several
cases, user privacy is sacriﬁced for the linking site’s anonymity and that not all
RASs can be trusted with private data.
The main contributions of this paper are:
– Large-scale study of RASs and their common features
– Experimental evidence of privacy and anonymity violations from RASs
– Identiﬁcation of types of RAS users and the rationale behind their usage
– Analysis of third-party site responses towards traﬃc relayed through RASs
showing that RAS-relayed traﬃc is occasionally not well-received
The rest of the paper is structured as follows: In Section 2 we provide back-
ground information about referrers and how they can be used but also abused.
In Section 3 we describe how RASs work in general followed by a taxonomy of
30 real-world RASs in Section 4. In Section 5 we study how well these 30 RASs
protect the anonymity and privacy of their users. In Section 6, we categorize
users of RASs and their purpose based on data gathered by our own RAS. In
Section 7, we expose popular Internet sites to requests anonymized by RASs and
measure their behavior. We discuss related work in Section 8 and we conclude
in Section 9.
1 The correct spelling is ‘referrer’. The misspelled word ‘referer’ was introduced by
mistake by Phillip Hallam-Baker [17] and later adopted into the HTTP speciﬁcation.

Exploring the Ecosystem of Referrer-Anonymizing Services
261
2
Background
In this section we brieﬂy go over the workings of the referrer header and we list
some valid use-cases as well as abuse-cases for this header.
2.1
Referrer Header
In the HTTP protocol, all client-side requests and server-side responses have
headers and optionally a data body. At the client-side, each request contains
headers that, at minimum, ask for a speciﬁc resource from the web-server, in
a GET or POST manner and in the context of a speciﬁc web-site (Host), since
typically a single web-server serves more than just one web-site. On top of these
headers, browsers add a wide range of other headers, the most common of which
are headers specifying the user’s cookies towards a speciﬁc web-site, the user-
agent and the encoding-schemes accepted by the current browser.
An HTTP header that is less known but as present in requests as all afore-
mentioned headers, is the ‘Referer’. The HTTP referrer header is automatically
added by the browser to outgoing requests, and identiﬁes the URI of the resource
from which the current request originated [24]. For instance, if a user while being
on www.example.com/index.php?id=42, clicks on a link to www.shopping.com,
her browser would emit a request similar to the following one:


GET / HTTP /1.1
Host: www.shopping .com
User -Agent : Mozilla /5.0 (X11; Linux
i686)
Accept : text/html , application/ xhtml+xml
Proxy - Connection: keep -alive
Referer : http ://www.example .com/index.php?p=42


In this request, the user’s browser provides to www.shopping.com the exact lo-
cation of the page containing the clicked link, resulting in the request towards
their servers. This behavior is true not only when a user clicks on a link, but
also on all the non-voluntary requests that a browser automatically initiates
while parsing a page. For example, all requests created while fetching remote
images, scripts, cascading style sheets and embedded objects will contain the re-
ferrer header. The referrer header is traditionally omitted in one of the following
cases:(i) when users manually type a URI in their browser’s address bar, (ii)
when users click on an existing browser bookmark and (iii) when users are on a
HTTPS site and click on an HTTP link.
In HTML5, the web-programmer can add a special ‘noreferrer’ attribute to
selected anchor link tags that will cause the browser not to emit the referrer
header when these links are clicked [26]. At the time of this writing, from the
most popular three browsers (Mozilla Firefox, Google Chrome and Internet Ex-
plorer), Google Chrome is the only browser which supports this new ‘noreferrer’
attribute. We believe that this lack of browser support will only amplify the
hesitation of web-developers in trying and adopting new security/privacy mech-
anisms [31]. For this reason, we do not expect widespread use of this ‘noreferrer’
attribute any time in the near future.

262
N. Nikiforakis et al.
2.2
Referrer Use-Cases
In this section we provide a non-exhaustive list of legitimate uses of the HTTP
referrer for the web-server which receives the referrer-containing request:
– Advertising Programs: In many cases, a web-site will buy banner/link
placement space on more than one third-party web-sites. Using the referrer
header, the advertised site can assess the percentage of visitors coming from
each third-party site and use this information to either renew or cancel its
advertising contracts.
– CSRF Protection: Cross-site Request Forgery (CSRF) is a type of attack
where the attacker abuses the established trust between a web-site and a
browser [30]. In the typical scenario, a victim who has an active session
cookie with a web-application is lured into visiting a malicious site which
initiates arbitrary requests towards that web-application in the background.
The victim’s browser appends the session cookie to each request thus validat-
ing them towards the web-server. Due to the way this attack is conducted,
the referrer header in the malicious requests will not be the same as when the
requests are conducted by the user, from within the web-application. Thus,
a simple countermeasure against CSRF attacks is to allow the requests con-
taining the expected referrer header and deny the rest.
– Deep-Linking Detection: Deep-linking or ‘hotlinking’ is the practice of
linking directly to an object on a remote site without linking to any other
part of the remote site’s content. In practice, this behavior is unwanted, when
the object that is deep-linked was originally given to users after a series of
necessary steps (e.g. giving access to a music ﬁle after ﬁlling out an online
survey) [8]. By checking the referrer header before releasing the object, the
site can protect itself from users who did not go through the expected series
of steps. Unfortunately this approach can be easily circumvented by users
who change the referrer header values of their requests to match the expected
value of the remote site, using a modiﬁed browser.
– Access-Control: Using the same reasoning as in deep-linking, a site can
enforce access control to individual pages by making sure that the visiting
user arrives there only from other selected destinations. This technique is
also used to provide special oﬀers when a site is visited from another site
that would normally not be visible to regular users. Wondracek et al. [28]
discovered that this technique is used by traﬃc brokers in adult-content web-
sites. As in the previous use case, this sort of access-control can be bypassed
by a user with malicious intent.
– Statistics Gathering: In large and complex web-sites, the web-developers
seek to understand whether the content layout is facilitating the user into
ﬁnding the data that they need. Through the use of the referrer, web-
applications can track users between pages (without the need for cookies)
and ﬁnd the most common visitor paths.

Exploring the Ecosystem of Referrer-Anonymizing Services
263
2.3
Referrer Abuse-Cases
The same referrer information that can be used for legitimate reasons, can be
abused by web-site operators to assault a user’s privacy and in certain cases even
perform user impersonation attacks.
– Tracking Visitors: Traditionally, users associate tracking with tracking
cookies. A web-site that wishes to track its users between page loads, or
collaborating sites that wish to track users as they transition from one to
the other can do so through the referrer header even if users delete their
cookies on a regular basis.
– Session Hijacking: As described in Section 2.1, the referrer header contains
not only the domain of the page where each request originated but the
full URI of that page. That becomes problematic when web-sites use GET
parameters to store sensitive data, as is the case in sites that add session
information to URIs instead of cookies. In this case, the HTTP referrer will
contain the session identiﬁer of the user on the originating site, allowing
a malicious operator of the target web-site to impersonate the user on the
originating site [19].
– Sensitive-Page Discovery: It is common for web-developers to hide the
existence of sensitive ﬁles and scripts in directories that are accessible from
the Web but are not linked to by any visible page of the web-site. These
ﬁles or directories sometimes have obfuscated names to stop attackers from
guessing them. Penetration-testing tools such as DirBuster 2 that attempt
to guess valid directories using dictionary and brute-force attacks, attest
towards this practice. In such cases, if one of the sensitive pages contains
an external link or external resource, the exact location of that page can be
leaked to the remote web-server through the referrer header.
3
Referrer-Anonymizing Services
In Section 2 we described the possible uses and abuses of the HTTP referrer.
Given the wealth of information that the referrer header provides, one can think
of many scenarios where the user doesn’t want to release this header to remote
web-servers.
Today, users can achieve this either by conﬁguring their browser not to send
the referrer header, or through the use of Referrer-Anonymizing Services when
clicking on links from sensitive web-pages. While the former approach is available
on many modern browsers, it works as an all-or-nothing setting in the sense that
the user cannot selectively allow the transmission of the referrer header. This can
be problematic when a user navigates to web-applications that use the referrer
header as a CSRF countermeasure. In such cases, the user wouldn’t be able to
use the web-application, unless they re-enable the transmission of the referrer
header. An additional problem is for web-site owners that wish to link to third-
party sites but not at the expense of uncovering their identity. A controversial
2 http://sourceforge.net/projects/dirbuster/

264
N. Nikiforakis et al.
1: GET page.html 
2: OK <html>… 
3: GET /red.php?u=www.example.com 
4: OK <html>… 
secret.com 
RAS 
example.com 
5: GET / 
Fig. 1. HTTP messages involved in the use of a Referrer-Anonymizing Service
but popular example are ‘warez forums’3, where the descriptions of the pirated
software or multimedia are usually given by linking back to the legitimate web-
sites. In these cases, the web-site operators cannot rely on privacy-aware users,
but must use a solution that will seamlessly work for all. This can be achieved
through the use of Referrer-Anonymizing Services.
A Referrer-Anonymizing Service (RAS) is a web-service that is responsible for
anonymizing the referrer header of a user before that user reaches a remote web-
server. Note that, for security reasons, a web-site is not allowed to arbitrarily
change a user’s referrer header. The referrer header is created and emitted by
the browser and thus the only way to anonymize this header is for the RAS
to place itself between the site that links to an external resource, and that
resource. By doing so, the RAS appears in the referrer header of the user’s
browser, instead of the original web-site, thus eﬀectively anonymizing the original
web-site. This technique is conceptually similar to the anonymizing techniques
applied by Crowds [23] and TOR [11] where instead of the user’s IP address, the
link-embedding web-site is hidden.
Figure 1 shows the series of steps involved when using a RAS. In steps 1 and 2,
a user requests and receives a page from secret.com. This page has a link that, if
clicked, will eventually lead to example.com. However, since secret.com wishes
to remain anonymous, it uses a RAS instead of linking directly to example.com.
In step 3, the user clicked on the link expecting that it leads to example.com.
However, the link creates a GET request towards a RAS with example.com as its
argument. In response, the RAS generates a page (step 4) that will automatically
redirect the user to example.com either directly, or after a timeout. In both cases,
as far as the user’s browser is concerned, the ﬁnal request towards example.com
originated not from secret.com but from the web-site of the RAS. Thus, the
request depicted in Step 5, will have the redirect-causing web-page of RAS as
its referrer, eﬀectively hiding the original source of the link. Note that in the
aforementioned process, secret.com will still reveal its presence to an external
entity, but it chooses to reveal itself to the RAS instead of example.com.
3 ‘warez’ is slang for pirated software.

Exploring the Ecosystem of Referrer-Anonymizing Services
265
The RAS can redirect the user’s browser to example.com using one of the
following ways:
– HTTP MOVE messages: When a web-server receives a request for a
resource, it can emit a 301/302 HTTP MOVE message, that informs the user’s
browser of the ‘move’ of the resource, and provides it with the new location.
Upon the receipt of such a message, a browser automatically initiates a new
request towards the instructed location, thus completing the redirect.
– HTML Meta-Refresh tag: One tag of the HTML speciﬁcation allows the
web-developer to ‘refresh’ a page after a conﬁgurable number of seconds.
The refresh can load the same page, or a new one. For example, <meta
http-equiv="refresh" content="5;url=http://www.example.com"> in-
structs the user’s browser to replace the current page with the main page of
example.com upon the expiration of 5 seconds.
– JavaScript: The same eﬀect can be achieved using JavaScript, by setting
the value of the window.location property to the desired site.
4
Taxonomy of RASs
In this section we analyze and compare common features of real-world Referrer-
Anonymizing Services. We obtained a list of 30 functional RASs (listed in the
appendix) by using a well-known search engine and searching for phrases related
to their services, such as ‘referrer anonymization’ and ‘hiding referrer’. The popu-
larity of some of these services is evidenced by their high ranking in the Alexa top
sites list. For instance, the most popular RAS, anonym.to, currently ranks higher
than well-known sites such as blackberry.com and barnesandnoble.com. We
summarize the discovered features of the studied RASs in Table 1.
4.1
Redirection Mechanism
By manually visiting and recording the redirection mechanisms of the 30 RASs,
we found out that 73% of them were redirecting their users using the meta-refresh
mechanism, 20% using JavaScript and 7% using a combination of both 302 and
meta-tags. The use of the HTML meta-refresh is the most common mechanism
because it doesn’t require JavaScript to execute and because it allows the RAS to
delay the redirect in order to show advertising banners to the visiting users. The
sites that used JavaScript to redirect a visitor, used it together with a timeout
function to emulate the eﬀect of the meta-refresh mechanism.
The HTTP MOVE messages were the least used among the services for the two
reasons. Firstly, redirects occurring through a 301/302 HTTP message retain the
original referrer header and are thus not suited for use from RASs. The services
that did utilize them, always combined them with a meta-header, where the 302
message would redirect the user to another page on the RAS’s web-site which
would then use a HTML meta-refresh tag. Secondly, even if the browser would
clear out the referrer header, the HTTP MOVE mechanism doesn’t allow for a
delayed redirect, thus the services cannot use it to show advertisements.

266
N. Nikiforakis et al.
Table 1. Common features of Referrer-Anonymizing Services
Common Feature
Percentage of RASs
Redirection:
HTML meta-refresh
73%
JavaScript
20%
HTTP MOVE+ meta-refresh
7%
Ads
36.66%
Mass Anonymization
50%
An interesting observation is the diverse redirection behavior that diﬀerent
browsers display. When using Mozilla Firefox (version 9) and Google Chrome
(version 16), a redirect implemented through JavaScript retains the referrer that
caused the redirect. That is not a problem for RASs since the page that causes
the redirect is not the original page that wishes to remain hidden, but a page of
the RAS (step 4 in Figure 1). On the other hand, the same redirection mechanism
in Microsoft’s Internet Explorer 8, clears out the referrer. Contrastingly, Firefox
and Internet Explorer clear out the referrer header in case of a HTML meta-
refresh but Chrome still retains the redirect-causing referrer. From a point of
user privacy, the complete clearing of the referrer is the best option for the user
since the web-server cannot distinguish between users coming from web-sites that
protect themselves and users who typed in the URIs or clicked on their browser
bookmarks. However, the same mechanism that protects a user’s privacy may
negatively aﬀect a user’s security, as later explained in Section 8.
4.2
Delay and Advertising
Since all RASs that we encountered were providing their redirection services for
free, there is a high probability that they attempt to capitalize on the number
of incoming users through the use of advertising. From our set of 30 services,
11 (36.66%) were displaying advertising banners to the users waiting to be redi-
rected to the destination web-site. From these services, 10 of them were con-
structing advertisements on the ﬂy (through the use of client-side scripting and
a chain of redirections) and only one had the same banners, statically embed-
ded in its web-site. We also noticed that the sites that included advertising had,
on average, a higher delay than the non-advertising web-sites which sometimes
didn’t delay the user at all. More speciﬁcally, the RASs with advertisements
were redirecting the user after an average delay of 11.8 seconds whereas the non-
advertising RASs were redirecting the user after an average delay of 2 seconds.
An interesting observation for the RASs that create dynamic advertisements
is that all the requests towards the advertising agencies contain a referrer header
which is the URI of the RAS page where the user is waiting to be redirected
to the destination site. Since all RASs work by receiving the destination URI
(example.com in Figure 1) as a GET parameter, the various advertising agencies
get access, not only to the IP addresses of the RAS visitors, but also to their
eventual destination. By combining this knowledge with other data, they may

Exploring the Ecosystem of Referrer-Anonymizing Services
267
be able to associate users with sites, even if the destination web-site doesn’t
collaborate with a speciﬁc advertising agency. Thus, in one third of the cases,
the privacy of individual users is sacriﬁced for the anonymity of the linking site.
4.3
Mass Anonymization
Most RASs have a simple API for use of their services. All a RAS needs, is a
remote URI to which it will redirect users while clearing, or substituting, their
referrer header. For this reason, all RASs work in a stateless fashion. Unlike URL
shortening services, where a user needs to ﬁrst visit the service and generate a
new short URL for their long URL, a user can utilize a RAS without ﬁrst visiting
the RAS’s web-site. In our example in Figure 1, the administrator of secret.com
can create an anonymized-referrer link to example.com simply by making a GET
request with example.com in the u parameter.
This stateless nature of RASs allows for mass-anonymization of links with-
out the hassle of registering each and every link to a remote service. From the
30 RASs that we analyzed, 50% were providing a mass-anonymization option
through the use of an anonymizing script. This script, which is supposed to be
included by remote web-sites, iterates over all <a> elements of the current HTML
page and converts all links to RAS-links. Additionally, the scripts usually pro-
vide a white-list option where domains that do not need be anonymized (such
as the links to local pages within a web-site) can be listed and excluded from
the anonymization process. While we didn’t encounter a misuse, a site including
a remote anonymizing script is implicitly trusting the RAS providing it to not
include malicious JavaScript along with the anonymizing functionality.
4.4
Background Activity
Apart from advertisements, RASs can use the browsers and IP addresses of
visiting users to conduct arbitrary requests before redirecting the users towards
their ﬁnal destination. This activity can range all the way from harmless but
unwanted to malicious. In our analysis of 30 RASs, we found that 2 services
were performing unexpected actions that were hidden from the user.
The ﬁrst service had embedded an invisible iframe that performed a search
request with the keyword ‘myautopass’ using Google Search. While the RAS
cannot steal any private data from that iframe (since the Same-Origin Policy
disallows such accesses), the requests were made by the user’s browser and user’s
IP address. As far as Google Search is concerned, tens of thousands of people 4
search for that word on a daily basis, an action which most likely aﬀects the
ranking and trend of that keyword, even if the shown links are never clicked.
The second service, instead of redirecting the user to the desired web-site,
created a frameset with two frames. In the ﬁrst frame, which spanned the entire
page, it loaded the requested site and on the second frame it loaded a local page
of that RAS. In this scenario, while the user gets access to the remote page,
they never actually leave the site of the RAS. This ‘sticky’ behavior is common
4 We obtained the RAS’s estimated number of daily visitors using quantcast.com

268
N. Nikiforakis et al.
in anonymizing web-proxies which request a page from a remote server on the
user’s behalf and then present the result to the user. To the remote web-server,
the request appears as coming from the anonymizing proxy’s IP address, thus
hiding the user’s IP address. Note however that in the case of RASs, this ‘sticky’
behavior adds no more privacy to the visiting user, since the requests are all
made from the client-side and thus using the user’s browser and IP address.
By analyzing the contents of the second frame, we observed that through a
series of redirects, the site was opening the registration page of a ﬁle-hosting
web-site, with a speciﬁc aﬃliate identiﬁer. It is unclear how the operators of
that RAS were expecting to ﬁll in the registration page in an invisible frame but
this method hints towards the RAS’s attempt to capitalize on visiting users for
monetary gains in aﬃliate programs.
5
Information Leakage
Whenever a user utilizes an online service, there is always the possibility that
the service will retain information from the user’s activity and use that infor-
mation in the future, possibly for ﬁnancial gains. In Section 4.2 we showed that
36.66% of all tested Referrer-Anonymizing Services used ads as a way of getting
monetary compensation for their free services. In almost all cases, the ads were
created dynamically, initiating a GET request for a script or an image from the
RAS to the advertising company. Through the referrer header, these requests
reveal to the advertising agencies which page the user is on (the RAS waiting
page) and the page that the user intends to go to (the destination GET argument
given to the RAS). This is problematic for two reasons: ﬁrst, if the destination
URI contains a secret parameter tied to a speciﬁc resource (e.g. a session iden-
tiﬁer, a ﬁle identiﬁer for ﬁle-hosting services or a document identiﬁer for online
collaboration platforms) this identiﬁer will be disclosed to an untrusted third
party (the advertising agency). The second reason is that advertising agencies
gain more information about users and the sites they visit even if the destination
sites do not collaborate directly with them.
Another instance of the same problem is encountered between the source site
and the RAS. The administrator of a RAS is able to view the referrer headers of
traﬃc towards their service and can thus discover the original pages that relay
visitors through them (e.g secret.com in Figure 1). If the source site hosted the
link to the RAS on a page with sensitive data in its URL (both path and GET
parameters) – e.g. secret.com/admin.php?pass=s3cr3t – this will be available
for inspection to the utilized RAS.
In order to measure whether the various advertising agencies of RASs make
use of users’ referrer headers and whether the administrators of RASs access
sensitive source pages, we conducted the following experiments.
5.1
Experimental Setup
We ﬁrst created and registered fileleaks.co.cc, a web-site supposedly provid-
ing leaked sensitive documents and then developed two crawlers that visited all

Exploring the Ecosystem of Referrer-Anonymizing Services
269
the RASs daily and requested a redirection towards URIs within our site. e.g.
http://anonym.to?http://fileleaks.co.cc/index.php?filename=[POPULA
R TOPIC]&dlid=[PER RAS UNIQUE ID]. The dlid contained a random identiﬁer
that was unique for all tested RASs and allowed us to accurately detect which
RASs leaked our destination site.
The ﬁrst crawler was simply requesting the URI in a wget-like way and pro-
ceeding to the next RAS. In this case, our destination URI could be leaked only
through the web-server logs of the target RAS since no scripts or images were
rendered. The second crawler was actually an instrumented instance of Firefox
that automatically visited each site and waited for 10 seconds before moving on
to the next target. The key diﬀerence between the two crawlers is that the latter
one was a functional browser which executed all needed image and JavaScript
requests to fully render each page. These two crawlers allowed us to roughly
distinguish between URIs leaked by the web-administrator of a RAS and URIs
leaked through the referrer header sent to advertising agencies.
To detect RAS administrators looking for sensitive pages in the referrer head-
ers of their logs, we added a fake password-protected administrative panel to
our site and programmed an additional wget-like crawler which constantly vis-
ited all RASs, pretending that the request for anonymization was originat-
ing at http://fileleaks.co.cc/admin/index.php?password=[SECRET PASS]
&pid=[PER RAS UNIQUE ID]. The fake administrative script was logging all ac-
cesses and the pid GET parameter was used to distinguish leakage between the
tested RASs as in our ﬁrst set of crawlers.
5.2
Results
Leakage
of
Destination
Site.
In a 30-day period our monitors on
fileleaks.co.cc recorded a total of 250 ﬁle requests using unique URIs that
were made available to the Referrer-Anonymization Services as destination sites.
By decoding each URI we identiﬁed that the URIs were leaked by 3 diﬀerent
RASs. Interestingly, all the recorded URIs were communicated to the services
through our instrumented Firefox crawler and not through the wget-like crawler,
implying that the URIs were most likely leaked by subsequent JavaScript and
image requests of each RAS-waiting page. For privacy and ethical reasons, we
do not identify the services by name and we refer to them as RAS1, RAS2 and
RAS3.
The unique URI of RAS1 was found in 1.2% of the requests. It appears that
the service leaked the URIs directly to a speciﬁc search engine, which at a later
time requested the ﬁles with the exact parameters originally provided to RAS1.
RAS2 and RAS3 were both leaking the requests towards services running on
Amazon’s Elastic Compute Cloud. The requests leaked by RAS2 (88.8% of the
total requests) were revealing, through their user-agent, that they were crawlers
working on behalf of a speciﬁc advertising company which specializes in iden-
tifying which pages would prove to be the best ones for ad placement for any
given product or service. The last set of requests (10% of the total) with leaked

270
N. Nikiforakis et al.
identiﬁers from RAS3 were also originating from hosts on Amazon’s Cloud but
their user-agent didn’t provide any identifying details.
In total, our experiment showed that 10% of the tested RASs were, knowingly
or not, leaking the referrer-header of their users to third-party advertising agen-
cies who were recording them and using them at a later time. Given the risks
associated with the leakage of a visiting user’s IP address and destination site,
we believe this to be a signiﬁcant privacy risk.
Leakage of Originating Site. In the same period, our administrative-panel
monitor recorded three visits from the same visitor. In the third visit, the user
provided the exact password and pid combination that our third crawler was
providing one of the 30 tested RASs through its referrer header. It is important
to realize that given the nature of our crawler, the referrer header containing
the fileleaks.co.cc administrative panel URI (and password as GET param-
eter) could only be retrieved from the RAS web-server’s logs since the pages
were always retrieved but never rendered in a real browser. Thus, no advertising
agencies, or legitimate Web traﬃc scripts could ever access our referrer header.
This shows that the administrator of one of the thirty RASs was actively search-
ing the logs of the RAS in an eﬀort to identify ‘interesting’ source sites and then
manually inspect them.
6
User Categorization
In the previous section, we explored the various features of RASs and recorded
which traits are prevalent and for what reasons. While this gives us an in-
sight of the motives behind these services, it doesn’t provide any speciﬁc de-
tails on the users who use these services or the actual reasons justifying their
usage. In order to investigate the user-part of the RAS ecosystem, we created
and advertised our own Referrer-Anonymizing Service, which we made available
at www.cleanref.us. In a period of 25 weeks, our service received a total of
2,223 requests for referrer-anonymization. Figure 2 shows that the weekly usage
of our service varied signiﬁcantly. In the next sections we describe a subset of
these requests according to their purpose.
6.1
Ethical Considerations
The data that was collected for this experiment are the following: For each
request we recorded i) its timestamp ii) the IP address of the host performing the
request, iii) its GET parameters and iv) the referring host. These were collected
in a single text ﬁle on the web server, in a password-protected directory that
only the authors of this paper had access to.
The data collected is a subset of the data collected by every web server on
the web in standard server logs. Many web developers even share this collected
information with third parties, such as Google Analytics, for the purpose of
gathering usage statistics. The reason for deploying our own RAS was to identify

Exploring the Ecosystem of Referrer-Anonymizing Services
271
0
100
200
300
400
500
600
700
800
Number of Anonymization Requests 
Fig. 2. Weekly number of anonymization requests that our RAS received during our
study
potential abuse of such services. Since reporting to users up front that they would
be part of an experiment would defeat the purpose of the experiment, our RAS
did not explicitly warn users of the data collection. Because of the nature of the
data collected, and the fact that these data are collected by every web server,
we believe this lack of user warning to be acceptable.
We are not planning on releasing the data to the general public, and we will
delete all data after publication of the paper.
6.2
Hiding Advertising Infrastructures
The greatest part of the ﬁrst peek of Figure 2 corresponds to 317 requests con-
ducted by 127 unique IP addresses. By analyzing our data, we discovered that
for two days in the 35th week of 2011, our service was part of two advertis-
ing campaigns and the requests were from users who were the targets of these
campaigns. While a signiﬁcant number of countries was involved, the campaigns
seem to have been targeting users mostly in Vietnam and the US, since these
countries received 31.49% and 16.53% of the total traﬃc, respectively. It is un-
clear whether the advertising campaign was conducted through spam emails
or through browser pop-up windows, however the fact that the advertiser used
a Referrer-Anonymizing Service shows that they wished to conceal the exact
method of driving traﬃc by hiding the referrer header from the ﬁnal web-servers.
In our referrer logs for those days, we found 3 types of links. One type of
link, was used to drive traﬃc directly from our service towards the destination
web-site. The second type of link, was a chain of RASs all connected together in
a way that allowed each RAS to redirect the user to next one, until the user is
redirected to their ﬁnal destination. For example, when the link:


http :// cleanref .us/?u=http ://www.refblock .com?http :// cloakedlink.com
/ zqkzqrfvgs



272
N. Nikiforakis et al.
is clicked (or opened in a pop-up) by a user, her browser will request a page from
cleanref.us which will redirect her to refblock.us which in turn will redirect
the user to cloackedlink.com. cloackedlink.com is the end of the chain and
when resolved, it will redirect the user to the actual site. The combination of
multiple RASs allows the advertiser to hide its presence behind multiple layers
of indirection. For instance, using this method, an inspection of the referrer
does not reveal whether cleanref.us was the ﬁrst part of a chain or whether
another RAS redirected the user to our service using a redirection method that
completely cleaned the referrer header. The last type of link, pointed to a service
that received an obfuscated string as its only parameter:


http :// linkfoobar.net/track -link.php?id= aHR0cDovL3d3dy5jbGVhbnJlZi51c
y8/ dT1odHRwOi8vd3d3LnJlZmJsb2NrLmNvbT9odHRwOi8vY2xvYWtlZGxpbmsuY29tL3
pxa3pxcmZ2Z3M=


The id argument passed to the anonymized linkfoobar.net web-site is a
Base64-encoded string that, when decoded, makes a chain of RASs similar to
our previous example. As the name suggests, this is most likely the ﬁrst part of
the chain where the advertiser ﬁrst tracks that someone clicked on a tracked link
and then proceeds to redirect the user to the ﬁnal destination through a chain
of RASs.
By combining the knowledge of all three categories of referrer URIs found in
our logs, it is evident that the advertiser mixes the order of RASs in their chains
in order to reveal only part of their traﬃc and infrastructure to each RAS. Thus
in some cases, our service was the last in the chain of RASs, sometimes in the
middle and occasionally the ﬁrst service that began the chain of RASs after the
user’s click was registered by linkfoobar.net.
6.3
Remote Image Linking
Throughout our experiment we noticed several groups of requests (e.g. weeks
42-11, 04-12 and 05-12 in Figure 2) towards image ﬁles, some of them located
on popular image hosting sites. By sampling some of the destination URLs, we
noticed that the requested images were almost always of an adult nature. For
the sampled requests, we also reviewed the referrer header when it was available.
The observed linking sites fell into two categories. In the ﬁrst category, the sites
linking to adult images through our RAS were forums where the users could
write new posts and include links to images. In this case, the RAS was added
to hide the linking site from the image hosting site, since the uploaded images
didn’t conform with the rules of the latter. Some of the requests were using more
than one RAS chained together as shown in Section 6.2.
In the second case, the linking sites were personal sites that were request-
ing images either from image hosting sites or directly from adult-content sites
forming a client-side image collage. As in the former category, the owners of
such pages were hiding the exact location of their personal pages from the sites
hosting the linked images.

Exploring the Ecosystem of Referrer-Anonymizing Services
273
6.4
Web-Mashups
A web-mashup is a combination of data and functionality from more than one
remote service, which has more value than any one of the remote services by
itself. The highest peek in Figure 2 stems from the adoption of our service from
a book price-comparison web-application. The destinations of these requests are
popular online bookstores and other price-comparison sites in Europe and the
US. Each request contained the ISBN number of a diﬀerent book. In a period of
9 weeks, the web-application initiated a total of 1,273 requests for anonymization
of which 1,198 (94.10%) formed a unique combination of ISBN number and third-
party service. Given the vast majority of unique queries for books, we believe that
the requests happen once and their results are cached in the web-application.
The usage of a RAS in between the book price-comparison web-application and
the other online bookstores, allows the former to retrieve information from the
latter without revealing its location or purpose.
7
Tracking of Anonymizers
In the previous sections, we analyzed the existing Referrer-Anonymizing Ser-
vices, we listed some ways that one can legitimately or illegitimately use them
and provided empirical data on the types of users that they attract. The ﬁnal
part of the RAS ecosystem are the destination sites (example.com in Figure 1).
It is interesting to know, whether popular web-sites are aware of the existence of
RASs and if they are, how do they react towards traﬃc relayed through them.
In order to identify diﬀerences in served content, we conducted an automated
experiment involving the top 1,000 sites of the Internet according to Alexa. The
ﬁrst time we visited each Alexa link, we provided the URL of a popular search
engine as our referrer header, simulating a user who followed a search result
from that search engine. We then repeated the same request 30 times, each time
providing as a referrer, the URL of one of our 30 RASs.
Given the dynamic nature of the Web, simply comparing the pages of diﬀerent
visits or their hashes is insuﬃcient to diﬀerentiate between changes that were
due to the web-site’s reaction to a certain referrer header and usual expected
changes, such as diﬀerent timestamps or randomly generated data embedded
in the source-code of the page. In order to overcome this obstacle we used a
combination of two methods. The ﬁrst method was to apply Arc90’s Readability
algorithm [3] which attempts to separate and show the most important content
on a page while hiding the less important. In the second method, we recorded the
number and type of HTML input elements present on the page. The rationale
behind the second method was that, even if a page legitimately changes between
successive visits, the number of visible and invisible input elements should not
change. If any one of the two methods provided diﬀerent results between the ﬁrst
search-engine-referred visit of the site and any of the the RAS-referred ones, the
site and its HTML code was recorded and the results were manually veriﬁed.
From a total of 1,000 web-sites we discovered that three of them were using the
referrer header to provide radically diﬀerent content. The ﬁrst, facebook.com,

274
N. Nikiforakis et al.
Fig. 3. Facebook’s main page when visited through a speciﬁc RAS
was serving a very diﬀerent page when our requests claimed to come from one
of the 30 studied RASs. By manually checking the resulting page, we realized
that instead of Facebook’s usual login-screen, we received a page that alerted
us that we had most likely been a victim of a phishing attack, and was inviting
us to start the procedure of resetting our password (Figure 3). Interestingly,
facebook.com reacted this way only when the referrer header was coming from
that speciﬁc RAS and provided the normal content for the remaining 29 RASs.
This could mean that Facebook was reacting to a real phishing attack where the
attackers were trying to hide the location of the phishing page by redirecting
their victims back to facebook.com through that speciﬁc RAS after the user
credentials had been stolen.
The second web-site was a photo gallery where users can upload pictures and
retrieve links that they can later use in various web-sites such as forums and
blogs. When the site was visited through one of the 30 RASs, instead of giving
us access to its main page, it provided us with a GIF image that stated that the
site had blocked access to the linking page. This picture will be provided to any
image requests that pass through that RAS. This veriﬁes the behavior that we
discovered in our own RAS, where visitors were linking to adult content uploaded
to generic image galleries and hiding the linking site through the anonymization
of their referrer header. The third site was a general information portal which
was consistently providing a 403 HTTP ‘forbidden’ error when visited through
a speciﬁc RAS, the same RAS blacklisted by the image gallery site.
The above experiment shows that even though only a small fraction of the
tested web-sites reacted visibly to speciﬁc referrers, their behavior was always
‘negative’ when the referrer appeared to be a RAS. This attests towards the
argument that RASs are associated more with illegal activity and less with a
legitimate user’s privacy concerns.
8
Related Work
Although the practice of cleaning the ‘Referer’ header through a RAS is common
knowledge, we are unaware of any research into the operation or usage of these

Exploring the Ecosystem of Referrer-Anonymizing Services
275
services, with regard to online privacy and anonymity. The related work that we
are aware of, falls in the following 4 categories:
Online privacy and anonymity. Online privacy and anonymity are important
for numerous reasons. The Internet was not built to provide anonymous commu-
nication mechanisms, which lead to the creation of various projects that provide
forms of anonymous networking. The Onion Router (Tor) [25] project and the In-
visible Internet Project (I2P) [16] are the most famous of these networks.
Experiments. It is easy to state that a security problem is real and danger-
ous. Providing evidence backing up this claim is often diﬃcult since it involves
covertly tracking the behavior of attackers and victims in an ethical way.
In our experiments in Section 5, we used enticing links to lure visitors to our
own ﬁleleaks.co.cc in a honeypot-like way. Honeypots [22] have been traditionally
used to study attacking techniques and post-exploitation trends. Yuil et al. [29]
introduce Honeyﬁles as an intrusion detection tool to identify attackers. Hon-
eyﬁles are bait ﬁles that are stored on, and monitored by, a server. These ﬁles
are intended to be opened by attackers and when they do so, the server emits
an alert. Similarly, Bowen et al. [7] use ﬁles with ‘stealthy beacons’ to identify
an insider thread. We have used these techniques in the past to detect whether
attackers are violating the assumed privacy in ﬁle-hosting services [20].
Referrer abuse. Referrer headers were designed to identify which URL a visitor
is coming from. This information is of great beneﬁt to content-providers because
it can provide some insight in the browsing habits of visitors, as discussed in
Section 2.2. The referrer data however, is optional and can be spoofed (e.g.
RefSpoof [12]), prompting the inception of referrer spam [27]: sending the URL
for a third-party web-site in the referrer header so it will show up in the logs of
a visited web-site.
Because of their use to track visitor movements, everyone naturally expects
referrer headers to contain URLs. This expectation can result in the development
of a web-application which displays the contents of the referrer header, without
suﬃcient sanitization. In such an application, the referrer header can be abused
to carry an XSS attack as documented in [14].
Many Cross-site Request Forgery (CSRF) countermeasures depend on the
referrer header to determine whether a request was sent from a trusted location.
The authors of some of these CSRF countermeasures, aware of visitors that
disable the transmission of referrer information, will implement lenient referrer
validation [5], which will allow requests without referrer header in order not to
break the web-application that is being protected. This deliberate loophole allows
an attacker to launch a CSRF attack by relaying a request from an untrusted
location through a RAS, which will remove the referrer information. Because
the request in this attack has no referrer information, it is allowed by the CSRF
countermeasure and the attack can succeed.

276
N. Nikiforakis et al.
Solutions dealing with referrers. There are only two parties that can beneﬁt
from non-disclosure of referrer information: the visiting browser and the author
of the web-site on which a link is hosted. The referrer-leakage problem can thus
be solved by either party.
Referrer-Anonymizing Services attempt to solve privacy and anonymity issues
that arise because a visitor’s browser is leaking information through the referrer
header by design. The author of a web-site linking to an external web-page used
to not have any other means to prevent the referrer header from exposing their
web-site. One way the author of a web-site could prevent the referrer header from
exposing their web-site, was to host their web-site using HTTPS. The HTTP
protocol speciﬁcation [24] advises that referrer information should not be sent
when navigating from an HTTPS web-site to an HTTP site. However, browsers
are free to preserve the referrer if the destination is also an HTTPS web-site,
even if the destination site is situated on a diﬀerent domain.
Recognizing the need for a better solution, the WHATWG has included the
‘noreferrer’ link type [26] in the HTML5 speciﬁcation. By annotating certain
HTML elements with this link type, a web-page author will prevent referrer
information from leaking when clicking the annotated link. RASs protect the
web-site’s anonymity as much as they protect a visitor’s privacy. Therefore it
makes sense for an Internet user to disable referrer information to safeguard that
privacy at the source. Many modern web-browsers provide means to disable
referrer header transmission [18,6,21]. For other browsers, the referrer can be
ﬁltered out using a client-side proxy like e.g. Privoxy [10]. Due to the privacy
problems associated with the referrer header, the ‘Origin’ header [4] has been
proposed because it only leaks the origin (scheme, hostname and port number)
of a URL to a remote web-site instead of the full URL.
9
Conclusion
In this paper, we explored the ecosystem of Referrer-Anonymizing Services and
classiﬁed their functionality, their user-base and their abuse. We showed that
in several cases, RASs were taking advantage of their position and leaked pri-
vate user information to advertising companies. Conversely, we discovered that
users were occasionaly using RASs to hide illegal or unethical activity and we
revealed that some popular Internet sites do not respond well to RAS-relayed
traﬃc. Overall we showed that, while protecting a user’s privacy through the
anonymization of the referrer header is desirable, not all RASs are equally noble
and thus care should be taken when choosing one. At the same time, browser
developers have the responsibility to facilitate a complete migration away from
such services through the support of privacy-preserving HTML5 tags.
Acknowledgments. This research is partially funded by the IBBT, the Re-
search Fund K.U.Leuven, the B-CCENTRE, the EU-funded FP7 project NES-
SoS and the IWT project SPION.

Exploring the Ecosystem of Referrer-Anonymizing Services
277
References
1. Panopticlick, http://panopticlick.eff.org/
2. Aggrawal, G., Bursztein, E., Jackson, C., Boneh, D.: An analysis of private brows-
ing modes in modern browsers. In: Proc. of 19th Usenix Security Symposium (2010)
3. Readability — Arc90 Lab, http://lab.arc90.com/2009/03/02/readability/
4. Barth, A.: The Web Origin Concept,
http://tools.ietf.org/html/draft-abarth-origin-09
5. Barth, A., Jackson, C., Mitchell, J.C.: Robust defenses for cross-site request forgery.
To appear at the 15th ACM Conference on Computer and Communications Secu-
rity, CCS 2008 (2008)
6. Beverloo, P.: List of Chromium Command Line Switches, –no-referrers,
http://peter.sh/experiments/
chromium-command-line-switches/#no-referrers
7. Bowen, B.M., Hershkop, S., Keromytis, A.D., Stolfo, S.J.: Baiting Inside At-
tackers Using Decoy Documents. In: Chen, Y., Dimitriou, T.D., Zhou, J. (eds.)
SecureComm 2009. LNICST, vol. 19, pp. 51–70. Springer, Heidelberg (2009),
http://www.springerlink.com/index/H4833U76H771L873.pdf
8. Chu, Z., Wang, H.: An investigation of hotlinking and its countermeasures. Com-
puter Communications 34, 577–590 (2011)
9. Clayton, R.C., Murdoch, S.J., Watson, R.N.M.: Ignoring the Great Firewall of
China. In: Danezis, G., Golle, P. (eds.) PET 2006. LNCS, vol. 4258, pp. 20–35.
Springer, Heidelberg (2006)
10. P. Developers. Privoxy, http://www.privoxy.org.
11. Dingledine, R., Mathewson, N., Syverson, P.: Tor: the second-generation onion
router. In: Proceedings of the 13th USENIX Security Symposium (2004)
12. ebricca. refspoof Firefox extension,
https://addons.mozilla.org/en-US/firefox/addon/refspoof/
13. Eckersley, P.: How Unique Is Your Web Browser? In: Atallah, M.J., Hopper,
N.J. (eds.) PETS 2010. LNCS, vol. 6205, pp. 1–18. Springer, Heidelberg (2010),
http://dx.doi.org/10.1007/978-3-642-14527-8_1
14. Recent referer xss vulnerabilities, http://evuln.com/xss/referer.html
15. Fioravanti, M.: Client ﬁngerprinting via analysis of browser scripting environment.
Technical report (2010)
16. I2P Anonymous Network, http://www.i2p2.de/
17. ietf-http-wg mailinglist. Re: Referer (sic) from Phillip M. Hallam-Baker (March
09, 1995), http://lists.w3.org/Archives/Public/ietf-http-wg-old/
1995JanApr/0109.html
18. MozillaZine. Network.http.sendRefererHeader - MozillaZine Knowledge Base,
http://kb.mozillazine.org/Network.http.sendRefererHeader
19. Murphey, L.: Secure session management: Preventing security voids in web appli-
cations (2005)
20. Nikiforakis, N., Balduzzi, M., Van Acker, S., Joosen, W., Balzarotti, D.: Exposing
the lack of privacy in ﬁle hosting services. In: Proceedings of the 4th USENIX
Conference on Large-Scale Exploits and Emergent Threats, LEET 2011. USENIX
Association, Berkeley (2011)
21. Opera. Disabling referrer logging - Opera Knowledge Base,
http://www.opera.com/support/kb/view/93/
22. Provos, N.: A virtual honeypot framework. In: Proceedings of the 13th Confer-
ence on USENIX Security Symposium, SSYM 2004, vol. 13, pp. 1–1. USENIX
Association, Berkeley (2004)

278
N. Nikiforakis et al.
23. Reiter, M.K., Rubin, A.D.: Crowds: anonymity for web transactions. ACM Trans.
Inf. Syst. Secur. 1, 66–92 (1998)
24. RFC 2616 - Hypertext Transfer Protocol
25. Tor Project: Anonymity Online, http://www.torproject.org
26. WHATWG. HTML - Living standard,
http://www.whatwg.org/specs/web-apps/current-work/
multipage/links.html#link-type-noreferrer
27. Wikipedia. Referrer spam, http://en.wikipedia.org/wiki/Referrer_spam
28. Wondracek, G., Holz, T., Platzer, C., Kirda, E., Kruegel, C.: Is the internet for
porn? an insight into the online adult industry. In: Proceedings of the Ninth Work-
shop on the Economics of Information Security, WEIS (2010)
29. Yuill, J., Zappe, M., Denning, D., Feer, F.: Honeyﬁles: deceptive ﬁles for intrusion
detection. In: Proceedings from the Fifth Annual IEEE SMC Information Assur-
ance Workshop, pp. 116–122 (June 2004)
30. Zeller, W., Felten, E.W.: Cross-site request forgeries: Exploitation and prevention.
Technical report (2008)
31. Zhou, Y., Evans, D.: Why Aren’t HTTP-only Cookies More Widely Deployed? In:
Proceedings of 4th Web 2.0 Security and Privacy Workshop (W2SP 2010) (2010)
A
Appendix
Table 2. Alexa Ranking of 30 tested RASs – gray color denotes RASs showing ads
URI
Ranking URI
Ranking URI
Ranking
anonym.to
661 nolink.in
152,160 anonym2.com
846,517
hidemyass.com
904 cloakedlink.com
187,162 sock.im
933,195
referer.us
7,662 th3-0utl4ws.com
257,867 hidelinks.net
1,170,906
anonymizeit.com
10,212 savanttools.com
305,880 anon.projectarchive.net
2,591,907
lolinez.com
35,526 a.myurl.in
311,033 1-url.net
4,032,638
nullrefer.com
37,227 privatelink.de
338,512 crypo.net
5,009,009
linkblur.com
62,993 carcabot.com
347,260 anonym.ehmig.net
5,510,217
refblock.com
63,834 linkanonymizer.net
433,913 hidehelp.com
9,470,830
dereferer.ws
104,803 spoofurl.com
645,526 devgizmo.com
No Data
anonymous-link.net
118,261 refhide.com
679,101 theybetrollin.com
No Data

Risk Communication Design: Video vs. Text
Vaibhav Garg, L. Jean Camp, Katherine Connelly, and Lesa Lorenzen-Huber
Indiana University
{gargv,ljcamp,connelly,lehuber}@indiana.edu
Abstract. There are signiﬁcant diﬀerences between older and younger
adults in terms of risk perception and risk behaviors oﬄine. The previ-
ously unexplored existence of this dissimilitude online is the motivation
for our work. What are the risk perceptions of older adults? How are
these correlated with the classic dimensions of risk perception oﬄine?
Can we leverage episodic memory, particularly relevant for older adults,
to increase the eﬃcacy of risk communication? We conduct a survey
based experiment with two groups: video (n=136) and text (113). We
ﬁnd that leveraging episodic memory using video risk communication can
improve the ability of elders to avoid phishing attacks and downloading
malware. The applicability of the dimensions of risk were diﬀerent based
not only the risk but also the mode of risk communication.
Keywords: Risk, Privacy, Video, Mental Models, Phishing, Malware.
1
Introduction
Emerging digital technologies are typically neither designed by or for older
adults, increasing older adults’ susceptibility to privacy violations. Older adults
tend to be less experienced with technology than younger adults. With certain
privacy risks, such as phishing, there is a tangible ﬁnancial cost to the victims,
their families and service providers. Older adults are the fastest growing demo-
graphic in United States [13] and they own a disproportional amount of ﬁnancial
assets [10]. They are also more susceptible to ﬁnancial fraud oﬄine [4], a vulner-
ability that may transfer online as well. Along with the rest of the population,
older adults are being encouraged to use digital technologies to conduct ﬁnancial
transactions - technologies with which they are often unfamiliar. For example,
from 2011, the United States Internal Revenue Services (IRS) will no longer mail
tax forms to every household, encouraging online ﬁling instead. For those who
wish to ﬁle their taxes with paper forms, the forms would be available at local
post oﬃces. However older adults often have less mobility than young adults,
and may not ﬁnd the post oﬃce accessible. In addition, tax returns ﬁled online
are processed faster, further encouraging taxpayers to do so. However, online
forms are unfamiliar to many, especially older adults. Thus, when an older adult
receives a phishing email claiming to be from the IRS that requests them to click
on a link to resolve discrepancies with their tax ﬁling, they are likely to believe
there has been a legitimate problem and will follow the link. Much has been said
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 279–298, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

280
V. Garg et al.
about younger adults, their use of social media, and their disregard for privacy.
But older adults may (or may not) be very diﬀerent in their privacy preferences
[31] and in fact may be more likely to engage in risky behavior [51].
There have been two approaches to inform privacy decisions in end users:
education [42] and risk communication [2]. Education is a long-term eﬀort that
targets risk comprehension and therefore attitudes. Older adults, however, have
limited cognitive plasticity, aﬀecting their ability and desire to understand the
mechanics of privacy risks such as phishing. Since the goal is to inform behaviors,
risk communication might be more pertinent. Thus, we examine the construction
of perceived risk online for older adults. Research in risk communication for
privacy risks is typically conducted with college undergraduates. Designing for
older adults may have additional constraints [23] that arise from limitations on
information retention and retrieval [46].
We compare textual and video risk communication. Both media types are
built using physical mental models, based on previous research [6,25]. We begin
the description of our work by placing it in the historical context of risk percep-
tion, and classic developments in risk communication in Section 2. Building on
that context, we detail our methodology in Section 3. We present the results in
Section 4. After discussing the results in Section 5, we conclude in Section 6.
2
Background and Related Work
The post World War II school of thought, regarding privacy was that of conﬁden-
tiality, engendered from the privacy needs of the Cold War [39]. Thus, privacy
was treated as an extension of security and privacy solutions were mostly techni-
cal, e.g. encryption. Information sharing can, however, be beneﬁcial when it, for
example, improves market ineﬃciencies [40]. That individuals share information
does not imply a lack of concern for privacy. It merely emphasizes the limitations
of privacy as conﬁdentiality [28].
Privacy then becomes not just the ability to hide information, but also the
ability to control information sharing. To that end there have been both techni-
cal solutions like Tor, as well as policy solutions like Do Not Track. Unfortunately,
privacy solutions are often limited by their usability [50]. For example, deploy-
ing a Tor client requires a certain level of technical competence [18], which may
hinder adoption [20]. This might be particularly critical for older adults who are
less technically versed. At the same time, usability evaluations are usually done
with younger adults, typically college undergraduates.
Previous risk studies have focused on the usability of privacy-enhancing
technologies [14,45,19]. Here we focus of purposeful and accidental information-
sharing, using phishing and malware respectively. Speciﬁcally we examine down-
loaded malware instead of drive-by malware because downloading requires user
action. The goal is not to educate users so that they understand the underlying
technical risks but rather to give them the skill set necessary to avoid the risk.
Privacy solutions, even when usable, are frequently not adopted [3]. Even
privacy protections that require no technological expertise are not adopted. A

Risk Communication Design
281
recent example of this is the Do Not Track initiative. In terms of usability, all
it requires is a click of a button to indicate that the user does not desire to be
tracked. Despite the simplicity of it’s design, ﬁgures indicate that less than 2%
of Firefox users opt in [8]. At the same time, survey reports indicate that users
are uncomfortable being tracked online. This disconnect between attitudes and
behaviors, termed “the privacy paradox” [37], is also encountered oﬄine and
should not be surprising [43]. One explanation is our failure to contextualize
privacy solutions appropriately [36]. The risk of information sharing is most
salient during survey-based elicitation of attitudes. However, in context it would
be the beneﬁts of information sharing that are most easily available.
Bonneau et al. argue that it is rational for online service providers to hide
the “privacy control interface and privacy policy to maximize sign-up numbers
and encourage data sharing from the pragmatic majority of users” [11]. Thus,
it is not a lack of user concern for privacy, rather a lack of clear signals that
makes privacy solutions impotent [15]. The paradigm of rationality fails system-
atically and predictably for both security [41,26] and privacy decisions [3]. Our
research seeks to address the deeper problem that privacy solutions are limited
due to the designers’ assumptions about a rational end-user by providing guid-
ance on the systematic, potentially predictable, patterns of irrationality. How
does rationality fail? And how can such failures be predicted by building on
the foundations built by researchers focusing on oﬄine risks? Ideally, privacy
risks would be evaluated as the product of probability of occurrence and the
magnitude of implications [9]. However, privacy risk decisions are often acted
upon by external factors, e.g. control [12]. Brandimarte et al. noted that partic-
ipants were more willing to share information when they had control over the
publication of information, even though they had no control over access to that
information or third party use. Thus, perceived control alleviates aversion to
risk. Privacy decisions are then subjective, with end-users balancing perceived
risk with perceived beneﬁt. Fischhoﬀet al. [22] developed the canonical nine-
dimensional model of perceived risk. This model has been used extensively to
study perceived risk oﬄine for a diverse set of risks, e.g. health risks [32] and en-
vironmental risks [24]. It has been used online to examine insider threats [21] as
well as security risks [25]. The nine dimensions consist of voluntary, immediacy,
knowledge to exposed, knowledge to expert, control, newness, common-dread,
chronic-catastrophic, and severity. These have the potential to impact privacy
decisions as well. Information sharing on social networks is done voluntarily,
implying control and alleviating perceived risk. However, behavioral advertising
raises privacy concerns as information is being involuntarily revealed [38]. The
beneﬁts of information sharing are immediate, while the consequences of privacy
violations may appear delayed [1]. Thus, regrets about privacy risks might ap-
pear later [49]. Privacy risks are often not known to the end-user. Even when
they are known, end-users may put too much value in expert opinion. Increased
trust in expert systems can increase risky behavior. For example, drivers with
ABS-enabled cars drive closer to other vehicles [33]. Similarly, perceptions of
control can increase risky behavior [12]. Online privacy risks are newer than

282
V. Garg et al.
those oﬄine. For example, information aggregation reveals information about
end-users that would not be as easily accessible by individual bits of data. Hu-
mans understand averages better than aggregates [48]. When an end-user shares
a single piece of information on Facebook, Google Plus, and Linked In, they eval-
uate it as one piece of information being shared on average, rather than as three
pieces of information being shared. It would be even more diﬃcult for end-users
to account for a fourth piece of information being revealed as a result of the ﬁrst
three pieces being combined.
Common risks are dreaded less than rarely encountered risks. For example,
terrorism appears more threatening than E. coli, despite the far higher death
rates for the latter. Likewise, privacy risks are commonly encountered and thus
may appear less threatening. Perceived risk is also directly proportional to the
number of people at risk. Severity of risk consequences is also directly correlated
with perceptions. Thus, decisions concerning risk are not strictly rational.
Risk decisions can be improved by a soft paternalistic nudge [2]. Nudging
must impinge both the intuitive as well as the rational decision processes [44].
The eﬀectiveness of intuitive systems is based on the decision system’s ability to
recognize risk [34]. Individuals respond irrationally to online risks in a manner
analogous to oﬄine risks. Ability to identify risk online is driven by the mental
models that are used for representation [16]. Accessible mental models would
make it easier for end-users to associate online risk with oﬄine risks that they
would be more familiar. When encountering an unfamiliar risk, individuals will
be guided by their perceptions, not by the calculus of risk [17].
Previous work argued that security experts use ﬁve mental models: physical,
criminal, warfare, medical, economic. End-users ﬁnd physical mental models
to be most accessible [6,25]. Thus, grounding risk communication designs in
physical mental models would make user intuitions more informed.
Simultaneously, the eﬀectiveness of a rule-based system is driven by its ability
to extract the relevant information. The design of risk communication must then
address information coding, storage, and retrieval. This is especially relevant for
older adults who may have less cognitive plasticity compared to the younger
adults [52]. The impact of aging on memory is severe [5], with retrieval becom-
ing more diﬃcult as older adults increasingly experience irrelevant intrusions
(i.e. interruptions, unrelated information, or background noise) [29]. One ap-
proach to address bounds on cognition is by the use of richer media [30]. Videos,
for example, are stored in the episodic memory [47] rather than semantic [35]
as coding is based in context rather than content [46]. Previous research indi-
cates that richer media can facilitate cognition [7], targeting episodic memory
[47] rather than semantic [35]. Simultaneously, the use of appropriate mental
models can make risk information more accessible [16]. Traditional online risk
communication, however, uses text. Thus, there is a need explore video-based
risk communication designs that privacy risks for older adults.
In this paper we present the design of narrative driven risk communication
videos grounded in physical mental models. We present the evaluation of these
videos by comparing them to traditional text based risk communication. We

Risk Communication Design
283
have designed these videos for older adults, thus our participant pool consists of
adults older than 65 years of age. We examine the diﬀerences in perceived risk
as experienced through diﬀerent risk communication media. In the next section
we present our methodology.
3
Methodology
We used an expressed preferences methodology to evaluate the diﬀerence be-
tween text and video based risk communication for phishing and malware risks.
We conducted a survey-based experiment with two groups. Group 1, referred
to as the video group, saw the risk information as a physical mental models
based narrative presented in a video. Group 2, referred to as the text group
read the risk information in a text form. Participants were randomly assigned
to each group. Participant risk was minimized by following the ethics guidelines,
for expedited studies, from Internal Review Board (IRB).
Participant recruitment and survey deployment was conducted by Knowledge
Networks. Knowledge Network provides access to a representative sample of
the U.S. population. They cover both online and oﬄine populations, listed and
unlisted phone numbers, households with or without phones as well as households
with only cellular phones. Sampling frame is constructed using address based
sampling (ABS) and random digit dialing (RDD). This is similar to methodology
used by CDC for national immunization surveys. Participants are chosen at
random from the sampling frame. Eligibility criteria is applied, i.e. participants
need to be over 65. Over-sampling and under-sampling concerns are addressed
by the use of post stratiﬁcation weights. Despite a rigorous methodology there
are always limitations. For example, even with ABS and RDD combined the
sampling frame is constructed from 98% of the US population. However, the
results can be considered to be reasonably generalizable.
Participants began by providing the consent to participate according to IRB
guidelines. Participants provided demographic information: age, frequency of
Internet use, frequency of cellphone use, as well as whether the participants lived
alone or with other people. Participants were then shown text based or video
based risk communication respectively. For each group half the participants rated
their attitude towards the risk, i.e. which did they consider higher, the risk of
responding or that of not responding. For example, for phishing emails the risk
of responding would imply clicking on a link and providing personal information.
The other half predicted their behavior in response to being exposed to the risk,
i.e. are they more likely to respond (or not)?
Risk Communication Design: Participants were communicated the risk of
phishing and malware. These risks were chosen due to their implications for
identity theft, which could result in personal ﬁnancial loss. This loss is more
detrimental to older adults, as their ability to replace lost income is limited. The
risk information was presented in text form to a subset of the participants, text
group, and in video form to the rest, video group. Here we will discuss the design
of both the text and video based risk communication. Due to space limitation

284
V. Garg et al.
we will discuss only phishing. A detailed discussion of the design process can be
found in [27].
In order to minimize psychological risks, such as anxiety, we created a false
persona of an older adult: Mr. Cullen. Participants were told that Mr. Cullen is
a retired older adult who had just received an email. Then the participants were
presented with a canonical phishing email:
Dear Mr. Cullen,
We are from the IRS and we are writing regarding your retirement funds and
bank accounts. It has come to our attention that there might be some discrep-
ancies with respect to some of the transactions made from your accounts. We
are conducting an investigation into this. We would like to get some information
from you. Please click on the link at the bottom of the e-mail and answer a few
questions. Please make sure that you have your bank account number, password,
and your social security number as you may be asked about them.
www.IRS.com
Regards IRS
The participants were then informed that Mr. Cullen clicked on the associated
link and provided the relevant information.
To present this information in the video form, we ﬁrst identiﬁed the key char-
acteristics of phishing from the above email. First, phishing emails appear legit-
imate. Secondly, they try to scare the recipient. Finally, they ask for the email
recipient’s ﬁnancial information. Based on these characteristics we developed a
mental models based narrative grounded in a physical analogy. This narrative
was later developed into a video. For consistency, the victim in the video was
also an older adult. The attacker had to be a legitimate ﬁnancial entity. Again
for consistency we chose an IRS agent, or rather an attacker pretending to be
one. Thus, the agent fashioned credentials that appeared authentic. The agent
contacted the older adult at home and informed him that he was under investi-
gation due to ﬁnancial discrepancies. The agent then asked for the older adult’s
ﬁnancial information. The older adult, wanting to comply with the investigation,
provided the information and, thus, was phished.
Risk Assessment: Text based risk communication would impinge perceived
risk diﬀerently from risk videos1. Half the participants identiﬁed whether they
felt the risk of responding to the risk was higher than not responding. The re-
maining participants predicted their behavior, i.e. if they would respond to the
risk or not respond. Participants rated the perceived beneﬁt of responding to
the risk on a seven point Likert scale (1=Not beneﬁcial; 7=Highly beneﬁcial).
Participants also rated the perceived risk of responding on seven point Likert
scale (1=Not risky; 7=Highly risky).
Risk of responding was also evaluated on a nine dimensions of perceived risk
identiﬁed by Fischhoﬀet al. [22]. Since its inception in 1978, this model has
been used extensively to study risks in a diversity of domains including health
1 Phishing Video: http://www.youtube.com/watch?v=4ZQ9pFTCdy4
Malware Video: http://www.youtube.com/watch?v=6zHJoZqrCB0

Risk Communication Design
285
and environmental risks. Participants were asked to rate the perceived risk of
responding to phishing and malware risks on each of the nine dimensions. The
rating was on a ﬁve point like scale and the dimensions were deﬁned as:
1. Voluntary: To what extent does Mr. Cullen have a choice in being exposed
to this risk? (1=Voluntary; 5=Involuntary)
2. Immediacy: Is the risk from the threat immediate or does it occur at a later
time? (1=Immediate; 5=Delayed)
3. Knowledge to the exposed: How much would a person like Mr. Cullen know
about the implications of this risk? (1=Knows a lot; 5=Knows nothing)
4. Knowledge to the expert: How much would an expert know about the im-
plications of this risk? (1=Knows a lot; 5=Knows nothing)
5. Control: To what extent can you control (or mitigate) the risk? (1=Uncon-
trollable; 5=Controllable)
6. Newness: Is this a new risk resulting from new technologies or is it a new
version of an old risk? (1=Old; 5=New)
7. Common-Dread: Is this risk commonplace or rarely encountered? (1=Com-
mon; 5=Rare)
8. Chronic-catastrophic: Does this risk aﬀect only Mr. Cullen or does it aﬀect
many people? (1=(Mr. Cullen) Individual; 5=(Many People) Global)
9. Severity: In the worst possible outcome, how severe would the consequences
be? (1=Not Severe; 5=Severe)
4
Results
There were a total of 249 participants. There were 113 participants in the text
group and 136 participants in the video group. 49 of the participants lived alone.
26 of those participated in the text group, while 23 participated in the video
group. 199 participants lived with other people. 86 of those participated in the
text group, while 113 participated in the video group. 122 of the participants
were men, 56 of whom were in the text group and 66 in the video group. There
were 127 women participants, of which 57 read the text and 70 saw the video.
The mean time for completing the text survey was 219.3186 minutes, while
that for completing the video survey was 303.6103 minutes. There was no statis-
tically signiﬁcant diﬀerence between the mean time taken by each group, p < .05.
The high values from mean time to complete are due to a handful of participants
taking over 6000 minutes to complete the survey, i.e. several days. Thus, we also
computed the medians.
The median time of completion for the text survey was 21 minutes and that for
the video survey was 30.5 mins. Medians were compared by using Mann-Whitney
or two-sample Wilcoxon test. This non-parametric test makes two assumptions:
(1) the distribution is continuous, and (2) that the shape of the distribution of
both samples is similar. Since time is a continuous variable assumption one is jus-
tiﬁed. For the second assumption, both distributions were heavily right skewed.
The lower bound was 0 while the upper bound was less than 6500 minutes. On

286
V. Garg et al.
conducting a one sided test, the time to complete was signiﬁcantly diﬀerent be-
tween text and video; w=4851.5, p-value=2.762e-07. Participants in the video
group took more time to complete the survey than those in the text group.
Risk Attitudes: The risk of responding to phishing emails is often underesti-
mated. Phishing emails create anxiety in recipients, thereby making the risks of
not responding salient, while appearing to alleviate the risks of responding; e.g.
‘If you do not contact us on the following, your account may be closed’. Thus,
participants were asked if they considered the risk of responding to be greater to
than the risk of not responding to phishing. 62 participants answered this ques-
tion for text while 65 answered this for video. 38 participants in the text group
indicated responding to the email was more risky, while 48 participants indicated
responding to the agent with the information was more risky. Test of proportions
did not indicate a statistically signiﬁcant diﬀerence; p-value= 0.1439.
Similarly, participants were asked if they considered the risk of responding to
be greater to than the risk of not responding to malware. 62 participants an-
swered this question for text while 64 answered this for video. 53 participants in
the text group indicated responding was more risky, while 62 participants in the
video group indicated that responding was risker. One sided test of proportions
was statistically signiﬁcant, p=0.02565.
Predicted Behavior: Even when the risk of responding is perceived to be
greater, participants may choose to respond, as the implications might, for ex-
ample, appear more controllable [12]. Thus, we asked participants to indicate
their likely action in response to the phishing attack if they had indicated that
responding was riskier than not responding (n=51 for text and n=72 for video).
All text participants said that they would not respond, while 71 video par-
ticipants said they would not respond. Test of proportions did not indicate a
statistically signiﬁcant diﬀerence; p-value= 1.
We also asked participants to indicate their likely action in response to the
malware scenario if they had indicated that responding was riskier than not
(n=52 for text and n=72 for video). 49 text participants said they would not
respond, while 69 video participants said they would not respond. Test of pro-
portions was not statistically signiﬁcant; p-value=0.2858.
Perceived Beneﬁt: Participants rated the beneﬁt of responding to phishing
risk on a seven point Likert scale (1=not beneﬁcial at all; 7=highly beneﬁcial).
The mean beneﬁt for the text group was 1.372881, while that for the video group
was 1.828125. An independent two sample T-test did not indicate statistically
signiﬁcant diﬀerence. Participants rated the beneﬁt of responding to malware
risk on a seven point Likert scale (1=not beneﬁcial at all; 7=highly beneﬁcial).
The mean beneﬁt for the text group was 1.271186, while that for the video group
was 1.281250. An independent two sample T-test did not indicate statistically
signiﬁcant diﬀerence.
Perceived Risk: Participants rated the risk of responding to phishing on a seven
point Likert scale (1=not risky; 7=highly risky). The mean risk for the text group
was 6.814815, while that for the video group was 6.830986. An independent two

Risk Communication Design
287
sample T-test did not indicate statistically signiﬁcant diﬀerence. Participants
rated the risk of responding to malware on a seven point Likert scale (1=not
risky; 7=highly risky). The mean risk for the text group was 6.018868, while that
for the video group was 6.555556. An independent two sample T-test indicates
statistically signiﬁcant diﬀerence, p=0.002392.
Table 1. Mean Risk Ratings of Fischhoﬀ’s nine dimensions
Phishing
Malware
Text vs. Video
Text
Video
p-value
Text
Video
p-value
Voluntary
2.389381 1.992593 0.02683 1.651786 1.955882
0.04183
Immediacy
1.876106 2.318519 0.005024 2.063063 2.977941 8.346e-07
Exposed
3.723214 3.830882
> 0.05
3.732143 4.082707 0.004020
Expert
1.153153 1.110294
> 0.05
1.099099 1.186567
> 0.05
Control
1.783784 2.514706 7.385e-05 3.027027 2.851852
> 0.05
Newness
2.765766 2.463235
> 0.05
3.442478 3.432836
> 0.05
Common Dread
1.781818 1.733333
> 0.05
1.928571 2.373134 0.0007834
Chronic-Catastrophic 3.919643 4.066176
> 0.05
3.814159 4.073529
0.04495
Severity
4.855856 4.867647
> 0.05
4.548673 4.850746 0.0001627
4.1
Nine Dimensional Model
Participants rated the risk of responding to phishing and malware on Fischhoﬀ’s
nine dimensions of perceived risk. Table 1 reports the mean rating given for each
dimension as well as the p-values of one sided T-tests between text and video.
Participants rated the voluntary nature of risk (1=Voluntary; 5=Involuntary).
Text and video were statistically diﬀerent for both phishing as well as malware.
While for phishing the risk of responding was more voluntary for the video group,
for malware risk of responding was more voluntary for text. Participants rated
the immediacy of the impact of risk (1=Immediate; 5=Delayed). The conse-
quences of responding were more delayed for the video than for text, in both
phishing as well as malware. Participants rated the knowledge a typical victim
would have regarding the risk (1=Knows a lot; 5=Knows nothing). There was no
statistical diﬀerence between text and video for phishing. However, knowledge
to the exposed was higher in the video group for malware. The relationship of
perceived risk with voluntary, immediacy, and knowledge to exposed is shown in
ﬁgure 1.
Participants rated the knowledge an expert would have regarding the impli-
cations of phishing or malware (1=Knows a lot; 5=Knows nothing). There was
no statistically signiﬁcant diﬀerence between text and video for either phish-
ing or malware. Participants rated the extent to which they can control the
risk consequences (1=Uncontrollable; 5=Controllable). Phishing risk was more
controllable for video than for text, while the diﬀerence was not statistically
signiﬁcant for malware. Participants rated the newness of phishing or malware
(1=Old; 5=New). There was no statistically signiﬁcant diﬀerence between text

288
V. Garg et al.
and video for either phishing or malware. The relationship of perceived risk with
knowledge to expert, control, and newness is shown in ﬁgure 2.
Participants were asked to rated how common they considered the risk to be
(1=Common; 5=Rare). Participants also rated whether the risk impact just the
victim or if the risk was global (1=Individual; 5=Global). Finally, participants
rated the severity of the consequences of phishing (1=Not Severe; 5=Severe). The
diﬀerence between text and video was not statistically signiﬁcant for phishing
on these three dimensions. However, for malware the diﬀerence was statistically
signiﬁcant. Video risk was more rare, catastrophic, and severe, compared to text.
The relationship of perceived risk with common-dread, chronic-catastrophic, and
severity is shown in ﬁgure 2.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Voluntary
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Immediacy
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Knowledge to Exposed
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Phishing Video
Voluntary
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Phishing Video
Immediacy
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Phishing Video
Knowledge to Exposed
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
3
4
5
6
7
Malware Text
Voluntary
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
3
4
5
6
7
Malware Text
Immediacy
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
3
4
5
6
7
Malware Text
Knowledge to Exposed
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Voluntary
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Immediacy
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Knowledge to Exposed
Perceived Risk
Fig. 1. Perceived Risk vs. Voluntary, Immediacy, and Knowledge to Exposed

Risk Communication Design
289
4.2
Regression Analysis: Perceived Risk vs. Nine Dimensions Model
Figures 1-3, note that the relationship between perceived risk and the nine di-
mensions is linear. Thus, linear regression analysis is applicable. We then iden-
tiﬁed the best ﬁt model, i.e. the subset of the nine dimensions that best explain
variance in perceived risk. Thus, we isolated the relevant dimensions evaluate
their ability to explain perceived risk.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Knowledge to Expert
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Control
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Newness
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1.0
2.0
3.0
4.0
4.0
5.0
6.0
7.0
Phishing Video
Knowledge to Expert
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Phishing Video
Control
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Phishing Video
Newness
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1.0
2.0
3.0
3
4
5
6
7
Malware Text
Knowledge to Expert
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
3
4
5
6
7
Malware Text
Control
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
1
2
3
4
5
3
4
5
6
7
Malware Text
Newness
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Knowledge to Expert
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Control
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Newness
Perceived Risk
Fig. 2. Perceived Risk vs. Knowledge to Expert, Control, and Newness
We considered additional variables: (1) time taken to complete the survey, (2)
whether the participant lived alone or not, (3) frequency of Internet use, and (4)
frequency of cellphone use. These will be referred to as confounding variables. We
measure the models explanatory power by examining adjusted R-square values,
rather than raw R-square values. Adjusted R-square: 1) accounts for collinearity
of independent variables, and 2) adjusts for extra variables. We ﬁrst conducted
the analysis for phishing text. The dependent variable was perceived risk, while

290
V. Garg et al.
the independent variables were the nine dimensions. The R-square value was
negative with the complete nine dimensional model. Reducing the dimensions
improved the model’s explanatory power. Best ﬁt for the model was given by
severity, common dread, knowledge to the exposed, knowledge to expert, and
chronic-catastrophic; R square=0.03208, p=0.27776. Adding the confounding
variables further improved the model’s explanatory power. The best ﬁt for the
model was given by immediacy, common dread, knowledge to expert, frequency
of internet use, and cellphone use; R-square=0.1921, p=0.01468.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1.0
2.0
3.0
4.0
2
3
4
5
6
7
Phishing Text
Common−Dread
Perceived Risk
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Chronic−Catastrophic
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
2
3
4
5
6
7
Phishing Text
Severity
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1.0
2.0
3.0
4.0
4.0
5.0
6.0
7.0
Phishing Video
Common−Dread
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Phishing Video
Chronic−Catastrophic
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Phishing Video
Severity
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1.0
2.0
3.0
4.0
3
4
5
6
7
Malware Text
Common−Dread
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
3
4
5
6
7
Malware Text
Chronic−Catastrophic
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
3
4
5
6
7
Malware Text
Severity
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Common−Dread
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
4.0
5.0
6.0
7.0
Malware Video
Chronic−Catastrophic
Perceived Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
3.0
4.0
5.0
4.0
5.0
6.0
7.0
Malware Video
Severity
Perceived Risk
Fig. 3. Perceived Risk vs. Common-Dread, Chronic-Catastrophic, and Severity
Next we conducted the analysis for phishing video. With just the nine dimen-
sions as the independent variables the model has good explanatory power; R
square= 0.2494, p= 0.001391. Severity was statistically signiﬁcant. Best ﬁt for
the model was given by voluntary, knowledge to the exposed, control, severity;

Risk Communication Design
291
R square= 0.2849, p=2.633e-05. Knowledge to the exposed and severity were
statistically signiﬁcant. Adding the confounding variables did not increase the
explanatory power of the model.
For malware, again, we ﬁrst analyzed text. With perceived risk as the dependent
variable and the nine dimensions as independent variable the model had signiﬁ-
cant explanatory power; R-square =0.6082, p=7.981e-07. Knowledge to expert,
severity, and chronic-catastrophic were statistically signiﬁcant. Best ﬁt was given
by voluntary, severe, newness, common-dread, knowledge to exposed, knowledge
to expert, and chronic-catastrophic; R-square=0.6511 p-value=3.133e-09. Again
knowledge to expert, severity, and chronic-catastrophic were statistically signiﬁ-
cant. Adding the confounding variables did not increase the model’s explanatory
power.
However, for malware video perceived risk was not explained by the nine di-
mensional model; R-square value was negative. Reducing the dimensions improved
the performance slightly. Best ﬁt was given by control, immediacy and chronic-
catastrophic;R-square=0.06659 p=0.05139.Adding theconfounding variablesand
reducing dimensions, the best ﬁt was given by control, knowledge to the exposed,
and frequency of Internet use; R-square=0.09577,p=0.02179.Controlwas the only
statistically signiﬁcant dimension.
4.3
Factor Analysis
We were also interested in the underlying structure of the nine dimensional
model, speciﬁcally how it diﬀerent for video vs. text as well as phishing vs.
malware. Thus, we are interested in the diﬀerences between not just media but
also the nature of the risks. To analyze the underlying structure we conducted
exploratory factor analysis. To determine the number of factors, we conducted
Scree test; ﬁgure 4. We consider eigenvalues greater than 1. Thus, we consider
four factors for each scenario.
We conducted factor analysis using R’s inbuilt factors analysis function fac-
tanal with varimax rotation and pairwise deletion of missing values. Tables 2, 3,
4, and 5 show the factor loadings for the diﬀerent factors, the communalities for
the nine dimensions, and the variance explained by the four factors.
5
Discussion
The hypotheses that this research examines are two: 1) videos are more eﬀective
risk communication media than text for older adults, and 2) perceived risk can
be grounded in Fischhoﬀ’s nine dimensional model. To test the ﬁrst hypothesis
we examined the diﬀerence in participants’ attitude towards responding to risk,
predicted behavior, ranking of perceived beneﬁt and perceived risk.
Unlike voluntary, immediacy has the same relationship with perceived risk for
text as well as video, for phishing as well as malware. When the risk is immediate

292
V. Garg et al.
●
●
●
●
●
●
●
●
●
2
4
6
8
0.5
1.0
1.5
2.0
Phishing Text
 component number
Eigen values of components
●
●
●
●
●
●
●
●
●
2
4
6
8
0.6
1.0
1.4
1.8
Phishing Video
 component number
Eigen values of components
●
●
●
●
●
●
●
●
●
2
4
6
8
0.5
1.0
1.5
2.0
Malware Text
 component number
Eigen values of components
●
●
●
●
●
●
●
●
●
2
4
6
8
0.6
0.8
1.0
1.2
1.4
1.6
Malware Video
 component number
Eigen values of components
Fig. 4. Scree Test
Table 2. Phishing Text Factor Loadings
Factor 1 Factor 2 Factor 3 Factor 4 Communality
Voluntary
0.979
0.189
0.995
Immediacy
0.157
0.232
0.112
0.211
0.136
Knowledge to Exposed
0.182
0.040
Knowledge to Expert
0.570
0.340
Control
0.175
0.979
0.995
Newness
0.186
0.106
0.047
Common-Dread
0.123
0.411
-0.222
0.873
0.995
Chronic-Catastrophic
-0.145
0.027
Severity
-0.957
-0.150
-0.226
0.995
Proportion Var
0.146
0.139
0.120
0.103
Cumulative Var
0.146
0.285
0.405
0.508
it is perceived to be more risky, when the eﬀects are delayed, less so, ﬁgure 1.
Thus, a lower value for immediacy informs perceived risk better. Thus, text was
better than video at informing perceived risk. For both phishing and malware,
participants in the text group perceived the impact of risks to be more immediate
than delayed.

Risk Communication Design
293
Table 3. Phishing Video Factor Loadings
Factor 1 Factor 2 Factor 3 Factor 4 Communality
Voluntary
0.199
0.043
Immediacy
-0.111
0.233
0.151
0.354
0.215
Knowledge to Exposed
-0.347
0.124
Knowledge to Expert
-0.192
0.148
0.129
0.082
Control
-0.144
0.971
0.965
Newness
0.128
0.025
Common-Dread
-0.195
0.817
0.109
0.721
Chronic-Catastrophic
0.132
-0.629
0.417
Severity
0.991
-0.105
0.995
Proportion Var
0.124
0.116
0.094
0.065
Cumulative Var
0.124
0.239
0.334
0.399
Table 4. Malware Text Factor Loadings
Factor 1 Factor 2 Factor 3 Factor 4 Communality
Voluntary
0.219
-0.319
0.157
Immediacy
-0.355
0.119
0.154
Knowledge to Exposed
-0.160
0.032
Knowledge to Expert
0.977
-0.113
0.138
0.995
Control
0.792
0.629
Newness
0.279
-0.173
0.363
0.246
Common-Dread
0.187
-0.292
0.165
0.581
0.485
Chronic-Catastrophic
0.139
-0.383
0.168
Severity
0.801
0.120
-0.203
0.706
Proportion Var
0.119
0.118
0.085
0.075
Cumulative Var
0.119
0.237
0.322
0.397
Table 5. Malware Video Factor Loadings
Factor 1 Factor 2 Factor 3 Factor 4 Communality
Voluntary
0.278
0.086
Immediacy
0.802
0.649
Knowledge to Exposed
0.390
0.292
0.244
Knowledge to Expert
-0.372
0.142
Control
0.114
0.159
-0.339
0.155
Newness
0.225
0.418
0.232
Common-Dread
0.719
0.247
0.266
0.650
Chronic-Catastrophic
-0.439
0.214
0.245
Severity
-0.254
0.430
0.256
Proportion Var
0.089
0.089
0.071
0.046
Cumulative Var
0.089
0.178
0.249
0.295

294
V. Garg et al.
For knowledge to the exposed, the results were mixed. There was no statistical
diﬀerence in means values for phishing text and video. However, in general higher
knowledge led to higher perceived risk. The importance of this is reinforced in
the linear regression model for phishing video, where knowledge to exposed is
one of the dimensions in the best ﬁt model, and is also statistically signiﬁcant.
For malware, knowledge to the exposed was rated lower in text than in video.
However, in text lower knowledge led to higher perceived risk, while in video
the relationship was reversed. Knowledge to exposed was present in the best ﬁt
models of both text and video, but was only statistically signiﬁcant in text. Thus,
it is unclear if text or video are better at leveraging this dimension. Knowledge to
the expert was similar between text and video, for both phishing and malware.
Knowledge to the expert, in general tends to reduce perceived risk. It would then
lead to more risk taking behaviors. Knowledge to the expert was in the best ﬁt
model for phishing text and malware text. It was not statistically signiﬁcant for
phishing text, however, for malware text it was signiﬁcant. Thus, text based risk
communication, for malware, may lead to more risk taking behaviors based on
knowledge to experts.
Control had a consistent relationship with perceived risk. Uncontrollable risks
were perceived more risky, ﬁgure 2. For phishing, text group perceived phishing
risk to be more uncontrollable than video group. Control was in the best ﬁt
model for both phishing video and malware video. It was statistically signiﬁcant
for malware video. Thus, video based risk communication would lower perceived
risk on the control dimension.
Newness was similar for both text and video, for phishing as well as malware.
Newness was only in the best ﬁt model of malware text. However, it was not
statistically signiﬁcant. In general, newer risks were perceived to be more risky.
However, there does not seem to be much evidence of newness having a signiﬁcant
impact on perceived risk.
Common-dread had a consistent relationship with perceived risk. Common
risks were perceived to be less risky than those rarely encountered. The diﬀer-
ence between phishing text and phishing video was not signiﬁcant. The diﬀerence
for malware was signiﬁcant. Text group perceived malware to be more common
than video. Common-dread was in the best ﬁt models for text but not for video.
Thus, text based risk communication may alleviate perceptions of risk by making
them appear common. Chronic-catastrophic did not have a signiﬁcant impact
on phishing text. For phishing video and malware, both text and video, risks
that impact more people were perceived to be more risky. The diﬀerence be-
tween phishing text and video was not statistically signiﬁcant. The diﬀerence
for malware was signiﬁcant. Malware was seen to impact more people for video
than for text. Chronic-catastrophic was in the best ﬁt model for both malware
text and malware video. It was statistically signiﬁcant for malware text. Thus,
text based risk communication might alleviate perceptions of malware risk.
Severity had a signiﬁcant impact on perceived risk. More severe risks were
perceived as more risky. The diﬀerence between phishing text and video was
not signiﬁcant. For malware the diﬀerence was signiﬁcant. Malware risks were

Risk Communication Design
295
perceived more severe for video than for text. Severity was in the best ﬁt model
for malware text, and was statistically signiﬁcant. Malware appears less severe
in text than video. Thus, perceived risk might be lower for text than video.
6
Conclusion and Future Work
Recall the purpose of our experiments was to test two hypotheses. The ﬁrst
is that grounding risk communication in mental models empowers older adults
(who are not familiar with information technology) to avoid common threats.
We built on previous ﬁndings for mental models, using physical mental models.
We concluded that video was more eﬀective in making risk salient and perceived
as severe; yet the prediction of behaviors was not signiﬁcantly diﬀerent between
text and videos.
The second hypothesis was that online risk perceptions could be understood
using the classic dimensions of oﬄine risks. The perceptions diﬀered based on
the presentation of the risk and the risk itself; however, in any case there were
multiple signiﬁcant dimensions. Knowledge to experts was stronger in the case of
text; thus text may perversely increase risk-seeking. Perceptions of severity were
greater with video than text. The perception of a risk as voluntary for phishing
made the risk more salient in the case of text and less so in video. Text made the
risk appear more immediate and in the case of malware, less voluntary. Neither
risk, reasonably so, was seen as particularly dreadful. Perceptions of severity had
by far the greatest impact on perceived risk. Neither knowledge to the exposed
nor newness were signiﬁcant in either case.
The use of mental models proved eﬀective in providing strategies for risk mit-
igation and illustrating the severity of risk; thus mental models rather than ex-
act technical information are more powerful in decreasing potentially hazardous
information-sharing. The use of video to leverage episodic memory proved some-
what mixed; yet on the most signiﬁcant variable (i.e., severity) video proved
more eﬀective. Risk communication is more eﬀective when grounded in the risk
itself (e.g., phishing) than in the technical vector in which the risk is embedded.
We conclude that targeting risk communication using videos and mental models
has the potential to be extremely eﬀective in the (also extremely vulnerable)
online population of older adults.
Acknowledgements. We would like to thank Dr. Kelly Caine who assisted us
at various stages of the research. We also acknowledge the insights provided by
the Stat/Math center at Indiana University. Any mistakes are, however, authors’
own responsibility.
This material is based upon work supported, in part, by the National Science
Foundation under Grants NSF IIS 1036918 and NSF CNS 0943382. Any opin-
ions, ﬁndings, and conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reﬂect the views of the National
Science Foundation.

296
V. Garg et al.
References
1. Acquisti, A.: Privacy in electronic commerce and the economics of immediate grat-
iﬁcation. In: Proceedings of the 5th ACM Conference on Electronic Commerce, pp.
21–29. ACM (2004)
2. Acquisti, A.: Nudging privacy: The behavioral economics of personal information.
IEEE Security & Privacy 7(6), 82–85 (2009)
3. Acquisti, A., Grossklags, J.: Privacy and rationality in individual decision making.
IEEE Security & Privacy 3(1), 26–33 (2005)
4. Anderson, K.: Consumer fraud in the United States: An FTC survey. Federal Trade
Commission (2004)
5. Anderson, N., Craik, F.: Memory in the aging brain. The Oxford Handbook of
Memory, 411–425 (2000)
6. Asgharpour, F., Liu, D., Camp, L.J.: Mental Models of Security Risks. In: Dietrich,
S., Dhamija, R. (eds.) FC 2007 and USEC 2007. LNCS, vol. 4886, pp. 367–377.
Springer, Heidelberg (2007)
7. Aslan, A., B¨auml, K.H., Past¨otter, B.: No inhibitory deﬁcit in older adults’ episodic
memory. Psychological Science 18(1), 72 (2007)
8. Bachman,
K.:
Study:
Internet
user
adoption
of
dnt
hard
to
predict.
Tech. rep., AdWeek (March 2012), http://www.adweek.com/news/technology/
study-internet-user-adoption-dnt-hard-predict-139091
9. Bernstein, P.: Against the gods: The remarkable story of risk. John Wiley & Sons
Inc. (1998)
10. Bertoni, D.: Identity Theft: Governments Have Acted to Protect Personally Identi-
ﬁable Information, But Vulnerabilities Remain: Congressional Testimony. DIANE
Publishing (2009)
11. Bonneau, J., Preibusch, S.: The privacy jungle: On the market for data protection
in social networks. Economics of Information Security and Privacy, 121–167 (2010)
12. Brandimarte, L., Acquisti, A., Loewenstein, G.: Misplaced conﬁdences: Privacy
and the control paradox. In: Workshop of Economics and Information Security
(WEIS). Harvard University (2010)
13. Breau,
C.:
Projected
population
of
the
united
states,
by
age
and
sex:
2000
to
2050.
Tech.
rep.,
U.
S.
Census
Bureau
(2000),
http://www.census.gov/population/www/projections/usinterimproj/
14. Brodie, C., Karat, C.M., Karat, J., Feng, J.: Usable security and privacy: a case
study of developing privacy management tools. In: Proceedings of the 2005 Sym-
posium on Usable Privacy and Security, SOUPS 2005, pp. 35–43. ACM, New York
(2005)
15. Camp, L.J.: Reliable, usable signaling to defeat masquerade attacks. ISJLP 3, 211
(2007)
16. Camp, L.J.: Mental models of privacy and security. IEEE Technology and Society
Magazine 28(3), 37–46 (2009)
17. Camp, L., McGrath, C., Genkina, A.: Security and morality: A tale of user deceit.
In: Models of Trust for the Web (MTW 2006), Edinburgh, Scotland, vol. 22 (2006)
18. Clark, J., Van Oorschot, P., Adams, C.: Usability of anonymous web browsing:
an examination of Tor interfaces and deployability. In: Proceedings of the 3rd
Symposium on Usable Privacy and Security, pp. 41–51. ACM (2007)
19. Cranor, L., Garﬁnkel, S.: Security and usability: Designing secure systems that
people can use. O’Reilly Media, Inc. (2005)

Risk Communication Design
297
20. Dingledine, R., Mathewson, N.: Anonymity loves company: Usability and the net-
work eﬀect. In: Proceedings of the Fifth Workshop on the Economics of Information
Security (WEIS 2006), Cambridge, UK (June 2006)
21. Farahmand, F., Spaﬀord, E.H.: Understanding insiders: An analysis of risk-taking
behavior. Information Systems Frontiers, 1–11 (2010)
22. Fischhoﬀ, B., Slovic, P., Lichtenstein, S., Read, S., Combs, B.: How safe is safe
enough? a psychometric study of attitudes towards technological risks and beneﬁts.
Policy Sciences 9(2), 127–152 (1978)
23. Fisk, A.D., Rogers, W.A., Charness, N., Sharit, J.: Designing for older adults:
Principles and creative human factors approaches, vol. 2. CRC (2009)
24. Flynn, J., Slovic, P., Mertz, C.K.: Gender, race, and perception of environmental
health risks. Risk Analysis 14(6), 1101–1108 (1994)
25. Garg, V., Camp, L.J.: End user perception of online risk under uncertainty. In:
45th Hawaii International Conference on System Sciences. IEEE (2012)
26. Garg, V., Camp, L.J.: Heuristics and biases: Implications for security and privacy
(2012), http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1933957
27. Garg, V., Camp, L.J., Lorenzen-Huber, L.M., Connelly, K.: Risk communication
design for older adults. In: ISG*ISARC 2012. International Society for Gerontech-
nology (in press, 2012)
28. G¨urses, S., Berendt, B.: Pets in the surveillance society: A critical review of the po-
tentials and limitations of the privacy as conﬁdentiality paradigm. Data Protection
in a Proﬁled World, 301–321 (2010)
29. Hasher, L., Stoltzfus, E., Zacks, R., Rypma, B.: Age and inhibition. Journal of
Experimental Psychology: Learning, Memory, and Cognition 17(1), 163–169 (1991)
30. Herron, C., York, H., Corrie, C., Cole, S.: A comparison study of the eﬀects of a
story-based video instructional package versus a text-based instructional package
in the intermediate-level foreign language classroom. Calico Journal 23(2), 281
(2006)
31. Hoofnagle, C.J., King, J., Li, S., Turow, J.: How Diﬀerent are Young Adults from
Older Adults When it Comes to Information Privacy Attitudes and Policies? SSRN
eLibrary (2010)
32. Johnson, B.B., Slovic, P.: Presenting uncertainty in health risk assessment: initial
studies of its eﬀects on risk perception and trust. Risk Analysis 15(4), 485–494
(1995)
33. Jonah, B.A., Thiessen, R., Au-Yeung, E.: Sensation seeking, risky driving and
behavioral adaptation. Accident Analysis & Prevention 33(5), 679–684 (2001)
34. Kahneman, D., Frederick, S.: Representativeness revisited: Attribute substitution
in intuitive judgment. Heuristics and Biases: The Psychology of Intuitive Judgment,
49–81 (2002)
35. L¨ovd´en, M., R¨onnlund, M., Wahlin, ˚A., B¨ackman, L., Nyberg, L., Nilsson, L.G.:
The extent of stability and change in episodic and semantic memory in old age:
Demographic predictors of level and change. The Journals of Gerontology Series
B: Psychological Sciences and Social Sciences 59(3), 130 (2004)
36. Nissenbaum, H.F.: Privacy in context: Technology, policy, and the integrity of social
life. Stanford Law & Politics (2010)
37. Norberg, P.A., Horne, D.R., Horne, D.A.: The privacy paradox: Personal infor-
mation disclosure intentions versus behaviors. Journal of Consumer Aﬀairs 41(1),
100–126 (2007)
38. Phelps, J., Nowak, G., Ferrell, E.: Privacy concerns and consumer willingness to
provide personal information. Journal of Public Policy & Marketing, 27–41 (2000)

298
V. Garg et al.
39. Posner, R.A.: Right of privacy, the. Ga. L. Rev. 12, 393 (1977)
40. Posner, R.A.: The economics of privacy. The American Economic Review 71(2),
405–409 (1981)
41. Schneier, B.: The psychology of security. Communications of the ACM 50(5), 128
(2007)
42. Sheng, S., Magnien, B., Kumaraguru, P., Acquisti, A., Cranor, L., Hong, J., Nunge,
E.: Anti-phishing phil: The design and evaluation of a game that teaches people
not to fall for phish. In: Proceedings of the 3rd Symposium on Usable Privacy and
Security, pp. 88–99. ACM (2007)
43. Sherman, S.J.: On the self-erasing nature of errors of prediction. Journal of Per-
sonality and Social Psychology 39(2), 211–221 (1980)
44. Sloman, S.A.: The empirical case for two systems of reasoning. Psychological Bul-
letin 119(1), 3 (1996)
45. Smetters, D.K., Grinter, R.E.: Moving from the design of usable security tech-
nologies to the design of useful secure applications. In: Proceedings of the 2002
Workshop on New Security Paradigms, NSPW 2002, pp. 82–89. ACM, New York
(2002)
46. Spencer, W.D., Raz, N.: Diﬀerential eﬀects of aging on memory for content and
context: A meta-analysis. Psychology and Aging 10(4), 527 (1995)
47. Tulving, E.: What is episodic memory? Current Directions in Psychological Sci-
ence 2(3), 67–70 (1993)
48. Tversky, A., Kahneman, D.: Judgment under uncertainty: Heuristics and biases.
Science 185(4157), 1124 (1974)
49. Wang, Y., Komanduri, S., Leon, P., Norcie, G., Acquisti, A., Cranor, L.: I regret-
ted the minute I pressed share.: A qualitative study of regrets on Facebook. In:
Symposium on Usable Privacy and Security (2011)
50. Whitten, A., Tygar, J.: Why Johnny cant encrypt: A usability evaluation of PGP
5.0. In: Proceedings of the 8th USENIX Security Symposium, vol. 99 (1999)
51. Wild, K., Boise, L., Lundell, J., Foucek, A.: Unobtrusive in-home monitoring of
cognitive and physical health: Reactions and perceptions of older adults. Journal
of Applied Gerontology 27(2), 181–200 (2008)
52. Willis, S., Schaie, K., Martin, M.: Cognitive plasticity. Handbook of Theories of
Aging, 295–322 (2009)

Use Fewer Instances of the Letter “i”: Toward
Writing Style Anonymization
Andrew W.E. McDonald, Sadia Afroz, Aylin Caliskan,
Ariel Stolerman, and Rachel Greenstadt
Drexel University, Philadelphia, PA
{awm32,sa499,ac993,ams573,greenie}@cs.drexel.edu
Abstract. This paper presents Anonymouth, a novel framework for
anonymizing writing style. Without accounting for style, anonymous au-
thors risk identiﬁcation. This framework is necessary to provide a tool for
testing the consistency of anonymized writing style and a mechanism for
adaptive attacks against stylometry techniques. Our framework deﬁnes
the steps necessary to anonymize documents and implements them. A
key contribution of this work is this framework, including novel meth-
ods for identifying which features of documents need to change and how
they must be changed to accomplish document anonymization. In our
experiment, 80% of the user study participants were able to anonymize
their documents in terms of a ﬁxed corpus and limited feature set used.
However, modifying pre-written documents were found to be diﬃcult
and the anonymization did not hold up to more extensive feature sets.
It is important to note that Anonymouth is only the ﬁrst step toward
a tool to acheive stylometric anonymity with respect to state-of-the-art
authorship attribution techniques. The topic needs further exploration
in order to accomplish signiﬁcant anonymity.
Keywords: stylometry, privacy, anonymity, machine learning.
1
Introduction
The Privacy Enhancing Technologies community has long been interested in
tools that enable people to participate in anonymous or pseudonymous speech1.
Current anonymity and circumvention systems focus strongly on location-based
privacy but do not address many avenues for the leakage of identiﬁcation through
the content of data. In particular, writing style as a marker of identity is not
addressed in current circumvention tools. Given the high accuracy of even basic
stylometry systems this is not a topic that can aﬀord to be overlooked.
Stylometry is a form of authorship recognition that relies on the linguistic in-
formation found in a document. While stylometry existed before computers and
artiﬁcial intelligence, the ﬁeld is currently dominated by AI techniques such as
neural networks and statistical pattern recognition. State-of-the-art stylometry
1 Selected Papers in Anonymity: http://freehaven.net/anonbib/
S. Fischer-H¨ubner and M. Wright (Eds.): PETS 2012, LNCS 7384, pp. 299–318, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

300
A.W.E. McDonald et al.
approaches can identify individuals in sets of 50 authors with over 90% accu-
racy [1]. Recent work has scaled stylometry methods to over 100,000 authors [2].
Stylometry is currently used in intelligence analsyis and forensics. The 2009
Technology Assessment for the State of the Art Biometrics Excellence Roadmap
(SABER) commissioned by the FBI stated that, “As non-handwritten commu-
nications become more prevalent, such as blogging, text messaging and emails,
there is a growing need to identify writers not by their written script, but by
analysis of the typed content [3].”
The stylometry ﬁeld has focused on creating new methods that attempt to
classify unknown works using known sets of authors, with little attention being
given to the question of what happens when an adversary tries to intention-
ally circumvent the classiﬁcation system that has been established. This paper
aims to provide a framework for researching and accomplishing writing style
anonymization: Anonymouth. Work by Brennan and Greenstadt has shown that
non-expert human subjects can defeat stylometry simply by consciously hiding
their writing style or imitating the style of another author [4]. However, when
analyzing the Brennan-Greenstadt Adversarial Stylometry Corpus, we ﬁnd that
some authors are more capable of composing anonymous documents than others.
Furthermore, a case study of the long-term pseudonymous blog, “A Gay Girl in
Damascus,” showed that even when authors were skilled at hiding their style,
doing so with consistency was diﬃcult [5]. There is currently active research in
ﬁnding stylometry methods that work on adversarial passages such as those in
the Brennan-Greenstadt corpus. Even if these methods succeed at identifying
these adversarial passages, they should be benchmarked against an adaptive at-
tack where the adversary has access to the features and tools used to identify
the text. Lastly, the limited research in circumventing stylometry has focused on
creating anonymous documents, whereas the Anonymouth framework provides
a mechanism to study the modiﬁcation/anonymization of documents that were
written without anonymity in mind.
This paper includes three key contributions.
1. Our Anonymouth framework deﬁnes the steps necessary to anonymize docu-
ments. Anonymouth’s novel feature clustering and prioritization algorithms
enable it to identify the changes necessary to anonymize a document relative
to a set of author documents and a set of linguistic features. We show that
modifying the features as suggested does result in anonymized documents.
2. We have implemented this framework via two tools, JStylo and Anonymouth,
that have been released under an open source license (GPL 3) and can serve
as a research platform for stylometry and adversarial stylometry2. This soft-
ware not only performs authorship attribution, but also calculates the fea-
tures that are most identifying and the ways the feature vectors must change
to provide anonymity. The software also provides suggestions to users to help
them anonymize their style. We found that 80% of user study participants
2 Available at https://psal.cs.drexel.edu/index.php/JStylo-Anonymouth

Toward Writing Style Anonymization
301
were able to anonymize their documents in terms of a data corpus and feature
set that is known to and chosen by the user before anonymization.
3. We have performed a user study to investigate whether and how users can
edit previously written documents so they obscure their authorship. We show
that this problem is harder than starting from scratch. Anonymouth can
suggest the right changes, but they are diﬃcult to implement. The methods
used to aid a user in anonymizing his document need further development
in order for Anonymouth to be eﬀective against state-of-the-art feature sets.
2
Related Work
Anonymization plays an important role in data privacy. Perfect anonymity is
hard to achieve. Private information about an individual can be revealed not
only from his name and physical and virtual addresses, but also from browser
conﬁguration [6], netﬂix movie ratings [7], and even from the public outputs
of a recommender system [8]. Anonymity at the network level can be achieved
through onion routing systems like Tor [9]. But the privacy concerns of writing
style are still not well-analyzed. Writing style is a serious threat to anonymity
and free speech. With the improvement of authorship recognition techniques, it
is possible to identify authorship of a document even among 100,000 authors [2].
Authorship attribution can be circumvented by changing writing style. All
authorship attribution techniques are based on the fact that people always write
in their regular style. Brennan et al. showed that current authorship attribution
techniques perform less than random chance if people hide their writing style by
imitating someone else or by obfuscating their regular style [4]. Though it is pos-
sible to change writing style, it is hard to maintain a separate style consistently
in anonymous writings [5].
Rao and Rohatgi suggested round trip machine translation (for example, En-
glish →German →English) as a possible method for document anonymization
[10]. But because of improvement in machine translation, empirical results have
shown that round trip machine translation is not eﬀective in obfuscating writing
style3.
Anonymization by obfuscating writing style was ﬁrst explored by Kacmarcik
et al. [11]. Their approach was to identify the features that a typical authorship
attribution method uses to attribute authorship and then adjust the frequencies
of these features to make them less eﬀective. They used the Federalist papers
and found that 14 changes per 1000 words are suﬃcient to reduce the likelihood
of identifying an author as himself. Our work diﬀers from this work in several
ways. First, they did not change the actual documents, only modiﬁed the feature
sets to prove obfuscation is possible to circumvent attribution. Anonymouth
helps the user to change the actual document. Second, their feature set was
limited, only function words were used. Anonymouth supports both the Basic-
9 [4] feature set and the Writeprints [1] features. Third, only the 12 disputed
3 http://events.ccc.de/congress/2009/Fahrplan/events/3468.en.html

302
A.W.E. McDonald et al.
Federalist papers were analyzed, whereas Anonymouth allows obfuscation of any
written document.
There is considerable prior work in authorship attribution [1,12,13]. But cur-
rently no tool is available to allow circumvention of the authorship attribution
techniques to achieve anonymity. Anonymouth is the ﬁrst research to explore the
idea of changing writing style to anonymize a written document. Anonymouth
makes a user aware of the idiosyncrasies of his writing style. It allows users to
choose a background corpus in terms of which a document can be anonymized.
It indicates features that are unique to the user and suggests how a feature value
can be adjusted to achieve a suﬃcient level of anonymity.
3
Problem Statement
An author A has a document D that he wants to anonymize. The author selects
a set of his own writing Dpre and a set B of N authors where A /∈B. Author
A also chooses a feature set F and authorship attribution method M. The goal
is to create a new document D′ from D where the feature values F are changed
suﬃciently so that D′ does not appear to be written by A. To evaluate the
level of anonymity, Dpre is used. D′ is anonymized if a classiﬁer trained on Dpre
and documents written by B, Bpre, attributes authorship of D′ to A with a
probability p less than random chance, i.e. p ≤
1
N+1.
4
Approach
Our writing style anonymization framework consists of two platforms: JStylo
and Anonymouth. JStylo is a standalone platform for authorship attribution. It
is used as an underlying feature extraction and authorship attribution engine for
the anonymization framework. Anonymouth is the writing style anonimization
platform. It uses the extracted stylometric features and classiﬁcation results
obtained through JStylo and provides suggestions to users to anonymize their
writing style.
4.1
JStylo: An Authorship-Attribution Platform
JStylo uses NLP techniques to extract linguistic features from documents, and
supervised machine learning methods to classify those documents based on the
extracted features. JStylo ﬁrst “learns” the style of known candidate authors
based on documents of those authors, and the style of a given set of anonymous
documents. It then attributes authorship of the anonymous documents to any of
the known authors. JStylo is a Java-based open-source software with a graphic
user interface and an extendable API.
Structure and Usage. The main work-ﬂow of JStylo consists of four consec-
utive phases: deﬁning a problem set, deﬁning a feature set, selecting classiﬁers
and running the analysis.

Toward Writing Style Anonymization
303
A problem set is deﬁned by a training corpus, constructed of documents of
all potential authors (as it is supervised learning), and a set of documents of
unknown authorship whose authorship are to be determined.
A feature set is deﬁned by a set of various stylistic features to be extracted
from the text. Currently there are just above 50 diﬀerent conﬁgurable features
available, spanning over diﬀerent levels of the text, like parts-of-speech in the
syntactic level or word frequencies in the lexical level.
The current version of JStylo supports three pre-deﬁned feature sets: Basic-
9, Writeprints, and Writeprints (limited). The Basic-9 feature set consists of
the nine features that were used in the neural network experiments in [4]. The
Writeprints feature set consists of the features used for the Writeprints tech-
nique [1]. The Writeprints (Limited) feature set consists of the same features
used for Writeprints, where feature classes with potential of exceeding 50 fea-
tures (e.g. letter bigrams, with a potential of 262 features) are limited to the
top 50 features. The documents in the training set are mined for the selected
features, which are later used for training the classiﬁer, basically proﬁling the
stylistic characteristics of each candidate author. The same features are mined
in the test set, for later classiﬁcation by the trained classiﬁers.
Each feature is deﬁned by 1) optional text pre-processing tools that allow var-
ious ﬁltering methods, to be applied before the feature extraction (e.g. stripping
all punctuation); 2) the “core” of the feature which is the feature extractor it-
self; 3) optional feature post-processing tools to be applied on the features after
extraction (e.g. picking the top features frequency-wise); and 4) optional nor-
malization baselines and factoring multipliers (e.g. normalizing over the number
of words in each document). The components in 1-3 are based on the JGAAP
API [14].
The classiﬁers available for selection are a subset of Weka [15] classiﬁers com-
monly used, such as support vector machine, Na¨ıve Bayes, decision tree, etc.
There are several analysis conﬁgurations available, the main choice being either
to run a 10-fold cross validation analysis over the training corpus or to train the
classiﬁers using a training corpus and classifying the test documents.
JStylo as a Stylometry Research Platform. The main advantages and
novelties of JStylo are 1) allowing integration of multiple features to represent
various stylistic characteristics of documents, and 2) a high level of feature-set
customizability, where each feature can be conﬁgured with its own text pre-
processing tools, feature extractors, feature post-processing tools and normal-
ization methods. Its user-friendly graphic interface and Java API allow a high
level of usage across both linguistic researchers and computer scientists, provid-
ing a convenient platform for stylometry research.
Details of the performance and accuracy of JStylo as a stylometry research
platform are discussed in section 6.1.
4.2
Anonymouth: An Authorship–Anonymization Framework
Anonymouth aims to use the tools of authorship attribution to systematically
render them ineﬀective on a text, while preserving the message of the document

304
A.W.E. McDonald et al.
in question to the highest degree possible. The task of actively changing the
document is however, at this point, left to the user. For Anonymouth to be
able to read in a document and output an anonymized version satisfying the
constraint that the meaning be preserved, it would need a deep understanding
of the structure of the English language (assuming English text), knowledge of
almost all words, and a reasonable grasp of things like metaphors and idioms -
which is quite a tall order.
After initialization via JStylo, Anonymouth performs an iterative two-step
approach to achieve writing style anonymization. These steps are: 1) feature
clustering and preferential ordering, and 2) feature modiﬁcation and document
reclassiﬁcation.
Initialization.
Anonymouth requires4 the user (A) to input three sets
of documents: 1) a single document consisting of approximately 500 ±
25 words, the documentToAnonymize (D); 2) a set (at least 2, though
preferably
more)
of
sample
documents
written
by
the
user,
totaling
6500 ± 500
words,
the
userSampleDocuments (Dpre);
and
3)
a
corpus
—
preferably
made
up
of
at
least
3
diﬀerent
authors
—
of
sample
documents, not
written by the user, containing 6500 ± 500 words per
author, the otherSampleDocuments (Bpre). The userSampleDocuments are
used to determine where the documentToAnonymize’s features should not
be, while the otherSampleDocuments are used to determine where the
documentToAnonymize’s features could be moved to.
After an initial classiﬁcation has been produced (by JStylo), four groups
of features result: 1) those extracted from D, toAnonymizeFeatures; 2)
those extracted from Dpre, userSampleFeatures; 3) those extracted from
Bpre, otherSampleFeatures; and 4) a combination of the two previous
groups, userAndOtherFeatures. Anonymouth then runs Weka’s information
gain method on the userAndOtherFeatures to select the top f features ac-
cording to information gain. These top f features will be used in the subsequent
computations to generate suggestions for changing writing style. Among the top
f features, any that are not present in D are excluded from the suggestions
Anonymouth deliveres. Resultantly, f becomes f′. This is done to provide eﬀec-
tive suggestions because it cannot be freely assumed that any given feature can
be reasonably added to the document. This only applies to JStylo’s Writeprints
feature sets, where without excluding the non-existing features from suggestions
(as an extreme example), a user might be asked to include the word, “Elec-
tromagnetic” — when that particular word has no business appearing in the
document the user is interested in anonymizing.
Feature Clustering and Preferential Ordering. Knowing what features
to change in order to anonymize a document says nothing about how much
4 In its present state as a research platform rather than a software designed for an
end-user, this is the case. However, these limitations are by no means absolute.

Toward Writing Style Anonymization
305
to change them, nor does it indicate how much they can be changed and still
represent a coherent document that adheres to the rules of grammar. The cause–
and–eﬀect relationship among the stylometric features is comparable to that of
a ﬁeld of Dominoes: altering the sentence count of a text will impact the average
sentence length; which will aﬀect the Gunning-Fog Readability Index — which
happens to be inversely related to the Flesch-Kincaid Reading Ease Score; all of
which will inevitably change the character count and will (probably) change the
number of occurrences of the three letter word “and”. Because of this, it is hard
to decide exactly what changes can/should be made in an existing document.
However, individually grouping the values of every feature across all Bpre seems
to provide a fairly decent guideline. It allows Anonymouth to decide how to
change each of the f′ features based upon where the ‘real’ document’s features
lie with respect to both one another as well as the user’s normal distribution for
each feature. The clustering of all instances of each feature assists Anonymouth
in selecting physically realizable ‘target’ values to represent the ‘suggested’ ﬁnal
document conﬁguration that the user should aim to achieve in order to evade
authorship detection. The mechanism behind this selection process is presented
through the rest of this section.
Objects containing otherSampleFeatures and their respective document
names are then fed into a modiﬁed k-means clustering algorithm (described
in Algorithm 1). The algorithm clusters the objects with respect to each
Object’s value with, k = numAuthors (where numAuthors is the total num-
ber of authors), means, using a modiﬁed k-means++ [16] initialization algo-
rithm on a per feature basis spanning across all documents represented by
otherSampleFeatures. The most signiﬁcant change to the k-means algorithm
is that if any clusters exist with less than three elements after the algorithm
converges, the algorithm is re-initialized with k = k −1 means. A more accurate
representation might be ak-means. The reasoning behind this adjustment is: be-
cause target values for the documentToAnonymize are chosen as the centroids
of clusters, more elements weighing in on the target value (centroid) increases
the potential for anonymization — as opposed to having a single element cluster
and eﬀectively copying another’s writing style5. It remains to be seen whether
it would be beneﬁcial to scale the minimum cluster size limit as the number of
documents increases; as of now, the value remains ﬁxed.
Implementing these changes in the k-means++ and k-means algorithms cre-
ates a safety net that allows Anonymouth to deal with many potential issues
that may arise while analyzing an unknown number of documents with unknown
similarities/diﬀerences. Anonymouth assumes that the documents it receives will
be easily clustered. It will adapt if this is not the case, and produce the most
beneﬁcial output it can.
5 There is no guarantee that each cluster will contain documents from more than one
author. However, limiting the minimum cluster size helps increase the chances of
this happening. In practice, clusters have been observed to contain documents by
more than one author more often than not.

306
A.W.E. McDonald et al.
Algorithm 1. The ak-means Clustering Algorithm (Done on a per-feature basis)
1. Initialization:
(a) run
k-means++
algorithm
to
initialize
cluster’s
based
on
otherSampleFeatures, with the following exceptions:
i. If 10,000 numbers have been tried before ﬁnding a new centroid, restart.
ii. If all remaining unchosen values are the same, update the number of to-
tal centroids to number of current centroids, set maxCentroidsFound =
T rue, and exit initialization; nothing else can be done.
(b) Assign all instances of the current feature (one per document) from
otherSampleFeatures to the centroid nearest to it based on one-dimensional
euclidean distance. These are the initial clusters.
2. Update Centroids:
(a) Calculate the average of the elements (features) contained within each cluster,
and update that cluster’s centroid with the calculated average.
3. Reorganization:
(a) Calculate the linear distance between each element, and each existing centroid.
(b) Assign each element to its closest centroid based on the distance calculation
in (a).
(c) If no elements moved:
i. If maxCentroidsFound is T rue, or there are at least two clusters with no
less than 3 elements per cluster, algorithm has converged.
ii. If there is only one cluster and maxCentroidsFound is False, increment
numMeans, and Initialize.
iii. If
there
are
any
clusters
with
less
than
3
elements
and
maxCentroidsFound is False, decrement numMeans, and Initial-
ize.
(d) Else if elements did move:
i. Update centroids.
Once the ak-means algorithm has converged, clusters are assigned a preference
value based on the primary preference calculation, and placed into an i×j array
after being sorted (from least to greatest).
pi,j = numElementsi,j× | centroidi,j −userSampleMeani |
(1)
where: pi,j is the primary preference of feature i’s jth cluster; numElementsi,j is
the number of elements in feature i’s jth cluster; centroidi,j is the average of fea-
ture i’s jth cluster’s elements; and userSampleAvgi is the average of the user’s
sample documents, userSampleDocuments, for feature i. The purpose of taking
the number of elements into account rather than basing a cluster’s preference
value oﬀits distance from the user’s average values alone is to avoid attempt-
ing to modify a user’s documentToAnonymize to take the form of a document
who’s features lie in the extremes due to speciﬁc content, while refraining from
unwittingly restricting the pool of potential target values due to a single feature.
Ordering each feature’s clusters in such a way that the most desirable cluster
has the highest value also lays the groundwork that allows cluster groups to be
ordered by a secondary preference calculation.

Toward Writing Style Anonymization
307
The secondary preference calculation weights features with respect to their
information gain ranking, and ensures that cluster groups that appear with high
frequency take precedence over those that appear less often. Because the most
desirable cluster, as determined by the primary preference calculation in Eq.
(1), has the highest value, weighting the secondary preference calculation in
this manner is intended to assign the greatest cluster group preference to the
most common cluster group that has the most impact on the features with high
information gain. The centroids of the cluster group with the highest ranking
are likely to be the best target values for the documentToAnonymize. However,
because the primary and secondary preference calculations have not been com-
pletely optimized, it is possible that the actual best target cluster will be found
slightly further down the list of cluster group preferences. For this reason, as
well as to help validate the approach by graphically displaying the workings of
Anonymouth, the Clusters tab was created.
The “Clusters” tab, as seen in Fig. 1, displays the clusters formed by the
Algorithm 1, represented by the empty green ellipses which contain clusters
of blue dots representing the otherSampleFeatures. A shaded purple ellipse
displays the user’s conﬁdence interval (CI) for a given feature. CI is computed
using the following formula,
CI = Dpremean ± 1.96 × σ
(2)
where, Dpremean = average of all userSampleDocuments(Dpre), and σ = stan-
dard deviation from the mean. The visible red dot displays the present value of
the same feature in the documentToAnonymize, which Anonymouth tries to ‘put’
in the most populated location as far away from the purple shaded ellipse as pos-
sible. By selecting one of the available cluster conﬁgurations from the drop-down
menu, the user may view conﬁgurations from, PCG0 (which should provide the
greatest potential to anonymize the document) to PCGu−1 (which should provide
the least potential to anonymize the document), where u is the number of unique
document cluster conﬁgurations, and PCGn is the nth document cluster group’s
cluster group preference. Upon choosing a conﬁguration, one cluster per feature
will be shaded green, and is the target cluster for that feature within that conﬁg-
uration. When a cluster conﬁguration is selected, each target cluster’s centroid
— represented by the empty black circle — is set to be the target value for each
feature (respectively) within the documentToAnonymize.
One might ask, why not simply pick the cluster farthest away from the author’s
average value for each feature? The danger in doing this, as has been determined
experimentally, is that many features are co-dependent upon one another; so, it
may be physically impossible to modify a document to be represented by a set
of independently chosen features. For example, it is impossible to increase the
average sentence length of a document, while increasing the number of sentences
(assuming the idea is to keep the document more or less the same length). Target
values for features must be selected while being mindful of other features. Why,
then, not just use the document with a cluster group conﬁguration farthest from
the author’s standard set of values? This is done because ideally it is more feasible

308
A.W.E. McDonald et al.
to alter an existing document to look ‘generic’ than it is to attempt to drive it
toward an extreme, which may only be that way as a result of content (including
unusual errors that one might have a hard time trying to, or would not want
to, reproduce). If many documents share more or less the same conﬁguration,
there is a greater chance that any given document can also be ﬁt to share that
conﬁguration while maintaining readability. Furthermore, changing a document
to look more like many other documents should be more eﬀective in making it
anonymous than simply altering it to look as much unlike the true author’s work
as possible.
Fig. 1. Anonymouth Clusters tab with cluster group PCG0 selected, using the Basic-9
feature set, with 6 ‘otherSample’ authors. The red circles display the present value of
each feature within the user’s ‘documentToAnonymize’, the purple ellipses depict the
user’s conﬁdence interval (assuming a normal distribution) for each feature, and the
shaded green ellipses show where the user’s feature’s will fall if all features are modiﬁed
as recommended by Anonymouth.
Feature Modiﬁcation and Document Reclassiﬁcation. Once the targets
are selected, the user is presented with a clickable list of features to change.
When a feature is selected, a suggestion appears that aids the user in changing
the present value of the feature to its target value. The suggestions for the
Basic-9 feature set have been optimized to guide the user to change the elements
in their document that will have the greatest overall impact on its classiﬁcation.
An example of this is, “[replace] some single use words with less than 3 syllables
with words that have already been used and have 3 or more syllables”, as seen
in Fig. 2. Once the document has been changed so that its present values re-
ﬂect the target values, the document is reclassiﬁed. If the document has reached a

Toward Writing Style Anonymization
309
suﬃciently low classiﬁcation6 the document is considered anonymized. Until that
point, the process loops back to ‘feature clustering and preferential ordering.’
Every time the features are clustered, slightly diﬀerent clusters may result; which
leads to changing target values. We found that in some cases (especially for the
Writeprints features) clustering the features only once is a better alternative to
continually re-clustering the features upon every classiﬁcation.
Fig. 2. Anonymouth Editor tab showing the ‘Unique Words Count’ suggestion selected,
with unique words (only used once) highlighted, and an initial classiﬁcation attributing
authorship of the displayed text to the user of the Anonymouth with 79.5% probability
The Editor tab contains a ‘Dictionary’ which brings up an interface to Prince-
ton’s WordNet 3.0, allowing a user to search for synonyms and words containing
various continuous character strings (e.g. ‘ie’). A ‘verbose’ button will bring up
a window that prints Anonymouth’s standard output and error streams in real
time as well. Finally, should the user want to revert back to a previous copy of
the documentToAnonymize, tabs that display where each copy of the document
originated from permit the user to trace back through processed changes, while
viewing each document’s classiﬁcation results.
5
Anonymouth User Study
We performed a user study with 10 participants to understand the eﬀectiveness
of Anonymouth in changing writing style. We evaluated Anonymouth based on
its eﬀectiveness in anonymizing a document and its ease of use from a user’s
perspective.
6 In this case, a suﬃciently low classiﬁcation means at or below random chance, which
is 1/(numAuthors), where numAuthors is the total number of authors.

310
A.W.E. McDonald et al.
We asked participants to anonymize their pre-written writing samples.
Anonymizing a pre-written text is more diﬃcult than writing in a changed style
from the start. Stylometry methods fail to attribute authorship when people
write in a diﬀerent style [4]. We wanted to evaluate how much anonymity can be
achieved by changing the writing style of a pre-existing document. The subjects
were asked to submit 6500 words of pre-existing writing, along with a document
of approximately 500 words to modify. The 6500-word sample was used as the
subject’s training sample. We chose to use 6500 words of writing as it has been
found to be enough to leak the identity of an author [10]. As a background cor-
pus, we used regular writing samples of six authors from the Brennan-Greenstadt
adversarial corpus. Anonymouth allows the user to choose any corpus as a back-
ground corpus. But for the purpose of the study we ﬁxed the background corpus
for all users.
During the experiment, each user was asked to use the Basic-9 feature set and
SMO SVM classiﬁer as authorship attribution method. Anonymouth provides
the option of choosing any feature set and any classiﬁer. The reason we ﬁxed the
feature set is because changing an existing document with Writeprints features
is very hard, and after the ﬁrst two participants failed to follow the suggestion
for changing Writeprints features we decided not to use it in this study. We
excluded the Writeprints result of the ﬁrst two participants. We only used all
of the 10 participants results with Basic-9 features. SMO SVM is used for its
high accuracy. The users were asked to perform classiﬁcation of their document,
choose the appropriate cluster from the clustering window and change the docu-
ment based on Anonymouth’s suggestion. We suggested the users to repeat the
process for one hour, or until the result of the classiﬁcation goes below random
chance (which was 14% accuracy in our case).
After a user successfully anonymized his/her document or used Anonymouth
for an hour, we asked them to rate several aspect of Anonymouth on a 10-point
Likert scale. The survey asked basic demographics questions.
6
Evaluation and Results
This section discusses the results of the Anonymouth user study. The following
subsections explain the eﬀectiveness of JStylo in attributing authorship, eﬀec-
tiveness of Anonymouth in anonymizing a document, the eﬀect of the choice of
background corpus and feature set on anonymity, which features were changed
by the users to achieve anonymity, and the user satisfaction survey.
6.1
Eﬀectiveness of JStylo
To evaluate the eﬀectiveness of JStylo as a suﬃciently accurate authorship
attribution engine for Anonymouth and as an authorship attribution research
platform in general, we conducted experiments using the Brennan-Greenstadt
Adversarial Stylometry Corpus, which includes 13 authors with 5000-word docu-
ments each. We then compared the results with those of two other state-of-the-art

Toward Writing Style Anonymization
311
authorship attribution methods in the literature: the Writeprints method and
the synonym-based approach [17]. The experiments with JStylo were conducted
using a SVM classiﬁer, over two feature sets: the Basic-9 and the Writeprints
(Limited). All experiments were evaluated using 10-folds cross-validation. The
results are summarized in table 1.
Table 1. Authorship attribution results using Writeprints, Synonym-based and JStylo
Method
Accuracy
Writeprints
73.33%
Synonym-based
89.61%
JStylo with Basic-9
53.98%
JStylo with Writeprints (Limited) 92.03%
Although the Basic-9 feature set did not produce as high results as the other
methods, it is still much higher than random chance (7.69%), and is used only
as baseline for authorship attribution features in JStylo, or anonymization fea-
tures baseline in Anonymouth. It is notable that using the Writeprints (Limited)
feature set with JStylo produced the highest results across all four experiments.
6.2
Eﬀectiveness of Anonymouth
Figure 3 shows authorship attribution accuracy of the modiﬁed and unmodiﬁed
documents. Using the Basic-9 features 80% participants were able to anonymize
their documents in terms of the corpus used. The ﬁrst participant’s (s1) original
document was not attributed to him as an author. The second participant (s2)
made no changes to his document. All other participants were able to anonymize
their documents.
6.3
Eﬀect of the Background Corpus on Anonymity
The background corpus, or set of reference authors and documents, is important
for document anonymization with Anonymouth as the tool calculates the average
value of each feature based on the background corpus and suggests changes to
users based on the average feature values.
We tested if documents anonymized in terms of one background corpus are
also anonymized against a diﬀerent background corpus. To test this, we used a
diﬀerent six author subset from the Brennan-Greenstadt adversarial corpus. We
also tested the results using the whole 13-author corpus. Results are shown in
Figure 6.3 (a) and Figure 6.3 (b). The eﬀectiveness of the anonymization changes
if the background corpus is changed. Unfortunately, the basic 9-Feature set is not
very eﬀective at stylometry. Where possible, we pre-selected documents that were
correctly classiﬁed for the anonymization with respect to the original background
corpus. However, when we switched to the new background corpus, only four of
these were correctly classiﬁed. Of these four, 50% (2) of the authors’ documents

312
A.W.E. McDonald et al.
0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
s1 
s2 
s3 
s4 
s5 
s6 
s7 
s8 
s9 
s10 
Authorship Attribution Accuracy 
Participants 
Anonymization in terms of original background corpus 
Original document 
Modified document 
Random Chance 
Fig. 3. Authorship attribution accuracy of modiﬁed and original documents using the
original background corpus. The Basic-9 feature set and SMO SVM classifer were used.
All subjects who made changes were able to anonymize their documents (8/10).
were still anonymized even in terms of a diﬀerent corpus of six authors and the
others remained anonymized (as shown in Figure 6.3 (a)). For the corpus of 13
authors, 5 subjects’ original documents were classiﬁed correctly and all modiﬁed
documents were classiﬁed incorrectly (as shown in Figure 6.3(b)).
6.4
Eﬀect of Feature Set on Anonymity
We wanted to see if documents anonymized with one authorship attribution
approach are detectable by another approach. Unfortunately in every case doc-
uments anonymized with Basic-9 features were attributed to the real author
when Writeprints (Limited) feature were used. The Writeprints feature set is
much larger than Basic-9, contains around 700 linguistic, content speciﬁc and
structural features. Most of these features are very low level features, for exam-
ple, frequencies of character uni-/bi-/tri-grams. Providing eﬀective suggestions
0 
20 
40 
60 
80 
100 
s1 
s3 
s4 
s5 
s6 
s7 
s8 
s9 
s10 
Authorship Attribution Accuracy 
Participants 
Anonymization in terms of different background 
corpus (six authors) 
Original document 
Modified document 
Random Chance
0 
20 
40 
60 
80 
100 
s1 
s3 
s4 
s5 
s6 
s7 
s8 
s9 
s10 
Authorship Attribution Accuracy  
Participants 
Anonymization in terms of Brennan-
Greenstadt corpus (13 authors) 
Original document 
Modified document 
Random Chance 
Fig. 4. Authorship attribution accuracy of modiﬁed and original documents using six
diﬀerent author samples as background corpus 6.3 (a) and 13 authors as background
corpus 6.3 (b). The Basic-9 feature set and SMO SVM classiﬁer were used.

Toward Writing Style Anonymization
313
for such low level features is challenging. Changing existing documents by fol-
lowing those suggestions to hide author speciﬁc features is also very diﬃcult.
For this reason, none of the participants in our study were able to anonymize
themselves using the Writeprints (Limited) features.
We wanted to evaluate functionality of Anonymouth using the Writeprints
(Limited) features to ﬁnd out the minimum number of features that need to
be changed to anonymize a document. To do this, we ﬁrst ranked the features
based on information gain ratio[ ?? ]. Then we calculated clusters of feature
values using Anonymouth. We chose the top K features based on information
gain ratio and changed their values with those of the ﬁrst cluster, where K= 25,
50, 75, ..., 300. Result of the experiment is shown in Figure 5.
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100  110  120  130  140  150  160  170  180
Authorship Prediction Confidence
Number of Top Feature Values Changed
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
Fig. 5. Number of Writeprints (Limited) features needs to be changed to achieve
anonymity. Authorship prediction accuracy decreases as the top feature values are
replaced with those predicted by Anonymouth. Suﬃcient anonymity can be achieved
after changing 13.79% (96 out of 696) of the features.
The result shows that authorship prediction accuracy decreases as the top
feature values are replaced with the values predicted by Anonymouth. After
changing 13.79% of the features, 90% of the documents were anonymized. This
experiment shows that the core approach of Anonymouth works successfully to
anonymize a document even against a robust feature set like Writeprints.
6.5
Change in Features
We compare the frequencies of diﬀerent features to understand which ones
people change to anonymize their writing style. The changes made to features
are shown in Figure 6. We only used samples of the participants who were
successful in anonymizing their documents. This graph illustrates the changes
in frequencies for each feature. The y-axis contains a list of features that have
been adjusted in the passages and x-axis of the graph denotes the change in
each feature. We compute the change in feature using the following formula:

314
A.W.E. McDonald et al.
Change in Feature f, Cf = (fmod −fori)/(fori)
where,
fmod = Value of feature f in the modiﬁed document.
fori = Values of feature f in the original document.
The amount to the right of the y-axis represents the increases in a feature and the
amount to the left represents the decreases. 87.5% of the successful participants
(7/8) increased average sentence length and decreased sentence count. Average
syllable count was increased in 75% of the cases. Increase in complexity was also
noticed in every anonymized document. This indicates that most participants
made their language complicated to anonymize their documents, which is also
evident by the increase of the Gunning-Fog (GF) readability index.






	



 !
 !
"#$
%
"##
&'$
(&$
	


)
*
+
,

-
.
Fig. 6. Feature changes to achieve anonymity
This result diﬀers from the feature changes in the Brennan-Greenstadt adver-
sarial documents where participants used simpler language to obfuscated their
document [5]. Ideally a document can be anonymized by using a language that
is either more complex or less complex than the original writing style of an au-
thor. As seen in [5], people usually use less complex language while obfuscating
their writing style, which is easily distinguishable from regular writings. Anony-
mouth allows user to choose his own background corpus and provide suggestions
to change his writing style. Thus by choosing a diverse background corpus an
author can hide both his writing style and the indication of changing style.
6.6
User Experience Survey
We had 10 participants in the study within 18-45 age limit who are daily com-
puter users. 2 of them were females and 8 of them were males. On average, the
users considered themselves to be moderately good writers. The participants all
either had or were working on college degrees, most with diﬀerent majors, and

Toward Writing Style Anonymization
315
90% of them were native English speakers. None of the subjects had any previous
knowledge of linguistics or stylometry.
The detailed evaluation of Anonymouth was covered in the ﬁrst part of the
survey which had a 10-point scale, with ‘0’ being the lowest/worst and ‘9’ being
the highest/best. The following graph summarizes the reaction of the subjects
to Anonymouth that was captured in the ﬁrst part of the survey.
0 1 2 3 4 5 6 7 8 9 
Anonymouth reuse 
Help of highlighting 
Input prompts 
Performing tasks 
Help messages 
Speed 
Anonymization success 
Required user level 
Fig. 7. Anonymouth user experience survey
On average, participants found Anonymouth was user-friendly and that it
did not require any speciﬁc background knowledge to use. Anonymouth’s word
highlighting feature was highly rated as helpful. The speed of Anonymouth was
considered very fast. Participants felt Anonymouth was moderately successful
in anonymizing documents (rated 4.9 on Likert chart). 7 of the participants said
they would recommend Anonymouth to other people.
7
Discussion
Although it appears to be quite challenging for a user to implement the changes
that Anonymouth asks for, even when only using Brennan and Greenstadt’s
‘Basic-9’ feature set, preliminary results suggest that when users are able to do
what is asked, they can successfully anonymize their documents — with respect
to that feature set. As shown in Fig. 3, 80% of participants were able to reduce
the accuracy of the SVM classiﬁer used with respect to the original background
corpus used. Furthermore, 60% of participants succeeded in achieving a ﬁnal
classiﬁcation probability below random chance, which for a total of 7 authors is
just under 14.3%.
Initial user tests using the ‘Writeprints (Limited)’ feature set implemented by
JStylo suggested less usability than existed when using the Basic-9 feature set in
terms of users being able to perform the actions requested by Anonymouth. Due
to the complex nature of the Writeprints (Limited) feature set, the user is asked
to do things like add more of the letter ‘i’ to his/her document, or to decrease
the number of occurrences of a part of speech n-gram. While no one was able to
anonymize their document with respect to the Writeprints (Limited) feature set,

316
A.W.E. McDonald et al.
it has been shown that in general, if approximately 15% of possible features are
changed to the values determined by Anonymouth, a document initially classiﬁed
as having been written by its actual author with 98% probability, will — about
80% of the time — end up being classiﬁed as having been written by another
author with over 95% probability.
This suggests that the core of Anonymouth — the methods used to determine
what and how much should be changed within a document — have some merit.
That is not to say that Anonymouth’s core has either been optimally adjusted
or is in fact the best way to decide how to anonymize a document. There is a
clear separation between knowing the degree to which certain things need to be
changed, and being able to execute those changes. Resulting from ﬁnding that
Anonymouth’s suggestions to the user regarding how to make these changes need
re-working, it is quite possible that Anonymouth’s algorithms will need to be
re-worked as well.
7.1
Future Work
In general, it seems as though the information presented to the user should be
of a higher level, such as, “re-write this sentence using the third person and in
the past tense”. Of course, doing just this is not the solution. In attempting to
anonymize a document via a set of na¨ıve algorithms, there appears to be a trade-
oﬀbetween anonymity and aﬀect. Assuming that the author of a document has
written that document in a style that he usually writes in, it is very diﬃcult for
that author to go back to another document and modify it to then appear in
a diﬀerent style, while retaining the document’s meaning (it is assumed that in
order to retain meaning, the imagery and tone would have to create the same
end result). Simply stripping descriptive words, modifying tense, and altering the
point of view (e.g. from third to ﬁrst person) would certainly increase anonymity;
though clearly at the expense of the documents impact on the audience (aﬀect).
While this is one approach that may be taken, it seems far from ideal, and as
though it ought to be considered as a last resort.
To achieve its goal, Anonymouth must be able to understand what a sen-
tence/passage means to the extent necessary to enable it to produce an output
passage expressed using language constructs foreign to the original author’s work
that can at least capture the main idea and tone of the original passage. While a
perfect system would be quite challenging to implement, constructing a system
that oﬀers a list of potentially reasonable alternatives to a given passage seems
to be a realizable goal.
Adding these features to Anonymouth would resolve the current usability
problem that limits the application of Anonymouth.
8
Conclusion
This paper presents Anonymouth, a novel framework for anonymizing writ-
ing style.Without accounting for style, anonymous authors risk identiﬁcation.

Toward Writing Style Anonymization
317
This framework is necessary to provide a tool for testing the consistency of
anonymized writing style and a mechanism for adaptive attacks against stylom-
etry techniques. Our framework deﬁnes the steps necessary to anonymize docu-
ments and implements them via Anonymouth and JStylo. These are (1) Analysis
of the documents using authorship attribution techniques relative to a corpus
of text and a set of linguistic features (implemented via JStylo), (2) Determin-
ing the features that need to be changed (using information gain), (3) Ordering
the features to be changed and determining where they need to go (using our
modiﬁed k-means clustering approach), (4) Suggesting changes to achieve these
changes (via Anonymouth). We have shown that these steps are eﬀective, that
users who make the suggested changes do anonymize their documents relative
to the suggested feature sets, and that Anonymouth does help users reduce the
accuracy of stylometry techniques. However, our user studies suggest that step
4 is quite diﬃcult and signiﬁcant research remains to determine the best way
to suggest changes that are easy to apply, especially for the large and complex
feature sets that result in the highest accuracy. It is not so easy to use fewer ’i’s.
This paper presents the ﬁrst study that evaluates modifying pre-written doc-
uments to anonymize style. We found that this was much more diﬃcult than
creating anonymous documents from scratch. Human subjects can create a doc-
ument that evades multiple state-of-the-art authorship techniques in 30-60 min-
utes without access to those techniques [4]. However, an hour was sometimes not
enough to anonymize pre-existing documents in reference to a limited feature set
and the changes made did not transfer to analysis with other feature sets, show-
ing that the choice of feature set is critical when using a tool like Anonymouth.
More research is needed to better anonymize pre-existing documents as people
do not often write with anonymity in mind and may wish to publish documents
previously written without compromising their identity.
Acknowledgements. We thank DARPA (grant N10AP20014) and the Intel
Science and Technology Center (ISTC) for Secure Computing for supporting
this work. We thank our colleagues Pavan Kantharaju, Michael Brennan for their
invaluable suggestions on the project. We would also like to thank our shepherd
Arvind Narayanan and the anonymous reviewers for their useful feedback.
References
1. Abbasi, A., Chen, H.: Writeprints: A stylometric approach to identity-level identi-
ﬁcation and similarity detection in cyberspace. ACM Trans. Inf. Syst. 26(2), 1–29
(2008)
2. Narayanan, A., Paskov, H., Gong, N., Bethencourt, J., Stefanov, E., Shin, R., Song,
D.: On the feasibility of internet-scale author identiﬁcation. In: Proceedings of the
33rd Conference on IEEE Symposium on Security and Privacy. IEEE (2012)
3. Wayman, J., Orlans, N., Hu, Q., Goodman, F., Ulrich, A., Valencia, V.: Technology
assessment for the state of the art biometrics excellence roadmap (March 2009),
http://www.biometriccoe.gov/SABER/index.htm

318
A.W.E. McDonald et al.
4. Brennan, M., Greenstadt, R.: Practical attacks against authorship recognition tech-
niques. In: Proceedings of the Twenty-First Innovative Applications of Artiﬁcial
Intelligence Conference (2009)
5. Afroz, S., Brennan, M., Greenstadt, R.: Detecting hoaxes, frauds, and deception in
writing style online. In: Proceedings of the 33rd Conference on IEEE Symposium
on Security and Privacy. IEEE (2012)
6. Eckersley, P.: How Unique Is Your Web Browser? In: Atallah, M.J., Hopper, N.J.
(eds.) PETS 2010. LNCS, vol. 6205, pp. 1–18. Springer, Heidelberg (2010)
7. Narayanan, A., Shmatikov, V.: Robust de-anonymization of large sparse datasets.
In: Proceedings of the 29th IEEE Symposium on Security and Privacy, pp. 111–125.
IEEE (2008)
8. Calandrino, J.A., Kilzer, A., Narayanan, A., Felten, E.W., Shmatikov, V.: You
might also like: Privacy risks of collaborative ﬁltering. In: Proceedings of the 32nd
IEEE Symposium on Security and Privacy, pp. 231–246. IEEE (2011)
9. Dingledine, R., Mathewson, N., Syverson, P.: Tor: The second-generation onion
router. In: Proceedings of the 13th Conference on USENIX Security Symposium,
vol. 13, pp. 21–21. USENIX Association (2004)
10. Rao, J.R., Rohatgi, P.: Can pseudonymity really guarantee privacy. In: Proceedings
of the Ninth USENIX Security Symposium, pp. 85–96 (2000)
11. Kacmarcik, G., Gamon, M.: Obfuscating document stylometry to preserve author
anonymity. In: Proceedings of the COLING/ACL on Main Conference Poster Ses-
sions, pp. 444–451. Association for Computational Linguistics (2006)
12. Juola, P.: Authorship attribution. Foundations and Trends in information Re-
trieval 1(3), 233–334 (2008)
13. Uzuner, ¨O., Katz, B.: A Comparative Study of Language Models for Book and
Author Recognition. In: Dale, R., Wong, K.-F., Su, J., Kwong, O.Y. (eds.) IJCNLP
2005. LNCS (LNAI), vol. 3651, pp. 969–980. Springer, Heidelberg (2005)
14. Juola, P.: Jgaap, a java-based, modular, program for textual analysis, text catego-
rization, and authorship attribution
15. Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.:
The weka data mining software: an update. ACM SIGKDD Explorations Newslet-
ter 11(1), 10–18 (2009)
16. Arthur, D., Vassilvitskii, S.: k-means++: the advantages of careful seeding. In:
Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algo-
rithms, SODA 2007, Philadelphia, PA, USA, pp. 1027–1035 (2007)
17. Clark, J.H., Hannon, C.J.: A Classiﬁer System for Author Recognition Using
Synonym-Based Features. In: Gelbukh, A., Kuri Morales, ´A.F. (eds.) MICAI 2007.
LNCS (LNAI), vol. 4827, pp. 839–849. Springer, Heidelberg (2007)

Author Index
Afroz, Sadia
299
Alhadidi, Dima
120
Blass, Erik-Oliver
180
Boneh, Dan
239
Borisov, Nikita
58
Caliskan, Aylin
299
Camp, L. Jean
279
Castelluccia, Claude
1
Chan, T.-H. Hubert
140
Chang, Ee-Chien
160
Cheng, Pengsu
79
Connelly, Katherine
279
Danezis, George
18
Debbabi, Mourad
79, 120
Dingledine, Roger
239
Di Pietro, Roberto
180
Ellithorpe, Jonathan
239
Fang, Chengfang
160
Fiﬁeld, David
239
Fung, Benjamin C.M.
120
Garg, Vaibhav
279
Gilad, Yossi
100
Gong, Xun
58
Greenstadt, Rachel
299
Hardison, Nate
239
Herzberg, Amir
100
Jawurek, Marek
221
Joosen, Wouter
259
Kaafar, Mohamed-Ali
1
Kerschbaum, Florian
221
Kiyavash, Negar
58
Kohlweiss, Markulf
18
Li, Mingfei
140
Liu, Wen Ming
79
Livshits, Benjamin
18
Lorenzen-Huber, Lesa
279
McDonald, Andrew W.E.
299
Mohammed, Noman
120
Molva, Reﬁk
180
Nikiforakis, Nick
259
¨Onen, Melek
180
P´erez-Gonz´alez, Fernando
38
Piessens, Frank
259
Pirker, Martin
201
Porras, Phil
239
Ren, Kui
79
Rial, Alfredo
18
Schear, Nabil
58
Shi, Elaine
140
Slamanig, Daniel
201
Stark, Emily
239
Stolerman, Ariel
299
Tran, Minh-Dung
1
Troncoso, Carmela
38
Van Acker, Steven
259
Wang, Lingyu
79
Winter, Johannes
201
Xu, Wenchang
140


www.allitebooks.com

VMware vSphere Resource 
Management Essentials
Optimum solutions to help you manage your VMware 
vSphere resources effectively
Jonathan Frappier
BIRMINGHAM - MUMBAI
www.allitebooks.com

VMware vSphere Resource Management Essentials
Copyright © 2014 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval 
system, or transmitted in any form or by any means, without the prior written 
permission of the publisher, except in the case of brief quotations embedded in 
critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy  
of the information presented. However, the information contained in this book  
is sold without warranty, either express or implied. Neither the author, nor Packt 
Publishing, and its dealers and distributors will be held liable for any damages 
caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the 
companies and products mentioned in this book by the appropriate use of capitals. 
However, Packt Publishing cannot guarantee the accuracy of this information.
First published: February 2014
Production Reference: 1130214
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham B3 2PB, UK.
ISBN 978-1-78217-046-4
www.packtpub.com
Cover Image by Jeremy Segal (info@jsegalphoto.com)
www.allitebooks.com

Credits
Author
Jonathan Frappier
Reviewers
Angelo Luciani
Mario Russo
Akmal Khaleeq Waheed
Acquisition Editor
Meeta Rajani
Content Development Editor
Poonam Jain
Technical Editors
Venu Manthena
Shruti Rawool
Copy Editors
Mradula Hegde
Gladson Monteiro
Project Coordinator
Aboli Ambardekar
Proofreader
Ameesha Green
Indexer
Mariammal Chettiyar
Graphics
Ronak Dhruv
Production Coordinator
Aditi Gajjar Patel
Cover Work
Aditi Gajjar Patel
www.allitebooks.com

About the Author
Jonathan Frappier is a hands-on technology professional with over 15 
years of experience in VMware-virtualized environments, focusing on system 
interoperability. He has specialization at the intersection of system administration, 
virtualization, security, cloud computing, and social enterprise collaboration.
He had not touched a computer until high school but then quickly found his  
passion. Jonathan holds a Bachelor's degree in Computer Science from Newbury 
College and a Master's degree in Computer Information Systems from Boston 
University, which he completed while working full time. He holds VMware 
certifications, including VCAP5-DCD, VCP5-DCV, VCA-Cloud, DCV, and WM.
Jonathan has worked in enterprises and start-ups throughout his career and has  
become a self-defined jack of all trades, but he is most passionate about virtualization 
and its community, and was honored as a vExpert 2013 for his contributions.
You can find Jonathan on Twitter @jfrappier, and on his blog at  
www.virtxpert.com, as well as at almost every Virtualization Technology 
User Group (VTUG) meet. He also supports the #vBrownBag podcast at 
professionalvmware.com.
First and foremost, I'd like to thank my wife for putting up with me 
for all these years and my daughter who will have to put up with me 
for many more. I am thankful to my parents for supporting me even 
when I had no idea why I was going to college. I would also not be 
here if it were not for my grandparents, who I was fortunate to know 
well in my adult life, and in loving memory of my grandfather, 
Norman L'Heurex, and Pepere Lionel Frappier. Just as much love is 
due to my friends Jim, Jeremy, Manny, Bob, and Igor.
I'd also like to thank the following people from the virtualization 
community: Matt, Sean, Luigi, and others—the "Nerdherd". I'm not 
sure I'd be doing this if it weren't for that dinner at Ichigo Ichie. Also, a 
big thank you to the #vBrownBag crew for giving me the opportunity 
to give back through this incredible organization; to all the folks out 
there blogging, sharing, and helping others; and the folks at VMware 
who support the community. Your contribution inspires me.
www.allitebooks.com

About the Reviewers
Angelo Luciani is an IT Professional in the financial industry who specializes in 
VMware virtualization and systems management. He is also the Toronto VMware 
User Group (VMUG) leader and enjoys contributing to the virtualization community. 
You can connect with Angelo at www.twitter.com/AngeloLuciani  
or head over to his blog at www.virtuwise.com.
Mario Russo has worked as an IT Architect, a Senior Technical VMware Trainer, and 
in the presales department. He has also worked on VMware technology since 2004.
In 2005, he worked for IBM on the first large project Consolidation for Telecom Italia 
on the Virtual VMware ESX 2.5.1 platform in Italy with the Physical to Virtual (P2V) 
tool. In 2007, he conducted a drafting course and training for BancoPosta, Italy, 
and project disaster and recovery (DR Open) for IBM and EMC. In 2008, he worked 
for the project Speed Up Consolidation BNP and the migration of P2V on the VI3 
infrastructure at BNP Cardif Insurance.
He is a VCI certified instructor level 2s of VMware and is certified in VCAP5-DCA.
He is the owner of Business to Virtual, which specializes in virtualization solutions.
He was also the technical reviewer of many books, including Implementing VMware 
Horizon View 5.2 by Jason Ventresco, Implementing VMware vCenter Server by Konstantin 
Kuminsky, Troubleshooting vSphere Storage by Mike Preston, VMware Horizon View 5.3 
Design Patterns and Best Practices by Jason Ventresco, all published by Packt Publishing.
I would like to thank my wife, Lina, and my daughter, Gaia. They're 
my strength.
www.allitebooks.com

Akmal Khaleeq Waheed is a solutions architect working on virtualization 
technologies. VMware being his primary expertise, he keeps an eye on competitive 
virtualization technologies such as MS, Citrix, and Red Hat. He has previously 
worked with Enterprise Server at Hewlett Packard and virtualization at VMware, 
Inc. He is VCA, VCP, VCAP-DCA, and DCD certified and the first winner of Virtual 
Design Master 2013-VDM001, first ever IT reality competition organized by the 
VMware community.
www.allitebooks.com

www.PacktPub.com
Support files, eBooks, discount offers and more
You might want to visit www.PacktPub.com for support files and downloads related to  
your book. 
Did you know that Packt offers eBook versions of every book published, with PDF and ePub 
files available? You can upgrade to the eBook version at www.PacktPub.com and as a print 
book customer, you are entitled to a discount on the eBook copy. Get in touch with us at 
service@packtpub.com for more details.
At www.PacktPub.com, you can also read a collection of free technical articles, sign up for a 
range of free newsletters and receive exclusive discounts and offers on Packt books and eBooks.
TM
http://PacktLib.PacktPub.com
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book 
library. Here, you can access, read and search across Packt's entire library of books. 
Why Subscribe?
•	
Fully searchable across every book published by Packt
•	
Copy and paste, print and bookmark content
•	
On demand and accessible via web browser
Free Access for Packt account holders
If you have an account with Packt at www.PacktPub.com, you can use this to access 
PacktLib today and view nine entirely free books. Simply use your login credentials for 
immediate access.
Instant Updates on New Packt Books
Get notified! Find out when new books are published by following @PacktEnterprise on 
Twitter, or the Packt Enterprise Facebook page.
www.allitebooks.com

www.allitebooks.com

Table of Contents
Preface	
1
Chapter 1: Understanding vSphere System Requirements	
5
ESXi system requirements	
6
vCenter components	
9
Understanding vSphere features	
10
Topology basics	
12
Understanding vSphere data center	
12
Familiarizing yourself with a vSphere cluster	
12
What is a vSphere host?	
13
Remembering configuration maximums	
14
Virtual machine maximums (per VM)	
14
ESXi host maximums (per host)	
14
Cluster maximums	
15
Determining resource utilization requirements	
15
Monitoring common resource statistics	
17
Sample workload	
18
Collecting statistics on Windows	
18
Collecting statistics on Linux	
20
Summary	
21
Chapter 2: Assigning Resources to VMs	
23
The basics of overcommitment and virtualization	
23
CPU scheduling and the effect of multiple vCPU VMs	
24
Memory assignment and management	
29
Memory overhead	
29
Transparent page sharing and memory compression	
30
Ballooning	
31
The VSWP swap file	
33
Monitoring memory usage	
34
www.allitebooks.com

Table of Contents
[ ii ]
Storage considerations and their effects on performance	
35
What is IOPS?	
36
RAID	
36
VMware vSphere Storage APIs – Array Integration (VAAI)	
38
Connectivity and throughput	
39
VMFS	
39
VM disk provisioning	
40
Monitoring storage	
41
Networking	
41
Uplinks	
42
What is a vSwitch?	
43
Monitoring network connectivity with ESXTOP	
43
Summary	
44
Chapter 3: Advanced Resource Management Features	
45
Understanding CPU power management	
46
Reservations, limits, and shares	
47
Resource limits	
48
Resource shares	
48
Resource pools	
49
vApps	
49
vMotion	
50
Enhanced vMotion Capability (EVC)	
52
How to enable EVC	
53
DRS	
54
DRS affinity and anti-affinity rules	
55
High Availability	
56
Admission control	
57
App HA	
59
Fault Tolerance	
60
Hot Add	
61
Storage vMotion, Storage DRS, and Datastore clusters	
61
Storage vMotion	
62
Datastore clusters	
63
Storage DRS	
64
vSphere Distributed Switches	
65
New in vSphere 5.5	
68
vSphere Flash Read Cache	
68
VSAN	
69
Summary	
70

Table of Contents
[ iii ]
Chapter 4: Automation and Monitoring Options	
71
Automation solutions for vSphere	
72
Cloning VMs	
73
VM templates	
74
Update Manager	
75
Host profiles	
76
Auto deploy	
76
PowerCLI basics	
77
vCenter Orchestrator basics	
78
Automating resource management	
79
Creating a new VM	
80
Creating a new VM with PowerCLI	
81
Creating a new VM with vCO	
83
Community automation resources	
84
Available monitoring options	
86
Alarms	
87
Configuring an alarm	
87
vCenter Operations Manager	
89
Summary	
91
Index	
93


Preface
VMware vSphere Resource Management Essentials provides readers with a high-level 
understanding of the various components, methodologies, and best practices for 
maintaining and managing resources in a virtual environment.
Readers will begin the book by going through an explanation of the requirements for 
ESXi and the foundation for VMware vSphere. Also, this book will provide readers 
with an understanding of how resources are supplied and the features that enable 
resource and virtual machine availability.
With an understanding of the requirements to build and run your environment, 
you will then move into understanding how ESXi manages resources such as CPU, 
memory, disk, and networks for multiple virtual machines and ensures there is 
resource availability.
With VMs built and resources assigned, readers will get to know the advanced 
features as well as the monitoring and automation tools included to make your 
VMware vSphere environment more efficient.
What this book covers
Chapter 1, Understanding vSphere System Requirements, provides specific resource 
requirements for installing ESXi and vCenter, as well as providing links to online 
resources such as the VMware HCL.
Chapter 2, Assigning Resources to VMs, covers how virtual machines use physical 
resources provided by ESXi hosts, and provides various techniques that ESXi uses  
to manage the allocation of physical resources.
Chapter 3, Advanced Resource Management Features, provides an overview of the 
various tools and features licensed with VMware vSphere to increase resource 
utilization and availability.

Preface
[ 2 ]
Chapter 4, Automation and Monitoring Options, takes a look at the two main 
automation tools available with VMware vSphere, PowerCLI, and vCenter 
Orchestrator; it also covers monitoring solutions built into vCenter and  
vCenter Operations Manager.
What you need for this book
This book can be read without any additional resources; however, we recommend 
that you have access to some physical lab resources to install ESXi and vCenter on,  
and at least a trial of the VMware vCenter Operations Management Suite  
available at www.vmware.com/go/try-vcenter-ops.
A reasonable lab setup can be one on a single physical host with at least a dual  
or quad core processor and 8 GB of RAM (4 GB for vCenter and 2 GB for a  
Windows Server VM to run Active Directory).
Who this book is for
If you are a vSphere administrator who wants to understand new features or an 
administrator aspiring to start your virtualization journey with VMware vSphere 
and want to learn how to manage your resources, this book is for you. 
Conventions
In this book, you will find a number of styles of text that distinguish between 
different kinds of information. Here are some examples of these styles, and an 
explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions, 
pathnames, dummy URLs, user input, and Twitter handles are shown as follows:  
"To see the information, you just need to run the sar command."
Any command-line input or output is written as follows:
Set-VM -VM <name_of_vm> -MemoryMB 8192
New terms and important words are shown in bold. Words that you see on the 
screen, in menus or dialog boxes for example, appear in the text like this: "Once  
the Performance Monitor window opens, expand Monitoring Tools and click  
on Performance Monitor."

Preface
[ 3 ]
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about 
this book—what you liked or may have disliked. Reader feedback is important for  
us to develop titles that you really get the most out of.
To send us general feedback, simply send an e-mail to feedback@packtpub.com,  
and mention the book title via the subject of your message. If there is a topic that  
you have expertise in and you are interested in either writing or contributing to  
a book, see our author guide on www.packtpub.com/authors.
Customer support
Now that you are the proud owner of a Packt book, we have a number of things  
to help you to get the most from your purchase.
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes  
do happen. If you find a mistake in one of our books—maybe a mistake in the text  
or the code—we would be grateful if you would report this to us. By doing so, you can 
save other readers from frustration and help us improve subsequent versions of this 
book. If you find any errata, please report them by visiting http://www.packtpub.
com/submit-errata, selecting your book, clicking on the errata submission form link, 
and entering the details of your errata. Once your errata are verified, your submission 
will be accepted and the errata will be uploaded on our website, or added to any list of 
existing errata, under the Errata section of that title. Any existing errata can be viewed 
by selecting your title from http://www.packtpub.com/support.

Preface
[ 4 ]
Piracy
Piracy of copyright material on the Internet is an ongoing problem across all media. 
At Packt, we take the protection of our copyright and licenses very seriously. If you 
come across any illegal copies of our works, in any form, on the Internet, please 
provide us with the location address or website name immediately so that we can 
pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected  
pirated material.
We appreciate your help in protecting our authors, and our ability to bring you 
valuable content.
Questions
You can contact us at questions@packtpub.com if you are having a problem  
with any aspect of the book, and we will do our best to address it.

Understanding vSphere 
System Requirements
VMware vSphere allows organizations to achieve extremely high levels of resource 
utilization for both new and existing physical servers by allowing administrators to 
easily virtualize and consolidate existing servers. By consolidating multiple servers 
and applications onto a single physical server, organizations can maximize their Total 
Cost of Ownership (TCO) by using more of the resources provided by the physical 
server. However, like any other system, there are certain requirements vSphere needs 
to fulfill in order to function properly. In addition to the system requirements for 
various vSphere components that make up a working environment, it also brings 
back the lost art of true infrastructure design. This ensures that you understand the 
requirements of the systems and applications you will be virtualizing so that they can 
operate according to your organization's Service Level Agreements (SLA).
In this chapter, we will review the requirements for installing various components  
of vSphere, including ESXi and vCenter. Also, we'll learn about the tools available  
to validate your hardware and look at the basics of determining resource utilization 
for your existing servers and applications.
The topics we'll cover in this chapter are as follows:
•	
Understanding ESXi system requirements and general best practices
•	
Understanding vCenter system requirements and options
•	
Understanding the features of vSphere
•	
Reviewing the basic components of a vSphere environment
•	
Understanding configuration maximums
•	
Determining resource utilization requirements

Understanding vSphere System Requirements
[ 6 ]
ESXi system requirements
VMware vSphere Hypervisor, also known as ESXi, is the foundation on which you 
will build your vSphere environment. A hypervisor is simply software that manages 
and assigns physical resources to virtual systems. Hypervisors are classified into two 
types: type 1 and type 2. Type 1 hypervisors are installed onto bare metal hardware 
such as ESXi. Type 2 hypervisors are installed on top of an existing operating system, 
for example, a VMware Workstation, which is installed on Windows or Linux, or 
VMware Fusion, installed on OSX. 
ESXi is installed on your physical server to a hard drive, USB flash drive, or a  
built-in chip directly from manufactures such as Hewlett Packard. Alternatively, 
it can be directly installed into the memory using an advanced feature called auto 
deploy. By using vSphere, applications that may have otherwise not utilized all 
of the resources in a physical server will now be shared among multiple Virtual 
Servers (VMs) and the applications running on them. VMware previously released 
other hypervisors, for example, VMware server that was installed on top of 
Windows systems that allowed you to create virtual servers. At the time of  
writing this, VMware is committed to using ESXi as its type 1 hypervisor.
As you have most likely become accustomed to other operating systems and 
applications, there are specific system requirements based on the version of 
ESXi that you are installing. The VMware Knowledge Base maintains up-to-date 
information not only about the system requirements for the version of ESXi that 
you are running, but also about known problems and steps for troubleshooting 
error messages you may encounter.
VMware KB 2052329 provides a list of the minimum requirements for installing  
ESXi 5.5 available at http://kb.vmware.com/kb/2052329.
If you have an older version of ESXi that you need to continue to support, you can 
find the requirements in VMware KB 1003661 available at http://kb.vmware.com/
kb/1003661.
Let's review some of the resource requirements for ESXi 5.5 as follows:
•	
A 64-bit x86 processor with at least two cores, such as an Intel Xeon E5-2640 
or AMD Opteron 6320, which supports hardware virtualization (Intel VT-x 
or AMD RVI).
•	
A minimum of 4 GB RAM to install ESXi is required. However, this is 
truly a minimum requirement as the consolidation ratios and application 
requirements you are targeting will ultimately drive the amount of RAM  
in your servers. We will discuss consolidation ratios later in this chapter.

Chapter 1
[ 7 ]
•	
At least one gigabit network adapter.
•	
A supported storage controller and a device with minimum of  
1 GB of space to install ESXi and 5.2 GB to create the scratch partition.
Additionally, you will want to confirm that all of your hardware is supported by 
visiting the VMware Compatibility Guide available at http://vmware.com/go/hcl.
Now, if you are thinking, "Oh great, I need to go through all the hardware in my 
server to make sure it works!", be rest assured that VMware has a vast partner 
network. Therefore, validating servers from vendors such as HP, Dell, and Cisco  
can be done with just a few clicks, as seen in the following screenshot:
www.allitebooks.com

Understanding vSphere System Requirements
[ 8 ]
In the preceding screenshot, we searched for Rackmount servers that are compatible 
with ESXi 5.5 from HP. Selecting these three options and clicking on the Update and 
View Results button give us a list of servers that match our search, as shown in the 
following screenshot:
Of course, if you build your own servers, this process may be a little more manual 
since you will have to verify each component.
One last site you should bookmark and get in the habit of visiting is the 
VMware Community site. The Community site is filled with people passionate 
about virtualization, supporting and giving back to others in the virtualization 
community. It's a great place to ask questions and see what others have been 
experiencing. For example, maybe you are trying to decide between two quad-port 
NICs for your server, which are both in the compatibility guide, but the chipset on  
one is more prone to errors than the other. If that were the case, I bet there would be 
a discussion on this! You can find the VMware Community site at  
https://communities.vmware.com/welcome.
Now that we know the requirements and how to find the requirements for your 
specific environment, installing ESXi on a server, starting to build virtual machines, 
or even converting current physical machines to virtual machines is within your 
reach. If you think consolidating multiple physical servers onto a single ESXi host is 
amazing, wait until you see what managing ESXi hosts with vCenter can do.

Chapter 1
[ 9 ]
vCenter components
While ESXi is the foundation on which we build our virtual environment, the 
amazing features that allow greater resource utilization of our physical servers 
comes from joining multiple ESXi hosts together and managing them with VMware 
vCenter. vCenter itself comes in two different versions, standard and foundation. 
The foundation edition is designed for small deployments, supporting only three 
hosts, and is generally packaged into essentials kits. The standard edition is required 
for anything beyond three managed hosts.
This probably won't be the last time I say this, but just as any other application, 
you need to plan your vCenter deployment properly to ensure it can support your 
environment. VMware provides two basic options for deploying vCenter: either 
installed as an application on top of Microsoft Windows or as a prepackaged Linux 
virtual appliance. The virtual appliance is not new to vSphere 5.5, but VMware has 
expanded capabilities of the appliance in vSphere 5.5 to support up to 100 ESXi hosts 
or 3,000 virtual machines using the built-in PostgreSQL database or up to 1000 hosts 
or 10,000 virtual machines with an external Oracle database. You can find more 
information in the vSphere Configuration Maximums guide, which we will review 
shortly. It is available at http://kb.vmware.com/kb/2052334.
As of vSphere 5.1, VMware separated many functions that were once bundled 
together into separate services, allowing you for having greater control (and  
greater complexity) of your vCenter deployment. The main components you  
need to be aware of are as follows:
•	
vCenter
•	
vCenter Single Sign-On
•	
vCenter Inventory Service
•	
vSphere Web Client
Covering each of these components is beyond the scope of this book. For a more 
in-depth look at each component, check out Implementing VMware vCenter Server, 
Konstantin Kuminsky from Packt Publishing or Mastering VMware vSphere 5.5 from 
Sybex, Nick Marshall and Scott Lowe and several other authors who are prominent 
members of the VMware community.

Understanding vSphere System Requirements
[ 10 ]
vCenter also requires a database server to store configuration information, collect 
statistics about the environment, and perform various tasks to maintain a healthy 
database. The database platform you require (yes, I'm going to say it again) needs to 
be planned properly in order to support your requirements. vCenter can be installed 
on the same server as your database, but I would treat defining system requirements 
as if they were going to be on separate servers.
VMware considers a small deployment for vCenter up to 100 hosts or 1000 VMs;  
for many organizations, this is probably adequate but can support much more.  
The resources required for vCenter to run efficiently are based on your host or  
VM inventory.
A Simple Install, which is a streamlined installation of vCenter, vCenter SSO, 
vCenter Inventory Service, and the web client, has the following requirements:
•	
2x 64-bit processor cores (such as ESXi; these can be either Intel or AMD) 
with a speed of 2 GHz
•	
12 GB of memory
•	
40 to 60 GB of disk space
•	
64-bit database server
Again, it's important to emphasize that this is in addition to your database 
server requirements. While the database server requirements, installation, and 
configuration are beyond the scope of this book, let's take a quick look at the  
system requirements for a database server.
Microsoft recommends a 2 GHz or a faster processor of 4 GB of RAM along with 
3.6 GB of free disk space for various SQL Server components. When added to the 
vCenter server requirements, you have what may be one of the larger servers in your 
environment. You should consult your DBA team, or if it is a one-man show, refer to 
Google and the SQL documentation online at http://technet.microsoft.com/en-
us/library/ms143506(v=sql.105).aspx.
Understanding vSphere features
So we know that vCenter is available in two different versions, but what is really 
important is the features that vCenter unlocks. VMware is constantly innovating and 
adding new features, so it's best to visit the VMware website for the latest version 
and features available.

Chapter 1
[ 11 ]
There are currently three editions of vSphere: Standard, Enterprise, and Enterprise 
Plus. Understanding what features are available in each edition is the most important 
step in building your virtualization plans so that you can purchase the edition that 
meets your requirement. Let's look at the three versions and what is available in each 
one of them. When you get to in Chapter 3, Advanced Resource Management Features, 
you'll notice that these line up pretty closely, so if you are unfamiliar with a feature 
mentioned in the following table, we will cover it shortly.
Feature
Standard
Enterprise
Enterprise Plus
vMotion
Yes
Yes
Yes
Storage vMotion
Yes
Yes
Yes
High Availability (HA) Yes
Yes
Yes
Fault Tolerance (FT)
Yes
Yes
Yes
Hot Add
Yes
Yes
Yes
App HA
Yes
Yes
Distributed Resource 
Scheduler (DRS)
Yes
Yes
Storage DRS
Yes
Flash Read Cache
Yes
Virtual Distributed 
Switch (vDS)
Yes
Network I/O control
Yes
Storage I/O control
Yes
Host Profiles and auto 
deploy
Yes
VSAN
Licensed separately
I added VSAN to this list and the book even though it is scheduled to be a separate 
product (VSAN is currently still in beta at the time of writing this); it is one of the 
most exciting pieces of technologies since virtualization.
As a VMware administrator, many of us would select Enterprise Plus for our 
licenses, but as always, cost is going to be a factor. As we review each of the features 
and their ability to help manage resources, you will be able to make a more informed 
decision on which version is right for your organization.

Understanding vSphere System Requirements
[ 12 ]
Topology basics
Now that you are familiar with the system requirements to get ESXi and vCenter 
installed, and have a list of the features available in different versions, it's time to 
look at how a typical vSphere deployment is organized. Different features of vSphere 
are configured and made available at various levels, so understanding each feature is 
critical. A basic vCenter environment will contain a data center, and within the data 
center, you create clusters. Hosts are placed into a cluster and then VMs are created 
on a host.
DATA
CENTER
CLUSTER
HOST VM
Understanding vSphere data center
The topology of a vSphere environment is very similar to what you might see from a 
physical perspective. After installing ESXi and vCenter, one of the first items you will 
be asked to create is a data center. A data center is, in its simplest form, the container 
(or a storage area if we are relating it to a physical environment) where your 
resources are stored. Since we live in a virtual world, your vCenter data center is not 
tied to one physical location. In fact, if you really wanted, you could create several 
vCenter data centers even if all the servers were in the same physical location, but 
there is not much need for that. It's common to mirror your vCenter data centers to 
physical locations of your ESXi hosts. For example, if you have a physical data center 
in Boston and Atlanta, you might create two data center objects in vCenter, one for 
Boston and one for Atlanta, which contain the physical resources located at each 
data center. Once your data center is created, you could move on to add hosts and 
leveraging features such as vMotion; however, you would be missing out on one of 
the critical areas as it relates to managing resources—vCenter clusters.
Familiarizing yourself with a vSphere cluster
Clusters are created within data centers and again, because this is all virtual, you 
could create as many clusters as you like. Clusters are a collection of ESXi hosts that 
can share resources amongst each other. In fact, two of the most important resource 
and availability features vSphere offers are enabled at the cluster level: High 
Availability (HA) and Distributed Resource Scheduler (DRS). There are several 
other settings configured at the cluster level that we will cover in Chapter 3, Advanced 
Resource Management Features.

Chapter 1
[ 13 ]
What is a vSphere host?
As mentioned in the previous section, within a cluster, we add ESXi hosts. There 
is quite a bit of configuration done at a host level, but without a host, we cannot 
virtualize any servers or desktops. As a quick aside, much of what we talk about in 
this book relates to server virtualization, but both ESXi and vCenter also provide 
the necessary infrastructure for virtual desktops. Yes, you can virtualize your 
desktops as well! If you are interested in learning more about Virtual Desktop 
Infrastructure (VDI), check out Implementing VMware Horizon View 5.2, Jason 
Ventresco, Packt Publishing.
Now that we have our hosts in a cluster and our cluster in a data center, we can 
start creating VMs or even use a tool such as VMware Converter to take an existing 
physical server and convert it into a virtual server. Here is what a basic vSphere 
environment might look like:
Using the preceding screenshot as a reference, we have a vCenter Server called  
vc-l-01a and a data center called Datacenter Site A. Within this data center, we  
have a cluster called Cluster Site A, and within the cluster, we have three ESXi  
hosts; on the hosts, there are five VMs.

Understanding vSphere System Requirements
[ 14 ]
Remembering configuration maximums
Up until now, we have focused on the minimum requirements to run ESXi and 
vCenter; however, it is important to remember that there are actually maximum 
supported configuration settings as well. For example, while we have seen that 
vCenter, with an appropriate amount of resources, can manage up to 400 hosts or 
4,000 VMs, a cluster can only contain 32 ESXi hosts. In this scenario, if you had 400 
hosts, you would have to create 13 clusters within your data center (or data centers) 
to support all of those hosts. Depending on your organization, you may also have 
deployed multiple vCenter servers to manage all of those hosts.
All of the various components that make up a vSphere environment have 
various configuration maximum settings. Depending on your physical to virtual 
consolidation ratio, you could find yourself in a scenario where you were within  
the limits of one configuration maximum setting but are constrained by another.  
For example, you can configure up to 10 virtual NICs per virtual machine but can 
only have 1016 active ports per host. If you were able to run 102 VMs on a single 
physical host and each VM had 10 active virtual NICs, then you would need 1020 
active ports, which is above the configuration maximum of 1016 per host. This 
scenario might be a bit extreme, but it's used to point out that just because you 
comply with the maximum configuration settings for one resource, it does not  
mean it would work with another.
Let's take a look at some of the more common configuration maximum settings.
Virtual machine maximums (per VM)
Every component within the hierarchy of a vSphere environment has certain 
maximum settings. Here are some common VM configuration maximums:
•	
64 virtual CPUs
•	
1 TB of RAM
•	
62 TB of virtual disk size
•	
10 virtual NICs
•	
20 USB devices
ESXi host maximums (per host)
Like the VM configuration maximum settings mentioned in the previous section, 
ESXi hosts also have several maximum settings you need to be aware of when you 
need to design and configure your host, mentioned as follows:

Chapter 1
[ 15 ]
•	
320 logical CPUs
•	
512 VMs
•	
4096 virtual CPUs (total number of assigned virtual CPUs across all VMs)
•	
32 virtual CPUs per physical core
•	
4 TB of RAM
•	
1 VSWP swap file per virtual machine
•	
256 iSCSI, FC LUNs, or NFS mounts
•	
64 TB of datastore volume size
•	
256 volumes per host
•	
24 e1000e 1 GB ethernet ports (Intel PCI-e)
•	
8 bnx2x 10 GB ethernet ports (Broadcom)
•	
Eight 10 GB and four 1 GB ports (combined)
Cluster maximums
Cluster maximums are one of the more likely areas you will reach when you use 
configuration maximum settings during your design and implementation. Some  
of them are mentioned as follows:
•	
32 hosts
•	
4000 virtual machines
•	
512 virtual machines per host
•	
1600 resource pools per host
Like anything else, configuration maximums tend to change from version to  
version, so make sure you check what the configuration maximums are for your 
version of vSphere. Configuration maximums for vSphere 5.5 can be found at 
http://www.vmware.com/pdf/vsphere5/r55/vsphere-55-configuration-
maximums.pdf.
Determining resource utilization 
requirements
Until now, we have focused on the minimum resource requirements to run ESXi and 
vCenter, but how do you determine the needs specific to your environment? Before 
virtualization, in many cases, the physical servers you were buying were so much 
more powerful than what many needed. This is why sizing became somewhat of a  
lost art.

Understanding vSphere System Requirements
[ 16 ]
For those hoping to find a magical catch-all formula that will work in every scenario, 
you'll have to keep looking. Remember every environment is unique, and even where 
similarities may arise, the use case your organization has, will most likely be different 
from another organization. Beyond your specific VM resource requirements, the 
hosts you are installing ESXi on will also vary; the hardware available to you will 
affect your consolidation ratio (the number of virtual machines you can fit on a single 
host). For example, if you have 10 servers that you want to virtualize, and you have 
determined each requires 4 GB of RAM, you might easily virtualize those 10 servers 
on a host with 48 GB of memory. However, if your host only has 16 GB of memory, 
you may need two or three hosts in order to achieve the required performance.
Another important aspect to consider is when to collect resource utilization statistics 
about your servers. Think about the requirements you have for a specific server; 
let's use your finance department as an example. You can certainly collect resource 
statistics over a period of time in the middle of the month, and that might work just 
fine; however, the people in your finance department are more likely to utilize the 
system heavily during the first few days of the month as they are working on their 
month end processes. If you collect resource statistics on the 15th, you might miss a 
huge increase in resource utilization requirements, which could lead to the system 
not working as expected, making unhappy users.
One last thing before we jump into some example statistics; you should consider 
collecting these statistics over at least two periods for each server:
•	
First, during the normal business hours of your organization or the specific 
department, during a time when systems are likely to be heavily utilized
•	
The second round should include an entire day or week so you are aware  
of the impact of after hours tasks such as backups and anti-virus scans on 
your environment
It's important to have a strong understanding of the use cases for all the systems you 
will be virtualizing. If you are running your test during the middle of the month, 
you might miss the increase of traffic for systems utilized heavily only at the end of 
the month, for example, accounting systems. The more information you collect, the 
better prepared you will be to determine your resource utilization requirements.

Chapter 1
[ 17 ]
There are quite a few commercial tools available to help determine the specific 
resource requirements for your environment. In fact, if you have an active project 
and/or budget, check with your server and storage vendor as they can most likely 
provide tools to assess your environment over a period of time to help you collect 
this information. If you work with a VMware Partner or the VMware Professional 
Services Organization (PSO), you could also work with them to run a tool called 
VMware Capacity Planner. This tool is only available to partners who have passed 
the corresponding partner exams.
For purposes of this section, however, we will look at the statistics we can capture 
natively within an operating system, for example, using Performance Monitor on 
Windows and the sar command in Linux.
If you are an OS X user, you might be wondering why we are not touching OS 
X. This is because while Apple allows virtualizing OS X 10.5 and later, it is only 
supported on the Apple hardware and is not likely an everyday use case. If your 
organization requires virtualizing OSX, ESXi 5.1 is supported on specific Mac Pro 
desktops with Intel Xeon 5600 series processors and 5.0 is supported on Xserve using 
Xeon 5500 series processors. The current Apple license agreement allows virtualizing 
OSX 10.5 and up; of course, you should check for the latest agreement to ensure you 
are adhering to the license agreement.
Monitoring common resource statistics
From a statistics perspective, there are four main types of resources you generally 
monitor: CPU, memory, disk, and network. Unless you have a very chatty 
application, network utilization is generally very low, but this doesn't mean we 
won't check on it; however, we probably won't dedicate as much time to it as we  
do for CPU, memory, and disk.
As we think about the CPU and memory, we generally look at utilization in terms of 
percentages. When we look at example servers, you will see that having an accurate 
inventory of the physical server is important so we can properly gauge the virtual 
CPU and memory requirements when we virtualize. If a physical server has dual 
quad core CPUs and 16 GB of memory, it does not necessarily mean we want to 
provide the same amount of virtual resources (you will learn why in the next chapter).
www.allitebooks.com

Understanding vSphere System Requirements
[ 18 ]
Disk performance is where many people spend the least amount of time, and 
those people generally have the most headaches after they have virtualized. Disk 
performance is probably the most critical aspect to think about when you are 
planning your virtualization project. Most people only think of storage in terms of 
storage capacity, generally gigabytes (GB) or terabytes (TB). However, from a server 
perspective, we are mostly concerned with the amount of input and output per 
second, otherwise known as IOPS and throughput. We break down IOPS in into 
reads and writes per second and then their ratio by comparing one with the other. 
Understanding your I/O patterns will help you design your storage architecture to 
properly support all your applications. Storage design and understanding is an art 
and science by itself. 
The vSphere High Performance Cookbook, Prasenjit Sarkar, Packt Publishing has an entire 
chapter dedicated to storage; Troubleshooting vSphere Storage, Mike Preston, Packt 
Publishing provides you with the key steps to help spot problems with storage from 
a vSphere perspective. Finally, one of the most in-depth books on storage is Storage 
Implementation in vSphere 5.0 from VMware Press, Mostafa Khalil (one of the original 
VMware Certified Design Experts in VCDX).
Sample workload
Let's break this down into a practical example so we can see how we are applying 
these concepts. In this example, we will look at two different types of servers that 
are likely to have various resource requirements: Windows Active Directory Domain 
Controller and a CentOS Apache web server. In this scenario, let's assume that 
each of these server operating systems and applications are running on dedicated 
hardware, that is, they are not yet virtual machines.
The first step you should take, if you do not have this already, is to document 
the physical systems, their components, and other relevant information such 
as computer or DNS name, IP address (es), location, and so on. For larger 
environments, you may also want to document installed software, user groups  
or departments, and so on.
Collecting statistics on Windows
On Windows servers, your first step would be to start performance monitoring. 
Perform the following steps to do so:
1.	 Navigate to Start | Run and enter perfmon.

Chapter 1
[ 19 ]
2.	 Once the Performance Monitor window opens, expand Monitoring  
Tools and click on Performance Monitor. Here, you could start adding 
various counters; however, as of Windows 2008/Windows 7, Performance 
Monitor includes Data Collector Sets.
3.	 Expand the Data Collector Sets folder and then the System folder; 
right-click on System Performance and select Start. Performance  
Monitor will start to collect key statistics about your system and its 
resource utilization.
4.	 When you are satisfied that you have collected an appropriate amount of  
data, click on System Performance and select Stop. Your reports will be  
saved into the Reports folder; navigate to Reports | System, click on the 
System Performance folder, and finally double-click on the report to see  
the report.
In the following screenshot for our domain controller, you can see we were using 
10 percent of the total CPU resources available, 54 percent of the memory, a low 18 
IOPS, and 0 percent of the available network resources (this is not really uncommon; 
I have had busy application servers that barely break 2 percent).
Now let's compare what we are utilizing with the actual physical resources available 
to the server. This server has two dual core processors (four total cores) running at 
2 GHz per core (8 GHz total available), 4 GB of memory, two 200 GB SAS drives 
configured in a RAID1, and a 1 Gbps network card.
Here, performance monitor shows averages, but you should also investigate peak 
usage. If you scroll down in the report, you will find a menu labeled CPU. Navigate 
to CPU | Process. Here you will see quite a bit of data, more than the space we have 
to review in this book; however, if you scroll down, you will see a section called 
Processor User Time by CPU. Here, your mean (that is, average) column should 
match fairly closely to the report overview provided for the total, but we also want  
to look at any spikes we may have encountered. As you can see, this CPU had 
one core that received a maximum of 35 percent utilization, slightly more than the 
average suggested.

Understanding vSphere System Requirements
[ 20 ]
If we take the average CPU utilization at 10 percent of the total CPU, it means we 
will theoretically require only 800 MHz of CPU power, something a single physical 
core could easily support. The, memory is also using only half of what is available, 
so we can most likely reduce the amount of memory to 3 GB and still have room for 
various changes in operating conditions we might have not encountered during our 
collection window. Finally, having only 18 IOPS used means that we have plenty of 
performance left in the drives; even a SATA 7200 RPM drive can provide around  
80 IOPS.
Collecting statistics on Linux
Now let's look at the Linux web server to see how we can collect this same set of 
information using sar in an additional package with sysstat that can monitor 
resource utilization over time. This is similar to what you might get from top or iotop. 
The sysstat package can easily be added to your system by running yum install 
sysstat, as it is a part of the base repository (yum install sysstat is command format).
Once the sysstat package is installed, it will start collecting information about 
resource utilization every 10 minutes and keep this information for a period of 
seven days. To see the information, you just need to run the sar command; there are 
different options to display different sets of information, which we will look at next.
Here, we can see that our system is idle right now by viewing the %idle column.
A simple way to generate some load on your system is to run dd if=/dev/zero 
of=/dev/null, which will spike your CPU load to 100 percent, so, don't do  
this on production systems! Let's look at the output with some CPU load. In this 
example, you can see that the CPU was under load for about half of the 10-minute 
collection window.

Chapter 1
[ 21 ]
One problem here is that unless the CPU spike, in this case to 100 percent, was not 
consistent for at least 10 minutes, we would potentially miss these spikes using sar 
with a 10-minute window. This is easily changed by editing /etc/cron.d/sysstat, 
which tells the system to run this every 10 minutes. During a collection window, one 
or two minutes may provide more valuable detail. In this example, you can see I am 
now logging at a five-minute interval instead of 10, so I will have a better chance to 
find maximum CPU usage during my monitoring period.
Now, we are not only concerned with the CPU, but we also want to see memory  
and disk utilization. To access those statistics, run sar with the following options:
•	
The sar –r command will show RAM (memory) statistics. At a basic level, 
the items we are concerned with here would be the percentage of memory 
used, which we could use to determine how much memory is actually  
being utilized.
•	
The sar –b command will show disk I/O. From a disk perspective, 
sar –b will tell us the total number of transactions per second (tps), read 
transactions per second (rtps), and write transactions per second (wtps).
As you can see, you are able to natively collect quite a bit of relevant data about 
resource utilization on our systems. However, without the help of a vendor or 
VMware PSO who has access to VMware Capacity Planner, another commercial 
tool, or a good automation system, this can become difficult to do on a large scale 
(hundreds or thousands of servers), but certainly not impossible.
Summary
In this chapter, we looked at the various resource requirements to build a working 
vSphere environment, including the resource requirements for both ESXi and 
vCenter. We also reviewed the VMware Compatibility Guide to validate the hardware 
where installing ESXi would be supported.

Understanding vSphere System Requirements
[ 22 ]
We then wrapped up by looking at the various resources we want to support when 
we create virtual machines to ensure the performance is acceptable and within 
your organization's SLAs. What we saw in the two examples is not different from 
what many organizations face; they have invested in resources by purchasing these 
servers. However, it will not receive the full Return on Investment (ROI) because 
the systems are not being more heavily leveraged.
In the next chapter, we will see how VMware vSphere can utilize server resources  
to provide your organization with a great ROI efficiently while most likely decreasing 
its TCO because of its ability to consolidate several physical machines onto a single 
host. This provides greater availability that you could achieve without the use of 
VMware vSphere.

Assigning Resources to VMs
Now that you understand the various resource requirements for installing ESXi and 
vCenter, we will look at how VMware vSphere manages resources assigned to an 
individual VM and how multiple VMs on a single physical host can affect resource 
availability. Throughout this chapter, we will compare and contrast common 
practices used while deploying operating systems to bare metal systems and then in 
deploying operating systems as VMs.
The topics we'll be covering in this chapter are as follows:
•	
The basics of overcommitment and virtualization
•	
CPU scheduling and the effect of multiple vCPU VMs
•	
Memory assignment and management
•	
Storage considerations and their effects on performance
•	
Networking, Virtual Switches, VM to VM, and VM to physical  
system communication
The basics of overcommitment and 
virtualization
As hardware evolved, processors and memory far outpaced the rate that applications 
could consume those resources. When multicore processors came out, many 
applications were still only able to utilize a single core, that left an entire CPU core 
idle unless some other application or process was multiprocessor capable. While 
an underutilized resource provides headroom for the applications, it reduces value 
to the business because the resource does not receive full returns on its hardware 
investment. When a system or application was idle, the investment made by the 
organization was further wasted.

Assigning Resources to VMs
[ 24 ]
Virtualization helps to solve this problem by being able to assign physical resources 
to virtual servers. VMware made this technology available to all organizations. 
It simplified deployment and built features that could be leveraged by a small 
organization with only a handful of servers, or by the world's largest organizations 
with thousands or even tens of thousands of servers.
Using the two example servers from Chapter 1, Understanding vSphere System 
Requirements, let's consider the benefits of virtualizing your servers compared to 
installing them on dedicated hardware.
We looked at two physical servers, each with a dual core CPU, so two processing 
cores. The Windows server, even at its peak utilization, never used more than 35 
percent of the available processing power of the server, leaving 65 percent of the 
resources idle. Our Linux server (also with a dual core CPU) was almost entirely  
idle, and even when we added a workload, we still only averaged out 45 percent 
CPU utilization. This means your organization purchased two physical servers, 
while the application only required the resources available in one physical server.
Let's take this example even a step further. While the Windows server required a 
peak of 35 percent of the CPU, it did not require this for the entire duration in which 
we collected the resource utilization statistics. If we assume for the sake of this 
example that the spike on the Windows server and on the Linux server happened at 
separate times, we would have even more resources available if we had other VMs 
running on the same physical hardware.
The same concept of sharing physical resources carry through the four major 
resources that we discussed in Chapter 1, Understanding vSphere System Requirements, 
that is, CPU, memory, disk, and networking. It's unlikely that a single OS and 
application are utilizing all of the physical resources available in a modern server. By 
virtualizing several OS and applications, we can maximize the resource utilization 
available in a physical server, providing a higher ROI and in many cases, a lower 
TCO for the organization.
CPU scheduling and the effect of multiple 
vCPU VMs
Modern CPUs are so powerful that we perceive the ability to multitask or use 
several programs at once. In reality, CPU cores act in a very serial fashion, that is 
to say, a single core is only ever really executing one task at a time. Because of the 
speed and logic in these modern processors, they are able to quickly switch between 
the requests from different applications, so when one request is finished, the CPU 
switches to the next task and so on.

Chapter 2
[ 25 ]
A technology such as Hyper-Threading (HT) by Intel or AMD-V/RVI will make 
a single core appear as two cores to the operating system and can help process 
multiple requests at once on a single CPU core.
If you have HT enabled, you will see that your OS, if supported with a hypervisor 
such as ESXi, perceives this to be an available physical processor core; however, for 
purposes of designing your infrastructure, I would recommend staying within the 
physical capabilities of your processor. With an Intel Xeon CPU running HT on four 
cores, the ESXi hypervisor will be presented with eight available cores. Although HT 
increases number of the available virtual cores, you should design your clusters for 
the load based on the number of physical cores.
With this basic understanding of how CPUs function, you can see why it's important 
to understand the resource requirements for a specific server or application. Servers 
or applications with relatively low CPU resource requirements could potentially 
work fine with 10 or 12 vCPUs per physical core; however, if the application was 
processor intensive, you might find that you can only run two to three vCPUs per 
physical core. It's important to note here that generally, you are not assigning a VM 
or vCPU to a physical CPU; however, you could through the use of CPU affinity in 
ESXi. We are simply saying that depending on your workload and the number of 
physical cores in your CPU, you could expect to receive certain consolidation ratios.
When possible, you should try to perform the following actions:
•	
Monitor resource utilization prior to any project—physical to virtual or 
virtual to virtual
•	
Do not overcommit the CPU; only assign what the VM and workload require
•	
Avoid CPU affinity as you will not be able to vMotion VMs with CPU affinity 
enabled until it is disabled
VMware has developed an advanced CPU scheduler to ensure the efficient operation 
of VMs in your environment. In fact, they have published an excellent technical 
white paper that you should read, as we don't have the space in this book to cover 
the CPU scheduler in that level of detail. You can find the white paper at http://
www.vmware.com/files/pdf/techpaper/VMware-vSphere-CPU-Sched-Perf.pdf.
If your VM has one vCPU, and the host it is running on has four pCPU, then 
it's the job of the CPU scheduler to find an available pCPU for the VM to access 
when required. That is an easy translation to make to a physical server with an OS 
installed. However, what happens when your VM has multiple vCPUs?

Assigning Resources to VMs
[ 26 ]
Using vCenter as an example, as we discussed in Chapter 1, Understanding vSphere 
System Requirements, vCenter recommends at least two vCPUs. However, at any 
given point, it may not require the processing power of both vCPUs. If vCenter was 
installed on its own physical server, the OS would expect to have access to all of the 
CPUs or cores on that server and would just operate as required. However, in a VM 
where you are sharing a pCPU between multiple VMs, the pCPU might already be 
busy processing a request from another VM. Since the OS expects all of the CPUs 
assigned to it to be available, it must have to wait for two pCPUs to be available since 
it is assigned two vCPUs. Let's look at another example visually.
In the following figure, we have an ESXi host with two quad core pCPU for a total of 
eight pCPU cores available to VMs with five VMs, each with two vCPUs assigned:
Multiple VMs each with multiple vCPUs
The CPU scheduler manages access to
the physical CPU/Cores in the host
V
V
VM
V
V
VM
V
V
VM
V
V
VM
V
V
VM
4-Core CPU
4-Core CPU
When all eight cores are available and the first VM makes a request, the CPU 
scheduler can provide the vCPUs with access to two pCPU cores, so the VM's 
operating system can operate as expected. The CPU scheduler can support 
simultaneous requests for only three more VMs running on this host. When the  
fifth VM makes a request, the CPU scheduler does not have access to any pCPU 
cores. Because the VM is unaware that it is virtual, it assumes it has access to the 
number of processors assigned.
In this scenario, which is shown in the following figure, the VM will have to wait 
until all of the assigned vCPUs have access to pCPUs to service the request. Even if 
only one vCPU is required, the OS has to wait until all vCPUs can be serviced.

Chapter 2
[ 27 ]
Multiple VMs each with multiple vCPUs
The CPU scheduler manages access to
the physical CPU/Cores in the host
V
V
VM
V
V
VM
V
V
VM
V
V
VM
V
V
VM
4-Core CPU
4-Core CPU
This is where the concept of rightsizing comes into play. Prior to virtualization, the 
OS had access to all pCPUs in a server, whether or not it needed this access. So, a 
web server with dual quad core processors could generally be idle a majority of the 
time depending, of course, on its workload. If we used the same configuration in a 
virtual machine, that is, assigning eight vCPUs, the VM would have to wait until 
the CPU scheduler could provide access to eight pCPUs at once, whether or not it 
needed all eight pCPUs.
Going back again to the example at the end of the previous chapter, where we had 
a physical server with two cores utilizing only 35 percent of the CPU, you would be 
better off assigning only one vCPU to the VM for the same workload. This would 
help since the scheduler would only ever need to access one pCPU that could 
provide all the necessary computing power that the workload required.
To build off yet another topic that we covered in Chapter 1, Understanding vSphere 
System Requirements, it's important to understand the resource requirements for 
environment and specific applications. Once virtual, you want to make sure you 
provide the required resources to a VM and do not carry over the practice from  
the physical world of providing extra resources that are not required. There are  
quite a few products in the market today to help you identify VMs that are  
over-provisioned, some are even free of charge.
www.allitebooks.com

Assigning Resources to VMs
[ 28 ]
One item that you should be aware of once virtualized is a command called ESXTOP. 
ESXTOP is a command-line utility to monitor resource utilization for your ESXi host 
and the VMs on that host. If you are having performance problems with a VM and 
you want to see whether it's due to the pCPU resources that have not been made 
available to the VM, you can check %RDY.
%RDY displays the time span that your VM is ready to run for, but the scheduler 
was not able to provide pCPU resources. This should be under five percent in normal 
operation. A number higher than five percent could mean you have overcommitted 
your pCPU or assigned too many vCPUs to a VM.
If your VM has multiple vCPUs assigned, you should also monitor %CSTP, which 
is only relevant for multiple vCPU VMs. A VM with a %CSTP of 3 percent or higher 
suggests you should lower the number of vCPUs assigned to the VM. In the case of a 
VM that has four vCPUs assigned but operates at 30-40 percent CPU utilization, you 
might actually improve performance by removing two vCPUs and leaving the VM 
with two vCPUs. Here, the VM operates at 40 percent capacity,  
so removing 50 percent of the CPUs should leave enough compute power for the VM 
to operate.
Other useful statistics include (host or VM-based statistics) the following:
•	
CPU load average: This gives the average utilization of all pCPUs in the host. 
A reading of 1.0 in the host means you are fully utilized, 0.5 means 50 percent 
utilized, 0.02 means two percent utilized, and so on.
•	
PCPU USED (%): This is the percent of all CPUs used by the host.
•	
%RUN: This is the amount of vCPUs utilized by the VM. This setting takes 
into account all vCPUs that are assigned. So, a value of 100 percent with one 
vCPU means that the vCPU is fully utilized. A value of 150 percent for a 
VM with two vCPUs means that it is roughly 75 percent utilized if you are 
looking within the guest OS (VM).
•	
%WAIT: This refers to how long the VM is waiting for other processes 
managed by ESXi to complete, such as I/O (VM).
To dive deeper into ESXTOP how-tos, check out VMware KB 2001003 at  
http://kb.vmware.com/kb/2001003 and the VMware communities post  
at https://communities.vmware.com/docs/DOC-9279.

Chapter 2
[ 29 ]
Memory assignment and management
While physical memory is allocated to VMs through the hypervisor just like CPU, 
the calculations and methods to determine memory requirements are much simpler. 
Memory like vCPUs, are assigned to a VM, and the OS on the VM assumes it has 
that amount of memory available to it; however, in many scenarios, you are likely 
to overcommit the memory to achieve a greater consolidation ratio or increase the 
VM density per host. The benefits of overcommitment for memory are again along 
the same lines as overcommitting CPUs; your VMs are not likely to use the entire 
amount of memory assigned to them at all times. As with all resources in a virtual 
environment, you want to rightsize your memory configuration as well. While you 
could easily give all VMs 16 GB of memory, the amount of memory assigned to a 
VM has other impacts on your infrastructure and the vSphere cluster, which we 
will see shortly. VMware has several memory management techniques to reduce 
the likelihood of contention or in the event that you run into a period of contention 
techniques to reclaim memory from VMs.
A current trend, thanks to decreasing memory costs and the manufacturer's ability 
to increase the amount of memory available within a system, is actually to not 
overcommit on memory. Take for example, a modern yet mid-range physical 
server such as a Dell R520 or Cisco UCS B22M blade that can contain up to 384 GB 
of memory. If you were to configure the VMs on these servers each with 4 GB of 
memory, you would have enough memory capacity for around 92 to 94 VMs, leaving 
enough memory for ESXi itself. Depending on the CPU you selected, anywhere from 
four to eight cores per processor, you would potentially be pushing a 12:1 vCPU to 
pCPU overcommitment ratio if each VM had a single vCPU. In this scenario, you 
might actually find CPU cores to be your limiting resource instead of memory. In this 
example, however, we have not factored in the memory required to run ESXi or the 
memory overhead for the VM. What is memory overhead you ask? Well, we were 
just getting to that.
Memory overhead
Memory overhead is additional memory that each VM requires in order for the ESXi 
to manage it. It is generally a very small amount of memory based on the amount 
of memory assigned to the VM and the number of vCPUs. Using the vCenter 
requirements from Chapter 1, Understanding vSphere System Requirements, a VM with 
two vCPUs and 16 GB of memory would require about 144 MB of memory overhead. 
Another example, a VM with eight vCPUs and 16 GB of memory should only require 
around 169 MB of memory overhead.

Assigning Resources to VMs
[ 30 ]
You can see the memory overhead for any particular VM on the summary page  
by expanding Memory, as seen in the following screenshot from the vSphere 5.5  
web client:
You can also find a sample memory overhead charge in the VMware Resource 
Management guide at http://pubs.vmware.com/vsphere-55/topic/com.vmware.
ICbase/PDF/vsphere-esxi-vcenter-server-55-resource-management-guide.
pdf. The following chart from VMware is meant to serve as a guide to help you 
determine memory overhead based on the resources assigned to the VM. It is useful 
while designing your infrastructure, but there are other advanced settings that also 
factor into memory reservations, such as the VMX swap file.
Memory (MB)
1 vCPU
2 vCPUs
4 vCPUs
8 vCPUs
256
20.29
24.28
32.23
48.16
1024
25.90
29.91
37.86
53.82
4096
48.64
52.72
60.67
76.78
16384
139.62
143.98
151.93
168.60
Transparent page sharing and memory 
compression
Two memory management techniques that ESXi implements to improve memory 
utilization is transparent page sharing (TPS) and memory compression. As you 
might expect from its name, ESXi compresses memory pages during times of 
contention to prevent swapping. This is done because the process to compress and 
thus decompress the memory when required is still faster than swapping to a disk. 
Memory compression is enabled by default, and unless you have been instructed to 
do so by support, it's a setting best left enabled.

Chapter 2
[ 31 ]
Transparent page sharing is where identical memory pages are shared among 
multiple virtual machines. There is some debate as to the usefulness of TPS as more 
guess, operating systems are leveraging large memory pages, typically 2 MB in size 
compared to 4 KB.
With 4 KB page sizes, it is theoretically easier to find a match among multiple 
memory pages, which ESXi can then basically deduplicate. If ESXi were to find 1000 
4 KB pages, it could reduce that to a single memory page, thus making more memory 
available to VMs and other processes. With large pages, ESXi still scans the page but 
rather than trying to match an entire 2 MB page size, which is less likely, it scans 4 
KB pages within the 2 MB page. If ESXi detects contention, it can then attempt to 
share, compress, or swap these smaller pages. For the purposes of this book, you 
should know that TPS attempts to deduplicate and share identical memory pages. 
For a deeper dive into TPS, VMware has published a white paper devoted entirely 
to TPS, which can be found at http://download3.vmware.com/software/vmw-
tools/papers/WP-2013-01E-FINAL.pdf. You should also read VMware vSphere 5.1 
Clustering Deepdive, Duncan Epping and Frank Denneman. This is hands down one of 
the best available technical books on VMware vSphere. I'll be suggesting this book 
again in the next chapter when we discuss HA and DRS.
Unless required by your application, leaving large memory pages 
disabled allows TPS to more efficiently deduplicate memory.
Ballooning
There is a driver installed on your guest OS when VMware Tools is installed. This 
allows ESXi to request memory from VMs that might otherwise be idle. ESXi asks 
the balloon driver, a process that is running in the guest OS, to request memory from 
the OS as any other application might. This allows the guest OS, which is aware of its 
current workload, to allocate memory without negatively impacting the performance 
of active processes.

Assigning Resources to VMs
[ 32 ]
Take, for example, a Windows server running IIS as shown in the following figure; 
an increase in demand may cause IIS and its related processes to require a specific 
amount of memory.
The guest OS is managing
current processes as it
normally would
IIS
Balloon
Driver
Windows Server 2012
The memory that IIS previously requested may not be returned to the OS 
immediately even though it is no longer required. If ESXi is experiencing memory 
contention, ESXi makes a request to the balloon driver to ask the guest OS for 
additional memory. The guest OS can then determine what, if any, memory can be 
reclaimed from other processes. The OS can reclaim the memory not needed from 
other applications and provide it to the balloon driver, which then notifies ESXi of 
how much memory can be reclaimed from the guest. By default, ESXi is configured 
to reclaim up to 65 percent of the assigned memory, and this can be alerted with 
advanced settings at both the ESXi hosts, thus affecting all VMs or individual VMs.
Make sure VMware Tools is installed on all VMs to allow ballooning to function.
IIS
Balloon Driver
Windows Server 2012
ESXi, via VMware Tools, has
the balloon driver request
memory for the guest OS

Chapter 2
[ 33 ]
The VSWP swap file
The VSWP swap file is a file equal to the amount of memory assigned to the VM 
minus the amount of memory reserved (we will discuss reservations in Chapter 3, 
Advanced Resource Management Features, but it's very literal). The swap file is one 
of the methods used to reclaim memory during times of contention. When this 
happens, ESXi swaps the running memory to disk, much like what you would see 
with the Windows swap file or Linux swap partition. It should be noted that OS 
swapping is separate from the VSWP swap file, and your OS will still utilize its 
native swapping techniques if the guest OS is low on memory. ESXi will utilize the 
VM swap file if it is low on memory, even if the VM itself is not low on memory.
The VSWP swap file is stored by default in the same location as your virtual hard 
drive. When you think about this, as it relates to rightsizing, you can see why 
assigning too much memory for no reason can have an impact on your environment. 
If you had 100 VMs running on your host, each with 16 GB of memory and no 
memory reservations, you would need an additional 1.6 TB of storage space to store 
all of your VSWP swap files. If those VMs really needed 4 GB of memory, then you 
would only need an additional 400 GB of storage, which is not a trivial amount but 
much less than the 1.6 TB.
Like CPU, it is important to rightsize the VM memory configuration.
Do not forget to plan for memory overhead as well as additional storage 
requirements for the VSWP swap file.
While swapping is generally considered a bad thing, after all, it means we are 
running low on memory in our ESXi host; the memory management techniques  
used in advanced can typically eliminate the need for swapping if planned properly. 
If possible, place the VSWP swap file on a separate storage, preferably flash, for the 
best possible performance during memory contention.

Assigning Resources to VMs
[ 34 ]
Monitoring memory usage
As we discussed in case of the CPU, there are several useful ESXTOP statistics for 
monitoring memory. When you run ESXTOP, you are given the CPU view, which is 
the default view. To switch to memory statistics, just hit M on your keyboard and the 
view will change. Refer to the VMware communities list again and the KB article that 
we mentioned earlier if you need a refresher (https://communities.vmware.com/
docs/DOC-9279 and http://kb.vmware.com/selfservice/microsites/search.
do?language=en_US&cmd=displayKC&externalId=2001003). The first item I tend 
to look at on a host is MEM overcommit avg. It shows how much memory you have 
overcommitted on your host.
A host that is not overcommitted (which is when the host has more memory than 
the VMs running on the host are assigned plus memory overhead) will have a return 
value of 0 as shown in the following screenshot:
You have probably noticed there are three values of 0.00, 0.00, and 0.00 returned in 
the preceding screenshot. These are the averages over a 1, 5, and 15 minute window. 
The other statistics that you should make note of are as follows:
•	
PSHARE: This is a count of the number of shared pages and memory saving 
due to TPS.
•	
SWAP: This is the total amount of memory being swapped to the disk. This 
will most likely cause noticeable performance problems and should relate to 
a high MEM overcommit avg. Consider adding more memory or reducing 
the number of VMs on the host if this is consistently high.
•	
MEMCTL: This shows how much memory has been reclaimed by the balloon 
driver. A consistently high value here could lead to future disk swapping; 
however, this is a valid memory overcommitment management feature. You 
should also consider monitoring your individual VM OS for swapping if this 
value is high, to ensure VM performance is optimal.

Chapter 2
[ 35 ]
There are also statistics to monitor at the VM level. For example, if MEMCTL is high, 
you may want to add the MCTLSZ column to see which VMs are returning memory 
via the balloon driver. To add this column that is not visible by default, perform the 
following actions:
1.	 Hit the F key on your keyboard.
2.	 Hit the J key on your keyboard to select MCTL.
3.	 Hit Esc and you will be returned to the ESXTOP statistics view with the  
new columns.
Now you can see how much memory has been reclaimed (if any) or could be reclaimed 
by the balloon driver by examining the MCTLSZ and MCTLMAX columns.
Storage considerations and their effects 
on performance
Storage is the one area that you cannot skip while evaluating your environment 
and workload, and no amount of CPU or RAM will make up for a misconfigured 
or underconfigured storage system. Storage in many environments is the biggest 
resource bottleneck due to its cost, general misunderstanding about how it works, 
and what is important. Outside of IT, storage is typically thought of in terms of 
capacity which, while a valid point, is only a small piece of the storage equation.
From a business perspective, cost is the driving factor on hard drives. Our job as 
administrators, engineers, and architects is to understand the workload so that we 
can select the storage appliances, drives, and connectivity that will support the 
current environment as well as future growth. You could spend an entire career 
learning about and mastering storage, but we have only few pages in this book.  
For a deeper dive into storage, I would suggest picking up Storage Implementation  
in vSphere, Mostafa Khalil and Troubleshooting vSphere Storage, Mike Preston.

Assigning Resources to VMs
[ 36 ]
What is IOPS?
For the purposes of our discussion, let's start at the storage appliance. In most 
vSphere environments, storage is provided by hardware and networks separate  
from the physical server known as the storage fabric. Two of the main metrics that 
we will look at is I/O per Second (IOPS) and throughput (the amount of time it takes 
for data to be transmitted). Let's start with IOPS, which is mostly determined by the 
hard drive, which also makes it one of the most important factors when it comes 
to performance. Typically, you will see four types of hard drives that are available 
(from the slowest and largest to the fastest and smallest); for example, SATA (or  
NL-SAS), SAS, FC, and SSD.
While SATA drives can generally provide enough storage capacity, they cannot 
generally provide enough IOPS, and SSDs can provide enough IOPS but generally 
not enough capacity. The following is a chart of generally accepted average hard 
drive IOPS:
Drive Type
IOPS
Capacity
SATA 7200 RPM
Up to 100 IOPS
Up to 4 TB
SATA 10,000 RPM
Up to 150 IOPS
Up to 1 TB
SAS 10,000 RPM
Up to 140 IOPS
Up to 1 TB
SAS 15,000 RPM
Up to 210 IOPS
Up to 600 GB
SSD
Up to 400 to 120,000 IOPS
Up to 1.2 TB
RAID
Unlike in a laptop or desktop where you usually have a single hard drive, storage 
appliances are composed of multiple drives grouped together called arrays and more 
specifically into a redundant array of independent disks (RAID), though you may 
also see Just of Bunch of Disks (JBOD), which does not provide any redundancy.
The IOPS of your array is the number of drives of that array multiplied 
by the IOPS of the drive minus any RAID write penalties.
You should check with your storage appliance manufacturer to determine the 
recommended number of drives to place in a single array, which may vary based  
on the type of array you created.

Chapter 2
[ 37 ]
Once the RAID or JBOD is created, you can then create a LUN. Depending on your 
storage appliance, you could create a single LUN on that RAID, or you could create 
multiple LUNS. Whenever possible, I prefer to create a single LUN on a single RAID 
or JBOD that fulfills the workload requirements being run by the VMs. By doing this, 
you can easily identify the workload and the associated VMs running that workload 
for a particular LUN. If you were to create multiple LUNS on a single RAID or JBOD, 
you may run into contention when a workload on one LUN is utilizing all of the I/O 
resources available from that RAID, which may not be immediately obvious while 
observing perceived performance problems on the other LUN.
RAID has a major impact on your available IOPS and adds another layer of 
complexity to account for workload patterns. Depending on your application, you 
could have read a heavy workload, that is, the one that is reading more data than it 
is writing, such as a reporting application, or a write heavy workload, that is, the one 
that performs more writes, such as applications that import large amounts of data.
You may even have applications where your workload changes depending on 
business cycles. One such example is a financial system which may have a write-
heavy workload at the end of a month when the information is being imported and 
which then changes to a read-heavy workload at the beginning of the month when 
the reports are being run.
While we don't have enough space to dive deep into RAID types, the following chart 
should give you what you need to begin your research. Depending on your storage 
appliance, you may be able to assign what is called a hot spare drive, which will be 
used to automatically replace a failed drive in a RAID, though generally it's only 
useful where there is already a redundancy.
RAID Type
Redundancy
Read or Write Favorable
RAID 0
No
Both
RAID 1
Yes; 1 disk
Both, limited by the disk type
RAID 5
Yes; 1 disk
Read
RAID 6
Yes; 2 disks
Read
RAID 0+1
Yes; depends on RAID 0
Both
www.allitebooks.com

Assigning Resources to VMs
[ 38 ]
RAID 5 is one of the more common RAID types deployed because of its ability to 
maximize storage capacity and provide failure of a single disk without losing data. In 
nonvirtualized environments, RAID 5 could typically provide enough IOPS for even 
a write-heavy workload since it was the only workload utilizing those drives. The 
downfall of RAID 5 (and RAID 6) is that in order to achieve the drive redundancy, 
extra data is written that creates what is known as a write penalty since a single write 
has to be performed on two drives: the drive that the write was originally intended 
for and a parity write on a separate disk, which provides the redundancy. In a virtual 
environment where the LUN may be servicing multiple workload, the write penalty 
is magnified even further.
Selecting the drive type as well as the RAID type go hand in hand. Most modern 
storage appliances employ a data tiering system where you can perform writes 
on faster drives such as SAS or SSD, and the data is then moved to the most cost-
effective SATA drives after a certain period of inactivity. Additionally, an increasing 
number of vendors are offering all-flash arrays that can provide extremely high 
levels of IOPS but with lower storage capacity; depending on your workload, this 
may be sufficient.
VMware vSphere Storage APIs – Array 
Integration (VAAI)
Most storage appliance vendors will partner VMware to offer what is called VMware 
vSphere Storage APIs – Array Integration or VAAI. VAAI allows VMware vSphere 
to offload certain tasks that would otherwise consume host resources such as cloning 
a VM to the storage appliance. These tasks, called primitives, can be managed more 
efficiently by the storage appliance by itself. When you are researching storage 
options for your vSphere environment, you should definitely consider appliances 
that support VAAI. You can find more information about VAAI at http://www.
vmware.com/files/pdf/techpaper/VMware-vSphere-Storage-API-Array-
Integration.pdf.
The following are some points to consider while determining your  
storage requirements:
•	
Spend extra time monitoring I/O and throughput requirements
•	
Select drives and RAID options that will fit your IOPS requirements  
today as well as allow for future expansion
•	
Understand your VM and application workload profile, that is, read or  
write-heavy and possibly both at different times
•	
Use storage appliances and vendors that support VAAI

Chapter 2
[ 39 ]
Connectivity and throughput
Once you have your drive type and RAID type designed to support your workload, 
you then need to consider how you will connect your storage appliance(s) to your 
physical servers. There are three options for connecting your storage appliance: 
Ethernet, which is the same as your typical networking equipment, that supports 
iSCSI, NFS and Fiber Channel over Ethernet (FCoE); Infiniband; and traditional 
Fiber Channel (FC). Infiniband currently provides the fastest connectivity, which 
will improve throughput but it is also the most costly. Having said that, nothing 
costs more than outages and negative business impact. If your workload calls for 
Infiniband, you should advocate it. FC has been generally provided the best balance 
between cost and performance, but as 10 GbE (10 Gigabit Ethernet) becomes more 
cost effective, you might see a shift towards FCoE, iSCSI, and NFS.
Since you are typically connecting your storage appliance to multiple physical 
servers, a switch is used just as you would use a switch for networking purposes. 
Most designs implement a completely separate storage switch; however, many 
modern switches support a modular architecture where you can combine Ethernet 
for both networking and storage as well as FC or Infiniband into the same switch. 
What you choose for your environment should largely depend on your workload 
requirements; if you are looking to cut budget, you should probably look at areas 
other than your storage fabric.
Throughput, which will be affected by the type of connectivity you use, is the 
measure of how much data can be written. While IOPS measures the number of 
read/write operations per second, throughput measures how many KBs, MBs, 
or GBs can be written in a certain amount of time, typically measured in MB per 
second. There are many factors within your storage appliance which will also affect 
throughput, so check with your storage vendor and ask them to help you determine 
what your workload profile is, and test that profile against their storage array.
VMFS
With most connectivity selections, you will format your storage in the Virtual 
Machine File System format (VMFS); however, if you have gone down the NFS 
route, then NFS will be your filesystem format. There are a few items to be aware 
of with NFS. First, you will thin provision your virtual hard drives (VMDK); thin 
provisioning consumes only the amount of space actually required by the virtual 
hard disk. If you assign a 100 GB virtual hard drive to a VM but it only uses 20 GB, 
you will only consume 20 GB of actual storage. We will touch on the pros and cons 
of thin provisioning shortly. Also, depending on your NFS network and uplink 
configuration, you may not be taking advantage of multiple uplinks for better 
performance. This is because the default load balancing method uses both the 

Assigning Resources to VMs
[ 40 ]
source and destination IP addresses, even with multiple bonded uplinks; these will 
always be the same if the NFS targets are all presented on the same IP address from 
your storage appliance. There are a few options to achieve better load balancing, 
such as creating multiple NFS servers with unique IP addresses. The VMware Best 
Practices for Running VMware vSphere on Network-Attached Storage document has 
more information on NFS considerations at (http://www.vmware.com/files/pdf/
techpaper/VMW-WP-vSPHR-NAS-USLET-101-WEB.pdf).
While the items mentioned previously may seem like limitations, they are simply 
considerations that you will also have to plan for and manage with block-based 
protocols such as FC, FCoE, and iSCSI. With these protocols, you still need to design 
your storage fabric to provide the necessary throughput and redundancy. The debate 
between block and NFS, which for many years favored block, has come down to 
requirements and supportability. What your infrastructure requires and how well  
can you support the infrastructure should ultimately dictate which storage protocol 
you select.
VM disk provisioning
While NFS makes the decision on thin provisioning for you, this is still an option 
with VMFS, and it's important to understand the differences. As mentioned in the 
previous section, thin provisioning only allocates the amount of space actually used 
within the VMDK. This allows you to overcommit your storage appliance, like we 
have discussed with the CPU and memory. This can be useful where growth is 
expected over a long period of time or when the VM is not easily taken offline for 
maintenance. While the overhead of expanding a thin provisioned disk has been 
tested to be negligible, applications with high write workload may still be impacted. 
In these scenarios, you may wish to thick provision your VMDK.
Thick provisioning, as you may have guessed, allocates the full amount of assigned 
storage immediately. If you create a VMDK with 100 GB of space, that 100 GB of 
space is allocated on your storage appliance immediately. There are two options 
available for thick provisioned VMDKs: Eager Zeroed and Lazy Zeroed. Lazy Zeroed 
allocates the entire amount of space immediately to the VMDK but does not wipe 
or zero the blocks until the first write request. With Eager Zeroed disks, all blocks 
are wiped when the disk is created. This causes the disk creation time to take longer 
but provides the best possible performance for write-intensive applications. For 
Windows administrators, you can draw a parallel to quick formatting of an NTFS 
partition to thick Lazy Zeroed versus a full format to an Eager Zeroed drive.

Chapter 2
[ 41 ]
Monitoring storage
Troubleshooting storage can be quite difficult; however, like CPU and memory, 
there are some key ESXTOP statistics to look at if you suspect a storage bottleneck. 
To access disk statistics, hit v on your keyboard while in ESXTOP. Unlike CPU 
and memory, this will be strictly VM-focused. The following counters are focused 
specifically on VMs:
•	
READS/s: This is the number of reads per second
•	
WRITES/s: This is the number of writes per second
The sum of these statistics should match the expected I/O from your datastore  
and LUN/array. For example, if you had seven SATA drives in a RAID 5, and your 
READS/s and WRITES/s were equal, that is a 50/50 split, then you could expect 
to get somewhere around 224 IOPS from that datastore/LUN/array (if you kept to 
a single datastore per array, otherwise you need to view all VMs on all datastores/
LUNs created on that array). If the combination of reads and writes exceeds 224, then 
you have more I/O than the LUN can handle.
As we mentioned earlier, I/O is not the only factor in performance, you also need to 
consider throughput. To view storage adapter-related statistics, press the d key on 
your keyboard while in ESXTOP. The statistics to monitor include the following:
•	
GAVG: This is a measure of the round trip latency seen by the VM.
•	
KAVG: This is the latency related to the ESXi kernel.
•	
DAVG: This is the latency seen at the device driver level. It includes the 
roundtrip time between the HBA and the storage.
These statistics would likely need to be related to similar statistics from your storage 
array to determine whether the bottleneck is host-related, a specific array, a storage 
appliance, or a storage network.
Networking
Networking in a vSphere environment is not all that different from physical 
networking. VMs are assigned virtual network cards which are attached to  
switches. A virtual switch has an uplink to a physical switch.

Assigning Resources to VMs
[ 42 ]
Uplinks
An uplink is a physical NIC on the ESXi hosts that connects the server to the 
network. In a typical ESXi deployment, you typically have multiple physical NICs 
to support various types of network traffic. It is also considered best practice to 
have redundancy for your various uplinks with multiple NICs supporting a single 
traffic type. Depending on your requirements, this may be a single active NIC with 
a standby NIC, which becomes active in the event of a failure, or you may bond 
multiple interfaces together and create an LACP group on your physical switch to 
support additional throughput. In extreme cases, I have even heard of architects 
using NICs from different manufacturers so that a driver failure or crash would not 
affect all NICs on a host.
Having multiple physical NICs/uplinks is only part of the equation. You also need 
to determine where these uplinks will connect to. If you have used multiple physical 
nicks for redundancy, but only one uplink to a single physical switch, then you still 
have a single point of failure in the network stack.
Wherever possible, consider the following requirements:
•	
A minimum of two physical NIC ports, preferably on two physical NICs for 
each type of traffic uplink (for example, one on board the NIC port and one 
on the PCI card port)
•	
Each pair of NIC ports should connect to separate physical switches or at 
least separate line cards in a modular switch
•	
If bonding multiple NIC ports together for throughput, verify the 
recommended configuration from your switch vendor
At a minimum, there are two types of traffic from an ESXi host: VM kernel and VM 
network traffic. VM network traffic is just that—the network traffic from your VMs 
to the physical network. VM kernel or management traffic is the interface used to 
manage the ESXi host, connect it to vCenter, and for advanced features such as 
vMotion or Fault Tolerance; we will cover vMotion and Fault Tolerance in the  
next section.
As we have said before, the network design ultimately depends on your specific 
requirements. You could certainly run your management and VM network traffic 
over the same uplink, or you could just easily have separate physical uplinks for 
management, vMotion, FT, and VM network and storage; beyond that, you may 
even have multiple vSwitches for spreading out the same type of traffic.

Chapter 2
[ 43 ]
What is a vSwitch?
A vSwitch is quite similar to a physical switch. A vSwitch has ports that VMs connect 
to as well as various settings that you might also configure on a physical switch, such 
as VLANs, or port groups on a vSwitch, and MTU settings if you need to configure 
jumbo frames. There are two types of vSwitches: a Standard Switch (vSS) and 
Distributed Switch (vDS)—think of a vSS like a typical 1U 24 or 48 port switch that 
has some management features. A vDS is more like an advanced enterprise grade 
switch that can support multiple line cards with more advanced features. Having 
said that, there is no reason why a vSS cannot be used in large environments. As we 
have said many times, it comes down to your requirements and whether the features 
you would like to use are available on a vSS or on a vDS.
Like physical switches, vSwitches support VLAN tagging to allow you to separate 
different types of network traffic. On a vSwitch, you would create a port group 
to support different VLANs. If you are using a vSS, you need to create vSwitches 
and port groups on each host. With a vDS, all port groups are managed centrally 
via vCenter and pushed to each ESXi host. In the case of a vDS, each ESXi host acts 
in a similar fashion to line cards in a modular switch. While vCenter is critical for 
managing a vDS, it will continue to operate if vCenter is unavailable, but you will 
not be able to make any changes until vCenter is brought back online.
Monitoring network connectivity with ESXTOP
Like CPU, memory, and disk, ESXTOP can monitor network connectivity of your 
physical NICS (uplinks) and virtual NICs associated with VMs. You can see typical 
data transmission statistics; in the case of ESXTOP, this is tracked in MegaBits sent 
and received per second in the MbTX/s (transmitted/sent) and MbRX/s (received). 
These statistics can be useful to understand the existing traffic and whether a 
physical NIC can continue to support additional VMs or whether a particular VM is 
producing excessive amounts of traffic.
Two statistics that can immediately show whether a physical NIC is overcommitted 
or even connected to a faulty physical switch port or cable is %DRPTX and %DRPRX. 
On a physical NIC, you should expect these values to be at or near 0.00. On a VM, 
%DRPRX could also indicate a VM does not have sufficient CPU, memory, or disk 
I/O resources; though more typically, CPU.

Assigning Resources to VMs
[ 44 ]
Summary
In this chapter, we looked how assigning resources to VMs affect all of the resources 
on the host as well as other VMs. In addition, we looked at the features implemented 
by VMware vSphere to manage overcommitment such as the CPU scheduler, 
transparent page sharing, and ballooning. We then discussed storage and the effect 
drive type and RAID selection has on performance; reviewed important ESXTOP 
statistics for each of them and wrapped up an overview of networking, typical 
practices with uplinks and vSwitches, and finally, tips on how to monitor network 
connectivity. Next, let's look at the advanced settings that help further manage and 
provide availability to VMs and their resources.

Advanced Resource 
Management Features
Up until now, we have looked at what is needed to run VMware vSphere and 
how resources are consumed by VMs. In this chapter, we will look at some of the 
advanced features available to ensure VMs have access to the resources necessary to 
run properly as well as features that ensure VMs remain accessible.
We will cover the followings topics in this chapter:
•	
Understanding CPU power management
•	
Reservations, limits, and shares
•	
Resource pools and vApps
•	
vMotion, EVC, and Storage vMotion
•	
HA, DRS, and App HA
•	
Affinity and anti-affinity rules
•	
Fault Tolerance
•	
Hot Add
•	
SDRS and datastore clusters
•	
vSphere Flash Read Cache
•	
Distributed switches
•	
Network and Storage I/O Control
•	
Host profiles and auto deployment
•	
VSAN

Advanced Resource Management Features
[ 46 ]
Understanding CPU power management
CPU power management is less of a vSphere feature and more of a feature available 
on certain CPUs. These power management features allow the CPU to enter a low 
power state where it does not provide the full capacity of the processor resources to 
the OS based on demand. When demand increases, it comes out of the lower power 
state. However, having a feature like this enabled on an ESXi host can affect the 
performance of your VMs.
ESXi expects a certain amount of processing resources based on the identified 
processor and number of cores. If the CPU power management features are enabled, 
VMs will not receive the full amount of resources that potentially cause an increase 
in CPU %RDY, thus degrading performance. This is because the vCPU expects more 
resources than are available.
Within ESXi, you can configure the following preferred power management  
policy settings:
•	
High performance
•	
Balanced
•	
Lower power
•	
Custom
If the system shows a not supported message, then the hardware does not allow 
power management from the operating system and you will need to configure it as 
per the vendor's documentation. Except for fringe use cases, you should apply the 
High performance setting to avoid any potential performance problems. To check 
or change the Power Management settings, log in to the vSphere Web Client and 
navigate to vCenter | Host and Clusters, select the host, and click on the Manage 
tab. Under the Hardware section, click on Power Management.
Here, you can see the default setting of Balanced as well as the other options we 
listed earlier:

Chapter 3
[ 47 ]
VMware has several useful resources on Power Management, including the following:
•	
KB article 1018206 (http://kb.vmware.com/kb/1018206)
•	
VMware VROOM! Blog by Rebecca Grider (http://blogs.vmware.com/
performance/2013/05/power-management-and-performance-in-
esxi-5-1.html)
•	
Host Power Management in VMware vSphere 5, a technical white paper 
(http://www.vmware.com/files/pdf/hpm-perf-vsphere5.pdf)
Reservations, limits, and shares
Reservations act very much like you would expect them to—they reserve a certain 
amount of CPU memory resources to a VM. This can ensure your VM always 
has access to a specific amount of processor MHz or physical memory. Setting 
a reservation on a per-VM basis is very straightforward. While logged into the 
vSphere Web Client, right-click on any VM and select Edit Settings. While on the 
Virtual Hardware tab, expand either the CPU or Memory section. In the following 
screenshot, you can see the Reservation value for this particular VM is set to 0 MB:
As we discussed in the previous chapter, just because we assign 16 GB of memory 
to a VM does not mean it always has access to 16 GB of physical memory. During 
times of resource contention, ESXi may swap memory to disk or request memory 
be returned via the VMware Tools balloon driver. Reservations prevent this from 
happening and ensure the VM always has access to the amount of physical memory 
or CPU specified in the reservation. Reservations, however, have an impact beyond 
just ensuring a VM is guaranteed a specific amount CPU or memory. Later in this 
chapter, we will discuss the admission control that is enabled along with HA.
www.allitebooks.com

Advanced Resource Management Features
[ 48 ]
Resource limits
Limits act as the opposite of reservations and cap the amount of resources a VM can 
consume. As we saw in the previous screenshot, limits are configured in the same 
place as reservations, at the VM level. When you assign a limit to a VM, either on 
CPU or Memory, the guest OS is unaware of the limit at the hypervisor level. Using 
the VM as an example again from the previous section, if we limit our VM (which 
has 16 GB assigned) to only 8 GB of memory, the guest OS would only have access 
to 8 GB of physical memory from the host, even though it believes it has 16 GB. ESXi 
would then have to use its various memory reclamation techniques, as if there was 
memory contention on the host, such as ballooning or swap to disk, to fulfill the 
request from the guest OS.
You should also be aware that setting a CPU limit is further limited by the number 
of vCPUs assigned to the host. For example, if we had a VM with 2 vCPUs and set 
a limit of 2000 MHz, each vCPU would only have access to 1000 MHz of compute 
resources from any core. One of the few use cases I have found for limits is in 
development, test, or quality assurance-type environments. Take an application 
that, for example, requires 8 GB of memory to install, but because this is a test 
environment, you know it will never need more than 4 GB. You could assign 8 GB 
of memory to the VM and set a limit of 4 GB. Now, when the application is installed 
in the guest OS, it will see 8 GB of memory but ESXi will never allow it to use more 
than the 4 GB limit.
Resource shares
Shares, when implemented properly, can provide the best of reservations and limits. 
Shares allow you to set priority for resource access, but this is only enacted during 
times of resource contention. Let's combine the last two scenarios: you have two 
VMs, a production VM that is assigned 16 GB of memory, a test VM that is assigned 
8 GB of memory, and your host with 20 GB of memory. Under normal circumstances, 
the host can likely manage the memory between both VMs. If the test VM was 
under increased load, it could cause performance problems for the production VM. 
By assigning shares for memory to both VMs, you can ensure the production VM 
always has access to memory resources over the test VM.

Chapter 3
[ 49 ]
Shares between two VMs are easy to conceptualize and plan; however, in large 
environments, they need careful planning and monitoring. Imagine you assign 
memory shares to two resource pools, each with 10 VMs. A resource pool A has 
20,000 shares assigned for production VMs and a resource pool B has 10,000 shares 
assigned for testing VMs within the pool. Six months later, it's determined that the 
resource pool B should have five of the 10 VMs shutdown, but the shares are not 
changed. By not changing the shares for the resource pool B, the VMs will have 
access to an equal amount of resources as the VMs in the resource pool A, because 
the 20,000 shares in resource pool A are divided across 10 VMs. However, the 10,000 
shares in resource pool B's shares are now only divided between five VMs.
Resource pools
While reservations, limits, and shares can all be set at the VM level, it's much easier 
to manage them at the resource pool level by grouping several VMs together in 
some meaningful way; for example, all web servers for a particular application or 
all VMs responsible for running a particular application. Before we go on, resource 
pools require DRS to be enabled on the cluster. So, if you aren't licensed for either 
Enterprise or Enterprise Plus, you won't be able to use resource pools.
As we discussed previously, resource pools must be carefully planned and managed; 
otherwise, the share settings could be restricting resources to key VMs during times 
of contention (recall that shares are only used if there is resource contention unlike 
limits and reservations).
vApps
vApps is another feature that, like resource pools, requires DRS, so you will again 
need an Enterprise or Enterprise Plus license. Similar to resource pools, you can use 
vApps to assign reservations, limits, and shares to groups of VMs. One of the more 
useful features of vApps is that you can control the power on order of all VMs within 
a vApp. Let's look at an example to make more sense of this.
Consider a basic two-tier web application that has a web server and database server. 
In order for the application on the web server to function properly, the database 
server must be powered on and accept connections. If both of these VMs were placed 
into a vApp, you could power on the vApp as a single command, which could first 
power on the database VM, then after a certain period of time, power on the web 
server. While it's not a foolproof way to guarantee all servers have started properly, 
it's another tool provided by VMware vSphere.

Advanced Resource Management Features
[ 50 ]
vApps also include settings to control IP assignment for VMs within the vApp, 
providing either DHCP, static, or Transient IP allocation from a vCenter IP Pool. 
IP pools, however, is an advanced feature that requires developers to use VMware 
Studio to create vApp in order to work properly. IP Pools are not a replacement for 
either DHCP or manual IP assignment.
vMotion
vMotion is the feature that puts VM and virtualization as a whole on the map. vMotion 
is the process of migrating a VM from one host to another while it is running with 
no downtime. If you have ever had to maintain hardware and perform maintenance 
that required the physical server to be shut down, you can already imagine what an 
amazing tool this is.
vMotion is included even with the Essentials Plus and Standard license kits, so this 
feature is available to most licensed customers. The only license kit that vMotion 
is not included in the vSphere Essentials kit, which is essentially just a license for 
vCenter to manage up to three ESXi servers each with up to two processors.
To vMotion a VM, you must have at least two hosts managed by vCenter with at 
least one VMkernel interface enabled for vMotion traffic. Both hosts must have 
compatible CPUs from the same vendor; you cannot vMotion a VM from a host 
with an Intel processor to a host with an AMD processor. As of vSphere 5.1, you are 
no longer required to have shared storage as it can migrate both the VM state and 
storage in the same process. We will discuss storage vMotion shortly, so for now,  
I will focus on just the migration of the VM state.
An initial copy of the VM state is made on the destination host. During this process, 
the VM is still running on the original host, and the copy running on the new host is 
not responding to requests yet. The initial pass does take some time to complete. So, 
in order to complete, the vMotion process will continue to make passes, copying any 
memory changes since the last copy started, eventually getting to a point where the 
state of the VM is completely copied to the second host. Once complete, the VM on 
the original host is stunned to give the destination host time to start managing the 
VM and respond to requests.

Chapter 3
[ 51 ]
In the following figure, a VM residing on one host is copied to another host. The copy 
process repeats until the VM's memory state is completely copied to the new host:
VM VM VM VM VM VM VM VM
VMware ESXI
VMware ESXI
vMotion
The amount of time it takes to fully migrate the VM will depend on your network, 
the speed of your NICs, and switches, as well as the amount of memory assigned to 
the VM. As we have discussed several times, overcommitting resources to a VM is 
not recommended.
Typically, vMotion traffic is set up using a dedicated physical uplink so that it 
does not interfere with the management or VM network traffic. However, it can 
be enabled over the default VMkernel (also known as management) port group, 
though this is typically not considered a good practice. To enable vMotion, log in to 
the vSphere Web Client and navigate to vCenter | Hosts and Clusters. Then, select 
the host you wish to enable vMotion on, then go to the Manage tab and click on 
Networking. Now, click on the VMkernel adapters that you wish to run vMotion 
traffic over, then click on the pencil icon as shown in the following screenshot:

Advanced Resource Management Features
[ 52 ]
If you have not created additional VMkernel adapters, then you will most likely 
only see the one that is marked for management traffic created during the ESXi 
installation. Now, check the box next to vMotion traffic and you are ready to start 
migrating VMs. While vMotion is very easy to enable, like any other feature, 
it requires careful planning against your business requirements and network 
environment to ensure vMotion can complete in a timely manner.
In vSphere 5.1, a new feature called Multi-NIC vMotion was introduced. This 
allowed you to use several physical NICs to migrate VMs more quickly, even if they 
were not bonded together and connected via a supported switch configuration such 
as LACP.
Enhanced vMotion Capability (EVC)
Enhanced vMotion Capability or EVC was introduced with ESX/ESXi 3.5 and allows 
you to mix and match servers with different CPU versions. You are still, however, 
required to have CPUs from the same manufacturer; you cannot vMotion a powered 
on VM from a host with an Intel CPU to one with an AMD CPU.
Using the VMware compatibility guide that we used in Chapter 1, Understanding 
vSphere System Requirements, you can also search for CPUs compatible with a specific 
EVC mode. For example, if we search for all CPUs compatible with the Intel Ivy-
Bridge Generation, we will be a given a list of all CPUs that can be used in that EVC 
mode, as seen in the following screenshot (note that not all CPUs are shown):

Chapter 3
[ 53 ]
EVC plays an important role in the ongoing life cycle of a cluster. Imagine a 
scenario where 10 hosts can adequately provide resources for all of the VMs in an 
environment. As the organization grows, you may need to add additional hosts to 
provide more capacity, but the CPUs used in the initial deployment may no longer 
be available. EVC allows you to buy new hosts and integrate them into your existing 
cluster and processes.
How to enable EVC
Enabling EVC is not difficult; however, you may need to power off existing VMs 
in the host if the CPU has a feature set newer than the EVC mode you are enabling. 
This is because those features from the newer CPU will no longer be available when 
the EVC is enabled. To enable EVC, log into vSphere Web Client and navigate to 
vCenter | Hosts and Clusters. Select the cluster you wish to enable EVC on and click 
on the Manage tab. Select VMware EVC and click on the Edit button. You can now 
choose either the EVC mode for AMD or the EVC mode for Intel hosts and then the 
appropriate EVC mode.
Once you select the EVC type you wish to use, vCenter will check the hosts to  
ensure they are compatible. For example, in the following screenshot, EVC for  
AMD Hosts has been selected, which we cannot use because the CPUs in the  
hosts are Intel-based.

Advanced Resource Management Features
[ 54 ]
You will be required to enable EVC if your cluster contains hosts with different CPU 
models and has DRS enabled. Once EVC is enabled, it masks features in the CPU that 
are not compatible with that mode. For example, if you set the EVC Mode to AMD 
Opteron Generation 1, you would not have access to features in Opteron Generation 
2, even if other hosts use that type of processors. For more information on VMware 
EVC modes, refer to the KB article 1003212 at http://kb.vmware.com/kb/1003212.
DRS
Both DRS and HA are extremely advanced topics. In fact, one of my all-time favorite 
books, VMware vSphere 5.1 Clustering Deepdive, Duncan Epping and Frank Denneman is 
devoted entirely to this subject, and I'd highly suggest you pick up a copy. Because 
of how deep the book dives, it takes a few reads, but it is well worth the time 
investment. Because of the value this book has provided me, I am honestly not sure I 
could write this section without thanking both authors of the book.
DRS builds up on your vMotion configuration. Whereas vMotion allows you to 
manually relocate VMs, DRS provides automated migration and initial placement of 
VMs based on the workload to ensure resources are readily available to your VMs. 
Additionally, DRS uses a feature called Distributed Power Management (DPM) to 
automatically power off unnecessary hosts to conserve power.
Enabling DRS is quite straightforward. Like EVC, it's enabled at the cluster level, but 
you do have to consider the impact on your environment. For example, since DRS 
uses vMotion to help balance the workload by migrating VMs to host with available 
resources, you'll need to consider the maximum number of concurrent vMotion 
operations per vMotion network, whether its 1 Gbps or 10 Gbps, the maximum 
number of operations per datastore, and so on.
DRS-enabled clusters can only have a host with the same type of processor, either 
Intel or AMD, and EVC configured if there are different CPU models to ensure 
vMotion will work as required. Each host in the cluster will also need access to a 
shared datastore(s) so that each host has access to all of the VM-related files such 
as the VMDK (the virtual hard disk), VMX (the configuration file), and swap file. 
You'll need to consider the swap file location if you have moved them out of a shared 
datastore, for example, if you have configured them to be stored on the local server 
flash storage.

Chapter 3
[ 55 ]
To enable DRS while in vSphere Web Client, navigate to vCenter | Hosts and 
Clusters. Select the cluster you wish to enable DRS on and click on the Manage  
tab. Under Services, select vSphere DRS and click on the Edit button. Check the  
box next to Turn ON vSphere DRS to configure the settings.
There are three automation levels you can choose to enable for DRS: Manual, 
Partially Automated, and Fully Automated.
•	
Manual: This DRS will make suggestions as to where to place new VMs  
and make suggestions for VM relocation when requested
•	
Partially Automated: This DRS will automatically place newly created  
and VMs being powered on and make suggestions for VM relocation  
when requested
•	
Fully Automated: This DRS will automatically manage the placement  
and relocation of VMs
DRS, while it is a fantastic feature, introduced an interesting problem. What if 
you have a clustered/highly available application composed of multiple VMs 
and DRS places and all of these VMs on a single host? All of the application-level 
configuration and planning would be for nothing if that single host were to go  
down. To address this, VMware uses affinity and anti-affinity rules.
DRS affinity and anti-affinity rules
There are two types of affinity and anti-affinity rules: host-based and VM-based. 
Host-based rules ensure a VM is either on (affinity) or not on (anti-affinity) a specific 
host. If we consider the example we discussed at the end of the DRS section, you 
could create a rule to ensure that VM-A is placed on Host-A, while VM-B is placed 
on Host-B. Now, if one of the two hosts goes offline, at least one of the VMs will still 
be running (assuming no previous action to migrate the VM was taken). An anti-
affinity host rule would prevent a VM from being placed onto a specific host.

Advanced Resource Management Features
[ 56 ]
You can also create affinity and anti-affinity rules on a VM-to-VM basis, which in 
my experience is a bit more useful. Rather than creating a rule that says a VM can 
or cannot be on a certain host, you can create rules that group VMs together on the 
same host (affinity) or ensure the VMs are running on different hosts (anti-affinity).
It should be noted that affinity and anti-affinity rules are not obeyed by VMware 
High Availability (HA).
High Availability
While DRS is a feature that allows the automatic placement of new and running VMs 
to hosts, which can ensure resource requirements are met, VMware vSphere High 
Availability (or HA as it's commonly referred to) is a feature that helps restore VMs 
to a running state in the event of an unplanned host failure.
It's important to understand that HA is for unplanned outages, such as a component 
failure in a host. VMs running on a host that suffers an unplanned outage are 
shutdown when the host fails and are powered up on an available host within  
the cluster.
While HA is enabled at the cluster level like DRS, it functions exclusively at the host 
level, so a failure in vCenter will not prevent HA from working normally. In fact, HA 
is a great way to ensure vCenter uptime since a clustered or load balanced version of 
the vCenter application is not supported. For more information on support vCenter 
HA options, see VMware KB 1024051 at http://kb.vmware.com/selfservice/
microsites/search.do?language=en_US&cmd=displayKC&externalId=1024051.
When HA is enabled, an agent is installed on each host. This agent is responsible for 
communication between all the hosts to ensure the hosts are online and running as 
expected. As of vSphere 5, the agents operate in a master/slave mode. A master is 
selected from the hosts and the other hosts act as slaves. HA will then create a file on 
the shared datastores in a folder called vSphere-HA with an inventory of all VMs and 
their power state.
HA communicates over the VMkernel interface for traffic management. However, it 
also uses shared datastores to monitor host availability in the event the management 
network is not available. As we discussed previously, it's quite common, if not 
best practice, to isolate various types of traffic related to your VMware vSphere 
environment; the examples we looked at were management, vMotion, and general 
VM network traffic. If your management network were to become unavailable, HA 
would verify through the selected datastores whether or not the VMs that belong to 
that host were still running.

Chapter 3
[ 57 ]
Admission control
One feature of HA that is often misunderstood is admission control. Admission 
control ensures that your cluster has enough computing resources to satisfy all of  
the running VMs. There are three admission control policy options that you can 
select from based on your specific requirements.
The default option, Define failover capacity by static number of hosts, ensures 
that the cluster has enough resources for the specified number of hosts to fail. To 
do this, it calculates a slot size based on the CPU and memory reservation of all the 
powered on VMs and then calculates how many slots each host can support. With 
this option configured, you can see how many slots your cluster has available and 
the number of used slots. Once logged into vSphere Web Client, navigate to vCenter 
| Hosts and Clusters. Select the cluster you wish to view the slot sizes for and click 
on the Monitor tab. In the vSphere HA menu, you will see Advanced Runtime Info. 
This section shows you the slot being used for the calculation, the number of slots 
available, and the number of slots in use, as seen in the following screenshot:
In this cluster, where there are no reservations set, you can see that it's only using 32 
MHz and 138 MB to calculate the slot size. While this will most likely ensure you can 
power on many VMs within the cluster, it also means you may not realistically be 
able to maintain an adequate amount of resources for all of the VMs running in the 
event of a host failure.
www.allitebooks.com

Advanced Resource Management Features
[ 58 ]
If, on the other hand, you have even one VM with a large reservation, you could alter 
the percentages negatively where you only have very few slots available. Let's look 
at the same cluster but after assigning a 1024 MB reservation for one of the VMs. As 
you can see, the slot size is now based on the 1024 MB limit plus memory overhead.
You may be wondering why the Total slots in cluster field does not match the 
available slots plus used slots. In this case, the hosts are not well balanced. One host 
has 8 GB of memory, while the other has 128 GB of memory. Combining this, based 
on the slot size calculation, we have 112 slots in the cluster. However, if the host with 
128 GB of memory were to fail, we would only be left with a host which has 8 GB of 
memory. The other option available for configuring a slot size is to manually specify 
it. This is useful if you have a few VMs with very high reservations that might 
otherwise impact your total number of available slots.
You can also configure HA to reserve a specific amount of resources in the cluster. 
Doing this requires a detailed understanding of your environment so that you can 
properly calculate what percentages you will need for CPU and memory to support 
your specific applications and workloads.
First, you need to calculate how many compute and memory resources you have 
in the cluster. Let's use CPU/compute as an example to determine the appropriate 
settings for your environment.

Chapter 3
[ 59 ]
If you have 10 hosts, each with 10 GHz of total compute, then 100 GHz would 
represent 100 percent of the total compute resources. Now, you need to determine 
how many compute resources all of the running VMs require. If all of your VMs 
require 25 GHz, that would leave 75 percent of the cluster resources to CPU failover 
capacity (100 GHz-25 GHz/100 GHz). If you set the Reserved failover CPU capacity 
setting to 25 percent and are using 25 percent of those resources already for current 
VMs, then admission control will allow you to use up to 50 percent, or 50 GHz of 
compute resources in this case. This is before admission control would prevent 
the power on of new VMs. The same calculation would be used to determine an 
appropriate Reserved failover Memory capacity setting.
The final admission control option is Use dedicated failover hosts. With this setting, 
admission control will keep the specified hosts completely idle, reserved just for the 
failover capacity. While this is a fairly safe option, it also works against leveraging 
the capacity of all the hosts in your cluster.
App HA
App HA is a new feature in vSphere 5.5, which brings application level restarts for 
critical applications. Like HA for VMs, App HA restarts applications based on the 
configuration and applications you have monitored. Since App HA is a new feature, 
the applications supported are slightly limited but certainly useful for most modern 
applications.
The applications supported at the time of writing this book include the following:
•	
Apache Tomcat 6.0 and 7.0
•	
IIS 6, 7, and 8
•	
Microsoft SQL 2005, 2008, 2008 R2, and 2012
•	
Apache HTTP 2.2
•	
SharePoint 2007 and 2010
•	
SprinceSource tc Runtime 6.0 and 7.0
The requirements are ESXi and vCenter 5.5 as well as vCenter Hyperic (a monitoring 
solution provided by VMware). Assuming you meet these requirements, App 
HA is deployed as a virtual appliance via the OVA provided (an OVA is simply a 
packaged VM that is platform agnostic, meaning you could import the OVA into 
ESXi, or Citrix Xen Server, and so on). Because App HA requires Hyperic, a feature 
outside the scope of this book, we will not go into the details of how to configure it; 
however, you can find more information in the vSphere App HA Documentation Center 
at http://pubs.vmware.com/appha-1/index.jsp.

Advanced Resource Management Features
[ 60 ]
Fault Tolerance
Fault Tolerance, included with standard and above licensing, is one of the more 
powerful features of vSphere but also possibly one of the least used at the same 
time. Fault Tolerance creates a copy of the protected VM and keeps it in sync during 
normal operation. If there is a failure to the host running the protected VM, the 
copy can immediately take over as if the original VM never went down. While FT is 
enabled at the VM level, it's important to note that it protects against host failure like 
HA does, but does not require the VM to restart like HA. After the secondary copy 
becomes active, HA will boot a new secondary VM to ensure FT is able to resume.
Because the secondary copy is kept in sync, it is unable to protect against an 
application failure. When an application within the protected VM fails, the 
application also fails on the secondary VM.
Fault Tolerance has the following specific requirements:
•	
The protected VM must be in an HA-enabled cluster.
•	
The hosts must have access to the same shared storage location that the 
protected VM is stored on.
•	
A VMkernel adapter must be configured for vMotion and Fault Tolerance 
logging. They can be on the same adapter, but if you are making heavy use of 
FT, you should consider a dedicated adapter.
•	
The VM cannot have any unsupported devices connected, such as local USB 
or CD drives.
•	
The VM must be configured with a VMDK or virtual mode RDM.
•	
The VM must only have a single vCPU, as virtual SMP is not supported.
The last bullet point is the largest pitfall of FT at this time. Most applications that 
might leverage FT are likely to also require multiple vCPUs; however, if your 
application does not, then FT is a great way to ensure continuous availability. Multi 
vCPUs/virtual SMPs have been rumored for some time but are not yet available.
For more information on FT, see the vSphere Availability guide at http://pubs.
vmware.com/vsphere-55/topic/com.vmware.ICbase/PDF/vsphere-esxi-
vcenter-server-55-availability-guide.pdf.

Chapter 3
[ 61 ]
Hot Add
Hot Add is a feature that allows you to add virtual hardware to a VM while  
running. This feature, previously present only in advanced editions of vSphere,  
is now available with standard and above licensing. While this feature is available 
via VMware vSphere, not all operating systems support adding resources such as 
CPU or memory while running.
Systems running Data Center Edition of Windows 2008 and above, and most  
modern Linux kernels support this feature, though you'll need to verify this for  
your specific edition.
Storage vMotion, Storage DRS, and 
Datastore clusters
Till now, we have focused on features and practices to ensure CPU and memory 
resources are available to your VMs. However, there are several features that, like 
their compute and memory-focused siblings, help to ensure your storage resources 
are readily available. The storage-focused version of the features we have previously 
discussed offer much of the same focus.
Feature 
(Compute/Memory)
Function
Feature 
(Storage)
Function
vMotion
This moves the CPU 
and memory from 
a running VM to 
another host
Storage 
vMotion
This moves the virtual 
hard drive, configuration 
file, and other related 
files to another datastore
DRS
This provides 
recommendations 
on placement and 
can automatically 
move VMs to a host 
that can support 
CPU and memory 
requirements
Storage DRS
This provides 
recommendations on 
virtual hard drive and 
other related files and 
can automatically move 
storage-related files to 
another datastore with 
adequate available 
space.
Clusters
This creates a logical 
group of compute 
resources that can be 
shared among VM 
through the use of 
vMotion and DRS
Datastore 
cluster
This creates a logical 
group of datastore 
resources which can 
be shared among VMs 
through the use of 
vMotion and DRS

Advanced Resource Management Features
[ 62 ]
Storage vMotion
Storage vMotion provides administrators with the ability to migrate VM storage 
to a new datastore while running. This allows you to manually balance space on 
your datastores or migrate a VM to a different tier of storage based on the new 
requirements. Let's consider a couple of examples. Storage vMotion, as of vSphere 
5.5, has been included with standard and above licensing. Prior to 5.5, this was only 
available in the Enterprise/Advanced and higher level licenses.
As more VMs are added to your environment, the space available on your datastore 
will decrease. Because of the increase in datastore utilization, you decide to add a 
new group of drives to your storage appliance, create a LUN, and present the LUN 
to your ESXi hosts as a new datastore. Storage vMotion, in this scenario, would 
allow you to relocate all of the files for a VM to the newly added datastore. Another 
option, as you might be thinking, would be to expand the datastore at the storage 
appliance level. While this is certainly possible, it would depend on your specific 
storage appliance and the impact it imposes on your datastore, such as decreased 
I/O performance while it adds the new drives. In some cases, you may decide to just 
add the additional datastore. Also, as we discussed in Chapter 2, Assigning Resources 
to VMs, most storage appliance vendors may have recommended array sizes, which 
you would not want to exceed so that you are not overloading the management or 
storage controllers in the appliance.
Now let's walk through a performance-related scenario. A VM is created for testing 
a specific application on a datastore with a low IOPS profile such as the one created 
on SATA drives. After testing, you decide to promote the VM into production for 
all users to access. With additional users accessing the application, there will be 
additional IOPS required, which the SATA-based datastore cannot meet. Here, you 
could migrate the VM storage to a datastore created on flash storage to meet the new 
IOPS requirements.
Without these features or on a physical server bound to specific hardware, this 
would have been much more difficult to manage without incurring some form of 
downtime. With Storage vMotion, you can do this while the VM is running and 
being accessed.
Unlike vMotion, there are no additional networking requirements to set. The 
vMotion process is managed by the ESXi host unless your storage appliance supports 
the VAAI XCOPY primitive (http://www.vmware.com/files/pdf/techpaper/
VMware-vSphere-Storage-API-Array-Integration.pdf ). As we discussed in 
Chapter 2, Assigning Resources to VMs, if you have a storage appliance that supports 
VAAI, it can handle requests that would otherwise need host CPU and memory 
resources to complete.

Chapter 3
[ 63 ]
Datastore clusters
Datastore clusters, just like compute clusters, are a logical representation of  
resources presented to your hosts. A datastore cluster is made up of independent 
datastores. Datastore clusters, similar to how a compute cluster requires CPUs from 
the same manufacturer, must be made up of the same type of datastores with the  
following considerations:
•	
A datastore cluster must either be a VMFS datastore (FC or iSCSI) or  
NFS; you cannot combine VMFS and NFS datastores into the same  
datastore cluster.
•	
You can only attach ESXi 5.x hosts to the datastore if it will be added to 
a cluster. If there are any hosts running 4.x or 3.x, you will need to either 
upgrade the host or disconnect it from the datastore.
To create a datastore cluster, navigate to vCenter | Storage in vCenter Web Client, 
click on the Actions button, and select New Datastore Cluster. The New Datastore 
Cluster wizard will begin by creating a compute cluster; name the datastore cluster, 
and select whether you would like to Turn ON Storage DRS, which we will discuss 
shortly. The steps for creating a datastore cluster are as follows:
1.	 Select the automation level for Storage DRS; you will be asked to choose  
this setting whether or not you selected to turn on Storage DRS on the 
previous page. These settings will then be saved if you decide to enable 
Storage DRS later.
2.	 Next, select whether you want to enable I/O Metric Inclusion, which will 
factor I/O latency into Storage DRS recommendations. On this tab, you can 
also select the storage utilization threshold you wish to attain on each of  
the datastores.
3.	 If you expand Advanced Options in step 2, you can choose the default  
VM affinity rule, which is to keep VMDKs together. If you do not select this 
option, then DRS may place different VMDKs for the same VM on different 
datastores. That's not necessarily a bad thing, just a design decision you need 
to make based on your VM and application requirements.
4.	 Select the compute cluster you wish to enable this datastore cluster in.
5.	 Next, select the datastores you wish to include. Datastores will need to be 
available on all hosts in the cluster, otherwise you will see a warning that 
reads Host Connections Missing.
6.	 Finally, you can review the settings before clicking on the Finish button.

Advanced Resource Management Features
[ 64 ]
Once you click on Finish, the datastore cluster will be created and any datastores 
selected will be placed into the cluster. While you can select local disks to be part of 
the cluster, carefully consider the impact on availability. If the host fails, there would 
be no way for the HA to restart the VMs on a new host. You can see a complete 
datastore cluster in the following screenshot:
While a datastore cluster can be created with a Standard vSphere license, the real 
automation comes from Storage DRS, which requires at least one datastore cluster to 
be configured and a vSphere Enterprise Plus license.
Storage DRS
While Storage vMotion is available with the vSphere Standard license now, Storage 
DRS still requires an Enterprise Plus license. If Storage vMotion is the tool for 
manual migrations and load balancing, then like DRS is for compute and memory, 
Storage DRS is a tool designed to automate the placement and migration of VM 
storage to a datastore capable of providing the resources needed by the VM.
Storage DRS has the following two modes:
•	
No Automation (manual mode): This provides recommendations that you 
will need to choose to apply.
•	
Fully Automated: This will make the initial placement and migrations for 
you. Placement and migration is based on two factors: space utilization and 
I/O latency.
Storage DRS employs the concept of affinity rules to control the placement of VM 
storage on specific datastores. As we saw during the datastore cluster creation, you 
have the option of enabling the default VM affinity rule to keep VMDKS together, 
or you can disable this setting, which would allow Storage DRS to select different 
datastores for different VMDK files belonging to the same VM.

Chapter 3
[ 65 ]
Storage DRS automation levels as well as affinity rules can also be set on a per VM 
basis. To do this, click on the Settings tab with the datastore cluster selected and 
click on VM Overrides. The VM Overrides section allows you to add specific VMs 
that will use different rules than the defaults for the cluster. For example, maybe you 
have a VM that must always run on a datastore backed by flash storage. Even if your 
Storage DRS automation settings are fully manual and the affinity rule is not set to 
keep VMDKs together, you could add the specific VM to the VM overrides section so 
that Storage DRS does not take any action on that VM.
One important thing to note when comparing DRS and Storage DRS is that DRS 
operates in real time, migrating VMs from host to host; however, Storage DRS 
operates at a set time interval (which you can change), that is, every eight hours by 
default. We can set the interval in the advanced settings of the datastore cluster.
vSphere Distributed Switches
We briefly mentioned distributed switches in Chapter 2, Assigning Resources to VMs. If 
you recall, a virtual switch acts in the same fashion as a physical switch; VMs connect 
to the switch or port group that controls network access through the use of VLANs. 
The virtual switch then uses a physical NIC as an uplink to a physical switch so that 
VMs can communicate with VMs on a different host and other devices connected to 
the network.
vSphere Distributed Switches (vDS) is an advanced version of vSwitches available 
with Enterprise Plus licenses. A vDS offers several advantages over standard 
vSwitches (vSS), including the following:
•	
Easier management across all hosts connected to the vDS; a change to the 
vDS is applied to all hosts whereas a change on a vSS has to be performed  
on each host
•	
Support for private VLANs
•	
Inbound traffic shaping
•	
Network IO Control (NIOC) that allows network bandwidth to be allocated 
to network resource pools using either predefined traffic types (VM network 
traffic, vMotion traffic, and so on) or custom-defined traffic types
A vDS operates like a modular physical switch. vCenter provides the management 
features, similar to what a supervisor card in a Cisco switch would do, and provides 
its configuration to each of the host, which then act in a similar fashion to a line card, 
that is, receiving its configuration from the supervisor.

Advanced Resource Management Features
[ 66 ]
There are two types of vDSs: the native VMware vDS and third-party vDSs such as the 
Cisco Nexus 1000v. In this section, we will focus on the native VMware vDS. If you use 
or need to use a third-party vDS, refer to the documentation from that vendor.
To create a vDS, log in to the vSphere Web Client, navigate to vCenter | 
Networking, and click on the Getting Started tab. At the bottom of the page, click 
on the link labeled Create a distributed switch. After naming your vDS, you will 
be given the option to select the vDS version; this allows backwards compatibility 
with other hosts in the cluster that may not be on the same version. Each version will 
specify the features available above and beyond the previous version. For example, 
with a vDS 5.5, there is an enhanced LACP support that is not available in a vDS 5.1.
Once you have selected the version, select how many uplinks should be added to the 
vDS, and whether to enable Network I/O Control and the name of the default port 
group. Once the vDS wizard finishes, the final step is to add hosts to the vDS and 
migrate any existing VMs.

Chapter 3
[ 67 ]
Network I/O Control enables resource pools to separate various types of traffic, 
ensuring they are provided a share of the available network resources. You set 
shares at the vDS level using the Resource Application tab. By default, NIOC adds 
several predefined types of traffic; however, you are free to add resource pools for 
your specific use cases as well. You can also set limits on a network resource pool, 
limiting the available amount of bandwidth available to it. As we discussed with 
limits earlier, limits are always in place, even if there is no contention, so shares are 
the preferred method of managing NIOC.
Select the newly created vDS and click on the Add and manage hosts link. Since this 
is a new switch, we will need to first click on Add hosts to add hosts to the vDS. On 
the select hosts screen, click on the Add button to do this. If all of your hosts do not 
appear, click on the Incompatible Hosts link at the top of the window. You will see 
incompatible hosts if any here. For example, if they are not licensed for Enterprise 
Plus or if they are running a version of ESXi not compatible with the vDS version 
you selected.
Once the hosts are added, click on next to select the options you wish to select. By 
default, it will add physical adapters and VMkernel adapters to the vDS. If you 
already have a vSS running with VMs connected, you can also select Migrate virtual 
machine networking to move the VM NICs to the vDS.
Select the adapters you wish to use for uplinks, and click on the Assign Uplink button 
to map the physical NIC to the vDS uplink. Next, you can choose whether to migrate 
the existing VMkernel adapters or create a new one. If you choose to create a new one, 
a new wizard will launch that will guide you through that process; otherwise, select 
the existing VMkernel adapter and click on the Assign port group button.
Next, you can choose which VMs you want to migrate to the vDS. Select the VM and 
click on the Assign port group button to select the desired port group. Finally, click 
on finish to add the host and any VMs to the vDS. While this sounds a bit scary, and 
it's certainly something you should test in your lab, VMware added a new feature in 
vSphere 5.1 called vSphere Network Rollback. If ESXi detects a misconfiguration, 
the task will fail and revert to the previous state. For more information on Network 
Rollback, see VMware KB 2032908 at http://kb.vmware.com/kb/2032908.
A new feature as of vSphere 5.1 is the ability to import, export, and restore vDS 
to correct errors. VMware KB 2034602 has more information on this at http://
kb.vmware.com/kb/2034602. One last option is the ability to restore networking 
through Direct Console User Interface (DCUI), the interface you interact with 
when a keyboard and monitor are directly connected to the ESXi host.

Advanced Resource Management Features
[ 68 ]
When using a vDS, be sure to have a good recovery plan for vCenter and its 
database. While each of the hosts will continue to operate if vCenter goes down, 
you will not be able to edit those vSwitches as all of the configuration is stored in 
the vCenter database. If you are using Microsoft SQL, see Chris Wahl's blog post 
on creating SQL maintenance plans at http://wahlnetwork.com/2012/04/06/
protecting-the-vcenter-sql-database-with-maintenance-plan-backups/.
New in vSphere 5.5
VMware announced two very exciting new features along with vSphere 5.5: vSphere 
Flash Read Cache and VSAN. Both of these features provide software solutions to 
common and complex problems in today's modern data center.
vSphere Flash Read Cache
VMware joined the ranks of new vendors such as Infinio, ProximalData, and 
PernixData who have all created server-side caching systems to improve storage 
performance by delivering hot data directly from the server instead of having to 
request it from the SAN.
vSphere Flash Read Cache (vFRC), as the name suggests, is a read cache that utilizes 
flash storage on a host to quickly serve read requests from VMs. The effects vFRC 
will have on your environment will be based on your workload; however, it stands 
to reason that any I/O that can be served from a local cache, rather than from your 
SAN, can help improve overall performance. The SAN does not have to service every 
read I/O because it is served from cache, and another I/O can process this directly 
from disk. If you move even just a few hundred IOPS to vFRC, that is a few hundred 
more, your SAN is able to serve beyond what it could do before vFRC.
Enabling vFRC has only a few requirements as follows:
•	
ESXi 5.5 is running on the host and is managed by vCenter 5.5
•	
Hosts are licensed for vSphere Enterprise Plus
•	
Hosts have local flash storage in your ESXi host that can be dedicated to 
vFRC (for example, it is not part of another datastore)
•	
VMs are using hardware compatibility mode v10 (5.5)
To enable vFRC, log in to the vSphere Web Client, navigate to vCenter | Hosts and 
Clusters, click on the ESXi host you wish to manage, and navigate to Manage | 
Settings | Virtual Flash Resource Management. Click on the Add Capacity button 
and select the SSD device(s) you wish to use for vFRC.

Chapter 3
[ 69 ]
Once vFRC is enabled on the host, you have to enable vFRC per VM (VMs do not 
automatically use VFRC). Right-click on the VM you wish to enable vFRC for and 
select Edit Settings. Click on the Virtual Hardware tab and then expand the hard 
disk(s) you wish to enable vFRC for and enter the amount of cache you wish to 
provide in the Virtual Flash Read Cache textbox.
While vFRC is a new feature, there is already some great content on it from VMware 
and within the community. For more information, check out the Performance of 
vSphere Flash Read Cache in VMware vSphere 5.5  technical white paper at http://
www.vmware.com/files/pdf/techpaper/vfrc-perf-vsphere55.pdf and the 
vSphere Flash Read Cache – Official FAQ white paper at http://blogs.vmware.com/
vsphere/2013/10/vsphere-flash-read-cache-official-faq.html.
To see vFRC in action, check out the #vBrownBag show by Nick Marshall on vFRC 
at http://professionalvmware.com/2013/09/vbrownbag-follow-up-nick-
marshall-nickmarshall9-covering-vsphere-5-5-vflash-read-cache/.
As always, the VMware vSphere 5.5 Documentation Center will have the most up-to-date 
information at http://pubs.vmware.com/vsphere-55/index.jsp#com.vmware.
vsphere.storage.doc/GUID-07ADB946-2337-4642-B660-34212F237E71.html.
VSAN
VSAN is an exciting new technology that was announced at VMworld in August 
2013. It is so new in fact that writing this book, it is still in beta stage.
VSAN creates a shared pool of storage from the local storage on each of your ESXi 
hosts. In order to ensure performance, each host that provides storage must have at 
least one SSD and for capacity, one HDD (traditional spinning disk). SSDs cannot 
be shared between VSAN and vFRC, they must be dedicated. An ESXi host can use 
VSAN storage presented from other hosts even if it is not providing storage to the 
VSAN cluster.
VSAN is easy to configure and has only a few requirements like vFRC:
•	
ESXi 5.5 is running on the host and managed by vCenter 5.5
•	
Each host must provide, at a minimum, 1 SSD and 1 HDD to contribute the 
storage to the VSAN cluster
•	
A VMkernel interface marked for Virtual SAN
At this time, it's believed that VSAN, while enabled in vCenter, will be licensed as 
a separate product and not included with any vSphere licenses, though this could 
always change in the future.

Advanced Resource Management Features
[ 70 ]
You can sign up for the beta version at http://www.vmware.com/vsan-beta-
register.html, and join the VSAN Beta community for news and updates at 
https://communities.vmware.com/community/vmtn/vsan.
VSAN is enabled at the cluster level like HA or DRS. To enable VSAN, log in to 
vSphere Web Client, navigate to vCenter | Hosts and Clusters, right-click on the 
cluster you wish to enable VSAN for, and click on Settings. Under Virtual SAN, 
select Settings and click on the Edit button.
Check the Turn ON Virtual SAN checkbox and choose how to provide disks to 
VSAN, either automatically or manually
•	
Automatically: This claims all empty disks on each host in the VSAN cluster
•	
Manual: With this, you select which disks are added
Because VSAN is so new, it would be best to use the VMware-provided resources 
as new information and features can be announced any time. You can learn more 
about VSAN in the What's New section in VMware Virtual SAN's technical white 
paper at http://www.vmware.com/files/pdf/products/vsan/VMware_Virtual_
SAN_Whats_New.pdf as well as on the VMware YouTube channel at http://www.
youtube.com/watch?v=cLfbeFoSQaI.
There is also a #vBrownBag podcast on VSAN at http://professionalvmware.
com/2013/09/vbrownbag-follow-up-nick-marshall-nickmarshall9-covering-
vsan/.
Summary
In this chapter, we looked at the various features available with VMware vSphere to 
not only provide resources to your VMs, but also ensure that your VMs are available 
even in the event of a host failure. We wrapped up with two new features made 
available in vSphere 5.5: vSphere Flash Read Cache and VSAN. In the next chapter, 
we will learn how to automate resource management through several features and 
tools available with VMware vSphere

Automation and  
Monitoring Options
While VMware vSphere provides several excellent features to enable you to take 
advantage of the computer, memory, disk, and network resources available, it has 
also added a new level of scale and management. There are several tools, some 
licensed and some free from VMware to help you automate the provisioning of  
new resources as well as monitoring the existing resources in your environment.
In this chapter, we will cover the following topics:
•	
The automation options available with vSphere for both VMs and hosts
•	
Examples of how to use the various automation tools to complete  
common tasks
•	
Monitoring options available from VMware
Most of the content of this chapter will focus on PowerCLI and the vCenter 
orchestrator, as they should be the focus going forward. The vSphere CLI, while  
still made available, is generally used for supporting legacy processes and scripts.

Automation and Monitoring Options
[ 72 ]
Automation solutions for vSphere
There are several tools that VMware makes available to vSphere administrators. 
These tools allow administrators to edit hosts or VM configuration settings and 
create scripts to automate routine tasks such as closing or creating a new VM. Some 
tools even provide a self-service style portal to end users so they can perform certain 
tasks without involving the IT/virtualization teams. In the following sections, we 
will focus on the application/features mentioned in the following table:
Application/feature
Description
Licensed with
PowerCLI
This provides a PowerShell 
snap-in to manage vSphere 
resources
This is free. It works with all 
vSphere editions and the free 
vSphere Hypervisor (ESXi)
vCenter Orchestrator 
(vCO)
This is a full-featured 
automation platform that 
includes a drag-and-drop 
interface for creating 
workflows
This is included in all licensed 
versions of vSphere and is 
provided with vCenter on 
Windows or as a separate 
appliance
There are, however, many other features that you can use to help automate specific 
tasks, which are mentioned in the following table:
Application/feature
Description
Licensed with
vSphere CLI (vCLI)
These are command-line 
utilities built into vSphere 
Management Appliance (vMA) 
or installed within an operating 
system that provide backward 
compatibility to older ESX 
commands.
This is free. It works with all 
vSphere editions and the free 
vSphere Hypervisor (ESXi)
Templates
This is a vSphere feature that 
allows you to store common 
VM types for easy deployment.
This is included under all 
licensed versions of vSphere
Cloning
This is a vSphere feature that 
allows you to copy existing 
VMs to create a new VM with 
the same settings and state.
This is included under all 
licensed versions of vSphere

Chapter 4
[ 73 ]
Application/feature
Description
Licensed with
Update Manager
This is a tool for managing 
updates on ESXi hosts or 
VMware tools installed on a 
guest OS.
This is included under all 
licensed versions of vSphere. 
It is provided with vCenter on 
Windows. If you are running 
the VCSA you will need a 
Windows server to install 
update manager.
Host profiles
This is a feature that allows you 
to specify the configuration 
options that should be applied 
to the specified hosts.
This is included under 
vSphere Enterprise Plus
Auto deploy
This is a feature that allows you 
to push ESXi to a host during 
boot time. You can choose a 
stateless installation, which 
runs in memory, or a stateful 
installation, which gets installed 
to a hard drive. Host profiles 
can then be used to ensure the 
configuration is intact.
This is included under 
vSphere Enterprise Plus
Automation is as much an art as it is a science. While some scripts and tools are 
likely to work in almost any environment, in most cases, you will need to apply 
your own business and technical requirements to your scripts and tools. Each of 
these tools require time to learn, time we don't have here; however, by looking at 
examples, you can most likely reverse engineer them to fit your needs. Let's look  
at some common tasks in the various tools. Let's take a quick look at some of the 
basic features before we get to PowerCLI and vCenter Orchestrator.
Cloning VMs
Cloning allows you to quickly and easily produce new VMs from an existing VM. 
When you clone an existing VM, it creates an exact copy of the original VM, which 
you can then reconfigure and power on. During the cloning process, a new MAC 
address will be provided to any present NIC to prevent the creation of duplicate 
MAC addresses. Further customization within the operating system would need 
to be accounted for before you make the clone. For example, in Windows, you may 
wish to run sysprep so it will boot and run through a mini setup to allow you to 
change the computer name in the operating system. In Linux operating systems, you 
may need to remove configuration files associated with the NIC to avoid conflicts or 
problems with NICs not being recognized.

Automation and Monitoring Options
[ 74 ]
You can create a clone from an existing VM to another VM, clone an existing VM  
to a template, or clone a template to a new VM.
VM templates
Templates are VMs stored in a different format that is comparable to how a 
Microsoft Word document template has an extension of .dotx, whereas a normal 
Word document uses .docx. Similarly, a normal VM configuration file uses a .vmx 
extension, whereas a template uses a .vmtx extension. An example given in the 
following screenshot shows the files used by a VM template:
The VMDK files for a normal VM and a VM you clone or convert into a template 
will roughly be of the same size. Using the previous example, it's only about 3 MB 
smaller than the VM we cloned into a template. You can still create new VMs from 
templates, similar to how you can clone a VM from an existing VM. However, 
templates cannot be powered, which in theory makes for a more secure baseline 
image to perform deployments from.
Even though templates cannot be powered on, you can convert a template back to 
a VM. So you can power it on and make changes to the application or operating 
system patches, for example; then, you can convert it back to a template. Don't forget 
any operating system-specific changes you need to make before converting it back to 
a template.
Another feature available to cloning, either from a VM or a template, is OS 
customization. OS customization is a vSphere feature similar to an answer file for 
Windows or a kickstart file for Linux. OS customization settings can be defined in 
vCenter and selected when deploying a template to provide control over operating 
system settings such as name, IP settings, and even domain membership. OS 
customization requires VMware tools to be installed in the VM. The VMware 
Virtual Machine Admin guide states that you need to have Perl installed for Linux 
distributions, but Perl is also required to install VMware tools. So it would have 
already been installed to support the VMware tools installation.

Chapter 4
[ 75 ]
Another form of a template is the Open Virtualization Format (OVF). If you have 
deployed any appliance from VMware such as the vCenter appliance, you would 
have most likely downloaded an Open Virtualization Appliance (OVA) file, which 
is the collection of all necessary files to deploy the VM from. You can also create your 
own OVF/OVA from an existing VM. If you are short on storage space or need to 
support other hypervisors in your environment, then storing your templates as an 
OVF may help. The amount of space you save may vary, but in testing, a 75 percent 
upwards reduction in storage utilization has been observed.
OVFs can also easily be deployed to other hypervisors or to local VMware 
applications such as VMware Workstation on Windows or VMware Fusion on OSX. 
You can find the official documentation on cloning, templates, and OS customization 
in the Virtual Machine Admin Guide available at http://pubs.vmware.com/
vsphere-55/topic/com.vmware.ICbase/PDF/vsphere-esxi-vcenter-server-
55-virtual-machine-admin-guide.pdf.
Update Manager
Update Manager is a tool that allows you to remediate patches for ESXi as well as 
older ESX versions and perform upgrades. Update Manager maintains information 
on all of the hosts and downloads updated patches, which can then be pushed 
to each of the hosts. Compare this to inserting a CD into every ESXi host in your 
environment, and you can quickly see that Update Manager is a must-have tool for 
maintaining a secure environment.
Since Update Manger downloads all of the patches needed within your environment, 
you will need to allocate sufficient amount of storage for patches and the Update 
Manager database. VMware has published a sizing estimator worksheet at http://
pubs.vmware.com/vsphere-55/topic/com.vmware.ICbase/PDF/vsphere-
update-manager-55-sizing-estimator.xls to help you properly size the Update 
Manager database and storage requirements giving specific details about your 
environment. Please note that Update Manager is a Windows-based application. It 
is not uncommon to find Update Manager installed on the same server as vCenter; 
however, you should install it on a separate server if possible.
If you are using the vCenter server appliance, you will need to install 
Update Manager on a separate server.
For more information about Update Manager, see the Update Manager Installation 
and Administration guide at http://pubs.vmware.com/vsphere-55/topic/com.
vmware.ICbase/PDF/vsphere-update-manager-55-install-administration-
guide.pdf.

Automation and Monitoring Options
[ 76 ]
Host profiles
Host profiles are a collection of configuration information about your ESXi hosts 
that you can use to quickly deploy new hosts with the same configuration settings 
as other hosts in your environment. It can also be used to ensure configuration 
settings are current and up to date. Host profiles are attached to a host, and vCenter 
then ensures the host is in compliance with the host profile settings. If a host is not 
in compliance, it can be remediated, bringing the settings back to the standards you 
have defined in your host profile.
You typically have a host profile to match a specific server build. For example, if you 
use Cisco UCS blade servers, you might have one host profile for blades with local 
storage and another host profile for blades that have no local storage. A host profile 
expecting local storage would report a host with no local storage as noncompliant.
Host profiles are created from an existing host, which is already configured with all 
of the configuration settings you want. Once the host profile is extracted from the 
reference host, you can attach the host profile to other hosts.
For more information on host profiles, see the vSphere host profiles documentation 
at http://pubs.vmware.com/vsphere-55/topic/com.vmware.ICbase/PDF/
vsphere-esxi-vcenter-server-55-host-profiles-guide.pdf.
Auto deploy
Auto deploy is a provisioning option for deploying ESXi hosts at boot time. There 
are two options when using auto deploy:
•	
Stateless: ESXi hosts use a Preboot Execution Environment (PXE) to request 
an image from the auto deploy server. Auto deploy pushes an ESXi image 
into the host's memory and runs ESXi in the memory as opposed to installing 
ESXi on the disk.
•	
Stateful: This feature installs ESXi over the network to a local disk in the 
server. Subsequent reboots of the server run from the local disk where ESXi 
was installed.

Chapter 4
[ 77 ]
Images are created using the vSphere ESXi image builder, a set of PowerCLI cmdlets 
used to create the image add third-party drivers to support your host. In addition, 
auto deploy relies heavily on host profiles because every time the server is restarted, 
ESXi is installed into memory. All configuration settings such as virtual switches, 
logfile settings, and other configurations are supplied through the host profile.
Auto deploy tools, such as host profiles, require vSphere Enterprise Plus. Complete 
documentation can be found in the VMware vSphere 5.5 Documentation Center 
at http://pubs.vmware.com/vsphere-55/index.jsp#com.vmware.vsphere.
install.doc/GUID-D0A72192-ED00-4A5D-970F-E44B1ED586C7.html.
PowerCLI basics
PowerCLI is a set of Windows PowerShell cmdlets; as such, you need to install 
PowerCLI on a Windows-based computer with PowerShell 2.0 or 3.0 and .NET 
2.0 SP2 or higher. Once PowerCLI is installed, you can start using it in place of the 
vSphere web client. The most useful and important command to remember is Get-
Help. The Get-Help command is used to get help on a specific command or even 
find commands. After Get-Help, the two most common types of commands are Get- 
and Set-. The Get- command will return some type of information and Set- will 
alter the configuration to the specified setting.
Say you want to get the name of the ESXi hosts connected to vCenter, but you are not 
sure of the command to do so. You could correctly assume that there is most likely a 
Get- command related to that. If you type Get-Help Get-*, it will return a list of all 
the Get- commands.

Automation and Monitoring Options
[ 78 ]
Scrolling through the list, you will see a command called Get-VMHost. Typing that 
command will return a list of all hosts connected to vCenter along with its power state, 
number of CPUs, and CPU usage. Compare that with logging into the vSphere web 
client, clicking on Hosts and Clusters, clicking on your cluster, clicking on the Related 
Objects tab, and clicking on the Hosts tab. Typing Get-VMHost is much quicker.
Also, using the published VMware vSphere 5.5 documentation would be much 
quicker than reading through all of the returned commands. A list of all PowerCLI 
cmdlets can be found at http://pubs.vmware.com/vsphere-55/index.
jsp#nav/2_2_1_0.
Now, let's consider a request to change the amount of memory in a VM from 4 GB to 
8 GB (memory hot plug is supported). By using a Set- command, in this case Set-
VM, you can change the amount of memory on your VM through one command:
Set-VM -VM <name_of_vm> -MemoryMB 8192
This is pretty quick compared to logging into the vSphere web client, clicking on 
VMs and templates, finding your VM, right-clicking on the VM, clicking on Edit 
Settings, changing the amount of memory, and clicking on the OK button.
As you can see, knowing just a few common PowerCLI cmdlets can save quite a bit 
of clicking around in the vSphere web client. What is really cool about PowerCLI 
though is its ability to take multiple commands and create reusable scripts.
vCenter Orchestrator basics
vCenter Orchestrator (vCO) is a tool for developing advanced workflows within 
your environment, and it provides thousands of sample workflows, which you can 
start leveraging right away to help you automate your environment. vCO could even 
be used as a self-service portal for your organization as it maintains a separate access 
control list outside vCenter.
vCO is deployed with vCenter on Windows or as a separate appliance-based VM. 
Once installed, you can access vCO within vCenter or through its own client, which 
supports Windows, OSX, or Linux.

Chapter 4
[ 79 ]
Out of the box, vCO provides thousands of workflows you can start using right  
away such as creating a new VM, deleting a VM, or cloning other VMs, as seen in  
the following screenshot:
The sample workflows provided are not editable; however, you can clone any 
existing workflow and edit it to meet your specific use case requirements.
With the out of the box workflows available with any licensed version of vSphere, 
there is really no reason to delay on automating routine tasks in your environment.
Automating resource management
In this section, we will look at how we can use PowerCLI and vCO to create a new 
VM. You will see how these automation tools can make these tasks easier and  
more consistent.

Automation and Monitoring Options
[ 80 ]
Creating a new VM
Creating new VMs is one of the most common tasks in vSphere and following the 
new VM wizard is quite easy. Let's look at an example request you may receive.
As the department responsible for your organizations in the vSphere environment, 
your team is asked to create new VMs for the development team. All development 
VMs must be created on the host specified in the request. They are in a folder named 
development for Windows Server 2012 with 2 vCPU, 8 GB of memory, a 40 GB of 
thin provisioned disk on the requested datastore. The Virtual Machine hardware 
version must be set to version 9, and add two VMXNET3 virtual NICs that are 
connected to the development port group and the VM Network port group.
For something as basic as creating a new VM, that's quite a list of requirements.  
If we were to do this simple task manually, you would do the following:
1.	 Log in to the vSphere web client.
2.	 Click on VMs and Templates.
3.	 Click on Create a new virtual machine.
4.	 Select Create new virtual machine and click on Next
5.	 Name the virtual machine, select the appropriate folder,  
and click on Next.
6.	 Select the appropriate host and click on Next.
7.	 Select the datastore cluster and click on Next.
8.	 Select the requested compatibility mode and click on Next.
9.	 Select the requested guest OS type and click on Next.
10.	 Conform or change the following options:
°°
The number of vCPU to 2
°°
The amount of memory to 8
°°
The hard disk size to 40; expand New Hard disk and select  
Thin provision
°°
Set the first virtual NIC to the development port group, expand  
New Network, and change the adapter type to VMXNET3
°°
Click on the pull-down option for New device, select the network, 
and click on the Add button
°°
Change the new virtual NIC to the VM network port group,  
expand New Network, and change the adapter type to VMXNET3
°°
Click on Next
11.	 Confirm your settings and click on Finish.

Chapter 4
[ 81 ]
That's about 33 mouse clicks, or as I like to think of it, 33 chances to get the request 
wrong. Thanks to the features and tools provided by VMware, we can cut down on 
all of those manual mouse clicks and typing.
Creating a new VM with PowerCLI
By leveraging PowerCLI, we can eliminate those manual steps and combine them 
into a single command as follows:
New-VM -Name dev-vm -Location development -VMHost esxi-host -Datastore 
esxi-host-datastore -Version v9 -GuestId windows8Server64Guest -NumCpu 
2 -MemoryMB 8192 -DiskMB 40960 -DiskStorageFormat Thin -NetworkName 
development, "VM Network"
By entering a single command, we were able to complete what 33 mouse clicks  
and some typing would have required. You might notice, however, a couple of 
problems with the preceding command.
Firstly, we did not specify the adapter type of VMXNET3, that here, one of the 
requirements for creating a new VM, because the New-VM cmdlet does not accept 
such a parameter. One of the great things about PowerCLI (and PowerShell for  
that matter) is its ability to pipe to other commands. Since we cannot change the 
network adapter with the New-VM cmdlet, we simply pipe to other cmdlets to  
achieve the requirement:
New-VM -Name dev-vm -Location development -VMHost esxi-host -Datastore 
esxi-host-datastore -Version v9 -GuestId windows8Server64Guest -NumCpu 
2 -MemoryMB 8192 -DiskMB 40960 -DiskStorageFormat Thin -NetworkName 
development, "VM Network" | Get-NetworAdapter | Set-NetworkAdamter -Type 
vmxnet3
Now, when the New-VM cmdlet finishes, the Get-NetworkAdapter cmdlet 
will retrieve the network adapters and set them to VMXNET3 using the Set-
NetworkAdapter cmdlet.
While the preceding command is useful, there are still a couple more problems. We 
have the name of the VM embedded within the command; once we create the VM, 
we cannot create another VM with the same name. The name of the ESXi host is also 
embedded, again not ideal as future requests might request an alternate host. Have no 
fear, PowerCLI's power also goes beyond using multiple cmdlets, and it also has the 
ability to combine multiple cmdlets into a script to achieve exactly what we want. Let's 
take a look at a script that meets all the requirements we described and can be reused 
because it will prompt for information that is likely to change from request to request:
$VMName = Read-host "Enter the name of the VM you wish to create"

Automation and Monitoring Options
[ 82 ]
#Lists host for the connected vCenter, prompts user to enter the desired 
host location for the VM
Get-VMHost | Format-Wide
$HOSTName = Read-host "Enter the Name of the Host you wish to create the 
VM on"
#Lists datastores for the connected vCenter, prompts user to enter the 
desired datastore location for the VM
Get-Datastore | Format-Wide
$DSName = Read-Host "Enter the name of the datastore you wish to create 
the VM on"
#Lists VM folders for the connected vCenter, prompts user to enter the 
desired folder location for the VM
Get-Folder | Format-Wide
$FOLDERName = Read-host "Enter the name of the folder you wish to create 
the VM in"
#Takes input from above steps and passes them to the New-VM command for 
VM creation
New-VM -Name $VMName -Location $FOLDER Name -VMHost $HOSTName -Datastore 
$DSName -Version v9 -GuestId windows8Server64Guest -NumCpu 2 -MemoryMB 
8192 -DiskMB 40960 -DiskStorageFormat Thin -NetworkName Development, "VM 
Network"
#Once VM creation is completed, VM network adapters are converted to 
vmxnet3
Get-VM $VMName | Get-NetworkAdapter | Set-NetworkAdapter -Type vmxnet3
You can see another example of piping to other cmdlets in the last line. We called the 
Get-VM #$VMName cmdlet (or you could type in a static name), then a pipe character 
(|) followed by another cmdlet. So long as the cmdlet can be piped (some cannot), 
you can string many of these together into a single command.
This example, while basic, uses several commands to display information about 
the environment, ask a user for input, perform an action, and make changes to an 
existing resource. You should also note that this example is not meant to reflect best 
practices, or even efficiency, rather it is a bare bones PowerCLI example of how 
multiple commands can be used within a script to perform common tasks.
You can find several sample scripts in the VMware vSphere Documentation Center 
at http://pubs.vmware.com/vsphere-55/index.jsp#com.vmware.powercli.
ug.doc/GUID-3E36F4EA-3742-48BA-BB4B-7E0A2EAAE83E.html.

Chapter 4
[ 83 ]
Creating a new VM with vCO
Let's look at what a vCO workflow looks like to create a new VM. There are two 
built-in workflows ready to go out of the box with vCO: Create simple virtual 
machine and Create custom virtual machine.
A workflow consists of several parts:
•	
Inputs: This is similar to what we used in our PowerCLI script to prompt for 
information
•	
Outputs: The expected outcome of the workflow, in this case, a new VM
•	
Schema: The steps used to move from start to finish
Here is the schema for the Create simple virtual machine work flow. Each of the 
icons in the workflow shown in the following figure represents an available action 
in vCO, which you can use to create your own workflows, though, there are many 
more than this.
Create ConfigSpec
vim3WaitTaskEnd
This workflow will look a bit different if you are following along with vCO at 
home so that we could fit the workflow into a smaller space. The task starts, collects 
information, runs through the process, and then ends. Now, let's run the workflow 
and see what it actually does.
1.	 Select the workflow in the vCO client and click on the green play button.
2.	 A form appears asking for the VM name, VM folder, size of the hard drive, 
memory, number of vCPU, guest OS type, and whether to thin provision.
3.	 Fill in all the information and click on Next. Note that if you click on a link 
and nothing appears, type something into the filter box. This is a quirk of 
vCO, one I hope they fix in a future release. For example, when you click in 
the Virtual machine guest OS box, an empty window appears. Type Windows 
in the filter box on the top-right corner, and it will display the matching 
items. Select your desired OS and click on the Select button.
4.	 Select the server, resource pool, network, and datastore and click on the 
Submit button.

Automation and Monitoring Options
[ 84 ]
The vCO workflow will take the input and create a new VM with the selected 
settings. If you have not created any resource pools in your environment, which is 
a required input of this workflow, expand Hosts | Cluster and you will see a pool 
called Resources, which you can select.
You can see in the following figure how the workflow takes place and creates  
my new VM.
vim3WaitTaskEnd
Create ConfigSpec
As you can see, vCO can be quite powerful even if you are using out of the box 
workflows. In addition to the default workflows available, or those that you 
can create, VMware has created a site at https://solutionexchange.vmware.
com where you can download additional workflows. The VMware vSphere 
documentation center, as you might expect, has information on installing, creating, 
and securing vCO at http://pubs.vmware.com/vsphere-55/index.jsp.
Community automation resources
While the technology that VMware vSphere provides is excellent, one of the things 
that really sets it apart is not only the technology, but the communities they have 
helped to build it. These are people helping one another through the official VMware 
communities forum, individual blogs, and other social media outlets. Here are some 
great free examples and community resources related to automation:

Chapter 4
[ 85 ]
VMware education services (www.vmwarelearning.com) is a website for free 
instructional videos on many virtualization topics, which include videos on vCenter 
Orchestrator.
VMware VMTN community refers to the official VMware community sites that include 
the vCO and PowerCLI communities, which are available at the following links:
•	
https://communities.vmware.com/welcome
•	
https://communities.vmware.com/community/vmtn/vcenter/
orchestrator
•	
https://communities.vmware.com/community/vmtn/automationtools/
powercli
Above and beyond the official VMware resources, there are several great user-
contributed sites as well.
Alan Renouf (https://twitter.com/alanrenouf) publishes several useful 
PowerCLI scripts at http://www.virtu-al.net. He also publishes one-liners, 
which are short and typically piped PowerCLI commands to perform specific tasks 
(http://www.virtu-al.net/2009/07/07/powercli-more-one-liner-power/). 
For example, he has a one liner that will help you determine which VM has a specific 
MAC address; this is useful if you are performing any network troubleshooting:
Get-vm | Select Name, @{N="Network";E={$_ | Get-networkAdapter | ? {$_.
macaddress -eq "00:50:56:A1:50:43"}}} |Where {$_.Network-ne ""}
Matt Vogt (https://twitter.com/mattvogt), who blogs at http://blog.
mattvogt.net, publishes articles on various virtualization topics. What makes the 
VMware community great is that you are likely to find others working on similar 
projects or who have run into similar problems. Matt published a blog post called 
VMs Grayed Out After NFS Datastore Restored, which was a modified PowerCLI script 
from another VMware community member Raphale Schitz. It will search vCenter for 
VMs that are marked as inaccessible and reconnect them.
This particular script saved me quite a bit of time after performing maintenance 
on a NAS servicing one of my vSphere clusters. I needed to shut down all of the 
VMs to carry out this maintenance; however, due to the time gap, the NAS was 
disconnected, the VMs were not able to reconnect, something I had not accounted 
for. This script is a great example of how scripting and automation can save time 
and how helpful the members of the VMware community are. You can find this 
script at http://blog.mattvogt.net/2013/02/08/vms-grayed-out-after-nfs-
datastore-restored/.

Automation and Monitoring Options
[ 86 ]
If you are looking for some community-contributed content on vCO, check out 
Mike Preston's blog at http://blog.mwpreston.net, where he blogs on various 
virtualization topics. In addition to vCO-related posts at http://blog.mwpreston.
net/tag/vco/, he has also recently published a series on the VCAP-DCA exam 
where he reviews lesser known features of vSphere that are part of the blueprint and 
has a great VCP study guide.
The last site featured here, which we have referenced before, is http://
professionalvmware.com, a group of community members who host podcasts 
on various virtualization topics. There were a series of podcasts dedicated to 
automation to wrap up 2013, which included PowerCLI, vCloud Automation 
Center, and vCO. You can find the podcasts at http://professionalvmware.com/
vbrownbag-automate-all-the-things-training-schedule/ or on their official 
iTunes channel.
This is certainly not all of the community-contributed content. You should make 
it a point to get involved with the virtualization community as it provides a great 
opportunity to learn and connect with others.
Available monitoring options
There are plenty of monitoring tools in the market; the need to monitor your servers 
and infrastructure was needed well before virtualization. However, VMware does 
have tools that can help you track the health of your environment and even help 
with forecasting.
In the following sections, we will look at the application/feature mentioned in the 
following table:
Application/feature
Description
Licensed with
Alarms
This provides the ability to 
monitor hosts and VMs for 
certain alerts.
This is included in all licensed 
editions of vSphere.
vCenter Operations 
Manager
This provides the ability 
to monitor hosts, VMs and 
datastores and providing 
trending and analysis
This product is including the 
vCenter Operations Manager 
Suite or as a separate product

Chapter 4
[ 87 ]
Alarms
Alarms are a basic but useful way to provide alerts on specific environment conditions 
in vCenter. Alarms can provide us with alerts on both VM-and host-level issues, 
including configuration problems. Since they are included in vCenter, there is 
generally little reason for not configuring some alarms.
You should avoid over-monitoring or having too many of the same 
processes monitored so you are not overwhelmed by alert e-mails.
Alarms maintain three states: green, yellow, and red. Compare this with Nagios,  
a popular open source monitoring application that would be the equivalent of OK, 
warning, and critical. After vCenter, the only requirement for alarms is to set up 
SMTP, assuming you want e-mail alerts. You could set up alarms with no SMTP 
server configured, but you would need to log in to the vSphere web client to see  
the alarms and might miss something important.
Assuming you have an accessible SMTP server, all you need to do to configure SMTP 
is log in to the vSphere web client, navigate to vCenter | vCenter Servers, select 
your vCenter server, click on the Manage tab, and then click on the Edit button. 
Click on Mail and fill in the Mail server text with the address of your SMTP server 
and Mail sender textbox with the from address.
You can see the alarm definitions available under the Manage tab for any object 
managed by vCenter. Although some, such as alarms for VMs, may have fewer 
alarms than a host. You wouldn't configure an alarm to monitor a host failure on  
a VM, for example.
Let's configure an alarm to notify us if one of our vSwitches does not have a 
redundant uplink (something that might be hard to monitor in third-party 
monitoring tools).
Configuring an alarm
Configuring alarms is very straightforward and should be done in most situations.
1.	 Log in to the vSphere Web Client
2.	 Navigate to vCenter | vCenter Servers and select the vCenter server you 
wish to configure alerts on.
3.	 Click on the Manage tab and then click on Alarm Definitions.
4.	 Select Network uplink redundancy lost and click on the Edit button.

Automation and Monitoring Options
[ 88 ]
5.	 On the General screen, you can only edit a few items; however, in this case, 
you will most likely want to leave it to the host to monitor the items so you can 
monitor a specific event, which we will configure next. Click on the Next button.
6.	 By default, this alarm ships with a few options that are preconfigured. 
Refer to the following screenshot; it will trigger an alert for Lost Network 
Redundancy and Lost Network Redundancy on DVPorts.
7.	 Here, you could add additional Triggers if you wish:
°°
Click on the green + icon and a new field will be added to the list of 
events.
°°
Click on the pull-down menu to scroll through the available events 
and select Network Redundancy Degraded, which ensures the 
Status is set to Alert.
8.	 Whether you add additional triggers or use defaults, click on the  
Next button.
9.	 The Actions screen is where you can configure how you are notified. You'll 
notice four columns for the options: OK to warning, warning to critical, critical 
to warning, and warning to OK.

Chapter 4
[ 89 ]
10.	 Click on the green + icon, and it will add the Send a notification email 
option. Click on the field for which you wish to be notified. For example, if 
you want an e-mail notification when it goes from OK to warning, click on 
that field and select either Once or Repeat and click on Finish.
You can also manage alarms through PowerCLI; there is a great blog post on 
the available cmdlets at http://blogs.vmware.com/vipowershell/2010/12/
managing-vsphere-alarms-with-powercli.html.
This is just one example of alarms; there are many you can configure to keep you 
informed about your environment. Planning which alarms you can use via vCenter 
and which you can manage through other monitoring tools requires some thought so 
you don't receive multiple alerts from a single event.
vCenter Operations Manager
vCenter Operations Manager, commonly referred to as vCOPS, is an advanced 
monitoring tool for vSphere environments. vCOPS is available in several editions. 
If you are licensed for vCenter, then at the very least, you have access to the vCOPS 
foundation edition. This includes monitoring, alerts, and self-learning analytics that 
understand how your environment functions. To understand how self-learning 
analytics work, let's consider a basic example.
Every night you run backups (or you should!), and the VM running on your backup 
software will typically have increased CPU utilization during the backup window, 
which is expected and normal for that workload. A typical monitoring application 
would not be aware of this behavior on its own; you would have to manually 
configure an exception or maintain a window where the software would not trigger 
an alert. The self-learning alerts in vCOPS would understand that this is normal and 
would not trigger an alert during the time window where this increase occurs.
vCOPS is deployed as an OVA (virtual appliance); there are just a few requirements. 
During the OVA deployment, you will be asked which server to deploy to, as you 
might expect; however, you won't be able to proceed without enabling DRS. If you 
are not licensed for Enterprise Plus, you will need to remove a host from the cluster. 
In order to deploy the OVA, you need to have a standalone host (refer to http://
kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=dis
playKC&externalId=2013695).
You will be prompted to select a configuration size. Each of these settings  
defines how many resources will be assigned to the VMs so that they can  
support your environment.
•	
Small (less than 1500 VMs): 4 vCPU and 16 GB of RAM

Automation and Monitoring Options
[ 90 ]
•	
Medium (1500 to 3000 VMs): 8 vCPU and 25 GB of RAM
•	
Large (greater than 3000 VMs): 16 vCPU and 24 GB of RAM
You can download the OVA from my.vmware.com and deploy any appliance in 
your environment. You can find full details on the installation at https://www.
vmware.com/pdf/vcops-vapp-57-deploy-guide.pdf or a quick start install guide 
I published at http://www.virtxpert.com/deploying-vcenter-operations-
manager-appliance/.
Once you have vCOPS installed and configured, you are able to view the health 
of your environment very easily. vCOPS factors all of the various parts of your 
environment into a world and provides a score based on the various factors in your 
environment. As you can see in the following screenshot, I have a score of 69, which 
vCOPS classifies as abnormal versus good (green) or degraded (orange).

Chapter 4
[ 91 ]
As you can see from the preceding screenshot, vCOPS further breaks down the 
various components that make up the WORLD. You can quickly see that one of my 
custom groups is in a bad (red) state and that several of my datastores are degraded 
and only one is bad.
If you double-click on the red icon in the Datastores section, it will bring you to the 
associated datastore and give you an overview of its health. As you can see here, 
my datastore was classified as bad likely because I have very little free disk space; 
otherwise, the I/O on that datastore is fine.
Summary
In this chapter, we looked at how automation can reduce the amount of effort to 
manage your VMware vSphere environment with the tools that VMware makes 
available: vCenter Orchestrator and PowerCLI. We also reviewed two monitoring 
options: alarms, which are built into vCenter, and vCenter operations manager, 
which comes in both a free foundation edition and advanced editions that require 
additional licensing.


Index
Symbols
%CSTP  28
%RDY  28
%RUN  28
%WAIT  28
A
admission control  57, 58
Alan Renouf
URL  85
Alarms
about  86, 87
configuring  87-89
App HA
about  59
URL, for documentation  59
auto deploy
about  6, 73, 76
Stateful option  76
Stateless option  76
automation solutions, vSphere  72, 73
B
ballooning  31, 32
C
Cloning  72, 73
cluster maximum  15
community automation resources  84, 86
components, vCenter  9, 10
installation, prerequisites  10
configuration maximums,  
vSphere environment
cluster maximum  15
ESXi host maximum  14, 15
VM configuration maximum  14
CPU load average  28
CPU power management  46, 47
CPU %RDY  46
CPU scheduler
URL  25
CPU scheduling  24-28
D
Datastore clusters  6-64
DAVG  41
Direct Console User Interface (DCUI)  67
Distributed Power Management (DPM)  54
Distributed Resource Scheduler (DRS)
about  12, 54, 55
Fully Automated  55
Manual automation  55
Partially Automated  55
DRS affinity rule
host-based rule  55
VM-based rule  55, 56
DRS anti-affinity rule
host based rule  55, 56
VM-based rule  55, 56
E
Enhanced vMotion Capability. See  EVC
ESXi
about  6
system requirements  6-8
ESXi 5.5
resource requirements  6, 7
ESXi host maximum  14, 15

[ 94 ]
ESXTOP
about  28
network connectivity, monitoring  43
EVC
about  52
enabling  53
F
Fault Tolerance
about  60
requirements  60
Fiber Channel (FC)  39
Fiber Channel over Ethernet (FCoE)  39
G
GAVG  41
Get- command  77
H
High Availability (HA)
about  12, 56
admission control  57, 58
host profile
about  73, 76
URL, for documentation  76
Hot Add  61
Hyper-Threading (HT)  24
hypervisors
about  6
type 1 hypervisors  6
type 2 hypervisors  6
I
Infiniband  39
I/O per Second (IOPS)  36
J
JBOD (Just a Bunch Of Disks)  36
K
KAVG  41
L
Linux
resource utilization statistics,  
collecting  20, 21
LUN  37
M
Matt Vogt
URL  85
memory assignment  29
memory compression  30
memory management
about  29
ballooning  31, 32
memory compression  30
memory overhead  29
transparent page sharing (TPS)  30
memory overhead  29, 30
memory usage
monitoring  34, 35
monitoring options, vSphere
about  86
Alarms  87
vCenter Operations Manager  89-91
Multi-NIC vMotion  52
N
network connectivity
monitoring, with ESXTOP  43
networking  41
Network IO Control (NIOC)  65
O
Open Virtualization Appliance (OVA)  75
Open Virtualization Format (OVF)  75
overcommitment, vSphere
managing  23, 24
P
PCPU USED (%)  28
PowerCLI
about  72, 77, 78

[ 95 ]
used, for creating VM  81, 82
PowerCLI cmdlets
URL  78
Preboot Execution Environment (PXE)  76
R
redundant array of independent  
disks (RAID)  36-38
reservation  47
resource limits  48
resource management automation
VM, creating  80, 81
VM, creating with PowerCLI  81, 82
VM, creating with vCO  83, 84
resource pool  49
resource shares  48
resource utilization requirements
determining  16, 17
sample workload  18
resource utilization statistics
collecting, on Linux  20, 21
collecting, on Windows  18, 19
monitoring  17
S
sar -b command  21
sar command  17
sar -r command  21
Service Level Agreements (SLA)  5
Set- command  77
SQL
URL, for documentation  10
Stateful option, auto deploy  76
Stateless option, auto deploy  76
storage
about  35
IOPS  36
monitoring  41
RAID  36-38
Storage DRS
about  61, 64
Fully Automated mode  64
No Automation mode  64
storage requirements
determining  38
Storage vMotion  61, 62
T
Templates  72, 74
Total Cost of Ownership (TCO)  5
transparent page sharing (TPS)  30
U
Update Manager
about  73, 75
URL, for installation guide  75
uplink
about  42
requirements  42
V
VAAI
about  38
connectivity  39
network connectivity, monitoring with 
ESXTOP  43
networking  41
storage, monitoring  41
throughput  39
uplink  42
URL  38
VM disk provisioning  40
VMFS  39, 40
vSwitch  43
vApps  49
vCenter
about  9
components  9, 10
foundation edition  9
standard edition  9
vCenter foundation edition  9
vCenter Operations Manager  89-91
vCenter Orchestrator (vCO)
about  72, 78, 79
used, for creating VM  83, 84
vCenter standard edition  9
Virtual Desktop Infrastructure (VDI)  13
virtualization  23, 24
Virtual Machine File System. See  VMFS
Virtual Servers (VMs)  6
VM
creating  80, 81

[ 96 ]
creating, with PowerCLI  81, 82
creating, with vCO  83, 84
VM configuration maximum  14
VM disk provisioning  40
VMFS  39, 40
VM kernel traffic  42
VM network traffic  42
vMotion  50, 51
VMware Community
URL  8, 28
VMware Compatibility Guide
URL  7
VMware KB 1003661
URL  6
VMware KB 1024051
URL  56
VMware KB 2001003
URL  28
VMware KB 2032908
URL  67
VMware KB 2034602
URL  67
VMware KB 2052329
URL  6
vmwarelearning
URL  85
VMware Professional Services Organization 
(PSO)  17
VMware Resource Management guide
URL  30
VMware Tools  31, 47
VMware VMTN community
URL  85
VMware vSphere Documentation Center 
URL  82
VMware vSphere Hypervisor. See  ESXi
VMware vSphere Storage APIs Array Inte-
gration. See  VAAI
VSAN
about  69, 70
requirements  69
vSphere
automation solutions  72, 73
CPU power management  46, 47
Enterprise edition  11
Enterprise Plus edition  11
monitoring options  86
overcommitment, managing  23, 24
Standard edition  11
vSphere 5.1  9, 52
vSphere 5.5
features  68
URL, for configuration maximum  15
vSphere 5.5 Documentation Center
URL  77
vSphere Availability
URL  60
vSphere CLI (vCLI)  72
vSphere cluster  12
vSphere Configuration Maximums guide
URL  9
vSphere data center  12
vSphere Distributed Switches (vDS)
about  65- 67
advantages  65
vSphere Enterprise edition
features  11
vSphere Enterprise Plus edition
features  11
vSphere environment
configuration maximums  14
resource utilization requirements, deter-
mining  16, 17
vSphere cluster  12
vSphere data center  12
vSphere host  13
vSphere Flash Read Cache (vFRC)
about  68, 69
requirements  68
URL  69
vSphere host  13
vSphere Network Rollback  67
vSphere Standard edition
features  11
vSwitch  43
VSWP swap file
about  33
memory usage, monitoring  34, 35
W
Windows
resource utilization statistics, collecting  18, 
19

 
Thank you for buying  
VMware vSphere Resource Management Essentials
About Packt Publishing
Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective 
MySQL Management" in April 2004 and subsequently continued to specialize in publishing 
highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting 
and customizing today's systems, applications, and frameworks. Our solution based books 
give you the knowledge and power to customize the software and technologies you're using 
to get the job done. Packt books are more specific and less general than the IT books you have 
seen in the past. Our unique business model allows us to bring you more focused information, 
giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, 
cutting-edge books for communities of developers, administrators, and newbies alike. For 
more information, please visit our website: www.packtpub.com.
About Packt Enterprise
In 2010, Packt launched two new brands, Packt Enterprise and Packt Open Source, in order to 
continue its focus on specialization. This book is part of the Packt Enterprise brand, home to 
books published on enterprise software – software created by major vendors, including (but 
not limited to) IBM, Microsoft and Oracle, often for use in other corporations. Its titles will offer 
information relevant to a range of users of this software, including administrators, developers, 
architects, and end users.
Writing for Packt
We welcome all inquiries from people who are interested in authoring. Book proposals 
should be sent to author@packtpub.com. If your book idea is still at an early stage and you 
would like to discuss it first before writing a formal book proposal, contact us; one of our 
commissioning editors will get in touch with you. 
We're not just looking for published authors; if you have strong technical skills but no writing 
experience, our experienced editors can help you develop a writing career, or simply get some 
additional reward for your expertise.

vSphere High Performance 
Cookbook
ISBN: 978-1-78217-000-6             Paperback: 240  pages
Over 60 recipes to help you improve vSphere 
performance and solve problems before they arise
1.	
Troubleshoot real-world vSphere performance 
issues and identify their root causes
2.	
Design and configure CPU, memory, 
networking, and storage for better and more 
reliable performance
3.	
Comprehensive coverage of performance issues 
and solutions, including vCenter Server design 
and virtual machine, and application tuning
Troubleshooting vSphere Storage
ISBN: 978-1-78217-206-2             Paperback: 150  pages
Become a master at troubleshooting and solving 
common storage issues in your vSphere environment
1.	
Identify key issues that affect vSphere storage 
visibility, performance, and capacity
2.	
Comprehend the storage metrics and statistics 
that are collected in vSphere
3.	
Get acquainted with the many vSphere features 
that can proactively protect your environment
Please check www.PacktPub.com for information on our titles

VMware vSphere 5.1 Cookbook
ISBN: 978-1-84968-402-6             Paperback: 466  pages
Over 130 task-oriented recipes to install, configure, 
and manage various vSphere 5.1 components
1.	
Install and configure vSphere 5.1 core 
components
2.	
Learn important aspects of vSphere such as 
administration, security, and performance
3.	
Configure vSphere Management Assistant 
(VMA) to run commands/scripts without the 
need to authenticate every attempt
Instant VMware Player for 
Virtualization 
ISBN: 978-1-84968-984-7            Paperback: 84  pages
A simple approach towards learning virtualization to 
play with virtual machines
1.	
Learn something new in an Instant! A short, 
fast, focused guide delivering immediate 
results
2.	
Discover the latest features of VMware  
Player 5.0
3.	
Evaluate new technologies without  
paying for additional hardware costs
4.	
Test your applications in an  
isolated environment
 
Please check www.PacktPub.com for information on our titles


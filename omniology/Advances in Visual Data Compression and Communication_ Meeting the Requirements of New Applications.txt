Series in Multimedia Computing, Communication and Intelligence
Advances in Visual
Data Compression
and Communication
Meeting the Requirements of New Applications
Feng Wu

Advances in Visual
Data Compression
and Communication
Meeting the Requirements
of New Applications

Multimedia Computing, Communication and Intelligence
Series Editors: Chang Wen Chen and Shiguo Lian
Advances in Visual Data Compression and Communication: Meeting the 
Requirements of New Applications 
Feng Wu
ISBN: 978-1-4822-3413-8
Effective Surveillance for Homeland Security: Balancing Technology and
Social Issues 
Edited by Francesco Flammini, Roberto Setola, and Giorgio Franceschetti
ISBN: 978-1-4398-8324-2 
Music Emotion Recognition 
Yi-Hsuan Yang and Homer H. Chen
ISBN: 978-1-4398-5046-6 
Optimal Resource Allocation for Distributed Video and Multimedia
Communications 
Yifeng He, Ling Guan, and Wenwu Zhu
ISBN: 978-1-4398-7514-8
TV Content Analysis: Techniques and Applications 
Edited by Yiannis Kompatsiaris, Bernard Merialdo, and Shiguo Lian
ISBN: 978-1-4398-5560-7

Advances in Visual
Data Compression
and Communication
Meeting the Requirements
of New Applications
Feng Wu

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2015 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Version Date: 20140501
International Standard Book Number-13: 978-1-4822-3415-2 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts 
have been made to publish reliable data and information, but the author and publisher cannot assume 
responsibility for the validity of all materials or the consequences of their use. The authors and publishers 
have attempted to trace the copyright holders of all material reproduced in this publication and apologize to 
copyright holders if permission to publish in this form has not been obtained. If any copyright material has 
not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmit-
ted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, 
including photocopying, microfilming, and recording, or in any information storage or retrieval system, 
without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.
com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood 
Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and 
registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, 
a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used 
only for identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

To the 15 years I spent with Microsoft
Research Asia.


Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxv
Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxix
Part I Basis for Compression and Communication
1
Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Source Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.1
Huffman Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.2.2
Arithmetic Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.2.3
Rate Distortion Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.3
Channel Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.3.1
Capacity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.3.2
Coding Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.3.3
Hamming Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.4
Joint Source and Channel Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2
Hybrid Video Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.1
Hybrid Coding Framework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2
Technical Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2.1
H.261 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2.2
MPEG-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2.3
MPEG-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.2.4
MPEG-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.2.5
H.264/MPEG-4 AVC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.2.6
HEVC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.2.7
Performance versus Encoding Complexity . . . . . . . . . . . . . . .
31
2.3
H.264 Standard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
vii

viii
Contents
2.3.1
Motion Compensation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.3.2
Intra Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.3.3
Transform and Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.3.4
Entropy Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.3.5
Deblocking Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.3.6
Rate Distortion Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.4
HEVC Standard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.4.1
Motion Compensation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.4.2
Intra Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.4.3
Transform and Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.4.4
Sample Adaptive Offset Filter . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3
Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.1
Analog Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.1.1
Analog Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.1.2
Multiplexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2
Digital Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.2.1
Low-Density Parity-Check (LDPC) Codes . . . . . . . . . . . . . . .
51
3.2.2
Turbo Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.2.3
Digital Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
Part II Scalable Video Coding
4
Progressive Fine Granularity Scalable (PFGS) Coding . . . . . . . . . . . . .
65
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.2
Fine Granularity Scalable Video Coding . . . . . . . . . . . . . . . . . . . . . . .
66
4.3
Basic PFGS Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.3.1
Basic Ideas to Build the PFGS Framework . . . . . . . . . . . . . . .
70
4.3.2
The Simpliﬁed PFGS Framework . . . . . . . . . . . . . . . . . . . . . . .
72
4.4
Improvements to the PFGS Framework . . . . . . . . . . . . . . . . . . . . . . . .
73
4.4.1
Potential Coding Inefﬁciency Due to Two References . . . . . .
73
4.4.2
A More Efﬁcient PFGS Framework . . . . . . . . . . . . . . . . . . . . .
76
4.5
Implementation of the PFGS Encoder and Decoder . . . . . . . . . . . . . .
79
4.6
Experimental Results and Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4.7
Simulation of Streaming PFGS Video over Wireless Channels . . . . .
85
4.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
5
Motion Threading for 3D Wavelet Coding. . . . . . . . . . . . . . . . . . . . . . . . .
91
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
5.2
Motion Threading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
5.3
Advanced Motion Threading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
5.3.1
Lifting-Based Motion Threading . . . . . . . . . . . . . . . . . . . . . . .
94
5.3.2
Many-to-One Mapping and Non-Referred Pixels . . . . . . . . . .
97
5.4
Multi-Layer Motion Threading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98

Contents
ix
5.5
Correlated Motion Estimation with R-D Optimization . . . . . . . . . . . . 101
5.5.1
Deﬁnition of the Mode Types . . . . . . . . . . . . . . . . . . . . . . . . . . 102
5.5.2
R-D Optimized Mode Decision . . . . . . . . . . . . . . . . . . . . . . . . 104
5.6
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.6.1
Coding Performance Comparison. . . . . . . . . . . . . . . . . . . . . . . 105
5.6.2
Macroblock Mode Distribution . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6
Barbell-Lifting Based 3D Wavelet Coding . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.2
Barbell-Lifting Coding Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.2.1
Barbell Lifting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
6.2.2
Layered Motion Coding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6.2.3
Entropy Coding in Brief . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
6.2.4
Base Layer Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.3
Comparisons with SVC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.3.1
Coding Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.3.2
Temporal Decorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6.3.3
Spatial Scalability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
6.3.4
Intra Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.4
Advances in 3D Wavelet Video Coding . . . . . . . . . . . . . . . . . . . . . . . . 123
6.4.1
In-Scale MCTF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.4.2
Subband Adaptive MCTF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
6.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.5.1
Comparison with Motion Compensated Embedded Zero
Block Coding (MC-EZBC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.5.2
Comparison with Scalable Video Coding (SVC) for
Signal-to-Noise Ratio (SNR) Scalability . . . . . . . . . . . . . . . . . 129
6.5.3
Comparison with SVC for Combined Scalability . . . . . . . . . . 130
6.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
Part III Directional Transforms
7
Directional Wavelet Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
7.2
2D Wavelet Transform via Adaptive Directional Lifting . . . . . . . . . . 138
7.2.1
ADL Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
7.2.2
Subpixel Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.3
R-D Optimized Segmentation for ADL . . . . . . . . . . . . . . . . . . . . . . . . 144
7.4
Experimental Results and Observations . . . . . . . . . . . . . . . . . . . . . . . . 145
7.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

x
Contents
8
Directional DCT Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.2
Lifting-Based Directional DCT-Like Transform . . . . . . . . . . . . . . . . . 154
8.2.1
Lifting Structure of Discrete Cosine Transform (DCT) . . . . . 154
8.2.2
Directional DCT-Like Transform . . . . . . . . . . . . . . . . . . . . . . . 157
8.2.3
Comparison with Rotated DCT. . . . . . . . . . . . . . . . . . . . . . . . . 159
8.3
Image Coding with Proposed Directional Transform . . . . . . . . . . . . . 161
8.3.1
Direction Transition on Block Boundary . . . . . . . . . . . . . . . . . 162
8.3.2
Direction Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
8.4
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
8.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
9
Directional Filtering Transform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
9.2
Adaptive Directional Lifting-Based 2D Wavelet Transform . . . . . . . . 175
9.3
Mathematical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
9.3.1
Coding Gain of ADL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
9.3.2
Numerical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
9.4
Directional Filtering Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
9.4.1
Proposed Intra-Coding Scheme . . . . . . . . . . . . . . . . . . . . . . . . 185
9.4.2
Directional Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
9.4.3
Optional Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
9.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
9.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
Part IV Vision-Based Compression
10
Edge-Based Inpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
10.2 The Proposed Framework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
10.3 Edge Extraction and Exemplar Selection . . . . . . . . . . . . . . . . . . . . . . . 201
10.3.1 Edge Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
10.3.2 Exemplar Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
10.4 Edge-Based Image Inpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
10.4.1 Structure Propagation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
10.4.2 Texture Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
10.5 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
10.5.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
10.5.2 Test Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
10.5.3 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219

Contents
xi
11
Cloud-Based Image Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
11.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
11.2.1 Visual Content Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
11.2.2 Local Feature Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
11.2.3 Image Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
11.3 The Proposed SIFT-Based Image Coding . . . . . . . . . . . . . . . . . . . . . . . 225
11.4 Extraction of Image Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
11.5 Compression of Image Descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.5.1 Prediction Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
11.5.2 Compression of SIFT Descriptors . . . . . . . . . . . . . . . . . . . . . . 230
11.6 Image Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
11.6.1 Patch Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
11.6.2 Patch Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
11.6.3 Patch Stitching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
11.7 Experimental Results and Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
11.7.1 Compression Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
11.7.2 Visual Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
11.7.3 Highly Correlated Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
11.7.4 Complexity Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
11.7.5 Comparison with SIFT Feature Vector Coding . . . . . . . . . . . . 240
11.8 Further Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
11.8.1 Typical Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
11.8.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
11.8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
11.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
12
Compression for Cloud Photo Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
12.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
12.2.1 Image Set Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
12.2.2 Local Feature Descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
12.3 Proposed Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
12.4 Feature-Based Prediction Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
12.4.1 Graph Building . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
12.4.2 Feature-Based Minimum Spanning Tree . . . . . . . . . . . . . . . . . 252
12.4.3 Prediction Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
12.5 Feature-Based Inter-Image Prediction. . . . . . . . . . . . . . . . . . . . . . . . . . 254
12.5.1 Feature-Based Geometric Deformations . . . . . . . . . . . . . . . . . 254
12.5.2 Feature-Based Photometric Transformation . . . . . . . . . . . . . . 257
12.5.3 Block-Based Motion Compensation. . . . . . . . . . . . . . . . . . . . . 258
12.6 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
12.6.1 Efﬁciency of Multi-Model Prediction . . . . . . . . . . . . . . . . . . . 260
12.6.2 Efﬁciency of Photometric Transformation . . . . . . . . . . . . . . . . 261

xii
Contents
12.6.3 Overall Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
12.6.4 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
12.7 Our Conjecture on Cloud Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
12.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
Part V Compressive Communication
13
Compressive Data Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
13.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
13.2.1 Conventional Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
13.2.2 Distributed Source Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
13.2.3 Compressive Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
13.3 Compressive Data Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
13.3.1 Data Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
13.3.2 Data Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
13.4 Network Capacity of Compressive Data Gathering . . . . . . . . . . . . . . . 279
13.4.1 Network Capacity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
13.4.2 NS-2 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
13.5 Experiments on Real Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
13.5.1 CTD Data from the Ocean. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
13.5.2 Temperature in the Data Center . . . . . . . . . . . . . . . . . . . . . . . . 289
13.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
14
Compressive Modulation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
14.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
14.2.1 Rate Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
14.2.2 Mismatched Decoding Problem . . . . . . . . . . . . . . . . . . . . . . . . 298
14.3 Compressive Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
14.3.1 Coding and Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
14.3.2 Soft Demodulation and Decoding. . . . . . . . . . . . . . . . . . . . . . . 302
14.3.3 Design RP Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
14.4 Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
14.4.1 Rate Adaptation Performance . . . . . . . . . . . . . . . . . . . . . . . . . . 307
14.4.2 Sensitivity to SNR Estimation. . . . . . . . . . . . . . . . . . . . . . . . . . 309
14.5 Testbed Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
14.5.1 Comparison to Oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
14.5.2 Comparison to ADM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
14.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
14.6.1 Coded Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
14.6.2 Compressive Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

Contents
xiii
15
Joint Source and Channel Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
15.2 Related Work and Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
15.2.1 Joint Source-Channel Coding . . . . . . . . . . . . . . . . . . . . . . . . . . 319
15.2.2 Coded Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
15.2.3 Rate Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
15.2.4 Compressive Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
15.3 Compressive Modulation (CM) for Sparse Binary Sources . . . . . . . . 322
15.3.1 Design Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
15.3.2 Weight Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
15.3.3 Encoding Matrix Construction . . . . . . . . . . . . . . . . . . . . . . . . . 327
15.4 Belief Propagation Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
15.5 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
15.5.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
15.5.2 Simulations over an AWGN Channel . . . . . . . . . . . . . . . . . . . . 334
15.5.3 Emulation in Real Channel Environment. . . . . . . . . . . . . . . . . 335
15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
Part VI Pseudo-Analog Transmission
16
DCast: Distributed Video Multicast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
16.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
16.2.1 Distributed Video Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
16.2.2 Distributed Video Transmission . . . . . . . . . . . . . . . . . . . . . . . . 343
16.2.3 SoftCast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
16.3 Proposed DCast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
16.3.1 Coset Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
16.3.2 Coset Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
16.3.3 Power Allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
16.3.4 Packaging and Transmission . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
16.3.5 LMMSE Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
16.4 Power-Distortion Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
16.4.1 Relationship between Variables . . . . . . . . . . . . . . . . . . . . . . . . 353
16.4.2 MV Transmission Power and Distortion . . . . . . . . . . . . . . . . . 353
16.4.3 MV Distortion and Prediction Noise Variance . . . . . . . . . . . . 354
16.4.4 Distortion Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
16.4.5 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
16.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
16.5.1 PDO Model Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
16.5.2 Unicast Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
16.5.3 Evaluation of Each Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
16.5.4 Robustness Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
16.5.5 Multicast Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363

xiv
Contents
16.5.6 Complexity and Bit Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
16.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
17
Denoising in Communications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
17.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
17.2.1 Image Denoising. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
17.2.2 Video Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
17.3 System Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
17.3.1 System Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
17.3.2 Sender Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
17.3.3 Receiver Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
17.4 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
17.4.1 Cactus Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
17.4.2 GPU Implementation of BM3D . . . . . . . . . . . . . . . . . . . . . . . . 380
17.5 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
17.5.1 Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
17.5.2 Micro-Benchmarks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
17.5.3 Comparison against Reference Systems . . . . . . . . . . . . . . . . . 388
17.5.4 Transmitting High-Deﬁnition Videos . . . . . . . . . . . . . . . . . . . . 390
17.5.5 Robustness to Packet Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
17.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
17.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
18
MIMO Broadcasting with Receiver Antenna Heterogeneity . . . . . . . . . 395
18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
18.2 Background and Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
18.2.1 Multi-Antenna Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
18.2.2 Layered Source-Channel Schemes . . . . . . . . . . . . . . . . . . . . . . 398
18.2.3 Compressive Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
18.2.4 SoftCast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
18.3 Compressive Image Broadcasting System . . . . . . . . . . . . . . . . . . . . . . 400
18.3.1 The Encoder and Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
18.3.2 Addressing Heterogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
18.4 Power Allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
18.4.1 Power Scaling Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
18.4.2 Aggregating Coefﬁcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
18.5 Compressive Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
18.6 Amplitude Modulation and Transmission. . . . . . . . . . . . . . . . . . . . . . . 406
18.7 The CS Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
18.8 Simulation Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
18.8.1 Micro-Benchmarks for Our System . . . . . . . . . . . . . . . . . . . . . 409
18.8.2 Performance Comparison with Other Broadcast Systems . . . 412
18.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418

Contents
xv
Part VII Future Work
19
Computational Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
19.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
19.2 Cloud Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
19.3 Source Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
19.3.1 Coding of Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
19.3.2 Coding of Cloud Image Sources . . . . . . . . . . . . . . . . . . . . . . . . 427
19.3.3 Coding of Cloud Video Sources . . . . . . . . . . . . . . . . . . . . . . . . 429
19.3.4 Distributed Coding Using Cloud Sources . . . . . . . . . . . . . . . . 430
19.4 Channel Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
19.4.1 Power Allocation and Bandwidth Matching . . . . . . . . . . . . . . 432
19.4.2 Multiple Level Channel Coding . . . . . . . . . . . . . . . . . . . . . . . . 434
19.4.3 Channel Denoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
19.5 Joint Source and Channel Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
19.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
A
Our Published Journal and Conference Papers Related to This Book
439
A.1 Scalable Video Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
A.2 Directional Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
A.3 Vision-Based Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
A.4 Compressive Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
A.5 Pseudo-Analog Transmission. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465


Preface
I still clearly remember that it was September 13, 1999, when I started my job at
Microsoft Research Asia (MSRA), formerly named Microsoft Research China. It
was the ﬁrst year the lab was established. Twenty-eight Chinese Ph.D.s passed the
strict interviews and were hired as associate researchers by MSRA from among
more than 1000 applicants. Even so, at that time we were not formal employees
at Microsoft and only had a two-year contract probably because we received our
Ph.D. degrees in China. Thus, we had to prove that we had the ability to do excellent
research. All of us worked very hard, usually from 9 A.M. to 9 P.M. every day, seven
days a week. When I recall the early years I spent at MSRA, I feel like it may have
been the most wonderful time of my life.
At that time, Dr. Ya-Qin Zhang was the chief scientist at MSRA and the manager
of the Internet Media Group (IMG). There were three researchers in this group who
had come back from the United States, Dr. Jin Li, Dr. Shipeng Li, and Dr. Wenwu
Zhu. And among the 28 associate researchers, four were in IMG, Dr. Huirong Shao,
Dr. Hongwu Wang, Dr. Qian Zhang, and myself. Although I had several discussions
with Ya-Qin Zhang regarding my research project, I mainly worked with Shipeng Li.
Almost 15 years have passed, Li and I are still with MSRA. Zhang is now a Microsoft
corporate vice president, and Shipeng is the president of Microsoft R&D in China.
Jin Li works at Microsoft Research Redmond. Wenwu Zhu is a full professor at
Tsinghua University and Qian is a full professor at Hong Kong University of Science
and Technology. Huirong Shao and Hongwu Wang left MSRA early and work in
high-tech companies in Silicon Valley, California.
In my Ph.D. thesis, I studied object-based video coding. Since it was difﬁcult to
automatically and accurately extract visual objects from image and video, the re-
search topic looked less promising in real applications after I graduated. As I was
trying to ﬁnd a new research area, Dr. Weiping Li, who was Shipeng’s Ph.D. su-
pervisor, visited MSRA in October 1999 after the Moving Picture Experts Group
(MPEG) meeting in Melbourne, Australia. At that time, he was actively developing a
ﬁne granularity scalable (FGS) video coding standard in MPEG. He introduced us to
his bit-plane coding technology. He also told us that the coding efﬁciency of FGS was
considerably low because it used low-quality reference for motion compensation in
xvii

xviii
Preface
order to avoid drift error. Shipeng and I felt that it was a good research topic and im-
mediately started to study the problem. This was my ﬁrst research project at MSRA.
After two months, Shipeng and I brought our ﬁrst technical proposal, the earliest
version of progressive ﬁne granularity scalable (PFGS) video coding, to the MPEG
meeting in Maui, Hawaii, in December 1999. It was the ﬁrst time that I went abroad.
By using multiple references, our PFGS scheme signiﬁcantly improved the perfor-
mance of FGS coding. Although MPEG experts were very interested in our technol-
ogy, they were concerned about the complexity of using multiple references. Thus,
we continued to improve our PFGS scheme after the MPEG meeting. The scheme
was simpliﬁed by using only one additional reference and its performance was even
better than when multiple references were used. In March 2000, we proposed the
improved PFGS scheme to MPEG. At the meeting, the FGS standard would be
promoted as a committee draft and afterward, no new technical changes would be
allowed. It was our last chance to get the PFGS scheme into the standard. Unfortu-
nately, because of the complexity our effort failed again.
In the subsequent two to three years, I continued to improve the PFGS scheme and
proposed the improved technologies to MPEG as an advanced FGS standard. At the
same time, I also visited the Windows Media Player group several months every year
to write our PFGS scheme into Windows Media Video 8 (WMV8) and Windows Me-
dia Video 9 (WMV9) for shipping in Microsoft products. Both efforts failed to make
substantial progress. In order to prove the advantages of our PFGS scheme, we de-
veloped a real-time video streaming system using our scalable codec. We showed the
system in MPEG over the Internet by streaming video content from Beijing to Wash-
ington DC. The performance was much better than that of Windows Media Player,
which used a nonscalable codec. My research into PFGS continued until the emer-
gence of the motion compensated temporal ﬁltering (MCTF) technology. Although
these efforts were not successful, our lab recognized my research capability through
this project. I became a formal employee in May 2000 and was then promoted to a
researcher in February 2001.
In addition to the PFGS scheme, Shipeng Li and Jin Li began to study how to
integrate motion alignment into temporal wavelet transforms very early at MSRA.
Their work used the lifting-based wavelet for temporal transform and motion vectors
among frames. It was the same as the famous MCTF technology. I was not involved
in this project in the beginning. When I started to mentor our interns who worked on
this project in 2002, from my experiences in developing the PFGS scheme, I imme-
diately realized that it was a better solution than the PFGS scheme for scalable video
coding. We continued to improve the solution in various detailed technologies. All of
these technologies made up our Barbell-based lifting coding scheme. When MPEG
launched a call for proposals in 2003 to collect scalable video coding schemes and
evaluate their performance, our Barbell-based lifting coding scheme ranked ﬁrst and
third among 21 schemes in the two testing scenarios, respectively. It was adopted as
common software by MPEG for further exploration of wavelet-based video coding.
Our research on scalable video coding is presented in Part II of this book.
Our research in directional transforms was actually inspired by our Barbell-based
lifting coding scheme. I remembered one time when we brainstormed about the

Preface
xix
lifting-based temporal wavelet transform. I suddenly thought that we could apply
the idea to the 2D spatial wavelet transform. In MCTF, each temporal lifting step
can use motion vectors that indicate which pixels in different frames are transformed
together. In the 2D spatial transform, an angle can also be used to indicate which
neighboring pixels in the given direction are transformed. With this idea, I rapidly
implemented our lifting-based directional wavelet transform in the JPEG 2000 refer-
ence software and observed that the energy of the transform coefﬁcients was signiﬁ-
cantly reduced with directional adaptation. Later, my student continued to complete
the coding process and showed a more than 2 dB gain over JPEG 2000 on images
with rich orientation features.
Although there were many research papers on directional transforms already pub-
lished, one obvious advantage of our lifting-based directional transform was that
transformed coefﬁcients have a similar structure and distribution to that of the non-
directional counterpart. Therefore, existing quantization and entropy coding tech-
nologies can be applied efﬁciently without any changes. More importantly, the idea
of constructing directional transforms through the incorporation of directional oper-
ators into the lifting structure offers a uniﬁed and efﬁcient way to enable directional
adaptation in various existing nondirectional transforms. Subsequently, we further
developed the directional discrete cosine transform (DCT) by incorporating direc-
tional operators into its lifting implementation. Applying the proposed directional
DCT in JPEG gets a similar performance gain to the directional wavelet in JPEG
2000. Furthermore, we theoretically analyzed the order of two 1D transforms in a
2D directional transform and proposed the order-adaptive directional lifting trans-
form, which is more suitable for integration in the intra-frame coding of H.264. Our
research into directional transforms is presented in Part III of this book.
After the ﬁrst part of the H.264 standard was completed in 2003, I analyzed the
developments in image and video compression. In most existing schemes and stan-
dards, the statistical correlation among pixels is considered a dominant factor in de-
signing prediction, transforms, and entropy coding. During the past three decades,
these coding technologies have evolved a great deal. A few coding standards were
made and led to products. However, it is viewed to be a difﬁcult challenge to fur-
ther improve the coding technologies. In order to improve the coding performance,
the computational complexity of encoding has been and will continue to increase
signiﬁcantly. This has been further veriﬁed by the new High Efﬁcient Video Cod-
ing (HEVC) standard. So, at that time we started to look for other technologies that
could potentially lead to a breakthrough. A possible option was to explore the visual
correlation in image and video compression. Although using visual correlation has
been recognized for a while, it faces great challenges in compression.
Thanks to the strong research group in computer vision at MSRA, we rapidly
became familiar with the advances in computer vision. Our ﬁrst attempt was image
inpainting, which is an advanced vision-based synthesis technology. We proposed in-
corporating image inpainting into conventional coding schemes to synthesize struc-
ture regions by using coded edges. It not only introduced inpainting in compression
to reduce the total amount of coded information, but it also enhanced the robustness
and commonness of inpainting by explicitly coding important information. Toward

xx
Preface
the convergence of signal-processing-based compression and vision-based analy-
sis/synthesis for image coding, this interaction offered new opportunities to break
the waveform compression bottleneck. We thus received the best paper award at the
IEEE Transactions on Circuits and Systems for Video Technology (CSVT) in 2009.
With the rapid development of digital cameras and other mobile devices equipped
with cameras, there is a large number of images and videos available on the Internet.
When you randomly take a picture with your phone on the street, you can often ﬁnd
some highly correlated images on the Internet that were taken at the same location
from different viewpoints and angles, focal lengths, and illuminations. I personally
think that large-scale data is an important factor that can bring signiﬁcant changes
in image and video compression. However, conventional compression technologies
such as prediction and transform make it difﬁcult to take advantage of highly corre-
lated external data. Another attempt in vision-based coding was a cloud-based image
coding scheme, which is different from current image coding even on the ground.
The proposed scheme no longer compressed images pixel by pixel and instead tried
to describe images and reconstruct them from a large-scale image database via the
descriptions. It can achieve a thousand-to-one compression ratio with good visual
quality.
Previous compression schemes and standards mainly target the transmission of
images and video. However, the storage of large-scale images and video is becoming
a valuable research topic with the emergence of applications in photo sharing and
video surveillance. Our third attempt at vision-based coding was the compression
of large-scale images for storage. We ﬁrst analyzed their correlations and organized
them as multiple trees. All images described by one tree have strong correlations. The
correlations are represented by the relationship among parent and children nodes in
the tree. According to the relationship, we can process correlated images as a se-
quence and jointly optimize the coding of the sequence. Our experimental results
have shown that the size coding them as a sequence is only one-tenth that of coding
them one by one. Large-scale storage is an important application that supports the
use of external data for compression. Furthermore, the storage-oriented compression
may not need to be standardized because images and video can be transcoded to a
common format before they are output. It provides more ﬂexibility for developing in-
novative compression technologies. Our research in vision-based coding is presented
in Part IV of this book.
Before Chong Luo, who was an associate researcher at MSRA, asked to transfer
to my group in 2008, I had never worked on communications and had no knowledge
of even the basic concepts. Although my group is named Internet Media, all the
researchers in this group only had expertise in media. Luo’s participation provided
me a good opportunity to learn communications. I have found that the best way of
learning about a ﬁeld is working with an expert in that ﬁeld. Soon I could discuss
detailed communication technologies with Luo and others. Luo previously worked
on application-layer multicasts. I felt the work was too engineering focused and the
technology was less likely to impact practical applications. We should look for more
prospective research topics.

Preface
xxi
At that time, I had already spent a year studying the methodology of low-density
parity-check (LDPC) codes for the performance analysis of compressive sensing
with binary input. There are many similarities between LDPC codes and compres-
sive sensing. Why didn’t we use compressive sensing in communications such as
LDPC codes? One obvious advantage is that compressive sensing provides an in-
herent compression when the source is sparse. Our ﬁrst work applied compressive
sensing for data gathering in a sensor network. The proposed compressive data gath-
ering was able to reduce the global communications cost by using the correlation
among sensor readings without introducing intensive computations or complicated
transmission controls. This work was submitted to MobiCom 2009. At that time,
I did not know how difﬁcult it was for a paper to be accepted by the conference.
Fortunately, our paper received high review scores and was accepted.
We next applied binary-input compressive sensing in the physical layer of a wire-
less network. In contrast to LDPC codes that use logical XOR, compressive sensing
uses an arithmetic weighted sum to generate coded symbols from input bits. Thus it
can be viewed as arithmetic codes. Even for binary input, the values of coded sym-
bols are in a ﬁnite alphabet. This mapping from binary to a ﬁnite alphabet matches
the requirements of modulation. In our proposed compressive modulation, the con-
stellation usually contains hundreds of points, which is much denser than that used in
current wireless networks. Received symbols are still decodable even in poor channel
conditions by jointly decoding the received symbols together. The greatest advantage
of our proposed compressive modulation is that it can adapt to a wide range of chan-
nel conditions without changing the coding and constellation, which is desirable for
rate adaptation in wireless communications.
Although Shannon’s information theory tells us that source and channel coding
can be separate, they often need to be considered jointly in practical applications due
to the limited length of data. However, source coding is achieved by entropy coding,
and channel coding by linear codes, cyclic codes, or convolutional codes. They are
hard to jointly optimize because of the different properties. Another important ad-
vantage of our proposed compressive modulation is that it provides a nice solution
for joint source and channel coding in the physical layer. The source sparsity can
be utilized to reduce the number of coded symbols or help the decoding of source
bits in worse channel conditions. The performance of source coding provided by
our proposed compressive modulation is close to that of ideal source coding. Our
research into applying compressive sensing to communications is generally called
compressive communications, which is presented in Part V of this book.
In digital systems, the redundancy in image and video is fully removed by com-
pression, whereas in analog systems the redundancy in image and video is not re-
moved at all. They present two extreme cases. Digital systems have good perfor-
mance for image and video communication. But in order to ensure a nearly error-free
transmission, the sending rate has to adapt to actual channel conditions by adjusting
the channel coding rate and modulation. On the one hand, if the sending rate is too
low, channel capacity cannot be fully utilized. On the other hand, if the sending rate
is too high, the decoded bit error rate (BER) will increase dramatically. This phe-
nomenon is commonly called the cliff effect in digital communications. However, it

xxii
Preface
is widely known that wireless channels vary quickly and drastically. It is a challenge
to instantly make an accurate estimation of channel conditions. In addition, digital
systems cannot support image and video multicasts well. Although analog systems
have been discarded by academia and the industry, the processing still has its own
advantages especially in channel adaptation and multicast.
Recently, I have become interested in looking for a communications system be-
tween digital and analog systems, where images and video are not compressed com-
pletely and still have some redundancy. Such redundancy can be used for resisting
channel noise, ﬁnding external correlated images and video, and introducing signal
processing in the transmission. It provides an opportunity to make a breakthrough
in visual data communications. Our ﬁrst effort was motivated by SoftCast, which
was proposed by Professor D. Katabi et al. at MIT. In SoftCast, video coefﬁcients
are directly transmitted without entropy coding by simulating the analog process-
ing, although the scheme is implemented in a digital manner. However, the temporal
transform in SoftCast is inefﬁcient at exploiting the correlation among frames. We
proposed introducing distributed video coding instead of the 3D transform and thus
our scheme was called DCast. It can better utilize the temporal correlation through
motion alignment at the receiver.
When data transmitted over a channel is no longer compressed into random bits,
it actually enables us to introduce more processing in communications far beyond
channel coding. Our second effort was to introduce the denoising technology in the
physical layer of wireless transmissions. Denoising is a classical signal processing
technology and seems irrelevant to communications. However, if transmitted data is
correlated and added channel noise is random, denoising can be applied to attenuate
and even remove channel noise by utilizing the data correlation. In our proposed
scheme, we apply MCTF to remove the temporal correlation among frames but keep
the spatial correlation in every frame. The received frames are ﬁrst denoised by using
the spatial correlation before reconstruction. Denoising is only an example and we
strongly believe that more intellectual processing can be applied to communications
in the future.
Multiple-input multiple-output (MIMO) technologies have become building blocks
for high capacity links in contemporary and future wireless networks. Any new com-
munications system cannot be successful if it cannot fully take advantage of MIMO.
With mobile devices equipped with different numbers of antennae, broadcasting has
to handle both channel and receiver antenna heterogeneity. It is a big challenge for
existing transmission technologies to achieve adequate performance when matching
the antenna number and channel conditions of every receiver. We have observed that
when the source is not compressed completely, it is more ﬂexible and suitable to be
transmitted through MIMO. Furthermore, we have also proposed using compressive
sensing to combine multiple data elements as one transmitting symbol. Our results
have shown that all receivers with different numbers of antennae can achieve good
performance. In this category of our research, since the transmission simulates the
processing of analog systems but is still implemented in a digital manner, the pro-
posed schemes are called pseudo-analog transmission, which is presented in Part VI
of this book.

Preface
xxiii
How time ﬂies! It is my ﬁfteenth year with MSRA. When I started my ﬁrst re-
search project on scalable video coding, I did not know what I would do next. Doing
research is just like playing Go. After I did some in-depth research into scalable
video coding, which is similar to an occupied corner in Go, I started to look for new
opportunities and extended my research. Every extension seems unintended, but ﬁ-
nally, when I started to prepare my material for IEEE Fellow in 2012, I realized that
the research is very relevant and systematic. I would summarize my research as “Vi-
sual data compression and communication.” Although I completed my education in
China and did not go abroad before I joined MSRA, I was elected an IEEE Fellow as
quickly as others. I appreciate that MSRA has provided me a totally open research
atmosphere. I am granted the freedom to work on my preferred research topics and
projects, and to pursue long-term goals that extend far beyond current product cycles.
When this book was almost completed, I made the biggest decision in my career–
to leave Microsoft Research Asia and join the University of Science and Technology
of China (USTC) as a professor. It is deﬁnitely a new page to write in my life.
Feng Wu, Beijing
MATLAB R⃝is a registered trademark of The MathWorks, Inc. For product informa-
tion, please contact:
The MathWorks, Inc.
3 Apple Hill Drive
Natick, MA 01760-2098 USA
Tel: 508 647 7000
Fax: 508-647-7001
E-mail: info@mathworks.com
Web: www.mathworks.com


Acknowledgments
I would like to ﬁrst thank Prof. Jinpei Xu at the Harbin Institute of Technology (HIT)
and Prof. Wen Gao at Peking University, although they did not directly contribute
to the research presented in this book. Prof. Xu was my supervisor when I was a
master’s student at HIT. My ﬁrst research project, continuous speech recognition us-
ing neural networks, was done with Prof. Xu. Every week we spent several hours
discussing all the details, including the initial idea, the implementation, the experi-
mental results, and planning for the next step. I really appreciate what I learned from
Prof. Xu about the basic skills for doing research.
Prof. Gao was my Ph.D. supervisor. When I completed my courses and began
my research, Prof. Gao had already moved from HIT to the Institute of Computing
Technology (ICT) at the Chinese Academy of Science (CAS). Prof. Gao originally
arranged for me to continue to work on speech recognition. I told him that I would
like to change my research to video coding because my Chinese pronunciation has a
severe southern accent. The training models could not recognize my speech using the
data from others, and vice versa. Prof. Gao respected my choice and I started research
on video coding. At that time, Prof. Gao was the head of the China delegation at the
International Organization for Standardization (ISO)/International Electrotechnical
Commission (IEC) MPEG and attended MPEG meetings every three months. Thus,
I was well aware of the latest progress in video coding and received information as
fast as my foreign peers. It was very helpful for my research. The problems I studied
were almost at the same line as foreign experts. I think that this may be a reason why
I was able to pass the MSRA interview, even though I did not have any international
journal or conference papers published at that time.
Dr. Shipeng Li is my supervisor at MSRA and has further trained me. When I
worked on the PFGS coding scheme, we often had some creative discussions in his
ofﬁce. The key progress in our PFGS scheme could not happen without his contribu-
tions. Dr. Li also trained me in how to write high-quality papers in English. My ﬁrst
journal paper, published in the IEEE Transaction on Circuits and Systems for Video
Technology, was revised by Dr. Li. When I got the revised manuscript from him al-
most every paragraph had been revised by him. I carefully studied the revisions one
xxv

xxvi
Acknowledgments
by one and learned how to effectively and accurately express what I wanted to say in
English. These writing skills were very helpful for my career.
I would like to thank Dr. Lin Luo, Dr. Jizheng Xu, and Dr. Ruiqin Xiong for
their contributions to our research on scalable video coding. At that time, Luo was a
Ph.D. student at the University of Science and Technology of China (USTC) and had
worked as an intern at MSRA for several years. After graduation, she joined the IBM
research lab in Beijing. Xu was an undergraduate student at USTC when he started to
work as an intern at MSRA. Later, he got his master’s degree at ICT with Dr. Ya-Qin
Zhang and then joined MSRA as an assistant researcher. His research in this period
was mainly related to scalable video coding. Xiong was a Ph.D. student at ICT with
Ya-Qin Zhang as his supervisor. After graduation, he worked on a postdoctoral in
Australia for two years and then came back as a faculty at Peking University.
I would like to thank Dr. Wenpeng Ding, Prof. Xiaolin Wu, Dr. Hao Xu, Dr. Xi-
ulian Peng, and Dr. Jizheng Xu for their contributions to our research in directional
transforms. At that time, Ding was a Ph.D. student supervised by me at USTC and he
joined Beijing University of Technology as a faculty after graduation. Prof. Xiaolin
Wu contributed the algorithm for estimating directions in directional wavelet trans-
forms when he worked as a visiting professor at MSRA. Xu was a Ph.D. student
at USTC and was a postdoctoral researcher in Hong Kong after graduation. Peng
was a Ph.D. student supervised by me at USTC and joined MSRA as an associate
researcher after graduation. When Xu worked on directional transforms, he was an
associate researcher at MSRA and was also pursuing a Ph.D. degree co-supervised
by me at Shanghai Jiaotong University.
I would like to thank Dr. Dong Liu, Dr. Lican Dai, Huanjin Yue, Zhongbo Shi,
and Dr. Xiaoyan Sun for their contributions to our research on vision-based cod-
ing. At that time, Liu was a Ph.D. student at USTC supervised by me and he then
joined Nokia Research Beijing after graduation. Recently, he joined USTC as an
associate professor. Dai was a Ph.D. student at USTC and joined a high-tech com-
pany in Chengdu after graduation. Shi is my Ph.D. student at USTC and Yue is my
Ph.D. student at Tianjing University. Sun is a lead researcher at MSRA. She leads
our research efforts in this ﬁeld.
I would like to thank Prof. Xiaopeng Fan, Hao Cui, Dr. Xiaolin Liu, Prof. Chang
Wen Chen, and Dr. Chong Luo for their contributions to our research on compressive
communications and pseudo-analog communications. Fan is an associate professor
at HIT working on distributed video coding. When he visited MSRA for six months,
I mentored him and extended his research from coding to communications. DCast
was done while he was at MSRA. Cui is a Ph.D. student at USTC and Prof. Chang
Wen Chen is his supervisor. He works at MSRA as an intern. Liu was a Ph.D. student
at USTC supervised by me. Prof. Chen closely collaborates with me at MSRA. He
visited us two to three weeks every year under a consulting contract. Luo joined
my group as an associate researcher and then got her Ph.D. co-supervised by me at
Shanghai Jiaotong University.
Finally, I would like to thank all the members of the Internet Media Group at
MSRA. Most of them have worked with me for more than ﬁve years. Dr. Jizheng
Xu started his internship at MSRA in 1999. He joined MSRA as an assistant

Acknowledgments
xxvii
researcher in September 2004 and now is a lead researcher. He is conducting our re-
search projects on signal-processing-based compression and video coding standards.
Dr. Xiaoyan Sun started her internship at MSRA in 2000. She joined MSRA as an as-
sociate researcher in November 2003 and is now a lead researcher. She is conducting
our research projects on vision-based compression. Luo joined MSRA as an assis-
tant researcher in September 2004 and is now a lead researcher. She is conducting
our research projects on media communications. In addition, Dr. Zhiwei Xiong, Dr.
Xiulian Peng, Dr. Bin Li, and Dr. Cuilin Lan joined MSRA as associate researchers
in the past three years after they completed their Ph.D. research at MSRA.


Acronyms
ADC
Analog-to-Digital Conversion
ADL
Adaptive Directional Lifting
AEP
Asymptotic Equipartition Property
AGC
Automatic Gain Control
AM
Amplitude Modulation
AMC
Adaptive Modulation and Coding
AMVP
Advanced Motion Vector Prediction
APP
A Posteriori Probability
ASK
Amplitude-Shift Keying
AVC
Advanced Video Coding
AWGN
Additive White Gaussian Noise
BDF
Bidirectional Filtering
BER
Bit Error Rate
BM3D
Block Matching with 3D Filtering
BRP
Block Removal Pattern
CABAC Context Adaptive Binary Arithmetic Coding
CAVLC Context Adaptive Variable Length Coding
CB
Coding Block
CD
Compact Disc
CDD
Curvature Driven Diffusion
CDG
Compressive Data Gathering
CG
Computer Graphics
CHoG
Compressed Histogram of Gradient
CIF
Common Intermediate Format
CM
Compressive Modulation
CME
Correlated Motion Estimation
CTB
Coding Tree Block
CTU
Coding Tree Unit
CU
Coding Unit
DAC
Digital-to-Analog Conversion
DBF
Deblocking Filter
xxix

xxx
Acronyms
DCT
Discrete Cosine Transform
DFD
Displaced Frame Difference
dFT
directional Filtering Transform
DSCQS Double Stimulus Continuous Quality Scale
DST
Discrete Sine Transform
DVC
Distributed Video Coding
DVD
Digital Video Disk
DWT
Discrete Wavelet Transform
EBCOT Embedded Block Coding with Optimized Truncation
ESCOT Embedded Subband Coding with Optimized Truncation
EXIT
Extrinsic Information Transfer
EZBC
Embedded Zero Block Coding
FDM
Frequency Division Multiplexing
FGS
Fine Granularity Scalable
FIR
Finite Impulse Response
FLC
Fixed Length Coding
FM
Frequency Modulation
FSK
Frequency-Shift Keying
GOP
Group of Pictures
GPU
Graphic Processing Unit
HD
High Deﬁnition
HEC
Header Extension Code
HEVC
High Efﬁcient Video Coding
HM
Hierarchical Modulation
HVS
Human Visual System
HVSBM Hierarchical Variable Size Block Matching
i.i.d.
Independent and identically distributed
ISDN
Integrated Services Digital Network
JPEG
Joint Photographic Experts Group
JSCC
Joint Source-Channel Coding
JVT
Joint Video Team
LDPC
Low-Density Parity-Check
LLR
Logarithm of Likelihood Ratio
LLSE
Linear Least Square Estimator
LMMSE Linear Minimum Mean Square Error
LTE
Long-Term Evolution
MAC
Medium Access Control
MAP
Maximum A Posteriori
MB
Macroblock
MC
Motion Compensation
MCE
Motion Compensated Extrapolation
MCI
Motion Compensated Interpolation
MCTF
Motion Compensated Temporal Filtering
ME
Motion Estimation
MIMO
Multiple-Input Multiple-Output

Acronyms
xxxi
MISO
Multiple-Input Single-Output
ML
Maximum Likelihood
MPE
Minimum Probability of Error
MPEG
Moving Picture Experts Group
MSB
Most Signiﬁcant Bit
MSE
Mean Square Error
MSRA
Microsoft Research Asia
MST
Minimum Spanning Tree
MT
Motion Threading
MV
Motion Vector
MVD
Motion Vector Difference
OBMC
Overlapped Block Motion Compensation
P2P
Peer-to-Peer
PAM
Pulse Amplitude Modulation
PB
Prediction Block
PCA
Principal Components Analysis
PDE
Partial Differential Equation
PDF
Probability Density Function
PDM
Pulse Duration Modulation
PFGS
Progressive Fine Granularity Scalable
PM
Phase Modulation
PMF
Probability Mass Function
PPM
Pulse Position Modulation
PSD
Power Spectral Density
PSK
Phase-Shift Keying
PSNR
Peak Signal-to-Noise Ratio
PU
Prediction Unit
QAM
Quadrature Amplitude Modulation
QCIF
Quarter Common Intermediate Format
RDO
Rate Distortion Optimization
RP
Random Projection
RSC
Recursive Systematic Convolutional
SAD
Sum of Absolution Difference
SAO
Sample Adaptive Offset
SFMST SIFT Feature-Based Minimum Spanning Tree
SIFT
Scale Invariant Feature Transform
SNR
Signal-to-Noise Ratio
SSB
Single Sideband
SSD
Sum of Squared Difference
STBC
Space Time Block Code
SURF
Speed Up Robust Feature
SVC
Scalable Video Coding
TB
Transform Block
TDM
Time Division Multiplexing
TDMA
Time Division Multiple Access

xxxii
Acronyms
TU
Transform Unit
TV
Television
UDF
Unidirectional Filtering
UHD
Ultra High Deﬁnition
URQ
Uniform Reconstruction Quantization
VCEG
Video Coding Experts Group
VHS
Video Home System
VLC
Variable Length Coding
VLD
Variable Length Decoding
VoD
Video on Demand

Part I
Basis for Compression and Communication
This part provides a basis for our advanced research on visual data compression
and communication both theoretically and technically.
Chapter 1 discusses Shannon’s information theory, which is the theoretical basis
for visual data compression and communication. First, we introduce the concept of
entropy, which is the minimum length for lossless compression of a source, accord-
ing to Shannon’s source coding theorem. Huffman coding and arithmetic coding are
taken as examples to explain what source coding is and how a source is compressed
toward its entropy. The rate distortion theorem is also discussed because of its im-
portance to guiding the lossy compression of a source. Second, we introduce the
concept of channel capacity, which is the maximum rate that a channel can be used
with negligible transmission errors, according to Shannon’s channel coding theorem.
Hamming code is taken as an example to explain how channel coding works. Finally,
the joint source and channel coding is introduced. The conditions for source channel
separation are discussed. It is the theoretical justiﬁcation why data compression and
transmission can be studied independently.
As a concrete application of source coding, Chapter 2 discusses the basic frame-
work of video compression and the major developments in recent decades. This
serves as the technical basis for our research presented in Parts II, III, and IV of
this book. First, we introduce the hybrid prediction and transform framework of
video compression and the basic technologies used in the framework. They were
established in the 1980s without fundamental changes until now. Second, the de-
velopments of compression technologies are discussed according to the evolution
of video coding standards, from the earliest H.261, MPEG-1, MPEG-2, MPEG-4,
H.264/MPEG-4 Advanced Video Coding (AVC) until the latest High Efﬁcient Video
Coding (HEVC) standard. These technical developments have improved the perfor-
mance of video compression about one order of magnitude. Finally, to better un-
derstand modern compression technologies, the H.264/MPEG-4 AVC and HEVC
standards are discussed in detail.
As a concrete application of channel coding, Chapter 3 discusses analog and dig-
ital transmission technologies. It is the technical basis for our research presented
in Parts V and VI of this book. The analog transmission is introduced, although
it is almost thrown away by academics and industry experts because of the domi-
nance of digital systems. But some recent research shows that using the idea of anal-
ogy methods applied to modern digital communication have some unique advan-
tages, especially for visual data communication. The analog modulation and mul-
tiplexing technologies are brieﬂy discussed here. Then the digital transmission is

introduced. We mainly focus on the technologies used in the physical layer, which
consists of channel coding and modulation. The capacity-approached low-density
parity-check (LDPC) codes and Turbo codes with digital modulation are discussed
in detail.

Chapter 1
Information Theory
1.1 Introduction
Over the past few decades, with the rapid development of digital cameras and var-
ious mobile devices equipped with cameras, which are convenient to use anywhere
and anytime, we have witnessed the magnitude of visual data (images, photos, and
videos), produced by humans in their daily lives, increasing explosively beyond ex-
pectations. In 2012, Facebook announced that the number of photos uploaded by its
users had exceeded 220 billion [1]. Furthermore, the amount is still increasing at an
average rate of several hundred million per day. Additionally, the magnitude of vi-
sual data has increased due to increasing resolution. The resolution of images and
photos has been increased from several hundred thousand pixels to several million
pixels. Video resolution has been increasing from early quarter common intermediate
format (QCIF) to current high deﬁnition (HD).
The increasing magnitude of visual data is also veriﬁed by wired and wireless net-
work trafﬁc. According to the Cisco visual networking index: forecast and method-
ology (2011–2016) [2], Internet video trafﬁc will comprise 55% of all Internet trafﬁc
by 2016, rising from 51% in 2011. This does not include videos exchanged through
peer-to-peer (P2P) ﬁle sharing. The sum of all forms of video, including televi-
sion (TV), video on demand (VoD), Internet, and P2P, will be approximately 86%
of all Internet trafﬁc by 2016. The study further indicates that mobile video trafﬁc
exceeded 50% for the ﬁrst time in 2012 [3]. Mobile video trafﬁc will be 66.5% of all
mobile trafﬁc in 2016.
Therefore, no matter how fast wired and wireless network bandwidth is being
increased, highly efﬁcient compression of visual data and its highly efﬁcient trans-
mission will need to continuously improve in quality. The theoretical basis of com-
pression and transmission of visual data is Shannon’s information theory [4]. The
typical scenario studied in information theory is shown in Figure 1.1. A transmitter
sends a source S over a noisy channel to a receiver. The channel adds a certain noise
to the transmitted source. The receiver has to recover the source from the received
3

4
1 Information Theory
Source 
Coding
Channel 
Coding
+
Source 
Decoding
Channel 
Decoding
Channel
Noise
Source 
S
Decoded
Source S
Transmitter
Receiver
Channel
X
Y
Ŷ 
X
Figure 1.1 The basic framework for source and channel coding.
data with noise. To efﬁciently achieve it, there are two core technologies needed:
source coding and channel coding.
Source usually contains a certain redundancy and thus directly transmitting it
would cost more channel bandwidth than what is really needed. In the transmit-
ter, source coding tries to remove the redundancy from the source when possible,
by using various compression technologies. The output after source coding should
be randomly distributed bits. Since the channel is noisy, the compressed bits will
be corrupted by adding channel noise. Channel coding tries to introduce a certain
redundancy back to the source, which protects the compressed bits. At the receiver,
channel decoding corrects the received errors by using redundancy. Source decoding
recovers the source from the compressed bits.
This chapter is organized according to T. M. Cover and J. A. Thomas’s book
Elements of Information Theory [5], which is a comprehensive book about Shannon’s
information theory. We do not pay much attention to either the completeness of the
theory or the details of how to prove the theorems step by step. Instead, we try to
highlight the core ideas behind the theory and make them more intuitively understood
for further practical research on visual data compression and communication. The
readers, who are interested in the entire information theory, can ﬁnd information on
this subject by Shannon [4] and Cover and Thomas [5].
Let us assume a ﬁnite symbol alphabet A = {a0,a1,··· ,aq−1}, whose probabil-
ities of occurrence are p = {p0, p1,··· , pq−1}, satisfying ∑i pi = 1. These probabil-
ities are known. Let S be a discrete random variable, taking the symbol from the
alphabet A according to the probability p(s = ai) = pi, where s is an instance of
S. For different probability distributions, the information carried by S is different.
Shannon’s information theory deﬁnes entropy to measure the information.
Deﬁnition 1 The entropy of a discrete random variable S is deﬁned as
H(S) = −∑
i
pi log pi.
(1.1)
H() plays a central role in the information theory as measures of information and
uncertainty. If and only if all the pi but one are zero, the entropy H(S) = 0. In this
case, S always takes ai whose probability is 1. The uncertainty is zero and thus the
entropy is zero. If p0 = p1 = ··· = pq−1 = 1/q, S has the same probability taking any
ai. The entropy H(S) is the maximum, which is equal to log2 q.

1.1 Introduction
5
The deﬁnition of entropy can be extended to a pair of random variables Si and Sj.
In information theory, the relationship between two random variables is described by
joint entropy and conditional entropy.
Deﬁnition 2 The joint entropy H(Si,Sj) of a pair of discrete random variables
(Si,Sj) with a joint distribution p(si,sj) is deﬁned as
H(Si,Sj) = −∑
si∈A ∑
sj∈A
p(si,sj)log p(si,sj) = −E log p(Si,S j).
(1.2)
The conditional entropy of Si given Sj is deﬁned as the expected value of the en-
tropies of the conditional distributions.
Deﬁnition 3 If (Si,Sj) ∼p(si,sj), the conditional entropy H(Si | Sj) is deﬁned as
H(Si | Sj) = ∑
sj∈A
p(sj)H(Si | Sj = sj),
(1.3)
= −∑
sj∈A
p(sj) ∑
si∈A
p(si | s j)log p(si | sj),
(1.4)
= −∑
sj∈A ∑
si∈A
p(si,s j)log p(si | sj),
(1.5)
= −Ep(si,sj) log p(Si | Sj).
(1.6)
As we have discussed, entropy is a measure of the uncertainty of a random vari-
able. It is also a measure of the amount of information required on average to de-
scribe the random variable. However, mutual information is a measure of the amount
of information that one random variable contains about another random variable. It
is the reduction in the uncertainty of one random variable due to the knowledge of
the other.
Deﬁnition 4 Assume two random variables Si and Sj with a joint probability mass
function (PMF) p(si,sj). Their marginal probability mass functions are p(si) and
p(s j), respectively. The mutual information I(Si;Sj) is deﬁned as
I(Si;Sj) = ∑
si∈A ∑
sj∈A
p(si,s j)log p(si,sj)
p(si)p(s j),
(1.7)
= ∑
si∈A ∑
sj∈A
p(si,s j)log p(si | s j)
p(si)
,
(1.8)
= −∑
si∈A ∑
sj∈A
p(si,sj)log p(si)+ ∑
si∈A ∑
s j∈A
p(si,sj)log p(si | sj), (1.9)
= −∑
si∈A
p(si)log p(si)−
 
−∑
si∈A ∑
sj∈A
p(si,sj)log p(si | sj)
!
, (1.10)
= H(Si)−H(Si | Sj).
(1.11)

6
1 Information Theory
Next, we consider a set of random variables Sn = {S1,S2,··· ,Sn}. All random
variables have the same distribution p(s). Furthermore, for any i ̸= j, Si is indepen-
dent of Sj. In other words, Sn is an independent and identically distributed (i.i.d.)
source. In information theory, the analog of the law of large numbers is the asymp-
totic equipartition property (AEP).
Deﬁnition 5 If S1,S2,··· ,Sn are i.i.d. ∼p(s), the AEP indicates
−1
n log p(S1,S2,··· ,Sn) →H(S)
(1.12)
in probability.
It is a direct consequence of the weak law of large numbers. The law of large numbers
states that, for i.i.d. variables, 1/n∑i Si is close to its expected value E(S) for large
n. The AEP states that −1/nlog p(S1,S2,··· ,Sn) is close to the entropy H(S), where
p(S1,S2,··· ,Sn) is the probability of observing the sequence S1,S2,··· ,Sn. Thus, the
probability p(S1,S2,··· ,Sn) will be close to 2−nH(S).
Finally, we discuss the continuous random variable. Let S be a random variable
with a cumulative distribution F(s) = Pr(S ≤s). If F(s) is continuous, the random
variable is said to be continuous. Let f(s) = F′(s) when the derivative exists. If
R ∞
−∞f(s) = 1, f(s) is called the probability density function (PDF) for S. The set
where f(s) > 0 is called the support set of S.
Deﬁnition 6 The differential entropy h(S) of a continuous random variable S with a
PDF f(s) is deﬁned as
h(S) = −
Z
Ωf(s)log f(s)ds,
(1.13)
where Ωis the support set of the random variable.
If S is a zero-mean Gaussian variable, that is, S ∼f(s) = (1/
√
2πσ2)×e−s2/2σ2,
h(S) = −
Z
f(s)ln f(s)ds,
(1.14)
= −
Z
f(s)

−s2
2σ2 −ln
√
2πσ2

ds,
(1.15)
= ES2
2σ2 + 1
2 ln2πσ2,
(1.16)
= 1
2 + 1
2 ln2πσ2,
(1.17)
= 1
2 lne+ 1
2 ln2πσ2,
(1.18)
= 1
2 ln2πeσ2.
(1.19)
Changing the base of the logarithm, the differential entropy for a zero-mean Gaussian
variable is h(S) = 1/2log2πeσ2.

1.2 Source Coding
7
1.2 Source Coding
With the above basic information theory deﬁnitions, we are ready to discuss source
coding. The objective of the so-called source coding is to ﬁnd a source code C map-
ping all possible values of a random variable S in the alphabet A to a set of ﬁnite
length of binary strings (also called codewords). The mapping must be one-to-one so
that we can only recover a unique value from one given codeword. Let C(ai) denote
the codeword corresponding to s = ai and l(ai) denote the length of C(ai). Therefore,
the expected length L(S) is given by
L(S) = ∑
i
pil(ai).
(1.20)
Shannon’s ﬁrst theorem states the relationship between the expected length and
the entropy. It is described here.
Theorem 1.1. Let l∗
0, l∗
1, ···, l∗
q−1 be the optimal codeword lengths for a source
distribution p, and let L∗be the associated expected length of the optimal code
(L∗= ∑i pil∗
i ), then
H(S) ≤L∗< H(S)+1.
(1.21)
This theorem is the binary version of Theorem 5.4.1 proved by Cover and Thomas
[5]. The optimal expected length L∗is impossibly less than the entropy H(S). In
other words, the lossless compression of a random variable cannot be less than its
entropy. Note that there is an overhead which is at most 1 bit because of the fact that
log1/pi is not always an integer.
Let us take a simple example with four symbols A = {a0,a1,a2,a3}. Their prob-
abilities are {0.5,0.25,0.125,0.125}. According to Eq. (1.1), the entropy H(S) is
equal to
H(S) = −(1
2 log 1
2 + 1
4 log 1
4 + 2
8 log 1
8) = 1.75 bits.
The most straightforward way to conduct source coding is to assign 2 different bits to
every symbol. It is so-called ﬁxed length coding (FLC). The expected length L(S) is
2 bits too. It is larger than the entropy by 0.25 bits and thus FLC is not optimal. This
simple example shows the importance of source coding in the information theory.
According to the theory, shorter codewords should be assigned to symbols with larger
probabilities, while longer codewords to symbols with smaller probabilities. This is
the basis of variable length coding (VLC). How to design the set of codewords and
how to assign them to the symbols are just the problems to be solved in source
coding. The optimization of source coding aims to make the excepted coding length
L(S) equal to the entropy H(S). There are several source coding methods that are
optimal and can reach information entropy. We will discuss Huffman coding and
arithmetic coding in the following sections.

8
1 Information Theory
1.2.1 Huffman Coding
An optimal preﬁx code for a given distribution can be constructed by a simple al-
gorithm proposed by Huffman [6], widely known as Huffman coding. In Huffman
coding, a binary tree of nodes is ﬁrst created. The size is equal to the number of sym-
bols to code, q. Initially, all nodes are leaf nodes containing the probabilities of the
symbols they represent. The coding process essentially begins with the leaf nodes. A
new node whose children are the two nodes with the smallest probability is created.
The new node’s probability is equal to the sum of the children’s probabilities. With
the previous two nodes merged into one node, they are no longer considered. Instead
the new node is considered. This procedure is repeated until only one node remains.
As a common convention, bit “0” represents the left child and bit “1” represents the
right child. A ﬁnished tree has up to q leaf nodes and q −1 internal nodes. This
Huffman tree produces the optimal code length.
For the simple example with four symbols, a binary tree is established according
to the algorithm in Figure 1.2a. The bits “0” and “1” are assigned in Figure 1.2b.
Therefore, the codewords for the symbols a0, a1, a2, and a3 are “1,” “01,” “001,” and
“000,” respectively. The shortest codeword “1” is assigned to the symbol a0. Using
Eq. (1.20) to calculate the expected length of the codewords, it is 1.75 bits, which is
exactly equal to the entropy.
1.2.2 Arithmetic Coding
If the alphabet A is small (e.g., only two symbols a0 = 0 and a1 = 1), although their
probabilities are different, Huffman codes still use one bit to represent them because
0.125
0.125
0.25
0.25
0.5
0.5
a3
a2
a1
a0
(a)
0
1
0
1
0
1
a3
a2
a1
a0
(b)
Figure 1.2 Huffman coding. (a) The process of building a binary tree; (b) the designed Huffman
codes.

1.2 Source Coding
9
they cannot adjust codeword lengths at fractional bit precision. In this case, Huffman
codes are not optimal. One alternative is to jointly code a source sequence instead
of the individual source. According to the AEP, we can achieve an expected length
per every source close to the entropy of H(S). Therefore, it is desirable to have an
efﬁcient coding procedure that works for a long block of source letters. Huffman
coding is not ideal for this situation, since it is a bottom-up procedure that requires
the calculation of the probabilities of all source sequences of a particular block length
and the construction of the corresponding complete code tree. Thus the block length
is limited. A better scheme is one which can be easily extended to longer block
lengths without having to redo all the calculations. Arithmetic coding achieves this
goal.
Assume sn = s1s2 ···sn is a source sequence. The essential idea of arithmetic
coding is to efﬁciently calculate the PMF p(sn) and the cumulative distribution
function F(sn) = ∑un≤sn p(un) for the source sequence sn. We can use a num-
ber in the interval [(F(sn) −p(sn)],[F(sn)] as the codeword for sn. The code-
words for various sequences are different because they are within different interval
[(F(sn) −p(sn)],[F(sn)]. A simpliﬁed version of the arithmetic coding is described
here to illustrate the primary.
A is assumed to be binary. A simple procedure is assumed to calculate p(s1,s2,
···, sn) for any string sn = s1s2 ···sn. The natural lexicographic order is used on
strings, so that a string sn is greater than a string un if si = 1, ui = 0 for the ﬁrst
i such that si ̸= ui. Equivalently, sn > un if ∑i si2−i > ∑i ui2−i. We can arrange the
strings as the leaves of a tree of depth n, where each level of the tree corresponds to
one bit. Such a tree is illustrated in Figure 1.3. In Figure 1.3, the ordering sn > un
corresponds to the fact that sn is to the right of un on the same level of the tree.
We need to ﬁnd p(un) for all un ≤sn and use that to calculate F(sn). Looking at
the tree in Figure 1.3, we might suspect that we need to calculate the probabilities of
1
0
T1
T2
T3 sn
Figure 1.3 Tree of strings for arithmetic coding.

10
1 Information Theory
all the leaves to the left of sn to ﬁnd F(sn). The sum of these probabilities is the sum
of the probabilities of all the subtrees to the left of sn. Let Ts1s2···si−10 be a subtree
starting with s1s2 ···si−10. The probability of this subtree is
p(Ts1s2···si−10) = ∑
ui+1···un
p(s1s2 ···si−10ui+1 ···un) = p(s1s2 ···si−10).
(1.22)
It can be calculated easily. Therefore, we can write F(sn) as
F(sn) = ∑
un≤sn p(un) =
∑
T:T is to the le ft o f S
p(T) = ∑
i:si=1
p(s1s2 ···si−10).
(1.23)
Thus, F(sn) can be calculated quickly from p(sn). To encode the next bit of the
source sequence, we need only calculate p(snsn+1) and update F(snsn+1) using the
above method. Encoding can be done sequentially, by looking at the bits as they
come in.
To decode the sequence, we use the same procedure to calculate the cumulative
distribution function and check whether it exceeds the value corresponding to the
codeword. We then use the tree in Figure 1.3 as a decision tree. At the top node, we
check if the received codeword F(sn) is greater than p(0). If it is, then the subtree
starting with 0 is to the left of sn and hence s1 = 1. Continuing this process down the
tree, we can decode the bits in sequence. Thus, we can compress and decompress a
source sequence in a sequential manner.
The above procedure depends on a model for which we can easily compute p(sn).
Two examples of such models are i.i.d. sources, where
p(sn) =
n
∏
i=1
p(si),
(1.24)
and the Markov source, where
p(sn) = p(s1)
n
∏
i=2
p(si | si−1).
(1.25)
In both cases, we can easily calculate p(snsn+1) from p(sn).
Note that it is not essential that the probabilities used in the encoding be equal to
the true distribution of the source. In some cases, such as in image and video com-
pression, it is difﬁcult to describe a true distribution for the source. Even then, it is
possible to apply the above arithmetic coding procedure. The procedure will be efﬁ-
cient only if the model distribution is close to the empirical distribution of the source.
A more sophisticated use of arithmetic coding is to change the model dynamically
to adapt to the source, which has been adopted in the recent video coding standards.
Adaptive models work well for large classes of sources.

1.2 Source Coding
11
1.2.3 Rate Distortion Theory
In Huffman coding and arithmetic coding, the coding process does not lose any
source information and the source can be exactly reproduced from coded data, known
as lossless coding. In many cases, such as with image and video compression, some
source information has to be discarded during the coding process so that the coding
rate is not larger than a given rate. The decoded source always contains a certain dif-
ference from the original source. Therefore, this coding is called lossy coding. Rate
distortion theory provides the theoretical principle for optimizing lossy coding.
A classical example of lossy coding is the alphabet A as an arbitrary real number.
It will require an inﬁnite number of bits to represent it. So a ﬁnite representation of
a continuous random variable can never be perfect. How well can we do in getting
as close to perfect as possible? To answer this question, it is necessary to deﬁne a
distortion measure, which is a measure of distance between the random variable and
its representation. The basic problem with the rate distortion theory can be stated as
follows: given a source distribution and a distortion measure, what is the minimum
expected distortion achievable at a particular rate? Or, what is the minimum rate
description required to achieve a particular distortion?
We ﬁrst consider the problem of representing a single continuous random variable
S. Let the representation of S be denoted as ˆS = Q(S), where Q() is a quantization
and de-quantization function. If we are given R bits to represent S, ˆS can take on
2R values. Here, we assume that all values have the same probability to occur and
thus the coding process is not considered. The problem is ﬁnding the optimum set of
values for ˆS (called the reproduction points) and the regions that are associated with
each value ˆS.
For example, let S ∼N(0,σ2
s ) and assume a squared distortion measure. If 1 bit
is given to represent S, it is clear that the bit should distinguish whether S > 0 or not.
To minimize the squared error, each reproduced symbol should be at the conditional
mean of its region, thus
Q(s) =







r
2
π σs, if s ≥0
−
r
2
π σs, if s < 0
.
(1.26)
If we are given 2 bits to represent the source, the situation is not simple. Clearly, we
want to divide the real line into four regions and use a point within each region to
represent the sample. But it is no longer immediately obvious what the representation
regions and the reconstruction points should be.
For general cases, let us assume a set of n i.i.d. random variables drawn accord-
ing to a given distribution. These random variables are to be represented using nR
bits. Since the source is i.i.d., the samples are independent. It may appear that the
representation of each sample is an independent problem to be treated separately.
But this is not true as we will show at the end of this section. The entire sequence is
represented by a single index taking 2nR values. This treatment of entire sequences

12
1 Information Theory
immediately achieves a lower distortion for the same rate than independent quanti-
zation of the individual samples.
A (2nR,n) rate distortion code consists of an encoding function
fn :
Sn →1,2,··· ,2nR,
(1.27)
and a decoding function,
gn :
1,2,··· ,2nR →ˆSn.
(1.28)
The distortion associated with the (2nR,n) code is deﬁned as
D = Ed(Sn,gn(fn(Sn))),
(1.29)
where the expectation is with respect to the probability distribution on Sn, that is,
D = ∑
sn p(sn)d(sn,gn(fn(sn))).
(1.30)
The distortion function d(sn, ˆsn) is deﬁned as the squared error d(sn, ˆsn) = 1/n
∑i(si −ˆsi)2.
A rate distortion pair (R,D) is said to be achievable if there exists a sequence
of (2nR,n) rate distortion codes (fn,gn) with limn→∞Ed(Sn,gn(fn(Sn))) ≤D. A rate
distortion region for a source is the closure of the set of achievable rate distortion
pairs (R,D). The rate distortion function R(D) is the inﬁmum of rates R such that
(R,D) is in the rate distortion region of the source for a given distortion D. The
distortion rate function D(R) is the inﬁmum of all distortions D such that (R,D) is in
the rate distortion region of the source for a given rate R.
The main theorem in the rate distortion theory can now be stated as follows:
Theorem 1.2. The rate distortion function for an i.i.d. source Sn with distribution
p(sn) and bounded distortion function d(sn, ˆsn) is equal to the associated information
rate distortion function. Thus
R(D) =
min
p(ˆsn|sn):∑(sn,ˆsn) p(sn)p(ˆsn|sn)d(sn,ˆsn)≤DI(Sn; ˆSn)
(1.31)
is the minimum achievable rate at distortion D.
This is Theorem 13.2.1 proven by Cover and Thomas [5]. From the theorem, for a
given distortion D, the minimum rate is the minimum mutual information between
Sn and ˆSn for all pairs of (sn, ˆsn), which the distortions are less than or equal to D.
With the rate distortion theorem, we can derive the close-form rate distortion func-
tion of a single Gaussian source. Let S be ∼N(0,σ2
s ). According to the rate distortion
theorem, we have
R(D) =
min
f(ˆs|s):E( ˆS−S)2≤D
I(S; ˆS).
(1.32)
We ﬁrst ﬁnd a lower bound for the above rate distortion function. Since E(S−ˆS)2 ≤
D, we observe

1.2 Source Coding
13
I(S; ˆS) = h(S)−h(S | ˆS),
(1.33)
= 1
2 log(2πe)σ2
s −h(S−ˆS | ˆS),
(1.34)
≥1
2 log(2πe)σ2
s −h(S−ˆS),
(1.35)
≥1
2 log(2πe)σ2
s −h(N(0,E(S−ˆS)2)),
(1.36)
= 1
2 log(2πe)σ2
s −1
2 log(2πe)E(S−ˆS)2,
(1.37)
≥1
2 log(2πe)σ2
s −1
2 log(2πe)D,
(1.38)
= 1
2 log σ2
s
D ,
(1.39)
where Eq. (1.35) follows from the fact that conditioning reduces entropy and Eq.
(1.36) follows from the fact that normal distribution maximizes entropy for a given
second moment. Hence
R(D) ≥1
2 log σ2
s
D .
(1.40)
To ﬁnd the conditional density f(ˆs | s) that achieves this lower bound Eq. (1.40),
it is usually more convenient to look at the conditional density f(s | ˆs), which is
sometimes called the test channel. If D ≤σ2
s , we choose
S = ˆS+Z,
ˆS ∼N(0,σ2
s −D), Z ∼N(0,D),
(1.41)
where ˆS and Z are independent. For this joint distribution, we can calculate
I(S; ˆS) = 1
2 log σ2
s
D ,
(1.42)
and E(S−ˆS)2 = D, thus achieving the bound in Eq. (1.40). If D > σ2
s , we choose ˆS =
0 with probability 1, achieving R(D) = 0. Hence, the rate distortion for a Gaussian
source with the squared error distortion is
R(D) =



1
2 log σ2
s
D , 0 ≤D ≤σ2
s
0,
D > σ2
s
.
(1.43)
We can rewrite Eq. (1.43) to express the distortion in terms of the rate,
D(R) = σ2
s 2−2R.
(1.44)
Each bit of description reduces the expected distortion by a factor of 4. With a 1 bit
description, the best expected squared error is σ2
s /4 = 0.25σ2. We can compare this
distortion with the quantization and de-quantization function given in Eq. (1.26).

14
1 Information Theory
According to the reproduction points, the expected distortion can be calculated as
(π −2)σ2
s /π ≈0.3633σ2
s . Obviously, it is larger than the distortion given by Eq.
(1.44). This is because the result in Eq. (1.44) is derived from the long block length.
However, the result in Eq. (1.26) only considers one random variable. Therefore, the
rate distortion limit R(D) should be achieved by considering long block lengths.
1.3 Channel Coding
As shown in Figure 1.1, after source coding, coded source X no longer contains any
redundancy. If X is a k-dimensional bit vector, the space 2k will be fully occupied.
Every value represents a valid message. It is very sensitive to channel noise because
any corrupted bit in x will change x to another valid message. In this case, it is
difﬁcult to tell which x is really sent out. Therefore, in channel coding X is mapped
into some sequence Y of channel codewords. Y is usually redundant because there
are no other valid codewords in a neighboring space of every valid codeword. ˆY is
the output of the channel. The output sequence ˆY is random but has a distribution that
depends on the input sequence Y. From the output sequence, we attempt to recover
the transmitted source.
Each of the possible input sequences induces a probability distribution on the
output sequences. Since two different input sequences may give rise to the same
output sequence, the inputs are confusable. In channel coding, we try to choose a
non-confusable subset of input sequences so that with high probability, there is only
one highly likely input that could cause a particular output. We can then reconstruct
the input sequence at the output with a negligible probability of error.
1.3.1 Capacity
We deﬁne a discreet channel to be a system consisting of an input alphabet Y and
output alphabet
ˆ
Y and a probability transition matrix p(y | ˆy) that expresses the
probability of observing the output symbol ˆy given that we send the symbol y. The
channel is said to be without memory if the probability distribution of the output
depends only on the input at that time and is conditionally independent of previous
channel inputs or outputs.
Deﬁnition 7 The channel capacity of a discrete channel without memory is deﬁned
as
C = max
p(y) I(Y; ˆY),
(1.45)
where the maximum is taken over by all possible input distributions p(y).

1.3 Channel Coding
15
The channel capacity is actually evaluated by the entropy H(Y). The entropy is re-
duced from H(Y) to I(Y; ˆY) = H(Y)−H(Y | ˆY) because we have ˆY. In other words,
the entropy reduction is equal to what we get from the channel.
An intuitive idea is described here about why we can transmit C bits of informa-
tion over a channel. The basic idea is that, for large block lengths, every channel
looks like the noisy typewriter channel and the channel has a subset of inputs that
produce essentially disjointed sequences at the output. For each input n-sequence,
we wish to ensure that no two Y sequences produce the same ˆY output sequence.
Otherwise, we will not be able to decide which Y sequences was sent. The total
number of possible ˆY sequences is ≈2nH(ˆY). This set has to be divided into sets of
size 2nH(ˆY|Y) corresponding to the different input Y sequences. The total number of
disjointed sets is less than or equal to 2n(H(ˆY)−H(ˆY|Y)) = 2nI(Y;ˆY). Hence, we can send
at most ≈2nI(Y;ˆY) distinguishable sequences of length n.
The most important continuous alphabet channel is the Gaussian channel. This is
a time discrete channel with output ˆYi at time i, where ˆYi is the sum of the input Yi and
the noise Zi. The noise Zi is drawn i.i.d. from a Gaussian distribution with variable
σ2. Thus
ˆYi = Yi +Zi, Zi ∼N(0,σ2).
(1.46)
The noise Zi is assumed to be independent of the signal Yi. Without further con-
ditions, the capacity of this channel may be inﬁnite. If the noise variance is zero,
then the receiver receives the transmitted symbol perfectly. If the noise variance is
nonzero and there is no constraint on the input, we can choose an inﬁnite subset of
inputs arbitrarily far apart, so that they are distinguishable at the output with an ar-
bitrarily small probability of error. Such a scheme has an inﬁnite capacity as well.
Thus if the noise variance is zero or the input is unconstrained, the capacity of the
channel is inﬁnite.
The most common limitation on the input is an energy or power constraint. We
assume an average power constraint. For any codeword (y1,y2,...,yn) transmitted
over the channel, we require
1
n
n
∑
i=1
y2
i ≤P.
(1.47)
This communication channel models many practical channels, including radio and
satellite links. The additive noise in such channels may be due to a variety of reasons.
However, by the central limit theorem, the cumulative effect of a large number of
small random effects will be approximately normal, so the Gaussian assumption is
valid in a large number of situations.
The information capacity of the Gaussian channel with power constraint P is
C =
max
p(y):EY 2≤PI(Y; ˆY).
(1.48)

16
1 Information Theory
We can calculate the information capacity as follows. Expanding I(Y; ˆY), we have
I(Y; ˆY) = h(ˆY)−h(ˆY | Y),
= h(ˆY)−h(Y +Z | Y),
= h(ˆY)−h(Z | Y),
= h(ˆY)−h(Z),
(1.49)
since Z is independent of Y. Now, h(Z) = 1/2log2πeσ2. Also, we have
E ˆY 2 = E(Y +Z)2 = EY 2 +2EYEZ +EZ2 = P+σ2,
(1.50)
since Y and Z are independent and EZ = 0. Given E ˆY 2 = P + σ2, the entropy of ˆY
is bounded by 1/2log2πe(P+σ2) because the normal maximizes the entropy for a
given variance. Applying this result to bound the mutual information, we obtain
I(Y; ˆY) = h(ˆY)−h(Z),
(1.51)
≤1
2 log2πe(P+σ2)−1
2 log2πeσ2,
(1.52)
= 1
2 log(1+ P
σ2 ).
(1.53)
Hence the information capacity of the Gaussian channel is
C =
max
p(y):EY 2≤PI(Y; ˆY) = 1
2 log(1+ P
σ2 ).
(1.54)
1.3.2 Coding Theorem
An (M,n) code for the channel (Y , p(ˆy | y), ˆ
Y ) consists of the following:
1. An index set {1,2,··· ,M}.
2. An encoding function Y n : {1,2,··· ,M} →Y n, yielding codewords Y n(1),
Y n(2), ···, Y n(M). The set of codewords is called the codebook.
3. A decoding function g :
ˆ
Y n →{1,2,··· ,M}, which is a deterministic rule to
assign a guess to each possible received vector.
Let
λi = Pr(g(ˆY n) ̸= i | Y n = Y n(i)) = ∑
ˆyn
p(ˆyn | yn(i))I(g(ˆyn) ̸= i)
(1.55)
be the conditional probability of error given that index i was sent, where I() is the in-
dicator function. The maximal probability of error λ (n) for an (M,n) code is deﬁned
as
λ (n) =
max
i∈{1,2,···,M}λi.
(1.56)

1.3 Channel Coding
17
The rate R of an (M,n) code is
R = logM
n
.
(1.57)
Shannon’s second theorem states the relation between channel capacity and the
maximum transmitted rate.
Theorem 1.3. All rates below capacity C are achievable. Speciﬁcally, for every rate
R < C, there exists a sequence of (2nR,n) codes with maximum probability of error
λ (n) →0. Conversely, any sequence of (2nR,n) codes with λ (n) →0 must have R ≤C.
This is the Theorem 8.7.1 proved by Cover and Thomas [5]. The channel coding the-
orem promises the existence of block codes that will allow us to transmit informa-
tion at rates below capacity with an arbitrarily small probability of error if the block
length is large enough. In this chapter, we will not discuss the capacity-achieving
channel codes. Instead, we will use simple Hamming codes to further explain chan-
nel coding.
1.3.3 Hamming Codes
The object of channel coding is to introduce redundancy so that even if some of the
information is lost or corrupted, it will still be possible to recover the message at
the receiver. The most obvious channel coding scheme is to repeat information. For
example, to send a 1, we send 11111, and to send a 0, we send 00000. This scheme
uses 5 symbols to send 1 bit, and therefore has a rate of 1/5 bits per symbol. If 3 or
more bits are 1, we decode the block as a 1; otherwise we decode it as 0. An error
occurs if and only if more than 3 of the bits are changed. By using longer repetition
codes, we can achieve an arbitrarily low probability of error. But the rate of the code
also goes to zero with block length. It is not a very useful code.
Hamming codes are a family of linear error-correcting block codes that general-
ized the Hamming (7,4) code invented by Richard Hamming in 1950 [7]. A (7.4)
Hamming code produces 7 bits of output for every 4 bits of input. Hamming codes
are linear block codes, which means that the encoding operation can be described in
terms of a 4×7 generator matrix, such as
G =


1 1 0 1 0 0 0
0 1 1 0 1 0 0
0 0 1 1 0 1 0
0 0 0 1 1 0 1

.
(1.58)
The codewords are obtained as a linear combination of the rows of G, where all the
operations are computed as modulo 2 in each vector element. That is, the code is the
row space of G. For a source vector x = [x1,x2,x3,x4], the codeword is
y = xG.
(1.59)

18
1 Information Theory
For example, if x = [0,0,1,1], the codeword is
y = [0,0,1,1,0,1,0]+[0,0,0,1,1,0,1] = [0,0,1,0,1,1,1].
All codewords for this code are
[0,0,0,0,0,0,0],[1,1,0,1,0,0,0],[0,1,1,0,1,0,0],[1,0,1,1,1,0,0]
[0,0,1,1,0,1,0],[1,1,1,0,0,1,0],[0,1,0,1,1,1,0],[1,0,0,0,1,1,0]
[0,0,0,1,1,0,1],[1,1,0,0,1,0,1],[0,1,1,1,0,0,1],[1,0,1,0,0,0,1]
[0,0,1,0,1,1,1],[1,1,1,1,1,1,1],[0,1,0,0,0,1,1],[1,0,0,1,0,1,1]
.
(1.60)
It can be veriﬁed that the number of different bits between any two codewords is 3.
This is known as Hamming distance. Therefore, Hamming (7,4) codes can detect up
to two bits of error and correct up to one bit of error.
Every (n,k) linear block code has associated with a (n −k) × n matrix H called
the parity check matrix, which has the property
yHT = 0,
(1.61)
if and only if the vector y is a codeword. The parity check matrix is not unique. For
the generator G of Eq. (1.58), the parity check matrix can be written as
H =


1 0 1 1 1 0 0
0 1 0 1 1 1 0
0 0 1 0 1 1 1

.
(1.62)
It can be veriﬁed that GHT = 0. The matrix H is expressed in terms of its column as
H = [h1 h2 h3 h4 h5 h6 h7].
(1.63)
We can observe that the columns of H consist of the binary representations of the
numbers 1 through 7, though not in numerical order. On the basis of this observation,
we can generalize other Hamming codes. Hamming codes of length n = 2m−1 and
dimension k = 2m−m−1 exist for every m ≥2, having parity check matrices whose
columns are binary representations of the numbers from 1 through n.
Supposing that a codeword y is sent and the received vector is
ˆy = y+z (addition modulo 2).
(1.64)
The decoding computes the syndrome
d = ˆyHT = (y+z)HT = zHT.
(1.65)
Because of the property in Eq. (1.61), the syndrome depends only on the error Z and
not on the transmitted codeword. If d = 0, then the decoded codeword is y. If d ̸= 0,
then let i denote the column of H, which is equal to dT. There is an error in position

1.4 Joint Source and Channel Coding
19
i of ˆy. The decoded codeword is y = ˆy + zi, where zi is a vector which is all zeros
except for a 1 in the ith position.
Hamming codes demonstrate how channel coding works in communication. Al-
though Hamming codes are very simple in both encoding and decoding, they are
not capacity-approached channel codes. In other words, using Hamming codes can-
not approach the channel capacity. The capacity-approached channel codes will be
discussed in Chapter 3.
1.4 Joint Source and Channel Coding
It is now time to combine the two main results that we have so far: source coding
(R > H: Theorem 1.1) and channel coding (R < C: Theorem 1.3). Is the condition
H < C necessary and sufﬁcient for sending a source over a channel?
For example, consider sending digital image or video over a discrete channel that
is without memory. We could design a code to map the sequence of visual samples
directly into the input of the channel, or we could compress the visual samples into its
most efﬁcient representation, then using the appropriate channel code to send it over
the channel. It is not immediately clear that we are not losing something by using the
two-stage method, since the data compression does not depend on the channel and
the channel coding does not depend on the source distribution.
To answer this question, let us take the scenario shown in Figure 1.1. We have
a source S that generates symbols from the alphabet A . It is from a ﬁnite alphabet
and satisﬁes the AEP. The sequence of symbols Sn = {S1,S2,··· ,Sn} is sent over
the channel so that the receiver can reconstruct the sequence. Assume the one stage
coding is considered. We map the sequence onto a codeword Y n(Sn) and send the
codeword over the channel. The receiver looks at his received sequence ˆY n and makes
an estimation ˆSn of the sequence Sn that was sent. The receiver makes an error if
ˆSn ̸= Sn. We deﬁne the probability of error P(n)
e
as
P(n)
e
= Pr(Sn ̸= ˆSn) = ∑
ˆyn ∑
sn p(sn)p(ˆyn | yn(sn))I(g(ˆyn) ̸= sn),
(1.66)
where I is the indicator function and g(ˆyn) is the decoding function.
The joint source channel coding theorem is stated here:
Theorem 1.4. If S1,S2,··· ,Sn is a ﬁnite alphabet stochastic process that satisﬁes the
AEP, then there exists a source channel code with P(n)
e
→0 if H(S) < C. Conversely,
for any stationary stochastic process, if H(S) >C, the probability of error is bounded
away from zero, and it is not possible to send the process over the channel with an
arbitrarily low probability of error.
This theorem is Theorem 8.13.1 proved by Cover and Thomas [5]. It was proven us-
ing the two-stage method. In other words, the source sequence Sn is ﬁrst compressed
into its most efﬁcient representation and then the coded source sequence is protected

20
1 Information Theory
by the appropriate channel code. If the source sequence is AEP and the block length
n is large enough, the two-stage method is as good as any other method of trans-
mitting information over a noisy channel. It is known as Shannon’s separate source
channel.
This separate result has some important practical implications. It implies that we
can consider the design of a communications system as a combination of two parts,
source coding and channel coding. We can design source codes for the most efﬁcient
representation of data. We separately and independently design channel codes ap-
propriate for the channel. The combination will be as efﬁcient as anything we could
design by considering both problems together. This offers an enormous reduction in
complexity. Therefore, in practical applications, source coding and channel coding
are often studied separately.
However, it may be appropriate to point out that this is not always true. There are
examples of multiuser channels where decomposition breaks down. Another simple
example is that of sending English text over an erasure channel. We can look for
the most efﬁcient binary representation of the text and send it over the channel. But
the errors will be very difﬁcult to decode. If however we send the English text di-
rectly over the channel, we can lose up to roughly half the letters and yet be able to
make sense of the message. A similar phenomenon happens on the analog and digital
systems for TV transmission.

Chapter 2
Hybrid Video Coding
Image and video compression is a concrete application of Shannon’s source coding.
For example, the entropy coding used in image and video compression is variants
of Huffman coding and arithmetic coding that were discussed in Sections 1.2.1 and
1.2.2, respectively. The quantization is also developed from Shannon’s rate distor-
tion theory that was discussed in Section 1.2.3. However, the redundancy of image
and video is more complicated than the sources studied in information theory. It is
hard to characterize it by simple statistical models. Therefore, more compression
technologies have to be developed to handle the complicated redundancy.
2.1 Hybrid Coding Framework
Let us take video as an example to discuss redundancy in visual data. Video con-
sists of a series of continuously captured pictures. Each individual picture can be
viewed as an image or photo. Visual data typically contains four different types of
redundancies that are summarized as follows.
1. Temporal redundancy — Adjacent pictures look very similar. The differences
among them are often caused by a camera moving and/or moving objects in the
scene. Most background regions are the same after compensating for camera
motion. Moving objects can be accommodated by some motion models.
2. Spatial redundancy — Neighboring pixels in a picture have similar values be-
cause the captured objects and background in a scene usually have some texture
consistent regions. The strong correlation among pixels indicate that they can be
efﬁciently represented in frequency domain.
3. Visual redundancy — Human visual system (HVS) is insensitive to a certain loss
in visual data. It indicates that we can change visual data to some extent so that it
can be compressed more efﬁciently. The distortion incurred by such information
loss is often invisible to HVS.
21

22
2 Hybrid Video Coding
Transform
Quantization
+
Reconstructed
Picture
Motion 
Estimation
Pictures
-
Motion
Compensation
Entropy 
Coding
Inverse
Transform
De-
Quantization
Compressed 
Stream
Motion Vectors
+
Figure 2.1 The framework of hybrid video coding.
4. Statistic redundancy — The symbols that describe visual information are not
random. They present a strong nonuniform distribution. According to what we
have discussed in Section 1.2, they can be efﬁciently represented by source cod-
ing.
All image and video compression schemes are designed in an effort to fully ex-
ploit the four types of redundancies. The target is to minimize distortion for a given
compression rate or minimize compression rate for a given distortion. From the ear-
liest video compression standard H.261 to the latest standard High Efﬁcient Video
Coding (HEVC), although compression technologies have been developed over sev-
eral decades and coding efﬁciency has improved more than ten times, they share
the same framework, as shown in Figure 2.1. This framework is featured in hybrid
motion compensation and transform. So it is called hybrid video coding.
The temporal redundancies among video frames are exploited by motion estima-
tion (ME) and motion compensation (MC). In the hybrid coding framework, every
coded picture has to be reconstructed at both the encoder and the decoder. Further-
more, the reconstructed picture is required to be exactly the same at both the en-
coder and the decoder. It is an important assumption in hybrid video coding. For
every input block, the motion estimation module in Figure 2.1 will ﬁnd the most
matched block in the reconstructed picture and record the motion vector (MV).
The motion compensation module takes the MVs and the reconstructed picture to
generate a predictive picture. After subtracting the predictive picture, the input pic-
ture becomes a residual picture that is much easier to compress than the original
picture.
The process of motion estimation and motion compensation is shown in Figure
2.2. Figure 2.2a is one input picture in the Foreman sequence. The reconstructed
picture is shown in Figure 2.2b. We can observe that the reconstructed picture has
some slight block artifacts because of compression. Although the input picture and
the reconstructed picture look similar, the head in the input picture has a rotation with
respect to the reconstructed picture. Figure 2.2c shows the moving displacement of
every input 8 × 8 block. It is a 2D MV, whose direction indicates the moving direc-
tion and magnitude is the moving distance. Figure 2.2d is the prediction by mapping

2.1 Hybrid Coding Framework
23
(a) Input picture
(b) Reconstructed picture
(c) Motion vectors
(d) Prediction
Figure 2.2 An example of motion estimation and compensation.
the corresponding blocks in the reconstructed picture toward the input picture. We
can observe that the prediction is much more similar to the input picture than the
reconstructed picture.
For the ﬁrst picture, there is no reconstructed picture available and thus it is com-
pressed directly without motion compensation. This picture is called an intra picture.
Correspondingly, the pictures coded by using motion compensation are called inter
pictures. As we have discussed, neighboring pixels have similar values. For inter pic-
tures, although they have been predicted by the reconstructed pictures, neighboring
residual pixels still have similar values, as shown in Figure 2.3a. The spatial redun-
dancies are exploited by transform. Discrete cosine transform (DCT) is widely used
in most existing image and video coding standards. Its transform matrix T is obtained
as a function of cosines
[ti,j] =







r
1
N cos (2j +1)iπ
2N
i = 0, j = 0,1,··· ,N −1
r
2
N cos (2j +1)iπ
2N
i = 1,2,··· ,N −1, j = 0,1,··· ,N −1
.
(2.1)
The transform tries to describe visual data in frequency domain instead of pixel
domain. The result of 8 × 8 DCT transform on the motion compensated picture

24
2 Hybrid Video Coding
(a) Residual picture after motion com-
pensation
(b) The results of 8 × 8 DCT transform
Figure 2.3 An example of DCT transform on a motion compensated picture.
is shown in Figure 2.3b. We observe that every block in the residual picture has a
distributed energy. After DCT transform, the energy of every block has been con-
centrated to the upper left region. The DCT coefﬁcients in the upper left region are
of low frequency, whereas the coefﬁcients in the bottom right region are of high
frequency. In other words, the energy of every block becomes more compact after
transform. This is convenient for subsequent quantization and entropy coding.
Quantization in the hybrid video coding exploits visual redundancy. After DCT
transform, each coefﬁcient needs more than 10 bits to represent. The quantization
objective is to map DCT coefﬁcients from a large alphabet to a small alphabet. A
simple uniform scalar quantization is shown in Figure 2.4. For all input coefﬁcients,
Input
Output
Δ 
2Δ 
3Δ 
-3Δ 
-2Δ 
-Δ 
-Δ/2 
-3Δ/2 
-5Δ/2 
-7Δ/2 
7Δ/2 
5Δ/2 
3Δ/2 
Δ/2 
Figure 2.4 A scalar quantization.

2.1 Hybrid Coding Framework
25
(a) An 8 × 8 DCT block
(b) Scan order
Figure 2.5 The ZigZag scan.
the minimum unit is quantized to ∆. The parameter ∆is selected according to the
compression rate. If the compression rate is low, the parameter ∆is large and the
reconstructed pictures have large distortion. If the compression rate is high, the pa-
rameter ∆is small and we cannot observe the compression distortion. In addition,
the quantization in Figure 2.4 assumes that the coefﬁcients have uniform distribution.
If the distribution is different (e.g., Gaussian), the different quantization needs to be
designed.
In hybrid video coding, every 2D block has to be converted into a 1D array by the
ZigZag scan. To clearly explain this procedure, Figure 2.5a is an 8 × 8 DCT block
after quantization. In statistics, DCT coefﬁcients have a larger amount of energy
in low frequency than in high frequency. After quantization, low-frequency coefﬁ-
cients have a large probability as nonzero. Many high-frequency coefﬁcients may
be quantized to zero. Therefore, the scan order is a ZigZag from low frequency to
high frequency as shown in Figure 2.5b. For the DCT block in Figure 2.5a, after the
ZigZag scan the array of DCT coefﬁcients is
9, −1, 0, 0, 0, 0, 2, 0, 0, 2, 0, ··· , 0, −5, 0, ··· , 0, 2, 0, ··· , 0.
Finally, the 1D array is organized into (run, level) symbols. run indicates the num-
ber of continuous zeros and level indicates the value of nonzero coefﬁcients. For our
example, the generated symbols are
(0,9),(0,1),(4,2),(2,2),(18,5),(12,2).
The distribution of symbols are not uniform. Therefore, a VLC (similar to Huffman
coding) is designed for the symbols or an arithmetic code is selected. The sign bit
of every nonzero coefﬁcient is coded with the symbol. In addition to the compressed
coefﬁcients, motion vectors and some metadata (like sequence headers, frame head-
ers, and block headers) are also compressed by VLC. All these parts form the ﬁnal
compressed stream.

26
2 Hybrid Video Coding
2.2 Technical Evolution
Although the framework of hybrid video coding has changed little in the past few
decades, compression technologies of motion compensation, transform, quantiza-
tion, and entropy coding have been continuously improving. Furthermore, new com-
pression technologies such as intra prediction, deblocking ﬁltering, and sample adap-
tive offset (SAO) ﬁltering are introduced into the framework. Next, we will compare
the compression technologies used in H.261 [8], MPEG-1 [9], MPEG-2 [10], MPEG-
4 [11], H.264 [12, 13], and HEVC [14]. By the comparison, we can clearly see the
trend of the developments on video compression.
2.2.1 H.261
H.261 is the ﬁrst standard based on the hybrid video coding framework, ratiﬁed in
November 1988. It was originally designed for transmission over Integrated Services
Digital Network (ISDN) lines on which data rates are multiples of 64 kbit/s. The in-
put video resolutions are common intermediate format (CIF) and QCIF using a 4:2:0
format. The compression of H.261 is very simple and is similar to the framework
shown in Figure 2.1.
In H.261, only one reconstructed picture is used as reference for motion com-
pensation. It is known as predictive (P) picture coding. The motion estimation and
compensation are carried out at 8 × 8 blocks and each block has one motion vector at
the integer pixel precision. Transform is the 8 × 8 DCT. H.261 designed 32 different
quantizers. One quantizer with a step size of 8 is reserved for the intra DCT coefﬁ-
cients. The other 31 quantizers with a step size of an even value between 2 and 62
are used for other coefﬁcients. In entropy coding, the 20 most commonly occurring
(run, level) symbols are coded with a single VLC. All other combinations are coded
with a 20-bit codeword, made up of a 6-bit escape code, a 6-bit run, and an 8-bit
level. One difference from the framework shown in Figure 2.1 is that H.261 uses a
loop ﬁlter. Before the temporal prediction is subtracted from the input picture, it can
be ﬁltered by a 2D ﬁlter with the 1D coefﬁcients as [1,2,1]/4.
2.2.2 MPEG-1
MPEG-1 is a standard for lossy compression of video and audio. It is designed to
compress video home system (VHS) quality raw digital video and compact disc (CD)
audio down to 1.5 Mbit/s without excessive quality loss. Its video compression ratio
is about 26:1. MPEG-1 was the ﬁrst video standard to be widely adopted by the
industry. It provides video and audio compression for CD.

2.2 Technical Evolution
27
Compared with H.261, MPEG-1 ﬁrst introduces bidirectionally predictive coding.
It is known as B picture coding, where two reconstructed pictures are used for mo-
tion compensation. One is the most recently reconstructed picture and another is the
closest future reconstructed picture. By using both past and future pictures for pre-
diction, MPEG-1 can get better coding performance. MPEG-1 also organizes four
luma 8 × 8 blocks and two 8 × 8 chroma blocks as a 16 × 16 macroblock (MB).
It was used as a basic coding unit until H.264/MPEG-4 Advanced Video Coding
(AVC). P pictures have 1 motion vector per macroblock. B pictures have two motion
vectors: one for the previous picture and one for the future picture. Motion vector
is allowed at the half-pixel precision. The transform is still the 8 × 8 DCT. A 8 ×
8 quantization matrix is introduced in MPEG-1. For the same step size, it enables
low-frequency coefﬁcients with low distortion and high-frequency coefﬁcients with
relative large distortion. The entropy coding uses a 2D Huffman table to code (Run,
Level) symbols.
2.2.3 MPEG-2
MPEG-2 is widely used as the format of digital television signals that are broadcast
by terrestrial (over-the-air), cable, and satellite systems. It also speciﬁes the format of
movies and other programs that are distributed on digital video disk (DVD) and simi-
lar discs. TV stations, TV receivers, DVD players, and other equipment are often de-
signed to this standard. MPEG-2 is forward compatible with MPEG-1. It means that
MPEG-1 streams observing typical constraints (e.g., in frame sizes and data rates)
can be decoded by MPEG-2 decoders. The VLC tables were extended in MPEG-2
for better compression performance in higher data rates and resolutions.
In the TV industry, in order to reduce the amount of data each picture is separated
into two ﬁelds: the top ﬁeld, which is comprised of the odd numbered horizontal
lines, and the bottom ﬁeld, which is the even numbered lines. The two ﬁelds are
displayed alternately with the lines of one ﬁeld interleaving between the lines of the
previous ﬁeld. This format is called interlaced video. Two successive ﬁelds can be
combined to a picture. To support the properties of interlaced video, MPEG-2 speci-
ﬁes different methods for ﬁeld/picture adaptive motion compensation (picture based,
ﬁeld based, and dual prime prediction modes), as well as switching between ﬁeld-
/picture-wise DCT. Further, it is possible to switch into a 16 × 8 prediction mode,
where separate motion vectors can be deﬁned for the top and bottom halves of a MB.
MPEG-2 is the ﬁrst standard to support scalable video coding (SVC), which pro-
vides signal-to-noise ratio (SNR) scalability and spatial scalability over a limited
number of layers. In this context, the bitstream is subdivided into two or three parts,
which means that by retaining or receiving only core parts of the stream, it is possible
to reconstruct pictures at lower quality and resolution. To encode DCT coefﬁcients
related to different resolutions, differently optimized variable length codes are pro-
vided in the spatial scalable mode. MPEG-2 also deﬁnes a method of temporal scal-
ability, which allows prediction of additional inserted pictures either from the base
layer sequence or from an enhancement layer sequence.

28
2 Hybrid Video Coding
2.2.4 MPEG-4
MPEG-4 is an innovative video standard. It tries to make a breakthrough on the
hybrid video coding framework by considering video contents. The most famous
concept introduced by MPEG-4 is object-based coding, although it has never been
adopted by the industry. MPEG-1 and MPEG-2 are only able to encode rectangular
video pictures. However, MPEG-4 extends the encoding of video objects, which can
have arbitrary shapes. Video scenes can be composed from several objects, which
may change in position, appearance, size, and so on, independent of each other. Al-
though the compression of visual objects is novel and the compression performance
is good, until now it is still hard to extract visual objects from video accurately and
automatically.
Compared with MPEG-1 and MPEG-2, MPEG-4 has made some progress on
the hybrid video coding framework. First, a macroblock can have up to four motion
vectors. In other words, each 8 × 8 block can have its own motion vector. The motion
vector has the quarter-pixel precision. Both of them greatly improve the performance
of motion compensation. Second, MPEG-4 introduces global motion compensation,
which allows to express the effect of camera motion by using only a small number of
parameters. Third, different VLC tables can be selected where the codes are designed
for more efﬁciently encoding at ranges of lower or higher rates. The selection is
controlled by the encoder and depends on the target rate. Finally, the direct mode can
determine the motion vectors within B pictures by inferencing from the co-located
motion vectors in P picture without rate overhead.
In addition to layered scalable video coding, MPEG-4 also speciﬁes ﬁne granu-
larity scalable (FGS) coding. It enables the compressed stream to be truncated at any
length, which better ﬁts the bandwidth ﬂuctuation in the Internet. In order to guar-
antee that the reconstructed pictures are the same at both the encoder and decoder,
MPEG-4 FGS only uses the base layer for motion compensation in the enhancement
layer. Therefore, the coding performance of MPEG-4 FGS is poor.
2.2.5 H.264/MPEG-4 AVC
H.264/MPEG-4 Advanced Video Coding (AVC) is a video coding standard devel-
oped by the ITU-T Video Coding Experts Group (VCEG) together with the ISO/IEC
JTC1 Moving Picture Experts Group (MPEG). The project partnership effort is
known as the Joint Video Team (JVT). The ITU-T H.264 standard and the ISO/IEC
MPEG-4 AVC standard (formally, ISO/IEC 14496-10 MPEG-4 Part 10, Advanced
Video Coding) are jointly maintained so that they have identical technical contents.
Although H.264 still follows the hybrid video coding framework, many compression
technologies have been signiﬁcantly improved.
In motion compensation, H.264 uses variable block sizes of 16 × 16, 16 × 8,
8 × 16, 8 × 8, 8 × 4, 4 × 8, or 4 × 4. The number of motion vectors for a

2.2 Technical Evolution
29
macroblock varies from 1 to 16. Motion vectors have the quarter-pixel precision with
high-quality interpolation ﬁlters. Multiple reference picture prediction allows one to
deﬁne references for prediction of any macroblock from one of up to ﬁve previously
decoded pictures. The B-type slices are generally compared to previous standards,
denoted as bipredictive instead of bidirectional. This allows one to deﬁne structures
of prediction of individual regions from two previous or two subsequent pictures,
provided that a causal processing order is available. Furthermore, the prediction of
B pictures from other B pictures is possible, which allows implementation of a B-
picture pyramid. Different weighting factors can be used for the reference pictures in
the B pictures.
In transform, H.264 uses an integer transform of block size 4 × 4 and 8 × 8. The
transform design is not exactly a DCT, but could be interpreted as an integer ap-
proximation. As an entire processing of transform and quantization, implementation
by 16-bit integer arithmetic precision is possible both for encoding and decoding. In
contrast to previous standards based on the DCT, there is no dependency on a ﬂoat-
ing point implementation, such that no drift between the encoder and decoder picture
representations can occur in the error-free transmission.
H.264 deﬁnes two different entropy coding methods, one of which is context
adaptive variable length coding (CAVLC), and the other is context adaptive binary
arithmetic coding (CABAC). Both are universally applicable to all elements of the
code syntax, which are based on a systematic construction of VLC tables. By proper
deﬁnition of the contexts, it is possible to exploit nonlinear dependencies between
the different elements to be encoded. CABAC is a coding method for binary signals,
and a binarization of multilevel values such as transform coefﬁcients and motion vec-
tors must be performed before CABAC can be applied. Unary codes, truncated unary
codes, Exp-Golomb codes, or ﬁxed-length codes are used for this purpose. Four dif-
ferent basic context models are deﬁned, where the usage depends on the speciﬁc
values to be encoded.
H.264 is the ﬁrst to introduce prediction in intra coding, which is performed by
ﬁrst predicting the entire block from boundary pixels of adjacent blocks. Prediction
is possible for 4 × 4, 8 × 8, or 16 × 16 blocks, where for the 16 × 16 case only
horizontal, vertical, direct current (DC), and planar prediction are allowed. In the 8
× 8 and 4 × 4 cases, nine prediction types are supported (DC and eight directional
spatial prediction modes).
Unlike the loop ﬁlter in H.261, an adaptive deblocking ﬁlter (DBF) is applied
in the prediction loop. The adaptation process of the ﬁlter is nonlinear, with the
low-pass strength of the ﬁlter steered by the quantization parameter and by syntax
under the control of the encoder. Further parameters considered in the ﬁlter selec-
tion are the difference between motion vectors at the respective block edges, the
coding mode used (e.g., stronger ﬁltering is made for the intra mode), the presence
of coded coefﬁcients, and the differences between reconstruction values across the
block boundaries.

30
2 Hybrid Video Coding
2.2.6 HEVC
High Efﬁcient Video Coding (HEVC) is a video compression standard, a successor to
H.264/MPEG-4 AVC, under the joint development by the ISO/IEC JTC1 MPEG and
ITU-T VCEG as ISO/IEC 23008-2 MPEG-H Part 2 and ITU-T H.265. MPEG and
VCEG have established a Joint Collaborative Team on Video Coding (JCT-VC) to
develop the HEVC standard. HEVC can double the data compression ratio compared
to H.264/MPEG-4 AVC, and can support the compression of 8K ultra high deﬁnition
(UHD) video and resolutions up to 8192 × 4320.
HEVC replaces macroblocks, which were used by previous standards, with a new
coding scheme that uses larger block structures of up to 64 × 64 pixels and can better
subpartition the picture into variable sized structures. HEVC initially divides the
picture into Coding tree units (CTUs), which are then divided for each luma/chroma
component into coding tree blocks (CTBs). A CTB can be 64 × 64, 32 × 32, or 16
× 16 with a larger block size usually increasing the coding efﬁciency. CTBs are then
divided into Coding units (CUs). The arrangement of CUs within a CTB is known as
a quadtree since a subdivision results in four smaller regions. CUs are then divided
into Prediction units (PUs) of either intra-picture or inter-picture prediction, which
can vary in size from 64 × 64 to 4 × 4.
HEVC deﬁnes a signed 16-bit range for both horizontal and vertical MVs. HEVC
horizontal/vertical MVs have a range of −32768 to 32767, which given the quarter-
pixel precision used by HEVC allows for a MV range of −8192 to 8191.75 luma
samples. This compares to H.264/MPEG-4 AVC, which allows for a horizontal MV
range of −2048 to 2047.75 luma samples and a vertical MV range of −512 to
511.75 luma samples. HEVC allows for two MV modes, which are advanced mo-
tion vector prediction (AMVP) mode and merge mode. AMVP mode uses data from
the reference picture and can also use data from adjacent prediction blocks. The
merge mode allows for the MVs to be inherited from neighboring prediction blocks.
Merge mode in HEVC is similar to the skipped and direct motion inference modes
in H.264/MPEG-4 AVC but with two improvements. The ﬁrst improvement is that
HEVC uses index information to select one of several available candidates. The sec-
ond improvement is that HEVC uses information from the reference picture list and
reference picture index.
HEVC speciﬁes four Transform unit (TU) sizes of 4 × 4, 8 × 8, 16 × 16, and 32
× 32 to code the predicted residual. A CTB may be recursively partitioned into four
or more TUs. TUs use integer transform that are similar to DCT. In addition, 4 × 4
luma transform blocks that belong to an intra-coded region are transformed using an
integer transform that is derived from discrete sine transform (DST). This provides
a 1% bit-rate reduction but was restricted to 4 × 4 luma transform blocks due to
marginal beneﬁts for the other transform cases. Chroma uses the same TU sizes as
luma so there is no 2 × 2 transform for chroma.
HEVC uses a CABAC algorithm that is fundamentally similar to CABAC in
H.264/MPEG-4 AVC. CABAC is the only entropy coding method that is allowed
in HEVC, while there are two entropy coding methods allowed by H.264/MPEG-4

2.2 Technical Evolution
31
AVC. CABAC in HEVC is designed for higher throughput. For instance, the number
of context coded bins has been reduced by a factor of 8 and the CABAC bypass-mode
has been improved in terms of its design to increase throughput. Another improve-
ment with HEVC is that the dependencies between the coded data have been changed
to further increase throughput. Context modelling in HEVC has also been improved
so that CABAC can better select a context that increases efﬁciency when compared
to H.264/MPEG-4 AVC.
HEVC speciﬁes 33 directional modes for intra prediction compared to the 8 di-
rectional modes for intra prediction speciﬁed by H.264/MPEG-4 AVC. HEVC also
speciﬁes planar and DC intra-prediction modes. The intra-prediction modes use data
from neighboring prediction blocks that have been previously decoded.
HEVC speciﬁes two loop ﬁlters that are applied in order, with the DBF applied
ﬁrst and the SAO ﬁlter applied afterward. Both loop ﬁlters operate during the inter-
picture prediction loop. The DBF is similar to the one used by H.264/MPEG-4 AVC
but with a simpler design and better support for parallel processing. The SAO ﬁlter
is applied after the DBF and is made to allow for better reconstruction of the original
signal amplitudes by using offsets from a transmitted look-up table. Per CTB the
SAO ﬁlter can be disabled or applied in one of two modes: edge offset mode or
band offset mode. The edge offset mode operates by comparing the value of a pixel
to two of its eight neighbors using one of four directional gradient patterns. Based
on a comparison with these two neighbors, the pixel is classiﬁed into one of ﬁve
categories: minimum, two types of edges, maximum, or neither. For each of the ﬁrst
four categories an offset is applied. The band offset mode applies an offset based on
the amplitude of a single pixel. The pixel is categorized by its amplitude into one of
32 bands. Offsets are speciﬁed for four consecutive of the 32 bands, because in ﬂat
areas which are prone to banding artifacts, pixel amplitudes tend to be clustered in a
small range. The SAO ﬁlter was designed to increase picture quality, reduce banding
artifacts, and reduce ringing artifacts.
2.2.7 Performance versus Encoding Complexity
As we have discussed, the framework of image and video compression does not have
any signiﬁcant changes. All listed video standards share the common hybrid coding
framework. However, compression technologies have developed signiﬁcantly. The
technical evolution in different video coding standards are summarized in Table 2.1.
We clearly observe some trends on the development of compression technologies.
In all compression technologies, motion compensation plays a critical role in im-
proving coding performance. To increase the accuracy of motion compensation, the
precision of motion vectors increases from integer pixel to quarter pixel. In the cur-
rent resolution of video, 1/8 pixel precision or more of motion vectors does not bring
much beneﬁt to coding performance relative to the increased computation and mem-
ory needed for ﬁner interpolation. The block partition provides ﬂexibility to use the
number of motion vectors adaptive to video contents. We observe that they bring a

32
2 Hybrid Video Coding
Table 2.1 Technical evolution with video coding standards.
H.261 MPEG-1 MPEG-2 MPEG-4
H.264
HEVC
MVs per MB
4
1
1
4
16
16
MV precison
1
1/2
1/2
1/4
1/4
1/4
Sizes in MB
1
1
1
2
7
7
References
1
1
1
1
5
5
B-frame
No
Yes
Yes
Yes
Bipredictive
Bipredictive
Intra prediction
No
No
No
No
9
35
Large MB
No
No
No
No
No
Yes
Transforms
DCT
DCT
DCT
DCT
4 × 4, 8 × 8
4 × 4, 8 × 8,
16 × 16, 32 × 32
Integer
Integer and 4 × 4 DST
Entropy
VLC
VLC
VLC
VLC
CAVLC
CABAC
CABAC
RDO
No
No
No
No
Yes
Yes
signiﬁcant gain for H.264 and HEVC, respectively. Increasing the number of refer-
ences and ﬂexible prediction structures are also a way to improve motion compen-
sation accuracy. For videos with complicated scene changing, they may still have
potential for further optimization in the future. It is worth mentioning that the rate
for coding partitions, motion vectors, and references is no longer a small percentage
of the total rate in HEVC. It would be advantageous to be able to compress them
more efﬁciently.
Intra prediction is an important technology for improving the performance of intra
coding in H.264 and HEVC. H.264 uses only 8 directional spatial prediction modes,
while HEVC already increases the directional modes to 33. The current intra pre-
diction only exploits the correlation with neighboring pixels. It is well known that
nonlocal pixels often have stronger correlation. However, nonlocal intra prediction
introduces a relatively large overhead to describe displacement like motion vectors.
In addition, a geometric transform may be needed to better match the current block
with nonlocal blocks. It would be advantageous to develop a technology for a better
trade-off between the accuracy of nonlocal intra prediction and overhead.
The transform technologies have had few changes in standards except for trans-
form size. The entropy coding has converged to context-adaptive arithmetic coding.
HEVC discards the context-adaptive VLC because even for current mobile devices
the real-time arithmetic decoding of HD video contents is not a problem anymore.
The sample adaptive offset ﬁlter provides a new way to represent visual data. Al-
though it shows a certain performance gain in HEVC, it is still unclear if it has much
potential in future video compression. Figure 2.6 shows the performance improve-
ments with the development of these compression technologies. The vast improve-
ments in the performance of video compression are quite remarkable.
What is the basis for supporting the development of compression technologies in
the past few decades? Actually, it is the Moore’s law on computing hardware, which
states that the number of transistors on integrated circuits doubles approximately
every two years. When H.261 was speciﬁed in 1988, the fastest personal computers

2.2 Technical Evolution
33
Figure 2.6 Performance improvements from H.261 to H.264.
at that time were even slower than current mobile phones. It is impossible to specify
a video compression standard with similar computation as H.264 and HEVC. Even
if some high-computation, high-efﬁciency compression technologies were available
at that time, they could not be adopted by H.261. Therefore, every video coding
standard actually tries to achieve a better trade-off between coding performance and
coding complexity corresponding to the computation power available at that time.
The computation increasing for video encoding and decoding is not symmetric.
The increased computation for decoding is very smaller to that for encoding. It is
useful for practical applications. Some compression technologies will increase the
decoding complexity. Fractional pixel motion compensation needs to perform inter-
polation, which increases computation and memory needed in the decoder. Multiple
reference prediction mainly increases the decoding memory requirement. CABAC
is the most intensive computation in decoding. Fortunately, with the development of
computers and devices, it is not a problem to complete real-time decoding of HD
video contents compressed by H.264 and HEVC.
The encoding complexity is mainly increased by using rate-distortion optimiza-
tion to select coding parameters. We should note that H.264 already has many param-
eters for coding of a macroblock. The best way to select these parameters is to test
all combinations. Therefore, for coding a macroblock, we have to evaluate hundreds
of different coding selections. Even for H.264, many current software and hardware
encoding implementations have to simplify the rate-distortion optimization. It will
bring a certain performance loss. However, the parameter selection in HEVC is per-
formed at 64 × 64 blocks. There are many thousands of parameter combinations.
The fast and high-performance encoding is still a great challenge. Although HEVC’s
reference software is not yet optimized, it takes almost one day to compress a 10-
second HD video sequence using even the fastest computer.
One solution to the above problem is to use many cores. From the research on
H.264 encoding optimization, it is very difﬁcult to use many cores for H.264 en-
coding because of the strong dependency among macroblocks. Furthermore, the

34
2 Hybrid Video Coding
macroblock size is a bit small for using many cores for coding of a macroblock
in parallel. Fortunately, HEVC has changed the coding unit from a macroblock to
a 64 × 64 block. It has the potential to use hundreds or more cores evaluating the
coding parameter combinations within a 64 × 64 block. The latest graphic process-
ing unit (GPU) NVIDIA GeForce GTX 690 has provided us with 3076 parallel cores
for the task. The next urgent problem for video encoding optimization will be how
to evaluate the coding parameter combinations in parallel. It is not easy because of
the structure of the GPU, where 192 cores are organized as a group sharing the same
program and only 64KB cache memory is available for a group of cores.
2.3 H.264 Standard
Here we will discuss the key compression technologies in H.264, which were brieﬂy
introduced in Section 2.2.5. This section is organized according to the contents in the
H.264 overview paper [13]. The framework of H.264 is shown in Fig 2.7. Compared
with the framework of hybrid video coding shown in Figure 2.1, it includes intra
prediction and DBF. They are a complement to the framework of hybrid video cod-
ing. The module of coder control is the rate-distortion optimization, which selects
the coding parameters macroblock by macroblock.
2.3.1 Motion Compensation
In H.264, each 16 × 16 macroblock can be partitioned into block sizes of 16 × 16,
16 × 8, 8 × 16, and 8 × 8 pixels, as shown in the upper part of Figure 2.8. In case
Figure 2.7 The framework of H.264.

2.3 H.264 Standard
35
0
0
1
0
1
0
1
2
3
0
0
1
0
1
0
1
2
3
16x16
16x8
8x16
8x8
8x8
8x4
4x8
4x4
Macroblock partition
Sub-macroblock partition
Figure 2.8 The macroblock and sub-macroblock partitions as deﬁned in H.264.
partitions with 8 × 8 pixels are chosen, the block can be further partitioned into 8
× 4, 4 × 8, and 4 × 4, as shown in the bottom part of Figure 2.8. In other words,
one macroblock in H.264 is allowed to have no more than 16 motion vectors. In
H.264, no more than ﬁve references are allowed. The reference index parameter is
transmitted for each motion-compensated 16 × 16, 16 × 8, 8 × 16, or 8 × 8 luma
block. Motion compensation for smaller regions than 8 × 8 uses the same reference
index for prediction of all blocks within the 8 × 8 region.
We note that the ﬂexible macroblock partitions should be used with high preci-
sion of motion vectors together. The former provides a good adaptation to the com-
plicated motion and the latter can fully take advantage of using multiple motion
vectors. In H.264, the accuracy of motion compensation is in units of one quarter
of the distance between luma pixels. Therefore, H.264 needs the quarter-pixel in-
terpolation. It is completed in two steps. Half pixels are ﬁrst generated by a 6-tap
ﬁlter {1,−5,20,20,−5,1} and then quarter pixels are generated by averaging the
two nearest pixels at integer and half positions. The prediction values for the chroma
component are always obtained by bilinear interpolation.
The concept of B picture coding is generalized in H.264. The substantial differ-
ence between B and P pictures is that B pictures are coded in a manner in which
some macroblocks or blocks may use a weighted average of two distinct motion-
compensated prediction values for building the prediction signal. B pictures utilize
two distinct lists of reference pictures, which are referred to as the ﬁrst (list 0) and
second (list 1) reference picture lists, respectively. Four different types of inter-
picture predictions are supported: list 0, list 1, bipredictive, and direct prediction.
The same partitions are also applied to B picture coding.

36
2 Hybrid Video Coding
Figure 2.9 Intra 4 × 4 prediction. (a) the prediction of pixels a–p is conducted from pixels A∼Q;
(b) Eight prediction directions for a 4 × 4 block.
2.3.2 Intra Prediction
Originally, H.264 had two different intra predictions: 4 × 4 prediction and 16 × 16
prediction. The 4 × 4 prediction is well suited for coding of parts of a picture with
signiﬁcant detail. The 16 × 16 prediction is more suited for coding very smooth areas
of a picture. The 8 × 8 prediction is introduced later in the high proﬁle.
When using the 4 × 4 prediction, each 4 × 4 block is predicted from spatially
neighboring pixels as illustrated in Figure 2.9a. The 16 pixels of the 4 × 4 block
which are labeled as a∼p are predicted using prior decoded pixels in adjacent blocks
labeled as A–Q. For each 4 × 4 block, one of nine prediction modes can be utilized.
In addition to the DC prediction (where one value is used to predict the entire 4 × 4
block), eight directional prediction modes are speciﬁed as illustrated in Figure 2.9b.
Those modes are suitable to predict directional structures in a picture such as edges
at various angles. The 8 × 8 intra prediction is similar to the 4 × 4 intra prediction
expect for the larger block size.
When utilizing the 16 × 16 prediction, the whole luma component of a mac-
roblock is predicted. Four prediction modes are supported. Prediction mode 0 (ver-
tical prediction), mode 1 (horizontal prediction), and mode 2 (DC prediction) are
speciﬁed similar to the modes in the 4 × 4 prediction except that instead of 4 neigh-
bors on each side to predict a 4 × 4 block, 16 neighbors on each side to predict a 16
× 16 block are used. The prediction mode 4 is a plane prediction. It tries to generate
the prediction by using a plane approximation.
2.3.3 Transform and Quantization
In H.264, the transformation is applied to 4 × 4 blocks, and instead of a 4 × 4 DCT,
a separable integer transform with similar properties as a 4 × 4 DCT is used. The
transform matrix is given as

2.3 H.264 Standard
37
H =


1
1
1
1
2
1 −1 −2
1 −1 −1
1
1 −2
2 −1

.
(2.2)
Since the inverse transform is deﬁned by exact integer operations, inverse-transform
mismatches are avoided. Intra 16 × 16 prediction modes and chroma intra modes
are intended for coding smooth areas. For that reason, the DC coefﬁcients undergo
a second transform with the resulting in transform coefﬁcients covering the whole
macroblock. An additional 2 × 2 Hadamard transform is also applied to the DC
coefﬁcients of the four 4 × 4 blocks of each chroma component.
A quantization parameter is used for determining the quantization of transform
coefﬁcients in H.264. The parameter can take 52 values. These values are arranged
so that an increase of 1 in the quantization parameter means an increase of the quan-
tization step size by approximately 12% (an increase of 6 means an increase of the
quantization step size by exactly a factor of 2). A change of step size by approxi-
mately 12% also means roughly a reduction of bit rate by approximately 12%.
2.3.4 Entropy Coding
In H.264/AVC, two methods of entropy coding are supported. One is the CAVLC.
In CAVLC, the number of nonzero quantized coefﬁcients N, the actual values, and
positions of the coefﬁcients are coded separately. Without loss of generality, the co-
efﬁcients for a luma 4 × 4 block are assumed as follows
8,6,−2,0,−1,0,0,1,0,0,0,0,0,0,0,0.
The following data elements are used to convey information of the above quantized
coefﬁcients.
• Number of nonzero coefﬁcients N and Trailing 1s: Trailing 1s (T1s) indicate the
number of coefﬁcients with absolute value equal to 1. In the example, N = 5 and
T1s = 2. These two values are coded as a combined event. One out of four VLC
tables is used based on the number of coefﬁcients in neighboring blocks.
• Value of coefﬁcients: The T1s need only sign speciﬁcation since they all are
equal to +1 or −1. Since the coefﬁcient values have less spread for the last
nonzero coefﬁcients than for the ﬁrst one, coefﬁcient values are coded in reverse
scan order. In the example, –2 is the ﬁrst coefﬁcient value to be coded. A starting
VLC is used for that. When coding the next coefﬁcient (having a value of 6 in
the example), a new VLC may be used based on the just coded coefﬁcient. In
this way, adaptation is obtained by the use of VLC tables. Six exp-Golomb code
tables are available for this adaptation.
• Signs: For T1s, this is sent as single bits. For the other coefﬁcients, the sign bit
is included in the exp-Golomb codes.

38
2 Hybrid Video Coding
Positions of each nonzero coefﬁcient are coded by specifying the positions of 0s
before the last nonzero coefﬁcient. It is split into two parts:
• Total-zeros: This codeword speciﬁes the number of zeros between the last
nonzero coefﬁcient and its start. In the example, the Total-zeros is 3. Since it is
already known that N = 5, the number must be in the range 0–11. Fifteen tables
are available for N in the range 1–15. (If N = 16, there is no zero coefﬁcient.)
• RunBefore: In the example it must be speciﬁed how the three zeros are dis-
tributed. First the number of 0s before the last coefﬁcient is coded. In the exam-
ple the number is 2. Since it must be in the range 0–3, a suitable VLC is used.
Now there is only one 0 left. The number of 0s before the second to last coef-
ﬁcient must therefore be 0 or 1. In the example the number is 1. At this point,
there are no 0s left and no more information is coded.
The efﬁciency of entropy coding can be improved further if the CABAC is used.
On the one hand, the usage of arithmetic coding allows the assignment of a non-
integer number of bits to each symbol of an alphabet, which is extremely beneﬁcial
for symbol probabilities that are greater than 0.5. On the other hand, the usage of
adaptive codes permits adaptation to nonstationary symbol statistics. Another impor-
tant property of CABAC is its context modelling. The statistics of already coded syn-
tax elements are used to estimate conditional probabilities. These conditional prob-
abilities are used for switching several estimated probability models. In H.264, the
arithmetic coding core engine and its associated probability estimation are speciﬁed
as multiplication-free low-complexity methods using only shifts and table look-ups.
Compared to CAVLC, CABAC typically provides a reduction in bit rate between
5%–15%.
2.3.5 Deblocking Filtering
H.264 deﬁnes an adaptive in-loop deblocking ﬁlter. Figure 2.10 illustrates the princi-
ple of the deblocking ﬁlter using a visualization of a one-dimensional edge. Whether
the pixels p0 and q0 as well as p1 and q1 are ﬁltered is determined by using quan-
tization parameter (QP) dependent on thresholds α(QP) and β(QP). Thus, ﬁltering
of p0 and q0 only takes place if each of the following conditions is satisﬁed:



|p0 −q0| < α(QP)
|p1 −p0| < β(QP)
|q1 −q0| < β(QP)
,
(2.3)
where β(QP) is considerably smaller than α(QP). Accordingly, ﬁltering of p1 or q1
takes place if the corresponding following condition is satisﬁed:
|p2 −p0| < β(QP)
or
|q2q0| < β(QP).
(2.4)

2.3 H.264 Standard
39
Figure 2.10 Principle of a deblocking ﬁlter.
The basic idea is that if a relatively large absolute difference between pixels near
a block edge is measured, it is quite likely a blocking artifact and should therefore
be reduced. However, if the magnitude of that difference is so large that it cannot
be explained by the coarseness of the quantization used in the encoding, the edge
is more likely to reﬂect the actual behavior of the source picture and should not be
smoothed over. The blockiness is reduced, while the sharpness of the content is basi-
cally unchanged. Consequently, the subjective quality is signiﬁcantly improved. The
ﬁlter reduces the bit rate typically by 5%–10% while producing the same objective
quality as the nonﬁltered video.
2.3.6 Rate Distortion Optimization
As we have discussed, H.264 provides many coding parameters (intra modes, inter
modes, motion vectors, and references) to select in coding a macroblock. According
to Shannon’s rate distortion theorem discussed in Section 1.2.3, the parameter se-
lection should target the ﬁnal rate distortion optimization. Assume that the video to
code is S and the parameter set is P, for a given target rate R0, the optimization can
be formulated as
min
P D(S,P), subject to R(S,P) ≤R0.
(2.5)
Here, D(S,P) and R(S,P) are the total distortion and rate, respectively, resulting from
the quantization of S with a particular set of coding parameters P.
In practice, rather than solving the constrained problem in Eq. (2.5), an uncon-
strained formulation is employed, that is

40
2 Hybrid Video Coding
P∗= arg min
P
D(S,P)+λR(S,P).
(2.6)
λ ≥0 is the Lagrange parameter. In H.264, the Lagrange parameter for mode selec-
tion have been determined via experimental results
λmode = 0.85×2(Q−12)/3.
(2.7)
Q is the used quantizer value. The Lagrange parameter λmotion is equal to
p
λmode if
the sum of absolution difference (SAD) is used as a measure in motion search, or is
equal to λmode if the sum of squared difference (SSD) is used as a measure.
From the rate-distortion optimization, it is easy to understand why the H.264 en-
coding has high complexity. In theory, video should be encoded for any combinations
of coding parameters to get the distortion and generated rate. And then, an optimal set
of parameters is selected. However, it is impossible to check all combinations. H.264
usually use several steps to decide the coding parameters. It is a local optimization.
Even so, the encoding complexity of H.264 is high. In the practical applications, only
key parameters are selected by the rate-distortion optimization. The other parameters
are selected by only comparing the distortion.
2.4 HEVC Standard
Here we will discuss the key compression technologies in HEVC, which have been
brieﬂy introduced in Section 2.2.6. This section is organized according to the con-
tents in the HEVC overview paper [14]. The framework of the HEVC standard is
shown in Figure 2.11. Compared with the H.264 framework in Figure 2.7, only the
SAO ﬁlter module is introduced. However, since HEVC discards the macroblock
unit that was widely used in H.264 and other previous standards, instead it uses
larger block structures of up to 64 × 64 pixels and better subpartitions the picture
into variable sized structures. It greatly changes the encoding and decoding process.
2.4.1 Motion Compensation
Coding tree units (CTUs) and coding tree blocks (CTBs) — Pictures in a video se-
quence are partitioned into the same size of CTUs. Each CTU contains a luma CTB
and two chroma CTBs for the 4:2:0 format. In HEVC, the term “unit” means all color
components and the term “block” means a color component. A luma CTB covers a
square picture area of L × L pixels of the luma component and the corresponding
chroma CTBs cover L/2 × L/2 pixels of each of the two chroma components. The
value of L may be equal to 16, 32, or 64. Compared with the traditional 16×16 mac-
roblock, HEVC supports variable-size CTUs from 64 × 64 to 16 × 16. The support

2.4 HEVC Standard
41
Figure 2.11 The framework of the HEVC standard.
of CTUs larger than in previous standards is particularly beneﬁcial when encoding
high-resolution video content.
Coding units (CUs) and coding blocks (CBs) — CTUs are further partitioned
into CUs. As shown in Figure 2.12, the partitioning is achieved using quarter-tree
structures and is iterated until the size for a luma CB reaches a minimum allowed
CB size that is always 8 × 8 or larger. Thus CUs always cover a square picture
area. Intra- or inter-prediction mode is determined at the CU level. For every CU, PU
decides its prediction partition and TU decides its transform partition.
Prediction units (PUs) and prediction blocks (PBs) — As shown in Figure 2.13,
HEVC deﬁnes eight different modes for splitting a CU into PUs. Note that the parti-
tion to a CU is not recursive and is only carried out once. In intra prediction, PB size
is equal to CB size, or a CB is further partitioned into four PBs that each has their
(a)
(b)
Figure 2.12 A CTB is partitioned into CBs and TBs. Solid lines indicate CB boundaries and dotted
lines indicate TB boundaries. (a) CTB with its partitioning; (b) Corresponding quadtree.

42
2 Hybrid Video Coding
MxM
M/2xM
MxM/2
M/2xM/2
M/4xM(L)
M/4xM(R)
MxM/4(U)
MxM/4(D)
Figure 2.13 Modes for splitting a CU into PUs, subject to certain size constraints. For intra predic-
tion, only M × M and M/2 × M/2 are supported.
own intra-prediction mode. The minimum PB size is 4 × 4. In inter prediction, it is
speciﬁed whether the luma and chroma CBs are split into one, two, or four PBs. The
splitting into four PBs is allowed only when the CB size is equal to the minimum
allowed CB size. When a CB is split into four PBs, each PB covers a quadrant of the
CB. When a CB is split into two PBs, as shown in Figure 2.13, six types of splitting
are possible. Each PB is assigned one or two motion vectors and reference picture
indices.
Transform units (TUs) and transform blocks (TBs) — Only square CB and TB
partitioning are speciﬁed, where a block can be recursively split into quadrants, as
illustrated in Figure 2.12. The partitioning is represented by a residual quadtree. The
leaf node blocks resulting from the residual quadtree are the transform blocks that
are further processed by transform coding. The encoder indicates the maximum and
minimum luma TB sizes that it will use. Splitting is implicit when the CB size is
larger than the maximum TB size. Not splitting is implicit when splitting would
result in a luma TB size smaller than the indicated minimum. The chroma TB size is
half of the luma TB size in each dimension, except when the luma TB size is 4×4,
in which case a single 4×4 chroma TB is used for the region covered by four 4×4
luma TBs.
2.4.2 Intra Prediction
Intra prediction operates according to the TB size, and previously decoded bound-
ary pixels from spatially neighboring TBs are used to form the prediction signal.
Directional prediction with 33 different directional orientations is deﬁned for square

2.4 HEVC Standard
43
Figure 2.14 The directions of intra prediction in HEVC.
TB sizes from 4 × 4 up to 32 × 32. The possible prediction directions are shown in
Figure 2.14. Alternatively, planar prediction (assuming an amplitude surface with a
horizontal and vertical slope derived from the boundaries) and DC prediction (a ﬂat
surface with a value matching the mean value of the boundary pixels) can also be
used. For chroma, the horizontal, vertical, planar, and DC prediction modes can be
explicitly signalled, or the chroma prediction mode can be indicated as the same as
the luma prediction mode.
In HEVC, the reference pixels used for the intra prediction are sometimes ﬁltered
by a three-tap [1,2,1]/4 smoothing ﬁlter in a manner similar to what was used for 8
× 8 intra prediction in H.264. HEVC applies smoothing operations more adaptively,
according to the directionality, the amount of detected discontinuity, and block size.
As in H.264, the smoothing ﬁlter is not applied for 4 × 4 blocks. For 8 × 8 blocks,
only the diagonal directions with directions as 2, 18, or 34 use the reference pixel
smoothing. For 16 × 16 blocks, the reference pixels are ﬁltered for most directions
except the near-horizontal and near-vertical directions in the range of 9–11 and 25–
27. For 32 × 32 blocks, all directions except the exactly horizontal direction 10 and
the exactly vertical direction 26 use the smoothing ﬁlter, and when the amount of
detected discontinuity exceeds a threshold, bilinear interpolation from three neigh-
boring region pixels is applied to form a smooth prediction. The intra-planar mode
also uses the smoothing ﬁlter when the block size is greater than or equal to 8 × 8,
and the smoothing is not used for the intra-DC mode.
2.4.3 Transform and Quantization
HEVC supports transform block sizes as 4 × 4, 8 × 8, 16 × 16, and 32 × 32. Two-
dimensional transforms are computed by applying 1-D transforms in the horizontal
and vertical directions. The elements of the core transform matrices were derived

44
2 Hybrid Video Coding
by approximating scaled DCT basis functions, under considerations such as limiting
the necessary dynamic range for transform computation and maximizing the preci-
sion and closeness to orthogonality when the matrix entries are speciﬁed as integer
values. For simplicity, only one integer matrix 32 points in length is speciﬁed, and
subsampled versions are used for other sizes. For example, the matrix for the length-
16 transform is shown here
H =


64
64
64
64
64
64
64
64
64
64
64
64
64
64
64
64
90
87
80
70
57
43
25
9 −9 −25 −43 −57 −70 −80 −87
90
89
75
50
18 −18 −50 −75 −89 −89 −75 −50 −18
18
50
75
89
87
57
9 −43 −80 −90 −70 −25
25
70
90
80
43 −9 −57 −87
83
36
36
83
83
36
36
83
83
36
36
83
83
36
36
83
80
9
70
87
25
57
90
43
43
90
57
25
87
70
9
80
75 −18 −89 −50
50
89
18 −75 −75
18
89
50 −50 −89 −18
75
70 −43 −87
9
90
25 −80 −57
57
80 −25 −90 −9
87
43 −70
64
64
64
64
64
64
64
64
64
64
64
64
64
64
64
64
57
80
25
90
9
87
43
70
70
43
87
9
90
25
80
57
50
89
18
75
75
18
89
50
50
89
18
75
75
18
89
50
43
90
57
25
87
70
9
80
80
9
70
87
25
57
90
43
36
83
83
36
36
83
83
36
36
83
83
36
36
83
83
36
25 −70
90 −80
43
9 −57
87 −87
57 −9 −43
80 −90
70 −25
18
50
75
89
89
75
50
18
18
50
75
89
89
75
50
18
9
25
43
57
70
80
87
90
90
87
80
70
57
43
25
9


The matrices for the length-8 and length-4 transforms can be derived by using the
ﬁrst eight entries of rows 0, 2, 4, ···, and using the ﬁrst four entries of rows 0, 4, 8,
···, respectively.
For the transform block size of 4 × 4, an alternative integer transform derived
from a DST is applied to the luma residual blocks for intra-prediction modes, with
the transform matrix
H =


29
55
74
84
74
74
0 −74
84 −29 −74
55
55 −84
74 −29

.
The basis functions of the DST better ﬁt the statistical property that the residual
amplitudes tend to increase as the distance from the boundary pixels. In terms of
complexity, the 4 × 4 DST-like transform is not much more computationally de-
manding than the 4 × 4 DCT-like transform. The usage of the DST type of transform
is restricted to only 4 × 4 luma transform blocks, since for other cases the additional
coding efﬁciency improvement for including the additional transform type was found
to be marginal.
For quantization, HEVC uses essentially the same uniform reconstruction quan-
tization (URQ) scheme controlled by a quantization parameter QP as in H.264. The

2.4 HEVC Standard
45
range of the QP values is deﬁned from 0 to 51, and an increase by 6 doubles the
quantization step size such that the mapping of QP values to step sizes is approxi-
mately logarithmic. Quantization scaling matrices are also supported. To reduce the
memory needed to store frequency-speciﬁc scaling values, only quantization matri-
ces of sizes 4 × 4 and 8 × 8 are used. For the larger transforms of 16 × 16 and 32
× 32, an 8 × 8 scaling matrix is sent and is applied by sharing values within 2 × 2
and 4 × 4 coefﬁcient groups in frequency subspaces except for the value at the DC
position, for which a distinct value is sent and applied.
2.4.4 Sample Adaptive Offset Filter
SAO is a process that modiﬁes the decoded pixels by conditionally adding an offset
value to each pixel after the application of DBF, based on values in look-up tables
transmitted by the encoder. SAO ﬁltering is performed on a region basis, based on
a ﬁltering type selected per CTB. A value of 0 for SAO type indicates that the SAO
ﬁlter is not applied to the CTB, and the values 1 and 2 signal the use of the band
offset and edge offset modes, respectively.
In the band offset mode, the selected offset value directly depends on the pixel
amplitude. In this mode, the full pixel amplitude range is uniformly split into 32 seg-
ments called bands, and the pixel values belonging to four of these bands (which are
consecutive within the 32 bands) are modiﬁed by adding transmitted values denoted
as band offsets, which can be positive or negative. The main reason for using four
consecutive bands is that in the smooth areas where banding artifacts can appear, the
pixel amplitudes in a CTB tend to be concentrated in only a few of the bands. In
addition, the design choice of using four offsets is uniﬁed with the edge offset mode,
which also uses four offset values.
In the edge offset mode, SAO edge class with values from 0 to 3 signals whether
a horizontal, vertical, or one of two diagonal gradient directions is used for the edge
offset classiﬁcation in a CTB. Figure 2.15 shows the four gradient patterns used for
the respective SAO edge class. Each pixel in the CTB is classiﬁed into one of ﬁve
categories by comparing the pixel value p located at some position with the values
n0 and n1 of two pixels located at neighboring positions as shown in Table 2.2. This
classiﬁcation is done for each pixel based on decoded pixel values, so no additional
signalling is required for the edge classiﬁcation. Depending on the edge category at
the pixel position, for edge categories from 1 to 4, an offset value from a transmitted
look-up table is added to the pixel value. The offset values are always positive for
categories 1 and 2 and negative for categories 3 and 4 — thus the ﬁlter generally has
a smoothing effect in the edge offset mode.
For SAO modes 1 and 2, a total of four amplitude offset values are transmitted to
the decoder for each CTB. For mode 1, the sign is also encoded. The offset values,
SAO mode, and SAO class are determined by the encoder — typically using criteria
that optimize rate-distortion performance. The SAO parameters can be indicated to
be inherited from the left or above CTB using a merge ﬂag to make the signalling

46
2 Hybrid Video Coding
n0
n1
p
n0
p
n1
n0
p
n1
n0
p
n1
(a)
(b)
(c)
(d)
Figure 2.15 Four gradient patterns used in SAO. Sample labeled p indicates a center pixel to be
considered. Two pixels labeled n0 and n1 specify two neighboring pixels along the (a) horizontal,
(b) vertical, (c) 135 diagonal, and (d) 45 gradient patterns.
Table 2.2 Pixel edge categories in the SAO edge offset mode.
Edge Category
Condition
Meaning
0
Cases not listed below
Monotonic area
1
p < n0 and p < n1
Local min
2
p < n0 and p = n1 or p < n1 and p = n0
Edge
3
p > n0 and p = n1 or p > n1 and p = n0
Edge
4
p > n0 and p > n1
Local max
efﬁcient. In summary, SAO is a nonlinear ﬁltering operation which allows additional
reﬁnement of the reconstructed signal, and it can enhance the signal representation
in both smooth areas and around the edges.

Chapter 3
Communication
In this chapter, we ﬁrst introduce analog communication, which has been almost for-
gotten by both academics and industry experts for many years because of the dom-
inance of digital communication. But our recent research discovers that simulating
the behavior of analog communication in digital video communication can provide
many interesting features that do not exist in digital communication. This is the rea-
son why we discuss analog transmission technologies here. We also discuss digital
communication in the physical layer (PHY), which consists of channel coding and
modulation.
3.1 Analog Communication
The content in this section is organized according to the book Analog Communica-
tion [15]. Television (TV) was invented in 1923 and the color TV began in 1954. The
core of these early TV systems is an analog transmission system. The analog system
is simple and is designed for converting real-value video and audio samples into con-
tinuous high-frequency electromagnetic waves that are sent through the air or other
medium. The analog system mainly consists of ampliﬁer circuits. They amplify in-
put signals before transmission, which help in accurate reception of the transmitted
information at the receiver.
In general, video and audio signals are not suitable for direct transmission over
a medium. For example, voice signals cannot travel longer distance in air and the
signal gets attenuated rapidly. Hence, video and audio signals have to be converted
to high-frequency electronic signals for transmission. Modulation is exactly the tech-
nology best suited for the conversion. The advantages of using modulation technolo-
gies includes reducing the size of the antenna, avoiding mixing signals, increasing
communication distance, allowing multiplexing of signals, adjusting bandwidth, and
so on.
47

48
3 Communication
3.1.1 Analog Modulation
Modulation technologies can be categorized as continuous-wave modulation or pulse
modulation. In the continuous-wave modulation, a sinusoidal wave is used as the car-
rier. When the amplitude of the carrier is varied in accordance with the information
signal, it is an amplitude modulation (AM), and when the angle of the carrier is var-
ied, it is angle modulation. The angle modulation is further subdivided into frequency
modulation (FM) and phase modulation (PM), in which the instantaneous frequency
and phase of the carrier are varied according to the information signal, respectively.
In a pulse module, the carrier consists of a periodic sequence of rectangular pulses.
The pulse modulation is further subdivided into analog and digital type. In analog
pulse modulation, the amplitude, duration, or position of a pulse is varied in accor-
dance with sample values of the signal to have pulse amplitude modulation (PAM),
pulse duration modulation (PDM), or pulse position modulation (PPM), respectively.
Let us deﬁne information signal and carrier signal as s(t) = As cos(2π fst) and
c(t) = Ac cos(2π fct), respectively. As and Ac are the amplitude of the information
and carrier signals. fs and fc are the frequency of the information and carrier signals.
The standard form of an amplitude modulated wave is deﬁned as
m(t) = Ac [1+Kas(t)]cos(2π fct),
(3.1)
where Ka is a constant called the amplitude sensitivity of the modulator. In order to
faithfully recover the information signal, the absolute value of Kas(t) must be less
than 1. The frequency fc should be far larger than the signal frequency fs. Substitut-
ing s(t) into Eq. (3.1), we get
m(t) = Ac [1+ µ cos(2π fst)]cos(2π fct),
= Ac cos(2π fct)+ µAc cos(2π fst)cos(2π fct),
= Ac cos(2π fct)+ µAc
2
cos(2π(fc −fs)t)−µAc
2
cos(2π(fc + fs)t),(3.2)
where µ = KaAs.
As shown in Eq. (3.2), the modulated carrier has new signals at different frequen-
cies that are called as side frequencies or sidebands. They occur in the frequency
spectrum directly above and below the carrier frequency

fUSB = fc + fs
fLSB = fc −fs .
(3.3)
The upper sideband is called fUSB and the lower sideband is called fLSB. We know
that bandwidth can be measured by subtracting the lowest frequency of the signal
from the highest frequency of the signal. For the amplitude modulated wave it is
BW = fUSB −fLSB = 2fs = 2W,
(3.4)

3.1 Analog Communication
49
where W is the signal bandwidth. The bandwidth required for the amplitude mod-
ulation is twice the bandwidth of the information signal. Thus AM is inefﬁcient in
bandwidth and power usage. The single sideband (SSB) modulation is an improve-
ment over AM by suppressing one sideband and keeping another sideband. It is more
efﬁcient in using transmitter bandwidth and power because it avoids bandwidth dou-
bling and the power wasted on the carrier.
FM conveys information over a carrier wave by varying its instantaneous fre-
quency. This contrasts with AM, in which the amplitude of the carrier is varied
while its frequency remains constant. The difference between the spontaneity and
the frequency of the carrier is directly proportional to the instantaneous value of the
amplitude of information signal. The frequency modulation is formulated as
m(t) = Ac cos

2π
Z t
0 f(τ)dτ

= Ac cos

2π
Z t
0 [fc + f∆s(τ)]dτ

= Ac cos

2π fct +2π f△
Z t
0 s(τ)dτ

.
(3.5)
In Eq. (3.5), f(τ) is the instantaneous frequency of the carrier and f∆is the frequency
deviation, which represents the maximum shift away from fc. Here we assume As =
1. Substituting s(t) into Eq. (3.5), we can get
m(t) = Ac cos

2π fct + f∆
fs
sin(2π fst)

.
(3.6)
PM is a form of modulation that represents information as variations in the instanta-
neous phase of a carrier wave. Unlike the more popular FM, PM is not widely used
for radio transmissions. This is because it tends to require more complex receiving
hardware and there can be ambiguity problems in determining whether, for example,
the signal has changed phase by +180◦or −180◦. The modulation can be formulated
m(t) = Ac cos(2π fct +s(t)+φc).
(3.7)
This shows how to modulate the phase — the greater s(t) is at a point in time, the
greater the phase shift of the modulated signal at that point. It can also be viewed as
a change of the frequency of the carrier signal. Thus PM can be considered a special
case of FM in which the carrier frequency modulation is given by the time derivative
of the phase modulation.
3.1.2 Multiplexing
More efﬁcient communications systems can be obtained if more than one signal
is transmitted by a transmitter on the same carrier and on the same channel, or

50
3 Communication
Low-pass
filter
Modulator
Band-pass
filter
Low-pass
filter
Modulator
Band-pass
filter
Low-pass
filter
Modulator
Band-pass
filter
Carrier
Supply
...
...
Channel
Signal 1
Signal 2
Signal n
Band-pass
filter
Band-pass
filter
Band-pass
filter
Demodulator
Demodulator
Demodulator
Carrier
Supply
Low-pass
filter
Low-pass
filter
Low-pass
filter
Signal 1
Signal 2
Signal n
...
...
Transmitter
Receiver
Figure 3.1 Block diagram of an FDM system.
multiple transmitters are transmitting simultaneously on the same channel. This pro-
cess is known as “multiplexing.” Multiplexing requires that the signals be kept apart
so that they do not interfere with each other, and thus they can be separated at the
receiving end. This is accomplished by separating the signals by either frequency or
time. There are two types of multiplexing in analog transmissions: frequency division
multiplexing (FDM) and time division multiplexing (TDM).
Figure 3.1 shows the block diagram of an FDM system. As shown in the ﬁgure,
signals, assumed to be low-pass, are passed through low-pass ﬁlters. This ﬁltering
removes high-frequency components that do not contribute signiﬁcantly to signal
representation but may disturb other signals that share a common channel. The ﬁl-
tered signals are then modulated with necessary carrier frequencies with the help of
modulators. The most common method of modulation in FDM is SSB modulation,
which requires a bandwidth that is approximately equal to that of the original sig-
nal. Then band-pass ﬁlters following the modulators are used to restrict the band of
each modulated wave to its prescribed range. The outputs of the band-pass ﬁlters are
combined in parallel, which form the input to the common channel. At the receiving
end, the band-pass ﬁlters connected to the common channel separate the message
signals on the frequency occupancy basis. Finally, the original signals are recovered
by individual demodulators.
In TDM, each signal transmitted is sampled sequentially and the resulting pulse
code is used to modulate the carrier. The same carrier frequency is used to transmit
different pulses sequentially, one after another, to be transmitted, each of which is
allotted a given time slot. Since only one signal modulates the carrier at any time,
no added equipment and no increase in bandwidth are needed when multiplexing.
The number of sequential channels that can be handled is limited by the time span
required by any one channel pulse and interval between samples. Thus, in TDM,
each signal occupies the entire bandwidth of the channel. However, each signal is
transmitted for only a short period of time.
Figure 3.2 shows the block diagram of a TDM system, which is used to multi-
plex three signals. Each signal is allowed to use the channel for a ﬁxed interval of
time, called a time slot. The three signals use the channel sequentially one after the

3.2 Digital Communication
51
Pulse 
modulator 1
Pulse
modulator 2
Low-pass
filter
Transmitter 
Synchronizer
Channel
Signal 1
Signal 2
Signal 3
Receiver
Synchronizer
Signal 1
Signal 2
Signal 3
1
2
3
1
2
3
Demodulator 
1
Demodulator 
2
Demodulator 
3
Transmitter
Receiver
Figure 3.2 Block diagram of a TDM system.
other. One transmission of each channel completes one cycle of operation, called a
frame. Once all the signals have been transmitted, the cycle repeats again and again,
at a high rate of speed. A rotating switch called a commutator connects the output
of each channel modulator to the input of the communication channel in turn. The
commutator is realized with electronic switches since it has to rotate at high speeds.
The commutator remains at each contact for a ﬁxed interval of time, which is the
time slot allotted for each channel. At the receiver, another switch is used, rotating
in synchronization with the commutator in the sending end. This switch connects the
pulses received to the appropriate demodulator. For the proper operation of the sys-
tem, absolution synchronization is essential between the transmitter and the receiver.
3.2 Digital Communication
The digital communication in the physical layer consists of channel coding and dig-
ital modulation. The former introduces redundancy to transmitted data, which can
be used to correct transmission errors at the receiver. The channel coding has been
discussed by using Hamming codes as an example in Section 1.3.3. But Hamming
codes are not capacity-achieving channel codes. Here, we will discuss low-density
parity-check (LDPC) codes [16,17] and Turbo codes [18,19]. They can approach the
Shannon limit quite closely. The latter multiplexes multiple channel coded bits into
a symbol for transmission, which increases the transmission rate.
3.2.1 Low-Density Parity-Check (LDPC) Codes
LDPC codes were invented by Gallager in his doctoral thesis in 1961 [16]. These
codes were almost forgotten until the mid-1990s. After the introduction of Turbo

52
3 Communication
codes, they were independently rediscovered by MacKay et al. [17]. Because of the
simple structure, they have been the focus of many theoretical analyses. They have
been proven to be capable of approaching the Shannon limit more closely than any
other class of codes.
Similar to Hamming codes, LDPC codes are linear block codes. They can be rep-
resented as (n,k) codes, where n is the code length and k is the information dimen-
sion. Here we consider binary LDPC codes only. For a LDPC code (n,k), the source
is a k×1 vector and a codeword is a n×1 vector. The generator matrix G is n×k and
the parity check matrix H is m×n, where m = n−k. The LDPC code can be speciﬁed
by the parity check matrix H. Before we discuss how to generate an H, we introduce
the concept of weight of a binary vector as the number of nonzero elements in it.
The column weight wc of a column of a matrix is the weight of the column; similar
for row weight wr. An LDPC code is regular if the column weights are all the same
and the row weights are all the same; otherwise it is irregular. For a regular LDPC
code, wcn = wrm. This structure says that every bit participates in wc checks and
each check involves wr bits. The rate of a regular LDPC code is R = 1−wc/wr.
One way to construct a (wc,wr) parity matrix is as follows. First, the matrix H0 is
constructed as
H0 =


1 1 ··· 1
1 1 ··· 1
...
1 1 ··· 1

,
(3.8)
with m/wc rows and n columns. There are wr’s 1 in every row in Eq. (3.8). This
deﬁnes a (1,wr) regular parity check code. Then we form H by stacking permutations
of H0,
H =


π1(H0)
π2(H0)
...
πwc(H0)

,
(3.9)
where each πi(H0) denotes a matrix obtained by permuting the columns of H0. Ob-
viously, the choice of the permutations determines the distance structure of the code.
However, a random choice of permutation will produce a good code on average.
Gallager showed that if each permutation is chosen at random out of the n! possible
permutations, then the average minimum distance increases linearly with n. In many
LDPC codes, n is quite large (such as n > 10000).
Now we have a parity check matrix H. It is expedient for encoding to determine
the corresponding generator matrix G. A systematic generator matrix may be found
as follows. Using Gaussian elimination (with binary arithmetic) determines an m×m
matrix H−1
p
so that
H = H−1
p H = [I H2 ].
(3.10)

3.2 Digital Communication
53
Messages
Codewords
Degree
(a) Encoding
0
0
0
Codewords
(b) Parity check
Figure 3.3 An example of graph code.
If such a matrix Hp does not exist, it indicates that H is rank deﬁcient rank(H) < M.
The matrix H with the form of Eq. (3.10) is called systematic. We can directly write
a systematic G from the systematic parity matrix H as
G =

H2
I

.
(3.11)
The HG = 0, so G is a generator matrix for H.
LDPC codes can be represented by bipartite graphs. Figure 3.3 shows a simple
LDPC code. Figure 3.3a is the encoding process and Figure 3.3b is the parity-check
process. The generator matrix and the parity-check matrix are represented by edges,
where the nonzero elements indicate edges. If nonzero elements are dominant in the
matrix, the codes are referred to as dense; otherwise they are referred to as sparse
(also referred to as low density). One important parameter to characterize the LDPC
codes in bipartite graphs is degree, which corresponds to row and column weights.
The node degree is deﬁned as the number of edges incident to one node. As shown
in Figure 3.3a, the degree of left node x1 is 3. The edge of degree on the right and left
is deﬁned as the degree of its incident node at the right and left sides, respectively.
For a k-dimension binary source x = [x1,x2,··· ,xk]T, the LDPC codeword is gen-
erated by
y = Gx.
(3.12)

54
3 Communication
…
…
…
…
x1
x2
xu
ŷ1
ŷ2
ŷv
ŷi
xj
qt
1i
rt
2j
qt
2i
qt
ui
rt
vj
qt
ji
rt
ij
rt
1j
Figure 3.4 The graph fragment of the message passing algorithm.
The calculation is carried out in binary and so the result is a binary vector y =
[y1,y2,··· ,yn]T. The codeword y will be transmitted over the noisy channel. If the
channel is additive white Gaussian noise (AWGN), the received codeword is de-
scribed as
ˆy = y+z,or ˆyi = yi +zi.
(3.13)
zi is a zero-mean Gaussian noise with variation σ2. The decoding can be formulated
as
ˆx =
arg max
x∈GF(2)kP(x|ˆy)
s.t.
ˆy = GxT +z
(3.14)
The decoding algorithm for Eq. (3.14) is an example of a message passing algo-
rithm. Messages are passed among the nodes in the bipartite graph shown in Figure
3.3a, where the left nodes are source bits and the right nodes are coded bits. In the
horizontal step, messages in the form of probability are passed to the source nodes.
In the vertical step, messages are passed to the check nodes. Figure 3.4 shows one
iteration in the message passing decoding algorithm. Let us take a connecting pair
of source bit xj and check bit ˆyi as an example. qt
ji() is the probability message sent
from xj to ˆyi in the tth iteration and rt
i j() is the message from ˆyi to xj. Denote Ri as the
set of source bits connected to a check bit ˆyi, and Ri \ j as set Ri excluding source bit
xj. Denote Cj as the set of check bits connected to xj, and Cj \i as set Cj excluding
symbol ˆyi.
The standard message passing algorithm for LDPC decoding is described as fol-
lows.

3.2 Digital Communication
55
(1) Initialization
The initial message sent from xj to ˆyi is q0
ji(xj = 0) = pj and q0
ji(xj = 1) = 1−pj,
where pj is an estimation from the channel for a systematic LDPC code
pj = Pr(xj = 0 | ˆyj) =
1
1+e−2ˆy j/σ2 ,
(3.15)
or the prior probability.
(2) Horizontal processing
The message sent from ˆyi to xj in the tth iteration is
(
rt
i j(xj = 0) = 1
2 + 1
2 ∏m∈Ri\ j
 1−2qt−1
mi (xj = 1)

rt
i j(xj = 1) = 1−rt
i j(xj = 0)
.
(3.16)
(3) Vertical processing
The message sent from xj to ˆyi in the tth iteration is
 qt
ji(x j = 0) = Kji · p j ·∏m∈Cj\i rt
mj(x j = 0)
qt
ji(x j = 1) = Kji ·(1−pj)·∏m∈Cj\i rt
m j(xj = 1) ,
(3.17)
where Kji is the normalization constant to make qt
ji(xj = 0)+qt
ji(x j = 1) = 1.
After T iterations, the message passing algorithm stops and makes a hard decision
on values of each source bit xj by calculating
qT
ji(xj = 0) = Kji · pj ·∏m∈Cj rT
mj(xj = 0)
qT
ji(xj = 1) = Kji ·(1−pj)·∏m∈Cj rT
m j(xj = 1) .
(3.18)
Then the values of input binary digits are estimated by:
xj =
(
0
qT
ji(xj = 0) ≥qT
ji(xj = 1)
1
otherwise
(3.19)
3.2.2 Turbo Codes
Shannon’s channel coding theorem implies good coding performance for random
codes as the code block length increases, but increasing block length typically im-
plies an exponentially increasing decoding complexity. In 1993, an approach for er-
ror correction coding was introduced, which provided very long codewords with only
(relatively) modest decoding complexity. These codes were called Turbo codes [27,
28].
The Turbo code encoder consists of two (or more) systematic block codes which
share message data via interleaving. In its most conventional realization, the codes
are obtained from recursive systematic convolutional (RSC) codes — but other codes

56
3 Communication
T
T
+
Interleaving
dk
xk
y1k
y2k
C1
C2
T
+
T
T
+
T
+
Figure 3.5 An example of a Turbo encoder.
can be used as well. Figure 3.5 shows a Turbo code adopted in the long-term evo-
lution (LTE) wireless communication standard. The data ﬂow (dk at time k) goes
directly to a ﬁrst elementary RSC encoder C1. After interleaving, it feeds (dn at time
k) a second elementary RSC encoder C2. Although these two encoders are identical
in Figure 3.5, it is not necessary for Turbo codes. Both C1 and C2 encoders have the
constraint length K = 4 and memory ν = K −1. The transfer function of each RSC
encoder is the rational function
G(x) = 1+D2 +D3
1+D+D3 ,
(3.20)
where D indicates a delay.
Data dk is systematically transmitted as symbol xk and redundancies y1k and y2k
produced by C1 and C2 may be completely transmitted for an R = 1/3 encoding or
punctured for high rates. The two elementary coding rates R1 and R2 associated with
C1 and C2 after puncturing may be different, but for the best decoding performance,
they will satisfy R1 ≤R2. The global rate R of the composite code, R1 and R2, are
linked by
1
R = 1
R1
+ 1
R2
−1.
(3.21)
A key development in Turbo codes is the iterative decoding algorithm. In the
iterative decoding algorithm, the decoders take turns operating on the received data.
Each decoder produces an estimate of the probabilities of the transmitted symbols.
Probabilities of the symbols from one decoder known as extrinsic probabilities are
passed to the other decoder in the symbol order appropriate for the encoder, where
they are used as prior probabilities for the other decoder. The decoder thus passes
probabilities back and forth between the decoders, with each decoder combining the

3.2 Digital Communication
57
16 State 
Decoder 
DEC1
16 State 
Decoder 
DEC2
Interleaving
yk
xk
y1k
y2k
Latency: L1
Latency: L2
Λ1(dk)
Λ1(dn)
dn-L2
Figure 3.6 The decoding of Turbo codes.
evidence it receives from the incoming prior probabilities with the parity information
provided by the code. After a number of iterations, the decoder converges to an
estimate of the transmitted codeword.
The decoder of Turbo codes is shown in Figure 3.6, which is made up of two
elementary decoders (DEC1 and DEC2) in a serial concatenation scheme. The ﬁrst
elementary decoder DEC1 is associated with the lower rate R1 encoder C1 and yields
a weighted decision. For a discrete Gaussian channel without memory, the decoder
input is made up of a couple rk of two random variables ˆxk and ˆyk, at time k

ˆxk = xk +ik
ˆyk = yk +qk ,
(3.22)
where ik and qk are two independent noises with the same variance σ2. The redundant
information ˆyk is demultiplexed and sent to decoder DEC1 when yk = y1k and to
decoder DEC2 when yk = y2k. When the redundant information of a given encoder
(C1 or C2) is not emitted, the corresponding decoder input is set to zero.
Consider an RSC code with the constraint length K. At time k, the encoder state
Sk is represented by
Sk = (ak,ak−1,··· ,ak−K+2).
Let us also suppose that the information bit sequence {dk} is made up of N indepen-
dent bits dk, taking values zero and one with equal probability. The encoder initial
state S0 and ﬁnal state SN are both equal to zero
S0 = SN = (0,0,··· ,0) = 0.
The encoder outputs codeword sequence, noted CN
1 = {c1 ···ck ···cN}, where ck =
(xk,yk) is the input to a discrete Gaussian memoryless channel. The channel output
is the sequence RN
1 = {r1 ···rk ···rN}, where rk = (ˆxk, ˆyk) is deﬁned by Eq. (3.22).
The a posteriori probability (APP) of a decoded bit dk can be derived from the
joint probability λ i
k(m) deﬁned by
λ i
k(m) = Pr{dk = i,Sk = m | RN
1 }.
(3.23)
Thus, the APP of a decoded bit dk is equal to

58
3 Communication
Pr{dk = 1 | RN
1 } = ∑
m
λ i
k(m) i = 0,i.
(3.24)
The logarithm of likelihood ratio (LLR) Λ(dk) associated with a decoded bit dk can
be written as
Λ(dk) = log ∑m λ 1
k (m)
∑m λ 0
k (m).
(3.25)
Finally, the decoder can make a decision by comparing Λ(dk) to a threshold equal to
zero
 ˆdk = 1, ifΛ(dk) ≥0
ˆdk = 0, ifΛ(dk) < 0 .
(3.26)
According to the algorithm proposed by Bahl et al. [20], the LLR Λ(dk) in Eq.
(3.25) can be calculated as
Λ(dk) = log ∑m ∑m′ Pr{dk = 1,Sk = m,Sk−1 = m′,Rk−1
1
,rk,RN
k+1}
∑m ∑m′ Pr{dk = 0,Sk = m,Sk−1 = m′,Rk−1
1
,rk,RN
k+1}
.
(3.27)
Using the Bayes rule and taking into account that events after time k are not inﬂu-
enced by observation Rk
1 and bit dk, if state Sk is known, the LLR Λ(dk) is equal
Λ(dk) =log
∑m ∑m′ Pr{RN
k+1 | Sk = m}Pr{Sk−1 = m′ | Rk−1
1
}
∑m ∑m′ Pr{RN
k+1 | Sk = m}Pr{Sk−1 = m′ | Rk−1
1
}
× Pr{dk = 1,Sk = m,rk | Sk−1 = m′}
Pr{dk = 0,Sk = m,rk | Sk−1 = m′}
(3.28)
In order to compute the LLR Λ(dk), let us introduce the probability functions αk(m),
βk(m), and γi(rk,m′,m) deﬁned by
αk(m) = Pr{Sk = m | Rk
1},
(3.29a)
βk(m) = Pr{RN
k+1 | Sk = m}
Pr{RN
k+1 | Rk
1}
,
(3.29b)
γi(rk,m′,m) = Pr{dk = i,Sk = m,rk | Sk−1 = m′}.
(3.29c)
Substituting Eq. (3.29) into Eq. (3.27), Λ(dk) is equal to
Λ(dk) = log ∑m ∑m′ γ1(rk,m′,m)αk−1(m′)βk(m)
∑m ∑m′ γ0(rk,m′,m)αk−1(m′)βk(m),
(3.30)
where probabilities αk(m) and βk(m) can be recursively calculated from probability
γi(rk,m′,m) as

3.2 Digital Communication
59
αk(m) =
∑m′ ∑1
i=0 γi(rk,m′,mαk−1(m′)
∑m ∑m′ ∑1
i=0 γi(rk,m′,m)αk−1(m′),
(3.31)
βk(m) = ∑m′ ∑1
i=0 γi(rk+1,m′,m)βk+1(m′)
∑m ∑m′ ∑1
i=0 γi(rk+1,m′,m)αk(m′).
(3.32)
The probability γi(rk,m′,m) can be determined from transition probabilities of the
discrete Gaussian memoryless channel and transition probabilities of the encoder
trellis. γi(rk,m′,m) is given by
γi(rk,m′,m) =p(rk | dk = i,Sk = m,Sk−1 = m′)
×q(dk = i | Sk = m,Sk−1 = m′)×π(Sk = m | Sk−1 = m′). (3.33)
p(· | ·) is the transition probability of the discrete Gaussian memoryless channel.
Conditionally to (dk = i,Sk = m,Sk−1 = m′), ˆxk and ˆyk(rk = (ˆxk, ˆyk)) are two uncor-
related Gaussian variables and thus we obtain
p(rk | dk = i,Sk = m,Sk−1 = m′) =
p(ˆxk | dk = i,Sk = m,Sk−1 = m′)p(ˆyk | dk = i,Sk = m,Sk−1 = m′).
(3.34)
Since the convolutional encoder is a deterministic machine q(dk = i | Sk = m,Sk−1 =
m′) is equal to 0 or 1. The transition state probabilities π(Sk = m | Sk−1 = m′) of the
trellis are deﬁned by the encoder input statistic. Generally, Pr{dk = 1} = Pr{dk =
0} = 1/2 and since there are two possible transitions from each state, π(Sk = m |
Sk−1 = m′) = 1/2 for each of these transitions.
Since the encoder is systematic (xk = dk), the transition probability p(ˆxk | dk =
i,Sk = m,Sk−1 = m′) in expression γi(rk,m′,m) is independent of state values Sk
and Sk−1. Therefore, we can factorize this transition probability in the numerator
and in the denominator of Eq. (3.30).
Λ(dk) = log p(ˆxk | dk = 1)
p(ˆxk | dk = 0) +log ∑m ∑m′ γ1(ˆyk,m′,m)αk−1(m′)βk(m)
∑m ∑m′ γ0(ˆyk,m′,m)αk−1(m′)βk(m).
(3.35)
Conditional to dk = 1 (resp. dk = 0), variable xk is Gaussian with mean 1 (resp. –1)
and variance σ2, thus the LLR Γ (dk) is still equal to
Λ(dk) = 2
σ2 ˆxk +Wk,
(3.36)
where
Wk = Λ(dk) |ˆxk=0= log ∑m ∑m′ γ1(ˆyk,m′,m)αk−1(m′)βk(m)
∑m ∑m′ γ0(ˆyk,m′,m)αk−1(m′)βk(m).
(3.37)
Both decoders DEC1 and DEC2 now use the above algorithm. As shown by Eq.
(3.36), the LLR at the decoder output can be expressed as a sum of two terms if the

60
3 Communication
noises at the decoder inputs are independent at each time k. Hence, if the noise at the
decoder DEC2 inputs are independent, the LLR Λ2(dk) at the decoder DEC2 output
can be written as
Λ2(dk) = f(Λ(dk))+W2k
(3.38)
with
Λ1(dk) = 2
σ2 ˆxk +W1k.
(3.39)
From Eq. (3.37), we can see that the decoder DEC2 extrinsic information W2k is a
function of the sequence {Λ1(dn)}n̸=k. Since Λ1(dn) depends on observation RN
1 ,
extrinsic information W2k is correlated with observations ˆxk and ˆy1k, regarding the
noise. Nevertheless, from Eq. (3.37), the greater | n −k | is, the less correlated are
Λ1(dn) and observations ˆxk, ˆyk. Thus, due to the presence of interleaving between
decoders DEC1 and DEC2, extrinsic information W2k and observations ˆxk, ˆy1k are
weakly correlated. Therefore, extrinsic information W2k and observations ˆxk, ˆy1k can
be used jointly for carrying out a new decoding bit dk.
3.2.3 Digital Modulation
In digital modulation, an analog carrier signal is modulated by a discrete signal. Dig-
ital modulation can be considered digital-to-analog conversion, and the correspond-
ing demodulation or detection as analog-to-digital conversion. The changes in the
carrier signal are chosen from a ﬁnite number of M alternative symbols (the mod-
ulation alphabet). The most fundamental digital modulation techniques are based
on keying: phase-shift keying (PSK), frequency-shift keying (FSK), amplitude-shift
keying (ASK), and quadrature amplitude modulation (QAM).
In all of the above methods, each of these phases, frequencies, or amplitudes are
assigned a unique pattern of binary bits. Usually, each phase, frequency, or amplitude
encodes an equal number of bits. This number of bits comprises the symbol that is
represented by the particular phase, frequency, or amplitude. In QAM, an in-phase
signal (the I signal, for example, a cosine waveform) and a quadrature phase signal
(the Q signal, for example, a sine wave) are modulated with a ﬁnite number of am-
plitudes and added together. It can be seen as a two-channel system, each channel
using ASK. The resulting signal is equivalent to a combination of PSK and ASK.
These are the general steps used by the modulator to transmit data: (1) Group
the incoming data bits into codewords, one for each symbol that will be transmitted.
(2) Map the codewords to attributes, for example, amplitudes of the I and Q sig-
nals, or frequency or phase values. (3) Adapt pulse shaping or some other ﬁltering
to limit the bandwidth and form the spectrum of the equivalent low-pass signal. (4)
Perform digital-to-analog conversion (DAC) of the I and Q signals. (5) Generate a
high frequency sine carrier waveform, and perhaps also a cosine quadrature compo-
nent. Carrying out the modulation, for example, by multiplying the sine and cosine
waveform with the I and Q signals, results in the equivalent low-pass signal being

3.2 Digital Communication
61
frequency shifting to the modulated passband signal or RF signal. (6) Ampliﬁcation
and analog band-pass ﬁltering to avoid harmonic distortion and periodic spectrum.
At the receiver side, the demodulator typically performs the following steps: (1)
Band-pass ﬁltering. (2) Automatic gain control (AGC) to compensate for attenuation.
(3) Frequency shifting of the RF signal to the equivalent baseband I and Q signals,
or to an intermediate frequency (IF) signal, by multiplying the RF signal with a local
oscillator sine wave and cosine wave frequency. (4) Sampling and analog-to-digital
conversion (ADC). (5) Equalization ﬁltering, for example, a matched ﬁlter, compen-
sation for multipath propagation, time spreading, phase distortion, and frequency se-
lective fading to avoid inter-symbol interference and symbol distortion. (6) Detection
of the amplitudes of the I and Q signals, or the frequency or phase of the IF signal.
(7) Quantization of the amplitudes, frequencies, or phases to the nearest allowed
symbol values. (8) Mapping of the quantized amplitudes, frequencies, or phases to
codewords (bit groups). (9) Parallel-to-serial conversion of the codewords into a bit-
stream. (10) Pass the resulting bitstream for further processing such as removal of
any error-correcting codes.
The above modulation and demodulation processing can be described mathemat-
ically as follows. An N-dimensional signal constellation is denoted by
A = {yj,1 ≤j ≤M}.
(3.40)
Its M elements yj will be called signal points, vectors, or N-tuples. The basic param-
eters of a signal constellation A are its dimension N; its size M (number of signal
points); its average energy E(A ) = 1/M ∑j ∥yj ∥2; and its minimum squared dis-
tance d2
min(A ), which is an elementary measure of its noise resistance. A secondary
parameter is the average number Kmin(A ) of nearest neighbors (points at distance
dmin(A )).
From these basic parameters we can derive such parameters as: the bit rate (nom-
inal spectral efﬁciency) ρ = (2/N)log2 M(b/2D); the average energy per two di-
mensions Es = (2/N)E(A ), or the average energy per bit Eb = E(A )/(log2 M) =
Es/ρ; energy-normalized ﬁgures of merit such as d2
min(A )/E(A ), d2
min(A )/Es,
or d2
min(A )/Eb, which are independent of scale. Figure 3.7 shows an example of
Figure 3.7 16-QAM.

62
3 Communication
16-QAM. It can carry M = 16 different symbols and a symbol consists of 4 bits.
Thus the modulation rate is ρ = 4.
In digital communications, we are usually interested in the minimum probability
of error (MPE) decision rule: given a received vector ˆy, choose the signal point y∗∈
A to minimize the probability of decision error Pr{E}. Since the probability that a
decision y∗is correct is simply the a posteriori probability p(y∗| ˆy), the MPE rule
is equivalent to the maximum a posteriori (MAP) rule: choose the y∗∈A such that
p(y∗| ˆy) is maximum among all p(yj | ˆy),yj ∈A . By Bayes law,
p(yj | ˆy) = p(ˆy | yj)p(yj)
p(ˆy)
.
(3.41)
If the signals yj are equiprobable, so p(yj) = 1/M for all j, then the MAP rule
is equivalent to the maximum likelihood (ML) rule: choose the y∗∈A such that
p(ˆy | y∗) is maximum among all p(ˆy | yj),yj ∈A . Using the noise PDF, we can
write
p(ˆy | yj) =
1
(2πσ2)N/2 e−∥ˆy−y j∥2/2σ2.
(3.42)
The symbol variance is σ2 = N0/2.

Part II
Scalable Video Coding
This part studies scalable video coding (SVC). The fundamental problem with
SVC is closely related to motion compensation. As we have discussed in Chapter 2,
every input picture will subtract a predictive picture in motion compensation. The
predictive picture must be exactly the same at the encoder and decoder. If there are
some differences between the predictive pictures used in the encoder and the decoder,
the differences in different pictures will be accumulated and thus the quality of de-
coded pictures will continuously drop, which is known as drifting error. In SVC, it is
hard to keep the predictive picture the same in both the encoder and decoder because
part of the compressed data may be dropped during transmission due to bandwidth
variation. Thus, the encoder does not know how many bits are really decoded and ex-
actly which predictive pictures are used in the decoder. This inevitably results in drift
error in SVC as long as motion compensation exists. In the following three chapters,
we will study how to reduce or eliminate the drift error in SVC, while keeping high
coding efﬁciency.
Chapter 4 introduces SVC and MPEG-4 ﬁne granularity scalable (FGS) coding.
A basic framework for efﬁcient scalable video coding, namely, progressive ﬁne gran-
ularity scalable (PFGS) coding, is presented. The PFGS framework has all the fea-
tures of MPEG-4 FGS, such as ﬁne granularity scalability, channel adaptation, and
error recovery. Unlike the FGS coding, the PFGS framework uses multiple layers of
references with increasing quality to make motion compensation more accurate for
improved coding efﬁciency. However, using multiple layers of references with dif-
ferent quality also creates several issues. First, extra picture buffers are needed for
storing the multiple reconstructed reference layers. This would increase the memory
cost and computational complexity of the PFGS scheme. Based on the basic frame-
work, a simpliﬁed and efﬁcient PFGS framework is further proposed. The simpliﬁed
PFGS framework needs only one extra picture buffer with almost the same coding
efﬁciency as the original framework. Second, there might be an undesirable increase
and ﬂuctuation of coefﬁcients to be coded when switching from a low-quality ref-
erence to a high-quality one, which could partially offset the advantage of using a
high-quality reference. A further improved PFGS scheme can eliminate the ﬂuctu-
ation of enhancement-layer coefﬁcients when switching references by always using
only one high-quality prediction reference for all enhancement layers.
Chapter 5 presents our research on using advanced motion threading technology
for improved performance in 3D wavelet coding. First, we extend an original motion
threading idea to a lifting-based implementation. Methods for enabling fractional-
pixel alignment in motion threading and for processing many-to-one pixel mapping

and non-referred pixels are proposed to reduce the wavelet boundary effects. Sec-
ond, we devise an advanced motion threading technology, in which one set of mo-
tion vectors is generated for each temporal layer of wavelet coefﬁcients for temporal
scalability. In order to reduce the motion overhead information, especially at low
bit rates, several correlated motion prediction modes at the macroblock level are de-
veloped to exploit the intra/inter-layer correlation in motion vector coding. Finally,
rate-distortion optimization is utilized in motion estimation to select the best motion
prediction mode for each macroblock.
Chapter 6 provides an overview of our Barbell-lifting coding scheme that was
adopted as the common software by the MPEG ad hoc group on further exploration
of wavelet-based video coding. The core technologies used in this scheme, such as
Barbell lifting, layered motion coding, 3D entropy coding, and base-layer embed-
ding, are discussed. The paper also analyzes and compares the proposed scheme
with the H.264 SVC standard because the hierarchical temporal prediction technique
used in H.264 SVC has a close relationship with motion compensated temporal ﬁl-
tering (MCTF) in wavelet coding. The commonalities and differences between these
two schemes are exhibited for readers to better understand modern scalable video
coding technologies. Several challenges that still exist in our Barbell-lifting coding
scheme, for example, performance of spatial scalable coding and accurate motion
compensated lifting, are also discussed. Two additional technologies are presented
here, although they are not yet integrated into the common software. Finally, we
compare it with H.264 SVC and another well-known 3D wavelet coding scheme,
Motion Compensated Embedded Zero Block Coding (MC-EZBC).

Chapter 4
Progressive Fine Granularity Scalable (PFGS)
Coding
4.1 Introduction
With recent developments in computing technology, such as high-capacity storage
devices, high-speed wired and wireless networks, and compression and transmission
technologies, more and more users expect to enjoy high-quality multimedia services
over the Internet [21–23]. Typically, there are two approaches to providing multime-
dia services on-demand: ofﬂine downloading and online streaming. Since the stream-
ing approach enables users to experience a multimedia presentation on the ﬂy while
it is being downloaded from the Internet, it has prevailed in both academia and in-
dustry circles. In virtue of the streaming technology, users no longer have to suffer
from long and even unacceptable transmission time for full downloads.
Figure 4.1 exempliﬁes a typical scenario for streaming video contents to users.
Raw video sequences are usually compressed in advance and then saved on the
storage device. Upon the client’s request, the streaming server retrieves compressed
video data from the storage device and delivers it through the Internet that consists of
many heterogeneous sub-networks. Receivers may use different devices for decod-
ing and presenting received video data with different resolutions, frame rates, and
qualities. Scalable video coding (SVC) provides a good compression solution for the
above application. A video sequence is coded into a scalable stream, which consists
of multiple layer streams. Only the base layer stream can be independently decoded
and provides the lowest visual quality, whereas the enhancement layer streams are
decoded together with the base layer stream and lower enhancement layer streams
and enhance the base or lower layer video from frame rate, resolution, and quality,
respectively. Therefore, SVC techniques are typically categorized into three types,
that is, quality scalability, temporal scalability, and spatial scalability.
Quality scalability provides different quality representations of coded visual infor-
mation within a single scalable stream [24–26]. The base layer is coarsely quantized,
and the difference between the reconstructed base layer picture and original picture
is coded into one or multiple enhancement layers with ﬁner quantization. Temporal
scalability is able to present coded visual information at varying frame rates from a
65

66
4 Progressive Fine Granularity Scalable (PFGS) Coding
Figure 4.1 An example scenario of streaming video.
single scalable stream [27–30]. The base layer video is set at a low frame rate. By
decoding all temporal pictures, users can get the smoothest presentation. Although
the ofﬁcial emergence of temporal scalability in the video coding standard is from
MPEG-2, it can be essentially fulﬁlled even in MPEG-1 by dropping B pictures. Spa-
tial scalability provides a multi-resolution representation of coded visual information
within a single scalable stream [31–34]. In general, the base layer represents a low-
resolution video, and the enhancement layer provides additional data to reproduce
video at high resolution. Such compression techniques are particularly useful for the
streaming server to simultaneously serve PC devices and non-PC devices only with
low-resolution screen and limited computational power.
All these scalable video coding techniques (quality, temporal, and spatial) pro-
vides only coarse granularity scalability, in which an enhancement layer is either
available and decoded as a whole or not available at all. Accordingly, this is known
as layered coding. These techniques only have limited adaptation on channel band-
width variations. Furthermore, multiple-layer coding techniques suffer from coding
efﬁciency loss due to inefﬁcient motion compensation and overhead bits. This is the
reason why the industry has chosen to use stream switching in commercial stream-
ing systems instead of scalable video coding, although the latter has achieved great
success in video coding standards, such as MPEG-2, MPEG-4, and H.264.
4.2 Fine Granularity Scalable Video Coding
Unlike the layered video coding techniques we have discussed, MPEG-4 FGS pro-
vides ﬂexible and precise adaptation on channel bandwidth variations, where the
enhancement layer streams can be arbitrarily truncated. The requirements on FGS

4.2 Fine Granularity Scalable Video Coding
67
video coding were brought forward in MPEG [35]. It is well known that motion
compensation is extensively used to fully exploit temporal redundancy among pic-
tures of a video for high coding efﬁciency. However, utilizing motion compensation
usually creates a problem with providing ﬁne granularity scalability due to temporal
dependency of one picture on another. Any truncation on reference bits will bring
drifting errors without exception.
When MPEG-4 launched FGS, it had already adopted a wavelet-based coding
scheme for still texture. Matching pursuit [36–38] and bit-plane coding [39] tech-
niques were being evaluated. For backwards compatibility, four candidate solutions
were proposed by Chen et al. [40]: discrete cosine transform (DCT) base layer plus
wavelet enhancement layer; DCT base layer plus matching pursuit enhancement
layer; DCT base layer plus DCT enhancement layer; and matching pursuit base
layer plus matching pursuit enhancement layer. The solution with wavelet-based cod-
ing techniques [41, 42] for both the base layer and the enhancement layer is out of
the question, although it inherently provides ﬁne granularity scalability. In the suc-
ceeding meetings, MPEG established a core experiment on ﬁne granularity scala-
bility [43]. Based on common conditions, experimental results of wavelet coding of
image residues [44–46], matching pursuit coding of image residues [47,48], and bit
plane coding of DCT residues [49] were proposed to MPEG and evaluated. Finally,
the bit plane coding of DCT residues was chosen as the baseline by MPEG-4 due to
its comparable coding efﬁciency and implementation simplicity.
Figure 4.2 illustrates the block diagram of the MPEG-4 FGS encoder. The lower
part outlined by a box is the base layer encoder. If an input picture is coded with
temporal prediction, the displacement of each macroblock is ﬁrst estimated based
on the association with its reference stored in the Buffer. Each inter macroblock has
either one or four MVs. The precision of a motion vector is up to a quarter of a pixel.
Temporal prediction is generated by displacing the reference macroblocks or blocks,
where integer pixels are directly copied from the reference and fractal pixels are ob-
tained from neighboring integer pixels in terms of interpolation. The input picture for
intra coding or the predictive residual picture obtained by subtracting the temporal
prediction from the input picture is performed with a DCT at each 8 × 8 block. Af-
ter quantization and ZigZag scanning, the obtained DCT data is coded with variable
length coding (VLC) to form the output base layer stream. With de-quantization and
an inverse DCT transform, the reconstructed picture or the reconstructed residual
picture plus the temporal prediction is used to update the Buffer after clipping for
the next picture coding.
The upper part in Figure 4.2 outlined by a box is the enhancement layer encoder.
The quantized residues of the base layer video calculated in the image domain by
subtracting the reconstructed image block from the input image block are coded with
bit-plane coding. The obtained residues are then performed with a DCT transform on
each 8 × 8 block. The DCT residues may be shifted up with frequency weighting
and selective enhancement techniques for better visual quality and enhanced inter-
ested regions. Finally, the DCT residual picture is processed bit-plane by bit-plane to
generate the output enhancement layer stream.

68
4 Progressive Fine Granularity Scalable (PFGS) Coding
Base Layer
Stream
Enhancement
Layer Stream
ME
MC
+
Buffer
DCT
Q
Q-1
IDCT
+
Clip
VLC
-
+
Video
DCT
BP VLC
Frequency
Weighting
Selective
Enhancement
Figure 4.2 The block diagram of the MPEG-4 FGS encoder.
Base Layer
Stream
Enhancement
Layer
Video
Base Layer
Video
(optional)
Enhancement
Layer
Stream
VLD
Q-1
MC
IDCT
+
Buffer
Clip
BP VLD
IDCT
+
Clip
Frequency 
Weighting
Selective
Enhancement
Figure 4.3 The block diagram of the MPEG-4 FGS decoder.
Figure 4.3 illustrates the block diagram of the MPEG-4 FGS decoder. The lower
part outlined by a box is the base layer decoder, and the upper part is the enhance-
ment layer decoder. The base layer stream is ﬁrst decoded with variable length de-
coding (VLD) to reconstruct DCT data. After de-quantization and inverse DCT, the
reconstructed picture or the reconstructed residual picture plus the temporal predic-
tion forms the decoded base layer video. In general, it is stored in the Buffer for the

4.3 Basic PFGS Framework
69
next picture coding. It is also an optional display picture if the enhancement layer is
not available.
To reconstruct the enhancement layer picture, the input stream is ﬁrst decoded
using bit-plane VLD. The decoded block bit-planes are used to compose DCT data
according to their binary locations, which may be then shifted down based on the fre-
quency weighting and selective enhancement shifting factors. After inverse discrete
cosine transform (IDCT), the image domain residues are reconstructed. They are
added to the reconstructed clipped base layer pixels to reconstruct the enhancement
layer pixels. The reconstructed enhancement layer pixels are limited to a value range
between 0 and 255 by the clipping process at the enhancement layer to generate the
ﬁnal enhancement layer picture.
4.3 Basic PFGS Framework
One major feature of the FGS coding scheme is that the base layer and all enhance-
ment layers in a predicted picture are always predicted from the reconstructed ver-
sion of the base layer in the reference picture. Therefore, the FGS coding scheme
provides excellent error recovery from occasional data losses or errors in enhance-
ment layers. By predicting all enhancement layers from the base layer, losses or
corruptions of one or more enhancement layers during transmission have no effect
on the pictures that follow. However, since the prediction is always based on the low-
est quality base layer, the coding efﬁciency of the FGS scheme is not as good as, and
sometimes much worse than, traditional SNR scalability schemes, such as in Macni-
col et al. [50]. On the other hand, in traditional signal-to-noise ratio (SNR) scalability
schemes, same layer references are used to provide better predictions, which in turn
normally provide better coding efﬁciency. But once there is an error or packet loss in
the enhancement layers, it would propagate to the end of a group of pictures (GOP)
and would cause serious drifting errors in the higher layers of the following predicted
pictures. Even though there may be sufﬁcient bandwidth available later, the decoder
could not recover to the highest quality until another GOP starts. Therefore, the tradi-
tional SNR scalability schemes are normally only suitable for simulcasting in stable
channels.
In order to improve the coding efﬁciency of FGS, a basic framework for more efﬁ-
cient scalable video coding was ﬁrst proposed to MPEG-4 by Li et al. [51], referred to
as PFGS video coding. Similar to FGS, the PFGS coding scheme also encodes video
pictures into multiple layers, including a base layer of relatively lower quality video
and multiple enhancement layers of increasingly higher quality video. However, in
the PFGS framework we try to use several high-quality references for the predictions
in the enhancement-layer encoding rather than always using the base layer. Using
high-quality references would make motion prediction more accurate and thus could
improve coding efﬁciency. Our experimental results show that the PFGS scheme can
achieve consistently better coding efﬁciency than the FGS scheme while keeping all

70
4 Progressive Fine Granularity Scalable (PFGS) Coding
the properties of FGS, such as ﬁne granularity scalability, channel adaptation, and
error recovery.
There are still several issues to be addressed in the basic PFGS framework pro-
posed by Li et al. [51]. First, it needs multiple extra picture buffers to save multiple
reconstructed layers as references, which increases the memory cost and computa-
tional complexity of the PFGS encoder and decoder. Fortunately, not every reference
layer makes the same contribution to the improvement of coding efﬁciency. Only a
few among the reference layers make signiﬁcant contributions to improving coding
efﬁciency, while others have little effect. How to choose a minimal number of ref-
erence layers to achieve high coding efﬁciency remains an open problem. Another
problem is the ﬂuctuation and increase of DCT coefﬁcients when switching from
a low-quality reference to a high-quality one. An efﬁcient approach that can take
full advantage of a high-quality reference without causing any ﬂuctuation should be
investigated to further improve the coding efﬁciency of the basic PFGS framework.
4.3.1 Basic Ideas to Build the PFGS Framework
As discussed in the previous section, the FGS video-coding scheme provides very
good bandwidth adaptation and error recovery properties, but it sacriﬁces coding
efﬁciency. On the other hand, the SNR scalability traditional schemes have good
coding efﬁciency, but they lose bandwidth adaptation and error recovery properties.
Is there a new framework that can balance between coding efﬁciency and scalability
properties? In this section, we try to present such a general framework that can keep
the properties of the FGS coding scheme, such as ﬁne granularity scalability, chan-
nel adaptation, and error recovery, while using as many predictions from the same
reference layer as possible as in the traditional SNR scalability schemes.
There are two key points in designing such a framework. The ﬁrst point is to use
as many predictions from the enhancement reference layers as possible (for coding
efﬁciency), instead of always using the base layer as in the FGS scheme. Since the
quality of an enhancement layer is better than that of the base layer, such a framework
makes motion compensation as accurate as possible for any given video layer to
maintain coding efﬁciency. The second point is to keep a prediction path from the
base layer to the highest quality layers across several pictures (for error recovery
and channel adaptation). This will make sure that the coding schemes can gracefully
recover from losses or errors. Lost or erroneous higher quality enhancement layers
may be automatically reconstructed from lower layers gradually over a few pictures
with such a prediction path.
Figure 4.4 conceptually illustrates such an exemplary framework for efﬁcient
video coding with no drifting problem. In the illustrated framework, picture 2 is pre-
dicted from the base layer and even enhancement layers of picture 1 (i.e., the second
and fourth enhancement layers). Picture 3 is predicted from the base layer and odd
enhancement layers of picture 2 (i.e., the ﬁrst and third enhancement layers). Picture
4 is again predicted from the base layer and even enhancement layers of picture 3,

4.3 Basic PFGS Framework
71
Base Layer
1st Enhancement
Layer 
2nd Enhancement
Layer
3rd Enhancement
Layer
1
2
3
4
5
Pictures
4th Enhancement
Layer
Figure 4.4 PFGS framework.
and so on. It is obvious that the three highest enhancement layers in picture 2 are
predicted from high-quality reference layers in picture 1, rather than from the base
layer in picture 1. Since the quality of an enhancement layer is higher than that of
the base layer, the framework shown in Figure 4.4 provides more accurate motion
prediction to improve coding efﬁciency. On the other hand, many prediction paths
from the lowest quality layer to the highest quality layer are preserved. For instance,
the base layer of picture 1, the ﬁrst enhancement layer of picture 2, the second en-
hancement layer of picture 3, the third enhancement layer of picture 4, and the fourth
enhancement layer of picture 5 constitute such a complete path.
The advantages of the PFGS framework are obvious when it is applied to video
transmission over the Internet or wireless channels. The encoded bitstream can adapt
to the available bandwidth of the channel without any drifting problem. Figure 4.4
shows an example of this bandwidth adaptation process. The dashed line traces the
transmitted video layers. Note that at picture 2, there is a reduction in bandwidth.
At this picture, the transmitter (server) simply drops the bits of higher layers (from
the second to the fourth enhancement layers). However, after picture 2, as the band-
width increases, the transmitter simply transmits more layers of video bits. After
three pictures (at picture 5), the decoder side can obtain up to the highest quality
video layer again. Note that in all these operations, no re-encoding or retransmission

72
4 Progressive Fine Granularity Scalable (PFGS) Coding
of the video bitstream is required. Similarly, when several enhancement layers in one
or more pictures have packet losses or errors, the recovery process is the same as that
of the bandwidth adaptation case. We can see that the processes of bandwidth adap-
tation and error recovery are graceful and gradual progressively recovering across
several frames. Therefore, this framework is called PFGS video coding.
Figure 4.4 exempliﬁes a case where the group depth is 2. Group depth deﬁnes
how many layers may refer back to a common reference layer. The group depth can
be changed in each picture. If the group depth is 1, the PFGS framework essentially
becomes the traditional SNR scalability schemes as in Macnicol et al. [50]. If the
group depth is equal to the total number of layers, the PFGS framework essentially
represents FGS in Ling et al. [52] and Li [49]. The above description is a special case
of a more general case where in each picture the reference layers used for prediction
can be randomly assigned as long as a prediction path from lowest layer to highest
layer is maintained across several pictures.
4.3.2 The Simpliﬁed PFGS Framework
It is very clear that compared to FGS, the implementation of such a PFGS framework
as shown in Figure 4.4 needs several extra picture buffers to save the reconstructed
enhancement layers as references. In Figure 4.4, there are two extra buffers needed
for encoding each picture. For example, the second and fourth enhancement layers
in picture 1 are used as references for enhancement layer coding in picture 2, and
the ﬁrst and third enhancement layers in picture 2 are also used as references for the
enhancement layer coding in picture 3, and so on. In fact, the number of extra picture
buffers increases as the number of enhancement layers increases. If we could reduce
the number of extra picture buffers to a minimum number while still maintaining
almost the same coding efﬁciency, it will be a very signiﬁcant complexity reduction
for the PFGS implementation, especially for the hardware implementation.
Although the PFGS framework can use as many additional reference buffers as
possible to achieve greater coding efﬁciency improvement, we still prefer a much
more simpliﬁed PFGS framework that can provide a good trade-off between cod-
ing efﬁciency versus the memory cost and computational complexity. Not all en-
hancement layers are well suited to be used as references. Only a few enhancement
layers as references can make a signiﬁcant contribution to improving coding efﬁ-
ciency, whereas others have little effect. Generally, the lower enhancement layers are
not good references. Since these layers may comprise errors with large magnitudes
caused by motion across frames, the correlations of lower enhancement layers be-
tween adjacent pictures are weak. Here, the correlation between the two same layers
in adjacent pictures is deﬁned as the absolute sum of binary difference of two bit
planes. The larger the absolute value is, the weaker the correlation between them.
The higher enhancement layers are not good references either. First, the bit rate of
the higher enhancement layers is too high for most applications. The gain by using
a high-quality reference will appear only at a very high bit rate. Second, the small

4.4 Improvements to the PFGS Framework
73
magnitude errors encoded in these layers may be just produced by noise. Therefore,
the correlation of higher enhancement layers between adjacent pictures are weak
too. Only by using the middle enhancement layers as references can we achieve
maximum coding efﬁciency improvement, since the DCT coefﬁcients in the middle
enhancement layers show strong correlations between adjacent pictures.
A simpliﬁed PFGS framework with only two picture buffers could offer a good
trade-off between coding efﬁciency versus the extra memory cost and computational
complexity. In the simpliﬁed PFGS framework, the ﬁrst picture buffer is used to save
the reconstructed base layer in a previous picture as a reference for the base layer
and the lower quality enhancement layers in a predicted picture. The second picture
buffer is used to save a reconstructed enhancement layer in a previous picture as
a reference for the higher quality enhancement layers. An exemplary framework is
shown in Figure 4.5. The base layer and the ﬁrst two enhancement layers of picture
2 are predicted from the base layer of picture 1, and other higher quality enhance-
ment layers of picture 2 are predicted from the third enhancement layer of picture
1. Instead of using a ﬁxed enhancement layer, the reference alternates between two
different enhancement layers to form a complete prediction path from the base layer
to the highest enhancement layer for error recovery and channel adaptation. Which
enhancement layer is used as the high-quality reference depends on the contents of
the video sequence and the bit rate of the base layer. How to choose such an en-
hancement layer is an optimization issue in the encoding process. The index of the
enhancement layer selected as the reference can be encoded as part of the video
bitstream.
Since the framework in Figure 4.5 uses two reference layers for the prediction,
it produces two sets of predicted DCT coefﬁcients: (1) a ﬁrst set of predicted DCT
coefﬁcients are prediction errors formed by referencing the base layer, which is a
low-quality reference layer and (2) a second set of predicted DCT coefﬁcients are
prediction errors formed by referencing a higher quality enhancement layer. The ﬁrst
set of predicted DCT coefﬁcients are encoded in the base layer and lower enhance-
ment layers, and the difference between the second set of predicted DCT coefﬁcients
and those reconstructed from the base layer and low enhancement layers is encoded
to form higher enhancement layers.
4.4 Improvements to the PFGS Framework
4.4.1 Potential Coding Inefﬁciency Due to Two References
The goal of using the second high-quality reference is to reduce the bit rate of the
higher enhancement layers. As shown in Figure 4.5, the second set of predicted DCT
coefﬁcients will have a statistically lower magnitude compared with the ﬁrst set of
predicted DCT coefﬁcients because its reference is higher quality and hence closer
to the original image. Theoretically, we also expect that the differences between the

74
4 Progressive Fine Granularity Scalable (PFGS) Coding
Base Layer
1st Enhancement
Layer 
2nd Enhancement
Layer
3rd Enhancement
Layer
1
2
3
4
5
Pictures
4th Enhancement
Layer
Figure 4.5 Simpliﬁed PFGS framework with only two buffers.
second set of predicted DCT coefﬁcients and those reconstructed from the base layer
and lower enhancement layers are smaller than the residues of the ﬁrst set of pre-
dicted DCT coefﬁcients after the base layer and lower enhancement layer encoding.
Lower DCT differences translate into fewer coding layers, therefore resulting in bet-
ter coding efﬁciency. However, the expectation is only valid statistically.
The displaced frame difference (DFD) image is deﬁned as the difference between
the original image and the motion predicted image. With the linear DCT transforms,
the DCT coefﬁcients of the DFD image are equal to the difference between the DCT
coefﬁcients of the original image and that of the predicted image. There are two kinds
of differential operations in PFGS coding schemes for improving coding efﬁciency.
The ﬁrst kind of differential operations are essentially the normal motion compen-
sation operations that are used to generate the DFD in order to reduce temporal re-
dundancy. These subtractive processes are denoted with horizontal solid-line arrows
between adjacent frames in Figure 4.5. The second kind of differential operations are
performed on the predicted DCT coefﬁcients within one picture after switching from
a lower quality reference to a higher quality one to reduce DCT coefﬁcient redun-
dancy. Normally, the results of the second kind of differential operations are called
residues. For convenience, we deﬁne the following terminology for referencing these
differences:

4.4 Improvements to the PFGS Framework
75
• LQPi—Low-quality predicted image that is generated by motion compensation
from the lower quality reference
• LQPd—Difference between LQPi and the original image
• LQPD—DCT coefﬁcients of LQPd
• LQBR—DCT residues that are produced by subtracting the already encoded
(quantized and then de-quantized) DCT coefﬁcients in the base layer from LQPD
coefﬁcients
• LQPR—DCT residues that are produced by subtracting the already encoded
DCT coefﬁcients in the previous layers from the LQPD coefﬁcients
• HQPi—A high-quality predicted image that is generated by motion compensa-
tion from the higher quality reference
• HQPd—Differences between HQPi and the original image
• HQPD—DCT coefﬁcients of HQPd
• HQPR—DCT residues that are produced by subtracting the already encoded
DCT coefﬁcients in the previous layers from the HQPD coefﬁcients
• HQBR—DCT residues that are produced by subtracting the already encoded
DCT coefﬁcients in the base layer from the HQPD coefﬁcients
Speciﬁcally, Figure 4.6 illustrates the relationships between the above terminol-
ogy. It is obvious that the HQPi will produce lower DFD DCT coefﬁcients compared
with the LQPi because the reference is of a higher quality. However, the dynamic
range of the HQPD coefﬁcients is not necessarily always less than that of the LQPD
coefﬁcients. For some instances, the magnitude of individual HQPD coefﬁcients may
actually increase compared with that of the LQPD coefﬁcients due to non-ideal mo-
tion compensation. Moreover, in order to reduce the redundancy between LQPD co-
efﬁcients and HQPD coefﬁcients and to further improve coding efﬁciency, normally
only the difference between the HQPD and the reconstructed low-layer LQPD; that
is, the residue HQPR is coded in higher enhancement layers. Although doing so gen-
erally will reduce the energy of the coefﬁcients to be coded, the dynamic range of
the difference actually would increase (causing more ﬂuctuation) and an additional
sign map would be required to code the different HQPR.
Both the undesired ﬂuctuation in magnitude and the additional sign bit are partic-
ularly inefﬁcient for bit-plane coding. First, if the undesired ﬂuctuation and increase
in magnitude exceed the range represented by residual bit planes, it would seriously
affect the coding efﬁciency of bit-plane coding. For example, assume that three bit
planes are used to encode the LQPR in higher enhancement layers when referenc-
ing a low-quality layer. However, after switching to a higher quality reference, if the
absolute value of an individual HQPR coefﬁcient exceeds 7, one or multiple addi-
tional bit planes have to be inserted between lower and higher enhancement layers to
represent the excess range. Second, the sign of HQPR may be different from that of
LQBR. Since the sign bit of every coefﬁcient is encoded after the MSB of that coef-
ﬁcient, for those coefﬁcients whose most signiﬁcant bits (MSBs) have been encoded
in the base layer and lower enhancement layers, new signs have to be encoded again.

76
4 Progressive Fine Granularity Scalable (PFGS) Coding
Base Layer 
Reference 
 Original 
Image 
+
Enhanced 
Layer 
Reference
Qb
Qb-1
QL
QL-1
QH
QH-1
+
+
+
+
HQPD
LQPD
-
-
-
Base Layer Bitstream
Lower Enhancement 
Layer Bitstream
Higher Enhancement 
Layer Bitstream
Reconstructed 
Lower Layer LQPD 
Reconstructed 
Base layer 
LQPD 
+
Reconstructed 
Higher Layer HQPD 
DCT
DCT
LQPd
HQPd
HQPR
LQBR
HQPi
LQPi
-
+
-
HQBR
For all  enhancement 
coding
MC
MC
+
-
LQPR
Figure 4.6 Illustration of the relationships of all the differences in the PFGS framework.
4.4.2 A More Efﬁcient PFGS Framework
Ideally, in order to avoid excess ﬂuctuation and increase in magnitude of the predic-
tion residues mentioned above, the bit planes encoded in lower and higher enhance-
ment layers should be from one set of prediction residues. In addition, the extra sign
layer is also completely avoided by doing so. In the baseline FGS case, the bit planes
of all enhancement layers are indeed always from the same set of residue LQBRs.
However, since it always uses base layer video as references, the overall coding efﬁ-
ciency is limited, especially for higher enhancement layers. On the other hand, in the
PFGS framework shown in Figure 4.5, only the lower enhancement layers include
bit planes from the LQBR, while the higher enhancement layers encode bit planes
from HQPR as shown in Figure 4.6, causing exactly the same problem.
A conditional replenishment method described by Tan et al. [53] can be used to
eliminate this kind of ﬂuctuation when switching from a lower quality reference to
a higher quality one. In the conditional replenishment scheme, not all the LQPR
coefﬁcients are replaced by the HQPR coefﬁcients. The lower layer prediction coef-
ﬁcients LQPR are conditionally replaced by the higher layer prediction coefﬁcients
HQPR depending on the values of the reconstructed lower layer LQPD. If the re-
constructed lower layer coefﬁcient LQPD is zero then the corresponding coefﬁcient
in LQPR is replaced with that in HQPR. If the reconstructed coefﬁcient is not zero,
then no replacement is done. Though the conditional replenishment can solve the
ﬂuctuation problem, it partially loses the advantages of using a high-quality refer-
ence. Nonzeros in the reconstructed lower layer coefﬁcients LQPD are an essential

4.4 Improvements to the PFGS Framework
77
condition to cause ﬂuctuation, but not a sufﬁcient one. In many cases, even though
there are nonzeros in the reconstructed lower layer coefﬁcients, the residues can still
be reduced by replacing the prediction coefﬁcients LQPR with HQPR. On the other
hand, the conditional replenishment method essentially needs a new reference mixed
with the lower quality reference and the higher quality reference in DCT domain. For
the decoder, since only two predicted image LQPi and HQPi are available, two extra
DCT transforms will be needed in order to get the corresponding LQPD and HQPD,
thus the computational complexity of a decoder will be increased. Therefore, a more
efﬁcient and simple approach that can take full advantage of a high-quality reference
without causing any ﬂuctuation should be investigated to further improve the coding
efﬁciency.
We propose solving this problem within the PFGS framework. The improved
PFGS framework based on a new structure is given in Figure 4.7. In this improved
framework, while the base layer encoding is still the same as that in the baseline
FGS and PFGS framework in Figure 4.5, all enhancement layers encode differences
between the HQPD and the de-quantized LQPD from the base layer, that is, HQBR.
Note that now the lower enhancement layers contain the ﬁrst few most signiﬁcant bits
in the HQBR coefﬁcients instead of that of the LQBR coefﬁcients as in the baseline
FGS and the PFGS framework in Figure 4.5. Since all enhancement layers always
encode the same set of coefﬁcient HQBRs in the improved PFGS framework, there
is neither sign change nor ﬂuctuation among any enhancement layers.
The lower enhancement layers, which also encode the differences between HQPD
and reconstructed base layer LQPD, would introduce several problems. It seems that
this would cause drifting errors if the HQPi reference were not available at the de-
coder. Moreover, this seems to destroy the channel adaptation and error recovery
properties with FGS/PFGS. Indeed, when the HQPi reference is not available at the
decoder, we have to use the LQPi reference instead; this introduces some errors in
lower enhancement layers due to the different references used in the encoder and
the decoder. In fact, the overall quality loss in the lower enhancement layers is very
small, because a better prediction compensates most of the loss caused by using dif-
ferent references.
How to minimize the quality loss in the lower enhancement layers is not impor-
tant. It is much more important to ﬁnd a way to prevent these errors in one picture
from propagating to other pictures. Fortunately, the framework shown in Figure 4.7
suggests a scheme to solve the potential error-drifting problem completely. The key
here is to make sure that the encoder and decoder have the same reconstructed ref-
erences for any future picture prediction although the reconstructed reference may
not have the best quality it could get if reconstructed using a high-quality reference.
We will show this through an example. As in FGS, we still assume there is no error
in the base layer. In the decoder end, if there are packet losses or errors in the third
enhancement layer in picture 1, which is used in the encoder end to get the HQBR
coefﬁcients, all the enhancement layers in picture 2 will have to use the base layer
in picture 1 as a reference. Of course, there would be some quality loss by doing so.
However, as long as in both the encoder end and the decoder end the reconstruction
of the second enhancement layer of picture 2 always uses the base layer of picture

78
4 Progressive Fine Granularity Scalable (PFGS) Coding
Base Layer
1st Enhancement
Layer 
2nd Enhancement
Layer
3rd Enhancement
Layer
1
2
3
4
5
Pictures
4th Enhancement
Layer
Figure 4.7 Improved PFGS framework. Solid arrows are for prediction references and hollow ar-
rows with solid lines for reconstruction references, hollow arrows with dashed lines are for recon-
struction of lower layers when the previous enhancement reference layer is not available.
1 as the reference, then the errors in the reconstruction of the second enhancement
layer could not further propagate to any pictures followed. The unique feature of this
improved framework is that in a prediction picture, the reference used for prediction
could be different from the reference used for reconstruction. This feature prevents
error drifting and preserves all bandwidth adaptation and error recovery features of
PFGS. Moreover, it brings more coding efﬁciency gain in the higher enhancement
layers. In Figure 4.7, for example, we notice that the second enhancement layer in
picture 2 can always be reconstructed by referencing the base layer of picture 1 to
prevent the error drifting into future pictures. However, if the third enhancement
layer of picture 1 is available, a better quality second enhancement layer of picture
2 can still be reconstructed with it for display purpose only. The reconstruction of a
display image can be different from that of reference image is yet another feature of
the improved framework.
The architecture of encoding B pictures using the improved framework is shown
in Figure 4.8. The bidirectional motion estimation determines the type of motion
compensation and motion vectors by referencing the original picture of a previous
intra I or prediction P picture and the original picture of the next I or P picture. The

4.5 Implementation of the PFGS Encoder and Decoder
79
Base Layer
1st Enhancement
Layer
2nd Enhancement
Layer
3rd Enhancement
Layer
I/P
B
Pictures
4th Enhancement
Layer
I/P
Figure 4.8 B pictures encoding in the improved PFGS framework.
LQPD coefﬁcients of the B picture are formed by referencing the base layer in a
forward reference picture and/or the base layer in a backward reference picture. At
the same time, the HQPD coefﬁcients of the B picture are formed by an enhancement
layer in a forward reference picture and/or the enhancement reference in a backward
reference picture. Similar to P pictures, all the enhancement layers in a B picture are
encoded with the difference between HQPD and de-quantized LQPD from the base
layer. Because none of the enhancement layers in a B picture is used as a reference
for other pictures, the errors in lower enhancement layers of pictures have no effect to
any other pictures. However, when there are enhancement reference layers available,
they can be certainly used to produce better quality B pictures.
4.5 Implementation of the PFGS Encoder and Decoder
In the previous section, we discussed an improved PFGS framework. How to imple-
ment the encoder and decoder based on the improved PFGS framework will be the
focus of this section. First, an encoder with two reference buffers for video prediction
is given in Figure 4.9. Picture Buffer 0 is used to save the reconstructed base layer in
a previous picture as a reference for the base layer coding. Picture Buffer 1 is used

80
4 Progressive Fine Granularity Scalable (PFGS) Coding
-
Picture
Buffer 0
Picture
Buffer 1
Find
Max
Bit Plane
ME
MC
DCT
IDCT
VLC
IDCT
DCT
Q-1
Q
V
L
C
Bitstream
Bitstream
+
+
n(t)
+
+
MC
+
+
Base Layer 
Encoder
-
-
Video
S
Enhancement 
Layer Encoder
HQPD 
LQPD 
Figure 4.9 Encoding diagram of the improved PFGS framework.
to save a reconstructed enhancement layer in a previous picture as a reference for
coding all enhancement layers.
The base layer encoding is the same as that of baseline FGS, which can be compat-
ible with other standards, such as MPEG-2, MPEG-4, and H.263. The motion estima-
tion (ME) module gets the motion vectors between two adjacent original pictures and
outputs its results to two motion compensators (MCs). The ﬁrst motion compensator
predicts the picture by referencing the reconstructed base layer in Picture Buffer 0.
The second motion compensator predicts the picture by referencing a reconstructed
enhancement layer in Picture Buffer 1. After the ﬁrst motion compensation and DCT
transform of the DFD image, we obtain LQPD coefﬁcients in the base layer encoder
as shown in Figure 4.9. The LQPD coefﬁcients are quantized by scalar quantization
and compressed by VLC into the base layer bitstream. Generally, the step size of the
scalar quantizer is large in order to generate a relatively short bitstream.

4.5 Implementation of the PFGS Encoder and Decoder
81
On the other hand, the second motion compensator and the second DCT mod-
ule generate the HQPD, which are the DCT transforms of the DFD image with a
high-quality reference. The differences (residues) between HQPD and the recon-
structed LQPD from the base layer are encoded in all the enhancement layers. The
bit plane coding technique is used in the enhancement layers to provide an embed-
ded bitstream and ﬁne granularity scalability. For the improved PFGS framework, the
difference now is that all the enhancement layers always use an enhancement layer
in the previous picture as a reference as opposed in the baseline FGS case that all
enhancement layers always use the base layer in the previous picture as a reference.
The maximum absolute value of the differences determines the maximum number of
bit planes in a picture. The 64 absolute values in a 8×8 difference block are arranged
in a ZigZag order into an array. A bit plane is deﬁned as an array of 64 bits, which are
taken from each of the 64 absolute values at the same signiﬁcant bit position. This
implies that the quantization steps of the enhancement layers are a series of factor 2i ,
where i is from the maximum number of bit planes to 0. For the lowest enhancement
layer, i equals the maximum number of bit planes and for the highest enhancement
layer, i equals 0. For each bit plane of each block, (RUN, EOP) symbols are formed
and encoded using variable length codes to produce the enhancement bitstream for
that bit plane. EOP stands for the “End Of Plane.” The sign bit of each difference
coefﬁcient is encoded with one bit following the MSB of that coefﬁcient as the coef-
ﬁcient’s MSB gets encoded. The binary “0” denotes a positive difference and binary
“1” denotes a negative one.
The enhancement layer in the current picture used for the prediction of enhance-
ment layers in the next picture is reconstructed using the ﬁrst n(t) bit planes in the
bitstream and the reference of either the base layer or the enhancement reference
layer in the previous picture, which is controlled by the switch S. If the enhancement
reference layer for the next picture is a higher layer than the enhancement reference
layer for the current picture, that is, n(t) > n(t −1), then the enhancement reference
layer in the previous picture will be used to reconstruct the enhancement reference
layer for the next picture. Otherwise, the base layer in the previous reference picture
will be used to reconstruct the enhancement reference layer for the next picture.
Figure 4.10 gives a diagram of a PFGS video decoder with two reference buffers.
The two-buffer conﬁguration offers a good trade-off between coding efﬁciency and
extra cost in memory and computational complexity. The ﬁrst picture buffer is used
to save the reconstructed base layer in a previous picture as the reference for the
base layer. It can also be used as the reconstruction reference for some lower quality
enhancement layers to generate a display image when the higher quality reference
in a previous picture is not available due to errors or packet losses. A second picture
buffer is used to save the reconstructed enhancement layer in a previous picture as
the prediction reference for all enhancement layers. The decoder is very similar to
a baseline FGS decoder but with additional modules to reconstruct and save a mid-
dle enhancement layer as a second reference. The switch S in the decoder is used to
control which buffer to be used to form the next enhancement layer reference. Com-
pared with FGS, an additional buffer, an additional MC module, and an additional
IDCT are needed in the improved PFGS framework. In Figure 4.10, n(t) denotes the

82
4 Progressive Fine Granularity Scalable (PFGS) Coding
Picture
Buffer 1
MC
IDCT
V
L
D
Bitstream
MC
Lower Enhancement 
Layer Decoder
Base Layer 
Decoder
Video
IDCT
Q-1
+
Clipping
Picture
Buffer 0
MVs
S
+
+
Video
Clipping
MVs
VLD
Bit Plane
IDCT
Higher Enhancement 
Layer Decoder
Bitstream
+
+
Video
Clipping
m(t)
n(t)
Figure 4.10 Decoding diagram of the improved PFGS framework.
number of bit-plane layers needed to reconstruct the next enhancement reference
layer. m(t) denotes the number of bit-plane layers used to reconstruct the current
display image.
4.6 Experimental Results and Analyses
Extensive experiments and simulations have been performed to test the perfor-
mance of the PFGS framework. First, a simple simulation experiment is designed to

4.6 Experimental Results and Analyses
83
demonstrate that with only one extra picture buffer the simpliﬁed PFGS framework
can still provide most of the coding efﬁciency gain obtained in the original PFGS
framework with multiple buffers. The PFGS framework with multiple references is
shown in Figure 4.4 and the simpliﬁed PFGS framework with only two references
is shown in Figure 4.5. These two frameworks are almost the same except for the
number of references. At the same time, this experiment also gives the FGS results
at the same testing conditions.
All test conditions are the same as those speciﬁed in the MPEG-4 core exper-
iments. The sequences Akiyo, Foreman, and Coastguard (CIF format) are used in
this experiment. Only the ﬁrst picture is encoded as an I picture and all the other
pictures are encoded as P pictures. The base layer encoder is a predictive encoder
that includes motion compensation and DCT transformation modules, and it could
be compatible to other standards, such as H.263, MPEG-2, or MPEG-4. In this ex-
periment, we use MPEG-4 baseline video coder for the base layer coding. A simple
half-pixel motion estimation scheme using linear interpolation is implemented to ex-
tract the motion vectors between video pictures. The range of the motion vector is
set to ±31.5 pixels. The same motion vectors are applied to all MCs, which in turn
produce the predicted images.
The bit rate of the base layer is 128 kbits/s with TM5 rate control, and the encod-
ing frame rate is 10 Hz. The bit rate of the enhancement layers is not constrained.
Since the enhancement layers produce an embedded bitstream, the streaming server
can truncate it at any place to match the channel bandwidth. The truncating proce-
dure can be independent of a decoder. In our simulation experiments, the enhance-
ment layer bitstream is truncated at 64 kbits/s, 128 kbits/s, ···, until 384 kbits/s, with
an interval of 64 kbits/s. In order to reduce ﬂuctuation when switching between ref-
erences with different quality, the conditional replenishment scheme is used in the
two PFGS frameworks.
We deﬁne the least signiﬁcant bit plane as index “0” and the next least signiﬁcant
bit plane as index “1,” and so on. For the PFGS framework with multiple picture
buffers, even pictures are predicted from the even layers of the previous picture,
and the odd pictures are predicted from the odd layers of the previous picture. This
alternating structure repeats throughout the encoding of the whole video sequence.
For the simpliﬁed PFGS framework with only two picture buffers, we choose the
most efﬁcient enhancement layers as references through experiments. For the Akiyo
sequence, the second reference alternates between the second bit plane and the third
bit plane. For the Foreman sequence, the second reference alternates between the
third bit plane and the fourth bit plane. For the Coastguard sequence, the second
reference alternates between the fourth bit plane and the ﬁfth bit plane.
Table 4.1 gives some experimental results of coding these video sequences using
the methods described above. For the Akiyo and Coastguard sequences, the lumi-
nance PSNR in the PFGS framework with two picture buffers is almost the same
as with multiple picture buffers. For the Foreman sequence, the luminance PSNR in
PFGS framework with two picture buffers is about 0.15 dB less than with multiple
picture buffers. It is clearly shown that by only using one additional middle enhance-
ment layer as the reference, we can achieve almost the same coding efﬁciency as

84
4 Progressive Fine Granularity Scalable (PFGS) Coding
Table 4.1 PSNR versus bit-rate comparison between FGS, PFGSM, and PFGST for the Akiyo,
Coastguard, and Foreman sequences. PFGSM denotes the PFGS framework with multiple buffers
and PFGST denotes the PFGS framework with two buffers.
Bit Rate (kbit/s)
Akiyo
Coastguard
Foreman
FGS PFGSM PFGST
FGS PFGSM PFGST
FGS PFGSM PFGST
192
42.10
42.12
42.10
27.75
27.86
27.84
31.18
31.32
31.30
256
42.75
42.77
42.76
28.52
28.72
28.70
31.89
32.14
32.12
320
43.13
43.21
43.20
29.27
29.54
29.52
32.51
32.86
32.83
384
43.47
43.60
43.59
29.79
30.16
30.14
33.15
33.60
33.54
448
43.84
44.03
44.02
30.19
30.62
30.60
33.70
34.27
34.15
512
44.27
44.50
44.48
30.62
31.10
31.08
34.17
34.76
34.61
using multiple references. On the other hand, the conditional replenishment approach
used in the PFGS architecture with multiple references discards most parts of the
higher quality references, since HQPR coefﬁcients do not replace LQPR coefﬁcients
when the MSBs of the DCT coefﬁcients have been encoded in previous enhancement
layers. Meanwhile, from the results presented in Table 4.1, we can see that the two
PFGS frameworks can only achieve up to 0.5 dB PSNR gain on average. The main
reason is that the conditional replenishment scheme is not efﬁcient although it can
eliminate the ﬂuctuation.
Extensive simulations have been conducted to test the performance of the im-
proved PFGS framework. There are two picture buffers in the improved PFGS frame-
work. One is used to save the reconstructed base layer, and another is used to save
the reconstructed enhancement reference layer. The base layer is always predicted
from the ﬁrst buffer and all enhancement layers are predicted from the second buffer.
The results of the improved PFGS framework are compared with that of FGS. In
addition, the results of the nonscalable video-coding scheme are also presented to
show the cost of implementing the ﬁne granularity functionality. The nonscalable
video scheme is exactly the same as the base layer of the FGS scheme, where TM5
controls the output bit rate.
In this experiment, each picture is followed by 59 predicted pictures including P
pictures and B pictures. There are three B pictures between every two P pictures. The
range of the motion vector is set to ±31.5 pixels. The same motion vectors are ap-
plied to the two motion compensators, which in turn produce the predicted images.
The bit rate of the base layer is 256 kbits/s with TM5 rate control, and the encoding
frame rate is 30 Hz. The enhancement bitstreams are truncated at 128 kbits/s, 256
kbits/s, ···, until 2048 kbits/s, with an interval of 128 kbits/s. In the PFGS scheme, to
get a constant video quality, the enhancement layer bits cannot be equally allocated
to each picture. The reason is very simple. For instance, in Figure 4.7, picture 2 is
predicted from the third enhancement layer of picture 1. Since the reference is of
higher quality and is closer to the original image, a shorter bitstream will be gener-
ated due to the smaller prediction errors. However, picture 3 is predicted from the
second enhancement layer with lower quality. Larger prediction errors would result
in a longer bitstream in picture 3. Therefore, to get more or less the same quality
across each picture, bits spent for encoding odd frames should be more than that for

4.7 Simulation of Streaming PFGS Video over Wireless Channels
85
encoding even frames. On the other hand, if allocating a ﬁxed bit rate for all pictures,
for example, at a bit rate where the third enhancement layer in an even picture is
partially decoded, the previous odd picture may be only decoded up to the second
enhancement layer due to the fact that its bitstream is longer. In turn, the quality
of even pictures may be limited since its higher layer reference (third enhancement
layer) in odd pictures is not available. This will not only cause the ﬂuctuation of
PSNR in adjacent pictures, but also affect the overall coding efﬁciency of the video
sequence. Fortunately, the PFGS scheme provides an embedded and fully scalable
bitstream, so that the rate control does not have to occur at the encoding time. A
simple rate allocation or truncation module can be used in the streaming server to
obtain the optimized quality given a bit-rate constraint.
Some experimental results of the improved PFGS framework are given in Fig-
ure 4.11 for the Akiyo, Coastguard, and Foreman sequences, respectively. As we ex-
pected, generally, there is not too much performance gain at lower bit rates compared
with FGS. Sometimes there is even quality loss since the reconstruction reference in
the decoder may be different from that in the encoder for the lowest few enhance-
ment layers. For the Akiyo sequence, there is only some small quality loss at lower
bit rates. This is because in lower enhancement layers, we have to use LQPi instead
of HQPi to reconstruct the display image since the HQPi is not available yet at lower
bit rates. For the Coastguard and Foreman sequences, we cannot see the quality loss,
because a better prediction compensates most of the loss caused by using different
references. However, as the bit rate increases, we can see consistently signiﬁcant
coding efﬁciency gain. For the Coastguard sequence, the gain in Y component can
be up to 1 dB. For the Foreman sequence, the gain in Y component can be up to 1.3
dB. The reason why the Akiyo sequence does not show signiﬁcant gain is that the
base layer bit rate is already very high for this sequence. The quality is already very
good at that bit rate.
The experimental results also show that the coding efﬁciency gap between the
FGS scheme and the nonscalable video coding exceeds 3.0 dB. Even though the
PFGS scheme has signiﬁcantly improved the coding efﬁciency of FGS, the gap be-
tween the PFGS scheme and the nonscalable video coding is still large. How to
further improve the coding efﬁciency of the PFGS scheme is still an open question.
4.7 Simulation of Streaming PFGS Video over Wireless Channels
For most applications over the Internet, the fundamental technical problem is that
the network bandwidth varies widely in range from one user to another (user vari-
ations) and from time to time (temporal variations). The major advantage of the
PFGS scheme is its bandwidth adaptation capability that provides a good solu-
tion for bandwidth ﬂuctuation. With the rapid development of wireless communica-
tions, the wireless channel becomes an increasingly popular and convenient means
to access the Internet. As we know, wireless channels have different characteristics
than wired Internet channels. They are typically noisy and suffer from a number of

86
4 Progressive Fine Granularity Scalable (PFGS) Coding
41
42
43
44
45
46
47
48
49
256
384
512
640
768
896
1024
1152
1280
1408
1536
1664
1792
1920
2048
2176
2304
dB
kb/s
single
FGS
PFGS
(a)
27
28
29
30
31
32
33
34
35
36
37
256
384
512
640
768
896
1024
1152
1280
1408
1536
1664
1792
1920
2048
2176
2304
dB
kb/s
Single
FGS
PFGS
(b)
30
31
32
33
34
35
36
37
38
39
40
256
384
512
640
768
896
1024
1152
1280
1408
1536
1664
1792
1920
2048
2176
2304
kb/s
Single
FGS
PFGS
(c)
Figure 4.11 PSNR versus bit-rate comparison between FGS and PFGS for the Y component, (a)
Akiyo, (b) Coastguard, (c) Foreman.
channel degradations, such as random bit errors and burst errors due to fading and
multiple path reﬂections. When compressed video data is sent over these channels,
the effect of channel errors on the compressed video bitstream can be severe. As a
result, the video decoder that is decoding the corrupted video bitstream often loses

4.7 Simulation of Streaming PFGS Video over Wireless Channels
87
synchronization. Moreover, predictive coding techniques such as motion compensa-
tion used in various video compression standards make the situation even worse. The
decoders based on these techniques would quickly propagate the effects of channel
errors across the video sequence and rapidly degrade video quality. To deal with this
problem, some error resilience and concealment methods are introduced by Wang
and Zhu [54] and Talluri [55].
For applications involving transmitting video over error-prone channels, such as
wireless channels, the PFGS framework provides a more robust solution. PFGS pro-
vides an inherent error recovery feature that can gracefully recover from any en-
hancement layer errors. The PFGS framework also provides a layered bitstream
structure with different importance at different layers. In this layered bitstream struc-
ture, the most important information can be sent separately and with increased er-
ror protection compared to the less important enhancement information. There are
basically two bitstreams in the PFGS framework: the base layer bitstream and the
enhancement layer bitstream. The base layer bitstream is very sensitive to channel
errors. Any random errors or burst errors may cause the decoder to lose synchroniza-
tion, and the decoded errors will propagate to the start of the next GOP. However,
the enhancement layers can tolerate channel errors. When there are errors in the en-
hancement layer bitstream, a decoder can simply drop the rest of the enhancement
bitstream of the given picture and search for the next synchronization marker. There
should be neither obvious visual artifacts nor error propagation due to the error re-
covery feature of the PFGS scheme. Generally, since the bit rate of the base layer is
very low, channel coding with the same number of bits can provide better protection
to the base layer than to the overall bitstream. If the channel is a time-varying chan-
nel, as in many wireless communication situations, it is very feasible to adaptively
adjust the rate allocation between the source and channel coding operations seam-
lessly with the PFGS scheme. Therefore, the quality of base layer can be maintained
at a stable level.
In this section, we simulate the delivery of two kinds of video bitstreams over
wireless channels. The ﬁrst kind of bitstream is a MPEG-4 single-layer bitstream.
Some error-resilience tools are applied from the source coding viewpoint, such as
Resynchronization Marker, Data Partitioning, and header extension code (HEC). The
second kind of bitstream is the PFGS bitstream. The same error-resilience tools are
applied to the base layer, and the enhancement layers are encoded without any error
protection and concealment. In the PFGS coding scheme, both the overall source
bit rate and bit allocation between source and channel coding can be dynamically
adjusted depending on the channel feedback. However, it is very difﬁcult for the
single-layer case to adapt to bandwidth ﬂuctuation for bit re-allocation. Therefore,
the channel bandwidth and bit allocation between source and channel coding are
ﬁxed in our simulation.
We use a two-state Markov model to simulate the wireless channel as in Figure
4.12 [56]. This model can characterize the error sequences generated by data trans-
mission channels. In the good state, G errors occur with low probability α, while
in the bad state B they occur with high probability β. The errors occur in clusters
or bursts with relatively long error-free intervals (gaps) between them. The state

88
4 Progressive Fine Granularity Scalable (PFGS) Coding
G
B




Figure 4.12 Gilbert model for simulating a wireless channel.
transitions are shown in Figure 4.12 and summarized by its transition probability
matrix
M =

1−α
α
1−β
β

The model can be used to generate sequences of symbol error. In this case, it is
common to set α ≈0 and β ≈0.5. However, in situations where a ReedSolomon
(RS) code over GF(2m) is to be used, it is more appropriate for the model to generate
m-bit symbol errors. If β = 0, this model will simulate a random error, where α is
the bit error rate.
The QCIF News sequence is used in our simulation, and the encoding frame rate
is 10 Hz. Only the ﬁrst picture is encoded as an I picture, and other pictures are
encoded as P pictures. The total channel bandwidth is 64 kbits/s and 8 kbits/s of it is
used for channel coding for each bitstream. For the MPEG-4 single-layer bitstream,
the bit rate of source coding is 56 kbits/s. For the PFGS bitstream, 32 kbits/s is used
for base layer source coding. The enhancement layer bitstream can be truncated to
24 kbits/s to ﬁt in the channel bandwidth. It is obvious that the PFGS bitstream
can be decoded from 32 kbits/s to 56 kbits/s. The channel parameter α randomly
varies from 0.04 to 0.1, and parameter β also randomly varies from 0.3 to 0.6 during
simulation. The channel coding is implemented with a general RS code. An RS (64,
56, 8) code is applied to the MPEG-4 single-layer bitstream (a block of 64 bytes with
56 bytes of data and 8 bytes of protection). And an RS (40, 32, 8) code is applied to
the PFGS base layer bitstream (a block of 40 bytes with 32 bytes of data and 8 bytes
of protection). Both decoders have included some basic error-concealment tools such
as copying from motion-compensated areas in the previous picture.
The experimental results are shown in Figure 4.13. After going through the sim-
ulated wireless channel, it is clear that there are too many visual artifacts in the
decoded images from the received MPEG-4 single-layer bitstream which contains
errors now, while the results using the improved PFGS framework still provide very
good image quality. The reason is that the PFGS framework provides a robust en-
hancement layer bitstream. Since only the sensitive base layer bitstream needs the
protection of channel coding, the 8 kbits/s channel coding can provide stronger
protection for the base layer bitstream. For the MPEG-4 single-layer bitstream, the
whole bitstream is very sensitive to channel errors. The 8 kbits/s channel coding has
to provide protection for every bit in that bitstream. Therefore, the channel coding

4.7 Simulation of Streaming PFGS Video over Wireless Channels
89
Figure 4.13 Simulation results of delivering different video bitstreams over wireless channels. The
ﬁrst four images are decoded from the received MPEG-4 single-layer bitstream. The latter four
images are decoded from the received PFGS bitstream.
protection becomes relatively weak and more channel errors could not be corrected
under the same channel condition.

90
4 Progressive Fine Granularity Scalable (PFGS) Coding
4.8 Summary
In this chapter, a highly efﬁcient scalable video-coding framework is ﬁrst presented.
Compared with the FGS scheme in MPEG-4, the PFGS framework tries to use some
higher quality references to improve the coding efﬁciency, since higher quality refer-
ences make motion predictions more accurate. In order to keep the scalability prop-
erties, such as ﬁne granularity scalability, bandwidth adaptation, and error recovery,
the basic PFGS framework preserves a complete prediction path from the lowest
layer to the highest layer. However, multiple extra picture buffers are needed to save
the reconstructed reference layers in the basic PFGS framework, which increases the
memory cost and computational complexity. We simplify the basic PFGS framework
from multiple extra picture buffers to one extra picture buffer, while still keeping
almost the same coding efﬁciency. Moreover, an improved PFGS framework is pre-
sented to further improve coding efﬁciency by effectively eliminating the ﬂuctuation
at enhancement layers when switching references. While the original PFGS frame-
work provides about 0.5 dB coding efﬁciency gain over the FGS in MPEG-4, the
improved PFGS framework provides about 1 dB coding efﬁciency gain. The coding
efﬁciency gap between scalable video coding and the nonscalable video coding is
closing.
The advantages of the PFGS scheme, such as layered coding and error recovery,
are further demonstrated through a wireless transmission simulation. The simulation
results clearly show that under the same conditions in an error-prone channel, the
PFGS scheme can produce much better decoded images than the single-layer non-
scalable MPEG-4 scheme.
In the future, more studies need to be done on how to further improve the coding
efﬁciency and robustness of the PFGS framework, and on how to optimally transport
it through the Internet and wireless channels. The coding efﬁciency gap between
the nonscalable video coding and the PFGS video coding is still big although it is
closing. How can we design an optimized PFGS encoder with improved coding efﬁ-
ciency? How can we optimally allocate (truncate) bits among different frames under
an overall bit-rate constraint? Sometimes, the base layer bitstream may still be too
long for certain applications, so how shall we add spatial scalability in the PFGS
framework? How can real-time video communication issues be addressed? What are
the best network protocols for streaming the PFGS video over the Internet? These
are all open questions that need to be answered.
Some simple error-detection and resynchronization tools should be added to the
PFGS enhancement-layer bitstream to improve its robustness and efﬁciency. In the
present enhancement-layer bitstream, once an error is detected, the rest of the bit-
stream in the current picture is simply dropped. If there are some resynchronization
markers in the enhancement-layer bitstream, we can just discard the part of the bit-
stream between two resynchronization markers and continue decoding the rest of the
bitstream to minimize the error effects. On the other hand, how to allocate bit rate
between source and channel coding dynamically according to channel conditions is
another topic that needs further consideration.

Chapter 5
Motion Threading for 3D Wavelet Coding
5.1 Introduction
Wavelet transform [57] provides a multi-scale representation of image and video
signals in the space-frequency domain. Aside from energy compaction and decor-
relation properties that facilitate efﬁcient compression of natural images, a major
advantage of wavelet representation is its inherent scalability. It endows compressed
video and image streams with ﬂexibility and scalability in adapting to heterogeneous
and dynamic networks, diverse client devices, and the like. Furthermore, recent pro-
gresses in literature have shown that 3D wavelet-based video coding schemes are
able to compete in performance with the conventional hybrid standard approaches
(e.g., H.264/AVC [13]).
In 3D wavelet-based video coding, wavelet transforms are applied temporally
across frames, and horizontally and vertically within each frame, respectively. The
correlations among frames are exploited by a temporal wavelet transform operated
on original frames instead of motion compensation from previously reconstructed
frames. Here we will focus on the scenarios in which the temporal transform exists
prior to 2D spatial transform, namely, the T+2D case. Due to the object and/or cam-
era motion in a scene, the same point on a moving object can be located at different
pixel positions in consecutive frames. To take full advantage of temporal correla-
tion and the ability to achieve high coding efﬁciency, the temporal transform should
be performed along motion trajectories. Due to complications in combining wavelet
transform and motion alignment, the efﬁciency of a temporal transform can become
a bottleneck in high performance 3D wavelet-based video coding schemes.
Some global and local motion models are proposed for motion alignment in tem-
poral wavelet transform. Taubman and Zakhor [58] predistort the video sequence
before the temporal transform by translating pictures relative to one another, while
Wang et al. [59] use a mosaic technique to deform each video frame into a com-
mon coordinate system. Both schemes assume a global motion model, which may
be inadequate for video sequences with local motion. To overcome this limitation,
Ohm [60] proposes a block-matching technique that is similar to the one used in
91

92
5 Motion Threading for 3D Wavelet Coding
standard video coding schemes, while paying special attention to covered/uncovered
and connected/unconnected regions. But it fails to achieve perfect reconstruction
with motion alignment with respect to subpixel accuracy. Other related works also
adopt similar motion models but focus on different aspects, such as tri-zerotree [61],
rate allocation [62], and set partitioning in hierarchical trees (SPIHTs) [63].
Since 2001, several groups have looked into combining motion alignment with the
lifting-structure of wavelet transform. These techniques are generally known as mo-
tion compensated temporal ﬁltering (MCTF). One noteworthy work is by Pesquet-
Popescu and Bottreau [64], in which the authors implement the ﬁrst subpixel (1/2-
pel) motion alignment with perfect reconstruction in the motion-compensated lift-
ing framework. They are the ﬁrst to incorporate overlapped-block motion alignment
into the lifting-based temporal transform. However, only the Haar ﬁlters are used
by Pesquet-Popescu and Bottreau [64]. Luo et al. ﬁrst employ the biorthogonal 5/3
wavelet ﬁlters with 1/2-pel motion alignment [65]. Secker and Taubman use both
the Haar and 5/3 ﬁlters again with 1/2-pel motion alignment [66]. Several follow-up
works, for example, Chen and Woods [67] and Flierl and Girod [68], also demon-
strate the advantage of the 5/3 ﬁlters for temporal transform. There are a number of
other related publications with different focuses (e.g., using 1/4-pel motion align-
ment, 9/7 or longer ﬁlters, adaptive or low-complexity implementation, scalable mo-
tion coding, etc.) [69–73].
At that time, Moving Picture Experts Group (MPEG) was actively exploring and
promoting inter-frame wavelet coding techniques. The Motion Compensated Embed-
ded Zero Block Coding (MC-EZBC) coder [67, 69] proposed by Chen and Woods
has become well known because of its good performance. In MC-EZBC, each pair
of frames is motion estimated in a hierarchical fashion to achieve up to 1/8-pel ac-
curacy before going through a motion-aligned lifting-based Haar transform. Addi-
tional wavelet decompositions along the temporal direction are performed on the
low-pass frames by following the same procedure. Using sub-pel motion alignment,
MC-EZBC performs about the same as the H.264 standard for some sequences. In
the original MC-EZBC coder [69], the Haar ﬁlters do not fully exploit the long-term
correlation across video frames. In addition, motion vectors at each temporal decom-
position layer are estimated and coded independently without exploiting cross-layer
correlations among them. In an improved version of MC-EZBC [67], the 5/3 ﬁlters
are used in conjunction with improved motion vector coding.
5.2 Motion Threading
Xu et al. [74] propose a motion threading (MT) approach that employs longer
wavelet ﬁlters to exploit the long-term correlation across frames along the motion
trajectory. The baseline architecture of the original 3D MT wavelet coder is shown
in Figure 5.1, where each column represents a frame. Backward motion estimation
is performed on each pair of frames from Frame0 to Frame1 at the macroblock
level. Pixels along the same motion trajectory are linked to form a nonoverlapping

5.2 Motion Threading
93
Figure 5.1 The original MT implementation by Xu et al. [74].
thread according to the motion vectors of the macroblocks they belong to. The bi-
orthogonal 9-7 or 5-3 wavelet ﬁlters are used for transforming the threads before
each resulting frame goes through a 2D spatial wavelet transform with the 9-7 ﬁlters.
After the 3D wavelet transform, all wavelet coefﬁcients are coded into one embedded
bitstream.
Motion threading offers a ﬂexible yet efﬁcient way of incorporating motion align-
ment into the temporal-domain wavelet transform, with encouraging results reported
by Xu et al. [74]. However, MT still has limitations. On the one hand, motion threads
are not allowed to overlap so as to guarantee perfect reconstruction at the decoder
side. If multiple pixels in the current frame are mapped to the same pixel in the next
frame, that is, whenever a scenario of many-to-one mapping occurs, only one of them
in the current frame can be linked to the one in the next frame. Each of the remaining
pixels has to be marked as a terminating pixel to signify that its associated thread
ends at the current frame. On the other hand, for pixels in the current frame that are
not linked by those in the previous frame, that is, when there are non-referred pixels,
we have to start a new motion thread for each one. When the input sequence has com-
plex motion, the number of many-to-one mappings or non-referred pixels (or thread
boundaries) will be large. Due to the boundary effects in wavelet reconstruction [75],
these artiﬁcial thread boundaries will degrade the coding performance.
Motion threading is only performed at full-pel resolution by Xu et al. [74]. It
is well known that motion estimation at 1/2- or 1/4-pel resolution can signiﬁcantly
improve coding efﬁciency in standard video coding. We thus aim to extend MT to
sub-pel resolution here (as was done by Luo et al. [65]) while maintaining perfect
reconstruction for improved 3D wavelet video coding. We ﬁrst adopt a lifting-based
wavelet transform for the motion threads. We carefully manage cases with many-
to-one pixel mapping and non-referred pixels in MT in order to reduce the number

94
5 Motion Threading for 3D Wavelet Coding
of thread boundary pixels (hence the boundary effects). We propose techniques for
1/2- or 1/4-pel motion estimation and motion alignment under the perfect reconstruc-
tion constraint [76]. We also generate one set of motion vectors for each temporal
layer of wavelet coefﬁcients for temporal scalability. Since motion vectors in adja-
cent regions within a temporal layer and the same region in different layers have
strong correlations, we devise a correlated motion estimation (CME) scheme to fa-
cilitate motion vector coding [77]. Eight modes are designed to capture the variation
of local motion correlation at the macroblock level and a rate-distortion (R-D) opti-
mized algorithm is employed to select the best mode for each macroblock.
Compared to Secker and Taubman [78], CME by Luo et al. [77] shares the same
idea of exploiting the correlation among cross-layer motion vectors. However, the
two approaches are different: CME reduces the number of motion vectors by deﬁn-
ing eight modes, whereas the scheme by Luo et al. [77] aims to improve prediction
accuracy in motion vector coding. Thus, although our CME technique is designed
for MT, the idea equally applies to other 3D wavelet coding schemes, such as MC-
EZBC [67] and those by Secker and Taubman [78,79].
5.3 Advanced Motion Threading
The advanced MT technique assumes a lifting-based wavelet transform and it aims
to reduce the number of artiﬁcially terminating/emerging threads in the original im-
plementation of MT. The accuracy of motion alignment is increased to sub-pel level
while maintaining perfect reconstruction.
5.3.1 Lifting-Based Motion Threading
The lifting structure is an efﬁcient implementation of the wavelet transform with low
memory and computational complexity [80,81]. Every ﬁnite impulse response (FIR)
wavelet ﬁlter can be factored into a few lifting stages [80]. As an example, the lifting
steps of a one-level biorthogonal 5-3 wavelet transform are shown in Figure 5.2.
The input signal x0,x1,··· ,x6 starts from Figure 5.2a, and the wavelet coefﬁcients
L0,L1,L2 and H0,H1,H2 come out at Figure 5.2b. From Figure 5.2, we have the
lifting steps as:

Hi = x2i+1 +a×(x2i +x2i+2)
Li = x2i +b×(Hi−1 +Hi)
,
(5.1)
with a = −1/2, b = 1/4. This equivalently implements the convolution kernel of 5-3
wavelet transform (up to scaling).
In the lifting-based implementation, each lifting step only updates half the nodes,
and the original value of the updated nodes will not be needed in subsequent steps.
The elementary lifting step is circled in Figure 5.2, which only involves three nodes.
Thus the updated values can be saved in the same memory that holds the original

5.3 Advanced Motion Threading
95
Figure 5.2 Lifting-based wavelet implementation of the 5-3 biorthogonal wavelet transform. (a)
forward transform with lifting steps shown in circles and (b) the inverse transform.
values with in-place computation. For the wavelet transform, low-pass and high-pass
coefﬁcients are interleaved. Each lifting step can be straightforwardly inversed with
an inverse lifting unit. Thus the inverse transform structure is easy to obtain, as shown
in Figure 5.2b.
We implement the wavelet transform of motion threads using lifting. Video frames
go through the lifting process step by step. We ﬁrst update the odd frames to be-
come high-pass frames and then the even frames to become low-pass frames. Figure
5.3 shows the lifting-based 5-3 temporal wavelet structure, where each column is a
frame, and each block represents a pixel. Macroblock-based motion estimation and
alignment are always performed from an odd frame to the forward and backward
adjacent even frames. In each lifting step, either the pixels in odd frames or those in
even frames are lifted along the motion threads.
Therefore, different from Eq. (5.1), the lifting steps with taking motion threads
into account can be formulated by
(
Hi(x) = F2i+1(x)+a×(F2i (MT2i+1,2i(x))+F2i+2 (MT2i+1,2i+2(x)))
Li(x) = F2i(x)+b×

Hi−1

MT −1
2i−1,2i(x)

+Hi

MT −1
2i+1,2i(x)

,
(5.2)
where a = −1/2, b = 1/4. F2i+1 is the pixel of the input frame. Hi() and Li() are the
generated high-pass and low-pass wavelet coefﬁcients. x is the position of the pixel
or wavelet coefﬁcient.
The ﬁrst equation in Eq. (5.2) describes the prediction step. MT2i+1,2i() and
MT2i+1,2i+2() represent the motion alignment from the frame 2i + 1 to the frame
2i and from the frame 2i + 1 to the frame 2i + 2, respectively. In this step, motion
estimation is ﬁrst performed from an odd frame to the forward and backward adjacent
even frames. The estimated motion vector is used to form the function MT2i+1,2i()

96
5 Motion Threading for 3D Wavelet Coding
Figure 5.3 Lifting-based MT with bidirectional motion search.
and MT2i+1,2i+2() as follows:

MT2i+1,2i(x) = x+MV2i+1,2i(x)
MT2i+1,2i+2(x) = x+MV2i+1,2i+2(x) ,
(5.3)
where MV2i+1,2i(x) and MV2i+1,2i+2(x) are the motion vectors of the pixel x in the
frame 2i + 1. In the proposed scheme, MV2i+1,2i(x) and MV2i+1,2i+2(x) are allowed
to point out a noninteger pixel. F2i(MV2i+1,2i(x)) and F2i+2(MV2i+1,2i+2(x)) in the
fractional-pixel position are calculated with Sinc interpolation [82].
The second equation in Eq. (5.2) describes the update step. Obviously, the update
step needs the motion alignment MT −1
2i−1,2i() and MT −1
2i+1,2i() from an even frame to
the forward and backward adjacent odd frames. In most 3D wavelet coding schemes,
motion alignment of the update step is derived from the motion vectors of the pre-
diction stage so as to save bits for coding motion vectors. We use MT −1
2i+1,2i() as a
example to describe the motion alignment in the update step as follows:
MT −1
2i+1,2i (x+⌊MV2i+1,2i(x)⌋) = x+⌊MV2i+1,2i(x)⌋−MV2i+1,2i(x).
(5.4)
Since MV2i+1,2i(x) has the precision of fractional pixel, the operator ⌊⌋rounds
every element of a vector to the nearest integer. In the fractional-pixel position
Hi−1(MT −1
2i−1,2i(x)) and Hi(MT −1
2i+1,2i(x)) are calculated with Sinc interpolation.
Figure 5.4 assists in explaining Eq. (5.4). Black circles represent pixels at full-
pel resolution, gray ones at 1/2-pel resolution, and white ones at 1/4-pel resolu-
tion. Solid curves with solid arrows represent motion vectors generated from motion

5.3 Advanced Motion Threading
97
Figure 5.4 Motion alignment with 1/4-pel resolution.
estimation, and dashed curves with solid arrows represent motion vectors which are
direct inverses of the solid curves. It is easy to understand Eq. (5.4) if MV2i+1,i(x) is
an integer motion vector. For example, the motion vector of pixel x2 in F2n+1 points
to the pixel x1 in F2n+2 in the prediction step. As a result, the pixel x1 in F2n+2 is
aligned to the high-pass coefﬁcient of pixel x2 in F2n+1 in the update step. In other
words, MT −1
2i+1,2i() is the inverse function of MT2i+1,2i in this case. In another case, if
the motion vector points to a fractional-pixel position, the corresponding pixel in the
update step is ﬁrst found with x+⌊MV2i+1,2i(x)⌋and its motion vector is the inverse
version of MV2i+1,2i(x). The motion alignment of pixel x2 from F2n+1 to F2n in Figure
5.4 gives such an example.
Since the length of each wavelet ﬁlter is short, as long as needed frames have
entered the input buffer, related lifting steps described in Eq. (5.2) can be executed
without waiting for more frames to come in. Thus we implement a temporal ex-
tension of the line-based wavelet transform scheme used in JPEG 2000 [83] with
minimum buffer size during encoding and decoding (see Hu et al. [75]).
5.3.2 Many-to-One Mapping and Non-Referred Pixels
Due to object movement, camera movement, and scene changes, there are usually
different types of motion in real-world video. This would result in many-to-one

98
5 Motion Threading for 3D Wavelet Coding
mappings and non-referred pixels in MT. With lifting, we can carry on temporal
ﬁltering on pixels that are originally terminated by Xu et al. [74] due to many-to-
one mappings. As shown in Figure 5.3, during each basic lifting step, the originally
terminated pixel in Frame1 can be updated using both its left (forward) and right
(backward) connected pixels instead of being stopped on the right. When the cor-
responding pixel in Frame2 is to be lifted, although several pixels in Frame1 are
mapped to it, only the ﬁrst mapped one of them is used. For a non-referred pixel in
an even frame, which originally indicates the boundary of a motion thread, it will
be linked on both sides using the motion vectors from adjacent motion threads as
shown by the dashed curve with a hollow arrow in Figure 5.4. This way, each pixel
is guaranteed to be linked in both forward and backward directions. With these rules,
Eq. (5.4) can still be used to derive the motion alignment of many-to-one mapping
and non-referred pixels in the update step.
Although we can always link each pixel in the current frame with a pair of pixels
in the forward and the backward frames, sometimes this arrangement is counter-
productive. For example, when camera motion, occlusion, or scene change occurs,
many pixels in the current frame might have good matches in the previous or next
frame. Owing to the weak correlation among these pixels, linking them into motion
threads will actually hurt the coding efﬁciency and result in ghost artifacts after the
wavelet transform.
Since motion vectors are searched at the macroblock level in our implementa-
tion, mismatched pixels are also processed at the macroblock level. We mark a mac-
roblock as a terminating one when it contains many terminating pixels. The termi-
nating MB indicates the boundaries of the relative motion threads. The forward sum
of absolution difference (SAD), backward SAD, and the average SAD values of the
MB are used in mode decision. Accordingly, besides the normal Bid mode in which a
pixel is bi-directionally linked, two additional MB modes, Fwd and Bwd, are used to
represent the terminating status at the backward and forward directions, respectively.
For each MB, deciding the best mode among Bid, Fwd, and Bwd is an optimiza-
tion problem. As we have discussed, the reconstruction error at the motion thread
boundaries is larger than within the threads, hence the boundary effects. To avoid
these boundary effects due to artiﬁcial thread terminations, we favor Bid in mode de-
cision. Since terminating MBs only have unidirectional motion vectors, choosing the
Fwd or Bwd mode saves the cost of motion vector coding. Details on mode selection
are given later.
5.4 Multi-Layer Motion Threading
In our 3D wavelet video coder, temporal domain wavelet transform is applied before
the spatial-domain wavelet transform. A 4-level dyadic wavelet transform structure
along the temporal axis (or motion threads) is shown in Figure 5.5. In our original MT
implementation [74], motion vectors are estimated and transmitted only at the highest
layer. When performing the dyadic wavelet transform, we used different levels of

5.4 Multi-Layer Motion Threading
99
Figure 5.5 A 4-level dyadic wavelet transform structure along the temporal axis.
aggregation of the motion vectors generated at the highest layer. However, by simply
aggregating motion vectors generated from the highest layer, motion alignment in
lower frame rate layers may be inaccurate. In addition, in the temporal scalability
mode when the frame rate is less than the full rate, motion vectors at the highest
layer have to be fully transmitted to generate motion vectors at lower layers. This
incurs too much overhead in motion vector coding.
To rectify this shortcoming by Xu et al. [74], we propose an advanced MT to esti-
mate and transmit motion vectors at each temporal decomposition layer [76]. At each
layer, motion estimation is performed on the original frames (after appropriate down-
sampling) instead of the wavelet transformed low-pass frames. With multiple sets of
motion vectors, it is imperative to exploit correlation among them for efﬁcient cod-
ing. We ﬁrst examine the correlation of motion vectors between two adjacent frames.
The bidirectional motion estimations are always from odd frames to two neighboring
even frames as shown in Figure 5.6. When the video frames undergo a constant speed
motion, an assumption can be made that if a block at a position (x,y) in Frame2n+1
has a matching block at the position (x+dx,y+dy) in Frame2n; then using a uniform
temporal motion extrapolation, the same block must move to position (x−dx,y−dy)
in Frame2n+2. This assumption is also used in B-frames in traditional video coding.
When encoding the motion vectors, the forward and backward motion vectors have
the same absolute value but with opposite signs, thus only one directional motion
vector needs to be coded. This type of correlation is represented by two direct in-
verse (FwdDir/BwdDir) MB modes in motion estimation.

100
5 Motion Threading for 3D Wavelet Coding
Figure 5.6 Motion correlation between two adjacent frames.
There are several different kinds of orders to perform motion vector estimation
and coding of multiple layers. One is that both motion estimation and coding are
performed from a high layer to a low layer, which ﬁrst estimates the motion vectors
at the highest layer using traditional block motion estimation. Then the lower layer
motion vectors are predicted and encoded based on the higher layer. Similar to the
temporal scalability problem we mentioned regarding the original MT, although this
method ensures the motion accuracy, the higher layer motion vectors always needs
to be transmitted to generate the lower layer one even at low frame-rate case. Hence
the high-to-low method is not efﬁcient for low bit-rate applications.
Another is that both motion estimation and coding are performed from a low layer
to a high layer. Since lower layer motion vectors do not depend on higher layer ones,
the latter do not have to be transmitted in down-sampled frame-rate applications.
The disadvantage of this method lies in the precision of the motion vectors due to the
long frame interval at lower level. For a 4-layer temporal decomposition, the interval
is 8 frames. Furthermore, the prediction of the higher layer motion vectors may be
affected as well. For achieving the temporal scalability, we adopt this order to utilize
inter-layer correlation.
When the order of motion estimation is different from that of motion vector cod-
ing, another solution is that motion estimation is performed from a high layer to a low
layer to obtain more accurate motion vectors and motion vectors are encoded from
a low layer to a high layer to maintain temporal scalability. This solution should be
further investigated in the future to achieve a better trade-off between the precision
of motion vectors and the temporal scalability.
Figure 5.7 illustrates the inter-layer correlation of motion vectors between two
adjacent temporal layers. The inter-layer correlative modes assume that the sum of

5.5 Correlated Motion Estimation with R-D Optimization
101
(x+dx, y+dy)
Forward ME
(x,y)
(x+dxfwd, y+dyfwd)
(x+dxbwd, y+dybwd)
Frame2n+1
Frame2n+2
Frame2n+2
Forward
ME
Backward
ME
(x,y)
MV (dx,dy)
Frame2n
Frame2n
Lower layer
Higher layer
Figure 5.7 Correlation of motion vectors between two adjacent temporal layers.
the absolute value of the forward and the backward motion vectors are equal to the
motion vector of the same MB location in the previous layer. Therefore, one motion
vector can be saved within each forward-backward motion vector pair. An extreme
case of this assumption is that the forward and the backward motion vectors are
equal, while no motion vector in the higher layer needs to be transmitted. Accord-
ingly, three modes are designed to represent these correlation types.
5.5 Correlated Motion Estimation with R-D Optimization
In the previous sections, we have discussed the advanced motion-threading scheme
and motion vector correlations in multi-layer motion threading. In this section, eight
block modes are designed to represent different motion and correlation types. An R-
D optimized CME scheme is proposed to select from the designed modes based on
a compound cost function [77]. The cost function considers not only the matching
correctness but also the motion vector bits.

102
5 Motion Threading for 3D Wavelet Coding
Mode Decision
Bidirectional 
ME
Mode type 
MB
Thread 
assembler
for temporal filtering
Updated 
MV
Figure 5.8 Flowchart of the correlated motion estimation.
5.5.1 Deﬁnition of the Mode Types
Figure 5.8 shows the ﬂowchart of the proposed inter-layer motion estimation. Each
MB in the odd frames is ﬁrst bidirectionally motion searched using a motion search
technique similar to H.264. Here, only the 16×16 MB mode is used. A pair of in-
dependent forward and backward MVs is then generated. After that, according to
previous analysis, we deﬁne eight MB modes to represent the diverse local motion
properties. The mode selection is based on an R-D optimized criterion. The forward
or backward motion vectors are updated according to the chosen mode. The next
MB within the same frame may use the updated motion vectors to initiate the cen-
ter value in the motion search. Then motion threads are assembled according to the
chosen MB mode.
As shown in Figure 5.9, each column denotes one frame, and each block can be
considered a macroblock. The current frame F2n+1 is divided into MBs. A dark gray
MB represents an anchor block referred by the current MB. With camera zooming,
the mapping block of the dark gray MB may not exist, as shown by the light gray
blocks in the Fwd and Bwd mode in Figure 5.9. Each mode is deﬁned as follows:
DirL: An inter-layer correlated bidirectional mode. The forward and backward
motion vectors use half the absolute value of the inter-layer predicting MV, with ac-
cording inverse signs. No motion bits need to be transmitted in this mode. Some rel-
atively smooth motion types, such as background motion, may belong to this mode.

5.5 Correlated Motion Estimation with R-D Optimization
103
DirL mode
FT_BDL
(BT_FDL) mode 
FwdDir
(BwdDir) mode 
Bid mode
Fwd mode
Bwd mode
Lower
 layer
Higher
layer
Lower
 layer
Higher
layer
F2n
F2n+1
F2n+2
MV
-½ MV
½ MV
MV
MV
F2n
F2n+1
F2n+2
-MVfwd
MVfwd
F2n
F2n+1
F2n+2
MVfwd-MV
MVfwd
MV
MV
MV
F2n
F2n+1
F2n+2
MVFwd
F2n
F2n+1
F2n+2
MVBwd
F2n
F2n+1
F2n+2
MVFwd
MVBwd
Figure 5.9 Eight motion modes in CME.
FT BDL (BT FDL): Two inter-layer correlated bidirectional modes. The forward
(backward) MV is transmitted, while the relative backward (forward) MV is calcu-
lated as: MVbwd = MVfwd ± MVprevlayer (MVfwd = MVbwd ∓MVprevlayer).
The selection of + or - operator depends on the backward or forward direction of
the MVprevlayer. These two modes imply that the absolute values of the bidirec-
tional motion vectors are unequal. Only one motion vector needs to be sent. Between
FT BDL and BT FDL modes, the one with less motion cost is chosen.
FwdDir (BwdDir): Two intra-layer correlated bidirectional mode, where the for-
ward and backward MVs have the same absolute value and inverse signs: MVbwd =
−MV fwd or (MV fwd = −MVbwd). Only forward (backward) MV needs to be
transmitted.
Fwd (Bwd): Two single-directional modes. Only the forward (backward) MV is
transmitted, and the other direction of the thread is terminated. When some particular
motion such as occlusion occurs, completely matched pixels may only exist either

104
5 Motion Threading for 3D Wavelet Coding
in the forward or in the backward frame. Therefore, the motion threads should be
terminated in the mismatched direction.
Bid: The normal bidirectional macroblock mode. Both the forward and the back-
ward MVs are transmitted.
The single direction prediction modes, such as Fwd and Bwd modes, will inﬂu-
ence the prediction and update steps given in Eq. (5.2). When these two modes are
selected, some pixels only have the prediction from one side. In the same way, the
corresponding pixels derived from Eq. (5.4) may be updated from one side as well.
They are treated as boundary pixels. The ﬁltering parameters a and b in Eq. (5.2) are
multiplied with two similar to symmetric boundary extensions.
5.5.2 R-D Optimized Mode Decision
After a pair of independent forward and backward MVs are generated, from the
designed eight MB modes, the one with the smallest cost is selected based on an R-
D optimized criterion. A mode symbol is attached to identify the selected mode. The
mode selection criterion is deﬁned as: Cost = ηSAD+λBitsmotion, which consists of
two terms:
SAD is the sum of the absolute difference between the current macroblock and the
motion compensated matching blocks. The SAD value evaluates the matching cor-
rectness of the forward and backward motion vectors. For the bidirectionally linked
modes, such as Bid and Direct, the SAD term is the absolute difference between the
original MB and the average of the forward and backward matching blocks. For the
Fwd and Bwd modes, the SAD term takes only the one directional SAD value be-
cause the other direction is terminated. Pixels belonging to an MB with Fwd or Bwd
modes are the boundaries of the relative motion threads. As we have mentioned, the
wavelet synthesis error on the thread boundary is larger than the within-thread pix-
els. Therefore, we maintain a bias against the Fwd and Bwd modes as they introduce
artiﬁcial boundaries in MT. We test several sequences and empirically set η to two
for the Fwd and Bwd modes and one for the other six modes. peak signal-to-noise
ratio (PSNR) for η as one shows about 0.2 dB loss.
Bitsmotion represents the bits for coding the motion vector difference (MVD).
Since the mode distribution varies considerably for different sequence motion types
and different temporal decomposition layers, the bits for identifying the mode sym-
bol are Huffman coded accordingly after the motion estimated process. Thus the
symbol bit cost is not counted in the Bitsmotion term during the motion estimation.
In Bid mode, both the forward and the backward motion vectors are coded, hence
Bitsmotion consists of the bits for the bidirectional MVs; in the other modes, Bitsmotion
only involves single directional motion bits. λ is the weight parameter to control the
R-D contribution of the motion cost in the cost function. Generally, at the low bit
rate, the slope of the rate-distortion curve is sharper than at the high bit rate, thus
λ should be larger at a low bit rate to properly estimate the motion cost percentage.
However, because of the non-recursive wavelet coding structure, motion estimation
is performed prior to the transform and quantization. Therefore, λ can only be set

5.6 Experimental Results
105
to a ﬁxed value for all the bit rates in our coder (we note that a novel scheme based
on scalable motion vector coding was recently proposed by Secker and Taubman
[72] that effectively accommodates different λs in one coder). In our experiments,
we target at an average PSNR range of 30 dB to 40 dB with the mean of 34 dB.
According to PSNR = 10log10(2552/D), D = Q2/12 under the high rate assumption
for uniform scalar quantization, and the empirical formulae λ =
√
0.85×Q [84], we
can relate λ to the target PSNR. For PSNR = 34 dB, we have λ ≈16 and we use
λ = 16 in our experiments with six sequences.
For each MB, the mode which has the smallest total cost value is selected. Various
mode distributions are calculated according to different sequence and temporal lay-
ers. During the MB mode decision, the occurrence of each mode is counted for each
temporal decomposition layer. The probability distribution of each mode is used in
training the Huffman tables. Then the selected mode of each MB is Huffman en-
coded according to the trained tables. The Huffman tables for each temporal layer
are written into the header of the transmitted bitstream.
5.6 Experimental Results
5.6.1 Coding Performance Comparison
In this section, we compare the proposed advanced MT scheme with three bench-
mark coders: original MT [74], MC-EZBC [67], and H.264 [13] JM61e. The re-
sults from MC-EZBC are cited from the report in the Excel ﬁle shown by Chen and
Woods [85]. Advanced MT uses all eight modes, and Bidmode MT means only the
Bid mode selected. In advanced MT presented here and the original MT coder, the
input sequence is temporally ﬁltered as a whole with the extension of the line-based
implementation in JPEG 2000, without being explicitly divided into a group of pic-
tures (GOP). Since the interval of the four-layer decomposed low-pass frames is 16,
the GOP size can be regarded as 16. The temporally ﬁltered coefﬁcient frames are
further 3-layer spatially transformed with Spacl wavelet packet in JPEG 2000 [69],
where the three ﬁrst-layer spatial high-bands are further vertically and horizontally
analyzed, and then entropy coded as the operation by Luo et al. [65]. In advanced MT,
a quarter-pixel motion is used. Since the real motion range almost doubles when the
temporal interval is doubled, the motion search size is set as 32 for the highest tem-
poral layer, and 64 for the second layer, and 128 for the rest of the layers. In H.264
coder, the GOP is set as a whole sequence with only one I-frame, and two B-frames
are inserted into every two P-frames. Quarter-pixel prediction with search-size 32
and among 5 referenced frames is applied to H.264. CABAC and R-D optimization
are also turned on. Experiments are performed on six MPEG standard CIF sequences.
All the sequences are of 300 frames with the frame rate as 30 Hz.
We ﬁrst compare the proposed advanced MT of eight-mode selection with that
of only Bid mode. We intend to clarify the contribution of the mode selection in the

106
5 Motion Threading for 3D Wavelet Coding
Figure 5.10 Comparison between Bid mode only and eight-mode advanced MT.
advanced MT. For table tennis in Figure 5.10, eight-mode selection improves about
1.3 dB at 250 kbps and 0.4 dB at 2 Mbps; for silence, the improvement is 1.0 dB at
250 kbps and 0.1 dB at 2Mbps. The improvement mainly comes from the reduction
of motion cost, thus it is more obvious at lower bit rates and the two curves are
convergent at the high end. The improvement for the table sequence is signiﬁcant
even at the high end because a scene change occurring at the 131th frame makes the
bidirectional linkage improper.
Next, we compare the eight-mode advanced MT with three benchmark codecs.
Figure 5.11 shows that the original MT scheme achieves a close performance to
H.264 with Coastguard. However, with other sequences, the performance is not sat-
isfactory. This is due to the boundary effect of a large number of truncated motion
threads and the insufﬁcient integer motion accuracy. The proposed advanced MT
improves coding efﬁciency by up to 6.0 dB with Mobile and achieves a similar per-
formance to H.264. The coding gain for Mobile mainly comes from enhanced motion
accuracy because Mobile possesses many texture details. With Table, the advanced
MT outperforms the original MT by more than 3 dB, and approaches the perfor-
mance of H.264. With Silent, Foreman, and Stefan, where the irregular motion gen-
erates a lot of truncated motion threads in the original MT, advanced MT improves
about 2.0–4.0 dB with the avoidance of truncation based on the temporal lifting struc-
ture. With these three sequences, advanced MT is still inferior to H.264 by about 1.5
dB due to the shortage in motion thread alignment for complex motion. However,
we should note that the advanced MT can provide frame-rate and PSNR scalability,
which is not achievable for the single-layer H.264 bitstream. With the Coastguard
sequence, advanced MT improves by about 1.5 dB, and even outperforms H.264 by
0.6 dB. The improvement of the advanced MT is considerably signiﬁcant.
5.6.2 Macroblock Mode Distribution
In this experiment, we investigate the performance of mode-selective motion esti-
mation. Three standard sequences: Foreman, Coastguard, and Mobile are used to

5.6 Experimental Results
107
Figure 5.11 Comparisons among MC-EZBC, H.264, original MT, and advanced MT.
represent diverse motion types. All of them are in CIF format, 30 Hz, with 300 frames
in all.
First, we examine the distribution of the modes. The test sequences are tempo-
rally decomposed into four layers. For the lowest layer, DirL, FT BDL, and BT FDL
modes are disabled since the lower predictor is not available. Table 5.1 shows the dis-
tribution versus decomposition layer of the three test sequences.
In the lowest layer, Bid mode takes a large percentage. As the layer gets higher,
the distribution becomes biased toward the inter-layer and intra-layer modes. At
the highest layer, DirL is about 50% of the total modes. With the Foreman se-
quence, which contains more uneven and irregular face motions and camera mo-
tions, FT BDL and BT FDL modes are frequently used instead of DirL. In Mobile,

108
5 Motion Threading for 3D Wavelet Coding
Table 5.1 Mode distribution percentage versus temporal layer.
Foreman
Mode Type
Lowest Layer 4
Layer 3
Layer 2
Highest Layer 1
DirL
0
6.38
14.13
38.00
FT BDL
0
34.72
36.14
23.85
BT FDL
0
17.70
17.28
12.74
FwdDir
5.17
2.46
3.56
7.19
BwdDir
5.27
2.76
3.04
4.54
Fwd
3.44
2.06
1.21
0.41
Bwd
5.79
2.25
1.05
0.30
Bid
80.33
31.66
23.59
12.96
Mobile
Mode Type
Lowest Layer 4
Layer 3
Layer 2
Highest Layer 1
DirL
0
33.95
60.37
67.94
FT BDL
0
21.74
7.64
0.75
BT FDL
0
8.20
3.36
0.55
FwdDir
12.70
8.87
14.85
24.57
BwdDir
5.54
4.44
6.30
5.42
Fwd
1.99
0.65
0.12
0.03
Bwd
0.58
0.28
0.10
0.02
Bid
79.19
21.87
7.25
0.72
Coastguard
Mode Type
Lowest Layer 4
Layer 3
Layer 2
Highest Layer 1
DirL
0
9.59
24.78
57.63
FT BDL
0
24.63
23.17
14.64
BT FDL
0
11.19
9.93
6.08
FwdDir
14.49
13.13
13.14
9.73
BwdDir
15.08
11.55
8.50
4.44
Fwd
3.09
0.66
0.21
0.04
Bwd
1.39
0.97
0.25
0.03
Bid
65.95
28.28
20.00
7.40
the motion is more regular, thus the mode distribution concentrates around the DirL,
FwdDir, and BwdDir modes where the forward and backward MVs are equal. The
mode distribution of Coastguard is between Foreman and Mobile. The water wave,
with which motion estimation fails to catch an accurate motion, leads to the relatively
uniform mode distribution.
Next, we analyze the contribution of the layer correlated ME scheme. As shown
in Table 5.2, intra layer denotes the scheme with only intra-layer modes such as
FwdDir, BwdDir, Fwd, Bwd, and Bid modes; CME is the proposed scheme which
considers both the intra-layer modes and the inter layer one such as DirL, FT BDL,
and BT FDL. The total bits consist of the motion bits and the mode bits. With the
same PSNR performance, the motion bits reduction is shown in Table 5.2. The bits
reduction is 18.1% for Foreman, 19.2% for Coastguard, and 29.2% for Mobile. Since
Mobile contains more regular motion, for most MBs, DirL mode is selected, thus the

5.7 Summary
109
Table 5.2 Motion bits reduction.
Foreman
Mobile
Coastguard
Intra Layer (kbps)
MV bits
96.5
49.4
69.2
Mode bits
15.0
12.5
14.7
Total bits
111.5
61.9
83.9
CME (kbps)
MV bits
66.8
25.2
43.2
Mode bits
24.5
18.6
24.6
Total bits
91.3
43.8
67.8
Total Reduction
Bits (kbps)
20.2
18.1
16.1
Percentage
18.1%
29.2%
16.1%
bits reduction is relatively larger. At the low bit rate (e.g., 384 kb/s), the bit reduction
is equivalent to about 0.3 dB PSNR improvement.
5.7 Summary
An advanced motion threading technique is presented to improve the existing mo-
tion threading technique for wavelet video coding. With a lifting-based temporal
wavelet structure, the boundary effect problem caused by artiﬁcial thread truncation
in the original MT is solved well. Fractional-pixel motion can also be applied in the
new structure. Multi-layer motion threading is used to achieve efﬁcient frame-rate
scalability. To efﬁciently reduce the motion cost as well as achieve good frame-rate
scalability, we have analyzed both inter-layer and intra-layer correlations. A novel
R-D optimized correlated motion estimation scheme is proposed for the motion esti-
mation process. Experimental results show that the saving of the motion vector bits
is up to 29.2% with the Mobile sequence. The lifting-based motion threading has
many beneﬁts. In future works, adaptive length wavelet ﬁlters can be applied into the
lifting structure to adapt the various local properties along a video sequence.


Chapter 6
Barbell-Lifting Based 3D Wavelet Coding
6.1 Introduction
We started the study of 3D wavelet-based video coding from motion threading for
exploiting long-term correlations across frames along motion trajectories [74, 86].
The work by Xu et al. [74] also proposes an efﬁcient entropy coding, 3D embed-
ded subband coding with optimized truncation (ESCOT) for 3D wavelet coefﬁcients.
However, the motion threading technique still has limitations on handling many-to-
one mapping and non-referred pixels. To solve these problems, Luo et al. further
develop the lifting-based motion threading technique [65,76,87].
Subsequently, additional effort has been invested in this area. Xiong et al. propose
multiple modes with different block sizes (similar to that in H.264/AVC) for accurate
motion alignment and overlapped block motion alignment to suppress the blocking
boundaries in prediction frames [88]. Feng et al. propose an energy distributed update
technique to eliminate mismatches between the prediction and update steps in the
motion-aligned temporal lifting transform [89]. All these techniques are integrated
into a general lifting framework called Barbell lifting [90]. In addition, to maintain
high performance of 3D wavelet-based video coding in a broad range of bit rates,
Xiong et al. also investigate layered motion vector estimation and coding [91]. Ji
et al. propose an approach to incorporate a close-loop H.264/AVC into 3D wavelet
video coding to improve performance at low bit rates [92].
Moving Picture Experts Group (MPEG) plays an important role in actively ex-
ploring and promoting scalable video coding technologies from Fine Granularity
Scalable (FGS) coding to inter-frame wavelet coding. In 2003, a call for proposals
(CfP) was released to collect scalable video coding technologies and evaluate their
performances [93]. A total of 21 submissions were ﬁnally made, which met all the
deadlines listed in the CfP, including our Barbell-lifting coding scheme [94]. There
are two test scenarios in the CfP. For scenario 1, the working bandwidth range is large
and three levels of temporal/spatial scalabilities are required. Scenario 2 contains a
comparatively narrow working bandwidth range and only has two levels of tempo-
ral/spatial scalabilities. The Barbell-lifting coding scheme ranks ﬁrst in scenario 1
111

112
6 Barbell-Lifting Based 3D Wavelet Coding
and third in scenario 2 [95]. Finally, this scheme was adopted as common software
by the MPEG ad hoc group on further exploration of wavelet video coding [96], and
is, therefore, publicly available for all MPEG members.
6.2 Barbell-Lifting Coding Scheme
The overall block diagram of the Barbell-lifting coding scheme is depicted in Fig-
ure 6.1. First, to exploit the correlation among neighboring video frames, wavelet
transform is temporally performed on the original frames to decompose them into
low-pass frames and high-pass frames. To handle motion in video frames, motion
compensation is incorporated with the Barbell lifting, which is a high-dimensional
extension of the basic 1D lifting structure of wavelet transform [80]. Second, to fur-
ther exploit the spatial correlation within the resulting temporal subband frames, 2D
wavelet transform is applied to decompose each frame into some spatial subbands.
Third, coefﬁcients in the ultimate spatio-temporal subbands are processed bit plane
by bit plane to form an embedded compressed stream. The side information in Fig-
ure 6.1, including motion vectors, macroblock modes, and other auxiliary parameters
to control the decoding process, is also entropy coded. Finally, the streams of sub-
band coefﬁcients and side information are assembled to form the ﬁnal video stream
packets.
The stream generated by the above scheme is highly scalable. The bit plane cod-
ing technique for subband coefﬁcients provides ﬁne granularity scalability in recon-
struction quality. The hierarchical lifting structure of temporal transform is ready to
provide scalability in the frame rate. The multi-resolution property of wavelet repre-
sentation naturally provides scalability in resolution. When the bit rate, frame rate,
and spatial resolution of a target video are speciﬁed, a sub-stream for reconstructing
the video can be easily extracted by identifying relevant spatio-temporal subbands
and retaining a partial or complete stream of them while discarding the others.
Video
Frames
Temporal
Wavelet
Transform
Motion
Estimation
2D Spatial
Wavelet
Transform
Side Information
Coding 
Entropy
Coding
...
...
...
Stream
Packetizer
Compressed
Stream
 
Figure 6.1 Block diagram of the Barbell-lifting coding scheme.

6.2 Barbell-Lifting Coding Scheme
113
The following sections will discuss the core techniques employed in our proposed
coding scheme, such as Barbell lifting, layered motion coding, 3D entropy coding,
and base layer embedding. At the same time, we also cite several related techniques
used in other schemes so as to give the audience a fuller picture.
6.2.1 Barbell Lifting
In many previous 3D wavelet coding schemes, the concept of lifting-based 1D
wavelet transform is simply extended to the temporal direction as a transform along
motion trajectories. In this case, the temporal lifting is actually performed as if in 1D
signal space. This requests an invertible one-to-one pixel mapping between neighbor-
ing frames so as to guarantee that the prediction and update lifting steps operate on
the same pixels. However, the motion trajectories within real-world video sequences
are not always as regular as expected, and are sometimes even unavailable. For ex-
ample, pixels with fractional-pixel motion vector are mapped to “virtual pixels” on
reference, which cannot be directly updated. In the case of multiple pixels, mapping
to one pixel on reference, the related motion trajectories will merge. For covered and
uncovered regions, motion trajectories will disappear and appear. The direct adop-
tion of 1D lifting in temporal transform cannot naturally handle these situations. It
motivates us to develop a more general lifting scheme for 1D wavelet transform in
a high-dimensional signal space, where multiple predicting and updating signals are
supported explicitly through Barbell functions.
When the lifting scheme developed by Daubechies and Sweldens [80] is directly
used in temporal direction, the basic lifting step can be illustrated in Figure 6.2a. A
frame f1 is replaced by superimposing two neighboring frames on it with a scalar
(a)
(b)
Figure 6.2 Basic lifting step. (a) Conventional lifting. (b) Proposed Barbell lifting.

114
6 Barbell-Lifting Based 3D Wavelet Coding
factor β speciﬁed by the lifting representation of the temporal wavelet ﬁlter. No-
tice that only one pixel, of the signals f0 and f2 respectively, is involved in the lifting
step. In the proposed Barbell lifting as shown in Figure 6.2b, instead of using a single
pixel, we use a function of a set of nearby pixels as the input. The functions B0() and
B2() are referred to as Barbell functions. They can be any linear or nonlinear func-
tions that take any pixel values on the frame as variables. The Barbell function can
also vary from pixel to pixel. Therefore, the basic Barbell-lifting step is formulated
as
f1 = βB0 (f0)+ f1 +βB2 (f2).
(6.1)
According to the deﬁnition of the basic Barbell-lifting step, we give a general for-
mulation for M-level motion compensated temporal ﬁltering (MCTF), where the mth
MCTF (0 ≤m ≤M −1) consists of Λ(m) lifting steps. Assume that f (m,0) = f (m,0)
k
denotes input frames of the mth MCTF and f (m,λ) = f (m,λ)
k
denotes the result of the
λth lifting step 1 ≤λ ≤Λ(m) of the mth MCTF. k indicates the frame index.
For odd λ, the λth lifting step modiﬁes odd-indexed frames based on the even-
indexed frames, as formulated in Eq. (6.2). For even λ, the λth lifting step mod-
iﬁes even-indexed frames based on the odd-indexed frames, as formulated in Eq.
(6.3). Here β (m,λ)
0
and β (m,λ)
1
are ﬁlter coefﬁcients speciﬁed by the lifting represen-
tation of the mth level temporal wavelet ﬁlter. BO
2i→2k+1 (i = k,k + 1) and BE
2i+1→2k
(i = k −1,k) are the Barbell function operators to generate lifting signal in odd and
even steps, respectively. After all the lifting steps, we get the low-pass frames and
high-pass frames, deﬁned by L(m+1)
k
= f (m,Λ(m))
2k
and H(m+1)
k
= f (m,Λ(m))
2k+1
, respec-
tively. Theoretically, arbitrary discrete wavelet ﬁlter can be adopted in MCTF easily
based on Eq. (6.2) and Eq. (6.3). But the biorthogonal 5/3 ﬁlter is the one that has
already been veriﬁed as practical with good coding performance so far. It consists of
two lifting steps: Λ(m) = 2, β (m,1)
0
= β (m,1)
1
= −0.5, and β (m,2)
0
= β (m,2)
1
= 0.25. In
this case, BO
2i→2k+1 and BE
2i+1→2k are commonly called prediction and update steps,
respectively. In multilevel MCTF, the low-pass frames of a MCTF level are fed to
the next MCTF level by f (m+1)
k
= L(m+1)
k
. Finally, the M-level MCTF outputs M +1
temporal subbands: high-pass subbands H(1)
k ,H(2)
k ,··· ,H(M)
k
, and low-pass subband
L(M)
k
,
f (m,λ)
2k+1 = f (m,λ−1)
2k+1
+β (m,λ)
0
BO
2k→2k+1

f (m,λ−1)
2k

+β (m,λ)
1
BO
2k+2→2k+1

f (m,λ−1)
2k+2

.
(6.2)
f (m,λ)
2k
= f (m,λ−1)
2k
+β (m,λ)
0
BE
2k−1→2k

f (m,λ−1)
2k−1

+β (m,λ)
1
BE
2k+1→2k

f (m,λ−1)
2k+1

.
(6.3)
(1) MC Prediction—We discuss the Barbell function of MC prediction. Assume
there is a multiple-to-multiple mapping from frame f (m,0)
2k+1 to frame f (m,0)
2i
, based
on the motion between these frames and the correlation in related pixels. For any

6.2 Barbell-Lifting Coding Scheme
115
pixel a ∈f (m,0)
2k+1 , we deﬁne M(m)
2k+1→2i(a) ⊂f (m,0)
2i
as the set of pixels in f (m,0)
2i
that a is
mapped to. For each pair of pixels (a,b)(b ∈M(m)
2k+1→2i), weighting parameter w(a,b)
is introduced for prediction, to indicate the correlation strength between pixels a and
b. The operator fp = BO
2i→2k+1(f (m,λ−1)
2i
) based on Barbell lifting is deﬁned as
fp(xa) =
∑
b∈M(m)
2k+1→2i(a)
w(a,b)f (m,λ−1)
2i
(xb).
(6.4)
Here, xa and xb are coordinates of pixels a and b, in frames f (m,λ−1)
2k+1
and f (m,λ−1)
2i
,
respectively. The weighting parameters are subject to the constraint
∑
b∈M(m)
2k+1→2i(a)
w(a,b) = 1.
There are two types of parameters in the Barbell function: the mapping from f (m,0)
2k+1
to f (m,0)
2i
and the weighting parameters w(a,b). The mapping can be derived from
motion vectors estimated based on the block-based motion model. In general, mo-
tion vector is measured up to fractional pixels for accurate prediction, such as 1/2-pel
and 1/4-pel in H.264/AVC. Barbell lifting also supports fractional-pixel motion accu-
racy. In this case, each pixel in the current frame is mapped to multiple pixels in the
neighboring reference frame, while the weighting parameters w(a,b) are determined
by the interpolation ﬁlter (the formulation is given by Xiong et al. [90]).
To achieve a proper trade-off between the efﬁciency of motion prediction and the
coding cost of motion information, variable block-size partitioning is used for motion
representation in the Barbell function. All macroblock partitions in H.264/AVC, such
as 16 × 16, 16 × 8, 8 × 16, 8 × 8 and subpartitions in an 8 × 8 block, are supported
in Barbell lifting. In addition, ﬁve motion coding modes are deﬁned [88], including
the Bid, FwD, BwD, DirInv, and Skip modes, to further reduce the cost of coding the
mapping relationship. These modes jointly signal the connectivity in two directions
and the motion vector assigned with a macroblock. In the FwD and BwD modes, the
coding for motion on one side is skipped. Furthermore, the Skip and DirInv modes
exploit the spatial and temporal correlations in motion ﬁelds, respectively, and there-
fore save coding bits.
Although the smaller block size allowed in the variable block-size partition in
motion alignment can signiﬁcantly reduce the average energy of predicted errors, it
also increases the number of blocks used in motion compensation and causes more
blocking boundaries in the prediction frame. This leads to many high-amplitude
coefﬁcients in spatial high-pass subbands of residue frame after spatial subband
decomposition. The overlapped block motion compensation (OBMC) technique is
adopted in Barbell lifting [88] to smooth the transition at block boundaries. In this
case, the parameters w(a,b) are determined by both the interpolation ﬁlter and the
weighting window of OBMC (the formulation is given by Xiong et al. [90]). In
addition to the OBMC technique, it is also possible to support any other multi-
hypothesis techniques, for example, the multiple-reference MC prediction by the

116
6 Barbell-Lifting Based 3D Wavelet Coding
proposed Barbell-lifting model. These techniques can improve the compression efﬁ-
ciency of 3D wavelet video coding.
In the prediction step of our proposed coding scheme, we mainly borrow some
mature MC prediction techniques from conventional video coding schemes and in-
corporate them into the Barbell-lifting framework. There are also several other tech-
niques developed in other wavelet coding schemes for the MC prediction step. A
typical one is hierarchical variable size block matching (HVSBM) [62, 67], which
consists of constructing an initial full motion vector tree and pruning it subject to a
given bit rate.
(2) Motion Compensated Update—The update step in Barbell lifting is performed
according to the idea proposed by Feng et al. [89]. For a pair of pixels (a,b), where
a ∈f (m,0)
2i+1 , b ∈f (m,0)
2k
, and w(a,b) > 0, since the pixel a is predicted from pixel b with
a weighting parameter w(a,b) in the prediction step, we propose to use the prediction
error on the pixel a to update the pixel b, with the same weighting parameter. For any
pixel b ∈f (m,0)
2k
, we further deﬁne
M(m)
2k→2i+1(b) =
n
a ∈f (m,0)
2i+1 | b ∈M(m)
2i+1→2k(a)
o
as the set of pixels in f (m,0)
2i+1 that b is mapped from. Therefore, the operator fu =
BE
2i+1→2k(f (m,λ−1)
2i+1
) based on Barbell lifting is deﬁned as
fu(xb) =
∑
a∈M(m)
2k→2i+1(b)
w(a,b)f (m,λ−1)
2i+1
(xb).
(6.5)
Generally, the update step has an effect of temporal smoothing for regions with
accurate motion alignment, and thus can improve coding performance. But when
the motion in the video sequence is too complicated to be accurately represented by
the employed motion model, temporal high-pass frames may contain large predic-
tion residues. This makes the coding of temporal low-pass frames difﬁcult when the
prediction residue is superimposed on the even frames. It also results in ghost-like ar-
tifacts in the temporal low-pass frames, which is not desired for temporal scalability.
To solve this problem, a threshold function Tc() is applied to the updating signal
fu(xb) = Tc



∑
a∈M(m)
2k→2i+1(b)
w(a,b)f (m,λ−1)
2i+1
(xa)


,
(6.6)
where
Tc(x) =



−c,
if x < −c
x,
if ∥x∥≤c
c,
if x > −c
.
By empirically setting c = 2 −5, most visually noticeable artifacts can be removed
from low-pass frames but the advantage of the update step in coding performance is
still maintained.

6.2 Barbell-Lifting Coding Scheme
117
In addition to the simple but effective threshold approach used in the Barbell-
lifting coding scheme, Mehrseresht and Taubman [97] and Turaga and Van der
Schaar [98] also introduce some techniques to adjust the update signal and, hence,
reduce ghosting effects. Song et al. [99] propose regularizing the updating signal
based on HVS. In another more interesting work, Girod et al. [100] investigate the
update step as an optimal problem and derive a closed-form expression of the update
step for a given linear prediction step. Chen et al. [101] further reveal the relationship
between the energy distributed update [89] and the optimum update [100], and pro-
poses a set of new update coefﬁcients for improving coding efﬁciency and reducing
quality ﬂuctuations.
6.2.2 Layered Motion Coding
A stream generated by the Barbell-lifting coding scheme can be decoded at different
bit rates, frame rates, and resolutions. In terms of rate-distortion optimization (RDO),
a ﬁxed set of motion vectors is not an optimum solution for different reconstructions.
To achieve an optimum trade-off between motion and texture data, the Barbell-lifting
coding scheme requests at least a layered motion coding.
If we use D(RT;MV) to denote the distortion of reconstructed video, RT is the
bit rate allocated to texture data, MV is motion data, RMV is the bit rate for motion
data, the optimization problem is to minimize D(RT;MV) subject to the total bit
rate constraint R = RT +RMV ≤RMAX. Using the Lagrange approach, it leads to the
optimum problem as Eq. (6.7) and the solution is given in Eq. (6.8)
argminRT ,RMV J = D(RT;MV)+λ(RT +RMV).
(6.7)
∂D(RT;MV)
∂RT
= ∂D(RT;MV)
∂RMV
= −λ.
(6.8)
It means that the texture and motion stream should achieve the equal R-D slope on
its respective distortion-rate curve.
We propose a layered structure for the representation of motion data by Xiong
et al. [91], which consists of multiple layers, LMV
1
, LMV
2
, ···, LMV
n
. These motion
layers are selected with different lambda λ1 > λ2 > ··· > λn. During the motion
estimation and coding of the layer LMV
i
, its previous layer LMV
i−1 is used as prediction.
The motion data can be reﬁned in two ways: increasing the accuracy of the motion
vector or splitting a block into smaller subblocks. When two adjacent motion layers
have different resolutions, the motion vectors in the lower layer should be scaled
and the macroblock partition modes should be converted before they are used as
predictors.
The layered motion is applied to our coding scheme in the following way.
Encoder uses the ﬁnest motion to perform temporal transform on video frames.
This accurate motion provides efﬁcient energy compaction and guarantees optimal

118
6 Barbell-Lifting Based 3D Wavelet Coding
coding performance with an increase in bit rate. But the decoder may receive only
some of these motion layers for synthesis when the bit rate is low, giving a higher
priority to textures. When the motion used in the encoder and decoder is different,
perfect reconstruction cannot be obtained even if all texture coefﬁcients are decoded
without losses. From the observations by Xiong et al. [91], the distortion produced
by motion mismatch is nearly constant in terms of mean square error (MSE) in a
wide range of bit rates for texture. In other words, the distortion from motion mis-
match is highly independent of texture quantization. This facilitates the estimation
of the rate distortion property of the compressed texture and motion data. Let us use
Di(R) to denote the distortion function when the decoder employs LMV
i
. Since a bit
rate of RLMV
i
is allocated to motion and RT = R−RLMV
i
is allocated to texture, it can
be approximated by
Di(R) = D
 RT;LMV
n

+Dmismatch
 LMV
n
,LMV
i

.
(6.9)
The ﬁrst item on the right side of Eq. (6.9) is the quantized distortion with motion
LMV
n
, and the second item is the distortion of motion mismatch. Based on Eq. (6.9),
the R-D optimized motion layer selection can be performed during stream truncation
at either the frame level or the sequence level.
In additon to the proposed layered motion coding used in the Barbell-lifting cod-
ing scheme, Secker et al. also propose a scalable motion representation by applying
a subband transform and bit plane coding on motion ﬁeld [72, 102]. Furthermore,
the effect of motion parameter quantization on the reconstructed video distortion is
estimated based on the power spectrum of the reference frame.
6.2.3 Entropy Coding in Brief
After temporal transform and 2D spatial wavelet transform, spatio-temporal sub-
bands are available for entropy coding. Taking each spatio-temporal subband as a
three-dimensional coefﬁcient volume, we code it bit plane by bit plane using context-
based adaptive arithmetic coding technique. The entropy coding is similar to the
Embedded Block Coding with Optimized Truncation (EBCOT) algorithm [103,104]
employed in JPEG-2000. But unlike the coding in JPEG-2000, the coding of 3D
wavelet coefﬁcients involves exploiting correlations in all three dimensions. We pro-
pose a coding algorithm 3D-ESCOT [74] as an extension of EBCOT.
We divide each spatio-temporal subband into coding blocks and code each block
separately bit plane by bit plane. For each bit plane, three coding passes are ap-
plied. The signiﬁcance propagation pass codes the signiﬁcance information for the
coefﬁcients, which are still insigniﬁcant but have signiﬁcant neighbors. The magni-
tude reﬁnement pass codes the reﬁnement information for the coefﬁcients that have
already become signiﬁcant. And the normalization pass codes the signiﬁcance in-
formation for the remaining insigniﬁcant coefﬁcients. In 3D-ESCOT, the formation
of the contexts to code the signiﬁcance information and the magnitude reﬁnement

6.2 Barbell-Lifting Coding Scheme
119
information involves both temporal and spatial neighboring coefﬁcients. Further-
more, the correlation of temporal coefﬁcients is often stronger than that of spatial
coefﬁcients.
In addition to 3D-ESCOT being used in the Barbell-lifting coding scheme, 3D
Embedded Zero Block Coding (EZBC) [105], 3D set partitioning in hierarchical
trees (SPIHT) [63], and embedded morphological dilation coding (EMDC) [106] are
other approaches that can be used to code 3D wavelet coefﬁcients. 3D EZBC and
3D SPIHT use zero-tree algorithm to exploit the strong cross-subband dependency
in the quadtree of subband coefﬁcients. Furthermore, to code the signiﬁcance of the
quadtree nodes, the context-based arithmetic coding is used. The EMDC algorithm
[106] predicts the clusters of signiﬁcant coefﬁcients by some form of morphological
dilation.
6.2.4 Base Layer Embedding
As mentioned above, the MCTF decomposes original frames temporally in an open-
loop manner. It means that the decomposition at the encoder does not take the re-
construction of video frames at the decoder into account. Open-loop decomposition
makes the encoder simple because no reconstruction is needed. However, the weak-
ness of no reconstruction at the encoder is that for a certain bit rate, the encoder
does not know the mismatch between the encoder and the decoder so that the coding
performance cannot be well optimized. For instance, at the encoder, original frames
are used in motion compensation, motion estimation, and mode decision. While at
the decoder, reconstructed frames will actually be used. The motion data and coding
mode estimated on original frames may not be optimum for the motion compensation
on reconstructed frames. The mismatch is large when the quality of the reconstructed
frame is low, for example, at a low bit rate. That deteriorates the coding performance
especially at a low bit rate. To improve the coding performance at a low bit rate, we
incorporate a base layer into the Barbell-lifting coding scheme, which is coded using
a close-loop standard codec. Another advantage in doing so is that such a base layer
provides the compatibility to the standard coding scheme. The base layer can further
exploit redundancy within the temporal low-pass subband L(M)
k
without introducing
further coding delay.
Suppose that a base layer is embedded into the Barbell-lifting coding scheme af-
ter the mth level (0 ≤m ≤M −1) MCTF. The output low-pass video after MCTF
is f (m)
k
, with a frame rate of F/2m, where F is the frame rate of the original video.
D(f (m)
k
), a down-sampled version of f (m)
k
, is fed to a standard video codec, for ex-
ample, H.264/MPEG-AVC, where D() is a down-sampling operator. Let ENC() and
DEC() denote the base layer encoding and decoding processes. We can get the re-
constructed frames at the low resolution by
ˆf (m)
k
= DEC(ENC(D(f m
k ))).
(6.10)

120
6 Barbell-Lifting Based 3D Wavelet Coding
The reconstructed frames ˆf (m)
k
are both available at the encoder and the decoder,
which provide a low resolution and low frame-rate base layer at a certain bit rate.
The coding of base layer can be fully optimized as done in H.264/AVC. Any standard
compliant decoder can decode this base layer.
The up-sampled version of the reconstructed frames U( ˆf (m)
k
) is also used as a
prediction candidate in the prediction step of the remaining MCTF. For example, in
the m1th MCTF (m < m1 ≤M −1), for those macroblocks which use the base layer
as prediction, the prediction step is
f (m1,2)
2k+1 = f (m1,1)
2k+1 −U

ˆf (m)
(2k+1)∗2∧(m1−m)

.
(6.11)
And for those macroblocks, no update step is performed. To make the base layer
coding ﬁt for the spatial transform of the Barbell-lifting coding scheme, the down-
sampling operator extracts the low-pass subband after one or several levels of spatial
wavelet transform. The up-sampling operation is the corresponding wavelet synthesis
process.
6.3 Comparisons with SVC
The H.264/AVC scalable extension, or the SVC standard for short, is a new scalable
video coding standard developed jointly by the ITU Telecommunication Standard-
ization Sector (ITU-T) and the International Organization for Standardization (ISO).
The SVC standard was originally developed from the Heinrich Hertz Institute (HHI)
proposal [107], which extends the hybrid video coding approach of H.264/AVC to-
ward MCTF [108]. Since both schemes are developed from MCTF, they have many
commonalities, especially in the temporal decorrelation part. They also have many
differences. In this section, we discuss the major commonalities and differences of
the Barbell-lifting video coding to the SVC standard.
6.3.1 Coding Framework
The SVC standard uses a bottom-up layered structure to fulﬁll scalabilities, which is
similar to the scalability supported in previous MPEG and ITU-T standards. A base
layer is coded using H.264/AVC compliant encoder to provide a reconstruction at
low resolution, low frame-rate and/or low bit rate. An enhancement layer that may
be predicted from the base layer is coded to enhance the signal-to-noise ratio (SNR)
quality for SNR scalability or to provide a higher resolution for spatial scalability.
Multilevel scalability is supported by multiple enhancement layers. Once several
lower layers are given, the coding of the current layer can be optimized, which leads
to a one-by-one layer optimization. However, inefﬁcient inter-layer prediction cannot

6.3 Comparisons with SVC
121
totally remove redundancy in neighboring layers and will sacriﬁce performance in
higher layers.
Unlike the SVC standard, the Barbell-lifting scheme uses a top-down coding
structure. As mentioned above, video signal is decomposed temporally, horizon-
tally, and vertically to spatio-temporal subbands. In a given frame-rate and resolu-
tion required, a certain set of spatio-temporal subbands corresponding to that spatio-
temporal resolution are extracted and sent to the decoder. Decorrelation is achieved
by temporal and 2D spatial wavelet transforms. The advantage is to represent the
signal in a multi-resolution way so that scalability is inherently supported. The dis-
advantage of the top-down structure is that it may not favor coding performance at a
low resolution or bit rate, since all decomposition is done with an open-loop structure
and on the full resolution.
6.3.2 Temporal Decorrelation
Temporal decorrelation is one of the most important issues in video coding. Although
MCTF can be supported at the encoder, the close-loop hierarchical B-structure [109],
is the de facto decorrelation process in SVC. Suppose a M-level hierarchical B-
prediction is performed on video sequence fi, the mth level prediction can be ex-
pressed as
r(2k+1)∗2∧(M−m) =f(2k+1)∗2∧(M−m)
−1
2BO
2k∗2∧(M−m)→(2k+1)∗2∧(M−m)
  ˜f2k∗2∧(M−m)

−1
2BO
(2k+2)∗2∧(M−m)→(2k+1)∗2∧(M−m)
  ˜f(2k+2)∗2∧(M−m)

,
(6.12)
where ri is the residual frame after prediction and ˜fi is the reconstructed image of
fi, which is available at both the encoder and the decoder. BO
i→j is deﬁned as in
Section 6.2.1. Basically, it corresponds to the prediction stage at the (M −m + 1)th
MCTF. The difference is that the prediction in the close-loop hierarchical B-structure
is generated from the reconstructed images, while the MCTF is performed on the
original images. Since in the case of lossy coding the decoder cannot get the original
images, mismatch exists at the prediction stage of MCTF between the encoder and
the decoder. It may cause coding performance degradation, especially at a low bit
rate, where the mismatch between the original images and the reconstructed images
is large.
The other difference is that there is no update step in the hierarchical B-prediction
while there is in MCTF. The update step in MCTF, together with the prediction
step, constructs a low-pass ﬁlter that makes the output low-pass frames smooth so
that they can be better coded. It has been observed that the update step is effective
in improving coding performance in 3D wavelet video coding. However, in most

122
6 Barbell-Lifting Based 3D Wavelet Coding
cases, the update step in SVC does not show much difference in terms of coding
performance. Even for some cases in which the update step improves the coding
performance in SVC, the gain can be similarly achieved using preﬁltering. A possible
reason for the different coding performance improvement of the update step in SVC
and in 3D wavelet coding is that in SVC, integer approximation is applied to both
temporal decorrelation and spatial transform. This may absorb most signals at the
update step, which are often of low energy.
Despite these differences, the close-loop hierarchical B-prediction and MCTF
have similar prediction structures. Actually, if high-pass temporal frames are skipped,
the close-loop hierarchical B-prediction and MCTF are the same at the decoder be-
cause both prediction steps are performed on reconstructed images. The hierarchical
prediction structure can both exploit the short-term correlation and the long-term one.
Most frames are predicted bidirectionally in both structures. This accounts for why
both schemes have shown signiﬁcant coding performance gains over the H.264/AVC
codec in traditional I-B-P prediction structure. Schwarz, Marpe, and Wiegand [110]
give an analysis on hierarchical B-frames and MCTF.
6.3.3 Spatial Scalability
In the layered coding scheme of SVC, spatial scalability is supported by coding mul-
tiple resolution layers. The original full-resolution input video is down-sampled to
provide input at lower resolution layers. To exploit cross-layer redundancy, the recon-
structed images at a lower resolution can be used as prediction for some macroblocks
when the prediction within the current resolution is not effective. The advantage is
that the different resolution input can be ﬂexibly chosen, which enables arbitrary
down-sampling to generate low resolution video and non-dyadic spatial scalability.
However, in the coding of a higher-resolution video, many macroblocks do not use
the lower resolution as prediction, which means the corresponding bits at lower res-
olution do not contribute to the coding of higher resolutions. This affects coding
performance in spatial scalability scenarios.
In the Barbell-lifting coding scheme, any lower resolution video is always em-
bedded in higher-resolution video. The spatial low-pass subbands are used to re-
construct the low resolution video. Because of the critical sampling of the wavelet
transform, the number of transform coefﬁcients to be coded is the same as the num-
ber of pixels, even when multiple spatial scalability levels are supported. Bits of the
low-pass subband contribute both to the lower resolution layer and the higher resolu-
tion layer. However, the constraint is that the low-resolution video is corresponding
to the wavelet low-pass ﬁlter, which may not ﬁt for all applications. It is also difﬁcult
to support an arbitrary ratio of spatial scalability, since dyadic wavelet transform is
generally used.

6.4 Advances in 3D Wavelet Video Coding
123
6.3.4 Intra Prediction
As an extension of H.264/AVC, SVC still uses block-based DCT transform for spa-
tial decorrelation. Such a block-based transform enables that each macroblock can be
reconstructed instantly after the encoding to assist the coding of neighboring blocks.
Intra prediction in H.264/AVC and SVC are such examples of this technology. By
further introducing several directional intra-prediction modes, H.264 can efﬁciently
exploit the directional correlation within images. It can signiﬁcantly improve the cod-
ing performance of intra-frame and intra-macroblocks in P- or B-frames. However,
similar technology is relatively difﬁcult to use in 3D wavelet video coding since the
spatial transform of each macroblock is not independent.
6.4 Advances in 3D Wavelet Video Coding
There are still several challenges in scalable video coding. One is how to achieve
efﬁcient spatial scalability. Both the Barbell-lifting coding scheme and SVC suffer
from considerable performance degradation when spatial scalability is enabled. The
other challenge is how to further improve the performance of temporal decorrela-
tion. Two techniques, in-scale MCTF and subband adaptive MCTF, are developed,
although the integrated Barbell-lifting coding scheme is not available yet in MPEG.
6.4.1 In-Scale MCTF
As shown in Figure 6.3, the temporal transform is performed prior to the 2D spatial
transform in the encoder of the Barbell-lifting coding scheme. When a low-resolution
video is requested at the decoder, spatial high-pass subbands higher than the target
resolution are dropped. The other subbands are decoded by inverse wavelet transform
and inverse MCTF at low resolution to reconstruct the target video.
Two kinds of mismatches exist between the encoder and the decoder in Figure 6.3.
First, MCTF at the encoder and the decoder is performed at different resolutions,
which results in artifacts at those regions with complex motion [111]. Second, as
reported [97, 111, 112], all spatial subbands of video signals are coupled during the
MCTF process due to motion alignment. The dropped spatial high-pass subbands are
effectively referenced during the temporal transform at the encoder. But they become
unavailable at the decoder, and thus result in an extra reconstruction error. To remove
the ﬁrst kind of mismatch, several modiﬁed decoding schemes are investigated by
Xiong et al. [111]. To solve the second kind of mismatch, a new rate allocation
scheme is proposed by Xiong et al. [113,114], which allocates part of the bit budget
to spatial high-pass subbands based on their importance in reconstruction.

124
6 Barbell-Lifting Based 3D Wavelet Coding
MCTF
ME
Entropy
Decoder
Entropy
Coder
Inverse MCTF
Inverse DWT
DWT
Transmission Channel
 
 

   
       

 
 

   
       

Encoder
Decoder
 
Figure 6.3 Encoding and decoding process for spatial scalability in the Barbell-lifting coding
scheme.
(
)
β
×
+
IDWT
IDWT
MC
MC
MC
DWT
DWT
(a) ReferenceFrame (b) Redundant LiftingFrame (c) LiftingFrame
(d) Composed LiftingFrame
(e) TargetFrame
1
R
2
R
0
R
1
L
2
L
0
L
1
T
2
T
0
T
 
Figure 6.4 Lifting steps of in-scale MC temporal ﬁltering. (a) ReferenceFrame, (b) Redundant
LiftingFrame, (c) LiftingFrame, (d) Composed LiftingFrame, (e) TargetFrame.
The better way to solve the problem of spatial scalability is from the aspect of
coding structure. Thus, an elegant in-scale MCTF is ﬁrst proposed by Xiong et
al. [115,116], as shown in Figure 6.4. Assume there are three resolutions to be sup-
ported. In addition to the input resolution of frames denoted by subscript 2, two
low-resolution versions, denoted by subscript 1 and 0, respectively, are generated
by the wavelet ﬁlter that is also used in spatial transform. These frames constitute a
redundant pyramid representation of original frames. But the multi-resolution tem-
poral transform is designed as a whole so that coded coefﬁcients are not redundant.
The multi-resolution temporal transform is depicted in Figure 6.4. First, from Fig-
ure 6.4a to 6.4b, each independent motion compensation is performed on reference
frame Ri (i = 0,1,2) to generate corresponding prediction. Second, from Figure 6.4b
to 6.4c, a one-level wavelet transform, which has ﬁlters identical to those in spatial
transform, is performed on each prediction except for that of the lowest resolution.
The low-pass subband of each prediction is dropped in Figure 6.4c. Third, from Fig-
ure 6.4c to 6.4d, a new prediction is generated by inverse transforming the remaining
high-pass subbands and all information available in all low-resolution layers. Finally,
the signal Li (i = 0,1,2) is used in the temporal lifting transform. In this way, the sig-
nal at a lower resolution layer is always exactly the wavelet low-pass subband of the
signal at the next higher resolution layer. Thus, redundancy can be removed.

6.4 Advances in 3D Wavelet Video Coding
125
The proposed in-scale transform can also be described in the Barbell-lifting
framework. We deﬁne AL,n, AH,n, SL,n, and SH,n as analysis and synthesis op-
erators of n-level discrete wavelet transform (DWT). After n-level DWT, SI =
{(L,n),(H,n),(H,n −1),...,(H,1)} is the set of subband index, and fs(s ∈SI) de-
notes the subband s of any frame f. For example, fL,n is the coarsest scale of f and
fH,j(j = n,n −1,...,2,1) is ﬁner scales containing high-frequency details at high
resolutions. With these notations, the in-scale lifting steps are formulated as follows.
For odd λ, the λth lifting step modiﬁes odd-indexed frames based on the even-
indexed frames. The lifting step for low-pass subband at the coarsest scale is per-
formed according to Eq. (6.13), and the lifting steps for subbands at ﬁner scales are
performed according to Eq. (6.14),
f (m,λ)
2k+1,L,n =f (m,λ−1)
2k+1,L,n +β (m,λ)
0
BO
2k→2k+1

f (m,λ−1)
2k,L,n

+β (m,λ)
1
BO
2k+2→2k+1

f (m,λ−1)
2k+2,L,n

,
(6.13)
f (m,λ)
2k+1,H,j =f (m,λ−1)
2k+1,H,j +β (m,λ)
0
AH,1

BO
2k→2k+1

f (m,λ−1)
2k,L,j−1

+β (m,λ)
1
AH,1

BO
2k+2→2k+1

f (m,λ−1)
2k+2,L,j−1

.
(6.14)
For even λ, the λth lifting step modiﬁes even-indexed frames based on the odd-
indexed frames similarly, as formulated in Eq. (6.15) and Eq. (6.16),
f (m,λ)
2k,L,n =f (m,λ−1)
2k,L,n
+β (m,λ)
0
BE
2k−1→2k

f (m,λ−1)
2k−1,L,n

+β (m,λ)
1
BE
2k+1→2k

f (m,λ−1)
2k+1,L,n

,
(6.15)
f (m,λ)
2k,H,j =f (m,λ−1)
2k,H,j
+β (m,λ)
0
AH,1

BE
2k−1→2k

f (m,λ−1)
2k−1,L,j−1

+β (m,λ)
1
AH,1

BE
2k+1→2k

f (m,λ−1)
2k+1,L,j−1

.
(6.16)
The operator AH,1 can be viewed as a part of B(). But, to easily understand Eq.
(6.13) to Eq. (6.16), we keep it separate. The performance of the proposed technique
in wavelet video coding can be found by Xiong et al. [115, 116]. Furthermore, the
in-scale motion compensation technique is also applicable to current SVC because
of the pyramidal multi-resolution coding structure in SVC. We extend the in-scale
technique to support arbitrary up- and down-sampling ﬁlters and applied it to SVC
in both open-loop and close-loop form, with macroblock-level R-D optimized mode
selection [117–119]. Experimental results show that the proposed techniques can
signiﬁcantly improve spatial scalability performance of SVC, especially when the bit
rate ratio of lower resolution bitstream to higher resolution bitstream is considerable
[120].

126
6 Barbell-Lifting Based 3D Wavelet Coding
6.4.2 Subband Adaptive MCTF
In general, a frame to be coded highly correlates with previous frames. This correla-
tion can be exploited by generating a prediction through motion compensation. The
correlation strength is dependent on the distance between this frame and its refer-
ences and the accuracy of estimated motion vectors. Additionally, for a pair of the
current frame and its prediction, the correlation strength also varies in different spa-
tial frequency components. As shown in Figure 6.5, there is a frame to be coded
and its prediction. After packet wavelet transform, 16 subbands are generated. The
correlation coefﬁcients between them in different subbands are quite different. For
example, the correlation of low-pass subband is 0.98 but that of the highest subband
is only 0.08.
It motivates us to differentiate various spatial subbands during MCTF. The basic
idea comes from the optimum prediction problem of random signals. Let X and Y be
two correlated random signals. We predict Y from X by a linear model Z = Y −αX.
The optimum parameter to minimize the mean square prediction error can be solved
as
∂E[z2]
∂α
= ∂E[Y 2 −2αYX +α2X2]
∂α
.
(6.17)
In the case σX ≈σY, mX ≪σX, and mY ≪σY, Eq. (6.17) can be approximated to
α ≈ρX,Y. It means the best parameter to achieve the optimum prediction is mainly
determined by the correlation of the two signals. Therefore, we similarly adjust the
strength of temporal ﬁltering for various spatial subbands. It is formulated as fol-
lows. For odd λ, the λth lifting step modiﬁes odd-indexed frames based on the even-
indexed frames. The lifting step is performed for each subband s as in Eq. (6.18). For
even λ, the λth lifting step modiﬁes even-indexed frames based on the odd-indexed
frame to be coded
prediction
DWT
correlation
DWT
 
Figure 6.5 Correlation coefﬁcients between a frame and its prediction in different subbands.

6.5 Experimental Results
127
frames similarly, as in Eq. (6.19). The parameters α(m)(s)
2k→2k+1 and α(m)(s)
2k+2→2k+1 are de-
termined by characteristic of subband-wise temporal correlation in the mth MCTF
according to the method discussed by Xiong et al. [121]. Similarly, the operator As
can be viewed as a part of B(). But, to easily understand Eq. (6.18) and Eq. (6.19),
we keep it separate. The performance gain of this technique is reported by Xiong et
al. [121].
f (m,λ)
2k+1,s =f (m,λ−1)
2k+1,s
+β (m,λ)
0
α(m)(s)
2k→2k+1As

BO
2k→2k+1

f (m,λ−1)
2k

+β (m,λ)
1
α(m)(s)
2k+2→2k+1As

BO
2k+2→2k+1

f (m,λ−1)
2k+2

.
(6.18)
f (m,λ)
2k,s
=f (m,λ−1)
2k,s
+β (m,λ)
0
α(m)(s)
2k→2k−1As

BE
2k−1→2k

f (m,λ−1)
2k−1

+β (m,λ)
1
α(m)(s)
2k→2k+1As

BE
2k+1→2k

f (m,λ−1)
2k+1

.
(6.19)
6.5 Experimental Results
In this section, we conduct experiments to evaluate the coding performance of the
proposed Barbell-lifting coding scheme. In Section 6.5.1, we compare our scheme to
MC-EZBC [67,69], a well-recognized scheme in the literature of 3D wavelet video
coding. In Sections 6.5.2 and 6.5.3, we compare our scheme to SVC, the state-of-
the-art scalable coding standard, from the aspect of SNR scalability and combined
scalability, respectively.
6.5.1 Comparison with Motion Compensated Embedded Zero Block
Coding (MC-EZBC)
We compare the Barbell-lifting scheme with MC-EZBC in this section. Only SNR
scalability is considered here. Experiments are conducted with Bus, Foreman, Coast-
guard, Mobile, Stefan, and Silence CIF 30 Hz sequences, which represent different
kinds of video. For MC-EZBC, two versions are investigated. One is the old scheme
described in MPEG document m9034 [122], in which a comprehensive summary
on its SNR scalability performance is provided. The other one is the latest im-
proved MC-EZBC developed by Rensselaer Polytechnic Institute (RPI) [123]. Its
performance is obtained based on the executables and conﬁgurations provided by
Dr. Yongjun Wu and Professor John W. Woods.

128
6 Barbell-Lifting Based 3D Wavelet Coding
Bus (CIF 30Hz 150frames)
27
28
29
30
31
32
33
34
35
36
37
38
0
200
400
600
800
1000
1200
1400
1600
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
MC-EZBC (m9034)
MC-EZBC (Improved)
 
Foreman (CIF 30Hz 300frames)
31
32
33
34
35
36
37
38
39
40
41
42
0
200
400
600
800
1000
1200
1400
1600
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
MC-EZBC (m9034)
MC-EZBC (Improved)
Coastguard (CIF 30Hz 300frames)
28
29
30
31
32
33
34
35
36
37
38
39
0
200
400
600
800
1000
1200
1400
1600
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
MC-EZBC (m9034)
MC-EZBC (Improved)
Mobile (CIF 30Hz 300frames)
26
27
28
29
30
31
32
33
34
35
36
37
0
200
400
600
800
1000
1200
1400
1600
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
MC-EZBC (m9034)
MC-EZBC (Improved)
 
Stefan (CIF 30Hz 300frames)
27
28
29
30
31
32
33
34
35
36
37
38
0
200
400
600
800
1000
1200
1400
1600
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
MC-EZBC (m9034)
MC-EZBC (Improved)
Silence (CIF 30Hz 300frames)
33
34
35
36
37
38
39
40
41
42
43
0
100
200
300
400
500
600
700
800
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
MC-EZBC (m9034)
MC-EZBC (Improved)
Figure 6.6 Coding performance comparison between the MSRA Barbell codec and MC-EZBC.
To obtain the performance of our proposed scheme, the bit rate ranges by Chen
and Woods [122] are used. Four levels of MCTF are applied to all sequences. The re-
sulting temporal subbands are spatially decomposed by a Spacl transform [83]. Base
layer coding is not enabled in this experiment. The lambdas for motion estimation
at all MCTF levels are set to 16 in our scheme. Figure 6.6 shows the results of our
Barbell-lifting coding scheme, basic MC-EZBC (MPEG m9034), and the latest im-
proved MC-EZBC. From Figure 6.6, one can see that for each sequence, the Barbell-
lifting coding scheme and improved MC-EZBC scheme outperforms the basic MC-
EZBC (m9034) over a wide bit-rate range. The PSNR gain can be about 1.3–3.2
dB. The Barbell-lifting coding scheme still performs better than the improved MC-
EZBC scheme. Since many differences exist among the three schemes, it is difﬁcult
to analyze which part of the coding algorithm leads to performance difference and
how much. But the main reasons accounting for the gain may be the following.

6.5 Experimental Results
129
1. In basic MC-EZBC, Haar transform is used in MCTF, while in the Barbell-lifting
coding scheme, 5/3 ﬁlter is used. The prediction step of wavelet transform using
5/3 ﬁlter is bidirectional, which is more effective than the uni-directional predic-
tion in Haar transform. The difference between them is similar to the difference
between B-picture coding and P-picture coding in video coding standards. A
low-pass ﬁlter of 5/3 is better than that of Haar in terms of low-pass property,
which makes the low-pass subband generated using 5/3 ﬁlter easier to be coded.
2. In the Barbell-lifting coding scheme, adaptively choosing the Barbell functions
contributes to the performance gain. Variable block-size motion model similar
to the one in H.264/AVC is used, with ﬁve motion coding modes, which has
been shown to be effective. It makes a good trade-off between the prediction
efﬁciency and the overhead of motion information. Moreover, overlapped block
motion compensation and the update operator matching with the prediction step
further improve the efﬁciency of temporal decomposition of the video signals.
6.5.2 Comparison with SVC for SNR Scalability
We also compare our proposed scheme with the latest SVC under the testing con-
ditions deﬁned by JVT [124]. First, we test the SNR scalability performance of
both schemes. For SVC, its performance is quoted directly from JVT-T008 [125],
which presents the results of the latest stable JSVM reference software, that is,
JSVM6 [126]. To obtain the performance of our proposed scheme, the parameters
for MCTF levels are set to 5, 5, 4, 4, 4, and 2, for Mobile, Foreman, Bus, Harbour,
Crew, and Football, respectively. For each sequence, the spatio-temporal low-pass
subband is coded as a base layer by a H.264/AVC codec. Temporal subbands are fur-
ther spatially decomposed by a three-level Spacl DWT transform. The lambdas for
motion estimation at all MCTF levels are set to 16. Figure 6.7 shows the performance
of our scheme and the SVC (JVT-T008).
In general, the Barbell-lifting coding scheme works worse than SVC in the test-
ing conditions of SNR scalability. In spite of SVC being developed and optimized
extensively by JVT, there are still several possible reasons for the performance dif-
ferences.
1. The close-loop prediction structure of SVC can reduce or remove the mis-
match of the prediction between the encoder and the decoder. However, for the
open-loop prediction structure in the Barbell-lifting scheme, the mismatch de-
grades coding performance. Although the base layer is used in the Barbell-lifting
scheme, it only improves the efﬁciency of the spatio-temporal low-pass subband.
It does not contribute to the coding of other subbands or reduce mismatch. How-
ever, for 4CIF sequences which are coded at comparatively high bit rates to lead
to fewer mismatches, the performance gaps between these two schemes become
small. In some cases, for example, with the Harbour sequence, the Barbell-lifting
coding scheme can even outperform the SVC.

130
6 Barbell-Lifting Based 3D Wavelet Coding
Mobile (CIF 30Hz 300frames)
28
28.5
29
29.5
30
30.5
31
31.5
32
192
256
320
384
448
512
576
640
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
JSVM (JVT-T008)
Football (CIF 30Hz 260frames)
32.5
33
33.5
34
34.5
35
35.5
36
36.5
37
37.5
640
768
896
1024
1152
1280
1408
1536
1664
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
JSVM (JVT-T008)
Foreman (CIF 30Hz 300frames)
33.5
34
34.5
35
35.5
36
36.5
37
37.5
160
192
224
256
288
320
352
384
416
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
JSVM (JVT-T008)
Bus (CIF 30Hz 300frames)
30
30.5
31
31.5
32
32.5
33
33.5
34
320
384
448
512
576
640
704
768
832
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
JSVM (JVT-T008)
Harbour (4CIF 60Hz 600frames)
31.5
32
32.5
33
33.5
34
34.5
35
1280
1536
1792
2048
2304
2560
2816
3072
3328
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
JSVM (JVT-T008)
Crew (4CIF 60Hz 600frames)
34
34.5
35
35.5
36
36.5
37
37.5
1280
1536
1792
2048
2304
2560
2816
3072
3328
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec
JSVM (JVT-T008)
Figure 6.7 Coding performance comparison between the MSRA Barbell codec and SVC for SNR
scalability.
2. In SVC, each macroblock is reconstructed instantly after its encoding. It enables
effective intra prediction of the next macroblock. However, the absence of intra
prediction prevents our scheme from efﬁciently coding the macroblocks, where
motion compensation does not work well. That accounts for why the perfor-
mance gap is large for the Football and Foreman sequences, which are either
high motion sequences or have complex motion.
6.5.3 Comparison with SVC for Combined Scalability
The combined testing conditions deﬁned by Wien and Schwarz [124] support both
SNR scalability and spatial scalability. The stream to decode the low-resolution video

6.5 Experimental Results
131
is extracted from the high resolution one. In the Barbell-lifting coding scheme, a low-
resolution video corresponds to the video down-sampled from a high-resolution one
using a wavelet low-pass ﬁlter, speciﬁcally the 9/7 ﬁlter used in coding. Therefore,
we also use the video down-sampled with a 9/7 wavelet ﬁlter as the low-resolution
input in SVC, although it can support arbitrary down-sample ﬁltering. Using the
same down-sampling ﬁlter makes it possible to compare the low-resolution recon-
struction qualities of the two schemes in PSNR.
For SVC, the conﬁguration ﬁle in JVT-T008 [125] is reused except that the QP is
adjusted slightly to support the lowest bit rate speciﬁed by Wien and Schwarz [124].
Bitstream is adapted to the given bit rate using quality layers. In the Barbell-lifting
scheme, the number of MCTF levels are 5, 4, 3, and 2 for Mobile, Foreman, Bus,
and Football, respectively. The lambdas are set to comparatively large values to favor
performance at low resolution.
Figure 6.8 shows the comparison results for the Foreman, Football, Mobile, and
Bus sequences in CIF format. For the performance at low-resolution, the Barbell-
lifting coding scheme is still worse than SVC for the same reason addressed in Sec-
tion 6.5.2 and the mismatch of MCTF between the encoder and the decoder. But
for performance at high resolution, the Barbell-lifting coding scheme shows a cod-
ing performance close to SVC. For the Football sequence, the Barbell-lifting scheme
even outperforms SVC by up to 0.6 dB, in spite of the higher bit rate at the low
resolution. The reason may come from the different structures to support spatial
Foreman (300 frames)
31
32
33
34
35
36
37
48
96
144
192
240
288
336
384
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec (CIF30Hz)
JSVM6 (CIF30Hz)
MSRA Barbell Codec (QCIF15Hz)
JSVM6 (QCIF15Hz)
Football (260 frames)
29
30
31
32
33
34
35
36
37
192
384
576
768
960
1152
1344
1536
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec (CIF30Hz)
JSVM6 (CIF30Hz)
MSRA Barbell Codec (QCIF15Hz)
JSVM6 (QCIF15Hz)
Mobile (300 frames)
24
25
26
27
28
29
30
31
64
128
192
256
320
384
448
512
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec (CIF30Hz)
JSVM6 (CIF30Hz)
MSRA Barbell Codec (QCIF15Hz)
JSVM6 (QCIF15Hz)
Bus (150 frames)
26
27
28
29
30
31
32
33
96
192
288
384
480
576
672
768
Rate(kbps)
PSNR(dB)
MSRA Barbell Codec (CIF30Hz)
JSVM6 (CIF30Hz)
MSRA Barbell Codec (QCIF15Hz)
JSVM6 (QCIF15Hz)
Figure 6.8 Coding performance comparison between the MSRA Barbell codec and SVC for com-
bined scalability.

132
6 Barbell-Lifting Based 3D Wavelet Coding
scalability, as described in Section 6.3.3. The inter-layer redundancy in SVC may
lead to coding performance degradation at a high resolution, especially when the bit
rate of the low resolution is high. However, for the Barbell-lifting coding scheme, the
embedded structure of spatio-temporal decomposition aligns the coding of the low
resolution and the high resolution.
6.6 Summary
This chapter ﬁrst overviews the Barbell-lifting coding scheme. The commonalities
and differences between the Barbell-lifting coding scheme and SVC are then ex-
hibited for readers to better understand modern scalable video coding technolo-
gies. We then discuss two new techniques to further improve the performance of
the wavelet-based scalable coding schemes that are also suitable for SVC. From the
comparisons with SVC in terms of technique and performance, there is still a long
way to go in perfecting wavelet-based video coding. For example, intra blocks in
a high-pass frame are difﬁcult to code efﬁciently because of global spatial trans-
form; up-sampling and down-sampling ﬁlters are constrained to those used in spatial
transform, which may result in aliasing visual artifacts in low-resolution video. Fur-
thermore, an R-D optimized result is difﬁcult to achieve because of the open-loop
prediction structure used in 3D wavelet video coding.

Part III
Directional Transforms
This part studies directional transforms, another important component in image
and video compression. Natural images inherently contain rich directional structures,
beyond simple horizontal or vertical lines, as in the contemporary digital images we
perceived today. It is true that conventional rectilinear transforms we have been us-
ing to represent images are unable to fully capture such directional correlations. For
any transform to become superior in coding of natural images, not only does it need
to properly capture the directional correlations, it also needs to generate the trans-
form coefﬁcients in such a way that they can be optimally encoded by subsequent
entropy coding. It is based on such challenging principle that we propose an inno-
vative mechanism to design directional transforms through incorporating directional
operators in the lifting structure.
Chapter 7 presents a novel 2D wavelet transform scheme of adaptive directional
lifting (ADL) in image coding. Instead of alternately applying horizontal and ver-
tical lifting, as in present practice, ADL performs lifting-based prediction in local
windows in the direction of high pixel correlation. Hence, it adapts far better to the
image orientation features in local windows. The ADL transform is achieved by ex-
isting 1D wavelets and is seamlessly integrated into the global wavelet transform.
The predicting and updating signals of ADL can be derived even at the fractional
pixel precision level to achieve high directional resolution, while still maintaining
perfect reconstruction. To enhance the ADL performance, a rate-distortion optimized
directional segmentation scheme is also proposed to form and code a hierarchical im-
age partition adapting to local features. Experimental results show that the proposed
ADL-based image coding technique outperforms JPEG 2000 in both PSNR and vi-
sual quality, with the improvement up to 2.0 dB on images with rich orientation
features.
In Chapter 8, we introduce directional primary operators to the lifting-based DCT
and thereby derive a new directional DCT-like transform, whose transform matrix
is dependent on the directional angle and interpolation used there. Furthermore, the
proposed transform is compared with the straightforward one, ﬁrst rotated and then
transformed. A JPEG-wise image coding scheme is also proposed to evaluate the
performance of the proposed directional DCT-like transform. The ﬁrst 1D transform
is performed according to image orientation features, and the second 1D transform
still in the horizontal or vertical direction. At the same time, an approach is proposed
to optimally select transform direction of each block because selected directions of
neighboring blocks will inﬂuence each other. The experimental results show that the

performance of the proposed directional DCT-like transform can dramatically out-
perform the conventional DCT up to 2 dB even without modifying entropy coding.
While directional adaptation is introduced into traditional transforms, different
orders of two 1D transforms will result in different results of one 2D transform.
Based upon an anisotropic image model, Chapter 9 analyzes the effect of transform
orders in terms of theoretical coding gain. Our results reveal that the transform orders
have little effect on the coding gain with full decomposition, good directional modes,
and good interpolation. However, in practical compression schemes, since high-pass
bands are not decomposed fully because of the consideration on complexity, differ-
ent transform orders have different coding performances, which can be utilized by
an adaptive transform order. Motivated by our analyzed results, a directional ﬁlter-
ing transform (dFT) (dFT, in order to distinguish from the common usage on DFT) is
proposed to better exploit correlations among pixels in H.264 intra-frame coding. It
provides an evenly distributed set of prediction modes with an adaptive transform
order. Both inter-block and intra-block correlations are exploited in this scheme.
Experimental results in H.264 intra-frame coding demonstrate its superiority both
objectively and subjectively.

Chapter 7
Directional Wavelet Transform
7.1 Introduction
The past decade has seen increased sophistication and maturity of wavelet-based
image compression technologies. Within the family of mathematical transforms for
image coding, discrete wavelet transform has unseated discrete cosine transform
(DCT) [127–129] as the transform of choice. The wavelet-based JPEG 2000 inter-
national standard for still image compression [130] not only obtains superior com-
pression performance over the DCT-based old JPEG standard [129], but also offers
scalability advantages in reconstruction quality and spatial resolution that are desir-
able features for many consumer and network applications.
However, the prevailing practice of 2D wavelet transform has a legacy from tra-
ditional 2D DCT transform in that it is implemented by separable 1D ﬁltering in
horizontal and vertical directions. This separable 2D wavelet transform is referred to
as rectilinear 2D wavelet transform to distinguish it from another separable but adap-
tive 2D wavelet transform to be proposed, called adaptive directional lifting (ADL).
A serious drawback of rectilinear wavelet transforms is that they are ill suited to
approximate image features with arbitrary orientation that is neither vertical nor
horizontal. In these cases, rectilinear wavelets transform results in large-magnitude
high-frequency coefﬁcients. At low bit rates, the quantization noise from these co-
efﬁcients is clearly visible, in particular causing annoying Gibbs artifacts at image
edges with arbitrary directions. This problem has been identiﬁed by numerous re-
searchers [127,128,131,132].
How to fully exploit the directional correlation in either the image or frequency
domains has been a research topic for many years. Research on DCT-based image
coding has converged at directional prediction. Feig et al. incorporate spatial predic-
tion into a JPEG-like code in a manner similar to the fractal-based image compres-
sion [127]. Even though this method does not offer a better rate-distortion perfor-
mance than pure DCT-based coding, it has far fewer block artifacts and markedly
better visual quality at very low bit rates. Kondo and Oishi perform directional pre-
diction on DCT blocks. Their prediction is based on one of four coded neighboring
135

136
7 Directional Wavelet Transform
DCT blocks [128]. The new video coding standard H.264 successfully applies the
block-based spatial prediction technique to intra-frame coding. Signiﬁcant coding
gains are made over the version without spatial prediction [13].
Direction-aware wavelet/subband-based image coding methods fall into two ma-
jor categories in their ways of using and coding directional information.
1. Directional Filter and Transform — Ikonomopoulos and Kunt ﬁrst proposed a
2D ﬁlter bank that produces one low-pass component image and N directional
component images containing high-frequency components in a given direction
[131]. Li and He incorporate subband decomposition into the Ikonomopoulos
scheme [133], where each rectangular subband contains a given direction. Bam-
berger and Smith propose a ﬁlter bank with critically sampled and wedge-shaped
regions to describe directional information [134], which receives more atten-
tion for the virtues of maximal decimation and perfect reconstruction. Some
new directional ﬁlter banks based on Bamberger’s method are recently reported
by Nguyen and Oraintara [135] and Lu and Do [136]. In addition, many new
wavelet transforms have been proposed to preserve ﬁne directional information
in the wavelet domain, such as ridgelet [137, 138], curvelet [139], directional
wavelet transform [140], contourlet [141], complex wavelet [142, 143], brush-
let [144], and so on. These directional ﬁlters and transforms provide good pre-
sentations of directional data in the frequency domain. They are extensively ap-
plied in feature extraction, image enhancement, denoising, classiﬁcation, and
even retrieval. However, they are not suited for compression for lack of efﬁcient
entropy coding to exploit directional information in each wavelet region.
2. Directional Prediction — The key issue on applying directional prediction to
wavelet decomposition is the conﬂict of global transform and local features. Nat-
ural images usually contain rich orientation features. Partitioning an image into
many small regions according to correlation direction may cause severe bound-
ary effects and hurt coding efﬁciency. Taubman et al. propose a technique to
resample an image before conventional subband decomposition [132]. The re-
sampling process rotates image edges into horizontal or vertical directions so
that following subband decomposition can gain accurate predictions from neigh-
boring horizontal or vertical pixels. Wang et al. use a similar idea but further
propose an overlapped extension to prevent coding artifacts around the bound-
aries of different directional regions [145]. A similar idea on wavelet packets
is also reported by Zhang and Wu [146] and Carre et al. [147]. An additional
method for directional prediction is to separate images into two or more parts, in
which pixels of one part can be directionally predicted from other parts during
wavelet decomposition [148, 149]. Both of them are not appropriate to handle
varying orientations of image features. Furthermore, the 2D transform used in
this category usually is unable to be separated as two 1D transforms.
This chapter presents a new technique of seamlessly integrating directional pre-
diction in arbitrary orientation into the familiar framework of lifting-based 2D
wavelet transform. The lifting structure developed by W. Sweldens is an efﬁcient

7.1 Introduction
137
and popular implementation of wavelet transform, in which each ﬁnite impulse re-
sponse (FIR) wavelet ﬁlter can be factored into several lifting stages [150]. A local
spatial prediction can be readily incorporated into each lifting stage because the lift-
ing stage only involves a few neighboring pixels in the calculation of predicting and
updating signals.
Works in this line of thinking have been reported recently. Taubman proposes
an orientation adaptive lifting transform for image compression [151]. Boulgouris
and Strintzis propose interpolative pyramids for lossless and progressive image cod-
ing [152] and an adaptive lifting scheme for lossless image coding [153]. Aiming
to minimize the predicted error variance, this method derives four directional ﬁlters
from the quincunx sampling scheme and selects one of them with a median opera-
tion. Claypoole et al. investigate the order of prediction and update in adaptive lifting
transform [154]. Li et al. propose a variance-normalized autocorrelation function of
the difference image to reconstruct a linear predictor [155]. Gerek and Cetin pro-
pose a 2D orientation adaptive prediction ﬁlter in lifting structure [156]. In all these
schemes, feature directions are estimated from causal data of lower spatial resolu-
tion with limited accuracy. Moreover, most of them have only integer precision in
directional lifting transform. These two drawbacks reduce coding efﬁciency.
In this chapter we focus on the ADL wavelet transform. The preliminary results
of ADL were ﬁrst reported by Ding et al. [157], in which simple direction estimation
is used on variable-size blocks similar to those in H.264. Chang et al. subsequently
proposed to use quincunx sampling in directional lifting transform [158]. ADL is a
general framework that allows the use of any 1D wavelet ﬁlter, such as the popular
Haar, 5/3 and 9/7 wavelets, to perform 2D decomposition on images. The proposed
ADL-based image transform is not a true 2D wavelet transform and can be imple-
mented by two 1D transforms, but it has the following advantages not shared by
rectilinear 2D wavelet transforms.
• In each lifting stage, the predicting or updating operations are carried out in the
direction of image edges and textures in a local window, and are not necessarily
horizontal or vertical. This adaptation can signiﬁcantly reduce the signal energy
of high-pass subbands.
• High angular resolution in prediction is achieved by the use of fractional pixels
in prediction and update operations. The fractional pixels can be calculated by
any existing interpolation method.
• In order to guarantee perfect reconstruction, the predicted and updated pixels are
always in integer pixel positions.
• When 2D transform is separated as two 1D transforms, the two 1D transforms
are not necessarily performed in two perpendicular directions. However, the split
of the low and high subbands is in the horizontal and vertical directions, still
generating rectangular subbands for operational convenience.
Due to nonstationary orientation statistics of typical natural images, the proposed
ADL image transform works best if coupled with an adaptive image segmentation
that classiﬁes the input image into regions of approximately uniform edge/texture
orientations. In each of these homogeneous regions, ADL maximizes the compaction

138
7 Directional Wavelet Transform
of signal energy into the low subband by adjusting the prediction and interpolation
direction. To this end, we propose a quadtree-based segmentation technique to con-
struct an adaptive image segmentation in a rate-distortion optimal sense. The rate-
distortion optimization is done by the well-known Breiman, Friedman, Olshen, and
Stone (BFOS) optimal tree pruning algorithm [159].
Other than the replacement of conventional rectilinear wavelet transform by ADL
and the adaptive segmentation component, the other components of the proposed
image-coding system resemble their counterparts of JPEG 2000. The coefﬁcient
quantization is done by embedded bit plane coding, which preserves the scalabil-
ity of our code stream. The entropy coding of coefﬁcients is done by the EBCOT
technique [103].
7.2 2D Wavelet Transform via Adaptive Directional Lifting
This section presents a new adaptive directional lifting (ADL)-based 2D wavelet
transform. The ADL-based transform can overcome the difﬁculty of rectilinear 2D
wavelet transforms in approximating image signals of edges and textures in arbitrary
directions. Since the prediction of ADL uses the fractional pixels, the interpolation
methods are also described.
7.2.1 ADL Structure
The fundamental difference between the conventional lifting and the proposed ADL
lies in the prediction. Instead of always making the predictions in the horizontal or
vertical direction, the ADL analyzes the local spatial correlations in all directions,
and then chooses a direction of prediction in which the prediction error is minimal.
Consider a 2D signal x(m,n)m,n∈Z. Without loss of generality, we assume that this
signal is ﬁrst decomposed into high and low subbands by a 1D wavelet transform in
the vertical direction and then in the horizontal direction. With the technique given
by Kondi et al. [25], each 1D wavelet transform can be factored into one or multiple
lifting stages. A typical lifting stage consists of three steps: split, prediction, and
update.
First, all samples are split into two parts: the even polyphase samples xe and the
odd polyphase samples xo

xe (m,n) = x(m,2n)
xo (m,n) = x(m,2n+1) .
(7.1)
In the prediction step, the odd polyphase samples located at integer positions are
predicted from the neighboring even polyphase samples. The resulting prediction
residuals, or high subband coefﬁcients, are
h(m,n) = xo (m,n)−pe (m,n).
(7.2)

7.2 2D Wavelet Transform via Adaptive Directional Lifting
139
-45º
45º
x(m-1,2n)
x(m-1,2n+1)
x(m,2n+1)
x(m+1,2n+1)
x(m-1,2n+2)
x(m,2n+2)
x(m+1,2n+2)
x(m-2,2n+2)
x(m-2,2n+1)
x(m-2,2n)
x(m,2n)
x(m+1,2n)
x(m+2,2n)
x(m+2,2n+1)
x(m+2,2n+2)
-45º
45º
x(m-1,2n+3)
x(m,2n+3)
x(m+1,2n+3)
x(m-2,2n+3)
x(m+2,2n+3)
(a) The prediction process
-45º
45º
xe(m-1,n)
h(m-1,n)
h(m,n)
h(m+1,n)
xe(m-1,n+1)
xe(m,n+1)
xe(m+1,n+1)
xe(m-2,n+1)
h(m-2,n)
xe(m-2,n)
xe(m,n)
xe(m+1,n)
xe(m+2,n)
h(m+2,n)
xe(m+2,n+1)
-45º
45º
h(m-1,n+1)
h(m,n+1)
h(m+1,n+1)
h(m-2,n+1)
h(m+2,n+1)
(b) The update process
Figure 7.1 Prediction and update processes with the vertical angle θv in the proposed ADL, where
the integer pixels are marked by “o,” the half pixels by “+,” and the quarter pixels by “x.” (a) The
prediction process. (b) The update process.
The prediction of each xo(m,n) is a linear combination of neighboring even coef-
ﬁcients with strong correlation. As shown in Figure 7.1, assume that the pixels have
a strong correlation in the angle θv, where the integer pixels are marked by “o,” the
half pixels by “+,” and the quarter pixels by “x.” The prediction of x(m,2n + 1) is

140
7 Directional Wavelet Transform
taken as a linear combination of the even polyphase samples identiﬁed by the arrows
in Figure 7.1, speciﬁcally
Pe (m,n) = ∑
i
αixe (m+sign(i−1)tanθv,n+i),
(7.3)
where sign(x) is 1 for x ≥0 and -1 otherwise. The weight αi are given by the ﬁlter
taps. Note that xe(m+sign(i−1)tanθv,n+i) is not necessarily sampled at an integer
position. The corresponding ﬁnite impulse response function in z domain is
P(z1,z2) =
b
∑
i=a
αizsign(i−1)tanθv
i
zi
2.
(7.4)
Here, the indexes a and b delimit the ﬁnite support of the FIR wavelet ﬁlter. Since the
prediction is still calculated from the even polyphase samples, if the angle is known,
the ADL can still perfectly reconstruct the odd polyphase samples with Eq. (7.2).
In the updating step, the even polyphase samples are replaced with
l (m,n) = xe (m,n)+uh (m,n).
(7.5)
Note that l(m,n) always locates at an integer position. The update step of the pro-
posed ADL scheme is performed in the same angle as that in the prediction step. We
stress that the ADL framework is very general, and it does not have any restriction
on the update angle. We keep the prediction and update angles the same to save the
side information of coding the angles, also for the fact that the optimal update angle
is consistent with the prediction angle for most images. Consequently, in the update
step of the proposed ADL, the even polyphase samples are predicted as
uh (m,n) = ∑
j
β jh(m+sign(j)tanθv,n+ j).
(7.6)
The weights βj are also given by the ﬁlter taps. h(m + sign(j)tanθv,n + j) may
not be integer high-pass coefﬁcient due to tanθv. The corresponding ﬁnite impulse
response function in z domain is
U (z1,z2) =
d
∑
j=c
βjzsign(j)tanθv
1
z j
2.
(7.7)
Here, the indexes d and c delimit the ﬁnite support of the FIR wavelet ﬁlter. This step
is also trivially invertible. Given l(m,n) and h(m,n), one can perfectly reconstruct the
even polyphase samples. In addition, in order to achieve perfect reconstruction of
the original 2D signal, we impose the predicted and updated samples to be at integer
pixel positions in Eq. (7.2) and Eq. (7.5).
In summary, in Figure 7.2 we present a schematic diagram of the proposed ADL-
based wavelet transform. The proposed FIR functions of the extended Haar, 5/3 and
9/7 ﬁlters are given as follows, respectively

7.2 2D Wavelet Transform via Adaptive Directional Lifting
141
Haar :



P0 (z1,z2) = −z−tanθv
1
U0 (z1,z2) = ztanθv
1
/2
s0 = s1 = 1
,
(7.8)
5/3 :







P0 (z1,z2) = −

z−tanθv
1
+ztanθv
1
z2

/2
U0 (z1,z2) =

ztanθv
1
+z−tanθv
1
z−1
2

/4
s0 = s1 = 1
,
(7.9)
9/7 :























−1.586134

z−tanθv
1
+ztanθv
1
z2

−0.05298

ztanθv
1
+z−tanθv
1
z−1
2

0.882911

z−tanθv
1
+ztanθv
1
z2

0.443506

ztanθv
1
+z−tanθv
1
z−1
2

s0 = 1.230174
s1 = 1/s0
.
(7.10)
We can view the conventional lifting as a special case of the ADL, when θv = 0.
Upon completion of the 1D ADL wavelet transform running on index n, which can
be viewed as a generalized vertical transform, the generalized horizontal transform
is performed in the same way running on index m. Note that the prediction angle θh
of generalized horizontal transform is not required to be perpendicular to the vertical
counterpart θv. In other words, the generalized horizontal transform can optimize the
prediction direction for its lifting decomposition. We would like to emphasize the
ﬂexibility of the ADL scheme. When a 1D wavelet transform is factored into more
than one lifting stage (e.g., the 9/7 ﬁlter), except for the ﬁrst lifting stage, the spatial
prediction may be disabled in subsequent stages by setting θv to zero if the previous
lifting decomposition has removed the directional correlations.
Just like in the conventional rectilinear 2D wavelet transform, the 2D ADL
wavelet transform can decompose an image into multiple levels of different scales.
  2
  2
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
  2
  2
(a)
(b)
Figure 7.2 Generic 1D ADL transform: (a) analysis side and (b) synthesis side.

142
7 Directional Wavelet Transform
Figure 7.3 Exempliﬁed image and the resulting four subbands with the proposed ADL decompo-
sition. (a) The original image. (b) LL. (c) LH. (d) HL. (e) HH.
Although the generalized vertical and horizontal transforms of the ADL technique
do not necessarily have their lifting directions to be perpendicular to each other, they
still generate a subband structure that is identical to that of the rectilinear 2D wavelet
transform. This is because the 2D ADL wavelet transform splits the low and high
subbands horizontally and vertically in turn in exactly the same way as the conven-
tional wavelet transform, creating LL, LH, HL, and HH subbands in one level of
decomposition.
To visualize the effect of the 2D ADL wavelet transform, in Figure 7.3 we present
the four subbands of the test image of Figure 7.3a, which are the results of one level
of ADL decomposition. In the HL subband, only the top row of horizontal stripes
contains signiﬁcant amount of energy after the generalized vertical transform. The
adaptive directional prediction successfully removes the statistical redundancy in all
other directional patterns, which is exhibited by uniformly small prediction errors in
the bottom three rows of the HL subband. The HH subband has even less amount of
energy with no recognizable signal structures remaining, after the generalized verti-
cal and horizontal transforms. In the LH subband, the energy compaction is some-
what less effective than in HL and HH subbands. In addition to the vertical stripes
that still remain, some high-frequency diagonal textures also exist in the LH sub-
band. This is because the down-sampling process of the generalized vertical trans-
form makes the spatial resolution insufﬁcient for the ADL scheme to ﬁnd an accurate
prediction direction. However, even in this case, the signal energy left in the LH sub-
band is far lower than that of the conventional lifting scheme. To see the advantage
of an ADL transform over rectilinear wavelet transform on real images, the reader is
referred to Figure 7.6 for a quick preview.

7.2 2D Wavelet Transform via Adaptive Directional Lifting
143
7.2.2 Subpixel Interpolation
In order to perform directional prediction in an arbitrary angle θv, the proposed ADL
scheme needs to know the intensity values at fractional pixel locations. In other
words, tanθv used in Eq. (7.3) and Eq. (7.6) is generally not an integer. Hence, the
interpolation of subpixels becomes an issue.
We present an interpolation technique using Eq. (7.3) as an example. For perfect
reconstruction, the integer pixels used to interpolate the fractional pixel at angle θv
have to be even polyphase samples xe(m,n). No odd polyphase samples xo(m,n) can
participate in the prediction in this case. The interpolation can be generally described
as
xe (m+sign(i−1)tanθv,n+i) = ∑
k
akxe(m+k,n+i).
(7.11)
Here, the subscript k indexes the integers around sign(i −1)tanθv, and ak is the in-
terpolation ﬁltering parameters. Based on Eq. (7.3) and the z transform of Eq. (7.11),
we have
zsign(i−1)tanθv
1
= ∑
k
akzk
1.
(7.12)
We adopt the popular Sinc interpolation technique [160]. The interpolation of Eq.
(7.6) is performed the same way.
The subpixel interpolation problem can also be cast into an optimal ﬁlter design
problem. Since the interpolation ﬁlter can be generalized to any FIR ﬁlter, one can
design the ﬁlter to minimize the energy of the high subband
D = ∑
m,n
h2 (m,n) = ∑
m,n
|xo (m,n)−∑
k
akx(m,n+k)|2.
(7.13)
The minimization problem
min···,ak−1,ak,ak+1,···∑
m,n
"
xo (m,n)−∑
k
akx(m,n+k)
#2
(7.14)
can be solved by the standard least-square method.
The ﬁlter coefﬁcients ··· ,ak−1,ak,ak+1,··· can be optimized for a given input
image or a set of training images. In the former case, the optimal coefﬁcients have
to be sent as side information, whereas in the latter case the training set should have
the same statistics of the input image.
In principle, the prediction angle θv can be a continuous variable. However, in
practice, we found that nine uniformly quantized discrete angles, θi, i = 0, ±1, ±2,
±3, ±4, sufﬁce to reap all the coding gains of ADL. Equivalently, the interpolation
is done at the spatial resolution of the quarter pixel.

144
7 Directional Wavelet Transform
7.3 R-D Optimized Segmentation for ADL
In order for an image codec to beneﬁt from the adaptability of ADL to the edge or
texture orientation, it has to segment an image into regions of textures and edges of
clear directional bias. This poses the following rate-distortion optimization problem.
For ease of implementation, we partition the image recursively into blocks of vari-
able sizes by quad-tree. All pixels in a quad-tree block will be subject to the same
ADL transform. The ﬁner the segmentation, the greater degree of gradient unifor-
mity in the resulting blocks. This leads to better directional prediction of the image
signal, hence lower distortion. However, the improved signal approximation of ADL
is achieved at the expense of increased side information to describe the segmentation
tree and the lifting directions of individual blocks. To ﬁnd the quad-tree of optimal
balance between the cost of coding the segmentation tree and the cost of coding
ADL transform coefﬁcients, we apply the well-known BFOS algorithm for optimal
tree pruning [159].
First, we build a full quad-tree T by recursively partitioning each block into four
subblocks until reaching a prespeciﬁed minimum block size. For a subtree S ⊂T we
deﬁne its distortion
D(S) = ∑
τ∈S∑
m,n
| hτ,ϑ (m,n) |,
(7.15)
where hτ,ϑ (m,n) is the high subband coefﬁcient of the ADL decomposition in the
optimal direction ϑ, and the subscript τ denotes a terminal (leaf) node of the subtree
S. Also, we deﬁne the rate of the subtree S as
R(S) = ∑
τ∈S
rC (τ)+ ∑
v∈S
rT (v),
(7.16)
where rC(τ) is the rate of entropy-coding all high subband coefﬁcients in terminal
node τ, and rT(v) is the rate of coding the side information of the tree node v ∈S.
Recall from the previous section that ADL chooses the optimal prediction direc-
tion ϑ out of nine discrete angles θi,i = 0,±1,±2,±3,±4. We adopt a predictive
coding scheme to code ϑ based on the observation that the image gradient changes
smoothly. Referring to Figure 7.4, in a general location of a terminal quad-tree block
Figure 7.4 Prediction of lifting direction.

7.4 Experimental Results and Observations
145
related to its neighboring blocks of lifting angles αn, αw, and αd, we predict the
optimal lifting direction ϑ to be
ˆϑ =

αw
| αd −αw |>| αd −αn |
αn
| αd −αw |≤| αd −αn | .
(7.17)
Then, the prediction error eϑ = ϑ −ˆϑ is arithmetic coded. The coded segment struc-
ture and predicted angle error are embedded in the user data part of JPEG 2000.
If v ∈S is an internal node, then rT(v) = 1, that is, the bit to indicate the event
of splitting the corresponding block; otherwise, for each leaf node τ ∈S,rT(τ) is the
number of bits to code the coefﬁcients hτ,ϑ(m,n) in node τ and the optimal direction
ϑ of the ADL, plus one bit to signal the terminal node. The rate of coding is estimated
by the entropy of ADL coefﬁcients hτ,ϑ(m,n) in node τ, and the rate of coding ϑ is
estimated by the entropy of prediction residual eϑ = ϑ −ˆϑ.
Among all the pruned subtrees of T, an R-D optimal segmentation tree S∗is the
one that minimizes the cost function
J = D(S)+λR(S),
(7.18)
where λ is a Lagrangian multiplier that determines the total rate of coding the subtree
S∗and the coefﬁcients hτ,ϑ(m,n) of all the terminal nodes τ ∈S. Given a λ, we ap-
ply the BFOS optimal tree pruning algorithm to compute S∗. In order to meet a target
rate R0 of coding an image, one can apply the BFOS algorithm iteratively in a binary
search of the corresponding value of λ0. However, this process is expensive. Alter-
natively, we ﬁnd a simple fast trick to approximate the value of λ0, using the JPEG
2000 image code as a reference. It is well known that the EBCOT technique of JPEG
2000 generates an almost convex operational R-D curve. The proposed ADL image
coding method also adopts the EBCOT technique for entropy coding of hτ,ϑ(m,n)
and, hence, has an almost convex operational R-D curve as well. The only difference
is that the ADL scheme adapts to the image signal better than the rectilinear wavelet
transform. Everything else being equal, the former method approximately translates
the operational R-D curve of the latter. Based on this observation, we run the JPEG
2000 code ﬁrst to obtain its R-D slope λJ(R0) at the target rate R0. Then we estimate
the Lagrangian multiplier of our method for the target rate R0 to be λ0 = λJ(R0)−δ,
where δ is an offset. Figure 7.5 shows the segmentation of the test image Barbara
generated by the above algorithm and the optimal prediction directions of ADL in
the quad-tree terminal nodes.
7.4 Experimental Results and Observations
The proposed ADL wavelet transform is implemented. In order to evaluate the ADL
performance only as objectively as possible, we simply replace the wavelet trans-
form module of JPEG 2000 by the ADL transform and use the same bit-plane coding
(quantization module) and EBCOT technique (entropy coding module) as in JPEG

146
7 Directional Wavelet Transform
Figure 7.5 Partition of Barbara and the directions for each block.
2000. On a side note, the high modularity of the proposed ADL wavelet transform
and its identical subband decomposition structure to other separable 2D transforms
make our experiments very easy to conduct within the JPEG 2000 framework. This
also means that the proposed ADL-based image codec can be made nearly compati-
ble to the JPEG 2000 standard.
We report the experimental results of six common testing images: Barbara (512×
512), Lena (512×512), Baboon (512×512), Bike (2560×2048), Woman (2048×
2560), and Cafe (2560 × 2048), plus the ﬁrst frame of the Foreman (352 × 288)
video sequence from the MPEG testing set. The reference software VM 9.0 of JPEG
2000 is used in our comparison study. We compare the abilities of the proposed ADL
wavelet transform and that of JPEG 2000 in energy packing, or spatial de-correlation.
The same 5/3 ﬁlter is used in two methods.
Table 7.1 tabulates the average coefﬁcient magnitudes of the LH, HL, and HH
subbands. The ADL technique has a signiﬁcant advantage over the conventional rec-
tilinear wavelet transform on Barbara. This should be expected because this test-
ing image contains strong directional textures. The reduction in average coefﬁcient
Table 7.1 Average coefﬁcient magnitudes in the LH, HL, and HH subbands; the numbers in brack-
ets represent the percentage of reduction.
Subbands
Methods
Barbara
Bike
Cafe
Foreman
LH
JPEG 2000
5.31
3.01
5.01
1.3
ADL
2.35(55.8%)
2.25(27.3%)
4.04(19.2%)
0.92(24.3%)
HL
JPEG 2000
2.26
3.19
5.51
2.07
ADL
1.44(36.0%)
2.39(25.1%)
4.4(20.1%)
1.43(31.5%)
HH
JPEG 2000
1.48
1.05
1.56
0.6
ADL
0.97(34.6%)
0.88(15.5%)
1.5(4.5%)
0.51(16.0%)

7.4 Experimental Results and Observations
147
magnitude is 55.8% in the LH subband, 36% in the HL subband, and 34.6% in the
HH subband. On other test images, the ADL technique also outperforms JPEG 2000.
Parallel to Table 7.1, Figure 7.6 shows one level of wavelet decomposition of test-
ing image Barbara by JPEG 2000 and the proposed ADL. For clear visualization, the
magnitudes of coefﬁcients in the LH, HL, and HH subbands are scaled into the range
[0, 255] in the plot. We bring the readers’ attention to the fact that the ADL technique
not only has an appreciably lower signal energy in the high-frequency subbands in
comparison with JPEG 2000, but also its residual signals in the high-frequency sub-
bands are much less correlated to the original spatial features of the image. This
property contributes to superior visual quality of the reconstructed images by ADL,
as we will see in Figure 7.7.
To compare the coding performance of the two methods, we present in Table
7.2 the peak signal-to-noise ratio (PSNR) results of all the test images at 0.125, 0.25,
0.5, and 1.0 bpp. For either method, all the images are decomposed by the three-level
2D transform. Both the popular biorthogonal 9/7 and 5/3 ﬁlters are tested. The Sinc
interpolation is used by the ADL to generate subpixels. The coding gain can be up to
2.0 dB for Barbara coded with the 5/3 ﬁlter. On the test set the gain of ADL ranged
from 0.21 dB to 1.36 dB, depending on the presence or lack of orientation features in
the image. However, even on relatively smooth images like Lena, the ADL method
still enjoys an advantage over the rectilinear 2D wavelet transform. The results with
different prediction precisions are also given in Table 7.2. The results of subpixel
prediction are signiﬁcantly better than those of the integer prediction with both the
5/3 and 9/7 ﬁlters. However, the results of quarter-pixel precision are only marginally
better than those of half-pixel prediction on images of strong edge features such as
Barbara and Women. On relatively smooth images, the angular prediction of quarter-
pixel precision can be even slightly worse than that of half-pixel precision (see the
case of Foreman at 1.0 bpp).
Table 7.3 shows the average number of side information bits for coding segmen-
tation tree and associated directions, which depends on overall bit rates and image
contents. One can see that the side information bits constitute only a small portion
of total code length. The percentage of overhead bits at low bit rates is less than that
at high bit rates because of the proposed R-D optimized segmentation and direction
estimation.
Due to its adaptability to directional signal features, the ADL wavelet transform
tends to reconstruct image edges better than the rectilinear 2D wavelet transforms at
the same bit rates. Since the human visual system is highly sensitive to edges, the
former method should have superior visual quality than the latter method, which is
indeed corroborated by our experimental results. As an example, Figure 7.7 presents
the decoded Barbara images by the two methods, both at the rate 0.3 bpp and using
the 5/3 ﬁlter. The image decoded by JPEG 2000 has severe moir patterns on the scarf
and pants, which even change the texture orientations of the original image. Also,
edge ringing effects are clearly visible in the JPEG 2000 decoded image. In contrast,
the ADL-based image code eliminates most of the above artifacts. It reproduces the
high-frequency scarf pattern almost perfectly, and greatly reduces the ringing effects
around the edges.

148
7 Directional Wavelet Transform
(a)
(b)
Figure 7.6 Coefﬁcient magnitudes of (a) JPEG 2000 and (b) ADL after one level of 2D decompo-
sition.

7.4 Experimental Results and Observations
149
(a)
(b)
Figure 7.7 Visual quality comparison of decoded Barbara at 0.3 bpp: (a) JPEG 2000 versus (b)
ADL.

150
7 Directional Wavelet Transform
Table 7.2 Comparisons of coding performance (in decibels) between JPEG 2000 and the proposed
ADL-based coding scheme with the 5/3 and 9/7 ﬁlters.
Images
bpp
5/3 ﬁlter
9/7 ﬁlter
J2K
ADL
ADL
ADL
J2K
ADL
ADL
ADL
1-pel
1/2-pel 1/4-pel
1-pel
1/2-pel 1/4-pel
Barbara
0.125
24.59
25.69
26.07
25.95
25.02
26.16
26.32
26.45
0.25
27.38
28.79
29.13
29.22
28.27
29.37
29.70
29.78
0.5
30.95
32.35
32.87
32.95
32.15
33.10
33.50
22.58
1
36.04
36.89
37.16
37.24
37.11
37.58
37.84
37.88
Lena
0.125
30.11
30.66
30.75
30.68
30.41
30.88
31.00
31.02
0.25
33.22
33.76
33.90
33.88
33.78
34.13
34.18
34.25
0.5
36.45
36.80
36.91
36.94
37.02
37.17
37.23
37.30
1
39.51
39.68
39.74
39.73
40.06
40.09
40.17
40.20
Baboon
0.125
21.40
21.49
21.44
21.39
21.50
21.50
21.70
21.69
0.25
22.87
23.03
23.05
23.04
23.10
23.10
23.25
23.23
0.5
25.17
25.35
25.39
25.34
25.52
25.52
25.73
25.73
1
28.62
28.79
28.83
28.88
29.02
29.02
29.29
29.32
Bike
0.125
25.74
26.62
26.66
26.61
25.93
26.77
26.78
26.78
0.25
29.06
29.96
29.96
29.97
29.36
30.09
30.11
30.12
0.5
33.09
33.80
33.79
33.76
33.38
33.98
33.99
34.00
1
37.73
38.27
38.29
38.28
38.04
38.46
38.47
38.48
Cafe
0.125
20.42
20.42
20.57
20.55
20.70
20.80
20.83
20.84
0.25
22.74
22.74
23.00
22.99
23.09
23.25
23.29
23.32
0.5
26.42
26.42
26.66
26.67
26.78
26.93
27.02
27.06
1
31.72
31.72
31.87
31.90
32.02
32.08
32.13
32.18
Women
0.125
26.59
26.77
26.96
27.00
26.94
27.16
27.33
27.35
0.25
29.24
29.35
29.57
29.65
29.67
29.82
29.98
30.06
0.5
33.00
33.03
33.14
33.24
33.42
33.51
33.60
33.72
1
37.96
37.96
37.97
38.07
38.31
38.33
38.36
38.47
Foreman
0.125
28.71
29.82
29.85
29.94
29.15
30.06
30.11
30.16
0.25
32.32
33.48
33.50
33.53
32.81
33.68
33.70
33.77
0.5
35.89
37.19
37.19
37.25
36.33
37.27
37.44
37.43
1
40.60
41.34
41.38
41.30
40.85
41.55
41.68
41.63
The ADL wavelet transform retains the PSNR and spatial scalability of JPEG
2000. Although the segmentation tree for ADL is optimized for a given target rate as
we discussed in Section 7.3, the rate-distortion performance of the proposed ADL-
based image coding still outperforms JPEG 2000 in a wide range of bit rates when
operating progressively. In Figure 7.8, we compare the rate-distortion performances
of the scalable ADL coded stream optimized for the mid bit rate of 0.6 bpp versus
JPEG 2000 of four quality layers. Only when the bit rate is extremely low (at or
below 0.1 bpp), the ADL-based image coding has inferior performance to that of
JPEG 2000 due to the fact that the side information for the segmentation tree has to
be placed at the beginning of the coded stream, and, hence, this part of the coded
stream is not scalable.
We have incorporated the ADL wavelet transform into the set partitioning in hi-
erarchical tree (SPIHT) scheme to evaluate its performance when coupled with a

7.4 Experimental Results and Observations
151
Table 7.3 Bits for coding the segmentation tree and associated directions (bits per pixel).
Total bpp
Barbara
Lena
Baboon
Bike
Cafe
Women
Foreman
ADL 5/3
0.125
0.015
0.009
0.008
0.007
0.006
0.004
0.017
0.25
0.019
0.020
0.012
0.013
0.009
0.007
0.021
0.5
0.034
0.040
0.021
0.031
0.021
0.021
0.033
1
0.034
0.040
0.021
0.031
0.021
0.021
0.033
ADL 9/7
0.125
0.008
0.005
0.004
0.003
0.002
0.002
0.011
0.25
0.013
0.009
0.007
0.006
0.003
0.003
0.013
0.5
0.018
0.018
0.008
0.013
0.007
0.005
0.021
1
0.018
0.018
0.008
0.013
0.007
0.005
0.021
Figure 7.8 Scalable-rate PSNR curves of ADL-based code optimized for 0.6 bpp versus those of
JPEG 2000.
Table 7.4 Comparison of the coding performance between SPIHT and ADL-based SPIHT in
PSNR.
Total bpp
Barbara
Lena
Baboon
Bike
Cafe
Women
Foreman
SPIHT
0.125
24.83
21.69
31.04
25.89
20.67
27.33
29.34
0.25
27.56
23.24
34.03
29.13
23.03
29.95
32.76
0.5
31.38
25.61
37.07
33.02
26.49
33.59
36.33
1
36.40
29.16
40.18
37.71
31.73
38.28
40.95
ADL-Based SPIHT
0.125
26.38
21.79
31.13
26.41
20.78
27.60
30.19
0.25
29.28
23.37
34.18
29.72
23.19
30.20
33.76
0.5
33.14
25.76
37.22
33.54
26.69
33.74
37.34
1
37.63
29.29
40.25
38.06
31.86
38.33
41.60
different entropy coding scheme. Table 7.4 tabulates the coding results of the ADL-
based SPIHT at quarter-pixel precision and the original SPIHT. The parameters used
are the same as those in the original SPIHT. Only the wavelet transform is replaced
with the proposed ADL wavelet transform. One can see that the ADL approach
makes similar gains with the zero-tree entropy coding technique of SPIHT as with
the EBCOT entropy coding technique of JPEG 2000.

152
7 Directional Wavelet Transform
Finally, let us discuss the computational complexity of the proposed ADL-based
image codec in comparison with that of JPEG 2000. Clearly, the ADL-based image
encoder has considerably higher complexity since it has to compute the image depen-
dent segmentation tree T, and then searches for the optimal prediction direction in
each terminal node of T. However, the decoder complexity of an ADL-based image
compression method can be made comparable to that of JPEG 2000. This is because
the inverse ADL transform is only slightly more expensive than inverse rectilinear
transform, once the optimal lifting direction is given as side information. The asym-
metric design of the ADL technique makes it suitable for some Internet and wireless
applications that aim to achieve the highest possible rate-distortion performance by
heavily optimizing the image coding ofﬂine and once and for all, while maintaining
a reasonably low decoding complexity.
7.5 Summary
The rigidity of the existing rectilinear 2D wavelet transforms makes them ill suited
for approximating image features of arbitrary orientations. This weakness can be
overcome by the proposed new ADL technique. It can be seamlessly integrated into
the conventional, global, and separable 2D wavelet transform, and can be imple-
mented by any wavelet ﬁlter. In each ADL lifting stage, the prediction step can be
performed in the direction of the strongest pixel correlation rather than stay me-
chanically ﬁxed in the horizontal or vertical direction. Even though its prediction
and update operations are based on fractional pixels, the ADL wavelet transform can
guarantee perfect reconstruction without imposing any constraint on the interpolation
method. A rate-distortion, optimized image-segmentation method is also developed
so that ADL can efﬁciently approximate directional image features in local regions.
Empirical results demonstrate the superior objective and perceived quality of the
ADL-based image codec.
There are still several open issues to be investigated in the proposed ADL-based
coding schemes. First, the interpolation used in the ADL wavelet transform is always
performed in either the horizontal or vertical direction. It may blur the orientation
property existing in raw images. Second, image blocks with different directions are
continuously processed, which may cause boundary effects in the block boundaries.
Third, the entropy coding does not take directional information into account, which
should be used in the context model of arithmetic coding.

Chapter 8
Directional DCT Transform
8.1 Introduction
Most of the image compression schemes are constructed upon the identical architec-
ture of the 2D transform followed by entropy coding, for example, JPEG uses the
2D discrete cosine transform (DCT) and variable length coding (VLC) [129], and
JPEG 2000 uses the 2D wavelet transform and Embedded Block Coding with Op-
timized Truncation (EBCOT) [103, 130]. Either 2D DCT or 2D wavelet transform
is implemented by separable 1D transform in horizontal and vertical directions. A
serious drawback of these transforms is that they are ill-suited to approximate im-
age features with arbitrary orientation that is neither vertical nor horizontal. In these
cases, they result in large-magnitude high-frequency coefﬁcients. At low bit rates,
the quantization noise from these coefﬁcients is clearly visible, in particular causing
annoying Gibbs artifacts at image edges with arbitrary directions.
How to incorporate directional information into transform is a challenging prob-
lem in both image and video coding. Some work has been reported in wavelet and
subband transforms. Taubman et al. ﬁrst proposed a technique to resample an image
before conventional subband decomposition [132]. The resampling process actually
rotates image edges into horizontal or vertical direction so that following subband
decomposition can gain accurate predictions from neighboring horizontal or ver-
tical pixels. The lifting structure developed by Sweldens provides a good way to
incorporate directional information into wavelet transform, in which each ﬁnite im-
pulse response (FIR) wavelet ﬁlter can be factorized into several lifting stages [150].
Consequently, some adaptive lifting transforms have been proposed for the purpose
[151,153–158,161]. In particular, the proposed adaptive directional lifting (ADL) by
Ding et al. [157,161] can outperform JPEG 2000 by up to 2 dB on images with rich
orientation features by introducing directional data coding and subpixel directional
prediction.
The efforts in DCT are to ﬁrst introduce spatial prediction before transform. Feig
et al. incorporate the idea of spatial prediction into a JPEG-like code in a manner
similar to the fractal-based image compression [127]. Even though this method does
153

154
8 Directional DCT Transform
not offer a better rate-distortion (R-D) performance than pure DCT-based coding, it
has far fewer block artifacts and markedly better visual quality at very low bit rates.
Kondo and Oishi perform directional prediction on DCT blocks. Their prediction
is based on one of four coded neighboring DCT blocks [128]. The new video cod-
ing standard H.264 successfully applies the block-based spatial prediction technique
to intra-frame coding. Signiﬁcant coding gains are made over the version without
spatial prediction [13].
To the best of our knowledge, the paper from Zeng and Fu is the ﬁrst effort on how
to incorporate directional information into DCT [162]. Their directional DCT is mo-
tivated by SA-DCT (shape-adaptive DCT) [163]. For an N-by-N block, the ﬁrst 1D
DCT is performed along with the direction selected, where each column is of a dif-
ferent length. Then DCT coefﬁcients are reorganized according to their frequencies
from DC to AC. Coefﬁcients at the same frequency viewed as a row are transformed
by the second 1D DCT, where each row is also of a different length. Since the direc-
tional transform is constrained within one block, direction selection is relatively easy
in Zeng’s scheme. But this also potentially hurts the performance of directional DCT
because the correlation among neighboring blocks with the similar direction is not
exploited. In addition, directions at fractional pixels are not considered yet by Zeng
and Fu [162].
Inspired by the lifting-based transform [150] and our previous work on ADL
[157, 161], a lifting-based directional DCT-like transform is presented in this chap-
ter, which can be performed along arbitrary direction in theory. The lifting scheme
factorizes 1D DCT into a series of so-called primary operations. We propose the
corresponding directional form of each primary operation because introducing di-
rectional information in primary operation is much easier than in a whole transform.
All of the directional primary operations construct the proposed directional DCT-like
transform. Its matrix is dependent on selected direction angle and interpolation used
there. The perfect reconstruction property is guaranteed by the lifting scheme when
transform coefﬁcients are not quantized at all.
8.2 Lifting-Based Directional DCT-Like Transform
In this section, the lifting structure of 1D DCT is ﬁrst introduced in a self-contained
way. And then the proposed 1D directional DCT-like transform is developed from
the lifting structure. Since the 8-point DCT is extensively used in image coding, only
the 8-point directional transform is taken as an example to be discussed here. The
proposed ideas can be easily applied to a DCT of arbitrary size.
8.2.1 Lifting Structure of DCT
There are many approaches reported to factorize any N-point 1D DCT into a series of
primary operations through plane rotation and butterﬂy for the fast implementation

8.2 Lifting-Based Directional DCT-Like Transform
155
cn = cos(nπ/16)−1
sin(nπ/16)
dn = sin(nπ/16)
Figure 8.1 Factorizing 8-point DCT into primary operations. x[n] and y[n] (n = 0;1;...;7) are the
input signal and output DCT coefﬁcient, respectively. O(i = 1;2;...;35) is the primary operation.
in either software or hardware [164–167]. With the lifting structure emerging, several
lifting-based fast multiplierless approximations of DCT have been reported by Tran
[168] and Liang and Tran [169]. It has been proven that any orthogonal ﬁlterbank
including DCT can be decomposed into delay elements and plane rotations by lattice
factorizations [170]. Since the lifting structure enables ﬂexible implementation of a
transform with perfect reconstruction, we take the work from Liang and Tran [169]
as the basis to develop directional DCT-like transform.
Similar to the work of Liang and Tran [169], 8-point 1D DCT is implemented
in the form of lifting structure as shown in Figure 8.1, where O21 −O30 are slightly
modiﬁed according to Loefﬂers factorization [166] so that the pixel distances in these
operations are as close as possible, thus fully exploiting pixel correlation. Eight hor-
izontal lines of Figure 8.1 represent 8 pixels to be transformed. The arrow lines and
scales denote primary operations, where the DCT transform is factorized to totally
35 steps indicated by O1 −O35 in Figure 8.1. O9 −O17 and O18 −O30 form two inde-
pendent paths. The order of primary operations in different paths has no dependency.
To make the terminologies and discussions of this chapter consistent and clear,
we will discuss both the lifting-based DCT given in Figure 8.1 and the proposed
directional DCT-like transform in Z domain because the proposed 1D directional
transform of a row or a column may inﬂuence each other with the transforms of other
neighboring rows or columns through interpolation. Assume that x is a 2D image of
N rows and M columns. 1D lifting DCT is performed in the vertical direction. Pixels
in each row are granted as a 1D signal. The Z transform of this signal is deﬁned as
X [n] =
M
∑
m=1
x[n,m]z−m, n = 1,··· ,N.
(8.1)

156
8 Directional DCT Transform
1D lifting DCT is performed on X[n] according to the same lifting structure and
primary operations in Figure 8.1. It actually contains M of 1D vertical lifting DCT
although they are independent.
All primary operations in Figure 8.1 can be categorized into two types. If there
is an arrow line from a pixel to another with a parameter α, the operation can be
represented in Z domain as
O(X [ni],X [nj],α) =























X ←−


1
1
1
1
1
α
1
1
1


X























.
(8.2)
X is a row vector, which consists of 8 continuous X[n]. For the convenience of later
discussion, we call X[ni] and X[nj] as the target and source of a primary operation,
respectively. α locates at the ni-th row and the nj-th column of operation matrix. If
there is a scale parameter α beside a pixel, the second type of primary operation is
deﬁned as
O(X [ni],α) =























X ←−


1
1
1
α
1
1
1
1


X























.
(8.3)
α is the diagonal element at the ni-th row of operation matrix. According to Figure
8.1, the 8-point 1D DCT is factorized to 35 primary operations from left to right
Y = DCT8 (X) = O35 ◦O34 ◦···O2 ◦O1 (X).
(8.4)
Y is obtained DCT coefﬁcient vector in Z domain. It contains all coefﬁcients of 1D
vertical DCT in all columns. Each column has a unique z−m. DCT8() denotes the
8-point 1D DCT. Ok is the kth primary operation. Its parameter and operated pixels
are depicted in Figure 8.1.
The inverse primary operations of O(X[ni],X[nj],α) and O(X[ni],α) are deﬁned
as O−1(X[ni],X[nj],−α) and O−1(X[ni],1/α), respectively. Let O−1
k
denote the in-
verse operation of Ok. Then, the inverse 8-point DCT transform is represented as
X = DCT −1
8
(Y) = O−1
1 ◦O−1
2 ◦···O−1
34 ◦O−1
3 5(Y).
(8.5)

8.2 Lifting-Based Directional DCT-Like Transform
157
Since each of these primary operations is perfectly reversible, the DCT given in Eq.
(8.4) is also perfectly reversible.
8.2.2 Directional DCT-Like Transform
Introducing directional information in primary operations is much easier than in a
whole DCT because a primary operation only involves a few pixels. The ﬁrst step of
developing directional transform is to deﬁne the directional primary operation in the
lifting structure. As shown in Figure 8.2a, if a primary operation does not involve
directional information, 1D vertical DCT is performed on each column and does not
use any pixels in other columns. But, if a primary operation is performed along a di-
rection rather than the vertical one, the 1D DCT transform may have to involve pixels
of multiple columns. As shown in Figure 8.2b, if the source of a primary operation
locates at a fractional pixel illustrated by a gray circle, its value is interpolated from
neighboring integer pixels in a row. If the target of a primary operation locates at a
fractional pixel, the resulting value is only an intermediate one. Similar to the energy
distributed update (EDU) method proposed by Lee et al. [26], the intermediate value
is immediately distributed to neighboring integer pixels according to their weighting
factors of interpolation. Note that the target at a fractional pixel only happens on
the upward primary operations. The downward and upward primary operations are
illustrated by downward and upward arrows in Figure 8.1, respectively.
(a)
(b)
Figure 8.2 Exempliﬁed primary operations: (a) Nondirectional and (b) directional, where the white
circles denote integer pixels and the gray circles denote half pixels.

158
8 Directional DCT Transform
For a given direction angle θ unequal to zero as shown in Figure 8.2b, the ﬁrst
type of directional primary operation with a parameter α from one pixel to another
is deﬁned in Z domain as
O(X [ni],X [n j],θ,α) =























X ←−


1
1
1
1
1
αF(θ)
1
1
1


X























,
(8.6)
where ni = n5 and nj = n2 in the case illustrated in Figure 8.2b. F(θ) is the function
of shifting and interpolation in Z domain and locates at the ni-th row and the nj-th
column of operation matrix. It is deﬁned as
F (θ) = Z−|ni−n j|tan(θ).
(8.7)
If |ni −n j|tan(θ) is an integer, F(θ) only means a left shifting of |ni −n j|tan(θ)
pixels; otherwise the interpolation is needed and F(θ) is redeﬁned as
F(θ) = ∑
k
wkz−k.
(8.8)
Here, k indexes the integers around |ni −n j|tan(θ), and w′
ks are the interpolation
ﬁltering parameters. Therefore, the directional primary operation Eq. (8.6) actually
involves more than one pixel. Similarly, the inverse operation of Eq. (8.6) should be
O−1(X[ni],X[nj],θ,−α). For the scaling primary operation, there is no difference
between the directional version and the nondirectional version because it involves
only one pixel at one time.
After the directional primary operations have been deﬁned, it is not difﬁcult to
derive a new directional transform by assembling directional primary operations ac-
cording to the order and the structure depicted in Figure 8.1. Similar to Eq. (8.4), the
directional transform can be generally described as follows:
Y = DCTdir8 (X,θ) = O35 ◦O34 ◦···◦O2 ◦O1(X,θ).
(8.9)
DCTdir8 indicates 1D 8-point directional DCT-like transform. The directional angle
of every primary operation is θ. The ﬁrst type of primary operation used in Eq. (8.9)
is deﬁned in Eq. (8.6) and the second type of primary operation in Eq. (8.3). The
corresponding inverse 1D directional DCT-like transform is given as
X = DCT −1
dir8 (Y,θ) = O−1
1 ◦O−1
2 ◦···◦O−1
34 ◦O−1
35 (Y,θ).
(8.10)
The proposed directional DCT-like transform can guarantee the perfect reconstruc-
tion property if transform coefﬁcients are not quantized at all, namely,

8.2 Lifting-Based Directional DCT-Like Transform
159
X = DCT −1
dir8 (DCTdir8 (X,θ),θ),
(8.11)
because the product of operation matrix of each primary operation and its inverse
operation matrix is always equal to a unit matrix.
What is the exact transform matrix of the proposed directional DCT-like trans-
form? From Eq. (8.6), it is dependent on directional angle and interpolation used
there. The transform matrix in the case of θ = argtan(1/2) and linear interpolation
is discussed here. Numerical values of the transform matrix are calculated from Eq.
(8.9) and illustrated in Figure 8.3a. The transiting result of a numerical matrix multi-
plying a row vector [z6,z5,··· ,z−3]T in the ﬁrst row is a column vector. It is the ﬁrst
row of an 8 × 8 transform matrix. For example, the ﬁrst element of the ﬁrst row is
equal to
−0.006z−3 +0.099z−2 −0.613z−1 +1.392−0.613z+0.099z2 −0.006z3.
It is a low-pass ﬁlter with 7 nonzero coefﬁcients. It means that the 1D vertical trans-
form will involve 6 neighboring columns. The longer the interpolation ﬁlter is, the
more neighboring columns that are involved in the transform. Other elements are
similar to the ﬁrst element. Although the transform matrix looks a bit complicated,
the computation of transform is still very easy thanks to the lifting structure.
8.2.3 Comparison with Rotated DCT
The straightforward way to make DCT adaptive to different orientations is to ﬁrst
rotate the image to a horizontal or vertical direction and then perform DCT on the
rotated image, just like that done by Taubman and Marcellin [132]. For the same
example of θ = argtan(1/2) and linear interpolation, numerical values of the trans-
form matrix of the straightforward way are calculated and depicted in Figure 8.3b in
the similar form. For the given θ = argtan(1/2), pixels at the ﬁrst, third, ﬁfth, and
seventh row locate at a half pixel, thereby corresponding elements in the transform
matrix contains two items in Z domain due to linear interpolation. Other elements
only contain one item because they correspond to integer pixels.
One can observe the signiﬁcant differences on the transform matrixes between
the proposed directional DCT-like transform and the rotated DCT. It is not difﬁcult
to be explained. In the proposed transform, the interpolation is performed in each
primary operation of lifting structure. Just as we have mentioned, if the source of
a primary operation is a fractional pixel, its value is interpolated from neighboring
integer pixel; and if the target of a primary operation is a fractional pixel, the result
of primary operation is propagated to neighboring integer pixels. However, in the
rotated DCT, the interpolation is performed before DCT and there is no interpolation
anymore during the transform.
In addition to the differences on transform matrixes, the results of inverse trans-
form are also different. The result of inverse transform in the proposed method

160
8 Directional DCT Transform
(a)
(b)
Figure 8.3 The transform matrixes: (a) the proposed directional DCT-like transform; (b) the
straightforward directional DCT transform after rotation.
directly is the image to be reconstructed. The perfect reconstruction can be guar-
anteed by the lifting structure. However, in the rotated DCT, the result of inverse
transform is the image after rotation and interpolation. The ﬁnal reconstructed im-
age is obtained by rotating the result back. Without the special process as proposed

8.3 Image Coding with Proposed Directional Transform
161
by Taubman and Zakhor [132], the perfect reconstruction cannot be guaranteed. The
serious problem in the rotated DCT is that it is only suitable to the adaptation on
large image regions. In other words, the whole image or some rows of image has
the same direction of correlation. It is difﬁcult to be applied on small size blocks.
One can imagine how difﬁcult it is rotating each small block (e.g., 8 × 8 size) with
different directions and still assembling rotated blocks into an image for later rectan-
gular block-based compression. It is the reason to prevent this technique from being
applied in practical coding schemes.
8.3 Image Coding with Proposed Directional Transform
In this section, we apply the proposed directional DCT-like transform into a JPEG-
wise image coding scheme as shown in Figure 8.4. Except for those modules re-
lated to the directional transform, quantization denoted by “Q” in Figure 8.4, de-
quantization denoted by “Q−1,” entropy coding and entropy decoding are the same
as those in JPEG.
In the encoder side, input image is ﬁrst analyzed block by block to decide their
transform directions. Then 2D transform is performed in each block along with the
selected directional angle. Note that only 1D directional DCT-like transform is dis-
cussed in Section 8.2. But in a practical image and video coding scheme, 2D trans-
form is needed. Therefore, the proposed directional DCT-like transform is applied
in a block as the ﬁrst 1D transform (e.g., vertical direction). After the 1D transform,
generated coefﬁcients do not own orientation property anymore. They are organized
in different rows, saying DC, AC1, ··· and AC7, from low frequency to high fre-
quency. Then, the second 1D transform is a normal one and performed on each row.
For simpliﬁcation, the transforms are vertically aligned, so that they are well ﬁlled in
the 8 × 8 blocks. When the 2D transform is completed, the coefﬁcient at the upper-
left corner is the DC of this 8 × 8 block, just the same as the conventional DCT.
They are quantized after ZigZag scan and coded by VLC to generate compressed
stream. In the decoder side, received data is decoded to recover coefﬁcients. Coefﬁ-
cients are ﬁrst transformed by the normal 1D inverse DCT on each row and then are
Figure 8.4 The coding scheme using a directional DCT-like transform.

162
8 Directional DCT Transform
Figure 8.5 Available direction modes that are predeﬁned in this chapter.
transformed by the proposed directional inverse transform along with the direction
of each block to reconstruct the image.
In order to reduce the cost of such directional data, 9 modes are predeﬁned as
available directions, depicted in Figure 8.5 as the set {−4,··· ,0,··· ,4}, where inte-
ger pixels are marked by “o,” half pixels by “+,” and quarter pixels by “x.” The angle
is constrained to ±45◦with quarter-pixel precision. If image features mainly orient
45◦to 135◦, the whole image can be rotated 90◦before compression. When the pre-
cision is only a half pixel, there are 5 directional modes only. Each 8×8 block has to
be assigned with a direction mode. The directional data is coded by predicted VLC
method in the module of directional coding and sent to the decoder before coded
coefﬁcients.
In the mathematical analysis of the proposed directional DCT-like transform in
Section 8.2, we assume that all blocks in a row have the same direction. It is im-
practical in coding a real image because of local orientation properties of the image.
Therefore, in the proposed coding scheme in Figure 8.4, each block can have a unique
direction. When neighboring blocks have different directions, a question is how to
deal with direction transition in the proposed directional transform. In addition, the
direction selection is also discussed here based on the dynamic programming algo-
rithm.
8.3.1 Direction Transition on Block Boundary
To adapt to local orientation properties of the image, each block has its own transform
direction. If two blocks have different transform directions, the direction transition
will take place on the boundary of these two blocks. As shown in Figure 8.6, the
direction of each block is indicated by dashed lines. The dashed-dotted line indicates
the boundary of one block. The region that consists of fractional pixels between
two dashed-dotted lines is the transition one. The solid line in the transition region
illustrates each individual transition. The transition is assumed to always connect to
the ﬁrst column of integer pixels of the right block.
There are three different cases on transition according to our predeﬁned modes.
In the ﬁrst case, both the directions of the two blocks are positive or negative modes,
and the direction of the left block is not mode 3. The example of the direction of the

8.3 Image Coding with Proposed Directional Transform
163
(a)
(b)
(c)
Figure 8.6 Three different transition cases and their processing.

164
8 Directional DCT Transform
left block equal to mode 1 and that of the right block equal to mode 2 is shown in
Figure 8.6a. Along with the direction of the left block, each connection will be linked
to one integer pixel on the ﬁrst column of the right block. Just as the motion threading
proposed by Xu et al. [74], we call the connection a directional thread. In this case,
there only exists the one-to-one mapping during the transition. It is easy to perform
the proposed directional DCT-like transform along with each directional thread.
In the second case, both the directions of two blocks are positive modes or nega-
tive modes, and the direction of the left block is mode 3. An example of the direction
of the left block equal to mode 3 and that of the right block equal to mode 2 is shown
in Figure 8.6b. Each directional thread of the left block is extended along with its di-
rection to one of the fractional pixels in the transition region. Before it goes through
the dashed-dotted line of the right block, it is forced to link to one integer pixel on the
ﬁrst column of the right block. In this case, there exists multi-to-one mapping during
the transition. In the downward primary operation, the unique source can be found
along with directional thread, no matter whether multi-to-one mapping exists or not.
But in the upward primary operation, multiple sources may be used to modify a tar-
get according to the directional threads. Fortunately, this problem has been solved by
the EDU proposed by Feng et al. [89]. We adopt the same approach in this chapter.
In the third case, the sign of the direction mode in the left block is reverse to that
of the direction mode in the right block, or one of the direction modes in these two
blocks is zero. The example of the direction of the left block equal to mode 3 and that
of right block equal to mode –2 is shown in Figure 8.6c. In this case, these two blocks
are granted as independent. When direction threads in the left block move to the last
column of this block, they turn to the vertical direction along with the last column.
Similarly, direction threads in the right block also turn to the vertical direction when
they move to the ﬁrst column of this block. Multi-to-one mapping also exists in the
third case. They are also dealt with by the EDU approach.
8.3.2 Direction Selection
As we have discussed in Section 8.2, different rows of blocks of the proposed direc-
tional DCT-like transform are independent in mode selection. Assume that the mode
set is M = {−4,··· ,0,··· ,4}, with the total number of K candidates, which means
the predeﬁned modes as shown in Figure 8.5. The task of the mode selection is, sub-
ject to a given bit budget, to select direction modes of blocks so that the minimum
R-D cost of all these blocks can be achieved by applying the directional transform
along the select directions. This optimum problem can be formulated as
argminm0,···,mN−1∈M
N−1
∑
n=0
P(n|m0,··· ,mN−1).
(8.12)
N is the number of blocks in a row of blocks. mn is the selected mode of the n-th
block and is also used to denote the n-th block sometimes. P() denotes the R-D cost

8.3 Image Coding with Proposed Directional Transform
165
of a block. Since P() of a block is affected by not only the direction of this block but
also the directions of neighboring blocks in the same row, the optimum problem Eq.
(8.12) is not easy to be solved.
According to the predeﬁned modes in this section, the direction of a block is con-
strained to ±45◦. At the same time, if the direction mode signs of two neighboring
blocks are inverse to each other, these two blocks will be granted as independent.
Assume that there are three blocks mn−1, mn, and mn+1. Only when all signs of these
three blocks are positive or negative, the block mn−1 may inﬂuence the coding per-
formance of the block mn+1. Without loss of generality, all signs of these three blocks
are positive, namely, their directions are within (0◦,45◦]. Because the maximum di-
rection angle is 45◦, the block mn−1 will only use the pixels in the upper-left triangle
of the block mn, and the block mn+1 in the right-bottom triangle of the block mn.
Therefore, the block mn−1 does not directly use the pixels that are used by the block
mn+1. Of course, this discussion does not take the interpolation into account. But at
least it indicates the effects between mn−1 and mn+1 is not as strong as expected.
To simplify the optimum problem Eq. (8.12), we consider the R-D cost of a block
only to be inﬂuenced by neighboring left and right blocks, namely, the R-D cost of
the block mn is decided by the directions of mn−1, mn, and mn+1. With this simpliﬁ-
cation, P() is deﬁned as
P(n|mn−1,mn,mn+1) = D(n|mn−1,mn,mn+1)+λR(n|mn−1,mn,mn+1).
(8.13)
Given the three directions, D(n|mn−1,mn,mn+1) is the reconstructed distortion of the
n-th block, and R(n|mn−1,mn,mn+1) are the bits for coding coefﬁcients after quanti-
zation and direction data of the n-th block. λ is the Lagrange multiplier depending
on the quantization step.
A solution using the dynamic programming algorithm [171] is proposed to select
directions of blocks. Before we start to discuss the algorithm, some terminologies
are deﬁned here.
• mk
n indicates that the k-th candidate mode is selected by the n-th block with
0 ≤n < N and 0 ≤k < K.
• P(n|mkn−1
n−1 ,mkn
n ,mkn+1
n+1 ) is the R-D cost of the n-th block when mkn−1
n−1 , mkn
n , and
mkn+1
n+1 are given.
• ˜P(n|mkn
n ,mkn+1
n+1 ) is the minimum R-D cost of previous n + 1 blocks, i.e., b0, b1,
···, bn, where bi is the i-th block, with the directions of the boundary block mkn+1
n+1
and the last block mkn
n of this layer are given.
•
˜M(n|mkn
n ,mkn+1
n+1 ) denotes the modes assigned to the previous n blocks, i.e.,
b0,b1,··· ,bn−1, to get ˜P(n|mkn
n ,mkn+1
n+1 ).
The dynamic programming algorithm is conducted layer by layer. The basic idea
is, in the n-th layer, if the directions of the boundary block (i.e., the [n+1]-th block)
and the n-th block are given, we ﬁnd the best one from all possible mode combina-
tions of previous n blocks, that is, b0,b1,··· ,bn−1, to get ˜P(n|mkn
n ,mkn+1
n+1 ). Then the

166
8 Directional DCT Transform
triple of ˜P(n|mkn
n ,mkn+1
n+1 ), ˜M(n|mkn
n ,mkn+1
n+1 ), and (mkn
n ,mkn+1
n+1 ) is recorded. For all K2
combinations of (mkn
n ,mkn+1
n+1 ), we have K2 such triples available in the nth layer. In
the (n+1)-th layer, for a given boundary (mkn+1
n+1 ,mkn+2
n+2 ), only the recorded K2 triples
are checked instead of all combinations of the previous n + 1 blocks. The proposed
direction selection algorithm is described as follows.
1. For n = 0, ˜P(0|mk0
0 ,mk1
1 ) and ˜M(0|mk0
0 ,mk1
1 ) with 0 ≤k0 < K and 0 ≤k1 < K
are obtained by checking all K2 combinations. The K2 triples of ˜P(0|mk0
0 ,mk1
1 ),
˜M(0|mk0
0 ,mk1
1 ), and (mk0
0 ,mk1
1 ) are recorded (here ˜M(0|mk0
0 ,mk1
1 ) = φ for any
(mk0
0 ,mk1
1 ) ).
2. Given the (n−1)-th layer, for each (mkn
n ,mkn+1
n+1 ) with 0 ≤kn < K and 0 ≤kn+1 <
K, calculate
˜P(n|mkn
n ,mkn+1
n+1 ) =
argmin0≤kn−1<K

˜P

n−1|mkn−1
n−1 ,mkn
n

+P

n|mkn−1
n−1 ,mkn
n ,mkn+1
n+1

.
(8.14)
Based on the above conditional costs, merge the best ˜M(n−1|mkn−1
n−1 ,mkn
n ) under
mkn+1
n+1 to generate ˜M(n|mkn
n ,mkn+1
n+1 ), that is,
˜M(n|mkn
n ,mkn+1
n+1 ) =
n
˜M(n−1|mkn−1
n−1 ,mkn
n )∪mkn+1
n+1
o
.
(8.15)
Thus, K2 triples of ˜P (n|mkn
n ,mkn+1
n+1 ),
˜M (n|mkn
n ,mkn+1
n+1 ), and (mkn
n ,mkn+1
n+1 ) are
recorded at the n-th layer.
3. In the last layer, where n = N −1, the last K2 candidates are checked to get the
modes for all blocks.
Since the direction selection is a forward recursion processing, the proposed direc-
tional DCT-like transform has much higher computation than normal DCT especially
at the encoder side. But for the compression scenarios, the decoding computation is
more important. The decoding computation of the proposed transform is also higher
than that of normal DCT because of additional interpolation. The quantiﬁed com-
parison is given here for 1D 8-point transform. Normal DCT needs 29 additions, 12
shifts, and 13 multiplications. The proposed transform needs 29 + 29 × (T −1) ad-
ditions, 12 shifts, and 13 + 29 × T/2 multiplications for half-pel interpolation. T is
the number of taps of the interpolation ﬁlter.
8.4 Experimental Results
Extensive experiments have been conducted to evaluate the performance of the pro-
posed directional DCT-like transform. The proposed coding scheme is developed

8.4 Experimental Results
167
Barbara
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
0
0.5
1
1.5
2
2.5
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
lena
32
33
34
35
36
37
38
39
40
41
42
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
peppers
31
32
33
34
35
36
37
38
39
40
0
0.5
1
1.5
2
2.5
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
Foreman (CIF)
31
32
33
34
35
36
37
38
39
40
41
42
43
0
0.5
1
1.5
2
0
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
Photographer (256x256)
280
0.5
1
1.5
2
2.5
29
30
31
32
33
34
35
36
37
38
39
40
41
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
House (256x256)
320
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
33
34
35
36
37
38
39
40
41
42
43
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
man
30
31
32
33
34
35
36
37
38
39
40
41
0
0.5
1
1.5
2
2.5
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
boat
30
31
32
33
34
35
36
37
38
39
40
41
42
0
0.5
1
1.5
2
2.5
JPEG
Proposed transform
(9 modes)
Proposed transform
(5 modes)
Figure 8.7 The PSNR versus bpp curves of JPEG and the proposed scheme.

168
8 Directional DCT Transform
(a)
(b)
Figure 8.8 Direction modes selected in Barbara with 1.07 bpp and in Lena with 0.69 bpp.
from JPEG, but the ﬁrst 1D transform is replaced by the proposed directional DCT-
like transform. The values of fractional pixels needed are calculated by the 8-tap Sinc
ﬁlter. The direction of a block is ﬁrst predicted from the coded directions of neigh-
boring blocks and then is coded by the Huffman coding losslessly. Therefore, JPEG
is selected as an anchor for comparisons.
We report the experimental results of eight common images: Barbara (512×512),
Lena (512 × 512), Peppers (512 × 512), Photographer (256 × 256), House (256 ×
256), Man (512×512), Boat (512×512), and the ﬁrst frame of the Foreman (352×
288) video sequence from the MPEG testing set. The Lagrange multiplier λ is set
to 7 empirically. Two cases of 9 modes (quarter-pixel precision) and 5 modes (half-
pixel precision) are tested in this experiment. The PSNR versus bit per pixel (bpp)
curves are depicted in Figure 8.7, while the coding quality is set from 20 to 90 with
an incremental step 10.
The coding gain can be up to 2.0 dB for Barbara. On the testing set the gain of the
proposed directional DCT-like transform ranges from 0.2 dB to 2.0 dB, depending
on the presence or lack of orientation features in the image. But even on relatively
smooth images like Lena, the proposed method still enjoys an advantage over the
normal DCT transform. The results with different prediction precisions are also given
in Figure 8.7. The results of quarter-pixel precision can gain about 0.1 dB to 0.3 dB
better than those of half-pixel prediction on images of strong edge features such as
Barbara and Man. On relatively smooth images, the angular prediction of quarter-
pixel precision can be similar to that of half-pixel precision. The similar results are
observed in the adaptive directional lifting-based wavelet [161]. For the proposed
scheme, the quarter-pixel precision does not give us much performance gain.
The modes selected for each 8 × 8 block in Barbara with 1.0 bpp and Lena with
0.69 bpp are depicted in Figure 8.8. The contrast of the two images is adjusted to
highlight the direction modes drawn on the images, where the direction of the short

8.4 Experimental Results
169
(a)
(b)
(c)
(d)
(e)
(f)
Figure 8.9 Visual quality comparison: (a) Barbara, (c) Lena, and (e) Foreman are coded by JPEG
at 0.58, 0.46, and 0.48 bpp, respectively; (b), (d), and (f) are coded by our scheme at 0.60, 0.44, and
0.49 bpp, respectively.

170
8 Directional DCT Transform
line is equal to the transform direction. Just as we expect, the areas with strong direc-
tional texture are mostly covered with the well matched modes, such as the clothes
of Barbara and the hat of Lena.
Due to its adaptability to directional signal features, the proposed directional
DCT-like transform tends to reconstruct image edges better than the normal DCT
transform at the same bit rates. Since the human visual system is highly sensitive
to edges, the former method should have superior visual quality than the latter one,
which is indeed corroborated by our experimental results. As an example, Figure
8.9 presents the decoded Barbara, Lena, and Foreman images by the two methods.
The image decoded by JPEG has severe moir patterns on the scarf and pants, which
even change the texture orientations of the original image. Also, edge ringing effects
are clearly visible in the JPEG decoded images. In contrast, the proposed directional
transform coding eliminates most of the above artifacts. It reproduces the high fre-
quency scarf pattern almost perfectly, and greatly reduces the ringing effects around
the edges.
Tables 8.1 and 8.2 show the average number of side information bits for coding
directions, which depends on overall bit rates and image contents. One can see that
the side information bits constitute only a small portion of total coded size. The
percentage of overhead bits at low qualities is more than that at high qualities because
the constant λ is used in this experiment.
Table 8.1 Overhead bits (in bpp and percentage) for Barbara, Lena, Peppers, Man, and Boat.
Quality
Barbara
Lena
Peppers
Man
Boat
9 modes
20
0.040 6.64% 0.047 10.59% 0.047 10.58% 0.050 8.98% 0.049 9.49%
30
0.038 5.15% 0.045 8.48% 0.044 8.21% 0.048 6.97% 0.048 7.67%
40
0.035 4.17% 0.043 7.07% 0.043 6.91% 0.046 5.72% 0.047 6.50%
50
0.034 3.57% 0.040 5.88% 0.042 5.90% 0.044 4.77% 0.045 5.55%
60
0.033 3.11% 0.039 5.03% 0.040 4.95% 0.043 4.11% 0.044 4.83%
70
0.032 2.55% 0.038 4.12% 0.039 4.02% 0.041 3.40% 0.040 3.83%
80
0.031 2.00% 0.037 3.19% 0.039 3.07% 0.040 2.64% 0.037 2.90%
90
0.030 1.37% 0.036 2.01% 0.040 1.95% 0.039 1.75% 0.036 1.89%
5 modes
20
0.031 5.29% 0.036 8.45% 0.037 8.59% 0.036 6.59% 0.032 6.39%
30
0.029 4.01% 0.034 6.63% 0.035 6.69% 0.033 4.90% 0.031 4.98%
40
0.028 3.30% 0.033 5.46% 0.034 5.74% 0.031 3.88% 0.030 4.26%
50
0.027 2.79% 0.031 4.56% 0.032 4.59% 0.030 3.34% 0.029 3.61%
60
0.026 2.42% 0.030 3.88% 0.031 3.89% 0.029 2.81% 0.029 3.17%
70
0.025 1.99% 0.029 3.15% 0.030 3.06% 0.028 2.28% 0.027 2.53%
80
0.023 1.52% 0.027 2.37% 0.030 2.35% 0.027 1.75% 0.025 1.92%
90
0.023 1.04% 0.027 1.50% 0.030 1.49% 0.026 1.16% 0.024 1.24%

8.5 Summary
171
Table 8.2 Overhead bits (in bpp and percentage) for the Mandrill, Photographer, House, and Fore-
man.
Quality
Mandrill
Photographer
House
Foreman
9 modes
20
0.046 5.39% 0.055 8.88% 0.048 9.22% 0.045 9.59%
30
0.043 4.00% 0.054 7.31% 0.051 8.28% 0.043 7.70%
40
0.043 3.34% 0.053 6.31% 0.048 6.86% 0.042 6.46%
50
0.042 2.87% 0.052 5.43% 0.047 6.02% 0.041 5.62%
60
0.042 2.52% 0.051 4.75% 0.047 5.34% 0.040 4.92%
70
0.041 2.09% 0.048 3.87% 0.045 4.40% 0.039 4.06%
80
0.041 1.68% 0.046 3.01% 0.042 3.42% 0.039 3.21%
90
0.041 1.16% 0.047 2.17% 0.041 2.33% 0.039 2.18%
5 modes
20
0.030 3.52% 0.041 6.67% 0.034 6.72% 0.036 7.67%
30
0.028 2.60% 0.040 5.42% 0.035 5.90% 0.034 6.10%
40
0.028 2.18% 0.038 4.57% 0.033 4.85% 0.032 4.98%
50
0.027 1.86% 0.037 3.87% 0.033 4.32% 0.031 4.32%
60
0.027 1.64% 0.035 3.34% 0.034 3.84% 0.030 3.70%
70
0.026 1.32% 0.034 2.71% 0.031 3.07% 0.030 3.09%
80
0.027 1.11% 0.033 2.13% 0.031 2.49% 0.028 2.30%
90
0.027 0.77% 0.032 1.47% 0.029 1.62% 0.029 1.62%
8.5 Summary
In this chapter, we develop a directional DCT-like transform, which can be done
along arbitrary direction in theory. The directional transform is based on the lifting
structure of DCT by introducing directional primary operations. At the same time, a
JPEG-wise coding scheme is also developed by using the proposed directional DCT-
like transform. In the coding scheme, each 8 × 8 block can have its own transform
direction in order to better adapt to local orientation properties of the image. In this
scheme, the direction transition between block boundaries is investigated and dealt
with carefully. Furthermore, a direction selection approach is proposed based on the
dynamic programming algorithm. Experimental results show that the lifting-based
directional DCT-like transform signiﬁcantly outperforms the normal DCT transform
in terms of PSNR and visual quality.


Chapter 9
Directional Filtering Transform
9.1 Introduction
In natural images, there usually exist many textures, lines, and edges. Along the
edges, sample intensities change smoothly; but across the edges they will change
dramatically even with a small shift from the edges. This kind of correlation shows
a strong anisotropic feature because it depends upon not only the distance of sam-
ples but also their link orientation. Furthermore, the link orientation often goes in
all directions rather than only the vertical or the horizontal. Traditional transforms
such as the discrete cosine transform (DCT) and discrete wavelet transform (DWT)
cannot handle this directional correlation well and energy compaction is not fully
achieved. Therefore, many methods have been proposed to exploit the directionality
in images/intra frames in terms of transforms and predictions.
In this chapter, we mainly focus on those methods for image and intra-frame com-
pression. First, some transforms are proposed from a ﬁlter bank to well represent the
multidirectional information of images in the frequency domain [134,138,141,172–
177]. Some attempts on applying these transforms to image compression have been
reported [175–178]. The main challenge for them is that coefﬁcients after these trans-
forms hardly use existing mature quantization and entropy coding methods. On the
other hand, some methods are proposed to introduce directionality into the traditional
transforms. All these directional transforms have demonstrated a consistent and sta-
ble performance gain over their nondirectional counterparts [157,158,161,179–186],
no matter which compression schemes are used. These methods can be categorized
into two classes, according to which traditional transform is used.
One category is the directional wavelet transform. Taubman and Zakhor ﬁrst pro-
pose to adapt the ﬁltering direction to local image content by resampling each rect-
angular block along its dominant edge orientation and then perform conventional
subband transforms in the resampled domain [132]. Different from this work, some
researchers propose to use a lifting structure for local directional ﬁltering [151,153,
154, 156, 187], where all subbands reside on the rectilinear grid. Since they do not
transmit ﬁltering directions to the decoder, instead using certain synchronization
173

174
9 Directional Filtering Transform
between the encoder and the decoder, their performances are constrained. No sig-
niﬁcant objective improvements are reported.
However, with direction information estimated at the encoder and explicitly trans-
mitted to the decoder, both the curved wavelet transform [179,180] and the adaptive
directional lifting (ADL) [157, 161] report good objective and subjective perfor-
mance over JPEG 2000. To improve the interpolation accuracy in ADL, Liu and
Ngan [181] and Dong et al. [182] propose adaptive interpolation ﬁlters. Further,
Chang and Girod [183] propose a direction-adaptive DWT with long-distance pre-
diction to avoid interpolation and also demonstrate its advantages over ADL. With-
out being limited by rectilinear subsampling, Chang et al. also propose direction-
adaptive lifting by quincunx subsampling [158].
Another category considers the directionality in DCT transform. Zeng and Fu
[184] propose a block-independent directional DCT from the idea of shape-adaptive
DCT. Xu et al. [185] introduce directional primary operations into a lifting-based
DCT, which exploits inter-block correlations, too, but it has to estimate prediction
directions of all blocks jointly by dynamic programming. A more recent work by
Chang et al. incorporate the idea of directional DCT into H.264 intra-frame coding
[13] and propose the direction-adaptive partitioned block transform [186].
In addition to directional transforms, H.264 intra-frame coding utilizes directional
prediction to exploit directional correlation, where a block is unidirectionally pre-
dicted from reconstructed boundary samples in its neighboring blocks along a certain
direction. However, the correlation among samples is not fully exploited yet since
samples close to neighboring blocks tend to have better prediction than the others.
Many schemes have been proposed recently to improve the performance, such as
bidirectional prediction [188] and template matching [189].
In these 2D directional transforms, since every 1D transform is often performed
neither horizontally nor vertically, it is difﬁcult to prove the equivalence on the or-
ders of two 1D transforms ﬁrst horizontal and then vertical, and vice versa. Chang et
al. [158] report that the order of two 1D transforms results in different coding perfor-
mances for a 2D transform. Chang and Girod [183] also point out this problem in the
coding gain analysis. However, it is still not clear what exact roles the 1D transform
order plays in 2D directional transforms if the best nonseparable prediction ﬁlters are
chosen.
In this chapter, the mathematical analysis based upon an anisotropic image model
is ﬁrst presented for this problem. The analysis results show that, if the neighbor-
ing samples used to predict one pixel can angularly cover the whole plane and the
best weights based upon the mean square error (MSE) criterion are provided in each
prediction, lifting in either order brings no difference. But in practice, the conditions
cannot be satisﬁed because high-pass bands are often simply decomposed by conven-
tional lifting or are not decomposed at all. Also the prediction modes cannot cover as
wide an angular scope as supposed. Therefore, the lifting order problem for images
with different orientations and sharpness exists.
Subsequently, we propose the directional ﬁltering transform (dFT, in order to dis-
tinguish from the common usage on DFT) to better exploit image and intra-frame
correlations in H.264 intra-frame coding. Some preliminary results are reported by

9.2 Adaptive Directional Lifting-Based 2D Wavelet Transform
175
Peng et al. [190]. dFT consists of directional ﬁltering and an optional DCT. In the
directional ﬁltering, there are two different modes. One is the unidirectional ﬁl-
tering (UDF), similar to H.264 intra prediction. Another is the bidirectional ﬁlter-
ing (BDF), which is inspired by ADL but without an update. According to our anal-
yses, an important feature in dFT is that every block can adaptively select its lifting
order, which is difﬁcult to achieve in directional wavelet transforms. Since dFT not
only enables closer neighboring samples for prediction but also well exploits the di-
rectional correlation, it can achieve better performance than the current intra-frame
coding in H.264.
9.2 Adaptive Directional Lifting-Based 2D Wavelet Transform
The ADL locally adapts the ﬁltering direction to image orientations within a lifting-
based DWT framework and still preserves the rectangular shape of subbands. Like
traditional DWT, it consists of 2D lifting processes in one-level decomposition. With-
out loss of generality, we suppose that lifting in the vertical dimension is performed
ﬁrst. By lifting in the ﬁrst dimension along a certain direction, the L and H bands are
generated. Afterward, another lifting in the horizontal dimension is performed on the
L and H bands, respectively, resulting in the ﬁnal LL, LH, HL, and HH subbands.
Different from conventional DWT, the ﬁltering directions in the two dimensions may
not be orthogonal and are not limited to the vertical or horizontal directions only.
Let x[t], t = [t1,t2] denote the original image signal. In the ﬁrst dimension lifting,
odd-row pixels are predicted from even rows along an angle d. Then even-row sam-
ples are updated from the odd-row residues along the same direction d. Though any
biorthogonal wavelet transform can be factored into one or several lifting stages, we
only consider one-step lifting here, which means one prediction step followed by one
update step and scaling. Then the lifting process can be expressed as
y1 [t] = gH ·

x1 [t]−Pd
x0 [t]

,
(9.1)
y0 [t] = gL ·

x0 [t]+g−1
H ·Ud
y1 [t]

,
(9.2)
where x0[t] is the even-row samples and x1[t] the odd-row samples. Pd
x0[t] is the pre-
diction for x1[t] and Ud
y1[t] is the update for x0[t] . y0[t] and y1[t] denote the L and H
bands, respectively. gH and gL are scaling factors.
Since the ﬁltering can adapt to image orientations, samples across multiple
columns may be used in the prediction step, but only even-row samples are avail-
able to predict odd-row samples. Thus, the prediction and update function for the 5/3
wavelet, which is used throughout this chapter, can be expressed as
Pd
x0[t] = 1
2
1
∑
i=0
L−1
∑
k=0
ωi,kx0

t1 +△t1i,k,t2 +△t2i,k

,
(9.3)
Ud
y1[t] = 1
4
0
∑
j=−1
L−1
∑
l=0
ωj,ly1

t1 +△t1 j,l,t2 +△t2 j,l

,
(9.4)

176
9 Directional Filtering Transform
Figure 9.1 Modes in two dimensions for ADL with lifting in the vertical dimension performed ﬁrst:
(a) the ﬁrst dimension; (b) the second dimension. The black circles denote the pixel to be predicted
and the white circles denote samples used for prediction. Interpolation is needed where there is no
sample at all.
where [△t1i,k,△t2i,k] is the displacement vector between the pixel used for prediction
and x0[t]. [△t1 j,l,△t2 j,l] is for the update stage. L is the number of samples used for
the prediction of each i. ωi,k and ωj,l are weighting factors.
After the ﬁrst dimension lifting, the second is similarly performed on the L and
H bands, producing four subband signals: z00[t], z01[t] , z10[t], and z11[t]. They cor-
respond to the LL, LH, HL, and HH subbands, respectively. Until now, one-level
decomposition is completed. The multilayer wavelet structure can be obtained by
iteratively performing 2D ADL on the LL subband.
Since ADL is operated on a global basis, a ﬁxed lifting order, vertical lifting ﬁrst
followed by horizontal lifting, is chosen to guarantee the rectangular shape of each
subband and the convenience for cross-block processing. However, this will hurt the
performance. If a block has an orientation near horizontal, the vertical lifting cannot
be just performed along the block orientation (seeing all possible prediction modes
for two dimensions in Figure 9.1) and consequently energy is spread into the high-
pass band. In this case, it may favor a horizontal lifting ﬁrst. Considering the noneven
distribution of modes in ADL, Chang et al. introduce a more evenly distributed set
of modes in each dimension by using samples far away [183]. Since these modes
can angularly cover almost the whole plane, insensitiveness to image transposition
is achieved despite a ﬁxed lifting order. However, the long prediction distance de-
creases the correlation and deteriorates the prediction accuracy. Thus, it is only use-
ful for images with sharp edges. Moreover, there is no exact mode to handle the right
horizontal direction.
So the question that arises here is, what is the effect of the order of two 1D
transforms in a one directional 2D transform? In the next section, we try to answer
this question by analyzing the coding gain of the directional 2D transform on an
anisotropic image model.
9.3 Mathematical Analysis
In this section, the coding gain for an adaptive directional lifting-based wavelet sim-
ilar to that by Chang and Girod [183] is ﬁrst presented in terms of power spectral
density (PSD). Afterward, numerical results will be given to analyze the effect of the
lifting order.

9.3 Mathematical Analysis
177
9.3.1 Coding Gain of ADL
First, we analyze the coding gain of lifting in the vertical dimension. It consists of
split, prediction, update, and scaling steps [see Eq. (9.1) and Eq. (9.2)]. The process
is rewritten as
y1[t] = gH ·

x1[t]−hd
P[t]∗x0[t]

,
(9.5)
y0[t] = gL ·

x0[t]+g−1
H ·

hd
U[t]∗y1[t]

,
(9.6)
where hd
P[t] and hd
U[t] are prediction and update ﬁlters, respectively, on the down-
sampled grid of x[t].
To simplify the computation, we extend related signals onto the original image
sampling grid x[t]. Let ˆx0[t] denote the up-sampled even-row samples and ˆhd
P[t] the
ﬁlters applied to ˆx0[t]. According to Eq. (9.3) and Eq. (9.4), we get
ˆhd
P[t] =
( ωi,k
2 ,t =
h
△t1i,k,2△t2i,k +1
i
,k = 0,1,...,L−1
0
.
(9.7)
Its Fourier transform is
ˆHd
P
 ejω
= 1
2
1
∑
i=0
L−1
∑
k=0
ωi,ke−j

△t1i,kω1+

2△t2i,k+1

ω2

.
(9.8)
Since hd
P[t]∗x0[t] can be obtained by convoluting ˆx0[t] with ˆhd
P[t], shifting the result-
ing signal by -1 along the vertical dimension and then down-sampling it, we get the
transfer function of hd
P[t] as
Hd
P
 ejω
= ejω2/2 ˆHd
P

ejω1,ejω2/2
.
(9.9)
Similarly
Hd
U
 ejω
= e−jω2/2 ˆHd
U

ejω1,ejω2/2
= 1
2e−jω2/2 ˆHd
P

e jω1,ejω2/2
.
(9.10)
Assume that the image x[t] comes from a 2D stationary continuous random ﬁeld ˜x
with zero mean and a variance σ2. Similar to the work of Chang and Girod [183],
we model the anisotropic auto-correlation as
R˜x˜x(τ) = E{˜xt ˜xt−τ} = σ2e−
√
τTΓ τ,
(9.11)
where Γ =

cosϕ sinϕ
−sinϕ cosϕ
 Ω2
α
0
0 Ω2
β

cosϕ sinϕ
−sinϕ cosϕ
T
, Ωα > Ωβ > 0. In this
model, contours of equal correlation form an ellipse with the major axis aligned
to the edge direction. ϕ denotes the orientation of the minor axis. Ωα and Ωβ are

178
9 Directional Filtering Transform
parameters related to the minor and major axes, respectively. The larger Ωα is, the
faster the correlation orthogonal to the edge direction decays; while the smaller Ωβ
is, the slower the correlation along the edge direction declines. By Fourier transform,
we get the PSD of ˜x as
˜X˜x˜x(Ω) = 2πσ2
ΩαΩβ
 1+ΩT ¯Γ Ω
−3
2 ,
(9.12)
where Ω= (Ω1,Ω2)T and ¯Γ =

cosϕ sinϕ
−sinϕ cosϕ
 Ω−2
α
0
0
Ω−2
β

cosϕ sinϕ
−sinϕ cosϕ
T
.
For the discrete signal x[t], its PSD is
Xxx
 ejω
=
B
∑
i=−B
B
∑
j=−B
˜X˜x˜x ((ω1 −2iπ) f1,(ω2 −2 jπ) f2),
(9.13)
where B is a band-limit parameter. f1 and f2 are the sampling frequencies in the
horizontal and vertical directions, respectively.
Based upon this image model, we can deduce the PSD of y1[t] and y0[t] according
to their relation to x[t] in Eq. (9.5) and Eq.(9.6). That is
Y d
y1y1
 ejω
= g2
H
 
1+|Hd
P
 e jω
|2
Xx0x0
 e jω
−2Hd
P
 ejω
Xx0x1
 e jω
,
(9.14)
Y d
y0y0
 ejω
= g2
L
 
|1−Hd
U
 ejω
Hd
P
 e jω
|2 +|Hd
U
 ejω
|2
Xx0x0
 ejω
+2

1−Hd
U
 ejω
Hd
P
 ejω
Hd∗
U (ejω)Xx0x1
 ejω
,
(9.15)
where
Xx0x0
 ejω
= 1
2
1
∑
k=0
Xxx

ejω1,ej((ω2/2)+kπ)
,
Xx0x1
 ejω
= 1
2
1
∑
k=0
(−1)k e−j(ω2/2)Xxx

e jω1,e j((ω2/2)+kπ)
.
Substituting Eq. (9.9) and Eq. (9.10) into Eq. (9.14) and Eq. (9.15), we get
Y d
y1y1(ejω) = 1
2g2
H
1
∑
k=0
h
1−(−1)k ˆHd
P(ejω1,ejω2/2)
i2
·Xxx(ejω1,ej(ω2/2+kπ)),
(9.16)

9.3 Mathematical Analysis
179
Y d
y0y0(ejω) = 1
2g2
L
1
∑
k=0

1+(−1)k 1
2
ˆHd
P(ejω1,ejω2/2)−1
2

ˆHd
P(ejω1,ejω2/2)
22
·Xxx(ejω1,ej(ω2/2+kπ)),
(9.17)
where the scaling factors normalizing the energy of the synthesis ﬁlters are computed
as
g2
L = Aver
h
1+ ˆHd
P
 ejωi2
,
(9.18)
g2
H = Aver
 
1−1
2
ˆHd
P
 ejω
−1
2

ˆHd
P
 ejω22!
.
(9.19)
Aver(·) = (1/4π2)
R R π
−π(·)dω1dω2 is an integral function.
Thus, the variances of y1[t] and y0[t] are
σ2
y1 = Aver

Y d
y1y1
 ejω
= g2
HAver
h
1−ˆHd
P
 ejωi2
Xxx
 ejω
,
(9.20)
σ2
y0 = Aver

Y d
y0y0
 ejω
= g2
LAver
 
1+ 1
2
ˆHd
P
 ejω
−1
2

ˆHd
P
 ejω22
Xxx
 e jω
!
.
(9.21)
Assuming high-rate scalar quantization and the independence of quantization errors
in two bands, the coding gain of a one-dimensional ADL can be deﬁned as that by
Taubman and Marcellin [130]
GADL =
σ2
q
σ2y0σ2y1
.
(9.22)
Although ADL cannot be orthonormal, this deﬁnition can still be applied due to the
normalization of synthesis ﬁlter energy.
The process can be easily extended to the 2D case, where a second-dimension
lifting is again performed on y0[t] and y1[t] before scaling, generating four subband
signals z00[t], z01[t], z10[t], and z11[t]. It can be expressed as
y1[t] = x1[t]−hd1
P [t]∗x0[t],
(9.23)
y0[t] = x0[t]+hd1
U [t]∗y1[t],
(9.24)
z01[t] = g01

y01[t]−hd2
P [t]∗y00[t]

,
(9.25)

180
9 Directional Filtering Transform
z00[t] = g00

y00[t]+g−1
01

hd2
U [t]∗z01[t]

,
(9.26)
z11[t] = g11

y11[t]−hd3
P [t]∗y10[t]

,
(9.27)
z10[t] = g10

y10[t]+g−1
11

hd3
U [t]∗z11[t]

.
(9.28)
Above d1 is the prediction direction vector for the ﬁrst dimension lifting and d2
and d3 for the second dimension lifting performed on the low and high pass bands,
respectively. y00[t] and y01[t] are even and odd columns of y0[t]; y10[t] and y11[t] are
that of y1[t]. hd2
P [t] and hd2
U [t] are prediction and update ﬁlters on the low-pass signal
in the second dimension; and are that on the high-pass signal. g00, g01, g10, and g11
are scaling factors which normalize the energy of four synthesis ﬁlters.
Similarly, we can compute the four subband variances as
σ2
z01z01 = Aver

Gd2
1
 ejω1,ej2ω2
Gd1
0
 ejω
·Aver

Hd2
1
 ejω1,ej2ω2
Hd1
0
 ejω
Xxx
 ejω
,
(9.29)
σ2
z00z00 = Aver

Gd2
0
 ejω1,ej2ω2
Gd1
0
 ejω
·Aver

Hd2
0
 ejω1,ej2ω2
Hd1
0
 ejω
Xxx
 ejω
,
(9.30)
σ2
z11z11 = Aver

Gd3
1
 ejω1,ej2ω2
Gd1
0
 ejω
·Aver

Hd3
1
 ejω1,e j2ω2
Hd1
1
 ejω
Xxx
 ejω
,
(9.31)
σ2
z10z10 = Aver

Gd3
0
 ejω1,ej2ω2
Gd1
1
 ejω
·Aver

Hd3
0
 ejω1,e j2ω2
Hd1
0
 ejω
Xxx
 ejω
,
(9.32)
where
Hd
0
 e jω
=

1+ 1
2
ˆHd
P
 ejω
−1
2

ˆHd
P
 ejω22
,
Hd
1
 ejω
=
h
1−ˆHd
P
 ejωi2
,
Gd
0
 ejω
=
h
1+ ˆHd
P
 ejωi2
,
Gd
1
 ejω
=

1−1
2
ˆHd
P
 ejω
−1
2

ˆHd
P
 ejω22
.
The coding gain then is deﬁned as
GADL =
σ2
4q
σ2z01z01σ2z00z00σ2z11z11σ2z10z10
.
(9.33)

9.3 Mathematical Analysis
181
9.3.2 Numerical Analysis
In this section, all schemes are analyzed from their coding gains when one-level
decomposition is considered. If vertical lifting is performed ﬁrst, x[t] is ﬁrst decom-
posed into L and H bands in the vertical dimension and then a horizontal dimension
lifting is performed on the L and H bands, and vice versa. Hereafter, we represent
the two cases as VF and HF, respectively.
In order to examine the impact of the lifting order independent of the angular pre-
cision and concrete mode settings, we choose the adaptive weighting based approach
for analysis. It ﬁnds the best weighting factors of samples used for prediction by min-
imizing the prediction MSE under high rate condition. From another viewpoint, it is
equal to maximizing the coding gain since the high-pass variance is minimized and
the update brings little change to the low-pass variance. According to Eq. (9.1) and
Eq. (9.2), this problem can be formulated as





D = E
"
x1[t]−1
2 ∑1
i=0 ∑L−1
k=0 ωi,kx0
h
t1 +△t1i,k,t2 +△t2i,k
i2#
,
ωi,k = argminD,ω0,k = ω1,k.
(9.34)
It can be solved by the Wiener-Hopf equation based upon the correlation model in
Eq. (9.11).
As mentioned in Section 9.2, different lifting orders can affect the performance of
ADL for its nonevenly distributed modes, which cover only part of the whole plane.
Lifting performed ﬁrst in the vertical dimension will be beneﬁcial for images with a
near-vertical orientation rather than ones near-horizontal. To simulate this condition,
samples denoted by black circles in Figure 9.2 are used to predict the pixel denoted
by the gray circles in the VF case. For the HF case, similar samples are adopted.
Obviously, the prediction direction covers the same range as ADL in two dimen-
sions. We have drawn the coding gain curves in the two cases for x with different
orientations and sharpness in Figure 9.3, based upon the coding gain deﬁnition in
Eq. (9.33) and the image model. The horizontal axis represents the orientation of the
signal x with ϕ varying from −45◦to 135◦and the vertical axis denotes the maxi-
mized coding gain. At each ϕ, the prediction weighting factors in both dimensions
are recomputed according to Eq. (9.34) for adaptation. According to the statistical
results on practical images by Chang and Girod [183], we set Ωβ/Ω0 to 0.4, where
Ω0 is a normalization parameter related to (f1, f2). With ﬁxed Ωβ/Ω0, the sharpness
of images is determined only by Ωα/Ω0. The larger Ωα/Ω0 is, the sharper the image
is. In our analysis, Ωα/Ω0 is set to 1, 2, 4, and 8 for comparison.
From Figure 9.3, we can see when the angular scope of one-dimension lifting is
conﬁned to that between the two dashed lines, ADL with different lifting orders per-
forms quite differently. For VF, it is superior to HF ranging from −45◦to 45◦while it
is worse than HF at other orientations. This is consistent with our supposition, since
ϕ corresponds to the direction with sharp intensity transitions. With the decrease of

182
9 Directional Filtering Transform
Figure 9.2 Pixels used in the prediction step of the VF case for lifting order analysis with adaptive
weighting; (a) the ﬁrst dimension, (b) the second dimension. The gray circles denote the pixel to be
predicted on odd rows. The black circles residing on the even rows are samples used for prediction.
Figure 9.3 Lifting order analysis of ADL. The three solid lines correspond to the VF case with
Ωα/Ω0 = 8, 2, 1, respectively, from the bottom to top and the three dashed lines denote the corre-
sponding coding gain curves for the HF case.
Ωα/Ω0, the image becomes smooth. Therefore, coding gain at different orientations
varies little and the difference between VF and HF vanishes gradually.
As shown previously, we consider the case that the directional scope of each lift-
ing is limited and show that it is sensitive to image transposition. Then a natural
question will be: how do they perform if lifting can cover almost the whole plane?
Hence, a lifting order analysis based upon more samples in prediction and a wider
angular scope (see Figure 9.4) is presented. In Figure 9.4, only ﬁrst dimension lifting
is depicted for VF. The similar samples are used for the second dimension and the
HF case. The coding gain curves are depicted in Figure 9.5. It can be observed that
VF and HF perform almost the same at different Ωα/Ω0. When it equals 8, there are
two peaks between 0◦and 45◦in VF, which correspond to d′
1 and d′
2 in Figure 9.4 and
there are also two peaks between 45◦and 90◦along d′
3 and d′
4, respectively. In other
words, the coding gain reaches its local maximum when there is an integer pixel re-
siding at the even rows of the orthogonal sampling grid along the image orientation.

9.3 Mathematical Analysis
183
Figure 9.4 Samples used in the prediction step of the VF case for lifting order analysis with more
samples and a wider angular scope. The gray circle denotes the pixel to be predicted on odd rows.
The black circles residing on the even rows are samples used for prediction.
Figure 9.5 Lifting order analysis of ADL with more pixels for prediction. The four solid lines
correspond to the VF case with Ωα/Ω0 = 8, 2, 1, respectively, from the bottom to top and the four
dashed lines denote the corresponding coding gain curves for the HF case.
This validates the effectiveness of the mode setting by Chang and Girod [183], where
d′
2 does not exist. However, this is only the case when the edge is extremely sharp.
When it becomes smooth, weighting factors fall on several nearer pixels at 26.6◦in
the ﬁrst dimension of the VF case and adaptive interpolation [181,182] takes effect.
Anyhow, this experiment indeed illustrates, if the lifting in each dimension can cover
almost the whole plane, the lifting order will make little sense.
All the previously mentioned analyses consider that the high-pass bands are de-
composed as fully as the low-pass bands. However, in practical compression systems
the high-pass residues are usually no longer decomposed if the transform complex-
ity is taken into account. Without further decomposition on high-pass bands, there

184
9 Directional Filtering Transform
Figure 9.6 Lifting order analysis of ADL without decomposing the high-pass bands. The three
solid lines correspond to the VF case with Ωα/Ω0 = 8, 2, 1, respectively, from the bottom to top
and the three dashed lines denote the corresponding coding gain curves for the HF case.
sometimes exist some large coefﬁcients in these bands, thus, hurting the perfor-
mance. The coding gain comparisons for the case that high-pass bands are not de-
composed are depicted in Figure 9.6, where lifting in the ﬁrst dimension dominates
the performance. Now the VF curves apparently differ from the HF curves, and
which is better depends upon both the image orientation and its sharpness. For the
vertical direction, lifting in the vertical dimension should be performed ﬁrst because
the correlation among rows is much stronger than that among columns. For diagonal
directions, the two cases perform the same since they are symmetrical relative to the
diagonal lines. But, at half-pel directions, horizontal lifting performs better when the
edge is sharp because there is an integer pixel “a” on the right column of the predicted
pixel “A” with the strongest correlation, as shown in Figure 9.4. With the decrease
of Ωα/Ω0, the correlation of “b” and “c” with “A” becomes stronger and dominates
the directional row correlation. Therefore, we see that VF becomes superior to HF
when Ωα/Ω0 equals 1. From these, we can conclude that directionality shows more
importance when the edge is sharp; while the distance is more important when it
is smooth. The coding gain curves for the cases with or without decomposing the
high-pass bands are also depicted in Figure 9.7. Here the lifting order is adaptively
selected. It demonstrates, without decomposing the high-pass bands, that almost the
same performance can be achieved with an adaptive lifting order. In practice, it can
save both bits and computations since there is no need to transmit its modes to the
decoder.
According to the analyses in this section, it can be concluded that the adaptive lift-
ing order can compensate the shortcomings brought by nonevenly covered or long-
distance modes and bring us better performance. Although the analyses are theoret-
ical and the absolutely adapted weighting factors for prediction cannot be achieved
in practice, it indeed gives us a qualitative study of the compression performance

9.4 Directional Filtering Transform
185
Figure 9.7 Comparison of adaptive lifting order based ADL with or without decomposing the high-
pass band. The four solid lines correspond to the case that the high-pass bands are decomposed with
adaptive prediction ﬁlters where Ωα/Ω0 = 8, 2, 1, respectively, from the bottom to top; the four
dashed lines denote the corresponding coding gain curves for the case that the high-pass bands are
not decomposed at all.
inﬂuenced by different lifting orders. These analyses motivate us to develop our dFT
compression scheme, which chooses an evenly distributed set of modes with an adap-
tive lifting order and short-distance prediction.
9.4 Directional Filtering Transform
In this section, the directional ﬁltering transform (dFT) allowing an adaptive lifting
order is proposed to exploit intra-frame correlations more fully. It overcomes the
disadvantage of directional intra prediction in H.264 by allowing bidirectional pre-
diction from the neighborhood, which may be samples from not only neighboring
blocks but also the current block.
9.4.1 Proposed Intra-Coding Scheme
The proposed intra-coding scheme is depicted in Figure 9.8. There are two ways to
exploit intra-frame correlations. One is that the input image is predicted by UDF
from neighboring block boundaries. After that, residues are decorrelated by a DCT
transform and then quantized with a uniform quantizer. Another is to predict the in-
put image by BDF, which possesses evenly distributed ﬁltering modes by which the
lifting order can be adaptively selected. After the ﬁltering, two choices are presented.
If the ﬁltered residues exhibit an isotropic correlation, a DCT-like transform is per-
formed on them to further decorrelate them and the resulting coefﬁcients are then

186
9 Directional Filtering Transform
Figure 9.8 Proposed intra-coding scheme. T denotes the DCT-like integer transform in H.264 and
Q denotes scalar quantization.
quantized and coded; otherwise the transform is skipped and residues are directly
quantized and entropy coded. Rate distortion optimization is adopted to make this
decision. The BDF, UDF, and optional transform are generically referred to as dFT.
9.4.2 Directional Filtering
x is a zero-mean 2D random signal representing an image block (N +M)×(N +M).
{xt | t = (t1,t2),t1 = M,··· ,M +N −1,t2 = M,··· ,M +N −1} is the current block
N ×N to be coded. The samples in the ﬁrst M columns and rows are the reconstructed
boundaries of neighboring blocks. {yt | t = (t1,t2),t1 = M,··· , M + N −1,t2 =
M,··· ,M + N −1} is the residue block after ﬁltering. In the following discussion,
the case of M = 1 and N = 4 is considered.
The BDF is discussed ﬁrst. To locally adapt to the orientations of images/intra
frames, eight directional modes are adopted, which are the same as that in H.264
intra-frame coding (see Figures 9.9 and 9.10). Along each direction, there are three
ﬁltering steps within one-level decomposition to explore both intra-block and inter-
block correlations.
For modes V0 −V4, a vertical ﬁltering is performed ﬁrst on odd rows denoted by
black circles in Figure 9.9a, leaving even rows unprocessed and denoted as ˆy1[t]. This
process corresponds to the vertical lifting without an update in ADL and, thus, it can
be expressed as

y1[t] = x1[t]−hd
P[t]∗x0[t].
ˆy1[t] = x0[t].
(9.35)
The prediction ﬁlter hd
P[t] is a two-point average ﬁlter along d for integer-precision
modes such as V0, V3, and V4 in Figure 9.9a. For half-precision modes V1 and V2,

9.4 Directional Filtering Transform
187
Figure 9.9 Three-step ﬁltering in dFT with vertical ﬁltering ﬁrst. (a) First step. (b) Second step. (c)
Third step.
Figure 9.10 Three-step ﬁltering in dFT with horizontal ﬁltering ﬁrst. (a) First step. (b) Second step.
(c) Third step.
a 4-tap interpolation ﬁlter is used before prediction. At block boundaries, recon-
structed pixels of neighboring up, left or left-up blocks are also used for prediction,
which are denoted as crosses in Figure 9.9.
After the ﬁrst step, horizontal ﬁltering is performed on the odd columns of ˆy1[t],
which are denoted as gray circles in Figure 9.9b. It is expressed as

y2[t] = y11[t]−hd′
P [t]∗y10[t].
ˆy2[t] = ˆy10[t].
(9.36)
ˆy10[t] is the down-sampled block of ˆy1[t] by keeping even columns and ˆy11[t] the
odd columns. y2[t] is the ﬁltered residues and ˆy2[t] the even columns of ˆy1[t]. d′ is
the prediction direction for this ﬁltering. Orientation is considered as unchanged in
up-sampled ˆy1[t] and therefore d′ should be along the same direction as d. At V3 and
V4, this rule holds and it also needs to interpolate within even columns. But at V0, no
sample exists along the vertical direction. The prediction is then operated along the
horizontal direction at V0 for closer distance and corresponding strong correlation.
For the third step, ˆy2[t] is unidirectionally predicted from the reconstructed bound-
aries of neighboring blocks along the same orientation as d. The predicted residual
signal y3[t] then is expressed as
y3 (t1,t2) =

ˆy2 (t1,t2)−x(2(t1 −t2 tanθ),0),if t1 ≥t2 tanθ,
ˆy2 (t1,t2)−x(0,2(t2 −t1/tanθ)),if t1 < t2 tanθ,
(9.37)

188
9 Directional Filtering Transform
t1,t2 = 1,2,··· ,N/2, where θ is the angle of prediction direction and the vertical
axis in Figure 9.9.
For the modes H0−H2, the horizontal ﬁltering is performed ﬁrst, followed by
the vertical lifting and unidirectional prediction (see Figure 9.10). With the adap-
tive lifting order, BDF provides us with an evenly distributed set of modes covering
the whole plane. If the image block exhibits a strong vertical correlation, lifting in
the vertical dimension is performed ﬁrst, and vice versa. Therefore, the directional
correlation can be fully exploited.
As previously mentioned, we take one-level lifting as an example. The hierarchi-
cal structure can be obtained by iteratively performing two dimensional lifting on the
remaining samples, such as ˆy2[t], before the third prediction step like that in ADL.
Moreover, all residues are also scaled to preserve the energy of quantization noise
in the reconstructed image. In our experiment, these scaling factors are computed
numerically to normalize the synthesis ﬁlters.
The other ﬁltering method is UDF, which can be similarly expressed as
y(t1,t2) =

x(t1,t2)−x(t1 −t2 tanθ,0),if t1 ≥t2 tanθ,
x(t1,t2)−x(0,t2 −t1/tanθ),if t1 < t2 tanθ,
(9.38)
t1,t2 = 1,2,··· ,N/2. Here we only give a simpliﬁed expression. The weighted pre-
diction and DC prediction are also applied to ensure prediction accuracy and accom-
modate smooth blocks.
With these two ﬁltering modes, the directional correlation can be effectively ex-
ploited. Rate-distortion optimization (RDO) is adopted to choose between them and
1-bit ﬂag is used to indicate which mode is selected. At the entropy coding stage,
these ﬂags are arithmetically coded by using neighboring ones as the context.
9.4.3 Optional Transform
After the ﬁltering, there are two ways to process the residues. If there is still correla-
tion among them, a 2D transform is performed to further decorrelate them; otherwise
the transform is skipped and residues are directly quantized and entropy coded.
Similar to that by Ye and Karczewicz [188], a nonseparable directional transform
derived from the Karhunen-Lo`eve transform (KLT) is adopted for blocks at each
orientation. Each residue block is ﬁrst divided into 4×4 blocks and then optionally
transformed with its corresponding transform matrix. Let y denote the 1×16 vector
of one 4×4 residue block, the transform is given as
z = T ′y.
(9.39)
T is the 4 × 4 transform matrix with each column representing the basis vector.
Residue blocks of a data set with the same orientations and block types are gathered
to train the matrix T. After the transform, the coefﬁcients z are scalar quantized and
then entropy coded. Separate context models are used for different types of blocks.

9.5 Experimental Results
189
If there is no need to perform the transform, the quantization method that has been
integrated into the Key Technical Area (KTA) software [191] is adopted to quantize
the residues. Quantized coefﬁcients are then scanned row by row for simplicity and
entropy coded. Since the residuals still preserve some spatial features, the signiﬁcant
map is coded by taking the spatial correlations into account, where the left and upper
signiﬁcant bits are taken as the context for the current bit.
In order to determine whether the transform is performed or not, the rate-distortion
optimization in H.264 is modiﬁed and used as the criterion. For the 4 × 4 mode, a
transform is always performed at each 4×4 block so there are no overhead bits. For
the 8×8 and 16×16 modes, the transform can be selected for each 4×4 block and,
thus, there is one bit for each 4×4 block. All these bits are coded by context-adaptive
arithmetic coding.
9.5 Experimental Results
In our experiment, H.264 intra-frame coding (JM14.1 High Proﬁle) is taken as the
benchmark. Experiments for dFT are performed across 4 × 4, 8 × 8, and 16 × 16
blocks. At 16×16, a two-level ﬁltering is performed for BDF with the same ﬁltering
directions, while at 4 × 4 and 8 × 8 one-level ﬁltering is performed for each block.
The eight ﬁltering directions are explicitly coded as with that of H.264 4 × 4 intra
mode. Context adaptive binary arithmetic coding (CABAC) is chosen at the entropy
coding stage.
The comparisons between dFT and H.264 intra coding are depicted in Figure 9.11.
“H.264 Intra” denotes the intra coding of H.264 and “dFT” denotes the proposed
scheme with two ﬁltering modes: UDF and BDF. One can see that dFT shows its
superiority especially on images with rich edges. There is up to a 1 dB gain for
Foreman, 0.4 dB for Barbara, 1.8 dB for Spoke, 1.2 dB for Straw, and 0.1 dB for Lena
and Monarch. In order to examine the roles that two ﬁltering modes play, experiments
on ﬁxed block sizes are also carried out.
The performances of dFT on 16 × 16 blocks only are depicted in Figure 9.12.
“H.264 16” denotes the intra coding of H.264 on 16×16 blocks, which is equivalent
to the UDF mode of dFT. “BDF 16” denotes the dFT scheme with only the BDF
mode being enabled. And “dFT 16” denotes the dFT scheme with the two ﬁltering
modes BDF and UDF both being enabled. We can see, when the block size is large,
dFT shows a strong superiority to H.264 intra prediction by combination of the two
ﬁltering modes. Moreover, BDF performs much better than H.264 intra prediction
on 16 × 16 blocks since it decreases the prediction distance by predicting from the
neighborhood rather than the boundaries of neighboring blocks. On the other hand,
the intra prediction in H.264 can compensate the disadvantages of BDF too, such as
poor interpolation on blocks with sharp edges, weak prediction at V0 in the second-
step ﬁltering (see Figure 9.9b) on blocks with sharp vertical edges, and lack of inter-
band correlation exploration. Also the DC prediction in H.264 intra coding followed
by a DCT-like integer transform and a subsequent Hadamard transform can give us

190
9 Directional Filtering Transform
Figure 9.11 Experimental results for Foreman (176×144), Barbara (512×512), Lena (512×512),
Spoke (512 × 512), Monarch (768 × 512), and Straw (512 × 512). The dashed curves denote the
performance of H.264 intra coding and the real ones denote that of the proposed dFT.
Figure 9.12 Experimental results for Foreman (176 × 144) and Spoke (512 × 512) on 16 × 16
blocks.
a compact representation of smooth blocks with little directionality, which further
brings beneﬁts to H.264 intra prediction. Therefore, both of the two ﬁltering modes
have their advantages and the combination of BDF and UDF can well exploit intra
frame/image correlations. There is up to a 2.4 dB gain for Foreman and a 3.8 dB gain
for Spoke with rich edges.
The evaluation of H.264 and dFT on 8×8 blocks only is also depicted in Fig-
ure 9.13. Now the performance gap between “H.264 8” and “BDF 8” diminishes
and the two ﬁltering modes show comparable performance on Foreman, since the
block size is smaller. But by combination of BDF and UDF, dFT can still outperform
H.264 intra prediction by up to 0.8 dB on Foreman and 2.2 dB for Spoke. While for
4 × 4 blocks (see Figure 9.14), the H.264 intra prediction can perform well enough
because the 4 × 4 size prediction can describe the local characteristics of images

9.5 Experimental Results
191
Figure 9.13 Experimental results for Foreman (176×144) and Spoke (512×512) on 8×8 blocks.
Figure 9.14 Experimental results for Foreman (176×144) and Spoke (512×512) on 4×4 blocks.
more precisely. Consequently, only a comparable performance is achieved by BDF
on Foreman and Spoke at low bit rates. At high bit rates, there is even a loss on both
images since there is no DC mode for BDF. When combining BDF with UDF, dFT
can achieve up to a 0.5 dB gain on Foreman and 0.8 dB on Spoke, which are smaller
than those for 8 × 8 and 16 × 16 blocks. Also we can see from Figure 9.11 that the
superiority of dFT over H.264 intra coding is weakened by allowing variable block
size predictions, which can partially adapt to the local characteristics of images.
In addition to objective evaluations, reconstructed frames of Foreman and Spoke
at 0.3 bpp are also depicted in Figure 9.15. Apparently, the quality of dFT is much
better than H.264 intra prediction. For Foreman, there is a clearer outline of the right
eye and ringing artifacts around sharp edges disappear with dFT. For Spoke, apparent
ringing artifacts are seen with H.264 intra prediction, while the edges with dFT look
more sharp and smooth. Similar results are also observed in Straw. For Barbara,
Lena, and Monarch, the subjective performances of H.264 intra coding are preserved
by dFT. In conclusion, dFT not only presents a signiﬁcant objective performance gain
but also shows a much better subjective performance over H.264 intra coding.
As for computational complexity, BDF modes clearly have higher complexity
than UDF modes at the encoder. To be speciﬁc, the prediction of BDF modes has
similar computations as that of UDF modes. The increased computation from BDF
modes lies in the nonseparable transforms and the RDO selection on each block

192
9 Directional Filtering Transform
Figure 9.15 Reconstructed frame of Foreman and part of Spoke at 0.3 bpp. (a) H.264 Intra; (b)
dFT.
deciding whether a transform is performed or not. According to current implementa-
tion without any speed optimization, the encoding time of dFT is about 7 to 8 times
that of H.264 intra coding. From the experiences on H.264 speed optimization, the
encoding computation can be signiﬁcantly reduced and still maintain a similar per-
formance. At the decoder side, since all prediction modes and transform decisions
are known, there is hardly any increase in complexity.
9.6 Summary
We present a mathematical analysis of the lifting order problem of ADL in this chap-
ter and show the advantages of lifting with an adaptive order. Also the directional
ﬁltering transform (dFT) is proposed according to the theoretical analysis to better
exploit the directional correlations in images/intra frames. The band-based feature
of BDF makes up the shortcomings of directional intra prediction from neighboring
block boundaries in H.264. Also with an adaptive lifting order, an evenly distributed
set of ﬁltering modes occupying the whole plane offers it the possibility to predict
from the most correlated samples. Both inter-block and intra-block correlations are
exploited by dFT. Experiments in H.264 intra coding demonstrate its effectiveness
especially in images with sharp edges. Subjective results also show its superiority in
preserving the geometric ﬂow of edges.

Part IV
Vision-Based Compression
Conventional visual data compression technologies base on Shannon’s informa-
tion theory. In most of existing image compression schemes and standards, the sta-
tistical correlation among pixels is considered as a dominant factor on designing pre-
diction, transform, quantization, and entropy coding. During the past three decades,
the coding technologies have evolved signiﬁcantly. However, it has been felt dif-
ﬁcult to further improve the coding technologies. In order to improve the coding
performance, the computational complexity has been and still will be increased sig-
niﬁcantly. Nevertheless, it is more and more difﬁcult to get a decent improvement
even with very complicated schemes. It is time to reconsider if the current coding
framework can have a bright future and if there are new and promising frameworks.
In this part, we are trying to extend the current coding framework from different
aspects.
In Chapter 10, image compression utilizing visual redundancy is investigated. In-
spired by recent advancements in image inpainting techniques, we present an image
compression framework toward visual quality rather than pixel-wise ﬁdelity. In this
framework, an original image is analyzed at the encoder side so that portions of the
image are intentionally and automatically skipped. Instead, some information is ex-
tracted from these skipped regions and delivered to the decoder as assistant informa-
tion in the compressed fashion. The delivered assistant information plays a key role
in the proposed framework because it guides image inpainting to accurately restore
these regions at the decoder side. Moreover, to fully take advantage of the assistant
information, a compression-oriented edge-based inpainting algorithm is proposed
for image restoration, integrating pixel-wise structure propagation and patch-wise
texture synthesis. We also construct a practical system to verify the effectiveness of
the compression approach in which edge map serves as assistant information and
the edge extraction and region removal approaches are developed accordingly. Eval-
uations have been made in comparison with baseline JPEG and standard MPEG-4
AVC/H.264 intra-picture coding. Experimental results show that our system achieves
up to 44% and 33% bits-savings, respectively, at similar visual quality levels. Our
proposed framework is a promising exploration toward future image and video com-
pression.
Current image coding schemes make them hard to utilize external images for
compression even if highly correlated images can be found in the cloud. To solve
this problem, in Chapter 11 we present a method of cloud-based image coding that
is different from current image coding even on the ground. It no longer compresses
images pixel by pixel and instead tries to describe images and reconstruct them from

a large-scale image database via the descriptions. First, we describe an input image
based on its down-sampled version and local feature descriptors. The descriptors
are used to retrieve highly correlated images in the cloud and identify corresponding
patches. The down-sampled image serves as a target to stitch retrieved image patches
together. Second, the down-sampled image is compressed using current image cod-
ing. The feature vectors of local descriptors are predicted by the corresponding vec-
tors extracted in the decoded down-sampled image. The predicted residual vectors
are compressed by transform, quantization, and entropy coding. The experimental re-
sults show that the visual quality of reconstructed images is signiﬁcantly better than
that of intra-frame coding in HEVC and JPEG at thousands to one compression.
In Chapter 12, we present a novel feature-based coding scheme by using inter-
image correlation for cloud photo storage. Different from other schemes on image
set compression, our key technical contribution is to use local feature descriptors
on analyzing the correlation among images. This content-based matching is invari-
ant to scale and rotation, and is less sensitive to illumination changes. According
to the analysis result, we can reorganize correlated images as a pseudo sequence
for compression by minimizing predictive cost. We also observe that the correlation
in pseudo sequences is much more complicated than that in natural video. By us-
ing locally matched regions, a multi-model approach is presented characterizing lo-
cal geometric deformation and photometric transformation between images, which
signiﬁcantly outperforms global transformation. For one reference image, multiple
predictions can be generated according to the multi-model approach. Finally, we
further exploit the correlation between current image and multiple predictions by
block-based motion compensation, similar to video compression. Experimental re-
sults show that our scheme coding of correlated images as a pseudo sequence is 10
times more efﬁcient than individual JPEG compression.

Chapter 10
Edge-Based Inpainting
10.1 Introduction
Over the last two decades, great improvements have been made in image and video
compression techniques driven by a growing demand for storage and transmission
of visual information. State-of-the-art JPEG 2000 and MPEG-4 AVC/H.264 are two
examples that signiﬁcantly outperform their previous rivals in terms of coding ef-
ﬁciency. However, these mainstream signal-processing-based compression schemes
share a common architecture, namely, transform followed by entropy coding, where
only the statistical redundancy among pixels is considered as the adversary of coding.
Through two decades of development, it has been becoming difﬁcult to continuously
improve coding performance under such architecture. Speciﬁcally, to achieve high
compression performance, more and more modes are introduced to deal with regions
of different properties in image and video coding. Consequently, intensive compu-
tational efforts are required to perform mode selection subject to the principle of
rate-distortion optimization. At the same time, more and more memory-cost context
models are utilized in entropy coding to adapt to different kinds of correlations. As a
result, small improvements in coding efﬁciency are accomplished with great pain of
increased complexity in both encoder and decoder.
In addition to statistical redundancy, visual redundancy in videos and images has
also been considered in several works. They are motivated by the generally accepted
fact that minimizing overall pixel-wise distortion, such as mean square error (MSE),
is not able to guarantee good perceptual quality of reconstructed visual objects, es-
pecially in low bit-rate scenarios. Thus, the human visual system (HVS) has been in-
corporated into compression schemes by Jayant et al. [192] and Hontsch and Karam
[193], trying to remove some visual redundancy and to improve coding efﬁciency as
well as visual quality. Moreover, attempts have been made to develop compression
techniques by identifying and utilizing features within images to achieve high coding
efﬁciency. These kinds of coding approaches are categorized as second-generation
techniques by Reid et al. [194], and have raised a lot of interest due to the poten-
tial of high compression performance. Nevertheless, taking the segmentation-based
195

196
10 Edge-Based Inpainting
coding method as an example, the development of these coding schemes is greatly
inﬂuenced by the availability as well as effectiveness of appropriate image analysis
algorithms, such as edge detection, segmentation, and texture modelling tools.
Recently, technologies in computer vision as well as computer graphics have
shown remarkable progress in hallucinating pictures of good perceptual quality. In-
deed, advancements in structure/texture analysis [195,196] and synthesis are leading
to promising efforts to exploit visual redundancy. So far, attractive results have been
achieved by newly presented texture synthesis techniques to generate regions of ho-
mogeneous textures from their surroundings [197–205]. Furthermore, various image
inpainting methods have been presented, aiming to ﬁll-in missing data in more gen-
eral regions of an image in a visually plausible way. In fact, the word inpainting was
initially invented by museum or art restoration workers. It is ﬁrst introduced into dig-
ital image processing by Bertalmio et al. [206], where a third order partial differential
equation (PDE) model is used to recover missing regions by smoothly propagating
information from the surrounding areas in isophote directions. Subsequently, more
models are introduced and investigated in image inpainting, for example, total vari-
ation (TV) model [207], coupled second order PDE model taking into account the
gradient orientations [208], curvature driven diffusion (CDD) model [209], and so
on. All these approaches work at pixel level and are good at recovering small ﬂaws
and thin structures. Additionally, exemplar-based approaches have been proposed to
generate textural coarseness; by augmenting texture synthesis with certain automatic
guidance, edge sharpness and structure continuity can also be preserved [210–212].
Combining PDE diffusion and exemplar-based synthesis presents more encouraging
inpainting results [213–215]. Moreover, inpainting capability is further improved by
simple human interactions when human knowledge is borrowed to imagine what
unknown regions should be, so that the restoration results look natural to view-
ers [216,217].
Due to its potential in image recovery, image inpainting likewise provides cur-
rent transform-based coding schemes another way to utilize visual redundancy in
addition to those that have been done [192–194]. This inference has been success-
fully exempliﬁed in error concealment when compressed visual data is transmitted
over error-prone channels [215, 218]. Moreover, it has been reported that improve-
ment is achieved by employing image inpainting techniques in image compression
even though in a straightforward fashion [215]. In addition, image compression also
brings new opportunities to image inpainting, as we have pointed out [219]. Since
the complete source images are available, many kinds of assistant information can
be extracted to help inpainting deal with complex regions that contain structures or
other features and which are unable to be properly inferred from the surroundings.
Thus, inpainting here becomes a guided optimization for visual quality instead of
a blind optimization for image restoration. Accordingly, new inpainting techniques
may be developed to better serve image compression.
When image inpainting and image compression are jointly considered in an in-
tegrated coding system, two main problems need to be addressed. The ﬁrst: what
should be extracted from a source image as assistant information to represent im-
portant visual information. The second: how to reconstruct an image with this

10.2 The Proposed Framework
197
assistant information. On the one hand, it has been reported that using different image
analyzers, various kinds of assistant information can be extracted, including edge,
object, sketch [196], epitome [220,221], and so on, to represent an image or portion
of an image. Then, given a speciﬁc kind of assistant information, the correspond-
ing restoration method should be developed to complete a desired reconstruction by
making full use of it. On the other hand, from the compression point of view, the
effectiveness of restoration methods as well as the efﬁciency of the compression of
assistant information would also inﬂuence the choice of assistant information. Such
dependency makes the problems more complicated.
In this chapter, we present an image coding framework in which currently de-
veloped vision techniques are incorporated with traditional transform-based coding
methods to exploit visual redundancy in images. In this scheme, some regions are
intentionally and automatically removed at the encoder and are restored naturally
by image inpainting at the decoder. In addition, binary edge information consisting
of lines of one-pixel width is extracted at the encoder and delivered to the decoder
to help restoration. Techniques, including edge thinning and exemplar selection, are
proposed and an edge-based inpainting method is presented in which distance-related
structure propagation is proposed to recover salient structures, followed by texture
synthesis. The basic idea has been discussed in our conference papers [219, 222].
However, some problems have not been investigated carefully in those papers, in-
cluding questions such as why the edges of an image are selected as assistant infor-
mation, or how to select the exemplar blocks automatically, and so on.
10.2 The Proposed Framework
As the basic idea of “encoder removes whereas decoder restores” has been men-
tioned in literature for image compression [215,223], we would like to point out the
novelties of our proposed method here. First, in our approach, the original image is
not simply partitioned into two parts: one is coded by conventional transform-based
approach, and the other is skipped during encoding and restored during decoding. In-
stead, techniques for image partition, block removal, and restoration in our proposed
scheme are carefully designed toward compression rather than straightforward adop-
tion. Furthermore, skipped regions will not be completely dropped at the encoder
side if they contain a portion of information that is difﬁcult to be properly recovered
by conventional image inpainting methods. In fact, assistant information is extracted
from the skipped regions to guide the restoration process and further induce new
inpainting techniques.
The framework of our proposed compression scheme is depicted in Figure 10.1.
In this scheme, an original image is ﬁrst analyzed at the encoder side. The image
analysis module automatically preserves partial image regions as exemplars and
sends them to the exemplar encoder module for compression using conventional
approaches. Meanwhile, it extracts designated information from skipped regions as
assistant information and sends it to the assistant info encoder module. Then, the

198
10 Edge-Based Inpainting
Figure 10.1 The framework of our proposed image compression scheme.
coded exemplars and coded assistant information are banded together to form ﬁnal
compressed data of this image. Correspondingly, at the decoder side, exemplars and
assistant information are ﬁrst decoded and reconstructed. Then, the regions skipped
at the encoder are restored by image inpainting based on the twofold information.
At the end, the restored regions are combined with the decoded exemplar regions to
present the entire reconstructed image.
Figure 10.1 shows a general framework of the proposed compression scheme that
does not constrain which kind of assistant information should be used there. Since
source image is always available at the encoder side, there are many choices of as-
sistant information extracted from the skipped regions, for example, semantic object,
visual pattern, complete structure, simple edges, and so on. Here we start from the
mathematical models in image inpainting to discuss what on earth the assistant in-
formation should be.
As shown in Figure 10.2, suppose that we are given an image function f(x), x ∈I,
where I is a square region in R2. Ω, depicted as the gray region in Figure 10.2, is an
open bounded subset of I with Lipschitz continuous boundary. It is just the region to
be restored by image compression, image inpainting, or a combination of them. This
restoration problem can be generalized as
argmin
Z
ΩD
 fΩ(x)−ˆfΩ(x)

dx+λR

.
(10.1)
Figure 10.2 Illustration of image inpainting, where the gray region is to be restored.

10.2 The Proposed Framework
199
Here, fΩ(x) is the original image function in Ω, where it should satisfy fΩ(x) =
f(x) for any x ∈Ω. ˆfΩ(x) is a reconstruction of fΩ(x) at decoder. λ is a Lagrange
factor. Clearly, Eq. (10.1) is to ﬁnd the optimal function ˆfΩ(x) by minimizing the
joint cost consisting of reconstructed distortion D() and coding bits R for Ω. Thus,
image compression and image inpainting can be viewed as two extreme cases of
Eq. 10.1. Speciﬁcally, in traditional image compression, fΩ(x) is directly coded and
sent to the decoder, where many bits may be needed to represent fΩ(x); whereas
in image inpainting, there is no bit to represent fΩ(x) since ˆfΩ(x) is inferred from
fI\Ω(x). However, our proposed method, which is quite different from compression
or inpainting, can be granted as a combination of them.
In typical inpainting scenarios, the restoration of fΩ(x) is usually an ill-posed
problem because information in Ωis totally unknown. Fortunately, an image is a 2D
projection of the 3D real world. The lost region often has similar statistic, geometric,
and surface reﬂectivity regularities as those in the surroundings. It makes the above
ill-posed problem possible to be solved. Therefore, some models are introduced in
image inpainting to characterize statistic, geometric, and surface regularities. These
models should employ generic regularities, rather than rely on a speciﬁc class of
images, so that model-based inpainting can be applied in generic images.
One such model, the TV model, is presented by Chan and Shen [207] for image
inpainting, in which the variation regularity is ﬁrst introduced. Since local statistical
correlation usually plays a more important role than the global one, as shown in
Figure 10.2, B instead of I \ Ωis used to infer the regularities in Ω, where B is a
band around Ω. Then, the TV model is to ﬁnd a function ˆfΩ(x) on the extended
inpainting region B∪Ωsuch that it minimizes the following energy function:
argmin
Z
B∪Ω|▽ˆfΩ(x)|dx+λ
Z
B | ˆfΩ(x)−f(x)|2dx

.
(10.2)
The ﬁrst term in Eq. (10.2) is to measure local homogeneity of image function
in the region B ∪Ω, and the second term, called the ﬁdelity term, is the sum of
the squared difference (SSD) between the reconstructed and the original B in ˆfΩ(x)
and B the original in f(x). Eq. (10.2) can be solved by the Euler-Lagrange method
described by Chan and Shen [207]. Accordingly, TV inpainting is good at restoring
homogenous regions. But, if the lost region contains rich structures, it does not work
well, especially when structures are separated far apart by the lost region.
To solve it, another parameter θ is introduced in the inpainting model [208]. Let θ
be the vector ﬁeld of normalized gradient of f(x). ˆθΩis the corresponding parameter
to be restored on Ω. With the new parameter of gradient directions, the inpainting
problem is posed as extending the pair of functions (f,θ) on B to a pair of functions
( ˆfΩ, ˆθΩ) on Ω. It is completed by minimizing the following function:
argmin(
Z
B∪Ω|div
  ˆθΩ(x)

|p  a+b|▽k ∗ˆfΩ(x)|

dx
+α
Z
B∪Ω
 |▽ˆfΩ(x)|−ˆθΩ(x)·▽ˆfΩ(x)

dx)
.
(10.3)

200
10 Edge-Based Inpainting
The ﬁrst term presents smooth continuation demand on ˆθΩ, where a and b are
positive constants, and k is a smoothing kernel. It is the integral of the divergence
(in Lp function space) of the vector ﬁeld ˆθΩ, with respect to the gradients of the
smoothed ˆfΩ(x). The second term is a constraint between ˆθΩand ˆfΩ(x), where α
is a positive weighting factor. ˆθΩshould be related to ˆfΩ(x) by trying to impose
ˆθΩ· ▽ˆfΩ= | ▽ˆfΩ|. The use of the vector ﬁeld θ is the main point of the model
given in Eq. (10.3). Thus, it enables image inpainting to restore missing regions by
continuing both the geometric and photometric regularities of images.
However, the model in Eq. (10.3) assumes that the parameter ˆθΩcan be inferred
from θ under a certain smooth constraint. But this assumption is not always true for
nature images. Taking Figure 10.2 as an example, the area to be restored consists of
two homogenous regions divided by an edge denoted by the solid curve. The dashed
curve is the inferred edge in Ωaccording to Eq. (10.3), which is quite different from
the actual one. This problem is hard to be solved in conventional inpainting scenar-
ios even using human intelligence as proposed by Sun et al. [217]. Therefore, in our
proposed coding framework, assistant information should be used to correctly infer
ˆθΩon Ω. As we have discussed, ˆθΩis the vector ﬁeld of normalized gradient and
is independent from the absolute magnitudes of gradients. It contains two parts of
information: where ˆθΩexists and what its direction is. Commonly, it can be simply
represented by binary edges of one-pixel width for the purpose of efﬁcient compres-
sion.
Consequently, edge information is selected as assistant information for image in-
painting. With assistant information, we could remove more regions in an image.
Thus, it greatly enhances the compression power of our method. Since edges are
low-level features in image, there are some mature tools available to automatically
track them in an image. Moreover, edge information is concise and easy to describe
in compressed fashion. Therefore, the employment of edge information can, on the
one hand, help preserve good visual quality of the reconstructed image. On the other
hand, it enables high compression performance by removing some structural regions
and efﬁciently coding the edge information.
Accordingly, an overview of our approach is exempliﬁed in Figure 10.3. Fig-
ure 10.3a is the input of the original image of Lena. After image analysis, an edge
map denoted by curves in Figure 10.3b is generated, based on which the exemplars
denoted by the non-black blocks in Figure 10.3c are selected. Consequently, the ex-
emplars and the needed edge information shown as curves in Figure 10.3c will be
coded into bitstream. Then, at the decoder side, the edge information is utilized to
guide the structure propagation for the recovery of edge-related regions. The corre-
sponding result is given in Figure 10.3d. The remainder of unknown regions will be
restored by texture synthesis. The ﬁnal reconstructed image after region combination
is given in Figure 10.3e.
In the following two sections, we will explain the modules in our framework in
detail, especially on the two most important modules, namely image analysis and
assisted image inpainting. Here, we would like to emphasize that the introduction of
assistant edge information raises different demands on both the encoder and decoder.
We deal with them comprehensively.

10.3 Edge Extraction and Exemplar Selection
201
(a)
(b)
(c)
(d)
(e)
(f)
Figure 10.3 Comparison with baseline JPEG on test image Lena. (a) Original image. (b) Edge
map. (c) Removed blocks (black blocks) and assistant edge information, note that the assistant edge
information is a subset of the entire edge map. (d) Reconstructed image after structure propagation.
(e) Reconstructed image after texture synthesis. (f) Reconstructed image by baseline JPEG.
10.3 Edge Extraction and Exemplar Selection
The image analysis module at the encoder side consists of two sub-modules: The
ﬁrst is to extract edge information from image and the second is to select exemplar

202
10 Edge-Based Inpainting
and skipped regions at block level according to available edge information. They are
discussed in the following two sections.
10.3.1 Edge Extraction
As discussed in Section 10.2, edge information plays an important role in the pro-
posed coding scheme. It assists the encoder to select exemplar and skipped regions
and the decoder to restore skipped regions with our proposed edge-based inpaint-
ing. Extracted edges do not need to represent complete and continuous topological
properties of an image because our purpose is not to segment or restore an object.
Discontinuous edges can likewise play the role of assistant information in the pro-
posed scheme. But taking the topological properties into account in edge extraction
will make edges more meaningful in terms of low-level vision.
Therefore, though there are many mature tools available to extract edges from
images, the topology-based algorithm presented by Rothwell et al. [224] is adopted
in our system to extract assistant information. The algorithm presents good results
especially on extracting intersection edges. According to this method, an input image
is ﬁrst smoothed by a two-dimensional isotropic Gaussian ﬁlter so as to avoid noise.
Second, | ▽f(x)| and θ are calculated on the ﬁltered image for each pixel x. If | ▽
f(x)| is the local maximum gradient along the direction θ and larger than a threshold,
then pixel x belongs to an edge. At last, the pixels with non-maximum gradients
are checked by spatially adapted thresholds to prevent missing edges caused by the
unreliable estimation of θ.
As shown in Figure 10.4a by the curves, edges extracted with the above algorithm
(or most of the existing methods as well) are often of more than one-pixel width.
This causes ambiguous directions in guiding the restoration at the decoder side and
also increases the number of bits to code the edge information. Although [224] also
proposes a thinning method, it does not satisfy the special requirement in our pro-
posed edge-based inpainting. It is because pixel values on edges are not coded but
rather inferred from connected surrounding edges in our proposed scheme. Thus, a
new thinning method is proposed here by taking into account the consistence of pixel
values on edges as well as the smoothness of edges.
Here, we present the details of our proposed thinning method. Given the detected
edge pixels, we ﬁrst group them into eight connective links and each edge-link (also
known as a connected component in the graph that is made up by edge pixels) is
thinned independently. Complying with the terminologies deﬁned in Section 10.2,
our goal is to ﬁnd a one-pixel-width line which contains N pixels, that is, f(xn) for
n = 1,2,··· ,N, yielding the minimal energy
J = α
N
∑
n=1
|△f(xn)|+β
N
∑
n=1
N
∑
k=1
d (f(xn), f(xk))+γ
N
∑
n=1
|κ(xn)|p,
(10.4)
where α, β, and γ are positive weighting factors. The energy function Eq. (10.4)
consists of three terms. The ﬁrst term is the Laplacian of each edge pixel. The second

10.3 Edge Extraction and Exemplar Selection
203
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 10.4 Step-wise results of our scheme on test image Lena, zoomed-in partition. (a) Original
image with detected edge pixels. (b) Thinned edges. (c) Chosen necessary structural blocks. (d)
Chosen additional structural blocks. (e) Chosen necessary textural blocks. (f) Chosen additional
textural blocks. (g) Structure propagation result. (h) Texture synthesis result.
term is the constraint on pixel values of all edge pixels. After thinning, remaining
edge pixels should have similar values. To make this constraint as simple as possible,
only the difference among eight neighboring pixels are considered, and the function
d() is deﬁned as
d (f(xn), f(xk)) =

|f(xn)−f(xk)|, ifxk ∈µ8(xn)
0, otherwise
.
(10.5)
µ8(xn) denotes the 8-neighbor of xn. The last term of Eq. (10.4) evaluates the curva-
ture of the edge at each pixel. Similar to [208,209], is deﬁned as
κ(xn) = div
 ▽f(xn)
|▽f(xn)|

.
(10.6)

204
10 Edge-Based Inpainting
In addition, we want to emphasize that the thinning process should not shorten the
edge, thus only redundant pixels on the edge can be removed.
The optimal thinning solution for each edge-link is obtained through dynamic
programming algorithm. Given a start point of each edge-link, the energies of all
possible paths, linked in eight connective manner, are calculated according to Eq.
(10.4). Referring to the width of the initial edge-link, several paths with smaller
energies are recorded in the dynamic programming. Then, each recorded path is
extended consequently by adding one neighbor pixel, which results in the minimal
energy. Note that the thinning algorithm can be performed in parallel manner for all
edge-links in an image, because they are independent in terms of thinning process.
Figure 10.4b presents the corresponding thinning results using our proposed method.
10.3.2 Exemplar Selection
After the edges are extracted, exemplar selection is performed based on these avail-
able edges. Here, for simplicity, the exemplar selection process is performed at
block level. Speciﬁcally, an input image is ﬁrst partitioned into non-overlapped 8×8
blocks, and each block is classiﬁed as structural or textural according to its distance
from edges. In detail, if more than one-fourth of pixels in a block are within a short
distance (e.g., ﬁve-pixel) from edges, it is regarded as a structural block, otherwise
a textural one. Then, different mechanisms are used to select the exemplars for tex-
tural blocks and structural blocks. Blocks that are not selected as exemplars will be
skipped during encoding. Moreover, exemplar blocks are further classiﬁed into two
types, the necessary ones and the additional ones, based on their impacts on inpaint-
ing as well as on visual ﬁdelity. Generally, one image can not be properly restored
without necessary exemplar blocks, whereas additional blocks help to further im-
prove visual quality.
(1) Textural Exemplar Selection: Figure 10.5a illustrates the process of exemplar
selection for textural blocks. In this ﬁgure, edge information is denoted by thickened
lines, based on which the image is separated into structural regions (indicated by
gray blocks) and textural regions (indicated by white and black blocks).
It is generally accepted that pure textures can be satisfactorily generated even
given a small sample. However, in practice, image regions are often not pure textures,
but rather contain kinds of local variations, such as lighting, shading, and gradual
changing. Furthermore, exemplar-based texture synthesis is sensitive to the chosen
samples. In image inpainting, a common solution to unknown textural regions is to
synthesize them from samples in their neighborhood.
In our scheme, the necessary textural exemplars are selected in the border of textu-
ral regions. That is, as shown in Figure 10.5a, denoted by white blocks, if a textural
block is next to a structural one, along either horizontal or vertical direction, it is
considered as necessary. Such blocks are selected because they contain the informa-
tion of transitions between different textural regions, which are hard to be restored

10.3 Edge Extraction and Exemplar Selection
205
(a)
(b)
Figure 10.5 An example of necessary exemplar selection in which curves denote edges and black
blocks denote skipped regions. (a) Textural exemplar selection in which white blocks are necessary
textural exemplars; (b) structural exemplar selection in which white blocks are necessary structural
exemplars, four types of edges are also distinguished in (b). Also see Figure 10.4 for a practical
example.
by inner samples. In addition, propagation of these blocks, from outer to inner, can
reconstruct the related textural regions.
To further improve visual quality of reconstructed images, additional blocks can
be progressively selected to enrich exemplars. In this process, we consider additional
blocks as representatives of local variations. On the one hand, if a block contains ob-
vious variation, it should be preserved in advance. On the other hand, because the
variation is a local feature, removing large-scale regions should be avoided in ex-
emplar selection. Thus, each non-necessary textural block Bi is related to a variation
parameter Vi deﬁned as
Vi = wiVar(Bi)+w2
∑
B j∈µ4(Bi)
|E(Bi)−E(Bj)|.
(10.7)
Here, w1 and w2 are positive weighting factors. µ4() indicates 4-neighbor of a
certain block. The functions Var() and E() are the variance and mean value of the
pixel values in a block, respectively. In our system, according to an input ratio, the
blocks with higher variation parameters will be selected, during which we also check
the connective degree of each block so that the removed blocks do not constitute a
large region.
(2) Structural Exemplar Selection: Figure 10.5b shows the exemplar selection
method for structural blocks. In this ﬁgure, edges are represented by lines with indi-
cated different types, and structural regions are indicated in white and black blocks,
whereas all textural regions are in gray.

206
10 Edge-Based Inpainting
As we have discussed, besides many textural blocks, some structural blocks are
also skipped at the encoder side and restored at the decoder side by the guidance
of edge information. Therefore, necessary and additional structural exemplars are
also selected based on available edges. To better introduce the method, edges are
categorized into four types according to their topological properties, as indicated in
Figure 10.5b: “isolated” edge traces from a free end (i.e., an edge pixel connected
with only one other edge pixel) to another free end; “branch” edge traces from a free
end to a conjunction (i.e., an edge pixel connected with more than three other edge
pixels); “bridge” edge connects two conjunctions; and “circle” edge gives a loop
trace.
Commonly, edge acts as the boundary of different region partitions. For the sake
of visual quality, in image inpainting, two textural partitions along both sides of an
edge should be restored independently. The tough problem here is how to restore
the transition between two partitions. We may use a model to interpolate the tran-
sition from textures of two partitions, but usually the results look very artiﬁcial and
unnatural. Therefore, the blocks containing the neighborhood of free ends should
be selected as exemplar so that the transitions of textural partitions can be restored
by propagating information in these blocks along the edges. Conjunction blocks of
edges are also selected as exemplar for similar reason because there are transitions
among more than three textural regions. For circle edges, a circle completely divides
the image into two partitions — inner part and outer part — so we choose two blocks
as necessary exemplars, which contain the most pixels belonging to inner region and
outer region of a circle edge, respectively. In a few words, by necessary exemplars,
we provide not only samples for different textures separated by an edge, but also the
information of the transitions between these textures, and thus the decoder is able to
restore the structural regions.
Additional structural blocks can also be selected as exemplars to further improve
visual quality. Given an input ratio, the process is quite similar to that for textural
blocks. Each non-necessary structural block is also related to a variation parameter,
which can be calculated by Eq. (10.7). Here, the different partitions separated by
the edges are independently considered in calculating the mean value as well as the
variance, and resulting parameters of different partitions are summed up to get the
total variation parameter of a block.
In Figure 10.4, we present the step-wise results of the exemplar selection method.
Based on the edge information shown in Figure 10.4b, the selected necessary and
additional structural exemplars are shown in Figure 10.4c,d gradually. Similarly, in
Figure 10.4e,f we add the necessary and additional textural exemplars, as well.
10.4 Edge-Based Image Inpainting
Based on the received edges and exemplars, we propose an edge-based image in-
painting method to recover the non-exemplar regions at the decoder side. Different
from the encoder, the inpainting algorithm is not block-wise but rather designed to

10.4 Edge-Based Image Inpainting
207
deal with arbitrary-shaped regions. Still, the non-exemplar regions are classiﬁed into
structures and textures according to their distances to the edge as the encoder. Gen-
erally, structures are propagated ﬁrst, followed by texture synthesis (as shown in Fig-
ure 10.3d,e). A conﬁdence map, similar to that by Drori et al. [211] and Criminisi et
al. [212], is constructed to guide the order of structure propagation as well as texture
synthesis. Speciﬁcally, at the very beginning, known pixels (pixels in decoded exem-
plars) are marked with conﬁdence 1 and unknown pixels (pixels in removed blocks)
are marked with conﬁdence 0. Afterward, each generated pixel is related with a con-
ﬁdence value between 0 and 1 during the inpainting process. In addition, known
pixels as well as generated pixels are all called “available” ones in this section.
In the literature, exemplar-based inpainting methods can be roughly classiﬁed into
two types, that is, pixel-wise schemes and patch-wise schemes. Pixel-wise meth-
ods are suitable for restoration of small gaps, but may introduce blurring effects or
ruin texture pattern while dealing with large areas. Patch-wise methods, on the con-
trary, are good at keeping texture pattern, but may introduce seams between different
patches, which are quite annoying. In our scheme, these two strategies are adapted
for different circumstances.
10.4.1 Structure Propagation
A sketch map of structure propagation is shown in Figure 10.6. The gray block in
Figure 10.6 indicates an unknown structural block; the black curve with circle points
represents an edge piece and related pixels; and four dash-dot lines restrict a region,
namely inﬂuencing region, including unknown pixels within a short distance (e.g.,
ten-pixel) from the edge. Notice that it is the edge piece together with the inﬂuenc-
ing region, rather than a structural block, that is treated as a basic unit in the structure
propagation. Since the free ends and conjunctions of edges are all selected as exem-
plars, the textural regions along an edge can be readily divided and independently
generated in inpainting process.
To recover a basic unit, the unknown pixels belonging to the edge piece are ﬁrst
generated. As shown in Figure 10.6a, the unknown pixels (denoted by black points)
are generated from the known pixels (indicated by white points) using linear inter-
polation, that is,
ˆf(xn) = ∑N
k=1 λnk ˆf(xk)
∑N
k=1 λnk
,
(10.8)
λnk =

|n−k|−2, if xk is known
0, otherwise
.
where, similar to Eq. (10.4), N gives the number of pixels in this edge piece and n
and k index different pixels.
After the edge restoration, the neighboring structure as well as texture within
the inﬂuencing region will be ﬁlled-in with regard to the recovered edge. The
inpainting method for completion of inﬂuencing region is designed concerning the

208
10 Edge-Based Inpainting
(a)
(b)
Figure 10.6 Pixel-wise structure propagation. (a) A piece of edge and its inﬂuencing region, with
arrowed dash-dot lines and dashed lines showing the propagation directions. (b) Restoration of the
inﬂuencing region in which each generated pixel is copied from one of two candidate pixels.
following facts. First, the pixel-wise approach is preferred since the narrow regions
along the edge pieces are to be handled. Second, edges are expressed by one-pixel-
width curves, which can be quite different in geometric shapes among exemplar and
non-exemplar regions, so we have to wrap the edges to reconstruct the unknown
structure. Finally, the widths of structures are local variant, which means that it is
hard to tell the exact boundary between structure and texture in an inﬂuencing re-
gion. Therefore, in our scheme, each pixel in the inﬂuencing region will have two
candidates: one is treated as a structural pixel to be propagated parallel along the
edge; the other is regarded as a textural pixel to be generated from the neighboring
available pixels. Then, the one that makes a smooth transition from structure to tex-
ture will be selected to ﬁll-in the unknown pixel. Moreover, as the decision making
on candidate pixels is highly relevant to its available neighbors, the order for pixel
completion is another important issue that should be considered. Thus, we also con-
struct a conﬁdence map, as mentioned at the beginning of this section, to control the
generation order. For the unknown pixel, the higher the neighboring conﬁdence is,
the earlier it will be generated.
Accordingly, the recovery of inﬂuencing region is performed as follows. Here,
unknown pixels to be recovered in the inﬂuencing region are called target pixels.
They are denoted by black points in Figure 10.6b. For each target pixel, two can-
didate pixels are searched out from the surrounding available pixels. The structural
candidate (S-candidate) of the target pixel, which lies within the inﬂuencing region,
is indicated by horizontal striped point in Figure 10.6b; whereas the textural can-
didate (T-candidate) of the target pixel is denoted by vertical striped point, which
locates within a short distance from the target pixel despite whether it is within the
inﬂuencing region or not.

10.4 Edge-Based Image Inpainting
209
Figure 10.7 Pair matching in our structure propagation algorithm.
A pair matching method, similar to that by Ashikhmin [199], is utilized to gener-
ate both the S-candidate and the T-candidate. As illustrated in Figure 10.7, for each
assistant pixel, also known as any available pixel belonging to the 8-adjacent neigh-
borhood of the target pixel, we will search for its match pixel(s) with the most similar
value to it. Then, complying with the spatial relation between the assistant pixel and
the target one, a pixel adjacent to a match pixel in the same relative spatial position
is selected as a source pixel. As indicated in Figure 10.7, an assistant pixel may cor-
respond to several match pixels and gives several source pixels; meanwhile, several
assistant pixels in 8-adjacent neighborhood may generate the same source pixel, as
well. After obtaining several source pixels, we propose to use a weighted-SSD (sum
of squared difference) criterion to choose the S-candidate, as given in
Ds = ∑
i
 |d(xi
S)−d(xi
t)|+1

×| ˆf(xi
S)−ˆf(xi
t)|2,
(10.9)
where xi
S and xi
t are corresponding, the ith pixel in the neighborhood of the S-
candidate and the target pixel, respectively, and d() indicates the distance from each
pixel to the edge, ˆf() , as used before, is the reconstructed image. By minimizing Eq.
(10.9), we can ﬁnd the S-candidate from the obtained source pixels, which is situated
in a similar relative position to that of the target pixel with respect to the edge, thus
ensuring the parallel diffusion of structural information.
Differently, since no direction information is involved in the textural region, only
the ordinary SSD between the neighborhood of source pixels and target pixel is con-
sidered as the criterion to choose the T-candidate,
DT = ∑
i
| ˆf(xi
T)−ˆf(xi
t)|2.
(10.10)

210
10 Edge-Based Inpainting
Similar to that in Eq. (10.9), xi
t here represents the ith pixel in the neighborhood of
the T-candidate. Thus, the source pixel that has the most similar neighboring values
to the target one will be selected as the T-candidate.
In fact, the two diffusions, or S-candidate selection and T-candidate selection,
are simultaneous and competitive. These two candidates have to compete with each
other and only one of them will be chosen to ﬁll-in the target pixel. Normally, if
target pixel nears edge, the choice will bias to the S-candidate. In addition, it can be
observed that long-distant parallel diffusion of structural information often leads to
blurring artifacts. Thus, the determination is made by comparing DT and D′
S, which
are deﬁned in Eq. (10.10) and Eq. (10.11), respectively
D′
S =
 d
d0
+ pd
pd0

×∑
i
| ˆf(xi
S)−ˆf(xi
t)|2.
(10.11)
Here, d0 and pd0 are constants and stands for the distance from the target pixel to the
edge and pd indicates the distance from the target pixel to the S-candidate, as shown
in Figure 10.6b. If DT is less than D′
S, then the T-candidate is chosen to ﬁll-in the
target pixel, otherwise the S-candidate is selected. In this way, all unknown pixels
within the inﬂuencing region of an edge are generated.
10.4.2 Texture Synthesis
The edges as well as their inﬂuencing regions are readily restored by structure propa-
gation. Then, in this section, the remainder of unknown regions are treated as textural
regions, so texture synthesis is employed to ﬁll in these holes.
For textural regions, we prefer patch-wise algorithms because they are good at
preserving large-scale texture pattern. We choose square patches as the fundamen-
tal elements while a conﬁdence map is introduced to guide the order of synthesis.
Unknown textural regions are progressively restored during texture synthesis by ﬁrst
reconstructing the prior patches and then the others that remain. The priority of a
patch is determined by calculation of conﬁdence and the distance from the edge. As
shown in Figure 10.8, for each patch centered at a marginal pixel of unknown regions
(denoted by target patch), we calculate the average conﬁdence value of all pixels in
this patch, as well as the average distance of all pixels from the edge. Then the patch
with the highest conﬁdence rating and the greatest distance from the edge will be
synthesized ﬁrst.
Afterward, a source patch, which is most similar to the target patch, will be
searched out from the neighborhood of the target patch. Here, the similarity of two
patches is measured by the SSD of pixel values between overlapped available pixels
of two patches. A patch that results in the least SSD will be chosen as the source
patch. Note that the ﬁlling-in process is not as simple as copy-paste work, we have
to deal with overlapped regions as well as seams. In our algorithm, the graph-cut
method proposed by Kwatra et al. [203] is used to merge the source patch into the
existing image, and Poisson editing [216] is utilized to erase the seams. After one

10.5 Experimental Results
211
Figure 10.8 Pair matching in our structure propagation algorithm.
patch is restored, the conﬁdence map is updated. All newly recovered pixels are
treated as available pixels in the following synthesis steps. Then, the next target patch
is searched and processed until no unknown pixel exists.
10.5 Experimental Results
10.5.1 Implementation
Our presented approach can be integrated with the state-of-the-art coding schemes to
enhance compression performance. In our experiments, two compression standards,
JPEG and MPEG-4 AVC/H.264 (referred to simply as H.264 hereafter), are adopted.
Thus, two fully automatic image coding systems, based on JPEG and H.264 respec-
tively, have been constructed to evaluate the effectiveness of our proposed compres-
sion approach. In this section, we would like to clarify several implementation details
of the systems.
First, in both systems, the one-pixel-width edge information is coded using JBIG
method. Note that the edge information coded into the ﬁnal bitstream is only a sub-
set of the entire edge map. In other words, the edges that are fully covered by the
exemplar regions will not be coded (it can be observed by comparing the edges in
Figure 10.4b and 10.4f). Second, in the JPEG-based system, the exemplar locations
are denoted at block level by a binary map, in which 1 stands for a removed block
and 0 for an exemplar block, and the map is coded by an arithmetic coder. The

212
10 Edge-Based Inpainting
original image is then coded by JPEG coding method, during which the removed
blocks will be skipped in encoding but to be ﬁlled with the DC values copied from
previous blocks, so that the DC prediction in JPEG can still be performed in exemplar
block compression. Third, in the H.264-based system, since the basic coding unit is
macro-block 16×16, we consider two instances: if a macro-block is totally removed,
then a new macroblock type I SKIP is coded; otherwise, the macro-block has a new
element called block removal pattern (BRP) for indicating which of the four 8 × 8
blocks is removed, and the BRP is later coded by the arithmetic coder, too. Similar
to the JPEG-based method, the exemplar blocks are coded using H.264 scheme and
DC values from previous blocks are ﬁlled to the removed blocks to enable the intra
prediction of H.264 scheme.
In addition, there are some predeﬁned parameters in both the encoder and de-
coder. To test the robustness of our system, we ﬁx these parameters as follows for
all test images. At the encoder side, the weighting factors are deﬁned as α = 1.0,
β = γ = 0.2, and p = 1 (suggested by Ballester et al. [208]) for Eq. (10.4) in edge
thinning, while w1 = w2 = 1.0 for Eq. (10.7) in exemplar selection. At the decoder
side, structure propagation works on pixels that have less than 10-pixel distances
from edges. The search range for T-candidate is 9×9, and the S-candidate is found
in the entire inﬂuencing region. The search range and patch size for texture synthesis
are 11 × 11 and 7 × 7, respectively. The parameters in Eq. (10.11) are set to 5. We
would like to remark that the weighting factors for edge thinning have been carefully
tuned using our test images, while the other parameters are just empirically selected
with consulting the existing systems (e.g., [217]). However, it should be noted that
the parameters can greatly inﬂuence the computational complexity of both the en-
coder and decoder, which will be further analyzed in the following section. At last,
the only two ﬂexible parameters in our experiments are the additional block ratios
for the structural exemplar and textural exemplar; actually, they act as quality control
parameters in our system.
10.5.2 Test Results
We test our compression systems on a number of standard color images from the
USC-SIPI image database1 and the Kodak image library.2 Some results are presented
here to evaluate the compression ratio as well as reconstructed quality of our scheme.
In all tests, the quality parameter (QP) of JPEG coding method is set to 75, while the
QP of H.264 intra coding is set to 24. Bit-rate savings are listed in Table 10.1.
Figure 10.3 shows test image Lena and corresponding results of our JPEG-based
system. As mentioned before, the coded exemplars and the edge information are
denoted in Figure 10.3c. In this test, 10% additional structural blocks as well as
50% additional textural blocks are preserved. Based on the preserved blocks and
1 http://sipi.usc.edu/services/database/
2 http://r0k.us/graphics/kodak/

10.5 Experimental Results
213
Table 10.1 Bit-rate savings of our scheme compared to JPEG (QP is set to 75) and H.264 (QP is
set to 24).
Images
Additional Block Ratio
JPEG-Based Comparison
H.264-Based Comparison
Structural
Textural
JPEG Proposed Bit Saving H.264 Proposed Bit Saving
Jet
0.1
0.3
1.156
0.919
20.6%
0.985
0.880
10.7%
Lena
0.1
0.5
1.112
0.888
20.1%
0.993
0.869
12.5%
Milk
0.1
0.3
0.913
0.512
44.0%
0.616
0.415
32.7%
Peppers
0.1
0.3
1.217
0.965
20.8%
1.311
1.080
17.6%
Kodim02
0.1
0.2
1.058
0.709
33.0%
0.948
0.701
26.0%
Kodim03
0.2
0.3
0.895
0.608
32.1%
0.710
0.562
20.9%
Kodim05
0.1
0.3
2.027
1.719
15.2%
2.086
1.845
11.6%
Kodim07
0.1
0.5
1.079
0.802
25.7%
0.876
0.751
14.3%
Kodim11
0.2
0.3
1.368
1.047
23.5%
1.354
1.098
18.9%
Kodim19
0.2
0.5
1.276
0.915
28.4%
1.246
0.956
23.3%
Kodim20
0.1
0.4
0.897
0.638
28.9%
0.823
0.636
22.6%
Kodim23
0.2
0.3
0.821
0.567
30.9%
0.622
0.504
18.9%
assistant edge information, our presented structure propagation gives inpainting re-
sults in Figure 10.3d, and the ﬁnal reconstructed image after texture synthesis is
shown in Figure 10.3e. Compared with the reconstructed image shown in Figure
10.3f by baseline JPEG, our scheme saves 20% bits (as given in Table 10.1) but
presents similar visual quality. More comparison results in visual quality concerning
standard images can be found in Figure 10.9. It shows that up to 44% bits-saving
(shown in Table 10.1) is achieved by our scheme at similar visual quality levels,
compared to baseline JPEG.
In Figure 10.10, our proposed structure propagation method is evaluated. In this
test, we remove only structural blocks and use different approaches to recover them.
The details of partial images together with the assistant edge information are given
in the ﬁrst column. Then, results generated by the PDE-based diffusion [225], which
is the traditional solution to structural regions, are shown in the second column. This
method works well only for smooth regions and certain simple structures. In addi-
tion, image completion method presented by Sun et al. [217] is also tested in the third
column, only the user interaction is omitted. Since no edge information is utilized,
these two methods result in annoying distortion in most of the structural regions. In
the last column, we give our results that are accomplished by the help of edge in-
formation. It is clearly demonstrated that the assistant edges help greatly in structure
restoration, and thus empower the encoder to remove more blocks.
Furthermore, our JPEG-based system is also tested using the images in the Kodak
image library, which contains photographs of natural scenes at high resolution. The
comparison results on visual quality are shown in Figure 10.11, in which the top
row shows our results whereas the bottom row presents baseline JPEG results. It can
be observed that the visual quality of our resulting image is very similar to that of
the JPEG. The bits-saving of our JPEG-based system is indicated in Table 10.1. Our
method on average saves 27% bits for the ﬁve images shown in Figure 10.11 at the
similar visual quality levels.

214
10 Edge-Based Inpainting
Figure 10.9 Comparisons with baseline JPEG on test images Jet, Milk, and Peppers. From left to
right: removed blocks (black blocks) and assistant edge information; reconstructed image by our
scheme; reconstructed image by baseline JPEG.
To investigate the detailed bit-rate cost in our scheme, we list the percentage of
different coded elements in Table 10.2, from which we note that even different im-
ages will lead to different allocations of coded elements, the exemplar location infor-
mation as well as the edge information still costs only a little in overhead. Commonly,
the bits used to code the exemplar blocks occupy more than 90% of total bits cost.
However, it is still possible to further reduce the bits cost on edge information, tak-
ing into account the exemplar locations, or skipping those edges that can be inferred
from the exemplar blocks.
In Figures 10.12 and 10.13 we show the reconstructed images by our H.264-based
system in comparison with standard H.264 intra coding. Both results show similar
visual quality to each other, as in the JPEG comparisons. The bit-rate saving is also
noticeable, shown in Table 10.1, but not as much as the comparison with JPEG.
This is caused by two reasons. On the one hand, the H.264 intra coding is more

10.5 Experimental Results
215
Figure 10.10 Comparisons of different structure propagation approaches, zoomed-in partitions.
From left to right: removed blocks (black blocks) with assistant edge information; inpainting result
by PDE-diffusion [225]; inpainting result by patch-wise synthesis [217]; inpainting result by our
scheme. Note that only our scheme takes advantage of the edges.
Table 10.2 Percentage of different coded elements in our JPEG-based system (QP is set to 75).
Images
Exemplar locations Assistant edges Exemplar blocks
(arithmetic coder)
(JBIG)
(JPEG)
Jet
1.4%
6.4%
92.3%
Lena
1.4%
6.6%
92.0%
Milk
2.7%
5.2%
92.1%
Peppers
1.4%
6.7%
91.9%
efﬁcient than JPEG in coding performance, so the nonexemplar blocks, especially
the textural ones, will cost fewer bits in standard H.264 than in baseline JPEG, but
in our scheme the edge information still cost the same bits in either realization. On
the other hand, due to the complicated spatial predictions performed in the H.264
intra coding, the ﬁlling of only DC values for removed blocks is proved not good
enough, since it breaks the original spatial relations between neighboring blocks,
but for JPEG this ﬁlling of DC values seems enough since JPEG only conduct DC
prediction. Nevertheless, our scheme can still acquire up to 33% bit-rate savings
compared to the state-of-the-art H.264 intra coding.
10.5.3 Discussions
It can be observed that the ratio of additional textural exemplar has a big effect on
visual quality of the reconstructed images. As given in Table 10.1, for homogeneous

216
10 Edge-Based Inpainting
(a)
(b)
(c)
(d)
(e)
Figure 10.11 Comparisons with baseline JPEG on the Kodak Image Library. (a) kodim02; (b)
kodim03; (c) kodim05; (d) kodim07; (e) kodim19. The top row shows the reconstructed images by
our scheme and the bottom row shows the reconstructed images by baseline JPEG.

10.5 Experimental Results
217
(a)
(b)
(c)
(d)
Figure 10.12 Comparisons with standard H.264 intra-picture coding. (a) Jet. (b) Lena. (c) Milk.
(d) Peppers. The top row shows the reconstructed images by our scheme and the bottom row shows
the reconstructed images by standard H.264 intra coding.

218
10 Edge-Based Inpainting
(a)
(b)
(c)
Figure 10.13 Comparisons with standard H.264 intra-picture coding. (a) kodim11. (b) kodim20. (c)
kodim23. The top row shows the reconstructed images by our scheme and the bottom row shows
the reconstructed images by standard H.264 intra coding.
textural regions, such as the wooden door in kodim02 (Figure 10.11a), low exemplar
ratio is used to pursue high compression ratio; whereas for complex and irregular
textural regions, for example, ﬂowers and leaves in kodim07 (Figure 10.11d), high
ratio is preferred to ensure good visual quality. However, thanks to the given edge
information, the reconstructed quality of structural regions is less sensitive to the
additional structural exemplar ratio.
In addition, the improvement of our scheme in terms of compression ratio is var-
ious for different images. Commonly, the more complicated the image is, the less
gain we can provide. It is because when coding images with lots of details (such
as kodim05, Figure 10.11c), the extracted edge map usually contains miscellaneous

10.6 Summary
219
edges which makes many blocks as necessary exemplars. Thus, only a limited num-
ber of regions can be removed at encoder. However, in this case, 15% bits-saving is
still provided by our JPEG-based system without noticeable visual loss, as shown in
Table 10.1.
The computational complexity of our scheme is relatively higher than that of the
traditional coding schemes, since at the encoder side we perform extra edge extrac-
tion and exemplar selection, and at the decoder side we add the inpainting process. In
particular, the computation of the decoder is greatly related with the parameters used
in the inpainting, such as search range and patch size, which determine the necessary
number of SSD calculations. There are several previous works proposed to reduce
the computations of SSD, so as to accelerate the image synthesis [198,202,211], and
those methods can be adopted in our system as well.
The visual quality assessment is highly related to our work, that is, if we have a
good metric used to measure visual quality, we are able to not only better evaluate
our scheme, but also further improve the performance by rate-“distortion” optimiza-
tion, where “distortion” measures the perceptual quality in addition to the statistical
ﬁdelity. Unfortunately, we have not yet found such a good metric for our purposes.
Thus, for visual quality comparisons in our experiments, we always set the same
quality parameters for both the standard compression scheme and our inpainting-
based scheme. Thus, the exemplar regions will have the same quality (both subjec-
tively and objectively). Additionally, the restored regions still have acceptable visual
quality. Accordingly, the “comparable quality” or “similar visual quality levels” in-
dicates visually similar qualities, which are examined by human observations.
10.6 Summary
In this chapter, we present an image compression framework that adopts inpainting
techniques to remove visual redundancy inherent in natural images. In this frame-
work, some kinds of distinctive features are extracted from images at the encoder
side. Based on the obtained features, some regions of an image are skipped during
encoding, only to be recovered by the assisted inpainting method at the decoder side.
Due to the delivered assistant information, our presented framework is able to re-
move enough regions so that the compression ratio can be greatly increased. Our
presented inpainting method is capable in effectively restoring the removed regions
for good visual quality, as well.
Moreover, we present an automatic image compression system, in which edge
information is selected as the assistant information because of its importance in pre-
serving good visual quality. The main techniques we proposed for this compression
system, that is, edge thinning, exemplar selection, and edge-based inpainting, are also
addressed here. Experimental results using many standard color images validate the
ability of our proposed scheme in achieving higher compression ratio while preserv-
ing good visual quality. Compared to JPEG and H.264, at the similar visual quality
levels, up to 44% and 33% bits-saving can be acquired by our approach, respectively.

220
10 Edge-Based Inpainting
Further improvements of the current scheme are still promising. First, the assistant
information as well as the selected exemplars can be described and compressed into
bitstream in a more compact fashion. Second, extraction of the distinctive features
can be more ﬂexible and adaptable. In addition to edge information, there are other
candidates, such as sketch [196] and epitome [220, 221], which could be derived
from source images to assist the vision technologies and the compression methods
as well. Furthermore, image inpainting is still a challenging problem when some
kinds of assistant information are provided, into which we need to put more effort in
the future.

Chapter 11
Cloud-Based Image Compression
11.1 Introduction
The cloud is characterized by a large amount of computing resources, storage, and
data [226]. Imagining a cloud that collects a huge number of images, for example,
Google street view images [227], when you randomly take a picture with your phone
on the street, you can often ﬁnd some highly correlated images in the cloud that
were taken at the same location at different viewpoints and angles, focal lengths,
and illuminations. If you try to share the photo with friends through the cloud, it is
problematic to use conventional image coding (e.g., JPEG) that usually provides only
8:1 compression ratio [228]. It will consume a lot of precious power and network
bandwidth to transmit such a high-resolution and high-quality JPEG image. It would
be more convenient to take advantage of the cloud for compression and transmission
if there is a high probability of ﬁnding very similar images in the cloud.
However, state-of-the-art image coding, consisting of directional intra prediction
and transform [13,229], makes it difﬁcult to take advantage of highly correlated im-
ages in the cloud. Intra prediction uses decoded neighboring pixels from the same
image to generate predictions and then the pixels are coded by subtracting the pre-
dictions. It requires that the predictions used at the encoder and decoder must be
identical. The idea of intra prediction cannot be extended to external images. First,
when mobile devices compress images, it is difﬁcult to know which highly correlated
images can be found in the cloud. Second, the number of images in the cloud is huge,
so it is impossible to store them in mobile devices even partially. Third, the images
in the cloud are changing dynamically. The result is that it will be quite costly to
maintain image consistency between the cloud and all mobile devices.
Image search has been demonstrated to be a successful application on the Internet
[230]. By submitting the description of one image, including semantic content [231],
outline [232, 233], and local feature descriptors [234, 235], one can easily retrieve
many similar images. Near and partial duplicate image detection is a hot research
topic in this ﬁeld [234, 236, 237]. However, the purpose of image search is not to
221

222
11 Cloud-Based Image Compression
generate an image from search results. In fact, reconstructing a given image from
similar images is tougher than the image search itself.
Recent efforts have shed light on using a large-scale image database to recover,
compose, and even reconstruct an image [238–243]. In particular, Weinzaepfel et al.
are the ﬁrst to reconstruct an image by using local feature descriptors SIFT (Scale
Invariant Feature Transform) [244]. The follow-up work presented by Daneshi and
Guo [245] tries to reconstruct an image using SIFT and SURF (Speed Up Robust
Features) descriptors. However, it is a very challenging problem to reconstruct a
visually pleasing image using local feature descriptors only.
To solve the above problem, we propose describing input images by SIFT descrip-
tors and their down-sampled images. SIFT descriptors are extracted from the original
images and are used to retrieve near and partial duplicate images in the cloud and
identify corresponding patches. The down-sampled images play an important role
in making the reconstructed images visually pleasing. They are used to verify every
retrieved image patch and guide how to stitch image patches like the given images.
The down-sampled image is compressed by conventional image coding or intra-
frame coding in video. Using the correlation between the down-sampled image and
SIFT descriptors to compress SIFT feature vectors is another important technical
contribution. We ﬁrst compress locations, scales, and orientations of SIFT descrip-
tors and then use them to extract prediction vectors from the decoded down-sampled
image so that high-dimension SIFT vectors can be efﬁciently compressed by predic-
tion and transform coding.
The reconstruction of images is similar to that by Weinzaepfel et al. [244]. In
addition to using the down-sampled image, there are two other technical differences.
First, we adopt the approach proposed by Wu et al. [236] and Zhou et al. [237] for
partial duplicate image retrieval. It uses bundling features instead of a single feature
that signiﬁcantly increases the retrieval accuracy. Second, we use all SIFT descriptors
in a patch to estimate patch transformation with the RANdom SAmple Consensus
(RANSAC) algorithm [246] and a perspective projection model is used.
11.2 Related Work
11.2.1 Visual Content Generation
Image inpainting is the task of ﬁlling in or replacing a region of an image [206].
Many approaches have been proposed to learn from the known regions of an image
and then recover an unknown region from what has been learned. Hays and Efros are
the ﬁrst to propose image completion from a large-scale image database [238]. The
proposed approach uses GIST descriptors to retrieve images of similar scenes that
are applied to recover unknown regions of an image. Whyte et al. [239] later propose
retrieving images of the same scene from the Internet via a viewpoint invariant search
and replacing a user speciﬁed region.

11.2 Related Work
223
Image composition is a long-standing research topic. Here, we focus on the pa-
pers that deal with composing an image from a sketch based on a large-scale image
database. Eitz et al. propose the ﬁrst sketch-to-photo scheme [240, 241]. It takes a
sketch drawn by a user as input and retrieves correlated images from a sketch-based
search. Finally, it composes an image by graph cut and possion blending. Chen et al.
improve the scheme by taking both sketch and text labels as input [242].
Image composition can be viewed as the recovery of all objects in an image.
The objects are only represented by input sketch. It is a much tougher problem than
image inpainting and completion. Human interactions are required in current sketch-
to-photo schemes. Although it requires only a small number of bits to represent a
sketch and text labels, it is unacceptable to compress an image using interactions.
From the compression viewpoint, composed images must look like input images
in detail. But for an input sketch, image composition may generate quite different
results in color and texture. Taking CG (computer graphic) images as input is an
improvement over sketch [243].
Recently, Weinzaepfel et al. propose reconstructing an image from SIFT [244].
Although the purpose of this work is to study privacy in image search, it is actually
the ﬁrst to reconstruct an image from local feature descriptors. Daneshi and Guo
further use SURF descriptors and study the role of scale data in reconstruction [245].
Although content can be observed from reconstructed images, which is consistent
with the goal of image compression, reconstructed visual quality is terrible because
SIFT descriptors only provide local information.
To make image description and reconstruction meet the goals of image compres-
sion, it is important to select descriptions with a better trade-off between reconstruc-
tion distortion and data size. Sketch is a highly abstracted description that does not
contain any details. Local feature descriptors only provide local details. Neither of
them can guarantee that reconstructed images look like input images. To solve this
problem, we propose describing input images by local feature descriptors and their
down-sampled images.
11.2.2 Local Feature Compression
SIFT descriptors, proposed by Lowe [247], present distinctive invariant features of
images that consist of location, scale, orientation, and feature vectors. The scale and
location of SIFT descriptors are determined by maxima and minima of difference-
of-Gaussian images. One orientation is assigned to each SIFT descriptor according
to the dominant direction of the local gradient histogram. The feature vector is a 128-
dimension vector that characterizes a local region by gradient histogram in different
directions. Since SIFT descriptors have a good interpretation of the response proper-
ties of complex neurons in the visual cortex [248] and an excellent practical perfor-
mance, they have been extensively applied to object recognition, image retrieval, 3D
reconstruction, annotation, watermarking, and so on.

224
11 Cloud-Based Image Compression
Compression of SIFT descriptors has recently become a requirement of mobile-
based applications. One image usually has thousands of SIFT descriptors. Without
any data reduction and compression, the total size of SIFT descriptors may be even
larger than the image size. Ke and Sukthankar propose applying principal compo-
nents analysis (PCA) to greatly reduce the dimension of the feature vector [249].
Hua et al. propose linear discriminant analysis to reduce the dimension of the feature
vector [250]. Chandrasekhar et al. propose the use of transform coding of the feature
vectors [251]. Yeo et al. propose using coarsely quantized random projections of de-
scriptors to build binary hashes [252]. Jegou et al. decompose feature vectors into a
Cartesian product of low-dimension vectors that are quantized separately [253].
Several approaches propose directly reducing generated dimension of feature vec-
tors. SURF reduces the dimension to 64 [245] with a performance similar to the SIFT.
Compressed histogram of gradient (CHoG) descriptors, proposed by Chandrasekhar
et al. [254, 255], are designed for compression. It not only changes the generation
of the gradient histogram but also compresses it by tree coding and entropy coding.
It achieves a high compression ratio. However, from both practical adoptions and
recent evaluations in MPEG [256], SIFT descriptors are still a good choice in many
applications.
Compared with conventional image coding, the approaches for the compression
of SIFT descriptors are far away from being mature. Thus, several papers propose
compressing images ﬁrst and then extracting SIFT descriptors from decompressed
images. Makar et al. propose compressing image patches at different scales and then
generating SIFT descriptors from decompressed patches [257]. It performs better
than the direct compression of SIFT descriptors. However, if there are many patches
to code in an image, they will overlap each other and thus lead to low performance.
Chao and Steinbach analyze locations of SIFT descriptors in an image and assign
more bits to the regions with SIFT descriptors in JPEG compression [258].
All of the above approaches target the compression of SIFT feature vectors only.
Since we target reconstructing an image, what we need is not only SIFT feature vec-
tors but also SIFT locations, scales, and orientations because the latter tells us where
retrieved image patches are stitched. For the same reason, we also need the down-
sampled image. Since SIFT feature vectors have a strong correlation with the image,
the problem examined in this chapter is how to compress both the down-sampled im-
age and all information of the SIFT descriptors efﬁciently. The basic idea presented
in this chapter mimics inter-frame coding in video. The prediction of feature vec-
tors is extracted from the up-sampled decoded image. Residual feature vectors are
compressed after prediction by transform, quantization, and entropy coding.
11.2.3 Image Reconstruction
Many approaches are reported to use SIFT descriptors for image retrieval [259–263],
where SIFT feature vectors are quantized into visual words [260]. Quantization of

11.3 The Proposed SIFT-Based Image Coding
225
SIFT feature vectors makes image retrieval applicable to a large-scale database. But
it also reduces the discriminative power of SIFT feature vectors.
To solve this problem, Chum et al. propose expanding highly ranked images from
original query as new queries [260]. Philbin et al. propose quantizing a SIFT de-
scriptor to multiple visual words [262]. Jegou et al. introduce binary signatures to
reﬁne visual words [263]. Zhou et al. propose quantizing feature vectors to bit-
vectors [264]. For near and partial duplicate image retrieval, the geometric relation-
ship of visual words plays an important role. To utilize this information, Wu et al.
propose bundling a maximally stable region and visual words together [236]. Zhou
et al. propose using spatial coding to represent spatial relationships among SIFT de-
scriptors in an image [237].
Image alignment is a historic research topic and most key papers have been sur-
veyed by Talluri [265]. When the SIFT descriptors are available for two images that
are to be aligned, the most popular approach for estimating the transformation be-
tween them is RANSAC [246]. Torr et al. improve the algorithm by using the maxi-
mum likelihood estimation instead of the number of inliers [266]. Torr and Zisserman
further introduce a pyramid structure with ascending resolutions to improve the per-
formance of RANSAC [267]. Chum and Matas greatly speed up the approach by
introducing conﬁdent match [268].
11.3 The Proposed SIFT-Based Image Coding
The block diagram of the proposed cloud-based image encoder is shown in Figure
11.1. For the input image, a down-sampled image is ﬁrst generated and compressed.
SIFT descriptors are also extracted from the original image. The location, scale, and
orientation of every extracted SIFT descriptor guide to extract a feature vector as
Figure 11.1 The block diagram of the proposed cloud-based image encoder.

226
11 Cloud-Based Image Compression
Figure 11.2 The block diagram of the proposed cloud-based image decoder.
prediction from the decompressed image after up-sampling. Finally, the prediction
is subtracted from the feature vector. All components of SIFT descriptors are com-
pressed and transmitted to the cloud with the compressed down-sampled image.
The block diagram of the proposed cloud-based image decoder is depicted in
Figure 11.2. In the cloud, a server ﬁrst decompresses the down-sampled image and
SIFT data. By using the decompressed location, scale, and orientation of every SIFT
descriptor again, one prediction vector, exactly the same as that in Figure 11.1, is
extracted from the decompressed image after up-sampling. Then, the SIFT feature
vector is reconstructed by adding the prediction. To reconstruct the input image,
decompressed SIFT descriptors are used to retrieve highly correlated image patches.
For every retrieved image patch, we estimate the transformation and then stitch it
to the up-sampled decompressed image. Finally, a high-resolution and high-quality
image is obtained.
The block diagrams of the encoder and decoder shown in Figures 11.1 and 11.2
look very different from those of conventional image coding. In the following sec-
tions, we will discuss the details of the proposed cloud-based image coding.
11.4 Extraction of Image Description
In this chapter, a down-sampled image is used to describe the target for reconstruc-
tion in the cloud because it carries enough information, including outline, shape,
color, and objects. Furthermore, after an image is down-sampled, it can be efﬁciently
compressed by conventional image coding. The down-sample process is described
as follows
Ig(x,y) = ∑
T/2
n=−T/2∑
T/2
m=−T/2 I(x,y)G(x−m,y−n,σ).
(11.1)

11.4 Extraction of Image Description
227
Figure 11.3 Part of the Gaussian scale space with 3rd and 4th octaves. p indicates an extreme point
detected in D3,1. The rectangle in I3,1 indicates the corresponding SIFT descriptor.
G() is the discrete low-pass ﬁlter with the support T + 1. The kernel of the ﬁlter is
a Gaussian function with variance σ. After the ﬁltering, the down-sampled image is
generated as
Id(x,y) = Ig(rx,ry),r = 20,21,...,2n,
(11.2)
where n is an integer. Parameter r is the down-sampling ratio at one dimension. r is
often set as 2 or 4 in most current applications. In this chapter, r is set as 8 or 16.
Thus the down-sampling factor of images will be 64:1 or 256:1. It is much larger
than that to be considered in super-resolution so that our targeted compression ratio
can be achieved.
By repeatedly applying the ﬁltering Eq. (11.1) and down-sampling Eq. (11.2) with
r = 2 to the input image, a Gaussian scale space is generated, as shown on the left side
of Figure 11.3. The image used here comes from the INRIA Holiday data set [269].
It consists of a set of images {(I0,0,...,I0,K),...,(IN,0,...,IN,K)}. n (0 ≤n ≤N) is an
octave index in the space. It indicates that the input image is down-sampled n times.
k (0 ≤k ≤K) is a ﬁltering index in an octave. The differences of Gaussian images
are generated by Dn,k = In,k+1 −In,k. They are depicted on the right side of Figure
11.3.
In fact, all Dn,k constitute a Laplacian scale-space L(x) with x = (x,y,s)T. s is
a scale index and is deﬁned as s = n + (k −1)/(K −2) with K ≥3 and 1 ≤k ≤
K −2. Feature points are detected by maxima and minima in Dn,k. If one sample
is the largest or smallest of its 8 neighbors and 9 neighbors in Dn,k+1 and Dn,k−1
respectively, it will be a feature point in the image. As shown in Figure 11.3, p is
such a point at (xf0,yf0) and sf0 is the scale index of Dn,k.

228
11 Cloud-Based Image Compression
Let us deﬁne a vector x f0 = (x f0,yf0,sf0)T. The subpixel and ﬁner scale index to
x f0 are derived by ﬁtting a 3D quadratic to L(x). The Taylor expansion of L(x) at x f0
is described as follows
L(x) = L(x f0)+ ∂LT(x)
∂x
|x f0∆x+ 1
2∆xT ∂2L(x)
∂x2
|x f0∆x,
(11.3)
where ∆x = x−x f0. The accurate extreme point can be calculated by
x f = x f0 +(∂2L(x)
∂x2
|x f0)
−1 ∂LT(x)
∂x
|x f0.
(11.4)
x f is the location and scale index of a SIFT descriptor. Its scale value is σf = 1.6×
2(s f −n). In the Gaussian scale space, only the limited number of In,k is available.
Assume the scale index of In,k′ is the closest one to s f .
The orientation θ of the SIFT descriptor is calculated in a region of In,k′ around
(xf ,y f ). Its size is set as Wo ×Wo, where Wo = 9 × σf . Local image gradients dx
and dy are calculated using a centered derivative mask [−1,0,1]. The orientation
arctan(dy/dx) is evenly partitioned into 36 bins covering 360 degrees. A histogram
with 36 bins can be calculated by
θ(i) = ∑(dx,dy)∈Ωθ (i) ωθ(x,y) 2q
d2x +d2y,1 ≤i ≤36,
(11.5)
with Ωθ(i) = {(dx,dy)| π(i−1)
18
≤arctan( dy
dx ) < πi
18}. ωθ(x,y) is a weighting factor and
is deﬁned as a Gaussian function σθ = 1.5σf centered at (xf ,y f ). The histogram
is normalized and approximated to a polynomial function. The orientation at the
highest peak of the function is θ. In addition to the highest peak, if other peaks
exist with a value above 80% of the highest peak, multiple SIFT descriptors will be
generated for these peak orientations with the same location and scale index.
The feature vector is also extracted in a region of In,k′ around (x f ,yf ). Its size
is set as Wv ×Wv, where Wv =
√
2 × B × 3σ f .The region is ﬁrst rotated to θ, which
provides rotation invariance for the SIFT descriptors. The factor
√
2 guarantees a
complete rectangle region after rotation. The rectangle region is further partitioned
into uniform 4 × 4 sub-regions. Thus, B = 4 and each sub-region has 9σ2
f samples.
The gradient orientation is evenly partitioned into 8 bins. For a sub-region j, an 8-
dimension vector vj can be generated by
vj(i) = ∑(dx,dy)∈Ωv(i) ωv(x,y) 2q
d2x +d2y,1 ≤i ≤8,
(11.6)
with Ωv(i) = {(dx,dy)| π(i−1)
4
≤arctan( dy
dx ) < πi
4 }. ωv(x,y) is a weighting factor and
is deﬁned as a Gaussian function σv = B/2 centered at (xf ,y f ). Combining all 16
sub-regions, we can get one 128-dimensional vector v = (v1,...,v16). After normal-
ization, it is the feature vector of a SIFT descriptor.

11.5 Compression of Image Descriptors
229
Figure 11.4 An exempliﬁed prediction of SIFT descriptors. The right side is the Gaussian scale-
space generated from original image and several extracted SIFT descriptors are drawn there. The left
side is the Gaussian scale-space from the up-sampled decompressed image. The SIFT descriptors
in this space are extracted by using the same locations and orientations as those in the right side.
11.5 Compression of Image Descriptors
The down-sampled image is directly compressed by the intra-frame coding of HEVC
[14]. The core problem in our proposed scheme is how to efﬁciently compress SIFT
descriptors. As shown in Figure 11.4, the key idea presented in this chapter is to use
feature vectors extracted from a decompressed image as the prediction. It is simi-
lar to inter-frame coding in video. Two questions will be answered here. The ﬁrst is
whether the feature vectors extracted from the down-sampled image are good pre-
dictions. The second is how to efﬁciently use the predictions for the compression of
SIFT descriptors.
11.5.1 Prediction Evaluation
First, we observe that SIFT descriptors extracted from a down-sampled image are
different from those from the original image on location (xf ,yf ) and scale index s f .
This is reasonable because of the down-sampling process. Fortunately, we observe a
strong correlation between feature vectors extracted from the down-sampled image
and from the original image when scale index s f is larger than a certain value. We
will evaluate this correlation here.
A decompressed down-sampled image is ﬁrst up-sampled to the original resolu-
tion. A Gaussian scale space is generated by the approach described in Section 11.4.
For a SIFT descriptor S = (x f ,yf ,sf ,θ,v) extracted from the original image, we can

230
11 Cloud-Based Image Compression
Figure 11.5 The predicted MSE with regard to v in different octaves.
generate a prediction vector ¯v in the scale-space by using location (xf ,yf ), scale s f ,
and orientation θ. We evaluate the prediction using the normalized mean square error
(MSE)
e = ∑16
j=1 ∑8
i=1(vj(i)−¯vj(i))2
∑16
j=1 ∑8
i=1(vj(i))2
.
(11.7)
We take the same image used in Figures 11.3 and 11.4 as an example and draw
the curves of the average e of all SIFT descriptors in an octave versus octave index
as shown in Figure 11.5. We have tested other images in the data set and observed
similar results.
In Figure 11.5, we use different approaches to generate ¯v. The curve “Non-coded”
indicates that the down-sampled image is not coded. The curves “QP=x” indicate that
the down-sampled image is coded by HEVC with a given QP. In this experiment, the
image is down-sampled by r = 24, which corresponds to the scale index 4. We ﬁrst
observe that, when QP is equal to or less than 22, the curves are close to that without
compression. We also observe that, if the scale index is equal to or larger than 4, the
average e approaches zero. It indicates that feature vector ¯v can predict v very well.
When the scale index is 2 or 3, the prediction is good but has some errors. For the
scale indices smaller than 2, the prediction becomes considerably poorer because the
details have been removed by down-sampling.
11.5.2 Compression of SIFT Descriptors
Based on the above evaluation, we propose different strategies for compression of
SIFT feature vectors at different octaves. For octaves with scale index sf ≥log2 r,
feature vector v is not coded and instead ¯v is its reconstruction. For octaves with
scale index log2 r −s0 ≤s f < log2 r, where s0 is a constant and is usually set as

11.5 Compression of Image Descriptors
231
Figure 11.6 The coding process of SIFT descriptors in one image.
a small integer (e.g., 1 or 2), the residual vector v −¯v is coded by transformation,
quantization, and entropy coding. For the rest of the octaves, since the scale in-
dices are small, retrieved patches are too small to help image reconstruction. Those
SIFT descriptors are discarded directly. Selecting part of the SIFT descriptors is a
common method of mobile applications [254]. Although it may have a negative im-
pact on search results and reconstructed quality, it is a cost-effective way for practical
applications.
The coding process of SIFT descriptors is depicted in Figure 11.6. The locations
of all SIFT descriptors are compressed ﬁrst. We can determine the number of SIFT
descriptors based on the number of locations. All scale indices and orientations are
then compressed in raster scan order, respectively. According to the discussed coding
strategies and coded scale indices, we know which residual feature vectors should be
coded. Finally, they are compressed one by one.
Locations are important in the proposed scheme. First, they indicate where ori-
entations and feature vectors are calculated. Second, they are used to calculate the
transformation between retrieved image patches and the input image, which requires
high precision. Thus, they are only quantized into integers in the original resolution.
Assume that L = {(xf (1),yf (1)),...,(xf (M),yf (M))} is the set of all SIFT loca-
tions, where M is the total number. We ﬁrst generate a binary matrix Bl of the same
size as the input image, for which the element is deﬁned as
bl(x,y) =

1,if(x,y) ∈L
0,if(x,y) /∈L .
(11.8)
This matrix is compressed by binary arithmetic coding. For any bl(x,y) = 1, an inte-
ger is used to indicate the number of SIFT descriptors at (x,y). It is most likely one
but the maximum number allowed is four. It is compressed by small-alphabet arith-
metic coding. This approach is similar to the coding of a block of discrete cosine
transform (DCT) coefﬁcients in H.264 [13].
sf decides the region sizes to calculate θ and v, respectively. For every sf , its
octave index n is coded by 3 bits, and sf −n is quantized into 16 levels at precision
of 1/16 that is coded by 4 bits. θ decides rotation invariance of SIFT descriptors.
Every θ is quantized into 128 levels at precision of 45/16 ≈3 degrees and coded by
7 bits. Therefore, for a SIFT descriptor, sf and θ will take 14 bits.
In the compression of the residual feature vector, a binary string is ﬁrst generated,
where each bit indicates if one residual feature vector is zero or not after transform
and quantization. This string is compressed by binary arithmetic coding too. Given a
residual vector to code, we organize it as two 8×8 matrices

232
11 Cloud-Based Image Compression
Figure 11.7 Energy compaction of residual feature vectors.

B1 = (∆v1,...,∆v8)T
B2 = (∆v9,...,∆v16)T
(11.9)
where ∆vi = vi −¯vi is an 8-dimension row vector. Each matrix is transformed by
8 × 8 DCT. After quantization, most of the coefﬁcients are zero and nonzero coef-
ﬁcients, which are most likely small integers. Similar to approach, Eq. (11.8), DCT
coefﬁcients are converted to a binary matrix and a set of nonzero integers. One bit
indicates if one coefﬁcient is zero or not. An integer is the value of a nonzero coefﬁ-
cient.
Finally, we evaluate the DCT transformation in our scheme. The role of transfor-
mation is to compact signal energy. We randomly select 10,000 residual feature vec-
tors in our scheme and calculate their average energy distribution in a pixel domain
and DCT domain in Figure 11.7. We can observe that the energy after transformation
concentrates at low and middle frequency and is reduced at high frequency. But ob-
viously the energy compaction is not as good as in the residual image. Considering
the computation of transform, it should be further studied if the transformation can
be skipped in future.
11.6 Image Reconstruction
The decoding of SIFT descriptors is an inverse process to what we have described in
Section 11.5. Thus we skip the details of decoding and assume that both the down-
sampled image and SIFT descriptors have been decoded. This section will focus on
image reconstruction.
11.6.1 Patch Retrieval
The ﬁrst step for reconstruction is to retrieve highly correlated images. Assume that
the SIFT descriptors of all images in the cloud have been extracted with sf ≥2.

11.6 Image Reconstruction
233
Fifty million SIFT feature vectors are selected randomly and trained into 1 million
visual words by approximate k-means [260]. Every SIFT feature vector in the cloud
is quantized to a visual word if it has the minimum Euclidian distance to the visual
word. In an image, the region of a SIFT descriptor with a large-scale index often
partially or completely covers the regions of some SIFT descriptors with small scale
indices. We bundle them as a group. One image often has tens to hundreds of groups.
Every group is represented by a set of visual words and their geometric relationship
in the image.
Decoded SIFT feature vectors are quantized to visual words and organized into
groups too. Every group is matched with all groups in the cloud. The matching is
scored by the number of matched visual words and their geometric relationship. This
score is assigned to the image that contains the group. After all groups are matched,
several images with high sum scores are selected as highly correlated images for
reconstruction. More details can be found in Wu et al. [236] and Zhou et al. [237].
In order to guarantee matching precision, patch retrieval is operated on 128-
dimension vectors instead of visual words. All decoded SIFT descriptors are de-
noted as ˜Ω, and all of the SIFT descriptors in the selected images as Ω. Every de-
coded SIFT descriptor ˜S = (˜xf , ˜yf , ˜s f , ˜θ, ˜v) ∈˜Ωis independently matched with every
S = (x f ,yf ,s f ,θ,v) ∈Ω. The matching criterion between ˜S and S is deﬁned as
M ˜Ω,Ω( ˜S,S) =
(
1, if S = Smin,DS,S′
min/DS,Smin > C
0, otherwise.
(11.10)
The feature vector of SIFT descriptor Smin is the closest one to that of ˜S at the
square Euclidean distance, and the feature vector of S
′
min is the second-closest one.
DS,S′
min and DS,Smin are the Euclidean distances. C is a constant and usually set as 1.5.
From a matched SIFT descriptor S, we can get a patch PS from the retrieved images
according to its location, scale, and orientation. Similarly, we can get another patch
P˜S from the up-sampled decompressed image by ˜S.
11.6.2 Patch Transformation
The ﬂowchart of image reconstruction is similar to that by Weinzaepfel et al. [244].
In addition to the thumbnail being applied to verify every patch and guide the stitch-
ing, another key technical difference from that by Weinzaepfel et al. [244] is that
we use all SIFT descriptors located on a patch to estimate the transformation of the
patch. Therefore, the RANSAC algorithm is adopted [246] and a perspective projec-
tion model is used.
PS is a high-resolution patch and P˜S is a low-resolution one. It is difﬁcult to es-
timate an accurate transformation between them by pixel-based matching. Fortu-
nately, SIFT locations of PS and P˜S are extracted at high precision. Thus, feature-
based matching is adopted in the proposed scheme. The corresponding feature
points are detected by matching SIFT descriptors of PS and P˜S. The set of SIFT

234
11 Cloud-Based Image Compression
descriptors in PS can be found by ΩS = {S|S ∈Ω∩(x f ,yf ) ∈XS}, and the set in P˜S
by Ω˜S = { ˜S| ˜S ∈˜Ω∩(˜xf , ˜yf ) ∈X ˜S}. XS and X ˜S are the sets of samples in PS and P˜S,
respectively. Since the set size of ΩS and Ω˜S are much smaller than that of Ωand ˜Ω,
more pairs of matched SIFT descriptors can be found by the same criterion as Eq.
(11.10). They are written as
ΩS∩˜S = {( ˜S,S)|MΩ˜S,ΩS( ˜S,S) = 1, ˜S ∈Ω˜S,S ∈ΩS}.
In general cases, the transformation from PS to P˜S is deﬁned as a planar projection
H with eight parameters. The parameters are estimated by RANSAC [246]. Several
pairs of SIFT locations in ΩS∩˜S are randomly selected to calculate H. The remaining
pairs of SIFT locations are used to verify H. For a pair ( ˜S(i),S(i)), if the Euclidean
distance between (˜xf (i), ˜y f (i)) and the point of (x f (i),yf (i)) after H transformation
is smaller than a given threshold ε, it is called as an inlier to H; otherwise it is an
outlier. H and the number of inliers are recorded. This process is repeated. If a new
H has more inliers, H and the number of inliers are updated. Finally, we can get H
with the maximum number of inliers.
Besides the estimated H, we can immediately write another H0 from the location,
scale index, and orientation of ˜S and S if the transformation between PS and P˜S is a
combination of translation, rotation, and scaling
H0 =

˜sf /s f R T

,
(11.11)
where R =

cos( ˜θ −θ) −sin( ˜θ −θ)
sin( ˜θ −θ) cos( ˜θ −θ)

, T =

˜xf −xf
˜yf −yf

. In the proposed scheme, H0
is a rival transformation to H and we will select a better one during patch stitching.
Although H0 is usually not as accurate as H, it can be used to control error if H is
not correct. Furthermore, it is easier to obtain the H0 transformation because there is
no need to run RANSAC.
11.6.3 Patch Stitching
Both transformations H and H0 are applied to PS to get two patches PH
S and PH0
S . The
up-sampled decompressed image ˜Iu is used to guide the patch stitching. Two patches
PH
S and PH0
S
are matched with a region of ˜Iu centered at (˜xf , ˜y f ), respectively. The
match at every location is scored by the MSE between the patch and a corresponding
patch in the up-sampled decompressed image. The better patch and the best loca-
tion are decided by the minimum score. Finally, we get the patch ˆP˜S that will be
stitched. If the minimum score of ˆP˜S is larger than a threshold, we will discard this
patch because it most likely is not correct for ˜Iu. Since ˆP˜S may come from an im-
age with different illumination, it is blended to the up-sampled image ˜Iu by Poisson
editing [244,270].

11.7 Experimental Results and Analyses
235
Figure 11.8 The selected images as inputs in our experiments, denoted as “a” to “j” in the raster
scan order.
11.7 Experimental Results and Analyses
We use the INRIA Holiday data set [269], which has 1491 images in total, for our
experiments in this chapter. Images in this data set have a resolution of up to 8M
pixels. There are multiple images in the same scene captured at different viewpoints
and focal lengths. As shown in Figure 11.8, 10 images are selected as input images
and the rest are used as images in the cloud. For convenience, we denote them from
“a” to “j” in the raster scan order. We select the intra-frame coding in HEVC and
JPEG as the anchors. HEVC is an on-going video coding standard and is much better
than H.264 [14]. JPEG is the most popular image coding standard on the Internet,
although its coding efﬁciency is not good to date. Three schemes will be evaluated
for both compression ratio and visual quality.
11.7.1 Compression Ratio
In the proposed scheme, all images are down-sampled by 256:1 and compressed by
the intra-frame coding of HEVC. In order to get a high compression ratio, all coding
tools in HEVC are enabled. The quantization step for all images is set as 22. All
SIFT descriptors in the octaves with scale indices more than 2 are compressed. Since
there are too many SIFT descriptors in the second octaves, only part of the SIFT
descriptors, which can be predicted better, is selected for compression. The residual
feature vectors are transformed and quantized. The quantization step is set as 30. It
is a normal parameter for the inter-frame coding.
The experimental results are listed in Table 11.1. The average size of down-
sampled images is 6.14 KB after HEVC compression. Each image has a different
number of SIFT descriptors ranging from 227 to 722, which is dependent on im-
age content. The average number of SIFT descriptors per image is 500. The average
size of all SIFT descriptors per image is 1.82 KB after compression by the proposed
scheme. It is equivalent to 30 bits per SIFT descriptor on average.
The total size and compression ratio of every image are listed in Table 11.2. For
the proposed scheme, the total size is 7.96 KB on average and the corresponding
compression ratio is 1885:1. The maximum compression ratio is as high as 4000:1.

236
11 Cloud-Based Image Compression
Table 11.1 Image sizes, compressed HEVC sizes, and compressed SIFT sizes in the proposed
scheme.
Image
Size
HEVC No. SIFT Bits per
(KB) SIFT (KB)
SIFT
A
2816×2112
2.80
464
1.63
29
B
2560×1920
7.75
722
2.45
28
C
3264×2448 10.13
679
2.36
28
D
2048×1536
5.26
415
1.55
31
E
1536×2048
4.95
315
1.08
28
F
2048×1536
3.16
390
1.33
28
G
3264×2448
9.48
542
1.91
29
H
1536×1152
2.41
227
0.78
28
I
3264×2448 10.36
654
2.27
28
J
2048×1536
5.09
593
2.82
39
Ave.
4.69M
6.14
500
1.82
30
Table 11.2 Comparisons with intra-frame coding of HEVC and JPEG.
Image
JPEG
HEVC
Proposed
Size Ratio Size Ratio Size Ratio
a
39.0
447
4.67 3731 4.43 3933
b
52.7
273 17.62 817 10.20 1411
c
66.1
354 22.07 1061 12.49 1874
d
27.0
341
8.65 1065 6.81 1353
e
35.4
260 13.13 702
6.03 1528
f
24.6
375
5.93 1554 4.49 2052
g
64.4
363 19.39 1207 11.39 2055
h
14.3
362
3.85 1347 3.19 1626
i
72.2
324 28.71 815 12.63 1853
j
32.7
282 10.36 890
7.91 1164
Ave. 42.84 338 13.44 1319 7.96 1885
JPEG results are generated by the software IrfanView. We try to make JPEG ﬁle
sizes close to that of the proposed scheme. Even setting the lowest quality, the ﬁle
size is still 42.84 KB on average and the corresponding compression ratio is only
338. HEVC results are generated using the reference software 4.0 by setting the
quantization step as 51. The ﬁle size is 13.44 KB on average and the compression
ratio is 1319. Even for HEVC, the most efﬁcient image coding scheme to date, the
ﬁle size is still larger than that of the proposed scheme by 70% on average.
11.7.2 Visual Quality
For the generated ﬁles listed in Table 11.2, we evaluate their visual qualities here.
Since the coding artifacts are quite different even for JPEG and HEVC, it is hard
to evaluate them by an objective criterion. We adopt the double-stimulus continuous

11.7 Experimental Results and Analyses
237
Table 11.3 Average scores and PSNR for JPEG, HEVC, and the proposed scheme.
Image
JPEG
HEVC
Proposed
Score PSNR Score PSNR Score PSNR
a
1
25.06
1.79
31.26
4.42
29.65
b
1.16
20.62
3
21.55
2.47
18.29
c
1.16
23.28
3.42
26.70
4.32
17.50
d
1
22.58
2.42
24.93
4.21
20.74
e
1.05
20.62
1.63
21.24
3
19.27
f
1
24.19
2.16
26.65
2.58
22.78
g
1.11
22.04
2.26
25.01
3.11
20.77
h
1.16
22.66
2.32
26.54
4.05
23.35
i
1
20.28
2.26
21.89
4.21
18.51
j
1.37
22.61
2.95
25.00
3.74
22.36
Ave.
1.1
22.39
2.42
25.08
3.61
21.32
quality-scale (DSCQS) method [271] for subjective testing. Twenty undergraduate
students without any coding experience served as assessors. The decoded images are
displayed on an ultra HD monitor in random order. Image quality is scored from 1 to
5, indicating bad, poor, fair, good, and excellent.
The average scores for every image are listed in Table 11.3. The scores of JPEG
and HEVC are only 1.1 and 2.42, respectively. These indicate that the quality of
JPEG and HEVC are bad and poor. The average scores of the proposed scheme are
3.61. This indicates that the visual quality is between fair and good. We note that
the scores of 50% for the images are more than 4 in the proposed scheme. In other
words, their quality is close to the excellent grade even after the thousand-to-one
compression. Images “b” and “f” have low scores for the proposed scheme because
there are several reconstruction artifacts. The PSNR is also given in Table 11.3 as a
reference.
Limited by the pages, only the reconstructed results of images “a,” “e,” and “f”
at different scores are shown in Figure 11.9. Each group of results in Figure 11.9
consists of two rows. In the ﬁrst row, images from left to right are the original image
and the results of the proposed scheme, HEVC, and JPEG. We can observe that the
results of the proposed scheme are very consistent with the input images. The second
row shows the details in a corresponding region indicated by rectangles. The artifacts
of HEVC and JPEG can be observed clearly. However, the results of the proposed
scheme present the details clearly. As we have pointed out, the score of image “f” is
low in the proposed scheme. Although the result looks like the original image, some
people in front of the building are not constructed well as marked by the circles. This
is the typical artifacts in the proposed scheme.
11.7.3 Highly Correlated Image
Highly correlated images in this chapter mainly indicate those images taken of the
same scene with different viewpoints, focal lengths, and illuminations. In fact, the

238
11 Cloud-Based Image Compression
Figure 11.9 The reconstructed results of images “a,” “e,” and “f.” Each group of results consists of
two rows. In the ﬁrst row, images from left to right are original images and the results of the pro-
posed scheme, HEVC, and JPEG. The second row is the details in a corresponding region indicated
by the rectangles. The circles indicate the reconstructed artifacts in the proposed scheme.
proposed scheme can be extended to the cases of partial duplicate images [236,237],
where scenes are different but some regions and objects are similar. Highly corre-
lated images play an important role in the proposed scheme. We show four retrieved
images for images “a,” “e,” and “f” in Figure 11.10. We can observe that the image

11.7 Experimental Results and Analyses
239
Figure 11.10 Highly correlated images retrieved for images “a,” “e,” and “f.”
Figure 11.11 Images in the ﬁrst row are the new correlated images for image “a.” The second row
is the result of the proposed scheme in this case. The rectangle indicates the stitched one patch.
retrieval performs well and highly correlated images can be found and ranked ﬁrst.
For images “a” and “e’,’ one and two images retrieved are non-correlated. But since
their patches have big MSE values with the decompressed image, they are ﬁnally
discarded and do not result in bad results.
What is the result of the proposed scheme if there are no highly correlated images
retrieved? For image “a,” we remove the ﬁrst four correlated images in Figure 11.10
from the data set and then reconstruct it. Four new retrieved images are shown at
the top of Figure 11.11 and obviously they are not correlated to image “a.” The re-
constructed result is also shown in Figure 11.11. We can observe that all patches
except for one are removed because of the large MSE values of the image. The
patch is stitched in the region marked by a rectangle. It does not introduce annoy-
ing artifacts. The ﬁnal result is the up-sampled decompressed image that looks very
blurry.

240
11 Cloud-Based Image Compression
11.7.4 Complexity Analyses
The client needs to encode and decode a down-sampled image, extract SIFT de-
scriptors, and encode SIFT descriptors. Since the size of the down-sampled image
is small, its encoding and decoding are simple. As discussed in Section 11.5, the
proposed SIFT compression has low computation without training, complicated pre-
diction, and large size transform. The highest computation is needed for building a
pyramid image space and extracting SIFT feature vectors. Since two octaves with
small scales are skipped, the computation has been reduced greatly. In our current
implementation, it usually takes 5−7 seconds in a single-core phone at 1 GHz fre-
quency. This time is expected to decrease to 1 second or less in a four-core phone
that is now available on the market.
Computation in the cloud is a key problem. Correlated images are retrieved by vi-
sual words, and high correlation patches are retrieved only in several selected images.
Their computations are not high. However, both the RANSAC [246] algorithm and
Poisson editing have high computation. In our current implementation, it often takes
a minute or more to reconstruct an image. The RANSAC algorithm of every patch is
very suitable for parallel computation. We can take hundreds of cores in the cloud to
calculate projection models of all patches simultaneously. Poisson editing can only
be performed one by one. Recently, we have found that it can be approximated by
the membrane interpolation [272]. In our parallel implementation, the reconstruction
time has been reduced to several seconds [273].
11.7.5 Comparison with SIFT Feature Vector Coding
Finally, we evaluate the proposed compression approach in the image retrieval sce-
nario by comparing it to CHoG [254,255]. The Zurich Building Database (ZuBuD)
is used [274]. It contains 1005 images of 201 buildings in Zurich of resolution
640 × 480. It also has 115 query images of resolution 320 × 240. The experiment
is set up strictly according to the method by Chandrasekhar et al. [255].
For the proposed scheme, SIFT descriptors are extracted from the original query
images. Thumbnails are generated by 2:1 down-sampling in each dimension and
then compressed by HEVC intra-frame coding. SIFT locations, scales, orientations,
and residual vectors are compressed by the approach described in Section 11.5. The
compressed description size is adjusted by changing the quantization steps for the
thumbnail from 20 to 50 at an interval 5. At the same time, the number of SIFT
descriptors varies by changing scale indices and predicted MSE values. As shown
in Figure 11.12, seven compressed descriptions have sizes from 0.33 kB to 8.62
kB, including the thumbnails. The vertical axis is the recall, which is deﬁned as the
percentage of query images correctly retrieved [255].
The SIFT curve shows the results without any compression. It should provide
the best retrieval results by using SIFT descriptors. As shown in Figure 11.12, point
D is the un-compressed version of points D1, D2, and D3, which have different

11.8 Further Discussion
241
Figure 11.12 The recall rate versus query size of SIFT, CHoG, and the proposed compression
approach.
quantization steps. We can observe that the proposed compression only causes up to
1.8% recall loss at point B1 as compared with point B. But the query size is reduced
more than 90%. The curve of CHoG in the research by Chandrasekhar et al. [255]
is drawn in Figure 11.12, too. We can observe that they have similar recall at similar
query sizes. Both of them have a largely decreasing slope on recall when query sizes
become too small. One thing that we have to mention here is, that for the proposed
compression, the query includes the thumbnail that can be potentially applied to
improve retrieval quality and re-rank searching results.
11.8 Further Discussion
11.8.1 Typical Applications
The proposed cloud-based image coding is not a replacement of the conventional im-
age coding. The conventional image coding is mature on how to use the correlations
of pixels in the same image for compression. Conventional image coding is more
suitable and stable for image archives. Only when the large-scale database of images
is available in the cloud, can the proposed coding scheme exploit the correlations
of external images and signiﬁcantly reduce the compressed data size. Therefore, the
proposed scheme is more suitable for image sharing and browsing on the Internet.
What are the typical applications for cloud-based image coding? Two are discussed
here.
(1) Image Sharing — When you visit a sightseeing place, you take some images
by your mobile phone and want to share them to your friends immediately. Each
image is 4−5 MB in JPEG. It will take minutes to transmit an image over current
wireless networks and also consume a lot of precious power on your mobile phone.
One current solution is that the images are signiﬁcantly down-sampled and sent to
your friends. But these images look small and unimpressive because many details

242
11 Cloud-Based Image Compression
are lost. If you select to transmit the down-sampled images to the cloud with an ad-
ditional several kilobytes of compressed SIFT descriptors, the high-quality and high-
resolution images can be reconstructed in the cloud and shared with your friends in
JPEG over broadband networks.
(2) Image Search — Modern image search engines have stored thumbnails for
result presentation and visual words for content-based matching. SIFT descriptors of
crawled images can be stored efﬁciently by using the proposed compression. Search
engines can deﬁnitely do more with SIFT descriptors. When users point to a thumb-
nail that they are interested in, a high-quality and high-resolution image can be gen-
erated and presented to them immediately without jumping to the host webpage.
Who will be the service providers? Many companies have owned large-scale
databases of images, like Google Street View Image, Microsoft Bing, Yahoo Flickr,
Apple iCloud, and so on. They are potential service providers. The network providers
are also possible to provide this service. They have the advantage of being able
to distribute large-scale databases of images in network nodes and wireless base
stations.
11.8.2 Limitations
The large-scale database of images plays the most important role in the quality of re-
constructed images. For an input image, if we cannot ﬁnd highly correlated images in
the cloud, the scheme cannot output a good quality reconstruction. Although mobile
photo sharing is discussed as an exempliﬁed application, it is not clear which prac-
tical applications in the cloud can always have highly correlated images available. It
will limit the usage of the proposed scheme.
There are some technical limitations to the proposed scheme too. Images in some
complicated scenes may be difﬁcult to reconstruct. For example, there are often peo-
ple in front of famous places. Although the background can be reconstructed from
the correlated images, it is hard to accurately reconstruct a face from other images.
One solution for this issue is to segment the foreground by several user interactions
and then compress the foreground by image compression. In addition, an image in a
dynamic scene (e.g., billboards, windmills, etc.) may also be difﬁcult to reconstruct.
11.8.3 Future Work
The description of the input image needs to be further studied. If the dimension of
feature vectors can be reduced as in SURF and CHoG, it will save more bits for resid-
ual feature vector coding. In addition, the optimization between distortion and rate
should be carefully studied for coding SIFT descriptors. Some coding parameters are
set empirically.
Last but not least, retrieved image patches are stitched into the up-sampled de-
compressed image one by one. This can be improved twofold. First, for complicated

11.9 Summary
243
scenes, it is difﬁcult to ﬁnd a correct transformation model for a large patch. It can
be split into multiple pieces and each piece is reﬁned before stitching. Second, a
patch replaces a corresponding region completely in the proposed scheme. Since the
decompressed image is more reliable in low frequencies, the patch stitching can be
improved in the frequency domain.
11.9 Summary
This chapter presents a cloud-based image coding scheme, where images are de-
scribed by SIFT descriptors and down-sampled images. The SIFT descriptors are
compressed by predicting from the corresponding SIFT descriptors extracted from
the down-sampled images and transform coding. Finally, high-quality and high-
resolution images are reconstructed from a set of images in the cloud by the proposed
description. Experimental results show that the proposed scheme not only achieves
1885:1 compression on average but also achieves high subjective scores. The recon-
structed quality in half of the testing images is close to that of the original images.


Chapter 12
Compression for Cloud Photo Storage
12.1 Introduction
During the last decade, there was an explosive growth of images and photos on the
Web. Facebook announced in October 2012 that they have hosted 220 billion pho-
tos, and further, the amount was increasing at several hundred million every day [1].
Chinese Tencent announced in August 2012 that they have hosted over 150 billion
photos [275]. Microsoft also announced in October 2012 that its cloud storage Sky-
Drive has hosted 11 billion photos [276]. Let us assume the size of every photo is
2.5 MB on the average, 200 billion photos needs 500,000 1T hard disks to store. For
data safety, multiple copies (e.g., three copies) of every photo are needed and hard
disks have to be replaced every three years. The current price of 1T hard disk is about
$60 and thus the storage cost will be about $90 million per three years even without
considering the cost of power supply and air conditioning. As a result, there is an
urgent need for more efﬁcient compression for cloud photo storage.
Images in cloud storage are currently stored as individual JPEG ﬁles. Although
it is easy to access every image, it is inefﬁcient for storage because the correlation
among images is not used at all. To solve this problem, one class of approaches
is proposed generating a representative signal (RS) (e.g., an average image) from
correlated images and then compressing all correlated images by subtracting the
RS [277–280]. But such one-to-multiple prediction structure is not efﬁcient because
other coded images cannot be used for prediction. The other class of approaches is
proposed organizing correlated images as a pseudo sequence and then compressing
the sequence like video [281–285]. This prediction structure looks good. But these
approaches mainly use global transformation and block-based motion compensation,
and ignore the fact that pseudo sequences are not natural video.
Figure 12.1a shows several frames in the Foreman video sequence, which present
slow changes and strong correlation. However, the changes among images in photo
album are large and complicated, as shown in Figure 12.1b. First, although they are
taken in the same scene, the location, viewpoint, and focus length may be very dif-
ferent. Furthermore, the objects in the scene have different distances to camera lens.
245

246
12 Compression for Cloud Photo Storage
(a) Video frames
(b) Images in photo album
Figure 12.1 Video frames and images in photo album.
Thus the geometric deformation between two images is difﬁcult to be represented by
one global model. Second, they may be taken at different time and thus there exist
different illuminations and shadows. It is difﬁcult to be compensated by block-based
motion compensation. Third, images often contain foreground objects, which cover
part of the background. In a word, although the images in Figure 12.1b look similar
visually, the correlation among them is weak if we evaluate it pixel by pixel even with
motion compensation. Obviously, it requires the advanced approaches and models to
detect and exploit this kind of correlation.
In this chapter, we present a novel compression scheme based on local feature
descriptors. By extracting local feature descriptors of images in photo album, we can
match the corresponding regions among images, no matter what scale, rotation, and
illumination are. It is one content-based matching similar to the classical content-
based image retrieval [286, 287]. The correlated images and their correlation are
then described by a directed graph. In order to decide the prediction structure, we
further convert the graph to a minimum spanning tree (MST) by minimizing the pre-
dictive cost. According to the tree, the correlated images are organized into a pseudo
sequence. One of the important contributions in this chapter is that the proposed
scheme is based on the distance between local feature vectors instead of pixel val-
ues.
To characterize the complicated correlation between images, we further present
a feature-based three-step prediction. First, we formulate the deformations of two
images as a minimum energy problem based on the locations of matched local fea-
ture descriptors. By solving the minimum problem, we can automatically derive the
number of deformations and the parameters of every deformation. Therefore, multi-
ple prediction images can be generated from one reference image. It well solves the
local region deformation problem between two images. Second, the feature-based
linear photometric transformation is developed to compensate illumination changes
from one prediction image to the current image. Finally, the advanced block-based

12.2 Related Work
247
motion compensation in HEVC [14] is used to handle the displacements of blocks.
Experimental results show that our scheme can better exploit the inter-image corre-
lation and its compression ratio is 10 times that of individual JPEG compression at
the same quality.
12.2 Related Work
In this section, we will review the related work on image set compression and local
feature descriptors.
12.2.1 Image Set Compression
The ﬁrst class of approaches for image set compression is to generate a RS from
all correlated images and then compress every correlated image by subtracting
the RS. The research in this line focus on how to generate the RS and the pro-
posed approaches, which include Karhunen-Lo`eve transform (KLT) [277], the cen-
troid method [278], the max-min differential method [279], the max-min predictive
method [279], and the low-frequency template [280]. This class of approaches has
two constraints. First, they are only applicable for highly correlated images (e.g.,
medical and satellite images) because good alignment among images is required to
generate the RS. Second, the one-to-multiple prediction structure is not efﬁcient be-
cause other coded images except for the RS cannot be used for prediction.
The second class of approaches for image set compression relaxes the alignment
constraint. They assume that an image set contains well separated clusters of corre-
lated images. Each cluster of correlated images is described by a minimum spanning
tree (MST). Cheng et al. evaluated the hierarchical clustering methods for image
set compression [281]. Chen et al. optimized the coding order of correlated im-
ages by minimizing the predictive cost [283]. Lu et al. adopted MPEG-like block-
based motion compensation to exploit the correlation among images [282]. Au et al.
introduced global motion compensation before the block-based motion compensa-
tion [284]. Zou et al. introduced the advanced block-based motion compensation in
HEVC to image set compression [285].
The above approaches assume that correlated images in photo album have the
similar correlation as video sequences. As the comparison shows in Figure 12.1, it
is obviously not true. It is well known that global motion compensation only han-
dles consistent motion and block-based motion compensation mainly handles 2D
displacement. Therefore, these approaches cannot fully exploit the correlation of im-
ages in photo album. To well model local geometric deformation and photometric
transformation, we present the feature-based matching for better aligning correlated
images and the feature-based multi-model for compensating complicated local mo-
tion. It is an efﬁcient way to exploit the correlation in photo album.

248
12 Compression for Cloud Photo Storage
12.2.2 Local Feature Descriptors
Local feature descriptors are interesting points in images that can be extracted to
identify and represent the images. The most famous local feature descriptors are
scale-invariant feature transform (SIFT), proposed by Lowe [247]. SIFT descriptors
present distinctive invariant features of images that consist of location, scale, orienta-
tion, and feature vector. The scale and location of SIFT descriptors are determined by
maxima and minima of difference-of-Gaussian images. One orientation is assigned
to each SIFT descriptor according to the dominant direction of the local gradient
histogram. The feature vector is a 128-dimension vector that characterizes a local
region by gradient histogram in different directions. Since SIFT descriptors have a
good interpretation of the response properties of complex neurons in the visual cor-
tex [248] and an excellent practical performance, they have been extensively applied
to object recognition, image retrieval, 3D reconstruction, annotation, watermarking,
and so on.
In addition to SIFT descriptors, some other local descriptors are proposed. SURF
(Speed Up Robust Feature) descriptors reduce the dimension to feature vectors to
64 with a performance similar to the SIFT [288]. CHoG (Compressed Histogram
of Gradient) descriptors, proposed by Chandrasekhar et al. [254, 255], are designed
for compression. They not only change the generation of the gradient histogram but
also compress them by tree coding and entropy coding. The successes on apply-
ing local feature descriptors to image retrieval have demonstrated the advantage of
feature-based content matching in dealing with scale and rotation variations. Image
retrieval can be regarded as an attempt to ﬁnd inter-image correlation in a large-scale
image set. Therefore, different from traditional pixel-based similarity measures, the
proposed feature-based approach is more robust in evaluating the correlation among
images in photo album.
Furthermore, with matched local feature vectors, the locations of correspond-
ing points between two correlated images can be found. The deformation between
them can be thus estimated by the RANdom SAmple Consensus (RANSAC) algo-
rithm [246]. It is called the feature-based image alignment, which has been surveyed
by Talluri [265]. Following this idea, we further take multiple local deformations into
account and present the multi-model approach to estimate multiple geometric defor-
mations and generate multiple predictions from one reference image. It can better
exploit the correlation between images.
Before this chapter, feature-based image compression schemes have been pro-
posed [273, 289]. Instead of compressing images pixel by pixel, they proposed
describing images by thumbnails and local feature descriptors. Images are recon-
structed from a large-scale image set via the compressed description. Although they
have high compression ratio, they cannot achieve a good reconstruction under the
MSE (mean square error) criteria. However, our scheme in this chapter uses local
feature descriptors only to estimate geometric deformations and photometric trans-
formations. The compression scheme still consists of prediction, transform, and en-
tropy coding, like existing image and video coding schemes.

12.3 Proposed Scheme
249
Photo album
Compressed 
album
Clustering
Generation of 
prediction 
structure 
(Feature-
based MST)
Feature-
based three-
step 
prediction
Residue 
coding
Correlated image set 1
Correlated image set n
...
Correlated image set 2
Figure 12.2 The proposed scheme. The solid boxes indicate the technologies studied in this chapter.
The dashed boxes indicate using the existing technologies.
12.3 Proposed Scheme
Our scheme is shown in Figure 12.2. Photo albums often contain images taken in
different scenes. They can be clustered by the approaches of content-based image
retrieval especially for near and partial duplicated image retrieval [234, 236, 237].
Most of these approaches also use local feature descriptors. In this chapter, we do
not discuss the clustering problem and assume that correlated image sets have been
clustered in photo albums.
For a set of correlated images, we have to ﬁrst determine an efﬁcient prediction
structure for compression. It can be formulated as a graph optimization problem.
Each image is regarded as a node in a directed graph. The weighted edges are set
by the predictive cost using one image to predict another. Therefore, the prediction
structure with the minimum predictive cost can be found by searching the minimum
spanning tree (MST) of the graph [290]. Since the feature-based measure is more
robust to geometric deformations (e.g., zooming, rotation, and ﬂection) and is also
less sensitive to illumination changes and noise than the pixel-based measure, in our
scheme edges are weighted by SIFT feature-based distances. Based on this graph,
we search the SIFT feature-based MST (SFMST in short) by minimizing the overall
SIFT feature-based distance.
According to the SFMST as shown in the middle of Figure 12.2, the inter-image
prediction is performed from the root to each leaf node. Here, we present a feature-
based three-step prediction to exploit inter-image correlation, which consists of SIFT
feature-based geometric deformation, SIFT feature-based photometric transforma-
tion, and block-based motion compensation. Speciﬁcally, in the SIFT feature-based
geometric deformation, we adopt multiple models to characterize the correlation of
local corresponding regions between two images. With the models, multiple predic-
tions are generated from the same reference image for coding the current image. In
the SIFT feature-based photometric transformation, we estimate a linear model based
on matched SIFT descriptors in corresponding regions to reduce intensity differ-
ences. Finally, the advanced block-based motion compensation in HEVC is adopted
to further reduce the displacement of blocks and improve prediction accuracy.

250
12 Compression for Cloud Photo Storage
After the proposed feature-based prediction, the residue images are further com-
pressed by transform and entropy coding to generate the compressed data. In our
implementation, these two modules come from HEVC. In the following sections,
we will mainly discuss the proposed feature-based prediction structure and feature-
based inter-image prediction in details.
12.4 Feature-Based Prediction Structure
Let us assume a correlated image set as I = {I1,I2,··· ,IN}, where N is the num-
ber of images in the set. For every image In, there is a set of SIFT descriptors as
Fn = { fn(1), fn(2),··· , fn(Kn)}. In other words, there are Kn SIFT local feature de-
scriptors detected and extracted in the image In. Every SIFT local feature descriptor
is expressed as fn(k) = {pn(k),sn(k),on(k),vn(k)}. pn(k) = {xn(k),yn(k)} is the lo-
cation of fn(k) in the image. vn(k) is the 128 dimensional feature vector, which is
a histogram representing the directional gradients in the local region around pn(k).
sn(k) is the scale of fn(k). It indicates the image area covered by fn(k). on(k) is the
dominant gradient orientation of fn(k).
12.4.1 Graph Building
For the correlated image set, we can build a directed graph G = {I ,E }, as shown
in Figure 12.3a. In the example, there are four correlated images I = {I1,··· ,I4}.
Every node represents an image. The key problem in this graph is how to decide the
edge weights ei,j ∈E , which indicates the predictive cost using image Ii to predict
image Ij. In general, the stronger the correlation between Ii and Ij, the smaller the
predictive cost ei,j is. But it is decided by how well the correlation can be utilized.
The measure for evaluating the correlation in pixel domain is often deﬁned as
DI(Ii,Ij) = 1
M ∑∥Ij −H(Ii) ∥p .
(12.1)
∥· ∥p is the p norm and M is the number of pixels in Ij. H(·) is the transformation
from image Ii toward image Ij. For highly correlated images, Ii and Ij are aligned well
and thus Eq. (12.1) can be simpliﬁed as ∑∥Ij −Ii ∥p /M. For the complicated cases,
some corresponding points in Ii and Ij have to be found and then the transformation
H(·) is estimated based on the corresponding points. It is the so-called pixel-based
alignment, which is readily affected by selected corresponding points and noise.
Therefore, we propose using the local feature descriptors to estimate the corre-
lation among images. Let us ﬁrst deﬁne the distance of any two SIFT local feature
descriptors fi(ki) and f j(kj) as

12.4 Feature-Based Prediction Structure
251
(a)
(b)
Intra
Inter
Inter
Inter
(c)
Figure 12.3 Feature-based prediction structure. (a) Directed graph; (b) feature-based minimum
spanning tree; (c) prediction structure.
Df (fi(ki), f j(kj)) =
1
| vj(kj) | ∥vj(kj)−vi(ki) ∥2 .
(12.2)
| vj(kj) | is the dimension of vj(kj). For a given SIFT local feature descriptor f j(kj)
in Ij, assume that fi(ki) and fi(km) are the closest and second closest SIFT local
feature descriptors in Ii, namely,
fi(ki) = argmin∀fi(k)∈FiDf (fi(k), f j(kj)),
fi(km) = argmin∀fi(k)∈Fi& fi(k)̸=fi(ki)Df (fi(k), f j(kj)).
We can say fi(ki) and f j(kj) as the matched SIFT local feature descriptors, if
Df (fi(ki), f j(kj)) < αDf (fi(km), f j(kj)).
(12.3)

252
12 Compression for Cloud Photo Storage
α is a constant and is usually set as 0.6. In other words, the distance of f j(kj) respect
to the second closest SIFT local feature descriptor fi(km) in Ii should be larger than
a certain ratio compared with the distance respect to the closest SIFT local feature
descriptor fi(ki).
By using the above matching approach, we can ﬁnd all matched SIFT local feature
descriptors in Ii and Ij, which is expressed as
Fi,j = {(fi(1), f j(1)),··· ,(fi(Ki,j), f j(Ki,j))}.
(12.4)
Ki,j is the number of matched SIFT local feature descriptors, which should be equal
to or less than min(Ki,Kj). The edge weight ei,j using image Ii to predict image Ij is
estimated as
ei,j =
1
| Fi,j |
∑
∀(fi(k),f j(k))∈Fi,j
Df (fi(k), f j(k)).
(12.5)
| Fi,j | is the number of elements in Fi,j. After computing all edge weights, we can
get the directed graph as shown in Figure 12.3a.
12.4.2 Feature-Based Minimum Spanning Tree
A directed graph, as shown in Figure 12.3a, can be converted to different trees. We
look for the tree which represents the correlation among images to facilitate com-
pression. Since the edge weights are the predictive costs using one image to predict
another, the prediction structure can be obtained by ﬁnding the tree with the min-
imum predictive cost. It can be described as the problem of seeking a MST in the
graph.
Given a directed graph G = {I ,E }, where I = {I1,··· ,IN} and E is the edge
set, we can calculate every edge weight ei,j ∈E according to Eq. (12.5). Then the
total predictive cost of the graph G is deﬁned as
C(G) = ∑
ei,j∈E
ei,j.
(12.6)
Our target is to generate a tree T = {I ,ET} with ET ⊂E , which contains no cycle
and has the minimal total predictive cost
T ∗= argmin∀T⊂GC(T), s.t. d(Ii) < dmax
for ∀Ii ∈I .
(12.7)
d(Ii) denotes the depth of image Ii in the tree, starting from the root node as 0. dmax
is the maximum depth allowed in the tree. We introduce this constraint to control the
maximum delay in accessing an image randomly. The larger dmax, the more images
need to be processed for decoding images at leaf nodes.
We solve this minimization problem by two stages. In the ﬁrst stage, we construct
a MST according to the method described by Chu and Liu [291] without considering

12.4 Feature-Based Prediction Structure
253
the depth constraint. To build the MST, we ﬁrst generate a set of edges
E0 = {e1,··· ,eN}, where en = min∀ei,n∈E ei,n.
(12.8)
en denotes the edge with the minimal predictive cost connecting to In. There are N
edges in E0. As we know, the MST with N nodes should have N −1 edges. So we
remove the edge with the maximum predictive cost from the set E0, resulting in E1.
If there is no circle in E1, we have ET1 = E1; otherwise, all nodes of each circle are
replaced by a virtual node and edge weights related to the virtual node are updated
correspondingly. Then we repeatedly calculate E0 and E1 with reduced nodes in the
graph until ﬁnding ET1 (for more details, please refer to [291]). At last, we have MST
T ′ = {I ,ET1}.
In the second stage, we ﬁrst check the depth of T ′. The process stops if the depth
of T ′ is less than dmax; otherwise, we modify T ′ with regard to the constraint on depth
dmax. We ﬁrst select one path from the root node to a leaf node with depth larger than
dmax in T ′ and denote it as Tsub = {Isub,Esub}. And then, we modify the subtree by
E ′
sub = Esub
SB and B = {ei,j | d(Ii) < d(Ij),Ii ∈Isub,Ij ∈Isub}. It actually adds
edges in the subtree. Assuming Ir ∈Isub as the root of T ′, we have subset nodes
I ′
sub = Isub −{Ir}. Then we generate the optimal subtree T ∗
sub in T ′
sub = {I ′
sub,E ′
sub}
by the minimum spanning tree algorithm again [291]
T ∗
sub = argmin(∑
i
(C(T ′
sub,i)+er,i)), s.t. d(Ii) < dmax −1 for ∀Ii ∈I ′
sub. (12.9)
T ′
sub,i presents one of the subtree of T ′
sub with a children node Ii ∈I ′
sub as root. This
process is repeated on all the subtree whose depths exceed dmax −1, so that we get
the ﬁnal SFMST T = {I ,ET}, as exempliﬁed in Figure 12.3b.
12.4.3 Prediction Structure
Once the SFMST T = {I ,ET} is produced, the prediction structure of the images
in I is determined by depth-ﬁrst traversing the SFMST T. Accordingly, we or-
ganize the images into a sequence QI. The root image in T is the ﬁrst frame in
the sequence, which is intra coded without inter-image prediction. The other im-
ages are coded as inter frames predicted from their parents. The exempliﬁed SFMST
T = {I ,ET}, shown in Figure 12.3b, consists of the image set I = {I1,I2,I3,I4}
and edges ET = {e2,3,e2,1,e1,4}. We can generate the sequence QI shown in Figure
12.3c. The corresponding prediction structure is that both I1 and I3 are predicted from
I2 and I4 is predicted from I1 and I2.
Note that this prediction structure can be readily supported by the state-of-the-art
video coding standards (e.g., H.264 and HEVC), which enable multiple references
and long-term prediction.

254
12 Compression for Cloud Photo Storage
12.5 Feature-Based Inter-Image Prediction
After the prediction structure of images in a photo album is decided, the feature-
based inter-image prediction is applied to better exploit the correlation among im-
ages. The block diagram of the feature-based inter-image prediction is shown in Fig-
ure 12.4. As we have discussed, since the correlation of images in photo album is
very complicated, the proposed prediction consists of three steps. First, we estimate
geometric deformations between current and reference images in the module “De-
formation Estimation.” Multiple deformations are allowed during this estimation.
But, since High Efﬁcient Video Coding (HEVC) video coding deﬁnes four reference
buffers, as shown by r0,··· ,r3 in Figure 12.4, we ﬁnally select at most four defor-
mations. The modules “G” generate prediction images using the deformation models
H = {H0,··· ,H3}. Second, the modules “P” estimate illumination changes between
current image and prediction images and compensate them by a linear model. It has
been supported by the HEVC standard, known as weighted prediction. Finally, the
block-based motion estimation is applied to stored prediction images and current im-
age. The one with the minimum rate-distortion cost is selected to predict the blocks
of the current image.
12.5.1 Feature-Based Geometric Deformations
For a pair of images Ii and Ij, it is a hard problem to derive the number of defor-
mations between them and the parameters of every deformation. We propose solv-
ing this problem based on the matched SIFT descriptors Fi,j instead of pixel val-
ues. In fact, any four pairs of matched SIFT descriptors can deduce a projective
Deformaon
Esmaon
G
G
G
G
P
P
P
P
r0
r1
r2
r3
Moon 
Esmaon
H0
H1
H2
H3
Current 
Image
Current 
Image
Predicon 
Blocks
Reference 
Image
Block-Based
Moon Esmaon
Feature-Based 
Geometric 
Deformaon 
Feature-Based 
Photometric 
Transformaons 
Figure 12.4 The block diagram of SIFT feature-based inter-image prediction.

12.5 Feature-Based Inter-Image Prediction
255
transformation. However, modeling the deformations in this way will lead to a
huge number of transformations. Fischler and Bolles proposed the RANSAC algo-
rithm [246] to ﬁt the deformations with a single model, which covers the largest
set of matched features. This single model ﬁtting works well when there is a
uniﬁed deformation between two images. However, when there exist multiple re-
gions/objects following different motions, the algorithm cannot get a good
result.
(a) Current Image Ij
(b) Reference Image Ii
(c) Triangular Mesh
(d) Partitions
(e) Prediction 1
(f) Prediction 2
Figure 12.5 Multi-model geometric deformations.

256
12 Compression for Cloud Photo Storage
For two images Ij and Ii, as shown in Figure 12.5a,b, our multi-model deformation
manages to automatically divide all the matched SIFT features between Ii and Ij into
several groups and accordingly generate the deformation models H by minimizing
the following energy function
E = D+λ ×S.
(12.10)
E is the energy of the estimated models, D is the data term, S is the smoothness term,
and λ is a weighted parameter. The ﬁrst term D measures the geometric distance of
the matched feature points under a deformation model set H = {Hl | l = 1,··· ,L}
D = ∑
l
∑
(fi(ki,j),f j(ki,j))∈F l
i,j
Dl(ki,j).
(12.11)
F l
i,j is the set of matched local feature descriptors in Fi,j ﬁtting the model Hl. For
each pair of matched features (fi(ki,j), f j(ki,j)) ∈F l
i,j, the deformation distance be-
tween their locations under the model Hl is deﬁned as
Dl(ki,j) =∥pj(ki,j)−pi(ki,j)×Hl ∥.
(12.12)
The smoothness term S in Eq. (12.10) evaluates the connectivity between features.
We model the neighboring relationship among the feature points in image Ii by the
Delaunay triangulation, as shown in Figure 12.5c. Then the smoothness between two
neighboring feature points pi(ki,j) and pi(km,n) is deﬁned as
Sl(pi(ki,j),pi(km,n)) =

0, if both pi(ki,j) and pi(km,n) fit in Hl
1,
otherwise
(12.13)
Thus
S = ∑
l
∑
ki,j,km,n∈N(ki,j)
Sl(pi(ki,j),pi(km,n)).
(12.14)
N(ki,j) denotes the set of neighboring feature points of pi(ki,j) in the triangulation
mesh. The weighted parameter λ is the discontinuity penalty factor.
According to Isack and Boykov [292], the approximate minimization of Eq.
(12.10) can be solved using a graph-cut based approach. In the graph, each data
node denotes a feature point pi(ki,j) and each labelling node represents a model
Hl. The edge weight between a data node and a labeling node is deﬁned as the
corresponding Dl(ki,j), and the edge weight between two data nodes is deﬁned as
Sl(pi(ki,j),pi(km,n)). Note that the edge weights change with the variation of as-
signed models. Based on the deductions discussed by Boykov et al. [293], the mini-
mization of E is approximated by ﬁnding the minimum cuts on the graph. Then the
matched features in image Ii is partitioned into L sets and each set has a deformation
model Hl. As shown in Figure 12.5d, the matched features in image Ii are partitioned
into two sets and two deformation models are produced accordingly. For more details
on graph cut, please refer to Boykov et al. [293].

12.5 Feature-Based Inter-Image Prediction
257
Once the deformation model set H is available, we can generate a deformed
prediction set
¯
Ii = {¯I1,i,··· , ¯IL,i} from image Ii to image Ij, where each deformed
prediction ¯Il,i ∈
¯
Ii is obtained by ¯Il,i = Ii ∗Hl. The number of prediction images from
Ii to Ij is L. However, we note that the more the models, the higher the complexity
is in the following block-based motion compensation. Meanwhile, we also observe
that the models with limited inliers usually cover small regions and the correspond-
ing deformed images may suffer from severe distortions outside the corresponding
regions. This leads to the fact that these deformed images are not effective for predic-
tion. According to the buffer size of H.264 and HEVC, we limit the model number
L up to 4 in our scheme. In other words, we use at most four models to produce the
deformed prediction images
¯
Ii = {¯I1,i,··· , ¯IL,i} with L ≤4. The models with more
inlier SIFT pairs have the higher propriety to be selected. Figure 12.5e,f show two
deformed predictions.
12.5.2 Feature-Based Photometric Transformation
Our feature-based photometric transformation module is presented to reduce the in-
tensity disparities between the geometrically deformed prediction images in
¯
I i and
the current image Ij. For each prediction image ¯Il,i, we deﬁne the photometric trans-
formation as
¯¯Il,i = αl ¯Il,i +βl.
(12.15)
αl and βl denote the scale and offset parameters, respectively. We continue to use the
matched local feature descriptors to estimate photometric transform. The values of
αl and βl are determined by minimizing the distance between ¯¯ l,iI and Ij based on the
matched features set F l
i,j
z =
∑
(fi(ki,j),f j(ki,j))∈F l
i,j
∥Ij(pj(ki,j))−(αl ¯Il,i(¯pl,i(ki,j))+βl) ∥2 .
(12.16)
¯pl,i(ki,j) is the feature location after geometric deformation in image Il,i. Ij(pj(ki,j))
and ¯Il,i(¯pl,i(ki,j)) are the gray value of pixels. For every pair of matched local feature
descriptors, we only use the pixel values at the central point to estimate the param-
eters. But as shown in Figure 12.5d, every matched region has tens to hundreds of
matched local descriptors. They are enough to estimate the parameters robustly. The
parameters αl and βl are calculated by solving the partial differential functions of
Eq. (12.16) as
αl = N ·A−B·C
N ·D−B2 ,
(12.17)
βl = C ·D−A·B
N ·D−B2 ,
(12.18)

258
12 Compression for Cloud Photo Storage
where
A =
∑
(fi(ki,j),f j(ki,j))∈F l
i,j
¯Il,i(¯pl,i(ki,j))×Ij(pj(ki,j)),
B =
∑
(fi(ki,j),f j(ki,j))∈F l
i,j
¯Il,i(¯pl,i(ki,j)),
C =
∑
(fi(ki,j),f j(ki,j))∈F l
i,j
Ij(pj(ki,j)),
D =
∑
(fi(ki,j),f j(ki,j))∈F l
i,j
¯I2
l,i(¯pl,i(ki,j)),
and N =| F l
i,j |.
Having the values of αl and βl, we generate the photometric deformed prediction
¯¯Il,i for each prediction image ¯Il,i by Eq. (12.15). Then all the deformed prediction
images will be stored in the reference buffers r0,··· ,r3 and used as prediction images
in the following block-based motion compensation, as shown in Figure 12.4.
12.5.3 Block-Based Motion Compensation
After the geometric deformation and photometric transformation, the majority of
geometric and illuminate disparities has been compensated efﬁciently. However, we
note that the inter-image prediction still suffers from small local disparities (e.g.,
local shifting). To deal with the local disparities, we employ the block-based motion
compensation, which follows the approach adopted in HEVC. Here, we would like to
point out that the parameters of deformation models as well as those of photometric
transformations are encoded and transmitted to the decoder side. Each parameter is
scaled and rounded into integers before entropy coding.
12.6 Experimental Results
In this section, we evaluate the performance of our feature-based compression
scheme in comparison with the state-of-the-art methods. We demonstrate the efﬁ-
ciency of our scheme using ﬁve different photo albums. “CastleEntry” is a set of
images provided by Strecha. “RockBoat” is a personal album captured at Summer
Palace. “WadhamCollege” is provided by VGG. “MailRoom” is a set of indoor im-
ages. “NotreDame” contains 45 public photos randomly crawled from Flickr. Figure
12.6 presents three examples of each image set for preview. The properties of each
image set are summarized in Table. 12.1. All images in these sets are shot at different
camera positions without duplicate images. In addition, RockBoat and NotreDame

12.6 Experimental Results
259
Figure 12.6 Three example images of the ﬁve test sets. From top to bottom: CastleEntry, RockBoat,
WadhamCollege, MailRoom, and NotreDame.
Table 12.1 Properties of the ﬁve image sets.
Image Set
Number of
Resolution
Location
Time
Camera
Bright
Name
Images
Variation Variation Variation Variation
CastleEntry
10
3072 × 2048
Y
N
N
N
RockBoat
20
2592 × 1728
Y
Y
N
Y
WadhamCollege
5
1024 × 768
Y
N
N
N
MailRoom
7
1072 × 600
Y
N
N
N
NotreDame
45
Variable
Y
Y
Y
Y
sets are collected at different time under different lightings. Moreover, NotreDame
set is shot by different people with different cameras at different resolutions.
In the following tests, we ﬁrst evaluate the performance of our multi-model pre-
diction and photometric transformation, respectively. Then we compare our scheme
with the most popular image compression scheme JPEG. We also evaluate our
scheme compared with the latest research work on the photo album compression,
including Prof. Oscar’s approach [284] and Zou’s approach [285]. The HEVC

260
12 Compression for Cloud Photo Storage
standard software HM7.0 is used here as the coding platform [14]. At last, the com-
plexity of our scheme is discussed.
In our tests, all these images in an album are randomly indexed before coding. For
each album, the SFMST is ﬁrst generated to determine the coding order, resulting in a
sequence. The ﬁrst image in the sequence is intra encoded and the rest of the images
are inter coded. We use the nearest parent image in the SFMST as the reference
image in the three-step inter-image prediction. Following HEVC coding, we generate
the ﬁnal compression results. All coding tools in HEVC are enabled and the QPs
(quantization parameters) are set to 22, 27, and 32 for the images.
12.6.1 Efﬁciency of Multi-Model Prediction
We evaluate the efﬁciency of our multi-model prediction in Figure 12.7. The la-
bels “multiple,” “single,” “inter,” and “intra” denote the results of our multi-model,
RANSAC-based single model, HEVC inter coding, and HEVC intra coding, respec-
tively. Figure 12.7 shows the results of two image pairs and two image sets. In this
test, the four prediction methods are applied to replace the prediction process in our
scheme to generate the coded results, respectively. One can observe that our approach
achieves 15% bits saving compared with the single-model prediction and 30% bits
saving compared with the inter prediction of HEVC.
Figure 12.7 Performance evaluation of our multi-model prediction.

12.6 Experimental Results
261
We also note that the improvement of our multi-model prediction differs for differ-
ent sets. Our multi-model prediction prefers the case that the images contain multiple
objects or regions, which follow different motions under different camera positions.
For example, there are two walls with different motions and different camera lo-
cations in the scene of WedhamCollege. Our multi-model prediction generates two
motion models and two predictions for the two walls, respectively. Thus, it can re-
duce the local redundancy much more efﬁciently than the single-model prediction,
which ﬁts only one of the walls. Similarly, MailRoom contains images with multiple
objects shot in a limited space. The cabinet, the water canteen, and two bookshelves
in the scene follow different motion models. In this case, both single and multiple
model achieve good results. Our multi-model prediction performs the best.
12.6.2 Efﬁciency of Photometric Transformation
Different from indoor environments, outdoor scenes usually suffer luminance vari-
ances. In this test, our scheme is used to generate the results by enabling or disabling
the photometric transformation (PT in the ﬁgures). The curves marked as PT on and
PT off denote the results with PT enabled and disabled, respectively.
RockBoat contains 20 images, which are collected at different times under dif-
ferent lightings. Figure 12.8 shows the comparison results by rate-distortion curves.
One can observe that the photometric transformation helps to reduce brightness vari-
ances and achieves more than a 5% bits saving.
NotreDame has 45 images collected at not only different lightings but also differ-
ent resolutions. The columns in Figure 12.9 denote the bits savings compared with
Figure 12.8 Efﬁciency of photometric transformation on RockBoat image set.

262
12 Compression for Cloud Photo Storage
Figure 12.9 Efﬁciency of photometric transformation on NotreDame image set.
HEVC intra coding. The QP setting here is 27. The solid and hollow columns denote
the performance by disabling and enabling the photometric transformation, respec-
tively. The root image in this set is that marked as “Image 5.” One can observe that
the maximum bits savings is over 90% for “Image 43.” The average bits savings for
all the 45 images are 30.2% without PT and 34.6% with PT. One can also note that
our scheme has no gain for several images (e.g., Images 7, 12, and 16). In fact, if
the brightness variances of these images are too large (e.g., suffering overexposure),
our photometric transformation cannot work well. Our scheme also loses efﬁciency
when the noise is high enough to greatly reduce the accuracy of feature detection or
matching.
12.6.3 Overall Performance
Figure 12.10 shows the coding performance of our feature-based compression method.
In Figure 12.10a,b, the comparison results for our method, Oscar’s approach, Zou’s
approach, and individual JPEG on CastleEntry and RockBoat are presented, respec-
tively. The curve marked as “Oscar’s” denotes the average performance of Oscar’s
approach. In this method, one center image is selected via the MSE-based selection
and both global and local motion compensations are used in the inter-image pre-
diction. The curve marked as “Zou’s” shows the performance of Zou’s approach,
which utilizes the MSE-based MST to determine the coding structure followed by
the block-based motion compensation in HEVC for the inter prediction. The results
of JPEG are marked as “JPEG.” One can observe that our scheme achieves the best
coding performance. Our scheme outperforms JPEG with 10-times compression ra-
tio at the same quality level. It also obtains more than 2 dB PSNR gains or 40%

12.6 Experimental Results
263
Figure 12.10 Performance comparisons: (a) CastleEntry image set; (b) RockBoat image set; (c)
NotreDame image set.
bits saving compared with the other two solutions. One may note that Oscar’s ap-
proach and Zou’s approach have similar performance. It is because the complicated
geometric distortions between the images cannot be reduced efﬁciently by either the
pixel-based global motion compensation or the block-based motion compensation.
Figure 12.10c shows the results for NotreDame. Since the images in this set are
with different resolutions, the traditional inter-prediction methods proposed for video
coding cannot work directly in coding this set. Thus we evaluate the performance
in comparison with JPEG only. The columns in this ﬁgure show how many times
smaller our compressed ﬁles are than those of JPEG. For this set, our scheme con-
stantly outperforms JPEG and achieves averagely 9.6 times compression ratio as
shown by the dashed line in Figure 12.10.
12.6.4 Complexity
At the decoder side, we introduce the geometric deformations and photometric trans-
formations to traditional video decoding. Since each geometric deformation can be
regarded as an interpolation process, the complexity of the geometric deformation is
O(n). The complexity of the photometric transformation is O(n), too. So the com-
plexity increment in decoding is limited.
The encoder complexity of our scheme is higher than that of the corresponding
video schemes. The complexity increment mainly comes from four aspects: SIFT
detection, SIFT-based distance calculation, SFMST generation, and multi-model

264
12 Compression for Cloud Photo Storage
inter prediction. We can reduce the complexity of SIFT detection by computing at
low resolution counterparts of the input images. The complexity of distance calcula-
tion can be accelerated by the k-dimensional tree (KD-tree) and approximate nearest
neighbor (ANN) search. Moreover, the complexity of our scheme can be reduced
by taking advantage of fast algorithms [294] and parallel mechanisms. The com-
plexity of SFMST generation is proportional to the number of images in the graph.
The multi-model inter prediction is based on the energy-based model ﬁtting by Isack
and Boykov [292], which has a much faster convergence speed compared with other
multi-model ﬁtting methods. Since our scheme targets the image storage in cloud
environments, we believe that the computational resource should no longer be the
bottleneck of the system.
12.7 Our Conjecture on Cloud Storage
If photos in cloud storage are stored as individual JPEG ﬁles, with the increasing
number of photos, the storage needed also increases linearly, as shown by the dash-
dot line in Figure 12.11. It indicates that the service cost in cloud storage will increase
with the size of cloud storage. Users thus have to pay more money to increase their
storage sizes in cloud.
However, if our scheme is adopted for image and photo storage in cloud, we guess
that the service cost is not increasing linearly as the number of increased data. With
more and more images and photos stored in cloud, if the correlation with stored
data can be fully utilized, the actual size for an uploaded image after compression
will become fewer and fewer because most of the contents in the image already
exist. In other words, when the scale of cloud storage reaches a certain threshold, the
Figure 12.11 Our conjecture on the capacity of cloud storage.

12.8 Summary
265
increasing cost for storing a new image becomes very cheap. Our conjecture is that
the cost for cloud storage will converge to a constant no matter how many images
and photos are uploaded, as shown by the solid curve in Figure 12.11. It is important
for the service providers of cloud storage and users because it indicates that one day
the cloud storage paid by you is a ﬁxed fee but the storage size is unlimited.
The same principle should also exist for video storage, although currently we do
not have a scheme for coding video using the inter-video correlation, similar to that
we present in this chapter. Since image and video are dominant data in cloud storage,
if our conjecture is true, cloud storage will be a trend for personal data storage with
low cost and high reliability.
12.8 Summary
We present a feature-based image compression scheme for photo album. It adopts
SIFT local descriptors as measurements to analyze the correlation of images in photo
album and generate the models for geometric deformation and photometric transform
between images. The analysis is robust to scale and rotation variations and is less
sensitive to illumination changes. With the presented prediction technologies, the
images in photo album can be compressed with 10-times efﬁciency than individual
JPEG compress. It points out a trend on compressing large-scale image and video in
cloud storage, namely, the correlation across images and video.
In this chapter, we only consider the predictive coding for correlated images.
Like H.264 and the HEVC video standard, the more efﬁcient coding approach is
the bipredictive approach. In the next step, we will further extend our scheme by
using the bipredictive approach. It can better handle the complicated scenes with
foreground and background objects. It can also handle the images belonging to two
correlated image sets or more. Another important problem is that images in photo
album are dynamic in cloud storage. Our current solution can deal with deleting and
inserting images. If some new images are inserted into a compressed album, our
scheme may have to rebuild the SFMST and re-encode the whole albums. Also, if
some images need to be deleted from the compressed album, we need to revise the
SFMST and may have to transcode the related images. Though useful, the current
way in supporting dynamic album is straightforward. Advance approaches should be
investigated in the future.


Part V
Compressive Communication
Compressive sensing (CS) is a theory for sampling. However, when it is described
by a bipartite graph and is also constrained by binary input, it looks very similar to
low-density parity-check (LDPC) code but with two differences. The ﬁrst one is that
the symbols in CS are generated by arithmetic sum instead of logical exclusive-OR
in LDPC code. Therefore, we called binary input CS arithmetic code. In addition,
when the operation is arithmetic, the bipartite graph is no longer necessary as binary
weights. The second one is that the connections are assigned with weights. In this
part, we will study how to apply compressive sensing to solving communication
problems. We call this research compressive communication.
Chapter 13 presents the ﬁrst complete design to apply compressive sampling the-
ory to sensor data gathering for large-scale wireless sensor networks. The successful
scheme developed in this research is expected to offer fresh frame of mind for re-
search in both compressive sampling applications and large-scale wireless sensor
networks. We consider the scenario in which a large number of sensor nodes are
densely deployed and sensor readings are spatially correlated. The proposed com-
pressive data gathering is able to reduce global scale communication cost without in-
troducing intensive computation or complicated transmission control. The load bal-
ancing characteristic resulting from compressive sampling principles is capable of
extending the lifetime of the entire sensor network as well as individual sensors. Fur-
thermore, the proposed scheme can cope with abnormal sensor readings gracefully.
We also carry out the analysis of the network capacity of the proposed compressive
data gathering and validate the analysis through ns-2 simulations. More importantly,
this novel compressive data gathering has been tested on real sensor data and the
results show the efﬁciency and robustness of the proposed scheme.
Chapter 14 presents a novel compressive modulation (CM) scheme for ﬁne-
grained rate adaptation in time-varying wireless channel. Such compressive modula-
tion concepts open up a new research avenue in modulation from symbol generation,
mapping, to demodulation. Motivated by the emerging compressive sensing theory,
we ﬁrst propose random projection (RP) code that properly converts binary source to
multilevel rateless symbols. Then, the generated symbols are sequentially and evenly
mapped to a dense square quadrature amplitude modulation (QAM) constellation. At
the receiver, the RP decoder is capable of recovering source bits directly from mul-
tilevel symbols so as to avoid that information used in the decoding mismatches to
what is received from the channel. CM is a receiver adaptation scheme that allows the
sender to transmit at ﬁxed rate, but the achieved rate is decided by the actual chan-
nel condition. We carry out extensive simulations over random binary sources. CM

consistently outperforms sender rate adaptation that is based on conventional modu-
lation schemes. More importantly, we implement CM on OFDM physical layer and
evaluate its performance on a software radio testbed. Results show that CM outper-
forms “Oracle” sender adaptation by 41%–100%, and achieves a 33% gain over the
state-of-the-art receiver rate adaptation scheme.
Chapter 15 presents our proposed CM to simultaneously achieve joint source-
channel coding and seamless rate adaptation. The embedding of source compres-
sion into modulation brings signiﬁcant throughput gain when the physical layer data
contains non-negligible redundancy. Two key design issues in the proposed CM are
addressed in this chapter. First, we consider the RP code design for sources with dif-
ferent redundancies. Three principles are established and a concrete implementation
is given. Second, we devise a linear-time decoding algorithm for the proposed RP
code. In this belief propagation (BP) algorithm, we ﬁnd that computing convolution
in time domain is more efﬁcient than that in frequency domain for binary variable
nodes. Moreover, we invent a ZigZag deconvolution to further reduce the complexity.
Analysis show that the proposed decoding algorithm is nearly 20 times faster than the
state-of-the-art BP algorithm for CS called CS-BP. Emulations on traced data show
that CM achieves signiﬁcant throughput gain, up to 33% and 70%, respectively, over
the Hybrid automatic repeat request (ARQ) with compression and bit interleaved
coded modulation (BICM) with compression, under practical time-varying wireless
channels.

Chapter 13
Compressive Data Gathering
13.1 Introduction
This chapter considers the data gathering problem in a large-scale wireless sensor
network. Such a large-scale data gathering sensor network ﬁnds a variety of applica-
tions in habitat monitoring, trafﬁc control, and surveillance. In general, large-scale
sensor data gathering is accomplished through multi-hop routing from individual
sensor nodes to the data sink. Successful deployment of such large-scale sensor net-
works faces two major challenges in effective global communication cost reduction
and in energy consumption load balancing.
The need for global communication cost reduction is obvious because such sensor
networks typically are composed of hundreds to thousands of sensors, generating a
tremendous amount of sensor data to be delivered to a data sink. It is very much
desired to take full advantage of the correlations among the sensor data to reduce
the cost of communication. Existing approaches adopt in-network data compression,
such as entropy coding or transform coding, to reduce global trafﬁc. However, these
approaches have introduced signiﬁcant computation and control overheads that are
often not suitable for sensor network applications.
The need for energy consumption load balancing is also clear because of the re-
quired multi-hop data transmission for such large-scale sensor networks. Figure 13.1
shows such a network where sensors are densely deployed in the region of interest
and monitor the environment on a regular basis. A simple but typical example is the
highlighted route in Figure 13.1. Suppose N sensors, denoted as s1, s2, ..., and sN,
form a multi-hop route to the sink. Let d j denote the readings obtained by node s j.
The intuitive way to transmit dj, j = 1,2,...N to the sink is through multi-hop relay as
depicted in Figure 13.2a. Node s1 transmits its reading d1 to s2, and s2 transmits both
its reading d2 and the relayed reading d1 to s3. At the end of the route, sN transmits
all N readings to the sink. It can be observed that the closer a sensor is to the sink,
the more energy is consumed. Clearly, the sensor nodes closer to the data sink will
soon run out of energy and the lifetime of the sensor network will be signiﬁcantly
shortened.
269

270
13 Compressive Data Gathering
Figure 13.1 Data gathering sensor network.
(a) Baseline data collection
(b) Compressive data gathering
Figure 13.2 Comparing baseline data collection and compressive data gathering in a multi-hop
route.
This chapter presents the ﬁrst complete design to apply compressive sensing the-
ory [295–297] to sensor data gathering for large-scale wireless sensor networks, suc-
cessfully addressing the two major challenges as outlined above. First, the proposed
data gathering is able to achieve substantial sensor data compression without intro-
ducing excessive computation and control overheads. With elegant design, the pro-
posed scheme is also able to disperse the communication costs to all sensor nodes
along a given sensor data gathering route. This will result in a natural load balancing
and extend the lifetime of the sensor network.
The basic idea of the proposed compressive data gathering (CDG) is depicted in
Figure 13.2b. Instead of receiving individual sensor readings, the sink will be sent
a few weighted sums of all the readings from which to restore the original data. To
transmit the ith sum to the sink, s1 multiplies its reading d1 with a random coefﬁcient
φi1 and sends the product to s2. Upon receiving this message, s2 multiplies its reading
d2 with a random coefﬁcient φi2 and then sends the sum φi1d1 +φi2d2 to s3. Similarly,
each node sj contributes to the relayed message by adding its own product. Finally,
the sink receives ∑N
j=1 φi jd j, a weighted sum of all the readings. This process is
repeated using M sets of different weights so that the sink will receive M weighted
sums.

13.2 Related Work
271
With such design, all nodes transmit M messages and consume same the amount
of energy. Each node only performs one addition and one multiplication in order to
compute one weighted sum. Comparing Figure 13.2a,b, careful readers will observe
that the ﬁrst M nodes send more messages in CDG than in the baseline transmission,
while the rest of the nodes send less messages in CDG. When N is large and M is
much smaller than N, CDG can signiﬁcantly reduce the total number of transmis-
sions and save energy. The key problem now becomes whether the sink is able to
restore N individual readings from M measurements, when M is far smaller than N.
Fortunately, the compressive sensing theory has a positive answer to this question.
There are three main contributions in the proposed scheme. First, we extend the
application of compressive sensing theory from one or a few sensors to large-scale
multi-hop sensor networks. Beyond the basic idea, we propose a scheme which al-
lows CDG to be practically applied to large sensor networks. Second, we carry out
a theoretical analysis of the network capacity for CDG and validate the capacity
gain of CDG through ns-2 simulations. Third and more importantly, we test CDG on
two sets of real sensor data. The results show that CDG is practically applicable to
various data gathering sensor networks. Even when sensor data exhibit little spatial
correlations in which case conventional in-network compression approaches would
fail, CDG is still able to reduce the trafﬁc of the bottleneck node by two to three
times and signiﬁcantly prolong the network lifetime.
13.2 Related Work
The fundamental assumption of in-network data compression is that sensor nodes
have spatial correlations in their readings. According to where the spatial correlation
is utilized, we can classify existing in-network data compression techniques into two
categories.
13.2.1 Conventional Compression
Conventional compression techniques utilize the correlation during the encoding pro-
cess and require explicit data communication among sensors. Cristescu et al. [298]
propose a joint entropy coding approach, where nodes use relayed data as side in-
formation to encode their readings. Again take the multi-hop route in Figure 13.2 as
an example. First, node s1 encodes its reading d1 into message p1 using H(d1) bits,
where H(d1) is the entropy of d1. Then, when s2 receives p1, it encodes its reading
d2 into message p2 using H(d2|d1) bits, where H(d2|d1) is the conditional entropy.
Since d1 and d2 are correlated, H(d2|d1) is smaller than H(d2). Therefore, jointly
encoded messages cost less bits than independently encoded messages.
The above approach utilizes data correlation only unidirectionally. If data are al-
lowed to be communicated back and forth during encoding, nodes may cooperatively

272
13 Compressive Data Gathering
Figure 13.3 Cooperative wavelet compression.
perform transform to better utilize the correlation. Ciancio et al. [299] and Aci-
movic et al. [300] propose to compress piece-wise smooth data through a distributed
wavelet transform. In doing so, even nodes ﬁrst broadcast their readings. Upon re-
ceiving the readings from both sides, odd nodes compute the high pass coefﬁcients
h(·). Then, odd nodes transmit h(·) back and even nodes compute the low pass coefﬁ-
cients l(·). This process is illustrated in Figure 13.3. Although wavelet decorrelation
can be performed for multiple levels, it is not suggested in distributed processing
because of the communication overhead. After the transform, nodes transmit signif-
icant coefﬁcients to the sink, usually in their raw form to avoid the complexity of
entropy coding.
Quantization of a group of readings to one representative value is another form of
conventional compression. The clustered aggregation (CAG) technique [301] forms
clusters based on sensing values. By grouping sensors with similar readings, CAG
only transmits one reading per group to achieve a predeﬁned error threshold. Gupta
et al. [302] exploit a similar idea. In each round of data gathering, it only involves a
subset of nodes, which is sufﬁcient to reconstruct data for the entire network.
There are two main problems with conventional compression techniques. First,
the compression performance relies heavily on how the routes are organized. In order
to achieve the highest compression ratio, compression and routing algorithms need
to be jointly optimized. This has been proved to be a nondeterministic polynomial
(NP)-hard problem [298]. Second, the efﬁciency of an in-network data compression
scheme is not solely determined by the compression ratio, but also depends on the
computational and communication overheads. However, joint entropy coding tech-
niques perform complex computation in sensors, while transform-based techniques
require a large amount of data exchanges.
13.2.2 Distributed Source Coding
Distributed source coding techniques [303–305] intend to reduce the complexity at
the sensor nodes and utilize correlation at the sink. They are based on the Slepian-
Wolf coding theory [306], which claims that compression of correlated readings,
when separately encoded, can achieve the same efﬁciency as if they are jointly en-
coded, provided that the messages are jointly decoded. This important conclusion not

13.2 Related Work
273
only eliminates data exchanges, but decouples routing from compression. After en-
coding sensor readings independently, each node simply sends the compressed mes-
sage along the shortest path to the sink to achieve the minimum energy consumption.
However, a prerequisite of Slepian-Wolf coding is that the global correlation
structure needs to be known in order to allocate the appropriate number of bits to be
used by each node. This is hard to fulﬁll in a large-scale wireless sensor network. In
view of this, Yuen et al. [307] adopt a localized Slepian-Wolf coding scheme. Based
on the assumption that sensors outside the immediate neighborhood have weak cor-
relation in their readings, a node may only consider its data correlation with one-hop
neighbors when determining the size of the encoded message. We will show that, for
a set of real sensor data which do not satisfy this assumption, the localized coding
scheme will fail to compress such data.
Distributed source coding techniques perform well for static correlation patterns.
However, when the correlation pattern changes or abnormal events show up, the
decoding accuracy will be greatly affected. Since detecting abnormal events is an
important task of the sensor network, when an abnormal event is captured by a side
node, the originally assigned number of bits will be inadequate to encode the reading,
and cause decoding error at the sink. More seriously, when the abnormal reading
appears at a main node, it will cause errors within a large range of reconstructed
sensor readings.
13.2.3 Compressive Sensing
With the emergence of compressive sensing theory [295–297], we have seen a new
avenue of research in the ﬁeld of in-network data compression. The intuition behind
compressive sensing is that a small number of linear projections (or so called mea-
surements) of a sparse signal contains adequate information for its reconstruction.
Mathematically, let x = [x1x2 ...xN]T be a K-sparse signal, that is, there are only
K(K ≪N) nonzero elements in xi’s. Let us take M measurements of x through linear
projection:
y = Φ ·x,
(13.1)
where Φ is an M ×N(M < N) random matrix, then x can be perfectly reconstructed
from y under certain conditions.
Compressive wireless sensing (CWS) [308] appears to be able to reduce the la-
tency of data gathering in a single-hop network where every sensor can directly com-
municate with the sink. CWS delivers linear projections of sensor readings through
synchronized amplitude-modulated analog transmissions. Due to the difﬁculties in
analog synchronization, CWS is less practical for large-scale sensor networks. Rab-
bat et al. [309] leverages compressive sensing for data persistence, instead of data
gathering, in a wireless sensor network. In an overview paper, Haupt et al. [310] also
speculate the potential of using compressive sensing theory for data aggregation in
a multi-hop wireless sensor network. However, no real scheme has been reported
based on this initial idea.

274
13 Compressive Data Gathering
When compressive sensing is applied to in-network data compression, it will bring
a wealth of similar beneﬁts as distributed source coding including the simple encod-
ing process, saving of inter-node data exchange, and decoupling of compression from
routing. Furthermore, compressive sensing has two additional advantages. First, it
can deal with abnormal sensor readings gracefully. This advantage will be detailed
in the next section. Second, data reconstruction is not sensitive to packet losses. In
compressive sensing, all messages received by the sink are equally important. How-
ever, in distributed source coding, received data are predeﬁned as main or side in-
formation. Losing main information will cause fatal errors to the decoder. All these
desired merits make compressive sensing a promising solution to the data gathering
problem in large-scale wireless sensor networks.
13.3 Compressive Data Gathering
The objective of compressive data gathering is two-fold: compress sensor readings to
reduce global data trafﬁc and distribute energy consumption evenly to prolong net-
work lifetime. Similar to distributed source coding, the data correlation pattern shall
be utilized on the decoder end. In addition, compression and routing are decoupled
and therefore can be optimized separately.
13.3.1 Data Gathering
The intuition behind CDG is that higher efﬁciency can be achieved if correlated sen-
sor readings are transmitted jointly rather than separately. We have given a simple
example in Section 13.1, showing how sensor readings are combined while being
relayed along a chain-type topology to the sink. In practice, sensors usually spread
in a two-dimensional area, and the ensemble of routing paths presents a tree struc-
ture. Figure 13.4a shows a typical routing tree in a wireless sensor network. In this
example, the sink has four children, each of them leading a subtree delimited by the
dotted lines. Data gathering and reconstruction of CDG are performed on the subtree
basis.
Figure 13.4 Data gathering in a typical routing tree.

13.3 Compressive Data Gathering
275
In order to combine sensor readings while relaying them, every node needs to
know its local routing structure. That is, whether or not a given node is a leaf node
in the routing tree or how many children the node has if it is an inner node. To
facilitate efﬁcient aggregation, we have made a small modiﬁcation to the standard
ad hoc routing protocol: when a node chooses a parent node, it sends a “subscribe
notiﬁcation” to that node; when a node changes parent, it sends an “unsubscribe
notiﬁcation” to the old parent.
The data gathering process of CDG is illustrated through an example shown in
Figure 13.4b. It is the detailed view of a small fraction of the routing tree marked
in Figure 13.4a. After all nodes acquire their readings, leaf nodes initiate the trans-
mission. In this example, s2 generates a random number φi2, computes φi2d2, and
transmits the value to s1. The index i denotes the ith weighted sum ranging from 1 to
M. Similarly, s4, s5, and s6 transmits φi4d4, φi5d5, and φi6d6 to s3. Once s3 receives the
three values from its children, it computes φi3d3, adds it to the sum of relayed values,
and transmits ∑6
j=3 φi jdj to s1. Then, s1 computes φi1d1 and transmits ∑8
j=1 φ1 jdj to
its parent. Finally, the message is forwarded to the sink, which contains the weighted
sum of all readings in a subtree.
Assume that there are N nodes in a particular subtree, then the ith weighted sum
can be represented by:
yi =
N
∑
j=1
φijdj.
(13.2)
For this subtree, the sink obtains M weighted sums {yi}, i = 1,2,...M. Mathemati-
cally, we have:





y1
y2
...
yM




=





φ11 φ12 ... φ1N
φ21 φ22 ... φ2N
...
...
φM1 φM2 ... φMN











d1
d2
...
dN






.
(13.3)
In this equation, each column of {φi j} contains the series of random numbers gen-
erated at a corresponding node. In order to avoid transmitting this random matrix
from sensors to the sink, we can adopt a simple strategy: before data transmission,
the sink broadcasts a random seed to the entire network. Each node uses this seed
and its unique identiﬁcation to generate the series of pseudo-random numbers. This
generation process can be reproduced by the sink to obtain the {φi j} matrix.
In Eq. (13.3), di (i = 1,2,...N) is a scalar value. In a practical sensor network,
each node is possibly attached with a few sensors of different type, for example,
a temperature sensor and a humidity sensor. Thus sensor readings from each node
become a multi-dimensional vector. To deal with this, we may separate readings
of each dimension and process them respectively. Alternatively, since the random
coefﬁcients φi j are irrelevant to sensor readings, we may treat di as a vector. Then,
the weighted sums yi become vectors of the same dimension too.

276
13 Compressive Data Gathering
When M < N, solving a set of M linear equations with N unknown variables is an
ill-posed problem. However, sensor readings are not independent variables, rather,
they are spatially correlated. Under this assumption, in the following section we will
explain whether the set of linear equations are solvable, what requirements M should
meet to solve them, and how these equations can be solved.
13.3.2 Data Recovery
13.3.2.1 Recovery of Spatially Correlated Data
According to the compressive sensing theory, a K-sparse signal can be reconstructed
from a small number of measurements with a probability close to one. The weighted
sums obtained in Eq. (13.3) are a typical type of measurements. Signal sparsity char-
acterizes the correlations within a signal. An N-dimensional signal is considered as
a K-sparse signal if there exists a domain in which this signal can be represented by
K (K ≪N) nonzero coefﬁcients. Figure 13.5a shows a 100-dimensional signal in its
original time domain. Obviously, it is not sparse at all in this domain. Because of the
signal correlation, it can be described more compactly in transform domains such as
wavelet and discrete cosine transform (DCT). Figure 13.5b gives a representation of
the same signal in DCT domain. We can see that there are only ﬁve nonzero DCT
coefﬁcients. Therefore, this signal is a 5-sparse signal in the DCT domain.
In densely deployed sensor networks, sensors have spatial correlations in their
readings. Let N sensor readings form a vector d = [d1 d2 ... dN]T, then d is a K-sparse
signal in a particular domain Ψ. Denote Ψ = [ψ1 ψ2 ...ψN] as the representation
Figure 13.5 A 5-sparse signal in the DCT domain.

13.3 Compressive Data Gathering
277
basis with vectors {ψi} as columns, and x = [x1,x2,...xN]T are the corresponding
coefﬁcients. Then, d can be represented in the Ψ domain as:
d =
N
∑
i=1
xiψi, or d =Ψx.
(13.4)
The compressive sensing theory tells that a K-sparse signal can be reconstructed
from M measurements if M satisﬁes the following conditions [311]:
M ≥c· µ2(Φ,Ψ)·K ·logN,
(13.5)
where c is a positive constant, Φ is the sensing matrix as deﬁned in Eq. (13.3), and
µ(Φ,Ψ) is the coherence between sampling basis Φ and representation basis Ψ.
The coherence metric measures the largest correlation between any two elements of
Φ and Ψ, and is deﬁned as:
µ(Φ,Ψ) =
√
N · max
1≤i,j≤N |⟨φi,ψj⟩|.
(13.6)
From Eq. (13.6), we can see that the smaller the coherence between Φ and Ψ is, the
less measurements are needed to reconstruct the signal. In practice, using random
measurement matrix is a convenient choice, since a random basis has been shown to
be largely incoherent with any ﬁxed basis, and M = 3K ∼4K is usually sufﬁcient to
satisfy Eq. (13.5).
With sufﬁcient number of measurements, the sink is able to reconstruct sensor
readings through solving an l1-minimization problem:
min
x∈RN ∥x∥l1
s.t.
y = Φd, d =Ψx.
(13.7)
In addition, for sparse signals whose random projections are contaminated with
noise, reconstruction can be achieved through solving a relaxed l1-minimization
problem, where ε is a predeﬁned error threshold:
min
x∈RN ∥x∥l1
s.t.
∥y−Φd∥l2 < ε, d =Ψx.
(13.8)
Suppose ˜x is the solution to this convex optimization problem, then the proposed
reconstruction of the original signal is ˜d = Ψ ˜x. Here, the Ψ matrix describes the
correlation pattern among sensor readings. For example, if sensor readings are piece-
wise smooth, we may use wavelet transform matrix as Ψ. As we have mentioned
earlier, sensors do not use the correlation pattern during the data gathering process.
This information is only used by the sink during data recovery.
13.3.2.2 Recovery of Data with Abnormal Readings
One of the main purposes of the sensor network is to monitor abnormal events.
However, when abnormal events take place, the sparsity of sensor readings is

278
13 Compressive Data Gathering
Figure 13.6 A signal with two abnormal readings.
compromised. As an example, Figure 13.6a differs from Figure 13.5a by only two
abnormal readings. The corresponding DCT coefﬁcients shown in Figure 13.6b are
not sparse anymore. Therefore, the signal in Figure 13.6 is not sparse in either time
domain or transform domain. In this situation, conventional compression techniques
need to transmit signiﬁcantly more data in order to reconstruct the original signal.
Distributed source coding techniques will have a big degradation.
We have a better solution in compressive data gathering. Sensor data with abnor-
mal readings can be decomposed into two vectors:
d = d0 +ds,
(13.9)
where d0 contains the normal readings, which are sparse in a certain transform do-
main, and ds contains the deviated values of abnormal readings. Since abnormal
readings are sporadic, ds is a sparse signal in the time domain. Suppose the normal
readings are sparse in Ψ domain, then Eq. (13.9) can be rewritten into:
d =Ψx0 +Ixs,
(13.10)
where I is the identical matrix, and both x0 and xs are sparse. We can see that signal
d is decomposed into two signals which are sparse in different domains. We can
construct an overcomplete basis Ψ ′ = [Ψ I], then d should be sparse in Ψ ′ domain:
d =Ψ ′x, x = [x0TxsT]T.
(13.11)
Incorporating Eq. (13.11) into Eq. (13.7) or Eq. (13.8), the signal recovery with
abnormal readings can be solved similarly by the l1-norm optimization. Donoho et
al. [312] showed the possibility of stable recovery under a combination of sufﬁcient

13.4 Network Capacity of Compressive Data Gathering
279
sparsity and favorable structure of the overcomplete system. Moreover, they also
proved that stable recovery of the sparse signal in an overcomplete dictionary also
works for noisy data, and the optimally sparse approximation to the noisy data, to
within the noise level, differs from the optimally sparse decomposition of the ideal
noiseless signal by at most a constant multiple of the noise level.
Suppose ˜x is a vector of length 2N, and is the solution to the l1-minimization
problem deﬁned in Eq. (13.8) when an overcomplete dictionary is used. Similarly,
the original sensor readings can be reconstructed by ˜d = Ψ ′˜x. Denote ˜xs as an N-
dimensional vector composed of the last N elements of ˜x, then the nonzero values in
˜xs indicate the positions of abnormal readings.
13.4 Network Capacity of Compressive Data Gathering
The previous section illustrated how to gather and recover sensor readings acquired
in one time instance. This section will investigate the beneﬁt of CDG from the view-
point of network capacity, that is, how frequent CDG allows sensors to acquire data
while ensuring all readings can be transmitted to the sink. The capacity of a data
gathering network is deﬁned as follows.
Deﬁnition 8 (Network Capacity) We shall deﬁne that a rate λ is achievable in a
data gathering sensor network, if there exists a time instance t0 and duration T such
that during [t0,t0 + T) the sink receives λT bits of data generated by each of the
sensors si, i = 1,2,...N. Then, network capacity C is deﬁned as the supremum of the
achievable rate, or C = sup{λ}.
Different from the pioneering work on network capacity analysis [313], the trafﬁc
pattern in our study is many-to-one. We let all sensors generate data at the same rate,
and assume that sensor readings acquired at the same time instance are K-sparse.
13.4.1 Network Capacity Analysis
We assume a discal sensing area in which N sensor nodes are uniformly distributed,
and the sink is located in the middle of the disk. All sensor nodes and the sink com-
municate over single frequency shared radio channel, accessed through time-division
multiple access (TDMA) control. We denote W as the amount of data a node trans-
mits in one time slot, and restrict that a node cannot transmit and receive at the same
time.
Let {Xk, k ∈V} be the subset of nodes simultaneously transmitting over the
shared channel in a speciﬁc time slot. Then a successful transmission from Xi, i ∈V
to Xj can be deﬁned under two interference models.

280
13 Compressive Data Gathering
Deﬁnition 9 (Protocol Model) Transmission from node Xi to Xj is successful under
protocol model if and only if the following two conditions are satisﬁed:
• ∥Xi −Xj∥≤r,
• ∥Xk −Xj∥> (1+δ)r, δ > 0 for k ∈V −{i}.
The ﬁrst condition requires that the two communicating nodes are within a distance
r. The second condition requires that the receiving node is at least (1 + δ)r away
from any other transmitting nodes.
Deﬁnition 10 (Physical Model) Transmission from node Xi to Xj is successful un-
der physical model if and only if:
Pi
∥Xi−Xj∥α
NG +∑k∈V,k̸=i
Pk
∥Xk−Xj∥α
≥β,
where Pi is the transmission power for Xi, α is the fading parameter, and NG is
noise power level. The expression on the left is the signal to interference and noise
ratio (SINR) at the receiving node. A successful transmission under physical model
requires the SINR to be greater than a predeﬁned threshold β.
13.4.1.1 Capacity under the Protocol Model
The capacity under protocol model can be analyzed in a similar way as Marco et
al. [314]. Let us ﬁrst recall the following lemma.
Lemma 13.1. N nodes are uniformly distributed in a region of area A. When N is
large, the number of nodes n within a sub-region R of area AR can be bounded with
high probability.
Pr
NAR
A
−
p
αNN ≤n ≤NAR
A
+
p
αNN

→1,
as N →∞.
Sequence αN is chosen such that αN →∞as N →∞, and limN→∞
αN
N = ε, where ε
is positive but arbitrarily small.
Proof. Each of the N nodes has the same probability AR/A to fall in region R. There-
fore, n follows binomial distribution with the mean being µn = NAR
A
and the variance
being δ 2
n = NAR
A (1−AR
A ). According to Chebychev’s inequality:
Pr(|n−µn| ≥
p
αNN) ≤δ 2
n
αNN =
AR
A ·(1−AR
A )
αN
.
(13.12)
The probability goes to 0 as N →∞and αN →∞.

13.4 Network Capacity of Compressive Data Gathering
281
Theorem 13.1. In a wireless sensor network with N uniformly distributed nodes,
compressive data gathering can achieve network capacity of λ ≥W
M
πr2−√ε
π(2+δ)2r2+√ε
with a probability close to 1 as N →∞, where ε is arbitrarily close to 0, and M is
the number of random measurements. Usually M = c1K, and c1 is a constant in the
range of [1,4].
Proof. Consider a node in transmission. According to Deﬁnition 9, the distance from
any interfering source to this node is at most (2+δ)r. In other words, all the interfer-
ing sources are contained in a disk of area AR1 = π(2+δ)2r2. Based on Lemma 13.1,
the number of nodes in this region, denoted by nit f , is less than n1 with high proba-
bility:
nit f ≤n1 = NAR1
A
+
p
αNN.
(13.13)
Next we build the contention graph of the network by connecting interfering
nodes. With high probability, the maximal node degree in the contention graph is
n1 −1. According to the graph coloring theory, all nodes can be colored with at most
n1 different colors. If we associate each color with a transmission slot, every node
gets one chance to transmit in n1 slots. Therefore, the average transmission rate of
each node is:
γ = W
n1
.
(13.14)
Then consider the one-hop neighbors of the sink. They are contained in a disk
centered at the sink and with a radius of r. The area of the disk is AR2 = πr2. Accord-
ing to Lemma 1, the number of nodes in this region, denoted by n2, can be bounded
with high probability:
NAR2
A
−
p
αNN ≤n2 ≤NAR2
A
+
p
αNN.
(13.15)
Recall that compressive data gathering is performed on a subtree basis. We shall
adopt an appropriate routing protocol such that all subtrees are roughly of equal size.
For simplicity, we consider the size of each subtree is Np = N
n2 . Since the sensor data
from the entire network is K-sparse, when N →∞, we can consider that each subset
of the nodes are proportionally sparse, that is, K/n2-sparse. The number of random
measurements needed to reconstruct data is M/n2 per subtree. To achieve the rate
λ, the transmission rate of the subtree root should be Mλ/n2. Take Eq. (13.14) into
account, we have:
W
n1
= Mλ
n2
.
(13.16)
Substituting Eq. (13.13) and Eq. (13.15) into Eq. (13.16), we have:

282
13 Compressive Data Gathering
λ = W
M
n2
n1
≥W
M
NAR2
A
−√αNN
NAR1
A
+√αNN
= W
M
AR2
A −√ε
AR1
A +√ε
= W
M
πr2 −√ε
π(2+δ)2r2 +√ε .
(13.17)
As N →∞, √ε →0, the lower bound of achievable capacity is arbitrarily close to
W
M(2+δ)2 .
13.4.1.2 Capacity under the Physical Model
Without loss of generality, we assume the following constraints for the physical
model:
• All nodes transmit with equal and ﬁnite power P0.
• All noises are of the same variance. Therefore, for a given small positive number
η, there exist a noise level N0 such that prob(N0 > NG) < η.
• Given α and β, P0 is chosen such that the network is a connected graph when
the noise level is N0.
Theorem 13.2. In a wireless sensor network with N uniformly distributed nodes,
compressive data gathering can achieve network capacity of λ ≥W
M
πr2
0−√ε
π(2+δ0)2r2
0+√ε
with a probability close to 1 as N →∞, given r0 < αq
P0
βN0 and δ0 > α−1q
2πβc2
1−βrαN0/P0 −
1.
Proof. Theorem 13.1 gives the network capacity under protocol model. We will
prove that when r = r0 and δ = δ0, a feasible transmission schedule under proto-
col model is also feasible under physical model.
First, we restrict the communication to nodes within a distance of r0. When node
Xi transmits data to node Xj, the SINR at Xj is:
SINRj =
P0
|Xi−Xj|α
N0 +∑k∈V,k̸=i
P0
|Xk−Xj|α
.
(13.18)
Denote Ps as the received signal strength and Pf as the interference strength in Eq.
(13.18). Since |Xi −Xj| < r0, we have:
Ps =
P0
|Xi −Xj|α > P0
rα
0
.
(13.19)
A necessary condition is that when Pf = 0, the selection of r0 should ensure SINRj >
β. This can be satisﬁed if r0 <
αq
P0
βN0 .

13.4 Network Capacity of Compressive Data Gathering
283
Figure 13.7 Connecting adjacent transmitting nodes in an annulus.
Next, let us look at the interference part Pf . A feasible schedule under protocol
model ensures that there is no other simultaneous transmitter in the circular area cen-
tered at Xj and with a radius of (1+δ)r. The interference comes from the transmitters
outside this region. Divide the sensing region by concentric circlesCi, i = 1,2,... cen-
tered at Xj. The radius of circle Ci is ri = (1+δ)ri. Denote Ai as the annulus formed
by Ci and Ci+1. Next, we quantify the interference to Xj caused by the transmitters
in each annulus.
Denote ai as the number of simultaneous transmitters within a particular annulus
Ai. Since the distance from Xj to any node in this annulus is larger than (1+δ)ri, the
interference from this annulus is:
Pf (Ai) = ∑
k,Xk∈Ai
P0
|Xk −Xj|α <
aiP0
((1+δ)ri)α .
(13.20)
Pf (Ai) can be bounded once ai is bounded. In doing so, we connect adjacent
transmitters clockwise with line segments, as Figure 13.7a shows. Figure 13.7b gives
an enlarged view of two adjacent transmitting nodes Xs and Xt. Connect the center of
circle with the two nodes and extend the lines so that they intersect Ci+1 at points T1
and T2. From T2 draw a line parallel to XsXt and intersect XsT1 at point T3. Then we
have:
|XsXt| ≤|T2T3| < |
⌢
T1T2|+(1+δ)r.
(13.21)
A feasible schedule under protocol model ensures that the distance of each line
segment is at least (2 + δ)r. Summing up all the segments in annulus Ai and using
the inequality in Eq. (13.21), we have:
(2+δ)rai ≤∑
s,t
|XsXt|
< 2π(1+δ)r(i+1)+(1+δ)rai
⇒ai < 2π(1+δ)(i+1).
(13.22)

284
13 Compressive Data Gathering
Substitute Eq. (13.22) into Eq. (13.20), and sum up the interferences from all annu-
luses, we have:
Pf =
∞
∑
i=1
Pf (Ai) <
∞
∑
i=1
2πP0(1+δ)(i+1)
((1+δ)ri)α
=
2πP0
rα(1+δ)α−1
∞
∑
i=1
 1
iα−1 + 1
iα

= 2πP0 (ζ(α −1)+ζ(α))
rα(1+δ)α−1
,
(13.23)
where ζ(·) is the Riemann Zeta function. When α > 2, ζ(α) < π2
6 , and ζ(α −1)
converges to a constant. Denote c2 = ζ(α)+ζ(α −1). Then, when r = r0 and δ =
δ0 >
α−1q
2πβc2
1−βrαN0/P0 −1, Eq. (13.23) can be written into:
Pf < P0
rα
0 β −N0.
(13.24)
Substitute Eq. (13.19 and Eq. (13.24) into Eq. (13.18), we obtain SINRj > β. This
proves that a feasible scheduling under protocol model with r = r0 and δ = δ0 is
also feasible under physical model. Therefore, the network capacity achieved under
protocol model when r = r0 and δ = δ0 can also be achieved under physical model.
13.4.1.3 Capacity Gain over Naive Transmission
Collorary 13.1 In a wireless sensor network with N uniformly distributed nodes,
CDG can achieve a capacity gain of N/M over baseline transmission under both
interference models, given that sensor readings are K-sparse, and M = c1K.
Denote λ1 as the network capacity of baseline transmission. It is achieved when
every node is allowed to transmit once every n1 slots, and trafﬁc is evenly distributed
among n2 one-hop neighbors of the sink. Then we have W
n1 = Nλ1
n2 . Denote λ2 as the
network capacity of CDG. If the same transmission schedule and routing structure
are adopted, we have W
n1 = Mλ2
n2 . From these two equations, we can conclude that
CDG can achieve a capacity gain of N/M over baseline transmission.
13.4.2 NS-2 Simulation
The network capacity analysis is based on scheduled medium access control (MAC).
In practice, the computational and communication overhead of MAC scheduling is
too high. Contention-based MAC is more often adopted in wireless sensor networks.

13.4 Network Capacity of Compressive Data Gathering
285
Table 13.1 Simulation parameters.
MAC protocol
802.11
Physical data rate
2 Mbps
Transmission range 15 meters
Interference range
25 meters
Payload size
20 bytes
RTS/CTS status
OFF
Retry limit
7
IFQ length
200
K/N (data sparsity) 0.05
c1 = M/K
4
In order to understand how CDG performs in practical settings, we evaluate its per-
formance through ns-2 [315] simulations and compare it with baseline transmission
on two typical topologies: chain [316] and grid topologies [317]. Table 13.1 lists the
main parameters used in the simulation. We adopt 802.11 instead of ZigBee because
the implementation of 802.11 in ns-2 is well established.
The payload size of each packet is 20 bytes for both baseline transmission and
CDG. Although both approaches can combine multiple messages in one packet and
improve transmission efﬁciency, we keep only one message per packet because we
are only interested in the comparison between them. Data sparsity is assumed to be
5%. For example, when N = 1000, K = 50, and we assume that the sink can recover
the original data from M = 200 random measurements. In the best case, CDG should
achieve a capacity gain of N/M = 5.
13.4.2.1 Chain Topology
The chain topology is composed of 1000 sensors and one sink locating at one extreme
of the chain. The distance between any two adjacent nodes are 10 meters. Under
the given transmission and interference range, nodes can only communicate with
adjacent nodes, and may cause interfere to two-hop neighbors.
In the simulation, we vary the input interval and evaluate how output interval and
packet loss ratio change accordingly. In general, as the input interval decreases, the
output interval decreases and the packet loss ratio increases. However, if an input
interval is not achievable, the output interval will cease to decrease and may slightly
increase as a result of congestion collapse. We may infer the network capacity from
the minimum achieved output interval.
Figure 13.8a shows that the minimum output interval of baseline transmission is
10.6 seconds per message, and it is achieved when the input interval is 10.2 seconds
per message. There is a small gap between these two values because of network
jitters and packet losses. Figure 13.8b shows the performance of CDG. The minimum
output interval is 2.11 seconds per message achieved when the input interval is 1.92

286
13 Compressive Data Gathering
Figure 13.8 Output interval versus input interval in chain topology.
seconds per message. We can see that CDG can achieve a capacity gain of 5 over
baseline transmission.
In addition, the packet loss ratio of CDG is zero when the input interval is
1.92 seconds per message and above. In contrast, even when the network is not
overloaded, baseline transmission incurs a constant packet loss ratio between 3%
to 4% as a result of trafﬁc burst.
In this chain topology, CDG introduces an initial delay of 1.80 seconds. This is
because transmission starts from the leaf node, which is 1000-hops away from the
sink. This initial delay does not affect the network capacity because transmitting
readings acquired at different time instance can be pipelined.
13.4.2.2 Grid Topology
The grid topology contains 1089 nodes in 33 rows by 33 columns. The distance
between adjacent nodes in the same row or column is 14 meters. Therefore, any node
not at the border of the network can communicate with four neighbors. Figure 13.9
shows a typical tree on the grid topology. The sink is in the middle of the network and
four subtrees are represented by four different colors. The subtrees contain a similar
number of sensor nodes, though not exactly the same. In the simulation, we assume
that data from each subtree can be reconstructed from 55 random measurements.
Different from the chain topology where the routing path is deterministic, the grid
topology produces changing routing trees in each test run. Therefore, we run three
independent tests for each parameter setting and present the average results. In each
test run, ten messages per node are collected at given intervals.
Figure 13.10a shows that baseline transmission achieves the minimum output
interval of 5.93 seconds per message when the input interval is 4.7 seconds per
message. Figure 13.10b shows that CDG achieves the minimum output interval of
2.54 seconds per message when the input interval is 2.2 seconds per message. The

13.4 Network Capacity of Compressive Data Gathering
287
Figure 13.9 Grid topology and a typical routing tree.
Figure 13.10 Output interval versus input interval in grid topology.
capacity gain is 2.3 instead of 5. The reason is that in contention based MAC, the
transmission slots allocated to each node are not even. Nodes with heavier loads get
more time slots to transmit. Therefore, baseline transmission transmits faster than
what is assumed in scheduled MAC.
Figure 13.11 compares the packet loss ratio of the two approaches. Similar to the
results in chain topology, CDG achieves near-zero loss ratio when the network is
not overloaded. In contrast, the packet loss ratio in baseline transmission is much
higher. Even when the input interval is 10 seconds per message, the packet loss ratio
is still higher than 20%. In the grid topology, the initial delay of CDG is neglectable

288
13 Compressive Data Gathering
Figure 13.11 Packet loss ratio in grid topology.
because the tree depth is 32 hops. In our simulations, the average initial delay is less
than 0.1 second.
13.5 Experiments on Real Data Sets
The previous section demonstrated the efﬁciency of CDG under the assumption that
data are sparse and can be reconstructed from ideal random measurements. This sec-
tion will show that sensor data are indeed sparse in reality. Further, data reconstruc-
tion is highly robust and efﬁcient although real data are contaminated with noise.
13.5.1 CTD Data from the Ocean
The CTD (Conductivity, Temperature, and Depth) data come from National Oceanic
and Atmospheric Administration’s (NOAA) National Data Buoy Center (NDBC).
CTD is a shipboard device consisting of many small probes. When collecting data,
it is lowered down to the seaﬂoor, and then measures data as it ascends. Although
the CTD data are collected by one moving instrument, they demonstrate the same
properties as if they were collected by a collection of sensors.
We look into the temperature data collected in the Paciﬁc Sea at (7.0N, 180W) on
March 29, 2008 [318]. The data set contains 1000 readings obtained at different sea
depths. We plot the original data by a solid curve in Figure 13.12a. Since the readings
are piece-wise smooth, they should be sparse in the wavelet domain. Figure 13.12b
shows the 1000 coefﬁcients after a 6-level 5/3 wavelet decorrelation. There are only
40 coefﬁcients whose absolute value is larger than 0.2, accounting for only 4.0% of

13.5 Experiments on Real Data Sets
289
Figure 13.12 Results on temperature data from the Paciﬁc Ocean.
the total coefﬁcients. Although the rest of the coefﬁcients are not strictly zero, we
may set K = 40.
The compressive sensing theory suggests that data can be reconstructed with high
probability from M = 3K −4K random measurements. Figure 13.12c shows the re-
construction performance with different numbers of random measurements. Each
indicated data point is averaged over 10 test runs to avoid ﬂuctuations. Apparently,
the reconstruction precision increases as M increases. A steep rise is observed in
both ﬁgures when M becomes greater than K. When M = K ∼40, a reasonable re-
construction SNR of 35 dB can be achieved. This translates to a precision over 98%.
When M = 100 and M = 200, the reconstruction precision is 99.2% (41.9 dB) and
99.5% (46.5 dB). The black dotted curve in Figure 13.12a shows the reconstructed
data when M = 100.
13.5.2 Temperature in the Data Center
A contemporary practical application of wireless sensor networks is to monitor
server temperatures in data centers. The temperature is an indication of server load
and abnormal readings in temperature usually sound a note of warning. The sensor
data used in this research are collected from a fraction of a data center as shown in
Figure 13.13. Each rectangular shape represents a rack and the oval shape indicates
a sensor placed at the top, middle, and bottom of the rack. As the ﬁgure shows, most
of the racks are equipped with three sensors while some racks are not monitored and
a few others have one or two malfunctioned sensors. There are 498 sensors in total.
The data are measured every 30 seconds and transmitted to a sink through a baseline
scheme. We analyze these data ofﬂine to see how much trafﬁc would be reduced if
CDG was used. In this network, each node only communicates with adjacent nodes.
For simplicity, we assume that all 498 sensors form one subtree to the sink. The
energy gain over a baseline scheme is similar if sensors form two or more subtrees.
An important observation on this set of data is that sensor readings exhibit little
spatial correlations. Although the racks are physically close to each other, temper-
ature readings are dominated by server loads instead of ambient temperature. Fig-
ure 13.14 plots a snapshot of the sensor readings. For clarity, we only show the sensor
readings from the bottom of each rack (167 sensors in total) and put the data of each
column side by side. Obviously, these data are not sparse in any intuitively known

290
13 Compressive Data Gathering
Figure 13.13 Rack and temperature sensor locations.
Figure 13.14 Temperature data of the lowest slot.
domain. We have also checked the entire data set containing sensor readings from
all 498 sensors and they are not apparently sparse either. Therefore, conventional
compression mechanisms will fail in this situation.
In fact, since the 498 sensors all take values between 10 to 30 degrees centigrade,
we have elegantly reorganized di into an apparently sparse signal. In particular, we
sort di in ascending order according to their sensing values at a particular moment
t0. The resulting d vector is piece-wise smooth and sparse in the wavelet domain.
Furthermore, since server temperatures do not change violently, sensor readings col-
lected within a relatively short time period can also be regarded as piece-wise smooth
if organized in the same order. Figure 13.15a and Figure 13.16a show the ordered
sensor readings 10 minutes and 30 minutes after t0, respectively. They are generally
in ascending order with only some small ﬂuctuations. There are also a few signiﬁcant
spikes indicating abnormal temperature readings.
Based on proposed compressive data gathering scheme, we are able to recon-
struct such noisy sparse signals with spikes from M(M < N) random measurements.

13.5 Experiments on Real Data Sets
291
Figure 13.15 Original and reconstructed sensor readings at t = t0 +10.
Figure 13.15b,c and Figure 13.16b,c show the reconstruction results from M = 0.5N
and M = 0.3N measurements at two time instances. The average reconstruction pre-
cision is over 98%. More importantly, the abnormal readings are accurately captured.
To cope with the situation that temporal correlation becomes weak when the time
interval increases, we can refresh the ordering of di periodically. In particular, for
every one or two hours, the sink requests M(M = N) random measurements in one
data gathering process. When M = N, the set of equations in Eq. (13.3) is solvable
and the sink is able to obtain the exact values of di. Then, the sink can re-sort di and
use this new ordering for data reconstruction in the subsequent hour or two.
We would like to point out that both conventional compression and distributed
source coding are unable to exploit this type of sparsity, which is observed only
at certain reshufﬂed ordering. In conventional compression, explicit data communi-
cation is required between correlated nodes. If correlated nodes are not physically
close to each other, the communication between them may take multiple hops. This
introduces high overheads and makes a compression procedure costly. In distributed
source coding, nodes are classiﬁed into main nodes and side nodes. The sink allo-
cates an appropriate number of bits to each node according to the correlation pattern.
However, if the correlation pattern is based on changing sensor ordering, the sink
needs to carry out these two tasks and communicate the results to every single node
periodically. In contrast, the data gathering process in CDG is unaffected, even when

292
13 Compressive Data Gathering
Figure 13.16 Original and reconstructed sensor readings at t = t0 +30.
the ordering of di changes. The knowledge of correlation is only used in the data
reconstruction procedure.
Recall that CDG solves an l1-minimization problem deﬁned in Eq. (13.8) to re-
construct data. In previous sections, we have discussed how to select the Ψ matrix
such that sensor readings d can be represented by a sparse vector x inΨ domain. This
section shows how d can be reorganized to be a sparse signal. This unprecedented
ﬂexibility of CDG demonstrates how CDG can achieve a compression ratio of two
to three at bottleneck nodes when other conventional mechanisms fail.
13.6 Summary
We have described a novel scheme for energy efﬁcient data gathering in large-scale
wireless sensor networks based on the compressive sensing theory. We believe this is
the ﬁrst complete design to extend the original idea of the compressive sensing the-
ory in combining data acquisition and efﬁcient representation into one single process
from single and a few sensors to massive sensor networks. Through such a combi-
nation, we have shown that the proposed scheme is not only able to achieve the
needed sensor data compression; hence, lowering the global communication cost,
but also to accomplish the desired load balancing; hence, extending the network

13.6 Summary
293
lifetime, for large-scale sensor networks. In essence, the proposed scheme con-
verts the traditional compress-then-transmit process into a compressive gathering
(compress-with-transmission) process to address the two major technical challenges
that today’s large-scale sensor networks are facing. In addition, compressive data
gathering provides an elegant solution to the tough problem of coping with abnormal
reading within sensor data.
In the development of the proposed scheme, we have carried out the analysis of ca-
pacity for wireless sensor network when compressive data gathering is adopted. We
have shown that CDG can achieve a capacity gain of N/M over baseline transmis-
sion. We have also designed ns-2 simulations to validate the proposed scheme when
contention-based MAC is used. Furthermore, numerical studies based on real sensor
data not only veriﬁed data sparsity in practical data acquisition, but also demon-
strated the efﬁciency and robustness of the sensor data reconstruction with and with-
out abnormal readings.
It should be noted that the proposed CDG is not suitable for small scale sen-
sor networks when signal sparsity may not be prominent enough and the potential
capacity gain may be too small. The proposed CDG is also more effective for net-
works with a stable routing structure. This is because frequent node failure or dy-
namic route change will lead to high control overhead that potentially cancel out the
gain from data compression. We are currently investigating the extension of CDG to
more challenging networking scenarios and the exploitation of fault tolerance of the
compressive sensing principles to achieve more robust performance in sensor data
gathering.


Chapter 14
Compressive Modulation
14.1 Introduction
In digital communications, as we have discussed in Chapter 3, modulation is the pro-
cess of impressing a bitstream on a carrier wave by varying its amplitude, frequency,
or phase relative to a certain point in time. Every state of modulated waveform lasts
for a ﬁxed period of time, which is referred to as a symbol. In order to convey K bits
of data in one symbol, it needs to have 2K different states. The symbol rate of a com-
munications system is usually ﬁxed. Thus, a sender realizes different transmission
rates by adjusting K.
A symbol can also be conveniently represented as a point on a complex plane,
called constellation diagram. So, there are 2K possible points on the sender’s con-
stellation diagram. Due to channel distortion, the received symbol has a Euclidean
shift from its transmitted position. Then, the demodulation process is to estimate the
transmitted symbol through maximum likelihood detection, that is, select the closest
among 2K constellation points to the received symbol. When the noise is large, the
received symbol point may be closer to other constellation points than the one being
transmitted and thus an error occurs. The error resilience capability of a modulation
scheme is usually determined by the minimum Euclidean distance between any two
constellation points.
When the average symbol energy is ﬁxed, increasing data rate will reduce the
minimum Euclidean distance of a constellation and increase the error probability.
Therefore, the rate adaptation problem arises as how to adjust the transmission rate
by means of adjusting the number of constellation points such that the minimum
Euclidean distance of the constellation is at least two times larger than the noise shift
in most cases. Rate adaptation can be achieved through either sender adaptation or
receiver adaptation.
Most existing rate adaption schemes adopt sender adaptation. In particular, the
receiver estimates the channel condition from its received symbols and feeds it back
to the sender. According to this information, the sender chooses the most appropriate
constellation size. However, as we all know, a wireless channel is time varying and
295

296
14 Compressive Modulation
channel estimation cannot be both accurate and instant. Therefore, there is a dilemma
for sender adaptation schemes about whether to make a prompt adaptation based on
inaccurate channel parameters or to wait for accurate parameters and react slowly to
the fast changing channel.
Receiver adaptation avoids this dilemma by letting the sender always transmit at a
high rate using a ﬁxed dense constellation. The receiver decides how many and which
bits can be reliably decoded from the received symbol according to its perceived
channel quality. Two issues immediately arise: (1) how the receiver decides which
bits are correct; and (2) how to recover these corrupted bits. The ﬁrst issue is usually
addressed by using soft information to compute the conﬁdence level of a bit when
demodulating a received symbol. Only the bits with high conﬁdence are selected
by the demodulator. The second issue is more challenging. Since the positions of
corrupted bits are randomly spread, it is difﬁcult to use retransmissions to recover
them without signiﬁcant signaling overhead. Thus, a better way is to use a rateless
code, also known as fountain code, to ensure reliable data transmission.
Rateless codes are binary codes that can generate unlimited number of encoding
symbols, and have the property that the source bits can be recovered from any sub-
set of encoding symbols, as long as the number of encoding symbols is equal to or
slightly larger than the number of source bits. For receiver adaptation, the sender may
continuously generate and transmit rateless symbols. The receiver waits to collect an
adequate number of reliably received rateless symbols, and when decoding is suc-
cessful, it signals the sender to stop transmission by sending an acknowledgement.
However, in receiver-based rate adaptation, the interaction between the binary
rateless code and the soft-information decoding causes a severe performance degra-
dation when the channel condition is poor and the soft information obtained by the
demodulator cannot be used in rateless decoding. This problem is also referred to as
the mismatched decoding problem by Martinez et al. [319].
This chapter presents a novel compressive modulation (CM) scheme that solves
this problem by decoding directly from modulation symbols instead of from sepa-
rate bits. CM achieves receiver side rate adaptation and smoothly adapts its rate to a
wireless channel condition. The core component of CM is a random projection (RP)
code for generating rateless multilevel symbols from binary streams. Based on the
RP code, we develop a completely novel modulation scheme which is capable of
achieving ﬁne-grained receiver rate adaption over a large range of channel condi-
tions. Since the rateless symbol generation process in RP code resembles the linear
projection process in the emerging compressive sensing theory, we name our scheme
as compressive modulation (CM).
14.2 Background
14.2.1 Rate Adaptation
Rate adaption addresses the problem of matching the transmission rate to the time-
varying channel condition. It can be realized through both sender adaptation and

14.2 Background
297
receiver adaptation. Sender adaptation is the mainstream approach in which the
sender selects the appropriate coding and modulation parameters according to the
receiver’s feedback on the channel condition. The core problem in this approach is
which channel properties should be estimated and be used as the rate selection met-
ric.
Channel signal-to-noise ratio (SNR) is the most intuitive metric, since the Shan-
non capacity is deﬁned as a function of SNR. Receiver-based autorate (RBAR) [320]
adapts a rate based on the per-packet SNR estimation during each ready to send/clear
to send (RTS/CTS) exchange. Channel-aware rate adaptation (CHARM) [321] pro-
poses to estimate channel SNR in a longer time frame by letting a potential receiver
overhear data packets from its potential transmitter before the two parties actually
communicate. However, it is well recognized that SNR estimation cannot be both
accurate and instant.
Frame loss ratio is a more accurate but much slower metric that describes channel
condition. SampleRate [322] uses a two-second time window to obtain the packet
loss statistics. A robust rate adaptation algorithm (RRAA) [323] uses short-term ob-
servation of frame loss ratio. However, the frame loss ratio is a very coarse metric
when the statistics are based on a small number of frames.
The recent work, SoftRate [324], uses per-bit conﬁdences exported by the phys-
ical layer to estimate the interference-free bit error rate (BER) of received frames.
Using this metric can react more quickly to channel variation than using SNR or
frame loss ratio. However, in a fast fading channel where channel conditions may
change even during the transmission of one packet, none of the above metrics is
accurate enough for instant sender rate adaptation.
The alternative to sender adaptation is receiver adaptation, in which coding and
modulation parameters remain unchanged, and only demodulation and decoding pa-
rameters are adapted to channel condition. Since both channel estimation and adapta-
tion take place at the receiver, it completely avoids the feedback loop and overcomes
the dilemma about estimation accuracy and speed.
The main challenge for receiver adaptation is how to design a “one-ﬁts-all” mod-
ulation scheme such that the transmission rate can downgrade gracefully as the chan-
nel condition deteriorates. Simply using a dense constellation and transmitting at a
high rate cannot achieve this goal because it is well-known that using more points
in the constellation diagram than the channel condition permits will dramatically
increase error probability.
Adaptive demodulation [325] is the most representative work of receiver adap-
tation. It employs a rateless code to protect source bits and combat high frequency
of bit errors. In particular, source bits are ﬁrst encoded by a Raptor code [326] to
generate rateless bits. Then every 4 rateless bits form a modulation symbol, which
is mapped to a square 16-QAM constellation with Gray code. Upon receiving a
symbol, the receiver ﬁrst decides how many bits, denoted by K, can be accurately
retrieved from the modulation symbol at its perceived channel condition. Then it ad-
justs decision regions of possible bit combinations to retrieve the most reliable K bits.
These rateless bits are then accumulated for decoding. Obviously, this approach has
a saturation rate at 4 bits/Hz, because it used 16-QAM constellation at the sender.

298
14 Compressive Modulation
Increasing the density of the sender constellation may relieve the rate saturation
problem, but it will aggravate the mismatched decoding problem that degrades the
decoding performance.
14.2.2 Mismatched Decoding Problem
The problem of mismatched decoding may exist in both sender adaptation and re-
ceiver adaption approaches as long as binary codes, whether or not they are channel
codes or rateless codes, are used before modulation. This problem was most recently
studied by Martinez et al. [319] in analyzing the performance of bit-interleaved coded
modulation [327]. In this section, we will review this problem in the context of re-
ceiver rate adaptation with binary rateless codes.
Figure 14.1 shows the typical coding and modulation process in a receiver adap-
tation scheme. We denote the input bits of the system as a block x = (x1,x2,...,xN),
with block length N. The sender generates the rateless bit bi(i = 1,2,3,...) using
rateless codes, such as the LT code [328] or Raptor code [326]. Each encoded bit bi
is the logical exclusive-OR (XOR) of a randomly selected subset of input bits. A bi-
partite graph is formed to describe the relationship between source bits and encoded
bits. In order to achieve a good recovery performance, the design of graph connec-
tivity needs to follow some guidelines. In addition, Raptor code includes a precoding
stage which generates some redundant bits from the input bits. For simplicity, the
precoding process is not shown in Figure 14.1 and the redundant bits are assumed as
part of input bits.
Figure 14.1 Rateless coding and symbol mapping for receiver adaptation.

14.2 Background
299
The rateless bits are then concatenated in groups of K into a modulation sym-
bol sequence (s1,s2,s3,...). The size K is preselected and determines the maximum
transmission rate. These symbols belong to a ﬁnite alphabet S = {S1,S2,...,SM}
with M
∆= |S| = 2K. Finally, the symbol set is mapped to square quadrature amplitude
modulation (QAM) constellation using Gray code. Here, Si denotes both the modu-
lation symbol and its Euclidean coordinate to keep the notations simple. The noisy
version of the transmitted symbol is obtained at the receiver due to channel noise.
The received symbols can be represented by points in the constellation plane, and
their Euclidean coordinates can be denoted by ri(i = 1,2,3...).
The demodulation process starts from solving the maximum a posteriori proba-
bility problem based on the received symbols. In general, the transmitted symbol si
(si ∈S and i = 1,2,3,...) is assumed to be uniformly distributed. Then, si’s posterior
probability conditioned on its noisy version ri can be expressed as
P(si = Sj|ri) =
f(∥S j −ri∥2)
∑M
m=1 f(∥Sm −ri∥2),
(14.1)
where f(·) is the probability density function of channel symbol transition deﬁned
in the 2D constellation plane. If the channel is AWGN (additive white Gaussian
noise), f(·) is an exponential function. ∥·∥2 is the 2D Euclidean distance between
two points in the constellation plane. Accordingly, the posterior probability of each
bit bk(bk ∈{0,1} and k = 1,2,...,K) in si can be calculated as
P(bk|ri) =
M
∑
j=1
P(si = Sj|ri)P(bk|si = Sj).
(14.2)
With P(bk|ri), the rateless decoder is able to recover input bits in block x by a
soft-decision iterative decoding. However, a rateless decoder requires that coded bits
be independent from each other. This prerequisite does not hold if several coded bits
are sent within one wireless symbol. This can be easily proved by contradiction. In
particular, if the K bits b1b2 ...bK in symbol si are independent, then the probability
of P(si = Sj|ri) would be
P(si = Sj|ri) =
K
∏
k=1
P(bk(Sj)|ri).
(14.3)
bk(Sj) indicates that the k-th bit of si is equal to the corresponding digit value in Sj.
The P(si = Sj|ri) calculated by Eq. (14.3) is obviously different from that obtained
by Eq. (14.1).
The effect of information mismatch is minor if the noise shift is within the min-
imum Euclidean distance of a constellation. The problem of mismatched decoding
is hardly noticeable in sender adaptation schemes with channel coding, because in
these schemes the minimum Euclidean distance is adjusted to be larger than twice the
noise shift with very high probability. However, in receiver adaptation, the problem
becomes prominent because the sender always transmits at a high rate, that is, the

300
14 Compressive Modulation
minimum Euclidean distance of the constellation could be smaller than noise shift in
a large portion of modulation symbols. For this reason, a receiver adaptation scheme
will have a signiﬁcantly degraded performance at low channel SNR if it aggressively
uses a very dense constellation.
Mismatched decoding cannot be avoided if the receiver adaptation scheme uses
existing rateless codes, such as LT codes and Raptor codes, because they can only
decode from rateless bits. If there is a rateless encoder that can directly generate
multilevel symbols, and a corresponding decoder that can directly decode from the
multilevel symbols retrieved from each independent channel dimension, the problem
of mismatched decoding vanishes and the soft information can be fully and accu-
rately utilized. It is based on this understanding that we propose the completely new
RP codes inspired by compressive sensing theory to ﬁll in such seemingly unbridge-
able gap.
14.3 Compressive Modulation
The proposed compressive modulation is considered as a ﬁne-grained receiver rate
adaptation scheme. As shown in Figure 14.2, the framework consists of RP coding,
modulation, soft demodulation, and RP decoding. This section will describe these
four modules in sequence, and then discuss the RP code design for wireless modula-
tion.
14.3.1 Coding and Modulation
The key to realizing receiver rate adaptation is to design a multilevel rateless code
and a gracefully degradable modulation scheme. RP code is designed as a rateless
code that encodes binary digits into multilevel values. Let x = (x1,x2,...,xN) be a
block of input bits of length N, RP encodes them into a series of symbols (s1,s2 ...)
by random projection:
si = ci ·xT,
(14.4)
where ci is a low-density random vector. In particular, only L entries in ci are nonzero,
and they take values from a weight set {w1,w2,...,wL}. The size of L and the values
of weights are key design parameters of RP code. In Section 14.3.3, we will provide
details about the design rules and our ﬁnal choices for CM framework. The position
xà
x
s
r
Figure 14.2 The framework of the proposed compressive modulation.

14.3 Compressive Modulation
301
of the nonzero entry in ci that takes weight wl can be denoted by il(l = 1,2,...,L). In
input vector x, only the entries at corresponding positions are sampled, so Eq. (14.4)
can also be written into:
si =
L
∑
l=1
wl ·xil.
(14.5)
This random projection process can be repeated for an arbitrary number of times
and generate unlimited number of symbols. Each symbol carries virtually the same
amount of information, and can be accumulated to perform decoding. Therefore, RP
code is a rateless code.
Different from conventional rateless codes, which generate binary symbols by
logical XOR, RP code generates multilevel values through arithmetic weighted sum,
creating a ﬁnite alphabet S :
S = {∑
l∈Λ
wl|Λ ⊆{1,2,...,L}}.
(14.6)
In this alphabet, the minimum and the maximum values are smin = ∑wl<0 wl and
smax = ∑wl>0 wl.
We sequentially and evenly map symbol values to the modulated parameters of
each channel dimension. In amplitude modulation, the mapping is from [smin,smax]
to [−A,A], where A represents the maximum amplitude in use. Therefore, the actual
amplitude used to transmit si is given in Eq. (14.7). In QAM modulation, two conse-
quently generated symbols are represented by one constellation point in the complex
plane.
ai = −A+
2A
smax −smin
(si −smin).
(14.7)
Figure 14.3 illustrates the coding and modulation process with simple parame-
ter settings. In particular, the size of the weight set is L = 3, and the weights are
{1,1,1}. Obviously, the range of symbol values is [0,3], so a 4×4-QAM is used for
modulation.
For practical applications, RP code should have a larger L and more diverse
weights than the parameters given above. As a result, the encoded symbols have
a much larger alphabet size, and the constellation becomes much denser than any
practical ones in current use. In addition, the symbol values generated by RP code
are not uniformly distributed in [smin,smax]. They are concentrated in the middle of
the range. This property has its pros and cons. On the one hand, when the mean
energy of the constellation is given, the minimum Euclidean distance of the CM
constellation is larger than that of an evenly distributed constellation with the same
number of points. This partially cancels out the downside of using a dense constel-
lation. On the other hand, our constellation demands a higher peak-to-noise ratio
(PAR) of the wireless channel. Therefore, the RP code should be designed such that
the corresponding PAR is kept within a reasonable range.
In the above example, when the input bits are purely random, that is, there is
equal probability of a bit being 0 or 1, the percentage of encoded symbols 1 and
2 is three times of the symbols 0 and 3. Therefore, when the mean energy of the

302
14 Compressive Modulation
Figure 14.3 The proposed compressive modulation.
constellation is given, the minimum Euclidean distance of the CM constellation is
p
5/3 times longer than that in a normal 16-QAM constellation. The PAR of the
CM constellation is also
p
5/3 times larger.
14.3.2 Soft Demodulation and Decoding
In the proposed CM, each rateless multilevel symbol si is modulated to one channel
dimension. In amplitude modulation, the modulated amplitude ai for si is given in
(14.7). Due to channel noise, the amplitude of the received waveform, denoted as ˆai,
is a noisy version of ai. When a Gaussian channel is considered, we have:
ˆai = ai +ec
i ,ec
i N (0,σ2
c ),
(14.8)
where ec
i is the Gaussian noise with variance σ2
c . Then the received rateless symbol
ri corresponding to si can be computed as:
ri = smin + smax −smin
2A
( ˆai +A).
(14.9)
From Eq. (14.7), Eq. (14.8), and Eq. (14.9), we have:
ri = si + smax −smin
2A
ec
i .
(14.10)
Denote ei = smax−smin
2A
ec
i as the symbol noise. Since smax, smin, and A are all constants
for a given RP code design, ei is also a Gaussian variable, and its variance is σ2 =
smax−smin
2A
σ2
c .

14.3 Compressive Modulation
303
Through sequential mapping, CM successfully converts the Gaussian channel
noise into Gaussian symbol noise, making it possible for the rateless decoder to re-
cover input bits directly from multilevel symbols. It should be noted that CM utilizes
and beneﬁts from soft information. Although the received symbol ri may not belong
to the encoding alphabet S , its value is in the range of [smin,smax] in most cases.
Decoding from rateless symbols into binary digits is achieved through maximum
a posteriori estimation. Assume that M symbols are accumulated for decoding, then
the rateless decoding problem can be formulated as:
ˆx =
arg max
x∈GF(2)NP(x|r1,r2,...,rM),
s.t.
r = CxT +e,
(14.11)
where r = (r1,r2,...,rM)T is the vector of received symbols, and e = (e1,e2,...,eM)T
is the corresponding noise vector. C is an M × N matrix, in which each row is a
random projection vector.
The problem formulation of RP decoding resembles that of low-density parity-
check (LDPC) decoding. It can be similarly solved by the belief propagation al-
gorithm. However, since the symbol generation operator in RP code is arithmetic
addition instead of logical XOR, the messages sent out by symbols are calculated in
a signiﬁcantly different fashion from that in a conventional BP algorithm.
The RP decoding algorithm is described as follows.
1. Initialization:
Input bits are assumed to be random. Initial probability of both P(xj = 0) and
P(xj = 1) is 0.5. The message sent by every input node xj to its connected sym-
bol node si is initialized as
(
u(0)
ji (0) = P(x j = 0)
u(0)
ji (1) = P(xj = 1)
.
(14.12)
2. Iterations:
(i) Messages sent out by symbol nodes: According to Eq. (14.5) and Eq. (14.10),
received rateless symbol ri can be written into:
ri = w1xi1 +w2xi2 +...+wLxiL +ei.
(14.13)
Let j = il and deﬁne Xj = ri −wlxj. Then the message sent from the i-th symbol
node to the j-th variable node in the round t is:
(
v(t)
i j (0) = P(xj = 0|ri) = P(Xj = ri)
v(t)
i j (1) = P(xj = 1|ri) = P(Xj = ri −wl)
.
(14.14)
The distribution of Xj is the convolution product of the weighted distribution of
every other variable node and ei

304
14 Compressive Modulation
P(Xj) = {
⊗
ik,0<k≤L,k̸=lP(wkxik)}⊗P(ei),
(14.15)
where the distribution of weighted variable is
(
P(wkxik = 0) = u(t−1)
iki
(0)
P(wkxik = wk) = u(t−1)
kki
(1)
.
(14.16)
Inspired by Baron et al. [329], we implement the convolution through simple ad-
dition in the Fourier domain. We further simplify the decoding algorithm based
on the prior knowledge that input digits are binary.
(ii) Messages sent out by symbol nodes: The message sent by the j-th variable
node to the i-th symbol node in round t is:
(
u(t)
ji (0) = CjiP(xj = 0)∏k∈Tj\i v(t)
k j (0)
u(t)
ji (1) = CjiP(xj = 1)∏k∈Tj\i v(t)
k j (1)
,
(14.17)
where Tj is the index set of the symbol nodes connected with variable node j. If
there is an edge between variable node j and symbol node i, then i ∈Tj. Cji is a
normalization constant which ensures u(t)
ji (0)+u(t)
ji (1) = 1.
3. Stop and Output:
After T rounds of iteration, the belief propagation process stops and outputs the
following posterior probabilities of the j-th variable node being 0 and 1:
(
u(T)
j
(0) = CjP(xj = 0)∏k∈Tj v(T)
k j (0)
u(T)
j
(1) = CjP(xj = 0)∏k∈Tj v(T)
k j (1)
.
(14.18)
Then the values of input binary digits are estimated by:
xj =
(
0
u(T)
j
(0) ≥u(T)
j
(1)
1
otherwise
.
(14.19)
RP decoding through belief propagation can fully utilize the statistics of channel
noise. When the noise variance increases, successful demodulation and decoding
can be achieved by accumulating more rateless symbols. Therefore, the proposed
RP code has an inherent capability of rate adaptation. Suppose a block of N bits are
successfully decoded from M rateless symbols that are transmitted within M
2 channel
uses, the transmission rate can be calculated as 2N
M . The granularity of rate adaptation
is very ﬁne, because the transmission can be adjusted on a per-symbol basis.
The belief propagation algorithm for RP decoding shares the same assumption
with LDPC code about the connections between variable nodes and symbol nodes.
The bipartite graph should have no loops longer than T. Because our symbols are
generated randomly, the assumption cannot be guaranteed. Therefore, the damping
method, Message Damping Belief Propagation (MDBP) [330] has been applied to
the iterations for improving the recovery performance.

14.3 Compressive Modulation
305
14.3.3 Design RP Codes
An RP code is a linear rateless code which encodes binary digits into multilevel
symbols. It can be represented by a bipartite graph, where left nodes are binary input
and right nodes are encoded multilevel symbols. Every right node has the same ﬁxed
node degree L and the weights of connecting edges are taken from the same set
{w1,w2,...,wL}. The key design parameters for a RP code are the right node degree
L and the set of edge weights.
As the pursuit for RP code arises from the modulation problem, the design objec-
tive naturally is to ﬁnd a low-complexity code whose output symbols can be easily
modulated by a digital modulator and achieves high transmission rate within typical
channel SNR range.
We only consider symmetric weight sets which can be represented by {−wL/2,
...,−w1,w1,...,wL/2} and L is an even number. Symmetric weights generate zero-
centric RP symbols which can be easily mapped to the constellation. In the rest of
this section, we use only the positive weights to represent the symmetric weight set.
Without loss of generality, we only select integral power of 2 as weights, that is,
{w1,...,wL/2} ⊆{2n|n ∈N}. As the input digits are binary, such weight selection is
sufﬁcient to generate any integer number.
In wireless communications, we can assume that the operational range of chan-
nel SNR is between 5 dB to 25 dB. Conventional rate adaption selects among binary
phase shift keying (BPSK), quadrature phase shift keying (QPSK), 16-QAM, and 64-
QAM modulations, which offer rates from 1 bits/Hz to 6 bits/Hz. As each modulation
symbol contains two RP symbols, in order to achieve 6 bits/Hz, the average informa-
tion carried in one RP symbol should be no less than 3 bits. Let the encoder generate
M RP symbols, then the average symbol information can be calculated through the
mutual information between RP symbols and the input bits.
IRP = 1
M I(s1,...,sM;x1,...,xN)
= 1
M {H(s1,...,sM)−H(s1,...,sM|x1,...,xN)},
(14.20)
where H(x) is the entropy of variable x. However, RP code is considered as a rateless
code. Symbols are generated on-the-ﬂy with random projection vectors. Therefore,
we can only obtain an upper bound of the average symbol information.
IRP ≤I(si;xi,1,...,xi,L)
= H(si)−H(si|xi,1,...,xi,L)
= H(si) = −
T
∑
k=1
pk log2 pk,
(14.21)
where pk is the distribution of RP alphabet S . Equality holds if and only if RP
symbols are independent. It can be shown that this upper bound only depends on the
distribution of RP symbols.

306
14 Compressive Modulation
1248
1244
1224
124
1222
1122
122
1112
112
1111
111
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
bits/symbol
weights
Figure 14.4 Upper bound of mutual information for considered weight sets.
In general, using large L can increase per symbol information. However, we prefer
small L in RP code design for two reasons. First, in BP decoding, the number of
message passing increases exponentially with the symbol degree L. A larger L also
causes more loops in the bipartite graph that will impair BP decoding performance.
Second, large L generally results in large alphabet size of RP symbols. When the
modulation symbol energy is ﬁxed, it decreases the minimum Euclidean distance of
the constellation, and the transmission is susceptible to errors.
When L ≤4, the upper bound of IRP is less than 3 bits. Therefore, we consider
the weight selection when L ≥6. Figure 14.4 shows the upper bound of IRP for
all the possible weight sets when L = 6 and L = 8. We ﬁnd that there are seven
sets of weights whose symbol information is above 3 bits. Therefore, it may not be
necessary to consider the selections when L ≥10.
In the practical modulation, symbol information should be slightly more than
the requested because mutual information is the upper bound. Therefore, we take
{1,2,4,8}, {1,2,4,4}, {1,2,2,4}, and {1,2,4} as candidates. In order to evalu-
ate which set of weights is the most appropriate for modulation purpose, we set
up simulation experiments at typical channel SNRs and measure their achievable
rates. From Figure 14.5, we see that {1,2,4,4} outperforms other selections in most
cases. With this selection, the RP symbol range is [−11,11]. Therefore, we use
a 23 × 23 QAM constellation in the proposed CM. The mapping is shifted by a
small offset in both channel dimensions to avoid modulation symbols sitting on the
origin.

14.4 Simulation Study
307
5
10
15
20
25
30
0
2
4
6
8
EsN0 (dB)
Goodput (bits/Hz)
 
 
124
1224
1244
1248
Figure 14.5 Rate versus SNR curves for considered weight sets.
14.4 Simulation Study
We ﬁrst evaluate CM through simulations. Our simulation is set up on MATLAB R⃝
2009a. We are particularly interested in ﬁnding out how CM adapts to different
channel conditions. For wireless communications, both throughput and bit error
rate (BER) are important performance metrics. We use the joint metric goodput as
the evaluation metric. Let RT be the throughput, and Pe be the bit error rate, then
the goodput for a 1K-byte block can be computed as: RG = (1 −Pe)N · RT, where
N = 1024×8, is the number of bits in one block. Throughout the evaluation sections,
the term rate or achievable rate is referred to as goodput if not otherwise indicated.
14.4.1 Rate Adaptation Performance
In this section, we study the granularity of CM rate adaptation. The block size of
RP coding is 400 bits. It should be noted that the constellation point of CM is not
uniformly distributed. The average symbol energy is normalized to 1 by its distri-
bution, so the channel SNR can be controlled by adjusting noise power. We carry
out experiments at integer SNR values from 5 dB to 30 dB. At each SNR value,
5000 randomly generated blocks are transmitted. Each transmission stops when the
receiver has successfully decoded the input bits. The decoding interval is set to 10
RP symbols.

308
14 Compressive Modulation
5
10
15
20
25
30
0
1
2
3
4
5
6
7
EsN0 (dB)
Goodput (bits/Hz)
BPSK
QPSK
16-QAM
64-QAM
CM
Figure 14.6 Goodput of CM without channel coding.
5
10
15
20
25
30
0
1
2
3
4
5
6
7
EsN0 (dB)
Goodput (bits/Hz)
BPSK 1/2
BPSK 3/4
QPSK 1/2
QPSK 3/4
16-QAM 1/2
16-QAM 34/
64-QAM 2/3
64-QAM 3/4
CM 3/4
Figure 14.7 Goodput of CM with channel coding.
Figure 14.6 compares the goodput of CM with that of conventional modulation
schemes, including BPSK, QPSK, 16-QAM, and 64-QAM. It shows the result when
channel coding is not applied. Figure 14.7 shows the result with channel coding. In
both cases, the proposed CM can achieve smooth rate adaptation, in contrast to the
stair-shaped rates that can be achieved by switching between conventional modula-
tions. The rate for CM is virtually continuous from 0 bits/Hz to 6.4 bits/Hz without

14.5 Testbed Evaluation
309
channel coding and from 0.5 bits/Hz to 5.3 bits/Hz with 3/4 convolutional code as
speciﬁed by IEEE [331].
For conventional modulation schemes adopted in IEEE 802.11a [331], the rate is
almost zero if channel SNR is below a certain threshold and sharply rises to its sat-
urate rate when channel SNR is above the threshold. The target of conventional rate
adaptation is to make the rate as close to the envelope of all rate curves of candidate
modulations as possible. However, the envelope can only achieve a discrete rate set
due to its sharp rising characteristic. Even though the rate set can be extended by
combining the modulation rate with channel coding rate, there is still a noticeable
gap from that of CM. It is obvious that the proposed CM outperforms conventional
rate adaptation. The gain over conventional rate adaptation is the accumulated area
of the gap between the combined performance curves for the entire set of modulation
schemes and that of CM over the range of channel SNRs.
14.4.2 Sensitivity to SNR Estimation
In RP decoding, the noise variance is an important input parameter. However, in prac-
tical system, channel SNR is difﬁcult to estimate. Accurate estimation needs long
term observation while short term estimation may suffer from unacceptable uncer-
tainty. Therefore, we propose to evaluate how sensitive RP decoding is to imprecise
noise variance.
In this simulation, transmissions are taking place at four typical SNR values: 10
dB, 15 dB, 20 dB, and 25 dB. For symbols received at a ﬁxed SNR, we try to perform
RP decoding at an imprecise SNR input. The difference between actual channel SNR
and the SNR input is varied from −10 dB to 10 dB at a 2 dB step. For example, when
data are transmitted over a 10 dB AWGN channel, we use {0, 2, 4, 6, 8, 10, 12, 14,
16, 18, 20} dB as the SNR input to RP decoder. We run 500 tests for each parameter
setting.
Figure 14.8 shows the mean and standard deviation of achieved goodput. It can be
found that RP decoder is not sensitive to imprecise SNR input, especially when the
input SNR underestimates the actual channel. The performance loss is not signiﬁcant
even when the input SNR is 10 dB lower than the actual value. However, if the input
SNR is much higher than the actual SNR, a large variation in goodput is observed. In
fact, an overestimated SNR unreasonably increases the conﬁdence probability of RP
symbols, causing oscillations in belief propagation that may ﬁnally result in decoding
errors.
14.5 Testbed Evaluation
We evaluate CM performance using a SORA [332] platform. SORA is a pro-
grammable software radio platform built on commodity general-purpose PCs. Since

310
14 Compressive Modulation
0
5
10
15
20
0
2
4
6
8
Estimate EsN0 (dB)
Throughput bits/Hz
(a) Accurate SNR 10 dB
5
10
15
20
25
0
2
4
6
8
Estimate EsN0 (dB)
Throughput bits/Hz
(b) Accurate SNR 15 dB
10
15
20
25
30
0
2
4
6
8
Estimate EsN0 (dB)
Throughput bits/Hz
(c) Accurate SNR 20 dB
15
20
25
30
35
0
2
4
6
8
Estimate EsN0 (dB)
Throughput bits/Hz
(d) Accurate SNR 25 dB
Figure 14.8 CM performance with inaccurate SNR estimation.
RP decoder requires a fair amount of computation for high-speed communication,
we leave it as our future work to optimize it on software platform. In this chapter, we
mainly evaluate the modulation and demodulation performance using an ofﬂine pro-
cess. We assume instant ACK (acknowledge) after a block is successfully decoded
at the receiver.
We evaluate CM under both a static scenario, where the channel changes slowly,
and a mobile scenario, where the channel fades quickly due to multi-path effects. We
compare CM to an “Oracle” rate adaptation scheme, which adjusts the modulation
rate, for example, BPSK, QPSK, 16-QAM, or 64-QAM, based on posterior knowl-
edge of channel SNR. Obviously, this “Oracle” rate adaptation has the upper bound
performance compared to any practical rate adaptation schemes. We further compare
CM to adaptive demodulation (ADM), the state-of-the-art receiver adaptation. In our
comparisons, while we directly measure the performance of CM, we emulate “Ora-
cle” and ADM based on the same channel traces as CM to ensure these schemes are
compared under exactly the same channel conditions.

14.5 Testbed Evaluation
311
0
5
10
15
20
25
0
0.2
0.4
Percentage
EsN0 (dB)
0
1
2
3
4
5
6
7
0
0.5
1
CDF
Goodput (bits/Hz)
 
 
CM
Oracle
(a) LOS near
0
5
10
15
20
25
0
0.2
0.4
Percentage
EsN0 (dB)
0
1
2
3
4
5
6
7
0
0.5
1
CDF
Goodput (bits/Hz)
 
 
CM
Oracle
(b) LOS far
0
5
10
15
20
25
0
0.2
0.4
Percentage
EsN0 (dB)
0
1
2
3
4
5
6
7
0
0.5
1
CDF
Goodput (bits/Hz)
 
 
CM
Oracle
(c) NLOS far
0
5
10
15
20
25
0
0.2
0.4
Percentage
EsN0 (dB)
0
1
2
3
4
5
6
7
0
0.5
1
CDF
Goodput (bits/Hz)
 
 
CM
Oracle
(d) Moving
Figure 14.9 Performance comparison of CM to the “Oracle” sender-based rate adaption scheme.
The “Oracle” scheme shows an upper bound performance of any practical rate adaptation scheme.
14.5.1 Comparison to Oracle
In a static scenario, we carry out the experiments with different sender-receiver dis-
tances, with and without line-of-sight (LOS) path. The upper-row ﬁgures in Figure
14.9a,b,c summarize the SNR statistics in three sets of experiments. The lower-row
ﬁgures show the cumulative distribution function (CDF) of the achieved rate of both
the CM and “Oracle” sender rate adaptation.
The results show that CM outperforms “Oracle” in all three settings. The average
goodput CM achieved in three cases are 5.1 bits/Hz (LOS near), 4.0 bits/Hz (LOS
near), and 2.3 bits/Hz (NLOS near). It yields a performance gain from 41.6% to
100% compared to “Oracle,” which achieves an average goodput of 3.6 bits/Hz, 2.0
bits/Hz, and 1.6 bits/Hz under the same three conditions.

312
14 Compressive Modulation
5
10
15
20
25
30
0
1
2
3
4
5
6
7
EsN0 (dB)
Goodput (bits/Hz)
 
 
ADM_16QAM
ADM_64QAM
CM
(a) SNR Statistics
0
1
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
CDF
Goodput (bits/Hz)
 
 
CM
ADM_16QAM
ADM_64QAM
(b) Goodput Comparison
0
10
20
30
0
0.02
0.04
0.06
0.08
0.1
0.12
Percentage
EsN0 (dB)
(c) Simulation
Figure 14.10 Performance comparison of CM to the ADM receiver-based rate adaption scheme by
using traced channel condition.
The mobile scenario shows the similar results, as shown in Figure 14.9d. CM
achieves 56% gain over oracle adaptation in mobile scenario. The goodput of the
two schemes are 3.9 bits/Hz and 2.5 bits/Hz, respectively. This demonstrates CM
works well even when the channel condition changes very quickly.
The major performance gain of CM comes from its ability to seamlessly adjust its
modulation rate, while conventional rate adaptation has only a limited set of modu-
lation rates. Therefore, a low modulation rate has to be used even if the channel con-
dition is much higher than the requirement, but still not enough to support a higher
rate. Thus, the channel capacity is wasted. In contrast, CM achieves a modulation
rate that best ﬁts the instantaneous wireless channel and thus make full use of the
channel capacity.
14.5.2 Comparison to ADM
We compare CM to the state-of-the-art receiver rate adaptation scheme, ADM. Both
CM and ADM use the same transmission power at the sender. Figure 14.10a shows
the channel SNR statistics of the experiment.
Our ADM implementation includes the original scheme based on ﬁxed 16-QAM
as described by Brown et al. [325]. This ﬁxed 16-QAM modulation will become a
bottleneck when the channel SNR is high. For example, when the SNR is higher than
21dB, 64-QAM can actually be used. Thus, we also extend their work to use ﬁxed
64-QAM modulation. We compare CM with both variants of ADM.
Figure 14.10b shows the goodput achieved by CM, 16-QAM-, and 64-QAM-
based ADM under the same channel trace. The average goodputs achieved by the
three schemes are 3.75 bits/Hz, 2.82 bits/Hz, and 3.26 bits/Hz. The performance
gains of CM over two ADM schemes are 33.0% and 15.0%, respectively.
To have a better understanding of why CM performs better, let’s refer to Figure
14.10c. It shows simulated results of CM, 16-QAM-, and 64-QAM-based ADM with
different channel SNRs. We can see that when we choose a low ﬁxed modulation rate
(i.e., 16-QAM) for ADM, it performs well with a low SNR range (close to CM when

14.6 Related Work
313
SNR is less than 15 dB). But the rate is saturated when SNR becomes high and higher
modulation rates should be used. However, if we ﬁx a high modulation (i.e., 64-
QAM) for ADM, its performance in low SNR range is bad, due to the aforementioned
mismatched decoding problem. In a real wireless environment, the channel condition
can change widely, and thus it is difﬁcult for ADM to choose a single modulation rate
for all SNR conditions. In contrast, CM can seamlessly adapt to a wide range of SNR
and thus outperform ADM in general wireless conditions.
14.6 Related Work
This section discusses related work ranging from coded modulation to the recent
development in compressive sensing.
14.6.1 Coded Modulation
Because the proposed CM introduces rateless multilevel codes into the modulation,
it is natural to start our discussion from coded modulation even though most of them
are not designed for rate adaptation. The trellis-coded modulation (TCM) proposed
by Ungerbocke is the ﬁrst scheme of coded modulation [333, 334]. It uses trellis
codes to generate symbols from input bits with a certain redundancy and then mul-
tilevel symbols are mapped to discrete constellation points through set partitioning.
At receiver, the Viterbi algorithm is adopted for optimal joint decoding of all bits in
symbols. Inspired by this work, it is recognized that channel coding and modulation
should be combined as an entity to improve performance.
Multilevel coded (MLC) modulation is another way to combine channel coding
and modulation [335]. It ﬁrst partitions input bits into several subsets and every sub-
set is protected by a binary channel coding. Output bits at the same time instant from
all channel encoders are concatenated as a symbol. At receiver, instead of optimally
joint decoding of all bits in symbols, a sub-optimal multi-stage decoding (MSD)
is proposed to achieve good performance with limited complexity [336]. The the-
oretical analyses by Wachsmann et al. [337] show that the multi-stage decoder can
achieve channel capacity.
In order to be robust in a Rayleigh channel, Zehavi proposes bit-interleaved coded
modulation (BICM) from TCM [338], where a bit interleave module separates trellis
code from modulation. Trellis coded symbols are bit-wise interleaved with a depth
exceeding the coherence time of the fading process. In order to limit information
loss arising in this separation approach, soft information of coded bits are used in
the decoder. Caire et al. present detailed analyses on BICM from the perspective of
information theory and show that the loss in BICM is limited [327]. The fact that
BICM is a mismatched decoding has been recently recognized [319].

314
14 Compressive Modulation
The performance of coded modulation not only depends on how to combine them
together but also depends on which channel codes are selected. The discovery of
Turbo code [18] and the rediscovery of LDPC codes [16, 17] provide new research
avenues for the further development on coded modulation. These modern codes can
approach the capacity of binary-input channel with low complexity.
14.6.2 Compressive Sensing
The design of RP codes is inspired by the emerging compressive sensing theory
[295, 311] and its recent development in using low-density matrix to generate mea-
surements. Mathematically, let x = [x1x2 ...xN]T be a K-sparse signal, that is, there
are only K(K ≪N) nonzero elements in xi’s. Let us take M measurements of x
through linear projection:
y = Φ ·x
(14.22)
where Φ is an M ×N(M < N) random matrix, then x can be perfectly reconstructed
from y under certain conditions. The central problem in CS is how matrix Φ should
be designed and what algorithm should be used to recover x from the underdeter-
mined linear system deﬁned in Eq. (14.22).
The random matrix Φ is often considered as a dense matrix in the early stage
of CS theory development. Recently, several papers have reported the consideration
of the CS reconstruction from a Bayesian inference if the statistical characterization
of the signal is available [339–341]. This essentially bridges CS with low-density
parity-check (LDPC) codes, although the operation in CS is arithmetic plus instead
of logical XOR. Due to the complexity in belief propagation, CS reconstruction by
Bayesian inference can be practically implemented only if Φ is a low-density matrix.
Sarvotham et al. ﬁrst propose a sparse Φ with elements drawn from {0,1} and its
decoding is similar to that of erasure code [342]. Wu et al. formulate the decoding
from graph evolution [343]. Baron et al. further extend the sparse Φ with elements
drawn from {0,1,−1} and solve the reconstruction by belief propagation [329].
The proposed RP codes are not a simple extension to the low-density CS codes.
In particular, the pursuit for multilevel rateless codes arises from the modulation
problem in communications systems. Therefore, the code design needs to allow for
ﬁne-grained rate adaptation in a wide range of channel conditions. Our analysis on
the design of RP codes illustrates that the sparse matrices Φ with elements either
{0,1} or {0,1,−1} will create rate ceilings because the generated multilevel symbols
carry little mutual information.

14.7 Summary
315
14.7 Summary
We have described in this chapter a completely novel compressive modulation
scheme for receiver rate adaptation in wireless communication. It achieves ﬁne-
grained adaptation through a rateless multilevel code named RP code and a universal
modulation constellation. The RP encoder generates multilevel symbols from input
binary digits, and each encoded symbol is modulated to one channel dimension. The
sequential mapping from RP symbol values to modulation parameters allows for
Gaussian characterization of symbol errors, which can be readily used in the belief
propagation-based RP decoding.
We carry out extensive simulations and obtain the achievable rates of proposed
CM in the operational range of channel SNR. These results show that CM con-
sistently outperforms the rate envelope of conventional modulation, whether or not
the channel coding is applied. More importantly, we have implemented and evalu-
ated CM on a software radio testbed. The performance gain of CM over oracle rate
adaptation is above 40% in all four sets of experiments under both static and mo-
bile settings. The trace based evaluation shows that CM achieves 33% gain over the
state-of-the-art adaptive demodulation scheme. These promising results validate the
practical use of the proposed CM in real wireless environments.


Chapter 15
Joint Source and Channel Coding
15.1 Introduction
The end-to-end performance of wireless communications depends on the effective
processing in both the source and the channel. In practical communications systems,
source coding is handled at the application layer, mostly for multimedia contents, and
channel coding is most commonly performed at the physical layer (PHY). Guided
by Shannon’s separation principle, source coding and channel coding are usually ac-
complished independently. Hence, an implicit assumption at PHY is that the data
to be transmitted are compressed source without redundancies. Contrary to this as-
sumption, numerous applications inject uncompressed data into the network (e.g.,
e-mails, Web pages, and uncompressed ﬁles). The measurement on web pages [344]
shows that there exist a substantial amount of compressible data in practical network-
ing trafﬁc. Undoubtedly, intelligently utilizing data redundancy in transmission will
greatly boost communication efﬁciency.
A wireless channel is time-varying and subject to fading, additive noise, and inter-
ference. In order to increase spectrum efﬁciency, PHY has to adapt the transmission
rate to varying channel conditions. In conventional adaptive modulation and cod-
ing (AMC), the sender selects the best combination of channel coding and modula-
tion that achieves the highest transmission rate under the estimated channel condi-
tion. However, the success of AMC depends on the instant and accurate channel es-
timation, which cannot be obtained simultaneously. In addition, AMC only achieves
stair-shaped rates due to the limited number of rate combinations. To overcome these
two drawbacks, there are three recently published pieces of research on smooth and
blind rate adaptation [345–347]. The adaptation is blind in the sense that channel
information is not required at the sender, and the adaptation is smooth as it pro-
vides ﬁne-grained rate adjustment. These smooth and blind rate adaptations are very
valuable to the wireless communications, especially in fast fading channels. Unfor-
tunately, none of them take into account possible redundancies in the source data.
As it is not practical to require all applications to compress their data before
transmission, a natural question to ask will be whether it is possible to achieve
317

318
15 Joint Source and Channel Coding
high-efﬁciency compression at PHY. A straightforward answer would be no, because
conventional data compression techniques, such as Huffman coding and arithmetic
coding, need sufﬁcient statistics for a speciﬁc data type. This requirement usually
cannot be fulﬁlled because PHY contains small sets of hybrid data (the maximum IP
packet size is 1.5 KB). In addition, application data have already been represented
by streams of bits instead of logical data structures at PHY. It would require dra-
matic change to the entire network stack if the PHY needs high-level information of
application data.
In this chapter, we will further study how our proposed CM can simultaneously
achieve source compression, channel protection, and seamless rate adaptation over
the existing network stack. As we have discussed in Chapter 14, CM operates on
binary sources, and is widely applicable to time-varying wireless channels. The core
of the proposed CM is a random projection (RP) code inspired by the compressive
sensing (CS) theory [295,311]. The proposed RP code is signiﬁcantly different from
the random measurement matrices used in CS, because the source is binary and the
generated symbols will pass the existing DAC (digital-to-analog converter) and be
transmitted with the existing digital communication hardware. The RP code is also
fundamentally different from the contemporary channel codes because it generates
multilevel symbols instead of binary symbols through weighted sum operations. Ac-
cording to CS theory, the required number of symbols for successful decoding de-
creases as the source redundancy increases, and also decreases as the channel quality
increases. Therefore, a source with higher redundancy can be decoded from less
number of RP symbols and achieves a higher transmission rate. In contrast, more RP
symbols need to be transmitted when channel condition worsens, and hence it leads
to a lower transmission rate. Because the rate can be adjusted on a per symbol basis,
the adaptation becomes seamless. It is worth noting that the proposed CM design
can be easily integrated into the existing PHY layer. In particular, we have imple-
mented CM in 802.11a PHY. It simply bypasses the forward error correction (FEC)
and modulation in the existing design and directly generates the modulation symbols
on the data subcarriers. Experiments over a software radio platform have proved the
feasibility of our design.
Two technical problems in the proposed CM are studied in this chapter. First, we
shall design code for binary sources with different levels of redundancy or sparsity.
Three principles are established based on both source and channel considerations.
In particular, we shall derive the upper bound of the achievable rate for any given
RP code under source sparsity p and a given channel error distribution. Based on
these principles, we design weight multisets of RP code for typical source sparsities.
Although RP code has a saturation effect at high signal-to-noise ratios (SNRs), our
evaluations show that the same weight multiset can be used over the typical SNR
range (from 5 dB to 25 dB) for moderate source sparsity. Second, we shall design
an algorithm to reduce the decoding complexity of RP code so that the decoding
speed matches with the contemporary wireless communication rate. This is a linear
time belief propagation (BP) algorithm for the decoding of RP code. Harnessing the
binary-input property of the variable nodes, we propose to compute convolutions (at
constraint nodes) in the time domain. We shall show that this is more efﬁcient than

15.2 Related Work and Background
319
the conventional computation in the frequency domain. Observing that the proba-
bility distribution functions (PDFs) of the constraint nodes have limited support, we
devise a novel ZigZag deconvolution to further reduce the decoding complexity. We
perform careful analysis to show that this decoding algorithm is nearly 20 times
faster than the CS-BP algorithm for a typical code.
15.2 Related Work and Background
In this section, we will ﬁrst discuss the related work on joint source-channel coding
and rate adaptation, and then provide background information on the compressive
sensing theory.
15.2.1 Joint Source-Channel Coding
Joint source-channel coding (JSCC), in its broad sense, covers all schemes that
jointly consider source coding and channel coding under a uniﬁed resource con-
straint and/or toward a common optimization objective. Existing work on JSCC can
be classiﬁed into three categories.
The ﬁrst category usually applies to image/video transmission. Among them,
some works have exploited the rate-distortion behavior of the source and study the
problem of allocating the transmission rate between the source coder and the channel
coder to minimize expected distortion [348–350] or optimize the power consump-
tion [351]. Others have designed unequal error protection (UEP) for progressively
coded sources. They are called JSCC because more important contents (source) are
encoded with stronger channel codes [352]. Both variations of LDPC (low-density
parity-check) codes, such as rate-compatible LDPC codes by Pan et al. [353], and
rateless codes, such as Raptor codes over GF(2) by Aditya and Katti [354], and Rap-
tor codes over GF(4) by Bursalioglu et al. [355], have been studied in the literature.
Works in the second category jointly consider the compression of correlated
sources and channel protection of them. Liveris et al. [356] show that LDPC codes
are able to compress correlated binary sources close to the Slepian-Wolf limit. Sar-
tipi and Fekri [357] apply a similar idea to wireless sensor networks (WSNs) for
data collection. Zhong and Garcia-Frias [358] and Xu et al. [359] use low-density
generator matrix (LDGM) codes and Raptor codes for distributed JSCC.
More related to the proposed scheme are the works in the third category, which
apply the same code for both compression and protection of a single source. Research
has shown that conventional channel codes can be used for source compression. In
particular, LDPC codes, Raptor codes, and Turbo codes [360–362] can be concate-
nated with a Burrows-Wheeler transform (BWT) to serve this purpose. Furthermore,
Fresia et al. [363] use the concatenation of two independent LDPC codes in the trans-
mitter, and implement a joint BP decoder at the receiver. Zhu and Alajaji [364] and

320
15 Joint Source and Channel Coding
Cabarcas et al. [365] design Turbo code for transmitting nonuniform binary mem-
oryless or identically distributed (i.i.d.) sources over noisy channels. However, all
these codes are designed at a ﬁxed rate. None of them is able to achieve smooth rate
adaptation.
15.2.2 Coded Modulation
The proposed CM scheme is essentially an enhanced coded modulation with em-
bedded compression, so we also brieﬂy review related works on coded modulation.
The trellis-coded modulation (TCM) proposed by Ungerboeck is the ﬁrst scheme
of coded modulation [333, 334]. By introducing trellis coding and set partitioning
in the PHY layer, the minimum free Euclidean distance is maximized. Multilevel
coded (MLC) modulation [335, 337] is another elegant way for coded modulation.
For a M bits modulation, the channel coding is divided into M levels and each level
is designed independently. The advantage of MLC is that there is no need to design
the coding on Euclidean distance. Any binary code designed on Hamming distance
can be used.
To address the issue of robustness under a Rayleigh channel, Zehavi proposes bit-
interleaved coded modulation (BICM) [338] and Caire et al. [327] present a detail
analysis on BICM from the information theoretic perspectives. On the one hand, bit
interleaving increases the channel diversity, which equivalently increases the Ham-
ming distance of coding. On the other hand, inﬁnite depth interleaving tackles the
mismatch decoding issue [319].
The discovery of Turbo code [18] and the rediscovery of LDPC codes [16, 17]
open up new research avenues for advance development in contemporary coded mod-
ulation. With these capacity-achieving codes, most of the existing coded modulation
schemes can approach the Shannon limit at the designed SNR. However, they lack
rate adaptation capability, and have to rely on separate adaptive modulation to change
the rate when channel varies dramatically. In addition, the capacity-achieving perfor-
mance usually requires inﬁnite block coding length and high complexity.
15.2.3 Rate Adaptation
Rate adaptation refers to the techniques which adjust the modulation, coding, power,
and other protocol parameters based on varying channel conditions. The most well-
known rate adaptation technique is adaptive modulation and coding (AMC), which
has been extensively studied [323,366–368] and practically applied to wireless sys-
tems. However, AMC has two major drawbacks. First, rate selection relies heavily on
accurate and instant channel estimation, which cannot be simultaneously obtained in
practice. Second, AMC is only able to achieve stair-shaped rates due to limited rate
combinations.

15.2 Related Work and Background
321
Hybrid automatic repeat request (HARQ) [369] is a supplemental technique to
AMC. The incremental redundancy scheme, also known as type II HARQ, can pro-
vide a smoother rate by employing either a rate compatible code, for example, punc-
tured Turbo code [370] or rateless codes, for example, Raptor code [371]. However,
both codes have a limited rate adaptation range.
A more recent thread of research is able to achieve smooth and blind rate adap-
tation in a wide range of channel conditions. Three independent works on this topic
have been published in recent years. Gudipati and Katti [346] propose a scheme
named Strider which appends a minimum distance transformer (MDT) after exist-
ing coding and modulation. The free distance of constellation can be transformed to
ﬁt the channel condition. Perry et al. [347] invent the spinal codes, which replace
the state transition of convolution code in TCM by an inﬁnite state hash function.
The rateless property is achieved by reﬁning the state transition. The previous work
on rateless compressive modulation [345] proposes a novel bit to symbol mapping,
and the averaged bit energy can be smoothly reﬁned by accumulating symbols at the
receiver. The rate adaptation is therefore seamless.
Though not explicitly stated, all these rate adaptation works assume that PHY
data are incompressible. When this is not the case, that is, when data contain non-
negligible redundancy, the end-to-end performance of these schemes becomes sub-
optimal. Through extensive analyses, we discover that the RCM we developed earlier
can be naturally extended to opportunistically exploit data compressibility. In other
words, the bit-to-symbol mapping can be used for JSCC. When properly designed,
RCM has the potential to signiﬁcantly improve the system throughput.
15.2.4 Compressive Sensing
The bit-to-symbol mapping in RCM converts source bits into multilevel symbols
through weighted sum operation. It is essentially a CS [295,311] scheme with binary
input. Therefore, we ﬁrst review this emerging theory with special emphasis on its
connection with channel codes.
CS theory states that an n-dimensional signal having a sparse or compressible
representation can be reconstructed from m linear measurements even if m < n
[295,372]. Traditionally, the signals under consideration are sparse or compressible
signals in RN, the projection matrix Φ is a dense matrix in Rm ×Rn, and CS recon-
struction can be achieved by l1-minimization [373], matching pursuit [374], iterative
threshold methods [375], and subspace pursuit [376]. Although these algorithms can
be extended to sparse Φ or the binary signal, the decoding complexity is high due to
the nature of the iterative greedy algorithms.
It is observed that CS can be used as a joint source-channel coding scheme
[377–381]. Bajwa et al. [378] and Chen et al. [381] examine the energy-performance
design space of CS, and show that CS is robust to channel fading and noise. Duarte
et al. [377] and Feizi et al. [379, 380] apply CS to compress correlated sources
and protect the measurements under noisy channels. Their results show that a CS

322
15 Joint Source and Channel Coding
decoder can provide trade-offs between rate and decoding complexity. In addition,
CS-based joint source-channel coding scheme has continuous rate-distortion perfor-
mance in a noisy Gaussian channel. This research is done in the context of wireless
sensor systems and the application of CS theory is rather “traditional,” that is, the
sources are highly sparse real-valued sensor readings and the CS decoding is based
on computation-intensive convex optimization. These schemes are not readily ex-
tended to the transmission of generic binary data seen in the current physical layer
of communications systems.
A less traditional application of CS theory is to use it purely as channel codes and
to apply various restrictions to reduce its complexity. Sarvotham et al. [342] design
the Sudocodes, which can be used as erasure codes for real-valued data. Both encod-
ing and decoding complexity can be reduced by limiting Φ to sparse binary matrices.
Wu et al. [343] and Liu et al. [382] consider binary source in CS, and derive closed-
form formulation for its decoding. However, these works only consider noiseless
case. CS-BP [329] provides a solution to decode noisy CS under certain constraints.
It considers a sparse Φ whose nonzero entries are drawn from Rademacher distribu-
tion, and adopts BP algorithms for decoding.
The complexity of CS-BP decoding is much higher than LDPC decoding because
the constraint nodes have to compute convolutions of several PDFs, since CS com-
putes measurements via weighted sum operation instead of logical exclusive-OR
(XOR). A standard solution will be to perform processing in frequency domain via
fast Fourier transform (FFT) and inverse FFT (IFFT), in order to convert convolution
to multiplication and deconvolution to division. However, such processing is not ef-
ﬁcient for binary-input PDFs. A better solution to this complexity problem is critical
for practical deployment of CM, so as to facilitate the broader application of CS.
15.3 Compressive Modulation (CM) for Sparse Binary Sources
Note that the physical layer is responsible for transmitting raw bits instead of logical
data packets over a physical link. The redundancy of physical layer data can be easily
modeled by unequal probabilities on the appearance of 0’s and 1’s [383]. In partic-
ular, we model the input bits as i.i.d. Bernoulli random variables with probability
P(b = 1) = p. Without loss of generality, we consider only the cases where p ≤0.5,
as a bitstream with p > 0.5 can be converted to a stream with P(b = 1) = 1 −p
through simple bit ﬂipping. Borrowing the terminology from the CS theory, we call
p the source sparsity or we call the length-N bit vectors p-sparse.
For a sparse binary source, the bit-to-symbol mapping serves dual purposes in
data compression and channel coding. We name this process random projection cod-
ing. According to the CS theory, the number of symbols (M) required to decode
the source is proportional to the source sparsity p. Therefore, the modulation rate
for sparse source, that is, p < 0.5, will be higher than that for non-sparse source
(p = 0.5). The compression gain is thus achieved.

15.3 Compressive Modulation (CM) for Sparse Binary Sources
323
In Chapter 14, we selected the weight multiset W = {±1,±2,±4,±4} for encod-
ing non-sparse sources. However, for sparse sources, this weight selection may not
be optimal. The reason is that the distribution of the symbol alphabet Ψ, denoted by
P(Ψ), plays a key role in achieving the communication efﬁciency. As P(Ψ) is jointly
decided by weight multiset and source sparsity, it is crucial to study how these two
factors affect each other and how to select the proper weight multiset for different
source sparsity.
15.3.1 Design Principles
This section outlines three design principles which should be enforced in RP code de-
sign. The ﬁrst two principles concern individual RP symbols, and the third principle
takes the dependency of RP symbols into consideration. Following these principles,
which are necessary conditions to ﬁnd an optimal code, we derive several guidelines
to facilitate code design.
Principle 1 The entropy of RP symbols should exceed half of the desired maximum
transmission rate.
One major objective of any physical layer design is to achieve high transmission
rate. The achievable rate is bounded by the entropy of wireless symbols. As every
two RP symbols constitute one wireless symbol, the entropy of RP symbols should
exceed half of the desired maximum transmission rate.
We have deﬁned Ψ = {ψ1,ψ2,...ψK} as the symbol alphabet and P(Ψ) as the
symbol distribution. In particular, we denote P(ψk) as the probability that symbol s
is equal to ψk. By deﬁnition, the entropy of RP symbols, denoted by H(s), is:
H(s) = −
K
∑
k=1
P(ψk)·logP(ψk).
(15.1)
It should be noted that both Ψ and P(Ψ) are functions of weight multiset W and
source sparsity p, such that H(s) is a function of W and p too.
We consider two anchors on symbol entropy. First, the highest modulation rate
used in Wireless Local Area Network (WLAN) is 6 bits/s/Hz achieved with 64-
QAM. Therefore, each RP symbols should carry at least 3 bit information (if the
source has been optimally compressed) to avoid early rate saturation. Second, the
typical SNR range of WLAN is 5 dB to 25 dB, and the Shannon capacity for 25 dB
additive white Gaussian noise (AWGN) channel is around 8.3 bits. Therefore, it is
not necessary to consider weight sets whose RP symbol entropy is larger than 4.15
bits.
Principle 2 The generated RP symbols should have a ﬁxed mean regardless of
source sparsity.

324
15 Joint Source and Channel Coding
We consider a communications system with an average (i.e., per wireless symbol)
power constraint of 2E, or equivalently a constraint of E per RP symbol. Let ERP
denote the average symbol energy before power scaling. We shall minimize ERP so
that each RP symbol can be scaled by a larger factor in transmission, and becomes
more robust to channel noise.
Given a set of RP symbol Ψ and its distribution P(Ψ), the mean and variance of
the RP symbols can be calculated as:
E[ψk] =
K
∑
k=1
ψk ·P(ψk),
(15.2)
Var[ψk] =
K
∑
k=1
|ψk −E[ψk]|2 P(ψk).
(15.3)
It is known that ERP is minimized when each RP symbol is shifted by E[ψk] in the
modulation constellation, and the minimum achieved is exactly the symbol variance.
As it is not practical to shift symbols differently when source sparsity p varies, the
RP symbols should have a ﬁxed mean regardless of source sparsity.
Lemma 1 The weight multiset has a zero mean is the sufﬁcient and necessary condi-
tion for the generated RP symbols to have a ﬁxed mean regardless of source sparsity
p.
Proof. The mean of RP symbols can be written into:
E[ψk] = E
"
L
∑
l=1
wlbl
#
=
L
∑
l=1
wl ·E[bl] =
L
∑
l=1
wl · p,
(15.4)
which is a function of p. It does not change with p iff ∑L
l=1 wl = 0, that is, the weights
are zero-mean.
Principle 3 Let GM be the sub-matrix composed of the ﬁrst M rows of the encoding
matrix G. Denote GM[i] as the ith column of GM. Given a weight multiset, G should
be organized such that mini ||GM[i]||2 is maximized for all possible M.
CM can be viewed as a coded modulation scheme. Therefore, it is important to
measure the free distance d free of the code, which is deﬁned as the minimum distance
between any two codewords (vectors of RP symbols). A large free distance indicates
that a sequence of wireless symbols are robust against channel noise.
Denote s(b) as the codeword generated from bit block b by a RP code, then the
free distance of the code is:
d2
free =
min
b1̸=b2∈{0,1}N ||s1(b1)−s2(b2)||2.
(15.5)
Usually, the free distance is evaluated by taking an all-zero vector as reference. Since
the RP encoding of an all-zero bit sequence will generate an all-zero codeword, the

15.3 Compressive Modulation (CM) for Sparse Binary Sources
325
free distance can be deduced to a simpler form d2
free = min||s(b)||2. The minimum
is usually taken when there is only one bit 1 in the source vector b, and the minimum
value is ||G[i]||2.
However, the proposed CM is a rate adaptation scheme. The transmission could
stop at any time before all the N symbols generated by G are transmitted. Suppose
the decoding is successful after the ﬁrst M symbols are transmitted, the actually
encoding matrix used is GM. Therefore, we shall require that GM creates a large free
distance for all possible M.
15.3.2 Weight Selection
The achievable transmission rate of CM is a function of source sparsity, channel
condition, and RP code design. Ideally, there is an optimal code for each source
sparsity and channel condition. However, CM is designed for “blind” rate adaptation,
that is, the channel condition is not known to the sender. Therefore, the objective of
RP code design is to ﬁnd a set of weights which achieve an overall high throughput
for the primary SNR range of wireless channels.
We consider source sparsity between 0.1 and 0.5, and pick four representative
values of source sparsity including 0.1, 0.15, 0.25, and 0.5 for the performance study.
These values are selected such that their entropies are evenly spaced. For the sake
of simplicity, we focus on the integer weights {1,2,4...}, which are powers of 2.
According to Principle 2, the weights should have zero-mean. A simple choice is
to have positive-negative symmetric weights. In addition, the size of weight multiset
does not need to exceed 20. According to the numerical results by Barron et al. [329],
for weights {−1,1} the optimal check node degree is Lopt ≈2/p beyond which the
performance gains will become marginal. Therefore, Lmax = 20 will be sufﬁcient for
source sparsity 0.1 and above.
Table 15.1 lists several candidate weight multisets that satisfy the above require-
ments and Principle 1. The entropies of the generated RP symbols are listed in the
last column. Next, we could select among these weight multisets by two ways.
The ﬁrst way is through MATLAB simulations. For each weight multiset and
source sparsity combination, 106 bits are transmitted over AWGN channel with
SNR ranging from 5 dB to 25 dB. These throughputs are shown in Figure 15.1. We
Table 15.1 Candidate weight multisets.
W1 {±1,±1,±1,±1,±1,±1,±1,±1,±1,±1} 6.42
W2 {±1,±1,±1,±1,±1,±1,±1,±1,±2,±2} 7.09
W3 {±1,±1,±1,±1,±1,±1,±2,±2}
6.90
W4 {±1,±1,±1,±2,±2,±2}
7.00
W5 {±1,±1,±1,±2,±4,±4}
8.37
W6 {±1,±2,±4,±4}
8.28
W7 {±1,±1,±4,±4}
7.91

326
15 Joint Source and Channel Coding
5
10
15
20
25
0
2
4
6
8
10
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
W1
W2
W3
W4
W5
W6
W7
R
(a) p = 0.50
5
10
15
20
25
0
2
4
6
8
10
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
W1
W2
W3
W4
W5
W6
W7
R
(b) p = 0.25
5
10
15
20
25
0
2
4
6
8
10
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
W1
W2
W3
W4
W5
W6
W7
R
(c) p = 0.15
5
10
15
20
25
0
2
4
6
8
10
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
W1
W2
W3
W4
W5
W6
W7
R
(d) p = 0.10
Figure 15.1 Weight selection according to throughput in a simulated AWGN channel.
observe that no single weight multiset can excel for all cases of source sparsity over
the entire channel SNR range. However, if the primary SNR range is 5 dB to 20 dB,
as in a typical WLAN, we could make the following choices: W0.5 = W6, W0.25 = W5,
W0.15 = W4, and W0.1 = W3. For completeness, the black curve in Figure 15.1 shows
the performance of using random weights drawn from the standard Gaussian distri-
bution. We adopt the decoding algorithm proposed by Rangan [384]. Comparison
shows that random weights do not perform well especially when the source is not
sparse or has moderate sparsity.
Note that when the primary SNR range changes, different choices can be made.
For example, if the channel varies between 18 dB to 25 dB, W5 (instead of W4) should
be selected for source sparsity p = 0.15 according to Figure 15.1c. If the channel is
always below 12 dB, and the source sparsity is 0.5, then according to Figure 15.1a,
all the listed weight multisets have similar performance. In addition, the saturation
effect which has been observed in linear encoding of real symbols also appears here.
Different weight multisets have different saturation SNR and rates, and it is also
affected by source sparsity.
The second way to select weights is through extrinsic information transfer (EXIT)
chart analysis [385]. The EXIT chart has been widely used for the design and anal-
ysis of LDPC codes [386]. The RP codes resemble LDPC codes in that they can be

15.3 Compressive Modulation (CM) for Sparse Binary Sources
327
represented by bipartite graphs too, where variable nodes are source bits and check
nodes are RP symbols. We refer to Brink et al. [385, 386] for further details on ex-
trinsic information processing.
As it is difﬁcult to obtain a closed-form expression for the extrinsic information
transfer in RP decoding, we measure the mutual information after each iteration of
decoder processing. We use the notation by Brink [385] and write IA for the average
mutual information between the bits and the a priori probabilities. Similarly, we write
IE for the average mutual information between the bits and the extrinsic probabilities.
However, since the source has sparsity, [Eq. (12) in 385] should be modiﬁed to:
IA = (1−p)
Z +∞
−∞pA(ξ|X = 0)log2
pA(ξ|X = 0)
pA(ξ)
dξ
+ p
Z +∞
−∞pA(ξ|X = 1)log2
pA(ξ|X = 1)
pA(ξ)
dξ,
(15.6)
where pA(ξ) = p · pA(ξ|X = 1) + (1 −p) · pA(ξ|X = 0). The extrinsic information
IE can be calculated similarly.
Due to the space limit, we only show the EXIT chart for source sparsity p = 0.5
and p = 0.1 and compare weight multiset W3 through W6 in Figures 15.2 and 15.3.
For successful decoding, two curves in a chart should cross at the source entropy
(H(p)). Figure 15.2 is obtained when SNR is 20 dB and 190 RP symbols are trans-
mitted for a length-480 bit block (spectrum efﬁciency of 5.05 bit/s/Hz). It is clear
from the ﬁgure that weight multiset W3 and W4 cannot decode successfully under
this setting. W5 is very close to successful decoding, but the two curves do not reach
1 in Y-axis. W6 is the weight multiset we chose for p = 0.5, and it can be seen that
it ensures successful decoding at such a high spectrum efﬁciency. In addition, the
tunnel between the two curves is still quite wide, indicating that a higher spectrum
efﬁciency could be achieved.
Figure 15.3 can be interpreted similarly. Since the source sparsity is 0.1, the two
curves should cross at H(0.1) = 0.469. Under this test setting, W5 and W6 fail decod-
ing, and W3 and W4 succeed. We further decrease the value of M to 120, and ﬁnd that
only W3 decodes successfully. This is also consistent with our previous selection.
15.3.3 Encoding Matrix Construction
According to Principle 3, the construction of the encoding matrix G affects CM
performance, especially when the channel condition is good and the required number
of RP symbols for decoding is small. Next, we present three steps to construct G that
satisﬁes Principle 3. We will take W6 as an example.
First, we construct three elementary matrices A1, A2, and A4. The structure of A1
is shown as follows. Matrices A2 and A4 have the same structure, but the nonzero
values are replaced with +2/−2 or +4/−4,

328
15 Joint Source and Channel Coding
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(a) W3
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(b) W4
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(c) W5
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(d) W6
Figure 15.2 EXIT chart for different weight multisets when SNR = 20 dB, p = 0.5, M = 190 for
N = 480.
0
0.1
0.2
0.3
0.4
0.469
0
0.1
0.2
0.3
0.4
0.469
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(a) W3
0
0.1
0.2
0.3
0.4
0.469
0
0.1
0.2
0.3
0.4
0.469
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(b) W4
0
0.1
0.2
0.3
0.4
0.469
0
0.1
0.2
0.3
0.4
0.469
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(c) W5
0
0.1
0.2
0.3
0.4
0.469
0
0.1
0.2
0.3
0.4
0.469
IA,VND, IE,CND
IE,VND, IA,CND
 
 
CND
VND
(d) W6
Figure 15.3 The EXIT chart for different weight multisets when SNR = 15 dB, p = 0.1, M = 120
for N = 480.

15.4 Belief Propagation Decoding
329
A1 =


+1 −1
+1 −1
...
+1 −1

.
Second, we form matrix G0 by stacking random permutations of A1, A2, and A4
as follows:
G0 =


π(A4) π(A4) π(A2) π(A1)
π(A2) π(A1) π(A4) π(A4)
π(A4) π(A4) π(A1) π(A2)
π(A1) π(A2) π(A4) π(A4)

,
where π(·) denotes randomly permutated columns of a matrix. This matrix has a
dimension of N/2 × N, and ensures that mini ||GN/2[i]||2 = 12 + 22 + 42 + 42 = 37.
Further, mini ||GN/4[i]||2 = 12 +42 = 17. By using different permutation choices, we
may construct a virtually unlimited number of matrix G0.
The third and ﬁnal step in constructing G is to stack all the randomly generated
G′
0s. In practice, we only stack two matrices to form an N ×N matrix, and repeatedly
use it when the channel condition is poor.
15.4 Belief Propagation Decoding
The RP code can be represented by a bipartite graph as LDPC code, where variable
nodes are source bits and constraint nodes are RP symbols. Hence, the RP code
can be decoded through belief propagation in a similar fashion. However, since RP
symbols are generated by arithmetic addition rather than logical XOR, the belief
computation at constraint nodes is much more complex than that in LDPC decoding.
Reducing the computational complexity of the decoding algorithm has become the
key to the success of the entire CM scheme.
We discover that, for binary variable nodes, the belief computation at constraint
nodes can be more efﬁciently accomplished by direct convolution rather than the
fast Fourier transform (FFT) as used in CS-BP. For each constraint node, we com-
pute the convolution of the distributions for all its neighboring variable nodes, and
then reuse the convolution for all outgoing messages by deconvolving the distribu-
tion of the node being processed. To save computational cost, the zero multiplication
in the convolution is avoided. In addition, we propose ZigZag iteration to guaran-
tee a unique solution to the deconvolution. Finally, we show through analysis that
the proposed decoding algorithm is computationally more efﬁcient than the CS-BP
algorithm.
Let v and c denote the variable node and the constraint node, respectively.
Throughout the algorithm description, we introduce some notations as listed in Ta-
ble 15.2.

330
15 Joint Source and Channel Coding
Table 15.2 Deﬁnitions for belief propagation.
w(v,c)
The weight on the edge between v and c.
µv→c
The message sent from v to c.
µc→v
The message sent from c to v.
p
The a priori probability of bit 1 for all variable nodes.
pv(·)
The Probability Density Function (PDF) of variable node v.
pc(·)
The PDF of constraint node c calculated based on the PDFs of all its neighboring
variable nodes.
pc\v(·)
The PDF of constraint node c calculated based on the PDFs of all its neighboring
variable nodes except variable node v.
RP-BP Decoding Algorithm
1. Initialization: Initialize messages from variable nodes to constraint nodes with a
priori probability
µv→c = pv(1) = p.
2. Computation at constraint nodes: For each constraint node c, compute the prob-
ability distribution function pc(·) via convolution Eq. (15.7). For each neighbor-
ing variable node v ∈n(c), compute pc\v(·) via deconvolution Eq. (15.8):
pc = (∗)v∈n(c) (w(c,v)· pv),
(15.7)
pc\v = pc ˜∗(w(c,v)· pv).
(15.8)
Then, compute pv(0) and pv(1) based on the noise PDF pe and the received
symbol sc:
pv(0) = ∑
i
pc\v(i)· pe(sc −i),
(15.9)
pv(1) = ∑
i
pc\v(i)· pe(sc −i−w(c,v)).
(15.10)
Finally, compute the message µc→v via normalization:
µc→v =
pv(1)
pv(0)+ pv(1).
(15.11)
3. Computation at variable nodes: For each variable node v, compute pv(0) and
pv(1) via multiplication:
pv(0) = (1−p) ∏
u∈n(v)
(1−µu→v),
(15.12)
pv(1) = p ∏
u∈n(v)
µu→v.
(15.13)
Then for each neighboring constraint node c ∈n(v), compute µv→c via division
and normalization:

15.4 Belief Propagation Decoding
331
Figure 15.4 Convolution by shift addition (from top to bottom) and ZigZag deconvolution (from
bottom to top).
µv→c =
pv(1)/µc→v
pv(0)/(1−µc→v)+ pv(1)/µc→v
.
(15.14)
Repeat steps 2 and 3 until the maximum iteration time is reached. We ﬁnd that
the maximum iteration time can be set to 15, beyond which any performance
gain is marginal.
4. Output: For each variable node v, compute pv(0) and pv(1) according to Eq.
(15.12), and output the estimated bit value via hard decision.
The main difference between this RP-BP decoding algorithm and the CS-BP de-
coding algorithm lies in the computation at the constraint nodes (step 2). First, we
process the variable nodes and the noise node in separate steps, because the former
is binary and the latter is continuously valued. Second, we propose a ZigZag decon-
volution to compute pc\v in Eq. (15.8). Figure 15.4 depicts the convolution by shift
addition and ZigZag deconvolution.
The convolution ﬂow is shown from top to bottom. In this example, the weight
between variable node v and constraint node c is w(v,c) = −4. Hence, the PDF of
w(v,c) · pv only has two spikes at −4 and 0. The convolution of w(v,c) · pv and any
PDF pc′ (including but not limited to pc\v) can be computed by
(pc′ ∗w(v,c)· pv)(n) = pv(0)· pc′(n)+ pv(1)· pc′(n−w(v,c)).
(15.15)
The addition in the equation is shown in the middle of the ﬁgure.

332
15 Joint Source and Channel Coding
Table 15.3 Complexity comparison between RP-BP and CS-BP.
W
RP-BP
CS-BP
×
+
×
+
±(1244)
492 246 8192
9216
±(111244)
856 428 12288 13824
±(111222)
640 320 12288 13824
±(11111122) 954 477 16384 18432
The deconvolution ﬂow is shown from bottom to top in Figure 15.4, in which the
rectangles highlight the ZigZag process. In this ﬁgure we demonstrate the ZigZag
deconvolution from right to left. In theory, it can be performed in both directions.
However, due to the computational accuracy of the C program, the practical direction
should be determined by the values of pv(0) and pv(1). When pv(0) > pv(1), it
should be computed by
pc\v(n) = pc(n)−pc\v(n−w(v,c))× pv(1)
pv(0)
.
(15.16)
When pv(0) ≤pv(1), it should be computed by
pc\v(n) = pc\v(n+w(v,c))× pv(0)−pc(n+w(v,c))
pv(1)
.
(15.17)
Let nmin and nmax be the minimum and the maximum value of constraint node c. For
n /∈[nmin,nmax], pc\v(n) = 0. Thus, we can get a unique solution for the deconvolution
by recursion.
Similar to CS-BP, the complexity of RP-BP algorithm is O(N). However, the
linear scaler in front of N in RP-BP is much smaller than CS-BP. Since most of the
computation is taken by the constraint node message and other computation cost of
both algorithms are the same, we compare the computation cost on constraint node
in each iteration, as shown in Table 15.3. We can see that the computation cost of
CS-BP is around 20 times of RP-BP.
15.5 Performance Evaluation
In this section, we will report the performance evaluation of the proposed CM
through the simulations on MATLAB 2011b and the emulations based on traced
channel state information. Four representative values of source sparsity including
0.1, 0.15, 0.25, and 0.5 are considered. The primary evaluation metric is throughput
(in bit/s/Hz).

15.5 Performance Evaluation
333
15.5.1 Implementation
We implement CM and three reference schemes as follows.
CM: For each block of information bits, the sender progressively transmits RP
symbols with a given step size until an acknowledgment is received or the maximum
number of transmission is reached. At the receiver end, the RP symbols are accu-
mulated for RP-BP decoding. If the demodulated bits pass the cyclic redundancy
check (CRC), an acknowledgment will be delivered to the sender. Otherwise, more
RP symbols are needed to perform the next round of decoding.
The bit block length for the RP code is set to be N = 480, which is a multiple of
all the possible weight multiset sizes. The progressive transmission step is set to be
∆M = 24 and the maximum number of symbols for transmission is set to be Mmax =
1920, which is equivalent to the rate for the Physical Layer Convergence Protocol
(PLCP) header transmission in 802.11 PHY. In the PLCP header, we introduce 3
bits. One is the bit ﬂipping indicator and the remaining two are the source sparsity
indicator. These three bits can be placed in the reserved position of the PLCP header.
Therefore, the overhead is negligible.
BICM with ideal source compression (denoted by BICM for brevity): We imple-
ment BICM [327], which is the state-of-the-art coded modulation scheme. In order
to avoid the performance loss due to short block length, we implement BICM with
2304 block length and 23040 interleaver length, which is the longest block length
setting deﬁned in the WiMax standard [387]. The coding rates are 1
2, 2
3, 3
4, and 5
6,
and the modulation schemes are QPSK, 16-QAM, 64-QAM, and 256-QAM. Alto-
gether, they create 16 combinations and 14 identical rates.
As BICM does not have compression capability, we assume an ideal source com-
pressor for BICM capable of representing a length-N source block with NH(p) bits,
where H(p) is the source entropy. For simplicity, we just generate source blocks of
length NH(p) from non-biased Bernoulli trial, but count N bits for each block when
computing the throughput.
HARQ with ideal source compression (denoted by HARQ for brevity): We imple-
ment type-II HARQ (also known as incremental redundancy) which is the state-of-
the-art rate adaptation scheme. Rate-1/3 Turbo code is used as the mother code. As
in Wideband Code-Division Multiple Access (WCDMA) and long-term evolution
(LTE), the code length is 1024 bits, and the component encoder is based on recur-
sive convolution code with polynomial (13,15) in octal. The puncture period is set
to 8 for smooth rate adaptation, and the puncture pattern is the same as that em-
ployed by Rowitch and Milstein [370]. Rates corresponding to
8
8+l (l = 0,2,...16)
are thus created. Due to the limited adaptation range of HARQ, three modulation
schemes QPSK, 16-QAM, and 64-QAM are adopted. Different modulation and cod-
ing schemes create 27 combinations and 21 identical rates. The decoder we used at
the receiver is the soft input Viterbi decoder with 8 iterations.
Similarly, we assume an ideal source compressor to be concatenated in front of
HARQ. Note that the ideal rates are not practically achievable for either BICM or
HARQ, because the ideal source compressor does not exist for short block length on

334
15 Joint Source and Channel Coding
the order of one thousand bits. However, we consider these two reference schemes
as the upper bounds in the achievable rates by the modulation schemes for these two
categories of rate adaptation.
JSCC: We aslo implement a practical joint source-channel coding scheme as de-
scribed by Zhu and Alajaji [364]. It is essentially an HARQ scheme, so the same
punctured Turbo code as in HARQ is used. The difference is that, instead of using a
separate source encoder, JSCC utilizes the prior information at the decoder. There-
fore, JSCC and HARQ become exactly the same when p = 0.5.
15.5.2 Simulations over an AWGN Channel
Simulations are carried out under static AWGN channels at integer channel SNRs (in
dB) from 5 dB to 25 dB, which covers the main SNR range of most 802.11 systems.
Figure 15.5 shows the results in terms of throughput versus the required channel SNR
for BER less than 10−6. For each scheme, we plot one curve for each sender setting
(in terms of modulation and coding combination for BICM, modulation scheme for
HARQ, and weight multiset for CM), and the curves beyond the effective range
may be omitted for clarity. The Shannon limits, calculated by dividing the Shannon
capacity by the source entropy, are also plotted.
5
10
15
20
25
0
2
4
6
8
10
12
14
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
CM
BICM
HARQ
Shannon
(a) p = 0.50
5
10
15
20
25
0
2
4
6
8
10
12
14
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
CM
BICM
HARQ
JSCC
Shannon
(b) p = 0.25
5
10
15
20
25
0
2
4
6
8
10
12
14
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
CM
BICM
HARQ
JSCC
Shannon
(c) p = 0.15
5
10
15
20
25
0
2
4
6
8
10
12
14
Es/N0 (dB)
Throughput (bit/s/Hz)
 
 
CM
BICM
HARQ
JSCC
Shannon
(d) p = 0.10
Figure 15.5 Comparing the throughput of CM and two reference schemes for different source
sparsities.

15.5 Performance Evaluation
335
Note that CM is not expected to achieve a higher throughput than the two refer-
ence schemes with ideal source coding. Through the simulations, we would like to
show that CM has a much wider adaptation range than the two reference schemes
while achieving a similar throughput as the ideal reference schemes. The perfor-
mance of any practical implementations of separate source coding combined with
BICM or HARQ would not achieve the throughput as the reference schemes, due to
the inevitable loss in source coding.
Figure 15.5a,b show that, when the source is not sparse or has moderate sparsity,
CM can use the same weight multiset for channels from 5 dB to 25 dB, with an
adaptation range spanning over 20 dB. In contrast, for the same SNR range, BICM
switches among 12 modulation and coding schemes, and HARQ switches among
three modulation schemes. Such a wide adaptation range of CM brings signiﬁcant
beneﬁts as the sender does not require channel state feedback, and avoids potential
performance loss due to mismatch between the estimated and the actual channel
states. The other reference scheme HARQ+JSCC has a similar adaptation range as
the proposed CM, but its achievable rate is much lower.
Figure 15.5c,d show the results when the source sparsity is more prominent. We
observe that the rate for CM saturates at lower SNR when p decreases. The saturation
point is around 19 dB when p = 0.15 and is around 16 dB when p = 0.1. This
suggests that, for very sparse data, CM may need to change the weight multiset
when the channel becomes good. Figure 15.5c,d These two ﬁgures show that using
two weight multisets is sufﬁcient for CM to cover the main channel SNR range when
p = 0.15 and p = 0.1.
15.5.3 Emulation in Real Channel Environment
We also implement the CM scheme on a software radio platform called SORA [332].
The evaluation scenarios include mobile Line-of-Sight (LOS), stationary Non-Line-
of-Sight (NLOS) and mobile NLOS. For each scenario, the channel state is traced
and a fair comparison between CM and two reference schemes HARQ and BICM are
carried out over the traced CSI. We do not include JSCC in the emulation because
the earlier simulations have shown that its achievable rate is much lower than that of
the other schemes.
In this evaluation, CM selects a ﬁxed weight multiset for the given source sparsity
and does not adapt it to the channel condition (even when p = 0.15 and p = 0.1).
Thus, CM completely avoid channel estimation and CSI feedback. The two reference
schemes, as they need to switch between several modulation and coding settings,
require channel feedback. We assume that the receiver could precisely measure the
actual SNR from the received packet and provide immediate feedback, and then the
sender uses the algorithm by Chen et al. [388] for rate adaptation.
Table 15.4 lists the average throughput of the three schemes under different source
and channel settings. To provide more details of how each scheme behaves, we show
the CDFs (cumulative distribution functions) of instant rates in Figures 15.6–15.8,
respectively, for the three evaluated scenarios. Each point (d,T) on the curve can be

336
15 Joint Source and Channel Coding
Table 15.4 Average rate in each scenario (bits/s/Hz).
Scenario
Scheme
p = 0.5
p = 0.25
p = 0.15
p = 0.1
LOS M
CM
4.74
5.83
7.23
8.50
HARQ
4.02
4.95
6.58
8.56
BICM
3.64
4.49
5.97
7.76
NLOS S
CM
3.74
4.79
6.18
7.42
HARQ
2.93
3.62
4.81
6.25
BICM
2.28
2.81
3.74
4.87
NLOS M
CM
2.83
3.73
5.30
6.61
HARQ
2.43
2.99
3.98
5.18
BICM
2.16
2.66
3.54
4.60
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(a) p = 0.50
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(b) p = 0.25
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(c) p = 0.15
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(d) p = 0.10
Figure 15.6 CDF of throughput in a mobile LOS case.
interpreted as “The rate in d portion of the time is below T.” Therefore, the rightmost
curve achieves the highest throughput.
Overall, CM performs the best and BICM the worst in the trace-driven emula-
tion, despite the simulation results that suggest that BICM could achieve the high-
est rate in static channel settings. This clearly demonstrates the advantage of CM
brought by using a ﬁxed modulation. In the two NLOS scenarios, the performance
gap between reference schemes and CM is even greater than that in LOS scenario,
because the channel varies more dramatically and the two reference schemes make
more improper decisions in rate selection. In NLOS static scenario, CM achieves
70% throughput gain over BICM when p = 0.25. In NLOS mobile scenario, CM
achieves 33% throughput gain over HARQ when p = 0.15. These results amply con-
ﬁrm that CM has successfully achieved the design goal to embed data compression
into seamless rate adaptation.

15.6 Summary
337
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(a) p = 0.50
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(b) p = 0.25
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(c) p = 0.15
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(d) p = 0.10
Figure 15.7 CDF of throughput in stationary NLOS case.
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(a) p = 0.50
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(b) p = 0.25
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(c) p = 0.15
0
2
4
6
8
10
0
0.2
0.4
0.6
0.8
1
CDF
Throughput (bits/s/Hz)
 
 
CM
HARQ
BICM
(d) p = 0.10
Figure 15.8 CDF of throughput in a mobile NLOS case.
15.6 Summary
We have described in this chapter a novel compressive coded modulation scheme ca-
pable of achieving simultaneously both source compression and seamless rate adap-
tation. In this novel scheme, data compression is elegantly embedded into coded

338
15 Joint Source and Channel Coding
modulation through a virtually rateless multilevel code named RP code. Through
careful design, this RP encoder in turn generates multilevel symbols, which are se-
quentially and evenly mapped to a dense and universal constellation. Such innovative
mapping maintains the statistics of channel noise to be used effectively during the
process of RP-BP decoding.
We have also developed practical implementation of the proposed CM scheme.
EXIT chart analysis is used for the weight selection of RP codes. Extensive simula-
tions and emulations have been carried out to evaluate the performance of CM. The
comparisons against conventional schemes show that CM has a much wider adapta-
tion range than existing approaches, and consequently achieves a higher throughput
under varying channel conditions. These promising results validate the practical fea-
sibility of CM.

Part VI
Pseudo-Analog Transmission
In the current digital communications systems, image and video are compressed
fully and thus the redundancy is completely removed in the compressed data. How-
ever, in the analog communications systems, image and video are not compressed
at all. In this part, we are looking for new communications systems between digital
and analog systems. A typical feature in such systems is that entropy coding is re-
moved in source compression. Furthermore, when source is no longer entropy coded,
conventional bit-based channel coding is unsuitable to source protection. Another
typical feature in such systems is that channel coding is removed in data protection.
Although the transmission is still implemented in a digital manner, the transmission
behavior looks like that of analog systems. Therefore, we call this research pseudo-
analog transmission.
Chapter 16 ﬁrst presents a novel framework called DCast for distributed video
coding and transmission over wireless networks, which is different from existing
distributed schemes in three aspects. First, coset quantized DCT coefﬁcients and
motion data are directly delivered to the channel coding layer without syndrome or
entropy coding. Second, transmission power is directly allocated to coset data and
motion data according to their distributions and magnitudes without forward error
correction (FEC). Third, these data are transformed by Hadamard and then directly
mapped using a dense constellation (64K-QAM) for transmission without Gray cod-
ing. One of the most important properties in this framework is that the coding and
transmission rate is ﬁxed and distortion is minimized by allocating the transmission
power. Thus we further propose a power distortion optimization algorithm to replace
the traditional rate distortion optimization. This framework avoids the annoying cliff
effect caused by the mismatch between transmission rate and channel condition. In
multicast, each user can get almost the best quality matching its channel condition.
Our experiment results show that the proposed DCast outperforms the typical solu-
tion using H.264 over 802.11 up to 8 dB in video peak signal-to-noise ratio (PSNR)
in video broadcast. Even in video unicast, the proposed DCast is still comparable to
the typical solution.
Chapter 17 challenges the conventional wisdom that video redundancy should
be removed as much as possible for efﬁcient communications. We discover that, by
keeping spatial redundancy at the sender and properly utilizing it at the receiver,
we can build a more robust and even more efﬁcient wireless video communications
system than existing ones. In the proposed framework, inter-frame (temporal) re-
dundancy in video is removed at the encoder, but intra-frame (spatial) redundancy
is retained. In doing so, pixel values after a transform-domain scaling are directly

transmitted with amplitude modulation. At the receiver, spatial redundancy is utilized
by image denoising. Note that denoising in our decoder is not a post-processing,
but has to be immediately performed on channel output. We implement the video
communications system, which we call Cactus on a SORA platform. The adopted
image denoising algorithm is made real-time through graphic processing unit (GPU)
implementation. Cactus is extensively evaluated in 802.11 a/g Wireless Local Area
Network (WLAN) environment. On average, Cactus outperforms SoftCast by 4.7 dB
in video PSNR and is robust to packet losses. In addition, Cactus is shown to be ca-
pable of transmitting high-deﬁnition videos in WLAN, and the performance is even
better than an omniscient MPEG scheme.
Chapter 18 considers image delivery in multiple-input multiple-output (MIMO)
broadcasting networks with diverse channel quality and varying numbers of anten-
nas across receivers. In such systems, performance is normally constrained by the
weakest users with either a low channel signal-to-noise ratio (SNR) or only a single
receiver antenna. To address both dimensions of heterogeneity, we present a new ana-
log image delivery system that adapts seamlessly along both dimensions simultane-
ously. The sender scales the DWT coefﬁcients according to a power allocation strat-
egy, and generates linear combinations of the coefﬁcients using compressive sensing
(CS), before transmitting them with amplitude modulation. On the receiving side,
the received physical layer symbols are passed directly to the source decoder without
conventional MIMO decoding, and the DWT coefﬁcients are recovered using a CS
decoder. There are two main contributions of our system. First, integrating CS into
MIMO transmission ensures that the reconstructed image quality at the receivers is
commensurate with both the channel SNR and the MIMO channel dimension. Sec-
ond, a power allocation strategy is introduced to achieve a performance trade-off
between receivers with different antenna numbers. Experimental results show that
the presented system outperforms both the analog reference SoftCast and the conven-
tional digital system known as HM-STBC. The average gain is 2.92 dB over SoftCast
for single-antenna users and 1.53 dB over HM-STBC for two-antenna users.

Chapter 16
DCast: Distributed Video Multicast
16.1 Introduction
distributed video coding (DVC) [389–392] is an attractive scheme for video com-
pression that has emerged over the past decade. Different from conventional video
coding schemes, it utilizes cross-frame correlation only at decoder. This brings sev-
eral unique advantages. First, DVC can shift intensive computation from encoder to
decoder, which is appealing for low-complexity video encoding applications. Sec-
ond, DVC framework is robust to transmission errors, which is desirable for wireless
applications. Although it has been proven that the theoretical coding performance
should be equivalent no matter what the source correlation is utilized at encoder or
decoder for some typical sources [306,393], the actual coding performance of DVC
is still far inferior to that of the conventional H.264 standard [13].
In DVC, quantized transform coefﬁcients are converted to bit planes and com-
pressed to bits by syndrome or entropy coding [390,392,394]. The syndrome coding
is implemented via channel codes (e.g., low-density parity-check codes). These chan-
nel codes are also typically applied for error protection in the physical (PHY) layer.
Therefore, Xu et al. propose the ﬁrst work on designing the joint source-channel
coding for distributed video transmission [359]. Except for this, the transmission of
distributed coded video still looks like that of conventional coded video. All data
from the PHY layer have been corrected by channel coding and thus are error-free.
But it does not fully take the advantage that distributed coded video is robust to
transmission errors.
Recently, a joint video coding and transmission scheme, named SoftCast [395,
396], was proposed for wireless video multicasting. The key idea in SoftCast is that
transform coefﬁcients are not compressed by entropy coding. Instead, they are di-
rectly transmitted through a dense constellation after allocating a certain power, such
that the received data can be decoded at any channel conditions. The decoded data
is not error-free and its signal-to-noise ratio (SNR) is dependent on channel condi-
tion for a given transmission power. Although the video coding of SoftCast is simply
341

342
16 DCast: Distributed Video Multicast
through 2D or 3D transformation, the overall performance of SoftCast still outper-
forms the typical solution using H.264 over 802.11 in video multicast.
The current SoftCast only adopts 3D DCT to exploit cross-frame correlation.
Many researches in scalable video coding have fully demonstrated that this is inefﬁ-
cient due to lack of motion alignment among frames [62,79,397]. However, motion
compensation (MC) in H.264 is difﬁcult to adopt in SoftCast because in SoftCast the
reconstructed frames are determined by channel noise and the encoder cannot ob-
tain the same reconstructed frames as the decoder. Thus this chapter presents a novel
framework called DCast, which can not only utilize the cross-frame correlation by
motion alignment but also keep the nice properties provided by SoftCast.
In the proposed DCast, transformed coefﬁcients are ﬁrst coset quantized and then
are transmitted as SoftCast. Similar to other DVC frameworks, DCast utilizes the
cross-frame correlation at the decoder. The proposed DCast has two different ap-
proaches to process motion vectors (MVs). Like most traditional DVC schemes, in
the ﬁrst approach motion vectors are estimated at the decoder. It does not need ref-
erence frames at the encoder and greatly reduces encoding complexity. But the side
information may be inaccurate, thus leading to low coding efﬁciency. Several DVC
schemes also propose to estimate motion vectors at the encoder and transmit them
to the decoder for improving the quality of side information [398, 399]. In the sec-
ond approach, motion vectors are estimated at the encoder and then transmitted to
the decoder. The initial results of these two approaches have been reported in our
conference papers [400, 401]. In this chapter we will focus our study on the second
approach but both of them will be evaluated.
The key technical contribution is the proposed power distortion optimization. In
the proposed DCast, each pair of quantized discrete cosine transform (DCT) co-
efﬁcients or transformed motion vector is transmitted by a time slot and thus the
transmission rate is ﬁxed. The distortion is minimized by optimally allocating trans-
mission power. This chapter evaluates the impact of channel noise on the distortion
of motion vectors and then the impact of this distortion on the distortion of recon-
structed video via the power spectrum approach [72]. Furthermore, a joint power
optimization among coefﬁcients and motion data is derived. Our experimental re-
sults show that the proposed DCast can outperform SoftCast up to 2 dB in video
PSNR because it can better utilize the cross-frame correlation. Compared with the
typical solution using H.264 over 802.11, the proposed DCast can gain up to 8 dB in
video PSNR in multicast. Even in unicast, it is still comparable to the typical solution
of H.264 over 802.11.
16.2 Related Works
16.2.1 Distributed Video Coding
To compress a source with its prediction only available at the decoder is a typi-
cal problem in distributed source coding (DSC). As shown in Figure 16.1, X is the

16.2 Related Works
343
Encoder
Decoder
S
X
Xˆ
Figure 16.1 Compression of X when its side information S is available at the decoder.
source to be compressed (possibly representing the source video), and S is its side
information (possibly representing the predicted frame). The theoretical foundations
of DSC, the Slepian-Wolf theorem [306] and the Wyner-Ziv theorem [393], show
that the source X can be efﬁciently compressed with its predictor S only available
at the decoder. In practice, efﬁcient DSC can be achieved by coset coding, Turbo
coding, and LDPC coding [356,402].
Accompanied by the advances of practical DSC solutions, DVC has emerged for
a decade. Puri et al. propose a DVC framework called PRISM, which implements
DVC by coset coding and supports motion estimation (ME) at the decoder [391,392].
The main attributes of PRISM include increased robustness to channel losses and
more ﬂexible sharing of computational complexity between encoder and decoder.
Another DVC work is the low-complexity framework proposed by Aaron et al. [389,
390]. In this framework, the DVC is implemented by Turbo code, while the motion
estimation at the decoder is based on motion compensated interpolation (MCI) and
motion compensated extrapolation (MCE).
Although DVC has shown unique advantages in visual communication, its com-
pression efﬁciency is still much lower than conventional framework such as H.264
[13]. In recent years, many researchers have focused on improving the perfor-
mance of DVC. Enabling transform coding [403, 404] and intra/inter-mode selec-
tion [405–407] allow DVC to exploit not only inter but also intra-frame redun-
dancy. Hash-based DVC lets the encoder send hash code to the decoder to improve
the accuracy of ME and the side information quality [408]. Successive reﬁnement
schemes [409–412] perform ME and DVC decoding alternatively and recursively,
such that the MVs and reconstruction frame are successively reﬁned during decod-
ing process. More accurate correlation estimation in DVC improves the utilization of
the side information [413–416].
Compared with these DVC schemes, the proposed DCast does not use syndrome
[390] or entropy coding, and directly delivers coefﬁcients and motion vectors to the
PHY layer. Furthermore, when coefﬁcients and motion vectors are transmitted from
encoder to decoder, they are allowed to be corrupted by channel noise. It is clear
from our results that DVC is robust to noise added in the received data.
16.2.2 Distributed Video Transmission
The transmission of distributed coded video is usually similar to the transmission
of conventional coded video in the PHY layer of wireless network. Coded binary
data is ﬁrst protected by channel coding and then is mapped to a constellation for

344
16 DCast: Distributed Video Multicast
transmission. When syndrome coding is adopted, DVC coding and channel coding
can be jointly optimized. The ﬁrst attempt to study DVC from a joint source-channel
coding (JSCC) perspective is by Xu et al. [359]. It is a layered coding scheme, where
the enhancement layer uses Raptor code for both video compression and data pro-
tection. In another framed-based JSCC scheme [398], the functionality of both DVC
and channel coding is implemented universally by one error correction code.
In these JSCC schemes, distributed video transmission is actually processed as
data transmission. The successfully decoded data in the PHY layer does not con-
tain any transmission errors. Thus many bits are paid on channel coding to correct
all transmission errors. Obviously, it does not fully take the advantage of distributed
video coding. In the proposed DCast, quantized coefﬁcients and transformed mo-
tion vectors are directly transmitted after allocating a certain power. Although the
received data after decoding may still contain a certain amount of channel noise, it is
more efﬁcient on power consumption because some received noise can be tolerated
by DVC.
16.2.3 SoftCast
SoftCast is a simple but comprehensive design for wireless video multicast, cov-
ering the functionality of video compression, data protection, and transmission in
one scheme [395]. The SoftCast encoder consists of the following components:
DCT transform, power allocation, Hadamard transform, and direct dense modu-
lation. Transform removes spatial redundancy of a video frame. Power allocation
minimizes the total distortion by optimally scaling transform coefﬁcients. Hadamard
transform is in some sense a precoding to make packets with equal power and equal
importance. After that, the data is directly mapped into wireless symbols by a very
dense quadrature amplitude modulation (QAM). The decoder uses the linear least
square estimator (LLSE) algorithm to reconstruct the signal. All the components in
SoftCast are linear operations and thus channel noise is directly transferred into re-
construction noise of the video. Therefore, SoftCast is asymptotically robust in the
sense that each user can get the visual quality matching its channel condition.
However, SoftCast exploits intra-frame correlation only and thus is not efﬁcient
in the aspect of video compression. Recently, Aditya et al. [354] propose another
video coding and transmission scheme, called Flexcast. It removes entropy coding
from conventional video coding and adopts rateless channel coding for channel vari-
ation. Thus it has better coding efﬁciency. However, Flexcast is a unicast approach
and can hardly multicast or broadcast video to the users with different SNRs simulta-
neously because of motion compensation. In a recent improved version of SoftCast,
the utilization of 3D DCT partially enables inter-frame compression [396]. However,
without motion alignment inter-frame correlation is still not fully exploited yet.
The proposed DCast not only fully utilizes the cross-frame correlation but also
keeps the good properties of SoftCast. DCast enables inter-frame coding by DVC
rather than conventional motion compensation. Instead of transmitting a video frame

16.3 Proposed DCast
345
itself like SoftCast, DCast transmits coset codes of the video frame such that the
frame can be reconstructed by utilizing the prediction frame as side information at
decoder. This saves the transmission power (or equivalently increases the SNR) be-
cause coset data has typically much smaller magnitude than original data. Recently,
we also noted that Kochman and Zamir [417] have studied the utilization of coset
coding in the Wyner-Ziv Dirty-Paper problem and proved its optimality and asymp-
totical robustness in multicast. It can be considered in general as the theoretical foun-
dation to support the proposed DCast.
16.3 Proposed DCast
DCast divides an input video sequence into groups of pictures (GOP). In each GOP,
the ﬁrst frame is intra coded, while the following frames are inter coded. The com-
pression and transmission of intra frame in DCast is the same as in SoftCast, which
consists of DCT, power allocation, and Hadamard transform. In the rest of this chap-
ter, we will focus on the compression and transmission of inter frames. For simplicity,
we mainly discuss the case with motion vectors estimated at encoder.
Figure 16.2 depicts the server side of DCast. DCast ﬁrst transforms the current
frame into DCT domain. Meanwhile, DCast performs ME and MC on the original
video sequence to get predictions and MVs. Then DCast applies coset coding on
the transform coefﬁcients of the original frame to get, for each DCT coefﬁcient, the
coset data. The quantization step size of the coset coding is determined at encoder
according to the estimated prediction noise of the decoder. The MVs of the current
frame, in the form of a matrix, are also transformed by DCT. The coset data and the
motion data are then scaled for power distortion optimization (PDO).
The scaling factors and other meta data are transmitted by using a conventional
scheme consisting of variable length coding (VLC), forward error correction (FEC),
and binary phase shift keying (BPSK) modulation. The scaled coefﬁcients are trans-
formed by Hadamard as precoding to make packets with equal power and equal
importance. After that, the resulting coefﬁcients are mapped to complex symbols
directly by a dense constellation (64K-QAM): each coefﬁcient is quantized into
Coset
Hadamard
Raw
OFDM
Channel
Original video
sequence
ME&MC
DCT
MVs
DCT
Predicted
frame
DCT
FEC
Meta
data
Scaled
coeﬃcients
X
M
BPSK
64K-
QAM
Power-Distor"on
Op"miza"on
(PDO)
VLC
C
Figure 16.2 DCast server for inter frames.

346
16 DCast: Distributed Video Multicast
Raw
OFDM
Channel
Hadamard-1
LMMSE
DCT-1
MC
MVs
DCT
Coset-1
DCT-1
Reconstructed
video
Coset
values
Predicted
frame
LMMSE
FEC-1
Meta data
Scaled
coeﬃcients
64K-QAM-1
BPSK-1
Side
informa"on
VLC-1
Figure 16.3 DCast receiver for inter frames.
8-bit integer number and every two integers compose one complex number of 64K
possible values. At last, these complex numbers are passed into a raw orthogonal
frequency division multiplexing (OFDM) module undergoing inverse fast Fourier
transform (iFFT) and D/A conversion for transmission.
The receiver side of DCast is depicted in Figure 16.3. The raw OFDM module per-
forms A/D conversion and FFT to reconstruct modulated data including both scaled
coefﬁcients and metadata. The metadata is demodulated and decoded ﬁrst. Then the
scaled coefﬁcients are reconstructed by inverse 64K-QAM and inverse Hadamard
transform. The inverse 64K-QAM here does nothing but splitting each complex value
back into two real values. Each real value here is actually the 8-bit integer number
plus channel noise.
After inverse Hadamard transform, the linear minimum mean square error
(LMMSE) estimation of residual coefﬁcients and MV coefﬁcients is performed.
Then the MVs are transformed back to a spatial domain by inverse DCT. After this,
the MC module generates the predicted frame by the MVs and the reference frame.
The predicted frame is transformed into a frequency domain by DCT. Then with the
coset residues and the predictors, the coset decoding module recovers the DCT co-
efﬁcients of the current frame. At last, the signals are transformed back to a spatial
domain, and are linearly combined with the predicted signals by LMMSE to generate
the ﬁnal reconstruction.
16.3.1 Coset Coding
Coset coding is a typical technique used in DSC. It partitions the set of possible input
source values into several cosets and transmits the coset index to the decoder. With
the coset index and the predictor, the decoder can recover the source value by choos-
ing the one in the coset closest to the predictor. Coset coding achieves compression
because the coset index has typically lower entropy than the source value.
Let X be DCT coefﬁcients of the original video frame. DCast encodes X to get
coset values C. DCast divides the coefﬁcients into 64 subbands according to the
frequency. Let Xi be the ith subband of X, and Ci be the ith subband of C. For each

16.3 Proposed DCast
347
i, DCast quantizes the ith subband of X by a uniform scaler quantizer Qi(·) and gets
the residual value [417] by
Ci = Xi −Qi(Xi) = Xi −⌊Xi
qi
+ 1
2⌋qi.
(16.1)
This coset coding is actually throwing away the main part of X. In some sense C
represents the detail of X.
At the client side, with the side information S (i.e., the predicted DCT coefﬁcients)
and the received coset value ˆC, the receiver reconstructs the DCT coefﬁcients by
coset decoding. Let Si be the ith subband of S, and ˆCi be the ith subband of ˆC. Since
Si is close to Xi, Si −ˆCi is around Xi −Ci. Thus Si −ˆCi is around Qi(Xi) from Eq.
(16.1). The quantizers are carefully designed such that applying quantization Qi(·)
on Si −ˆCi we could get Qi(Xi), that is,
Qi(Xi) = Qi(Si −ˆCi),
(16.2)
in high probability. Therefore, each subband of coefﬁcients is decoded by
ˆXi = Qi(Si −ˆCi)+ ˆCi,
(16.3)
where ˆX is the reconstruction of X, and each ˆXi is the ith subband of ˆX. When the
coset decoding is successful, that is, Qi(Xi) = Qi(Si −ˆCi), the reconstruction noise is
ˆXi −Xi = ˆCi −Ci.
(16.4)
16.3.2 Coset Quantization
The value of each coset step qi is crucial to the coding performance of DCast. If qi is
too small, the coset decoding may suffer failure. On the other side, if qi is too large,
the coset value Ci in Eq. (16.1) will be large and will consume a lot of transmission
power to keep the distortion small. The value of each qi is determined as follows.
Injecting Eq. (16.1) into Eq. (16.2), we get
Qi(Xi) = Qi(Si −ˆCi +Ci −Xi +Qi(Xi))
= Qi(Xi)+Qi(Si −ˆCi +Ci −Xi).
(16.5)
To guarantee successful coset decoding, the last item should be 0. This means the
quantization step qi should satisfy
qi
2 ≥|Si −Xi +Ci −ˆCi|.
(16.6)

348
16 DCast: Distributed Video Multicast
In this equation, Si −Xi is the prediction noise at decoder and Ci −ˆCi is the recon-
struction noise of coset valueCi due to transmission. In this work, we assume they are
independent Gaussian sources. We let each qi be 2n times of the standard deviation
of Si −Xi +Ci −ˆCi, that is,
q2
i = 4n2σ2
Si−Xi+Ci−ˆCi,
(16.7)
and this guarantees the condition Eq. (16.6) is satisﬁed in probability
Pr = erf(n/
√
2).
(16.8)
Under the same assumption, the variance of Si −Xi +Ci −ˆCi is the summation of the
variance of Si −Xi and Ci −ˆCi, that is,
σ2
Si−Xi+Ci−ˆCi = σ2
Si−Xi +σ2
Ci−ˆCi,
(16.9)
and each qi can be calculated by
q2
i = 4n2(σ2
Si−Xi +σ2
Ci−ˆCi).
(16.10)
In our implementation, we let n = 3 such that the coset decoding is successful for
more than 99.7% coefﬁcients. In Eq. (16.10), σ2
Si−Xi is the variance of the hypothetic
residue between the source and the side information, and it is estimated by simulating
at encoder a receiver with target channel SNR. σ2
Ci−ˆCi is the distortion of coset value
Ci due to transmission. It is also the distortion of the source Xi according to Eq.
(16.5). σ2
Ci−ˆCi is related to both the residue σ2
Si−Xi and the channel SNR. The explicit
expression of σ2
Ci−ˆCi is given in Section 16.4.4.
16.3.3 Power Allocation
DCast transmits both coset values and motion information. Thus it has two levels
of power allocation. The ﬁrst allocation is between MV data and coset data. The
second level means the allocation within MV coefﬁcients or coset coefﬁcients. The
optimal power allocation between MV data and coset data is given in Section 16.4.
The optimal power allocation within coset coefﬁcients and MV coefﬁcients are as
follows.
Let Pcoset be the total power for coset data, and gCi be the gain (scaling factor)
of Ci. The problem is how to minimize the reconstruction distortion of X, by op-
timally allocating power among Ci. Under the assumption that the coset decoding
is successful in high probability, the reconstruction distortion of X will be equal to
the reconstruction distortion of C according to Eq. (16.10). This means the problem
becomes how to minimize the reconstruction distortion of C, by optimally allocating
power among Ci. Thus, the solution has similar form as the one in SoftCast [396],

16.3 Proposed DCast
349
that is,
˜Ci = gCiCi,
gCi =
 
Pcoset
σCi ∑j σCj
!1/2
,
(16.11)
where ˜C is the coset value after power allocation, ˜Ci is the ith subband of ˜C, and
σCi is the standard deviation of Ci. This power allocation tends to scale down large
coefﬁcients to get better performance under the constrained total power. The encoder
calculates the variance σ2
Ci for each subband and transmits it to the decoder. With σ2
Ci,
both encoder and decoder calculate the gain gCi for each Ci by Eq. (16.11).
For MV data, DCast also performs power allocation. To apply power allocation,
the encoder performs 2D DCT on MVs (the whole MV ﬁeld) and gets transform
coefﬁcients M. Note that each MV contains horizontal and vertical components and
the transform is actually applied to both components separately. Each coefﬁcient
Mi is then considered as a subband. The encoder applies a similar optimal power
allocation over M, that is,
˜Mi = gMiMi,
gMi =
 
Pmv
σMi ∑j σMj
!1/2
,
(16.12)
where ˜M is MV data after power allocation, ˜Mi is the ith subband of ˜M, σMi is
the standard deviation of Mi, and Pmv is the total power for motion data. Since
each subband of M only contains one coefﬁcient, it is not efﬁcient to transmit the
variance of each subband. In light of this, DCast only transmits the average vari-
ance σ2
M = 1
n ∑i σ2
Mi where n is the number of subbands. As shown in our previous
work [401], the σ2
Mi and gMi are calculated by using σ2
M. Under the assumption that
the motion ﬁeld is random Markov ﬁeld, where the correlation coefﬁcient between
two neighboring MVs is ρ, each σ2
Mi can be calculated by
σ2
Mi = σ2
MVMi,
(16.13)
where VMi is the ith element of matrix VM, and
VM = diag(2D DCT(R(h)))diag(2D DCT(R(w)))T
(16.14)
is a constant matrix for given ρ. Here, function diag(·) produces the diagonal ele-
ments of the input matrix in the form of a column vector. 2D DCT(·) means 2D DCT
transform. w and h are the width and height of the motion ﬁeld, respectively, and
R(k) =


1
ρ
··· ρk−1
ρ
1
··· ρk−2
...
...
...
...
ρk−1 ρk−2 ···
1

.
(16.15)

350
16 DCast: Distributed Video Multicast
The value of σ2
M is calculated at the encoder and is transmitted to decoder as men-
tioned in the previous section. Both the encoder and the decoder calculate the value
of each σ2
Mi by Eqs. (16.13 to 16.15). In our experiments, we let ρ = 0.7 according
to statistics over several different video sequences. With each σ2
Mi, the optimal power
allocation gain gMi for each subband is calculated at both encoder and decoder by Eq.
(16.12). The decoder needs the value of gMi in Eq. (16.12) to reconstruct the signal.
16.3.4 Packaging and Transmission
Similar to SoftCast [396], DCast transmits not only a small amount of binary sym-
bols but also mainly real value symbols. The organization of the symbol stream is as
follows. The symbol stream consists of a header and a following data stream
symbol stream = {header bitstream,data stream}.
(16.16)
The header bitstream contains coset variances σ2
Ci, quantization steps qi, average MV
variance σ2
M and other useful parameters,
header bitstream ←{coset variances,
quantizationsteps,
averageMV variance,
parameters}.
(16.17)
The header information is coded in a conventional way. The encoder applies 8-bits
scalar quantization on σCi, qi, and σM, respectively. Then the quantization results are
compressed by variable length coding (VLC). The VLC is the universal one used
for coding motion vectors in H.264 [13]. The compressed header bitstream is trans-
mitted by the standard 802.11 PHY layer at the lowest speed, that is, by using 1/2
convolutional code and BPSK modulation. This is to make sure that the header bits
are decoded correctly when channel SNR is in typical working range (5–25 dB) of
802.11. Note that the size of the header is very small with respect to the whole data of
one frame. According to our experiments, the proportion of the bandwidth required
by the header is less than 3%.
The data stream contains coset data ˜C and MV data ˜M. Similar to SoftCast [396],
DCast applies Hadamard transform on the coset data ˜C and the MV data ˜M to create
packets with equal energy. Coset data and MV data are mixed together and then every
64 numbers are grouped for Hadamard transform. This forms the data stream
data stream H←{coset data, MV data}.
(16.18)
Note that the data stream consists of real values rather than binary values. In the
PHY layer, these real values are mapped to complex symbols directly by 64K-QAM

16.3 Proposed DCast
351
constellation [396]. This constellation is a typical N-QAM constellation with N equal
to 65,536 (256 by 256). Each input real value is quantized into an 8-bit integer num-
ber by uniform scalar quantizer. The dynamic range of the quantizer is formed by
the minimal and maximal input values. It is calculated for each frame at encoder and
sent to decoder as a parameter in Eq. (16.17). After this quantization, every two in-
tegers compose one complex number as the output of the 64K-QAM constellation.
An inverse FFT is computed on each packet of symbols, giving a set of complex
time-domain samples. These samples are then quadrature-mixed to passband in the
standard way. The real and imaginary components are ﬁrst converted to the analog
domain using D/A converters; the analog signals are then used to modulate cosine
and sine waves at the carrier frequency, respectively. These signals are then summed
to generate the transmission signal.
In DCast, both MV data and coset data are transmitted by the aforementioned
direct source channel mapping. This makes the system adaptive to the ﬂuctuation of
the channel SNR. Given the same transmitter, high SNR users would receive accurate
MVs and coset values, and reconstruct high quality video. Meanwhile, low SNR
users would receive noisy MVs and coset values, and derive noisy prediction frames
based on the noisy MVs. However, the coset decoding in DCast has good tolerance to
the noise of the prediction. Thus the low SNR users would still reconstruct the video.
16.3.5 LMMSE Decoding
The proposed approach contains two linear minimal mean square error (LMMSE)
estimators, operating in transform domain and spatial domain, respectively.
The ﬁrst LMMSE estimator is to reconstruct coset data C and MV data M in
transform domain with minimum distortion. Let Y be the received signal after inverse
Hadamard transform. Y contains the noisy version of coset data and MV data. Y can
be written as:
Y =
 ˙C
˙M

,
(16.19)
where ˙C is the noisy version of coset data, ˙M is the noisy version of MV data. Let
W (C) and W (M) be the channel noise in ˙C and ˙M, respectively. Let ˙Ci, ˙Mi, W (C)
i
,
and W (M)
i
be the ith subband of ˙C, ˙M, W (C), and W (M), respectively. We model each
element in W (C) and W (M) as identically distributed (i.i.d.) Gaussian source with
variance N0. Each subband of ˙C and ˙M can be expressed as
˙Ci = gCiCi +W (C)
i
,
˙Mi = gMiMi +W (M)
i
.
(16.20)
Therefore, the LMMSE reconstruction of original signals is
ˆCi =
σ2
Ci
σ2
Cig2
Ci +N0
˙Ci,
ˆMi =
σ2
Mi
σ2
Mig2
Mi +N0
˙Mi.
(16.21)

352
16 DCast: Distributed Video Multicast
And the reconstruction distortion of each subband is
E{( ˆCi −Ci)2} =
σ2
CiN0
σ2
Cig2
Ci +N0
,
(16.22)
E{( ˆMi −Mi)2} =
σ2
MiN0
σ2
Mig2
Mi +N0
.
(16.23)
The purpose of the second LMMSE estimator is to reconstruct each pixel x in spa-
tial domain with minimum distortion. DCast decoder applies inverse DCT transform
on coset reconstruction ˆX and gets a pixel-domain preliminary reconstruction ˆx. ˆx is
considered as the ﬁrst noisy version of x. DCast also has the predicted pixel s as the
second noisy version of x. With ˆx and s, the optimal LMMSE estimation x∗is given
by:
x∗= θs+(1−θ)ˆx,
(16.24)
where
θ =
σ2
ˆx−x
σ2s−x +σ2
ˆx−x
.
(16.25)
σ2
ˆx−x is the variance of ˆx−x, and σ2
s−x is the variance of s−x. In DCast, the prediction
noise variance σ2
s−x is estimated at block level. Since ˆx is close to x, σ2
s−x is estimated
by calculating E{(s−ˆx)2}. The variance σ2
ˆx−x is calculated as follows. According to
Parseval’s theorem and Eq. (16.5), we have
σ2
ˆx−x = E{(ˆx−x)2} = E{( ˆX −X)2} = E{( ˆC −C)2},
(16.26)
where E{( ˆC −C)2} is directly calculated by summation on Eq. (16.22).
16.4 Power-Distortion Optimization
In DCast, both MVs and coset values require power to transmit. Thus it is necessary
to investigate the optimal power allocation between MVs and coset values. Let D be
the reconstruction distortion, and P be the transmission power. Let Pcoset and Pmv be
the transmission power for coset values and MVs, respectively. The optimal power
allocation is the one minimizing the reconstruction distortion D for a given power P,
that is, the optimization problem is
min
D,
(16.27)
s.t.
Pmv +Pcoset ≤P.

16.4 Power-Distortion Optimization
353
16.4.1 Relationship between Variables
The distortion D is directly related to both the decoder prediction noise variance
σ2
S−X, and the coset transmission power Pcoset. Intuitively, using larger transmission
power Pcoset decreases the variance of the coset error ˆC −C at decoder. This means
smaller D because the reconstruction error ˆX −X equals to the coset error ˆC −C
according to Eq. (16.5). Meanwhile, larger σ2
S−X means lower quality of side infor-
mation (SI), and lower quality SI leads to larger reconstruction distortion. Therefore,
the distortion D should be a decreasing function of the coset power Pcoset and an
increasing function of the prediction noise variance σ2
S−X.
Furthermore, the prediction noise variance σ2
S−X is related to the MV transmis-
sion power Pmv. We use two dimensional random vector ∆∼N (0,σ2
∆I2×2) to model
MV error, while σ2
∆= 1
2E{∆T∆} is the distortion of MV. Using larger transmission
power Pmv decreases the MV distortion σ2
∆and this means more accurate MVs. More
accurate MVs produces higher quality of SI S at decoder, and hence smaller predic-
tion noise variance σ2
S−X. Thus the prediction noise variance σ2
S−X should decrease
with the increase of the MV transmission power Pmv.
However, due to the power constraint, allocating more power to coset (i.e., larger
Pcoset) means less power to MV (i.e., smaller Pmv), and vice versa. This is why we
need power distortion optimization. In the following part of this section, before solv-
ing Eq. (16.19), we will derive the relationship between
• MV transmission power Pmv and MV distortion σ2
∆
• MV distortion σ2
∆and prediction noise variance σ2
S−X
• Distortion D, coset power Pcoset, and prediction noise variance σ2
S−X
16.4.2 MV Transmission Power and Distortion
This section focuses on the relationship between motion vector (MV) transmission
power Pmv and MV distortion σ2
∆. According to Parseval’s theorem, the MV distor-
tion σ2
∆in spatial domain equals to the MV distortion in DCT domain, that is,
σ2
∆= 1
nmv ∑
i
E{( ˆMi −Mi)2},
(16.28)
where nmv is the number of MV coefﬁcients. From Eq. (16.22), we get
σ2
∆= 1
nmv ∑
i
σ2
MiN0
σ2
Mig2
Mi +N0
≈1
nmv ∑
i
N0
g2
Mi
,
(16.29)

354
16 DCast: Distributed Video Multicast
where the approximation is accurate when Pmv ≫N0. Substituting Eq. (16.12) into
Eq. (16.29), we get
σ2
∆≈N0(∑i σMi)2
nmvPmv
.
(16.30)
Then using Eq. (16.13), we get
σ2
∆≈
N0σ2
M(∑iV
1
2
Mi)2
nmvPmv
.
(16.31)
By deﬁning
αmv = ( 1
nmv ∑
i
V
1
2
Mi)2,
(16.32)
we can rewrite Eq. (16.31) as
σ2
∆≈nmvN0σ2
Mαmv
Pmv
= αmvσ2
M
 Pmv
nmvN0
−1
.
(16.33)
In this equation, σ2
M is the variance of the MV signal to transmit,
Pmv
nmvN0 is the SNR
for MV signal. Thus αmv can be considered as the extra gain owning to the power
allocation in Eq. (16.12). From this equation, the MV distortion σ2
∆is proportional
to the inverse of the MV transmission power Pmv.
16.4.3 MV Distortion and Prediction Noise Variance
This section focuses on the relationship between MV distortion σ2
∆and prediction
noise variance σ2
S−X. Let ˙S be the original decoder prediction when the MVs are per-
fectly received. The practical decoder prediction noise S−X consists of two compo-
nents: the original prediction noise ˙S −X, and the additional prediction noise S −˙S
caused by erroneous MVs. In this chapter, we assume they are independent of each
other, and therefore
σ2
S−X = σ2
˙S−X +σ2
S−˙S.
(16.34)
Given that the ˙S is a phase-shift version of S, σ2
S−˙S can be analyzed by using power
density. Similar to the derivation by Secker and Taubman [72], we have
σ2
S−˙S =
1
4π2
Z π
−π
Z π
−π 2Φss(ω)(1−E{cos(ωT∆)})dω,
(16.35)

16.4 Power-Distortion Optimization
355
where Φss(·) is the power density function of side information, ω is two-dimensional
frequency (in radians), and ∆∼N (0,σ2
∆I2×2) is the MV error. For small σ2
∆, we
have
1−E{cos(ωT∆)}) ≈1
2E(ωT∆)2 = 1
2σ2
∆ωTω,
(16.36)
and thus
σ2
S−˙S ≈
1
4π2 σ2
∆
Z π
−π
Z π
−π Φss(ω)ωTωdω.
(16.37)
We deﬁne
γ =
1
4π2
Z π
−π
Z π
−π Φss(ω)ωTωdω,
(16.38)
and γ is a constant for a given video frame. Then we get
σ2
S−˙S ≈γσ2
∆.
(16.39)
Substituting Eq. (16.39) into Eq. (16.34), we get
σ2
S−X = σ2
˙S−X +γσ2
∆.
(16.40)
Therefore, the prediction noise variance σ2
S−X is linear to the MV distortion σ2
∆.
16.4.4 Distortion Formulation
The derivation of the distortion D is as follows. First, from Eq. (16.5) we have ˆX −
X = ˆC −C in high probability. Thus the distortion D approximately equals to the
distortion of the coset value, that is,
D = σ2
ˆX−X ≈σ2
ˆC−C.
(16.41)
Similar to Section 16.4.2, we can derive and express the coset distortion as
σ2
ˆC−C ≈αcosetσ2
C
 Pcoset
ncosetN0
−1
,
(16.42)
where αcoset is the coding gain of power allocation, σ2
C is the variance of C, and ncoset
is the number of coset subbands.
In general, our DCast transmits the coset values of the source X over Gaussian
channel, with the side information S at the receiver side. Therefore, for each sub-
band, it forms a typical Wyner-Ziv Dirty-Paper problem, in which transmitting the
coset values has been proven to be as efﬁcient as transmitting the residue S−X over

356
16 DCast: Distributed Video Multicast
the same channel (if assuming that S −X is available to the encoder) [417]. Actu-
ally, according to the theorem by Kochman and Zamir [417] (the existence of good
lattice), the coset value C of each subband has the same variance with the prediction
residue S−X of each subband, that is,
σ2
Ci = E{C2
i } = E{(Si −Xi)2}.
(16.43)
Thus, the coset value and the prediction residue have the same variance in frame
level, that is,
σ2
C = E{(S−X)2} = σ2
S−X.
(16.44)
Therefore, Eq. (16.41), Eq. (16.42), and Eq. (16.44) implies
D = σ2
ˆC−C ≈αcosetσ2
S−X
 Pcoset
ncosetN0
−1
.
(16.45)
This means D is proportional to the prediction noise variance σ2
S−X and the inverse
of coset power Pcoset.
16.4.5 Solution
Substituting Eq. (16.33) and Eq. (16.40) into Eq. (16.45), we get
D = (σ2
˙S−X +γαmvσ2
MnmvN0P−1
mv )αcosetncosetN0P−1
coset.
(16.46)
Then taking Eq. (16.46) into the problem Eq. (16.27), and solving the problem, we
get
Pmv = [(A2 +A)1/2 −A]P,
(16.47)
A = γαmvσ2
MnmvN0P−1
σ2
˙S−X
.
Although it seems A contains so many variables, there is actually a quite straight-
forward way to estimate A. In A, σ2
M is the variance of the MV signal to transmit,
P
nmvN0 is the SNR when all power is allocated to MV, and αmv is the coding gain of
the power allocation. This means that, if all power is allocated to MV, the MV distor-
tion σ2
∆will be αmvσ2
MnmvN0P−1 according to Eq. (16.33). Furthermore, Eq. (16.33)
together with Eq. (16.39) implies that γαmvσ2
MnmvN0P−1 is the variance of the ad-
ditional prediction noise caused by erroneous MVs when all transmission power is
allocated to MV. Therefore, the parameter A is estimated as follows. DCast sim-
ulates the transmission and decoding process to get a hypothetic side information
S∗, which is the side information when all transmission power is allocated to MV
data. DCast also calculates another hypothetical side information ˙S, which is the side

16.5 Experiments
357
information assuming the transmission of MVs are lossless. Since S∗−˙S is the addi-
tional prediction noise caused by erroneous MVs, we have
σ2
S∗−˙S = γαmvσ2
MnmvN0P−1.
(16.48)
With Eq. (16.48), the solution Eq. (16.47) is rewritten as
Pmv = [(A2 +A)1/2 −A]P,
(16.49)
A =
σ2
S∗−˙S
σ2
˙S−X
.
Therefore, for optimal power distortion optimization, the encoder ﬁrst estimates
σ2
S∗−˙S and σ2
˙S−X, and then calculates optimal MV transmission power Pmv by Eq.
(16.49).
16.5 Experiments
In our experiments, we evaluate the performance of the proposed DCast in video
streaming applications including both unicast and multicast. We compare DCast
with SoftCast [395, 396] and conventional frameworks. We have implemented two
versions of SoftCast based on 2D DCT and 3D DCT respectively, that is, Soft-
Cast2D [395] and SoftCast3D [396].
We have also implemented two conventional frameworks. One uses H.264 as
video encoder and the other uses a DVC codec named Witsenhausen-Wyner Video
Codec (WWVC) [399]. Both of the two frameworks use the standard 802.11 PHY
layer with FEC and QAM modulations. We use JM14.2 software as H.264 codec. For
error resilience, the intra MB refresh rate is set to be 10%. Each video slice is packed
into one RTP packet. We set the maximal slice size to be 1192 bytes such that the
length of RTP packet is no greater than 1200 bytes. The WWVC coded bitstream is
also packed into RTP packet of maximal length 1200 bytes. We append to each RTP
packet a 32-bit CRC, and then encode each packet separately. Similar to the experi-
ments by Jakubczak and Katabi [396], for error protection we apply on each packet
an outer Reed-Solomon code with the same parameters (188/204) used for digital
TV. Each packet is individually interleaved between the outer Reed-Solomon code
and the inner FEC in accordance with the same recommendation. For inner FEC,
we generate the 1/2 convolutional code with polynomials {133, 171} and puncture
it to get 2/3 and 3/4 convolutional codes. The FEC coded bits are mapped to the
complex symbols by BPSK, QPSK, 16-QAM, or 64-QAM. The complex symbols
are then transmitted over OFDM. We assume the channel noise is Gaussian and the
channel bandwidth is 1.15 MHz. The FEC decoding is by soft Viterbi algorithm. Af-
ter the FEC decoding and RS decoding, the decoder performs CRC check for each
RTP packet and forward those error-free packets to video decoders. The WWVC de-
coder performs Wyner-Ziv decoding and is able to reconstruct the video frames when

358
16 DCast: Distributed Video Multicast
Table 16.1 Summary of the four frameworks.
Frameworks
SoftCast2D
SoftCast3D
DCast
H.264/WWVC
GOP
IIII...
-
IPPP...
IPPP...
Reference frames
0
-
1
1
ME
N
N
Y
Y
ME block size
-
-
ﬁxed
variable
ME search range
-
-
32×32
32×32
MV precision
-
-
1/4
1/4
DCT
2D
3D
2D
2D
Coding delay
1 frame
4 frames
1 frame
1 frame
Modulation
OFDM
OFDM
OFDM
OFDM
Constellation
64K-QAM,
64K-QAM,
64K-QAM,
BPSK, QPSK,
BPSK
BPSK
BPSK
16- or 64-QAM
FEC rate
1/2 (BPSK only)
1/2 (BPSK only)
1/2 (BPSK only)
1/2, 2/3, 3/4
RS rate
-
-
-
188/204
the reference frames have some error. The H.264 decoder can also tolerate a small
percentage of RTP packet loss, by utilizing error concealment. In our test, we have
conﬁgured the H.264 decoder to use the most complex error concealment method in
JM14.2, the motion copy one, to get the best reconstruction quality.
The test video sequences are standard CIF sequences (352×288, 30Hz), includ-
ing “akiyo,” “bus,” “coastguard,” “crew,” “ﬂower,” “football,” “foreman,” “harbour,”
“husky,” “ice,” “news,” “soccer,” “stefan,” “tempete,” “tennis,” and “waterfall.” To
evaluate the average performance of each framework, we also create a monochrome
512-frame test video sequence, called “all seq,” by combining the ﬁrst 32 frames of
the above 16 test sequences.
For DCast, H.264, and WWVC, the GOP structure is “IPPP” and the GOP length
is 32. In the following tests, all the PSNR results are for all the frames including both
intra and inter frames. The number of reference frames for inter frame is 1. In DCast,
the intra-frame coding is exactly the same as SoftCast with 2D DCT and the inter-
frame coding is by the proposed scheme. The transmission power allocated to an intra
frame is set to be 4 times the power of an inter frame. According to our experiments,
this approximately makes intra and inter frames have similar video PSNR. The search
range of ME is 32×32 and the MV precision is 1/4 pixel. In ME, DCast uses only
8 × 8 block size, while H.264 and WWVC use all the 7 block size from 4 × 4 to
16 × 16. Table 16.1 gives a summary of the techniques and conﬁgurations of these
frameworks.
16.5.1 PDO Model Veriﬁcation
This test is to verify the models of power distortion optimization (PDO) in Section
16.4. We use “all seq” as test sequence. In the ﬁrst test, we ﬁx the coset transmission

16.5 Experiments
359
10
−3
10
−2
10
−1
10
0
10
−2
10
−1
10
0
10
1
Pmv
−1
MV distortion σ∆
2
(a)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
20
40
60
80
100
120
140
160
180
200
MV distortion σ∆
2
Prediction noise variance σS−X
2
(b)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
10
20
30
40
50
60
Pmv
−1
D
(c) p = 0.15
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
10
20
30
40
50
60
Pcoset
−1
D
(d) p = 0.10
Figure 16.4 Veriﬁcation of the models of power distortion optimization in Part IV. Pcoset and PMV
are the transmission power of coset data and MV data respectively. D is reconstruction distortion.
power Pcoset and let the MV transmission power PMV change. The channel noise
power N0 is set to 1. The results are given in Figure 16.4. Figure 16.4a shows the re-
lation between the MV transmission power PMV and the MV distortion σ2
∆. Accord-
ing to the result, the inverse of PMV is proportional to the MV distortion. This con-
ﬁrms Eq. (16.33). Figure 16.4b shows the linear relation between the MV distortion
σ2
∆and the prediction noise variance σ2
S−X. This veriﬁes the model of Eq. (16.40).
Figure 16.4c shows the relation between the MV transmission power PMV and the
reconstruction distortion D. They are approximately in linear relation as shown in
Eq. (16.46).
In the second test, we ﬁx the MV transmission power PMV and let the coset trans-
mission power Pcoset change. The channel noise power N0 is set to 1. The result is
given in Figure 16.4d. The reconstruction distortion D is proportional to the inverse
of the coset transmission power Pcoset. This veriﬁes the model in Eq. (16.45) and Eq.
(16.46).

360
16 DCast: Distributed Video Multicast
5
10
15
20
30
35
40
45
Channel SNR
Video PSNR
DCast
DCast with decoder ME
SoftCast (3D−DCT)
SoftCast (2D−DCT)
H.264+802.11 (no RS)
WWVC+802.11 (no RS)
Figure 16.5 Unicast performance comparison. Both the encoder and decoder are assumed to know
the channel SNR.
16.5.2 Unicast Performance
This test is to compare unicast performance among all the above frameworks. In
this test the input video is “all seq” and the channel SNR is 5 dB−20 dB. Both
the encoder and decoder are assumed to know the channel SNR. For each channel
SNR, the parameters of DCast are optimally tuned. The total transmission power is
optimally allocated to the coset data and motion data as explained in Section 16.4.
The conventional framework is assumed to be able to choose the best combinations
of the FEC and the QAM methods recommended by 802.11 according to the channel
SNR, to get the maximal bit rate for the source coding layer. The source coding
layer, that is, the H.264 codec, performs rate control to utilize the bit rate as much as
possible.
The experimental result is given in Figure 16.5. This ﬁgure compares the recon-
struction quality of all the ﬁve frameworks at the different channel SNRs. The re-
construction quality is measured by PSNR. DCast is uniformly 4 dB better in video
PSNR than SoftCast2D at all channel SNRs, mainly due to enabling inter-frame pre-
diction. DCast gains about 1.5 dB in video PSNR over SoftCast3D, which mainly
comes from motion alignment. Compared with an H.264-based framework, DCast is
about 0.8 dB worse in video PSNR at the low channel SNR but is about 2.9 dB bet-
ter in video PSNR at the high channel SNR. A WWVC-based framework performs
slightly lower than the H.264-based framework. In this test, we also implement an-
other version of DCast in which the ME is performed at the decoder by motion
compensated extrapolation [390]. Like most other DVC frameworks, the DCast with
ME at the decoder has a low encoding complexity but a high decoding complexity.
Compared with the conventional framework, the DCast with ME at the decoder is
about 1.6 dB worse in video PSNR at the low channel SNR but is 1.7 dB better in
video PSNR at the high channel SNR.

16.5 Experiments
361
Note that the result in Figure 16.5 does not mean that DCast can outperform
H.264 in compression efﬁciency. H.264 is a video coding standard, while DCast
is a wireless video transmission framework. H.264 has high compression efﬁciency
but the coded stream is not robust to error. This is why the H.264 coded stream
needs additional FEC bits to protect. DCast may not be as efﬁcient as H.264 in video
compression, but it is robust to channel noise. Thus, it can skip FEC and can use a
dense 64K-QAM modulation, and can achieve high system efﬁciency.
16.5.3 Evaluation of Each Module
DCast has several modules such as coset coding, motion estimation (ME), and
power distortion optimization (PDO). In the following test, we incrementally turn
off these modules in DCast to evaluate their contribution. In this test the input video
is “all seq” and the channel SNR is 5 dB to 15 dB. The test results are given in Figure
16.6. In this ﬁgure, “PDO off” means that there is no PDO. The encoder utilizes a
straightforward power allocation, where the total transmission power is equally allo-
cated between the motion data and coset data, that is, Pmv
Nmv = Pcoset
Ncoset . “ME off” means
that there is no ME and the decoder uses a previous reconstructed frame directly as
side information. Note that there are dependencies between the three modules (coset,
ME, and PDO). When the ME is disabled, the PDO must be off because there is no
MV to transmit. When coset coding is disabled, the ME should be disabled too be-
cause the decoder no longer needs side information. Furthermore, when all three
modules (coset, ME, and PDO) are off, the DCast becomes the same as SoftCast2D.
According to the results in Figure 16.6, the contributions of coset coding, ME, and
PDO are about 2.7 dB, 0.8 dB, and 0.5 dB in video PSNR, respectively.
5
6
7
8
9
10
11
12
13
14
15
30
32
34
36
38
40
42
Channel SNR
Video PSNR
DCAST
DCAST, PDO off
DCAST, PDO off, ME off
DCAST, all off (i.e., SoftCast2D)
Figure 16.6 Evaluation of each module. The contributions of coset coding, ME, and PDO are about
2.7 dB, 0.8 dB, and 0.5 dB in video PSNR, respectively.

362
16 DCast: Distributed Video Multicast
5
6
7
8
9
10
11
12
13
14
15
30
32
34
36
38
40
42
Channel SNR
Video PSNR
DCast (target channel SNR=5dB)
DCast (target channel SNR=10dB)
DCast (target channel SNR=15dB)
Figure 16.7 Robustness test. DCast is conﬁgured to be optimized for the target channel SNR of 5
dB, 10 dB, and 15 dB, respectively, and then tested under the different channel SNR.
16.5.4 Robustness Test
In practical wireless applications, the channel SNR may not be perfectly known to
the encoder. In this test, we will evaluate the performance of DCast in this situation.
The input video is “all seq” and the channel SNR is 5 dB to 15 dB. We let DCast
optimize for the target channel SNR of 5 dB, 10 dB, and 15 dB, respectively. The
video PSNR are compared in Figure16.7. According to the results, each of the three
encoders performs best when the practical channel SNR matches its optimization
target, but performs worse than the best one when the practical channel SNR does
not match the target. The one optimized for the 15 dB channel performs 1 dB lower
in video PSNR than the other two when the practical channel SNR is 5 dB, mainly
due to unsuccessful coset decoding.
We then compare DCast with the conventional frameworks based on H.264 and
WWVC. We still assume that only the decoder knows the channel SNR. DCast is
optimized for a target channel SNR of 5 dB in this test. For conventional framework,
we implement all the eight recommended combinations of channel coding and mod-
ulation of 802.11. We calculate the corresponding bit rates, respectively, according to
the bandwidth, and set the bit-rate constraint to the H.264 encoder and WWVC en-
coder for rate control. Both the video bit rates and the channel bit rates (the bit rates
after RS coding and FEC) under the eight transmission approaches are given in Table
16.2 (note that WWVC and H.264 have same bit-rate constraints). For DCast, there
is no bit rate but only a channel symbol rate. Note that all the frameworks consume
the same bandwidth and transmission power.
The video PSNR of each framework under the different channel SNR is given
in Figure 16.8. In Figure 16.8a, all eight conventional transmission approaches suf-
fer a serious cliff effect. For example, the approach “H.264,1/2FEC,16-QAM” per-
forms well when channel SNR is between 13 dB to 14 dB, but is not good when

16.5 Experiments
363
5
10
15
20
25
28
30
32
34
36
38
40
42
44
Channel SNR
Video PSNR
DCast
SoftCast (3D−DCT)
SoftCast (2D−DCT)
H.264,1/2FEC,BPSK
H.264,3/4FEC,BPSK
H.264,1/2FEC,QPSK
H.264,3/4FEC,QPSK
H.264,1/2FEC,16QAM
H.264,3/4FEC,16QAM
H.264,2/3FEC,64QAM
H.264,3/4FEC,64QAM
(a)
4
6
8
10
12
14
16
18
28
30
32
34
36
38
40
Channel SNR
Video PSNR
DCast
SoftCast (3D−DCT)
SoftCast (2D−DCT)
H.264,3/4FEC,BPSK
WWVC,3/4FEC,BPSK
H.264,1/2FEC,16QAM
WWVC,1/2FEC,16QAM
(b)
Figure 16.8 Robustness comparison between DCast and (a) H.264 and (b) another DVC frame-
work: WWVC. Channel SNR is unknown to all the encoders. The DCast encoder is optimized for
channel SNR of 5 dB.
channel SNR is out of this range. When the channel SNR becomes more than 14
dB, the reconstruction quality does not increase. When the channel SNR becomes 12
dB, the reconstruction quality drops quickly. When the channel SNR becomes even
lower, the video decoder cannot work since almost all received RTP packets have bit
errors. Note that the cliff effect can be partially mitigated in a layered approach [418],
combining the scalable video extension of H.264 and a hierarchical modulation PHY
layer. However, as shown by Jakubczak and Katabi [396], the layered approach needs
a higher channel SNR than the single layer approach to achieve the same PSNR. Fig-
ure 16.8b shows the performance of WWVC based framework. Although WWVC
can beneﬁt from Wyner-Ziv decoding and achieves some gain over H.264 in erro-
neous situations, it still suffers a serious cliff effect.
In contrast, the three all-in-one frameworks do not suffer the cliff effect. When
the channel SNR increases, the reconstruction PSNR increases accordingly, and vice
versa. DCast is still the best one among the three all-in-one frameworks. At low
channel SNR, DCast is 1.5 dB and 4 dB better in video PSNR than SoftCast3D
and SoftCast2D, respectively. However, when the channel SNR increases, the gain
of DCast decreases. When channel SNR is 25 dB, DCast performs similar to Soft-
Cast3D and gains only about 2.5 dB in video PSNR over SoftCast2D. Compared
with the unicast result in Figure 16.5, the performance of DCast becomes 1.5 dB
worse in video PSNR at high channel SNR. This is mainly due to the fact that the
optimization of DCast (including both the PDO and the coset quantization step) is
for 5 dB channel SNR in this test. Figure16.9 gives the performance comparison on
different video sequences.
16.5.5 Multicast Performance
We then let all the frameworks serve a group of three receivers with diverse channel
SNRs. The channel SNR for each receiver is 6 dB, 12 dB, and 18 dB, respectively.

364
16 DCast: Distributed Video Multicast
5
10
15
20
25
25
30
35
40
45
50
Channel SNR
Video PSNR
DCAST
SoftCast (3D−DCT)
SoftCast (2D−DCT)
(a) Foreman cif
5
10
15
20
25
25
30
35
40
45
50
Channel SNR
Video PSNR
DCAST
SoftCast (3D−DCT)
SoftCast (2D−DCT)
(b) News cif
5
10
15
20
25
25
30
35
40
45
50
Channel SNR
Video PSNR
DCAST
SoftCast (3D−DCT)
SoftCast (2D−DCT)
(c) Bus cif
5
10
15
20
25
20
25
30
35
40
45
50
Channel SNR
Video PSNR
DCAST
SoftCast (3D−DCT)
SoftCast (2D−DCT)
(d) Flower cif
Figure 16.9 Multicast performance on different video sequences.
The test result is shown in Figure 16.10. In conventional frameworks based on H.264
and WWVC, the server transmits the video stream by using 3/4 FEC and BPSK. It
cannot use a higher transmission rate because otherwise the 6 dB user will not be able
to decode the video. Due to this, although the other two receivers have better channel
conditions, they will also only receive a low speed 802.11 signal and reconstruct low
quality video. In SoftCast and DCast, the server can accommodate all the receivers
simultaneously. Using DCast, the 6 dB user can get a slightly lower reconstruction
quality than using H.264- or WWVC-based conventional frameworks. However, the
12 dB and 18 dB users get 4 dB and 8 dB better reconstruction quality, respectively,
by using DCast than conventional frameworks.
Figure 16.11 compares the multicast performance of four frameworks, with re-
spect to the range of receiver SNR. The range of receiver SNR is deﬁned as the
difference of the maximal and minimal channel SNR of the users in the group. The
average channel SNR of the users in the group is 14 dB. When the channel SNR
range is 0 dB, that is, the channel SNR of all the users are equally 14 dB, DCast, Soft-
Cast3D, and H.264 frameworks perform similarly. However, when the users’ channel
SNRs become diverse, the performance of the H.264 framework drops quickly.

16.5 Experiments
365
26
28
30
32
34
36
38
40
42
44
H.264
WWVC
Softcast2D
Softcast3D
DCast
Receiver 1 (6dB)
Receiver 2 (12dB)
Receiver 3 (18dB)
Figure 16.10 Multicast to three receivers.
0
2
4
6
8
10
12
14
16
18
32
33
34
35
36
37
38
39
40
Range of Receiver SNR
Average Video PSNR
DCast
SoftCast (3D−DCT)
SoftCast (2D−DCT)
H.264+802.11
Figure 16.11 Serving a group of receivers with diverse channel SNRs. The average channel SNR
of each group is 14 dB.
The visual quality comparison is shown in Figure 16.12. The channel SNR is set
as 5 dB. DCast has a clearly better visual quality than both SoftCast2D and Soft-
Cast3D. In all the tests including unicast and multicast, DCast performs better than
both SoftCast2D and SoftCast3D. Moreover, DCast does not introduce frame delays
as SoftCast3D and is applicable for real-time video multicast like SoftCast2D.
16.5.6 Complexity and Bit Rate
The proposed DCast allows ME to be performed at the encoder. Therefore the en-
coder would be in high complexity but the decoder would be in low complex-
ity. Table 16.2 shows the average encoding time and decoding time per frame in
millisecond. The test machine has a Pentium (R) Dual-Core CPU E5300 @ 2.60

366
16 DCast: Distributed Video Multicast
(a) Original
(b) SoftCast2D
(c) SoftCast3D
(d) DCast
Figure 16.12 Visual quality comparison with channel SNR as 5 dB.
GHz, 2G internal memory and Microsoft Windows XP Professional 5.1.2600, with
Service Pack 3. The input video is “all seq” with CIF size at 30 frames per second.
DCast has less encoding time than H.264 codec (JM14.2) probably because DCast
has no mode decision and no entropy coding. As to the decoding time, DCast is
comparable to the H.264 codec.
Table 16.2 also shows the video bit rate and channel bit rate of H.264 solutions.
For example, when the modulation is BPSK, the channel bit rate is equal to the

16.6 Summary
367
Table 16.2 Comparison of complexity and bit rate.
Schemes
Encode
Decode
Video Rate
Channel Rate
Symbol Rate
Time
Time
H.264+1/2FEC+BPSK
387 ms
7 ms
530 Kb/s
1.15 Mb/s
1.15 M/s
H.264+3/4FEC+BPSK
387 ms
8 ms
795 Kb/s
H.264+1/2FEC+QPSK
406 ms
9 ms
1060 Kb/s
2.3 Mb/s
H.264+3/4FEC+QPSK
389 ms
10 ms
1590 Kb/s
H.264+1/2FEC+16-QAM
381 ms
11 ms
2120 Kb/s
4.6 Mb/s
H.264+3/4FEC+16-QAM
385 ms
14 ms
3180 Kb/s
H.264+2/3FEC+64-QAM
371 ms
15 ms
4240 Kb/s
6.9 Mb/s
H.264+3/4FEC+64-QAM
427 ms
16 ms
4770 Kb/s
DCast
304 ms
10 ms
-
-
channel symbol rate, that is, 1.15 M/s. If the FEC is 1/2 convolutional code and the
RS code is 188/204, then the video bit rate is 1.15M × 1
2 × 188
204 = 530 Kb/s. When the
modulation is QPSK and the FEC is 3/4 convolutional code, then the channel bit rate
is 2.3Mb/s and the video bit rate is 1590 Kb/s. The decoding time of H.264 codec
depends on the video bit rate. Basically, the decoding time becomes longer when
the bit rate increases. The DCast framework has no bit rate but a universal channel
symbol rate. Its decoding time is ﬁxed and is similar to the decoding time of the
H.264 decoder at a bit rate of 1590 Kb/s.
16.6 Summary
In this chapter, we present a novel framework called DCast for distributed video
coding and transmission over wireless networks. DCast ﬁrst presents a new design
on how to efﬁciently transmit distributed coded video data over the Gaussian channel.
Furthermore, we also propose a new power distortion optimization for the proposed
DCast.
DCast avoids the annoying cliff effect of conventional frameworks caused by
the mismatch between the transmission rate and channel condition. A single DCast
server can accommodate multiple users with diverse channel SNRs simultaneously
in multicast without sacriﬁcing any user’s coding performance. As shown in the ex-
periments, DCast performs competitively with the H.264 framework in unicast but
gains up to 8 dB in video PSNR in multicast.
DCast, as a unique DVC framework, does not utilize some sophisticated video
coding tools such as variable block ME, intra mode, or mode decision. How to en-
able these tools to further improve the performance of DCast is one possible future
work. Furthermore, the DCast in this chapter is mainly designed and optimized for
the Gaussian channel. Another opportunity for future work is to extend the proposed
DCast to the fading channel, which may require more complicated channel estima-
tion and power distortion optimization.


Chapter 17
Denoising in Communications
17.1 Introduction
In 2011, mobile video trafﬁc exceeded 50% of mobile trafﬁc for the ﬁrst time, and
it is predicted to increase 25-fold in the next ﬁve years, according to Cisco Visual
Networking Index (VNI) [3]. Wireless video communications are facing a dilemma
in achieving efﬁciency and robustness. On the one hand, videos in their raw format
are huge in size, and they need to be efﬁciently compressed for transmission. On the
other hand, compressed video sequences have too little redundancy left, and therefore
are susceptible to channel errors.
Direct application of Shannon’s separation theorem [4] suggests that source re-
dundancy should be completely removed and channel coding is responsible for
adding redundancy against noise. However, joint source-channel coding (JSCC) sug-
gests to keep a certain amount of source redundancy and has been shown to achieve
better performance at limited complexity and delay. This inspires us to consider the
following questions: How much source redundancy should be retained in wireless
video communications in order to achieve both efﬁciency and robustness? Is it pos-
sible to skip channel coding and completely rely on source redundancy for channel
protection?
Interestingly, the answer to the second question is a resounding YES, and the an-
swer to the ﬁrst question becomes clear after we carefully examine the two types of
redundancy in video and their respective characteristics. Our research ﬁnds that: (1)
Inter-frame (temporal) redundancy should be removed as much as possible at the en-
coder for high efﬁciency, while intra-frame (spatial) redundancy should be retained
to protect videos against channel noise. (2) Residual frames should be transmitted
in spatial domain (e.g., scaled pixel values) instead of transform domain (i.e., coefﬁ-
cients) through analog transmission to combat losses and noises. (3) The key to fully
utilize the source redundancy is to perform image denoising at the decoder based on
both source and channel characteristics.
Based on these ﬁndings, we propose a hybrid digital-analog video communica-
tions system called Cactus. At the encoder, temporal redundancy is removed by
369

370
17 Denoising in Communications
motion compensated temporal ﬁltering [62]. The motion information is entropy
coded and protected by the strongest channel codes during transmission. Pixel val-
ues in residual frames are transmitted using amplitude modulation. In order to mini-
mize the mean squared error (MSE) under the average power constraint, a transform-
domain scaling is performed. However, we emphasize that the sender should transmit
scaled pixel values instead of transform-domain coefﬁcients. This allows the receiver
to fully utilize the source redundancy through applying image denoising techniques.
In particular, Cactus employs a median ﬁlter [419] to deal with packet losses and
block matching with 3D transform (BM3D) [420] to deal with additive noises.
We have implemented Cactus on a SORA [332] platform and have evaluated it
in 802.11a/g-based wireless LAN environments. It conﬁrms that our design achieves
high received video quality and is robust to channel variations. In addition, Cac-
tus allows for graceful degradation in a wide range of receiver signal-to-noise ratios
(SNRs), and therefore can be readily used for multicasting. Trace-driven experiments
show that Cactus outperforms a recent analog mobile video system SoftCast by 4.7
dB in average video peak signal-to-noise ratio (PSNR). In addition, Cactus is shown
to be capable of transmitting high-deﬁnition videos (720 p) in WLAN, and the per-
formance is even better than an omniscient Moving Picture Experts Group (MPEG)
scheme.
17.2 Background
Visual content, including both still images and motion pictures (videos), contain a
huge amount of redundancy. In the spatial domain, near pixels are likely to have
similar values, and every small patch is likely to have many similar patches in the
same image. In the temporal domain, successive frames usually have small differ-
ences, especially when the frame rate is high. In image/video processing, both types
of redundancy are well understood, precisely modeled, and fully exploited in vari-
ous applications. This section will introduce visual redundancy through the review
of two related topics, namely image denoising and video coding. We do not intend
to provide a comprehensive review of the two topics here. There is more information
for interested readers [13,14,421,422].
17.2.1 Image Denoising
The basic principle behind any image denoising technique is that natural images
contain certain structural redundancy while noise does not. The structural redun-
dancy could be local or nonlocal. The assumption for local structural redundancy is
that nearby pixels (belonging to the same object) usually have similar gray level val-
ues. The assumption for nonlocal structure redundancy is that every small window

17.2 Background
371
in a natural image has many similar windows in the same image. Interestingly, this
regularity and periodicity assumption turns out to be very general and accurate.
The basic denoising techniques can be classiﬁed into spatial-domain smoothing
and transform-domain thresholding. The median ﬁlter [419] is a well-known spatial-
domain denoising technique which harnesses the local redundancy. It replaces the
noisy pixel with the median of neighboring pixel values. In the presence of impul-
sive noise (or salt-and-pepper noise), the median ﬁlter has been proved to be very
effective. Gaussian smoothing is another spatial-domain denoising technique. The
representative methods include SUSAN noise ﬁlter (SNF) [423] and the bilateral ﬁl-
ter [424]. Both of them use the Gaussian mean of the neighborhood as the denoised
value. The main drawback of local smoothing techniques is that they are not able
to preserve ﬁne structures, details, and texture [421]. Based on this observation, the
nonlocal means (NLM) [421] ﬁlter considers both local and nonlocal redundancies.
The denoised value of a pixel is a mean of the values of all pixels whose Gaussian
neighborhood looks like the neighborhood of this pixel.
Transform-domain thresholding techniques [425, 426] typically assume that the
signal has a sparse representation in a transform domain, such as the discrete cosine
transform (DCT), wavelet, or curvelet. Hence, one could preserve the few coefﬁ-
cients whose magnitudes are larger than a threshold and discard the rest, which is
more likely introduced by random noise. These techniques are basically based on
the local redundancy assumption, but have the ability to preserve certain ﬁne struc-
tures. Actually, the nonlocal redundancy can be incorporated into transform-domain
thresholding techniques by prediction, or block matching. The BM3D (block match-
ing with 3D transform) denoising algorithm [420] exploits nonlocal redundancy by
selecting sets of blocks similar to a given reference block and grouping them into 3D
data arrays. There is also a video version of the BM3D algorithm, which ﬁnds match-
ing blocks in both current and adjacent frames. By such, the temporal redundancy is
also exploited.
It should be noted that, although the topic of image denoising has been exten-
sively studied, and there exist many mature algorithms, they can hardly be applied
to digitally encoded images or videos. This is due to the mismatch between noise
assumptions in denoising algorithms and the error patterns in digital transmissions.
However, if the visual contents are transmitted with analog modulation and the noise
is additive, most of the existing image denoising techniques can directly apply.
17.2.2 Video Compression
Videos usually have a very large data volume in their raw formats. Video compres-
sion signiﬁcantly reduces the data size so that they can be practically stored and/or
transmitted. This is achieved by removing spatial and temporal redundancy. Modern
video compression standards all rely on two basic techniques: block-based motion
compensation (MC) and transform domain based compression [13,14].

372
17 Denoising in Communications
Figure 17.1 Demonstration of temporal redundancy by part of successive frames in standard video
test sequence mobile.
Figure 17.1 shows part of two successive frames in mobile sequence. Obviously,
there is signiﬁcant temporal redundancy. This redundancy is reduced by block-based
MC. The assumption behind it is that each patch of the current picture can be mod-
eled as a translation of the picture at some previous time. In particular, when encod-
ing frame i (current frame), it uses the reconstruction of frame i −1 as a reference.
For each block in the current frame (e.g., the block marked by a bold box), it searches
for similar blocks in the reference frame. Once the best match (the bold-boxed block
in frame i −1) is found under the SAD (sum of absolute differences) criterion, the
encoder will call it a prediction. The difference between the original block and the
prediction is called residual. Both residual and motion vectors (i.e., amplitude and
direction of the displacement between current block and its best match) need to be
encoded in the following steps.
Spatial redundancy is utilized by encoding both I-frames (i.e., frames coded with-
out reference to any other frame) and the residual of P-frames (i.e., frames coded
using MC prediction from one reference frame) and B-frames (i.e., frames coded
using bidirectional MC). Encoding of I-frames has an intra-prediction mode, which
reduces the local redundancy through prediction. Figure 17.2 shows part of an I-
Bi,j–1
Bi–1,j–1
Bi–1,j+1
Bi–1,j
Bi,j
(current)
Pixels used for
intra prediction
of block Bi,j
Reconstructed MB
Figure 17.2 Utilizing spatial redundancy by intra prediction.

17.3 System Design
373
frame. Now the encoder is processing block Bi,j, and all the gray blocks shown in
the ﬁgure have been encoded and reconstructed. The pixel in the bottom-right corner
of Bi−1,j−1, the pixels in the bottom row of Bi−1,j, and pixels in the right column
of Bi,j−1 are used for the prediction of Bi,j. In this example, the prediction direc-
tion is vertical. Then each column in Bi,j will be subtracted with the corresponding
predicted value, and only the residual needs to be encoded.
It is observed that both the original frames and residuals have spatial redundancy.
Based on the sparse representation assumption, the encoder could perform transform-
based compression. The discrete cosine transform (DCT) is known as the best esti-
mate of the optimal Karhunen-Lo`eve transform (KLT), and therefore is widely used
in video coding standards. Note that the transform only decorrelates the pixels, the
reduction of redundancy is actually performed by the following quantization and en-
tropy coding steps. The motion information generated from both intra prediction and
inter prediction is entropy coded too.
Note that the MC in present video coding standards [13] is based on a closed-
loop prediction, that is, the prediction is based on the reconstructed frame at the de-
coder, not the original frames. In the conventional digital transmission paradigm, the
transmission is assumed to be lossless if channel coding provides enough protection.
Thus, the encoder could implement a decoder, and create the reconstruction from
the bitstream it generated. However, in a hybrid digital-analog transmission scheme,
the encoder is not able to know the exact reconstructed frame at the receiver, not to
mention that in a multicast session different receivers will receive different recon-
structions. In this case, the closed-loop prediction will bring drifting errors.
In the video coding literature, there is an open-loop alternative for inter-frame
prediction, which is called motion-compensated temporal ﬁltering (MCTF) [62]. The
prediction in MCTF is based on original frames, so drifting errors are avoided. This
allows us to evaluate the design choice to remove temporal redundancy at the en-
coder. Unfortunately, intra-frame prediction does not have an open-loop alternative.
Direct application of the closed-loop prediction is not possible too. This is because
the long prediction path (from top-left corner to the right-bottom corner) will create
dramatic drifting errors if blocks are transmitted through analog modulation.
17.3 System Design
17.3.1 System Overview
We are seeking for a joint source channel coding design for wireless video commu-
nications. The intuition behind it is that source redundancy may be used for channel
protection, but the trade-off between robustness and efﬁciency needs to be balanced.
Figure 17.3 provides an overview of the designed hybrid digital-analog communica-
tions system named Cactus.

374
17 Denoising in Communications
Figure 17.3 Cactus overview.
At the sender, a video sequence is ﬁrst divided into group of pictures (GOP).
Commonly used GOP sizes vary from 4, 8, 16 to 32 depending on the application
requirements. We select GOP size equaling to 8 in our system. Each GOP is ﬁrst de-
correlated in the temporal axis via motion-compensated temporal ﬁltering (MCTF).
The motion information, including mode and motion vector, needs to be faithfully
received by every receiver, so they are entropy coded and transmitted using a robust
digital scheme. We adopt the combination of 1/2-rate channel coding and binary
phase shift keying (BPSK) modulation.
The temporally ﬁltered frames are then transformed into THE frequency domain
by DCT. According to the remaining channel bandwidth budget, a certain portion
of the coefﬁcients need to be discarded. This resource allocation is performed on
a GOP basis. The remaining coefﬁcients in each frame are then divided into 10 L-
shaped chunks, and are scaled accordingly. The scaling parameters are transmitted
through digital methods too.
Finally, inverse DCT is performed on each frame. This is a key step in our design
in order to fully utilize the spatial redundancy, because the loss of pixels is more
friendly to image denoising algorithms than the loss of frequency coefﬁcients. The
scaled pixel values are interleaved and transmitted with amplitude modulation. In
particular, every two pixel values are transmitted as the I and Q components of a
complex symbol. It should be noted that the amplitude modulation we used is actu-
ally pseudo-analog, because we use a discrete modulation constellation, except that it
is much denser than the commonly used 16-QAM or 64-QAM. This pseudo-analog
implementation allows our design to be easily integrated into an existing network
stack.
At the receiver, the digitally transmitted symbols are processed with a sequence of
inverse operations including demodulation, channel decoding, and entropy decoding.
Correct motion information and metadata can be obtained. Meanwhile, the receiver
directly reads the scaled pixel values from the I/Q components of wireless symbols,
and pieces together all the frames. Denoising is immediately applied on the scaled
frames. Then transform-domain descaling is performed for each individual frame.

17.3 System Design
375
Finally, frames from the same GOP are processed with inverse MCTF to output the
reconstructed video sequence.
17.3.2 Sender Design
17.3.2.1 Reduction of Temporal Redundancy
For natural video sequences, motion compensation (MC) is an essential step to re-
move temporal redundancy. However, we have discussed in Section 17.2.2 that the
closed-loop prediction used in the current video coding standards is not suitable for
analog transmission, due to the fact that the encoder is unable to obtain the exact
reconstruction at the receiver. SoftCast simply adopts 3D-DCT to get around this
problem, but transform without motion alignment cannot fully exploit the temporal
correlation.
In our system, we adopt an alternative approach called MCTF [62] to reduce tem-
poral redundancy. MCTF is essentially motion-aligned temporal transform. It is at-
tractive to our system because it is based on an open-loop prediction model, that
is, the prediction is based on original pixel values, not the reconstructed ones. It has
been shown that the drifting errors are much smaller than its closed-loop counterpart.
Figure 17.4 demonstrates the lifting structure of a 2-layer 5/3 temporal ﬁlter for
the ith GOP when the GOP size is 4. The even frames (frame 4i + 2 and 4i + 4) are
set as high-pass frames. For each block in a high-pass frame, two similar blocks are
identiﬁed in the previous and following frames. The average of these two blocks cre-
ates a prediction of the current block, so that the high-pass component is computed
by subtracting the prediction from the current block. After the ﬁrst-layer high-pass
frames are generated, the ﬁrst-layer low-pass frames can be computed by adding
one fourth of the high-pass components from the two adjacent frames to the cur-
rent frame. It can be seen that each high-pass frame is generated from 3 original
frames and each low-pass frame is generated from 5 original frames, so this process
is called 5/3 ﬁlter. Similar processing steps are applied to the two low-pass frames to
perform the second layer temporal ﬁltering. We implement the Barbell-lifting MCTF
proposed by Xiong et al. [397], and perform 3-layer ﬁltering for each 8-frame GOP.
17.3.2.2 Bandwidth Allocation and Reduction
We deﬁne bandwidth ratio, denoted by ρ, as the ratio of channel bandwidth to source
bandwidth. In our system, the digital transmission of motion information will occupy
a certain portion of bandwidth. The exact amount can be computed from the result
of entropy coding. When (BPSK, 1/2) is used, each entropy coded bit takes two
complex symbols to transmit. The remaining bandwidth, denoted by the ratio ρc, is
used to transmit pixels. When ρc < 1, not all pixel values can be transmitted, and

376
17 Denoising in Communications
Figure 17.4 Lifting structure of a 2-layer 5/3 temporal ﬁlter for GOP size 4.
the sender needs to decide how to reduce the bandwidth usage and how to allocate
bandwidth among frames.
It is well understood from digital image/video coding that the truncation of data
should be based on energy. Therefore, we perform DCT for each individual frame.
As the low-pass and high-pass frames in a GOP differ drastically in energy, the band-
width allocation should be per GOP basis. A straightforward solution, which di-
vides the transform coefﬁcients into equal-sized blocks and discards the least-energy
blocks, cannot be applied in our design. This is because we transmit scaled pixel
values instead of DCT coefﬁcients. Even though a right portion of DCT coefﬁcients
are discarded and padded with zeros, the number of pixels after inverse DCT does
not change.
We solve this problem by transmitting a down-sampled frame. It is based on an
interesting property of DCT. Let I be an image with resolution W ×H, and C be its
DCT coefﬁcients. If we truncate C into a W ′ ×H′ matrix C′ where C′(w,h) =C(w,h)
for all 1 ≤w ≤W ′ and 1 ≤h ≤H′, then the inverse DCT transform of C′ using a
W ′ × H′ transform matrix will create I′
W ′×H′, which is a down-sampled image of I.
Therefore, transmitting I′ instead of I achieves bandwidth reduction.

17.3 System Design
377
17.3.2.3 L-Shaped Chunk Division and Scaling
To optimally transmit the pixels under MSE criterion in a power-constrained system,
one should ﬁrst decorrelate the pixel values through transform, then each transform
coefﬁcient should be scaled by a factor which is inversely proportional to the fourth
root of its variance [427]. As it is not practical to scale each coefﬁcient individually,
Jakubczak and Katabi [395] propose to group nearby coefﬁcients into chunks and
model the values in each chunk as random variables (RVs) from the same distribu-
tion. Then the coefﬁcients in the same chunk will be scaled by the same factor. The
scaling factors (which is also called metadata) need to be reliably transmitted to the
receiver for decoding.
We propose a new adaptive L-shaped chunk division method. The motivations
are twofold. First, in the previous step of our system, bandwidth deduction will dis-
card L-shaped coefﬁcients from the peripheral of the frame. Second, we observe
that transform coefﬁcients decay rapidly from low-frequency to high-frequency, and
those belonging to a similar frequency band are more likely to have similar values.
The problem can be mathematically described as follows. Let P be the total power
budget. Divide the transform coefﬁcients into M chunks, and let λi and gi denote the
variance and scaling factor of the ith chunk. It is known that:
gi = λ
−1
4
i
s
P
∑i
√λi
(17.1)
An optimal chunk division should minimize ∑i
√λi. For L-shaped chunk division,
the adjustable parameters are rj (j = 1,2,...M −1), which are the positions of chunk
boundaries.
We adopt an iterative approach to search for the optimal set of {rj}. The initial
values of r′
js are evenly spaced. Then the algorithm iteratively updates the parameters
one by one. In updating rj, the values of rj−1 and r j+1 are ﬁxed. Figure 17.5 shows
Figure 17.5 L-shaped chunk division for the ﬁrst frame of Foreman.

378
17 Denoising in Communications
(a) Channel input
(b) Channel output
(c) After denoising by median ﬁlter
(d) After denoising by BM3D
Figure 17.6 Transmitting a low-pass frame of Foreman over a 5 dB AWGN channel and 1% loss
rate.
our chunk division for the ﬁrst frame of Foreman when M = 10. In this case, only 20
metadata (10 scaling factors and 10 chunk boundaries) need to be transmitted.
Actually, both bandwidth reduction and power scaling are performed on transform
domain. Therefore, the sender should perform an inverse discrete cosine transform
(IDCT) after these two steps. Transmitting scaled pixel values does not change the
overall power because IDCT is an orthonormal transform. Figure 17.6a shows the
channel input for the ﬁrst frame of Foreman. The original frame is 8-bit grayscale
(pixel values are from 0 to 255). After transform-domain scaling, the pixel values
range from −8.82 to 10.46 for this particular frame. We amplify each value by 10
times (then plus the shift 128) just for the viewing purpose.
17.3.3 Receiver Design
One key ﬁnding in our research is that source redundancy can provide channel pro-
tection under the premise that it is fully utilized at the receiver. We propose to use

17.4 Implementation
379
image denoising techniques at the receiver, and emphasize that denoising should be
immediately applied to channel output.
The denoising processes for low-pass and high-pass frames are identical. We use
different denoising techniques to deal with packet losses and random-valued noises.
In particular, we adopt the classic median ﬁlter [419] to handle losses. Under ideal
interleaving, packet loss creates randomly dispersed pixel “holes” in the frame. These
holes are ﬁlled with the median of surrounding eight pixel values. We have tried more
advanced median ﬁlters such as the directional weighted median ﬁlter [428], but the
performance improvement is marginal at moderate packet loss ratios.
Then BM3D [420] is adopted to reduce the random noise for two reasons. First,
BM3D is the state-of-the-art denoising algorithm. Second, there is a video version of
BM3D which utilizes temporal redundancy to denoise. This provides an alternative to
our MCTF design and could help us to evaluate whether and in which cases temporal
redundancy should be removed at the encoder.
The complete BM3D algorithm has two estimate steps: basic estimate and ﬁnal
estimate. Each estimate is again composed of two steps: block-wise estimate and ag-
gregation. In a block-wise estimate, each block ﬁnds similar blocks in a large neigh-
borhood and stacks them in a 3D array. Then, 3D transformation, hard thresholding
(Weiner ﬁltering in ﬁnal estimate), and inverse 3D transformation are consecutively
performed to generate estimates for all the involved pixels. After all the blocks are
processed, overlapping estimates are aggregated through weighted sum operation.
Figure 17.6 uses an example to illustrate the denoising process in our decoder.
We assume an additive white Gaussian noise (AWGN) channel with 5 dB receiver
SNR and additional 1% loss rate. Figure 17.6b shows the channel output where white
dots indicate the lost pixels. The entire image is contaminated with noise, but inter-
estingly, most image features are still recognizable. This phenomenon supports our
argument that spatial redundancy can provide channel protection, and image denois-
ing is the necessary step to utilize such redundancy. Figure 17.6c,d shows the result
after the median ﬁlter and BM3D, respectively. The resulting image is very similar
to channel input.
After denoising, transform-domain de-scaling is performed on each frame. This
is accomplished by DCT transform, scaling, and inverse DCT transform. If the frame
size is smaller than the regular size, indicating a portion of the coefﬁcients have been
dropped, the decoder will pad zeros to form a frame of regular size, then perform
inverse DCT. The de-scaled frames and decoded motion information will then be
used to reconstruct the GOP by inverse MCTF.
17.4 Implementation
17.4.1 Cactus Implementation
The Cactus system is composed of application-layer CODEC (coder and decoder)
and physical-layer modules. The Cactus encoder only needs the available bandwidth

380
17 Denoising in Communications
information from the channel, which can be predetermined. In the Cactus encoder,
we use a reference C code for MCTF, and implement all the other modules, includ-
ing transform, bandwidth allocation, and reduction, L-shaped chunk division and
scaling, entropy coding, and channel coding, in MATLAB. All the modules except
MCTF could process CIF (352×288) videos in real time. However, we believe that
MCTF can be run in real time, because it has very similar processing steps and
complexity as the hierarchical-B coding structure in H.264 [397]. The latter already
has a real-time implementation x.264 [429], which can encode four or more 1080
p streams in real time on a single consumer-level computer. In particular, the two
schemes have similar computational complexity in motion estimation step (both ﬁnd
motions in previous and following frames), which is known to be the most time-
consuming module in a video encoder.
We implement Cactus on orthogonal frequency division multiplexing (OFDM)
physical layer (PHY) deﬁned in IEEE 802.11 a/g. Speciﬁcally, the channel is di-
vided into 64 subcarriers and 48 of them are used to transmit modulation symbols.
To reduce the overhead of a Physical Layer Convergence Protocol (PLCP) header,
we use 100 OFDM symbols in each PLCP frame for data transmission. Therefore,
the total number of modulation symbols in each transmission is 4800. Metadata is
transmitted at the lowest rate 0.5 bits/s/Hz (BPSK with 1/2 coding) in 802.11 a/g.
To resist packet loss, the adjacent symbols from a picture are pseudo-randomly
shufﬂed across different PLCP frames. We limit the shufﬂing in a GOP of video
frames to reduce the decoding delay. We generate the shufﬂe mapping through sort-
ing a set of random numbers between 0 and 1. The sorted index is used for shufﬂe
mapping. The random numbers can be produced by a predeﬁned random seed and it
does not introduce additional overhead. The shufﬂed symbols are sequentially placed
on each OFDM symbol. Therefore, when a PLCP frame is lost, it creates randomly
dispersed “holes” in the video frame, which can be easily processed by the median
ﬁlter.
In the Cactus decoder, we implement channel decoding, entropy decoding, and
transform-domain de-scaling in MATLAB. The inverse MCTF has a much lower
computational complexity than MCTF encoder. Therefore, the decoder also can be
implemented in real time. We use the median function in MATLAB to perform the
median ﬁlter denoising, and use the MATLAB code published by Matteo Maggioni,
Alessandro, and Foi [430] to perform BM3D denoising for all the evaluations. The
processing time for one CIF video frame is around 1.4 seconds using Intel Core Quad
CPU (Q9550) 2.83 GHz.
17.4.2 GPU Implementation of BM3D
We note that the current implementation of BM3D has a high computational com-
plexity. Fortunately, it is very suitable for parallel computing (e.g., a specially de-
signed chip, a ﬁeld programmable gate array [FPGA], or a GPU). In order to validate
that our system can run in real time, we implement BM3D through GPU NVIDIA

17.4 Implementation
381
Figure 17.7 GPU implementation of the basic estimate step of the BM3D algorithm.
Table 17.1 Memory and core usage in one SM by each BM3D processing step.
Module
Data
Size (KB)
Cores
Block Matching
1 block
1.5
192
Haar
24 blocks
6.1
192
Hadamard
48 blocks
12.3
192
Inverse Hadamard
48 blocks
12.3
192
Inverse Haar
16 blocks
4
128
Blending
384 blocks
9.3
192
GTX680. It has 8 streaming multiprocessors (SMs). Each SM has 192 cores and 64
KB shared memory in which 48 KB can be used to store data. We implement BM3D
in GPU following two optimization rules. The ﬁrst rule is to fully utilize all 192
cores. Second, because accessing the display memory (size up to 2 GB) is slow, the
data processed by the 192 cores should not exceed the SM’s memory size, which is
48 KB.
We implement the basic estimate step of BM3D as shown in Figure 17.7. Every 8
× 8 block looks for matching blocks in a given rectangle region. The original block
and matched blocks are organized in a 3D array. Then it is transformed by 2D Haar
and 1D Hadamard transform. The noise is removed by hard thresholding. Finally, in-
verse transforms are performed, and pixel values corresponding to the same position
are aggregated. Table 17.1 shows the memory usage, core usage, and involved data
size for each SM. It can be seen that we make full use of 192 cores in almost all the
processing steps. All the eight SMs perform identical operations.
We evaluate our GPU implementation of BM3D over 16 CIF test video sequences
under 5 dB AWGN channel. The denoising results are listed in Table 17.2. The an-
chor results in the second column is achieved by the ofﬁcial MATLAB code [430].
On average, our implementation has 0.19 dB loss in video PSNR. This is due to two
simpliﬁcations. First, we do not implement the ﬁnal estimation step of the complete
BM3D algorithm. Second, the original 2D biorthogonal transform is replaced by the
Haar wavelet transform. The last column in the table shows the processing speed in
fps. On average, GPU can process CIF videos at the speed of 35 fps, which is almost
50× of the CPU speed. It veriﬁes the feasibility to use BM3D as part of a real-time
video communications system.

382
17 Denoising in Communications
Table 17.2 Reconstructed PSNR and speed of GPU implementaion for CIF videos under 5 dB
AWGN channel.
Sequence Anchor (dB) GPU (dB) Speed (fps)
Akiyo
42.92
42.33
33.60
Bus
32.32
32.20
35.60
Coastguard
35.12
34.89
33.97
Crew
39.63
39.53
35.00
Ower
32.32
32.23
35.07
Football
31.97
31.89
39.46
Foreman
37.48
36.97
34.83
Harbour
32.95
32.81
34.46
Husky
26.79
26.81
34.76
Ice
38.54
38.20
35.66
News
38.80
38.53
34.50
Soccer
36.03
35.88
35.49
Stefan
33.53
33.32
35.75
Tempete
33.20
33.31
34.43
Tennis
35.62
35.52
34.27
Waterfall
38.04
37.79
33.64
Average
35.33
35.14
35.03
17.5 Evaluation
17.5.1 Settings
Wireless environment: Evaluations are carried out with SORA (equipped with
WARP radio board) over 802.11 a/g-based WLAN. The carrier frequency is 2.4 GHz.
The channel bandwidth is 12 MHz and the data bandwidth is around 11.4 MHz. We
deﬁne bandwidth ratio ρ as the ratio of channel bandwidth to source bandwidth.
We perform 52 test runs. In each test run, we transmit data generated by the Cactus
encoder. The receiver records the received wireless symbols. These symbols are not
only used for Cactus decoding, but are compared with the exact channel inputs to
generate the traced channel noise. The traced data are labeled from 1 to 52 (according
to the time they are obtained). Our comparisons with reference schemes are trace-
driven to ensure fairness.
The effect of packet loss is evaluated by assuming an interferer who sends packets
at constant intervals.
Video source: We create two monochrome video sequences of different reso-
lutions for our evaluation. The common intermediate format (CIF) sequence has a
resolution of 352×288, and the frame rate is 30 fps (frame per second). Hence, the
source bandwidth is 1.52 MHz (in complex symbols). This sequence is created by
extracting the ﬁrst 32 frames from the following 16 standard video test sequences
including akiyo, bus, coastguard, crew, ower, football, foreman, harbour, husky, ice,
news, soccer, stefan, tempete, tennis, waterfall. Hence, it has 512 frames in total. It

17.5 Evaluation
383
is similar to the test sequence used in SoftCast [395], with the only difference that
the resolution used in SoftCast is 352×240.
The other HD (720 p) sequence has a resolution of 1280×720, and the frame rate
is 30 fps too. Hence, the source bandwidth is 13.8 MHz. In order to transmit it in a
11.4 MHz channel, bandwidth compaction is needed and the ratio is around 0.826.
This sequence contains the ﬁrst 32 frames from 10 standard video test sequences,
including Intotree, Shields, Stockholm, City, Jets, Panslow, Parkrun, Sheriff, Shut-
tleStart, Spincalendar. Therefore, the total length is 320 frames.
Reference schemes: Two reference schemes are considered. The ﬁrst one is Soft-
Cast. Our implementation has only one difference from that described by Jakubczak
and Katabi [395]. In order for a fair comparison with Cactus, which uses GOP size
8, the GOP size of SoftCast is set to 8 too. We actually evaluated both schemes when
the GOP size is increased to 16, and found that both schemes will have a 0.3–0.5 dB
performance gain in PSNR. SoftCast needs to transmit metadata with digital method
too. There are 64 variances per each video frame. We do not actually transmit these
metadata for SoftCast.
The other reference is based on H.264 or MPEG4 AVC digital video coding stan-
dard [13]. We adopt a publicly available encoder called x264 [429] to encode test
sequences at different rates and obtain a R-D (rate-distortion) curve. Similarly, GOP
size is set to 8 for fairness. In the case of multicast, we simply call it MPEG. In the
case of multicast, we name it Omni-MPEG because we assume the sender in this
scheme can immediately obtain the SNR of the previous packet, and use this SNR
to guide the rate selection of the next packet. The possible rates are those deﬁned in
802.11 a/g. We then calculate the goodput rate for an entire test run, and ﬁnd the cor-
responding distortion from the R-D curve, as if the encoder had known the channel
conditions in advance. The performance of Omni-MPEG provides an upper bound
for the conventional digital schemes in Unicast.
We do not compare with the scalable video coding (SVC) extension of H.264/AVC
because it has been shown in SoftCast that the performance is inferior to SoftCast in
all cases.
Performance metric: We evaluate the video delivery quality with the standard
peak signal-to-noise ratio (PSNR) in dB. We compute the PSNR for each video frame
by PSNR = 10log10 2552
MSE , where MSE is the mean squared error of all pixels. Then
the PSNR is averaged across frames.
17.5.2 Micro-Benchmarks
Micro-benchmarks verify our design choices. The results are obtained with the CIF
video sequence, and the bandwidth ratio ρ is 1, if not otherwise stated.
Use of temporal redundancy: Cactus removes temporal redundancy by MCTF,
and encodes the motion information into digital streams. The digital stream will share
bandwidth with the analog transmission of scaled pixel values. We ﬁrst examine the
bandwidth percentage of motion information assuming a very robust transmission

384
17 Denoising in Communications
0
2
4
6
8
 10
 12
 14
 16
 18
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16
MV Bandwidth Ratio (%)
Video Sequence Number
Figure 17.8 The bandwidth percentage used by transmitting motion information in (BPSK, 1/2).
scheme (BPSK, 1/2), and check whether the use of bandwidth is worthy. Figure 17.8
shows the bandwidth ratio and we can see that the amount of motion information
differs greatly among sequences. Sequence #6 (football) has very complex motions,
while Sequence #1 (akiyo) and #11 (news) have simple or small motions.
Then we verify our claim that temporal redundancy should be removed at the en-
coder, and examine in what cases this claim does not hold. We compare the ﬁnal
design of Cactus (MCTF at the encoder and BM3D at the decoder) against two al-
ternatives. Method (DCT3d, BM3D) uses 3D-DCT at the encoder. Hence the tempo-
ral redundancy is not fully exploited. Method (DCT3d, VBM3D) exploits temporal
redundancy at the decoder by using video BM3D. We have mentioned earlier that
video BM3D searches for matching blocks not only in the current frame but in adja-
cent frames as well. Both alternatives have more bandwidth than Cactus to transmit
coefﬁcients, because the encoder does not generate motion information.
We run this test over Trace 6 (average receiver SNR = 9.97 dB) for all 16 CIF
sequences. On average, Cactus has 1.67 dB gain over (DCT3d, VBM3D), and the
latter has 2.39 dB gain over (DCT3d, BM3D). Figure 17.9 presents four representa-
tive sequences. Most of the skipped sequences share the same trend as news. For this
sequence, Cactus performs the best, and using video BM3D brings certain gain over
using BM3D. Both bus and ice show an interesting result that video BM3D does not
bring any gain over BM3D. This means that most patches in the video have enough
similar patches in the same frame to smooth out the noise. This is exactly the case
in bus where the trees, fences, and bricks have similar patterns, and in ice too where
the ice patches resemble each other.
The football is the only sequence that Cactus does not perform the best. This is
because the motion information occupies too much bandwidth (around 16%), and

17.5 Evaluation
385
 25
 30
 35
 40
 45
news
bus
football
tempete
PSNR (dB)
DCT3d,BM3D
DCT3d,VBM3D
Cactus
Figure 17.9 Three schemes, which use temporal redundancy differently, are compared on four
representative sequences.
too many coefﬁcients are dropped, which introduces loss. This suggests that there
exists a trade-off point beyond which the temporal redundancy is better utilized at
the receiver.
Image denoising: Image denoising is a key module in the Cactus decoder. This
experiment evaluates the performance gain brought by this module, and examines
the impacting factors. Figure 17.10 shows the average denoising gain for all 16
CIF sequences under different receiver SNRs. The gain shows a clear decreasing
trend. This suggests that for a Cactus receiver, if the measured channel condition is

386
17 Denoising in Communications
4
6
8
10
12
14
0
0.5
1
1.5
2
2.5
Receiver SNR (dB)
PSNR Gain (dB)
Figure 17.10 Denoising gain as a function of receiver SNR.
better than a certain SNR, it may turn off the denoising module without much loss in
performance.
The denoising gain also depends on the video characteristics. Figure 17.11 shows
the PSNR gain on each of the sequences, and the value presented is averaged over all
32 frames and all traces. Sequence #10 (ice) beneﬁts the most from denoising, with
an average gain over 2.5 dB. This is because the frames in this sequence are all very
smooth. Sequence #16 (waterfall) gains the least from denoising, because the frames
contain too much texture and details.
Transmission in spatial domain versus in transform domain: Actually, Cac-
tus could choose between transmitting in spatial domain (scaled pixel values) or in
transform domain (scaled DCT coefﬁcients). The two choices cost the same trans-
mission power as DCT transform is orthonormal. We have embraced the spatial-
domain transmission in the ﬁnal design of Cactus, and this experiment is going to
verify this choice.
We compare our spatial-domain transmission with two transform-domain alterna-
tives, one with Hadamard transform (as used in SoftCast) and the other without any
transform. In this evaluation, we let ρc = 1, that is, there is just enough channel band-
width to transmit all the pixels or coefﬁcients. We do this simpliﬁcation because the
dimension of Hadamard matrix has to be a power of 2. We run the experiments for
video Sequences #10 and #16 on Trace 8 (receiver SNR = 7.24 dB). These two se-
quences are chosen because they beneﬁt the most and the least from image denoising
as shown by the previous experiment.
Figures 17.12 and 17.13 show the comparison results for Sequences #10 and #16,
respectively. It is not surprising that transmitting ice in spatial domain signiﬁcantly
outperforms the other two choices, since ice is very friendly to image denoising. The
gain over the transform-domain transmission with Hadamard is as high as 7.44 dB

17.5 Evaluation
387
 0
 0.5
 1
 1.5
 2
 2.5
 3
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16
PSNR Gain (dB)
Video Sequence Number
Figure 17.11 Denoising gain on different sequences.
when the packet loss ratio is 0.1. The experiment on waterfall obtains slightly differ-
ent results. When the packet loss ratio is small, transmitting the video in spatial do-
main does not bring any gain. However, this is the sequence which beneﬁts the least
from image denoising. Even for this sequence, Cactus outperforms the other choices
in most cases. This experiment validates our choice to transmit video in spatial
domain.

388
17 Denoising in Communications
0.001
0.002
0.005
0.01
0.02
0.05
0.1
24
26
28
30
32
34
36
38
40
42
Fraction of Lost Packets
PSNR (dB)
 
 
Cactus
w/ Hadamard
w/o Hadamard
Figure 17.12 Comparing transmitting Sequence #10 (ice) in spatial domain or transform domain
under different loss rates.
0.001
0.002
0.005
0.01
0.02
0.05
0.1
24
26
28
30
32
34
36
38
40
42
Fraction of Lost Packets
PSNR (dB)
 
 
Cactus
w/ Hadamard
w/o Hadamard
Figure 17.13 Comparing transmitting Sequence #16 (waterfall) in spatial domain or transform
domain under different loss rates.
17.5.3 Comparison against Reference Systems
Figure 17.14 compares the performance of Cactus against two reference schemes,
namely SoftCast and Omni-MPEG under multicast scenario. We run 52 traces over
the CIF sequence to emulate a 52-receiver multicast session. For Cactus and Soft-
Cast, in each test run, we compute the average receiver SNR across PHY packets

17.5 Evaluation
389
5
6
7
8
9
10
11
12
13
14
15
10
15
20
25
30
35
40
45
Receiver SNR (dB)
PSNR (dB)
Cactus
SoftCast
BPSK 1/2
QPSK 1/2
QPSK 3/4
16-QAM 1/2
Figure 17.14 Compare Cactus against two reference schemes for a CIF sequence in a multicast
session when bandwidth ratio ρ = 1.
and average video PSNR across sequences. To plot the video PSNR as a function
of receiver SNR, we divide the receiver SNR range into 1 dB bins, and average all
the (receiver SNR, PSNR) pairs whose receiver SNR fall into the same bin. Results
show that although both Cactus and SoftCast achieve graceful rate adaptation, Cac-
tus consistently outperforms SoftCast in video PSNR, and the average gain is 4.7
dB.
For MPEG, the sender has to ﬁx a transmission rate in each test run. We run four
tests, using the lower half of the PHY rates deﬁned in 802.11a/g. Once the transmis-
sion rate is ﬁxed, the video PSNR can be found in the R-D curve. We run each trace
for each PHY rate, if the instantaneous receiver SNR is higher than expected, trans-
mission is successful. Otherwise, the receiver can get nothing. We average the PSNR
along each trace, and also plot (receiver SNR, PSNR) in bins. Figure 17.14 clearly
shows that Cactus outperform MPEG because the latter suffers from the threshold
effect.
Note that although Cactus transmits motion information through the digital method,
they are always protected with the lowest rate (1/2) channel coding and transmitted
using BPSK modulation. Therefore, it is insensitive to channel variations.

390
17 Denoising in Communications
 26
 30
 34
 38
 42
 46
Trace_1 
(4.62dB)
Trace_20 
(8.40dB)
Trace_9 
(10.98dB)
Trace_4 
(14.70dB)
PSNR (dB)
Omni-MPEG
SoftCast
Cactus
Figure 17.15 Compare three schemes for a 720 p sequence under bandwidth ratio 0.82.
17.5.4 Transmitting High-Deﬁnition Videos
High-deﬁnition videos have huge size in its raw form. It is important to evaluate
whether a nearly uncompressed video communications system could transmit such a
huge amount of data under the current wireless channel condition. The video source
we use for this experiment is 720 p (1280×720), and the source bandwidth is 13.8
MHz. As the available channel bandwidth is 11.4 MHz, bandwidth compaction is
needed, and ρ = 0.82.
We pick four traces whose average receiver SNR ranges from 4.62 dB to 14.7 dB
to carry out this experiment. Figure 17.15 shows the average video PSNR achieved

17.5 Evaluation
391
0
0.001
0.002
0.005
0.01
0.02
0.05
0.1
24
28
32
36
40
44
Fraction of Lost Packets
PSNR (dB)
 
 
SoftCast
Cactus
Figure 17.16 Comparing the error resilience capability of Cactus and SoftCast under trace #16
(receiver SNR = 13.59 dB).
by Cactus and the two reference schemes. It is very encouraging that Cactus achieves
better performance than Omni-MPEG even in this Unicast transmission scenario. In
the case of multicast, the gain of Cactus will be more signiﬁcant, since both Cactus
and SoftCast transmit the same content in these four traces, and they would allow
different receivers to get a video quality that is commensurate with their channel
conditions.
Cactus consistently outperforms SoftCast too. When the receiver SNR is 4.62 dB,
Cactus achieves 5.6 dB gain over SoftCast. We also observe an interesting trend that,
as the receiver SNR increases, the performance difference between Cactus and Soft-
Cast decreases. When the receiver SNR is 14.70 dB, the performance gain decreases
to 3.18 dB. This is due to the fact that image denoising is more useful in poor chan-
nels. The denoising effect is more prominent for HD videos than CIF videos because
there is more spatial redundancy in HD videos.
We would like to mention that although bandwidth expansion is also a possible
case in theory, it is less likely to happen in practice, especially for HD videos. There-
fore, we do not evaluate such cases in this work.
17.5.5 Robustness to Packet Loss
This experiment evaluates whether Cactus is robustness to packet loss. The traces
used in this experiment is #16, which has a receiver SNR of 13.59 dB, and #22,
which has a receiver SNR of 5.82 dB. The packet loss is simulated by assuming an
interferer who transmits packets at constant intervals. We evaluate both Cactus and

392
17 Denoising in Communications
0
0.001
0.002
0.005
0.01
0.02
0.05
0.1
24
28
32
36
40
44
Fraction of Lost Packets
PSNR (dB)
 
 
SoftCast
Cactus
Figure 17.17 Comparing the error resilience capability of Cactus and SoftCast under trace #22
(receiver SNR = 5.82 dB).
SoftCast when the packet loss ratios are 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, and
0.1.
Figures 17.16 and 17.17 show the average video PSNR achieved by Cactus and
SoftCast as a function of loss rate under channel trace #16 and #22, respectively. In
both ﬁgures, the x-axis is in logarithm. When the channel condition is poor (trace
#22), both Cactus and SoftCast are not sensitive to packet losses. The PSNR loss
of Cactus is less than 1 dB when the loss ratio is 0.02. However, when the channel
condition is good (trace #16), the transmissions are sensitive to losses.
Though not very obvious in the ﬁgures, when the packet loss ratio is 0.1, the
PSNR loss of Cactus with respect to no loss case is 6.3 dB for trace #16 and 2.6
dB for trace #22. Under the same packet loss ratio, the PSNR loss of SoftCast with
respect to its no loss case is 7.4 dB and 3.1 dB, respectively. This shows that Cactus
has even higher robustness to packet loss than SoftCast.
17.6 Related Work
The design of Cactus essentially belongs to joint source-channel coding (JSCC).
JSCC is an extensively studied topic both from an information theoretical perspective
and for the speciﬁc application on video communications.
In the category of JSCC for digital video communications, Cheung and Zakhor
[431] proposed to distribute the available source and channel coding bits among the
subbands to minimize the expected distortion. This is analogous to the transform-
domain scaling (power allocation) in Cactus design. He et al. [350] proposed a JSCC

17.7 Summary
393
scheme which determines the optimal β in source coding, which is the percentage of
blocks coded without prediction, based on the channel parameters such as bandwidth
and bit error rate (BER). A higher β implies retaining more redundancy in the source.
Flexcast [354] replaces the entropy coding module in H.264 with a rateless code, thus
to achieve graceful quality degradation in video Unicast.
Research on analog JSCC mostly considers general source and channel. For in-
stance, it is well-known that a Gaussian source achieves the capacity of an AWGN
channel [432]. Gastpar et al. [433] observed that channel coding is not necessary
in some cases for optimal communication, but the source and the channel have to
be matched in a probabilistic sense. Compression techniques, like vector quanti-
zation (VQ) [434] and Wyner-Ziv coding [417, 435], have been adopted in hybrid
digital-analog transmissions to match source and channel. Recently, Kochman and
Zamir [436] showed that, by combining prediction and modulo-lattice arithmetic,
one can match any stationary Gaussian source to any colored-noise Gaussian chan-
nel, hence achieve Shannon’s capacity limit. They also pointed out that the analog
transmission scheme is more robust than its digital counterpart and is not sensitive to
exact channel knowledge at the sender.
Recently, an analog mobile video system named SoftCast has been proposed
[395]. In SoftCast, 3D-DCT is performed on a group of pictures, and the trans-
form coefﬁcients are transmitted as the I and Q components of a complex symbol
after power scaling. As quantization and entropy coding are skipped, SoftCast keeps
most of the source redundancy. Such a scheme is robust to channel variations and
is capable of achieving graceful degradation in a wide range of channel conditions.
However, an important fact that has been neglected in SoftCast is that the retained
source redundancy should be actively utilized at the receiver. In addition, SoftCast
does not explore the possibility to remove partial redundancy at the encoder.
17.7 Summary
We have described in this chapter an efﬁcient and robust wireless video communi-
cations system — Cactus. Our design validates that it is possible in video commu-
nications to skip channel coding and rely solely on source redundancy for channel
protection. We have also successfully used Cactus for the transmission of HD videos.
Surprisingly, Cactus achieves better performance than Omni-MPEG even in the rate
adaptation enabled unicast scenario, which demonstrates the great potential of such
hybrid digital-analog scheme.


Chapter 18
MIMO Broadcasting with Receiver Antenna
Heterogeneity
18.1 Introduction
Multimedia broadcasting has recently attracted a great deal of research interest [437,
438] along with growing wireless communication capabilities [439, 440] and the
demand for audio and video streaming [2]. However, broadcasting systems tradi-
tionally suffer from heterogeneous channel conditions across receivers. A trans-
mission scheme should normally ensure that all receivers can receive broadcasted
information, resulting in the system performance often being constrained by the
worst channel. This is likely to be exacerbated in future systems featuring multiple-
input multiple-output (MIMO) technologies, which have increasingly become build-
ing blocks for high-capacity wireless link technologies such as 802.11n [439],
WiMAX [440], and 3GPP LTE [441]. Devices in such networks may be equipped
with varying numbers of antennas. We refer to this as receiver antenna heterogene-
ity.
Figure 18.1 shows an exempliﬁed scenario. With two antennas, the base station
can send two concurrent blocks of information that are either distinct, to achieve
a higher rate, or duplicate, for better reliability. The former is only possible if the
receiver also has two antennas. If we design a system based on the higher rate ca-
pability of the two-antenna receiver B1, the single-antenna receivers, A1 and A2,
are likely to get no information, since they would have to solve an under-determined
linear system. Conversely, some of B1’s channel capacity would be wasted. Ideally,
a transmission strategy is needed that allows both types of users to receive infor-
mation whose quality is commensurate with the antenna setting and channel SNR
simultaneously.
Some previous work has considered channel SNR heterogeneity or antenna het-
erogeneity. Hierarchical modulation (HM) schemes [418, 442] and SoftCast [396,
443] consider SNR heterogeneity for single antenna systems. Although the latter can
achieve a smooth rate increase with the channel SNR (CSNR), it does not readily ex-
tend to addressing antenna heterogeneity. Diversity embedded space time code [444],
designed for a broadcast system with four transmitting antennas and varying receiver
395

396
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
User A1
User B1
User A2
Base Station
Figure 18.1 A MIMO broadcast system with receiver antenna heterogeneity.
antenna numbers, does address the antenna heterogeneity issue. The scheme sepa-
rates the source into layers of different priorities, and superimposes different layers
by embedding a high diversity code within a high rate code. Each receiver derives an
amount of information matching its antenna number. Various superimposed MIMO
codes have also been designed for cooperative channels [445], 2 × 1 MIMO [446],
and 2 × 2 MIMO systems [447]. In these schemes, the strong users are expected to
decode both layers of data with the help of relay nodes or good channel conditions,
with weak users only able to derive the base layer. As with HM, however, only a
few discrete levels of recovery quality can be achieved in all these schemes, and the
bandwidth of the weak receivers is reduced by the transmission of low-priority data.
In this chapter we present a new analog image broadcast system in such MIMO
systems with receiver antenna heterogeneity. As in conventional systems, raw image
is ﬁrst decomposed into compressible wavelet coefﬁcients to remove spatial corre-
lation. Unlike conventional systems, which proceed with quantization and entropy
coding, our system employs a power allocation strategy to redistribute the transmit
power among coefﬁcients of different wavelet bands. This step scales the coefﬁ-
cients with respect to their contribution to the total distortion and balances the per-
formance trade-off between the single-antenna and two-antenna users. The scaled
coefﬁcients are then sampled by a Hadamard matrix to generate linear combinations
of wavelet coefﬁcients, or measurements, before proceeding with amplitude modu-
lation (AM) and pseudo-analog transmission. By pseudo-analog, we mean that the
transmission is actually implemented on digital communication hardware but the
constellation is much denser and the performance of AM is analogous to that in ana-
log transmission. At the receiver, the received physical layer symbols are passed di-
rectly to the image decoder without conventional MIMO receiver processing, and the
wavelet coefﬁcients are decoded using a weighted l1-minimization based decoding
algorithm [448]. Finally, the image is recovered after an inverse wavelet transform.
Our system design makes two major contributions. First, our system addresses the
two dimensions of heterogeneity simultaneously by integrating compressive sens-
ing (CS) into MIMO transmission. By applying CS, receivers with an insufﬁcient
number of antennas can solve the under-determined problem with an approximate
solution. The decoding performance scales with the quality and the number of

18.2 Background and Related Work
397
received measurements, which is determined by the CSNR and the antenna setting
respectively. While the generic framework has proved feasible in our previous work
using an abstract statistical source model [449], the work in this chapter considers
performance optimization for a practical end-to-end system for image transmissions.
Second, we introduce a power allocation strategy to achieve a performance trade-off
between single-antenna and two-antenna users in a way that is more ﬂexible than
in conventional digital systems. Our system is evaluated with extensive simulations
and compared to an extension of SoftCast to MIMO systems and two conventional
layered source-channel schemes.
18.2 Background and Related Work
18.2.1 Multi-Antenna Systems
Due to their high spectral efﬁciency or improved reliability, MIMO technologies
have been increasingly incorporated as an important building block for next genera-
tion wireless networks [439–441]. By exploiting the spatial dimension across multi-
ple antennas at the sender and the receiver, MIMO could potentially improve a wire-
less system’s capacity, range, and reliability [450–452]. In such a setup, each of the
nt transmit antennas could transmit an independent packet fragment, called a spatial
stream, concurrent with other antennas. Each of the nr receiver antennas would mea-
sure a linear combination of the transmitted signals. The original streams could be
decoded as long as enough linear combinations are received, that is, nr ≥nt, through
a process analogous to solving simultaneous linear equations. This is referred to
as spatial multiplexing. The maximum number of streams that can be supported is
M = min(nt,nr). This nt ×nr system could potentially achieve M times the 1×1 rate.
Even with a single antenna at the receiver end, but multiple antennas on the trans-
mitter, the system could beneﬁt from improved reliability, or diversity gain, by hav-
ing antennas transmit redundant information. Even better, more diversity could be
achieved through space time block codes (STBC) [452]. The Alamouti scheme pro-
vides an example for the 2 × 1 case [453], extending over two time slots, where the
two antennas transmit the symbols x1 and x2 respectively in the ﬁrst time slot, and
−x∗
2 and x∗
1 in the second slot.
A fundamental challenge of point-to-point MIMO transmission lies in how to
derive the optimal diversity-multiplexing trade-off [454]. For broadcasting, there is
also a performance trade-off among different users. First, if all users have the same
number of antennas, the trade-off is mainly across different CSNRs. Assuming a
rich scattering environment that favors MIMO, the situation is then analogous to
traditional single-antenna broadcasting systems, where sufﬁcient reliability must be
provided for the lowest-SNR users. Second, if receivers have varying numbers of an-
tennas, that is, there is receiver antenna heterogeneity, the transmitter is normally
constrained to a single spatial stream to ensure decoding at the single-antenna

398
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
receivers. Therefore, the system can normally only beneﬁt from diversity gain from
the extra transmit antennas, and the multiplexing capacity of multi-antenna users
would be wasted. This exacerbates the heterogeneity challenge in a broadcasting
system. We need a scheme that will strike a balance between achieving diversity
gain for low-SNR or single-antenna users and multiplexing gain for multi-antenna
users when channel condition permits.
18.2.2 Layered Source-Channel Schemes
In conventional broadcasting systems, the overall performance tends to tailor to the
receiver with the worst channel condition, since every user in the system needs the
broadcasting content. In a multimedia broadcasting system, multi-resolution coding
is often used to successively reﬁne the source rate-distortion trade-off, such as the
scalable video coding extension of H.264 (SVC) [455] and JPEG 2000 [456, 457].
The entropy coded bits are separated into layers with varying levels of importance.
The base layer data are always expected to achieve acceptable quality, while higher
quality is derived from the enhancement layers for users with sufﬁcient channel con-
ditions.
At the channel, superposition coding is recognized as an efﬁcient approach [418,
442] to deal with diverse channel conditions in a broadcasting system. The PHY
implementation of superposition coding is also known as hierarchical modulation
(HM), where part of the symbol is decoded ﬁrst and its effects are removed before
the remaining symbol is decoded. Multi-resolution coding at the source naturally
combines with superposition coding at the channel, such that layered source data
with different priorities are mapped to different parts of the HM symbol. The weak
receivers only aim to decode high-priority but low-quality content, while strong re-
ceivers are also expected to decode the enhancement layers.
There have been several efforts to combine superposition coding with multi-
antenna technologies. For multiple-input single-output (MISO) systems, it is natural
to transmit HM modulated signals with STBC to harness diversity gain [458, 459].
For MIMO systems, the main issue is whether users with good channel conditions
can derive multiplexing gain. Given that a pure spatial multiplexing scheme is too ag-
gressive for low-SNR users, a number of schemes [444,445,447] have been proposed
to use multiplexing and STBC for different parts of an HM symbol, corresponding to
different source layers. The more important data are sent via STBC to ensure reliable
transmission, while the less important data, coded at a higher rate, are sent via spatial
multiplexing.
However, these layered source-channel schemes only alleviate the cliff effects of
digital systems and introduce new design challenges. First, multi-resolution coding
imposes a particular decoding order and dependency between layers, and the band-
width of the weak receivers is reduced by the transmission of the enhancement layer
data. Second, due to the power difference requirement between layers for success-
ful decoding, only two or three levels can be embedded in an HM symbol, which

18.2 Background and Related Work
399
constrains the granularity of code rates. Third, STBC codes for MIMO systems with
more than two transmit antennas cannot achieve full diversity. Therefore, such sys-
tems are still far from fully accommodating users with diverse antenna settings and
channel conditions.
18.2.3 Compressive Sensing
Compressive sensing (CS) [295, 372], also known as compressive sampling, is an
emerging theory that deals with the acquisition and recovery of sparse or compress-
ible signals. According to CS theory, even when the number of sampled measure-
ments of a source vector is smaller than the source dimension, which gives rise
to an ill-posed problem, source decoding from the measurements is still feasible.
It has been used for multimedia coding and transmission in wireless communica-
tion [460–463]. The CS measurements of raw signals are transmitted and the receiver
is expected to achieve ﬁne-grained quality scalability with the number and noise en-
ergy of the received measurements.
We ﬁrst introduce the basic process of CS based image transmission. Let XW× H
denote a W × H image that we vectorize into a column vector x of size N × 1
(N = W ×H). Transforms like discrete cosine transform (DCT) and discrete wavelet
transform (DWT) [464] can convert the image signal from the spatial domain to the
frequency domain, in which images are normally regarded as compressible for CS
sampling and reconstruction [465, 466]. Hence, the signal x can be represented by
a set of coefﬁcients θ1,θ2,··· ,θN in the frequency domain. Let us sort the coefﬁ-
cients in descending order according to their absolute values, and stack them into an
N-dimensional signal θ = (θ1,θ2,··· ,θN)T. Then θ is a compressible signal if |θi|
decays with i−1/p, which means that it obeys:
|θi| ≲C0 ·i−1/p
(18.1)
where ≲means “less than or approximately equal to” and C0 is a constant. Previous
research has shown that, for natural images, p is approximately 1.67 [467]. It should
be noted that the parameter p dominates the degree to which a signal is compressible.
A smaller p indicates a more compressible signal, as the magnitudes decay faster.
In compressive sensing-based image transmission, the measurements of the sig-
nal x are transmitted instead of the raw signal. In particular, we obtain these mea-
surements by m = (m1,m2,...,mk)T = Φx, where Φ is a k × n matrix, which is
referred to as a sensing matrix or a measurement matrix. The received measurements
at the users are always contaminated by channel noise. When matrix Φ satisﬁes
certain conditions, the reconstruction of signal x can be achieved by solving an l1-
minimization problem, which is a relaxed form of the l0 norm-based minimization
problem of a combinatorial nature. Candes et al. [448] constructed a weighted l1-
minimization problem, and showed that using weights that are inversely proportional
to the true signal magnitudes can improve CS reconstruction performance.

400
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
Multimedia coding via compressive sensing has three desirable properties that
motivate its emerging application in wireless transmission. First, the generated mea-
surements are democratic, which ensures that the transmitted measurement symbols
have equal importance [468]. Second, for a compressible source that is not strictly
sparse, CS recovery performance scales with the number of available measurements
when they have similar noise levels [460,469]. Third, the lower the noise level of the
received measurements, the better the CS decoding performance [470]. These prop-
erties illustrate that multimedia transmission based on compressive sensing could
achieve ﬁne-grained quality scalability matching the channel condition and avoid-
ing the cliff effect in conventional multimedia broadcasting. However, to the best of
our knowledge, no existing work applies compressive multimedia broadcasting to
MIMO systems with receiver antenna heterogeneity.
18.2.4 SoftCast
Unlike most digital systems, SoftCast [396, 443] uses a linear codec that combines
video and channel coding to achieve graceful video recovery performance in line
with the CSNR. It avoids a digital scheme of prespeciﬁed bit rates altogether. It starts
with a DCT transform, and then allocates power weights based on the energy distri-
bution of these DCT coefﬁcients. After power allocation, a Hadamard transform-
based whitening is applied across packets to ensure that transmitted packets have
equal power and importance. In addition to the linear encoder, the transmitted signal
strength is designed to be proportional to the source magnitude, and hence graceful
decoding performance can be achieved for users with diverse channel quality.
However, SoftCast’s decoder always constructs an approximately full-rank matrix
to feed into a linear least square estimator (LLSE) [427]. The decoding performance
degrades signiﬁcantly when the number of received symbols is far smaller than the
source dimension, such as in our scenario of a single-antenna setup. Therefore, Soft-
Cast cannot be readily extended to our setting while maintaining the same perfor-
mance as in a homogeneous case. Since we only consider image transmission in this
work, we will mainly refer to the 2D version of SoftCast [443] hereafter.
18.3 Compressive Image Broadcasting System
We propose an analog image delivery system over MIMO links with receiver an-
tenna heterogeneity as shown in Figure 18.1. The base station is equipped with two
antennas, that is, nt = 2, and single-antenna and two-antenna receivers coexist, that
is, nr = 1 or nr = 2. For simplicity, we will refer to such a setup as a 2 × {1,2}
MIMO system throughout the paper. We assume the number of available time slots
is T = N/4 to transmit an image of size N = W ×H in this system.

18.3 Compressive Image Broadcasting System
401
Compressive Sampling by m    
Amplitude Modulation
Demodulation
CS Decoding
Discrete Wavelet Transform 
Power Allocation with a
Inverse Transform
Wireless channel
Figure 18.2 Block diagram of the compressive image broadcasting system.
Figure 18.2 depicts a block diagram of our proposed compressive image broad-
casting system. There are four steps on the encoder side, DWT transform, power allo-
cation, compressive sampling to generate measurements, and amplitude modulation
and transmission of the measurements. The main decoding steps are CS decoding
and inverse transform.
18.3.1 The Encoder and Decoder
The encoder ﬁrst transforms the image to the frequency domain to remove the spatial
correlation of the raw image using DWT, as in the JPEG 2000 standard [457]. We
use orthogonal Daubechies wavelets [471] as the DWT basis, with a ﬁlter length of
8 and decomposition level L = 5.
Next, the encoder redistributes power across the DWT coefﬁcients of different
frequency bands. The wavelet coefﬁcients are grouped into chunks, and we calcu-
late the variance of the coefﬁcients within each chunk. The total transmit power is

402
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
split among these chunks according to their variance information, with consideration
given to the antenna heterogeneity across receivers.
The scaled coefﬁcients are then divided into a number of base vectors for the
compressive sampling stage, from which measurements are generated by using a
Hadamard sensing matrix. Note that this sampling process preserves the power dis-
tribution across coefﬁcients after the power allocation step.
Finally, the measurements are directly transmitted as real (I) or imaginary (Q)
components of complex wireless signals instead of being quantized and digitized. A
distinct complex symbol is sent from each transmit antenna via spatial multiplexing.
The per-chunk variance information is also broadcasting to all receivers as metadata,
but through a reliable digital modulation and coding scheme.
At the receiver, the real and imaginary parts of the complex symbol on each
antenna are separated and collected into the overall measurement vector. Regular
MIMO decoding is skipped, and the original coefﬁcients per vector are recovered
via a weighted l1-minimization CS decoder with the help of the variance metadata.
The raw image is then reconstructed from the coefﬁcients through an inverse DWT.
18.3.2 Addressing Heterogeneity
The power allocation strategy and the CS module combine to address CSNR het-
erogeneity and receiver antenna heterogeneity simultaneously. Our power allocation
strategy adjusts the source data so that the subsequent CS module is more effective,
speciﬁcally tuning the performance trade-off between receivers with varying antenna
numbers.
CS theory makes it feasible to decode under-determined systems at single-antenna
users, as shown when we previously proved that the MIMO channel would not in-
validate CS recovery [472]. On the other hand, multi-antenna receivers are able to
beneﬁt from additional antennas via higher rates, because the CS recovery qual-
ity scales with the number of received measurements. Furthermore, the lower the
noise level in each measurement, the higher the CS decoding performance. With the
antenna setting determining the number of received measurements, and the CSNR
governing the quality of each measurement, the overall recovery quality will then be
simultaneously in line with the antenna number and the CSNR.
18.4 Power Allocation
Since we transmit the DWT coefﬁcient via amplitude modulation, the energy car-
ried in a coefﬁcient can serve as protection at the channel to guard against noise.
Therefore, transmit power allocation directly affects the reconstructed image’s qual-
ity. In particular, we need to adjust the relative power differences between the coefﬁ-
cients of different frequency bands. This is essential for two reasons. First, the energy

18.4 Power Allocation
403
of DWT coefﬁcients is concentrated in low-frequency bands. If the coefﬁcients are
transmitted verbatim without any power redistribution, low-frequency coefﬁcients
would consume most of the transmit power, so much so that high-frequency coefﬁ-
cients would be left with very little power budget. Second, receivers with different
capabilities would favor different power allocation strategies. Since we target a het-
erogeneous broadcast system with diverse antenna settings, we need to balance the
reconstruction performance of all types of users with a single scheme. This is a dis-
tinctive feature of our system.
18.4.1 Power Scaling Factors
It has been proven that, for a SISO (single-input single-output) AWGN channel and
a linear decoder, linear scaling proportional to λ −1/4 achieves optimal performance
under the mean square error (MSE) criterion [396,427]. Here, λ denotes the variance
of the random variable representing the transform coefﬁcients. Although this works
well for SoftCast, it is suboptimal for our system for two reasons. First, the optimality
of a power allocation scheme depends on the decoding algorithm. Unlike SoftCast,
our system adopts CS decoding instead of LLSE. Second, since single-antenna and
two-antenna users coexist in our target broadcasting system, the power allocation
strategy should balance the achievable multiplexing capability of two-antenna users
and the diversity beneﬁt for single-antenna users.
Let us ﬁrst provide some intuition to this problem. The purpose of power alloca-
tion is to adjust the compressibility of the DWT coefﬁcients s. As mentioned earlier
in Section 18.2, the parameter p dominates the degree to which the signal s is com-
pressible. Therefore, we limit our discussion to scaling operations in which the power
scaling factors are exponential functions of the coefﬁcients, or mathematically:
gi = C1 ·s2α
i
(18.2)
where gi is the scaling factor for coefﬁcient si, and C1 is a constant to ensure that
the allocated power satisﬁes the total transmit power constraint. The constant 2 in
the exponent is included so that α can be compared directly to its counterpart in
SoftCast.
Since we have assumed that the compressibility parameter of the wavelet coefﬁ-
cients s is p, the scaled coefﬁcient s′
i after power adjustment is:
|s′
i| = gi ·|si| = C1 ·|si|1+2α ≲C1 ·C1+2α
0
·i−
1
p/(1+2α)
(18.3)
The compressibility parameter now becomes p/(1+2α). Since a smaller p indicates
higher compressibility, a positive α improves compressibility and a negative α does
the opposite. From a channel protection perspective, on the other hand, a larger α
tends to provide more protection to low-frequency bands. Conversely, the smaller
the α value, the better protected the high-frequency bands.

404
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
Table 18.1 The effect of different numbers of chunks.
Number of chunks
1
16
25
70
259
PSNR Gain (dB)
0
2.77 2.83 2.96
3.19
Overhead (%)
0.14 2.40 3.07 6.69 21.11
From Eq. (18.3), we can see that setting α = −1/2 means ﬂattening out all coefﬁ-
cients, and setting α = 0 means transmitting the coefﬁcients as they are. In SoftCast,
α is selected to be −1/4, which sets larger power scaling factors for high-frequency
coefﬁcients to avoid overprotecting the low-frequency bands. In our system, how-
ever, two types of receivers have different preferences. In particular, two-antenna
users receive as many CS measurements as the number of unknown coefﬁcients.
They prefer a power scaling factor that properly ampliﬁes high-frequency coefﬁ-
cients as in SoftCast, while single-antenna users only receive half of the CS mea-
surements and would prefer a larger α, which suppresses the high-frequency coef-
ﬁcients, because those coefﬁcients cannot be decoded anyway. Therefore, the best
α that optimizes the entire multicast performance would depend on the proportion
of the two types of receivers. In our design, we simply assume an equal number of
single-antenna and two-antenna receivers, and choose α based on extensive simula-
tions. We ﬁnally select α = −1/8, which strikes a good balance for system perfor-
mance. More detailed simulation results will be presented in the next section.
18.4.2 Aggregating Coefﬁcients
A practical problem arises as the power scaling parameters need to be reliably trans-
mitted to the receivers. If we select unique scaling factors for each of the coefﬁcients
and transmit them all, it immediately makes our system a trivial design at a huge
overhead, because all si can be directly computed from the scaling factors. Fortu-
nately, it has been shown [396] that adjacent coefﬁcients in the same frequency band
can be viewed as samples of the same zero-mean Gaussian distributed random vari-
able. Therefore, we can group the coefﬁcients into chunks and perform power scaling
on a per-chunk basis. All coefﬁcients in the same chunk share the same scaling factor
gi, in the computing of which the s2
i in Eq. (18.2) will be replaced by the variance
of chunk i, denoted by λi. Then, the variance information used to calculate scaling
factors are broadcasting to all receivers through a reliable modulation and coding
scheme.
Obviously, smaller chunks allow for ﬁner-grained adjustment, but they will in-
cur higher overhead when transmitting metadata. Table 18.1 shows the bandwidth
overhead and the corresponding PSNR gain for the image Lena divided into differ-
ent numbers of chunks. Here, the overhead is evaluated as the ratio of time slots
for transmitting metadata to that of transmitting DWT coefﬁcients. We ﬁnd that us-
ing 70 chunks achieves a good trade-off between overhead and performance gain.

18.5 Compressive Sampling
405
Figure 18.3 Pyramidal structure of 5-level wavelet decomposition and chunk division for Lena.
The variance metadata are quantized into integers and then encoded by Exponential-
Golomb codes. After this step, they are transmitted using 1/2 rate channel coding,
QPSK, and STBC. The chunk division details are shown in Figure 18.3. The 10 upper
left wavelet bands correspond to 10 chunks, while each of the remaining 6 bands is
split into several chunks of equal size. The chunks in the upper-left corner are smaller
than the others because there are fewer coefﬁcients in the lowest-frequency band, and
coefﬁcients from different frequency bands should not be mixed into the same chunk.
In summary, in the power allocation module, the DWT coefﬁcients are divided
into 70 chunks, and the transmit power is allocated among these chunks according to
their variance. The matrix representation of the power allocation process is:
s′ = Gs
(18.4)
where G is a diagonal power weight matrix with diagonal elements gi Eq. (18.2),
while s and s′ are the image coefﬁcient vectors before and after power scaling.
18.5 Compressive Sampling
After power allocation, the CS sampling module generates measurements through
linear projections. However, the number of DWT coefﬁcients to represent an image

406
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
is usually in the hundreds of thousands. Performing CS sampling and decoding on
such a large vector is impractical. Since coefﬁcients in the same chunk have the same
statistical characteristics, we may form short vectors with the same compressibility
characteristic by taking one element from each chunk.
However, due to the wavelet transform, not all chunks are the same size. We tackle
this problem with an approximation. In particular, we merge the 7 upper left bands
into one sampling chunk, while still computing and transmitting the variance for
each band separately, and retain the other 63 chunks. As such, we form 64 new
chunks of equal size. Since CS decoding favors a larger source dimension for better
recovery performance [466], we generate compressible source vectors of length 256
by taking 4 coefﬁcients from each chunk at a time. n = 256 is chosen to strike a
balance between good recovery performance and decoding complexity.
Let s′
b be one such source vector of length n and sb be the corresponding vector be-
fore power allocation. Therefore s′
b = Gbsb where Gb is the corresponding submatrix
of G. Then we can obtain the linear projections mi of s′
b by:
m = (m1,m2,...,mn)T = Φs′
b
(18.5)
where Φ is an n×n sensing matrix.
To ensure information capture capability and CS decoding performance, Φ should
satisfy some requirements, such as the restricted isometry property (RIP) and mu-
tual coherence [297, 473]. The most commonly used sensing matrix ensembles,
such as random Gaussian matrices and Hadamard matrices, have normalized or ap-
proximately normalized columns. This means that the transmit power distribution
among different coefﬁcients will not be changed in the sampling process. We use a
Hadamard sampling matrix in the proposed system. It has been shown [474,475] that
a Hadamard sampling matrix has the same desirable properties and comparable re-
covery performance as the optimal random Gaussian matrix ensemble, but allows us
to generate measurements at a lower complexity. It is known that using the Hadamard
sampling matrix may impose some constraints on the size of the source vector. We
circumvent this problem by dividing the image coefﬁcients into length-256 vectors
according to the method described above.
18.6 Amplitude Modulation and Transmission
Since the measurements generated after the power allocation and linear sampling
steps are compact and resilient representations of the original coefﬁcients, further
channel coding, such as that deﬁned in the conventional 802.11 PHY layer, is not
necessary. Instead, pairs of these linear projections are directly mapped to the I and Q
components of the complex signal to be transmitted. This is the same pseudo-analog
modulation as proposed in SoftCast. Each transmit antenna at the base station sends
out a distinct complex symbol.

18.7 The CS Decoder
407
Mathematically, the sender transmits xt
1 and xt
2 on the two antennas in the tth time
slot, where
xt
1 = m4t−3 + j m4t−2
xt
2 = m4t−1 + j m4t.
(18.6)
For each source vector with length n, the sender generates n measurements and takes
T0 = n/4 time slots to transmit them.
18.7 The CS Decoder
For each receiver, the received signal quality is determined by the matrix channel
deﬁned by the nt transmit antennas and nr receiver antennas. Within the broadcasting
system, the source transmission strategy is independent of the exact channel details
between the sender and any receiver, and we assume such knowledge is not available
to the sender. However, the receivers can normally obtain such knowledge through
the frame preamble. Let Ht be the nr ×nt channel matrix in the tth time slot, and each
matrix element ht
i,j denotes the path gain from transmit antenna j to receiver antenna
i.
For a single-antenna receiver, or a 2×1 case, the path gain from the two transmit
antennas to the receiver antenna are ht
1,1 and ht
1,2, respectively, in the tth time slot.
Then the received signal yt in the tth time slot is:
yt = ht
1,1xt
1 +ht
1,2xt
2 +et
(18.7)
where et is the Gaussian noise. From yt, the receiver obtains two new measurements:

m′
2t−1
m′
2t

= Ht
2×1




m4t−3
m4t−2
m4t−1
m4t



+

ℜ(et)
ℑ(et)

.
(18.8)
Matrix Ht
2×1 can be written as:
Ht
2×1 =
ℜ(ht
1,1) −ℑ(ht
1,1) ℜ(ht
1,2) −ℑ(ht
1,2)
ℑ(ht
1,1) ℜ(ht
1,1) ℑ(ht
1,2) ℜ(ht
1,2)

(18.9)
where ℜ(·) and ℑ(·) are the real and imaginary parts of a complex number.
For two-antenna receivers, or 2×2 cases, each receiver derives four new measure-
ments per time slot, 2 on each antenna:

408
18 MIMO Broadcasting with Receiver Antenna Heterogeneity




m′
4t−3
m′
4t−2
m′
4t−1
m′
4t



= Ht
2×2




m4t−3
m4t−2
m4t−1
m4t



+




ℜ(et
1)
ℑ(et
1)
ℜ(et
2)
ℑ(et
2)




(18.10)
where matrix Ht
2×2 can be written as:
Ht
2×2 =




ℜ(ht
1,1) −ℑ(ht
1,1) ℜ(ht
1,2) −ℑ(ht
1,2)
ℑ(ht
1,1) ℜ(ht
1,1) ℑ(ht
1,2) ℜ(ht
1,2)
ℜ(ht
2,1) −ℑ(ht
2,1) ℜ(ht
2,2) −ℑ(ht
2,2)
ℑ(ht
2,1) ℜ(ht
2,1) ℑ(ht
2,2) ℜ(ht
2,2)




(18.11)
After T0 time slots, the single-antenna receiver gets 2T0 new measurements. Stack-
ing these new measurements to form a vector m1:
m1 =





H1
2×1
0
···
0
0
H2
2×1 ···
0
...
...
...
...
0
0
··· HT0
2×1












m1
m2
...
m4T0−1
m4T0







+e
= HcΦs′
b +e
= HcΦGbsb +e
(18.12)
The two-antenna receiver gets 4T0 new measurements, and the measurement vector
m2 can be represented similarly.
For both types of users, the conventional MIMO decoding at the channel is
skipped, and the receiver collects the raw signal samples for the measurements across
all of its antennas over T0 time slots. Decoding over many slots permits CS operations
over a much larger matrix than a typical MIMO channel matrix, whose dimension is
far too small to satisfy CS requirements.
Since the variance information can be sent to the decoder as metadata, we adopt
the weighted l1-minimization decoder [448], instead of the standard l1-minimization
for better recovery performance. Deﬁne wi = 1/√λi, where λi is the known variance
of the ith source element at the decoder. Then the minimization problem for decoding
the source vector sb is:
ˆsb = min
sb∈Rn ∥Wsb∥l1
s.t.||m1 −HcΦGbsb|| < ε
(18.13)
where W is the diagonal matrix with wi on the diagonal and ε represents the noise
power at the receiver.
The weighted l1-minimization decoding problem for two-antenna users can be
presented similarly. As we stated before, T0 = n/4 time slots are needed to transmit
each source vector. Therefore, the problem to be solved for single-antenna users
is under-determined, and the CS decoding algorithm based on Eq. (18.13) is quite

18.8 Simulation Evaluation
409
efﬁcient. Although the linear system for two-antenna users has full rank, we ﬁnd that
the performance of the weighted l1-minimization based CS decoder matches that of
the optimal linear decoder LLSE.
18.8 Simulation Evaluation
This section serves two purposes. First, we evaluate the performance of the proposed
compressive image broadcasting system, and validate some key design parameters.
Second, we compare our system with alternatives, and demonstrate how well we
achieve the design goal, that is, every receiver obtains image recovery quality that is
commensurate with its antenna setting and channel quality.
In line with our previous description, we mainly focus on the 2 × {1,2} MIMO
setting and will discuss cases with more antennas later. Block Rayleigh fading chan-
nel with perfect receiver CSI is assumed if not otherwise stated. For clarity and with-
out loss of generality, we assume that the two receiver antennas of two-antenna users
have the same average per receiver antenna SNR. The recovery performance is eval-
uated using the standard objective measure peak signal-to-noise ratio (PSNR).
18.8.1 Micro-Benchmarks for Our System
We ﬁrst evaluate the two main components of our system separately, the transmit
power allocation strategy and the compressive sensing decoding algorithm. In partic-
ular, we evaluate the effect of different power scaling factors. In addition, the LLSE
decoder and the l1-minimization based CS decoder with or without reweighting are
also evaluated. We use three 256 × 256 gray images, Cameraman, Lena, and Pep-
pers, which are frequently used as test images in related literature.
18.8.1.1 Performance of Power Allocation
Recall that the sender scales the DWT coefﬁcients by λ α, where λ is the variance
of the coefﬁcients of a chunk and α is a parameter to be determined, which adjusts
the coefﬁcients’ compressibility. When the α value is too large (> 0), too much
power is distributed to the signiﬁcant coefﬁcients. Similarly, when α is too small
(< −1/4), too much power is distributed to the insigniﬁcant coefﬁcients. In either
case, the recovery performance is bad for both single-antenna and two-antenna users.
Therefore, we mainly consider α in the range [−1/4,0]. Further simulation results
show that a minor change of α has little inﬂuence on system performance. Therefore,
we test ﬁve values of α (−1/4, −3/16, −1/8, −1/16, and 0) and compare their
performance. The exact choice also depends on the decoder and the CSNR.

410
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
5
15
25
20
25
30
(a)  Cameraman, 2x1
CSNR (dB)
PSNR (dB)
5
15
25
20
25
30
(b)  Lena, 2x1
CSNR (dB)
PSNR (dB)
5
15
25
20
25
30
(c)  Peppers, 2x1
CSNR (dB)
PSNR (dB)
5
15
25
20
30
40
(d)  Cameraman, 2x2
CSNR (dB)
PSNR (dB)
5
15
25
20
30
40
(e)  Lena, 2x2
CSNR (dB)
PSNR (dB)
5
15
25
20
30
40
(f)  Peppers, 2x2
CSNR (dB)
PSNR (dB)
 
 
α=−1/4,LLSE
α=−1/4,CS
α=−1/8,LLSE
α=−1/8,CS
α=0,LLSE
α=0,CS
Figure 18.4 The recovery PSNR (dB) for images Cameraman, Lena, and Peppers under various
power scaling factors and decoding methods. The subﬁgures in the two rows indicate the perfor-
mance for single-antenna (labeled 2 × 1) and two-antenna users (labeled 2 × 2), respectively.
Figure 18.4 shows the received image PSNR for both single-antenna users (marked
with 2 × 1) and two-antenna users (marked with 2 × 2) under different parameter
settings. Here we only show the performance when α equals −1/4, −1/8, and 0,
respectively, for the sake of clear presentation. From Figure 18.4d,e,f, we can see
that α = 0 performs the worst for two-antenna users, introducing a 2.4–3.8 dB loss
in the received image PSNR. This conﬁrms our assumption that transmitting wavelet
coefﬁcients verbatim tends to overprotect low-frequency bands and is suboptimal.
From Figure 18.4a,b,c, we have two ﬁndings. First, unlike the 2 × 2 cases where
the LLSE decoder and the CS decoder have similar performances, the CS decoder
shows its superiority in 2 × 1 cases. This is because single-antenna users receive only
half as many measurements as there are coefﬁcients. The CS decoder is designed
precisely for such an underdetermined system and therefore outperforms the LLSE
decoder. Second, among the ﬁve α settings, α = −1/8 achieves the best performance
in most channel conditions. As discussed earlier, the parameter used in SoftCast
α = −1/4 is unfavorable to single-antenna users, and it incurs a PSNR loss of as
much as 2.4 dB when the CSNR is 25 dB.
Given all the combinations of the α settings and the decoding algorithms, the
option of α = −1/8 combined with CS decoding achieves the best performance for
both single-antenna users and two-antenna users.

18.8 Simulation Evaluation
411
5
10
15
20
25
20
24
28
32
36
CSNR (dB)
PSNR (dB)
 
 
2x2, W−L1
2x2, S−L1
2x1, W−L1
2x1, S−L1
Figure 18.5 Recovery PSNR of weighted l1 and standard l1 decoders.
18.8.1.2 Performance of the Decoder
Next, we compare the performance of using weighted l1-minimization (referred to as
W-L1) against using the standard l1-minimization CS decoder (referred to as S-L1)
for decoding. The results for transmitting image Lena for different channel condi-
tions are presented in Figure 18.5. We can observe that the weighted l1-minimization
using the coefﬁcient variance metadata wins by more than 3 dB in PSNR.
18.8.1.3 Impact of Channel Estimation Errors
In practical systems, the receiver may not obtain perfect channel state information.
We evaluate the impact of misestimated channel matrices in our system. Without
loss of generalization, we assume the channel estimation error he is zero-mean com-
plex Gaussian distributed. Hence, the estimated path gain from transmit antenna j to
receiver antenna i at some time slot can be written as:
h′
i,j = hi,j +he.
(18.14)
The energy ratio of estimation error to actual channel parameter, which could be
denoted by ρh, determines the magnitude of channel estimation error. We evaluate
the impact of misestimated channel matrices by comparing the system performance
between ρh = 0 and ρh = 0.01, where ρh = 0 means that there is no channel es-
timation error. The experimental results for transmitting image Lena are shown in
Figure 18.6. They show that the channel estimation error has a larger impact on
high-SNR receivers than low-SNR receivers. When the channel estimation error pa-
rameter ρh is 0.01, the reconstruction PSNR is decreased only by 0.1 dB to 0.6 dB
for single-antenna users and 0.1 dB to 0.9 dB for two-antenna users, when the SNR
is between 5 dB to 20 dB. The loss is larger (around 2.6 dB) for two-antenna users
when channel SNR equals to 25 dB.

412
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
5
10
15
20
25
24
28
32
36
CSNR (dB)
PSNR (dB)
 
 
2x2, ρh = 0
2x2, ρh = 0.01
2x1, ρh = 0
2x1, ρh= 0.01
Figure 18.6 The effect of channel estimation error on image recovery PSNR values.
18.8.2 Performance Comparison with Other Broadcast Systems
In this section, we compare our proposed system with representatives of two classes
of broadcast systems. One reference system is SoftCast, which adopts a similar ana-
log framework and was originally designed for SISO broadcast. We extend it to
MIMO settings and adopt a spatial multiplexing based MIMO transmission to trans-
mit the Hadamard whitened linear projections. The other class encompasses conven-
tional digital systems that adopt layered source-channel schemes. We also use more
test images in addition to the three previously tested.
18.8.2.1 Comparison with SoftCast
SoftCast implements power allocation with α = −1/4 and adopts LLSE decoding
for all receivers. Figure 18.7 shows the comparison between our system and Soft-
4
8
12
16
20
24
20
24
28
32
36
(a)  Cameraman
CSNR (dB)
PSNR (dB)
2x1,Ours
2x1,SoftCast
2x2,Ours
2x2,SoftCast
4
8
12
16
20
24
20
24
28
32
36
(b)  Lena
CSNR (dB)
PSNR (dB)
4
8
12
16
20
24
20
24
28
32
36
(c)  Peppers
CSNR (dB)
PSNR (dB)
Figure 18.7 Our system versus SoftCast using images Cameraman, Lena, and Peppers under dif-
ferent CSNRs.

18.8 Simulation Evaluation
413
Cast when transmitting the three test images. Not surprisingly, our system achieves
signiﬁcant gains of up to 4.96 dB over SoftCast for single-antenna users. Meanwhile,
our system does not incur any performance degradation for two-antenna users. For
test image Cameraman, our system even achieves a 1 dB gain in 2 × 2 cases because
DWT compacts energy much better than frame-based DCT for this image. This com-
parison clearly demonstrates the importance of power allocation and weighted CS
decoding in a MIMO broadcast system with antenna heterogeneity.
18.8.2.2 Comparison with Conventional Digital Systems
We compare our system with two layered source-channel schemes. They adopt the
same layered source coding but different MIMO transmission strategies.
In these two schemes, the state-of-the-art image codec JPEG 2000 [456, 457] is
used to generate a layered source. A (204,188) short Reed-Solomon code is em-
ployed as suggested by the DVB-H standard. The encoded bitstream is then divided
into the base layer and the enhancement layer. The encoded packets of both layers are
subjected to convolutional coding and interleaving before modulation. The convolu-
tional coding rate for the base and the enhancement layers is set to 1/2 and 2/3 re-
spectively in our simulation. Correspondingly, the receiver performs de-interleaving,
de-convolution, and RS decoding to obtain the JPEG 2000 stream.
Regarding MIMO transmission, our ﬁrst reference system, referred to as HM-
STBC, relies on STBC to fully exploit transmit diversity from multiple antennas on
the base station [458, 459]. In this system, the channel coded bitstream is modu-
lated into HM symbols. We implemented two typical HM variants, namely QPSK-
in-16-QAM (i.e., hierarchical 4/16-QAM, QPSK for both layers) and QPSK-in-64-
QAM (i.e., hierarchical 4/64-QAM, QPSK, and 16-QAM for the two layers respec-
tively) [418]. The parameter that governs the ratio of protection of the base over the
enhancement layer is set to 2. The HM symbols are sent using the Alamouti scheme.
Our second reference system, referred to as SP-MIMO, employs superposition
MIMO coding [445, 447]. In this system, the base layer data are coded with the
Alamouti scheme for higher reliability, and spatial multiplexing is used to code the
enhancement layer to leverage the higher capacity of strong receivers. The transmit-
ter combines them into HM symbols with a parameter ρ [445], which is the ratio of
the amplitude of the second layer to that of the ﬁrst layer. The value of ρ determines
the extent of protection of the two layers in signal superposition, and based on exper-
iments we set ρ = 0.3 to balance protecting both layers. As an example, if x1 and x2
are two successive symbols for the base layer data, and xi, for i = 3,4,5,6 represents
successive enhancement layer data symbols, then SP-MIMO transmits (x1 + x3) and
(x2 + x4) on the two antennas in the ﬁrst time slot, and (−x∗
2 + x5) and (x∗
1 + x6)
in the second slot, where ‘+’ means superposition. Weak receivers with low CSNRs
or a single antenna are expected to decode x1 and x2, while two-antenna receivers
with good channel conditions can also decode x3 to x6. To ensure that SP-MIMO
sends the enhancement layer at the same rate as in HM-STBC, we use both BPSK
and QPSK to match the two HM settings in HM-STBC.

414
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
0
5
10
15
20
25
30
20
22
24
26
28
30
32
34
36
38
40
CSNR (dB)
PSNR (dB)
 
 
HM−STBC, Hierarchical 4/16QAM
SP−MIMO, Hierarchical 4/8QAM
HM−STBC, Hierarchical 4/64QAM
SP−MIMO, Hierarchical 4/16QAM
New proposed
(a) Single-Antenna Users
0
5
10
15
20
25
30
20
22
24
26
28
30
32
34
36
38
40
CSNR (dB)
PSNR (dB)
 
 
HM−STBC, Hierarchical 4/16QAM
SP−MIMO, Hierarchical 4/8QAM
HM−STBC, Hierarchical 4/64QAM
SP−MIMO, Hierarchical 4/16QAM
New proposed
(b) Two-Antenna Users
Figure 18.8 Our system versus two conventional reference systems, HM-STBC and SP-MIMO,
using the image Cameraman.
Figure 18.8 (image Cameraman) and Figure 18.9 (image Lena) show the per-
formance of our system as well as the two reference digital systems. For the same
enhancement layer data rate, we can make three major observations between the
two reference digital systems. First, for both single-antenna and two-antenna users,
the CSNR required to decode the base layer in SP-MIMO is always lower than for
HM-STBC. Second, single-antenna users cannot decode the enhancement layer at
all in SP-MIMO, but they can in HM-STBC when the channel condition permits it.
Third, in order to decode the enhancement layer, two-antenna users in SP-MIMO
need much higher CSNRs than in HM-STBC.
When comparing our system with HM-STBC and SP-MIMO, an immediate ob-
servation is that the two digital systems only obtain stair-shaped incremental in-
creases in quality while our system has a very smooth quality scaling behavior. Fur-
thermore, in our system, two-antenna users always derive higher PSNRs over the
single-antenna users when the per-antenna CSNR is the same. In HM-STBC and

18.8 Simulation Evaluation
415
0
5
10
15
20
25
30
20
22
24
26
28
30
32
34
36
38
40
CSNR (dB)
PSNR (dB)
 
 
HM−STBC, Hierarchical 4/16QAM
SP−MIMO, Hierarchical 4/8QAM
HM−STBC, Hierarchical 4/64QAM
SP−MIMO, Hierarchical 4/16QAM
New proposed
(a) Single-Antenna Users
0
5
10
15
20
25
30
20
22
24
26
28
30
32
34
36
38
40
CSNR (dB)
PSNR (dB)
 
 
HM−STBC, Hierarchical 4/16QAM
SP−MIMO, Hierarchical 4/8QAM
HM−STBC, Hierarchical 4/64QAM
SP−MIMO, Hierarchical 4/16QAM
New proposed
(b) Two-Antenna Users
Figure 18.9 Our system versus two conventional reference systems, HM-STBC and SP-MIMO,
using the image Lena.
SP-MIMO, however, the additional antenna is underutilized in most cases, be-
cause the system performance stops improving when the CSNR exceeds a threshold
(known as the level-off effect of digital systems).
Between our system and SP-MIMO, we ﬁnd that both single-antenna and two-
antenna users in our system can derive higher PSNRs at most CSNRs. The exceptions
are when the CSNR is just high enough to allow the decoding of the base layer data,
for example, at 6 dB for single-antenna users and 4 dB for two-antenna users.
Between our proposed system and HM-STBC, our system always performs better
for two-antenna users at CSNRs of 8 dB or higher. At a CSNR of 26 dB, the PSNR
from our system is more than 3 dB higher than that for the QPSK-in-16-QAM variant
of HM-STBC, and about 7 dB higher than the QPSK-in-64-QAM variant of HM-
STBC. For single-antenna receivers, our system is superior over HM-STBC at low
CSNRs. However, when the channel is good enough to allow single-antenna users
to correctly receive enhancement layer bits, our system incurs some performance
loss. This is the cost our system has to pay in order to trade-off the performance

416
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
Figure 18.10 Visual quality comparison of decoded Cameraman at CSNR = 12 dB. The images
in the top and bottom rows show the perceived quality by the single-antenna (labeled 2 × 1) and
two-antenna users (labeled 2 × 2).
between single-antenna and two-antenna users. However, although HM-STBC with
hierarchical 4/64-QAM can achieve high PSNR at high CSNR, the single-antenna
users can barely get anything at a CSNR below 15 dB. In contrast, our system has a
signiﬁcantly larger operational range.
Figures 18.10 and 18.11 show the visual quality of Cameraman and Lena, re-
spectively. Our scheme consistently produces better visual quality than SoftCast,
especially for single-antenna users. Compared to HM-STBC, although our system
delivers images that appear less smooth in some places, the image details are better
preserved.
18.8.2.3 Overall Performance in a Broadcasting Session
We compare the overall performance of our system with two reference systems,
namely SoftCast and HM-STBC, in a broadcasting session. We assume 100 single-
antenna users and 100 two-antenna users in the session, with average per-antenna
SNR 15 dB, and variance 4 dB. Table 18.2 lists the average received PSNR for eight
test images. In the table, J2K denotes the digital system, which encodes images with
the JPEG 2000 standard and transmits them using HM-STBC scheme. We do not
show the results for SP-MIMO, because previous experiments have shown that it
signiﬁcantly underperforms our system.
From the last row of the table, we can see that our system achieves the highest
PSNR among the three systems for both types of receivers. On average, our system

18.8 Simulation Evaluation
417
Figure 18.11 Visual quality comparison of decoded Lena at CSNR = 14 dB.
Table 18.2 Comparison between our system, SoftCast, and J2K.
Single-Antenna Users
Two-Antenna Users
Ours
SoftCast
J2K
Ours
SoftCast
J2K
Cameraman 27.73
23.76
27.14
31.58
30.13
29.56
Lena
28.76
25.77
28.55
32.77
32.18
30.08
Peppers
26.16
22.80
25.94
29.73
29.30
28.49
Boat
29.42
28.02
28.39
33.32
34.13
31.26
Couple
30.26
26.25
30.86
33.32
32.22
33.23
Girl
30.45
27.17
31.35
33.62
33.39
33.16
House
32.41
29.86
33.24
35.91
36.32
34.45
Tree
26.48
24.72
25.76
30.23
31.07
28.01
Average
28.96
26.04
28.90
32.56
32.34
31.03
obtains 2.92 dB gain over SoftCast for single-antenna users, and 1.53 dB gain over
HM-STBC for two-antenna users. As more advanced techniques on CS reconstruc-
tion for images are proposed, the compressive image broadcast system is expected
to achieve even better performance. For example, it is reported in a recent body of
work [476] that the reconstruction PSNR for Lena (256 × 256) could be as high as
29.16 dB and 33.58 dB when the number of available CS measurements is only 15%
and 30% of the original pixel numbers. This suggests great potential for the proposed
system.

418
18 MIMO Broadcasting with Receiver Antenna Heterogeneity
18.9 Summary
In this chapter we have presented a compressive image broadcast system to simulta-
neously address heterogeneity across receiving antenna numbers and diverse channel
SNRs. By integrating compressive sensing into the MIMO transmission of multime-
dia data, our framework ensures decoding for single-antenna users while two-antenna
users can achieve multiplexing gain. By designing a suitable power allocation strat-
egy, we can reach a performance trade-off between single-antenna and two-antenna
users in a ﬂexible way. Simulation results demonstrate the advantages of our sys-
tem when compared with an analog system extended from SoftCast and two other
conventional digital systems.
Our system is likely more beneﬁcial for systems with more antennas. A digital
approach would be even more constrained in such a scenario, since we would need
to divide the source into more layers. Hybrid multiplexing and STBC schemes would
also be limited by the inherent difﬁculty of designing efﬁcient STBC codes for large
channel dimensions. The LLSE decoder in SoftCast would suffer further if the num-
ber of received measurements becomes even smaller, for example, for a 3 × 1 or
4 × 1 scenario. In comparison, the main components in our system naturally cope
with higher channel dimensions, merely using slightly different parameter choices
for two possible reasons. First, the most suitable power scaling factor may be dif-
ferent. Second, it is likely that the system should send more measurements than are
needed by the user with the most antennas. This would trade-off more multiplexing
gain for diversity gain and help the single-antenna users.
Finally, although this work focuses on image transmission, we can treat images as
intra-coded video frames without considering temporal correlation among successive
frames. Our system can be extended to video broadcasting by using a 3D wavelet
transform, which requires consideration of the trade-off between system performance
and the decoding complexity.

Part VII
Future Work
This book starts from information theory. I would like to end by discussing the de-
velopments of information theory in future. Although most of our research presented
in this book is technical and solution-oriented, the fundamental problems behind
them are closely related to information theory. Shannon’s information theory started
from the end-to-end telegraph and has been developed for more than 60 years. With
the advanced compression and communication technologies presented in this book, it
is a right time to discuss the possible theoretical breakthrough in information theory.
The contents presented in this part are our thoughts that we are strenuously working
on or plan to do soon. I hope that we can present some solid progress on theoretical
research in the near future.


Chapter 19
Computational Information Theory
19.1 Introduction
Since Shannon published his seminal paper “A Mathematical Theory of Communica-
tion” in 1948 [4], information theory has developed for more than 60 years. In Shan-
non’s time, the communications system was a simple end-to-end telegraph. There
were two fundamental problems to be solved. The ﬁrst one was entropy-approached
source coding. This problem was solved early on with arithmetic coding. The sec-
ond problem was to study how to complete transmissions approaching the capacity
of a channel, especially an end-to-end channel. When Turbo code was invented in
1993 [18,19] and LDPC code was re-invented in 1996 [17], the problem of capacity-
approached channel codes was also solved.
With the later emergence of networks, a natural extension of Shannon’s infor-
mation theory was toward more complicated channel models, with multiple senders
and receivers in a network. This is called network information theory [477]. How-
ever, once channel models are extended beyond the end-to-end structure, it is hard
to derive the close-form channel capacity. Theoretical results can be derived only in
simple network channel models, for example, broadcast channels, multiple access
channels, and 2-to-2 channels. For generic network channel models, only a max-
ﬂow min-cut theorem is available. Recently, there has been little progress in network
information theory.
Our research into compression and communications has motivated us to con-
sider another extension of Shannon’s information theory. In Chapter 11, we study
the SIFT-based image compression. The central question of this work is how to use
the correlation of external images included in a large-scale database. Our scheme is
totally different from existing schemes for image and video compression. In Chap-
ter 17, we study the use of internal correlations of a video for channel denoising in
the physical layer of wireless networks. One extension of this research is to use the
correlation of external images in a large-scale database for channel denoising. Both
of these areas of research point to new opportunities for information theory when a
large amount of data is available.
421

422
19 Computational Information Theory
Therefore, the extension we are currently considering is new source models. We
call them Cloud sources. When you compress and transmit a source, we assume
that the channel and receiver have a large-scale database available. Shannon’s infor-
mation theory already studied correlated sources. For a pair of correlated sources,
joint source coding can improve compression efﬁciency. When a correlated source
is available at the receiver, distributed source coding (DSC) can use this correlation
at the receiver as efﬁciently as it is used at the sender. Conditional entropy is used to
study these cases. Cloud sources are signiﬁcantly different from previous correlated
sources and thus will require corresponding source coding and channel coding, as
well as joint source and channel coding.
The fundamental difference is that in Cloud sources you do not know which ones
are correlated to the input source and how they are correlated. An important assump-
tion in Cloud sources is that, for any input source, you can ﬁnd a few correlated
sources from a large-scale database with a probability close to one. At the same
time, most sources in the large-scale database are not correlated to the input source.
For example, Facebook has 220 billion photos [1]. When you upload one photo to
Facebook, most likely you can ﬁnd some correlated ones from among the 220 billion
photos. But the percentage of photos correlated to the uploaded photo in the database
is small and almost close to zero. In other words, most photos are not correlated to the
uploaded photo. Before using the correlation of Cloud sources for compression and
transmission, you have to identify the correlated ones and estimate their correlation.
According to Shannon’s separate source and channel coding, an input source is
usually compressed. However, once the source is fully compressed, compressed data
looks like random bits and cannot be used to identify which ones are correlated in
the Cloud sources. In order to ﬁnd the correlated sources, the input source cannot
be fully compressed and some redundancy has to be kept or introduced. If the exist-
ing redundancy can be further utilized during transmission, the system performance
is still optimal. Therefore, for using the correlation of Cloud sources, a distinguish-
ing feature is that the input source cannot be fully compressed and joint source and
channel coding is needed. The existing redundancy can bring the additional beneﬁt
of ﬁnding the correlated sources in the Cloud sources. If the correlated sources can
be used in source and channel coding, the system performance will most likely be
much better without external correlation. Since the source model needs a computa-
tion to ﬁnd the correlated sources, we call this extension computational information
theory.
19.2 Cloud Sources
In the computational information theory, an important concept is Cloud sources. Let
us deﬁne them as C, which contains a large number of sources C = {c(1),c(2),...,
c(N)}. N is a huge number. As we have mentioned, Cloud sources have two proper-
ties.

19.2 Cloud Sources
423
Deﬁnition 11 (Correlation Property) For any input source s, the probability of
ﬁnding a few correlated sources in the Cloud sources is close to one, namely,
P{Cs = φ} = ε. Cs is a set of sources correlated to the input source s. φ is an empty
set and ε is a small number close to zero.
How is the correlated set Cs selected? It depends on the correlation deﬁnition. We
can still use the conditional entropy to characterize the correlation. For example, if
cs(i) is a correlated source in Cs, the conditional entropy is H(s|cs(i)) ≤kH(s). k
is a factor less than 1. The ﬁrst property indicates that there is always a correlation
between any input source s and Cloud sources C. Obviously, Cloud sources are not
traditional sources studied in information theory. They are the hottest concept in Big
Data, which contains almost all the sources that you will probably need.
Deﬁnition 12 (Diversity Property) For any input source s, Cs is the set of sources
correlated to s in Cloud sources C. |Cs|/|C| is close to zero.
|·| is the number of elements in a set. This property actually poses a large diversity
constraint in Cloud sourcesC. If all sources in the Cloud sources are similar or belong
to one category, the problem may become easy because you can arbitrarily take some
of them for source and channel coding. However, because only a very small set in C
is correlated to input source s, it is important to keep or introduce some redundancy
in s so as to accurately ﬁnd the correlated sources Cs in C.
Although Facebook has been mentioned in this book as an example of Cloud
sources, strictly speaking, only the collection of large-scale data is hard to say that
they are Cloud sources. The collected data has to be analyzed and reorganized as
structured data before it can serve as Cloud sources. First, the collection of data
should be classiﬁed into different categories according to their statistics. As shown
in Figure 19.1, images are classiﬁed according to the layout and histogram of pixel
values. But such a simple classiﬁcation is not enough for efﬁciently exploiting the
correlation in Cloud sources.
From our experiences, there are at least four types of metadata needed from low
level to high level to make use of Cloud sources efﬁciently. According to our research
presented in Chapters 11 and 12, local descriptors, such as SIFT, SURF, and CHoG
are the ﬁrst type of important metadata for an individual source in Cloud sources. An
example of local SIFT descriptors is shown in Figure 19.2a. This type of metadata
is used to identify the correlated sources and estimate the geometric transformations
among the correlated sources. They are much better than the global transformation
between two correlated sources because the corresponding SIFT descriptors actually
indicate the matched regions of correlated sources. It allows multiple transformations
between two correlated sources. Furthermore, they are scale-invariant and rotation-
invariant, and less sensitive to illumination.
More importantly, the second type of metadata is the correlation of sources in
Cloud sources. Since the number of sources is huge, it is an intensive computation
to analyze their correlations. They have to be computed off-line and stored as part of
the Cloud sources. According to our research presented in Chapter 12, the analysis of
the correlation of the sources in Cloud sources can build many relationship graphs,

424
19 Computational Information Theory
Figure 19.1 Part of the images in Cloud sources.
as shown Figure 19.2b. Every graph can be further simpliﬁed as a tree and, according
to the tree, individual sources can be reorganized as a source sequence. Such models
are still too simple to characterize local matching regions among sources. Further-
more, since Cloud sources are dynamic, it is necessary to study how to dynamically
modify the tree models to achieve a better trade-off between computational cost and
modeling accuracy.
Further extending our research presented in Chapter 12, the more accurate de-
scription of the correlation among sources should be scene information. It may be
difﬁcult for generic content to reconstruct scenes. But with modern technical devel-
opments, it has become feasible for some constrained scenarios. For some popular
locations (e.g., tourist sites), as shown in Figure 19.3a, a large number of collected
images and videos are available on the Internet and the technology of structure from
motion can be used to reconstruct the 3D scenes from the collected data [478,479],
as shown in Figure 19.3b. For surveillance video, it is also possible to reconstruct
3D scenes using this technology because the scenes are constrained. Indoors, Mi-
crosoft Kinect data, including both depth and RGB data, can be used to reconstruct
3D scenes [480]. With the scene description, it is possible to code every object in
the scene with the correct geometric transformation. Every object can ﬁnd correlated
external objects instead of correlated external sources.

19.2 Cloud Sources
425
(a)
(b)
Figure 19.2 Metadata in Cloud sources.
(a)
(b)
Figure 19.3 Structure from motion.
In addition to the above three types of metadata, the highest level of metadata in
Cloud sources is still semantic. Although there is a gap when getting the semantics
from media content, some technologies have been developed to propagate human an-
notations on individual sources to a mass of correlated sources. From this viewpoint,
it is easier to solve the semantic problem in Cloud sources than a single source. With
the annotated Cloud sources, the semantic-based coding of input sources is becoming
possible.

426
19 Computational Information Theory
19.3 Source Coding
In this section, we ﬁrst discuss how to efﬁciently compress the metadata. After that,
we discuss the coding of Cloud image sources and Cloud video sources, respectively.
One more challenging problem is how to compress an input source by using Cloud
sources at the receiver. This is distributed cloud-based coding.
19.3.1 Coding of Metadata
In Shannon’s information theory, the coding of metadata is studied relatively less
than other areas, probably because metadata is easy to compress. However, in com-
putational information theory, there is rich metadata that has to be generated ofﬂine
and stored with Cloud sources, including local feature descriptors, source correlation,
scene descriptors, and so on. Therefore, it is important to study how to efﬁciently
compress various types of metadata in computational information theory.
The compression of local feature descriptors has been surveyed in Chapter 11,
Section 11.2.2. It has been a popular research topic in the past decade. There are
several different types of technologies that have been developed, including the di-
mension reduction of feature vectors, the quantization of feature vectors, the design
of low-dimension feature vectors, and the compression of feature vectors via image
compression. These technologies are often evaluated according to the accuracy of
data retrieval, namely, the curve of the recall rate versus the compressed size of fea-
ture vectors. The recall rate is a reasonable measurement for data retrieval but not
for source coding. One local feature vector represents a block of source data. The di-
mension reduction and quantization can be viewed as a kind of vector quantization to
feature vectors. Instead of evaluating the distortion of local feature vectors, it would
be better to evaluate the distortion of sources caused by the compression of feature
vectors. The source distortion may be caused by source mismatch but most likely by
the decrease of matched accuracy.
Therefore, a key research problem in the coding of local descriptors is how to
measure the distortion of source data through the distortion of feature vectors. It de-
pends on the purpose of using the feature vectors. When local descriptors are used
to analyze the correlation of sources in Cloud sources, the distance of sources within
one cluster and the distance of sources between clusters is a kind of distortion mea-
surement. When local descriptors are used to retrieve external images for reconstruct-
ing one source, as we have done in Chapter 11, the reconstruction quality is another
kind of distortion measurement. Once the distortion is deﬁned, the corresponding
rate of distortion optimization should be studied via the approaches for analyzing
vector quantization.
As we have discussed in Chapter 12, the correlation of sources in Cloud sources is
described by graphs and trees. The graphs and trees should be compressed losslessly.
Our current work only studies the correlation of entire sources. This is not precise

19.3 Source Coding
427
enough to characterize the correlation in Cloud sources. As a matter of fact, matched
local descriptors represent corresponding local regions in two sources. In addition
to the correlation of sources by weighting all matched regions, it is expected to de-
velop the technology representing the correlation of local regions in Cloud sources.
If so, one source is included in multiple graphs and trees of correlated regions. Obvi-
ously, the correlation will become considerably complicated. In addition, since Cloud
sources are dynamic, many sources are continuously added and some sources may
be deleted. The graphs and trees of correlated regions should be easy to adjust with
a low computational cost.
The scene descriptors are depth images if they are reconstructed from Microsoft
Kinect data. The compression of depth images has been developed based on the tech-
nologies of image and video coding [481,482]. The scene descriptors are a 3D point
cloud if they are reconstructed by the structure from motion approach. In general, a
3D point cloud consists of some discrete feature points in the scene. These points
may not be precisely measured in the scene. Several approaches have been proposed
to compress the point cloud of 3D objects [483,484]. But the scene descriptors have a
much larger scale than 3D objects. More importantly, in contrast to the compression
of 3D objects, the compression of scene descriptors is not used for reconstructing
scenes. Instead, it provides us 3D information at the region level. The regularization
should be allowed before compression.
The above methods for the compression of scenes are still at the signal level. The
more compact description of scenes should be object-based and even semantic. If so,
the compression problem becomes simple. But it is still difﬁcult to accurately extract
objects, especially with semantics. It is outside the scope of information theory.
19.3.2 Coding of Cloud Image Sources
In Shannon’s information theory, source coding is mainly studied for communica-
tion. Storage is seldom considered as an independent research problem. However, in
the Cloud era, compression for large-scale storage is becoming more and more im-
portant. For example, Facebook has stored 220 billion images in its data center and
this amount is continuously increasing at several hundred million every day. Cur-
rently, every image is stored as a JPEG ﬁle independently. Theoretical total size of
independently compressing Cloud sources C is equal to
RI
C =
N
∑
i=1
H(c(i)).
(19.1)
The superscript I means independent compression. At least 1 million 1T hard disks
would be necessary for the images in Facebook. This obviously creates a huge cost
in power supply and air conditioning. Every hard disk has to be replaced every three
years in order to make the storage safe.

428
19 Computational Information Theory
As with the properties discussed in Section 19.2, every source in the Cloud
sources has a few correlated ones, but overall the sources display a huge diversity.
According to the correlation property of Cloud sources, the most efﬁcient way to
compress them is jointly. The theoretical size of joint compression is
RJ
C = H(c(1),c(2),...,c(N)),
{c(1),c(2),...,c(N)} ∈C.
(19.2)
The superscript J means joint compression. However, N is huge and it is impracti-
cal to jointly compress all the sources in Cloud sources because of random access
requirements and computational costs. According to the diversity property, it is not
necessary because there is no gain for jointly compressing non-correlated sources. A
feasible method would be to jointly compress correlated sources only.
RC
C = RJ
C1 +RJ
C2 +···+RJ
CK,
C = C1 +C2 +···+CK.
(19.3)
The superscript C means correlated joint compression. C1, C2, ..., CK are the set of
correlated sources. Note that RC
C −→RJ
C.
In Chapter 12, we proposed analyzing the correlation of sources in the large-
scale storage and organizing the sources as many trees. The correlated sources are
described in the same tree, where the relationship among parents and children char-
acterizes their correlation. Instead of compressing them one by one, we proposed
compressing sources, described in the same tree, as a pseudo sequence. Our experi-
mental results show that the compressed size of a pseudo sequence is only one-tenth
of the size of independent compression. It indicates that using the special source
coding designed for Cloud sources can save 90% of storage in the data center. An
additional cost is that, when one source is accessed, multiple sources may need to be
decoded. If the depth of a tree is constrained, such a computational cost should be
able to be offered by the data center.
Our research presented in Chapter 12 demonstrates the technical feasibility and
the advantages of coding Cloud sources jointly. However, the trees are built by an-
alyzing the correlation of sources using local descriptors. It should not be optimal
from the viewpoint of information theory. To simplify the computation of building
graphs, it is ﬁne to use correlation analysis to establish initial graphs. It is better to
introduce the rate distortion optimization in the subsequent processing.
1. Building graphs — Some sources are included in multiple graphs. RDO should
be used to evaluate in which graph the sources should be and whether the graphs
should be merged together.
2. Conversion from a graph to a tree — When a graph is converted to a tree for
the generation of a pseudo sequence, the rate distortion cost should be evaluated
too. It may greatly beneﬁt the trees with many sources and strong correlation.
For a given model of Cloud sources, the rate distortion theory should be studied
to guide how to efﬁciently form different clusters of sources and how to efﬁciently
compress them.
If correlated local regions are considered in joint compression instead of corre-
lated sources, one source may be included in different correlated sets. For example,

19.3 Source Coding
429
one source contains two object regions. It can be categorized into the sets of two
different objects. Obviously, it will cause repeated compression of the source. In
particular, when the number of sources in Cloud sources is huge, it may become im-
possible to ﬁnd the optimal classiﬁcation by using the correlated local regions. The
graph building is a much tougher problem than what we considered in Chapter 12.
An interesting but fundamental theoretical problem for the coding of Cloud
sources is whether the compression size RJ
C will converge when the number of
sources in Cloud sources is huge. If this is the case, when more sources are added
to the Cloud sources, the compressed size does not increase clearly because all the
information carried by the added sources has been included in the Cloud sources. We
only need to store the descriptions of how to use the Cloud sources to reconstruct the
added sources. After that, the added sources can be thrown away. Although it is dif-
ﬁcult to prove mathematically, it is not difﬁcult to understand that the Cloud sources
already contain all the information in the world. It should be reachable for the speci-
ﬁed content. Google street images should help with this if the company captures the
images of all streets in the world. Based on this conjecture, the cost of Cloud storage
is a constant no matter how much data is uploaded by users.
19.3.3 Coding of Cloud Video Sources
The coding of Cloud video sources can be distinguished from the coding of Cloud
image sources in that the frames in a video already present strong correlation. Is it
also necessary to use the correlation of external videos in the Cloud? It seems rea-
sonable to answer this question with a no. In particular, with the developments of the
last two decades, the technologies to use the correlations between frames are mature
and have a very good performance. In the HEVC standard, after motion compen-
sation from a correlated frame, the energy of residual signals is small with random
distribution. It requires few bits to compress. However, we also observe that the
intra-coded data is a big part of HEVC. It is mainly caused by scene changes and
covered/uncovered regions. Obviously, externally correlated videos can help greatly
reduce this part of the data.
However, it is not easy to use externally correlated videos to compress a video
in the existing hybrid coding framework. Here we present our idea for Cloud video
storage and video surveillance. In state-of-the-art video coding standards, at most
four references are allowed. It is limited by memory cost and computational cost
for motion compensation. However, in Cloud video storage and video surveillance,
we propose increasing the number of references to millions or even more. Then, for
any video, almost all of the frames can be coded by prediction from the references
because most of the content in the video exists in the references. The references are
actually Cloud sources discussed in this chapter. Although this idea sounds interest-
ing and promising, there are several tough technical problems to solve.
First, where are the large number of references stored? Obviously, it is impos-
sible to store them in the memory. Instead, they should be stored in hard disks as

430
19 Computational Information Theory
compressed data. In other words, every reference is compressed as intra-coded data
and stored in the storage. Once it is selected as the reference for compressing a frame,
it will be loaded into the memory and decoded immediately. In one data center, there
are many CPU cores available and the computation for decoding an intra-coded ref-
erence is not high even for HEVC. It should not be a problem to simultaneously
decode hundreds of references in a short time. In addition, the references should be
compressed with a small quantization step and thus will be of high quality. Although
frames in a video present strong correlation, they are compressed as inter frames and
the quantization step is not small. In this case, the external references still have a
better chance of being selected as a reference than as coded frames in the video.
Second, how are the external references selected for compressing a video? Obvi-
ously, it is impossible to select one external reference from a large number of can-
didates by rate-distortion optimization. Similar to our work on compressing Cloud
image sources, we extract local feature descriptors from every reference and store
them as metadata. For a frame to code, we also extract its local feature descriptors.
Therefore, similar to content-based image retrieval, several external references are
ﬁrst selected by matching local feature descriptors. The matching process can be
done in a very short time even for millions of references. The selected external refer-
ences and their internal references are further selected by rate-distortion optimization
to compress every block.
Third, which frames should be selected as external references? Obviously, they
should be high-resolution and high-quality frames. The most challenging problem
in this process is how to select them according to its content and the content of
the selected references. One approach is similar to the correlation analysis by using
local feature descriptors in Chapter 12. When a frame is not correlated to any stored
references, it should be compressed as an intra frame with a small quantization size
and marked as a new external reference. Once the frame is selected as an external
reference, it cannot be deleted even when the video is deleted; otherwise some videos
using the frame as a reference cannot be decoded correctly. In the beginning, the
number of references will increase rapidly. We hope that the increase is not linear
and will become slower and slower with the increasing number of references.
19.3.4 Distributed Coding Using Cloud Sources
If an input source is compressed by conventional source coding, it cannot utilize the
correlation of Cloud sources at all because all the information has been included in
the compressed data. The theoretical coding rate should be H(s). The input source
can only be reconstructed by decoding the compressed data. Therefore, this approach
cannot beneﬁt from the correlation of Cloud sources. One solution for using Cloud
sources is the scheme we presented in Chapter 11. We describe images by local fea-
ture descriptors and thumbnails. The input images can be reconstructed from Cloud
sources via the compressed descriptions. The signiﬁcance of this work is to demon-
strate that high-quality images can be reconstructed in this way. However, from the

19.3 Source Coding
431
Source 
Encoder
(a)
Source
Decoder
s
y
ŝ 
Source
Encoder
(b)
Source
Decoder
s
C={ c(1),c(2),...,c(N) }
ŝ 
Figure 19.4 Distributed source coding without/with Cloud sources.
viewpoint of Shannon’s information theory, it does not look like source coding. Fur-
thermore, it cannot achieve a good reconstruction under the MSE criteria.
Because Cloud sources are assumed to be available at the receiver, one more rea-
sonable solution should be distributed source coding (DSC). In conventional DSC, as
shown in Figure 19.4a, a correlated source y, called side information, has been iden-
tiﬁed at the receiver. Input source s is ﬁrst coset quantized and then is compressed
by channel codes (e.g., Turbo code and LDPC code). The theoretical coding rate
should be H(s | y), which is less than H(s). The compressed data is used to correct
the correlated source y toward the input source s at receiver. When Cloud sources
are available at the receiver, as shown in Figure 19.4b, the side information is not
yet identiﬁed. Furthermore, since the number of sources is huge and most of them
are not correlated to input source s, the distributed coding rate H(s|C), directly using
C, should be close to H(s) because the compressed data by channel codes cannot be
used to identify which of the ones in the Cloud sources are correlated to s.
To identify the correlated sources, similar to the scheme presented in Chapter 11,
local feature descriptors of the input source s have to be sent to the receiver. Let us
assume that the correlated sources Cs can be found from C with the received local
feature descriptors. In general, any one of the correlated sources should not be the
best side information that we can get. With our approach presented in Chapter 11,
the best side information can be generated for combining the most matched parts of
all the correlated sources. The distributed coding rate in this case should be H(s |
Cs). y, used in the conventional DSC, can be viewed as one correlated source in Cs.
Therefore, H(s | Cs) is less than H(s | y). This is the advantage of Cloud sources. A
key technical problem here is how to generate accurate side information. Since the
number of combinations of correlated patches is big, the generation should be guided
or checked by received parity data.
Theoretically, the above scheme should not be optimal because local feature de-
scriptors are only used for data retrieval. From the viewpoint of compression, they
are overhead. As we have discussed in Chapter 11, SIFT feature vectors are extracted
in the feature regions of input source s. Every SIFT vector is a 128-dimension his-
togram of the gradient in different directions. Can we slightly change the generation
of feature vectors and make them the linear combinations of some samples in the
feature regions? If so, the feature vectors are generated by arithmetical calculations
on samples. The channel coded data for reconstruction is the result of logical cal-
culations on samples. Similar to our work in compressive modulation presented in

432
19 Computational Information Theory
Chapter 14, they can be represented as the mixed logical and arithmetical nodes on
the left side of a bipartite graph for joint reconstruction of the source nodes on the
right side. A key technical problem here is how to design the modiﬁed feature vectors
that are still accurate at identifying correlated sources.
Another alternative solution is to remove the syndrome coding from DSC. At the
same time, channel coding in the traditional digital transmission is also removed.
Quantized data is directly transmitted by the pseudo-analog approach presented in
Chapter 16. If so, the quantized data remains part of the source redundancy and the
transmission does not remove the redundancy. Therefore, with the remaining redun-
dancy, the received quantized data can be used to retrieve the correlated sources. But
the challenge here is whether the quantized data can be used for correlated source
retrieval as efﬁciently as local feature descriptors because it has been quantized.
19.4 Channel Coding
As we have discussed, in order to use the correlation of Cloud sources, input sources
cannot be fully compressed. For uncompressed or partially compressed sources, cur-
rent transmission approaches are inefﬁcient because they cannot use the remaining
source redundancy. Furthermore, channel coding introduces new redundancy. It will
further degrade the transmission performance. The pseudo-analog transmission pre-
sented in Chapters 16 and 17 should be one promising solution. In addition to the
power allocation and channel denoising that we have discussed, currently we do not
have a channel protection approach in the pseudo-analog transmission, such as chan-
nel coding in the current digital transmission.
Before we start to discuss detailed technologies to reduce channel errors, we
would like to deﬁne the pseudo-analog transmission more strictly. As previously
mentioned, pseudo-analog transmission is still implemented in a digital manner and
transmitted data is also digital. But compared with current digital transmission, it
removes quantization, entropy coding, and channel coding. All of these are non-
linear processes in the current digital transmission, while the other processes are
linear. The strict deﬁnition for the pseudo-analog transmission should be linear dig-
ital transmission. These two deﬁnitions indicate the same transmission approach but
from different viewpoints. They can be used interchangeably hereafter.
19.4.1 Power Allocation and Bandwidth Matching
Power allocation and bandwidth matching are two basic components in linear dig-
ital transmission. As discussed in Chapter 16, transmission power is directly allo-
cated to elements of the source data. For a given total transmission power, the power
allocation should take source properties, channel models and conditions, and even
receiving antennae into account. In Chapter 16, an algorithm has been presented to

19.4 Channel Coding
433
allocate the power to every element of source data to minimize the mean square
error of received data. This is a way to take source redundancy into account in the
transmission because the power is actually allocated according to source distribution.
Next, the channel model and conditions should be considered in power allocation. In
particular, a Gaussian channel and fading channel deﬁnitely need different strategies
for power allocation. Furthermore, since more and more devices are equipped with
multiple antennae, this factor should also be considered in power allocation.
In the linear digital transmission, one element of the source data uses one channel
transmission. If the number of elements needed to transmit in a unit of time (source
bandwidth) is equal to the number of channel transmissions in a unit of time (channel
bandwidth), we call this matched source and channel bandwidth in the linear digi-
tal transmission. But in real applications, source bandwidth seldom matches channel
bandwidth. When source bandwidth is larger than channel bandwidth, source ele-
ments have to be reduced and this is called bandwidth compression. If source band-
width is less than channel bandwidth, source elements have to be increased and this is
called bandwidth expansion. Currently, bandwidth compression is achieved by drop-
ping some of the less important elements, while bandwidth expansion is achieved by
repeating some of the important elements.
Obviously, modifying the number of elements for source and channel bandwidth
matching is not optimal. For bandwidth compression, the elements of source data
should have more than two choices (transmitted or not transmitted). In the quantiza-
tion adopted in nonlinear digital transmission, less important elements are reduced
in size before transmission. It means that some of the less important elements are
still transmitted and not dropped totally. Similarly, for bandwidth expansion, most
of the likely important elements may need to be transmitted by non-integer channel
usages. One idea for increasing and decreasing the channel usage of every element at
fractional times is to introduce network coding. For example, let us assume two ele-
ments s(1) and s(2). If we transmit s(1), s(2), and s(1)+s(2), every element has 1.5
channel usages. If we transmit s(1) and s(1)+s(2), the element s(1) has 1.5 channel
usages and s(2) has 0.5 channel usages. It is desirable to develop optimal network
coding for source and channel bandwidth matching.
In the nonlinear digital transmission, the joint optimization of power allocation
and bandwidth matching is achieved by separate source coding and channel cod-
ing. The source data is ﬁrst compressed toward the given channel bandwidth and
the resulting data is compressed bits. In general, complicated elements require more
bits to describe and easy elements use few bits. It is equal to allocate bits to differ-
ent elements. After the bit allocation, all the resulting bits are equally important in
transmission and thus channel coding protects them by uniformly introducing parity
bits. Finally, transmission power is averagely allocated to transmitted bits. However,
entropy coding and channel coding have been removed in the linear digital transmis-
sion. We have to ﬁnd a new way to achieve joint optimization in the linear digital
transmission.
Power and channel bandwidth are two different resources for transmission. We
should study the joint allocation of transmission power and channel bandwidth to
the source elements. For an element s(1), we can select to transmit it twice or

434
19 Computational Information Theory
increase its transmission power with one channel usage. The fundamental theoretical
problem in the joint optimization is determining the relationship between transmis-
sion power and channel usage. With a formulated relationship, we can alternatively
choose increasing transmission power or channel usages for important elements or
decreasing transmission power or channel usages for less important elements. This
is a new research topic that has appeared in linear digital transmission.
Next, a wild thought presented here is that source and channel bandwidth match-
ing can use Cloud sources as well. From the viewpoint of signal processing, band-
width compression and expansion are similar to down-sampling and up-sampling
one source, respectively. But the down-sampling is not a reversible process. In band-
width compression, once the source is down-sampled in order to match channel
bandwidth, the receiver cannot get the source at the original resolution anymore
because the down-sampling process drops a lot of the source detail. Our recent re-
search has shown that if Cloud sources are available and correlated sources can be
found, the up-sampled results can be signiﬁcantly improved [485]. In other words,
we should further study bandwidth compression and even bandwidth expansion with
the assumption of Cloud sources being available.
The number of Cloud sources is huge. But in the network nodes (e.g., router and
base station), it is difﬁcult to have enough storage for the Cloud sources. There-
fore, bandwidth compression and expansion using Cloud sources are impossible
in network nodes. One possible solution is to reduce the size of Cloud sources by
learning. For example, the hallucination technology in computer vision can be ap-
plied here [486, 487]. We can ﬁrst train a data library for one category of sources
in Cloud sources. The library includes many pairs of low-resolution patches and
matched high-resolution patches. Thus, the size is no longer too large and can be
stored on the network nodes. At a network node, one down-sampled patch can ﬁnd a
corresponding high-resolution patch for up-sampling the received source.
19.4.2 Multiple Level Channel Coding
Conventional channel coding such as LDPC codes and Turbo codes used in the non-
linear digital transmission mainly targets binary data. In linear digital transmission,
source data is transmitted element by element. The protection should be operated
on multilevel elements. Although conventional channel coding can be extended to
multilevel Galois ﬁelds GF(q) [488], they cannot fully beneﬁt from the redundancy
existing in the source data except for using the probability distribution. The elements
of the source data implicate complicated and rich redundant information. It would be
desirable to develop a new channel coding for linear digital transmission. It supports
taking multilevel data as input and can fully employ source redundancy as prior
information in channel decoding.
Random projection codes presented in Chapters 14 and 15 have shown that
they can be used for error protection and source compression in the physical layer
of wireless networks. This is a promising solution for channel coding in linear

19.4 Channel Coding
435
digital transmission. Although they currently only take binary data as input, they
can be readily extended to multilevel data and generate multilevel parity data with
the weighted sum of randomly selected source elements. If source elements are a
part of the output, the channel coding in the linear digital transmission is systematic;
otherwise it is non-systematic. The topic of how to design multilevel random projec-
tion codes with optimal degree and weighting parameters is interesting. In addition,
it may be better not to generate parity data by randomly selecting source elements.
The decoding algorithm cannot be based on the belief propagation presented in
Chapters 14 and 15 because input data is not binary. But we can use the algorithms
for the reconstruction in compressive sensing. One advantage of these algorithms is
that we can readily adopt the redundancy existing in source data as prior information.
Some regulations such as sparsity and low rank can be added in the optimization to
improve reconstructed quality. Furthermore, with more prior information, required
parity data can be reduced for error correction. Some results in compressive sensing
show that images can be reconstructed at high quality with only 20% of the measure-
ments. The challenge is the decoding complexity. The complexity of such algorithms
is very high, especially for both input data and weighted sums as multilevel values.
The source redundancy can be used to reduce the decoding complexity because the
solving space is reduced with prior information.
19.4.3 Channel Denoising
As shown in Figure 19.5, Cloud sources C are available in the channel decoder to
suppress and even remove channel noise. In channel decoding, input signal ˆy is equal
to y plus noise z. Both source encoding from s to x and channel encoding from x to
y make s become random bits. As we mentioned earlier, if input source s is fully
compressed and compressed data x cannot be used to identify the correlated sources
in the Cloud sources, it is difﬁcult for channel decoding to beneﬁt from the correla-
tion of Cloud sources. In the linear digital transmission, entropy coding and channel
coding have been removed. The transmission is similar to the pseudo-analog scheme
presented in Chapter 17.
If source coding and channel coding are similar to that presented in Chapter 17,
input source s is not compressed at all or is compressed partially. Therefore, y with
additive noise z has the potential to identify the correlated sources in Cloud sources.
We can directly extract local features from received data ˆy. Our initial results show
that when channel SNR is higher than a threshold (e.g., 5 dB), extracted local features
can still be used for correlated source retrieval. But it is important to study local
feature descriptors robust to channel noise and to further reduce the channel SNR
threshold. One solution could be to consider feature extraction with power allocation
in communication together. The power allocation provides an active way to protect
against channel noise.
Here, channel coding is neither traditional Turbo codes and LDPC codes nor
our random projection codes. Instead, it becomes a denoising process similar to the

436
19 Computational Information Theory
s
C={ c(1),c(2),...,c(N) }
ŝ 
z
Source
Encoder
Source
Decoder
Channel
Decoder
Channel
Encoder
x
y
ŷ
Figure 19.5 Channel denoising using Cloud sources.
approach presented in Chapter 17. Because channel noise is random, we can use the
existing correlation among pixels or coefﬁcients to suppress additive channel noise.
Our experimental results show that the received quality is even better than that of
the state-of-the-art digital systems. However, this scheme has revealed limitations
because it only uses the correlation within input source. If channel SNR is too low
(i.e., channel noise is too large), all received data contains large channel noise and it
will be difﬁcult to get good results by denoising.
As shown in Figure 19.5, when Cloud sources are available in the channel decod-
ing, using correlated sources without channel noise can be a good way to suppress
large additive channel noise. Once the correlated sources are identiﬁed, they can be
added in the algorithm of denoising (e.g., BM3D) and generate 3D correlated data
with the received noise image. Since the correlated sources do not contain channel
noise, the results of denoising should be much better than when using the received
noise data only. One technical problem is that only the received data contains noise.
After a 3D transform, the noise energy is not concentrated at a high frequency. More
intelligent noise removing approaches need to be studied.
19.5 Joint Source and Channel Coding
From the pseudo-analog schemes presented in Chapters 16 and 17, since the input
source is not fully compressed and still has some redundancy, the transmission has to
exploit this redundancy in order to make the system performance optimal. From the
viewpoint of Shannon’s rate-distortion theory, digital systems provide an extreme so-
lution, where the source is fully compressed, while analog systems provide the other
extreme, where the source is uncompressed. In computational information theory,
we try to achieve the optimal rate-distortion performance at the middle points of the
curve. Therefore, it is important to jointly design source and channel coding. The so-
lution presented in Chapters 16 and 17 allocates the transmission energy according
to the distribution of the transmitted source.
As shown in Figure 19.6, when Cloud sources are available in both the channel
and receiver, it is important to study how to efﬁciently use the correlation of Cloud
sources with joint source and channel coding. If the source coding is distributed
as discussed in Section 19.3.3 and the channel coding is denoised as discussed in

19.6 Summary
437
s
C={ c(1),c(2),...,c(N) }
ŝ 
z
Source
Encoder
Source
Decoder
Channel
Decoder
Channel
Encoder
x
y
ŷ
Figure 19.6 Joint source and channel coding with Cloud sources.
Section 19.4, we have to study how to efﬁciently use the correlation of Cloud sources
in source coding and channel coding.
19.6 Summary
This chapter presents our thoughts about the future developments of visual data com-
pression and communication. The core idea is to consider compression and trans-
mission with the available Cloud sources. Based on our current research, some new
schemes are discussed but more effort is needed to prove that they can be effective.
No matter what the performance of these new schemes is at the moment, we believe
that the computational information theory is a move in the right direction toward
developing Shannon’s information theory.


Appendix A
Our Published Journal and Conference Papers
Related to This Book
This appendix lists our conference and journal publications closely related to the
research content presented in this book. They are organized according to ﬁve parts.
A.1 Scalable Video Coding
1. Feng Wu, Shipeng Li, Ya-Qin Zhang, “A framework for efﬁcient progressive ﬁne
granular scalable video coding,” IEEE Transactions on Circuits and Systems for
Video Technology, special issue on streaming video, vol. 11, no. 3, pp. 332–344,
2001.
2. Lin Luo, Feng Wu, Shipeng Li, Zixiang Xiong, Zhenquan Zhuang, “Advanced
motion threading for 3D wavelet video coding,” Signal Processing: Image Com-
munication, special issue on subband/wavelet video coding, vol. 19, no. 7, pp.
601–616, 2004.
3. Ruiqin Xiong, Jizheng Xu, Feng Wu, Shipeng Li, “Barbell-lifting based 3-D
wavelet coding scheme,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 17, no. 9, pp. 1256–1269, 2007.
4. Feng Wu, Honghui Sun, Guobin Shen, Shipeng Li, Ya-Qin Zhang, Bruce Lin,
Ming-Chieh Li, “SMART: An efﬁcient, scalable and robust streaming video sys-
tem,” EURASIP Journal on Applied Signal Processing, special issue on Multi-
media over IP and Wireless Networks, vol. 2004, no. 2, pp. 192–206, 2004.
A.2 Directional Transforms
1. Wenpeng Ding, Feng Wu, Xiaolin Wu, Shipeng Li, Houqiang Li, “Adaptive di-
rectional lifting-based wavelet transform for image coding,” IEEE Transactions
on Image Processing, vol. 16, no. 2, pp. 416–427, 2007.
439

440
A Our Published Journal and Conference Papers Related to This Book
2. Hao Xu, Jizheng Xu, Feng Wu, “Lifting-based directional DCT-like transform
for image coding,” IEEE Transactions on Circuits and Systems for Video Tech-
nology, vol. 17, no. 10, pp. 1325–1335, 2007.
3. Jizheng Xu, Feng Wu, Jie Liang, Wenjun Zhang, “Directional lapped transforms
for image coding,” IEEE Transactions on Image Processing, vol. 19, no. 1, pp.
85–97, 2010.
4. Xiulian Peng, Jizheng Xu, Feng Wu, “Directional ﬁltering transform for image/
intra-frame compression,” IEEE Transactions on Image Processing, vol. 19, no.
11, pp. 2935–2946, 2010.
5. Jizheng Xu, Feng Wu, Wenjun Zhang, “Intra-predictive transforms for block-
based image coding,” IEEE Transactions on Signal Processing, vol. 57, no. 8,
pp. 3030–3040, 2009.
A.3 Vision-Based Compression
1. Dong Liu, Xiaoyan Sun, Feng Wu, Shipeng Li, Ya-qin Zhang, “Image compres-
sion with edge-based inpainting,” IEEE Transactions on Circuits and Systems
for Video Technology, vol. 17, no. 10, pp. 1273–1287, 2007.
2. Huanjing Yue, Xiaoyan Sun, Jingyu Yang, Feng Wu, “Cloud-based image cod-
ing for mobile devices — Toward thousands to one compression,” IEEE Trans-
action on Multimedia, vol. 15, no. 4, pp. 845–857, 2013.
3. Lican Dai, Huanjing Yue, Xiaoyan Sun, Feng Wu, “IMShare: Instantly sharing
your mobile landmark images by search-based reconstruction,” ACM Multime-
dia, pp. 579–588, 2012.
4. Zhongbo Shi, Xiaoyan Sun, Feng Wu, “Photo album compression by using local
features for cloud storage,” IEEE Journal of Emerging and Selected Topics in
Circuits and Systems, 2014.
A.4 Compressive Communication
1. Chong Luo, Feng Wu, Jun Sun, Changwen Chen, “Compressive data gathering
for large-scale wireless sensor networks,” International Conference on Mobile
Computing and Networking (MobiCom), pp. 145–156, 2009.
2. Chong Luo, Feng Wu, Jun Sun, Chang Wen Chen, “Efﬁcient measurement gen-
eration and pervasive sparsity for compressive data gathering,” IEEE Transac-
tions on Wireless Communication, vol. 9, no. 12, pp. 3728–3738, 2010.
3. Hao Cui, Chong Luo, Kun Tan, Feng Wu, Changwen Chen, “Seamless rate adap-
tation for wireless networking,” ACM International Conference on Modeling,
Analysis and Simulation of Wireless and Mobile Systems (MSWiM), pp. 437–
446, 2011.

A.5 Pseudo-Analog Transmission
441
4. Hao Cui, Chong Luo, Jun Wu, Chang Wen Chen, Feng Wu, “Compressive coded
modulation for seamless rate adaptation,” IEEE Transactions on Wireless Com-
munication, vol. 12, no. 10, pp. 4892–4904, 2013.
5. Min Wang, Jun Wu, Saifeng Shi, Chong Luo, Feng Wu, “Fast decoding and hard-
ware design for binary-input compressive sensing,” IEEE Journal of Emerging
and Selected Topic in Circuits and Systems, vol. 2, no. 3, pp. 591–603, 2012.
A.5 Pseudo-Analog Transmission
1. Xiaopeng Fan, Feng Wu, Debin Zhao, Oscar C. Au, “Distributed wireless vi-
sual communication with power distortion optimization,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 23, no. 6, pp. 1040–1053, 2013.
2. Hao Cui, Zhihai Song, Chong Luo, Ruiqin Xiong, Feng Wu, “Cactus: Keeping
redundancy for efﬁcient and robust wireless video communications,” ACM In-
ternational Conference on Modeling, Analysis and Simulation of Wireless and
Mobile Systems (MSWiM), pp. 273–278, 2013.
3. Xiaolin Liu, Wenjun Hu, Chong Luo, Feng Wu, “Compressive Image Broadcast-
ing in MIMO Systems with Receiver Antenna Heterogeneity,” Signal Process-
ing: Image Communication, 2014.
4. Feng Wu, Xiulian Peng, Ji-zheng Xu, “Linecast: Line-based distributed coding
and transmission for broadcasting satellite images,” IEEE Trasactions on Image
Processing, vol. 23, no. 3, pp. 1015–1027, 2014.


References
443
References
1. Gigaom. Facebook has 220 billion of your photos to put on ice, 2012. http://gigaom.com.
2. Cisco.
Cisco visual networking index: Forecast and methodology (2011–2016), 2012.
http://www.cisco.com.
3. Cisco. Cisco visual networking index: Global mobile data trafﬁc forecast (2012–2017), 2012.
http://www.cisco.com.
4. C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal,
27:379–423, 1948.
5. T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, 2006.
6. D. A. Huffman. A method for the construction of minimum redundancy codes. Proc. IRE,
40:1098–1101, 1952.
7. T. K. Moon. Error Correction Coding. John Wiley & Sons, 2005.
8. ITU-T. H.261: Video codec for audiovisual services at p × 384 kbit/s — Recommendation
h.261, 1988. http://www.itu.int.
9. ISO. Iso/iec 11172-2: 1993 — Information technology — Coding of moving pictures and
associated audio for digital storage media at up to about 1.5 mbit/s — Part 2: Video, 1993.
http://www.iso.org.
10. ISO. Iso/iec 13818-2: 1996 — Information technology — Generic coding of moving pictures
and associated audio information: Video, 1996. http://www.iso.org.
11. ISO. Iso/iec 14496-2: 1998 — Information technology — Coding of audio-visual objects —
Part 2: Visual, 1998. http://www.iso.org.
12. ISO. ISO/IEC 14496-10: 2003 — Information technology — Coding of audio-visual objects
— Part 10: Advanced video coding, 2003. http://www.iso.org.
13. T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra. Overview of the H.264/AVC
video coding standard. Circuits and Systems for Video Technology, IEEE Transactions on,
13(7):560–576, 2003.
14. G. J. Sullivan, J. Ohm, Woo-Jin Han, and T. Wiegand. Overview of the high-efﬁciency video
coding (HEVC) standard. Circuits and Systems for Video Technology, IEEE Transactions on,
22(12):1649–1668, 2012.
15. A. P. Godse and U. A. Bakshi. Analog Communication. Technical Publications Pune, 2009.
16. R. G. Gallager. Low-Density Parity-Check Codes. M.I.T. Press, 1963.
17. D. J. C. MacKay and R. M. Neal. Near Shannon limit performance of low density parity
check codes. Electronics Letters, 32(18):1645–1946, 1996.
18. C. Berrou, A. Glavieux, and P. Thitimajshima. Near Shannon limit error-correcting coding and
decoding: Turbo-codes. In Communications, IEEE International Conference on, volume 2,
pages 1064–1070, 1993.
19. C. Berrou and A. Glavieux. Near optimum error correcting coding and decoding: Turbo-
codes. Communications, IEEE Transactions on, 44(10):1261–1271, 1996.
20. L. Bahl, J. Cocke, F. Jelinek, and J. Raviv. Optimal decoding of linear codes for minimizing
symbol error rate. Information Theory, IEEE Transactions on, 20(2):284–287, 1974.
21. J. Lu. Signal processing for internet video streaming: A review. In Image and Video Commu-
nication and Processing, SPIE International Conference on, volume 3974, pages 246–258,
2000.
22. G. J. Conklin, G. S. Greenbaum, K. O. Lillevold, A. F. Lippman, and Y. A. Reznik. Video cod-
ing for streaming media delivery on the Internet. Circuits and Systems for Video Technology,
IEEE Transactions on, 11(3):269–281, 2001.
23. D. P. Wu, Y. Hou, W. W. Zhu, Y. Q. Zhang, and J. M. Peha. Streaming video over the Internet:
Approaches and directions. Circuits and Systems for Video Technology, IEEE Transactions
on, 11(3):282–300, 2001.
24. J. F. Arnold, M. R. Fracter, and Y. Q. Wang. Efﬁcient drift-free signal-to-noise ratio scalability.
Circuits and Systems for Video Technology, IEEE Transactions on, 10(1):70–82, 2000.
25. L. P. Kondi, F. Ishtiaq, and A. K. Katsaggelos. On video SNR scalability. In Image Processing,
International Conference on, volume 3, pages 934–938, 1998.

444
A Our Published Journal and Conference Papers Related to This Book
26. B. R. Lee, K. K. Park, and J. J. Hwang. H.263-based SNR scalable video codec. Consumer
Electronics, IEEE Transactions on, 43(3):614–622, 1997.
27. A. Puri, L. Yan, and B.G. Haskell. Temporal resolution scalable video coding. In Image
Processing, IEEE International Conference, volume 2, pages 947–951, 1994.
28. G.J. Conklin and S.S. Hemami. A comparison of temporal scalability techniques. Circuits
and Systems for Video Technology, IEEE Transactions on, 9(6):909–919, 1999.
29. H. Katata, N. Ito, and H. Kusao. Temporal-scalable coding based on image content. Circuits
and Systems for Video Technology, IEEE Transactions on, 7(1):52–59, 1997.
30. Stephan Wenger. Temporal scalability using p-pictures for low-latency applications. In Mul-
timedia Signal Processing, IEEE Second Workshop on, pages 559–564, 1998.
31. U. Benzler. Spatial scalable video coding using a combined subband-DCT approach. Circuits
and Systems for Video Technology, IEEE Transactions on, 10(7):1080–1087, 2000.
32. Q. Hu and S. Panchanathan. A comparative evaluation of spatial scalability techniques in the
compressed domain. In Electrical and Computer Engineering, 1996. Canadian Conference
on, volume 1, pages 474–477, 1996.
33. K. Illgner and F. Muller. Multiresolution video compression: Motion estimation and vector
ﬁeld coding. In Global Telecommunications Conference, 1996. GLOBECOM ’96. ’Commu-
nications: The Key to Global Prosperity, volume 3, pages 1478–1482, 1996.
34. Tihao Chiang, Huifang Sun, and J.W. Zdepski. Spatial scalable HDTV coding. In Image Pro-
cessing, 1995. Proceedings., International Conference on, volume 2, pages 571–574, 1995.
35. MPEG requirement group.
MPEG-4 requirements version 8, 2001.
ISO/IEC
JTC1/SC29/WG11, w2194, Dublin.
36. R. Neff and A. Zakhor. Matching pursuit video coding at very low bit rates. In Data Com-
pression Conference, 1995. DCC ’95. Proceedings, pages 411–420, 1995.
37. R. Neff and A. Zakhor. Matching pursuit video coding. I. Dictionary approximation. Circuits
and Systems for Video Technology, IEEE Transactions on, 12(1):13–26, 2002.
38. R. Neff and A. Zakhor. Matching-pursuit video coding. II. Operational models for rate and
distortion. Circuits and Systems for Video Technology, IEEE Transactions on, 12(1):27–39,
2002.
39. W. Li. Status report on bit plane coding, 1998. ISO/IEC JTC1/SC29/WG11, m3792, Dublin.
40. Y. Chen, C. Dufour, H. Radha, R. Cohem, and M. Buteau. Request for ﬁne granular video scal-
ability for media streaming applications, 1998. ISO/IEC JTC1/SC29/WG11, m3792, Dublin.
41. J. M. Shapiro.
Embedded image coding using zero trees of wavelet coefﬁcients.
Signal
Processing, IEEE Transactions on, 41(12):3445–3462, 1993.
42. A. Said and W. A. Pearlman. A new, fast, and efﬁcient image codec based on set partition-
ing in hierarchical trees. Circuits and Systems for Video Technology, IEEE Transactions on,
6(3):243–250, 1996.
43. J. Ohm.
Description of core experiments in MPEG-4 video, 1998.
ISO/IEC
JTC1/SC29/WG11, N2474, Atlantic City.
44. Y. Chen, H. Radha, and R. Cohem. Results of experiment on ﬁne granular scalability with
wavelet encoding of residuals, 1998. ISO/IEC JTC1/SC29/WG11, m3988, Atlantic City.
45. B. Schuster.
Fine granular scalability with wavelets coding, 1998.
ISO/IEC
JTC1/SC29/WG11, m4021, Atlantic City.
46. J. Liang, J. Yu, Y. Wang, M. Srinath, and M. Zhou. Fine granularity scalable video cod-
ing using combination of MPEG-4 video objects and still texture objects, 1998. ISO/IEC
JTC1/SC29/WG11, m4025, Atlantic City.
47. E. Miloslavsky, S. Cheung, and A. Zakhor. SNR scalability using matching pursuits, 1998.
ISO/IEC JTC1/SC29/WG11, m3833, Atlantic City.
48. M. Benetiere and C. Dufour. Matching pursuits residual coding for video ﬁne granular scala-
bility, 1998. ISO/IEC JTC1/SC29/WG11, m3833, Atlantic City.
49. W. Li. Bit-plane coding of DCT coefﬁcients for ﬁne granularity scalability, 1998. ISO/IEC
JTC1/SC29/WG11, m3989, Atlantic City.
50. J. Macnicol, M. Frater, and J. Arnold. Results on ﬁne granularity scalability, 1999. ISO/IEC
JTC1/SC29/WG11, m5122.

References
445
51. S. Li, F. Wu, and Y. Q. Zhang.
Study of a new approach to improve FGS video coding
efﬁciency, 1999. ISO/IEC JTC1/SC29/WG11, M5583.
52. F. Ling, W. Li, and H.-Q. Sun. Bitplane coding of DCT coefﬁcients for image and video com-
pression. In Image and Video Communication and Processing, SPIE International Conference
on, pages 500–508, 1999.
53. T. K. Tan, K. K. Pang, and K. N. Ngan. A frequency scalable coding scheme employing pyra-
mid and subband techniques. Circuits and Systems for Video Technology, IEEE Transactions
on, 4(2):203–207, 1994.
54. Yao Wang and Qin-Fan Zhu. Error control and concealment for video communication: A
review. Proceedings of the IEEE, 86(5):974–997, 1998.
55. R. Talluri. Error-resilient video coding in the ISO MPEG-4 standard. Communications Mag-
azine, IEEE, 36(6):112–119, 1998.
56. J. R. Yee and Jr. Weldon, E. J. Evaluation of the performance of error-correcting codes on a
Gilbert channel. Communications, IEEE Transactions on, 43(8):2316–2323, 1995.
57. M. Vetterli and J. Kovacevic. Wavelets and Subband Coding. Englewood Cliffs, NJ: Prentice-
Hall, 1995.
58. D. Taubman and A. Zakhor. Multirate 3-D subband coding of video. Image Processing, IEEE
Transactions on, 3(5):572–588, 1994.
59. A. Wang, Z. Xiong, P. A. Chou, and S. Mehrotra. Three-dimensional wavelet coding of video
with global motion compensation. In Data Compression, International Conference on, pages
404–413, 1999.
60. J.-R. Ohm. Three-dimensional subband coding with motion compensation. Image Processing,
IEEE Transactions on, 3(5):559–571, 1994.
61. Jo Yew Tham, Surendra Ranganath, and A. A. Kassim. Highly scalable wavelet-based video
codec for very low bit-rate environment. Selected Areas in Communications, IEEE Journal
on, 16(1):12–27, 1998.
62. Seung-Jong Choi and J. W. Woods. Motion-compensated 3-D subband coding of video. Image
Processing, IEEE Transactions on, 8(2):155–167, 1999.
63. Beong-Jo Kim, Zixiang Xiong, and W. A. Pearlman.
Low bit-rate scalable video coding
with 3-D set partitioning in hierarchical trees (3-D SPIHT). Circuits and Systems for Video
Technology, IEEE Transactions on, 10(8):1374–1387, 2000.
64. B. Pesquet-Popescu and V. Bottreau. Three-dimensional lifting schemes for motion compen-
sated video compression. In Acoustics, Speech, and Signal Processing, 2001. Proceedings.
(ICASSP ’01). 2001 IEEE International Conference on, volume 3, pages 1793–1796, 2001.
65. Lin Luo, Jin Li, Shipeng Li, Zhenquan Zhuang, and Ya-Qin Zhang. Motion compensated
lifting wavelet and its application in video coding. In Multimedia and Expo, 2001. ICME
2001. IEEE International Conference on, pages 365–368, 2001.
66. A. Secker and D. Taubman. Motion-compensated highly scalable video compression using
an adaptive 3D wavelet transform based on lifting. In Image Processing, 2001. Proceedings.
2001 International Conference on, volume 2, pages 1029–1032, 2001.
67. Peisong Chen and J. W. Woods. Bidirectional MC-EZBC with lifting implementation. Cir-
cuits and Systems for Video Technology, IEEE Transactions on, 14(10):1183–1194, 2004.
68. M. Flierl and B. Girod. Video coding with motion-compensated lifted wavelet transforms.
Signal Process.: Image Commun., 19(7):561–575, 2004.
69. P. Chen and J. W. Woods. Improved MC-EZBC with quarter-pixel motion vectors, 2002.
ISO/IEC JTC1/SC29/WG11, M8366, Fairfax, VA.
70. J.-R. Ohm. Motion-compensated wavelet lifting ﬁlters with ﬂexible adaptation. In Digital
Communication, International Workshop on, pages 113–120, 2002.
71. D. S. Turaga, M. Van der Schaar, and B. Pesquet-Popescu. Complexity scalable motion com-
pensated wavelet video encoding. Circuits and Systems for Video Technology, IEEE Transac-
tions on, 15(8):982–993, 2005.
72. A. Secker and D. Taubman. Highly scalable video compression with scalable motion coding.
Image Processing, IEEE Transactions on, 13(8):1029–1041, 2004.

446
A Our Published Journal and Conference Papers Related to This Book
73. D. Turaga, M. van der Schaar, Y. Andreopoulos, A. Munteanu, and P. Schelkens. Uncon-
strained motion compensated temporal ﬁltering (UMCTF) for efﬁcient and ﬂexible interframe
wavelet video coding. Signal Process.: Image Commun., 20(1):1–19, 2005.
74. J. Xu, Z. Xiong, S. Li, and Y.-Q. Zhang. Three-dimensional embedded subband coding with
optimal truncation (3-D ESCOT). Appl. Comput. Harmonic Anal., International Journal,
10:290–315, 2001.
75. Jizheng Xu, Zixiang Xiong, Shipeng Li, and Ya-Qin Zhang. Memory-constrained 3D wavelet
transform for video coding without boundary effects. Circuits and Systems for Video Tech-
nology, IEEE Transactions on, 12(9):812–818, 2002.
76. Lin Luo, Feng Wu, Shipeng Li, and Zhenquan Zhuang.
Advanced lifting-based motion-
threading techniques for 3-D wavelet video coding. In Visual Communication and Image
Processing, SPIE International Conference on, pages 707–718, 2003.
77. Lin Luo, Feng Wu, Shipeng Li, and Zhenquan Zhuang. Layer-correlated motion estimation
and motion vector coding for the 3D-wavelet video coding. In Image Processing, 2003. ICIP
2003. Proceedings. 2003 International Conference on, volume 2, pages 791–794, 2003.
78. A. Secker and D. Taubman. Highly scalable video compression using a lifting-based 3D
wavelet transform with deformable mesh motion compensation. In Image Processing. 2002.
Proceedings. 2002 International Conference on, volume 3, pages 749–752, 2002.
79. A. Secker and D. Taubman. Lifting-based invertible motion adaptive transform (LIMAT)
framework for highly scalable video compression. Image Processing, IEEE Transactions on,
12(12):1530–1542, 2003.
80. I. Daubechies and W. Sweldens. Factoring wavelet transforms into lifting steps. Journal of
Fourier Analysis and Applications, 4:247–269, 1998.
81. W. Sweldens. The lifting scheme: A new philosophy in biorthogonal wavelet construction.
In Wavelet Applications in Signal and Image Processing, SPIE International Conference on,
1995.
82. B. Girod. Motion-compensating prediction with fractional-pel accuracy. Communications,
IEEE Transactions on, 41(4):604–612, 1993.
83. JPEG. Veriﬁcation model ad-hoc, JPEG 2000 VM 8.5, 2000. ISO/IEC JTC1/SC29/WG1,
N1878.
84. T. Wiegand and B. Girod. Lagrange multiplier selection in hybrid video coder control. In
Image Processing, 2001. Proceedings. 2001 International Conference on, volume 3, pages
542–545, 2001.
85. P. Chen and J. W. Woods. Contributions to interframe wavelet and scalable video coding,
2002. ISO/IEC JTC1/SC29/WG11, M9034, Shanghai, China.
86. J. Xu, S. Li, and Y.-Q. Zhang. Three-dimensional shape-adaptive discrete wavelet transforms
for efﬁcient object-based video coding. In Visual Communication and Image Processing,
SPIE International Conference on, volume 4067, pages 336–344, 2000.
87. Lin Luo, Feng Wu, Shipeng Li, Zixiang Xiong, and Zhenquan Zhuang. Advanced motion
threading for 3-D wavelet video coding. Signal Process.: Image Commun., 19:601–616, 2004.
88. R. Xiong, F. Wu, S. Li, Z. Xiong, and Y.-Q. Zhang. Exploiting temporal correlation with
adaptive block-size motion alignment for 3-D wavelet coding. In Visual Communication and
Image Processing, SPIE International Conference on, volume 5308, pages 144–155, 2004.
89. Bo Feng, Jizheng Xu, Feng Wu, Shiqiang Yang, and Shipeng Li. Energy distributed update
steps (EDU) in lifting based motion compensated video coding. In Image Processing, 2004.
ICIP ’04. 2004 International Conference on, volume 4, pages 2267–2270, 2004.
90. R. Xiong, F. Wu, J. Xu, S. Li, and Y.-Q. Zhang. Barbell-lifting wavelet transform for highly
scalable video coding. In Picture Coding Symposium, pages 237–242, 2004.
91. Ruiqin Xiong, Jizheng Xu, Feng Wu, Shipeng Li, and Ya-Qin Zhang. Layered motion esti-
mation and coding for fully scalable 3D wavelet video coding. In Image Processing, 2004.
ICIP ’04. 2004 International Conference on, volume 4, pages 2271–2274, 2004.
92. X. Ji, J. Xu, D. Zhao, and F. Wu. Architectures of incorporating MPEG-4 AVC into three-
dimensional wavelet video coding. In Picture Coding Symposium, 2004.
93. MPEG.
Call for proposals on scalable video coding technology, 2003.
ISO/IEC
JTC1/SC29/WG11, N6193.

References
447
94. MPEG.
Registered responses to the call for proposals on scalable video coding, 2004.
ISO/IEC JTC1/SC29/WG11, M10569.
95. MPEG.
Subjective test results for the CFP on scalable video coding technology, 2004.
ISO/IEC JTC1/SC29/WG11, N6383.
96. MPEG. Exploration experiments on tools evaluation in wavelet video coding, 2005. ISO/IEC
JTC1/SC29/WG11, N6914.
97. N. Mehrseresht and D. Taubman. A ﬂexible structure for fully scalable motion-compensated
3-D DWT with emphasis on the impact of spatial scalability. Image Processing, IEEE Trans-
actions on, 15(3):740–753, 2006.
98. D.S. Turaga and M. Van der Schaar. Content-adaptive ﬁltering in the UMCTF framework.
In Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP ’03). 2003 IEEE
International Conference on, volume 3, pages 621–624, 2003.
99. L. Song, J. Xu, H. Xiong, and F. Wu. Content adaptive update for lifting-based motion-
compensated temporal ﬁltering. Electronics Letters, 41(1):14–16, 2005.
100. B. Girod and Sangeun Han. Optimum update for motion-compensated lifting. Signal Pro-
cessing Letters, IEEE, 12(2):150–153, 2005.
101. Yihua Chen, Jizheng Xu, Feng Wu, and Hongkai Xiong. An improved update operator for
H.264 scalable extension. In Multimedia Signal Processing, 2005 IEEE 7th Workshop on,
pages 1–4, 2005.
102. D. Taubman and A. Secker. Highly scalable video compression with scalable motion cod-
ing. In Image Processing, 2003. ICIP 2003. Proceedings. 2003 International Conference on,
volume 3, pages 273–276, 2003.
103. D. Taubman. High performance scalable image compression with EBCOT. Image Processing,
IEEE Transactions on, 9(7):1158–1170, 2000.
104. D. Taubman, E. Ordentlich, M. Weinberger, and G. Seroussi. Embedded block coding in
JPEG 2000. Signal Process.: Image Commun., 17(1):49–72, 2002.
105. Shih-Ta Hsiang and J.W. Woods.
Embedded image coding using zeroblocks of sub-
band/wavelet coefﬁcients and context modeling. In Circuits and Systems, 2000. Proceedings.
ISCAS 2000 Geneva. The 2000 IEEE International Symposium on, volume 3, pages 662–665,
2000.
106. F. Lazzaroni, A. Signoroni, and R. Leonardi. Embedded morphological dilation coding for
2-D and 3-D images. In Visual Communication and Image Processing, SPIE International
Conference on, volume 4671, pages 923–934, 2002.
107. H. Schwarz, T. Hinz, H. Kirchhoffer, D. Marpe, and T. Wiegand. Technical description of the
HHI proposal for SVC CE1, 2004. ISO/IEC JTC1/SC29/WG11, M11244.
108. R. Schafer, H. Schwarz, D. Marpe, T. Schierl, and T. Wiegand. MCTF and scalability exten-
sion of H.264/AVC and its application to video transmission. In Visual Communication and
Image Processing, SPIE International Conference on, volume 5960, pages 343–354, 2005.
109. M. Flierl and B. Girod. Generalized B pictures and the draft H.264/AVC video-compression
standard. Circuits and Systems for Video Technology, IEEE Transactions on, 13(7):587–597,
2003.
110. H. Schwarz, D. Marpe, and T. Wiegand. Analysis of hierarchical B pictures and MCTF. In
Multimedia and Expo, 2006 IEEE International Conference on, pages 1929–1932, 2006.
111. Ruiqin Xiong, Jizheng Xu, Feng Wu, Shipeng Li, and Ya-Qin Zhang. Spatial scalability in
3-D wavelet coding with spatial domain MCTF encoder. In Picture Coding Symposium, pages
583–588, 2004.
112. N. Mehrseresht and D. Taubman. Spatial scalability and compression efﬁciency within a ﬂex-
ible motion compensated 3D-DWT. In Image Processing, 2004. ICIP ’04. 2004 International
Conference on, volume 2, pages 1325–1328, 2004.
113. R. Xiong, Jizheng Xu, Feng Wu, Shipeng Li, and Ya-Qin Zhang. Optimal subband rate alloca-
tion for spatial scalability in 3-D wavelet video coding with motion aligned temporal ﬁltering.
In Visual Communication and Image Processing, SPIE International Conference on, volume
5960, pages 381–392, 2005.

448
A Our Published Journal and Conference Papers Related to This Book
114. Ruiqin Xiong, Jizheng Xu, Feng Wu, Shipeng Li, and Ya-Qin Zhang. Subband coupling aware
rate allocation for spatial scalability in 3-D wavelet video coding. Circuits and Systems for
Video Technology, IEEE Transactions on, 17(10):1311–1324, 2007.
115. Ruiqin Xiong, Jizheng Xu, Feng Wu, and Shipeng Li. Studies on spatial scalable frame-
works for motion aligned 3-D wavelet video coding. In Visual Communication and Image
Processing, SPIE International Conference on, volume 5960, pages 189–200, 2005.
116. Ruiqin Xiong, Jizheng Xu, Feng Wu, and Shipeng Li.
In-scale motion aligned temporal
ﬁltering. In Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE International
Symposium on, pages 3017–3020, 2006.
117. R. Xiong, J. Xu, and F. Wu. A new method for inter-layer prediction in spatial scalable video
coding, 2006. Joint Video Team of ITU-T VCEG and ISO/IEC MPEG, JVT-T081, Klagenfurt,
Austria.
118. R. Xiong, Jizheng Xu, Feng Wu, and Shipeng Li. Generalized in-scale motion compensation
framework for spatial scalable video coding. In Visual Communication and Image Processing,
SPIE International Conference on, volume 6508, 2006.
119. Ruiqin Xiong, Jizheng Xu, Feng Wu, and Shipeng Li. Macroblock-based adaptive in-scale
prediction for scalable video coding. In Circuits and Systems, 2007. ISCAS 2007. IEEE Inter-
national Symposium on, pages 1763–1766, 2007.
120. Ruiqin Xiong, Jizheng Xu, and Feng Wu. In-scale motion compensation for spatially scalable
video coding. Circuits and Systems for Video Technology, IEEE Transactions on, 18(2):145–
158, 2008.
121. Ruiqin Xiong, Jizheng Xu, Feng Wu, and Shipeng Li. Adaptive MCTF based on correlation
noise model for SNR scalable video coding. In Multimedia and Expo, 2006 IEEE Interna-
tional Conference on, pages 1865–1868, 2006.
122. P. Chen and J. W. Woods. Contributions to interframe wavelet and scalable video coding,
2002. ISO/IEC JTC1/SC29/WG11, m9034, Shanghai, China.
123. Y. Wu. Fully scalable subband/wavelet video coding system, 2005. Ph.D. dissertation, Rens-
selaer Polytechnic Inst., Troy, NY.
124. M. Wien and H. Schwarz. Testing conditions for SVC coding efﬁciency and JSVM perfor-
mance evaluation, 2005. Joint Video Team of ISO/IEC MPEG and ITU-T VCEG, JVT-Q205,
Poznan, Poland.
125. M. Wien and H. Schwarz. AHG on coding EFF and JSVM coding efﬁciency testing condi-
tions, 2007. Joint Video Team of ISO/IEC MPEG and ITU-T VCEG, JVT-T008, Klagenfurt,
Austria.
126. M. Wien and H. Schwarz. JSVM 6 software, 2006. Joint Video Team of ISO/IEC MPEG and
ITU-T VCEG, JVT-S203, Geneva, Switzerland.
127. E. Feig, H. Peterson, and V. Ratnakar. Image compression using spatial prediction. In Acous-
tics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on,
volume 4, pages 2339–2342, 1995.
128. H. Kondo and Y. Oishi. Digital image compression using directional sub-block DCT. In Com-
munication Technology Proceedings, 2000. WCC — ICCT 2000. International Conference on,
volume 1, pages 985–992, 2000.
129. W. B. Pennebaker and J. L. Mitchell. JPEG Still Image Data Compression Standard. New
York: Van Nostrand, 1993.
130. D. Taubman and M. Marcellin. JPEG 2000: Image Compression Fundamentals, Standards,
and Practice. Norwell, MA: Kluwer, 2001.
131. A. Ikonomopoulos and M. Kunt. High compression image coding via directional ﬁltering.
Signal Processing, 8(2):179–203, 1985.
132. D. Taubman and A. Zakhor. Orientation adaptive subband coding of images. Image Process-
ing, IEEE Transactions on, 3(4):421–437, 1994.
133. Haibo Li and Zhenya He. Directional subband coding of images. In Acoustics, Speech, and
Signal Processing, 1989. ICASSP-89, 1989 International Conference on, pages 1823–1826,
1989.
134. R.H. Bamberger and M. J. T. Smith. A ﬁlter bank for the directional decomposition of images:
Theory and design. Signal Processing, IEEE Transactions on, 40(4):882–893, 1992.

References
449
135. T. T. Nguyen and Soontorn Oraintara.
A directional decomposition: Theory, design, and
implementation. In Circuits and Systems, 2004. ISCAS ’04. Proceedings of the 2004 Interna-
tional Symposium on, volume 3, pages 281–284, 2004.
136. Yue Lu and M. N. Do. The ﬁner directional wavelet transform [image processing applica-
tions]. In Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP ’05). IEEE
International Conference on, volume 4, pages 573–576, 2005.
137. E. J. Candes. Monoscale ridgelets for the representation of images with edges, 1999. Dept.
Statistic, Stanford Univ., Tech. Reporter.
138. M. N. Do and M. Vetterli. The ﬁnite ridgelet transform for image representation. Image
Processing, IEEE Transactions on, 12(1):16–28, 2003.
139. E. J. Candes and D. L. Donoho. Curvelets, multiresolution representation, and scaling laws.
In SPIE Wavelet Applications in Signal and Image Processing VIII, volume 4119, 2000.
140. V. Velisavljevic, P.-L. Dragotti, and M. Vetterli. Directional wavelet transforms and frames.
In Image Processing. 2002. Proceedings. 2002 International Conference on, volume 3, pages
589–592, 2002.
141. M. N. Do and M. Vetterli. The contourlet transform: An efﬁcient directional multiresolution
image representation. Image Processing, IEEE Transactions on, 14(12):2091–2106, 2005.
142. F. C. A. Fernandes, R. L. C. van Spaendonck, and C. S. Burrus. A new framework for complex
wavelet transforms. Signal Processing, IEEE Transactions on, 51(7):1825–1837, 2003.
143. F. C. A. Fernandes, M. B. Wakin, and R. G. Baraniuk. Non-redundant, linear-phase, semi-
orthogonal, directional complex wavelets. In Acoustics, Speech, and Signal Processing, 2004.
Proceedings. (ICASSP ’04). IEEE International Conference on, volume 2, pages 953–956,
2004.
144. F. G. Meyer and R. R. Coifman. Brushlets: A tool for directional image analysis and image
compression. Journal of Application and Computer Harmonic Analysis, 5:147–187, 1997.
145. D. Wang, L. Zhang, and A. Vincent. Curved wavelet transform for scalable video coding,
2004. ISO/IEC JTC1/SC29/WG11, M10535, Munich.
146. Chang-N Zhang and X. Wu. A hybrid approach of wavelet packet and directional decomposi-
tion for image compression. In Electrical and Computer Engineering, 1999 IEEE Canadian
Conference on, volume 2, pages 755–760, 1999.
147. P. Carre, E. Andres, and C. Fernandez-Maloigne. Discrete rotation for directional orthogonal
wavelet packets. In Image Processing, 2001. Proceedings. 2001 International Conference on,
volume 2, pages 257–260, 2001.
148. R. Vargic. An approach to directional wavelet construction and their use for image com-
pression. In Video/Image Processing and Multimedia Communications 4th EURASIP-IEEE
Region 8 International Symposium on VIPromCom, pages 201–204, 2002.
149. Chin-Hwa Kuo, Tzu-Chuan Chou, and Tay-Shen Wang. An efﬁcient spatial prediction-based
image compression scheme. Circuits and Systems for Video Technology, IEEE Transactions
on, 12(10):850–856, 2002.
150. W. Sweldens. The lifting scheme: A custom-design construction of biorthogonal wavelets,
1994. Technical Report 1994:7, Industrial Mathematics Initiative, Department of Mathemat-
ics, University of South Carolina.
151. D. Taubman. Adaptive, non-separable lifting transforms for image compression. In Image
Processing, 1999. ICIP 99. Proceedings. 1999 International Conference on, volume 3, pages
772–776, 1999.
152. N. V. Boulgouris and M. G. Strintzis. Orientation-sensitive interpolative pyramids for lossless
and progressive image coding. Image Processing, IEEE Transactions on, 9(4):710–715, 2000.
153. N. V. Boulgouris, D. Tzovaras, and M. G. Strintzis. Lossless image compression based on
optimal prediction, adaptive lifting, and conditional arithmetic coding. Image Processing,
IEEE Transactions on, 10(1):1–14, 2001.
154. R. L. Claypoole, G. M. Davis, W. Sweldens, and R. G. Baraniuk. Nonlinear wavelet trans-
forms for image coding via lifting. Image Processing, IEEE Transactions on, 12(12):1449–
1459, 2003.

450
A Our Published Journal and Conference Papers Related to This Book
155. Hongliang Li, Guizhong Liu, and Zhongwei Zhang. Optimization of integer wavelet trans-
forms based on difference correlation structures. Image Processing, IEEE Transactions on,
14(11):1831–1847, 2005.
156. O. N. Gerek and A. E. Cetin. A 2-D orientation-adaptive prediction ﬁlter in lifting structures
for image coding. Image Processing, IEEE Transactions on, 15(1):106–111, 2006.
157. W. Ding, F. Wu, and S. Li. Lifting-based wavelet transform with directionally spatial predic-
tion. In Picture Coding Symposium, 2004.
158. Chuo-Ling Chang, A. Maleki, and B. Girod. Adaptive wavelet transform for image com-
pression via directional quincunx lifting. In Multimedia Signal Processing, 2005 IEEE 7th
Workshop on, pages 537–540, 2005.
159. L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and regression trees,
1984. The Wadsworth Statistics/Probability Series, Belmont, CA.
160. L. Yaroslavsky. Fast signal sinc-interpolation and its applications in signal and image pro-
cessing. In SPIE 14th Annual Symposium Electronic Image 2003, volume 4667, 2002.
161. Wenpeng Ding, Feng Wu, Xiaolin Wu, Shipeng Li, and Li Houqiang. Adaptive directional
lifting-based wavelet transform for image coding. Image Processing, IEEE Transactions on,
16(2):416–427, 2007.
162. Bing Zeng and Jingjing Fu. Directional discrete cosine transforms for image coding. In
Multimedia and Expo, 2006 IEEE International Conference on, pages 721–724, 2006.
163. P. Kauff and K. Schuur. Shape-adaptive DCT with block-based DC separation and delta;DC
correction. Circuits and Systems for Video Technology, IEEE Transactions on, 8(3):237–242,
1998.
164. Wen-Hsiung Chen, C. Smith, and S. Fralick. A fast computational algorithm for the discrete
cosine transform. Communications, IEEE Transactions on, 25(9):1004–1009, 1977.
165. Wen-Hsiung Chen and C. Smith. Adaptive coding of monochrome and color images. Com-
munications, IEEE Transactions on, 25(11):1285–1292, 1977.
166. C. Loefﬂer, A. Ligtenberg, and George S. Moschytz. Practical fast 1-D DCT algorithms with
11 multiplications. In Acoustics, Speech, and Signal Processing, 1989. ICASSP-89., 1989
International Conference on, pages 988–991, 1989.
167. F. A. M. L. Bruekers and A. W. M. van den Enden. New networks for perfect inversion and
perfect reconstruction. Selected Areas in Communications, IEEE Journal on, 10(1):129–137,
1992.
168. T. D. Tran. Fast multiplierless approximation of the DCT. In Proc. 33rd Annu. Conf. Inform.
Science System, pages 933–938, 1999.
169. Jie Liang and T. D. Tran. Fast multiplierless approximations of the DCT with the lifting
scheme. Signal Processing, IEEE Transactions on, 49(12):3032–3044, 2001.
170. P. P. Vaidyanathan and P.-Q. Hoang. Lattice structures for optimal design and robust imple-
mentation of two-channel perfect-reconstruction QMF banks. Acoustics, Speech and Signal
Processing, IEEE Transactions on, 36(1):81–94, 1988.
171. T. H. Cormen, R. L. Rivest, C. E. Leiserson, and C. Stein. Introduction to Algorithms. Cam-
bridge, MA: MIT Press, 2001.
172. J. Kovacevic and M. Vetterli. Nonseparable multidimensional perfect reconstruction ﬁlter
banks and wavelet bases for RN. Information Theory, IEEE Transactions on, 38(2):533–555,
1992.
173. E. J. Candes and D. L. Donoho. Curvelets — A surprisingly effective non-adaptive represen-
tation for objects with edges, 1999. in Curve and Surface Fitting, A. Cohen, C. Rabut, and L.
L. Schumaker, Eds. Saint-Malo: Vanderbilt University Press.
174. J. K. Romberg, M. Wakin, and R. Baraniuk. Multiscale wedgelet image analysis: Fast de-
compositions and modeling. In Image Processing. 2002. Proceedings. 2002 International
Conference on, volume 3, pages 585–588, 2002.
175. T. T. Nguyen and Soontorn Oraintara. A multiresolution directional ﬁlter bank for image
applications. In Acoustics, Speech, and Signal Processing, 2004. Proceedings. (ICASSP ’04).
IEEE International Conference on, volume 3, pages 37–40, 2004.

References
451
176. Yilong Liu, T. T. Nguyen, and Soontorn Oraintara.
Low bit-rate image coding based on
pyramidal directional ﬁlter banks. In Acoustics, Speech and Signal Processing, 2006. ICASSP
2006 Proceedings. 2006 IEEE International Conference on, volume 2, pages 437–440, 2006.
177. S. Esakkirajan, T. Veerakumar, V. S. Murugan, and R. Sudhakar. Image compression using
contourlet transform and multistage vector quantization. GVIP Journal, 6(1):19–28, 2006.
178. R. Ansari, A. E. Cetin, and S. H. Lee. Subband coding of images using nonrectangular ﬁlter
banks. In Applications of Digital Image Processing XI, SPIE, volume 974, pages 315–322,
1988.
179. Demin Wang, L. Zhang, and A. Vincent. Improvement of JPEG 2000 using curved wavelet
transform. In Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP ’05).
IEEE International Conference on, volume 2, pages 365–368, 2005.
180. Demin Wang, L. Zhang, A. Vincent, and F. Speranza. Curved wavelet transform for image
coding. Image Processing, IEEE Transactions on, 15(8):2413–2421, 2006.
181. Yu Liu and King Ngi Ngan. Weighted adaptive lifting-based wavelet transform for image
coding. Image Processing, IEEE Transactions on, 17(4):500–511, 2008.
182. Weisheng Dong, Guangming Shi, and Jizheng Xu. Adaptive nonseparable interpolation for
image compression with directional wavelet transform.
Signal Processing Letters, IEEE,
15:233–236, 2008.
183. Chuo-Ling Chang and B. Girod. Direction-adaptive discrete wavelet transform for image
compression. Image Processing, IEEE Transactions on, 16(5):1289–1302, 2007.
184. Bing Zeng and Jingjing Fu. Directional discrete cosine transforms — A new framework for
image coding. Circuits and Systems for Video Technology, IEEE Transactions on, 18(3):305–
313, 2008.
185. Hao Xu, Jizheng Xu, and Feng Wu. Lifting-based directional DCT-like transform for image
coding. Circuits and Systems for Video Technology, IEEE Transactions on, 17(10):1325–
1335, 2007.
186. Chuo-Ling Chang and B. Girod. Direction-adaptive partitioned block transform for image
coding.
In Image Processing, 2008. ICIP 2008. 15th IEEE International Conference on,
pages 145–148, 2008.
187. O. N. Gerek and A. E. Cetin. Adaptive polyphase subband decomposition structures for image
compression. Image Processing, IEEE Transactions on, 9(10):1649–1660, 2000.
188. Yan Ye and M. Karczewicz. Improved H.264 intra coding based on bi-directional intra pre-
diction, directional transform, and adaptive coefﬁcient scanning. In Image Processing, 2008.
ICIP 2008. 15th IEEE International Conference on, pages 2116–2119, 2008.
189. T. K. Tan, C. S. Boon, and Y. Suzuki. Intra prediction by template matching. In Image
Processing, 2006 IEEE International Conference on, pages 1693–1696, 2006.
190. Xiulian Peng, Feng Wu, and Jizheng Xu. Directional ﬁltering transform. In Multimedia and
Expo, 2009. ICME 2009. IEEE International Conference on, pages 1–4, 2009.
191. ITU-T.
Key technical area software of the ITU-T, version jm11.0kta2.0, 2008.
http://iphome.hhi.de/suehring/tml/download/KTA.
192. N. Jayant, J. Johnston, and R. Safranek.
Signal compression based on models of human
perception. Proceedings of the IEEE, 81(10):1385–1422, 1993.
193. I. Hontsch and L. J. Karam. Locally adaptive perceptual image coding. Image Processing,
IEEE Transactions on, 9(9):1472–1483, 2000.
194. M. M. Reid, R. J. Millar, and N. D. Black. Second-generation image coding: An overview.
ACM Computing Surveys, 29(1):2–29, 1997.
195. M. Tuceryan and A. K. Jain. Texture analysis. In The Handbook of Pattern Recognition and
Computer Vision (2nd Edition), C. H. Chen, L. F. Pau, and P. S. P. Wang, Eds. World Scientiﬁc
Publishing Co., pages 207–248, 1998.
196. Cheng en Guo, Song-Chun Zhu, and Ying Nian Wu. Efﬁcient gathering of correlated data in
sensor network. In Computer Vision, 2003. Proceedings. Ninth IEEE International Confer-
ence on, pages 1228–1235, 2003.
197. A. A. Efros and T. K. Leung. Texture synthesis by non-parametric sampling. In Computer
Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2,
pages 1033–1038, 1999.

452
A Our Published Journal and Conference Papers Related to This Book
198. L.-Y. Wei and M. Levoy. Fast texture synthesis using tree-structured vector quantization. In
ACM SIGGRAPH, pages 479–488, 2000.
199. M. Ashikhmin. Synthesizing natural textures. In ACM Symposium on Interactive 3D Graph-
ics, pages 217–226, 2001.
200. A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and D. H. Salesin. Image analogies. In
ACM SIGGRAPH, pages 327–340, 2001.
201. A. A. Efros and W. T. Freeman. Image quilting for texture synthesis and transfer. In ACM
SIGGRAPH, pages 341–346, 2001.
202. L. Liang, C. Liu, Y.-Q. Xu, B. Guo, and H.-Y. Shum. Real-time texture synthesis by patch-
based sampling. Graphics, ACM Transactions on, 20(3):127–150, 2001.
203. V. Kwatra, A. Schdl, I. Essa, G. Turk, and A. Bobick. Graphcut textures: Image and video
synthesis using graph cuts. In ACM SIGGRAPH, pages 277–286, 2003.
204. S. Lefebvre and H. Hoppe. Parallel controllable texture synthesis. In ACM SIGGRAPH, pages
777–786, 2005.
205. V. Kwatra, I. Essa, A. Bobick, and N. Kwatra. Texture optimization for example-based syn-
thesis. In ACM SIGGRAPH, pages 795–802, 2005.
206. M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. Image inpainting. In ACM SIGGRAPH,
pages 417–424, 2000.
207. T. F. Chan and J. Shen. Mathematical models for local nontexture inpaintings. 62(3):1019–
1043, 2002.
208. C. Ballester, M. Bertalmio, V. Caselles, G. Sapiro, and J. Verdera. Filling-in by joint interpo-
lation of vector ﬁelds and gray levels. Image Processing, IEEE Transactions on, 10(8):1200–
1211, 2001.
209. T. F. Chan and J. Shen.
Non-texture inpainting by curvature-driven diffusions (CDD).
12(4):436–449, 2001.
210. Jiaya Jia and Chi-Keung Tang. Image repairing: Robust image synthesis by adaptive ND
tensor voting. In Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE
Computer Society Conference on, volume 1, pages 643–650, 2003.
211. I. Drori, D. Cohen-Or, and H. Yeshurun. Fragment-based image completion. In ACM SIG-
GRAPH, pages 303–312, 2003.
212. Antonio Criminisi, P. Perez, and K. Toyama. Region ﬁlling and object removal by exemplar-
based image inpainting. Image Processing, IEEE Transactions on, 13(9):1200–1212, 2004.
213. M. Bertalmio, L. Vese, G. Sapiro, and S. Osher. Simultaneous structure and texture image
inpainting. Image Processing, IEEE Transactions on, 12(8):882–889, 2003.
214. H. Grossauer. A combined PDE and texture synthesis approach to inpainting. In Computer
Vision, Euro. Conference on, pages 214–224, 2004.
215. S. D. Rane, G. Sapiro, and M. Bertalmio. Structure and texture ﬁlling-in of missing im-
age blocks in wireless transmission and compression applications. Image Processing, IEEE
Transactions on, 12(3):296–303, 2003.
216. P. Prez, M. Gangnet, and A. Blake. Patchworks: Example-based region tiling for image edit-
ing, 2004. Microsoft Research, Tech. Rep. MSR-TR-2004-04.
217. J. Sun, L. Yuan, J. Jia, and H.-Y. Shum. Image completion with structure propagation. In
ACM SIGGRAPH, pages 861–868, 2005.
218. L. Atzori and F. G. B. De Natale. Error concealment in video transmission over packet net-
works by a sketch-based approach. Signal Processing: Image Communication, 15(1-2):57–76,
1999.
219. X. Sun, F. Wu, and S. Li. Compression with vision technologies. In Picture Coding Sympo-
sium, 2006.
220. N. Jojic, B. J. Frey, and A. Kannan. Epitomic analysis of appearance and shape. In Computer
Vision, 2003. Proceedings. Ninth IEEE International Conference on, pages 34–41, 2003.
221. V. Cheung, B. J. Frey, and Nebojsa Jojic. Video epitomes. In Computer Vision and Pattern
Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages
42–49, 2005.

References
453
222. Chen Wang, Xiaoyan Sun, Feng Wu, and Hongkai Xiong. Image compression with structure-
aware inpainting. In Circuits and Systems, 2006. ISCAS 2006. Proceedings. 2006 IEEE Inter-
national Symposium on, pages 1816–1819, 2006.
223. Wenjun Zeng and B. Liu. Geometric-structure-based error concealment with novel applica-
tions in block-based low-bit-rate coding. Circuits and Systems for Video Technology, IEEE
Transactions on, 9(4):648–665, 1999.
224. C. A. Rothwell, J. L. Mundy, W. Hoffman, and V.-D. Nguyen. Driving vision by topology. In
Computer Vision. International Symposium on, pages 395–400, 1995.
225. D. Tschumperle and R. Deriche. Vector-valued image regularization with PDES: A com-
mon framework for different applications. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 27(4):506–517, 2005.
226. M. Armbrust, A. Fox, R. Grifﬁth, A. D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson,
A. Rabkin, I. Stoica, and M. Zaharia. A view of Cloud computing. Comm. of the ACM,
53:50–58, 2010.
227. Google. Streetview, 2010. https://developers.google.com/maps/documentation/streetview.
228. G. K. Wallace. The JPEG still picture compression standard. Comm. of the ACM, 34:30–44,
1991.
229. G. J. Sullivan and J. R. Ohm. Recent developments in standardization of high efﬁciency
video coding (HEVC). In SPIE Applications of Digital Image Processing XXXIII, volume
7798, pages 1823–1826, 2010.
230. Y. Rui, T. S. Huang, and S. F. Chang. Image retrieval: Current techniques, promising direc-
tions, open issues. Journal of Visual Communication and Image Representation, 10:39–62,
1999.
231. M. S. Lew, N. Sebe, C. Djeraba, and R. Jain. Content-based multimedia information retrieval:
State of the art and challenges. Multimedia Computing, Comm. and Appl., ACM Transactions
on, 2:1–19, 2006.
232. J. R. Smith and S. F. Chang. Visualseek: A fully automated content-based image query system.
In ACM Multimedia, page 8798, 1996.
233. C. H. Wang, Z. W. Li, and L. Zhang. Mindﬁnder: Image search by interactive sketching and
tagging. In Proc. of WWW, pages 1309–1312, 2010.
234. Y. Ke, R. Sukthankar, and L. Huston. An efﬁcient parts-based near-duplicate and sub-image
retrieval system. In ACM Multimedia, pages 869–876, 2004.
235. Q. F. Zheng, W. Q. Wang, and W. Gao. Effective and efﬁcient object-based image retrieval
using visual phrases. In ACM Multimedia, pages 77–80, 2006.
236. Zhong Wu, Qifa Ke, M. Isard, and Jian Sun. Bundling features for large-scale partial-duplicate
Web image search. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE
Conference on, pages 25–32, 2009.
237. W. G. Zhou, Y. J. Lu, H. Q. Li, Y. B. Song, and Q. Tian. Spatial coding for large-scale
partial-duplicate Web image search. In ACM Multimedia, page 511–520, 2010.
238. J. Hays and A. A. Efros. Scene completion using millions of photographs. Graphics, ACM
Transactions on, 26(3), 2007.
239. O. Whyte, J. Sivic, and A. Zisserman. Get out of my picture! Internet-based inpainting. In
British Machine Vision Conference, 2009.
240. M. Eitz, K. Hildebrand, T. Boubekeur, and M. Alexa. Photosketck: A sketch based image
query and composition. In ACM SIGGRAPH, 2009.
241. M. Eitz, R. Richter, K. Hildebrand, T. Boubekeur, and M. Alexa. Photosketcher: Interactive
sketch-based image synthesis.
Computer Graphics and Applications, IEEE, 31(6):56–66,
2011.
242. T. Chen, M. M. Cheng, P. Tan, A. Shamir, and S. M. Hu. Photosketch: Internet image montage.
In ACM SIGGRAPH Asia, 2009.
243. M. K. Johnson, K. Dale, S. Avidan, H. Pﬁster, W. T. Freeman, and W. Matusik. Cg2Real:
Improving the realism of computer generated images using a large collection of photographs.
Visualization and Computer Graphics, IEEE Transactions on, 17(9):1273–1285, 2011.

454
A Our Published Journal and Conference Papers Related to This Book
244. P. Weinzaepfel, H. Jegou, and P. Perez. Reconstructing an image from its local descriptors.
In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 337–
344, 2011.
245. M. Daneshi and J. Q. Guo. Image reconstruction based on local feature descriptors, 2011.
http://www.stanford.edu/class/ee368/project 11/reports.
246. M. A. Fischler and R. C. Bolles. Random sample consensus: A paradigm for model ﬁtting
with applications to image analysis and automated cartography. Comm. of the ACM, 24:381–
395, 1981.
247. D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal
of Computer Vision, 60:91–110, 2004.
248. S. Edelman, N. Intrator, and T. Poggio.
Complex cells and object recognition, 1997.
http://kybele.psych.cornell.edu.
249. Yan Ke and R. Sukthankar. PCA-SIFT: A more distinctive representation for local image
descriptors. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of
the 2004 IEEE Computer Society Conference on, volume 2, pages 506–513, 2004.
250. Gang Hua, M. Brown, and S. Winder. Discriminant embedding for local image descriptors.
In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1–8,
2007.
251. V. Chandrasekhar, G. Takacs, D. Chen, S. S. Tsai, J. Singh, and B. Girod. Transform coding of
image feature descriptors. In Visual Communication and Image Processing, SPIE Conference
on, volume 7257, 2009.
252. Chuohao Yeo, P. Ahammad, and K. Ramchandran.
Rate-efﬁcient visual correspondences
using random projections. In Image Processing, 2008. ICIP 2008. 15th IEEE International
Conference on, pages 217–220, 2008.
253. H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 33(1):117–128, 2011.
254. V. Chandrasekhar, G. Takacs, D. Chen, S. Tsai, R. Grzeszczuk, and B. Girod. CHoG: Com-
pressed histogram of gradients a low bit-rate feature descriptor.
In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 2504–2511, 2009.
255. V. Chandrasekhar, G. Takacs, D. Chen, S. S. Tsai, R. Grzeszczuk, and B. Girod. Compressed
histogram of gradients: A low-bitrate descriptor. 96:384–399, 2012.
256. G. Francini, S. Lepsoy, and M. Balestri. Description of test model under consideration for
CDVS, 2011. ISO/IEC JTC1/SC29/WG11, N12367, Geneva.
257. M. Makar, Chuo-Ling Chang, D. Chen, S.S. Tsai, and B. Girod.
Compression of image
patches for local feature extraction.
In Acoustics, Speech and Signal Processing, 2009.
ICASSP 2009. IEEE International Conference on, pages 821–824, 2009.
258. Jianshu Chao and E. Steinbach. Preserving Sift features in JPEG-encoded images. In Image
Processing (ICIP), 2011 18th IEEE International Conference on, pages 301–304, 2011.
259. J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in
videos. In Computer Vision, 2003. Proceedings. 9th IEEE International Conference on, pages
1470–1477, 2003.
260. J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Object retrieval with large vocab-
ularies and fast spatial matching. In Computer Vision and Pattern Recognition, 2007. CVPR
’07. IEEE Conference on, pages 1–8, 2007.
261. O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman. Total recall: Automatic query
expansion with a generative feature model for object retrieval. In Computer Vision, 2007.
ICCV 2007. IEEE 11th International Conference on, pages 1–8, 2007.
262. J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman. Lost in quantization: Improving
particular object retrieval in large-scale image databases. In Computer Vision and Pattern
Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8, 2008.
263. H. Jegou, M. Douze, and C. Schmid. Hamming embedding and weak geometric consistency
for large-scale image search. In Computer Vision, European Conference on, 2008.
264. W. G. Zhou, Y. J. Lu, H. Q. Li, and Q. Tian. Scalar quantization for large-scale image search.
In ACM Multimedia, 2012.

References
455
265. R. Talluri. Image alignment and stitching: A tutorial. Foundations and Trends in Computer
Graphics and Vision, 2:1–104, 2012.
266. P. Torr and A. Zisserman. MLESAC: A new robust estimator with application to estimating
image geometry. Journal of Computer Vision and Image Understanding, 78:138–156, 2000.
267. P. H. S. Torr and C. Davidson. IMPSAC: Synthesis of importance sampling and random sam-
ple consensus. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 25(3):354–
364, 2003.
268. O. Chum and J. Matas. Matching with PROSAC–Progressive sample consensus. In Computer
Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on,
volume 1, pages 220–226, vol. 1, 2005.
269. H. Jegou and M. Douze.
Inria holiday dataset, 2008.
http://lear.inrialpes.fr/people/
jegou/data.php.
270. P. Perez, M. Gangnet, and A. Blake. Poisson image editing. Graphics, ACM Transactions on,
22:313–318, 2003.
271. ITU-T. Methodology for the subjective assessment of the quality of television pictures, 2002.
Recommendation ITU-R BT. 5000-11.
272. Z. Farbman, R. Fattal, and D. Lischinski. Convolution pyramids. In ACM SIGGRAPH Asia,
2011.
273. L. Dai, H. Yue, X. Sun, and F. Wu. IMShare: Instantly sharing your mobile landmark images
by search-based reconstruction. In ACM Multimedia, pages 579–588, 2012.
274. H. Shao, T. Svoboda, and L. V. Gool. Zubud-Zurich buildings database for image based
recognition, 2003. Technical Report 206, ETH Zurich.
275. Josh Ong. Picture this: Chinese Internet giant Tencent’s Qzone social network now hosts over
150 billion photos, 2012. http://thenextweb.com/asia.
276. Microsoft. Windows 8 launch, 2012. http://www.microsoft.com.
277. Yurij S. Musatenko and Vitalij N. Kurashov. Correlated image set compression system based
on new fast efﬁcient algorithm of Karhunen-Lo`eve transform. volume 3527, pages 518–529,
1998.
278. Kosmas Karadimitriou and John M. Tyler.
The centroid method for compressing sets of
similar images. Pattern Recognition Letters, 19(7):585–593, 1998.
279. Samy Ait-Aoudia and Abdelhalim Gabis. A comparison of set redundancy compression tech-
niques. EURASIP J. Appl. Signal Process., 2006:1–13, January 2006.
280. Chi-Ho Yeung, O. C. Au, Ketan Tang, Zhiding Yu, Enming Luo, Yannan Wu, and Shing-Fat
Tu. Compressing similar image sets using low frequency template. In Multimedia and Expo
(ICME), 2011 IEEE International Conference on, pages 1–6, 2011.
281. Cheng H., Li X., and Schmieder A. A study of clustering algorithm and validity for lossy
image set compression.
In Image Processing, Computer Vision and Pattern Recognition,
2009 International Conference on, pages 501–506, 2009.
282. Yang Lu, Tien-Tsin Wong, and Pheng-Ann Heng. Digital photo similarity analysis in fre-
quency domain and photo album compression. In Proceedings of the 3rd International Con-
ference on Mobile and Ubiquitous Multimedia, MUM ’04, pages 237–244, 2004.
283. Chia-Ping Chen, Chu-Song Chen, Kuo-Liang Chung, H.-I. Lu, and G.Y. Tang. Image set
compression through minimal-cost prediction structure. In Image Processing, 2004. ICIP
’04. 2004 International Conference on, volume 2, pages 1289–1292, Vol. 2, 2004.
284. O. Au, Sijin Li, Ruobing Zou, Wei Dai, and Lin Sun. Digital photo album compression based
on global motion compensation and intra/inter prediction. In Audio, Language and Image
Processing (ICALIP), 2012 International Conference on, pages 84–90, 2012.
285. Ruobing Zou, Oscar C. Au, Guyue Zhou, Wei Dai, Wei Hu, and Pengfei Wan. Personal
photo album compression and management. In Circuits and Systems (ISCAS), 2013 IEEE
International Symposium on, pages 1428–1431, 2013.
286. A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain. Content-based image
retrieval at the end of the early years. Pattern Analysis and Machine Intelligence, IEEE Trans-
actions on, 22(12):1349–1380, 2000.

456
A Our Published Journal and Conference Papers Related to This Book
287. Yong Rui, T. S. Huang, M. Ortega, and S. Mehrotra. Relevance feedback: A power tool for
interactive content-based image retrieval. Circuits and Systems for Video Technology, IEEE
Transactions on, 8(5):644–655, 1998.
288. H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robust features. In Computer Vision,
2006. European Conference on, pages 404–417, 2006.
289. Huanjing Yue, Xiaoyan Sun, Jingyu Yang, and Feng Wu. Cloud-based image coding for
mobile devices — Toward thousands to one compression. Multimedia, IEEE Transactions
on, 15(4):845–857, 2013.
290. J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem.
Proceedings of American Mathematical Society, 7:48–50, 1956.
291. Y. J. Chu and T. H. Liu. On the shortest arborescence of a directed graph. Science Sinica,
14:1396–1400, 1965.
292. H. Isack and Y. Boykov.
Energy-based geometric multi-model ﬁtting.
Computer Vision,
International Journal of, 97(2):123–147, 2012.
293. Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(11):1222–1239, 2001.
294. Alhwarin, Faraj, Danijela Ristic-Durrant, and Axel Graser. VF-SIFT: Very fast SIFT feature
matching. Pattern Recognition, International Journal on, pages 222–231, 2010.
295. D. L. Donoho. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289–
1306, 2006.
296. R. G. Baraniuk. Compressive sensing [lecture notes]. Signal Processing Magazine, IEEE,
24(4):118–121, 2007.
297. E. J. Candes and M. B. Wakin. An introduction to compressive sampling. Signal Processing
Magazine, IEEE, 25(2):21–30, 2008.
298. R. Cristescu, B. Beferull-Lozano, M. Vetterli, and R. Wattenhofer.
Network correlated
data gathering with explicit communication: NP-completeness and algorithms. Networking,
IEEE/ACM Transactions on, 14(1):41–54, 2006.
299. A. Ciancio, S. Pattem, A. Ortega, and B. Krishnamachari. Energy-efﬁcient data represen-
tation and routing for wireless sensor networks based on a distributed wavelet compression
algorithm. In Information Processing in Sensor Networks, 2006. IPSN 2006. The Fifth Inter-
national Conference on, pages 309–316, 2006.
300. J. Acimovic, B. Beferull-Lozano, and R. Cristescu. Adaptive distributed algorithms for power-
efﬁcient data gathering in sensor networks. In Wireless Networks, Communications and Mo-
bile Computing, 2005 International Conference on, volume 2, pages 946–951, vol. 2, 2005.
301. S. Yoon and C. Shahabi. The clustered aggregation (CAG) technique leveraging spatial and
temporal correlations in wireless sensor networks. Sensor Networks, ACM Transactions on,
3(8), 2007.
302. H. Gupta, V. Navda, S. Das, and V. Chowdhary. Efﬁcient gathering of correlated data in sensor
network. Sensor Networks, ACM Transactions on, 4, 2008.
303. J. Chou, D. Petrovic, and Kannan Ramachandran. A distributed and adaptive signal pro-
cessing approach to reducing energy consumption in sensor networks. In INFOCOM 2003.
22nd Annual Joint Conference of the IEEE Computer and Communications. IEEE Societies,
volume 2, pages 1054–1062, 2003.
304. R. Cristescu, B. Beferull-Lozano, and M. Vetterli. On network correlated data gathering. In
INFOCOM 2004. 23rd Annual Joint Conference of the IEEE Computer and Communications
Societies, volume 4, pages 2571–2582, 2004.
305. G. Hua and W. Chen. Correlated data gathering in wireless sensor networks based on dis-
tributed source coding. International Journal of Sensor Networks, 4:13–22, 2008.
306. D. Slepian and J. K. Wolf. Noiseless coding of correlated information sources. Information
Theory, IEEE Transactions on, 19(4):471–480, 1973.
307. K. Yuen, Ben Liang, and Baochun Li. A distributed framework for correlated data gathering
in sensor networks. Vehicular Technology, IEEE Transactions on, 57(1):578–593, 2008.
308. W. Bajwa, J. Haupt, A. Sayeed, and R. Nowak. Compressive wireless sensing. In Information
Processing in Sensor Networks, 2006. IPSN 2006. The Fifth International Conference on,
pages 134–142, 2006.

References
457
309. M. Rabbat, J. Haupt, A. Singh, and R. Nowak. Decentralized compression and predistribution
via randomized gossiping. In Information Processing in Sensor Networks, 2006. IPSN 2006.
The Fifth International Conference on, pages 51–59, 2006.
310. J. Haupt, W. U. Bajwa, M. Rabbat, and R. Nowak. Compressed sensing for networked data.
Signal Processing Magazine, IEEE, 25(2):92–101, 2008.
311. E. J. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruc-
tion from highly incomplete frequency information. Information Theory, IEEE Transactions
on, 52(2):489–509, 2006.
312. D. L. Donoho, M. Elad, and V. N. Temlyakov. Stable recovery of sparse overcomplete repre-
sentations in the presence of noise. Information Theory, IEEE Transactions on, 52(1):6–18,
2006.
313. P. Gupta and P. R. Kumar. The capacity of wireless networks. Information Theory, IEEE
Transactions on, 46(2):388–404, 2000.
314. D. Marco, E. Duarte-Melo, M. Liu, and D. Neuhoff. On the many-to-one transport capacity of
a dense wireless sensor network and the compressibility of its data. In Information Processing
in Sensor Networks, 2003. IPSN 2003. The Fifth International Conference on, pages 1–16,
2003.
315. S. McCanne and S. Floyd. Network simulator NS-2. http://www.isi.edu/nsnam/ns/.
316. C. Chen and Y. Wang. Chain-type wireless sensor network for monitoring long range infras-
tructures: Architecture and protocols. International Journal on Distributed Sensor Networks,
4, 2008.
317. S. Tilak, N. Abu-Gahazaleh, and W. Heinzelman. Infrastructure trade-offs for sensor net-
works. In Proc. of ACM IWWSNA, pages 49–58, 2002.
318. NBDC. CTD data. http://tao.noaa.gov/refreshed/ctd delivery.php.
319. A. Martinez, A. Guillen i Fabregas, G. Caire, and F. M. J. Willems. Bit-interleaved coded
modulation revisited: A mismatched decoding perspective. Information Theory, IEEE Trans-
actions on, 55(6):2756–2765, 2009.
320. G. Holland, N. Vaidya, and P. Bahl. A rate-adaptive MAC protocol for multi-hop wireless
networks. In ACM MobiCom, pages 236–251, 2001.
321. G. Judd, X. Wang, and P. Steenkiste. Efﬁcient channel-aware rate adaptation in dynamic
environments. In ACM MobiSys, pages 118–131, 2008.
322. R. T. Morris, J. C. Bicket, and J. C. Bicket. Bit-rate selection in wireless networks, 2005.
Technical report, Masters thesis, MIT.
323. S. H. Y. Wong, H. Yang, S. Lu, and V. Bharghavan. Robust rate adaptation for 802.11 wireless
networks. In ACM MobiCom, pages 146–157, 2006.
324. M. Vutukuru, H. Balakrishnan, and K. Jamieson. Cross-layer wireless bit rate adaptation. In
ACM SIGCOMM, pages 3–14, 2009.
325. J. D. Brown, S. Pasupathy, and K. N. Plataniotis. Adaptive demodulation using rateless erasure
codes. Communications, IEEE Transactions on, 54(9):1574–1585, 2006.
326. A. Shokrollahi. Raptor codes. Information Theory, IEEE Transactions on, 52(6):2551–2567,
2006.
327. G. Caire, Giorgio Taricco, and Ezio Biglieri. Bit-interleaved coded modulation. Information
Theory, IEEE Transactions on, 44(3):927–946, 1998.
328. M. Luby. Lt codes. In In FOCS ’02: Proceedings of the 43rd Symposium on Foundations of
Computer Science, page 271, 2002.
329. D. Baron, Shriram Sarvotham, and R. G. Baraniuk. Bayesian compressive sensing via belief
propagation. Signal Processing, IEEE Transactions on, 58(1):269–280, 2010.
330. Macro Pretti. A message-passing algorithm with damping. Journal of Statistic Mech., 2005.
331. IEEE. IEEE standard for information technology — telecommunications and information ex-
change between systems — local and metropolitan area networks — speciﬁc requirements.
part 11: Wireless LAN medium access control (MAC) and physical layer (PHY) speciﬁca-
tions, 2007. IEEE Std 802.11-2007.
332. K. Tan, J. Zhang, J. Fang, H. Liu, Y. Ye, S. Wang, Y. Zhang, H. Wu, W. Wang, and G. M.
Voelker. SORA: High performance software radio using general purpose multi-core proces-
sors. In NSDI, USENIX Association, pages 75–90, 2009.

458
A Our Published Journal and Conference Papers Related to This Book
333. G. Ungerboeck. On improving data-link performance by increasing channel alphabet and
introducing sequence coding. In IEEE International Symposium on Information Theory, 1976.
334. G. Ungerboeck. Channel coding with multilevel/phase signals. Information Theory, IEEE
Transactions on, 28(1):55–66, 1982.
335. H. Imai and S. Hirakawa. A new multilevel coding method using error-correcting codes.
Information Theory, IEEE Transaction on, 23(3):371–377, 1977.
336. A. R. Calderbank. Multilevel codes and multistage decoding. Communications, IEEE Trans-
actions on, 37(3):222–229, 1989.
337. U. Wachsmann, R. F. H. Fischer, and J. B. Huber. Multilevel codes: Theoretical concepts and
practical design rules. Information Theory, IEEE Transactions on, 45(5):1361–1391, 1999.
338. E. Zehavi. 8-PSK trellis codes for a Rayleigh channel. Communications, IEEE Transactions
on, 40(5):873–884, 1992.
339. Shihao Ji, Ya Xue, and L. Carin. Bayesian compressive sensing. Signal Processing, IEEE
Transactions on, 56(6):2346–2356, 2008.
340. M. W. Seeger and H. Nickisch. Compressed sensing and Bayesian experimental design. In
ACM ICML, pages 912–919, 2008.
341. P. Schniter, L. C. Potter, and J. Ziniel. Fast Bayesian matching pursuit. In Information Theory
and Applications Workshop, 2008, pages 326–333, 2008.
342. Shriram Sarvotham, D. Baron, and R. G. Baraniuk. Sudocodes: Fast measurement and recon-
struction of sparse signals. In Information Theory, 2006 IEEE International Symposium on,
pages 2804–2808, 2006.
343. F. Wu, J. Fu, Z. Lin, and B. Zeng. Analysis on rate-distortion performance of compressive
sensing for binary sparse source. In Data Compression Conference, pages 113–122, 2009.
344. Panagiotis Destounis, John D. Garofalakis, Panagiotis Kappos, and J. Tzimas. Measuring the
mean Web page size and its compression to limit latency and improve download time. Internet
Research, 11(1):10–17, 2001.
345. Hao Cui, Chong Luo, Kun Tan, Feng Wu, and Chang Wen Chen. Seamless rate adaptation for
wireless networking. In Proceedings of the 14th ACM MSWiM ’11, pages 437–446, 2011.
346. Aditya Gudipati and Sachin Katti. Strider: Automatic rate adaptation and collision handling.
In Proceedings of the ACM SIGCOMM 2011 Conference, pages 158–169, 2011.
347. Jonathan Perry, Hari Balakrishnan, and Devavrat Shah. Rateless spinal codes. In Proceedings
of the 10th ACM Workshop on Hot Topics in Networks, HotNets ’11, pages 6:1–6:6, 2011.
348. R. Hamzaoui, V. Stankovic, and Zixiang Xiong. Optimized error protection of scalable image
bit streams [advances in joint source-channel coding for images]. Signal Processing Maga-
zine, IEEE, 22(6):91–107, November 2005.
349. L. P. Kondi, F. Ishtiaq, and A. K. Katsaggelos.
Joint source-channel coding for motion-
compensated DCT-based SNR scalable video.
Image Processing, IEEE Transactions on,
11(9):1043–1052, September 2002.
350. Zhihai He, Jianfei Cai, and Chang Wen Chen. Joint source channel rate-distortion analysis
for adaptive mode selection and rate control in wireless video coding. Circuits and Systems
for Video Technology, IEEE Transactions on, 12(6):511–523, June 2002.
351. Qian Zhang, Wenwu Zhu, Zu Ji, and Ya-Qin Zhang. A power-optimized joint source channel
coding for scalable video streaming over wireless channel. In Proceedings of IEEE ISCAS
2001, volume 5, pages 137–140 vol. 5, 2001.
352. S. S. Arslan, P. C. Cosman, and L. B. Milstein. Generalized unequal error protection lt codes
for progressive data transmission. Image Processing, IEEE Transactions on, PP(99):1, 2012.
353. Xiang Pan, A. Cuhadar, and A. H. Banihashemi. Combined source and channel coding with
JPEG 2000 and rate-compatible low-density parity-check codes. Signal Processing, IEEE
Transactions on, 54(3):1160–1164, March 2006.
354. S. Aditya and S. Katti. Flexcast: Graceful wireless video streaming. In Proceedings of the
17th Annual International Conference on Mobile Computing and Networking, 2011.
355. O. Y. Bursalioglu, G. Caire, and D. Divsalar. Joint source-channel coding for deep space
image transmission using rateless codes. In Information Theory and Applications Workshop
(ITA), 2011, pages 1–10, February 2011.

References
459
356. A. D. Liveris, Zixiang Xiong, and C. N. Georghiades.
Compression of binary sources
with side information at the decoder using LDPC codes. Communications Letters, IEEE,
6(10):440–442, 2002.
357. M. Sartipi and F. Fekri. Source and channel coding in wireless sensor networks using LDPC
codes. In Proceedings of IEEE SECON 2004, pages 309–316, October 2004.
358. Wei Zhong and Javier Garcia-Frias. LDGM codes for channel coding and joint source-channel
coding of correlated sources. EURASIP J. Appl. Signal Process., 2005:942–953, January
2005.
359. Qian Xu, V. Stankovic, and Zixiang Xiong. Distributed joint source-channel coding of video
using raptor codes. Selected Areas in Communications, IEEE Journal on, 25(4):851–861,
2007.
360. G. Caire, S. Shamai, and S. Verdu.
A new data compression algorithm for sources with
memory based on error correcting codes. In IEEE ITW, pages 291–295, March 4-April 2003.
361. G. Caire, S. Shamai, A. Shokrollahi, and S. Verdu. Universal variable-length data compression
of binary sources using fountain codes. In IEEE ITW, pages 123–128, October 2004.
362. J. DelSer, P. M. Crespo, I. Esnaola, and J. Garcia-Frias. Joint source-channel coding of sources
with memory using Turbo codes and the Burrows-Wheeler Transform.
Communications,
IEEE Transactions on, 58(7):1984–1992, July 2010.
363. M. Fresia, F. Perez-Cruz, and H. V. Poor. Optimized concatenated LDPC codes for joint
source-channel coding. In IEEE ISIT, pages 2131–2135, June 28-July 3, 2009.
364. Guang-Chong Zhu and F. Alajaji. Turbo codes for nonuniform memoryless sources over noisy
channels. Communications Letters, IEEE, 6(2):64–66, February 2002.
365. F. Cabarcas, R. D. Souza, and J. Garcia-Frias. Turbo coding of strongly nonuniform mem-
oryless sources with unequal energy allocation and pam signaling. Signal Processing, IEEE
Transactions on, 54(5):1942–1946, May 2006.
366. S. Nanda, K. Balachandran, and S. Kumar. Adaptation techniques in wireless packet data
services. Communications Magazine, IEEE, 38(1):54–64, January 2000.
367. Qiuyan Xia and M. Hamdi. Smart sender: A practical rate adaptation algorithm for multirate
IEEE 802.11 WLANs. Wireless Communications, IEEE Transactions on, 7(5):1764–1775,
May 2008.
368. Yang Song, Xiaoyan Zhu, Yuguang Fang, and Hailin Zhang. Threshold optimization for rate
adaptation algorithms in IEEE 802.11 WLANs. Wireless Communications, IEEE Transac-
tions on, 9(1):318–327, January 2010.
369. Su Min Kim, Wan Choi, Tae Won Ban, and Dan Keun Sung. Optimal rate adaptation for
hybrid ARQ in time-correlated Rayleigh fading channels. Wireless Communications, IEEE
Transactions on, 10(3):968–979, March 2011.
370. D. N. Rowitch and L. B. Milstein. On the performance of hybrid FEC/ARQ systems using
rate compatible punctured turbo (RCPT) codes. Communications, IEEE Transactions on,
48(6):948–959, June 2000.
371. E. Soijanin, N. Varnica, and P. Whiting. Punctured vs. rateless codes for HARQ. In IEEE
ITW, pages 155–159, March 2006.
372. E. Candes. Compressive sampling. In Proc. of the Intl. Congress of Mathematicians, pages
1433–1452, March 2006.
373. D. L. Donoho and Y. Tsaig. Fast solution of l1-norm minimization problems when the solution
may be sparse. IEEE TIT, 54(11):4789–4812, November 2008.
374. J. A. Tropp and A. C. Gilbert. Signal recovery from random measurements via orthogonal
matching pursuit. IEEE TIT, 53(12):4655–4666, December 2007.
375. Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing.
Applied and Computational Harmonic Analysis, 27(3):265–274, 2009.
376. Wei Dai and O. Milenkovic. Subspace pursuit for compressive sensing signal reconstruction.
IEEE TIT, 55(5):2230–2249, May 2009.
377. Marco F. Duarte, Michael B. Wakin, Dror Baron, and Richard G. Baraniuk. Universal dis-
tributed sensing via random projections. In IEEE IPSN, pages 177–185, 2006.
378. W. Bajwa, J. Haupt, A. Sayeed, and R. Nowak. Joint source-channel communication for
distributed estimation in sensor networks. IEEE TIT, 53(10):3629–3653, October 2007.

460
A Our Published Journal and Conference Papers Related to This Book
379. S. Feizi, M. Medard, and M. Effros. Compressive sensing over networks. In 48th Annual
Allerton Conference on Communication, Control, and Computing, pages 112–1136, October
2010.
380. S. Feizi and M. Medard. A power efﬁcient sensing/communication scheme: Joint source-
channel-network coding by using compressive sensing. In 49th Annual Allerton Conference
on Comm., Control, and Comp., pages 1048–1054, September 2011.
381. F. Chen, F. Lim, O. Abari, A. Chandrakasan, and V. Stojanovi´c. Energy-aware design of com-
pressed sensing systems for wireless sensors under performance and reliability constraints.
Circuits and Systems I, IEEE Transactions on, 2013.
382. Xiao Lin Liu, Chong Luo, and Feng Wu. Formulating binary compressive sensing decoding
with asymmetrical property. In Data Compression Conference (DCC), 2011, pages 213–222,
March 2011.
383. Giuseppe Caire, Shlomo Shitz Shamai, A Shokrollahi, and Sergio Verdu. Fountain codes
for lossless data compression. In DIMACS Series in Discrete Mathematics and Theoretical
Computer Science, number 68, December 2005.
384. S. Rangan.
Generalized approximate message passing for estimation with random linear
mixing. In IEEE ISIT, pages 2168–2172, 2011.
385. S. ten Brink. Convergence behavior of iteratively decoded parallel concatenated codes. Com-
munications, IEEE Transactions on, 49(10):1727–1737, October 2001.
386. S. ten Brink, G. Kramer, and A. Ashikhmin. Design of low-density parity-check codes for
modulation and detection. Communications, IEEE Transactions on, 52(4):670–678, April
2004.
387. IEEE Standard for Local and Metropolitan Area Networks—Part 16: Air Interface for Broad-
band Wireless Access Systems. IEEE Std 802.16-2009, May 2009.
388. Xi Chen, P. Gangwal, and D. Qiao. Practical rate adaptation in mobile environments. In Per-
vasive Computing and Communications, 2009. PerCom 2009. IEEE International Conference
on, pages 1–10, 2009.
389. A. Aaron, Rui Zhang, and B. Girod. Wyner-Ziv coding of motion video. In Signals, Systems
and Computers, 2002. Conference Record of the 36th Asilomar Conference on, volume 1,
pages 240–244, 2002.
390. B. Girod, A. M. Aaron, S. Rane, and D. Rebollo-Monedero. Distributed video coding. Pro-
ceedings of the IEEE, 93(1):71–83, 2005.
391. R. Puri and K. Ramchandran. PRISM: A new robust video coding architecture based on
distributed compression principles. In the Annual Allerton Conference on Communication
Control and Computing, 2002.
392. Rohit Puri, A. Majumdar, and K. Ramchandran. PRISM: A video coding paradigm with
motion estimation at the decoder. Image Processing, IEEE Transactions on, 16(10):2436–
2448, 2007.
393. A. D. Wyner and J. Ziv. The rate-distortion function for source coding with side information
at the decoder. Information Theory, IEEE Transactions on, 22(1):1–10, 1976.
394. Qian Xu and Zixiang Xiong. Layered Wyner-Ziv video coding. Image Processing, IEEE
Transactions on, 15(12):3791–3803, 2006.
395. S. Jakubczak and D. Katabi. SoftCast: One-size-ﬁts-all wireless video. In ACM SIGCOMM
Computer Communication Review, 2010.
396. Szymon Jakubczak and Dina Katabi. A cross-layer design for scalable mobile video. In Proc.
of the 17th Annual International Conference on Mobile Computing and Networking, pages
289–300. ACM, 2011.
397. Ruiqin Xiong, Jizheng Xu, Feng Wu, and Shipeng Li. Barbell-lifting based 3-D wavelet cod-
ing scheme. Circuits and Systems for Video Technology, IEEE Transactions on, 17(9):1256–
1269, 2007.
398. Yixuan Zhang, Ce Zhu, and Kim-Hui Yap. A joint source-channel video coding scheme based
on distributed source coding. Multimedia, IEEE Transactions on, 10(8):1648–1656, 2008.
399. Mei Guo, Zixiang Xiong, Feng Wu, Debin Zhao, Xiangyang Ji, and Wen Gao. Witsenhausen-
Wyner video coding.
Circuits and Systems for Video Technology, IEEE Transactions on,
21(8):1049–1060, 2011.

References
461
400. X. Fan, F. Wu, and D. Zhao. D-Cast: DSC based soft mobile video broadcast. In Proceedings
of the 10th International Conference on Mobile and Ubiquitous Multimedia, 2011.
401. Xiaopeng Fan, Feng Wu, Debin Zhao, O. C. Au, and Wen Gao. Distributed soft video broad-
cast (DCAST) with explicit motion. In Data Compression Conference (DCC), 2012, pages
199–208, 2012.
402. J. Garcia-Frias. Compression of correlated binary sources using Turbo codes. Communica-
tions Letters, IEEE, 5(10):417–419, 2001.
403. A. Aaron, S. Rane, E. Setton, and B. Girod. Transform-domain Wyner-Ziv codec for video.
In SPIE Visual Communications and Image Processing, 2004.
404. Xun Guo, Yan Lu, Feng Wu, Debin Zhao, and Wen Gao. Wyner-Ziv-Based multiview video
coding. Circuits and Systems for Video Technology, IEEE Transactions on, 18(6):713–724,
2008.
405. M. Tagliasacchi, A. Trapanese, S. Tubaro, J. Ascenso, C. Brites, and F. Pereira. Intra mode
decision based on spatio-temporal cues in pixel domain Wyner-Ziv video coding. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International
Conference on, volume 2, pages II–II, 2006.
406. J. Slowack, S. Mys, J. Skorupa, N. Deligiannis, P. Lambert, A. Munteanu, and R. Van
de Walle. Rate-distortion driven decoder-side bitplane mode decision for distributed video
coding. Signal Processing: Image Communication, 25(9):660–673, 2010.
407. S. Benierbah and M. Khamadja. Generalized hybrid intra and Wyner-Ziv video coding. Cir-
cuits and Systems for Video Technology, IEEE Transactions on, 21(12):1929–1934, 2011.
408. A. Aaron, S. Rane, and B. Girod. Wyner-Ziv video coding with hash-based motion compen-
sation at the receiver. In Image Processing, 2004. ICIP ’04. 2004 International Conference
on, volume 5, pages 3097–3100, 2004.
409. R. Martins, C. Brites, J. Ascenso, and F. Pereira. Reﬁning side information for improved
transform domain Wyner-Ziv video coding. Circuits and Systems for Video Technology, IEEE
Transactions on, 19(9):1327–1341, 2009.
410. B. Macchiavello, D. Mukherjee, and R. L. De Queiroz. Iterative side-information generation
in a mixed resolution Wyner-Ziv framework. Circuits and Systems for Video Technology,
IEEE Transactions on, 19(10):1409–1423, 2009.
411. X. Fan, O. C. Au, N. M. Cheung, Y. Chen, , and J. Zhou. Successive reﬁnement based Wyner-
Ziv video compression. Signal Processing: Image Communication, 25(1):47–63, 2010.
412. Wei Liu, Lina Dong, and Wenjun Zeng.
Motion reﬁnement based progressive side-
information estimation for Wyner-Ziv video coding. Circuits and Systems for Video Tech-
nology, IEEE Transactions on, 20(12):1863–1875, 2010.
413. C. Brites and F. Pereira. Correlation noise modeling for efﬁcient pixel and transform domain
Wyner-Ziv video coding. Circuits and Systems for Video Technology, IEEE Transactions on,
18(9):1177–1190, 2008.
414. Xiaopeng Fan, O. C. Au, and Ngai-Man Cheung. Transform-domain adaptive correlation
estimation (trace) for Wyner-Ziv video coding. Circuits and Systems for Video Technology,
IEEE Transactions on, 20(11):1423–1436, 2010.
415. N. Deligiannis, J. Barbarien, M. Jacobs, A. Munteanu, A. Skodras, and P. Schelkens. Side-
information-dependent correlation channel estimation in hash-based distributed video coding.
Image Processing, IEEE Transactions on, 21(4):1934–1949, 2012.
416. G. R. Esmaili and P. C. Cosman. Wyner-Ziv video coding with classiﬁed correlation noise
estimation and key frame coding mode selection. Image Processing, IEEE Transactions on,
20(9):2463–2474, 2011.
417. Y. Kochman and R. Zamir. Joint Wyner-Ziv/Dirty-Paper coding by modulo-lattice modula-
tion. Information Theory, IEEE Transactions on, 55(11):4878–4889, 2009.
418. T. Kratochvil. Hierarchical modulation in DVB-T/H mobile tv transmission. In Multi-Carrier
Systems & Solutions, pages 333–341, 2009.
419. W. K. Pratt. Median ﬁltering, 1975. Technical report, Image Processing Institute, University
of Southern California.

462
A Our Published Journal and Conference Papers Related to This Book
420. K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-D transform-
domain collaborative ﬁltering. Image Processing, IEEE Transactions on, 16(8):2080–2095,
August 2007.
421. A. Buades, B. Coll, and J. Morel. A review of image denoising algorithms, with a new one.
Multiscale Modeling and Simulation, 4(2):490–530, 2005.
422. A. Buades, B. Coll, and J. Morel. Image denoising methods. A new nonlocal principle. SIAM
Review, 52(1):113–147, 2010.
423. S. Smith and J. Brady. SUSANA–A new approach to low level image processing. Interna-
tional Journal of Computer Vision, 23:45–78, 1997.
424. C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and color images. In Computer Vision,
1998. Sixth International Conference on, pages 839–846, January 1998.
425. S. G. Chang, Bin Yu, and M. Vetterli. Adaptive wavelet thresholding for image denoising and
compression. Image Processing, IEEE Transactions on, 9(9):1532–1546, September 2000.
426. Jean-Luc Starck, E. J. Candes, and D. L. Donoho. The curvelet transform for image denoising.
Image Processing, IEEE Transactions on, 11(6):670–684, June 2002.
427. Kyong-Hwa Lee and D. Petersen. Optimal linear coding for vector channels. Communica-
tions, IEEE Transactions on, 24(12):1283–1290, December 1976.
428. Yiqiu Dong and Shufang Xu. A new directional weighted median ﬁlter for removal of random-
valued impulse noise. Signal Processing Letters, IEEE, 14(3):193–196, March 2007.
429. X264, http://www.videolan.org/developers/x264.html.
430. BM3D algorithm and its extensions. Technical report, http://www.cs.tut.ﬁ/ foi/GCF-BM3D/.
431. G. Cheung and A. Zakhor. Bit allocation for joint source/channel coding of scalable video.
Image Processing, IEEE Transactions on, 9(3):340–356, March 2000.
432. Jr. Goblick, T. Theoretical limitations on the transmission of data from analog sources. Infor-
mation Theory, IEEE Transactions on, 11(4):558–567, October 1965.
433. M. Gastpar, B. Rimoldi, and M. Vetterli.
To code, or not to code: Lossy source-channel
communication revisited. Information Theory, IEEE Transactions on, 49(5):1147–1158, May
2003.
434. M. Skoglund, N. Phamdo, and F. Alajaji. Design and performance of VQ-based hybrid digital-
analog joint source-channel codes. Information Theory, IEEE Transactions on, 48(3):708–
720, March 2002.
435. K. Narayanan, M. P. Wilson, and G. Caire. Joint source channel coding with side information
using hybrid digital analog codes. Information Theory, IEEE Transactions on, 56:4922–4940,
July 2010.
436. Y. Kochman and R. Zamir. Analog matching of colored sources to colored channels. Infor-
mation Theory, IEEE Transactions on, 57(6):3180–3195, June 2011.
437. Z. Yang and Y. Zhao. Scalable video multicast over multi-antenna OFDM systems. Wireless
Personal Communications, pages 1–18, 2012.
438. W. Ji, Z. Li, and Y. Chen. Joint source-channel coding and optimization for layered video
broadcasting to heterogeneous devices. IEEE Trans. on Multimedia, 14(2):443–455, 2012.
439. IEEE Std. 802.11n-2009: Enhancements for Higher Throughput, 2009.
440. J. G. Andrews, A. Ghosh, and R. Muhamed. Fundamentals of WiMAX: Understanding broad-
band wireless networking. Prentice Hall PTR, 2007.
441. J. Lee, J. K. Han, and J. Zhang.
MIMO technologies in 3GPP LTE and LTE-advanced.
EURASIP Journal on Wireless Communications and Networking, 2009:3, 2009.
442. C. Nokes and J. Mitchell. Potential beneﬁts of hierarchical modes of the DVB-T speciﬁcation.
In IEE Digest, volume 10, 1999.
443. Szymon Jakubczak and Dina Katabi. SoftCast: One-size-ﬁts-all wireless video. In HotNets,
2009.
444. C.-H. Kuo and C.-C. J. Kuo. An embedded space-time coding (STC) scheme for broadcasting.
Broadcasting, IEEE Transactions on, 53(1):48–58, March 2007.
445. C. H. Kuo, C. M. Wang, and J. L. Lin. Cooperative wireless broadcast for scalable video
coding. IEEE Trans. on Circuits and Systems for Video Technology, 21(6):816–824, 2011.
446. C. Bilen, E. Erkip, and Y. Wang. Layered video multicast using diversity embedded space
time codes. In Sarnoff Symposium, pages 1–5. IEEE, 2009.

References
463
447. S. Chang, M. Rim, P. Cosman, and L. Milstein. Superposition MIMO coding for the broadcast
of layered sources. IEEE Trans. on Communications, (99):1–9, 2011.
448. E. J. Candes, M. B. Wakin, and S. P. Boyd. Enhancing sparsity by reweighted L1 minimiza-
tion. Journal of Fourier Analysis and Applications, 14(5):877–905, 2008.
449. Xiao Lin Liu, Chong Luo, Wenjun Hu, and Feng Wu. Compressive broadcast in MIMO sys-
tems with receive antenna heterogeneity. In IEEE INFOCOM’12, pages 3011–3015, March
2012.
450. G. J. Foschini and M. J. Gans. On limits of wireless communications in a fading environment
when using multiple antennas. Wireless Personal Communications, 6(3):311–335, 1998.
451. P. W. Wolniansky, G. J. Foschini, G. D. Golden, and R. A. Valenzuela. V-BLAST: An archi-
tecture for realizing very high data rates over the rich-scattering wireless channel. In Proc. of
URSI International Symposium on Signals, Systems, and Electronics, pages 295–300. IEEE,
1998.
452. V. Tarokh, N. Seshadri, and A. R. Calderbank. Space-time codes for high data rate wireless
communication: Performance criterion and code construction. IEEE Trans. on Information
Theory, 44(2):744–765, 1998.
453. S. M. Alamouti. A simple transmit diversity technique for wireless communications. IEEE
JSAC, 16(8):1451–1458, October 1998.
454. L. Zheng and D. N. C. Tse. Diversity and multiplexing: A fundamental trade-off in multiple-
antenna channels. IEEE Trans. on Information Theory, 49(5):1073–1096, 2003.
455. H. Schwarz, D. Marpe, and T. Wiegand. Overview of the scalable video coding extension of
the H.264/AVC standard. IEEE Transactions on CSVT, 17(9):1103–1120, September 2007.
456. M. W. Marcellin, M. J. Gormish, A. Bilgin, and M. P. Boliek. An overview of JPEG-2000. In
Proc. of Data Compression Conference, pages 523–541. IEEE, 2000.
457. A. Skodras, C. Christopoulos, and T. Ebrahimi. The JPEG 2000 still image compression
standard. IEEE Signal Processing Magazine, 18(5):36–58, 2001.
458. P. Luo. Hierarchical modulation for the downlink of MIMO multi-user channels. In Proc. of
1st International Conference on CCSP with Special Track on Biomedical Engineering, pages
77–80. IEEE, 2005.
459. A. M. C. Correia, J. C. M. Silva, N. M. B. Souto, L. A. C. Silva, A. B. Boal, and A. B. Soares.
Multi-resolution broadcast/multicast systems for MBMS. Broadcasting, IEEE Transactions
on, 53(1):224–234, March 2007.
460. J. Prades-Nebot, Y. Ma, and T. Huang. Distributed video coding using compressive sampling.
In Proc. of Picture Coding Symposium, 2009, pages 1–4. IEEE, 2009.
461. C. Li, H. Jiang, P. Wilford, and Y. Zhang. Video coding using compressive sensing for wireless
communications. In Proc. of Wireless Communications and Networking Conference, pages
2077–2082. IEEE, 2011.
462. S. Pudlewski, A. Prasanna, and T. Melodia. Compressed-sensing-enabled video streaming for
wireless multimedia sensor networks. IEEE Trans. on Mobile Computing, (99), 2011.
463. H. Jiang, C. Li, R. Haimi-Cohen, P. A. Wilford, and Y. Zhang. Scalable video coding using
compressive sensing. Bell Labs Technical Journal, 16(4):149–169, 2012.
464. Z. Xiong, K. Ramchandran, M. T. Orchard, and Y. Q. Zhang. A comparative study of DCT-
and wavelet-based image coding. IEEE Trans. on Circuits and Systems for Video Technology,
9(5):692–695, 1999.
465. L. W. Kang and C. S. Lu. Distributed compressive video sensing. In Proc. of IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing, pages 1169–1172. IEEE,
2009.
466. Y. Yang, O. C. Au, L. Fang, X. Wen, and W. Tang. Perceptual compressive sensing for image
signals. In Proc. of IEEE International Conference on Multimedia and Expo, pages 89–92.
IEEE, 2009.
467. V. Cevher. Learning with compressible priors. NIPS, Vancouver, BC, Canada, pages 7–12,
2008.
468. J. N. Laska, P. T. Boufounos, M. A. Davenport, and R. G. Baraniuk. Democracy in action:
Quantization, saturation, and compressive sensing. Applied and Computational Harmonic
Analysis, 2011.

464
A Our Published Journal and Conference Papers Related to This Book
469. A. K. Fletcher, S. Rangan, and V. K. Goyal. On the rate-distortion performance of com-
pressed sensing. In Proc. of IEEE International Conference on Acoustics, Speech and Signal
Processing, volume 3, pages III–885. IEEE, 2007.
470. E. J. Candes, J. K. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccu-
rate measurements. Communications on Pure and Applied Mathematics, 59(8):1207–1223,
2006.
471. I. Daubechies. Orthonormal bases of compactly supported wavelets. Communications on
Pure and Applied Mathematics, 41(7):909–996, 1988.
472. Xiao Lin Liu, Chong Luo, Wenjun Hu, and Feng Wu. Compressive broadcast in MIMO
systems with receive antenna heterogeneity. In INFOCOM, pages 3011–3015. IEEE, March
2012.
473. E. Cand`es and T. Tao. Decoding by linear programming. IEEE Trans. on Information Theory,
51(12):4203–4215, December 2005.
474. L. Gan, T. Do, and T. D. Tran. Fast compressive imaging using scrambled block Hadamard
ensemble. Preprint, 2008.
475. S. Haghighatshoar, E. Abbe, and E. Telatar.
Adaptive sensing using deterministic partial
Hadamard matrices. In IEEE International Symposium on Information Theory Proceedings
(ISIT), pages 1842–1846, 2012.
476. Jian Zhang, Debin Zhao, Chen Zhao, Ruiqin Xiong, Siwei Ma, and Wen Gao. Image com-
pressive sensing recovery via collaborative sparsity. IEEE Journal on Emerging and Selected
Topics in Circuits and Systems, 2(3):380–391, 2012.
477. Abbas El Gamal and Young-Han Kim. Network Information Theory. Cambridge University
Press, New York, NY, 2012.
478. M. Brown and D. G. Lowe. Unsupervised 3d object recognition and reconstruction in un-
ordered datasets. In 3-D Digital Imaging and Modeling, 2005. 3DIM 2005. Fifth International
Conference on, pages 56–63, 2005.
479. Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo tourism: Exploring photo collec-
tions in 3D. ACM Trans. Graph., 25(3):835–846, July 2006.
480. Tianjia Shao, Weiwei Xu, Kun Zhou, Jingdong Wang, Dongping Li, and Baining Guo. An
interactive approach to semantic modeling of indoor scenes with an RGBD camera. ACM
Trans. Graph., 31(6):136:1–136:11, November 2012.
481. Kwan-Jung Oh, A. Vetro, and Yo-Sung Ho. Depth coding using a boundary reconstruction
ﬁlter for 3-D video systems. Circuits and Systems for Video Technology, IEEE Transactions
on, 21(3):350–359, 2011.
482. Shujie Liu, PoLin Lai, Dong Tian, and Chang Wen Chen. New depth coding techniques
with utilization of corresponding video. Broadcasting, IEEE Transactions on, 57(2):551–561,
2011.
483. Stefan Gumhold, Zachi Kami, Martin Isenburg, and Hans-Peter Seidel. Predictive point-cloud
compression. In ACM SIGGRAPH 2005 Sketches, SIGGRAPH ’05, 2005.
484. Ruwen Schnabel and Reinhard Klein. Octree-based point-cloud compression. In Proceedings
of the 3rd Eurographics / IEEE VGTC Conference on Point-Based Graphics, SPBG’06, pages
111–121, 2006.
485. H. Yue, X. Sun, J. Yang, and F. Wu. Landmark image super-resolution by retrieving web
images. Image Processing, IEEE Transactions on, PP(99):1–1, 2013.
486. Jian Sun, Nan-Ning Zheng, Hai Tao, and Heung-Yeung Shum.
Image hallucination with
primal sketch priors. In Computer Vision and Pattern Recognition, 2003. Proceedings. 2003
IEEE Computer Society Conference on, volume 2, pages II–729–736, 2003.
487. Jianchao Yang, J. Wright, T. S. Huang, and Yi Ma. Image super-resolution via sparse repre-
sentation. Image Processing, IEEE Transactions on, 19(11):2861–2873, 2010.
488. M. C. Davey and D. J. C. MacKay. Low density parity check codes over GF(Q). In Informa-
tion Theory Workshop, 1998, pages 70–71, 1998.

Index
A
Adaptive directional lifting (ADL), 135, 138,
153, 174
Adaptive modulation and coding (AMC), 317,
320
Additive white Gaussian noise (AWGN), 299,
323, 379
Advanced motion threading, 94
Advanced motion vector prediction (AMVP),
30
Advanced Video Coding (AVC), 27, 28
AEP; See Asymptotic equipartition property
AMC; See Adaptive modulation and coding
Amplitude modulation (AM), 48
Amplitude sensitivity, 48
Amplitude-shift keying (ASK), 60
AMVP; See Advanced motion vector prediction
Analog communication, 47–51
amplitude modulation, 48
amplitude sensitivity, 48
analog modulation, 48–49
analog transmission, 47
angle modulation, 48
continuous-wave modulation, 48
frequency division multiplexing, 50
frequency modulation, 48
modulation, 47
multiplexing, 49–51
phase modulation, 48
pulse amplitude modulation, 48
pulse duration modulation, 48
pulse position modulation, 48
single sideband modulation, 49
time division multiplexing, 50
Analog transmission, 47
Angle modulation, 48
Anisotropic auto-correlation, 177
Approximate nearest neighbor (ANN) search,
264
Arithmetic coding, 9, 267
ASK; See Amplitude-shift keying
Asymptotic equipartition property (AEP), 6
AVC; See Advanced Video Coding
AWGN; See Additive white Gaussian noise
B
Band offset mode, 31, 45
Bandwidth
allocation and reduction, 375
compression, 433
expansion, 433
Barbell
function, 114
lifting, 64, 111, 113
lifting coding, 112
Barbell-lifting based 3D wavelet coding,
111–132
advances in 3D wavelet video coding,
123–127
discrete wavelet transform, 125
in-scale MCTF, 123–125
motion alignment, 123
multi-resolution temporal transform, 124
redundancy, 124
subband adaptive MCTF, 126–127
barbell-lifting coding scheme, 112–120
barbell functions, 114
barbell lifting, 113–117
base layer embedding, 119–120
embedded subband coding with optimal
truncation, 118
entropy coding in brief, 118–119
layered motion coding, 117–118
motion compensated prediction, 114
465

466
Index
motion compensated temporal ﬁltering,
114
motion compensated update, 116
overlapped block motion compensation,
115
rate distortion optimization, 117
comparisons with SVC, 120–123
coding framework, 120–121
intra prediction, 123
one-by-one layer optimization, 120
preﬁltering, 122
SNR scalability, 120
spatial scalability, 122
temporal decorrelation, 121–122
experimental results, 127–132
comparison with MC-EZBC, 127–129
comparison with SVC for combined
scalability, 130–132
comparison with SVC for SNR scalability,
129–130
Base layer embedding, 119
Basic PFGS framework, 69
BDF; See Bidirectional ﬁltering
Belief propagation, 329
BER; See Bit error rate
BFOS algorithm, 144
BICM; See Bit-interleaved coded modulation
Bidirectional ﬁltering (BDF), 175, 186
Binary phase shift keying (BPSK), 305, 345,
374
Bipartite graph, 53
Bit error rate (BER), 307, 393
Bit-interleaved coded modulation (BICM),
313, 320
Bit plane coding, 67, 81
Block removal pattern (BRP), 212
BM3D GPU implementation, 380
Boundary effect, 94
B picture coding, 27
BPSK; See Binary phase shift keying
BRP; See Block removal pattern
Burrows-Wheeler transform (BWT), 319
C
CABAC; See Context-adaptive binary
arithmetic coding
CAVLC; See Context-adaptive variable length
coding
CDD model; See Curvature driven diffusion
model
CDF; See Cumulative distribution function
CDG; See Compressive data gathering
Channel
bandwidth, 433
capacity, 14
coding, 14, 432–436; See also Joint source
and channel coding
bandwidth compression, 433
bandwidth expansion, 433
belief propagation, 435
channel bandwidth, 433
channel denoising, 435–436
linear digital transmission, 433
matched source and channel bandwidth,
433
multiple level channel coding, 434–435
power allocation and bandwidth matching,
432–434
random projection code, 434
source bandwidth, 433
theorem, 17
denoising, 435
SNR (CSNR), 395
Chunk division and scaling, 377
CIF; See Common intermediate format
Cisco visual networking index, 3, 369
Close-loop prediction, 373
Cloud
images, 221
photo storage; See Compression for cloud
photo storage
sources, 422
storage, 245
Cloud-based image coding, 193, 222
Cloud-based image compression, 221–243
cloud images, 221
compression of image descriptors, 229–232
compression of image descriptors, 229
compression of SIFT descriptors, 230–232
discrete cosine transform, 231
feature vector, 230
Gaussian scale space, 229
HEVC, 229
prediction evaluation, 229–230
down-sampled image, 222
experimental results and analyses, 235–241
comparison with SIFT feature vector
coding, 240–241
complexity analyses, 240
compression ratio, 235–236
HEVC, 237
highly correlated image, 237–239
IrfanView software, 236
JPEG, 237
quantization step, 235
query images, 240
RANSAC algorithm, 240
retrieved images, 239

Index
467
up-sampled decompressed image, 239
visual quality, 236–237
extraction of image description, 226–228
discrete low-pass ﬁlter, 227
down-sample process, 226
feature vector, 228
Gaussian scale space, 227
INRIA Holiday data set, 227
Laplacian scale-space, 227
SIFT descriptor, 228
further discussion, 241–243
cloud-based image coding, 241
conventional image coding, 241
future work, 242–243
image search, 242
image sharing, 241
limitations, 242
service providers, 242
typical applications, 241–242
image database, 222
image reconstruction, 224, 232–234
Euclidean distances, 233, 234
matching precision, 233
patch retrieval, 232–233
patch stitching, 234
patch transformation, 233–234
RANSAC algorithm, 233
image search, 221
intra prediction, 221
proposed SIFT-based image coding, 225–226
block diagrams, 226
cloud-based image decoder, 226
image decoder, 226
SIFT descriptors, 225
RANSAC algorithm, 222
related work, 222–225
cloud-based image coding, 222
computer graphics images, 223
GIST descriptors, 222
human interactions, 223
image alignment, 225
image reconstruction, 224–225
image retrieval, 224
local feature compression, 223–224
principal components analysis, 224
residual feature vectors, 224
SIFT descriptor, 223
visual content generation, 222–223
SIFT descriptors, 222
Cloud-based image decoder, 226
Cloud-based image encoder, 225
CM; See Compressive modulation
CME; See Correlated motion estimation
Codebook, 16
Coded modulation, 313, 320
Codeword, 7
Coding
block, 41
framework, 120
gain of transform, 177
tree block (CTB), 30, 40
tree unit (CTU), 30, 40
unit (CU), 30, 41
Coding of Cloud image sources, 427
Coding of Cloud video sources, 429
Column weight, 52
Common intermediate format (CIF), 26
Communication, 47–62
analog communication, 47–51
amplitude modulation, 48
amplitude sensitivity, 48
analog modulation, 48–49
analog transmission, 47
angle modulation, 48
continuous-wave modulation, 48
frequency division multiplexing, 50
frequency modulation, 48
modulation, 47
multiplexing, 49–51
phase modulation, 48
pulse amplitude modulation, 48
pulse duration modulation, 48
pulse position modulation, 48
single sideband modulation, 49
time division multiplexing, 50
digital communication, 51–62
amplitude-shift keying, 60
bipartite graph, 53
column weight, 52
dense codes, 53
digital-to-analog conversion, 60
digital modulation, 60–62
extrinsic probabilities, 56
frequency-shift keying, 60
generator matrix, 52
long term evolution wireless communica-
tion standard, 56
Low-Density Parity-Check codes, 51–55
message passing, 54
parity check matrix, 52
quadrature amplitude modulation, 60
row weight, 52
sparse codes, 53
Turbo codes, 55–60
Compression for cloud photo storage, 245–265
cloud storage, 245
conjecture on cloud storage, 264–265
experimental results, 258–264

468
Index
approximate nearest neighbor search, 264
complexity, 263–264
efﬁciency of multi-model prediction,
260–261
efﬁciency of photometric transformation,
261–262
example images, 259
inter prediction, 262
k-dimensional tree, 264
overall performance, 262–263
properties of image sets, 259
feature-based inter-image prediction, 249,
254–258
block-based motion compensation, 258
Delaunay triangulation, 256
geometric deformations, 254–257
High Efﬁcient Video Coding, 254
multi-model geometric deformations, 255
photometric transformation, 257–258
RANSAC algorithm, 255
weighted prediction, 254
feature-based prediction structure, 250–253
feature-based minimum spanning tree,
252–253
graph building, 250–252
matching approach, 251–252
minimization problem, 252
pixel-based alignment, 250
prediction structure, 253
minimum spanning tree, 246, 252
proposed scheme, 249–250
inter-image prediction, 249
minimum spanning tree, 249
residue images, 250
related work, 247–249
feature-based image set compression, 247
image set compression, 247
Karhunen-Lo`eve transform, 247
local feature descriptors, 248
minimum spanning tree, 247
RANSAC algorithm, 248
SIFT descriptors, 248
representative signal, 245
three-step prediction, 246
Compression of image descriptors, 229
Compression of SIFT descriptors, 230
Compressive communication, 267
Compressive data gathering (CDG), 267,
269–293
compressive sensing, 273, 277
data gathering, 274–276
data recovery, 276–279
discrete cosine transform, 276
distributed source coding techniques, 278
energy consumption load balancing, 269
experiments on real data sets, 288–292
compressive sensing theory, 289
CTD data from the ocean, 288–289
distributed source coding, 291
sparse signal, 290, 292
temperature in the data center, 289–292
wavelet decorrelation, 288
global communication cost, 269
identical matrix, 278
intuition behind, 274
network capacity of, 279–288
capacity gain over naive transmission, 284
capacity under physical model, 282–284
capacity under protocol model, 280–282
chain topology, 285–286
Chebychevs inequality, 280
graph coloring theory, 281
grid topology, 286–288
medium access control, 284
network capacity analysis, 279–284
NS-2 simulation, 284–288
packet loss ratio, 285
routing trees, 286
signal to interference and noise ratio, 280
time-division multiple access, 279
objective, 274
recovery of data with abnormal readings,
277–279
recovery of spatially correlated data,
276–277
related work, 271–274
compressive sensing, 273–274
compressive sensing measurement, 273
compressive wireless sensing, 273
conventional compression, 271–272
distributed source coding, 272–273
in-network data compression, 271
joint entropy coding techniques, 272
nondeterministic polynomial-hard
problem, 272
Slepian-Wolf coding, 273
routing protocol, 275
sensor network, 269
typical routing tree, 274
wavelet transform matrix, 277
Compressive data recovery, 276
Compressive image broadcast, 400
Compressive modulation (CM), 295–315
background, 296–300
adaptive demodulation, 297
additive white Gaussian noise, 299
channel signal-to-noise ratio, 297
demodulation process, 299

Index
469
fast fading channel, 297
frame loss ratio, 297
Gray code, 299
information mismatch, 299
interference-free bit error rate, 297
logical exclusive-OR, 298
minimum Euclidean distance, 299
mismatched decoding problem, 298–300
quadrature amplitude modulation, 299
random projection code, 296, 300
Raptor code, 298
rate adaptation, 296–298
receiver adaptation, 296
channel distortion, 295
compressive modulation, 295, 296, 300–306
amplitude modulation, 301
binary phase shift keying, 305
coding and modulation, 300–302
design RP codes, 305–306
Gaussian channel, 302
low-density parity-check decoding, 303
Message Damping Belief Propagation, 304
messages sent out by symbol nodes, 303,
304
quadrature amplitude modulation, 305
quadrature phase shift keying, 305
rateless code, 300
RP code design, 305
RP decoding algorithm, 303
soft demodulation and decoding, 302–304
symbol information, 306
transmission errors, 306
constellation, 295
demodulation, 299, 302
error resilience capability, 295
fountain code, 296
mismatched decoding problem, 296
random projection, 296
rate adaptation, 295
rateless code, 296
receiver adaptation, 295
related work, 313–314
bit-interleaved coded modulation, 313
coded modulation, 313–314
compressive sensing, 314
LDPC codes, 314
multilevel coded modulation, 313
multi-stage decoding, 313
trellis-coded modulation, 313
sender adaptation, 295
simulation study, 307–309
average symbol energy, 307
AWGN channel, 309
bit error rate, 307
goodput, 307
MATLAB, 307
rate adaptation performance, 307–309
sensitivity to SNR estimation, 309
smooth rate adaptation, 308
symbol errors, Gaussian characterization of,
315
testbed evaluation, 309–313
adaptive demodulation, 310
comparison to ADM, 312–313
comparison to Oracle, 311–312
instant ACK, 310
line-of-sight path, 311
mobile scenario, 310, 312
Oracle rate adaptation scheme, 310
SORA platform, 309
static scenario, 310, 311
Compressive sensing, 267, 273, 277, 399, 409
Compressive sensing measurement, 273
Compressive wireless sensing (CWS), 273
Computational information theory, 421–437
channel coding, 432–436
bandwidth compression, 433
bandwidth expansion, 433
belief propagation, 435
channel bandwidth, 433
channel denoising, 435–436
linear digital transmission, 433
matched source and channel bandwidth,
433
multiple level channel coding, 434–435
power allocation and bandwidth matching,
432–434
random projection code, 434
source bandwidth, 433
Cloud sources, 422–425
constrained scenarios, 424
correlated sources, 422
correlation property, 423
distributed source coding, 422
diversity property, 423
Microsoft Kinect data, 424
SIFT descriptors, 423
structure from motion, 422
distributed coding using Cloud sources,
430–432
distributed source coding, 431
LDPC code, 431
pseudo analog approach, 432
SIFT vector, 431
Turbo code, 431
information theory, 421
joint source and channel coding, 436–437
computational information theory, 436

470
Index
redundancy, 436
Shannon’s rate-distortion theory, 436
network information theory, 421
source coding, 426–430
building graphs, 428
coding of Cloud image sources, 427–429
coding of Cloud video sources, 429–430
coding of metadata, 426–427
conversion from a graph to a tree, 428
external references, 430
HEVC standard, 429
local feature coding, 426
Conditional entropy, 5
Conditional replenishment, 76
Conference papers; See Published journal and
Conference papers
Constellation, 295
Context-adaptive binary arithmetic coding
(CABAC), 29, 30, 38, 189
Context-adaptive variable length coding
(CAVLC), 29, 37
Continuous-wave modulation, 48
Correlated motion estimation (CME), 94, 101
Correlated sources, 422
Coset coding, 346
Coset quantization, 347
CRC; See Cyclic redundancy check
CSNR; See Channel SNR
CTB; See Coding tree block
CTU; See Coding tree unit
CU; See Coding unit
Cumulative distribution function (CDF), 311,
335
Curvature driven diffusion model, 196
CWS; See Compressive wireless sensing
Cyclic redundancy check (CRC), 333
D
DAC; See Digital-to-analog conversion
Data gathering process, 275; See also
Compressive data gathering
Daubechies wavelets, 401
DBF; See Deblocking ﬁlter
DCast (distributed video multicast), 341–367
distributed video coding, 341, 343
experiments, 357–367
complexity and bit rate, 365–367
evaluation of each module, 361
multicast performance, 363–365
PDO model veriﬁcation, 358–359
robustness test, 362–363
unicast performance, 360–361
video streaming, 357
Witsenhausen-Wyner Video Codec, 357
power-distortion optimization, 352–357
distortion formulation, 355–356
MV distortion and prediction noise
variance, 354–355
MV transmission power and distortion,
353–354
relationship between variables, 353
solution, 356–357
proposed DCast, 345–352
binary phase shift keying modulation, 345
coset coding, 346–347
coset quantization, 347–348
DCast receiver, 346
DCast server, 345
DCast transmission, 350
groups of pictures, 345
Hadamard transform, 345
inverse fast Fourier transform, 346
LMMSE decoding, 351–352
orthogonal frequency division multiplex-
ing, 346
packaging and transmission, 350–351
power allocation, 348–350
power distortion optimization, 345
variable length coding, 345, 350
related works, 342–345
DCast, 342, 345
distributed source coding, 342
distributed video coding, 342–343
distributed video multicast, 341
distributed video transmission, 343–344
Flexcast, 344
forward error correction, 345
joint source-channel coding, 344
linear least square estimator algorithm, 344
motion compensated extrapolation, 343
motion compensated interpolation, 343
power distortion optimization, 342
quadrature amplitude modulation, 344
Slepian-Wolf theorem, 343
SoftCast, 344–345
Wyner-Ziv Dirty-Paper problem, 345
Wyner-Ziv theorem, 343
SoftCast, 341, 344
DCast receiver, 346
DCast server, 345
DCast transmission, 350
DCT; See Discrete cosine transform
Deblocking ﬁlter (DBF), 29, 31, 38
Denoising in communication, 369–393
background, 370–373
close-loop prediction, 373
image denoising, 370–371
image local correlation, 370

Index
471
image nonlocal correlation, 370
intra prediction, 372
Karhunen-Lo`eve transform, 373
motion-compensated temporal ﬁltering,
373
nonlocal means ﬁlter, 371
SUSAN noise ﬁlter, 371
transform-based compression, 373
video compression, 371–373
evaluation, 382–392
comparison against reference systems,
388–389
Hadamard matrix, 386
image denoising, 385
micro-benchmarks, 383–387
multicast scenario, 388
performance metric, 383
reference schemes, 383
robustness to packet loss, 391–392
settings, 382–383
temporal redundancy, 383
transmission in spatial domain versus in
transform domain, 386
transmitting high-deﬁnition videos,
390–391
hybrid digital-analog video communication,
369, 373
implementation, 379–382
BM3D algorithm, 381
Cactus implementation, 379–380
ﬁeld programmable gate array, 380
GPU implementation of BM3D, 380–382
Hadamard transform, 381
MATLAB, 380
orthogonal frequency division multiplex-
ing, 380
Physical Layer Convergence Protocol, 380
joint source-channel coding, 369, 392
related work, 392–393
bit error rate, 393
joint source-channel coding, 392
SoftCast, 393
vector quantization, 393
Wyner-Ziv coding, 393
system design, 373–379
additive white Gaussian noise channel, 379
bandwidth allocation and reduction,
375–376
binary phase-shift keying modulation, 374
chunk division and scaling, 377
group of pictures, 374
inverse discrete cosine transform, 378
L-shaped chunk division and scaling,
377–378
motion-compensated temporal ﬁltering,
374
open-loop prediction, 375
pseudo-analog amplitude modulation, 374
receiver design, 378–379
reduction of temporal redundancy, 375
sender design, 375–378
system overview, 373–375
DFD image; See Displaced frame difference
image
Differential entropy, 6
Digital-to-analog conversion (DAC), 60
Digital communication, 51–62
amplitude-shift keying, 60
bipartite graph, 53
column weight, 52
dense codes, 53
digital-to-analog conversion, 60
digital modulation, 60–62
extrinsic probabilities, 56
frequency-shift keying, 60
generator matrix, 52
long term evolution wireless communication
standard, 56
Low-Density Parity-Check codes, 51–55
message passing, 54
parity check matrix, 52
quadrature amplitude modulation, 60
row weight, 52
sparse codes, 53
Turbo codes, 55–60
Digital modulation, 60
Directional DCT, 133
Directional DCT coding, 161
Directional DCT transform, 153–171
adaptive directional lifting, 153
embedded block coding with optimized
truncation, 153
experimental results, 166–171
coding gain, 168
common images, 168
JPEG decoded images, 170
overhead bits, 170, 171
quarter-pixel precision, 168
visual quality comparison, 169
Gibbs artifacts, 153
image coding with proposed directional
transform, 161–166
directional DCT coding, 161
direction selection, 164–166
direction transition on block boundary,
162–164
dynamic programming algorithm, 165
example, 164

472
Index
half pixels, 162
interpolation ﬁlter, 166
JPEG image coding scheme, 161
Lagrange multiplier, 165
optimum problem, 164
quarter pixels, 162
lifting-based directional DCT-like transform,
154–161
comparison with rotated DCT, 159–161
directional DCT-like transform, 157–159
energy distributed update, 157
lifting structure of DCT, 154–157
pixel correlation, 155
primary operations, 158
rotated DCT, 159
Z transform of signal, 155
lifting structure, 153
variable length coding, 153
Directional ﬁltering transform, 134, 173–192
adaptive directional lifting, 174
adaptive directional lifting-based 2D wavelet
transform, 175–176
ADL-based wavelet transform, 175
bidirectional ﬁltering, 175
ﬁrst dimension lifting, 175
ﬁxed lifting order, 176
prediction, 175
unidirectional ﬁltering, 175
directional DCT, 174
directional ﬁltering transform, 185–189
bidirectional ﬁltering, 186
directional ﬁltering, 186–188
down-sampled block, 187
interpolation ﬁlter, 187
Karhunen-Lo`eve transform, 188
Key Technical Area software, 189
optional transform, 188–189
proposed intra-coding scheme, 185–186
rate-distortion optimization, 188
unidirectional ﬁltering, 186
weighted prediction, 188
directional prediction, 174
directional wavelet, 173
discrete cosine transform, 173
discrete wavelet transform, 173
experimental results, 189–192
computational complexity, 191
context adaptive binary arithmetic coding,
189
ﬁltering modes, 189
Hadamard transform, 189
performance gap, 190
reconstructed frames, 191
image anisotropic model, 174
mathematical analysis, 176–185
anisotropic auto-correlation, 177
band-limit parameter, 178
coding gain of ADL, 177–180
coding gain of transform, 177
Fourier transform, 177
high-pass residues, 183
integer pixel, 184
numerical analysis, 181–185
power spectral density, 176, 178
prediction direction vector, 180
subband signals, 179
Wiener-Hopf equation, 181
transform order, 174
Directional prediction, 136, 174
Directional transforms, 133
Directional wavelet transform (DWT), 135–152
adaptive directional lifting, 135, 138
directional ﬁlter and transform, 136
directional prediction, 136
directional wavelet transform, 135
discrete cosine transform, 135
experimental results and observations,
145–152
EBCOT technique, 145
peak signal-to-noise ratio, 147
reference software, 146
set partitioning in hierarchical trees, 150
wavelet decomposition, 147
Gibbs artifacts, 135
lifting structure, 136–137
R-D optimized segmentation for ADL,
144–145
BFOS algorithm, 144
EBCOT technique, 145
JPEG 2000 code, 145
Lagrangian multiplier, 145
quad-tree, 144
rate distortion optimization, 144
rectilinear 2D wavelet transform, 135
rectilinear wavelet transform, 135
2D wavelet transform via adaptive
directional lifting, 138–143
ADL-based wavelet transform, 140
ADL structure, 138–142
ﬁlter coefﬁcients, 143
ﬁlter taps, 140
generalized vertical transform, 141
interpolation, 143
polyphase samples, 138
subbands of test image, 142
subpixel interpolation, 143
update step, 140
Direct mode, 28

Index
473
Discrete cosine transform (DCT), 23, 135, 173,
231
Discrete wavelet transform (DWT), 125, 173,
399
Displaced frame difference (DFD) image, 74
Distributed source coding (DSC), 272, 342,
422
Distributed video
coding (DVC), 341, 343
multicast, 341
transmission, 343
Diversity gain, 397
Drifting error, 63, 67
E
Edge-based inpainting, 195–220
curvature driven diffusion model, 196
edge-based image inpainting, 206–211
assistant pixel, 209
conﬁdence map, 207
inﬂuencing region, 207
S-candidate, 208
structure propagation, 207–210
T-candidate, 208
texture synthesis, 210–211
unknown pixel, 208
edge-based inpainting, 195
edge extraction and exemplar selection,
201–206
circle edges, 206
edge extraction, 202–204
energy function, 202
exemplar selection, 204–206
positive weighting factors, 205
structural exemplar selection, 205–206
textural exemplar selection, 204–205
thinning method, 202
topology-based algorithm, 202
experimental results, 211–219
bit-rate savings, 214
block removal pattern, 212
compression ratio, 218
computational complexity, 219
discussions, 215–219
implementation, 211–212
JBIG method, 211
macro-block, 212
PDE-based diffusion, 213
quality parameter, 212
S-candidate, 212
T-candidate, 212
test results, 212–215
visual quality assessment, 219
human visual system, 195
image hallucinating, 196
image inpainting, 196
partial differential equation model, 196
proposed framework, 197–201
assistant information, 198
basic idea, 197
decoder side, 198
edge information, 200
encoder side, 198
model-based inpainting, 199
TV model, 199
typical inpainting scenarios, 199
statistical redundancy, 195
total variation model, 196, 199
visual redundancy, 195
Edge extraction, 202
Edge offset mode, 31, 45
Embedded subband coding with optimal
truncation, 118
Energy consumption load balancing, 269
Energy distributed update (EDU), 157
Entropy, 4
Entropy coding, 37
Exclusive-OR (XOR), 322
Exemplar selection, 204
EXIT chart, 326
F
Fast Fourier transform (FFT), 322, 329
FDM; See Frequency division multiplexing
Feature-based geometric deformations, 254
Feature-based image set compression, 247
Feature-based inter-image prediction, 249, 254
Feature-based photometric transformation, 257
Feature-based prediction structure, 250
FEC; See Forward error correction
FFT; See Fast Fourier transform
FGS coding; See Fine granularity scalable
coding
FGS decoder, 68
FGS encoder, 67
Field programmable gate array (FPGA), 380
Fine granularity scalable (FGS) coding, 28, 63,
111
discrete cosine transform, 67
drifting error, 67
FGS decoder, 68
FGS encoder, 67
inverse discrete cosine transform, 69
layered coding, 66
layered scalable video coding, 66
matching pursuit, 67
progressive ﬁne granularity scalable, 65, 69
quality scalability, 65

474
Index
scalable video coding, 65
temporal scalability, 65
variable length coding, 67
video streaming, 65
Fixed length coding (FLC), 7
Flexcast, 344
FM; See Frequency modulation
Forward error correction (FEC), 318, 345
Fountain code, 296
FPGA; See Field programmable gate array
Frequency division multiplexing (FDM), 50
Frequency modulation (FM), 48
Frequency-shift keying (FSK), 60
G
Generator matrix, 17, 52
Gibbs artifacts, 135, 153
Gilbert model, 88
Global communication cost, 269
Global motion compensation, 28
Graph coloring theory, 281
Graphic processing unit (GPU), 34
Gray code, 299
Group of pictures (GOP), 69, 105, 345, 374
H
H.261, 26
H.264, 34
H.264/MPEG-4 AVC, 28
Hadamard transform, 189, 345, 381
Hamming codes, 17
Hamming distance, 18
HARQ; See Hybrid automatic repeat request
Header extension code (HEC), 87
Heinrich Hertz Institute, 120
HEVC standard, 40–46
coding block, 41
intra prediction, 42–43
motion compensation, 40–42
prediction block, 41
sample adaptive offset ﬁlter, 45–46
transform block, 42
transform and quantization, 43–45
uniform reconstruction quantization scheme,
44
Hierarchical modulation (HM), 395, 398
High Efﬁciency Video Coding (HEVC), 22, 30,
40, 254
Huffman coding, 8
Human visual system (HVS), 21, 195
Hybrid automatic repeat request (HARQ), 321
Hybrid digital-analog video communication,
369, 373
Hybrid video coding, 21–46
band offset mode, 31, 45
coding tree block, 30, 40
coding tree unit, 30, 40
coding unit, 30, 41
context-adaptive binary arithmetic coding,
29, 30, 38
context-adaptive variable length coding, 29,
37
deblocking ﬁlter, 29, 31, 38
edge offset mode, 31, 45
H.264 standard, 34–40
deblocking ﬁltering, 38–39
entropy coding, 37–38
intra prediction, 36
Lagrange parameter, 40
motion compensation, 34–35
rate distortion optimization, 39–40
reference picture lists, 35
transform and quantization, 36–37
HEVC standard, 40–46
coding block, 41
intra prediction, 42–43
motion compensation, 40–42
prediction block, 41
sample adaptive offset ﬁlter, 45–46
transform block, 42
transform and quantization, 43–45
uniform reconstruction quantization
scheme, 44
High Efﬁciency Video Coding, 22, 30, 40
hybrid coding framework, 21–25
discrete cosine transform, 23
High Efﬁcient Video Coding, 22
human visual system, 21
inter picture, 23
intra picture, 23
motion estimation, 22
spatial redundancy, 21
statistic redundancy, 22
temporal redundancy, 21
visual redundancy, 21
intra prediction, 31, 36, 42
motion compensation, 22, 28, 31, 34
prediction unit, 30, 41
quantization, 24, 36, 43
sample adaptive offset ﬁlter, 32, 45
technical evolution, 26–34
advanced motion vector prediction, 30
Advanced Video Coding, 27
B picture coding, 27
common intermediate format, 26
digital video disk, 27
direct mode, 28
ﬁne granularity scalable coding, 28

Index
475
global motion compensation, 28
graphic processing unit, 34
H.261, 26
H.264, 34
H.264/MPEG-4 AVC, 28–29
HEVC, 30–31
interlaced video, 27
macroblock, 27
merge mode, 30
MPEG-1, 26–27
MPEG-2, 27
MPEG-4, 28
object-based coding, 28
performance versus encoding complexity,
31–34
P picture coding, 26
predictive picture coding, 26
quadtree, 30
sample adaptive offset ﬁltering, 26
scalable video coding, 27
ultra high deﬁnition video, 30
ZigZag scan, 25
transform, 23, 29, 36, 43
transform unit, 30, 42
I
IDCT; See Inverse discrete cosine transform
Image
alignment, 225
anisotropic model, 174
compression, cloud-based; See Cloud-based
image compression
decoder, 226
denoising, 370
hallucinating, 196
inpainting, 196
local correlation, 370
nonlocal correlation, 370
reconstruction, 224, 232
retrieval, 224
search, 221
Image set compression, 247
Improved PFGS framework, 77, 78
Information theory, 3–20, 421
asymptotic equipartition property, 6
channel coding, 14–19
capacity, 14–16
channel coding theorem, 17
codebook, 16
coding theorem, 16–17
generator matrix, 17
Hamming codes, 17–19
Hamming distance, 18
parity check matrix, 18
Cisco visual networking index, 3
conditional entropy, 5
differential entropy, 6
entropy, 4
information theory, 3
joint entropy, 5
joint source and channel coding, 19–20
mutual information, 5
separate source channel, 20
source coding, 7–14
arithmetic coding, 8–10
binary strings, 7
ﬁxed length coding, 7
Huffman coding, 8
lossless coding, 11
lossy coding, 11
Markov source, 10
rate distortion theory, 11–14
variable length coding, 7
In-network data compression, 271
Inpainting, 193; See also Edge-based inpainting
In-scale motion compensated temporal
ﬁltering, 123
Integrated Services Digital Network (ISDN)
lines, 26
Interlaced video, 27
International Organization for Standardization,
120
Inter picture, 23
Interpolation, 143
Intra picture, 23
Intra prediction, 31, 36, 42, 123, 372
Inverse discrete cosine transform (IDCT), 69,
378
Inverse fast Fourier transform, 322, 346
ISDN lines; See Integrated Services Digital
Network lines
ITU Telecommunication Standardization
Sector, 120
J
Joint entropy, 5
Joint source and channel coding, 317–338, 436
adaptive modulation and coding, 317, 320
belief propagation algorithm, 318
belief propagation decoding, 329–332
convolution ﬂow, 331
deconvolution ﬂow, 332
fast Fourier transform, 329
logical XOR, 329
RP-BP decoding algorithm, 330
RP code decoding, 329
ZigZag iteration, 329

476
Index
compressive modulation for sparse binary
sources, 322–329
additive white Gaussian noise channel, 323
blind rate adaptation, 325
coded modulation scheme, 324
design principles, 323–325
encoding matrix construction, 327–329
EXIT chart, 326
MATLAB simulations, 325
RP code design, 323
saturation effect, 326
source sparsity, 322, 327
weight selection, 325–327
Wireless Local Area Network, 323
wireless symbols, entropy of, 323
digital-to-analog converter, 318
forward error correction, 318
performance evaluation, 332–336
BICM with ideal source compression, 333
cyclic redundancy check, 333
emulation in real channel environment,
335–336
evaluation scenarios, 335
HARQ with ideal source compression, 333
implementation, 333–334
incremental redundancy, 333
Line-of-Sight, 335
MATLAB, 332
Non-Line-of-Sight, 335
Physical Layer Convergence Protocol, 333
simulations over AWGN channel, 334–335
SoftCast, 341
Viterbi decoder, 333
Wideband Code-Division Multiple Access,
333
probability distribution functions, 319
random projection code, 318
rate adaptation, 317, 320
related work and background, 319–322
adaptive modulation and coding, 320
binary code, 320
bit-interleaved coded modulation, 320
Burrows-Wheeler transform, 319
coded modulation, 320
compressive sensing, 321–322
fast Fourier transform, 322
hybrid automatic repeat request, 321
inverse FFT, 322
joint source-channel coding, 319–320
logical exclusive-OR, 322
low-density generator matrix codes, 319
minimum distance transformer, 321
Raptor codes, 319
rate adaptation, 320–321
Rayleigh channel, 320
trellis-coded modulation, 320
Turbo code, 320
unequal error protection, 319
wireless sensor networks, 319
Shannon’s separation principle, 317
signal-to-noise ratios, 318
technical problems, 318
Joint source-channel coding theorem, 19
Joint Video Team (JVT), 28
JPEG
coding method, quality parameter of, 212
decoded images, 170
image coding scheme, 161
K
Karhunen-Lo`eve transform (KLT), 188, 247,
373
KD-tree, 264
Key Technical Area (KTA) software, 189
L
Layered coding, 66
Layered motion coding, 117
Layered scalable video coding, 66
Layered source channel coding, 398
LDGM codes; See Low-density generator
matrix codes
Level-off effect of digital systems, 415
Lifting structure, 94, 136–137
Lifting structure of DCT, 154
Linear digital transmission, 433
Linear least square estimator (LLSE), 344, 400
Line-of-Sight (LOS), 311, 335
LMMSE decoding, 351
Local feature
coding, 426
compression, 223
descriptors, 248
Long-term evolution (LTE) wireless
communication standard, 56
LOS; See Line-of-Sight
Lossless coding, 11
Lossy coding, 11
Low-density generator matrix (LDGM) codes,
319
Low-density parity-check codes, 51, 303, 431
LTE wireless communication standard;
See Long-term evolution wireless
communication standard
M
MAC; See Medium access control
Macroblock (MB), 27

Index
477
Many-to-one mapping, 97
Markov source, 10
Matched source and channel bandwidth, 433
Matching pursuit, 67
MC; See Motion compensation
MCE; See Motion compensated extrapolation
MCI; See Motion compensated interpolation
MCTF; See Motion compensated temporal
ﬁltering
MDBP; See Message Damping Belief
Propagation
MDT; See Minimum distance transformer
ME; See Motion estimation
Measurement matrix, 399
Medium access control (MAC), 284
Merge mode, 30
Message Damping Belief Propagation
(MDBP), 304
Message passing, 54
Microsoft
Kinect data, 424
SkyDrive, 245
MIMO (multiple-input-multiple-output)
broadcasting with receiver antenna
heterogeneity, 395–418
amplitude modulation and transmission,
406–407
background and related work, 397–400
compressive sensing, 399–400
discrete cosine transform, 399
discrete wavelet transform, 399
diversity gain, 397
hierarchical modulation, 398
layered source-channel schemes, 398–399
linear least square estimator, 400
measurement matrix, 399
multi-antenna systems, 397–398
multiple-input-single-output systems, 398
multi-resolution coding, 398
power allocation, 402
receiver antenna heterogeneity, 397
sensing matrix, 399
SoftCast, 400
space time block codes, 397
spatial multiplexing, 397
spatial stream, 397
superposition code, 398
channel SNR, 395
compressive image broadcasting system,
400–402
addressing heterogeneity, 402
block diagram, 401
Daubechies wavelets, 401
encoder and decoder, 401–402
JPEG 2000 standard, 401
MIMO links, 400
power distribution, 402
scaled coefﬁcients, 402
compressive sampling, 405–406
DWT coefﬁcients, 405
Hadamard sampling matrix, 406
restricted isometry property, 406
wavelet transform, 406
CS decoder, 407–409
Gaussian noise, 407
MIMO decoding, 408
minimization decoding problem, 408
single-antenna receiver, 407
source transmission strategy, 407
two-antenna receiver, 408
hierarchical modulation schemes, 395
MIMO, 395
multimedia broadcast, 395
power allocation, 402–405
aggregating coefﬁcients, 404–405
amplitude modulation, 402
compressibility parameter, 403
Exponential-Golomb codes, 405
mean square error criterion, 403
power scaling factors, 403–404
SISO AWGN channel, 403
pseudo analog, 396
receiver antenna heterogeneity, 395
simulation evaluation, 409–411
comparison with conventional digital
systems, 413–416
comparison with SoftCast, 412–413
compressive sensing, 409
HM-STBC, 413
impact of channel estimation errors, 411
level-off effect of digital systems, 415
micro-benchmarks for our system,
409–411
overall performance in broadcasting
session, 416–417
performance comparison with other
broadcast systems, 412–417
performance of decoder, 411
performance of power allocation, 409–411
space time coding, 395
Minimum distance transformer (MDT), 321
Minimum spanning tree (MST), 246, 247, 252
Mismatched decoding, 296, 298
MISO systems; See Multiple-input single-
output systems
MLC modulation; See Multilevel coded
modulation
Modulation, 47, 295

478
Index
Motion compensated Embedded Zero Block
Coding, 92, 127
Motion compensated extrapolation (MCE), 343
Motion compensated interpolation (MCI), 343
Motion compensated lifting, 92
Motion compensated prediction, 114
Motion compensated temporal ﬁltering
(MCTF), 64, 92, 114, 373, 374
Motion compensated update, 116
Motion compensation (MC), 22, 28, 31, 34, 40,
371
Motion estimation (ME), 22
Motion threading, 92
Motion threading for 3D wavelet coding,
91–109
advanced motion threading, 94–98
lifting-based motion threading, 94–47
lifting structure, 94
many-to-one mapping and non-referred
pixels, 97–98
motion alignment, 97
sum of absolution difference, 98
update step, 96
wavelet transform of motion threads, 95
correlated motion estimation, 94, 101
correlated motion estimation with R-D
optimization, 101–105
background motion, 102
bidirectional modes, 103
CME scheme, 101
deﬁnition of mode types, 102–104
macroblock, 102
motion vector difference, 104
peak signal-to-noise ratio, 104
rate distortion optimization, 104
R-D optimized mode decision, 104
single-directional modes, 103
sum of absolution difference, 104
experimental results, 105–109
coding gain, 106
coding performance comparison, 105–106
Group of Pictures, 105
macroblock mode distribution, 106–109
standard sequences, 106–107
mosaic technique, 91
Motion Compensated Embedded Zero Block
Coding, 92
motion compensated temporal ﬁltering, 92
motion threading, 92–94
boundary effect, 94
correlated motion estimation, 94
limitations, 93
macroblock, 92
pixels, 92
resolution, 93
multi-layer motion-threading, 98–101
assumption, 99
B-frame, 99
dyadic wavelet transform, 98
inter-layer correlation, 101
orders, 100
set partitioning in hierarchical trees, 92
wavelet-based scalable video coding, 91
Motion vector difference (MVD), 104
Moving Picture Experts Group (MPEG), 28,
92, 111
MPEG-1, 26
MPEG-2, 27
MPEG-4, 28
MSD; See Multi-stage decoding
MST; See Minimum spanning tree
Multi-layer motion threading, 98
Multilevel coded (MLC) modulation, 313
Multimedia broadcast, 395
Multiple-input-single-output (MISO) systems,
398
Multiple level channel coding, 434
Multiplexing, 49–51
Multi-stage decoding (MSD), 313
Mutual information, 5
MVD; See Motion vector difference
N
Network
capacity of compressive data gathering, 279
information theory, 421
Nondeterministic polynomial (NP)-hard
problem, 272
Non-Line-of-Sight (NLOS), 335
Nonlocal means (NLM) ﬁlter, 371
Non-referred pixel, 98
NP-hard problem; See Nondeterministic
polynomial-hard problem
O
Object-based coding, 28
Open-loop prediction, 375
Orthogonal frequency division multiplexing
(OFDM), 346, 357, 380
Overlapped block motion compensation
(OBMC), 115
P
PAM; See Pulse amplitude modulation
Parity check matrix, 18, 52
Partial differential equation (PDE) model, 196
Patch
retrieval, 232

Index
479
stitching, 234
transformation, 233
PB; See Prediction block
PCA; See Principal components analysis
PDFs; See Probability distribution functions
PDM; See Pulse duration modulation
PDO; See Power distortion optimization
Peak signal-to-noise ratio (PSNR), 104, 147
PFGS; See Progressive ﬁne granularity scalable
Phase modulation (PM), 48
Physical Layer Convergence Protocol (PLCP),
333, 380
Power
allocation, 348, 402, 432
distortion optimization (PDO), 342, 345, 352
spectral density (PSD), 176, 178
Power and distortion for MV transmission, 353
P picture coding, 26
PPM; See Pulse position modulation
Prediction block (PB), 41
Prediction unit (PU), 30, 41
Principal components analysis (PCA), 224
Probability distribution functions (PDFs), 319
Progressive ﬁne granularity scalable (PFGS),
63, 65, 69
decoder, 82
design principles, 70
encoder, 79
framework, 70
wireless transmission, 90
Progressive ﬁne granularity scalable (PFGS)
coding, 65–90
basic PFGS framework, 69–73
basic ideas to build PFGS framework,
70–72
group of pictures, 69
PFGS design principles, 70
PFGS framework, 70
signal-to-noise ratio scalability schemes,
69
simpliﬁed PFGS framework, 72–73
bit plane coding, 67, 81
experimental results and analyses, 82–85
bit rate of base layer, 83
coding efﬁciency gap, 85
enhancement bitstreams, 84
even pictures, 83
luminance PSNR, 83
odd pictures, 83
ﬁne granularity scalable video coding, 66–69
discrete cosine transform, 67
drifting error, 67
FGS decoder, 68
FGS encoder, 67
inverse discrete cosine transform, 69
layered coding, 66
layered scalable video coding, 66
matching pursuit, 67
progressive ﬁne granularity scalable, 65,
69
quality scalability, 65
scalable video coding, 65
temporal scalability, 65
variable length coding, 67
video streaming, 65
implementation of PFGS encoder and
decoder, 79–82
HQPD, 81
LQPD coefﬁcients, 80
motion compensators, 80
motion estimation module, 80
PFGS decoder, 82
PFGS encoder, 79
improvements to PFGS framework, 73–79
conditional replenishment, 76
displaced frame difference image, 74
improved PFGS framework, 77, 78
more efﬁcient PFGS framework, 76–79
potential coding inefﬁciency due to two
references, 73–75
PFGS wireless transmission, 90
simulation of streaming PFGS video over
wireless channels, 85–89
Gilbert model, 88
header extension code, 87
Reed-Solomon code, 88
spatial scalability, 66
PSD; See Power spectral density
Pseudo analog, 396
Pseudo-analog transmission, 339
PSNR; See Peak signal-to-noise ratio
PU; See Prediction unit
Published journal and Conference papers,
439–441
compressive communication, 440–441
directional transforms, 439–440
pseudo-analog transmission, 441
scalable video coding, 439
vision-based compression, 440
Pulse
amplitude modulation (PAM), 48
duration modulation (PDM), 48
modulation, 48
position modulation (PPM), 48
Q
Quadrature

480
Index
amplitude modulation (QAM), 60, 267, 299,
344
Phase Shift Keying (QPSK), 305
Quadtree, 30
Quality
parameter (QP), 212
scalability, 65
Quantization, 24, 36, 43
R
Random projection (RP)
code, 267, 296, 300, 318, 434
construction, 327
decoding, 329
design, 305, 323
decoding algorithm, 303
weight selection, 325
Raptor codes, 298, 319
Rate adaptation, 295, 317, 320
Rate adaption, 296
Rate distortion
optimization (RDO), 39, 104, 117, 144, 188
theorem, 12
theory, 11
Rateless code, 296
Rayleigh channel, 320
Receiver
adaptation, 296
antenna heterogeneity, 395, 397; See also
MIMO broadcasting with receiver
antenna heterogeneity, 395
Recovery with abnormal readings, 278
Rectilinear wavelet transform, 135
Reed-Solomon code, 88
Representative signal (RS), 245
Restricted isometry property (RIP), 406
Rotated DCT, 159
Row weight, 52
RP; See Random projection
RS; See Representative signal
S
SAD; See Sum of absolution difference
Sample adaptive offset (SAO) ﬁlter, 32, 45–46
Scalable video coding (SVC), 27, 63, 65, 129
Sender adaptation, 295
Sensing matrix, 399
Sensor network, 269
Separate source channel, 20
Set partitioning in hierarchical trees (SPIHTs),
92, 150
Shannon’s separate source channel, 20
SIFT descriptor, 223
Signal to interference and noise ratio (SINR),
280
Signal-to-noise ratio (SNR) scalability, 69, 129
Simpliﬁed PFGS framework, 72
Single sideband (SSB) modulation, 49
Slepian-Wolf theorem, 343
SNF; See SUSAN noise ﬁlter
SoftCast, 341, 344
decoder, 400
source redundancy, 393
SORA platform, 309
Source
bandwidth, 433
code, 7
coding, 7
coding theorem, 7
Space time block code (STBC), 397
Space time coding, 395
Spatial multiplexing, 397
Spatial redundancy, 21
Spatial scalability, 66, 122
SPIHTs; See Set partitioning in hierarchical
trees
SSB modulation; See Single sideband
modulation
SSD; See Sum of squared difference
Statistical redundancy, 195
Statistic redundancy, 22
STBC; See Space time block code
Structure from motion, 422
Structure propagation, 207
Subband adaptive motion compensated
temporal ﬁltering, 126
Sum of absolution difference (SAD), 40, 98
Sum of squared difference (SSD), 40
Superposition code, 398
SUSAN noise ﬁlter (SNF), 371
SVC; See Scalable video coding
T
TCM; See Trellis-coded modulation
Temporal decorrelation, 121
Temporal redundancy, 21
Temporal scalability, 65
Texture synthesis, 210
3D wavelet coding; See Barbell-lifting based
3D wavelet coding; Motion threading for
3D wavelet coding
Time-division multiple access (TDMA), 279
Time division multiplexing (TDM), 50
Total variation (TV) model, 196, 199
Transform, 23, 29, 36, 43
block, 42
order, 174

Index
481
unit (TU), 30, 42
Transform-based compression, 373
Trellis-coded modulation (TCM), 313, 320
Turbo code, 55, 320, 431
TV model; See Total variation model
U
Ultra high deﬁnition (UHD) video, 30
Unequal error protection (UEP), 319
Unidirectional ﬁltering (UDF), 175, 186
Uniform reconstruction quantization (URQ)
scheme, 44
V
Variable length coding (VLC), 7, 67, 153, 345,
350
Vector quantization (VQ), 393
Video Coding Experts Group (VCEG), 28
Video compression, 371
Video streaming, 65, 357
Visual content generation, 222
Visual Networking Index (VNI), 369
Visual redundancy, 21, 195
Viterbi decoder, 333
W
Wavelet-based scalable video coding, 91
Weighted prediction, 188, 254
Wideband Code-Division Multiple Access
(WCDMA), 333
Wireless Local Area Network (WLAN), 323
Wireless sensor networks (WSNs), 319
Witsenhausen-Wyner Video Codec (WWVC),
357
Wyner-Ziv coding, 393
Wyner-Ziv theorem, 343
X
XOR; See Exclusive-OR
Y
ZigBee, 285
ZigZag deconvolution, 268
ZigZag iteration, 329
ZigZag scan, 25


ISBN: 978-1-4822-3413-8
9 781482 234138
90000
6000 Broken Sound Parkway, NW 
Suite 300, Boca Raton, FL 33487
711 Third Avenue 
New York, NY 10017
2 Park Square, Milton Park 
Abingdon, Oxon OX14 4RN, UK
an informa business
www.crcpress.com
www.auerbach-publications.com
K22985
Electrical Engineering / Image Processing
Visual information is one of the richest and most bandwidth-consuming modes of 
communication. To meet the requirements of emerging applications, powerful data 
compression and transmission techniques are required to achieve highly efficient 
communication, even in the presence of growing communication channels that 
offer increased bandwidth.
Presenting the results of the author’s years of research on visual data compression 
and transmission, Advances in Visual Data Compression and Communication: 
Meeting the Requirements of New Applications provides a theoretical 
and technical basis for advanced research on visual data compression and 
communication. 
The book studies the drifting problem in scalable video coding, analyzes the 
reasons causing the problem, and proposes various solutions to the problem. It 
explores the author’s Barbell-based lifting coding scheme that has been adopted 
as common software by MPEG. It also proposes a unified framework for deriving 
a directional transform from the nondirectional counterpart. The structure of the 
framework and the statistic distribution of coefficients are similar to those of the 
nondirectional transforms, which facilitates subsequent entropy coding.
Exploring the visual correlation that exists in media, the text extends the 
current coding framework from different aspects, including advanced image 
synthesis—from description and reconstruction to organizing correlated 
images as a pseudo sequence. It explains how to apply compressive sensing 
to solve the data compression problem during transmission and covers novel 
research on compressive sensor data gathering, random projection codes, and 
compressive modulation. 
For analog and digital transmission technologies, the book develops the pseudo-
analog transmission for media and explores cutting-edge research on distributed 
pseudo-analog transmission, denoising in pseudo-analog transmission, and 
supporting MIMO. It concludes by considering emerging developments of 
information theory for future applications.


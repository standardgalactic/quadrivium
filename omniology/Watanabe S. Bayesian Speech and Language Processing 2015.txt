
Bayesian Speech and Language Processing
With this comprehensive guide you will learn how to apply Bayesian machine learning
techniques systematically to solve various problems in speech and language processing.
A range of statistical models is detailed, from hidden Markov models to Gaussian
mixture models, n-gram models, and latent topic models, along with applications
including automatic speech recognition, speaker veriﬁcation, and information retrieval.
Approximate Bayesian inferences based on MAP, Evidence, Asymptotic, VB, and
MCMC approximations are provided as well as full derivations of calculations, useful
notations, formulas, and rules.
The authors address the difﬁculties of straightforward applications and provide detailed
examples and case studies to demonstrate how you can successfully use practical
Bayesian inference methods to improve the performance of information systems.
This is an invaluable resource for students, researchers, and industry practitioners
working in machine learning, signal processing, and speech and language processing.
Shinji Watanabe received his Ph.D. from Waseda University in 2006. He has been a
research scientist at NTT Communication Science Laboratories, a visiting scholar at
Georgia Institute of Technology and a senior principal member at Mitsubishi Electric
Research Laboratories (MERL), as well as having been an associate editor of the IEEE
Transactions on Audio Speech and Language Processing, and an elected member of the
IEEE Speech and Language Processing Technical Committee. He has published more
than 100 papers in journals and conferences, and received several awards including the
Best Paper Award from IEICE in 2003.
Jen-Tzung Chien is with the Department of Electrical and Computer Engineering and the
Department of Computer Science at the National Chiao Tung University, Taiwan, where
he is now the University Chair Professor. He received the Distinguished Research Award
from the Ministry of Science and Technology, Taiwan, and the Best Paper Award of the
2011 IEEE Automatic Speech Recognition and Understanding Workshop. He serves
currently as an elected member of the IEEE Machine Learning for Signal Processing
Technical Committee.

“This book provides an overview of a wide range of fundamental theories of Bayesian
learning, inference, and prediction for uncertainty modeling in speech and language
processing. The uncertainty modeling is crucial in increasing the robustness of prac-
tical systems based on statistical modeling under real environment, such as automatic
speech recognition systems under noise, and question answering systems based on lim-
ited size of training data. This is the most advanced and comprehensive book for learning
fundamental Bayesian approaches and practical techniques.”
Sadaoki Furui, Tokyo Institute of Technology

Bayesian Speech and Language
Processing
SHINJI WATANABE
Mitsubishi Electric Research Laboratories
JEN-TZUNG CHIEN
National Chiao Tung University

University Printing House, Cambridge CB2 8BS, United Kingdom
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107055575
c⃝Cambridge University Press 2015
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2015
Printed in the United Kingdom by Clays, St Ives plc
A catalog record for this publication is available from the British Library
Library of Congress Cataloging in Publication data
Watanabe, Shinji (Communications engineer) author.
Bayesian speech and language processing / Shinji Watanabe, Mitsubishi Electric Research
Laboratories; Jen-Tzung Chien, National Chiao Tung University.
pages
cm
ISBN 978-1-107-05557-5 (hardback)
1. Language and languages – Study and teaching – Statistical methods.
2. Bayesian statistical
decision theory.
I. Title.
P53.815.W38
2015
410.1′51–dc23
2014050265
ISBN 978-1-107-05557-5 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication,
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

Contents
Preface
page xi
Notation and abbreviations
xiii
Part I General discussion
1
1
Introduction
3
1.1
Machine learning and speech and language processing
3
1.2
Bayesian approach
4
1.3
History of Bayesian speech and language processing
8
1.4
Applications
9
1.5
Organization of this book
11
2
Bayesian approach
13
2.1
Bayesian probabilities
13
2.1.1
Sum and product rules
14
2.1.2
Prior and posterior distributions
15
2.1.3
Exponential family distributions
16
2.1.4
Conjugate distributions
24
2.1.5
Conditional independence
38
2.2
Graphical model representation
40
2.2.1
Directed graph
40
2.2.2
Conditional independence in graphical model
40
2.2.3
Observation, latent variable, non-probabilistic variable
42
2.2.4
Generative process
44
2.2.5
Undirected graph
44
2.2.6
Inference on graphs
46
2.3
Difference between ML and Bayes
47
2.3.1
Use of prior knowledge
48
2.3.2
Model selection
49
2.3.3
Marginalization
50
2.4
Summary
51

vi
Contents
3
Statistical models in speech and language processing
53
3.1
Bayes decision for speech recognition
54
3.2
Hidden Markov model
59
3.2.1
Lexical unit for HMM
59
3.2.2
Likelihood function of HMM
60
3.2.3
Continuous density HMM
63
3.2.4
Gaussian mixture model
66
3.2.5
Graphical models and generative process of CDHMM
67
3.3
Forward–backward and Viterbi algorithms
70
3.3.1
Forward–backward algorithm
70
3.3.2
Viterbi algorithm
74
3.4
Maximum likelihood estimation and EM algorithm
76
3.4.1
Jensen’s inequality
77
3.4.2
Expectation step
79
3.4.3
Maximization step
86
3.5
Maximum likelihood linear regression for hidden Markov model
91
3.5.1
Linear regression for hidden Markov models
92
3.6
n-gram with smoothing techniques
97
3.6.1
Class-based model smoothing
101
3.6.2
Jelinek–Mercer smoothing
101
3.6.3
Witten–Bell smoothing
103
3.6.4
Absolute discounting
104
3.6.5
Katz smoothing
106
3.6.6
Kneser–Ney smoothing
107
3.7
Latent semantic information
113
3.7.1
Latent semantic analysis
113
3.7.2
LSA language model
116
3.7.3
Probabilistic latent semantic analysis
119
3.7.4
PLSA language model
125
3.8
Revisit of automatic speech recognition with Bayesian manner
128
3.8.1
Training and test (unseen) data for ASR
128
3.8.2
Bayesian manner
129
3.8.3
Learning generative models
131
3.8.4
Sum rule for model
131
3.8.5
Sum rule for model parameters and latent variables
132
3.8.6
Factorization by product rule and conditional independence
132
3.8.7
Posterior distributions
133
3.8.8
Difﬁculties in speech and language applications
134
Part II Approximate inference
135
4
Maximum a-posteriori approximation
137
4.1
MAP criterion for model parameters
138

Contents
vii
4.2
MAP extension of EM algorithm
141
4.2.1
Auxiliary function
141
4.2.2
A recipe
143
4.3
Continuous density hidden Markov model
143
4.3.1
Likelihood function
144
4.3.2
Conjugate priors (full covariance case)
144
4.3.3
Conjugate priors (diagonal covariance case)
146
4.3.4
Expectation step
146
4.3.5
Maximization step
149
4.3.6
Sufﬁcient statistics
158
4.3.7
Meaning of the MAP solution
160
4.4
Speaker adaptation
163
4.4.1
Speaker adaptation by a transformation of
CDHMM
163
4.4.2
MAP-based speaker adaptation
165
4.5
Regularization in discriminative parameter estimation
166
4.5.1
Extended Baum–Welch algorithm
167
4.5.2
MAP interpretation of i-smoothing
169
4.6
Speaker recognition/veriﬁcation
171
4.6.1
Universal background model
172
4.6.2
Gaussian super vector
173
4.7
n-gram adaptation
174
4.7.1
MAP estimation of n-gram parameters
175
4.7.2
Adaptation method
175
4.8
Adaptive topic model
176
4.8.1
MAP estimation for corrective training
177
4.8.2
Quasi-Bayes estimation for incremental learning
179
4.8.3
System performance
182
4.9
Summary
183
5
Evidence approximation
184
5.1
Evidence framework
185
5.1.1
Bayesian model comparison
185
5.1.2
Type-2 maximum likelihood estimation
187
5.1.3
Regularization in regression model
188
5.1.4
Evidence framework for HMM and SVM
190
5.2
Bayesian sensing HMMs
191
5.2.1
Basis representation
192
5.2.2
Model construction
192
5.2.3
Automatic relevance determination
193
5.2.4
Model inference
195
5.2.5
Evidence function or marginal likelihood
196
5.2.6
Maximum a-posteriori sensing weights
197
5.2.7
Optimal parameters and hyperparameters
197

viii
Contents
5.2.8
Discriminative training
200
5.2.9
System performance
203
5.3
Hierarchical Dirichlet language model
205
5.3.1
n-gram smoothing revisited
205
5.3.2
Dirichlet prior and posterior
206
5.3.3
Evidence function
207
5.3.4
Bayesian smoothed language model
208
5.3.5
Optimal hyperparameters
208
6
Asymptotic approximation
211
6.1
Laplace approximation
211
6.2
Bayesian information criterion
214
6.3
Bayesian predictive classiﬁcation
218
6.3.1
Robust decision rule
218
6.3.2
Laplace approximation for BPC decision
220
6.3.3
BPC decision considering uncertainty of HMM means
222
6.4
Neural network acoustic modeling
224
6.4.1
Neural network modeling and learning
225
6.4.2
Bayesian neural networks and hidden Markov models
226
6.4.3
Laplace approximation for Bayesian neural networks
229
6.5
Decision tree clustering
230
6.5.1
Decision tree clustering using ML criterion
230
6.5.2
Decision tree clustering using BIC
235
6.6
Speaker clustering/segmentation
237
6.6.1
Speaker segmentation
237
6.6.2
Speaker clustering
239
6.7
Summary
240
7
Variational Bayes
242
7.1
Variational inference in general
242
7.1.1
Joint posterior distribution
243
7.1.2
Factorized posterior distribution
244
7.1.3
Variational method
246
7.2
Variational inference for classiﬁcation problems
248
7.2.1
VB posterior distributions for model parameters
249
7.2.2
VB posterior distributions for latent variables
251
7.2.3
VB–EM algorithm
251
7.2.4
VB posterior distribution for model structure
252
7.3
Continuous density hidden Markov model
254
7.3.1
Generative model
254
7.3.2
Prior distribution
255
7.3.3
VB Baum–Welch algorithm
257
7.3.4
Variational lower bound
269
7.3.5
VB posterior for Bayesian predictive classiﬁcation
274

Contents
ix
7.3.6
Decision tree clustering
282
7.3.7
Determination of HMM topology
285
7.4
Structural Bayesian linear regression for hidden Markov model
287
7.4.1
Variational Bayesian linear regression
288
7.4.2
Generative model
289
7.4.3
Variational lower bound
289
7.4.4
Optimization of hyperparameters and model structure
303
7.4.5
Hyperparameter optimization
304
7.5
Variational Bayesian speaker veriﬁcation
306
7.5.1
Generative model
307
7.5.2
Prior distributions
308
7.5.3
Variational posteriors
310
7.5.4
Variational lower bound
316
7.6
Latent Dirichlet allocation
318
7.6.1
Model construction
318
7.6.2
VB inference: lower bound
320
7.6.3
VB inference: variational parameters
321
7.6.4
VB inference: model parameters
323
7.7
Latent topic language model
324
7.7.1
LDA language model
324
7.7.2
Dirichlet class language model
326
7.7.3
Model construction
327
7.7.4
VB inference: lower bound
328
7.7.5
VB inference: parameter estimation
330
7.7.6
Cache Dirichlet class language model
332
7.7.7
System performance
334
7.8
Summary
335
8
Markov chain Monte Carlo
337
8.1
Sampling methods
338
8.1.1
Importance sampling
338
8.1.2
Markov chain
340
8.1.3
The Metropolis–Hastings algorithm
341
8.1.4
Gibbs sampling
343
8.1.5
Slice sampling
344
8.2
Bayesian nonparametrics
345
8.2.1
Modeling via exchangeability
346
8.2.2
Dirichlet process
348
8.2.3
DP: Stick-breaking construction
348
8.2.4
DP: Chinese restaurant process
349
8.2.5
Dirichlet process mixture model
351
8.2.6
Hierarchical Dirichlet process
352
8.2.7
HDP: Stick-breaking construction
353
8.2.8
HDP: Chinese restaurant franchise
355

x
Contents
8.2.9
MCMC inference by Chinese restaurant franchise
356
8.2.10 MCMC inference by direct assignment
358
8.2.11 Relation of HDP to other methods
360
8.3
Gibbs sampling-based speaker clustering
360
8.3.1
Generative model
361
8.3.2
GMM marginal likelihood for complete data
362
8.3.3
GMM Gibbs sampler
365
8.3.4
Generative process and graphical model of multi-scale GMM
367
8.3.5
Marginal likelihood for the complete data
368
8.3.6
Gibbs sampler
370
8.4
Nonparametric Bayesian HMMs to acoustic unit discovery
372
8.4.1
Generative model and generative process
373
8.4.2
Inference
375
8.5
Hierarchical Pitman–Yor language model
378
8.5.1
Pitman–Yor process
379
8.5.2
Language model smoothing revisited
380
8.5.3
Hierarchical Pitman–Yor language model
383
8.5.4
MCMC inference for HPYLM
385
8.6
Summary
387
Appendix A
Basic formulas
388
Appendix B
Vector and matrix formulas
390
Appendix C
Probabilistic distribution functions
392
References
405
Index
422

Preface
In general, speech and language processing involves extensive knowledge of statistical
models. The acoustic model using hidden Markov models and the language model using
n-grams are mainly introduced here. Both acoustic and language models are important
parts of modern speech recognition systems where the learned models from real-world
data are full of complexity, ambiguity, and uncertainty. The uncertainty modeling is
crucial to tackle the lack of robustness for speech and language processing.
This book addresses fundamental theories of Bayesian learning, inference, and pre-
diction for the uncertainty modeling. Uniquely, compared with standard textbooks for
dealing with the fundamental Bayesian approaches, this book focuses on the practi-
cal methods of the approaches to make them applicable to actual speech and language
problems. We (the authors) have been studying these topics for a long time with a
strong belief that the Bayesian approaches could solve “robustness” issues in speech
and language processing, which are the most difﬁcult problem and most serious short-
coming of real systems based on speech and language processing. In our experience,
the most difﬁcult issue in applying Bayesian approaches is how to appropriately choose
a speciﬁc technique among the many Bayesian techniques proposed in statistics and
machine learning so far. One of our answers to this question is to provide the approxi-
mated Bayesian inference methods rather than focusing on covering the whole Bayesian
techniques. We categorize the Bayesian approaches into ﬁve categories: the maximum
a-posteriori estimation; evidence approximation; asymptotic approximation; variational
Bayes; and Markov chain Monte Carlo. We also describe the speech and language pro-
cessing applications within this categorization so that readers can appropriately choose
the approximated Bayesian techniques for their problems.
This book is part of our long-term cooperative efforts to promote the Bayesian
approaches in speech and language processing. We have been pursuing this goal for
more than ten years, and part of our efforts was to organize a tutorial lecture with this
theme at the 37th International Conference on Acoustics, Speech, and Signal Processing
(ICASSP) in Kyoto, Japan, March 2012. The success of this tutorial lecture prompted
the idea of writing a textbook with this theme. We strongly believe in the importance
of the Bayesian approaches, and we sincerely encourage the researchers who work with
Bayesian speech and language processing.

xii
Preface
Acknowledgments
First we want to thank all of our colleagues and research friends, especially members of
NTT Communication Science Laboratories, Mitsubishi Electric Research Laboratories
(MERL), National Cheng Kung University, IBM T. J. Watson Research Center, and
National Chiao Tung University (NCTU). Some of the studies in this book were actu-
ally conducted when the authors were working in these institutes. We also would like to
thank many people for reading a draft and giving us valuable comments which greatly
improved this book, including Tawara Naohiro, Yotaro Kubo, Seong-Jun Hahm, Yu
Tsao, and all of the students from the Machine Learning Laboratory at NCTU. We are
very grateful for support from Anthony Vetro, John R. Hershey, and Jonathan Le Roux
at MERL, and Sin-Horng Chen, Hsueh-Ming Hang, Yu-Chee Tseng, and Li-Chun Wang
at NCTU. The great efforts of the editors of Cambridge University Press, Phil Meyler,
Sarah Marsh, and Heather Brolly, are also appreciated. Finally, we would like to thank
our families for supporting our whole research lives.
Shinji Watanabe
Jen-Tzung Chien

Notation and abbreviations
General notation
This book observes the following general mathematical notation to avoid any confusion
arising from notation:
B = {true, false}
Set of boolean values
Z+ = {1, 2, · · · }
Set of positive integers
R
Set of real numbers
R>0
Set of positive real numbers
RD
Set of D dimensional real numbers
∗
Set of all possible strings composed of letters
∅
Empty set
a
Scalar variable
a
Vector variable

xiv
Notation and abbreviations
a =

a1
· · ·
aN
⊺=
⎡
⎢⎣
a1
...
aN
⎤
⎥⎦
Elements of a vector, which can be described with the square brackets [· · · ], ⊺
denotes the transpose operation
A
Matrix variable
A =

a
b
c
d

Elements of a matrix, which can be described with the square brackets [· · · ]
ID
D × D identity matrix
|A|
Determinant of square matrix
tr[A]
Trace of square matrix
A, A
Set or sequential variable
A = {a1, · · · , aN} = {an}N
n=1
Elements in a set, which can be described with the curly brackets {· · · }
A = {an}
Elements in a set, where the range of index n is omitted for simplicity
an:n′ = {an, · · · , an′}
n′ > n
A set of sequential variables, which explicitly describes the range of elements
from n to n′ by using : in the subscript
|A|
The number of elements in a set A. For example |{an}N
n=1| = N
f(x) or fx
Function of x

Notation and abbreviations
xv
p(x) or q(x)
Probabilistic distribution function of x
F[f]
Functional of f. Note that a functional uses the square brackets [·] while a
function uses the bracket (·).
Ep(x|y)[f(x)|y] =

f(x)p(x|y)dx
The expectation of f(x) with respect to probability distribution p(x|y)
E(x)[f(x)|y] =

f(x)p(x|y)dx or E(x)[f(x)] =

f(x)p(x|y)dx
Another form of the expectation of f(x), where the subscript with the prob-
ability distribution and/or the conditional variable is omitted, when it is
trivial.
δ(a, a′) =

1
a = a′
0
Otherwise
Kronecker delta function for discrete variables a and a′
δ(x −x′)
Dirac delta function for continuous variables x and x′
AML, AML2, AMAP, ADT, · · ·
The variables estimated by a speciﬁc criterion (e.g., Maximum Likelihood
(ML)) are represented with the superscript of the abbreviation of the criterion.
Basic notation used for speech and language processing
We also list the notation speciﬁc for speech and language processing. This book tries
to maintain consistency by using the same notation, while it also tries to use commonly
used notation in each application. Therefore, some of the same characters are used to
denote different variables, since this book needs to introduce many variables.
Common notation

Set of model parameters
M
Model variable including types of models, structure, hyperparameters, etc.

xvi
Notation and abbreviations

Set of hyperparameters
Q(·|·)
Auxiliary function used in the EM algorithm
H
Hessian matrix
Acoustic modeling
T ∈Z+
Number of speech frames
t ∈{1, · · · , T}
Speech frame index
ot ∈RD
D dimensional feature vector at time t
O = {ot|t = 1, · · · , T}
Sequence of T feature vectors
J ∈Z+
Number of unique HMM states in an HMM
st ∈{1, · · · , J}
HMM state at time t
S = {st|t = 1, · · · , T}
Sequence of HMM states for T speech frames
K ∈Z+
Number of unique mixture components in a GMM
vt ∈{1, · · · , K}
Latent mixture variable at time t

Notation and abbreviations
xvii
V = {vt|t = 1, · · · , T}
Sequence of latent mixture variables for T speech frames
αt(j) ∈[0, 1]
Forward probability of the partial observations {o1, · · · , ot} until time t and
state j at time t
βt(j) ∈[0, 1]
Backward probability of the partial observations {ot+1, · · · , oT} from t + 1 to
the end given state j at time t
δt(j) ∈[0, 1]
The highest probability along a single path, at time t which accounts for
previous observations {o1, · · · , ot} and ends in state j at time t
ξt(i, j) ∈[0, 1]
Posterior probability of staying state i at time t and state j at time t + 1
γt(j, k) ∈[0, 1]
Posterior probability of staying at state j and mixture component k at time t
πj ∈[0, 1]
Initial state probability of state j at time t = 1
aij ∈[0, 1]
State transition probability from state st−1 = i to state st = j
ωjk ∈[0, 1]
Gaussian mixture weight at component k of state j
μjk ∈RD
Gaussian mean vector at component k of state j
jk ∈RD×D
Gaussian covariance matrix at component k of state j. Symmetric matrix
Rjk ∈RD×D
Gaussian precision matrix at component k of state j. Symmetric matrix, and the
inverse of covariance matrix jk

xviii
Notation and abbreviations
Language modeling
w ∈∗
Category (e.g., word in most cases, phoneme sometimes). The element is rep-
resented by a string in ∗(e.g., “I” and “apple” for words and /a/ and /k/ for
phonemes) or a natural number in Z+ when the elements of categories are
numbered.
V ⊂∗
Vocabulary (dictionary), i.e., a set of distinct words, which is a subset of ∗
|V|
Vocabulary size
v ∈{1, · · · , |V|}
Ordered index number of distinct words in vocabulary V
w(v) ∈V
Word pointed by an ordered index v
{w(v)|v = 1, · · · , |V|} = V
A set of distinct words, which is equivalent to vocabulary V
J ∈Z+
Number of categories in a chunk (e.g., number of words in a sentence or num-
ber of phonemes or HMM states in a speech segment)
i ∈{1, · · · , J}
ith position of category (e.g., word or phoneme)
wi ∈V
Word at ith position
W = {wi|i = 1, · · · , J}
Word sequence from 1 to J
wi
i−n+1 = {wi−n+1 · · · wi}
Word sequence from i −n + 1 to i
p(wi|wi−1
i−n+1) ∈[0, 1]
n-gram probability, which considers n −1 order Markov model

Notation and abbreviations
xix
c(wi−1
i−n+1) ∈Z+
Number of occurrences of word sequence wi−1
i−n+1 in a training corpus
λwi−1
i−n+1
Interpolation weight for each wi−1
i−n+1
M ∈Z+
Number of documents
m ∈{1, · · · , M}
Document index
dm
mth document, which would be represented by a string or positive integer
c(w(v), dm) ∈Z+
Number of co-occurrences of word w(v) in document dm
K ∈Z+
Number of unique latent topics
zi ∈{1, · · · , K}
ith latent topic variable for word wi
Z = {zj|j = 1, · · · , J}
Sequence of latent topic variables for J words
Abbreviations
AIC: Akaike Information Criterion (page 217)
AM: Acoustic Model (page 3)
ARD: Automatic Relevance Determination (page 194)
ASR: Automatic Speech Recognition (page 58)
BIC: Bayesian Information Criterion (page 8)
BNP: Bayesian Nonparametrics (pages 337, 345)
BPC: Bayesian Predictive Classiﬁcation (page 218)
CDHMM: Continuous Density Hidden Markov Model (page 157)
CRP: Chinese Restaurant Process (page 350)
CSR: Continuous Speech Recognition (page 334)
DCLM: Dirichlet Class Language Model (page 326)

xx
Notation and abbreviations
DHMM: Discrete Hidden Markov Model (page 62)
DNN: Deep Neural Network (page 224)
DP: Dirichlet Process (page 348)
EM: Expectation Maximization (page 9)
fMLLR: feature-space MLLR (page 204)
GMM: Gaussian Mixture Model (page 63)
HDP: Hierarchical Dirichlet Process (page 337)
HMM: Hidden Markov Model (page 59)
HPY: Hierarchical Pitman–Yor Process (page 383)
HPYLM: Hierarchical Pitman–Yor Language Model (page 384)
iid: Independently, identically distributed (page 216)
KL: Kullback–Leibler (page 79)
KN: Kneser–Ney (page 102)
LDA: Latent Dirichlet Allocation (page 318)
LM: Language Model (page 3)
LSA: Latent Semantic Analysis (page 113)
LVCSR: Large Vocabulary Continuous Speech Recognition (page 97)
MAP: Maximum A-Posteriori (page 7)
MAPLR: Maximum A-Posteriori Linear Regression (page 287)
MBR: Minimum Bayes Risk (page 56)
MCE: Minimum Classiﬁcation Error (page 59)
MCMC: Markov Chain Monte Carlo (page 337)
MDL: Minimum Description Length (page 9)
MFCC: Mel-Frequency Cepstrum Coefﬁcients (page 249)
MKN: Modiﬁed Kneser–Ney (page 111)
ML: Maximum Likelihood (page 77)
ML2: Type-2 Maximum Likelihood (page 188)
MLLR: Maximum Likelihood Linear Regression (page 200)
MLP: MultiLayer Perceptron (page 326)
MMI: Maximum Mutual Information (page 167)
MMSE: Minimum Mean Square Error (page 139)
MPE: Minimum Phone Error (page 167)
nCRP: nested Chinese Restaurant Process (page 360)
NDP: Nested Dirichlet Process (page 360)
NMF: Non-negative Matrix Factorization (page 124)
pdf: probability density function (page 63)
PLP: Perceptual Linear Prediction (page 54)
PLSA: Probabilistic Latent Semantic Analysis (page 113)
PY: Pitman–Yor Process (page 379)
QB: Quasi-Bayes (page 180)
RHS: Right-Hand Side (page 199)
RLS: Regularized Least-Squares (page 188)
RVM: Relevance Vector Machine (page 192)
SBL: Sparse Bayesian Learning (page 194)

Notation and abbreviations
xxi
SBP: Stick Breaking Process (page 348)
SMAP: Structural Maximum A-Posteriori (page 288)
SMAPLR: Structural Maximum A-Posteriori Linear Regression (page 288)
SVD: Singular Value Decomposition (page 114)
SVM: Support Vector Machine (page 188)
tf–idf: term frequency – inverse document frequency (page 113)
UBM: Universal Background Model (page 172)
VB: Variational Bayes (page 7)
VC: Vapnik–Chervonenkis (page 191)
VQ: Vector Quantization (page 62)
WB: Witten–Bell (page 102)
WER: Word Error Rate (page 56)
WFST: Weighted Finite State Transducer (page 60)
WSJ: Wall Street Journal (page 108)


Part I
General discussion


1
Introduction
1.1
Machine learning and speech and language processing
Speech and language processing is one of the most successful examples of applying
machine learning techniques to real problems. Current speech and language tech-
niques embody our real-world information processing, automatically including informa-
tion extraction, question answering, summarization, dialog, conversational agent, and
machine translation (Jurafsky & Martin 2000). Among these, one of the most excit-
ing applications of speech and language processing is speech recognition based voice
search technologies (by Google, Nuance) and conversational agent technologies (by
Apple) (Schalkwyk, Beeferman, Beaufays et al. 2010). These successful applications
started to make people in general casually use speech interface rather than text interface
in mobile devices, and the applications of speech and language processing are widely
expanding.
One of the core technologies of speech and language processing is automatic speech
recognition (ASR) and related techniques. Surprisingly, these techniques are fully based
on statistical approaches by using large amounts of data. The machine learning tech-
niques are applied to utilize these data. For example, the main components of ASR are
acoustic and language models. The acoustic model (AM) provides a statistical model
of each phoneme/word unit, and it is represented by a hidden Markov model (HMM).
The HMM is one of the most typical examples of dealing with sequential data based
on machine learning techniques (Bishop 2006), and machine learning techniques pro-
vide an efﬁcient method of computing a maximum likelihood value for the HMM and
an efﬁcient training algorithm of the HMM parameters. The language model (LM) also
provides an n-gram based statistical model for word sequences, which is also trained
by using the large amount of data based on machine learning techniques. These statis-
tical models and their variants are used for the other speech and language applications,
including speaker veriﬁcation and information retrieval, and thus, machine learning is a
core component of speech and language processing.
Machine learning covers a wide range of applications in addition to speech and lan-
guage processing, including bioinformatics, data mining, and computer vision. Machine
learning also covers various theoretical ﬁelds including pattern recognition, infor-
mation theory, statistics, control theory, and applied mathematics. Therefore, many
people are studying and developing machine learning techniques, and the progress of
machine learning is rather fast. By following the rapid progress of machine learning,

4
Introduction
researchers in speech and language processing interact positively with the machine
learning community or communities in the machine learning application ﬁeld by import-
ing (and sometimes exporting) advanced machine learning techniques. For example,
the recent great improvement of ASR comes from this interaction for discriminative
approaches (recent progress summaries for discriminative speech recognition techniques
are found in Gales, Watanabe & Fossler-Lussier (2012), Heigold, Ney, Schluter et al.
(2012), Saon & Chien (2012b), Hinton, Deng, Yu et al. (2012). The discriminative train-
ing of HMM parameters has been mainly studied in speech recognition research since
the 1990s, and became a standard technique around the 2000s. In addition, the deep neu-
ral network replaces the emission probability of the HMM from the Gaussian mixture
model (GMM) (or is used as feature extraction (Hermansky, Ellis & Sharma 2000, Grézl,
Karaﬁát, Kontár et al. 2007) for the GMM) and achieves further improvement on the dis-
criminative training based ASR performance. Actually, current successful applications
of speech and language processing are highly supported by these breakthroughs based
on the discriminative techniques developed through the interaction with the machine
learning community. By following the successful experience, researchers in speech and
language processing try to collaborate with the machine learning community further to
ﬁnd new technologies.
1.2
Bayesian approach
This book also follows the trend of tight interaction with the machine learning com-
munity, but focuses on another active research topic in machine learning, called the
Bayesian approach. The Bayesian approach is a major probabilistic theory that repre-
sents a causal relationship of data. By dealing with variables introduced in a model as
probabilistic variables, we can consider uncertainties included in these variables based
on the probabilistic theory.
As a simple example of uncertainty, we think of statistically modeling several data (x1,
x2, · · · , xN) by a Gaussian distribution N(x|μ, ) with mean and variance parameters μ
and , as shown in Figure 1.1, i.e.,
p(x) ≈N(x|μ, ).
(1.1)
Now, we consider the Bayesian approach, where the mean parameter is uncertain, and
is distributed by a probabilistic function p(μ). Since μ is uncertain, we can consider
the several possible μs instead of one ﬁxed μ, and the Bayesian approach considers
representing a distribution of x by several Gaussians with possible mean parameters
(μ1, μ2, and μ3 in the example of Figure 1.1),
p(x) ≈1
N

μ={μ1,μ2,··· ,μN}
N(x|μ, ),
(1.2)
where μ1, μ2, · · · are generated from the distribution p(μ). The extreme case of this
uncertainty consideration is to represent the distribution of x, which is represented by

1.2 Bayesian approach
5
Figure 1.1
The uncertainty of the mean parameter of a Gaussian distribution.
all possible Gaussian distributions, with all possible mean parameters weighted by the
probability p(μ), which is represented as the following integral equation:
p(x) ≈

N(x|μ, )p(μ)dμ.
(1.3)
This expectation over uncertain variables is called marginalization. The grayed band in
Figure 1.1 provides an image of the expected distribution, where the mean parameter
is marginalized over all possible inﬁnite mean values. This is a unique aspect of the
Bayesian approach that represents variables in a model by probabilistic distributions,
and holds their uncertainties.
This uncertainty consideration often improves the generalization capability of a
model that yields to mitigate the mismatch between training and unseen data and avoid
over-ﬁtting problems by the effect of regularization and marginalization. For example,
Figure 1.2 points out the over-ﬁtting problem. In real applications, we often face phe-
nomena where observed data cannot cover all possible unobserved data (unseen data)
and their distributions are mismatched. The Gaussian distribution with a ﬁxed mean
parameter that is estimated from observed data can well represent the observed data,
but it cannot represent unseen data properly. This is a well-known over-ﬁtting prob-
lem, that the model overly ﬁts its parameters to represent observed data. However,
since the Bayesian approach considers all possible mean parameters by the marginal-
ization, some of these parameters would properly model unseen data more accurately
than the ﬁxed mean parameter case. The effect of this more powerful representation
ability for unseen data leads to improved generalization capability, which is a famous
advantage of the Bayesian approach. In addition, the example can also be viewed as

6
Introduction
Figure 1.2
The Bayesian approach that holds uncertainty of the mean parameter has an ability to describe
unseen data.
showing that the mean parameter is regularized not to overly ﬁt the observed data to
p(μ). The effect of setting the constraints for variables via their probabilistic distri-
butions is a typical example of the regularization, and the regularization also leads to
improved generalization capability.
Furthermore, the Bayesian theory provides a straightforward mathematical way to
infer (predict) unobservable probabilistic variables by using the basic rules (including
the Bayes theorem) within the basic probabilistic theory. The beauties of this mathemat-
ical treatment based on probabilistic theory and the expected robustness by considering
uncertainties attract many machine learning researchers to study the Bayesian approach.
Actually, Bayesian machine learning has also rapidly grown similarly to discrimina-
tive techniques, and various interesting approaches have been proposed (Bishop 2006,
Barber 2012).
However, compared with the successful examples of the discriminative techniques,
the applications of the Bayesian approach in speech and language processing are rather
limited despite its many advantages. One of the most difﬁcult problems is that the exact
Bayesian approach cannot be applied in our speech and language processing without
some approximations. For example, the Bayesian approach is based on the conditional
distribution given datum x (that is called posterior distribution, p(a|x)). However, it is
generally difﬁcult to obtain the posterior distribution analytically, since we cannot solve
the equation analytically to obtain the posterior distribution. The computation is often
performed numerically, which limits the practical applications of the Bayesian approach,
especially for speech and language processing that deals with large amounts of data.
Thus, how to make the Bayesian approach more practical for speech and language
processing is the most critical problem in Bayesian speech and language processing.
This book is aimed to guide readers in machine learning or speech and language
processing to apply the Bayesian approach to speech and language processing in
a systematic way. In other words, this book aims to bridge the gap between the
machine learning community and the speech and language community by removing

1.2 Bayesian approach
7
Bayesian approaches are quite popular
in machine learning
But still have some difficulties
This book wants to
bridge this gap!!
in speech and language processing.....
Figure 1.3
The aim of this book.
their preconceived ideas of the difﬁculty in these applications (Figure 1.3) (Watanabe
& Chien 2012). The key idea for this guidance is how to approximate the Bayesian
approach for speciﬁc speech and language applications (Ghahramani 2004). There
are several approximations developed mainly in the machine learning and Bayesian
statistics ﬁelds to deal with the problems. This book mainly deals with the following
approximations:
• Chapter 4: Maximum a-posteriori approximation (MAP);
• Chapter 5: Evidence approximation;
• Chapter 6: Asymptotic approximation;
• Chapter 7: Variational Bayes (VB);
• Chapter 8: Markov chain Monte Carlo (MCMC).
Note that there are some other interesting approximations (e.g., loopy belief propaga-
tion (Murphy, Weiss & Jordan 1999, Yedidia, Freeman & Weiss 2003) and expectation
propagation (Minka 2001)), but our book focuses on the above approximations. These
approximations are described in the corresponding chapters in detail. We organize the
chapters in Part II categorized by these approximation techniques, unlike application-
oriented categorization (e.g., speech recognition and speech synthesis) as has appeared
in many other speech and language books (Jurafsky & Martin 2000, Huang, Acero
& Hon 2001), to emphasize how to approximate the Bayesian approach for practical
applications. For example, the ﬁrst sections in each chapter in Part II describe the intro-
duction of the corresponding approximation, and provide the recipe for how to use
the approximation in machine learning problems in general. The following sections
in the chapter provide the approximated solutions of statistical models used in spe-
ciﬁc speech and language applications by following the recipe. This book mainly deals
with popular statistical models including HMM, GMM, neural network, factor anal-
ysis, n-gram, and latent topic models. Table 1.1 summarizes approximated Bayesian
inferences for statistical models discussed in this book. The applications covered by
this book are typical topics in speech and language processing based on these statisti-
cal models with the approximated Bayesian treatment, and mainly related to automatic
speech recognition.

8
Introduction
Table 1.1 Approximated Bayesian inference for statistical models discussed in this book.
Approximation
HMM
GMM
Neural
Factor
n-gram
Latent topic
network
analysis
model
MAP
4.3, 4.5
4.6
4.7
4.8
Evidence
5.2
5.3
Asymptotic
6.3, 6.5
6.6
6.4
VB
7.3
7.5
7.6
MCMC
8.3, 8.4
8.5
1.3
History of Bayesian speech and language processing
There have been various studies to apply the Bayesian approach in speech and language
processing. Although one of the aims of this book is to summarize these studies in a
more systematic way in terms of an approximated Bayesian inference view, this section
brieﬂy reviews the history of these studies, according to time, by categorizing them
within four trends.
The major earliest trend of using the Bayesian approach to speech and language
processing started with the statistical modeling of automatic speech recognition in the
1980s. Furui (2010) reviews a historical perspective of automatic speech recognition and
calls the technologies developed with statistical modeling around the 1980s the third
generation technology. In a Bayesian perspective, this statistical modeling movement
corresponded to the ﬁrst introduction of probabilistic variables for speech recognition
outputs (e.g., word sequences). As a result, the statistical modeling of automatic speech
recognition formulates the speech recognition process as Bayes decision theory, and
the noisy channel model is provided to solve the decision problem of determining most
probable word sequences by considering the product of acoustic and language model
distributions based on the Bayes theory (Jelinek 1976). The language model is used to
provide a prior distribution of word sequences.
The second trend in the 1990s was to expand the Bayesian perspective from the
speech recognition outputs to model parameters by regarding these as probabilistic vari-
ables. Maximum a-posteriori (MAP) estimation of HMM parameters is known as the
most successful application of the Bayesian approach (Lee, Lin & Juang 1991, Gauvain
& Lee 1994). This approach was used for speaker adaptation, where the prior distri-
bution of HMM parameters is estimated from a large amount of speaker-independent
data and the MAP estimation is used to estimate target speaker’s HMM parameters
with a small amount of target speaker data. The prior distribution regularizes the tar-
get speaker’s HMM parameters to guarantee the performance of speaker independent
HMMs instead of overly tuning to the model. Another example of the expansion was to
treat model structure as a probabilistic variable in the late 1990s. This treatment enables
model selection by selecting a most probable model structure from the posterior dis-
tribution of the model structure given training data (e.g., the numbers of HMM states
and Gaussians). The Bayesian information criterion (BIC) and the minimum description

1.4 Applications
9
length (MDL) criterion are successful examples (Shinoda & Watanabe 1996, Chen &
Gopinath 1999, Chou & Reichl 1999, Zhou & Hansen 2000) that cover wide appli-
cations of speech and language processing (e.g., acoustic model selection, speaker
clustering, and speaker segmentation).
This second trend made the importance of the Bayesian approach come alive within
the speech and language communities. From the late 1990s to 2000s, many other
Bayesian studies have been applied to speech and language processing, which are clas-
siﬁed as the third trend. The Bayesian techniques (MAP and BIC) used in the second
trend miss the important Bayesian concept, marginalization, which makes the full treat-
ment of the Bayesian approach difﬁcult. By following the progress of VB, MCMC,
Evidence approximation, and graphical model techniques in machine learning in the
1990s, people in speech and language processing started to apply more exact Bayesian
approaches by fully incorporating marginalization. These studies covered almost all
statistical models in speech and language processing (Bilmes & Zweig 2002, Watan-
abe, Minami, Nakamura & Ueda 2002, Blei, Ng & Jordan 2003, Saon & Chien 2011).
The most successful approach in the third trend is latent Dirichlet allocation (LDA
(Blei et al. 2003)), which provides a VB solution of a latent topic model from a
probabilistic latent semantic analysis (Hofmann 1999b). LDA has been mainly devel-
oped in machine learning and natural language processing by incorporating a Gibbs
sampling solution (Grifﬁths & Steyvers 2004), and structured topic models (Wallach
2006), and LDA was extended to incorporate Bayesian nonparametrics (Teh, Jordan,
Beal & Blei 2006), which became the fourth trend in Bayesian speech and language
processing.
The fourth trend is a still ongoing trend that tries to fully incorporate Bayesian non-
parametrics with speech and language processing. Due to their computational costs and
algorithmic difference from the standard expectation maximization (EM) type algo-
rithms (Dempster, Laird & Rubin 1976), the applications were limited to latent topic
models and related studies. However, recent computational progress (e.g., many core
processing and GPU processing) has enabled broadening of the fourth trend in statisti-
cal models in speech and language processing other than extended latent topic models
(Teh 2006, Goldwater 2007, Fox, Sudderth, Jordan et al. 2008, Ding & Ou 2010, Lee &
Glass 2012).
This book covers all four trends and categorizes these popular techniques with the
approximated Bayesian inference techniques.
1.4
Applications
This book aims to describe the following target applications:
• Automatic speech recognition (ASR, Figure 1.4)
This is a main application in this book, that converts human speech to texts. The
main techniques required in ASR are based on speech enhancement for noise reduc-
tion, speech feature extraction, acoustic modeling, language modeling, pronunciation

10
Introduction
Figure 1.4
Automatic speech recognition.
lexicon modeling, and search. The book mainly deals with acoustic models repre-
sented by HMM and neural networks, and language models represented by n-gram
and latent topic models.
• Speaker veriﬁcation/segmentation/clustering (Figure 1.5)
Speaker recognition techniques automatically provide a speaker identity given speech
data. This often requires extracting speech segments uttered by target speakers from
the recorded utterances (speaker segmentation). In addition, some of the real appli-
cations cannot have speaker labels in advance, and clustering speech segments to
a speciﬁc speaker cluster (speaker clustering) is also another important direction.
Gaussian or GMMs are used as statistical models. However, state-of-the-art speaker
veriﬁcation systems use GMMs as preprocessing, namely GMM parameters esti-
mated from a speech segment are used as speaker features. The estimated features
are further processed by a factor analysis to remove speaker-independent feature
characteristics statistically.
• (Spoken) Information retrieval (IR, Figure 1.6)
Information retrieval via document classiﬁcation given a text (or spoken) query is
another application of speech and language processing. The document can be repre-
sented by various units including newspaper articles, web pages, dialog conversations,
sentences/utterances; which is used depends on applications. The most successful
application for information retrieval is a search engine that uses a web page as a
document unit. Voice search is an instance of a search engine based on spoken infor-
mation retrieval where spoken terms are converted to text-form terms by using ASR
and these terms are used as a query. The approach is based on the vector space model,
which represents documents and queries as vectors in a vector space. The vector is
simply represented by count or weighted count of unique words in a vocabulary. The
approach often uses n-gram or latent topic models to provide more informative vector
representation of documents.

1.5 Organization of this book
11
Mary
Who is 
speaking in 
this segment?
3
Cluster 2
How many people
are speaking?
Which speaker cluster 
does this segment belong to?
When is Mary
speaking?
From 3’5 s
to 4’9 s
Speaker
segmentaon
Speaker
recognion
Speaker
clustering
Figure 1.5
Speaker veriﬁcation/segmentation/clustering.
1.5
Organization of this book
This book is divided into two parts. Part I starts by describing the Bayesian approach
in general in Chapter 2, but the topics discussed in this book are focused on practical
machine learning problems regarding speech and language processing rather than on
general Bayesian statistics. Chapter 3 also provides general discussion of speech and
language processing by providing some important statistical models. The discussion
is based on the conventional style based on the maximum likelihood approach. These
models are re-formulated in the latter sections in a Bayesian manner with an appropri-
ated approximation. The ﬁnal section in Chapter 3 also describes the application of the
Bayesian approach to statistical models in automatic speech recognition, and discusses
the difﬁculty of straightforward application.
As described before, Part II provides approximated Bayesian inferences based on
MAP, Evidence, Asymptotic, VB, and MCMC approximations, and relevant sections
describe statistical models and their applications, as shown in Table 1.1. We also provide
in the appendices some useful formulas and rules used.
For readers
Our book assumes three types of readers: 1) undergraduate and graduate students; 2)
those who have some knowledge in machine learning; and 3) those who have some

12
Introduction
Restaurants in
Boston
Restaurants in Boston
Figure 1.6
(Spoken) Information retrieval.
knowledge in speech and language processing. We recommend undergraduate and
graduate students to read through this book, as we have made it self-consistent, espe-
cially for deriving equations. Readers who have knowledge in machine learning can skip
the beginning part of Chapter 2 about the “Bayesian approach” in general. Similarly,
readers who have knowledge in speech and language processing may skip Chapter 3
about statistical models in “speech and language processing”. However, compared with
conference and journal papers in this ﬁeld, the chapter provides comprehensive under-
standing of standard statistical models with full derivations of equations. Therefore, we
highly recommend readers familiar with these models to read through the whole book.

2
Bayesian approach
This chapter describes a general concept and statistics of the Bayesian approach. The
Bayesian approach covers wide areas of statistics (Bernardo & Smith 2009, Gelman,
Carlin, Stern et al. 2013), pattern recognition (Fukunaga 1990), machine learning
(Bishop 2006, Barber 2012), and applications of these approaches. In this chapter, we
start the discussion from the basic probabilistic theory, and mainly describe the Bayesian
approach by aiming to follow a machine learning fashion of constructing and reﬁning
statistical models from data. The role of the Bayesian approach in machine learning is
very important since the Bayesian approach provides a systematic way to infer unob-
served variables (e.g., classiﬁcation category, model parameters, latent variables, model
structure) given data. This chapter limits the discussions considering the speech and lan-
guage problems in the latter chapters, by providing simple probabilistic rules, and prior
and posterior distributions in Section 2.1. The section also provides analytical solutions
of posterior distributions of simple models. Based on the basic introduction, Section
2.2 introduces a useful representation of the relationship of probabilistic variables in
the Bayesian approach, called the Graphical model. The graphical model representation
gives us an intuitive view of statistical models even when they have complicated rela-
tionships between their variables. Section 2.3 explains the difference between Bayesian
and maximum likelihood (ML) approaches. The following chapters extend the general
Bayesian approach described in this chapter to deal with statistical models in speech and
language processing.
2.1
Bayesian probabilities
This section describes the basic Bayesian framework based on probabilistic theory.
Although some of the deﬁnitions, equations, and concepts are trivial, this section reviews
the basics to assist readers to fully understand the Bayesian approach.
In the Bayesian approach, all the variables that are introduced when models are
parameterized, such as model parameters and latent variables, are regarded as proba-
bilistic variables. Thus, let a be a discrete valuable, then the Bayesian approach deals
with a as a probabilistic variable, and aims to obtain p(a):
a →p(a).
(2.1)

14
Bayesian approach
Hereinafter, we assume that a is a discrete variable, and the expectation is performed
by the summation over a for simplicity. Since p(a) is a probabilistic distribution, p(a)
always satisﬁes the following condition:

a
p(a) = 1,
p(a) ≥0
∀a.
(2.2)
These properties help us to solve some calculations appearing in the following sections.
In the continuous variable case, the summation  is replaced with the integral

.
2.1.1
Sum and product rules
Since the Bayesian approach treats all variables as probabilistic variables, the proba-
bilistic theory gives us the two important probabilistic rules to govern the relationship
between the variables. Let a and b be arbitrary probabilistic variables,
• Sum rule
p(b) =

a
p(a, b);
(2.3)
• Product rule
p(a, b) = p(a|b)p(b) = p(b|a)p(a).
(2.4)
Here, p(a, b) is a joint probability that represents a probability of all possible joint events
of a and b. p(a|b) or p(b|a) is a conditional probability. These are generalized to N
probabilistic variables, for example, if we have a1, · · · , aN probabilistic variables, there
rules are represented as:
• Sum rule
p(ai) =

a1
· · ·

ai−1

ai+1
· · ·

aN
p(a1, · · · , aN);
(2.5)
• Product rule
p(a1, · · · , aN) = p(a1|a2, · · · , aN)p(a2, · · · , aN) = · · ·
= p(aN)
N−1

n=1
p(an|an+1, · · · , aN).
(2.6)
In a Bayesian manner, we formulate the probability distributions based on these rules.
For example, the famous Bayes theorem can be derived by reforming the product rule
in Eq. (2.4), as follows:
p(a|b) = p(a, b)
p(b)
= p(b|a)p(a)
p(b)
(2.7)
=
p(b|a)p(a)

a p(b|a)p(a).
(2.8)
To derive Eq. (2.8), we use the sum and product rules for p(a). The following discussion
provides more practical examples based on this discussion.

2.1 Bayesian probabilities
15
2.1.2
Prior and posterior distributions
The above Bayes theorem has an interesting meaning if we consider a conditional prob-
ability distribution of a given an observation x. The conditional distribution p(a|x) is
called posterior distribution, and the main purpose of the Bayesian approach is to infer
the posterior distribution of various valuables. Based on the Bayes theorem in Eq. (2.7),
the posterior distribution is decomposed in Eq. (2.9) to the following three distributions:
p(a|x) = p(x|a)p(a)
p(x)
(2.9)
=
p(x|a)p(a)

a p(x|a)p(a),
(2.10)
where p(x|a) is a likelihood function of x and p(a) is a distribution without considering
any observation, and called prior distribution. p(x) is a distribution of an observation,
and can be computed by using p(x|a) and p(a) based on Eq. (2.10). In most speech pro-
cessing applications, it is difﬁcult to estimate the posterior distribution directly (Section
3.8 describes it in detail). Therefore, the posterior distribution p(a|x) is indirectly esti-
mated via this Bayes theorem, which is derived from the sum and product rules, which
are equivalence equations without approximation.
Since the posterior distribution provides a probability of a given data x, this matches
one of the machine learning goals of reﬁning information of a from data x. Therefore,
the posterior distribution plays an important role in machine learning, and obtaining an
appropriate posterior distribution for our problems in speech and language processing is
a main goal of this book.
Once we obtain the posterior distribution p(a|x), we can obtain the values of a via:
• Maximum a-posteriori (MAP) procedure:
aMAP = arg max
a
p(a|x);
(2.11)
• Expectation with respect to the posterior distribution:
aEXP = E(a)[a|x] ≜

a
a · p(a|x).
(2.12)
The MAP and expectation are typical ways to obtain meaningful information about a
given x in terms of the probabilistic theory. From Eq. (2.10), p(x) is disregarded in the
MAP procedure as a constant factor that is independent of a, MAP, and expectation,
which makes the calculation simple. The MAP and expectation are generalized to obtain
meaningful information f(a) given the posterior distribution p(a|x). More speciﬁcally, if
we consider a likelihood function of unseen data y given a, i.e., p(y|a), these procedures
are rewritten as:
• Maximum a-posteriori (MAP) procedure:
pMAP(y|a) = p(y| arg max
a
p(a|x)) = p(y|aMAP);
(2.13)

16
Bayesian approach
• Expectation with respect to the posterior distribution:
pEXP(y) = E(a)[p(y|a)|x] ≜

a
p(y|a)p(a|x).
(2.14)
Thus, we can predict y by using these procedures. Note that the MAP procedure decides
a deterministic value of a, while the expectation procedure keeps possible a for the
expectation. Therefore, the MAP procedure is called hard decision and the expectation
procedure is called soft decision.
The expectation is a more general operation than MAP in terms of considering the
distribution shape. For example, if we approximate p(a|x) with a speciﬁc Kronecker
delta function δ(a, aMAP) where aMAP = arg maxa p(a|x), Eq. (2.14) is represented as
pEXP(y) =

a
p(y|a)δ(a, aMAP) = p(y|aMAP)
= p(y| arg max
a
p(a|x))
= pMAP(y|a),
(2.15)
where
δ(a, a′) =

1
a = a′
0
otherwise.
(2.16)
Thus, the MAP value is obtained from the speciﬁc case of the expectation value without
considering the distribution shape of p(a|x). However, in many cases, the MAP value is
also often used since the expectation needs a complex computation due to the summa-
tion over a. Note that the above derivation via a Kronecker delta function (or Dirac
delta function when we consider continuous variables) is often used to provide the
relationship of the MAP and expectation values.
2.1.3
Exponential family distributions
The previous section introduces the posterior distribution. This section focuses on a
speciﬁc problem of posterior distributions that consider the model parameter  given
a set of D dimensional observation vectors, i.e., X = {xn ∈RD|n = 1, · · · , N}. The
problem here is to obtain the posterior distribution p(|X), i.e., it is a general estimation
problem of obtaining the distribution of  from data X. Once we obtain p(|X), for
example, we can estimate MAP or compute some expectation values, as we discussed
in Section 2.1.2.
Then, the Bayes theorem, which provides the relationship between prior and posterior
distributions in Eq. (2.9) or (2.10), can be represented as follows:
p(|X) = p(X|)p()
p(X)
(2.17)
=
p(X|)p()

p(X|)p()d.
(2.18)

2.1 Bayesian probabilities
17
Here, we use the integral

rather than  in Eq. (2.18), since model parameters are often
represented by continuous variables (e.g., mean and variance parameters in a Gaussian
distribution). In this particular case, the Bayes theorem has the more practical meaning
that the posterior distribution p(|X) is represented by the likelihood function p(X|),
and the prior distribution of the model parameters p(). Thus
p(X) =

p(X|)p()d,
(2.19)
which is also called evidence function or marginal likelihood . The evidence plays an
important role in Bayesian inference, which is described in Chapter 5 in detail.
Basically, we can set any distributions (e.g., Gaussian, gamma, Dirichlet, Laplace,
Rayleigh distributions, etc.) to prior and posterior distributions. However, a particular
family of distributions called conjugate distribution makes analytical derivation sim-
pler. Before we describe the conjugate distributions, the following section explains
exponential family distributions, which are required to explain conjugate distributions.
Exponential family
The exponential family is a general distribution family, which contains standard distribu-
tions including Gaussian distribution, gamma distribution, and multinomial distribution.
Let θ be a vector form of model parameters. A distribution of a set of observation vec-
tors X = {x1, · · · , xN} given θ (likelihood function), which belongs to the exponential
family, is represented by the following exponential form:
p(X|θ) ≜h(X) exp

γ (θ)⊺t(X) −g(γ )

,
(2.20)
where t(X) is a sufﬁcient statistics vector obtained from observation vector X, g(γ ) is a
logarithmic normalization factor. γ is a transformed vector of θ, and is called a natural
parameter vector. If γ (θ) = θ, it is called the canonical form, that simpliﬁes Eq. (2.20)
as follows:
p(X|θ) = h(X) exp

θ⊺t(X) −g(θ)

.
(2.21)
The canonical form makes the calculation of posterior distributions simple.
If we have J multiple parameter vectors, we can represent the exponential form as the
factorized form:
p(X|θ1, · · · , θJ) ≜h(X)
J
i=1
exp

γ i(θi)⊺ti(x) −gi(γ i)

.
(2.22)
When the transformed model parameters are composed of a matrix or a vector, we can
also deﬁne the exponential family distribution. For example, a multivariate Gaussian
distribution N(·|μ, ) is parameterized by a mean vector μ and a covariance matrix ,
and the corresponding transformed parameters are also represented by vector γ 1 and
matrix 2. Then, the exponential family distribution for  = {θ1, 2} is deﬁned as
follows:
p(X|) ≜h(X) exp

γ ⊺
1 t1(X) + tr[⊺
2 T2(x)] −g(γ 1, 2)

.
(2.23)

18
Bayesian approach
Here, t1 and T2 are vector and matrix representations of sufﬁcient statistics, respec-
tively. The rest of this section provides examples of h(·), g(·), γ (·), and t(·) for standard
distributions (Gaussian, multivariate Gaussian, and multinomial distributions).
Example 2.1
Gaussian (unknown mean):
We focus on the exponential family form of the Gaussian distribution for scalar observa-
tion X = {xn ∈R|n = 1, · · · , N}. As a simple example, we only focus on the Gaussian
mean as a model parameter, and regard the precision parameter r as a constant value, i.e.,
N(xn|μ; r−1) where the variables located right after the semicolon; means that these are
not treated as probabilistic variables, but speciﬁc values. That is θ = μ in Eq. (2.20).
We use the precision parameter r instead of the variance parameter ,1 which makes
the solution simple. Based on the deﬁnition in Appendix C.5, the standard form of the
Gaussian distribution is represented as
N

n=1
N(xn|μ; r−1) =
2π
r
−N
2
exp

−
N

n=1
r
2(xn −μ)2

.
(2.24)
We assume that x1, · · · , xN are independent and identically distributed random variables
from the Gaussian. The standard form of the Gaussian distribution is rewritten as the
following exponential form:
N

n=1
N(xn|μ; r−1) =
2π
r
−N
2
exp

−r
2
N

n=1
x2
n

exp

rμ
N

n=1
xn −Nμ2r
2

=
2π
r
−N
2
exp

−r
2
N

n=1
x2
n




=h(X)
exp
⎛
⎜⎜⎜⎜⎜⎝
r
N

n=1
xn
  
=t(X)
μ

=γ
−Nμ2r
2
  
=g(γ )
⎞
⎟⎟⎟⎟⎟⎠
.
(2.25)
Thus, the Gaussian distribution is represented by the following exponential form in
Eq. (2.20):
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t(X) = r
N

n=1
xn
h(X) =
2π
r
−N
2
exp

−r N
n=1 x2
n
2

γ (μ) = μ
g(γ ) = Nγ 2r
2
.
(2.26)
1 This book regards  as the variance parameter (not the standard deviation, which is represented as σ), as
shown in Appendix C.5, to make the notation consistent with the covariance matrix .

2.1 Bayesian probabilities
19
Since γ (μ) = μ in Eq. (2.26), it is regarded as a canonical form, as discussed in
Eq. (2.21). Note that the parameterization of γ (μ) and t(X) is not unique. For example,
we can obtain the following parameterization from:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t(X) =
N

n=1
xn
h(X) =
2π
r
−N
2
exp

−r N
n=1 x2
n
2

γ (μ) = μr
g(γ ) = Nμ2r
2
= Nγ 2
2r .
(2.27)
This is also another exponential form of the Gaussian distribution with unknown mean.
Example 2.2
Gaussian (unknown mean and precision):
Similarly to Example 2.1, we focus on the exponential family form of the Gaussian dis-
tribution for scalar observation X, but regard r as also unknown. Therefore, θ = [μ, r]⊺.
Thus, unlike the scalar forms of the natural parameter γ and sufﬁcient statistics t in
Eq. (2.24), the Gaussian distribution is represented by the vector form of these as
N

n=1
N(xn|μ, r−1) =
2π
r
−N
2
exp

−Nμ2r
2

exp

−r
2
N

n=1
x2
n + μr
N

n=1
xn

= exp
⎛
⎜⎜⎜⎜⎝

μr
r
⊺
  
=γ (θ)
&N
n=1 xn
−
N
n=1 x2n
2
'



t(X)
−
N
2 log
2π
r

+ Nμ2r
2




=g(γ )
⎞
⎟⎟⎟⎟⎠
.
(2.28)
Therefore,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t(X) =
&N
n=1 xn
−
N
n=1 x2
n
2
'
h(X) = 1
γ (θ) =

μr
r

g(γ ) = N
2

log 2π
r + μ2r

= N
2

log 2π
γ2
+ γ 2
1
γ2

.
(2.29)
Again, the parameterization of γ (θ) and t(x) is not unique and the parameterization of
γ (θ) =

μr, −r
2
⊺and t(X) = [N
n=1 xn, N
n=1 x2
n]⊺is also possible.

20
Bayesian approach
Example 2.3
Multivariate Gaussian (unknown mean and precision):
The next example is to derive an exponential form of the multivariate Gaussian
distribution with D dimensional mean vector μ and D × D precision matrix R (we
use precision matrix R instead of covariance matrix  to make the solution simple).
A set of the parameters is  = {μ, R}. This is the most important example in this
book, since statistical models in speech and language processing are often represented
by multivariate Gaussian distributions, as discussed in Chapter 3. Let X = {xn ∈
RD|n = 1, · · · , N} be independent and identically distributed random variables from
the multivariate Gaussian distribution. Again, based on the deﬁnition in Appendix C.6,
the standard form of the Gaussian distribution is represented as
N

n=1
N(xn|μ, R−1) =
N

n=1
(2π)−D
2 |R|
1
2 exp

−1
2(xn −μ)⊺R(xn −μ)

= (2π)−ND
2 |R|
N
2 exp

−1
2
N

n=1
(xn −μ)⊺R(xn −μ)

.
(2.30)
Now we focus on the exponential part in Eq. (2.30), which is rewritten as follows:
N

n=1
(xn −μ)⊺R(xn −μ)
= −μ⊺R
N

n=1
xn −
 N

n=1
x⊺
n

Rμ +
N

n=1
x⊺
n Rxn + Nμ⊺Rμ.
(2.31)
To make the observation vector and parameter the inner product form, we ﬁrst use the
trace representation of the quadratic term of xn as
N

n=1
x⊺
n Rxn = tr
& N

n=1
x⊺
n Rxn
'
= tr
& N

n=1
Rxnx⊺
n
'
= tr
&
R
N

n=1
xnx⊺
n
'
,
(2.32)
where we use the fact that the trace of the scalar value is equal to the original
scalar value, the cyclic property, and the distributive property of the trace as in
Appendix B:
a = tr[a],
(2.33)
tr[ABC] = tr[BCA],
(2.34)
tr[A(B + C)] = tr[AB + AC].
(2.35)

2.1 Bayesian probabilities
21
In addition, we can also use the following equation:
μ⊺R
N

n=1
xn +
 N

n=1
x⊺
n

Rμ = 2μ⊺R
N

n=1
xn.
(2.36)
Here, since these values are scalar values, we use the following equation to derive
Eq. (2.36).
 N

n=1
x⊺
n

Rμ =
 N

n=1
x⊺
n

Rμ
⊺
= μ⊺R⊺
 N

n=1
x⊺
n
⊺
= μ⊺R
N

n=1
xn,
(2.37)
since the transpose of the scalar value is the same as the original scalar value (a⊺= a)
and R is a symmetric matrix (R⊺= R). Thus, by substituting Eqs. (2.32) and (2.36) into
Eq. (2.31), Eq. (2.31) is rewritten as
N

n=1
(xn −μ)⊺R(xn −μ)
= −2μ⊺R
N

n=1
xn + tr
&
R
N

n=1
xnx⊺
n
'
+ Nμ⊺Rμ.
(2.38)
Note that Eq. (2.38) is a useful form, and it is used in the following sections to calculate
the various equations for the multivariate Gaussian distribution.
Therefore, by substituting Eq. (2.38) into Eq. (2.30), we can obtain the exponential
form of the multivariate Gaussian distribution as follows:
N

n=1
N(xn|μ, R−1)
= (2π)−ND
2 |R|
N
2 exp

μ⊺R
N

n=1
xn −1
2tr
&
R
N

n=1
xnx⊺
n
'
−N
2 μ⊺Rμ

= exp

μ⊺R
N

n=1
xn −1
2tr
&
R
N

n=1
xnx⊺
n
'
−N
2
(
log((2π)D|R|−1) + μ⊺Rμ
)
.
(2.39)
Thus, by comparing with Eq. (2.23), we obtain the following parameterization for the
multivariate Gaussian distribution:

22
Bayesian approach
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t1(X) =
N

n=1
xn
T2(X) = −1
2
N

n=1
xnx⊺
n
h(x) = 1
γ 1() = Rμ
2() = R
g(γ 1, 2) = N
2
(
log((2π)D|R|−1) + μ⊺Rμ
)
= N
2
(
log((2π)D|2|−1) + γ ⊺
1 −1
2 γ 1
)
.
(2.40)
Note that if D →1, we have xn →xn, μ →μ, R →r, and Eq. (2.40) is equivalent to
Eq. (2.29).
Example 2.4
Multinomial distribution:
The standard form of the multinomial distribution (Eq. (C.2)) is represented as follows:
Mult(x1, · · · , xJ|ω1, · · · , ωJ) ≜
N!
*J
j=1 xj!
J
j=1
ωxj
j ,
(2.41)
where xj is a non-negative integer, and
J

j=1
xj = N.
(2.42)
The parameter {ω1, · · · , ωJ} has the following constraint:
J

j=1
ωj = 1,
0 ≤ωj ≤1
∀j.
(2.43)
Therefore, the number of the free parameters is J−1. To deal with the constraint, we ﬁrst
consider the {ω1, · · · , ωJ−1} as the target vector parameters, i.e., θ ≜[ω1, · · · , ωJ−1]⊺.
ωJ is represented by
ωJ = 1 −
J−1

j=1
ωj.
(2.44)
Similarly to the previous Gaussian-based distributions, the multinomial distribution is
also represented as the exponential form as follows:

2.1 Bayesian probabilities
23
Mult(x1, · · · , xJ|ω1, · · · , ωJ) =
N!
*J
j=1 xj!
exp
⎛
⎝log
⎛
⎝
J
j=1
ωxj
j
⎞
⎠
⎞
⎠
=
N!
*J
j=1 xj!
exp
⎛
⎝
J

j=1
xj log ωj
⎞
⎠.
(2.45)
By using Eqs. (2.42) and (2.44) for xJ and ωJ, respectively, the exponential part of
Eq. (2.45) is rewritten as
Mult(x1, · · · , xJ|ω1, · · · , ωJ)
∝exp
⎛
⎝
J−1

j=1
xj log ωj +
⎛
⎝N −
J−1

j=1
xj
⎞
⎠log
⎛
⎝1 −
J−1

j=1
ωj
⎞
⎠
⎞
⎠
= exp
⎛
⎝
J−1

j=1
xj log ωj −
J−1

j=1
xj log
⎛
⎝1 −
J−1

j=1
ωj
⎞
⎠+ N log
⎛
⎝1 −
J−1

j=1
ωj
⎞
⎠
⎞
⎠
= exp
⎛
⎜⎜⎜⎜⎜⎜⎝
J−1

j=1
xj log
ωj
1 −J−1
j=1 ωj



≜x⊺γ
+N log
⎛
⎝1 −
J−1

j′=1
ωj′
⎞
⎠
⎞
⎟⎟⎟⎟⎟⎟⎠
,
(2.46)
where ∝denotes the proportional relation between left- and right-hand-side equations.
Since the probabilistic function has the normalization factor, which can be neglected
for most of the calculations, ∝is often used to omit the normalization constant from
the equations. Thus, we can derive the linear relationship between xj and γj, which is
deﬁned with {ωj}J−1
j=1 as follows:
γj ≜log
ωj
1 −J−1
j′=1 ωj′
.
(2.47)
Note that ωj is represented by γj by using the following equation:
ωj =
exp(γj)
1 + J−1
j′=1 exp(γj′)
.
(2.48)
This is conﬁrmed by substituting Eq. (2.47) into Eq. (2.48) as
exp(γj)
1 + J−1
j′=1 γj′
=
ωj
1−J−1
j′=1 ωj′
1 + J−1
j′=1
ωj′
1−J−1
j′′=1 ωj′′
=
ωj
1 −J−1
j′=1 ωj′ + J−1
j′=1 ωj′
= ωj.
(2.49)

24
Bayesian approach
Therefore, the canonical form of the multinomial distribution with the parameter θ =
[ω1, · · · , ωJ−1]⊺for x = [x1, · · · , xJ−1]⊺is represented as
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t(x) = x
h(x) =
N!
*J
j=1 xj!
γ (θ) =
+
log
ω1
1−J−1
j=1 ωj , · · · , log
ωJ−1
1−J−1
j=1 ωj
,⊺
g(γ ) = −N log
⎛
⎝1 −
J−1

j=1
ωj
⎞
⎠
= −N log
⎛
⎝1 −
J−1

j=1
exp(γj)
1 + J−1
j′=1 exp(γj′)
⎞
⎠
= N log
⎛
⎝1 +
J−1

j=1
exp(γj)
⎞
⎠.
(2.50)
Note that since the multinomial distribution has constraints for the observation xj in
Eq. (2.42) and the parameter ωj in Eq. (2.44), the obtained canonical form of the multi-
nomial distribution involves these constraints with J −1 variables for sufﬁcient statistics
t and the transformed vector γ .
The obtained exponential family forms for Gaussian, multivariate Gaussian, and multi-
nomial distributions are often used in the Bayesian treatment of statistical models in
speech and language processing.
2.1.4
Conjugate distributions
The previous section introduces the exponential family distributions and provides some
examples of these distributions. Based on the exponential family distributions, this sec-
tion explains how to obtain the posterior distributions when we use the exponential
family distributions as the likelihood functions. For such a distribution, we can ﬁnd a
nice property to obtain the posterior distribution analytically if we set a particular type
of distribution.
Let p(X|θ) be a likelihood function for a set of observation vectors X = {x1, · · · , xN}.
We ﬁrst start the discussion from the simple case that the parameters are represented
as a vector form, i.e., θ. An exponential family distribution of p(X|θ) is deﬁned in
Eq. (2.20) as
p(X|θ) = h(X) exp

γ ⊺t(X) −g(γ )

.
(2.51)
Here use γ (θ) →γ for simplicity. Then, we use the following Bayes theorem for θ
based on Eq. (2.18) to calculate the posterior distribution p(θ|X):
p(θ|X) ∝p(X|θ)p(θ),
(2.52)

2.1 Bayesian probabilities
25
where we disregard the normalization factor p(X). For this calculation, we need to pre-
pare a prior distribution p(θ). Instead of considering the prior distribution of p(θ), we
consider the prior distribution of p(γ ). We set a prior distribution for p(γ ), which is
parameterized with additional variables ν and φ, where the parameters of prior and pos-
terior distributions are called hyperparameters. The hyperparameter appearing in this
book is often used as the parameter of prior or posterior distributions. Then, the prior
distribution is proportional to the following function form:
p(θ) →p(γ |ν, φ) ∝exp

γ ⊺ν −φg(γ )

.
(2.53)
Here, g(γ ) is introduced in Eqs. (2.20) and (2.51) as a logarithmic normalization factor
of the likelihood function. This form of prior distribution is called conjugate prior
distribution.
We can calculate the posterior distribution of p(θ|X) via γ by substituting Eqs. (2.51)
and (2.53) into Eq. (2.52):
p(θ|X) →p(X|θ)p(γ |ν, φ)
= h(X) exp

γ ⊺t(X) −g(γ )

exp

γ ⊺ν −φg(γ )

∝exp

γ ⊺(ν + t(X)) −(φ + 1)g(γ )

= p(γ |ν + t(X), φ + 1),
(2.54)
where we use the deﬁnition used in the conjugate prior distribution (Eq. (2.53)). This
solution means that the conjugate posterior distribution is analytically obtained with the
same distribution function as the conjugate prior distribution by just using the simple
rule of changing hyperparameters from (ν, φ) to (ν + t(X), φ + 1).
Note that the setting of φ is not unique. We consider the case that g(γ ) is decomposed
into M functions, i.e.,
g(γ ) ≜
M

m=1
gm(γ ).
(2.55)
Then, similarly to Eq. (2.53), we can provide M hyperparameters for a prior distribution
as follows:
p(θ) →p(γ |ν, φ) ∝exp

γ ⊺ν −
M

m=1
φmgm(γ )

.
(2.56)
The corresponding posterior distribution is similarly derived by substituting Eqs. (2.51),
(2.55), and (2.56) into Eq. (2.52) as:
p(θ|X) →p(X|θ)p(γ |ν, {φm}M
m=1)
= h(X) exp

γ ⊺t(X) −
M

m=1
gm(γ )

exp

γ ⊺ν −
M

m=1
φmgm(γ )

∝exp

γ ⊺(ν + t(X)) −
M

m=1
(φm + 1)gm(γ )

= p(γ |ν + t(X), {φm + 1}M
m=1).
(2.57)

26
Bayesian approach
Thus, we can derive the posterior distribution with M hyperparameters. The setting of
{φm} is an additional ﬂexibility of the prior distribution. If we use many {φm}, we could
precisely represent a prior distribution. However, by using a few {φm}, we can easily
control the shape of a prior distribution with a few free parameters.
If the transformed model parameters are composed of a vector γ 1 and matrix 2,
as discussed in Eq. (2.23), we also have similar result. A likelihood function of this
exponential family distribution is represented by the following general form:
p(X|) ≜h(X) exp

γ ⊺
1 t1(X) + tr[⊺
2 T2(x)] −
M

m=1
gm(γ 1, 2)

.
(2.58)
Here, similarly to Eq. (2.55), we use the following equation for the g(·) function:
g(γ 1, 2) ≜
M

m=1
gm(γ 1, 2).
(2.59)
Therefore, by providing the following prior distribution form as a conjugate prior with
hyperparameters ν1, N2 and {φm}M
m=1:
p(γ 1, 2|ν1, N2, {φ}M
m=1) ∝exp

γ ⊺
1 ν1 + tr[⊺
2 N2] −
M

m=1
φmgm(γ 1, 2)

.
(2.60)
We can calculate the posterior distribution by substituting Eqs. (2.60), (2.58), and (2.59)
into Eq. (2.52):
p(|X)
→p(X|)p(γ 1, 2|N, {φm}M
m=1)
∝exp

γ ⊺(ν1 + t1(X)) + tr[⊺
2 (N2 + T2(X))] −
M

m=1
(φm + 1)gm(γ 1, 2)

= p(γ 1, 2|ν1 + t1(X), N2 + T2(X), {φm + 1}M
m=1).
(2.61)
Here we use the distributive property of the trace in Appendix B that:
tr[AB] + tr[AC] = tr[A(B + C)].
(2.62)
Now, we summarize the conjugate prior and posterior distributions. The exponential
family distributions with the vector form parameters θ have the following relationship:

Prior: p(γ |ν, {φm}M
m=1)
Posterior: p(γ |ν + t(X), {φm + 1}M
m=1).
(2.63)
When the distribution has vector and matrix parameters, we have the following
relationship:

Prior: p(γ 1, 2|ν1, N2, {φm}M
m=1)
Posterior: p(γ 1, 2|ν1 + t1(X), N2 + T2(X), {φm + 1}M
m=1).
(2.64)

2.1 Bayesian probabilities
27
Therefore, the posterior distribution of the natural parameters (γ , γ 1, 2) is analytically
obtained by using Eqs. (2.63) and (2.64) as a rule. The posterior distribution of the
original parameters p(|X) is obtained by transforming the posterior distribution of the
natural parameters.
The rest of this section provides examples of the conjugate prior and posterior
distributions for some exponential family distributions.
Example 2.5
Conjugate distributions for Gaussian (unknown mean):
We ﬁrst describe the case that we only consider a Gaussian mean parameter μ, and the
precision parameter r = −1 is regarded as a constant value. Based on the discussion in
Example 2.1, the canonical form of the Gaussian distribution is represented as follows:
N

n=1
N(xn|μ; r−1) = h(X) exp (γ t(X) −g(γ )) ,
(2.65)
where
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t(X) =
N

n=1
xn
h(X) =
2π
r
−N
2
exp

−r N
n=1 x2
n
2

γ = μr
g(γ ) = Nγ 2
2r .
(2.66)
Therefore, by substituting γ and g(γ ) in Eq. (2.66) into the general form of the conjugate
distribution in Eq. (2.53), we can derive the function of mean μ as follows:
p(γ |ν, φ) ∝exp (γ ν −φg(γ )) = exp

μrν −φ Nμ2r
2

∝exp

−Nφr
2

μ −ν
Nφ
2
.
(2.67)
Thus, the prior distribution of μ is represented by a Gaussian distribution with
ν
Nφ and
Nφr as the mean and precision parameters, respectively:
p(μ) ∝N

μ
----
ν
Nφ , (Nφr)−1

.
(2.68)
Based on the conjugate distribution rule (Eq. (2.63)), the posterior distribution is easily
solved by just replacing ν →ν + t(X) and φ →φ + 1 in Eq. (2.68) without complex
calculations:

28
Bayesian approach
p (γ |ν + t(X), φ + 1) ∝exp
⎛
⎝−N(φ + 1)r
2

μ −ν + N
n=1 xn
N(φ + 1)
2⎞
⎠
→N

μ
-----
ν + N
n=1 xn
N(φ + 1)
, (N(φ + 1)r)−1

.
(2.69)
Therefore, similarly to the prior distribution, the posterior distribution of μ is repre-
sented by a Gaussian distribution with ν+N
n=1 xn
N(φ+1)
and (N(φ + 1)r)−1 as the mean and
variance parameters, respectively:
p(μ|X) ∝N

μ
-----
ν + N
n=1 xn
N(φ + 1)
, (N(φ + 1)r)−1

.
(2.70)
Thus, both prior and posterior distributions are represented in the same form as a
Gaussian distribution with different parameters.
Now we consider the meaning of the solution of Eqs. (2.68) and (2.70). We
parameterize the φ and ν by newly introducing the following parameters:
φ ≜φμ
N
ν ≜φμμ0.
(2.71)
Then, the prior and posterior distributions of μ in Eqs. (2.68) and (2.70) are rewritten
as:
⎧
⎪⎨
⎪⎩
p(μ) = N
(
μ
---μ0, (φ0r)−1 )
p(μ|X) = N
(
μ
--- ˆμ, ( ˆφμr)−1 )
.
(2.72)
where
ˆφμ ≜φμ + N
ˆμ ≜φμμ0 + N
n=1 xn
φμ + N
.
(2.73)
These are famous Bayesian solutions of the posterior distribution of the Gaussian mean.
We can consider the two extreme cases that the amount of data is zero or very large.
Then, the posterior distribution is represented as:
• N →0
lim
N→0 p(μ|X) = N
(
μ
---μ0, (φμr)−1 )
= p(μ).
(2.74)
This solution means that we only use the prior information when we don’t have data.
• N ≫1
lim
N→∞p(μ|X) ≈lim
N→∞N

μ
-----
N
n=1 xn
N
, 1
Nr

→δ(μ −μML),
(2.75)

2.1 Bayesian probabilities
29
where μML is the ML estimate of μ, and the posterior distribution is close to the ML
value with small standard deviation, which is similar to the delta function that has a
peak value at the ML estimate.
Thus, the solution of Eq. (2.72) approaches the delta function with the ML estimate
when the amount of data is very large and approaches the prior distribution when the
amount of data is very small. The mean parameter of the posterior distribution,
φμμ0 + N
n=1 xn
φμ + N
,
(2.76)
is interpolated by the prior mean parameter μ0 and the ML estimate, and φμ can control
an interpolation ratio.
Example 2.6
Conjugate distributions for Gaussian (unknown mean and precision):
Similarly to Example 2.5, we ﬁrst rewrite a Gaussian distribution. In this situation, the
set of the parameters is θ = {μ, r}. From Eq. (2.29), the Gaussian distribution with
precision r has the following exponential form:
N

n=1
N(xn|μ, r−1) = h(X) exp

γ t(X)⊺−g(γ )

= h(X) exp

γ t(X)⊺−φ1g1(γ ) −φ2g2(γ )

,
(2.77)
where we introduce φ1 and φ2 that are discussed in Eq. (2.55). The variables in the above
equations are represented as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t(X) =
&N
n=1 xn
−
N
n=1 x2n
2
'
h(X) = 1
γ =

μr
r

g1(γ ) = N
2

γ 2
1
γ2

g2(γ ) = N
2

log 2π
γ2

.
(2.78)
Therefore, by substituting γ , g1(γ ), and g2(γ ) in Eq. (2.78) into the general form of the
conjugate distribution in Eq. (2.56), we can derive the function of mean μ and precision
r as follows:
p(γ |ν, φ1, φ2) ∝exp

γ ⊺ν −φ1g1(γ ) −φ2g2(γ )

∝exp

[μr, r]

ν1
ν2

−Nφ1
2 rμ2 −Nφ2
2
log
2π
r

∝r
Nφ2
2 exp

ν1rμ −Nφ1
rμ2
2
+ rν2

,
(2.79)

30
Bayesian approach
where we omit the factor that does not depend on r and μ. By making a complete square
form of μ, we can obtain a Gaussian distribution of μ with mean
ν1
Nφ1 and precision
Nφ1r:
p(γ |ν, φ1, φ2)
∝r
Nφ2
2 exp

−Nφ1r
2

μ −ν1
Nφ1
2
+ rν2
1
2Nφ1
+ rν2

= r
Nφ2
2
 2π
Nφ1r
 1
2  2π
Nφ1r
−1
2
exp

−Nφ1r
2

μ −ν1
Nφ1
2
+ rν2
1
2Nφ1
+ rν2

∝N

μ
----
ν1
Nφ1
, (Nφ1r)−1

r
Nφ2
2 r−1
2 exp

r ν2
1
2Nφ1
+ rν2




≜(∗1)
.
(2.80)
Now we consider the rest of the exponential factor (∗1). By focusing on r and using the
deﬁnition of a gamma distribution (Appendix C.11), the factor is rewritten as follows:
(∗1) ∝r
Nφ2+1
2
−1 exp

−

−ν2
1
2Nφ1
−ν2

r

∝Gam

r
-----
Nφ2 + 1
2
, −ν2
1
2Nφ1
−ν2

,
(2.81)
where the deﬁnition of a gamma distribution is as follows:
Gam(r|α, β) ≜
1
(α)βαrα−1 exp (−βr) ,
(2.82)
where (·) is a Gamma function (Appendix A.4). Thus, precision r = 1
 is represented
by a gamma distribution with Nφ2+1
2
and −ν2
1
2Nφ1 −ν2 as parameters.
This representation can be simpliﬁed by using the following deﬁnition for the other
deﬁnition of the gamma distribution Gam2(y|φ, r0) described in Eq. (C.81) instead of
the original gamma distribution deﬁned in Eq. (C.74):
Gam2(y|φ, r0) ≜Gam

y
----
φ
2 , r0
2

∝y
φ
2 −1 exp

−r0y
2

.
(2.83)
Equation (2.81) is rewritten as
(∗1) ∝Gam2

r
-----Nφ2 + 1, −ν2
1
Nφ1
−2ν2

.
(2.84)

2.1 Bayesian probabilities
31
Thus, the conjugate prior distribution is represented as the product form of the
following Gaussian and gamma distributions:
p(μ, r) = N

μ
----
ν1
Nφ1
, (Nφ1r)−1

Gam2

r
-----Nφ2 + 1, −ν2
1
Nφ1
−2ν2

.
(2.85)
This can be also represented as a Gaussian-gamma distribution (or so-called normal-
gamma) deﬁned in Appendix C.13, as follows:
p(μ, r) = NGam

μ, r
-----
ν1
Nφ1
, (Nφ1r)−1, −ν2
1
Nφ1
−2ν2, Nφ2 + 1

.
(2.86)
The Gaussian-gamma distribution is a conjugate prior distribution of the joint variable
μ and r.
Similarly to the previous example, we introduce the following new parameters:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
φμ ≜Nφ1
μ0 ≜
ν1
Nφ1
φr ≜Nφ2 + 1
r0 ≜−ν2
1
Nφ1
−2ν2.
(2.87)
By using Eq. (2.87), the conjugate prior distribution of Eq. (2.85) is rewritten by using
these new parameters as follows:
p(μ, r) = N
(
μ
---μ0, (φμr)−1 )
Gam2
(
r
---φr, r0 )
.
(2.88)
Note that we can also use Gaussian-gamma distribution as:
p(μ, r) = N(μ|μ0, (rφμ)−1) Gam2
(
r
---φr, r0 )
= NGam(μ, r|μ0, φμ, r0, φr).
(2.89)
Thus, we can derive the prior distribution of joint variable μ and r as the product of
the Gaussian and gamma distributions in Eq. (2.88), or the single Gaussian-gamma
distribution in Eq. (2.89).
Now, we focus on the posterior distribution of μ and r. Based on the conjugate
distribution theory, the posterior distribution is represented as the same form of the
Gaussian-gamma distribution as the prior distribution (2.89) with hyperparameters
ˆφμ, ˆμ, ˆφr, and ˆr as follows:
p(μ, r|X) = NGam(μ, r| ˆμ0, ˆφμ, ˆr0, ˆφr).
(2.90)
Based on the conjugate distribution rule (Eq. (2.63)), the hyperparameters of the poste-
rior distribution are easily solved by just replacing ν →ν + t(X) and φm →φm + 1 in
Eq. (2.87) without complex calculations, as follows:

32
Bayesian approach
ˆφμ = N(φ1 + 1) = φμ + N
ˆμ = ν1 + N
n=1 xn
N(φ1 + 1)
= φμμ0 + N
n=1 xn
φμ + N
ˆφr = N(φ2 + 1 + 1) + 1 = N(φ2 + 1) + 1 + N
= φr + N
ˆr = −
(
ν1 + N
n=1 xn
)2
N(φ1 + 1)
−

2ν2 −
N

n=1
x2
n

= −
(
φμμ0 + N
n=1 xn
)2
φμ + N
−

−ν2
1
Nφ1
−r0 −
N

n=1
x2
n

= −
(
φμμ0 + N
n=1 xn
)2
φμ + N
+ φμ(μ0)2 + r0 +
N

n=1
x2
n.
(2.91)
Thus, we summarize the result of the hyperparameters of the conjugate posterior
distribution as
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
ˆφμ = φμ + N
ˆμ = φμμ0 + N
n=1 xn
φμ + N
ˆφr = φr + N
ˆr = −ˆφμ( ˆμ)2 + φμ(μ0)2 + r0 +
N

n=1
x2
n.
(2.92)
Note that in this representation, the posterior distribution parameters of ˆφμ and ˆφr
are obtained by simply adding the number of observations N to the prior distribution
parameters of φμ and φr, respectively.
Finally, we summarize the result. The prior and posterior distributions of μ and r in
Eqs. (2.89) and (2.90) are also summarized as:

p(μ, r) = NGam(μ, r|μ0, φμ, r0, φr)
p(μ, r|X) = NGam(μ, r| ˆμ, ˆφμ, ˆr, ˆφr),
(2.93)
or

p(μ, r) = p(μ|r)p(r) = N(μ|μ0, (φμr)−1) Gam2(r|φr, r0)
p(μ, r|X) = p(μ|r, X)p(r|X) = N(μ| ˆμ ( ˆφμr)−1) Gam2(r| ˆφr, ˆr).
(2.94)
Similarly to the discussion about the mean parameter μ in Example 2.5, we can consider
the two extreme cases, that the amount of data is zero or very large, for the behavior of
the precision parameter solution r. The posterior distribution of r is represented as:
• N →0
lim
N→0 p(r|X) = Gam2(r|φr, r0) = p(r).
(2.95)

2.1 Bayesian probabilities
33
This solution means that we only use the prior information when we don’t have data.
• N ≫1
lim
N≫1 p(r|X) ≈Gam2

r
-----N,
N

n=1
x2
n −(N
n=1 xn)2
N

.
(2.96)
Since the mean of the gamma distribution rMean (with
1
2 factor) is deﬁned in
Eq. (C.83), the mean of r in this limit is represented as:
rMean =
N
N
n=1 x2n −(N
n=1 xn)2
N
=
⎛
⎝
N
n=1 x2
n
N
−
N
n=1 xn
N
2⎞
⎠
−1
.
(2.97)
This is equivalent to the maximum likelihood estimation of rML represented as
follows:
rML =
(
Mean[x2] −(Mean[x])2)−1
= rMean.
(2.98)
Thus, the mean of the posterior distribution approaches the ML estimate of r when
the amount of data is large.
Similarly, based on the deﬁnition of the variance of the gamma distribution (with
1
2 factor) in Eq. (C.84), the variance is also represented as
rVariance =
2N
N
n=1 x2n −(N
n=1 xn)2
N
2
=
2
N
 N
n=1 x2n
N
−
 N
n=1 xn
N
22
= 2

rML2
N
≈0.
(2.99)
Note that the order of rML in Eq. (2.98) is a constant for N, and the variance of the
precision parameter rVariance approaches 0, i.e., the posterior distribution of r has a
strong peak at rML with a very small variance. Therefore, the posterior distribution of
precision parameter p(r|X) in the case of a large amount of data can be approximated
as the following Dirac delta function with the ML estimate:
lim
N≫1 p(r|X) ≈δ(r −rML).
(2.100)
This conclusion is similar to the case of the large amount limitation of the posterior
distribution of mean parameter p(μ|X) in Eq. (2.75).
This Gaussian-gamma distribution is used to model the prior and posterior distributions
of Gaussian parameters (μ and r) for scalar continuous observations, or can be used for
vector continuous observations when we use a diagonal covariance matrix.

34
Bayesian approach
Example 2.7
Conjugate distributions for multivariate Gaussian (unknown mean vector
and precision matrix):
Based on the discussion of Eq. (2.39) in Example 2.3, the canonical form of the
multivariate Gaussian distribution is represented as follows:
N

n=1
N(xn|μ, R−1)
∝exp

μ⊺R
N

n=1
xn −1
2tr
&
R
N

n=1
xnx⊺
n
'
−N
2
(
log(2π|R|−1) + μ⊺Rμ
)
,
(2.101)
where
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t1(X) =
N

n=1
xn
T2(X) = −1
2
N

n=1
xnx⊺
n
h(x) = 1
γ 1() = Rμ
2() = R
g1(γ 1, 2) = N
2 γ ⊺
1 −1
2 γ 1
g2(γ 1, 2) = N
2 log(2π|2|−1).
(2.102)
Therefore, by substituting γ , g1(γ 1, 2), and g2(γ 1, 2) in Eq. (2.102) into the general
form of the conjugate distribution in Eq. (2.60), we can derive the function of mean μ
and r as follows:
p(γ 1, 2|ν1, N2, φ1, φ2)
∝exp

γ ⊺
1 ν1 + tr[⊺
2 N2] −φ1g1(γ 1, 2) −φ2g2(γ 1, 2)

∝exp

μ⊺Rν1 + tr[R⊺N2] −Nφ1
2 γ ⊺
1 −1
2 γ 1 −Nφ2
2
log(2π|2|−1)

∝exp

μ⊺Rν1 + tr[RN2] −Nφ1
2 μ⊺Rμ −Nφ2
2
log(|R|−1)

,
(2.103)
where we omit the factor that does not depend on R and μ. Similarly to Example 2.6, we
ﬁrst use a complete square form of μ to derive a Gaussian distribution from Eq. (2.103).
In Appendix B.4, we have the following formula for the complete square form of
vectors:
x⊺Ax −2x⊺b + c = (x −u)⊺A (x −u) + v,
(2.104)
where
u ≜A−1b
v ≜c −b⊺A−1b.
(2.105)

2.1 Bayesian probabilities
35
Therefore, by x →μ, A →Nφ1R, and b →Rν1 in Eqs. (2.104) and (2.105),
Eq. (2.103) is rewritten as follows:
p(γ 1, 2|ν1, N2, φ1, φ2)
∝|R|
Nφ2
2 exp
⎛
⎜⎜⎜⎝−Nφ1
2

μ −ν1
Nφ1
⊺
R

μ −ν1
Nφ1

+ ν⊺
1 Rν1
2Nφ1
+ tr[RN2]



(∗)
⎞
⎟⎟⎟⎠.
(2.106)
Now we focus on the (∗) term in Eq. (2.106). By using the matrix formula in Appendix
B, (∗) is rewritten as
(∗) = tr

ν1ν⊺
1 R
2Nφ1
+ N2R

= tr

 ν1ν⊺
1
2Nφ1
+ N2

R

.
(2.107)
Thus, the conjugate prior distribution is rewritten as:
p(γ 1, 2|ν1, N2, φ1, φ2)
∝|R|
Nφ2
2 exp

−Nφ1
2

μ −ν1
Nφ1
⊺
R

μ −ν1
Nφ1

+ tr

 ν1ν⊺
1
2Nφ1
+ N2

R

.
(2.108)
Therefore, Eq. (2.108) is represented as the following Gaussian–Wishart distribution in
Appendix C.15:
NW(μ, R|μ0, φμ, R0, φR)
≜CNW(φμ, R0, φR)|R|
φR−D
2
× exp

−1
2tr
+
R0R
,
−φμ
2 (μ −μ0)⊺R(μ −μ0)

,
(2.109)
where
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
φμ = Nφ1
μ0 = ν1
Nφ1
φR = Nφ2 + D
R0 = −ν1ν⊺
1
Nφ1
−2N2.
(2.110)
Thus, we can derive the prior distribution as the Gaussian–Wishart distribution.
Now, we focus on the posterior distribution of μ and R. Similarly, the posterior distri-
bution is represented as the same form of the Gaussian–Wishart distribution as the prior
distribution (2.109), with hyperparameters ˆφμ, ˆμ, ˆφR, and ˆR as follows:
p(μ, R|X) = NW(μ, R| ˆμ, ˆφμ, ˆR, ˆφR).
(2.111)

36
Bayesian approach
Based on the conjugate distribution rule, Eq. (2.64), the hyperparameters of the poste-
rior distribution are easily solved by just replacing ν1 →ν1 + t(X), N2 →N2 + T(X),
and φm →φm + 1 in Eq. (2.110) without complex calculations, as follows:
ˆφμ = N(φ1 + 1) = φμ + N,
ˆμ = ν1 + N
n=1 xn
N(φ1 + 1)
= φμμ0 + N
n=1 xn
φμ + N
,
ˆφR = N(φ2 + 1) + D = φR + N,
ˆR = −
(
ν1 + N
n=1 xn
) (
ν1 + N
n=1 xn
)⊺
N(φ1 + 1)
−2

N2 −1
2
N

n=1
xnx⊺
n

= −ˆφμ ˆμ ˆμ⊺+ ν1ν⊺
1
Nφ1
+ R0 +
N

n=1
xnx⊺
n
= −ˆφμ ˆμ ˆμ⊺+ φμμμ⊺+ R0 +
N

n=1
xnx⊺
n .
(2.112)
Thus, we derive the posterior distribution that is also represented as a Gaussian–Wishart
distribution. Finally, the prior and posterior distributions of μ and R in Eqs. (2.109) and
(2.111) are also summarized as:

p(μ, R) = NW(μ, R|μ0, φμ, R0, φR)
p(μ, R|X) = NW(μ, R| ˆμ, ˆφμ, ˆR, ˆφR),
(2.113)
or

p(μ, R) = p(μ|R)p(R) = N(μ|μ0, (φμR)−1)W(R|φR, R0)
p(μ, R|X) = p(μ|R, X)p(R|X) = N(μ| ˆμ, ( ˆφμR)−1)W(R| ˆφR, ˆR).
(2.114)
Example 2.8
Conjugate distributions for multinomial distribution:
Based on the discussion of Eq.
(2.50) in Example 2.4, the canonical form of the
multivariate Gaussian distribution is represented as follows:
Mult(x1, · · · , xJ|ω1, · · · , ωJ) = h(x) exp

γ ⊺t(x)

,
(2.115)
where
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
t(x) = x
h(x) =
N!
*J
j=1 xj!
γ =
+
log
ω1
1−J−1
j=1 ωj , · · · , log
ωJ−1
1−J−1
j=1 ωj
,⊺
g(γ ) = N log
⎛
⎝1 +
J−1

j=1
exp(γj)
⎞
⎠.
(2.116)

2.1 Bayesian probabilities
37
Note that we have the following constraints:
J

j=1
xj = N
J

j=1
ωj = 1.
(2.117)
Therefore, by substituting γ , g1(γ ), and g2(γ ) in Eq. (2.78) into the general form of the
conjugate distribution in Eq. (2.56), we can derive the function of γ as follows:
p(γ |ν, φ) ∝exp

γ ⊺ν −φg(γ )

= exp
⎛
⎝
+
log
ω1
1−J−1
j=1 ω1 , · · · , log
ωJ−1
1−J−1
j=1 ωj
,
ν + Nφ log
⎛
⎝1 −
J−1

j=1
ωj
⎞
⎠
⎞
⎠
= exp
(
log ω1, · · · , log ωJ
 +
ν⊺, Nφ −J−1
j=1 νj
,⊺)
=
J
j=1
(ωj)φω
j −1,
(2.118)
where hyperparameters {φω
j }J
j=1 are deﬁned as follows:
φω
j ≜νj + 1 for j = 1, · · · , J −1
φω
J ≜Nφ −
J−1

j=1
νj + 1.
(2.119)
Thus, the conjugate prior distribution is represented as a Dirichlet distribution deﬁned
in Appendix C.4 as
Dir({ωj}J
j=1|{φω
j }J
j=1) ≜
(J
j=1 φω
j )
*J
j=1 (φω
j )
J
j=1
(ωj)φω
j −1.
(2.120)
Based on the conjugate distribution rule, Eq. (2.63), the hyperparameters of the pos-
terior distribution are easily solved by just replacing ν →ν + t(x) and φ →φ + 1 in
Eq. (2.118) without complex calculations, as follows:
p(γ |X) →Dir({ωj}J
j=1|{ ˆφω
j }J
j=1),
(2.121)
where hyperparameters { ˆφω
j }J
j=1 are obtained as follows:
ˆφω
j ≜νj + xj + 1 = φω
j + xj
ˆφω
J ≜N(φ + 1) −
J−1

j=1
(νj + xj) + 1
= Nφ + xJ −
J−1

j=1
νj + 1
= φω
J + xJ.
(2.122)

38
Bayesian approach
Thus, we derive the posterior distribution that is represented as a Dirichlet distribution.
Finally, the prior and posterior distributions of ω are given as:
⎧
⎪⎨
⎪⎩
p({ωj}J
j=1) = Dir
(
{ωj}J
j=1
---{φω
j }J
j=1
)
p({ωj}J
j=1|X) = Dir
(
{ωj}J
j=1
---{ ˆφω
j }J
j=1
)
.
(2.123)
Table 2.1 shows a recipe of the kind of distributions we use as a conjugate prior.
This section provides a solution of the posterior distribution for rather simple statisti-
cal models. However, in practical applications, we still face the problems of solving the
equations, and often require the approximation to solve them efﬁciently. The next sec-
tion explains a powerful approximation method, conditional independence, in Bayesian
probabilities.
2.1.5
Conditional independence
Another important mathematical operation of the Bayesian approach, as well as the
product and sum rules (Section 2.1.1), is called conditional independence. Let a, b, and
c be probabilistic variables, the conditional independence of a and b on c is represented
as follows:
p(a, b|c) = p(a|b, c)p(b|c) = p(a|c)p(b|a, c),
(2.124)
≈p(a|c)p(b|c).
(2.125)
This is a useful assumption for the Bayesian approach when factorizing the joint proba-
bility distribution. For example, Eq. (2.124) based on the product rule needs to consider
p(b|a, c) or p(a|b, c). Suppose a, b, and c are discrete elements of sets, i.e., a ∈A,
b ∈B, and c ∈C, p(b|a, c) or p(a|b, c) considers the probability of all combinations of
a, b, and c, which correspond to |A|×|B|×|C|. The number of combinations is increased
exponentially, if the number of valuables is increased. Therefore, it is computationally
very expensive to obtain the conditional distribution, and almost impossible to consider
Table 2.1 Conjugate priors.
Likelihood function
Unknown variable
Conjugate prior
Gaussian
μ ∈R
Gaussian C.5
Gaussian
r ∈R>0
Gamma C.11
Gaussian
μ, r
Gaussian–gamma C.13
Multivariate Gaussian
μ ∈RD
Multivariate Gaussian C.6
Multivariate Gaussian
R ∈RD×D
Wishart C.14
Multivariate Gaussian
μ, R
Gaussian–Wishart C.15
Multinomial
ωi ∈[0, 1], 
i ωi = 1
Dirichlet C.4

2.1 Bayesian probabilities
39
large amounts of data as probabilistic valuables. Thus, the conditional independence
approximation in Eq. (2.125) greatly reduces the computational complexity, and makes
the Bayesian treatment of speech and language processing tractable.
By using the product rule, the conditional independence equation is rewritten as
follows:
p(a|c) ≈p(a, b|c)
p(b|c)
= p(a|b, c)p(b|c)
p(b|c)
= p(a|b, c).
(2.126)
Thus,
p(a|b, c) ≈p(a|c)
(2.127)
is also equivalently used as the conditional independence assumption of Eq. (2.125).
The conditional independence is often used in the following sections to make the
complicated relationship between probabilistic variables simple. For example, speech
recognition has many probabilistic variables which come from acoustic and language
models. It is very natural and effective to assume conditional independence between
acoustic model and language model variables because these do not depend on each
other explicitly.
Example 2.9
Naive Bayes classiﬁer:
One of the simplest classiﬁers in the machine learning approach is the naive Bayes
classiﬁer. The approach is used for many applications including document classiﬁcation
(Lewis 1998, McCallum & Nigam 1998). For example, if we have N data (x1, x2, · · · xN),
and want to classify the data to a speciﬁc category ˆc, this can be performed by using the
posterior distribution of category c as follows:
ˆc = arg max
c
p(c|{xn}N
n=1).
(2.128)
The naive Bayes classiﬁer approximates this posterior distribution with the product rule
and conditional independence assumption as follows:
p(c|{xn}N
n=1) ∝p({xn}N
n=1|c)p(c)
≈
N

n−1
p(xn|c)p(c).
(2.129)
This approach approximates the posterior distribution p(c|{xn}N
n=1) with the product of
likelihood p(xn|c) for all samples and prior distribution p(c). Since the naive Bayes clas-
siﬁer is very simple and easy to implement, it is often used as an initial attempt of the
machine learning approach if we have training data with labels to obtain p(xn|c) for all c.
For example, in document classiﬁcation, a multinomial distribution is used to represent
the likelihood function p(xn|c).

40
Bayesian approach
2.2
Graphical model representation
The previous sections (especially Sections 2.1.1 and 2.1.5) discuss how to provide
the mathematical relationship between probabilistic variables in a Bayesian manner.
This section brieﬂy introduces a graphical model representation that visualizes the
relationship between these probabilistic valuables to provide a more intuitive way of
understanding the model. A graphical model framework is also widely used in Bayesian
machine learning studies, and this book introduces basic graphical model descriptions,
which are used in the following sections.
2.2.1
Directed graph
First, we simply consider the following joint distribution of a and b, which can be
rewritten as the following two factorization forms based on the product rule:
p(a, b) = p(b|a)p(a),
(2.130)
= p(a|b)p(b).
(2.131)
Therefore, to obtain the joint distribution, we compute either Eq. (2.130) or (2.131)
depending on the problem. The graphical model can separately represent these factor-
ization forms intuitively. Figure 2.1 represents the graphical models of p(b|a)p(a) and
p(a|b)p(b), respectively. The node represents a probabilistic variable, and the directed
link represents the conditional dependency of two probabilistic variables. For example,
Eq. (2.130) is composed of the conditional distribution p(b|a) and then the corre-
sponding graphical representation provides the directed link from node a to node
b in Figure 2.1(a). Conversely, the conditional distribution p(a|b) in Eq. (2.131) is
represented by the directed link from node b to node a in Figure 2.1(b).
Thus, the graphical model speciﬁes a unique factorization form of a joint distribution,
intuitively. The graph composed of the directed link, which represents the conditional
distribution, is called a directed graph. The graphical model can also deal with an undi-
rected graph, which is a graphical representation of a Markov random ﬁeld, but this
book focuses on the directed graph representation, which is often used in the later
applications.
2.2.2
Conditional independence in graphical model
As we discussed in Section 2.1.5, practical applications often need some approximations
in the dependency of probabilistic variables to avoid a complicated dependency of the
Figure 2.1
Graphical models of p(b|a)p(a) and p(a|b)p(b).

2.2 Graphical model representation
41
Figure 2.2
Graphical models of p(a|b, c)p(b|c)p(c) and p(a|c)p(b|c)p(c).
factorized distribution. We can represent this approximation in the graphical model rep-
resentation. If we consider the joint distribution of a, b, and c, the joint distribution is,
for example, represented as the following factorization form based on the product rule:
p(a, b, c) = p(a|b, c)p(b|c)p(c).
(2.132)
The graphical model of Eq. (2.132) is represented in Figure 2.2(a). Note that all nodes
are connected to each other by directed links. This graph is called a full connected graph.
On the other hand, the joint distribution with the following conditional independence
can also be represented as a graphical model in Figure 2.2(b):
p(a, b, c) = p(a, b|c)p(c) ≈p(a|c)p(b|c)p(c).
(2.133)
Note that the link between a and b has disappeared from Figure 2.2(b). Thus, the condi-
tional independence in the graphical model is represented by pruning links in the graphs,
which corresponds to reducing the dependencies in probabilistic variables, and leads to
reduced computational cost.
In real applications, we need to consider large numbers of variables. For example, the
naive Bayes classiﬁer introduced in Example 2.9 has to consider N + 1 probabilistic
variables ( {xn}N
n=1 and c):
p(x1, · · · , xN|c)p(c) ≈p(x1|c) · · · p(xN|c)p(c) =
N

n=1
p(xn|c)p(c).
(2.134)
The graphical model of this case can be simpliﬁed from Figure 2.3(a) to 2.3(b) by using
the plate. Based on the plate, we can represent a complicated relationship of probabilis-
tic variables intuitively. In Section 8.2, we also consider the case when the number of
probabilistic variables is dealt with as inﬁnite in Bayesian nonparametrics. Then, the
number of variables can be represented by using ∞in a graphical model, as shown in
Figure 2.4.
Thus, the graphical model can represent the dependencies of variables based on the
product rule and conditional independence graphically. This dependency-network-based
Bayesian method is also called a Bayesian network. In particular the Bayesian treatment
that considers the dynamical relationship between probabilistic variables is also called
a dynamic Bayesian network (Ghahramani 1998, Murphy 2002). A dynamic Bayesian

42
Bayesian approach
…….
…….
Figure 2.3
Graphical model of p(x1|c) · · · p(xN|c) = *N
n=1 p(xn|c).
Figure 2.4
Graphical model of p(x1|c) · · · p(x∞|c) = *∞
n=1 p(xn|c).
network provides efﬁcient solutions to the time-series statistical models similarly to
HMM and Kalman ﬁlters, which are also used in speech recognition (Zweig & Russell
1998, Neﬁan, Liang, Pi et al. 2002, Livescu, Glass & Bilmes 2003). It is helpful to
understand probabilistic models, even when they are very complicated in the equation
form.
2.2.3
Observation, latent variable, non-probabilistic variable
Previous sections deal with the graphical model of all probabilistic variables. However,
our machine learning problems for speech and language processing have three types of
variables: observation, latent variables, and non-probabilistic variables. For example, let
x be an observation, z is a latent variable, and θ is a model parameter, which we don’t
deal with as a probabilistic variable in this section, unlike the full Bayesian approach.
The probability distribution of x is represented as follows:
p(x|θ) =

z
p(x, z|θ) =

z
p(x|z, θ)p(z|θ).
(2.135)
The corresponding graphical model is represented in Figure 2.5(a). Note that three vari-
ables x, z, θ have different roles in this equation. For example, x is a ﬁnal output of this

2.2 Graphical model representation
43
Figure 2.5
Graphical models that have observation x, latent variable z, model parameter θ, and
hyperparameter ψ. Part (a) treats θ as a non-probabilistic variable, and (b) treats θ as a
probabilistic variable to be marginalized.
equation as an observation, which is not marginalized, while z is a latent variable and
should be marginalized. To distinguish the observation and latent variables, the node
representing an observation is tinted. θ is not a probabilistic variable in this explanation,
and so it is put in the graph without a circle.
Similarly, if we consider the same model, but treat θ as a probabilistic variable, θ
is marginalized by the prior distribution of θ with hyperparameter ψ. The probability
distribution of x is represented as follows:
p(x|ψ) =
 
z
p(x, z, θ|ψ)dθ
=
 
z
p(x|z, θ)p(z|θ)p(θ|ψ)dθ.
(2.136)
Here we assume θ to be a continuous variable, and use the integral instead of the sum-
mation. We can regard θ as a latent variable in a broad sense, but the other sections
distinguish the model parameters and latent variables. The corresponding graphical
model is represented in Figure 2.5(b). Thus, by using the representations of observa-
tion, latent variables, and non-probabilistic variables, we can provide graphical models
of various distributions other than joint distributions. These are basic rules of providing
a directed graphical model from the corresponding probabilistic equation.
The directed graph basically describes how observation variables are generated con-
ditioned on the other probabilistic variables. This statistical model of describing the
generation of observation variables is called a Generative model. HMM, GMM, Kalman
ﬁlter, n-gram, latent topic model, and deep belief network are typical examples of gen-
erative models that can generate speech feature vectors and word sequences. The next
section also introduces another way of intuitively understanding our complicated statisti-
cal models by describing how observation variables are generated from the distributions
in our models.

44
Bayesian approach
2.2.4
Generative process
This section also explains another representation of the Bayesian approach based on the
generative process. This representation is used to generate the probabilistic variables in
an algorithmic way. The generative process is used to express the joint distribution. The
basic syntax of the generative process is as follows:
• Non-probabilistic variables: placed as “require”;
• Latent variables: “drawn” from their probability distribution;
• Model parameters: “drawn” from their (prior) probability distribution;
• Observations: ﬁnally “drawn” from their probability distribution given sampled latent
variables and model parameters.
If we also want to represent the marginalization of a probabilistic variable, we can use
an additional syntax “Average” for the marginalization.
As an example of Eq. (2.136), Algorithm 1 represents the generative process of the
joint distribution p(x, z, θ|ψ), which is represented as:
p(x, z, θ|ψ) = p(x|z, θ)p(z|θ)p(θ|ψ),
(2.137)
where x, z, θ, and ψ are observations, latent variables, model parameters, and hyperpara-
meter (non-probabilistic variables), respectively.
Algorithm 1 Generative process of p(x, z, θ|ψ) = p(x|z, θ)p(z|θ)p(θ|ψ)
Require: ψ
1: Draw θ from p(θ|ψ)
2: Draw z from p(z|θ)
3: Draw x from p(x|z, θ)
This generative process also helps us to understand models intuitively by under-
standing how probabilistic variables are generated algorithmically. Therefore, both the
generative process and graphical model are often provided in the Bayesian approach
to represent a complicated generative model. In some of the statistical models used in
this book, we provide the generative process and graphical model to allow readers to
understand the models intuitively.
2.2.5
Undirected graph
Another example of a graphical model is called an undirected graph (Figure 2.6), that
represents the relationship of probabilistic variables but does not have explicit parent–
child relationships compared with the directed graph. The network is called a Markov
random ﬁeld, and the probabilistic distribution is usually expressed by a potential
function ψ(a, b, c) (a positive, but otherwise arbitrary, real-valued function):
p(a, b, c) = 1
Z ψ(a, b, c),
(2.138)

2.2 Graphical model representation
45
Figure 2.6
Undirected graph.
Figure 2.7
Factor graph.
where Z is a normalization constant of this distribution, and is called the partition
function for this special case. This approach is often used as a context of a log linear
discriminative model, where ψ(a, b, c) is a linear function of a feature obtained by a, b,
and c and the corresponding weight.
A factor graph is another class of graphical model representing the conditional inde-
pendence relationship between variables. Actually, the factor graph can provide a more
concrete representation of the joint distribution of variables than that of the undirected
graph. The factor graph introduces additional square nodes to a graph, which can
explicitly represent the dependency of several variables.
For example, the partition function can be represented by several cases, as shown in
Figures 2.7 and 2.8. Both graphs are fully connected and can represent the joint distri-
bution of a, b, and c. However, the partition function of Figure 2.7 is computed by using
the three pairs of partition functions as follows:
p(a, b, c) = 1
Z ψ(a, b)ψ(b, c)ψ(c, a).
(2.139)
The possible partition functions are |A|×|B|+|B|×|C|+|A|×|C|. On the other hand,
Figure 2.8 considers the potential function of the joint event for a, b, and c:
p(a, b, c) = 1
Z ψ(a, b, c).
(2.140)

46
Bayesian approach
Figure 2.8
Factor graph.
The possible partition functions are |A|×|B|×|C|. Therefore, if the number of possible
variables (|A|, |B|, and |C|) is very large, Figure 2.7 is a more compact representation
since the number of possible functions would be smaller.
Thus, factor graphs are more speciﬁc about the precise form of the factorization of
undirected graphs, and can be used to mainly represent some discriminative models
(logistic regression, conditional random ﬁeld (Lafferty, McCallum & Pereira 2001)).
This book generally deals with generative models (HMM, GMM, n-gram, and latent
topic model), and does not deal with these discriminative models. However, there are
several important applications of discriminative models to speech and language process-
ing (e.g., Gunawardana, Mahajan, Acero et al. 2005, Fosler & Morris 2008, Zweig &
Nguyen 2009, Gales et al. 2012) in addition to the recent trend of deep neural networks
(Hinton et al. 2012). We present an example of a Bayesian treatment of neural network
acoustic models in Section 6.4. The fully Bayesian treatment of the other discriminative
models in speech and language processing is an interesting future direction.
2.2.6
Inference on graphs
One of the powerful advantages of the graphical model representation is that once we
ﬁx a graphical model, we can infer all variables in the graph efﬁciently by using belief
propagation if the graph does not have a loop.
For example, belief propagation provides a sum product algorithm that can efﬁciently
compute the distribution p(xi) of the probabilistic variable in an arbitrary node by using
message passing. In the HMM case, this sum product algorithm corresponds to the
forward–backward algorithm, as discussed in Section 3.3.1. Similarly, belief propa-
gation provides a max sum algorithm that can efﬁciently compute the arg max value
(ˆxi = arg maxxi p(xi)) in an arbitrary node by using message passing. Similarly to the
sum product algorithm, the max sum algorithm corresponds to the Viterbi algorithm,
as discussed in Section 3.3.2. A detailed discussion about the relationship between the
forward–backward/Viterbi algorithms in the HMM and these algorithms can be found
in Bishop (2006).
However, most of our applications have a loop in a graph, and we cannot use the
exact inference based on the above algorithms. The following chapters introduce the
approximations of the Bayesian inferences, and especially variational Bayes (VB), as
discussed in Chapter 7, and Markov chain Monte Carlo (MCMC), as discussed in

2.3 Difference between ML and Bayes
47
Chapter 8, these being promising approaches to obtain approximate inferences in a
graphical model. Actually, progress of the graphical model approach has been linked
to the progress of these Bayesian inference techniques.
The other approximated approach to inference in a graphical model that contains
cycles or loops is to use the sum-product algorithm for the graph even though there is
no guarantee of convergence. This approach is called loopy belief propagation, and it is
empirically known that it is convergent in some applications.
2.3
Difference between ML and Bayes
As discussed in previous sections, the Bayesian approach deals with all variables intro-
duced for modeling as probabilistic variables. This is the unique difference between the
Bayesian approach and the other standard statistical framework, the Maximum Likeli-
hood (ML) approach. Actually this difference can yield various advantages over ML.
This section overviews the advantage of the Bayesian approach over the ML approach
in general. We discuss this, along with a general pattern recognition problem, as we
consider practical speech and language processing issues in the following chapters.
Let O, Z, , M, and W be a set of observation features, latent variables, model
parameters, model structure (hyperparameter) variables, and classiﬁcation categories,
respectively, details of which will be introduced in the following chapters. For compar-
ison, we summarize the difference between the approaches in terms of model setting,
training, and classiﬁcation.
• Model setting
– ML:
Generative model distribution p(O, Z|, M).
– Bayes:
Generative model distribution p(O, Z|, M)
Prior distributions p(|M) and p(M).
In addition to the generative model distribution, the Bayesian approach needs to set
prior distributions.
• Training
– ML: Point estimation
ˆ.
– Bayes: Distribution estimation
p(|M, O) and p(M|O).
ML point-estimates are given by the optimal values ˆ by using the EM algorithm
generally when the model has latent variables, while the Bayesian approach estimates
posterior distributions. In addition, ML only focuses on model parameters , but the
Bayesian approach focuses on both model parameters  and model M.

48
Bayesian approach
• Classiﬁcation
– ML:
arg max
W′

Z′
p(O′, Z′| ˆ, ˆM, W′)p(W′).
(2.141)
– Bayes:
arg max
W′
 
M,Z′
p(O′, Z′|, M, W′)p(|M, O, W)p(M|O, W)p(W′)d.
(2.142)
Here, ˆ is obtained in the ML training step, and ˆM is usually set in advance by an
expert or optimized by evaluating the performance of model M using a development
set. Equation (2.142) is obtained by the probabilistic sum and product rules and con-
ditional independence, as discussed in the previous sections. Compared with ML,
the Bayes approach marginalizes  and M through the expectations of the posterior
distributions p(|M, O, W) and p(M|O, W), respectively.
Thus, the main differences between ML and Bayes are (i) use of prior distributions,
(ii) use of distributions of model M, (iii) expectation with respect to probabilistic vari-
ables based on posterior distributions. These differences yield several advantages of the
Bayesian approach over ML. The following sections describe the three main advantages.
2.3.1
Use of prior knowledge
First, we describe the most famous Bayesian advantage over ML based on the use of
prior knowledge. Figure 2.9 depicts this advantage focusing on the estimation of model
parameters (the mean and variance of a Gaussian). The dashed line shows the true dis-
tribution, and the solid line shows the estimated Gaussian distributions based on ML
and Bayes. If data to be used to estimate parameters are not sufﬁcient and biased to the
Q
Q
Q
Figure 2.9
Use of prior knowledge.

2.3 Difference between ML and Bayes
49
small peak of the true distribution, ML tends to estimate the wrong parameter by using
the biased data. This is because ML estimates only consider the likelihood function,
which leads it to estimate the parameters that generate the observed data:
ML = arg max
 p(O|, M).
(2.143)
Thus, ML can correctly estimate parameters only when the amount of data is sufﬁcient.
On the other hand, the Bayesian approach also considers the prior distribution of
model parameters p(|M). Now, we consider point estimation using the maximum a-
posteriori (MAP) value, instead of considering the Bayesian distribution estimation for
simply comparing the prior effect with ML. For example, based on Eq. (3.345), the MAP
estimate of  is represented as follows:
MAP = arg max
 p(|O, M)
= arg max
 p(O|, M)p(|M).
(2.144)
The result considers the prior distribution as a regularization term. So if we set a con-
straint on a distribution form by appropriate prior knowledge, we can recover a wrong
estimation due to the sparse data problem in ML, and we can estimate the parameter
correctly. Details are discussed in Chapter 4.
2.3.2
Model selection
The model selection is a unique function of the Bayesian approach, which determines
a model structure from data automatically. For example, Figure 2.10 shows how many
Gaussians we use to estimate the parameters. It is well known that likelihood values
always increase as the number of parameters increases. Therefore, if we enforce use
of the ML criterion for the model selection, ML tends to select too many Gaussians,
which results in over-ﬁtting. There are some extensions of ML to deal with model
selection based on information criteria (e.g., Akaike 1974, Rissanen 1984). However,
in most cases of speech and language processing, the ML framework usually optimizes
model structure by evaluating the performance of the model using a development set.
The development set is usually obtained from a part of training/test data. Although this
Figure 2.10
Model selection.

50
Bayesian approach
optimization is straightforward, it is very difﬁcult to use in some of the applications
when the performance evaluation is difﬁcult (e.g., it has a large computational cost for
evaluation or there is no objective performance measure).
The Bayesian approach can deal with model selection within the framework. For
example, our Bayesian approach to acoustic modeling involves the posterior distribu-
tion of model p(M|O), which will be described in Section 3.8.7. Once we obtain the
posterior disquisition, we can select an appropriate model structure in terms of the MAP
procedure introduced in Section 2.1.2, as follows:
MMAP = arg max
M p(M|O).
(2.145)
Thus, an appropriate model structure (e.g., the topology of HMMs (Stolcke & Omo-
hundro 1993, Watanabe, Minami, Nakamura et al. 2004)) can be selected according to
training data, without splitting them to create development data.
Instead of using the MAP procedure, we can use expectation based on p(M|O). This is
stricter in the Bayesian sense, and Eq. (2.142) actually includes the expectation over the
posterior distribution of models. This approach corresponds to using multiple models
with different model structures to classify unseen categories. However, in terms of the
computational costs (needs large memory and computational time for the multiple model
case), people usually carry out model selection by using the MAP procedure.
We also note that M involves other model variations than model structure as ele-
ments. For example, hyperparameters introduced in the model can be optimized by
using the same MAP procedure or marginalized out by using the expectation. In par-
ticular, the optimization of hyperparameters through the posterior distributions of M is
also a powerful example of the Bayesian advantage over ML.
2.3.3
Marginalization
The ﬁnal Bayesian advantage over ML is the marginalization effect, which was dis-
cussed in the expectation effect of the Bayesian approach in the previous section. Since
the stochastic ﬂuctuation in the expectation absorbs estimation errors, the marginaliza-
tion improves the robustness in estimation, classiﬁcation, and regression over unknown
data. In Figure 2.11, the left ﬁgure shows the maximum likelihood based distribu-
tion with ˆ, while the right ﬁgure shows the example of marginalization over model
Q
Q
Q
Figure 2.11
Marginalization over a set of model parameters .

2.4 Summary
51
parameters  in the likelihood function p(O′|, M) given a model structure M. Since
the right ﬁgure considers the probabilistic ﬂuctuations of  by the posterior distribu-
tion p(|O, M), the marginalized function (expected with respect to ) can mitigate the
error effects in estimating  with the variance, and make the likelihood function robust
to unseen data. The marginalization can be performed to all probabilistic variables in a
model including latent variables Z, model structure M (and hyperparameters), in addi-
tion to the model parameter  example in Figure 2.11, if we obtain the prior/posterior
distributions of these probabilistic variables.
The marginalization is another unique advantage of the Bayesian approach over ML,
whereby incorporating the uncertainty of variables introduced in a model based on prob-
abilistic theory achieves robustness for unseen data. However, it requires an expectation
with respect to variables that essentially needs to consider the integral or summations
over the variables. Again, this is the main difﬁculty of the practical Bayesian approach,
and it needs some approximations especially to utilize the Bayesian advantage of this
marginalization effect.
Although marginalization is not usually performed for observation O, observation
features in speech and language processing often include noises, and marginalization
over observation features is effective for some applications. For example, if we use a
speech enhancement technique as a front-end denoising process of automatic speech
recognition, the process inevitably includes noise estimation errors, and the errors can
be propagated to speech recognition, which degrades the performance greatly. The
approach called uncertainty techniques tries to mitigate the errors by using the follow-
ing Bayesian marginalization of the conventional continuous-density HMM (CDHMM)
likelihood function over observation features O (Droppo, Acero & Deng 2002, Delcroix,
Nakatani & Watanabe 2009, Kolossa & Haeb-Umbach 2011):
p(, uns
O′ , M) ≈

p(O′|, M)p(O′|uns
O′ )dO′.
(2.146)
The main challenges of the uncertainty techniques are how to estimate feature uncer-
tainties uns
O′ (the distribution of observation features p(O|uns
O′ ) with hyperparameter
uns
O′ ) and how to integrate the marginal likelihood function with the decoding algo-
rithm of the HMM. The approaches have been successfully applied to noisy speech
recognition tasks, and show improvements by mitigating the error effects in speech
enhancement techniques (Barker, Vincent, Ma et al. 2013, Vincent, Barker, Watanabe
et al. 2013).
Thus, we have explained the three main practical advantages of the Bayesian
approaches. Note that all of the advantages are based on the posterior distributions,
and obtaining the posterior distributions for our target applications is a main issue of the
Bayesian approaches. Once we obtain the posterior distributions, Bayesian inference
allows us to achieve robust performance for our applications.
2.4
Summary
This chapter introduces the selected Bayesian approaches used for speech and language
processing by starting from the basic Bayesian probabilistic theory with graphical

52
Bayesian approach
models, and concludes with a summarization of the Bayesian advantages over ML. The
discussion is rather general, and to apply Bayesian approaches to our practical prob-
lems in speech and language processing, we still need to bridge a gap between the
theoretical Bayesian approaches and these practical problems. This is the main goal
of this book. The next chapter deals with basic statistical models used in speech and
language processing based on ML, and it will be extended in the latter chapters toward
this main goal.

3
Statistical models in speech and
language processing
This chapter focuses on basic statistical models (Gaussian mixture models (GMM), hid-
den Markov models (HMM), n-gram models and latent topic models), which are widely
used in speech and language processing. These are well-known generative models, and
these probabilistic models can generate speech and language features based on their
likelihood functions. We also provide parameter-learning schemes based on maximum
likelihood (ML) estimation which is derived according to the expectation and maximiza-
tion (EM) algorithm (Dempster et al. 1976). Basically, the following chapters extend
these statistical models from ML schemes to Bayesian schemes. These models are fun-
damental for speech and language processing. We speciﬁcally build an automatic speech
recognition (ASR) system based on these models and extend them to deal with different
problems in speaker clustering, speech veriﬁcation, speech separation and other natural
language processing systems.
In this chapter, Section 3.1 ﬁrst introduces the probabilistic approach to ASR, which
aims to ﬁnd the most likely word sequence W corresponding to the input speech feature
vectors O. Bayes decision theory provides a theoretical solution to build up a speech
recognition system based on the posterior distribution of the word sequence p(W|O)
given speech feature vectors O. Then the Bayes theorem decomposes the problem
based on p(W|O) into two problems based on two generative models of speech features
p(O|W) (acoustic model) and language features p(W) (language model), respectively.
Therefore, the Bayes theorem changes the original problem to these two independent
generative model problems.
Next, Section 3.2 introduces the HMM with the corresponding likelihood function
as a generative model of speech features. The section ﬁrst describes the dis-
crete HMM, which has a multinomial distribution as a state observation distribution,
and Section 3.2.4 introduces the GMM as a state observation distribution of the con-
tinuous density HMM for acoustic modeling. The GMM by itself is also used as a
powerful statistical model for other speech processing approaches in the later chapters.
Section 3.3 provides the basic algorithms of forward–backward and Viterbi algorithms.
In Section 3.4, ML estimation of HMM parameters is derived according to the EM algo-
rithm to deal with latent variables included in the HMM efﬁciently. Thus, we provide
the conventional ML treatment of basic statistical models for acoustic models based on
the HMM.
From Section 3.6, we go on to describe statistical language models as a genera-
tive model of language features. As a standard language model, we introduce n-gram
models. Similarly to the HMM parameters, the n-gram parameters are also calculated

54
Statistical models in speech and language processing
by using the ML estimation. However, ML solutions to n-gram parameters are easily
overestimated due to the intrinsic sparse data problems in natural languages. Therefore,
the section also describes conventional (rather heuristic) smoothing techniques. Some
of the smoothing techniques introduced here are revisited in later chapters to be inter-
preted as the Bayesian approach, where a Bayesian principle provides these smoothing
techniques to regularize these models. In addition, it is well known that the number of
n-gram parameters is exponentially increased with large n, which makes it impossible
to model a whole document structure within the n-gram model.
Section 3.7 provides another generative model of language features, called the latent
topic model, which deals with a statistical treatment of a document model. The section
also discusses a way of combining such document models and n-gram models.
Finally, following the discussions of statistical acoustic and language models,
Section 3.8 provides an example of applying the Bayesian approach to ASR, as a
case study. The section provides an exact Bayesian manner of formulating the stan-
dard statistical model (HMM) in ASR, and introduces the posterior distributions of the
variables used in acoustic models. The section points out the problem arising mainly
due to the posterior distributions, which can be solved in the later chapters.
3.1
Bayes decision for speech recognition
This section describes a statistical speech recognition framework as an example of
speech and language processing based on the Bayes decision theory. Before the Bayes
decision discussion, we ﬁrst introduce general speech recognition brieﬂy.
Automatic speech recognition aims to extract helpful text information from speech
signals, where both speech and text are represented by sequential patterns. These pat-
terns correspond to the time-series signals which are observed in sequences of random
variables. Speech recognition is processed in a temporal domain. There are some other
technical data, e.g., music signal, video signal, text document, seismic signal, gene
sequence, EEG signal, ECG signal, ﬁnancial data, which are also collected in a time
domain. The sequential pattern property is unique and different from data analysis
in a spatial domain for general image processing and spectral signal processing. In
general, speech and text data are driven under some specialized stochastic process
and probabilistic model, e.g., HMMs (Rabiner & Juang 1986) are used to represent
speech signals and n-gram models are used to characterize word sequences. In front-end
processing, speech signals are ﬁrst chunked into different time frames t with frame
length 25 ms and then transformed to a sequence of speech feature vectors by using
mel-frequency cepstral coefﬁcients (MFCCs) (Davis & Mermelstein 1980) or percep-
tual linear prediction (PLP) coefﬁcients (Hermansky 1990). Each feature vector ot is
regarded as a random vector consisting of entries with continuous value. However,
the nth word wn in a word sequence is a discrete value or label among all words
in a dictionary V with vocabulary size |V|. Thus, speech recognition, which involves
acoustic and language models, handles the modeling of continuous data as well as
discrete data.

3.1 Bayes decision for speech recognition
55
Now we provide a mathematical notation for a speech recognition problem, which
recognizes a speech utterance and outputs the corresponding word sequence in the
utterance. Let ot ∈RD be a D dimensional feature vector at frame t, and O = {ot|t =
1, · · · , T} be a speech feature sequence for T frames of one utterance. The number of
dimensions (D) is usually 39, which consists of 12 dimensional MFCCs + log power,
with delta and delta delta coefﬁcients (Furui 1981). On the other hand, the correspond-
ing word sequence is represented by W = wN
1 = {wn|n = 1, · · · , N}. Here, wn ∈V is
the nth word in this word sequence with N words. The continuous-valued speech feature
sequence O and the discrete-valued word sequence W are sequential patterns in an auto-
matic speech recognition system. Based on the mathematical notations of the speech
feature and word sequences, the Bayes decision theory is introduced to ﬁnd a decision
rule or mapping function d(·) which maps an input feature sequence O into an output
word sequence W by
W = d(O).
(3.1)
A popular decision rule is designed to ﬁnd the most likely word sequence ˆW corres-
ponding to input feature sequence O based on the maximum a-posteriori (MAP)
decision rule,
ˆW = dMAP(O) ≜arg max
W
p(W|O),
(3.2)
where p(W|O) is the posterior distribution of W given O. The posterior distribution is
often rewritten as
ˆW = dMAP(O) = arg max
W
p(O|W)p(W)
p(O)
= arg max
W
p(O|W)
  
acoustic model
×
p(W)

language model
.
(3.3)
The probabilistic product rule decomposes the posterior distribution into likelihood
function p(O|W) and prior probability p(W) based on acoustic model and language
model, respectively. This is a well-known process, called the noisy channel model,
that can deal with a speech recognition problem based on acoustic and language mod-
els. The same scheme of decomposition of this decision rule is widely used for other
speech and language processing including machine translation (Brown, Cocke, Pietra
et al. 1990, Brants, Popat, Xu et al. 2007), spell correction (Brill & Moore 2000), and
voice conversion (Saito, Watanabe, Nakamura et al. 2012).
However, it is more general to follow a Bayesian perspective for pattern recogni-
tion and fulﬁl an optimal Bayes decision to estimate the decision rule ˆd(O) of an input
sentence O by minimizing the expected loss function or Bayes risk, which is deﬁned by
Lee & Huo (2000) as a functional of the decision rule:
r[d] ≜E(W,O)[ℓ(W, d(O))]
=

W

ℓ(W, d(O))p(W, O)dO

56
Statistical models in speech and language processing
=

p(O)

W
ℓ(W, d(O))p(W|O)

dO
=

W
p(W)

ℓ(W, d(O))p(O|W)dO,
(3.4)
where E(W,O)[·] denotes the expectation function over joint distribution p(W, O). For the
later derivations, Eq. (3.4) provides the two equivalent equations in the third and fourth
lines by using the product rule. The loss function satisﬁes this property:
0 ≤ℓ(W, d(O) = W) ≤ℓ(W, d(O) ̸= W),
(3.5)
meaning that the loss due to misclassiﬁcation d(O) ̸= W is larger than or equal to the
loss without misclassiﬁcation d(O) = W. Therefore, the optimal decision rule ˆd(O)
can be obtained by minimizing the Bayes risk, which corresponds to minimizing the
expected loss function.
Bayes risk is expanded into two expressions, which are shown in the third and fourth
equations in the right-hand-side of Eq. (3.4). Following the third equation in Eq. (3.4),
we ﬁnd that Bayes decision rule is equivalent to dealing with a minimization problem:
min
d∈D r[d] = min
d∈D

p(O)

W
ℓ(W, d(O))p(W|O)

dO.
(3.6)
Here D denotes a set of all possible decision functions. This optimization can be solved
by minimizing the expression in the brackets in the above equation, since the deci-
sion rule function does not depend on O in general. This minimization is satisﬁed by
considering the following optimal decision rule given any O:
ˆd(O) = arg min
d(O)∈D

W
ℓ(W, d(O))p(W|O)
= arg min
d(O)∈D E(W)[ℓ(W, d(O))|O].
(3.7)
That is, ﬁnding the optimal decision rule ˆd(O) in terms of minimizing the expected loss
(in Eq. (3.4)) is equivalent to ﬁnding the optimal decision rule function in terms of the
expected loss function given O.
In Goel & Byrne (2000) and Chien, Huang, Shinoda et al. (2006), a minimum Bayes
risk (MBR) classiﬁcation was proposed to fulﬁl an optimal Bayes decision in Eq. (3.7)
for automatic speech recognition by using a predeﬁned loss function. In Goel & Byrne
(2000), the word error rate (WER) loss function ℓWER(W, O) was calculated using Lev-
enshtein distance between word sequence hypotheses. This function was used to build
an MBR decision rule:
dMBR(·) = arg min
d(·)

W
p(W)
×

ℓWER(W, d(O))p(O|W)dO,
(3.8)
which is derived according to the fourth equation in the right-hand-side of Eq. (3.4).

3.1 Bayes decision for speech recognition
57
More popularly, a meaningful loss function for speech recognition is simply speciﬁed
by a so-called zero-one loss function which treats misclassiﬁcation of each observation
sample O equally, namely by using
ℓ01(W, d(O)) =
. 0
if correctly classiﬁed or d(O) = W
1
if wrongly classiﬁed or d(O) ̸= W.
(3.9)
Substituting Eq. (3.9) into the fourth line in Eq. (3.4) leads to zero-one Bayes risk:
r01[d] =

W
p(W)

d(O)̸=W
ℓ01(W, d(O))p(O|W)dO
+

W
p(W)

d(O)=W
ℓ01(W, d(O))p(O|W)dO
=

W
p(W)

d(O)̸=W
p(O|W)dO.
(3.10)
In the third line of Eq. (3.10), the expectation operation using zero-one loss function
r01(d(·)) is calculated over all observations which are wrongly classiﬁed, i.e., d(O) ̸= W.
This loss function corresponds to unconditional error probability, which is reasonable to
act as a measure of goodness of the decision rule for speech recognition.
In addition, Eq. (3.10) is further rewritten as
r01[d] =

W
p(W)

1 −

d(O)=W
p(O|W)dO

= 1 −

W

d(O)=W
p(W)p(O|W)dO
(3.11)
by using the following properties:

W
p(W) = 1,
(3.12)

p(O|W)dO = 1.
(3.13)
The resulting decision rule d01(·) follows the minimum classiﬁcation error criterion
which leads to the MAP decision rule as addressed in Eq. (3.2), i.e.,
ˆW = d01(O) = dMAP(O) = arg max
W
p(W|O).
(3.14)
The most likely word sequence ˆW is found so as to achieve the highest posterior
probability for correctly classiﬁed observation sequence d(O) = W.
Using an MAP decision rule, the probability measure p(O|W) calculates how likely
the acoustic observation sequence O is, based on the word sequence hypothesis W. We
also name p(O|W) as the acoustic likelihood function. There are many kinds of acoustic
models which are assumed in calculation of the statistical model p(O|W) based on
a set of acoustic parameters . In this chapter, the hidden Markov model (HMM) is
considered for acoustic modeling and the HMM parameters  are plugged into the

58
Statistical models in speech and language processing
probability measure estimator ˆp(O|W). On the other hand, the probability measure
p(W) is deﬁned as the prior probability of word sequence W. This measure calculates
the joint probability for a sequence of words based on a set of multinomial parameters or
n-gram parameters . The plug-in language model ˆp(W) is determined as a language
model estimator. Here, acoustic parameters and linguistic parameters are both included
in model parameters . For an automatic speech recognition (ASR) system, we will
estimate the acoustic parameters and the linguistic parameters from a set of training
utterances O and their word transcriptions W according to the Maximum Likelihood
(ML) estimation. We assume that these plug-in models {ˆp(O|W), ˆp(W)} given ML
parameters  are true. The prediction of new test utterance O based on the estimated
MAP decision rule ˆdMAP(O) is performed by
ˆdMAP(O) = arg max
W
ˆp(W|O)
= arg max
W
ˆp(O|W)ˆp(W).
(3.15)
However, the point estimates of the ML-based acoustic model and language model 
from the given observation space o using the collected training data may not generalize
well for the unknown test data outside the training space o. The distributions p(O|W)
and p(W) may not be correctly assumed or may be over-trained or under-trained. From
a Bayesian perspective, these issues could be tackled by treating acoustic parameters
and linguistic parameters  as random variables. Consideration of these uncertainties is
helpful for recognition of new test data. For this consideration, the expected loss function
in Eq. (3.4) is calculated by additionally marginalizing over continuous parameters :
rBPC(d(·)) = E(W,O,)[ℓ(W, O, )].
(3.16)
The Bayesian predictive classiﬁcation (BPC) rule,
dBPC(·) = arg min
d(·)∈d
rBPC(d(·)),
(3.17)
(Jiang, Hirose & Huo 1999, Huo & Lee 2000, Lee & Huo 2000, Chien & Liao 2001) was
proposed to establish a robust decision rule for unknown test speech. The zero-one loss
function ℓ01(·) was applied in previous studies on the BPC rule. Details of BPC-based
speech recognition will be addressed in Section 6.3.
In addition, Bayes decision theory was developed to estimate the discriminative
acoustic model for speech recognition (Juang & Katagiri 1992). The idea is to minimize
the expected loss function based on the logistic sigmoid function,
ℓ(dk(O, )) =
1
1 + exp−αdk(O,) ,
(3.18)
which is a function of misclassiﬁcation measure deﬁned as
dk(O, ) =−gk(O; )
+ log
⎛
⎝
1
K −1

j,j ̸=k
exp(gj(O; ))
⎞
⎠.
(3.19)

3.2 Hidden Markov model
59
Figure 3.1
Graphical model of HMM without model parameters.
In Eq. (3.18), α is a tuning parameter and the misclassiﬁcation measure of
phone class Ck from training data O = {ot} is calculated by measuring the distance
between the discriminant functions gk(O, ) of the target phone ot ∈Ck and its
competing phones ot /∈Ck. The discriminant function of competing phones is aver-
aged over all phones except the target phone. The discriminative acoustic model
was trained according to the minimum classiﬁcation error (MCE) criterion which is
closely related to the minimum Bayes risk for optimal Bayes decision (Juang & Katagiri
1992).
3.2
Hidden Markov model
The previous section describes the Bayes decision theory and introduces acoustic model
p(O|W) and language models p(W). This section describes hidden Markov models
(HMMs) (Rabiner & Juang 1986) (Figure 3.1) as a standard statistical acoustic model
in detail. Before describing the HMM in detail, we ﬁrst explain what HMM represents
in speech recognition.
3.2.1
Lexical unit for HMM
The acoustic model p(O|W) means that we provide a likelihood function of the obser-
vations O given the word sequence W. However, since the number of all possible word
sequences is an exponential order, we cannot prepare a likelihood function for each word
sequence. Instead, we ﬁrst introduce a lexical sequence L = {lm ∈L|m = 1, · · · , M}
that is composed of phonemes (e.g., /a/,/k/), context-dependent phonemes (e.g., /a/-
/k/-/i/,/a/-/k/-/a/), or words (e.g., numbers, commands) as lm. A discussion about the
context-dependent phoneme (allophone) unit can be found in Section 6.5. We usually
use a phoneme unit deﬁned by linguistics, and the automatic discovery of the phoneme
unit from speech data by using Bayesian nonparametrics can be found in Section 8.4.
The deﬁnition of the lexical unit depends on application, but the standard acoustic model
for LVCSR uses (context-dependent) phonemes as a lexical unit, and hereinafter in
this chapter, we use phonemes to deﬁne a lexical unit. Then, L is a set of all distinct
phonemes.

60
Statistical models in speech and language processing
By using the lexical sequence L, we can revisit the MAP decision rule in Eq. (3.15)
for ASR as follows:
dMAP(O) = arg max
W p(W|O)
= arg max
W p(O|W)p(W)
= arg max
W

L
p(O, L|W)p(W).
(3.20)
By using the product rule, and assuming that the likelihood function only depends on
the lexical sequence L, it is rewritten as:
dMAP(O) = arg max
W

L
p(O|L, W)p(L|W)p(W)
≈arg max
W

L
p(O|L)p(L|W)p(W),
(3.21)
where p(L|W) is called a lexical model. Usually, the lexical model is not a probabilistic
model, but is obtained deterministically by using a lexical dictionary, which provides a
phoneme sequence (or multiple phoneme sequences) given a word. We further assume
that the alignment of O for phoneme lm is already given. This means that O is segmented
to Om = {otm−1+1, · · · , otm} where t0 = 0, tM = T, and {tm}M−1
m=1 is given.1
By assuming that Om is independent and identically distributed (iid) for phoneme lm,
the acoustic model is factorized by m as:
p(O|L) =
M

m=1
p(Om|lm).
(3.22)
This p(Om|lm) is an actual likelihood function that we deal with for ASR, and is rep-
resented by an HMM. Therefore, the acoustic model is composed of |L| HMMs where
|L| denotes the number of distinct phonemes. The following section explains the HMM
for one phoneme, and omits phoneme lm in the explanation. In addition, since the align-
ment is already given, we omit the segmentation information m based on tm, and use
O = {ot ∈RD|t = 1, · · · , T} instead of Om to be modeled by an HMM.
3.2.2
Likelihood function of HMM
This section describes a likelihood function of the HMM for a phoneme, where the
HMM is a popular formalism for representation of sequential patterns. The likelihood
function deﬁned here is used to estimate HMM parameters.
In a set of D dimensional continuous-valued speech feature vectors O, each observa-
tion ot is represented under a Markov state st as illustrated in Figure 3.1. We assume
1
In the actual search (arg maxW) process in Eq. (3.21), this alignment information is not ﬁxed, and is
searched at the same time. Many techniques have been developed to efﬁciently search this huge space
considering the lexicon and word sequences (e.g., Ney, Haeb-Umbach, Tran et al. (1992) and weighted
ﬁnite state transducer (WFST) based techniques (Mohri, Pereira & Riley 2002, Hori & Nakamura 2013)).

3.2 Hidden Markov model
61
that each speech frame ot is independent conditionally on its state label st. Starting from
the state at the ﬁrst time frame with an initial state probability πs1, which is a model
parameter of HMM,
p(s1) ≜πs1,
(3.23)
where each state at time t depends on the state at its previous time t −1. The state
transition probability
p(st|st−1) ≜ast−1st
(3.24)
is also introduced as HMM parameters to drive the change of HMM states or equiva-
lently characterize the transition of acoustic events under a phone unit. The parameters
of initial state probabilities and state transition probabilities should satisfy the following
constraints:
J

j=1
πj = 1,
J

j=1
aij = 1,
∀i
(3.25)
respectively. A sequence of hidden states S = {st ∈{1, · · · , J}|t = 1, · · · , T} corre-
sponding to a sequence of observations O is marginalized in calculation of likelihood
function. HMMs basically involve a doubly stochastic process. One is for the observed
speech frame sequence O and the other is for the corresponding HMM state sequence
S, which is not observed and is regarded as a latent variable. The model which includes
a latent variable is called the latent model.
Since S is not observed, the likelihood function of O is given by the summation of
the joint distribution of O and S over all possible state sequences conditioned on a set of
HMM parameters :
p(O|) =

S∈S
p(O, S|).
(3.26)
S denotes a set of all possible state sequences, which is often omitted when it is trivial.
Note that the summation over all possible state sequences requires the exponential order
of computations, which is not feasible in a practical use. The joint distribution p(O, S|)
is also called complete data likelihood, where a set of observed data and latent variables
({O, S}) is called complete data. The joint distribution (complete data likelihood) is a
useful representation to provide approximate solutions for the latent model (Dempster
et al. 1976, Huang, Ariki & Jack 1990). Therefore, we focus on the joint likelihood
function of the HMM.
By using the product rule, the joint distribution in Eq. (3.26) is decomposed into
the likelihood function p(O|S, ) given the state sequence S, and the state sequence
probability P(S|) as follows:
p(O, S|) = p(O|S, )p(S|).
(3.27)

62
Statistical models in speech and language processing
Since ot is independent and identically distributed (iid) given the state sequence S =
{s1, · · · , ST}, the likelihood function p(O|S, ) is represented as follows:
p(O|S, ) =
T

t=1
p(ot|st).
(3.28)
Here, j is a set of state-dependent HMM parameters, where j is the index of the HMM
state. Similarly, the state sequence probability P(S|) given the state sequence S =
{s1, · · · , ST} can be represented by the initial probability in Eq. (3.23) and transition
probabilities in Eq. (3.24), as follows:
p(S|) = πs1
T

t=2
ast−1st.
(3.29)
Thus, by substituting Eqs. (3.28), (3.29), and (3.27) into Eq. (3.26), we can obtain the
following equation:
p(O|) =

S={st}∈S

πs1p(o1|s1)
T

t=2
ast−1stp(ot|st)

.
(3.30)
This is the basic equation of the likelihood function of the HMM with the parameter
 = {{πj}J
j=1, {{aij}J
j=1}J
i=1, {j}J
j=1}. p(ot|j) is called the emission probability, and
can be any distribution of ot.
If we use discrete values as observations for ot, the emission probability of the
HMM is represented by a multinomial distribution. This HMM is called a dis-
crete HMM (DHMM). In the DHMM, the HMM parameters are formed as  =
{{πj}J
j=1, {{aij}J
j=1}J
i=1, {{bjk}K
k=1}J
j=1} where the emission probabilities of the discrete
observation values k given state j are represented as the multinomial distribution param-
eter bjk. Assuming that the vector space of observations O = {ot} in state j is partitioned
into K subspaces by clustering or vector quantization, we express the state observation
probability bjk using the probability p(ot ∈Ck|j), which is constrained by the property
K

k=1
p(ot ∈Ck|j) =
K

k=1
bjk = 1.
(3.31)
Here Ck denotes a set of discrete feature vectors that belongs to a partitioned subspace
k. This partition is undertaken by using Vector Quantization (VQ) techniques.
Historically, DHMM was ﬁrst used for speech modeling with the VQ techniques
(Matsui & Furui 1994, Jelinek 1997). However, since the speech feature vector is a
continuous value, the Gaussian distribution or the mixture of Gaussian distributions is
often used for an acoustic model. That is, an acoustic model, which dealt with discrete
observation values obtained from VQ codes, was shifted to so-called continuous density
HMM (CDHMM), where the HMM with Gaussian distribution based emission prob-
abilities could model continuous observation values more appropriately. CDHMM is
described in the next section.

3.2 Hidden Markov model
63
3.2.3
Continuous density HMM
For the HMM framework addressed in Section 3.2, the state observation probability
p(ot|j) is calculated according to the type of observation data. When using discrete
observation symbols, each observation vector ot ∈Ck is simply represented by a code-
book partition k which is determined through vector quantization over the vector space
of all training samples O. Nonetheless, the representation of a high dimensional fea-
ture vector ot ∈RD based on a set of discrete codebook labels is not adequate to
fully reﬂect the randomness of the observation vector. More generally, we calculate
the probability density function (pdf) of the observation vector ot given an HMM state
st = j to determine the state observation probability p(ot|j). It is popular to represent
the randomness of continuous-valued ot using the multivariate Gaussian distribution
N(ot|μj, j) deﬁned in Appendix C.6 as:
p(ot|j) = N(ot|μj, j)
≜
1
(2π)D/2|j|1/2 exp

−1
2(ot −μj)⊺−1
j
(ot −μj)

,
(3.32)
with state-dependent mean vector μj ∈RD and covariance matrix j ∈RD×D.
However, a single Gaussian distribution is insufﬁcient to represent the state-dependent
observation space for an HMM state j because there are large amounts of training data
collected from varying speakers with different genders, ages, accents, speaking rates,
channel distortions and background noises, which are used to train the parameters of
individual HMM states. Accordingly, a Gaussian mixture model (GMM) is adopted to
represent the state-dependent observation space. This is based on a set of Gaussian dis-
tributions which reﬂect the variations of speech feature vectors within an HMM state
due to various acoustic conditions. The state observation probability density function
of a feature vector ot at time t and in state j is expressed by GMM with K mixture
components:
p(ot|st = j, j) =
K

k=1
p(ot, vt = k|st = j, j)
=
K

k=1
p(vt = k|st = j, j)p(ot|st = j, vt = k, j)
=
K

k=1
ωjkN(ot|μjk, jk).
(3.33)
In Eq. (3.33), each mixture component of state j is expressed by a Gaussian distribution:
p(ot|st = j, vt = k, j) = N(ot|μjk, jk)
≜
1
(2π)D/2|jk|1/2 exp

−1
2(ot −μjk)⊺−1
jk (ot −μjk)

,
(3.34)

64
Statistical models in speech and language processing
and the prior probability of a mixture component vt = k is deﬁned as a mixture weight,
p(vt = k|st = j, j) ≜ωjk.
(3.35)
The state-dependent GMM parameters {ωjk, μjk, jk}K
k=1 consist of mixture weights ωjk,
mean vectors μjk, and covariance matrices jk for K Gaussian mixture components. The
resulting realization of HMM using GMM as state observation probability is also called
the continuous density HMM (CDHMM). The CDHMM parameters are formed as
 ≜{{πj}J
j=1, {{aij}J
j=1}J
i=1, {{ωjk, μjk, jk}K
k=1}J
j=1},
(3.36)
with an additional constraint on the prior probability of mixture components or the
mixture weights.
Typically, HMM covariance matrices are assumed to be diagonal in practical imple-
mentation, i.e., jk = diag(jk1, · · · , jkd, · · · , jkD). Note that this book uses  to
represent a diagonal component of covariance, or variance when D = 1. Then, the diag-
onal covariance version of Eq. (3.34) is factorized to the univariate Gaussian distribution
(Appendix C.5) by the dimension index d as follows:
p(ot|st = j, vt = k, j) =
D

d=1
N(otd|μjkd, jkd)
≜
D

d=1
1
(2π)1/2(jkd)1/2 exp

−
1
2jkd
(otd −μjkd)2

.
(3.37)
This diagonal covariance representation can represent the distribution with all scalar
variables, which makes the calculation very simple compared with the vector and matrix
representation in the multivariate Gaussian distribution in Eq. (3.34). In addition, the
number of parameters for the covariance matrix is reduced from D ∗(D + 1)/2 to D,
which can reduce the computational and memory costs. The full covariance Gaussian in
Eq. (3.34) also requires computation of the inverse and the determinant of the covariance
matrix (−1
jk
and |jk|), which makes the computation numerically unstable in addi-
tion to incurring the matrix computation cost. However, the speech feature vectors have
correlations over dimensions, and the diagonal covariance assumption is not adequate.
There are more sophisticated models such as the subspace mean and variance meth-
ods, aiming to bridge the gap between full and diagonal covariance modeling, which
have been proposed (Gales 1999, Axelrod, Gopinath & Olsen 2002). In this book, we
keep the vector and matrix representation for the multivariate Gaussian distribution for
generality.
Based on the above likelihood discussion of the CDHMM for single observation vec-
tor ot given HMM state st = j at frame t, we consider the marginal likelihood for a whole
observation sequence O. Under CDHMM, we have an additional latent variable vt = k
from the mixture component of GMM for each speech frame ot at time t, compared with
Eq. (3.30). Thus, the sequence of mixture components
V ≜{vt ∈{1, · · · , K}|t = 1, · · · , T},
(3.38)

3.2 Hidden Markov model
65
corresponding to the sequence of observed speech frames O = {ot|t = 1, · · · , T}, is
introduced in estimation of HMM parameters in addition to S = {st ∈{1, · · · , J}|t =
1, · · · , T}. Considering two sets of latent variables S and V, we consider the likeli-
hood function of the CDHMM by following the formulation in Section 3.2.2. First,
the marginal likelihood function of the CDHMM is represented as the joint distribution
p(O, S, V|):
p(O|) =

S,V
p(O, S, V|),
(3.39)
where all possible state sequences S and mixture component sequences V are consid-
ered in calculation of the marginal distribution. The joint distribution p(O, S, V|) is
also called the complete data likelihood, as we discussed in Eq. (3.26) with additional
latent variable V. This equation is further decomposed by using the product rule as
follows:
p(O, S, V|) = p(O|S, V, )p(V|S, )p(S|).
(3.40)
We provide an actual distribution for each probabilistic function.
Since the distributions p(S|) are given in Eq. (3.29), we focus on p(V|S, ) and
p(O|S, V, ). Since vt only depends on st, given the state sequence S = {s1, · · · , ST},
p(V|S, ), it is represented by the factorized form with frame t as follows:
p(V|S, ) =
T

t=1
p(vt|st, ) =
T

t=1
ωstvt.
(3.41)
Similarly, from Eq. (3.34), p(O|S, V, ) is also represented by the factorized form with
frame t given S and V as:
p(O|S, V, ) =
T

t=1
p(ot|st, vt, )
=
T

t=1
N(ot|μstvt, stvt).
(3.42)
Thus, by substituting Eqs. (3.29), (3.41), and (3.42) into Eq. (3.40), the complete data
likelihood function of the CDHMM is represented as
p(O, S, V|) = πs1ωs1v1N(o1|μs1v1, s1v1)
 T

t=2
ast−1stωstvtN(ot|μstvt, stvt)

.
(3.43)
Thus, the marginal likelihood of the CDHMM is extended from Eq. (3.30) as
p(O|) =

S,V
πs1ωs1v1N(o1|μs1v1, s1v1)
 T

t=2
ast−1stωstvtN(ot|μstvt, stvt)

. (3.44)

66
Statistical models in speech and language processing
This is the basic equation of the likelihood function of the CDHMM with the parameter
 = {{πj}, {aij}, {ωjk}, {μjk}, {jk}}. As we discussed in the previous section, the sum-
mation over all possible state and mixture component sequences requires an exponential
order of computations, which is not tractable in practical use. Section 3.3 describes
how to efﬁciently compute the marginal likelihood by using the forward and back-
ward algorithms, and most probable state sequence based on the Viterbi algorithm.
Section 3.4 also describes how to estimate the HMM parameters  efﬁciently based
on these algorithms and the expectation and maximization algorithm.
Before moving to these explanations, as a subset model of the CDHMM, we introduce
a simple GMM without an HMM, which is also widely used in speech and language
processing.
3.2.4
Gaussian mixture model
The GMM is a simpliﬁed model of the CDHMM, without considering the state sequence
S in the previous section. However, this simple GMM is still very powerful for modeling
speech features. For example, the GMM is used in speech and noise models in speech
enhancement, and in speaker models in speaker veriﬁcation and clustering, which are
discussed in Section 4.6. The GMM can also be widely used to model other signals than
speech, e.g., image processing, and biosignals. Therefore, this section only introduces
the marginal likelihood of a GMM to be used in later chapters, similar to that of the
CDHMM in Eq. (3.44):
p(O|) =

V
p(O, V|) =

V
p(O|V, )p(V|)
=

V
T

t=1
p(ot|vt, )p(vt|)
=
T

t=1
K

vt=1
ωvtN(ot|μvt, vt),
(3.45)
with the parameter  = {{ωk}, {μk}, {k}}. Since vt is independent of vt′̸=t in a GMM,
the sequential summation over V is independently applied to each p(ot|vt, )p(vt|). So,
we can factorize p(O|) into the following tth frame distribution given the kth mixture
component :
p(ot|vt = k, ) = N(ot|μk, k)
p(vt = k|) = ωk.
(3.46)
Thus, unlike the HMM case that needs to consider the HMM state sequence S (compu-
tation based on an exponential order for the frame length), the GMM can compute the
likelihood of all possible V by using the straightforward computation of Eq. (3.45) with
the linear order computation of the frame length. As regards the parameter estimation,
the EM algorithm is widely used, and this is discussed in Section 3.4.
Figure 3.2 illustrates a univariate GMM distribution p(o) = 1
3N(o|μ1 = 2, 1 =
36) + 1
3N(o|μ2 = 30, 2 = 64) + 1
3N(o|μ3 = 45, 3 = 16) which is plotted with

3.2 Hidden Markov model
67
Figure 3.2
Distributions of three individual Gaussians (dashed line) and the corresponding GMM (solid
line).
a dashed line and is formed as a mixture of three Gaussian distributions with different
means {μ1, μ2, μ3} and variances {1, 2, 3}, which are shown with a solid line. As
shown in the ﬁgure, the GMM is a multi-modal distribution with multiple peaks, which
can model multiple factors in speech features. Therefore, the GMM can accurately
represent speech variations that cannot be represented by a single Gaussian distribution.
3.2.5
Graphical models and generative process of CDHMM
The previous sections explain the marginal likelihood functions of the GMM and
CDHMM based on their joint likelihood distributions. As we discussed in the graph-
ical model representation in Section 2.2, once we obtain the joint likelihood distribution
of a model, we can obtain the corresponding graphical model and generative process
of the model. This section provides the graphical model and generative process of the
GMM and CDHMM, respectively, and these are used in the following chapters to deal
with these models by using the Bayesian approach.
Graphical models and generative process of GMM
Based on the previous explanations, we can provide a generative process and graphical
model of a K-component GMM discussed in Section 3.2.4 as an example. Given fea-
ture vectors O, latent variables V, and model parameters , the joint distribution of a
complete data set {O, V} is represented from Eq. (3.45) as follows:
p(O, V|) =
T

t=1
p(ot|vt, )p(vt|).
(3.47)

68
Statistical models in speech and language processing
Figure 3.3
Graphical model of Gaussian mixture model.
Thus, the joint distribution of the GMM is parameterized as
p(O, V|) =
T

t=1
ωvtN(ot|μvt, vt).
(3.48)
Thus, we have the following three kinds of variables:
• Observation: O = {o1, · · · , oT};
• Latent: V = {v1, · · · , vT};
• Non-probabilistic:  = {ωk, μk, k}K
k=1.
The dependencies of the above variables are expressed in Eq. (3.48), and we provide the
generative process of the GMM in Algorithm 2. Given the mixture weight ωvt, latent
variable vt is sampled from the multinomial distribution.
In addition, by using the plate for t and k, as discussed in Section 2.2.2, we can simply
write the graphical model of the GMM, as shown in Figure 3.3.
Algorithm 2 Generative process of Gaussian mixture model
Require: T, {ωk, μk, k}K
k=1
1: for t = 1, · · · , T do
2:
Draw vt from Mult(vt|{ωk}K
k=1)
3:
Draw ot from N(ot|μvt, vt)
4: end for
Graphical models and generative process of CDHMM
Similar to the GMM case, we can also provide a generative process and graphical
model of a continuous density HMM discussed in Section 3.2 as another example.
Given feature vectors O, latent variables S and V, and model parameters , the joint
likelihood function of complete data O, V, and S based on a continuous density HMM
is represented as follows:

3.2 Hidden Markov model
69
p(O, S, V|) = πs1ωs1v1N(o1|μs1v1, s1v1)
×
T

t=2
ast−1stωstvtN(ot|μstvt, stvt).
(3.49)
The CDHMM has the following three kinds of variables:
• Observation: O = {o1, · · · , oT};
• Latent: S = {s1, · · · , sT} and V = {v1, · · · , vT};
• Non-probabilistic:  = {{πj}J
j=1, {{aij}J
j=1}J
i=1, {{ωjk, μjk, jk}K
k=1}J
j=1}.
Algorithm 3 Generative process of continuous density hidden Markov model
Require: T, 
1: Draw s1 from Mult(s1|{πj}J
j=1)
2: Draw v1 from Mult(v1|{ωs1k}K
k=1)
3: Draw o1 from N(o1|μs1v1, s1v1)
4: for t = 2, · · · , T do
5:
Draw st from Mult(st|{ast−1j}J
j=1)
6:
Draw vt from Mult(vt|{ωstk}K
k=1)
7:
Draw ot from N(ot|μstvt, stvt)
8: end for
Similarly to the GMM, dependencies of the above variables are expressed in
Eq. (3.49), and we provide the generative process of the CDHMM in Algorithm 3.
Figure 3.4 shows the graphical model of the CDHMM that uses the plates for the num-
ber of HMM states J, and the number of GMM components K. The graphical model of
the CDHMM is rather complicated since the state transition makes it difﬁcult to use the
plate representation for t.
Figure 3.4
Graphical model of continuous density hidden Markov model given parameters .

70
Statistical models in speech and language processing
Thus, we provide the graphical model and generative process examples of our typi-
cal target model, CDHMM. Note that these are rather simple based on a non-Bayesian
approach since we do not deal with the model parameters as probabilistic variables (do
not use circle representations for them).
3.3
Forward–backward and Viterbi algorithms
This section describes how to compute the likelihood values of HMMs (given a latent
variable) by using the famous forward–backward and Viterbi algorithms, which are used
to solve the statistical speech recognition problem in Eq. (3.3). Although the direct
computation of the likelihood values causes the combinatorial explosion problem, these
algorithms can provide feasible computational costs. These algorithms are also used to
estimate the model parameters, which is described in Section 3.4
3.3.1
Forward–backward algorithm
Basically, the direct computation of marginal likelihood p(O|) = 
S p(O, S|) in
Eq. (3.30) involves on the order of 2T · JT calculations, since at every t = 1, 2, · · · , T,
there are J possible states that can be reached (i.e., there are JT possible state sequences),
and for each such state sequence about 2T calculations are required for each term in
the sum of Eq. (3.30). By considering the latent variable of mixture components of
the CDHMM in Eq. (3.44), the direct computation of marginal likelihood p(O|) =

S,V p(O, S, V|) is further increased to the order of 2T · JTKT. However, this large
computation problem could be tackled by applying the forward–backward algorithm
(Rabiner & Juang 1986). According to this algorithm, the forward variable αt(j) is
deﬁned by
αt(j) ≜p(o1, · · · , ot, st = j|).
(3.50)
This forward variable is known as the probability of the partial observation sequence
{o1, o2, · · · , ot} until time t and state j at time t given the current HMM parameters .
We also deﬁne an emission probability given HMM state j at frame t as
bj(ot) ≜p(ot|st = j, j).
(3.51)
In the CDHMM, this is represented by the likelihood function of the GMM, as described
in Eq. (3.33). A forward procedure, which is a dynamic programming method, is
inductively derived to speed up computation of p(O|) as follows:
Forward algorithm
• Initialization
α1(j) = p(o1, s1 = j|)
= p(o1|s1 = j, j)p(s1 = j|j)
= πjbj(o1),
1 ≤j ≤J.
(3.52)

3.3 Forward–backward and Viterbi algorithms
71
• Induction
αt(j) = p(o1, · · · , ot, st = j|)
=
J

i=1
p(o1, · · · , ot, st−1 = i, st = j|)
=
J

i=1
p(ot|st = j, o1, · · · , ot−1, st−1 = i, )
× p(st = j|o1, · · · , ot−1, st−1 = i, )
× p(o1, · · · , ot−1, st−1 = i|)
= p(ot|st = j, j)
J

i=1
p(st = j|st−1 = i, )p(o1, · · · , ot−1, st−1 = i|)
=
 J

i=1
αt−1(i)aij

bj(ot),
2 ≤t ≤T
1 ≤j ≤J.
(3.53)
• Termination
p(O|) =
J

j=1
p(o1, · · · , oT, sT = j|)
=
J

j=1
αT(j).
(3.54)
Thus, we can compute the likelihood value p(O|) recursively by using the forward
variable αt(j). This iterative algorithm for computing the forward variable is called the
forward algorithm. In Eq. (3.52) and Eq. (3.53), we see that the calculation of {αt(j)|1 ≤
t ≤T, 1 ≤j ≤J} requires on the order of J2T calculations which is dramatically
reduced from 2T · JT as required in the direct calculation. Figure 3.5 illustrates the
sequence of operations for computation of the forward variable αt(j).
Figure 3.5
Propagation of forward variable from αt−1 = i to αt = j. All possible states at time t −1 are
considered. Adapted from Rabiner & Juang (1993).

72
Statistical models in speech and language processing
Similarly, we consider a backward variable which is deﬁned as
βt(j) ≜p(ot+1, · · · , oT|st = j, ).
(3.55)
This variable represents the probability of the partial observation sequence from t + 1
to the end, given state j at time t and model parameters . Again, we can inductively
derive the following backward procedure:
Backward algorithm
• Initialization
βT(j) = 1,
1 ≤j ≤J.
(3.56)
• Induction
By using the sum and product rules, we can rewrite βt(i) as
βt(i) = p(ot+1, · · · , oT|st = i, )
=
J

j=1
p(ot+1, · · · , oT, st+1 = j|st = i, )
=
J

j=1
p(ot+1, · · · , oT|st+1 = j, st = i, )p(st+1 = j|st = i, ).
(3.57)
By using the conditional independence property of the HMM,
βt(i) =
J

j=1
p(ot+1, · · · , oT|st+1 = j, )p(st+1 = j|st = i, )
=
J

j=1
p(ot+2, · · · , oT|st+1 = j, )p(ot+1|st+1 = j, j)
× p(st+1 = j|st = i, )
=
J

j=1
aijbj(ot+1)βt+1(j), t = T −1, T −2, · · · , 1,
1 ≤i ≤J.
(3.58)
• Termination
β0 ≜p(O|)
=
J

j=1
p(o1, · · · , oT, s1 = j|)
=
J

j=1
p(o1, · · · , oT|s1 = j, )p(s1 = j|)
=
J

j=1
p(o1|s1 = j, )p(o2, · · · , ot|s1 = j, )p(s1 = j|)
=
J

j=1
πjbj(o1)β1(j).
(3.59)

3.3 Forward–backward and Viterbi algorithms
73
Figure 3.6
Propagation of backward variable from αt+1 = j to αt = i. All possible states at time t + 1 are
considered. Adapted from Rabiner & Juang (1993).
Thus, the backward algorithm can also compute the likelihood value p(O|) recursively
by using the backward variable βt(j). In the initialization step of the backward procedure,
βT(j) is arbitrarily assigned to be 1. In the induction step, the backward variable βt(i) is
calculated in a backward fashion from t = T−1 to the beginning t = 1. Figure 3.6 shows
the sequence of operations required for computation of the backward variable βt(i).
This iterative algorithm for computing the backward variable is called the backward
algorithm.
The forward variable αt(j) and backward variable βt(j) are used to calculate the poste-
rior probability of a speciﬁc case. For example, if we consider the posterior probability
p(st = j|O, ) when the HMM state is j at frame t, we ﬁrst deﬁne
γt(j) ≜p(st = j|O, ),
(3.60)
which is useful in the later explanations. By using the sum and product rules, γt(j) is
represented by the likelihood ratio of
γt(j) ≜p(st = j, O|)
p(O|)
=
p(O, st = j|)
J
i=1 p(O, st = i|)
.
(3.61)
Now we focus on the joint distribution p(st = j, O|), which is rewritten by forward
variable αt(j) in Eq. (3.50) and backward variable βt(j) in Eq. (3.55), as follows:
p(st = j, O|) = p(O|st = j, )p(st = j|)
= p(o1, · · · , ot|st = j, )p(ot+1, · · · , oT|st = j, )p(st = j|)
= p(o1, · · · , ot, st = j|)



αt(j)
p(ot+1, · · · , oT|st = j, )



βt(j)
.
(3.62)
From the ﬁrst to second lines, we use the conditional independence assumption of the
HMM. Thus, ﬁnally the posterior distribution γt(j) is represented as:
γt(j) ≜p(st = j|O, ) =
αt(j)βt(j)
J
j′=1 αt(j′)βt(j′)
.
(3.63)

74
Statistical models in speech and language processing
This probability will be used to ﬁnd the optimal state sequence based on the Viterbi
algorithm, as mentioned in Section 3.3.2, as well as to estimate the HMM parameters
based on the maximum likelihood method, as addressed in Section 3.4.
3.3.2
Viterbi algorithm
Finding the optimal state sequence ˆS = {ˆst|t = 1, · · · , T} of observation sequence O, in
terms of the maximum a-posteriori sense, is seen as a fundamental problem in the HMM
framework. It is formulated as
ˆS = arg max
S
p(S|O, ),
(3.64)
where arg maxS ﬁnd the most probable ˆS from all possible state sequences, and this also
involves on the order of JT calculations, similar to the likelihood computation p(O|)
in Section 3.3.1. In addition, since p(O|) does not depend on S, we can also rewrite
Eq. (3.64) as
ˆS = arg max
S
p(S|O, ) = arg max
S
p(S|O, )p(O|)
= arg max
S
p(S, O|).
(3.65)
That is, the optimal state sequences obtained in terms of the posterior distribution
p(S|O, ) and the joint distribution p(S, O|) are equivalent.
The optimal state sequence can be used to determine the segmentation of a speech
sentence into phones or sub-phones, if we assume these as latent variables. In this case,
the speech frames staying in the same state behave similarly, and the transition from one
state to the other is treated as a segmentation boundary.
In addition, the optimal state sequence ˆS is applied to approximate the calculation of
the marginal likelihood function over S as follows:
p(O|) =

S
p(S, O|) ≈p(ˆS, O|).
(3.66)
That is, the marginal likelihood function is approximately represented as the joint
likelihood distribution of ˆS and O, which can be represented with Eq. (3.30) as
p(ˆS, O|) =

πˆs1p(o1|ˆs1)
T

t=2
aˆst−1ˆstp(ot|ˆst)

≜ˆp.
(3.67)
Therefore, the approximation of marginal likelihood using the optimal state sequence
provides the solution to the segmental likelihood function ˆp since likelihood calcu-
lation is based on the speech segmentation using ˆS. Considering all possible state
sequences, {S} is simpliﬁed to applying only the optimal state sequence ˆS. The number
of calculations is further reduced to 2T as referred to Eq. (3.67).

3.3 Forward–backward and Viterbi algorithms
75
We can individually determine the most likely state ˆst for each time frame 1 ≤t ≤T
according to the posterior probability as
ˆst = arg max
1≤j≤J p(st = j|O, ).
(3.68)
The posterior probability p(st
= j|O, ) is calculated according to the forward-
backward algorithm. As shown in Eq. (3.63), the joint distribution of observation
sequence O and state st = j at time t is calculated as a product of the forward variable,
which accounts for partial observations {o1, · · · , ot} and state j at time t, and the back-
ward variable, which accounts for the remainder of observations {ot+1, · · · , oT} given
state j at time t with the normalization factor. However, Eq. (3.68) may not determine
a valid optimal state sequence since the probability of occurrence of the sequences of
states is not considered.
To cope with this problem, we need to ﬁnd the single best state sequence by using
Eq. (3.64) or an equivalent form of (3.65), directly. A dynamic programming method,
called the Viterbi algorithm (Viterbi 1967), was proposed to efﬁciently ﬁnd the single
best state sequence. To do so, we deﬁne the highest probability along a single path, at
time t, which accounts for the ﬁrst t observations and ends in state j using a new notation:
δt(j) ≜
max
s1,··· ,st−1 p(s1, · · · , st = j, o1, · · · , ot|).
(3.69)
By induction, a recursive formula of δt+1(j) from δt(j) is derived to calculate this proba-
bility. To derive the equation, we ﬁrst focus on the joint distribution appearing in δt+1(j),
which can be rewritten when st = i and st+1 = j as:
p(s1, · · · , i, j, o1, · · · , ot, ot+1|)
= p(s1, · · · , i, o1, · · · , ot|)p(j, ot+1|s1, · · · , i, o1, · · · , ot, )
= p(s1, · · · , i, o1, · · · , ot|)p(j|i, )p(ot+1|j, )
= p(s1, · · · , i, o1, · · · , ot|)aijbj(ot+1).
(3.70)
Here we use the conditional independence of the HMM from the second to the third
lines. Thus, by using Eq. (3.69), δt+1(j) is computed recursively from δt+1(i) as:
δt+1(j) = max
s1,··· ,i p(s1, · · · , i, o1, · · · , ot|)aijbj(ot+1)
=

max
i
δt(i)aij

bj(ot+1).
(3.71)
We need to keep track of the state that maximized Eq. (3.71) so as to backtrack to the
single best state sequence in the following Viterbi algorithm:
• Initialization
δ1(i) = πibi(o1),
ψ1(i) = 0,
1 ≤i ≤J.
(3.72)
• Recursion
δt(j) =

max
1≤i≤J δt−1(i)aij

· bj(ot+1),

76
Statistical models in speech and language processing
ψt(j) =

arg max
1≤i≤J δt−1(i)aij

,
2 ≤t ≤T
1 ≤j ≤J.
(3.73)
• Termination
ˆp = max
1≤j≤J δT(i),
ˆsT = arg max
1≤j≤J δT(i).
(3.74)
• State sequence backtracking
ˆst = ψt+1(ˆst+1),
t = T −1, T −2, · · · , 1.
(3.75)
In the termination step, the segmental likelihood function ˆp is calculated and is equiv-
alent to Eq. (3.67). It is noteworthy that this Viterbi algorithm is similar to the forward
procedure in the forward–backward algorithm. The key difference is the maximization
in Eq. (3.73) over previous states, which is used in place of a summation operation
in Eq. (3.53). The variable δt(j) in the Viterbi algorithm is meaningfully related to the
forward variable αt(j) in the forward–backward algorithm.
Now, we summarize what we can compute from the HMM without taking on the
combinatorial explosion problem. These values are used in the decoding step and the
training step of estimating model parameters , which is discussed in the next section.
• p(O|):
The marginalized likelihood function from the forward or backward algorithm.
• γt(j) ≜p(st = j|O, ):
The posterior probability of st = j from the forward–backward algorithm.
• ˆS = arg maxS p(S|O, ) = arg maxS p(S, O|):
The optimal state sequence from the Viterbi algorithm.
• p(ˆS, O|):
The segmental joint likelihood function from the Viterbi algorithm.
3.4
Maximum likelihood estimation and EM algorithm
Previous sections discuss the HMM-based speech modeling given model parameters 
to compute the likelihood values and so on, efﬁciently based on the forward, backward,
and Viterbi algorithms. One of the powerful properties of the HMM is that it also pro-
vides an efﬁcient algorithm to obtain the model parameters based on the ML estimation
from training data. The training algorithm is based on the EM algorithm, which can
tackle the incomplete data problem in ML estimation. In what follows, we address how
the EM algorithm is derived by applying Jensen’s inequality and show the procedure of
EM steps for estimation of HMM parameters, which involves latent variables in prob-
abilistic functions (Nakagawa 1988, Huang et al. 1990, Rabiner & Juang 1993, Bilmes
1998).

3.4 Maximum likelihood estimation and EM algorithm
77
3.4.1
Jensen’s inequality
We start the discussion of HMM parameter estimation based on the Maximum Like-
lihood (ML) criterion, so that the optimal model parameters ˆ are computed by the
likelihood function p(O|):2
ˆ = arg max
 p(O|).
(3.76)
We assume that speech data O are observed and are generated by some distribution.
However, as we discussed in Section 3.2, the HMM has additional hidden variables S
(HMM state sequence) and V (GMM component sequence), which are not observed.
In this situation where the model includes unobserved variables, O is called incomplete
data (S and V). ( would also be included as unobserved variables in a broad sense, but
this book considers latent variables as unobserved variables.). Conversely, the complete
data Y are composed of the observed data O as well as the hidden variables {S, V}.
In general, the maximum likelihood estimation of the model parameter  for
complete data O is difﬁcult, since we need to consider the following equation:
ˆ = arg max


S,V
p(O, S, V|).
(3.77)
As we discussed in Section 3.3, the summation over all possible S and V has to overcome
the combinatorial explosion problem, and the direct optimization of  for this equation
is not feasible. This problem is called the incomplete data problem.
According to the EM algorithm, the incomplete data problem in the ML estimation is
resolved by iteratively and alternatively performing the expectation step (E-step) and the
maximization step (M-step), which results in obtaining the local optimum solution of the
model parameters. In the E-step, we calculate an auxiliary function Q(′|), which is
an expectation of the logarithm of the likelihood function using new HMM parameters
′ given the current parameters , i.e.
Q(′|) = E(S,V)[log p(O, S, V|′)|O, ]
=

S

V
p(S, V|O, ) log p(O, S, V|′).
(3.78)
In the M-step, we maximize the auxiliary function instead of Eq. (3.77) with respect to
the HMM parameters ′ and estimate new parameters by
ˆ′ = arg max
′ Q(′|).
(3.79)
The updated HMM parameters ˆ′ are then treated as the current parameters for the
next iteration of EM steps. This iterative estimation only obtains local optimum solu-
tions, and not global optimum solutions. However, a careful setting of the initial model
parameters would help the solution to reach appropriate parameter values, and more-
over, the algorithm theoretically guarantees that the likelihood value is always increased
as the number of iterations increases. This property is very useful in the implementation,
2 The optimization with respect to the posterior distribution p(|O) is more reasonable in the Bayesian
sense, and it will be discussed in the following chapters.

78
Statistical models in speech and language processing
since we can easily debug training source codes based on the EM algorithm by checking
likelihood values.
Now, we prove how this indirect optimization for the auxiliary function Q(′|)
increases a likelihood value. For this proof, we ﬁrst deﬁne the following logarithmic
marginal function L(′):
L(′) ≜log p(O|′).
(3.80)
Note that since the logarithmic function is a monotonically increasing function, we can
check L(′) instead of p(O|′) for the proof. We introduce the notation of complete
data:
Y ≜{O, S, V}.
(3.81)
Then, the conditional distribution of Y given incomplete data O and parameters  has
the following relationship from the product rule:
p(Y|O, ′) = p(Y, O|′)
p(O|′)
= p(Y|′)
p(O|′),
(3.82)
where the likelihood function p(Y, O|′) is rewritten as p(Y|′), since Y includes O.
Therefore, by taking the logarithmic operation for both sides in Eq. (3.82), we derive
the equation for L(′) as
L(′) = log p(Y|′) −log p(Y|O, ′).
(3.83)
Then, by taking the expectation operation with respect to the posterior distribution of
latent variables p(S, V|O, ) for Eq. (3.83), Eq. (3.83) is represented as
L(′) = E(S,V)[log p(Y|′)|O, ] −E(S,V)[log p(Y|O, ′)|O, ]
= Q(′|) −H(′|),
(3.84)
where H(′|) is deﬁned as follows:
H(′|) ≜E(Y)[log p(Y|O, ′)|O, ].
(3.85)
Q(′|) is deﬁned in Eq. (3.78). Then, we obtain this relation from Eq. (3.85):
Q(′|) = L(′) + H(′|).
(3.86)
Here, Jensen’s inequality is introduced to prove the inequality (Dempster et al. 1976),
H(′|) ≤H(|).
(3.87)
• Jensen’s inequality
If x is a random variable with mean value E(x)[x] = μ and f(x) is a convex function,
then
E(x)[f(x)] ≥f[E(x)(x)],
(3.88)
with equality if and only if x is a degenerate distribution at μ.

3.4 Maximum likelihood estimation and EM algorithm
79
The inequality in Eq. (3.87) is derived by
H(|) −H(′|) = E(Y)

log p(Y|O, )
p(Y|O, ′)
---- O, 

=

p(Y|O, )

−log p(Y|O, ′)
p(Y|O, )

dY
= KL(p(Y|O, )∥p(Y|O, ′))
≥−log

p(Y|O, )p(Y|O, ′)
p(Y|O, ) dY

= 0,
(3.89)
where a convex function based on the negative logarithm f(x) = −log(x) is adopted.
As shown in Eq. (3.89), the difference H(|) −H(′|) is obtained as the relative
entropy or Kullback–Leibler (KL) divergence (Kullback & Leibler 1951) between the
distributions p(Y|O, ) and p(Y|O, ′), that is
KL(p(Y|O, )∥p(Y|O, ′)) ≜

p(Y|O, )

−log p(Y|O, ′)
p(Y|O, )

dY.
(3.90)
Given Eqs. (3.87) and (3.86), it is straightforward to see that if ′ satisﬁes
Q(′|) ≥Q(|),
(3.91)
then we can prove that
L(′) = Q(′|) −H(′|) ≥Q(|) −H(|) = L().
(3.92)
Since L() = log p(O|) and the logarithmic function is a monotonic function,
Q(′|) ≥Q(|) ⇒p(O|′) ≥p(O|).
(3.93)
Since ˆ′ = arg max′ Q(′|) always satisﬁes the inequality Eq. (3.91), we prove
that the parameter estimated by the EM procedure, ˆ′, always increases the likelihood
value as:
ˆ′ = arg max
′ Q(′|) ⇒p(O|′) ≥p(O|).
(3.94)
Such an EM procedure is bound to monotonically increase the auxiliary function
Q(′|) as well as the original likelihood function p(O|′). Note that since it is not a
direct optimization of the original likelihood function, the optimization of the auxiliary
function leads to a local optimum solution to the ML parameter estimation.
3.4.2
Expectation step
To ﬁnd ML estimates of HMM parameters, we expand the auxiliary function in
Eq. (3.78) and rewrite it by substituting the joint distribution of complete data likelihood
(Eq. (3.49)) into Eq. (3.78) as

80
Statistical models in speech and language processing
Q(′|) = E(S,V)[log p(O, S, V|′)|O, ]
=

S,V
p(S, V|O, )

log π′
s1 +
 T

t=2
log a′
st−1st

+
 T

t=1
log ω′
stvt + log N(ot|μ′
stvt, ′
stvt)

.
(3.95)
Note that four terms depend on the initial weight πj, state transition probability aij,
mixture weight wjk, and Gaussian parameters {μjk, jk}, respectively. We provide the
solution for each term:
• Q(π′|π)
We ﬁrst focus on the ﬁrst term depending on the initial weight πj, and deﬁne the
following auxiliary function for πj:
Q(π′|π) ≜

S,V
p(S, V|O, ) log π′
s1.
(3.96)
Since π′
s1 only depends on s1, we obtain the following equation that marginalizes
p(S, V|O, ) over S\s1 = {s2, · · · , sT} and V as:

S\s1,V
p(S, V|O, ) = p(s1|O, ).
(3.97)
Therefore, Q(π′|π) can be rewritten as
Q(π′|π) =

s1
p(s1|O, ) log π′
s1
=
J

j=1
p(s1 = j|O, ) log π′
j
=
J

j=1
γ1(j) log π′
j,
(3.98)
where γ1(j) is an occupation probability deﬁned in Eq. (3.60) as:
γ1(j) ≜p(s1 = j|O, ).
(3.99)
This is computed from the forward–backward algorithm.
• Q(A′|A)
Next, we focus on the second term in Eq. (3.95), which depends on the state transition
probability aij, and deﬁne the following auxiliary function for aij:
Q(A′|A) ≜

S,V
p(S, V|O, )
T−1

t=1
log a′
stst+1.
(3.100)

3.4 Maximum likelihood estimation and EM algorithm
81
Here we replace the summation from T
t=2 log a′
st−1st to T−1
t=1 log a′
stst+1 for nota-
tional convention. Similar to Q(π′|π), we obtain

S\st,st+1,V
p(S, V|O, ) = p(st, st+1|O, ).
(3.101)
Therefore, by replacing the summation over t with the summation over S, V, we obtain
Q(A′|A) =
T−1

t=1

S,V
p(S, V|O, ) log a′
stst+1
=
T−1

t=1

st,st+1
p(st, st+1|O, ) log a′
stst+1
=
T−1

t=1
J

i=1
J

j=1
p(st = i, st+1 = j|O, ) log a′
ij
=
T−1

t=1
J

i=1
J

j=1
ξt(i, j) log a′
ij,
(3.102)
where ξt(i, j) is an expected transition probability from st = i to st+1 = j, and is
deﬁned as:
ξt(i, j) ≜p(st = i, st+1 = j|O, ).
(3.103)
Note that by using this technique, the summation over all possible sequences (that
leads to a combinatorial explosion) can be replaced with a summation over the num-
ber of HMM states and the number of frames. Thus, this auxiliary function can be
computed feasibly. For this computation, we need to obtain ξt(i, j), which is discussed
later as a variant of the forward–backward algorithm.
• Q(ω′|ω)
The third term in Eq. (3.95) depends on the mixture weight ωjk, and so we deﬁne the
following auxiliary function for ωjk:
Q(ω′|ω) ≜

S,V
p(S, V|O, )
T

t=1
log ω′
stvt.
(3.104)
Similarly to the case of aij, we ﬁrst obtain the following equation:

S\st,V\vt
p(S, V|O, ) = p(st, vt|O, ).
(3.105)
Therefore,
Q(ω′|ω) =
T

t=1

S,V
p(S, V|O, ) log ω′
stvt
=
T

t=1

st,vt
p(st, vt|O, ) log ω′
stvt

82
Statistical models in speech and language processing
=
T

t=1
J

j=1
K

k=1
p(st = j, vt = k|O, ) log ω′
jk
=
T

t=1
J

j=1
K

k=1
γt(j, k) log ω′
jk,
(3.106)
where γt(j, k) is an expected occupation probability at st = j and vt = k, and is
deﬁned as:
γt(j, k) ≜p(st = j, vt = k|O, ).
(3.107)
The computation of γt(j, k) is also discussed later as a variant of the forward–
backward algorithm.
• Q(μ′, ′|μ, )
Finally, the fourth term in Eq. (3.95) depends on the Gaussian parameters μjk and
jk, and deﬁnes the following auxiliary function for μjk and jk:
Q(μ′, ′|μ, ) ≜

S,V
p(S, V|O, )
T

t=1
log N(ot|μ′
stvt, ′
stvt).
(3.108)
Similarly to Q(ω′|ω), by using Eq. (3.105), Q(μ′, ′|μ, ) can be rewritten with
γt(j, k) as
Q(μ′, ′|μ, ) =
T

t=1
J

j=1
K

k=1
γt(j, k) log N(ot|μ′
jk, ′
jk).
(3.109)
By using the deﬁnition of the multivariate Gaussian distribution in Appendix C.6:
N(x|μ, ) ≜(2π)−D
2 ||−1
2 exp

−1
2(x −μ)⊺−1(x −μ)

.
(3.110)
Equation (3.109) can be rewritten as
Q(μ′, ′|μ, )
=
T

t=1
J

j=1
K

k=1
γt(j, k) log
(
(2π)−D
2 |′
jk|−1
2
× exp

−1
2(ot −μ′
jk)⊺(′
jk)−1(ot −μ′
jk)

∝
T

t=1
J

j=1
K

k=1
−γt(j, k)
2
(
log
(
|′
jk|
)
+ (ot −μ′
jk)⊺(′
jk)−1(ot −μ′
jk)
)
, (3.111)
where ∝denotes the proportional relationship in the logarithmic domain. That is, the
normalization factor in the linear domain is changed for the normalization constant
in the logarithmic domain, and we shall continue to use ∝in this book. Therefore,
the normalization constant term that does not depend on μ,  is omitted from this
expression.

3.4 Maximum likelihood estimation and EM algorithm
83
Thus, we summarize the auxiliary function Q(′|):
Q(′|) = Q(π′|π) + Q(A′|A) + Q(ω′|ω) + Q(μ′, ′|μ, ),
(3.112)
where each term is deﬁned as follows:
Q(π′|π) =
J

j=1
γ1(j) log π′
j,
(3.113)
Q(A′|A) =
T−1

t=1
J

i=1
J

j=1
ξt(i, j) log a′
ij,
(3.114)
Q(ω′|ω) =
T

t=1
J

j=1
K

k=1
γt(j, k) log ω′
jk,
(3.115)
Q(μ′, ′|μ, ) ∝
T

t=1
J

j=1
K

k=1
−γt(j, k)
2
(
log |′
jk|
+(ot −μ′
jk)⊺(′
jk)−1(ot −μ′
jk)
)
.
(3.116)
As an equivalent form of Eq. (3.116), we also write the following auxiliary func-
tion Q(μ′, R′|μ, R), which replaces the covariance matrix  with the precision matrix
R = −1 as:
Q(μ′, R′|μ, R) ∝
T

t=1
J

j=1
K

k=1
−γt(j, k)
2
×
(
−log |R′
jk| + (ot −μ′
jk)⊺R′
jk(ot −μ′
jk)
)
.
(3.117)
This equivalent representation is used to make the computation simple in the follow-
ing sections. Note that Eqs. (3.113)–(3.117) are represented by the following posterior
distributions:
ξt(i, j) = p(st = i, st+1 = j|O, ),
(3.118)
γt(j, k) = p(st = j, vt = k|O, ).
(3.119)
Similarly to γt(j) = p(st = j|O, ), as discussed in Section 3.3.1, these values are also
computed efﬁciently by using the forward–backward algorithm.
First, γt(j) has the following relationships with γt(j, k) and ξt(i, j):
γt(j) ≜p(st = j|O, ) =
K

k=1
γt(j, k)
=
J

i=1
ξt(i, j).
(3.120)
The posterior probability γt(j) can be calculated by using Eq. (3.63) based on the forward
variables and backward variables {αt(j), βt(j)}, but this can also be computed from ξt(i, j)
or γt(j, k).

84
Statistical models in speech and language processing
The posterior probability γt(j, k) of occupying state j and Gaussian component k in
Eq. (3.119) can be computed by the forward–backward algorithm. By using the sum
and product rules, γt(j, k) is represented by the posterior distribution p(st = j|O, ) and
joint likelihood function of p(O, vt = k|st = j, ) as:
γt(j, k) = p(st = j|O, )p(vt = k|st = j, O, )
= p(st = j|O, )
p(O, vt = k|st = j, )
K
k′=1 p(O, vt = k′|st = j, )
.
(3.121)
By using Eq. (3.33) for p(O, vt = k|st = j, ) and Eq. (3.63) for p(st = j|O, ),
Eq. (3.121) is represented as follows:
γt(j, k) =
αt(j)βt(j)
J
j′=1 αt(j′)βt(j′)
·
ωjkN(ot|μjk, jk)
K
k′=1 ωjk′N(ot|μjk′, jk′)
.
(3.122)
In a similar manner, we express the posterior probability ξt(i, j) in Eq. (3.118) by
ξt(i, j) =
p(st = i, st+1 = j, O|)
J
i′=1
J
j′=1 p(st = i′, st+1 = j′, O|)
.
(3.123)
Now we focus on the joint likelihood function p(st = i, st+1 = j, O|), which is
factorized by
p(st = i, st+1 = j, O|)
= p(O|st = i, st+1 = j, )p(st = i, st+1 = j|)
= p(O|st = i, st+1 = j, )p(st+1 = j|st = i, )p(st = i|).
(3.124)
By using the conditional independence assumption of the HMM, we can rewrite the
equation as:
p(st = i, st+1 = j, O|)
= p(o1, · · · , ot|st = i, )p(ot+1, · · · , oT|st = i, st+1 = j, )
× p(st+1 = j|st = i, )p(st = i|)
= p(o1, · · · , ot|st = i, )p(ot+1|st+1 = j, )p(ot+2, · · · , oT|st+1 = j, )
× p(st+1 = j|st = i, )p(st = i|)
= p(o1, · · · , ot, st = i, )



=αt(i)
p(ot+1|st+1 = j, )



=bj(ot+1)
p(ot+2, · · · , oT|st+1 = j, )



=βt+1(j)
× p(st+1 = j|st = i, )



=aij
.
(3.125)
Thus, by using the forward variable αt(i) in Eq. (3.50) and the backward variable βt+1(j)
in Eq. (3.55), Eq. (3.123) is ﬁnally written as

3.4 Maximum likelihood estimation and EM algorithm
85
Figure 3.7
Calculation of posterior probability ξt(i, j) based on forward variable αt(i) and backward variable
βt+1(j). Adapted from Rabiner & Juang (1993).
ξt(i, j) =
αt(i)aijbj(ot+1)βt+1(j)
J
i′=1
J
j′=1 αt(i′)ai′j′bj′(ot+1)βt+1(j′)
=
αt(i)aij
(K
k=1 ωjkN(ot+1|μjk, jk)
)
βt+1(j)
J
i′=1
J
j′=1 αt(i′)ai′j′
(K
k=1 ωj′kN(ot+1|μj′k, j′k)
)
βt+1(j′)
.
(3.126)
Figure 3.7 illustrates how the posterior probability ξt(i, j) of visiting states i and j in
consecutive time frames is calculated by applying the forward–backward algorithm.
In Eq. (3.63), Eq. (3.122) and Eq. (3.123), the posterior probabilities γt(j), γt(j, k), and
ξt(i, j) are calculated, respectively, through a kind of soft computation, i.e., the assign-
ment information is represented by the probabilistic values for the elements i, j, and
k. Instead of this soft computation, a simple and efﬁcient approximation is to ﬁnd the
segmental ML estimates based on a hard computation where only the single best state
sequence ˆS = {ˆst} and mixture component sequence ˆV = {ˆvt} are considered with 0 (not
assigned) or 1 (assigned) values for the elements i, j, and k. This computation complexity
is signiﬁcantly reduced. For this consideration, the calculation of posterior probabilities
is simpliﬁed as
γt(j) = δ(ˆst, j),
(3.127)
γt(j, k) = δ(ˆst, j)δ(ˆvt, k),
(3.128)
ξt(i, j) = δ(ˆst, i)δ(ˆst+1, j),
(3.129)
where δ(a, a′) denotes a Kronecker delta function that returns 1 when a = a′ and 0
otherwise. These probabilities are 1 for the cases of the best states i, j, and Gaussians
k, and 0 for all of the other cases. Note that ˆst is computed by using the Viterbi algo-
rithm that maximizes the following segmental joint likelihood function as discussed in
Section 3.3.2:
ˆS = {ˆs1, · · · , ˆsT} = arg max
S
p(S, O|).
(3.130)
The value of ˆvt, given HMM state j, is computed by
ˆvt = arg max
k
ωjkN(ot|μjk, jk).
(3.131)
The E-step in the EM algorithm is completed by calculating these posterior probabilities.

86
Statistical models in speech and language processing
3.4.3
Maximization step
In the maximization step, we aim to maximize Q(π′|π), Q(A′|A), Q(ω′|ω), and
Q(μ′, ′|μ, ) with respect to π′, A′, ω′, and {μ′, ′} so as to estimate ML parameters
π′, A′, ω′, and {μ′, ′}, respectively. However, the constraints of probability parameters
J

j=1
π′
j = 1,
J

j=1
a′
ij = 1, ∀i,
K

k=1
ω′
jk = 1, ∀j,
(3.132)
have to be imposed in the constrained optimization problem. For example, when con-
sidering the estimation of initial state probabilities π′ = {π′
j}, we construct a Lagrange
function (or Lagrangian):
/Q(π′|π) =
J

j=1
γ1(j) log π′
j + η
⎛
⎝
J

j=1
π′
j −1
⎞
⎠,
(3.133)
by combining the original auxiliary function in Eq. (3.113) and the constraint in
Eq. (3.132) with an additional Lagrange multiplier η as a scaling factor. Then we differ-
entiate this Lagrangian with respect to individual probability parameter π′
j and set it to
zero to obtain
∂/Q(π′|π)
∂π′
j
= γ1(j) 1
π′
j
+ η = 0
⇒
ˆπ′
j = −1
ηγ1(j).
(3.134)
By substituting Eq. (3.134) into the constrains in Eq. (3.132), we obtain
J

j=1
π′
j =
J

j=1

−1
η

γ1(j) = 1
⇒
η = −
J

j=1
γ1(j).
(3.135)
The ML estimate of new initial state probability is derived by substituting Eq. (3.135)
into Eq. (3.134):
ˆπ′
j =
γ1(j)
J
j′=1 γ1(j′)
= γ1(j).
(3.136)
In the same manner, we derive the ML estimates of new state transition probability and
new mixture component probability, which are provided by
ˆa′
ij =
T−1
t=1 ξt(i, j)
T−1
t=1
J
i′=1 ξt(i′, j)
=
T−1
t=1 ξt(i, j)
T−1
t=1 γt(j)
,
(3.137)
ˆω′
jk =
T
t=1 γt(j, k)
T
t=1
K
k′=1 γt(j, k′)
=
T
t=1 γt(j, k)
T
t=1 γt(j)
.
(3.138)

3.4 Maximum likelihood estimation and EM algorithm
87
These results show that the initial state probability, state transition probability, and
mixture weight can be computed using the ratio of the occupation statistics.
On the other hand, when estimating new HMM mean vectors μ′ = {μ′
jk}, we individ-
ually differentiate Q(μ′, ′|μ, ) in Eq. (3.117) with respect to μ′
jk for each Gaussian
component k in state j and set it to zero:
∂Q(μ′, ′|μ, )
∂μ′
jk
=
∂
∂μ′
jk
T

t=1
J

j′=1
K

k′=1
−γt(j′, k′)
2
(
log
(
|′
j′k′|
)
+ (ot −μ′
j′k′)⊺(′
j′k′)−1(ot −μ′
j′k′)
)
=
∂
∂μ′
jk
T

t=1
−γt(j, k)
2
(ot −μ′
jk)⊺(′
jk)−1(ot −μ′
jk)
= (′
jk)−1
T

t=1
γt(j, k)(ot −μ′
jk) = 0,
(3.139)
where we use the following vector derivative rule in Eq. (B.9):
∂a⊺b
∂a
= ∂b⊺a
∂a
= b.
(3.140)
Therefore, the ML estimate of the HMM mean vector is derived as shown by
T

t=1
γt(j, k)(ot −μ′
jk) = 0
⇒
ˆμ′
jk =
T
t=1 γt(j, k)ot
T
t=1 γt(j, k)
.
(3.141)
Note that the mean vector is represented as the ﬁrst-order expected value of ot by using
the occupation probability of γt(j, k).
At the same time, the new HMM covariance matrices ′ = {′
jk} or their inverse
matrices (′)−1 = {(′
jk)−1 ≜R′
jk} (also called the precision matrices) are estimated
by differentiation of Q(μ′, R′|μ, R) in Eq. (3.117) with respect to R′
jk for each Gaussian
k at each state j and setting it to zero:
∂Q(μ′, R′|μ, R)
∂R′
jk
=
∂
∂R′
jk
T

t=1
−γt(j, k)
2
(
−log
(
|R′
jk|
)
+ (ot −μ′
jk)⊺R′
jk(ot −μ′
jk)
)
= 1
2
T

t=1
γt(j, k)(R′
jk)−1 −1
2
T

t=1
γt(j, k)(ot −μ′
jk)(ot −μ′
jk)⊺= 0,
(3.142)

88
Statistical models in speech and language processing
where we use the following matrix derivative rule in Eqs. (B.8) and (B.10):
∂log |A|
∂A
= A−1,
(3.143)
∂a⊺Cb
∂C
= ab⊺.
(3.144)
Thus, the new estimates of the HMM precision and covariance matrices are derived by
using the ML estimate ˆμ′
jk for μ′
jk in Eq. (3.142) as
ˆR′
jk =
T
t=1 γt(j, k)(ot −ˆμ′
jk)(ot −ˆμ′
jk)⊺
T
t=1 γt(j, k)
−1
,
ˆ
′
jk =
T
t=1 γt(j, k)(ot −ˆμ′
jk)(ot −ˆμ′
jk)⊺
T
t=1 γt(j, k)
.
(3.145)
Interestingly, the calculation of the derived covariance matrices  ={jk} in Eq. (3.145)
is interpreted as the weighted ensemble expectation and covariance matrices, as well
as the calculation of the ML mean vectors μ = {μjk} in Eq. (3.141). The occupation
probability γt(j, k) of state j and mixture component k at time t is treated as the weighting
factor in calculation of the weighted expectation function. Note that Eq. (3.145) is
ˆ
′
jk =
T
t=1 γt(j, k)oto⊺
t
T
t=1 γt(j, k)
−2
T
t=1 γt(j, k)ot( ˆμ′
jk)⊺
T
t=1 γt(j, k)
+ ˆμ′
jk( ˆμ′
jk)⊺
=
T
t=1 γt(j, k)oto⊺
t
T
t=1 γt(j, k)
−2 ˆμ′
jk( ˆμ′
jk)⊺+ ˆμ′
jk( ˆμ′
jk)⊺
=
T
t=1 γt(j, k)oto⊺
t
T
t=1 γt(j, k)
−ˆμ′
jk( ˆμ′
jk)⊺.
(3.146)
Thus, the ML estimate of the covariance matrix is computed from the second-order
statistic and the ML estimate of the mean vector.
Now, we summarize the ML estimates of the CDHMM as follows:
ˆπ′
j = γ1(j),
(3.147)
ˆa′
ij =
T−1
t=1 ξt(i, j)
T−1
t=1
J
j′=1 ξt(i, j′)
,
(3.148)
ˆω′
jk =
T
t=1 γt(j, k)
T
t=1
K
k′=1 γt(j, k′)
,
(3.149)
ˆμ′
jk =
T
t=1 γt(j, k)ot
T
t=1 γt(j, k)
,
(3.150)
ˆ
′
jk =
T
t=1 γt(j, k)oto⊺
t
T
t=1 γt(j, k)
−ˆμ′
jk( ˆμ′
jk)⊺.
(3.151)

3.4 Maximum likelihood estimation and EM algorithm
89
If we consider the diagonal covariance matrix, the dth diagonal element of Eq. (3.151)
is modiﬁed as follows:
ˆ′
jkd =
T
t=1 γt(j, k)o2
td
T
t=1 γt(j, k)
−( ˆμ′
jkd)2.
(3.152)
To compute these estimated values, we need to compute the following values:
ξ(i, j) ≜
T−1

t=1
ξt(i, j)
γ (j, k) ≜
T

t=1
γt(j, k)
γ jk ≜
T

t=1
γt(j, k)ot
jk ≜
T

t=1
γt(j, k)oto⊺
t .
(3.153)
These statistics are sufﬁcient to compute the parameters, and are called sufﬁcient statis-
tics of the CDHMM. In particular, γ (j, k), γ jk, and jk are called the 0th, 1st, and
2nd order sufﬁcient statistics of the Gaussian at HMM state j and mixture compo-
nent k, respectively. The sufﬁcient statistic is ﬁrst mentioned in Section 2.1.3, where
the estimation problems are rather simple, and they do not include latent variables. In
the latent models, the probabilistic assignment information of the occupation probabil-
ities γ1(j), γt(j, k), ξt(i, j) is important to obtain the sufﬁcient statistics. These statistics,
composed of the occupation probabilities γ1(j), γt(j, k), ξt(i, j), are computed from the
forward–backward algorithm, as discussed in the expectation step (Section 3.4.2).
Based on these sufﬁcient statistics, we can rewrite the auxiliary function as follows:
Q(π′|π) =
J

j=1
γ1(j) log π′
j,
(3.154)
Q(A′|A) =
J

i=1
J

j=1
ξ(i, j) log a′
ij,
(3.155)
Q(ω′|ω) =
J

j=1
K

k=1
γ (j, k) log ω′
jk,
(3.156)
Q(μ′, ′|μ, ) =
J

j=1
K

k=1
−γ (j, k)
2
(
D log(2π) + log |′
jk| + μ′
jk
⊺(′
jk)−1μ′
jk)
)
+ μ′
jk
⊺(′
jk)−1γ jk −1
2tr
+
(′
jk)−1jk
,
.
(3.157)
These forms are used in the analytical discussions in later chapters.

90
Statistical models in speech and language processing
This total EM algorithm for iteratively estimating the HMM parameters is called the
Baum–Welch algorithm (Baum, Petrie, Soules et al. 1970), and is based on the forward-
backward algorithm, the accumulation of the sufﬁcient statistics, and the update of
the HMM parameters, as shown in Algorithm 4. The Baum–Welch algorithm can be
extended based on the Bayesian approach (see Section 4.3 and Section 7.3).
Algorithm 4 Baum–Welch algorithm
Require:  ←init
1: repeat
2:
Compute the forward variable αt(j) from the forward algorithm
3:
Compute the backward variable βt(j) from the backward algorithm
4:
Compute the occupation probabilities γ1(j), γt(j, k), and ξt(i, j)
5:
Accumulate the sufﬁcient statistics ξ(i, j), γ (j, k), γ jk, and jk
6:
Estimate the new HMM parameters ˆ′
7:
Update the HMM parameters  ←ˆ′
8: until Convergence
In the implementation of Viterbi or segmental ML estimation, we apply the current
HMM parameter estimates  = {πj, aij, ωjk, μjk, jk} and the Viterbi algorithm to per-
form Viterbi decoding to ﬁnd the state alignment. Given the best state sequence ˆS and
mixture component sequence ˆV, new ML estimates ′ = {μ′
jk, ′
jk} are computed as the
ensemble mean vector and covariance matrix, where T
t=1 γt(j, k) is seen as the count
Njk of training samples O = {ot} which are aligned in state j and Gaussian compo-
nent k (Juang & Rabiner 1990). This method of using the Viterbi training instead of the
forward–backward algorithm to obtain (a part of) occupation probabilities in the training
step is called segmental K-means training or Viterbi training.
We should note that to make the Baum–Welch algorithm work in real speech data, we
need to consider some heuristics in the ML EM algorithm. For example, how to provide
initial values is always an important question, and it is usually resolved by using a simple
algorithm (e.g., K-means clustering or random initialization). The other important issue
comes from the data sparseness, e.g., some mixture components or hidden states can-
not have sufﬁcient data assigned in the Viterbi or forward–backward algorithm, which
makes the parameter estimation (especially covariance parameter estimation) unstable.
This can be heuristically avoided by setting a threshold to update these parameters, or
setting minimum threshold values for covariance parameters. These problems can be
solved by the Bayesian approaches.
Despite the above problems, the Baum–Welch algorithm based on the EM algorithm
is widely used in current CDHMM training. This Baum–Welch algorithm has several
advantages. For example, the algorithm is unique in that the computational cost of the E
step is much more than that of the M-step, since the E step computational cost depends
on the training data size. However, the E-step can be parallelized with many computers
by distributing a chunk of data to a computer and computing a sufﬁcient statistic of
the chunk independently of the other computers. Therefore, the Baum–Welch algorithm

3.5 Maximum likelihood linear regression for hidden Markov model
91
has a very nice data scalability, which enables it to use a large amount of training data.
In addition, within the algorithm, the likelihood always increases by the EM theory.
Therefore, by monitoring the likelihood values, we can easily detect errors and bugs
during a training procedure.
3.5
Maximum likelihood linear regression for hidden Markov model
As we discussed in the previous section, CDHMM parameters can be estimated by sta-
tistical ML methods, the effectiveness of which depends on the quality and quantity of
available data that should distribute according to the statistical features of the intended
signal space or conditions. As there is no sure way of collecting sufﬁcient data to cover
all conditions, adaptive training of HMM parameters from a set of previously obtained
parameters to a new set that beﬁts a speciﬁc environment with a small amount of new
data is an important research issue.
In speech recognition, one approach is to view the adaptation of model parameters to
new data (e.g., speaker adaptation) as a transformation problem; that is, the new set of
model parameters is a transformed version of the old set: n+1 = f(n, {o}n), where
{o}n denotes the new set of data available at moment n for the existing model parameters
n to adapt to. Most frequently and practically, the function f is chosen to be of an afﬁne
transformation type (Digalakis, Ritischev & Neumeyer 1995, Leggetter & Woodland
1995):
θn+1 = Aθn + b,
(3.158)
where various parts of the model parameters, e.g., the mean vectors or the variances, are
envisaged in a vector space. The adaptation algorithm therefore involves deriving the
afﬁne map components, A and b, from the adaptation data {o}n. A number of algorithms
have been proposed for this purpose (see Lee & Huo (2000), and Shinoda (2010) for
details).
The linear regression method for HMM parameters estimates the afﬁne transforma-
tion parameters from a set of adaptation data, usually limited in size. The transformation
with the estimated parameters is then applied to the previously trained HMMs, resulting
in the set of “adapted models.” Note that for automatic speech recognition, the number
of the Gaussian distributions or simply Gaussians, which are used as component distri-
butions in forming state-dependent mixture distributions, is typically in the thousands
or more. If each mean vector in the set of Gaussians is to be modiﬁed by a unique trans-
formation matrix, the number of “adaptation parameters” can be quite large. The main
problem of this method is thus how to improve “generalization capability” by avoiding
the over-training problem when the amount of adaptation data is small. To solve the
problem, we introduce a model selection approach.
The model selection approach was originally proposed within the estimation of lin-
ear transformation parameters by using the maximum likelihood EM algorithm, as
discussed in Section 3.4. The technique is called maximum likelihood linear regression
(MLLR). MLLR proposes to share one linear transformation in a cluster of many
Gaussians in the HMM set, thereby effectively reducing the number of free parameters
that can then be trained with a small amount of adaptation data. The Gaussian clusters

92
Statistical models in speech and language processing
Figure 3.8
Gaussian tree representation of linear regression parameters.
are usually constructed as a tree structure, as shown in Figure 3.8, which is pre-
determined and ﬁxed throughout adaptation. This tree (called a regression tree) is
constructed based on a centroid splitting algorithm, described in Young, Evermann,
Gales et al. (2006). This algorithm ﬁrst makes two centroid vectors from a random
perturbation of the global mean vector computed from Gaussians assigned to a target
leaf node. Then it splits a set of these Gaussians according to the Euclidean distance
between Gaussian mean vectors and two centroid vectors. The two sets of Gaussians
obtained are assigned to child nodes, and this procedure is continued to ﬁnally build a
tree.
The utility of the tree structure is commensurate with the amount of adaptation data;
namely, if we have a small amount of data, it uses only coarse clusters (e.g., the root
node of a tree in the top layer of Figure 3.8) where the number of free parameters in the
linear transformation matrices is small. On the other hand, if we have a sufﬁciently large
amount of data, it can use ﬁne clusters where the number of free parameters in the linear
transformation matrices is large, potentially improving the precision of the estimated
parameters. This framework needs to select appropriate Gaussian clusters according to
the amount of data, i.e., it needs an appropriate model selection function. Usually, model
selection is performed by setting a threshold value manually (e.g., the total number of
speech frames assigned to a set of Gaussians in a node).
3.5.1
Linear regression for hidden Markov models
This section brieﬂy explains a solution for the linear regression parameters for HMMs
within a maximum likelihood EM algorithm framework. It uses a solution based on a

3.5 Maximum likelihood linear regression for hidden Markov model
93
variance normalized representation of Gaussian mean vectors to simplify the solution.3
In this section, we only focus on the transformation of Gaussian mean vectors in
CDHMMs.
First, we review the basic EM algorithm of the conventional HMM parameter estima-
tion, as discussed in Section 3.4, to set the notational convention and to align with the
subsequent development of the MLLR approach. Let O ≜{ot ∈RD|t = 1, · · · , T} be a
sequence of D dimensional feature vectors for T speech frames. The latent variables in
a continuous density HMM are composed of HMM states and mixture components of
GMMs. A sequence of HMM states is represented by S ≜{st|t = 1, · · · , T}, where the
value of st denotes an HMM state index at frame t. Similarly, a sequence of mixture com-
ponents is represented by V ≜{vt|t = 1, · · · , T}, where the value of vt denotes a mixture
component index at frame t. As introduced in Eq. (3.78), the EM algorithm deals with
the following auxiliary function as an optimization function instead of directly using the
model likelihood:
Q(′|) = E(S,V)[log p(O, S, V|′)|O, ]
=

S

V
p(S, V|O, ) log p(O, S, V|′),
(3.159)
where  is a set of HMM parameters and p(O, S, V|) is a complete data likelihood
given . p(S, V|O, ) is the posterior distribution of the latent variables given the pre-
viously estimated HMM parameters . Equation (3.78) is an expected value, and is
efﬁciently computed by using the forward–backward algorithm as the E-step of the EM
algorithm, as we discussed in Section 3.3.
The M-step of the EM algorithm estimates HMM parameters, as follows:
ML = arg max
′ Q(′|).
(3.160)
The E-step and the M-step are performed iteratively until convergence, and ﬁnally we
obtain the HMM parameters as a close approximation of the stationary point solution.
Now we focus on the linear transformation parameters within the EM algorithm. We
prepare a transformation parameter matrix Wj ∈RD×(D+1) for each leaf node j in a
Gaussian tree. Here, we assume that the Gaussian tree is pruned by a model selection
approach as a model structure M, and the set of leaf nodes in the pruned tree is repre-
sented by JM. Hereinafter, we use Z to denote a joint event of S and V (i.e., Z ≜{S, V}).
This will much simplify the following development pertaining to the adaptation of the
mean and the covariance parameters. Similarly to Eq. (3.159), the auxiliary function
with respect to a set of transformation parameters 
JM = {Wj|j = 1, · · · , |JM|} can be
represented as follows:
Q(
′
JM|
JM) = E(Z)
+
log p(O, Z|
′
JM, )
,
=
K

k=1
T

t=1
γt(k) log N(ot|μad
k
′, k).
(3.161)
3 This is ﬁrst described in Gales & Woodland (1996) as normalized domain MLLR. The structural Bayes
approach Shinoda & Lee (2001) for bias vector estimation in HMM adaptation also uses this normalized
representation.

94
Statistical models in speech and language processing
Here k denotes a unique mixture component index of all Gaussians in the target HMMs
(for all phoneme HMMs in a speech recognition case), and K is the total number of
Gaussians. γt(k) ≜p(vt = k|O; , 
JM) is the posterior probability of mixture com-
ponent k at t, derived from the previously estimated transformation parameters 
JM.4
Expression μad
k is a transformed mean vector with 
JM, and the concrete form of this
vector is discussed in the next paragraph. In the Q function, we disregard the parameters
of the state transition probabilities and the mixture weights, since they do not depend
on the optimization with respect to 
JM. Expression N(·|μ, ) denotes a Gaussian dis-
tribution with mean parameter μ and covariance matrix parameter , and is deﬁned in
Appendix C.6 as follows:
N(ot|μad
k , k) ≜CN (k) exp

−1
2tr
+
(k)−1(ot −μad
k )(ot −μad
k )⊺,
.
(3.162)
We use the trace based representation. Factor CN (k) is a normalization factor, and is
deﬁned as follows:
CN (k) ≜(2π)−D
2 |k|−1
2 .
(3.163)
In the following paragraphs, we derive Eq. (3.161) as a function of 
JM to optimize

JM.
We consider the concrete form of the transformed mean vector μad
k
based on the
variance normalized representation. We ﬁrst deﬁne the Cholesky decomposition matrix
Ck as follows:
k ≜Ck(Ck)⊺,
(3.164)
where Ck is a D×D triangular matrix. If the Gaussian k is included in a set of Gaussians
Kj in leaf node j (i.e., k ∈Kj), the afﬁne transformation of a Gaussian mean vector in a
covariance normalized space (Ck)−1μad
k is represented as follows:
(Ck)−1μad
k = Wj

1
(Ck)−1μini
k

⇒μad
k = CkWj

1
(Ck)−1μini
k

≜CkWjξk,
(3.165)
where ξk is an augmented normalized vector of an initial (non-adapted) Gaussian mean
vector μini
k and j is a leaf node index that holds a set of Gaussians. Thus, transformation
parameter Wj is shared among a set of Gaussians Kj. The clustered structure of the
Gaussians is usually represented as a binary tree where a set of Gaussians belongs to
each node.
4
k denotes a combination of all possible HMM states and mixture components. In the common HMM
representation (e.g., in this chapter), k can be represented by these two indexes in Eq. (3.109).

3.5 Maximum likelihood linear regression for hidden Markov model
95
Now we focus on how to obtain the Q function of 
JM. By following the equations in
Example 2.3 with considering the occupation probability γt(k) in Eq. (2.39), Eq. (3.161)
is represented as follows:
Q(
′
JM|
JM)
=

j∈JM

k∈Kj
T

t=1
γt(k) log N(ot|μad
k
′, k)
=

j∈JM

k∈Kj

(μad
k
′)⊺(k)−1
T

t=1
γt(k)ot −1
2tr
&
(k)−1
T

t=1
γt(k)oto⊺
t
'
+
T

t=1
γt(k)

log CN (k) −1
2(μad
k
′)⊺(k)−1μad
k
′
=

j∈JM

k∈Kj

(μad
k
′)⊺(k)−1γ k −1
2tr
+
(k)−1k
,
+ γk

log CN (k) −1
2(μad
k
′)⊺(k)−1μad
k
′
,
(3.166)
where γk, γ k, and k are deﬁned as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
γk =
T

t=1
γt(k)
γ k =
T

t=1
γt(k)ot
k =
T

t=1
γt(k)oto⊺
t .
(3.167)
As introduced in Eq. (3.153), these are the 0th, 1st, and 2nd order sufﬁcient statistics
of Gaussians in HMMs, respectively. Then, the Q function of 
JM is represented by
substituting Eq. (3.165) into Eq. (3.166) as follows:
Q(
′
JM|
JM)
=

j∈JM

k∈Kj

(CkW′
jξk)⊺(k)−1γ k −1
2tr
+
(k)−1k
,
+ γk

log CN (k) −1
2(CkW′
jξk)⊺(k)−1CkW′
jξk

=

j∈JM

k∈Kj

tr
+
W′
j
⊺(Ck)−1γ kξ⊺
k
,
−1
2tr
+
(k)−1k
,
+ γk

log CN (k) −1
2tr
+
W′
j
⊺W′
jξkξ⊺
k γk
,

96
Statistical models in speech and language processing
=

j∈JM

k∈Kj
γk log CN (k) −1
2tr

W′
j
⊺W′
jj −2W′
j
⊺Zj +

k∈Kj
(k)−1k

,
(3.168)
where j and Zj are 0th and 1st order sufﬁcient statistics of linear regression parameters
deﬁned as:
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
j ≜

k∈Kj
ξkξ⊺
k γk
Zj ≜

k∈Kj
(Ck)−1γ kξ⊺
k .
(3.169)
Here Zj is a D × (D + 1) matrix and j is a (D + 1) × (D + 1) symmetric matrix.
To derive Eq. (3.168), we use the fact that the trace of the scalar value is equal to the
original scalar value, the cyclic property, and the distributive property of the trace as in
Appendix B:
a = tr[a],
(3.170)
tr[ABC] = tr[BCA],
(3.171)
tr[A(B + C)] = tr[AB + AC].
(3.172)
We also use the deﬁnition of the Cholesky decomposition in Eq. (3.164).
Since Eq. (3.168) is represented as a quadratic form with respect to Wj, we can obtain
the optimal WML
j
in the sense of ML, similar to the discussion in Section 3.4.3. By
differentiating the Q function with respect to Wj, we can derive the following equation:
∂
∂W′
j
Q(
′
JM|
JM) = 0. ⇒Zj −WML
j
j = 0.
(3.173)
Here, we use the following matrix formulas for the derivation in Appendix B.3:
∂
∂Xtr[X′A] = A
∂
∂Xtr[X′XA] = 2XA.
(A is a symmetric matrix)
(3.174)
Thus, we can obtain the following analytical solution:
WML
j
= Zj−1
j
.
(3.175)
Therefore, the optimized mean vector parameter is represented as:
μad
k
ML = CkZj−1
j
ξk.
(3.176)
Therefore, μad
k is analytically obtained by using the statistics (Zj and j in Eq. (3.169))
and initial HMM parameters (Ck and ξk). This solution corresponds to the M-step of
the EM algorithm, and the E-step is performed by the forward–backward algorithm,
similar to that of HMMs, to compute these statistics. The training procedure is shown in
Algorithm 5.

3.6 n-gram with smoothing techniques
97
Algorithm 5 Maximum likelihood linear regression
Require:  and 
JM ←
init
JM
1: repeat
2:
Compute the occupation probability γt(k).
3:
Accumulate the sufﬁcient statistics γk, γ k, k, Zj, and j
4:
Estimate the transformation parameters 
ML
JM
5:
Update the HMM parameters 
6: until Convergence
MLLR is one of the most popular techniques for acoustic modeling, and there are
many variants of transformation types for HMMs, e.g., Sankar & Lee (1996), Chien, Lee
& Wang (1997), Chen, Liau, Wang et al. (2000), Mak, Kwok & Ho (2005) and Delcroix
et al. (2009). In addition to speech recognition, there are many other applications which
are based on the adaptive training of HMMs (e.g., speech synthesis (Tamura, Masuko,
Tokuda et al. 2001), speaker veriﬁcation (Stolcke, Ferrer, Kajarekar et al. 2005), face
recognition (Sanderson, Bengio & Gao 2006) and activity recognition (Maekawa &
Watanabe 2011)).
3.6
n-gram with smoothing techniques
As we discussed in Section 3.1, a language model (LM) is known as crucial prior infor-
mation for large vocabulary continuous speech recognition (LVCSR), according to the
Bayes decision rule:
ˆW = dMAP(O) = arg max
W
p(O|W)
  
acoustic model
×
p(W)

language model
.
(3.177)
Many other applications include document classiﬁcation, information retrieval, optical
character recognition, machine translation, writing correction, and bio-informatics. An
overview of language modeling in LVCSR systems has been given in Chen & Goodman
(1999), Kita (1999), Rosenfeld (2000), Bellegarda (2004), and Saon & Chien (2012b).
A language model is a probability distribution p(W) over a sequence of word strings
W = {w1, · · · , wi, · · · , wJ} ≜wJ
1 that describes how likely it is that the word sequence
W occurs as a sentence in some domain of interest.5 Recall that word w is represented
by a string, and it is an element of a set of distinct words V, which is also called a
vocabulary or dictionary. Here, wi is a word at position i. The word string wi ∈V and
the continuous speech vector ot ∈RD are both sequential patterns but in different data
types and different time scales.
5 Some languages do not have word boundaries explicitly marked by white-space in a text (e.g., Japanese
and Chinese). Therefore, to process a text for language molding, these languages need an additional word
segmentation step (Sproat, Gale, Shih & Chang 1996, Matsumoto, Kitauchi, Yamashita et al. 1999, Kudo
2005).

98
Statistical models in speech and language processing
To get used to this notation, we provide several examples to represent the following
word sequence:
my wife used my car.
This sentence has ﬁve words and one period at the end of the sentence. By regarding
the period as one word,6 this sentence is totally composed of a six-word sequence (i.e.,
J = 6), and can be represented by
w6
1 = {w1 = “my”, w2 = “wife”, w3 = “used”, w4 = “my”, w5 = “car”, w6 = “.”}.
(3.178)
Note that the vocabulary for this sentence is composed of distinct unique words
represented as:
V = {“my”, “wife”, “used”, “car”, “.”}
(3.179)
and the vocabulary size in this example is
|V| = 5.
(3.180)
We can also introduce the following summation over vocabulary V, which is important
in this section:

wi∈V
f(wi).
(3.181)
This summation is performed over each vocabulary in Eq. (3.179), and not over a
position i in Eq. (3.178).
Basically, the prior word probability is employed to characterize the regularities in
natural language. The probability of a word sequence {w1, · · · , wJ} is represented based
on the product rule as:
p(w1, · · · , wJ) = p(wJ|w1, · · · , wJ−1)p(w1, · · · , wJ−1)
= p(wJ|w1, · · · , wJ−1)p(wJ−1|w1, · · · , wJ−2)p(w1, · · · , wJ−2)
...
=
J
i=1
p(wi|wi−1
1
),
(3.182)
where  denotes the n-gram parameters, namely the n-gram probabilities, which is
explained later. Here, to describe the word sequence from ith word to nth word, we
use the following notation:
{wi, · · · , wn} ≜wn
i .
(3.183)
We also deﬁne the following special cases:
wi
i = wi
wn
i = ∅,
when i > n
(3.184)
6 In the implementation, we additionally deﬁne the start of a sentence with an auxiliary symbol for practical
use, which makes the number of words seven in this example.

3.6 n-gram with smoothing techniques
99
where ∅denotes an empty set. For example, the word sequences {w1, · · · , wi−1} are
represented as
wi−1
1
≜{w1, · · · , wi−1}
(3.185)
and so on. When i = 1 in Eq. (3.182), the conditional distribution is written as:
p(w1|w0
1) ≜p(w1), where w0
1 = ∅.
(3.186)
However, it makes the model larger, as the number of words in a sequence is larger, and
we need to model it with the ﬁxed model size. Thus, Eq. (3.182) is approximated with
the (n −1)th order Markov assumption by multiplying the probabilities of a predicted
word wi conditional on its preceding n −1 words {wi−n+1, · · · , wi−1}:
p(wJ
1) ≈
J
i=1
p(wi|wi−1
i−n+1).
(3.187)
This model is called an n-gram model. Usually n is taken to be from 2 to 5, which
depends on the size of the training data and applications. When n = 1 in Eq. (3.187),
the conditional distribution is written as:
p(wi|wi−1
i
) ≜p(wi),
wi−1
i
≜∅,
(3.188)
which is called a unigram model that does not depend on any history of words.
The n-gram parameter is deﬁned as the weight given to a conditional word sequence.
The probabilistic distribution of p(wi|wi−1
i−n+1) is parameterized as with a multinomial
distribution as:
p(wi|wi−1
i−n+1) ≜θwi|wi−1
i−n+1,
(3.189)
where

wi∈V
θwi|wi−1
i−n+1 = 1
θwi|wi−1
i−n+1 ≥0
∀wi ∈V.
(3.190)
Note that the number of distinct n-gram parameters for θwi|wi−1
i−n+1 would be the index
to the power of the vocabulary size |V|, i.e., |V|n. In this section, we use the following
notation to present a set of n-gram parameters:
n ≜{θwi|wi−1
i−n+1|∀wi ∈V, · · · , wi−n+1 ∈V}
≜{p(wi|wi−1
i−n+1)}.
(3.191)
The number of parameters is a very large since the vocabulary size of LVCSR would
be more than 50 000, and the main problem of language modeling is how to compactly
represent these parameters.
The straightforward way to estimate the multinomial distribution of an n-gram
θwi|wi−1
i−n+1 = p(wi|wi−1
i−n+1) from a text corpus D is to compute the ML estimate by

100
Statistical models in speech and language processing
θML
wi|wi−1
i−n+1 = pML(wi|wi−1
i−n+1)
= arg
max
θwi|wi−1
i−n+1
p(D|θwi|wi−1
i−n+1),
(3.192)
where the multinomial likelihood function is obtained from the deﬁnition in
Appendix C.2 by
p(D|{θwi|wi−1
i−n+1|wi ∈V}) =

wi∈V
(θwi|wi−1
i−n+1)c(wi
i−n+1),
(3.193)
where c(wi−1
i−n+1) denotes the number of occurrences of word sequence wi−1
i−n+1 in train-
ing corpus D. To estimate the parameter, similarly to the state transitions and mixture
weights in the HMM in Section 3.4.3, we introduce a Lagrange multiplier η and solve
the constrained optimization problem by maximizing

wi∈V
c(wi
i−n+1) log θwi|wi−1
i−n+1 + η
⎛
⎝
wi∈V
θwi|wi−1
i−n+1 −1
⎞
⎠.
(3.194)
Setting the derivative of Eq. (3.194) with respect to θwi|wi−1
i−n+1 to zero, we obtain
θwi|wi−1
i−n+1 = −1
ηc(wi
i−n+1).
(3.195)
By substituting this result into constraint Eq. (3.190), we ﬁnd the value of the Lagrange
multiplier

wi∈V
θwi|wi−1
i−n+1 = −1
η

wi∈V
c(wi
i−n+1) = 1
⇒η = −

wi∈V
c(wi
i−n+1),
(3.196)
and the ML solution in the form of
θML
wi|wi−1
i−n+1 =
c(wi
i−n+1)

wi c(wi
i−n+1)
=
c(wi
i−n+1)
c(wi−1
i−n+1)
.
(3.197)
Without loss of generality, we neglect the notation  in the following expressions. The
goal of the most popularly used language models, trigram models, is to determine the
probability of a word given the previous two words p(wi|wi−2, wi−1), which is estimated
as the number of times the word sequence {wi−2, wi−1, wi} occurs in some corpus of
training data divided by the number of times the word sequence {wi−2, wi−1} occurs.
However, again the number of n-gram parameters depends on the number of word
combinations in a word sequence {wi−n+1, · · · , wi−1, wi}, which is counted as the num-
ber of different words wn ∈V at different temporal positions from i −n + 1 to i. This
number is exponentially increased by involving large n. Although n-gram is effective

3.6 n-gram with smoothing techniques
101
at exploiting local lexical regularities, it suffers from the inadequacies of training data
and long-distance information due to too many word combinations and too narrow an
n-gram window size, respectively. These limitations substantially weaken the regulariza-
tion of the trained n-gram models and the prediction for unseen words. The limitation
of n-gram window size could be resolved by exploiting large-span latent semantic infor-
mation (Hofmann 1999b, Bellegarda 2000), which is addressed in Section 3.7. In what
follows, we address different smoothing solutions to the problem of insufﬁcient training
data in the n-gram model.
3.6.1
Class-based model smoothing
A simple and meaningful approach to tackle the data sparseness problem is to consider
the transition probabilities between classes rather than words, namely to adopt the class-
based n-gram language model (Brown, Desouza, Mercer et al. 1992, Chen 2009):
p(wi|wi−1
i−n+1) ≈p(wi|ci)p(ci|ci−1
i−n+1),
(3.198)
where ci ∈C is the class assignment of word wi, p(wi|ci) is the probability of word wi,
generated from class ci, and p(ci|ci−1
i−n+1) is the class n-gram. The class assignments of
different words are determined beforehand according to the word clustering using the
metric of mutual information. An existing linguistic class (e.g., part of speech) is also
used to provide the class assignments.
The word probability given a class p(wi|ci) is usually estimated by using the ML
estimation, similarly to Eq. (3.197), as:
pML(wi|ci) = θML
wi|ci =
c(wi, ci)

wi∈V c(wi, ci),
(3.199)
where c(wi, ci) is the number of word counts labeled by both {wi, ci} in the corpus D. The
class n-gram probability p(ci|ci−1
i−n+1) is estimated by using the ML estimation (replacing
the word counts in Eq. (3.197) with class counts), or a smoothing technique, explained
in the following sections.
The model parameters are composed of
 = {{p(wi|ci)}, {p(ci|ci−1
i−n+1)}}.
(3.200)
Both are represented by the multinomial distributions. Since the number of distinct
classes |C| is much smaller than the vocabulary size |V|, the model size is signiﬁ-
cantly reduced. Model parameters could be reliably estimated. For example, the number
of parameters of {{p(wi|ci)} is |V∥C|, and the number of parameters of {p(ci|ci−1
i−n+1)}
is |C|n. Since |V| ≫|C|, the total number of parameters of a class n-gram model
|V∥C| + |C|n is much smaller than an n-gram model |V|n. The class-based n-gram is
also seen as a smoothed language model.
3.6.2
Jelinek–Mercer smoothing
As reported in Chen & Goodman (1999), it is usual to deal with the issue of data sparse-
ness in an n-gram model by using a linear interpolation method where the nth order

102
Statistical models in speech and language processing
language model pinterp(wi|wi−1
i−n+1) is estimated by interpolating with the (n −1)th order
language model in a form of
pinterp(wi|wi−1
i−n+1) = λwi−1
i−n+1pML(wi|wi−1
i−n+1)
+ (1 −λwi−1
i−n+1)pinterp(wi|wi−1
i−n+2),
(3.201)
where λwi−1
i−n+1 denotes the interpolation weight which is estimated for each wi−1
i−n+1 in
accordance with the ML method. The reason the interpolation weight λwi−1
i−n+1 does not
depend on wi comes from the constraint of the sum-to-one property of an n-gram model.
The nth order smoothed model is calculated recursively as a linear interpolation between
the nth order ML model and the (n −1)th order smoothed model, that is
pinterp(wi|wi−1
i−n+1) = λwi−1
i−n+1pML(wi|wi−1
i−n+1) + (1 −λwi−1
i−n+1)
×
(
λwi−1
i−n+2pML(wi|wi−1
i−n+2) + (1 −λwi−1
i−n+2)pinterp(wi|wi−1
i−n+3)
)
= · · · .
(3.202)
To carry through this recursive process to the end, we can take the smoothed unigram or
ﬁrst-order model to be the ML distribution in Eq. (3.192), i.e., when n = 1,
pinterp(wi|wi−1
i
) = pinterp(wi)
= pML(wi) = θML
wi ,
(3.203)
where θML
wi is obtained from Eq. (3.197) as:
θML
wi =
c(wi)

wi c(wi).
(3.204)
Or we can take the smoothed zeroth-order model to be the discrete uniform distribution
(Appendix C.1),
pinterp(wi) = λpML(wi) + (1 −λ)Unif(wi),
(3.205)
where the uniform distribution is deﬁned with the vocabulary size |V| as:
Unif(wi) ≜1
|V|.
(3.206)
Now, let us consider the original interpolation equation Eq. (3.201). The parameter
λwi−1
i−n+1 is estimated individually for the wi−1
i−n+1 that maximizes the probability of some
of the data. Practically, the selection could be done for buckets of parameters. In general,
this class of interpolated models is also known as the n-gram model with Jelinek–Mercer
smoothing (Jelinek & Mercer 1980), which is a standard form of interpolation smooth-
ing. The smoothing techniques in the following sections (Witten–Bell (WB) smoothing
in Section 3.6.3, absolute discount in Section 3.6.4, Kneser–Ney (KN) smoothing in
Eq. (3.232), and PLSA smoothing in Eq. (3.319)) provide speciﬁc interpolation weights
in this standard form, as shown in Table 3.1.

3.6 n-gram with smoothing techniques
103
An important constraint of the n-gram model is that the summation over wi goes to 1,
as shown in Eq. (3.190). The Jelinek–Mercer smoothing (and the following smoothing
techniques) satisﬁes this condition, i.e.,

wi∈V
pinterp(wi|wi−1
i−n+1) = 1.
(3.207)
We can prove this condition recursively from the unigram case to the n-gram case. First,
it is obvious that for the unigram models for the Eqs. (3.203) and (3.205) cases:

wi∈V
pinterp(wi) = 1.
(3.208)
Next, the bi-gram case is also proved by:

wi∈V
pinterp(wi|wi−1) =

wi∈V
(
λwi−1pML(wi|wi−1) + (1 −λwi−1)pinterp(wi)
)
= λwi−1
⎛
⎝
wi∈V
pML(wi|wi−1)
⎞
⎠+ (1 −λwi−1)
⎛
⎝
wi∈V
pinterp(wi)
⎞
⎠
= λwi−1 + (1 −λwi−1) = 1,
(3.209)
where we use Eq. (3.208) and 
wi pML(wi|·) = 1. That is proven in the n-gram case,
trivially. The important property of this proof of the sum-to-one condition is that the
summation over wi does not depend on the interpolation weight λwi−1
i−n+1.
3.6.3
Witten–Bell smoothing
Witten–Bell smoothing (Witten & Bell 1991) is considered to be an instance of inter-
polation smoothing as addressed in Section 3.6.2 by setting a speciﬁc value for the
interpolation parameter λwi−1
i−n+1. The Witten–Bell smoothing ﬁrst deﬁnes the following
number based on the number of unique words that follow the history wi−1
i−n+1:
N1+(wi−1
i−n+1, •) ≜|{wi|c(wi−1
i−n+1, wi) > 0}|.
(3.210)
The notation N1+ represents the number of distinct words that have one or more
counts, and the • represents any possible words at i with this condition. By using
N1+(wi−1
i−n+1, •), the Witten–Bell smoothing assigns the factor 1 −λwi−1
i−n+1 for the
lower-order model (the second term in Eq. (3.201)) where
1 −λwi−1
i−n+1 ≜
N1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) + N1+(wi−1
i−n+1, •)
=
N1+(wi−1
i−n+1, •)
c(wi−1
i−n+1) + N1+(wi−1
i−n+1, •)
.
(3.211)

104
Statistical models in speech and language processing
This factor is interpreted as the frequency with which we should use the lower-order
model to predict the next word. It is meaningful that more unique words appearing after
history words wi−1
i−n+1, i.e., larger N1+(wi−1
i−n+1, •), implies that more reliable values of
the (n −1)-grams are estimated, and the higher weight factor 1 −λwi−1
i−n+1 should be
assigned for (n −1)-grams. Similarly, the rest of the weight λwi−1
i−n+1 (the ﬁrst term in
Eq. (3.201)) is represented from Eq. (3.211) as
λwi−1
i−n+1 = 1 −
N1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) + N1+(wi−1
i−n+1, •)
=

wi c(wi
i−n+1)

wi c(wi
i−n+1) + N1+(wi−1
i−n+1, •)
.
(3.212)
This is the modiﬁed ML estimate with the unique word count N1+(wi−1
i−n+1, •).
According to the interpolation smoothing in Eq. (3.201) and Eqs. (3.211) and (3.212),
the Witten–Bell smoothing is expressed by
pWB(wi|wi−1
i−n+1)
=
c(wi
i−n+1)pML(wi|wi−1
i−n+1) + N1+(wi−1
i−n+1, •)pWB(wi|wi−1
i−n+2)

wi c(wi
i−n+1) + N1+(wi−1
i−n+1, •)
.
(3.213)
Note that the factor used in the Witten–Bell smoothing in Eq. (3.211) only depends
on wi−1
i−n+1, and has the same dependency as λwi−1
i−n+1. Therefore, it is trivial that the
Witten–Bell smoothing satisﬁes the sum-to-one condition, as it is a special solution of
the interpolation smoothing (Eq. (3.201)) that satisﬁes the sum-to-one condition.
3.6.4
Absolute discounting
Absolute discounting is also considered to be an instance of interpolation smoothing
as addressed in Section 3.6.2. However, the equation form is not represented as the
Jelinek–Mercer form, which we set down again as follows for comparison:
pinterp(wi|wi−1
i−n+1) = λwi−1
i−n+1pML(wi|wi−1
i−n+1) + (1 −λwi−1
i−n+1)pinterp(wi|wi−1
i−n+2). (3.214)
Recall that Eq. (3.214) has the weight λ depending on the previous word sequence
wi−1
i−n+1, and is composed of the ML probability pML(wi|wi−1
i−n+1) and lower-order prob-
ability pinterp(wi|wi−1
i−n+2). However, in absolute discounting, instead of multiplying the
higher-order ML model by a factor λwi−1
i−n+1, the higher-order distribution is created by
subtracting a ﬁxed discount d for the case of non-zero count. The absolute discounting
is deﬁned by
pABS(wi|wi−1
i−n+1) ≜
max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
+ (1 −λwi−1
i−n+1)pABS(wi|wi−1
i−n+2),
(3.215)
where 0 ≤d ≤1 denotes a discounting parameter.

3.6 n-gram with smoothing techniques
105
The interpolation weight (1 −λwi−1
i−n+1) is formed with the unique word count
N1+(wi−1
i−n+1, •) deﬁned in Eq. (3.210) as:
1 −λwi−1
i−n+1 =
dN1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) .
(3.216)
Note that the weight does not depend on wi, but wi−1
i−n+1, similarly to the Jelinek–Mercer
form in Eq. (3.214). The way to ﬁnd weight parameter 1 −λwi−1
i−n+1 is again based on the
sum-to-one condition. Consider the condition

wi
pABS(wi|·) = 1.
(3.217)
Then, by taking the summation over wi for both sides in Eq. (3.215), Eq. (3.215) can be
rewritten as:
1 =

wi
pABS(wi|wi−1
i−n+1)
=

wi
max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
+ (1 −λwi−1
i−n+1)

wi
pABS(wi|wi−1
i−n+2)
=

wi max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
+ (1 −λwi−1
i−n+1).
(3.218)
Therefore,
1 −λwi−1
i−n+1 = 1 −

wi max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
=

wi c(wi
i−n+1) −
wi max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
=

wi

c(wi
i−n+1) −max{c(wi
i−n+1) −d, 0}


wi c(wi
i−n+1)
.
(3.219)
Now we focus on the numerator in Eq. (3.219) that represents the total discount value
from d. By considering the cases when c(wi
i−n+1) > 0 and c(wi
i−n+1) = 0, we can derive
the following equation:
c(wi
i−n+1) −max{c(wi
i−n+1) −d, 0} =

d
if c(wi
i−n+1) > 0
0
if c(wi
i−n+1) = 0.
(3.220)
Therefore, by substituting Eq. (3.220) into Eq. (3.219), and by using the unique word
count N1+(wi−1
i−n+1, •), Eq. (3.219) is ﬁnally represented as:
1 −λwi−1
i−n+1 =

wi|c(wi
i−n+1)>0 d

wi c(wi
i−n+1)
=
dN1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) ,
(3.221)

106
Statistical models in speech and language processing
where 
wi|c(wi
i−n+1)>0 means that the summation is undertaken for a subset of dis-
tinct words in the vocabulary V that satisﬁes the condition c(wi
i−n+1) > 0. Since d
does not depend on wi, the numerator can be represented with d times the number
of distinct words wi that satisﬁes the condition c(wi
i−n+1) > 0, which corresponds to
N1+(wi−1
i−n+1, •). Thus, we prove Eq. (3.216).
This weight means that the discount d in an observed n-gram event c(wi−1
i−n+1wi) > 0
at current word wi is distributed to compensate for those unseen events c(wi−1
i−n+1wi) = 0
where a lower-order model pABS(wi|wi−1
i−n+2) is adopted. The discount d is shared for all
n-grams and could be measured by using the total number of n-grams with exactly one
and two counts, i.e., c(wi−1
i−n+1) = 1 and c(wi−1
i−n+1) = 2 in the training data, respectively
(Ney, Essen & Kneser 1994).
In summary, the absolute discounting is represented by
pABS(wi|wi−1
i−n+1) ≜
max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
+
dN1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) pABS(wi|wi−1
i−n+2).
(3.222)
Note that the absolute discounting does not include the exact ML probabilities (although
they can be included by re-arranging the ﬁrst term). The weight parameter for the lower-
order model is proportional to the value N1+(wi−1
i−n+1, •), which has a similarity to that
in Witten–Bell smoothed n-grams, as seen in Eq. (3.211).
Thus, we have explained Witten–Bell smoothing in Section 3.6.3 and absolute
discount in Section 3.6.4 as instances of the interpolation (Jelinek–Mercer) smooth-
ing techniques. The next section introduces another type of well-known smoothing
technique called backoff smoothing, with Katz smoothing as an example.
3.6.5
Katz smoothing
Katz smoothing (Katz 1987) was developed by intuitively combining higher-order mod-
els with lower-order models through scaling of the ML distribution. Taking a bi-gram
as an example, the Katz smoothing is performed by calculating the probability by con-
sidering the cases where the co-occurrence count c(wi−1, wi) of wi−1 and wi is zero or
positive as follows:
pKZ(wi|wi−1) ≜

dc(wi−1,wi)pML(wi|wi−1)
if c(wi−1, wi) > 0
α(wi−1)pKZ(wi)
if c(wi−1, wi) = 0,
(3.223)
where dr < 1 is a discount ratio that reduces the probability estimated from the ML.
The discount ratio is usually is obtained by the Good–Turing estimate (Good 1953), and
this book does not describe it in detail. The expression α(wi−1) is a scaling factor that
only depends on the previous word wi−1, while pKZ(wi) is a unigram probability, and we
usually use the ML unigram probability for the bi-gram case, i.e.,
pKZ(wi) = pML(wi).
(3.224)

3.6 n-gram with smoothing techniques
107
Now we focus on the scaling factor α(wi−1), which is obtained by the sum-to-one
condition of pKZ(wi|·) and pML(wi|·) that must be satisﬁed for any probabilistic distribu-
tions. By summing over wi in both sides of Eq. (3.223), the equation can be rewritten as

wi
pKZ(wi|wi−1) =

wi dc(wi−1,wi)pML(wi|wi−1)
if c(wi−1, wi) > 0

wi α(wi−1)pML(wi)
if c(wi−1, wi) = 0.
(3.225)
This leads to the following equation:
1 =

wi|c(wi−1,wi)>0
dc(wi−1,wi)pML(wi|wi−1) + α(wi−1)

wi|c(wi−1,wi)=0
pML(wi),
(3.226)
where 
wi|c(wi−1,wi)>0 or 
wi|c(wi−1,wi)=0 means that the summation is undertaken for a
subset of distinct words in the vocabulary V that satisﬁes the condition c(wi−1, wi) > 0
or c(wi−1, wi) = 0. Thus, we obtain that
α(wi−1) =
1 −
wi|c(wi−1,wi)>0 dc(wi−1,wi)pML(wi|wi−1)

wi|c(wi−1,wi)=0 pML(wi)
=
1 −
wi|c(wi−1,wi)>0 dc(wi−1,wi)pML(wi|wi−1)
1 −
wi|c(wi−1,wi)>0 pML(wi)
.
(3.227)
This smoothing technique can be generalized to the n-gram probability as
pKZ(wi|wi−1
i−n+1) ≜

dwi
i−n+1pML(wi|wi−1
i−n+1)
if c(wi
i−n+1) > 0
α(wi−1
i−n+1)pKZ(wi|wi−1
i−n+2)
if c(wi
i−n+1) = 0,
(3.228)
where
α(wi−1
i−n+1) =
1 −
wi|c(wi
i−n+1)>0 dc(wi
i−n+1)pML(wi|wi−1
i−n+1)
1 −
wi|c(wi
i−n+1)>0 pKZ(wi|wi−1
i−n+2)
.
(3.229)
This smoothing scheme is known as a realization of backoff smoothing. This smooth-
ing is obtained by the weighted multiplication of the ML probability and is different
from interpolation (Jelinek–Mercer) smoothing, as discussed in Section 3.6.2, which
is obtained by the weighted summation of the ML probability. Note that both smooth-
ing techniques include some free parameters (weight λ in interpolation smoothing and
the discount factor d in backoff smoothing), but the other parameters are obtained by
using the sum-to-one constraint of the probability distribution. Similarly to the meaning
of λ, backoff smoothing relies more on the lower-order n-gram probability when d is
small (close to 0), while backoff smoothing relies more on the ML n-gram probability
when d is large (close to 1). The next section describes a famous (modiﬁed) Kneser–
Ney smoothing, which can be realized with both interpolation and backoff smoothing
methods.
3.6.6
Kneser–Ney smoothing
Kneser–Ney (KN) smoothing (Kneser & Ney 1995) can be interpreted as an extension
of the absolute discount approach, as discussed in Section 3.6.4. When the highest-order

108
Statistical models in speech and language processing
probability case is considered, it is same as the absolute discount. However, the unique
property of KN smoothing is that it provides a special lower-order probability based on
the numbers of distinct words.
For example, in the highest-order probability case, the original Kneser–Ney method
was developed as the following backoff smoothed model:
pKN(wi|wi−1
i−n+1) =
⎧
⎨
⎩
max{c(wi
i−n+1)−d,0}

wi c(wi
i−n+1)
if c(wi
i−n+1) > 0
γ (wi−1
i−n+1)pKN(wi|wi−1
i−n+2)
if c(wi
i−n+1) = 0,
(3.230)
where γ (wi−1
i−n+1) is chosen to make the distribution sum to 1 and has the form
γ (wi−1
i−n+1) =
dN1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) .
(3.231)
Alternatively, the Kneser–Ney model could be estimated according to the interpolation
smoothing scheme in the highest-order probability case based on
pKN(wi|wi−1
i−n+1) =
max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
+
dN1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) pKN(wi|wi−1
i−n+2).
(3.232)
Note that the ﬁrst term of Eq. (3.232) is calculated from a modiﬁcation of the ML prob-
ability, and Eq. (3.232) is exactly the same as the absolute discounting in Eq. (3.222),
except for the lower-order probability pKN(wi|wi−1
i−n+2). However, in the lower-order
probability, instead of using Eq. (3.232) recursively by changing n to n −1, the Kneser–
Ney smoothing provides more intuitive probability than the ML probability for the ﬁrst
term by considering the continuation of words.
For a simple explanation, we consider the bi-gram case (the highest-order probability
is a bi-gram probability), i.e., Eq. (3.232) is represented as:
pKN(wi|wi−1) = max{c(wi−1, wi) −d, 0}

wi c(wi−1, wi)
+ dN1+(wi−1, •)

wi c(wi−1, wi)pKN(wi).
(3.233)
The question here is whether we really use the unigram probability for pKN(wi). This is
often illustrated with an example of the bi-gram “San Francisco” (Chen & Goodman
1999, Jurafsky 2014). In the training data of the Wall Street Journal (WSJ0) corpus
(Paul & Baker 1992), the bi-gram “San Francisco” appears 3222 times, while the word
“Francisco” appeared 3329 times. The other word “glasses” appears 185 times. That is
c(wi−1 = “San”, wi = “Francisco”) = 3222
c(wi = “Francisco”) = 3329
c(wi = “glasses”) = 185.
(3.234)
From these statistics, we can say that:
• The word “Francisco” almost always (96.8%) follows “San,” and does not follow the
other words in most cases.
• However, the word “Francisco” is more common (18 times more) than “glasses.”

3.6 n-gram with smoothing techniques
109
Then, let us consider the following sentences:
1. I can’t see without my reading “glasses.”
2. I can’t see without my reading “Francisco.”
We consider the case when there are no “reading glasses” and “reading Francisco” in
a training corpus, and the lower-order probability pKN(wi) (the second term in (3.233))
is used to compute these sentence probabilities. Intuitively, we want to make the prob-
ability of the ﬁrst sentence with “glasses” higher than the other. However, if we use
the count-oriented (ML-like) probability for pKN(wi), the sentence with “Francisco” is
assigned higher probability because the simple word count of “Francisco” is much larger
than that of “glasses.” This problem can be avoided by considering the continuity of
word sequences rather than word counts in the lower-order probability.7
The KN smoothing provides the following unigram probability that does not use the
word count (ML-like) information:
pKN(wi) ≜N1+(•, wi)
N1+(•, •) .
(3.235)
Here, from the deﬁnition of the unique count N1+ in Eq. (3.210), N1+(•, wi) and
N1+(•, •) are represented as follows:
N1+(•, wi) = |{wi−1|c(wi−1, wi) > 0}|,
(3.236)
N1+(•, •) = |{{wi, wi−1}|c(wi−1, wi) > 0}| =

wi
N1+(•, wi).
(3.237)
In the previous example,
N1+(•, wi = “Francisco”) = 58
N1+(•, wi = “glasses”) = 88.
(3.238)
The unique count of N1+(•, wi = “glasses”) is larger than that of N1+(•, wi =
“Francisco”). Thus, the probability of the sentence with “glasses” becomes larger when
we use the number of unique words in (3.235), which is a more intuitive result in this
example.
The probability based on the unique counts can be generalized from the unigram case
in Eq. (3.235) to higher-order n-gram probabilities, except for the highest-order n-gram
that uses the absolute discounting from Eq. (3.232). For example, pKN(wi|wi−1
i−n+2), which
is the second term in Eq. (3.232), is represented as follows:
pKN(wi|wi−1
i−n+2) =
max{N1+(•, wi−1
i−n+2) −d, 0}
N1+(•, wi−1
i−n+2, •)
+
dN1+(wi−1
i−n+2, •)
N1+(•, wi−1
i−n+2, •)
pKN(wi|wi−1
i−n+3),
(3.239)
7 Actually “reading glasses” has appeared twice in the WSJ0 corpus, and the lower-order probability cannot
be used so much in the WSJ0 language model when we set d very small. This is another solution to resolve
this discontinuity problem by correcting a very large size of corpus.

110
Statistical models in speech and language processing
where
N1+(•, wi−1
i−n+2) ≜|{wi−n+1|c(wi−1
i−n+1) > 0}|
N1+(•, wi−1
i−n+2, •) ≜|{wi−n+1, wi|c(wi
i−n+1) > 0}| =

wi
N1+(•, wi−1
i−n+2).
(3.240)
Thus, we can obtain the interpolation version of the Kneser–Ney smoothing for all cases,
which is summarized as follows:
• The highest-order n-gram probability
pKN(wi|wi−1
i−n+1) =
max{c(wi
i−n+1) −d, 0}

wi c(wi
i−n+1)
+
dN1+(wi−1
i−n+1, •)

wi c(wi
i−n+1) pKN(wi|wi−1
i−n+2);
(3.241)
• Lower-order n-gram probability
pKN(wi|wi−1
i−n+2) =
max{N1+(•, wi−1
i−n+2) −d, 0}
N1+(•, wi−1
i−n+2, •)
+
dN1+(wi−1
i−n+2, •)
N1+(•, wi−1
i−n+2, •)
pKN(wi|wi−1
i−n+3);
(3.242)
• Unigram probability
pKN(wi) = N1+(•, wi)
N1+(•, •) .
(3.243)
In general, the interpolated n-gram model yields better performance than the backoff
n-gram model (Chen & Goodman 1999).
Modiﬁed Kneser–Ney smoothing
In Kneser–Ney smoothing and discounting smoothing, a discount parameter d in
Eq. (3.241) plays an important role in striking a balance between the target-order n-gram
probability and the lower-order one. Then, it is a simple question whether the single dis-
count parameter d is precise enough to handle the balance. Basically, the distribution of
distinct words follows the power-law property where only a few frequent words form a
high proportion of the total, but a lot of rare words occur only one or two times, i.e.,
|{wi|c(wi) = 1}| ≫|{wi|c(wi) = 2}| ≫|{wi|c(wi) > 2}|.
(3.244)
This power-law property comes from the “rich-get-richer behavior” in natural language,
and this property also applies to n-grams, as well as words. Thus, Kneser–Ney smooth-
ing is modiﬁed by separately using three discount parameters d1, d2, and d3+ for those

3.6 n-gram with smoothing techniques
111
n-grams with one, two, and three or more counts, respectively, instead of using a single
discount d for all non-zero counts, as follows:
d(c) ≜
⎧
⎪⎪⎨
⎪⎪⎩
0
if c = 0
d1
if c = 1
d2
if c = 2
d3+
if c ≥3.
(3.245)
These parameters are empirically determined as (Chen & Goodman 1999):
dbase =
m1
m1 + 2m2
,
(3.246)
d1 = 1 −2dbase
m2
m1
,
(3.247)
d2 = 2 −3dbase
m3
m2
,
(3.248)
d3+ = 3 −4dbase
m4
m3
,
(3.249)
where mj is the total number of n-grams appearing j times in a training corpus, i.e.,
mj ≜

wi
i−n:c(wi
i−n)=j
c(wi
i−n).
(3.250)
Based on the new discounting parameter d(c) in Eq. (3.245), a modiﬁed Kneser–Ney
(MKN) smoothing is conducted to estimate the smoothed n-grams using
pMKN(wi|wi−1
i−n+1) =
c(wi
i−n+1) −d(c(wi−1
i−n+1))

wi c(wi
i−n+1)
+ γ (wi−1
i−n+1)pMKN(wi|wi−1
i−n+2),
(3.251)
where γ (wi−1
i−n+1) is derived by considering the sum-to-one condition. By taking the
summation over wi in both sides of Eq. (3.251), Eq. (3.251) is represented as follows:

wi
pMKN(wi|wi−1
i−n+1) = 1
=

wi
c(wi
i−n+1) −d(c(wi−1
i−n+1))

wi c(wi
i−n+1)
+ γ (wi−1
i−n+1)

wi
pMKN(wi|wi−1
i−n+2)
=

wi
c(wi
i−n+1) −d(c(wi−1
i−n+1))

wi c(wi
i−n+1)
+ γ (wi−1
i−n+1).
(3.252)

112
Statistical models in speech and language processing
Now we focus on the ﬁrst term of the 4th line in Eq. (3.252). By using Eq. (3.245), this
term can be represented as:

wi
c(wi
i−n+1) −d(c(wi−1
i−n+1))
=

wi
c(wi
i−n+1) −d1

wi|c(wi−1
i−n+1)=1
1 −d2

wi|c(wi−1
i−n+1)=2
1 −d3+

wi|c(wi−1
i−n+1)≥3
1
=

wi
c(wi
i−n+1) −d1N1(wi−1
i−n+1, •) −d2N2(wi−1
i−n+1, •) −d3+N3+(wi−1
i−n+1, •).
(3.253)
Thus, by substituting Eq. (3.253) into Eq. (3.252), we obtain
γ (wi−1
i−n+1) =
d1N1(wi−1
i−n+1, •) + d2N2(wi−1
i−n+1, •) + d3+N3+(wi−1
i−n+1, •)

wi c(wi
i−n+1)
.
(3.254)
The MKN smoothed n-grams were shown to perform better than the previously
described smoothed n-grams in terms of perplexity and word error rates for LVCSR
tasks (Chen & Goodman 1999).
Table 3.1 summarizes different language model smoothing methods and their corre-
sponding smoothing techniques including interpolation smoothing and backoff smooth-
ing. (Modiﬁed) Kneser–Ney smoothing, addressed in Section 3.6.6, can be implemented
in both interpolation smoothing and backoff smoothing. Bayesian approaches extend
these standard n-gram language models to MAP estimation of an n-gram language
model in Section 4.7, a hierarchical Dirichlet language model in Section 5.3, and a
hierarchical Pitman–Yor language model in Section 8.5. Since n-gram language models
always have to address the sparse data problem, Bayesian approaches provide an elegant
solution to deal with the problem theoretically.
The next section considers another generative model of a text that can deal with
document information rather than a simple word sequence modeled by an n-gram model.
Table 3.1 Summary of different language model smoothing methods in terms of interpolation smoothing
and backoff smoothing.
Interpolation smoothing
Backoff smoothing
Jelinek–Mercer smoothing
Section 3.6.2
Witten–Bell smoothing
Section 3.6.3
Absolute discount
Section 3.6.4
Katz smoothing
Section 3.6.5
Kneser–Ney smoothing
Eq. (3.232)
Eq. (3.230)
PLSA smoothing
Eq. (3.319)

3.7 Latent semantic information
113
3.7
Latent semantic information
One of the main limitations of the standard n-gram model is the inadequacy of
long-distance information caused by n-gram window size. How to extract semanti-
cally meaningful information outside an n-gram window becomes crucial to achieve
large-span language modeling. Traditionally, the cache-based language model (Kuhn &
De Mori 1990) was proposed to exploit long-distance information where the short-term
pattern in history words is continuously captured for word prediction. On the other hand,
we may characterize long-distance information by ﬁnding long-term semantic depen-
dencies which are seen as global constraints. The short-term statistics within an n-gram
window serve as local constraints. Combining local and global constraints provides
complete and structural information for word sequence probabilities.
In the literature, latent semantic analysis (LSA) (Berry, Dumais & O’Brien 1995) has
been popular for many years (Manning & Schütze 1999) to construct latent topic space
in information retrieval areas to evaluate the similarity between a query and a docu-
ment in that space. LSA was extended to probabilistic latent semantic analysis (PLSA)
(Hofmann 1999b, Hofmann 2001) by dealing with latent semantics as latent variables,
an approach which is based on the maximum likelihood theory with the EM algorithm.
Thus, PLSA provides an additional generative model to an n-gram for a text that con-
siders long-term semantic information in the text. LSA and PLSA have been applied
to develop large-span language models or topic-based language models in Bellegarda
(2000) and in Gildea & Hofmann (1999), respectively, by combining these with n-gram
language models, and these are addressed below.
3.7.1
Latent semantic analysis
Latent semantic analysis (LSA) introduces an additional longer-term index document
d to word w. The document usually holds several to thousands of sentences, and the
deﬁnition (e.g., a paragraph, article, journal paper, etc.) depends on target applications.
Suppose there are M documents in a corpus D, which include words with vocabulary
size |V|. LSA focuses on the |V| × M word-document matrix W:
W =
⎡
⎢⎣
ω1,1
· · ·
ω1,M
...
...
...
ω|V|,1
· · ·
ω|V|,M
⎤
⎥⎦=

ω1
· · ·
ωM

.
(3.255)
The element of a word-document matrix ωv,m is often represented based on a co-
occurrence based count c(w(v), dm), which is the number of occurrences of word w(v) in
document dm. The word w(v) ∈V is a word in vocabulary V pointed by an ordered index
v ∈{1, · · · , |V|}, and (v) in the subscript does not denote a position in a word sequence.
The co-occurrence element is often weighted by considering the importance of words
in documents. As we discuss later, tf–idf (term frequency–inverse document frequency)
or information-theoretic measure is used as an instance of the co-occurrence element.
Note that in this word-document matrix representation, we only consider the count infor-
mation (or related information) for each distinct word w(v) and do not consider the

114
Statistical models in speech and language processing
sequential information of words (e.g., wi) in a document. This feature representation
of words is also called bag of words (Lewis 1998, Joachims 2002, Blei et al. 2003). The
column vector ωm ∈R|V| can represent information about document m with a vector.
This representation is called a vector space model of a text.
Document similarity
The problem of dealing with this matrix is that the column vector ωm is a sparse rep-
resentation from natural language, and it is difﬁcult to obtain the semantic information
from this representation. For example, if we consider the similarity between documents
dm and dm′, from the word-document matrix W with well-known cosine similarity, the
cosine similarity is deﬁned as
cos(a, b) ≜
a⊺b
∥a∥∥b∥.
(3.256)
The cosine similarity has the following property, and it is often used to measure the
similarity in natural language processing:
−1 ≤cos(a, b) ≤1,
cos(a, a) = 1.
(3.257)
The similarity between documents dm and dm′ based on the |V| dimensional space can
be calculated as follows:
Sim|V|(dm, dm′) = cos(ωm, ωm′) =
|V|
v=1 ωvmωvm′
∥ωm∥∥ωm′∥.
(3.258)
Since ωvm and ωvm′ are very sparse, most of the products are zero, and the cosine simi-
larity cannot obtain meaningful similarity scores. In addition, the number of dimensions
(vocabulary size |V|, the number of documents M, or both) is too large to use. Therefore,
LSA conducts a singular value decomposition (SVD) over the |V| × M word-document
matrix W and obtains
W ≈USV⊺,
(3.259)
as shown in Figure 3.9. In Eq. (3.259), S is a K × K diagonal matrix with reduced
dimension, K < min(|V|, M), U is a |V| × K matrix whose columns are the ﬁrst K
eigenvectors derived from word-by-word correlation matrix WW⊺, and V is an M ×
K matrix whose columns are the ﬁrst K eigenvectors derived from the document-by-
document correlation matrix W⊺W.
Document similarity in LSA
After the projection, instead of focusing on the original |V| × M word-document matrix
W, LSA focuses on the factored K × M matrix V⊺. Each column of SV⊺characterizes
the location of a particular document in the reduced K-dimensional semantic space.
Therefore, we deﬁne the following document vector vm ∈RK as an mth column vector
of V⊺, which is a lower dimensional vector than ωm:
V⊺≜

v1
· · ·
vm
· · ·
vM

.
(3.260)

3.7 Latent semantic information
115
Figure 3.9
Singular value decomposition for latent semantic analysis.
This can be weighted by the diagonal matrix S. Therefore, the similarity between
documents dm and dm′ based on this lower K-dimensional representation is calculated as
SimK(dm, dm′) = cos(Svm, Svm′) =
v⊺
mS2vm′
∥Svm∥∥Svm′∥.
(3.261)
Compared with Eq. (3.258), Svm is a denser feature, and can provide more (semanti-
cally) meaningful information (Manning & Schütze 1999).
Now, we consider the application based on information retrieval. We have a query q,
which is composed of several words and can be regarded as an instance of a document.
Then the problem of information retrieval is to search similar documents to query q,
and computing the similarity between q and existing document dm is very important. To
evaluate the cosine similarity between the existing document dm and this query q in the
K-dimensional space, we ﬁrst transform the corresponding occurrence vector ωq to the
K-dimensional vector vq as follows:
vq = S−1U⊺ωq.
(3.262)
Thus, the cosine similarity between q and dm can be computed based on Eq. (3.261) as
follows:
SimK(dm, q) = cos(Svm, Svq) ∝v⊺
mSSS−1U⊺ωq = (Svm)⊺U⊺ωq
⇒SimK(dm, q) = (Svm)⊺U⊺ωq
∥Svm∥∥U⊺ωq∥.
(3.263)
Thus, we can also obtain the similarity between the query q and document dm with more
(semantically) meaningful space, which can be used for information retrieval.
Word-document matrix
The discussion so far does not explicitly introduce a suitable value for the co-occurrence
element (v, m) of the word-document matrix W. Actually it is empirically determined,
and the straightforward representation is based on the co-occurrence based count
c(w(v), dm):

116
Statistical models in speech and language processing
[W](v,m) = ωv,m = c(w(v), dm).
(3.264)
This is the most simple representation of W, and this representation is actually extended
as a probabilistic version of LSA (PLSA) in the next section. Another popular repre-
sentation is based on tf–idf (Salton & Buckley 1988, Manning & Schütze 1999), which
represents the element as follows:
[W](v,m) = ωv,m = tf(w(v), dm)idf(w(v))
=
c(w(v), dm)
|V|
j=1 c(w(j), dm)
log
M
|{dm|c(w(v), dm) > 0}|,
(3.265)
where tf(w(v), dm) is called term frequency, deﬁned as
tf(w(v), dm) ≜
c(w(v), dm)
|V|
j=1 c(w(j), dm)
.
(3.266)
This is computed from the co-occurrence based count c(w(v), dm) in Eq. (3.264) with
a normalization factor. The quantity idf(w(v)) is called the inverse document frequency,
and it is computed from the number of documents |{dm|c(w(v), dm) > 0}| that include
word w(v):
idf(w(v)) ≜log
M
|{dm|c(w(v), dm) > 0}|.
(3.267)
idf(w(v)) would score a lower weight for the co-occurrence element when the word w(v)
has appeared in many documents, since such a word would be less important.
Bellegarda (2000) also proposes another weight based on an information-theoretic
measure as follows:
[W](v,m) = ωv,m = (1 −εw(v))
c(w(v), dm)
|V|
j=1 c(w(v), dm)
.
(3.268)
Compared with Eq. (3.265), the inverse document frequency in Eq. (3.265) is replaced
with (1 −εw(v)), where εv denotes the normalized entropy of word w(v) in a corpus,
deﬁned as
εw(v) ≜−
1
log M
M

m=1
c(w(v), dm)
M
j=1 c(w(v), dj)
log
c(w(v), dm)
M
j=1 c(w(v), dj)
.
(3.269)
The entropy of w(v) would be increased when w(v) is distributed among many docu-
ments, which decreases the weight (1 −εw(v)). Therefore, similarly to tf–idf, the word
distributed in many documents would be less important, and have lower weight for the
co-occurrence element.
3.7.2
LSA language model
The LSA language model was proposed to capture long-range word dependencies
through discovery of latent topics from a text corpus. When incorporating LSA into an
n-gram model, the prediction of word wi making use of n-gram probability p(wi|wi−1
i−n+1)
is calculated from two information sources in history:

3.7 Latent semantic information
117
1. The n-gram history words
h(n)
i−1 ≜wi−1
i−n+1.
(3.270)
2. The long-term topic information of all history words wi−1
1
, which is represented
by co-occurrence count (Eq. (3.264)), tf–idf (Eq. (3.265)), or information-theoretic
measure (Eq. (3.268)), as discussed in the previous section:
h(l)
i−1 ≜ωi−1.
(3.271)
The K-dimensional projected vector ˜vi−1 ∈RD, from the history vector in the
original space ωi−1 ∈R|V| onto the LSA space, is obtained based on Eq. (3.262)
as:
˜vi−1 ≜Svi−1 = U⊺ωi−1.
(3.272)
By using these two types of history information, the LSA language model is represented
by using the product and sum rules as:
pLSA(wi|wi−1
i−n+1) ≜p(wi|h(n)
i−1, h(l)
i−1)
=
p(wi|h(n)
i−1)p(h(l)
i−1|wi, h(n)
i−1)
p(h(l)
i−1|h(n)
i−1)
=
p(wi|h(n)
i−1)p(h(l)
i−1|wi, h(n)
i−1)

wj∈V p(wj|h(n)
i−1)p(h(l)
i−1|wj, h(n)
i−1)
.
(3.273)
From the deﬁnition of h(n)
i−1 in Eq. (3.270), p(wi|h(n)
i−1) is represented as the following
n-gram probability:
p(wi|h(n)
i−1) = p(wi|wi−1
i−n+1).
(3.274)
This can be calculated based on Section 3.6. Next, we focus on the distribution of the
long-term topic information p(h(l)
i−1|wi, h(n)
i−1) in Eq. (3.273). By using the deﬁnitions of
h(l)
i−1 and h(n)
i−1 in Eqs. (3.270) and (3.271), the distribution can be rewritten as:
p(h(l)
i−1|wi, h(n)
i−1) = p(ωi−1|wi
i−n+1) ≈p(ωi−1|wi).
(3.275)
The above approximation assumes that the document vector ωi−1 only depends on wi
and does not depend on the word history wi−1
i−n+1, in accordance with the discussion in
Bellegarda (2000). This approximation would be effective when wi is a content word. In
addition, by using the Bayes theorem, p(ωi−1|wi) can be further rewritten as follows:
p(ωi−1|wi) ∝p(wi|ωi−1)
p(wi)
,
(3.276)
where p(wi) is easily calculated from a unigram probability. The p(wi|ωi−1) is a uni-
gram probability given the document vector. Thus, the LSA language model is ﬁnally
represented as follows:
pLSA(wi|wi−1
i−n+1) =
p(wi|wi−1
i−n+1) p(wi|ωi−1)
p(wi)

wj p(wj|wi−1
i−n+1) p(wj|ωi−1)
p(wj)
.
(3.277)

118
Statistical models in speech and language processing
Here, how to compute p(wi|ωi−1) is an important problem. This is performed in the
projected LSA space, rather than the original high dimensional space.
The p(wi|ωi−1) is determined by the cosine measure between word wi and pseudo
document wi−1
1
in the LSA space, as we discussed in Eq. (3.261). First, wi is converted
with the corresponding co-occurrence vector in the |V| dimensional space as
ωwi = [0, · · · , 0,
v
∨
1, 0, · · · , 0]⊺.
(3.278)
Vector ωwi is a one shot vector in the |V| dimensional space, where the element is 1
when wi = w(v) and 0 otherwise. The document vector in the LSA space for ωwi is
obtained from Eq. (3.262) as
˜vwi = Svwi = U⊺ωwi.
(3.279)
Therefore, based on this equation and Eq. (3.272), the cosine similarity in the LSA
K-dimensional space is obtained as:
SimK(wi, h(l)
i−1) = cos
˜vwi, ˜vi−1

.
(3.280)
However, since a cosine value goes to negative, it cannot be used as a probability of
p(wi|ωi−1) that must satisfy the non-negativity. Coccaro & Jurafsky (1998) propose the
following value as a probabilistic distribution:
p(wi|ωi−1) =
(
cos
˜vwi, ˜vi−1

−minw′
i∈V cos
(
˜vw′
i, ˜vi−1
))γ
Z
,
(3.281)
where Z is a normalization constant. The minimum cosine similarity prevents a negative
value, and it is also scaled by a tuning parameter γ .
In Eq. (3.280), the pseudo document vector ωi−1 is recursively updated from wi−1 to
wi by using
ωi = ni −1
ni
ωi−1 + 1 −εwi
ni
ωwi,
(3.282)
where ni denotes the number of words appearing in word sequence wi
1:
ni ≜
|V|

v=1
c(w(v), wi
1),
(3.283)
and εwi is a normalized entropy, as we deﬁned in Eq. (3.269). Eq. (3.282) is obtained as
a pseudo document vector in matrix W, which is deﬁned according to the entry value of
matrix W as given in Eq. (3.268). This updating is equivalent to computing
˜vi = Svi = 1
ni
[(ni −1)˜vi−1 + (1 −εwi)˜vwi].
(3.284)
This equation is obtained in accordance with Eqs. (3.272) and (3.279). Having the updat-
ing formula for ωi−1 or ˜vi−1 for pseudo document vector or history words wi−1
1
and the
projection vector ˜vwi of current word wi in common topic space, we calculate the n-gram

3.7 Latent semantic information
119
based on topic information in Eq. (3.280) and substitute it into an LSA language model
according to Eq. (3.273).
Thus, we can consider a long-term effect in a language model by using LSA. How-
ever, to integrate it with a language model, we need to provide a probabilistic treatment
for LSA, e.g., Eq. (3.281), which is a rather heuristic approach. The next section intro-
duces a fully probabilistic formulation of LSA, PLSA with an elegant inference and
statistical learning methods based on the EM algorithm.
3.7.3
Probabilistic latent semantic analysis
LSA was extended to the probabilistic latent semantic analysis (PLSA) and applied for
document representation in Hofmann (1999b) and Hofmann (2001). We ﬁrst introduce
a Jm-length word sequence, given a document dm, as
wJm
1 = {w1, · · · , wi, · · · , wJm} = {wi ∈V|i = 1, · · · , Jm} for m = 1, · · · , M.
(3.285)
We also introduce a set of M documents as
dM
1 = {d1, · · · , dM} = {dm|m = 1, · · · , M}.
(3.286)
The ﬁrst step of the ML procedure within the statistical model framework is to provide
a joint likelihood function of D of the word sequence and document, which can be
represented as:
p(D) = p({wJm
1 }M
m=1, dM
1 ),
(3.287)
where D is a set of the word sequences and documents in a corpus, which is deﬁned as:
D ≜{{wJm
1 }M
m=1, dM
1 }.
(3.288)
Equation (3.287) can be factorized by using the product rule and the conditional
independence assumption of a word generative model given document dm as:
p(D) = p({wJm
1 }M
m=1|dM
1 )p(dM
1 )
=
M

m=1
p(wJm
1 |dm)p(dm).
(3.289)
This can be modeled by the word-document representation (e.g., word-document matrix
W in Eq. (3.255)), as we discussed in Section 3.7.1. However, similarly to the LSA
case, this representation has too many sparse variables, and it is difﬁcult to deal with
this representation directly. Instead, we introduce a latent topic variable k ∈{1, · · · , K},
where K is the number of topics and each topic corresponds to a higher concept of words
(e.g., politics and sports, if we deal with news articles). This model is called a latent
topic model. Then we also introduce a latent topic sequence zJm
1
for a corresponding
word sequence wJm
1 for a document dm as
zJm
1 = {zi ∈{1, · · · , K}|i = 1, · · · , Jm}
Z = {zJm
1 }M
m=1.
(3.290)

120
Statistical models in speech and language processing
Then, we model that the words wJm
1 are conditionally independent given the correspond-
ing topic zJm
1 , and the probability of wi given dm is represented by the following mixture
model:
p(wi|dm) =
K

k=1
p(wi|zi = k)p(zi = k|dm).
(3.291)
This model factorizes the word probability into the topic-dependent unigram probabil-
ity p(wi|zi = k) and the topic proportion probability p(zi = k|dm). This factorization
corresponds to representing the huge size of a |V| × M word-document matrix with the
reduced subspace K used in an LSA, as discussed in Section 3.7.1.
Thus, we can construct a likelihood function of D as
p(D|) =
M

m=1
Jm

i=1
K

k=1
p(wi|zi = k)p(zi = k|dm)p(dm).
(3.292)
This model for representing a generative process of the word sequences and docu-
ments is called probabilistic latent semantic analysis (PLSA). Here, the PLSA model
parameters  = {p(w(v)|k), p(k|dm)} consist of two sets of multinomial parameters. The
complete data likelihood function of D and Z can be represented as:
p(D, Z|) =
M

m=1
Jm

i=1
p(wi|zi)p(zi|dm)p(dm).
(3.293)
Figure 3.10 provides a graphical model of PLSA and Algorithm 6 provides a generative
process of PLSA.
Algorithm 6 Generative process of probabilistic latent semantic analysis.
Require: M, Jm, ωd
m, ωz
k, ωw
kv
1: for m = 1, · · · , M do
2:
Draw dm from Mult(·|{ωd
m}M
m=1)
3:
for i = 1, · · · , Jm do
4:
Draw zi from Mult(·|{ωz
k}K
k=1)
5:
Draw wi from Mult(·|{ωw
kv}|V|
v=1)
6:
end for
7: end for
Parameter estimation based on EM algorithm
Once we obtain the likelihood function, maximum likelihood (ML) theory is applied to
estimate PLSA parameters by
ML = arg max
 p(D|).
(3.294)
Since PLSA has a latent variable z, it can be efﬁciently solved by using the EM algo-
rithm, as we discussed in Section 3.4. The EM algorithm is introduced to ﬁnd ML

3.7 Latent semantic information
121
Figure 3.10
Graphical model of probabilistic latent semantic analysis.
estimates ML. In the E-step of the EM algorithm, we calculate an auxiliary function of
new parameters ′ given the current parameters . The function is obtained by replac-
ing HMM latent variables S and V, and speech feature observation O with PLSA latent
variable Z and document observation D, giving
QML(′|) = E(Z)[log p(D, Z|′)|D, ]
=

Z
p(Z|D, ) log p(D, Z|′).
(3.295)
By substituting Eq. (3.293) into Eq. (3.295), the auxiliary function can be rewritten as
QML(′|) =

Z
p(Z|D, ) log
 M

m=1
Jm

i=1
p′(wi|zi)p′(zi|dm)p′(dm)

=

Z
p(Z|D, )
M

m=1
Jm

i=1
log p′(wi|zi)p′(zi|dm)p′(dm),
(3.296)
where p′(·) means a multinomial distribution with parameter ′. By using a similar trick
to that used in Eq. (3.102), we can change the order of the summations, and by executing
the summation over Z\zi, Eq. (3.296) can be further rewritten as
QML(′|) =
M

m=1
Jm

i=1

Z
p(Z|D, ) log

p′(wi|zi)p′(zi|dm)p′(dm)

=
M

m=1
Jm

i=1

zi
p(zi|D, ) log

p′(wi|zi)p′(zi|dm)p′(dm)

=
M

m=1
Jm log p′(dm) +
M

m=1
Jm

i=1

zi
p(zi|D, ) log

p′(wi|zi)p′(zi|dm)

.
(3.297)
PLSA assumes that the above probabilistic distributions p′(wi|zi) and p′(zi|dm)
are
represented
by
using
the
multinomial
distribution,
which
is
deﬁned
in
Appendix C.2 as:

122
Statistical models in speech and language processing
Mult(xj|{ωj}J
j=1) ≜ωj.
(3.298)
Therefore, the topic-dependent unigram distribution p′(wi = w(v)|zi = k) and topic
proportion distribution p′(zi = k|dm) are parameterized as:
p′(wi = w(v)|zi = k) = ω′
vk
p′(zi = k|dm) = ω′
km.
(3.299)
Thus, the parameter  is represented as the following set of these multinomial
distributions:
′ ≜{ω′
vk, ω′
km|v = 1, · · · , |V|, k = 1, · · · , K, m = 1, · · · , M}.
(3.300)
Now, we focus on the posterior distribution of latent variable p(zi|D, ). From the
graphical model, zi has the following conditional independence property:
p(zi = k|D, ) = p(zi = k|wi = w(v), dm, )
= p(k|w(v), dm, ).
(3.301)
The second probability means that the probability of topic k is given by the word w(v)
with dm and , and it is iid for the position i in dm. Then, we can rewrite Eq. (3.297) as
the following equation:
QML(′|)
=
M

m=1
Jm log p′(dm) +
M

m=1
|V|

v=1
c(w(v), dm)
K

k=1
p(k|w(v), dm, ) log

ω′
vkω′
km

,
(3.302)
where c(w(v), dm) is a co-occurrence count of word w(v) in document dm. Note that the
summation over words in a document Jm
i=1 is replaced with the summation over distinct
words within a dictionary |V|
v=1. Equation (3.302) is factorized into the topic-dependent
unigram parameter ω′
vk and the topic proportion parameter ω′
km as follows:
QML(′|) =
M

m=1
Jm log p′(dm)
+
M

m=1
|V|

v=1
c(w(v), dm)
K

k=1
p(k|w(v), dm, ) log

ω′
vk




≜QML({ω′
vk}|{ωvk})
+
M

m=1
|V|

v=1
c(w(v), dm)
K

k=1
p(k|w(v), dm, ) log

ω′
km




≜QML({ω′
km}|{ωkm})
≜
M

m=1
Jm log p′(dm) + QML({ω′
vk}|{ωvk}) + QML({ω′
km}|{ωkm}).
(3.303)

3.7 Latent semantic information
123
Two multinomial parameters ωvk and ωkm should meet the constraints for probability
parameters,
|V|

v=1
ωvk = 1
K

k=1
ωkm = 1.
(3.304)
Thus, Eq. (3.302) provides the auxiliary function of a PLSA with the parameter con-
straint Eq. (3.304). Therefore, similarly to Section 3.4, we can iteratively estimate
parameters based on the EM algorithm.
M-step
In the M-step, we maximize the extended auxiliary function with respect to new param-
eters ′ under constraints of Eq. (3.304). Again, as we discussed in Section 3.4.3, this
constrained optimization problem is solved through introducing a Lagrange multiplier
η and establishing the extended auxiliary function of QML({ω′
vk}|{ωvk} in Eq. (3.303) as
¯Q({ω′
vk}|{ωvk})
=
M

m=1
|V|

v=1
c(w(v), dm)
K

k=1
p(k|w(v), dm, ) log

ω′
vk

+ η
⎛
⎝
|V|

v=1
ω′
vk −1
⎞
⎠.
(3.305)
By differentiating Eq. (3.305) with respect to ω′
vk and setting it to zero, we obtain
∂¯Q({ω′
vk}|{ωvk})
∂ω′
vk
=
M

m=1
c(w(v), dm)p(k|w(v), dm, ) 1
ω′
vk
+ η = 0
⇒ω′
vk = −1
η
M

m=1
c(w(v), dm)p(k|w(v), dm, ).
(3.306)
Again, substituting this equation into constraint Eq. (3.304), we obtain the value of
Lagrange multiplier η,
|V|

v=1
ω′
vk = −1
η
|V|

v=1
M

m=1
c(w(v), dm)p(k|w(v), dm, ) = 1
⇒η = −
|V|

v=1
M

m=1
c(w(v), dm)p(k|w(v), dm, ).
(3.307)
Consequently, we ﬁnd the following ML estimate of ω′
vk (i.e., the topic-dependent
unigram proportion probability) for a PLSA topic model:
pML′(wi = w(v)|zi = k) = ω′
vk =
M
m=1 c(w(v), dm)p(k|w(v), dm, )
|V|
v=1
M
m=1 c(w(v), dm)p(k|w(v), dm, )
.
(3.308)

124
Statistical models in speech and language processing
Similarly, the parameter of topic proportion probability ω′
km is estimated based on the
M-step as
pML′(zi = k|dm) = ω′
km =
|V|
v=1 c(w(v), dm)p(k|w(v), dm, )
K
k′=1
|V|
v=1 c(w(v), dm)p(k′|w(v), dm, )
=
|V|
v=1 c(w(v), dm)p(k|w(v), dm, )
|V|
v=1 c(w(v), dm)
.
(3.309)
Thus, we obtain the ML solutions of the PLSA parameters based on the EM algorithm.
E-step
The E-step needs to consider the following posterior distribution of the latent topic k
with the previously estimated PLSA parameter :
p(k|w(v), dm, ).
(3.310)
By using the sum and product rules and the conditional independence (Eq. (3.291)), the
posterior distribution is rewritten as
p(k|w(v), dm, ) =
p(w(v), k|dm, )
K
k′=1 p(w(v), k′|dm, )
=
p(w(v)|k, )p(k|dm, )
K
k′=1 p(w(v)|k′, )p(k′|dm, )
.
(3.311)
By using the multinomial parameters ωvk and ωkm, this can be represented by
p(k|w(v), dm, ) =
ωvkωkm
K
k′=1 ωvk′ωk′m
.
(3.312)
Thus, the posterior distribution can be computed by using the previously estimated
PLSA parameter . We summarize the parameter estimation algorithm of the PLSA
in Algorithm 7. In a practical implementation, storing the above posterior distribu-
tion p(k|w(v), dm, ) explicitly needs a huge size of memory (that corresponds to store
M × |V| × K data), and it is impossible to do that for large-scale text data. Instead,
the PLSA parameter update is performed by using a matrix multiplication based oper-
ation. Actually, this PLSA parameter update corresponds to the multiplicative matrix
update of non-negative matrix factorization (NMF) (Lee & Seung 1999) based on the
KL divergence cost function (Gaussier & Goutte 2005).
PLSA is another well-known generative model in addition to the n-gram model that
generates words with exponential family distributions (multinomial distribution). Since
the generative model is represented with exponential family distributions, PLSA can be
extended by using Bayesian approaches, as we discussed in Section 2.1.3. This exten-
sion is called the latent Dirichlet allocation (Blei et al. 2003), where we use a Dirichlet
distribution as a conjugate prior, which is discussed in Section 7.6. We also show that
PLSA parameters can be estimated efﬁciently by using the EM algorithm. However,
PLSA does not consider the short-span language property, unlike an n-gram model, and

3.7 Latent semantic information
125
the next section discusses how to involve the PLSA and n-gram for the use of a language
model.
Algorithm 7 EM algorithm for PLSA
Require:  ←init, {c(w(v), dm)|v = 1, · · · , |V|, m = 1, · · · , M}
1: repeat
2:
for m = 1, · · · , M do
3:
for v = 1, · · · , |V| do
4:
for k = 1, · · · , K do
5:
Compute the posterior probability p(k|w(v), dm, )
6:
end for
7:
end for
8:
end for
9:
for v = 1, · · · , |V| do
10:
for k = 1, · · · , K do
11:
Compute the topic-dependent unigram probability
12:
pML′(wi = w(v)|zi = k) = ω′
vk
13:
end for
14:
end for
15:
for m = 1, · · · , M do
16:
for k = 1, · · · , K do
17:
Compute the topic proportion probability
18:
pML′(zi = k|dm) = ω′
km
19:
end for
20:
end for
21:
 ←′
22: until Convergence
3.7.4
PLSA language model
In Gildea & Hofmann (1999), Akita & Kawahara (2004) and Mrva & Woodland (2004),
a PLSA framework was applied to estimate the n-gram model, which is seen as a combi-
nation of large-span and short-span language models. That is, we consider the short-span
language model (n-gram)
p(wi|wi−1
i−n+1)
(3.313)
and long-span language model
p(wi|D) ≈
K

k=1
p(wi|k)p(k|D)
(3.314)
to obtain the probability of wi. As an alternative, a long-span language model p(wi|D),
a cache-based language model (Kuhn & De Mori 1990), or a trigger-based language

126
Statistical models in speech and language processing
model (Lau, Rosenfeld & Roukos 1993) are used as conventional approaches. As an
example of the document D, we consider the history of all words from the beginning
wi−1
1
. Given word sequence wi−1
1
, latent topic probabilities of the history p(k|wi−1
1
) are
inferred by using a PLSA model and incorporated into an n-gram language model.
To obtain the latent topic probabilities p(k|wi−1
1
), maximum likelihood PLSA param-
eters  = {p(w(v)|k), p(k|dm)} = {ωvk, ωkm|v = 1, · · · , |V|, k = 1, · · · , K, m =
1, · · · , M} are estimated in advance from a large amount of training documents in
the training step, as we discussed in the previous section. Then, given  as an initial
parameter set of Algorithm 7, we estimate p(k|wi−1
1
) in the test step. However, to avoid
over-training as we only use wi−1
i
and to avoid a high computational cost in the test step,
we only update pML′(zi = k|dm) = ω′
km with fewer iterations than the full estimation
of the PLSA parameter  in the training step. In addition, we use the following online
EM algorithm (Neal & Hinton 1998) to update topic posterior probabilities to make the
estimation in an on-line manner for every word position i:
p(k|wi−1
1
) =
1
i + 1
p(wi−1 = v|k)p(k|wi−2
1
)
K
k′=1 p(wi−1 = v|k′)p(k′|wi−2
1
)
+
i
i + 1p(k|wi−2
1
)
=
1
i + 1
ωvkp(k|wi−2
1
)
K
k′=1 ωvk′p(k′|wi−2
1
)
+
i
i + 1p(k|wi−2
1
).
(3.315)
Here, p(k|wi−2
1
) in the ﬁrst and second terms in the right-hand-side is obtained by the pre-
vious estimation, and the ﬁrst term is computed on-the-ﬂy based on a pre-computation
of p(wi−1|k), which is obtained from ωvk in Eq. (3.308). The factors
1
i+1 and
i
i+1 denote
a linear interpolation ratio, and the contribution of the second term becomes larger when
the length of the word sequence i is larger. These factors are derived as a speciﬁc solu-
tion of the on-line EM algorithm, and Mrva & Woodland (2004) suggest modifying
these factors for practical use.
In an initialization stage (i.e., wi=0
1
= ∅in Eq. (3.184)), the initial topic posterior
probability could be obtained from a prior topic probability in the training stage as
follows:
p(k|wi=0
1
) = p(k)
=
M

m=1
p(k|dm)p(dm)
=
M
m=1
|V|
v=1 c(w(v), dm)p(k|dm)
M
m=1
|V|
v=1 c(w(v), dm)
=
M
m=1
|V|
v=1 c(w(v), dm)ωkm
M
m=1
|V|
v=1 c(w(v), dm)
,
(3.316)
where p(dm) is estimated from the maximum likelihood equation based on the word
co-occurrence counts c(dm) = |V|
v=1 c(w(v), dm), i.e.,
p(dm) =
c(dm)
M
m=1 c(dm)
.
(3.317)

3.7 Latent semantic information
127
Once we obtain the topic proportion probability p(k|wi−1
1
), the PLSA based n-gram
probability is ﬁnally calculated by
pPLSA(wi = v|wi−1
1
) =
K

k=1
p(wi = v|k)p(k|wi−1
1
)
=
K

k=1
ωvkp(k|wi−1
1
).
(3.318)
Instead of hard-clustering computation in class-based n-gram in Eq. (3.198), PLSA
n-gram in Eq. (3.318) performs a so-called soft-clustering computation by marginal-
izing the topic index k in a Bayesian sense. This model is known as a large-span
language model because long-distance topics k = 1, · · · , K and history words wi−1
1
within the n-gram window as well as outside the n-gram window are all taken into
account for word prediction. However, since the computation can be performed by con-
sidering the history words wi−1
1
, it is not pre-computed, unlike n-gram models, which
makes the implementation of the PLSA language model harder than n-gram models in
ASR.
Smoothing with n-grams
PLSA n-grams could be further improved by combining with ML n-grams based on
additional linear interpolation with factor λ:
ˆp(wi|wi−1
1
) =λpML(wi|wi−1
i−n+1)
+ (1 −λ)pPLSA(wi|wi−1
1
).
(3.319)
The interpolation parameter λ could be found by maximizing likelihood or tuned by
maximizing the ASR performance of some validation data.
We can also use a unigram rescaling technique, which is approximated based on the
conditional independence assumption of wi−1
1
and wi−1
i−n+1:
ˆp(wi|wi−1
1
) ≈pML(wi|wi−1
i−n+1)pPLSA(wi|wi−1
1
)
pML(wi)
.
(3.320)
Note that this representation does not hold the sum-to-one condition, and it must be
normalized.
A more precise unigram rescaling technique can be performed by using the dynamic
unigram marginal (also known as the minimum discrimination information (MDI) adap-
tation), which can consider backoff probabilities in an n-gram probability (Kneser,
Peters & Klakow 1997, Niesler & Willett 2002, Tam & Schultz 2006). First, we deﬁne
the following unigram rescaling factor π for word wi:
π(wi, wi−1
1
) ≜

pPLSA(wi|wi−1
1
)
pML(wi)
ρ
.
(3.321)

128
Statistical models in speech and language processing
Then the new n-gram probability with a history of n-gram word sequence wi−1
i−n+1 is
represented as follows:
p(wi|wi−1
i−n+1, wi−1
1
) =
⎧
⎪⎨
⎪⎩
π(wi,wi−1
1
)
C0(wi−1
i−n+1,wi−1
1
)p(wi|wi−1
i−n+1)
if
c(wi
i−n+1) > 0
1
C1(wi−1
i−n+1,wi−1
1
)p(wi|wi−1
i−n+2, wi−1
1
)
otherwise,
(3.322)
where p(wi|wi−1
i−n+1) is an n-gram probability obtained in advance, and C0 and C1 are
normalization constants, which can be computed by:
C0(wi−1
i−n+1, wi−1
1
) =

{wi|c(wi
i−n+1)>0} π(wi, wi−1
1
)p(wi|wi−1
i−n+1)

{wi|c(wi
i−n+1)>0} p(wi|wi−1
i−n+1)
,
(3.323)
and
C1(wi−1
i−n+1, wi−1
1
) =
1 −
{wi|c(wi
i−n+1)>0} p(wi|wi−1
i−n+2, wi−1
1
)
1 −
{wi|c(wi
i−n+1)>0} p(wi|wi−1
i−n+1)
.
(3.324)
Then, p(wi|wi−1
i−n+2) is also iteratively calculated by π(wi, wi−1
1
), n −1-gram probability
p(wi|wi−1
i−n+2), and p(wi|wi−1
i−n+3). Thus, the unigram rescaled language model is obtained
by modifying the backoff coefﬁcients.
In this chapter, we address several language model methods tackling the issues of
small sample size and long-distance information. Different language model smoothing
methods including interpolation smoothing, backoff smoothing and unigram smoothing
are summarized. The Bayesian language modeling based on MAP estimation and VB
learning is addressed in Section 4.7 and Section 7.7, respectively.
3.8
Revisit of automatic speech recognition with Bayesian manner
In Chapter 2, we introduce the basic mathematical tools in the Bayesian approach,
including the sum and product rules, and the conditional independence, and provide
simple ways to obtain the posterior distributions. In addition, throughout the discus-
sions from Section 3.1 to Section 3.7, we also introduce statistical models of speech
and language, which are mainly used for ASR. Based on these mathematical tools and
statistical models, this section revisits methods to formulate the whole ASR framework
in a Bayesian manner, as consistently as possible, by only using the sum rule, product
rule, and conditional independence, as discussed in Section 2.1. In particular, we focus
on how to train our ASR model based on the Bayesian manner. The aim of this section is
to show the limitation of simply using basic Bayesian tools for ASR, which leads to the
requirement of approximated Bayesian inference techniques in the following chapters.
3.8.1
Training and test (unseen) data for ASR
To deal with the ASR problem within a machine learning framework, we need to con-
sider two sets of data: training data and test data. The training data usually consist of

3.8 Revisit of automatic speech recognition with Bayesian manner
129
a sequence of speech feature vectors (O) and the corresponding label sequence (W) for
all utterances. The test data consist of only a sequence of speech feature vectors (O′)
for all utterances, and there is a candidate label sequence (W′) among all possible word
sequences. The ASR problem is to correctly estimate the corresponding label sequence
( ˆW′). Therefore, we summarize these four variables as:
• O: speech feature sequence for training (observed);
• W: word sequence for training (observed);
• O′: speech feature sequence for test (observed);
• W′: a candidate of word sequences for test (not observed).
Since we have various possible candidates for W′, it is natural to consider the following
conditional probabilistic distribution of W′, given the other observed variables, as:
p(W′|O′, O, W).
(3.325)
Therefore, this section starts with the formulation based on this conditional probability.
Note that if we want to estimate the correct word sequence, it is performed by using
the well-known MAP decision rule, as we discussed in Eq. (3.2):
ˆW′ = dMAP(O′) = arg max
W′ p(W′|O′, O, W).
(3.326)
Therefore, the following section focuses on p(W′|O′, O, W) in more detail.
3.8.2
Bayesian manner
Recalling the discussion in Section 2.1, the Bayesian manner deals with these variables
as the arguments of probability distributions as follows:
O →p(O)
W →p(W)
O′ →p(O′)
W′ →p(W′).
(3.327)
We should also recall that these probabilistic variables (x: continuous variable and n:
discrete variable) have the following mathematical properties:
p(n) ≥0,
(3.328)

n
p(n) = 1,
(3.329)
p(x) ≥0,
(3.330)

p(x)dx = 1.
(3.331)
These properties yield various beneﬁts for probabilistic pattern recognition problems.
Based on this probabilistic variable treatment, we can apply the sum, product rules for
all the variables, and we can also use the Bayes theorem for Eq. (3.325).

130
Statistical models in speech and language processing
MAP decision rule
By replacing a with word sequence W′ and b with speech feature vector O′ in Bayes
theorem Eq. (2.7) in Section 2.1.1, we can derive the following noisy channel model of
speech recognition based on the well-known MAP decision rule (Eq. (3.2)):
ˆW′ = dMAP(O′) = arg max
W′ p(W′|O′),
(3.332)
= arg max
W′
p(O′|W′)p(W′)
p(O′)
,
(3.333)
= arg max
W′ p(O′|W′)p(W′).
(3.334)
Here, the denominator of Eq. (3.333) is disregarded with the arg max operation, since
p(O′) does not depend on W′.
The product rule (Bayes theorem) based on the MAP decision theory decomposes
the posterior distribution p(W′|O′) into the two generative models of O′ and W′, i.e.,
acoustic model p(O′|W′) and language model p(W′). Solving Eq. (3.332) by directly
obtaining p(W′|O′) is called the discriminative approach. The approach tries to infer
what a speaker wants to say in his/her brain from the observation O′, as shown in
Figure 3.11.
On the other hand, the generative approach obtains p(W′|O′) indirectly via the gen-
erative models p(O′|W′) and p(W′) based on Eq. (3.333). Therefore, the approach tries
to imitate the generation process of O′ given W′ in the acoustic model and W itself in
the language model, as shown in Figure 3.12. Since the generation process comes from
a physical phenomenon or linguistic phenomena, we can involve various knowledge
of the phenomena (e.g., articulatory models in speech production and grammatical or
semantic models in linguistics) in principle. The rest of this section further discusses the
Bayesian formulation along with the generative approach.
Figure 3.11
Probabilistic speech recognition: discriminative approach.
Figure 3.12
Probabilistic speech recognition: generative approach.

3.8 Revisit of automatic speech recognition with Bayesian manner
131
3.8.3
Learning generative models
Bayesian formulation reasonably extends the generative models to teach them from
training data, by adding training data (O and W) to conditional variables in the target
distribution, namely,
p(O′|W′) →p(O′|W′, O, W)
p(W′) →p(W′|O, W).
(3.335)
Usually, we also use the reasonable conditional independence assumption for the lan-
guage model expressed by p(W′|O, W) ≈p(W′|W). Now, we focus on the acoustic
model p(O′|W′, O, W), and make this abstract distribution more concrete.
3.8.4
Sum rule for model
To make the distribution more concrete, we usually provide a model with the distri-
bution. For example, when we have some data to analyze, we ﬁrst start to consider
what kind of models we use to make the problems concrete by considering HMMs or
GMMs for speech feature sequences, and n-gram for word sequences. However, we
do not know whether the model provided is correct or not, and it should be tested
by different model settings. Thus, the model settings can be regarded as probabilis-
tic variables in the Bayesian formulation. In addition, the variation of setting model
topologies, distribution forms of prior and posterior distributions, and hyperparam-
eters of these distributions can also be treated as probabilistic variables, as shown
in Table 3.2. We call this probabilistic variable to denote the model setting model
variable M.
For example, once we decide to use an HMM for speech feature modeling, we have
many arbitrary properties for the model topology: (i) whether we use a word unit,
context-independent phoneme unit, or context-dependent phoneme unit; (ii) left to right
HMM, including skip transitions, or fully connected ergodic HMM; (iii) how many
shared-triphone HMM states and how many Gaussians in these states. Some of the
model variables dealing with this model structure are called model structure variables,
and so-called structure learning in the machine learning ﬁeld tries to optimize the model
structure using a more general framework than the Bayesian framework.
Table 3.2 Examples of model variables.
Variation
Examples
Model types
HMM, GMM, SVM, neural network, etc.
Model topologies
# HMM states, # Gaussians, # hidden layers, etc.
Priors/posteriors
Gaussian, Laplace, Gamma, Bernoulli, etc.
Hyperparameters
parameters of priors/posteriors, kernel parameters, etc.

132
Statistical models in speech and language processing
The model variable M can be involved in our abstract distribution (Eq. (3.335)) of the
acoustic model by using the sum rule (Eq. (2.3)) as follows:
p(O′|W′, O, W) =

M
p(O′, M|W′, O, W).
(3.336)
As M would include continuous values (e.g., hyperparameters), the marginalization over
M should involve both summation and integration. However, for simplicity, we use the
summation in this formulation.
3.8.5
Sum rule for model parameters and latent variables
Once we set a model variable to a speciﬁc value, we can also provide the correspond-
ing model parameters  and latent variables Z for training data and Z′ for test data.
For example, once we decide a model for the speciﬁc setting based on a standard
HMM–GMM with a ﬁxed model topology, these variables can also be involved in the
distribution with M (Eq. (3.336)) by using the sum rule (Eq. (2.3)) as follows:
p(O′|W′, O, W) =


M,Z,Z′
p(O′, M, , Z, Z′|W′, O, W)d.
(3.337)
However, it is really difﬁcult to deal with this joint distribution, and we need to factorize
the distribution.
3.8.6
Factorization by product rule and conditional independence
First, we factorize Eq. (3.337) by using the product rule (Eq. (2.4)), as follows:
p(O′|W′, O, W) =


M,Z,Z′
p(O′, M, , Z, Z′|W′, O, W)d
=


M,Z,Z′
p(O′, Z′|Z, , M, W′, O, W)p(Z|, M, W′, O, W)
× p(|M, W′, O, W)p(M|W′, O, W)d.
(3.338)
In this formulation, we keep the joint distribution of O′ and Z′ and do not factorize them.
This is because the distribution corresponds to the complete data likelihood function,
which is useful to handle latent models based on the EM algorithm, as discussed in
Section 3.4.
Since the dependencies of the distributions in Eq. (3.338) are very complicated,
we use the reasonable conditional independence assumptions for these distributions as
follows:
p(O′, Z′|Z, , M, W′, O, W) ≈p(O′, Z′|, M, W′),
(3.339)
p(Z|, M, W′, O, W) ≈p(Z|, M, O, W),
(3.340)
p(|M, W′, O, W) ≈p(|M, O, W),
(3.341)
p(M|W′, O, W) ≈p(M|O, W).
(3.342)

3.8 Revisit of automatic speech recognition with Bayesian manner
133
Equation (3.339) means that the test data are generated by the  and M and do not
depend on the training data (O and W) and their latent variable Z, explicitly. The other
assumptions just use the conditional independence of W′. By using the assumptions, we
can approximate the original distribution of p(O′|W′, O, W) as follows:
Eq. (3.338) ≈


M,Z,Z′
p(O′, Z′|M, , W′)p(Z|, M, O, W)
× p(|M, O, W)p(M|O, W)d
≈
 
M,Z′
p(O′, Z′|M, , W′)p(|M, O, W)p(M|O, W)d,
(3.343)
where we use the fact that 
Z p(Z|, M, O, W) = 1 since Z does not depend on
the other distributions. We also introduce lexical category c, and further factorize the
equation as:
Eq. (3.343) ≈
 
M,Z′

c′
p(O′, Z′|M, , c′)

c
p(|M, O, c)p(M|O, c)d.
(3.344)
Thus, the acoustic model can be represented by the joint distribution of O′ and Z′ (com-
plete data likelihood) and the posterior distributions of model parameters  and model
M. Since p(O′, Z′|M, , c′) does not depend on training data (O and W), which can be
obtained by setting the model and its parameters, we only focus on the two posterior
distributions in Eq. (3.344).
3.8.7
Posterior distributions
The posterior distributions of models and model parameters can be rewritten by the
following equations by using the sum and product rules (Eqs. (2.3) and (2.4)).
• Model parameter :
p(|M, O) = p(O|, M)p(|M)
p(O|M)
,
(3.345)
∝

Z p(O, Z|, M)p(|M).
(3.346)
• Model M:
p(M|O) =
p(O|M)p(M)

M p(O|M)p(M),
(3.347)
∝

Z

p(O, Z|, M)p(|M)dp(M).
(3.348)
Note that the posterior distributions are represented by the following two types of
distributions:
• Joint distribution of training data O and Z:
p(O, Z|, M);
• Prior distributions of  and M :
p(|M) and p(M).

134
Statistical models in speech and language processing
Again, the joint distribution can be obtained by setting the model and its parameters.
Once we set the prior distributions p(|M) and p(M), we can obtain the posterior dis-
tributions p(|M, O) and p(M|O) by solving (3.346) and (3.348). Then, the acoustic
model likelihood can be computed by using Eq. (3.344).
3.8.8
Difﬁculties in speech and language applications
Although the Bayesian approach is powerful, it is very difﬁcult to realize. One of the
most critical aspects is that we cannot solve the above equations. The practical Bayesian
approach rests on how to ﬁnd appropriate approximations:
• Chapter 4: Maximum a-posteriori approximation;
• Chapter 5: Evidence approximation;
• Chapter 6: Asymptotic approximation;
• Chapter 7: Variational Bayes;
• Chapter 8: Markov chain Monte Carlo.
In the machine learning ﬁeld, other approximations are also studied actively (e.g., loopy
belief propagation (Murphy et al. 1999), expectation propagation (Minka 2001)). This
book introduces the above approximations in speech and language applications.

Part II
Approximate inference


4
Maximum a-posteriori approximation
Maximum a-posteriori (MAP) approximation is a well-known and widely used approx-
imation for Bayesian inference. The approximation covers all variables including
model parameters , latent variables Z, and classiﬁcation categories C (word sequence
W in the automatic speech recognition case). For example, the Viterbi algorithm
(arg maxZ p(Z|O)) in the continuous density hidden Markov model (CDHMM), as dis-
cussed in Section 3.3.2, corresponds to the MAP approximation of latent variables,
while the forward–backward algorithm, as discussed in Section 3.3.1, corresponds to
an exact inference of these variables. As another example, the MAP decision rule
(arg maxC p(C|O)) in Eq. (3.2) also corresponds to the MAP approximation of inferring
the posterior distribution of classiﬁcation categories. Since the ﬁnal goal of auto-
matic speech recognition is to output the word sequence, the MAP approximation
of the word sequence matches the ﬁnal goal.1 Thus, the MAP approximation can be
applied to all probabilistic variables in speech and language processing as an essential
technique.
This chapter starts to discuss the MAP approximation of Bayesian inference in detail,
but limits the discussion only to model parameters  in Section 4.1. In the MAP
approximation for model parameters, the prior distributions work as a regularization
of these parameters, which makes the estimation of the parameters more robust than
that of the maximum likelihood (ML) approach. Another interesting property of the
MAP approximation for model parameters is that we can easily involve the inference of
latent variables by extending the EM algorithm from ML to MAP estimation. Section
4.2 describes the general EM algorithm with the MAP approximation by following the
ML-based EM algorithm, as discussed in Section 3.4. Based on the general MAP–EM
algorithm, Section 4.3 provides MAP–EM solutions for CDHMM parameters, and intro-
duces the well-known applications based on speaker adaptation. Section 4.5 describes
the parameter smoothing method in discriminative training of the CDHMM, which actu-
ally corresponds to the MAP solution for discriminative parameter estimation. Section
4.6 focuses on the MAP estimation of GMM parameters, which is a subset of the MAP
estimation of CDHMM parameters. It is used to construct speaker GMMs that are used
1 However, if we consider some other spoken language processing applications given automatic speech
recognition inputs (e.g., dialog, machine translation, and information retrieval), we need to consider how
to provide p(W|O) rather than ˆW = arg maxW p(W|O) to avoid propagating any speech recognition errors
to the post-processing applications.

138
Maximum a-posteriori approximation
for speaker veriﬁcation. Section 4.7 provides an MAP solution of n-gram parameters
that leads to one instance of interpolation smoothing, as discussed in Section 3.6.2.
Finally, Section 4.8 deals with the adaptive MAP estimation of latent topic model
parameters.
4.1
MAP criterion for model parameters
This section begins with a general discussion of the MAP approximation for model
parameters . For simplicity, we ﬁrst review the posterior distribution of model param-
eters given observations O without latent variables Z. Instead of estimating posterior
distributions, the MAP estimation focuses on the following parameter estimation:
MAP = arg max
 p(|O).
(4.1)
This corresponds to estimating the model parameter MAP given training data O. By
using the product and sum rules, as discussed in Section 2.1.1, we can rewrite the above
equation as follows:
MAP = arg max
 p(|O)
= arg max

p(O|)p()

p(O|)p()d
= arg max

p(O|)
  
likelihood
× p()

prior
.
(4.2)
Since p(O) =

p(O|)p()d does not depend on , we can avoid computing
this integral directly.2 Furthermore, if we use an exponential family distribution for a
likelihood function and a conjugate distribution for a prior distribution, as discussed
in Section 2.1.3, the MAP estimate is represented as the mode of the corresponding
conjugate posterior distribution, analytically. This is an advantage of using conjugate
distributions.3
Equation (4.2) also tells us that the posterior distribution is composed of the likelihood
function and the prior distribution, thus the estimation is based on the maximum likeli-
hood function with the additional contribution of the prior distribution. That is, the prior
distribution acts to regularize model parameters in the ML estimation, as we discussed
in Section 2.3.1 as the best-known Bayesian advantage over ML. For example, let us
consider the likelihood function p(O|) = *
t N(ot|μ, 1) as a one-dimensional Gaus-
sian distribution with mean μ and precision as 1, and the prior distribution p() as a
2 This term is called the evidence, which is neglected in the MAP approximation. However, the importance
of the evidence is discussed in Chapter 5.
3 In other words, it is not simple to obtain the mode of the posterior distribution, if we do not use the
conjugate distribution, since we cannot obtain the posterior distribution analytically. For example, if we
use the Laplace distribution for the prior distribution, the mode of posterior distributions cannot be
obtained analytically, and we need some numerical computation.

4.1 MAP criterion for model parameters
139
one-dimensional Gaussian distribution of the mean vector μ0 and the scale parameter r.
Then, the MAP estimation can be represented as follows:
arg max
μ log p(O|) + log p()
= arg max
μ log

t
N(ot|μ, 1)

+ log N(μ|μ0, r−1)
= arg max
μ

t
(ot −μ)2 + r(μ −μ0)2,
(4.3)
where from the second to the third lines, we use the deﬁnition of a Gaussian distribution
(Appendix C.5) as follows:
N(x|μ, r−1) ≜(2π)−1
2 (r)
1
2 exp

−r(x −μ)2
2

.
(4.4)
Thus, the optimization problem of the MAP solution corresponds to solving the mini-
mum mean square error (MMSE) estimation with the l2 regularization term around μ0.
The scale parameter r behaves as a tuning parameter to balance the MMSE estima-
tion and the regularization term. These parameters are called regularization parameters,
which can be hyperparameters of the prior distribution. Equation (4.3) can be analyti-
cally solved by using the conjugate distribution rule, as discussed in Section 2.1.4, or by
using the following derivative method:
∂
∂μ
T

t=1
(ot −μ)2 + r(μ −μ0)2 = −2
T

t=1
(ot −μ) + 2r(μ −μ0)
= −2
 T

t=1
ot + rμ0

+ 2(T + r)μ = 0.
(4.5)
We obtain the MAP estimate of μ analytically as:
μMAP =
T
t ot + rμ0
T + r
= μML + r
T μ0
1 + r
T
.
(4.6)
Thus, the regularization term sets a constraint for the ML estimate μML =
T
t ot
T
with a
regularization constant r.
Similarly, if we use a Laplace distribution as a prior distribution, the prior distribu-
tion works as an l1 regularization term. The Laplace distribution is deﬁned as follows
(Appendix C.10):
Lap(x|μ, β) ≜1
2β exp

−|x −μ|
β

.
(4.7)

140
Maximum a-posteriori approximation
Therefore, using Lap(μ|μ0, β) instead of N(μ|μ0, r−1), Eq. (4.3) is rewritten as follows:
arg max
μ log p(O|) + log p()
= arg max
μ log

t
N(ot|μ, 1)

+ log Lap(μ|μ0, β)
= arg max
μ

t
(ot −μ)2 + 1
β |μ −μ0|.
(4.8)
Thus, the prior distribution effect in the MAP parameter estimation is often regarded as
a regularization of parameters. Consequently, Eq. (4.3) can incorporate the prior knowl-
edge of parameters via hyperparameters μ0, and the MAP approximation retains the
Bayesian advantage of use of prior knowledge, as discussed in Section 2.3.1. Note
that Eq. (4.8) is not differentiable with respect to μ, and it does not have a well-
deﬁned conjugate distribution. Therefore, the MAP estimation with the Laplace prior
(l1 regularization) is often undertaken by a numerical method.
Now, we introduce a useful mathematical operation for the MAP approximation of
model parameters. To compute the expected values of the posterior distribution with
respect to model parameters , the MAP approximation can use the following posterior
distribution represented by a Dirac delta function:
p(|O) = δ( −MAP),
(4.9)
where the Dirac delta function has the following property:

f(a)δ(a −a∗) = f(a∗).
(4.10)
This posterior distribution intuitively corresponds to having a location parameter with
the MAP estimate MAP and very small (0) variance. If the model parameters are rep-
resented by discrete variables, we can use the Kronecker delta function. Therefore, once
we obtain the MAP estimation of model parameters MAP, we can compute the expected
value of function f() based on Eq. (4.9) as follows:
E()[f()|O] =

f()p()d =

f()δ( −MAP)d
= f(MAP).
(4.11)
Here we use Eq. (4.10) to calculate the integral. Since this is equivalent to just plug-
ging in the MAP estimates to the f(), this procedure is called plug-in MAP (Lee &
Huo 2000). For example, if we use the likelihood function of unseen data O′ for f(),
Eq. (4.11) is rewritten as follows:
E()[p(O′|)|O] = p(O′|MAP).
(4.12)
That is, the likelihood function of unseen data can be obtained by simply replacing 
with the MAP estimate MAP. This can be used as a likelihood function to compute
likelihood values in prediction and classiﬁcation steps. The Dirac delta-function-based

4.2 MAP extension of EM algorithm
141
posterior representation is very useful, since the representation connects the analyti-
cal relationship between the MAP-based point estimation and the Bayesian distribution
estimation.
Thus, the MAP approximation does not need to solve the marginalization explic-
itly in the training and prediction/classiﬁcation steps. Although the approximation lacks
the Bayesian advantages of the model selection and marginalization, as discussed in
Section 2.3, it still has the most effective Bayesian advantage over ML, namely use of
prior knowledge. Equation (4.3) also shows that the effect of the prior distribution in the
MAP estimation works as a regularization. Therefore, the MAP approximation is widely
used in practical Bayesian applications. In addition, the MAP approximation is simply
extended to deal with latent variables based on the EM algorithm, which is a key tech-
nique in training statistical models in speech and language processing, as we discussed
in Chapter 3. The next section discusses the MAP version of the EM algorithm.
4.2
MAP extension of EM algorithm
As we discussed in Section 3.4, most statistical models used in speech and language
processing have to deal with latent variables Z, e.g., HMM states and mixture compo-
nents of the CDHMM in acoustic modeling, and latent topics in language modeling.
The maximum likelihood approach has an efﬁcient solution based on the EM algorithm,
which optimizes the auxiliary function Q(′|) instead of a (log) likelihood function.
This section describes the EM extension of MAP parameter estimation in general.
4.2.1
Auxiliary function
Following the discussion in Section 3.4, we prove that the EM steps ultimately lead to
the local optimum value MAP in terms of the MAP criterion. First, since the logarith-
mic function is a monotonic function, the MAP criterion in Eq. (4.2) is represented as
follows:
MAP = arg max
 p(O|)p()
= arg max
 log (p(O|)p()) .
(4.13)
By introducing latent variable Z, the above equation can be written as
MAP = arg max
 log

Z
p(O, Z|)p()

.
(4.14)
As discussed in the ML–EM algorithm, the summation over latent variable 
Z is com-
putationally very difﬁcult since the latent variable in speech and language processing
is represented as a possible sequence, and the number of these variables is exponential.
Therefore, we need to avoid having to compute 
Z directly.

142
Maximum a-posteriori approximation
Similarly to the ML–EM algorithm in Section 3.4, in M-step, we maximize the MAP
version of the auxiliary function with respect to the parameters , and estimate new
parameters by
MAP = arg max
′ QMAP(′|).
(4.15)
The updated parameters ′ are then treated as the current parameters for the next itera-
tion of EM steps. The QMAP(′|) is deﬁned as the expectation of the joint distribution
p(O, Z, ′) with respect to p(Z|O, ) as follows:
QMAP(′|) ≜E(Z)[log p(O, Z, ′)|O, ]
= E(Z)[log p(O, Z|′)p(′)|O, ]
= E(Z)[log p(O, Z|′)|O, ]



QML(′|)
+ log p(′),
(4.16)
where log p(′) does not depend on Z, and can be separated from the expectation oper-
ation. Compared with the ML auxiliary function QML(′|) (Eq. (3.78)), we have an
additional term log p(′), which comes from a prior distribution of model parameters.
Now we explain how optimization of the auxiliary function QMAP leads to the local
optimization of p(O|)p() or p(|O). For the explanation, we deﬁne the logarithmic
function of the joint distribution p(O, ′) = p(O|′)p(′) as follows:
LMAP(′) ≜log (p(O|)p()) .
(4.17)
This is similar to L(′) in Eq. (3.83), but LMAP(′) has an additional factor from p().
Now, we represent p(O|) in the above equation with the distributions of latent variable
Z based on the product rule of probabilistic variables, as follows:
p(O|′) = p(O, Z|′)
p(Z|O, ′).
(4.18)
Therefore, by substituting Eq. (4.18) into Eq. (4.17), we obtain
LMAP(′) = log p(O, Z|′) −log p(Z|O, ′) + log p(′).
(4.19)
Now we perform the expectation operation with respect to p(Z|O, ) for both sides of
the equation, and obtain the following relationship:
LMAP(′) = E(Z)[log p(O, Z|′)|O, ] −E(Z)[log p(Z|O, ′)|O, ] + log p(′)
= E(Z)[log p(O, Z|′)|O, ] + log p(′)



QMAP(′|)
−E(Z)[log p(Z|O, ′)|O, ]



H(′|)
,
(4.20)
where LMAP(′) and log p(′) are not changed since these do not depend on Z. Note
that the third term of Eq. (4.20) is represented as H(′|), which is exactly the same
deﬁnition as Eq. (3.85). Thus, we derive a similar equation to that of the ML auxiliary
function Eq. (3.86):
QMAP(′|) = L(′)MAP + H(′|).
(4.21)

4.3 Continuous density hidden Markov model
143
Since H(′|) is the same as in Eq. (3.86) and has a bound based on the Jensen’s
inequality, we can apply the same discussion to QMAP(′|) to show that QMAP(′|)
is the auxiliary function of the MAP criterion.
Thus, we prove that the E-step performing the expectation and M-step maximizing
the auxiliary function with respect to the model parameters , always increase the joint
likelihood value as:
MAP = arg max
′ QMAP(′|) ⇒p(O, MAP) ≥p(O, ).
(4.22)
This leads to a local optimization of the joint likelihood function p(O, ), which cor-
responds to the MAP criterion in Eq. (4.13). We call this procedure the MAP–EM
algorithm.
4.2.2
A recipe
Based on the previous discussions, we summarize in the text box a general procedure
to obtain the MAP estimation of model parameters. The following section describes the
concrete form of MAP–EM solutions for CDHMMs similarly to Section 3.4.
1. Set a likelihood function for a statistical model (generative model) with model
parameters.
2. Set appropriate prior distributions for model parameters (possibly conjugate
distributions to obtain analytical results based on the conjugate distribution
discussion in Section 2.1.4).
3. Solve the parameter estimation by maximizing the MAP objective
function:
i. Solve posterior distributions for model parameters when we can use
conjugate priors;
ii. Solve the parameter estimation by getting the modes of posterior
distributions.
4.3
Continuous density hidden Markov model
This section describes the MAP estimation of HMM parameters based on the
MAP–EM algorithm (Lee et al. 1991, Gauvain & Lee 1994). Following the gen-
eral procedure for MAP estimation (as set out in Section 4.2.2), we ﬁrst review a
likelihood function of the CDHMM, as discussed in Section 3.2.3. Then we pro-
vide a concrete form of the prior distribution p() for full and diagonal covari-
ance cases. Then, according to the derivation of the ML–EM algorithm in Section
3.4, we also derive the concrete form solutions of the MAP–EM algorithm of the
CDHMM.

144
Maximum a-posteriori approximation
4.3.1
Likelihood function
We ﬁrst provide the complete data likelihood function with speech feature sequence O =
{ot ∈RD|t = 1, · · · , T}, HMM state sequence S = {st ∈{1, · · · , J}|t = 1, · · · , T}, and
GMM component sequence V = {vt ∈{1, · · · , K}|t = 1, · · · , T}, which is introduced
in Eq. (3.43) as follows:
p(O, S, V|) = πs1ωs1v1p(o1|s1v1)
T

t=2
ast−1stωstvtp(ot|stvt).
(4.23)
Recall that a set of HMM parameters  holds:
• Initial state probability πj;
• State transition probability aij;
• Mixture weight ωjk;
• Gaussian mean vector μjk;
• Gaussian covariance matrix jk.
The next section provides prior distribution p().
4.3.2
Conjugate priors (full covariance case)
The prior distribution is considered to be the following joint distribution form:
p() = p
(
{πj}J
j=1), {{aij}J
i=1}J
j=1, {{ωjk, μjk, jk}J
j=1}K
k=1
)
.
(4.24)
However, since it is difﬁcult to handle this joint distribution, we usually factorize it by
assuming conditional independence for each HMM state and mixture component. Then,
the prior distribution is rewritten as follows:
p() = p(π)p(A)p(ω)p(μ, R)
= p({πj}J
j=1)
 J
i=1
p({aij}J
j=1)
 ⎛
⎝
J
j=1
p({ωjk}K
k=1)
⎞
⎠
⎛
⎝
J
j=1
K

k=1
p(μjk, jk)
⎞
⎠,
(4.25)
where we also assume that πj, aij, ωjk, and {μjk, jk} are independent of each other,
although we keep the dependency of μjk and jk.
Now we provide the concrete forms of the above prior distributions for the HMM
parameters based on the conjugate distribution discussion in Section 2.1.4. We ﬁrst focus
on the prior distributions of the initial state probability πj, state transition probability aij,
and Gaussian mixture weight ωjk. Note that these probabilistic variables have the same
constraint that πj ≥0, J
j=1 πj = 1, aij ≥0, J
j=1 aij = 1, and ωjk ≥0, K
k=1 ωjk = 1.
In addition, these are represented by a multinomial distribution in the complete data like-
lihood function. Therefore, according to Table 2.1, these are represented by a Dirichlet
distribution with hyperparameters as follows:

4.3 Continuous density hidden Markov model
145
p({πj}J
j=1) = Dir({πj}J
j=1|{φπ
j }J
j=1),
p({aij}J
j=1) = Dir({aij}J
j=1|{φa
ij}J
j=1),
p({ωjk}K
k=1) = Dir({ωjk}K
k=1|{φω
jk}K
k=1),
(4.26)
where φπ
j ≥0, φa
ij ≥0, and φω
jk ≥0.
Next we consider the prior distribution of Gaussian mean vector μjk and Gaussian
precision matrix jk. For simplicity, we focus on the precision matrix R, which is the
inverse matrix of the covariance matrix , i.e.,
Rjk ≜(jk)−1.
(4.27)
According to Table 2.1, the joint prior distribution of Gaussian mean vector μjk and
Gaussian precision matrix Rjk can be written as follows:
p(μjk, Rjk) = p(μjk|Rjk)p(Rjk)
= N(μjk|μ0
jk, (φμ
jkRjk)−1)W(Rjk|R0
jk, φR
jk),
(4.28)
where W(·) is a Wishart distribution, which is deﬁned in Appendix C.14. Note that the
prior distribution p(μjk|Rjk) of the mean vector depends on covariance matrix Rjk, and
cannot be factorized independently. Instead, these parameters are represented by the
joint prior distribution p(μjk, Rjk) with the Gaussian–Wishart distribution (Appendix
C.15) or the product form in Eq. (4.28), as we discussed in Section 2.1.4. We can
also provide the prior distribution for the original covariance matrix  by using the
inverse-Wishart distribution instead of the Wishart distribution in Eq. (4.28). Both
representations yield the same result in the MAP estimation of HMM parameters.
Consequently, the conjugate prior distribution of a CDHMM is represented by the
following factorization form with each parameter:
p() = Dir({πj}J
j=1|{φπ
j }J
j=1)
×
 J
i=1
Dir({aij}J
j=1|{φa
ij}J
j=1)
 ⎛
⎝
J
j=1
Dir({ωjk}K
k=1|{φω
jk}K
k=1)
⎞
⎠
×
⎛
⎝
J
j=1
K

k=1
N(μjk|μ0
jk, (φμ
jkRjk)−1)W(Rjk|R0
jk, φR
jk)
⎞
⎠.
(4.29)
Note that the prior distribution of a CDHMM is represented by three types of
distributions, i.e., Dirichlet, Gaussian, and Wishart distributions. The prior distribution
has ﬁve scalar hyperparameters φπ, φa, φω, φμ, φR, one vector hyperparameter μ0, and
one matrix hyperparameter R0. A set of these hyperparameters is written as  in this
chapter, i.e.,
 ≜{φπ
j , φa
ij, φω
jk, φμ
jk, φR
jk, μ0
jk, R0
jk|i = 1, · · · , J, j = 1, · · · , J, k = 1, · · · , K}.
(4.30)
In the following sections, we sometimes represent the prior distribution as p(|)
instead of p() to deal with the hyperparameter dependency on the prior distributions
explicitly.

146
Maximum a-posteriori approximation
4.3.3
Conjugate priors (diagonal covariance case)
In practical use, the Gaussians in a CDHMM are often represented by a diagonal covari-
ance matrix, as we discussed in Section 3.2.3. To deal with a conjugate distribution of
a diagonal covariance matrix, we need to provide a speciﬁc distribution rather than the
Wishart distribution since the off-diagonal elements are always zero, and it is not suit-
able to represent these random variables as the Wishart distribution. Instead, we use the
gamma distribution for each diagonal component. We ﬁrst deﬁne the d −d element of
the precision matrix as rd:
rd ≜[R]dd.
(4.31)
Then the joint prior distribution of Eq. (4.28) is factorized by a feature dimension, and
it is replaced with the gamma distribution as follows:
p(μjk, Rjk) =
D

d=1
p(μjkd|rjkd)p(rjkd)
=
D

d=1
N(μjkd|μ0
jkd, (φμ
jkrjkd)−1) Gam(rjkd|r0
jkd, φR
jk),
(4.32)
where a set of hyperparameters  is represented as
 ≜{φπ
j , φa
ij, φω
jk, φμ
jk, φR
jk, μ0
jk, r0
jk|i = 1, · · · , J, j = 1, · · · , J, k = 1, · · · , K},
(4.33)
where
r0 ≜[r0
1, · · · , r0
D]⊺.
(4.34)
Similarly to the full covariance case, Eq. (4.32) can also be represented by a Gaussian-
gamma distribution (Appendix C.13).
The dependency of the hyperparameter φ is not unique and can be arranged by
considering applications due to the ﬂexible parameterization of an exponential fam-
ily distribution, as discussed in Section 2.1.3. For example, φμ
jk and φR
jk can be changed
depending on a dimension d (i.e., φμ
jkd, φR
jkd). These make the model more precise, but
need more effort to set φμ
jkd, φR
jkd for all dimensions manually or automatically. Actually,
these values are often shared among all js and ks (i.e., φμ
jk →φμ etc.).
4.3.4
Expectation step
Once we set the prior distributions and likelihood function, by following the recipe in
Section 4.2.2, we can perform the MAP–EM algorithm to estimate the model param-
eter . This section considers the concrete form of the MAP expectation step. This
procedure is very similar to Section 3.4 except for the additional consideration of
the prior distribution p(). The auxiliary function used in the MAP–EM algorithm is
represented as
QMAP(′|) = QML(′|) + log p(′).
(4.35)

4.3 Continuous density hidden Markov model
147
According to Section 3.4, the auxiliary function QML(′|) is factorized by a sum of
four individual auxiliary functions as:
QML(′|) =

S

V
p(S, V|O, )

log π′
s1 + log ω′
s1v1 + log p(o1|μ′
s1v1, ′
s1v1)
+
T

t=2
(
log a′
st−1st + log ω′
stvt + log p(ot|μ′
stvt, ′
stvt)
)
= QML(π′|π) + QML(A′|A) + QML(ω′|ω) + QML(μ′, R′|μ, R).
(4.36)
Similarly, from Eq. (4.29), the prior distribution of all model parameters p() can be
decomposed into the four individual prior distributions as
log p() = log
(
Dir({πj}J
j=1|{φπ
j }J
j=1)
)



≜p(π)
+ log
J
i=1
(
Dir({aij}J
j=1|{φa
ij}J
j=1)
)



≜p(A)
+ log
J
i=1
(
Dir({ωjk}K
k=1|{φω
jk}K
k=1)
)



≜p(ω)
+ log
J
j=1
K

k=1
(
N(μjk|μ0
jk, (φμ
jkRjk)−1)W(Rjk|R0
jk, φR
jk)
)



≜p(μ,R)
.
(4.37)
Therefore, by using the factorization forms of Eqs. (4.36) and (4.37), similarly to the
ML case, Eq. (4.36) is also represented as a sum of four individual auxiliary functions
deﬁned as follows:
QMAP(′|) = QMAP(π′|π) + QMAP(A′|A)
+ QMAP(ω′|ω) + QMAP(μ′, R′|μ, R),
(4.38)
where
QMAP(π′|π) = QML(π′|π) + log p(π)
=
J

j=1
ξ1(j) log π′
j + log
(
Dir({πj}J
j=1|{φπ
j }J
j=1)
)
,
(4.39)
QMAP(A′|A) = QML(A′|A) + log p(A)
=
T

t=1
J

i=1
J

j=1
ξt(i, j) log a′
ij +
J

i=1
log
(
Dir({a′
ij}J
j=1|{φa
ij}J
j=1)
)
,
(4.40)

148
Maximum a-posteriori approximation
QMAP(ω′|ω) = QML(ω′|ω) + log p(ω)
=
T

t=1
J

j=1
K

k=1
γt(j, k) log ω′
j,k +
J

j=1
log
(
Dir({ω′
jk}K
k=1|{φω
jk}K
k=1)
)
, (4.41)
QMAP(μ′, R′|μ, R) = QML(μ′, R′|μ, R) + log p(μ, R)
∝
T

t=1
J

j=1
K

k=1
γt(j, k)
2

log |R′
jk| −(ot −μ′
jk)⊺R′
jk(ot −μ′
jk)

+
J

j=1
K

k=1
log
(
N(μ′
jk|μ0
jk, (φμ
jkR′
jk)−1)W(R′
jk|R0
jk, φR
jk)
)
,
(4.42)
where ξ1(j), ξt(i, j), and γt(j, k) are the posterior probabilities, which are introduced in
Eqs. (3.99), (3.118), (3.119) as follows:
ξ1(j) ≜p(s1 = j|O, )
ξt(i, j) ≜p(st = i, st+1 = j|O, )
γt(j, k) ≜p(st = j, vt = k|O, ).
(4.43)
Note that  is estimated by using the MAP estimation MAP for  instead of the ML
estimation, which is discussed in Section 4.3.6.
We can also obtain the auxiliary function of diagonal covariance Gaussians instead of
Eq. (4.42) by using a Gaussian-gamma distribution as a prior distribution, as discussed
in Eq. (4.32) as follows:
QMAP(μ′, R′|μ, R)
∝
T

t=1
J

j=1
K

k=1
D

d=1
γt(j, k)
2
+
log r′
jk −(otd −μ′
jkd)2r′
jkd
,
+
J

j=1
K

k=1
D

d=1
log
(
N(μ′
jkd|μ0
jkd, (φμ
jkr′
jkd)−1)Gam2
(
r′
jkd
---φR
jk, rjkd
))
.
(4.44)
Here we use the gamma distribution Gam2(y|φ, r0) described in Eq. (C.81) instead of
the original gamma distribution deﬁned in Eq. (C.74), which provides a good relation-
ship with the Wishart distribution, i.e., if R is a scalar value (the number of dimension
D = 1), the hyperparameters of the Wishart distribution become the same as φr
jkd and
r0
jk,4 as we discussed in Example 2.6. Note that the vector and matrix operations in
Eq. (4.42) are represented as scalar operations with the summation over the dimension.
This is a very good property for which to obtain the analytical solutions due to the
simplicity of the scalar calculations. In addition, this representation avoids vector and
matrix computations, which also makes implementation simple. This section provides
4
We can set a hyperparameter φ which depends on each element d, i.e., φR
jk →φR
jkd. Considering the
compatibility with the Wishart distribution, this book uses φR
jk, which is independent of d.

4.3 Continuous density hidden Markov model
149
both full and diagonal covariance solutions, but the diagonal covariance solution is used
for most of our applications.
4.3.5
Maximization step
The maximization step in the ML–EM algorithm obtains the maximum values of param-
eters by using derivative techniques, as discussed in Section 3.4.3. In this section, we
provide other solutions for this problem which:
1. Calculate the posterior distributions;
2. Obtain the mode values of the posterior distributions, which are used as the MAP
estimates.
In general, it is difﬁcult to analytically obtain the posterior distributions. However, since
we use the conjugate prior distributions of a CDHMM, as discussed in Section 2.1.3, we
can easily obtain the posterior distributions for these problems.
Initial weight
We ﬁrst focus on QMAP(π′|π) in Eq. (4.39):
QMAP(π′|π) =
J

j=1
ξ1(j) log π′
j + log
(
Dir({π′
j}J
j=1|{φπ
j }J
j=1)
)
.
(4.45)
Recall that the Dirichlet distribution (Appendix C.4) is represented as follows:
Dir({πj}J
j=1|{φπ
j }J
j=1) = CDir({φπ
j }J
j=1)
J
j=1
(πj)φπ
j −1.
(4.46)
Then, by substituting Eq. (4.46) into Eq. (4.45), Eq. (4.45) is re-written as
follows:
QMAP(π′|π) =
J

j=1
ξ1(j) log π′
j + (φπ
j −1) log π′
j + log CDir({φπ
j }J
j=1)
=
J

j=1
(ξ1(j) + φπ
j −1) log π′
j + log CDir({φπ
j }J
j=1)
= log
J
j=1
(π′
j)ξ1(j)+φπ
j −1 + log CDir({φπ
j }J
j=1).
(4.47)
Comparing the result with Eq. (4.46), it is the same function form with different
hyperparameters. Therefore, the auxiliary function QMAP(π′|π) is represented by the
following Dirichlet distribution:
QMAP(π′|π) = log
(
Dir({π′
j}J
j=1|{ ˆφπ
j }J
j=1)
)
−log CDir({ ˆφπ
j }J
j=1)
+ log CDir({φπ
j }J
j=1)

150
Maximum a-posteriori approximation
= log
(
Dir({π′
j}J
j=1|{ ˆφπ
j }J
j=1)
)
+ log
CDir({φπ
j }J
j=1)
CDir({ ˆφπ
j }J
j=1)
∝log
(
Dir({π′
j}J
j=1|{ ˆφπ
j }J
j=1)
)
,
(4.48)
where
ˆφπ
j ≜φπ
j + ξ1(j).
(4.49)
We ﬁnally omit the ratio of the normalization factor of the prior and posterior distribu-
tions, which do not depend on π′
j. Actually, this Dirichlet distribution corresponds to
the posterior distribution of π with new hyperparameter ˆφ. This result is similar to that
of the conjugate prior and posterior distributions for multinomial likelihood function, as
we discussed in Example 2.8.
Once we obtain the analytical form of the posterior distribution, the MAP estimate
can be obtained as the mode of the Dirichlet distribution (Appendix C.4):
πMAP
j
=
φπ
j + ξ1(j) −1
J
j′=1(φπ
j′ + ξ1(j′) −1)
.
(4.50)
Thus, we obtain the MAP estimate of the initial weight πMAP, which is proportional to
the hyperparameter ˆφπ. We discuss the meaning of this solution in Section 4.3.7.
State transition
Similarly, the auxiliary function of state transition parameters A is obtained as follows:
QMAP(A′|A) = log
 J
i=1
Dir({a′
ij}J
j=1|{ ˆφa
ij}J
j=1)

+ log
 J
i=1
CDir({φa
ij}J
j=1)
CDir({ ˆφa
ij}J
j=1)

∝log
 J
i=1
Dir({a′
ij}J
j=1|{ ˆφa
ij}J
j=1)

,
(4.51)
where
ˆφa
ij ≜φa
ij +
T−1

t=1
ξt(i, j).
(4.52)
Therefore, the mode of the Dirichlet distribution is obtained as:
aMAP
ij
=
φa
ij + T−1
t=1 ξt(i, j) −1
J
j′=1(φa
ij′ + T−1
t=1 ξt(i, j′) −1)
.
(4.53)
The solution is similar to the initial weight in Eq. (4.50), and it is computed from the
statistics of the accumulated posterior values of the state transition T−1
t=1 ξt(i, j) and
prior parameter φa
ij.
Again, the result indicates that the auxiliary function of the state transition is rep-
resented by the same Dirichlet distribution as that used in the prior distribution with
different hyperparameters. This result corresponds to the conjugate distribution analy-
sis for multinomial distribution, as we discussed in Section 2.8. Therefore, although we
need to handle the latent variables within a MAP–EM framework based on the iterative

4.3 Continuous density hidden Markov model
151
calculation, the conjugate prior provides an analytic solution in the M-step, and it makes
the estimation process efﬁcient.
Mixture weight
Similar to the state transition, the auxiliary function of the mixture weight parameters ω
is as follows:
QMAP(ω′|ω) =
T

t=1
J

j=1
K

k=1
γt(j, k) log ω′
jk + log
⎛
⎝
J
j=1
Dir({ω′
jk}K
k=1|{φω
jk}K
k=1)
⎞
⎠
= log
⎛
⎝
J
j=1
Dir({ω′
jk}K
k=1|{ ˆφω
jk}K
k=1)
⎞
⎠+ log
⎛
⎝
J
j=1
CDir({φω
jk}K
k=1)
CDir({ ˆφω
jk}K
k=1)
⎞
⎠
∝log
⎛
⎝
J
j=1
Dir({ω′
jk}K
k=1|{ ˆφω
jk}K
k=1)
⎞
⎠,
(4.54)
where
ˆφω
jk ≜φω
jk +
T

t=1
γt(j, k).
(4.55)
Therefore, the mode of the Dirichlet distribution is obtained as:
ωMAP
jk
=
φω
jk + T
t=1 γt(j, k) −1
K
k′=1(φω
jk′ + T
t=1 γt(j, k′) −1)
.
(4.56)
Again, it is computed from the statistics of the accumulated posterior values of the state
occupancy T
t=1 γt(j, k) and prior parameter φω
jk.
Mean vector and covariance matrix
Finally, we focus on the auxiliary function of the mean vector μ and precision matrix
R. Recall that the multivariate Gaussian distribution (Appendix C.6) is represented as
follows:
N(x|μ, R−1) = CN (R−1) exp

−1
2(x −μ)⊺R(x −μ)

,
(4.57)
and the Wishart distribution (Appendix C.14) is represented as follows:
W(Y|R0, φ) = CW(R0, φ)|Y|
φ−D−1
2
exp

−1
2tr
+
R0Y
,
.
(4.58)
Then, QMAP(μ′, R′|μ, R) is represented by using Eqs. (4.42) with the normalization
constant, (4.57), and (4.58), as follows:
QMAP(μ′, R′|μ, R) =
T

t=1
J

j=1
K

k=1
γt(j, k)
2
(
log |R′
jk| −(ot −μ′
jk)⊺R′
jk(ot −μ′
jk)
)
+
J

j=1
K

k=1
log
(
N(μ′
jk|μ0
jk, (φμ
jkR′
jk)−1)W(R′
jk|R0
jk, φR
jk)
)
−
T

t=1
J

j=1
K

k=1
γt(j, k)D
2
log(2π)

152
Maximum a-posteriori approximation
=
T

t=1
J

j=1
K

k=1
γt(j, k)
2
(
log |R′
jk| −(ot −μ′
jk)⊺R′
jk(ot −μ′
jk)
)
+ 1
2
J

j=1
K

k=1

log
---R′
jk
--- −(μ′
jk −μ0
jk)⊺φμ
jkR′
jk(μ′
jk −μ0
jk)
+ (φR
jk −D −1) log |R′
jk| −tr
+
R0
jkR′
jk
,
+
J

j=1
K

k=1

−
T

t=1
γt(j, k)D
2
log(2π) −D
2 log(2π) + D
2 log φμ
jk
+ log CW(R0
jk, φR
jk)

,
(4.59)
where the ﬁnal line includes the terms that do not depend on μ and R. Then we rearrange
Eq. (4.59) so that we can write it in a probabilistic form. First, we omit j, k, and ′ in
Eq. (4.59) for simplicity, and consider the following function:
g(μ, R) ≜1
2
 T

t=1
γt

log |R| −(ot −μ)⊺R(ot −μ)

+ log |R| −(μ −μ0)⊺φμR(μ −μ0)
+ (φR −D −1) log |R| −tr
+
R0R
,
= 1
2
 T

t=1
γt log |R| + log |R| + (φR −D −1) log |R| −tr
+
R0R
,
−1
2
(
(ot −μ)⊺R(ot −μ) −(μ −μ0)⊺φμR(μ −μ0)
)



≜f(μ,R)
.
(4.60)
Then, we focus on f(μ, R) that has the terms in g(μ, R) that depend on μ. f(μ, R) is
re-written as follows:
−2f(μ, R) = μ⊺

t
γt + φμ

R

μ −2μ⊺

R

t
γtot + φμRμ0

+

t
γto⊺
t Rot + (μ0)⊺φμRμ0.
(4.61)
Since this is a quadratic form of μ, it can be represented by a Gaussian distribution by
arranging Eq. (4.61) into the complete square form. Although it is complicated to deal
with the complete square form for vectors, we can use the following complete square
rules found in Eqs. (B.16) and (B.17):
x⊺Ax −2x⊺b + c = (x −u)⊺A (x −u) + v,
(4.62)

4.3 Continuous density hidden Markov model
153
where
u ≜A−1b
v ≜c −b⊺A−1b.
(4.63)
Therefore, by using this rule (e.g., x →μ, A →

t γt + φμ
R, b →R 
t γtot +
φμRμ0, and c →
t γto⊺
t Rot+(μ0)⊺φμRμ0), Eq. (4.61) is rewritten with the complete
square form as follows:
−2f(μ, R) = μ⊺

t
γt + φμ

R

μ −2μ⊺

R

t
γtot + φμRμ0

+

t
γto⊺
t Rot + (μ0)⊺φμRμ0
=

μ −ˆμ
⊺(
ˆφμR
) 
μ −ˆμ

+ v(R),
(4.64)
where ˆφμ, ˆμ, and v(R) are deﬁned as follows:
ˆφμ ≜φμ +

t
γt,
ˆμ ≜

t
γt + φμ

R
−1 
R

t
γtot + φμRμ0

= φμμ0 + 
t γtot
φμ + 
t γt
,
v(R) ≜

t
γto⊺
t Rot + (μ0)⊺φμRμ0 −

R

t
γtot + φμRμ0
⊺
ˆμ
=

t
γto⊺
t Rot + (μ0)⊺φμRμ0 −ˆμ⊺ˆφμR ˆμ.
(4.65)
Note that ˆφμ and ˆμ correspond to the mean and covariance hyperparameters of the
conjugate Gaussian distribution of μ. Thus, f(μ, R) is rewritten with the deﬁnition of
the Gaussian distribution in Eq. (4.57) as follows:
f(μ, R) = −1
2

μ −ˆμ
⊺(
ˆφμR
) 
μ −ˆμ

−1
2v(R)
= log N(μ| ˆμ, ( ˆφμR)−1) −log CN (R−1) −1
2v(R)
= log N(μ| ˆμ, ( ˆφμR)−1) + D
2 log(2π) −D
2 log ˆφμ −1
2 log |R| −1
2v(R),
(4.66)
where v(R) is used to obtain the analytic form of R with the rest of the R-dependent
terms in Eq. (4.60). That is, the auxiliary function (Eq. (4.60)) with Eq. (4.66) is
represented as follows:

154
Maximum a-posteriori approximation
g(μ, R) = log N(μ| ˆμ, ( ˆφμR)−1) + D
2 log(2π) −D
2 log ˆφμ
× 1
2

−v(R) +

t
γt + φR −D −1

log |R| −tr
+
R0R
,



≜h(R)
.
(4.67)
Now we focus on h(R) that has the terms in the second line in Eq. (4.67). By using the
deﬁnition of v(R) in Eq. (4.65), we can rewrite h(R) as follows:
h(R) = 1
2

−v(R) +

t
γt + φR −D −1

log |R| −tr
+
R0R
,
= 1
2

−

t
γto⊺
t Rot −(μ0)⊺φμRμ0 + ˆμ⊺ˆφμR ˆμ −tr
+
R0R
,
+ 1
2

t
γt + φR −D −1

log |R|.
(4.68)
Since Eq. (4.68) includes the trace operation, it is difﬁcult to re-arrange this equation.
Therefore, by using the trace rule of a = tr[a] (Eq. (B.1)), we represent all terms except
for the log |R| term in Eq. (4.68) as follows:
h(R) = 1
2

−tr
&
t
γtoto⊺
t R
'
−tr
+
φμμ0(μ0)⊺R
,
+ tr
+
ˆφμ ˆμ ˆμ⊺R
,
−tr
+
R0R
,)
+ 1
2

t
γt + φR −D −1

log |R|.
(4.69)
In addition, by using the trace rules of tr[ABC] = tr[BCA] and tr [A + B] = tr [A] +
tr [B] (Eqs. (B.2) and (B.3)), Eq. (4.69) is ﬁnally represented by comparing h(R) with
the deﬁnition of the Wishart distribution (Eq. (4.58)), as follows:
h(R) = −1
2tr


t
γtoto⊺
t + φμμ0(μ0)⊺−ˆφμ ˆμ ˆμ⊺+ R0




≜ˆR
R

+ 1
2

t
γt + φR




≜ˆφR
−D −1

log |R|
= log W(R| ˆR, ˆφR) −log CW( ˆR, ˆφR).
(4.70)

4.3 Continuous density hidden Markov model
155
Thus, Eq. (4.70) can be represented as the Wishart distribution with the following
hyperparameters:
ˆφR ≜φR +

t
γt,
ˆR ≜

t
γtoto⊺
t + φμμ0(μ0)⊺−ˆφμ ˆμ ˆμ⊺+ R0.
(4.71)
Note that these hyperparameters are almost equivalent to those of the conjugate dis-
tribution analysis discussed in Eq. (2.112). The only difference is that the statistics in
Eq. (4.71) are computed from the expectation value of the posterior distribution of a
latent variable γt.
Here, g(μ, R) in the original Q function, Eq. (4.59), is represented by Eqs. (4.67) and
(4.70) as follows:
g(μ, R) = log N(μ| ˆμ, ( ˆφμR)−1) + D
2 log(2π) −D
2 log ˆφμ
+ log W(R| ˆR, ˆφR) −log CW( ˆR, ˆφR).
(4.72)
Thus, we have found that QMAP(μ′, R′|μ, R) can be represented with the same distri-
bution form as the prior distributions, which is represented by Gaussian and Wishart
distributions as follows:
QMAP(μ′, R′|μ, R)
=
J

j=1
K

k=1
log
(
N(μ′
jk| ˆμjk, ( ˆφμ
jkR′
jk)−1)W(R′
jk| ˆRjk, ˆφR
jk)
)
+
J

j=1
K

k=1

−
T

t=1
γt(j, k)D
2
log(2π) + D
2 log
φμ
jk
ˆφμ
jk
+ log
CW(R0
jk, φR
jk)
CW( ˆRjk, ˆφR
jk)

∝
J

j=1
K

k=1
log
(
N(μ′
jk| ˆμjk, ( ˆφμ
jkR′
jk)−1)W(R′
jk| ˆRjk, ˆφR
jk)
)
,
(4.73)
where we recover the indexes j, k, and ′. By using the deﬁnition of the Gaussian–Wishart
distribution in Appendix C.15, QMAP(μ′, R′|μ, R) can also be represented as:
QMAP(μ′, R′|μ, R) ∝
J

j=1
K

k=1
log
(
NW(μ′
jk, R′
jk| ˆμjk, ˆφμ
jk, ˆRjk, ˆφR
jk)
)
.
(4.74)
By summarizing the result of Eqs. (4.65) and (4.71), the hyperparameters of these
distributions are deﬁned as:
ˆφμ
jk ≜φμ
jk +

t
γt(j, k),
ˆμjk ≜
φμ
jkμ0
jk + 
t γt(j, k)ot
φμ
jk + 
t γt(j, k)
,

156
Maximum a-posteriori approximation
ˆφR
jk ≜φR
jk +

t
γt(j, k),
ˆRjk ≜

t
γt(j, k)oto⊺
t + φμ
jkμ0
jk(μ0
jk)⊺−ˆφμ
jk ˆμjk ˆμ⊺
jk + R0
jk.
(4.75)
The MAP estimates of these values are obtained by considering the modes of the
Gaussian–Wishart distribution in Appendix C.15. The modes of μ′
jk and ′
jk are
represented as:
μMAP
jk
= ˆμjk,
MAP
jk
=
(
RMAP
jk
)−1
= ( ˆφR
jk −D −1)−1 ˆRjk.
(4.76)
Note that MAP
jk
cannot be obtained when ˆφR
jk −D −1 ≤0. Thus, we can analytically
obtain the M-step solutions of CDHMM parameters (i.e., initial weight, state transition,
mixture weight, mean vector and covariance matrix) in the MAP sense, thanks to the
conjugate prior distributions. We summarize the solutions below. The hyperparameters
of the posterior distributions are represented with Gaussian sufﬁcient statistics and the
prior hyperparameters as:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
ˆφπ
j ≜φπ
j + ξ1(j),
ˆφa
ij ≜φa
ij +
T−1

t=1
ξt(i, j),
ˆφω
jk ≜φω
jk +
T

t=1
γt(j, k),
ˆφμ
jk ≜φμ
jk +
T

t=1
γt(j, k),
ˆμjk ≜
φμ
jkμ0
jk + T
t=1 γt(j, k)ot
φμ
jk + T
t=1 γt(j, k)
,
ˆφR
jk ≜φR
jk +
T

t=1
γt(j, k),
ˆRjk ≜
T

t=1
γt(j, k)oto⊺
t + φμ
jkμ0
jk(μ0
jk)⊺−ˆφμ
jk ˆμjk ˆμ⊺
jk + R0
jk.
(4.77)
A set of hyperparameters ˆ is deﬁned as follows:
ˆ ≜{ ˆφπ
j , ˆφa
ij, ˆφω
jk, ˆφμ
jk, ˆφR
jk, ˆμjk, ˆRjk|i = 1, · · · , J, j = 1, · · · , J, k = 1, · · · , K}.
(4.78)

4.3 Continuous density hidden Markov model
157
Then, the MAP solutions of HMM parameters are represented with the posterior
hyperparameters as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
πMAP
j
=
ˆφπ
j −1
J
j′=1( ˆφπ
j′ −1)
,
aMAP
ij
=
ˆφa
ij −1
J
j′=1( ˆφa
ij′ −1)
,
ωMAP
jk
=
ˆφω
jk −1
K
k′=1( ˆφω
jk′ −1)
,
μMAP
jk
= ˆμjk,
MAP
jk
= ( ˆφR
jk −D −1)−1 ˆRjk.
(4.79)
Compared with the ML M-step of CDHMM parameters in Eq. (3.151), these solutions
are more complicated, and actually incur more computational cost than that of the ML
solution. However, the computational cost of the M-step is much smaller than that of the
E-step, and the additional computational cost of the MAP estimate can be disregarded
in practical use.
Mean vector and diagonal covariance matrix
In practise, we often use the diagonal covariance matrix for a multivariate Gaussian
distribution for HMMs, as we discussed in Section 3.2.3. This section provides the
MAP solution for the diagonal covariance case (Gauvain & Lee 1991). Since the
one-dimensional solution of the full-covariance Gaussian posterior distribution in the
previous discussion corresponds to that of the diagonal covariance Gaussian posterior
distribution of a diagonal element, we can obtain the hyperparameters of the posterior
distribution of the CDHMM parameters by using D →1 for each diagonal component.
We also summarize the solution of the MAP estimates of HMM parameters of the diag-
onal covariance Gaussian case below. The hyperparameters of the posterior distributions
are represented with Gaussian sufﬁcient statistics and the prior hyperparameters as:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
ˆφπ
j
≜φπ
j + ξ1(j),
ˆφa
ij
≜φa
ij + T−1
t=1 ξt(i, j),
ˆφω
jk
≜φω
jk + T
t=1 γt(j, k),
ˆφμ
jk
≜φμ
jk + T
t=1 γt(j, k),
ˆμjk
≜
φμ
jkμ0
jk+T
t=1 γt(j,k)ot
φμ
jk+T
t=1 γt(j,k)
,
ˆφR
jk
≜φR
jk + T
t=1 γt(j, k),
ˆrjkd
≜T
t=1 γt(j, k)o2
td + φμ
jk(μ0
jkd)2 −ˆφμ
jk( ˆμjkd)2 + r0
jkd.
(4.80)

158
Maximum a-posteriori approximation
In this case, a set of hyperparameters ˆ is deﬁned as follows:
ˆ ≜{ ˆφπ
j , ˆφa
ij, ˆφω
jk, ˆφμ
jk, ˆφR
jk, ˆμjk, ˆrjk|i = 1, · · · , J, j = 1, · · · , J, k = 1, · · · , K},
(4.81)
where
ˆr ≜[ˆr1, · · · , ˆrD]⊺.
(4.82)
Then, the MAP solutions of HMM parameters can be represented with the posterior
hyperparameters as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
πMAP
j
=
ˆφπ
j −1
J
j′=1( ˆφπ
j′ −1)
,
aMAP
ij
=
ˆφa
ij −1
J
j′=1( ˆφa
ij′ −1)
,
ωMAP
jk
=
ˆφω
jk −1
K
k′=1( ˆφω
jk′ −1)
,
μMAP
jk
= ˆμjk,
MAP
jkd
=
ˆrjkd
ˆφR
jk −2
.
(4.83)
Thus, we obtain the MAP estimates of HMM parameters in both diagonal and full
covariance matrix cases.
4.3.6
Sufﬁcient statistics
As we discussed in Eq. (4.43), the posterior probabilities of the state transition ξt(i, j),
and mixture occupation γt(j, k) can be computed by plugging the MAP estimates MAP
obtained by using Eqs. (4.79) or (4.83) into the variables in Sections 3.3 and 3.4.2. Note
that the analytical results here are exactly the same as those in the ML–EM algorithm
(except for the MAP estimates MAP): we list these for convenience.
First, the MAP forward variable αt(j) is computed by using the following equation:
• Initialization
α1(j) = p(o1, s1 = j|MAP)
= πMAP
j
bMAP
j
(o1),
1 ≤j ≤J.
(4.84)
• Induction
αt(j) = p(o1, · · · , ot, st = j|MAP)
=
 J

i=1
αt−1(i)aMAP
ij

bMAP
j
(ot),
2 ≤t ≤T
1 ≤j ≤J.
(4.85)

4.3 Continuous density hidden Markov model
159
• Termination
p(O|MAP) =
J

j=1
αT(j).
(4.86)
Here, bMAP
j
(ot) is a GMM emission probability distribution with the MAP estimate
parameters deﬁned as:
bMAP
j
(ot) ≜
K

k=1
ωMAP
jk
N(ot|μMAP
jk
, MAP
jk
).
(4.87)
The MAP backward variable βt(j) is computed by using the following equations:
• Initialization
βT(j) = 1,
1 ≤j ≤J.
(4.88)
• Induction
βt(i) = p(ot+1, · · · , oT|st = i, MAP)
=
J

j=1
aMAP
ij
bMAP
j
(ot+1)βt+1(j),
t = T −1, T −2, · · · , 1,
1 ≤i ≤J.
(4.89)
• Termination
β0 ≜p(O|MAP)
=
J

j=1
πMAP
j
bMAP
j
(o1)β1(j).
(4.90)
Therefore, based on the MAP forward and backward variables, we can compute the
posterior probabilities as follows:
ξt(i, j) =
αt(i)aMAP
ij
(K
k=1 ωMAP
jk
N(ot|μMAP
jk
, MAP
jk
)
)
βt+1(j)
J
i′=1
J
j′=1 αt(i′)aMAP
i′j′
(K
k=1 ωMAP
j′k
N(ot|μMAP
j′k
, MAP
j′k
)
)
βt+1(j′)
, (4.91)
γt(j, k) =
αt(j)βt(j)
J
j′=1 αt(j′)βt(j′)
·
ωMAP
jk
N(ot|μMAP
jk
, MAP
jk
)
K
k′=1 ωMAP
jk′
N(ot|μMAP
jk′
, MAP
jk′
)
.
(4.92)
Once we have computed the posterior probabilities, we can compute the following
sufﬁcient statistics:
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
T−1
t=1 ξt(i, j)
≜ξij,
T
t=1 γt(j, k)
≜γjk,
T
t=1 γt(j, k)ot
≜γ (1)
jk ,
T
t=1 γt(j, k)oto⊺
t
≜(2)
jk ,
T
t=1 γt(j, k)o2
td
≜γ (2)
jkd .
(4.93)

160
Maximum a-posteriori approximation
The superscripts (1) and (2) denote the 1st and 2nd order statistics.
This total EM algorithm for iteratively estimating the HMM parameters based on the
MAP estimate is set out in Algorithm 8. Compared with the ML Baum–Welch algo-
rithm, Algorithm 4, it requires the hyperparameters  of the CDHMMs. Section 7.3
also introduces a variant of the Baum–Welch algorithm based on variational Bayes. Note
Algorithm 8 MAP Baum–Welch algorithm
Require:  and MAP ←init
1: repeat
2:
Compute the forward variable αt(j) from the forward algorithm
3:
Compute the backward variable βt(j) from the backward algorithm
4:
Compute the occupation probabilities γ1(j), γt(j, k), and ξt(i, j)
5:
Accumulate the sufﬁcient statistics ξ(i, j), γ (j, k), γ (1)
jk , and (2)
jk (or γ (2)
jkd )
6:
Estimate the new hyperparameters ˆ
7:
Estimate the new HMM parameters (MAP)′
8:
Update the HMM parameters MAP ←(MAP)′
9: until Convergence
again that the MAP E-step (computing forward variables, occupation probabilities, and
accumulation) is exactly the same as that of the ML E-step, and retains the nice property
of the parallelization and data scalability. In addition, since the E-step computation is
dominant in the algorithm, the computational costs of the ML and MAP Baum–Welch
algorithms are almost same.
4.3.7
Meaning of the MAP solution
This section discusses the meaning of the MAP solution obtained by Eqs. (4.79) and
(4.83). We consider the two extreme case of the MAP solution, where the amount of
data is small and large. That is, we consider the small data limit as ξij, γjk →0. On the
other hand, the large data limit corresponds to ξij, γjk →∞.
• Mixture weight
We ﬁrst focus on the MAP estimate of the mixture weight ω, but the discussion can
be applied to the state transition a.
Large sample:
The MAP estimate of the mixture weight is represented as follows:
ωMAP
jk
=
φω
jk + γjk −1
K
k′=1(φω
jk′ + γjk′ −1)
=
γjk

φω
jk−1
γjk
+ 1

−1
K
k′=1 γjk′

φω
jk′−1
γjk′
+ 1

−1
.
(4.94)

4.3 Continuous density hidden Markov model
161
Since
φω
jk−1
γjk
→0 in the large sample case, the MAP estimate ωMAP
jk
approaches the
ML estimate ωML
jk
when γjk is sufﬁciently larger than φω
jk −1:
ωMAP
jk
≈
γjk
K
k′=1 γjk′
= ωML
jk
(γjk ≫φω
jk −1).
(4.95)
Small sample:
Similarly, the MAP estimate ωMAP
jk
approaches the following value when γjk is
sufﬁciently smaller than φω
jk −1:
ωMAP
jk
≈
φω
jk −1
K
k′=1(φω
jk′ −1)
(γjk ≪φω
jk −1).
(4.96)
The weight is only computed from the prior hyperparameter φω
jk. Thus, the mix-
ture weight approaches the ML estimate when the amount of data is large, while it
approaches the weight obtained only from the prior hyperparameters when the amount
is small. Hyperparameter φω
jk can be regarded as a scale.
• Mean
By using Eqs. (4.93) and (4.79), the MAP estimate of the mean vector can be rewritten
as follows:
μMAP
jk
=
φμ
jkμ0
jk + 
t γt(j, k)ot
φμ
jk + 
t γt(j, k)
=
φμ
jkμ0
jk + γ (1)
jk
φμ
jk + γjk
=
φμ
jk
γjk μ0
jk + μML
jk
φμ
jk
γjk + 1
.
(4.97)
This equation means that the MAP estimate μMAP
jk
is linearly interpolated by the ML
estimate μML
jk
and the hyperparameter μ0
jk, as shown in Figure 4.1.
φμ
jk
γjk is an interpola-
tion ratio, and it has a speciﬁc meaning when the amount of data is sufﬁciently large
(γjk ≫φω
jk) or small (γjk ≪φω
jk).
Large sample:
μMAP
jk
≈μML
jk .
(4.98)
Similarly to the discussion of the mixture weight, the MAP estimate of the mean
vector theoretically converges to the ML estimate.
Small sample:
μMAP
jk
≈μ0
jk.
(4.99)
This is a good property of the MAP estimate. Although the ML estimate with a
small amount of data incorrectly estimates the mean vector, which degrades the

162
Maximum a-posteriori approximation
Figure 4.1
Geometric meaning of the MAP estimate of the Gaussian mean vector. It is represented as the
linear interpolation of the prior mean vector μ0
jk and the ML estimate of the mean vector μML
jk .
The interpolation ratio depends on the hyperparameter φμ
jk and the amount of data γjk assigned to
the Gaussian.
performance drastically, the MAP estimate can smooth the incorrect estimation based
on the hyperparameter μ0
jk. In the practical situation, we also often have a zero count
problem (i.e., γjk = 0), which makes the ML estimate singular due to the zero divide.
However, the MAP solution of the mean vector avoids this problem and provides a
reasonable estimate obtained from the hyperparameter μ0
jk.
• Covariance matrix
By using Eqs. (4.93) and (4.79), the MAP estimate of the covariance matrix can be
rewritten as follows:
MAP
jk
= (φR
jk + γjk −D −1)−1
×
(
(2)
jk + φμ
jkμ0
jk(μ0
jk)⊺−ˆφμ
jk ˆμjk ˆμ⊺
jk + R0
jk
)
.
(4.100)
Large sample:
MAP
jk
≈(γjk)−1 (
(2)
jk −γjkμML
jk (μML
jk )⊺)
= ML
jk .
(4.101)
The result is the same when we use the diagonal covariance.
Small sample (full covariance):
MAP
jk
≈(φR
jk −D −1)−1 (
φμ
jkμ0
jk(μ0
jk)⊺−φμ
jkμ0
jk(μ0
jk)⊺+ R0
jk
)
= (φR
jk −D −1)−1R0
jk.
(4.102)
Unlike the mean vector case, the covariance matrix of the small sample limit is rep-
resented by the two hyperparameters R0
jk and φR
jk. To make the solution meaningful,
we need to set φR
jk > D + 1 to avoid a negative or zero value of the variance.

4.4 Speaker adaptation
163
• Small sample (diagonal covariance):
By using Eq. (4.83), we can also obtain the variance parameter for dimension d as
follows:
MAP
jkd
≈
r0
jkd
φR
jk −2
.
(4.103)
Similarly to the diagonal case, we need to set φR
jk > 2 to avoid a negative or zero
value of the variance.
In summary, the MAP solutions can smooth the estimation values with the hyperparam-
eters when the amount of data is small, and the solutions approach the ML estimates
when the amount of data is large. A similar discussion has already been presented in
Section 2.1.4, as a general property of the Bayesian approach.
The MAP estimation of the CDHMM parameters can be applied to general CDHMM
training. However, since CDHMM is usually trained with a sufﬁcient amount of training
data, we do not have to use MAP estimation, and ML estimation is enough in most cases,
which corresponds to the case of the large sample limitation in the above discussion.
However, we often face the case when the amount of data is small at an adaptation
scenario. The following section introduces one of the most successful applications of
the MAP estimation for speaker adaptation.
4.4
Speaker adaptation
Speaker adaptation is one of the most important techniques in speech recognition,
mainly to deal with speaker variations in speech (Lee & Huo 2000, Shinoda 2010). The
speech features of a speaker are different from those of another speaker, which degrades
the performance of speech recognition. A straightforward solution for this problem is to
build a speaker-dependent acoustic model for a speciﬁc person. However, it is difﬁcult
to collect sufﬁcient training data with labels.
Speaker adaptation aims to solve the problem by ﬁrst building a speaker-independent
(SI) acoustic model SI by using many speakers’ data, and updates the model as a
speaker-dependent (SD) acoustic model SD with a small amount of data of the target
speaker, as shown in Figure 4.2. The speaker-independent acoustic model is usually
made by the conventional maximum likelihood procedure, as we discussed in Chapter
3, or discriminative training. It is also obtained by using so-called speaker adaptive
training (Anastasakos, McDonough, Schwartz et al. 1996) or cluster adaptive training
(Gales, Center & Heights 2000), which normalizes the speaker characteristics (or some
other characteristics (e.g., noises, speaking styles) obtained from clustering of speech
utterances) by using a variant of the maximum likelihood linear regression (MLLR)
technique, which is discussed in Section 3.5.1.
4.4.1
Speaker adaptation by a transformation of CDHMM
Once we have SI model parameters SI, the problem is how to estimate the SD model
parameters SD without over-training. Basically, the number of SI model parameters

164
Maximum a-posteriori approximation
Figure 4.2
Speaker adaptation of HMM parameters. The initial HMMs are trained with many speakers, and
then the HMMs are adapted to the target speaker’s model with a small amount of adaptation data.
is very large. For example, in the famous speech recognition task using read speech of
Wall Street Journal (WSJ) sentences, the number of CDHMM parameters amounts to
several millions or more. On the other hand, the amount of speech data for the target
speaker with text labels would be a few minutes at most, and the number of frames
corresponds to the order of tens of thousands, and is even smaller than the number of
standard CDHMM parameters. The following ML estimate with the EM algorithm, as
we discussed in Section 3.4, causes serious over-training:
SD, ML = arg max
SD′ QML(SD′|SD).
(4.104)
There are several approaches to overcoming the problem by using the maximum like-
lihood linear regression (MLLR) (Digalakis et al. 1995, Leggetter & Woodland 1995,
Gales & Woodland 1996), as discussed in Section 3.5, eigenvoice approaches (Kuhn,
Junqua, Ngyuen et al. 2000), and so on. These approaches set a parametric constraint
of fewer CDHMM parameters, and estimate these parameters () instead of CDHMM
parameters with ML indirectly, that is:
ML = arg max
′ QML(′|; SI).
(4.105)
Detailed discussions of these adaptation techniques can be found in Lee & Huo (2000)
and Shinoda (2010). We can also consider the Bayesian treatment of this indirect
estimation of transformation parameters , which is discussed in Section 7.4.

4.4 Speaker adaptation
165
4.4.2
MAP-based speaker adaptation
In speaker adaptation using the MAP estimation (MAP adaptation), we directly esti-
mate the SD CDHMM parameters SD, unlike the MLLR and eigenvoice techniques.5
The MAP estimation can avoid the over-training problem. Then, we use SI CDHMM
parameters SI as hyperparameters of the prior distributions, i.e.,
SD, MAP = arg max
SD′ QMAP(SD′|SD; (SI)),
(4.106)
= arg max
SD′ QML(SD′|SD) + log p(SD′|(SI)),
(4.107)
where p(SD′|(SI)) is a prior distribution with hyperparameters of the prior distribu-
tion, and it is set as a conjugate distribution of CDHMM, as discussed in Section 4.3.3.
We discuss below how to set SI to hyperparameters  in detail.
Let πSI
j , aSI
ij , ωSI
jk , μSI
jk , and SI
jkd be the SI CDHMM parameters with diagonal
covariance. Although there are several ways to determine hyperparameters from the
speaker-independent HMM parameters, we can set the following relationship between
hyperparameters and SI parameters by using Eqs. (4.96), (4.99), and (4.103):
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
φπ
j −1
J
j′=1(φπ
j′ −1)
= πSI
j ,
φa
ij−1
J
j′=1(φa
ij′−1)
= aSI
ij ,
φω
jk−1
K
k′=1(φω
jk′−1)
= ωSI
jk ,
μ0
jk
= μSI
jk ,
r0
jkd
φR
jk−2
= SI
jk .
(4.108)
This equation is obtained based on the constraint that we can obtain the SI performance
when the amount of adaptation data for the target speaker is zero. To satisfy the above
equations, we can use the following hyperparameter setting:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
φπ
j
= λπSI
j
+ 1,
φa
ij
= λaSI
ij + 1,
φω
jk
= λωSI
jk + 1,
φμ
jk
= φ,
μ0
jk
= μSI
jk ,
r0
jkd
= SI
jkd(φ −2).
(4.109)
5 There are several approaches combining indirect adaptation via the estimation of transformation
parameters and MAP-based direct estimation of CDHMM parameters (Digalakis & Neumeyer 1996,
Takahashi & Sagayama 1997).

166
Maximum a-posteriori approximation
Note that the above hyperparameter setting has two additional parameters φ and λ. These
are often set with ﬁxed values (e.g., φ = 10, λ = 1). Thus, by substituting Eq. (4.109)
into Eq. (4.80) and (4.83), the MAP estimates of SD HMM parameters are obtained as:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
πSD, MAP
j
=
ˆφπ
j −1
J
j′=1( ˆφπ
j′ −1)
=
λπSI
j +ξ1(j)
J
j′=1(λπSI
j′ +ξ1(j′)),
aSD, MAP
ij
=
ˆφa
ij−1
J
j′=1( ˆφa
ij′−1)
=
λaSI
ij +T
t=1 ξt(i,j)
J
j′=1(λaSI
ij′+T
t=1 ξt(i,j′)),
ωSD, MAP
jk
=
ˆφω
jk−1
K
k′=1( ˆφω
jk′−1)
=
λωSI
jk +T
t=1 γt(j,k)
K
k′=1(λωSI
jk′+T
t=1 γt(j,k′)),
μSD, MAP
jk
= ˆμjk
=
φμSI
jk +T
t=1 γt(j,k)ot
φ+T
t=1 γt(j,k)
,
SD, MAP
jkd
=
ˆrjkd
ˆφR
jk−2
=
T
t=1 γt(j,k)o2
td+φ(μSI
jkd)2−(φ+T
t=1 γt(j,k))(μSD, MAP
jkd
)2+SI
jkd(φ−2)
φ+T
t=1 γt(j,k)−2
.
(4.110)
Gauvain & Lee (1994) compare speaker adaptation performance by employing ML and
MAP estimations of acoustic model parameters using the DARPA Naval Resources
Management (RM) task (Price, Fisher, Bernstein et al. 1988). With 2 minutes of adap-
tation data, the ML word error rate was 31.5 % and was worse than the speaker
independent word error rate (13.9 %) due to the over-training effect. However, the MAP
word error rate was 8.7 %, clearly showing the effectiveness of the MAP approach. MAP
estimation has also been used in speaker veriﬁcation based on universal background
models (Reynolds, Quatieri & Dunn 2000), which is described in Section 4.6, and in
the discriminative training of acoustic models in speech recognition as a parameter
smoothing technique (Povey 2003), which is described in the next section.
4.5
Regularization in discriminative parameter estimation
This section describes another well-known application of MAP estimation in discrimi-
native training of CDHMM parameters. Discriminative training is based on discrimina-
tive criteria, which minimizes the ASR errors directly rather than maximizing likelihood
values (Juang & Katagiri 1992), and improves the performance further from the ML-
based CDHMM. However, discriminative training of CDHMM parameters always has a

4.5 Regularization in discriminative parameter estimation
167
problem of over-estimation, and the regularization effect of the MAP estimation helps to
avoid this problem. Discriminative training has been studied by many researchers, and
there are many approaches to realize it for ASR based on different discriminative criteria
and optimization techniques (e.g., maximum mutual information (MMI) criterion (Bahl,
Brown, de Souza et al. 1986), MMI with extended Baum–Welch algorithm (Normandin
1992), minimum classiﬁcation error (MCE) criterion (Juang & Katagiri 1992), MCE
with various gradient methods (McDermott, Hazen, Le Roux et al. 2007), minimum
phone error (MPE) criterion (Povey & Woodland 2002), and the uniﬁed interpretation
of these techniques (Schlüter, Macherey, Müller et al. 2001, Nakamura, McDermott,
Watanabe et al. 2009)).
This section explains the regularization effect of the MMI estimation of HMM param-
eters with the extended Baum–Welch algorithm (Povey & Woodland 2002). In this
section, we limit the discussion of discriminative training to focus on introducing the
application of MAP estimation.
4.5.1
Extended Baum–Welch algorithm
The MMI estimation of HMM parameters can be performed by the extended Baum–
Welch algorithm or variants of gradient based methods. The MMI estimation starts
from the following objective function based on the posterior distribution of the word
sequence:
FMMI() =
R

r=1
log p(Wr|Or; )
=
R

r=1
log

SWr

p

Or, SWr|
κ pL(Wr)

W

SW (p (Or, SW|))κ pL(W),
(4.111)
where Or = {ot|t = 1, · · · , Tr} is the rth utterance’s acoustic feature sequence whose
length is Tr. The total number of the utterances is R. Wr is a correct word sequence of
the utterance r, and SWr is a set of all possible state sequences given Wr.6 Similarly,
W is a word sequence hypothesis, and the summation over W is performed among all
possible word sequences. κ is the acoustic score scale, and p

Or, SWr|

is an acoustic
likelihood, and pL is the language model probability.  is a set of all acoustic model
(CDHMM) parameters for all context-dependent phonemes, unlike the deﬁnition of the
CDHMM parameters for single context-dependent phonemes in Section 4.2. The MMI
estimate of  can be obtained by optimizing this objective function as follows:
6 The summation over state sequences SWr in the numerator in Eq. (4.111) is often approximated by the
Viterbi sequence without the summation obtained by the Viterbi algorithm in Section 3.3.2, i.e.,

SWr
p

Or, SWr|

≈max
SWr
p

Or, SWr|

.
(4.112)
Similarly, the exact summation over W in the denominator is almost impossible in the large-scale ASR,
and it is also approximated by the summation over pruned word sequences in a lattice, which is obtained
after the ASR decoding process.

168
Maximum a-posteriori approximation
DT = arg max
 FMMI().
(4.113)
When we only consider the numerator of Eq. (4.111) in this optimization, that
corresponds to the maximum likelihood estimation of the CDHMM parameters.
By using the extended Baum–Welch algorithm, a new mean vector and variance at
dimension d are iteratively updated from the previously estimated μDT
jkd[τ] and DT
jkd [τ]7
at the τ iteration step, as follows:
μDT
jk [τ + 1] =
γ (1),num
jk
−γ (1),den
jk
+ DμDT
jk [τ]
γ num
jk
−γ den
jk
+ D
,
DT
jkd [τ + 1] =
γ (2),num
jkd
−γ (2),den
jkd
+ D

DT
jkd [τ] +
(
μDT
jkd[τ]
)2
γ num
jk
−γ den
jk
+ D
−
(
μDT
jkd[τ + 1]
)2
.
(4.114)
The derivation of the extended Baum–Welch algorithm can also be found in Section
5.2.8. Here, γ num
jk
, γ (1),num
jk
, and γ (2),num
jkd
are the Gaussian sufﬁcient statistics deﬁned
in Eq. (4.93), but these are obtained from the numerator of the lattice. Similarly, γ den
jk ,
γ (1),den
jk
, and γ (2),den
jkd
are obtained from the denominator of the lattice. D is a smoothing
parameter used with the previous estimated parameters.
By comparison with the ML estimates of μ and  in Eq. (3.151), which is only
computed from the Gaussian sufﬁcient statistics, the MMI estimates are computed from
μDT
jk [τ] and DT
jkd [τ], and the numerator and denominator statistics. This is the main
difference between ML and MMI estimation methods. However, by setting D, γ den
jk ,
γ (1),den
jk
, and γ (2),den
jkd
to 0, Eq. (4.114) is close to the ML estimates if we consider that the
numerator statistics can be regarded as the statistics used in the ML–EM, i.e.,
lim
D,γ den→0
μDT
jk [τ + 1] =
γ (1),num
jk
γ num
jk
≈μML
jk ,
lim
D,γ den→0
DT
jkd [τ + 1] =
γ (2),num
jkd
γ num
jk
−
⎛
⎝γ (1),num
jkd
γ num
jk
⎞
⎠
2
≈ML
jkd .
(4.115)
Therefore, the MMI estimate can also involve the ML-like solution in a speciﬁc
limitation.
We can further provide an interesting interpretation of the MMI estimate. First we
focus on the following difference statistics between the numerator and denominator
statistics:
δjk ≜γ num
jk
−γ den
jk ,
δ(1)
jk ≜γ (1),num
jk
−γ (1),den
jk
,
δ(2)
jkd ≜γ (2),num
jkd
−γ (2),den
jkd
.
(4.116)
7 Note again that  means the diagonal component of the covariance matrix, and does not mean the
standard deviation σ.

4.5 Regularization in discriminative parameter estimation
169
Then, we can rewrite Eq. (4.114) with these difference statistics as follows:
μDT
jk [τ + 1] =
δ(1)
jk + DμDT
jk [τ]
δjk + D
,
DT
jkd [τ + 1] =
δ(2)
jkd + D

DT
jkd [τ] +
(
μDT
jkd[τ]
)2
δjk + D
−
(
μDT
jkd[τ + 1]
)2
.
(4.117)
Therefore, Eq. (4.117) means that the MMI estimates are represented by the linear inter-
polation between the difference-based Gaussian statistics and the previously estimated
parameters. D plays a role of tuning the linear interpolation ratio.
Note that the MMI estimates are based on the difference statistics, and if the denomi-
nator statistics are large, the difference statistics become small, and the MMI estimates
would meet an over-training problem. The smoothing based on the D with the previous
estimated parameters could mitigate the over-training problem, and the combination of
the MMI estimation with the MAP estimation can further mitigate it.
4.5.2
MAP interpretation of i-smoothing
In MMI and MPE training (Povey & Woodland 2002), the following smoothing terms
are introduced for the numerator statistics in Eq. (4.114):
γ
′num
jk
= γ num
jk
+ η,
γ
′(1),num
jk
= γ (1),num
jk
+ ημ0
jk,
γ
′(2),num
jkd
= γ (2),num
jkd
+ η
(
(μ0
jkd)2 + 0
jkd
)
,
(4.118)
where η is called the i-smoothing factor. This section reviews this statistics update,
which can be interpreted as the MAP estimation where η behaves as a hyperparame-
ter in the MAP estimation. μ0
jkd and 0
jkd are obtained from the maximum likelihood
estimation (i.e., μ0
jkd = μML
jkd and 0
jkd = ML
jkd ), or estimation based on discriminative
training.
To derive the above smoothing factor, we ﬁrst consider the conjugate distribution of
the diagonal-covariance Gaussian distribution, which is based on the Gaussian–gamma
distribution, as shown in Table 2.1. The Gaussian–gamma distribution is deﬁned in
Appendix C.13 as follows:
NGam(μ, r|μ0, φμ, r0, φr)
= CN Gam(φμ, r0, φr)r
φr−1
2
exp

−r0r
2 −φμr(μ −μ0)2
2

,
(4.119)
where we omit state index j, mixture component k, and dimension index d for simplicity.
By setting hyperparameters μ0, φμ, r0, and φr with the following variables:

170
Maximum a-posteriori approximation
⎧
⎪⎨
⎪⎩
φμ = η,
r0 = 0η,
φr = η + 2,
(4.120)
the prior distribution is represented as follows:
p(μ, ) = NGam(μ, r|μ0, η, 0η, η + 2)
∝r
η+ 1
2
2
exp

−0rη
2
−ηr(μ −μ0)2
2

.
(4.121)
By using this prior distribution similarly to the MAP auxiliary function in Section 4.2,
the MMI objective function with the prior distribution can be obtained as follows:
FMMI(μ, ) + log p(μ, ).
(4.122)
Based on the extended Baum–Welch calculation with the additional prior distribution,
we can obtain the following update equation (Povey & Woodland 2002):
μDT[τ + 1] = γ (1),num + ημ0 −γ (1),den + DμDT[τ]
γ num + η −γ den + D
,
DT[τ + 1] =
γ (2),num + η

(μ0)2 + 0
−γ (2),den + D
(
DT[τ] +

μDT[τ]
2)
γ num + η −γ den + D
−

μDT[τ + 1]
2 .
(4.123)
This equation is based on Eq. (4.117), with the effect of prior distribution p(μ, )
through Eq. (4.118).
Below, we discuss this update equation with the MAP solution by following the sim-
ilar discussion in the previous section based on the ML–EM conversion. By setting D,
γ den, γ (1),den, and γ (2),den to 0, Eq. (4.123) is represented as follows:
lim
D,γ den→0
μDT[τ + 1] = γ (1),num + ημ0
γ num + η
,
(4.124)
lim
D,γ den→0
DT[τ + 1] = γ (2),num + η

(μ0)2 + 0
γ num + η
−
γ (1),num + ημ0
γ num + η
2
.
(4.125)
By comparing the MAP solutions for μ and  in Eq. (4.83), we ﬁnd that Eqs. (4.124) and
(4.125) correspond to the MAP solutions. Thus, we have found that the i-smoothing in
the MMI and MPE solutions can be interpreted as MAP. From Eq. (4.123), the smooth-
ing terms that come from D can also be similarly interpreted as MAP, when we consider
the following Gaussian–gamma prior distribution:
p(μ, ) = NGam(μ, r|μDT[τ], D, DT[τ]D, D + 2).
(4.126)

4.6 Speaker recognition/veriﬁcation
171
Therefore, we could also provide the MAP interpretation of the D-related terms in
the extended Baum–Welch algorithm. However, how to provide this prior distribution in
the objective function is not trivial, and the theoretical analysis of this interpretation in
the discriminative training framework is an interesting open question.
Povey & Woodland (2002) use this i-smoothing technique with MMI and MPE esti-
mation methods, and report 1% absolute WER improvement from the MMI estimation
method without the i-smoothing technique. In addition, using the speaker independent
HMM parameters as prior hyperparameters (Povey, Gales, Kim et al. 2003, Povey,
Woodland & Gales 2003) also realizes discriminative acoustic model adaptation based
on the MAP estimation. There are several studies of using Bayesian approaches to
discriminative training of acoustic models (e.g., based on minimum relative entropy
discrimination (Kubo, Watanabe, Nakamura et al. 2010)), and Section 5.2 also intro-
duces the Bayesian sensing HMM with discriminative training based on the evidence
framework.
4.6
Speaker recognition/veriﬁcation
In this section we focus on text-independent speaker recognition or speaker veriﬁca-
tion systems, and show how MAP estimation is used. Speaker recognition is a similar
problem to automatic speech recognition. Let O ∈RD be a sequence of D dimensional
speech feature vectors. Usually O is one utterance by a speciﬁc speaker. Similarly to
speech recognition, MFCC is usually used as a feature. The speaker recognition task is
to estimate speaker label ˆc among a speaker set C by using the maximum a-posteriori
estimation for the posterior distribution:
ˆc = arg max
c∈C p(c|O),
(4.127)
where p(c|O) is obtained from a statistical speaker model, and is discussed later. This is
similar to a speech recognition problem, as shown in Eq. (3.2) by replacing the estima-
tion target from the word sequence W with the speaker index c. Since the output is not
structured compared with W, speaker recognition can be realized by relatively simple
models compared with ASR.
Speaker veriﬁcation is to determine whether O is spoken by a target speaker s, which
is regarded as single-speaker detection. This is reformulated as a basic test between two
hypotheses:
• H0 : O is from the hypothesized speaker s;
• H1 : O is not from the hypothesized speaker s.
And the veriﬁcation is performed by comparing the posterior distributions of H0 and H1
as follows:
p(H0|O)
p(H1|O)

≥ϵ
accept
H0
< ϵ
reject
H0
,
(4.128)

172
Maximum a-posteriori approximation
where ϵ is a decision threshold. By using the Bayes rule, we can rewrite Eq. (4.128)
with a likelihood ratio as follows:
p(H0|O)
p(H1|O) = p(O|H0)p(H0)
p(O|H1)p(H1) ≈p(O|H0)
p(O|H1),
(4.129)
where we disregard the contribution of the prior distributions of each hypothesis. Thus,
by using the generative model of the H0 and H1, we can compare the two hypotheses.
We use GMM as these generative models. Hence, p(O|H0) and p(O|H1) are represented
from p(O|H0) and p(O|H1), where H0 and H1 are sets of GMM parameters.
4.6.1
Universal background model
The generative model of H1 must consider the characteristics of many speakers. That
can be achieved by training the GMM parameters H1 from many speakers. The GMM
of H1 is called the universal background model (UBM), and UBM denotes its GMM
parameters. Therefore, the likelihood for test data O can be computed by:
p(O|H1) = p(O|UBM) =
T

t=1
K

k=1
ωUBM
k
N(ot|μUBM
k
, UBM
k
),
(4.130)
where UBM are computed from many training data O in advance, uttered by various
speakers to train UBM with the ML training as follows:
UBM = arg max
 p(O|),
(4.131)
which can be performed efﬁciently by using the EM algorithm, as we discussed in
Section 3.4.
While we use many data O to train UBM, the hypothesis speaker model H0 with
model parameters HYP can be trained by using only a small amount of data O (e.g., one
utterance). Although ML has an over-training problem in this setting, MAP estimation
can avoid the problem, and estimate HYP as follows:
HYP = arg max
 p(|O)
= arg max
 p(O|)p(|(UBM)).
(4.132)
where p(|(UBM)) is a prior distribution and (UBM) are hyperparameters of
GMM parameters. Note that some of the hyperparameters are obtained from the UBM
parameters UBM. This is a very similar technique to MAP adaptation of CDHMM
parameters, as we discussed in Section 4.4.2, where the target model is based on a
speaker-dependent CDHMM while the prior model is based on a speaker-independent
CDHMM. Equation (4.132) is a subset solution of CDHMM, as we discussed in Section
4.3, and the HYP is obtained as follows:
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
ωHYP
k
=
ˆφω
k −1
K
k′=1( ˆφω
k′−1),
μHYP
k
= ˆμk,
HYP
kd
=
ˆrkd
ˆφR
k −2,
(4.133)

4.6 Speaker recognition/veriﬁcation
173
where
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
ˆφω
k
≜φω
k + T
t=1 γt(k),
ˆφμ
k
≜φμ
k + T
t=1 γt(k),
ˆμk
≜φμ
k μ0
k+T
t=1 γt(k)ot
φμ
k +T
t=1 γt(k)
,
ˆφR
k
≜φR
k + T
t=1 γt(k),
ˆrkd
≜T
t=1 γt(k)o2
td + φμ
k (μ0
kd)2 −ˆφμ
k ( ˆμkd)2 + r0
kd.
(4.134)
Reynolds et al. (2000) suggest using speciﬁc hyperparameter settings for φω
k , φμ
k , μ0
k,
r0
kd to obtain the following forms:
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
ωHYP
k
=
αw
k γk/T + (1 −αw
k )ωUBM
k
K
k′=1 αw
k′γk′/T + (1 −αw
k′)ωUBM
k′
,
μHYP
k
= αm
k γ (1)
k
+ (1 −αm
k )μUBM
k
,
HYP
kd
= αv
kγ (2)
kd + (1 −αv
k)((μUBM
kd
)2 + UBM
kd
) −(μHYP
kd
)2,
(4.135)
where αw
k , αm
k , and αv
k are hyperparameters, and can be controlled by a tuning parameter
β, as follows:
αw,m,v
k
=
γk
γk + β .
(4.136)
Note that this solution also has the MAP property of avoiding sparse data problems.
The hypothesis test in Eq. (4.128) can be performed by considering the likelihood
ratio test of UBM and HYP GMMs as
p(O|HYP)
p(O|UBM)

≥ϵ
accept
H0
< ϵ
reject
H0.
(4.137)
Thus, we have shown that MAP estimation plays an important role in speaker veriﬁca-
tion based on UBM, especially in estimating the hypothesis speaker model.
4.6.2
Gaussian super vector
The MAP estimation of speaker models is further developed by using the Gaussian
super vector technique (Campbell, Sturim & Reynolds 2006). The idea of this approach
is to consider the MAP estimated GMM parameters as a feature of speaker veriﬁ-
cation or speaker recognition. The veriﬁcation/recognition is performed by using a
multi-class Support Vector Machine (SVM) (Vapnik 1995), cosine similarity scoring,
or other simple classiﬁer. Suppose we have On features, the GMM–UBM process
can create the following super vector by concatenating the Gaussian mean vector
{μHYP
k,n |k = 1, · · · , K}, estimated from On:

174
Maximum a-posteriori approximation
μn ≜
⎡
⎢⎢⎢⎢⎣
μHYP
1,n
μHYP
2,n
...
μHYP
K,n
⎤
⎥⎥⎥⎥⎦
.
(4.138)
The super vector is also obtained by using the vectorized form of the transformation
matrix estimated by using the MLLR algorithm (Stolcke et al. 2005). The technique is
widely used for speaker and language recognition tasks (Kinnunen & Li 2010), and
it can usually be used with factor analysis techniques (Kenny 2010, Dehak, Kenny,
Dehak et al. 2011) by representing the super vector with the speaker-speciﬁc and other
(channel) factors:
μn = μ + U1x1

speaker
+ U2x2n
  
channel
+ϵn,
(4.139)
where the speaker and channel speciﬁc factors are also represented by the linear model
with the transformation matrices U1 and U2. μ is a bias vector, and ϵn is a noise vector.
The approach is also applied to video processing (Shinoda & Inoue 2013). Thus, MAP
estimation is still used as an important component of speaker veriﬁcation tasks, but the
techniques have been developed further based on the above factor analysis. Section 7.5
describes a VB solution of this factor analysis.
4.7
n-gram adaptation
MAP estimation is also used to obtain the n-gram language model (Federico 1996,
Masataki, Sagisaka, Hisaki et al. 1997). In the n-gram language model, the genera-
tive probability of the word sequence wN
1 = {wi ∈V|i = 1, · · · , N} with vocabulary V
can be basically represented as a product of multinomial distributions, as discussed in
Section 3.6:
p(wN
1 ) =
N

i=1
p(wi|wi−1
1
) ≈
N

i=1
p(wi|wi−1
i−n+1)
≈
N

i=1
Mult(wi|θwi|wi−1
i−n+1),
(4.140)
where  = {θwi|wi−1
i−n+1} denotes the n-gram parameters. As discussed in Eq. (3.197), the
ML estimate of  is obtained by using the number of occurrences c(wi
i−n+1) of word
sequence wi
i−n+1 in training corpus D:
θML
wi|wi−1
i−n+1 = arg
max
θwi|wi−1
i−n+1
p(D|θwi|wi−1
i−n+1)
=
c(wi
i−n+1)

wi c(wi
i−n+1).
(4.141)

4.7 n-gram adaptation
175
Note that we do not consider the smoothing techniques in this section to make the
discussion simple.
4.7.1
MAP estimation of n-gram parameters
The MAP extension from the above ML framework can be performed by considering
the posterior distribution and introducing the prior distribution as follows:
θMAP
wi|wi−1
i−n+1 = arg
max
θwi|wi−1
i−n+1
p(θwi|wi−1
i−n+1|D)
= arg
max
θwi|wi−1
i−n+1
p(D|θwi|wi−1
i−n+1)p(θwi|wi−1
i−n+1).
(4.142)
Since p(D|θwi|wi−1
i−n+1) is a multinomial distribution, as discussed in Section 2.1.4, we use
the following Dirichlet distribution for p(θwi|wi−1
i−n+1) in Appendix C.4:
p(θwi|wi−1
i−n+1) = Dir({θwi|wi−1
i−n+1}wi|{φwi|wi−1
i−n+1}wi).
(4.143)
Thus, we can analytically solve Eq. (4.142) as follows:
θMAP
wi|wi−1
i−n+1 =
φwi|wi−1
i−n+1 −1 + c(wi
i−n+1)

wi φwi|wi−1
i−n+1 −1 + c(wi
i−n+1).
(4.144)
This is a similar result to the MAP solutions of mixture weights or transition proba-
bilities in Section 4.3.5. Since the n-gram parameter estimation in this setting does not
include the latent variables, we can obtain the solution without using the EM algorithm.
Therefore, the difference between Eq. (4.144) and those in Section 4.3.5 is between
using discrete counts c(wi
i−n+1) or EM-based expected counts γ and ξ, which are con-
tinuous values. Note that the parameters represented by a Dirichlet distribution always
satisfy the sum-to-one condition required for n-gram language modeling.
4.7.2
Adaptation method
Similarly to MAP estimation based speaker adaptation for HMM parameters, MAP esti-
mation of n-gram parameters can be used for speaker/task adaptations (Federico 1996,
Masataki et al. 1997). Let DSI be the speaker (or task) independent corpus, and DSD be
the speaker (or task) dependent corpus. The following hyperparameter setting is often
used:
φwi|wi−1
i−n+1 = α

wi
cSI(wi
i−n+1)θSI, ML
wi|wi−1
i−n+1
+ 1.
(4.145)
Here θSI, ML
wi|wi−1
i−n+1
is obtained from the ML estimation in Eq. (4.141) by using DSI. Simi-
larly, cSI(wi
i−n+1) is a word count obtained from DSI. Then, the numerator of the MAP
estimation in Eq. (4.144) is rewritten as:

176
Maximum a-posteriori approximation
θMAP
wi|wi−1
i−n+1 ∝α

wi
cSI(wi
i−n+1)θSI, ML
wi|wi−1
i−n+1
+ c(wi
i−n+1)
∝α

wi
cSI(wi
i−n+1)θSI, ML
wi|wi−1
i−n+1
+

wi
cSD(wi
i−n+1)θSD, ML
wi|wi−1
i−n+1
∝
α 
wi cSI(wi
i−n+1)
α 
wi cSI(wi
i−n+1) + 
wi cSD(wi
i−n+1)θSI, ML
wi|wi−1
i−n+1
+

wi cSD(wi
i−n+1)
α 
wi cSI(wi
i−n+1) + 
wi cSD(wi
i−n+1)θSD, ML
wi|wi−1
i−n+1
.
(4.146)
Note that

wi cSD(wi
i−n+1)
α 
wi cSI(wi
i−n+1) + 
wi cSD(wi
i−n+1)
+
α 
wi cSI(wi
i−n+1)
α 
wi cSI(wi
i−n+1) + 
wi cSD(wi
i−n+1) = 1.
(4.147)
Therefore, Eq. (4.146) can be regarded as a well-known linear interpolation of two n-
gram language model parameters θSI, ML and θSD, ML, i.e.,
θMAP
wi|wi−1
i−n+1 = α(wi
i−n+1)θSI, ML
wi|wi−1
i−n+1
+

1 −α(wi
i−n+1)

θSD, ML
wi|wi−1
i−n+1
,
(4.148)
where α(wi
i−n+1) is a linear interpolation ratio deﬁned as:
α(wi
i−n+1) ≜
α 
wi cSI(wi
i−n+1)
α 
wi cSI(wi
i−n+1) + 
wi cSD(wi
i−n+1).
(4.149)
The linear interpolation ratio depends on the count of each corpus and hyperparameter α.
This linear interpolation based MAP solution can be regarded as an instance of
well-known interpolation smoothing techniques in n-gram language modeling (Chen
& Goodman 1999, Rosenfeld 2000), as discussed in Section 3.6.2. The analytical result
shows that the linear interpolation technique can be viewed as the MAP estimation of
n-gram parameters in a Bayesian sense.
4.8
Adaptive topic model
We are facing the era of big data. The volume of data collections grows vastly. Statis-
tical document modeling becomes increasingly important in language processing areas.
As addressed in Section 3.7.3, probabilistic latent semantic analysis (PLSA) has been
developed to represent a set of documents according to the maximum likelihood (ML)
principle. The semantics and statistics can be effectively captured for document repre-
sentation. However, PLSA is highly sensitive to the target domain, which is continuously
changing in real-world applications. Similarly to the adaptation of hidden Markov mod-
els to a new speaker in Section 4.3 and the adaptation of n-gram models to a new

4.8 Adaptive topic model
177
recognition task in Section 4.7, we are interested in adapting the topic-based docu-
ment model using PLSA to a new application domain from a set of application-speciﬁc
documents.
A Bayesian PLSA framework (Chien & Wu 2008) is presented to establish an adap-
tive topic model to improve document representation by incrementally extracting the
up-to-date latent semantic information to match the changing domains at run time. The
Dirichlet distribution is introduced to serve as the conjugate priors for PLSA param-
eters, which are multinomial distributed. The reproducible prior/posterior distributions
facilitate two kinds of adaptation applications. One is corrective training while the other
is incremental learning. An incremental PLSA is constructed to accomplish the param-
eter estimation as well as the hyperparameter updating. Differently from standard PLSA
using an ML estimate, the Bayesian PLDA is capable of performing dynamic document
indexing and modeling. The mechanism of adapting a topic model based on Bayesian
PLSA is similar to the mechanisms of folding-in (Berry et al. 1995) and SVD updating
(Bellegarda 2002) based on latent semantic analysis (LSA)(Berry et al. 1995, Bellegarda
2000), which is known as a nonparametric approach. The updating and downdating in
an SVD-based LSA framework could not be directly applied for an ML-based PLSA
framework. To add up-to-date or remove out-of-date knowledge, the adaptive PLSA is
developed for document modeling. The goal of adaptive PLSA aims to use the newly
collected documents, called adaptation documents, to adapt an existing PLSA model
to match the domains of new queries or documents in information retrieval systems. In
Chien & Wu (2008), adaptive PLSA is shown to be superior to adaptive LSA in informa-
tion retrieval tasks. In what follows, we address the methods of maximum a-posteriori
estimation and quasi-Bayes estimation designed for corrective training and incremental
learning, respectively.
4.8.1
MAP estimation for corrective training
Corrective training is intended to use batch collection data to correct the ML-based
PLSA parameters ML to ﬁt new domain knowledge via the MAP estimation. In a topic
model based on PLSA, two sets of multinomial parameters  = {p(w(v)|k), p(k|dm)}
have been estimated in the training phase subject to the constraints of multinomial distri-
butions as given in Eq. (3.304). The ﬁrst one is the topic-dependent unigram probability
p(w(v)|k) of a vocabulary word w(v), and the second one is the posterior probability
p(k|dm) of topic k given an observed document dm. According to MAP estimation, we
adapt PLSA parameters  = {p(w(v)|k), p(k|dm)} by maximizing the a-posteriori prob-
ability or the sum of logarithms of likelihood function p(D|) of adaptation words and
documents D = {w(v), dm|v = 1, · · · , |V|, m = 1, · · · , M} and prior distribution p():
MAP = arg max
 p(|D)
= arg max
 log p(D|) + log p().
(4.150)
Here, prior distribution p() represents the randomness of multinomial parameters
{p(w(v)|k)} and {p(k|dm)}. Again, it is mathematically attractive to select the conjugate

178
Maximum a-posteriori approximation
prior as the candidate for Bayesian inference. The Dirichlet distribution is known as the
conjugate prior for multinomial parameters. Owing to the selection of conjugate prior,
two properties of Bayesian learning could be obtained: 1) a closed-form solution for
rapid adaptation; and 2) a reproducible prior/posterior distribution pair for incremen-
tal learning. Assuming parameters {p(w(v)|k)} and {p(k|dm)} are independent, the prior
distribution of the entire parameter set based on Dirichlet density is expressed by
p(|) ∝
K

k=1
⎡
⎣
|V|

v=1
p(w(v)|k)αvk−1
M

m=1
p(k|dm)βkm−1
⎤
⎦,
(4.151)
where  = {αvk, βkm} denote the hyperparameters of Dirichlet densities. Following the
EM algorithm, we implement Eq. (4.151) by calculating the posterior auxiliary function
QMAP(′|) = E(Z)[log p(D, Z|′)|D, ] + log p(′|).
(4.152)
By imposing the constraints of multinomial parameters in Eq. (3.304) into the con-
strained optimization, we form the extended auxiliary function as
˜QMAP(′|)
∝
K

k=1
|V|

v=1

 M

m=1
c(w(v), dm)p(zw(v) = k|w(v), dm) + (αvk −1)

× log p′(w(v)|k)

+ ηw
⎛
⎝1 −
|V|

v=1
p′(w(v)|k)
⎞
⎠
+
K

k=1
M

m=1

⎛
⎝
|V|

v=1
c(w(v), dm)p(zw(v) = k|w(v), dm) + (βkm −1)
⎞
⎠
× log p′(k|dm)

+ ηd

1 −
K

k=1
p′(k|dm)

,
(4.153)
which is manipulated and extended from the ML auxiliary function QML(′|) given
in Eq. (3.295). In Eq. (4.153), ηw and ηd denote the Lagrange multipliers for two con-
straints of multinomial parameters. Then we differentiate Eq. (4.153) with respect to
individual multinomial parameters p′(w(v)|k) and set it to zero:
∂˜QMAP(′|)
∂p′(w(v)|k)
=
M
m=1 c(w(v), dm)p(zw(v) = k|w(v), dm) + (αvk −1)
p′(w(v)|k)
−ηw = 0,
(4.154)
and obtain
p′(w(v)|k) = 1
ηw

 M

m=1
c(w(v), dm)p(zw(v) = k|w(v), dm) + (αvk −1)

.
(4.155)

4.8 Adaptive topic model
179
By substituting this result into the constraint |V|
v=1 p′(w(v)|k) = 1, we ﬁnd the Lagrange
parameter
ηw =
|V|

v=1
& M

m=1
c(w(v), dm)p(zw(v) = k|w(v), dm) + (αvk −1)
'
.
(4.156)
Accordingly, we derive the MAP estimates of two PLSA parameters in closed form:
pMAP(w(v)|k) =
M
m=1 c(w(v), dm)p(zw(v) = k|w(v), dm) + (αvk −1)
|V|
j=1
+M
m=1 c(w( j), dm)p(zwj = k|w( j), dm) + (αjk −1)
,.
(4.157)
pMAP(k|dm) =
|V|
v=1 c(w(v), dm)p(zw(v) = k|w(v), dm) + (βkm −1)
K
j=1
+|V|
v=1 c(w(v), dm)p(zw(v) = j|w(v), dm) + (βjm −1)
,
=
|V|
v=1 c(w(v), dm)p(zw(v) = k|w(v), dm) + (βkm −1)
c(dm) + K
j=1(βjm −1)
,
(4.158)
where the posterior probability p(zw(v)
=
k|w(v), dm) is calculated according to
Eq. (3.311) by using adaptation documents D based on the current estimates  =
{p(w(v)|k), p(k|dm)}. MAP estimates in Eq. (4.158) are seen as an extension of ML
estimates of Eq. (3.308) and Eq. (3.309) by interpolating with the prior statistics
{αvk} and {βkm}, respectively. If prior density is non-informative or adaptation data
D are abundant, MAP estimates are reduced to ML estimates. The MAP PLSA algo-
rithm is developed for corrective training or batch adaptation, and adapts the existing
parameters to MAP in a single epoch. In MAP PLSA, the Dirichlet priors and their
hyperparameters  = {αvk, βkm} are adopted to characterize the variations of topic-
dependent document and word probabilities. These priors are used to express the
environmental variations. MAP PLSA involves both word-level p(w(v)|k) and document-
level p(k|dm) parameters. In general, MAP parameters MAP perform better than
ML parameters ML when classifying future documents with new terms, topics, and
domains.
4.8.2
Quasi-Bayes estimation for incremental learning
Using MAP estimation, only a single learning epoch is performed to correct PLSA
parameters. Batch learning is performed. However, batch learning cannot catch the
continuously changing domain knowledge or deal with the non-stationary documents
collected from real-world applications. An adaptive information system should con-
tinuously update system parameters with new words and topics. Out-of-date words or
documents should fade away from the system as time moves on. Accordingly, we tackle
the updating and downdating problems simultaneously for latent semantic indexing.
MAP PLSA could not incrementally accumulate statistics for adaptive topic modeling.
It is more interesting to develop an incremental learning algorithm to track the changing

180
Maximum a-posteriori approximation
topics and domains in test documents. A learning procedure is executed repeatedly in
different epochs. Incremental learning is also known as sequential learning or online
learning, which is important for speaker adaptation in automatic speech recognition
systems where speaker characteristics gradually change with time (Huo & Lee 1997,
Chien 1999).
To implement incremental learning for adaptive topic modeling, we continuously
estimate PLSA parameters in different learning epochs using the incrementally observed
adaptation documents. At the n learning epoch, we estimate PLSA parameters by
maximizing the posterior distribution using a sequence of adaptation documents
Dn = {D1, · · · , Dn}:
((n))QB = arg max

p(|Dn)
= arg max

p(Dn|)p(|Dn−1)
≈arg max

p(Dn|)p(|(n−1)),
(4.159)
where the posterior distribution p(|Dn−1) is approximated by the closest tractable
prior distribution p(|(n−1)) with sufﬁcient statistics or hyperparameters (n−1) which
are evolved from history documents Dn−1. This estimation method is also called the
quasi-Bayes (QB) estimation (Huo & Lee 1997, Chien 1999). QB estimation provides
recursive learning of PLSA parameters,
(1) →(2) →· · · →(n),
(4.160)
from incrementally observed documents,
D1 →D2 →· · · →Dn.
(4.161)
At each epoch, we only use the current block of documents Dn = {w(n)
(v), d(n)
m |v =
1, · · · , |V|, m = 1, · · · , Mn} and the accumulated statistics (n−1) to update PLSA
parameters from (n−1) to (n). Current block data Dn are released after accumulat-
ing statistics from (n−1) to (n). Memory and computation requirements are reduced at
each epoch. The key technique in QB PLSA comes from the introduction of incremental
hyperparameters. If we substitute the hyperparameters (n−1) = {α(n−1)
vk
, β(n−1)
km
} into
Eq. (4.157) and Eq. (4.158), the QB estimates ((n))QB = {pQB(w(n)
(v)|k), pQB(k|d(n)
m )} are
obtained. This QB PLSA method is geared with the updating mechanism of hyperpa-
rameters, which is derived by the E-step for QB estimation in Eq. (4.159). By referring
to the auxiliary function of MAP PLSA in Eq. (4.152) and Eq. (4.153), the QB auxil-
iary function of new estimates ((n))′ = {p′(w(n)
(v)|k), p′(k|d(n)
m )} given current estimates
(n) = {p(w(n)
(v)|k), p(k|d(n)
m )} is deﬁned by

4.8 Adaptive topic model
181
QQB(((n))′|(n))
∝
K

k=1
|V|

v=1

 Mn

m=1
c(w(n)
(v), d(n)
m )p(k|w(n)
(v), d(n)
m )
+ (α(n−1)
vk
−1)

log p′(w(n)
(v)|k)

+
K

k=1
Mn

m=1

 |V|

v=1
c(w(n)
(v), d(n)
m )p(k|w(n)
(v), d(n)
m )
+ (β(n−1)
km
−1)

log p′(k|d(n)
m )

.
(4.162)
It is important that the exponential of the QB auxiliary function in Eq. (4.162) can be
arranged as a new Dirichlet distribution:
exp
0
QQB(((n))′|(n))
1
∝
K

k=1
⎡
⎣
V

v=1
p′(w(n)
(v)|k)α(n)
vk −1
Mn

m=1
p′(k|d(n)
m )β(n)
km−1
⎤
⎦,
(4.163)
with the updated hyperparameters (n) = {α(n)
vk , β(n)
km} derived thus:
α(n)
vk =
Mn

m=1
c(w(n)
(v), d(n)
m )p(k|w(n)
(v), d(n)
m ) + α(n−1)
vk
,
(4.164)
β(n)
km =
|V|

v=1
c(w(n)
(v), d(n)
m )p(k|w(n)
(v), d(n)
m ) + β(n−1)
km
,
(4.165)
where the posterior probability
p(k|w(n)
(v), d(n)
m ) =
p(w(n)
(v)|k)p(k|d(n)
m )
K
j=1 p(w(n)
(v)|j)p(j|d(n)
m )
(4.166)
is obtained by using the current block of adaptation data Dn = {w(n)
(v), d(n)
m } based on
current QB estimates (n) = {p(w(n)
(v)|k), p(k|d(n)
m )}. Importantly, a reproducible dis-
tribution pair of a prior in Eq. (4.151) and a posterior in Eq. (4.163) is established.
This property is crucial to activate the updating mechanism of hyperparameters for
incremental learning. New hyperparameters (n) = {α(n)
vk , β(n)
km} are estimated by com-
bining the previous hyperparameters (n−1) = {α(n−1)
vk
, β(n−1)
km
} with the accumulated
statistics from adaptation documents Dn = {w(n)
(v), d(n)
m } at learning epoch n.

182
Maximum a-posteriori approximation
Table 4.1 Numbers of training, adaptation, and test documents for ﬁve populous classes in the
Reuters-21578 dataset.
Acquisitions
Crude
Earn
Money-fx
Trade
Number of
training documents
825
196
1447
284
189
Number of added
documents per epoch
275
65
475
85
60
Number of
test documents
719
189
1087
180
117
Basically, QB estimation ﬁnds the point estimate for adaptive topic modeling. This
estimation is seen as an extended realization of MAP estimation by activating the
mechanism of hyperparameter updating so that incremental learning is established to
compensate the non-stationary variations in observation data which may be speech
signals, word sequences, or text documents. Incremental learning based on QB esti-
mation is helpful for speech and language applications including speech recognition,
information retrieval, and others.
4.8.3
System performance
The performance of the adaptive topic model was evaluated through the tasks of correc-
tive training and incremental learning. The evaluation was performed for the application
of document categorization. Table 4.1 shows the set-up of experimental data of the
Reuters-21578 dataset. We collected training, adaptation, and test documents from
the ﬁve most populous categories in the Reuters-21578 dataset for system evaluation.
Preprocessing stages of stemming and stop word removal were done. In the task of
incremental learning, we used one-third of the adaptation documents at each epoch and
investigated the effect of incremental learning in three learning epochs. The performance
of corrective training and incremental learning was compared. Training samples of each
category were roughly partitioned into half for training and the other half for adapta-
tion. A ﬁvefold cross validation over training and adaptation sets was performed. In the
implementation, we determined PLSA probability for each test document. The cosine
similarity of feature vectors between a test document and a given class model was calcu-
lated for pattern classiﬁcation. The class feature vector consisted of PLSA probabilities
averaging over all documents corresponding to a class. The classiﬁcation error rate was
computed over all test documents in ﬁve populous classes. We obtained the classiﬁcation
error rates for PLSA (Hofmann 1999b, 2001) (3.47%), SVD updating (Bellegarda 2002)
(3.39%), MAP PLSA (Chien & Wu 2008) (3.04%), QB PLSA (Chien & Wu 2008) at
1st learning epoch (3.13%), QB PLSA at 2nd learning epoch (3.04%), and QB PLSA
at 3rd learning epoch (3%). Corrective training using SVD updating and MAP PLSA
and incremental learning using QB PLSA at different epochs decrease the classiﬁca-
tion error rates. SVD updating is worse than MAP PLSA and QB PLSA. Incremental
learning using QB PLSA performs slightly better than batch learning using MAP PLSA.

4.9 Summary
183
4.9
Summary
This chapter introduced various applications of MAP approximation for model
parameter posteriors. Since the approach can be easily realized from the existing ML
based approaches by simply considering the regularization term based on model param-
eter priors, it is widely used for speech and language processing including acoustic and
language modeling in ASR, speaker veriﬁcation, and document processing. Although
the MAP approximation can utilize the most famous Bayesian advantage of “use of prior
knowledge,” it does not deal with probabilistic variables explicitly with the marginaliza-
tion, and it does not fully utilize the Bayesian concept. The following chapters consider
more strict Bayesian approaches for speech and language processing.

5
Evidence approximation
In a maximum a-posteriori (MAP) approximation as addressed in Chapter 4, we treat
model parameters  of a target model M as unknown but deterministic variables which
are estimated by maximizing the posterior probability p(|O) using some observation
data O. Prior distribution p(|) with heuristically determined hyperparameters  is
introduced. This is known as the point estimation of a target model. However, from a
full Bayesian perspective, we treat model parameters as random variables where the ran-
domness is represented by a prior distribution p(|). In contrast with point estimation
in a MAP approximation based on heuristic hyperparameters, the distribution estima-
tion is implemented for full Bayesian learning. According to the distribution estimation,
we ﬁnd the whole prior distribution p(|), or equivalently estimate the corresponding
hyperparameters  from observations O in an empirical fashion. In this implementation,
the marginalization of likelihood function p(O|) with respect to model parameters 
should be calculated for model construction, as follows:1
p(O|) =

p(O|)p(|)d.
(5.1)
Rather than trusting the point estimate in MAP approximation, the resulting evidence
function p(O|) in evidence framework considers all possible values of model parame-
ters when making a prediction of O as new observation data or training data depending
on the task. In cases of complicated model structure and coupled latent variables, this
evidence function is prone to be intractable and should be approximated to estimate the
optimal hyperparameters . For the applications of speech and language processing, we
focus on acoustic modeling and language modeling in accordance with the distribution
estimation based on evidence approximation.
In what follows, Section 5.1 ﬁrst presents the evidence framework and addresses
the type-2 maximum likelihood estimation for general pattern recognition. The optimal
hyperparameters are estimated based on this framework. The optimization of evidence
function or marginal likelihood is then extended to sparse Bayesian learning for acous-
tic modeling based on Bayesian sensing hidden Markov models (Saon & Chien 2012a)
in Section 5.2. In this section, the scheme of automatic relevance determination is
introduced and illustrated. In addition, evidence approximation to a hierarchical Dirich-
let language model (MacKay & Peto 1995) is detailed in Section 5.3. The optimal
1 This chapter regards model structure M and parameters of prior distribution  as the hyperparameters in a
broad sense. Equation (5.1) formulates the likelihood function given , but the discussion can be applied
to the case of using M.

5.1 Evidence framework
185
hyperparameters are obtained for the acoustic model as well as the language model.
These Bayesian models are beneﬁcial for noise-robust speech recognition and large
vocabulary continuous speech recognition.
5.1
Evidence framework
This section begins with a general discussion of the evidence framework. We ﬁrst review
the well-known Bayes theorem, as discussed in Section 2.1. The evidence function
p(O|), given prior parameters , is introduced in the Bayes theorem, which relates
posterior distribution p(|O, ) to the likelihood function p(O|), prior distribution
p(|), and the evidence function p(O|) by
p(|O, ) = p(O|)p(|)
p(O|)
=
p(O|)p(|)

p(O|)p(|)d.
(5.2)
In words:
Posterior = Likelihood × Prior
Evidence
.
(5.3)
The evidence function is a marginal likelihood function which takes all values of model
parameters  into account. It is precisely the normalization term that appears in the
denominator in Bayes’ theorem as shown in Eq. (5.2).
Although the evidence function has appeared in the previous sections (e.g., the MAP
approximation in Eq. (4.2)), it has not been explicitly considered so far. However, the
evidence function p(O|) can directly link the hyperparameters  and observations O
by marginalizing the model parameters , and can be used to infer the hyperparameters
 in the Bayesian framework.
5.1.1
Bayesian model comparison
This section also considers the model structure M in the evidence framework. In
MacKay (1992a), the evidence framework was proposed to conduct a Bayesian model
comparison, where the best model or model structure M is selected according to the
posterior distribution of M as
ˆM = arg max
M p(M|O) = arg max
M p(O|M)p(M).
(5.4)
Here p(M) is a prior distribution of the model structure M. In the case that each model
is equally probable or has uniform probability, i.e., p(M) = constant, different mod-
els M are ranked according to the evidence function p(O|M), which is obtained by
marginalizing the model parameters  based on the product and sum rules as
p(O|M) =

p(O, |M)d
=

p(O|, M)p(|M)d.
(5.5)

186
Evidence approximation
Figure 5.1
Evidence approximation. Adapted from Bishop (2006).
The marginalization of likelihood function over model parameters  or model structure
is calculated to come out with a meaningful objective for model selection. However,
we may obtain some insight into the model evidence by making a simple approxima-
tion to the integral over parameters  (MacKay 1992a, Bishop 2006). Consider the
case of a model with a posterior distribution which is sharply peaked around the most
probable value MAP, with the width △posterior. Then the posterior distribution is rep-
resented as p(|O) =
1
△posterior . Similarly, we also assume that the prior is ﬂat with
width △prior, and we have p() =
1
△prior . Therefore, by using the Bayes theorem, the
evidence function in Eq. (5.5) can be approximated without solving the integral as:
p(O) =

p(O|)p()d = p(O|)p()
p(|O)
≈p(O|MAP)△posterior
△prior
,
(5.6)
where we omit the model structure index M for simplicity. The approximation to model
evidence is illustrated by Figure 5.1. Without loss of generality, the notation  in
Eq. (5.6) is treated as a single parameter. Taking the logarithm, we obtain
log p(O) ≈log p(O|MAP) + log
△posterior
△prior




Occam factor
.
(5.7)
In this approximation, the ﬁrst term represents the goodness-of-ﬁt to the data O given
the most probable parameter value MAP. The second term is known as the Occam factor
(MacKay 1992a) which penalizes the model according to its complexity. Theoretically,
we have the property △posterior < △prior. The Occam factor is negative and it increases
in magnitude as the ratio △posterior
△prior
gets smaller. Thus, if parameters are ﬁnely tuned to
the data in posterior distribution, then the penalty term is large. In practice, the model
complexity or the Occam factor is multiplied by the number of adaptive parameters N
in . The optimal model complexity, as determined by the maximum evidence, is given
by a trade-off between these two competing terms. A reﬁned version of this evidence
approximation could be further derived as:

5.1 Evidence framework
187
log p(O) ≈log p(O|MAP) + log p(MAP) + N
2 log(2π) −1
2 log |H|



Occam factor
,
(5.8)
by using the Laplace approximation, which is detailed in Section 6.1. In Eq. (5.8), H is
the Hessian matrix:
H = −∇∇log p(O|MAP)p(MAP) = −∇∇log p(MAP|O),
(5.9)
which is the second derivative of the negative log posterior. The determinant of
this matrix plays an important role in the Occam factor, which penalizes the model
complexity.
We can attain further insight into Bayesian model comparison and understand how
the marginal likelihood is favorable to the models with intermediate complexity by
considering Figure 5.2 (Bishop 2006). Here, the horizontal axis is a one-dimensional
representation of the data space O. We consider three models M1, M2, and M3 in which
M1 is the simplest and M3 is the most complex. When generating a particular data set
from a speciﬁc model, we ﬁrst choose the values of the parameters from their prior dis-
tribution p(). Then, given these parameter values, we sample the data from p(O|). A
simple or shallow model has little variability and so will generate data sets using p(O),
which is conﬁned to a relatively small region in space O. In contrast, a complex or deep
model can generate a variety of data sets, and so its distribution p(O) is spread over a
large region of the data space O. In this example, for the particular observed data set
OM, the model M2 with intermediate complexity has the largest evidence.
5.1.2
Type-2 maximum likelihood estimation
The complexity of a model M is generally deﬁned by the scope of data set O that model
M could predict. This scope is not only determined by the model size (e.g., model
structure, model order, or number of parameters N), but is also affected by the hyperpa-
rameters  of the parameters  which are used to generate the observations O. Instead
of point estimation of model parameters  in MAP approximation based on heuristically
determined hyperparameters, the evidence approximation conducts distribution estima-
tion which determines the optimal prior distribution ˆp(|) as a whole, or equivalently
Figure 5.2
Model comparison based on the maximum evidence. Adapted from Bishop (2006).

188
Evidence approximation
infers the optimal hyperparameters ˆ corresponding to the prior distribution according
to
ˆ = ML2 = arg max

p(O|)
= arg max


p(O|, )p(|)d.
(5.10)
Differently from conventional ML estimation for model parameters , the type-2 max-
imum likelihood (ML2) estimation aims to search the optimal hyperparameters ML2
from the observation data O. In the literature, a point estimation based on ML or MAP
conducts the so-called level-1 inference, while the distribution estimation based on ML2
undertakes the level-2 inference (MacKay 1995, Kwok 2000). The level-3 inference is
performed to rank different models M according to the posterior probability p(M|O) or
the evidence function p(O|M) with equally probable model M. Three levels of inference
can be iterated.
The evidence framework or the ML2 estimation has been developed for different
learning machines including linear regression/classiﬁcation networks (MacKay 1992b,
Bishop 2006), feed-forward neural network (NN) (MacKay 1992c), support vector
machine (SVM) (Kwok 2000), and hidden Markov model (Zhang, Liu, Chien et al.
2009). For the cases of linear regression and neural network regression models, the
optimal hyperparameters of weight parameters and modeling errors are estimated by
maximizing the evidence function of training data which is marginalized with respect to
the weight parameters. In Zhang et al. (2009), the optimal hyperparameters were derived
for the mixture model of exponential family distributions and then realized to build the
robust HMMs for noisy speech recognition. Practically, these hyperparameters  are
interpreted as the regularization parameter λ, which plays a crucial role in the regular-
ized regression models. The regularized models are developed to prevent the over-ﬁtting
problem in conventional models based on ML estimation or least-squares regression.
In what follows, the evidence framework is illustrated to be closely connected to the
regularization theory, which has been developed to regularize model structure and deal
with the over-ﬁtting problem when building generative models for speech recognition
and other information systems.
5.1.3
Regularization in regression model
Model regularization is a fundamental issue in pattern recognition. It aims to estimate
the smoothed parameters or construct a generalized model which has good prediction
capability for future unseen data. The gap or mismatch between training data and test
data can be compensated by tackling this issue. The over-ﬁtting problem or the ill-
posed problem is also resolved by following the regularization theory. In the regularized
least-squares (RLS) model f(·), the over-ﬁtting problem is avoided by incorporating a
regularization term Ew(w) into the training objective, and this penalizes too complex a
model. Here, model parameters  are rewritten by an N-dimensional weight vector w.
The regularization term is determined by the weight parameters w and the corresponding

5.1 Evidence framework
189
model structure M. The simplest form of this regularizer is given by a sum-of-squares
of the parameter elements:
Ew(w) = 1
2w⊺w = 1
2∥w∥2 = 1
2
N−1

j=0
w2
j .
(5.11)
The sum-of-squares error function is calculated by introducing training samples O =
{ot|t = 1, · · · , T} given by model parameters w, i.e.
Eo(w) = 1
2
T

t=1
(f(ot, w) −yt)2,
(5.12)
where yt is the target value of the observation ot at time t. Accordingly, the training
objective for RLS parameters wRLS is yielded as a regularized least-squares function
which is formed by
wRLS = arg min
w
{Eo(w) + λEw(w)}.
(5.13)
Notably, a regularization parameter λ is introduced in Eq. (5.13) to balance the trade-
off between the sum-of-squares error function Eo(w) and the model complexity penalty
function Ew(w). Minimization of the training objective in Eq. (5.13) eventually obtains a
set of parameters w which works simultaneously towards the goals of ﬁtting the data and
reducing the norm of the solution. Regularization theory is beneﬁcial for model selec-
tion. Regularization parameter λ is generally selected by applying the cross validation
method using a small set of validation data which is outside the training data O.
Nevertheless, it is more attractive to pursue Bayesian interpretation of model reg-
ularization. Considering the same regression problem, but now under a probabilistic
framework, we assume that the modeling error f(ot, w) −yt has a Gaussian distribution:
p(yt|ot, w, β) = N(f(ot, w) −yt)|0, β−1),
(5.14)
and the parameter vector w comes from a Gaussian distribution
p(w|α) = N(w|0, α−1I),
(5.15)
where I is the N × N identity matrix and β and α are the precision parameters for mod-
eling error and parameter vector, respectively. Here, the hyperparameters  = {α, β}
consist of α and β. The MAP estimate of model parameters wMAP is obtained by
maximizing the posterior distribution
p(w|ot, yt, α, β) ∝p(yt|ot, w, β)p(w|α),
(5.16)
or equivalently minimizing the negative log posterior distribution,
β
2
T

t=1
{f(ot, w) −yt}2 + α
2 w⊺w,
(5.17)
by using training samples O = {ot|t = 1, · · · , T}. It is interesting to see that maximiz-
ing the posterior distribution in Eq. (5.17) is equivalent to minimizing the regularized
least-squares error function in Eq. (5.13) with a regularization parameter λ = α/β.

190
Evidence approximation
Readers may refer to MacKay (1992c) and Bishop (2006) for detailed solution to opti-
mal hyperparameters ML2 = {αML2, βML2} of a linear regression model with regression
function
f(ot, w) = w0 +
N−1

j=1
wjφj(ot) = w⊺φ(ot).
(5.18)
A linear combination of ﬁxed non-linear functions of the input variable ot is considered.
Here, φ = [φ0, · · · , φN−1]⊺denotes the basis functions and φ0(ot) = 1 is assigned. For
the case of a neural network regression model, ML2 estimation of hyperparameters is
addressed in Section 6.4.
The comparison among RLS estimation, MAP estimation, and ML2 estimation is fur-
ther investigated below. According to RLS estimation, level-1 inference is performed to
ﬁnd model parameters wRLS by using training data O while the hyperparameter λ is esti-
mated in level-2 inference via a cross validation scheme by using additional validation
data. However, Bayesian inference is implemented to calculate model parameters wMAP
in level-1 inference based on MAP estimation. In level-2 inference, the hyperparameters
ML2 = {αML2, βML2} are inferred using ML2 estimation. By applying MAP and ML2
methods, the same training data O are used to estimate parameters wMAP and hyperpa-
rameters ML2, respectively. It is desirable that no validation data are required by using
the Bayesian approach. Besides, RLS and MAP methods fulﬁl the point estimation and
assume that the estimates ˆw = wRLS and ˆw = wMAP are true values for prediction of
new data O in a test phase according to likelihood function p(O| ˆw). Instead of relying
on single parameter values ˆw in the RLS or MAP method, the ML2 method implements
the distribution estimation and directly infers the optimal hyperparameters ˆ = ML2
by maximizing the predictive distribution or the marginal likelihood of training data
p(O|) =

p(O|w, )p(w|)dw, where all possible values of parameters w are con-
sidered. In a test phase, the same marginal distribution p(O| ˆ), given the estimated
hyperparameters ˆ, is applied for prediction of new data O.
5.1.4
Evidence framework for HMM and SVM
Although the evidence framework is only addressed for a linear regression model f(·),
extensions to the classiﬁcation models including HMM and SVM have been proposed
in Zhang et al. (2009) and Kwok (2000), respectively. When ML2 estimation is applied
for the HMM framework, we estimate the hyperparameters of continuous-density HMM
parameters including initial state probabilities {πj}, state transition probabilities {aij},
mixture weights {ωjk}, mean vectors {μjk}, and covariance matrices {jk}. For the prob-
ability parameters {πj}, {aij} and {ωjk} in multinomial distributions, ML2 estimation
is performed to ﬁnd the corresponding hyperparameters  which are the parameters of
Dirichlet priors for multinomial or discrete variables of states j, state pairs (i, j), and mix-
ture components k, respectively. For the remaining Gaussian parameters {μjk} and {jk}
of continuous feature vectors {ot}, ML2 estimation aims to calculate the corresponding
hyperparameters  which are the parameters of Gaussian–Wishart priors for Gaussian
mean vectors {μjk} and precision (or inverse covariance) matrices {−1
jk }. In general,

5.2 Bayesian sensing HMMs
191
Dirichlet distribution is known as the conjugate prior for multinomial variables while
Gaussian–Wishart distribution is seen as the conjugate prior for Gaussian variables. By
following this guideline, the closed-form solution to the integral in marginal likelihood
does exist, so that the optimization of marginal distribution with respect to individ-
ual hyperparameters has an analytical solution. These hyperparameters characterize the
uncertainties of HMM parameters which could be applied for robust speech recogni-
tion according to the Bayesian predictive classiﬁcation as addressed in Eq. (3.16) and
Section 6.3. This approach is different from conventional BPC based on the hyperpa-
rameters which are heuristically determined or calculated in an ensemble way (Huo &
Lee 2000, Chien & Liao 2001).
The support vector machine (SVM) approach is based on the idea of structural risk
minimization, which shows that the generalization error is bounded by the sum of the
training set error and the Vapnik-Chervonenkis (VC) dimension of the learning machine
(Abu-Mostafa 1989, Vapnik 1995). By minimizing this upper bound, generalization to
future data is improved. Generalization error is related not to the number of inputs, but
to the margin with which it separates the data. SVM has been successfully applied in
many classiﬁcation problems including speech recognition (Ganapathiraju, Hamaker &
Picone 2004). Although SVM is a nonparametric method, the probabilistic framework
and Bayesian perspective have been introduced to deal with the selection of two tuning
parameters or hyperparameters, including:
• a regularization parameter λ, which determines the trade-off between minimizing the
training errors and minimizing the model complexity;
• a kernel parameter, which implicitly deﬁnes the high dimensional feature space to be
used.
Conventionally, these hyperparameters are empirically selected by hand or via cross
validation. The evidence framework has been applied to ﬁnd the optimal regulariza-
tion parameter for SVM (Kwok 2000). Next, we address the detailed estimation of
hyperparameters for two practical solutions to speech recognition. One is developed for
sparse Bayesian acoustic modeling while the other is proposed for hierarchical Dirichlet
language modeling.
5.2
Bayesian sensing HMMs
Speech recognition systems are usually constructed by collecting large amounts of train-
ing data and estimating a large number of model parameters to achieve the desired
recognition accuracy on test data. A large set of context-dependent Gaussian compo-
nents (several hundred thousand components is usually the norm) is trained to build
context-dependent phone models. GMMs with Gaussian mean vectors and diagonal
covariance matrices may not be an accurate representation of high dimensional acous-
tic features. The Gaussian components may be overdetermined. The mismatch between
training data and test conditions may not be carefully compensated. The uncertainty

192
Evidence approximation
of estimated HMM parameters may not be properly characterized. A Bayesian learn-
ing approach is introduced to tackle these issues based on the basis representation.
ML2 estimation is conducted to estimate the automatic relevance determination (ARD)
parameter (MacKay 1995, Tipping 2001) which is the state-dependent hyperparameter
of weight parameter in basis representation. Sparse Bayesian learning is performed by
using the ARD parameter.
5.2.1
Basis representation
An acoustic feature vector o can be viewed as lying in a vector space spanned by a set
of basis vectors. Such a basis representation has been popular for regression problems
in machine learning and for signal recovery in the signal processing literature. This
approach is now increasingly important for acoustic feature representation. Compressive
sensing and sparse representation are popular topics in the signal processing community.
The basic idea of compressive sensing is to encode a feature vector o ∈RD based on a
set of over-determined dictionary or basis vectors  = [φ1, · · · , φN] via
o = w1φ1 + · · · + wNφN = w,
(5.19)
where the sensing weights w = [w1, · · · , wN]⊺are sparse and the basis vectors  are
formed by training samples. A relatively small set of relevant basis vectors is used for
sparse representation based on this exemplar-based method. The sparse solution to w can
be derived by optimizing the ℓ1-regularized objective function (Sainath, Ramabhadran,
Picheny et al. 2011). However, the exemplar-based method is a memory-based method,
which is time-consuming with high memory cost. It is also important to integrate HMMs
into sparse representation of continuous speech frames O = {ot|t = 1, · · · , T}.
5.2.2
Model construction
Bayesian sensing HMMs (BS-HMMs) (Saon & Chien 2012a) are developed by incor-
porating Markov chains into the basis representation of continuous speech. A Bayesian
sensing framework is presented for speech recognition. The underlying aspect of BS-
HMMs is to measure an observed feature vector ot of a speech sentence O based on
a compact set of state-dependent dictionary j =

φj1, · · · , φjN

at state j. For each
frame, the reconstruction error between measurement ot and its representation jwt,
where wt = [wt1, · · · , wtN]⊺, is assumed to be Gaussian distributed with zero mean and
a state-dependent covariance matrix or inverse precision matrix j = R−1
j
. The state
likelihood function with time-dependent sensing weights wt is deﬁned by
p(ot|j) ≜N(ot|jwt, R−1
j
).
(5.20)
The Bayesian perspective in BS-HMMs has its origin from the relevance vector machine
(RVM) (Tipping 2001). Figure 5.3 illustrates the graphical model based on BS-HMMs.
The RVM is known as a sparse Bayesian learning approach for regression and classi-
ﬁcation problems. We would like to apply RVM to conduct sparse basis representation
and combine it with HMMs to characterize the dynamics in the time domain. Therefore,

5.2 Bayesian sensing HMMs
193
Figure 5.3
Graphical model of BS-HMM.
BS-HMM parameters are obtained by  = {πj, aij, j, Rj}. Obviously, similarly to con-
ventional GMM-based HMMs, we can extend BS-HMM to deal with a mixture model
for basis representation where the mixture weight ωjk, the basis vectors jk, and the pre-
cision matrix Rjk of individual mixture component k are incorporated. In what follows,
we neglect the extension to a mixture model and exclude the time-dependent weight
parameters wt from the parameter set .
5.2.3
Automatic relevance determination
However, the sensing weights wt play a crucial role in basis representation, and so we
introduce Bayesian compressive sensing (Ji, Xue & Carin 2008) for acoustic modeling.
The idea of Bayesian learning in BS-HMMs is to yield the distribution estimates of the
speech feature vectors ot due to the variations of sensing weights wt in basis represen-
tation. A Gaussian prior with zero mean and state-dependent diagonal precision matrix
Aj = diag{αjn} is introduced to characterize the weight vector, i.e.
p

wt|Aj

= N(wt|0, diag{α−1
jn })
=
N

n=1
N(wtn|0, α−1
jn ).
(5.21)
Considering a hierarchical prior model where precision parameter αjn is represented by
a gamma prior with parameters a and b in Appendix C.11:
p(αjn) = Gam(αjn|a, b) =
1
(a)baαa−1
jn
exp(−bαjn).
(5.22)
The marginal prior distribution is derived as a Student’s t-distribution as deﬁned in
Appendix C.16:
p(wtn|a, b) =
 ∞
0
N(wtn|0, α−1
jn )Gam(αjn|a, b)dαjn
=
ba
(a)
 1
2π
1/2 
b + w2
tn
2
−a−1/2
(a + 1/2)
= St

wtn
----0, b
a, 2a

,
(5.23)

194
Evidence approximation
Figure 5.4
An example of two-dimensional Gaussian distribution.
Figure 5.5
An example of two-dimensional Student’s t-distribution.
which is known as a sparse prior distribution, since this distribution has a heavy tail and
steep peak. Student’s t-distribution, illustrated in Figure 5.5, is a heavy tailed distribution
and is more robust to outliers than a Gaussian distribution (Figure 5.4) (Bishop 2006).
Correspondingly, if the precision parameter αjn in N(wtn|0, α−1
jn ) is large, the weight
parameter wtn is likely to be zero, wtn →0, which implies that the associated basis
vector φjn is irrelevant to Bayesian basis representation of a target observation ot. The
physical meaning of automatic relevance determination (ARD) is then reﬂected by the
precision parameter αjn in state-dependent hyperparameters,  = {Aj = diag{αjn}}.
We simply call αjn the ARD parameter. According to the ARD scheme, only relevant
basis vectors are selected to represent sequence data O. Sparse Bayesian learning (SBL)
can be realized by using an ARD scheme (Tipping 2001). In the implementation, the
values of state-dependent ARD parameters {αj1, · · · , αjN} can be used to rank or select
salient basis vectors {φj1, · · · , φjN} which are relevant to a target HMM state j. The
larger the estimated value αjn, the more likely it is that the basis vector φjn should be
pruned from the parameter set. The compressed model can be achieved by applying this
property (Saon & Chien 2011). The ARD parameter serves as a compression factor for

5.2 Bayesian sensing HMMs
195
model complexity control. One can initially train a large model and then prune it to a
smaller size by removing basis elements which correspond to the larger ARD values.
Although we appear to be utilizing a non-sparse Gaussian prior over the weights, in
truth the hierarchical formulation implies that the real weight prior is clearly recognized
as encouraging sparsity. Considering this sparse Bayesian basis representation, BS-
HMM parameters and hyperparameters are formed by {, } = {πj, aij, j, Rj, Aj|j =
1, · · · , J} consisting of initial state probability πj, state transition probability aij, basis
vectors j, and precision matrices of reconstruction errors Rj and sensing weights Aj.
Level-1 inference and level-2 inference are done simultaneously in BS-HMMs.
5.2.4
Model inference
We estimate BS-HMM parameters and hyperparameters from the observed speech data
O according to the type-2 ML (ML2) estimation:
{πML2
j
, aML2
ij
, ML2
j
, RML2
j
, AML2
j
} = arg
max
{πj,aij,j,Rj,Aj} p(O|{πj, aij, j, Rj, Aj}).
(5.24)
Without loss of generality, we view Eq. (5.24) as the ML2 estimation because the
marginal likelihood with respect to sensing weights wt is calculated at each frame t in
likelihood function p(O|{πj, aij, j, Rj, Aj}). However, the marginalization over πj, aij,
j, and Rj is not considered. Since the optimization procedure is affected by an incom-
plete data problem, the EM algorithm (Dempster et al. 1976) is applied to ﬁnd the
optimal solution to {πj, aij, j, Rj, Aj}. In E-step, an auxiliary function is calculated
by averaging the log likelihood function of the new estimates {′, ′}, given the old
estimates {, } over all latent variables {S, V}:
Q(′, ′|, ) = E(S,V)[log p(O, S, V|′, ′)|O, , ]
=

S

V
p(S, V|O, , ) log p(O, S, V|′, ′).
(5.25)
In the M-step, we maximize the auxiliary function with respect to new parameters and
hyperparameters {′, ′},
{′, ′} = arg max
{′,′} Q(′, ′|, ),
(5.26)
to ﬁnd optimal parameters and hyperparameters. The auxiliary function is expanded by

S

V
p(S, V|O, , )
& T

t=1
(log a′
st−1st + log p(ot|′
st, ′
st))
'
=

j
T

t=1
γt(j)

log a′
st−1j + log

p(ot|wt, ′
j, R′
j)p(wt|A′
j)dwt

,
(5.27)
where γt(j) = p(st = j|O, , ) is the posterior probability of being in state j at time t
given the current parameters and hyperparameters {, } generating measurements O.

196
Evidence approximation
We tacitly use the convention as0s1 = πs1. Since the estimation of initial state probabil-
ity πj and state transition probability aij is the same as that in HMMs, we neglect the
estimation of {πj, aij} hereafter.
5.2.5
Evidence function or marginal likelihood
The key issue in the E-step is to calculate the frame-based evidence function or marginal
likelihood p(ot|′
st, ′
st) = p(ot|′
j, R′
j, A′
j), which is marginalized over sensing weights
wt at state st = j and is proportional to

|R′
j|1/2 exp

−1
2(ot −′
jwt)⊺R′
j(ot −′
jwt)

× |A′
j|1/2 exp

−1
2w⊺
t A′
jwt

dwt
= |R′
j|1/2|A′
j|1/2

exp

−1
2

o⊺
t R′
jot
−2o⊺
t R′
j′
jwt + w⊺
t
(
(′
j)⊺R′
j′
j + A′
j
)
wt

dwt
= |R′
j|1/2|A′
j|1/2 exp

−1
2(o⊺
t R′
jot)

×

exp
⎡
⎢⎣−1
2
⎛
⎜⎝
w⊺
t (′
j)−1wt −2(o⊺
t R′
j′
j′
j)(′
j)−1wt
+(o⊺
t R′
j′
j′
j)(′
j)−1(′
j(′
j)⊺R′
jot)
−(o⊺
t R′
j′
j′
j)(′
j)−1(′
j(′
j)⊺R′
jot)
⎞
⎟⎠
⎤
⎥⎦dwt
∝|R′
j|1/2|A′
j|1/2|′
j|1/2 exp

−1
2o⊺
t (R′
j −R′
j′
j′
j(′
j)⊺R′
j)ot

= |R′
j|1/2|A′
j|1/2|′
j|1/2 exp

−1
2(o⊺
t R′
jot −(m′
tj)⊺(′
j)−1m′
tj)

.
(5.28)
In Eq. (5.28), the notations
(′
j)−1 ≜(′
j)⊺R′
j′
j + A′
j,
(5.29)
m′
tj ≜′
j(′
j)⊺R′
jot,
(5.30)
are introduced, and the integral of a Gaussian distribution
N(wt|′
j(′
j)⊺R′
jot, ′
j)
(5.31)
is manipulated. By applying the Woodbury matrix inversion (Eq. (B.20))
(A + UCV)−1 = A−1 −A−1U(C−1 + VA−1U)−1VA−1,
(5.32)
given the dimensionally compatible matrices A, U, C, and V, the marginal likelihood
p(ot|′
j, R′
j, A′
j) is derived as
N(ot|0, (R′
j −R′
j′
j
(
(′
j)⊺R′
j′
j + A′
j
)−1
(′
j)⊺R′
j)−1)
= N(ot|0, (R′
j)−1 + ′
j(A′
j)−1(′
i)⊺),
(5.33)

5.2 Bayesian sensing HMMs
197
which is a Gaussian likelihood function with zero mean. The equality of the determinant
|R′
j −R′
j′
j′
j(′
j)⊺R′
j| = |R′
j∥A′
j∥′
j|
(5.34)
is held. This implies that frame discrimination among different states is done solely
on the basis of the covariance matrix. Apparently, the covariance matrix (R′
j)−1 +
′
j(A′
j)−1(′
i)⊺is positive deﬁnite, so that p(ot|′
j, R′
j, A′
j) is a valid probability density
function. For diagonal R′
j, the marginal likelihood is seen as a new Gaussian distribution
with a factor analyzed covariance matrix (R′
j)−1+′
j(A′
j)−1(′
j)⊺, where the factor load-
ing matrix ′
j(A′
j)−1/2 is seen as a rank-N correction to (R′
j)−1 (Saon & Chien 2011).
5.2.6
Maximum a-posteriori sensing weights
In BS-HMMs, we can determine the maximum a-posteriori (MAP) estimate of Bayesian
sensing weights wMAP
t
for each observation ot from
wMAP
t
= arg max
wt
p(wt|ot, ′, ′)
= arg max
wt
p(ot|wt, ′
j, R′
j)p(wt|A′
j)
= ′
j(′
j)⊺R′
jot ≜mtj,
(5.35)
which is seen as a weighted product in vector space of observation ot and transposed
basis vectors (′
j)⊺. The notations m′
tj (or equivalently wMAP
t
) and ′
j are the mean vector
and the covariance matrix of the posterior distribution p(wt|ot, ′, ′), respectively. The
precision matrix for wt is modiﬁed from A′
j of a-priori density p(wt) to (′
j)⊺R′
j′
j + A′
j
of the a-posteriori distribution p(wt|ot). The difference term (′
j)⊺R′
j′
j comes from
the likelihood function p(ot|wt), and is caused by the measurement ′
jwt for obser-
vation ot at frame t represented by new basis vectors ′
j of state j. This is meaningful
because Bayesian learning performs subjective inference, naturally increasing the model
precision.
5.2.7
Optimal parameters and hyperparameters
By substituting Eq. (5.28) into Eq. (5.27), the optimal BS-HMM parameters and hyper-
parameters are estimated by maximizing the expanded auxiliary function with respect
to individual parameters and hyperparameters {′
j, R′
j, A′
j}. The auxiliary function in a
BS-HMM state is simpliﬁed to
Q(′
j, R′
j, A′
j|j, Rj, Aj)
=
T

t=1
γt(j) log

p(ot|wt, ′
j, R′
j)p(wt|A′
j)dwt
∝
⊺

t=1
γt(j)

log |R′
j| + log |A′
j|
+ log |j| −o⊺
t R′
jot + (m′
tj)⊺(′
j)−1m′
tj

.
(5.36)

198
Evidence approximation
Let us ﬁrst consider the maximization of Eq. (5.28) with respect to N × N hyperparam-
eter matrix A′
j. We take the gradient of Eq. (5.28) with respect to A′
j and set it to zero to
obtain
T

t=1
γt(j)
⎡
⎢⎢⎣(A′
j)−1 −′
j −′
j(′
j)⊺R′
jot



m′
tj
· o⊺
t R′
j′
j′
j



(m′
tj)⊺
⎤
⎥⎥⎦= 0.
(5.37)
The value of A′
j that maximizes the auxiliary function satisﬁes
(AML2
j
)−1 = ′
j +
T
t=1 γt(j)m′
tj(m′
tj)⊺
T
t=1 γt(j)
≜FML2(A′
j).
(5.38)
Notably, Eq. (5.38) is an implicit solution to A′
j because Fa is a function of A′
j. The
hyperparameter (AML2
j
)−1 of sensing weights is obtained by adding the covariance
matrix ′
j of posterior p(wt|ot, ′, ′) and the weighted autocorrelation of MAP sensing
weights {m′
tj ≜wMAP
t
}.
To ﬁnd an ML2 estimate of basis vectors ′
j, we maximize Eq. (5.36) by taking the
gradient of the terms related to ′
j and setting it to zero, which leads to
∂
∂′
j

 T

t=1
γt(j)

−log |(′
j)⊺R′
j′
j + A′
j|
+ o⊺
t R′
j′
j
(
(′
j)⊺R′
j′
j + A′
j
)−1
(′
j)⊺R′
jot

= 0,
(5.39)
where the gradients of the two terms are derived as the D × N matrices given by
∂
∂′
j
log|(′
j)⊺R′
j′
j + A′
j|
= 2R′
j′
j
(
(′
j)⊺R′
j′
j + A′
j
)−1
= 2R′
j′
j′
j,
(5.40)
∂
∂′
j

o⊺
t R′
j′
j
(
(′
j)⊺R′
j′
j + A′
j
)−1
(′
j)⊺R′
jot

=
∂
∂′
j
tr{′
j(′
j)⊺R′
joto⊺
t R′
j′
j}
= −2R′
j′
j′
j(′
j)⊺R′
joto⊺
t R′
j′
j′
j + 2R′
joto⊺
t R′
j′
j′
j.
(5.41)
The optimal solution ML2
j
, which is a D × N matrix, satisﬁes
T

t=1
γt(j)

−R′
j′
j′
j −R′
j′
j′
j(′
j)⊺R′
joto⊺
t R′
j′
j′
j + R′
joto⊺
t R′
j′
j′
j

=
T

t=1
γt(j)R′
j
+
−′
j′
j −′
jm′
tj(m′
tj)⊺+ ot(m′
tj)⊺,
= 0.
(5.42)

5.2 Bayesian sensing HMMs
199
Similarly to the solution to A′
j, the ML2 estimate can be expressed in an implicit form
written by a function Fφ(′
j), namely
ML2
j
=
& T

t=1
γt(j)ot(m′
tj)⊺
' & T

t=1
γt(j)(′
j + m′
tj(m′
tj)⊺)
'−1
=
T
t=1 γt(j)ot(m′
tj)⊺
T
t=1 γt(j)
· AML2
j
≜FML2(′
j).
(5.43)
This solution is viewed as a weighted operation in the outer product space of observa-
tions {ot} and MAP sensing weights {m′
tj ≜wMAP
t
}. The posterior probabilities {γt(j)}
and the ML2 hyperparameters AML2
j
serve as the weights and the rotation operator of the
weighted average, respectively.
To ﬁnd the ML2 estimate of precision matrix R′
j, we maximize Eq. (5.36) with respect
to R′
j and obtain
T

t=1
γt(j)

(R′
j)−1 −′
j′
j(′
j)⊺−oto⊺
t
+
∂
∂R′
j
(
o⊺
t R′
j′
j′
j(′
j)⊺R′
jot
)
= 0,
(5.44)
where
∂
∂R′
j
(
o⊺
t R′
j′
j′
j(′
j)⊺R′
jot
)
=
∂
∂R′
j
tr{′
j(′
j)⊺R′
joto⊺
t R′
j′
j}
= −′
j′
j(′
j)⊺R′
joto⊺
t R′
j′
j′
j(′
j)⊺
+ ′
j′
j(′
j)⊺R′
joto⊺
t + oto⊺
t R′
j′
j′
j(′
j)⊺.
(5.45)
We derive the ML2 solution as
(RML2
j
)−1 =
T
t=1 γt(j)
⎡
⎢⎣
′
j′
j(′
j)⊺+ oto⊺
t
+′
j′
j(′
j)⊺R′
joto⊺
t R′
j′
j′
j(′
j)⊺
−′
j′
j(′
j)⊺R′
joto⊺
t −oto⊺
t R′
j′
j′
j(′
j)⊺
⎤
⎥⎦
T
t=1 γt(j)
= ′
j′
j(′
j)⊺+
T
t=1 γt(j)(ot −′
jm′
tj)(ot −′
jm′
tj)⊺
T
t=1 γt(j)
≜FML2(R′
j),
(5.46)
which is also an implicit solution to R′
j since the right-hand-side (RHS) of Eq. (5.46)
depends on R′
j. Note that the RHS of Eq. (5.46) is symmetric positive deﬁnite. The ﬁrst
term is a scaled covariance matrix ′
j of the posterior distribution p(wt|ot, ′, ′), which
is doubly transformed by ′
j. The second term is interpreted as a covariance matrix
weighted by posterior probabilities γt(j), and is calculated using observations {ot} and

200
Evidence approximation
“mean” vectors ′
jm′
tj. This corresponds to performing Bayesian sensing, again using
the MAP estimates {m′
tj ≜wMAP
t
}.
We can see that type-2 ML (ML2) estimates of BS-HMM parameters and hyper-
parameters are consistently formulated as the implicit solutions, which are beneﬁcial
for efﬁcient implementation and good convergence in parameter estimation. Differ-
ently from conventional basis representation, where basis vectors and sensing weights
are found separately, BS-HMMs provide a multivariate Bayesian approach to hybrid
estimation of the compact basis vectors and the precision matrices of sensing weights
under a consistent objective function. No training examples are stored for memory-based
implementation.
To improve LVCSR performance based on BS-HMMs, there have been several exten-
sions developed for acoustic modeling. As mentioned in Section 5.2.2, the mixture
model of BS-HMMs can be extended by considering multiple sets of basis vectors per
state. A mixture model of basis vectors is included for acoustic modeling. Using this
mixture model, each observation ot at state st = j is expressed by
p(ot|j) ≜
K

k=1
ωjkN(ot|jkwt, R−1
jk ),
(5.47)
where ωjk is the mixture weight of jth component with the constraint
K

k=1
ωjk = 1.
(5.48)
Here, the reconstruction error of an observation vector ot due to the jth component
with basis vectors jk = [φjk1, · · · , φjkN] is assumed to be Gaussian distributed with
zero mean and precision matrix Rjk. In addition, BS-HMMs can be constructed by
incorporating a non-zero mean vector μw
j in the prior density of sensing weights, i.e.,
p(wt|0, Aj) →p(wt|μw
j , Aj).
(5.49)
Similarly to the Maximum Likelihood Linear Regression (MLLR) adaptation
for
HMMs, BS-HMMs are developed for speaker adaptation where the nth BS-HMM basis
vector is transformed by
ˆφjn = M/φjn,
(5.50)
where M is a D × (D + 1) regression matrix and /φjn = [φ⊺
jn 1]⊺is the extended basis
vector. The type-2 ML estimation can be applied to calculate the optimal solutions to
non-zero mean vector μw
j and regression matrix M.
5.2.8
Discriminative training
Finally, BS-HMMs are sophisticated, incorporating both model-space and feature-space
discriminative training, which is crucial to improve classiﬁcation of confusing patterns
in pattern recognition systems. Developing discriminative training for BS-HMMs is
important for LVCSR. Instead of the goodness-of-ﬁt criterion using marginal likelihood

5.2 Bayesian sensing HMMs
201
function, the objective function for discriminative training is established according to the
mutual information between observation data O and the sequence of reference words Wr
(Bahl et al. 1986, Povey & Woodland 2002, Povey, Kanevsky, Kingsbury et al. 2008):
F() ≜I(O, Wr) = log p(O, Wr)
p(O)p(Wr)
= log p(O|Wr) −log

W
p(O|W)p(W)
≜Fnum() −Fden(),
(5.51)
which consists of a numerator term Fnum() and a denominator term Fden(). The
Maximum Mutual Information (MMI) estimation of BS-HMMs MMI is performed
for discriminative training. To solve the optimization problem, we calculate the weak-
sense auxiliary function (Povey & Woodland 2002, Povey 2003), where the HMM state
sequence S is incorporated as follows:
Q(′|) = Qnum(′|) −Qden(′|) + Qsm(′|)
=

S
p(S|O, Wr, ) log p(S, O|′)
−

S

W
p(S, W|O, ) log p(S, O|′)
+ Qsm(′|).
(5.52)
The property of weak-sense auxiliary function turns out to meet the condition
∂Q(′|)
∂′
----
′=
= ∂F(′)
∂′
----
′=
,
(5.53)
where the mode of MMI auxiliary function and its weak-sense auxiliary function have
the same value. The smoothing function Qsm(′|) in Eq. (5.52) is added to ensure that
the objective function Q(′|) is improved by this extended EM algorithm. For this,
the smoothing function should satisfy
∂Qsm(′|)
∂′
----
′=
= 0.
(5.54)
In what follows, we address the discriminative training of basis vectors j of state j.
The same procedure can be applied to estimate the discriminative precision matrix of
reconstruction errors Rj.
One possible choice of smoothing function meeting Eq. (5.54) is formed by the
Kullback–Leibler divergence KL(·∥·) between marginal likelihoods of the current
estimate p(ot|) and the new estimate p(ot|′) given by
Qsm({′
j}|{j}) ≜−
J

j=1
DjKL(p(o|j)∥p(o|′
j))
∝
J

j=1
Dj

p(o|j) log p(o|′
j)do

202
Evidence approximation
∝
J

j=1
Dj

log |R′
j| −

p(o|j)
× (o −jw + jw −′
jw)⊺
× R′
j(o −jw + jw −′
jw)do

=
J

j=1
Dj

log |R′
j| −

p(o|j)
× (o −jw)⊺R′
j(o −jw)do
−(jw −′
jw)⊺R′
j(jw −′
jw)

.
(5.55)
Here, Dj is a state-dependent smoothing constant. Typically, the smoothing function
is mathematically intractable when applying marginal likelihoods. Noting this, we can
approximate the marginal likelihood by using an average plug-in MAP estimate:
w ≈1
T
T

t=1
wMAP
t
,
(5.56)
obtained by taking an ensemble average of the MAP estimates in Eq. (5.35) using all
observation frames {o1, · · · , oT}. Ignoring the terms independent of j, the smoothing
function is obtained by substituting the approximate MAP estimate of Eq. (5.56) into
Qsm({′
j}|{j}) = −
J

j=1
Djw⊺(j −′
j)⊺R′
j(j −′
j)w.
(5.57)
As a result, the weak-sense auxiliary function is expressed in terms of state occupation
posteriors as follows:
Q({′
j}|{j}) =
T

t=1
J

j=1
(γ num
t
(j) −γ den
t
(j))
×

log |A′
j| + log |R′
j| + log |′
j|
−o⊺
t R′
jot + (m′
tj)⊺(′
j)−1m′
tj

−
J

j=1
Djw⊺(j −′
j)⊺R′
j(j −′
j)w,
(5.58)
where the state occupation posteriors of staying in state st = j at time t in the numerator
and denominator terms are calculated by
γ num
t
(j) ≜p(st = j|O, Wr, ),
(5.59)
γ den
t
(j) ≜

W
p(st = j|O, W, ),
(5.60)

5.2 Bayesian sensing HMMs
203
given the reference word sequence Wr and all possible word sequences {W}, respec-
tively. The current estimates {j} are used in this calculation. To ﬁnd an MMI estimate
for basis parameters, we differentiate Eq. (5.58) with respect to ′
j and set it to zero:
∂
∂′
j
Q(′
j|j) ∝R′
j
T

t=1
(γ num
t
(j) −γ den
t
(j))
× (−′
j′
j −′
jm′
tj(m′
tj)⊺+ ot(m′
tj)⊺)
−R′
jDj(′
j −j)ww⊺= 0,
(5.61)
which is derived by considering the deﬁnition of variables (′
j)−1 and m′
j in Eq. (5.29)
and Eq. (5.30), respectively. Again, the implicit solution to an MMI estimate of ′
j
should satisfy
MMI2
j
=
& T

t=1
(γ num
t
(j) −γ den
t
(j))ot(m′
tj)⊺+ Djjww⊺
'
×

 T

t=1
(γ num
t
(j) −γ den
t
(j))
× (′
j + m′
tj(m′
tj)⊺) + Djww⊺
−1
≜FMMI2(′
j).
(5.62)
This solution is expressed as a recursive function FMMI2 of new basis parameter ′
j.
Strictly speaking, an MMI estimation based on marginal likelihood is seen as a type
2 MMI (MMI2) estimation, which is different from conventional MMI training (Bahl
et al. 1986, Povey & Woodland 2002, Povey et al. 2008) based on likelihood func-
tion without marginalization. In Eq. (5.62), the second terms in the numerator and the
denominator come from smoothing function Qsm(′
j|j) and serve to prevent instability
in the MMI2 optimization procedure. The solution is highly affected by the differ-
ence between statistics for the reference hypothesis and statistics for the competing
hypotheses γ num
t
(j) −γ den
t
(j).
It is clear that discriminative training and Bayesian learning are simultaneously per-
formed in a type 2 MMI estimation. By doing this, the performance of LVCSR can be
signiﬁcantly improved (Saon & Chien 2012b). The robustness to uncertainty of sens-
ing weights in a basis representation can be assured. Some experimental results are
described below.
5.2.9
System performance
Evaluation of BS-HMMs was performed by using the LVCSR task in the domain of
Arabic broadcast news transcription which was part of the DARPA GALE program.
In total, 1800 hours of manually transcribed Arabic broadcast news and conversations
were used in this evaluation (Saon & Chien 2011). The results on several test sets were

204
Evidence approximation
Table 5.1 Comparison of the number of free parameters and word error rates for baseline acoustic
models after ML training and BS-HMMs after ML2 training.
System
Nb. parameters
WER
DEV’07
DEV’08
DEV’09
Baseline 800K
64.8M
13.8%
16.4%
19.6%
Baseline 2.8M
226.8M
14.1%
16.2%
19.3%
BS-HMM 417K
148.5M
13.6%
16.0%
18.9%
reported: DEV’07 (2.5 hours), DEV’08 (3 hours) and DEV’09 (3 hours). The front-end
processing was performed as mentioned in Saon & Chien (2012b). The vocal-tract-
length-warped PLP (Hermansky 1990) cepstrum features were extracted with a context
window of nine frames. The features were mean and variance normalized on a per
speaker basis. Linear discriminant analysis was used to reduce the feature dimension
to 40. The maximum likelihood training of the acoustic model was interleaved with
the estimation of a global semi-tied covariance transform (Gales 1998). All models in
this evaluation were estimated based on pentaphones and speaker adaptively trained
with feature-space MLLR (fMLLR). Each pentaphone was modeled by a 3-state left-to-
right HMM without state skipping. At test time, speaker adaptation was performed with
vocal-tract-length normalization (Wegmann, McAllaster, Orloff et al. 1996), fMLLR
and multiple regression MLLR transforms. The vocabulary contained 795K words. The
decoding was done with 4-gram language models which were estimated with modiﬁed
Kneser–Ney smoothing. The acoustic models were discriminatively trained in both fea-
ture space and model space according to the boosted MMI criterion (Povey, Kingsbury,
Mangu et al. 2005). The baseline acoustic models had 5000 context-dependent HMM
states and 800K 40-dimensional diagonal covariance Gaussians.
In the implementation, BS-HMM parameters were initialized by training a large
HMM model with 2.8M diagonal covariance Gaussians by maximum likelihood
method. The means of GMM were clustered and then treated as the initial basis jk
for state j and mixture component k. The resulting number of mixture components in
BS-HMMs after the clustering step was 417K. The precision matrices Rjk and Ajk were
assumed to be diagonal and were initialized to the identity matrix. Table 5.1 compares
the performance of the baseline 800K Gaussians model and the 2.8M Gaussians model
used to train baseline acoustic models after ML training and to seed BS-HMMs after
ML2 training. The number of free parameters is included in this comparison. As we can
see, BS-HMMs outperform both baseline systems in terms of word error rates (%). The
possible reasons are twofold. The ﬁrst one is that the covariance modeling in BS-HMMs
is more accurate than that in HMMs. The second one is due to the Bayesian parameter
updates which provide an effective smoothing.
In contrast, we conducted a model comparison for BS-HMMs according to the esti-
mated hyperparameters of sensing weights. Model compression was performed by
discarding 50% of the basis vectors jkn corresponding to the largest hyperparameters
αjkn. This results in a compressed model with approximately 91M free parameters (about

5.3 Hierarchical Dirichlet language model
205
Table 5.2 Comparison of word error rates for original and compressed BS-HMMs before and after
model-space discriminative training with MMI2 training.
Model
Training
DEV07
DEV08
DEV09
Original
ML2
12.0%
13.9%
17.4%
Compressed
ML2
12.4%
14.2%
17.6%
Original
MMI2
10.7%
11.9%
15.0%
Compressed
MMI2
10.4%
11.7%
14.8%
30% larger than the 800K baseline HMM). For both original and compressed models,
we performed model-space discriminative training using the MMI2 criterion. Table 5.2
reports the recognition performance before and after model-space discriminative train-
ing. We ﬁnd that the compressed models outperform the originals after discriminative
training even though they start from a higher word error rate after ML2 estimation. How-
ever, discriminative training is signiﬁcantly more expensive than ML estimation which
makes it difﬁcult to ﬁnd the optimal model size.
5.3
Hierarchical Dirichlet language model
In what follows, we revisit the interpolation smoothing methods presented in Section
3.6 and used to deal with the small sample size problem in ML estimation of n-gram
parameters ML = {pML(wi|wi−1
i−n+1)}. Different from the heuristic solutions to language
model smoothing in Section 3.6, a full Bayesian language model is proposed to realize
interpolation smoothing for n-grams. The theory of evidence approximation is devel-
oped and applied to construct the hierarchical Dirichlet language model (MacKay &
Peto 1995, Kawabata & Tamoto 1996).
5.3.1
n-gram smoothing revisited
In general, the frequency estimator in a higher-order language model has large variance,
because there are so many possible word combinations in an n-gram event {wi−1
i−n+1, wi}
that only a small fraction of them have been observed in the data. A simple linear inter-
polation scheme for an n-gram language model is performed by interpolating an ML
model of an n-gram pML(wi|wi−1
i−n+1) with that of an (n −1)-gram pML(wi|wi−1
i−n+2) using
ˆp(wi|wi−1
i−n+1) = λpML(wi|wi−1
i−n+1) + (1 −λ)pML(wi|wi−1
i−n+2),
(5.63)
where λ denotes the interpolation weight, which can be determined empirically from
validation data. Basically, it is not possible to make language models without making
a-priori assumptions. The smoothed n-gram ˆp(wi|wi−1
i−n+1) in Eq. (5.63) can be seen as
an integrated n-gram model which is determined from a hierarchical model based on
the smoothing parameters a-posteriori from the data. In what follows, we would like to
reverse-engineer the underlying model, which gives a probabilistic meaning to language

206
Evidence approximation
model smoothing. We will explain the construction of a hierarchical prior model and
illustrate how this model is used to justify the smoothed language model from a Bayesian
perspective. A type-2 ML estimation is conducted to ﬁnd optimal hyperparameters from
training data. No cross validation procedure is required in a Bayesian language model.
5.3.2
Dirichlet prior and posterior
n-gram model parameters  = {p(wi|wi−1
i−n+1)} are known as multinomial parameters
which are used to predict n-gram events of word wi appearing after observing history
words wi−1
i−n+1. The ML estimation of an n-gram θwi|wi−1
i−n+1 = p(wi|wi−1
i−n+1) from a text
corpus D has been shown in Eq. (3.192). Here, we are interested in a Bayesian language
model where the prior density of multinomial parameters is introduced. A parameter
vector consists of N multinomial parameters:
θ = vec() = [θ1, · · · , θN]⊺subject to
(5.64)
0 ≤θi ≤1 and
N

i=1
θi = 1.
The conjugate prior over multinomial parameters θ is speciﬁed by a Dirichlet prior with
hyperparameters α = [α1, · · · , αN]⊺, which is a multivariate distribution in the form of
p(θ|α) = Dir(θ|α) =
1
Z(α)
N

i=1
θαi−1
i
.
(5.65)
We express the normalization constant of the Dirichlet distribution by
Z(α) =
*N
i=1 (αi)
(N
i=1 αi)
.
(5.66)
The mean vector of Dirichlet distribution is given by

θDir(θ|α)dθ =
α
N
i=1 αi
.
(5.67)
When we observe the training samples D, the posterior distribution is derived as another
Dirichlet distribution:
p(θ|D, α) = p(D|θ)p(θ|α)
p(D|α)
=
*N
i=1 θc(θi)
i
*N
i=1 θαi−1
i
p(D|α)Z(α)
=
*N
i=1 θc(θi)+αi−1
i
Z(c + α)
= Dir(θ|c + α),
(5.68)
with the updated hyperparameters c + α. In Eq. (5.68), each entry c(θi) of
c = [c(θ1), · · · , c(θN)]⊺
(5.69)

5.3 Hierarchical Dirichlet language model
207
denotes the number of occurrences of the ith n-gram event in θi in training data D. This
shows the property of a conjugate prior by using the Dirichlet distribution.
5.3.3
Evidence function
To obtain the predictive probability of a word wi given history words wi−1
i−n+1 and train-
ing data D, we apply the sum rule to calculate the evidence function or the marginal
likelihood:
p(wi|wi−1
i−n+1, D, α) =

p(wi|wi−1
i−n+1, D, α)p(θ|D, α)dθ
=

θwi|wi−1
i−n+1Dir(θ|c + α)dθ
=
c(θi|j) + αi
N
k=1[c(θk|j) + αk]
,
(5.70)
which is marginalized over all values of parameter θ. This predictive distribution is
equivalent to calculating the mean of an n-gram parameter:
p(wi|wi−1
i−n+1) ≜θi|j,
(5.71)
based on the posterior distribution p(θ|D, α), which is a Dirichlet distribution with
hyperparameters c + α. Here, the n-gram probability p(wi|wi−1
i−n+1) is simply expressed
by θi|j where j denotes the back-off smoothing information from the lower-order model
p(wi|wi−2
i−n+1), which is addressed in Section 5.3.4.
We may further conduct the next level of inference by inferring the hyperparameters
given the data. The posterior distribution of α is expressed by
p(α|D) = p(D|α)p(α)
p(D)
.
(5.72)
The hierarchical prior/posterior model of parameters θ and hyperparameters α is
constructed accordingly. The marginal likelihood over hyperparameters α is yielded by
p(wi|wi−1
i−n+1, D) =

p(wi|wi−1
i−n+1, D, α)p(α|D)dα.
(5.73)
We may ﬁnd the most probable MAP estimate αMAP by
αMAP = arg max
α
p(α|D).
(5.74)
Then the marginal distribution is approximated as
p(wi|wi−1
i−n+1, D) ≈p(wi|wi−1
i−n+1, D, αMAP).
(5.75)
In addition, we would like to calculate the optimal hyperparameters via ML2 esti-
mation from training data D based on the evidence framework. To do so, we need
to determine the evidence function given hyperparameters p(D|α). By referring to
Eq. (5.68), this function is derived as
p(D|α) = Z(c + α)
Z(α)
=

j
 *N
i=1 (c(θi|j) + αi)
(N
i=1 c(θi|j) + αi)
· (N
i=1 αi)
*N
i=1 (αi)

,
(5.76)

208
Evidence approximation
which is viewed as a ratio of normalization constants of posterior probability p(θ|D, α)
over prior probability p(θ|α).
5.3.4
Bayesian smoothed language model
It is important to illustrate the physical meaning of predictive distribution in Eq. (5.70).
The hyperparameter αi appears as an effective initial count for an n-gram event wi
i−n+1.
This marginal likelihood is integrated by the information sources from prior statistics
α as well as training data D or their counts of occurrences c. On the other hand, the
predictive distribution in Eq. (5.70) can be rewritten as
p(wi|wi−1
i−n+1, D, α) =
c(wi
i−n+1) + αwi|wi−1
i−n+2

wi
+
c(wi
i−n+1) + αwi|wi−1
i−n+2
,
= λwi−1
i−n+1pML(wi|wi−1
i−n+1) + (1 −λwi−1
i−n+1)
αwi|wi−1
i−n+2

wi αwi|wi−1
i−n+2
,
(5.77)
where pML(wi|wi−1
i−n+1) denotes the ML model introduced in Eq. (3.192) and 1 −λwi−1
i−n+1
implies the interpolation weight for prior statistics and is herein obtained as
1 −λwi−1
i−n+1 =

wi αwi|wi−1
i−n+2

wi
+
c(wi
i−n+1) + αwi|wi−1
i−n+2
,.
(5.78)
It is interesting to see that the predictive distribution in Eq. (5.77) is interpreted as the
smoothed n-gram based on the interpolation smoothing. We build the tight connec-
tion between the Bayesian language model and a linearly smoothed language model as
addressed in Section 3.6. The prior statistics or hyperparameters αwi|wi−1
i−n+2 should suf-
ﬁciently reﬂect the backoff information from the low-order model p(wi|wi−1
i−n+2) when
calculating the predictive n-gram probability p(wi|wi−1
i−n+1, D, α). Comparing Eq. (5.78)
and Eq. (3.211), the Bayesian language model is shown to be equivalent to the Witten–
Bell smoothed language model in the case that the hyperparameters αwi|wi−1
i−n+2 are
selected to meet the condition

wi
αwi|wi−1
i−n+2 = N1+(wi−1
i−n+1, •).
(5.79)
(As before, the • represents any possible words at i that are summed over.)
Nevertheless, the advantage of the Bayesian smoothed language model is to automat-
ically determine the optimal hyperparameters αwi|wi−1
i−n+2 from training data D.
5.3.5
Optimal hyperparameters
According to the evidence framework, a type-2 ML estimation is carried out to ﬁnd
optimal hyperparameters α = [α1, · · · , αN]⊺by maximizing the evidence function
αML2 = arg max
α
p(D|α).
(5.80)

5.3 Hierarchical Dirichlet language model
209
More speciﬁcally, we ﬁnd individual parameter αML2
i
through calculating the
differentiation
∂
∂αi
log p(D|α) =

j

(c(θi|j) + αi) −
 N

i=1
c(θi|j) + αi

+ 
 N

i=1
αi

−(αi)

,
(5.81)
where the di-gamma function
(x) ≜∂
∂x log (x)
(5.82)
is incorporated. We may use the conjugate gradient algorithm to ﬁnd αML2
i
or apply some
approximation to derive an explicit optimization algorithm.
In general, it is reasonable that N
i=1 αi > 1 and αi < 1. We can use the recursive
formula of the di-gamma function (MacKay & Peto 1995),
(x + 1) = (x) + 1
x,
(5.83)
to combine the ﬁrst and fourth terms in the brackets of Eq. (5.81) to obtain
(c(θi|j) + αi) −(αi) =
1
c(θi|j) −1 + αi
+
1
c(θi|j) −2 + αi
+ · · · +
1
2 + αi
+
1
1 + αi
+ 1
αi
.
(5.84)
The number of terms in the right-hand-side of Eq. (5.84) is c(θi|j). Assuming αi is smaller
than 1, we can approximate Eq. (5.84), for c(θi|j) ≥1, by
(c(θi|j) + αi) −(αi) = 1
αi
+
c(θi|j)

c=2

1
c −1 + αi

≈1
αi
+
c(θi|j)

c=2

1
c −1 −
αi
(c −1)2 + O(α2
i )

= 1
αi
+
c(θi|j)

c=2
1
c −1 −αi
c(θi|j)

c=2
1
(c −1)2 + O(α2
i ).
(5.85)
Here, the function inside the brackets,
f(αi) =
1
c −1 + αi
,
(5.86)
is approximated by a Taylor series at the point αi
= 0. Further, we apply the
approximation of a di-gamma function,
(x) ≈log(x) −1
2x + O
 1
x2

,
(5.87)

210
Evidence approximation
to approximate the second and third terms in Eq. (5.81) (MacKay & Peto 1995):
K(α) =

j


 N

i=1
αi

−
 N

i=1
c(θi|j) + αi
2
≈

j
log
&N
i=1 c(θi|j) + αi
N
i=1 αi
'
+ 1
2

j
⎡
⎣
N
i=1 c(θi|j)
(N
i=1 αj
) (N
i=1 c(θi|j) + αi
)
⎤
⎦.
(5.88)
For each count c and word i, let Nci be the number of back-off contexts j such that
c(θi|j) ≥c, and let cmax
i
denote the largest c such that Nci > 0. Denote the number of
entries in row i of c(θi|j) that are non-zero N1i by Vi. We compute the quantities:
Gi =
cmax
i
c=2
Nci
c −1,
(5.89)
Hi =
cmax
i
c=2
Nci
(c −1)2 .
(5.90)
Finally, the solution to optimal hyperparameters αML2 is obtained. The hyperparameter
αML2
i
corresponding to each word i should satisfy
αML2
i
=
2Vi
K(α) −Gi +
3
(K(α) −Gi)2 + 4HiVi
≜fi(α).
(5.91)
Again, this solution to ML2 estimation is expressed as an implicit solution because the
right-hand-side of Eq. (5.91) is a function of hyperparameters α. Starting from the initial
hyperparameters α and the resulting function K(α), the individual hyperparameter is
then updated to α(1)
j . The function K(α(1)) is updated as well. The estimation procedure
α →α(1) →· · · converges very rapidly.

6
Asymptotic approximation
Asymptotic approximation is also well known in practical Bayesian approaches
(De Bruijn 1970) for approximately obtaining the posterior distributions. For example,
as we discussed in Chapter 2, the posterior distributions of a model parameter p(|O)
and a model p(M|O) given an observation O = {ot ∈RD|t = 1, · · · , T}) are usu-
ally difﬁcult to solve. The approach assumes that we have enough data (i.e., T is
sufﬁciently large), which also makes Bayesian inference mathematically tractable. As a
particular example of asymptotic approximations, we introduce the Laplace approxima-
tion and Bayesian information criterion, which are widely used for speech and language
processing.
The Laplace approximation is used to approximate a complex distribution as a Gaus-
sian distribution (Kass & Raftery 1995, Bernardo & Smith 2009). It assumes that the
posterior distribution is highly peaked at about its maximum value, which corresponds
to the mode of the posterior distribution. Then the posterior distribution is modeled as a
Gaussian distribution with the mode as a mean parameter. By using the approximation,
we can obtain the posterior distributions analytically to some extent. Section 6.1 ﬁrst
explains the Laplace approximation in general. In Sections 6.3 and 6.4 we also discuss
use of the Laplace approximation for analytically obtaining Bayesian predictive distribu-
tions for acoustic modeling and Bayesian extension of successful neural-network-based
acoustic modeling, respectively.
Another example of this asymptotic approximation is the Bayesian information cri-
terion (or Schwarz criterion (Schwarz 1978)). The Bayesian information criterion also
assumes the large sample case, and approximates the posterior distribution of a model
p(M|O) with a simple equation. Since the Bayesian information criterion assumes the
large sample case, it is also described as an instance of asymptotic approximations.
Section 6.2 explains the Bayesian information criterion in general; it is used for model
selection problems in speech processing. For example, Section 6.5 discusses the opti-
mization of an appropriate model structure of hidden Markov models, and Section 6.6
discusses estimation of the number of speakers and detecting speech segments in
conversations by regarding these approaches as model selection problems.
6.1
Laplace approximation
This section ﬁrst describes the basic theory of the Laplace approximation. We ﬁrst
consider a simple case where a model does not have a latent variable. We focus on

212
Asymptotic approximation
the posterior distributions of model parameters, but this approximation can be applied
to the other continuous probabilistic variables. Let θ ∈RJ be a J dimensional vector
form of continuous model parameters and O = {ot ∈RD|t = 1, · · · , T} be a T-frame
sequence of D dimensional feature vectors. We consider the posterior distribution of θ,
which has the following distribution p(θ|O):
p(θ|O) ≜exp(−f(θ)),
(6.1)
where f(θ) is a continuous function over θ. Note that most parametric distributions can
be represented by this equation. The Laplace approximation approximates p(θ|O) as
a Gaussian distribution with the mode of p(θ|O) as a mean parameter, which can be
obtained numerically or analytically for some speciﬁc distributions. Let θMAP be the
mode (MAP value, as discussed in Chapter 4) of p(θ|O), that is:
θMAP = arg max
θ
p(θ|O)
= arg min
θ
f(θ).
(6.2)
Here we use Eq. (6.1). Since the mode θMAP is the minimum value in f(θ), θMAP also
satisﬁes the following equation:
∇θf(θ)|θ=θMAP = 0,
(6.3)
where ∇θ is the gradient operator with respect to θ. Based on the property, a Taylor
expansion around θMAP approximates f(θ) as follows:
f(θ) ≈f(θMAP) + (θ −θMAP)⊺∇θf(θ)|θ=θMAP + 1
2(θ −θMAP)⊺H(θ −θMAP)
= f(θMAP) + 1
2(θ −θMAP)⊺H(θ −θMAP),
(6.4)
where the second term in the ﬁrst line is canceled by using Eq. (6.3). Equation (6.4) is a
basic equation in this chapter. H is a J × J Hessian matrix deﬁned as
H ≜∇θ∇θf(θ)|θ=θMAP .
(6.5)
By using Eq. (6.1), it can also be represented as
H ≜−∇θ∇θ log p(θ|O)|θ=θMAP .
(6.6)
The Hessian matrix also appeared in Eq. (5.9), as the second derivative of the negative
log posterior. The determinant of this matrix plays an important role in the Occam factor,
which penalizes the model complexity. Note that the Hessian matrix is a symmetric
matrix based on the following property:
[H]ij = ∂
∂θi
∂
∂θj
f(θ)|θ=θMAP
= ∂
∂θj
∂
∂θi
f(θ)|θ=θMAP = [H]ji.
(6.7)

6.1 Laplace approximation
213
Thus, by substituting the Taylor expansion of f(θ) (Eq. (6.4)) into Eq. (6.1), we obtain
the approximated form of p(θ|O), as follows:
p(θ|O) = exp (−f(θ))
≈exp

−f(θMAP) −1
2(θ −θMAP)⊺H(θ −θMAP)

∝exp

−1
2(θ −θMAP)⊺H(θ −θMAP)

.
(6.8)
Here, exp
(
−f(θMAP)
)
does not depend on θ, and we can neglect it. Thus, p(θ|O)
is approximated by the following Gaussian distribution with mean vector θMAP and
covariance matrix H−1:
p(θ|O) ≈N(θ|θMAP, H−1).
(6.9)
Note that since the dimensionality of the Hessian matrix H is the number of param-
eters J, and it is often large in our practical problems, we would have a numerical
issue as to how to obtain H−1. To summarize the Laplace approximation, if we have
an arbitrary distribution p(θ|O) with given continuous distribution f(θ), the Laplace
approximation provides an analytical Gaussian distribution by using the mode θMAP
and the Hessian matrix H. θMAP is usually obtained by some numerical computation or
the MAP estimation.
Now, let us consider the relationship between MAP (Chapter 4) and Laplace approx-
imations. As we discussed in Eq. (4.9), the MAP approximation of the posterior
distribution is represented by a Dirac delta function as follows:
pMAP(θ|O) = δ(θ −θMAP).
(6.10)
Therefore, Eq. (6.9) approaches the MAP solution, when the variance in the Gaussian
distribution in Eq. (6.9) becomes very small. This often arises when the amount of
training data is very large. In this sense, the Laplace approximation is a more precise
approximation for the true posterior distribution than MAP, in terms of the asymptotic
approximation. This precision of the Laplace approximation comes from consideration
of the covariance matrix (Hessian matrix H) effect in Eq. (6.9).
The Laplace approximation is widely used as an approximated Bayesian inference
method for various topics (e.g., Bayesian logistic regression (Spiegelhalter & Lauritzen
1990, Genkin, Lewis & Madigan 2007), Gaussian processes (Rasmussen & Williams
2006), and Bayesian neural networks (MacKay 1992c)). Sections 6.3 and 6.4 discuss
the applications of the Laplace approximation to Bayesian predictive classiﬁcation and
neural networks in acoustic modeling, respectively.
Note that the Laplace approximation is usually used to obtain the posterior distribu-
tion for non-exponential family distributions. Although the Laplace approximation can
be used to deal with latent variable problems in the posterior distribution (e.g., Gaussian
mixture model), the assumption of approximating a multiple peak distribution with a

214
Asymptotic approximation
single peak Gaussian is not adequate in many cases. Therefore, the Laplace approxima-
tion is often used with the other approximations based on the EM algorithm (MAP–EM,
VB–EM) or sampling algorithm (MCMC) that handle latent variable problems.
6.2
Bayesian information criterion
This section describes the Bayesian information criterion in general. We focus on the
posterior distribution of model p(M|O), which is introduced in Section 3.8.7. We ﬁrst
think about a simple case when there is no latent variable. Then, p(M|O) is represented
by using the Bayes rule and the sum rule as follows:
p(M|O) = p(O|M)p(M)
p(O)
=

p(O|θ, M)p(θ|M)dθp(M)
p(O)
,
(6.11)
where p(M) denotes a prior distribution for a model M. Similarly to the formalization
for Laplace approximation, we use a vector form of model parameters, that is θ ∈RJ,
and J is the number of parameters. Then, p(θ|M) denotes a prior distribution for a model
parameter θ given M, and p(O|θ, M) denotes a likelihood function given θ and M.
Suppose we want to compare two types of models (M1 and M2) in terms of the
posterior values of these models. Figure 6.1 compares ﬁtting the same data with a
single Gaussian (M1) or two Gaussians (M2). For this comparison, we can compute
the following ratio of p(M1|O) and p(M2|O) and check whether the ratio is larger/less
than 1:
p(M1|O)
p(M2|O) =

p(O|θ1,M1)p(θ1|M1)dθ1p(M1)
p(O)

p(O|θ2,M2)p(θ2|M2)dθ2p(M2)
p(O)
=

p(O|θ1, M1)p(θ1|M1)dθ1p(M1)

p(O|θ2, M2)p(θ2|M2)dθ2p(M2).
(6.12)
This factor is called the Bayes factor (Kass & Raftery 1993, 1995). To obtain the Bayes
factor, we need to compute the marginal likelihood term:

p(O|θ, M)p(θ|M)dθ.
(6.13)
Figure 6.1
An example of model comparison, ﬁtting the same data with a single Gaussian (M1) or two
Gaussians (M2). By comparing the posterior probabilities of p(M1|O) and p(M2|O), we can
select a more probable model.

6.2 Bayesian information criterion
215
Since it is difﬁcult to solve this integral analytically, the following section approxi-
mates this calculation based on the asymptotic approximation, similarly to the Laplace
approximation, as discussed in Section 6.1.
We ﬁrst deﬁne the following continuous function g over θ:
p(O|θ, M)p(θ|M) ≜exp (−g(θ)) .
(6.14)
This is slightly different from the deﬁnition of function f in Eq. (6.1), as Eq. (6.1) focuses
on the posterior distribution of the parameters. Then, similarly to the previous section,
we deﬁne the following MAP value:
θ∗= arg max
θ
p(O|θ, M)p(θ|M)
= arg min
θ
g(θ).
(6.15)
By using the Taylor expansion result in Eq. (6.4) with θ∗, Eq. (6.13) can be represented
as follows:

p(O|θ, M)p(θ|M)dθ
≈exp

−g(θ∗)
 
exp

−1
2(θ −θ∗)⊺H(θ −θ∗)

dθ
= exp

−g(θ∗)
 (2π)
J
2
|H|
1
2
,
(6.16)
where J is the number of dimensions of θ, that is the number of parameters. The Hessian
matrix H is deﬁned with g(θ) as
H ≜∇θ∇θg(θ)|θ=θ∗.
(6.17)
Or it is deﬁned with p(O|θ, M) and p(θ|M) as
H ≜−∇θ∇θ log (p(O|θ, M)p(θ|M))|θ=θ∗.
(6.18)
The integral in Eq. (6.16) can be solved by using the following normalization property
of a Gaussian distribution:

N(θ|θ∗, H−1)dθ = |H|
1
2
(2π)
J
2

exp

−1
2(θ −θ∗)⊺H(θ −θ∗)

dθ = 1
=⇒

exp

−1
2(θ −θ∗)⊺H(θ −θ∗)

dθ = (2π)
J
2
|H|
1
2
.
(6.19)
Thus, from Eq. (6.16), the marginal likelihood is computed by using the determinant of
the Hessian matrix |H|
1
2 .
Here, we focus on the determinant of the Hessian matrix |H|. From the deﬁnition in
Eq. (6.18), the Hessian matrix can be represented as:
H = −∇θ∇θ log p(O|θ, M)|θ=θ∗−∇θ∇θ log p(θ|M)|θ=θ∗
≈−∇θ∇θ log p(O|θ, M)|θ=θ∗.
(6.20)

216
Asymptotic approximation
Thus, the Hessian matrix is decomposed to the logarithmic likelihood and prior term,
and we neglect the prior term that does not depend on the amount of data. If we assume
that the observed data are independently and identically distributed (iid), the likelihood
term of T frame feature vectors is approximated when T is very large as1
∇θ∇θ log p(O|θ, M)|θ=θ∗≈−TI,
(6.21)
where I is a Fisher information matrix obtained from a single observation, which does
not depend on the amount of data. Therefore, we can approximate the determinant of
the Hessian matrix as:
|H|−1
2 ≈|TI|−1
2 = T−J
2 |I|.
(6.22)
This approximation implies that the Hessian matrix approaches zero when the amount
of data is large. That is
N(θ|θ∗, H−1) →δ(θ −θ∗) when T →∞.
(6.23)
Therefore, the Laplace approximation can be equivalent to the MAP approximation
when the amount of data is large, which is a reasonable outcome.
In the BIC approximation, by substituting Eq. (6.22) into Eq. (6.16), we obtain
log

p(O|θ, M)p(θ|M)

≈log p(O|θ∗, M) + log p(θ∗|M) + J
2 log(2π) −J
2 log T −J
2 log |I|.
(6.24)
Further, by neglecting the terms that do not depend on T, we ﬁnally obtain the following
simple equation:
log p(M|O) ∝log

p(O|θ, M)p(θ|M)

≈log p(O|θ∗, M)



log likelihood
−
J
2 log T
  
penalty term
.
(6.25)
Here, θ∗is often approximated by the ML estimate θML instead of θMAP, since θMAP
approaches θML when T →∞, as discussed in Section 4.3.7. Model selection based
on this equation is said to be based on the Bayesian information criterion (BIC), or
Schwartz information criterion, which approximates the logarithmic function of the
posterior distribution of a model p(M|O). The ﬁrst term on the right-hand-side is a
log-likelihood term with the MAP estimate (it is often approximated by the ML esti-
mate) of model parameter θ. It is well known that this log-likelihood value is always
increased when the number of parameters increases. Therefore, the log-likelihood value
from the maximum likelihood criterion cannot be used for model selection since it will
always prefer the model that has the largest number of parameters. On the other hand,
Eq. (6.25), which is based on the Bayesian criterion, has an additional term, which
1 This proof is not obvious. See Ghosh, Delampady & Samanta (2007) for more detailed derivations.

6.2 Bayesian information criterion
217
is proportional to the number of model parameters, denoted by J, and the logarith-
mic number of observation frames, denoted by log T. This term provides the penalty
so that the total value in Eq. (6.25) is penalized not to select a model with too many
parameters.
Comparing Eq. (6.25) with the regularized form of the objective function in Eq. (4.3)
in the MAP estimation, Eq. (6.25) can be viewed as having an l0 regularization term:
log p(M|O) ≈log p(O|θ∗, M) −J
2 log T
= log p(O|θ∗, M)



log likelihood
−
∥θ∥0
2
log T



l0 regularization term
.
(6.26)
Therefore, the BIC objective function can also be discussed within a regularization
perspective. Another well-known model selection criterion, the Akaike information
criterion (AIC) (Akaike 1974), is similarly represented by
−1
2AIC = log p(O|θ∗, M) −J
= log p(O|θ∗, M) −∥θ∥0.
(6.27)
It also has the l0 regularization term, but it does not depend on the amount of data T.
Using the BIC, we can simplify the model comparison based on the Bayes factor.
That is, by substituting Eq. (6.25) into the logarithm of the Bayes factor (Eq. (6.12)) we
can also obtain the following equation:
log
p(M1|O)
p(M2|O)

≈log p(O|θ∗
1, M1) −log p(O|θ∗
2, M2) −J1 −J2
2
log T.
(6.28)
Therefore, we can perform the model comparison by checking the sign of Eq. (6.28). If
the sign is positive, M1 is a more appropriate model in the BIC sense.
However, since we usually cannot assume that a single Gaussian distribution applies
to a complicated model and/or that there are enough data to satisfy the large sample
approximation, the BIC assumption is not valid for our practical problems. Therefore,
in practical use, we introduce a tuning parameter λ which heuristically controls the
balance of the log-likelihood and penalty terms, as follows:
log

p(O|θ, M)p(θ|M)

≈log p(O|θ∗, M) −λJ
2 log T.
(6.29)
This actually works effectively for the model selection problems of HMMs and GMMs,
including speech processing (Chou & Reichl 1999, Shinoda & Watanabe 2000) and
image processing (Stenger, Ramesh, Paragios et al. 2001). We discuss the applications
of BIC to speech processing in Sections 6.5 and 6.6.

218
Asymptotic approximation
6.3
Bayesian predictive classiﬁcation
This section addresses how the Laplace approximation is developed to build the
Bayesian predictive classiﬁcation (BPC) approach which has been successfully applied
for robust speech recognition in noisy environments (Jiang et al. 1999, Huo & Lee
2000, Lee & Huo 2000, Chien & Liao 2001). As mentioned in Section 3.1, the Bayes
decision theory for speech recognition can be realized as the BPC rule where the
expected loss function in Eq. (3.16) is calculated by marginalizing over the continu-
ous density HMM parameters  = {π = {πj}, A = {aij}, B = {ωjk, μjk, jk}}, as
discussed in Section 3.2.3, as well as the n-gram parameters  = {p(wi|wi−1
i−n+1)},
as discussed in Section 3.6. Here, we only focus on the acoustic models based on
HMMs. The marginalization over all possible parameter values aims to construct a
decision rule which considers the prior uncertainty of HMM parameters. The result-
ing speech recognition is robust to mismatch conditions between training and test data.
In general, such marginalization could compensate some other ill-posed conditions
due to the uncertainties and variations from the observed data and the assumed mod-
els. In this section, we ﬁrst describe how a Bayesian decision rule is established for
robust speech recognition and how the uncertainties of model parameters are charac-
terized. Then, the approximate solution to the HMM-based decision rule is formulated
by using the Laplace approximation. We also present some other extensions of the BPC
decision rule.
6.3.1
Robust decision rule
In automatic speech recognition, we usually apply the plug-in maximum a-posteriori
(MAP) decision rule, as given in Eq. (3.2), and combine the approximate acoustic model
ˆp(O|W) and language model ˆp(W) for ﬁnding the most likely word sequence W
corresponding to an input speech sentence O = {ot}. The maximum likelihood (ML)
parameters ML, as described in Section 3.4, and the MAP parameters MAP, as for-
mulated in Section 4.3, are treated as the point estimates and plugged into the MAP
decision for speech recognition. However, in practical situations, speech recognition
systems are occasionally applied in noisy environments. The training data may be col-
lected in ill-posed conditions, where the HMM parameters may be over-trained with
limited training data conditions, and can lose the generalization capability for future
data conditions. Thus, the assumed model may not properly represent the real-world
speech data. For this situation, the plug-in MAP decision, relying only on the single
best HMM parameter values ML or MAP, is risky for speech recognition. Because of
these considerations, we are motivated to establish a robust decision rule which accom-
modates the uncertainties of the assumed model and the collected data so that the error
risks in speech recognition can be reduced.
In the HMM framework (Section 3.2), the prior uncertainty of GMM parameters
j = {ωjk, μjk, jk|k = 1, · · · , K} in an HMM state j should be characterized from
training data and then applied for a BPC decision of future data. In this section, we
use the following notation to clearly distinguish training and future data:

6.3 Bayesian predictive classiﬁcation
219
O : future data,
O : training data.
(6.30)
Let the distribution p(ot|j) of a future observation frame ot under an HMM state j be
distorted in an admissible set Pj(ϵj) with a distortion level ϵj ≤0 which is denoted by
Pj(ϵj) = {p(ot|j)|j ∈(ϵj)},
(6.31)
where (ϵj) denotes the admission region of the GMM parameter space. In the special
case of no distortion (ϵj = 0), Pj(0) = {p(ot|(0)
j )} is a singleton set which consists
of the ideal and non-distorted distribution of an HMM state j with the parameters (0)
j
estimated from a training set O. However, in real-world applications, there exist many
kinds of distortions (ϵj > 0) between the trained models and the test observations. These
distortions and variations should be modeled and incorporated in the Bayes decision to
achieve robustness of speech recognition in adverse environments.
The Bayesian inference approach provides a good way to formalize this parameter
uncertainty problem and formulate the solution to the robust decision rule. To do so,
we intend to consider the uncertainty of the HMM parameters  to be random. An
a-priori distribution p(|) could serve as the prior knowledge about , where  ∈
are located in a region of interest of the HMM parameter space , and  denotes
the corresponding hyperparameters. Such prior information may come from subject
considerations and/or from previous experience. Therefore, the BPC decision rule is
established as
ˆW = dBPC(O) = arg max
W
˜p(W|O)
= arg max
W
˜p(O|W)p(W),
(6.32)
where
˜p(O|W) =


p(O|, W)p(|O, W)d.
(6.33)
Here, p(|O, W) denotes the posterior distribution of HMM parameters  given the
training utterances O, which is written as
p(|O, W) =
p(O|, W)p()

 p(O|, W)p()d.
(6.34)
In Eq. (6.33), the marginalization is performed over all possible HMM parameter val-
ues . This marginalized distribution is also called the predictive distribution. Model
uncertainties are considered in the BPC decision rule. The optimum BPC decision rule
was illustrated in Nadas (1985).
The crucial difference between the plug-in MAP decision and the BPC decision
is that the former acts as if the estimated HMM parameters were the true ones,
whereas the latter averages over the uncertainty of parameters. In an extreme case, if
p(|O, W) = δ( −ML/MAP) with δ(·) denoting the Dirac delta function with the
ML or MAP estimate, the BPC decision rule coincides with the plug-in MAP decision

220
Asymptotic approximation
rule based on ML HMM parameters ML/MAP, i.e., the predictive distribution can be
approximated as:
˜p(O|W) =


p(O|, W)p(|O, W)d
≈


p(O|, W)δ( −ML/MAP)d
= p(O|ML/MAP, W).
(6.35)
Therefore, the plug-in MAP decision rule with the ML/MAP estimate corresponds to an
approximation of the BPC decision rule.
Using the continuous density HMMs, the missing data problem happens so that the
predictive distribution is calculated by considering all sequences of HMM states S = {st}
and mixture components V = {vt}:
˜p(O|W) =

p(O|, W)p(|O, W)d
=

p(O|, W)p(|, W)d
=

S,V

p(O, S, V|, W)p(|, W)d
=

S,V

p(O|S, V, , W)p(S, V, |, W)p(|, W)d
≜E(S,V,)[p(O|S, V, , W)],
(6.36)
where the posterior distribution p(|O, W) given training data O is replaced by the
prior distribution p(|, W) given hyperparameters  in accordance with an empirical
Bayes theory. This predictive distribution is seen as an integration of likelihood func-
tion p(O|S, V, , W) over all possible values of {S, V, }. However, the computation of
predictive distribution over all state sequences S and mixture component sequences V is
complicated and expensive. A popular way to deal with this computation is to employ
the Viterbi approximation, as we discussed in Section 3.3.2, to compute the approximate
predictive distribution (Jiang et al. 1999):
˜p(O|W) ≈max
S,V

p(O, S, V|, W)p(|, W)d.
(6.37)
Therefore, the approximated predictive distribution only considers marginalization over
the model parameter . The following sections describe how to deal with model
parameter marginalization.
6.3.2
Laplace approximation for BPC decision
In this section, Laplace approximation is adopted to approximate the integral in calcula-
tion of the predictive distribution ˜p(O, S, V|W) in Eq. (6.36) or in Eq. (6.37), given state

6.3 Bayesian predictive classiﬁcation
221
and mixture component sequences S and V. Let us deﬁne the continuous function g(),
similarly to Eqs. (6.1) and (6.14):
p(O, S, V|, W)p(|, W) = exp(−g()).
(6.38)
The value of  that minimizes g() is obtained as a mode or an MAP estimate of HMM
parameters,
MAP = arg min

g()
= arg max

p(O, S, V|, W)p(|, W).
(6.39)
Note that we have a solution to the above optimization based on the MAP estimation,
as we discussed in Section 4.3. Then since we have the equation ∇g()|=MAP =
0 from Eq. (6.39), a second-order Taylor expansion around MAP can be used to
approximate g() as follows:
g() ≈g(MAP) + 1
2( −MAP)⊺H( −MAP),
(6.40)
where H denotes a Hessian matrix deﬁned as
H ≜∇∇g()|=MAP.
(6.41)
We should note that we use a vector representation of  (i.e.,  ∈RM and M
is the number of all CDHMM parameters) by arranging all CDHMM parameters
({aij, ωjk, μjk, jk}) to form a huge vector, to make the above gradient well deﬁned.2
Therefore, we obtain the approximate predictive distribution, which is derived from
˜p(O, S, V|W) =

exp (−g())d
≈exp (−g(MAP))

exp

−1
2( −MAP)⊺H( −MAP)

d.
(6.42)
Similarly to Eq. (6.18), the predictive distribution in Eq. (6.42) is approximately
obtained through arranging a multivariate Gaussian distribution in the integrand of
Eq. (6.42) as follows:
˜p(O, S, V|W) ≈p(O, S, V|MAP, W)
× p(MAP|, W)(2π)M/2|H|−1/2.
(6.43)
As a result, the BPC decision based on Laplace approximation is implemented by
Eq. (6.43). The logarithm of predictive distribution, log ˜p(O, S, V|W), is seen as follows:
log ˜p(O, S, V|W)
≈log
(
p(O, S, V|MAP, W)p(MAP|, W)
)
+ log
(
(2π)M/2|H|−1/2)
.
(6.44)
2 We should also note that most CDHMM parameters have some constraint (e.g., state transitions and
mixture weights have positivity and sum-to-one constraint) and the gradient of these parameters should
consider these constraints. However, the following example only considers mean vector parameters, and
we do not have this constraint problem.

222
Asymptotic approximation
The likelihood of the ﬁrst term can be obtained by using the MAP–EM (or Viterbi
approximation), as we discussed in Section 4.3. The second term is based on the log-
arithm of the determinant of the Hessian matrix. From Eq. (6.9), the Hessian matrix
can be interpreted as the precision (inverse covariance) matrix of the model parameters
(p(|O) ≈N(|MAP, H−1)). Therefore, if the model parameters are uncertain, the
determinant of the covariance matrix of  becomes large (i.e., the determinant of the
Hessian matrix H becomes small), the log likelihood of the predictive distribution is
decreased. Thus, considering model uncertainty in the BPC decision is meaningfully
reﬂected in Eq. (6.43).
6.3.3
BPC decision considering uncertainty of HMM means
To simplify the discussion, we only consider the uncertainty of the mean vectors in
HMM parameters for BPC decoding, as the mean vectors are the most dominant param-
eters for the ASR performance. In addition, by only focusing on the mean vectors, we
can avoid the difﬁculty of applying the Laplace approximation to the covariance and
weight parameters, which have constraints, assuming the HMM means of state j 3 and
mixture component k in dimension d are independently generated from Gaussian prior
distributions with the Hessian matrix as the precision matrix.
The joint prior distribution p(|, W) is expressed by
p({μjkd}|{μ0
jkd, 0
jkd}, W) =
J
j=1
K

k=1
D

d=1
1
4
2π0
jkd
exp
&
−
(μjkd −μ0
jkd)2
20
jkd
'
,
(6.45)
with a collection of hyperparameters including Gaussian mean vectors μ0
jk = {μ0
jkd}
and diagonal covariance matrices 0
jk = diag {0
jkd|d = 1, · · · , D} for different states
and mixture components. The GMM mixture weights {ωjk} and diagonal covariance
matrices {jk = diag {jkd|d = 1, · · · , D}} of the original CDHMM are assumed to be
deterministic without uncertainty. Note that the above prior distribution is conditional on
the word sequence W, as it is required in the BPC decision rule in Eq. (6.36). This condi-
tion means that we only deal with the prior distributions of HMM parameters appearing
in the state sequences obtained by hypothesized word sequence W. But this condition
can be disregarded since this condition for the prior distribution does not matter in the
following analysis.
Given an unknown test utterance O = {ot|t = 1, · · · , T} and the unobserved
state sequence S
=
{st|t
=
1, · · · , T} and mixture component sequence V
=
{vt|t = 1, · · · , T} obtained by hypothesized word sequence W, we combine the
Gaussian likelihood function p(O|S, V, {μ′
jkd}, W) and the Gaussian prior distribution
p({μ′
jkd}|{μ0
jkd, 0
jkd}, W) based on the MAP–EM approach (Section 4.2) to give:
3 Note that index j denotes all HMM states for all phonemes, unlike index j for a phoneme in Section 3.2.
Therefore, the number of HMM states J appearing later in this section also means the number of all HMM
states used in the acoustic models, which could be several thousand when we use a standard LVCSR setup.

6.3 Bayesian predictive classiﬁcation
223
log p({μ′
jkd}|O, W) ≈QMAP({μ′
jkd}|{μjkd})
= E(S,V)[log p(O, S, V|{μ′
jkd}, W)|O, {μjkd}]
+ log p({μ′
jkd}|{μ0
jkd, 0
jkd}, W).
(6.46)
Equation (6.46) is further rewritten to come up with an approximate posterior distribu-
tion which is arranged as a Gaussian distribution:
log p({μ′
jkd}|O, W) ≈
T

t=1
J

j=1
K

k=1
γt(j, k)

log p(ot|st = j, vt = k, μ′
jkd, W)
+ log p(μ′
jkd|μ0
jkd, 0
jkd, W)

=
T

t=1
J

j=1
K

k=1
log N(μ′
jkd| ˆμjkd, ˆjkd),
(6.47)
with the hyperparameters derived as
ˆμjkd =
jkd
cjk0
jkd + jkd
μ0
jkd +
cjk0
jkd
cjk0
jkd + jkd
¯ojkd,
(6.48)
1
ˆjkd
=
1
0
jkd
+ cjk
jkd
,
(6.49)
where
γt(j, k) = p(st = j, vt = k|O, {μjkd}, W),
(6.50)
cjk =
T

t=1
γt(j, k),
(6.51)
¯ojk =
T
t=1 γt(j, k)ot
cjk
.
(6.52)
The posterior distribution in Eq. (6.46) is obtained because we adopt the conjugate prior
to model the uncertainty of HMM means. Recall the MAP estimation result of the mean
vector in Eq. (6.53):
ˆμjkd ≜
φμ
jkdμ0
jkd + T
t=1 γt(j, k)otd
φμ
jk + T
t=1 γt(j, k)
.
(6.53)
Comparing Eqs. (6.53) and (6.48), it is apparent that Eq. (6.48) based on the BPC
solution considers the effect of 0
jkd.
An EM algorithm is iteratively applied to approximate the log posterior distribution
of new estimate {μ′
jkd}, log p({μ′
jkd}|O, W), by using the posterior auxiliary function
of new estimate {μ′
jkd} given the current estimate {μjkd}, QMAP({μ′
jkd}|{μjkd}). The
calculation converges after several EM iterations. Given the updated occupation proba-
bility, the Gaussian posterior distribution of {μ′
jkd} = [μ′
11
⊺, μ′
12
⊺, · · · , μ′
JK
⊺]⊺, which

224
Asymptotic approximation
concatenates all Gaussian mean vectors, is calculated and used to ﬁnd the approximation
in Eq. (6.9):
p({μ′
jkd}|O, W) ≈N({μ′
jkd}|{ ˆμjkd}, { ˆjkd})
≈N({μ′
jkd}|{μMAP
jkd }, {[(Hμ)−1]jkd}).
(6.54)
Therefore, we substitute
μMAP
jkd
= ˆμjkd,
(6.55)
[(Hμ)−1]jkd = ˆjkd,
(6.56)
into Eq. (6.43). This approximation avoids a need to directly compute the Hessian
matrix, where the number of dimensions of the Hessian matrix is very large (J ×K ×D).
The BPC-based recognition is accordingly implemented for robust speech recognition.
In Chien & Liao (2001), the BPC decision was extended to a transformation-based
BPC decision where the uncertainties of transformation parameters were taken into
account. The transformation parameters of HMM means and variances were charac-
terized by the Gaussian–Wishart distribution. Using this decision, HMM parameters
were treated as deterministic values. In Chien (2003), the transformation-based BPC was
further combined with the maximum likelihood linear regression adaptation (Leggetter
& Woodland 1995). The linear regression based BPC was proposed for noisy speech
recognition. The transformation regression parameters were represented by the mul-
tivariate Gaussian distribution. The predictive distributions in these two methods were
approximated by calculating the predictive distributions for individual frames ot without
involving the Laplace approximation.
6.4
Neural network acoustic modeling
Acoustic modeling based on the deep neural network (DNN)
is now a new trend
towards achieving high performance in automatic speech recognition (Dahl, Yu, Deng
et al. 2012, Hinton et al. 2012). The context-dependent DNNs were combined with
HMMs and have recently shown signiﬁcant improvements over discriminatively trained
HMMs with state-dependent GMMs (Seide, Li, Chen et al. 2011). In particular, the
DNN was proposed to conduct a greedy and layer-wise pretraining of the weight
parameters with either a supervised or unsupervised criterion (Hinton, Osindero &
Teh 2006, Salakhutdinov 2009). This pretraining step prevents the supervised train-
ing of the network from being trapped in a poor local optimum. In general, there are
ﬁve to seven hidden layers with thousands of sigmoid non-linear neurons in a DNN
model. The output layer consists of softmax non-linear neurons. In addition to the pre-
training scheme, the success of the DNN acoustic model also comes from the tandem
processing, frame randomization (Seide et al. 2011) and (Hessian-free) sequence train-
ing (Kingsbury, Sainath & Soltau 2012, Vesel`y, Ghoshal, Burget et al. 2013). Without
loss of generality, we address the artiﬁcial neural network (NN) acoustic model for
ASR, based on the feed-forward multilayer perceptron with a single hidden layer, as

6.4 Neural network acoustic modeling
225
Figure 6.2
A neural network based on the multilayer perceptron with a single hidden layer. ot ∈RD,
zt ∈RM, and yt ∈RK denote the input vector, hidden unit vector, and output vector at frame t,
respectively. ot0 and zt0 are introduced to consider the bias terms in the transformation, and these
are usually set with some ﬁxed values (e.g., ot0 = zt0 = 1). w(1) ∈RM(D+1) and
w(2) ∈RK(M+1) denote the weight vectors (to use a Laplace approximation, we represent these
values as the vector representation rather than the matrix representation) from the input to hidden
layers and from the hidden to output layers, respectively.
illustrated in Figure 6.2. We bring in the issue of model regularization in construction
of NNs and present the Bayesian neural networks (MacKay 1992c, Bishop 2006) for
robust speech recognition. This framework can obviously be extended to DNN-based
speech recognition with many hidden layers. Importantly, we introduce a prior distribu-
tion to express the uncertainty of synaptic weights in Bayesian NNs. This uncertainty
serves as a penalty factor to avoid too-complicated or over-trained models. The varia-
tions of acoustic models due to the heterogeneous training data could be compensated
through Bayesian treatment. The Laplace approximation is applied to derive the pre-
dictive distribution for robust classiﬁcation of a speech signal. In what follows, we ﬁrst
describe the combination of Bayesian NNs and HMMs for the application of speech
recognition. MAP estimation is applied to ﬁnd a solution to adaptive NN training. A
Laplace approximation is developed to construct Bayesian NNs for HMM-based speech
recognition.
6.4.1
Neural network modeling and learning
A standard neural network for a K-class classiﬁcation problem is depicted in Figure 6.2,
which is established as a feed-forward network function with a layer structure.
This function maps the D input neurons ot = {oti} ∈RD to K output neurons
yt(ot, w) = {ytk(ot, w)} ∈RK where
ytk(ot, w) =
exp (ak(ot, w))

j exp

aj(ot, w)

(6.57)

226
Asymptotic approximation
is given by a softmax non-linear function, or the normalized exponential function which
satisﬁes 0 ≤ytk ≤1 and K
k=1 ytk = 1. In Eq. (6.57), ak(ot, w) is calculated as the
output unit activation given by
ak(ot, w) =
M

j=0
w(2)
kj Sigmoid
 D

i=0
w(1)
ji oti

,
(6.58)
where
Sigmoid(a) =
1
1 + exp(−a)
(6.59)
denotes the logistic sigmoid function and
w = {w(1)
ji , w(2)
kj |1 ≤i ≤D, 1 ≤j ≤M, 1 ≤k ≤K}
(6.60)
denotes the synaptic weights of the ﬁrst and the second layers, respectively. For model
training, the target values of output neurons are assigned as Bernoulli variables where
value 1 in the kth neuron means that ot belongs to class k and the other neurons have
target value 0. In this ﬁgure, øt0 = zt0 = 1 and the corresponding weights {w(1)
j0 , w(2)
k0 }
denote the bias parameters in neurons. This model is also known as the multilayer per-
ceptron. Such a neural network model performs well for pattern recognition for two
reasons. One is the nested non-linear function, which can learn the complicated deep
structure from real-world data through feed-forwarding the input signals to the output
classes layer by layer. The second reason is that it adopts the softmax function or the
logistic sigmoid function Sigmoid(·) as the neuron processing unit, which is beneﬁcial
for building high-performance discriminative models. In general, deep neural networks
are affordable for learning the invariance information and extracting the hierarchy of
concepts or features from data. Higher-level concepts are deﬁned from lower-level ones.
The logistic sigmoid function and softmax function play an important role for binary
classiﬁcation and multi-class classiﬁcation, respectively. These functions are essential
for calculating posterior probability and establishing the discriminative model. Basi-
cally, we collect a set of training samples and their target values {ot, tt}. The synaptic
weights are estimated by calculating an error function E(w) and minimizing it through
the gradient descent algorithm,
w(τ+1) = w(τ) −η∇E(w(τ)),
(6.61)
where η denotes the learning rate and τ denotes the iteration index. The sum-of-squares
error function is calculated for the regression problem while the cross entropy error
function is applied for the classiﬁcation problem. The error back-propagation algorithm
has been proposed to ﬁnd gradient vector ∇E(w) and widely applied for training of
individual synaptic weights w = {w(1)
ji , w(2)
kj } in different layers.
6.4.2
Bayesian neural networks and hidden Markov models
For the application of speech recognition, the NNs should be combined with the HMMs
(denoted by NN–HMMs) to deal with sequential training of synaptic weights w from a
sequence of speech feature vectors O = {ot ∈RD|t = 1, · · · , T}. As discussed before,

6.4 Neural network acoustic modeling
227
this feature can be obtained by stacking the neighboring MFCC or ﬁlter bank features.
For example, if the original features are represented by Oorg = {oorg
t
∈RDorg|t =
1, · · · , T}, the stacked feature for the NN input is represented by
ot = [(oorg
t−L)⊺, · · · , (oorg
t
)⊺, · · · , (oorg
t−L)⊺]⊺.
(6.62)
Then, the number of feature dimensions becomes D = (2L + 1)Dorg. L is set between
ﬁve and seven depending on the task. This expanded feature can model the short-range
dynamics of speech features, as we model in the conventional GMM framework by
using the delta cepstrum or linear discriminant analysis techniques (Furui 1986, Haeb-
Umbach & Ney 1992).
However, the stacking technique cannot fully model the long-range speech dynamics.
The Markov states are used to characterize the temporal correlation in sequential pat-
terns. We estimate the NN–HMM parameters  = {π, A, w}, consisting of initial state
probabilities π, state transition probabilities A and the N-dimensional synaptic weights
w, by maximizing the likelihood function p(O|). Each output neuron calculates the
posterior probability of a context dependent HMM state k given a feature vector ot using
synaptic weights w, i.e.,
yk(ot, w) ≜p(st = k|ot, w).
(6.63)
ML estimation of  has an incomplete data problem and should be solved according
to the EM algorithm. The EM algorithm is an iterative procedure where the E-step is to
calculate an auxiliary function of training utterances O using new NN–HMM parameters
′ given parameters  at the current iteration:
QML(′|) = E(S){log p(O, S|′)|O, }
=

S
p(S|O, )
& T

t=1
(log a′
st−1st + log p(ot|st, w′))
'
∝
T

t=1
K

k=1
γt(k)

log a′
st−1st + log p(st = k|ot, w′) −log p(st = k)

,
(6.64)
where γt(k) = p(st = k|O, ) denotes the state occupation probability. NN–HMM
parameters ′ at a new iteration are then estimated by M-step:
w′ = arg min
w′
{−QML(w′|w)}.
(6.65)
Here, we only consider the estimation of the synaptic weights w by minimizing the
corresponding negative auxiliary function:
−QML(w′|w) ∝−
T

t=1
K

k=1
γt(k) log p(st = k|ot, w)
= −
T

t=1
K

k=1
γt(k) log yk(ot, w).
(6.66)

228
Asymptotic approximation
To implement the ML supervised training of NN–HMM parameters w, we ﬁrst
perform Viterbi decoding of training speech O = {ot|t = 1, · · · , T} by using true tran-
scription and NN–HMM parameters  or w at the current iteration. Each frame ot is
assigned an associated HMM state or output neuron st = k. This Viterbi alignment is
equivalent to assigning a label or target value ttk ≜γt(k) of a speech frame ot at an
output neuron k of either 0 or 1. That is, γt(k) ∈{0, 1} is treated as a Bernoulli target
variable. The negative auxiliary function in ML estimation is accordingly viewed as the
cross entropy error function between the Bernoulli target values based on current esti-
mate w and the posterior distributions from the NN outputs using new estimate w′. By
minimizing the cross entropy between the desired outputs {γt(k)} and the actual out-
puts {yk(ot, w)} over all time frames 1 ≤t ≤T and all context-dependent HMM states
1 ≤k ≤K, we establish the discriminative acoustic models (NN–HMMs) for speech
recognition. The ML NN–HMM parameters ML = {wML} are estimated. The error
back-propagation algorithm can be implemented to train the synaptic weights wML by
minimizing −Q(w′|w), as given in Eq. (6.66).
More importantly, we address the maximum a-posteriori (MAP) estimation of
NN–HMM parameters where the uncertainty of synaptic weights is compensated by
using a prior distribution. For simplicity, we assume that the continuous values of
weights in different layers come from a multivariate Gaussian distribution,
p(w) = N(w|μw, w),
(6.67)
where μw denotes the mean vector and w denotes the covariance matrix. This prior
information could be empirically inferred from training data. There are two Bayesian
inference stages in the so-called Bayesian NN–HMMs. The ﬁrst stage is to ﬁnd the
point estimate of synaptic weights according to the MAP estimation. This idea is simi-
lar to MAP estimation of HMM parameters (Gauvain & Lee 1994). MAP estimates of
NN–HMM parameters are calculated for adaptive training or speaker adaptation. The
second stage is to conduct the distribution estimation, and this stage tries to fulﬁl a full
Bayesian analysis by calculating the marginal likelihood with respect to all values of
synaptic weights. Robustness of speech recognition to variations of synaptic weights
due to heterogeneous data is assured.
In the ﬁrst inference stage, MAP estimates of model parameters MAP = {wMAP} are
calculated by maximizing a-posteriori probability or the product of likelihood function
and prior distribution. An EM algorithm is applied to ﬁnd MAP estimates by maximiz-
ing the posterior auxiliary function QMAP(′|). A MAP estimate of synaptic weights
wMAP is derived by minimizing the corresponding auxiliary function:
−QMAP(w′|w) ∝−
T

t=1
K

k=1
γt(k) log p(st = k|ot, w)
+ 1
2(w′ −μw)⊺−1
w (w′ −μw),
(6.68)
which is viewed as a kind of penalized cross entropy error function. The prior distribu-
tion provides subjective information for Bayesian learning, or equivalently, serves as a

6.4 Neural network acoustic modeling
229
penalty function for model training. w plays the role of a metric matrix of the w′ −μw
distance. Again, the error back-propagation algorithm is applied to estimate the synaptic
weights w′ at a new iteration. Using this algorithm, the derivative of penalty function
with respect to w′ contributes the estimation of wMAP. The estimated wMAP is treated as
a point estimate or deterministic value for prediction or classiﬁcation of future data.
6.4.3
Laplace approximation for Bayesian neural networks
In the second Bayesian inference stage, the synaptic weights w are treated as a latent
random variable. The hyperparameters  = {μw, w} are used to represent the uncer-
tainty of random parameters w. Such uncertainty information plays an important role in
subjective inference, adaptive learning, and robust classiﬁcation. Similarly to the BPC
based on the standard HMMs as addressed in Section 6.3, we would like to present the
BPC decision rule based on the combined NN–HMMs. To do so, we need to calculate
the predictive distribution of test utterance O which is marginalized by considering all
values of weight parameters w in the likelihood function p(O|w):
˜p(O|μw, w) =

p(O, w|μw, w)dw
=

p(O|w)p(w|μw, w)dw
≜E(w)[p(O|w)].
(6.69)
The integral is performed over the parameter space of w. However, due to the non-linear
feed-forward network function yk(ot, w) = p(st = k|ot, w), the closed-form solution to
the integral calculation does not exist and should be carried out by using the Laplace
approximation. By applying the Laplace approximation for integral operation in BIC
and BPC decision, as given in Eqs. (6.16) and (6.43), we develop the BPC decision
based on the NN–HMMs according to the approximate predictive distribution:
˜p(O|μw, w) ≈p(O|wMAP) · p(wMAP|μw, w) · |H|−1/2
= p(wMAP|O, μw, w) · |H|−1/2
≈exp
(
QMAP(w′|w)
)
|w′=wMAP · |H|−1/2,
(6.70)
where the mode wMAP of posterior distribution p(w|O, μw, w) is obtained through the
EM iteration procedure,
wMAP = arg max
w′
{QMAP(w′|w)},
(6.71)
and the Hessian matrix H is also calculated according to the EM algorithm through
H = ∇w′∇w′ log{p(O|w′)p(w′|μw, w)}|w′=wMAP
≈∇w′∇w′ QMAP(w′|w)|w′=wMAP
= ∇w′∇w′ QML(w′|w)|w′=wMAP −−1
w ,
(6.72)

230
Asymptotic approximation
which is the second-order differentiation of the log posterior distribution with respect
to NN–HMM parameters at the mode wMAP. Here, the auxiliary function QMAP(w′|w)
is introduced because the NN–HMM framework involves the missing data problem. As
explained in Section 6.3.3, the EM algorithm is applied to iteratively approximate the
posterior distribution at its mode by
p(w′|O, μw, w)|w′=wMAP ≈exp
(
QMAP(w′|w)
)
|w′=wMAP.
(6.73)
Accordingly, we construct the BPC decision based on NN–HMMs, which can achieve
robustness in automatic speech recognition.
6.5
Decision tree clustering
This section introduces the application of BIC-based model selection to decision
tree clustering of context-dependent HMMs, as performed in Shinoda & Watanabe
(1997) and Chou & Reichl (1999), without having to set a heuristic stopping crite-
rion, as is done in Young, Odell & Woodland (1994).4 This section ﬁrst describes
decision tree clustering based on the ML criterion, which determines the tying struc-
ture of context-dependent HMMs efﬁciently. Then, the approach is extended by
using BIC.
6.5.1
Decision tree clustering using ML criterion
The decision tree method has been widely used to construct a tied state HMM effectively
by utilizing the phonetic knowledge-based constraint and the binary tree search (Odell
1995) approaches. Here, we introduce a conventional decision tree method using the
ML criterion.
Let (n) denote a set of states that tree node n holds. We start with only a root node
(n = 0) which holds a set of all context-dependent phone (e.g., triphone hereinafter,
as shown in Figure 6.5) HMM states (0) for an identical center phoneme. The set of
triphone states is then split into two sets depending on question Q, (nQ
Y ) and (nQ
N),
which are held by two new nodes, nQ
Y and nQ
N, respectively, as shown in Figure 6.3.
The partition is determined by an answer to a phonemic question Q, such as “is the
preceding phoneme a vowel” and “is the following phoneme a nasal.” A particular ques-
tion is chosen so that the partition is the optimal of all the possibilities, based on the
likelihood value. We continue this splitting successively for every new set of states to
obtain a binary tree, as shown in Figure 6.4, where each leaf node holds a shared set
of triphone states. The states belonging to the same cluster are merged into a single
HMM state. A set of triphones is thus represented by a set of tied-state HMMs. An
HMM in an acoustic model usually has a left-to-right topology with three or four tem-
poral states. A decision tree is usually produced for each state in the sequence, and
4
Acoustic model selections based on BIC and minimum description length (MDL) criteria have been
independently proposed, but they are practically the same if we deal with Gaussian distributions.
Therefore, they are identiﬁed in this book and referred to as BIC.

6.5 Decision tree clustering
231
Figure 6.3
Splitting a set of HMM states (n) in node n into two sets of states (nQ
Y ) and (nQ
N) according
to question Q. The objective function is changed from L(n) to L(nQ
Y ) + L(nQ
N) after the split.
Figure 6.4
Binary decision tree. A context-dependent HMM state is represented by a single Gaussian
distribution, and a set of states is assigned to each node. Two child nodes are obtained based on a
yes/no answer to a (phonemic) question.
the trees are independent of each other, or a single decision tree is produced for all
states.
The phonetic question concerns the preceding and following phoneme context, and
is obtained through knowledge of the phonetics (Odell 1995) or it is also decided

232
Asymptotic approximation
Figure 6.5
Examples of context-dependent (triphone) HMM and sharing structure. We prepare a three-state
left-to-right HMM for each triphone, where /a/-/k/j-/a/ denotes a jth state in a triphone HMM
with preceding phoneme /a/, central phoneme /k/, and following phoneme /a/. In this example,
triphone HMM states /a/-/k/1-/a/, /a/-/k/1-/i/, and /a/-/s/1-/i/ are shared with the same Gaussian
distribution.
in a data-driven manner (Povey, Ghoshal, Boulianne et al. 2011). Table 6.1 shows
examples of the question. In a conventional ML-based approach, an appropriate question
is obtained by maximizing a likelihood value as follows:
Q = arg max
Q′ L(Q′),
(6.74)
where L(Q) denotes the gain of log-likelihood when a state set in a node is split by a
question Q. To calculate L(Q), we assume the following constraints:

6.5 Decision tree clustering
233
Table 6.1 Examples of phonetic questions for a j(= 1)th HMM state of a phoneme /a/.
Question
Yes
No
Preceding phoneme
{/a/, /i/, /u/, /e/, /o/} - /a/j=1 - { all }
otherwise
is vowel?
Following phoneme
{ all } - /a/j=1 - {/a/, /i/, /u/, /e/, /o/}
otherwise
is vowel?
Following phoneme
{ all } - /a/j=1 - {/b/, /d/, /g/}
otherwise
is media ?
Preceding phoneme
{/u/, /o/} - /a/j=1 - { all }
otherwise
is back vowel?
...
...
...
• Data alignments γt(j, k) and ξt(i, j) for each state are ﬁxed while splitting.
• Emission probability distribution in a state is represented by a single Gaussian
distribution (i.e., K = 1).
• Covariance matrices have only diagonal elements.
• A contribution of state transitions aij and initial weights πj for likelihood is disre-
garded.
These constraints simplify the likelihood calculation without using an iterative calcula-
tion, which greatly reduces the computational time.
Equation (6.74) corresponds to an approximation of the Bayes factor, as discussed in
Eq. (6.12) to compare two models: M1 models a set of triphones in the node n with a
single Gaussian; and M2 models two sets of triphones in the nodes nQ
Y and nQ
N separated
by a question Q:
L(Q′) = log
p(M2|O)
p(M1|O)

≈log
p(O|ML, M2)
p(O|ML, M1)

.
(6.75)
We obtain the gain of log-likelihood L(Q) in Eq. (6.74) under the above constraints.
Let O(i) = {ot(i) ∈RD : t = 1, . . . , T(i)} be a set of feature vectors that are assigned
to HMM state i by the Viterbi algorithm. T(i) denotes the frame number of training data
assigned to state i, and D denotes the number of feature vector dimensions. From the
constraints, log-likelihood L for a training data set, assigned to state set , is expressed
by the following D dimensional Gaussian distribution:
L = log p({O(i)}i∈|μ, )
= log

i∈
T(i)

t=1
N(Ot(i)|μ, )
= log

i∈
T(i)

t=1
(2π)−D
2 ||−1
2 e−1
2 (ot(i)−μ)⊺−1
 (ot(i)−μ),
(6.76)

234
Asymptotic approximation
where μ and  denote a D dimensional mean vector and a D×D diagonal covariance
matrix for a data set in , respectively. ML estimates μML

and ML

are obtained by
using the derivative technique. First, Eq. (6.76) is rewritten as
L = −D
2 log(2π)

i∈
T(i) −1
2

i∈
T(i)

t=1
D

d=1

log d + (otd(i) −μd)2
d

,
(6.77)
where d is a d-d element of the diagonal covariance matrix . Then, by using the
following derivative with respect to μd,
∂
∂μd
L = −1
2

i∈
T(i)

t=1
2otd(i) −μd
d
= 0,
(6.78)
we can obtain the ML estimate of the mean vector μML

as:
μML

=

i∈
T(i)
t=1 ot(i)
T
.
(6.79)
Similarly, by using the following derivative with respect to d,
∂
∂d
L = −1
2

i∈
T(i)

t=1

1
d
−(otd(i) −μd)2
2
d

= 0,
(6.80)
we can obtain the ML estimate of the diagonal component of the covariance matrix
ML
d as:
ML
d =

i∈
T(i)
t=1(otd(i) −μML
d )2
T
.
(6.81)
We summarize the above ML estimates as:
μML

=

i∈
T(i)
t=1 ot(i)
T
,
[ML
 ]dd = ML
d =

i∈
T(i)
t=1(otd(i) −μML
d )2
T
.
(6.82)
T ≜
i∈ T(i) denotes the frame number of training data assigned to HMM states
belonging to . ML
d denotes a d-d component for matrix ML
 . By substituting
Eq. (6.82) into Eq. (6.76), we can derive the following log-likelihood L with the ML
estimates of ML
 :
L = −D
2 log(2π)

i∈
T(i) −1
2

i∈
T(i)

t=1
D

d=1

log d + (otd(i) −μd)2
d
------μd→μML
d
d→ML
d
= −D
2 log(2π)T −1
2T
D

d=1
log ML
d −1
2T
D

d=1
ML
d
ML
d
= −T
2

D(1 + log(2π)) + log
--ML

--
.
(6.83)

6.5 Decision tree clustering
235
Note that the likelihood value is represented by the determinant of the ML estimate of
the covariance matrix, which can easily be calculated as we use the diagonal covariance.
Therefore, a gain of log-likelihood L(Q) can be solved as follows (Odell 1995):
L(Q) = L(nQ
Y ) + L(nQ
N) −L(n)
= l((nQ
Y )) + l((nQ
N)) −l((n)).
(6.84)
Here l in Eq. (6.84) is deﬁned as:
l() ≜−1
2

T log
--ML

--
for  = {(nQ
Y ), (nQ
N), (n)},
where we use the following relation:
T(n) = T(nQ
Y ) + T(nQ
N).
(6.85)
Equations (6.84) and (6.85) show that L(Q) can be calculated using the ML estimate
ML

and frame number T. Finally, the appropriate question in the sense of the ML
criterion can be computed by substituting Eq. (6.84) into Eq. (6.74).
However, we cannot use this likelihood criterion for stopping the split of nodes in
a tree. That is, the positivity of L(Q) for any split causes the ML criterion to always
select the model structure in which the number of states is the largest. That is, no states
are shared at all. To avoid this, the ML criterion requires the following threshold to be
set to stop splitting manually:
L ≤Thd.
(6.86)
There exist other approaches to stopping splitting manually by setting the number of
total states, or the maximum depth of the tree, as well as a hybrid approach combining
those approaches. However, the effectiveness of the thresholds in all of these manual
approaches has to be judged by the performance of the development set.
6.5.2
Decision tree clustering using BIC
We consider automatic model selection based on the BIC criterion, which is widely
used in model selection for various aspects of statistical modeling. Recall the following
deﬁnition of a BIC-based objective function for O = {ot|t = 1, · · · T} in Eq. (6.29):
LBIC = log p(O|θ∗, M) −λJ
2 log T,
(6.87)
where θ∗is the ML or MAP estimate of model parameters for model M, J is the number
of model parameters, and λ is a tuning parameter. Similarly to Section 6.5.1, we use a
single Gaussian for each node. Then, Eq. (6.87) for node  can be rewritten as follows:
LBIC

= L −λD log T
= −T
2

D(1 + log(2π)) + log
--ML

--
−λD log T,
(6.88)

236
Asymptotic approximation
where J = 2D when we use the diagonal covariance matrix. Therefore, the gain of objec-
tive function LBIC
(Q) using the BIC criterion (the logarithmic Bayes factor) is obtained
while splitting a state set by question Q, as follows:
LBIC
(Q) = L(Q) −λD log T(0),
(6.89)
where λ is a tuning parameter in the BIC criterion, and T(0) denotes the frame number
of data assigned to a root node.5 Equation (6.89) suggests that the BIC objective function
penalizes the gain in log-likelihood on the basis of the balance between the number of
free parameters and the amount of training data, and the penalty can be controlled by
varying λ. Model structure selection is achieved according to the amount of training data
by using LBIC
(Q) instead of using L(Q) in Eq. (6.74), that is
Q = arg max
Q′ LBIC
(Q′).
(6.91)
We can also use LBIC
(Q) for stopping splitting when this value is negative, without using
a threshold in Eq. (6.86), that is
LBIC
(Q) ≤0.
(6.92)
Thus, we can obtain the tied-state HMM by using the BIC model selection criterion.6
Note that the BIC criterion is an asymptotic criterion that is theoretically effective
only when the amount of training data is sufﬁciently large. Therefore, in the case of
a small amount of training data, model selection does not perform well because of
the uncertain ML estimates μML and ML. Shinoda & Watanabe (2000) show the
effectiveness of the BIC criterion for decision tree clustering in a 5000 Japanese word
recognition task by comparing the performance of acoustic models based on BIC with
models based on heuristic stopping criteria (namely, the state occupancy count and the
likelihood threshold). BIC selected 2069 triphone HMM states automatically with an
80.4 % recognition rate, while heuristic stopping criteria selected 1248 and 591 states
with recognition rates of 77.9 % and 66.6 % in the best and worst cases, respectively.
This result clearly shows the effectiveness of model selection using BIC. An extension
of the BIC objective function by considering a tree structure is also discussed in Hu
& Zhao (2007), and an extension based on variational Bayes is discussed in Section
7.3.6. In addition, BIC is used for Gaussian pruning in acoustic models Shinoda & Iso
(2001), and speaker segmentation (Chen & Gopinath 1999), which is discussed in the
next section.
5
The following objective function
LBIC
(Q) = L(Q) −λD log T(0)
(6.90)
is derived in Shinoda & Watanabe (2000). There are several ways to set the penalty term from Eq. (6.89).
6 The BIC criterion also has a tuning parameter λ in Eq. (6.87). However, the λ setting is more robust than
the threshold setting in Eq. (6.86) (Shinoda & Watanabe 2000).

6.6 Speaker clustering/segmentation
237
6.6
Speaker clustering/segmentation
BIC-based speaker segmentation is a particularly important technique for speaker
diarization, which has been widely studied recently (Chen & Gopinath 1999,
Anguera Miro, Bozonnet, Evans et al. 2012). The approach is usually performed in
two steps; the ﬁrst step segments a long sequence of speech to several chunks by using
a model selection approach. Then the second step clusters these chunks to form speaker
segments, which can also be performed by a model selection approach. Both steps use
BIC as model selection. This section considers the ﬁrst step of speaker segmentation
based on the BIC criterion.
6.6.1
Speaker segmentation
First, we consider the simple segmentation problems of segmenting a speech sequence
to two chunks where these chunks are uttered by two speakers, respectively, as shown
in Figure 6.6. That is:
{o1, · · · , oT} →{o1, · · · , ot} and {ot+1, · · · , oT},
(6.93)
where t is a change point from one speaker to the other speakers or noises. Assuming
that each speaker utterance is modeled by a single Gaussian, which is a very simple
assumption but works well in a practical use, this problem can be regarded as a model
selection problem where we have a set of candidates represented by:
• M0 : o1, · · · , oT ∼N(μ, );
• Mt : o1, · · · , ot ∼N(μ1,t, 1,t), ot+1, · · · , oT ∼N(μ2,t, 2,t)
for t = [2, T −1].
Figure 6.6
Speaker segmentation for audio data, which ﬁnd a speaker boundary t given speech features
{o1, · · · , oT}. The BIC segmentation method assumes that each segmentation is represented by a
Gaussian distribution.

238
Asymptotic approximation
The model M0 means that the sequence can be represented by one Gaussian with mean
vector μ and the diagonal covariance matrix , that is, the sequence only has one
speaker utterance. Similarly, the model Mt for t = [2, T−1] means that the sequence has
two speakers with a change point t with two Gaussians that have mean vectors μ1,t and
μ2,t, and diagonal covariance matrices 1,t and 2,t, depending on the change point t.
Based on these hypothesized models, we can also consider the Bayes factor in
Eq. (6.12) to compare two models of M0 and Mt:
log
 p(Mt|O)
p(M0|O)

for t = [2, T −1].
(6.94)
Similarly to the decision tree clustering, we can select an appropriate model from M0
and {Mt}T−1
t=2 by using BIC. Recall the following deﬁnition of a BIC-based objective
function of model M for O = {ot|t = 1, · · · N} in Eq. (6.29):
LBIC = log p(O|θ∗, M) −λJ
2 log N,
(6.95)
where θ∗is the ML or MAP estimate of model parameters for model M, J is the number
of model parameters, and λ is a tuning parameter. Since we use a Gaussian (with diago-
nal covariance matrix) for the likelihood function in Eq. (6.95), the BIC function for M0
is written as follows:
LBIC(M0) = log
T

t=1
N(ot|μML, ML) −λ2D
2 log T
= −T
2

D(1 + log(2π)) + log
--ML--
−λD log T,
(6.96)
where ML is the ML estimate of the diagonal covariance matrix:
μML =
T
t=1 ot
T
,
ML
d
=
T
t=1(otd −μML
d
)2
T
.
(6.97)
This result is obtained by assuming that we only have one state in Eq. (6.83).
Similarly, Mt is written as follows:
LBIC(Mt) = log
t
t′=1
N(ot′|μML
1,t , ML
1,t ) + log
T

t′=t+1
N(ot′|μML
2,t , ML
2,t ) −λ4D
2 log T
= −t
2

D(1 + log(2π)) + log
--ML
1,t
--
−T −t
2

D(1 + log(2π)) + log
--ML
2,t
--
−2λD log T
= −t
2 log
--ML
1,t
-- −T −t
2
log
--ML
2,t
-- −T
2 (D(1 + log(2π))) −2λD log T,
(6.98)

6.6 Speaker clustering/segmentation
239
where ML
1,t and ML
2,t are the ML estimates of the diagonal covariance matrices, and are
obtained as follows:
μML
1,t =
t
t′=1 ot′
t
,
ML
1,td =
t
t′=1(ot′d −μML
1,td)2
t
,
μML
2,t =
T
t′=t+1 ot′
t
,
ML
2,td =
T
t′=t+1(ot′d −μML
2,td)2
t
.
(6.99)
Again, this result is obtained by assuming that we only have two separated states
(M0 and Mt) in Eq. (6.83).
Therefore, the difference of BIC values between M0 and Mt is represented as follows:
LBIC(t) ≜LBIC(Mt) −LBIC(M0)
= −T
2 log
--ML-- + t
2 log
--ML
1,t
-- + T −t
2
log
--ML
2,t
-- −λD log T.
(6.100)
This is the objective function of ﬁnding the segmentation boundary based on the BIC
criterion. If the LBIC(t) is positive, t is a segmentation boundary, and the optimal
boundary ˆt is obtained by maximizing LBIC(t) as follows:
ˆt = arg max
t
LBIC(t) if LBIC(t) > 0.
(6.101)
Thus, we can ﬁnd the optimal boundary of two speech segments by using BIC. If
we consider multiple changing points based on BIC, we can use Algorithm 9. The
approach has also been extended to deal with prior distributions (Watanabe & Nakamura
2009).
Algorithm 9 Detecting multiple changing points
Require: interval [ta, tb] : ta = 1; tb = 2
1: while ta < T do
2:
Detect if there is one changing point in [ta, tb] via BIC
3:
if no change in [ta, tb] then
4:
tb = tb + 1
5:
else
6:
ta = ˆt + 1; tb = ta + 1
7:
end if
8: end while
6.6.2
Speaker clustering
Once we have the speech segments obtained by the BIC-based speech segmentation,
we can again use the BIC criterion to cluster the speech segments, where the cluster

240
Asymptotic approximation
obtained can be interpreted as a speciﬁc speaker. This can be obtained by similar
techniques to the decision tree clustering of context-dependent HMMs in Section 6.5.2.
The main difference between them is that the decision tree clustering is performed by
splitting the nodes from the root node by using the (phonetic) question, while the speaker
clustering starts to merge the leaf nodes, which are represented by a speech segment,
successively to build a tree.
Let i be a speech segment index obtained by speech segmentation techniques, and
we have in total J segments. Then, similarly to the previous sections, we can compute
the approximated Bayes factor based on the following difference of the BIC value when
merging the speech segments i and i′ as
LBIC
i,i′
≜LBIC
i,i′ −LBIC
i
+ LBIC
i′
,
(6.102)
where the BIC values of LBIC
i
and LBIC
i′
are represented as:
LBIC
i
= −Ti
2

D(1 + log(2π)) + log
--ML
i
--
−λD log Ti,
(6.103)
where Ti and ML
i
are the number of frames assigned to the speech segment i and the
ML estimate of the covariance matrix of the speech segment i, respectively. Similarly,
the BIC value of LBIC
i,i′ , which merges two nodes i and i′, is represented as
LBIC
i,i′
= −Ti,i′
2
(
D(1 + log(2π)) + log
---ML
i,i′
---
)
−λD log Ti,i′,
(6.104)
where Ti,i′ = Ti + Ti′, and ML
i,i′ is the ML estimate of the covariance matrix when we
represent the speech segments i and i′ as a single Gaussian.
The most appropriate combination of merging two nodes can be obtained by
considering all possible i and i′ combinations and selecting the combination
ˆi, ˆi′ = arg
max
1≤i≤J,i<i′<J LBIC
i,i′ .
(6.105)
And the selected ˆi and ˆi′ can be merged to a new node. This process is iteratively
performed while it satisﬁes LBIC
i,i′
> 0. The ﬁnal leaf nodes can represent a speaker
cluster. This approach is also called agglomerative clustering, and BIC based agglom-
erative clustering is actually used to build state-of-the-art speaker diarization systems
(Wooters & Huijbregts 2008), which provide the “who spoke when” information in
speech data.
6.7
Summary
This chapter describes the asymptotic approximation of the Bayesian approach, and
introduces the Laplace approximation and the BIC criterion. Both approaches are very
powerful for approximating the Bayesian inference of acoustic model parameters in
speech recognition and Bayesian inference of model selection problems in speech

6.7 Summary
241
clustering and segmentation. However, the asymptotic approximation makes the single
Gaussian assumption for the prior and posterior distributions and large sample assump-
tions for target data, which limits the application for speech and language processing.
The next chapter provides another approximation technique, variational Bayes, which
can handle the problem in the asymptotic approximation.

7
Variational Bayes
Variational Bayes (VB) was developed in the machine learning community in the 1990s
(Attias 1999, Jordan, Ghahramani, Jaakkola et al. 1999) and has now become a stan-
dard technique to approximated Bayesian inference for latent models, based on the
EM-like algorithm. In Chapter 4, we have also dealt with latent models based on the
maximum a-posteriori (MAP) EM algorithm. However, the MAP approximation uses
the point estimation of model parameters instead of the distribution estimation, which
is far from a true Bayesian manner of regarding all the variables introduced in our prob-
lem as probabilistic random variables. Another approximation based on the asymptotic
approximation in Chapter 6 assumes a complex posterior distribution as a single Gaus-
sian distribution without latent variables, which is not a true assumption for many of
our applications. The evidence approximation in Chapter 5 also does not explicitly deal
with latent models (can be obtained by combining MAP, VB, or MCMC). Instead of
considering the MAP, evidence, and asymptotic approximations, VB can efﬁciently
approximate complicated integrals and expectations over model parameters, based on
variational method within a speciﬁc family of distribution types (exponential family, as
discussed in Section 2.1.3). The key idea of the variational technique is to ﬁnd the lower
bound of the marginal log likelihood, similar to the EM algorithm in Section 3.4, and
obtain the posterior distributions directly based on the variational method.
This chapter starts to explain the general framework of VB in Section 7.1, and more
speciﬁc pattern recognition problems in Section 7.2. Then this chapter goes on to pro-
vide a VB version of the EM algorithm for statistical models and model selection in
speech and language processing, including speech recognition in Sections 7.3 and 7.4
and speaker veriﬁcation in Section 7.5. Sections 7.6 and 7.7 also deal with latent topic
models and their extensions; these try to capture long-range topic information from
(spoken) documents, based on VB solutions.
7.1
Variational inference in general
This section starts by describing a general latent model with observation data X =
{xn|n = 1, · · · , N}, and the set of all variables introduced in our model including latent
variables, parameters, hyperparameters, and model structure Z. The latter sections spec-
ify Z with more speciﬁc variables. The goal of Bayesian inference is to obtain posterior
distributions of any variables introduced in the problem, that is:
p(Z|X).
(7.1)

7.1 Variational inference in general
243
As discussed in Section 2.1.2, once we obtain p(Z|X), we can estimate various informa-
tion by the MAP or expectation procedure. In VB, we consider an arbitrary posterior
distribution q(Z|X). We use the approximated posterior distribution denoted by q(·)
to distinguish it from the true posterior distribution p(·). Then the problem is how to
obtain a q(Z|X) that is close to p(Z|X), so as to obtain a well-approximated posterior
distribution.
7.1.1
Joint posterior distribution
As a measure of evaluating the difference between two distributions, we use the
Kullback–Leibler divergence (Kullback & Leibler 1951), as introduced in the ML–EM
algorithm (Section 3.4). The Kullback–Leibler divergence between q(Z|X) and p(Z|X)
is deﬁned as follows:
KL(q(Z|X)∥p(Z|X)) ≜

q(Z|X) log q(Z|X)
p(Z|X)dZ.
(7.2)
Z can be a set of discrete variables or a set of both continuous and discrete variables, and,
strictly speaking, we should use the summation sumZ for discrete variables and

dZ for
continuous variables in such a case. However, for simplicity we use

dZ instead of
mixing integrals and summations in the following formulation.
The KL divergence (Eq. (7.2)) is represented as
KL(q(Z|X)∥p(Z|X)) =

q(Z|X) log q(Z|X)
p(X,Z)
p(X)
dZ
= log p(X) −

q(Z|X) log p(X, Z)
q(Z|X) dZ



≜F[q(Z|X)]
,
(7.3)
where
F[q(Z|X)] ≜

q(Z|X) log p(X, Z)
q(Z|X) dZ
(7.4)
is called the variational lower bound. The reason we call it the lower bound is that
F[q(Z|X)] is a lower bound of the evidence (marginal log likelihood) log p(X) because
of the non-negativity of the KL divergence, i.e.,
KL(q(Z|X)∥p(Z|X)) ≥0 ⇐⇒log p(X) ≥F[q(Z|X)].
(7.5)
Equation (7.3) means that we can obtain the optimal q(Z|X) by maximizing the varia-
tional lower bound F[q(Z|X)] that corresponds to minimizing the KL divergence since
the log evidence log p(X) does not depend on Z. That is
˜q(Z|X) = arg max
q(Z|X) F[q(Z|X)] ⇐⇒arg max
q(Z|X) KL(q(Z|X)∥p(Z|X)).
(7.6)
To obtain the optimal ˜q(Z|X), we use a variational method for a functional F[q(Z|X)],
which achieves mapping a function to a real (or complex) value, i.e., f →a ∈R.
Thus, the approach is called variational Bayes. The variational method is discussed in
Section 7.1.3.

244
Variational Bayes
7.1.2
Factorized posterior distribution
In practical applications, we need to consider the factorized form of the joint distribution
q(Z|X) to make the calculation simple. To do this, we consider the jth element of Z and
make the following conditional independence assumption:
q(Z|X) =
J
j=1
q(Zj|X).
(7.7)
J is the total number of elements. This is an essential approximation requirement of VB
to make the problem practical. Zj would be a subset of model parameters (e.g., Gaussian
mean vector μ and covariance matrix  in a particular component of GMM/CDHMM),
or an HMM state indicator st at frame t, for instance. Note that we do not assume the
factorization form for the true posterior p(Z|X). Instead, the true posterior p(Zi|X) can
be represented as the following marginalized distribution of p(Z|X) over all Zj except Zi:
p(Zi|X) =

· · ·

p(Z|X)
J
j̸=i
dZj ≜

p(Z|X)dZ\i,
(7.8)
where Z\i denotes the complementary set of Zi.
By using Eq. (7.8), the KL divergence between q(Zi|X) and p(Zi|X) (not the KL
divergence between the joint distributions in Eq. (7.2)) is represented as follows:
KL(q(Zi|X)∥p(Zi|X)) =

q(Zi|X) log
q(Zi|X)

p(Z|X)dZ\i
dZi
=

q(Zi|X) log
q(Zi|X)
 p(X,Z)
p(X) dZ\i
dZi
= log p(X) −

q(Zi|X) log

p(X, Z)dZ\i
q(Zi|X)
dZi.
(7.9)
Since Eq. (7.9) has integrals in the logarithmic function, it is difﬁcult to deal with.
Therefore, we use the following Jensen’s inequality for a concave function f, distri-
bution function p(Y) (

p(Y)dY = 1), and an arbitrary function g(Y) introduced in
Section 3.4.1:
f

p(Y)g(Y)dY

≥

p(Y)f(g(Y))dY.
(7.10)
In the special case of f(·) = log(·), Y = Z\i, p(Y) = q(Z\i|X), and g(Y) =
p(X,Z)
q(Z|X),
Eq. (7.10) can be rewritten as follows:
log

q(Z\i|X)p(X, Z)
q(Z|X) dZ\i

= log

p(X, Z)dZ\i
q(Zi|X)

≥

q(Z\i|X) log
p(X, Z)
q(Z|X)

dZ\i,
(7.11)
where we use Eq. (7.7) to cancel q(Z\i|X) in the fraction of the ﬁrst line. We also use the
following relationship, which is true when p(Y) ≥0:

7.1 Variational inference in general
245
a(Y) ≥b(Y) ∀Y ⇒

p(Y)a(Y)dY ≥

p(Y)b(Y)dY.
(7.12)
In the special case of
f(·) = log(·),
Y = Zi,
p(Y) = q(Zi|X),
a(Y) = log

p(X, Z)dZ\i
q(Zi|X)

,
b(Y) =

q(Z\i|X) log
p(X, Z)
q(Z|X)

dZ\i,
(7.13)
Eq. (7.12) is represented as

q(Zi|X) log

p(X, Z)dZ\i
q(Zi|X)

dZi
≥

q(Zi|X)

q(Z\i|X) log
p(X, Z)
q(Z|X)

dZ\idZi.
=

q(Z|X) log
p(X, Z)
q(Z|X)

dZ = F[q(Z|X)],
(7.14)
where we use Eq. (7.7) to obtain q(Z|X), and use the deﬁnition of the variational
lower bound in Eq. (7.4). Therefore, by substituting Eq. (7.14) into the KL divergence
Eq. (7.9), Eq. (7.9) is ﬁnally represented as follows:
KL(q(Zi|X)∥p(Zi|X)) ≤log p(X) −F[q(Z|X)].
(7.15)
Compared with Eq. (7.3) that is the equality relationship, Eq. (7.15) is the inequality
relationship. Equation (7.15) still has the nice property that the maximization of the
variational lower bound corresponds to reducing the KL divergence that then causes the
approximated posterior q(Zi|X) to approach the true posterior p(Zi|X). That is, if we
obtain the following posterior distribution:
˜q(Zi|X) = arg max
q(Zi|X) F[q(Z|X)],
(7.16)
˜q(Zi|X) could be a well-approximated posterior distribution in terms of reducing the KL
divergence between the true posterior p(Zi|X) and approximated posterior ˜q(Zi|X). In
this section, a tilde / is added to indicate variationally optimized values or functions.
However, the maximization of the posterior distribution in terms of the lower bound
F[q(Z|X)] does not directly correspond to minimization of the KL divergence, and we
cannot globally optimize q(Zi|X) in terms of the KL divergence when we use the lower
bound as the objective functional. This is a shortcoming of the factorization approxima-
tion in Eq. (7.7), but it enables us to obtain the posterior distribution of each variable
q(Zi|X), unlike the joint distribution q(Z|X), which is more practical. The next section
discusses how to optimize the approximated posterior by using the variational method.

246
Variational Bayes
7.1.3
Variational method
The variational method is based on functional differentiation, which is a technique
for obtaining an optimal function based on a variational calculation, and is deﬁned as
follows:
Continuous function case
δ
δg(y)H[g(x)] = lim
ϵ→0
H[g(x) + ϵδ(x −y)] −H[g(x)]
ε
,
(7.17)
where g(x) is a continuous function to be optimized, H[g(x)] is a functional of g(x) and
δ(x −y) is a Dirac delta function.
Discrete function case
δ
δgl
H[gn] = lim
ϵ→0
H[gn + ϵδ(n, l)] −H[gn]
ε
.
(7.18)
Similarly, gn is a discrete function to be optimized, and δ(n, l) is a Kronecker delta func-
tion. This section aims to obtain the following optimized posterior distribution based on
the above variational method:
˜q(Zi|X) = arg max
q(Zi|X) F[q(Z|X)]
= arg max
q(Zi|X)

q(Z|X) log
p(X, Z)
q(Z|X)

dZ.
(7.19)
For simplicity of the calculation, we simplify q(Zi|X) to q(Zi) in this section.
If we consider

q(Zi)dZi = 1 constraint, the functional differentiation is represented
by substituting F[q(Z)] and q(Zi) into H and g(y) in Eq. (7.17), respectively, as follows:
δ
δq(Z′
i)

F[q(Z)] + K

q(Zi)dZi −1

= lim
ϵ→0
1
ϵ
 
q(Zi) + ϵδ(Zi −Z′
i)

E(Z\i)
&
log
p(X, Z)

q(Zi) + ϵδ(Zi −Z′
i)

q(Z\i)
'
dZi
−F[q(Z)] + K
 
q(Zi) + ϵδ(Zi −Z′
i)

dZi −1

−K

q(Zi)dZi −1

,
(7.20)
where K is a Lagrange multiplier, as introduced in Section 3.4.3 for the function
derivative. We focus on the ﬁrst term in the brackets in the second line of Eq. (7.20).
The ﬁrst term is rewritten as follows:

7.1 Variational inference in general
247
 
q(Zi) + ϵδ(Zi −Z′
i)

E(Z\i)
&
log
p(X, Z)

q(Zi) + ϵδ(Zi −Z′
i)

q(Z\i)
'
dZi
=
 
q(Zi) + ϵδ(Zi −Z′
i)

E(Z\i)
&
log
p(X, Z)
q(Z) + q(Z)
q(Zi)ϵδ(Zi −Z′
i)
'
dZi
=
 
q(Zi) + ϵδ(Zi −Z′
i)

E(Z\i)

log p(X, Z)
q(Z)
−log

1 + ϵ δ(Zi −Z′
i)
q(Zi)

dZi.
(7.21)
By expanding the logarithmic term in Eq. (7.21) with respect to ϵ, Eq. (7.21) can be
represented thus:
Equation (7.21)
=
 
q(Zi) + ϵδ(Zi −Z′
i)
 
E(Z\i)

log p(X, Z)
q(Z)

−ϵ δ(Zi −Z′
i)
q(Zi)

dZi + o(ϵ2)
=

q(Zi)E(Z\i)

log p(X, Z)
q(Z)

dZi −ϵ

δ(Zi −Z′
i)dZi
+ ϵ

δ(Zi −Z′
i)E(Z\i)

log p(X, Z)
q(Z)

dZi + o(ϵ2)
= F[q(Z)] −ϵ + ϵE(Z\i)

log p(X, Z′)
q(Z′)

+ o(ϵ2)
= F[q(Z)] + ϵ

−1 + E(Z\i)

log p(X, Z′)
q(Z′)

+ o(ϵ2),
(7.22)
where o(ϵ2) denotes a set of terms of no less than the second power of ϵ. Z′ ≜{Z′
i, Z\i},
but in the following equations, we simply use Z instead of Z′, as we do not have
to distinguish them. Therefore, by substituting Eq. (7.22) into Eq. (7.20), it can be
represented as:
Equation (7.20)
= lim
ϵ→0
1
ϵ

ϵ

−1 + E(Z\i)

log p(X, Z)
q(Z)

+ K

+ o(ϵ2)

= −1 + E(Z\i)

log p(X, Z)
q(Z)

+ K
= −1 + E(Z\i)

log p(X, Z)

−E(Z\i)

log q(Z)

+ K
= −1 + E(Z\i)

log p(X, Z)

−log q(Zi) −E(Z\i)

log q(Z\i)

+ K.
(7.23)
We use Eq. (7.7) to factorize q(Zi) and q(Z\i). Therefore, the optimal posterior (VB
posterior)/q(Zi) satisﬁes the relation whereby Eq. (7.23) = 0, and is obtained as:
log/q(Zi) = −1 + E(Z\i)

log p(X, Z)

−E(Z\i)

log q(Z\i)

+ K.
(7.24)
Since only the second term in the right-hand-side depends on Zi, the optimal VB
posterior is ﬁnally derived as:
/q(Zi|X) ∝exp

E(Z\i|X)

log p(X, Z)

,
(7.25)

248
Variational Bayes
or by considering the normalization constant, it is derived as
/q(Zi|X) =
exp

E(Z\i|X)

log p(X, Z)


exp

E(Z\i|X)

log p(X, Z)

dZi
,
(7.26)
where the omitted notations are recovered (q(Zi) →q(Zi|X)). Thus, we obtain the gen-
eral form of the VB posterior distribution /q(Zi|X) by using the variational method.
Equation (7.25) tells us that if we want to infer some probabilistic variables, we ﬁrst
need to prepare the joint distribution of the observation X and target variables. Note that
/q(Zi|X) and the other posterior distributions /q(Z\i|X) = *J
j̸=i q(Zj|X) depend on each
other due to the expectation in Eq. (7.25). Therefore, this optimization can be performed
iteratively from the initial posterior distributions for all/q(Zi|X). The following sections
provide more practical forms of VB posteriors.
7.2
Variational inference for classiﬁcation problems
This section provides more speciﬁc formulations for our speech and language processing
issues which focus more on pattern classiﬁcation problems. Let O be a training data set
of feature vectors, and Z be a set of discrete latent variables. Then, with a ﬁxed model
structure M, posterior distributions for model parameters p((c)|O, M) and p(Z(c)|O, M)
given category c are expressed as follows:1
p((c)|O, M) =

Z
 p(O, Z|, M)p(|M)
p(O|M)
d(\c)
(7.27)
and
p(Z(c)|O, M) =

Z(\c)
 p(O, Z|, M)p(|M)
p(O|M)
d,
(7.28)
where p(|M) is a prior distribution for . Here, \c represents the set of all categories
without c. In this section, we can also regard the prior hyperparameter setting as the
model structure setting, and include its variations in index M. The posterior distributions
for the model structure p(M|O) are expressed as follows:
p(M|O) =

Z
 p(O, Z|, M)p(|M)p(M)
p(O)
d,
(7.29)
where p(M) denotes a prior distribution for model structure M.
These equations cannot be solved analytically, because the acoustic model for speech
recognition includes latent variables in HMMs and GMMs, as discussed in Section 3.2,
and the total number of model parameters amounts to more than one million. In addition,
these parameters depend on each other hierarchically. Solving all integrals and expecta-
tions numerically requires huge amounts of computation time. Therefore, when applying
1 It is reasonable to deal with the prior distribution p(|M) of model parameters given model M instead of
p(), since the actual functional form of model parameters is determined by model M. Conversely, it is
very difﬁcult to consider the prior distribution of model parameters p() without the model setting.

7.2 Variational inference for classiﬁcation problems
249
the Bayesian approach to acoustic modeling for speech recognition, an effective approx-
imation technique is necessary. Therefore, this section focuses on the VB approach
and derives general solutions for VB posterior distributions q(|O, M), q(Z|O, M), and
q(M|O) to approximate the corresponding true posteriors. To begin with, by following
the general VB formulation in Section 7.1.2, we assume that
q(, Z|O, M) =

c
q((c)|O(c), M)q(Z(c)|O(c), M),
p(, Z|O, M) =

c
p((c)|O(c), M)p(Z(c)|O(c), M).
(7.30)
This assumption means that probabilistic variables associated with each category are
statistically independent from other categories. In addition, these posterior distributions
depend on the model variable M, which is not marginalized. The speech data used are
assumed to be well transcribed and the label information is assumed to be reliable.
In addition, the frequently used feature extraction (e.g., MFCC) from the speech is
good enough for the statistical independence assumption of the observation data to be
guaranteed. Therefore, the assumption of class independence is reasonable.
7.2.1
VB posterior distributions for model parameters
This subsection discusses VB posterior distributions for model parameters with ﬁxed
model structure M. Initially, arbitrary posterior distribution q((c)|O, M) is intro-
duced, and the Kullback–Leibler (KL) divergence (Kullback & Leibler 1951) between
q((c)|O, M) and true posterior distribution p((c)|O, M) is considered:
KL(q((c)|O, M)∥p((c)|O, M)) =

q((c)|O, M) log q((c)|O, M)
p((c)|O, M)d(c).
(7.31)
Substituting Eq. (7.27) into Eq. (7.31), Eq. (7.31) is rewritten as follows:
KL(q((c)|O, M)∥p((c)|O, M))
=

q((c)|O, M) log
q((c)|O, M)

Z
 p(O,Z|,M)p(|M)
p(O|M)
d(\c) d(c)
= log p(O|M) −

q((c)|O, M)
× log

Z

p(O, Z|, M)p(|M)d(\c)
q((c)|O, M)
d(c).
(7.32)
Then applying the continuous Jensen’s inequality Eq. (7.10) to Eq. (7.32), the following
inequality is obtained:
KL(q((c)|O, M)∥p((c)|O, M))
≤log p(O|M) −

Z

q((c)|O, M)q((\c)|O, M)q(Z|O, M)
× log
p(O, Z|, M)p(|M)
q((c)|O, M)q((\c)|O, M)q(Z)d(c)d(\c)

250
Variational Bayes
= log p(O|M) −

Z

q(|O, M)q(Z|O, M)
× log
p(O, Z|, M)p(|M)
q(|O, M)q(Z|O, M)p(O|M)d.
(7.33)
From the third to the fourth line, we use the deﬁnition d(c)d(\c) ≡d and the relation
q((c)|O, M)q((\c)|O, M) = q(|O, M), which is derived from Eq. (7.30). Thus, we
ﬁnally obtain the following inequality:
KL(q((c)|O, M)∥p((c)|O, M))
≤log p(O|M) −FM[q(|O, M), q(Z|O, M)],
(7.34)
where
FM[q(|O, M), q(Z|O, M)]
≜

Z

q(|O, M)q(Z|O, M) log p(O, Z|, M)p(|M)
q(|O, M)q(Z|O, M)d
= E(,Z)

log p(O, Z|, M)p(|M)
q(|O, M)q(Z|O, M)

.
(7.35)
This corresponds to the variational lower bound, as discussed in Eq. (7.3). The inequality
(7.34) is strict unless q(|O, M) = p(|O, M) and q(Z|O, M) = p(Z|O, M) (i.e., the
arbitrary posterior distribution q is equivalent to the true posterior distribution p). From
the assumption Eq. (7.30), FM is decomposed into each category as follows:
FM[q(|O, M), q(Z|O, M)]
=

c
E((c),Z(c))

log p(O(c), Z(c)|(c), M)p((c)|M)
q((c)|O(c), M)q(Z(c)|O(c), M)

=

c
FM,(c)[q((c)|O(c), M), q(Z(c)|O(c), M)].
(7.36)
This indicates that the total objective function is calculated by summing all objective
functions for each category.
From inequality Eq. (7.34), q((c)|O, M) approaches p((c)|O, M) as the right-hand-
side decreases. Therefore, the optimal posterior distribution can be obtained by a
variational method. Since the term log p(O|M) can be disregarded, minimization is
changed to maximization of FM with respect to q((c)|O, M), and is given by the
following variational equation:
δ
δq((c)|O, M)FM[q(|O, M), q(Z|O, M)]
=
δ
δq((c)|O, M)FM,(c)[q((c)|O(c), M), q(Z(c)|O(c), M)] = 0.
(7.37)
From this equation, the optimal VB posterior distribution /q((c)|O, M) is obtained by
using the variational method as follows:
/q((c)|O, M) ∝p((c)|M) exp
(
E(Z(c))
+
log p(O(c), Z(c)|(c), M)
,)
.
(7.38)

7.2 Variational inference for classiﬁcation problems
251
This result can also be obtained by using the general formula of the variational posterior
in Eq. (7.25). By using the replacement Zi →(c), Eq. (7.25) can be rewritten as
follows:
/q((c)|O(c), M) ∝exp

E((\c),Z)

log p(O, , Z|M)

= exp

E((\c),Z)

log p(O, Z|, M)p(|M)

= exp
⎛
⎝
c′̸=c
E((c′),Z(c′))
+
log p(O(c′), Z(c′)|(c′), M)p((c′)|M)
,
⎞
⎠
× exp
(
E(Z(c))
+
log p(O(c), Z(c)|(c), M)p((c)|M)
,)
∝p((c)|M) exp
(
E(Z(c))
+
log p(O(c), Z(c)|(c), M)
,)
.
(7.39)
Here, we use the factorization property of the posterior distributions in Eq. (7.30). This
result means that the optimal posterior distribution of model parameters/q((c)|O(c), M)
is obtained by its prior distribution p((c)|M) and the expected complete data likelihood
p(O(c), Z(c)|(c), M).
7.2.2
VB posterior distributions for latent variables
A similar method is used for the optimal VB posterior distribution /q(Z(c)|O, M). An
inequality similar to Eq. (7.37) is obtained by considering the KL divergence between
the arbitrary posterior distribution q(Z(c)|O, M) and the true posterior distribution
p(Z(c)|O, M) as follows:
KL(q(Z(c)|O, M)∥p(Z(c)|O, M))
≤log p(O|M) −FM[q(|O, M), q(Z|O, M)].
(7.40)
The optimal VB posterior distribution/q(Z(c)|O, M) is also obtained by maximizing FM
with respect to q(Z(c)|O, M) with the variational method as follows:
/q(Z(c)|O, M) ∝exp
(
E((c))
+
log p(O(c), Z(c)|(c), M)
,)
.
(7.41)
This result is also obtained by using the general formula of the variational posterior in
Eq. (7.25). Compared with the result for /q((c)|O(c), M) in Eq. (7.38), Eq. (7.41) does
not need to prepare the prior distribution for Z.
7.2.3
VB–EM algorithm
Equations (7.38) and (7.41) are closed-form expressions, and these optimizations can be
effectively performed by iterative calculations analogous to the expectation and maxi-
mization (EM) algorithm (Dempster et al. 1976), as discussed in Sections 3.4 and 4.2,
which increases FM at every iteration up to a converged value. Then, Eqs. (7.38) and
(7.41), respectively, correspond to the maximization step (M-step) and the expectation

252
Variational Bayes
Table 7.1 Training speciﬁcations for ML and VB.
Training
Min-max optimization
Objective function
ML
ML–EM
differential method
Q function
VB
VB–EM
variational method
FM functional
step (E-step) in the VB approach. We call this algorithm the variational Bayes expecta-
tion and maximization (VB–EM) algorithm. Therefore, by substituting q into /q, these
equations can be represented as follows:

/q((c)|O, M) ∝p((c)|M) exp

E(Z(c))

log p(O(c), Z(c)|(c), M)

,
/q(Z(c)|O, M) ∝exp

E((c))

log p(O(c), Z(c)|(c), M)

.
(7.42)
Note that optimal posterior distributions for a particular category can be obtained simply
by using the category’s variables, i.e., we are not concerned with the other categories in
the calculation, since Eq. (7.42) only depends on category c, which is based on the
assumption given by Eq. (7.30).
Finally, to compare the VB approach with the conventional ML approach for training
latent variable models, the training speciﬁcations for ML and VB are summarized in
Table 7.1.
7.2.4
VB posterior distribution for model structure
The VB posterior distributions for a model structure are derived in the same way as in
Section 7.2.1, and model selection is carried out employing the posterior distribution.
Arbitrary posterior distribution q(M|O) is introduced and the KL divergence between
q(M|O) and the true posterior distribution p(M|O) is considered:
KL(q(M|O)∥p(M|O)) =

M
q(M|O) log q(M|O)
p(M|O).
(7.43)
Substituting Eq. (7.29) into Eq. (7.43) and using Jensen’s inequality, the inequality of
Eq. (7.43) can be obtained as follows:
KL (q(M|O)∥p(M|O))
≤log p(O) + E(M)

log q(M|O)
p(M)
−FM[q(|O, M), q(Z|O, M)]

.
(7.44)
Similarly to the discussion in Section 7.2.1, from the inequality Eq. (7.44), q(M|O)
approaches p(M|O) as the right-hand-side decreases.
Compared with the posterior distributions of model parameters and latent variables,
we cannot use the formula Eq. (7.25), since it is not practical to marginalize all possible
model structures M. Therefore, the optimal posterior distribution for a model structure
can again be obtained by a variational method, as explained in Section 7.1.3. If we
consider the constraint 
M q(M|O) = 1, the functional differentiation is represented by
substituting respectively FM and q(M|O) into H and gn in Eq. (7.18) as follows:

7.2 Variational inference for classiﬁcation problems
253
δ
δq(M′|O)

E(M)

log q(M|O)
p(M)
−FM

+ K

M
q(M|O) −1

= lim
ϵ→0
1
ϵ

M
(q(M|O) + ϵδMM′)

log q(M|O) + ϵδMM′
p(M)
−FM

−E(M)

log q(M|O)
p(M)
−FM

+K

M
q(M|O) + ϵδMM′ −1

−K

M
q(M|O) −1

,
(7.45)
where K is a Lagrange multiplier. We focus on the ﬁrst term in the brackets in the 2nd
line of Eq. (7.45). This term can be rewritten as follows:

M
(q(M|O) + ϵδMM′)

log q(M|O) + ϵδMM′
p(M)
−FM

=

M
(q(M|O) + ϵδMM′)

log q(M|O)
p(M)
+ log

1 + ϵ δMM′
q(M|O)

−FM

.
(7.46)
By expanding the logarithmic term in Eq. (7.46) with respect to ϵ, Eq. (7.46) is
represented as:
Equation (7.46)
= E(M)

log q(M|O)
p(M)
−FM

+ ϵ

log q(M′|O)
p(M′)
−FM′ + 1

+ o(ϵ2).
(7.47)
Therefore, by substituting Eq. (7.47) into Eq. (7.45), Eq. (7.45) is represented as:
Equation (7.45) = log q(M′|O)
p(M′)
−FM′ + 1 + K.
(7.48)
Therefore, the optimal posterior (VB posterior) /q(M|O) satisﬁes the relation whereby
Eq. (7.48) = 0, and is obtained as:
log /q(M|O)
p(M)
−FM + 1 + K = 0.
(7.49)
By disregarding the normalization constant, the optimal VB posterior is ﬁnally
derived as:
/q(M|O) ∝p(M) exp

FM[q(|O, M), q(Z|O, M)]

.
(7.50)
Compared with Eqs. (7.38) and (7.41), the posterior obtained is represented by the total
variational lower bound.
Assuming that p(M) is a uniform distribution,2 the proportional relation between
/q(M|O) and FM is obtained as follows, based on the convexity of the logarithmic
function:
FM′ ≥FM ⇔/q(M′|O) ≥/q(M|O).
(7.51)
2 We can set an informative prior distribution for p(M) instead of the uniform distribution. Several prior
distributions for model structure are considered in Chapter 8.

254
Variational Bayes
Therefore, an optimal model structure in the sense of maximum posterior probability
estimation can be selected as follows:
/
M = arg max
M /q(M|O) = arg max
M FM.
(7.52)
This indicates that by maximizing total FM with respect to both q(|O, M), q(Z|O, M),
and M, we can obtain the optimal parameter distributions and select the optimal model
structure simultaneously (Attias 1999, Ueda & Ghahramani 2002).
Thus, we analytically derive the variational posterior distributions of general latent
models. The next section applies these solutions to the continuous density hidden
Markov model (CDHMM), as we apply ML–EM and MAP–EM to CDHMMs in
Sections 3.4 and 4.3, respectively.
7.3
Continuous density hidden Markov model
This section reformulates the CDHMM training for speech processing based on the
VB framework (Valente & Wellekens 2003, Somervuo 2004, Watanabe et al. 2004).
The four formulations are obtained by using the VB framework to perform acoustic
model construction (model setting, training, and selection) and speech classiﬁcation
consistently, based on the Bayesian approach. Consequently, the conventional formu-
lations based on the ML and MAP approaches in Sections 3.4 and 4.3 are replaced by
formulations based on the Bayesian approach as follows:
• Set generative model distributions
→Set generative model distributions and prior distributions (Section 7.3.1 and
7.3.2);
• ML/MAP Baum–Welch algorithm
→VB Baum–Welch algorithm (Section 7.3.3);
• Log likelihood
→VB objective function (Section 7.3.4);
• ML/MAP-based classiﬁcation
→VB–BPC (Section 7.3.5).
These four formulations are explained in the following four subsections, by applying
the acoustic model for speech recognition to the general solution in Section 7.2.
7.3.1
Generative model
Similarly to the MAP–EM algorithm in Section 4.3, setting of the emission and prior
distributions is required when calculating the VB posterior distributions. This section
provides these distributions for CDHMM again, to provide the VB-based analytical
solutions.

7.3 Continuous density hidden Markov model
255
Let O = {ot ∈RD|t = 1, . . . , T} be a sequential speech data set for a speech seg-
ment of a phoneme category. Since the formulations for the posterior distributions are
common to all phoneme categories, the phoneme category index c is omitted from this
section to simplify the equation forms. D is used to denote the dimension number of the
feature vector and T to denote the frame number. The complete data likelihood func-
tion with speech, HMM state, and GMM component sequences (O, S, and V), which is
introduced in Eqs. (3.44) and (4.23), is expressed by
p(O, S, V|, M) =
T

t=1
ast−1stωstvtp(ot|stvt, M),
(7.53)
where as0s1 = πs1.3 Although we have many segments for each phoneme category
and the generative model distribution must consider the product of each segment
in Eq. (7.53), this is also omitted in this book. Here, S and V are sets of discrete
latent variables, which are the concrete forms of Z in Section 7.2. The parame-
ter aij denotes the state transition probability from state i to state j, and ωjk is
the kth weight factor of the Gaussian mixture for state j. In addition, p(ot|jk)(=
N(ot|μjk, jk)) denotes the Gaussian with mean vector μjk and covariance matrix jk
deﬁned as:
N(ot|μjk, jk) ≜(2π)−D
2 |jk|−1
2 exp

−1
2(ot −μjk)⊺−1
jk (ot −μjk)

.
(7.54)
 = {aij, ωjk, μjk, −1
jk |i, j = 1, . . . , J, k = 1, . . . , K} is a set of model parameters.
Here, J denotes the number of states in an HMM sequence and K denotes the number
of Gaussian components in a state. This section only considers the diagonal covariance
matrix case.
7.3.2
Prior distribution
Conjugate distributions, which are based on the exponential function, are as easy to
use as prior distributions since the function forms of prior and posterior distributions
become the same (Berger 1985, Gauvain & Lee 1994, Bernardo & Smith 2009), as
discussed in Sections 2.1.3 and 4.3.3. Then a distribution is selected where the proba-
bilistic variable constraint is the same as that of the model parameter. The state transition
probability aij and the mixture weight factor ωjk have the constraint that 
j aij = 1 and

k ωjk = 1. Therefore, the Dirichlet distributions for πj, aij, and ωjk are used, where the
variables of the Dirichlet distribution satisfy the above constraint. Similarly, the diago-
nal elements of the inverse covariance matrix −1
jk are always positive, and the gamma
distribution is used. The range of the mean vector μjk is from −∞to ∞, and the mul-
tivariate Gaussian distribution is used. Thus, as introduced in Eqs. (4.29) and (4.32),
the prior distribution for a CDHMM with a diagonal covariance matrix is expressed as
follows:
3 This section does not explicitly provide the posterior solution of the initial weight, as it is trivial.

256
Variational Bayes
p(|M) ≜
J
i=1
p({aij′}J
j′=1|M)
J
j=1
p({ωjk′}K
k′=1|M)
K

k=1
p(μjk, jk|M)
≜
J
i=1
Dir({aij′}J
j′=1|{φa
ij′}J
j′=1)
J
j=1
Dir({ωjk′}K
k′=1|{φω
jk′}K
k′=1)
×
K

k=1
D

d=1
N(μjkd|μ0
jkd, (φμ
jkrjkd)−1)Gam2(rjkd|φr
jk, r0
jkd).
(7.55)
Here, 0 ≜{φa
ij, φω
jk, φμ
jk, μ0
jkd, φr
jk, r0
jkd|i, j = 1, . . . , J, k = 1, . . . , K, d = 1, · · · , D}
is a set of prior parameters. In Eq. (7.55), Dir(·) denotes a Dirichlet distribution and
Gam2(·) denotes a gamma distribution. (It is different from the conventional deﬁnition
of the gamma distribution, see Appendix C.11.) If the covariance matrix elements are
off the diagonal, a Gaussian–Wishart distribution is used as the prior distribution of μjk
and jk. The explicit forms of the distributions are deﬁned as follows (Appendixes C.4,
C.5, and C.11):
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
Dir({aij}J
j=1|{φa
ij}J
j=1)
≜CDir({φa
ij}J
j=1) *J
j=1(aij)φa
ij−1,
Dir({ωjk}K
k=1|{φω
jk}K
k=1)
≜CDir({φω
jk}K
k=1) *K
k=1(ωjk)φω
jk−1,
N(μjkd|μ0
jkd, (φμ
jkrjkd)−1)
≜CN (φμ
jk)(rjkd)
1
2 exp

−
φμ
jkrjkd(μjkd−μ0
jkd)2
2

,
Gam2(rjkd|φr
jk, r0
jkd)
≜CGam2(φr
jk, r0
jkd)(rjkd)
φr
jk
2 −1 exp

−
r0
jkdrjkd
2

,
(7.56)
where the normalization constants are deﬁned as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
CDir({φa
ij}J
j=1)
≜
(J
j=1 φa
ij)
*J
j=1 (φa
ij) ,
CDir({φω
jk}K
k=1)
≜
(K
k=1 φω
jk)
*K
k=1 (φω
jk) ,
CN (φμ
jk)
≜

φμ
jk
2π
 1
2
,
CGam2(φr
jk, r0
jkd)
≜

r0
jkd
2 2
 φr
jk
2

 φr
jk
2
 .
(7.57)
In the Bayesian approach, an important problem is how to set the prior parameters. Here,
two kinds of prior parameters of μ0 and r0 are set using sufﬁcient amounts of data from:
• Statistics of higher hierarchy acoustic models for the acoustic model construction
task;
• Statistics of speaker independent models for the speaker adaptation task.
The other parameters (φa, φω, φμ, and φr) have a meaning as regarding tuning the
balance between the values obtained from training data and the above statistics.
These parameters are set appropriately based on experiments, as discussed in speaker
adaptation (Section 4.4).

7.3 Continuous density hidden Markov model
257
Finally, Algorithm 10 provides a generative process for a CDHMM with prior distri-
bution. For simplicity, the initial weight, the hyperparameters, and the model structure
are given in this generative process, but it can also be sampled from some distribu-
tions. Compared with Algorithm 3, CDHMM parameters are also sampled from the
prior distributions.
Algorithm 10 Generative process for continuous density hidden Markov model with
prior distributions
Require: , M, and {πj}J
j=1
1: for i, j = 1, · · · , J do
2:
Draw aij from Dir({aij}J
j=1|{φa
ij}J
j=1)
3: end for
4: for j = 1, · · · , J do
5:
for k = 1, · · · , K do
6:
Draw ωjk from Dir({ωjk}K
k=1|{φω
jk}K
k=1)
7:
for d = 1, · · · , D do
8:
Draw rjkd from Gam2(rjkd|φr
jk, r0
jkd)
9:
Draw μjkd from N(μjkd|μ0
jkd, (φμ
jkrjkd)−1)
10:
end for
11:
end for
12: end for
13: Draw s1 from Mult(s1|{πj}J
j=1)
14: Draw v1 from Mult(v1|{ωs1k}K
k=1)
15: Draw o1 from N(o1|μs1v1, s1v1)
16: for t = 2, · · · , T do
17:
Draw st from Mult(st|{ast−1j}J
j=1)
18:
Draw vt from Mult(vt|{ωstk}K
k=1)
19:
Draw ot from N(ot|μstvt, stvt)
20: end for
7.3.3
VB Baum–Welch algorithm
This subsection introduces concrete forms of the VB posterior distributions for model
parameters q(|O, M) and for latent variables q(Z|O, M) in acoustic modeling, which
are efﬁciently computed by VB iterative calculations within the VB framework. This
calculation is effectively carried out by the VB Baum–Welch algorithm (MacKay 1997).
VB M-step
First, the VB M-step for acoustic model training is explained. This is solved by
substituting the acoustic model setting in Section 7.3.1 into the general solution for
the VB M-step in Section 7.2. From Eq. (7.42), the VB posterior distributions for the
model parameters are represented as follows:

258
Variational Bayes
/q(|O, M) ∝p(|M) exp

E(S,V)

log p(O, S, V|, M)

.
(7.58)
Taking the logarithmic operation, Eq. (7.58) is represented as:
log/q(|O, M) ∝log p(|M) + E(S,V)

log p(O, S, V|, M)

≜˜Q().
(7.59)
Here, ˜Q() is a VB auxiliary function deﬁned as follows:
˜Q() ≜E(S,V)

log p(O, S, V|, M)

+ log p(|M)
=

S,V
˜q(S, V|O, M) log p(O, S, V|, M) + log p(|M).
(7.60)
On the other hand, the MAP auxiliary function in Eq. (4.16) is deﬁned as follows:
QMAP(′|) ≜

S,V
p(S, V|O, , M) log p(O, S, V|′, M) + log p(′|M).
(7.61)
Comparing the VB and MAP auxiliary functions, these are almost equivalent except the
posterior distributions of latent variables, i.e., ˜q(S, V|O, M) vs. p(S, V|O, , M). Since
˜q(S, V|O, M) is obtained by marginalizing , ˜Q() is more appropriate in terms of the
Bayesian treatment.
Therefore, this VB-M step is solved by using the result of the MAP-M step solution,
except that ˜q(S, V|O, M) is obtained by using the VB-E step. The calculated results
for the optimal VB posterior distributions for the model parameters are summarized as
follows:
/q(|M) ≜
J
i=1
/q({aij′}J
j′=1|M)
J
j=1
/q({ωjk′}K
k′=1|M)
K

k=1
/q(μjk, jk|M)
≜
J
i=1
Dir({aij′}J
j′=1|{/φa
ij′}J
j′=1)
J
j=1
Dir({ωjk′}K
k′=1|{/φω
jk′}K
k′=1)
×
K

k=1
D

d=1
N(μjkd|/μjkd, (/φμ
jkrjkd)−1)Gam2(rjkd|/φr
jk,/rjkd).
(7.62)
The concrete forms of the distributions are deﬁned as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
Dir({aij}J
j=1|{/φa
ij}J
j=1)
≜CDir({/φa
ij}J
j=1) *J
j=1(aij)
/φa
ij−1,
Dir({ωjk}K
k=1|{/φω
jk}K
k=1)
≜CDir({/φω
jk}K
k=1) *K
k=1(ωjk)
/φω
jk−1,
N(μjkd|/μjkd, (/φμ
jkrjkd)−1)
≜CN (/φμ
jk)(rjkd)
1
2 exp

−
/φμ
jkrjkd(μjkd−/μjkd)2
2

,
Gam2(rjkd|/φr
jk,/rjkd)
≜CGam2(/φr
jk,/rjkd)(rjkd)
/φr
jk
2 −1 exp
(
−/rjkdrjkd
2
)
,
(7.63)

7.3 Continuous density hidden Markov model
259
where the normalization constants are:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
CDir({/φa
ij}J
j=1)
≜
(J
j=1 /φa
ij)
*J
j=1 (/φa
ij) ,
CDir({/φω
jk}K
k=1)
≜
(K
k=1 /φω
jk)
*K
k=1 (/φω
jk) ,
CN (/φμ
jk)
≜
 /φμ
jk
2π
 1
2
,
CGam2(/φr
jk,/rjkd)
≜
(/rjkd
2
)
/φr
jk
2

 /φr
jk
2
 .
(7.64)
Note that Eqs. (7.55) and (7.62) are members of the same function family, and the only
difference is that the set of prior parameters 0 in Eq. (7.55) is replaced with a set of
posterior distribution parameters / in Eq. (7.62), where / is deﬁned as:
/ ≜{/φa
ij, /φω
jk, /φμ
jk, /μjkd, /φr
jk,/rjkd
|i, j = 1, . . . , J, k = 1, . . . , K, d = 1, · · · , D}.
(7.65)
The conjugate prior distribution is adopted because the posterior distribution is theoret-
ically a member of the same function family as the prior distribution, and is obtained
analytically, which is a characteristic of the exponential distribution family, as discussed
in Section 2.1.4. Here, / values are calculated from:
/φa
ij = φa
ij +/ξij,
/φω
jk = φω
jk + /γjk,
/φμ
jk = φμ
jk + /γjk,
/μjkd =
φμ
jkμ0
jkd + γ (1)
jkd
φμ
jk + /γjk
,
/φr
jk = φr
jk + /γjk,
/rjkd = /γ (2)
jkd + φμ
jk(μ0
jkd)2 −/φμ
jk(/μjkd)2 + r0
jkd.
(7.66)
/ξij denotes the sufﬁcient statistics of the transition matrix, and /γjk, /γ (1)
jkd , and /γ (2)
jkd denote
zeroth-, ﬁrst-, and second-order sufﬁcient statistics of the GMM, respectively, and are
deﬁned as follows:
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
/ξij
≜
T−1
t=1 /ξt(i, j),
/γjk
≜
T
t=1 /γt(j, k),
/γ (1)
jkd
≜
T
t=1 /γt(j, k)otd,
/γ (2)
jkd
≜
T
t=1 /γt(j, k)(otd)2.
(7.67)

260
Variational Bayes
These sufﬁcient statistics / ≜{/ξij, /γjk, /γ (1)
jkd , /γ (2)
jkd |i, j = 1, . . . , J, k = 1, . . . , K, d =
1, · · · , D} are computed by using/ξt(i, j) and /γt(j, k), deﬁned as follows:
. /ξt(i, j)
≜
/q(st = i, st+1 = j|O, M),
/γt(j, k)
≜
/q(st = j, vt = k|O, M).
(7.68)
Here, /ξt(i, j) is a VB transition posterior distribution, which denotes the transition prob-
ability from a state i to a state j at a frame t, and /γt(j, k) is a VB occupation posterior
distribution, which denotes the occupation probability of a mixture component k in a
state j at a frame t, in the VB approach. These are similar to those deﬁned in Eq. (3.119)
by using the ML–EM algorithm of a CDHMM. Therefore, / can be calculated from
0,/ξt(i, j), and /γt(j, k), enabling/q(|O, M) to be obtained.
VB E-step
Before we focus on the calculation of /ξt(i, j) and /γt(j, k), we ﬁrst focus on the posterior
distribution of the joint distribution of the HMM state and mixture component sequences
/q(S, V|, M). From Eq. (7.25),/q(S, V|, M) is represented as follows:
/q(S, V|O, M) = /q(O, S, V|M)
/q(O|M)
∝exp

E()

log p(O, S, V|, M)

.
(7.69)
Since /q(O|M) does not depend on S and V, we ﬁnd that the complete data likelihood
marginalized by the model parameter  can also be obtained by considering the same
expectation with Eq. (7.69):
/q(O, S, V|M) ∝exp

E()

log p(O, S, V|, M)

.
(7.70)
Once we obtain the complete data likelihood/q(O, S, V|M), we can compute the posterior
probabilities /ξt(i, j) and /γt(j, k) by using the forward–backward algorithm, similarly to
Section 3.3. Therefore, we focus on how to compute the following expectation of the
complete data log likelihood:
E()

log p(O, S, V|, M)

.
(7.71)
By substituting Eq. (7.53) into Eq. (7.71), Eq. (7.71) can be represented as follows:
E()

log p(O, S, V|, M)

= E()
&
log
T

t=1
ast−1stωstvtN(ot|μstvt, stvt)
'
=
T

t=1
E()

log(ast−1st) + log(ωstvt) + log(N(ot|μstvt, stvt))

=
T

t=1
E()

log(ast−1st)

+ E()

log(ωstvt)

+ E()

log(N(ot|μstvt, stvt))

.
(7.72)

7.3 Continuous density hidden Markov model
261
Now we focus on the case when st−1 = i, st = j, and vt = k, where we need to compute
the following equations:
log/aij ≜E(aij)

log(aij)

,
log /ωjk ≜E(ωjk)

log(ωjk)

,
log/bjk(ot) ≜E(μjk,jk)

log(N(ot|μjk, jk))

.
(7.73)
We can also deﬁne the following function based on Eq. (7.73):
˜u(O, S, V|M) ≜
T

t=1
/ast−1st/ωstvt/bstvt(ot).
(7.74)
This equation behaves similarly to the likelihood function of p(O, S, V|, M) in
Eq. (7.53). Note that ˜u(O, S, V|M) is not properly normalized, and cannot be dealt with
as a probabilistic distribution. However, from Eq. (7.70), ˜u(O, S, V|M) is proportional to
˜q(O, S, V|M), and this function has the following relationship from Eq. (7.69):
˜q(O, S, V|M) = ˜q(O|M)˜q(S, V|O, M)
=
˜q(O|M)

S′,V′ ˜u(O, S′, V′|M) ˜u(O, S, V|M)
≜
T

t=1
Ca/ast−1stCωb/ωstvt/bstvt(ot),
(7.75)
where Ca and Cωb are normalization constants of/aij and /ωjk/bjk(ot) respectively for each
frame, and satisfy the following condition:
˜q(O|M)

S′,V′ ˜u(O, S′, V′|M) = (CaCωb)T.
(7.76)
Note that it is not easy to obtain the normalization factors Ca and Cωb explicitly, since
it requires ˜q(O|M) and 
S′,V′ ˜u(O, S′, V′|M). However, it will be shown later that the
calculation of the occupation probabilities does not require computation of the normal-
ization factors explicitly, but only requires /aij, /ωjk, and /bjk. Therefore, we can compute
various values from Eq. (7.74) (e.g., the forward and backward variables and the occupa-
tion probabilities), as discussed in Section 3.3. Thus, the following paragraphs provide
the analytical solutions of/aij, /ωjk, and/bjk in detail.
State transition/aij
First, the integral over aij is solved from Eq. (7.63) by using a partial integral technique
and a normalization constant:
log/aij =

/q({aij′}J
j′=1|M) log aij
J
j′=1
daij′
= CDir({/φa
ij′}J
j′=1)

log aij
J
j′=1
(aij′)
/φa
ij′−1daij′.
(7.77)

262
Variational Bayes
Then we use the following derivative formula:
∂
∂/φa
ij
(aij)
/φa
ij−1 = (log aij)(aij)
/φa
ij−1.
(7.78)
By substituting Eq. (7.78) into Eq. (7.77), Eq. (7.77) can be rewritten as:
log/aij = CDir({/φa
ij′}J
j′=1)

∂
∂/φa
ij
(aij)
/φa
ij−1daij

J
j′̸=j
(aij′)
/φa
ij′−1daij′
= CDir({/φa
ij′}J
j′=1) ∂
∂/φa
ij

J
j′=1
(aij′)
/φa
ij′−1daij′
= CDir({/φa
ij′}J
j′=1) ∂
∂/φa
ij
1
CDir({/φa
ij′}J
j′=1),
(7.79)
where we replace the derivative and integral, and the integral can be performed to derive
the inverse of the normalization constant of the Dirichlet distribution.
From Eq. (7.64), this derivative can be calculated as follows:
∂
∂/φa
ij
1
CDir({/φa
ij′}J
j′=1)
=
∂
∂/φa
ij
*J
j′=1 (/φa
ij′)
(J
j′=1 /φa
ij′)
=

∂
∂/φa
ij
(/φa
ij)
 *J
j′̸=j (/φa
ij′)(J
j′=1 /φa
ij′) −*J
j′=1 (/φa
ij′)

∂
∂/φa
ij
(J
j′=1 /φa
ij′)

(
(J
j′=1 /φa
ij′)
)2
=
(/φa
ij) *J
j′=1 (/φa
ij′)(J
j′=1 /φa
ij′) −*J
j′=1 (/φa
ij′)(J
j′=1 /φa
ij′)(J
j′=1 /φa
ij′)
(
(J
j′=1 /φa
ij′)
)2
=
*J
j′=1 (/φa
ij′)
(
(/φa
ij) −(J
j′=1 /φa
ij′)
)
(J
j′=1 /φa
ij′)
=
1
CDir({/φa
ij′}J
j′=1)
⎛
⎝(/φa
ij) −(
J

j′=1
/φa
ij′)
⎞
⎠,
(7.80)
where (y) is a di-gamma function, which ﬁrst appeared in Eq. (5.82), and is deﬁned as
(y) ≜∂
∂y log (y) =
∂
∂y(y)
(y) .
(7.81)
Thus,/aij is ﬁnally obtained as follows:
log/aij = (/φa
ij) −(
J

j′=1
/φa
ij′).
(7.82)

7.3 Continuous density hidden Markov model
263
Mixture weight /ωjk
In a way similar to that used for/aij, the integral over ωjk is solved from Eq. (7.63), and
/ωjk is obtained as follows:
log /ωjk = (/φω
jk) −(
K

k′=1
/φω
jk′).
(7.83)
Gaussian distribution/bjk(ot)
First, log/bjk(ot) can be factorized for each dimension:
log/bjk(ot) = E(μjk,jk)

log(N(ot|μjk, jk))

= E(μjk,jk)
&
log
 D

d=1
N(otd|μjkd, jkd)
'
=
D

d=1
E(μjkd,jkd)

log

N(otd|μjkd, jkd)

.
(7.84)
Therefore, we focus on calculation of the d element. Since the calculation is more com-
plicated than the two previous calculations, the indexes j, k, t, and d are removed to
simplify the derivation. By using (7.63), log/b(o) can be rewritten as follows:
log/b(o)
=

N(μ|/μ, (/φμr)−1)Gam2(r|/φr,/r)
×

−1
2
(
log(2π) −log(r) + r(o −μ)2)
dμdr.
(7.85)
Now we focus on the integral over mean parameter μ. To calculate the integral, we ﬁrst
rewrite the part that is related to μ as follows:

N(μ|/μ, (/φμr)−1)r(o −μ)2dμ
= r

N(μ|/μ, (/φμr)−1)(o −μ + /μ −/μ)2dμ
= r

N(μ|/μ, (/φμr)−1)
(
(μ −/μ)2 + (o −/μ)2 −2(μ −/μ)(o −/μ)
)
dμ.
(7.86)
The integral of the above terms can be analytically solved. We ﬁrst consider the
following partial derivative:
∂
∂/φμr exp

−1
2(μ −/μ)2/φμr

= −1
2(μ −/μ)2 exp

−1
2(μ −/μ)2/φμr

.
(7.87)
Therefore, the ﬁrst integral can be represented as:

264
Variational Bayes

N(μ|/μ, (/φμr)−1)(μ −/μ)2dμ
= (2π)−1
2 (/φμr)
1
2

exp

−1
2(μ −/μ)2/φμr

(μ −/μ)2dμ
= (2π)−1
2 (/φμr)
1
2 (−2)

∂
∂/φμr exp

−1
2(μ −/μ)2/φμr

dμ.
(7.88)
By replacing the integral with the partial derivative, we can solve the integral as:

N(μ|/μ, (/φμr)−1)(μ −/μ)2dμ
= (−2)(2π)−1
2 (/φμr)
1
2
∂
∂/φμr

exp

−1
2(μ −/μ)2/φμr

dμ
= (−2)(2π)−1
2 (/φμr)
1
2
∂
∂/φμr(2π)
1
2 (/φμr)−1
2
= (−2)(/φμr)
1
2

−1
2

(/φμr)−3
2 = (/φμr)−1.
(7.89)
The other two integrals are trivially solved as follows:

N(μ|/μ, (/φμr)−1)(o −/μ)2dμ = (o −/μ)2,

N(μ|/μ, (/φμr)−1)(μ −/μ)(o −/μ)dμ = 0.
(7.90)
Therefore, Eq. (7.86) is solved as:

N(μ|/μ, (/φμr)−1)r(o −μ)2dμ
= r
(
(o −/μ)2 + (/φμr)−1)
= r(o −/μ)2 + 1
/φμ .
(7.91)
Now, we focus on the integral over r, because the integral without log(r) can be
easily computed by the result of the mean value of the gamma distribution in
Appendix C.11 as:
log/b(o)
=

Gam2(r|/φr,/r)

−1
2

log(2π) −log(r) + r(o −/μ)2 + 1
/φμ

dr
= −1
2

log(2π) +
/φr
/r (o −/μ)2 + 1
/φμ

+ 1
2

Gam2(r|/φr,/r) log(r)dr.
(7.92)
Therefore, we focus on the ﬁnal term. From Eqs. (7.63) and (7.64), the concrete form of
the gamma distribution, Gam2(·), is deﬁned as follows:
Gam2(r|/φr,/r) = CGam2(/φr,/r)(r)
/φr
2 −1 exp

−/rr
2

,
(7.93)
where
CGam2(/φr,/r) =
/r
2
 /φr
2

( /φr
2
).
(7.94)

7.3 Continuous density hidden Markov model
265
Similarly to the Dirichlet and Gaussian distributions, we consider the following
derivative:
∂
∂/φr (r)
/φr
2 −1 = 1
2(r)
/φr
2 −1 log(r).
(7.95)
Therefore, the integral is solved by using this relationship as:

Gam2(r|/φr,/r ) log(r)dr
= CGam2(/φr,/r )

(r)
/φr
2 −1 exp

−/rr
2

log(r)dr
= CGam2(/φr,/r )

2 ∂
∂/φr (r)
/φr
2 −1 exp

−/rr
2

dr
= 2CGam2(/φr,/r ) ∂
∂/φr
1
CGam2(/φr,/r).
(7.96)
The derivative with respect to /φr is calculated as follows:
∂
∂/φr


/φr
2

/r
2
 /φr
2
=
1
2
∂
∂/φr
2

( /φr
2
) /r
2
 /φr
2 + 1
2 log
/r
2
 /r
2
 /φr
2 
( /φr
2
)
/r
2
/φr
=
1
2
( /φr
2
)

( /φr
2
)
+ 1
2 log
/r
2


( /φr
2
)
/r
2
 /φr
2
,
(7.97)
where (·) is a di-gamma function deﬁned in Eq. (7.81). Therefore,
2CGam2(/φr,/r) ∂
∂/φr
1
CGam2(/φr,/r ) = 
/φr
2

+ log
/r
2

.
(7.98)
Thus, ﬁnally log/b(o) is obtained analytically as follows:
log/b(o)
= −1
2

Gam2(r|/φr,/r)

log(2π) + 1
/φμ −log(r) + r(o −/μ)2

dr
= −1
2

log(2π) + 1
/φμ −
/φr
2

−1
2

log
/r
2

+ (o −/μ)2 /φr
/r

.
(7.99)
Reverting to the indexes k, j, t, and d, log/bjk(ot) is represented as
log/bjk(ot) = −D
2

log(2π) + 1
/φμ
jk
−
/φr
jk
2

−1
2
D

d=1

log
/rjk
2

+
/φr
jk(otd −/μjkd)2
/rjkd

.
(7.100)

266
Variational Bayes
Thus, we obtain/aij, /ωjk and/bjk(ot), which are summarized as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
/aij
≜exp

(/φa
ij) −(
j′ /φa
ij′)

,
/ωjk
≜exp

(/φω
jk) −(
k′ /φω
jk′)

,
/bjk(ot)
≜exp

−D
2

log(2π) +
1
/φμ
jk −
 /φr
jk
2

−1
2
D
d=1

log
(/rjk
2
)
+
/φr
jk(otd−/μjkd)2
/rjkd

.
(7.101)
These variables are used to compute the VB transition probability /ξt(i, j) and VB
occupation probability /γt(j, k).
VB transition probability/ξt(i, j) and occupation probability /γt(j, k) (VB E-step)
From Eq. (7.25), VB transition probability/ξt(i, j) is represented as:
/ξt(i, j) ≜/q(st = i, st+1 = j|O, M).
(7.102)
Section 3.4.2 shows an efﬁcient computation of the transition probability based on the
complete data likelihood p(O, S, V|). Here we also consider how to obtain it based on
the VB version of the complete data likelihood ˜q(O, S, V|M), as introduced in Eq. (7.69).
However, from Eq. (7.74), /aij, /ωjk, and /bjk(ot) can only compute the unnormalized
likelihood function ˜u(O, S, V|M), that is
˜u(O, S, V|M) =
T

t=1
/ast−1st/ωstvt/bstvt(ot).
(7.103)
Therefore, as we discussed in Section 3.4.2, from the dependency of the HMM, we can
represent/q(O, st = i, st+1 = j|M) as follows:
/q(st = i, st+1 = j, O|M)
= /q(o1, · · · , ot, st = i|M)



=/αt(i)
/q(ot+1|st+1 = j, M)



=Cωb
K
k=1 /ωjk/bjk(ot+1)
/q(ot+2, · · · , oT|st+1 = j, M)



=/βt+1(j)
×/q(st+1 = j|st = i, M)



=Ca/aij
.
(7.104)
Here, /αt(i) is a forward variable at frame t in state i, as introduced in Eq. (3.50). Simi-
larly, /βt+1(j) is a backward variable at frame t + 1 in state j, as introduced in Eq. (3.55).
The forward and backward variables based on the VB formulation are represented as
described below.
First, the VB forward variable /αt(j) is computed by using the following equation:
• Initialization
/α1(j) = /q(o1, s1 = j|M)
= /q(o1|s1 = j, M)/q(s1 = j|M)
= Ca/ajCωb
K

k=1
/ωjk/bjk(o1),
1 ≤j ≤J.
(7.105)

7.3 Continuous density hidden Markov model
267
Then, unnormalized forward variable ˜˜α1(j) is deﬁned as:
˜˜α1(j) ≜/aj
K

k=1
/ωjk/bjk(o1)
= /α1(j)
CaCωb
.
(7.106)
• Induction
/αt(j) = /q(o1, · · · , ot, st = j|M)
= /q(ot|st = j, M)
J

i=1
/q(st = j|st−1 = i, M)/q(o1, · · · , ot−1, st−1 = i|M)
=

Ca
J

i=1
/αt−1(i)/aij

Cωb
K

k=1
/ωjk/bjk(ot)
=

Ca(CaCωb)t−1
J

i=1
˜˜αt−1(i)/aij

Cωb
K

k=1
/ωjk/bjk(ot)
= (CaCωb)t
 J

i=1
˜˜αt−1(i)/aij
 K

k=1
/ωjk/bjk(ot),
2 ≤t ≤T
1 ≤j ≤J,
(7.107)
where the unnormalized forward variable ˜˜αt(j) is represented as
˜˜αt(j) =
 J

i=1
˜˜αt−1(i)/aij
 K

k=1
/ωjk/bjk(ot),
2 ≤t ≤T
1 ≤j ≤J. .
(7.108)
• Termination
/q(O|M) =
J

j=1
/αT(j)
= (CaCωb)T
J

j=1
˜˜αT(j).
(7.109)
The VB forward variable ˜αt(j) is obtained with the unnormalized forward variable ˜˜αt(j)
and normalization constants Ca and Cωb. From this algorithm, we can compute the
unnormalized forward variable ˜˜αt(j) similarly to the original forward algorithm, but we
should be careful that the unnormalized forward variable is not a probability, and prob-
abilistic calculation (sum and product rules etc.) must be performed via the normalized
VB forward variable ˜αt(j).
Similarly, the VB backward variable /βt(j) is computed by using the following
equations:
• Initialization
/βT(j) = 1,
1 ≤j ≤J.
(7.110)

268
Variational Bayes
• Induction
/βt(i) = /q(ot+1, · · · , oT|st = i, M)
=
J

j=1
/q(ot+2, · · · , oT|st+1 = j, M)/q(ot+1|st+1 = j, M)/q(st+1 = j|st = i, M)
=
J

j=1
Ca/aij
K

k=1
Cωb/ωjk/bjk(ot+1)/βt+1(j)
= (CaCωb)T−t
J

j=1
/aij
K

k=1
/ωjk/bjk(ot+1) ˜˜βt+1(j),
t = T −1, T −2, · · · , 1,
1 ≤i ≤J,
(7.111)
where the unnormalized backward variable ˜˜βt(i) is represented as
˜˜βt(i) =
J

j=1
/aij
K

k=1
/ωjk/bjk(ot+1) ˜˜βt+1(j).
(7.112)
• Termination
β0 ≜/q(O|M)
=
J

j=1
/aj
K

k=1
/ωjk/bjk(o1)/β1(j)
= (CaCωb)T
J

j=1
/aj
K

k=1
/ωjk/bjk(o1) ˜˜β1(j).
(7.113)
Therefore, based on the VB forward and backward variables, we can compute the
posterior probabilities as follows:
/ξt(i, j)
=
/αt(i)/aij
(K
k=1 /ωjk/bjk(ot)
)
/βt+1(j)
J
i′=1
J
j′=1 /αt(i′)/ai′j′
(K
k=1 /ωj′k/bj′k(ot)
)
/βt+1(j′)
=
(CaCωb)t ˜˜αt(i)Ca/aijCωb
(K
k=1 /ωjk/bjk(ot)
)
(CaCωb)T−t−1 ˜˜βt+1(j)
J
i′=1
J
j′=1(CaCωb)t ˜˜αt(i′)Ca/ai′j′Cωb
(K
k=1 /ωj′k/bj′k(ot)
)
(CaCωb)T−t−1 ˜˜βt+1(j′)
=
˜˜αt(i)/aij
(K
k=1 /ωjk/bjk(ot)
) ˜˜βt+1(j)
J
i′=1
J
j′=1 ˜˜αt(i′)/ai′j′
(K
k=1 /ωj′k/bj′k(ot)
) ˜˜βt+1(j′)
.
(7.114)
Thus, we can compute the transition probability with unnormalized VB variables/aij, /ωjk,
and /bjk(ot), and unnormalized forward and backward variables ˜˜αt(i) and ˜˜βt+1(j), where
the normalization constants Ca and Cωb are canceled out. This is based on a well-known

7.3 Continuous density hidden Markov model
269
scaling property of the HMM forward–backward algorithm (Rabiner & Juang 1993).
Similarly the occupation probability is also calculated as:
/γt(j, k) =
˜˜αt(j) ˜˜βt(j)
J
j′=1 ˜˜αt(j′) ˜˜βt(j′)
·
/ωjk/bjk(ot)
K
k′=1 /ωjk′/bjk′(ot)
.
(7.115)
These probabilities are obtained similarly to the ML cases in Eqs. (3.126) and (3.122),
and the MAP case in Eqs. (4.91) and (4.92) with the VB-based variables obtained by
Eq. (7.101). Thus, /ξt(i, j) and /γt(j, k) are calculated efﬁciently by using a probabilistic
assignment via the familiar forward–backward algorithm. This algorithm is called the
VB forward–backward algorithm.
Similarly to the VB forward–backward algorithm, the Viterbi algorithm is also
derived within the VB approach by exchanging the summation over i for the maxi-
mization over i in the calculation of the unnormalized forward probability ˜˜αt(j). This
algorithm is called the VB Viterbi algorithm.
Thus, VB posteriors can be calculated iteratively in the same way as the Baum–Welch
algorithm, even for a complicated sequential model that includes latent variables such
as HMM and GMM for acoustic models. These calculations are referred to as a VB
Baum–Welch algorithm, as proposed in MacKay (1997), Watanabe et al. (2002), Beal
(2003) and Watanabe et al. (2004).
7.3.4
Variational lower bound
This section discusses the VB objective function FM for a whole acoustic model
topology, i.e., the variational lower bound, and provides general calculation results.
The variational lower bound is a criterion for both posterior distribution estimation,
and model topology optimization in acoustic model construction. This section begins
by focusing on one phoneme category. By substituting the VB posterior distribution
obtained in Section 7.3.3, we obtain analytical results for FM, and therefore, this calcu-
lation also requires a VB iterative calculation based on the VB Baum–Welch algorithm
used in the VB posterior calculation. We can separate FM into two components: one is
composed solely of/q(S, V|O, M), whereas the other is mainly composed of/q(|O, M).
Therefore, we deﬁne FM
 and FM
S,V, and represent FM as follows:
FM = E(,S,V)

log p(O, S, V|, M)p(|M)
/q(|O, M)

−E(S,V)

log/q(S, V|O, M)

= FM
 −FM
S,V,
(7.116)
where
FM
 ≜E(,S,V)

log p(O, S, V|, M)p(|M)
/q(|O, M)

,
FM
S,V ≜E(S,V)

log/q(S, V|O, M)

.
(7.117)

270
Variational Bayes
First, we focus on FM
 . Based on the variational solution of /q(S, V|O, M) in Eq. (7.42),
FM
 is rewritten as follows:
FM
 ≜E(,S,V)
&
log
p(O, S, V|, M)p(|M)
1
Z p(|M) exp

E(S,V)

log p(O, S, V|, M)

'
= E(,S,V)

log p(O, S, V|, M)

+ E()

log p(|M)

−E(,S,V)

log p(O, S, V|, M)

−E()

log p(|M)

+ log Z
= log Z,
(7.118)
where Z is a normalization constant. This equation means that FM
 is represented by the
logarithmic function of the normalization constant Z. By using the deﬁnition of the VB
auxiliary function ˜q() in Eq. (7.60), Z can be rewritten as
Z ≜

p(|M) exp

E(S,V)

log p(O, S, V|, M)

d
=

exp
(
˜Q()
)
d.
(7.119)
Here, from the similarity of the VB and MAP auxiliary functions, as discussed in
Section 7.3.3, ˜Q() can be obtained by using the analytical results of the MAP aux-
iliary function. From Eq. (4.38), ˜Q() is decomposed into the following auxiliary
functions:
˜Q() = ˜Q(A) + ˜Q(ω) + ˜Q(μ, ),
(7.120)
where ˜Q(A), ˜Q(ω), and ˜Q(μ, ) are obtained from the analytical solutions of the
corresponding MAP auxiliary functions in Eqs. (4.51), (4.54), and (4.73), as follows:
˜Q(A) =
J

i=1
log
(
Dir({aij}J
j=1|{ ˜φa
ij}J
j=1)
)
+
J

i=1
log
CDir({φa
ij}J
j=1)
CDir({ ˜φa
ij}J
j=1)
,
(7.121)
˜Q(ω) =
J

j=1
log
(
Dir({ωjk}K
k=1|{ ˜φω
jk}K
k=1)
)
+
J

j=1
log
CDir({φω
jk}K
k=1)
CDir({ ˜φω
jk}K
k=1)
,
(7.122)
˜Q(μ, R)
=
J

j=1
K

k=1
log
(
N(μjk| ˜μjk, ( ˜φμ
jkRjk)−1)W(Rjk| ˜Rjk, ˜φR
jk)
)
+
J

j=1
K

k=1

−
T

t=1
˜γt(j, k)D
2
log(2π) + D
2 log φμ
˜φμ + log
CW(R0
jk, φR
jk)
CW( ˜Rjk, ˜φR
jk)

.
(7.123)
If we consider the diagonal covariance matrix, Eq. (7.123) is modiﬁed by using the
gamma distribution as follows:

7.3 Continuous density hidden Markov model
271
˜Q(μ, R)
=
J

j=1
K

k=1
D

d=1
log
(
N(μjkd| ˜μjkd, ( ˜φμ
jkrjkd)−1)Gam2(rjkd|˜rjkd, ˜φr
jk)
)
+
J

j=1
K

k=1

−
T

t=1
˜γt(j, k)D
2
log(2π) + D
2 log
φμ
jk
˜φμ
jk
+ log
CGam2(r0
jkd, φr
jk)
CGam2(˜rjkd, ˜φr
jk)

.
(7.124)
Here CGam2 in Eq. (7.124) and CDir in Eqs. (7.121) and (7.122) are normalization
constants of Gamma and Dirichlet distributions, respectively, and are deﬁned as follows:
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
CDir({φj}J
j=1)
=

(J
j=1 φj
)
*J
j=1 (φj) ,
CGam2(φ, r0)
=
(
r0
2
) φ
2

(
φ
2
) .
(7.125)
Therefore, by substituting Eqs. (7.121), (7.122), and (7.124) into Eq. (7.119), and by
using the deﬁnition of the normalization constant of the Dirichlet and gamma distribu-
tions in Eq. (7.125), the integral in Z is performed with the normalization of , and Z is
simply obtained as follows:
FM
 = log Z
= log
⎛
⎝
 
i,j
exp
(
˜Q(A)
)
daij
⎞
⎠+ log
⎛
⎝
 
j,k
exp
(
˜Q(ω)
)
dωjk
⎞
⎠
+ log
⎛
⎝
 
j,k
exp
(
˜Q(μ, R)
)
dμjkdRjk
⎞
⎠
=

i
log
(
j φa
ij)
(
j′ /φa
ij)
*
j (/φa
ij)
*
j (φa
ij) +

j
log
(
k φω
jk)
(
k /φω
jk)
*
k (/φω
jk)
*
k (φω
jk)
+

j,k
log
⎛
⎜⎜⎜⎜⎜⎝
(2π)−
/γjkD
2

φμ
jk
/φμ
jk
 D
2


 /φr
jk
2
D *
d

r0
jkd
2
 φr
jk
2
(

( φr
jk
2
))D *
d
(/rjkd
2
)
/φr
jk
2
⎞
⎟⎟⎟⎟⎟⎠
.
(7.126)
From Eq. (7.126), FM
 can be calculated by using the statistics of the posterior distribu-
tion parameters / given in Eq. (7.66). This part is equivalent to the objective function
for model selection based on Akaike’s Bayesian information criterion (Akaike 1980).
The whole FM for all categories is obtained by simply summing up the FM results
obtained in this section for all categories as in Eq. (7.36).
Now we focus on FM
S,V. From the deﬁnition in Eq. (7.117), −FM
S,V can be represented
as follows:

272
Variational Bayes
−FM
S,V = −E(S,V)

log/q(S, V|O, M)

= −

S,V
/q(S, V|O, M) log/q(S, V|O, M).
(7.127)
Therefore, FM
S,V denotes the entropy of the posterior distribution /q(S, V|O, M). As we
discussed in Section 7.69, it is difﬁcult to obtain the analytical form of/q(S, V|O, M) due
to the normalization constant, and direct calculation of the above entropy is also difﬁcult.
Instead, we focus on the variational complete data likelihood form/q(O, S, V|M), which
is obtained by the Bayes theorem as follows:
/q(S, V|O, M) =
/q(O, S, V|M)

S,V /q(O, S, V|M).
(7.128)
Based on the discussion in Eq. (7.73),/q(O, S, V|M) is represented with the unnormalized
function/u(O, S, V|M) as follows:
/q(O, S, V|M) = C/u(O, S, V|M)
= C
T

t=1
˜ast−1st ˜ωstvt ˜bstvt(ot),
(7.129)
where C is a normalization constant, deﬁned as follows:
C =
 
S,V
/u(O, S, V|M)dO.
(7.130)
˜aij, ˜ωjk, ˜bjk(ot) are analytically calculated in the VB-E step (Eq. (7.101)). Therefore, by
substituting (7.129) into (7.128), the normalization constant is canceled out, and we can
obtain the following equation:
/q(S, V|O, M) =
C/u(O, S, V|M)

S′,V′ C/u(O, S′, V′|M)
=
/u(O, S, V|M)

S′,V′/u(O, S′, V′|M).
(7.131)
Therefore, FM
S,V can be rewritten as follows:
FM
S,V =

S,V
/q(S, V|O, M) log/q(S, V|O, M)
=

S,V
/q(S, V|O, M) log (/u(O, S, V|M))
−log
⎛
⎝
S,V
/u(O, S, V|M)
⎞
⎠.
(7.132)
Note that the second term corresponds to the summation of all possible S and V
for unnormalized function /u(O, S, V|M), which can be computed in the VB forward
algorithm in Eq. (7.109) as follows:

7.3 Continuous density hidden Markov model
273

S,V
/u(O, S, V|M) =
J

j=1
˜˜αT(j).
(7.133)
Now we focus on the ﬁrst term in Eq. (7.132). From the discussions in Section 3.4.2, we
can convert summation over sequences S, V to a summation over HMM states i and j,
and mixture component k in this term. Therefore, this term is represented as follows:

S,V
/q(S, V|O, M) log (/u(O, S, V|M))
=

S,V
/q(S, V|O, M)
T

t=1
(
log ˜ast−1st + log ˜ωstvt + log ˜bstvt(ot)
)
=

i,j,t
˜ξt(i, j) log ˜aij +

j,k,t
˜γt(j, k)
(
log ˜ωjk + log ˜bjk(ot)
)
.
(7.134)
Thus, we obtain the term without computing the summation over S and V.
Finally, we summarize calculation of FM
S,V by using the deﬁnitions of ˜aij, ˜ωjk, ˜bjk(ot)
in Eq. (7.101), as follows:
FM
S,V =

i,j
/ξij
(

(
/φa
ij
)
−
(
j′ /φa
ij′
))
+

j,k
/γjk
(

(
/φω
jk
)
−
(
k′ /φω
jk′
))
−1
2

j,k
/γjk

D

log(2π) + 1
/φμ
jk
−
/φr
jk
2

+

d
log/rjkd
2

−1
2

j,k
⎛
⎝/φr
jk

t,d
/γt(j, k)(otd −/μjkd)2
/rjkd
⎞
⎠−log
⎛
⎝
j
//αT(j)
⎞
⎠.
(7.135)
Thus, we also obtain the analytical result for FM
S,V, which corresponds to the latent
variable effect for the variational lower bound.
The analytical result for the variational lower bound FM for the CDHMM is deter-
mined using FM
 in Eq. (7.126) and FM
S,V in Eq. (7.135). Although the analytical result
looks complicated, all variables are already computed in the VB expectation and max-
imization steps. We also want to emphasize that the computation is quite feasible since
it is carried out without a summation over all possible latent variable sequences S and
V. The variational lower bound is derived analytically so that it retains the effects of the
dependence between model parameters and of the latent variables, deﬁned in the gen-
erative model distribution in Eq. (7.53), unlike the conventional Bayesian information
criterion and minimum description length (BIC/MDL) approaches, as discussed in Sec-
tion 6.5. Therefore, the variational lower bound can compare any acoustic models with
respect to all topological aspects and their combinations, e.g., contextual and temporal
topologies in HMMs, the number of components per GMM in an HMM state, and the
dimensional size of feature vectors, based on the following equation:
/
M = arg
max
M∈(T×S×G×D) FM.
(7.136)

274
Variational Bayes
Here T, S, G, and D denote search spaces of HMM-temporal, HMM-contextual, GMM
and feature vector topologies, respectively.
Based on the discussion in Section 7.3, the seven steps in Algorithm 11 provide a
VB training algorithm for acoustic modeling. Here, τ denotes an iteration count, and ε
denotes a threshold that checks whether FM converges. Thus, the posterior distribution
estimation in the VB framework can be effectively constructed based on the VB Baum–
Welch algorithm, which is analogous to the ML Baum–Welch algorithm (Algorithm
4). In addition, VB can realize the model selection using the VB objective function as
shown in Step 9. Thus, VB can construct an acoustic model consistently based on the
Bayesian approach.
Algorithm 11 Variational Bayesian Baum–Welch algorithm for CDHMMs with model
selection
Require: Set posterior parameter /[τ = 0] from initialized transition probability
/ξ[τ = 0], occupation probability /γ [τ = 0], and model structure M (prior parameter
0 is included) for each category
1: repeat
2:
Compute/a[τ + 1], /ω[τ + 1], and/b(O)[τ + 1] using /[τ]. (By Eq. (7.101))
3:
Update /ξ[τ + 1] and /γ [τ + 1] via the Viterbi algorithm or forward–backward
algorithm. (By Eqs. (7.114) and (7.115))
4:
Accumulate the sufﬁcient statistics/ξ[τ + 1], /γ [τ + 1] ˜γ (1)[τ + 1], ˜γ (2)[τ + 1] (by
Eq. (7.67)
5:
Compute /[τ + 1] using /[τ + 1] and 0. (By Eq. (7.66))
6:
Calculate total FM[τ + 1] for all categories. (By using Eqs. (7.126) and (7.135)
and summing up all categories’ FM)
7:
Calculate  = |(FM[τ + 1] −FM[τ])/FM[τ + 1]|, τ ←τ + 1
8: until  ≤ε
Calculate FM for all possible M and ﬁnd /
M(= arg maxM FM)
Note that if we change /→5 (a value with 5 attached indicates an ML estimate),
 → and FM →ŁM (where ŁM means the log-likelihood for a model M), this
algorithm becomes an ML-based framework, except for the model selection. Therefore,
in the implementation phase, the VB framework can be realized in the conventional
systems of acoustic model construction by adding the prior distribution setting and by
changing the estimation procedure and objective function calculation.
7.3.5
VB posterior for Bayesian predictive classiﬁcation
This subsection deals with the Bayes decision rule based on the VB approach. It is
related to the Bayesian predictive classiﬁcation, as discussed in Section 6.3.1 with the
Laplace approximation, but this section deals with the same issue with VB. In this

7.3 Continuous density hidden Markov model
275
section, we use the following notation to clearly distinguish the training and recognition
data, similar to Section 6.3.1:
O : future data,
O : training data.
(7.137)
In recognition, O = {Ot ∈RD|t = 1, · · · , T} denotes the feature vector sequence
of input speech, and S = {st ∈{1, · · · , J}|t = 1, · · · , T} denotes the corresponding
HMM state sequence. Although our target application is ASR, which outputs the word
sequence W, this section simpliﬁes the decoding rule for the explanation. That is, the
decoding needs to handle the word sequence W in addition to the state sequence S, but it
can be combined with the LVCSR decoder if we can build the Viterbi algorithm. There-
fore, this section focuses on formulating the Viterbi algorithm within the VB framework,
similarly to that within the ML framework, as discussed in Section 3.3.2.
The Viterbi algorithm can achieve the optimal state sequence/S by using a conditional
probability function p(S|O, O) given input data O and training data O, as follows:
¯S = arg max
S
p(S|O, O) = arg max
S
p(O, S|O)
p(O|O)
= arg max
S
p(O, S|O).
(7.138)
p(O, S|O) is a variant of predictive distribution (Berger 1985, Bernardo & Smith 2009),
because this distribution predicts the probability of unknown data O conditioned by
training data O. Note that Eq. (7.138) does not depend on parameters  and model M,
and these can be explicitly involved by considering the following sum rule:
p(O, S|O) =

M

p(O, S|, O, M)p(|O, M)p(M|O)d.
(7.139)
This predictive distribution based approach involves considering the integrals and true
posterior distributions, an approach which is also applied to speech recognition (Huo
& Lee 2000, Jiang et al. 1999, Lee & Huo 2000, Chien & Liao 2001), as discussed in
Section 6.3.1, with the Laplace approximation.
After VB-based acoustic modeling in Section 7.3.4, an appropriate model structure /
M
is selected based on the VB model selection Eq. (7.136), and the optimal VB posterior
distributions are obtained /q(|O, /
M). Therefore, the true posterior distributions can be
approximated by the VB posteriors, and Eq. (7.139) is approximated as:
p(O, S|O) ≈

M

p(O, S|, M)/q(|O, M)δ(M, /
M)d
=

p(O, S|, /
M)/q(|O, /
M)d
= E()

p(O, S|, /
M)

.
(7.140)

276
Variational Bayes
Thus, we can build the Viterbi algorithm for the expectation over the model parameter 
by using the VB posterior. In the following section we omit the models structure index
/
M for simplicity.
Similarly to Section 3.3.2, we ﬁrst deﬁne the following expected highest probability
along a single path, at time t, which accounts for the ﬁrst t observations and ends in
state j:
/δt(j) ≜
max
s1,··· ,st−1 E()

p(s1, · · · , st = j, o1, · · · , ot|)

.
(7.141)
By using /δt(j) recursively, we can obtain the most probable state sequence as follows:
• Initialization
/δ1(i) = E(π) [πi]
K

k=1
E(ω) [ωik] E(μ,)

N(o1|μik, ik)

,
ψ1(i) = 0,
1 ≤i ≤J.
(7.142)
• Recursion
/δt(j) =

max
1≤i≤J
/δt−1(i)E(A)

aij
 K

k=1
E(ω) [ωik] E(μ,)

N(o1|μjk, jk)

,
ψt(j) =

arg max
1≤i≤J
/δt−1(i)E(A)

aij

,
2 ≤t ≤T
1 ≤j ≤J.
(7.143)
• Termination
p(/S, O|O) = max
1≤j≤J
/δT(i),
/sT = arg max
1≤j≤J
/δT(i).
(7.144)
• State sequence backtracking
/st = ψt+1(/st+1),
t = T −1, T −2, · · · , 1.
(7.145)
Thus, we can perform the Viterbi algorithm for the predictive distribution based on
the VB posteriors. To realize the Viterbi algorithm, we need to consider the following
expectation:
E(A)

aij

,
E(ω) [ωik] ,
E(μ,)

N(ot|μjk, jk)

.
(7.146)
We provide the solution for each of the expected variables. Note that these are different
from the expected variables of the CDHMM parameters in the VB E-step, as discussed
in Eq. (7.73),4 i.e.,
4 Again we omit the initial transition parameters for simplicity.

7.3 Continuous density hidden Markov model
277
/aij = exp

E(aij)

log(aij)

,
/ωjk = exp

E(ωjk)

log(ωjk)

,
/bjk(ot) = exp
(
E(μjk,jk)

log(N(ot|μjk, jk))
)
.
(7.147)
This is because the VB E-step is a training step, and we need to optimize these values
based on the variational method, which necessitates considering the expectation of the
parameters in the logarithmic domain. In the prediction case, since we already have the
posterior distributions in the training step, Eq. (7.146) simply performs the expectation
determination for the CDHMM parameters directly.
Expected state transition E(A)

aij

We ﬁrst focus on calculation of the expected state transition /aij. Although this can be
obtained as the mean result of the Dirichlet distribution in Appendix C.4, we provide the
derivation for its educational value. Based on the deﬁnition of the Dirichlet distribution
in Eq. (7.62), we can obtain the following equation:
E(A)

aij

=

aijDir({aij′}J
j′=1|{/φa
ij′}J
j′=1)
J
j′=1
daij′
= CDir({/φa
ij}J
j=1)

aij
J
j′=1
(aij′)
/φa
ij′−1daij′.
(7.148)
Now we deﬁne the following variable:
ˆφa
ij′ ≜
/φa
ij′ + 1
j′ = j
/φa
ij′
j′ ̸= j.
(7.149)
By using ˆφa
ij′, the integral in Eq. (7.148) is solved as:
E(A)

aij

= CDir({/φa
ij}J
j=1)

J
j′=1
(aij′)
ˆφa
ij′−1daij′
=
CDir({/φa
ij}J
j=1)
CDir({ ˆφa
ij}J
j=1)
.
(7.150)
The normalization constant of the Dirichlet distribution is deﬁned (Appendix C.4) as
CDir({φj}J
j=1) ≜
(J
j=1 φj)
*J
j=1 (φj)
.
(7.151)
Therefore, by substituting the concrete form of the normalization constant into
Eq. (7.150) and by using the deﬁnition of ˆφa
ij′, Eq. (7.150) can be represented as follows:

278
Variational Bayes
E(A)

aij

=
*J
j′=1 ( ˆφa
ij′)
*J
j′=1 (/φa
ij′)
(J
j′=1 /φa
ij′)
(J
j′=1 ˆφa
ij′)
=
*J
j′̸=j (/φa
ij′)
*J
j′̸=j (/φa
ij′)
(/φa
ij + 1)
(/φa
ij)
(J
j′=1 /φa
ij′)
(1 + J
j′=1 /φa
ij′)
=
(/φa
ij + 1)
(/φa
ij)
(J
j′=1 /φa
ij′)
(1 + J
j′=1 /φa
ij′)
.
(7.152)
Finally, we use the following formula for the gamma function:
(x + 1) = x(x).
(7.153)
Then, Eq. (7.152) is analytically obtained as the following simple equation:
E(A)

aij

=
/φa
ij(/φa
ij)
(/φa
ij)
(J
j′=1 /φa
ij′)
J
j′=1 /φa
ij′(J
j′=1 /φa
ij′)
=
/φa
ij
J
j′=1 /φa
ij′
.
(7.154)
Note that the state transition probability is obtained from the normalized weight, which
is proportional to the posterior hyperparameter /φa
ij, and the result is very intuitive.
Expected mixture weight E(ω)

ωjk

Since the mixture weight ωjk is represented by a multinomial distribution, it is simi-
lar to the state transition aij. Similarly to E(A)

aij

, the expected state transition /ωjk is
calculated as follows:
E(ω)

ωjk

=
/φω
jk
K
k′=1 /φω
jk′
.
(7.155)
Again, the mixture weight probability is obtained using the normalized weight of the
posterior hyperparameter /φω
jk.
Expected Gaussian distribution E(μ,)

N (ot|μjk, jk)

Finally, we calculate the expected value of the Gaussian distribution with VB posteriors
for Gaussian parameters. First the expectation is factorized for each dimension when we
use the diagonal covariance as follows:
E(μ,)

N(ot|μjk, jk)

=
D

d=1
E(μjkd,rjkd)
+
N(otd|μjkd, (rjkd)−1)
,
.
(7.156)
The indexes of state ij, mixture component k, frame t, and dimension d are removed to
simplify the derivation.

7.3 Continuous density hidden Markov model
279
Based on the deﬁnition of the Gaussian and gamma distributions in Eq. (7.62), we
can obtain the following equation:
E(μ,r)
+
N(o|μ, r−1)
,
=

N(μ|/μ, (/φμr)−1)Gam2(r|/φr,/r )N(o|μ, r−1)dμdr
= CN (/φμ)CGam2(/φr,/r )CN
×

r
1
2 exp

−
/φμr(μ −/μ)2
2

r
/φr
2 −1 exp

−/rr
2

r
1
2 exp

−r(o −μ)2
2

dμdr
∝

r
/φr
2 exp

−/rr
2

exp

−
/φμr(μ −/μ)2
2

exp

−r(o −μ)2
2

dμdr.
(7.157)
First, we focus on the integration with respect to μ, and completing the square with
respect to μ. Then, by integrating with respect to μ, and arranging the equation, the
following equation is obtained:

exp
(
−r
2
(
(o −μ)2 + /φμ(μ −/μ)2))
dμ
=

exp

−r
2

(1 + /φμ)

μ −o + /φμ/μ
1 + /φμ
2
−(o + /φμ/μ)2
1 + /φμ
+ o2 + /φμ/μ2

dμ
∝r−1
2 exp

−r
2

−(o + /φμ/μ)2
1 + /φμ
+ o2 + /φμ/μ2

= r−1
2 exp

−
r
2(1 + /φμ)
(
−(o + /φμ/μ)2 + (1 + /φμ)(o2 + /φμ/μ2)
)
= r−1
2 exp

−r
/φμ(o −/μ)2
2(1 + /φμ)

.
(7.158)
Here we discuss the case when the VB posterior for r is the Dirac delta function around
the MAP value of r, and the argument of its Dirac delta function is the maximum value
of the VB posterior. Then, the result of the integration with respect to r is obtained by
changing r to the MAP value (/φr −2)/r−1 in Eq. (7.158) in Appendix C.11. Therefore,
the following equation is obtained:
E(μ)
+
N(o|μ, r−1)
,
∝exp

−
/φr −2
/r
/φμ(o −/μ)2
2(1 + /φμ)

= N

o
----/μ,
1 + /φμ
(/φr −2)/φμ/r

.
(7.159)
Thus, by recovering the omitted indexes, we can obtain
E(μ) [N(ot|μ, )] =
D

d=1
N

otd
-----/μjkd,
1 + /φμ
jk
(/φr
jk −2)/φμ
jk
/rjkd

.
(7.160)
This is the analytical result of the expected function of a Gaussian distribution with
expectation only over the mean parameter μ.

280
Variational Bayes
By substituting Eq. (7.158) into Eq. (7.157), we can obtain the following integral:

r
/φr+1
2
−1 exp

−r
/φμ(o −/μ)2 + (1 + /φμ)/r
2(1 + /φμ)

dr.
(7.161)
First we use the following notation to simplify the integral:
α ≜
/φr + 1
2
,
β ≜
/φμ(o −/μ)2 + (1 + /φμ)/r
2(1 + /φμ)
.
(7.162)
Note that β depends on the observation o. Then, the integral with the explicit range of r
is rewritten as follows:
 ∞
0
rα−1e−βrdr.
(7.163)
Now, we convert r with the following variable x:
r = x
β ,
dr = 1
β dx,
r ∈[0, ∞] →x ∈[0, ∞].
(7.164)
Now /φμ is a hyperparameter of the Dirichlet distribution, and /φμ > 0, therefore
β > 0 from Eq. (7.162), and the range of x becomes [0, ∞]. Then, the integral can
be rewritten as:
 ∞
0
rα−1e−βrdr =
 ∞
0
 x
β
α−1
e−x 1
β dx
=
 1
β
α  ∞
0
xα−1e−xdx.
(7.165)
Here, from the formula of the gamma function, we can further rewrite the above
integral as:
 ∞
0
rα−1e−βrdr =
 1
β
α
(α) =
 1
β
α
(α −1)!
∝
 1
β
α
.
(7.166)
Since (α −1)! does not depend on the observation o, we can disregard it as a constant
value. Finally, by recovering the variables of α and β from Eq. (7.162), Eq. (7.161) is
obtained as the following equation:

r
/φr+1
2
−1 exp

−r
/φμ(o −/μ)2 + (1 + /φμ)/r
2(1 + /φμ)

dr
∝
/φμ(o −/μ)2 + (1 + /φμ)/r
2(1 + /φμ)
−/φr+1
2
∝

1 +
/φμ
(1 + /φμ)/r(o −/μ)2
−/φr+1
2
.
(7.167)

7.3 Continuous density hidden Markov model
281
Here we refer to the concrete form of the Student’s t-distribution given in
Appendix C.16:
St(x|μ, λ, κ) ≜CSt

1 + 1
κλ(x −μ)2
−κ+1
2
.
(7.168)
The parameters μ, κ, and λ of the Student’s t-distribution correspond to those of the
above equation as follows:
⎧
⎪⎪⎨
⎪⎪⎩
μ
= /μ,
λ
= (1+/φμ
)/r
/φμ
/φr
,
κ
= /φr.
(7.169)
Thus, the result of the integral with respect to μ and r (Eq. (7.157)) is represented as the
Student’s t-distribution:
St

o
----/μ, (1 + /φμ)/r
/φμ/φr
, /φr

.
(7.170)
The third parameter in the t-distribution is called the degree of freedom, and if this value
is large, the distribution approaches the Gaussian distribution theoretically.
Thus, by recovering the omitted indexes, we can obtain
E(μ,) [N(ot|μ, )] =
D

d=1
St

otd
-----/μjkd,
(1 + /φμ
jk)/rjkd
/φμ
jk/φr
jk
, /φr
jk

.
(7.171)
This is the analytical result of the expected Gaussian distribution with marginalization of
both mean and precision parameters μ and r. Compared with Eq. (7.160), the marginal-
ization of both parameters changes the distribution from the Gaussian distribution to the
Student’s t-distribution. The latter is called a long tail distribution since it is a power law
function, and it provides a robust classiﬁcation in general, when the amount of training
data is small.
Since the degree of freedom in this solution is the posterior hyperparameter of the pre-
cision parameter /φr, and it is proportional to the amount of data, as shown in Eq. (7.67),
this solution approaches the Gaussian distribution. Then, the variance parameters in
Eq. (7.171) also approximately approach the following value:
(1 + /φμ
jk)/rjkd
/φμ
jk/φr
jk
≈/rjkd
/φr
jk
.
(7.172)
Thus, Eq. (7.171) is approximated by the following Gaussian distribution when the
amount of data O is large:
E(μ,) [N(ot|μ, )] ≈
D

d=1
N

otd
-----/μjkd,/rjkd
/φr
jk

.
(7.173)
This solution corresponds to the MAP estimation result of the Gaussian distribution in
CDHMM, as discussed in Section 4.3.5.

282
Variational Bayes
In Watanabe & Nakamura (2006), experimental results were reported to show the
effectiveness of the Bayesian predictive classiﬁcation without marginalization (it cor-
responds to the MAP estimation in Section 4.3), with marginalization of only mean
parameters (corresponds to Eq. (7.160)), and with marginalization of both mean and
covariance parameters (corresponds to Eq. (7.171)). Speaker adaptation experiments for
LVCSR (30 000 vocabulary size) show that the Student’s t-distribution-based Bayesian
predictive classiﬁcation improved the performance from the MAP estimation and the
marginalization results, reducing the WERs by 2.3 % and 1.2 %, respectively, when we
only used one utterance (3.3 seconds on average) for the adaptation data. Since all the
results use the same prior hyperparameter values, the improvement purely comes from
the marginalization effect. In addition, if the amount of adaptation data increased, the
performance of these three methods converged to the same value, which is also expected,
based on the discussion of analytical results of the Student’s t-distribution in Eq. (7.173).
The use of VB-based Bayesian predictive classiﬁcation makes acoustic modeling in
speech recognition a totally Bayesian framework that follows a consistent concept,
whereby all acoustic procedures (model parameter estimation, model selection, and
speech classiﬁcation) are carried out based on posterior distributions. For example,
compare the variational Bayesian speech recognition framework with a conventional
ML-BIC approach: the model parameter estimation, model selection and speech clas-
siﬁcation are based on ML (Chapter 3) and BIC (Chapter 6). BIC is an asymptotic
criterion that is theoretically effective only when the amount of training data is sufﬁ-
ciently large. Therefore, for a small amount of training data, model selection does not
perform well because of the uncertainty of the ML estimates. The next section aims at
solving the problem caused by a small amount of training data by using VB.
7.3.6
Decision tree clustering
This section revisits decision tree clustering of the context-dependent HMM states,
as we discussed in Section 6.5, based on the VB framework (Watanabe et al. 2004,
Hashimoto, Zen, Nankaku et al. 2008, Shiota, Hashimoto, Nankaku et al. 2009). Sim-
ilarly to Eq. (6.74), we approximate the Bayes factor in Section 6.2 by selecting an
appropriate question at each split, chosen to increase the variational lower bound/VB
objective function FM in the VB framework, as discussed in Section 7.3.4. When node
n is split into a Yes node (nQ
Y ) and No node (nQ
N) by question Q (we use MQ(n) with this
hypothesized model, obtained from question Q, and Mn with the original model), the
appropriate question /Q(n) is chosen from a set of questions as follows:
/Q(n) = arg max
Q log
p(MQ(n)|O)
p(Mn|O)

≈arg max
Q FQ(n),
(7.174)
where FQ(n) is the gain in the VB objective function when node n is split by Q, which
is deﬁned as:
FQ(n) = arg max
Q F(nQ
Y ) + F(nQ
N) −F(n).
(7.175)

7.3 Continuous density hidden Markov model
283
(n) denotes a set of (non-shared) context-dependent HMM states at node n in a deci-
sion tree. The question is chosen to maximize the gain in FM by splitting. The VB
objective function for decision tree construction is also simply calculated under the
following same constraints as the ML approach:
• Data alignments /γt(j, k) and/ξt(i, j) for each state are ﬁxed while splitting.
• Emission probability distribution in a state is represented by a single Gaussian
distribution (i.e., K = 1).
• Covariance matrices have only diagonal elements.
• A contribution of state transitions aij and initial weights πj for likelihood is disre-
garded.
By using these conditions, the objective function is obtained without iterative calcula-
tions, which reduces the calculation time. Under conditions of ﬁxed data assignment
and single Gaussian assumptions, the latent variable part of FM can be disregarded, i.e.,
Eq. (7.116) is approximated as
FM ≈FM
 .
(7.176)
In the VB objective function of model parameter FM
 (Eq. (7.126)), the factors of
posterior parameters of state transition /φa and mixture component /φω can also be dis-
regarded under the above conditions. Therefore, the objective function F in node n
(we omit the index n for simplicity) for assigned data set O = {O(i)|i ∈}, where
O(i) = {ot(i) ∈RD|t = 1, · · · , T(i)} can be obtained from the modiﬁcation of FM
 in
Eq. (7.126) as follows:
F = log
⎛
⎜⎜⎝(2π)−/γD
2

φμ

/φμ

 D
2 2
/φr
D
2
(

( /φr

2
))D *D
d=1

r0
d
 φr

2
2
φr
D
2
(

( φr

2
))D *D
d=1 (/rd)
/φr

2
⎞
⎟⎟⎠,
(7.177)
where {/φμ
,/μ, /φr
, {/rd}D
d=1}( ≜
/) is a subset of the posterior parameters in
Eq. (7.66), and is represented by:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
/φμ

= φμ
 + /γ,
/μ
= φμ
μ0
+/γ (1)

φμ
+/γ
,
/φr

= φr
 + /γ,
/rd
= /γ (2)
d + φμ
(μ0
d)2 −/φμ
(/μd)2 + r0
d.
(7.178)
/γ,/γ (1)
 and /γ (2)
d are the sufﬁcient statistics of a set of states in node n, as deﬁned as
follows.
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
/γ
≜

i∈
T(i)
t=1,
/γ (1)

≜

i∈
T(i)
t=1 ot(i),
/γ (2)
d
≜

i∈
T(i)
t=1(otd(i))2.
(7.179)
Note that since we use the hard aligned data O(i) (based on the Viterbi algo-
rithm), the assignment information /γt(i) is included in this representation. Here,

284
Variational Bayes
{φμ
, μ0
, φr
, {r0
d}D
d=1}( ≜) is a set of prior parameters. One choice of setting the
prior hyperparameters μ0
 and r0
d would be to set them by using monophone (root
node) HMM state statistics (/γroot, /γ (1)
root and /γ (2)
root) as follows:
μ0
 = /γ (1)
root
/γroot
,
r0
d = φr


/γ (2)
root, d
/γroot
−
(
μ0
root,d
)2

.
(7.180)
The other parameters φμ
 and φr
 are set manually. By substituting Eq. (7.178) into
Eq. (7.177), the gain FQ(n) can be obtained when n is split into nQ
Y , nQ
N by question Q:
FQ(n) = f(/(nQ
Y )) + f(/(nQ
N)) −f(/(n)) −f((n)).
(7.181)
Here, f() is deﬁned by:
f() ≜−D
2 log φμ −φr
2
D

d=1
log rd + D log 
φr
2

.
(7.182)
The terms that do not contribute to FQ(n) are disregarded. The ﬁnal term in Eq. (7.181)
is only computed from the prior hyperparameter . Similarly to the BIC criterion in
Eq. (6.92), node splitting stops when the condition
FQ(n) ≤0
(7.183)
is satisﬁed. A model structure based on the VB framework can be obtained by executing
this construction for all trees, resulting in the maximization of total FM. This implemen-
tation based on the decision tree method does not require iterative calculations, and can
construct clustered-state HMMs efﬁciently. There is another major method for the con-
struction of clustered-state HMMs that uses a successive state splitting algorithm, and
which does not remove latent variables in HMMs (Takami & Sagayama 1992, Ostendorf
& Singer 1997). Therefore, this requires the VB Baum–Welch algorithm and calculation
of the latent variable part of the lower bound/VB objective function for each splitting.
This is realized as the VB SSS algorithm by Jitsuhiro & Nakamura (2004).
The relationship between VB model selection and the conventional BIC model selec-
tion, based on Eqs. (7.181) and (6.89), respectively, is discussed below. Based on
the condition of a sufﬁciently large amount of data, the posterior hyperparameters in
Eq. (7.178) are approximated as follows:
/φμ
, /φr
 →/γ,
/μ →/γ (1)

/γ
,
/rd →/γ (2)
d −
(
/γ (1)
d
)2
/γ
.
(7.184)

7.3 Continuous density hidden Markov model
285
In addition, from Stirling’s approximation, the logarithmic gamma function has the
following relationship:
log 
(x
2
)
→x
2 log
(x
2
)
−x
2 −1
2 log
( x
2π
)
,
(7.185)
when |x| →∞. By substituting Eq. (7.184) into Eq. (7.182) and using Eq. (7.185), f(/)
is approximated as
f(/) →−D
2 log /γ −/γ
2
D

d=1
log
⎛
⎜⎝/γ (2)
d −
(
/γ (1)
d
)2
/γ
⎞
⎟⎠
+ D
/γ
2 log
/γ
2

−/γ
2 −1
2 log
/γ
2π

= −/γ
2

D (1 + log(2π)) +
D

d=1
log
⎛
⎜⎝
/γ (2)
d
/γ
−
(
/γ (1)
d
)2
(/γ)2
⎞
⎟⎠



≈log |ML
 |

−D log /γ. (7.186)
Then, an asymptotic form of Eq. (7.182) is composed of a log-likelihood gain term and
a penalty term depending on the number of free parameters (2D in this diagonal covari-
ance Gaussian case) and the amount of training data, i.e., the asymptotic form becomes
the BIC-type objective function form, as shown in Eq. (6.88). Therefore, VB theoreti-
cally involves the BIC objective function, and so BIC model selection is asymptotically
equivalent to VB model selection, which demonstrates the advantages of VB, especially
for small amounts of training data.
7.3.7
Determination of HMM topology
Once a clustered-state model structure is obtained, acoustic model selection is com-
pleted by determining the number of mixture components per state. GMMs include
latent variables, and their determination requires the VB Baum–Welch algorithm and
computation of the latent variable part of the variational lower bound, unlike the
clustering triphone HMM states in Section 7.4.5. Therefore, this section deals with
determination of the number of GMM components per state by considering the latent
variable effects. Then, the effectiveness of VB model selection in latent variable models
is conﬁrmed (Jitsuhiro & Nakamura 2004) for the successive state splitting algorithm,
and the effectiveness of VB model selection for GMMs is re-conﬁrmed (Valente &
Wellekens 2003). In general, there are two methods for determining the number of mix-
ture components. With the ﬁrst method, the number of mixture components per state is
the same for all states. The objective function FM is calculated for each number of mix-
ture components, and the number of mixture components that maximizes the total FM is
determined as being the appropriate one (ﬁxed-number GMM method). With the second
method, the number of mixture components per state can vary by state; here, Gaussians
are split and merged to increase FM and determine the number of mixture components

286
Variational Bayes
in each state (varying-number GMM method). A model obtained by the varying-number
GMM method is expected to be more accurate than one obtained by the ﬁxed-number
GMM method, although the varying-number GMM method requires more computation
time.
We require the variational lower bound for each state to determine the number of
mixture components. In this case, the state alignments vary and states are expressed as
GMMs. Therefore, the model includes latent variables and the component FM
S,V cannot
be disregarded, unlike the case of triphone HMM state clustering. However, since the
number of mixture components is determined for each state and the state alignments do
not change greatly, the contribution of the state transitions to the objective function is
expected to be small, and can be ignored. Therefore, the objective function FM for a
particular state j is represented from Eqs. (7.126) and (7.135) as follows:
(FM)j = (FM
 )j −(FM
V )j,
(7.187)
where (FM
 )j is represented by removing the HMM terms in Eq. (7.126) as follows:
(FM
 )j = log
(
k φω
jk)
(
k /φω
jk)
*
k (/φω
jk)
*
k (φω
jk)
+

k
log
⎛
⎜⎜⎜⎜⎜⎝
(2π)−
/γjkD
2

φμ
jk
/φμ
jk
 D
2


 /φr
jk
2
D *
d

r0
jkd
2
 φr
jk
2
(

( φr
jk
2
))D *
d
(/rjkd
2
)
/φr
jk
2
⎞
⎟⎟⎟⎟⎟⎠
.
(7.188)
Similarly, (FM
V )j is also represented as follows:
(FM
V )j =

k
/γjk
(

(
/φω
jk
)
−
(
k′ /φω
jk′
))
−1
2

k
/γjk

D

log(2π) + 1
/φμ
jk
−
/φr
jk
2

+

d
log/rjkd
2

−1
2

k
⎛
⎝/φr
jk

t,d
/γt(j, k)(otd −/μjkd)2
/rjkd
⎞
⎠−log

V
˜u(O, V|M)

.
(7.189)
Therefore, with the ﬁxed-number GMM method, the total FM is obtained by summing
up all states’ (FM)j, which determines the number of mixture components per state.
With the varying-number GMM method, the change of (FM)j per state is compared
after merging or splitting the Gaussians, which also determines the number of mixture
components. The number of mixture components is also automatically determined by
using the BIC/MDL objective function (Chen & Gopinath 1999, Shinoda & Iso 2001).
However, the BIC/MDL objective function is based on the asymptotic condition and
cannot be applied to latent models in principle. On the other hand, the variational lower
bound derived by VB does not need the asymptotic condition and can determine an
appropriate model structure with latent variables.

7.4 Structural Bayesian linear regression for hidden Markov model
287
Table 7.2 Automatic determination of acoustic model topology.
Read speech
Read speech
Isolated word
Lecture
(JNAS)
(WSJ)
(JEIDA)
(CSJ)
VB
91.7 %
91.3 %
97.9 %
74.5 %
# states
912
2504
254
1986
# components
40
32
35
32
ML + Dev. Set
91.4 %
91.3 %
98.1 %
74.2 %
# states
1000
7500
1000
3000
# components
30
32
15
32
Table 7.2 shows experimental results for automatic determination of the acoustic
model topology by using VB and the conventional heuristic approach that determines
the model topology by evaluating ASR performance on development sets. Note that VB
was only used for the model topology determination, and the other procedures (e.g.,
training and decoding) were performed by using the conventional (ML) approaches.
Therefore, Table 7.2 simply shows the effectiveness of the model selection. We used
two tasks based on read speech recognition of news articles, JNAS (Shikano, Kawahara,
Kobayashi et al. 1999) and WSJ (Paul & Baker 1992), an isolated word speech recog-
nition task (JEIDA 100 city name recognition), and a lecture speech recognition task,
CSJ (Furui, Maekawa & Isahara 2000). Table 7.2 provides the ASR performance of the
determined model topology with the number of total HMM states and a mixture com-
ponent in an HMM state, where we used the same number of mixture component for all
states. In the various ASR tasks, VB achieved comparable performance to the conven-
tional method by selecting appropriate model topologies without using a development
set. Thus, these experiments proved that the VB model selection method can automati-
cally determine an appropriate acoustic model topology with a comparable performance
to that obtained by using a development set.
7.4
Structural Bayesian linear regression for hidden Markov model
As discussed in Section 3.5, a Bayesian treatment of the afﬁne transformation param-
eters of CDHMM is an important issue to improve the generalization capability of the
model adaptation. While the regression tree used in the conventional maximum likeli-
hood linear regression (MLLR) can be considered one form of prior knowledge, i.e.,
how various Gaussian distributions are related, another approach is to explicitly con-
struct and use prior knowledge of regression parameters in an approximated Bayesian
paradigm.
For example, maximum a-posteriori linear regression (MAPLR) (Chesta, Siohan &
Lee 1999) replaces the ML criterion with the MAP criterion introduced in Chapter 4
in the estimation of regression parameters. Quasi-Bayes linear regression (Chien 2002)
also replaces the ML/MAP criterion with a quasi-Bayes criterion. With the explicit prior
knowledge acting as a regularization term, MAPLR appears to be less susceptible to the

288
Variational Bayes
overﬁtting problem. The MAPLR is extended to the structural MAP (SMAP) (Shinoda
& Lee 2001) and the structural MAPLR (SMAPLR) (Siohan, Myrvoll & Lee 2002), both
of which fully utilize the Gaussian tree structure used in the model selection approach
to efﬁciently set the hyperparameters in prior distributions. In SMAP and SMAPLR, the
hyperparameters in the prior distribution in a target node are obtained from the statistics
in its parent node. Since the total number of speech frames assigned to a set of Gaussians
in the parent node is always larger than that in the target node, the statistics obtained in
the parent node are more reliable than those in the target node, and these can be good
prior knowledge for transformation parameter estimation in the target node.
Another extension of MAPLR is to replace MAP approximation by a fully Bayesian
treatment of latent models, using VB. This section employs VB for the linear regres-
sion problem (Watanabe & Nakamura 2004, Yu & Gales 2006, Watanabe, Nakamura
& Juang 2013), but we focus on model selection and efﬁcient prior utilization at the
same time, in addition to estimation of the linear transformation parameters of HMMs
proposed in previous work (Watanabe & Nakamura 2004, Yu & Gales 2006). In partic-
ular, we consistently use the variational lower bound as the optimization criterion for
the model structure and hyperparameters, in addition to the posterior distributions of the
transformation parameters and the latent variables. As we discussed in Section 7.2, since
this optimization leads the approximated variational posterior distributions to the true
posterior distributions theoretically in the sense of minimizing Kullback–Leibler diver-
gence between them, the above consistent approach leads to improved generalization
capability (Neal & Hinton 1998, Attias 1999, Ueda & Ghahramani 2002).
This section provides an analytical solution to the variational lower bound by
marginalizing all possible transformation parameters and latent variables introduced in
the linear regression problem. The solution is based on a variance-normalized represen-
tation of Gaussian mean vectors to simplify the solution as normalized domain MLLR.
As a result of variational calculation, we can marginalize the transformation parameters
in all nodes used in the structural prior setting. This is a part of the solution of the varia-
tional message passing algorithm (Winn & Bishop 2006), which is a general framework
of variational inference in a graphical model. Furthermore, the optimization of the model
topology and hyperparameters in the proposed approach yields an additional beneﬁt in
the improvement of the generalization capability. For example, this approach infers the
linear regression without controlling the Gaussian cluster topology and hyperparameters
as the tuning parameters. Thus linear regression for HMM parameters is accomplished
without excessive parameterization in a Bayesian sense.
7.4.1
Variational Bayesian linear regression
This section provides an analytical solution for Bayesian linear regression by using a
variational lower bound. The previous section only considers a regression matrix in leaf
node j ∈JM, but we also consider a regression matrix in leaf or non-leaf node i ∈IM in
the Gaussian tree given model structure M. Then we focus on a set of regression matrices
in all nodes 
IM = {Wi|i = 1, · · · , |IM|}, instead of 
JM, and marginalize 
IM in a
Bayesian manner. This extension involves the structural prior setting as proposed in

7.4 Structural Bayesian linear regression for hidden Markov model
289
SMAP and SMAPLR (Shinoda & Lee 2001, Siohan et al. 2002, Yamagishi, Kobayashi,
Nakano et al. 2009).
In this section, we mainly deal with:
• prior distribution of model parameters p(
IM; M, );
• true posterior distribution of model parameters and latent variables
p(
IM, Z|O; M, );
• variational posterior distribution of model parameters and latent variables
q(
IM, Z|O; M, );
• generative model distribution p(O, Z|
IM; ).
Note that the prior and generative model distributions are given, as shown in the genera-
tive process of Algorithm 12, and we obtain the variational posterior distribution, which
is an approximation of the true posterior distribution.
7.4.2
Generative model
As discussed in Section 3.5, the generative model distribution with the expectation with
respect to the posterior distributions of latent variables is represented as follows:
E(Z)

log p(O, Z|
IM; )

=
K

k=1
T

t=1
γt(k) log N(ot|μad
k , k),
(7.190)
where p(O, Z|
IM; ) is the generative model distribution of the transformed HMM
parameters with transformed mean vectors μad
k . We use μad
k
based on the following
variance normalized representation:
μad
k = CkWjξk.
(7.191)
Ck is the Cholesky decomposition matrix of k, and ξk = [1, ((Ck)−1μini
k )⊺]⊺is
obtained based on the initial mean vector μini
k . This representation makes the calculation
simple.5
7.4.3
Variational lower bound
With regard to variational Bayesian approaches, we ﬁrst focus on the following marginal
log-likelihood p(O; , M, ) with a set of HMM parameters , a set of hyperparameters
, and a model structure:6,7
log p(O; , M, )
= log
 
Z
p(O, Z|
IM; )p(
IM; M, )d
IM

.
(7.192)
5 Hahm, Ogawa, Fujimoto et al. (2012) discuss the use of conventional MLLR estimation without the
variance normalization in the VB framework and its application to feature-space MLLR (fVBLR).
6  and M can also be marginalized by setting their distributions. This section point-estimates  and M by
a MAP approach, similar to the evidence approximation in Chapter 5.
7 We can also marginalize the HMM parameters . This corresponds to jointly optimizing HMM and linear
regression parameters.

290
Variational Bayes
p(
IM; M, ) is a prior distribution of transformation matrices 
IM. In the follow-
ing explanation, we omit , M, and  in the prior distribution and generative model
distribution for simplicity, i.e., p(
IM; M, ) →p(
IM), and p(O, Z|
IM; ) →
p(O, Z|
IM).
Similarly to Eq. (7.15), since the variational Bayesian approach focuses on the varia-
tional lower bound of the marginal log likelihood F(M, ) with a set of hyperparameters
 and a model structure M, Eq. (7.192) is represented as follows:
log p(O; , M, )
= log
 
Z
p(O, Z|
IM)p(
IM)
q(
IM, Z)
q(
IM, Z)d
IM

≥E(
IM ,Z)

log p(O, Z|
IM)p(
IM)
q(
IM, Z)




≜F(M,)
.
(7.193)
The inequality in Eq. (7.193) is supported by the Jensen’s inequality in Eq. (7.10).
q(
IM, Z) is an arbitrary distribution, and is optimized by using a variational method
to be discussed later. For simplicity, we omit M, , and O from the distributions. As
discussed in Section 7.1, the variational lower bound is a better approximation of the
marginal log likelihood than the auxiliary functions of maximum likelihood EM and
maximum a-posteriori EM algorithms that point-estimate model parameters, especially
for small amount of training data. Therefore, the variational Bayes can mitigate the
sparse data problem that the conventional approaches must resolve.
The variational Bayes regards the variational lower bound F(M, ) as an objective
function for the model structure and hyperparameter, and an objective functional for the
joint posterior distribution of the transformation parameters and latent variables (Attias
1999, Ueda & Ghahramani 2002). In particular, if we consider the true posterior distri-
bution p(
IM, Z|O) (we omit conditional variables M and  for simplicity), we obtain
the following relationship:
KL

q(
IM, Z)∥p(
IM, Z|O)

= log p(O; , M, ) −F(M, ).
(7.194)
This equation means that maximizing the variational lower bound F(M, ) with respect
to q(
IM, Z) corresponds to minimizing the KL divergence between q(
IM, Z) and
p(
IM, Z|O) indirectly. Therefore, this optimization leads to ﬁnding q(
IM, Z), which
approaches the true posterior distribution.8
8 The following sections assume factorization forms of q(
IM , Z) to make solutions mathematically
tractable. However, this factorization assumption weakens the relationship between the KL divergence and
the variational lower bound. For example, if we assume q(
IM , Z) = q(
IM )q(Z), and focus on the KL
divergence between q(
IM ) and p(
IM |O), we obtain the following inequality:
KL

q(
IM )∥p(
IM |O)

≤log p(O; , M, ) −F(M, ).
(7.195)
Compared with Eq. (7.194), the relationship between the KL divergence and the variational lower bound is
less direct due to the inequality relationship. In general, the factorization assumption distances optimal
variational posteriors from the true posterior within the VB framework.

7.4 Structural Bayesian linear regression for hidden Markov model
291
Figure 7.1
Binary tree structure with transformation matrices. If we focus on node i, the transformation
matrices in the parent node, left child node, and right child node are represented as Wp(i), Wl(i),
and Wr(i), respectively.
Thus, in principle, we can straightforwardly obtain the (sub) optimal model structure,
hyperparameters, and posterior distribution, as follows:
˜m = arg max
M F(M, ),
˜ = arg max
 F(M, ),
˜q(
IM, Z) = arg
max
q(
IM ,Z) F(M, ).
(7.196)
These optimization steps are performed alternately, and ﬁnally lead to local optimum
solutions, similar to the EM algorithm. However, it is difﬁcult to deal with the joint
distribution q(
IM, Z) directly, and we propose factorizing them by utilizing a Gaussian
tree structure. In addition, we also set a conjugate form of the prior distribution p(
IM).
This procedure is a typical recipe of VB to make a solution mathematically tractable
similarly to that of the classical Bayesian adaptation approach.
Structural prior distribution setting in a binary tree
We utilize a Gaussian tree structure to factorize the prior distribution p(
IM). We con-
sider a binary tree structure, but the formulation is applicable to a general non-binary
tree. We deﬁne the parent node of i as p(i), the left child node of i as l(i), and the right
child node of i as r(i), as shown in Figure 7.1, where a transformation matrix is prepared
for each corresponding node i. If we deﬁne W1 as the transformation matrix in the
root node, we assume the following factorization for the hierarchical prior distribution
p(
IM):
p(
IM) = p(W1, · · · , W|IM|)
= p(W1)p(Wl(1)|W1)p(Wr(1)|W1)
p(Wl(l(1))|Wl(1))p(Wr(l(1))|Wl(1))
p(Wl(r(1))|Wr(1))p(Wr(r(1))|Wr(1)) · · ·
=

i∈IM
p(Wi|Wp(i)).
(7.197)

292
Variational Bayes
To make the prior distribution a product form in the last line of Eq. (7.197), we deﬁne
p(W1) ≜p(W1|Wp(1)). As seen, the effect of the transformation matrix in a target node
propagates to its child nodes.
This hierarchical prior setting is based on an intuitive assumption that the statistics in
a target node are highly correlated with the statistics in its parent node. In addition, since
the total number of speech frames assigned to a set of Gaussians in the parent node is
always larger than that in the target node, the statistics obtained in the parent node are
more reliable than in the target node, and these can be good prior knowledge for the
transformation parameter estimation in the target node.
With a Bayesian approach, we need to set a practical form of the above prior distri-
butions. A conjugate distribution in Section 2.1.4 is preferable as far as obtaining an
analytical solution is concerned, and we set a matrix variate Gaussian distribution simi-
lar to maximum a-posteriori linear regression (MAPLR (Chesta et al. 1999)). A matrix
variate Gaussian distribution is deﬁned in Appendix C.9 as follows:
p(Wi) = N(Wi|Mi, i, i)
≜CN (i, i) exp

−1
2tr
+
(Wi −Mi)⊺−1
i
(Wi −Mi)−1
i
,
,
(7.198)
where CN (i, i) is a normalization constant deﬁned as:
CN (i, i) ≜(2π)−D(D+1)
2
|i|−D
2 |i|−D+1
2 .
(7.199)
Mi is a D × (D + 1) location matrix, i is a (D + 1) × (D + 1) symmetric scale matrix,
and i is a D×D symmetric scale matrix. The term i represents correlation of column
vectors, and i represents correlation of raw vectors. These are hyperparameters of the
matrix variate Gaussian distribution. There are many hyperparameters to be set, and this
makes the implementation complicated. In this section, we try to ﬁnd another conjugate
distribution with fewer hyperparameters than Eq. (7.198). To obtain a simple solution
for the ﬁnal analytical results, we use a spherical Gaussian distribution that has the
following constraints on i and i:
i ≈ID,
i ≈ρ−1
i
ID+1,
(7.200)
where ID is the D × D identity matrix and ρi indicates a precision parameter. Then
Eq. (7.198) can be rewritten as follows:
N(Wi|Mi, ID, ρ−1
i
ID+1)
= CN (ID, ρ−1
i
ID+1) exp

−1
2tr

ρi(Wi −Mi)⊺(Wi −Mi)

,
(7.201)
where CN (ID, ρ−1
i
ID+1) is a normalization factor, and is deﬁned as
CN (ID, ρ−1
i
ID+1) ≜
( ρi
2π
) D(D+1)
2
.
(7.202)

7.4 Structural Bayesian linear regression for hidden Markov model
293
This approximation means that matrix elements do not have any correlation with each
other. This can produce simple solutions for Bayesian linear regression.9
Based on the spherical matrix variate Gaussian distribution, the conditional prior
distribution p(Wi|Wp(i)) in Eq. (7.197) is obtaining by setting the location matrix as
the transformation matrix Wp(i) in the parent node with the precision parameter ρi as
follows:
p(Wi|Wp(i)) = N(Wi|Wp(i), ID, ρ−1
i
ID+1).
(7.204)
Note that in the following sections Wi and Wp(i) are marginalized. In addition, we set
the location matrix in the root node as the deterministic value of Wp(1) = [0, ID]. Since
μad
k
= CkWp(1)ξk = μini
k
from Eq. (7.191), this hyperparameter setting means that
the initial mean vectors are not changed if we only use the prior knowledge. This
makes sense in the case of a small amount of data by ﬁxing the HMM parameters
as their initial values; this in a sense also inherits the philosophical background of
Bayesian adaptation, although the objective function has been changed from a-posteriori
probability to a lower bound of the marginal likelihood. Therefore, we just have
{ρi|i = 1, · · · , |IM|} as a set of hyperparameters , which will also be optimized in
our framework.
Algorithm 12 Generative process of structural Bayesian transformation of CDHMM
Require:  and 
1: Draw 
IM from p(
IM)
2: Update ad from transformation matrices in leaf nodes 
JM
3: Draw O from CDHMM with ad
Variational calculus
In VB, we also assume the following factorization form for the posterior distribution
q(Z, 
IM):
q(Z, 
IM) = q(Z)q(
IM) = q(Z)

i∈IM
q(Wi).
(7.205)
Then, from the general variational calculation for F(M, ) with respect to q(Wi) based
on Eq. (7.25), we obtain the following (sub) optimal solution for q(Wi):
9 A matrix variate Gaussian distribution in Eq. (7.198) is also represented by the following multivariate
Gaussian distribution (Dawid 1981):
N(Wi|Mi, i, i)
∝exp

−1
2 vec(Wi −Mi)⊺(i ⊗i)−1 vec(Wi −Mi)−1

,
(7.203)
where vec(Wi −Mi) is a vector formed by the concatenation of the columns of (Wi −Mi), and ⊗denotes
the Kronecker product. Based on this form, a VB solution in this section could be extended without
considering the variance normalized representation used (Chien 2002).

294
Variational Bayes
log ˜q(Wi)
∝E(Z,W\i)

log p(O, Z, 
IM)

∝E(Z,W\i)

log p(O, Z|
IM)

+ E(W\i)

log p(
IM)

.
(7.206)
W\i means a set of transformation matrices at a set of nodes for IM that does not include
Wi, i.e.,
W\i = {Wi′|i′ ∈IM \ i}.
(7.207)
Then, by using Eqs. (7.197) for p(
IM) and (7.205) for q(W\i), we can rewrite the
equation, as follows:
log ˜q(Wi)
∝E(W\i)
⎡
⎣log

i′∈IM
p(Wi′|Wp(i′))
⎤
⎦+ E(Z,W\i)

log p(O, Z|
IM)

∝

i′∈IM
E(W\i)

log p(Wi′|Wp(i′))

+ E(Z,W\i)

log p(O, Z|
IM)

.
(7.208)
Note that the second term depends on two nodes i′ and p(i′), and the expectation over i′
is not trivial. In this expectation, we can consider the following two cases of variational
posterior distributions:
1) Leaf node
We ﬁrst focus on the initial term of Eq. (7.208). If i is a leaf node, we can disregard the
expectation with respect to *
i′̸=i∈IM q(Wi′) in the nodes other than the parent node p(i)
of the target leaf node. Thus, we obtain the following simple solution:
log ˜q(Wi) ∝E(Wp(i))

log p(Wi|Wp(i))

+ E(Z,W\i)

log p(O, Z|
IM)

.
(7.209)
2) Non-leaf node (with child nodes)
Similarly, if i is a non-leaf node, in addition to the parent node p(i) of the target node, we
also have to consider the child nodes l(i) and r(i) of the target node for the expectation,
as follows:
log ˜q(Wi) ∝
E(Wp(i))

log p(Wi|Wp(i))

(7.210)
+ E(Wl(i))

log p(Wl(i)|Wi)

(7.211)
+ E(Wr(i))

log p(Wr(i)|Wi)

(7.212)
+ E(Z,W\i)

log p(O, Z|
IM)

.
(7.213)
In both cases, the posterior distribution of the transformation matrix in the target node
depends on those in the parent and child nodes. Therefore, the posterior distributions
are iteratively calculated. This inference is known as a variational message passing
algorithm (Winn & Bishop 2006), and Eqs. (7.209)–(7.213) are speciﬁc solutions of
the variational message passing algorithm to a binary tree structure. The next section
provides a concrete form of the posterior distribution of the transformation matrix.

7.4 Structural Bayesian linear regression for hidden Markov model
295
Posterior distribution of transformation matrix
We ﬁrst focus on Eq. (7.213), which is a general form of Eq. (7.209) that has additional
terms based on child nodes to Eq. (7.209). Equation (7.213) is based on the expecta-
tion with respect to *
i′̸=i∈IM q(Wi′) and q(Z). The term with q(Z) is represented as the
following expression similar to Eqs. (3.168) and (3.161):
E(Z)

log p(O, Z|
IM)

=
K

k=1
T

t=1
γt(k) log N(ot|μad
k , k)
=

i∈IM

k∈Ki
γk log CN (k) −1
2tr
+
W⊺
i Wii −2W⊺
i Zi +

k∈Ki
−1
k k
,
.
(7.214)
This equation is calculated from the sufﬁcient statistics (γk, Sk, i, and Zi in
Eqs. (3.169) and (3.167)), that is
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
γk =
T

t=1
γt(k),
γ k =
T

t=1
γt(k)ot,
k =
T

t=1
γt(k)oto⊺
t ,
(7.215)
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
i ≜

k∈Ki
ξkξ⊺
k γk,
Zi ≜

k∈Ki
(Ck)−1γ kξ⊺
k .
(7.216)
These are computed by the VB-E step (e.g., γt(k) = q(vt = k)), which is described in
the next section. This equation form means that the term can be factorized by node i.
This factorization property is important for the following analytic solutions and algo-
rithm. Actually, by considering the expectation with respect to *
i′̸=i∈IM q(Wi′), we can
integrate out the terms that do not depend on Wi, as follows:
E(W\i)

E(Z)

log p(O, Z|
IM)

∝−1
2tr

W⊺
i Wii −2W⊺
i Zi

.
(7.217)
Thus, we can obtain the simple quadratic form for this expectation.
Next, we consider Eq. (7.210). Since we use a conjugate prior distribution, q(Wp(i))
is also represented by the following matrix variate Gaussian distribution as the same
distribution family with the prior distribution:
q(Wp(i)) = N(Wp(i)|Mp(i), ID, p(i)).
(7.218)
Note that the posterior distribution has a unique form in that the ﬁrst covariance matrix
is an identity matrix while the second is a symmetric matrix. We discuss this form with
the analytical solution, later.

296
Variational Bayes
By substituting Eqs. (7.197) and (7.218) into Eq. (7.210), Eq. (7.210) is represented
as follows:
E(Wp(i))

log p(Wi|Wp(i))

=

N(Wp(i)|Mp(i), ID, p(i)) log N(Wi|Wp(i), ID, ρ−1
i
ID+1)dWp(i).
(7.219)
To solve the integral, we use the following matrix distribution formula:

N(Wp(i)|Mp(i), ID, p(i))dWp(i) = 1,

Wp(i)N(Wp(i)|Mp(i), ID, p(i))dWp(i) = Mp(i).
(7.220)
Then, by using the concrete form of the prior distribution in Eq. (7.201) and by disre-
garding the terms that do not depend on Wi, Eq. (7.219) can be solved as the logarithmic
function of the matrix variate Gaussian distribution that has the posterior distribution
parameter Mp(i) as a hyperparameter:
E(Wp(i))

log p(Wi|Wp(i))

∝ρi

tr

W⊺
i Wp(i)

N(Wp(i)|Mp(i), ID, p(i))dWp(i)
−ρi
2

tr

W⊺
i Wi

N(Wp(i)|Mp(i), ID, p(i))dWp(i)
∝ρitr

W⊺
i Mp(i)

−ρi
2 tr

W⊺
i Wi

∝log N(Wi|Mp(i), ID, ρ−1
i
ID+1).
(7.221)
Similarly, Eqs. (7.211) and (7.212) are solved as follows:
E(Wl(i))

log p(Wl(i)|Wi)

∝log N(Wi|Ml(i), ID, ρ−1
l(i) ID+1),
E(Wr(i))

log p(Wr(i)|Wi)

∝log N(Wi|Mr(i), ID, ρ−1
r(i)ID+1).
(7.222)
Thus, the expected value terms of the three prior distributions in Eq. (7.210) are
represented as the following matrix variate Gaussian distribution:
E(Wp(i))

log p(Wi|Wp(i))

+ E(Wl(i))

log p(Wl(i)|Wi)

+ E(Wr(i))

log p(Wr(i)|Wi)

∝log N(Wi|Mp(i), ID, ρ−1
i
ID+1) + log N(Wi|Ml(i), ID, ρ−1
l(i) ID+1)
+ log N(Wi|Mr(i), ID, ρ−1
r(i)ID+1)
∝ρitr

W⊺
i Mp(i)

−ρi
2 tr

W⊺
i Wi

+ ρl(i)tr

W⊺
i Ml(i)

−ρl(i)
2 tr

W⊺
i Wi

+ ρr(i)tr

W⊺
i Mr(i)

−ρr(i)
2 tr

W⊺
i Wi


7.4 Structural Bayesian linear regression for hidden Markov model
297
∝−ρi + ρl(i) + ρr(i)
2
tr

W⊺
i Wi

+ tr

W⊺
i

ρiMp(i) + ρl(i)Ml(i) + ρr(i)Mr(i)

∝log N

Wi
----
ρiMp(i) + ρl(i)Ml(i) + ρr(i)Mr(i)
ρi + ρl(i) + ρr(i)
, ID, (ρi + ρl(i) + ρr(i))−1ID+1

.
(7.223)
It is an intuitive solution, since the location parameter Wi is represented as a linear
interpolation of the location values of the posterior distributions in the parent and child
nodes. The precision parameters control the linear interpolation ratio.
Similarly, we can also obtain the expected value term of the prior term in Eq. (7.209),
and we summarize the prior terms of the non-leaf and leaf node cases as follows:
ˆq(Wi) = N(Wi| ˆMi, ID, ˆρ−1
i
ID+1),
(7.224)
where
ˆMi =
 ρiMp(i)+ρl(i)Ml(i)+ρr(i)Mr(i)
ρi+ρl(i)+ρr(i)
Non-leaf node,
Mp(i)
Leaf node,
ˆρi =

ρi + ρl(i) + ρr(i)
Non-leaf node,
ρi
Leaf node.
(7.225)
Thus, the effect of prior distributions becomes different depending on whether the target
node is a non-leaf node or leaf node. The solution is different from our previous solution
(Watanabe, Nakamura & Juang 2011), since the previous solution does not marginalize
the transformation parameters in non-leaf nodes. In the Bayesian sense, this solution is
stricter than the previous solution.
Based on Eqs. (7.214) and (7.224), we can ﬁnally derive the quadratic form of Wi as
follows:
log ˜q(Wi) ∝−1
2tr
+
ˆρiW⊺
i Wi + W⊺
i Wii −2 ˆρiW⊺
i ˆMi −2W⊺
i Zi
,
∝−1
2tr
+
W⊺
i Wi( ˆρiID+1 + i) −2W⊺
i ( ˆρi ˆMi + Zi)
,
,
(7.226)
where we disregard the terms that do not depend on Wi. Thus, by deﬁning the following
matrix variables:
˜i =

ˆρiID+1 + i
−1
=

(ρi + ρl(i) + ρr(i))ID+1 + i
−1
Non-leaf node,
(ρiID+1 + i)−1
Leaf node,
˜Mi =
(
ˆρi ˆMi + Zi
)
˜
=

ρiMp(i) + ρl(i)Ml(i) + ρr(i)Mr(i) + Zi
 ˜
Non-leaf node,

ρiMp(i) + Zi
 ˜
Leaf node,
(7.227)

298
Variational Bayes
we can derive the posterior distribution of Wi analytically. The analytical solution is
expressed as
˜q(Wi) = N(Wi| ˜Mi, ID, ˜i)
= CN (ID, ˜i) exp

−1
2tr
+
(Wi −˜Mi)⊺(Wi −˜Mi) ˜
−1
i
,
,
(7.228)
where
CN (ID, ˜i) = (2π)−D(D+1)
2
| ˜i|−D
2 .
(7.229)
The posterior distribution also becomes a matrix variate Gaussian distribution, since we
use a conjugate prior distribution for Wi. From Eq. (7.227), ˜Mi are linearly interpolated
by hyperparameter ˆMi and the ﬁrst-order statistics of the linear regression matrix Zi. ˆρi
controls the balance between the effects of the prior distribution and adaptation data.
This solution is the M-step of the VB–EM algorithm, and corresponds to that of the
ML–EM algorithm in Section 3.5.
Compared with Eq. (7.201), Eq. (7.228) keeps the ﬁrst covariance matrix as a diagonal
matrix, while the second covariance matrix ˜ has off-diagonal elements. This means
that the posterior distribution only considers the correlation between column vectors in
W. This unique property comes from the variance normalized representation introduced
in Section 3.5, which makes multivariate Gaussian distributions in HMMs uncorrelated,
and this relationship is taken over to the VB solutions.
Although the solution for a non-leaf node would make the prior distribution robust by
taking account of the child node hyperparameters, this structure makes the dependency
of the target node on the other linked nodes complex. Therefore, in the implementation
step, we approximate the hyperparameters of the posterior distribution for a non-leaf
node to those for a leaf node by ˆMi ≈Mp(i) and ˆρi ≈ρi in the Eq. (7.225), and this
makes an algorithm simple.
The next section explains the E-step of the VB–EM algorithm, which com-
putes sufﬁcient statistics γk, k, i, and Zi in Eqs. (3.169) and (3.167). These are
obtained by using ˜q(Wi), of which mode ˜Mi is used for the Gaussian mean vector
transformation.
Posterior distribution of latent variables
From the variational calculation of F(M, ) with respect to q(Z) based on Eq. (7.25),
we also obtain the following posterior distribution:
log ˜q(Z) ∝E(
IM )

log p(O, Z|
IM)

.
(7.230)
By using the factorization form of the variational posterior (Eq. (7.205)), we can dis-
regard the expectation with respect to the variational posteriors other than that of the
target node i. Therefore, to obtain the above VB posteriors of latent variables, we have
to consider the following integral:

/q(Wi) log N(ot|CkWiξk, k)dWi.
(7.231)

7.4 Structural Bayesian linear regression for hidden Markov model
299
Since the Gaussian mean vectors are only updated in the leaf nodes, node i in this section
is regarded as a leaf node. By substituting Eqs. (7.228) and (3.162) into Eq. (7.231), the
equation can be represented as:

/q(Wi) log N(ot|CkWiξk, k)dWi
= log N(ot| ˜μk, k) −1
2tr
+
ξkξ⊺
k ˜i
,
.
(7.232)
where
˜μk = Ck ˜Miξk.
(7.233)
The analytical result is almost equivalent to the E-step of conventional MLLR, which
means that the computation time is almost the same as that of the conventional MLLR
E-step.
We derive the posterior distribution of latent variables ˜q(Z), introduced in Sec-
tion 7.4.3, based on the VB framework. In this derivation, we omit indexes i, k, and t for
simplicity. By substituting the concrete form (Eq. (3.162)) of the multivariate Gaussian
distribution into Eq. (7.231), the equation can be represented as:

/q(W) log N(o|CWξ, )dW
= −D
2 log(2π||) −1
2

˜q(W)
(
(o −CWξ)⊺−1(o −CWξ)
)



(∗1)
dW,
(7.234)
where we use the following equation for the normalization term:

˜q(W)dW = 1.
(7.235)
Let us now focus on the quadratic form (∗1) of Eq. (7.234). By considering  = C(C)⊺
in Eq. (3.164), (∗1) can be rewritten as follows:
(∗1) = (C−1o −Wξ)⊺(C−1o −Wξ)
= tr
+
(C−1o −Wξ)(C−1o −Wξ)⊺,
= tr

RW⊺W −2WY⊺+ U

,
(7.236)
where we use the fact that the trace of the scalar value is equal to the original scalar
value and the cyclic property of the trace in Appendix B:
a = tr[a],
(7.237)
tr[ABC] = tr[BCA].
(7.238)
We also deﬁne (D + 1) × (D + 1) matrix R, D × (D + 1) matrix Y, and D × D matrix U
in Eq. (7.236) as follows:
R ≜ξξ⊺,
Y ≜C−1oξ⊺,
U ≜−1oo⊺.
(7.239)

300
Variational Bayes
The integral of Eq. (7.236) over W can be decomposed into the following three terms:

˜q(W)tr

RW⊺W −2WY⊺+ U

dW
=

˜q(W)tr

RW⊺W

dW



(∗2)
−2

˜q(W)tr

WY⊺
dW



(∗3)
+tr [U] ,
(7.240)
where we use the distributive property of the trace in Appendix B:
tr[A(B + C)] = tr[AB + AC],
(7.241)
and use Eq. (7.235) in the third term of the second line in Eq. (7.240).
We focus on the integrals (∗2) and (∗3). Since ˜q(W) is a scalar value, (∗3) can be
rewritten as follows:
(∗3) =

tr

˜q(W)WY⊺
dW
= tr


˜q(W)WY⊺dW

.
(7.242)
Here, we use the following matrix properties:
tr[aA] = a tr[A],
(7.243)

tr[f(A)]dA = tr


f(A)dA

.
(7.244)
Thus, the integral is ﬁnally solved as
(∗3) = tr


˜q(W)WdW

Y⊺

= tr
+
˜MY⊺,
,
(7.245)
where we use

˜q(W)WdW = ˜M.
(7.246)
Similarly, we also rewrite (∗2) in Eq. (7.240) based on Eqs. (7.243) and (7.244) as
follows:
(∗2) =

tr

˜q(W)RW⊺W

dW
= tr


˜q(W)RW⊺WdW

= tr

R

˜q(W)W⊺WdW

.
(7.247)
Thus, the integral is ﬁnally solved as
(∗2) = tr
+
R
(
˜ + ˜M⊺˜M
),
,
(7.248)

7.4 Structural Bayesian linear regression for hidden Markov model
301
where we use

˜q(W)W⊺WdW = ˜ + ˜M⊺˜M.
(7.249)
Thus, we have solved all the integrals in Eq. (7.240).
Finally, we substitute the integral results of (∗2) and (∗3) (i.e., Eqs. (7.248) and
(7.245)) into Eq. (7.240), and rewrite Eq. (7.240) based on the concrete forms of R,
Y, and U deﬁned in Eq. (7.239) as follows:
Eq. (7.240)
= tr
+
R
(
˜ + ˜M⊺˜M
)
−2 ˜MY⊺+ U
,
= tr
+
ξξ⊺( ˜ + ˜M⊺˜M) −2 ˜Mξo⊺(C−1)⊺+ −1oo⊺,
.
(7.250)
Then, by using the cyclic property in Eq. (7.238) and  = C(C)⊺in Eq. (3.164), we can
further rewrite Eq. (7.240) as follows:
Eq. (7.240)
= tr
+
ξξ⊺˜ + −1 (
 ˜Mξξ⊺˜M⊺−2C ˜Mξo⊺+ oo⊺),
= tr
+
ξξ⊺˜ + −1 (
o −C ˜Mξ
) (
o −C ˜Mξ
)⊺,
.
(7.251)
Thus, we obtain the quadratic form with respect to o, which becomes a multi-
variate Gaussian distribution form. By recovering the omitted indexes i, k, and t,
and substituting the integral result in Eq. (7.251) into Eq. (7.234), we ﬁnally solve
Eq. (7.231) as:

˜q(Wi) log N(ot|CkWiξk, k)dWi
= −D
2 log(2π|k|) −1
2tr
+
ξkξ⊺
k ˜i + (k)−1 (
ot −Ck ˜Miξk
) (
ot −Ck ˜Miξk
)⊺,
= log N(ot|Ck ˜Miξk, k) −1
2tr
+
ξkξ⊺
k ˜i
,
.
(7.252)
Here, we use the concrete form of the multivariate Gaussian distribution in Eq. (3.162).
Note that the Gaussian mean vectors are updated in the leaf nodes in this result, while
the posterior distributions of the transformation parameters are updated for all nodes.
Variational lower bound
By using the factorization form (Eq. (7.205)) of the variational posterior distribution,
the variational lower bound deﬁned in Eq. (7.193) is decomposed as follows:
F(M, ) = E(Z,
IM )
&
log p(O, Z|
IM)p(
IM)
q(Z) *
i∈IM q(Wi)
'
= E(Z,
IM )
&
log p(O, Z|
IM)p(
IM)
*
i∈IM q(Wi)
'



≜L(M,)
−E(Z)

log q(Z)

.
(7.253)

302
Variational Bayes
The second term, which contains q(Z), is an entropy value and is calculated at the E-step
in the VB–EM algorithm. The ﬁrst term (L(M, )) is a logarithmic evidence term for
M and  = {ρi|i = 1, · · · |IM|}, and we can obtain an analytical solution of L(M, ).
Because of the factorization forms in Eqs. (7.205), (7.197), and (7.214), L(M, ) can
be represented as the summation over i, as follows:
L(M, ) = E(Z,
IM )
&
log
*
i∈IM p(O, Z|Wi)p(Wi|Wp(i))
*
i∈IM q(Wi)
'
=

i∈IM
Li(ρi, ρl(i), ρr(i)),
(7.254)
where
Li(ρi, ρl(i), ρr(i)) ≜

i∈IM
E(Z,
IM )

log p(O, Z|Wi)p(Wi|Wp(i))
q(Wi)

.
(7.255)
Note that this factorization form has some dependencies from parent and child node
parameters through Eqs. (7.225) and (7.227). To derive an analytical solution, we
ﬁrst consider the expectation with respect to q(Z) only for cluster i. By substituting
Eqs. (3.168), (7.201), and (7.228) into Li(ρi, ρl(i), ρr(i)), the expectation can be rewritten,
as follows:
E(Z)

log p(O, Z|Wi)p(Wi|Wp(i))
q(Wi)

=

k∈Ki
γk log CN (k) −1
2tr
⎡
⎣W⊺
i Wii −2W⊺
i Zi +

k∈Ki
−1
k k
⎤
⎦
+ log CN (ID, ρ−1
i
ID+1) −1
2tr

ρi(Wi −Wp(i))⊺(Wi −Wp(i))

−log CN (ID, ˜i) + 1
2tr
+
(Wi −˜Mi)⊺(Wi −˜Mi) ˜
−1
i
,
=

k∈Ki
γk log CN (k) + log CN (ID, ˆρ−1
i
ID+1)
CN (ID, ˜i)
+ (∗).
(7.256)
If we consider only the leaf node case, by using Eq. (7.227), the expectation of (∗) part
can be rewritten as:
(∗) in Eq. (7.256)
= −1
2tr
⎡
⎣ˆρi ˆM⊺
i ˆMi −˜M⊺
i ˜Mi ˜
−1
i
+

k∈Ki
−1
k k
⎤
⎦.
(7.257)
The result obtained does not depend on Wi. Therefore, the expectation with respect to
q(Wi) can be disregarded in Li(ρi, ρl(i), ρr(i)). Consequently, we can obtain the following
analytical result for the lower bound:

7.4 Structural Bayesian linear regression for hidden Markov model
303
Li(ρi, ρl(i), ρr(i))
= −D
2 log(2π)

k∈Ki
γk −1
2

k∈Ki
γk log |k|
+ D(D + 1)
2
log ˆρi + D
2 log | ˜i|
−1
2tr
⎡
⎣ˆρi ˆM′
i ˆMi −˜M′
i ˜Mi ˜
−1
i
+

k∈Ki
−1
k k
⎤
⎦.
(7.258)
The ﬁrst line of the result obtained corresponds to the likelihood value given the
amount of data and the covariance matrices of the Gaussians. The other terms con-
sider the effect of the prior and posterior distributions of the model parameters. This
result is used as an optimization criterion with respect to the model structure M and the
hyperparameters .
Note that the objective function can be represented as a summation over i because
of the factorization form of the prior and posterior distributions. This representation
property is used for our model structure optimization in Section 7.4.5 for a binary tree
structure representing a set of Gaussians used in the conventional MLLR.
7.4.4
Optimization of hyperparameters and model structure
In this section, we describe how to optimize hyperparameters  and model structure M
by using the variational lower bound as an objective function. Once we obtain the varia-
tional lower bound, we can obtain an appropriate model structure and hyperparameters
that maximize the lower bound at the same time as follows:
{/, /
M} = arg max
M, F(M, ).
(7.259)
We use two approximations for the variational lower bound to make the inference algo-
rithm practical. First, we ﬁx latent variables Z during the above optimization. Then,
E(Z)

log q(Z)

in Eq. (7.253) is also ﬁxed for M and , and can be disregarded in the
objective function. Thus, we can only focus on L(M, ) in the optimization step, which
reduces computational cost greatly, as follows:
{/, /
M} ≈arg max
M, L(M, ).
(7.260)
This approximation is widely used in acoustic model selection (likelihood criterion
(Odell 1995) and Bayesian criterion (Watanabe et al. 2004)). Second, as we discussed
in Section 7.4.3, the solution for a non-leaf node (Eq. (7.224)) makes the dependency
of the target node on the other linked nodes complex. Therefore, we approximate
Li(ρi, ρl(i), ρr(i)) ≈Li(ρi) by ˆρi ≈ρi and so on, where Li(ρi) is deﬁned in the next sec-
tion. Therefore, in the implementation step, we approximate the posterior distribution
for a non-leaf node to that for a leaf node to make the algorithm simple.

304
Variational Bayes
7.4.5
Hyperparameter optimization
Even though we marginalize all of transformation matrix (Wi), we still have to set the
precision hyperparameters ρi for all nodes. Since we can derive the variational lower
bound, we can optimize the precision hyperparameter, and can remove the manual tun-
ing of the hyperparameters with the proposed approach. This is an advantage of the
proposed approach with regard to SMAPLR (Siohan et al. 2002), since SMAPLR has
to hand-tune its hyperparameters corresponding to {ρi}i.
Based on the leaf node approximation for variational posterior distributions, in
addition to the ﬁxed latent variable approximation (F(M, ) ≈L(M, )), in this sec-
tion the method we implement approximately optimizes the precision hyperparameter
as follows:
˜ρi = arg max
ρi L(M, )
=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
arg maxρi

Li(ρi, ρl(i), ρr(i)) + Lp(i)(ρp(i), ρi, ρr(p(i)))

i is a left child node of p(i)
arg maxρi

Li(ρi, ρl(i), ρr(i)) + Lp(i)(ρp(i), ρl(p(i)), ρi)

i is a right child node of p(i)
≈arg max
ρi Li(ρi),
(7.261)
where
Li(ρi) ≜D(D + 1)
2
log ρi + D
2 log | ˜i|
−1
2tr
+
ρiM⊺
p(i)Mp(i) −˜M⊺
i ˜Mi ˜
−1
i
,
.
(7.262)
This approximation makes the algorithm simple because we can optimize the precision
hyperparameter within the target and parent nodes, and do not have to consider the
child nodes. Since we only have one scalar parameter for this optimization step, we
simply used a line search algorithm to obtain the optimal precision hyperparameter. If
we consider a more complex precision structure (e.g., a precision matrix instead of a
scalar precision parameter in the prior distribution setting Eq. (7.200)), the line search
algorithm may not be adequate. In that case, we need to update hyperparameters by
using some other optimization technique (e.g., gradient ascent).
Model selection
The remaining tuning parameter in the proposed approach is how many clusters we
prepare. This is a model selection problem, and we can also automatically obtain the
number of clusters by optimizing the variational lower bound. In the binary tree struc-
ture, we focus on a subtree composed of a target non-leaf node i and its child nodes l(i)

7.4 Structural Bayesian linear regression for hidden Markov model
305
Algorithm 13 Structural Bayesian linear regression.
1: Prepare an initial Gaussian tree with a set of nodes I
2: Initialize ˜ = { ˜ρi, ˜Mi|i = 1, · · · |I|}
3: repeat
4:
VB E-step
5:
L(M, ) = Prune_tree(root node) // prune a tree by model selection
6:
# of leaf nodes = Transform_HMM(root node) // Transform HMMs in the pruned
tree
7: until Total lower bound is converged or a speciﬁed number of iterations has been
reached.
and r(i). We compute the following difference based on Eq. (7.262) of the parent and
that of the child nodes:10
Li(ρi) ≜Ll(i)(ρl(i)) + Lr(i)(ρr(i)) −Li(ρi).
(7.263)
This difference function is used for a stopping criterion in a top-down clustering strategy.
This difference function similarly appeared in the model selection of the context-
dependent CDHMM topologies in Sections 6.5 and 7.3.6. Then if the sign of L is
negative, the target non-leaf node is regarded as a new leaf node determined by the
model selection in terms of optimizing the lower bound. Next we prune the child nodes
l(i) and r(i). By checking the signs of Li for all possible nodes, and pruning the child
nodes when Li have negative signs, we can obtain the pruned tree structure, which
corresponds to maximizing the variational lower bound locally. This optimization is
efﬁciently accomplished by using a depth-ﬁrst search.
Thus, by optimizing the hyperparameters and model structure, we can avoid setting
any tuning parameters. We summarize this optimization in Algorithms 13, 14, and 15.
Algorithm 13 prepares a large Gaussian tree with a set of nodes I, prunes a tree based
on the model selection (Algorithm 14), and transforms HMMs (Algorithm 15). Algo-
rithm 14 ﬁrst optimizes the precision hyperparameters , and then the model structure
M. Algorithm 15 transforms Gaussian mean vectors in HMMs at the new root nodes in
the pruned tree IM obtained by Algorithm 14.
Watanabe et al. (2013) compare the VB linear regression method (VBLR) with
MLLR and SMAPLR, as regards the WSJ, for various amounts of adaptation data by
using LVCSR experiments for the Corpus of Spontaneous Japanese (CSJ). With a small
amount of adaptation data, VBLR outperforms the conventional approaches by about
1.0% absolute accuracy improvement, while with a large amount of adaptation data, the
accuracies of all approaches are comparable. This property is theoretically reasonable
10 Since we approximate the posterior distribution for a non-leaf node to that for a leaf node, the contribution
of the variational lower bounds from the non-leaf nodes to the total lower bounds can be disregarded, and
Eq. (7.263) is used as a pruning criterion. If we do not use this approximation, we just compare the
difference between the values Li(ρi, ρl(i), ρr(i)) of the leaf and non-leaf node cases in Eq. (7.258).

306
Variational Bayes
Algorithm 14 Prune_tree(node i)
1: if First iteration then
2:
˜ρi = arg maxρi Li(ρi) // These are used as
3:
Update ˜q(Wi) // hyperparameters of parent nodes
4: end if
5: if Node i has child nodes then
6:
˜ρi = arg maxρi Li(ρi)
7:
Update ˜q(Wi)
8:
L = Prune_tree(node left(i)) + Prune_tree(node right(i)) −Li( ˜ρi)
9:
if L < 0 then
10:
Prune child nodes // this node becomes a leaf node
11:
end if
12:
return Li( ˜ρi)
13: else
14:
˜ρi = arg maxρi Li(ρi)
15:
Update q(Wi)
16:
return Li( ˜ρi)
17: end if
Algorithm 15 Transform_HMM(node i)
1: if Node i has child nodes then
2:
return Transform_HMM(node left(i)) + Transform_HMM(node right(i))
3: else
4:
Update ˜μk = Ck ˜Miξk
5:
return 1
6: end if
since the variational lower bound would be tighter than the EM-based objective function
for a small amount of data, while the lower bound would approach it for a large amount
of data asymptotically. Therefore, it can be concluded that this improvement comes from
the optimization of the hyperparameters and the model structure in VBLR, in addition
to mitigation of the sparse data problem arising in the Bayesian approach.
7.5
Variational Bayesian speaker veriﬁcation
This section describes an application of VB to speaker veriﬁcation (Zhao, Dong, Zhao
et al. 2009, Kenny 2010, Villalba & Brümmer 2011). The main goal of this approach
is to obtain the feature representation that only holds speaker speciﬁc characteristics.
As discussed in Section 4.6.2, state-of-the-art speaker veriﬁcation systems use the super
vector obtained by the GMM–UBM or MLLR techniques, as a feature. However, the

7.5 Variational Bayesian speaker veriﬁcation
307
super vector still includes various factors other than the speaker characteristics with very
high dimensional representation. Use of factor analysis is critical in speaker veriﬁcation
to remove these irrelevant factors of the super vector and ﬁnd the lower dimensional
representation (Kenny 2010, Kinnunen & Li 2010, Dehak et al. 2011). In addition,
a Bayesian treatment of the factor analysis yields robust modeling of the speaker
veriﬁcation. This section discusses a VB treatment of the factor analysis model by
providing a generative model of the super vector (Section 7.5.1), prior distributions (Sec-
tion 7.5.2), variational posteriors (Section 7.5.3), and variational lower bound (Section
7.5.4).
7.5.1
Generative model
Let O = {on ∈RD|n = 1, · · · , N} be a D dimensional feature vector of n recordings.
Note that on is a super vector, and it can be the Gaussian super vector, vectorized form of
the MLLR matrix, or the factor vector obtained after the initial factor analysis process.11
If we use the Gaussian super vector, the number of dimensions D would be the product
of multiplying the number of mixture components in GMM (usually K = 1024) and the
number of speech feature dimensions (usually DMFCC = 39 when we use MFCC and
delta features) when we use GMM–UBM, that is
on = [μ⊺
1 , · · · , μ⊺
k , · · · , μ⊺
K]⊺,
μk ∈RDMFCC.
(7.264)
Therefore, D would be 1024×39 ≈40 thousands, and it is much larger than the number
of speech feature dimensions that we are dealing with at CDHMMs. Note also that the
feature on is obtained for each recording (utterance), and the frame level process is
performed when super vectors are extracted by GMM–UBM.
The generative model is represented as follows (Kenny, Boulianne, Ouellet et al.
2007, Kenny 2010):
on = m + U1x1 + U2x2n + εn,
(7.265)
where m ∈RD is a global mean vector for the feature vectors and can be regarded as a
bias vector of the feature vectors. Vector x1 ∈RD1 is a vector having a D1 dimensional
standard Gaussian distribution, which does not depend on the recording n, and it can
represent stationary speaker characteristics across recordings. On the other hand, x2n ∈
RD2 is a vector having a D2 dimensional standard Gaussian distribution depending on
the recording n, and it denotes channel characteristics changing over a recording. We
also deﬁne X2 ≜{x2n|n = 1, · · · , N}. εn ∈RD as a D dimensional vector having a
Gaussian distribution with 0 mean vector, and R ∈RD×D precision matrix.
In this book, m, U1 ∈RD×D1, U2 ∈RD×D2, and R ( = {m, U1, U2, R}) are
regarded as non-probabilistic model parameters and assumed to be estimated without
11 A Bayesian treatment of the factor analysis in a state-of-the-art speaker veriﬁcation system can be
performed after the ﬁrst step of the factor analysis, called i vector analysis (Dehak et al. 2011). That is,
Bayesian factor analysis is often performed for the ﬁrst-step factor vector (i vector) for each utterance,
instead of the Gaussian super vector. This book discusses Bayesian factor analysis for the Gaussian super
vector for simplicity.

308
Variational Bayes
the Bayesian framework based on ML/MAP. However, Villalba & Brümmer (2011) deal
with these parameters as probabilistic variables, and provide a fully Bayesian solution
of the factor analysis based speaker modeling by VB. The other probabilistic variables
x1, x2n, and εn are generated from the following Gaussian distributions:
x1 ∼N(0, u−1
1 ID1),
x2n ∼N(0, u−1
2n ID2),
εn ∼N(0, v−1
n R−1),
(7.266)
where we assume a zero mean spherical Gaussian distribution for x1 and x2n. The model
parameters u1 ∈R>0, u2n ∈R>0, and vn ∈R>0 are positive, and the probabilistic
treatment of these parameters is discussed later.
In summary, the conditional distribution of O is represented as follows:
p(O|x1, X2, {vn}N
n=1, ) =
N

n=1
N(on|m + U1x1 + U2x2n, v−1
n R−1).
(7.267)
The conditional joint distribution of O, x1, and X2 is represented as follows:
p(O, x1, X2|{vn}N
n=1, , u1, {u2n}N
n=1)
= p(O|x1, X2, {vn}N
n=1, )p(x1|u1)
N

n=1
p(x2n|u2n),
(7.268)
where
p(x1|u1) = N(x1|0, u−1
1 ID1),
p(x2n|u2n) = N(x2n|0, u−1
2n ID2).
(7.269)
The following section regards u1, u2n, and vn as probabilistic variables.
7.5.2
Prior distributions
We provide the conjugate prior distributions for u1, u2n, and vn that are represented by a
gamma distribution, as we discussed in Section 2.1.3. Kenny (2010) provides a simple
hyperparameter setting for each gamma distribution in Appendix C.11 by using only
one hyperparameter for each distribution, i.e., the model parameters u1, u2n, and vn are
generated from the following prior distributions:
u1 ∼Gam
φ1
2 , φ1
2

,
u2n ∼Gam
φ2
2 , φ2
2

,
vn ∼Gam
φv
2 , φv
2

,
(7.270)

7.5 Variational Bayesian speaker veriﬁcation
309
where φ1, φ2, and φv (≜) are hyperparameters in this model. Since the mean and
variance of the gamma distribution Gam(y|α, β) are α
β and α
β2 , respectively, this param-
eterization means that u1, u2n, and vn have the same mean value with 1, but the variance
values are changed with 2
φ1 , 2
φ2 , and 2
φv , respectively. Thus, we can provide the following
concrete forms of the prior distributions for u1, u2n, and vn:
p(u1|φ1) = Gam

u1
----
φ1
2 , φ1
2

,
p(u2n|φ2) = Gam

u2n
----
φ2
2 , φ2
2

,
p(vn|φv) = Gam

vn
----
φv
2 , φv
2

.
(7.271)
In this model, Z ≜{x1, u1, {x2n, u2n, vn}N
n=1} is a set of hidden variables, and the poste-
rior distribution of each variable can be obtained by using variational Bayes. Thus, the
joint prior distribution given hyperparameters  is represented as follows:
p(Z|) = p(x1, u1, {x2n, u2n, vn}N
n=1|)
= p(x1|u1)p(u1|φ1)
N

n=1
p(x2n|u2n)p(u2n|φ2)p(vn|φv).
(7.272)
Note that x1 depends on u1, and we cannot fully factorize them. A similar discussion
applies to x2n and u2n.
Now, we can provide the complete data likelihood function given hyperparameters 
and  based on Eqs. (7.267) and (7.272), and this can be used to obtain the variational
posteriors:
p(O, Z|, )
= p(O|, Z)p(Z|)
= N(x1|0, u−1
1 ID1)Gam

u1
----
φ1
2 , φ1
2

×
N

n=1
N(on|m + U1x1 + U2x2n, v−1
n R−1)N(x2n|0, u−1
2n ID2)
× Gam

u2n
----
φ2
2 , φ2
2

Gam

vn
----
φv
2 , φv
2

.
(7.273)
These probabilistic distributions are represented by Gaussian and gamma distribu-
tions. In the following sections we simplify the complete data likelihood function
p(O, Z|, ) to p(O, Z) to avoid complicated equations. Algorithm 16 provides a
generative process for the joint factor analysis speaker model with Eq. (7.273).

310
Variational Bayes
Algorithm 16 Generative process for joint factor analysis speaker model
Require:  = {m, U1, U2, R} and  = {φ1, φ2, φv}
1: Draw u1 from Gam
(
u1
--- φ1
2 , φ1
2
)
2: Draw x1 from N(x1|0, u−1
1 ID1)
3: for n = 1, · · · , N do
4:
Draw u2n from Gam
(
u2n
--- φ2
2 , φ2
2
)
5:
Draw vn from Gam
(
vn
--- φv
2 , φv
2
)
6:
Draw x2n from N(x2n|0, u−1
2n ID2)
7:
Draw on from N(on|m + U1x1 + U2x2n, v−1
n R−1)
8: end for
7.5.3
Variational posteriors
To deal with the variational posteriors, we assume the following factorization based on
the VB recipe:
q(Z|O) = q(x1|O)q(u1|O)
N

n=1
q(x2n|O)q(u2n|O)q(vn|O).
(7.274)
Section 7.1.3 discusses how we can obtain the following general solution for approxi-
mated variational posteriors:
/q(Zi|O) ∝exp

E(Z\i|O)

log p(O, Z)

.
(7.275)
We focus on the actual solutions for q(x1|O), q(u1|O), q(x2n), q(u2n|O), and q(vn|O).
• q(x1|O): this is calculated by substituting the factors depending on x1 in Eq. (7.273)
into Eq. (7.275) as follows:
log q(x1|O)
∝E(Z\x1)[log p(O, Z)]
∝E(Z\x1)
&
log

N(x1|0, u−1
1 ID1)
N

n=1
N(on|m + U1x1
+ U2x2n, v−1
n R−1)
'
∝E(u1)[log N(x1|0, u−1
1 ID1)]
+
N

n=1
E(x2n,vn)[log N(on|m + U1x1 + U2x2n, v−1
n R−1)].
(7.276)
Now let us consider the two expectations in the above equation. From the deﬁnition of
the multivariate Gaussian distribution in Appendix C.6, we can obtain the following
equation by disregarding the terms that do not depend on x1:

7.5 Variational Bayesian speaker veriﬁcation
311
E(u1)[log N(x1|0, u−1
1 ID1)] ∝E(u1)
+
−u1
2 x⊺
1 x1
,
∝−E [u1]
2
x⊺
1 x1,
(7.277)
where we omit the subscript (u1) in the expectation, as it is trivial. Similarly, the rest
of the expectations in Eq. (7.276) are also represented as follows:
E(x2n,vn)
+
log N(on|m + U1x1 + U2x2n, v−1
n R−1)
,
∝E(x2n,vn)
+
−vn
2 (on −(m + U1x1 + U2x2n))⊺
×R(on −(m + U1x1 + U2x2n))]
∝E(x2n,vn)
+
−vn
2

x⊺
1 U⊺
1 RU1x1

+ vn(on −m −U2x2n)⊺RU1x1
,
∝−E[vn]
2
x⊺
1 U⊺
1 RU1x1 + E[vn](on −m −U2E[x2n])⊺RU1x1.
(7.278)
Thus, by substituting Eqs. (7.277) and (7.278) into Eq. (7.276), we ﬁnd that
log q(x1|O)
∝−E [u1]
2
x⊺
1 x1 +
N

n=1
−E[vn]
2
x⊺
1 U⊺
1 RU1x1
+
N

n=1
E[vn](on −m −U2E[x2n])⊺RU1x1
∝−1
2tr
&
E [u1] ID1 +
N

n=1
E[vn]U⊺
1 RU1

x1x⊺
1
'
+ tr
&
x⊺
1
N

n=1
E[vn]U⊺
1 R(on −U2E[x2n] −m)
'
∝log N(x1|/μx1, /x1),
(7.279)
where we use the trace form deﬁnition of the multivariate Gaussian distribution in
Appendix C.6. Thus, q(x1|O) is represented as a Gaussian distribution, and /μx1 and
/x1 are posterior hyperparameters obtained as follows:
/μx1 ≜

E[u1]ID1 +
N

n=1
E[vn]U⊺
1 RU1
−1
×
N

n=1
E[vn]U⊺
1 R(on −U2E[x2] −m)
/x1 ≜

E[u1]ID1 +
N

n=1
E[vn]U⊺
1 RU1
−1
.
(7.280)
Thus, the hyperparameters obtained are represented with prior hyperparameters 
and the expected values of E[u1] and E[vn].

312
Variational Bayes
• q(x2n|O): this is similarly calculated by substituting the factors depending on x2n in
Eq. (7.273) into Eq. (7.275) as follows:
log q(x2n|O)
∝E(Z\x2n)[log p(O, Z)]
∝E(Z\x2n)
& N

n′=1
log (N(on′|m + U1x1
+U2x2n′, v−1
n′ R−1)N(x2n′|0, u−1
2n′ID2)
) '
∝E(x1,vn)
+
N(on|m + U1x1 + U2x2n, v−1
n R−1)
,
+ E(u2n)
+
N(x2n|0, u−1
2n ID2)
,
.
(7.281)
The expectations are rewritten as follows:
log q(x2n|O)
∝−1
2E[vn]tr

U⊺
2 RU2x2nx⊺
2n

+ 2E[vn]tr

x⊺
2nU⊺
2 R(on −U2E[x1] −m)

−1
2E[u2n]tr

x2nx⊺
2n

∝−1
2tr

E[u2n]ID2 + E[vn]U⊺
2 RU2

x2nx⊺
2n

+ tr

x⊺
2nE[vn]U⊺
2 R(on −U2E[x1] −m)

∝log N(x2n|/μx2n, /x2n).
(7.282)
Thus, q(x2n|O) is also represented as a Gaussian distribution, and /μx2n and /x2n are
posterior hyperparameters obtained as follows:
/μx2n ≜

E[u2n]ID2 + E[vn]U⊺
2 RU2
−1 E[vn]U⊺
2 R(on −U2E[x1] −m),
/x2n ≜

E[u2n]ID2 + E[vn]U⊺
2 RU2
−1 .
(7.283)
Note that the hyperparameters obtained are represented with prior hyperparameters
 and the expected values of E[u2n], E[vn], and E[x1]. Compared with Eq. (7.280),
Eq. (7.283) has a similar functional form, but it is computed recording by recording,
while Eq. (7.280) is computed with accumulation over every recording n.
• q(u1|O): this is also similarly calculated by substituting the factors depending on u1
in Eq. (7.273) into Eq. (7.275). However, compared with the previous two cases, the
gamma and Gaussian distributions appear in the formulation as follows:
log q(u1|O)
∝E(Z\u1)[log p(O, Z)]
∝E(Z\u1)

N(x1|0, u−1
1 ID1)Gam

u1
----
φ1
2 , φ1
2

∝E(x1)[log N(x1|0, u−1
1 ID1)] + log

Gam

u1
----
φ1
2 , φ1
2

.
(7.284)

7.5 Variational Bayesian speaker veriﬁcation
313
By using the deﬁnition of the gamma distribution in Appendix C.11, this equation can
be rewritten as follows:
log q(u1|O)
∝log
--u1ID1
--
1
2

−E[x⊺
1 x1]
2
u1 + log

u
φ1
2 −1
1

−φ1
2 u1
∝log

u
φ1+D1
2
−1
1

−φ1 + E[x⊺
1 x1]
2
u1
∝log

Gam

u1
-----
˜φ1
2 , ˜r1
2

.
(7.285)
Thus, q(u1|O) is represented as a gamma distribution, and ˜φ1 and ˜r1 are posterior
hyperparameters obtained as follows:
˜φ1 ≜φ1 + D1,
˜r1 ≜φ1 + E[x⊺
1 x1].
(7.286)
These posterior hyperparameters are obtained with their original prior hyperparameter
φ1 and the second-order expectation value of E[x⊺
1 x1].
• q(u2n|O): this is similarly calculated by substituting the factors depending on u2n in
Eq. (7.273) into Eq. (7.275):
log q(u2n|O)
∝E(Z\u2n)[log p(O, Z)]
∝E(Z\u2n)
& N

n′=1
N(x2n′|0, u−1
2n′ID2)Gam

u2n′
----
φ2
2 , φ2
2
'
∝E(x2n)[log N(x2n|0, u−1
2n ID2)] + log

Gam

u2n
----
φ2
2 , φ2
2

∝log

Gam

u2n
-----
˜r2n
2 ,
˜φ2n
2

.
(7.287)
q(u2n|O) is represented as a gamma distribution, and ˜φ2n and ˜r2n are posterior
hyperparameters, obtained as follows:
˜φ2n ≜φ2 + D2,
˜r2n ≜φ2 + E[x⊺
2nx2n].
(7.288)
These posterior hyperparameters are also obtained with their original prior hyperpa-
rameter φ2 and the second-order expectation value of E[x⊺
2nx2n], and these are very
similar to the posterior hyperparameters of q(u1|O) in Eq. (7.286).
• q(vn|O): ﬁnally, this is also calculated by substituting the factors depending on vn in
Eq. (7.273) into Eq. (7.275) as follows:
log q(vn|O)
∝E(Z\vn)[log p(O, Z)]

314
Variational Bayes
∝E(Z\vn)
& N

n′=1
N(on′|m + U1x1 + U2x2n′, v−1
n′ R−1)
×Gam

vn′
----
φv
2 , φv
2

∝E(x1,x2n)
+
N(on|m + U1x1 + U2x2n, v−1
n R−1)
,
+ log

Gam

vn
----
φv
2 , φv
2

∝log

Gam

vn
-----
˜rvn
2 ,
˜φvn
2

.
(7.289)
q(vn|O) is represented as a gamma distribution, and ˜φvn and ˜rvn are posterior
hyperparameters obtained as follows:
˜φvn ≜φv + D,
˜rvn ≜φv + E[ε⊺
n Rεn],
(7.290)
where εn is a residual vector appearing in the basic equation of the joint factor analysis
in Eq. (7.265), and is represented as:
εn = on −(m + U1x1 + U2x2n) .
(7.291)
E[ε⊺
n Rεn] is an expectation over both x1 and x2n, and deﬁned as follows:
E[ε⊺
n Rεn]
≜Ex1,x2n[(on −m −U1x1 −U2x2n)⊺R(on −m −U1x1 −U2x2n)]
= (on −m)⊺R(on −m) + tr

U⊺
1 RU1E

x1x⊺
1

+ tr

U⊺
2 RU2E

x2nx⊺
2n

−2(on −m)⊺RU1E [x1] −2(on −m)⊺RU2E [x2n]
+ 2tr

U⊺
1 RU2E [x2n] E

x⊺
1

.
(7.292)
This value is computed by the ﬁrst- and second-order expectation of x1 and x2n.
Thus, we can provide the VB posterior distributions of all hidden variables analytically.
Note that these equations are iteratively performed to obtain the sub-optimal posterior
distributions. That is, all the posterior distribution calculations need the expectation
values of hidden variables Z, which can be computed using the posterior distributions
obtained with the previous iteration. We provide the expectation values of x1 and x2n,
which are easily obtained by reference to the expectation formulas of a multivariate
Gaussian distribution in Appendix C.6 and posterior hyperparameters in Eqs. (7.280)
and (7.283):
E [x1] =

x1q(x1|O)dx1 = /μx1,
E

x1x⊺
1

=

x1x⊺
1 q(x1|O)dx1 = /x1,

7.5 Variational Bayesian speaker veriﬁcation
315
E [x2n] =

x2nq(x2n|O)dx2n = /μx2n,
E

x2nx⊺
2n

=

x2nx⊺
2nq(x2n|O)dx2n = /x2n.
(7.293)
Similarly, we provide the expectation values of u1, u2n, and vn, which are also easily
obtained by reference to the expectation formulas of gamma distribution in Appendix
C.11 and posterior hyperparameters in Eqs. (7.286), (7.288), and (7.290):
E [u1] =

u1q(u1|O)du1 =
/φ1
/r1
,
E [u2n] =

u2nq(u2n|O)du2n =
/φ2n
/r2n
,
E [vn] =

vnq(vn|O)dvn =
/φvn
/rvn
.
(7.294)
Finally, we summarize the analytical results of the posterior distributions q(Z|O), as
follows:
q(Z|O)
= q(x1|O)q(u1|O)
N

n=1
q(x2n|O)q(u2n|O)q(vn|O)
= N(x1|/μx1, /x1)Gam

u1
-----
˜φ1
2 , ˜r1
2

×
N

n=1
N(x2n|/μx2n, /x2n)Gam

u2n
-----
˜r2n
2 ,
˜φ2n
2

Gam

vn
-----
˜rvn
2 ,
˜φvn
2

,
(7.295)
where posterior hyperparameters are represented as:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
/μx1
≜
(
E[u1]ID1 + N
n=1 E[vn]U⊺
1 RU1
)−1
,
× N
n=1 E[vn]U⊺
1 R(on −U2E[x2] −m),
/x1
≜
(
E[u1]ID1 + N
n=1 E[vn]U⊺
1 RU1
)−1
,
/μx2n
≜

E[u2n]ID2 + E[vn]U⊺
2 RU2
−1 E[vn]U⊺
2 R(on −U2E[x1] −m),
/x2n
≜

E[u2n]ID2 + E[vn]U⊺
2 RU2
−1 ,
(7.296)
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
˜φ1
≜φ1 + D1,
˜r1
≜φ1 + E[x⊺
1 x1],
˜φ2n
≜φ2 + D2,
˜r2n
≜φ2 + E[x⊺
2nx2n],
˜φvn
≜φv + D,
˜rvn
≜φv + E[ε⊺
n Rεn].
(7.297)
Once we obtain the VB posterior distributions, we can also calculate the variational
lower bound, which is discussed in the next section.

316
Variational Bayes
7.5.4
Variational lower bound
As discussed in Section 4.6, speaker veriﬁcation can be performed by using the
likelihood ratio (Eq. (4.129)):
p(O|H0)
p(O|H1),
(7.298)
where H0 means that O is from the hypothesized speaker, while H1 means that O is
not from the hypothesized speaker. Instead of using the likelihood of p(O) where we
neglect the hypothesis index H, using the lower bound, we can treat speaker veriﬁcation
in a Bayesian sense. That is, we use the variational lower bound equation (in Eq. (7.5)):
p(O) =

log p(O, Z)dZ ≥F[q(Z|O)].
(7.299)
Then we use F[q(Z|O)] instead of p(O). From the deﬁnition of the variational lower
bound in Eq. (7.4), Eq. (7.299) can be decomposed into the following terms as follows:
F[q(Z|O)] ≜E(Z)

log p(O, Z)
q(Z|O)

= E(Z)

log p(O|Z)p(Z)
q(Z|O)

= E(Z)

log p(O|Z)

−KL(q(Z|O)∥p(Z)),
(7.300)
where the second term is the Kullback–Leibler divergence between the variational
posterior and prior distributions.
Let us focus on the ﬁrst term of the variational lower bound. The conditional
likelihood p(O|Z) is represented as the following Gaussian by using Eq. (7.267):
p(O|Z) =
N

n=1
N(on|m + U1x1 + U2x2n, v−1
n R−1).
(7.301)
By using Eq. (7.301), E(Z)

log p(O|Z)

is represented as follows:
E(Z)

log p(O|Z)

= E(Z)
&
log
N

n=1
N(on|m + U1x1 + U2x2n, v−1
n R−1)
'
=
N

n=1
E(x1,x2n,vn)
+
log N(on|m + U1x1 + U2x2n, v−1
n R−1)
,
.
(7.302)
This expectation is calculated by using the deﬁnition of multivariate Gaussian distribu-
tion in Appendix C.6 and the residual vector εn in Eq. (7.291), as follows:
E(x1,x2n,vn)
+
log N(on|m + U1x1 + U2x2n, v−1
n R−1)
,
= E(x1,x2n,vn)
+
log CN (v−1
n R−1)) −vn
2 ε⊺
n Rεn
,
= D
2 E[log vn] −D
2 log(2π) + 1
2 log |R| −1
2E[vn]E[ε⊺
n Rεn].
(7.303)

7.5 Variational Bayesian speaker veriﬁcation
317
Therefore, Eq. (7.302) is calculated as:
E(Z)

log p(O|Z)

=
N

n=1
D
2 E[log vn] −D
2 log(2π) + 1
2 log |R| −1
2E[vn]E[ε⊺
n Rεn]

.
(7.304)
Now, let us focus on the KL divergence in Eq. (7.301), which is decomposed into the
following terms based on the factorization forms of Eqs. (7.272) and (7.274):
KL(q(Z|O)∥p(Z)) = KL(q(x1, u1|O)∥p(x1, u1))
+
N

n=1
KL(q(x2n, u2n|O)∥p(x2n, u2n)) +
N

n=1
KL(q(vn|O)∥p(vn)).
(7.305)
The KL divergence is decomposed into the three KL divergence terms. Now, consider
KL(q(x1, u1|O)∥p(x1, u1)), which can be further factorized as follows:
KL(q(x1, u1|O)∥p(x1, u1))
= Eq(u1|O)

KL(q(x1|O)∥p(x1|u1))

+ KL(q(u1|O)∥p(u1)).
(7.306)
Thus, we need to compute the KL divergences of Gaussian and gamma distributions,
respectively. We use the following formulas, which are the analytical result of the KL
divergence between Gaussian and gamma distributions:
KL(N(x| ˜μ, ˜)∥N(x|μ, ))
= −D
2 −1
2 log |−1 ˜| + tr
+
−1 
 + ( ˜μ −μ)( ˜μ −μ)⊺,
,
(7.307)
and
KL(Gam(y|˜α, ˜β)∥Gam(y|α, β))
= log (α)
(˜α) + ˜α log ˜β −α log β + (˜α −α)(ψ(˜α) −log ˜β) + ˜α β −˜β
˜β
.
(7.308)
Therefore, by using Eq. (7.307), Eq(u1|O)

KL(q(x1|O)∥p(x1|u1))

can be rewritten as
follows:
Eq(u1|O)

KL(q(x1|O)∥p(x1|u1))

= Eq(u1|O)
+
KL(N(x1| ˜μx1, ˜x1∥N(x1|0, u−1
1 ID1))
,
= Eq(u1|O)

−D1
2 −1
2 log |u1 ˜x1| + tr
+
u1
(
u−1
1 ID1 + ( ˜μx1 −μ)( ˜μx1 −μ)⊺),
= −D1
2 −D1
2 E[log u1] −1
2 log | ˜x1| + 1
2E[log u1]E[x⊺
1 x1].
(7.309)
Similarly, other terms can be obtained by using VB analytically.
Thus, we can obtain the variational lower bound for the joint factor analysis. This can
be used as an objective function of the likelihood test, as discussed before, and is also
used to optimize the hyperparameters  and  based on the evidence approximation,
as discussed in Chapter 5.

318
Variational Bayes
7.6
Latent Dirichlet allocation
Latent Dirichlet allocation (LDA) (Blei et al. 2003) is known as the popular machine
learning approach which was proposed to build the latent topic model for information
retrieval, document modeling, text categorization, and collaborative ﬁltering. LDA has
been successfully applied for image modeling, music retrieval, speech recognition, and
many others. In general, LDA is an extended study from a topic model based on prob-
abilistic latent semantic analysis (PLSA) (Hofmann 1999b, Hofmann 2001), which was
addressed in Section 3.7.3. PLSA extracts the latent semantic information and estimates
the topic parameters according to maximum likelihood (ML) estimation, which suffers
from the over-trained problem. In addition, PLSA could not represent the unseen words
and documents. The number of PLSA parameters increases remarkably with the num-
ber of collected documents. To compensate these weaknesses, LDA incorporates the
Dirichlet priors to characterize the topic mixture probabilities. The marginal likelihood
over all possible values of topic mixture probabilities is calculated and maximized so
as to construct the LDA-based topic model for document representation. Unseen doc-
uments are generalized by using the LDA parameters, which are estimated according
to the variational Bayes inference procedure. The model complexity is controlled as
the training documents become larger. PLSA and LDA extract the topic information at
document level, and this could be combined in a language model for speech recognition.
In what follows, we ﬁrst address the construction of an LDA model from a set of train-
ing documents. The optimization objective is formulated for model training. Then the
variational Bayes (VB) inference is detailed. The variational distribution over multiple
latent variables is introduced to ﬁnd a VB solution to the LDA model.
7.6.1
Model construction
LDA provides a powerful mechanism to discover latent topic structure from a bag of M
documents with a bag of words w = {w1, · · · , wN}, where wn ∈V. The text corpus is
denoted by D = {w1, · · · , wM}. Figure 7.2 is a graphical representation of LDA. Using
LDA, each of the n words wn is represented by a multinomial distribution conditioned
on the topic zn:
wn|zn, β ∼Multi(β),
(7.310)
Figure 7.2
Representation of latent Dirichlet allocation.

7.6 Latent Dirichlet allocation
319
where β ∈R|V|×K denotes the multinomial parameters. The topic zn of word wn is also
generated by a multinomial distribution given parameters θ ∈RK:
zn|θ ∼Multi(θ),
(7.311)
where θk ≥0, K
k=1 θk = 1 and K topics are assumed. Importantly, the latent topics
of each document are treated as random variables. We assume that the multinomial
parameters θ of K topics are drawn from a Dirichlet distribution,
θ|α ∼Dir(α)
= (K
k=1 αk)
*K
k=1 (αk)
θα1−1
1
· · · θαK−1
K
,
(7.312)
where (·) is the gamma function and α is a K-vector parameter with component αk > 0.
This K-dimensional Dirichlet random variable θ lies in the (K −1)-simplex. Thus, LDA
parameters {α, β} contain the K-dimensional Dirichlet parameters α = [α1, · · · , αK]⊺
for K topic mixtures θ and the topic-dependent unigram parameters β = {βkv} =
{p(w = v|k)}. The parameters α and β are estimated by maximizing the marginal like-
lihood of a text corpus D or an N-word document w over the topic mixtures θ and the
topic labels z = {zn}:
{αML2, βML2} = arg max
{α,β} p(w|α, β),
(7.313)
where
p(w|α, β) =

p(θ|α)
 N

n=1
K

k=1
p(zn = k|θ) × p(wn = v|zn = k, β)

dθ.
(7.314)
The marginal likelihood is calculated by integrating over continuous variable θ and dis-
crete variable z. Note that LDA parameters {αML2, βML2} are estimated according to the
type-2 maximum likelihood method, as addressed in Section 5.1.2, because the likeli-
hood function considers all possible values of topic mixtures in different topics. Latent
variables in LDA include topic mixtures and topic labels {θ, z}. Strictly speaking, the
LDA model parameters  contain {θ, β} while the hyperparameters  should contain
α. But, using LDA, only the parameters θ are integrated out. Without loss of generality,
we treat both α and β as LDA parameters to be optimized from training data w.
However, in model inference, we should apply the EM algorithm which involves a
calculation of posterior distribution of latent variables {θ, z} given a document w:
p(θ, z|w, α, β) = p(θ, z, w|α, β)
p(w|α, β)
.
(7.315)
Unfortunately, this distribution is intractable because the normalization term
p(w|α, β) = (K
k=1 αk)
*K
k=1 (αk)
  K

k=1
θαk−1
k

×
⎛
⎝
N

n=1
K

k=1
|V|

v=1
(θkβkv)wv
n
⎞
⎠dθ
(7.316)
is an intractable function due to the coupling between θ and β in the summation over
latent topics. Here, the superscript v in wv
n denotes the component index, i.e., wv
n = 1

320
Variational Bayes
and wj
n = 0 for j ̸= v. Next, we address the variational Bayes (VB) inference procedure,
which is divided into three steps, namely ﬁnding the lower bound, ﬁnding the variational
parameters, and ﬁnding the model parameters.
7.6.2
VB inference: lower bound
Although the posterior distribution is intractable for exact inference, a variety of approx-
imate inference algorithms can be used for LDA including a Laplace approximation,
variational approximation, and Markov chain Monte Carlo. In this section, a simple
convexity-based variational inference is introduced to implement an LDA model by
using Jensen’s inequality. A variational distribution,
q(θ, z|γ , φ) = q(θ|γ )
N

n=1
q(zn|φn),
(7.317)
is used as a surrogate for the posterior distribution p(θ, z|w, α, β), where the variational
parameters γ and φ are estimated via an optimization. According to Jensen’s inequality
using the convex function −log(·), we have
log p(w|α, β) = log
 
z
p(θ, z, w|α, β)dθ
= log
 
z
p(θ, z, w|α, β)q(θ, z)
q(θ, z)
dθ
≥
 
z
q(θ, z) log p(θ, z, w|α, β)dθ
−
 
z
q(θ, z) log q(θ, z)dθ
= E(θ,z)[log p(θ, z, w|α, β)] −E(θ,z)[log q(θ, z)]
≜L[γ , φ; α, β] ≜F[q(θ|γ ), q(z|φ)],
(7.318)
where E[·] denotes the expectation operation and variational parameters γ and φ are
omitted for simplicity. Jensen’s inequality provides us with a lower bound L[γ , φ; α, β]
on the logarithm of marginal likelihood, given an arbitrary variational distribution
q(θ, z|γ , φ). It can be easily veriﬁed that the difference between the left-hand-side and
the right-hand-side of Eq. (7.318) is the KL divergence between the variational posterior
distribution and the true posterior distribution. We have
log p(w|α, β) = L(γ , φ; α, β) + KL(q(θ, z|γ , φ)∥p(θ, z|w, α, β)).
(7.319)
Therefore, maximizing the lower bound L[γ , φ; α, β] with respect to γ and φ is
equivalent to ﬁnding the optimal variational distribution q(θ, z| ˆγ , ˆφ) with variational
parameters ˆγ and ˆφ, which is closest to the true posterior distribution p(θ, z|w, α, β). To
do so, the lower bound is expanded by
L(γ , φ; α, β) = E(θ)[log p(θ|α)] + E(θ,z)[log p(z|θ)]
+ E(z)[log p(w|z, β)] −E(θ)[log q(θ)] −E(z)[log q(z)].
(7.320)

7.6 Latent Dirichlet allocation
321
Using the fact that the expectation of the sufﬁcient statistics is equivalent to the deriva-
tive of the log normalization factor with respect to the natural parameter, we obtain (Blei
et al. 2003)
E(θ)[log θk|α] = (αk) −
 K

i=1
αi

,
(7.321)
where  is the ﬁrst derivative of the log gamma function, also called the di-gamma
function, as used in Eqs. (5.82) and (7.81). The lower bound is further expanded in
terms of variational parameters {γ , φ} and model parameters {α, β} by
L(γ , φ; α, β) = log 
 K

k=1
αk

−
K

k=1
log (αk)
+
K

k=1
(αk −1)

(γk) −
 K

i=1
γi

+
N

n=1
K

k=1
φnk

(γk) −
 K

i=1
γi

+
N

n=1
K

k=1
|V|

v=1
φnkwv
n log βkv −log 
 K

k=1
γk

+
K

k=1
log (γk) −
K

k=1
(γk −1)

(γk) −
 K

i=1
γi

−
N

n=1
K

k=1
φnk log φnk.
(7.322)
Typically, ﬁnding the lower bound of marginal likelihood in VB inference is equivalent
to performing the VB E-step. However, we need to estimate the optimal variational
parameters γ and φ to ﬁnalize the VB E-step.
7.6.3
VB inference: variational parameters
The variational Dirichlet parameters γ and variational multinomial parameters φ are
estimated by maximizing the lower bound in Eq. (7.322). The terms related to γ are
collected and arranged thus:
L(γ ) =
K

k=1

(γk) −
 K

i=1
γi
 
αk +
N

n=1
φnk −γk

−log 
 K

k=1
γk

+
K

k=1
log (γk).
(7.323)

322
Variational Bayes
Differentiating with respect to the individual parameter γk, we have
∂L(γ )
∂γk
= ′(γk)

αk +
N

n=1
φnk −γk

−′
 K

i=1
γi
 K

i=1

αi +
N

n=1
φni −γi

.
(7.324)
Setting this equation to zero yields the optimal variational parameters
ˆγk = αk +
N

n=1
φnk
1 ≤k ≤K.
(7.325)
Note that the variational Dirichlet parameters ˆγ
= { ˆγk} are seen as the surrogate
of
the Dirichlet model parameters α, which sufﬁciently reﬂect the topic mixture
probabilities θ.
On the other hand, when optimizing the lower bound L(γ , φ; α, β) with respect to
variational multinomial parameters φ, a constrained maximization problem should be
tackled under the constraint
K

k=1
φnk = 1.
(7.326)
Therefore, we collect the terms in the lower bound related to the individual variational
parameter φnk and form the Lagrangian with the Lagrange multiplier λn:
L(φnk) = φnk

(γk) −
 K

i=1
γi

+ φnk log βkv −φnk log φnk + λn
 K

i=1
φni −1

,
(7.327)
where the unique word v is selected for wn such that wv
n = 1 and wj
n = 0 for j ̸= v.
Differentiating this Lagrangian with respect to φnk yields
∂L(φnk)
∂φnk
= (γk) −
 K

i=1
γi

+ log βkv −log φnk −1 + λn.
(7.328)
We set this differentiation to zero and derive the variational parameter φnk which is
written as a function of Lagrange multiplier λn. By substituting the derived variational
parameter φnk into the constraint given in Eq. (7.326), we can estimate the multiplier
and then obtain the optimal variational parameters:
ˆφnk =
βkv exp
(
(γk) −
(K
i=1 γi
))
K
j=1 βjv exp
(
(γj) −
(K
i=1 γi
))
(7.329)
1 ≤n ≤N,
1 ≤k ≤K.

7.6 Latent Dirichlet allocation
323
7.6.4
VB inference: model parameters
After ﬁnding the optimal variational parameters {γ , φ}, we ﬁx these parameters and
substitute the optimal variational distribution q(θ, z| ˆγ , ˆφ) to calculate the updated lower
bound L( ˆγ , ˆφ; α, β). The VB M-step treats the variational lower bound L( ˆγ , ˆφ; α, β) as
a surrogate of the intractable marginal log likelihood log p(w|α, β), and estimates the
LDA parameters by
{ˆα, ˆβ} = arg max
{α,β} L( ˆγ , ˆφ; α, β).
(7.330)
First, we deal with the optimization over the conditional multinomial distributions
β = {βkv} = {p(wn = v|zn = k)}. The lower bound is hereafter calculated from a
text corpus D = {w1, · · · , wM}. The terms containing model parameters β are collected
and the constraints
|V|

v=1
βkv = 1
(7.331)
are imposed, so as to form the Lagrangian with Lagrange multipliers {λk}:
L(β) =
M

d=1
Nd

n=1
K

k=1
|V|

v=1
ˆφdnkwv
dn log βkv
+
K

k=1
λk
⎛
⎝
|V|

v=1
βkv −1
⎞
⎠.
(7.332)
We differentiate this Lagrangian with respect to individual βkv and set it to zero to ﬁnd
the optimal conditional multinomial distributions:
ˆβkv =
M
d=1
Nd
n=1 ˆφdnkwv
dn
|V|
m=1
M
d=1
Nd
n=1 ˆφdnkwm
dn
(7.333)
1 ≤k ≤K,
1 ≤v ≤|V|.
To deal with the optimization over the Dirichlet parameters α = {αk}, we collect the
terms in the lower bound which contain α and give
L(α) =
M

d=1

log 
 K

k=1
αk

−
K

k=1
log (αk)
+
K

k=1
(αk −1)

( ˆγdk) −
 K

i=1
ˆγdi

.
(7.334)
Differentiating with respect to individual αk gives
∂L(α)
∂αk
= M


 K

i=1
αi

−(αk)

+
M

d=1

( ˆγdk) −
 K

i=1
ˆγdi

.
(7.335)

324
Variational Bayes
There is no closed-form solution to the optimal Dirichlet parameter αk, since the right-
hand-side of Eq. (7.335) depends on αi where i ̸= k. We should use an iterative
algorithm to ﬁnd the K × 1 optimal parameter vector α. Here, the Newton–Raphson
optimization is applied to ﬁnd the optimal parameters by iteratively performing
α(τ+1) = α(τ) −(H(α(τ)))−1∇L(α(τ)),
(7.336)
where τ is the iteration index, ∇L is the K × 1 gradient vector and H is the K × K
Hessian matrix consisting of the second-order differentiations in different entries:
∂L(α)
∂αkαj
= δ(k, j)M′(αk) −′
 K

i=1
αi

,
(7.337)
where δ(k, j) denotes a Kronecker delta function. The inverse of Hessian matrix
(H(α(τ)))−1 can be obtained by applying the Woodbury matrix inversion. As a result,
LDA model parameters {ˆα, ˆβ} are estimated in a VB M-step. The VB inference based on
an EM algorithm is accordingly completed by maximizing the variational lower bound
of marginal likelihood L(γ , φ; α, β) through performing a VB E-step for updating vari-
ational parameters {γ , φ} and a VB M-step for estimating model parameters {α, β}. The
increase of the lower bound is assured by VB–EM iterations.
7.7
Latent topic language model
LDA is established as a latent topic model which is designed for document representa-
tion by using a bag of words in the form of document w or text corpus D. LDA has been
successfully extended for language modeling and applied for continuous speech recog-
nition in Tam & Schultz (2005) and Chien & Chueh (2011). However, before the LDA
language model, the topic model based on PLSA was constructed and merged in the
n-gram language model (Gildea & Hofmann 1999), as addressed in Section 3.7.3. But,
the PLSA-based topic model suffers from the weaknesses of poor generalization and
redundant model complexity. The performance of the resulting PLSA language model
is limited. Compared to the PLSA language model, we are more interested in the LDA
language model and its application in speech recognition. In the literature, the LDA-
based topic model is incorporated into the n-gram language model based on an indirect
method (Tam & Schultz 2005) and a direct method (Chien & Chueh 2011), which are
introduced in Section 7.7.1 and Section 7.7.2, respectively.
7.7.1
LDA language model
LDA is generally indirect for characterizing the n-gram regularities of a current word wi
given its history words wi−1
i−n+1. The word index n in the document model differs from i
in the language model. A document w has N words while a sentence has T words. The
hierarchical Dirichlet language model in Section 5.3 was presented as an alternative
to language model smoothing (MacKay & Peto 1995). In Yaman, Chien & Lee (2007),

7.7 Latent topic language model
325
the hierarchical Dirichlet priors were estimated by a maximum a-posteriori method for
language model adaptation. These two methods have adopted the Dirichlet priors to
explore the structure of a language model from lower-order n-gram to higher-order n-
gram. There was no topic-based language model involved. In Wallach (2006), the LDA
bi-gram was proposed by considering a bag of bi-gram events from the collected docu-
ments in construction of an LDA. This LDA bi-gram was neither derived nor speciﬁcally
employed for speech recognition. The basic LDA model ignores the word order and is
not in accordance with sentence generation in speech recognition. In Tam & Schultz
(2005, 2006), the LDA model parameters {ˆα, ˆβ} were calculated from training doc-
uments D and then used to estimate the online topic probabilities or the variational
Dirichlet parameters ˆγ = { ˆγk}. The online parameters ˆγ were estimated by treating all
history words in a sentence wi−1
1
(Tam & Schultz 2005), or even the transcription of a
whole sentence wT
1 (Tam & Schultz 2006) as a single document w.
Similarly to the PLSA n-gram as illustrated in Section 3.7.3, the LDA n-gram is
formed as a soft-clustering model or a topic mixture model, which is calculated by
combining the topic probabilities p(zi = k|wi−1
1
) driven by history words wi−1
1
and the
topic-dependent unigrams β = {βkv} = {p(wi = v|zi = k)} of current word wi. The
combination is marginalized over different topics:
pLDA(wi = v|wi−1
1
) =
K

k=1
p(wi = v|zi = k)p(zi = k|wi−1
1
)
≈
K

k=1
βkv
ˆγk
K
j=1 ˆγj
.
(7.338)
Here, the topic multinomial distributions can be driven either by the history words
p(zi = k|wi−1
1
) or by the whole sentence words p(zi = k|wT
1 ). These multinomial
distributions are approximated and proportional to the variational Dirichlet parameters
ˆγ as calculated in Eq. (7.325). In this implementation, the pre-trained model parame-
ters {ˆα, ˆβ} and the online estimated variational parameters { ˆγ , ˆφ} should be calculated
to determine the LDA n-gram pLDA(wi|wi−1
1
) in an online fashion. In Tam & Schultz
(2005), the LDA language model was improved by further adapting the standard ML-
based n-gram model using the LDA n-gram according to a linear interpolation scheme
with a parameter 0 < λ < 1:
ˆp(wi|wi−1
1
) = λpML(wi|wi−1
i−n+1) + (1 −λ)pLDA(wi|wi−1
1
).
(7.339)
In Tam & Schultz (2006), the language model adaptation based on the unigram rescaling
was implemented by
ˆp(wi|wi−1
1
) = pML(wi|wi−1
i−n+1)pLDA(wi|wi−1
1
)
pML(wi)
.
(7.340)
Typically, the model parameters {ˆα, ˆβ} in the LDA language model are inferred at
the document level, catching the long-distance topic information but only indirectly
representing the n-gram events.

326
Variational Bayes
7.7.2
Dirichlet class language model
LDA (Blei et al. 2003) builds a hierarchical Bayesian topic model and extracts the latent
topics or clusters from a collection of M documents D = {wm}. The bag-of-words
scheme is adopted without considering the word sequence, and so it is not directly
designed for speech recognition where the performance is seriously affected by the prior
probability of a word string W = wT
1 ≜{w1, · · · , wT} or the language model p(W). In
Bengio, Ducharme, Vincent et al. (2003), the neural network language model was pro-
posed to deal with the data sparseness problem in language modeling by projecting the
ordered history vector into a continuous space and then calculating the n-gram language
model based on the multilayer perceptron. MLP involves the error back-propagation
training algorithm based on the least-squares estimation, which is vulnerable to the
overﬁtting problem (Bishop 2006). Considering the topic modeling in LDA and the con-
tinuous representation of history word sequence wi−1
i−n+1 in a neural network language
model, the Dirichlet class language model (DCLM) (Chien & Chueh 2011) is presented
to build a direct LDA language model for speech recognition. For a vocabulary with
|V| words, the n −1 history words wi−1
i−n+1 are ﬁrst represented by a (n −1)|V| × 1
vector hi−1
i−n+1 consisting of n −1 block subvectors. Each block is represented by the
1-to-|V| coding scheme with a |V| dimensional vector where the vth word of vocabu-
lary is encoded by setting the vth entry of the vector to be one and all the other entries
to be zero. The order of history words is considered in hi−1
i−n+1. Figure 7.3 shows the
system architecture of calculating a DCLM pDC(wi = v|wi−1
i−n+1). A global projection is
involved to project the ordered history vector hi−1
i−n+1 into a latent topic or class space
where the projection g(hi−1
i−n+1) could be either a linear function or a non-linear func-
tion. In the case of a linear function, a projection matrix A = [a1, · · · , aC] consisting of
Figure 7.3
System architecture for Dirichlet class language model.

7.7 Latent topic language model
327
Figure 7.4
Representation of a Dirichlet class language model.
C basis vectors {ac} is involved. The class structure of all n-gram events from training
corpus D = {wi, wi−1
i−n+1} is represented by Dirichlet distributions. The class uncertainty
is compensated by marginalizing the likelihood function over the Dirichlet priors. The
latent structure in the DCLM reﬂects the class of an n-gram event rather than the topic
in an LDA model. A DCLM is regarded as a kind of class-based language model, which
is inferred by the variational Bayes (VB) procedure by maximizing the variational lower
bound of a marginal likelihood of training data.
7.7.3
Model construction
A DCLM acts as a Bayesian class-based language model, which involves the prior dis-
tribution of the class variable. Figure 7.4 is a graphical representation of a DCLM from
a training set of current words and history words D = {wi, wi−1
i−n+1} = {wi, hi−1
i−n+1}. The
training corpus has H history events in {wi−1
i−n+1}. Each history event h = wi−1
i−n+1 has
Nh possible predicted words {wi}. Note that the (n −1)|V|-dimensional discrete history
vector is projected to a C-dimensional continuous class space using the class-dependent
linear discriminant function
gc(hi−1
i−n+1) = a⊺
c hi−1
i−n+1.
(7.341)
This function is used as the hyperparameter of a Dirichlet prior for the class mixture
probability θc or equivalently the class posterior probability of latent variable zi = c
given history vector hi−1
i−n+1:
θc ≜p(zi = c|θ, hi−1
i−n+1).
(7.342)
Thus, the uncertainty of class posterior probabilities g(hi−1
i−n+1) = {gc(hi−1
i−n+1)} is
characterized by a Dirichlet prior distribution:
θ = [θ1, · · · , θC]⊺|hi−1
i−n+1, A ∼Dir(g(hi−1
i−n+1))
= Dir(A⊺hi−1
i−n+1),
(7.343)
subject to the constraint gc(hi−1
i−n+1) > 0 or ac > 0. Each word wi = v is generated
by the conditional probability βcv ≜p(wi = v|zi = c) given a latent class zi = c.

328
Variational Bayes
The n-gram probability obtained using DCLM is calculated by marginalizing the joint
likelihood over the latent variables including class mixtures θ and C latent classes:
pDC(wi = v|hi−1
i−n+1, A, β) =
C

c=1
p(wi = v|zi = c, β)p(zi = c|hi−1
i−n+1, A)
=
C

c=1
p(wi = v|zi = c, β)
×

p(θ|hi−1
i−n+1, A)p(zi = c|θ)dθ
=
C

c=1
βcvE(θ)[p(zi = c|θ, hi−1
i−n+1, A)]
=
C

c=1
βcvE(θ)[θc|hi−1
i−n+1, A]
=
C

c=1
βcv
a⊺
c hi−1
i−n+1
C
j=1 a⊺
j hi−1
i−n+1
.
(7.344)
In Eq. (7.344), the integral is calculated over a multinomial variable p(zi
=
c|θ, hi−1
i−n+1, A)) with Dirichlet prior distribution p(θ|hi−1
i−n+1, A) and is equivalent to the
distribution mean. This integral is seen as a marginalization over different classes c, and
is also obtained as a class mixture model with the class-dependent unigram probabilities
{βcv|1 ≤c ≤C} weighted by the class mixture probabilities {p(zi = c|hi−1
i−n+1, A)|1 ≤
c ≤C} driven by the ordered history vector hi−1
i−n+1. However, we should estimate
DCLM parameters {A, β} and substitute them into Eq. (7.344) to calculate the Dirichlet
class (DC) n-gram probabilities.
7.7.4
VB inference: lower bound
Similarly to LDA, DCLM parameters {A, β} are estimated according to the type-2
maximum likelihood method where the marginal likelihood over latent variables is max-
imized. As seen in Figure 7.4, the latent variables in DCLM include the continuous
values of class mixture probabilities θ = {θc} and the discrete values of class labels
z = {zi}. The optimization problem is formulated as
{AML2, βML2} = arg max
A,β log p(D|A, β),
(7.345)
where
log p(D|A, β) =

{wi,hi−1
i−n+1}∈D
log p(wi|hi−1
i−n+1, A, β)
=

hi−1
i−n+1
log p(wh|hi−1
i−n+1, A, β)

7.7 Latent topic language model
329
=

hi−1
i−n+1
log
 Nh

i=1
p(wi|hi−1
i−n+1, A, β)

=

hi−1
i−n+1
log

p(θ|hi−1
i−n+1, A)
×
 Nh

i=1
C

c=1
p(wi = v|zi = c, β)p(zi = c|θ)

dθ

.
(7.346)
The log marginal likelihood is calculated by integrating over the continuous θ and sum-
ming the discrete {zi = c|1 ≤c ≤C}. However, directly optimizing Eq. (7.346)
is intractable because of the coupling between θ and β in the summation over latent
classes. The posterior distribution
p(θ, z|wh, hi−1
i−n+1, A, β) =
p(θ, z, wh|hi−1
i−n+1, A, β)
p(wh|hi−1
i−n+1, A, β)
(7.347)
becomes intractable for model inference. This posterior distribution is calculated from
the n-gram events with the Nh predicted words occurring after the ﬁxed history words
h ≜wi−1
i−n+1,
wh ≜{wi|c(hwi) > 0},
(7.348)
where c(·) denotes the count of an n-gram event. The variational Bayes (VB) inference
procedure is involved to construct the variational DCLM where the lower bound of
marginal likelihood in Eq. (7.346) serves as the surrogate to be maximized to ﬁnd the
optimal {AML2, βML2}.
To do so, a decomposed variational distribution,
q(θ, z|γ h, φh) = q(θ|γ h)
Nh

i=1
q(zi|φh,i),
(7.349)
is introduced to approximate the true posterior distribution p(θ, z|wh, hi−1
i−n+1, A, β). A
graphical representation of the variational DCLM is illustrated in Figure 7.5. Here,
γ h = {γh,c} and φh = {φh,ic} denote the history-dependent variational Dirichlet and
multinomial parameters, respectively. By referring to Section 7.6.2, the lower bound
on the log marginal likelihood is derived by applying the Jensen’s inequality and is
expanded by
Figure 7.5
Representation of a variational Dirichlet class language model.

330
Variational Bayes
L(γ h, φh; A, β) =

hi−1
i−n+1
.
E(θ)[log p(θ|hi−1
i−n+1, A)] + E(θ,z)[log p(z|θ)]
+ E(z)[log p(wh|z, β)] −E(θ)[log q(θ|γ h)] −E(z)[log q(z|φh)]
6
=

hi−1
i−n+1
.
log 
 C

c=1
a⊺
c hi−1
i−n+1

−
C

c=1
log 
(
a⊺
c hi−1
i−n+1
)
+
C

c=1
(
a⊺
c hi−1
i−n+1 −1
)
⎛
⎝(γh,c) −
⎛
⎝
C

j=1
γh,j
⎞
⎠
⎞
⎠
+
Nh

i=1
C

c=1
φh,ic
⎛
⎝(γh,c) −
⎛
⎝
C

j=1
γh,j
⎞
⎠
⎞
⎠
+
Nh

i=1
C

c=1
|V|

v=1
φh,icwv
i log βcv −log 
 C

c=1
γh,c

+
C

c=1
log (γh,c)
−
C

c=1
(γh,c −1)
⎛
⎝(γh,c) −
⎛
⎝
C

j=1
γh,j
⎞
⎠
⎞
⎠−
Nh

i=1
C

c=1
φh,ic log φh,ic
6
.
(7.350)
We have applied Eq. (7.321) to ﬁnd the variational lower bound for DCLM.
7.7.5
VB inference: parameter estimation
A VB–EM algorithm has been developed for inference of DCLM parameters. In this
VB–EM procedure, we ﬁrst conduct the VB E-step to ﬁnd the optimal expectation
function or lower bound L( ˆγ h, ˆφh; A, β) in Eq. (7.350), or equivalently to estimate the
optimal variational Dirichlet parameters ˆγ h and variational multinomial parameters ˆφh.
Similarly to Section 7.6.3, we collect the terms in Eq. (7.350) that are related to γ h
and maximize these terms, expressed by L(γ h), with respect to γ h to ﬁnd the optimal
variational Dirichlet parameters:
ˆγh,c = a⊺
c hi−1
i−n+1 +
Nh

i=1
φh,ic
1 ≤c ≤C.
(7.351)
In addition, when maximizing the lower bound L(γ h, φh; A, β) with respect to the vari-
ational multinomial parameters φh, we need to collect all terms related to φh and solve a
constrained optimization problem subject to a constraint for multinomial distributions:
C

c=1
φh,ic = 1.
(7.352)

7.7 Latent topic language model
331
The Lagrangian L(φ) given the history-dependent Lagrange multipliers {λh,i} is
arranged and maximized so as to ﬁnd the optimal variational multinomial distributions:
ˆφh,ic =
βcv exp
(
(γh,c) −
(C
j=1 γh,j
))
C
l=1 βlv exp
(
(γh,l) −
(C
j=1 γh,j
)),
(7.353)
1 ≤i ≤T,
1 ≤c ≤C
where the unique word v is selected for wi such that wv
i
= 1 and wl
i = 0 for
l ̸= v. The variational lower bound is updated with the optimal variational parameters
L( ˆγ h, ˆφh; A, β).
On the other hand, in a VB M-step, we ﬁx the variational parameters ˆγ h, ˆφh and
optimize the updated lower bound to estimate the DCLM model parameters:
{ ˆA, ˆβ} = arg max
{A,β} L( ˆγ h, ˆφh; A, β).
(7.354)
In estimating the conditional multinomial distributions β = {βcv} = {p(wi = v|zi = c)},
the terms containing model parameters β are collected and the constraints
|V|

v=1
βcv = 1
(7.355)
are imposed to arrange the Lagrangian L(β) with C Lagrange multipliers {λc}. By
solving
∂L(β)
∂βcv
= 0
(7.356)
and considering the constraints, we estimate the optimal conditional multinomial
distributions
ˆβcv =

hi−1
i−n+1
Nh
i=1 ˆφh,icwv
i
|V|
l=1

hi−1
i−n+1
Nh
i=1 ˆφh,icwl
i
(7.357)
1 ≤c ≤C,
1 ≤v ≤|V|
by substituting the updated variational multinomial distributions ˆφh = { ˆφh,ic}. However,
there is no closed-form solution to optimal DCLM parameter A = [a1, · · · , aC]. This
parameter is used to project the ordered history vector hi−1
i−n+1 into latent class space.
The projected parameter is treated as the Dirichlet class parameter. We may apply the
Newton–Raphson algorithm or simply adopt the gradient descent algorithm
a(τ+1)
c
= a(τ)
c
−η∇L(a(τ)
c )
1 ≤c ≤C
(7.358)
to derive the optimal Dirichlet class parameter ˆA by using the gradient

332
Variational Bayes
∇acL( ˆγ , ˆφ; A, β) =

hi−1
i−n+1
.

⎛
⎝
C

j=1
a⊺
j hi−1
i−n+1
⎞
⎠
−
(
a⊺
c hi−1
i−n+1
)
+ ( ˆγh,c) −
⎛
⎝
C

j=1
ˆγh,j
⎞
⎠
6
· hi−1
i−n+1.
(7.359)
Given the estimated model parameters { ˆA, ˆβ}, the DCLM n-gram pDC(wi
=
v|hi−1
i−n+1, A, β) in Eq. (7.344) is implemented. The DCLM n-gram could be improved
by interpolating with the modiﬁed Kneser–Ney (MKN) language model, as discussed
in Eq. (3.251). However, the performance of DCLM is limited due to the modeling of
Dirichlet classes only inside the n-gram window.
7.7.6
Cache Dirichlet class language model
In general, the long-distance information outside the n-gram window is not captured.
This weakness can be compensated in the cache DCLM (Chien & Chueh 2011). The
cache DCLM treats all history words wi−1
1
as cache memory and incorporates their class
information zi−1
1
= {z1, · · · , zi−1} into language modeling as
p(wi = v|hi−1
i−n+1, A, β, wi−1
1
)
=

zi−1
1
p(zi−1
1
|wi−1
1
)p(wi|zi−1
1
, hi−1
i−n+1, A, β)
=

zi−1
1
p(zi−1
1
|wi−1
1
)
C

c=1
p(wi = v|zi = c, β)
×

p(θ|hi−1
i−n+1, A, zi−1
1
)p(zi = c|θ)dθ,
(7.360)
where the marginalization over latent classes c and class mixtures θ is performed. We
have the posterior distribution
p(θ|hi−1
i−n+1, A, zi−1
1
) =
p(θ|hi−1
i−n+1, A)p(zi−1
1
|θ, hi−1
i−n+1, A)
p(zi−1
1
|hi−1
i−n+1, A)
,
(7.361)
where
p(zi−1
1
|θ, hi−1
i−n+1, A) = p(zi−1
1
|θ)
=
C

c=1
θ
i−1
j=1 δ(zj,c)
c
.
(7.362)
The denominator term of this posterior distribution is independent of wi and could be
ignored in calculating the cache DCLM in Eq. (7.360). For practical purposes, the sum-
mation over zi−1
1
is simpliﬁed by adopting a single best class sequence ˆzi−1
1
, where the

7.7 Latent topic language model
333
best class ˆzi−1 of word wi−1 is detected from the previous n−1 words wi−2
i−n and the best
classes ˆzi−2
1
corresponding to previous i −1 words, namely
ˆzi−1 = arg max
zi−1 p(wi−1 = v, zi−1|ˆzi−2
1
, wi−2
i−n, A, β)
= arg max
c
βcv
a⊺
c hi−2
i−n + i−2
j=1 δ(ˆzj, c)
C
l=1
+
a⊺
l hi−2
i−n + i−2
j=1 δ(ˆzj, l)
,.
(7.363)
A detailed derivation is given in Chien & Chueh (2011).
As a result, the recursive detection from ˆz1 to ˆzi−1 is done and applied to approximate
the cache DCLM as
p(wi = v|hi−1
i−n+1, A, β, wi−1
1
)
≈
C

c=1
p(wi = v|ˆzi = c, β)

p(θ|hi−1
i−n+1, A)
×
C

m=1
θ
i−1
j=1 δ(ˆzj,m)
m
p(zi = c|θ)dθ
≈
C

c=1
βcv
a⊺
c hi−1
i−n+1 + ρ i−1
j=1 τ i−j−1δ(ˆzj, c)
C
l=1
+
a⊺
l hi−1
i−n+1 + ρ i−1
j=1 τ i−j−1δ(ˆzj, l)
,.
(7.364)
Here, the product of the Dirichlet distribution p(θ|hi−1
i−n+1, A) and the multinomial
distribution *C
m=1 θ
i−1
j=1 δ(ˆzj,m)
m
is a new Dirichlet distribution. Taking the integral in
Eq. (7.364) is equivalent to ﬁnding the mean of the new Dirichlet distribution. In this
cache DCLM, we introduce a weighting factor 0 < ρ ≤1 to balance two terms in the
numerator and the denominator and a forgetting factor o < τ ≤1 to discount the distant
class information. A graphical representation of the cache DCLM is given in Figure 7.6.
The best classes {ˆz1, · · · ,
ˆ
zi−1} corresponding to all history words {w1, · · · , wi−1} are
recursively detected and then merged in prediction of the next word wi.
Figure 7.6
Representation of a cache Dirichlet class language model.

334
Variational Bayes
Table 7.3 Comparison of frequently used words of the latent variables extracted by LDA LM and DCLM.
Topic/class
Frequently used words in latent topics or classes
LDA LM
Family
toy, kids, theater, event, season, shoe, teen, children’s, plays,
ﬁlms, sports, magazines, Christmas, bowling, husband,
anniversary, girls, festival, couple, parents,wife, friends
Election
candidates, race, voters, challenger, democrat, state’s,
selection, county, front, delegates, elections, reverend,
republicans, polls, conventions, label, politician, ballots
War
troops, killed, Iraqi, attack, ship, violence, ﬁghting, soldiers,
mines, Iranian, independence, marines, revolution, died,
nation, protect, armed, democracy, violent, commander
DCLM
Quantity
ﬁve, seven, two, eight, cents, six, one, nine, four, three,
zero, million, point, percent, years, megabyte, minutes,
milligrams, bushels, miles, marks, pounds, yen, dollars
Business
exchange, prices, futures, index, market, sales, revenue,
earnings, trading, plans, development, business, funds,
organization, traders, ownership, holdings, investment
In+
addition, the, fact, American, October, recent, contrast,
Europe, June, Tokyo, July, March, turn, other, my,
Washington, order, Chicago, case, China, general, which
7.7.7
System performance
The Wall Street Journal (WSJ) corpus was utilized to evaluate different language models
for continuous speech recognition (CSR). The SI-84 training set was adopted to estimate
the HMM parameters. The feature vector consisted of 12 MFCCs and one log energy
and their ﬁrst and second derivatives. Triphone models were built for 39 phones and
one background silence. Each triphone model had three states with eight Gaussian com-
ponents. The 1987-1989 WSJ corpus with 38M words was used to train the baseline
backoff trigrams. A total of 86K documents and 3M trigram histories were used. The
20K non-verbalized punctuation, closed vocabularies were adopted. A total of 333 test
utterances were sampled from the November 1992 ARPA CSR benchmark test data.
In the implementation, the baseline tri-gram was used to generate 100-best lists. Vari-
ous language models were interpolated with a baseline tri-gram using an interpolation
weight, and were employed for N-best rescoring. The neural network LM (NNLM)
(Bengio et al. 2003), class-based LM (Brown et al. 1992), PLSA LM (Gildea & Hof-
mann 1999), LDA LM (Tam & Schultz 2005, Tam & Schultz 2006), DCLM, and cache
DCLM (Chien & Chueh 2011) were evaluated in the comparison.
Table 7.3 lists some examples of latent topics and classes, and the corresponding
frequently used words. The three topics and classes were selected from LDA LM with
K = 100 and DCLM with C = 100, respectively. The frequently used words were
identiﬁed according to the likelihood of the words given target topics or classes. We
can see that the frequently used words within a topic or class are semantically close to

7.8 Summary
335
Table 7.4 Comparison of word error rates for different methods with different sizes of training data and
numbers of classes.
Size of training data
6M
12M
18M
38M
Baseline LM
39.2%
21.3%
15.8%
12.9%
NNLM
35.5%
19.6%
15.0%
12.4%
Class-based LM
35.5%
19.7%
15.0%
12.4%
PLSA LM
36.0%
19.8%
15.0%
12.3%
LDA LM
35.9%
19.7%
14.7%
12.2%
DCLM (C=200)
35.9%
19.6%
14.6%
12.0%
Cache DCLM (C=200)
34.2%
19.3%
14.5%
11.9%
DCLM (C=500)
35.2%
19.2%
14.3%
11.7%
Cache DCLM (C=500)
33.9%
19.0%
14.2%
11.6%
each other. For some cases, the topically related words from LDA LM appear indepen-
dently. These topics are not suitable for generating natural language. In contrast, DCLM
extracts the class information from n-gram events and performs the history clustering
based on sentence generation. For example, the latent class “In+” denotes the category
of words that follow the preposition “in.” The words “fact,” “addition,” and ”June” usu-
ally follow the word “in,” and appear as frequent words of the same class. The word
order is reﬂected in the clustering procedure. Table 7.4 reports the word error rates for
baseline LM, NNLM, class-based LM, PLSA LM, LDA LM, DCLM and cache DCLM
with C = 200 and C = 500 under different sizes of training data. The issue of small
sample size is examined. It is consistent that the word error rate is reduced when the
amount of training data is increased. In the case of sparse training data (6M), the topic-
based and class-based methods work well. In particular, the cache DCLM with C = 200
achieved an error rate reduction of 12.9% which outperforms the other related methods.
When the number of classes is increased to C = 500, the improvement is obvious for
the case of large training data (38M).
7.8
Summary
This chapter has introduced various applications of VB to speech and language pro-
cessing, including CDHMM-based acoustic modeling, acoustic model adaptation, latent
topic models, and latent topic language models. Compared with the previous inference
approximations based on MAP, evidence, and asymptotic approximations, VB deals
with Bayesian inference based on a distribution estimation without considering the
asymptotic property, which often provides better solutions in terms of consistently using
the Bayesian manner. In addition, the inference algorithm obtained can be regarded as an
extension of the conventional EM type iterative algorithm. This makes implementation
easier as we can utilize existing source codes based on the ML and MAP–EM
algorithms. One of the difﬁculties in VB is that obtaining the analytical solutions of VB

336
Variational Bayes
posterior distributions and variational lower bound is hard due to the complicated expec-
tation and integral calculations. However, the solutions provided in this chapter (general
formulas and speciﬁc solutions for acoustic and language modeling issues) would cover
most of the mathematical analysis in the other VB applications in speech and language
processing. Actually, there have been various other aspects of VB including speech
feature extraction (Kwon, Lee & Chan 2002, Valente & Wellekens 2004a), voice activ-
ity detection (Cournapeau, Watanabe, Nakamura et al. 2010), speech/speaker GMM
(Valente 2006, Pettersen 2008), speaker diarization (Valente, Motlicek & Vijayasenan
2010, Ishiguro, Yamada, Araki et al. 2012), and statistical speech synthesis (Hashimoto,
Nankaku & Tokuda 2009, Hashimoto, Zen, Nankaku et al. 2009). Readers who are inter-
ested in these topics could follow these studies and develop new techniques with the
solutions provided in this chapter.
Although VB provides a fully Bayesian treatment, VB still cannot overcome the prob-
lem of local optimum solutions based on the EM style algorithm, and VB also cannot
provide analytical solutions for non-exponential family distributions. The next chap-
ter deals with MCMC-based Bayesian approaches, which potentially overcome these
problems with a fully Bayesian treatment.

8
Markov chain Monte Carlo
For most probabilistic models of practical interest, exact inference is intractable, and so
we have to resort to some form of approximation. Markov chain Monte Carlo (MCMC)
is another realization of full Bayesian treatment of practical Bayesian solutions (Neal
1993, Gilks, Richardson & Spiegelhalter 1996, Bishop 2006). MCMC is known as a
stochastic approximation which acts differently from the deterministic approximation
based on variational Bayes (VB) as addressed in Chapter 7. Variational inference using
VB approximates the posterior distribution through factorization of the distribution over
multiple latent variables and scales well to large applications. MCMC uses the numerical
sampling computation rather than solving integrals and expectation analytically. Since
MCMC can use any distributions in principle, it is capable of wide applications, and can
be used for Bayesian nonparametric (BNP) learning, which provides highly ﬂexible
models whose complexity grows appropriately with the amount of data. Although, due
to the computational cost, the application of MCMC to speech and language processing
is limited to small-scale problems currently, this chapter describes promising new direc-
tions of Bayesian nonparametrics for speech and language processing by automatically
growing models to deal with speaker diarization, acoustic modeling, language acqui-
sition, and hierarchical language modeling. The strengths and weaknesses using VB
and MCMC are complementary. In what follows, we ﬁrst introduce the general back-
ground of sampling methods including MCMC and Gibbs sampling algorithms. Next,
the Bayesian nonparametrics are calculated to build a ﬂexible topic model based on the
hierarchical Dirichlet process (HDP) (Teh et al. 2006). Several applications in speech
and language processing areas are surveyed. GMM-based speaker clustering, CDHMM-
based acoustic unit discovery by using MCMC, and the language model based on the
hierarchical Pitman–Yor process (Teh et al. 2006) are described.
The fundamental problem in MCMC involves ﬁnding the expectation of some func-
tion f(θ) with respect to a probability distribution p(θ) where the components of θ might
comprise discrete or continuous variables, which are some factors or parameters to be
inferred under a probabilistic model. In the case of continuous variables, we would like
to evaluate the expectation
E(θ)[f(θ)] =

f(θ)p(θ)dθ,
(8.1)
where the integral is replaced by summation in the case of discrete variables. We assume
that such expectations are too complicated to be evaluated analytically. The general

338
Markov chain Monte Carlo
idea behind sampling methods is to obtain a set of samples {θ(l), l = 1, · · · , L} drawn
independently from the distribution p(θ). We may approximate the integral by a sample
mean of function f(θ) over these samples θ(l):
5f = 1
L
L

l=1
f(θ(l)).
(8.2)
Since the samples θ(l) are drawn from the distribution p(θ), the estimator5f has the cor-
rect mean, i.e.,5f = E(θ)[f(θ)]. In general, ten to twenty independent samples may sufﬁce
to estimate an expectation. However, the samples {θ(l)} may not be drawn independently.
The effective sample size might be much smaller than the apparent sample size L. This
implies that a relatively large sample size is required to achieve sufﬁcient accuracy.
8.1
Sampling methods
A simple strategy can be designed to generate random samples from a given distribu-
tion. We ﬁrst consider how to generate random numbers from non-uniform distributions,
assuming that we already have a source of uniformly distributed random numbers. Let
θ be uniformly distributed by p(θ) = 1 over the interval (0, 1). We transform the values
of θ using some function f(·) by y = f(θ). The distributions of variables θ and y are
related by
p(y) = p(θ)
----
dθ
dy
---- .
(8.3)
Taking integrals for both sides of Eq. (8.3), we have
θ = h(y) =
 y
−∞
p(/y)d/y,
(8.4)
which is the indeﬁnite integral of p(y). Thus, y = h−1(θ), meaning that we have to trans-
form the uniformly distributed random numbers θ using a function h−1(·), which is the
inverse of the indeﬁnite integral of the desired distribution of y. Figure 8.1 depicts the
geometrical interpretation of the transformation method for generating non-uniformly
distributed random numbers. In addition, the generalization to multiple variables is
straightforward and involves the Jacobian of the transform of variables:
p(y1, · · · , yM) = p(θ1, · · · , θM)
----
∂(θ1, · · · , θM)
∂(y1, · · · , yM)
---- .
(8.5)
A similar scheme can be applied to draw a multivariate distribution with M variables.
8.1.1
Importance sampling
The technique of importance sampling provides a framework for approximating the
expectation in Eq. (8.1) directly but does not provide the mechanism for drawing sam-
ples from distribution p(θ). The ﬁnite sum approximation to expectation in Eq. (8.2)

8.1 Sampling methods
339
Figure 8.1
Transformation method for generating non-uniformly distributed random numbers from
probability distribution p(y). Function h(y) represents the indeﬁnite integral of p(y). Adapted
from Bishop (2006).
depends on being able to draw samples from the distribution p(θ). One simple strategy
for evaluating expectation function would be to discretize θ-space into a uniform grid,
and to evaluate the integrand as a sum of the form
E(θ)[f(θ)] ≃
L

l=1
p(θ(l))f(θ(l)).
(8.6)
However, the problem with this approach is that the number of terms in the summation
grows exponentially with the dimensionality of θ. In many cases, the probability distri-
butions of interest often have much of their mass conﬁned to relatively small regions of
θ space. Accordingly, uniform sampling is very inefﬁcient because in high-dimensional
space, only a very small proportion of the samples make a signiﬁcant contribution to the
sum. We would like to sample the points falling in regions where p(θ) is large, or where
the product p(θ)f(θ) is large.
Suppose we wish to sample from a distribution p(θ) that is not simple or a stan-
dard distribution. Sampling directly from p(θ) is difﬁcult. Importance sampling is based
on the use of a proposal distribution q(θ) from which it is easy to draw samples.
Figure 8.2 illustrates the proposal distribution for importance sampling. The expecta-
tion in Eq. (8.1) is expressed in the form of a ﬁnite sum over samples {θ(l)} drawn from
q(θ):
E(θ)[f(θ)] =

f(θ)p(θ)dθ
=

f(θ)p(θ)
q(θ)q(θ)dθ
≃1
L
L

l=1
p(θ(l))
q(θ(l))
f(θ(l)).
(8.7)
The quantities rl = p(θ(l))/q(θ(l)) are known as the importance weights, and they correct
the bias introduced by sampling from the wrong distribution.
Usually, the distribution p(θ) can only be evaluated up to a normalization constant,
so that p(θ) = /p(θ)/Zp, where/p(θ) can be evaluated easily and Zp is unknown. We may
use the importance sampling distribution q(θ) = /q(θ)/Zq to determine the expectation
function:

340
Markov chain Monte Carlo
Figure 8.2
Proposal distribution q(θ) for importance sampling in estimation of expectation of function f(θ)
with probability distribution p(θ). Adapted from Bishop (2006).
E(θ)[f(θ)] = Zq
Zp

f(θ)/p(θ)
/q(θ)q(θ)dθ
≃Zq
Zp
1
L
L

l=1
/rlf(θ(l)),
(8.8)
where/rl = /p(θ(l))//q(θ(l)). We have
Zp
Zq
= 1
Zq

/p(θ)dθ =
 /p(θ)
/q(θ)q(θ)dθ
≃1
L
L

l=1
/rl.
(8.9)
We then obtain
E(θ)[f(θ)] ≃
L

l=1
wlf(θ(l)),
(8.10)
where we deﬁne
wl =
/rl

m/rm
=
/p(θ(l))/q(θ(l))

m/p(θ(m))/q(θ(m))
.
(8.11)
Clearly, the performance of the importance sampling method highly depends on how
well the sampling distribution q(θ) matches the desired distribution p(θ).
8.1.2
Markov chain
One major weakness in evaluation of expectation function based on the importance
sampling strategy is the severe limitation in spaces of high dimensionality. We accord-
ingly turn to a very general and powerful framework called Markov chain Monte Carlo
(MCMC), which allows sampling from a large class of distributions and which scales
well with the dimensionality of the sample space. Before discussing MCMC methods in
more detail, it is useful to study some general properties of Markov chains and investi-
gate under what conditions a Markov chain can converge to the desired distribution. A

8.1 Sampling methods
341
ﬁrst-order Markov chain is deﬁned for a series of variables θ(1), · · · , θ(M) such that the
following conditional independence holds for m ∈{1, · · · , M −1}:
p(θ(m+1)|θ(1), · · · , θ(m)) = p(θ(m+1)|θ(m)).
(8.12)
This Markov chain starts from the probability distribution for initial variable p(θ(0)) and
operates with the transition probability p(θ(m+1)|θ(m)). A Markov chain is called homo-
geneous if the transition probabilities are unchanged for all m. The marginal probability
for a variable θ(m+1) is expressed in terms of the marginal probabilities over the previous
variable {θ(1), · · · , θ(m)} in the chain,
p(θ(m+1)) =

θ(m)
p(θ(m+1)|θ(m))p(θ(m)).
(8.13)
A distribution is said to be invariant or stationary with respect to a Markov chain if each
step in the chain keeps the distribution invariant. For a homogeneous Markov chain with
transition probability T(θ′, θ), the distribution p⋆(θ) is invariant if it has the following
property:
p⋆(θ) =

θ′
T(θ′, θ)p⋆(θ′).
(8.14)
Our goal is to use Markov chains to sample from a given distribution. We can achieve
this goal if we set up a Markov chain such that the desired distribution is invariant. It
is required that for m →∞, the distribution p(θ(m)) converges to the required invariant
distribution p⋆(θ), which is obtained irrespective of the choice of initial distribution
p(θ(0)). This invariant distribution is also called the equilibrium distribution. A sufﬁcient
condition for an invariant distribution p(θ) is to choose the transition probabilities to
satisfy the detailed balance, i.e.
p⋆(θ)T(θ, θ′) = p⋆(θ′)T(θ′, θ),
(8.15)
for a particular distribution p⋆(θ).
8.1.3
The Metropolis–Hastings algorithm
As discussed in importance sampling, we keep sampling from a proposal distribution
and maintain a record of the current state θ(τ). The proposal distribution q(θ|θ(τ))
depends on this current state. The sequence of samples θ(1), θ(2), · · · forms a Markov
chain. The proposal distribution is chosen to be sufﬁciently simple to draw samples
directly. At each sampling cycle, we generate a candidate sample θ⋆from the proposal
distribution and then accept the sample according to an appropriate criterion. In a basic
Metropolis algorithm (Metropolis, Rosenbluth, Rosenbluth et al. 1953), the proposal
distribution is assumed to be symmetric, namely q(θa|θb) = q(θb|θa) for all values of
θa and θb. The candidate sample is accepted with the probability
A(θ⋆, θ(τ)) = min

1, /p(θ⋆)
/p(θ(τ))

,
(8.16)

342
Markov chain Monte Carlo
where p(θ) = /p(θ)/Zp with a readily evaluated distribution /p(θ) and an unknown
normalization value Zp. To fulﬁl this algorithm, we can choose a random number
u with uniform distribution over the unit interval (0, 1) and accept the sample if
A(θ⋆, θ(τ)) > u. Deﬁnitely, if the step from θ(τ) to θ⋆causes an increase in the
value of p(θ), then the candidate point is accepted. Once the candidate sample is
accepted, then θ(τ+1) = θ⋆, otherwise the candidate sample θ⋆is discarded, θ(τ+1)
is set to θ(τ). The next candidate sample is drawn from the distribution q(θ|θ(τ+1)).
This leads to multiple copies of samples in the ﬁnal list of samples. As long as
q(θa|θb) is positive for any values of θa and θb, the distribution of θ(τ) approaches
p(θ) as τ →∞.
The basic Metropolis algorithm is further generalized to the Metropolis–Hastings
algorithm (Hastings 1970) which is widely adopted in MCMC inference. This gener-
alization is developed by relaxing the assumption in the Metropolis algorithm that the
proposal distribution is no longer a symmetric function of its arguments. Using this
algorithm, at step τ with current state θ(τ), we draw a sample θ⋆from the proposal
distribution qk(θ|θ(τ)) and then accept it with the probability
Ak(θ⋆, θ(τ)) = min

1, /p(θ⋆)qk(θ(τ)|θ⋆)
/p(θ(τ))qk(θ⋆|θ(τ))

,
(8.17)
where k denotes the members of the set of possible transitions. For the case of a symmet-
ric proposal distribution, the Metropolis–Hastings criterion in Eq. (8.17) is reduced to
the Metropolis criterion in Eq. (8.16). We can show that p(θ) is an invariant distribution
of the Markov chain generated by the Metropolis–Hastings algorithm by investigating
the property of the detailed balance in Eq. (8.15). We ﬁnd that
p(θ)qk(θ|θ′)Ak(θ′, θ) = min(p(θ)qk(θ|θ′), p(θ′)qk(θ′|θ))
= min(p(θ′)qk(θ′|θ), p(θ)qk(θ|θ′))
= p(θ′)qk(θ′|θ)Ak(θ, θ′).
(8.18)
The choice of proposal distribution is important in an MCMC algorithm. For continu-
ous state spaces, a common choice is a Gaussian centered on the current state, leading to
an important trade-off in determining the variance parameter of this distribution. If the
variance is small, the proportion of accepted transitions is high, but a slow random walk
is taken through the state space. On the other hand, if the variance parameter is large,
the rejection rate is high because the updated state has low probability p(θ). Figure 8.3
shows a schematic for selecting an isotropic Gaussian proposal distribution to sample
random numbers from a correlated multivariate Gaussian distribution. In order to keep
the rejection rate low, the scale ρ of the proposal distribution qk(θ|θ(τ)) should be com-
parable to the smallest standard deviation σmin, which leads to a random walk so that the
number of steps for separating states is of order (σmax/σmin)2 where σmax is the largest
standard deviation.

8.1 Sampling methods
343
Figure 8.3
Using an isotropic Gaussian proposal distribution (circle) to sample random numbers for a
correlated bivariate Gaussian distribution (ellipse). Adapted from Bishop (2006).
8.1.4
Gibbs sampling
Gibbs sampling (Geman & Geman 1984, Liu 2008) is a simple and widely applicable
realization of an MCMC algorithm which is a special case of the Metropolis–Hastings
algorithm. Consider the distribution of M random variables p(θ) = p(θ1, · · · , θM) and
suppose that we have some initial state for the Markov chain. Each step of the Gibbs
sampling procedure replaces the value of one of the variables by a value drawn from the
distribution of that variable conditioned on the values of the remaining states. That is to
say, we replace the ith component θi by a value drawn from the distribution p(θi|θ−i),
where θ−i denotes θ1, · · · , θM but with θi omitted. The sampling procedure is repeated
by cycling through the variables in a particular order or in a random order from some
distribution. This procedure samples the required distribution p(θ), which should be
invariant at each of the Gibbs sampling steps or in the whole Markov chain. This is
because the marginal distribution p(θ−i) is invariant and the conditional distribution
p(θi|θ−i) is correct at each sampling step. Gibbs sampling of M variables for T steps
follows this procedure:
• Initialize {θi : i = 1, · · · , M}.
• For τ = 1, · · · , T:
– Sample θ(τ+1)
1
∼p(θ1|θ(τ)
2 , θ(τ)
3 , · · · , θ(τ)
M ).
– Sample θ(τ+1)
2
∼p(θ2|θ(τ+1)
1
, θ(τ)
3 , · · · , θ(τ)
M ).
...
– Sample θ(τ+1)
j
∼p(θj|θ(τ+1)
1
, · · · , θ(τ+1)
j−1
, θ(τ)
j+1, · · · , θ(τ)
M ).
...
– Sample θ(τ+1)
M
∼p(θM|θ(τ+1)
1
, θ(τ+1)
2
, · · · , θ(τ+1)
M−1 ).
Gibbs sampling can be shown to be a special case of the Metropolis–Hastings algo-
rithm. Consider the Metropolis–Hastings sampling step involving the variable θk in
which the remaining variables θ−k are ﬁxed. The transition probability from θ to θ⋆is
then given by qk(θ⋆|θ) = p(θ⋆
k |θ−k), because the remaining variables are unchanged by
the sampling step, and (θ⋆)−k = θ−k. By using p(θ) = p(θk|θ−k)p(θ−k), the acceptance
probability in the Metropolis–Hastings algorithm is obtained by

344
Markov chain Monte Carlo
A(θ⋆, θ) = p(θ⋆)qk(θ|θ⋆)
p(θ)qk(θ⋆|θ)
= p(θ⋆
k |(θ⋆)−k)p((θ⋆)−k)p(θk|(θ⋆)−k)
p(θk|θ−k)p(θ−k)p(θ⋆
k |θ−k)
= 1,
(8.19)
where (θ⋆)−k = θ−k is applied. This result indicates that the sampling steps in the
Metropolis–Hastings algorithm are always accepted.
Collapsed Gibbs sampling
Collapsed Gibbs sampling (Liu 1994, Grifﬁths & Steyvers 2004) is a method for using a
marginal conditional distribution where some of variables are integrated out, instead of
sampling. For example, suppose we have a set of variables , the proposal distribution
in the Gibbs sampling is represented as follows:
p(θi|θ−i) =

p(θi|θ−i, )p()d.
(8.20)
p() is a prior distribution. This integral can be analytically solved when we use
a conjugate prior, and the following sections sometimes use a marginal conditional
distribution.
8.1.5
Slice sampling
One weakness in the Metropolis–Hastings algorithm is the sensitivity to step size. If this
is too small, the result has slow decorrelation due to random walk behavior, while if
it is too large, the sampling procedure is not efﬁcient due to a high rejection rate. The
slice sampling approach (Neal 2003) provides an adaptive step size which is automati-
cally adjusted to ﬁt the characteristics of the distribution. Again, it is required that the
unnormalized distribution/p(θ) is available to be evaluated. Consider the univariate case.
Slice sampling is performed by augmenting θ with an additional variable u and drawing
samples from the joint space of (θ, u). The goal is to sample uniformly from the area
under the distribution given by
/p(θ, u) =
. 1/Zp
if 0 ⩽u ⩽/p(θ),
0
otherwise,
(8.21)
where Zp =

/p(θ)dθ. The marginal distribution of θ is given by

/p(θ, u)du =
 /p(θ)
0
1
Zp
du = /p(θ)
Zp
= p(θ).
(8.22)
To carry out this scheme, we ﬁrst sample from p(θ) by sampling from /p(θ, u) and
then neglecting the u values. Alternatively sampling θ and u can be achieved. Given the
value of θ, we evaluate /p(θ) and then sample u uniformly in the range 0 ⩽u ⩽/p(θ).
We then ﬁx u and sample θ uniformly from the “slice” through the distribution deﬁned
by {θ : /p(θ) > u}. As illustrated in Figure 8.4(a), for a given value θ(τ), a value of u
is chosen uniformly in the region 0 ⩽u ⩽/p(θ(τ)), which deﬁnes a slice through the

8.2 Bayesian nonparametrics
345
Figure 8.4
Slice sampling over a distribution p(θ) through: (a) ﬁnding the slice containing current sample
θ(τ) with/p(θ) > u; and (b) detecting the region of interest with two end points θmin and θmax so
that we can uniformly draw a new sample θ(τ+1) within the slice. Adapted from Bishop (2006).
distribution, shown by solid horizontal lines. Because it is infeasible to sample directly
from a slice, a new sample of θ is drawn from a region θmin ⩽θ ⩽θmax, which contains
the previous value θ(τ), as illustrated in Figure 8.4(b). We want the region to encompass
as much of the slice as possible so as to allow large moves in θ space while having as
little as possible of this region lying outside the slice, because this makes the sampling
less efﬁcient.
We start with a region of width w which contains current sample θ(τ) and then judge
if both end points are within the slice. If either end point is within the slice, the region is
extended in this direction by increments of value w until the end point falls outside the
region. A candidate value θ⋆is then chosen uniformly from this region. If it lies inside
the slice, it forms new sample θ(τ+1). If it lies outside the slice, the region is shrunk such
that θ⋆forms an end point. A new candidate point is drawn uniformly from this reduced
region until a value of θ is found that lies within the slice. When applying slice sampling
to multivariate distributions, we repeatedly sample each variable in turn in the manner
of Gibbs sampling based on a conditional distribution p(θi|θ−i).
8.2
Bayesian nonparametrics
The sampling methods mentioned in Section 8.1 are widely employed to infer the
Bayesian nonparametrics (BNP) which are now seen as a new trend in speech and
language processing areas where the data representation is a particular concern and
is extensively studied. The BNP learning aims to deal with the issue that probabilis-
tic methods are often not viewed as sufﬁciently expressive because of a long list
of limitations and assumptions on probability distribution and ﬁxed model complex-
ity. It is attractive to pursue an expressive probabilistic representation with a less
assumption-laden approach to inference. We would like to move beyond the simple

346
Markov chain Monte Carlo
ﬁxed-dimensional random variables, e.g., multinomial distributions, Gaussians, and
other exponential family distributions, and to mimic the ﬂexibility in probabilistic rep-
resentations. BNP methods avoid the restrictive assumptions of parametric models by
deﬁning distributions on function spaces.
Therefore, the stochastic process, containing an indexed collection of random vari-
ables, provides the ﬂexibility to deﬁne probability distributions on spaces of probability
distributions. We relax the limitation of ﬁnite parameterizations and consider the mod-
els over inﬁnite-dimensional objects such as trees, lists, and collections of sets. The
expressive data structures could be explored by computationally efﬁcient reasoning and
learning through the so-called combinatorial stochastic processes (Pitman 2006). BNP
learning is accordingly developed from a Bayesian perspective by upgrading the pri-
ors in classic Bayesian analysis from parametric distributions to stochastic processes.
Prior distributions are then replaced by the prior process in BNP inference. The ﬂexible
Bayesian learning and representation is conducted from the prior stochastic process to
the posterior stochastic process. BNP involves the combinatorics of sums and products
over prior and posterior stochastic processes and automatically learned model structure
from the observed data. The model selection problem could be tackled by BNP learning.
8.2.1
Modeling via exchangeability
Some of the foundation of Bayesian inference is addressed in this section. The concept
of exchangeability is critical in motivating BNP learning. Consider a probabilistic model
with an inﬁnite sequence of random factors or parameters θ = {θ1, θ2, · · · } to be inferred
from observation data x = {x1, x2, · · · }, which could be either continuous such as the
speech features O or discrete such as the word labels W. We say that such a sequence is
inﬁnitely exchangeable if the joint probability distribution of any ﬁnite subset of those
random variables is invariant to permutation. For any N, we have
p(θ1, θ2, · · · , θN) = p(θπ(1), θπ(2), · · · , θπ(N)),
(8.23)
where π denotes a permutation. The assumption of exchangeability is weaker than that
of independence among random variables. This assumption often better describes the
data we encounter in realization of stochastic processes for BNP learning. De Finetti’s
theorem states that the inﬁnite sequence is exchangeable if and only if for any N random
variables the sequence has the following property (Bernardo & Smith 2009):
p(θ1, θ2, · · · , θN) =

N

i=1
p(θi|G)dP(G).
(8.24)
There exists an underlying random measure G and a distribution function P such that
random variables θi are conditionally independent given G, which is not restricted to be
a ﬁnite-dimensional object.
The P´olya urn model is a probability model for sequentially labeling the balls in an
urn. Consider an empty urn and a countably inﬁnite collection of colors. Randomly pick
a color according to some ﬁxed distribution G0 and place a ball having the same color in

8.2 Bayesian nonparametrics
347
the urn. For all subsequent balls, either choose a ball from the urn uniformly and return
that ball to the urn with another ball of the same color, or choose a new color from G0
and place a ball of that color k in the urn. We express this process mathematically by
p(θi = k|θ1, · · · , θi−1) ∝
. ck
if θj = k for some j ∈{1, · · · , i −1},
α0
otherwise,
(8.25)
where α0 > 0 is a parameter of the process and ck denotes the number of balls of color k.
Even though we deﬁne the model by considering a particular ordering of the balls, the
resulting distribution is independent of the order. It can be proved that the joint distribu-
tion p(θ1, θ2, · · · , θN) is written as a product of conditionals given in Eq. (8.25) and the
resulting expression is independent of the order of N random variables. The exchange-
ability in the P´olya urn model is conﬁrmed. Because of this property, by De Finetti’s
theorem, the existence of an underlying probability measure G renders the ball colors
conditionally independent. This random measure corresponds to a stochastic process
known as the Dirichlet process, which is introduced in Section 8.2.2.
The property of exchangeability is essential for an MCMC inference procedure. Let
us consider the joint distribution of θ and x given by
p(θ, x) = p(θ1, θ2, · · · , θN)
N

i=1
p(xi|θi),
(8.26)
which is viewed as the product of a prior in the ﬁrst term and a likelihood function in
the second term. The ﬁrst term p(θ1, θ2, · · · , θN) is modeled by the P´olya urn marginal
distributions. In particular, our goal is to sample θ from observation data x based on the
Gibbs sampling. The problem is to sample a particular component θi while all of the
other components are ﬁxed. Because the joint distribution of {θ1, · · · , θN} is invariant
to permutation, we can freely permute the vector to move θi to the end of the list. The
prior probability of the last component given all of the preceding components is given
by the urn model as given in Eq. (8.25). We multiply each of the distributions by the
likelihood function p(xi|θi) and integrate with respect to θi. We assume that the prior
measure G0 and the likelihood function are conjugate so that the integral can be done
in closed form. For each component, the derived result is the conditional distribution
of θi given the other components and given xi. This is done for different components
{θ1, · · · , θN} and the process iterates. This link between exchangeability and an efﬁcient
inference algorithm is important for BNP learning.
In addition, it is crucial to realize the P´olya urn model for Bayesian speech and lan-
guage processing over a speech and text corpus. The ball means a word wi or a feature
vector ot in the corpus and the ball color indicates the cluster label of this word or feature
vector. This P´olya urn model deﬁnes a distribution of acoustic features or word labels
which is not ﬁxed in dimensionality and can be used to induce a distribution on partitions
or clusterings. The distribution on partitions is known as the Chinese restaurant process
(Aldous 1985), which is addressed in Section 8.2.4. The Chinese restaurant process and
the P´olya urn model are viewed as the essentials of the BNP model for clustering where
the random partition provides a prior on clusterings and the color associated with a cell
can be represented by a distribution associated with a given cluster.

348
Markov chain Monte Carlo
8.2.2
Dirichlet process
The Dirichlet process (DP) (Ferguson 1973) plays a crucial role in BNP learning. It is a
stochastic process for a random probability measure G over a measurable space ,
G ∼DP(α0, G0),
(8.27)
such that, for any ﬁnite measurable partition (A1, A2, · · · , Ar), the random vector
(G(A1, · · · , G(Ar)) is distributed as a ﬁnite-dimensional Dirichlet distribution with
parameters (α0G0(A1), · · · , α0G0(Ar)), i.e.
(G(A1, · · · , G(Ar)) ∼Dir(α0G0(A1), · · · , α0G0(Ar)),
(8.28)
with two parameters, a positive scaling parameter or concentration parameter, α0 > 0,
and a base probability measure, G0 ∈. This process is a measure of measures over
space  where G() = 1. To realize the Dirichlet process in Eq. (8.27), an inﬁnite
sequence of points {φk} is drawn independently from the base probability measure G0
so that the probability measure of the process is established by
G =
∞

k=1
βkδφk
(8.29)
with probability 1 (Sethuraman 1994). In Eq. (8.29), δφk is an atom or a unit mass at the
point φk and {βk} are the random weights which depend on the concentration parameter
α0. Note that the Dirichlet process G is random in two ways. One random process is for
weights βk while the other is for locations φk. We can say that Dirichlet process G in
Eq. (8.29) has the Dirichlet marginals as given in Eq. (8.28). According to De Finetti’s
theorem, the DP is seen as the De Finetti mixture distribution underlying the P´olya urn
model as addressed in Section 8.2.1. DP can be presented from the perspectives of the
stick-breaking construction, the P´olya urn scheme, and a limit of ﬁnite mixture models
which is detailed in Sections 8.2.3, 8.2.4, and 8.2.5 respectively.
8.2.3
DP: Stick-breaking construction
The stick-breaking construction for DP was presented by Sethuraman (1994). In general,
the DP and the stick-breaking process (SBP) are essential tools in BNP. Consider the
stick-breaking weights {βk}∞
k=1 on a countably inﬁnite set. We want to ﬁnd a distribution
of the non-negative mixture weights β1, β2, · · · having the property
∞

k=1
βk = 1.
(8.30)
One solution to this problem is provided by a procedure known as “stick-breaking.”
Considering a unit-length stick, we independently draw a proportion β′
k in the kth break
from a Beta distribution with a concentration parameter α0:
β′
k|α0 ∼Beta(1, α0)
k = 1, 2, · · · .
(8.31)

8.2 Bayesian nonparametrics
349
Figure 8.5
The stick-breaking process.
Each break decides a proportion β′
k while the proportion of the remaining stick is 1−β′
k.
The mixture weight of the ﬁrst component is β1 = β′
1 and that of the kth component is
determined by
βk = β′
k
k−1

l=1
(1 −β′
l)
k = 1, 2, · · · .
(8.32)
Under this SBP, it is straightforward to show the property of mixture weights that sum
up to one with probability one. Figure 8.5 illustrates an inﬁnite sequence of segments βk
from SBP. Let us deﬁne an inﬁnite sequence of independent random variables {βk}∞
k=1
and {φk}∞
k=1 where
φk|G0 ∼G0.
(8.33)
Then, the probability measure G deﬁned in Eq. (8.29) can be shown to be the measure
distributed according to DP(α0, G0) (Sethuraman 1994). We may interpret the sequence
β = {βk}∞
k=1 as a random probability measure on the positive integers. This measure is
formally denoted by the GEM distribution (Pitman 2002)
β ∼GEM(α0).
(8.34)
8.2.4
DP: Chinese restaurant process
The second perspective on the DP is provided by the P´olya urn scheme (Blackwell &
MacQueen 1973) showing that the draws from DP are both discrete and exhibit a clus-
tering property. Let θ1, θ2, · · · be a sequence of independently and identically distributed
(iid) random factors or parameters drawn from G for individual observations x1, x2, · · ·
under some distribution function. The probability model over the inﬁnite sequence is
written as
θi|G ∼G
xi|θi ∼p(xi|θi)
for each i.
(8.35)
The factors θ1, θ2, · · · are conditionally independent given G, and hence are exchange-
able. Consider the successive conditional distributions of θi of xi given the previous
factors θ1, · · · , θi−1 of observations x1, · · · , xi−1, where G has been integrated out. It
was shown that these conditional distributions have the following form (Blackwell &
MacQueen 1973):

350
Markov chain Monte Carlo
θi|θ1, · · · , θi−1, α0, G0 ∼
i−1

l=1
1
i −1 + α0
δθl +
α0
i −1 + α0
G0.
(8.36)
We can interpret the conditional distributions as a simple urn model where a ball of
a distinct color is associated with each atom δθl. The balls are drawn equiprobably or
uniformly. As seen in the second term of Eq. (8.36), a new atom is created by drawing
from G0 with probability proportional to α0. A ball of new color is added to the urn.
Equation (8.36) means that θi has a positive probability of being equal to one of the
previous draws θ1, · · · , θi−1. This process results in a reinforcement effect: the more
often a point is drawn, the more probable it is to be drawn in the future. To make the
clustering more explicitly, we introduce a new set of variables that represent distinct
values of atoms. Let φ1, · · · , φK denote the distinct values taken from previous factors
θ1, · · · , θi−1 and ck be the number of customers or values θi′ that are sitting at or are
associated with φk of table or cluster k. Equation (8.36) is re-expressed by
θi|θ1, · · · , θi−1, α0, G0 ∼
K

k=1
ck
i −1 + α0
δφk +
α0
i −1 + α0
G0.
(8.37)
From this re-expression, we ﬁnd that the P´olya urn scheme produces a distribution
on partitions which is closely related to the one using a different metaphor called the
Chinese restaurant process (CRP) (Aldous 1985). The metaphor of CRP is addressed
as follows. Consider a Chinese restaurant with an unbounded number of tables. Each θi
corresponds to a customer xi who enters the restaurant. The distinct values φk correspond
to the tables at which the customers sit. The ith customer sits at the table indexed by φk
with probability proportional to the number of customers ck who are already seated there
(in this case we set θi = φk), or sits at a new table with probability proportional to α0
(in this case, we increment K, draw φK ∼G0, and set θi = φK). Figure 8.6 shows the
scenario of the CRP where the current customer θ11 either chooses an occupied table
from {φ1, · · · , φ4} or a new table φnew according to
p(occupied table k|previous customers) =
ck
i −1 + α0
,
p(next unoccupied table|previous customers) =
α0
i −1 + α0
.
(8.38)
Figure 8.6
The Chinese restaurant process.

8.2 Bayesian nonparametrics
351
The CRP probability model in Eq. (8.38) is closely related to the P´olya urn model in
Eq. (8.25).
8.2.5
Dirichlet process mixture model
One of the most important applications of the DP is to explore the nonparametric prior
over the parameters of a mixture model. The resulting model is referred to as the DP
mixture model (Antoniak 1974). Consider the probability model of an inﬁnite sequence
of observations x1, x2, · · · in Eq. (8.35), with graphical representation as in Figure 8.7(a).
The probability measure G can be represented by using a stick-breaking construction.
The factors θi take on values φk with probability βk. Here, an indicator value zi is
introduced to reveal the positive integer value or cluster index for factor θi, and it is
distributed according to β. A DP mixture model can be represented by the following
conditional distributions:
β|α0 ∼GEM(α0),
zi|β ∼β,
φk|G0 ∼G0,
xi|zi, {φk}∞
k=1 ∼p(xi|φzi).
(8.39)
Therefore, we have the mixture model G = ∞
k=1 βkδφk and θi = φzi.
Alternatively, the DP mixture model can be derived as the limit of a sequence of
ﬁnite mixture models where the number of mixture components is taken to inﬁnity
(Neal 1992). This limiting process provides the third perspective on DP. Suppose that
we have K mixture components. Let β = {β1, · · · , βK} denote the mixture weights.
In the limit K →∞, the vectors β are closely related and are equivalent up to a ran-
dom size-biased permutation. We use a Dirichlet prior on β with symmetric parameters
(α0/L, · · · , α0/L). We thus have the following model:
Figure 8.7
(a) Dirichlet process mixture model; (b) hierarchical Dirichlet process mixture model.

352
Markov chain Monte Carlo
β|α0 ∼Dir(α0/L, · · · , α0/L),
zi|β ∼β,
φk|G0 ∼G0,
xi|zi, {φk}K
k=1 ∼p(xi|φzi).
(8.40)
The corresponding mixture model GK = K
k=1 βkδφk was shown to have the property

f(θ)dGK(θ) −→

f(θ)dG(θ),
(8.41)
as K →∞for every measurable function f(·). The marginal distribution of the
observations x1, · · · , xn approaches that using a DP mixture model.
8.2.6
Hierarchical Dirichlet process
The spirit of the graphical model based on directed graphs, as mentioned in Section 2.2,
is mainly from that of hierarchical Bayesian modeling, while the graphical model lit-
erature has focused almost exclusively on parametric hierarchies where each of the
conditionals is a ﬁnite-dimensional distribution. Nevertheless, it is possible to build hier-
archies in which the components are stochastic processes. We illustrate how to do this
based on the Dirichlet process.
In particular, we are interested in ﬁnding solutions to the problems in which the
observations are organized into groups, where the observations are assumed to be
exchangeable both within each group and across groups. For example, in document
representation, the words in each document in a text corpus are associated with the data
in each group from a set of grouped data. We want to discover the structures of words
from a set of training documents. It is important to ﬁnd the clusterings of word labels
for data representation. Let j index the groups or documents and i index the observations
or words within each group. We assume that xj1, xj2, · · · are exchangeable within each
group j and also between groups. The group data xj = {xj1, xj2, · · · } in an inﬁnite set of
groups x1, x2, · · · are exchangeable.
Assuming that each observation is drawn independently from a mixture model, each
observation xji is associated with a mixture component k. Let θji denote a parameter or
factor specifying the mixture component φk associated with the observation xji. The fac-
tors θji are not generally distinct. Let p(xji|θji) denote the distribution of observation xji
given the factor θji. Let Gj denote a distribution for the factors θj = {θj1, θj2, · · · } asso-
ciated with group j. Due to exchangeability, the factors are conditionally independent
given Gj. The probability model for this stochastic process is expressed by
θji|Gj ∼Gj,
xji|θji ∼p(xji|θji)
for each j and i.
(8.42)
The hierarchical Dirichlet process (HDP) (Teh et al. 2006) was proposed to conduct
BNP learning of grouped data, in which each group is associated with a mixture model
and we wish to link these mixture models. An HDP is a nonparametric prior distribution

8.2 Bayesian nonparametrics
353
over a set of random probability measures. The process deﬁnes a set of probability mea-
sures Gj, one for each group j, and a global probability measure G0 shared for different
groups. The nonparametric priors in HDP are given by the following hierarchy:
G0|γ , H ∼DP(γ , H),
Gj|α0, G0 ∼DP(α0, G0)
for each j.
(8.43)
The prior random measure of jth group Gj is a DP with a shared base measure G0
for grouped data which is itself drawn from the DP with a base measure H. Here, the
hyperparameters γ and α are the concentration parameters and H provides the prior
distribution for the factors θji. Basically, the distribution G0 varies around the prior H
with variations governed by γ while the distribution Gj over the factors in the jth group
deviates from G0 with variations controlled by α0. Analogous to the DP mixture model
mentioned in Section 8.2.5, HDP is feasible to construct the HDP mixture model as
shown graphically in Figure 8.7(b). The HDP mixture model is expressed by:
G0|γ , H ∼DP(γ , H),
Gj|α0, G0 ∼DP(α0, G0)
for each j,
θji|Gj ∼Gj,
xji|θji ∼p(xji|θji)
for each j and i.
(8.44)
We can see that this model achieves the goal of sharing clusters across groups by
assigning the same parameters or factors to those observations. That is, if θji = θji′, the
observations xji and xji′ belong to the same cluster. The equality of factors is possible
because both θji and θji′ are possibly drawn from Gj, which is a discrete measure over a
measurable partition (A1, A2, · · · ). Since Gj from different groups shares the atoms from
G0, the observations in different groups j can be assigned to the same cluster. In what
follows, we present the realization of the HDP from the perspectives of a stick-breaking
construction and a Chinese restaurant process.
8.2.7
HDP: Stick-breaking construction
Using a stick-breaking construction, the global measure G0 and the individual measure
Gj in HDP are expressed by the mixture models with the shared atoms {φk}∞
k=1 but
different weights β = {βk}∞
k=1 and πj = {πjk}∞
k=1, respectively, as given by:
G0 =
∞

k=1
βkδφk,
Gj =
∞

k=1
πjkδφk,
(8.45)
where atom φk is drawn from base measure H and weights β are drawn from the GEM
distribution β ∼GEM(γ ). Note that the weights πj are independent given β because
the Gjs are independent given G0.

354
Markov chain Monte Carlo
Let (A1, · · · , Ar) denote a measurable partition and Kr = k : φk ∈Al for l = 1, · · · , r.
Here, (K1, · · · , Kr) is a ﬁnite partition of the positive integers. For each group j, we have
(Gj(Ai), · · · , Gj(Ar))
∼Dir(α0G0(A1), · · · , α0G0(Ar))
⇒
⎛
⎝
k∈K1
πjk, · · · ,

k∈Kr
πjk
⎞
⎠
∼Dir
⎛
⎝α0

k∈K1
βk, · · · , α0

k∈Kr
βk
⎞
⎠
⇒πj ∼DP(α0, β).
(8.46)
Hence, each πj is independently distributed according to DP(α0, β). The HDP mixture
model is then represented by:
β|γ ∼GEM(γ ),
πj|α0, β ∼DP(α0, β),
zji|πj ∼πj,
φk|H ∼H,
xji|zji, {φk}∞
k=1 ∼p(xji|φzji).
(8.47)
We may further show the explicit relationship between the elements of β and πj. The
stick-breaking construction for DP(γ , H) deﬁnes the variables βk as
β′
k ∼Beta(1, γ ),
βk = β′
k
k−1

l=1
(1 −β′
l).
(8.48)
Also, the stick-breaking construction for a probability measure πj ∼DP(α0, β) of group
j is performed by
π′
jk ∼Beta
⎛
⎝α0βk, α0
∞

l=k+1
βl
⎞
⎠
= Beta

α0βk, α0

1 −
k

l=1
βl

,
πjk = π′
jk
k−1

l=1
(1 −π′
jl).
(8.49)
This completes the stick-breaking construction for the HDP.

8.2 Bayesian nonparametrics
355
Figure 8.8
A Chinese restaurant franchise containing three restaurants (rectangles) with an inﬁnite number
of tables (circles) and dishes {φk}∞
k=1. The term θji denotes the ith customer in restaurant j, and
ψjt is the indicator of a dish on the tth table of restaurant j. Different dishes φ1, φ2, and φ3 are
marked by different shading patterns.
8.2.8
HDP: Chinese restaurant franchise
The Chinese restaurant process for a DP is further extended to the Chinese restaurant
franchise for an HDP, which allows for multiple restaurants sharing a set of dishes,
as depicted in Figure 8.8. The metaphor is as follows. There is a restaurant franchise
which shares the menu across restaurants. At each table of each restaurant, one dish is
ordered from the menu by the ﬁrst customer who sits there. This dish is shared among
all customers sitting at that table. Different tables in different restaurants can serve the
same dish. In this scenario, the restaurants correspond to the groups while the customers
correspond to the factors θji of observations xji. Let φ1, · · · , φK denote the dishes in a
global menu which are drawn from H. We introduce the variable, ψjt, that indicates the
dish served at table t in restaurant j. In this restaurant franchise, we ﬁrst consider the
conditional distribution for a customer θji, given the previous customers θj1, · · · , θj,i−1
in restaurant j, and G0, where the DP Gj is integrated out. From Eq. (8.37), we obtain
θji|θj1, · · · , θj,i−1, α0, G0 ∼
mj·

t=1
cjt·
i −1 + α0
δψjt +
α0
i −1 + α0
G0.
(8.50)
The notation cjtk denotes the number of customers in restaurant j at table t eating dish
k. Marginal count is represented by dot. Thus, cjt· denotes the number of customers in
restaurant j at table t. The notation mjk means the number of tables in restaurant j serving
dish k. Thus, mj· represents the number of tables in restaurant j. This conditional is a

356
Markov chain Monte Carlo
mixture or a draw, which can be obtained by drawing from the terms on the right-hand-
side of Eq. (8.50) with the probabilities given by the corresponding mixture weights for
the occupied tables and an unoccupied table. If a term in the ﬁrst summation is chosen,
we increment cjt· and set θji = ψjt. If the second term is chosen, we increment mj·, draw
ψjmj· ∼G0 and set θji = ψjmj·.
Next, we proceed to integrate out G0, which is a DP, and apply Eq. (8.37) again to
ﬁnd the conditional distribution of a factor ψjt given the previous factors in the different
restaurants:
ψjt|ψ11, ψ12, · · · , ψ21, · · · , ψj,t−1, γ , H ∼
K

k=1
m·k
m·· + γ δφk +
γ
m·· + γ H.
(8.51)
If we draw ψjt by choosing the term in the summation on the right-hand-side of
Eq. (8.51), we set ψjt = φk. If we choose the second term, we increment K, draw a
new φK ∼H and set ψjk = φK. It is meaningful that the mixture probability of the
ﬁrst term is proportional to m·k, which represents the number of tables serving dish k,
while the probability of the second term is proportional to the concentration parameter
γ of DP G0. This completes the HDP implementation based on the Chinese restaurant
franchise. To obtain samples θji for each j and i, we ﬁrst draw θji from Eq. (8.50). If a
new sample from G0 is needed, we use Eq. (8.51) to obtain a new sample ψjt and set
θji = ψjt.
Moreover, the HDP can be derived from the perspective of inﬁnite limit over the
ﬁnite mixture models. This is done by considering the collection of ﬁnite mixture mod-
els where L-dimensional β and πj are used as the global and group-dependent mixing
probabilities, respectively. The HDP ﬁnite mixture model is accordingly expressed by
β|γ ∼Dir(γ/L, · · · , γ/L),
πj|α0, β ∼Dir(α0β),
zjiπj ∼πj,
φk|H ∼H,
xji|zji, {φk}L
k=1 ∼p(xji|φzji).
(8.52)
It can be shown that the limit of this model as L →∞approaches the HDP mixture
model. The parametric hierarchical prior for β and π in Eq. (8.52) is also seen as the
hierarchical Dirichlet model, which has been applied for language modeling (MacKay
& Peto 1995), as addressed in Section 5.3.
8.2.9
MCMC inference by Chinese restaurant franchise
The MCMC sampling schemes have been developed for inference of the HDP mixture
model. A straightforward Gibbs sampler based on the Chinese restaurant franchise can
be implemented. To do so, the posterior probabilities for drawing tables t = {tji} and
dishes k = {kjt} should be determined. Let tji be the index of the factor ψjt associated
with θji, and let kjt be the index of φk associated with ψjt. In the Chinese restaurant
franchise, the customer i in restaurant j sits at table tji, whereas table t in restaurant j

8.2 Bayesian nonparametrics
357
serves dish kjt. Let x = {xji}, xjt = {xji, all i with tji = t}, zzji, m = mjk and φ =
{φ1, · · · , φK}. Notations k−jt = k\kjt and t−ji = t\tji denote the vectors k and t with
the exception of components kjt and tji, respectively, and c−ji
jt· denotes the number of
customers in restaurant j who enjoy the dish ψjt, but leaving out customer xji. Similarly,
we have x−ji = x\xji and x−jt = x\xjt. The concentration parameters γ and α0 of DPs
are assumed to be ﬁxed. When implementing HDP using MCMC sampling, we need
to calculate the conditional distribution of xji under mixture component k given all data
samples except xji,
p(xji|x−ji, k) =

p(xji|φk) *
j′i′̸=ji,zj′i′=k p(xj′i′|φk)h(φk)dφk
 *
j′i′̸=ji,zj′i′=k p(xj′i′|φk)h(φk)dφk
.
(8.53)
Here, h(φk) is a prior distribution from H and is conjugate to the likelihood p(x|φk).
We can integrate out the mixture component parameter φk in closed form. Similarly
to Eq. (8.53), the conditional distribution p(xjt|x−jt, k) can be calculated by using xjt,
the customers at restaurant j sitting at table t. In what follows, we derive the posterior
probabilities for sampling tables tji and dishes kjt which are then used to reconstruct θjis
and ψjts based on the φks. The property of exchangeability is employed in Eqs. (8.50)
and (8.51) for θjis and ψjts, or equivalently for tjis and kjts, respectively.
Sampling t: We want to calculate the conditional distribution of tji given the rest of
variables t−ji. This conditional posterior for tji is obtained by combining the conditional
prior for tji with the likelihood of generating xji. Basically, the likelihood function due to
xji given previously occupied table t is expressed by p(xji|x−ji, k), as shown in Eq. (8.53).
The likelihood function for a new table tji = tnew can be calculated by integrating out
all possible values of kjtnew using Eq. (8.51):
p(xji|t−ji, tji = tnew, k)
=
K

k=1
m·k
m·· + γ p(xji|x−ji, k) +
γ
m·· + γ p(xji|x−ji, knew),
(8.54)
where p(xji|x−ji, knew) is a prior density of xji given by
p(xji) =

p(xji|φ)h(φ)dφ.
(8.55)
According to Eq. (8.50), the conditional prior probability of taking a previously occu-
pied table tji = t is proportional to c−ji
jt· , while the probability of taking a new table
t = tnew = mj· + 1 is proportional to α0. Thus, the conditional posterior probability for
sampling table tji is derived from
p(tji = t|t−ji, x, k)
∝

c−ji
jt· · p(xji|x−ji, kjt)
if t previously occupied,
α0 · p(xji|t−ji, tji = tnew, k)
if t = tnew.
(8.56)
Sampling k: When the sampled table is a new one, tji = tnew, we need to further
order a new dish kjtnew from the global menu. This is done according to the conditional

358
Markov chain Monte Carlo
posterior probability, which is combined from a conditional prior for kjt and a likelihood
function due to xji. The conditional posterior probability is then derived from
p(kjtnew = k|t, x, k−jtnew)
∝
. m·k · p(xji|x−ji, k)
if k previously ordered,
γ · p(xji|x−ji, knew)
if k = knew.
(8.57)
Because changing kjt actually changes the component membership of all customers xjt
at table t, we can calculate the conditional posterior probability of ordering dish kjt = k
from
p(kjt = k|t, x, k−jt)
∝

m−jt
·k · p(xjt|x−jt, k)
if k previously ordered,
γ · p(xjt|x−jt, knew)
if k = knew.
(8.58)
where the likelihood function p(xjt|x−jt, k) due to the customers xjt at table t of restaurant
j is used.
8.2.10
MCMC inference by direct assignment
In the ﬁrst MCMC procedure based on the Chinese restaurant franchise representation,
the observations are ﬁrst assigned to some table tji, and the tables are then assigned to
some mixture component kjt. This indirect association with mixture components could
be realized by directly assigning mixture components through a new variable zji which is
the same as kjtji. The variable zji indicates the index of a mixture component correspond-
ing to the customer xji. Using this direct assignment, the tables are represented only in
terms of the number of tables mjk. A bookkeeping scheme is involved. We would like to
instantiate and sample from G0 by using the factorized posterior on G0 across groups.
To do so, an explicit construction for G0 ∼DP(γ , H) is expressed in the form of
β = (β1, · · · , βK, βu) ∼Dir(m·1, · · · , m·K, γ ),
Gu ∼DP(γ , H),
p(φk|z) ∝h(φk)

ji:zji=k
p(xji|φk, zji),
G0 =
K

k=1
βkδφk + βuGu,
(8.59)
which can be also expressed as
G0 ∼DP

γ + m··, γ H + K
k=1 m·kδφk
γ + m··

.
(8.60)
From the expressions in Eq. (8.59), we can see that the values m·k and γ in the ﬁrst
MCMC inference based on the Chinese restaurant franchise are replaced by βk and βu in
the second MCMC inference based on the respective direct assignments. In the MCMC

8.2 Bayesian nonparametrics
359
inference based on direct assignment, we need to sample the indicators of mixture com-
ponents z = {zij} and the numbers of tables m = {mjk}, according to the corresponding
posterior probabilities which are provided in what follows.
Sampling z: The idea of sampling z is to group together the terms associated with
each k in Eqs. (8.54) and (8.56) so as to calculate the conditional posterior probability:
p(zji = k|z−ji, x, m, β)
=

(c−ji
j·k + α0βk)p(xji|x−ji, k)
if k previously occupied,
α0βup(xji|x−ji, knew)
if k = knew.
(8.61)
Note that we have combined Eqs. (8.54) and (8.56) based on a new variable zji and have
replaced m·k with βk and γ with βu. To fulﬁl the sampling of z in Eq. (8.61), we have to
further sample m and β.
Sampling m: According to the direct assignment of data items to mixture components
z, it is sufﬁcient to sample m and β in place of t and k. To ﬁnd the conditional distribu-
tion of mjk, we consider the conditional distribution of tji under the condition kjtji = zji.
From Eq. (8.50), the prior probability that data item xji is assigned to some table t such
that kjt = k is
p(tji = t|kjt = k, t−ji, k, β) ∝c−ji
jt· ,
(8.62)
while the probability that is assigned to a new table under mixture component k is
p(tji = tnew|kjtnew = k, t−ji, k, β) ∝α0βk.
(8.63)
These probabilities in a Gibbs sampler have the equilibrium distribution, which is
the prior probability over the assignment of cj·k observations to components in a DP
with concentration parameter α0βk. The corresponding distribution over the number of
mixture components is the desired conditional distribution of mjk which is written as
(Antoniak 1974, Teh et al. 2006):
p(mjk = m|z, m−jk, β) =
(α0βk)
(α0βk + cj·k)s(cj·k, m)(α0βk)m.
(8.64)
Here, s(c, m) denotes the unsigned Stirling numbers of the ﬁrst kind, which are
calculated from
s(c + 1, m) = s(c, m −1) + cs(c, m),
(8.65)
with initial conditions s(0, 0) = s(1, 1) = 1, s(c, 0) = 0 for c > 0 and s(c, m) = 0 for
m > c.
Sampling β: Having the samples m and the ﬁxed hyperparameter γ , the βk parameters
are sampled from a Dirichlet distribution,
(β1, · · · , βK, βu)|m, γ ∼Dir(m·1, · · · , m·K, γ ).
(8.66)
This completes the MCMC inference for the HDP, based on the scheme of direct assign-
ment of mixture components. It was shown that MCMC inference with the scheme of
direct assignment is better than the scheme using the Chinese restaurant franchise in
terms of convergence speed.

360
Markov chain Monte Carlo
8.2.11
Relation of HDP to other methods
In general, HDP can be seen as a building block for a variety of speech and language pro-
cessing applications. An instance of application is the latent Dirichlet allocation (LDA)
model (Blei et al. 2003), where each entity is associated not with a single cluster but
with a set of clusters. In LDA terminology, each document is associated with a set of
topics. As described in Section 7.6, an LDA model is constructed as a Bayesian paramet-
ric model with a ﬁxed number of topics. HDP relaxes the limitation of a ﬁnite dimension
in latent topic space in LDA and builds the ﬂexible topic model with inﬁnite clusters or
topics. Multiple DPs are used to capture the uncertainty regarding the number of mixture
components. HDP is viewed as a BNP version of an LDA model. The topics or clusters
φk for the jth document are drawn from the nonparametric prior Gj, while the measures
Gj are drawn from a DP with a base measure G0. This allows the same topics to appear
in multiple documents.
There are some other ways to connect multiple DPs. One idea is based on the nested
Dirichlet process (NDP) (Rodriguez, Dunson & Gelfand 2008), which was proposed to
model a collection of dependent distributions by using random variables as atoms at the
higher level and random distributions as atoms at the lower level. This combinatorial
process borrows information across DPs while also allowing DPs to be clustered. The
simultaneous multilevel clustering can be achieved by such a nested setting. NDP is
characterized by
Gj ∼Q
for each j,
Q ∼DP(α0, DP(γ , H)).
(8.67)
Using HDP, the distributions {Gj} of different j share the same atoms but assign them
with different weights. However, using NDP, these distributions may have completely
different atoms and weights. By marginalizing over the DPs, the resulting urn model
is closely related to the nested Chinese restaurant process (nCRP) (Blei, Grifﬁths &
Jordan 2010), which is known as a tree model of Chinese restaurants. The scenario of
nCRP is addressed as follows. A customer enters the tree at a root Chinese restaurant
and sits at a table. This table points to another Chinese restaurant, where the customer
goes to dine the next evening. The construction is done recursively. Thus, a given cus-
tomer follows a path through the tree of restaurants. Through BNP inference of topic
hierarchies, a hierarchical topic model is established (Blei, Grifﬁths, Jordan et al. 2004).
Each document or customer chooses a tree path of topics while each word in the docu-
ment is represented by a mixture model of hierarchical topics along this tree path. In
the following sections, we address some BNP methods which have been successfully
applied for building the speaker diarization system and for developing the solutions of
acoustic and language models to speech recognition systems.
8.3
Gibbs sampling-based speaker clustering
This section describes an application of MCMC techniques for speech features. We
focus on speaker clustering, as discussed in Section 6.6.2 based on a Bayesian

8.3 Gibbs sampling-based speaker clustering
361
information criterion (BIC). Section 6.6.2 models a speaker cluster with a single
Gaussian where we assume a stationary property for speech features within a segment.
However, this is actually not correct since it has various temporal patterns based on
linguistic variations, speaking styles, and noises. Therefore, we model these short-term
variations with a GMM, while speaker cluster characteristics are modeled by a mixture
of GMMs to consider the multi-scale properties of speech dynamics (Moraru, Meignier,
Besacier et al. 2003, Valente & Wellekens 2004b, Wooters, Fung, Peskin et al. 2004,
Meignier, Moraru, Fredouille et al. 2006).
An important aspect of this speech modeling technique is the consideration of the
multi-scale property in dynamics within a probabilistic framework. For example, PLSA
(in Section 3.7.3) and LDA (in Section 7.6) are successful approaches in terms of the
multi-scale property. They deal accurately with two types of scales, namely, word-level
and document-level scales (i and m in the complete data likelihood function of the
latent topic model in Eq. (3.293)), based on a latent topic model (Hofmann 1999a). The
approach discussed in this section is inspired by these successful approaches, and aims
to apply a fully Bayesian treatment to the multi-scale properties of speech dynamics.
There have been several studies on Bayesian speech modeling, e.g., by using max-
imum a-posteriori (MAP) in Chapter 4 or variational Bayesian (VB) approaches in
Chapter 7 for speech recognition (Gauvain & Lee 1994, Watanabe et al. 2004), speaker
veriﬁcation (Reynolds et al. 2000), and speaker clustering (Valente & Wellekens 2004b).
While all of these approaches are based on the EM-type deterministic algorithm, this
section focuses on another method of realizing fully Bayesian treatment, namely sam-
pling approaches based on MCMC. The main advantage of the sampling approaches
is that they can avoid local optimum problems in addition to providing other Bayesian
advantages (mitigation of data sparseness problems and capability of model structure
optimization). While their heavy computational cost could be a problem in realization,
recent improvements in computational power and the development of theoretical and
practical aspects related to the sampling approaches allow us to apply them to practi-
cal problems (e.g., Grifﬁths & Steyvers (2004), Goldwater & Grifﬁths (2007), Porteous,
Newman, Ihler et al. (2008) in natural language processing). Therefore, the aim of this
work is to apply a sampling approach to speech modeling considering the multi-scale
properties of speech dynamics.
The following sections formulate the multi-scale GMM by utilizing a Gibbs sampling
approach. In this section, for its educational value, we ﬁrst describe Gibbs sampling for
a standard GMM. Section 8.3.2 derives the marginal likelihood of the GMM, which
is used for deriving GMM Gibbs samplers in Section 8.3.3. Section 8.3.4 provides
the generative process and a graphical model of multi-scale GMM for speaker cluster-
ing. Based on the analytical results of GMM Gibbs sampling, Section 8.3.5 derives the
marginal likelihood of the multi-scale GMM, which is used for deriving Gibbs samplers
in Section 8.3.6.
8.3.1
Generative model
This section considers the two types of observation vector sequences. One is an
utterance- (or segment-) level sequence and the other is a frame-level sequence. Then, a

362
Markov chain Monte Carlo
D dimensional observation vector (e.g., MFCC) at frame t in utterance u is represented
as out ∈RD. A set of observation vectors in utterance u is represented as
Ou ≜{out ∈RD|t = 1, · · · , Tu}.
(8.68)
Tu denotes the number of frames at an utterance u.
Next, we assume that the frame-level sequence is modeled by a GMM as usual, and
the utterance-level sequence is modeled by a mixture of these GMMs. Two kinds of
latent variables are involved in multi-scale GMM for each sequence: utterance-level
latent variable zu and frame-level latent variable vut. Utterance-level latent variables
may represent emotion, topic, and speaking style as well as speakers, depending on the
speech variation. The joint likelihood function of U observation vectors (O ≜{Ou|u =
1, · · · , U}) with the latent variable sequences (Z ≜{zu ∈{1, · · · , S}|u = 1, · · · , U} and
V ≜{vut ∈{1, · · · , K}|t = 1, · · · , Tu, u = 1, · · · , U}) can be expressed as follows:
p(O, Z, V|) =
U

u=1
hzu
Tu

t=1
wzuvutN(out|μzuvut, R−1
zuvut),
(8.69)
where hs ∈[0, 1] denotes the utterance-level mixture weight, and wsk ∈[0, 1] denotes
the frame-level mixture weight. The terms μsk ∈RD and Rsk ∈RD×D denote the mean
vector and precision matrix parameters of the Gaussian distribution, s and k denote
utterance-level and frame-level mixture indexes, respectively, and S and K denote the
number of speakers and the number of mixture components, respectively. Therefore, a
set of model parameters  is deﬁned as
 ≜{hs, wsk, μsk, Rsk|s = 1, · · · , S, k = 1, · · · , K}.
(8.70)
Note that this is almost equivalent to the following pdf of the GMM in Section 3.2.4:
p(O, V|) =
T

t=1
wvtN(ot|μvt, R−1
vt ).
(8.71)
The next section ﬁrst describes Gibbs sampling for the standard GMM.
8.3.2
GMM marginal likelihood for complete data
We assume a diagonal covariance matrix for the Gaussian distributions as usual, where
the d-d diagonal element of the precision/covariance matrix is expressed as rd/d.
The following conjugate distributions are used as the prior distributions of the model
parameters:
p(|0) :
⎧
⎨
⎩
w ∼Dir(φw),
μk ∼N(μ0
k, (φμ)−1R−1
k ),
rkd ∼Gam(φr, r0
kd),
(8.72)
where φw, μ0
k, φμ, r0
kd, φr(≜0) are the hyperparameters. Dir(·) and Gam(·) denote
Dirichlet and gamma distributions in Appendices C.4 and C.11, respectively.

8.3 Gibbs sampling-based speaker clustering
363
In a Bayesian inference framework, we focus on the marginal likelihood for the
complete data. In the complete data case, all of the latent variables are treated as obser-
vations, i.e., the assignments of all the latent variables are hypothesized to be given in
advance. Then, p(vu = k|·) ≜δ(vu, k) return 0 or 1 based on the assignment information,
and the sufﬁcient statistics of the GMM given all latent variables V can be represented
as follows:
⎧
⎪⎪⎨
⎪⎪⎩
γV,k
= 
t δ(vt, k),
γ (1)
V,k
= 
t δ(vt, k)ot,
γ (2)
V,kd = 
t δ(vt, k)(otd)2.
(8.73)
The quantity γV,k ∈Z+ is a count of frames assigned to k, and γ (1)
V,k and γ (2)
V,kd are
ﬁrst-order and second-order sufﬁcient statistics, respectively. We deﬁne a set of these
sufﬁcient statistics as
V,k ≜{γV,k, γ (1)
V,k, γ (2)
V,kd|k = 1, · · · , K, d = 1, · · · , D}.
(8.74)
The subscript V would be omitted when it is obvious. Note that the statistics γk is a
positive discrete number while those appearing in the EM algorithm (ML in Eq. (3.153),
MAP in Eq. (4.93), and VB in Eq. (7.67)) are positive continuous values since the
occurrences of latent variables in the EM algorithm are expectation values based on
the posterior probabilities of latent variables.
Based on the sufﬁcient statistics representation in Eq. (8.73), the complete data like-
lihood of Eq. (8.71) can be represented by using the product formula of the Kronecker
delta function in Eq. (A.4) (fb = *
a f δ(a,b)
a
) as follows:
p(O, V|) =
K

k=1
T

t=1
(
wδ(vt,k)
k
N(ot|μk, R−1
k )
)δ(vt,k)
=
K

k=1
(wk)γk
T

t=1
(
N(ot|μk, R−1
k )
)δ(vt,k)
.
(8.75)
Since the likelihood function is represented by the exponential distributions (multi-
nomial and Gaussian distributions), these parameters integrate out, and we can use
collapsed Gibbs sampling, as discussed in Section 8.1.4. The marginal likelihood for the
complete data, p(O, V|0), is represented by the following expectations by substituting
Eqs. (8.72) and (8.75) into the following integration:
p(O, V|0)
=

p(O, V|)p(|0)d
=

E(w)
& K

k=1
(wk)γk
'  K

k=1
E(μk,Rk)
& T

t=1
(
N(ot|μk, R−1
k )
)δ(vt,k)
'
.
(8.76)
Note that this calculation is similar to those of the MAP auxiliary function in Sec-
tion 4.3.5 and variational lower bound in Section 7.3.4. For example, E(w)
+*K
k=1(wk)γk
,
can be rewritten as

364
Markov chain Monte Carlo
E(w)
& K

k=1
(wk)γk
'
=

p({wk}K
k=1)
K

k=1
(wk)γkdwk
=

exp

log

Dir({wk}K
k=1|{φw
k }K
k=1)

+
K

k=1
γk log wk
 K

k=1
dwk.
(8.77)
This is the same as the MAP auxiliary function of Eq. (4.54), and by analogy, Eq. (8.77)
can be rewritten with the posterior distribution based equation in Eq. (4.54), as follows:
E(w)
& K

k=1
(wk)γk
'
=

exp

log
(
Dir({wk}K
k=1|{ ˜φw
k }K
k=1)
)
+ log

CDir({φw
k }K
k=1)
CDir({ ˜φw
k }K
k=1)
 K

k=1
dwk,
(8.78)
where ˜φw
s is a posterior hyperparameter, which is deﬁned as:
˜φw
k ≜φw
k + γk.
(8.79)
Therefore, the integral is ﬁnally solved as follows:
E(w)
& K

k=1
(wk)γk
'
= CDir({φw
k }K
k=1)
CDir({ ˜φw
k }K
k=1)

Dir({wk}K
k=1|{ ˜φw
k }K
k=1)
K

k=1
dwk
= CDir({φw
k }K
k=1)
CDir({ ˜φw
k }K
k=1)
.
(8.80)
Thus, the integration is calculated analytically, similarly to the calculation of the vari-
ational lower bound. The integral with respect to μk and Rk in Eq. (8.76) is also
analytically calculated by using Eq. (7.124). First, the expectation is rewritten as
follows:
E(μk,Rk)
& T

t=1
(
N(ot|μk, R−1
k )
)δ(vt,k)
'
=

exp
 D

d=1
log
(
N(μkd| ˜μkd, ( ˜φμ
k rkd)−1)Gam2(rkd|˜rkd, ˜φr
k)
)
+

−γkD
2
log(2π) + D
2 log φμ
˜φμ
k
+ log CGam2(r0
kd, φr)
CGam2(˜rkd, ˜φr
k)

dμkdRk
= exp

−γkD
2
log(2π) + D
2 log φμ
˜φμ
k
+ log CGam2(r0
kd, φr)
CGam2(˜rkd, ˜φr
k)

,
(8.81)

8.3 Gibbs sampling-based speaker clustering
365
where posterior hyperparameters ˜φμ
k , ˜μk, ˜φr
k, and ˜rkd are deﬁned as follows:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
˜φμ
k
≜φμ + γk,
˜μk
≜φμμ0
k+γ (1)
k
˜φμ
k
,
˜φr
k
≜φr + γk,
˜rkd ≜r0
kd + γ (2)
kd + φμ(μ0
kd)2 −˜φμ
k ( ˜μkd)2.
(8.82)
Thus, we ﬁnally solve all integrals in Eq. (8.76) with use of concrete forms of the nor-
malization constants of the Dirichlet and gamma distributions in Appendices C.4 and
C.11, as follows:
p(O, V|0)
= (
k φw
k )
*
k (φw
k )
*
k ( ˜φw
k )
(
k ˜φw
k )

k
(2π)−γkD
2
(φμ)
D
2
(

(
φr
2
))−D*
d
r0
kd
2
 φr
2
( ˜φμ
k )
D
2



˜φr
k
2
−D(*
d
˜rkd
2
) ˜φr
k
2
.
(8.83)
Below we summarize the posterior hyperparameters, which are obtained from the
hyperparameters of the prior distributions (0) and sufﬁcient statistics (Eq. (8.73)) as
follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
˜φw
k
= φw
k + γk,
˜φμ
k
= φμ + γk,
˜μk
= φμμ0
k+γ (1)
k
˜φμ
k
,
˜φr
k
= φr + γk,
˜rkd = r0
kd + γ (2)
kd + φμ(μ0
kd)2 −˜φμ
k ( ˜μkd)2.
(8.84)
The marginal likelihood obtained is quite similar to the model parameter part of the vari-
ational lower bound in Eq. (7.126), since both functions are obtained by integrating out
the Gaussian parameters for complete data likelihood. Based on the marginal likelihood
for these complete data, we can calculate the marginal conditional distribution of vt, as
shown below.
8.3.3
GMM Gibbs sampler
As discussed in Section 8.1.4, a collapsed Gibbs sampler can assign latent variables by
using the marginal conditional distribution. First, from the sum and product rules, the
marginal conditional distribution p(vt = k|O, V\t) is represented as follows:
p(vt = k|O, V\t) = p(O, V\t, vt = k)
p(O, V\t)
∝p(O, V\t, vt = k|0).
(8.85)
Here, V\t indicates a set that does not include the tth frame element. Therefore, by
considering the normalization constant, the posterior probability can be obtained by
using Eq. (8.83) as follows:

366
Markov chain Monte Carlo
p(vt = k|O, V\t) =
p(O, V\t, vt = k|0)
K
k′=1 p(O, V\t, vt = k′|0)
=
g( ˜V\t,vt=k)
K
k′=1 g( ˜V\t,vt=k′)
,
(8.86)
where ˜V\t,vt=k is a set of posterior hyperparameters computed given latent variables of
V\t, vt = k. Note that the denominators of ˜V\t,vt=k′ have the same latent variable for all
frames except t. To compute ˜V\t,vt=k′, we ﬁrst need to compute V\t,k, which is a set of
sufﬁcient statistics for all frames except t as follows:
V\t,k :
⎧
⎪⎪⎨
⎪⎪⎩
γV\t,k
= 
t′={1,··· ,T}\t δ(vt′, k),
γ (1)
V\t,k
= 
t′={1,··· ,T}\t δ(vt′, k)ot,
γ (2)
V\t,kd = 
t′={1,··· ,T}\t δ(vt′, k)(otd)2.
(8.87)
From Eq. (8.73), V\t,k can be computed by simply subtracting the zeroth-, ﬁrst-, and
second-order values of vt = k as:
V\t,k :
⎧
⎪⎪⎨
⎪⎪⎩
γV\t,k
= γk −δ(vt, k),
γ (1)
V\t,k
= γ (1)
k
−δ(vt, k)ot,
γ (2)
V\t,kd = γ (2)
kd −δ(vt′, k)(otd)2.
(8.88)
Thus, V\t,vt=k′ can be obtained by simply adding the zeroth-, ﬁrst-, and second-order
values of vt = k′ for all k′ as:
V\t,vt=k′ :
⎧
⎪⎪⎨
⎪⎪⎩
γV\t,vt=k′
= γV\t,k′ + δ(vt, k′),
γ (1)
V\t,vt=k′
= γ (1)
V\t,k′ + δ(vt, k′)ot,
γ (2)
V\t,vt=k′,d = γ (2)
V\t,k′d + δ(vt′, k′)(otd)2.
for all k′
(8.89)
g(·) in Eq. (8.86) is deﬁned as follows:
g( ˜k) ≜( ˜φw
k )( ˜φμ
k )−D
2


 ˜φr
k
2
D
d
˜rkd
2
−
˜φr
k
2
.
(8.90)
This equation is obtained by canceling out the factors in the numerator and denominator
of Eq. (8.83). Thus, we obtain the Gibbs sampler, which assigns mixture component k
at frame t.
Note that if we use multinomial distributions in LDA instead of Gaussian distri-
butions, the numerator and denominator of the Gibbs sampler are further canceled
out (see Grifﬁths & Steyvers (2004)) based on the formula of the gamma function in
Appendix A.4. Actually, ( ˜φw
k ) in Eq. (8.90) can be similarly canceled out, while the
other factors cannot be canceled out. Therefore, the computational cost of the Gaussian-
based Gibbs sampler is large compared with LDA, since we need to compute Eq. (8.90)
for every k and every frame t.

8.3 Gibbs sampling-based speaker clustering
367
8.3.4
Generative process and graphical model of multi-scale GMM
Based on solution of GMM Gibbs sampling in the previous section, we consider the
Bayesian treatment of this multi-scale GMM for speaker clustering, which is an exten-
sion of GMM. For the conditional likelihood equation in Eq. (8.69), we again assume
a diagonal covariance matrix for the Gaussian distributions. We also assume that the
prior hyperparameters of the GMM parameters {wsk}sk, {μsk}sk, and {Rsk}sk for each s
are shared with the parameters of one GMM (universal background model assumption
(Reynolds et al. 2000)), which is used for speaker and speech recognition (subspace
GMM (Povey, Burget, Agarwal et al. 2010)). Then the following conjugate distributions
are used as the prior distributions of the model parameters:
p(|0) :
⎧
⎪⎪⎨
⎪⎪⎩
h
∼Dir(φh),
ws ∼Dir(φw),
μsk ∼N(μ0
k, (φμ)−1R−1
sk ),
rskd ∼Gam(φr, r0
kd),
(8.91)
where φh, φw, μ0
k, φμ, r0
kd, φr(≜0) are the hyperparameters.
Based on the likelihood function and prior distributions, the generative process of
multi-scale GMM can be expressed in Algorithm 17. The corresponding graphical
model is shown in Figure 8.9. Now we have introduced multi-scale GMM, the following
sections derive a solution for multi-scale GMM based on Gibbs sampling.
Figure 8.9
Model of multi-scale Gaussian mixture model. The model deals with two time scales based on
frame t and utterance u.

368
Markov chain Monte Carlo
Algorithm 17 Generative process of multi-scale GMM
Require: 0
1: Draw h from Dir(φh)
2: for each utterance-level mixture component s = 1, · · · , S do
3:
Draw ws from Dir(φw)
4:
for each frame-level mixture component k = 1, · · · , K do
5:
for each dimension d = 1, · · · , D do
6:
Draw rskd from Gam(φr, r0
kd)
7:
end for
8:
Draw μsk from N(μ0
k, (φμ)−1R−1
sk )
9:
end for
10: end for
11: for each utterance u = 1, · · · , U do
12:
Draw zu from Mult (h)
13:
for each frame t = 1, · · · , Tu do
14:
Draw vut from Mult (wzu)
15:
Draw out from N(μzuvut, R−1
zuvut)
16:
end for
17: end for
8.3.5
Marginal likelihood for the complete data
Similarly to the GMM Gibbs sampler, we prepare p(vut = k|·) ≜δ(vut, k) given utter-
ance u, which returns 0 or 1 based on the assignment information. In addition, we also
prepare the assignment information of an utterance-level mixture with p(zu = s|·) ≜
δ(zu, s). The sufﬁcient statistics of multi-scale GMM can be represented as follows:
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
ξs
= 
u δ(zu, s),
γsk
= 
u,t δ(zu, s)δ(vut, k),
γ (1)
sk
= 
u,t δ(zu, s)δ(vut, k)out,
γ (2)
skd = 
u,t δ(zu, s)δ(vut, k)(outd)2.
(8.92)
Here ξs ∈Z+ is a count of utterances assigned to speaker cluster s, and γsk ∈Z+ is a
count of frames assigned to mixture component k in s. γ (1)
sk and γ (2)
skd are ﬁrst-order and
second-order sufﬁcient statistics, respectively.
Based on the sufﬁcient statistics representation in Eq. (8.92), the complete data like-
lihood of Eq. (8.69) can be represented by using the product formula of the Kronecker
delta function in Eq. (A.4) (fb = *
a f δ(a,b)
a
) as follows:
p(O, Z, V|)
=
S

s=1
U

u=1

hzu
K

k=1
Tu

t=1
(
wzuvutN(out|μzuvut, R−1
zuvut)
)δ(vut,k)
δ(zu,s)
=
S

s=1
(hs)ξs
K

k=1
(wsk)γsk
U

u=1
Tu

t=1
(
N(out|μsk, R−1
sk )
)δ(zu,s)δ(vut,k)
.
(8.93)

8.3 Gibbs sampling-based speaker clustering
369
The marginal likelihood for the complete data, p(O, Z, V|0), is represented by
the following expectations by substituting Eqs. (8.91) and (8.93) into the following
integration:
p(O, Z, V|0)
=

p(O, Z, V|)p(|0)d
= E(h)
& S

s=1
(hs)ξs
'  S

s=1
E(ws)
& K

k=1
(wsk)γsk
'
×
 S

s=1
K

k=1
E(μsk,Rsk)
& U

u=1
Tu

t=1
(
N(out|μsk, R−1
sk )
)δ(zu,s)δ(vut,k)
'
.
(8.94)
By following the derivations in Section 8.3.2, the expectations are calculated analyti-
cally. This section simply provides the analytical results of the expectations. By using
Eq. (4.45), E(h)
+*S
s=1(hs)ξs
,
is solved as:
E(h)
& S

s=1
(hs)ξs
'
= CDir({φh
s }S
s=1)
CDir({ ˜φhs }S
s=1)
,
(8.95)
where ˜φh
s is a posterior hyperparameter, which is deﬁned as:
˜φh
s ≜φh
s + ξs.
(8.96)
The other integral with respect to ws in Eq. (8.94) is also calculated as follows:
E(ws)
& K

k=1
(wsk)γsk
'
= CDir({φw
k }K
k=1)
CDir({ ˜φw
sk}K
k=1)
,
(8.97)
where ˜φw
sk is a posterior hyperparameter, which is deﬁned as:
˜φw
sk ≜φw
k + γsk.
(8.98)
The integral with respect to μsk and Rsk in Eq. (8.94) is also analytically calculated as
follows:
E(μsk,Rsk)
& U

u=1
Tu

t=1
(
N(out|μsk, R−1
sk )
)δ(zu,s)δ(vut,k)
'
= exp

−γskD
2
log(2π) + D
2 log φμ
˜φμ
sk
+ log CGam2(r0
kd, φr)
CGam2(˜rskd, ˜φr
sk)

,
(8.99)
where posterior hyperparameters ˜φμ
sk, ˜μsk, ˜φr
sk, and ˜rskd are deﬁned as follows:
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
˜φμ
sk
≜φμ + γsk,
˜μsk
≜φμμ0
k+γ (1)
sk
˜φμ
sk
,
˜φr
sk
≜φr + γsk,
˜rskd ≜r0
kd + γ (2)
skd + φμ(μ0
kd)2 −˜φμ
sk( ˜μskd)2.
(8.100)

370
Markov chain Monte Carlo
Thus, we ﬁnally solve all the integrals in Eq. (8.94) as follows:
p(O, Z, V|0) = (
s φh
s )
*
s (φhs )
*
s ( ˜φh
s )
(
s ˜φhs )

s
(
k φw
k )
*
k (φw
k )
*
k ( ˜φw
sk)
(
k ˜φw
sk)
×

s,k
(2π)−γskD
2
(φμ)
D
2
(

(
φr
2
))−D*
d
r0
kd
2
 φr
2
( ˜φμ
sk)
D
2



˜φr
sk
2
−D(*
d
˜rskd
2
) ˜φr
sk
2
.
(8.101)
We summarize below the posterior hyperparameters, which are obtained from the
hyperparameters of the prior distributions (0) and sufﬁcient statistics (Eq. (8.92)) as
follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
˜φh
s
= φh
s + ξs,
˜φw
sk
= φw
k + γsk,
˜φμ
sk
= φμ + γsk,
˜μsk
= φμμ0
k+γ (1)
sk
˜φμ
sk
,
˜φr
sk
= φr + γsk,
˜rskd = r0
kd + γ (2)
skd + φμ(μ0
kd)2 −˜φμ
sk( ˜μskd)2.
(8.102)
Based on the marginal likelihood for these complete data, we can calculate the marginal
conditional distribution of vut and zu, as shown below.
8.3.6
Gibbs sampler
We provide a collapsed Gibbs sampler p(vut = k|O, V\t, Z\u, zu = s) for a frame-
level mixture component k, which has similarities with the GMM Gibbs sampler
in Section 8.3.3. In addition, we also provide a collapsed Gibbs sampler p(vu,t =
k′|O, V\t, Z\u, zu = s) for utterance-level mixture component s, which is a result of
speaker clustering.
Frame-level mixture component
The Gibbs sampler assigns frame-level mixture component k at frame t by using the
following equation:
p(vut = k|O, V\t, Z\u, zu = s) ∝p(O, V\t, vut = k, Z\u, zu = s)
∝g( ˜V\t,vut=k,Z\u,zu=s),
(8.103)
where g(·) is deﬁned as follows:
g( ˜sk) ≜( ˜φw
sk)
(
˜φμ
sk
)−D
2


 ˜φr
sk
2
D 
d
˜rsd
2
−
˜φr
sk
2
.
(8.104)

8.3 Gibbs sampling-based speaker clustering
371
Algorithm 18 Gibbs sampling-based multi-scale mixture model.
1: Initialize 0
2: repeat
3:
for u = shufﬂe (1 · · · U) do
4:
for t = shufﬂe (1 · · · Tu) do
5:
Sample vu,t by using Eq. (8.105)
6:
end for
7:
end for
8:
for u = shufﬂe (1 · · · U) do
9:
Sample zu by using Eq. (8.107)
10:
end for
11: until some condition is met
Therefore, by considering the normalization constant, the posterior probability can be
obtained as follows:
p(vu,t = k|O, V\t, Z\u, zu = s) =
g( ˜V\t,vut=k,Z\u,zu=s)

k=1K g( ˜V\t,vut=k′,Z\u,zu=s)
.
(8.105)
This equation is analytically derived by using the marginal likelihood for complete data
(Eq. (8.101)).
Utterance-level mixture component
As with the frame-level mixture component case, the Gibbs sampler assigns utterance-
level mixture s at utterance u by using the following equation:
p(zu = s|O, V, Z\u) ∝p(O, V, Z\u, zu = s)
∝(
k ˜ws\u,k)
(
k ˜ws,k)

k
g( ˜sk).
(8.106)
The value of ˜s\u,k is computed by the sufﬁcient statistics using O\u and V\u. Therefore,
the posterior probability can be obtained as follows:
p(zu = s′|O, V, Z\u)
=
exp
(
log
(
k ˜ws′\u,k)
(
k ˜ws′,k) + 
k gs′,k( ˜s′,k) −gs′,k( ˜s′\u,k)
)

s,k exp
(
log (
k ˜ws\u,k)
(
k ˜ws,k) + gs,k( ˜s,k) −gs,k( ˜s\u,k)
) .
(8.107)
Thus, we can derive a solution for the multi-scale mixture model based on Gibbs
sampling, which jointly infers the latent variables by interleaving frame-level and
utterance-level samples. Algorithm 18 provides a sample code for the proposed
approach.

372
Markov chain Monte Carlo
Table 8.1 Comparison of MCMC and VB for speaker clustering. ACP: average cluster purity, ASP:
average speaker purity, and K value: geometric mean of ACP and ASP.
Evaluation data
Method
ACP
ASP
K value
CSJ-1
MCMC
0.808
0.898
0.851
(# spkr10, # utt 50)
VB
0.704
0.860
0.777
CSJ-2
MCMC
0.852
0.892
0.871
(# spkr10, # utt 100)
VB
0.695
0.846
0.782
CSJ-3
MCMC
0.866
0.892
0.879
(# spkr10, # utt 200)
VB
0.780
0.870
0.823
CSJ-4
MCMC
0.784
0.694
0.738
(# spkr10, # utt 2,491)
VB
0.773
0.673
0.721
CSJ-5
MCMC
0.740
0.627
0.681
(# spkr10, # utt 2,321)
VB
0.693
0.676
0.684
MCMC-based acoustic modeling for speaker clustering was investigated with respect
to the difference in the MCMC and VB estimation methods by Tawara, Ogawa, Watan-
abe et al. (2012). Table 8.1 shows speaker clustering results in terms of the average
cluster purity (ACP), average speaker purity (ASP), and geometric mean of those val-
ues (K value) with respect to the evaluation criteria in speaker clustering. We used the
Corpus of Spontaneous Japanese (CSJ) dataset (Furui et al. 2000) and investigated the
speaker clustering performance for MCMC and VB for various amounts of data. Table
8.1 shows that the MCMC-based method outperformed the VB method by avoiding
local optimum solutions, especially when only a few utterances could be used. These
results also supported the importance of the Gibbs-based Bayesian properties.
Since the mixture of GMM is trained by MCMC, it is a straightforward extension to
deal with a Dirichlet process mixture model, as discussed in Section 8.2.5, for speaker
clustering, where the number of speaker clusters is jointly optimized based on this
model. There are several studies of applying the Dirichlet process mixture model to
speaker clustering (Fox et al. 2008, Tawara, Ogawa, Watanabe et al. 2012b). The next
section introduces the application of an MCMC-based Dirichlet process mixture model
to cluster HMMs (mixture of HMMs).
8.4
Nonparametric Bayesian HMMs to acoustic unit discovery
This section describes an application of Bayesian nonparametrics in Section 8.2 to
acoustic unit discovery based on HMMs (Lee & Glass 2012, Lee, Zhang & Glass 2013,
Torbati, Picone & Sobel 2013, Lee 2014). Acoustic unit discovery aims to automat-
ically ﬁnd an acoustic unit (e.g., phoneme) from speech data without transcriptions,
and this is used to build ASR or spoken term detection systems with limited language
resources (Schultz & Waibel 2001, Lamel, Gauvain & Adda 2002, Jansen, Dupoux,
Goldwater et al. 2013). One of the powerful advantages of Bayesian nonparametrics
is to ﬁnd the model structure appropriately, and it is successfully applied to word unit

8.4 Nonparametric Bayesian HMMs to acoustic unit discovery
373
discovery in natural language processing (Goldwater, Grifﬁths & Johnson 2009, Mochi-
hashi, Yamada & Ueda 2009) and in spoken language processing (Neubig, Mimura,
Mori & Kawahara 2010). This section regards sub-word units as latent variables in
one nonparametric Bayesian model. More speciﬁcally, it formulates a Dirichlet pro-
cess mixture model where each mixture is an HMM used to model a sub-word unit and
to generate observed segments of that unit. This model seeks the set of sub-word units,
segmentation, clustering, and HMMs that best represents the observed data through an
iterative inference process. This inference process can be performed by using Gibbs
sampling.
To realize this acoustic unit discovery, the approach deals with the following
variables:
• D dimensional speech feature on
t ∈RD at frame t in utterance n.
• Binary boundary variable bn
t ∈{0, 1} that has value 1 when the speech frame t is at
the end point of a segment, and 0 otherwise.
• Boundary index gn
q = {1, · · · , t, · · · , Tn} returns the frame index of the qth boundary
in utterance n. The initial boundary gn
0 = 0.
• Segment of features On
t:t′ = {on
t , · · · , on
t′}.
• Unit label cn
t:t′ ∈{1, · · · , u, · · · , U} to specify the unit label of On
t:t′. U is the number
of the cluster, and u is a unit index. In addition, cn
t ∈{1, · · · , u, · · · , U} indicates a
unit label at frame t and utterance n.
• HMM u that represents one cluster unit u with a standard continuous density HMM
(Section 3.2.3) that has state transition auij ∈[0, 1] from HMM state i to j, mixture
weight ωujk ∈[0, 1] at mixture component k in state j, and the mean vector μujk ∈RD
and (diagonal) precision matrix Rujk ∈RD×D.
• HMM state and GMM component sn
t ∈{1, · · · , j, · · · , J} and vn
t ∈{1, · · · , k,
· · · , K}.
The difference between this HMM and the conventional CDHMM in Section 3.2.3 given
a phoneme unit is that this approach regards unit label cn
t:t′ and the number of units U
as a latent variable, which is obtained by a Dirichlet process. Therefore, the notation is
similar to that in Section 3.2.3 except that it includes the cluster index u explicitly. A
similar model is used in Gish, Siu, Chan et al. (2009) and Siu, Gish, Chan et al. (2014)
based on an ML-style iterative procedure instead of Bayesian nonparametrics. In this
section, the numbers of HMM states J and mixture components K are assumed to be
the same ﬁxed values for all units, but they can also be optimized by using Bayesian
nonparametrics (Rasmussen 2000, Beal, Ghahramani & Rasmussen 2002, Grifﬁths &
Ghahramani 2005).
8.4.1
Generative model and generative process
Since we use a fully Bayesian approach, the variables introduced in this model are
regarded as probabilistic variables. For simplicity we consider that the boundary vari-
able bn
t is given in this formulation. The Bayesian approach ﬁrst provides a generative
process for complete data. We deﬁne latent variable Z as

374
Markov chain Monte Carlo
Z ≜{C, S, V, {γu}∞
u=1, {u}∞
u=1}.
(8.108)
Here, we assume the number of units is inﬁnite, i.e., U = ∞, and latent variables will
be generated based on the Dirichlet process. The other variables are deﬁned as
C ≜{cn
t |t = 1, · · · , Tn, n = 1, · · · , N}
= {cn
(gq+1):gq+1|q = 0, · · · , Qn, n = 1, · · · , N},
S ≜{sn
t |t = 1, · · · , Tn, n = 1, · · · , N},
V ≜{vn
t |t = 1, · · · , Tn, n = 1, · · · , N}.
(8.109)
Qn is the number of units appearing in utterance n, and ζu ∈[0, 1] is a weight parameter
of unit u. Then, the conditional distribution is represented as follows:
p(O, C, S, V|{ζu}∞
u=1, {u}∞
u=1)
=
Qn

q=0
p(cn
(gq+1):gq+1 = u|{ζu}∞
u=1)p(on
gq+1, sn
gq+1, vn
gq+1|u, u)
×
gq+1

t=gq+2
p(on
t , sn
t−1, sn
t , vn
t |u, u),
(8.110)
where each likelihood function can be represented as follows:
p(cn
(gq+1):gq+1 = u|{ζu}∞
u=1) = ζu,
(8.111)
and

p(on
t , sn
t = j, vn
t = k|u, u) = aujωujkN(on
t |μujkujk)
(t = gq + 1),
p(on
t , sn
t−1 = i, sn
t = j, vn
t = k|u, u) = auijωujkN(on
t |μujkujk)
Otherwise.
(8.112)
auj is an initial weight in an HMM. For p({ζu}∞
u=1, {u}∞
u=1), we use the Dirichlet process
mixture model, described in Section 8.2.5.
The model parameters are sampled from a base distribution with hyperparameter
0, and we use a conjugate prior distribution of CDHMM p(u|0) with diagonal
covariance matrices, as we discussed in Section 4.3.2, which is represented as
p(u)
= p({auj}J
j=1)
 J
i=1
p({auij}J
j=1)
 ⎛
⎝
J
j=1
p({ωujk}K
k=1)
⎞
⎠
⎛
⎝
J
j=1
K

k=1
p(μujk, ujk)
⎞
⎠
= Dir({auj}J
j=1|φπ)
 J
i=1
Dir({auij}J
j=1|φa)
 ⎛
⎝
J
j=1
Dir({ωujk}K
k=1|φω)
⎞
⎠
×
⎛
⎝
J
j=1
K

k=1
D

d=1
N(μjkd|μ0
d, (φμrjkd)−1)Gam(rjkd|r0
d, φr)
⎞
⎠.
(8.113)

8.4 Nonparametric Bayesian HMMs to acoustic unit discovery
375
φa, φω, φμ, φr, μ0, and r0 are the prior hyperparameter (≜0). μ0 and r0 can be
obtained using the Gaussian mean and precision parameters of all data.
Algorithm 19 provides a generative process of a nonparametric Bayesian HMM. A
Dirichlet process mixture model (DPM) can sample the acoustic unit u from existing
clusters or a new cluster for every speech segment. Thus, the model ﬁnally generates a
sequence of speech features without ﬁxing an acoustic unit explicitly.
Algorithm 19 Generative process of a nonparametric Bayesian HMM
Require: Concentration parameter γ , Base distribution of DP 0, Boundary bn
t .
1: for every utterance n = 1, · · · , N do
2:
for every segment q = 1, · · · , Qn do
3:
Draw u and u from DPM(cn
(gq+1):gq+1, u|γ , 0) (from existing clusters or a
new one)
4:
Draw au from Dir(φπ
u )
5:
Draw aui from Dir(φa
ui)
6:
Draw ωuj from Dir(φω
uj)
7:
for every feature dimension d = 1, · · · , D do
8:
Draw rujkd from Gam(φr, r0
d)
9:
Draw μujkd from N(μ0
d, (φμrujkd)−1)
10:
end for
11:
Draw j from Mult(sn
gq+1|{auj′}J
j′=1)
12:
Draw k from Mult(vn
gq+1|{ωujk′}K
k′=1)
13:
Draw o from N(on
gq+1|μujk, ujk)
14:
for every frame t = gq + 2, · · · , gq+1 do
15:
Draw j from Mult(sn
t |{aust−1j′}J
j′=1)
16:
Draw k from Mult(vn
t |{ωujk′}K
k′=1)
17:
Draw o from N(on
t |μujk, ujk)
18:
end for
19:
end for
20: end for
8.4.2
Inference
The approach infers all latent variables by using Gibbs sampling, as discussed in Section
8.1.4, which samples a target latent variable z from the following conditional posterior
distribution:
z ∼p(z|Z\z, O),
(8.114)
where Z\z denotes a set of all hidden variables in Z except for z. In this section we
provide conditional distributions for cluster label cn
t:t′, HMM state sequence Sn
t:t′, GMM
component sequence Vn
t:t′, and HMM parameters u so that we can perform the Gibbs
sampling of these latent variables.

376
Markov chain Monte Carlo
• Cluster label cn
t:t′:
Let U be the set of distinctive cluster units. The conditional posterior distribution of
ct:t′ = u ∈U is represented as follows:
p(cn
t:t′ = u| · · · ) ∝p(cn
t:t′ = u|U\u, γ )p(On
t:t′|u)
=
nu
Nu −1 + γ p(On
t:t′|u),
(8.115)
where γ is a hyperparameter of the DP prior, nu represents the number of cluster
labels in U\u taking the value u, and Nu is the number of speech segments. In this
formulation, we do not marginalize u unlike Section 8.3, but use the sampled values,
as discussed below.
If cn
t:t′ belongs to a new cluster that has not existed before, the conditional
posterior distribution for this new cluster is represented as
p(cn
t:t′ ̸= u, u ∈U| · · · ) ∝
γ
Nu −1 + γ

p(On
t:t′|)G(|0)d,
(8.116)
where the integral is approximated by a Monte Carlo estimation (Rasmussen 1999,
Neal 2000, Tawara, Ogawa, Watanabe et al. 2012b). Note that Eqs. (8.115) and
(8.116) are a typical solution of the Dirichlet process in Eq. (8.38). The Gibbs sam-
pler for existing clusters p(cn
t:t′ = u| · · · ) depends on the number of occurrences nu
and their likelihood, while that for a new cluster p(cn
t:t′ ̸= u, u ∈U| · · · ) depends on
the concentration parameter γ and the marginalized likelihood.
The likelihood values of p(On
t:t′|u) and p(On
t:t′|) can be computed by consid-
ering all possible HMM states Sn
t:t′ and mixture components Vn
t:t′ based on the forward
algorithm in Section 3.3.1. However, since the following Gibbs samplers can sample
Sn
t:t′ and Vn
t:t′, we can use the following conditional likelihood values:
p(On
t:t′|) ≈p(On
t:t′|Sn
t:t′, Vn
t:t′, ).
(8.117)
This is easily computed by accumulating all Gaussian likelihood values given Sn
t:t′ and
Vn
t:t′.
• HMM state sn
t :
The conditional posterior distribution of HMM state sn
t is obtained from the following
distribution, given the previous state sn
t−1 and the succeeding state sn
t+1:
p(sn
t = j| · · · )
∝p(sn
t = j|sn
t−1)p(on
t |u, sn
t = j)p(sn
t+1|sn
t = j)
=
⎧
⎨
⎩
auj
(K
k=1 ωujkN(on
t |μujkujk)
)
aujst+1
(t = gq + 1)
aust−1j
(K
k=1 ωujkN(on
t |μujkujk)
)
aujst+1
otherwise.
(8.118)
Note that this algorithm does not require the forward–backward algorithm in Sec-
tion 3.3.1 to obtain the occupation probability, compared with the conventional HMM.
There is an alternative algorithm to sample a state sequence similar to the forward-
backward algorithm, called the forward ﬁltering backward sampling algorithm of an
HMM (Scott 2002, Mochihashi et al. 2009) in the MCMC framework.

8.4 Nonparametric Bayesian HMMs to acoustic unit discovery
377
Given vn
t
= k, which is also obtained by the Gibbs sampler, we can also
approximate Eq. (8.118) as
p(sn
t = j| · · · ) ≈p(sn
t = j|vn
t = k, · · · )
∝

aujωujkN(on
t |μujkujk)au,j,st+1
(t = gq + 1)
aust−1jωujkN(on
t |μujkujk)au,j,st+1
otherwise.
(8.119)
This avoids computing the Gaussian likelihoods for all mixture components.
• GMM component vn
t :
The conditional posterior distribution of GMM component vn
t = k at cluster u and
state sn
t = j is obtained from the following distribution:
p(vn
t = k| · · · ) ∝p(vn
t = k|u, sn
t = j)p(on
t |u, sn
t = j, vn
t = k)
= ωujkN(on
t |μujk, ujk).
(8.120)
Thus, the Gibbs samplers of p(cn
t:t′| · · · ), p(sn
t | · · · ), and p(vn
t | · · · ) can provide the latent
variable sequences of C, S, and V. Once we have C, S, and V, we can compute the
sufﬁcient statistics for the CDHMM, as follows:
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
ξui
= 
n,q δ(cn
(gq+1):gq+1, u)δ(sn
gq+1, i),
ξuij
= 
n,t δ(cn
t , u)δ(sn
t−1, i)δ(sn
t , j),
γujk
= 
n,t δ(cn
t , u)δ(sn
t , j)δ(vn
t , k),
γ (1)
ujk
= 
n,t δ(cn
t , u)δ(sn
t , j)δ(vn
t , k)on
t ,
γ (2)
ujkd = 
n,t δ(cn
t , u)δ(sn
t , j)δ(vn
t , k)(on
td)2.
(8.121)
Thus, we can obtain the posterior distribution analytically based on the conjugate analy-
sis, as we have shown in Sections 2.1.4 and 4.3. This section only provides the analytical
solutions for the posterior distributions of CDHMM parameters, which are used to
sample CDHMM parameters.
• HMM parameters u:
– Initial weight
p(au| · · · ) ∝Dir(au|φπ
u ),
(8.122)
where
˜φπ
ui = φπ + ξui.
(8.123)
– State transition
p(aui| · · · ) ∝Dir(aui|φa
ui),
(8.124)
where
˜φa
uij = φa + ξuij.
(8.125)
– Mixture weight
p(ωuj| · · · ) ∝Dir(ωuj|φω
uj),
(8.126)

378
Markov chain Monte Carlo
where
˜φω
ujk = φω + γujk.
(8.127)
– Mean vector and covariance matrix at dimension d
p(μujkd, rujkd| · · · )
∝N(μujkd| ˜μujkd, ( ˜φμ
ujkrujkd)−1)Gam(rujkd)| ˜φr
ujk, ˜rujkd),
(8.128)
where
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
˜φμ
ujk
= φμ + γujk,
˜μujk
=
φμμ0+γ (1)
ujk
φμ+γujk ,
˜φr
ujk
= φr + γujk,
˜rujkd
= γ (2)
ujkd + φμ(μ0
d)2 −˜φμ
ujk( ˜μujkd)2 + r0
d.
(8.129)
Note that compared with the collapsed Gibbs sampling solutions in Section 8.3, this
samples the Gaussian parameters, as well as the other variables. Therefore, the Gibbs
samplers for latent variables become rather simple equations.
Thus, we can obtain the nonparametric Bayesian HMMs for acoustic unit discovery
given cluster boundaries. The MCMC is performed by sampling latent variables C, S,
and V, and model parameters {u}U
u=1, iteratively. Note that the number of clusters U
is changing according to the Dirichlet process, and ﬁnally becomes a ﬁxed number.
This clustering corresponds to automatically obtaining the acoustic unit in a Bayesian
nonparametric manner.
Lee & Glass (2012) also considered cluster boundaries as latent variables which can
be obtained by Gibbs sampling. In addition, to provide an appropriate initialization of
the boundaries, the approach uses a pre-segmentation technique based on Glass (2003).
The model obtained was compared with the other acoustic unit discovery method based
on the dynamic time warping technique using the distance between GMM posteri-
ors (Zhang & Glass 2009). The nonparametric Bayesian HMM achieved comparable
results to the dynamic time warping technique in terms of the spoken term detection
measures, which shows the effectiveness of the nonparametric Bayesian acoustic unit
discovery. The approach carries a huge computational cost compared with the other
EM type approaches (based on ML, MAP, and VB), and its scalability for the amount
of data and the difﬁculty of parallelization remain serious problems. However, this is
one of a few successful approaches using Bayesian nonparametrics for speech data, and
it potentially has various advantages over the conventional EM type approaches (e.g., it
could mitigate the local optimum problem and use any other distributions than conjugate
distributions).
8.5
Hierarchical Pitman–Yor language model
In an LVCSR system, in addition to an acoustic model based on HMMs, the other key
component is a language model based on n-grams. In recent years, BNP learning has

8.5 Hierarchical Pitman–Yor language model
379
been substantially developed for language modeling (Teh et al. 2006) and successfully
applied for several LVCSR tasks in Huang & Renals (2008). In this section, we introduce
a hierarchical Bayesian interpretation for language modeling, based on a nonparamet-
ric prior called the Pitman–Yor (PY) process. The motivation of conducting Bayesian
learning of n-gram language models is to tackle the limitations such as overﬁtting of
maximum likelihood estimation and the lack of rich contextual knowledge sources. The
PY process offers a principled approach to language model smoothing which produces
a power-law distribution for natural language. The Bayesian language model based on
Bayesian nonparametrics is a realization of a full Bayesian solution according to the
MCMC inference procedure. Such a model is a distribution estimate, which is different
from the Bayesian language model based on maximum a-posteriori (MAP) estimation
as addressed in Section 4.7. A MAP-based language model is known as a point estimate
of language model. A BNP-based language model integrates the values of parameters
into a marginalized language model. It is interesting to note that the resulting hierarchi-
cal PY language model is a direct generalization of the hierarchical Dirichlet language
model.
In what follows, we ﬁrst survey the PY process and explain the importance of the
power-law property in language modeling. Then we revisit language model smoothing
based on Kneser–Ney smoothing and ﬁnd the clue for connection to BNP learning. Next
the hierarchical PY process is constructed to estimate a language model which provides
the hierarchical Bayesian interpretation for a Kneser–Ney smoothed language model.
The relation to the hierarchical Dirichlet language model will be illustrated. Lastly, the
MCMC inference for the hierarchical PY language model is addressed.
8.5.1
Pitman–Yor process
The PY process (Pitman & Yor 1997) is known as the two-parameter Poisson–Dirichlet
process PY(d, α0, G0), which is expressed as a three-parameter distribution over distri-
butions where 0 ≤d < 1 is a discount parameter, α0 is a strength parameter and G0 is a
base distribution. Base distribution G0 can be understood as the mean of draws from the
PY process. This process can be used to draw the unigram language model. Let G(w)
denote the unigram probability of a word w ∈V and G = [G(w)]w∈V denote the vector
of unigram probabilities, which is drawn from a PY process:
G ∼PY(d, α0, G0).
(8.130)
Here, G0 = [G0(w)]w∈V is a mean vector where G0(w) is the a-priori probability of word
w. In practice, this base measure is usually set to be uniform G0 = 1/|V| for all w ∈V.
The parameters d and α0 both control the degree of variability around G0 in different
ways. When d = 0, the PY process reverts to the DP, which is denoted by DP(α0, G0).
The PY process is seen as a generalization of the DP.
Basically, there is no analytic form for distribution of the PY process. We would like
to work out the nonparametric distribution over sequences of words from the PY pro-
cess. Let {w1, w2, · · · } be a sequence of words drawn independently and identically from
G given the mean distribution G0. The procedure of generating draws from a PY process

380
Markov chain Monte Carlo
that can be described according to the metaphor of the “Chinese restaurant process” (Pit-
man 2006). As introduced in Section 8.2.4, imagine a Chinese restaurant containing an
inﬁnite number of tables, each with inﬁnite seating capacity. Customers enter the restau-
rant and seat themselves. The ﬁrst customer sits at the ﬁrst table while each subsequent
customer sits at an occupied table with probability proportional to the number of cus-
tomers who are already sitting there ck−d, or at a new unoccupied table with probability
proportional to α0 + dm· where m· is the current number of occupied tables. That is, if
zi is the index of the table chosen by the ith customer, this customer sits at the table k
given the seating arrangement of the previous i −1 customers, z−i = {z1, · · · , zi−1},
with probability
p(zi = k|z−i, d, α0) =

ck−d
α0+c·
1 ≤k ≤m·,
α0+dm·
α0+c·
k = m· + 1,
(8.131)
where ck denotes the number of customers sitting at table k and c· = 
k ck is the total
number of customers. The above generative procedure produces a sequence of words
drawn independently from G with G marginalized out.
It is important to investigate the behaviors of drawing the sequence of words from
the PY process. Firstly, the rich-gets-richer clustering property can be observed. That
is, the more words have been assigned to a draw from G0, the more likely subsequent
words will be assigned to the draw. Secondly, the more we draw from G0, the more
likely a new word will be assigned to a new draw from G0. Combining these two behav-
iors produces the so-called power-law distribution where many unique or distinct words
are observed, most of them rarely. This distribution resembles the distribution of words
which is seen in natural language. The power-law distribution has been found to be one
of the most striking statistical properties of word frequencies in natural language. Fig-
ure 8.10 demonstrates the power-law behavior of the PY process which is controlled by
parameters d and α0, showing the average number of unique words among 25 sequences
of words drawn from G, as a function of the number of words, for various values of α0
and d. We ﬁnd that α0 controls the total number of unique words while d adjusts the
asymptotic growth of the number of unique words. Figure 8.11 displays the proportion
of words appearing only once among the unique words. These ﬁgures indicate the pro-
portion of words that occur rarely. We can see that larger d and α0 produce more rare
words. This phenomenon is reﬂected by the probability of producing a new unoccupied
table
p(zi = knew|z−i, d, α0) = α0 + dm·
α0 + c·
.
(8.132)
8.5.2
Language model smoothing revisited
A key issue in a language model is to handle the sparseness of training data for train-
ing n-gram parameters under the conditions of a large n-gram window size n and large
dictionary size |V|. As addressed in Section 3.6, a series of language model smoothing
methods has been developed to tackle the data sparseness issue in a statistical n-gram

8.5 Hierarchical Pitman–Yor language model
381
Figure 8.10
Power-law distribution: (a) the number of unique words as a function of the number of words,
drawn on a log–log scale with d = 0.6 and α0 = 100 (dashdot line), 20 (solid line), and 1
(dashed line); (b) the same as (a) with α0 = 20 and d = 0.9 (dashdot line), 0.6 (solid line), and 0
(dashed line).
model. One important trick in these methods is to incorporate an absolute discount
parameter d in the count of an observed n-gram event wi
i−n+1 = {wi wi−1
i−n+1} ≜{w u} of
a word w = wi and its preceding history words u = wi−1
i−n+1. Owing to this discount, we
modify the counts for lower order n-gram probabilities so as to construct the interpolated
Kneser–Ney smoothing (Kneser & Ney 1995):
pKN(w|u) = max{cuw −d|u|, 0}
cu·
+ d|u|mu·
cu·
pKN(w|π(u)),
(8.133)

382
Markov chain Monte Carlo
Figure 8.11
Power-law distribution: (a) the proportion of words appearing only once, as a function of the
number of words drawn with d = 0.6 and α0 = 100 (dashdot line), 20 (solid line), and 1 (dashed
line); (b) the same as (a) with α0 = 20 and d = 0.9 (dashdot line), 0.6 (solid line), and 0 (dashed
line).
which is rewritten from Eq. (3.232) with an order-dependent discount parameter
d →d|u| and a new notation N1+(wi−1
i−n+1, •) ≜mu· = |{w′|cuw′ > 0}|, expressing
the number of unique words that follow the history words u. The term π(u) denotes the
backoff context of u. If u = wi−1
i−n+1, then π(u) = wi−1
i−n+2.
We would like to investigate the relation between language model smoothing and
Bayesian learning. Such a relation is crucial to developing a BNP-based language model.
In a standard Bayesian framework for language modeling, a prior distribution is placed
over the predictive distribution for the language model. The predictive distribution is

8.5 Hierarchical Pitman–Yor language model
383
estimated by marginalizing out the latent variables from the posterior distribution. The
problem of zero-probability can be avoided by taking advantage of knowledge expressed
by the priors. A smoothed language model can be obtained. In Yaman et al. (2007),
the n-gram smoothing was conducted under a framework called the structural maxi-
mum a-posteriori (MAP) adaptation where the interpolation of n-gram statistics with an
n −1-gram model was performed in a hierarchical and recursive fashion. As mentioned
in Chapter 4, such a MAP approximation only ﬁnds a point estimate of a language model
without considering the predictive distribution. To pursue a full Bayesian language
model, the hierarchical Dirichlet language model (MacKay & Peto 1995) in Section 5.3
calculates the hierarchical predictive distribution of an n-gram by marginalizing the
Dirichlet posterior over the Dirichlet priors.
However, the PY process was shown to be more ﬁtted as a prior distribution than
a Dirichlet distribution to the applications in natural language processing (Goldwater,
Grifﬁths & Johnson 2006). It is because the power-law distributions of word frequencies
produced by the PY process are more likely to be close to the heavy-tailed distributions
observed in natural language. But, the PY process, addressed in Section 8.5.1, is only
designed for a unigram language model. In what follows, we extend the hierarchical
Dirichlet language model, which adopts the Dirichlet prior densities, to the hierarchi-
cal PY language model, which utilizes the PY process as nonparametric priors and
integrates out these prior measures.
8.5.3
Hierarchical Pitman–Yor language model
Similarly to the extension from the Dirichlet process to the hierarchical Dirichlet process
for representation of multiple documents, in this section, we address the extension from
the PY process to the hierarchical PY (HPY) process which is developed to realize the
probability measure for the smoothed n-gram language model. Let G∅= [G∅(w)]w∈V
represent the vector of word probability estimates for unigrams of all words w in a
vocabulary V. A PY process prior for unigram probabilities is expressed by
G∅∼PY(d0, α0, G0),
(8.134)
where G0 is a global mean vector in the form of a noninformative base distribution or
uniform distribution:
G0(w) = 1
|V|
for all w ∈V.
(8.135)
This PY process can be used to calculate the predictive unigram of a word w according
to the metaphor of the Chinese restaurant process. The customers or word tokens enter
the restaurant and seat themselves at either an occupied table or a new table with the
probabilities in Eq. (8.131). Each table is labeled by a word w ∈V initialized by the
ﬁrst customer sitting on it. The next customer can only sit on those tables with the same
label w. That is, those cw customers corresponding to the same word label w can sit
at different tables, with mw being the number of tables with label w. In our notation,
m· = 
k mk means the total number of tables. The number of customers at table k is
denoted by ck, and the total number of customers is expressed by c· = 
k ck. Given

384
Markov chain Monte Carlo
the seating arrangement of customers S, the discount parameter d0, and the strength
parameter α0, the predictive unigram probability of a new word w is given by
p(w|S, d0, α0) =
m·

k=1
ck −d0
α0 + c·
δ(k, w) + α0 + d0m·
α0 + c·
G0(w)
= cw −d0mw
α0 + c·
+ α0 + d0m·
α0 + c·
G0(w),
(8.136)
where δ(k, w) = 1 if table k has the label w, δ(k, w) = 0 otherwise. This equation is
derived similarly to the metaphor of the Chinese restaurant process designed for the
Dirichlet process, as mentioned in Section 8.2.4. The key difference of the PY process
compared to the Dirichlet process is the discount parameter d0 which is used to adjust
the power-law distribution of unigrams based on the PY process. By averaging over
seating arrangements and hyperparameters (S, d0, α0), we obtain the probability p(w)
for a unigram language model.
Interestingly, if we set d0 = 0, the PY process is reduced to a DP which produces the
Dirichlet distribution Dir(α0G0). In this case, the predictive unigram probability based
on the PY process prior in Eq. (8.136) is accordingly reduced to
p(w|S, α0) =
cw
α0 + c·
+ α0G0(w)
α0 + c·
=
cw + α0
|V|

w∈V[cw + α0
|V|],
(8.137)
where G0(w) = 1/|V| is used for all w. This equation is equivalent to the hierarchical
Dirichlet unigram model as given in Eq. (5.70). The only difference here is to adopt a
single shared hyperparameter α0 for all word labels w ∈V.
Next, we extend the unigram language model to the n-gram language model based on
the HPY process. An n-gram language model is deﬁned as a probability measure over
the current word w = wi given a history context u = wi−1
i−n+1. Let Gu = [Gu(w)]w∈V be
the vector of the target probability distributions of all vocabulary words w ∈V given the
history context u. We use a PY process as the prior for Gu[Gu(w)]w∈V in the form of
Gu ∼PY(d|u|, α|u|, Gπ(u)),
(8.138)
with the hyperparameters d|u| and α|u| speciﬁc to the length of context |π(u)|. However,
Gπ(u) is still an unknown base measure. A PY process is recursively placed over it:
Gπ(u) ∼PY(d|π(u)|, α|π(u)|, Gπ(π(u))),
(8.139)
with parameters which are functions of |π(π(u))|. This is repeated until we reach the
PY process prior G∅for a unigram model. Figure 8.12 is a schematic diagram showing
the hierarchical priors for the smoothed language model based on the HPY process.
This process enables us to generalize from the unigram language model to the n-gram
language model. The resulting probability distribution is called the hierarchical Pitman–
Yor language model (HPYLM).
A hierarchical Chinese restaurant process can be used to develop a generative pro-
cedure for drawing words from the HPYLM with all Gu marginalized out. The context

8.5 Hierarchical Pitman–Yor language model
385
Figure 8.12
Hierarchical Pitman–Yor process for n-gram language model.
u corresponds to a restaurant. This procedure gives us a representation of HPYLM for
efﬁcient inference using an MCMC algorithm, and easy computation of the predictive
probability distribution from new test words. Through this representation, the corre-
spondence between the Kneser–Ney language model and HPYLM can be illustrated.
In the metaphor of hierarchical CRP, there are multiple hierarchical restaurants or PY
processes, with each corresponding to one context. Different orders of n-gram mod-
els share information with each other through the interpolation of higher-order n-grams
with lower-order n-grams. We draw words from Gu of the PY process by using the
CRP as discussed in Section 8.5.1. Further, we draw words from another CRP to sample
the parent distribution Gπ(u), which is itself sampled according to a PY process. This is
recursively applied until we need draws from the global mean distributions G0. By refer-
ring to the predictive unigram probability from the PY process as shown in Eq. (8.136),
the predictive n-gram probability p(w|u, S, d|u|, α|u|) under a particular combination of
seating arrangement S and the hyperparameters d|u| and α|u| can be obtained from
p(w|u, S, d|u|, α|u|) = cuw· −d|u|muw
α|u| + cu··
+ α|u| + d|u|mu·
α|u| + cu··
p(w|π(u), S, d|π(u)|, α|π(u)|),
(8.140)
where cuwk is the number of customers sitting at table k with label w, cuw· = 
k cuwk
and muw is the number of occupied tables with label w. It is interesting to see that the
Kneser–Ney language model (KN–LM) in Eq. (8.133) is closely related to the HPYLM
in Eq. (8.140). HPYLM is a generalized realization of KN–LM with an additional con-
centration parameter α|u|. We can interpret the interpolated KN–LM as an approximate
inference scheme for the HPYLM.
8.5.4
MCMC inference for HPYLM
An MCMC algorithm can be used to infer the posterior probability of seating arrange-
ment S. Given some training data D, we count the number of occurrences cuw· of each
word w appearing after each context u of length n −1. This means that there are cuw·
samples drawn from the PY process Gu. We are interested in the posterior probability
over the latent variables G = {Gu | all contexts u} and the parameters  = {dm, αm} of
all lower-order models with 0 ≤m ≤n−1. Because the hierarchical Chinese restaurant
process marginalizes out each Gu, we can replace G by the seating arrangement in the

386
Markov chain Monte Carlo
corresponding restaurant using S = {Su | all contexts u}. The posterior probability is
then obtained from
p(S, |D) = p(S, , D)
p(D)
.
(8.141)
Given this posterior probability, we can calculate the predictive n-gram probability of a
test word w after observing a context u:
p(w|u, D) =

p(w|u, S, )p(S, |D)d(S, )
= E(S,)[p(w|u, S, )]
≈1
L
L

l=1
p(w|u, S(l), (l)),
(8.142)
which is an expectation of predictive probability under a particular set of seating
arrangements S and PY parameters . The overall predictive probability in the integral
of Eq. (8.142) is approximated by using the L samples {S(l), (l)} drawn from posterior
probability p(S, |D).
In the implementation of Teh et al. (2006) and Huang & Renals (2008), the dis-
count parameter and concentration parameter were drawn by a beta distribution and
a gamma distribution, respectively. Here, we address the approach to sampling the seat-
ing arrangement Su corresponding to Gu. We employ Gibbs sampling to keep track of
the current state of each variable of interest in the model, and iteratively re-sample the
state of each variable given the current states of all other variables. After a sufﬁcient
number of iterations, the states of variables in the seating arrangement S converge to
the required samples from the posterior probability. The variables consist of, for each
context (restaurant) u and each word (customer) xul drawn from Gu, the index kul of the
draw from Gπ(u) assigned xul. In the Chinese restaurant metaphor, this is the table index
of where the lth customer sat at the restaurant corresponding to Gu. If xul has value w,
it can only be assigned to draws from Gπ(u) that have the same value w. The posterior
probability of drawing a table kul for the last word xul from Gu is given below:
p(kul = k|S−ul, ) ∝
max(0, c−ul
uxulk −d)
α + c−ul
u··
,
(8.143)
p(kul = knew|S−ul, ) ∝α + dm−ul
u·
α + c−ul
u··
p(xul|π(u), S−ul, ),
(8.144)
where the superscript −ul means the corresponding set of variables or counts with xul
excluded.
One other key difference between the KN–LM in Eq. (8.133) and the HPYLM in
Eq. (8.140) is that the KN–LM adopts a ﬁxed discount d|u| while HPYLM produces
different discounts d|u|muw for different words w due to the values of muw, which counts
the number of occupied tables labeled by w. In general, muw is on average larger if cuw is
larger. The actual amount of discount grows gradually as the count cuw grows. The phys-
ical meaning of discounting in the smoothed n-grams based on HPYLM is consistent

8.6 Summary
387
with the discount scheme in the modiﬁed Kneser–Ney language model (MKN–LM),
which speciﬁes three discounts d1, d2, and d3+ for those n-grams with one (c = 1), two
(c = 2), and three or more (c ≥3) counts, as addressed in Section 3.6.6. The discounts
{d1, d2, d3+} in MKN–LM are empirically determined while the discounts d|u|muw in
HPYLM are automatically associated with each word w. In particular, if we restrict
muw· to be at most 1,
muw = min(1, cuw·),
(8.145)
we get the same discount value so long as cuw· > 0, or equivalently we conduct absolute
discounting. We also have the relationships among the cuws and muw:
cuw =

u′:π(u′)=u
mu′w.
(8.146)
If we further assume α|u| = αm = 0 for all model orders 0 ≤m ≤n −1, the predictive
probability of HPYLM in Eq. (8.140) is directly reduced to that given by the interpolated
Kneser–Ney model.
8.6
Summary
Due to the high computational cost and lack of scalability for data and model sizes,
MCMC approaches are still in a development stage for speech and language process-
ing applications that essentially need to deal with large-scale data. However, the recent
advance in computational powers (CPU speed, memory size, many cores, and GPU)
and algorithm development make it possible to realize middle-scale data processing of
MCMC, especially for language processing based on multinomial and Dirichlet distri-
butions, as discussed in this chapter. One of the most attractive features of MCMC is
that we can handle any types of expectation calculation by the Monte Carlo approxi-
mations, which can realize all Bayesian approaches in principle. Therefore, we expect
further development of computational powers and algorithms to widen the applications
of MCMC to large-scale problems, and enable fully Bayesian speech and language
processing based on MCMC in the near future.

Appendix A Basic formulas
A.1
Expectation
E(a)

f(a) + g(a)

= E(a)

f(a)

+ E(a)

g(a)

,
(A.1)
E(a)

bf(a)

= bE(a)

f(a)

.
(A.2)
A.2
Delta function
A.2.1
Kronecker delta function
fb =

a
δ(a, b)fa.
(A.3)
A.2.2
Product formula of Kronecker delta function
fb =

a
f δ(a,b)
a
.
(A.4)
(Proof)

a
f δ(a,b)
a
= exp

log

a
f δ(a,b)
a

= exp

a
δ(a, b) log (fa)

= exp (log (fb)) = fb.
(A.5)
A.2.3
Dirac delta function
f(y) =

δ(x −y)f(x)dx.
(A.6)

A.4 Gamma function
389
A.3
Jensen’s inequality
For a concave function f, where X is a probabilistic random variable sampled from a
distribution function, and an arbitrary function g(X), we have the following inequality:
f

E(X)[g(X)]

≥E(X)[f(g(X))].
(A.7)
In the special case of f(·) = log(·), (A.7) is rewritten as follows:
log

E(X)[g(X)]

≥E(X)[log(g(X))].
(A.8)
A.4
Gamma function
(x + 1) = x(x),
(A.9)

1
2

= π
1
2 .
(A.10)
A.4.1
Stirling’s approximation
log 
(x
2
)
→x
2 log
(x
2
)
−x
2 −1
2 log
( x
2π
)
,
x →∞.
(A.11)
A.4.2
Di-gamma function
(x) ≜∂
∂x log (x)
=
∂(x)
∂x
(x) .
(A.12)

Appendix B Vector and matrix formulas
This appendix lists some useful vector and matrix formulas. Note that these formulas
are selected for the purpose of deriving equations in this book in Bayesian speech and
language processing, and do not cover the whole ﬁeld of vector and matrix formulas.
B.1
Trace
tr[a] = a,
(B.1)
tr[ABC] = tr[BCA] = tr[CAB],
(B.2)
tr[A + B] = tr[A] + tr[B],
(B.3)
tr[A⊺] = tr[A],
(B.4)
tr[A(B + C)] = tr[AB + AC].
(B.5)
B.2
Transpose
(ABC)⊺= C⊺B⊺A⊺,
(B.6)
(A + B)⊺= A⊺+ B⊺.
(B.7)
B.3
Derivative
∂log |A|
∂A
= (A⊺)−1,
(B.8)
∂a⊺b
∂a
= ∂b⊺a
∂a
= b,
(B.9)
∂a⊺Cb
∂C
= ab⊺,
(B.10)
∂tr(AB)
∂A
= B⊺,
(B.11)

B.5 Woodbury matrix inversion
391
∂tr(ACB)
∂C
= A⊺B⊺,
(B.12)
∂tr(AC−1B)
∂C
= −(C−1BAC−1)⊺,
(B.13)
∂a⊺Ca
∂a
= (C + C⊺)a,
(B.14)
∂b⊺A⊺DAc
∂A
= D⊺Abc⊺+ DAcb⊺.
(B.15)
B.4
Complete square
When A is a symmetric matrix,
x⊺Ax −2x⊺b + c = (x −u)⊺A (x −u) + v,
(B.16)
where
u ≜A−1b
v ≜c −b⊺A−1b.
(B.17)
By using the above complete square formula, we can also derive the following formula
when matrices A1 and A2 are symmetric:
(x −b1)⊺A1(x −b1) + (x −b2)⊺A2(x −b2)
= x⊺(A1 + A2)



≜A
x −2x⊺(A1b1 + A2b2)



≜b
+ b⊺
1 A1b1 + b⊺
2 A2b2



≜c
= (x −u)⊺(A1 + A2) (x −u) + v,
(B.18)
where
u = (A1 + A2)−1(A1b1 + A2b2),
v = b⊺
1 A1b1 + b⊺
2 A2b2 −(A1b1 + A2b2)⊺(A1 + A2)−1(A1b1 + A2b2).
(B.19)
B.5
Woodbury matrix inversion
(A + UCV)−1 = A−1 −A−1U(C−1 + VA−1U)−1VA−1.
(B.20)

Appendix C Probabilistic distribution
functions
This appendix lists the probabilistic distribution functions (pdfs) used in the main dis-
cussions in this book. We basically use the deﬁnitions of these functions followed by
Bernardo & Smith (2009). Each pdf section also provides values of mean, mode, vari-
ance, etc., which are used in the book, although it does not include a complete set of
distribution values to avoid complicated descriptions in this appendix. The sections also
provide some derivations of these values if these derivations are not complicated.
C.1
Discrete uniform distribution
When we have a discrete variable a ∈{a1, a2, · · · , an, · · · , aN}, the discrete uniform
distribution is deﬁned as:
• pdf:
Unif(a) ≜
1
|{an}| = 1
N ,
(C.1)
where N is the number of distinct elements.
C.2
Multinomial distribution
• pdf:
Mult({xj}J
j=1|{ωj}J
j=1) ≜
(J
j=1 xj)!
*J
j=1 xj!
J
j=1
ωxj
j ,
(C.2)
where xj is a nonnegative integer, and has the following constraint:
xj ∈Z+,
J

j
xj = N.
(C.3)
The parameter {ω1, · · · , ωJ} has the following constraint:
J

j
ωj = 1,
0 ≤ωj ≤1
∀j.
(C.4)

C.3 Beta distribution
393
• pdf (xj = 1 and xj′ = 0 ∀j′ ̸= j):
Mult(xj|{ωj}J
j=1) ≜ωj.
(C.5)
C.3
Beta distribution
Beta distribution is a special case of the Dirichlet distribution with two probabilistic
variables x and y = 1 −x.
• pdf:
Beta(x|α, β) ≜CBeta(α, β)xα−1(1 −x)β−1,
(C.6)
where
x ∈[0, 1],
(C.7)
and
α, β > 0.
(C.8)
• Normalization constant:
CBeta(α, β) ≜(α + β)
(α)(β),
(C.9)
where (·) is a gamma function.
• Mean:
E(x)[x] =
α
α + β .
(C.10)
This is derived as follows:
E(x)[x] =

xBeta(x|α, β)dx
= CBeta(α, β)

xα(1 −x)β−1dx
=
CBeta(α, β)
CBeta(α + 1, β)
=
(α+β)
(α)(β)
(α+β+1)
(α+1)(β)
=
(α + β)
(α + β + 1)
(α + 1)
(α)
=
α
α + β ,
(C.11)
where we use the following property of the gamma function:
(x + 1) = x(x).
(C.12)
• Mode:
α −1
α + β −2,
α, β > 1.
(C.13)

394
Probabilistic distribution functions
It is derived as follows:
d
dxBeta(x|α, β)
= CBeta(α, β)
(
(α −1)xα−2(1 −x)β−1 −(β −1)xα−1(1 −x)β−2)
= CBeta(α, β)xα−2(1 −x)β−2 ((α −1)(1 −x) −(β −1)x)
= CBeta(α, β)xα−2(1 −x)β−2 (α −1 −(α + β −2)x) .
(C.14)
Therefore,
d
dxBeta(x|α, β) = 0
⇒xmode =
α −1
α + β −2.
(C.15)
C.4
Dirichlet distribution
A Dirichlet distribution is a generalized beta distribution with J probabilistic variables.
• pdf:
Dir({ωj}J
j=1|{φj}J
j=1) ≜CDir({φj}J
j=1)

j
(ωj)φj−1,
(C.16)
where
J

j
ωj = 1,
0 ≤ωj ≤1,
0 < φj.
(C.17)
• Normalization constant:
CDir({φj}J
j=1) ≜

(J
j=1 φj
)
*J
j=1 (φj)
.
(C.18)
• Mean:
E(ωj)[ωj] =
φj
J
j′=1 φj′
,
(C.19)
E(ωj)
⎡
⎣
j
ωγj
j
⎤
⎦=
CDir({φj}J
j=1)
CDir({φj + γj}J
j=1)
=

(J
j=1 φj
)

(J
j=1 φj + γj
)
*J
j=1 (φj + γj)
*J
j=1 (φj)
,
(C.20)
E(ωj)[log ωj] = exp
⎛
⎝(ωj) −
⎛
⎝
J

j′=1
ωj′
⎞
⎠
⎞
⎠.
(C.21)

C.5 Gaussian distribution
395
• Mode:
d
dωj′
⎛
⎝log
(
Dir({ωj}J
j=1|{φj}J
j=1)
)
+ λ
⎛
⎝1 −
J

j=1
ωj
⎞
⎠
⎞
⎠
= φj −1
ωj
+ λ = 0,
⇒ωmode
j
∝φj −1
⇒
φj −1
J
j′=1(φj′ −1)
,
φj > 1.
(C.22)
C.5
Gaussian distribution
Note that this book uses  as a variance. The standard deviation σ is represented as
σ =
√
. We also list the pdf with precision scale parameter r ≜1/, which can also
be used in this book.
• pdf (with variance ):
N(x|μ, ) ≜CN () exp

−(x −μ)2
2

,
(C.23)
where
x ∈R
(C.24)
and
μ ∈R,  > 0.
(C.25)
• Normalization constant (with variance ):
CN () ≜(2π)−1
2 ()−1
2 .
(C.26)
• pdf (with precision r):
N(x|μ, r−1) ≜CN (r−1) exp

−r(x −μ)2
2

,
(C.27)
where
r > 0.
(C.28)
• Normalization constant (with precision r):
CN (r−1) ≜(2π)−1
2 (r)
1
2 .
(C.29)
• Mean:
E(x)[x] = μ.
(C.30)

396
Probabilistic distribution functions
• Variance:
E(x)[x2] =  = r−1.
(C.31)
• Mode:
μ.
(C.32)
C.6
Multivariate Gaussian distribution
• pdf (with covariance matrix ):
N(x|μ, ) ≜CN () exp

−1
2(x −μ)⊺−1(x −μ)

= CN () exp

−1
2tr
+
−1(x −μ)(x −μ)⊺,
,
(C.33)
where
x ∈RD,
(C.34)
and
μ ∈RD,
 ∈RD×D.
(C.35)
 is positive deﬁnite.
• Normalization constant (with covariance matrix ):
CN () ≜(2π)−D
2 ||−1
2 .
(C.36)
• pdf (with precision matrix R):
N(x|μ, R−1) ≜CN (R−1) exp

−1
2(x −μ)⊺R(x −μ)

= CN (R−1) exp

−1
2tr

R(x −μ)(x −μ)⊺
.
(C.37)
• Normalization constant (with precision matrixR):
CN (R−1) ≜(2π)−D
2 |R|
1
2 .
(C.38)
• Mean:
E(x)[x] = μ.
(C.39)
• Variance:
E(x)[xx⊺] =  = R−1.
(C.40)
• Mode:
μ.
(C.41)

C.7 Multivariate Gaussian distribution (diagonal covariance matrix)
397
C.7
Multivariate Gaussian distribution (diagonal covariance matrix)
This is a special case of the multivariate Gaussian distribution. The diagonal covariance
matrix reduces the number of parameters and makes the analytical treatment simple (it
is simply represented as a product of scalar Gaussian distributions).
• pdf (with covariance matrix  = diag(σ), where σ = [1, · · · , D]⊺):
N(x|μ, ) ≜
D

d=1
N(xd|μd, d)
=
D

d=1
CN (d) exp

−1
2
(xd −μd)2
d

,
(C.42)
where
x ∈RD,
(C.43)
and
μ ∈RD,
σ ∈RD
>0.
(C.44)
• Normalization constant (with variance d):
CN (d) ≜(2π)−1
2 
−1
2
d
.
(C.45)
• pdf (with precision matrix R = diag(r), where r = [r1, · · · , rD]⊺):
N(x|μ, R−1) ≜
D

d=1
N(xd|μd, r−1
d )
=
D

d=1
CN (r−1
d ) exp
(
−rd
2 (xd −μd)2)
.
(C.46)
• Normalization constant (with precision rd):
CN (r−1
d ) ≜(2π)−1
2 r
1
2
d .
(C.47)
• Mean:
E(x)[x] = μ.
(C.48)
• Variance:
E(xd)[x2
d] = d = r−1
d .
(C.49)
• Mode:
μ.
(C.50)

398
Probabilistic distribution functions
C.8
Spherical Gaussian distribution
This is another special case of the multivariate Gaussian distribution where the diagonal
covariance matrix is represented with the identity matrix with a single scaling parameter.
• pdf (with covariance matrix  = ID):
N(x|μ, ) ≜
D

d=1
N(xd|μd, )
= (CN ())D
D

d=1
exp

−1
2
(xd −μd)2


,
(C.51)
where
x ∈RD,
(C.52)
and
μ ∈RD,
 ∈R>0.
(C.53)
• Normalization constant (with variance ):
CN () ≜(2π)−1
2 −1
2 .
(C.54)
• pdf (with precision matrix R = r−1ID):
N(x|μ, R−1) ≜
D

d=1
N(xd|μd, r−1)
=
(
CN (r−1)
)D
D

d=1
exp
(
−r
2(xd −μd)2)
.
(C.55)
• Normalization constant (with precision r):
CN (r−1) ≜(2π)−1
2 r
1
2 .
(C.56)
• Mean:
E(x)[x] = μ.
(C.57)
• Variance:
E(xd)[x2
d] =  = r−1.
(C.58)
• Mode:
μ.
(C.59)

C.10 Laplace distribution
399
C.9
Matrix variate Gaussian distribution
• pdf:
p(X) = N(X|M, , )
≜CN (, ) exp

−1
2tr
+
(X −M)⊺−1(X −M)−1,
,
(C.60)
where
X ∈RD×D′,
(C.61)
and
M ∈RD×D′,
 ∈RD×D,
 ∈RD′×D′.
(C.62)
 and  are correlation matrices and are positive deﬁnite. When D′ = 1, it becomes
the multivariate Gaussian distribution.
• Normalization constant:
CN (, ) ≜(2π)−DD′
2 ||−D
2 ||−D′
2 .
(C.63)
• Mean:
E(X)[X] = M.
(C.64)
• Variance:
E(X)[XX⊺] = ,
(C.65)
E(X)[X⊺X] = .
(C.66)
• Mode:
M.
(C.67)
C.10
Laplace distribution
• pdf:
Lap(x|μ, β) ≜CLap(β) exp

−|x −μ|
β

= CLap(β)
⎧
⎨
⎩
exp
(
−x−μ
β
)
if x ≥u,
exp
(
−μ−x
β
)
if x < u,
(C.68)
where
x ∈R,
(C.69)
and
μ ∈R,
β > 0.
(C.70)

400
Probabilistic distribution functions
• Normalization constant:
CLap(β) ≜1
2β .
(C.71)
• Mean:
E(x)[x] = μ.
(C.72)
• Mode:
μ.
(C.73)
Note that the probabilistic distribution function is not continuous at μ, and it is not
differentiable.
C.11
Gamma distribution
A gamma distribution is used as a prior/posterior distribution of precision parameter r
of a Gaussian distribution.
• pdf:
Gam(y|α, β) ≜CGam(α, β)yα−1 exp (−βy) ,
(C.74)
where
y > 0,
(C.75)
and
α, β > 0.
(C.76)
• Normalization constant:
CGam(α, β) ≜
βα
(α).
(C.77)
• Mean:
E(y)[y] = α
β .
(C.78)
• Variance:
E(y)[y2] = α
β2 .
(C.79)
• Mode:
d
dyGam(y|α, β) = CGam(α, β) (α −1 −βy) yα−2 exp(−βy) = 0
⇒α −1
β
,
α > 1.
(C.80)
The shape of the gamma distribution is not symmetric, and the mode and mean values
are different.

C.12 Inverse gamma distribution
401
To make the notation consistent with the Wishart distribution in Appendix C.14, we
also use the following deﬁnition for the gamma distribution, with α →φ
2 and β →r0
2
in the original gamma distribution deﬁned in Eq. (C.74):
• pdf (with 1
2 factor):
Gam2(y|φ, r0) ≜Gam

y
----
φ
2 , r0
2

= CGam2
(
φ, r0)
y
φ
2 −1 exp

−r0y
2

.
(C.81)
• Normalization constant (with 1
2 factor):
CGam2(φ, r0) ≜
(
r0
2
) φ
2

(
φ
2
).
(C.82)
• Mean (with 1
2 factor):
E(y)[y] =
φ
2
r0
2
= φ
r0 .
(C.83)
• Variance (with 1
2 factor):
E(y)[y2] =
φ
2
(
r0
2
)2 =
2φ

r02 .
(C.84)
• Mode (with 1
2 factor):
φ
2 −1
r0
2
= φ −2
r0
,
φ > 2.
(C.85)
It is shown in Appendix C.14 that this gamma distribution (Gam2(·)) with 1
2 factor is
equivalent to the Wishart distribution with the number of dimensions 1 (D = 1).
C.12
Inverse gamma distribution
An inverse gamma distribution is used as a prior/posterior distribution of variance
parameter . It can be obtained by simply replacing y in a gamma distribution by 1
y
with f(y) = g(y′)
--- dy′
dy
---.
• pdf:
IGam(y|α, β) = Gam
 1
y
---- α, β
 ----
d(y−1)
dy
----
= CGam(α, β)
1
y
α−1
exp

−β
y

y−2
≜CIGam(α, β)y−α−1 exp

−β
y

,
(C.86)

402
Probabilistic distribution functions
where
y > 0,
(C.87)
and
α, β > 0.
(C.88)
• Normalization constant:
CIGam(α, β) = CGam(α, β) ≜
βα
(α).
(C.89)
• Mean:
E(y)[y] =
β
α −1,
α > 1.
(C.90)
• Mode:
β
α + 1.
(C.91)
C.13
Gaussian–gamma distribution
We provide a Gaussian–gamma distribution, which is used as a conjugate prior distri-
bution of a Gaussian distribution with mean μ and precision r. This is also known as
the normal-gamma distribution. Note that we use the gamma distribution deﬁned in Eq.
(C.81) instead of the original deﬁnition of the gamma distribution in Eq. (C.74).
• pdf:
NGam(μ, r|μ0, φμ, r0, φr)
≜N(μ|μ0, (rφμ)−1)Gam2
(
r
---φr, r0 )
≜CN Gam(φμ, r0, φr)r
φr−1
2
exp

−r0r
2 −φμr(μ −μ0)2
2

.
(C.92)
Note that the power of r is φr−1
2
, which is different from the original gamma distribu-
tion deﬁnition in Section (C.11), because it also considers an r
1
2 factor in the Gaussian
distribution.
• Normalization constant:
CN Gam(r, α, β) ≜
√φμ
(
r0
2
) φr
2
√
2π
(
φr
2
) .
(C.93)
• Mean:
E(μ,r)[{μ, r}] =
.
μ0, φr
r0
6
.
(C.94)

C.15 Gaussian–Wishart distribution
403
• Mode:
.
μ0, φr −2
r0
6
,
φr > 2.
(C.95)
The means and modes of mean and precision parameters are the same as those of the
Gaussian and gamma distributions, respectively.
C.14
Wishart distribution
• pdf:
W(Y|R0, φ) ≜CW(R0, φ)|Y|
φ−D−1
2
exp

−1
2tr
+
R0Y
,
,
(C.96)
where
Y ∈RD×D,
(C.97)
and
R0 ∈RD×D,
φ > D −1.
(C.98)
Y and R0 are positive deﬁnite.
• Normalization constant:
CW(R0, φ) ≜
|R0|
φ
2
(2)
Dφ
2 D
(
φ
2
),
(C.99)
where D(·) is a multivariate Gamma function.
• Mean:
E(Y)[Y] = φ(R0)−1.
(C.100)
• Mode:
(φ −D −1) (R0)−1,
φ ≥D + 1.
(C.101)
When D →1, the Wishart distribution is equivalent to the gamma distribution deﬁned
in Eq. (C.81).
C.15
Gaussian–Wishart distribution
• pdf:
NW(μ, R|μ0, φμ, R0, φR)
≜N(μ|μ0, (φμR)−1)W(R|R0, φR)
≜CNW(φμ, R0, φR)|R|
φR−D
2
× exp

−1
2tr
+
R0R
,
−φμ
2 (μ −μ0)⊺R(μ −μ0)

.
(C.102)

404
Probabilistic distribution functions
Note that when D →1, Gaussian–Wishart distribution NW(μ, R|μ0, φμ, R0, φR)
approaches Gaussian–gamma distribution NGam(μ, r|μ0, φμ, r0, φr) in Appendix
C.13.
• Normalization constant:
CNW(φμ, R0, φR) ≜(2π)
D
2 (φμ)−D
2
|R0|
φR
2
(2)
DφR
2 D
(
φR
2
).
(C.103)
• Mean:
E(μ,R)[{μ, R}] =
0
μ0, φR(R0)−11
.
(C.104)
• Mode:
0
μ0,

φR −D −1

(R0)−11
,
φR ≥D + 1.
(C.105)
Again, when D →1, these modes are equivalent to the modes of the Gaussian–
gamma distribution in Appendix C.13.
C.16
Student’s t-distribution
• pdf:
St(x|μ, λ, κ) ≜CSt

1 + 1
κλ(x −μ)2
−κ+1
2
,
(C.106)
where
x ∈R,
(C.107)
and
μ ∈R,
κ > 0,
λ > 0.
(C.108)
• Normalization constant:
CSt(κ, λ) ≜

(
κ+1
2
)

 κ
2


(
1
2
)
 1
κλ
 1
2
,
(C.109)
• Mean:
E(x)[x] = μ,
(C.110)
• Mode:
μ.
(C.111)
Parameter κ is called the degrees of freedom, and if κ is large, the distribution
approaches the Gaussian distribution. Note that Student’s t-distribution is not included
in the exponential family (Section 2.1.3).

References
Abu-Mostafa, Y. S. (1989), “The Vapnik–Chervonenkis dimension: information versus
complexity in learning,” Neural Computation 1, 312–317.
Akaike, H. (1974), “A new look at the statistical model identiﬁcation,” IEEE Transactions on
Automatic Control 19(6), 716–723.
Akaike, H. (1980), “Likelihood and the Bayes procedure,” in J. M. Bernardo, M. H. DeGroot,
D. V. Lindley & A. F. M. Smith, eds, Bayesian Statistics, University Press, Valencia, Spain,
pp. 143–166.
Akita, Y., & Kawahara, T. (2004), “Language model adaptation based on PLSA of topics
and speakers,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 1045–1048.
Aldous, D. (1985), “Exchangeability and related topics,” École d’Été de Probabilités de
Saint-Flour XIII1983, pp. 1–198.
Anastasakos, T., McDonough, J., Schwartz, R., & Makhoul, J. (1996), “A compact model for
speaker-adaptive training,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 1137–1140.
Anguera Miro, X., Bozonnet, S., Evans, N., et al. (2012), “Speaker diarization: A review of recent
research,” IEEE Transactions on Audio, Speech, and Language Processing 20(2), 356–370.
Antoniak, C. E. (1974), “Mixtures of Dirichlet processes with applications to Bayesian nonpara-
metric problems,” Annals of Statistics 2(6), 1152–1174.
Attias, H. (1999), “Inferring parameters and structure of latent variable models by varia-
tional Bayes,” Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pp. 21–30.
Axelrod, S., Gopinath, R., & Olsen, P. (2002), “Modeling with a subspace constraint on
inverse covariance matrices,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 2177–2180.
Bahl, L. R., Brown, P. F., de Souza, P. V., & Mercer, R. L. (1986), “Maximum mutual informa-
tion estimation of hidden Markov model parameters for speech recognition,” Proceedings of
International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 49–52.
Barber, D. (2012), Bayesian Reasoning and Machine Learning, Cambridge University Press.
Barker, J., Vincent, E., Ma, N., Christensen, H., & Green, P. (2013), “The PASCAL CHiME
speech separation and recognition challenge,” Computer Speech and Language 27, 621–633.
Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970), “A maximization technique occur-
ring in the statistical analysis of probabilistic functions of Markov chains,” The Annals of
Mathematical Statistics, pp. 164–171.
Beal, M. J. (2003), Variational algorithms for approximate Bayesian inference, PhD thesis,
University of London.

406
References
Beal, M. J., Ghahramani, Z., & Rasmussen, C. E. (2002), “The inﬁnite hidden Markov model,”
Advances in Neural Information Processing Systems 14, 577–584.
Bellegarda, J. (2004), “Statistical language model adaptation: review and perspectives,” Speech
Communication 42(1), 93–108.
Bellegarda, J. R. (2000), “Exploiting latent semantic information in statistical language
modeling,” Proceedings of the IEEE 88(8), 1279–1296.
Bellegarda, J. R. (2002), “Fast update of latent semantic spaces using a linear transform frame-
work,” Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), pp. 769–772.
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003), “A neural probabilistic language
model,” Journal of Machine Learning Research 3, 1137–1155.
Berger, J. O. (1985), Statistical Decision Theory and Bayesian Analysis, Second Edition,
Springer-Verlag.
Bernardo, J. M., & Smith, A. F. M. (2009), Bayesian Theory, Wiley.
Berry, M. W., Dumais, S. T., & O’Brien, G. W. (1995), “Using linear algebra for intelligent
information retrieval,” SIAM Review 37(4), 573–595.
Bilmes, J. A. (1998), A gentle tutorial of the EM algorithm and its application to parameter
estimation for Gaussian mixture and hidden Markov models, Technical Report TR-97-021,
International Computer Science Institute.
Bilmes, J., & Zweig, G. (2002), “The graphical models toolkit: An open source software system
for speech and time-series processing,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pp. 3916–3919.
Bishop, C. M. (2006), Pattern Recognition and Machine Learning, Springer.
Blackwell, D., & MacQueen, J. B. (1973), “Ferguson distribution via P´olya urn schemes,” The
Annals of Statistics 1, 353–355.
Blei, D., Grifﬁths, T., & Jordan, M. (2010), “The nested Chinese restaurant process and Bayesian
nonparametric inference of topic hierarchies,” Journal of the ACM 57(2), article 7.
Blei, D., Grifﬁths, T., Jordan, M., & Tenenbaum, J. (2004), “Hierarchical topic models and
the nested Chinese restaurant process,” Advances in Neural Information Processing Systems
16, 17–24.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003), “Latent Dirichlet allocation,” Journal of Machine
Learning Research 3, 993–1022.
Brants, T., Popat, A. C., Xu, P., Och, F. J., & Dean, J. (2007), “Large language models in
machine translation,” Proceedings of the 2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),
Association for Computational Linguistics, pp. 858–867.
Brill, E., & Moore, R. C. (2000), “An improved error model for noisy channel spelling correc-
tion,” Proceedings of the 38th Annual Meeting of Association for Computational Linguistics,
Association for Computational Linguistics, pp. 286–293.
Brown, P., Desouza, P., Mercer, R., Pietra, V., & Lai, J. (1992), “Class-based n-gram models of
natural language,” Computational Linguistics 18(4), 467–479.
Brown, P. F., Cocke, J., Pietra, S. A. D., et al. (1990), “A statistical approach to machine
translation,” Computational Linguistics 16(2), 79–85.
Campbell, W. M., Sturim, D. E., & Reynolds, D. A. (2006), “Support vector machines using GMM
supervectors for speaker veriﬁcation,” Signal Processing Letters, IEEE 13(5), 308–311.
Chen, K.-T., Liau, W.-W., Wang, H.-M., & Lee, L.-S. (2000), “Fast speaker adaptation
using eigenspace-based maximum likelihood linear regression,” Proceedings of International
Conference on Spoken Language Processing (ICSLP), pp. 742–745.

References
407
Chen, S. F. (2009), “Shrinking exponential language models,” in Proceedings of Human Language
Technologies: The 2009 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, Association for Computational Linguistics, pp. 468–476.
Chen, S. F., & Goodman, J. (1999), “An empirical study of smoothing techniques for language
modeling,” Computer Speech & Language 13(4), 359–393.
Chen, S., & Gopinath, R. (1999), “Model selection in acoustic modeling,” Proceedings of
European Conference on Speech Communication and Technology (EUROSPEECH), pp. 1087–
1090.
Chesta, C., Siohan, O., & Lee, C.-H. (1999), “Maximum a posteriori linear regression for hidden
Markov model adaptation,” Proceedings of European Conference on Speech Communication
and Technology (EUROSPEECH), pp. 211–214.
Chien, J.-T. (1999), “Online hierarchical transformation of hidden Markov models for speech
recognition,” IEEE Transactions on Speech and Audio Processing 7(6), 656–667.
Chien, J.-T. (2002), “Quasi-Bayes linear regression for sequential learning of hidden Markov
models,” IEEE Transactions on Speech and Audio Processing 10(5), 268–278.
Chien, J.-T. (2003), “Linear regression based Bayesian predictive classiﬁcation for speech
recognition,” IEEE Transactions on Speech and Audio Processing 11(1), 70–79.
Chien, J.-T., & Chueh, C.-H. (2011), “Dirichlet class language models for speech recognition,”
IEEE Transactions on Audio, Speech, and Language Processing 19(3), 482–495.
Chien, J.-T., Huang, C.-H., Shinoda, K., & Furui, S. (2006), “Towards optimal Bayes decision for
speech recognition,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 45–48.
Chien, J.-T., Lee, C.-H., & Wang, H.-C. (1997), “Improved Bayesian learning of hidden Markov
models for speaker adaptation,” Proceedings of International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pp. 1027–1030.
Chien, J. T., & Liao, G.-H. (2001), “Transformation-based Bayesian predictive classiﬁcation using
online prior evolution,” IEEE Transactions on Speech and Audio Processing 9(4), 399–410.
Chien, J.-T., & Wu, M.-S. (2008), “Adaptive Bayesian latent semantic analysis,” IEEE
Transactions on Audio, Speech, and Language Processing 16(1), 198–207.
Chou, W., & Reichl, W., (1999), “Decision tree state tying based on penalized Bayesian
information criterion,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 345–348.
Coccaro, N., & Jurafsky, D. (1998), “Towards better integration of semantic predictors in
statistical language modeling,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 2403–2406.
Cournapeau, D., Watanabe, S., Nakamura, A., & Kawahara, T. (2010), “Online unsupervised
classiﬁcation with model comparison in the variational Bayes framework for voice activity
detection,” IEEE Journal of Selected Topics in Signal Processing 4(6), 1071–1083.
Dahl, G. E., Yu, D., Deng, L., & Acero, A. (2012), “Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition,” IEEE Transactions on Audio, Speech and
Language Processing 20(1), 30–42.
Davis, S. B., & Mermelstein, P. (1980), “Comparison of parametric representations for monosyl-
labic word recognition in continuously spoken sentences,” IEEE Transactions on Acoustics,
Speech, and Signal Processing 28(4), 357–366.
Dawid, A. P. (1981), “Some matrix-variate distribution theory: notational considerations and a
Bayesian application,” Biometrika 68(1), 265–274.
De Bruijn, N. G. (1970), Asymptotic Methods in Analysis, Dover Publications.

408
References
Dehak, N., Kenny, P., Dehak, R., Dumouchel, P., & Ouellet, P. (2011), “Front-end factor analysis
for speaker veriﬁcation,” IEEE Transactions on Audio, Speech, and Language Processing
19(4), 788–798.
Delcroix, M., Nakatani, T., & Watanabe, S. (2009), “Static and dynamic variance compensation
for recognition of reverberant speech with dereverberation preprocessing,” IEEE Transactions
on Audio, Speech, and Language Processing 17(2), 324–334.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1976), “Maximum likelihood from incomplete
data via the EM algorithm,” Journal of Royal Statistical Society B 39, 1–38.
Digalakis, V., & Neumeyer, L. (1996), “Speaker adaptation using combined transformation and
Bayesian methods,” IEEE Transactions on Speech and Audio Processing 4, 294–300.
Digalakis, V., Ritischev, D., & Neumeyer, L. (1995), “Speaker adaptation using constrained
reestimation of Gaussian mixtures,” IEEE Transactions on Speech and Audio Processing
3, 357–366.
Ding, N., & Ou, Z. (2010), “Variational nonparametric Bayesian hidden Markov model,”
Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), pp. 2098–2101.
Droppo, J., Acero, A., & Deng, L. (2002), “Uncertainty decoding with SPLICE for noise robust
speech recognition,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP)’, Vol. 1, pp. I–57.
Federico, M. (1996), “Bayesian estimation methods of n-gram language model adaptation,”
Proceedings of International Conference on Spoken Language Processing (ICSLP), pp. 240–
243.
Ferguson, T. (1973), “A Bayesian analysis of some nonparametric problems,” The Annals of
Statistics 1, 209–230.
Fosler, E., & Morris, J. (2008), “Crandem systems: Conditional random ﬁeld acoustic models for
hidden Markov models,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 4049–4052.
Fox, E. B., Sudderth, E. B., Jordan, M. I., & Willsky, A. S. (2008), “An HDP-HMM for systems
with state persistence,” Proceedings of International Conference on Machine Learning (ICML),
pp. 312–319.
Fukunaga, K. (1990), Introduction to Statistical Pattern Recognition, Academic Press.
Furui, S. (1981), “Cepstral analysis technique for automatic speaker veriﬁcation,” IEEE
Transactions on Acoustics, Speech and Signal Processing 29(2), 254–272.
Furui, S. (1986), “Speaker independent isolated word recognition using dynamic features of
speech spectrum,” IEEE Transactions on Acoustics, Speech and Signal Processing 34, 52–59.
Furui, S. (2010), “History and development of speech recognition,” in Speech Technology, F. Chen
and K. Jokinen, eds., Springer, pp. 1–18.
Furui, S., Maekawa, K., & H. Isahara, M. (2000), “A Japanese national project on spontaneous
speech corpus and processing technology,” Proceedings of ASR’00, pp. 244–248.
Gales, M. (1998), “Maximum likelihood linear transformations for HMM-based speech
recognition,” Computer Speech and Language 12, 75–98.
Gales, M., Center, I., & Heights, Y. (2000), “Cluster adaptive training of hidden Markov models,”
IEEE Transactions on Speech and Audio Processing 8(4), 417–428.
Gales, M. J. F. (1999), “Semi-tied covariance matrices for hidden Markov models,” IEEE
Transactions on Speech and Audio Processing 7(3), 272–281.
Gales, M. J. F., & Woodland, P. C. (1996), Variance compensation within the MLLR framework,
Technical Report 242, Cambridge University Engineering Department.

References
409
Gales, M., Watanabe, S., & Fossler-Lussier, E. (2012), “Structured discriminative models for
speech recognition,” IEEE Signal Processing Magazine 29(6), 70–81.
Ganapathiraju, A., Hamaker, J., & Picone, J. (2004), “Applications of support vector machines to
speech recognition,” IEEE Transactions on Signal Processing 52(8), 2348–2355.
Gaussier, E., & Goutte, C. (2005), “Relation between PLSA and NMF and implications,” Proceed-
ings of the 28th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, ACM, pp. 601–602.
Gauvain, J.-L., & Lee, C.-H. (1991), “Bayesian learning of Gaussian mixture densities for
hidden Markov models,” Proceedings of DARPA Speech and Natural Language Workshop,
pp. 272–277.
Gauvain, J.-L., & Lee, C.-H. (1994), “Maximum a posteriori estimation for multivariate Gaussian
mixture observations of Markov chains,” IEEE Transactions on Speech and Audio Processing
2, 291–298.
Gelman, A., Carlin, J. B., Stern, H. S., et al. (2013), Bayesian Data Analysis, CRC Press.
Geman, S., & Geman, D. (1984), “Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images,” IEEE Transactions on Pattern Analysis and Machine Intelligence
6(1), 721–741.
Genkin, A., Lewis, D. D., & Madigan, D. (2007), “Large-scale Bayesian logistic regression for
text categorization,” Technometrics 49(3), 291–304.
Ghahramani, Z. (1998), “Learning dynamic Bayesian networks,” in Adaptive Processing of
Sequences and Data Structures, Springer, pp. 168–197.
Ghahramani, Z. (2004), “Unsupervised learning,” Advanced Lectures on Machine Learning,
pp. 72–112.
Ghosh, J. K., Delampady, M., & Samanta, T. (2007), An Introduction to Bayesian Analysis:
Theory and Methods, Springer.
Gildea, D., & Hofmann, T. (1999), “Topic-based language models using EM,” Pro-
ceedings
of
European
Conference
on
Speech
Communication
and
Technology
(EUROSPEECH), pp. 2167–2170.
Gilks, W. R., Richardson, S., & Spiegelhalter, D. J. (1996), Markov Chain Monte Carlo in
Practice, Chapman & Hall/CRC Interdisciplinary Statistics.
Gish, H., Siu, M.-h., Chan, A., & Belﬁeld, W. (2009), “Unsupervised training of an HMM-based
speech recognizer for topic classiﬁcation,” Proceedings of Annual Conference of International
Speech Communication Association (INTERSPEECH), pp. 1935–1938.
Glass, J. (2003), “A probabilistic framework for segment-based speech recognition,” Computer
Speech & Language 17(2-3), 137–152.
Goel, V., & Byrne, W. (2000), “Minimum Bayes-risk automatic speech recognition,” Computer
Speech and Language 14, 115–135.
Goldwater, S. (2007), Nonparametric Bayesian models of lexical acquisition, PhD thesis, Brown
University.
Goldwater, S., & Grifﬁths, T. (2007), “A fully Bayesian approach to unsupervised part-of-speech
tagging,” Proceedings of Annual Meeting of the Association of Computational Linguistics,
pp. 744–751.
Goldwater, S., Grifﬁths, T., & Johnson, M. (2009), “A Bayesian framework for word
segmentation: Exploring the effects of context,” Cognition 112(1), 21–54.
Goldwater, S., Grifﬁths, T. L., & Johnson, M. (2006), “Interpolating between types and
tokens by estimating power-law generators,” Advances in Neural Information Processing
Systems 18.

410
References
Good, I. J. (1953), “The population frequencies of species and the estimation of populations,”
Biometrika 40, 237–264.
Grézl, F., Karaﬁát, M., Kontár, S., & Cernocky, J. (2007), “Probabilistic and bottle-neck features
for LVCSR of meetings,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 757–760.
Grifﬁths, T., & Ghahramani, Z. (2005), Inﬁnite latent feature models and the Indian buffet process,
Technical Report, Gatsby Unit.
Grifﬁths, T., & Steyvers, M. (2004), “Finding scientiﬁc topics,” in Proceedings of the National
Academy of Sciences, 101 Suppl. 1, 5228–5235.
Gunawardana, A., Mahajan, M., Acero, A., & Platt, J. C. (2005), “Hidden conditional random
ﬁelds for phone classiﬁcation,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 1117–1120.
Haeb-Umbach, R., & Ney, H. (1992), “Linear discriminant analysis for improved large vocabulary
continuous speech recognition,” International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 1, pp. 13–16.
Hahm, S. J., Ogawa, A., Fujimoto, M., Hori, T., & Nakamura, A. (2012), “Speaker adaptation
using variational Bayesian linear regression in normalized feature space,” in Proceedings of
Annual Conference of International Speech Communication Association (INTERSPEECH),
pp. 803–806.
Hashimoto, K., Nankaku, Y., & Tokuda, K. (2009), “A Bayesian approach to hidden semi-Markov
model based speech synthesis,” in Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 1751–1754.
Hashimoto, K., Zen, H., Nankaku, Y., Lee, A., & Tokuda, K. (2008), “Bayesian context cluster-
ing using cross valid prior distribution for HMM-based speech recognition,” Proceedings of
Annual Conference of International Speech Communication Association (INTERSPEECH),
pp. 936–939.
Hashimoto, K., Zen, H., Nankaku, Y., Masuko, T., & Tokuda, K. (2009), “A Bayesian approach to
HMM-based speech synthesis,” Proceedings of International Conference on Acoustics, Speech,
and Signal Processing (ICASSP) 2009, pp. 4029–4032.
Hastings, W. K. (1970), “Monte Carlo sampling methods using Markov chains and their
applications,” Biometrika 57, 97–109.
Heigold, G., Ney, H., Schluter, R., & Wiesler, S. (2012), “Discriminative training for automatic
speech recognition: Modeling, criteria, optimization, implementation, and performance,” IEEE
Signal Processing Magazine 29(6), 58–69.
Hermansky, H. (1990), “Perceptual linear predictive (PLP) analysis of speech,” Journal of the
Acoustic Society of America 87(4), 1738–1752.
Hermansky, H., Ellis, D., & Sharma, S. (2000), “Tandem connectionist feature extraction for
conventional HMM systems,” Proceedings of International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pp. 1635–1638.
Hinton, G., Deng, L., Yu, D., et al. (2012), “Deep neural networks for acoustic modeling in speech
recognition,” IEEE Signal Processing Magazine 29(6), 82–97.
Hinton, G., Osindero, S., & Teh, Y. (2006), “A fast learning algorithm for deep belief nets,” Neural
Computation 18, 1527–1554.
Hofmann, T. (1999a), “Probabilistic latent semantic analysis,” Proceedings of Uncertainty in
Artiﬁcial Intelligence, pp. 289–296.
Hofmann, T. (1999b), “Probabilistic latent semantic indexing,” Proceedings of the Annual Inter-
national ACM SIGIR Conference on Research and Development in Information Retrieval,
pp. 50–57.

References
411
Hofmann, T. (2001), “Unsupervised learning by probabilistic latent semantic analysis,” Machine
Learning 42(1-2), 177–196.
Hori, T., & Nakamura, A. (2013), “Speech recognition algorithms using weighted ﬁnite-state
transducers,” Synthesis Lectures on Speech and Audio Processing 9(1), 1–162.
Hu, R., & Zhao, Y. (2007), “Knowledge-based adaptive decision tree state tying for conversa-
tional speech recognition,” IEEE Transactions on Audio, Speech, and Language Processing
15(7), 2160–2168.
Huang, S., & Renals, S. (2008), “Unsupervised language model adaptation based on topic and
role information in multiparty meeting,” Proceedings of Annual Conference of International
Speech Communication Association (INTERSPEECH), pp. 833–836.
Huang, X. D., Acero, A., & Hon, H. W. (2001), Spoken Language Processing: A Guide to Theory,
Algorithm, and System Development, Prentice Hall.
Huang, X. D., Ariki, Y., & Jack, M. A. (1990), Hidden Markov Models for Speech Recognition,
Edinburgh University Press.
Huo, Q., & Lee, C.-H. (1997), “On-line adaptive learning of the continuous density hidden
Markov model based on approximate recursive Bayes estimate,” IEEE Transactions on Speech
and Audio Processing 5(2), 161–172.
Huo, Q., & Lee, C.-H. (2000), “A Bayesian predictive classiﬁcation approach to robust speech
recognition,” IEEE Transactions on Speech and Audio Processing 8, 200–204.
Ishiguro, K., Yamada, T., Araki, S., Nakatani, T., & Sawada, H. (2012), “Probabilistic
speaker diarization with bag-of-words representations of speaker angle information,” IEEE
Transactions on Audio, Speech, and Language Processing 20(2), 447–460.
Jansen, A., Dupoux, E., Goldwater, S., et al. (2013), “A summary of the 2012 JHU CLSP
workshop on zero resource speech technologies and models of early language acquisition,” Pro-
ceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP),
pp. 8111–8115.
Jelinek, F. (1976), “Continuous speech recognition by statistical methods,” Proceedings of the
IEEE 64(4), 532–556.
Jelinek, F. (1997), Statistical Methods for Speech Recognition, MIT Press.
Jelinek, F., & Mercer, R. L. (1980), “Interpolated estimation of Markov source parameters from
sparse data,” Proceedings of the Workshop on Pattern Recognition in Practice, pp. 381–397.
Ji, S., Xue, Y., & Carin, L. (2008), “Bayesian compressive sensing,” IEEE Transactions on Signal
Processing 56(6), 2346–2356.
Jiang, H., Hirose, K., & Huo, Q. (1999), “Robust speech recognition based on a Bayesian
prediction approach,” IEEE Transactions on Speech and Audio Processing 7, 426–440.
Jitsuhiro, T., & Nakamura, S. (2004), “Automatic generation of non-uniform HMM structures
based on variational Bayesian approach,” Proceedings of International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pp. 805–808.
Joachims, T. (2002), “Learning to classify text using support vector machines: Methods, theory,
and algorithms,” Computational Linguistics 29(4), 656–664.
Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1999), “An introduction to variational
methods for graphical models,” Machine Learning 37(2), 183–233.
Juang, B.-H., & Rabiner, L. (1990), “The segmental K-means algorithm for estimating parameters
of hidden Markov models,” IEEE Transactions on Acoustics, Speech and Signal Processing
38(9), 1639–1641.
Juang, B., & Katagiri, S. (1992), “Discriminative learning for minimum error classiﬁcation,” IEEE
Transactions on Signal Processing 40(12), 3043–3054.

412
References
Jurafsky, D. (2014), “From languages to information,” http://www.stanford.edu/class/cs124/lec/
languagemodeling.pdf.
Jurafsky, D., & Martin, J. H. (2000), Speech and Language Processing: An Introduc-
tion to Natural Language Processing, Computational Linguistics, and Speech Recognition,
Prentice Hall.
Kass, R. E., & Raftery, A. E. (1993), Bayes factors and model uncertainty, Technical Report 254,
Department of Statistics, University of Washington.
Kass, R. E., & Raftery, A. E. (1995), “Bayes factors,” Journal of the American Statistical
Association 90(430), 773–795.
Katz, S. (1987), “Estimation of probabilities from sparse data for the language model compo-
nent of a speech recognizer,” IEEE Transactions on Acoustics, Speech, and Signal Processing
35(3), 400–401.
Kawabata, T., & Tamoto, M. (1996), “Back-off method for n-gram smoothing based on binomial
posteriori distribution,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), Vol. 1, pp. 192–195.
Kenny, P. (2010), “Bayesian speaker veriﬁcation with heavy tailed priors,” Keynote Speech,
Odyssey Speaker and Language Recognition Workshop.
Kenny, P., Boulianne, G., Ouellet, P., & Dumouchel, P. (2007), “Joint factor analysis versus
eigenchannels in speaker recognition,” IEEE Transactions on Audio, Speech, and Language
Processing 15(4), 1435–1447.
Kingsbury, B., Sainath, T. N., & Soltau, H. (2012), “Scalable minimum Bayes risk training of deep
neural network acoustic models using distributed Hessian-free optimization,” Proceedings of
Annual Conference of International Speech Communication Association (INTERSPEECH),
pp. 10–13.
Kinnunen, T., & Li, H. (2010), “An overview of text-independent speaker recognition: from
features to supervectors,” Speech Communication 52(1), 12–40.
Kita, K. (1999), Probabilistic Language Models, University of Tokyo Press (in Japanese).
Kneser, R., & Ney, H. (1995), “Improved backing-off for m-gram language modeling,”
Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), pp. 181–184.
Kneser, R., Peters, J., & Klakow, D. (1997), “Language model adaptation using dynamic
marginals,” Proceedings of European Conference on Speech Communication and Technology
(EUROSPEECH), pp. 1971–1974.
Kolossa, D., & Haeb-Umbach, R. (2011), Robust Speech Recognition of Uncertain or Missing
Data: Theory and Applications, Springer.
Kubo, Y., Watanabe, S., Nakamura, A., & Kobayashi, T. (2010), “A regularized discriminative
training method of acoustic models derived by minimum relative entropy discrimination,”
Proceedings of Annual Conference of International Speech Communication Association
(INTERSPEECH), pp. 2954–2957.
Kudo, T. (2005), “Mecab: Yet another part-of-speech and morphological analyzer,” http://mecab.
sourceforge. net/.
Kuhn, R., & De Mori, R. (1990), “A cache-based natural language model for speech recognition,”
IEEE Transactions on Pattern Analysis and Machine Intelligence 12(6), 570–583.
Kuhn, R., Junqua, J., Ngyuen, P., & Niedzielski, N. (2000), “Rapid speaker adaptation in
eigenvoice space,” IEEE Transactions on Speech and Audio Processing 8(6), 695–707.
Kullback, S., & Leibler, R. A. (1951), “On information and sufﬁciency,” Annals of Mathematical
Statistics 22(1), 79–86.

References
413
Kwok, J. T.-Y. (2000), “The evidence framework applied to support vector machines,” IEEE
Transactions on Neural Networks 11(5), 1162–1173.
Kwon, O., Lee, T.-W., & Chan, K. (2002), “Application of variational Bayesian PCA for speech
feature extraction,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 1, pp. 825–828.
Lafferty, J., McCallum, A., & Pereira, F. (2001), “Conditional random ﬁelds: Probabilistic models
for segmenting and labeling sequence data,” Proceedings of International Conference on
Machine Learning, pp. 282–289.
Lamel, L., Gauvain, J.-L., & Adda, G. (2002), “Lightly supervised and unsupervised acoustic
model training,” Computer Speech & Language 16(1), 115–129.
Lau, R., Rosenfeld, R., & Roukos, S. (1993), “Trigger-based language models: A maximum
entropy approach,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 2, IEEE, pp. 45–48.
Lee, C.-H., & Huo, Q. (2000), “On adaptive decision rules and decision parameter adaptation for
automatic speech recognition,” Proceedings of the IEEE 88, 1241–1269.
Lee, C.-H., Lin, C.-H., & Juang, B.-H. (1991), “A study on speaker adaptation of the parameters
of continuous density hidden Markov models,” IEEE Transactions on Acoustics, Speech, and
Signal Processing 39, 806–814.
Lee, C.-Y. (2014), Discovering linguistic structures in speech: models and applications, PhD
thesis, Massachusetts Institute of Technology.
Lee, C.-Y., & Glass., J. (2012), “A nonparametric Bayesian approach to acoustic model discovery,”
Proceedings of Annual Meeting of the Association for Computational Linguistics, pp. 40–49.
Lee, C.-Y., Zhang, Y., & Glass, J. (2013), “Joint learning of phonetic units and word pronun-
ciations for ASR,” Proceedings of the 2013 Conference on Empirical Methods on Natural
Language Processing (EMNLP), pp. 182–192.
Lee, D. D., & Seung, H. S. (1999), “Learning the parts of objects by non-negative matrix
factorization,” Nature 401(6755), 788–791.
Leggetter, C. J., & Woodland, P. C. (1995), “Maximum likelihood linear regression for speaker
adaptation of continuous density hidden Markov models,” Computer Speech and Language
9, 171–185.
Lewis, D. D. (1998), “Naive (Bayes) at forty: The independence assumption in infor-
mation retrieval,” Proceedings of the 10th European Conference on Machine Learning,
Springer-Verlag, pp. 4–15.
Liu, J. (1994), “The collapsed Gibbs sampler in Bayesian computations with applications to a
gene regulation problem,” Journal of the American Statistical Association 89(427).
Liu, J. S. (2008), Monte Carlo Strategies in Scientiﬁc Computing, Springer.
Livescu, K., Glass, J. R., & Bilmes, J. (2003), “Hidden feature models for speech recognition
using dynamic Bayesian networks,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 2529–2532.
MacKay, D. J. C. (1992a), “Bayesian interpolation,” Neural Computation 4(3), 415–447.
MacKay, D. J. C. (1992b), “The evidence framework applied to classiﬁcation networks,” Neural
Computation 4(5), 720–736.
MacKay, D. J. C. (1992c), “A practical Bayesian framework for back-propagation networks,”
Neural Computation 4(3), 448–472.
MacKay, D. J. C. (1995), “Probable networks and plausible predictions – a review of practical
Bayesian methods for supervised neural networks,” Network: Computation in Neural Systems
6(3), 469–505.

414
References
MacKay, D. J. C. (1997), Ensemble learning for hidden Markov models, Technical Report,
Cavendish Laboratory, University of Cambridge.
MacKay, D. J. C., & Peto, L. C. B. (1995), “A hierarchical Dirichlet language model,” Natural
Language Engineering 1(3), 289–308.
Maekawa, T., & Watanabe, S. (2011), “Unsupervised activity recognition with user’s physi-
cal characteristics data,” Proceedings of International Symposium on Wearable Computers,
pp. 89–96.
Mak, B., Kwok, J., & Ho, S. (2005), “Kernel eigenvoice speaker adaptation,” IEEE Transactions
on Speech and Audio Processing 13(5), 984–992.
Manning, C. D., & Schütze, H. (1999), Foundations of Statistical Natural Language Processing,
MIT Press.
Masataki, H., Sagisaka, Y., Hisaki, K., & Kawahara, T. (1997), “Task adaptation using MAP esti-
mation in n-gram language modeling,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pp. 783–786.
Matsui, T., & Furui, S. (1994), “Comparison of text-independent speaker recognition methods
using VQ-distortion and discrete/continuous HMMs,” IEEE Transactions on Speech and Audio
Processing 2(3), 456–459.
Matsumoto, Y., Kitauchi, A., Yamashita, T., et al. (1999), “Japanese morphological analysis
system ChaSen version 2.0 manual,” NAIST Technical Report.
McCallum, A., & Nigam, K. (1998), “A comparison of event models for naive Bayes
text classiﬁcation,” in Proceedings of the Association for the Advancement of Arti-
ﬁcial Intelligence (AAAI) Workshop on Learning for Text Categorization, Vol. 752,
pp. 41–48.
McDermott, E., Hazen, T., Le Roux, J., Nakamura, A., & Katagiri, S. (2007), “Discriminative
training for large-vocabulary speech recognition using minimum classiﬁcation error,” IEEE
Transactions on Audio, Speech, and Language Processing 15(1), 203–223.
Meignier, S., Moraru, D., Fredouille, C., Bonastre, J.-F., & Besacier, L. (2006), “Step-by-step and
integrated approaches in broadcast news speaker diarization,” Computer Speech & Language
20(2), 303–330.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953),
“Equation of state calculations by fast computing machines,” Journal of Chemical Physics
21(6), 1087–1092.
Minka, T. P. (2001), “Expectation propagation for approximate Bayesian inference,” Proceedings
of Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp. 362–369.
Mochihashi, D., Yamada, T., & Ueda, N. (2009), “Bayesian unsupervised word segmentation with
nested Pitman–Yor language modeling,” Proceedings of Joint Conference of Annual Meeting
of the ACL and International Joint Conference on Natural Language Processing of the AFNLP,
pp. 100–108.
Mohri, M., Pereira, F., & Riley, M. (2002), “Weighted ﬁnite-state transducers in speech
recognition,” Computer Speech and Language 16, 69–88.
Moraru, D., Meignier, S., Besacier, L., Bonastre, J.-F., & Magrin-Chagnolleau, I. (2003), “The
ELISA consortium approaches in speaker segmentation during the NIST 2002 speaker recog-
nition evaluation,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), Vol. 2, pp. 89–92.
Mrva, D., & Woodland, P. C. (2004), “A PLSA-based language model for conversational
telephone speech,” in Proceedings of Annual Conference of International Speech Communica-
tion Association (INTERSPEECH), pp. 2257–2260.

References
415
Murphy, K. P. (2002), Dynamic Bayesian networks: representation, inference and learning, PhD
thesis, University of California, Berkeley.
Murphy, K. P., Weiss, Y., & Jordan, M. I. (1999), “Loopy belief propagation for approxi-
mate inference: An empirical study,” Proceedings of Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 467–475.
Nadas, A. (1985), “Optimal solution of a training problem in speech recognition,” IEEE
Transactions on Acoustics, Speech and Signal Processing 33(1), 326–329.
Nakagawa, S. (1988), Speech Recognition by Probabilistic Model, Institute of Electronics,
Information and Communication Engineers (IEICE) (in Japanese).
Nakamura, A., McDermott, E., Watanabe, S., & Katagiri, S. (2009), “A uniﬁed view for
discriminative objective functions based on negative exponential of difference measure
between strings,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 1633–1636.
Neal, R., & Hinton, G. (1998), “A view of the EM algorithm that justiﬁes incremental, sparse, and
other variants,” Learning in Graphical Models, pp. 355–368.
Neal, R. M. (1992), “Bayesian mixture modeling,” Proceedings of the Workshop on Maximum
Entropy and Bayesian Methods of Statistical Analysis 11, 197–211.
Neal, R. M. (1993), “Probabilistic inference using Markov chain Monte Carlo methods,”
Technical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto.
Neal, R. M. (2000), “Markov chain sampling methods for Dirichlet process mixture models,”
Journal of Computational and Graphical Statistics 9(2), 249–265.
Neal, R. M. (2003), “Slice sampling,” Annals of Statistics 31, 705–767.
Neﬁan, A. V., Liang, L., Pi, X., Liu, X., & Murphy, K. (2002), “Dynamic Bayesian net-
works for audio-visual speech recognition,” EURASIP Journal on Applied Signal Processing
11, 1274–1288.
Neubig, G., Mimura, M., Mori, S., & Kawahara, T. (2010), “Learning a language model from con-
tinuous speech,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 1053–1056.
Ney, H., Essen, U., & Kneser, R. (1994), “On structuring probabilistic dependences in stochastic
language modeling,” Computer Speech and Language 8, 1–38.
Ney, H., Haeb-Umbach, R., Tran, B.-H., & Oerder, M. (1992), “Improvements in beam search
for 10000-word continuous speech recognition,” Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), Vol. 1, IEEE, pp. 9–12.
Niesler, T., & Willett, D. (2002), “Unsupervised language model adaptation for lecture speech
transcription,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 1413–1416.
Normandin, Y. (1992), “Hidden Markov models, maximum mutual information estimation, and
the speech recognition problem,” PhD thesis, McGill University, Montreal, Canada.
Odell, J. J. (1995), The use of context in large vocabulary speech recognition, PhD thesis,
Cambridge University.
Ostendorf, M., & Singer, H. (1997), “HMM topology design using maximum likelihood succes-
sive state splitting,” Computer Speech and Language 11, 17–41.
Paul, D. B., & Baker, J. M. (1992), “The design for the Wall Street Journal-based CSR corpus,”
Proceedings of the Workshop on Speech and Natural Language, Association for Computational
Linguistics, pp. 357–362.
Pettersen, S. (2008), Robust speech recognition in the presence of additive noise, PhD thesis,
Norwegian University of Science and Technology.

416
References
Pitman, J. (2002), “Poisson-Dirichlet and GEM invariant distributions for split-and-merge trans-
formation of an interval partition,” Combinatorics, Probability and Computing 11, 501–514.
Pitman, J. (2006), Combinatorial Stochastic Processes, Springer-Verlag.
Pitman, J., & Yor, M. (1997), “The two-parameter Poisson-Dirichlet distribution derived from a
stable subordinator,” Annals of Probability 25(2), 855–900.
Porteous, I., Newman, D., Ihler, A., et al. (2008), “Fast collapsed Gibbs sampling for latent Dirich-
let allocation,” Proceedings of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 569–577.
Povey, D. (2003), Discriminative training for large vocabulary speech recognition, PhD thesis,
Cambridge University.
Povey, D., Burget, L., Agarwal, M., et al. (2010), “Subspace Gaussian mixture models for
speech recognition,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 4330–4333.
Povey, D., Gales, M. J. F., Kim, D., & Woodland, P. C. (2003), “MMI-MAP and MPE-MAP for
acoustic model adaptation,” Proceedings of European Conference on Speech Communication
and Technology (EUROSPEECH) 8, 1981–1984.
Povey, D., Ghoshal, A., Boulianne, G., et al. (2011), “The Kaldi speech recognition toolkit,”
Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).
Povey, D., Kanevsky, D., Kingsbury, B., et al. (2008), “Boosted MMI for model and feature-space
discriminative training,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 4057–4060.
Povey, D., Kingsbury, B., Mangu, L., et al. (2005), “fMPE: Discriminatively trained features for
speech recognition,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP) 1, 961–964.
Povey, D., & Woodland, P. C. (2002), “Minimum phone error and I-smoothing for improved
discriminative training,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 13–17.
Povey, D., Woodland, P., & Gales, M. (2003), “Discriminative MAP for acoustic model adapta-
tion,” Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP) 1, I–312.
Price, P., Fisher, W., Bernstein, J., & Pallett, D. (1988), “The DARPA 1000-word resource man-
agement database for continuous speech recognition,” Proceedings of International Conference
on Acoustics, Speech, and Signal Processing (ICASSP), pp. 651–654.
Rabiner, L. R., & Juang, B.-H. (1986), “An introduction to hidden Markov models,” IEEE ASSP
Magazine 3(1), 4–16.
Rabiner, L. R., & Juang, B.-H. (1993), Fundamentals of Speech Recognition, Vol. 14, PTR
Prentice Hall.
Rasmussen, C. E. (1999), “The inﬁnite Gaussian mixture model,” Advances in Neural Information
Processing Systems 12, 554–560.
Rasmussen, C. E., & Williams, C. K. I. (2006), Gaussian Processes for Machine Learning,
Adaptive Computation and Machine Learning, MIT Press.
Reynolds, D., Quatieri, T., & Dunn, R. (2000), “Speaker veriﬁcation using adapted Gaussian
mixture models,” Digital Signal Processing 10(1-3), 19–41.
Rissanen, J. (1984), “Universal coding, information, prediction and estimation,” IEEE Transac-
tions on Information Theory 30, 629–636.
Rodriguez, A., Dunson, D. B., & Gelfand, A. E. (2008), “The nested Dirichlet process,” Journal
of the American Statistical Association 103(483), 1131–1154.

References
417
Rosenfeld, R. (2000), “Two decades of statistical language modeling: Where do we go from
here?,” Proceedings of the IEEE 88(8), 1270–1278.
Sainath, T. N., Ramabhadran, B., Picheny, M., Nahamoo, D., & Kanevsky, D. (2011), “Exemplar-
based sparse representation features: from TIMIT to LVCSR,” IEEE Transactions on Audio,
Speech and Language Processing 19(8), 2598–2613.
Saito, D., Watanabe, S., Nakamura, A., & Minematsu, N. (2012), “Statistical voice conver-
sion based on noisy channel model,” IEEE Transactions on Audio, Speech, and Language
Processing 20(6), 1784–1794.
Salakhutdinov, R. (2009), Learning deep generative models, PhD thesis, University of Toronto.
Salton, G., & Buckley, C. (1988), “Term-weighting approaches in automatic text retrieval,”
Information Processing & Management 24(5), 513–523.
Sanderson, C., Bengio, S., & Gao, Y. (2006), “On transforming statistical models for non-frontal
face veriﬁcation,” Pattern Recognition 39(2), 288–302.
Sankar, A., & Lee, C.-H. (1996), “A maximum-likelihood approach to stochastic match-
ing for robust speech recognition,” IEEE Transactions on Speech and Audio Processing
4(3), 190–202.
Saon, G., & Chien, J.-T. (2011), “Some properties of Bayesian sensing hidden Markov models,”
Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),
pp. 65–70.
Saon, G., & Chien, J.-T. (2012a), “Bayesian sensing hidden Markov models,” IEEE Transactions
on Audio, Speech, and Language Processing 20(1), 43–54.
Saon, G., & Chien, J.-T. (2012b), “Large-vocabulary continuous speech recognition systems:
A look at some recent advances,” IEEE Signal Processing Magazine 29(6), 18–33.
Schalkwyk, J., Beeferman, D., Beaufays, F., et al. (2010), “ ‘Your word is my command’: Google
search by voice: A case study,” in Advances in Speech Recognition, Springer, pp. 61–90.
Schlüter, R., Macherey, W., Müller, B., & Ney, H. (2001), “Comparison of discriminative
training criteria and optimization methods for speech recognition,” Speech Communication
34(3), 287–310.
Schultz, T., & Waibel, A. (2001), “Language-independent and language-adaptive acoustic model-
ing for speech recognition,” Speech Communication 35(1), 31–51.
Schwarz, G. (1978), “Estimating the dimension of a model,” The Annals of Statistics
6(2), 461–464.
Scott, S. (2002), “Bayesian methods for hidden Markov models,” Journal of the American
Statistical Association 97(457), 337–351.
Seide, F., Li, G., Chen, X., & Yu, D. (2011), “Conversational speech transcription using context-
dependent deep neural networks,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 437–440.
Sethuraman, J. (1994), “A constructive deﬁnition of Dirichlet priors,” Statistica Sinica 4, 639–650.
Shikano, K., Kawahara, T., Kobayashi, T., et al. (1999), Japanese Dictation Toolkit –
Free Software Repository for Automatic Speech Recognition, http://www.ar.media.kyoto-
u.ac.jp/dictation/.
Shinoda, K. (2010), “Acoustic model adaptation for speech recognition,” IEICE Transactions on
Information and Systems 93(9), 2348–2362.
Shinoda, K., & Inoue, N. (2013), “Reusing speech techniques for video semantic indexing,” IEEE
Signal Processing Magazine 30(2), 118–122.
Shinoda, K., & Iso, K. (2001), “Efﬁcient reduction of Gaussian components using MDL criterion
for HMM-based speech recognition,” Proceedings of International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pp. 869–872.

418
References
Shinoda, K., & Lee, C.-H. (2001), “A structural Bayes approach to speaker adaptation,” IEEE
Transactions on Speech and Audio Processing 9, 276–287.
Shinoda, K., & Watanabe, T. (1996), “Speaker adaptation with autonomous model complexity
control by MDL principle,” Proceedings of International Conference on Acoustic, Speech, and
Signal Processing (ICASSP), pp. 717–720.
Shinoda, K., & Watanabe, T. (1997), “Acoustic modeling based on the MDL criterion for speech
recognition,” Proceedings of European Conference on Speech Communication and Technology
(EUROSPEECH), Vol. 1, pp. 99–102.
Shinoda, K., & Watanabe, T. (2000), “MDL-based context-dependent subword modeling for
speech recognition,” Journal of the Acoustical Society of Japan (E) 21, 79–86.
Shiota, S., Hashimoto, K., Nankaku, Y., & Tokuda, K. (2009), “Deterministic annealing based
training algorithm for Bayesian speech recognition,” Proceedings of Annual Conference of
International Speech Communication Association (INTERSPEECH), pp. 680–683.
Siohan, O., Myrvoll, T. A., & Lee, C. H. (2002), “Structural maximum a posteriori linear
regression for fast HMM adaptation,” Computer Speech and Language 16(1), 5–24.
Siu, M.-h., Gish, H., Chan, A., Belﬁeld, W., & Lowe, S. (2014), “Unsupervised training of
an HMM-based self-organizing unit recognizer with applications to topic classiﬁcation and
keyword discovery,” Computer Speech & Language 28(1), 210–223.
Somervuo, P. (2004), “Comparison of ML, MAP, and VB based acoustic models in large
vocabulary speech recognition,” Proceedings of International Conference on Spoken Language
Processing (ICSLP), pp. 830–833.
Spiegelhalter, D. J., & Lauritzen, S. L. (1990), “Sequential updating of conditional probabilities
on directed graphical structures,” Networks 20(5), 579–605.
Sproat, R., Gale, W., Shih, C., & Chang, N. (1996), “A stochastic ﬁnite-state word-segmentation
algorithm for Chinese,” Computational Linguistics 22(3), 377–404.
Stenger, B., Ramesh, V., Paragios, N., Coetzee, F., & Buhmann, J. M. (2001), “Topology free
hidden Markov models: Application to background modeling,” Proceedings of International
Conference on Computer Vision (ICCV)’, Vol. 1, pp. 294–301.
Stolcke, A., Ferrer, L., Kajarekar, S., Shriberg, E., & Venkataraman, A. (2005), “MLLR trans-
forms as features in speaker recognition,” Proceedings of Annual Conference of International
Speech Communication Association (INTERSPEECH), pp. 2425–2428.
Stolcke, A., & Omohundro, S. (1993), “Hidden Markov model induction by Bayesian model
merging,” Advances in Neural Information Processing Systems, pp. 11–18, Morgan Kaufmann.
Takahashi, J., & Sagayama, S. (1997), “Vector-ﬁeld-smoothed Bayesian learning for fast
and incremental speaker/telephone-channel adaptation,” Computer Speech and Language
11, 127–146.
Takami, J., & Sagayama, S. (1992), “A successive state splitting algorithm for efﬁcient allo-
phone modeling,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 573–576.
Tam, Y.-C., & Schultz, T. (2005), “Dynamic language model adaptation using variational
Bayes inference,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 5–8.
Tam, Y.-C., & Schultz, T. (2006), “Unsupervised language model adaptation using latent seman-
tic marginals,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 2206–2209.
Tamura, M., Masuko, T., Tokuda, K., & Kobayashi, T. (2001), “Adaptation of pitch and spectrum
for HMM-based speech synthesis using MLLR,” Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), pp. 805–808.

References
419
Tawara, N., Ogawa, T., Watanabe, S., & Kobayashi, T. (2012a), “Fully Bayesian inference
of multi-mixture Gaussian model and its evaluation using speaker clustering,” Proceed-
ings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP),
pp. 5253–5256.
Tawara, N., Ogawa, T., Watanabe, S., Nakamura, A., & Kobayashi, T. (2012b), “Fully Bayesian
speaker clustering based on hierarchically structured utterance-oriented Dirichlet process
mixture model,” Proceedings of Annual Conference of International Speech Communication
Association (INTERSPEECH), pp. 2166–2169.
Teh, Y. W. (2006), “A hierarchical Bayesian language model based on Pitman–Yor processes,”
Proceedings of International Conference on Computational Linguistics and Annual Meeting of
the Association for Computational Linguistics, pp. 985–992.
Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006), “Hierarchical Dirichlet processes,”
Journal of the American Statistical Association 101(476), 1566–1581.
Tipping, M. E. (2001), “Sparse Bayesian learning and the relevance vector machine,” Journal of
Machine Learning Research 1, 211–244.
Torbati, A. H. H. N., Picone, J., & Sobel, M. (2013), “Speech acoustic unit segmentation using
hierarchical Dirichlet processes,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 637–641.
Ueda, N., & Ghahramani, Z. (2002), “Bayesian model search for mixture models based on
optimizing variational bounds,” Neural Networks 15, 1223–1241.
Valente, F. (2006), “Inﬁnite models for speaker clustering,” Proceedings of Annual Conference of
International Speech Communication Association (INTERSPEECH), pp. 1329–1332.
Valente, F., Motlicek, P., & Vijayasenan, D. (2010), “Variational Bayesian speaker diarization of
meeting recordings,” Proceedings of International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 4954–4957.
Valente, F., & Wellekens, C. (2003), “Variational Bayesian GMM for speech recogni-
tion,” Proceedings of European Conference on Speech Communication and Technology
(EUROSPEECH), pp. 441–444.
Valente, F., & Wellekens, C. (2004a), “Variational Bayesian feature selection for Gaussian mixture
models,” Proceedings of International Conference on Acoustics, Speech, and Signal Processing
(ICASSP), Vol. 1, pp. 513–516.
Valente, F., & Wellekens, C. (2004b), “Variational Bayesian speaker clustering,” Proceedings of
ODYSSEY The Speaker and Language Recognition Workshop, pp. 207–214.
Vapnik, V. (1995), The Nature of Statistical Learning Theory, Springer-Verlag.
Vesel`y, K., Ghoshal, A., Burget, L., & Povey, D. (2013), “Sequence-discriminative train-
ing of deep neural networks,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 2345–2349.
Villalba, J., & Brümmer, N. (2011), “Towards fully Bayesian speaker recognition: Integrating out
the between-speaker covariance,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 505–508.
Vincent, E., Barker, J., Watanabe, S., et al. (2013), “The second ‘CHiME’ speech separation and
recognition challenge: An overview of challenge systems and outcomes,” Proceedings of IEEE
Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 162–167.
Viterbi, A. J. (1967), “Error bounds for convolutional codes and an asymptotically optimal
decoding algorithm,” IEEE Transactions on Information Theory IT-13, 260–269.
Wallach, H. M. (2006), “Topic modeling: beyond bag-of-words,” Proceedings of International
Conference on Machine Learning, pp. 977–984.

420
References
Watanabe, S., & Chien, J. T. (2012), “Tutorial: Bayesian learning for speech and language
processing,” International Conference on Acoustics, Speech, and Signal Processing (ICASSP).
Watanabe, S., Minami, Y., Nakamura, A., & Ueda, N. (2002), “Application of variational Bayesian
approach to speech recognition,” Advances in Neural Information Processing Systems.
Watanabe, S., Minami, Y., Nakamura, A., & Ueda, N. (2004), “Variational Bayesian estimation
and clustering for speech recognition,” IEEE Transactions on Speech and Audio Processing
12, 365–381.
Watanabe, S., & Nakamura, A. (2004), “Acoustic model adaptation based on coarse-ﬁne training
of transfer vectors and its application to a speaker adaptation task,” Proceedings of International
Conference on Spoken Language Processing (ICSLP), pp. 2933–2936.
Watanabe, S., & Nakamura, A. (2006), “Speech recognition based on Student’s t-distribution
derived from total Bayesian framework,” IEICE Transactions on Information and Systems
E89-D, 970–980.
Watanabe, S., & Nakamura, A. (2009), “On-line adaptation and Bayesian detection of envi-
ronmental changes based on a macroscopic time evolution system,” Proceedings of IEEE
International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 4373–
4376.
Watanabe, S., Nakamura, A., & Juang, B. (2011), “Bayesian linear regression for hidden Markov
model based on optimizing variational bounds,” Proceedings of IEEE Workshop on Machine
Learning for Signal Processing, pp. 1–6.
Watanabe, S., Nakamura, A., & Juang, B.-H. (2013), “Structural Bayesian linear regression for
hidden Markov models,” Journal of Signal Processing Systems, 1–18.
Wegmann, S., McAllaster, D., Orloff, J., & Peskin, B. (1996), “Speaker normalization on conver-
sational telephone speech,” Proceedings of International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pp. 339–341.
Winn, J., & Bishop, C. (2006), “Variational message passing,” Journal of Machine Learning
Research 6(1), 661.
Witten, I. H., & Bell, T. C. (1991), “The zero-frequency problem: estimating the probabilities
of novel events in adaptive text compression,” IEEE Transactions on Information Theory
37, 1085–1094.
Wooters, C., Fung, J., Peskin, B., & Anguera, X. (2004), “Towards robust speaker segmentation:
The ICSI-SRI fall 2004 diarization system,” in RT-04F Workshop, Vol. 23.
Wooters, C., & Huijbregts, M. (2008), “The ICSI RT07s speaker diarization system,” in
Multimodal Technologies for Perception of Humans, Springer, pp. 509–519.
Yamagishi, J., Kobayashi, T., Nakano, Y., Ogata, K., & Isogai, J. (2009), “Analysis of speaker
adaptation algorithms for HMM-based speech synthesis and a constrained SMAPLR adaptation
algorithm,” IEEE Transactions on Audio, Speech, and Language Processing 17(1), 66–83.
Yaman, S., Chien, J.-T., & Lee, C.-H. (2007), “Structural Bayesian language modeling and adapta-
tion,” Proceedings of Annual Conference of International Speech Communication Association
(INTERSPEECH), pp. 2365–2368.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2003), “Understanding belief propagation and its
generalizations,” Exploring Artiﬁcial Intelligence in the New Millennium 8, 236–239.
Young, S., Evermann, G., Gales, M., et al. (2006), “The HTK book (for HTK version 3.4),”
Cambridge University Engineering Department.
Young, S. J., Odell, J. J., & Woodland, P. C. (1994), “Tree-based state tying for high accu-
racy acoustic modelling,” Proceedings of the Workshop on Human Language Technology,
pp. 307–312.

References
421
Yu, K., & Gales, M. J. F. (2006), “Incremental adaptation using Bayesian inference,” Proceedings
of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp. 217–
220.
Zhang, Y., & Glass, J. R. (2009), “Unsupervised spoken keyword spotting via segmental
DTW on Gaussian posteriorgrams,” Proceedings of IEEE Automatic Speech Recognition &
Understanding Workshop (ASRU), pp. 398–403.
Zhang, Y., Liu, P., Chien, J.-T., & Soong, F. (2009), “An evidence framework for Bayesian learning
of continuous-density hidden Markov models,” Proceedings of International Conference on
Acoustics, Speech, and Signal Processing (ICASSP), pp. 3857–3860.
Zhao, X., Dong, Y., Zhao, J., et al. (2009), “Variational Bayesian joint factor analysis for speaker
veriﬁcation,” Proceedings of IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pp. 4049–4052.
Zhou, B., & Hansen, J. H. (2000), “Unsupervised audio stream segmentation and clustering via
the Bayesian information criterion,” Proceedings of Annual Conference of International Speech
Communication Association (INTERSPEECH), pp. 714–717.
Zweig, G., & Nguyen, P. (2009), “A segmental CRF approach to large vocabulary continuous
speech recognition,” Proceedings of IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU), pp. 152–157.
Zweig, G., & Russell, S. (1998), “Speech recognition with dynamic Bayesian networks,”
Proceedings of the National Conference on Artiﬁcial Intelligence, pp. 173–180.

Index
Page numbers in bold type indicate a main explanatory entry.
Acoustic model, 3, 55, 59
Acoustic unit discovery, 372
Agglomerative clustering, 240
Akaike information criterion, 217
Approximate inference, 47, 137, 320, 385
Automatic relevance determination, 192, 193
Backoff smoothing, 107, 108, 208, 380
Backward algorithm, 66, 73
Backward variable, 72, 85, 90, 159, 266
Bag of words, 114, 318, 324
Basis representation, 192
Baum–Welch algorithm, 90, 160, 254, 257, 284, 285
Bayes decision, 8, 54, 97, 218, 274
Bayes factor, 214, 233, 238, 240, 282
Bayes risk, 55, 56
Bayesian compressive sensing, 193
Bayesian information criterion, 211, 214, 216, 273,
361
Bayesian interpretation, 189, 379
Bayesian model comparison, 185
Bayesian network, 41
Bayesian neural network, 225
Bayesian nonparametrics, 337, 345
Bayesian predictive classiﬁcation, 58, 218, 274, 282
Belief propagation, 46
Bernoulli distribution, 131, 226
Beta distribution, 348, 386, 393, 394
Canonical form, 17, 24, 27, 34
Chinese restaurant franchise, 355
Chinese restaurant process, 349, 380, 383
Cholesky decomposition, 94, 289
Co-occurrence, 113
Collapsed Gibbs sampling, 344, 363
Complete data, 61, 77
Complete data likelihood, 61, 65
Complete square, 30, 34, 152, 391
Concave function, 244, 389
Concentration parameter, 348, 385
Conditional independence, 38, 40, 119, 132, 244,
341
Conditional probability, 14
Conjugate distribution, 17, 24, 138, 292
Conjugate prior, 25, 177, 178, 191, 206, 223, 344
Context-dependent phoneme, 59
Continuous density hidden Markov model, 63, 143,
254, 373
Convex function, 78, 320
Corrective training, 177
Correlation matrix, 114
Cosine similarity, 114, 118, 173
Cross entropy error function, 228
Cross validation, 189
De Finetti’s theorem, 346, 348
Decision tree clustering, 230, 282
Decoding algorithm, 51, 204
Deep neural network, 4, 224
Di-gamma function, 209, 262, 265, 321, 389
Dirac delta function, 16, 33, 140, 213, 246, 388
Dirichlet class language model, 326
Dirichlet distribution, 38, 144, 190, 256, 394
Dirichlet process, 348
Dirichlet process mixture model, 351, 373
Discrete uniform distribution, 102, 392
Discriminative approach, 130
Discriminative training, 166, 200
Distribution estimation, 47, 184, 187, 193, 228, 269,
379
Document model, 176, 318
Dynamic Bayesian network, 41
Dynamic programming, 70, 75
Eigenvoice, 164
EM algorithm, 66, 76, 113, 120, 123, 141, 178, 227,
251
Emission probability, 62
Entropy, 116
Evidence framework, 185, 190, 208
Evidence function, 17, 184, 185, 188, 196, 207, 208

Index
423
Exchangeability, 346, 357
Expectation propagation, 7
Exponential family, 17, 26, 188
Extended Baum–Welch algorithm, 167
Factor analysis, 7, 174, 197, 307, 317
Factor graph, 45
Factor loading, 197
Feature extraction, 4, 9, 336
Feature-space MLLR, 204, 289
Fisher information, 216
Forward algorithm, 66, 71
Forward variable, 70, 75, 85, 90, 158, 266
Forward–backward algorithm, 46, 70
Gamma distribution, 30, 38, 256, 308, 400
Gamma function, 30, 278, 389
Gaussian distribution, 18–20, 38, 395
Gaussian mixture model, 4, 66, 67, 172, 361
Gaussian supervector, 173, 306
Gaussian–gamma distribution, 31, 38, 402
Gaussian–Wishart distribution, 35, 38, 190, 256, 403
GEM distribution, 349
Generalization error, 191
Generative approach, 130
Generative model, 43, 53
Gibbs sampling, 343, 347, 373, 375, 386
Graphical model, 13, 40, 67, 367
Heavy-tailed distribution, 383
Hessian matrix, 187, 212, 215, 221, 229
Hidden Markov model, 3, 59, 68, 188
Hidden variable, 77, 309, 375
Hierarchical Dirichlet language model, 205, 379,
383
Hierarchical Dirichlet process, 337, 352
Hierarchical Pitman–Yor language model, 378, 384
Hierarchical Pitman–Yor process, 383
Hierarchical prior, 193, 206, 207, 291, 356, 384
Hyperparameter, 25, 43, 180, 184, 187, 208, 219,
229
i-smoothing factor, 169
Ill-posed problem, 188, 218
Implicit solution, 198, 200, 210
Importance sampling, 338
Importance weights, 339
Incomplete data, 77, 195, 227
Incremental hyperparameter, 180
Incremental learning, 177, 179
Independently, identically distributed (iid), 216, 349
Inﬁnitely exchangeable, 346
Information retrieval, 3, 10, 115
Interpolation smoothing, 102, 103, 104, 176, 208
Inverse gamma distribution, 401
Jacobian, 338
Jelinek–Mercer smoothing, 102
Jensen’s inequality, 76, 244, 290, 320, 389
Joint probability, 14
Katz smoothing, 106
Kernel parameter, 191
Kronecker delta function, 16, 85, 246, 324, 388
Kullback–Leibler divergence, 79, 201, 243, 288,
290, 316, 320
l0 regularization, 217
l1 regularization, 139
l2 regularization, 139
Lagrange multiplier, 86, 100, 123, 178, 246, 253
Language model, 3, 55, 97, 97, 116, 125, 205, 208,
324, 378, 383
Laplace approximation, 187, 211, 220, 229
Laplace distribution, 139, 399
Latent Dirichlet allocation, 360
Latent model, 61
Latent semantic analysis, 113, 177
Latent topic, 119, 319
Latent topic model, 7, 43, 53, 119, 123, 138, 176,
242, 318, 337, 360
Least-squares regression, 188
Level-1 inference, 188
Level-2 inference, 188
Levenshtein distance, 56
Lexical model, 60
Linear regression, 188
Logistic regression, 46
Logistic sigmoid, 58, 226
Loopy belief propagation, 47
Loss function, 55
Machine learning, 3
Machine translation, 3, 55, 97
MAP adaptation, 165, 172
MAP decision rule, 55, 58, 129, 130
MAP–EM algorithm, 143
Marginal likelihood, 17, 51, 65, 70, 184, 196, 208,
228, 293, 319, 361, 362, 368
Marginalization, 50
Markov chain, 340
Markov chain Monte Carlo, 337, 347, 356, 358, 385
Markov random ﬁeld, 40, 44
Matrix variate Gaussian distribution, 292, 399
Max sum algorithm, 46
Maximum a-posteriori, 49, 137, 177, 228
Maximum evidence, 186
Maximum likelihood, 76, 113, 120, 227
Maximum likelihood linear regression, 91, 163, 164,
174, 200, 287
Maximum mutual information, 167, 201
Message passing, 46
Metropolis–Hastings algorithm, 341
Minimum Bayes risk, 56
Minimum classiﬁcation error, 167

424
Index
Minimum description length, 230, 273
Minimum discrimination information, 127
Minimum mean square error, 139
Minimum phone error, 167
Model selection, 8, 49, 186
Model variable, 131
Modiﬁed Kneser–Ney smoothing, 110, 204, 387
Multilayer perceptron, 224, 226, 326
Multinomial distribution, 22, 62, 99, 392
Multivariate Gaussian distribution, 38, 396
n-gram, 3, 97, 99, 174
Naive Bayes classiﬁer, 39, 41
Natural parameter, 17
Nested Chinese restaurant process, 360
Nested Dirichlet process, 360
Neural network, 188, 224, 326
Noisy channel model, 8, 55
Noisy speech recognition, 188, 224
Non-negative matrix factorization, 124
Occam factor, 186
Over-ﬁtting problem, 5, 188
P´olya urn model, 346, 348
Partition function, 45
Phoneme, 59
Pitman–Yor process, 379
Plug-in MAP, 140
Point estimation, 47, 184, 187, 218
Positive deﬁnite, 197, 396
Posterior, 6, 15, 133, 185
Power-law, 110, 379, 380
Precision matrix, 34
Predictive distribution, 219, 275
Prior, 15, 48, 138, 185
Probabilistic latent semantic analysis, 119, 120, 176,
318
Product rule, 14, 132
Proposal distribution, 339
Quasi-Bayes, 179
Regression tree, 92, 287
Regularization, 6, 49, 138, 141, 167, 188
Regularization parameter, 139, 188, 191
Regularization theory, 188
Regularized least-squares, 188
Relevance vector machine, 192
Reproducible distribution pair, 181
Robust decision rule, 58, 218
Robustness, 50, 203
Segmental K-means, 90
Singular value decomposition, 114, 177
Slice sampling, 344
Sparse Bayesian learning, 184, 194
Sparse prior distribution, 194
Speaker adaptation, 91, 163, 200
Speaker adaptive training, 91, 97, 163
Speaker clustering, 10, 239, 360
Speaker dependent, 163
Speaker diarization, 240
Speaker independent, 163
Speaker segmentation, 10
Speaker veriﬁcation, 10, 171, 306
Speech recognition, 3, 7, 8, 9, 51, 53, 54, 59, 91, 97,
128, 180, 185, 191, 225, 254
Spherical Gaussian distribution, 292, 398
Stick-breaking construction, 348, 353
Stirling’s approximation, 285, 389
Student’s t-distribution, 193, 281, 404
Sufﬁcient statistics, 17, 89, 95, 159, 180, 259, 363,
368, 377
Sum product algorithm, 46
Sum rule, 14, 132
Sum-of-squares error function, 189
Support vector machine, 173, 188
tf-idf, 113
Tied-state HMM, 230
Triphone, 230
Type-2 maximum likelihood, 187, 319, 328
Uncertainty, 5
Uncertainty techniques, 51
Undirected graph, 44
Unigram model, 99
Unigram rescaling, 127, 325
Universal background model, 172, 306, 367
Variational Bayes, 242, 243, 318, 329
Variational Bayes (VB) auxiliary function, 258
Variational Bayes (VB) Viterbi algorithm, 269
Variational Bayes Baum–Welch algorithm, 269
Variational Bayes expectation and maximization
(VB–EM) algorithm, 252
Variational distribution, 320
Variational lower bound, 242, 243, 269, 285,
288–290, 316
Variational method, 242, 246, 252
Variational parameter, 320
Vector quantization, 62
Vector space model, 10, 114
Viterbi algorithm, 46, 66, 74, 275
Viterbi training, 90
Weak-sense auxiliary function, 201
Weighted ﬁnite state transducer, 60
Wishart distribution, 38, 145, 403
Woodbury matrix inversion, 196, 324, 391
Word-document matrix, 113

